{
    "paper_title": "From KMMLU-Redux to KMMLU-Pro: A Professional Korean Benchmark Suite for LLM Evaluation",
    "authors": [
        "Seokhee Hong",
        "Sunkyoung Kim",
        "Guijin Son",
        "Soyeon Kim",
        "Yeonjung Hong",
        "Jinsik Lee"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The development of Large Language Models (LLMs) requires robust benchmarks that encompass not only academic domains but also industrial fields to effectively evaluate their applicability in real-world scenarios. In this paper, we introduce two Korean expert-level benchmarks. KMMLU-Redux, reconstructed from the existing KMMLU, consists of questions from the Korean National Technical Qualification exams, with critical errors removed to enhance reliability. KMMLU-Pro is based on Korean National Professional Licensure exams to reflect professional knowledge in Korea. Our experiments demonstrate that these benchmarks comprehensively represent industrial knowledge in Korea. We release our dataset publicly available."
        },
        {
            "title": "Start",
            "content": "From KMMLU-REDUX to PRO: Professional Korean Benchmark Suite for LLM Evaluation Seokhee Hong1, Sunkyoung Kim1, Guijin Son2 Soyeon Kim1 Yeonjung Hong1 1LG AI Research 2OnelineAI Jinsik Lee 5 2 0 2 1 1 ] . [ 1 4 2 9 8 0 . 7 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "The development of Large Language Models (LLMs) requires robust benchmarks that encompass not only academic domains but also industrial fields to effectively evaluate their applicability in real-world scenarios. In this paper, we introduce two Korean expert-level benchmarks. KMMLU-REDUX, reconstructed from the existing KMMLU (Son et al., 2024a), consists of questions from the Korean National Technical Qualification exams, with critical errors removed to enhance reliability. KMMLUPRO is based on Korean National Professional Licensure exams to reflect professional knowledge in Korea. Our experiments demonstrate that these benchmarks comprehensively represent industrial knowledge in Korea. We release our dataset publicly available."
        },
        {
            "title": "Introduction",
            "content": "As LLMs continue to achieve strong performance across wide range of subjects (OpenAI et al., 2024b; Deepmind, 2024; DeepSeekAI et al., 2025a; Research et al., 2025), the demand for comprehensive benchmarks has grown. MMLU (Hendrycks et al., 2021) is widely used for its broad coverage of general knowledge from elementary to college level. However, its publicly available online problems has raised concerns about reliability and potential data contamination (Gema et al., 2025; Vendrow et al., 2025; Zhao et al., 2024). We identify similar issues in KMMLU (Son et al., 2024a), widely used benchmark for evaluating Korean expert-level knowledge. The dataset was constructed by crawling websites that provide 1https://huggingface.co/datasets/LGAI-EXAONE/ KMMLU-Redux https://huggingface.co/datasets/LGAI-EXAONE/ KMMLU-Pro Authors equally contributed. {seokhee.hong, Email to: sunkyoung.kim, jinsik.lee}@lgresearch.ai, guijin.son@onelineai.com questions from various exams. We observe noisy samples, including problems that explicitly reveal the answer or non-existent reference, which can mislead performance evaluation. Additionally, we find evidence of contamination between the train and test splits, as well as with common web corpus such as FineWeb2 (Penedo et al., 2025). Instead of collecting data from online sources directly, recent challenging benchmarks (Srivastava et al., 2023; Rein et al., 2024; Phan et al., 2025; Kazemi et al., 2025; Team et al., 2025b) have been constructed through problems and answers authored by human expert. Although this approach ensures high-quality, contamination-free benchmarks, it is costly to construct and maintain. The high construction cost hinders regular updates (White et al., 2025; Jain et al., 2025), leaving the benchmarks vulnerable to deprecation and contamination. Furthermore, existing benchmarks primarily focus on academic knowledge and often overlook the practical applicability of models in industrial or professional contexts. As LLMs are increasingly adopted in industrial domains (Chkirbene et al., 2024), it becomes essential to assess whether they possess the necessary expertise to support tasks that require certified knowledge. For example, before deploying an LLM as legal assistant, one must ensure that the model can reliably understand to meet professional certification standards. We introduce two benchmarks to address the limitations above and incorporate professional knowledge to evaluate the practical applicability of LLMs. First, KMMLU-REDUX, refined subset of KMMLU with 2,587 problems, is built through rigorous manual examination by authors to reduce errors and contamination within KMMLU. While the KMMLU contains problems from wide range of exams, even with high-school level, KMMLUREDUX only selects Korean National Technical Qualification (KNTQ) exams as sources of the benchmark. The exams require applicants to have Figure 1: Performance of leading models developed by diverse groups on industrial knowledge for KMMLU-REDUX (left) and licensure exam pass status (indicated by ) for KMMLU-PRO (right). either bachelors degree or at least nine years experience in industrial field, thus making the benchmark more challenging. Second, we build KMMLU-PRO, new challenging benchmark, which consists of 2,822 problems from acquisition exams for Korean National Professional Licensure (KNPL), representing highly specialized professions in Korea. We include 14 professions  (Table 1)  from diverse domains. Unlike KMMLU that crawls websites, we collect data directly from the official source of each license. After that, human annotators manually examine it to avoid noises. KMMLU-PRO only includes exams held in the most recent year and would be updated annually with the latest exam to maintain long-term reliability and prevent contamination. We conduct extensive evaluations of various LLMs on the two benchmarks. Our benchmarks, based on real-world exams, enable aligned analysis with industrial and professional qualifications, effectively revealing the practical strengths of each model. As shown in Figure 1, KMMLU-REDUX (left) covers wide range of industrial domains, allowing it to evaluate the breadth of industrial knowledge. LLMs show robust performance in engineering domains but exhibit notable declines in specialized fields such as Mining & Resources and Architecture. On the other hand, KMMLU-PRO (right) focuses on professional licensure exams and thus assesses whether model can pass the exams required for high-stakes professions in Korea. The results show that, in KMMLU-PRO, several stateof-the-art models perform strongly in the medicine domain, meeting the passing criteria of most licenses, yet nearly fail in law-related licenses. Moreover, we observe the significant performance gaps between our datasets and merely translated datasets like MMMLU (OpenAI, 2024), especially in domains such as laws, where in-depth knowledge of specific countries is required (see Section 6.1). We argue that these findings underscore the practicality of our benchmarks for assessing the capabilities of models in professional fields within Korea. To summarize, our contributions are as follows: We improve the previous benchmark, KMMLU, to construct the refined and compact version of the benchmark, KMMLUREDUX, by correcting various errors. We introduce KMMLU-PRO, new benchmark designed to evaluate high-level professional knowledge in Korea. By imitating realworld license acquisition systems, KMMLUPRO assesses the industrial practicality across various professions in Korea. We comprehensively analyze the results of two benchmarks, highlighting the importance of benchmarks specialized in Korea-specific professional knowledge. We then annotate the entire test set using GPT4o3 (OpenAI et al., 2024a). In total, we find that 7.66% of the data contains one of the errors mentioned above. We describe the details of the investigation of dataset errors in Appendix A. In Figure 2, we observe significant discrepancies in the LLMs performance between erroneous and correct samples. Notably, the performance of instances with leaked answers drops significantly when the LLMs are assessed on the clean dataset. Conversely, all models scores increase on ill-posed questions, underscoring their inability to identify the correct answer for poorly formulated questions. Contamination The KMMLU was primarily sourced online, making them highly susceptible to contamination from web-crawled training corpora. When applying n-gram contamination detection (Lambert et al., 2025; Grattafiori et al., 2024) to the Korean subset of FineWeb2 (Penedo et al., 2025) and the KMMLU dataset, 1.88% of the data were flagged as contaminated."
        },
        {
            "title": "2.2 Dataset Construction",
            "content": "KMMLU consists of approximately 35k examples, making evaluation resource-intensive. To reduce this burden, we first restrict the scope to subset of high-difficulty exams (Section 2.2.1). Furthermore, to ensure the reliability of the benchmark, we manually conduct thorough examination of the dataset to eliminate errors (Section 2.2.2)."
        },
        {
            "title": "2.2.1 Filtering Non-Challenging Problems",
            "content": "Since KMMLU includes variety of exams in Korea, spanning from high school to professional certification exams, we filter out the easier exams to build more challenging benchmark. Specifically, we choose Korean National Technical Qualification (KNTQ) exams, which are primarily designed to assess practical technical competencies required in industrial field. The qualifications require applicants to have either bachelors degree or at least nine years of professional experience. We adopt collection of 100 KNTQ exams across the 14 domains in total. We only include the most recent exam for each qualification, thereby avoiding outdated knowledge being evaluated4. 3gpt-4o-2024-11-20 4We have compiled list of all KNTQs exams alongside their most recent exam dates in Appendix B. Our aim is to make these available to LLM researchers and developers to help prevent data contamination. Figure 2: Performance differences in LLMs on the erroneous versus correct dataset. Leaked answer errors tend to overestimate model capabilities, while three other error types hinder LLMs to correctly predict true answers."
        },
        {
            "title": "2 KMMLU-REDUX",
            "content": "We first revisit KMMLU (Son et al., 2024a) to examine its quality. Building on these insights, we construct KMMLU-REDUX, cleaned and compact version of the KMMLU. We carefully denoise against the KMMLU and increase difficulty."
        },
        {
            "title": "2.1 Revisiting KMMLU",
            "content": "KMMLU plays significant role in the NLP community as de facto standard for evaluating LLMs on Korea-specific expert knowledge (Yoo et al., 2024; Research et al., 2024; Team et al., 2025a). The dataset was constructed by crawling websites2 where various exam questions are uploaded by people online, spanning from high school to professional qualification exams. Upon closer inspection, we identify several noises and limitations, which can be categorized into three types: 1) duplication issues, 2) dataset errors, and 3) contamination. Duplication Issue As the KMMLU crawls problems from hundreds of exams in Korea, we observe multiple duplicated questions across related exams. Notably, duplicated samples occur not only within the test set but also between the training and test sets. Using the Longest Common Sequence (LCS) algorithm to investigate overlaps, we could find 5.36% duplication within the test set and 5.46% contamination between train and test set. Dataset Errors Following Gema et al. (2025), we investigate the extent to which various error types appear in the KMMLU test set and their potential impact on LLM performance. We identify four representative error types: leaked answers, illposed questions, poor clarity, and notation errors. 2https://www.kinz.kr/"
        },
        {
            "title": "Names of KNPLs",
            "content": "U.S. Equivalent # of Instances"
        },
        {
            "title": "Law",
            "content": "Certified Judicial Scrivener Lawyer (Kim et al., 2024b) Certified Public Labor Attorney Certified Patent Attorney Paralegal, Legal Document Assistant, or Notary Public (no direct equivalent) Attorney-at-Law Labor & Employment Lawyer (requires J.D. and bar admission; no separate certification in the U.S.) Patent Attorney (JD + USPTO registration required) Tax & Accounting Certified Public Accountant (CPA) Certified Public Accountant (CPA) Exact Equivalent Enrolled Agent (IRS) or CPA with Tax Specialization Certified Tax Accountant U.S. Customs Broker (licensed by U.S. Customs and Border Protection - CBP) Certified Customs Broker"
        },
        {
            "title": "Value Estimation",
            "content": "Certified Damage Adjuster (CDA) Claims Adjuster / Insurance Adjuster (state-licensed) Certified Appraiser Certified Real Estate Appraiser (licensed at the state level)"
        },
        {
            "title": "Medicine",
            "content": "Doctor of Korean Medicine Dentist (Kweon et al., 2024) Pharmacist (Kweon et al., 2024) Herb Pharmacist Physician (Kweon et al., 2024) Licensed Acupuncturist (L.Ac.) or Doctor of Acupuncture and Oriental Medicine (D.A.O.M.) Doctor of Dental Surgery (D.D.S.) / Doctor of Dental Medicine (D.M.D.) Doctor of Pharmacy (Pharm.D.) Herbalist (non-licensed or CAM-certified depending on state) Medical Doctor (M.D./D.O.)"
        },
        {
            "title": "Total",
            "content": "198 150 239 109 208 238 159 120 196 288 252 271 244 150 2822 Table 1: The list of National Professional Licenses (NPLs) used for KMMLU-PRO and their corresponding statistics. The names of NPLs are translated from those in Korea and we also report equivalent licences in U.S. We use KorMedMCQA (Kweon et al., 2024) for three licenses in the Medical category, and KBL (Kim et al., 2024b) for the bar exam of lawyer. To further discriminate simple problems, we leverage the performances of LLMs on the data (Wang et al., 2024; Zellers et al., 2019; Lee et al., 2023). By employing seven smaller LLMs5, we mark data as easy if four or more models correctly predict its answer. Through this process, we remove 38.6% of the dataset."
        },
        {
            "title": "2.2.2 Denoising",
            "content": "To remove noises, we follow the processes described in Section 2.1. Specifically, we first manually review the dataset to minimize errors. Next, we perform decontamination to prevent potential data leakage from pre-training corpora. Additionally, we detect inner duplication and against the training and test sets in KMMLU, thus finally remove all duplicates."
        },
        {
            "title": "2.2.3 Final Statistics",
            "content": "For KMMLU-REDUX, we have collected 2,587 problems from 100 KNTQ exams. Among these, 596 problems are from exams that require over nine years of professional experience to acquire the qualification. To categorize the dataset into 14 domains, we follow the Korean Standard Industrial Classification (KSIC) published by Statistics Korea6 as the qualification system is primarily designed to align with industrial fields. Figure 7 in Appendix illustrates the distribution of domain of KMMLU-REDUX. 5Llama 3.2 3B (Meta, 2024c), Qwen 2.5 3B (Qwen et al., 2025), Gemma 3 4B IT (Team, 2025a), Kanana Nano 2.1B Instruct (Team et al., 2025a), EXAONE 3.5 2.4B (Research et al., 2024), DeepSeek-R1-Distill-Qwen-1.5B (DeepSeek-AI et al., 2025a) EXAONE Deep 2.4B (Research et al., 2025), and Ko-R1-7B-v2.1 (OneLineAI, 2025). 6https://www.kostat.go.kr/"
        },
        {
            "title": "3 KMMLU-PRO",
            "content": "A major challenge in building benchmarks from online sources is data contamination (Zhao et al., 2024; Jain et al., 2025; Roberts et al., 2024). While some studies address this by having experts manually create problems (Srivastava et al., 2023; Rein et al., 2024; Phan et al., 2025; Kazemi et al., 2025; Team et al., 2025b), this approach is costly and time-consuming. As an alternative, recent work explores periodic releases of fresh subsets. (White et al., 2025; Jain et al., 2025). Motivated by these approaches, we focus on the Korean National Professional Licensure (KNPL) exams. These are high-stakes exams administered annually that pose significant challenge. Unlike benchmarks crafted by small set of experts (Rein et al., 2024; Phan et al., 2025), our approach leverages the well-established curricula of professional licensing systems, designed to assess real-world professional knowledge."
        },
        {
            "title": "3.1 Korean National Professional Licensures",
            "content": "We choose KNPL exams for main source of KMMLU-PRO. KNPL exams target high-level professionals, such as lawyers, accountants, or physicians, requiring advanced knowledge, critical reasoning, and ethical judgment. Among them, we select 14 KNPLs representing highly specialized and regulated professions in Korea (See Table 1 for the list of KNPLs and their equivalent licensure in U.S.). These licenses are legally mandated credentials required to practice in their respective domains. As such, they serve as institutionalized gateways to high-status occupations with significant entry barriers. Our evaluation simulates real-world assessment standards by incorporating official exam pass criteria, aligning model performance with human standards."
        },
        {
            "title": "3.2 Dataset Collection and Annotation",
            "content": "In Korea, the government releases and manages the questions for KNPL acquisition exams. We directly download the PDF files from the governments websites for each license and use GPT-4o (OpenAI et al., 2024a) for OCR parsing. As our dataset sources from the official PDFs, we can enhance the quality of the dataset, avoiding potential errors when collecting from online text (Team et al., 2025b). Since GPT-4o has difficulty handling tables and low-resolution PDFs, we employ human annotators to review parsed questions. When problem contains an image, the annotators convert it into text that conveys the same meaning of the image, if possible. Notably, our process remains relatively cost-efficient as it requires human annotators solely for error reviewing tasks, not full annotation. We demonstrate detailed annotation process in Appendix We follow previous works, releasing new set of questions periodically (White et al., 2025; Jain et al., 2025). Since the exams are conducted annually, we commit to collect and release questions from the exams held just before the current year."
        },
        {
            "title": "3.3 Decontamination",
            "content": "We also adopt the same process outlined in Section 2.2.2 to ensure KMMLU-PRO is free from contamination. When conducting n-gram match between KMMLU-PRO, FineWeb2, and the training and validation sets of KMMLU, we did not find any contaminated examples. As result, we can retain all 2,822 data points in the KMMLU-PRO, maintaining its contamination-free integrity."
        },
        {
            "title": "4 Experiments",
            "content": "We select diverse set of baseline models varying in size, multilingual capability, and reasoning ability. By default, we apply zero-shot Chainof-Thought (CoT) (Kojima et al., 2022) prompt written in Korean. However, in early experiments, we observed that some reasoning models perform worse when prompted in Korean. Therefore, we evaluate those models using both Korean and English prompts and report the scores with the highest average score across KMMLU-REDUX and KMMLU-PRO7. We use greedy decoding for nonreasoning models, while for reasoning models, we adopt the temperature of 0.6 and top-p (Holtzman et al., 2020) of 0.95. For more details, see Appendix D. Evaluating opensourced models was conducted over four days using sixteen NVIDIA A100 GPUs. For closed models, we accessed them via API calls, which incurred total cost of approximately $4,000. Metrics In addition to accuracy, our primary evaluation metric, we also report the number of licensure exams each LLM passes in KMMLU-PRO. To better align with human evaluation standards, our procedure is designed to mirror the official certification standards. However, for licenses that rely heavily on image-based questions, full replication is not possible. See Appendix D.3 for details."
        },
        {
            "title": "5.1 Main Results",
            "content": "Table 2 presents the overall performance on KMMLU-REDUX and KMMLU-PRO. The o1 model achieves the highest average accuracy (79.55), followed by Claude 3.7 with Thinking (78.49). Among open-weight models, DeepSeeks R1 even outperforms many closed models. Models equipped with reasoning capabilities consistently perform better than their non-reasoning models, such as the Qwen3 series. Beyond accuracy, we evaluate each models ability to pass the licensure exams under the official criteria for KMMLU-PRO. Specifically, in most licenses, model must score at least 40% in each subject and achieve an overall average of 60% to pass. Claude 3.7 with Thinking succeeds in 12 out of 14 KNPL licensure exams, the highest among all evaluated models. In contrast, although the o1 model achieves higher accuracy, it qualifies for fewer licenses than Claude 3.7 with Thinking. This highlights the importance of balanced competence across subjects, as required in real-world certification exams."
        },
        {
            "title": "5.2 Performance Across Industrial Domains",
            "content": "in KMMLU-REDUX Through KMMLU-REDUX, we assess the technical competencies of models across diverse industrial fields. As shown in Figure 1 (left), models with 7For further details, please see Section 6.4. KMMLU-Redux Acc KMMLU-Pro"
        },
        {
            "title": "Acc",
            "content": "# of passed KNPLs Avg. Acc (micro) Open-weight Models Aya Expanse 32B (Dang et al., 2024) Gemma 3 12B IT (Team, 2025a) Phi-4 (14B) (Abdin et al., 2024) EXAONE 3.5 32B Instruct (Research et al., 2024) Mistral Small 3.1 Instruct (24B) (Mistral, 2025) Gemma 3 27B IT (Team, 2025a) Llama 3.3 70B Instruct (Meta, 2024a) Qwen3-14B (Yang et al., 2025) EXAONE Deep 32B (Research et al., 2025) Qwen3-30B-A3B (Yang et al., 2025) C4AI Command (111B) (Cohere, 2025) Qwen3-32B (Yang et al., 2025) Llama-4-Scout-17B-16E-Instruct (Meta, 2025) Qwen3-30B-A3B (w/ thinking) (Yang et al., 2025) Qwen3-14B (w/ thinking) (Yang et al., 2025) DeepSeek V3 (671B) (DeepSeek-AI et al., 2025b) Qwen3-32B (w/ thinking) (Yang et al., 2025) QwQ 32B (Team, 2025b) Qwen3-235B-A22B (Yang et al., 2025) Qwen3-235B-A22B (w/ thinking) (Yang et al., 2025) Llama-4-Maverick-17B-128E-Instruct (Meta, 2025) DeepSeek R1 (671B) (DeepSeek-AI et al., 2025a)"
        },
        {
            "title": "Closed Models",
            "content": "GPT-4.1 mini (2025-04-14) (OpenAI, 2025a) o3-mini (2025-01-31) (OpenAI, 2025c) Grok-3-mini-beta (xAI, 2025) Grok-3-beta (xAI, 2025) o4-mini (2025-04-16) (OpenAI, 2025b) GPT-4.1 (2025-04-14) (OpenAI, 2025a) Claude 3.7 Sonnet (Anthropic, 2025) o3 (OpenAI, 2025b) Claude 3.7 Sonnet (w/ thinking) (Anthropic, 2025) o1 (OpenAI et al., 2024b) 33.05 46.70 49.75 49.40 52.92 54.04 56.17 57.25 58.33 58.41 62.93 64.98 67.49 65.25 65.71 65.64 68.77 67.34 69.54 74.49 77.58 78.51 67.03 67.84 71.47 72.90 75.80 75.86 76.88 79.92 79.36 81.14 31.26 45.82 45.32 46.71 49.49 51.03 53.24 53.02 52.33 52.33 57.48 58.86 58.14 60.52 60.18 60.77 61.14 63.94 62.12 68.22 68.10 71.33 62.18 62.05 65.08 68.37 69.65 72.99 74.52 73.60 77.70 78.09 0/14 2/14 1/14 2/14 3/14 2/14 3/14 3/14 1/14 3/14 3/14 3/14 4/14 3/14 2/14 4/14 3/14 5/14 4/14 6/14 4/14 7/ 4/14 3/14 5/14 7/14 6/14 10/14 10/14 9/14 12/14 10/14 32.12 46.24 47.44 48.00 51.13 52.47 54.64 55.04 55.20 55.24 60.07 61.79 62.61 62.78 62.82 63.10 64.79 65.57 65.67 71.22 72.63 74.76 64.50 64.82 68.14 70.54 72.59 74.36 75.65 76.62 78.49 79.55 Table 2: The main evaluation results of KMMLU-REDUX and KMMLU-PRO benchmarks on various LLMs. The gray-shaded models stand for reasoning models. The results of models with size < 10B are presented in Table 6 in Appendix E.1. reasoning capabilities consistently outperform their non-reasoning counterparts across all domains. The improvement is particularly notable in Agriculture, Food & Processing, and Architecture. Despite these gains, all models continue to struggle in domains such as Mining Resources and Architecture, highlighting persistent challenges in underrepresented or highly specialized fields. The full results for all models across 14 domains are presented in Appendix E.2."
        },
        {
            "title": "5.3 Professional Licensure Exam",
            "content": "Performance on KMMLU-PRO We provide breakdown of the KMMLU-PRO results by analyzing which licensure exams are most frequently qualified by LLMs. Figure 1 (right) shows the pass rates of LLMs across 14 KNPLs. While many models pass the exams in the medicine domain, most fail in Law and Tax & Accounting, with only DeepSeek R1 passing the Customs Broker exam among open-weight models. Notably, no models pass the Judicial Scrivener or Public Accountant exams. This trend becomes even more evident in the full results across broader range of LLMs; the only licenses that relatively smaller models (<20B) are able to pass are in the medicine domain (see Table 8 in Appendix E.3). Moreover, many LLMs fail in the licensure exams even when scoring above 60%; for example, o3-mini, Qwen3-235B-A22B, and Llama-4Maverick score over 85% on the Pharmacist exam but still fail to qualify due to not meeting the threshFigure 3: Performance of four LLMs on {Medical(left), Accounting(center), Law(right)}-relevant subsets from the MMMLU (Korean) (OpenAI, 2024) and KMMLU-PRO. While the discrepancies in scores are narrow in the medicine domain, they are wider in law-related problems, emphasizing the need for datasets that reflecting real professional knowledge in Korea. old of law-related subject in the exam. These cases highlight the difficulty of acquiring region-specific domain knowledge, particularly in legal subjects governed by Korean law."
        },
        {
            "title": "Benchmarks",
            "content": "To highlight the importance of evaluation grounded in local context (Plaza et al., 2024; Singh et al., 2025), we compare category-level performance between benchmarks translated from English and KMMLU-PRO. Specifically, we focus on subjects related to law, accounting, and medicine, 8 selecting relevant subjects from the Korean subset of MMMLU (OpenAI, 2024), as well as KMMLUPRO. As shown in Figure 3, the performance gap is relatively small in categories such as medicine, where domain knowledge is largely consistent across countries and cultures. In contrast, categories such as law, where substantial differences in content are expected, show significantly larger gap. This suggests that MMMLU, which relies on direct translation of law questions based on U.S. standards, cannot adequately represent knowledge of Korean law. These findings highlight the importance of our dataset, which reflects authentic professional knowledge specific to the Korean context."
        },
        {
            "title": "6.2 KMMLU vs KMMLU-REDUX",
            "content": "Figure 4 illustrates the performance of LLMs on KMMLU and KMMLU-REDUX. The LLMs performances on KMMLU-REDUX is lower than on KMMLU, due to our filtration process which 8{professional_law, jurisprudence, international_law} for Law. {professional_accounting} for Accounting. {professional_medicine, clinical_knowledge, college_medicine, medical_genetics, anatomy} for Medicine. Figure 4: Performance of LLMs on KMMLU and KMMLU-REDUX. high ρ value indicates strong correlation between the results of the two benchmarks. aims to retain only challenging problems from KMMLU (see Section 2.2.1). Despite this decrease, there is near-perfect monotonic association between the results, with Spearmans rank correlation coefficient (ρ) of 0.995, suggesting they are highly correlated. 6."
        },
        {
            "title": "Impact of Reasoning Budget",
            "content": "Recent studies have shown that increasing reasoning efforts can enhance model performance (Muennighoff et al., 2025; Anthropic, 2025; Yang et al., 2025). To examine how reasoning budget affects performance on each licenses in KMMLU-PRO, we conduct an experiment varying the number of tokens allocated to the reasoning path. We adopt Qwen3-32B (Yang et al., 2025) and Claude 3.7 Sonnet (Anthropic, 2025)9. For each budget B, we generate different responses per question and average the scores. We set = 4 for Qwen3, but 9We select the models because they either natively support the thinking budget or have reported experimental results on thinking budgets in their technical report. Figure 5: Reasoning budget results of Qwen3-32B and Claude 3.7 Sonnet on KMMLU-PRO. indicates the Pearson Correlation coefficient value between the reasoning token budget and the accuracy. The responses are sampled multiple times for each thinking budget setting; = 4 and = 2, respectively, for Qwen and Claude. * and ** denote statistical significance, indicating p-value <0.05 and <0.01, respectively. = 2 for Claude due to the generation cost. As shown in Figure 5, we observe positive correlation between the reasoning budget and the overall score of KMMLU-PRO for both models. However, this trend does not hold uniformly across all licenses. For example, both models show trivial gains on the Judicial Scrivener and Herb Pharmacist licenses, indicating that more reasoning does not always boost performance. 6."
        },
        {
            "title": "Impact of Prompt Language",
            "content": "Prompt language can significantly influence model behavior, raising concerns about consistency in multilingual settings (Wang et al., 2025; Zhang et al., 2025; Lai and Nissim, 2024). Since all questions in our datasets are written in Korean, using Korean prompts is natural choice. However, we observe that some models perform worse when prompted in Korean. Table 3 presents the performance difference between English and KoKMMLU-REDUX KMMLU-PRO English Korean diff (%) English Korean diff (%) Qwen3-32B (w/ thinking) o4-mini (2025-04-16) Qwen3-235B-A22B (w/ thinking) Grok-3-mini-beta Qwen3-14B (w/ thinking) EXAONE Deep 32B DeepSeek R1 (671B) QwQ 32B EXAONE Deep 7.8B Llama-4-Maverick-17B-128E-Instruct Llama-4-Scout-17B-16E-Instruct 68.77 75.80 74.49 71.47 65.71 58.33 78.51 67.34 44.99 77.58 67.49 69.08 76.17 75.11 70.85 65.40 56.17 75.38 62.66 40.82 72.52 45.03 +0.5% 61.14 +0.5% 69.65 +0.8% 68.22 -0.9% 65.08 -0.5% 60.18 -3.7% 52.33 -4.0% 71.33 -6.9% 63.94 -9.3% 41.53 -7.0% 68.10 -33.3% 58.14 60.66 69.10 66.98 64.89 59.48 52.19 70.62 59.95 38.98 57.15 28.74 -0.8% -0.8% -1.8% -0.3% -1.2% -0.3% -1.0% -6.2% -6.1% -16.1% -50.6% Table 3: Comparison results between English and Korean prompts of models whose main results are reported on English prompts. The diff values are the relative difference in scores between two prompts. The specific prompts are detailed in Appendix D.1. rean prompts. The Llama-4 model series exhibits the most substantial drop in performance10, while closed models such as Grok-3-mini-beta and o4mini show minimal change. 10The Llama-4 models did not follow the prompt but ended their response with \"The best answer is\", even with the Korean prompt."
        },
        {
            "title": "7 Related Works",
            "content": "Reliability Issues of Benchmarks Recent works (Gema et al., 2025; Vendrow et al., 2025) have raised concerns about the reliability of LLM benchmarks due to dataset noise and contamination. MMLU-Redux (Gema et al., 2025) improved evaluation quality through systematic human reannotation, while GSM8K-Platinum (Vendrow et al., 2025) refined arithmetic benchmarks via automated and manual error detection. MMLU-CF (Zhao et al., 2024) prevents both unintentional and malicious contamination via sourcing diverse domains and question rewriting. LiveBench (White et al., 2025) and LiveCodeBench (Jain et al., 2025) adopted dynamic evaluation protocols with temporal cutoffs to prevent future leakage. Professional Benchmark With the rapid advancement of LLMs, more challenging benchmarks have become essential. GPQA (Rein et al., 2024) and SuperGPQA (Team et al., 2025b) assess graduate-level knowledge; MMLU-Pro (Wang et al., 2024) extends MMLU by increasing the share of college-level questions and expanding answer choices. Humanitys Last Exam (Phan et al., 2025) introduces frontier benchmark composed of manually authored, research-level questions. Korean Benchmark While prior benchmarks focus primarily on English, recent efforts have produced Korean-specific evaluations (Son et al., 2024b; Kim et al., 2024a). Some rely on translated datasets (Park et al., 2024; Kim et al., 2025; OpenAI, 2024; Singh et al., 2024), often with human post-editing, but these lack regional context, institutional norms, and domain-specific fluency. In contrast, native Korean benchmarks such as KMMLU (Son et al., 2024a), KorMedMCQA (Kweon et al., 2024), and KBL (Kim et al., 2024b) address cultural and linguistic specificity. However, KMMLU (Son et al., 2024a) suffers from quality issues, including leaked answers, and KorMedMCQA (Kweon et al., 2024) and KBL (Kim et al., 2024b) are limited to narrow domains. In this work, we introduce KMMLU-REDUX and KMMLU-PRO, two contamination-free, industry grade benchmarks, providing practical assessment of LLM capabilities in Korean industries."
        },
        {
            "title": "8 Conclusion",
            "content": "We present two benchmarks constructed from realworld professional licensing exams, designed to reflect industrial domain knowledge and practical application standards. To ensure reliability, we identify and eliminate various sources of errors. Through extensive experiments, we evaluate the professional knowledge capabilities of LLMs across wide range of domains. Our analysis further identifies key factors that influence performance, including region-specific knowledge, reasoning budget, and prompt language. We hope this work provides foundation for more rigorous evaluation and continued advancement of real-world competence in language models."
        },
        {
            "title": "Limitations",
            "content": "Our benchmarks are limited to text-only and multiple-choice questions for text-only LLMs. It restricts its coverage of real-world licensure exams. Many real-word professional qualification exams include non-textual modalities or require constructed responses such as essay. Our benchmark cannot fully assess all aspects of professional competence or reasoning required in such exams. Expanding to multimodal inputs and open-ended question formats is an important direction for future work."
        },
        {
            "title": "Ethical Statements",
            "content": "All data used in our benchmarks are either publicly available or collected from official licensing materials released by government or professional institutions. For quality control, we hired human annotators to review parsed questions from PDF; they were compensated over the mininum wage in Korea. Our benchmarks would be released under CC-BY-NC-ND 4.0 license."
        },
        {
            "title": "References",
            "content": "Marah Abdin, Jyoti Aneja, Harkirat Behl, Sébastien Bubeck, Ronen Eldan, Suriya Gunasekar, Michael Harrison, Russell J. Hewett, Mojan Javaheripi, Piero Kauffmann, James R. Lee, Yin Tat Lee, Yuanzhi Li, Weishung Liu, Caio C. T. Mendes, Anh Nguyen, Eric Price, Gustavo de Rosa, Olli Saarikivi, Adil Salim, Shital Shah, Xin Wang, Rachel Ward, Yue Wu, Dingli Yu, Cyril Zhang, and Yi Zhang. 2024. Phi-4 technical report. Preprint, arXiv:2412.08905. Anthropic. 2025. Claude 3.7 sonnet and claude code. Zina Chkirbene, Ridha Hamila, Ala Gouissem, and Unal Devrim. 2024. Large language models (llm) in industry: survey of applications, challenges, and trends. In 2024 IEEE 21st International Conference on Smart Communities: Improving Quality of Life using AI, Robotics and IoT (HONET), pages 229234. Cohere. 2025. Model card for c4ai command a. John Dang, Shivalika Singh, Daniel Dsouza, Arash Ahmadian, Alejandro Salamanca, Madeline Smith, Aidan Peppin, Sungjin Hong, Manoj Govindassamy, Terrence Zhao, Sandra Kublik, Meor Amer, Viraat Aryabumi, Jon Ander Campos, Yi-Chern Tan, Tom Kocmi, Florian Strub, Nathan Grinsztajn, Yannis Flet-Berliac, Acyr Locatelli, Hangyu Lin, Dwarak Talupuru, Bharat Venkitesh, David Cairuz, Bowen Yang, Tim Chung, Wei-Yin Ko, Sylvie Shang Shi, Amir Shukayev, Sammie Bae, Aleksandra Piktus, Roman Castagné, Felipe Cruz-Salinas, Eddie Kim, Lucas Crawhall-Stein, Adrien Morisot, Sudip Roy, Phil Blunsom, Ivan Zhang, Aidan Gomez, Nick Frosst, Marzieh Fadaee, Beyza Ermis, Ahmet Üstün, and Sara Hooker. 2024. Aya expanse: Combining research breakthroughs for new multilingual frontier. Preprint, arXiv:2412.04261. Google Deepmind. 2024. Introducing gemini 2.0: our new ai model for the agentic era. DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J. L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, R. J. Chen, R. L. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, S. S. Li, Shuang Zhou, Shaoqing Wu, Shengfeng Ye, Tao Yun, Tian Pei, Tianyu Sun, T. Wang, Wangding Zeng, Wanjia Zhao, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, W. L. Xiao, Wei An, Xiaodong Liu, Xiaohan Wang, Xiaokang Chen, Xiaotao Nie, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, X. Q. Li, Xiangyue Jin, Xiaojin Shen, Xiaosha Chen, Xiaowen Sun, Xiaoxiang Wang, Xinnan Song, Xinyi Zhou, Xianzu Wang, Xinxia Shan, Y. K. Li, Y. Q. Wang, Y. X. Wei, Yang Zhang, Yanhong Xu, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Wang, Yi Yu, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yuan Ou, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He, Yunfan Xiong, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Y. X. Zhu, Yanhong Xu, Yanping Huang, Yaohui Li, Yi Zheng, Yuchen Zhu, Yunxian Ma, Ying Tang, Yukun Zha, Yuting Yan, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhenda Xie, Zhengyan Zhang, Zhewen Hao, Zhicheng Ma, Zhigang Yan, Zhiyu Wu, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Zizheng Pan, Zhen Huang, Zhipeng Xu, Zhongyu Zhang, and Zhen Zhang. 2025a. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. Preprint, arXiv:2501.12948. DeepSeek-AI, Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Daya Guo, Dejian Yang, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Haowei Zhang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Li, Hui Qu, J. L. Cai, Jian Liang, Jianzhong Guo, Jiaqi Ni, Jiashi Li, Jiawei Wang, Jin Chen, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, Junxiao Song, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Lei Xu, Leyi Xia, Liang Zhao, Litong Wang, Liyue Zhang, Meng Li, Miaojun Wang, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Mingming Li, Ning Tian, Panpan Huang, Peiyi Wang, Peng Zhang, Qiancheng Wang, Qihao Zhu, Qinyu Chen, Qiushi Du, R. J. Chen, R. L. Jin, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, Runxin Xu, Ruoyu Zhang, Ruyi Chen, S. S. Li, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shaoqing Wu, Shengfeng Ye, Shengfeng Ye, Shirong Ma, Shiyu Wang, Shuang Zhou, Shuiping Yu, Shunfeng Zhou, Shuting Pan, T. Wang, Tao Yun, Tian Pei, Tianyu Sun, W. L. Xiao, Wangding Zeng, Wanjia Zhao, Wei An, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, X. Q. Li, Xiangyue Jin, Xianzu Wang, Xiao Bi, Xiaodong Liu, Xiaohan Wang, Xiaojin Shen, Xiaokang Chen, Xiaokang Zhang, Xiaosha Chen, Xiaotao Nie, Xiaowen Sun, Xiaoxiang Wang, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xingkai Yu, Xinnan Song, Xinxia Shan, Xinyi Zhou, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, Y. K. Li, Y. Q. Wang, Y. X. Wei, Y. X. Zhu, Yang Zhang, Yanhong Xu, Yanhong Xu, Yanping Huang, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Li, Yaohui Wang, Yi Yu, Yi Zheng, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Ying Tang, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yu Wu, Yuan Ou, Yuchen Zhu, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He, Yukun Zha, Yunfan Xiong, Yunxian Ma, Yuting Yan, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Z. F. Wu, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhen Huang, Zhen Zhang, Zhenda Xie, Zhengyan Zhang, Zhewen Hao, Zhibin Gou, Zhicheng Ma, Zhigang Yan, Zhihong Shao, Zhipeng Xu, Zhiyu Wu, Zhongyu Zhang, Zhuoshu Li, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Ziyi Gao, and Zizheng Pan. 2025b. Deepseek-v3 technical report. Preprint, arXiv:2412.19437. Aryo Pradipta Gema, Joshua Ong Jun Leang, Giwon Hong, Alessio Devoto, Alberto Carlo Maria Mancino, Rohit Saxena, Xuanli He, Yu Zhao, Xiaotang Du, Mohammad Reza Ghasemi Madani, Claire Barale, Robert McHardy, Joshua Harris, Jean Kaddour, Emile van Krieken, and Pasquale Minervini. 2025. Are we done with mmlu? Preprint, arXiv:2406.04127. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad AlDahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurelien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Roziere, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, Danny Wyatt, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip Radenovic, Francisco Guzmán, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Govind Thattai, Graeme Nail, Gregoire Mialon, Guan Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov, Imanol Arrieta Ibarra, Isabel Kloumann, Ishan Misra, Ivan Evtimov, Jack Zhang, Jade Copet, Jaewon Lee, Jan Geffert, Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasuden Alwala, Karthik Prasad, Kartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heafield, Kevin Stone, Khalid El-Arini, Krithika Iyer, Kshitiz Malik, Kuenley Chiu, Kunal Bhalla, Kushal Lakhotia, Lauren Rantala-Yeary, Laurens van der Maaten, Lawrence Chen, Liang Tan, Liz Jenkins, Louis Martin, Lovish Madaan, Lubo Malo, Lukas Blecher, Lukas Landzaat, Luke de Oliveira, Madeline Muzzi, Mahesh Pasupuleti, Mannat Singh, Manohar Paluri, Marcin Kardas, Maria Tsimpoukelli, Mathew Oldham, Mathieu Rita, Maya Pavlova, Melanie Kambadur, Mike Lewis, Min Si, Mitesh Kumar Singh, Mona Hassan, Naman Goyal, Narjes Torabi, Nikolay Bashlykov, Nikolay Bogoychev, Niladri Chatterji, Ning Zhang, Olivier Duchenne, Onur Çelebi, Patrick Alrassy, Pengchuan Zhang, Pengwei Li, Petar Vasic, Peter Weng, Prajjwal Bhargava, Pratik Dubal, Praveen Krishnan, Punit Singh Koura, Puxin Xu, Qing He, Qingxiao Dong, Ragavan Srinivasan, Raj Ganapathy, Ramon Calderer, Ricardo Silveira Cabral, Robert Stojnic, Roberta Raileanu, Rohan Maheswari, Rohit Girdhar, Rohit Patel, Romain Sauvestre, Ronnie Polidoro, Roshan Sumbaly, Ross Taylor, Ruan Silva, Rui Hou, Rui Wang, Saghar Hosseini, Sahana Chennabasappa, Sanjay Singh, Sean Bell, Seohyun Sonia Kim, Sergey Edunov, Shaoliang Nie, Sharan Narang, Sharath Raparthy, Sheng Shen, Shengye Wan, Shruti Bhosale, Shun Zhang, Simon Vandenhende, Soumya Batra, Spencer Whitman, Sten Sootla, Stephane Collot, Suchin Gururangan, Sydney Borodinsky, Tamar Herman, Tara Fowler, Tarek Sheasha, Thomas Georgiou, Thomas Scialom, Tobias Speckbacher, Todor Mihaylov, Tong Xiao, Ujjwal Karn, Vedanuj Goswami, Vibhor Gupta, Vignesh Ramanathan, Viktor Kerkez, Vincent Gonguet, Virginie Do, Vish Vogeti, Vítor Albiero, Vladan Petrovic, Weiwei Chu, Wenhan Xiong, Wenyin Fu, Whitney Meers, Xavier Martinet, Xiaodong Wang, Xiaofang Wang, Xiaoqing Ellen Tan, Xide Xia, Xinfeng Xie, Xuchao Jia, Xuewei Wang, Yaelle Goldschlag, Yashesh Gaur, Yasmine Babaei, Yi Wen, Yiwen Song, Yuchen Zhang, Yue Li, Yuning Mao, Zacharie Delpierre Coudert, Zheng Yan, Zhengxing Chen, Zoe Papakipos, Aaditya Singh, Aayushi Srivastava, Abha Jain, Adam Kelsey, Adam Shajnfeld, Adithya Gangidi, Adolfo Victoria, Ahuva Goldstand, Ajay Menon, Ajay Sharma, Alex Boesenberg, Alexei Baevski, Allie Feinstein, Amanda Kallet, Amit Sangani, Amos Teo, Anam Yunus, Andrei Lupu, Andres Alvarado, Andrew Caples, Andrew Gu, Andrew Ho, Andrew Poulton, Andrew Ryan, Ankit Ramchandani, Annie Dong, Annie Franco, Anuj Goyal, Aparajita Saraf, Arkabandhu Chowdhury, Ashley Gabriel, Ashwin Bharambe, Assaf Eisenman, Azadeh Yazdan, Beau James, Ben Maurer, Benjamin Leonhardi, Bernie Huang, Beth Loyd, Beto De Paola, Bhargavi Paranjape, Bing Liu, Bo Wu, Boyu Ni, Braden Hancock, Bram Wasti, Brandon Spence, Brani Stojkovic, Brian Gamido, Britt Montalvo, Carl Parker, Carly Burton, Catalina Mejia, Ce Liu, Changhan Wang, Changkyu Kim, Chao Zhou, Chester Hu, ChingHsiang Chu, Chris Cai, Chris Tindal, Christoph Feichtenhofer, Cynthia Gao, Damon Civin, Dana Beaty, Daniel Kreymer, Daniel Li, David Adkins, David Xu, Davide Testuggine, Delia David, Devi Parikh, Diana Liskovich, Didem Foss, Dingkang Wang, Duc Le, Dustin Holland, Edward Dowling, Eissa Jamil, Elaine Montgomery, Eleonora Presani, Emily Hahn, Emily Wood, Eric-Tuan Le, Erik Brinkman, Esteban Arcaute, Evan Dunbar, Evan Smothers, Fei Sun, Felix Kreuk, Feng Tian, Filippos Kokkinos, Firat Ozgenel, Francesco Caggioni, Frank Kanayet, Frank Seide, Gabriela Medina Florez, Gabriella Schwarz, Gada Badeer, Georgia Swee, Gil Halpern, Grant Herman, Grigory Sizov, Guangyi, Zhang, Guna Lakshminarayanan, Hakan Inan, Hamid Shojanazeri, Han Zou, Hannah Wang, Hanwen Zha, Haroun Habeeb, Harrison Rudolph, Helen Suk, Henry Aspegren, Hunter Goldman, Hongyuan Zhan, Ibrahim Damlaj, Igor Molybog, Igor Tufanov, Ilias Leontiadis, Irina-Elena Veliche, Itai Gat, Jake Weissman, James Geboski, James Kohli, Janice Lam, Japhet Asher, Jean-Baptiste Gaya, Jeff Marcus, Jeff Tang, Jennifer Chan, Jenny Zhen, Jeremy Reizenstein, Jeremy Teboul, Jessica Zhong, Jian Jin, Jingyi Yang, Joe Cummings, Jon Carvill, Jon Shepard, Jonathan McPhie, Jonathan Torres, Josh Ginsburg, Junjie Wang, Kai Wu, Kam Hou U, Karan Saxena, Kartikay Khandelwal, Katayoun Zand, Kathy Matosich, Kaushik Veeraraghavan, Kelly Michelena, Keqian Li, Kiran Jagadeesh, Kun Huang, Kunal Chawla, Kyle Huang, Lailin Chen, Lakshya Garg, Lavender A, Leandro Silva, Lee Bell, Lei Zhang, Liangpeng Guo, Licheng Yu, Liron Moshkovich, Luca Wehrstedt, Madian Khabsa, Manav Avalani, Manish Bhatt, Martynas Mankus, Matan Hasson, Matthew Lennie, Matthias Reso, Maxim Groshev, Maxim Naumov, Maya Lathi, Meghan Keneally, Miao Liu, Michael L. Seltzer, Michal Valko, Michelle Restrepo, Mihir Patel, Mik Vyatskov, Mikayel Samvelyan, Mike Clark, Mike Macey, Mike Wang, Miquel Jubert Hermoso, Mo Metanat, Mohammad Rastegari, Munish Bansal, Nandhini Santhanam, Natascha Parks, Natasha White, Navyata Bawa, Nayan Singhal, Nick Egebo, Nicolas Usunier, Nikhil Mehta, Nikolay Pavlovich Laptev, Ning Dong, Norman Cheng, Oleg Chernoguz, Olivia Hart, Omkar Salpekar, Ozlem Kalinli, Parkin Kent, Parth Parekh, Paul Saab, Pavan Balaji, Pedro Rittner, Philip Bontrager, Pierre Roux, Piotr Dollar, Polina Zvyagina, Prashant Ratanchandani, Pritish Yuvraj, Qian Liang, Rachad Alao, Rachel Rodriguez, Rafi Ayub, Raghotham Murthy, Raghu Nayani, Rahul Mitra, Rangaprabhu Parthasarathy, Raymond Li, Rebekkah Hogan, Robin Battey, Rocky Wang, Russ Howes, Ruty Rinott, Sachin Mehta, Sachin Siby, Sai Jayesh Bondu, Samyak Datta, Sara Chugh, Sara Hunt, Sargun Dhillon, Sasha Sidorov, Satadru Pan, Saurabh Mahajan, Saurabh Verma, Seiji Yamamoto, Sharadh Ramaswamy, Shaun Lindsay, Shaun Lindsay, Sheng Feng, Shenghao Lin, Shengxin Cindy Zha, Shishir Patil, Shiva Shankar, Shuqiang Zhang, Shuqiang Zhang, Sinong Wang, Sneha Agarwal, Soji Sajuyigbe, Soumith Chintala, Stephanie Max, Stephen Chen, Steve Kehoe, Steve Satterfield, Sudarshan Govindaprasad, Sumit Gupta, Summer Deng, Sungmin Cho, Sunny Virk, Suraj Subramanian, Sy Choudhury, Sydney Goldman, Tal Remez, Tamar Glaser, Tamara Best, Thilo Koehler, Thomas Robinson, Tianhe Li, Tianjun Zhang, Tim Matthews, Timothy Chou, Tzook Shaked, Varun Vontimitta, Victoria Ajayi, Victoria Montanez, Vijai Mohan, Vinay Satish Kumar, Vishal Mangla, Vlad Ionescu, Vlad Poenaru, Vlad Tiberiu Mihailescu, Vladimir Ivanov, Wei Li, Wenchen Wang, Wenwen Jiang, Wes Bouaziz, Will Constable, Xiaocheng Tang, Xiaojian Wu, Xiaolan Wang, Xilun Wu, Xinbo Gao, Yaniv Kleinman, Yanjun Chen, Ye Hu, Ye Jia, Ye Qi, Yenda Li, Yilin Zhang, Ying Zhang, Yossi Adi, Youngjin Nam, Yu, Wang, Yu Zhao, Yuchen Hao, Yundi Qian, Yunlu Li, Yuzi He, Zach Rait, Zachary DeVito, Zef Rosnbrick, Zhaoduo Wen, Zhenyu Yang, Zhiwei Zhao, and Zhiyu Ma. 2024. The llama 3 herd of models. Preprint, arXiv:2407.21783. standing. In International Conference on Learning Representations. Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. 2020. The curious case of neural text degeneration. In International Conference on Learning Representations. Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando SolarLezama, Koushik Sen, and Ion Stoica. 2025. Livecodebench: Holistic and contamination free evaluation of large language models for code. In The Thirteenth International Conference on Learning Representations. Mehran Kazemi, Bahare Fatemi, Hritik Bansal, John Palowitch, Chrysovalantis Anastasiou, Sanket Vaibhav Mehta, Lalit K. Jain, Virginia Aglietti, Disha Jindal, Peter Chen, Nishanth Dikkala, Gladys Tyen, Xin Liu, Uri Shalit, Silvia Chiappa, Kate Olszewska, Yi Tay, Vinh Q. Tran, Quoc V. Le, and Orhan Preprint, Firat. 2025. arXiv:2502.19187. Big-bench extra hard. Eunsu Kim, Juyoung Suk, Philhoon Oh, Haneul Yoo, James Thorne, and Alice Oh. 2024a. CLIcK: benchmark dataset of cultural and linguistic intelligence in Korean. In Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LRECCOLING 2024), pages 33353346, Torino, Italia. ELRA and ICCL. Hyeonwoo Kim, Dahyun Kim, Jihoo Kim, Sukyung Lee, Yungi Kim, and Chanjun Park. 2025. Open ko-llm leaderboard2: Bridging foundational and Preprint, practical evaluation for korean llms. arXiv:2410.12445. Yeeun Kim, Youngrok Choi, Eunkyung Choi, JinHwan Choi, Hai Jin Park, and Wonseok Hwang. 2024b. Developing pragmatic benchmark for assessing Korean legal language understanding in large language models. In Findings of the Association for Computational Linguistics: EMNLP 2024, pages 55735595, Miami, Florida, USA. Association for Computational Linguistics. Takeshi Kojima, Shixiang (Shane) Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022. Large language models are zero-shot reasoners. In Advances in Neural Information Processing Systems, volume 35, pages 2219922213. Sunjun Kweon, Byungjin Choi, Gyouk Chu, Junyeong Song, Daeun Hyeon, Sujin Gan, Jueon Kim, Minkyu Kim, Rae Woong Park, and Edward Choi. 2024. Kormedmcqa: Multi-choice question answering benchmark for korean healthcare professional licensing examinations. Preprint, arXiv:2403.01469. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2021. Measuring massive multitask language underWoosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. 2023. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles. Huiyuan Lai and Malvina Nissim. 2024. mCoT: Multilingual instruction tuning for reasoning consistency in language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 12012 12026, Bangkok, Thailand. Association for Computational Linguistics. Nathan Lambert, Jacob Morrison, Valentina Pyatkin, Shengyi Huang, Hamish Ivison, Faeze Brahman, Lester James V. Miranda, Alisa Liu, Nouha Dziri, Shane Lyu, Yuling Gu, Saumya Malik, Victoria Graf, Jena D. Hwang, Jiangjiang Yang, Ronan Le Bras, Oyvind Tafjord, Chris Wilhelm, Luca Soldaini, Noah A. Smith, Yizhong Wang, Pradeep Dasigi, and Hannaneh Hajishirzi. 2025. Tulu 3: Pushing frontiers in open language model post-training. Preprint, arXiv:2411.15124. Hwaran Lee, Seokhee Hong, Joonsuk Park, Takyoung Kim, Meeyoung Cha, Yejin Choi, Byoungpil Kim, Gunhee Kim, Eun-Ju Lee, Yong Lim, Alice Oh, Sangchul Park, and Jung-Woo Ha. 2023. SQuARe: large-scale dataset of sensitive questions and acceptable responses created through human-machine In Proceedings of the 61st Annual collaboration. Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 66926712, Toronto, Canada. Association for Computational Linguistics. Meta. 2024a. The future of ai: Built with llama. Meta. 2024b. Introducing llama 3.1: Our most capable models to date. Meta. 2024c. Llama 3.2: Revolutionizing edge ai and vision with open, customizable models. Meta. 2025. The llama 4 herd: The beginning of new era of natively multimodal ai innovation. Mistral. 2025. Mistral small 3.1. Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Candès, and s1: Simple test-time Tatsunori Hashimoto. 2025. scaling. Preprint, arXiv:2501.19393. naver hyperclovax. 2025. Model card for hyperclovaxseed-text-instruct-1.5b. OneLineAI. 2025. Ko-r1-7b-v2.1. huggingface.co/OLAIR/ko-r1-7b-v2.1. cessed: 26 March 2025. https:// AcOpenAI, :, Aaron Hurst, Adam Lerer, Adam P. Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, Aleksander adry, Alex Baker-Whitcomb, Alex Beutel, Alex Borzunov, Alex Carney, Alex Chow, Alex Kirillov, Alex Nichol, Alex Paino, Alex Renzin, Alex Tachard Passos, Alexander Kirillov, Alexi Christakis, Alexis Conneau, Ali Kamali, Allan Jabri, Allison Moyer, Allison Tam, Amadou Crookes, Amin Tootoochian, Amin Tootoonchian, Ananya Kumar, Andrea Vallone, Andrej Karpathy, Andrew Braunstein, Andrew Cann, Andrew Codispoti, Andrew Galu, Andrew Kondrich, Andrew Tulloch, Andrey Mishchenko, Angela Baek, Angela Jiang, Antoine Pelisse, Antonia Woodford, Anuj Gosalia, Arka Dhar, Ashley Pantuliano, Avi Nayak, Avital Oliver, Barret Zoph, Behrooz Ghorbani, Ben Leimberger, Ben Rossen, Ben Sokolowsky, Ben Wang, Benjamin Zweig, Beth Hoover, Blake Samic, Bob McGrew, Bobby Spero, Bogo Giertler, Bowen Cheng, Brad Lightcap, Brandon Walkin, Brendan Quinn, Brian Guarraci, Brian Hsu, Bright Kellogg, Brydon Eastman, Camillo Lugaresi, Carroll Wainwright, Cary Bassin, Cary Hudson, Casey Chu, Chad Nelson, Chak Li, Chan Jun Shern, Channing Conger, Charlotte Barette, Chelsea Voss, Chen Ding, Cheng Lu, Chong Zhang, Chris Beaumont, Chris Hallacy, Chris Koch, Christian Gibson, Christina Kim, Christine Choi, Christine McLeavey, Christopher Hesse, Claudia Fischer, Clemens Winter, Coley Czarnecki, Colin Jarvis, Colin Wei, Constantin Koumouzelis, Dane Sherburn, Daniel Kappler, Daniel Levin, Daniel Levy, David Carr, David Farhi, David Mely, David Robinson, David Sasaki, Denny Jin, Dev Valladares, Dimitris Tsipras, Doug Li, Duc Phong Nguyen, Duncan Findlay, Edede Oiwoh, Edmund Wong, Ehsan Asdar, Elizabeth Proehl, Elizabeth Yang, Eric Antonow, Eric Kramer, Eric Peterson, Eric Sigler, Eric Wallace, Eugene Brevdo, Evan Mays, Farzad Khorasani, Felipe Petroski Such, Filippo Raso, Francis Zhang, Fred von Lohmann, Freddie Sulit, Gabriel Goh, Gene Oden, Geoff Salmon, Giulio Starace, Greg Brockman, Hadi Salman, Haiming Bao, Haitang Hu, Hannah Wong, Haoyu Wang, Heather Schmidt, Heather Whitney, Heewoo Jun, Hendrik Kirchner, Henrique Ponde de Oliveira Pinto, Hongyu Ren, Huiwen Chang, Hyung Won Chung, Ian Kivlichan, Ian OConnell, Ian OConnell, Ian Osband, Ian Silber, Ian Sohl, Ibrahim Okuyucu, Ikai Lan, Ilya Kostrikov, Ilya Sutskever, Ingmar Kanitscheider, Ishaan Gulrajani, Jacob Coxon, Jacob Menick, Jakub Pachocki, James Aung, James Betker, James Crooks, James Lennon, Jamie Kiros, Jan Leike, Jane Park, Jason Kwon, Jason Phang, Jason Teplitz, Jason Wei, Jason Wolfe, Jay Chen, Jeff Harris, Jenia Varavva, Jessica Gan Lee, Jessica Shieh, Ji Lin, Jiahui Yu, Jiayi Weng, Jie Tang, Jieqi Yu, Joanne Jang, Joaquin Quinonero Candela, Joe Beutler, Joe Landers, Joel Parish, Johannes Heidecke, John Schulman, Jonathan Lachman, Jonathan McKay, Jonathan Uesato, Jonathan Ward, Jong Wook Kim, Joost Huizinga, Jordan Sitkin, Jos Kraaijeveld, Josh Gross, Josh Kaplan, Josh Snyder, Joshua Achiam, Joy Jiao, Joyce Lee, Juntang Zhuang, Justyn Harriman, Kai Fricke, Kai Hayashi, Karan Singhal, Katy Shi, Kavin Karthik, Kayla Wood, Kendra Rimbach, Kenny Hsu, Kenny Nguyen, Keren Gu-Lemberg, Kevin Button, Kevin Liu, Kiel Howe, Krithika Muthukumar, Kyle Luther, Lama Ahmad, Larry Kai, Lauren Itow, Lauren Workman, Leher Pathak, Leo Chen, Li Jing, Lia Guy, Liam Fedus, Liang Zhou, Lien Mamitsuka, Lilian Weng, Lindsay McCallum, Lindsey Held, Long Ouyang, Louis Feuvrier, Lu Zhang, Lukas Kondraciuk, Lukasz Kaiser, Luke Hewitt, Luke Metz, Lyric Doshi, Mada Aflak, Maddie Simens, Madelaine Boyd, Madeleine Thompson, Marat Dukhan, Mark Chen, Mark Gray, Mark Hudnall, Marvin Zhang, Marwan Aljubeh, Mateusz Litwin, Matthew Zeng, Max Johnson, Maya Shetty, Mayank Gupta, Meghan Shah, Mehmet Yatbaz, Meng Jia Yang, Mengchao Zhong, Mia Glaese, Mianna Chen, Michael Janner, Michael Lampe, Michael Petrov, Michael Wu, Michele Wang, Michelle Fradin, Michelle Pokrass, Miguel Castro, Miguel Oom Temudo de Castro, Mikhail Pavlov, Miles Brundage, Miles Wang, Minal Khan, Mira Murati, Mo Bavarian, Molly Lin, Murat Yesildal, Nacho Soto, Natalia Gimelshein, Natalie Cone, Natalie Staudacher, Natalie Summers, Natan LaFontaine, Neil Chowdhury, Nick Ryder, Nick Stathas, Nick Turley, Nik Tezak, Niko Felix, Nithanth Kudige, Nitish Keskar, Noah Deutsch, Noel Bundick, Nora Puckett, Ofir Nachum, Ola Okelola, Oleg Boiko, Oleg Murk, Oliver Jaffe, Olivia Watkins, Olivier Godement, Owen Campbell-Moore, Patrick Chao, Paul McMillan, Pavel Belov, Peng Su, Peter Bak, Peter Bakkum, Peter Deng, Peter Dolan, Peter Hoeschele, Peter Welinder, Phil Tillet, Philip Pronin, Philippe Tillet, Prafulla Dhariwal, Qiming Yuan, Rachel Dias, Rachel Lim, Rahul Arora, Rajan Troll, Randall Lin, Rapha Gontijo Lopes, Raul Puri, Reah Miyara, Reimar Leike, Renaud Gaubert, Reza Zamani, Ricky Wang, Rob Donnelly, Rob Honsby, Rocky Smith, Rohan Sahai, Rohit Ramchandani, Romain Huet, Rory Carmichael, Rowan Zellers, Roy Chen, Ruby Chen, Ruslan Nigmatullin, Ryan Cheu, Saachi Jain, Sam Altman, Sam Schoenholz, Sam Toizer, Samuel Miserendino, Sandhini Agarwal, Sara Culver, Scott Ethersmith, Scott Gray, Sean Grove, Sean Metzger, Shamez Hermani, Shantanu Jain, Shengjia Zhao, Sherwin Wu, Shino Jomoto, Shirong Wu, Shuaiqi, Xia, Sonia Phene, Spencer Papay, Srinivas Narayanan, Steve Coffey, Steve Lee, Stewart Hall, Suchir Balaji, Tal Broda, Tal Stramer, Tao Xu, Tarun Gogineni, Taya Christianson, Ted Sanders, Tejal Patwardhan, Thomas Cunninghman, Thomas Degry, Thomas Dimson, Thomas Raoux, Thomas Shadwell, Tianhao Zheng, Todd Underwood, Todor Markov, Toki Sherbakov, Tom Rubin, Tom Stasi, Tomer Kaftan, Tristan Heywood, Troy Peterson, Tyce Walters, Tyna Eloundou, Valerie Qi, Veit Moeller, Vinnie Monaco, Vishal Kuo, Vlad Fomenko, Wayne Chang, Weiyi Zheng, Wenda Zhou, Wesam Manassra, Will Sheu, Wojciech Zaremba, Yash Patil, Yilei Qian, Yongjik Kim, Youlong Cheng, Yu Zhang, Yuchen He, Yuchen Zhang, Yujia Jin, Yunxing Dai, and Yury Malkov. 2024a. Gpt-4o system card. Preprint, arXiv:2410.21276. OpenAI, :, Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, Alex Iftimie, Alex Karpenko, Alex Tachard Passos, Alexander Neitz, Alexander Prokofiev, Alexander Wei, Allison Tam, Ally Bennett, Ananya Kumar, Andre Saraiva, Andrea Vallone, Andrew Duberstein, Andrew Kondrich, Andrey Mishchenko, Andy Applebaum, Angela Jiang, Ashvin Nair, Barret Zoph, Behrooz Ghorbani, Ben Rossen, Benjamin Sokolowsky, Boaz Barak, Bob McGrew, Borys Minaiev, Botao Hao, Bowen Baker, Brandon Houghton, Brandon McKinzie, Brydon Eastman, Camillo Lugaresi, Cary Bassin, Cary Hudson, Chak Ming Li, Charles de Bourcy, Chelsea Voss, Chen Shen, Chong Zhang, Chris Koch, Chris Orsinger, Christopher Hesse, Claudia Fischer, Clive Chan, Dan Roberts, Daniel Kappler, Daniel Levy, Daniel Selsam, David Dohan, David Farhi, David Mely, David Robinson, Dimitris Tsipras, Doug Li, Dragos Oprica, Eben Freeman, Eddie Zhang, Edmund Wong, Elizabeth Proehl, Enoch Cheung, Eric Mitchell, Eric Wallace, Erik Ritter, Evan Mays, Fan Wang, Felipe Petroski Such, Filippo Raso, Florencia Leoni, Foivos Tsimpourlas, Francis Song, Fred von Lohmann, Freddie Sulit, Geoff Salmon, Giambattista Parascandolo, Gildas Chabot, Grace Zhao, Greg Brockman, Guillaume Leclerc, Hadi Salman, Haiming Bao, Hao Sheng, Hart Andrin, Hessam Bagherinezhad, Hongyu Ren, Hunter Lightman, Hyung Won Chung, Ian Kivlichan, Ian OConnell, Ian Osband, Ignasi Clavera Gilaberte, Ilge Akkaya, Ilya Kostrikov, Ilya Sutskever, Irina Kofman, Jakub Pachocki, James Lennon, Jason Wei, Jean Harb, Jerry Twore, Jiacheng Feng, Jiahui Yu, Jiayi Weng, Jie Tang, Jieqi Yu, Joaquin Quiñonero Candela, Joe Palermo, Joel Parish, Johannes Heidecke, John Hallman, John Rizzo, Jonathan Gordon, Jonathan Uesato, Jonathan Ward, Joost Huizinga, Julie Wang, Kai Chen, Kai Xiao, Karan Singhal, Karina Nguyen, Karl Cobbe, Katy Shi, Kayla Wood, Kendra Rimbach, Keren Gu-Lemberg, Kevin Liu, Kevin Lu, Kevin Stone, Kevin Yu, Lama Ahmad, Lauren Yang, Leo Liu, Leon Maksin, Leyton Ho, Liam Fedus, Lilian Weng, Linden Li, Lindsay McCallum, Lindsey Held, Lorenz Kuhn, Lukas Kondraciuk, Lukasz Kaiser, Luke Metz, Madelaine Boyd, Maja Trebacz, Manas Joglekar, Mark Chen, Marko Tintor, Mason Meyer, Matt Jones, Matt Kaufer, Max Schwarzer, Meghan Shah, Mehmet Yatbaz, Melody Y. Guan, Mengyuan Xu, Mengyuan Yan, Mia Glaese, Mianna Chen, Michael Lampe, Michael Malek, Michele Wang, Michelle Fradin, Mike McClay, Mikhail Pavlov, Miles Wang, Mingxuan Wang, Mira Murati, Mo Bavarian, Mostafa Rohaninejad, Nat McAleese, Neil Chowdhury, Neil Chowdhury, Nick Ryder, Nikolas Tezak, Noam Brown, Ofir Nachum, Oleg Boiko, Oleg Murk, Olivia Watkins, Patrick Chao, Paul Ashbourne, Pavel Izmailov, Peter Zhokhov, Rachel Dias, Rahul Arora, Randall Lin, Rapha Gontijo Lopes, Raz Gaon, Reah Miyara, Reimar Leike, Renny Hwang, Rhythm Garg, Robin Brown, Roshan James, Rui Shu, Ryan Cheu, Ryan Greene, Saachi Jain, Sam Altman, Sam Toizer, Sam Toyer, Samuel Miserendino, Sandhini Agarwal, Santiago Hernandez, Sasha Baker, Scott McKinney, Scottie Yan, Shengjia Zhao, Shengli Hu, Shibani Santurkar, Shraman Ray Chaudhuri, Shuyuan Zhang, Siyuan Fu, Spencer Papay, Steph Lin, Suchir Balaji, Suvansh Sanjeev, Szymon Sidor, Tal Broda, Aidan Clark, Tao Wang, Taylor Gordon, Ted Sanders, Tejal Patwardhan, Thibault Sottiaux, Thomas Degry, Thomas Dimson, Tianhao Zheng, Timur Garipov, Tom Stasi, Trapit Bansal, Trevor Creech, Troy Peterson, Tyna Eloundou, Valerie Qi, Vineet Kosaraju, Vinnie Monaco, Vitchyr Pong, Vlad Fomenko, Weiyi Zheng, Wenda Zhou, Wes McCabe, Wojciech Zaremba, Yann Dubois, Yinghai Lu, Yining Chen, Young Cha, Yu Bai, Yuchen He, Yuchen Zhang, Yunyun Wang, Zheng Shao, and Zhuohan Li. 2024b. Openai o1 system card. Preprint, arXiv:2412.16720. OpenAI. 2024. Multilingual massive multitask language understanding (mmmlu). OpenAI. 2025a. Introducing gpt-4.1 in the api. OpenAI. 2025b. Introducing openai o3 and o4-mini. OpenAI. 2025c. Openai o3-mini. Chanjun Park, Hyeonwoo Kim, Dahyun Kim, SeongHwan Cho, Sanghoon Kim, Sukyung Lee, Yungi Kim, and Hwalsuk Lee. 2024. Open Ko-LLM leaderboard: Evaluating large language models in Korean with Koh5 benchmark. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 32203234, Bangkok, Thailand. Association for Computational Linguistics. Guilherme Penedo, Hynek Kydlíˇcek, Vinko Sabolˇcec, Bettina Messmer, Negar Foroutan, Amir Hossein Kargaran, Colin Raffel, Martin Jaggi, Leandro Von Werra, and Thomas Wolf. 2025. Fineweb2: One pipeline to scale them all adapting pre-training Preprint, data processing to every language. arXiv:2506.20920. Long Phan, Alice Gatti, Ziwen Han, Nathaniel Li, Josephina Hu, Hugh Zhang, Chen Bo Calvin Zhang, Mohamed Shaaban, John Ling, Sean Shi, Michael Choi, Anish Agrawal, Arnav Chopra, Adam Khoja, Ryan Kim, Richard Ren, Jason Hausenloy, Oliver Zhang, Mantas Mazeika, Tung Nguyen, Daron Anderson, Imad Ali Shah, Mikhail Doroshenko, Alun Cennyth Stokes, Mobeen Mahmood, Jaeho Lee, Oleksandr Pokutnyi, Oleg Iskra, Jessica P. Wang, Robert Gerbicz, John-Clark Levin, Serguei Popov, Fiona Feng, Steven Y. Feng, Haoran Zhao, Michael Yu, Varun Gangal, Chelsea Zou, Zihan Wang, Mstyslav Kazakov, Geoff Galgon, Johannes Schmitt, Alvaro Sanchez, Yongki Lee, Will Yeadon, Scott Sauers, Marc Roth, Chidozie Agu, Søren Riis, Fabian Giska, Saiteja Utpala, Antrell Cheatom, Zachary Giboney, Gashaw M. Goshu, Sarah-Jane Crowson, Mohinder Maheshbhai Naiya, Noah Burns, Lennart Finke, Zerui Cheng, Hyunwoo Park, Francesco FournierFacio, Jennifer Zampese, John B. Wydallis, Ryan G. Hoerr, Mark Nandor, Tim Gehrunger, Jiaqi Cai, Ben McCarty, Jungbae Nam, Edwin Taylor, Jun Jin, Gautier Abou Loume, Hangrui Cao, Alexis Garretson, Damien Sileo, Qiuyu Ren, Doru Cojoc, Pavel Arkhipov, Usman Qazi, Aras Bacho, Lianghui Li, Sumeet Motwani, Christian Schroeder de Witt, Alexei Kopylov, Johannes Veith, Eric Singer, Paolo Rissone, Jaehyeok Jin, Jack Wei Lun Shi, Chris G. Willcocks, Ameya Prabhu, Longke Tang, Kevin Zhou, Emily de Oliveira Santos, Andrey Pupasov Maksimov, Edward Vendrow, Kengo Zenitani, Joshua Robinson, Aleksandar Mikov, Julien Guillod, Yuqi Li, Ben Pageler, Joshua Vendrow, Vladyslav Kuchkin, Pierre Marion, Denis Efremov, Jayson Lynch, Kaiqu Liang, Andrew Gritsevskiy, Dakotah Martinez, Nick Crispino, Dimitri Zvonkine, Natanael Wildner Fraga, Saeed Soori, Ori Press, Henry Tang, Julian Salazar, Sean R. Green, Lina Brüssel, Moon Twayana, Aymeric Dieuleveut, T. Ryan Rogers, Wenjin Zhang, Ross Finocchio, Bikun Li, Jinzhou Yang, Arun Rao, Gabriel Loiseau, Mikhail Kalinin, Marco Lukas, Ciprian Manolescu, Nate Stambaugh, Subrata Mishra, Ariel Ghislain Kemogne Kamdoum, Tad Hogg, Alvin Jin, Carlo Bosio, Gongbo Sun, Brian Coppola, Haline Heidinger, Rafael Sayous, Stefan Ivanov, Joseph Cavanagh, Jiawei Shen, Joseph Marvin Imperial, Philippe Schwaller, Shaipranesh Senthilkuma, Andres Bran, Andres Algaba, Brecht Verbeken, Kelsey Van den Houte, Lynn Van Der Sypt, David Noever, Lisa Schut, Ilia Sucholutsky, Evgenii Zheltonozhskii, Qiaochu Yuan, Derek Lim, Richard Stanley, Shankar Sivarajan, Tong Yang, John Maar, Julian Wykowski, Martí Oller, Jennifer Sandlin, Anmol Sahu, Cesare Giulio Ardito, Yuzheng Hu, Felipe Meneguitti Dias, Tobias Kreiman, Kaivalya Rawal, Tobias Garcia Vilchis, Yuexuan Zu, Martin Lackner, James Koppel, Jeremy Nguyen, Daniil S. Antonenko, Steffi Chern, Bingchen Zhao, Pierrot Arsene, Sergey Ivanov, Rafał Poswiata, Chenguang Wang, Daofeng Li, Donato Crisostomi, Ali Dehghan, Andrea Achilleos, John Arnold Ambay, Benjamin Myklebust, Archan Sen, David Perrella, Nurdin Kaparov, Mark Inlow, Allen Zang, Kalyan Ramakrishnan, Daniil Orel, Vladislav Poritski, Shalev Ben-David, Zachary Berger, Parker Whitfill, Michael Foster, Daniel Munro, Linh Ho, Dan Bar Hava, Aleksey Kuchkin, Robert Lauff, David Holmes, Frank Sommerhage, Anji Zhang, Richard Moat, Keith Schneider, Daniel Pyda, Zakayo Kazibwe, Mukhwinder Singh, Don Clarke, Dae Hyun Kim, Sara Fish, Veit Elser, Victor Efren Guadarrama Vilchis, Immo Klose, Christoph Demian, Ujjwala Anantheswaran, Adam Zweiger, Guglielmo Albani, Jeffery Li, Nicolas Daans, Maksim Radionov, Václav Rozhoˇn, Vincent Ginis, Ziqiao Ma, Christian Stump, Jacob Platnick, Volodymyr Nevirkovets, Luke Basler, Marco Piccardo, Niv Cohen, Virendra Singh, Josef Tkadlec, Paul Rosu, Alan Goldfarb, Piotr Padlewski, Stanislaw Barzowski, Kyle Montgomery, Aline Menezes, Arkil Patel, Zixuan Wang, Jamie Tucker-Foltz, Jack Stade, Declan Grabb, Tom Goertzen, Fereshteh Kazemi, Jeremiah Milbauer, Abhishek Shukla, Hossam Elgnainy, Yan Carlos Leyva Labrador, Hao He, Ling Zhang, Alan Givré, Hew Wolff, Gözdenur Demir, Muhammad Fayez Aziz, Younesse Kaddar, Ivar Ängquist, Yanxu Chen, Elliott Thornley, Robin Zhang, Jiayi Pan, Antonio Terpin, Niklas Muennighoff, Hailey Schoelkopf, Eric Zheng, Avishy Carmi, Jainam Shah, Ethan D. L. Brown, Kelin Zhu, Max Bartolo, Richard Wheeler, Andrew Ho, Shaul Barkan, Jiaqi Wang, Martin Stehberger, Egor Kretov, Peter Bradshaw, JP Heimonen, Kaustubh Sridhar, Zaki Hossain, Ido Akov, Yury Makarychev, Joanna Tam, Hieu Hoang, David M. Cunningham, Vladimir Goryachev, Demosthenes Patramanis, Michael Krause, Andrew Redenti, David Aldous, Jesyin Lai, Shannon Coleman, Jiangnan Xu, Sangwon Lee, Ilias Magoulas, Sandy Zhao, Ning Tang, Michael K. Cohen, Micah Carroll, Orr Paradise, Jan Hendrik Kirchner, Stefan Steinerberger, Maksym Ovchynnikov, Jason O. Matos, Adithya Shenoy, Michael Wang, Yuzhou Nie, Paolo Giordano, Philipp Petersen, Anna Sztyber-Betley, Paolo Faraboschi, Robin Riblet, Jonathan Crozier, Shiv Halasyamani, Antonella Pinto, Shreyas Verma, Prashant Joshi, Eli Meril, Zheng-Xin Yong, Allison Tee, Jérémy Andréoletti, Orion Weller, Raghav Singhal, Gang Zhang, Alexander Ivanov, Seri Khoury, Nils Gustafsson, Hamid Mostaghimi, Kunvar Thaman, Qijia Chen, Tran Quoc Khánh, Jacob Loader, Stefano Cavalleri, Hannah Szlyk, Zachary Brown, Himanshu Narayan, Jonathan Roberts, William Alley, Kunyang Sun, Ryan Stendall, Max Lamparth, Anka Reuel, Ting Wang, Hanmeng Xu, Pablo HernándezCámara, Freddie Martin, Thomas Preu, Tomek Korbak, Marcus Abramovitch, Dominic Williamson, Ida Bosio, Ziye Chen, Biró Bálint, Eve J. Y. Lo, Maria Inês S. Nunes, Yibo Jiang, Saiful Bari, Peyman Kassani, Zihao Wang, Behzad Ansarinejad, Yewen Sun, Stephane Durand, Guillaume Douville, Daniel Tordera, George Balabanian, Earth Anderson, Lynna Kvistad, Alejandro José Moyano, Hsiaoyun Milliron, Ahmad Sakor, Murat Eron, Isaac C. McAlister, Andrew Favre D. O., Shailesh Shah, Xiaoxiang Zhou, Firuz Kamalov, Ronald Clark, Sherwin Abdoli, Tim Santens, Harrison Wang, Evan Chen, Alessandro Tomasiello, G. Bruno De Luca, Shi-Zhuo Looi, Vinh-Kha Le, Noam Kolt, Niels Mündler, Avi Semler, Emma Rodman, Jacob Drori, Carl Fossum, Luk Gloor, Milind Jagota, Ronak Pradeep, Honglu Fan, Tej Shah, Jonathan Eicher, Michael Chen, Kushal Thaman, William Merrill, Moritz Firsching, Carter Harris, Stefan Ciobâca, Jason Gross, Rohan Pandey, Ilya Gusev, Adam Jones, Shashank Agnihotri, Pavel Zhelnov, Siranut Usawasutsakorn, Mohammadreza Mofayezi, Alexander Piperski, Marc Carauleanu, David K. Zhang, Kostiantyn Dobarskyi, Dylan Ler, Roman Leventov, Ignat Soroko, Thorben Jansen, Scott Creighton, Pascal Lauer, Joshua Duersch, Vage Taamazyan, Dario Bezzi, Wiktor Morak, Wenjie Ma, William Held, Tran Ðuc Huy, Ruicheng Xian, Armel Randy Zebaze, Mohanad Mohamed, Julian Noah Leser, Michelle Yuan, Laila Yacar, Johannes Lengler, Katarzyna Olszewska, Hossein Shahrtash, Edson Oliveira, Joseph W. Jackson, Daniel Espinosa Gonzalez, Andy Zou, Muthu Chidambaram, Timothy Manik, Hector Haffenden, Dashiell Stander, Ali Dasouqi, Alexander Shen, Emilien Duc, Bita Golshani, David Stap, Mikalai Uzhou, Alina Borisovna Zhidkovskaya, Lukas Lewark, Miguel Orbegozo Rodriguez, Mátyás Vincze, Dustin Wehr, Colin Tang, Shaun Phillips, Fortuna Samuele, Jiang Muzhen, Fredrik Ekström, Angela Hammon, Oam Patel, Faraz Farhidi, George Medley, Forough Mohammadzadeh, Madellene Peñaflor, Haile Kassahun, Alena Friedrich, Claire Sparrow, Rayner Hernandez Perez, Taom Sakal, Omkar Dhamane, Ali Khajegili Mirabadi, Eric Hallman, Kenchi Okutsu, Mike Battaglia, Mohammad Maghsoudimehrabani, Alon Amit, Dave Hulbert, Roberto Pereira, Simon Weber, Handoko, Anton Peristyy, Stephen Malina, Samuel Albanie, Will Cai, Mustafa Mehkary, Rami Aly, Frank Reidegeld, Anna-Katharina Dick, Cary Friday, Jasdeep Sidhu, Hassan Shapourian, Wanyoung Kim, Mariana Costa, Hubeyb Gurdogan, Brian Weber, Harsh Kumar, Tong Jiang, Arunim Agarwal, Chiara Ceconello, Warren S. Vaz, Chao Zhuang, Haon Park, Andrew R. Tawfeek, Daattavya Aggarwal, Michael Kirchhof, Linjie Dai, Evan Kim, Johan Ferret, Yuzhou Wang, Minghao Yan, Krzysztof Burdzy, Lixin Zhang, Antonio Franca, Diana T. Pham, Kang Yong Loh, Joshua Robinson, Abram Jackson, Shreen Gul, Gunjan Chhablani, Zhehang Du, Adrian Cosma, Jesus Colino, Colin White, Jacob Votava, Vladimir Vinnikov, Ethan Delaney, Petr Spelda, Vit Stritecky, Syed M. Shahid, Jean-Christophe Mourrat, Lavr Vetoshkin, Koen Sponselee, Renas Bacho, Florencia de la Rosa, Xiuyu Li, Guillaume Malod, Leon Lang, Julien Laurendeau, Dmitry Kazakov, Fatimah Adesanya, Julien Portier, Lawrence Hollom, Victor Souza, Yuchen Anna Zhou, Julien Degorre, Yigit Yalın, Gbenga Daniel Obikoya, Luca Arnaboldi, Rai, Filippo Bigi, M. C. Boscá, Oleg Shumar, Kaniuar Bacho, Pierre Clavier, Gabriel Recchia, Mara Popescu, Nikita Shulga, Ngefor Mildred Tanwie, Denis Peskoff, Thomas C. H. Lux, Ben Rank, Colin Ni, Matthew Brooks, Alesia Yakimchyk, Huanxu, Liu, Olle Häggström, Emil Verkama, Hans Gundlach, Leonor Brito-Santana, Brian Amaro, Vivek Vajipey, Rynaa Grover, Yiyang Fan, Gabriel Poesia Reis Silva, Linwei Xin, Yosi Kratish, Jakub Łucki, WenDing Li, Sivakanth Gopi, Andrea Caciolai, Justin Xu, Kevin Joseph Scaria, Freddie Vargus, Farzad Habibi, Long, Lian, Emanuele Rodolà, Jules Robins, Vincent Cheng, Tony Fruhauff, Brad Raynor, Hao Qi, Xi Jiang, Ben Segev, Jingxuan Fan, Sarah Martinson, Erik Y. Wang, Kaylie Hausknecht, Michael P. Brenner, Mao Mao, Xinyu Zhang, David Avagian, Eshawn Jessica Scipio, Alon Ragoler, Justin Tan, Blake Sims, Rebeka Plecnik, Aaron Kirtland, Omer Faruk Bodur, D. P. Shinde, Zahra Adoul, Mohamed Zekry, Ali Karakoc, Tania C. B. Santos, Samir Shamseldeen, Loukmane Karim, Anna Liakhovitskaia, Nate Resman, Nicholas Farina, Juan Carlos Gonzalez, Gabe Maayan, Sarah Hoback, Rodrigo De Oliveira Pena, Glen Sherman, Elizabeth Kelley, Hodjat Mariji, Rasoul Pouriamanesh, Wentao Wu, Sandra Mendoza, Ismail Alarab, Joshua Cole, Danyelle Ferreira, Bryan Johnson, Mohammad Safdari, Liangti Dai, Siriphan Arthornthurasuk, Alexey Pronin, Jing Fan, Angel Ramirez-Trinidad, Ashley Cartwright, Daphiny Pottmaier, Omid Taheri, David Outevsky, Stanley Stepanic, Samuel Perry, Luke Askew, Raúl Adrián Huerta Rodríguez, Ali M. R. Minissi, Sam Ali, Ricardo Lorena, Krishnamurthy Iyer, Arshad Anil Fasiludeen, Sk Md Salauddin, Murat Islam, Juan Gonzalez, Josh Ducey, Maja Somrak, Vasilios Mavroudis, Eric Vergo, Juehang Qin, Benjámin Borbás, Eric Chu, Jack Lindsey, Anil Radhakrishnan, Antoine Jallon, I. M. J. McInnis, Pawan Kumar, Laxman Prasad Goswami, Daniel Bugas, Nasser Heydari, Ferenc Jeanplong, Archimedes Apronti, Abdallah Galal, Ng Ze-An, Ankit Singh, Joan of Arc Xavier, Kanu Priya Agarwal, Mohammed Berkani, Benedito Alves de Oliveira Junior, Dmitry Malishev, Nicolas Remy, Taylor D. Hartman, Tim Tarver, Stephen Mensah, Javier Gimenez, Roselynn Grace Montecillo, Russell Campbell, Asankhaya Sharma, Khalida Meer, Xavier Alapont, Deepakkumar Patil, Rajat Maheshwari, Abdelkader Dendane, Priti Shukla, Sergei Bogdanov, Sören Möller, Muhammad Rehan Siddiqi, Prajvi Saxena, Himanshu Gupta, Innocent Enyekwe, Ragavendran V, Zienab EL-Wasif, Aleksandr Maksapetyan, Vivien Rossbach, Chris Harjadi, Mohsen Bahaloohoreh, Song Bian, John Lai, Justine Leon Uro, Greg Bateman, Mohamed Sayed, Ahmed Menshawy, Darling Duclosel, Yashaswini Jain, Ashley Aaron, Murat Tiryakioglu, Sheeshram Siddh, Keith Krenek, Alex Hoover, Joseph McGowan, Tejal Patwardhan, Summer Yue, Alexandr Wang, and Dan Hendrycks. 2025. Humanitys last exam. Preprint, arXiv:2501.14249. Irene Plaza, Nina Melero, Cristina del Pozo, Javier Conde, Pedro Reviriego, Marina Mayor-Rocher, and María Grandury. 2024. Spanish and llm benchPreprint, marks: arXiv:2406.17789. in translation? is mmlu lost Qwen, :, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tianyi Tang, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. 2025. Qwen2.5 technical report. Preprint, arXiv:2412.15115. David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel R. Bowman. 2024. GPQA: graduate-level google-proof q&a benchmark. In First Conference on Language Modeling. LG AI Research, Soyoung An, Kyunghoon Bae, Eunbi Choi, Kibong Choi, Stanley Jungkyu Choi, Seokhee Hong, Junwon Hwang, Hyojin Jeon, Gerrard Jeongwon Jo, Hyunjik Jo, Jiyeon Jung, Yountae Jung, Hyosang Kim, Joonkee Kim, Seonghwan Kim, Soyeon Kim, Sunkyoung Kim, Yireun Kim, Yongil Kim, Youchul Kim, Edward Hwayoung Lee, Haeju Lee, Honglak Lee, Jinsik Lee, Kyungmin Lee, Woohyung Lim, Sangha Park, Sooyoun Park, Yongmin Park, Sihoon Yang, Heuiyeen Yeen, and Hyeongu Yun. 2024. Exaone 3.5: Series of large language models for real-world use cases. Preprint, arXiv:2412.04862. LG AI Research, Kyunghoon Bae, Eunbi Choi, Kibong Choi, Stanley Jungkyu Choi, Yemuk Choi, Seokhee Hong, Junwon Hwang, Hyojin Jeon, Kijeong Jeon, Gerrard Jeongwon Jo, Hyunjik Jo, Jiyeon Jung, Hyosang Kim, Joonkee Kim, Seonghwan Kim, Soyeon Kim, Sunkyoung Kim, Yireun Kim, Yongil Kim, Youchul Kim, Edward Hwayoung Lee, Haeju Lee, Honglak Lee, Jinsik Lee, Kyungmin Lee, Sangha Park, Yongmin Park, Sihoon Yang, Heuiyeen Yeen, Sihyuk Yi, and Hyeongu Yun. 2025. Exaone deep: Reasoning enhanced language models. Preprint, arXiv:2503.12524. Manley Roberts, Himanshu Thakur, Christine Herlihy, Colin White, and Samuel Dooley. 2024. To the cutoff... and beyond? longitudinal perspective on LLM In The Twelfth International data contamination. Conference on Learning Representations. Shivalika Singh, Angelika Romanou, Clémentine Fourrier, David I. Adelani, Jian Gang Ngui, Daniel Vila-Suero, Peerat Limkonchotiwat, Kelly Marchisio, Wei Qi Leong, Yosephine Susanto, Raymond Ng, Shayne Longpre, Wei-Yin Ko, Sebastian Ruder, Madeline Smith, Antoine Bosselut, Alice Oh, Andre F. T. Martins, Leshem Choshen, Daphne Ippolito, Enzo Ferrante, Marzieh Fadaee, Beyza Ermis, and Sara Hooker. 2025. Global mmlu: Understanding and addressing cultural and linguistic biases in multilingual evaluation. Preprint, arXiv:2412.03304. Shivalika Singh, Angelika Romanou, Clémentine Fourrier, David I. Adelani, Jian Gang Ngui, Daniel Vila-Suero, Peerat Limkonchotiwat, Kelly Marchisio, Wei Qi Leong, Yosephine Susanto, Raymond Ng, Shayne Longpre, Wei-Yin Ko, Madeline Smith, Antoine Bosselut, Alice Oh, Andre F. T. Martins, Leshem Choshen, Daphne Ippolito, Enzo Ferrante, Marzieh Fadaee, Beyza Ermis, and Sara Hooker. 2024. Global mmlu: Understanding and addressing cultural and linguistic biases in multilingual evaluation. Preprint, arXiv:2412.03304. Guijin Son, Hanwool Lee, Sungdong Kim, Seungone Kim, Niklas Muennighoff, Taekyoon Choi, Cheonbok Park, Kang Min Yoo, and Stella Biderman. 2024a. Kmmlu: Measuring massive multitask language understanding in korean. Preprint, arXiv:2402.11548. Guijin Son, Hanwool Lee, Suwan Kim, Huiseo Kim, Jae cheol Lee, Je Won Yeom, Jihyu Jung, Jung woo Kim, and Songseong Kim. 2024b. HAE-RAE bench: Evaluation of Korean knowledge in language models. In Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024), pages 79938007, Torino, Italia. ELRA and ICCL. Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R. Brown, Adam Santoro, Aditya Gupta, Adrià Garriga-Alonso, Agnieszka Kluska, Aitor Lewkowycz, Akshat Agarwal, Alethea Power, Alex Ray, Alex Warstadt, Alexander W. Kocurek, Ali Safaya, Ali Tazarv, Alice Xiang, Alicia Parrish, Allen Nie, Aman Hussain, Amanda Askell, Amanda Dsouza, Ambrose Slone, Ameet Rahane, Anantharaman S. Iyer, Anders Johan Andreassen, Andrea Madotto, Andrea Santilli, Andreas Stuhlmüller, Andrew M. Dai, Andrew La, Andrew Kyle Lampinen, Andy Zou, Angela Jiang, Angelica Chen, Anh Vuong, Animesh Gupta, Anna Gottardi, Antonio Norelli, Anu Venkatesh, Arash Gholamidavoodi, Arfa Tabassum, Arul Menezes, Arun Kirubarajan, Asher Mullokandov, Ashish Sabharwal, Austin Herrick, Avia Efrat, Aykut Erdem, Ayla Karakas, B. Ryan Roberts, Bao Sheng Loe, Barret Zoph, Bartłomiej Bojanowski, Batuhan Özyurt, Behnam Hedayatnia, Behnam Neyshabur, Benjamin Inden, Benno Stein, Berk Ekmekci, Bill Yuchen Lin, Blake Howald, Bryan Orinion, Cameron Diao, Cameron Dour, Catherine Stinson, Cedrick Argueta, Cesar Ferri, Chandan Singh, Charles Rathkopf, Chenlin Meng, Chitta Baral, Chiyu Wu, Chris CallisonBurch, Christopher Waites, Christian Voigt, Christopher Manning, Christopher Potts, Cindy Ramirez, Clara E. Rivera, Clemencia Siro, Colin Raffel, Courtney Ashcraft, Cristina Garbacea, Damien Sileo, Dan Garrette, Dan Hendrycks, Dan Kilman, Dan Roth, C. Daniel Freeman, Daniel Khashabi, Daniel Levy, Daniel Moseguí González, Danielle Perszyk, Danny Hernandez, Danqi Chen, Daphne Ippolito, Dar Gilboa, David Dohan, David Drakard, David Jurgens, Debajyoti Datta, Deep Ganguli, Denis Emelin, Denis Kleyko, Deniz Yuret, Derek Chen, Derek Tam, Dieuwke Hupkes, Diganta Misra, Dilyar Buzan, Dimitri Coelho Mollo, Diyi Yang, Dong-Ho Lee, Dylan Schrader, Ekaterina Shutova, Ekin Dogus Cubuk, Elad Segal, Eleanor Hagerman, Elizabeth Barnes, Elizabeth Donoway, Ellie Pavlick, Emanuele Rodolà, Emma Lam, Eric Chu, Eric Tang, Erkut Erdem, Ernie Chang, Ethan Chi, Ethan Dyer, Ethan Jerzak, Ethan Kim, Eunice Engefu Manyasi, Evgenii Zheltonozhskii, Fanyue Xia, Fatemeh Siar, Fernando Martínez-Plumed, Francesca Happé, Francois Chollet, Frieda Rong, Gaurav Mishra, Genta Indra Winata, Gerard de Melo, Germàn Kruszewski, Giambattista Parascandolo, Giorgio Mariani, Gloria Xinyue Wang, Gonzalo Jaimovitch-Lopez, Gregor Betz, Guy Gur-Ari, Hana Galijasevic, Hannah Kim, Hannah Rashkin, Hannaneh Hajishirzi, Harsh Mehta, Hayden Bogar, Henry Francis Anthony Shevlin, Hinrich Schuetze, Hiromu Yakura, Hongming Zhang, Hugh Mee Wong, Ian Ng, Isaac Noble, Jaap Jumelet, Jack Geissinger, Jackson Kernion, Jacob Hilton, Jaehoon Lee, Jaime Fernández Fisac, James Simon, James Koppel, James Zheng, James Zou, Jan Kocon, Jana Thompson, Janelle Wingfield, Jared Kaplan, Jarema Radom, Jascha Sohl-Dickstein, Jason Phang, Jason Wei, Jason Yosinski, Jekaterina Novikova, Jelle Bosscher, Jennifer Marsh, Jeremy Kim, Jeroen Taal, Jesse Engel, Jesujoba Alabi, Jiacheng Xu, Jiaming Song, Jillian Tang, Joan Waweru, John Burden, John Miller, John U. Balis, Jonathan Batchelder, Jonathan Berant, Jörg Frohberg, Jos Rozen, Jose HernandezOrallo, Joseph Boudeman, Joseph Guerr, Joseph Jones, Joshua B. Tenenbaum, Joshua S. Rule, Joyce Chua, Kamil Kanclerz, Karen Livescu, Karl Krauth, Karthik Gopalakrishnan, Katerina Ignatyeva, Katja Markert, Kaustubh Dhole, Kevin Gimpel, Kevin Omondi, Kory Wallace Mathewson, Kristen Chiafullo, Ksenia Shkaruta, Kumar Shridhar, Kyle McDonell, Kyle Richardson, Laria Reynolds, Leo Gao, Li Zhang, Liam Dugan, Lianhui Qin, Lidia ContrerasOchando, Louis-Philippe Morency, Luca Moschella, Lucas Lam, Lucy Noble, Ludwig Schmidt, Luheng He, Luis Oliveros-Colón, Luke Metz, Lütfi Kerem Senel, Maarten Bosma, Maarten Sap, Maartje Ter Hoeve, Maheen Farooqi, Manaal Faruqui, Mantas Mazeika, Marco Baturan, Marco Marelli, Marco Maru, Maria Jose Ramirez-Quintana, Marie Tolkiehn, Mario Giulianelli, Martha Lewis, Martin Potthast, Matthew Leavitt, Matthias Hagen, Mátyás Schubert, Medina Orduna Baitemirova, Melody Arnaud, Melvin McElrath, Michael Andrew Yee, Michael Cohen, Michael Gu, Michael Ivanitskiy, Michael Starritt, Michael Strube, Michał Swedrowski, Michele Bevilacqua, Michihiro Yasunaga, Mihir Kale, Mike Cain, Mimee Xu, Mirac Suzgun, Mitch Walker, Mo Tiwari, Mohit Bansal, Moin Aminnaseri, Mor Geva, Mozhdeh Gheini, Mukund Varma T, Nanyun Peng, Nathan Andrew Chi, Nayeon Lee, Neta GurAri Krakover, Nicholas Cameron, Nicholas Roberts, Nick Doiron, Nicole Martinez, Nikita Nangia, Niklas Deckers, Niklas Muennighoff, Nitish Shirish Keskar, Niveditha S. Iyer, Noah Constant, Noah Fiedel, Nuan Wen, Oliver Zhang, Omar Agha, Omar Elbaghdadi, Omer Levy, Owain Evans, Pablo Antonio Moreno Casares, Parth Doshi, Pascale Fung, Paul Pu Liang, Paul Vicol, Pegah Alipoormolabashi, Peiyuan Liao, Percy Liang, Peter Chang, Peter Eckersley, Phu Mon Htut, Pinyu Hwang, Piotr Miłkowski, Piyush Patil, Pouya Pezeshkpour, Priti Oli, Qiaozhu Mei, Qing Lyu, Qinlang Chen, Rabin Banjade, Rachel Etta Rudolph, Raefer Gabriel, Rahel Habacker, Ramon Risco, Raphaël Millière, Rhythm Garg, Richard Barnes, Rif A. Saurous, Riku Arakawa, Robbe Raymaekers, Robert Frank, Rohan Sikand, Roman Novak, Roman Sitelew, Ronan Le Bras, Rosanne Liu, Rowan Jacobs, Rui Zhang, Russ Salakhutdinov, Ryan Andrew Chi, Seungjae Ryan Lee, Ryan Stovall, Ryan Teehan, Rylan Yang, Sahib Singh, Saif M. Mohammad, Sajant Anand, Sam Dillavou, Sam Shleifer, Sam Wiseman, Samuel Gruetter, Samuel R. Bowman, Samuel Stern Schoenholz, Sanghyun Han, Sanjeev Kwatra, Sarah A. Rous, Sarik Ghazarian, Sayan Ghosh, Sean Casey, Sebastian Bischoff, Sebastian Gehrmann, Sebastian Schuster, Sepideh Sadeghi, Shadi Hamdan, Sharon Zhou, Shashank Srivastava, Sherry Shi, Shikhar Singh, Shima Asaadi, Shixiang Shane Gu, Shubh Pachchigar, Shubham Toshniwal, Shyam Upadhyay, Shyamolima Shammie Debnath, Siamak Shakeri, Simon Thormeyer, Simone Melzi, Siva Reddy, Sneha Priscilla Makini, Soo-Hwan Lee, Spencer Torene, Sriharsha Hatwar, Stanislas Dehaene, Stefan Divic, Stefano Ermon, Stella Biderman, Stephanie Lin, Stephen Prasad, Steven Piantadosi, Stuart Shieber, Summer Misherghi, Svetlana Kiritchenko, Swaroop Mishra, Tal Linzen, Tal Schuster, Tao Li, Tao Yu, Tariq Ali, Tatsunori Hashimoto, Te-Lin Wu, Théo Desbordes, Theodore Rothschild, Thomas Phan, Tianle Wang, Tiberius Nkinyili, Timo Schick, Timofei Kornev, Titus Tunduny, Tobias Gerstenberg, Trenton Chang, Trishala Neeraj, Tushar Khot, Tyler Shultz, Uri Shaham, Vedant Misra, Vera Demberg, Victoria Nyamai, Vikas Raunak, Vinay Venkatesh Ramasesh, vinay uday prabhu, Vishakh Padmakumar, Vivek Srikumar, William Fedus, William Saunders, William Zhang, Wout Vossen, Xiang Ren, Xiaoyu Tong, Xinran Zhao, Xinyi Wu, Xudong Shen, Yadollah Yaghoobzadeh, Yair Lakretz, Yangqiu Song, Yasaman Bahri, Yejin Choi, Yichi Yang, Sophie Hao, Yifu Chen, Yonatan Belinkov, Yu Hou, Yufang Hou, Yuntao Bai, Zachary Seid, Zhuoye Zhao, Zijian Wang, Zijie J. Wang, Zirui Wang, and Ziyi Wu. 2023. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. Transactions on Machine Learning Research. Featured Certification. Google Team. 2025a. Gemma 3 technical report. Kanana LLM Team, Yunju Bak, Hojin Lee, Minho Ryu, Jiyeon Ham, Seungjae Jung, Daniel Wontae Nam, Taegyeong Eo, Donghun Lee, Doohae Jung, Boseop Kim, Nayeon Kim, Jaesun Park, Hyunho Kim, Hyunwoong Ko, Changmin Lee, Kyoung-Woon On, Seulye Baeg, Junrae Cho, Sunghee Jung, Jieun Kang, EungGyun Kim, Eunhwa Kim, Byeongil Ko, Daniel Lee, Minchul Lee, Miok Lee, Shinbok Lee, and Gaeun Seo. 2025a. Kanana: Compute-efficient bilingual language models. Preprint, arXiv:2502.18934. M-A-P Team, Xinrun Du, Yifan Yao, Kaijing Ma, Bingli Wang, Tianyu Zheng, Kang Zhu, Minghao Liu, Yiming Liang, Xiaolong Jin, Zhenlin Wei, Chujie Zheng, Kaixing Deng, Shuyue Guo, Shian Jia, Sichao Jiang, Yiyan Liao, Rui Li, Qinrui Li, Sirun Li, Yizhi Li, Yunwen Li, Dehua Ma, Yuansheng Ni, Haoran Que, Qiyao Wang, Zhoufutu Wen, Siwei Wu, Tianshun Xing, Ming Xu, Zhenzhu Yang, Zekun Moore Wang, Junting Zhou, Yuelin Bai, Xingyuan Bu, Chenglin Cai, Liang Chen, Yifan Chen, Chengtuo Cheng, Tianhao Cheng, Keyi Ding, Siming Huang, Yun Huang, Yaoru Li, Yizhe Li, Zhaoqun Li, Tianhao Liang, Chengdong Lin, Hongquan Lin, Yinghao Ma, Zhongyuan Peng, Zifan Peng, Qige Qi, Shi Qiu, Xingwei Qu, Yizhou Tan, Zili Wang, Chenqing Wang, Hao Wang, Yiya Wang, Yubo Wang, Jiajun Xu, Kexin Yang, Ruibin Yuan, Yuanhao Yue, Tianyang Zhan, Chun Zhang, Jingyang Zhang, Xiyue Zhang, Xingjian Zhang, Yue Zhang, Yongchi Zhao, Xiangyu Zheng, Chenghua Zhong, Yang Gao, Zhoujun Li, Dayiheng Liu, Qian Liu, Tianyu Liu, Shiwen Ni, Junran Peng, Yujia Qin, Wenbo Su, Guoyin Wang, Shi Wang, Jian Yang, Min Yang, Meng Cao, Xiang Yue, Zhaoxiang Zhang, Wangchunshu Zhou, Jiaheng Liu, Qunshu Lin, Wenhao Huang, and Ge Zhang. 2025b. Supergpqa: Scaling llm evaluation across 285 graduate disciplines. Preprint, arXiv:2502.14739. Qwen Team. 2025b. Qwq-32b: Embracing the power of reinforcement learning. Joshua Vendrow, Edward Vendrow, Sara Beery, and Do large language Aleksander Madry. 2025. model benchmarks test arXiv:2502.03461. reliability? Preprint, Yiming Wang, Pei Zhang, Jialong Tang, Haoran Wei, Baosong Yang, Rui Wang, Chenshu Sun, Feitong Sun, Jiran Zhang, Junxuan Wu, Qiqian Cang, Yichang Zhang, Fei Huang, Junyang Lin, Fei Huang, and Jingren Zhou. 2025. Polymath: Evaluating mathematical reasoning in multilingual contexts. Preprint, arXiv:2504.18428. Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, Tianle Li, Max Ku, Kai Wang, Alex Zhuang, Rongqi Fan, Xiang Yue, and Wenhu Chen. 2024. MMLU-pro: more robust and challenging multi-task language understanding benchmark. In The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track. Colin White, Samuel Dooley, Manley Roberts, Arka Pal, Benjamin Feuer, Siddhartha Jain, Ravid Shwartz-Ziv, Neel Jain, Khalid Saifullah, Sreemanti Dey, ShubhAgrawal, Sandeep Singh Sandha, Siddartha Venkat Naidu, Chinmay Hegde, Yann LeCun, Tom Goldstein, Willie Neiswanger, and Micah Goldblum. 2025. Livebench: challenging, contamination-limited LLM benchmark. In The Thirteenth International Conference on Learning Representations. xAI. 2025. Grok 3 beta the age of reasoning agents. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao Ge, Haoran Wei, Huan Lin, Jialong Tang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jing Zhou, Jingren Zhou, Junyang Lin, Kai Dang, Keqin Bao, Kexin Yang, Le Yu, Lianghao Deng, Mei Li, Mingfeng Xue, Mingze Li, Pei Zhang, Peng Wang, Qin Zhu, Rui Men, Ruize Gao, Shixuan Liu, Shuang Luo, Tianhao Li, Tianyi Tang, Wenbiao Yin, Xingzhang Ren, Xinyu Wang, Xinyu Zhang, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yinger Zhang, Yu Wan, Yuqiong Liu, Zekun Wang, Zeyu Cui, Zhenru Zhang, Zhipeng Zhou, and Zihan Qiu. 2025. Qwen3 technical report. Preprint, arXiv:2505.09388. Kang Min Yoo, Jaegeun Han, Sookyo In, Heewon Jeon, Jisu Jeong, Jaewook Kang, Hyunwook Kim, Kyung-Min Kim, Munhyong Kim, Sungju Kim, Donghyun Kwak, Hanock Kwak, Se Jung Kwon, Bado Lee, Dongsoo Lee, Gichang Lee, Jooho Lee, Baeseong Park, Seongjin Shin, Joonsang Yu, Seolki Baek, Sumin Byeon, Eungsup Cho, Dooseok Choe, Jeesung Han, Youngkyun Jin, Hyein Jun, Jaeseung Jung, Chanwoong Kim, Jinhong Kim, Jinuk Kim, Dokyeong Lee, Dongwook Park, Jeong Min Sohn, Sujung Han, Jiae Heo, Sungju Hong, Mina Jeon, Hyunhoon Jung, Jungeun Jung, Wangkyo Jung, Chungjoon Kim, Hyeri Kim, Jonghyun Kim, Min Young Kim, Soeun Lee, Joonhee Park, Jieun Shin, Sojin Yang, Jungsoon Yoon, Hwaran Lee, Sanghwan Bae, Jeehwan Cha, Karl Gylleus, Donghoon Ham, Mihak Hong, Youngki Hong, Yunki Hong, Dahyun Jang, Hyojun Jeon, Yujin Jeon, Yeji Jeong, Myunggeun Ji, Yeguk Jin, Chansong Jo, Shinyoung Joo, Seunghwan Jung, Adrian Jungmyung Kim, Byoung Hoon Kim, Hyomin Kim, Jungwhan Kim, Minkyoung Kim, Minseung Kim, Sungdong Kim, Yonghee Kim, Youngjun Kim, Youngkwan Kim, Donghyeon Ko, Dughyun Lee, Ha Young Lee, Jaehong Lee, Jieun Lee, Jonghyun Lee, Jongjin Lee, Min Young Lee, Yehbin Lee, Taehong Min, Yuri Min, Kiyoon Moon, Hyangnam Oh, Jaesun Park, Kyuyon Park, Younghun Park, Hanbae Seo, Seunghyun Seo, Mihyun Sim, Gyubin Son, Matt Yeo, Kyung Hoon Yeom, Wonjoon Yoo, Myungin You, Doheon Ahn, Homin Ahn, Joohee Ahn, Seongmin Ahn, Chanwoo An, Hyeryun An, Junho An, Sang-Min An, Boram Byun, Eunbin Byun, Jongho Cha, Minji Chang, Seunggyu Chang, Haesong Cho, Youngdo Cho, Dalnim Choi, Daseul Choi, Hyoseok Choi, Minseong Choi, Sangho Choi, Seongjae Choi, Wooyong Choi, Sewhan Chun, Dong Young Go, Chiheon Ham, Danbi Han, Jaemin Han, Moonyoung Hong, Sung Bum Hong, Dong-Hyun Hwang, Seongchan Hwang, Jinbae Im, Hyuk Jin Jang, Jaehyung Jang, Jaeni Jang, Sihyeon Jang, Sungwon Jang, Joonha Jeon, Daun Jeong, Joonhyun Jeong, Kyeongseok Jeong, Mini Jeong, Sol Jin, Hanbyeol Jo, Hanju Jo, Minjung Jo, Chaeyoon Jung, Hyungsik Jung, Jaeuk Jung, Ju Hwan Jung, Kwangsun Jung, Seungjae Jung, Soonwon Ka, Donghan Kang, Soyoung Kang, Taeho Kil, Areum Kim, Beomyoung Kim, Byeongwook Kim, Daehee Kim, Dong-Gyun Kim, Donggook Kim, Donghyun Kim, Euna Kim, Eunchul Kim, Geewook Kim, Gyu Ri Kim, Hanbyul Kim, Heesu Kim, Isaac Kim, Jeonghoon Kim, Jihye Kim, Joonghoon Kim, Minjae Kim, Minsub Kim, Pil Hwan Kim, Sammy Kim, Seokhun Kim, Seonghyeon Kim, Soojin Kim, Soong Kim, Soyoon Kim, Sunyoung Kim, Taeho Kim, Wonho Kim, Yoonsik Kim, You Jin Kim, Yuri Kim, Beomseok Kwon, Ohsung Kwon, YooHwan Kwon, Anna Lee, Byungwook Lee, Changho Lee, Daun Lee, Dongjae Lee, Ha-Ram Lee, Hodong Lee, Hwiyeong Lee, Hyunmi Lee, Injae Lee, Jaeung Lee, Jeongsang Lee, Jisoo Lee, Jongsoo Lee, Joongjae Lee, Juhan Lee, Jung Hyun Lee, Junghoon Lee, Junwoo Lee, Se Yun Lee, Sujin Lee, Sungjae Lee, Sungwoo Lee, Wonjae Lee, Zoo Hyun Lee, Jong Kun Lim, Kun Lim, Taemin Lim, Nuri Na, Jeongyeon Nam, Kyeong-Min Nam, Yeonseog Noh, Biro Oh, Jung-Sik Oh, Solgil Oh, Yeontaek Oh, Boyoun Park, Cheonbok Park, Dongju Park, Hyeonjin Park, Hyun Tae Park, Hyunjung Park, Jihye Park, Jooseok Park, Junghwan Park, Jungsoo Park, Miru Park, Sang Hee Park, Seunghyun Park, Soyoung Park, Taerim Park, Wonkyeong Park, Hyunjoon Ryu, Jeonghun Ryu, Nahyeon Ryu, Soonshin Seo, Suk Min Seo, Yoonjeong Shim, Kyuyong Shin, Wonkwang Shin, Hyun Sim, Woongseob Sim, Hyejin Soh, Bokyong Son, Hyunjun Son, Seulah Son, ChiYun Song, Chiyoung Song, Ka Yeon Song, Minchul Song, Seungmin Song, Jisung Wang, Yonggoo Yeo, Myeong Yeon Yi, Moon Bin Yim, Taehwan Yoo, Youngjoon Yoo, Sungmin Yoon, Young Jin Yoon, Hangyeol Yu, Ui Seon Yu, Xingdong Zuo, Jeongin Bae, Joungeun Bae, Hyunsoo Cho, Seonghyun Cho, Yongjin Cho, Taekyoon Choi, Yera Choi, Jiwan Chung, Zhenghui Han, Byeongho Heo, Euisuk Hong, Taebaek Hwang, Seonyeol Im, Sumin Jegal, Sumin Jeon, Yelim Jeong, Yonghyun Jeong, Can Jiang, Juyong Jiang, Jiho Jin, Ara Jo, Younghyun Jo, Hoyoun Jung, Juyoung Jung, Seunghyeong Kang, Dae Hee Kim, Ginam Kim, Hangyeol Kim, Heeseung Kim, Hyojin Kim, Hyojun Kim, Hyun-Ah Kim, Jeehye Kim, Jin-Hwa Kim, Jiseon Kim, Jonghak Kim, Jung Yoon Kim, Rak Yeong Kim, Seongjin Kim, Seoyoon Kim, Sewon Kim, Sooyoung Kim, Sukyoung Kim, Taeyong Kim, Naeun Ko, Bonseung Koo, Heeyoung Kwak, Haena Kwon, Youngjin Kwon, Boram Lee, Bruce W. Lee, Dagyeong Lee, Erin Lee, Euijin Lee, Ha Gyeong Lee, Hyojin Lee, Hyunjeong Lee, Jeeyoon Lee, Jeonghyun Lee, Jongheok Lee, Joonhyung Lee, Junhyuk Lee, Mingu Lee, Nayeon Lee, Sangkyu Lee, Se Young Lee, Seulgi Lee, Seung Jin Lee, Suhyeon Lee, Yeonjae Lee, Yesol Lee, Youngbeom Lee, Yujin Lee, Shaodong Li, Tianyu Liu, Seong-Eun Moon, Taehong Moon, Max-Lasse Nihlenramstroem, Wonseok Oh, Yuri Oh, Hongbeen Park, Hyekyung Park, Jaeho Park, Nohil Park, Sangjin Park, Jiwon Ryu, Miru Ryu, Simo Ryu, Ahreum Seo, Hee Seo, Kangdeok Seo, Jamin Shin, Seungyoun Shin, Heetae Sin, Jiangping Wang, Lei Wang, Ning Xiang, Longxiang Xiao, Jing Xu, Seonyeong Yi, Haanju Yoo, Haneul Yoo, Hwanhee Yoo, Liang Yu, Youngjae Yu, Weijie Yuan, Bo Zeng, Qian Zhou, Kyunghyun Cho, Jung-Woo Ha, Joonsuk Park, Jihyun Hwang, Hyoung Jo Kwon, Soonyong Kwon, Jungyeon Lee, Seungho Lee, Seonghyeon Lim, Hyunkyung Noh, Seungho Choi, Sang-Woo Lee, Jung Hwa Lim, and Nako Sung. 2024. Hyperclova technical report. Preprint, arXiv:2404.01954. Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. 2019. HellaSwag: Can machine really finish your sentence? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 47914800, Florence, Italy. Association for Computational Linguistics. Yidan Zhang, Yu Wan, Boyi Deng, Baosong Yang, Haoran Wei, Fei Huang, Bowen Yu, Junyang Lin, Fei Huang, and Jingren Zhou. 2025. P-mmeval: parallel multilingual multitask benchmark for consistent evaluation of llms. Preprint, arXiv:2411.09116. Qihao Zhao, Yangyu Huang, Tengchao Lv, Lei Cui, Qinzheng Sun, Shaoguang Mao, Xin Zhang, Ying Xin, Qiufeng Yin, Scarlett Li, and Furu Wei. 2024. Mmlu-cf: contamination-free multi-task Preprint, language understanding benchmark. arXiv:2412.15194. Lianmin Zheng, Liangsheng Yin, Zhiqiang Xie, Chuyue Sun, Jeff Huang, Cody Hao Yu, Shiyi Cao, Christos Kozyrakis, Ion Stoica, Joseph E. Gonzalez, Clark Barrett, and Ying Sheng. 2024. SGLang: Efficient execution of structured language model programs. In The Thirty-eighth Annual Conference on Neural Information Processing Systems. Details of KMMLU-REDUX Errors A.1 Error Statistics We define set of error types based on recurring issues observed during our analysis of the KMMLU. Then, we find out the number of data instances per each error type as shown in Table 4. To identify leaked answer cases, we apply rule-based filtering using string-overlap heuristics. For other error types, including notation errors, bad clarity, and ill-posed questions, we leverage GPT-4o to assist in annotation. Also, we provide the prompt used for annotation in Figure 6. Ill-posed Question : The question lacks critical references or contextual information. Leaked Answer : The ground truth is explicitly stated within the question itself. Notation Error : Errors in mathematical expressions or chemical equations. Bad Clarity : The data itself is unclear and contains grammatical errors."
        },
        {
            "title": "Error Type",
            "content": "# of Questions (Ratio) Ill-posed Question Leaked Answer Notation Error Bad Clarity 512 (1.46 %) 42 (0.11%) 846 (2.42%) 1284 (3.67%) Table 4: Statistics of the error types A.2 Example of Error Types Table 10 provides examples of the error types in 2.2 identified in KMMLU. Each example illustrates specific issue that affects benchmark reliability."
        },
        {
            "title": "B Korean National Technical",
            "content": "Qualification List of KMMLU-REDUX Table 9 presents the list of Korean National Technical Qualifications (KNTQs) included in our benchmark along with their corresponding official exam dates. To facilitate analysis, we categorize the 100 NTQs into Korean Standard Industrial Classification (KSIC) where mapping to the exam (see Figure 7). This categorization enables structured evaluation across diverse domains and better reflects the real-world industrial fields. GPT-4o Error Annotation Prompt You are helpful assistant that annotates error types in questions. - Ill-posed question stands for which question miss the critical reference information to solve the question (such as table, formular, image, etc.). - Notation correct stands for which question has correct math and chemical notation. Criteria of correctness whether the notation is impact to solve the question. (e.g. m2 is incorrect, but mˆ2 is correct. 센티미터 is correct.) - Grammar correct stands for which question has correct grammar and spelling. Criteria of correctness whether the notation is impact to understand the question. (e.g. 상 담 기 법 보 다 는 상 담 자 의 인 간 적 자 질 과 진 솔 한 태 도 를 중 시 한 다 . is grammatically incorrect due to spacing error.) Please check the following question whether it has error types. Check ill-posed question True/False. Check notation correct True/False. Check grammar correct True/False. Then, if there is any error, please explain the reason. Question {{question}} Figure 6: The prompt is used for error type annotation. Each sample is annotated as an error if the respective field returns True. The Grammar Correct field is used to detect Bad Clarity cases. Details of KMMLU-PRO Annotation C.1 Annotation Pipeline We first conduct OCR parsing with GPT-4o on PDF files of KNPL acquisition exams. With the parsed data, the main tasks for human annotators are: 1) reviewing parsing errors, 2) converting tables into latex format, and 3) converting images into text which conveys same meaning. If it is impossible to convert an image into text, we remove it. For the cases where multiple answers are allowed, commonly due to the ambiguity of question itself, we discard them. As we leverage the official PDF files managed and controlled by the government, we can guarantee the correctness of answer label. This help us to save costs because we do not need experts for annotations nor the answer relabeling to avoid risk of data from online (Gema et al., 2025; Team et al., 2025b). Before annotation, we explained the context of benchmark construction about the Korean professional license exams to human annotators. We present annotation instructions in Fig 8. Figure 7: Domain distribution of problems in KMMLUREDUX. The total size of the dataset is 2,587. Figure 8: Excerpt from the annotation guidelines for converting PDF documents into structured text. We carefully instruct LaTeX table formatting. C.2 Annotators Demographics"
        },
        {
            "title": "Female\nMale",
            "content": "20s 30s 40s"
        },
        {
            "title": "Academic background",
            "content": "Bachelors degrees Masters degrees 23 - 13 8 2 20 3 Table 5: Demographics of the human annotators for KMMLU-PRO. With 23 annotators, it took 8 business days to complete all annotations, including project setup. The detailed demographics of annotators are presented in Table 5. The total amount of annotations was approximately $14,000. The average hourly wage is 8.83 U.S. dollars, which is higher than the legal minimum wage at the time of hiring in South Korea. D.1 Evaluation Prompts The figure 9 and figure 10 present the prompt for the evaluation written in English and Korean, respectively. For the English prompt, we use the regex expression of r\"(?i)Answer[ˆA-E]*: [ˆA-E]*([A-E])\". For the Korean prompt, we use r\"정답[ˆA-E]*:[ˆA-E]*([A-E])\". The regex expressions for the flexible parsing are r\"Answer[ˆA-E]*([A-E])([A-E]))\" and r\" 정답[ˆA-E]*([A-E])([A-E]))\", respectively. D."
        },
        {
            "title": "Inference Engines",
            "content": "Excluding closed models, the main inference engine we use is SGLang (Zheng et al., 2024). However, for the Gemma 3 series and Mistral Small 3.1 Instruct models, we adopt vLLM (Kwon et al., 2023) due to their incompatibility with SGLang at the time of the paper writing. D.3 License Acquisition Criteria for KMMLU-PRO We follow the official scoring criteria used for each license examination. All licensing exams in"
        },
        {
            "title": "English Prompt",
            "content": "Answer the following multiple choice question. The last line of your response should be of the following format: Answer: $LETTER (without quotes) where LETTER is one of ABCD. Think step by step before answering. {{question}} A) {{option }} B) {{option }} C) {{option }} D) {{option }} Figure 9: The English prompt used for evaluating LLMs on our KMMLU-REDUX and KMMLU-PRO. This prompt is exactly same with the prompt used for Multiple-Choices Question Answering (MCQA) in OpenAIs simple-evals repository. The number of options is adjusted for each problems."
        },
        {
            "title": "Korean Prompt",
            "content": "다음 문제에 대해 정답을 고르세요. 당신의 최종 정답은 ABCD 중 하나이고, \"정답:\" 뒤에 와야 합니다. 정답을 고르기 전에 차근차근 생각하고 추론하세요. {{question}} A) {{option }} B) {{option }} C) {{option }} D) {{option }} Figure 10: The Korean prompt used for evaluating LLMs on our KMMLU-REDUX and KMMLU-PRO. This prompt is translated version of the English prompt. The number of options is adjusted for each problems. KMMLU-PRO are composed of multiple subjects. Candidates are typically required to score at least 40% in every subject and achieve an average score of at least 60%, except for the cases of the Certified Judicial Scrivener and the Lawyer. For the Certified Judicial Scrivener exam, candidates need only score at least 40% in each subject, with no requirement regarding the average. The Lawyer license exam uses relative grading system where only certain proportion of top-scoring candidates pass. The usual cut-off point is approximately 54.22 (900 out of 1660), which we use as the passing threshold. It is also important to note that our evaluation benchmark is text-based, not multi-modal. Therefore, we exclude questions that include images. In addition, candidates often need to go through multiple exam stages to obtain license, with the later stages, such as the second or third, containing descriptive questions. However, since we only collect multiple-choice questions, the descriptive problems are excluded. Lastly, we exclude questions with multiple answers introduced by the ambiguity of the question."
        },
        {
            "title": "E Detailed Results",
            "content": "E.1 The Results for Smaller (<10B) Models The Table 6 presents the results of KMMLUPRO and KMMLU-REDUX for smaller (<10B) the tiny models in models. Since many of this table were used to construct KMMLUREDUX through adversarial filtration, their KMMLU-REDUX scores are biased. Nevertheless, KMMLU-Redux Acc KMMLU-Pro"
        },
        {
            "title": "Acc",
            "content": "# of passed KNPLs Avg. Acc (micro) DeepSeek-R1-Distill-Qwen-1.5B (DeepSeek-AI et al., 2025a) Llama 3.2 3B Instruct (Meta, 2024c) Gemma 3 4B IT (Team, 2025a) Qwen 2.5 3B Instruct (Qwen et al., 2025) Qwen3-1.7B (Yang et al., 2025) Kanana Nano 2.1B Instruct (Team et al., 2025a) Aya Expanse 8B (Dang et al., 2024) EXAONE Deep 2.4B (Research et al., 2025) EXAONE 3.5 2.4B Instruct (Research et al., 2024) HyperCLOVAX-SEED-Text-Instruct-1.5B (naver hyperclovax, 2025) Llama 3.1 8B Instruct (Meta, 2024b) Qwen3-1.7B (w/ thinking) (Yang et al., 2025) Ko-R1-7B-v2.1 (OneLineAI, 2025) EXAONE 3.5 7.8B Instruct (Research et al., 2024) EXAONE Deep 7.8B (Research et al., 2025) Qwen3-8B (Yang et al., 2025) Qwen3-8B (w/ thinking) (Yang et al., 2025) 21.30 17.59 25.09 24.74 28.99 27.25 28.30 28.84 27.72 33.94 31.89 37.80 41.94 41.90 44.99 49.25 58.79 20.55 25.53 32.86 33.27 30.42 32.60 31.65 32.08 34.49 30.13 33.81 38.27 38.70 41.71 41.53 46.92 55.27 0/14 0/14 0/14 0/14 0/14 0/14 0/14 0/14 0/14 0/14 0/14 1/14 1/14 0/14 0/14 1/14 3/14 20.91 21.73 29.14 29.19 29.74 30.04 30.05 30.53 31.25 31.95 32.89 38.05 40.25 41.80 43.18 48.03 56. Table 6: The main evaluation results of KMMLU-REDUX and KMMLU-PRO benchmarks on smaller (< 10B) LLMs. The gray-shaded models are the dense-reasoning models. The KMMLU-REDUX scores with are biased as these models are used for the dataset filtration (Section 2.2.1). as shown by the results for larger models, models equipped with dense reasoning capabilities usually outperform their counterparts without reasoning (e.g., Qwen3-8B with and without thinking). E.2 Breakdown of Results of KMMLU-REDUX The Table 7 presents the breakdown results for 14 categories in KMMLU-REDUX across various LLMs. E.3 Breakdown of Results of KMMLU-PRO The Table 8 shows the breakdown results for all NPLs in KMMLU-PRO across various LLMs. While the models relatively easily pass licensing in the medicine domain, they struggle in the Law and Tax&Accounting domains. r & m i t e a e Domain Open-weight Models < 5B < 10B < 20B < 32B > 70B Llama 3.2 3B Instruct Qwen 2.5 3B Instruct DeepSeek-R1-Distill-Qwen-1.5B 20.50 13.00 16.50 Gemma 3 4B IT 23.00 24.75 26.75 EXAONE Deep 2.4B 27.50 Qwen3-1.7B 30.50 HyperCLOVAX-SEED-Text-Instruct-1.5B 28.00 37.25 Kanana Nano 2.1B Instruct EXAONE 3.5 2.4B Instruct Qwen3-1.7B (w/ thinking) Llama 3.1 8B Instruct EXAONE 3.5 7.8B Instruct Ko-R1-7B-v2.10 Aya Expanse 8B 25.00 30.50 41.75 42.50 EXAONE Deep 7.8B 40.75 Qwen3-8B 47.00 55.75 Qwen3-8B (w/ thinking) Gemma 3 12B IT 44.50 Phi-4 (14B) 48.50 Qwen3-14B 52.75 64.50 Qwen3-14B (w/ thinking) EXAONE 3.5 32B Instruct Mistral Small 3.1 Instruct (24B) Aya Expanse 32B 30.50 46.75 45.00 Gemma 3 27B IT 49.50 EXAONE Deep 32B 53.25 Qwen3-30B-A3B 54.25 Qwen3-32B 59.50 63.00 QwQ 32B 61.25 64.50 Qwen3-32B (w/ thinking) Qwen3-30B-A3B (w/ thinking) Llama 3.3 70B Instruct C4AI Command (111B) DeepSeek V3 (671B) Llama-4-Scout-17B-16E-Instruct 52.25 56.75 62.50 64.00 Qwen3-235B-A22B 64.25 69.75 73.50 72. Qwen3-235B-A22B (w/ thinking) Llama-4-Maverick-17B-128E-Instruct DeepSeek R1 (671B) Closed Models GPT-4.1 mini (2024-04-14) o3-mini (2025-01-31) Grok-3-mini-beta Grok-3-beta o4-mini (2025-04-16) GPT-4.1 Claude 3.7 Sonnet Claude 3.7 Sonnet (w/ thinking) o3 o1 61.00 63.75 69.75 70.00 67.75 71.50 70.75 71.50 77.75 75.75 17.77 12.20 17.24 23.87 27.32 26.79 30.77 27.06 25.99 35.54 20.42 25.99 37.40 37.93 45.09 48.01 57. 42.18 47.75 55.70 66.58 32.10 45.89 50.40 51.19 62.07 57.56 64.19 64.19 67.64 68.97 54.11 66.58 62.33 67.64 72.68 75.86 76.13 76.39 64.99 66.05 69.76 70.56 73.74 74.27 76.92 80.37 78.25 79.58 t n o & t o s e F & s F , t i A 26.49 15.68 20.54 28.65 22.16 28.65 30.27 29.73 29.19 41.62 29.19 33.51 48.11 50.81 51.35 56.22 62.16 50.81 52.43 65.95 68.65 39.46 54.05 58.38 62.70 67.03 61.08 69.19 69.19 72.97 70.81 64.32 67.57 72.97 71.35 72.43 76.22 80.54 81.62 72.43 75.14 77.30 76.76 81.08 81.62 77.84 82.16 78.92 80. 17.84 16.76 18.38 32.43 26.49 29.19 22.16 23.24 32.97 36.76 29.19 32.97 41.62 37.30 31.89 41.62 45.95 51.35 44.86 47.57 54.05 37.84 55.14 51.35 49.73 53.51 51.89 58.38 56.76 58.38 62.70 52.43 62.16 62.70 61.08 65.41 64.86 77.30 82.70 63.78 63.78 65.95 69.19 76.76 80.00 78.92 80.00 83.24 84. i c r & s , A , t s o e & i e n e P & F 19.33 17.65 26.05 21.01 25.21 31.93 25.21 23.53 35.29 35.29 23.53 35.29 47.06 49.58 42.02 47.06 57.14 57.98 54.62 57.98 63. 33.61 57.98 56.30 59.66 57.14 62.18 74.79 68.91 72.27 68.91 60.50 68.91 67.23 66.39 68.07 70.59 78.15 79.83 70.59 72.27 68.07 74.79 80.67 76.47 76.47 80.67 84.03 84.03 20.18 7.89 16.67 27.19 28.07 28.95 34.21 28.95 28.07 46.49 22.81 24.56 42.11 50.00 55.26 60.53 66.67 48.25 56.14 67.54 75. 35.09 50.00 58.77 62.28 60.53 68.42 72.81 72.81 71.05 72.81 60.53 64.04 69.30 71.93 73.68 78.95 79.82 81.58 66.67 75.44 73.68 69.30 79.82 77.19 78.07 79.82 85.09 81.58 15.38 16.67 11.54 21.79 28.21 23.08 25.64 25.64 32.05 42.31 20.51 29.49 38.46 33.33 47.44 46.15 56.41 46.15 50.00 52.56 51. 32.05 50.00 47.44 56.41 56.41 58.97 58.97 53.85 53.85 64.10 55.13 62.82 69.23 62.82 65.38 73.08 73.08 75.64 75.64 66.67 61.54 70.51 71.79 75.64 75.64 73.08 82.05 83.33 fi & t c , m n 20.59 5.88 14.71 14.71 26.47 26.47 17.65 20.59 17.65 41.18 26.47 29.41 38.24 50.00 44.12 50.00 58. 50.00 52.94 61.76 70.59 26.47 55.88 58.82 67.65 58.82 58.82 70.59 70.59 79.41 73.53 64.71 55.88 61.76 64.71 67.65 82.35 82.35 85.29 73.53 76.47 76.47 73.53 79.41 76.47 79.41 85.29 85.29 85.29 r e & n 31.58 15.79 10.53 10.53 10.53 31.58 15.79 15.79 26.32 42. 21.05 31.58 21.05 36.84 26.32 31.58 63.16 42.11 36.84 47.37 68.42 21.05 31.58 31.58 31.58 21.05 52.63 63.16 68.42 52.63 57.89 36.84 47.37 63.16 57.89 47.37 68.42 73.68 73.68 57.89 57.89 73.68 57.89 78.95 68.42 68.42 68.42 78.95 73.68 t i A 18.31 11.27 15.49 21.13 28.17 33.80 18.31 23.94 32.39 29.58 30.99 26.76 46.48 33.80 39.44 43.66 53.52 39.44 36.62 59.15 57.75 35.21 50.70 52.11 47.89 45.07 56.34 53.52 56.34 56.34 53.52 53.52 59.15 59.15 57.75 52.11 61.97 71.83 63.38 56.34 46.48 59.15 71.83 61.97 61.97 64.79 70.42 66.20 70. t e 20.71 11.43 11.43 25.71 23.57 32.86 40.00 30.00 22.86 40.71 21.43 22.14 40.00 52.14 57.14 58.57 65.71 50.71 58.57 61.43 73.57 28.57 47.14 58.57 60.71 62.86 65.00 68.57 69.29 71.43 74.29 50.71 60.71 66.43 69.29 71.43 75.00 75.00 77. 73.57 69.29 70.71 75.71 78.57 74.65 78.57 82.86 78.17 77.86 . 21.30 17.59 24.74 25.09 27.25 27.72 28.84 28.99 33.94 37.80 28.30 31.89 41.90 41.94 44.99 49.25 58.79 46.70 49.75 57.25 65.71 33.05 49.40 52.92 54.04 58.33 58.41 64.98 65.25 67.34 68. 56.17 62.93 65.64 67.49 69.54 74.49 77.58 78.51 67.03 67.84 71.47 72.90 75.80 75.86 76.88 79.36 79.92 81.14 r i l n e 20.00 14.81 17.41 25.56 26.67 28.15 28.52 36.67 32.59 39.63 21.11 28.15 44.81 41.48 49.26 49.63 67.78 44.81 52.59 63.33 70. 38.15 46.30 55.93 57.41 66.30 58.52 68.52 69.63 71.11 74.07 62.59 63.33 66.30 69.63 71.11 81.11 79.26 80.74 69.26 72.96 73.33 76.30 78.52 77.41 80.37 80.74 80.37 81.11 t t C 24.32 13.81 15.32 22.52 25.83 22.22 23.72 27.93 30.93 33.03 23.72 23.12 41.14 39.94 40.54 43.84 53. 46.25 48.05 51.95 60.36 27.03 46.25 51.95 47.45 53.75 54.65 60.66 62.46 68.17 66.97 52.85 63.06 63.66 65.77 66.37 71.47 78.38 79.58 63.66 63.96 69.07 68.47 76.28 72.07 76.28 79.88 77.78 81.98 i a 24.43 11.07 15.27 20.23 29.77 29.39 32.06 32.82 33.21 40. 24.05 30.53 42.37 38.55 46.56 56.11 64.89 46.95 49.62 61.45 72.90 35.50 55.34 59.54 58.02 59.54 64.50 72.52 70.23 74.05 75.57 59.92 64.89 72.52 77.10 82.06 85.11 83.97 85.11 74.81 73.66 83.97 83.97 82.06 85.11 83.97 87.02 85.50 90.84 Table 7: The break down results for 14 categories in KMMLU-REDUX. The gray-shaded models are the densereasoning models. The scores with are biased as these models are used for the dataset filtration (Section 2.2.1). Domain Law Tax & Accounting Value Estimation Medical e c i u Names of KNPLs Open-weight Models < 5B < 10B < 20B < 32B > 70B Llama 3.2 3B Instruct DeepSeek-R1-Distill-Qwen-1.5B 15.66 27.27 HyperCLOVAX-SEED-Text-Instruct-1.5B 24.24 Qwen3-1.7B 19.19 EXAONE Deep 2.4B 16.67 19.19 Gemma 3 4B IT 23.23 20.20 28.28 25.25 Qwen 2.5 3B Instruct EXAONE 3.5 2.4B Instruct Qwen3-1.7B (w/ thinking) Kanana Nano 2.1B Instruct Llama 3.1 8B Instruct Ko-R1-7B-v2. Aya Expanse 8B 20.20 28.28 40.59 EXAONE Deep 7.8B 26.26 28.79 Qwen3-8B 29.29 27.78 Qwen3-8B (w/ thinking) EXAONE 3.5 7.8B Instruct Phi-4 (14B) 33.33 Gemma 3 12B IT 28.79 Qwen3-14B 32.83 30.81 Qwen3-14B (w/ thinking) EXAONE 3.5 32B Instruct Mistral Small 3.1 Instruct (24B) Aya Expanse 32B 24.75 33.33 27.27 Gemma 3 27B IT 29.80 Qwen3-30B-A3B 31.31 EXAONE Deep 32B 30.30 Qwen3-32B 33.33 35.35 33.33 QwQ 32B 35.35 Qwen3-30B-A3B (w/ thinking) Qwen3-32B (w/ thinking) 32.83 Llama 3.3 70B Instruct 41.92 C4AI Command (111B) 35.86 Llama-4-Scout-17B-16E-Instruct DeepSeek V3 (671B) 38.89 Qwen3-235B-A22B 38.38 38.38 43.43 45.45 Llama-4-Maverick-17B-128E-Instruct Qwen3-235B-A22B (w/ thinking) DeepSeek R1 (671B) Closed Models GPT-4.1 mini (2024-04-14) o3-mini (2025-01-31) Grok-3-mini-beta Grok-3-beta o4-mini (2025-04-16) GPT-4.1 o3 Claude 3.7 Sonnet Claude 3.7 Sonnet (w/ thinking) o1 38.38 38.38 37.37 42.42 37.37 47.47 45.96 55.56 50.00 54.55 r A a l 24.27 25.94 30.13 30.96 31.38 30.13 32.22 35.15 32.22 31.8 30.54 30.96 38.33 38.91 38.49 46.03 49.37 41.00 40.59 39.75 48. 24.27 43.10 43.51 44.35 47.28 48.54 53.14 50.21 55.23 55.65 46.44 51.46 53.14 56.49 54.81 62.34 56.90 61.51 56.90 51.46 57.74 59.00 62.34 66.95 71.13 73.22 77.41 71.55 ) ( t c i P 23.56 16.35 25.48 21.63 25.00 23.08 27.4 21.63 33.17 25.48 23.56 21.15 35.22 35.10 32.21 33.17 48. 37.50 34.62 48.56 58.65 20.67 34.62 39.90 38.46 45.19 45.67 57.69 57.21 55.29 60.58 42.31 45.67 47.12 50.00 55.77 61.06 69.23 66.83 54.81 60.10 67.79 63.94 69.23 66.83 73.56 68.27 78.85 75.48 r A t 25.69 20.18 26.61 32.11 36.70 22.02 26.61 24.77 28.44 34. 22.02 23.85 39.8 42.20 35.78 35.78 41.28 36.70 39.45 47.71 47.71 32.11 38.53 39.45 33.94 35.78 46.79 43.12 45.87 43.12 44.04 42.20 49.54 50.46 48.62 45.87 55.96 57.80 57.80 56.88 47.71 48.62 55.96 49.54 63.30 57.80 63.30 74.31 65.14 w 13.33 24.67 24.67 20.67 21.33 24.67 26.67 25.33 24.67 25.33 20.00 21.33 28.85 28.00 28.00 34.0 36.0 34.00 23.33 30.67 36.67 18.67 26.67 33.33 31.33 26.00 36.00 35.33 37.33 34.67 39.33 41.33 34.00 34.00 36.00 39.33 41.33 42.67 38.00 32.00 38.67 34.67 41.33 46.0 50.00 41.33 53.33 56.00 49. ) ( s A m 20.00 23.33 41.67 30.0 37.50 28.33 29.17 35.00 35.00 34.17 33.33 33.33 29.86 39.17 40.00 43.33 50.83 44.17 39.17 49.17 47.50 34.17 52.50 47.50 43.33 49.17 55.00 53.33 54.17 57.50 56.67 54.17 54.17 54.17 52.50 60.0 69.17 59.17 68. 50.00 52.50 54.17 61.67 59.17 66.67 65.83 77.50 70.00 78.33 t c a 18.07 23.95 29.83 21.01 23.53 26.47 26.47 23.11 21.43 25.63 18.49 23.53 33.94 31.09 31.09 35.71 44.12 37.82 34.87 38.24 51.26 21.01 32.35 38.66 43.28 35.71 39.92 45.80 50.0 47.06 55. 38.66 45.80 43.70 42.44 49.58 56.72 61.76 60.50 48.32 47.06 49.58 57.98 55.04 59.24 61.76 64.29 70.17 67.23 o m u 18.24 23.9 30.19 23.9 32.08 26.42 27.67 30.19 27.67 29.56 37.11 33.33 26.47 38.99 37.74 34.59 49.69 42.77 42.14 42.14 52. 27.67 42.77 42.77 40.88 44.03 46.54 53.46 48.43 54.09 57.23 49.69 46.54 47.80 47.80 52.83 56.60 61.64 69.18 50.31 50.31 65.41 62.89 66.67 67.30 70.44 67.92 75.47 76.73 c M r o c 20.49 23.26 30.90 30.56 30.21 42.01 37.15 37.85 36.81 44.79 36.11 35.42 29.29 42.36 44.10 50.69 61. 45.49 52.08 57.64 65.28 32.64 48.96 52.08 52.08 56.60 53.47 61.46 70.49 70.14 71.53 51.39 61.46 61.46 71.53 64.24 75.00 71.88 79.51 67.36 64.24 66.32 70.49 76.74 80.21 77.43 70.83 76.39 83.33 i p 21.94 19.39 25.00 29.08 30.61 26.53 23.47 27.04 31.63 36. 22.45 26.02 42.21 40.31 33.16 40.82 53.06 43.37 35.71 50.00 54.59 23.98 41.33 41.33 44.39 47.96 48.98 48.98 54.08 55.61 64.29 43.37 52.04 47.45 53.06 57.65 66.84 62.76 64.80 55.10 57.65 64.80 66.84 67.35 68.37 71.94 73.47 81.12 78.06 c a 23.25 36.53 36.16 46.49 52.77 52.40 49.08 47.97 53.14 61.99 50.18 54.61 46.24 64.94 67.53 71.96 83.03 72.69 71.59 81.18 87.82 53.14 70.11 79.34 81.18 83.03 78.97 85.24 88.19 87.08 88.93 85.98 83.39 81.92 87.82 86.72 89.67 89.67 92.99 90.77 88.93 91.14 89.30 93.36 94.10 92.99 94.46 93.36 94. i D 18.49 26.45 27.31 30.32 29.25 31.61 36.56 35.05 35.91 38.06 29.25 33.76 48.67 44.73 43.44 46.02 54.19 41.29 49.46 55.91 61.94 31.83 49.89 49.89 58.49 53.98 52.26 60.86 59.78 66.88 66.24 56.77 57.63 68.17 64.95 70.97 76.34 77.42 81. 72.47 76.56 73.98 77.85 82.15 82.80 87.96 81.51 84.09 88.39 c a r 23.77 30.74 35.66 46.72 44.67 44.67 43.03 52.46 43.44 59.43 46.31 50.82 64.21 53.69 56.56 73.36 76.23 57.38 62.30 75.0 80.74 45.49 61.89 68.03 73.36 77.87 65.98 84.84 84.43 81.56 84. 71.72 79.10 85.25 87.70 86.07 90.98 89.34 94.67 81.97 80.33 84.84 91.80 89.75 92.62 91.39 92.21 93.03 95.49 c h 18.67 28.67 33.33 33.33 32.00 38.67 36.00 34.67 38.67 44.67 42.00 42.00 30.67 52.67 50.67 59.33 75.33 51.33 68.00 75.33 82. 38.67 66.00 72.00 72.67 72.00 72.67 84.00 84.67 88.67 88.67 74.00 80.67 82.67 84.67 84.67 90.67 88.00 92.67 90.00 91.33 90.0 94.67 92.0 94.67 94.67 93.33 92.67 96.67 . 20.55 25.53 30.13 30.42 32.08 32.60 32.86 33.27 34.49 38.27 31.65 33.81 38.70 41.53 41.71 46.92 55. 45.32 45.82 53.02 59.48 31.26 46.71 49.49 51.03 52.33 52.33 58.86 60.52 61.14 63.94 53.24 57.48 58.14 60.77 62.12 68.10 68.22 71.33 62.18 62.05 65.08 68.37 69.65 72.99 73.60 74.52 77.70 78.09 N s o # 0/14 0/14 0/14 0/14 0/14 0/14 0/14 0/14 0/14 1/ 0/14 0/14 1/14 0/14 0/14 1/14 3/14 1/14 2/14 3/14 3/14 0/14 2/14 3/14 2/14 3/14 1/14 3/14 3/14 3/14 5/14 3/14 3/14 4/14 4/14 4/14 4/14 6/14 7/14 4/14 3/14 5/14 7/14 6/14 10/14 9/14 10/14 12/14 10/14 Table 8: The break down results for all KNPLs in KMMLU-PRO. The gray-shaded models are the dense-reasoning models. The blue scores indicate that the LLM obtain the license. The details for the pass criteria of each license are described in Appendix D.3."
        },
        {
            "title": "Year",
            "content": "2005 2008 2010 2014 2016 2018 2020"
        },
        {
            "title": "Tests",
            "content": "Master Craftsman Construction Equipment Maintenance, Master Craftsman Building General Work, Master Craftsman Precious Metal Processing, Master Craftsman Confectionary Making, Master Craftsman Casting, Master Craftsman Sheet-Metal & Boiler Making Master Craftsman Architectural Carpentering, Master Craftsman Surface Treatment"
        },
        {
            "title": "Master Craftsman Steel Making",
            "content": "Master Craftsman Metal Material, Master Craftman Metal Mould, Master Craftsman Cook Master Craftsman Gas, Master Craftsman Machinery Maintenance, Master Craftsman Plumbing, Master Craftsman Rolling, Master Craftsman Energy Management, Master Craftsman Hazardous Material, Master Craftsman Motor Vehicles Maintenance, Master Craftsman Electricity, Master Craftman Electronics, Master Craftsman Iron Making Engineer Radio Electronic Communication, Engineer Floral Design Engineer Construction Equipment, Engineer Machinery Design, Engineer Agricultural Health and Safety, Engineer Leak Nondestructive Testing, Engineer Radiation Nondestructive Testing, Engineer Biology ClassificationAnimal, Engineer Aquaculture, Engineer Visual Communication Design, Engineer Eddy Current Nondestructive Testing, Engineer Welding, Engineer Biomedical, Engineer Magnetic Nondestructive Testing, Engineer Electric Railway, Engineer Computer, Engineer Concrete, Engineer Explosives Handling Engineer Gas, Engineer Construction Safety, Engineer Construction Material Testing, Engineer Architecture, Engineer Building Facilities, Engineer Air-Conditioning Refrigerating Machinery, Engineer Transportation, Engineer Metal, Engineer Meteorology, Engineer Air Pollution Environmental, Engineer Urban Planning, Engineer Bioprocess, Engineer Forest, Engineer Industrial Safety, Engineer Industrial Hygiene Management, Engineer Plant Maintenance, Engineer Fire Protection SystemMechanical, Engineer Fire Protection SystemElectrical, Engineer Noise & Vibration, Engineer Water Pollution Environmental, Engineer Elevator, Engineer Plant Protection, Engineer Food Processing Safety, New and Renewable Energy Equipment (Photovoltaic) Engineer, Engineer Interior Architecture, Engineer Energy Management, Engineer Greenhouse Gas Management, Engineer Organic Agriculture, Engineer Ergonomics, Engineer General Machinery, Engineer Motor Vehicles Maintenance, Engineer in Nature Environment and Ecological Restoration, Engineer Electric Work, Engineer Electricity, Engineer Computer System Application, Engineer Electronics, Engineer Information Processing, Engineer Landscape Architecture, Engineer Seeds, Engineer Cadastral Surveying, Engineer Railroad Signal Apparatus, Engineer Ultrasonic Nondestructive Testing, Engineer Livestock, Engineer Surveying Geo-Spatial Information, Engineer Penetrante Nondestructive Testing, Engineer Colorist, Engineer Civil Engineering, Engineer Soil Environment, Master Craftsman Telecommunication Apparatus, Engineer Wastes Treatment, Engineer Quality Management, Engineer Ocean Environment, Engineer Chemical Industry, Fire Investigation & Evaluation Engineer, Engineer Chemical Analysis Engineer Radio Telecommunication Equipment, Engineer Broadcasting Communication, Engineer Information Communication Table 9: Redux Years and National Qualification Test Additions"
        },
        {
            "title": "Examples",
            "content": "Ill-posed Question Category: Political science and sociology A, B에 대한 설명으로 옳은 것만을 <보기>에서 고르면? What is the correct explanation about A, in <Reference>?"
        },
        {
            "title": "Bad Clarity",
            "content": "Category: Ecology 산복수로에서 쌓기공작물의 높이가 3m이고, 수로깊이가 1m일 때 수로받이의 근사적 길이는? (문제 오류로 현재 복원중입니다. 보기 내용을 아시는 분들께서는 오류 신고를 통하여 보기 작성 부탁 드립니다. 정답은 3번입니다.) What is the approximate length of the culvert if the pile is 3 meters high and the channel is 1 meter deep? (This is currently being restored due to question error. If you know the referebce, please report the error. The correct answer is 3.) Category: Math 다항식 x2017-1을 x2-x로 나누었을 때의 나머지를 R(x)라 할 때, R(2017)의 값은? If the remainder of the polynomial x2017-1 divided by x2-x is called R(x), what is the value of R(2017)? Category: Education 정신분석 상담과 행동주의 상담의 공통점에 해당하는 것은? A. 상담과정에서 과거 경험보다 미래 경험을 중시한 다 . B. 상 담 기 법 보 다 는 상 담 자 의 인 간 적 자 질 과 진 솔 한 태 도 를 중 시 한 다 . C. 인간의 행동을 인과적 관계로 해석하는 결정론적 관점을 가진 다 . D. 비합리적 신념을 인식하고 수정하는 논박 과정을 중시한 다 . Which is common feature of psychoanalytic counseling and behavioral counseling? A. Emphasizes future experiences over past experiences in the counseling process. B. Prioritizes the counselors human qualities and sincerity over counseling techniques. C. Interprets human behavior through deterministic perspective based on causal relationships. D. Focuses on the disputation process to recognize and modify irrational beliefs. Table 10: Examples of error types in KMMLU. Each example demonstrates specific issue that impacts the reliability of the benchmark. Gray text represents translation of the examples in English"
        }
    ],
    "affiliations": [
        "LG AI Research",
        "OnelineAI"
    ]
}