{
    "paper_title": "UltraViCo: Breaking Extrapolation Limits in Video Diffusion Transformers",
    "authors": [
        "Min Zhao",
        "Hongzhou Zhu",
        "Yingze Wang",
        "Bokai Yan",
        "Jintao Zhang",
        "Guande He",
        "Ling Yang",
        "Chongxuan Li",
        "Jun Zhu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Despite advances, video diffusion transformers still struggle to generalize beyond their training length, a challenge we term video length extrapolation. We identify two failure modes: model-specific periodic content repetition and a universal quality degradation. Prior works attempt to solve repetition via positional encodings, overlooking quality degradation and achieving only limited extrapolation. In this paper, we revisit this challenge from a more fundamental view: attention maps, which directly govern how context influences outputs. We identify that both failure modes arise from a unified cause: attention dispersion, where tokens beyond the training window dilute learned attention patterns. This leads to quality degradation and repetition emerges as a special case when this dispersion becomes structured into periodic attention patterns, induced by harmonic properties of positional encodings. Building on this insight, we propose UltraViCo, a training-free, plug-and-play method that suppresses attention for tokens beyond the training window via a constant decay factor. By jointly addressing both failure modes, we outperform a broad set of baselines largely across models and extrapolation ratios, pushing the extrapolation limit from 2x to 4x. Remarkably, it improves Dynamic Degree and Imaging Quality by 233% and 40.5% over the previous best method at 4x extrapolation. Furthermore, our method generalizes seamlessly to downstream tasks such as controllable video synthesis and editing."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 2 ] . [ 1 3 2 1 0 2 . 1 1 5 2 : r Preprint. ULTRAVICO: BREAKING EXTRAPOLATION LIMITS IN VIDEO DIFFUSION TRANSFORMERS Min Zhao1,2 , Hongzhou Zhu1,2 , Yingze Wang1, Bokai Yan3, Jintao Zhang1,2, Guande He4, Ling Yang5, Chongxuan Li3, Jun Zhu1,2 1Dept. of Comp. Sci. & Tech., BNRist Center, THU-Bosch ML Center, Tsinghua University. 2ShengShu. 3Gaoling School of Artificial Intelligence, Renmin University of China. 4The University of Texas at Austin. 5 Princeton University. gracezhao1997@gmail.com, zhuhz22@mails.tsinghua.edu.cn"
        },
        {
            "title": "ABSTRACT",
            "content": "Despite advances, video diffusion transformers still struggle to generalize beyond their training length, challenge we term video length extrapolation. We identify two failure modes: model-specific periodic content repetition and universal quality degradation. Prior works attempt to solve repetition via positional encodings, overlooking quality degradation and achieving only limited extrapolation. In this paper, we revisit this challenge from more fundamental viewattention maps, which directly govern how context influences outputs. We identify that both failure modes arise from unified cause: attention dispersion, where tokens beyond the training window dilute learned attention patterns. This leads to quality degradation and repetition emerges as special case when this dispersion becomes structured into periodic attention patterns, induced by harmonic properties of positional encodings. Building on this insight, we propose UltraViCo, training-free, plug-and-play method that suppresses attention for tokens beyond the training window via constant decay factor. By jointly addressing both failure modes, we outperform broad set of baselines largely across models and extrapolation ratios, pushing the extrapolation limit from 2 to 4. Remarkably, it improves Dynamic Degree and Imaging Quality by 233% and 40.5% over the previous best method at 4 extrapolation. Furthermore, our method generalizes seamlessly to downstream tasks such as controllable video synthesis and editing. Project page is available at https://thu-ml.github.io/UltraViCo.github.io/."
        },
        {
            "title": "INTRODUCTION",
            "content": "Building upon the expressive power of diffusion transformers (DiTs) (Bao et al., 2023; Peebles & Xie, 2023), recent advances in text-to-video (T2V) generation Bao et al. (2024); Zheng et al. (2024b); Brooks et al. (2024); Wan et al. (2025); Kong et al. (2024); Hong et al. (2022) have enabled models to synthesize high-fidelity videos. However, these models are typically trained on fixed maximum sequence length (e.g., 5 seconds Wan et al. (2025); Kong et al. (2024); Hong et al. (2022)) and struggle to generate videos beyond their training length, task we term video length extrapolation, which is critical for practical applications. To investigate the core challenges of this task, we conduct experiments on range of models and identify two failure modes: (i) model-specific periodic content repetition, where short clips loop indefinitely in certain models; and (ii) universal quality degradation, manifested as blurred spatial details and frozen temporal dynamics across all models. Both failures become increasingly severe as the extrapolation length grows. Prior work, such as RIFLEx (Zhao et al., 2025), tackles repetition from the perspective of positional encodings, while overlooking quality degradation and therefore achieving limited extrapolation. We contend, however, that positional encodings play only an indirect role by perturbing queries and keys to influence attention. In contrast, attention itselfdirectly aggregating contextual information to generate outputsoffers more fundamental view. Equal contribution. 1 Preprint. (a) Extending T2V models up to 4, where existing method yields nearly static, low-quality videos. (b) Generalization to downstream tasks at 3. See more tasks in Appendix C.4. Figure 1: Visual results. UltraViCo achieves significant extrapolation improvement on (a) T2V models and (b) downstream tasks. See prompts and videos in supplementary materials. Therefore, we revisit extrapolation failures through the lens of attention maps. Our systematic analysis of attention maps shows that both failure modes arise from unified mechanism: attention dispersion. This occurs when new tokens beyond the training length dilute the learned attention patterns. This leads to quality degradation and repetition arises as special case when dispersion becomes organized into periodic attention patterns. Specifically, this happens when positional encoding frequencies form harmonics, enabling the largest-amplitude frequency and its harmonics to accumulate amplitude and contribute substantially to the overall amplitude. Building on this unified view, we propose Ultra-extrapolated Video via Attention Concentration (UltraViCo), plug-and-play method that suppresses attention for tokens beyond the training window with constant decay factor. This adjustment reallocates attention to reliable in-window context while naturally breaking periodic patterns, thus simultaneously addressing both failure modes. Notably, standard attention implementations encounter out-of-memory errors when modifying logits for long video sequences. We therefore develop memory-efficient CUDA kernel that enables scalable applications on large video models. To validate our approach, we conduct comprehensive evaluations on various T2V models (Kong et al., 2024; Yang et al., 2024; Wan et al., 2025) and extrapolation ratios, against large family of baselines (Chen et al., 2023b; bloc97, 2023; Zhuo et al., 2024; Peng et al., 2023; Zhao et al., 2025). Experiments demonstrate that our method consistently surpasses all baselines in all settings by simultaneously addressing both failure modes. Notably, while prior methods collapse beyond 3 extrapolation and yield static videos, ours maintains fluid motion, effectively extending the practical limit from 2 to 4. Remarkably, it improves Dynamic Degree and Imaging Quality by 233% and Preprint. 40.5% over the previous best method at 4 extrapolation. Beyond this, our method also generalizes seamlessly to downstream tasks such as various controllable video synthesis and editing. HunyuanVideo Wan Video of 129 frames Video of 81 frames (a) Periodic content repetition and quality degradation. (b) Quality degradation. Normal length 3 extra. Variable extra. (c) Both quality and repetition worsen as the extrapolation grows from 1 to 5. Figure 2: Failure modes of video length extrapolation. Some models exhibit periodic content repetition, while quality degradation occurs universally. Both failure modes intensify with longer extrapolations. extra. denotes extrapolation. See Appendix C.1 for additional models."
        },
        {
            "title": "2 PRELIMINARY",
            "content": "Attention mechanism with rotary position embedding. Modern video diffusion models are largely built on DiTs whose core is the attention mechanism (Vaswani et al., 2017). The input video is patched into tokens, each projected into queries, keys, and values. To encode the position information, DiTs mainly adopt Rotary Position Embedding (RoPE) (Su et al., 2024), which injects position into queries and keys through complex rotations. Concretely, for each query or key vector RD at position t, RoPE maps it to RD as RoPE(x, t)i = Ri(t) (cid:21) (cid:20) x2i x2i+1 , Ri(t) = (cid:20)cos(ϕit) sin(ϕit) cos(ϕit) sin(ϕit) (cid:21) , {0, . . . , D/2 1}. (1) Here, each frequency ϕi depends exponentially on and is used to encode the (2i, 2i+1) components of x. After RoPE, the queries and keys form matrices RLD and RLD. Their interaction yields the attention logits RLL, which are normalized by the softmax function to obtain the attention scores RLL. These scores are then applied to the value matrix RLD to produce the output RLD : = QK , = softmax( ), = . (2) For videos with temporal and spatial axes, Multimodal RoPE (M-RoPE) (Wang et al., 2024a) partitions the dimension = dT + dH + dW and encodes each subspace separately. Since we focus on temporal extrapolation, we consider only the temporal axis and denote dT as for simplicity (see details in Appendix B.2). Problem setting: video length extrapolation. Despite advances, DiT-based video generation models struggle to produce videos longer than their training duration. This task, known as video length extrapolation (Zhao et al., 2025), aims to adapt pre-trained model to generate high-quality videos of sequence length that exceeds its training length L, with the extrapolation ratio defined as = L/L > 1. Notably, video length extrapolation targets the models intrinsic ability to generate longer sequences in single forward generation, which is orthogonal to prior methods (Qiu et al., 2023; Wang et al., 2023; Kim et al., 2024; Wang et al., 2024c; Lu et al., 2024) that rely on inference-time modifications. See Appendix for more related work. Preprint."
        },
        {
            "title": "3.1 FAILURE MODES OF VIDEO LENGTH EXTRAPOLATION",
            "content": "In this section, we investigate the core challenges of video length extrapolation on range of SOTA video diffusion transformers, including Wan (Wan et al., 2025), HunyuanVideo (Kong et al., 2024), and CogVideoX (Yang et al., 2024). Qualitative results in Fig.2a and Fig.2b reveal two distinct failure modes. The first is periodic content repetition, which occurs in certain models such as HunyuanVideo and CogVideoX. The second is universal quality degradation, characterized by compromised spatial fidelity and temporal dynamics across all models. To further investigate their trends across extrapolation lengths, we perform quantitative analysis on 10 prompts using metrics including Imaging Quality (Huang et al., 2024), Dynamic Degree (Huang et al., 2024), and Repetition Count. Fig. 2c confirms that both failures become more severe as the extrapolation factor increases. These findings raise three critical questions: First, why does periodic content repetition only manifest in specific models? Second, what is the underlying cause of the universal quality degradation? Most importantly, is there unified cause behind these two seemingly independent failure modes? Existing work such as RIFLEx addresses only content repetition, neglecting quality degradation, which limits both model generalization and extrapolation capacity. While RIFLEx attributes repetition to positional encoding periodicity, we argue that positional encodings play only an indirect role by modulating queries and keys. Instead, as Eq. (2) shows, the attention map itself is fundamental, since it directly determines how context is aggregated. This motivates us to revisit extrapolation failures through attention analysis. 3.2 ATTENTION ANALYSIS OF THE CAUSE In this section, we first focus on the specific issue of periodic content repetition (Sec. 3.2.1). Through an in-depth attention analysis of its underlying mechanism, we find, surprisingly, that the solution designed to resolve repetition also improves video quality. This key finding then allows us to understand the cause of the more universal problem of quality degradation (Sec. 3.2.2), and ultimately reveals the intrinsic connection between the two failure modes. 3.2.1 THE CAUSE OF CONTENT REPETITION: PERIODIC ATTENTION PATTERNS Periodic attention induces output repetition. We analyze the cause of content repetition by inspecting the attention map RLL during 4 extrapolation, where is the extrapolated sequence length (i.e., video features flattened into 1D sequence). The entry at row i, column of , denoted Pij, is the attention score from query to key j. As shown in Fig. 3a, the attention map of HunyuanVideo reveals two properties that jointly induce periodic outputs. First, the map exhibits distinct row-wise periodicity. Specifically, for any query at position i, its attention scores to key positions and +T are nearly identical: Pi,j Pi,j+T , where corresponds to the observed repetition period in Sec. 3.1. As indicated in Fig. 3a, the blue and purple circles highlight nearly equal scores. Second, the map shows relative positional invariance: querykey pairs with the same relative displacement yield approximately equal scores, Pi,j Pi+p,j+p. This RoPE-induced property appears as uniform values along diagonals and subdiagonals; for example, when = , the scores marked by the blue and green circles are nearly identical. Combining these properties, we can derive that entire query rows also repeat periodically: Pi+T,j Pi,j, as shown by the green and purple circles. Thus, rows and + retrieve nearly the same weighted information from the value , leading to periodic outputs (see Appendix B.1 for details): Oi+T = L1 (cid:88) j=0 Pi+T,jVj L1 (cid:88) j=0 Pi,jVj = Oi. (3) This periodicity is directly reflected in repeated content in pixel space. Larger extrapolation ratios traverse more periods, thus increasing repetition counts, which is consistent with our observations in 4 Preprint. Model Attention maps Statistical row-wise attention analysis (a) Periodic attention: Pi,j Pi,j+T (b) Harmonic RoPE frequencies (ϕi/ϕN 1 N+) amplify the largest-amplitude frequency and its harmonics (dashed line), inducing periodic composite attention. Hun. Wan (c) Non-periodic attention: Pi,j = Pi,j+T (d) Inharmonic RoPE frequencies (ϕi/ϕN 1 / N+) disperse spectrum (dashed line), yielding non-periodicity in the final composite attention. Figure 3: Periodic attention patterns as cause of content repetition. Left: unlike Wan, HunyuanVideo exhibits row-wise periodic attention during 4 extrapolation, causing repeated outputs. Right: statistical row-wise attention can be expressed as linear combination of trigonometric functions of RoPE frequencies, whose properties govern this periodicity. Hun. denotes HunyuanVideo. Sec. 3.1. By contrast, the attention map of Wan (Fig. 3c) does not display such row-wise periodicity, and accordingly its outputs remain free of repetition. Origin of periodic attention patterns. Next, we show that such model-specific row-wise periodicity originates from the RoPE frequencies. To reveal the core row-wise attention structure from noise, we construct statistical row attention pattern S(t), which captures the relation between query and keys at the same spatial location but latent frames apart. This is achieved by taking the expectation of the pre-softmax attention logits across all layers, heads, and query positions. As derived in Appendix B.3 (based on Eq. (2)), this quantity admits the following trigonometric decomposition: S(t) = d/21 (cid:88) i= ai cos(ϕit + bi) + C, (4) i=0 are the RoPE frequencies defined in Sec. 2, and {ai}d/21 where {ϕi}d/21 , are constants determined by the statistics of queries and keys from models, with bi typically close to zero. Visualizations of these frequency components for HunyuanVideo and Wan highlight crucial difference (Fig. 3b,d, left). The periodicity of such superposition is decided by the frequency relationships, as formalized in Proposition 1. Proposition 1 (Period and Amplitude of Harmonics). For function (t) = (cid:80)N 1 i=0 ai cos(ϕit), where ai > 0, ϕi > 0 and mini ϕi = ϕN 1, if and only if i, ϕi/ϕN 1 N+ (i.e., they form set of harmonics), (t) is periodic with period TN 1 = 2π . In this case, maxt (t) = ϕN 1 (cid:80)N 1 i=0 ai, whenever = mTN 1, (i.e., whenever is at harmonic alignment positions). , {bi}d/21 i= i=0 We find that HunyuanVideos frequencies satisfy this harmonic condition in Proposition 1, allowing amplitude accumulation of the largest-amplitude frequency ϕ3 and its harmonics (i < 3) at harmonic alignment positions mT (dashed line in Fig. 3b), where Z. This yields dominant component that contributes 79.6% of the total amplitude, producing strongly periodic composite attention pattern (Fig. 3b, right). similar harmonic alignment is also observed in CogVideoX (Appendix B.6). In contrast, Wans frequencies are not harmonically aligned, resulting in dispersed spectrum where no frequency dominates (largest 31.6%), and thus no clear periodicity emerges (Fig. 3d). Notably, 5 Preprint. while the strict periodicity of HunyuanVideo is determined by the lowest frequency, its small amplitude and long period make it negligible; the observed periodicity is effectively governed by the dominant frequency (see Appendix B.6). In summary, our analysis establishes the causal chain: RoPE-induced frequency harmonics lead to periodic attention patterns, which in turn produce periodic output features and ultimately manifest as content repetition. To validate this, we mask tokens at harmonic alignment positions mT . Breaking these constructive interference points disrupts periodic attention and, as shown in Fig. 4a, effectively mitigates repetition. Model Generated videos: baseline vs. intervention Attention maps: baseline vs. intervention (a) Non-repetition and improved video quality after intervention (b) Attention focused centrally after intervention Hun. Wan (c) Improved video quality after intervention (d) Attention focused centrally after intervention Figure 4: Fixing repetition reveals attention dispersion as the fundamental cause. Left: our intervention, initially targeting repetition, surprisingly enhances video quality in both models. Right: the shared mechanism is revealed, where the intervention refocuses diffuse baseline attention toward the central training window. This suggests attention dispersion as the unified cause. 3.2.2 THE CAUSE OF QUALITY DEGRADATION: ATTENTION DISPERSION Surprisingly, we find the above repetition-resolving intervention also improves video quality across both models (Fig. 4a, c). This finding suggests more profound hypothesis: content repetition and quality degradation may arise from shared, fundamental underlying mechanism. comparison of attention maps shows our intervention consistently concentrates the initially diffuse attention (Fig. 4b, d). This occurs because masking the harmonic peaks forces softmax renormalization, which sharpens the attention distribution by proportionally increasing the remaining scores. To further identify where this sharpened focus is most beneficial, we systematically masked different attention regions and found that concentrating attention within the original central training window yielded the strongest improvements (see details in Appendix B.7). This leads us to hypothesize that attention dispersion is the underlying issue. New tokens during extrapolation dilute the learned attention patterns within the original training window. This dispersion has two detrimental effects. Spatially, the model needs to consider far-away extrapolated frames, which makes it difficult to focus on fine details and results in visual blurriness. Temporally, taking these distant frames into account mixes local motion with unrelated movements, causing the video to appear static and unnatural. These effects are consistent with the quality degradation observed in Sec. 3.1. To validate this hypothesis, we conduct controlled experiment where we progressively mask attention scores for tokens outside the training window, thereby forcing the attention to concentrate centrally. The results, presented in Fig. 5, demonstrate clear positive correlation: more concentrated attention (i.e., by increasing the proportion of masked out-of-window scores) consistently improves both the visual quality and motion dynamics of the generated video. This provides strong evidence that attention dispersion is the cause of quality degradation. Consequently, as the extrapolation ratio increases, attention becomes more dispersed, leading to worse quality, consistent with the observations in Sec. 3.1. unified view: periodic attention as case of attention dispersion. Building upon the above analysis, we can unify both failure modes under single perspective: attention dispersion is the fundamental cause of extrapolation failure, with periodic attention patterns representing special 6 Preprint. (a) Quantitative results. (b) Qualitative results. Figure 5: Validation of attention dispersion as the cause of quality degradation. Both (a) quantitative and (b) qualitative results show that video quality improves monotonically as the degree of attention central focusing (i.e., the masking ratio of out-of-window scores) increases. case. Specifically, when RoPE frequency contributes substantially to the overall amplitude (e.g., due to harmonic alignment), it induces strongly periodic attention pattern; otherwise, the model exhibits generic, non-periodic dispersion. 3.3 ULTRAVICO Building on the above unified view, we propose Ultra-extrapolated Video via Attention Concentration (UltraViCo), simple yet effective method that suppresses attention for tokens beyond the training window via decay factor, thereby restoring the models focusing ability. To achieve this, we introduce position-dependent decay factor λij applied to the original attention logits Sij, yielding the corrected attention ij: ij = λij Sij, where λij = (cid:26)1, α, if j L/2 or Sij < 0, otherwise, (5) where α < 1 is constant decay hyperparameter and is the training length. Here, λij is set to be 1 for all pairs within the training window, preserving the models core learned dynamics. For out-ofwindow tokens, only positive logits (Sij 0) are down-scaled because multiplying negative logits Sij < 0 by α < 1 can undesirably increase its value, while multiplying α > 1 or 1 for negative logits has negligible effect. We also experimented with various decay strategies, such as linear decay, but found the constant form is sufficient, indicating that the key is distinguishing in-window from out-of-window tokens rather than the decay shape itself (see Sec. 4.2 for details). However, in models showing periodic repetition (Sec. 3.2.1), harmonic alignment positions mT attract disproportionately high attention. Applying uniform small decay α would overly suppress all out-of-window context, harming temporal consistency. To address this, we apply stronger decay β < α specifically to these risky positions mT , while keeping α for other out-of-window tokens: λij = 1, β, α, if j L/2 or Sij < 0, else if (i, j) Prisk, otherwise, (6) where Prisk = { (i, j) mT γ mT + γ, Z, γ N+ } denotes the set of positions within γ frames around the harmonic alignment positions mT and β < α < 1. This targeted adjustment reallocates attention to reliable in-window context while eliminating spurious periodic patterns, allowing UltraViCo to mitigate both failure modes simultaneously. Efficient CUDA implementation. UltraViCo requires modifying attention logits, but standard PyTorch attention is infeasible for long sequences. At 3 extrapolation (200K tokens for Hunyuan7 Preprint. Table 1: Quantitative illustrative results on VBench for HunyuanVideo and Wan. For Wan, which does not exhibit content repetition, we omit the NoRepeat Score. Additional results for more extrapolation ratios and models are provided in Appendix C.3. Consist., Dyn., Qual., Over. and NoRe. denote Consistency, Dynamics, Quality, Overall and NoRepeat Score respectively. Normal. indicates the training length for reference. Method Consist. Dyn. Qual. Over. User Consist. NoRe. Dyn. Qual. Over. User Wan2.1-1.3B HunyuanVideo Normal. 0. 51 70.34 24.25 0.9786 71 69.31 26.81 PE PI NTK YaRN TASR RIFLEx Ours PE PI NTK YaRN TASR RIFLEx Ours 0.9419 0.9667 0.9437 0.9676 0.9434 0.9431 0.944 0.9415 0.9711 0.9477 0.9729 0.9495 0.9453 0.9484 6 7 3 5 6 5 46 11 12 11 7 9 10 47 3 extrapolation 3.82 4.69 4.40 4.71 4.47 4.90 1. 0.9795 0.9787 0.9802 0.9790 0.9807 0.9823 0.9465 4 extrapolation 3.75 4.87 4.24 4.57 4.72 4.84 1.01 0.9891 0.9885 0.9915 0.9877 0.9911 0.9906 0.9468 18.53 17.48 18.50 17.53 18.48 17.54 23.21 16.65 16.34 16.09 16.69 16.16 15.83 21. 56.28 52.16 57.73 53.46 57.41 53.79 62.43 55.25 50.44 55.37 51.16 55.18 51.05 59.36 53.17 90.23 84.80 88.74 80.74 73.97 100.0 31.41 70.93 72.39 62.87 51.28 52.84 99.87 16 1 24 0 22 17 62 14 0 10 1 14 11 51.85 46.30 53.11 47.05 51.95 50.57 65.00 47.12 42.19 50.01 41.37 46.81 41.02 66.54 21.62 21.29 22.14 21.42 22.02 21.22 26.45 17.61 17.83 18.92 18.53 18.47 16.47 24.52 3.96 4.91 3.74 5.05 4.65 4.67 1.02 3.70 4.82 4.23 5.03 4.51 4.69 1. Video), for instance, materializing 200K 200K attention mask consumes over 80GB of memory in bf16, causing an immediate out-of-memory error. To address this, we integrate UltraViCo into Triton-based FlashAttention (Dao et al., 2022) and SageAttention (Zhang et al., 2024b), where the online-softmax formulation avoids explicit mask construction. This yields scalable, memoryefficient computation, enabling UltraViCo on large video models."
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "4.1 SETUP Evaluation. We evaluate methods on three video diffusion models, including HunyuanVideo, Wan2.1-1.3B and CogVideoX-5B. Following RIFLEx, we use 100 prompts sampled from VBench (Huang et al., 2024). For quantitative evaluation, following RIFLEx, we adopt Imaging Quality (Quality), Dynamic Degree (Dynamics), and Overall Consistency (Overall) from VBench, along with the NoRepeat Score for models prone to content repetition. Notably, our NoRepeat Score is variant of that in RIFLEx, tailored for multiple-repetition (see Appendix C.2 for details). Finally, we conduct user study with 10 participants on 10 prompts, where users rank (User) the overall quality of videos across all methods. More details are provided in Appendix C.2. Implementation Details. The decay factor α is set to 0.9 for Wan and HunyuanVideo at 3 and 4 extrapolation. For HunyuanVideo, we set γ = 4 for all ratios, and β = 0.6 at 3 and 0.8 at 4. Our baseline configurations follow RIFLEx. Further details are provided in Appendix C.2. 4.2 RESULTS Performance comparison. We compare wide range of length extrapolation baselines on three SOTA models (Kong et al., 2024; Yang et al., 2024; Wan et al., 2025) across various extrapolation ratios, including PE (Zhao et al., 2025), PI (Chen et al., 2023b), NTK (bloc97, 2023), TASR (Zhuo et al., 2024), YaRN (Peng et al., 2023), and RIFLEx. Tab. 1 reports 3 and 4 results on HunyuanVideo and Wan, while Fig. 6 shows qualitative samples on HunyuanVideo. Results for additional ratios and models are provided in the Appendix C.3. As shown in Tab. 1, our method consistently outperforms all baselines across models and extrapolation ratios, simultaneously improving video quality and eliminating content repetition. Specifi8 Preprint. cally, PE suffers from severe repetition, reflected in low NoRepeat Scores. In contrast, our method achieves substantially higher scores, effectively removing repetition. Beyond repetition, unlike RIFLEx which targets only this issue, our method delivers broader gains in both visual quality and motion quality. For instance, it improves Dynamic Degree and Imaging Quality on HunyuanVideo by 233% and 40.5% over the previous best method at 4 extrapolation, respectively. Notably, on Wan beyond 3 extrapolation, while prior methods collapse and yield static videos (Dynamic Degree 12), our method restores fluid motion. By addressing both core failure modes, our method extends the extrapolation limit from 2 to 4. These improvements are further corroborated by user rankings (Tab. 1) and qualitative visualizations  (Fig. 6)  , which consistently confirm the superior quality of our generated videos over baselines. Figure 6: Qualitative results on HunyuanVideo. The baselines produce nearly static videos with poor visual quality, whereas our method achieves significantly better quality by addressing extrapolation failure modes. Additional qualitative results for other models are in Appendix C.4. Figure 7: Ablation studies. Top row: different decay strategies have minor impact, suggesting simple constant decay suffices. Bottom row: small α harms consistency while large α offers limited gains. An intermediate value (α = 0.9) enhances quality while preserving consistency. Ablation studies. We ablate the decay strategy and the decay factor α on Wan at 3 extrapolation. As shown in Fig. 7 (top), different decay strategies yield minor differences, indicating that simple constant decay suffices. As shown in Fig. 7 (bottom), strong decay harms consistency (i.e., the spare tire of the car disappears) while weak decay offers limited gains. An intermediate value (α = 0.9) enhances quality while preserving consistency. Further details are provided in Appendix C.2. sensitivity analysis for α and β  (Fig. 8)  shows stable trend: α 0.9 and β 0.6 improve visual quality and motion dynamics while keeping temporal consistency near baseline. We adopt α = 0.9 and β = 0.6 as robust defaults, with small adjustments possible (e.g., β = 0.8 for stronger consistency, α = 0.85 for better quality). Although larger α and β may introduce mild reduction 9 Preprint. in consistency, values above 0.94 remain visually stable, aligning with common long-video settings (e.g., Wans training-horizon consistency 0.95). See more metrics of α, β in Tab. 4, 5, 6, and Fig. 18. Connection with other long-video generation methods. UltraViCo aims to extend the effective training window of video diffusion transformers and is therefore orthogonal to existing long-video generation techniques such as FreeNoise (Qiu et al., 2023), FIFO-Diffusion (Kim et al., 2024), and sliding-window. As demonstrated in Table 2, enlarging the context window via UltraViCo consistently improves the long-term temporal consistency of these methods, without negatively affecting other performance. In Table 2, all methods follow the same evaluation setup (6 extrapolation for 30-second videos on Wan), where UltraViCo extends the base models training window by 3. Generalization to downstream tasks. Our method enhances the models inherent ability to handle longer sequences, making it naturally applicable to downstream tasks. As shown in Fig. 1, based on VACE (Jiang et al., 2025b), UltraViCo enables 3 extrapolation in controllable generation and video editing. See Appendix C.4 for additional results. Figure 8: Illustration of the hyperparameter sensitivity curve. (a) When α 0.9, motion dynamics improve while consistency stays stable; below 0.9, consistency drops sharply. (b) When β 0.6, dynamics remain high with comparable consistency; below 0.6, consistency degrades significantly. Table 2: Application of UltraViCo on existing long-video methods. Method Consistency Dynamics Quality Overall Sliding Window + UltraViCo FreeNoise + UltraViCo FIFO-Diffusion + UltraViCo 0.8478 0.9183 0.9243 0.9431 0.9131 0.9319 56 54 38 41 53 62.94 62.85 63.09 62.12 61.31 63.09 23.57 23.95 23.75 23.92 23.81 24. (a) Performance of the video-continuation baseline alone. (b) Illustration of combining UltraViCo with the video-continuation method. Figure 9: Application of UltraViCo to segment-wise long-video generation. (a) Wan2.2-TI2V uses only few ending frames, causing identity drift; (b) UltraViCo alleviates this issue. 10 Preprint."
        },
        {
            "title": "5 CONCLUSION",
            "content": "In this paper, we identify attention dispersion as the unified cause behind video length extrapolation failures. Based on this insight, we propose training-free method that suppresses attention scores for tokens beyond training length. Experiments show that it significantly improves video quality, extending the practical extrapolation limit from 2 to 4."
        },
        {
            "title": "ETHICS STATEMENT",
            "content": "This paper advances the field of video generation, while emphasizing the importance of responsible use to avoid potential negative societal impacts, such as the creation of misleading or harmful content."
        },
        {
            "title": "REPRODUCIBILITY STATEMENT",
            "content": "Our code and the prompts in the paper are included in the supplementary material, and the implementation details are described in Sec. 4.1."
        },
        {
            "title": "REFERENCES",
            "content": "Fan Bao, Shen Nie, Kaiwen Xue, Yue Cao, Chongxuan Li, Hang Su, and Jun Zhu. All are worth In Proceedings of the IEEE/CVF conference on words: vit backbone for diffusion models. computer vision and pattern recognition, pp. 2266922679, 2023. Fan Bao, Chendong Xiang, Gang Yue, Guande He, Hongzhou Zhu, Kaiwen Zheng, Min Zhao, Shilong Liu, Yaole Wang, and Jun Zhu. Vidu: highly consistent, dynamic and skilled text-tovideo generator with diffusion models. arXiv preprint arXiv:2405.04233, 2024. Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, Varun Jampani, and Robin Rombach. Stable video diffusion: Scaling latent video diffusion models to large datasets. NONE, 2023. bloc97. NTK-Aware Scaled RoPE allows LLaMA models to have extended (8k+) conURL text size without any fine-tuning and minimal perplexity degradation., 2023. https://www.reddit.com/r/LocalLLaMA/comments/14lz7j5/ntkaware_ scaled_rope_allows_llama_models_to_have/. Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, Clarence Ng, Ricky Wang, and Aditya Ramesh. Video generation models as world simulators. 2024. Minghong Cai, Xiaodong Cun, Xiaoyu Li, Wenze Liu, Zhaoyang Zhang, Yong Zhang, Ying Shan, and Xiangyu Yue. Ditctrl: Exploring attention control in multi-modal diffusion transformer for tuning-free multi-prompt longer video generation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 77637772, 2025. Boyuan Chen, Diego Martı Monso, Yilun Du, Max Simchowitz, Russ Tedrake, and Vincent Sitzmann. Diffusion forcing: Next-token prediction meets full-sequence diffusion. Advances in Neural Information Processing Systems, 37:2408124125, 2024a. Haoxin Chen, Menghan Xia, Yingqing He, Yong Zhang, Xiaodong Cun, Shaoshu Yang, Jinbo Xing, Yaofang Liu, Qifeng Chen, Xintao Wang, Chao Weng, and Ying Shan. Videocrafter1: Open diffusion models for high-quality video generation, 2023a. Haoxin Chen, Yong Zhang, Xiaodong Cun, Menghan Xia, Xintao Wang, Chao Weng, and Ying Shan. Videocrafter2: Overcoming data limitations for high-quality video diffusion models, 2024b. Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. Extending context window of large language models via positional interpolation. arXiv preprint arXiv:2306.15595, 2023b. 11 Preprint. Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Re. Flashattention: Fast and memoryefficient exact attention with io-awareness. Advances in neural information processing systems, 35:1634416359, 2022. Jianxiong Gao, Zhaoxi Chen, Xian Liu, Jianfeng Feng, Chenyang Si, Yanwei Fu, Yu Qiao, and Ziwei Liu. Longvie: Multimodal-guided controllable ultra-long video generation. arXiv preprint arXiv:2508.03694, 2025. Yuchao Gu, Weijia Mao, and Mike Zheng Shou. Long-context autoregressive video modeling with next-frame prediction. arXiv preprint arXiv:2503.19325, 2025. Yingqing He, Tianyu Yang, Yong Zhang, Ying Shan, and Qifeng Chen. Latent video diffusion models for high-fidelity long video generation. 2022. Roberto Henschel, Levon Khachatryan, Hayk Poghosyan, Daniil Hayrapetyan, Vahram Tadevosyan, Zhangyang Wang, Shant Navasardyan, and Humphrey Shi. Streamingt2v: Consistent, dynamic, In Proceedings of the Computer Vision and and extendable long video generation from text. Pattern Recognition Conference, pp. 25682577, 2025. Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik P. Kingma, Ben Poole, Mohammad Norouzi, David J. Fleet, and Tim Salimans. Imagen video: High definition video generation with diffusion models. arXiv preprint arXiv: 2210.02303, 2022. Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, and Jie Tang. Cogvideo: Large-scale pretraining for text-to-video generation via transformers. arXiv preprint arXiv:2205.15868, 2022. Xun Huang, Zhengqi Li, Guande He, Mingyuan Zhou, and Eli Shechtman. Self forcing: Bridging the train-test gap in autoregressive video diffusion. arXiv preprint arXiv:2506.08009, 2025. Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, et al. Vbench: Comprehensive benchmark suite for video generative models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2180721818, 2024. Jiaxiu Jiang, Wenbo Li, Jingjing Ren, Yuping Qiu, Yong Guo, Xiaogang Xu, Han Wu, and Wangmeng Zuo. Lovic: Efficient long video generation with context compression. arXiv preprint arXiv:2507.12952, 2025a. Zeyinzi Jiang, Zhen Han, Chaojie Mao, Jingfeng Zhang, Yulin Pan, and Yu Liu. Vace: All-in-one video creation and editing. arXiv preprint arXiv:2503.07598, 2025b. Jihwan Kim, Junoh Kang, Jinyoung Choi, and Bohyung Han. Fifo-diffusion: Generating infinite videos from text without training. Advances in Neural Information Processing Systems, 37: 8983489868, 2024. Dan Kondratyuk, Lijun Yu, Xiuye Gu, Jose Lezama, Jonathan Huang, Grant Schindler, Rachel Hornung, Vighnesh Birodkar, Jimmy Yan, Ming-Chang Chiu, et al. Videopoet: large language model for zero-shot video generation. arXiv preprint arXiv:2312.14125, 2023. Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. Zhuoling Li, Hossein Rahmani, Qiuhong Ke, and Jun Liu. Longdiff: Training-free long video generation in one go. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 1778917798, 2025. Yu Lu, Yuanzhi Liang, Linchao Zhu, and Yi Yang. Freelong: Training-free long video generation with spectralblend temporal attention. Advances in Neural Information Processing Systems, 37: 131434131455, 2024. 12 Preprint. William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 41954205, 2023. Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. Yarn: Efficient context window International Conference on Learning Representations., extension of large language models. 2023. Adam Polyak, Amit Zohar, Andrew Brown, Andros Tjandra, Animesh Sinha, Ann Lee, Apoorv Vyas, Bowen Shi, Chih-Yao Ma, Ching-Yao Chuang, David Yan, Dhruv Choudhary, Dingkang Wang, Geet Sethi, Guan Pang, Haoyu Ma, Ishan Misra, Ji Hou, Jialiang Wang, Kiran Jagadeesh, Kunpeng Li, Luxin Zhang, Mannat Singh, Mary Williamson, Matt Le, Matthew Yu, Mitesh Kumar Singh, Peizhao Zhang, Peter Vajda, Quentin Duval, Rohit Girdhar, Roshan Sumbaly, Sai Saketh Rambhatla, Sam Tsai, Samaneh Azadi, Samyak Datta, Sanyuan Chen, Sean Bell, Sharadh Ramaswamy, Shelly Sheynin, Siddharth Bhattacharya, Simran Motwani, Tao Xu, Tianhe Li, Tingbo Hou, Wei-Ning Hsu, Xi Yin, Xiaoliang Dai, Yaniv Taigman, Yaqiao Luo, YenCheng Liu, Yi-Chiao Wu, Yue Zhao, Yuval Kirstain, Zecheng He, Zijian He, Albert Pumarola, Ali Thabet, Artsiom Sanakoyeu, Arun Mallya, Baishan Guo, Boris Araya, Breena Kerr, Carleigh Wood, Ce Liu, Cen Peng, Dimitry Vengertsev, Edgar Schonfeld, Elliot Blanchard, Felix Juefei-Xu, Fraylie Nord, Jeff Liang, John Hoffman, Jonas Kohler, Kaolin Fire, Karthik Sivakumar, Lawrence Chen, Licheng Yu, Luya Gao, Markos Georgopoulos, Rashel Moritz, Sara K. Sampson, Shikai Li, Simone Parmeggiani, Steve Fine, Tara Fowler, Vladan Petrovic, and Yuming Du. Movie gen: cast of media foundation models. arXiv preprint arXiv: 2410.13720, 2024. Ofir Press, Noah Smith, and Mike Lewis. Train short, test long: Attention with linear biases enables input length extrapolation. arXiv preprint arXiv:2108.12409, 2021. Haonan Qiu, Menghan Xia, Yong Zhang, Yingqing He, Xintao Wang, Ying Shan, and Ziwei Liu. Freenoise: Tuning-free longer video diffusion via noise rescheduling. arXiv preprint arXiv:2310.15169, 2023. Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020. Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. Quan Sun, Yufeng Cui, Xiaosong Zhang, Fan Zhang, Qiying Yu, Yueze Wang, Yongming Rao, Jingjing Liu, Tiejun Huang, and Xinlong Wang. Generative multimodal models are in-context learners. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1439814409, 2024. Jiangtong Tan, Hu Yu, Jie Huang, Jie Xiao, and Feng Zhao. Freepca: Integrating consistency information across long-short frames in training-free long video generation via principal component analysis. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 27979 27988, 2025. Zhenxiong Tan, Xingyi Yang, Songhua Liu, and Xinchao Wang. Video-infinity: Distributed long video generation. arXiv preprint arXiv:2406.16260, 2024. Genmo Team. Mochi 1. https://github.com/genmoai/models, 2024a. The FastVideo Team. Fastvideo: unified framework for accelerated video generation, April 2024b. URL https://github.com/hao-ai-lab/FastVideo. Hansi Teng, Hongyu Jia, Lei Sun, Lingzhi Li, Maolin Li, Mingqiu Tang, Shuai Han, Tianning Zhang, WQ Zhang, Weifeng Luo, et al. Magi-1: Autoregressive video generation at scale. arXiv preprint arXiv:2505.13211, 2025. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. Preprint. Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, et al. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. Fu-Yun Wang, Wenshuo Chen, Guanglu Song, Han-Jia Ye, Yu Liu, and Hongsheng Li. Gen-l-video: Multi-text to long video generation via temporal co-denoising. arXiv preprint arXiv:2305.18264, 2023. Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024a. Xinlong Wang, Xiaosong Zhang, Zhengxiong Luo, Quan Sun, Yufeng Cui, Jinsheng Wang, Fan Zhang, Yueze Wang, Zhen Li, Qiying Yu, et al. Emu3: Next-token prediction is all you need. arXiv preprint arXiv:2409.18869, 2024b. Yuqing Wang, Tianwei Xiong, Daquan Zhou, Zhijie Lin, Yang Zhao, Bingyi Kang, Jiashi Feng, and Xihui Liu. Loong: Generating minute-level long videos with autoregressive language models. arXiv preprint arXiv:2410.02757, 2024c. Chenfei Wu, Lun Huang, Qianxi Zhang, Binyang Li, Lei Ji, Fan Yang, Guillermo Sapiro, and Nan Duan. Godiva: Generating open-domain videos from natural descriptions. arXiv preprint arXiv:2104.14806, 2021. Chenfei Wu, Jian Liang, Lei Ji, Fan Yang, Yuejian Fang, Daxin Jiang, and Nan Duan. Nuwa: Visual In European conference on computer synthesis pre-training for neural visual world creation. vision, pp. 720736. Springer, 2022. Yecheng Wu, Zhuoyang Zhang, Junyu Chen, Haotian Tang, Dacheng Li, Yunhao Fang, Ligeng Zhu, Enze Xie, Hongxu Yin, Li Yi, et al. Vila-u: unified foundation model integrating visual understanding and generation. arXiv preprint arXiv:2409.04429, 2024. Jinbo Xing, Menghan Xia, Yong Zhang, Haoxin Chen, Xintao Wang, Tien-Tsin Wong, and Ying Shan. Dynamicrafter: Animating open-domain images with video diffusion priors. 2023. Wilson Yan, Yunzhi Zhang, Pieter Abbeel, and Aravind Srinivas. Videogpt: Video generation using vq-vae and transformers. arXiv preprint arXiv:2104.10157, 2021. Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. Tianwei Yin, Qiang Zhang, Richard Zhang, William Freeman, Fredo Durand, Eli Shechtman, and Xun Huang. From slow bidirectional to fast autoregressive video diffusion models. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 2296322974, 2025. Jintao Zhang, Haofeng Huang, Pengle Zhang, Jia Wei, Jun Zhu, and Jianfei Chen. Sageattention2: Efficient attention with thorough outlier smoothing and per-thread int4 quantization. arXiv preprint arXiv:2411.10958, 2024a. Jintao Zhang, Jia Wei, Haofeng Huang, Pengle Zhang, Jun Zhu, and Jianfei Chen. SageatarXiv preprint tention: Accurate 8-bit attention for plug-and-play inference acceleration. arXiv:2410.02367, 2024b. Min Zhao, Fan Bao, Chongxuan Li, and Jun Zhu. Egsde: Unpaired image-to-image translation via energy-guided stochastic differential equations. Advances in Neural Information Processing Systems, 35:36093623, 2022. Min Zhao, Rongzhen Wang, Fan Bao, Chongxuan Li, and Jun Zhu. Controlvideo: Adding conditional control for one shot text-to-video editing. arXiv preprint arXiv:2305.17098, 2(3), 2023. Min Zhao, Hongzhou Zhu, Chendong Xiang, Kaiwen Zheng, Chongxuan Li, and Jun Zhu. Identifying and solving conditional image leakage in image-to-video diffusion model. Advances in Neural Information Processing Systems, 37:3030030326, 2024. 14 Preprint. Min Zhao, Guande He, Yixiao Chen, Hongzhou Zhu, Chongxuan Li, and Jun Zhu. Riflex: free lunch for length extrapolation in video diffusion transformers. arXiv preprint arXiv:2502.15894, 2025. Zangwei Zheng, Xiangyu Peng, Tianji Yang, Chenhui Shen, Shenggui Li, Hongxin Liu, Yukun Zhou, Tianyi Li, and Yang You. Open-sora: Democratizing efficient video production for all. arXiv preprint arXiv:2412.20404, 2024a. Zangwei Zheng, Xiangyu Peng, Tianji Yang, Chenhui Shen, Shenggui Li, Hongxin Liu, Yukun Zhou, Tianyi Li, and Yang You. Open-sora: Democratizing efficient video production for all. arXiv preprint arXiv: 2412.20404, 2024b. Yuan Zhou, Qiuyue Wang, Yuxuan Cai, and Huan Yang. Allegro: Open the black box of commercial-level video generation model. arXiv preprint arXiv: 2410.15458, 2024. Le Zhuo, Ruoyi Du, Han Xiao, Yangguang Li, Dongyang Liu, Rongjie Huang, Wenze Liu, Lirui Zhao, Fu-Yun Wang, Zhanyu Ma, et al. Lumina-next: Making lumina-t2x stronger and faster with next-dit. Advances in Neural Information Processing Systems., 2024."
        },
        {
            "title": "USE OF LARGE LANGUAGE MODELS",
            "content": "We used large language model solely to assist in polishing English writing and improving clarity. All research ideas, experiments, results, and interpretations are entirely our own."
        },
        {
            "title": "A RELATED WORK",
            "content": "Text-to-video Diffusion Transformers. The recent advances in text-to-video generation have been primarily driven by diffusion models (Ho et al., 2020; Song et al., 2020; Ho et al., 2022; He et al., 2022; Zhao et al., 2022; 2023; Blattmann et al., 2023; Xing et al., 2023; Chen et al., 2023a; Zhao et al., 2024; Polyak et al., 2024; Zhou et al., 2024; Team, 2024a; Chen et al., 2024b). With the development of diffusion transformers (DiTs) (Bao et al., 2023; Peebles & Xie, 2023), DiT-based text-to-video diffusion models have achieved remarkable performance, such as Sora (Brooks et al., 2024), Vidu (Bao et al., 2024), CogVideoX (Yang et al., 2024) and Open-Sora (Zheng et al., 2024a). Although achieving high quality, leading models are trained only on fixed maximum sequence length, limiting long-term capacity. During video length extrapolation, they suffer from repetition or quality degradation, underscoring the need for length extrapolation. Length Extrapolation in Transformers. The goal of length extrapolation is to enable transformers to generate sequences longer than those seen during training in single forward (Press et al., 2021). This is typically achieved by modifying positional encodings. For example, position interpolation (PI) (Chen et al., 2023b) improves performance by interpolating the frequencies in RoPE so that they remain within the training range even under extrapolation. NTK (bloc97, 2023), YaRN (Peng et al., 2023), and Time-aware Scaled RoPE (TASR) (Zhuo et al., 2024) combine interpolation with direct extrapolation, incorporating adjustments along the token dimension, denoising timesteps, and other factors to achieve better results. However, these methods perform poorly on image and video DiTs, often leading to content collapse or repetition. RIFLEx (Zhao et al., 2025) mitigates repetition by identifying and attenuating the intrinsic RoPE frequency, yet it still suffers from degraded visual quality. In contrast, our method effectively addresses both content repetition and quality degradation. Long Video Generation. There also exist many approaches to long video generation (Qiu et al., 2023; Wang et al., 2023; Henschel et al., 2025; Kim et al., 2024; Tan et al., 2024; Yin et al., 2025; Wang et al., 2024c; Cai et al., 2025; Li et al., 2025; Lu et al., 2024; Tan et al., 2025; Jiang et al., 2025a; Gao et al., 2025; Gu et al., 2025), most of which intervene in the diffusion inference process. For instance, FreeNoise (Qiu et al., 2023) enhances temporal consistency via noise initialization, FIFO-Diffusion (Kim et al., 2024) feeds frames sequentially into denoising window of training length, and Video-Infinity (Tan et al., 2024) exploits distributed computation to scale up video length. While effective for generating long videos, these methods are orthogonal to our length 15 Preprint. extrapolation strategy, which extends the intrinsic capacity of DiTs to longer sequences and can be readily integrated with them. In addition to diffusion-based approaches to long video generation, alternative modeling paradigms such as autoregressive methods (Wu et al., 2021; Yan et al., 2021; Hong et al., 2022; Wu et al., 2022; Kondratyuk et al., 2023; Wu et al., 2024; Sun et al., 2024; Wang et al., 2024b) and diffusion forcing (Chen et al., 2024a; Huang et al., 2025; Teng et al., 2025) are also capable of generating long videos. Although our method is designed for diffusion models, it may also offer insights into length extrapolation for these alternative paradigms."
        },
        {
            "title": "B MORE DETAILS OF OUR METHOD",
            "content": "B.1 DERIVATION OF THE PERIODIC OUTPUTS satisfies the following properties up to negligible error: In this section, we present formal derivation of Eq. (3). Specifically, the attention score matrix RLL Prop.1 (Row-wise periodicity): Pi,j = Pi,j+T , {0, . . . , 1}, {0, . . . , 1}, where N+ corresponds to the observed repetition period in Sec. 3.1. Prop.2 (Relative positional invariance): Pi,j = Pi+p,j+p, {0, . . . , Lp1}, {0, . . . , p1}, where N+ is the relative displacement. In the ffollowing derivation we instantiate = . On basis of the above properties, we derive the periodicity of the attention scores and outputs as follows. {0, . . . , 1}, Pi+T,jVj Oi+T = = Prop.1 = Prop.2 = Prop.1 = = (cid:88)L1 j=0 (cid:88)LT 1 j=0 (cid:88)LT 1 j=0 (cid:88)LT 1 j=0 (cid:88)LT 1 j=0 (cid:88)L1 j= Pi,jVj Pi+T,jVj + Pi+T,jVj (cid:88)L1 j=LT (cid:88)L1 Pi+T,j+T Vj + Pi+T,jVj j=LT Pi,jVj + Pi,jVj + (cid:88)L1 j=LT (cid:88)L1 j=LT Pi,jT Vj Pi,jVj = Oi. B.2 DETAILS OF THE MULTIMODAL ROTARY POSITION EMBEDDING (7) (8) (9) (10) (11) (12) (13) In this section, we provide the details of the Multimodal RoPE (M-RoPE) (Wang et al., 2024a) the input vector Specifically, for token at position (t, h, w), introduced in Sec. 2. RD is divided into three subspaces of dimensions dT , dH, dW , respectively assigned to temporal, height, and width encodings. Each subspace is modulated by its own frequency series {ϕT , {ϕW }dT 1 i=0 , {ϕH }dT +dH1 i=dT }D1 . Concretely, we define i=dT +dH (cid:20)cos(ϕα (cid:21) (cid:20) x2i sin(ϕα x2i+1 (pα) = , Rα pα) sin(ϕα cos(ϕα pα) pα) pα) (cid:21) , (14) RoPE(x, t, h, w)i = Rα (pα) where α {T , H, W} indexes the temporal, height, and width dimensions with corresponding positions pα {t, h, w} and frequency components {ϕα }. The index ranges are {0, . . . , dT /2 1}, α = , {dt/2, . . . , dT /2 + dH/2 1}, α = H, {dT /2 + dH/2, . . . , D/2 1}, α = W. (15) After M-RoPE encoding, the queries and keys form RLD and RLD. As in Eq. (2), they produce the attention logits matrix RLL , where the attention logit between the query at 16 Preprint. (t, h, w), denoted q(t,h,w), and the key at (t + t, + h, + w), denoted k(t+t,h+h,w+w), expands explicitly as: S(t,h,w),(t+t,h+h,w+w) = dT /21 (cid:88) i=0 q(2i:2i+1) (t,h,w) RT (t)k(2i:2i+1) (t+t,h+h,w+w)+ dT /2+dH/21 (cid:88) i=dT / D/21 (cid:88) i=dT /2+dH/2 q(2i:2i+1) (t,h,w) RH (h)k(2i:2i+1) (t+t,h+h,w+w)+ q(2i:2i+1) (t,h,w) RW (w)k(2i:2i+1) (t+t,h+h,w+w) (16) dT /21 (cid:88) (cid:104) = i=0 λ(i) 1 cos(ϕT t) + λ(i) 2 sin(ϕT t) (cid:105) + dT /2+dH/21 (cid:88) (cid:104) λ(i) 1 cos(ϕH h) + λ(i) 2 sin(ϕH h) (cid:105) + i=dT /2 D/21 (cid:88) (cid:104) i=dT /2+dH/ where λ(i) 1 cos(ϕW w) + λ(i) 2 sin(ϕW (cid:105) w) 1 = q(2i) λ(i) 2 = q(2i+1) λ(i) (t,h,w)k(2i) (t,h,w)k(2i) (t+t,h+h,w+w) + q(2i+1) (t+t,h+h,w+w) q(2i) (t,h,w)k(2i+1) (t,h,w)k(2i+1) (t+t,h+h,w+w), (t+t,h+h,w+w). , (17) (18) (19) B.3 DERIVATION OF THE STATISTICAL ATTENTION PATTERN S(t) In this section, we present the derivation of Eq. (4) in Sec. 3.2.1. We investigate the row-wise pattern of attention logits by examining the expectation of the attention logits between queries and (cid:3))1. This expectation is taken across keys at relative temporal distance (i.e., E(cid:2)S(t,h,w),(t+t,h,w) attention layers, heads, and query positions. In Appendix B.4, we further show that when the true variance is taken into account, the actual attention logits still follow the same patterns as indicated by this expectation. Specifically, on basis of the formula of M-RoPE (i.e., Eq. (16)), the target expectation is given by (cid:104) Et,h,w S(t,h,w),(t+t,h,w) (cid:105) = Et,h,w dT /21 (cid:88) (cid:104) i=0 q(2i:2i+1) (t,h,w) RT (t)k(2i:2i+1) (t+t,h,w)+ dT /2+dH/21 (cid:88) q(2i:2i+1) (t,h,w) RH (0)k(2i:2i+1) (t+t,h,w) + D/21 (cid:88) i=dT /2+dH/2 q(2i:2i+1) (t,h,w) RW (0)k(2i:2i+1) (t+t,h,w) i=dT /2 dT /21 (cid:88) (cid:104) = i=0 where 1 cos (cid:0)ϕT E(i) t(cid:1) + E(i) 2 sin (cid:0)ϕT t(cid:1)(cid:105) D/21 (cid:88) + i=dT /2 E(i) 1 , E(i) 1 = Et,h,w E(i) 2 = Et,h,w (cid:104) (cid:104) (t,h,w)k(2i) q(2i) (t,h,w)k(2i) q(2i+1) (t+t,h,w) + q(2i+1) (t+t,h,w) q(2i) (t,h,w)k(2i+1) (t,h,w)k(2i+1) (t+t,h,w) (t+t,h,w) (cid:105) (cid:105) , . (cid:105) (20) (21) (22) (23) 1Strictly speaking, the analysis should target S(t,h,w),(t+t,h+h,w+w) for all h, w, but as the phenomena are similar across h, w, we focus on S(t,h,w),(t+t,h,w) for simplicity. 2For brevity, we omit layer and head indices in the expectation notation. 17 Preprint. 1 and E(i) E(i) 1 Et,h,w,t In practice, though the integrands of these expectations are actually functions of t, the empirical statistics in Fig. 10 (col. 1) indicate that their variances with respect to are negligible. Hence, we approximate E(i) 2 as constants up to negligible error, which is defined by (t+t,h,w) + q(2i+1) (t+t,h,w) q(2i) By substituting these two expressions into Eq. (22) and Eq. (23), the expected attention logits can be well approximated as S(t), where (t,h,w)k(2i+1) (t,h,w)k(2i+1) (t,h,w)k(2i) q(2i) (t,h,w)k(2i) q(2i+1) 2 Et,h,w,t =: ˆE(i) 1 , =: ˆE(i) 2 . (t+t,h,w) (t+t,h,w) E(i) (24) (25) (cid:105) (cid:104) (cid:105) (cid:104) S(t) = dT /21 (cid:88) i=0 (cid:104) ˆE(i) 1 cos (cid:0)ϕT t(cid:1) + ˆE(i) 2 sin (cid:0)ϕT t(cid:1)(cid:105) D/21 (cid:88) + i=dT /2 ˆE(i) 1 . (26) To simplify the expression, we employ the auxiliary angle formula to rewrite the two trigonometric functions as one, i.e., S(t) = dT /21 (cid:88) i=0 (cid:105) (cid:104) ai cos(ϕit + bi) + C, (27) 1 + (cid:105)2 (cid:105)2 (cid:104) ˆE(i) (cid:114)(cid:104) ˆE(i) 2 , ˆE(i) 1 ). where ai = (col. 2), ˆE(i) remains consistently close to zero, which in turn makes bi nearly vanish (for example, 2 b0 is 0.039 for HunyuanVideo). This observation allows us to apply Proposition 1 in Sec. 3.2.1 up to an error of negligible magnitude. Detailed statistical data for ˆE(i) 2 , ai, bi are shown in Fig. 10 (col. 2, 3, 4). Interestingly, as shown in Fig. 10 , bi = atan2( ˆE(i) 1 , ˆE(i) 2 (a) Statistics of HunyuanVideo. (b) Statistics of Wan. 1 , E(i) Figure 10: Statistics of attention logits in HunyuanVideo and Wan. The variances of E(i) with respect to (col. 1) are negligible compared to their expectations (col. 2), making the approximation in Eq. (24), Eq. (25) accurate. The bias angles bi (col. 4) are close to zero, except for b9 and b15 in Wan whose impact is negligible since the corresponding a9, a15 are near zero (col. 3). 2 B.4 CONSISTENCY OF ACTUAL ATTENTION PATTERN WITH S(t) In this section, we investigate the actual attention scores under the true variance, demonstrating that they preserve the same characteristics as the averaged values described in Sec. 3.2.1. As shown in Fig. 11, when the standard deviation over attention layers, heads, and query positions is incorporated into the mean, the attention logits of HunyuanVideo still exhibit clear periodicity at their peaks, whereas those of Wan2.1 remain non-periodic. Therefore, the conclusions drawn in Sec. 3.2.1 from the mean-based analysis hold with strong generality in practice. 18 Preprint. Figure 11: Attention logits under actual variance. Even with standard deviation across layers, heads, and query positions, HunyuanVideo retains clear periodic peaks while Wan 2.1 remains nonperiodic, confirming the general validity of the mean-based analysis in Sec. 3.2.1. B.5 PROOF OF PROPOSITION 1 Proposition 1 is well-known in harmonic analysis and signal processing, and we provide the proof here only for completeness. Proof. Sufficiency. If ϕi/ϕN 1 N+ for all i, write ϕi = kiϕN 1 with ki N+. Let TN 1 = 2π/ϕN 1. Then for each i, cos (cid:0)ϕi(t + TN 1)(cid:1) = cos(cid:0)kiϕN 1t + 2πki (cid:1) = cos(ϕit), R, (28) so (t + TN 1) = (t), R. Hence TN 1 is period of . Necessity. Suppose TN 1 = 2π/ϕN 1 is period of . Then for all t, 0 = (t + TN 1) (t) = 1 (cid:88) i= (cid:2) cos(ϕit + ϕiTN 1) cos(ϕit)(cid:3). ai (29) Using cos(x + y) cos = (cos 1) cos sin sin x, 1 (cid:88) (cid:104) ai 0 = i=0 (cos(ϕiTN 1) 1) cos(ϕit) sin(ϕiTN 1) sin(ϕit) (cid:105) , R. (30) The family {cos(ϕi), sin(ϕi)}i with distinct positive ϕi is linearly independent over (e.g., via independence of eiϕit). Hence for each i, cos(ϕiTN 1) 1 = 0, sin(ϕiTN 1) = 0, so ϕiTN 1 2πZ. Substituting TN 1 = 2π/ϕN 1 yields ϕi ϕN 1 N+, (31) (32) as all ϕi > 0. B.6 REMARKS ON PROPOSITION Relaxed conditions under which the proposition holds approximately. Although the strict condition for forming harmonics in Proposition 1 is ϕi/ϕN 1 N+, in this section we highlight approximate conditions that can likewise induce dominant frequency leading to content repetition in videos. Specifically, if ϕi/ϕN 1 is sufficiently close to an integer, constructive amplification can still occur for small (e.g., 2TN 1). For example, for CogVideoX, the ratio of the first two frequencies is ϕ0/ϕ1 = 3.16, which is close to the integer 3, thereby producing dominant component that accounts for 50.80% of the total amplitude. This gives rise to an approximately periodic composite attention pattern  (Fig. 12)  , which in turn leads to content repetition (Fig. 14, right). 19 Preprint. Model Attention maps Statistical row attention analysis Hun. (a) Periodic attention: Pi,j Pi,j+T (b) Approximately harmonic RoPE frequencies (ϕ0/ϕ1 N+) amplify the largest amplitude ϕ1 (dashed line), inducing approximately periodic composite attention. Figure 12: Periodic attention patterns of CogVideoX. The RoPE frequencies of CogVideoX approximately satisfy the harmonic condition, which amplifies the largest-amplitude component and thereby induces periodic attention patterns. Remarks on the strict period of HunyuanVideo. We herein examine the strict periodicity of HunyuanVideo. Strictly speaking, its fundamental frequency is ϕ7, with ratios ϕi/ϕ7 = 27i, {0, . . . , 7}. According to Proposition 1, the theoretical period of S(t) is T7 = 2π . However, as ϕ7 shown in Fig. 10a (col. 3), the amplification contributed by ϕ7 is very small, accounting for only 6.677%, which makes its impact negligible. Moreover, its period of 804 is far larger than the extrapolation length (e.g., 132 at 4 extrapolation), rendering the variation of the corresponding component almost imperceptible within this range. The same reasoning applies to ϕi for {4, 5, 6}. Consequently, our analysis focuses on ϕi with {0, 1, 2, 3}, whose single-frequency contributions are both large enough in amplitude and sufficiently oscillatory to shape S(t). B.7 NECESSITY OF CONCENTRATING ON THE TRAINING WINDOW In this section, we provide detailed experimental evidence supporting the discussion in Sec. 3.2.2 on where sharpened attention focus is most beneficial. Specifically, on Wan with extrapolation ratio = 3, we test four strategies for sharpening attention: concentrating on the leading 1 of each , the training window, and the top 1 row, the trailing 1 tokens according to the original attention scores. As shown in Fig. 13, concentrating on the leading or trailing 1 of each row causes the video to collapse, while top 1 yields poor visual quality with little dynamics. In contrast, restricting attention to the training window leads to the most significant improvement in video quality. Figure 13: Comparison of attention concentration strategies on Wan at = 3. Concentrating on the leading or trailing 1 yields poor quality with little dynamics. Restricting attention to the training window proves most effective. of each row collapses the video, and top 1 20 Preprint."
        },
        {
            "title": "C MORE DETAILS OF EXPERIMENTS",
            "content": "C.1 FAILURE MODES OF COGVIDEOX In this section, we present the manifestation of the failure modes of video length extrapolation as discussed in Sec. 3.1 on an additional model, CogVideoX. As shown in Fig. 14, when extrapolated to three times the normal training length, the generated videos exhibit sharp decline in both dynamic degree and visual quality, along with noticeable content repetition. Figure 14: Failure modes of CogVideoX under 3 extrapolation. The generated videos show degraded visual quality, reduced dynamics, and clear content repetition, consistent with the failure modes discussed in Sec. 3.1. C.2 MORE IMPLEMENTATION DETAILS In this section, we provide further details of Sec. 4.2. The implementation of NoRepeat Score. The NoRepeat Score implemented in RIFLEx (Zhao et al., 2025) is only applicable when the content repeats once, which makes it unsuitable for longer extrapolation tasks. We therefore modify it accordingly. Specifically, the computation of the NoRepeat Score consists of two steps: static-video filtering and repeated-frame ratio calculation. In the first step, we uniformly sample 8 frames across the video; if the mean pairwise L2 distance among them falls below threshold, the video is considered static and discarded. This prevents completely static videos from interfering with subsequent repetition detection. In the second step, we measure the ratio of repeated frames to the total frame count, which defines the NoRepeat Score. Following RIFLEx, we first search around the dominant-frequency period for the frame with the minimal L2 distance to the first frame. This frame is then taken as the start of candidate repeated sequence. We then compare each frame in this candidate sequence with the corresponding frame at the beginning of the video; frames whose L2 distance is below the threshold are counted as repetitions. Empirically, threshold of 55 was found to align better with human perception and was consequently applied to both steps. Finally, we report the mean NoRepeat Score across all videos as the final result. The detailed implementation code is included in the supplementary material. The implementation of RIFLEx and UltraViCo on Wan. Since Wan does not exhibit content repetition, it is not applicable to determine the dominant frequency from the repetition period as performed in Zhao et al. (2025). Instead, following Sec. 3.2.1, we take the largest-amplitude frequency ϕ0 as the dominant frequency. For UltraViCo, the first frames decay factor is set negative to fix its blurring. We hypothesize that this is caused by the causal design of the video VAE, where the first frame is encoded independently and without temporal compression. As result, it exhibits different statistical properties from subsequent frames and becomes more sensitive to perturbations. Details of the ablation study. Herein, we detail the setup of the ablation study in Sec. 4.2. Specifically, as shown in Fig. 7 (top), we compare three decay strategiesparabolic, linear, and constant. The parabolic strategy takes the following form: ij = λij Sij, where λij = (cid:26)1, if j L/2 or Sij < 0, α1(i j/L)2 + α2(1 (i j/L)2), otherwise, whereas the linear strategy takes the following form: ij = λij Sij, where λij = (cid:26)1, if j L/2 or Sij < 0, α1i j/L + α2(1 j/L), otherwise, (33) (34) Preprint. and the constant strategy is ij = λij Sij, where λij = (cid:26)1, α, if j L/2 or Sij < 0, otherwise. (35) We set α = 0.9 for the constant strategy, and α1 = 0.85, α2 = 0.95 for the parabolic and the linear strategies. As shown in Fig. 7 (top), parabolic, linear, and constant decay yield only minor differences, indicating that the key is distinguishing in-window from out-of-window tokens rather than the decay shape. C.3 ADDITIONAL EXPERIMENTS OF DIFFERENT EXTRAPOLATION RATIOS AND MODELS In this section, we provide some additional extrapolation ratios from = 2 to 5 and Settings. models based on 25 prompts from VBench (Huang et al., 2024). To evaluate the generality of UltraViCo, we test 2 extrapolation on HunyuanVideo, Wan, and CogVideoX, as well as 3 and 4 extrapolation on CogVideoX. In addition, we assess 5 extrapolation on HunyuanVideo. For Wan, we set α = 0.9. For HunyuanVideo, we use γ = 4 across all ratios, with α = 0.95, β = 0.6 at 2 and α = 0.9, β = 0.8 at 5. For CogVideoX, we use γ = 1 and β = 0.6 for all ratios, with α = 0.9 at 2 and 3, and α = 0.85 at 4. The configurations of other baselines follow Sec. 4.1. Results. We compare UltraViCo with the baselines in Sec. 4.2. As shown in Tab. 3, UltraViCo achieves the best performance across all models and extrapolation ratios, not only avoiding content repetition but also substantially improving video quality. For example, CogVideoX exhibits nearly static videos at 4 extrapolation (Dynamic Degree 16) with poor visual quality (Imaging Quality 56), whereas our method significantly enhances both temporal dynamics and visual quality, with Dynamic Degree and Imaging Quality improving by 200% and 13.48%, respectively. Furthermore, at 5 extrapolation, UltraViCo also demonstrates strong performance, surpassing the best baseline scores by 350% in Dynamic Degree and 47.59% in Imaging Quality, indicating the potential of our method to extend to larger extrapolation ratios. C.4 MORE QUALITATIVE RESULTS OF OUR METHOD In this section, we provide additional qualitive results for the experiments in Sec. 4.2. As shown in Fig. 15 and Fig. 16, whether under 3 or 4 extrapolation ratios, and across Wan and CogVideoX, our method consistently achieves substantially superior visual quality and temporal dynamics compared to the baselines. For example, as shown in Fig. 15, the videos generated by various baselines for 3 and 4 extrapolation on Wan are nearly completely static, whereas our method produces highly fluid and natural large-scale motion. Similarly, as shown in Fig. 16, the videos from the baselines are very blurry with dull colors, while our method generates realistic, natural results with rich details. Moreover, we present another downstream task in Fig. 17, where generation is performed based on given pose. Our method achieves high quality and dynamic results while closely following the given conditions. C.5 ACCELERATION OF ULTRAVICO VIA SPARSE ATTENTION AND DISTILLATION Building upon recent advances in sparse-attention-based video acceleration and distillation (Team, 2024b), UltraViCo achieves about 16 speed-up without compromising performance (see Table 7). C.6 RUNTIME AND MEMORY COST As shown in Table 8, built on top of FlashAttention (Dao et al., 2022) and SageAttention (Zhang et al., 2024b;a), UltraViCo incurs almost no additional overhead in either latency or memory usage. 22 Preprint. Figure 15: Qualitative results on Wan. The baselines produce nearly static videos with poor visual quality, whereas our method achieves significantly better quality and much more motion. Figure 16: Qualitative results on CogVideoX. The baselines produce nearly static videos with poor visual quality, whereas our method generates realistic results with rich details and fluid motion. 23 Preprint. Table 3: Quantitative results on VBench for more models and extrapolation. Note that NoRepeat Score is essentially binary indicator: red entries indicate visually obvious repetitions, while others show no noticeable repetition. Method PE PI NTK YaRN TASR RIFLEx Ours Method PE PI NTK YaRN TASR RIFLEx Ours Method PE PI NTK YaRN TASR RIFLEx Ours Wan with 2 extrapolation CogVideoX with 3 extrapolation NoRepeat Dynamic Quality Overall NoRepeat Dynamic Quality Overall N/A N/A N/A N/A N/A N/A N/A 32 32 44 24 36 16 68 58.13 54.23 59.59 55.14 59.97 48.15 66.88 23.22 21.52 23.52 21.57 23.70 20.34 25.28 82.52 99.07 86.07 97.47 97.93 97.86 99.38 16 4 4 0 8 8 32 57.91 54.27 55.24 53.96 55.75 55.31 60. 19.59 18.17 19.33 18.05 19.24 19.03 24.77 HunyuanVideo with 2 extrapolation CogVideoX with 4 extrapolation NoRepeat Dynamic Quality Overall NoRepeat Dynamic Quality Overall 80.43 98.87 94.97 97.99 94.85 97.27 97. 40 4 32 4 36 36 44 62.67 52.35 65.47 52.87 64.55 65.19 66.50 24.36 23.55 24.62 23.26 24.59 24.52 24.82 76.57 88.53 78.89 94.75 99.13 97.00 96.79 16 4 2 4 16 12 48 55.25 46.82 52.74 47.36 46.75 50.59 62. 17.27 16.63 18.14 16.90 17.28 16.66 25.39 CogVideoX with 2 extrapolation HunyuanVideo with 5 extrapolation NoRepeat Dynamic Quality Overall NoRepeat Dynamic Quality Overall 92.31 98.85 94.66 98.81 95.91 99.42 98. 28 8 16 8 16 16 32 64.28 57.11 63.04 58.83 62.17 60.30 64.39 22.83 21.88 23.55 21.81 23.44 23.28 25.36 30.78 81.58 71.54 77.70 35.31 53.65 99.44 4 0 8 0 8 4 36 39.04 36.63 43.43 37.88 42.88 40.55 64. 15.64 16.76 17.78 17.85 17.88 15.71 24.16 Figure 17: Our method for pose-guided video generation. Our method closely aligns with the given pose conditions, while ensuring high dynamic range and excellent visual quality."
        },
        {
            "title": "D FURTHER DETAILS OF ULTRAVICO",
            "content": "D.1 ULTRAVICO WITH EFFIEIENT ONLINE ATTENTION UltraViCo does not require materializing the full attention matrix and can be seamlessly integrated into efficient online attention kernels. Herein, we present its implementation based on FlashAttention, as illustrated by Algorithm 1. D.2 ABLATION ON HYPERPARAMETERS In this section, we present more detailed illustrative ablation results for the hyperparameters α and β. The detailed sensitivity curve is shown in Fig. 18, while the illustrative ablations on the independent effects of α and β in the main experiments are reported in Tab. 6. 24 Preprint. for in [1, Tn] do Algorithm 1 UltraViCo FlashAttention Kernel Require: Matrices Q, K, RN d, block size bq, bkv. 1: Divide into Tm = N/bq blocks {Qm}, and divide K, into Tn = N/bkv blocks {Kn} and {Vn}; 2: for in [1, Tm] do 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: end for 15: return = {Om}; = bq + range(0, bq), = bkv + range(0, bkv), R1bq ,j R1bkv ; Initialize λ Rbq bkv to 0 ; λ = Eq. 6(i,j) ; = λQmK Sn ; , rowmax(Sn = max(pn1 pn pn (cid:101)P = exp(Sn m) ; = epn1 pn + rowsum( (cid:101)P ln1 ln pn = diag(epn1 + (cid:101)P )On1 On end for Om = diag(lTn ) ; mVn ; )1OTn m)) ; ; (a) Schematic diagram of the α sensitivity curve. (b) Schematic diagram of the β sensitivity curve. Figure 18: Illustration of the hyperparameter sensitivity curve. Table 4: Illustrative sensitivity analysis of α on Hunyuan at 3 extrapolation. We set β equal to α, i.e., single decay factor is shared globally. α 1.0 0.95 0.9 0.85 0.8 Consistency Dynamics Quality Overall NoRepeat 0.9795 0.9663 0.9647 0.9298 0.9231 16 25 32 68 73 51.85 54.92 57.53 69.93 70.35 21.62 24.07 26.25 26.89 26. 53.17 100 93.34 99.53 100 Table 5: Illustrative sensitivity analysis of β on Hunyuan at 3 extrapolation. We set α = 0.9 across all settings. β Consistency Dynamics Quality Overall NoRepeat 1.0 0.9 0.8 0.75 0.6 0.45 0.3 0.9716 0.9647 0.9510 0.9496 0.9465 0.9349 0. 28 32 45 51 62 65 66 55.23 57.53 59.35 62.11 65.00 68.34 70.45 24.52 26.25 26.42 26.98 26.45 26.99 26.98 57.42 93.34 97.25 95.77 100 100 100 25 Preprint. Table 6: Illustrative ablation experiments that independently examine the individual effects of α and β."
        },
        {
            "title": "Method",
            "content": "Consistency Dynamics Quality Overall NoRepeat HunyuanVideo with 3 extrapolation α = 1, β = 1 α = 0.9, β = 1 α = 1, β = 0.6 α = 0.9, β = 0.6 0.9795 0.9716 0.9784 0.9465 16 28 25 62 51.85 55.23 55.13 65. α = 1 α = 0.9 Wan2.1-1.3B with 3 extrapolation 0.9419 0.9444 6 46 56.28 62.43 21.62 24.52 23.13 26. 18.53 23.21 53.17 57.42 93.52 100 Table 7: Illustrative performance when combined with recent video-acceleration methods on HunyuanVideo. Setting Time Cost Consistency Dynamics Quality Overall NoRepeat 3 3 with FastVideo 4 4 with FastVideo 5 GPUhours 0.3 GPUhours 8 GPUhours 0.5 GPUhours 0.9465 0.9432 0.9491 0.9399 62 64 42 40 65.00 63.89 66.54 62. 26.45 25.98 24.52 24.83 100 100 99.87 96.32 Table 8: Illustrative runtime and memory comparison. Note that SageAttention is optimized for 4090-like architectures; on A800, its runtime is comparable to FlashAttention. Model / Method Time (s / iter) Memory (per GPU) HunyuanVideo (3 extrapolation) SageAttention SageAttention + Ours FlashAttention FlashAttention + Ours 341.2 349.6 349.3 355.3 Wan (3 extrapolation) SageAttention SageAttention + Ours FlashAttention FlashAttention + Ours 32.13 34.12 32.64 33.74 73188M 72346M 76030M 75932M 24342M 24342M 24349M 24346M"
        }
    ],
    "affiliations": [
        "Princeton University",
        "Renmin University of China",
        "ShengShu",
        "The University of Texas at Austin",
        "Tsinghua University"
    ]
}