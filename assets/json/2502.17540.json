{
    "paper_title": "PosterSum: A Multimodal Benchmark for Scientific Poster Summarization",
    "authors": [
        "Rohit Saxena",
        "Pasquale Minervini",
        "Frank Keller"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Generating accurate and concise textual summaries from multimodal documents is challenging, especially when dealing with visually complex content like scientific posters. We introduce PosterSum, a novel benchmark to advance the development of vision-language models that can understand and summarize scientific posters into research paper abstracts. Our dataset contains 16,305 conference posters paired with their corresponding abstracts as summaries. Each poster is provided in image format and presents diverse visual understanding challenges, such as complex layouts, dense text regions, tables, and figures. We benchmark state-of-the-art Multimodal Large Language Models (MLLMs) on PosterSum and demonstrate that they struggle to accurately interpret and summarize scientific posters. We propose Segment & Summarize, a hierarchical method that outperforms current MLLMs on automated metrics, achieving a 3.14% gain in ROUGE-L. This will serve as a starting point for future research on poster summarization."
        },
        {
            "title": "Start",
            "content": "POSTERSUM: Multimodal Benchmark for Scientific Poster Summarization"
        },
        {
            "title": "Rohit Saxena",
            "content": "Pasquale Minervini Institute for Language, Cognition and Computation School of Informatics, University of Edinburgh 10 Crichton Street, Edinburgh EH8 9AB p.minervini@ed.ac.uk"
        },
        {
            "title": "Frank Keller",
            "content": "rohit.saxena@ed.ac.uk keller@inf.ed.ac.uk 5 2 0 2 4 2 ] . [ 1 0 4 5 7 1 . 2 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Generating accurate and concise textual summaries from multimodal documents is challenging, especially when dealing with visually complex content like scientific posters. We introduce POSTERSUM12, novel benchmark to advance the development of vision-language models that can understand and summarize scientific posters into research paper abstracts. Our dataset contains 16,305 conference posters paired with their corresponding abstracts as summaries. Each poster is provided in image format and presents diverse visual understanding challenges, such as complex layouts, dense text regions, tables, and figures. We benchmark state-of-the-art Multimodal Large Language Models (MLLMs) on POSTERSUM and demonstrate that they struggle to accurately interpret and summarize scientific posters. We propose SEGMENT & SUMMARIZE, hierarchical method that outperforms current MLLMs on automated metrics, achieving 3.14% gain in ROUGE-L. This will serve as starting point for future research on poster summarization."
        },
        {
            "title": "Introduction",
            "content": "Scientific posters play critical role in academic communication, offering visually rich medium that combines text, images, charts, and other graphical elements to present research findings. Summarizing these visually complex posters into concise and accurate textual abstracts presents unique challenge, requiring models to integrate multimodal information effectively. Multimodal Large Language Models (MLLMs; OpenAI et al., 2024; Grattafiori et al., 2024) have demonstrated remarkable capabilities in vision-andlanguage tasks, including image captioning (Fu et al., 2024; Koh et al., 2023; Yu et al., 2024; Garg et al., 2024) and visual question answering (Liu 1The dataset is available at rohitsaxena/PosterSum. 2The code is available at this link. et al., 2024e; Yue et al., 2024). While these models exhibit strong generalization across various domains, their performance often declines when applied to scientific text (Li et al., 2024; Lu et al., 2024; Pramanick et al., 2024). Additionally, the complexity of poster layouts, the use of technical terminology, and the intricate interplay between text, tables, and figures make summarizing scientific posters particularly challenging task, which has remained under-explored due to the lack of specialized datasets. To address this gap, we introduce POSTERSUM, novel multimodal benchmark for summarizing scientific posters into research paper abstracts. Our dataset consists of 16,305 scientific posters and corresponding abstracts as summaries collected from the main Machine Learning conferences, namely ICLR, ICML, and NeurIPS. These posters cover broad range of scientific disciplines and present unique challenges, including complex layouts and intricate combinations of text, tables, and figures as shown in Fig. 1. Information is often distributed across the poster, requiring careful navigation and integration of diverse elements to identify and summarize the key points effectively. We benchmark state-of-the-art MLLMs on POSTERSUM and demonstrate that, despite their impressive performance on range of other multimodal tasks, these models face significant limitations when summarizing scientific posters. For instance, the best-performing closed-source model in our experiments, GPT-4o (OpenAI et al., 2024), achieves ROUGE-L score of 22.30, underscoring the difficulty of this task specifically with the posters with figures and tables. To address this challenge, we propose SEGMENT & SUMMARIZE, hierarchical approach inspired by the divide-and-conquer principle (Chen and Zhao, 2023). The method involves three key steps: (1) Segmentation: we segment each poster into coherent regions; (2) Localized SummarizaFigure 1: An example of scientific poster from the POSTERSUM dataset. The poster, describing the work in Gupta et al. (2024), contains visual elements such as structured tables with numerical results, charts, diagrams, and textual sections, demonstrating the multimodal complexity present in the dataset. tion: multimodal large language model extracts and interprets the text within each segment, generating localized summaries for each region; and (3) Global Summarization: these localized summaries are combined using text-based large language model to produce cohesive abstract that spans the entire poster. Notably, this approach does not require additional training or fine-tuning. Local summaries allow the model to focus on fine details within that specific area, which are useful for tables and figures. Also, it aligns with the inherent structure of the poster, which has sections with specific focus. This approach achieves ROUGE-L score of 24.18, outperforming both closed-source and open-source models, setting new benchmark for scientific poster summarization. The proposed dataset and baselines will enable future research in multimodal scientific poster understanding. Our contributions can be summarized as follows: We introduce POSTERSUM, large-scale multimodal dataset of 16,305 scientific posters paired with their abstracts, tailored for research poster summarization. We benchmark state-of-the-art MLLMs on POSTERSUM, showing their limitations in processing and summarizing scientific posters. We propose SEGMENT & SUMMARIZE, hierarchical approach that segments each poster into coherent regions, extracts the textual content from those regions and then composes final summary; we also demonstrate POSTERSUMs utility for fine-tuning MLLMs, showing promising improvements over zero-shot results."
        },
        {
            "title": "2 Related Work",
            "content": "Multimodal Large Language Models. After the emergence of LLMs, recent work (Liu et al., 2023; Wang et al., 2024b; Alayrac et al., 2022) investigated their use in processing multimodal inputs, giving rise to Multimodal Large Language Models (MLLMs). The core idea in this line of research is to align visual and textual features by using shared representations. This framework typically involves using pre-trained visual encoder to extract visual features, projection layer to map visual representations into corresponding text representations, and pre-trained LLM to generate textual responses, allowing the model to condition the output on visual and textual inputs. MLLM architectures such as LLaVA (Liu et al., 2023) and MiniCPM (Yao et al., 2024) demonstrated impressive zero-shot generalization across diverse visual and language tasks. However, most existing MLLMs focus on general domain tasks and relatively simple visual inputs; the challenge of understanding complex and information-dense visual documents like scientific posters remains under-explored. Summarization in Scientific Domains. Scientific summarization consists of generating concise summaries for scientific content (Yasunaga et al., 2019; Cachola et al., 2020; Ju et al., 2021; Sotudeh and Goharian, 2022). Several scientific summarization benchmarks have been proposed, designed to process modalities such as videos (Lev et al., 2019; Chen et al., 2024), slide decks (Tanaka et al., 2023), surveys (Liu et al., 2024d), and research papers (Takeshita et al., 2024; Liu et al., 2024a). However, scientific poster summarization remains unexplored despite the widespread use of posters in academic communication. Document Layout Analysis and Segmentation. Understanding document layouts plays significant role in processing complex visual documents like scientific posters. Recent work in document layout analysis (Peng et al., 2022; Wang et al., 2024a; Luo et al., 2024; Appalaraju et al., 2024) aims at identifying and classifying different regions within document considering spatial relationships and content type. Previous work has also focused on understanding individual elements in documents, such as charts (Masry et al., 2022) and tables (Zheng et al., 2024). However, most existing approaches are designed for either standard documents or individual elements like charts and tables and do not capture the complex layouts and the rich multimodal structure of scientific posters, which typically consist of text, charts, equations, and tables."
        },
        {
            "title": "3 The POSTERSUM Dataset",
            "content": "We introduce POSTERSUM, novel dataset and benchmark for multimodal abstractive summarization of scientific posters. The dataset consists of 16,305 pairs of academic posters as images (PNG format) and their corresponding research paper abstracts. These posters were collected from major Figure 2: Distribution of the POSTERSUM dataset. machine learning and artificial intelligence conferences, which accept papers from various subfields of machine learning, including computer vision, natural language processing, optimization, and computational biology. POSTERSUM captures the diverse and heterogeneous nature of academic posters, which are commonly used at conferences to present research findings. These posters vary in layout, content, and visual complexitysome are text-heavy, while others emphasize visual elements such as charts, graphs, and figures, as shown in Fig. 1. This variability presents significant challenge for MLLMs, requiring them to interpret and summarize multimodal information effectively. Each poster in the dataset is paired with its corresponding abstract, which serves as the ground-truth summary. The abstract highlights the key contributions and findings of the research, making it an ideal summary for the poster. Unlike image captioning, poster summarization requires deeper understanding of multiple elements in the poster to generate comprehensive and meaningful abstractbased summary."
        },
        {
            "title": "3.1 Dataset Creation",
            "content": "The POSTERSUM dataset was collected from the websites of top-tier machine learning and artificial intelligence conferences: ICLR, ICML, and NeurIPS. We selected these conferences based on the availability of research posters. We first collected research paper links and paper identifiers from the conference websites. We filtered out any entries where the poster of the paper was not available, ensuring that only papers with accessi- % Novel n-grams in Summary 1-grams 2-grams 3-grams 4-grams 54.54 81.13 88.67 91.41 Table 2: Statistics for percentage of novel n-grams in the POSTERSUM summaries. erage of seven sentences per summary. The poster images are of high-resolution, with mean size of 3547 2454. We randomly split the dataset into training, validation, and test sets using 10305/3000/3000 split, which can be utilized for training and fine-tuning models. To better understand the diversity within the dataset, we categorized each poster into topics. Since topics were not available on the conference websites, we employed the GPT-4o vision model to generate topic labels by prompting the model in zero-shot setting using the images of the posters. As result, we identified 137 distinct topics within machine learning and artificial intelligence for the posters, spanning areas such as reinforcement learning, natural language processing (NLP), computational biology, and healthcare applications. Fig. 3 illustrates the distribution of the top 25 topics by frequency. To assess the abstractiveness of the poster summaries, we report the percentage of novel n-grams in the summaries compared to the Optical Character Recognition (OCR) extracted text from the posters. We used MMOCR (Kuang et al., 2021) to extract the text. While most posters do not explicitly include abstracts, we found that approximately 8% of the total posters may contain an abstract in poster, based on the occurrence of the word \"abstract\" in the OCR text. As shown in Table 2, significant portion of the summaries contains novel content, particularly in the 3-gram and 4-gram categories. This demonstrates that the summaries are not simple restatements of poster text but instead provide more comprehensive abstraction. We also find mean CLIP score (Hessel et al., 2021) of 29.08 when we evaluate the alignment between the images of the posters and their summaries. This score was computed at the sentence level and averaged across the dataset. The relatively low CLIP score highlights the challenge that POSTERSUM poses for existing MLLMs. Unlike image-captioning tasks, where captions directly describe visual features, academic posters are composed of diverse and complex visual elements, such Figure 3: Distribution of top 25 topics for the posters in the dataset. POSTERSUM Statistics Total number of posters-summary Total number of unique categories Mean token length of the summary Mean summary sentences Train/Val/Test size Mean CLIP score Year range 16,305 137 224 7.21 10305/3000/3000 29.08 20222024 Table 1: Statistics of the POSTERSUM dataset. ble posters were included in the dataset. We exclusively collected posters from the years 2022 to 2024, as shown in Fig. 2. Additionally, we manually reviewed the dataset to remove any posters with placeholder images. We assume that the research reported in the posters is of high standard, and the posters are of high quality, as the corresponding papers appeared at top machine learning conferences. To build robust summarization dataset, it was essential to pair each poster with human-written summary. We collected the research paper abstracts from the corresponding paper pages using the paper identifiers. These abstracts serve as the summaries for the posters, as they highlight the core findings and contributions of the research. For papers where the abstract was missing from the webpage, we manually extracted the abstract from the research papers PDF to ensure completeness."
        },
        {
            "title": "3.2 Dataset Statistics and Analysis\nThis process resulted in the 16,305 poster-summary\npairs, providing a comprehensive multimodal re-\nsource for evaluating abstractive summarization of\nacademic research posters.",
            "content": "Table 1 provides an overview of key statistics for the dataset. The average length of the poster summaries is 224 word-piece tokens, with an avas charts, graphs, equations, and dense textual explanations. This complexity makes it more difficult for models to capture the semantic relationships between these elements and the corresponding abstract summaries. as closed-source MLLMs. All the models were prompted with the image of the poster in zeroshot setting to generate abstractive summaries based on the input. The prompt template can be found in Appendix B."
        },
        {
            "title": "4.1 Task Formulation",
            "content": "Given scientific poster in image format as input, the objective is to generate textual summary ˆY = {ˆy1, ˆy2, . . . , ˆym} that encapsulates the key points and essential content of the poster. Formally, model Mθ, parameterized by θ, takes the poster as input, optionally accompanied by prompt , and generates summary ˆY . The key challenge in this task is that model Mθ must effectively abstract from the diverse visual and textual elements present in the poster, including text, charts, diagrams, and equations, to produce coherent and informative summary."
        },
        {
            "title": "4.2 Baselines",
            "content": "We evaluate various multimodal models, both opensource and closed-source, to assess their performance on the abstractive summarization task for scientific posters. As the posters include textual elements, we also evaluate OCR-based methods as baselines. For MLLMs, evaluation is conducted in zero-shot and Chain-of-Thought (CoT) setting to assess the capability of models to generate accurate summaries. Additionally, we explore parameterefficient fine-tuning techniques on selected opensource models. Below are the categories of models used in our experiments. Optical Character Recognition (OCR). For OCR-based baselines, we used two OCR methods (MMOCR (Kuang et al., 2021) and Pytesseract3) to extract text from the poster images and concatenated the results to generate summary. Additionally, we combined the best OCR output with text-based large language model (LLM). In this approach, we first extract text from the posters and then use the Llama-3.1-8B-Instruct (Grattafiori et al., 2024) model for summarization. This allows us to evaluate the performance of text-only LLMs when provided with OCR-extracted text. Closed-source MLLMs. We evaluated GPT4o (OpenAI et al., 2024), Claude 3.5 Sonnet (Anthropic, 2024), and Gemini 2.0 (Anil et al., 2024) 3https://github.com/h/pytesseract Open-source MLLMs. As open-source/openweights models, we evaluated Llama-3.2-11B- (Meta, 2024), Qwen2-VL-7BVision-Instruct Instruct (Yang et al., 2024), LLaVA-NeXT (Liu et al., 2024c,b), mPLUG-DocOwl2 (Hu et al., 2024), and MiniCPM-Llama3-V-2.5 (Yao et al., 2024). Each model was evaluated in both zero-shot and CoT settings. The CoT prompt was used to steer the models to extract relevant information, such as the title, research problem, methods, results, and conclusion, from the poster. We report the full prompt template in Appendix B. Fine-tuned Models (LoRA). We also evaluated the fine-tuned Llama-3.2-11B-Vision-Instruct and LLaVA-NeXT models. We used parameterefficient fine-tuning using the Low-rank Adaptation (LoRA; Hu et al., 2022) method to fine-tune both of these models using the training and validation set of the POSTERSUM dataset."
        },
        {
            "title": "4.3 SEGMENT & SUMMARIZE",
            "content": "We now introduce SEGMENT & SUMMARIZE, hierarchical approach inspired by the divide-andconquer principle. Rather than processing the entire poster as single input, SEGMENT & SUMMARIZE decomposes the task into three key steps: (1) Segmentation and Clustering (2) Localized Summarization, and (3) Global Summarization. The SEGMENT & SUMMARIZE pipeline is outlined in Fig. 4. 1. Segmentation and Clustering. Given the image of poster I, the first step is to segment it into coherent regions = {M1, M2, . . . , Mn}. This is achieved using segmentation model Sϕ, parameterized by ϕ, Since the number of regions can be large and can contain redundant and small segments, the regions are further clustered into groups with the number of the clustered regions as using clustering algorithm such that n. The clustering step groups similar regions together, reducing redundancy and ensuring complete coverage of the poster. Formally, = Sϕ(I) and = C(M ). By segmenting the poster and summarizing each region independently, the method ensures detailed Closed-Source Models Gemini Claude-3.5 Sonnet GPT-4o OCR Pytesseract MMOCR MMOCR + Llama Zero-Shot Llama-3.2-11B-V Qwen2-VL-7B LLaVA-NeXT mPLUG-DocOwl2 MiniCPM Chain of Thought Llama 3.2-11B-V Qwen2-VL-7B LLaVA-NeXT mPLUG-DocOwl2 MiniCPM Fine-tuning MLLMs Qwen2-VL-7B LLaVA-NeXT Llama-3.2-11B-V SEGMENT & SUMMARIZE R-1 R-2 R-L RLSum SBLEU Met BSp BSr BSf1 39.89 43.45 44.98 12.38 11.42 13.12 20.89 19.51 22.30 26.27 24.35 28. 1.03 8.96 5.37 9.26 12.73 15.49 20.7 20.63 29.89 35.62 39.88 20.05 25.58 30.25 37.04 41.50 4.29 1.93 6.61 8.79 11.11 3.4 2.92 6.16 9.15 11. 11.01 12.08 16.0 19.06 20.14 10.77 13.75 16.25 19.71 21.04 28.77 31.77 35.16 6.11 9.94 13.33 15.18 18.25 20.64 36.21 39.08 40. 17.07 23.4 24.94 18.88 18.97 27.02 32.07 35.45 18.14 23.24 27.48 33.45 37.08 26.32 28.7 31.75 6.57 7.72 10.05 22.34 28.43 30. 59.46 59.3 60.31 59.6 60.3 60.22 59.53 59.8 60.77 0.06 4.03 2.42 21.18 27.62 25.0 34.89 34.32 52. 41.15 49.39 56.88 37.71 40.40 54.58 1.75 0.63 3.41 3.36 7.18 1.7 1.52 2.95 3.98 8.60 18.07 16.13 19.57 18.35 23.76 8.57 15.65 24.53 19.6 26. 43.51 46.81 53.02 58.35 59.54 42.43 54.48 48.79 58.59 59.32 44.46 48.35 51.10 55.69 58.91 45.89 51.97 50.89 56.26 58.29 43.75 47.53 51.89 56.99 59.22 43.86 53.16 49.78 57.40 58. 2.66 6.21 8.91 19.09 27.18 28.32 53.78 51.29 50.65 51.99 58.89 58.82 52.83 54.67 54."
        },
        {
            "title": "Ours",
            "content": "46.68 15.73 24.18 42.5 12.63 30. 61.21 61.62 61.37 Table 3: Summarization results on the POSTERSUM dataset. The results show ROUGE scores (R-1, R-2, R-L, R-LSum), BERTScores (BSp, BSr, BSf1), SacreBLEU, and METEOR scores for all the baseline and models. All the scores are percentages. and accurate understanding of the content. Localized Summarization. For each 2. clustered region Ri, localized summary ˆYi = {ˆyi1, ˆyi2, . . . , ˆyik} is generated using an MLLM Vϕ. The model is used to extract and interpret the content within Ri, including text, figures, and tables, to generate localized summary for that specific region. This also helps in processing the high-resolution image. 3. Global Summarization. The localized summaries ˆY1, ˆY2, . . . , ˆYk are combined into cohesive global summary ˆY using text-based large language model Lω, parameterized by ω. The model Lω takes as input the individual summaries and generates single, well-structured output that represents the overall content of the poster. This step ensures that the final abstract is not only comprehensive but also maintains logical flow and coherence. Formally, ˆY = Lω( ˆY1, ˆY2, . . . , ˆYk). This processing pipeline helps summary generation through structured, localized, and hierarchical approach. By segmenting the poster and summarizing each region independently, the method captures fine-grained details that might be overlooked in global approach. This also aligns with the structure of these posters, which are mostly divided into sections. This approach does not require additional training or fine-tuning, and both the models (Vϕ, Lω) are frozen."
        },
        {
            "title": "5 Experimental Details",
            "content": "All the models in each category were evaluated using the same hyperparameter settings for fair evaluation. We generate at most 768 new tokens Figure 4: Illustration of our SEGMENT & SUMMARIZE pipeline. The poster, describing the work in Rakitin et al. (2024), is first divided into segments, each of which is summarized by MLLM. These localized summaries are subsequently merged by text-based large language model to generate single, coherent summary. for all the experiments. For closed-source models, we used the default platform settings. Open-source models were evaluated with beam size of 4 with greedy decoding to ensure reproducibility. The finetuning experiments were conducted for 10 epochs with batch size of 4. More details about the hyperparameters and prompt templates can be found in Appendices and E. For SEGMENT & SUMMARIZE, we used the Segment Anything Model (Kirillov et al., 2023) for segmentation with k-Means for clustering. The number of clusters (k) was set to 8 based on the analysis in Appendix D. We used MiniCPM-Llama3-V-2.5 as the local summarize (Vϕ) and Llama 3.1-8BInstruct as the global summarizer (Lω). We used the training set for fine-tuning and the validation set for hyperparameter tuning. All the final results are evaluated on the test set. Evaluation Metrics. We use ROUGE F1 (R-1/2/L/LSum) (Lin, 2004), SacreBLEU (SBLEU; Post, 2018), METEOR (MET; Banerjee and Lavie, 2005), and BERTScore (Zhang et al., 2020) to evaluate the accuracy of all models. scores"
        },
        {
            "title": "6 Results",
            "content": "Table 3 presents the poster summarization performance of all baselines alongside our proposed SEGMENT & SUMMARIZE method, evaluated on the POSTERSUM test set. Our method outperforms both open-source and closed-source models, achieving the best results across all metrics. Closed-source Models. GPT-4o achieves relatively high performance among the closed-source models across all metrics, with ROUGE-1/2/L scores of 44.98, 13.12, and 22.30, respectively. Claude-3.5 Sonnet also performs well, attaining ROUGE-L score of 19.51. OCR Baselines. The two OCR-based methods, MMOCR and Pytesseract, achieve relatively low scores across all metrics. This is likely due to the limitation of concatenating raw OCR text without leveraging other visual elements. Combining OCR with the text-only Llama-3.1 model results in substantial improvement, with ROUGE-L, increasing from 12.73 to 15.49. Interestingly, these OCR methods still outperform certain multimodal models, indicating that text extraction remains challenge for some MLLMs. Open-source Models. Among the open-source MLLMs evaluated in zero-shot settings, MiniCPMLlama3-V-2.5 obtains the highest ROUGE-1/L score (39.88/20.14) and strong BERTScore-F1 of 59.22. Meanwhile, mPLUG-DocOwl2 achieves competitive ROUGE-L of 19.06 and BERTScoreF1 of 56.99. Chain of Thought (CoT). Adding an explicit CoT prompt improves the performance of most models. For instance, MiniCPM-Llama3-V2.5 improves its ROUGE-1/L/METEOR scores to 41.50/21.04/26.34, while mPLUG-DocOwl2s performance also increases (ROUGE-1/L of 37.04/19.71). Additionally, LLaVA-NeXT and Qwen2-VL-7B exhibit similar gains. Although the performance boosts are not large, these results suggest that guiding models via CoT prompt can help in extracting relevant poster content. Fine-tuned Models. Using LoRA substantially boosts performance for both MLLMs. In particular, Llama-3.2-11B-Instruct demonstrates noMethods R1 R-2 R-L MET Without clustering With clustering 42.25 46.68 14.30 15.73 22.76 24.18 23.97 30.87 Table 4: Comparison of SEGMENT & SUMMARIZE with and without clustering clustering the segments yields more accurate results. Methods R1 R-2 R-L MET 37.04 mPLUG-DocOwl2 Ours with DocOwl2 42.48 Ours with MiniCPM 46.68 9.15 11.18 15.73 19.71 20.61 24. 19.6 26.72 30.87 Table 5: Comparison of using mPLUG-DocOwl2 as local summarize. Applying SEGMENT & SUMMARIZE shows improvement compared to using the model itself. table improvements in ROUGE, ScareBLEU, and METEOR scores, though it does not surpass the best CoT variants of MPLUG-DOCOWL2 and MINICPM-LLAMA3-V-2.5, which likely benefit from pre-training on multimodal scientific data. SEGMENT & SUMMARIZE. Our proposed method outperforms all other models, including closed-source models, on all metrics, achieving ROUGE-1/2/L scores of 46.68, 15.73, and 24.18, respectively, with 3.14% gain on ROUGE-L compared to open-source models. It also attains substantially higher ScareBLEU score (12.63) and BERTScore-F1 of 61.37. These results indicate that local-region summaries effectively preserve small details and handle posters of varying complexity by processing each region independently rather than attempting to analyze the entire poster as single input."
        },
        {
            "title": "7 Ablation Studies and Analysis",
            "content": "Effect of Clustering on Summarization. To quantify the impact of clustering in our SEGMENT & SUMMARIZE approach, we conduct an ablation study that removes the clustering step. Specifically, we select the top-k segments (with = 8) based on their region size to generate local and global summaries. Table 4 shows that clustering improves the ROUGE-1 score by +4.43, ROUGE-2 by +1.43, and ROUGE-L by +1.42 over the non-clustered baseline. We hypothesize that clustering helps reduce redundant segments and improves context aggregation. Effect of Local Vision Summarization. To assess the role of the local summarization model in SEGMENT & SUMMARIZE, we replaced MiniCPM-Llama3-V-2.5 with mPLUG-DocOwl2, which previously ranked second among opensource models under the CoT setting. Table 5 shows that using mPLUG-DocOwl2 with our hierarchical approach boosts ROUGE-1 to 42.48 and METEOR to 26.72 compared to using the model in the CoT setting. However, it does not outperform our method using MiniCPM. These findings highlight that the segmentation and summarization approach substantially improves performance compared to using the poster as single input. Challenges in Human Evaluation and Reliance on Automatic Metrics Evaluating scientific summaries against their posters is both costly and logistically complex for human annotators. Scientific posters consist of dense technical content (including specialized terminology, tables, figures, and equations), requiring domain expertise and making the recruitment of qualified annotators timeconsuming and expensive. Moreover, the diversity of research topics could lead to inconsistent judgments even among experts. For this reason, we rely on automatic metrics. Additionally, we conducted factuality evaluation, as discussed in Appendix A. However, existing factuality metrics, such as SummaC Conv (Laban et al., 2022) and FActScore (Min et al., 2023), perform poorly on scientific text, highlighting the need for improved evaluation methods for multimodal scientific data."
        },
        {
            "title": "8 Conclusions",
            "content": "We presented POSTERSUM, multimodal benchmark for scientific poster summarization comprising 16,305 poster-abstract pairs. Our experiments show that even state-of-the-art MLLMs struggle with key aspects of scientific poster summarization. Furthermore, we propose SEGMENT & SUMMARIZE, hierarchical approach that outperforms existing models by breaking down the summarization task into localized segments before generating cohesive abstract. We find that our method outperforms MLLMs in both zero-shot and fine-tuned settings and that there remains significant room for improvement in multimodal understanding of complex scientific documents such as posters. We believe POSTERSUM will be valuable resource for developing and evaluating MLLMs capable of processing information-dense scientific content."
        },
        {
            "title": "Acknowledgments",
            "content": "This work was supported in part by the School of Informatics at the University of Edinburgh. Pasquale Minervini was partially funded by ELIAI (The Edinburgh Laboratory for Integrated Artificial Intelligence), EPSRC (grant no. EP/W002876/1), and donation from Accenture LLP. This work was supported by the Edinburgh International Data Facility (EIDF) and the Data-Driven Innovation Programme at the University of Edinburgh."
        },
        {
            "title": "Limitations",
            "content": "While our work advances scientific poster summarization, we should highlight few limitations. First, our dataset is restricted to machine learning conference posters from 2022 to 2024, which may limit the generalization to other scientific domains. Second, while practical, automated topic labeling using GPT-4o may introduce biases or inaccuracies in the topic distribution. The proposed SEGMENT & SUMMARIZE method relies heavily on the quality of the initial segmentation suboptimal segmentation can lead to fragmented or redundant local summaries. Our method also assumes that the content can be meaningfully decomposed into spatial regions, which may not hold for posters with complex cross-referencing or interdependent visual elements. We considered the abstract as ground-truth summary of the poster, but the poster may sometimes differ from the paper."
        },
        {
            "title": "Ethics Statement",
            "content": "Dataset. All the scientific posters and abstracts in our dataset are sourced from publicly accessible conference resources. Additionally, we sought permission from the conference website contacts to use the publicly available data for research purposes. Multimodal Large Language Models. This paper utilizes pre-trained multimodal large language models, which have been shown to exhibit various biases, occasionally hallucinate, and generate non-faithful text. Therefore, summaries generated using our dataset should not be released without automatic filtering or manual verification to ensure accuracy and reliability. Bias. Despite efforts to include wide range of posters, the dataset may not fully represent the diversity of research poster styles, languages, or scientific disciplines. As result, models trained on POSTERSUM may exhibit biases towards the types of posters included in the dataset. Future work should consider expanding the dataset to encompass broader spectrum of academic fields and visual formats to mitigate potential biases."
        },
        {
            "title": "References",
            "content": "Natalie Abreu, Nathan Vaska, and Victoria Helus. 2022. Addressing mistake severity in neural networks with semantic knowledge. CoRR, abs/2211.11880. Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob L. Menick, Sebastian Borgeaud, and 8 others. 2022. Flamingo: visual language model for few-shot learning. In NeurIPS. Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M. Dai, Anja Hauth, Katie Millican, David Silver, Melvin Johnson, Ioannis Antonoglou, Julian Schrittwieser, Amelia Glaese, Jilin Chen, Emily Pitler, Timothy Lillicrap, Angeliki Lazaridou, Orhan Firat, and 1330 others. 2024. Gemini: family of highly capable multimodal models. Preprint, arXiv:2312.11805. Anthropic. 2024. Claude 3.5 - sonnet. https://www. anthropic.com/news/claude-3-5-sonnet. Accessed: 2024-12-06. Srikar Appalaraju, Peng Tang, Qi Dong, Nishant Sankaran, Yichu Zhou, and R. Manmatha. 2024. Docformerv2: Local features for document understanding. In Thirty-Eighth AAAI Conference on Artificial Intelligence, AAAI 2024, Thirty-Sixth Conference on Innovative Applications of Artificial Intelligence, IAAI 2024, Fourteenth Symposium on Educational Advances in Artificial Intelligence, EAAI 2014, February 20-27, 2024, Vancouver, Canada, pages 709718. AAAI Press. Satanjeev Banerjee and Alon Lavie. 2005. METEOR: An automatic metric for MT evaluation with improved correlation with human judgments. In Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization, pages 6572, Ann Arbor, Michigan. Association for Computational Linguistics. Isabel Cachola, Kyle Lo, Arman Cohan, and Daniel Weld. 2020. TLDR: Extreme summarization of scientific documents. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 47664777, Online. Association for Computational Linguistics. Chaoqi Chen, Luyao Tang, Feng Liu, Gangming Zhao, Yue Huang, and Yizhou Yu. 2022. Mix and reason: Reasoning over semantic topology with data mixing for domain generalization. In Advances in Neural Information Processing Systems. Shi Chen and Qi Zhao. 2023. Divide and conquer: Answering questions with object factorization and compositional reasoning. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2023, Vancouver, BC, Canada, June 17-24, 2023, pages 67366745. IEEE. Zhe Chen, Heyang Liu, Wenyi Yu, Guangzhi Sun, Hongcheng Liu, Ji Wu, Chao Zhang, Yu Wang, and Yanfeng Wang. 2024. M3av: multimodal, multigenre, and multipurpose audio-visual academic lecture dataset. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024, pages 90419060. Association for Computational Linguistics. Alexander Fabbri, Chien-Sheng Wu, Wenhao Liu, and Caiming Xiong. 2022. QAFactEval: Improved QAbased factual consistency evaluation for summarization. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 25872601, Seattle, United States. Association for Computational Linguistics. Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, Yunsheng Wu, and Rongrong Ji. 2024. Mme: comprehensive evaluation benchmark for multimodal large language models. Preprint, arXiv:2306.13394. Roopal Garg, Andrea Burns, Burcu Karagol Ayan, Yonatan Bitton, Ceslee Montgomery, Yasumasa Onoe, Andrew Bunner, Ranjay Krishna, Jason Michael Baldridge, and Radu Soricut. 2024. ImageInWords: Unlocking hyper-detailed image descriptions. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 93127, Miami, Florida, USA. Association for Computational Linguistics. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, and Abhishek Kadian et al. 2024. The llama 3 herd of models. Preprint, arXiv:2407.21783. Sharut Gupta, Stefanie Jegelka, David Lopez-Paz, and Kartik Ahuja. 2024. Context is environment. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net. Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. 2021. CLIPScore: reference-free evaluation metric for image captioning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 75147528, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics. Anwen Hu, Haiyang Xu, Liang Zhang, Jiabo Ye, Ming Yan, Ji Zhang, Qin Jin, Fei Huang, and Jingren Zhou. 2024. mplug-docowl2: High-resolution compressing for ocr-free multi-page document understanding. Preprint, arXiv:2409.03420. Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2022. Lora: Low-rank adaptation of large language models. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net. Jiaxin Ju, Ming Liu, Huan Yee Koh, Yuan Jin, Lan Du, and Shirui Pan. 2021. Leveraging information bottleneck for scientific document summarization. In Findings of the Association for Computational Linguistics: EMNLP 2021, pages 40914098, Punta Cana, Dominican Republic. Association for Computational Linguistics. Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloé Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Dollár, and Ross B. Girshick. 2023. Segment anything. In ICCV, pages 39924003. IEEE. Jing Yu Koh, Ruslan Salakhutdinov, and Daniel Fried. 2023. Grounding language models to images for multimodal inputs and outputs. In Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pages 1728317300. PMLR. Zhanghui Kuang, Hongbin Sun, Zhizhong Li, Xiaoyu Yue, Tsui Hin Lin, Jianyong Chen, Huaqiang Wei, Yiqin Zhu, Tong Gao, Wenwei Zhang, Kai Chen, Wayne Zhang, and Dahua Lin. 2021. Mmocr: comprehensive toolbox for text detection, recognition and understanding. In Proceedings of the 29th ACM International Conference on Multimedia, MM 21, page 37913794, New York, NY, USA. Association for Computing Machinery. Philippe Laban, Tobias Schnabel, Paul N. Bennett, and Marti A. Hearst. 2022. SummaC: Re-visiting NLIbased models for inconsistency detection in summarization. Transactions of the Association for Computational Linguistics, 10:163177. Guy Lev, Michal Shmueli-Scheuer, Jonathan Herzig, Achiya Jerbi, and David Konopnicki. 2019. TalkSumm: dataset and scalable annotation method for scientific paper summarization based on conference talks. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 21252131, Florence, Italy. Association for Computational Linguistics. Lei Li, Yuqi Wang, Runxin Xu, Peiyi Wang, Xiachong Feng, Lingpeng Kong, and Qi Liu. 2024. Multimodal ArXiv: dataset for improving scientific comprehension of large vision-language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1436914387, Bangkok, Thailand. Association for Computational Linguistics. Chin-Yew Lin. 2004. ROUGE: package for automatic evaluation of summaries. In Text Summarization Branches Out, pages 7481, Barcelona, Spain. Association for Computational Linguistics. Dongqi Liu, Yifan Wang, Jia Loy, and Vera Demberg. 2024a. SciNews: From scholarly complexities to public narratives dataset for scientific news report generation. In Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LRECCOLING 2024), pages 1442914444, Torino, Italia. ELRA and ICCL. Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. 2024b. Improved baselines with visual instruction tuning. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2024, Seattle, WA, USA, June 16-22, 2024, pages 2628626296. IEEE. Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. 2024c. Llavanext: Improved reasoning, ocr, and world knowledge. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2023. Visual instruction tuning. In Advances in Neural Information Processing Systems, volume 36, pages 3489234916. Curran Associates, Inc. Ran Liu, Ming Liu, Min Yu, He Zhang, Jianguo Jiang, Gang Li, and Weiqing Huang. 2024d. SumSurvey: An abstractive dataset of scientific survey papers for long document summarization. In Findings of the Association for Computational Linguistics: ACL 2024, pages 96329651, Bangkok, Thailand. Association for Computational Linguistics. Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, Kai Chen, and Dahua Lin. 2024e. Mmbench: Is your multi-modal model an all-around player? In Computer Vision ECCV 2024: 18th European Conference, Milan, Italy, September 29October 4, 2024, Proceedings, Part VI, page 216233, Berlin, Heidelberg. Springer-Verlag. Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, KaiWei Chang, Michel Galley, and Jianfeng Gao. 2024. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. In The Twelfth International Conference on Learning Representations. Chuwei Luo, Yufan Shen, Zhaoqing Zhu, Qi Zheng, Zhi Yu, and Cong Yao. 2024. Layoutllm: Layout instruction tuning with large language models for document understanding. In CVPR, pages 15630 15640. IEEE. Ahmed Masry, Xuan Long Do, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. 2022. ChartQA: benchmark for question answering about charts with visual and logical reasoning. In Findings of the Association for Computational Linguistics: ACL 2022, pages 2263 2279, Dublin, Ireland. Association for Computational Linguistics. AI Meta. 2024. Llama 3.2: Revolutionizing edge ai and vision with open, customizable models. Meta AI Blog. Retrieved December, 20:2024. Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen-tau Yih, Pang Koh, Mohit Iyyer, Luke Zettlemoyer, and Hannaneh Hajishirzi. 2023. FActScore: Fine-grained atomic evaluation of factual precision in long form text generation. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 1207612100, Singapore. Association for Computational Linguistics. OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, and Lama Ahmad et al. 2024. Gpt-4 technical report. Preprint, arXiv:2303.08774. Qiming Peng, Yinxu Pan, Wenjin Wang, Bin Luo, Zhenyu Zhang, Zhengjie Huang, Yuhui Cao, Weichong Yin, Yongfeng Chen, Yin Zhang, Shikun Feng, Yu Sun, Hao Tian, Hua Wu, and Haifeng Wang. 2022. Ernie-layout: Layout knowledge enhanced pretraining for visually-rich document understanding. In Findings of the Association for Computational Linguistics: EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022, pages 37443756. Association for Computational Linguistics. Matt Post. 2018. call for clarity in reporting BLEU scores. In Proceedings of the Third Conference on Machine Translation: Research Papers, pages 186 191, Belgium, Brussels. Association for Computational Linguistics. Shraman Pramanick, Rama Chellappa, and Subhashini Venugopalan. 2024. SPIQA: dataset for multimodal question answering on scientific papers. In The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track. Denis Rakitin, Ivan Shchekotov, and Dmitry Vetrov. 2024. Regularized distribution matching distillation for one-step unpaired image-to-image translation. In ICML 2024 Workshop on Structured Probabilistic Inference & Generative Modeling. Rohit Saxena and Frank Keller. 2024. Select and summarize: Scene saliency for movie script summarization. In Findings of the Association for Computational Linguistics: NAACL 2024, pages 34393455, Mexico City, Mexico. Association for Computational Linguistics. Sajad Sotudeh and Nazli Goharian. 2022. TSTR: Too short to represent, summarize with details! introguided extended summary generation. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 325 335, Seattle, United States. Association for Computational Linguistics. on Computer Vision and Pattern Recognition (CVPR), pages 1402214032. Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, Cong Wei, Botao Yu, Ruibin Yuan, Renliang Sun, Ming Yin, Boyuan Zheng, Zhenzhu Yang, Yibo Liu, Wenhao Huang, and 3 others. 2024. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 95569567. Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi. 2020. Bertscore: EvalIn International uating text generation with bert. Conference on Learning Representations. Mingyu Zheng, Xinwei Feng, Qingyi Si, Qiaoqiao She, Zheng Lin, Wenbin Jiang, and Weiping Wang. 2024. In Proceedings Multimodal table understanding. of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 91029124, Bangkok, Thailand. Association for Computational Linguistics. Sotaro Takeshita, Tommaso Green, Ines Reinig, Kai Eckert, and Simone Ponzetto. 2024. ACLSum: new dataset for aspect-based summarization of scientific publications. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 66606675, Mexico City, Mexico. Association for Computational Linguistics. Ryota Tanaka, Kyosuke Nishida, Kosuke Nishida, Taku Hasegawa, Itsumi Saito, and Kuniko Saito. 2023. Slidevqa: dataset for document visual question answering on multiple images. Proceedings of the AAAI Conference on Artificial Intelligence, 37(11):1363613645. Dongsheng Wang, Natraj Raman, Mathieu Sibue, Zhiqiang Ma, Petr Babkin, Simerjot Kaur, Yulong Pei, Armineh Nourbakhsh, and Xiaomo Liu. 2024a. DocLLM: layout-aware generative language model for multimodal document understanding. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 85298548, Bangkok, Thailand. Association for Computational Linguistics. Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji Qi, Yan Wang, Junhui Ji, Zhuoyi Yang, Lei Zhao, Song XiXuan, Jiazheng Xu, Keqin Chen, Bin Xu, Juanzi Li, Yuxiao Dong, Ming Ding, and Jie Tang. 2024b. Cogvlm: Visual expert for preIn Advances in Neural trained language models. Information Processing Systems, volume 37, pages 121475121499. Curran Associates, Inc. An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, and Chang Zhou et al. 2024. Qwen2 technical report. Preprint, arXiv:2407.10671. Yuan Yao, Tianyu Yu, Ao Zhang, Chongyi Wang, Junbo Cui, Hongji Zhu, Tianchi Cai, Haoyu Li, Weilin Zhao, Zhihui He, Qianyu Chen, Huarong Zhou, Zhensheng Zou, Haoye Zhang, Shengding Hu, Zhi Zheng, Jie Zhou, Jie Cai, Xu Han, and 4 others. 2024. Minicpmv: gpt-4v level mllm on your phone. Preprint, arXiv:2408.01800. Michihiro Yasunaga, Jungo Kasai, Rui Zhang, Alexander R. Fabbri, Irene Li, Dan Friedman, and Dragomir R. Radev. 2019. Scisummnet: large annotated corpus and content-impact models for scientific paper summarization with citation networks. In The Thirty-Third AAAI Conference on Artificial Intelligence, AAAI 2019, The Thirty-First Innovative Applications of Artificial Intelligence Conference, IAAI 2019, The Ninth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2019, Honolulu, Hawaii, USA, January 27 - February 1, 2019, pages 73867393. AAAI Press. Qiying Yu, Quan Sun, Xiaosong Zhang, Yufeng Cui, Fan Zhang, Yue Cao, Xinlong Wang, and Jingjing Liu. 2024. Capsfusion: Rethinking image-text data at scale. In Proceedings of the IEEE/CVF Conference"
        },
        {
            "title": "SummaC FactScore",
            "content": "Prompt Template for CoT Llama-3.2 FT MiniCPM CoT GPT-4o Ours 21.55 23.03 23.25 25.41 76.67 92.10 94.44 94.67 Table 6: Results of automatic evaluation of factual consistency on the best model in each category"
        },
        {
            "title": "A Factuality Evaluation",
            "content": "To evaluate the performance of our method in generating factually correct summaries, we compute two text-based metrics: SummaC Conv (Laban et al., 2022) and FActScore (Min et al., 2023) on the best models in each category. Following common practice in long-document summarization evaluation (Fabbri et al., 2022; Saxena and Keller, 2024), we treat the reference summary as the ground truth (instead of the original document, which is poster image) when computing these metrics. Table 6 presents the results for both metrics on the generated summaries. Both metrics perform poorly, as they are not specialized for scientific text. SummaC scores were substantially low, while FActScore showed extremely high values, indicating failures in natural language inference and atomic fact extraction for scientific text. We found factuality evaluation to be challenging task in this domain, highlighting the need for new methods to measure factual accuracy in multimodal scientific documents such as posters."
        },
        {
            "title": "B Prompt Templates",
            "content": "Prompt Template for Zero-Shot Write an abstract for an AI conference paper for the given research poster image."
        },
        {
            "title": "C Effect of Poster Text Content on\nSummarization Performance",
            "content": "To investigate whether posters with high amount of text result in better summarization performance, we analyze the relationship between OCRextracted text length and ROUGE-L scores using our SEGMENT & SUMMARIZE method. Specifically, we use MMOCR to extract text from each Analyze the research poster image step by step. First, identify the title and main research problem. Then, briefly describe the methodology used. Next, summarize the key findings or results. Finally, note the conclusion or implications. Using this information, write an abstract for the given research poster image."
        },
        {
            "title": "Prompt Template for Local Summary",
            "content": "Describe all the text, tables, figures, and equations in the image. Figure 5: Effect of text present in the poster on summarization. We report mean ROUGE-L scores for different OCR-extracted character-length bins. The red dashed line represents the number of posters in each bin. poster and compute its total length in characters (not in tokens). Fig. 5 presents the mean ROUGE-L scores across different OCR text-length bins. The dotted line represents the number of posters in each text-length bin. We observe that summarization performance tends to improve as the amount of text in poster increases. However, the correlation remains weak (Pearson = 0.213, Spearman = 0.210), suggesting that text in poster alone is not strong predictor of summarization quality. Low performance in posters with minimal text also highlights the need for more robust multimodal understanding of figures, charts, equations, and tables."
        },
        {
            "title": "D Selecting the Number of Clusters",
            "content": "To select the number of clusters (k) for our SEGMENT & SUMMARIZE, we conducted an empirical"
        },
        {
            "title": "Summaries",
            "content": "Figure 6: Effect of varying the number of clusters on ROUGE-L performance on SEGMENT & SUMMARIZE Model Version GPT-4o Gemini 2.0 Claude 3.5 Sonnet gpt-4o-2024-08-06 gemini-2.0-flash-exp claude-3-5-sonnet-20241022 Table 7: Details of the closed-sourced models. analysis on subset of 100 posters from the validation set, varying the number of clusters from 2 to 10. Fig. 6 presents the mean ROUGE-L score for each cluster configuration. In these experiments, the local and global summarization components remained fixed. We observe that the best performance is achieved at = 8 which was used in our final experiments. Additionally, we limit the maximum number of clusters to 10 in the analysis to keep the inference time of our local summarization manageable."
        },
        {
            "title": "E Additional Experiment Details",
            "content": "Table 7 summarizes the versions of the closedsource models used in our experiments. For finetuning, we use learning rate of 1 104 with the Adam optimizer (β1 = 0.9, β2 = 0.999, ϵ = 1 108) and cosine learning rate schedule. We employ LoRA with rank = 8, α = 8, and dropout rate of 0.1. All images are processed and scaled by the respective models image processor for model specific sizes. In the case of closed-source models, we scale each image to maximum width of 2048 while preserving the original aspect ratio due to size limitations. All the models were trained using 2 A100 GPU with 80GB memory. We used the Huggingface evaluate library for the implementation of the metrics. Model Reference MiniCPM CoT Llama-3.2-11B-V FT GPT-4o SEGMENT & SUMMARIZE Output Domain generalization (DG) enables generalizing learning machine from multiple seen source domains to an unseen target one. The general objective of DG methods is to learn semantic representations that are independent of domain labels, which is theoretically sound but empirically challenged due to the complex mixture of common and domain-specific factors. Although disentangling the representations into two disjoint parts has been gaining momentum in DG, the strong presumption over the data limits its efficacy in many real-world scenarios. In this paper, we propose Mix and Reason (MiRe), new DG framework that learns semantic representations via enforcing the structural invariance of semantic topology. MiRe consists of two key components, namely, Category-aware Data Mixing (CDM) and Adaptive Semantic Topology Refinement (ASTR). CDM mixes two images from different domains in virtue of activation maps generated by two complementary classification losses, making the classifier focus on the representations of semantic objects. ASTR introduces relation graphs to represent semantic topology, which is progressively refined via the interactions between local feature aggregation and global cross-domain relational reasoning. Experiments on multiple DG benchmarks validate the effectiveness and robustness of the proposed MiRe. The paper is about Mix and Reason, methodology for domain generalization that reasons over semantic topology to mix data from different domains and construct new target domains. It addresses the problem of limited training data by leveraging the IID assumption and the representations learned from the source domain to perform well in the target domain. The proposed methodology involves pipeline of the proposed Mix and Reason (MiRe) component, which takes the adaptive semantic topology refinement (ASTR) component as its backbone. The key results show that the proposed methodology outperforms state-of-the-art methods on four benchmark datasets, including CelebA, CUB, Stanford 2007, and COCO. Deep neural networks (DNNs) are expected to be deployed across novel domains. However, out-ofdistribution (OOD) data does not satisfy the assumption and hinders DNNs from generalizing well to target domains. In this paper, we propose Mix and Reason (MIRa) for domain generalization (DG) via perceiving and reasoning over semantic topology. Specifically, MIRa divides input common and domain-specific components and captures their potential spurious correlations. Reasoning over semantic topology enables MIRa to disentangle attribute and domain-specific representations and learn domain-invariant representations. Extensive experiments on four standard DG benchmarks show that MIRa achieves state-of-the-art performance. Besides, MIRa can be applied to many DG methods and significantly improve their performance. Domain Generalization (DG) for deep neural networks poses significant challenge in adapting models to varied out-of-distribution data. This paper introduces \"Mix and Reason\" (MiRe), novel framework leveraging semantic topology reasoning and data mixing to enhance domain generalization. MiRe employs two-part approach, utilizing Anchored Semantic Topology Reasoning (ASTR) to retain relational features across domains and Contrastive Data Mixing (CDM) to generate robust feature representations. Experiments were conducted on four standard DG benchmarks, demonstrating substantial improvements in model adaptability and performance. Additionally, results on medical datasets underscore the frameworks efficacy in maintaining domain-invariant characteristics across diverse conditions. This research highlights the strength of integrating semantic relationships with innovative data augmentation techniques, offering scalable solution for realistic multi-domain scenarios. Domain Generalization (DG) is critical challenge in machine learning, where models are expected to generalize well across novel domains with different data distributions. Existing DG methods often rely on strong assumptions about the data or ignore potential spurious correlations. In this work, we propose novel DG framework, Mix and Reason (MiRe), which learns semantic representations by enforcing the structural invariance of semantic topology. MiRe consists of two key components: Category-Aware Data Mixing (CDM) and Adaptive Logic Semantic Topology Refinement (ASTR). CDM mixes two images from different domains using activation maps generated by complementary classification losses, focusing the classifier on semantic object representations. ASTR introduces relation graphs to represent semantic topology, which is progressively refined via interactions between local feature aggregation and global cross-domain relational reasoning. We evaluate MiRe on four standard DG benchmarks and medical data, achieving state-of-the-art performance and outperforming existing methods. Our results demonstrate the effectiveness of MiRe in handling domain shifts and provide insights into how to better perceive and maintain structural semantic relations in machine learning. Table 8: Sample of poster image from Chen et al. (2022) with gold reference and model generated summaries Model Reference MiniCPM CoT Llama-3.2-11B-V FT GPT-4o SEGMENT & SUMMARIZE Output Robustness in deep neural networks and machine learning algorithms in general is an open research challenge. In particular, it is difficult to ensure algorithmic performance is maintained on out-of-distribution inputs or anomalous instances that cannot be anticipated at training time. Embodied agents will be deployed in these conditions, and are likely to make incorrect predictions. An agent will be viewed as untrustworthy unless it can maintain its performance in dynamic environments. Most robust training techniques aim to improve model accuracy on perturbed inputs; as an alternate form of robustness, we aim to reduce the severity of mistakes made by neural networks in challenging conditions. We leverage current adversarial training methods to generate targeted adversarial attacks during the training process in order to increase the semantic similarity between models predictions and true labels of misclassified instances. Results demonstrate that our approach performs better with respect to mistake severity compared to standard and adversarially trained models. We also find an intriguing role that non-robust features play with regards to semantic similarity. The paper is about the problem of mistake severity in neural networks and the use of semantic knowledge to address it. The research problem is the inability of neural networks to accurately predict the severity of mistakes, leading to poor model accuracy. The methodology involves incorporating semantic knowledge into the training process to generate targeted adversarial attacks. The key results show that using semantic knowledge to guide the attacks leads to better model accuracy and improved performance on benchmarks. The conclusion suggests that the use of semantic knowledge is promising approach to addressing the problem of mistake severity in neural networks. It is difficult to ensure algorithmic performance is maintained at out-of-distribution (OOD) training time for neural networks due to the presence of perturbed inputs. An alternate form of robustness, we aim to reduce the conditions in which mistakes are made by neural networks. We leverage semantic similarity between models predictions and labels of OOD data as measure of mistake severity. We propose two-stage training method that generates targeted perturbations of semantic similarity between models predictions and true labels of OOD data. Our experiments show that our method improves upon state-of-the-art methods in aligning class similarity between models predictions and true labels of OOD data. Our code is available at https://github.com/natale-abreu/semitargeted-adversarial-training.git. This paper addresses the challenge of reducing mistake severity in neural networks under adversarial and anomalous conditions by incorporating semantic knowledge into the training process. Traditional robust training techniques typically focus on improving model accuracy on perturbed inputs, but often overlook the semantic alignment between model predictions and true labels. We propose novel method utilizing semantically targeted adversarial attacks to train neural networks, aiming to enhance semantic similarity and reduce the impact of severe errors.Our approach encompasses two-stage training process: firstly integrating semantic knowledge through targeted adversarial perturbations, followed by standard training protocols. The methodology was tested using metrics such as super class accuracy on mistakes and average path similarity of mistakes, which measure semantic closeness between mistaken and true labels. This research highlights the value of non-robust features in capturing semantic alignments and suggests future exploration into balancing visual and semantic similarities for enhanced model reliability. Ensuring algorithmic performance on out-of-distribution inputs or anomalous instances is significant challenge in neural networks. Traditional robust training techniques focus on improving model accuracy on perturbed inputs, whereas we propose an alternate form of robustness by reducing the severity of mistakes made by neural networks in challenging conditions. We leverage current adversarial training methods to generate targeted adversarial attacks during the training process, increasing the semantic similarity between models predictions and true labels of misclassified instances. Our approach involves two stages of training: Semantic Targeting and Prior Semantic Knowledge. We demonstrate that by incorporating semantic knowledge in the training process, we can reduce the severity of mistakes in challenging conditions, thereby improving user trust in the system. Our results show that the proposed method outperforms traditional robust training techniques in terms of reducing mistake severity, making it promising approach for addressing mistake severity in neural networks. Table 9: Sample of poster image from the work Abreu et al. (2022) with gold reference and model generated summaries"
        }
    ],
    "affiliations": [
        "Institute for Language, Cognition and Computation, School of Informatics, University of Edinburgh"
    ]
}