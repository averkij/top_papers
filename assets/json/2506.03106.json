{
    "paper_title": "Critique-GRPO: Advancing LLM Reasoning with Natural Language and Numerical Feedback",
    "authors": [
        "Xiaoying Zhang",
        "Hao Sun",
        "Yipeng Zhang",
        "Kaituo Feng",
        "Chaochao Lu",
        "Chao Yang",
        "Helen Meng"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advances in reinforcement learning (RL) with numerical feedback, such as scalar rewards, have significantly enhanced the complex reasoning capabilities of large language models (LLMs). Despite this success, we identify three key challenges encountered by RL with solely numerical feedback: performance plateaus, limited effectiveness of self-reflection, and persistent failures. We then demonstrate that RL-finetuned models, even after exhibiting performance plateaus, can generate correct refinements on persistently failed problems by leveraging natural language feedback in the form of critiques. Building on this insight, we propose Critique-GRPO, an online RL framework that integrates both natural language and numerical feedback for effective policy optimization. Critique-GRPO enables LLMs to learn from initial responses and critique-guided refinements simultaneously while maintaining exploration. Extensive experiments using Qwen2.5-7B-Base and Qwen3-8B-Base show that Critique-GRPO consistently outperforms supervised learning-based and RL-based fine-tuning approaches across eight challenging mathematical, STEM, and general reasoning tasks, improving average pass@1 scores by approximately 4.5% and 5%, respectively. Notably, Critique-GRPO surpasses a strong baseline that incorporates expert demonstrations within online RL. Further analysis reveals two critical insights about policy exploration: (1) higher entropy does not always guarantee efficient learning from exploration, and (2) longer responses do not necessarily lead to more effective exploration."
        },
        {
            "title": "Start",
            "content": "Critique-GRPO: Advancing LLM Reasoning with Natural Language and Numerical Feedback Xiaoying Zhang14, Hao Sun2, Yipeng Zhang1, Kaituo Feng3 Chaochao Lu4, Chao Yang4, Helen Meng1 1The Chinese University of Hong Kong, HCCL 3The Chinese University of Hong Kong, MMLab 2University of Cambridge 4Shanghai Artificial Intelligence Laboratory 5 2 0 J 4 ] . [ 2 6 0 1 3 0 . 6 0 5 2 : r zhangxy@se.cuhk.edu.hk, hs789@cam.ac.uk, yipengzhang97@gmail.com https://github.com/zhangxy-2019/critique-GRPO"
        },
        {
            "title": "Abstract",
            "content": "Recent advances in reinforcement learning (RL) with numerical feedback, such as scalar rewards, have significantly enhanced the complex reasoning capabilities of large language models (LLMs). Despite this success, we identify three key challenges encountered by RL with solely numerical feedback: performance plateaus, limited effectiveness of self-reflection, and persistent failures. We then demonstrate that RL-finetuned models, even after exhibiting performance plateaus, can generate correct refinements on persistently failed problems by leveraging natural language feedback in the form of critiques. Building on this insight, we propose CritiqueGRPO, an online RL framework that integrates both natural language and numerical feedback for effective policy optimization. Critique-GRPO enables LLMs to learn from initial responses and critique-guided refinements simultaneously while maintaining exploration. Extensive experiments using Qwen2.5-7B-Base and Qwen3-8B-Base show that Critique-GRPO consistently outperforms supervised learning-based and RL-based fine-tuning approaches across eight challenging mathematical, STEM, and general reasoning tasks, improving average pass@1 scores by approximately 4.5% and 5%, respectively. Notably, Critique-GRPO surpasses strong baseline that incorporates expert demonstrations within online RL. Further analysis reveals two critical insights about policy exploration: (i) higher entropy does not always guarantee efficient learning from exploration, and (ii) longer responses do not necessarily lead to more effective exploration."
        },
        {
            "title": "Introduction",
            "content": "Reinforcement learning (RL) has been key driver of recent advancements in enhancing the reasoning capabilities of large language models (LLMs) [1, 2, 3, 4]. In particular, reinforcement learning with numerical feedback, typically in the form of scalar rewards and often referred to as the R1-Zero training paradigm [2], enables base LLMs to learn from their own generations through trial-anderror learning. High-quality generations are rewarded positively, while low-quality generations are penalized. This paradigm has revolutionized the post-training pipeline for LLMs, shifting from imitation learning of expert demonstrations to learning from the models own generations (i.e., experiences) [5, 6], resulting in significant performance improvements. Despite recent advancements, our analysis of Qwen2.5-7B-Base [7] and Qwen3-8B-Base [1] in Section 3 highlights three significant challenges faced by RL with solely numerical feedback: (i) Project lead, core contributor. Work done during the internship at Shanghai AI Laboratory. Second authors. Corresponding authors. Preprint. Under review. Figure 1: (a) Critique-GRPO enhances online reinforcement learning by enabling the model to learn from both its initial responses and subsequent refinements through critiques (natural language feedback), highlighted in green, rather than relying solely on scalar rewards (numerical feedback). (b) Critique-GRPO improves average pass@1 scores by approximately 6.5% and 5.1% on five challenging mathematical reasoning (in-distribution) tasks and three STEM & general reasoning (out-of-distribution) tasks for Qwen2.5-7B-Base and Qwen3-8B-Base, respectively. Performance Plateaus: Scaling the number of training examples by 8x (from 4k to 32k) fails to improve peak performance. (ii) Limited Effectiveness of Self-Reflection: Increased self-reflection behaviors during RL finetuning, often touted as crucial Aha moments, contribute minimally to successful problem-solving. (iii) Persistent Failures: The models exhibit persistent failures on certain problems despite extensive trial-and-error finetuning. We hypothesize that key cause of these plateaus and persistent failures is the limited information contained within numerical feedback regarding why response is correct or incorrect and how to improve it. Furthermore, the limited effectiveness of self-reflection behaviors compounds these challenges. Together, these limitations underscore the need for external oversight or richer feedback mechanisms to support more effective learning. To address these limitations, natural language feedback (NLF), typically in the form of textual critiques, offers promising avenue. NLF provides detailed and targeted insights into flaws in model-generated outputs, enabling both accurate evaluation and effective response refinement [8, 9]. However, existing approaches often fail to fully exploit the potential of textual critiques. Many studies [10, 11, 12, 13, 14] primarily use critiques for evaluation, transforming them into numerical rewards for model improvement via RL algorithms such as Proximal Policy Optimization (PPO) [15] or Group Relative Policy Optimization (GRPO) [16]. This transformation often discards valuable constructive information embedded within the critiques. Some studies [9, 17] utilize critiques to generate refinements and fine-tune models on these refinements through supervised learning. While effective, these offline approaches are limited by their inability to support consistent exploration and online refinement. This raises key research question: Can we incorporate critiques into an online reinforcement learning framework to enable LLMs to spontaneously learn from both initial generations and refinements? To address the challenges of RL with solely numerical feedback, we first examine the potential of RL-finetuned models (which exhibit performance plateaus) to refine their responses using critiques. Specifically, we assess whether these models can generate correct refinements on persistently failed problems by leveraging critiques. The results in Section 3 demonstrate that RL-finetuned models exhibit effective refinements when provided with chain-of-thought (CoT) critiques [17, 11], which offer step-by-step analysis of whether the generated response is correct or not. Building on this, we propose Critique-GRPO, novel framework that enables LLMs to learn from both natural language feedback (NLF) and numerical feedback during online RL for effective policy optimization. As illustrated in Figure 1(a), Critique-GRPO allows the model to learn from both initial sampled responses and their refinements using critiques generated by reasoning-based reward model (highlighted in green). This approach encourages the model to integrate targeted refinements while preserving exploration. Additionally, shaping function [18] is applied to enhance learning from valid refinements and heavily penalize failed refinements, which often contain unresolved errors. 2 We evaluate Critique-GRPO on Qwen2.5-7B-Base [7] and Qwen3-8B-Base [1] across five challenging mathematical reasoning (in-distribution) tasks: MATH-500 [19], Minerva-Math [20], OlympiadBench [21], MATH [19], AIME 2024 [22], and AMC 2023 [22], using 4k prompts with mathematical questions. We also assess generalization capability on three scientific and general reasoning (out-of-distribution) tasks: TheoremQA [23], GPQA-Diamond [24], and MMLU-Pro [25]. The results demonstrate that Critique-GRPO significantly outperforms both supervised learning-based and RL-based fine-tuning methods across all tasks, achieving an average pass@1 improvement of approximately 4.5% and 5%, respectively, while promoting concise reasoning. Notably, CritiqueGRPO surpasses LUFFY [18], which incorporates expert demonstrations within online RL to boost reasoning. This suggests that learning from self-generated, online refinements is more effective than imitating expert demonstrations. Furthermore, our in-depth analysis of policy exploration reveals two key findings: (i) Higher entropy does not always guarantee efficient learning from exploration. (ii) Longer responses do not necessarily lead to more effective exploration. In summary, our contributions are three-fold: We conduct an in-depth analysis to identify three key challenges of RL using solely numerical feedback and highlight the potential of natural language feedback to address these limitations. We propose Critique-GRPO, framework that enables LLMs to simultaneously learn from both initial responses and their refinements during online RL by leveraging natural language and numerical feedback. We validate the efficacy of Critique-GRPO through extensive experiments, demonstrating its superior performance across eight mathematical and general reasoning tasks, and provide insights into efficient policy exploration."
        },
        {
            "title": "2 Related Work",
            "content": "Enhancing LLM Reasoning with Reinforcement Learning. Reinforcement Learning (RL) has proven highly effective in enhancing the reasoning abilities of LLMs [3, 2, 26, 27]. This is typically achieved by fine-tuning models on complex reasoning tasks to incentivize diverse reasoning behaviors [28, 29]. Recent advancements have utilized RL with numerical feedback (e.g., +1 for correct responses, -1 for incorrect ones) [3, 2, 30, 31]. These methods often leverage online policy optimization algorithms such as Proximal Policy Optimization (PPO) [15], Group Relative Policy Optimization (GRPO) [16], REINFORCE [32], and Decoupled Clip and Dynamic Sampling Policy Optimization (DAPO) [31]. However, numerical feedback is inherently sparse, and models frequently struggle with tasks that extend beyond their current knowledge boundaries, limiting their ability to achieve meaningful improvement [33, 28]. While recent approaches address this limitation by incorporating high-quality expert demonstrations alongside online exploration [18], our approach distinctly enables models to refine their outputs by incorporating textual feedback (e.g., CoT critiques) that directly addresses potential errors. This integration of textual feedback with online exploration for policy optimization results in superior performance. Learning from Natural Language Feedback. Natural Language Feedback (NLF), provided in the form of textual critiques, offers powerful mechanism for improving LLMs. NLF provides detailed and targeted insights into flaws in model-generated outputs, enabling both accurate evaluation and/or response refinement [8, 9]. Many existing methods convert NLF into numerical reward signals for reinforcement learning [10, 11, 12, 13, 34, 35, 36]. Other approaches explore learning directly from NLF, often by fine-tuning models to imitate the provided feedback [37, 17]. Additional strategies involve either employing dedicated refinement model [9] or using the primary model itself [17] to incorporate feedback into erroneous responses. These corrected responses are then used for further optimization, typically through supervised fine-tuning. In contrast, our approach enables LLMs to directly learn from NLF to iteratively refine their responses, while simultaneously maintaining online exploration through reinforcement learning. This integration of textual feedback and RL further enhances the models ability to address errors dynamically and improve performance."
        },
        {
            "title": "Language Guidance",
            "content": "3.1 Limitations of Learning with Numerical Feedback We investigate the limitations of fine-tuning with RL using numerical feedback from three key perspectives: (i) How performance evolves over time. (ii) The cognitive behaviors that contribute most significantly to successful problem-solving. (iii) The models ability to solve previously failed problems through trial-and-error. Setup. We conduct experiments on Qwen2.5-7B-Base [7] and Qwen3-8B-Base [1] for mathematical reasoning tasks. Specifically, we fine-tune the models using GRPO [16] with numerical feedback.4 Datasets and Evaluation Metrics. We utilize randomly sampled subsets of 4k, 8k, 16k, and 32k examples from reorganized 45k subset [18] of OpenR1-Math-220k [38]. The prompts are sourced from NuminaMath 1.5 [22], while the ground truth chain-of-thought (CoT) reasoning paths are generated by Deepseek-R1 [2]. Unless otherwise specified, experiments primarily use 4k training prompts. For validation, we randomly sample 500 examples from the validation set curated by [18], which includes examples from Olympiad Bench [39], MATH [19], Minerva-Math [20], AIME 2024 [22], and AMC 2023 [22]. To enable holistic evaluation, we assess performance on indistribution (ID) tasks using Minerva-Math [20] and on out-of-distribution (OOD) tasks using GPQA-Diamond (physics, chemistry, biology) [24]. During evaluation, we use greedy decoding (temperature = 0) and report accuracy (pass@1)5. Reward Design. We employ rule-based evaluation to provide numerical feedback (scalar rewards), using Math-Verify6 to validate the correctness of generated answers against ground truth during fine-tuning. Binary rewards are assigned as follows: +1 for correct final answers and 0 for incorrect ones. These rewards serve as proxy for assessing the accuracy of generated responses. Implementation Details. Our implementation leverages the VERL library [40] and samples four candidate responses per prompt during fine-tuning. (a) RL fine-tuning dynamics of Qwen2.5-7B. (b) RL fine-tuning dynamics of Qwen3-8B. Figure 2: RL fine-tuning dynamics of Qwen2.5-7B (left) and Qwen3-8B (right) using GRPO with numerical feedback on the validation set with varying numbers of training examples. Results. RL with solely numerical feedback frequently encounters performance plateaus. Figure 2 illustrates the RL fine-tuning dynamics of Qwen2.5-7B and Qwen3-8B across varying numbers of training examples. On-policy RL runs frequently stagnate, as shown by the validation set accuracy: Qwen2.5-7B reaches approximately 40% accuracy after 80 steps (left), while Qwen3-8B plateaus at around 52% accuracy after 250 steps (right). Notably, these performance plateaus persist 4GRPO is used without loss of generality, as RL algorithms such as PPO and GRPO exhibit comparable performance. 5pass@k measures the percentage of problems where the model produces correct solution within its first attempts. 6https://github.com/huggingface/Math-Verify 4 even with an 8x increase in the number of training prompts, showing minimal improvement in final performance. Self-reflection behaviors contribute minimally to successful problem-solving. Cognitive behaviors are widely recognized as key contributors to successful complex reasoning [2, 28]. In particular, increased self-reflection behaviors after RL fine-tuning, which mimic humans reflecting on past experiences and refining their approach to reach solution (commonly referred to as the Aha moment [2]), have drawn significant attention. However, does self-reflection play the most critical role in improved performance? To address this question, we characterize six key cognitive behaviors that contribute to self-improving reasoning during RL fine-tuning: (i) Subgoal Setting: Decomposing complex problems into smaller, manageable subtasks. (ii) Summarization: Summarizing the current state by identifying completed subtasks and determining the next steps in reasoning. (iii) Verification: Systematically checking intermediate results or computations to ensure correctness. (iv) Backtracking: Identifying errors or dead-ends in reasoning and revising previous methods or approaches. (v) Backward Chaining: Reasoning from desired outcomes back to the initial inputs or steps required to achieve the result. This is particularly useful in multiple-choice questions where answer options are provided. [28] (vi) Anticipation: Anticipating potential inaccuracies or considering alternative solutions to problem. We refer to the first two behaviors as planning behaviors, while the last four are categorized as self-reflection behaviors. We analyze the contribution of these behaviors to solving problems that the base model was previously unable to solve. For Qwen2.5-7B, we identify 87 previously unsolved problems from the Minerva-Math dataset and 37 from the GPQA-Diamond dataset. For Qwen38B, we identify 50 previously unsolved problems from the Minerva-Math dataset and 15 from the GPQA-Diamond dataset. More details can be found in Appendix A. (a) Reasoning behavior contributions to successful problem-solving in RL-tuned Qwen2.5-7B. (b) Reasoning behavior contributions to successful problem-solving in RL-tuned Qwen3-8B. Figure 3: Impact of different reasoning behaviors on solving previously failed problems after RL fine-tuning with numerical feedback for Qwen2.5-7B (left) and Qwen3-8B (right) on test tasks. Figure 3 presents the average success contribution of the behaviors, indicating that planning behaviors are the primary contributors to successful problem-solving, while self-reflection behaviors contribute significantly less in both the mathematical (Minerva-Math) and STEM (GPQA-Diamond) domains. This suggests limited effectiveness of self-reflection. Interestingly, self-reflection behaviors were minimal in the mathematical domain but more prominent in the STEM domain. We hypothesize that this disparity arises because STEM tasks, being closely aligned with real-world applications, inherently require multi-aspect considerations [41]. Such complex, multi-faceted problem-solving naturally elicits greater self-reflection. Models exhibit persistent failures on subset of problems despite trial-and-error fine-tuning. We evaluated the best-performing RL-finetuned Qwen2.5-7B and Qwen3-8B models on the 4k training prompts. As shown in the left panel of Table 1, these models consistently failed on approximately 29% and 17% of problems, respectively, with pass@4 = 0, despite undergoing trial-and-error fine-tuning, where correct responses are rewarded and incorrect responses are penalized. This suggests that the models struggle with certain problems when relying solely on numerical feedback. 5 likely cause of these performance plateaus and persistent failures is the sparse informational content of numerical feedback. Scalar rewards often fail to convey why response is correct or incorrect or how to improve multi-step reasoning. Furthermore, the limited effectiveness of internally generated self-reflection compounds these challenges, making it difficult for the models to address problems beyond their existing knowledge boundaries without additional guidance. Together, these limitations underscore the necessity of external oversight or richer feedback mechanisms to support more effective learning. 3.2 Promise of Learning from Natural Language Feedback To move beyond the limitations of purely numerical reward signals, we explore the potential of leveraging natural language feedback to help models identify errors and refine their responses. Specifically, we examine three types of critiques: (i) CoT Critique: model-generated critique providing step-by-step reasoning to justify correctness or incorrectness, concluding with binary correctness indication. (ii) Critique with Ground Truth (Critique w/ GT): heuristic-based critique that provides both the binary correctness indication and the ground truth answer. (iii) Indicative Critique: heuristic-based critique that merely indicates the binary correctness of the generated solution. Examples of these three critique types are shown later. We summarize the process for leveraging textual critiques to guide the refinement of LLM-generated responses in Algorithm 1. more detailed description is provided in Appendix C, and an illustrative example of the refinement process with CoT critique is available in Appendix E. Algorithm 1 Leveraging Textual Critiques for Refinement of LLM-Generated Responses Require: LLM πθ, Reasoning-based reward model πRM , evaluation function Eval, set of questions = {q}, predefined instructions Ic and Irefine, number of samples Sample initial responses: {y(i) Ensure: Refined responses {y(j) 1: for each question do 2: 3: end for 4: for each question do 5: for each initial response y(i) refined} for persistently failed questions # Step 1: Initial Response Sampling 0 }k i=1 πθ( q) # Step 2: Response Evaluation and Critique Generation Generate CoT critique: c(i) Evaluate correctness: Eval(q, y(i) if Eval(q, y(i) 0 ) = 0 then 0 {y(i) 0 }k i=1 do CoT πRM ( Ic, q, y(i) 0 ) 0 ) {0, 1} 6: 7: 8: Construct heuristic-based critiques: c(i) (indicative) and c(i) GT (with ground truth) end if end for 9: 10: 11: 12: end for 13: Identify persistently failed questions: Qfailed {q : Eval(q, y(i) 14: Form triplets (q, y(j) 15: for each triplet (q, y(j) 16: 17: end for 0 , c(j)) do Generate refined response: y(j) return Refined responses {y(j) 0 ) = 0} 0 , c(j)) for each Qfailed and each y(j) 0 , where c(j) {c(j) CoT, c(j) GT , c(j) } # Step 3: Refinement Generation refined πθ( Irefine, q, y(j) refined} for all (q, y(j) 0 , c(j)) 0 , c(j)) Setup. As described in the previous section, We evaluate the best-performing RL-finetuned Qwen2.5-7B and Qwen3-8B models, generating four responses per question for set of 4k prompts. Persistently failed question-response pairs are identified as those with pass@4 = 0. For each such pair, we prompt GPT-4o [42] to generate CoT critique and then construct the two corresponding heuristic-based critiques. Examples of these critique types are shown below. Subsequently, we prompt the models to refine these failed responses. Metrics. To assess the efficacy of the critique and refinement process, we define the following metrics: (i) Valid Critiques Generated: The percentage of initially erroneous responses for which 6 the critique model (πϕ) successfully generated well-formed and usable critique. This accounts for potential failures in the critique generation process itself. (ii) Successful Refinements: The percentage of initially erroneous responses (that received valid critique) for which the LLM (πθ) subsequently generated correct refined response. (iii) Critiqued Questions: The percentage of unique questions (all of whose initial responses were erroneous) for which at least one of their erroneous responses received valid critique. (iv) Questions Successfully Refined: The percentage of unique questions (all of whose initial responses were erroneous) for which at least one of their erroneous responses was successfully corrected through the refinement process. We evaluate the correctness of the refinements using the rule-based method described earlier. CoT Critique Lets analyze the students solution step-by-step and identify any errors: ### Step 1: Geometry Understanding The student correctly identifies the geometry of the problem, noting that the sphere is tangent to the base and sides of the cone, and that the axial section forms an isosceles triangle. ### Step 2: Variable Definitions The student defines as the radius of the sphere, as the radius of the base of the cone, and as the height of the cone. These definitions are appropriate. ### Step 3: Relating Sphere and Cone The student attempts to relate r, R, and using similar triangles. However, the equation derived: = Rh + is not directly applicable to the problem as it does not consider the specific condition that the areas are equal. ### Step 4: Equating Areas The student correctly equates the areas of the sphere and the base of the cone: 4πr2 = πR2 = = 2r This step is correct. ### Step 5: Substitution and Error The student substitutes = 2r into the earlier equation, leading to: = (2r)h + 2r This substitution is unnecessary and results in incorrect simplification. The correct approach would involve using the inradius formula for the triangle formed by the axial section of the cone. ### Step 6: Angle Calculation The student calculates tan θ = 1 and concludes θ = 45, leading to cos(90) = 0. This is incorrect because the angle calculation should be based on the relationship between and R, derived from the inradius formula. ### Step 7: Precise Calculation The student attempts to verify the angle calculation but repeats the same incorrect logic, leading to the same incorrect conclusion. ### Correct Solution Approach The correct approach involves using the inradius formula for the triangle formed by the axial section of the cone: - Calculate the inradius using the area and semi-perimeter of the triangle. - Substitute = 2r and solve for in terms of r. - Relate and θ using trigonometric identities. - Calculate cos(2θ) using the double angle identity. The correct final answer is: Conclusion: incorrect [END] cos(2θ) = 7 25 . Critique w/ GT The generated solution is incorrect, the ground truth is 7 25 . Indicative Critique The generated solution is incorrect. Results. CoT Critiques facilitate effective model refinement. Table 1 demonstrates that refinement guided by CoT critiques yields the highest valid refinement rate (36.47% and 44.71%) and the greatest percentage of questions successfully refined (55.37% and 66.96%). This significantly outperforms refinement based on indicative critiques or critiques with ground truth, even though 7 Table 1: Analysis of performance gains from critique-based refinement. Method % Failed Questions (Pass@4=0) Critique Type % Valid Critiques % Valid Refinements % Critiqued Questions % Questions Refined RL-finetuned Qwen2.5-7B RL-finetuned Qwen3-8B 29.07 17.18 Indicative Critique Critique w/ GT CoT Critique Indicative Critique Critique w/ GT CoT Critique 100.00 100.00 60.06 100.00 100.00 66. 2.09 1.98 36.47 3.57 3.93 44.71 100.00 100.00 95.10 100.00 100.00 98.25 7.05 6.88 55.37 11.21 12.23 66. the CoT critique generation process itself produces valid critiques for only 60%-66% of erroneous responses. The effectiveness of CoT critiques can be attributed to their richness; by providing step-by-step explanation of the reasoning along with targeted guidance on the correct solution approach, they offer considerably more informative feedback than simpler alternatives. Binary correctness signals alone provide limited refinement benefit. Refinement with indicative critiques and critiques with ground truth also yields some successful refinements, albeit at substantially lower rate (approximately 2%-4% valid refinements). This suggests that even simply indicating the correctness of response can provide minimal benefit, indicating some promise in leveraging natural language feedback to augment learning from numerical signals. However, the lack of substantial difference between indicative critiques and critiques with ground truth suggests that providing only the ground-truth answer, without any explanation or reasoning, provides little additional guidance to the model. The model appears to struggle to effectively leverage the ground truth answer without an understanding of why the initial response was incorrect or how to arrive at the correct solution."
        },
        {
            "title": "4 Critique-GRPO",
            "content": "Figure 4: Overview of Critique-GRPO. Given question, Critique-GRPO samples initial responses and then refines these responses using critiques generated by reasoning-based reward model. These refinements are combined with the initial responses to optimize the policy within an online reinforcement learning framework. weighted advantage function, combined with policy shaping, emphasizes correct refinements while strongly penalizing incorrect ones. Motivated by the potential of leveraging CoT critiques for effective model refinement (Section 3), we introduce Critique-GRPO, an online optimization algorithm that learns from both natural language 8 and numerical feedback. As illustrated in Figure 4, Critique-GRPO facilitates effective online learning and exploration by enabling the model to learn from both its generated responses and its effective refinements by incorporating natural language feedback (specifically, critiques). Before delving into the details of Critique-GRPO (Section 4.1), we briefly review Group Relative Policy Optimization (GRPO) [16] (Section 4.2). 4.1 From GRPO to Critique-GRPO GRPO is an online RL algorithm widely used during the fine-tuning stage of LLMs. It builds on Proximal Policy Optimization (PPO) [15], but eliminates the need for value function approximation by estimating advantages based on the relative performance of groups of actions. In the context of LLM policy optimization, let the model policy be parameterized by θ. For each question in given set Q, group of responses {y(i)}n i=1 is sampled from the old policy πold. reward model then scores these responses, yielding rewards {R(i)}n i=1. The GRPO training objective is formulated as: JGRPO(θ) = qQ,{y(i)}n i=1πold(q) 1 (cid:88) i= 1 y(i) y(i) (cid:88) t=1 (cid:110) min (cid:104) (θ) ˆA(i) r(i) , clip(r(i) (θ), 1 ϵ, 1 + ϵ) ˆA(i) (cid:105) βDKL[πθπref] (cid:111) , (1) where r(i) which the responses were sampled: (θ) is the probability ratio, comparing the current policy πθ to the old policy πold from r(i) (θ) = πθ(y(i) πold(y(i) q, y(i) <t) q, y(i) <t) , where r(i) (θold) = 1, (2) Here, ϵ and β are hyperparameters. The term ϵ controls the range of the clipped probability ratio, enforcing pessimistic lower bound on policy performance to prevent excessively large policy updates. Meanwhile, β regulates the KL divergence penalty, constraining the trained policy from deviating significantly from the reference policy. The advantage ˆA(i) using the group mean and standard deviation: for all tokens in response is calculated by normalizing the rewards {R(i)}n i=1 ˆA(i) = R(i) mean({R(1), . . . , R(n)}) std({R(1), . . . , R(n)}) . (3) Recent work [30] suggests that the token-level normalization and the standard deviation term in the advantage calculation (highlighted in gray) may introduce biased optimization. Following their implementation, we remove these terms to obtain an unbiased optimization objective. 4.2 Online Learning with Critique-GRPO We introduce Critique-GRPO, an online policy optimization framework that enables model to learn simultaneously from its generated responses and their refinements by incorporating CoT critiques generated by reasoning-based reward model. This approach helps the model discover new solutions and mitigate repeated failures on specific questions, as discussed in Section 3. Specifically, Critique-GRPO operates in three main steps (Figure 4): (i) Initial Response Sampling: Given an LLM and set of questions = {q}, we sample initial responses for each question from the old policy πold: {y(i)}n i=1 πold( q). These responses are scored using reasoning-based reward model πRM , which generates chain-of-thought (CoT) critiques: {c(i) i=1 πRM ( Ic, q, y(i)), where Ic is the critique instruction, and the critiques are based on the question-response pairs, as described in Section 3. The binary correctness labels from the critiques are then converted into scalar reward scores {R(i)}n CoT}n i=1. 9 (ii) Refinement with Critique: We prompt the LLM to generate refined responses conditioned on the question-response-critique triplet (q, y(i), c(i) CoT) and refinement instruction Irefine (detailed in Appendix B): y(i) refined πold( Irefine, q, y(i), c(i) CoT). The reward model scores these refinements, producing {R(i) refine}n i=1 (alternatively, the rule-based evaluation function discussed in Section 3 could be used). To mitigate potential distributional shifts induced by the refinements, we randomly sample subset of refinements, denoted by {y(i) i=1. This sampling prioritizes correct refinements; if no correct refinements are generated, incorrect refinements are sampled randomly. We then combine the sampled subset of refinements with the initial responses to form mixed group of responses.7 i=1, from the full refinement set {y(i) refined}n refined}k (iii) Online Policy Optimization: Finally, the model is fine-tuned on mixed set of initial and refined responses using scalar rewards. The training objective, adapted from GRPO, is given by: JCritique-GRPO(θ) =E qQ,{y(i)}n i=1πold(q),{y(i ) refined}k i=1 y(i) (cid:88) (cid:88) (cid:20) 1 (cid:124) (cid:125) (cid:123)(cid:122) Objective for Initial Responses (θ)A(i) r(i) t=1 i=1 + 1 (cid:124) πold(Irefine,q,y(i),c(i ) CoT ) y(i ) refined (cid:88) (cid:16) r(i) refined,t(θ) (cid:17) t=1 (cid:123)(cid:122) Objective for Refined Responses (cid:88) i=1 where the advantages A(i) , A(i) for all tokens in response are defined as: = R(i) mean(cid:0){R(j)}n A(i) A(i) = R(i) mean(cid:0){R(j)}n j=1 {R(j) j=1 {R(j) refined}k refined}k j=1 j=1 (cid:1), (cid:1), (θ) and (r(i) r(i) refined,t(θ)) represent the token-level probability ratios: (cid:21) , A(i) (cid:125) (4) (5) r(i) (θ) = πθ(y(i) πold(y(i) q, y(i) <t) q, y(i) <t) , (r(i) refined,t(θ)) = πθ(y(i) refined,tq, y(i) πθ(y(i) refined,tq, y(i) refined,<t) refined,<t) + γ . (6) The shaping function () [18], depicted in the lower right corner of Figure 4, reweights the gradients to assign greater importance to low-probability tokens in refined responses. This encourages the model to effectively learn from unfamiliar (i.e., low-probability under the current policy) but correct refinements while penalizing failed refinements. In addition, we remove the clipping function for probability ratios and the KL-divergence penalty term (present in the original GRPO formulation) to reduce restrictions on policy updates. This enables more substantial model adjustments and facilitates effective learning from refinements. We summarize Critique-GRPO in Algorithm 2 (Appendix D)."
        },
        {
            "title": "5 Experiments",
            "content": "In this section, we evaluate the efficacy of Critique-GRPO on challenging mathematical, scientific and general reasoning tasks. 5.1 Experimental Setup Datasets and Evaluation Metrics. We use randomly sampled subsets of 4k examples from reorganized 45k subset [18] of OpenR1-Math-220k [38] as the training set (as described in Section 3). For validation, we use the curated validation set provided by [18]. We evaluate the model on five well-established mathematical reasoning benchmarks: MATH-500 [19], Minerva-Math [20], OlympiadBench [21], MATH [19], AIME 2024 [22], and AMC 2023 [22]. For broader analysis, we assess the models generalization ability on three scientific and general reasoning tasks: TheoremQA (Math, Physics, EE&CS, and Finance) [23], GPQA-Diamond (Physics, Chemistry, and Biology) [24], 7Currently, only one refined response is retained. Future work may explore the optimal data ratio. 10 and MMLU-Pro (Business, Computer Science, Law, etc.) [25]. During evaluation, we use greedy decoding (temperature = 0) and report pass@1 over three runs. Reward Design. During RL fine-tuning, we use model-based evaluation to generate critiques and rule-based evaluation to provide binary scalar rewards, as described in Section 3. Compared Methods. We compare Critique-GRPO against the following representative approaches, categorized into supervised learning and reinforcement learning-based finetuning. All differences are considered significant at < 0.01. Supervised Learning-based Finetuning: (i) Supervised Finetuning (SFT): Finetuning the base model on high-quality annotated training data using supervised learning. (ii) Reward rAnked Finetuning (RAFT) [43]: Finetuning on self-generated correct responses, sampled based on rule-based evaluation. (iii) Refinement Finetuning (Refinement FT) [9]: Finetuning on refined correct responses generated conditionally on the question, initial response, and critiques. (iv) Critique Finetuning (Critique FT) [33]: Finetuning on annotated critique data to train the model to critique given query-response. (v) Critique-in-the-Loop Finetuning (CITL-FT) [33]: Finetuning on mixed data consisting of selfgenerated correct responses and refined correct responses, conditioned on the question-initial responsecritique triplet. Reinforcement Learning-based Finetuning: (vi) R1-GRPO [2]: Finetuning the base model on its own generations using the GRPO algorithm with binary scalar rewards. (vii) LUFFY [18]: Augmenting R1-GRPO by incorporating expert demonstrations, enabling the model to acquire advanced reasoning capabilities while balancing imitation and exploration. (viii) Critique-GRPO (Simple Critique with Ground Truth): variant of our framework that utilizes simple critiques paired with ground-truth answers (as discussed in Section 3) for refinements. Implementation Details. We conduct experiments on Qwen2.5-7B-Base and Qwen3-8B-Base models, using GPT-4o as the critique model (reasoning-based reward model), as described in Section 3. For supervised finetuning baselines, models are finetuned until convergence, and the best performance is reported. For reinforcement learning approaches, models are finetuned for 400 steps, and the best performance is recorded. To ensure fair comparison: In R1-GRPO, 8 responses (rollouts) are sampled per training prompt with temperature = 1. In LUFFY, 7 responses are sampled per prompt along with one ground truth response (expert demonstration). In Critique-GRPO, 7 responses are sampled per prompt, along with one refined response from the refinement sets. All experiments are conducted on 40 NVIDIA A800 85G GPUs. To ensure consistency, we only use critiques generated by the reward model that align with rule-based evaluations; otherwise, the reward model is prompted to regenerate the critiques. Note that evaluation results may vary across different machines; hence, we focus on relative improvements, specifically the performance gains compared to prior RL-based finetuning approaches. 5.2 Main Results Table 2 presents the comprehensive evaluation results, from which we highlight the following key observations: Incorporating refinements with critiques (natural language feedback) in online reinforcement learning facilitates effective policy optimization. Critique-GRPO consistently achieves the best performance on both Qwen2.5-7B and Qwen3-8B, across both in-distribution and out-of-distribution tasks. Notably, it significantly outperforms both supervised learning-based and RL-based fine-tuning approaches, improving average pass@1 scores by approximately 4.5 points (Critique-GRPO (CoT Critique): 48.07% vs. LUFFY: 43.48%) and 5 points (Critique-GRPO (CoT Critique): 65.86% vs. LUFFY: 60.91%), respectively, over these baselines. 11 Table 2: Evaluation results (pass@1) on Mathematical reasoning (In-Distribution) and Scientific and General (Out-of-Distribution) reasoning tasks. GT CoT refers to ground-truth CoT generated by Deepseek-R1, as described in Section 3. Num. Feedback and Lang. Feedback denote numerical and natural language feedback, respectively. Method Supervision Type Math (ID) Science & General (OOD) MATH 500 Minerva MATH Olympiad Bench AMC23 AIME24 Theorem QA GPQA Diamond MMLU Pro Avg. - + CITL-FT [33] Qwen2.5-7B-Base Supervised Learning-based Finetuning GT CoT + SFT Num. Feedback + RAFT [43] Lang. Feedback + Refinement FT [9] Lang. Feedback + Critique FT [17] Num. Feedback & Lang. Feedback (CoT Critique) Reinforcement Learning-based Finetuning Num. Feedback + R1-GRPO [2] Num. Feedback & GT CoT Num. Feedback & Lang. Feedback (Critique w/ GT) Num. Feedback & Lang. Feedback (CoT Critique) + Critique-GRPO (Ours) (Simple Critique w/ GT) + Critique-GRPO (Ours) (CoT Critique) + LUFFY [18] - + CITL-FT Qwen3-8B-Base Supervised Learning-based Finetuning + SFT GT CoT Num. Feedback + RAFT Lang. Feedback + Refinement FT Lang. Feedback + Critique FT Num. Feedback & Lang. Feedback (CoT Critique) Reinforcement Learning-based Finetuning Num. Feedback + R1-GRPO Num. Feedback & GT CoT Num. Feedback & Lang. Feedback (Critique w/ GT) Num. Feedback & Lang. Feedback (CoT Critique) + Critique-GRPO (Ours) (Simple Critique w/ GT) + Critique-GRPO (Ours) (CoT Critique) + LUFFY 58.60 17.60 29.90 35.00 13.30 21. 31.82 46.64 31.81 59.20 67.40 65.80 67.40 25.00 19.90 16.50 16.90 24.00 33.50 31.40 29.80 40.00 45.00 42.50 52. 10.00 16.70 6.70 10.00 26.00 25.90 24.50 28.70 27.27 25.76 24.24 25.76 50.81 32.79 46.77 35.74 47.61 32.41 44.56 34.70 60.60 20. 31.00 47.50 16.70 29.00 27.27 48.24 35. 77.40 29.40 36.90 45.00 16.70 41. 34.85 51.94 41.66 76.20 35.70 44.00 57. 10.00 41.60 26.77 56.10 43.48 77.60 35. 40.10 57.50 3.30 42.60 34.34 56.86 43. 79.00 40.10 44.40 60.00 20.00 43. 40.40 57.19 48.07 75.20 36.40 36.60 52. 23.30 39.80 30.30 66.39 45.06 80.00 77.20 82.20 78.20 38.20 35.70 46.30 30. 43.60 39.00 45.50 41.20 67.50 60.00 72.50 67.50 30.00 26.70 36.70 30.00 42.60 39.80 52.80 43.50 38.38 33.83 39.90 33.83 63.85 50.39 68.50 47.59 63.46 54.93 63.85 48. 78.20 34.60 38.40 70.00 26.70 39. 29.29 67.91 48.03 89.40 48.20 55.40 80. 40.00 54.50 46.46 69.46 60.68 89.20 50. 55.70 75.00 40.00 55.50 44.44 70.18 60. 89.40 50.40 58.80 85.00 40.00 56. 47.47 70.02 62.91 91.20 60.70 62.70 90. 46.70 57.60 48.99 70.93 65.86 Learning from online refinements with critiques is more effective than offline refinements. Critique-GRPO (CoT Critique) significantly outperforms Refinement FT by approximately 15.5 and 10.9 average pass@1 points on Qwen2.5-7B and Qwen3-8B, respectively. It also surpasses CITL-FT by approximately 12.5 and 17.8 points on Qwen2.5-7B and Qwen3-8B, respectively. Incorporating online refinements facilitates effective policy exploration. Critique-GRPO significantly outperforms R1-GRPO on all tasks by large margin, notably achieving an average pass@1 improvement of approximately 6.5 and 5.1 points on Qwen2.5-7B and Qwen3-8B respectively. In addition, Critique-GRPO evidently boosts the models generalization ability, particularly on science and general reasoning tasks. Learning from online, self-generated refinements is more effective than imitating expert demonstrations. Critique-GRPO outperforms LUFFY across all tasks by large margin, achieving 4.5% and 5% higher average pass@1 scores, despite LUFFY leveraging high-quality expert demonstrations. Additionally, simpler variant of Critique-GRPO, which uses critiques based solely on ground truth answers, achieves comparable or even superior performance to LUFFY. Further analysis of policy exploration is presented in Section 5.3. 12 Higher-quality refinements, guided by richer critiques, lead to more effective policy optimization. Critique-GRPO (CoT critique) consistently outperforms its variant, Critique-GRPO (simple critique with ground truth), across all tasks. This improvement stems from the superior ability of CoT critiques to facilitate effective model refinements compared to binary correctness signals paired with ground-truth answers, as discussed in Section 3. The detailed reasoning and guidance provided by CoT critiques allow for more precise and beneficial policy updates. 5.3 Investigation of Policy Exploration During RL Finetuning (a) Entropy dynamics during RL finetuning on Qwen2.5-7B. (b) Response length changes during RL finetuning on Qwen2.5-7B. Figure 5: Comparison of entropy dynamics (left) and response length changes (right) during RL finetuning on Qwen2.5-7B. To further investigate policy exploration, we present the entropy dynamics and response length changes during RL finetuning on Qwen2.5-7B in Figure 5. Higher entropy does not always guarantee efficient learning from exploration. As shown in Figure 5a, the policy entropy of Critique-GRPO generally remains higher than that of R1-GRPO, suggesting more consistent exploration. The peaks observed in Critique-GRPOs entropy dynamics (before step 200) likely occur when its self-generated refinements diverge from the initial sampled responses (leading to increased entropy), thereby inducing potentially beneficial distributional shifts. The subsequent decrease in entropy suggests the model quickly internalizes these refinements, reducing the distribution deviation. This dynamic aligns with the finding that rare action with high advantage can increase policy entropy (i.e., unfamiliar, correct responses that receive high rewards constitute effective exploration), while high-probability action with high advantage would reduce entropy [44]. Interestingly, while LUFFY exhibits higher entropy than Critique-GRPO, its underperformance (as detailed in Table 2) suggests that the quality and assimilability of exploration signals are more critical than the overall extent of exploration (as reflected solely by entropy). Critique-GRPOs self-generated refinements, being closer to the models current capabilities, likely result in more targeted and effective policy updates. In contrast, LUFFYs reliance on expert demonstrations, while diverse, may involve states or behaviors that are too distant for the model to efficiently achieve incrementally, thus reducing the effectiveness of learning from exploration. Longer responses do not necessarily imply more effective exploration. Figure 5b reveals that LUFFY, by imitating long expert demonstrations, learns to produce substantially longer responses (even exceed13 Figure 6: Response length changes during RL finetuning on Qwen3-8B. ing 6000 tokens), which might be interpreted as broader exploration. However, LUFFY consistently underperforms compared to Critique-GRPO, which generates much shorter responses. This difference likely arises from Critique-GRPOs use of critiques, which enable the model to efficiently identify errors and make targeted refinements, leading to better performance without requiring excessively long responses. Thus, encouraging longer responses does not necessarily translate to more effective exploration or improved task performance. Critique-GRPO facilitates concise reasoning. As demonstrated in Figure 5b, Critique-GRPO achieves superior performance  (Table 2)  while minimally increasing response length when applied to Qwen2.5-7B. Critique-GRPO yields response lengths comparable to other RL-based fine-tuning methods on Qwen3-8B (Figure 6). This efficiency may stem from its critique mechanism, which facilitates effective error identification and precise refinement, mitigating unnecessary self-reflection. Notably, all evaluated methods tend to reduce response length in Qwen3-8B, contrasting with the increasing response length trend observed in Qwen2.5-7B. We attribute this decrease in Qwen3-8B to the baseline Qwen2.5-7B models tendency to exhibit redundant and ineffective self-reflection (Section 5.5). 5. Impact of Policy Shaping on RL Finetuning Table 3: Impact of policy shaping on the token-level probability ratios of generated refinements (as introduced in Section 4) during RL finetuning of Qwen2.5-7B. Method Policy Shaping Math (ID) Science & General (OOD) MATH 500 Minerva MATH Olympiad Bench AMC23 AIME24 Theorem QA GPQA Diamond MMLU Pro Avg. Qwen2.5-7B-Base + Critique-GRPO (Ours) (CoT Critique) + Critique-GRPO (Ours) (CoT Critique) - 58.60 17. 29.90 35.00 13.30 21.60 31.82 46.64 31. w/o 76.20 41.00 39.70 55.00 13. 42.60 30.30 54.88 44.12 w/ 79.00 40. 44.40 60.00 20.00 43.50 40.40 57.19 48. To clarify the impact of policy shaping on the generated refinements during RL finetuning, we present the evaluation results of removing policy shaping during the RL finetuning of Qwen2.5-7B in Table 3. Policy shaping enhances learning from refinements during online RL finetuning. Critique-GRPO with policy shaping applied to the token-level probability ratios of generated refinements consistently outperforms the variant without policy shaping across nearly all tasks, improving average pass@1 scores by 4%. 5.5 Qualitative Analysis We present qualitative comparison between the responses generated by the base Qwen3-8B model and the RL-finetuned Qwen3-8B model using Critique-GRPO on the MATH-500 dataset. The given question is: Evaluate sin(arcsin 0.4 + arcsin 0.5), sin(arcsin 0.5 arcsin 0.4). The base Qwen3-8B model produces an incorrect response due to an incorrect formulation of the expression as sin(A + B) sin(A B), as shown in the red square. In contrast, the RL-finetuned Qwen3-8B model using Critique-GRPO generates correct response by correctly formulating the expression as sin(α + β) sin(β α), as shown in the green square. Furthermore, the base Qwen3-8B model exhibits numerous redundant and ineffective self-reflection attempts (e.g., Wait, ... highlighted in blue), which fail to help the model identify the actual errors and answer the question correctly. This results in an excessively long response (over 6000 tokens). In contrast, the RL-finetuned Qwen3-8B model using Critique-GRPO demonstrates concise and effective reasoning. It remains on the correct path to solve the problem, exhibits valid self-reflection to validate the answer (e.g., Let me check with approximate values. ...), and ultimately generates the correct answer. Detailed responses are provided in Appendix F. Response Generated by Qwen3-8B Base Model (Abridged) sin(arcsin 0.4 + arcsin 0.5) multiplied by Okay, so need to evaluate this expression: sin(arcsin 0.5 arcsin 0.4). Hmm, that looks bit complicated with all those inverse sine functions. Let me try to break it down step by step. First, let me recall some trigonometric identities that might help here. remember that there are formulas for sin(A+B) and sin(AB). Maybe can use those? Let me write them down: sin(A + B) = sin cos + cos sin B, sin(A B) = sin cos cos sin So if let = arcsin 0.4 and = arcsin 0.5, then the expression becomes: sin(A + B) sin(A B) Wait, so maybe can compute sin(A + B) and sin(A B) separately using those identities and then multiply them together? But before that, maybe should figure out what and are? Well, is arcsin 0.4, which means that sin = 0.4, and similarly sin = 0.5. But need cos and cos to use the identities. Since sin2 θ + cos2 θ = 1, can find cos and cos B. Let me compute cos first. For angle A, sin = 0.4, so (cid:112) 1 sin2 = 1 0.16 = 0.84. cos = 3 2 . 0.84... So cos = Wait, ... Okay, so now can compute sin(A + B) and sin(A B) using the identities. ... So ... Okay, now moving on to sin(A B): sin(A + B) = 0.2 3 + 21 ... So sin(A B) = sin cos cos sin sin(A B) = 21 2 3 10 Therefore, the original expression is: sin(A + B) sin(A B) = (cid:32) 3 + 10 21 2 (cid:33) (cid:32) 3 10 (cid:33) ... That seems correct. But let me verify with another approach. Maybe using another identity? Alternatively, remember that sin(A + B) sin(A B) can be expressed as sin2 sin2 B. Wait, is that correct? Wait, let me recall the identity: sin(A + B) sin(A B) = sin2 sin2 Let me check that. Yes, because: sin(A + B) sin(A B) = [sin cos + cos sin B][sin cos cos sin B] = (sin cos B)2 (cos sin B)2 = sin2 cos2 cos2 sin2 ... Let me compute sin2 sin2 B: sin2 sin2 = (0.4)2 (0.5)2 = 0.16 0.25 = 0.09 = 9 100 ... 15 Response Generated by RL-Finetuned Qwen3-8B using Critique-GRPO (Abridged) Okay, so need to evaluate this expression: sin(arcsin 0.5 arcsin 0.4). Hmm, let me think. sin(arcsin 0.4 + arcsin 0.5) multiplied by First, remember that arcsin is the inverse sine function, so arcsin 0.4 is an angle whose sine is 0.4, and similarly for arcsin 0.5. Let me denote some variables to make this easier. Let me set α = arcsin 0.4 and β = arcsin 0.5. Then the expression becomes sin(α + β) sin(β α). Alright, so need to compute sin(α+β) and sin(βα) separately and then multiply them. Maybe can use sineaddition formulas here. Let me recall that sin(A+B) = sin cos B+cos sin B, and sin(A B) = sin cos cos sin B. Since α and β are arcsin values, know that sin α = 0.4 and sin β = 0.5. Therefore, need to find cos α and cos β to use in the formulas. For cos α, since sin2 α + cos2 α = 1, we have Note that 0.84 = 84 100 = 25 , so cos α = (cid:112) cos α = 1 sin2 α = (cid:113) 21 21 5 . 25 = 1 0.16 = 0.84. Similarly, (cid:113) cos β = 1 sin2 β = 1 0.25 = 0.75 = (cid:113) 4 = 3 2 . Now, let me compute sin(α + β). Using the formula, sin(α + β) = sin α cos β + cos α sin β = 0. (cid:32) 3 2 (cid:33) + (cid:32) 21 (cid:33) (0.5). Compute each term: Therefore 3 = 2 5 3 2 3 5 , = 21 5 0.4 0.5 = 21 10 . sin(α + β) = 3 5 21 10 + = 3 + 10 21 . Similarly, compute sin(β α): sin(β α) = sin β cos α cos β sin α = 0. (cid:32) (cid:33) 3 2 (0.4). 21 5 Thus sin(β α) = 21 10 2 3 10 = 21 2 3 . Therefore, the original expression is 2 (cid:32) 3 + 10 21 (cid:33) (cid:32) (cid:33) 3 . 21 2 10 Let = 2 21)2 (2 ( ... 21 and = 21 2 3 + 3)2 = 21 12 = 9, so the value is 9 100 . 16 3; then the product is 100 . Note that ="
        },
        {
            "title": "6 Conclusion",
            "content": "In this work, we identify three key challenges faced by RL with solely numerical feedback: performance plateaus, limited effectiveness of self-reflection, and persistent failures. We demonstrate that RL-finetuned models, even after encountering performance plateaus, can generate correct refinements on persistently failed problems by leveraging natural language feedback in the form of critiques, addressing these challenges. To this end, we propose Critique-GRPO, an online RL framework that enables LLMs to learn simultaneously from both natural language and numerical feedback for effective policy optimization. Specifically, Critique-GRPO facilitates learning from initial responses and critique-guided refinements while preserving exploration. Additionally, we employ shaping function to enhance learning from valid refinements and heavily penalize failed refinements. Experimental results on Qwen2.5-7B and Qwen3-8B demonstrate that Critique-GRPO consistently achieves superior performance across eight challenging reasoning tasks. Furthermore, our in-depth analysis of policy exploration reveals two key insights: (i) Higher entropy does not always guarantee efficient learning from exploration. (ii) Longer responses do not necessarily lead to more effective exploration. Future work could explore applying Critique-GRPO to multimodal reasoning tasks [45, 46] to strengthen connections between visual understanding and textual reasoning or incorporating new knowledge [47, 48] for continual learning."
        },
        {
            "title": "Acknowledgments",
            "content": "We sincerely thank Zhanhui Zhou for valuable feedback."
        },
        {
            "title": "References",
            "content": "[1] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao Ge, Haoran Wei, Huan Lin, Jialong Tang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jing Zhou, Jingren Zhou, Junyang Lin, Kai Dang, Keqin Bao, Kexin Yang, Le Yu, Lianghao Deng, Mei Li, Mingfeng Xue, Mingze Li, Pei Zhang, Peng Wang, Qin Zhu, Rui Men, Ruize Gao, Shixuan Liu, Shuang Luo, Tianhao Li, Tianyi Tang, Wenbiao Yin, Xingzhang Ren, Xinyu Wang, Xinyu Zhang, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yinger Zhang, Yu Wan, Yuqiong Liu, Zekun Wang, Zeyu Cui, Zhenru Zhang, Zhipeng Zhou, and Zihan Qiu. Qwen3 technical report, 2025. [2] DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J. L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, R. J. Chen, R. L. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, S. S. Li, Shuang Zhou, Shaoqing Wu, Shengfeng Ye, Tao Yun, Tian Pei, Tianyu Sun, T. Wang, Wangding Zeng, Wanjia Zhao, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, W. L. Xiao, Wei An, Xiaodong Liu, Xiaohan Wang, Xiaokang Chen, Xiaotao Nie, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, X. Q. Li, Xiangyue Jin, Xiaojin Shen, Xiaosha Chen, Xiaowen Sun, Xiaoxiang Wang, Xinnan Song, Xinyi Zhou, Xianzu Wang, Xinxia Shan, Y. K. Li, Y. Q. Wang, Y. X. Wei, Yang Zhang, Yanhong Xu, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Wang, Yi Yu, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yuan Ou, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He, Yunfan Xiong, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Y. X. Zhu, Yanhong Xu, Yanping Huang, Yaohui Li, Yi Zheng, Yuchen Zhu, Yunxian Ma, Ying Tang, Yukun Zha, Yuting Yan, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhenda Xie, Zhengyan Zhang, Zhewen Hao, Zhicheng Ma, Zhigang Yan, Zhiyu Wu, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Zizheng Pan, Zhen Huang, Zhipeng Xu, Zhongyu Zhang, and Zhen Zhang. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. [3] OpenAI, :, Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, Alex Iftimie, Alex Karpenko, Alex Tachard Passos, Alexander Neitz, Alexander Prokofiev, Alexander Wei, Allison Tam, Ally Bennett, Ananya Kumar, Andre Saraiva, Andrea Vallone, Andrew Duberstein, Andrew Kondrich, Andrey Mishchenko, Andy Applebaum, Angela Jiang, Ashvin Nair, Barret Zoph, Behrooz Ghorbani, Ben Rossen, Benjamin Sokolowsky, Boaz Barak, Bob McGrew, Borys Minaiev, Botao Hao, Bowen Baker, Brandon Houghton, Brandon McKinzie, Brydon Eastman, Camillo Lugaresi, Cary Bassin, Cary Hudson, Chak Ming Li, Charles de Bourcy, Chelsea Voss, Chen Shen, Chong Zhang, Chris Koch, Chris Orsinger, Christopher Hesse, Claudia Fischer, Clive Chan, Dan Roberts, Daniel Kappler, Daniel Levy, Daniel Selsam, David Dohan, David Farhi, David Mely, David Robinson, Dimitris Tsipras, Doug Li, Dragos Oprica, Eben Freeman, Eddie Zhang, Edmund Wong, Elizabeth Proehl, Enoch Cheung, Eric Mitchell, Eric Wallace, Erik Ritter, Evan Mays, Fan Wang, Felipe Petroski Such, Filippo Raso, Florencia Leoni, Foivos Tsimpourlas, Francis Song, Fred von Lohmann, Freddie Sulit, Geoff Salmon, Giambattista Parascandolo, Gildas Chabot, Grace Zhao, Greg Brockman, Guillaume Leclerc, Hadi Salman, Haiming Bao, Hao Sheng, Hart Andrin, Hessam Bagherinezhad, Hongyu Ren, Hunter Lightman, Hyung Won Chung, Ian Kivlichan, Ian OConnell, Ian Osband, Ignasi Clavera Gilaberte, Ilge Akkaya, Ilya Kostrikov, Ilya Sutskever, Irina Kofman, Jakub Pachocki, James Lennon, Jason Wei, Jean Harb, Jerry Twore, Jiacheng Feng, Jiahui Yu, Jiayi Weng, Jie Tang, Jieqi Yu, Joaquin Quiñonero Candela, Joe Palermo, Joel Parish, Johannes Heidecke, John Hallman, John Rizzo, Jonathan Gordon, Jonathan Uesato, Jonathan Ward, Joost Huizinga, Julie Wang, Kai Chen, Kai Xiao, Karan Singhal, Karina Nguyen, Karl Cobbe, Katy Shi, Kayla Wood, Kendra Rimbach, Keren Gu-Lemberg, Kevin Liu, Kevin Lu, Kevin Stone, Kevin Yu, Lama Ahmad, Lauren Yang, Leo Liu, Leon Maksin, Leyton Ho, Liam Fedus, Lilian Weng, Linden Li, Lindsay McCallum, Lindsey Held, Lorenz Kuhn, Lukas Kondraciuk, Lukasz Kaiser, Luke Metz, Madelaine Boyd, Maja Trebacz, Manas Joglekar, Mark Chen, Marko Tintor, Mason Meyer, Matt Jones, Matt Kaufer, Max Schwarzer, Meghan Shah, Mehmet Yatbaz, Melody Y. Guan, Mengyuan Xu, Mengyuan Yan, Mia Glaese, Mianna Chen, Michael Lampe, Michael Malek, Michele Wang, Michelle Fradin, Mike McClay, Mikhail Pavlov, Miles Wang, Mingxuan Wang, Mira Murati, Mo Bavarian, Mostafa Rohaninejad, Nat McAleese, Neil Chowdhury, Neil Chowdhury, Nick Ryder, Nikolas Tezak, Noam Brown, Ofir Nachum, Oleg Boiko, Oleg Murk, Olivia Watkins, Patrick Chao, Paul Ashbourne, Pavel Izmailov, 18 Peter Zhokhov, Rachel Dias, Rahul Arora, Randall Lin, Rapha Gontijo Lopes, Raz Gaon, Reah Miyara, Reimar Leike, Renny Hwang, Rhythm Garg, Robin Brown, Roshan James, Rui Shu, Ryan Cheu, Ryan Greene, Saachi Jain, Sam Altman, Sam Toizer, Sam Toyer, Samuel Miserendino, Sandhini Agarwal, Santiago Hernandez, Sasha Baker, Scott McKinney, Scottie Yan, Shengjia Zhao, Shengli Hu, Shibani Santurkar, Shraman Ray Chaudhuri, Shuyuan Zhang, Siyuan Fu, Spencer Papay, Steph Lin, Suchir Balaji, Suvansh Sanjeev, Szymon Sidor, Tal Broda, Aidan Clark, Tao Wang, Taylor Gordon, Ted Sanders, Tejal Patwardhan, Thibault Sottiaux, Thomas Degry, Thomas Dimson, Tianhao Zheng, Timur Garipov, Tom Stasi, Trapit Bansal, Trevor Creech, Troy Peterson, Tyna Eloundou, Valerie Qi, Vineet Kosaraju, Vinnie Monaco, Vitchyr Pong, Vlad Fomenko, Weiyi Zheng, Wenda Zhou, Wes McCabe, Wojciech Zaremba, Yann Dubois, Yinghai Lu, Yining Chen, Young Cha, Yu Bai, Yuchen He, Yuchen Zhang, Yunyun Wang, Zheng Shao, and Zhuohan Li. Openai o1 system card, 2024. [4] OpenAI. Openai o3-mini. https://openai.com/index/openai-o3-mini/, 2025. [5] Xiaoying Zhang, Baolin Peng, Jianfeng Gao, and Helen Meng. Toward self-learning end-to-end taskoriented dialog systems. In Oliver Lemon, Dilek Hakkani-Tur, Junyi Jessy Li, Arash Ashrafzadeh, Daniel Hernández Garcia, Malihe Alikhani, David Vandyke, and Ondˇrej Dušek, editors, Proceedings of the 23rd Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 516530, Edinburgh, UK, September 2022. Association for Computational Linguistics. [6] David Silver and Richard Sutton. Welcome to the era of experience. Google AI, 2025. [7] Qwen, :, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tianyi Tang, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 technical report, 2025. [8] William Saunders, Catherine Yeh, Jeff Wu, Steven Bills, Long Ouyang, Jonathan Ward, and Jan Leike. Self-critiquing models for assisting human evaluators, 2022. [9] Angelica Chen, Jérémy Scheurer, Jon Ander Campos, Tomasz Korbak, Jun Shern Chan, Samuel R. Bowman, Kyunghyun Cho, and Ethan Perez. Learning from natural language feedback. Transactions on Machine Learning Research, 2024. [10] Seungone Kim, Jamin Shin, Yejin Cho, Joel Jang, Shayne Longpre, Hwaran Lee, Sangdoo Yun, Seongjin Shin, Sungdong Kim, James Thorne, and Minjoon Seo. Prometheus: Inducing fine-grained evaluation capability in language models. CoRR, abs/2310.08491, 2023. [11] Chenxi Whitehouse, Tianlu Wang, Ping Yu, Xian Li, Jason Weston, Ilia Kulikov, and Swarnadeep Saha. J1: Incentivizing thinking in llm-as-a-judge via reinforcement learning, 2025. [12] Zijun Liu, Peiyi Wang, Runxin Xu, Shirong Ma, Chong Ruan, Peng Li, Yang Liu, and Yu Wu. Inferencetime scaling for generalist reward modeling, 2025. [13] Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step. In The Twelfth International Conference on Learning Representations, 2024. [14] Xiaoying Zhang, Baolin Peng, Ye Tian, Jingyan Zhou, Lifeng Jin, Linfeng Song, Haitao Mi, and Helen Meng. Self-alignment for factuality: Mitigating hallucinations in LLMs via self-evaluation. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 19461965, Bangkok, Thailand, August 2024. Association for Computational Linguistics. [15] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms, 2017. [16] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. CoRR, abs/2402.03300, 2024. [17] Yubo Wang, Xiang Yue, and Wenhu Chen. Critique fine-tuning: Learning to critique is more effective than learning to imitate, 2025. [18] Jianhao Yan, Yafu Li, Zican Hu, Zhi Wang, Ganqu Cui, Xiaoye Qu, Yu Cheng, and Yue Zhang. Learning to reason under off-policy guidance, 2025. 19 [19] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the MATH dataset. CoRR, abs/2103.03874, 2021. [20] Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, and Vedant Misra. Solving quantitative reasoning problems with language models, 2022. [21] Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, Jie Liu, Lei Qi, Zhiyuan Liu, and Maosong Sun. OlympiadBench: challenging benchmark for promoting AGI with olympiad-level bilingual multimodal scientific problems. In LunWei Ku, Andre Martins, and Vivek Srikumar, editors, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 38283850, Bangkok, Thailand, August 2024. Association for Computational Linguistics. [22] Jia Li, Edward Beeching, Lewis Tunstall, Ben Lipkin, Roman Soletskyi, Shengyi Costa Huang, Kashif Rasul, Longhui Yu, Albert Jiang, Ziju Shen, Zihan Qin, Bin Dong, Li Zhou, Yann Fleureau, Guillaume Lample, and Stanislas Polu. Numinamath. https://github.com/project-numina/aimo-progress-prize/ blob/main/report/numina_dataset.pdf, 2024. [23] Wenhu Chen, Ming Yin, Max Ku, Pan Lu, Yixin Wan, Xueguang Ma, Jianyu Xu, Xinyi Wang, and Tony Xia. TheoremQA: theorem-driven question answering dataset. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 78897901, Singapore, December 2023. Association for Computational Linguistics. [24] David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel R. Bowman. GPQA: graduate-level google-proof q&a benchmark. In First Conference on Language Modeling, 2024. [25] Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, Tianle Li, Max Ku, Kai Wang, Alex Zhuang, Rongqi Fan, Xiang Yue, and Wenhu Chen. Mmlu-pro: more robust and challenging multi-task language understanding benchmark. In Amir Globersons, Lester Mackey, Danielle Belgrave, Angela Fan, Ulrich Paquet, Jakub M. Tomczak, and Cheng Zhang, editors, Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024, 2024. [26] Mehdi Fatemi, Banafsheh Rafiee, Mingjie Tang, and Kartik Talamadupula. Concise reasoning via reinforcement learning, 2025. [27] Gengyang Li, Yifeng Gao, Yuming Li, and Yunfang Wu. Thinkless: training-free inference-efficient method for reducing reasoning redundancy. arXiv preprint arXiv:2505.15684, 2025. [28] Kanishk Gandhi, Ayush Chakravarthy, Anikait Singh, Nathan Lile, and Noah D. Goodman. Cognitive behaviors that enable self-improving reasoners, or, four habits of highly effective stars, 2025. [29] Yang Yue, Zhiqi Chen, Rui Lu, Andrew Zhao, Zhaokai Wang, Yang Yue, Shiji Song, and Gao Huang. Does reinforcement learning really incentivize reasoning capacity in llms beyond the base model?, 2025. [30] Zichen Liu, Changyu Chen, Wenjun Li, Penghui Qi, Tianyu Pang, Chao Du, Wee Sun Lee, and Min Lin. Understanding r1-zero-like training: critical perspective, 2025. [31] Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Tiantian Fan, Gaohong Liu, Lingjun Liu, Xin Liu, Haibin Lin, Zhiqi Lin, Bole Ma, Guangming Sheng, Yuxuan Tong, Chi Zhang, Mofan Zhang, Wang Zhang, Hang Zhu, Jinhua Zhu, Jiaze Chen, Jiangjie Chen, Chengyi Wang, Hongli Yu, Weinan Dai, Yuxuan Song, Xiangpeng Wei, Hao Zhou, Jingjing Liu, Wei-Ying Ma, Ya-Qin Zhang, Lin Yan, Mu Qiao, Yonghui Wu, and Mingxuan Wang. Dapo: An open-source llm reinforcement learning system at scale, 2025. [32] Ronald Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine learning, 8:229256, 1992. [33] Zhiheng Xi, Dingwen Yang, Jixuan Huang, Jiafu Tang, Guanyu Li, Yiwen Ding, Wei He, Boyang Hong, Shihan Do, Wenyu Zhan, Xiao Wang, Rui Zheng, Tao Ji, Xiaowei Shi, Yitao Zhai, Rongxiang Weng, Jingang Wang, Xunliang Cai, Tao Gui, Zuxuan Wu, Qi Zhang, Xipeng Qiu, Xuanjing Huang, and Yu-Gang Jiang. Enhancing llm reasoning via critique models with test-time and training-time supervision, 2024. [34] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F. Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback. CoRR, abs/2203.02155, 2022. [35] Stephen Casper, Xander Davies, Claudia Shi, Thomas Krendl Gilbert, Jérémy Scheurer, Javier Rando, Rachel Freedman, Tomek Korbak, David Lindner, Pedro Freire, Tony Tong Wang, Samuel Marks, CharbelRaphael Segerie, Micah Carroll, Andi Peng, Phillip J.K. Christoffersen, Mehul Damani, Stewart Slocum, Usman Anwar, Anand Siththaranjan, Max Nadeau, Eric Michaud, Jacob Pfau, Dmitrii Krasheninnikov, Xin Chen, Lauro Langosco, Peter Hase, Erdem Biyik, Anca Dragan, David Krueger, Dorsa Sadigh, and Dylan Hadfield-Menell. Open problems and fundamental limitations of reinforcement learning from human feedback. Transactions on Machine Learning Research, 2023. Survey Certification, Featured Certification. [36] Rafael Rafailov, Yaswanth Chittepu, Ryan Park, Harshit Sikchi, Joey Hejna, W. Bradley Knox, Chelsea Finn, and Scott Niekum. Scaling laws for reward model overoptimization in direct alignment algorithms. In Amir Globersons, Lester Mackey, Danielle Belgrave, Angela Fan, Ulrich Paquet, Jakub M. Tomczak, and Cheng Zhang, editors, Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024, 2024. [37] Braden Hancock, Antoine Bordes, Pierre-Emmanuel Mazare, and Jason Weston. Learning from dialogue after deployment: Feed yourself, chatbot! In Anna Korhonen, David Traum, and Lluís Màrquez, editors, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3667 3684, Florence, Italy, July 2019. Association for Computational Linguistics. [38] Elie Bakouch, Leandro von Werra, and Lewis Tunstall. Open-r1: fully open reproduction of deepseek-r1. https://huggingface.co/blog/open-r1, 2025. [39] Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, Jie Liu, Lei Qi, Zhiyuan Liu, and Maosong Sun. Olympiadbench: challenging benchmark for promoting AGI with olympiad-level bilingual multimodal scientific problems. CoRR, abs/2402.14008, 2024. [40] Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: flexible and efficient rlhf framework. arXiv preprint arXiv: 2409.19256, 2024. [41] E.A. Dare. The role of STEM disciplines, real-world problems, 21st century skills, and STEM careers. https: // pdfs. semanticscholar. org/ 164b/ 8c3e8a1bd40fbd4cb907010541d828c2df89. pdf , 2021. Accessed via Semantic Scholar. [42] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. [43] Hanze Dong, Wei Xiong, Deepanshu Goyal, Yihan Zhang, Winnie Chow, Rui Pan, Shizhe Diao, Jipeng Zhang, Kashun Shum, and Tong Zhang. RAFT: reward ranked finetuning for generative foundation model alignment. Trans. Mach. Learn. Res., 2023, 2023. [44] Ganqu Cui, Yuchen Zhang, Jiacheng Chen, Lifan Yuan, Zhi Wang, Yuxin Zuo, Haozhan Li, Yuchen Fan, Huayu Chen, Weize Chen, Zhiyuan Liu, Hao Peng, Lei Bai, Wanli Ouyang, Yu Cheng, Bowen Zhou, and Ning Ding. The entropy mechanism of reinforcement learning for reasoning language models, 2025. [45] Xiaoying Zhang, Da Peng, Yipeng Zhang, Zonghao Guo, Chengyue Wu, Chi Chen, Wei Ke, Helen Meng, and Maosong Sun. Will pre-training ever end? first step toward next-generation foundation mllms via self-improving systematic cognition. arXiv preprint arXiv:2503.12303, 2025. [46] Yu Zhang, Jinlong Ma, Yongshuai Hou, Xuefeng Bai, Kehai Chen, Yang Xiang, Jun Yu, and Min Zhang. Evaluating and steering modality preferences in multimodal large language model. arXiv preprint arXiv:2505.20977, 2025. [47] Xiaoying Zhang, Baolin Peng, Ye Tian, Jingyan Zhou, Yipeng Zhang, Haitao Mi, and Helen Meng. Self-tuning: Instructing llms to effectively acquire new knowledge through self-teaching. arXiv preprint arXiv:2406.06326, 2024. 21 [48] Yu Zhang, Kehai Chen, Xuefeng Bai, Zhao Kang, Quanjiang Guo, and Min Zhang. Question-guided knowledge graph re-scoring and injection for knowledge graph question answering. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen, editors, Findings of the Association for Computational Linguistics: EMNLP 2024, pages 89728985, Miami, Florida, USA, November 2024. Association for Computational Linguistics. [49] Xiaoying Zhang, Baolin Peng, Kun Li, Jingyan Zhou, and Helen Meng. SGP-TOD: Building task bots effortlessly via schema-guided LLM prompting. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Findings of the Association for Computational Linguistics: EMNLP 2023, pages 1334813369, Singapore, December 2023. Association for Computational Linguistics."
        },
        {
            "title": "Contents",
            "content": "1 Introduction 2 Related Work 3 Limitations of RL with Numerical Feedback and the Promise of Natural Language Guidance"
        },
        {
            "title": "3.2 Promise of Learning from Natural Language Feedback . . . . . . . . . . . . . . .",
            "content": "4 Critique-GRPO"
        },
        {
            "title": "4.2 Online Learning with Critique-GRPO . . . . . . . . . . . . . . . . . . . . . . . .",
            "content": "5 Experiments 5.1 Experimental Setup . 5.2 Main Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.3 5.4 Investigation of Policy Exploration During RL Finetuning . . . . . . . . . . . . . Impact of Policy Shaping on RL Finetuning . . . . . . . . . . . . . . . . . . . . . 5.5 Qualitative Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 Conclusion Analysis of Cognitive Behaviors Prompts Leveraging Textual Critiques for Refining LLM Responses The Critique-GRPO Algorithm An Example of Successful Refinement using CoT Critique Responses Utilized in Qualitative Analysis 1 4 4 6 8 9 10 10 11 13 14 17 24 26 27 28"
        },
        {
            "title": "A Analysis of Cognitive Behaviors",
            "content": "To systematically investigate this question, we characterize six key cognitive behaviors that contribute to self-improving reasoning during RL fine-tuning, as follows: Subgoal Setting: Breaking down complex problems into smaller, manageable steps or subtasks. For example, Step 1... Step 2... Summarization: Summarizing the current state by identifying completed subtasks and determining what remains to be done. This helps guide the next steps in reasoning. For example, Now we have obtained..., next, we need to... Verification: Systematically checking intermediate results or computations to ensure correctness. For example, Lets verify this result by... Backtracking: Identifying errors or dead-ends in reasoning and explicitly revising previous methods or approaches. For example, This approach wont work because..., lets try another method... Backward Chaining: Reasoning from desired outcomes back to initial inputs or steps required to achieve the result. This is particularly applicable to multiple-choice questions where answer options are provided. For example, To get 24, could do 24 2 = 12... [28] Anticipation: Anticipating potential inaccuracies or exhaustively considering multiple possibilities to solve problem. For example, Alternatively, this problem can be solved by... We analyze the reasoning (cognitive) behaviors using the prompts shown below. When assessing the contributions of reasoning behaviors in Section 3 to successful problem-solving in RL fine-tuned models, we count each behavior appearing in the generated responses only once. For example, if the model produces multiple subgoals in single response, the occurrence of subgoal setting is counted as one. 24 Prompts for Analyzing Reasoning Behaviors System: You are helpful assistant. User: The following is chain-of-thought produced by language model in response to math & science problem: Question: <Question Content> Reasoning: <Model Reasoning> Ground Truth: <Ground Truth Content> Task 1: Answer Verification Determine whether the reasoning includes any explicit or implicit answer verification steps moments where the model checks intermediate computations or final results for correctness. Example: \"Lets verify this result by...\" - Report the number of distinct answer verification steps using: <count>n</count>. If none are found, return <count>0</count>. - If such behavior is present and the final answer matches the ground truth, indicate whether the behavior contributed to the correct answer using the format: contribution: yes/no. Task 2: Backtracking Behavior Determine whether the reasoning demonstrates backtracking where the model identifies an error or dead end and switches to different approach. Example: \"This approach wont work because..., lets try another method...\" - Report the number of distinct backtracking instances using: <count>n</count>. If none are found, return <count>0</count>. - If such behavior is present and the final answer matches the ground truth, indicate whether the behavior contributed to the correct answer using the format: contribution: yes/no. Task 3: Subgoal Setting Determine whether the reasoning includes any explicit subgoals intermediate steps that break the problem into smaller, manageable parts. Example: \"First, Ill try to..., then Ill...\" - Report the number of clearly defined subgoals using: <count>n</count>. If none are found, return <count>0</count>. - If such behavior is present and the final answer matches the ground truth, indicate whether the behavior contributed to the correct answer using the format: contribution: yes/no. Task 4: Backward Chaining Determine whether the reasoning includes backward chaining starting from the target result and reasoning backward to infer inputs or steps. Example: \"To get 24, could do 24 2 = 12...\" - Report the number of distinct backward chaining attempts using: <count>n</count>. If none are found, return <count>0</count>. - If such behavior is present and the final answer matches the ground truth, indicate whether the behavior contributed to the correct answer using the format: contribution: yes/no. Task 5: Anticipation Determine whether the reasoning includes enumeration or anticipation and re-proposal suggesting alternative approaches or revising prior methods. Examples: \"Alternatively, this problem can be solved by...\", \"Lets try different approach...\" - Report the number of such instances using: <count>n</count>. If none are found, return <count>0</count>. - If such behavior is present and the final answer matches the ground truth, indicate whether the behavior contributed to the correct answer using the format: contribution: yes/no. Task 6: Summarization Determine whether the reasoning includes summarization identifying completed subtasks, summarizing progress, and determining the next steps. Example: \"Now we have obtained..., next, we need to...\" - Report the number of summarization instances using: <count>n</count>. If none are found, return <count>0</count>. - If such behavior is present and the final answer matches the ground truth, indicate whether the behavior contributed to the correct answer using the format: contribution: yes/no."
        },
        {
            "title": "B Prompts",
            "content": "Training Prompt. The following training prompt is used during all RL fine-tuning experiments: Training Prompt System: You are helpful assistant. User: <Question Content> Please reason step by step and place your final answer within boxed. Prompt for Generating Chain-of-Thought Critique. We adopted prompt inspired by [17] to enable GPT-4o [42] to generate CoT critiques. For quality control, we retained only those model-generated critiques whose evaluative conclusions (correct/incorrect) aligned with rule-based verification. When inconsistencies occurred, we prompted the critique model to regenerate the critiques. Prompt for Generating Chain-of-Thought Critique System: You are science expert. student is trying to solve question. Please explain briefly (step-by-step) whether the students solution is correct or not. Finally, conclude your judgment with: Conclusion: correct/incorrect [END]. User: Question: <Question Content> Students Solution: <Generated Solution> Refinement Prompt. The following refinement prompt is used to guide the model in improving its response by incorporating the critique. Refinement Prompt System: You are helpful assistant. User: Given the following inputs: Question: <Question Content> Previous Solution: <Generated Solution> Critique: <Critique Content> Please re-answer by: - Correcting potential errors identified in the critique, if they exist. - Providing clear, step-by-step reasoning. - Placing your final answer within boxed. Ensure the revised solution addresses all issues raised in the critique. Future work could explore designing prompts [49] to enable LLMs to generate high-quality CoT critiques."
        },
        {
            "title": "C Leveraging Textual Critiques for Refining LLM Responses",
            "content": "We describe the process for leveraging these textual critiques to guide the refinement of LLMgenerated responses: 1. Initial Response Sampling: Given an LLM πθ parameterized by θ and set of questions {q}, we sample multiple initial responses for each question {y(i) i=1 πθ( q), where is the number of samples. 0 }k 2. Response Evaluation and Critique Generation: We use an evaluation function Eval(q, y0) to assess the correctness of each response y0. The function outputs 1 if y0 is correct and 0 otherwise. Specifically, we adopt model-based evaluation with reasoning-based reward model πRM . The reasoning-based reward model generates CoT critique c(i) 0 ), where Ic is predefined instruction (detailed in Appendix B). Based on the binary correctness label within c(i) CoT, we construct the corresponding heuristic-based critiques: an indicative critique c(i) (containing only the correctness label) and critique with ground truth c(i) GT (correctness label plus the known ground truth answer for q). CoT πRM ( Ic, q, y(i) . 0 }k To focus on the models ability to learn from critiques for initially incorrect solutions and to control for spontaneous self-correction, we identify persistently failed questions. question is classified as persistently failed if all of its initial responses {y(i) i=1 are deemed incorrect based on the labels from their respective CoT critiques. For each such incorrect response y(j) from persistently failed 0 question, we form triplet (q, y(j) GT , or c(j) 3. Refinement Generation: For each selected triplet (q, y(j) incorrect response, we prompt the original LLM πθ to generate refined response y(j) Irefine, q, y(j) in Appendix B), the original question q, the initial failed response y(j) c(j). 0 , c(j)) corresponding to an initial refined πθ( 0 , c(j)). This generation is conditioned on specific refinement instruction Irefine (detailed 0 , and its associated critique 0 , c(j)), where c(j) is one of the three critique types: c(j) CoT, c(j) The full process is summarized in Algorithm 1. An example illustrating the refinement process, including the application of CoT critique, is provided in Appendix E. 27 The Critique-GRPO Algorithm The Critique-GRPO algorithm is summarized in Algorithm 2. Algorithm 2 Critique-GRPO: Online Policy Optimization Framework with Critiques 1: Input: Pretrained LLM policy πold parameterized by θ, reward model πRM , set of questions = {q}, refinement instruction Irefine, critique instruction Ic 2: Goal: Improve LLM policy by learning from initial responses and their refinements 3: Step 1: Initial Response Sampling 4: for each question do 5: 6: Sample initial responses from the old policy: {y(i)}n Score the responses using the reward model πRM to obtain CoT critiques: i=1 πold(q) {c(i) CoT}n i=1 πRM ( Ic, q, y(i)) Translate binary correctness labels in the critiques into scalar reward scores: {R(i)}n i=1 7: 8: end for 9: Step 2: Refinement with Critique 10: for each initial response y(i) {y(i)}n 11: i=1 do Generate refined responses conditioned on (q, y(i), c(i) CoT) and instruction Irefine: refined πold( Irefine, q, y(i), c(i) y(i) Score the refined responses using the reward model (or rule-based evaluation function): CoT) 12: {R(i) refine}n i= 13: end for 14: Sample subset of refinements to mitigate distributional shifts: {y(i) 15: Combine the sampled refinements with the initial responses to form mixed group: refined}k i=1 {y(i) refined}n i=1 {y(i)}n i=1 {y(i) refined}k i=1 16: Step 3: On-Policy Optimization 17: Fine-tune the model on the mixed group of responses using scalar rewards with the CritiqueGRPO training objective: JCritique-GRPO(θ) =E qQ,{y(i)}n i=1πold(q),{y(i ) refined}k i=1 y(i) (cid:88) (cid:88) (cid:20) 1 (cid:124) (cid:125) (cid:123)(cid:122) Objective for Initial Responses (θ)A(i) r(i) t=1 i=1 + 1 (cid:124) πold(Irefine,q,y(i ),c(i) CoT ) y(i ) refined (cid:88) (cid:17) r(i) refined,t(θ) (cid:16) t= (cid:123)(cid:122) Objective for Refined Responses (cid:88) i=1 (cid:21) , A(i) (7) (cid:125) 18: Compute the advantage for each token in response: A(i)/(i) = R(i)/(i) mean({R(i)}n i=1 {R(i) refined}k i=1) 19: Calculate the probability ratios for initial and refined responses: πθ(y(i) refined,tq, y(i) πθ(y(i) refined,tq, y(i) refined,<t) refined,<t) + γ r(i) (θ) = πθ(y(i) πold(y(i) q, y(i) <t) q, y(i) <t) , (r(i) refined,t(θ)) = 20: Output: Fine-tuned LLM policy πθ"
        },
        {
            "title": "E An Example of Successful Refinement using a CoT Critique",
            "content": "The example below demonstrates successful refinement using CoT critique. This output was generated by the best-performing RL-finetuned Qwen3-8B model using GRPO algorithm in Section 3, utilizing the refinement prompt detailed in Appendix E. Question 2. sphere is inscribed in cone, such that the area of the sphere and the area of the base of the cone are equal. Calculate the cosine of the angle of the axial section of the cone at the vertex which is also the apex of the cone. CoT Critique Lets analyze the students solution step-by-step and identify any errors: ### Step 1: Geometry Understanding The student correctly identifies the geometry of the problem, noting that the sphere is tangent to the base and sides of the cone, and that the axial section forms an isosceles triangle. ### Step 2: Variable Definitions The student defines as the radius of the sphere, as the radius of the base of the cone, and as the height of the cone. These definitions are appropriate. ### Step 3: Relating Sphere and Cone The student attempts to relate r, R, and using similar triangles. However, the equation derived: = Rh + is not directly applicable to the problem as it does not consider the specific condition that the areas are equal. ### Step 4: Equating Areas The student correctly equates the areas of the sphere and the base of the cone: 4πr2 = πR2 = = 2r This step is correct. ### Step 5: Substitution and Error The student substitutes = 2r into the earlier equation, leading to: = (2r)h + 2r This substitution is unnecessary and results in incorrect simplification. The correct approach would involve using the inradius formula for the triangle formed by the axial section of the cone. ### Step 6: Angle Calculation The student calculates tan θ = 1 and concludes θ = 45, leading to cos(90) = 0. This is incorrect because the angle calculation should be based on the relationship between and R, derived from the inradius formula. ### Step 7: Precise Calculation The student attempts to verify the angle calculation but repeats the same incorrect logic, leading to the same incorrect conclusion. ### Correct Solution Approach The correct approach involves using the inradius formula for the triangle formed by the axial section of the cone: - Calculate the inradius using the area and semi-perimeter of the triangle. - Substitute = 2r and solve for in terms of r. - Relate and θ using trigonometric identities. - Calculate cos(2θ) using the double angle identity. The correct final answer is: Conclusion: incorrect [END] cos(2θ) = 29 7 25 . Previous Solution (Failed Response) To solve the problem, lets break it down step by step. ### Step 1: Geometry Understanding - **Sphere Inscribed in Cone**: The sphere is tangent to the base and the sides of the cone. - **Axial Section**: The cross-section along the cones central axis forms an isosceles triangle. - **Vertex Angle**: Let the vertex angle of the cone be 2θ, so each base angle is θ. ### Step 2: Variable Definitions - r: Radius of the sphere. - R: Radius of the base of the cone. - h: Height of the cone. ### Step 3: Relating the Sphere and the Cone The sphere is inscribed in the cone, so: - The vertical distance from the center of the sphere to the base is r. - The horizontal distance from the center of the sphere to the cones side is r. Using similar triangles, we relate r, R, and h: = = rh = R(h r) = r(h + R) = Rh = = Rh + . ### Step 4: Equating Areas The area of the sphere is 4πr2, and the area of the base of the cone is πR2. From the problem: 4πr2 = πR2 = 4r2 = R2 = = 2r. ### Step 5: Substituting = 2r Substituting = 2r into the earlier equation: = (2r)h + 2r = r(h + 2r) = 2rh = + 2r = 2h = = 2r. ### Step 6: Finding the Angle θ In the axial section of the cone, we have right triangle with: - Height = 2r. - Base radius = 2r. The tangent of θ is: The cosine of the vertex angle 2θ is: tan θ = = 2r 2r = 1 = θ = 45. cos(2θ) = cos(90) = 0. ### Step 7: Verifying with Precise Calculations For the axial section, the cone forms an isosceles triangle with: - Two equal slant heights l. - Base 2R = 4r. - Height = 2r. Using the Pythagorean theorem, the slant height is: (cid:112) = h2 + R2 = (cid:112)(2r)2 + (2r)2 = 8r2 = 2r 2. The tangent of the base angle θ is: Thus, the vertex angle is 2θ = 90, and: tan θ = = 2r 2r = 1 = θ = 45. cos(90) = 0. ### Final Answer 0 Refined Solution (1/2) Lets revise the solution by addressing the issues identified in the critique and providing clear, step-by-step reasoning. ### Step 1: Geometry Understanding The problem involves cone with sphere inscribed in it, such that the sphere is tangent to both the base and the sides of the cone. Additionally, the area of the sphere equals the area of the base of the cone. ### Step 2: Variable Definitions - r: Radius of the sphere. - R: Radius of the base of the cone. - h: Height of the cone. ### Step 3: Relating the Sphere and the Cone The sphere is inscribed in the cone, so: - The vertical distance from the center of the sphere to the base is r. - The horizontal distance from the center of the sphere to the cones side is r. Using similar triangles, we relate r, R, and h: = = r(h + R) = Rh = = Rh + . ### Step 4: Equating Areas The area of the sphere is 4πr2, and the area of the base of the cone is πR2. From the problem: 4πr2 = πR2 = 4r2 = R2 = = 2r. ### Step 5: Substituting = 2r Substitute = 2r into the earlier equation: = (2r)h + 2r = r(h + 2r) = 2rh = + 2r = 2h = = 2r. ### Step 6: Finding the Angle θ In the axial section of the cone: - The height is = 2r. - The base radius is = 2r. The tangent of θ is: The cosine of the vertex angle 2θ is: tan θ = = 2r 2r = 1 = θ = 45. cos(2θ) = cos(90) = 0. However, this leads to an incorrect conclusion. Lets apply the **inradius formula** for more accurate approach. ### Correct Approach: Using the Inradius Formula The axial section of the cone is an isosceles triangle with: - Height h. - Base 2R. - Slant height = The inradius of this triangle is given by: h2 + R2. = , where is the area, and is the semi-perimeter. ### Area and Semi-Perimeter: The area of the triangle is: = 1 2R = Rh. 31 Refined Solution (2/2) The semi-perimeter is: = 2l + 2R 2 = + = (cid:112) h2 + R2 + R. ### Inradius: Substitute and s: = Rh h2 + R2 + . Given = 2r and = 2r, substitute into the equation: = (2r)(2r) (cid:112)(2r)2 + (2r)2 + 2r = 4r2 8r2 + 2r = 4r2 2 + 2r . 2r Simplify: = 4r 2 + 1) 2( = 2r 2 + 1 . ### Rationalizing the Denominator: = 2r 2 + 1 2 1 2 1 = 2r( 2 1) 2 1 = 2r( 2 1). ### Step 7: Find the Final Angle Given = 2r and = 2r, the slant height is: (cid:112) h2 + R2 = (cid:112)(2r)2 + (2r)2 = = 8r2 = 2r 2. The tangent of θ is: tan θ = = 2r 2r = 1 = θ = 45. The cosine of the vertex angle 2θ is: 1 (cid:0) 3 1 + (cid:0) 3 4 4 (cid:1)2 (cid:1)2 = 7 16 25 16 = 7 25 . cos(2θ) = 1 tan2 θ 1 + tan2 θ = ### Final Answer: 7"
        },
        {
            "title": "F Responses Utilized in Qualitative Analysis",
            "content": "Question Evaluate sin(arcsin 0.4 + arcsin 0.5) sin(arcsin 0.5 arcsin 0.4). Response Generated by Qwen3-8B Base Model (1/4) sin(arcsin 0.4 + arcsin 0.5) multiplied by Okay, so need to evaluate this expression: sin(arcsin 0.5 arcsin 0.4). Hmm, that looks bit complicated with all those inverse sine functions. Let me try to break it down step by step. First, let me recall some trigonometric identities that might help here. remember that there are formulas for sin(A+B) and sin(AB). Maybe can use those? Let me write them down: sin(A + B) = sin cos + cos sin So if let = arcsin 0.4 and = arcsin 0.5, then the expression becomes: sin(A B) = sin cos cos sin sin(A + B) sin(A B) Wait, so maybe can compute sin(A + B) and sin(A B) separately using those identities and then multiply them together? But before that, maybe should figure out what and are? Well, is arcsin 0.4, which means that sin = 0.4, and similarly sin = 0.5. But need cos and cos to use the identities. Since sin2 θ + cos2 θ = 1, can find cos and cos B. Let me compute cos first. For angle A, sin = 0.4, so 1 sin2 = cos = 0.84... Let me calculate that. 0.84 is 84 1 0.16 = 100 , so 0.84 = (cid:112) 0.84. 84 10 . Simplify 84... 84 = Wait, 4 21 = 2 21. So 84 10 2 = 21 10 21 5 . = 0.84 = 84 is 21/5. Wait, but 21/5. So cos = 4 21 which is 2 Wait, let me check that again. 21/10 = 21/5 is about 0.9166. 21 is approximately 4.583, so Let me check if thats correct. Since sin is 0.4, then cos should be 0.84 1 0.16 = 0.9165, which matches. So thats correct. Similarly, for angle B, sin = 0.5, so cos = 2 0.8660. Thats right because sin (cid:0) π 3 1 0.25 = 2 . 3 2 . Okay, so now can compute sin(A + B) and sin(A B) using the identities. 84/10 = 2 (cid:1) = 0.5, so cos (cid:0) π 21, so yes, 0.75 = (cid:1) = 3 6 6 So cos = Starting with sin(A + B): sin(A + B) = sin cos + cos sin Plugging in the values: sin = 0.4, cos = 3 , cos = 21 5 , sin = 0. So: sin(A + B) = 0.4 (cid:32) 3 2 (cid:33) + (cid:32) 21 5 (cid:33) (0.5) 33 Response Generated by Qwen3-8B Base Model (2/4) Let me compute each term: First term: (cid:32) (cid:33) 0.4 3 2 (cid:19) = (cid:18) 0.4 3 = 0.2 3 Second term: So Hmm, maybe can write 0.2 as (cid:33) (cid:32) 21 5 (0.5) = (cid:32) 21 (cid:19) (cid:33) (cid:18) 1 2 = 21 10 21 10 5 , so sin(A + B) = 0.2 3 + 5 ? Let me check: 0.2 is 1 1 5 3 + 21 10 . To combine these terms, maybe get common denominator? Yes, common denominator is 10: So 2 3 10 + 21 10 = 3 + 10 21 sin(A + B) = 2 3 + 10 Okay, now moving on to sin(A B): sin(A B) = sin cos cos sin Which is similar to the previous one, but with minus sign. So plugging in the same values: sin cos cos sin = 0.4 (cid:32) 3 2 (cid:33) (cid:32) 21 5 (cid:33) (0.5) Which is the same as: First term: Again, converting 0.2 to 1 5 : So 0.2 3 21 10 1 5 3 21 10 = 3 10 21 sin(A B) = 2 3 10 Therefore, the original expression is: sin(A + B) sin(A B) = (cid:32) 2 3 + 10 21 (cid:33) (cid:32) 3 10 (cid:33) 21 34 Response Generated by Qwen3-8B Base Model (3/4) Hmm, this looks like difference of squares. Because (a + b)(a b) = a2 b2. Let me check: 21. Then the product is (a + b)(a b) = a2 b2. So compute Let me denote = 2 a2 b2: 3 and = a2 = (2 3)2 = 4 3 = 12 b2 = ( 21)2 = Therefore, a2 b2 = 12 21 = 9 Therefore, the product is 9 10 10 = 9 100 Wait, so the entire expression simplifies to 9 didnt make mistake. First, sin(A + B) and sin(A B) were calculated as 2 100 ? But let me check my steps again to make sure and 3+ 10 21 21 3 10 . Multiplying them gives: (2 3)2 ( 100 21)2 = 12 21 100 = 9 That seems correct. But let me verify with another approach. Maybe using another identity? Alternatively, remember that sin(A + B) sin(A B) can be expressed as sin2 sin2 B. Wait, is that correct? Wait, let me recall the identity: sin(A + B) sin(A B) = sin2 sin2 Let me check that. Yes, because: sin(A + B) sin(A B) = [sin cos + cos sin B][sin cos cos sin B] = (sin cos B)2 (cos sin B)2 = sin2 cos2 cos2 sin2 But thats not exactly sin2 sin2 B. Wait, unless theres another identity. Alternatively, maybe using product-to-sum formulas? Alternatively, let me compute sin2 sin2 B. Let me see: sin2 sin2 = (sin sin B)(sin + sin B). Not sure if that helps. But let me check if sin(A + B) sin(A B) equals sin2 sin2 B. Let me compute sin2 sin2 B: sin2 sin2 = (0.4)2 (0.5)2 = 0.16 0.25 = 0.09 = 9 100 Which is exactly the same as before! So thats shortcut. Therefore: sin(A + B) sin(A B) = sin2 sin2 = 9 100 Therefore, the answer is 9 100 . So that confirms the previous result. Therefore, the value of the original expression is 9 100 . But let me check once more with actual numbers to make sure. Let me compute arcsin(0.4) and arcsin(0.5) numerically, then compute the sines of their sum and difference, multiply them, and see if its approximately 0.09. First, arcsin(0.5) is π/6 0.5236 radians. arcsin(0.4)... Let me calculate that. Let me recall that sin(0.4115 radians) 0.4. Let me check with calculator: Well, sin(0.4115) sin(0.4115) 0.4. Let me check with calculator steps: But since dont have calculator here, maybe approximate. Alternatively, use known values. For example, sin(π/6) = 0.5, sin(π/12) 0.2588, so 0.4 is between π/12 and π/6. Let me approximate arcsin(0.4). 35 21 3 + 3+ 10 3 10 100 . 21 1. 3 3.464, 3 1.732, 21 4.5837. So 2 Response Generated by Qwen3-8B Base Model (4/4) Let me use Taylor series or something? Maybe not. Alternatively, just take that arcsin(0.4) 0.4115 radians (as thought before). Let me take that as approximate value. So 0.4115 radians, = π/6 0.5236 radians. Compute + 0.4115 + 0.5236 0.9351 radians. Compute sin(0.9351). Let me recall that sin(π/3) 0.8660, π/3 1.0472 radians. So 0.9351 is less than π/3. Let me compute sin(0.9351). Let me use calculatorlike approximation. Alternatively, use the value we found earlier: 2 . Let me compute that numerically. 21 21 4.5837. Then 2 First, 3.464 + 4.5837 8.0477. Divide by 10: 0.80477. So sin(A + B) 0.8048. Similarly, sin(A B) = 2 3.4644.5837 10 0.11197. Therefore, multiplying them: 0.8048 (0.11197) 0.090. Which is 0.09, which is 9 100 . So that matches. Therefore, the exact value is 9 100 , and the approximate decimal is 0.09. So that seems correct. Therefore, the answer is 9 But let me check once more if theres any mistake in the sign. Wait, when calculated sin(A B), had 2 21 is negative, so sin(A B) is negative. Then sin(A + B) is positive, so multiplying them gives negative. Which matches with 9 100 . So thats correct. Alternatively, if think about angles: is arcsin(0.4), which is in the first quadrant, is arcsin(0.5) which is π/6, also first quadrant. So + is the sum of two first quadrant angles. Depending on their measures, + could be in the first or second quadrant. Similarly, could be positive or negative. Let me check if AB is positive or negative. Since arcsin(0.4) 0.4115 radians, arcsin(0.5) 0.5236 radians. So 0.4115 0.5236 0.1121 radians. So negative angle, which is equivalent to positive angle by adding 2π, but sine is an odd function, so sin(x) = sin x. Therefore, sin(A B) = sin(B A). So the negative value makes sense. Therefore, multiplying sin(A + B) which is positive with sin(A B) which is negative gives negative result. So 9 100 . Let me check once more with another identity. Alternatively, using complex numbers or other methods? Maybe not necessary. Since two different approaches gave me the same answer, and numerical approximation also matches, think its safe. But just to make sure, let me compute sin2 sin2 B: 100 is correct. Therefore, think the answer is 9 21 4.583 and 3 3.464, so 2 . Since 3 10 3 21 sin2 = (0.4)2 = 0. 36 Response Generated by RL-Finetuned Qwen3-8B using Critique-GRPO (1/2) Okay, so need to evaluate this expression: sin(arcsin 0.5 arcsin 0.4). Hmm, let me think. sin(arcsin 0.4 + arcsin 0.5) multiplied by First, remember that arcsin is the inverse sine function, so arcsin 0.4 is an angle whose sine is 0.4, and similarly for arcsin 0.5. Let me denote some variables to make this easier. Let me set α = arcsin 0.4 and β = arcsin 0.5. Then the expression becomes sin(α + β) sin(β α). Alright, so need to compute sin(α+β) and sin(βα) separately and then multiply them. Maybe can use sineaddition formulas here. Let me recall that sin(A+B) = sin cos B+cos sin B, and sin(A B) = sin cos cos sin B. Since α and β are arcsin values, know that sin α = 0.4 and sin β = 0.5. Therefore, need to find cos α and cos β to use in the formulas. For cos α, since sin2 α + cos2 α = 1, we have Note that 0.84 = 84 100 = 21 25 , so cos α = (cid:112) cos α = 1 sin2 α = (cid:113) 21 21 5 . 25 = 1 0.16 = 0.84. Similarly, (cid:113) cos β = 1 sin2 β = 1 0.25 = 0.75 = (cid:113) 3 4 = 3 . Now, let me compute sin(α + β). Using the formula, sin(α + β) = sin α cos β + cos α sin β = 0.4 (cid:32) 3 2 (cid:33) + (cid:32) 21 5 (cid:33) (0.5). Compute each term: Therefore 3 2 = 2 5 3 2 3 5 , = 21 5 0.4 0.5 = 21 . sin(α + β) = 3 5 21 + = 2 3 + 10 . Similarly, compute sin(β α): sin(β α) = sin β cos α cos β sin α = 0.5 (cid:32) (cid:33) 3 2 (0.4). 21 5 Thus sin(β α) = 21 10 2 3 10 = 21 2 10 3 . Therefore, the original expression is (cid:32) 3 + 10 21 (cid:33) (cid:32) (cid:33) 3 . 21 2 10 Let = 2 21)2 (2 ( 21 and = 21 2 3 + 3)2 = 21 12 = 9, so the value is 9 100 . 37 3; then the product is 100 . Note that = Response Generated by RL-Finetuned Qwen3-8B using Critique-GRPO (2/2) Therefore, the value of the original expression is , which is 0.09. 9 100 Let me check with approximate values. Take arcsin 0.5 = π 6 0.5236 rad and arcsin 0.4 0.4115 rad. Then α + β 0.9351 rad, so sin(α + β) 0.805; also β α 0.1121 rad, so sin(β α) 0.1118. Their product 0.805 0.1118 0.09, confirming Final Answer 9 100 9 ."
        },
        {
            "title": "To evaluate the expression",
            "content": "sin(arcsin 0.4 + arcsin 0.5) sin(arcsin 0.5 arcsin 0.4), we begin by introducing substitution to simplify the notation. Let α = arcsin 0.4, β = arcsin 0.5, so that sin α = 0.4 and sin β = 0.5. We rewrite the expression as sin(α + β) sin(β α). Step 1: Use the Sine Addition and Subtraction Formulas Recall sin(α + β) = sin α cos β + cos α sin β, sin(β α) = sin β cos α cos β sin α. Using we compute cos α = 21 5 , cos β = 3 , sin(α + β) = 2 3 + 10 21 , sin(β α) = 21 2 3 . Step 2: Multiply the Two Sine Values 2 (cid:33) (cid:32) 21 (cid:32) 3 + 10 21 2 10 (cid:33) = ( 21)2 (2 100 3) = 9 100 . Final Answer 9"
        }
    ],
    "affiliations": [
        "Shanghai Artificial Intelligence Laboratory",
        "The Chinese University of Hong Kong, HCCL",
        "The Chinese University of Hong Kong, MMLab",
        "University of Cambridge"
    ]
}