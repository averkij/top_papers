{
    "paper_title": "OmniVideoBench: Towards Audio-Visual Understanding Evaluation for Omni MLLMs",
    "authors": [
        "Caorui Li",
        "Yu Chen",
        "Yiyan Ji",
        "Jin Xu",
        "Zhenyu Cui",
        "Shihao Li",
        "Yuanxing Zhang",
        "Jiafu Tang",
        "Zhenghao Song",
        "Dingling Zhang",
        "Ying He",
        "Haoxiang Liu",
        "Yuxuan Wang",
        "Qiufeng Wang",
        "Zhenhe Wu",
        "Jiehui Luo",
        "Zhiyu Pan",
        "Weihao Xie",
        "Chenchen Zhang",
        "Zhaohui Wang",
        "Jiayi Tian",
        "Yanghai Wang",
        "Zhe Cao",
        "Minxin Dai",
        "Ke Wang",
        "Runzhe Wen",
        "Yinghao Ma",
        "Yaning Pan",
        "Sungkyun Chang",
        "Termeh Taheri",
        "Haiwen Xia",
        "Christos Plachouras",
        "Emmanouil Benetos",
        "Yizhi Li",
        "Ge Zhang",
        "Jian Yang",
        "Tianhao Peng",
        "Zili Wang",
        "Minghao Liu",
        "Junran Peng",
        "Zhaoxiang Zhang",
        "Jiaheng Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advances in multimodal large language models (MLLMs) have demonstrated substantial potential in video understanding. However, existing benchmarks fail to comprehensively evaluate synergistic reasoning capabilities across audio and visual modalities, often neglecting either one of the modalities or integrating them in a logically inconsistent manner. To bridge this gap, we introduce OmniVideoBench, a large-scale and rigorously designed benchmark dedicated to assessing synergistic audio-visual understanding, with a strong emphasis on modality complementarity and logical consistency. Specifically, OmniVideoBench comprises 1000 high-quality question-answer(QA) pairs, each annotated with step-by-step reasoning traces, derived from 628 diverse videos ranging from several seconds to 30 minutes, and manually verified to guarantee complete correctness and uniqueness. Moreover, OmniVideoBench encompasses 13 carefully designed question types, covering temporal reasoning, spatial localization, counting, causal inference, summarization, and beyond, thereby capturing the essential challenges of video understanding. Evaluation of multiple MLLMs on OmniVideoBench reveals a pronounced gap between model performance and human reasoning, with open-source models lagging significantly behind their closed-source counterparts, underscoring the inherent difficulty of genuine audio-visual reasoning. We will release OmniVideoBench to foster the development of MLLMs with stronger and more generalizable reasoning capabilities."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 1 ] . [ 1 9 8 6 0 1 . 0 1 5 2 : r 2025-10OmniVideoBench: Towards Audio-Visual Understanding"
        },
        {
            "title": "Evaluation for Omni MLLMs",
            "content": "NJU-LINK Team"
        },
        {
            "title": "Full author list in Contributions",
            "content": "Abstract Recent advances in multimodal large language models (MLLMs) have demonstrated substantial potential in video understanding. However, existing benchmarks fail to comprehensively evaluate synergistic reasoning capabilities across audio and visual modalities, often neglecting either one of the modalities or integrating them in logically inconsistent manner. To bridge this gap, we introduce OmniVideoBencha, large-scale and rigorously designed benchmark dedicated to assessing synergistic audio-visual understanding, with strong emphasis on modality complementarity and logical consistency. Specifically, OmniVideoBench comprises 1000 high-quality question-answer(QA) pairs, each annotated with step-by-step reasoning traces, derived from 628 diverse videos ranging from several seconds to 30 minutes, and manually verified to guarantee complete correctness and uniqueness. Moreover, OmniVideoBench encompasses 13 carefully designed question types, covering temporal reasoning, spatial localization, counting, causal inference, summarization, and beyond, thereby capturing the essential challenges of video understanding. Evaluation of multiple MLLMs on OmniVideoBench reveals pronounced gap between model performance and human reasoning, with open-source models lagging significantly behind their closed-source counterparts, underscoring the inherent difficulty of genuine audio-visual reasoning. We will release OmniVideoBench to foster the development of MLLMs with stronger and more generalizable reasoning capabilities. ahttps://github.com/NJU-LINK/OmniVideoBench"
        },
        {
            "title": "1 Introduction",
            "content": "Multimodal large language models (MLLMs) have recently made impressive progress in bridging vision, language, and audio (Yin et al., 2024; Song et al., 2025; Cheng et al., 2025). While early benchmarks primarily focused on image-text alignment or visual reasoning (Xu et al., 2025a; Chen et al., 2024a; Yue et al., 2024a), the integration of video and audio presents quite different challenge: models must jointly process long temporal sequences, dynamic scene transitions, and complementary acoustic cues. Despite rapid advances, evaluation of MLLMs on audio-visual reasoning remains underdeveloped. Existing benchmarks (Li et al., 2024a; Hong et al., 2025) often (i) focus on short video clips that underrepresent long-term temporal dependencies, (ii) emphasize single modality (e.g., vision) while treating audio as auxiliary or optional. As result, current evaluations fail to capture the challenges inherent to comprehensive video understanding, where audio and vision must be integrated consistently and logically to support robust inference. To address these limitations, we introduce OmniVideoBench, high-quality benchmark designed for evaluating audio-visual reasoning abilities in MLLMs. Specifically, first, we collect 628 diverse videos spanning up to 30 minutes across 8 major categories and 68 subcategories, covering realistic contexts such as news, sports, documentaries, vlogs, and ego-centric recordings. Then, we construct 1,000 high-quality question-answer pairs based on these videos, and each pair is annotated with step-by-step reasoning chains as shown in Figure 1, where these reasoning steps explicitly indicate modality and evidence information. This design not only strengthens the reliability of the evaluation but also provides unique signal for analyzing how models reason, rather than just the final answers. Based on our OmniVideoBench, we conduct extensive evaluations of both closed-source and open-source MLLMs, and several insightful findings are as follows: OmniVideoBench poses significant challenges for Omni-Modal Language Models. Current MLLMs have not achieved passing score (<60%) on OmniVideoBench. The best-performing model, Gemini-2.0-Pro, only achieves an accuracy of 58.90%. Except for the newly proposed Qwen3-Omni, the performance of open-source models is close to random. 1 Figure 1: Examples in OmniVideoBench (V presents vision and presents audio), and we present the atomic reasoning traces for these examples. Omni-understanding abilities on long videos have significant improvement room. Although some leading models (such as Gemini-2.5-pro) demonstrate relatively robust performance on long videos, other models (e.g., Gemini-2.0-Flash, Qwen3-Omni-30B-A3B) still struggle on long video understanding. Performance varies lot for videos with different audio signals. Gemini-2.5-Pro only achieves 38.46% accuracy on videos with music signal, while the results on sound and speech are 57.72% and 61.66%, respectively. Performance on different task types differs lot. For example, Gemini-2.5-Pro achieves accuracy below 50% on the background and music understanding task, which requires low-semantic acoustic cues (e.g., musical style, tempo changes), and the accuracy results on the relationship reasoning and summarization tasks are more than 80%."
        },
        {
            "title": "2.1 Overview",
            "content": "OmniVideoBench is benchmark for evaluating the audio-visual collaborative reasoning of MLLMs. The main task in the evaluation requires model to process video, its audio, and associated text to generate textual answer supported by explicit reasoning steps. This process assesses the models ability to synthesize information across modalities, from recognizing objects to comprehending complex scene dynamics and context. This section details the benchmarks design principles, annotation protocols, and dataset statistics. 2 Figure 2: The complete pipeline of data collection, annotation, and refinement, where filtering and refinement serve as two key processes for quality assurance."
        },
        {
            "title": "2.2 Video Collection",
            "content": "OmniVideoBench is composed of real-world videos sourced from YouTube1 and Bilibili2. These videos feature rich audiovisual content. Therefore, comprehensive understanding necessitates the accurate processing and integration of both audio and visual modalities for reasoning. Regarding video richness, we primarily focus on two dimensions: type and duration. For type diversity, we categorize videos into eight broad classes: Vlog, News, Cartoon, Sports, Documentary, TV, Ego, and Others. Each class is further subdivided into nearly seventy fine-grained subcategories, which facilitates video retrieval and ensures broad coverage. Video categories are unevenly distributed. News and documentary videos have dense audio that nearly covers visual content, making them unsuitable for audio-visual reasoning tasks; thus, we manually controlled the video type distribution. For duration diversity, we restrict video lengths to the range of several seconds to 30 minutes, so as to evaluate reasoning across varying temporal scales. Building upon this foundation, we established set of rigorous video collection criteria that not only ensure the quality of the videos themselves, like resolution, but also guarantee the richness and diversity of their audio and visual content. To further avoid data overlap with existing training sets (e.g., popular TV shows), we restrict the selection to recent publications. The detailed collection principles are provided in Appendix B."
        },
        {
            "title": "2.3 Data Annotation",
            "content": "After collecting high-quality videos, we carried out manual annotation. Compared with automated annotation, automated methods cap the evaluation ceiling by the capabilities of the annotating model, whereas manual annotation produces questions that are closer to real-world needs. In Figure 2, we first designed multiple-choice questions consisting of the question stem, the correct answer, and several distractors, to facilitate convenient evaluation of model performance. At this stage, we obtained approximately 2,500 QA pairs. We categorize the tasks into 13 types: Fine-grained Perception, Spatial Reasoning, Attribute Comparison, Background & Music Understanding, Counting, Temporal Understanding, Summarization, Sentiment Analysis, Causal Reasoning, Relationship Reasoning, Reference Reasoning, Ego Reasoning, and Hypothetical Reasoning. In this design, each question requires reliance on audio-visual reasoning, and the answer must be both correct and unique with no alternative plausible interpretations in the video. Moreover, we require that questions should not depend on video resolution or frame rate. Cases where the target object is extremely small, blurred, and barely recognizable to the human eye, or where the relevant event occurs only within an instant, are excluded. In addition, we established the following rules to minimize the interference caused by extraneous textual information. Questions should avoid redundant information. We minimize unnecessary details in the question text, such as the gender, clothing, or exact speech of characters, as long as doing so does not affect the correctness or uniqueness of the answer. This serves two purposes: reducing textual cues that the model could exploit and increasing question difficulty to better test its audio-visual understanding. The length of answers is capped. To prevent the answer text itself from providing excessive cues to 1https://www.youtube.com/ 2https://www.bilibili.com/ 3 Video Statistics Annotation Statistics #Major Categories #Subcategories Avg. Duration Min. Resolution Max. Resolution 8 68 #Task Types Avg. Question Len. 384.24 Avg. Answer Len. 480p 1080p Avg. Reasoning Steps Audio Types (Sp:So:Mu) 13 14.68 words 4.92 words 5.68 762:147:91 Table 1: Dataset statistics divided into video-level and annotation-level information. the model, which could reduce the extent to which the evaluation reflects its understanding and reasoning over audio and visual modalities, we impose limit on answer length. This constraint ensures that the results more faithfully capture the models multimodal comprehension and reasoning capabilities. The format of options must be consistent. Here, format refers to aspects such as length, tone, style, and variation patterns. If these features are inconsistent, they may provide the model with unintended cues for reasoning. For instance, when three options are considerably longer than the remaining one, when three options adopt casual tone while the other is markedly formal, such discrepancies undermine the assumption that each option should have an equal probability of being chosen, thereby compromising the fairness of the evaluation. Negative options must be relevant to the question. We require that all distractors appear in the video and maintain relevance to the question. Without this constraint, the model could easily eliminate distractors, greatly reducing the need for reasoning. Options should maintain consistent semantic distance. We formalize semantic distance as the number of differing semantic units between options. Let an option oi be represented as set of semantic units Si. The semantic distance between two options oi and oj is defined as: d(oi, oj) = SiSj (1) where denotes the symmetric difference, capturing the distinct semantic units between two options. To prevent models from exploiting unbalanced textual cues rather than performing genuine audio-visual reasoning, we require that all distractors have consistent distances from one another and from the correct option."
        },
        {
            "title": "2.4 Quality Assurance",
            "content": "We employed an advanced MLLM (i.e., Gemini 2.0 Flash), with strong audiovisual perception and comprehension capabilities, as well as long-context processing ability, to filter out questions that could be resolved using only single modality. If the model successfully selected the correct answer with plausible explanation while relying solely on unimodal information, the corresponding question was removed. After this filtering stage, approximately 1,500 questions were retained. Subsequently, we employed large language model, DeepSeek-V3.1 (Liu et al., 2024a), with strong reasoning capabilities to filter out questions that could be answered solely based on textual information. Such cases primarily fall into two categories: first, questions that involve classical, well-known, or universally shared knowledge or objects, which can be answered without reference to the video content; and second, questions where the phrasing of the question, options, or answers provides unintended textual cues. For the former, we directly discarded the questions. For the latter, our annotators reviewed the reasoning process generated by the model and revised the textual formulations to eliminate such biases. After this stage of refinement, 1103 questions were retained. Another group of annotators conducts the final refinement stage, thoroughly reviewing all questions to identify and remove those with incorrect, non-unique, or mismatched answers. After this validation, annotators enriched each question with step-by-step reasoning chains, where each step consists of three elements: modality, evidence, and inference. The modality specifies whether the step relies on audio or visual information; the evidence denotes the specific information extracted from the video; and the inference describes the reasoning derived from that information. We required each step to be atomic, meaning that it should involve only one modality and capture minimal unit of evidence, such as spoken sentence, an action, or the appearance of character. This design ensures that the reasoning process is both detailed and comprehensive. Through this process, we obtained 1000 high-quality QA pairs with explicit step-by-step reasoning chains, forming robust dataset for multimodal audio-visual reasoning. 4 Figure 3: (a) OmniVideoBench covers 8 major categories and 68 subcategories. (b) OmniVideoBench comprises 13 task types. The above part shows the video duration distribution across different tasks, while the durations are categorized into four groups: Short for less than 1 minute, Medium for 15 minutes, Long for 510 minutes, and Ultralong for more than 10 minutes. The lower part illustrates the distribution of three types of audio (i.e., Speech, Sound and Music). (c) Distribution of video durations across four time intervals. (d) Distribution of three audio types. Benchmark Modality Qwen2.5-Omni Music-AVQA (Li et al., 2022) AV-Odyssey (Gong et al., 2024) OmniBench (Li et al., 2024b) Daily-Omni (Zhou et al., 2025) WorldSense (Hong et al., 2025) OmniVideoBench (Ours) V+A I+A I+A V+A V+A V+A / / 56.1 47.5 48. 29.3 Multiple Domains Video Type Audio Type Video Duration Answer Type R+S R Mu Sp+So+Mu Sp+So+Mu Sp+So+Mu Sp+So+Mu 60 / / 3060 15656 Sp+So+Mu 41955 CLS MC MC MC MC MC Table 2: Comparisons between different benchmarks. V, I, for modality represent video, image and audio. Qwen2.5-Omni represents the performance of Qwen2.5-Omni-7B on these benchmarks. Multiple Domains signifies whether the video includes diverse domains. and in Video Type denote real-world and synthetic data. Sp, So, and Mu represent Speech, Sound, and Music for Audio Type, respectively. Video Duration represents the duration in seconds. MC, CLS for Answer Type indicate Multiple Choice and Classification from fixed vocabulary, respectively."
        },
        {
            "title": "2.5 Dataset Statistics",
            "content": "As shown in Table 1, our OmniVideoBench dataset consists of 628 real-world videos with audio tracks, spanning 8 major categories and 68 subcategories. The videos are of high quality and diverse in content, with an average duration of 384.6 seconds, an average resolution of 480p, about 2k ASRtranscribed tokens per video, and roughly three speakers per video. On the annotation side, OmniVideoBench contains 1000 audiovisual reasoning QA pairs across 13 task types, with an average question length of 14.68 words and an average answer length of 4.92 words. Each QA pair is annotated with step-by-step reasoning chains averaging 5.68 steps. The reasoning process covers both modalities, with 54% of steps grounded in vision and 46% in audio. There are 762, 147, 91 QA pairs related to Speech, Sound and Music, respectively, highlighting the complementarity of modalities in multi-step reasoning. Moreover, we provide more detailed statistics in Figure 3. 5 Figure 4: Performance comparison of selected models on OmniVideoBench and Daily-Omni. Red line denotes random guessing. Models Audio Type Video Duration Music Sound Speech (0,1] min (1,5] min (5,10] min (10,30] min Omni-Modal Language Models (With Visual and Audio) Gemini-2.5-Pro Gemini-2.5-Flash Gemini-2.0-Flash Qwen3-Omni-30B-A3B Baichuan-Omni-1.5 HumanOmni-7B MiniCPM-o Qwen2.5-Omni-7B VideoLLaMA2-7B 38.46 39.56 29. 37.36 24.18 20.87 27.47 23.07 26.37 57.72 57.04 40.27 34.67 31.33 31.08 28.57 25.33 30.67 61.66 53.17 43.21 39.26 31.36 31.61 30.24 30.70 29.25 57.83 55.42 49. 45.78 28.92 36.57 31.43 41.57 32.00 64.43 55.10 43.15 37.03 31.78 29.36 28.49 27.41 28.20 Omni-Modal Language Models (Visual Only) Gemini-2.0-Flash Qwen2.5-Omni-7B 25.27 27.47 36.67 26.67 30.99 26. 33.73 28.31 35.86 27.11 Visual Language Models (Visual Only) Qwen2.5-VL-32B Qwen2.5-VL-7B Qwen2.5-VL-72B 32.97 29.67 26.37 32.00 31.33 29.33 31.49 29.51 29.91 38.55 25.90 33.13 31.20 30.03 30.03 Baseline LLMs 55.02 47.37 41.05 38.86 28.38 29.60 34.53 25.33 29.60 32.75 24.45 29.26 31.88 31.88 55.94 52.11 34. 35.11 32.44 29.25 26.15 26.72 28.29 22.48 25.95 30.53 30.15 24.43 Avg. 58.90 52.40 41. 38.40 30.70 30.50 29.70 29.30 29.20 31.30 26.40 31.80 29.80 29.50 DeepSeek-V3.1 28. 26.17 27.28 30.91 27.57 25.00 26. 27.60 Table 3: Results of different models. The table reports accuracy on videos across three audio types and four duration ranges. Boldface highlights the best performance within each column."
        },
        {
            "title": "2.6 Dataset Comparison",
            "content": "In Table 2, we compare OmniVideoBench with existing benchmarks. While AV-Odyssey and OmniBench handle only single images, OmniVideoBench targets more challenging videos, ranging from few seconds to 30 minutes, far exceeding the scope of other benchmarks. Besides, in Figure 4, OmniVideoBench imposes stringent demands on models cross-modal reasoning capabilities. Even the Qwen2.5-VL series models, which achieve 40.68% accuracy in the Daily-Omni (Zhou et al., 2025), perform near random levels on OmniVideoBench. This demonstrates that relying solely on pure visual information is insufficient. Moreover, OmniVideoBench presents significantly greater challenge than existing omnimodal benchmarks. The widely used Qwen2.5-Omni-7B performs close to random guessing, highlighting the challenges of OmniVideoBench."
        },
        {
            "title": "3.1 Baseline Models",
            "content": "We evaluate open-source MLLMs (i.e., Qwen3-Omni series (Xu et al., 2025b), Qwen2.5-Omni series (Xu et al., 2025c), Baichuan-Omni-1.5 (Li et al., 2025), HumanOmni (Zhao et al., 2025), MiniCPM-o (Yao et al., 2024), VideoLLaMA2 (Cheng et al., 2024)), and various closed-source MLLMs (i.e., Gemini-2.5-Pro, Gemini-2.5-Flash (Comanici et al., 2025), and Gemini-2.0-Flash). We also evaluate the Qwen2.5-VL series (Bai et al., 2025) and DeepSeek-V3.1 (Liu et al., 2024a)."
        },
        {
            "title": "3.2 Main Results",
            "content": "In Table 3, we present evaluation results on OmniVideoBench and have the following observations: Open-source models still lag significantly behind closed-source models. Gemini-2.5-Pro achieves the best performance across most tasks. This underscores the urgent need for current open-source models to improve in multiple areas, including fine-grained perception, cross-modal reasoning, and speech awareness. MLLMs show performance degradation when dealing with music-related audio. We observe that models exhibit lower accuracy in responding to music-dominated videos compared to those 6 Figure 5: Performance Comparison of some Open-Source and Closed-Source Omni Models on 13 Tasks in OmniVideoBench. Here, Attr: Attribute Comparison, Bac&Mu: Background and Music Understanding, Caus: Cause and Effect Reasoning, Coun: Counting, Ego: Ego Reasoning, Fine: Fine-grained Perception, Hypo: Hypothetical Reasoning, Ref: Referential Reasoning, Rela: Relationship Reasoning, Senti: Sentiment Analysis, Spati: Spatial Reasoning, Summ: Summarization, Tempo: Temporal Sequencing Understanding. containing human voices or ambient sounds, phenomenon particularly pronounced in opensource models. Unlike human voices conveying explicit semantic content or ambient sounds often corresponding to specific visual events, music primarily encodes abstract emotional and atmospheric information. Current MLLMs demonstrate limited capability to translate such implicit cues into effective reasoning, indicating that cross-modal alignment for emotional and atmospheric understanding remains an urgent challenge to be addressed. Current MLLMs still have room for improvement in long videos. Although some leading models like Gemini-2.5-Pro demonstrate relatively robust performance on long videos, most MLLMs (e.g., Gemini-2.0-Flash, Qwen3-Omni) still struggle in long videos, which highlights the widespread challenge in understanding long videos."
        },
        {
            "title": "3.3 Further Analysis",
            "content": "Performance of Models on Tasks across Different Types. Figure 5 presents fine-grained comparison of model accuracy on the 13 reasoning categories in OmniVideoBench. Several consistent patterns emerge. (1). Closed-source MLLMs demonstrate superior performance across nearly all task types. Gemini-2.5-Pro achieves the highest accuracy on 11 out of 13 tasks, demonstrating particularly strong performance in Relationship Reasoning, Spatial Reasoning, Referential Reasoning, and Cause and Effect Reasoning. These tasks require long-term sequence integration and multi-step cross-modal reasoning, highlighting Geminis strengths in long-context modeling and multimodal fusion. (2). MLLMs understanding of audio remains limited to relatively superficial surface-level information. Whether open-source or closed-source models, Background and Music Understanding remains the most challenging task, with even Gemini-2.5-Pro achieving accuracy below 50%. This is probably because such tasks require linking low-semantic acoustic cues (e.g., musical style, tempo changes) with high-level reasoning, while current models struggle to master the capability. In contrast, Relationship Reasoning and Summarization are relatively easier. This may be because they rely more on recognizing language within audio and visual observation capabilities, and less on cross-modal abstraction abilities. Effect of ASR Transcripts for Visual Only MLLMs. To further investigate the role of audio information in MLLMs reasoning performance, we evaluate several models using both the automatic speech recogni7 (a) Accuracy rates of selected MLLMs under different inputs. (b) Accuracy of Gemini-2.0-Flash on videos with different audio types. Figure 6: Accuracy comparison of MLLMs with and without ASR transcripts on OmniVideoBench. (a) Performance of Qwen2.5-Omni-7B and Qwen3-Omni-30BA3B at different numbers of frames. (b) Accuracy of Qwen3-Omni-30B-A3B on questions with videos of varying durations across different numbers of frames. Figure 7: Performance of selected models when inputting videos with different numbers of frames. tion (ASR) transcripts generated by the Voxtral-Mini-3B model (Liu et al., 2025a) and silent video frames as inputs. The results are shown in Figure 6. The observations are as follows: (1). Open-source models demonstrate weaker integration capabilities for audio information compared to their understanding of textual information. In Figure 6a, all tested models demonstrate significantly improved accuracy after extracting ASR text information compared to receiving only visual inputs. However, the Qwen2.5-Omni7B model, which processes both visual and audio inputs simultaneously, performed even worse than the Qwen2.5-VL-7B model with equivalent parameters. This highlights common challenge faced by most open-source Omni-Modal Language Models: insufficient cross-modal reasoning capabilities for audio-visual information. (2). In cross-modal video reasoning, audio comprehension capabilities remain irreplaceable by ASR. In Figure 6b, although ASR can help MLLMs achieve decent performance on certain tasks requiring speech recognition capabilities, its effectiveness is extremely limited for tasks demanding deeper and more abstract audio comprehension such as the videos whose audio type is Music or Sound. Models Open-ended QA MCQ Effect of Different Numbers of Frames. We conduct experiments on Qwen2.5-Omni7B and Qwen3-Omni-30B-A3B with total frame counts fixed at 32, 64, 128, and 256, respectively, and observe that both models benefit from more frequent time sampling. In Figure 7a, as the total frame counts increase, accuracy steadily improves, likely because richer temporal coverage provides more complete motion cues and reduces the risk of missing key events. As shown in Figure 7b, this improvement becomes more pronounced for longer videos. The consistent gains across different video durations further indicate that dense frame sampling not only captures fine-grained visual dynamics but also strengthens cross-modal alignment. This highlights the importance of dense temporal Table 4: Comparison of performance on Open-ended Question Answering (QA) and MultipleChoice Questions (MCQ) across various models. Gemini-2.0-Flash Qwen2.5-Omni-7B 27.06 17.25 41.50 29.30 information and long-context processing for achieving robust audiovisual reasoning. Open-ended QA vs. MCQ. To investigate whether the multiple-choice question (MCQ) format overstates model performance, we additionally evaluated several representative models on open-ended question-answering (QA) tasks, where no predefined answer options are provided. In this setting, models must directly generate textual responses, eliminating both the possibility of random guessing and any lexical cues potentially present in candidate options. In Table 4, the accuracy of all models drops significantly compared to their performance on multiple-choice questions. For instance, the Gemini-2.5-Pro, which leads in MCQ benchmarks, experiences relative accuracy decline exceeding 14 percent in open-ended scenarios, while open-source models exhibit even steeper drops."
        },
        {
            "title": "4 Related Works",
            "content": "Omni-Understanding MLLMs. The development of MLLMs (Chen et al., 2022; Awadalla et al., 2023; Liu et al., 2023) began with foundational focus on integrating the two primary modalities of vision and language. recent paradigm shift aims to develop Omni-modal MLLMs capable of processing and generating information across an arbitrary combination of modalities (Any-to-Any\"). This approach positions the LLM as central cognitive engine, unifying diverse data types like audio, video, and text within its semantic space (Liu et al., 2024b; Yuan et al., 2025). This has driven move from integrating pre-trained unimodal components towards developing natively multimodal\" architectures trained from the ground up, as exemplified by models like GPT-4o (Hurst et al., 2024). This ambition is showcased by state-of-the-art models (Xu et al., 2025c; Zhao et al., 2025; Li et al., 2024c; 2025; Yao et al., 2024; Sun et al., 2025; Liu et al., 2025b), which pioneer end-to-end streaming capabilities for simultaneously processing video and audio to generate text and speech. At the forefront of this paradigm, proprietary models like Gemini series (Team, 2024; Comanici et al., 2025) demonstrate pinnacle performance, powered by natively multimodal design and massive context window that together unlock superior understanding of complex, interwoven data streams. MLLM Benchmarks. The landscape of MLLM evaluation has matured significantly, evolving from foundational perception benchmarks (Liu et al., 2024c; Li et al., 2024a; Yu et al., 2024a;b; Chen et al., 2024b; Jiang et al., 2025) to more sophisticated frameworks (He et al., 2025; Du et al., 2025). Recent efforts probe deeper cognitive abilities, with MLLM-Bench (Ge et al., 2025) assessing hierarchy of cognitive skills. MMMU (Yue et al., 2023) and MMMU-Pro (Yue et al., 2024b) challenging models with expert-level, multi-disciplinary reasoning under stricter protocols like vision-only inputs. Simultaneously, evaluation has specialized into high-stakes domains such as finance (Gan et al., 2024) and medicine (Chen et al., 2024c). For video, some benchmarks (Wang et al., 2019; Li et al., 2021; 2023; Fang et al., 2024; Wu et al., 2024) now focus on the critical challenge of long-context temporal understanding (Liu et al., 2025c), revealing key limitations in current models."
        },
        {
            "title": "5 Conclusion",
            "content": "We presented OmniVideoBench, large-scale benchmark for evaluating audio-visual collaborative reasoning in MLLMs, with diverse videos, carefully verified QA pairs, and explicit reasoning annotations. Experiments show that both openand closed-source models still struggle with modality complementarity, long-form temporal reasoning, and music understanding, underscoring large gap from human-level performance. We hope this benchmark will drive future research toward more robust and generalizable multimodal reasoning systems."
        },
        {
            "title": "6 Contributions",
            "content": "Our team members contribute to the development of OmniVideoBench from the following perspectives: Data Annotation Management Data Annotation Data Quality Inspection Model Evaluation Result Analysis Paper Writing Co-First Authors Caorui Li, Southeast University, caoruili@seu.edu.cn Yu Chen, Southeast University, yu_chen@seu.edu.cn Yiyan Ji, Nanjing University, jiyiiiyyy@gmail.com"
        },
        {
            "title": "Core Contributors",
            "content": "Jin Xu, Alibaba Group Zhenyu Cui, Southeast University Shihao Li, Nanjing University Yuanxing Zhang, Kuaishou Technology Jiafu Tang, Nanjing University Zhenghao Song, M-A-P Dingling Zhang, Nanjing University Ying He, University of Science and Technology Beijing Haoxiang Liu, University of Science and Technology Beijing Yuxuan Wang, Alibaba Group Qiufeng Wang, Southeast University"
        },
        {
            "title": "Contributors",
            "content": "Zhenhe Wu, M-A-P Jiehui Luo, Central Conservatory of Music Zhiyu Pan, Nanjing University Weihao Xie, Huazhong University of Science and Technology Chenchen Zhang, M-A-P Zhaohui Wang, Nanjing University Jiayi Tian, Alibaba Group Yanghai Wang, Nanjing University Zhe Cao, Nanjing University Minxin Dai, Nanjing University Ke Wang, BUPT Runzhe Wen, Nanjing University Yinghao Ma, Queen Mary University of London Yaning Pan, Fudan University Sungkyun Chang, Queen Mary University of London Termeh Taheri, Queen Mary University of London Haiwen Xia, Peking University Christos Plachouras, Queen Mary University of London Emmanouil Benetos, Queen Mary University of London Yizhi Li, University of Manchester Ge Zhang, M-A-P 10 Jian Yang, M-A-P Tianhao Peng, M-A-P Zili Wang, M-A-P Minghao Liu, 2077AI Junran Peng, University of Science and Technology Beijing Zhaoxiang Zhang, Chinese Academy of Sciences"
        },
        {
            "title": "Corresponding Author",
            "content": "Jiaheng Liu, Nanjing University, liujiaheng@nju.edu.cn 11 References Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing Sun, Tong Xu, and Enhong Chen. survey on multimodal large language models. National Science Review, 11(12):nwae403, November 2024. ISSN 2095-5138. doi: 10.1093/nsr/nwae403. URL https://doi.org/10.1093/nsr/nwae403. _eprint: https://academic.oup.com/nsr/article-pdf/11/12/nwae403/61201557/nwae403.pdf. Shezheng Song, Xiaopeng Li, Shasha Li, Shan Zhao, Jie Yu, Jun Ma, Xiaoguang Mao, Weimin Zhang, and Meng Wang. How to bridge the gap between modalities: Survey on multimodal large language model. IEEE Transactions on Knowledge and Data Engineering, 37(9):53115329, 2025. doi: 10.1109/TKDE.2025. 3527978. Junhao Cheng, Yuying Ge, Teng Wang, Yixiao Ge, Jing Liao, and Ying Shan. Video-holmes: Can mllm think like holmes for complex video reasoning? ArXiv, abs/2505.21374, 2025. Weiye Xu, Jiahao Wang, Weiyun Wang, Zhe Chen, Wengang Zhou, Aijun Yang, Lewei Lu, Houqiang Li, Xiaohua Wang, Xizhou Zhu, et al. Visulogic: benchmark for evaluating visual reasoning in multi-modal large language models. arXiv preprint arXiv:2504.15279, 2025a. Qiguang Chen, Libo Qin, Jin Zhang, Zhi Chen, Xiao Xu, and Wanxiang Che. 3 cot: novel benchmark for multi-domain multi-step multi-modal chain-of-thought. arXiv preprint arXiv:2405.16473, 2024a. Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 95569567, 2024a. Bohao Li, Yuying Ge, Yixiao Ge, Guangzhi Wang, Rui Wang, Ruimao Zhang, and Ying Shan. Seedbench: Benchmarking multimodal large language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1329913308, 2024a. Jack Hong, Shilin Yan, Jiayin Cai, Xiaolong Jiang, Yao Hu, and Weidi Xie. Worldsense: Evaluating real-world omnimodal understanding for multimodal llms. ArXiv, abs/2502.04326, 2025. Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024a. Guangyao Li, Yake Wei, Yapeng Tian, Chenliang Xu, Ji rong Wen, and Di Hu. Learning to answer questions in dynamic audio-visual scenarios. 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1908619096, 2022. Kaixiong Gong, Kaituo Feng, Bohao Li, Yibing Wang, Mofan Cheng, Shijia Yang, Jiaming Han, Benyou Wang, Yutong Bai, Zhuoran Yang, and Xiangyu Yue. Av-odyssey bench: Can your multimodal llms really understand audio-visual information? ArXiv, abs/2412.02611, 2024. Yizhi Li, Ge Zhang, Yi Ma, Ruibin Yuan, Kang Zhu, Hangyu Guo, Yiming Liang, Jiaheng Liu, Jian Yang, Siwei Wu, Xingwei Qu, Jinjie Shi, Xinyue Zhang, Zhen Yang, Xiangzhou Wang, Zhaoxiang Zhang, Zachary Liu, Emmanouil Benetos, Wenhao Huang, and Chenghua Lin. Omnibench: Towards the future of universal omni-language models. ArXiv, abs/2409.15272, 2024b. Ziwei Zhou, Rui Wang, and Zuxuan Wu. Daily-omni: Towards audio-visual reasoning with temporal alignment across modalities. ArXiv, abs/2505.17862, 2025. Jin Xu, Zhifang Guo, Hangrui Hu, Yunfei Chu, Xiong Wang, Jinzheng He, Yuxuan Wang, Xian Shi, Ting He, Xinfa Zhu, Yuanjun Lv, Yongqi Wang, Dake Guo, He Wang, Linhan Ma, Pei Zhang, Xinyu Zhang, Hongkun Hao, Zishan Guo, Baosong Yang, Bin Zhang, Ziyang Ma, Xipin Wei, Shuai Bai, Keqin Chen, Xuejing Liu, Peng Wang, Mingkun Yang, Dayiheng Liu, Xingzhang Ren, Bo Zheng, Rui Men, Fan Zhou, Bowen Yu, Jianxin Yang, Le Yu, Jingren Zhou, and Junyang Lin. Qwen3-omni technical report. arXiv preprint arXiv:2509.17765, 2025b. Jin Xu, Zhifang Guo, Jinzheng He, Hangrui Hu, Ting He, Shuai Bai, Keqin Chen, Jialin Wang, Yang Fan, Kai Dang, et al. Qwen2. 5-omni technical report. arXiv preprint arXiv:2503.20215, 2025c. Yadong Li, Jun Liu, Tao Zhang, Song Chen, Tianpeng Li, Zehuan Li, Lijun Liu, Lingfeng Ming, Guosheng Dong, Dawei Pan, Chong Li, Yuanbo Fang, Dong-Ling Kuang, Mingrui Wang, Chenglin Zhu, Youwei Zhang, Hongyu Guo, Fengyu Zhang, Yuran Wang, Bowen Ding, Wei Song, Xu Li, Yuqiu Huo, Zheng Liang, Shusen Zhang, Xin Wu, Shuai Zhao, Lin-Xiao Xiong, Yozhen Wu, Jia-Reng Ye, Wenhao Lu, 12 Bowen Li, Yan Zhang, Yaqi Zhou, Xin Chen, Lei Su, Hongda Zhang, Fuzhong Chen, Xu Dong, Na Nie, Zhiying Wu, Bin Xiao, Ting Li, Shunya Dang, Ping Zhang, Yijia Sun, Jincheng Wu, Jinjie Yang, Xionghai Lin, Zhi-Xing Ma, Ke-Ye Wu, Jia Li, Ai-Min Yang, Hui Liu, Jianqiang Zhang, Xiaoxi Chen, Guangwei Ai, Wentao Zhang, Yicong Chen, Xiaoqin Huang, Kun Li, Wenjing Luo, Yi qun Duan, Lingling Zhu, Ran Xiao, Zhengquan Su, Jiani Pu, Dian Wang, Xu Jia, Tianyu Zhang, Mengyu Ai, Mang Wang, Yu Qiao, Lei Zhang, Yanjun Shen, Fan Yang, Miao Zhen, Yijie Zhou, Mingyang Chen, Fei Li, Chenzheng Zhu, Keer Lu, Yaqi Zhao, Hao Liang, Youquan Li, Yanzhao Qin, Lin-Lin Sun, Jianhua Xu, Haoze Sun, Mingan Lin, Zenan Zhou, and Weipeng Chen. Baichuan-omni-1.5 technical report. ArXiv, abs/2501.15368, 2025. Jiaxin Zhao, Qize Yang, Yi-Xing Peng, Detao Bai, Shimin Yao, Boyuan Sun, Xiang Chen, Shenghao Fu, Weixuan chen, Xihan Wei, and Liefeng Bo. Humanomni: large vision-speech language model for human-centric video understanding. ArXiv, abs/2501.15111, 2025. Yuan Yao, Tianyu Yu, Ao Zhang, Chongyi Wang, Junbo Cui, Hongji Zhu, Tianchi Cai, Haoyu Li, Weilin Zhao, Zhihui He, Qi-An Chen, Huarong Zhou, Zhensheng Zou, Haoye Zhang, Shengding Hu, Zhi Zheng, Jie Zhou, Jie Cai, Xu Han, Guoyang Zeng, Dahai Li, Zhiyuan Liu, and Maosong Sun. Minicpm-v: gpt-4v level mllm on your phone. ArXiv, abs/2408.01800, 2024. Zesen Cheng, Sicong Leng, Hang Zhang, Yifei Xin, Xin Li, Guanzheng Chen, Yongxin Zhu, Wenqi Zhang, Ziyang Luo, Deli Zhao, et al. Videollama 2: Advancing spatial-temporal modeling and audio understanding in video-llms. arXiv preprint arXiv:2406.07476, 2024. Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. Alexander Liu, Andy Ehrenberg, Andy Lo, ClÃ©ment Denoix, Corentin Barreau, Guillaume Lample, Jean-Malo Delignon, Khyathi Raghavi Chandu, Patrick von Platen, Pavankumar Reddy Muddireddy, et al. Voxtral. arXiv preprint arXiv:2507.13264, 2025a. Feilong Chen, Duzhen Zhang, Minglun Han, Xiuyi Chen, Jing Shi, Shuang Xu, and Bo Xu. Vlp: survey on vision-language pre-training. Machine Intelligence Research, 20:3856, 2022. Anas Awadalla, Irena Gao, Josh Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani S. Marathe, Yonatan Bitton, Samir Yitzhak Gadre, Shiori Sagawa, Jenia Jitsev, Simon Kornblith, Pang Wei Koh, Gabriel Ilharco, Mitchell Wortsman, and Ludwig Schmidt. Openflamingo: An open-source framework for training large autoregressive vision-language models. ArXiv, abs/2308.01390, 2023. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. ArXiv, abs/2304.08485, 2023. URL https://api.semanticscholar.org/CorpusID:258179774. Jiaheng Liu, Zehao Ni, Haoran Que, Tao Sun, Zekun Wang, Jian Yang, Jiakai Wang, Hongcheng Guo, Zhongyuan Peng, Ge Zhang, et al. Roleagent: Building, interacting, and benchmarking high-quality role-playing agents from scripts. Advances in Neural Information Processing Systems, 37:4940349428, 2024b. Ruibin Yuan, Hanfeng Lin, Shuyue Guo, Ge Zhang, Jiahao Pan, Yongyi Zang, Haohe Liu, Yiming Liang, Wenye Ma, Xingjian Du, et al. Yue: Scaling open foundation models for long-form music generation. arXiv preprint arXiv:2503.08638, 2025. Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. Yadong Li, Haoze Sun, Mingan Lin, Tianpeng Li, Guosheng Dong, Tao Zhang, Bowen Ding, Wei Song, Zhenglin Cheng, Yuqi Huo, Song Chen, Xu Li, Dawei Pan, Shusen Zhang, Xin Wu, Zheng Liang, Jun Liu, Keer Lu, Yaqi Zhao, Yan-Bin Shen, Fan Yang, Kaicheng yu, Tao Lin, Jianhua Xu, Zenan Zhou, and Weipeng Chen. Baichuan-omni technical report. ArXiv, abs/2410.08565, 2024c. Wei Sun, Linhan Cao, Yu Shan Cao, Weixia Zhang, Wen Wen, Kaiwei Zhang, Zijian Chen, Fangfang Lu, Xiongkuo Min, and Guangtao Zhai. Engagement prediction of short videos with large multimodal models. ArXiv, abs/2508.02516, 2025. Zuyan Liu, Yuhao Dong, Jiahui Wang, Ziwei Liu, Winston Hu, Jiwen Lu, and Yongming Rao. Ola: Pushing the frontiers of omni-modal language model with progressive modality alignment. ArXiv, abs/2502.04328, 2025b. Gemini Team. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. ArXiv, abs/2403.05530, 2024. Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, Kai Chen, and Dahua Lin. Mmbench: Is your multi-modal model an all-around player? In Computer Vision ECCV 2024: 18th European Conference, Milan, Italy, September 29October 4, 2024, Proceedings, Part VI, page 216233, Berlin, Heidelberg, 2024c. Springer-Verlag. ISBN 978-3-031-72657-6. doi: 10.1007/978-3-031-72658-3_13. Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang. Mm-vet: evaluating large multimodal models for integrated capabilities. In Proceedings of the 41st International Conference on Machine Learning, ICML24. JMLR.org, 2024a. Weihao Yu, Zhengyuan Yang, Linfeng Ren, Linjie Li, Jianfeng Wang, Kevin Qinghong Lin, Chung-Ching Lin, Zicheng Liu, Lijuan Wang, and Xinchao Wang. Mm-vet v2: challenging benchmark to evaluate large multimodal models for integrated capabilities. ArXiv, abs/2408.00765, 2024b. Liang Chen, Yichi Zhang, Shuhuai Ren, Haozhe Zhao, Zefan Cai, Yuchi Wang, Peiyi Wang, Xiangdi Meng, Tianyu Liu, and Baobao Chang. Pca-bench: Evaluating multimodal large language models in perception-cognition-action chain. ArXiv, abs/2402.15527, 2024b. Dongzhi Jiang, Renrui Zhang, Ziyu Guo, Yanwei Li, Yu Qi, Xinyan Chen, Liuhui Wang, Jianhan Jin, Claire Guo, Shen Yan, Bo Zhang, Chaoyou Fu, Peng Gao, and Hongsheng Li. Mme-cot: Benchmarking chain-of-thought in large multimodal models for reasoning quality, robustness, and efficiency. ArXiv, abs/2502.09621, 2025. Yancheng He, Shilong Li, Jiaheng Liu, Weixun Wang, Xingyuan Bu, Ge Zhang, Zhongyuan Peng, Zhaoxiang Zhang, Zhicheng Zheng, Wenbo Su, et al. Can large language models detect errors in long chain-of-thought reasoning? arXiv preprint arXiv:2502.19361, 2025. Xinrun Du, Yifan Yao, Kaijing Ma, Bingli Wang, Tianyu Zheng, King Zhu, Minghao Liu, Yiming Liang, Xiaolong Jin, Zhenlin Wei, et al. Supergpqa: Scaling llm evaluation across 285 graduate disciplines. arXiv preprint arXiv:2502.14739, 2025. Wentao Ge, Shunian Chen, Hardy Chen, Nuo Chen, Junying Chen, Zhihong Chen, Wenya Xie, Shuo Yan, Chenghao Zhu, Ziyue Lin, Dingjie Song, Xidong Wang, Anningzhe Gao, Zhang Zhiyi, Jianquan Li, Xiang Wan, and Benyou Wang. MLLM-bench: Evaluating multimodal LLMs with per-sample criteria. In Luis Chiruzzo, Alan Ritter, and Lu Wang, editors, Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 49514974, Albuquerque, New Mexico, April 2025. Association for Computational Linguistics. ISBN 979-8-89176-189-6. doi: 10.18653/v1/2025.naacl-long.256. URL https://aclanthology.org/2025.naacl-long.256/. Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, Cong Wei, Botao Yu, Ruibin Yuan, Renliang Sun, Ming Yin, Boyuan Zheng, Zhenzhu Yang, Yibo Liu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 95569567, 2023. Xiang Yue, Tianyu Zheng, Yuansheng Ni, Yubo Wang, Kai Zhang, Shengbang Tong, Yuxuan Sun, Ming Yin, Botao Yu, Ge Zhang, Huan Sun, Yu Su, Wenhu Chen, and Graham Neubig. Mmmu-pro: more robust multi-discipline multimodal understanding benchmark. In Annual Meeting of the Association for Computational Linguistics, 2024b. Ziliang Gan, Yu Lu, Dong Zhang, Haohan Li, Che Liu, Jian Liu, Ji Liu, Haipang Wu, Chaoyou Fu, Zenglin Xu, Rongjunchen Zhang, and Yong Dai. Mme-finance: multimodal finance benchmark for expert-level understanding and reasoning. ArXiv, abs/2411.03314, 2024. Pengcheng Chen, Jin Ye, Guoan Wang, Yanjun Li, Zhongying Deng, Wei Li, Tian-Xin Li, Haodong Duan, Ziyan Huang, Yan-Cheng Su, Benyou Wang, Shaoting Zhang, Bin Fu, Jianfei Cai, Bohan Zhuang, Eric J. Seibel, Junjun He, and Yu Qiao. Gmai-mmbench: comprehensive multimodal evaluation benchmark towards general medical ai. ArXiv, abs/2408.03361, 2024c. 14 Xin Eric Wang, Jiawei Wu, Junkun Chen, Lei Li, Yuan fang Wang, and William Yang Wang. Vatex: large-scale, high-quality multilingual dataset for video-and-language research. 2019 IEEE/CVF International Conference on Computer Vision (ICCV), pages 45804590, 2019. Linjie Li, Jie Lei, Zhe Gan, Licheng Yu, Yen-Chun Chen, Rohith Krishnan Pillai, Yu Cheng, Luowei Zhou, Xin Eric Wang, William Yang Wang, Tamara L. Berg, Mohit Bansal, Jingjing Liu, Lijuan Wang, and Zicheng Liu. Value: multi-task benchmark for video-and-language understanding evaluation. ArXiv, abs/2106.04632, 2021. Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, Limin Wang, and Yu Qiao. Mvbench: comprehensive multi-modal video understanding benchmark. 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 2219522206, 2023. Xinyu Fang, Kangrui Mao, Haodong Duan, Xiangyu Zhao, Yining Li, Dahua Lin, and Kai Chen. Mmbenchvideo: long-form multi-shot benchmark for holistic video understanding. ArXiv, abs/2406.14515, 2024. Haoning Wu, Dongxu Li, Bei Chen, and Junnan Li. Longvideobench: benchmark for long-context interleaved video-language understanding. ArXiv, abs/2407.15754, 2024. Jiaheng Liu, Dawei Zhu, Zhiqi Bai, Yancheng He, Huanxuan Liao, Haoran Que, Zekun Wang, Chenchen Zhang, Ge Zhang, Jiebin Zhang, et al. comprehensive survey on long context language modeling. arXiv preprint arXiv:2503.17407, 2025c."
        },
        {
            "title": "A Full Video Category Taxonomy",
            "content": "Table 5 shows that videos in OmniVideoBench span 8 major categories and 68 subcategories. Table 5: Full taxonomy of the video dataset. Main Category Subcategories Vlog News Cartoon Sports Documentary TV Others Ego Cooking & Cuisine; Travel & Outdoor; Art; Animals; Daily Life at Home; DIY & Handcraft; Gardening; Fitness; Sports; Interviews; Party Games; Makeup & Beauty; Fashion & Styling; Hiking & Trekking Politics; Economy; Society; Technology; Education; Healthcare; Military; Law & Justice; Sports; Culture; Entertainment; Weather; Disaster; Transportation 2D Animation; 3D Animation Basketball; Football (Soccer); Volleyball; Badminton; Table Tennis; Swimming; Figure Skating; Skiing; Gymnastics; Wrestling & Judo; Track & Field; Esports; Others Nature & Wildlife; History & Archaeology; Society & Humanity; Politics & Military; Science & Engineering; Medicine & Health; Crime & Law; Art & Culture; Education & Growth; Economy; Environment & Climate; Food & Culinary Culture; Religion & Belief Short; Dramas & Web Series; Variety; Stage Plays; Dance; Mime; Movies Live; Advertisement; Course Replay; Short Video First-person: People; First-person:Pets"
        },
        {
            "title": "B Detailed Principles of Video Collection",
            "content": "To ensure an objective and reliable evaluation of MLLMs, the videos included in the benchmark must satisfy multiple requirements, ensuring diversity in both type and duration. The content should provide rich information across audio and visual modalities, while maintaining complementarity between the two. In other words, the benchmark avoids cases where the visual content can be fully inferred from the audio alone, or where the audio is redundant given the visual stream. Furthermore, since many existing video training datasets overlap with the sources of our benchmarkfor example, clips from Friendsevaluation may otherwise reduce to simple answer memorization. To mitigate this unfairness, we additionally consider the publication year of videos when constructing the dataset. The detailed principles for video collection are as follows: Video publication date. Given that most existing training datasets are constructed from YouTube videos, similar to ours, or contain overlapping content such as identical TV shows, we restrict our selection to videos published after June 2024. We use the most recent videos possible to mitigate unfairness and potential overestimation issues arising from the model having already been exposed to similar content during training. Rich dynamic visual information. The distinguishing feature of videos compared to images lies in their rich dynamic visual information. prerequisite for evaluating models ability to understand visual information in videos is that the videos themselves contain sufficient dynamic content to be captured and analyzed. Consequently, videos lacking diverse dynamic visual information are excluded, such as those consisting of only several static scenes or perspectives throughout, or those that remain largely static with minimal motion confined to small corner of the frame. Effective audio information. In some videos, the audio is completely unrelated to the visual content, such as when only an independent background track is added. We consider such audio to be invalid. To fairly evaluate the models capability in audio-visual collaborative reasoning, the audiowhether speech, environmental sound, or musicmust align with the visual content. Absence of subtitle. We excluded videos with embedded subtitles, as such practices convey most of the audio information visually, enabling models to cheat through vision alone. Likewise, videos containing large text overlays were regarded as undesirable, since these overlays often 16 directly reveal information about characters speech, mental states, or ongoing events, thereby undermining the assessment of the models genuine understanding and reasoning abilities. Video resolution. To ensure video quality, we require minimum resolution of 480p, and the visual content must be free from issues such as distortion or blurriness that would hinder comprehension."
        },
        {
            "title": "C Prompts Used in This Work",
            "content": "C.1 Prompt for Overall Evaluation # Instruction: You are given video. Based on the content of the video, answer the following question: # Question: {Question} # Options: A: {Option A} B: {Option B} C: {Option C} D: {Option D} # Task: Answer with the options letter directly(e.g., A, B, C, or D). If your access to the video content is limited, at least one option that is more likely than the others must be chosen. Mustnt give any other reason for can not choose! C.2 Prompt to select questions that can be answered without relying on options # Role: You are an impartial judge. # Instruction: Your task is NOT to answer the question, but to determine whether the question is inherently DEPENDENT on the multiple-choice options in order to be answered. # Task: We aim to convert this multiple-choice question into an open-ended question. The video content is NOT provided here, but you should assume you have fully watched the video and know everything about it. Your job is ONLY to decide whether the question itself *requires* the options to be answerable. # Guidelines: - If the question can still be reasonably answered **without needing the options** (even if the exact wording might change slightly), return No. - If the question cannot be answered at all without the options (e.g., it explicitly asks Which of the following. . . ), return Yes. # Question: {Question} # Answer: {Answer} Respond ONLY with Yes or No. 17 C.3 Prompt for multiple-choice questions with step-by-step reasoning # Instruction: You are given video. Based on the content of the video, answer the following question: # Question: {Question} # Options: A: {Option A} B: {Option B} C: {Option C} D: {Option D} # Task: Note that you should first reason step by step, and then you should give your final choice in A, B, C, or D. Your answer format should be as follows: Step X: [Reasoning step X] The final choice is: bbox{{Answer with the options letter directly(A, B, C, or D).}}."
        }
    ],
    "affiliations": [
        "NJU-LINK"
    ]
}