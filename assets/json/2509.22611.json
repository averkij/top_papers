{
    "paper_title": "Quantile Advantage Estimation for Entropy-Safe Reasoning",
    "authors": [
        "Junkang Wu",
        "Kexin Huang",
        "Jiancan Wu",
        "An Zhang",
        "Xiang Wang",
        "Xiangnan He"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Reinforcement Learning with Verifiable Rewards (RLVR) strengthens LLM reasoning, but training often oscillates between {entropy collapse} and {entropy explosion}. We trace both hazards to the mean baseline used in value-free RL (e.g., GRPO and DAPO), which improperly penalizes negative-advantage samples under reward outliers. We propose {Quantile Advantage Estimation} (QAE), replacing the mean with a group-wise K-quantile baseline. QAE induces a response-level, two-regime gate: on hard queries (p <= 1 - K) it reinforces rare successes, while on easy queries (p > 1 - K) it targets remaining failures. Under first-order softmax updates, we prove {two-sided entropy safety}, giving lower and upper bounds on one-step entropy change that curb explosion and prevent collapse. Empirically, this minimal modification stabilizes entropy, sparsifies credit assignment (with tuned K, roughly 80% of responses receive zero advantage), and yields sustained pass@1 gains on Qwen3-8B/14B-Base across AIME 2024/2025 and AMC 2023. These results identify {baseline design} -- rather than token-level heuristics -- as the primary mechanism for scaling RLVR."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 2 ] . [ 1 1 1 6 2 2 . 9 0 5 2 : r Quantile Advantage Estimation for Entropy-Safe Reasoning QUANTILE ADVANTAGE ESTIMATION FOR ENTROPYSAFE REASONING Junkang Wu1 Kexin Huang1 Jiancan Wu1 An Zhang1 Xiang Wang1 Xiangnan He1 1University of Science and Technology of China {jkwu0909, xiangwang1223, xiangnanhe}@gmail.com"
        },
        {
            "title": "ABSTRACT",
            "content": "Reinforcement Learning with Verifiable Rewards (RLVR) strengthens LLM reasoning but training often oscillates between entropy collapse and entropy explosion. We trace both hazards to the mean-baseline used in value-free RL (e.g., GRPO & DAPO), which improperly penalizes negative-advantage samples under reward outliers. We propose Quantile Advantage Estimation (QAE), replacing the mean with group-wise K-quantile baseline. QAE induces response-level, two-regime gate: on hard queries (p 1K) it reinforces rare successes, while on easy queries (p > 1K) it targets remaining failures. Under first-order softmax updates, we prove two-sided entropy safety, giving lower/upper bounds on one-step entropy change that curb explosion and prevent collapse. Empirically, this minimal modification stabilizes entropy, sparsifies credit assignment (with tuned K, roughly 80% of responses receive zero advantage), and yields sustained pass@1 gains on Qwen3-8B/14B-Base across AIME24/25 and AMC23. These results identify baseline designrather than token-level heuristicsas the primary mechanism for scaling RLVR 1."
        },
        {
            "title": "INTRODUCTION",
            "content": "Reinforcement Learning with Verifiable Rewards (RLVR) (Lambert et al., 2024; DeepSeek-AI et al., 2025; Yang et al., 2025a) enhances Large Language Models (LLMs) by rewarding verifiable correctness (Phan et al., 2025; Rein et al., 2023). Yet reward-driven optimization often triggers entropy collapse (Yu et al., 2025; Cui et al., 2025): the policy distribution sharpens prematurely, suppressing exploration and ultimately limiting performance. This exposes fundamental tension between maximizing reward and preserving policy diversity during RLVR fine-tuning. Prior work focuses almost exclusively on preventing collapse, e.g., uplifting low-probability tokens (Yu et al., 2025), penalizing collapse-inducing tokens (Cui et al., 2025), or preserving policy diversity by primarily learning from negative samples (Zhu et al., 2025). While effective at avoiding collapse, these methods address only one side of the problem and largely overlook its symmetric counterpart: entropy explosion. Uncontrolled entropy growth is equally harmful, leading to inefficient exploration and stalled progress. This risk is practical, not merely theoretical. On Qwen3-8B-Base with DAPO, Figure 1 (left) shows that Clip-Higher averts collapse but induces an early entropy spike (steps 10 80) that, while not immediately harming performance, creates long-term instability. After step 100, entropy remains high and volatile, while performance plateaus. These dynamics highlight key shortcomings of unconstrained entropy growth: (i) higher policy entropy does not guarantee continued effective explorationperformance can plateau despite ongoing behavioral variability reflected in high entropy; and (ii) the initial entropy spike indicates period of over-exploration that, though not immediately destructive, ultimately undermines the models ability to consolidate learning from high-reward reasoning trajectories. The dual challenge, therefore, is to avoid both premature convergence (collapse) and unproductive, signal-degrading divergence (explosion). Merely avoiding collapse is therefore insufficienteffective RLVR requires keeping entropy within productive range. 1The code is available at https://github.com/junkangwu/QAE. 1 Quantile Advantage Estimation for Entropy-Safe Reasoning Figure 1: Entropyperformance dynamics on Qwen3-8B-Base. Left: DAPO with Clip-Higher prevents early collapse but triggers an early entropy spike (steps 1080) and later performance plateau. Right: our quantile baseline (QAE) stabilizes policy entropy and sustains pass@1 gains by steering training into balanced exploration regime. We address this dual challenge with Quantile Advantage Estimation (QAE), which dynamically regulates policy entropy by replacing the conventional mean reward baseline with group-wise Kquantile. The key idea is that the baseline choice controls how many samples receive positive vs. negative advantages, which directly impacts exploration behavior. Specifically, lower marks more samples as having positive advantage, encouraging the model to exploit these successful patterns and reducing entropy. Conversely, higher makes fewer samples appear successful, pushing the model to diversify its behavior patterns, thereby increasing entropy. By tuning the quantile parameter K, we can control the exploration-exploitation balance. As shown in Figure 1 (right), with an appropriately chosen K, this mechanism steers training toward stable entropy regime neither collapsing nor exploding enabling sustained performance gains beyond the prior plateau. This mechanism has striking empirical consequence: it naturally sparsifies updates. With tuned K, roughly 80% of responses receive zero advantage. This concentrates computational effort on the most informative samples and revealing deep redundancy in standard mean-baseline approaches. We trace both early entropy spikes and late plateaus to the mean-baseline in value-free RL; substituting K-quantile baseline (QAE) implements response-level gate that routes updates to rare successes on hard queries and to remaining failures on easy ones. We prove two-sided entropy safety guarantee and derive discriminative objective that explains the observed stability, which leads to significant pass@1 gains and solid pass@16 performance. Empirically, the one-line swap boosts Clip-Higher (Yu et al., 2025) on QWEN3-8B/14B-BASE, pairs well with Clip-Cov/KL-Cov (Cui et al., 2025) on QWEN3-8B-BASE, and works with GSPO (Zheng et al., 2025) on QWEN330B-A3B-BASE, yielding consistent pass@1 gains and strong pass@16 on AIME24, AIME25, and AMC23. Overall, QAE reframes entropy regulation as baseline-design problem rather than token-level tuning problem."
        },
        {
            "title": "2 PRELIMINARIES",
            "content": "In this section, we review the policy optimization algorithms that form the foundation of our work, starting with Proximal Policy Optimization (PPO) and its value-free variants, GRPO and DAPO. Proximal Policy Optimization (PPO) PPO (Schulman et al., 2017) is foundational on-policy algorithm that stabilizes training by constraining policy updates to trust region around the previous policy πθold. It maximizes clipped surrogate objective: JPPO(θ) = E(q,a)D,oπθold (q) rt(θ) ˆAt, clip(rt(θ), 1 ϵ, 1 + ϵ) ˆAt min (1) (cid:17)(cid:105) (cid:16) (cid:104) , 2 Quantile Advantage Estimation for Entropy-Safe Reasoning where rt(θ) = πθ (otq,o<t) value network, and ϵ is the clipping hyperparameter (e.g., 0.2). πθold (otq,o<t) is the probability ratio. The advantage ˆAt is typically estimated by Group Relative Policy Optimization (GRPO) To eliminate the need for value network, GRPO (Shao et al., 2024) adapts the PPO objective by proposing relative advantage estimator. For each query, GRPO samples group of responses {oi}G i=1 from πθold . Each response is assigned binary reward Ri based on its correctness against ground-truth answer a. The advantage for the i-th sample is then estimated by normalizing its reward against the groups statistics: ˆAi = Ri mean({Rk}G std({Rk}G k=1) k=1) , where Ri = (cid:26)1.0 0.0 if is equivalent(a, oi), otherwise. (2) GRPO further incorporates KL divergence penalty against πθold to regularize the policy update. Dynamic Sampling Policy Optimization (DAPO) We use DAPO (Yu et al., 2025), state-ofthe-art value-free method, as our baseline. DAPO refines GRPO with several key modifications. It removes the KL penalty but introduces an asymmetric clipping range (1 ϵlow, 1 + ϵhigh), allowing larger updates for advantageous actions. The objective is also normalized at the token level: JDAPO(θ) =E (q,a)D, i=1πθold (q) {oi}G (cid:34) 1 (cid:88) oi (cid:88) (cid:18) min i=1 t=1 ri,t(θ) ˆAi,t, clip(cid:0)ri,t(θ), 1 ϵlow, 1 + ϵhigh (cid:1) ˆAi,t (cid:19)(cid:35) where = (cid:80)G as in GRPO. Crucially, DAPO employs dynamic sampling constraint: i=1 oi is the total number of tokens in the group, and the advantage ˆAt,i is computed 0 < {oi is equivalent(a, oi)} < G. This ensures that each training batch contains both positive and negative examples, guaranteeing meaningful advantage signal and stable gradients."
        },
        {
            "title": "EXPLOSION",
            "content": "Policy entropy is central to reinforcement learning, governing the explorationexploitation trade-off. This balance is especially fragile in RLVR for large models. When entropy is too low, the policy converges prematurely to suboptimal behaviors (entropy collapse); when it is too high, uncontrolled stochasticity attenuates learning signals (entropy explosion). Navigating this entropy dilemma is therefore pivotal for scaling RLVR. 3.1 THE TWO PERILS OF POLICY ENTROPY Entropy collapse. Well documented in RLVR (Yu et al., 2025; Cui et al., 2025; Zhu et al., 2025), collapse occurs when the policy becomes overly deterministic too early. The resulting loss of exploration traps training in narrow reasoning modes and limits generalization. Entropy explosion. At the other extreme, the policy becomes overly stochastic: gradients are swamped by noise, credit assignment deteriorates, and learning turns unstable and inefficientan equally limiting regime that has been comparatively underexplored (Ahmed et al., 2019; Geist et al., 2019; Haarnoja et al., 2018; Xu et al., 2021; Zhang et al., 2025). The dilemma. Most prior work targets collapse alone. Treating it as the sole bottleneck is critical oversight: in practice, mitigating collapse with existing techniques can inadvertently induce explosion. Addressing only one side is insufficient; effective RLVR requires keeping policy entropy within productive, stable range. We next analyze the mechanisms that drive entropy explosion and motivate our remedy. Quantile Advantage Estimation for Entropy-Safe Reasoning Figure 2: DAPO training dynamics on Qwen38B. Left: without Clip-Higher; Right: with Clip-Higher. In both settings we observe two phasesan early correlated growth between anthropomorphic token frequency and pass@1, followed by decoupling then plateau. While Clip-Higher averts collapse, it does not prevent the later performance stall. Figure 3: Evolution of high-entropy token usage under DAPO (steps 20/80/200). Early training exhibits diverse anthropomorphic tokens (e.g., wait, perhaps); by steps 80200 the distribution homogenizes around rigid reasoning templates (e.g., so, let), indicating reduced exploratory diversity consistent with entropy explosion. 3.2 AN ANALYSIS OF ENTROPY EXPLOSION IN RLVR To investigate the drivers of entropy explosion, we analyze prevalent class of value-free RL methods that apply policy gradients at the token level. We use DAPO (Yu et al., 2025) as representative case, focusing on its Clip-Higher mechanisma token-level control designed to prevent entropy collapse but, as we will show, one that also illustrates the pitfalls of fine-grained control. Unless otherwise noted, we follow the recommended configurations in Yu et al. (2025); full details appear in Appendix B.1. Observation 1: Token-level control does not guarantee sustained reasoning gains. In Figure 2, Clip-Higher triggers an early spike (steps 2080) in anthropomorphic tokensproposed by Yang et al. (2025b) as markers of aha-moment reasoningthat coincides with sharp pass@1 gains. However, after step 150, anthropomorphic token frequency returns toward baseline while performance plateaus. Thus, although Clip-Higher mitigates early collapse, its rapid escalation is coupled with entropy explosion that ultimately limits scaling. Observation 2: Token-level control yields homogenized, low-quality exploration. To probe the stall, we examine the distribution of high-entropy tokens at steps 20, 80, and 200 (cf. Figure 3). Early in training, diverse markers such as wait and perhaps are frequent. By step 80, usage concentrates on assertive, formulaic tokens like so and let. This convergence reflects loss of diversity in highentropy states: the model increasingly relies on rigid reasoning templates rather than exploring alternatives, aligning with the observed plateau. 4 Quantile Advantage Estimation for Entropy-Safe Reasoning Figure 4: Quantile baseline reshapes weighting and entropy dynamics. Left: policy entropy over training split by advantage signnegative-advantage samples drive the surge. Middle/Right: querylevel weights vs. success rate p; GRPO & DAPO use symmetric (cid:112)p(1 p) weighting, whereas our method applies thresholded scheme (K = 0.4). Observation 3: Entropy explosion is disproportionately driven by negative-advantage samples. We decompose entropy dynamics by sample advantage, where positive-advantage samples contribute positive updates and negative-advantage samples contribute non-positive updates. As shown in Figure 4 (Left), entropy growth is dominated by negativeadvantage samples, which show both the steepest increase and the largest share of entropy early in training. Positive-advantage samples remain comparatively stable. This imbalance indicates over-exploration induced by negative-advantage samples in the early phase, followed by insufficient exploitation later. Table 1: Different ϵhigh values in DAPO. ϵhigh AIME24 0.20 0.22 0.24 0. 0.28 32.2918.6% 34.9012.1% 34.1713.9% 40.63+2.4% 39.69 Observation 4: Tuning token-level hyperparameters is insufficient. One might lower the token-level high clip threshold ϵhigh to curb update magnitude. Table 1 (varying ϵhigh from 0.20 to 0.28) shows only marginal effects: performance peaks near ϵhigh = 0.26, but the overall improvement is limited and the late-stage plateau persists. Simply adjusting token-level clipping cannot resolve the core explorationexploitation tension. TAKEAWAY Our analysis indicates that fine-grained, token-level controls provide temporary fix with notable side effects: They prevent entropy collapse but can inadvertently induce performance-limiting entropy explosion. The explosion is mechanically rooted in the advantage baseline, which systematically mishandles negative-advantage samples under reward outliers. The issue is therefore baseline-design flaw, not hyperparameter tuning problem at the token level."
        },
        {
            "title": "REGULATION",
            "content": "Building on the analysis in Section 3, we identify the advantage baseline as the primary source of instability in RLVR. Value-free methods such as GRPO (Shao et al., 2024) and DAPO (Yu et al., 2025) use an empirical mean baseline that is sensitive to reward outliers: few high-reward samples can inflate the baseline, turning otherwise competent responses into negative-advantage examples and penalizing useful exploration, which induces entropy collapse. We address this by quantile-based advantage estimation. Replacing the mean with distributional quantile yields baseline that is (i) statistically robust and (ii) explicitly controllable. single hyperparameter (0, 1) shifts the update focus between exploration and exploitation. 5 Quantile Advantage Estimation for Entropy-Safe Reasoning"
        },
        {
            "title": "4.1 FORMULATION AND INTUITION",
            "content": "For query q, sample responses {(oi, Ri)}G {0, 1}. Let i=1 with oi πold( q) and binary rewards Ri p(q) :="
        },
        {
            "title": "1\nG",
            "content": "G (cid:88) i=1 Ri be the empirical success rate under πold. Define the group empirical CDF (cid:98)Fq(x) :="
        },
        {
            "title": "1\nG",
            "content": "G (cid:88) j=1 1{Rj x}, and the (right-continuous) K-quantile baseline bK(q) := QK({Rj}G j=1) = inf{x : (cid:98)Fq(x) K}, (0, 1)."
        },
        {
            "title": "We then define the standardized advantage",
            "content": "ˆAi = Ri bK(q) std({Rj}G j=1) + ε , ε > 0, (3) where ε prevents division by zero when {0, 1}. For binary rewards, the baseline reduces to threshold on p(q): (cid:40) bK(q) = 0, p(q) 1K, 1, p(q) > 1K. (4) This yields two regimes governed by the difficulty threshold 1K: Hard (exploitation-focused), p(q) 1K. The baseline is 0. Incorrect responses (R = 0) have ˆA = 0, while rare correct responses (R = 1) receive ˆA > 0, reinforcing nascent successful trajectories. Easy (exploration-focused), p(q) > 1K. The baseline is 1. Correct responses have ˆA = 0, while remaining failures (R = 0) yield ˆA < 0, discouraging residual failure modes on alreadysolved queries. Hence acts as direct lever that regulates policy entropy by switching updates between rare successes (hard) and remaining failures (easy). 4.2 GRADIENT ANALYSIS We adopt the discriminative perspective of GRPO introduced by DisCO (Li et al., 2025), which separates query-level weight from discriminative term. Let π+ old( q) denote the conditional distributions of responses with rewards 1 and 0, respectively. For response o, let s+ θ (o, q) denote score functions based on token-normalized policy ratios for positive/negative examples (see Appendix A.2 for exact forms). old( q) and π θ (o, q) and GRPO revisited. Li et al. (2025) show that the GRPO objective can be written as JGRPO(θ) = Eq (cid:104) (cid:113) p(q)(cid:0)1 p(q)(cid:1) (cid:125) (cid:123)(cid:122) query weight (cid:124) (cid:124) oπ+ old, oπ (cid:2)s+ θ (o, q) (cid:123)(cid:122) discriminative term θ (o, q)(cid:3) (cid:125) old (cid:105) , (5) with symmetric weight that down-weights both very easy and very hard queries (cf. Fig. 4). Quantile-based objective. Under Eqs. 34, the standardized advantage is non-zero on only one outcome type per regime. Substituting into GRPO-style objective yields: Proposition 4.1 (Quantile-regulated objective). Assume binary rewards, group size 2, and the right-continuous empirical quantile. Using the standardized advantage in Eqs. 34, the learning objective is (up to constant factor depending on ε) equivalent to (cid:104) JQuantile(θ) = Eq 1{p(q) 1K} (cid:113) p(q) 1p(q) (cid:113) 1p(q) p(q) oπ+ oπ old(q)s+ old(q)s θ (o, q) θ (o, q) (cid:105) . (6) 1{p(q) > 1K} 6 Quantile Advantage Estimation for Entropy-Safe Reasoning Remark. Please check Appendix for all proofs. Compared to the GRPO objective in Eq. 5, QAE makes two crucial changes: (i) it selectively nullifies one of the discriminative terms based on query difficulty, and (ii) it replaces the symmetric, bell-shaped weight (cid:112)p(1 p) with asymmetric, monotonic factorseither (cid:112)p/(1 p) for hard queries or (cid:112)(1 p)/p for easy queries. This transforms the update mechanism from focusing on moderately difficult problems to amplifying signals from rare successes or residual failures (cf. Fig. 4)."
        },
        {
            "title": "4.3 THEORETICAL ANALYSIS: TWO-REGIME ENTROPY SAFETY",
            "content": "Setup. Adopt bandit reduction in which producing full response to is single action. Let π( q) be the current softmax policy and H(q) the token-averaged (length-normalized) policy entropy. Let (cid:98)A denote the GRPO/DAPO-style token-normalized advantage (Sec. 4.2); more generally, write Ab(y, q) = r(y, q) b(q) for the response-level advantage with baseline b(q). For binary rewards with group success rate p(q), we use the right-continuous K-quantile baseline bK(q) (Eq. 4), i.e., bK(q) = 0 if p(q) 1K and 1 otherwise. Under first-order logit updates of softmax policy with step size η > 0, the entropycovariance identity (adapted from Cui et al. (2025)) yields, H(q) η Covyπ(q) (cid:0)log π(y q), π(y q) Ab(y, q)(cid:1), η > 0. Baseline as linear knob. For [0, 1], define Fq(b) := Covπ By linearity, (cid:0)log π, π (r b)(cid:1) for {0, 1}. Fq(b) = Fq(0) Covπ(log π, π), Covπ(log π, π) > 0 whenever π( q) is non-uniform. Hence H(q; b) = η Fq(b) is strictly increasing in [0, 1]. Proposition 4.2 (Two-regime entropy safety of K-quantile). Fix and non-uniform π( q). Then: 1. Low-success (explosion-proof). If p(q) 1K so bK(q) = 0, then for any baseline [0, 1] (including the mean = p(q) or token-level clipping/KL that keep unchanged), H(q; bK) H(q; b). 2. High-success (collapse-proof). If p(q) > 1K so bK(q) = 1, then for any [0, 1], H(q; bK) H(q; b). Sequences vs. token-level controls. Existing token-level controls are one-sided: they rescale step sizes but leave the response-level baseline b(q) unchanged, so they cannot prevent explosion driven by negative-advantage samples. In contrast, the K-quantile baseline is two-sided (Prop. 4.2): bK = 0 when p(q) 1K (explosion-proof) and bK = 1 when p(q) > 1K (collapse-proof), matching the two training regimes in Fig. 4. TAKEAWAY Method takeaways (QAE). K-quantile as response-level gate. single parameter yields deterministic switch (Eqs. 34): hard queries (p(q) 1K) update on rare successes only; easy queries (p(q) > 1K) update on remaining failures only  (Fig. 4)  . Two-sided entropy safety (provable). Under first-order softmax updates, the K-quantile baseline attains the extremal one-step entropy shiftminimal at p(q) 1K (prevents explosion) and maximal at p(q) > 1K (prevents collapse); see Prop. 4.2. Note: Token-level mechanisms only rescale steps and do not change the response-level baseline, so they cannot realize these guarantees."
        },
        {
            "title": "5 EXPERIMENTS",
            "content": "Evaluation protocol. We evaluate on three standard mathreasoning benchmarks: AIME24, AIME25, and AMC23. All evaluations are zero-shot. For each query we sample k=32 completions with temperature =0.7. We report pass@1 and pass@16 as accuracy metrics, together with 7 Quantile Advantage Estimation for Entropy-Safe Reasoning Table 2: Overall performance on the AIME24/25 and AMC23 benchmarks. Our drop-in QAE consistently improves pass@1 across different models and methods, while maintaining comparable pass@16 scores. Red denotes an improvement and blue decline. Model Method AIME25 AIME24 AMC23 Pass@1 Pass@16 Pass@ Pass@16 Pass@1 Pass@16 Qwen38B-Base Qwen3-30BA3B-Base Clip-Higher + QAE 32.71 34.90+6.7% 56.66 57.92+2.2% 39.69 48.23+21.5% 71.63+0.6% 92.97+0.9% 97.50+0.0% 92.11 97.50 71. CLIP-Cov + QAE KL-Cov + QAE GSPO + QAE 33.02 37.40+13.3% 56.29+7.7% 52.27 42.40 46.04+8.6% 33.33 33.44+0.3% 31.15 32.50+4.3% 45.86 51.62+12.6% 44.690.5% 44. 46.59 48.01+3.0% 43.75 47.50+8.6% 68.58 73.16+6.7% 90.23+3.2% 96.25+0.0% 87.42 96.25 73.00 77.08+5.6% 87.97+2.3% 96.25+1.3% 86.02 95.00 67.91 71.72+5.6% 89.380.7% 97.212.2% 99.39 90.00 Figure 5: Training dynamics and sparsity. (a) AIME24 (Qwen38B): QAE boosts pass@1 while keeping pass@16 comparableshowing higher sample efficiency. (b) Entropy by sign: DAPOs explosion stems from negative-advantage samples; QAE suppresses it. (c) Response sparsity: 80% responses have zero advantage, focusing updates on informative subsets. the average tokens per response. Unless noted, we keep all training and decoding hyper-parameters identical across baselines and our method, changing only the response-level baseline from the mean to K-quantile (default K=0.4). This value is chosen to robustly balance exploration and exploitation; we present detailed sensitivity analysis on in Appendix B.3. 5.1 OVERALL PERFORMANCE ACROSS MODELS & RECIPES Drop-in gains across model sizes. Table 2 summarizes results on Qwen3-8B-Base and Qwen330B-A3B-Base. Replacing the mean baseline in DAPO with our K-quantile baseline (QAE) yields consistent pass@1 improvements across datasets and model sizes, while keeping pass@16 performance highly comparable. The stability of this process is further illustrated by the training dynamics curves for both 8B and 14B models in Appendix B.4, which show QAE consistently mitigates the entropy explosion seen in the baseline. Compatibility with strong recipes. QAE is orthogonal to token-level controls (e.g., CLIP-COV, KL-COV) and sequence-level optimization (GSPO). When layered on top of these methods, QAE consistently provides further gains without altering their hyper-parameters. 5.2 TRAINING DYNAMICS & ENTROPY SAFETY Pass@1 improves while pass@16 stays comparable. Figure 5 (Left) plots AIME24 performance over training for Qwen3-8B-Base. From step 100, DAPO exhibits an entropy surge and pass@1 stalls, while QAE maintains stable training and continues to improve. Pass@16 remains similar, reinforcing the sample-efficiency reading. Negative-advantage entropy is the driver of instability. Figure 5 (Middle) decomposes entropy by the sign of the advantage. The growth is dominated by negative-advantage samples; QAE sup8 Quantile Advantage Estimation for Entropy-Safe Reasoning Figure 6: Performance and ablations. (a) QAE improves DAPO on the 14B model for both AIME25 and AIME24 (pass@1). (b) With weaker high-end clipping (ϵhigh=0.28), controlling negative-advantage updates (NEG-MASK) is most critical, closely tracking full QAE. (c) With stronger clipping (ϵhigh=0.20), positive-advantage control (POS-MASK) dominates. presses this component and keeps the overall entropy within productive range. This behavior follows directly from using quantile baseline that down-weights uninformative negatives. Response-level sparsity: the 80/20 rule. Figure 5 (Right) shows that 80% of sampled responses have zero advantage throughout training. This response-level 80/20 rule focuses updates on the informative minority, explaining QAEs stability and efficiency. In contrast to the baseline, which leads to homogenized exploration (Sec. 3.2), QAE sustains productive co-growth of diverse exploratory tokens and reasoning accuracy, as detailed in Appendix B.2. 5.3 ABLATIONS & COMPOSITION Masking mechanisms. QAE can be viewed as selectively masking updates. To disentangle their roles, we define two one-sided objectives: JPOS-MASK(θ) = Eq (cid:2)1{p(q)1K} (cid:113) p(q) 1p(q) oπ+ old s+ θ (o, q) (cid:113) 1p(q) p(q) oπ old θ (o, q)(cid:3). (7) JNEG-MASK(θ) = Eq (cid:2)(cid:113) p(q) 1p(q) oπ+ old s+ θ (o, q) 1{p(q)>1K} (cid:113) 1p(q) p(q) oπ old θ (o, q)(cid:3). (8) Masking mechanisms. QAE can be interpreted as masking positives on easy queries and negatives on hard queries. We isolate each side by constructing two objectives: POS-MASK (Eq. 7) and NEG-MASK (Eq. 8), leaving the other side unmasked. Explosion vs. collapse regimes. As shown in Fig. 6 (b-c), when the high-end clipping is weak (ϵhigh=0.28), the dominant failure mode is entropy explosion; NEG-MASK nearly matches QAE and outperforms POS-MASK. With strong clipping (ϵhigh=0.20), collapse pressure dominates and the ordering flips (POS-MASK > NEG-MASK). This matches the two-regime analysis in Sec. 4.3."
        },
        {
            "title": "6 DISCUSSION",
            "content": "K as an entropyguided explorationexploitation knob. We use single hyperparameter to control how many responses receive nonzero advantage, thereby steering the explorationexploitation trade-off by modulating entropy. Operational rule-of-thumb: we select once per baseline by inspecting the entropy of the baseline policy, rather than the evaluation metric. When entropy is low (risk of mode collapse), choose = 0.6 to inject diversity; when entropy is high (risk of unstable updates), choose = 0.4 to temper exploration. Since all our recipes use Clip-Higher, we default to = 0.4; finer-grained tuning can yield further gains. QAE prioritizes who learns over how much. Updating only small subset of samples ( 20%; Figure 5 (Right)) makes RLVR more stable and improves scaling behavior, indicating that selection, not update magnitude, is the primary bottleneck. QAE implements binary reward with quantile baseline at the query level: for difficult queries it assigns credit to successes, whereas for easy queries 9 Quantile Advantage Estimation for Entropy-Safe Reasoning it assigns credit to failures. By adjusting the masking rangeor by introducing dual masksnaıve DAPO/GRPO reduce to special cases, providing safe fallback within the same framework. Baseline design as third knob for entropy control. Prior work has studied positive/negative ratios (Zhu et al., 2025), entropy dynamics (Cui et al., 2025), and advantage shaping (Cheng et al., 2025). QAE is orthogonal: it uses baseline designa shift from the mean to the K-quantileas the primary entropy lever and composes cleanly with existing techniques  (Table 2)  . Related to Arnal et al. (2025), which analyzes tunable baselines in REINFORCE, our quantile baseline is data-adaptive, group-level instantiation that improves robustness while preserving standard policygradient updates."
        },
        {
            "title": "7 RELATED WORK",
            "content": "Reinforcement learning for LLM RL has become key technique for eliciting advanced reasoning in large language models (LLMs), paradigm shift from its earlier applications in preference alignment via RLHF (Ouyang et al., 2022). This modern approach, termed Reinforcement Learning with Verifiable Rewards (RLVR) (Lambert et al., 2024), leverages outcome-based optimization to achieve state-of-the-art performance in complex domains like mathematics and programming. Seminal works, including OpenAIs o1 (ope) and DeepSeek R1 (DeepSeek-AI et al., 2025), demonstrated that RL can effectively scale reasoning capabilities, spurring new line of research (Yang et al., 2025a; Team et al., 2025). Central to this progress are online, value-free algorithms that have generally outperformed offline preference optimization methods (Rafailov et al., 2023; Wu et al., 2024; 2025). In particular, Group Relative Policy Optimization (GRPO) (Shao et al., 2024) and its successor, Dynamic Sampling Policy Optimization (DAPO) (Yu et al., 2025), have emerged as foundational baselines for many contemporary reasoning systems (Yue et al., 2025; Zeng et al., 2025; Hu et al., 2025). Our work uses DAPO as representative algorithm to investigate critical, unresolved challenge in this domain: the training instability caused by dysregulated policy entropy, which limits the performance and scalability of current RLVR methods. Exploration, entropy dynamics, and collapse/explosion in RLVR. Evidence in RLVR links exploration tightly to entropy dynamics: performance gains concentrate on minority of high-entropy forking tokens (Wang et al., 2025b), with thinking tokens acting as information peaks (Qian et al., 2025). At the sequence level, entropy can collapse early or explode if left unchecked (Cui et al., 2025). Extremes such as pure entropy minimization (Agarwal et al., 2025) or heavy upweighting of negative-advantage samples (Zhu et al., 2025) underscore the need for principled regulation, in line with cautions against indiscriminate maximum-entropy optimization (Zhang et al., 2025) and classic guidance to schedule target entropy (Xu et al., 2021) within regularized MDP theory (Geist et al., 2019; Ahmed et al., 2019). On the recipe side, entropy as advantage shaping (Cheng et al., 2025), Pass@k-based training (Chen et al., 2025), rubric-scaffolded exploration (Zhou et al., 2025), entropy-modulated policy gradients for long-horizon agents (Wang et al., 2025a), and outcomebased exploration (Song et al., 2025), together with agentic systems such as rStar2-Agent (Shang et al., 2025), provide practical means to prevent collapse/explosion while improving diversity."
        },
        {
            "title": "8 CONCLUSION",
            "content": "Conclusion We propose Quantile Advantage Estimation (QAE), replacing the mean baseline with group-wise K-quantile to implement two-regime gate that amplifies rare successes and suppresses residual failures. Under first-order policy updates, QAE provides two-sided entropy control with bounded one-step entropy change, curbing both collapse and explosion. Empirically, QAE stabilizes entropy, sparsifies credit assignment, and improves pass@1 across reasoning benchmarks while composing cleanly with standard sequenceand token-level controls. Limitations and Future Work (i) Dynamic K: Beyond fixed K, explore simple schedules or two-phase curricula to better balance exploration and exploitation; (ii) Automatic K: Adapt to model state (e.g., success rate, entropy, or gradient variance) to remove manual tuning; (iii) PPO integration: Embed the quantile-baseline idea into PPOs whitening/normalizatione.g., batchwise quantile baselinesto test robustness across algorithms and scales. 10 Quantile Advantage Estimation for Entropy-Safe Reasoning"
        },
        {
            "title": "Learning",
            "content": "Reason with learning-to-reason-with-llms/. LLMs. to"
        },
        {
            "title": "URL",
            "content": "https://openai.com/index/ Shivam Agarwal, Zimin Zhang, Lifan Yuan, Jiawei Han, and Hao Peng. The unreasonable effectiveness of entropy minimization in LLM reasoning. arXiv preprint arXiv:2505.15134, 2025. Zafarali Ahmed, Nicolas Le Roux, Mohammad Norouzi, and Dale Schuurmans. Understanding the impact of entropy on policy optimization. In ICML, 2019. Charles Arnal, Gaetan Narozniak, Vivien Cabannes, Yunhao Tang, Julia Kempe, and Remi Munos. Asymmetric REINFORCE for off-policy reinforcement learning: Balancing positive and negative rewards. CoRR, abs/2506.20520, 2025. Zhipeng Chen, Yingqian Min, Beichen Zhang, Jie Chen, Jinhao Jiang, Zheng Liu, and Wayne Xin Zhao. Pass@k training for adaptively balancing exploration and exploitation of large reasoning models. arXiv preprint arXiv:2508.10751, 2025. URL https://arxiv.org/abs/2508. 10751. Daixuan Cheng, Shaohan Huang, Xuekai Zhu, Bo Dai, Wayne Xin Zhao, Zhenliang Zhang, and Furu Wei. Reasoning with exploration: An entropy perspective. arXiv preprint arXiv:2506.14758, 2025. Ganqu Cui, Yuchen Zhang, Jiacheng Chen, Lifan Yuan, Zhi Wang, Yuxin Zuo, Haozhan Li, Yuchen Fan, Huayu Chen, Weize Chen, Zhiyuan Liu, Hao Peng, Lei Bai, Wanli Ouyang, Yu Cheng, Bowen Zhou, and Ning Ding. The entropy mechanism of reinforcement learning for reasoning language models. CoRR, abs/2505.22617, 2025. DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J. L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, R. J. Chen, R. L. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, and S. S. Li. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. CoRR, abs/2501.12948, 2025. Matthieu Geist, Bruno Scherrer, and Olivier Pietquin. theory of regularized markov decision processes. In ICML, 2019. Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with stochastic actor. In ICML, 2018. Godfrey Harold Hardy, John Edensor Littlewood, and George Polya. Inequalities. Cambridge university press, 1952. Jingcheng Hu, Yinmin Zhang, Qi Han, Daxin Jiang, Xiangyu Zhang, and Heung-Yeung Shum. Open-reasoner-zero: An open source approach to scaling up reinforcement learning on the base model. CoRR, abs/2503.24290, 2025. Nathan Lambert, Jacob Morrison, Valentina Pyatkin, Shengyi Huang, Hamish Ivison, Faeze Brahman, Lester James V. Miranda, Alisa Liu, Nouha Dziri, Shane Lyu, Yuling Gu, Saumya Malik, Victoria Graf, Jena D. Hwang, Jiangjiang Yang, Ronan Le Bras, Oyvind Tafjord, Chris Wilhelm, Luca Soldaini, Noah A. Smith, Yizhong Wang, Pradeep Dasigi, and Hannaneh Hajishirzi. Tulu 3: Pushing frontiers in open language model post-training. CoRR, abs/2411.15124, 2024. 11 Quantile Advantage Estimation for Entropy-Safe Reasoning Gang Li, Ming Lin, Tomer Galanti, Zhengzhong Tu, and Tianbao Yang. Disco: Reinforcing large reasoning models with discriminative constrained optimization. CoRR, abs/2505.12366, 2025. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F. Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback. In NeurIPS, 2022. Long Phan, Alice Gatti, Ziwen Han, Nathaniel Li, Josephina Hu, Hugh Zhang, Sean Shi, Michael Choi, Anish Agrawal, Arnav Chopra, Adam Khoja, Ryan Kim, Jason Hausenloy, Oliver Zhang, Mantas Mazeika, Daron Anderson, Tung Nguyen, Mobeen Mahmood, Fiona Feng, Steven Y. Feng, Haoran Zhao, Michael Yu, Varun Gangal, Chelsea Zou, Zihan Wang, Jessica P. Wang, Pawan Kumar, Oleksandr Pokutnyi, Robert Gerbicz, Serguei Popov, John-Clark Levin, Mstyslav Kazakov, Johannes Schmitt, Geoff Galgon, Alvaro Sanchez, Yongki Lee, Will Yeadon, Scott Sauers, Marc Roth, Chidozie Agu, Søren Riis, Fabian Giska, Saiteja Utpala, Zachary Giboney, Gashaw M. Goshu, Joan of Arc Xavier, Sarah-Jane Crowson, Mohinder Maheshbhai Naiya, Noah Burns, Lennart Finke, Zerui Cheng, Hyunwoo Park, Francesco Fournier-Facio, John Wydallis, Mark Nandor, Ankit Singh, Tim Gehrunger, Jiaqi Cai, Ben McCarty, Darling Duclosel, Jungbae Nam, Jennifer Zampese, Ryan G. Hoerr, Aras Bacho, Gautier Abou Loume, Abdallah Galal, Hangrui Cao, Alexis C. Garretson, Damien Sileo, Qiuyu Ren, Doru Cojoc, Pavel Arkhipov, Usman Qazi, Lianghui Li, Sumeet Motwani, Christian Schroder de Witt, Edwin Taylor, Johannes Veith, Eric Singer, Taylor D. Hartman, Paolo Rissone, Jaehyeok Jin, Jack Wei Lun Shi, Chris G. Willcocks, Joshua Robinson, Aleksandar Mikov, Ameya Prabhu, Longke Tang, Xavier Alapont, Justine Leon Uro, Kevin Zhou, Emily de Oliveira Santos, Andrey Pupasov Maksimov, Edward Vendrow, Kengo Zenitani, Julien Guillod, Yuqi Li, Joshua Vendrow, Vladyslav Kuchkin, and Ng Ze-An. Humanitys last exam. CoRR, abs/2501.14249, 2025. Chen Qian, Dongrui Liu, Haochen Wen, Zhen Bai, Yong Liu, and Jing Shao. Demystifying reasoning dynamics with mutual information: Thinking tokens are information peaks in llm reasoning. arXiv preprint arXiv:2506.02867, 2025. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D. Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. In NeurIPS, 2023. David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel R. Bowman. GPQA: graduate-level google-proof q&a benchmark. CoRR, abs/2311.12022, 2023. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. CoRR, abs/1707.06347, 2017. URL http://arxiv.org/abs/ 1707.06347. Ning Shang, Yifei Liu, Yi Zhu, Li Lyna Zhang, Weijiang Xu, Xinyu Guan, Buze Zhang, Bingcheng Dong, Xudong Zhou, Bowen Zhang, Ying Xin, Ziming Miao, Scarlett Li, Fan Yang, and Mao Yang. rstar2-agent: Agentic reasoning technical report. arXiv preprint arXiv:2508.20722, 2025. URL https://arxiv.org/abs/2508.20722. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. CoRR, abs/2402.03300, 2024. Yuda Song, Julia Kempe, and Remi Munos. Outcome-based exploration for llm reasoning. arXiv preprint arXiv:2509.06941, 2025. URL https://arxiv.org/abs/2509.06941. Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, Chuning Tang, Congcong Wang, Dehao Zhang, Enming Yuan, Enzhe Lu, Fengxiang Tang, Flood Sung, Guangda Wei, Guokun Lai, Haiqing Guo, Han Zhu, Hao Ding, Hao Hu, Hao Yang, Hao Zhang, Haotian Yao, Haotian Zhao, Haoyu Lu, Haoze Li, Haozhen Yu, Hongcheng Gao, Huabin Zheng, Huan Yuan, Jia Chen, Jianhang Guo, Jianlin Su, Jianzhou Wang, Jie Zhao, Jin Zhang, Jingyuan Liu, Junjie Yan, Junyan Wu, Lidong Shi, Ling Ye, 12 Quantile Advantage Estimation for Entropy-Safe Reasoning Longhui Yu, Mengnan Dong, Neo Zhang, Ningchen Ma, Qiwei Pan, Qucheng Gong, Shaowei Liu, Shengling Ma, Shupeng Wei, Sihan Cao, Siying Huang, Tao Jiang, Weihao Gao, Weimin Xiong, Weiran He, Weixiao Huang, Wenhao Wu, Wenyang He, Xianghui Wei, Xianqing Jia, Xingzhe Wu, Xinran Xu, Xinxing Zu, Xinyu Zhou, Xuehai Pan, Y. Charles, Yang Li, Yangyang Hu, Yangyang Liu, Yanru Chen, Yejie Wang, Yibo Liu, Yidao Qin, Yifeng Liu, Ying Yang, Yiping Bao, Yulun Du, Yuxin Wu, Yuzhi Wang, Zaida Zhou, Zhaoji Wang, Zhaowei Li, Zhen Zhu, Zheng Zhang, Zhexu Wang, Zhilin Yang, Zhiqi Huang, Zihao Huang, Ziyao Xu, and Zonghan Yang. Kimi k1.5: Scaling reinforcement learning with llms. CoRR, abs/2501.12599, 2025. Jiawei Wang, Jiacai Liu, Yuqian Fu, Yingru Li, Xintao Wang, Yuan Lin, Yu Yue, Lin Zhang, Yang Wang, and Ke Wang. Harnessing uncertainty: Entropy-modulated policy gradients for longhorizon llm agents. arXiv preprint arXiv:2509.09265, 2025a. URL https://arxiv.org/ abs/2509.09265. Shenzhi Wang, Le Yu, Chang Gao, Chujie Zheng, Shixuan Liu, Rui Lu, Kai Dang, Xionghui Chen, Jianxin Yang, Zhenru Zhang, et al. Beyond the 80/20 rule: High-entropy minority tokens drive effective reinforcement learning for llm reasoning. arXiv preprint arXiv:2506.01939, 2025b. Junkang Wu, Yuexiang Xie, Zhengyi Yang, Jiancan Wu, Jinyang Gao, Bolin Ding, Xiang Wang, and Xiangnan He. β-dpo: Direct preference optimization with dynamic β. In NeurIPS, 2024. Junkang Wu, Kexin Huang, Xue Wang, Jinyang Gao, Bolin Ding, Jiancan Wu, Xiangnan He, and Xiang Wang. Repo: Relu-based preference optimization. CoRR, abs/2503.07426, 2025. Yaosheng Xu, Dailin Hu, Litian Liang, Stephen McAleer, Pieter Abbeel, and Roy Fox. Target entropy annealing for discrete soft actorcritic. In NeurIPS 2021 Workshop, 2021. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao Ge, Haoran Wei, Huan Lin, Jialong Tang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jian Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keqin Bao, Kexin Yang, Le Yu, Lianghao Deng, Mei Li, Mingfeng Xue, Mingze Li, Pei Zhang, Peng Wang, Qin Zhu, Rui Men, Ruize Gao, Shixuan Liu, Shuang Luo, Tianhao Li, Tianyi Tang, Wenbiao Yin, Xingzhang Ren, Xinyu Wang, Xinyu Zhang, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yinger Zhang, Yu Wan, Yuqiong Liu, Zekun Wang, Zeyu Cui, Zhenru Zhang, Zhipeng Zhou, and Zihan Qiu. Qwen3 technical report. CoRR, abs/2505.09388, 2025a. Shu Yang, Junchao Wu, Xin Chen, Yunze Xiao, Xinyi Yang, Derek F. Wong, and Di Wang. from external observations to internal mechanisms. CoRR, Understanding aha moments: abs/2504.02956, 2025b. Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Tiantian Fan, Gaohong Liu, Lingjun Liu, Xin Liu, Haibin Lin, Zhiqi Lin, Bole Ma, Guangming Sheng, Yuxuan Tong, Chi Zhang, Mofan Zhang, Wang Zhang, Hang Zhu, Jinhua Zhu, Jiaze Chen, Jiangjie Chen, Chengyi Wang, Hongli Yu, Weinan Dai, Yuxuan Song, Xiangpeng Wei, Hao Zhou, Jingjing Liu, WeiYing Ma, Ya-Qin Zhang, Lin Yan, Mu Qiao, Yonghui Wu, and Mingxuan Wang. DAPO: an open-source LLM reinforcement learning system at scale. CoRR, abs/2503.14476, 2025. Yu Yue, Yufeng Yuan, Qiying Yu, Xiaochen Zuo, Ruofei Zhu, Wenyuan Xu, Jiaze Chen, ChengXiang Wang, Tiantian Fan, Zhengyin Du, Xiangpeng Wei, Xiangyu Yu, Gaohong Liu, Juncai Liu, Lingjun Liu, Haibin Lin, Zhiqi Lin, Bole Ma, Chi Zhang, Mofan Zhang, Wang Zhang, Hang Zhu, Ru Zhang, Xin Liu, Mingxuan Wang, Yonghui Wu, and Lin Yan. VAPO: efficient and reliable reinforcement learning for advanced reasoning tasks. CoRR, abs/2504.05118, 2025. Weihao Zeng, Yuzhen Huang, Qian Liu, Wei Liu, Keqing He, Zejun Ma, and Junxian He. Simplerlzoo: Investigating and taming zero reinforcement learning for open base models in the wild. CoRR, abs/2503.18892, 2025. Ruipeng Zhang, Ya-Chien Chang, and Sicun Gao. When maximum entropy misleads policy optimization. In ICML, 2025. 13 Quantile Advantage Estimation for Entropy-Safe Reasoning Chujie Zheng, Shixuan Liu, Mingze Li, Xiong-Hui Chen, Bowen Yu, Chang Gao, Kai Dang, Yuqiong Liu, Rui Men, An Yang, Jingren Zhou, and Junyang Lin. Group sequence policy optimization. CoRR, abs/2507.18071, 2025. Yang Zhou, Sunzhu Li, Shunyu Liu, Wenkai Fang, Jiale Zhao, Jingwen Yang, Jianwei Lv, Kongcheng Zhang, Yihe Zhou, Hengtong Lu, Wei Chen, Yan Xie, and Mingli Song. Breaking the exploration bottleneck: Rubric-scaffolded reinforcement learning for general llm reasoning. arXiv preprint arXiv:2508.16949, 2025. URL https://arxiv.org/abs/2508.16949. Xinyu Zhu, Mengzhou Xia, Zhepei Wei, Wei-Lin Chen, Danqi Chen, and Yu Meng. The surprising effectiveness of negative reinforcement in LLM reasoning. CoRR, abs/2506.01347, 2025. 14 Quantile Advantage Estimation for Entropy-Safe Reasoning"
        },
        {
            "title": "A PROOF",
            "content": "A.1 PROOF OF PROPOSITION 4.1 (cid:105) . (6) (9) Proposition 4.1 (Quantile-regulated objective). Assume binary rewards, group size 2, and the right-continuous empirical quantile. Using the standardized advantage in Eqs. 34, the learning objective is (up to constant factor depending on ε) equivalent to (cid:104) JQuantile(θ) = Eq 1{p(q) 1K} 1{p(q) > 1K} (cid:113) p(q) 1p(q) (cid:113) 1p(q) p(q) oπ+ oπ old(q)s+ old(q)s θ (o, q) θ (o, q) Proof. Write = p(q) for brevity. Recall the token-normalized surrogate (θ) = Eq Eoπ0(q) 1 o (cid:88) t=1 (cid:18) πθ(ot q, o<t) π0(ot q, o<t) (cid:19) , A(o q) , and the positive/negative homogeneous scaling of (the same convention as in the main text): (x, c) = (cid:40) +(x, 1), c(cid:0) (x, 1)(cid:1), > 0, < 0, (x, c) = (x, 1) (c > 0). (10) For the binary reward r(o q) {0, 1} and the group statistics Eoπ0(q)r(o q) = and Varoπ0(q)r(o q) = p(1 p), the standardized advantage used in the paper takes the form A(o q) = (cid:114) 1 (cid:114) 1 , , r(o q) = 1, r(o q) = 0. (11) Under the K-quantile baseline described in Section 4 (right-continuous), responses are masked asymmetrically by the regime of p: if 1 : A+(q) = 1 (cid:112)p(1 p) , A(q) = 0; if > 1 : A+(q) = 0, A(q) = 1 (cid:112)p(1 p) . (12) (13) Equivalently, among {r = 1, = 0} only one label contributes in each regime. Plug equation 12 into equation 9 and decompose over {1, 0} (writing π+ for π0( q) conditioned on = 1 and = 0, respectively): 0 ( q) and π 0 ( q) (cid:34) (θ) = Eq 1{p 1 K} oπ+ 0 (q) (cid:32) 1 (cid:88) + 1{p > 1 K} (1 p) oπ 0 (q) 1 (cid:88) (cid:33) πθ(ot q, o<t) π0(ot q, o<t) (cid:32) , πθ(ot q, o<t) π0(ot q, o<t) 1 (cid:112)p(1 p) 1 (cid:112)p(1 p) , (14) (cid:33) (cid:35) . Apply the homogeneity equation 10 separately to the two terms in equation 14. For 1 the scalar is positive, and for > 1 it is negative, hence (cid:34) (θ) = Eq 1{p 1K} (cid:114) 1 E oπ+ 0 (q) 1 (cid:88) + (cid:18) πθ(ot q, o<t) π0(ot q, o<t) (cid:19) , 1 (15) 1{p > 1K} (cid:114) 1 oπ 0 (q) 1 (cid:88) (cid:18) πθ(ot q, o<t) π0(ot q, o<t) (cid:19) (cid:35) , 1 . 15 Quantile Advantage Estimation for Entropy-Safe Reasoning Equation 15 is the claimed quantile-regulated objective: compared with the symmetric GRPO/- DAPO weight (cid:112)p(1 p), the quantile baseline (i) masks one side (positives on easy queries with > 1 or negatives on hard queries with 1 K) and (ii) re-weights the active side by the asymmetric factors (cid:112)p/(1 p) or (cid:112)(1 p)/p. This completes the proof. Instantiating for GRPO. For GRPO we use +(x, 1) = min(cid:0)x, clip(x, 1 ϵ, 1 + ϵ)(cid:1) = min(x, 1 + ϵ), (x, 1) = max(cid:0)x, clip(x, 1 ϵ, 1 + ϵ)(cid:1) = max(x, 1 ϵ), (16) (17) which can be plugged into equation 15 directly. A.2 PROOF OF PROPOSITION 4.2 Proposition 4.2 (Two-regime entropy safety of K-quantile). Fix and non-uniform π( q). Then: 1. Low-success (explosion-proof). If p(q) 1K so bK(q) = 0, then for any baseline [0, 1] (including the mean = p(q) or token-level clipping/KL that keep unchanged), H(q; bK) H(q; b). 2. High-success (collapse-proof). If p(q) > 1K so bK(q) = 1, then for any [0, 1], H(q; bK) H(q; b). Proof. Fix and non-uniform softmax policy π( q). For any baseline [0, 1] and binary reward {0, 1}, write Ab(y, q) = r(y, q) b, Fq(b) := Covyπ(q) (cid:0)log π(y q), π(y q) (r(y, q) b)(cid:1). The entropycovariance identity for softmax policies under first-order logit updates (adapted from Cui et al. (2025)) gives Step 1: Baseline monotonicity. By bilinearity of covariance, H(q; b) η Fq(b), η > 0. Fq(b) = Covπ (cid:0)log π, πr(cid:1) Covπ (cid:0)log π, π(cid:1) =: Fq(0) Cq. (18) (19) Let := π(Y q) for π( q). Then Cq = Cov(log U, ). Since (cid:55) log and (cid:55) are strictly increasing on (0, 1], they are co-monotone; hence Cov(log U, ) > 0 whenever is non-constant, i.e., whenever π( q) is non-uniform (see, e.g., Chebyshevs sum / rearrangement inequality (Hardy et al., 1952)). Therefore Cq > 0 and equation 19 shows that Fq(b) is strictly decreasing in b, so by equation 18 the entropy change H(q; b) is strictly increasing in [0, 1]. Step 2: Two-regime extremality of the K-quantile baseline. For Bernoulli rewards with success rate p(q), the K-quantile baseline is bK(q) = (cid:26)0, p(q) 1 K, 1, p(q) > 1 K, (Eq. 4). Because H(q; b) increases in (Step 1), we have, for any [0, 1], p(q) 1 bK(q) = 0 = min[0, 1] H(q; bK) H(q; b), p(q) > 1 bK(q) = 1 = max[0, 1] H(q; bK) H(q; b). Strict inequalities hold whenever π( q) is non-uniform and = bK(q). These are exactly Items (1) and (2) of Proposition 4.2. This establishes the claimed two-regime entropy safety: in the low-success regime (p 1 K) the quantile choice bK = 0 minimizes the entropy increment (explosion-proof), whereas in the high-success regime (p > 1 K) the choice bK = 1 maximizes it (collapse-proof). 16 Quantile Advantage Estimation for Entropy-Safe Reasoning Figure 7: High-entropy token diagnostics under QAE. Green bars: counts of anthropomorphic high-entropy tokens; orange line: overall pass@1. Early coupled growth transitions to later decouplingtoken counts plateau while accuracy improvesindicating entropy-safe, selective exploration."
        },
        {
            "title": "B EXPERIMENTS",
            "content": "B.1 IMPLEMENTATION DETAILS Experimental Setup: Our configuration includes clip-higher, dynamic sampling, token-level policy gradient loss, and overlong reward shaping, as proposed in DAPO. We use the recommended hyperparameters: ϵhigh = 0.28 and ϵlow = 0.2 for clip-higher, and maximum response length of 20,480 with 4,096-token cache for reward shaping. Training Details: We train with global batch size of 512, using 16 gradient accumulation steps with mini-batch size of 32. The learning rate is fixed at 106 with no warmup or decay schedule. Importantly, we exclude both KL divergence and entropy losses. Evaluation: To analyze scaling effects, we apply this method to the Qwen3-32B and Qwen3-8B base models, training them on the DAPO-Math-17K dataset (Yu et al., 2025). Additional Experiments: We also conduct cold-start experiment with the GSPO algorithm, initializing from the Qwen3-30B-A3B-Base model. In this configuration, we use four gradient accumulation steps per batch. The GSPO clipping ranges are set to 3 104 (left) and 4 104 (right), aligning with the official VERL implementation script2. B.2 MORE EXPERIMENTS QAE sustains co-growth of aha markers and accuracy. Contrasting with Clip-Higher, Fig. 7 shows that under QAE the anthropomorphic token count and pass@1 rise together across training. From early to late steps, the green bars (aha markers) increase and remain elevated, while the orange curve improves monotonically, indicating that exploration is converted into productive reasoning rather than unchecked entropy. High-entropy token diagnostics under QAE (fine-grained snapshots). finer-grained inspection at representative steps20/80/200 in Fig. 8corroborates this interpretation. At step 20, anthropomorphic markers are sparse, consistent with exploration just being activated; by step 80, these tokens separate more distinctly, aligning with the performance uptick seen in the coupled-growth regime; by step 200, their counts stabilize despite continued pass@1 gains, evidencing shift from more randomness to targeted refinement. Taken together with the trajectory view, these snapshots confirm that QAE leverages high-entropy branches when beneficial and then curbs their proliferation once they cease to deliver marginal utility. 2https://github.com/volcengine/verl/blob/main/recipe/gspo/test_gspo_3b_ math.sh Quantile Advantage Estimation for Entropy-Safe Reasoning Figure 8: Token-level diagnostics. Probability mass over top high-entropy tokens at different training steps. Under QAE, exploratory tokens increase in controlled manner, aligning with the stableentropy regime in Fig. 5b. Figure 9: Training curves under different on Qwen3-8B-Base. Left: entropy; middle: accuracy (AIME24@32); right: response length. B.3 QUANTILE PARAMETER ANALYSIS Trade-offs governed by K. Figure 9 (left/middle/right) shows how the quantile tunes entropy, accuracy, and response length for Qwen3-8B-Base. Large (e.g., 0.8) marks most samples as negative-advantage, driving entropy upward, inflating response length, and yielding volatile training with an early accuracy plateau. Small (e.g., 0.2) marks most samples as positive-advantage, producing low-entropy, over-regularized regime that is stable but exploration-poor, with limited accuracy gains. These trends align with Sec. 4: simultaneously sets the share of responses updated and the direction of entropy flow. Stability at = 0.4 (with Clip-Higher). All main experiments use = 0.4 with ϵhigh = 0.28. This configuration avoids the high-entropy instability seen at = 0.8 while maintaining sufficient stochasticity to prevent collapse. Empirically it yields bounded entropy (Fig. 9, left), stable lengths (right), and sustained accuracy improvements (middle), striking robust explorationexploitation balance and matching the two-sided entropy safety predicted by our analysis. B.4 ANALYSIS OF TRAINING DYNAMICS ON 8B AND 14B MODELS QAE stabilizes entropy and sustains performance gains across model scales. We compare the baseline DAPO with DAPO+QAE on Qwen3-8B-Base (Figure 10) and Qwen3-14B-Base (Figure 11). Across both model sizes, QAE consistently reduces and stabilizes policy entropy, keeps response length bounded, and yields smoother, longer-lasting accuracy improvements. On Qwen3-8B, the mean-baseline variant exhibits pronounced entropy surge around step 100, accompanied by divergence in response length and subsequent accuracy plateau. In contrast, QAE maintains entropy within productive range throughout training and avoids the plateau, leading to sustained accuracy gains in later stages. 18 Quantile Advantage Estimation for Entropy-Safe Reasoning Figure 10: Training curves under DAPO and DAPO + QAE on Qwen3-8B-Base. Left: entropy; middle: accuracy (AIME24@32); right: response length. Figure 11: Training curves under DAPO and DAPO + QAE on Qwen3-14B-Base. Left: entropy; middle: accuracy (AIME24@32); right: response length. The same pattern appears on Qwen3-14B. Although the entropy spike under the baseline is less severe, its entropy remains higher and more volatile than with QAE. QAE again moderates entropy and response length and produces smoother, more monotonic accuracy trajectory. Taken together, these results indicate that QAE addresses the sensitivity of the mean baseline in value-free RL training and that principled baseline design provides an effective mechanism for scale-robust entropy control in RLVR."
        }
    ],
    "affiliations": [
        "University of Science and Technology of China"
    ]
}