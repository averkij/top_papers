{
    "paper_title": "Text-conditioned State Space Model For Domain-generalized Change Detection Visual Question Answering",
    "authors": [
        "Elman Ghazaei",
        "Erchan Aptoula"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The Earth's surface is constantly changing, and detecting these changes provides valuable insights that benefit various aspects of human society. While traditional change detection methods have been employed to detect changes from bi-temporal images, these approaches typically require expert knowledge for accurate interpretation. To enable broader and more flexible access to change information by non-expert users, the task of Change Detection Visual Question Answering (CDVQA) has been introduced. However, existing CDVQA methods have been developed under the assumption that training and testing datasets share similar distributions. This assumption does not hold in real-world applications, where domain shifts often occur. In this paper, the CDVQA task is revisited with a focus on addressing domain shift. To this end, a new multi-modal and multi-domain dataset, BrightVQA, is introduced to facilitate domain generalization research in CDVQA. Furthermore, a novel state space model, termed Text-Conditioned State Space Model (TCSSM), is proposed. The TCSSM framework is designed to leverage both bi-temporal imagery and geo-disaster-related textual information in an unified manner to extract domain-invariant features across domains. Input-dependent parameters existing in TCSSM are dynamically predicted by using both bi-temporal images and geo-disaster-related description, thereby facilitating the alignment between bi-temporal visual data and the associated textual descriptions. Extensive experiments are conducted to evaluate the proposed method against state-of-the-art models, and superior performance is consistently demonstrated. The code and dataset will be made publicly available upon acceptance at https://github.com/Elman295/TCSSM."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 1 ] . [ 1 4 7 9 8 0 . 8 0 5 2 : r TEXT-CONDITIONED STATE SPACE MODEL FOR DOMAIN-GENERALIZED CHANGE DETECTION VISUAL QUESTION ANSWERING Elman Ghazaei Faculty of Engineering and Natural Sciences (VPALab), Sabanci University, Türkiye elman.ghazaei@sabanciuniv.edu Erchan Aptoula Faculty of Engineering and Natural Sciences (VPALab), Sabanci University, Türkiye erchan.aptoula@sabanciuniv.edu"
        },
        {
            "title": "ABSTRACT",
            "content": "The Earths surface is constantly changing, and detecting these changes provides valuable insights that benefit various aspects of human society. While traditional change detection methods have been employed to detect changes from bi-temporal images, these approaches typically require expert knowledge for accurate interpretation. To enable broader and more flexible access to change information by non-expert users, the task of Change Detection Visual Question Answering (CDVQA) has been introduced. However, existing CDVQA methods have been developed under the assumption that training and testing datasets share similar distributions. This assumption does not hold in real-world applications, where domain shifts often occur. In this paper, the CDVQA task is revisited with focus on addressing domain shift. To this end, new multi-modal and multi-domain dataset, BrightVQA, is introduced to facilitate domain generalization research in CDVQA. Furthermore, novel state space model, termed Text-Conditioned State Space Model (TCSSM), is proposed. The TCSSM framework is designed to leverage both bi-temporal imagery and geo-disaster-related textual information in an unified manner to extract domain-invariant features across domains. Input-dependent parameters existing in TCSSM are dynamically predicted by using both bi-temporal images and geo-disaster-related description, thereby facilitating the alignment between bi-temporal visual data and the associated textual descriptions. Extensive experiments are conducted to evaluate the proposed method against state-of-the-art models, and superior performance is consistently demonstrated. The code and dataset will be made publicly available upon acceptance at https://github.com/Elman295/TCSSM. Keywords Visual Question Answering State Space Model Change Detection Domain generalization"
        },
        {
            "title": "Introduction",
            "content": "Change Detection (CD) is defined as the process of identifying regions of change from multi-temporal images captured over the same geographic area [1], and constitutes an increasingly active research area [2, 3, 4, 5]. Effective and accurate CD techniques can be utilized in wide range of applications such as urban development planning [6], natural disaster assessment [7], [8], environmental monitoring [9], and agricultural analysis [10]. CD methods can be categorized into two broad classes: The first is binary change detection, where regions are typically classified as changed or unchanged, and the second is semantic change detection, where changed areas are identified along with their specific change type, thus providing more informative outputs [11]. However, the interpretation of CD results typically requires expert knowledge, making it challenging for end-users [12]. To address this limitation, the Text-conditioned State Space Model For Domain-generalized Change Detection Visual Question Answering integration of Computer Vision and Natural Language Processing (NLP) has emerged as prominent research direction [13, 14, 15] and has given rise to various multi-modal tasks such as image captioning [16, 17], visual storytelling [18], visual question answering (VQA) [19, 20, 21], and visual dialog [22], where natural language is used to interpret visual content. In the context of CD, VQA offers powerful framework for enabling users to query specific changes in multi-temporal images using natural language, thus simplifying the interpretation process. This specialized application is referred to as CDVQA, which is subset of the broader Remote Sensing Visual Question Answering (RSVQA) framework. [21, 12, 23, 24]. However, so far most, if not all, CDVQA studies have been conducted under the assumption that the training and testing data are drawn from similar distributions [25]. This assumption is evidently unrealistic w.r.t real-world scenarios, where domain shifts between training and testing data are commonly encountered [26, 27]. Such shifts can lead to significant performance degradation during the evaluation and deployment phases. Moreover, various types of domain shifts may arise in CDVQA, including visual shift, question shift, and answer shift, among others [28]. Several strategies have been proposed to address domain shift, including Domain Adaptation (DA), Unsupervised Domain Adaptation (UDA), and Domain Generalization (DG). In DA settings, it is assumed that labeled data from the target domain are available during training [29, 30, 31]. In UDA scenarios, only unlabeled target domain images are accessible during training [32, 33, 34]. However, both DA and UDA rely on the availability of target domain data, which is often impractical in real-world applications where collecting (and annotating) data from the target domain is time-consuming and resource-intensive [27]. As more realistic alternative, DG assumes that no data nor labels from the target domain are available during training [35, 36, 37, 38]. Under this setting, models are expected to generalize to unseen domains solely based on the knowledge learned from source/training domain(s). In this study, we focus on the CDVQA task under DG setting. Specifically, geographical shift is considered, where bitemporal (preand post-event) images are analyzed from different countries. In order to properly validate the proposed domain generalized CDVQA approach, new dataset named BrightVQA is introduced. It has been constructed from the existing Bright dataset [39] especially for model evaluation in terms of generalization across diverse geographic regions. It contains eight distinct question types, approximately 2.1 million questionanswer pairs, and total of 62 unique answers, making it large-scale and diverse benchmark for evaluating RSVQA models under domain generalization settings. In addition, novel state space model, called Text-Conditioned State Space Model (TCSSM), is proposed. TCSSM extends traditional state space models (SSMs) by generating input-dependent parameters through the fusion of preand post-event imagery and geo-disaster-related textual descriptions. This conditioning mechanism allows the model to focus more effectively on domain-invariant features, as it leverages generic textual context to adaptively guide parameter estimation. The motivation for this design stems from recent findings [40] indicating that domain-invariant features in vision-language models (VLMs) can be better extracted by employing generic descriptive texts rather than instance-specific captions. To the best of our knowledge, this is the first study to investigate domain generalization in CDVQA and to propose dedicated dataset explicitly designed for this purpose. The main contributions of this article can be summarized as follows: 1. This is the first study to explore DG in the context of CDVQA. 2. An automatic VQA dataset generation pipeline is designed, resulting in the creation of the BrightVQA dataset. The proposed dataset includes eight question categories, 54,224 bi-temporal image pairs, and approximately 2.1 million questionanswer pairs from 9 different countries (10 different cities), providing large-scale and diverse benchmark for DG research. In comparison to existing CDVQA datasets, the proposed dataset encompasses greater variety of question types and incorporates two modalitiesRGB and SARfor preand post-event images. Furthermore, the dataset size is larger and samples are distributed globally across multiple countries. 3. novel SSM-based architecture, TCSSM, is proposed, in which bi-temporal imagery and geo-disasterrelated textual features are jointly utilized to predict input-dependent parameters. In contrast to traditional SSMswhere parameters are derived solely from visual or textual inputsTCSSM enables dynamic parameter generation through cross-modal conditioning. 4. To enhance generalization across diverse regions, geo-disaster-related textual cues are incorporated, allowing for text-aware representation learning. This integration facilitates deeper alignment between visual and textual modalities during feature extraction. The remainder of this article is organized as follows. Section 2 reviews the body of related work. Section 3 provides detailed overview of the proposed BrightVQA dataset. The methodology, including the design and implementation of the proposed TCSSM, is described in Section 4. Experimental settings, evaluation protocols, and ablation studies are presented in Section 5. Finally, conclusions are provided in Section 6. 2 Text-conditioned State Space Model For Domain-generalized Change Detection Visual Question Answering"
        },
        {
            "title": "2.1 Close-ended RSVQA",
            "content": "In the close-ended setting of RSVQA, the task is formulated as classification problem, where fixed and predefined set of possible answers is utilized. VQA for remote sensing applications was first introduced in [21], where two datasets - High-resolution and Low-resolution RSVQA - were proposed, along with baseline model consisting of four main components: vision backbone, question backbone, fusion module, and prediction head. ResNet-152 and simple Recurrent Neural Network (RNN) were employed as the vision and question backbones, respectively. The fusion of visual and textual features was performed using the Hadamard product. Next, the RSIVQA dataset was presented [24] and novel fusion model was proposed based on an attention mechanism. Moreover, the CDVQA task was proposed along with new dataset specifically designed for this problem in [12]. baseline model was also presented, which employed convolutional neural network (CNN) and RNN as the vision and question backbones, respectively. The model incorporated both multi-temporal fusion (across bi-temporal images) and multi-modal fusion (across different modalities), enabling it to effectively integrate spatial, temporal, and textual information. In another line of work [41], transformer-based architecture for both the encoder and decoder components was employed, where the roles of the key and value in the decoder were swapped, demonstrating that such modification can enhance the performance of VQA models. Furthermore, question augmentation strategy was proposed in [42] by translating and re-translating questions across multiple languages, thereby enriching the diversity of training samples. contrastive loss was also incorporated to improve model robustness. In separate study [43], self-paced learning strategy was presented for VQA, where model was trained progressively from easy to hard examples to extract more robust features. The use of object-level image features on the other hand was explored in [44], to improve model performance. In addition, Prompt-RSVQA [45], incorporated multi-label supervision and utilized DistilBERT [46] to better understand and answer visual questions from extracted multi labels. RSAdapter [47], was further studied, as lightweight adaptation module that allows models to be fine-tuned without training from scratch - this is especially useful when large-scale annotated datasets are unavailable. And last, new dataset along with semantic label-aware model were introduced in [48, 49], and were designed to focus on relevant visual regions during reasoning. Despite these advances, all aforementioned methods have been developed under the assumption that the training and testing data are drawn from similar distributions. As such, the challenge of domain shift remains unaddressed in prior RSVQA research."
        },
        {
            "title": "2.2 Open-ended RSVQA",
            "content": "In the open-ended setting of RSVQA, the task is approached as generative problem, wherein answers are produced in free-form natural language without restriction to predefined answer set. To address the scarcity of high-quality imagetext datasets in remote sensing, Yuan et al. [50] introduced human-annotated captioning and open-ended RSVQA dataset specifically designed to facilitate the development of large VLMs. Building upon this, Liu et al. [51] proposed the novel task of Adaptive Disaster Interpretation, which is aimed at managing complex and multi-faceted disaster-related requests through the sequential execution of interrelated interpretation tasks, thereby enabling comprehensive understanding of disaster scenarios. In related development, Kuckreja et al. [52] presented the first multitask conversational VLM tailored for remote sensing, capable of engaging in both image-level and region-specific dialogues. Zhao et al. [53] proposed unified framework that simultaneously addresses semantic segmentation, image captioning, and VQA on high-resolution remote sensing images. More recently, Zhang et al. [54] employed unified instruction tuning method to handle diverse remote sensing tasks including scene classification, image captioning, VQA, and visual grounding. However, current open-ended RSVQA methods have been developed under the assumption of in-distribution data, where training and testing samples share similar characteristics."
        },
        {
            "title": "2.3 Existing Datasets",
            "content": "Table 1: Comparison of the BrightVQA dataset against existing benchmark RSVQA datasets. The symbol highlights the absence of such information in the original paper. Dataset Total Images Total Questions Question Types Image Size LR RSVQA (TGRS20) HR RSVQA (TGRS20) RSVQAxBen (IGARSS21) RSIVQA(TGRS22) CDVQA (TGRS22) TextRS-VQA (JSTAR23) EarthVQANet (ISPRS24) 772 10,659 590,326 37,264 2,968 2,144 6, 77,232 1,066,316 14,075,801 111,134 122,000 6,245 208,593 BrightVQA (Ours) 54,224 2,168,960 4 4 2 3 4 4 6 256256 512512 120120 512512 256256 1,0241,024 256256 3 Countries Neatherland USA China China Modality Supports CD Supports DG RGB RGB RGB RGB RGB RGB RGB Congo, Equatorial Guinea, Haiti, Lebanon, Libya, Morocco, Spain, Türkiye, USA RGB, SAR Text-conditioned State Space Model For Domain-generalized Change Detection Visual Question Answering"
        },
        {
            "title": "2.3.1 Low Resolution RSVQA [21]",
            "content": "A Sentinel-2 based VQA dataset was constructed using imagery over the Netherlands, with 10-meter resolution RGB bands. total of nine low-cloud-cover tiles were selected and divided into 772 image patches of size 256 256 pixels (each covering 6.55 km2). From these, 77,232 questionanswer pairs were generated, covering four different question types: count, rural/urban classification, comparative, and presence."
        },
        {
            "title": "2.3.2 High Resolution RSVQA [21]",
            "content": "A high-resolution VQA dataset was developed using 15 cm RGB aerial imagery from the USGS High-Resolution Orthoimagery collection, which primarily covers urban areas across the United States. From this collection, 161 tiles from the North-East coast were selected and divided into 10,659 images of size 512 512 pixels (each covering 5,898 m2). total of 1,066,316 questionanswer pairs were generated, covering four different question categories: count, comparative, area, and presence."
        },
        {
            "title": "2.3.3 RSVQAxBen [55]",
            "content": "A Sentinel-2 based VQA dataset was constructed using imagery and land cover annotations from the BigEarthNet S2 dataset [56]. total of 590,326 image patches were selected, and for each patch, the 10-meter resolution RGB bands were extracted and rescaled from 12-bit to 8-bit depth. Utilizing these preprocessed patches, set of 25 diverse questions was generated per image, resulting in total of approximately 14,075,801 imagequestionanswer triplets. These triplets encompass variety of question types grounded in land cover semantics and spatial characteristics derived from the BigEarthNet labels, enabling robust multi-modal learning and evaluation within the remote sensing domain."
        },
        {
            "title": "2.3.4 RSIVQA [24]",
            "content": "The RSIVQA dataset was constructed using combination of five existing remote sensing datasets, with both automatically generated and human-annotated VQA triplets. In total, 111,134 VQA triplets were created, categorized into three answer types: yes/no, numerical, and other. portion of the dataset was enriched using human annotations based on the UCM dataset [57], with five questions annotated per image. For classification-based sources, one question per image was generated, while for datasets like DOTA [58] and HRRSD [59], the number of questions per image varied."
        },
        {
            "title": "2.3.5 CDVQA [12]",
            "content": "Unlike traditional VQA tasks, CDVQA requires reasoning over multi-temporal aerial imagery, making time-series analysis essential. To support this, Yuan et al. constructed CDVQA dataset based on the SECOND [60] semantic change detection dataset. SECOND [60] consists of bi-temporal high-resolution RGB aerial images acquired from various platforms and sensors, with spatial resolutions ranging from 0.5 to 3 m. The dataset covers several Chinese cities, including Shanghai, Hangzhou, and Chengdu. It contains 4,662 image pairs (512 512 pixels), of which 2,968 pairs are publicly available. Each pair includes preand post-event images, along with corresponding semantic change maps annotated at the pixel level. These maps label both unchanged regions and six change-related land-cover classes: non-vegetated ground, buildings, playgrounds, water, low vegetation, and trees."
        },
        {
            "title": "2.3.6 TextRS-VQA [61]",
            "content": "A RSVQA dataset was constructed based on images from the TextRS dataset [62], originally introduced for remote sensing image retrieval and captioning tasks. The dataset comprises 2,144 images collected from four widely used scene classification datasets: AID [63], PatternNet [64], UC-Merced [65], and NWPU [66], each offering varying spatial resolutions and image dimensions. From each dataset, 16 random images per class were selected to ensure balanced representation. Each image was manually annotated with two to five questions, resulting in total of 6,245 questions spanning four categories: object counting, presence/absence, class type, and diverse other category. The questions were specifically crafted to reflect realistic remote sensing use cases."
        },
        {
            "title": "2.3.7 EarthVQANet [48]",
            "content": "The EarthVQA dataset was developed as an extension of the LoveDA dataset [67], which covers 18 urban and rural regions across Nanjing, Changzhou, and Wuhan. LoveDA provides 5,987 high-spatial-resolution (HSR) images along with semantic masks for seven common land-cover classes. EarthVQA introduces three major enhancements: (1) an expansion in dataset size by adding 13 new samples (8 urban and 5 rural), increasing the total to 6,000 WorldView-3 images at 0.3 resolution; (2) refinement of semantic labels, including the addition of playground class and correction of annotation errors; and (3) incorporation of 208,593 questionanswer pairs to enable visual question 4 Text-conditioned State Space Model For Domain-generalized Change Detection Visual Question Answering answering for city planning applications. Each urban image is paired with 42 question-answer pairs, and each rural image with 29 question-answers. Table 1 summarizes the characteristics of existing datasets in comparison to the proposed dataset."
        },
        {
            "title": "3 BrightVQA dataset",
            "content": "Figure 1: Examples of the proposed BrightVQA dataset. T1 and T2 stands for preand post-event images, respectively. Different from the RSVQA task, CDVQA requires analysis of multi-temporal images. To address this, the existing semantic change detection dataset Bright [39] is chosen as the basis for automatically generating CDVQA dataset called BrightVQA  (Fig. 1)  . The Bright dataset [39] comprises optical pre-event and SAR post-event images with spatial resolutions ranging from 0.3 to 1 meter. Bi-temporal images and corresponding masks are collected from multiple countries, making the dataset suitable for DG task. Geographical locations include Congo, Equatorial Guinea, Haiti, Japan, Lebanon, Libya, Mexico, Morocco, Myanmar, Spain, Türkiye, Ukraine, USA. Due to data availability constraints, nine countries were used in this work  (Table 1)  . The dataset contains 4,246 pairs of multi-modal images, each with size of 1,024 1,024 pixels. For computational efficiency, images were resized to 256 256 pixels. single semantic mask is provided per image pair, containing four human-verified labels: intact, destroyed, damaged, and background. For VQA generation purposes, the BrightVQA dataset contains exactly 40 questionanswer pairs per image, automatically constructed based on the available imagemask information. These questions are categorized into eight semantic types: damage detection, quantitative, comparative, severity, spatial, contextual, threshold, and recovery assessment."
        },
        {
            "title": "3.1 Multitemporal Question–Answer pairs Construction",
            "content": "Formally, in each pair of multitemporal and multimodal images, let xpre R3HW be the optical image at the pre-event time, and xpost R1HW be the Synthetic Aperture Radar (SAR) image at the post-event time, and denote the spatial height and width of the images, respectively. Each image pair is associated with single semantic mask RHW , in which each pixel denotes one of four semantic change classes: 0 = background, 1 = intact, 2 = damaged, and 3 = destroyed. These masks are human-verified and describe the semantic state change of regions from pre-event to post-event."
        },
        {
            "title": "3.1.1 Damage Detection",
            "content": "This category aims to identify the presence of damage, destruction, or intact structures within the image. The number of pixels corresponding to specific label is computed as Nl = (cid:80)H j=1 1(si,j = l), where 1() is the indicator function. For each label of interest, binary (Yes/No) answer is generated based on whether Nl > 0. For example, the presence of damaged structures is indicated by: (cid:80)W i=1 Answer = (cid:26)Yes, No, if N2 > 0, otherwise. (1) total of six questions are generated in this category, focusing on the detection of intact, damaged, or destroyed buildings, as well as general structural damage evidence. 5 Text-conditioned State Space Model For Domain-generalized Change Detection Visual Question Answering"
        },
        {
            "title": "3.1.2 Quantitative",
            "content": "This category quantifies the extent of damage, destruction, or intact areas as percentages. Let the total number of pixels in the semantic mask be Ntotal = W. The percentage of each class relative to the entire image is calculated as (cid:22) Pl = 100 (cid:23) ,"
        },
        {
            "title": "Nl\nNtotal",
            "content": "(2) Additionally, building-specific percentages are computed relative to total building pixels, defined as Nbuilding = N1 + N2 + N3, and (cid:22)"
        },
        {
            "title": "P building",
            "content": "l = 100 (cid:23)"
        },
        {
            "title": "Nl\nNbuilding",
            "content": ". (3) The percentages are then converted into categorical intervals (e.g., 010%, 1020%) for answer generation. total of eight questions focus on both overall and building-specific proportions."
        },
        {
            "title": "3.1.3 Comparative",
            "content": "This category compares the prevalence of different destruction types or intact versus affected areas. The dominant destruction type between damaged and destroyed is determined by comparing N2 and N3: Answer = (cid:26)Damaged, if N2 > N3, Destroyed, otherwise. (4) Similarly, the dominance between intact and affected areas is assessed by comparing intact pixels N1 to the total damaged pixels N2 + N3: Answer = (cid:26)Intact, if N1 > N2 + N3, Affected, otherwise. total of six questions are generated focusing on these comparative assessments."
        },
        {
            "title": "3.1.4 Severity",
            "content": "This category assesses the overall severity (os) of destruction in the scene based on the proportion of affected areas. The damage percentage is computed as The severity level is then determined based on predefined thresholds: os(%) = 100 N2 + N3(destruction) Ntotal Severity = No damage, Minor damage, Moderate damage, Severe damage, Extensive damage, if os = 0, if 0 < os < 10, if 10 os < 30, if 30 os < 60, if os 60. (6) (7) To validate these thresholds, The class distribution across the dataset was analyzed. Let fi be the frequency of samples in severity class Ci: {d : range(Ci)} where is the set of all destruction percentages in the dataset. fi = The resulting distribution shows reasonable balance with frequencies = {0.18, 0.22, 0.28, 0.21, 0.11} for the five severity classes. This balance is quantified using the imbalance ratio: max(fi) min(fi) This indicates acceptable class balance, as perfectly balanced classes would yield IR = 1, while highly imbalanced datasets typically exhibit IR > 10 [68], confirming that our thresholds create well-distributed severity classes. = 2.55 IR = (9) total of four questions are generated to evaluate the level of severity, including the overall damage extent and whether the area is considered widely or catastrophically affected. 6 (5) (8) Text-conditioned State Space Model For Domain-generalized Change Detection Visual Question Answering"
        },
        {
            "title": "3.1.5 Spatial",
            "content": "This category evaluates the spatial distribution of destruction within the image, distinguishing between concentrated and widespread destruction patterns. binary destruction mask is created by selecting pixels labeled as damaged or destroyed maskdestruction = (mask = 2) (mask = 3). Let the coordinates of destructed pixels be denoted as {(xi, yi)}Nd i=1. The Euclidean distances of these points from their centroid (x, y) are computed and compared to their standard deviation σ. (cid:112)(xi x)2 + (yi y)2 < σ. If more than 70% of the destructed pixels fall within this threshold, the distribution is labeled as concentrated in one area; otherwise, it is considered spread throughout. (10) The 70% threshold yields balanced distribution with 52% of samples classified as concentrated and 48% as spread throughout ensuring adequate representation of both damage patterns in the BrightVQA dataset. total of two questions are generated to assess the spatial damage distribution."
        },
        {
            "title": "3.1.6 Contextual",
            "content": "This category provides disaster-specific insights, focusing on the type and severity of structural destruction. The proportion of intact buildings is used as proxy for structural resilience: = N1 N1 + N2 + N3 (11) total of 4 questions are generated to evaluate the contextual aspect of the disaster."
        },
        {
            "title": "3.1.7 Threshold Questions",
            "content": "This category assesses the extent of damage in an area by asking binary (yes/no) questions based on destruction percentage (pdestruction). These questions help determine whether the damage exceeds or falls below specific critical levels. For each question, predefined list of thresholds is used: 5%, 10%, 25%, 50%, 75%, and 90%. The actual damage percentage is calculated as: For each threshold , the answer is computed as: pdestruction = N2 + N3 Ntotal 100 Answer(T ) = Yes, No, Yes, No, if {5, 10} and pdestruction < if {5, 10} and pdestruction if {25, 50, 75, 90} and pdestruction > if {25, 50, 75, 90} and pdestruction (12) (13) total of 6 questions are generated to evaluate the threshold questions."
        },
        {
            "title": "3.1.8 Recovery Assessment",
            "content": "This category evaluates the necessity for reconstruction or emergency services based on the extent of destruction. Let pdestruction denote the destruction percentage. Reconstruction need is indicated if pdestruction > 40%. Four questions evaluate reconstruction needs, emergency service requirements, and habitability. The 40% threshold yields moderately balanced distribution, with 49% of samples classified as reconstruction needed and 51% as no reconstruction."
        },
        {
            "title": "3.2 Question Distributions",
            "content": "The distribution of question categories in the BrightVQA dataset is presented in Table 2. The dataset includes eight distinct question types, each designed to probe different aspect of reasoning within the context of disaster-related visual information. Among them, quantitative questions dominate the distribution, accounting for 433,792 instances. These questions typically involve numerical reasoning, such as estimating damage percentages or comparing the extent of destruction. This is followed by damage detection, comparative, and threshold categories, each contributing 325,344 questions. These categories often require binary or ordinal reasoning, such as detecting whether damage occurred, comparing severity levels, or determining if damage exceeds certain threshold. 7 Text-conditioned State Space Model For Domain-generalized Change Detection Visual Question Answering Table 2: Question Category Distribution of BrightVQA"
        },
        {
            "title": "Question Category\nQuantitative\nDamage Detection\nComparative\nThreshold\nSeverity\nContextual\nRecovery Assessment\nSpatial",
            "content": "Number of Questions 433,792 325,344 325,344 325,344 216,896 216,896 216,896 108,"
        },
        {
            "title": "Total",
            "content": "2,168,"
        },
        {
            "title": "4 Text-Conditioned State Space Model",
            "content": "In this study, the CDVQA task is formulated as classification problem, consistent with other state-of-the-art (SOTA) approaches [21, 24, 48, 49, 47]. During both training and evaluation phases, only bi-temporal images, question-answer pairs, and geo-disaster-related descriptions are utilized; segmentation masks are not employed. As illustrated in Fig. 2, the proposed TCSSM model receives question, geo-disaster-related description, and bi-temporal images as inputs, and produces predicted answer as output. Figure 2: The figure illustrates the TCSSM model, which is designed to receive bi-temporal images, corresponding questions, and geo-disaster-related textual descriptions as input, based on which an answer is predicted. Specifically, the architecture comprises four main components. First, vision backbone is used to extract deep features from the bi-temporal images by using CNN. Second, question backbone based on the Bidirectional Encoder Representations from Transformers (BERT) [69] model is employed to encode the semantic representation of the input question. Third, the TCSSM module is designed to extract domain-invariant features by leveraging the geo-disaster description and bi-temporal images to predict input-dependent parameters existing in the SSM. Finally, multi-layer perceptron (MLP) head is used to classify the final answer."
        },
        {
            "title": "4.1 Vision Backbone",
            "content": "The vision backbone is designed to extract high-level features from heterogeneous bi-temporal remote sensing images, where the pre-disaster image is an optical RGB image and the post-disaster image is SAR image. Due to the significant modality gap between optical and SAR imagery, each image is processed independently using the same CNN architecture. This parallel design allows the network to learn modality-specific features while preserving architectural consistency for downstream fusion. For the SAR images, the single-channel data were replicated across all three channels, resulting in three-channel input. The RGB image already has 3 channels. Each image is then passed through the same CNN architecture consisting of four convolutional blocks. Each block contains two convolutional 8 Text-conditioned State Space Model For Domain-generalized Change Detection Visual Question Answering (a) Selection Mechanism in Mamba (b) Selection Mechanism in TCSSM (our proposed model) Figure 3: Selection Mechanisms in Mamba and TCSSM. layers with kernel size 3 3, followed by batch normalization [70] and ReLU activation. After each convolutional block, max-pooling with kernel size and stride of 2 is applied to reduce spatial resolution. The number of channels is progressively increased through the layers as 3 16 32 64 128, enabling the extraction of both fine-grained and abstract representations. This separate yet symmetric processing scheme prepares both modalities for effective cross-temporal comparison and subsequent integration within the model."
        },
        {
            "title": "4.2 Question Backbone",
            "content": "The question backbone is responsible for extracting contextualized representations from natural language questions. In this work, the BERT model is adopted as the core component for question understanding, due to its strong performance in capturing semantic and syntactic information. Specifically, pre-trained BERT-base model is utilized to encode each question into sequence of contextualized token embeddings. To tailor BERT for the disaster reasoning task, additional task-specific layers are appended to its output. These layers include fully connected projection layer followed by ReLU activation function, enabling the transformation of BERTs high-dimensional output into feature space compatible with other modalities. This extension allows the model to align question semantics with TCSSM outputs."
        },
        {
            "title": "4.3.1 Motivation",
            "content": "As noted in [40], domain-invariant features in VLMs can be extracted by leveraging generic descriptive text. For instance, the use of such generic descriptions in place of image-specific captions within frameworks like CLIP [71] has been shown to enhance the generalization capabilities of the model. In the context of the VQA task, however, the questions themselves cannot be relied upon for extracting domain-invariant information, as they typically lack explicit and transferable semantic content about the visual input. This limitation is particularly evident in disaster scenarios such as those represented in the BrightVQA dataset, where the identification of appropriate generic descriptions poses unique challenge. To address this, two-pronged strategy was adopted: first, geographical information about the affected country was collected; second, general textual descriptions of the disaster type were gathered. Both sources of information were obtained from openly accessible platform, Wikipedia, ensuring universal availability and reproducibility. Subsequently, the Grok LLM was employed to merge these two categories of textual data into unified, coherent description. During this process, the model was also prompted to identify and correct any inconsistencies or errors within the merged text. No issues were flagged by Grok, and the final output was used as the generic descriptive text for conditioning the model. It was assumed that domain-invariant features in the disaster VQA task could be extracted by utilizing geo-disaster-related textual descriptions. To address the challenge of aligning images with descriptive text in domain-generalized setting, novel approach termed TCSSM has been proposed. While traditional methods, such as contrastive loss frameworks like CLIP [71], align modalities through learned similarity metrics, the TCSSM introduces more integrated fusion mechanism. In the standard SSM, parameters including δ, B, and are typically derived solely from visual input. However, in the TCSSM, these parameters are dynamically generated by conditioning on both the image and its corresponding descriptive text. By incorporating geo-disaster-related descriptions during the visual encoding process, the model is made contextually text-aware. This cross-modal conditioning is designed to guide the network toward capturing domain-invariant representations, thereby enhancing generalization in VQA tasks across varying location scenarios. 9 Text-conditioned State Space Model For Domain-generalized Change Detection Visual Question Answering"
        },
        {
            "title": "4.3.2 SSM",
            "content": "Mamba [72] is inspired by linear time-invariant (LTI) systems, where an input sequence xt is processed via hidden state ht to produce an output yt using set of linear ordinary differential equations. These dynamics are governed by matrices A, B, C. To enable efficient discrete computation, Mamba applies the zero-order hold method for discretization, resulting in the recurrence form: (cid:26)ht = Aht1 + Bxt yt = Cht (14) where RN is the evolution parameter, and RN 1, R1N , are the projection parameters. Unlike classical LTI systems, Mamba generates the parameters B, dynamically based on the input sequence (Fig. 3a), allowing the model to adapt its computations in an input-dependent manner and improving its ability to model complex temporal patterns."
        },
        {
            "title": "4.3.3 TCSSM",
            "content": "We adopted the Change State Space Model [73] (Eq. (15)), which demonstrated the capability to focus on relevant features in bi-temporal images by computing the L1 distance between postand pre-event features. (cid:26)ht = Aht1 + Bx = Cht yt = Cht, Bxt (15) where RN is the evolution parameter, and RN 1, RN 1, R1N , R1N are the projection parameters, and . stands for L1 distance. In our proposed TCSSM, the prediction of input-dependent parameters was modified by incorporating multi-modal features (Eq. (16) and Eq. (17)). These features are aligned with geo-disaster-related textual descriptions (Fig. 3b), enabling the extraction of domain-invariant representations shared across different geographical domains. B, , δ = (Fpre Fpost Ftext) + Fpost (16) B, C, δ = (Fpre Fpost Ftext) + Fpre (17) In Equations (16) and (17), Fpre and Fpost denote the visual features from preand post-event images respectively, while Ftext refers to the geo-disaster-related text features. The expressions for B, C, δ and B, , δ represent estimated parameters. The symbol = here denotes that these parameters are predicted through the feature interaction, not mathematically identical. Moreover, denotes the Hadamard product, which has been predominantly used in multimodal fusion due to its ability to capture non-linear relationships between different modalities while maintaining linear computational complexity [74]. Each block receives three inputspre-event and post-event image features, as well as geo-disaster-related textual embeddingsand processes them using sequence of linear projections, depthwise convolutions, and SiLU activation functions."
        },
        {
            "title": "4.4 Answer Prediction Backone",
            "content": "For answer prediction, simple MLP was employed to predict the correct answer from predefined set of 62 possible answer classes present in the dataset."
        },
        {
            "title": "4.5 Loss Function",
            "content": "To train the answer prediction model, we employed the categorical cross-entropy loss [75], which is widely used for multi-class classification problems. This loss function measures the dissimilarity between the predicted probability distribution and the true distribution (i.e., the ground-truth class label encoded as one-hot vector). It penalizes confident but incorrect predictions more heavily, encouraging the model to assign higher probabilities to the correct class. (cid:88) LCE = yi log(ˆyi) (18) where is the total number of classes (in our case, 62), yi is the ground-truth label (1 for the correct class and 0 otherwise), and ˆyi is the predicted probability for class i. The loss is minimized when the predicted probability for the true class approaches 1. i=1 10 Text-conditioned State Space Model For Domain-generalized Change Detection Visual Question Answering"
        },
        {
            "title": "5.1 Dataset",
            "content": "To construct the CDVQA dataset used in this work, we adapt the existing Bright semantic change detection dataset [39], resulting in new dataset named BrightVQA. The Bright dataset consists of pre-event optical and post-event SAR images with spatial resolutions between 0.3 and 1 meter, collected from various countries. For this study, data from nine countries are used, comprising 4,246 image pairs, each of size 1, 024 1, 024 pixels (resized to 256 256 for computational efficiency). Each pair is annotated with single semantic mask containing four human-verified classes: intact, damaged, destroyed, and background. To enable question answering, 40 automatically generated questionanswer pairs are provided per image, categorized into eight types: damage detection, quantitative, comparative, severity, spatial, contextual, threshold, and recovery assessment. 5."
        },
        {
            "title": "Implementation Details",
            "content": "Our experiments were implemented using PyTorch and conducted on system equipped with NVIDIA GEFORCE RTX 4090 GPU. All models were trained using the Adam optimizer [76] with an initial learning rate of 1e-4, weight decay of 1e-5, and batch size of 32. The learning rate was decayed by factor of 0.1 every 5 epochs."
        },
        {
            "title": "5.3 Evaluation Metrics",
            "content": "To ensure comprehensive assessment of model performance in the context of CDVQA, three widely recognized evaluation metrics were employed: overall accuracy (OA), average accuracy (AA), and F1-score. The mathematical definitions of these metrics are as follows: Precision = P + Recall = P + Precision Recall Precision + Recall + P + + + F1-score = 2 OA = AA ="
        },
        {
            "title": "1\nQ",
            "content": "Q (cid:88) i=1 Pi Pi + Ni (19) (20) (21) (22) (23) Here, , , , and denote the number of true positives, true negatives, false positives, false negatives, and the number of question categories, respectively."
        },
        {
            "title": "5.4 Performance Comparison",
            "content": "To evaluate the effectiveness of the proposed TCSSM model, extensive comparisons were conducted against broad spectrum of recent RSVQA and CDVQA methods: RSVQA [21], RSIVQA [24], CDVQA [12], BiModal [41], SOBA [49], RSADapter [47], and EarthVQANet [48]. Moreover, as the aforementioned methods were not originally designed for CDVQA tasks, uniform evaluation protocol was adopted to ensure fairness. Specifically, for all models (except those indicated with +), the preand post-event images were concatenated along the channel dimension and fed into the visual backbone in consistent manner. In total, ten different regions were considered in the experiments, each corresponding to distinct target domain: Bata, Beirut, Goma, Les Cayes, Hawaii, La Palma, Derna, Marshall, Moulay Brahim, and Antakya. For example, in the case where Bata serves as the target domain, the remaining regionsBeirut, Goma, Les Cayes, Hawaii, La Palma, Derna, Marshall, Moulay Brahim, and Antakyaare used as source domains. For all models, the training and validation splits were derived exclusively from the source domains. To ensure consistency throughout the analysis, the region names were used as labels in Table 3. As demonstrated in Table 3, the proposed methods, TCSSM and its CDVQA variant TCSSM+, consistently outperform wide range of SOTA approaches across ten diverse geographic regions. 11 Text-conditioned State Space Model For Domain-generalized Change Detection Visual Question Answering Table 3: Performances across 10 regions in terms of OA, AA, and F1-score. The best result per region is highlighted in red, and the second-best in blue. Location Metric RSVQA (TGRS20) RSIVQA (TGRS22) CDVQA (TGRS22) BiModal (TGRS22) SOBA (AAAI24) RSAdapter (TGRS24) EarthVQANet (ISPRS24) TCSSM TCSSM+ (Ours) (Ours) Bata Beirut Goma Les Cayes Hawaii"
        },
        {
            "title": "Average\nAcross\nRegions",
            "content": "OA(%) AA (%) F1(%) OA(%) AA (%) F1(%) OA(%) AA (%) F1(%) OA(%) AA (%) F1(%) OA(%) AA (%) F1(%) OA(%) AA (%) F1(%) OA(%) AA (%) F1(%) OA(%) AA (%) F1(%) OA(%) AA (%) F1(%) OA(%) AA (%) F1(%) OA(%) AA (%) F1(%) 82.63 78.00 26. 89.02 90.58 28.78 89.90 91.30 29.92 83.73 85.91 27.87 70.19 70.59 22.73 87.45 88.56 29.05 80.46 81.29 27. 86.49 88.19 28.84 80.49 84.62 24.65 82.64 83.10 25.92 83.30 84.21 27.15 82.76 79.01 26.74 89.54 89.92 29. 90.12 91.66 30.55 81.54 85.04 25.66 70.58 69.65 24.67 87.66 88.06 29.24 79.94 79.52 27.30 86.43 88.64 29. 83.67 85.97 25.32 83.69 80.27 26.18 83.59 83.77 27.38 84.53 86.59 25.66 86.76 87.79 24.87 84.99 87.57 24. 86.12 88.49 24.34 66.08 66.60 18.98 79.94 82.31 24.11 76.75 78.58 23.46 86.26 87.94 27.55 80.08 84.55 23. 73.42 72.61 22.11 80.49 82.30 23.87 84.96 86.81 24.47 84.37 85.10 22.51 85.34 84.70 24.88 76.91 78.02 21. 68.78 65.13 20.09 80.67 82.82 23.49 77.37 76.75 22.84 85.83 86.59 24.94 81.20 83.07 22.84 74.81 75.30 22. 80.02 80.42 22.94 83.12 81.30 27.37 90.14 90.70 29.47 91.24 92.59 31.97 85.14 86.17 29.22 72.23 70.20 27. 88.05 88.34 30.12 80.35 79.94 28.14 87.86 89.32 30.48 85.31 88.25 27.38 84.17 84.20 27.11 84.76 85.10 28. 86.47 87.95 28.70 91.33 92.61 30.64 92.45 93.10 31.86 91.88 91.65 31.74 70.25 71.45 24.15 82.70 83.50 25. 80.14 81.14 27.47 92.13 91.46 31.86 91.67 92.80 29.69 88.40 89.26 30.10 86.74 87.49 29.15 83.28 83.38 27. 91.47 91.25 30.19 92.67 93.38 32.42 86.71 87.55 30.00 72.38 70.76 27.90 88.68 88.63 30.68 80.54 80.91 28. 88.24 89.98 31.46 86.17 89.16 29.49 84.78 84.90 28.25 85.49 85.99 29.70 86.83 88.44 29.53 91.64 92.74 32. 94.56 94.90 33.92 94.24 90.23 33.20 71.93 69.65 27.47 89.39 90.05 31.39 80.47 80.76 29.71 91.23 91.30 32. 85.99 94.33 28.22 85.42 86.78 30.85 87.17 87.91 30.89 86.87 85.94 29.66 92.31 92.38 34.00 94.21 94.53 34. 93.03 90.72 35.10 72.91 70.96 29.76 88.10 88.74 31.92 80.24 79.89 32.63 92.47 92.75 33.99 88.70 96.03 31. 88.05 89.12 32.01 87.68 88.10 32.50 Overall Performance: On average, TCSSM+ achieves the highest OA of 87.68%, surpassing all competing models including established CNN-based methods such as RSVQA and RSIVQA, Transformer-based methods like SOBA and EarthVQANet, and other recent frameworks like RSAdapter. The original TCSSM also demonstrates strong performance, ranking second with an OA of 87.17%. In terms of AA, which measures the mean accuracy across different question categories and better reflects balanced performance, TCSSM+ again leads with 88.10%, followed closely by TCSSM at 87.91%. Similarly, in terms of F1-scorewhich balances precision and recall and is crucial for imbalanced class distributionsTCSSM+ attains the best average score of 32.50%, while TCSSM follows closely at 30.89%. These results indicate that the proposed models not only achieve high OA but also maintain robust and balanced detection capabilities across classes. Region-wise Analysis: The superiority of the proposed methods is evident across multiple challenging locations. For instance, in Bata, both TCSSM and TCSSM+ deliver the highest OA, AA, and F1-scores, with TCSSM+ slightly outperforming TCSSM (86.87% vs. 86.83% OA, 85.94% vs. 88.44% AA, and 29.66% vs. 29.53% F1). In Beirut, the gap widens, with TCSSM+ achieving an OA of 92.31%, an AA of 92.38%, and an F1-score of 34.00%, outperforming the best baseline EarthVQANet by approximately 0.8% in OA, 1.1% in AA, and nearly 4% in F1-score. In Goma, TCSSM attains the highest OA at 94.56%, while TCSSM+ leads in AA (94.53%) and F1-score (34.12%). Notably, in the Les Cayes region, TCSSM outperforms all other models with an OA of 94.24% and an AA of 90.23%, while TCSSM+ records the best F1-score of 35.10%, highlighting its exceptional ability to handle complex disaster-affected environments. In more challenging cases such as Hawaii, where baseline models achieve lower scores, both TCSSM and TCSSM+ demonstrate clear improvements, with TCSSM+ reaching an OA of 72.91%, an AA of 70.96%, and an F1-score of 29.76%, outperforming the next best model EarthVQANet by nearly 1% in accuracy, 0.2% in AA, and 1.8% in F1-score. 12 Text-conditioned State Space Model For Domain-generalized Change Detection Visual Question Answering Similar trends can be observed for regions like La Palma, Marshall, and Moulay Brahim, where the proposed methods substantially exceed prior results across all three metrics. Comparison to Recent Methods: While models like RSAdapter and EarthVQANet represent the closest competitors, especially in some specific regions (e.g., Derna and Moulay Brahim), TCSSM+ consistently maintains an edge in all metrics across the majority of regions. The proposed methods leverage multi-temporal and multi-modal features effectively, enabling improved domain generalization and feature alignment compared to models primarily based on CNN or Transformer backbones."
        },
        {
            "title": "5.5 Visualizations",
            "content": "To qualitatively evaluate model performance, several representative VQA cases were analyzed. range of disasterrelated questions was posed, and the responses generated by various SOTA models were compared against the ground truth. As shown in the examples  (Fig. 4)  , models such as RSVQA, RSIVQA, CDVQA, BiModal, SOBA, RSAdapter, and EarthVQANet were benchmarked. It was observed that many models provided inconsistent or incorrect answers, particularly in scenarios requiring fine-grained damage assessment. In contrast, responses generated by the proposed TCSSM+ model demonstrated closer alignment with ground truth annotations, especially for questions involving percentage estimation of partial damage and binary judgments about area integrity. These results suggest that improved domain alignment and multi-modal understanding were successfully facilitated by the proposed approach. Figure 4: Qualitative comparison of VQA model responses across various disaster-related questions. T1 and T2 stands for preand post-event images, respectively."
        },
        {
            "title": "5.6.1 Cross-Dataset Evaluation",
            "content": "To assess the performance of the proposed method across diverse scenarios, the EarthVQANet dataset was selected due to its suitability for domain generalization tasks, featuring images from various regions including both rural and urban areas. Specifically, the model was evaluated on the urban management scenario to rigorously test its robustness in complex, real-world environments. Since geo-disaster-related descriptions are not available for this dataset, generic textual descriptions of Chinese rural and urban areas were utilized, which were obtained from Wikipedia. Moreover, as the task pertains to RSVQA, the TCSSM+ model was not employed for this dataset. As shown in Table 4, the proposed TCSSM method demonstrates superior performance in cross-dataset evaluation scenarios, which are critical for assessing model robustness under domain shift conditions. Two challenging settings were considered: training on urban data and testing on rural data (Urban to Rural), and vice versa (Rural to Urban). In the Urban to Rural setting, TCSSM achieved the highest OA of 73.65% and an F1-score of 24.29%, outperforming all compared methods. Notably, it surpassed the previous best model, RSAdapter, by approximately 1% in OA and 1.5% in F1-score. Other competitive approaches such as SOBA and EarthVQANet scored lower, demonstrating the challenge of generalizing from urban to rural domains. This result underscores the effectiveness of TCSSM in capturing domain-invariant features for urban management scenario. 13 Text-conditioned State Space Model For Domain-generalized Change Detection Visual Question Answering Table 4: Cross-dataset Evaluation. OA (%) Method Setting F1 (%) Urban to Rural Rural to Urban RSVQA (TGRS20) RSIVQA (TGRS22) BiModal (TGRS22) SOBA (AAAI24) RSAdapter (TGRS24) EarthVQANet (ISPRS24) TCSSM (Ours) RSVQA (TGRS20) RSIVQA (TGRS22) BiModal (TGRS22) SOBA (AAAI24) RSAdapter (TGRS24) EarthVQANet (ISPRS24) TCSSM (Ours) 68.49 67.49 66.34 70.34 72.66 70.85 73.65 58.63 57.42 59.87 63.27 72.49 64.23 71.95 20.65 20.36 19.23 21.79 22.81 21.90 24.29 18.71 17.24 18.69 19.86 24.12 21.36 23.88 In the Rural to Urban scenario, RSAdapter achieved the highest OA (72.49%) and F1-score (24.12%), narrowly outperforming TCSSM, which obtained an OA of 71.95% and F1-score of 23.88%. Despite this slight difference, TCSSM still delivered competitive results, significantly outperforming earlier methods such as RSVQA, RSIVQA, BiModal, SOBA, and EarthVQANet by large margin in both metrics. This illustrates that TCSSM maintains strong generalization ability even when adapting from rural to urban domains, which are often characterized by distinct spatial patterns and object distributions."
        },
        {
            "title": "5.6.2 Question-based Performance Comparison",
            "content": "Table 5: Question-based performance comparison for the Bata region. Location Category RSVQA (TGRS20) RSIVQA (TGRS22) CDVQA (TGRS22) BiModal (TGRS22) SOBA (AAAI24) RSAdapter (TGRS24) EarthVQANet (ISPRS24) TCSSM TCSSM+ (Ours) (Ours) Bata Comparative (%) Contextual (%) Damage Detection (%) Quantitative (%) Recovery Assessment (%) Severity (%) Spatial (%) threshold (%) AA (%) OA (%) 76.83 78.60 76.53 63.55 80.26 81.32 82.83 84.10 78.00 82.63 77.65 79.24 78.32 64.84 80.37 82.41 83.96 85.29 79.01 82.76 78.07 80.83 81.17 67.54 99.78 97.69 88.42 99.23 86.59 84.53 80.31 80.83 81.17 68.03 99.71 97.69 88.10 98.71 86.81 84. 78.34 80.24 78.13 65.76 86.36 87.81 84.52 89.30 81.30 83.12 79.48 80.97 83.01 74.76 99.78 97.65 88.62 99.35 87.95 86.47 78.88 81.43 79.64 66.05 89.75 91.27 86.74 93.28 83.38 83.28 80.34 83.26 82.78 76.07 99.81 97.61 88.30 99.37 88.44 86.83 80.06 66.19 83.35 72.72 99.83 97.57 88.40 99.43 85.94 86.87 Table 5 presents the question category-based performance comparison of various models on the BrightVQA dataset, focusing exclusively on the Bata region. The ablation study was conducted only for Bata, as similar performance trends were observed across all regions; thus, only Batas results are reported for conciseness. The table reports accuracy percentages for diverse question categoriesComparative, Contextual, Damage Detection, Quantitative, Recovery Assessment, Severity, Spatial, and Thresholdacross multiple SOTA methods including RSVQA, RSIVQA, CDVQA, BiModal, SOBA, RSAdapter, EarthVQANet, and the proposed TCSSM and TCSSM+ models. For instance, in the Recovery Assessment category, TCSSM+ attains the highest accuracy of 99.83%, slightly surpassing TCSSM with 99.81%. Similarly, in the Damage Detection category, TCSSM+ achieves 83.35% accuracy, outperforming the second-best RSAdapter at 83.01%. The AA and OA metrics further confirm the superior performance of the proposed models, with TCSSM reaching 88.44% AA and 86.87% OA, both marking the highest scores among the compared methods. These results demonstrate the effectiveness and robustness of the proposed models in handling diverse question categories within the CDVQA task."
        },
        {
            "title": "5.6.3 Language Bias",
            "content": "To investigate the potential impact of language bias in the proposed model, an ablation study was conducted by selectively removing key componentsnamely, the question input, vision inputs, and the geo-disaster-related description. Through this setup, thorough analysis was enabled regarding the relative importance and contribution of each input type, as well as the effects of their combinations on model performance. The results are presented in Table 6, where detailed insights into performance variations under different input configurations are provided. Moreover, this ablation was conducted for other regions as well; however, due to the similarity in observed patterns, only the results for the Bata region are presented in the paper. 14 Text-conditioned State Space Model For Domain-generalized Change Detection Visual Question Answering Table 6: Effect of language bias on TCSSM+ performance (in %) for the Bata region. Question Vision Description Recall Prec. OA F1 26.48 10.12 12.36 27.45 26. 22.32 7.65 10.57 22.66 22.49 81.43 36.05 40.24 82.17 81.68 24.22 8.71 11.39 24.82 24.34 29.76 29.58 86. 29.66 As shown in Table 6, the configuration in which only the question input was retained resulted in relatively high OA of 81.43%, with moderate recall and precision. This suggests that correct answers could be produced by relying primarily on question semantics, indicating the presence of language biasthat is, the ability to answer questions accurately without utilizing visual or descriptive inputs. This observation was also substantiated in [77], where it was demonstrated that the majority of VQA models predominantly rely on textual information from the questions rather than visual content. In contrast, when only vision or vision plus description modalities were used without the question, sharp decline in performance was observed, particularly in recall and F1-score. These results confirm that, in the absence of the question input, accurate predictions could not be reliably generated. Notably, when the question was combined with description but vision was excluded, slight improvement was observed over the question-only setting, with the F1-score increasing to 24.82%. This indicates that textual descriptions can provide complementary context when visual features are unavailable. Further gains were achieved when the question and vision were combined, excluding the description, resulting in an F1-score of 24.34%. This suggests that visual features were better exploited when paired with the question, compared to when paired only with descriptions. The best performance was observed in the full model setting, where all three modalities were incorporated. In this configuration, an F1-score of 29.66% and OA of 86.87% were obtained. This demonstrates that the full integration of question, vision, and description inputs contributes significantly to the models success."
        },
        {
            "title": "5.6.4 Single Domain Generalization",
            "content": "Table 7: Single domain generalization results in terms of Overall Accuracy (in %), with Antakya as the source domain due to its large data volume; all other regions serve as target domains. Experiment RSVQA (TGRS20) RSIVQA (TGRS22) CDVQA (TGRS22) BiModal (TGRS22) SOBA (AAAI24) RSAdapter (TGRS24) EarthVQANet (ISPRS24) TCSSM TCSSM+ (Ours) (Ours) Antakya Bata Antakya Beirut Antakya Goma Antakya Les Cayes Antakya Hawaii Antakya La Palma Antakya Derna Antakya Marshall Antakya Moulay Brahim Avg. Across Regions 84.72 89.58 77.97 88.16 57.94 71.76 74.42 79.65 78.01 78.02 85.10 88.41 78.00 89.25 55.30 72.03 75.13 76.17 77.93 77.48 83.84 89.78 79.66 87.63 58.86 73.75 74.22 80.98 77. 78.51 84.96 89.74 85.68 88.48 63.31 79.90 75.58 86.52 80.08 81.58 85.13 88.35 83.78 89.05 61.38 75.66 76.21 83.42 79.93 80.32 85.95 90.60 81.97 88.65 59.30 74.70 76.47 83.31 79. 80.08 85.32 89.70 83.54 89.10 61.56 75.17 76.21 83.76 80.12 80.50 86.81 91.33 91.25 92.40 68.55 80.83 77.75 90.56 85.33 84.97 87.59 90.94 91.80 88.50 71.10 87.53 81.74 91.07 85. 86.24 To evaluate the robustness of the proposed method under different DG scenarios, single-domain generalization setting was adopted. In this setting, Antakya was selected as the source domain due to its inclusion of the largest number of images among all available regions, thereby providing strong foundation for model training. The table 7 presents comparative analysis of several baseline models, including RSVQA, RSIVQA, CDVQA, BiModal, SOBA, RSAdapter, and EarthVQANet, as well as our proposed methods TCSSM and TCSSM+. Across all target regions, TCSSM+ consistently outperformed the baselines, achieving the highest average accuracy of 86.24%, followed closely by TCSSM with 84.97%. These results demonstrate the effectiveness of our multi-modal approach in learning domain-invariant representations that transfer well across geographically and visually diverse regions. 15 Text-conditioned State Space Model For Domain-generalized Change Detection Visual Question Answering Table 8: Effect of different Numbers of TCSSM Block Layers on performance (in %) for the Bata region (Validation split was used). # of Layers = 1 = 2 = 3 = 4 Recall Precision OA Params. (M) FLOPs (G) 30.45 30.64 31.34 27.66 27.13 29.02 27.99 27.06 84.50 90.32 89.02 83.38 28.69 29.80 29.57 27. 17.96 18.66 19.37 20.07 2.02 2.22 2.41 2."
        },
        {
            "title": "5.6.5 Number of Layers",
            "content": "Table 8 presents the results of ablation experiments conducted on the Bata dataset (validation split) to evaluate the impact of varying the number of layers in the model. The goal of this experiment was to identify the optimal layer depth that balances performance and computational efficiency. It can be observed that performance varies with the number of layers, with the best results achieved using 2-layer configuration. Specifically, the 2-layer model achieved the recall (30.64%) and strong F1 score (29.80%), while also delivering high OA of 90.32%. This configuration offers favorable trade-off between predictive performance and computational complexity, requiring 18.66 million parameters and 2.22 GFLOPs. The 3-layer model also showed competitive results, attaining the best recall (31.34%) and the OA (89.02%), along with strong F1 score (29.57%). Moreover, these experiments were also conducted for other regions, which produced similar results. Therefore, only the results from the Bata region are presented in the paper for clarity."
        },
        {
            "title": "5.6.6 Effect on performance of different multi-temporal fusion strategies",
            "content": "Table 9: Effect on performance of different multi-temporal fusion strategies. Location Metric Concat Sum Mul Sub Bata Beirut Goma Les Cayes Hawaii La Palma Derna Marshall Moulay Brahim Antakya Avg.Across Regions Recall(%) Precision(%) OA(%) F1 (%) OA(%) F1 (%) OA(%) F1 (%) OA(%) F1 (%) OA(%) F1 (%) OA(%) F1 (%) OA(%) F1 (%) OA(%) F1 (%) OA(%) F1 (%) OA(%) F1 (%) OA(%) F1 (%) 27.80 29.13 83.44 28.44 90.65 32. 94.57 33.71 93.08 32.64 70.02 23.48 88.00 29.44 81.47 28.13 90.70 31. 92.79 31.44 86.14 28.74 87.08 29.98 29.67 28.15 87.68 28.89 91.87 31.42 94.25 34. 94.14 33.34 69.78 22.80 88.25 30.11 80.97 29.00 90.73 32.44 90.78 30. 88.83 31.48 87.72 30.39 29.76 29.58 86.87 29.66 92.31 34.00 94.21 34.12 93.03 35. 72.91 29.76 88.10 31.92 80.24 32.63 92.47 33.99 88.70 31.90 88.05 32. 87.68 32.50 30.22 28.58 88.27 29.37 91.67 30.90 93.35 32.30 94.13 33.37 68.34 23. 88.13 29.82 81.30 28.12 91.97 31.20 88.62 29.82 87.32 30.28 87.31 29. Nsub 25.41 22.34 83.10 23.77 82.54 23.45 85.67 24.26 86.59 25.44 66.07 19. 80.48 22.63 76.40 22.26 82.95 23.87 82.98 23.76 83.47 26.50 81.02 23. To investigate the optimal strategy for fusing question and visual features before the answer prediction stage, several fusion operations were explored. Specifically, concatenation, element-wise summation, Hadamard product (multiplication), subtraction, and normalized subtraction were tested. As noted in prior work [74], the Hadamard product can effectively model non-linear interactions between different modalities with linear computational complexity, making it promising candidate. The results of these fusion strategies are reported in Table 9. As shown, the Hadamard product consistently yielded strong performance across most metrics and regions, achieving the best average F1-score (32.50%) and competitive average accuracy (87.68%). In particular, the subtraction-based fusion achieved the highest OA in the Bata region (88.27%), while the normalized subtraction approach underperformed across all regions. These 16 Text-conditioned State Space Model For Domain-generalized Change Detection Visual Question Answering Figure 5: Average accuracy (%) of various VQA models evaluated across different training data sizes (10%, 20%, and 100%). results highlight the effectiveness of element-wise operationsespecially multiplication and subtractionin capturing meaningful interactions between multi-modal features for VQA in disaster scenarios."
        },
        {
            "title": "5.6.7 Data Efficiency",
            "content": "To evaluate the robustness of the proposed model under limited supervision, experiments were conducted using varying fractions of the training data. This setting was motivated by the practical constraint that large-scale disaster datasets are not always readily available in real-world scenarios. Specifically, subsets containing 10%, 20%, and 100% of the original training data were sampled and used for training. As illustrated in Fig. 5 (These fractions were computed for all regions and subsequently averaged, with the resulting mean value presented in the figure), the model demonstrated strong resilience to data scarcity, maintaining competitive performance even when trained on only small portion of the data. The observed stability across different training scales highlights the models capacity to generalize effectively under low-resource conditions, making it suitable candidate for deployment in disaster response applications where labeled data is often limited or costly to obtain."
        },
        {
            "title": "5.7 Limitations",
            "content": "Despite its strong performance, one limitation of the TCSSM framework is its potential sensitivity to language bias. Specifically, because the model relies heavily on English-language disaster descriptions, it may tend to focus disproportionately on the textual input, potentially neglecting or underutilizing the rich visual information from the remote sensing images. This reliance on the question or textual cues can lead to decreased generalization when the language context changes or when visual features alone are crucial for accurate understanding."
        },
        {
            "title": "6 Conclusion",
            "content": "In conclusion, this study addresses the critical challenge of domain shift in the CDVQA task by introducing new benchmark dataset, BrightVQA, and proposing the TCSSM framework, which integrates bi-temporal visual information with geo-disaster-related textual descriptions to learn domain-invariant representations. Experimental results demonstrate that the proposed approach consistently outperforms existing methods across diverse domains, underscoring its effectiveness in enhancing generalization to real-world scenarios. 17 Text-conditioned State Space Model For Domain-generalized Change Detection Visual Question Answering"
        },
        {
            "title": "References",
            "content": "[1] Dengsheng Lu, Paul Mausel, Eduardo Brondizio, and Emilio Moran. Change detection techniques. International Journal of Remote Sensing, 25(12):23652401, 2004. [2] Hongruixuan Chen, Jian Song, Chengxi Han, Junshi Xia, and Naoto Yokoya. ChangeMamba: Remote sensing change detection with spatiotemporal state space model. IEEE TGRS, 62:120, 2024. [3] Haotian Zhang, Keyan Chen, Chenyang Liu, Hao Chen, Zhengxia Zou, and Zhenwei Shi. CDMamba: Incorporating local clues into mamba for remote sensing image binary change detection. IEEE TGRS, 2025. [4] Cui Zhang, Liejun Wang, Shuli Cheng, and Yongming Li. SwinSUNet: Pure transformer network for remote sensing image change detection. IEEE TGRS, 60:113, 2022. [5] Qingyang Li, Ruofei Zhong, Xin Du, and Yu Du. TransUNetCD: hybrid transformer network for change detection in optical remote-sensing images. IEEE TGRS, 60:119, 2022. [6] A. M. Olteanu-Raimond et al. Use of automated change detection and vgi sources for identifying and validating urban land use change. Remote Sensing, 12(7):1186, 2020. [7] Jiaojiao Tian, Allan Aasbjerg Nielsen, and Peter Reinartz. Improving change detection in forest areas based on stereo panchromatic imagery using kernel MNF. IEEE TGRS, 52(11):71307139, 2014. [8] Prosper Washaya, Timo Balz, and Bahaa Mohamadi. Coherence change-detection with sentinel-1 for natural and anthropogenic disaster monitoring in urban areas. Remote Sensing, 10(7):1026, 2018. [9] Corey Baker, Rick Lawrence, Clifford Montagne, and Duncan Patten. Change detection of wetland ecosystems using landsat imagery and change vector analysis. Wetlands, 27(3):610619, 2007. [10] Andreas Schmitt and Brian Brisco. Wetland monitoring using the curvelet-based change detection method on polarimetric sar imagery. Water, 5(3):10361051, 2013. [11] Guangliang Cheng, Yunmeng Huang, Xiangtai Li, Shuchang Lyu, Zhaoyang Xu, Hongbo Zhao, Qi Zhao, and Shiming Xiang. Change detection methods for remote sensing in the last decade: comprehensive review. Remote Sensing, 16(13):2355, 2024. [12] Zhenghang Yuan, Lichao Mou, Zhitong Xiong, and Xiao Xiang Zhu. Change detection meets visual question answering. IEEE TGRS, 60:113, 2022. [13] Jingyi Zhang, Jiaxing Huang, Sheng Jin, and Shijian Lu. Vision-language models for vision tasks: survey. IEEE TPAMI, 46(8):56255644, 2024. [14] Akash Ghosh, Arkadeep Acharya, Sriparna Saha, Vinija Jain, and Aman Chadha. Exploring the frontier of vision-language models: survey of current methodologies and future directions. arXiv:2404.07214, 2024. [15] Xiang Li, Congcong Wen, Yuan Hu, Zhenghang Yuan, and Xiao Xiang Zhu. Vision-language models in remote sensing: Current progress and future trends. IEEE GRSM, 12(2):3266, 2024. [16] Ke Zhang, Peijie Li, and Jianqiang Wang. review of deep learning-based remote sensing image caption: Methods, models, comparisons and future directions. Remote Sensing, 16(21):4113, 2024. [17] Huanyu Li, Hao Wang, Ying Zhang, Li Li, and Peng Ren. Underwater image captioning: Challenges, models, and datasets. ISPRS Journal of Photogrammetry and Remote Sensing, 220:440453, 2025. [18] Qian Wan, Xin Feng, Yining Bei, Zhiqi Gao, and Zhicong Lu. Metamorpheus: Interactive, affective, and creative dream narration through metaphorical visual storytelling. In Conference on Human Factors in Computing Systems, pages 116, 2024. [19] Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, Lawrence Zitnick, and Devi Parikh. VQA: Visual question answering. In IEEE ICCV, pages 24252433, 2015. [20] Qi Wu, Damien Teney, Peng Wang, Chunhua Shen, Anthony Dick, and Anton Van Den Hengel. Visual question answering: survey of methods and datasets. Computer Vision and Image Understanding, 163:2140, 2017. [21] Sylvain Lobry, Diego Marcos, Jesse Murray, and Devis Tuia. RSVQA: Visual question answering for remote sensing data. IEEE TGRS, 58(12):85558566, 2020. [22] Abhishek Das, Satwik Kottur, Khushi Gupta, Avi Singh, Deshraj Yadav, José MF Moura, Devi Parikh, and Dhruv Batra. Visual dialog. In IEEE CVPR, pages 326335, 2017. [23] Zixiao Zhang, Licheng Jiao, Lingling Li, Xu Liu, Puhua Chen, Fang Liu, Yuxuan Li, and Zhicheng Guo. spatial hierarchical reasoning network for remote sensing visual question answering. IEEE TGRS, 61:115, 2023. 18 Text-conditioned State Space Model For Domain-generalized Change Detection Visual Question Answering [24] Xiangtao Zheng, Binqiang Wang, Xingqian Du, and Xiaoqiang Lu. Mutual attention inception network for remote sensing visual question answering. IEEE TGRS, 60:114, 2021. [25] Sylvain Lobry and Devis Tuia. Visual question answering on remote sensing images. In Advances in Machine Learning and Image Analysis for GeoAI, pages 237254. Elsevier, 2024. [26] Marvin Zhang, Henrik Marklund, Nikita Dhawan, Abhishek Gupta, Sergey Levine, and Chelsea Finn. Adaptive risk minimization: Learning to adapt to domain shift. Advances in Neural Information Processing Systems, 34:2366423678, 2021. [27] Kaiyang Zhou, Ziwei Liu, Yu Qiao, Tao Xiang, and Chen Change Loy. Domain generalization: survey. IEEE TPAMI, 45(4):43964415, 2022. [28] Chengyue Huang, Brisa Maneechotesuwan, Shivang Chopra, and Zsolt Kira. FRAMES-VQA: Benchmarking fine-tuning robustness across multi-modal shifts in visual question answering. In IEEE CVPR, pages 39093918, 2025. [29] Yiming Xu, Lin Chen, Zhongwei Cheng, Lixin Duan, and Jiebo Luo. Open-ended visual question answering by multi-modal domain adaptation. arXiv:1911.04058, 2019. [30] Devis Tuia, Claudio Persello, and Lorenzo Bruzzone. Domain adaptation for the classification of remote sensing data: An overview of recent advances. IEEE GRSM, 4(2):4157, 2016. [31] Jiangtao Peng, Yi Huang, Weiwei Sun, Na Chen, Yujie Ning, and Qian Du. Domain adaptation in remote sensing image classification: survey. IEEE JSTARS, 15:98429859, 2022. [32] Mengqiu Xu, Ming Wu, Kaixin Chen, Chuang Zhang, and Jun Guo. The eyes of the gods: survey of unsupervised domain adaptation methods based on remote sensing data. Remote Sensing, 14(17):4380, 2022. [33] Valerio Marsocci, Nicolas Gonthier, Anatol Garioud, Simone Scardapane, and Clément Mallet. GeoMultiTaskNet: Remote sensing unsupervised domain adaptation using geographical coordinates. In IEEE CVPR, pages 2075 2085, 2023. [34] Xiaoshu Chen, Shaoming Pan, and Yanwen Chong. Unsupervised domain adaptation for remote sensing image semantic segmentation using region and category adaptive domain discriminator. IEEE TGRS, 60:113, 2022. [35] Suraj Jyothi Unni, Raha Moraffah, and Huan Liu. VQA-GEN: visual question answering benchmark for domain generalization. arXiv:2311.00807, 2023. [36] Mingda Zhang, Tristan Maidment, Ahmad Diab, Adriana Kovashka, and Rebecca Hwa. Domain-robust vqa with diverse datasets and methods but no target labels. In IEEE CVPR, pages 70467056, 2021. [37] Chenbin Liang, Weibin Li, Yunyun Dong, and Wenlin Fu. Single domain generalization method for remote sensing image segmentation via category consistency on domain randomization. IEEE TGRS, 62:116, 2024. [38] Yuxiang Zhang, Mengmeng Zhang, Wei Li, Shuai Wang, and Ran Tao. Language-aware domain generalization network for cross-scene hyperspectral image classification. IEEE TGRS, 61:112, 2023. [39] Hongruixuan Chen et al. BRIGHT: globally distributed multimodal building damage assessment dataset with very-high-resolution for all-weather disaster response. Earth System Science Data Discussions, 2025:151, 2025. [40] Shirsha Bose, Ankit Jha, Enrico Fini, Mainak Singha, Elisa Ricci, and Biplab Banerjee. StyLIP: Multi-scale style-conditioned prompt learning for clip-based domain generalization. In IEEE WACV, pages 55425552, 2024. [41] Yakoub Bazi, Mohamad Mahmoud Al Rahhal, Mohamed Lamine Mekhalfi, Mansour Abdulaziz Al Zuair, and Farid Melgani. Bi-modal transformer-based approach for visual question answering in remote sensing imagery. IEEE TGRS, 60:111, 2022. [42] Zhenghang Yuan, Lichao Mou, and Xiao Xiang Zhu. Multilingual augmentation for robust visual question answering in remote sensing images. In Joint Urban Remote Sensing Event, pages 14, 2023. [43] Zhenghang Yuan, Lichao Mou, Qi Wang, and Xiao Xiang Zhu. From easy to hard: Learning language-guided curriculum for visual question answering on remote sensing data. IEEE TGRS, 60:111, 2022. [44] Rafael Felix, Boris Repasky, Samuel Hodge, Reza Zolfaghari, Ehsan Abbasnejad, and Jamie Sherrah. Cross-modal visual question answering for remote sensing data. In Digital Image Computing: Techniques and Applications, pages 19, 2021. [45] Christel Chappuis, Valérie Zermatten, Sylvain Lobry, Bertrand Le Saux, and Devis Tuia. Prompt-RSVQA: Prompting visual context to language model for remote sensing visual question answering. In IEEE CVPR, pages 13721381, 2022. [46] Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. DistilBERT, distilled version of BERT: smaller, faster, cheaper and lighter. arXiv:1910.01108, 2019. 19 Text-conditioned State Space Model For Domain-generalized Change Detection Visual Question Answering [47] Yuduo Wang and Pedram Ghamisi. RSAdapter: Adapting multimodal models for remote sensing visual question answering. IEEE TGRS, 62:113, 2024. [48] Junjue Wang, Ailong Ma, Zihang Chen, Zhuo Zheng, Yuting Wan, Liangpei Zhang, and Yanfei Zhong. EarthVQANet: Multi-task visual question answering for remote sensing image understanding. ISPRS Journal of Photogrammetry and Remote Sensing, 212:422439, 2024. [49] Junjue Wang, Zhuo Zheng, Zihang Chen, Ailong Ma, and Yanfei Zhong. Earthvqa: Towards queryable Earth via relational reasoning-based remote sensing visual question answering. In AAAI, volume 38, pages 54815489, 2024. [50] Yuan Hu, Jianlong Yuan, Congcong Wen, Xiaonan Lu, Yu Liu, and Xiang Li. RSGPT: remote sensing vision language model and benchmark. ISPRS Journal of Photogrammetry and Remote Sensing, 224:272286, 2025. [51] Zhuoran Liu, Danpei Zhao, Bo Yuan, and Zhiguo Jiang. RescueADI: adaptive disaster interpretation in remote sensing images with autonomous agents. IEEE TGRS, 2025. [52] Kartik Kuckreja, Muhammad Sohail Danish, Muzammal Naseer, Abhijit Das, Salman Khan, and Fahad Shahbaz Khan. Geochat: Grounded large vision-language model for remote sensing. In IEEE CVPR, pages 2783127840, 2024. [53] Danpei Zhao, Jiankai Lu, and Bo Yuan. See, perceive, and answer: unified benchmark for high-resolution postdisaster evaluation in remote sensing images. IEEE TGRS, 62:114, 2024. [54] Wei Zhang, Miaoxin Cai, Tong Zhang, Yin Zhuang, and Xuerui Mao. EarthGPT: universal multimodal large language model for multisensor image comprehension in remote sensing domain. IEEE TGRS, 62:120, 2024. [55] Sylvain Lobry, Begüm Demir, and Devis Tuia. RSVQA meets BigEarthNet: new, large-scale, visual question answering dataset for remote sensing. In IEEE IGARSS, pages 12181221, 2021. [56] Gencer Sumbul, Marcela Charfuelan, Begüm Demir, and Volker Markl. Bigearthnet: large-scale benchmark archive for remote sensing image understanding. In IEEE IGARSS, pages 59015904. IEEE, 2019. [57] Xiaoqiang Lu, Binqiang Wang, Xiangtao Zheng, and Xuelong Li. Exploring models and data for remote sensing image caption generation. IEEE TGRS, 56(4):21832195. [58] Gui-Song Xia et al. DOTA: large-scale dataset for object detection in aerial images. In IEEE CVPR, June 2018. [59] Yuanlin Zhang, Yuan Yuan, Yachuang Feng, and Xiaoqiang Lu. Hierarchical and robust convolutional neural network for very high-resolution remote sensing object detection. IEEE TGRS, 57(8):55355548, 2019. [60] Kunping Yang, Gui-Song Xia, Zicheng Liu, Bo Du, Wen Yang, Marcello Pelillo, and Liangpei Zhang. Semantic change detection with asymmetric siamese networks. arXiv:2010.05687, 2020. [61] Laila Bashmal, Yakoub Bazi, Farid Melgani, Riccardo Ricci, and Mohamad Al. Visual question generation from remote sensing images. IEEE JSTARS, 16:3279, 2023. [62] Taghreed Abdullah, Yakoub Bazi, Mohamad Al Rahhal, Mohamed Mekhalfi, Lalitha Rangarajan, and Mansour Zuair. TextRS: Deep bidirectional triplet network for matching text to remote sensing images. Remote Sensing, 12(3):405, 2020. [63] Zhitong Xiong, Fahong Zhang, Yi Wang, Yilei Shi, and Xiao Xiang Zhu. Earthnets: Empowering ai in earth observation. arXiv:2210.04936, 2022. [64] Weixun Zhou, Shawn Newsam, Congmin Li, and Zhenfeng Shao. PatternNet: benchmark dataset for performance evaluation of remote sensing image retrieval. ISPRS journal of photogrammetry and remote sensing, 145:197209, 2018. [65] Yi Yang and Shawn Newsam. Bag-of-visual-words and spatial extensions for land-use classification. In International conference on advances in geographic information systems, pages 270279, 2010. [66] Gong Cheng, Junwei Han, and Xiaoqiang Lu. Remote sensing image scene classification: Benchmark and state of the art. Proceedings of the IEEE, 105(10):18651883, 2017. [67] Junjue Wang, Zhuo Zheng, Ailong Ma, Xiaoyan Lu, and Yanfei Zhong. LoveDA: remote sensing land-cover dataset for domain adaptive semantic segmentation. arXiv:2110.08733, 2021. [68] Rui Zhu, Yiwen Guo, and Jing-Hao Xue. Adjusting the imbalance ratio by the dimensionality of imbalanced data. Pattern Recognition Letters, 133:217223, 2020. [69] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In conference of the North American chapter of the association for computational linguistics: human language technologies, pages 41714186, 2019. 20 Text-conditioned State Space Model For Domain-generalized Change Detection Visual Question Answering [70] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In ICML, pages 448456, 2015. [71] Alec Radford et al. Learning transferable visual models from natural language supervision. In ICML, pages 87488763, 2021. [72] Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. arXiv:2312.00752, 2023. [73] Elman Ghazaei and Erchan Aptoula. Change state space models for remote sensing change detection. arXiv:2504.11080, 2025. [74] Grigorios Chrysos, Yongtao Wu, Razvan Pascanu, Philip Torr, and Volkan Cevher. Hadamard product in deep learning: Introduction, advances and challenges. IEEE TPAMI, 2025. [75] Anqi Mao, Mehryar Mohri, and Yutao Zhong. Cross-entropy loss functions: Theoretical analysis and applications. In ICML, pages 2380323828, 2023. [76] Diederik Kingma and Jimmy Ba. Adam: method for stochastic optimization. arXiv:1412.6980, 2014. [77] Ailin Deng, Tri Cao, Zhirui Chen, and Bryan Hooi. Words or vision: Do vision-language models have blind faith in text? In IEEE CVPR, pages 38673876, 2025."
        }
    ],
    "affiliations": [
        "Faculty of Engineering and Natural Sciences (VPALab), Sabanci University, Türkiye"
    ]
}