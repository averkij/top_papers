{
    "paper_title": "Group Distributionally Robust Optimization-Driven Reinforcement Learning for LLM Reasoning",
    "authors": [
        "Kishan Panaganti",
        "Zhenwen Liang",
        "Wenhao Yu",
        "Haitao Mi",
        "Dong Yu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent progress in Large Language Model (LLM) reasoning is increasingly driven by the refinement of post-training loss functions and alignment strategies. However, standard Reinforcement Learning (RL) paradigms like Group Relative Policy Optimization (GRPO) remain constrained by static uniformity: uniform prompt sampling and a fixed number of rollouts per prompt. For heterogeneous, heavy-tailed reasoning data, this creates structural inefficiencies that waste compute on already-solved patterns while under-training the long tail of hard problems. To address this, we propose Multi-Adversary Group Distributionally Robust Optimization (GDRO), an optimization-first framework that moves beyond uniform reasoning models by dynamically adapting the training distribution. We introduce an Online Difficulty Classifier that partitions prompts into dynamic pass@k difficulty groups. We then propose two independent GDRO games for post-training: (1) Prompt-GDRO, which employs an EMA-debiased multiplicative-weights bandit sampler to target the intensive difficulty margin and upweight persistently hard groups without frequency bias; and (2) Rollout-GDRO, which uses a shadow-price controller to reallocate rollouts across groups, maximizing gradient variance reduction on hard tasks under a fixed mean budget (compute-neutral). We provide no-regret guarantees for both controllers and additionally a variance-proxy analysis motivating a square-root optimal rollout allocation for Rollout-GDRO. We validate our framework on the DAPO 14.1k dataset using Qwen3-Base models. Prompt-GDRO and Rollout-GDRO achieve average relative gains of +10.6% and +10.1%, respectively, in pass@8 accuracy across 1.7B, 4B, and 8B scales compared to the GRPO baseline. Qualitative analysis shows an emergent curriculum: the adversaries shift resources to the evolving reasoning frontier, enhancing the reasoning model's performance."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 7 2 ] . [ 1 0 8 2 9 1 . 1 0 6 2 : r Technical Report Group Distributionally Robust Optimization-Driven Reinforcement Learning for LLM Reasoning Kishan Panaganti, Zhenwen Liang, Wenhao Yu, Haitao Mi, Dong Yu Tencent AI Lab in Bellevue, WA, USA Correspondence to: kpb@global.tencent.com"
        },
        {
            "title": "Abstract",
            "content": "Recent progress in Large Language Model (LLM) reasoning is increasingly driven by the refinement of post-training loss functions and alignment strategies1. However, standard Reinforcement Learning (RL) paradigms like Group Relative Policy Optimization (GRPO) remain constrained by static uniformity: uniform prompt sampling and fixed number of rollouts per prompt. For heterogeneous, heavy-tailed reasoning data, this creates structural inefficiencies that waste compute on already-solved patterns while under-training the long tail of hard problems. To address this, we propose MultiAdversary Group Distributionally Robust Optimization (GDRO), an optimization-first framework that moves beyond uniform reasoning models by dynamically adapting the training distribution. We introduce an Online Difficulty Classifier that partitions prompts into dynamic pass@k difficulty groups. We then propose two independent GDRO games for post-training: (1) Prompt-GDRO, which employs an EMA-debiased multiplicative-weights bandit sampler to target the intensive difficulty margin and upweight persistently hard groups without frequency bias; and (2) Rollout-GDRO, which uses shadow-price controller to reallocate rollouts across groups, maximizing gradient variance reduction on hard tasks under fixed mean budget (compute-neutral). We provide no-regret guarantees for Prompt-GDRO (via an entropy-regularized GDRO surrogate) and variance-proxy analysis motivating square-root optimal rollout allocation for Rollout-GDRO. We validate our framework on the DAPO 14.1k dataset using Qwen3-Base models. Prompt-GDRO and Rollout-GDRO achieve average relative gains of +10.6% and +10.1%, respectively, in pass@8 accuracy across 1.7B, 4B, and 8B scales compared to the GRPO baseline. Qualitative analysis shows an emergent curriculum: the adversaries shift resources to the evolving reasoning frontier, enhancing the reasoning models performance. Figure 1: Beyond Uniform ReasoningA Multi-Adversary Post-Training Framework. Plots on the right represent training steps tail averages (60th percentile) capturing the curriculum. (Left) Our framework significantly outperforms the standard GRPO baseline across mathematical reasoning benchmarks via dynamic adaptation. (Center) Prompt-GDRO: The adversary learns non-uniform curriculum. Instead of uniform sampling (dashed line), probability mass (purple bars) shifts to the reasoning frontier (bins 68), targeting the specific difficulty level where learning is most efficient. (Right) Rollout-GDRO: The adversary optimizes compute utility. Under fixed global budget (dashed line), it reallocates rollouts (orange bars) from solved tasks (bin 0) to high-variance tasks, scaling exploration with difficulty. Note: Bars represent rollout count per prompt (policy intensity). 1Adam Marblestone (paraphrased) on Dwarkeshs podcast (YouTube): The brains secret sauce is its loss functions, not its architecture. 1 Technical Report Figure 2: Conceptual Illustration: Static Uniformity vs. Multi-Adversary GDRO (Dynamic). (Left) Standard GRPO samples prompts uniformly (q = 1/B) and assigns fixed number of rollouts (schematically = 16), causing it to overfit easy tasks while under-exploring the frontier. (Right) Our framework employs an Online Difficulty Classifier to dynamically partition prompts based on real-time pass@k. It introduces two independent adversarial feedback loops (not coupled): (1) Prompt-GDRO (Data Distributor) uses an EMA-debiased scorer to shift the sampling distribution toward hard bins, creating traveling wave of difficulty; (2) Rollout-GDRO (Resource Allocator) uses shadow price µ to solve constrained optimization problem, allocating discrete rollout arms (nmin . . . nmax) to maximize gradient variance reduction on high-uncertainty tasks."
        },
        {
            "title": "Introduction",
            "content": "The capabilities of Large Language Models (LLMs) in complex reasoning are increasingly shaped not only by architectural scaling, but by the design of post-training objectives and alignment pipelines. Reinforcement Learning (RL), particularly methods like Proximal Policy Optimization (PPO) (Schulman et al., 2017) and Group Relative Policy Optimization (GRPO) (Shao et al., 2024), has emerged as standard approach for aligning models with rigorous logical constraints. By optimizing for sparse verifiable rewards or process-based supervision, these methods have enabled significant breakthroughs in mathematical problem solving (Wang et al., 2024b) and code generation. At high level, our perspective is that reasoning post-training exposes two distinct sources of non-uniformity: (i) which prompts remain unsolved as the model improves (a shifting difficulty landscape), and (ii) how much exploration different prompts require to yield low-variance learning signals. Standard pipelines treat both knobs as staticuniform prompt sampling and fixed rollouts which we argue is mismatched to the heavy-tailed structure of reasoning. Recent discussions in the broader ML community further motivate moving beyond uniformity from data value perspective. Finzi et al. (Finzi et al., 2026) propose epiplexitya notion of information that aims to quantify the structural content that computationally bounded learner can extract from data (distinct from time-bounded entropy/noise). From this viewpoint, the value of prompt is not determined by its frequency, but by whether it still contains learnable structure for the current policy under fixed compute budget. This perspective is aligned with our Prompt-GDRO mechanism: by steering sampling toward the evolving reasoning frontier, we concentrate updates on prompts that remain high-value for learning, rather than repeatedly training on already-solved, low-value examples. Concurrently, empirical analyses of RL with verifiable rewards suggest that learning progress can be dominated by small fraction of high-entropy decision points, challenging the implicit assumption that uniform averaging over all tokens/prompts is the most compute-efficient choice (Wang et al., 2025). Related work on thinking and the accuracycompute Pareto frontier emphasizes that additional computation is most beneficial when applied selectively, and can be inefficient when applied uniformly across instances (Madaan et al., 2025). Taken together, these perspectives echo community intuition that both data selection and compute allocation should be adaptive (Finzi, 2 Technical Report 2026)precisely the two control knobs instantiated by our Prompt-GDRO and Rollout-GDRO adversaries. However, prevalent RL pipelines rely on fundamental assumption of static uniformity: they sample prompts uniformly from the training distribution and allocate fixed computational budget (number of rollouts) to every prompt. We argue that this rigidity creates structural inefficiencies. As formalized in our analysis (Section 4), reasoning datasets are inherently heterogeneous, composed of disjoint sub-domains (e.g., elementary algebra vs. Olympiad number theory) with vastly different difficulty profiles. Under uniform sampling, optimization is dominated by the most frequent, often easier patterns, so learning signal concentrates on the easy core while errors persist in long, difficult tail. long line of supervised learning work has addressed this asymmetry by making training explicitly difficulty-awarefrom curriculum and self-paced learning that schedule examples from easy to hard (Bengio et al., 2009; Kumar et al., 2010), to boosting and hard-example mining that upweight misclassified or high-loss instances (Freund & Schapire, 1997; Shrivastava et al., 2016), and focal losses that downweight well-classified examples (Lin et al., 2017). This motivates viewing robustness through difficulty-defined groups and optimizing worst-bin performance in the spirit of GDRO (Sagawa et al., 2020). Furthermore, the value of computational exploration is non-uniform. In reasoning tasks, solved prompts yield low-variance gradients, while high-entropy frontier prompts require massive exploration to reduce gradient variance (Setlur et al., 2025). static budget allocation fails to capture this dynamic, wasting resources on redundant verification while under-exploring critical failure modes. To address these limitations, we propose Multi-Adversary Group Distributionally Robust Optimization (GDRO) framework. Motivated by the biological hypothesis that intelligent systems distinguish between core representation learning module and specialized steering subsystem that optimizes cost functions (Marblestone et al., 2016), we implement dynamic, data-agnostic grouping mechanism. Specifically, we replace static uniformity with an Online Difficulty Classifier that partitions data based on real-time empirical error rates (pass@k), effectively allowing the optimization process to steer itself. We then formulate post-training as zero-sum game and instantiate two complementary adversaries via the GDRO-EXP3P algorithm (Soma et al., 2022). Importantly, these adversaries are designed as independent modules: Prompt-GDRO plays GDRO reweighting game against the learner, while Rollout-GDRO plays separate constrained compute-allocation game. In this work we analyze and evaluate the two games in isolation (no coupling); jointly coupling both adversaries into single multi-time-scale system is left to future work. Our contributions are summarized as follows: 1. Prompt-GDRO (A Data Adversary): We employ an adversarial reweighting rule that targets the intensive difficulty margin (mean loss) instead of uniform sampling. We introduce an EMADebiased scoring rule to prevent the adversary from succumbing to frequency bias, ensuring that rare, high-difficulty groups are upweighted effectively. This acts as regularizer against over-optimizing the easy core, improving worst-bin robustness as the difficulty frontier shifts. Theory: In Section 4 (proofs in Appendix B), we show that exponential-weights Prompt-GDRO corresponds to optimizing an entropy-regularized GDRO surrogate (a log-sum-exp soft worstgroup objective) and admits no-regret game interpretation. 2. Rollout-GDRO (A Compute Adversary): We challenge the convention of fixed rollout budgets (e.g., = 4). We formulate rollout allocation as constrained resource allocation game where second adversary dynamically assigns rollout counts to maximize gradient variance reduction on hard tasks, subject to global mean-rollout compute constraint. This enables the model to efficiently explore the solution space of complex problems without increasing the total training budget. Theory: In Section 4 (proofs in Appendix B), we derive variance proxy for GRPO rollouts and show that the variance-optimal compute-neutral allocation obeys square-root law, motivating the shadow-price controller used by Rollout-GDRO. 3. Empirical results. On the DAPO 14.1k reasoning dataset with Qwen3-Base models (1.7B/4B/8B), Prompt-GDRO improves pass@8 by +9.74%, +13.13%, and +8.96%, and Rollout-GDRO by +10.64%, +10.59%, and +9.20% over GRPO. Qualitative analyses reveal an emergent curriculum that shifts sampling weight and rollout budget toward the evolving reasoning frontier. 3 Technical Report"
        },
        {
            "title": "2 Preliminaries",
            "content": "2.1 Reinforcement Learning for Reasoning We formalize post-training for reasoning as Reinforcement Learning (RL) over an autoregressive language policy. Let denote prompt and let = (y1, . . . , yL) denote response sampled from policy πθ( x) parameterized by θ. The conditional sequence probability factorizes as πθ(y x) = t=1 πθ(yt x, y<t) . The RL objective is to maximize expected reward J(θ) = xD, yπθ (x)[r(x, y)] , (1) (2) where r(x, y) is task-dependent, generally non-differentiable reward. In mathematical reasoning, r(x, y) is typically sparse (e.g., binary correctness) or semi-sparse (e.g., verifier-based signals). 2.2 Group-Relative Policy Optimization (GRPO) Group-Relative Policy Optimization (GRPO) (Shao et al., 2024) is computationally efficient alternative to Proximal Policy Optimization (PPO) (Schulman et al., 2017). GRPO eliminates learned value critic by constructing baseline from within-prompt group of rollouts. Remark 2.1 (Two notions of group). Throughout the paper, the term group can refer to two different objects: (i) GRPO rollout group (multiple rollouts for fixed prompt), and (ii) GDRO group/bin (a subset of prompts induced by grouping rule, e.g., an online difficulty bin). We explicitly index GRPO rollouts by (i, j) and GDRO bins by {1, . . . , B}. Remark 2.2 (Notation: vs. k). We use for the GRPO rollout-group size (train-time rollouts per prompt), and for best-of-k statistics such as pass@k and mean@k. In our experiments, we use = 4 and = 8. Group sampling and advantage estimation. For each prompt xi, we sample responses {yi,j}n j=1 from behavior policy πθold (the policy used to generate rollouts). Each response receives scalar r(xi, yi,j). GRPO computes group-relative advantage by standardizing rewards reward ri,j within the rollout group: Ai,j = ri,j µi σi + ε , (3) 1 j=1(ri,j µi)2, and ε > 0 is small constant for numerical stability. where µi The scalar advantage Ai,j is applied token-wise to all tokens in yi,j in the surrogate objective below. j=1 ri,j, σi (cid:113) 1 The clipped surrogate objective. Let ρi,j,t(θ) πθ πθold (cid:0)yi,j,t xi, yi,j,<t (cid:1) (cid:0)yi,j,t xi, yi,j,<t (cid:1) (4) denote the token-level importance ratio. The GRPO/PPO-style clipped surrogate for response yi,j is CLIP i,j (θ) = 1 Li,j Li,j t=1 (cid:16) min ρi,j,t(θ) Ai,j, clip(cid:0)ρi,j,t(θ), 1 ϵ, 1 + ϵ(cid:1) Ai,j (cid:17) , (5) where ϵ > 0 is the PPO clipping parameter. Observable loss signal. For our robust optimization controllers, we require scalar loss-like signal per generated response. We define the per-response loss as the negative KL-regularized 4 Technical Report surrogate: ℓi,j(θ) = CLIP i,j (θ) + βKL DKL(πθ( xi) πref( xi)) , (6) where βKL > 0 controls the KL penalty to fixed reference policy πref. In our experiments we operate in zero-SFT setting, so πref is simply the initial base checkpoint (i.e., the same Qwen3-{1.7B,4B,8B}- Base model we start RL from) and is held frozen during training. In practice, DKL(πθ( xi) πref( xi)) is evaluated on the sampled responses using token-level log-probabilities under πθ and πref. Finally, we define the prompt-level loss as the mean over rollouts: ℓ(xi; θ) 1 j=1 ℓi,j(θ). (7) This prompt-level signal will be aggregated by bins and fed to the GDRO adversaries. 2.3 Group Distributionally Robust Optimization Standard Empirical Risk Minimization (ERM) minimizes average loss under the empirical mixture, which can be dominated by high-frequency and/or easy instances. long line of supervised learning work addresses this imbalance by making training explicitly difficulty-awarefrom curriculum and self-paced learning that schedule examples from easy to hard (Bengio et al., 2009; Kumar et al., 2010), to boosting and hard-example mining that upweight misclassified or high-loss instances (Freund & Schapire, 1997; Shrivastava et al., 2016), and focal losses that downweight well-classified examples (Lin et al., 2017). GDRO provides complementary, principled objective: treat subpopulations (here, difficulty bins) as groups and optimize worst-group risk. We adopt Group Distributionally Robust Optimization (GDRO) (Sagawa et al., 2020). Assume prompts are partitioned into disjoint groups (domains) {D1, . . . , DB}. Let Lb(θ) xDb [ℓ(x; θ)] (8) denote the expected prompt-level loss for group b. GDRO optimizes worst-group performance by solving b=1 where is the probability simplex. Following Soma et al. (2022), (9) admits zero-sum game interpretation between learner (θ) and an adversary (q) and can be optimized via no-regret online learning. Our method instantiates this perspective with multiple adversarial levers on top of the GRPO loss signal in (6). qb Lb(θ), max B min θ (9)"
        },
        {
            "title": "3 Method",
            "content": "Standard instruction tuning paradigms are characterized by static uniformity: uniform distribution over prompts and fixed computational budget per prompt. We argue that this rigidity creates structural inefficiencies in learning, particularly for heterogeneous reasoning tasks where difficulty varies significantly across domains. To address this, we propose Multi-Adversary Framework that decomposes the training process into dynamic distributional levers. Our method operates in shared environment where prompts are dynamically categorized by difficulty (Section 3.1), serving as the basis for distinct adversarial processes: 1. Adversarial Sampler (qprompt): Optimizes the probability of sampling prompts from different difficulty bins to expose model weaknesses (Section 3.2). 2. Adversarial Budgeter (nrollout): Optimizes the allocation of rollout counts nb to different bins to maximize gradient information under global compute constraint (Section 3.3). 5 Technical Report 3.1 Dynamic Grouping via Online Pass@k critical prerequisite for GDRO is the definition of domains (groups) Db. Relying on static dataset metadata (e.g., Level 5) is suboptimal because theoretical difficulty often diverges from empirical model capability. Furthermore, reliance on explicit labels limits applicability to datasets with rich metadata. 3.1.1 Data-Agnostic Difficulty Estimation Our method removes the dependency on static annotations by defining groups solely through empirical interaction. We employ an Online Difficulty Classifier to construct dynamic groups based on the models real-time performance. By utilizing the policys own error rate as the grouping criterion, the framework becomes data-agnostic. Whether the underlying data is math, code, or creative writing, the difficulty is emergent, allowing the pipeline to automatically discover and upweight the subsets of data that are currently challenging for the policy. 3.1.2 Implementation Details We assign each prompt unique identifier (UID) and track an online pass@k statistic, where is fixed hyperparameter that defines the difficulty scale (e.g., = 8 in our experiments). At training step t, we sample rollouts {yj}k j=1 for prompt and define an any-of-k correctness indicator ct(x) I{j {1, . . . , k} : r(x, yj) = 1}, which equals 1 iff at least one of the rollouts is correct. We then maintain moving estimate of pass@k using sliding window of length H, pass@kt(x) 1 (cid:92) s=tH+1 cs(x), (10) with the convention that the sum is taken over available history for newly seen UIDs. We map prompts to discrete accuracy bins (e.g., accbin 0 for [0, 0.1), accbin 1 for [0.1, 0.2), etc.). Let 0 = a0 < a1 < < aB = 1 denote bin edges. This induces partition of the input space = (cid:83)B b=1 Db, where Db (cid:8)x : (cid:92) pass@kt(x) [ab1, ab)(cid:9). (11) We define the (time-varying) grouping map gt : {1, . . . , B} by gt(x) = if Db. Remark 3.1 (Bins as dynamic groups). We use bin and group interchangeably: the bin index corresponds to the GDRO group Db induced by the online pass@k estimate (10). Within one training step, we treat gt as fixed; across steps, the partition evolves as the policy improves. To ensure stability in the optimization landscape, we implement hysteresis: prompt is only reassigned to new bin if its moving average accuracy crosses the bin boundary by margin δ. This prevents prompts from oscillating between groups due to stochastic noise, ensuring that the adversaries target stable difficulty tiers rather than transient fluctuations. 3.2 Adversarial Prompt Reweighting (Prompt-GDRO) The goal of the prompt reweighting adversary is to construct distribution qt over the dynamic groups defined above. The adversary seeks to maximize the expected loss, thereby forcing the policy to improve on the Pareto frontier of difficulty. In summary, we realize Prompt-GDRO by bin-wise reweighting of GRPO updates (via per-sample weights applied to advantages), rather than by physically resampling prompts. 3.2.1 EMA-Debiased EXP3P Optimization We solve the inner maximization problem using the GDRO-EXP3P algorithm (Soma et al., 2022). Intuitively, the adversary maintains distribution qt over bins and increases pressure on bins with high recent loss. key subtlety is frequency bias: if the adversary were to optimize cumulative 6 Technical Report (extensive) loss, then naturally frequent bins would dominate the score even if they are easy. Our design instead targets the intensive difficulty margin (mean loss), which is the quantity that indicates how much typical prompt in the bin is currently challenging. To correct this, we propose an EMA-Debiased scoring rule. For each bin {1, . . . , B}, we maintain difficulty score St(b) updated via an Exponential Moving Average (EMA) with decay β: St(b) (1 β)St1(b) + β ℓt(b), (12) where ℓt(b) is the empirical mean prompt-level loss for bin at step t. Let Bt be the batch of prompts at step t, and let Bb,t = {x Bt gt(x) = b} denote the subset in bin b. We compute ℓt(b) = 1 Bb,t xBb,t ℓ(x; θ). (13) Let ˆqt(b) Bb,t/Bt denote the (realized) prompt share of bin in the current batch. In practice we optionally normalize the update by ˆqt(b) (with small floor) to ensure that rare but consistently high-loss bins can compete with common bins in the EXP3P update. Crucially, St(b) tracks intensive difficulty (mean loss) rather than extensive loss, which decouples the adversarial signal from static dataset frequency. The adversarial (unnormalized) bin weights ωt(b) are derived by exponentiating these scores: ωt(b) = exp (cid:0)ηq clip(St(b), C, C)(cid:1) , (14) where ηq is the learning rate (sharpness). The final sampling probability includes uniform mixing term γ to guarantee exploration: qt(b) = (1 γ) ωt(b) j=1 ωt(j) + γ . (15) Rather than physically resampling the dataset, we realize qt by reweighting the GRPO gradient contribution of each prompt. Concretely, for prompt xi we scale its rollout advantages as Ai,j Ai,j min{ωt(gt(xi)), ωmax}, (16) where ωmax is cap for numerical stability. Note that using the unnormalized score ωt(b) (rather than the normalized probability qt(b)) preserves the same relative weighting across bins; the omitted normalization constant is common scalar factor that is absorbed into the effective step size of the GRPO update. Intuitively, this increases the effective step size on bins the adversary considers difficult, which is equivalent (in expectation) to optimizing the GDRO objective with adversarial weights. 3.3 Adversarial Rollout Budgeting (Rollout-GDRO) In standard GRPO, the number of rollouts is fixed (e.g., = 4). However, easy prompts yield diminishing returns from extra rollouts, while hard prompts require more exploration to reduce gradient variance and to stabilize group statistics (e.g., the mean and standard deviation in (3)). We formulate the choice of rollout counts as second GDRO-style resource allocation problem over the same dynamic bins induced by gt. 3.3.1 Constrained Maximization Formulation We treat the number of rollouts for bin as discrete variable nb {nmin, . . . , nmax}. Let ˆqt(b) denote the realized bin share in the prompt batch at step t. 7 Technical Report Bin-level utility from nb rollouts. Recall from Section 2 that GRPO provides per-response loss signal ℓi,j(θ) (Eq. (6)) and aggregates it into prompt-level loss by averaging over rollouts, ℓ(xi; θ) = 1 j=1 ℓi,j(θ). When using bin-specific rollout count nb, we compute the same quantity with nb samples: ℓ(xi; θ, nb) 1 nb nb j=1 ℓi,j(θ). (17) Given the current batch Bt and the induced bin partition {Bb,t}B utility (negative loss) under nb rollouts as b=1, we define the empirical bin-level ˆJb(θ; nb) 1 Bb,t xiBb,t ℓ(xi; θ, nb), (18) where the hat emphasizes that this is finite-sample estimate computed on the current batch. Mean-rollout constraint. The budgeter chooses {nb} to concentrate rollouts on bins where they are most valuable, subject to strict global budget on the mean rollouts per prompt n: b= max {nb} ˆqt(b) ˆJb(θ; nb) s.t. b=1 ˆqt(b) nb = n. (19) This formulation incentivizes allocating more compute to bins where additional rollouts improve gradient quality the most: more rollouts both (i) reduce Monte Carlo noise and (ii) increase the number of informative samples contributing to the update. The global budget (typically set to the baseline rollout count, e.g., = 4) ensures the total computational cost matches the uniform baseline. 3.3.2 Dual Ascent Solver We solve the constrained maximization above via Lagrangian relaxation with single multiplier µ for the mean-rollout constraint. For candidate rollout count in bin b, we define the corresponding penalized bandit loss as: Lb(n) = ˆJb(θ; n) + µ n. We maintain separate EXP3P instance (Soma et al., 2022) where the arms are the discrete integers in [nmin, nmax]. At each step, we select the configuration {nb}B b=1 that maximizes the joint probability while strictly satisfying the global budget equality constraint b=1 ˆqt(b)nb = n. This exact matching is implemented via dynamic programming selection step over the active bins. (20) After the batch is processed and the actual rollout consumption ˆnrealized is observed, the dual variable µ is updated: µ µ + αµ( ˆnrealized n), where αµ is the dual learning rate. This mechanism ensures the method remains compute-neutral compared to the baseline, dynamically shifting resources from easy to hard groups based on the shadow price of compute µ. (21)"
        },
        {
            "title": "4 Analysis",
            "content": "This section provides first-principles interpretation of why static uniformity can be structurally inefficient in RL post-training for reasoning, and how our two controllers implement targeted robustness improvements. We intentionally avoid re-defining Prompt-GDRO and Rollout-GDRO (Section 3); here we connect the method to robust objectives and state the core theoretical messages. Detailed derivations and proofs are deferred to Appendix B. 8 Technical Report 4.1 Motivation: Why uniformity can fail in reasoning post-training Reasoning datasets are heterogeneous: prompts belong to latent sub-domains (topics, formats, reasoning styles) with widely varying difficulty. Uniform sampling and fixed rollout budget implicitly assume that (a) all prompts are equally informative to train on and (b) the same amount of exploration is warranted everywhere. Both assumptions are brittle in practice. In particular, once large fraction of prompts become nearly solved, their rollouts yield low-variance gradients and diminishing marginal learning signal, while the remaining frontier prompts continue to exhibit high uncertainty. Under uniform sampling, optimization is dominated by the most frequent, often easier patterns, so learning signal concentrates on the easy core while errors persist in long, difficult tail (Bengio et al., 2009; Sagawa et al., 2020). GDRO lens. We use GDRO (Section 2.3, Eq. (9)) as compact way to reason about worst-bin robustness. In our setting the groups are the online difficulty bins induced by gt (Section 3.1); within training step we treat gt as fixed and interpret the group losses Lb(θ) in Eq. (9) as binconditional GRPO prompt losses. Under this lens, Prompt-GDRO is an online approximation to the inner adversary over q, while Rollout-GDRO is second adversary that reallocates rollout compute across the same bins to improve the signal-to-noise ratio of the update under compute-neutral budget. 4.2 Adversary (Prompt-GDRO): Robustness via EMA-debiased difficulty pressure From GDRO to bandit adversary. If bins were fixed and losses were observed noiselessly, the inner maximization in Eq. (9) could be solved by concentrating on the current worst-loss bin. In post-training, however, (i) losses are stochastic Monte Carlo estimates, and (ii) the grouping gt is online and non-stationary as the model improves. Following Soma et al. (2022), we view the adversary as an online learner that updates distribution over bins using no-regret bandit-style updates. Why EMA-debiased? Frequency bias vs. intensive difficulty. Methodologically, Prompt-GDRO is fully specified in Section 3.2. The key point for the analysis is that the adversary is driven by an intensive statistic (mean prompt loss per bin) rather than an extensive cumulative loss, and that this statistic is smoothed over time. Concretely, Prompt-GDRO maintains an EMA difficulty score St(b) (Eq. (12)), exponentiates the (clipped) scores to obtain unnormalized weights ωt(b) (Eq. (14)), and forms the adversarial bin distribution qt with an exploration mixture (Eq. (15)). Optionally normalizing by the realized bin share ˆqt(b) mitigates frequency bias so that persistently hard but rare bins can remain competitive. How qt acts on GRPO updates (intuition). Although qt can be interpreted as sampling distribution over bins, we realize it in compute-neutral way by reweighting gradient contributions (Section 3.2). Scaling prompts rollout advantages by bin-dependent multiplier increases its effective learning rate, which implements the same pay more attention to hard bins principle as Eq. (9). 4.2.1 Entropic GDRO view: log-sum-exp surrogate and softmax best response Prompt-GDRO implements the adversary via exponential weights (Eqs. (14)(15)), which corresponds to entropic mirror ascent on the simplex. key consequence is that the hard inner maximum in Eq. (9) is implicitly replaced by an entropy-regularized one, yielding smooth soft worst-group objective. To keep notation uncluttered, for the remainder of this subsection we treat gt as fixed within step and omit the subscript t. Lemma 4.1 (Entropic GDRO surrogate and softmax best response). For any η > 0, define the entropyregularized inner problem Rη(θ) max (cid:40) b=1 q(b)Lb(θ) + (cid:41) H(q) , 1 η H(q) b=1 q(b) log q(b). (22) 9 Technical Report"
        },
        {
            "title": "Then",
            "content": "Rη(θ) = 1 η log (cid:32) b=1 (cid:33) eηLb(θ) , and the (unique) maximizer is the softmax distribution qη(b; θ) = exp(ηLb(θ)) j=1 exp(ηLj(θ)) . Moreover, Rη approximates the hard worst-group loss up to log B/η: max b[B] Lb(θ) Rη(θ) max b[B] Lb(θ) + log η . Proof. Appendix B.1.1. (23) (24) (25) Gradient interpretation. Differentiating Eq. (23) yields weighted mixture of group gradients, θRη(θ) = b=1 qη(b; θ) θ Lb(θ), (26) so applying larger weights to higher-loss bins can be read as taking gradient step on smooth approximation to the worst-group objective. Connection to Prompt-GDRO. Eq. (24) is the entropy-regularized best response of the adversary to θ. In an idealized full-information setting where the adversary observes Lb(θt) directly, exponential weights tracks this softmax best response online. In our implementation, the EMA score St(b) (Eq. (12)) is smoothed (and optionally debiased) estimator of the current bin losses, so the bin distribution qt (Eq. (15)) can be interpreted as tracking qηq (; θt) up to bandit noise and exploration. 4.2.2 No-regret interpretation: why this game structure is sensible The entropic GDRO view explains what objective the exponential-weights adversary is tracking. complementary (standard) lens explains why coupling this adversary with gradient-based policy updates is sensible: in convexconcave game, if both players have sublinear regret, time-averaged iterates converge to an approximate saddle point. Concretely, letting (θ, q) b=1 q(b)Lb(θ) and writing θ = 1 t=1 qt, one obtains the generic bound t=1 θt and = 1 T max B ( θ, q) min θΘ (θ, q) Regretθ(T) + Regretq(T) , (27) where Regretθ(T) and Regretq(T) are the cumulative regrets of the learner and adversary, respectively. Appendix B.1.2 states precise version (with step sizes) for the convex bounded regime. In deep RL, we read Eq. (27) as an optimization principle: if (i) the GRPO updates behave like low-regret learner and (ii) the Prompt-GDRO adversary behaves like low-regret reweighting rule, then the training dynamics push down the robust (worst-group) objective over time. 4.2.3 Toy validation on MATH: entropy and worst-group robustness The analysis above predicts that well-behaved Prompt-GDRO adversary should (i) avoid collapsing onto tiny set of bins due to noisy estimates and (ii) improve worst-group performance. We validate these qualitative predictions on the MATH benchmark as toy study (Prompt-GDRO only). The standard GRPO baseline achieves Worst-Group Pass@1 of roughly 33.92%. While class-based GDRO baseline improves this to 37.7%, it exhibits instability in the adversary entropy, collapsing to 10 Technical Report narrow effective support of 12 groups. In contrast, our EMA-debiased formulation achieves the highest robustness with Worst-Group Pass@1 of 39.6%, and maintains broader active support of 24 groups throughout training. This broader support is consistent with the intensive difficulty objective in Eq. (12): instead of chasing few high-variance bins, the adversary sustains diversified portfolio of challenging bins, forcing the policy to improve along wider Pareto frontier of hard prompts. 4.3 Adversary II (Rollout-GDRO): Compute allocation as an economic control problem Prompt reweighting changes which bins receive learning pressure; rollout budgeting changes how much exploration is used to estimate and optimize that pressure. The key empirical fact is that rollouts are not equally useful everywhere: once bin is nearly solved, additional rollouts mostly repeat correct solutions and contribute low-variance gradients; on frontier bins, additional rollouts reduce Monte Carlo noise and improve both reward estimation and GRPOs within-prompt normalization. Recap: compute-neutral rollout budgeting. Rollout-GDRO is defined in Section 3.3. At each step, the controller selects bin-specific rollout count nb {nmin, . . . , nmax} under strict mean-rollout budget n. Operationally, it uses Lagrangian relaxation with multiplier µ (a shadow price of compute): the penalized rollout arm loss Lb(n) is defined in Eq. (20), and µ is updated by dual ascent (Eq. (21)). This closed-loop control is what produces the budget frontier and multiplier effect patterns in Section 5.3. 4.3.1 variance proxy for why allocating more rollouts helps The rollout allocator is most useful when extra rollouts reduce the Monte Carlo noise of the GRPO update. simple proxy is to focus on the stochastic variance of the per-prompt gradient estimate. Even though GRPO normalizes advantages within rollout group (coupling the rollouts for prompt), mild bounded-differences condition on the resulting prompt-level gradient estimator implies the conditional variance still contracts at rate O(1/nb) with the number of rollouts. Abstractly, we can write bin-dependent intrinsic variance parameter vb(θ) so that the per-prompt conditional variance obeys Var[ ˆg(x; θ, nb) gt(x) = b] vb(θ)/nb, and approximate the batch-level noise by VarProxy(n; θ) b=1 qb vb(θ) nb , qb ˆqt(b), (28) where qb are the realized bin fractions in the current batch. Appendix B.2.1 makes this precise (via bounded-differences / EfronStein argument) under i.i.d. rollout sampling, and notes that the condition accommodates GRPOs within-prompt normalization. 4.3.2 The variance-optimal allocation obeys square-root law Motivated by Eq. (28), consider the continuous relaxation of variance-aware compute-neutral allocation: min nRB + b= qb vb(θ) nb s.t. b=1 qb nb = n. (29) Theorem 4.2 (Square-root law for variance-optimal allocation). Let {b [B] : qb > 0} denote the set of bins present in the batch. If vb(θ) > 0 for all A, then the minimizer of Eq. (29) is unique on the active coordinates and equals , A. (30) = Proof. Appendix B.2.2. (cid:112) vb(θ) (cid:113) j=1 qj vj(θ) Implications for Rollout-GDRO. Eq. (30) is the classical Neyman/square-root allocation: bins with higher intrinsic variance receive more rollouts, with vb(θ). The associated KKT condition can (cid:112) 11 Technical Report be written as shadow-price rule: for multiplier µ > 0, the per-bin best response of the continuous relaxation solves nb(µ) = arg min n>0 vb(θ) + µn = vb(θ) µ , (31) (cid:115) and µ is chosen to satisfy the mean-budget constraint. This clarifies why Rollout-GDRO naturally admits an economic interpretation: the dual variable µ trades off variance reduction against compute. In practice we choose nb from discrete set of rollout arms and only observe bandit feedback (the value of the chosen arm). Rollout-GDROs EXP3P updates and DP-based selection step (Section 3.3) can be read as an online approximation to the shadow-price solution above; the resulting bestresponse map is piecewise constant in µ, which matches the staircase/threshold transitions in our rollout heatmaps. We defer the corresponding online-learning formalization (and extensions beyond the variance proxy) to the appendix."
        },
        {
            "title": "5 Experiments",
            "content": "In this section, we present the empirical evaluation of our Multi-Adversary GDRO framework. We use the DAPO 14.1k English dataset2 for all training runs, following standard post-training pipeline. Metrics. Unless stated otherwise, we report mean@8 for benchmark accuracies: each prompt is evaluated with 8 sampled completions and we average the binary correctness indicator across those 8 trials. We additionally report pass@8, the probability that at least one of the 8 completions is correct (estimated from the same samples). This distinction matters for reasoning: mean@8 reflects typical performance, while pass@8 captures best-of-k robustness under limited search. Compute neutrality. Prompt-GDRO keeps the rollout budget fixed and changes training pressure through bin-wise reweighting of GRPO updates. Rollout-GDRO keeps the mean rollout budget fixed (e.g., = 4) and redistributes rollouts across bins. Thus, improvements reflect better use of the same overall training compute rather than larger sampling budget. We first report the quantitative improvements on standard mathematical reasoning benchmarks. Subsequently, we provide rigorous qualitative analysis of the dynamic difficulty landscape, interpreting how the interaction between model capacity and data heterogeneity drives the emergent curriculum observed in our prompt reweighting distribution and rollout allocation strategies. 5.1 Main Results We evaluate our method across three model scales: Qwen3-1.7B-Base, Qwen3-4B-Base, and Qwen38B-Base. We compare the standard GRPO baseline against our two proposed mechanisms: PromptGDRO (adversarial prompt reweighting driven by online difficulty bins) and Rollout-GDRO (adversarial compute budgeting). Table 1 summarizes the performance at the peak checkpoint for each stabilized run. Our framework demonstrates robust gains across all settings. Prompt-GDRO consistently improves performance by explicitly targeting hard data groups, achieving peak gain of +13.13% on the 4B model. Remarkably, Rollout-GDRO achieves comparable or superior resultsimproving the 1.7B model by +10.64% and the 8B model by +9.20%without altering the data distribution. This validates our hypothesis that compute allocation is an equally powerful lever for robustness: by dynamically assigning more rollouts to high-variance prompts, the adversary reduces gradient variance exactly where the model is most uncertain, yielding gains that rival direct data curriculum learning. 2https://huggingface.co/datasets/open-r1/DAPO-Math-17k-Processed 12 Technical Report Table 1: Results on mathematical reasoning benchmarks for Prompt Reweighting GDRO and Rollout Budgeting GDRO vs GRPO Baseline. Bold values indicate methods that outperform the GRPO Baseline (independent comparison). The AIME column reports the average accuracy of AIME 2024 and AIME 2025. The percentage improvement for pass@8 is shown in brackets. All other metrics reported are mean@8. Models MATH 500 AIME AMC MINERVA OLYMPIAD GPQA pass@8 Qwen3-1.7B-Base GRPO (Baseline) Prompt-GDRO Rollout-GDRO Qwen3-4B-Base GRPO (Baseline) Prompt-GDRO Rollout-GDRO Qwen3-8B-Base GRPO (Baseline) Prompt-GDRO Rollout-GDRO Key Finding 1 50.62 63.20 63.98 72.05 75.78 75.20 73.45 76.18 77. 5.42 6.88 7.50 11.25 12.92 13.96 14.38 16.04 15.63 34.69 39.38 36.87 60.94 64.06 67.50 66.56 70.94 66. 14.56 14.61 17.28 17.79 26.72 26.47 28.17 32.17 29.55 23.07 25.07 26.71 30.48 40.98 39.28 36.92 42.43 43. 26.39 29.29 27.72 35.54 40.88 38.51 42.25 43.81 43.31 50.74 55.68 (+9.74%) 56.14 (+10.64%) 56.31 63.70 (+13.13%) 62.27 (+10.59%) 62.04 67.60 (+8.96%) 67.75 (+9.20%) Both prompt reweighting (Prompt-GDRO) and adaptive compute allocation (Rollout-GDRO) independently yield significant performance gains, improving pass@8 accuracy by up to 13.13% and 10.64% respectively across model scales. 5.2 Qualitative Analysis: The Dynamics of Difficulty (Prompt-GDRO) To understand the mechanism behind these performance gains, we visualize the temporal evolution of the training distribution. Figure 3 presents comprehensive triptych tracking the causal chain of our adversarial mechanism: from data availability (prompt share) to adversarial pressure (weights) to learning payoff (reward). Capacity-Dependent Distribution Shift. The training dynamics reveal stark correlation between model capacity and the velocity of learning, as quantified by the macro-level metrics in Figure 4. Qwen3-1.7B (Top Row, Fig 3): The model exhibits high inertia. While the dataset mass often lags in accbin 0, the scalar summary (Figure 4, top panel) shows the mean bin index steadily climbing past 2.0. The mass in bins 3 trace reveals that even this smaller model is successfully pushed out of the trivial zone, preventing the stagnation typical of uniform baselines. Qwen3-4B (Middle Row, Fig 3): This scale occupies Goldilocks zone. The data distribution shows steady migration to intermediate bins. The adversarial weights form distinct, highintensity diagonal frontier that leads the data distribution. By Step 366, the weight entropy stabilizes, indicating the adversary has locked into high-precision curriculum targeting specific intermediate failure modes. Qwen3-8B (Bottom Row, Fig 3): The largest model exhibits rapid collapse of the unsolved mass. The scalar summaries show the mass in bins 8 (dashed lines) spiking early, confirming that for capable models, the adversary aggressively focuses on the last mile of robustnesssolving the few remaining hard casesrather than wasting capacity on solved arithmetic. Visualizing the Wave of Progress. Figure 5 offers discrete checkpoints that clarify the exact distributional shape at key training intervals. The Heavy Tail (1.7B): Even at the final step (Step 605), the 1.7B model retains significant plurality of mass ( 45%) in accbin 0. The distribution remains right-skewed, indicating the reasoning frontier is anchored in fundamental difficulties. 13 Technical Report Figure 3: The Causal Chain of Curriculum. triptych comparing the Prompt Distribution (Left), Adversarial Weights (Middle), and Realized Reward (Right) for 1.7B, 4B, and 8B models. This visualizes the mechanism: even when hard bins are rare in the data (dark regions in Left), the adversary applies disproportionate pressure (bright bands in Middle), forcing the model to eventually crack these problems and yield positive rewards (emergence of red/blue bands in Right). This effectively proves that Prompt-GDRO decouples the learning signal from dataset frequency. The High-Entropy Plateau (4B): By Step 366, the 4B model allocates over 50% of its probability mass to the intermediate accbin 5accbin 7 range. This plateau represents diverse curriculum where the model simultaneously refines intermediate concepts and attempts advanced problems. The Traveling Peak (8B): The 8B model demonstrates clear wave dynamic. The peak of the distribution physically travels from left to right. By Step 430, the mass in accbin 0 has virtually vanished, and the bulk of the sampling budget is dedicated to accbin 8 and accbin 9. This confirms that for sufficient capacity, the primary challenge shifts rapidly from correctness to robustness, necessitating the dynamic budget reallocation our method provides. Adversary leadlag. The heatmaps in Figure 3 suggest that the adversarial weights form frontier that precedes the empirical data distribution. We quantify this intuition with lightweight leadlag proxy computed from logged bin statistics. Let qt denote the empirical prompt share over bins at training step t, and let ˆwt denote the normalized Prompt-GDRO weights across bins at the same step (the weight-only distribution, not reweighted by qt). Define the mean bin index under the data and under the weights as µdata(t) b=1 qt(b), µweight(t) b=1 ˆwt(b), (32) and the leadlag gap µ(t) µweight(t) µdata(t). positive µ(t) indicates that the adversarys weight distribution is shifted toward higher-index bins than the empirical prompt share. Under our convention (Section 3.1) that accbin 0 corresponds to the lowest pass@8 interval and larger indices correspond to higher pass@8 (more solvable) prompts, this means the adversary emphasizes the current learnable frontier rather than simply mirroring the bulk of unsolved mass. Figure 6 (left) shows that µ(t) is strongly positive early in training and decays over time, with faster decay at larger model scales. For Qwen3-8B, µ(t) eventually becomes slightly negative, consistent with the data distribution migrating quickly into high pass@8 bins while the adversary maintains pressure on the remaining low-pass@8 cases. Overall, the decay of µ(t) provides compact scalar signature of the traveling wave curriculum: the adversary leads and the data distribution catches up as the policy improves. Technical Report Figure 4: Training Dynamics Quantified (Prompt-GDRO). (Top) The Mean Accuracy Bin Index tracks the rising difficulty of targeted prompts. (Middle) The fraction of prompts in difficulty bands 3 (solid) and 8 (dashed) highlights scaling laws: 1.7B struggles to clear the 3 bar, while 8B rapidly saturates even the 8 band. (Bottom) Entropy metrics confirm that the adversary maintains diverse portfolio of difficulty, preventing mode collapse to single bin. Key Finding 2 Prompt-GDRO generates an emergent curriculum that decouples the learning signal from dataset frequency. It applies disproportionate pressure to rare, hard bins, creating traveling wave of difficulty that adapts to the models capacity. 5.3 Qualitative Analysis: Adaptive Compute Allocation (Rollout-GDRO) While Prompt-GDRO improves robustness by altering what data the model sees, Rollout-GDRO improves robustness by altering how deeply the model explores that data. To understand this mechanism, we analyze the adversarial budgeters behavior through three complementary visualizations: the continuous budget frontier, macro-level scalar dynamics, and discrete allocation snapshots. 5.3.1 The Budget Frontier Figure 7 visualizes the direct contrast between data frequency and resource allocation. The left column shows the prompt share (dataset mass), while the right column shows the allocated rollout budget per prompt. 15 Technical Report Figure 5: Snapshots of the Learning Distribution. The probability mass of the training set across difficulty bins at four distinct training stages (Start, Early-Mid, Late-Mid, End). These publicationfriendly checkpoints reveal the exact shape of the curriculum: note how the 4B model (Middle Row) transitions from uniform start to heavy emphasis on accbin 5accbin 7 by mid-training, whereas the 8B model (Bottom Row) shifts almost its entire mass to the hardest bins by the final step. This comparison offers the clearest qualitative proof of our methods economic policy. For the 8B model (bottom row), the dataset mass (left) remains concentrated in easier bins for hundreds of steps. However, the budgeter (right) rapidly identifies the emerging capability in accbin 6 and above, locking high-compute resources onto these rare prompts. In contrast, the 1.7B model (top row) takes significantly longer to leave accbin 0, and the budget frontier moves sluggishly, confirming that the adversary adapts the curriculum pace to the models intrinsic scaling laws. 5.3.2 Macro-Dynamics of Allocation To quantify these trends, Figure 8 presents the macro-level training dynamics. The four stacked tracesMean Accuracy Bin Index, High-Bin Mass, High-Bin Budget Share, and Entropyprovide unified view of how Rollout-GDRO migrates compute toward harder domains. The Mass in High Bins trace is particularly revealing. It serves as direct evidence of the DRO objective in action: as the models improve, the adversary steadily reallocates the fixed global budget toward bins 7. This reallocation correlates directly with the inflection points observed in the pass@8 accuracy tables. Furthermore, the shared axes highlight distinct scaling trends: the 8B model (green line) learns to push its budget into high-difficulty bins significantly earlier than the 1.7B model (blue line), validating that larger capacity enables more aggressive curriculum acceleration. Variance-aware compute efficiency. Beyond shifting budget toward harder bins, we can directly test whether the rollout adversary reduces the uncertainty of the bin-wise training signal at fixed compute. Let nb(t) denote the realized number of rollouts per prompt allocated to bin at step t, and let qt(b) denote the prompt share. Using an offline bin-wise variability proxy ˆσb (estimated once from the training logs as the empirical standard deviation of per-prompt GRPO signal within each bin, e.g., the prompt-level loss ℓ(x; θ)), we define weighted standard-error proxy WSE(t) b= qt(b) (cid:112) ˆσb nb(t) . 16 (33) Technical Report µ(t) = (a) Prompt-GDRO leadlag proxy. µweight(t) µdata(t), where µdata(t) = qt(b) and µweight(t) = ˆwt(b). (b) Rollout-GDRO weighted SE proxy. WSE(t) = qt(b) ˆσb/ nb(t) compared to compute-matched uniform baseline. (cid:112) Figure 6: Two diagnostics for the two adversaries. (Left) Prompt-GDRO weights initially lead the empirical data distribution in bin index (a learnable frontier) and progressively align as the prompt-share distribution catches up. (Right) Rollout-GDRO reduces an offline weighted standarderror proxy relative to uniform allocation with the same mean rollout budget, consistent with variance-aware compute allocation. We compare against compute-matched uniform-rollout baseline by setting nb(t) for all bins b, which satisfies the mean-rollout constraint b=1 qt(b) nb(t) = and therefore matches overall sampling compute at each step. As shown in Figure 6 (right), Rollout-GDRO consistently attains lower WSE(t) than the uniform baseline throughout training. Averaged over the plotted horizon, this corresponds to relative reductions of 37.1% (1.7B), 22.6% (4B), and 33.4% (8B), supporting the interpretation that Rollout-GDRO improves gradient information efficiency by allocating more rollouts to high-variance bins. 5.3.3 Discrete Economic Phases Finally, Figure 9 decomposes the continuous training process into discrete chapters of the curriculum. These snapshots clarify the magnitude of the adversarial intervention at key training stages (Start, Early, Mid, Late). The paired bars (Dataset Share vs. Rollout Budget) illustrate massive Multiplier Effect. For example, at Step 300, the 4B model allocates over 80% of its compute budget to bins 5, despite these bins constituting less than 20% of the training data. This confirms that our method creates highly non-uniform economic policy that gives 510 more rollouts to the reasoning frontier than uniform baseline would. This behavior is model-specific: the 8B row shows an even faster shift, embracing high-bin budgeting early in training (Step 118), which aligns with the rapid saturation of easy tasks observed in our qualitative analysis. 17 Technical Report Figure 7: The Budget Frontier. comparison of the datasets natural difficulty distribution (Left) versus the adversarial rollout allocation (Right) for 1.7B, 4B, and 8B models. The color intensity in the Right column represents the number of rollouts assigned per prompt (from purple 2 to yellow 12). Note how the budgeter shifts compute intensity to the transition zone, decoupling resource allocation from data frequency: even when hard bins are rare (dark left), they receive maximum compute (bright right). Key Finding 3 Rollout-GDRO autonomously identifies the transition zone of difficulty and concentrates computational resources there. This results in highly non-uniform economic policy where rare, high-value prompts receive up to 10 more rollouts than uniform baseline, maximizing gradient information efficiency."
        },
        {
            "title": "6 Additional Related Work",
            "content": "Our work studies reasoning post-training at the intersection of (i) RLVR/GRPO-style policy optimization, (ii) distributionally robust optimization (DRO) and group robustness, and (iii) adaptive allocation of training-time compute. 6.1 RLVR and Post-training for Reasoning RL-based post-training has become central for improving reasoning behaviors in LLMs. PPO (Schulman et al., 2017) underpins RLHF-style alignment (Ouyang et al., 2022), while GRPO (Shao et al., 2024) offers value-free, group-normalized alternative that has proven effective for verifiablereward domains such as math. Parallel research improves reward quality and credit assignment through process supervision and step-level signals (Lightman et al., 2023; Wang et al., 2024b), as well as iterative refinement and self-improvement mechanisms (Gulcehre et al., 2023). More recently, large-scale RLVR has enabled open reasoning models trained primarily from verifiable rewards, highlighting the potential of pure RL to elicit sophisticated behaviors such as self-reflection and verification (Guo et al., 2025). However, growing body of work suggests that where RLVR learns and how compute is spent are both highly non-uniform. Token-level analyses indicate that RLVR gains can concentrate on small fraction of high-entropy forking tokens that control reasoning branches (Wang et al., 2025), while 18 Technical Report Figure 8: Macro-Level Allocation Dynamics (Rollout-GDRO). (Top) The Mean Accuracy Bin Index tracks the rising difficulty of targeted prompts. (Middle) The Mass in High Bins trace serves as quantitative evidence of the rollout adversary: it shows the dual variable mechanism keeping the total budget fixed while aggressively reweighting toward hard groups ( accbin 8). (Bottom) Entropy metrics confirm that the 8B model (green) sustains diverse allocation strategy even as it conquers lower difficulties, contrasting with the slower migration of the 1.7B model (blue). other studies argue RLVR may implicitly incentivize correct reasoning patterns already latent in the base model (Wen et al., 2025b). At inference time, test-time scaling via longer thinking traces can be non-monotonic and may induce overthinking (Ghosal et al., 2025), and long-CoT reasoning models can exhibit looping pathologies under low-temperature decoding (Pipis et al., 2025). In parallel, several works explore alternative ways to trade off accuracy and compute, including budgetaware evaluation and compute-normalized comparisons (Wang et al., 2024a), and inference-time orchestration strategies that decouple accuracy from raw CoT length (Madaan et al., 2025). Our work is complementary: rather than proposing new reward or inference strategy, we focus on training-time mechanisms that adaptively steer (a) which prompts are sampled and (b) how many rollouts are allocated, with the goal of improving robustness and compute efficiency. 6.2 Distributionally Robust Optimization in Supervised Learning DRO formalizes robustness by minimizing worst-case risk over an ambiguity set of distributions around the empirical training distribution (Ben-Tal & Nemirovski, 1999; Rahimian & Mehrotra, 2019). In supervised learning, common choice is divergence-based ambiguity sets, yielding objectives that emphasize performance under distribution shift and provide statistical guarantees (Namkoong & Duchi, 2016; Duchi et al., 2021). Wasserstein-based DRO offers an alternative geometry with 19 Technical Report Figure 9: Snapshots of the Allocation Economy. Paired bars at four canonical steps showing the dataset share (dark blue) versus the normalized rollout budget (light blue) for each bin. This visualizes the Multiplier Effect: by Step 300, the 4B model allocates > 80% of its budget to accbin 5+, even though these bins contain < 20% of the data. This explicitly demonstrates how Rollout-GDRO amplifies the signal from rare, high-value prompts. tractable reformulations and strong finite-sample guarantees (Esfahani & Kuhn, 2018). Within deep learning, GDRO (Sagawa et al., 2020) operationalizes robustness to hidden stratification and spurious correlations by optimizing the maximum loss across pre-defined groups, and has become standard tool for improving worst-group accuracy. On the algorithmic side, group-robust learning admits natural game-theoretic and online learning interpretation; in particular, Soma et al. (2022) connect GDRO to no-regret dynamics (e.g., EXP3 variants), which directly motivates our use of GDRO-EXP3P for adversarial prompt reweighting. Our setting departs from classical supervised GDRO in two ways. First, we do not assume static group labels; instead we use an online difficulty classifier based on pass@k to define groups that evolve with the policy. Second, we introduce second adversary that controls compute allocation (rollouts) under budget constraint, extending the DRO perspective beyond data distribution shifts to training-time resource shifts. 6.3 Robust and Distributionally Robust Reinforcement Learning Robust RL traditionally models uncertainty in environment dynamics and seeks policies that perform well under worst-case transition perturbations, e.g., robust Markov decision processes (Iyengar, 2005; Nilim & El Ghaoui, 2005) and distributionally robust MDP formulations (Xu & Mannor, 2012). Recent work develops statistical and computational characterizations of robust RL (Panaganti et al., 2022). In contrast, our work keeps the underlying environment fixed and instead treats prompt difficulty and compute allocation as adversarially controlled quantities during LLM post-training. This yields robustness lens that is closer to group robustness over tasks/prompts than to worst-case transition uncertainty. 6.4 Curriculum, Adaptive Compute, and Data Value Adaptive curricula and compute allocation are increasingly recognized as first-class components of reasoning systems. Curriculum-based post-training pipelines such as Light-R1 (Wen et al., 2025a) explicitly stage data difficulty and objectives (SFT/DPO/RL) to elicit long-CoT behaviors. At 20 Technical Report inference time, scaling laws and compute-allocation policies highlight that difficult instances require more budget, but that naive increases in thinking can be inefficient or unstable (Snell et al., 2024; Ghosal et al., 2025). Recent work proposes learning policies to allocate test-time compute (Setlur et al., 2025) and explores search-based or tree-structured generation to improve exploration of the reasoning space (Xie et al., 2024; Hou et al., 2025). Our Rollout-GDRO can be viewed as moving this idea to training time: we allocate rollout budgets to groups to improve gradient estimator quality under global constraint, akin to adaptive variance reduction (Rubinstein & Kroese, 2016). Recent work has also begun to articulate compute-optimal RL scaling workflows for LLM posttraining by empirically studying how to allocate fixed sampling budget across (i) the number of problems per batch, (ii) the number of parallel rollouts per problem (GRPO group size), and (iii) the number of sequential update steps. In particular, the IsoCompute Playbook reports that computeoptimal rollout parallelism can often be summarized by simple sigmoidal/logit fits as total sampling compute grows (Cheng et al., 2026). In contrast, our approach is more algorithmic: Prompt-GDRO and Rollout-GDRO adaptively reshape the effective prompt distribution and per-group compute online, without assuming pre-fit scaling law. Concretely, we hold the mean sampling budget fixed and redistribute it across online-defined difficulty subgroups within each batch, rather than optimizing compute allocation across global axes such as problems-per-batch, rollouts-per-problem, or number of update steps. Relatedly, Qi et al. (2025) propose Budget Relative Policy Optimization (BRPO) to optimize anytime reasoning performance across varying token budgets, complementing our training-time allocation perspective. Finally, recent theoretical discussion emphasizes that the value of data depends on the learners computational constraints and even on data ordering. The notion of epiplexity formalizes this perspective and motivates principled data selection and dataset interventions (Finzi et al., 2026); see also community discussion (Finzi, 2026). Our work is exploratory in this broader direction: we study whether simple, online, adversarial control loops over prompt reweighting and compute allocation can serve as practical steering subsystem for reasoning post-training, complementing concurrent efforts that analyze RLVR mechanisms (Wang et al., 2025; Wen et al., 2025b), rethink thinking-token tradeoffs (Madaan et al., 2025), and diagnose instabilities such as looping (Pipis et al., 2025)."
        },
        {
            "title": "7 Limitations and Future Work",
            "content": "Bridge: from two adversaries to open questions. Our empirical results suggest that the two adversaries introduced in this workPrompt-GDRO (adaptive prompt reweighting over online difficulty bins) and Rollout-GDRO (adaptive rollout allocation under global compute budget) capture complementary levers for improving reasoning post-training. Both adversaries are driven by the same online difficulty signal (stable binning via empirical pass@k), but intervene at different points in the pipeline: Prompt-GDRO shapes the data distribution presented to the learner, while Rollout-GDRO shapes the per-sample signal-to-noise ratio of policy-gradient updates by modulating rollout counts. This coupling is central to the beyond uniform thesis of our framework, yet our current study is necessarily exploratory and leaves several important questions unresolved. Empirical scope and missing full-factorial ablations. This paper is intended as an exploratory report to the community: we demonstrate that distribution-shaping tools from GDRO-style thinking can be productively instantiated inside modern reasoning post-training stack, but we do not claim to have exhaustively optimized the design space. In particular, we have not yet performed full factorial ablation over all distribution-shaping components and their interactions (e.g., prompt selection/reweighting, compute allocation, and online binning choices). Concrete future work includes: Binning and classifier hyperparameters. We used fixed binning scheme in most experiments; broader sweeps over the number of bins, smoothing horizons, and bin-stability heuristics are needed. In preliminary sweeps we often observed performance peaking around 6 bins, but this is not yet robust conclusion. Joint training with multiple adversaries. Most experiments isolate Prompt-GDRO or RolloutGDRO; systematic study of their joint behavior (and staged curricula) is still missing. Technical Report Rollout allocator design. We only explored limited set of discrete rollout arms and budget schedules; future work should examine broader arm sets, alternative constrained optimizers, and adaptive rollout bounds. RL scaling and compute-optimal post-training. Our experiments are conducted at single (moderate) scale, and we do not yet understand how adversarial prompt reweighting and rollout budgeting interact with emerging RL scaling behavior as total post-training compute grows. Recent work (Liu et al., 2025b;a; Khatri et al., 2025; Cheng et al., 2026) study compute-optimal allocation rules and scaling behavior for RL of LLMs (e.g., by fitting simple sigmoidal/logit trends for optimal rollouts-per-problem under larger budgets). natural next step is to evaluate whether PromptGDRO and Rollout-GDRO shift these compute-optimal frontiers (or their saturation points) across larger compute budgets, model sizes, and prompt mixtures. Adversarial game computation and systems overhead. practical limitation of our approach is that it introduces additional online machinery beyond standard GRPO: (i) computing and maintaining difficulty classifier (pass@k statistics, stable bin assignment), (ii) updating adversarial distributions (EXP3P-style weight updates), and (iii) (for Rollout-GDRO) enforcing compute constraints while selecting discrete rollout arms. As an illustration for trade-offs, we measure the driver-side advantage stage time (timing s/adv, in sec/step) and find that for the Qwen3-4B runs (mean over the last 100 logged steps after warmup) GRPO requires 0.043 sec/step, Prompt-GDRO requires 0.355 sec/step, and Rollout-GDRO requires 0.446 sec/step. This overhead is distinct from our sampling-compute neutrality claims, which refer to the mean rollout budget. We find GDROs adversary/advantage-side bookkeeping can materially increase the driver-side advantage stage, and is one contributor to end-to-end slowdown. While each component is lightweight in isolation, their combination can create nontrivial systems overhead at scale. An important engineering direction is to design asynchronous and streaming variants (e.g., delayed bin updates, batched adversary steps, or partially offloaded bookkeeping) that preserve the core objective while minimizing training slowdown. Sensitivity to online binning and reward noise. Our group notion is induced by an online estimator of difficulty, rather than static metadata. While this avoids reliance on brittle humandefined group labels, it also introduces potential noise sources: estimated pass@k may have high variance early in training, and bin boundaries can induce discontinuous group reassignment. These effects can bias both adversaries if not handled carefully. Future work should explore principled uncertainty-aware binning (e.g., Bayesian estimators or confidence-bound assignment rules), as well as robustness to verifier calibration drift and reward-model nonstationarity. It may also be valuable to incorporate process-level or stepwise supervision signals (when available) to stabilize difficulty estimation, rather than relying solely on outcome-only pass@k. Generalization and evaluation beyond the current pipeline. Although we report improvements on advanced benchmarks, we do not yet provide systematic evaluation protocol for distribution shift. natural next step is to test whether the adversarial curricula learned on one training mixture transfer to new mixtures, or whether they overfit to dataset-specific artifacts. More broadly, our method suggests future direction where GDRO-style training is used not only to improve average in-distribution metrics, but to target robustness to hidden stratification and distribution shift (Sagawa et al., 2020; Oakden-Rayner et al., 2020). Crucially, this should be framed as future work: out-of-distribution generalization is not primary motivation of this paper, but an important downstream question enabled by the methodology. Toward learning from the models own experience. key longer-term direction is to couple our adversarial distribution shaping with experience generation and continual post-training. For example, one can imagine closed loop where the model: (i) proposes new problems or perturbations, (ii) evaluates its own failures, and (iii) uses Prompt-GDRO/Rollout-GDRO to prioritize the resulting frontier. This connects naturally to self-training and self-improvement paradigms such as STaRstyle bootstrapping (Zelikman et al., 2022), Quiet-STaR-style implicit thinking training (Zelikman et al., 2024), self-rewarding / judge-based optimization (Yuan et al., 2024), and RLAIF-style scalable feedback (Lee et al., 2023). Realizing such pipeline requires addressing continual-learning issues 22 Technical Report (e.g., catastrophic forgetting (Rolnick et al., 2019)), maintaining replay buffers (Schaul et al., 2015), and preventing reward hacking under self-generated supervision. Beyond exponential-weights GDRO: richer ambiguity sets and scalable solvers. Our current instantiation uses exponential-weights style updates over discrete set of groups/arms. However, the broader DRO literature offers many alternative ambiguity sets and solution methods that may -divergence DRO admits stochastic-gradient formulations be better suited for future scaling: (Namkoong & Duchi, 2016), and recent work studies computationally efficient large-scale solvers for DRO objectives such as CVaR and χ2-based uncertainty sets (Levy et al., 2020). Wasserstein DRO provides complementary metric-based robustness lens with tractable reformulations and finitesample guarantees (Esfahani & Kuhn, 2018). On the RL side, robust MDP formulations (Iyengar, 2005; Nilim & El Ghaoui, 2005) and scalable ϕ-divergence regularization approaches (Panaganti et al., 2024) suggest additional ways to model uncertainty and allocate resources under environment shift. concrete research agenda is to identify which ambiguity sets best correspond to the operational failure modes of reasoning post-training (e.g., verifier mismatch, data mixture drift, or hard-sample scarcity), and to develop scalable solvers compatible with modern LLM training. Beyond robustness: adversarial reasoning objectives for safety and risk. Finally, our adversaries currently act on what is trained (prompt distribution) and how intensively it is trained (rollout budgets), but not on richer forms of adversarial reasoning objectives. An important future direction is to design adversaries that target specific reasoning desiderata beyond accuracy, such as safety, risk avoidance, and constraint satisfaction. This connects to alignment frameworks that rely on rulebased or AI-generated feedback (Bai et al., 2022; Lee et al., 2023), as well as risk-sensitive optimization objectives. We view this as distinct problem from classical DRO: rather than protecting against distributional uncertainty alone, the goal becomes to adversarially surface and correct undesirable reasoning behaviors (e.g., unsafe tool use, brittle shortcuts, or overconfident hallucinations) under realistic deployment constraints."
        },
        {
            "title": "8 Conclusion",
            "content": "We introduced two multi-adversary GDRO frameworks for reasoning post-training that move beyond the static uniformity of standard GRPO by defining dynamic groups via an online difficulty classifier (stable pass@k binning). On this shared grouping layer, Prompt-GDRO uses an EMA-debiased GDRO-EXP3P reweighting to concentrate updates on persistently hard bins without frequency artifacts, and Rollout-GDRO uses GDRO-EXP3P adversary with shadow-price controller to redistribute rollouts under fixed mean compute budget. In this work, we evaluate Prompt-GDRO and Rollout-GDRO independently (no coupling); studying their joint dynamics is left to future work (7). Across Qwen3-Base scales, both mechanisms are compute-neutral in the sense of 5 yet improve pass@8 over GRPO by up to 13.13% (Prompt-GDRO) and 10.64% (Rollout-GDRO), and diagnostics indicate an emergent curriculum as sampling weights and rollout budgets track the evolving reasoning frontier. We hope these results motivate further work on dynamic grouping and DRO-style training games as principled components of future reasoning post-training pipelines. 23 Technical Report"
        },
        {
            "title": "References",
            "content": "Alekh Agarwal, Nan Jiang, Sham Kakade, and Wen Sun. Reinforcement learning: Theory and algorithms. CS Dept., UW Seattle, Seattle, WA, USA, Tech. Rep, 2019. URL https://rltheorybook. github.io/. Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, et al. Constitutional AI: Harmlessness from AI feedback, 2022. URL https://arxiv.org/abs/2212.08073. Aharon Ben-Tal and Arkadi Nemirovski. Robust solutions of uncertain linear programs. Operations Research Letters, 25(1):113, 1999. doi: 10.1016/S0167-6377(99)00016-4. URL https://www2.isye. gatech.edu/nemirovs/stablpn.pdf. Yoshua Bengio, Jer ˆome Louradour, Ronan Collobert, and Jason Weston. Curriculum learning. In Proceedings of the 26th annual international conference on machine learning, 2009. doi: 10.1145/1553374. 1553380. URL https://ronan.collobert.com/pub/2009_curriculum_icml.pdf. Stephane Boucheron, Gabor Lugosi, and Pascal Massart. Concentration Inequalities: Nonasymptotic Theory of Independence. Oxford University Press, 2013. doi: 10.1093/acprof:oso/9780199535255.001. 0001. URL https://academic.oup.com/book/26549. Oxford University Press, 2013. Stephen Boyd and Lieven Vandenberghe. Convex Optimization. Cambridge University Press, 2004. URL https://web.stanford.edu/boyd/cvxbook/. Sebastien Bubeck. Convex optimization: Algorithms and complexity. Foundations and Trends in Machine Learning, 8(3-4):231357, 2015. doi: 10.1561/2200000050. URL https://arxiv.org/abs/ 1405.4980. Also available as arXiv:1405.4980. Nicol `o Cesa-Bianchi and Gabor Lugosi. Prediction, Learning, and Games. Cambridge University Press, 2006. doi: 10.1017/CBO9780511546921. URL https://www.cambridge.org/core/books/ prediction-learning-and-games/A05C9F6ABC752FAB8954C885D0065C8F. Zhoujun Cheng, Yutao Xie, Yuxiao Qu, Amrith Setlur, Shibo Hao, Varad Pimpalkhute, Tongtong Liang, Feng Yao, Hector Liu, Eric Xing, Virginia Smith, Ruslan Salakhutdinov, Zhiting Hu, Taylor Killian, and Aviral Kumar. Isocompute playbook: Optimally scaling sampling compute for rl training of llms. Online playbook, 2026. URL https://compute-optimal-rl-llm-scaling. github.io/. Accessed: 2026-01-26. John C. Duchi, Peter W. Glynn, and Hongseok Namkoong. Statistics of robust optimization: generalized empirical likelihood approach. Mathematics of Operations Research, 46(3):946969, aug 2021. doi: 10.1287/moor.2020.1085. URL https://arxiv.org/abs/1610.03425. Peyman Mohajerin Esfahani and Daniel Kuhn. Data-driven distributionally robust optimization using the wasserstein metric: Performance guarantees and tractable reformulations. Mathematical Programming, 171(12):115166, 2018. doi: 10.1007/s10107-017-1172-1. URL https://arxiv.org/ abs/1505.05116. Marc Finzi. https://x. Online discussion thread on epiplexity and data value. com/m_finzi/status/2008934727156453661, 2026. URL https://x.com/yaeltriv/status/ 1877490526486003901. Accessed: 2026-01-09. Marc Finzi, Shikai Qiu, Yiding Jiang, Pavel Izmailov, J. Zico Kolter, and Andrew Gordon Wilson. From entropy to epiplexity: Rethinking information for computationally bounded intelligence, 2026. URL https://arxiv.org/abs/2601.03220. Yoav Freund and Robert E. Schapire. decision-theoretic generalization of on-line learning Journal of Computer and System Sciences, 55(1):119139, 1997. and an application to boosting. doi: 10.1006/jcss.1997.1504. URL https://collaborate.princeton.edu/en/publications/ a-decision-theoretic-generalization-of-on-line-learning-and-an-ap/. Soumya Suvra Ghosal, Souradip Chakraborty, Avinash Reddy, Yifu Lu, Mengdi Wang, Dinesh Manocha, Furong Huang, Mohammad Ghavamzadeh, and Amrit Singh Bedi. Does thinking more always help? understanding test-time scaling in reasoning models, 2025. URL https: //arxiv.org/abs/2506.04210. 24 Technical Report Caglar Gulcehre, Tom Le Paine, Srivatsan Srinivasan, Ksenia Konyushkova, Lotte Weerts, Abhishek Sharma, Aditya Siddhant, Alex Ahern, Miaosen Wang, Chenjie Gu, et al. Reinforced self-training (rest) for language modeling, 2023. URL https://arxiv.org/abs/2308.08998. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in LLMs via reinforcement learning, 2025. URL https://arxiv.org/abs/2501.12948. Elad Hazan. Introduction to online convex optimization. Foundations and Trends in Optimization, 2 (3-4):157325, 2016. doi: 10.1561/2400000013. URL https://arxiv.org/abs/1909.05207. Zhenyu Hou, Ziniu Hu, Yujiang Li, Rui Lu, Jie Tang, and Yuxiao Dong. Treerl: LLM reinforcement learning with on-policy tree search, 2025. URL https://arxiv.org/abs/2506.11902. Garud N. Iyengar. Robust dynamic programming. Mathematics of Operations Research, 30(2):257 280, 2005. doi: 10.1287/moor.1040.0129. URL https://www.researchgate.net/publication/ 220442530_Robust_Dynamic_Programming. Devvrit Khatri, Lovish Madaan, Rishabh Tiwari, Rachit Bansal, Sai Surya Duvvuri, Manzil Zaheer, Inderjit S. Dhillon, David Brandfonbrener, and Rishabh Agarwal. The art of scaling reinforcement learning compute for llms, 2025. URL https://arxiv.org/abs/2510.13786. M. Pawan Kumar, Benjamin Packer, and Daphne Koller. Self-paced learning for latent variable models. In Advances in Neural Information Processing Systems (NeurIPS), 2010. URL https://papers. nips.cc/paper_files/paper/2010/hash/e57c6b956a6521b28495f2886ca0977a-Abstract. html. Harrison Lee, Samrat Phatale, Hassan Mansoor, Thomas Mesnard, Johan Ferret, Kellie Lu, Colton Bishop, Ethan Hall, Victor Carbune, Abhinav Rastogi, and Sushant Prakash. RLAIF vs. RLHF: Scaling reinforcement learning from human feedback with AI feedback, 2023. URL https: //arxiv.org/abs/2309.00267. Daniel Levy, Yair Carmon, John C. Duchi, and Aaron Sidford. Large-scale methods for distribuIn Advances in Neural Information Processing Systems (NeurIPS), tionally robust optimization. volume 33, 2020. URL https://proceedings.neurips.cc/paper_files/paper/2020/hash/ 64986d86a17424eeac96b08a6d519059-Abstract.html. Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step, 2023. URL https://arxiv.org/abs/2305.20050. Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Dollar. Focal loss for In Proceedings of the IEEE International Conference on Computer Vision dense object detection. (ICCV), 2017. URL https://openaccess.thecvf.com/content_iccv_2017/html/Lin_Focal_ Loss_for_ICCV_2017_paper.html. Mingjie Liu, Shizhe Diao, Ximing Lu, Jian Hu, Xin Dong, Yejin Choi, Jan Kautz, and Yi Dong. ProRL: Prolonged reinforcement learning expands reasoning boundaries in large language models, 2025a. URL https://arxiv.org/abs/2505.24864. Zihe Liu, Jiashun Liu, Yancheng He, Weixun Wang, Jiaheng Liu, Ling Pan, Xinyu Hu, Shaopan Xiong, Ju Huang, Jian Hu, Shengyi Huang, Johan Obando-Ceron, Siran Yang, Jiamang Wang, Wenbo Su, and Bo Zheng. Part i: Tricks or traps? deep dive into RL for LLM reasoning, 2025b. URL https://arxiv.org/abs/2508.08221. Lovish Madaan, Aniket Didolkar, Suchin Gururangan, John Quan, Ruan Silva, Ruslan Salakhutdinov, Manzil Zaheer, Sanjeev Arora, and Anirudh Goyal. Rethinking thinking tokens: LLMs as improvement operators, 2025. URL https://arxiv.org/abs/2510.01123. Adam Marblestone, Greg Wayne, and Konrad Kording. Toward an integration of deep learning and neuroscience. Frontiers in computational neuroscience, 10:94, 2016. doi: 10.3389/fncom. 2016.00094. URL https://www.frontiersin.org/journals/computational-neuroscience/ articles/10.3389/fncom.2016.00094/full. 25 Technical Report Hongseok Namkoong and John C. Duchi. Stochastic gradient methods for distributionally robust optimization with -divergences. In Advances in Neural Information Processing Systems, volume 29, pp. 22162224, 2016. doi: 10.5555/3157096.3157344. URL https://papers.nips.cc/paper/ 6040-stochastic-gradient-methods-for-distributionally-robust-optimization-with-f-divergences. Arnab Nilim and Laurent El Ghaoui. Robust control of Markov decision processes with uncertain transition matrices. Operations Research, 53(5):780798, 2005. doi: 10.1287/opre.1050.0216. URL https://people.eecs.berkeley.edu/elghaoui/pubs_rob_mdp.html. Luke Oakden-Rayner, Jared Dunnmon, Gustavo Carneiro, and Christopher Re. Hidden stratification causes clinically meaningful failures in machine learning for medical imaging. In Proceedings of the ACM Conference on Health, Inference, and Learning (CHIL), pp. 151159, 2020. doi: 10.1145/3368555. 3384468. URL https://arxiv.org/abs/1909.12475. Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F. Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35:2773027744, 2022. doi: 10.48550/arXiv.2203.02155. URL https://arxiv.org/abs/2203.02155. Kishan Panaganti, Zaiyan Xu, Dileep Kalathil, and Mohammad Ghavamzadeh. Robust reinforcement learning using offline data. In Advances in Neural Information Processing Systems, pp. 32211 32224, 2022. doi: 10.48550/arXiv.2208.05129. URL https://arxiv.org/abs/2208.05129. Kishan Panaganti, Adam Wierman, and Eric Mazumdar. Model-free robust ϕ-divergence reinforcement learning using both offline and online data. In Proceedings of the 41st International Conference on Machine Learning (ICML), volume 235 of Proceedings of Machine Learning Research, 2024. URL https://proceedings.mlr.press/v235/panaganti24a.html. Charilaos Pipis, Shivam Garg, Vasilis Kontonis, Vaishnavi Shrivastava, Akshay Krishnamurthy, and Dimitris Papailiopoulos. Wait, wait, wait... why do reasoning models loop?, 2025. URL https://arxiv.org/abs/2512.12895. Penghui Qi, Zichen Liu, Tianyu Pang, Chao Du, Wee Sun Lee, and Min Lin. Optimizing anytime reasoning via budget relative policy optimization. arXiv preprint arXiv:2505.13438, 2025. Hamed Rahimian and Sanjay Mehrotra. Distributionally robust optimization: review, 2019. URL https://arxiv.org/abs/1908.05659. David Rolnick, Arun Ahuja, Jonathan Schwarz, Timothy Lillicrap, and Gregory Wayne. ExIn Advances in Neural Information Processing Systems perience replay for continual learning. (NeurIPS), volume 32, 2019. URL https://papers.nips.cc/paper_files/paper/2019/hash/ fa7cdfad1a5aaf8370ebeda47a1ff1c3-Abstract.html. Reuven Y. Rubinstein and Dirk P. Kroese. Simulation and the Monte Carlo Method. John Wiley & Sons, 3rd edition, 2016. doi: 10.1002/9781118631980. URL https://www.vitalsource.com/products/ simulation-and-the-monte-carlo-method-reuven-y-rubinstein-dirk-p-v9781118632383. Shiori Sagawa, Pang Wei Koh, Tatsunori B. Hashimoto, and Percy Liang. Distributionally robust neural networks for group shifts: On the importance of regularization for worst-case genIn International Conference on Learning Representations (ICLR), 2020. URL https: eralization. //arxiv.org/abs/1911.08731. Tom Schaul, John Quan, Ioannis Antonoglou, and David Silver. Prioritized experience replay, 2015. URL https://arxiv.org/abs/1511.05952. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms, 2017. URL https://arxiv.org/abs/1707.06347. Amrith Setlur, Matthew Y. R. Yang, Charlie Snell, Jeremy Greer, Ian Wu, Virginia Smith, Max Simchowitz, and Aviral Kumar. e3: Learning to explore enables extrapolation of test-time compute for llms, 2025. URL https://arxiv.org/abs/2506.09026. Technical Report Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Mingchuan Xiao, Y. K. Li, Y. Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models, 2024. URL https://arxiv.org/abs/2402.03300. Abhinav Shrivastava, Abhinav Gupta, and Ross Girshick. Training region-based object detectors with online hard example mining. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016. URL https://openaccess.thecvf.com/content_cvpr_2016/ html/Shrivastava_Training_Region-Based_Object_CVPR_2016_paper.html. Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling llm test-time compute optimally can be more effective than scaling model parameters, 2024. URL https://arxiv.org/abs/2408. 03314. Tasuku Soma, Khashayar Gatmiry, Sharut Gupta, and Stefanie Jegelka. Near-optimal algorithms for group distributionally robust optimization and beyond, 2022. URL https://arxiv.org/abs/ 2212.13669. Martin J. Wainwright and Michael I. Jordan. Graphical models, exponential families, and variational inference. Foundations and Trends in Machine Learning, 1(1-2):1305, 2008. doi: 10.1561/2200000001. URL https://www.nowpublishers.com/article/Details/MAL-001. Junlin Wang, Siddhartha Jain, Dejiao Zhang, Baishakhi Ray, Varun Kumar, and Ben Athiwaratkun. Reasoning in token economies: Budget-aware evaluation of LLM reasoning strategies, 2024a. URL https://arxiv.org/abs/2406.06461. Peiyi Wang, Lei Li, Zhihong Shao, Runxin Xu, Damai Dai, Yifei Li, Deli Chen, Yu Wu, and Zhifang Sui. Math-shepherd: Verify and reinforce LLMs step-by-step without human annotations. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 94269439. Association for Computational Linguistics, 2024b. doi: 10.18653/v1/2024. acl-long.510. URL https://aclanthology.org/2024.acl-long.510/. Shenzhi Wang, Le Yu, Chang Gao, Chujie Zheng, Shixuan Liu, Rui Lu, Kai Dang, Xionghui Chen, Jianxin Yang, Zhenru Zhang, Yuqiong Liu, An Yang, Andrew Zhao, Yang Yue, Shiji Song, Bowen Yu, Gao Huang, and Junyang Lin. Beyond the 80/20 rule: High-entropy minority tokens drive effective reinforcement learning for LLM reasoning, 2025. URL https://arxiv.org/abs/2506. 01939. Liang Wen, Yunke Cai, Fenrui Xiao, Xin He, Qi An, Zhenyu Duan, Yimin Du, Junchen Liu, Lifu Tang, Xiaowei Lv, Haosheng Zou, Yongchao Deng, Shousheng Jia, and Xiangzheng Zhang. Light-r1: Curriculum SFT, DPO and RL for long COT from scratch and beyond, 2025a. URL https://arxiv.org/abs/2503.10460. Xumeng Wen, Zihan Liu, Shun Zheng, Zhijian Xu, Shengyu Ye, Zhirong Wu, Xiao Liang, Yang Wang, Junjie Li, Ziming Miao, Jiang Bian, and Mao Yang. Reinforcement learning with verifiable rewards implicitly incentivizes correct reasoning in base LLMs, 2025b. URL https://arxiv.org/abs/ 2506.14245. Yuxi Xie, Anirudh Goyal, Wenyue Zheng, Min-Yen Kan, Timothy P. Lillicrap, Kenji Kawaguchi, and Michael Shieh. Monte carlo tree search boosts reasoning via iterative preference learning, 2024. URL https://arxiv.org/abs/2405.00451. Huan Xu and Shie Mannor. Mathematics cesses. 10.1287/moor.1120.0540. distributionally-robust-markov-decision-processes. of Operations Research, URL Distributionally robust markov decision prodoi: 37(2):288300, may 2012. https://cris.technion.ac.il/en/publications/ Weizhe Yuan, Richard Yuanzhe Pang, Kyunghyun Cho, Xian Li, Sainbayar Sukhbaatar, Jing Xu, and Jason Weston. Self-rewarding language models, 2024. URL https://arxiv.org/abs/2401.10020. Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah D. Goodman. STaR: Bootstrapping reasoning with reasoning, 2022. URL https://arxiv.org/abs/2203.14465. Eric Zelikman, Georges Harik, Yijia Shao, Varuna Jayasiri, Nick Haber, and Noah D. Goodman. Quiet-STaR: Language models can teach themselves to think before speaking, 2024. URL https: //arxiv.org/abs/2403.09629. 27 Technical Report"
        },
        {
            "title": "A Experiment Details",
            "content": "In this section, we detail the experimental setup, including the shared optimization hyperparameters and the specific configurations for our adversarial mechanisms. All experiments were conducted using the Qwen3-Base model family (1.7B, 4B, 8B) using BFloat16 (BF16) mixed precision and FlashAttention 2. The code and configuration files used to reproduce these results are available at https://github.com/kishanpb/verl-gdro. A.1 Shared Training Hyperparameters All methods (GRPO Baseline, Prompt-GDRO, and Rollout-GDRO) utilize common post-training foundation based on the Group Relative Policy Optimization (GRPO) objective. A.1.1 Optimization & Architecture Global Train Batch Size: 256 Global Validation Batch Size: Total Training Steps: 1000 Optimizer: AdamW Actor Learning Rate: 1 106 KL Penalty Coefficient (βKL): 0.001 PPO Clip Range: [1 ϵlow, 1 + ϵhigh] where ϵlow = 0.2, ϵhigh = 0.28 Advantage Normalization: Yes (Normalized by group standard deviation) Advantage Clipping: [5, 5] A.1.2 Rollout Generation Inference Engine: vLLM Training Rollouts per Prompt (G): 4 (Base setting) Validation Rollouts per Prompt: 8 Sampling Temperature: 0.6 (Training) Top-p: 0.8 Top-k: 20 Reward: Verifiable math correctness with r(x, y) {1, +1} [1, 1], implemented via the math/math-dapo modules in verl. A.2 Adversarial Configuration Our Multi-Adversary framework introduces specific hyperparameters for the EXP3P algorithms governing data sampling and compute allocation. 28 Technical Report A.2.1 Prompt-GDRO (The Data Adversary) This mechanism reweights the prompt distribution based on the intensive difficulty of online groups. Grouping Mechanism: Online Pass@k (10 bins) Adversary Algorithm: EMA-Debiased GDRO-EXP3P Adversary Learning Rate (ηq): 0.65 Exploration Rate (γ): 0.01 Score EMA Decay (β): 0.12 Max Class Weight Cap: 15.0 Loss Normalization: Normalized by class share (to prevent frequency bias) A.2.2 Rollout-GDRO (The Compute Adversary) This mechanism allocates discrete rollout counts nb to minimize gradient variance under global budget constraint. Grouping Mechanism: Online Pass@k (10 bins, edges at 0.1, 0.2, . . . , 0.9) Rollout Arm Range: [nmin, nmax] where nmin = 2, nmax = 12 (Multiplier 3.0 base) Global Budget Constraint ( n): 4 rollouts (average per prompt) Dual Learning Rate (αµ): 0.05 Arm Learning Rate (η): 0. Arm Exploration Rate (γ): 0.01 Arm Score EMA Decay: 0.4 Budget Matching: Exact constrained selection via Dynamic Programming 29 Technical Report Main Theoretical Results: Game-and-Variance View This section develops unified theoretical lens for the two adversarial controllers in our framework: (i) Prompt-GDRO, which adaptively reshapes the prompt distribution to emphasize difficult bins, and (ii) Rollout-GDRO, which adaptively reallocates rollouts across bins to reduce estimation noise under compute budget. Our goal is not to provide deep-network convergence guarantees for GRPO, but rather to (a) formalize the surrogate objectives implicitly optimized by these controllers, and (b) connect their update rules to standard no-regret / mirror-descent analyses that explain the qualitative behaviors observed empirically (e.g., traveling waves and staircase compute allocation). condensed statement of these results (omitting most proofs) appears in Section 4; this appendix provides complete statements and proofs for reference. Throughout, we adopt the GDRO formulation from Section 2 (Eq. (9)). Let g(x) {1, . . . , B} be the (online) grouping rule from Section 4, and define the group losses below. We use {Lb}B b=1 primarily in the Prompt-GDRO analysis; Rollout-GDRO instead optimizes separate budgeted variance objective. Lb(θ) E[ℓ(x; θ) g(x) = b] , {1, . . . , B}, (34) where ℓ(x; θ) is the prompt-level GRPO loss from Section 2.2. B.1 Prompt-GDRO as Entropic GDRO and No-Regret Game Dynamics We start from the canonical finite-group robust objective min θ max f (θ, q), (θ, q) b=1 q(b) Lb(θ), (35) where is the probability simplex over groups. The inner maximization selects worst-case mixture of groups, while the outer minimization trains policy robust to this mixture. B.1.1 Entropy-regularized inner maximization and the log-sum-exp surrogate recurring theme in this paper is that our adversaries are implemented by exponential-weights updates (i.e., entropic mirror descent/ascent). key consequence is that the exact max/min over the simplex is replaced by an entropy-regularized version, yielding smooth soft worst-group objective. The next lemma is standard variational identity (often called the Gibbs variational principle / the convex conjugacy between log-sum-exp and negative entropy); see, e.g., Boyd & Vandenberghe (2004) or Wainwright & Jordan (2008). We include proof for completeness. Lemma B.1 (Entropy-regularized maximum and log-sum-exp). For any Rm and η > 0, (cid:26) max pm p, + (cid:27) = H(p) 1 η 1 η log (cid:32) i=1 (cid:33) eηvi , (36) where H(p) softmax distribution i=1 pi log pi is the Shannon entropy. Moreover, the maximizer is unique and equals the (v) = eηvi j=1 eηvj (i = 1, . . . , m). (37) Proof. Consider the Lagrangian for maximizing pivi 1 pi log pi subject to pi = 1 and η pi 0. At an interior optimum (which holds here because the entropy term makes the objective 3In reward maximization form, one may take Lb(θ) = Jb(θ), where Jb is the group-conditional expected reward (this sign convention is standard in RL; see, e.g., Agarwal et al., 2019). 30 Technical Report strictly concave and favors pi > 0), stationarity gives vi 1 η (log pi + 1) = λ for all i, Here λ is the Lagrange multiplier for the equality constraint pi = 1 (so its sign is unconeηvi . Normalizing yields (37). Plugging (37) into the objective strained); rearranging yields pi gives vi + 1 η H(p) = 1 η log (cid:33) eηvi , (cid:32) i=1 which proves (36). Uniqueness follows from strict concavity of H() on m. Corollary B.2 (Smooth approximation quality). For any Rm and η > 0, max vi 1 η log (cid:32) i=1 (cid:33) eηvi max vi + log η . (38) Proof. Let vmax = maxi vi. Then eηvi eηvmax , giving the lower bound. Also eηvi meηvmax , giving the upper bound. Remark B.3 (KL-robust view and implicit regularization). Let be the uniform distribution over [m]. Using KL(pu) = pi log pi + log and H(p) = pi log pi, (36) is equivalently 1 η log (cid:32) i=1 (cid:33) eηvi = log η + max pm (cid:26) p, KL(pu) (cid:27) . 1 η (39) Thus, the softmax distribution in (37) can be interpreted as the optimizer of KL-penalized DRO objective. In our implementation, this entropy/KL term is not added explicitly to (35); it appears implicitly because the adversary is implemented by entropic mirror ascent (exponential weights). Corollary B.2 quantifies the resulting max-gap: the soft objective approximates the hard max within log m/η. Entropic GDRO surrogate. Applying Lemma B.1 with = and = L(θ) (L1(θ), . . . , LB(θ)) shows that the entropy-regularized inner problem in (35) yields the smooth robust surrogate Rη(θ) max B (cid:40) b=1 q(b)Lb(θ) + (cid:41) H(q) = 1 η 1 η log (cid:32) b=1 (cid:33) eηLb(θ) . (40) Corollary B.4 (Entropic GDRO as surrogate for worst-group loss). For any θ, the entropic surrogate in (40) satisfies max b[B] Lb(θ) Rη(θ) max b[B] Lb(θ) + log η . (41) Equivalently, Rη(θ) is the value of the entropy-regularized variant of the inner maximization in (35). Proof. Apply Corollary B.2 to = L(θ) with = and note that maxi vi = maxb[B] Lb(θ). By Corollary B.4, Rη(θ) is differentiable approximation to maxb Lb(θ). Lemma B.5 (Gradient of the entropic worst-group surrogate). Assume each Lb(θ) is differentiable. Define qη(; θ) by qη(b; θ) exp(ηLb(θ)) j=1 exp(ηLj(θ)) . 31 (42) Technical Report Then θRη(θ) = b=1 qη(b; θ) θ Lb(θ). Proof. This identity follows directly from Danskins theorem applied to the inner maximization in (40). Indeed, is compact and the entropy regularizer makes the inner problem strictly concave in q, hence the maximizer qη(; θ) is unique. We include the short closed-form differentiation of (40) below for completeness: θRη(θ) = 1 η b=1 eηLb(θ) η θ Lb(θ) j=1 eηLj(θ) = b= qη(b; θ) θ Lb(θ). Interpretation. The distribution qη(; θ) in (42) is exactly the entropy-regularized best response of the η H(q)}. Thus qη(; θ) is smooth proxy for adversary to θ: it solves arg maxq the hard worst-group selector in (35): as η , qη(; θ) concentrates on (ties among) arg maxb Lb(θ), whereas for finite η it spreads mass across near-worst groups. This is the same soft best response tracked online by the exponential-weights update used in Prompt-GDRO. {b q(b)Lb(θ) + 1 B.1.2 No-regret dynamics imply approximate robust optimality We now connect Prompt-GDRO to standard minmax optimization via no-regret dynamics. The main message is classical: if the learner (policy) and adversary (group distribution) each run no-regret algorithm, then their time averages approach an approximate saddle point of (35). We provide self-contained theorem in an idealized convex bounded regime, mainly to make the core maximin/no-regret logic behind Prompt-GDRO explicit. Assumption B.6 (Convex bounded regime). Assume Θ Rd is convex and compact with diameter in ℓ2. Assume each group loss Lb(θ) is convex in θ, differentiable, and G-Lipschitz: θ Lb(θ)2 for all θ Θ. Assume the losses are bounded: 0 Lb(θ) for all b, θ. Theorem B.7 (No-regret dynamics yield an approximate GDRO solution). Consider the zero-sum game (35) with payoff (θ, q) 1. (Learner) Online gradient descent on θ with step size ηθ: θt+1 = ΠΘ(θt ηθ gt), where gt is (possibly stochastic) subgradient satisfying E[gt θt, qt] = θ (θt, qt) and conditional second-moment bound E[gt2 b=1 q(b)Lb(θ). Let Assumption B.6 hold. Suppose we run rounds of: 2 θt, qt] G2 sg. 2. (Adversary) Exponentiated-gradient mirror ascent on with step size ηq: qt+1(b) qt(b) exp(ηq ˆLt,b), where ˆLt,b satisfies E[ ˆLt,b θt] = Lb(θt) and 0 ˆLt,b a.s. Let θ 1 t=1 θt and T t=1 qt. Then the expected saddle-point gap satisfies (cid:104) max B ( θ, q) min θΘ (θ, q) (cid:105) D2 2ηθ + ηθ G2 sg 2 + log ηqT + ηq M2 8 . Consequently, since maxq ( θ, q) = maxb Lb( θ) and minθ maxq (θ, q) is the GDRO optimum, E(cid:2) max Lb( θ)(cid:3) min θΘ max Lb(θ) + D2 2ηθ + ηθ G2 sg 2 + log ηqT + ηq M2 8 . (43) (44) Proof. We prove (43) by combining standard regret bounds for OGD (learner) and exponentialweights on the simplex (adversary); see, e.g., Bubeck, 2015; Hazan, 2016; Cesa-Bianchi & Lugosi, 2006. We follow these textbook proofs and include the derivation here mainly to track constants. Technical Report Step 1: learner regret. By non-expansiveness of Euclidean projection and the standard OGD one-step inequality, for any θ Θ, θt+1 θ2 2 θt ηθ gt θ2 2 = θt θ2 2 2ηθgt, θt θ + η θ gt2 2. Rearranging and summing over = 1, . . . , yields t=1 gt, θt θ θ1 θ2 2 2ηθ + ηθ 2 t=1 gt2 2 D2 2ηθ + ηθ 2 t=1 gt2 2. Taking conditional expectation and using E[gt θt, qt] = θ (θt, qt), convexity of (, qt), and the conditional second-moment bound E[gt2 2 θt, qt] G2 sg gives (cid:34) t=1 (cid:35) (θt, qt) (θ, qt) D2 2ηθ + ηθ G2 sgT 2 . Dividing by gives the learners average regret bound. Step 2: adversary regret. Let ut RB denote the (possibly estimated) payoff vector with entries ˆLt,b. Exponentiated-gradient mirror ascent on the simplex with entropy regularizer satisfies ut,b the standard bound: for any B, t=1 q, ut qt, ut log ηq + ηq 8 t=1 ut2 , where the constant 1/8 follows from Hoeffdings lemma when ut,b [0, M]. Since ut a.s., we have (cid:35) (cid:34) t=1 q, ˆLt qt, ˆLt log ηq + ηq M2T . Using unbiasedness E[ ˆLt,b θt] = Lb(θt) yields the same bound with L(θt) in place of ˆLt. Step 3: combine regrets into saddle-point gap. The learner regret implies 1 t=1 (θt, qt) min θΘ 1 T t=1 (θ, qt) + D2 2ηθ + ηθ G2 sg 2 . The adversary regret implies max 1 t=1 (θt, q) 1 T t=1 (θt, qt) + log ηqT + ηq M2 8 . Combining and using Jensens inequality, max f ( θ, q) max 1 t= (θt, q) min θΘ 1 t=1 (θ, qt) + D2 2ηθ + ηθ G2 sg 2 + log ηqT + ηq M2 8 . Finally, convexity of (θ, ) in implies minθΘ (θ, q) minθΘ 1 (43). The suboptimality bound (44) follows since maxq f ( θ, q) = maxb Lb( θ). t=1 (θ, qt). Rearranging yields Remark B.8 (Reading Theorem B.7 in the deep RL regime). Assumption B.6 is not satisfied by neural policies trained with GRPO, so Theorem B.7 should be read as an idealized online-learning lens rather than literal convergence guarantee. It formalizes the conceptual statement that if (i) the policy updates behave like low-regret learner and (ii) the group reweighting behaves like low-regret 33 Technical Report adversary (implemented via exponential weights), then the time averages approximate GDRO saddle point. Empirically, several qualitative predictions of this lens appear in our Prompt-GDRO dynamics: the adversary maintains non-degenerate, entropy-regularized weight distribution over bins (see the entropy traces in Figure 4), and the weights form traveling wave frontier that leads the empirical prompt-share distribution early in training (visible in the triptych of Figure 3 and summarized by the leadlag proxy in Figure 6a). In practice, Prompt-GDRO uses bandit feedback (we only observe losses for sampled prompts/bins), hence our implementation uses the GDRO-EXP3P estimator/updates of Soma et al. (2022), which preserve no-regret guarantees under partial information. Remark B.9 (Where Rollout-GDRO enters the saddle-gap bound). The learner term ηθ 2 in 2 Theorem B.7 makes explicit that estimation noise impacts the constants in our no-regret analysis. In GRPO, gt is formed from Monte Carlo rollouts, and is therefore stochastic even when conditioning on (θt, qt). To separate optimization from estimation effects, write gt = θ (θt, qt) + ξt with E[ξt θt, qt] = 0. Then t=1 gt2 Egt2 2 = θ (θt, qt) 2 + Eξt2 2. Now consider batch of prompts whose (empirical) bin fractions are qt,1:B and whose per-bin rollout allocation is nt = (nt,1, . . . , nt,B). Under Lemma B.15, the stochastic component of the batch gradient obeys the proxy bound Eξt2 2 1 b= qt,b vb(θt) nt,b , where vb(θt) is the intrinsic per-bin gradient-variance term. Rollout-GDRO is designed to adaptively choose nt to reduce this variance proxy under mean compute constraint. In Section B.2.4, we show that the rollout controller can itself be viewed as no-regret primaldual algorithm for budgeted variance objective, making precise (in an idealized model) how variance-aware compute allocation tightens the gt2 2 term relative to uniform rollouts. B.2 Rollout-GDRO as Variance-Aware Allocation Under Budget and No-Regret Game Dynamics We now analyze the second controller: allocating the number of rollouts per prompt as function of group/bin. The key idea is classical: if some bins induce higher intrinsic variance (more stochastic rewards / longer reasoning / more fragile completions), then allocating more rollouts to these bins reduces gradient variance and improves stability. B.2.1 variance proxy for GRPO rollouts Reference equations from the main paper. In the main paper (Section 4), Rollout-GDRO is posed as compute-neutral allocation problem driven by empirical bin utilities. At training step t, the allocator observes the realized bin fractions ˆqt in the current prompt batch and chooses discrete rollout counts nb {nmin, . . . , nmax} for each bin under the mean rollout budget constraint in (19) (compute neutrality). The bin-level quantity ˆJb(θ; nb) in (18) is itself estimated from nb rollouts per prompt, so changing nb affects both the quality (variance) and possibly the bias of the signal provided to the learner. What is guaranteed by the rollout controller vs. what is explained by the variance proxy. Equation (19) is the implemented allocation problem: at each step, the controller selects discrete arms {nb} to maximize an empirical utility under strict mean-rollout constraint. This can be cast as an online constrained bandit problem by defining an (arm) loss Vb(n; θ) ˆJb(θ; n) (or any bounded monotone transform thereof), and augmenting it with the linear budget penalty µn in (20). Our no-regret result in Section B.2.4 applies to this generic primaldual EXP3P controller for (19). The variance-proxy analysis below is separate guarantee: it upper-bounds the conditional variance of GRPOs Monte Carlo gradient estimator as function of the allocation n, yielding the proxy Technical Report objective qb vb(θ)/nb (Eq. (29)). In particular, if the controller is instantiated with feedback that estimates (minus) this proxy cost, then the same no-regret machinery yields provably near-optimal variance-minimizing allocation. Our theoretical results in this subsection focus on the stabilizing effect of increasing nb through variance reduction. Formally, we derive simple proxy for the stochastic component of the GRPO batch gradient as function of the rollout allocation = (n1, . . . , nB), which yields an optimization problem of the form minn qb vb(θ)/nb under the same mean budget. Informally, the bin-dependent quantity vb(θ) measures how noisy the per-rollout GRPO gradient is within bin b: larger vb(θ) means completions in that bin yield higher-variance gradients, so averaging more rollouts (larger nb) is more valuable there. For the cleanest bound, we define vb(θ) as uniform (worst-case) upper bound over prompts in bin b; see Lemma B.15. This makes explicit how Rollout-GDRO can be interpreted as variance-aware compute redistribution in the sense visualized by our weighted ˆqt(b) for the realized standard error diagnostics. For clarity, we fix training step and write qb bin fractions. That is, for the step-t prompt batch {xi}M i=1 1{g(xi) = b}, so qb is the empirical fraction of bin-b prompts in the batch. Fix prompt and policy parameters θ. Let {yj}nb j=1 denote nb rollouts sampled from πθ( x). In GRPO, these rollouts are used to form prompt-level empirical loss (cf. Section 2.2), which may involve within-prompt normalization across the rollout group (e.g., the standardized advantages in (3)). We write this generic prompt-level estimator as ˆℓ(x; θ, nb) and define the corresponding per-prompt gradient estimator i=1, ˆqt(b) 1 M ˆg(x; θ, nb) θ ˆℓ(x; θ, nb), g(x) = b. (45) The analysis below only requires that the rollouts are i.i.d. and that ˆg is (possibly coupled) function of these rollouts. Assumption B.10 (Rollout sampling model and differentiation under the expectation). Conditional on prompt and parameters θ, the rollouts y1, . . . , ynb are drawn i.i.d. from πθ( x). Moreover, the prompt-level estimator ˆℓ(x; θ, nb) is differentiable in θ and we may interchange gradient and conditional expectation: θ Lemma B.11 (Connecting ˆJb(θ; nb) to per-prompt gradient estimators). Recall that ˆℓ(x; θ, nb) denotes ˆℓ(x; θ, nb). If the prompt-level empirical loss estimator computed from nb rollouts, and ˆg(x; θ, nb) = θ bin-level estimator ˆJb(θ; nb) (as in (18)) is formed by averaging ˆℓ(x; θ, nb) over the prompts in bin in the current batch (up to the sign convention Lb = Jb), then θ ˆJb(θ; nb) is the corresponding average of the per-prompt estimators ˆg(x; θ, nb). E[ˆℓ(x; θ, nb) x] = E[θ ˆℓ(x; θ, nb) x]. Proof. Both claims follow immediately from linearity of averaging and differentiation. Assumption B.12 (Bounded differences for the prompt-gradient estimator). For any prompt with g(x) = and any rollout count nb, the estimator ˆg(x; θ, nb) is measurable function of the rollout group (y1, . . . , ynb ). Assume there exists (bin-dependent) constant Cb(θ) 0 such that, for every coordinate {1, . . . , nb} and any replacement rollout j, the estimator satisfies the bounded-differences property (cid:13) (cid:13) ˆg(x; θ, nb; y1, . . . , yj, . . . , ynb ) ˆg(x; θ, nb; y1, . . . , j, . . . , ynb )(cid:13) (cid:13) Cb(θ) nb . (46) Lemma B.13 (GRPO prompt-gradient satisfies bounded differences under within-group normalization). Consider fixed prompt in bin and nb i.i.d. rollouts y1, . . . , ynb πθ( x) with bounded rewards nb r(x, yj) [0, R]. Let 1 j=1(rj r)2, and define standardized advantages rj nb (rj r)/(s + ε) for some fixed ε > 0. Suppose the (per-rollout) score function is uniformly bounded, j=1 rj, (cid:113) 1 nb nb Aj i.e., (cid:13)θ log πθ(y x)(cid:13) (cid:13) (cid:13)2 Gπ for all (x, y, θ). (47) 35 Technical Report Consider the normalized prompt-gradient estimator ˆg(x; θ, nb) 1 nb nb j= Aj θ log πθ(yj x). Then Assumption B.12 holds with constant of the form Cb(θ) Gπ (cid:16) 3R2 ε2 + (cid:17) . 5R ε (48) (49) The same conclusion holds (up to replacing Gπ by an appropriate bound on the per-rollout GRPO score term) when additional bounded multiplicative factors are present, such as PPO ratio clipping. Proof. Fix an index {1, . . . , nb} and let = (r1, . . . , rnb ) and denote the reward vectors that differ only at coordinate (corresponding to replacing yk by some alternative rollout k). Let (r, s) and (r, s) denote the corresponding means and standard deviations, and define Aj and from and r. First, the mean changes by at most r rk k nb nb . (50) Next, writing the (biased) sample variance as s2 = 1 nb vs. unbiased only changes constants), we have r2 r2 (and similarly s2; biased v 1 nb R2 nb r2 2 + r2 r2 R2 nb + r + + nb 2R 3R2 nb . (51) By the identity = v/(s + s) (interpreting the right-hand side as 0 when = = 0), s v + 3R2 nb (s + s) . (52) Now decompose the change in the estimator (48) as (cid:13) ˆg ˆg(cid:13) (cid:13) (cid:13) 1 nb nb j=1 (cid:13) (cid:13)(Aj j)θ log πθ(yj x)(cid:13) (cid:13) + (cid:13) (cid:13)A 1 nb (cid:0)θ log πθ(yk x) θ log πθ(y x)(cid:1)(cid:13) (cid:13)2. Using (47) and k kr ε ε , the second term in (53) is at most 2RGπ εnb . (53) For the first term, we bound the total change in standardized advantages. For any = (so rj = j), we can write Aj j = (cid:12) rj (cid:12) (cid:12) (cid:12) + ε (cid:12) r + ε rj s + ε (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) + rj (cid:12) (cid:12) (cid:12) (cid:12) 1 + ε 1 + ε (cid:12) (cid:12) (cid:12) (cid:12) . 36 (54) Technical Report Summing (54) over = and using (50) gives r + ε j=k (nb 1) R/nb ε ε . (55) For the denominator term, note that 1 nb s+ε 1 s+ε nb j(rj r)2 = nbs. Combining with (52) and (51) yields = ss (s+ε)(s+ε) ss j=1 rj (cid:113) ε2 . Also, by CauchySchwarz, rj j=k (cid:12) (cid:12) (cid:12) (cid:12) 1 + ε 1 + ε (cid:12) (cid:12) (cid:12) (cid:12) nbs ε2 3R2 nb(s + s) 3R2 ε2 . (56) Finally, using the crude bound Ak obtain Ak + k 2R/ε for the changed coordinate, we nb j=1 Aj j 3R2 ε2 + 3R ε . (57) Plugging (57) into the first term of (53) and combining with the second-term bound gives ˆg ˆg2 Gπ nb ε ), which is the stated bounded-differences form. ε2 + 5R ( 3R2 Lemma B.14 (A 1/n variance bound (covers within-prompt normalization in GRPO)). Assume Assumptions B.10 and B.12 hold. Then for any prompt with g(x) = b, (cid:104) ˆg(x; θ, nb) E[ ˆg(x; θ, nb) x]2 2 (cid:105) (cid:12) (cid:12) (cid:12) Cb(θ)2 2nb . (58) In particular, defining vb(θ) Cb(θ)2/2 yields the convenient form E[ ˆg(x; θ, nb) E[ ˆg(x; θ, nb) x]2 2 x] vb(θ)/nb. Proof. Fix and write = (y1, . . . , ynb ) for the rollout group. Let Y(j) denote the vector obtained by replacing only the j-th coordinate yj with an independent copy πθ( x). Fact (vector-valued EfronStein). The EfronStein inequality extends to Rd-valued estimators with 2 (more generally, Hilbert space-valued) by applying the scalar inequality coordinatewise and summing; see, e.g., Boucheron et al. (2013, Ch. 3). The EfronStein inequality (vector-valued EfronStein / Hilbert space version) implies (cid:104) ˆg(Y) E[ ˆg(Y) x]2 2 (cid:105) (cid:12) (cid:12) (cid:12) 1 2 nb j=1 (cid:20)(cid:13) (cid:13) (cid:13) ˆg(Y) ˆg (cid:16) Y(j)(cid:17)(cid:13) (cid:13) (cid:13) 2 (cid:21) (cid:12) (cid:12) (cid:12) . By Equation (46), each summand is at most Cb(θ)2/n2 b. Summing over gives (58). Next, consider batch of prompts {xi}M rollout counts, and define the batch gradient estimator i=1. Let = (n1, . . . , nB) denote the vector of per-bin ˆg(θ; n) 1 i=1 ˆg(xi; θ, ng(xi)). (59) Lemma B.15 (A variance proxy decomposes over groups). Assume that, conditioned on the batch prompts {xi}M i=1, the rollout groups used to form the per-prompt estimators ˆg(xi; θ, ng(xi)) are independent across i. Fix batch of prompts {xi}M i=1 1{g(xi) = b}. i=1 and define the empirical bin fractions qb 1 37 Technical Report Then, conditioned on the batch prompts {xi}M i=1, (cid:104)(cid:13) (cid:13) ˆg(θ; n) E[ ˆg(θ; n) {xi}M i=1](cid:13) (cid:13) (cid:12) (cid:12) {xi}M (cid:12) i=1 2 (cid:105) 1 b=1 qb vb(θ) nb , (60) where vb(θ) is any uniform bin-wise constant such that the per-prompt conditional variance obeys E[ ˆg(x; θ, nb) E[ ˆg(x; θ, nb) x]2 2 x] vb(θ)/nb for all prompts with g(x) = b. Under Assumption B.12 and Lemma B.14, we may take vb(θ) = Cb(θ)2/2. Proof. Write ˆg(θ; n) E[ ˆg(θ; n) E[ ˆg(xi; θ, ng(xi)) xi]. Conditioned on the prompts, {Zi}M i=1] = 1 {xi}M i=1 Zi, where Zi ˆg(xi; θ, ng(xi)) i=1 are independent and zero-mean. Thus, (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) 1 i=1 Zi (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) 2 (cid:12) (cid:12) {xi}M (cid:12) i=1 = (cid:104) 1 M i=1 Zi2 2 xi (cid:105) 1 M i=1 vg(xi)(θ) ng(xi) , where the last inequality uses Lemma B.14 and the definition of vb(θ). Grouping terms by bins yields 1 M2 , which proves (60). i:g(xi)=b b=1 qb = 1 vb(θ) nb vb(θ) nb B b=1 Remark B.16 (From variance proxy to the plotted weighted standard error). The quantity on the right-hand side of (60) is variance proxy. In experiments we additionally visualize the more interpretable weighted standard error proxy qb vb(θ)/nb. This proxy is often easier to vb(θ)/nb behaves like standard error: it is within-bin scale interpret because each term nb rate when using nb rollouts of the prompt-gradient noise, which shrinks at the canonical 1/ (Lemma B.14), and the weights qb simply average these standard-error contributions across the observed batch composition (see Figure 6b). By CauchySchwarz, (cid:112) (cid:112) (cid:33)2 (cid:115) (cid:32) b=1 qb vb(θ) nb (cid:32) b=1 (cid:33) (cid:32) b= qb qb vb(θ) nb (cid:33) = b= qb vb(θ) nb . Thus, minimizing the variance proxy also controls the weighted standard-error proxy we plot. B.2.2 The variance-optimal allocation obeys square-root law Lemma B.15 suggests that, under compute neutrality, the rollout allocation controls leading-order proxy for the stochastic component of the learners update through the term qb vb(θ)/nb. This motivates studying variance-aware relaxation of the allocator objective (18), in which the marginal value of additional rollouts is captured by the reduction in estimator variance. Concretely, we fix θ and treat vb(θ) and qb as constants for given step (with qb = ˆqt(b)), and we analyze the continuous relaxation min nRB + b=1 qb vb(θ) nb s.t. b= qb nb = n. (61) This objective is convex in (each term is 1/nb), and the constraint is linear. Theorem B.17 (Square-root law for variance-optimal allocation). Let {b [B] : qb > 0} denote the set of bins present in the batch. If vb(θ) > 0 for all A, then the minimizer of (61) is unique on the active coordinates and is = (cid:112) vb(θ) (cid:113) j=1 qj vj(θ) , A. (62) For bins with qb = 0 (i.e., / A), the variable nb does not affect the objective nor the budget constraint and may be chosen arbitrarily. 38 Technical Report Moreover, the minimal objective value equals b=1 qb vb(θ) = (cid:16) b=1 qb (cid:17) (cid:112) vb(θ) . Compared to the uniform allocation nb n, the optimal allocation never increases the variance proxy: b= qb vb(θ) b=1 qb vb(θ) = b=1 qbvb(θ) , with equality iff vb(θ) is constant across the active bins A. (63) (64) Proof. If vb(θ) = 0 for some active bin A, then its term qbvb(θ)/nb is identically zero, so allocating additional rollouts to that bin cannot reduce the variance proxy; in that degenerate case one may treat such bins as free and apply the same square-root allocation to the remaining bins with positive vb(θ) (in our discrete implementation, this corresponds to choosing the smallest rollout arm). Form the Lagrangian (with multiplier µ, matching the shadow price interpretation in (19)) L(n, µ) = b=1 qb vb(θ) nb + µ (cid:32) b=1 (cid:33) qbnb . Bins with qb = 0 do not appear in the objective nor the constraint in (61), so we may ignore them and restrict attention to the active set A. Stationarity gives, for each active bin (so qb > 0), qb vb(θ) n2 + µ qb = 0 = nb = (cid:115) vb(θ) µ . Enforcing the constraint yields qb vb(θ)/µ = n, hence , and substituting gives (62). Uniqueness on the active coordinates follows because the objective in (61) is strictly convex in {nb}bA when vb(θ) > 0. Plugging (62) into the objective yields (63). µ = (cid:112) qj vj(θ) Finally, to show (64), apply CauchySchwarz: (cid:33)2 (cid:113) qb vb(θ) (cid:32) b=1 (cid:32) b=1 (cid:33) (cid:32) b=1 qb (cid:33) qbvb(θ) = b=1 qbvb(θ). Divide by and use (63) to obtain (64). Equality holds iff (cid:112) vb(θ) is constant across A. Remark B.18 (Interpretation). Theorem B.17 shows that the variance-optimal allocation equalizes the marginal variance proxy across bins: at the optimum, vb(θ)/n2 = µ is constant in (the KKT multiplier / shadow price). Consequently, bins with larger intrinsic variance vb(θ) receive more (cid:112) rollouts, with per-prompt rollouts scaling as vb(θ). This square-root law is the same structure as the classical Neyman allocation in stratified sampling and Monte Carlo budgeting (see, e.g., Rubinstein & Kroese, 2016). Notably, the bin fraction qb cancels from the stationarity condition, meaning that the marginal value of extra rollouts depends on vb(θ) rather than frequency. At the same time, the budget is weighted by qb, so rare bins can receive large without violating compute neutrality qbnb = n. See the rollout heatmaps and time snapshots in Figures 7 and 9 for the corresponding piecewise-constant (staircase) transitions between discrete rollout arms as the shadow price µ adapts. This square-root allocation intuition is consistent with the empirically observed rollout-allocation patterns in Rollout-GDRO (see, e.g., Figures 7 and 9), where bins deemed noisier receive larger rollout arms under fixed mean budget. 39 Technical Report B.2.3 Discrete rollout arms and an entropic primaldual view The continuous analysis in Section B.2.1Section B.2.2 yields closed-form allocation for the idealized variance proxy objective when the rollout counts {nb} are allowed to be any positive reals. RolloutGDRO, however, must choose discrete rollout counts nb = {nmin, . . . , nmax} and enforce compute neutrality online. Mirroring the Lagrangian perspective in the main paper (cf. (19)), we view the allocator as maintaining dual variable µ that acts as shadow price of compute. To connect the discrete implementation to the variance proxy derived above, we consider an entropy-regularized relaxation of the same constrained problem. Concretely, for each bin we maintain distribution pb over rollout arms . We model the per-bin cost of choosing arm as function Vb(n; θ) that decreases with n. In the variance-proxy setting of Section B.2.1, canonical choice is Vb(n; θ) = vb(θ)/n. (Equivalently, one may interpret the utility in (18) as monotone transform of Vb; our analysis isolates the variance-reduction component that is most directly controlled by n.) The entropy-regularized Lagrangian for fixed θ takes the form L({pb}, µ; θ) = (cid:18) b=1 qb Enpb [Vb(n; θ)] (cid:19) + µ H(pb) 1 η (cid:32) b=1 (cid:33) qb Enpb [n] , (65) where µ 0 is the shadow price and the entropy term encourages exploration over arms (stability). Lemma B.19 (Soft-min solution for rollout arms). For fixed µ and bin b, the minimizer of (65) over pb is b,µ(n) = exp(cid:0) η [Vb(n; θ) + µn](cid:1) nN exp(cid:0) η [Vb(n; θ) + µn](cid:1) . (66) Moreover, the optimal value of the inner minimization equals soft-min: (cid:26) min pbN Enpb [Vb(n; θ) + µn] (cid:27) 1 η H(pb) = 1 η log (cid:32) nN (cid:33) eη(Vb(n;θ)+µn) . (67) Proof. Apply Lemma B.1 to the vector RN with entries vn (cid:0)Vb(n; θ) + µn(cid:1). Then (cid:26) p, + (cid:27) = H(p) 1 η max pN 1 η log nN eηvn = 1 η log nN eη(Vb(n;θ)+µn). Negating both sides turns the maximization into the minimization in (67). The maximizer in Lemma B.1 becomes the minimizer here and equals (66). Corollary B.20 (Soft-min approximation quality). Let sminη(z) 1 RK, η log i=1 eηzi . Then for any sminη(z) min zi sminη(z) + log η . (68) Proof. Apply Corollary B.2 to = z. Remark B.21 (Shadow price and the staircase compute pattern). Lemma B.19 makes explicit that µ acts as shadow price on rollouts: increasing µ shifts probability mass in (66) toward smaller rollout counts n. In our implementation, µ is updated by (projected) dual ascent on the budget violation, so the system dynamically finds price at which the expected rollout budget is approximately satisfied. This primaldual viewpoint also clarifies the connection to the square-root law. If we set Vb(n; θ) = vb(θ)/n and temporarily relax to be continuous, then the unregularized best response to fixed price µ solves minn>0 vb(θ)/n + µn, whose minimizer is nb(µ) = (cid:112) vb(θ)/µ. Choosing µ to satisfy the mean-budget constraint recovers the closed-form allocation in Theorem B.17. With finite discrete arm set , the minimizing arm is the nearest available rollout count to vb(θ)/µ, so as µ changes the optimizer can switch abruptly between arms. This explains the staircase patterns (cid:112) 40 Technical Report observed in the rollout-allocation figures. See Figures 7 and 9 for these staircase-like transitions between discrete rollout arms as the learned shadow price µ changes. B.2.4 No-regret primaldual analysis for Rollout-GDRO The previous subsection gave (regularized) Lagrangian view of the rollout controller. Here we show that the same structure admits clean no-regret interpretation, mirroring the Prompt-GDRO analysis in Section B.1. Throughout this subsection, we idealize the rollout controller as operating on fixed rollout-cost landscape, i.e., we condition on fixed policy parameter θ and fixed group fractions B. This subsection analyzes Rollout-GDRO as an independent online budget-allocation game for fixed (θ, q). (We comment on the coupled, multi-time-scale setting in Remark B.24.) truncated Lagrangian game. Let = {nmin, . . . , nmax} be the discrete set of rollout arms and denote . For each bin b, the rollout controller chooses distribution pb over arms; write = (p1, . . . , pB). Let Vb(n; θ) 0 denote the (idealized) variance proxy for choosing rollouts in bin at policy θ (e.g., Vb(n; θ) = vb(θ)/n as in Equation (61)). Let a(n) denote the compute cost of arm n. We consider the convexconcave truncated Lagrangian Lroll(p, µ; θ) b= qb Enpb [Vb(n; θ)] + µ (cid:32) b=1 (cid:33) qb Enpb [a(n)] , µ [0, µmax], (69) where µ is the shadow price and µmax is chosen truncation level (projection radius). When µmax is large, maximizing over µ approximately enforces the budget constraint. Primaldual updates. natural no-regret dynamics for (69) is: pt+1,b = arg min pbN (cid:26) (cid:10)pb, ℓt,b (cid:11) + 1 ηp KL(pb pt,b) (cid:27) , ℓt,b(n) qb (cid:0)Vb(n; θ) + µta(n)(cid:1), (70) µt+1 = Π [0,µmax] (cid:0)µt + ηµ gt (cid:1) , gt b=1 qb Enpt,b [a(n)] n, (71) where Π [0,µmax] is Euclidean projection and KL() is the KL divergence. The update (70) is entropic mirror descent (a.k.a. exponentiated gradient) on the simplex, applied independently in each bin. Lemma B.19 shows that for fixed µ, the minimizer has Gibbs / soft-min form; the dynamics (70) tracks this solution online as µt evolves. Theorem B.22 (No-regret guarantee for Rollout-GDROs primaldual controller). Assume that for all and we have 0 Vb(n; θ) Vmax and 0 a(n) amax. Let = and initialize p1,b to the uniform distribution on and µ1 = 0. Run the updates (70)(71) for steps with step sizes ηp, ηµ > 0. Define the averaged iterates pb satisfies 1 t=1 pt,b and µ 1 t=1 µt. Then the (truncated) saddle-point gap max µ[0,µmax] Lroll( p, µ; θ) min p(N )B Lroll(p, µ; θ) log ηpT + ηp 8 (Vmax + µmaxamax)2 + In particular, the explicit step sizes ηp (cid:112) 8B log (Vmax + µmaxamax) , ηµ µmax amax , µ2 max 2ηµT + a2 max. ηµ 2 (72) (73) Technical Report yield the concrete bound GapT (Vmax + µmaxamax) (cid:114) log 2T + µmaxamax . Moreover, let be an optimal solution of the budgeted variance problem min p(N )B b=1 qb Enpb [Vb(n; θ)] s.t. b=1 qb Enpb [a(n)] n. Then the averaged rollout policy is nearly optimal and nearly feasible: b= qb En pb [Vb(n; θ)] b=1 qb np [Vb(n; θ)] GapT, (74) (75) (76) (cid:34) b=1 (cid:35) qb En pb [a(n)] + b=1 qb [Vb(n; θ)] + GapT np µmax Vmax + GapT µmax , (77) where GapT denotes the right-hand side of (72). Proof. We apply standard two-player no-regret-to-equilibrium argument to the convexconcave game (69); see, e.g., Cesa-Bianchi & Lugosi, 2006; Hazan, 2016; Bubeck, 2015 for the standard regret bounds and the conversion to saddle-point guarantees. Step 1: primal regret bound (entropic mirror descent). Fix bin b. The primal update (70) is entropic mirror descent on with loss vector ℓt,b RK. Because 0 Vb(n; θ) Vmax, 0 a(n) amax, and 0 µt µmax, each coordinate satisfies 0 ℓt,b(n) qb (Vmax + µmaxamax). The standard entropic-mirror-descent regret bound (via the log-sum-exp potential and Hoeffdings lemma) gives, for any fixed comparator pb , t=1 pt,b, ℓt,b t= pb, ℓt,b log ηp + ηp 8 q2 (Vmax + µmaxamax)2. Summing (78) over = 1, . . . , yields t=1 Lroll(pt, µt; θ) t=1 Lroll(p, µt; θ) log ηp log ηp + + ηp 8 ηp (Vmax + µmaxamax)2 b=1 q2 (Vmax + µmaxamax)2, (78) (79) (80) the last inequality holds due to the algebraic fact that a2 + + b2 (a + + b)2 1 for any probability measure (a, , b) , where = (p1, . . . , pB) and pt = (pt,1, . . . , pt,B). Step 2: dual regret bound (projected gradient ascent). The dual player maximizes Lroll(pt, µ; θ) over µ [0, µmax]. Since Lroll(pt, µ; θ) is linear in µ, the dual gradient is exactly gt from (71). Moreover, because 0 a(n) amax and B, we have gt amax. The standard regret bound for projected online gradient ascent on an interval gives, for any fixed µ [0, µmax], t=1 Lroll(pt, µ; θ) t= Lroll(pt, µt; θ) µ2 max 2ηµ + ηµ 2 a2 max. (81) 42 Technical Report Step 3: combine regrets and average. Adding (79) and (81) and dividing by yields, for all and µ, 1 t= Lroll(pt, µ; θ) 1 t=1 Lroll(p, µt; θ) GapT. (82) Because Lroll(, µ; θ) is convex in and Lroll(p, ; θ) is linear in µ, Jensens inequality gives Lroll( p, µ; θ) 1 t=1 Lroll(pt, µ; θ), 1 t= Lroll(p, µt; θ) = Lroll(p, µ; θ). Substitute into (82) to obtain (72). Step 4: deduce objective and budget bounds. Let be feasible for (75). For any µ [0, µmax], feasibility implies Lroll(p, µ; θ) Lroll(p, 0; θ). Thus Lroll(p, µ; θ) Lroll(p, µ; θ) Lroll(p, 0; θ). min Combining with (72) yields max µ[0,µmax] Lroll( p, µ; θ) Lroll(p, 0; θ) + GapT. Finally, observe that max µ[0,µmax] Lroll( p, µ; θ) = b= qb En pb [Vb(n; θ)] + µmax (cid:34) b=1 qb En pb [a(n)] (cid:35) , + which immediately implies (76) and (77). Remark B.23 (Bandit feedback and EXP3-style updates). Our implementation uses EXP3P-style updates because Rollout-GDRO only observes the variance proxy corresponding to the chosen arm. The full-information analysis above can be extended to the bandit setting by replacing ℓt,b with factor in the regret term. an unbiased importance-weighted estimator and adding the usual We omit the (standard) bandit algebra here, since the qualitative conclusion remains the same: the rollout controller is no-regret player in budgeted Lagrangian game. Remark B.24 (Decoupled analysis and possible coupling in the full system). See the Limitations and Future Work section for further discussion of this coupled setting and open problems it raises. In this paper we analyze and evaluate Prompt-GDRO and Rollout-GDRO as separate controllers. Accordingly, in the Rollout-GDRO analysis we treat the group fractions B as exogenous (a property of the current training data pipeline and grouping rule), and we study how the rollout allocator responds to group-dependent variance proxies under the mean-budget constraint b=1 qb nb = n. If both controllers are enabled simultaneously, then becomes endogenous to the Prompt-GDRO sampler and the training loop induces coupled multi-time-scale game between the prompt sampler, the rollout allocator, and the learner. Characterizing stability and convergence of this coupled regime is left to future work. Remark B.25 (Practical proxies for vb(θ) in GRPO and connection to our implementation). The quantity vb(θ) in Lemma B.15 is group-dependent second-moment / variance proxy for singleprompt stochastic gradient contribution. It enters the idealized variance-aware relaxation (61) only through the characteristic 1/nb scaling obtained when using nb rollouts (Lemma B.14). In our RolloutGDRO implementation (Section 4), each bin selects discrete rollout arm {nmin, . . . , nmax} using GDRO-EXP3P bandit update driven by the augmented arm loss Lb(n) = (cid:98)Lb(θ; n) + µ (n n) (Eq. (20)), while the dual variable µ is updated by dual ascent to enforce compute neutrality under the mean-budget constraint (Eq. (21)). Operationally, vb(θ) can be estimated from the same rollouts used to compute (cid:98)Lb(θ; n). For prompt with rollouts {yj}n j=1, let gj(x; θ) denote the per-rollout GRPO policy-gradient contribution. natural within-prompt proxy is the sample variance (biased Technical Report vs. unbiased only changes constants) (cid:100)Var(g x) 1 1 j=1 (cid:13)gj(x; θ) g(x; θ)(cid:13) (cid:13) (cid:13) 2 2, g(x; θ) = 1 j=1 gj(x; θ), which can be aggregated over prompts in bin to form bin-level proxy ˆvb(θ). In practice we use monotone scalar surrogate of this signal (e.g., the empirical variance of reward or advantage across rollouts within each bin) to guide arm selection, consistent with the variance-reduction viewpoint developed in Section B.2.1Section B.2.3."
        }
    ],
    "affiliations": [
        "Tencent AI Lab"
    ]
}