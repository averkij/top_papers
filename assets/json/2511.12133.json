{
    "paper_title": "AI-Salesman: Towards Reliable Large Language Model Driven Telemarketing",
    "authors": [
        "Qingyu Zhang",
        "Chunlei Xin",
        "Xuanang Chen",
        "Yaojie Lu",
        "Hongyu Lin",
        "Xianpei Han",
        "Le Sun",
        "Qing Ye",
        "Qianlong Xie",
        "Xingxing Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Goal-driven persuasive dialogue, exemplified by applications like telemarketing, requires sophisticated multi-turn planning and strict factual faithfulness, which remains a significant challenge for even state-of-the-art Large Language Models (LLMs). A lack of task-specific data often limits previous works, and direct LLM application suffers from strategic brittleness and factual hallucination. In this paper, we first construct and release TeleSalesCorpus, the first real-world-grounded dialogue dataset for this domain. We then propose AI-Salesman, a novel framework featuring a dual-stage architecture. For the training stage, we design a Bayesian-supervised reinforcement learning algorithm that learns robust sales strategies from noisy dialogues. For the inference stage, we introduce the Dynamic Outline-Guided Agent (DOGA), which leverages a pre-built script library to provide dynamic, turn-by-turn strategic guidance. Moreover, we design a comprehensive evaluation framework that combines fine-grained metrics for key sales skills with the LLM-as-a-Judge paradigm. Experimental results demonstrate that our proposed AI-Salesman significantly outperforms baseline models in both automatic metrics and comprehensive human evaluations, showcasing its effectiveness in complex persuasive scenarios."
        },
        {
            "title": "Start",
            "content": "AI-Salesman: Towards Reliable Large Language Model Driven Telemarketing Qingyu Zhang1, 2*, Chunlei Xin1, 2*, Xuanang Chen1, Yaojie Lu1, Hongyu Lin1, Xianpei Han1, 2, Le Sun1, 2, Qing Ye3, Qianlong Xie3, Xingxing Wang3 1Chinese Information Processing Laboratory, Institute of Software, Chinese Academy of Sciences 2University of Chinese Academy of Sciences 3Independent Researcher 5 2 0 2 5 1 ] . [ 1 3 3 1 2 1 . 1 1 5 2 : r Abstract Goal-driven persuasive dialogue, exemplified by applications like telemarketing, requires sophisticated multi-turn planning and strict factual faithfulness, which remains significant challenge for even state-of-the-art Large Language Models (LLMs). lack of task-specific data often limits previous works, and direct LLM application suffers from strategic brittleness and factual hallucination. In this paper, we first construct and release TeleSalesCorpus, the first real-worldgrounded dialogue dataset for this domain. We then propose AI-Salesman, novel framework featuring dual-stage architecture. For the training stage, we design Bayesiansupervised reinforcement learning algorithm that learns robust sales strategies from noisy dialogues. For the inference stage, we introduce the Dynamic Outline-Guided Agent (DOGA), which leverages pre-built script library to provide dynamic, turn-by-turn strategic guidance. Moreover, we design comprehensive evaluation framework that combines fine-grained metrics for key sales skills with the LLMas-a-Judge paradigm. Experimental results demonstrate that our proposed AI-Salesman significantly outperforms baseline models in both automatic metrics and comprehensive human evaluations, showcasing its effectiveness in complex persuasive scenarios."
        },
        {
            "title": "Introduction",
            "content": "While conversational AI has made significant strides in both structured task-oriented dialogue (Ham et al. 2020; Hosseini-Asl et al. 2020; Xu et al. 2024) and unconstrained open-domain chit-chat (Gao, Galley, and Li 2018a; Roller et al. 2021; Friedman, Panigrahi, and Chen 2025), critical and challenging frontier remains underexplored: goal-driven persuasive dialogue for intelligent marketing, unlike conventional dialogue tasks, intelligent marketing, exemplified by telemarketing, requires conversational AI to actively strategize, persuade, and guide users toward specific outcomes. This presents unique confluence of high-stakes challenges that current large language models (LLMs) struggle to address effectively. The core challenges of intelligent telemarketing are threefold. First is the challenge of satisfaction. The AI must *These authors contributed equally. Corresponding author. 1The TeleSalesCorpus is available at https://huggingface.co/ datasets/ICIP/TeleSalesCorpus. not only generate human-like responses but also navigate wide variety of marketing scenarios, each with its own complex strategies and logical flows. General-purpose large language models, despite their fluency, struggle to capture and reliably execute these diverse, long-horizon conversational plans (Valmeekam et al. 2023; Pan et al. 2025; Chen et al. 2025a; Lin et al. 2025), failing to satisfy the strategic requirements of the task. Second is the challenge of faithfulness. In high-stakes sales interactions, the AI must adhere strictly to the constraints of the product or service. However, the propensity of LLMs for factual hallucination (Maynez et al. 2020; Rawte et al. 2023; Atanasova et al. 2023; Chen et al. 2025b,c) poses an unacceptable risk, potentially resulting in misleading claims or inaccurate commitments. Third is the challenge of customization. Each customer possesses unique background, with distinct concerns and points of interest. Effective persuasion requires tailoring arguments and information delivery to individual needs. Yet, LLMs frequently produce generic responses and lack the strategic reasoning necessary to address specific objections effectively (Fu et al. 2023). To address these multifaceted challenges, this paper introduces AI-Salesman, an end-to-end framework that tackles these issues through innovations at both the training and inference stages, as illustrated in Figure 1. Specifically, AISalesman integrates two core mechanisms to achieve this. First, to satisfy the critical demands of satisfaction and faithfulness, we introduce novel reward function grounded in Bayesian principles into our Group Relative Policy Optimization (GRPO) training process (Shao et al. 2024). Moving beyond conventional outcome-based rewards, our approach directly supervises the models intermediate reasoning. Inspired by Bayesian principles, we decompose the reward signal for thought process into two intuitive criteria: prior that captures the intrinsic coherence of the reasoning itself, and likelihood that measures its strategic utility in justifying the experts final response. By optimizing for both coherent reasoning and effective outcomes, the model learns to generate responses that are both factually grounded and persuasive, thereby enhancing user satisfaction and faithfulness. Second, to enable customization, we propose the Dynamic Outline-Guided Agent (DOGA), framework that operates during the inference stage. To overcome the generic responses common with static prompting, DOGA dynamiFigure 1: Overview of Training and Inference for the AI Salesman. cally constructs tailored strategy outline for each turn. By analyzing the users profile, real-time intent, and dialogue history, it retrieves the most relevant persuasive strategies from pre-verified library. This curated outline then guides the LLM, ensuring its responses are strategically targeted to each customers unique concerns and objections. Unfortunately, significant barrier to progress in this domain is the absence of specialized training data and effective evaluation methods for telemarketing (He et al. 2018; Wang et al. 2019). To address this gap, we first introduce TeleSalesCorpus, large-scale corpus of high-fidelity dialogues generated through state-aware simulation grounded in realworld expert interactions. This corpus captures the complex patterns, customer objections, and conversational nuances characteristic of authentic sales conversations. Second, moving beyond simplistic success metrics, we propose comprehensive evaluation framework specifically designed for telemarketing to enable fine-grained analysis. To systematically assess models ability to achieve strategic satisfaction, maintain factual faithfulness, and deliver persuasive customization, we define six sales capabilities, ranging from Business Analysis to Objection Handling, each assessed using detailed rubric composed of seven qualitative metrics. By integrating this structured evaluation schema with the LLM-as-a-Judge paradigm (Zheng et al. 2023; Chan et al. 2024), our framework supports rigorous and comprehensive assessment of model performance across diverse scenarios. This evaluation approach provides scalable offline alternative to resource-intensive online A/B tests. Overall, our contributions can be summarized as follows: We propose AI-Salesman, novel end-to-end framework that integrates reasoning-aware reinforcement learning with dynamic outline-guided inference. To the best of our knowledge, this is the first LLM-based framework specifically designed for real-world telemarketing that systematically addresses the challenges of satisfaction, faithfulness, and customization. We construct and release TeleSalesCorpus, the first largescale, high-fidelity dialogue dataset grounded in realworld sales conversations, specifically designed for training and evaluating telemarketing models. We propose comprehensive offline evaluation framework across six core sales capabilities, enabling efficient and rigorous assessment of models practical sales proficiency in diverse scenarios."
        },
        {
            "title": "2.1 Task Definition\nWe model telemarketing dialogue as a conditionally con-\nstrained sequence generation task. At each turn t, the model\ngenerates a response based on the system prompt P and the\ndialogue history Ht = Ht−1 ⊕ Ut, where Ut is the user’s\nutterance at turn t. The prompt P defines the task’s global\ncontext, including a set of goals G = {g1, . . . , gn} and con-\nstraints C = {c1, . . . , cm}.",
            "content": "The models objective is to generate response sequence At that maximizes its conditional probability given the inputs (P, Ht). Formally, we seek the optimal response : = arg max AtV (AtP, Ht) (1) where is the models vocabulary and denotes its Kleene closure, representing the set of all possible sequences the model can generate. This generation is subject to two primary conditions. First, the response At must adhere to all predefined rules, such that for every constraint C, the condition c(At) = 1 is satisfied. Second, the response must be goal-oriented, designed to maximize the expectation of achieving the final task goals defined in G."
        },
        {
            "title": "2.2 Evaluation Framework\nOur evaluation framework is built upon two core compo-\nnents: six fundamental sales capabilities required for the task",
            "content": "overriding incorrect state predictions from the sales agent. Then, it queries pre-compiled library of real-world interaction examples, retrieving strategically relevant example based on the current state. This example is used to dynamically guide the Sales Agent in crafting response that is both contextually appropriate and strategically sound. This process is grounded in assets distilled from real dialogues and diverse, LLM-authored business scenarios. Following rigorous, multi-faceted quality assurance protocol, our pipeline produced final dataset of 2,000 highfidelity conversations. The detailed methodology for each stageasset distillation, dialogue simulation, and quality assuranceis provided in Appendix C."
        },
        {
            "title": "3.2 Stage-1: GRPO Training\nTo address the core challenges of satisfaction and faithful-\nness in intelligent telemarketing, we propose a policy opti-\nmization framework that synergizes the Group Relative Pol-\nicy Optimization (GRPO) algorithm(Shao et al. 2024) with\na novel Bayesian-Supervised Reasoning reward. GRPO fa-\ncilitates online exploration of sales strategies, enabling the\nmodel to learn robust policies from noisy data. This explo-\nration is guided by our Bayesian reward, which uniquely\nassesses the model’s intermediate reasoning process. It as-\nsigns a higher value to reasoning that provides a logically\nsound and factually grounded justification for the final re-\nsponse. This core signal is supplemented by several auxil-\niary rewards designed to maintain structural and semantic\nintegrity. By optimizing this reward via GRPO, the model\nlearns to generate responses that are both persuasive, to en-\nhance Satisfaction, and factually accurate, to ensure Faith-\nfulness.",
            "content": "As illustrated in Figure 3, our end-to-end training is driven by the GRPO algorithm. For the t-th turn given input H, the model first performs parallel rollouts to generate group of candidate sequences {A(i) i=1. The algorithm then uses the reward signal R(i) from each sequence to compute normalized group advantage score, A(i), and subsequently updates the policy model. The details of the GRPO algorithm are provided in Appendix D. }G Reward Function Design The total reward is weighted sum of four components, evaluating different aspects of the generated sequence A(i) against the ground-truth reference : R(A(i) , ) = (cid:88) wkRk(A(i) , ) (2) k{bayes, format, len, sem} where wk are hyperparameter weights. Core Reward Bayesian-Supervised Reasoning (Rbayes) This reward guides the models internal reasoning chain, ht. Grounded in Bayesian principles, our objective is to align this reasoning chain with the reference answer by maximizing their joint probability, (T ht, ). Accordingly, the reward is defined as the log-joint probability, which decomposes into Figure 2: Data Construction Framework Overview. and rubric of seven evaluation metrics for granular, turnby-turn assessment. Detailed descriptions of these components are provided in Appendix B. The six capabilities cover the entire lifecycle of sales call: Role-playing, Business Analysis, Activity Introduction, Idle-chat Rejection, Objection Handling, and Operational Guidance. To provide fine-grained assessment across these capabilities, we evaluate each response using seven qualitative metrics: Guideline Adherence(Gui.), Factual Correctness(Fac.), Logical Coherence(Log.), User Need Fulfillment(Use.), Response Richness(Res.), Safety(Saf.), and Completeness(Com.). To operationalize this framework at scale, we employ GPT-4 as judge. For each dialogue, the LLM-judge is given the conversation history, ground-truth data, and our metric definitions. Then it synthesizes these inputs to generate holistic quality score on 1-10 scale. This approach enables nuanced, context-aware evaluation that approximates human judgment for robustly benchmarking different models."
        },
        {
            "title": "3.1 Data Construction\nThe availability of suitable training data fundamentally con-\nstrains the development of a robust, goal-oriented persua-\nsive dialogue system. Existing datasets (Wang et al. 2019;\nHe et al. 2018) do not adequately address the unique chal-\nlenges of telemarketing, such as complex business rules and\nspecific promotional objectives. To bridge this gap, we con-\nstructed TeleSalesCorpus, a dataset using a semi-synthetic\nframework that leverages real-world expertise to generate\nhigh-fidelity, goal-oriented dialogues.",
            "content": "Our data creation process employs state-aware, threeagent simulation, as illustrated in the Figure 2 provided. The framework features User Agent with distinct persona, Sales Agent responsible for persuasion, and central Dialogue Manager that orchestrates the interaction. At each turn, when the User Agent responds, the Dialogue Manager intervenes. It first adjudicates the true conversational state, Figure 3: AI Salesman Framework Overview. two terms estimated by the model πθ itself. The detailed theoretical derivation is provided in Appendix E. Rbayes(T h(i) , ) = + (cid:88) j=1 (cid:124) (cid:88) k=1 (cid:124) log πθ(th(i) ˆP, th(i) <j) (cid:123)(cid:122) Prior: Reasoning Fluency (cid:125) (3) log πθ(y ˆP, h(i) , <k) (cid:123)(cid:122) Likelihood: Reasoning Utility (cid:125) where ˆP is the shared context."
        },
        {
            "title": "Auxiliary Reward",
            "content": "Adherence the Format that ensures predefined \"<think>...</think><answer>...</answer>\" schema. (Rformat) follows reward output the Rformat(A(i) ) = fformat(A(i) ) (4) where fformat() is function that yields 1 if the sequence A(i) conforms to the required schema, and 0 otherwise. Relative Length Consistency (Rlen) This aims to align the output length with the reference answer by penalizing the squared relative deviation from the target length L(A ). Semantic Similarity (Rsem) To measure semantic alignment, we compute the cosine similarity s(i) between the generated and reference answers using sentence-embedding model. The score is normalized against baseline similarity sbase for more robust signal. Rsem(A(i) , ) = s(i) sbase 1 sbase + ϵ (6)"
        },
        {
            "title": "3.3 Stage-2: Inference With Dynamic Prompt\nWe propose the Dynamic Outline-Guided Agent (DOGA)\nto enable customization in telemarketing by overcoming the\nrigidity of static prompts. Our framework decouples high-\nlevel strategy from turn-level execution by generating turn-\nspecific guidance from a pre-structured script library. This\nprocess is composed of two stages: an offline library con-\nstruction phase and a real-time dynamic prompt assembly\npipeline. This structure ensures that model responses are\npersonalized and contextually appropriate. More details of\nthe DOGA framework are detailed in Appendix F.",
            "content": "Offline Stage: Structured Script Library Construction The foundation of our framework is high-quality library of sales scripts and templates. This library is created offline by extracting, clustering, and summarizing effective strategies from corpus of successful historical dialogues. This process distills best practices into reusable resource indexed by dialogue intent. Rlen(A(i) , ) = 1 (cid:32) L(A(i) ) L(A L(A ) ) (cid:33)2 (5) Online Stage: Real-time Dialogue Management During live conversation, DOGA employs the real-time pipeline shown in Figure 3. At each turn, an Intent Classification Capability Model Mean Gui. Role-playing Business Analysis Activity Introduction Idle-chat Rejection Objection Handling Operational Guidance Baseline SFT-only GRPO w/ SFT Ours Baseline SFT-only GRPO w/ SFT Ours Baseline SFT-only GRPO w/ SFT Ours Baseline SFT-only GRPO w/ SFT Ours Baseline SFT-only GRPO w/ SFT Ours Baseline SFT-only GRPO w/ SFT Ours 5.54 5.66 5.75 6.31 6.49 6.78 6.86 7.40 5.91 5.86 5.94 6. 4.66 4.86 4.95 5.73 4.77 5.24 5.33 6.00 5.39 5.71 5.78 6.74 4.83 4.78 4.79 5.81 5.42 5.39 5.51 5.96 5.39 5.16 5.08 6. 4.36 3.96 4.11 5.49 4.60 5.19 5.41 6.24 4.44 4.84 4.90 6.26 Fac. 5.70 5.90 5.95 6.5 6.44 7.15 7.39 7. 5.28 5.32 5.41 5.98 4.35 4.68 4.72 5.52 3.92 4.64 4.58 4.65 6.13 6.15 6.20 7.33 Log. Use. Res. 5.94 6.05 6.16 6.62 7.05 7.24 7.24 7.78 6.43 6.38 6.49 7.13 5.10 5.36 5.48 6.19 5.18 5.75 5.82 6. 5.52 5.87 5.81 6.71 5.40 5.61 5.72 6.16 6.67 7.04 6.97 7.61 6.18 6.23 6.15 7.07 4.48 4.83 4.90 5.59 4.97 5.22 5.09 6. 5.33 5.54 5.68 6.50 5.02 5.08 5.08 5.76 6.19 6.39 6.59 7.23 5.76 5.56 5.62 6.71 4.41 4.67 4.59 5.50 4.47 5.01 5.23 6. 4.68 5.29 5.16 6.09 Saf. 7.20 7.27 7.41 7.6 7.72 7.83 7.88 7.94 7.39 7.33 7.45 7.94 6.31 6.63 6.78 6. 6.46 6.56 6.69 7.49 6.56 6.99 7.18 7.63 Com. 4.68 4.92 5.13 5.75 5.91 6.41 6.44 7.43 4.97 5.04 5.36 5. 3.59 3.89 4.09 4.81 3.80 4.34 4.49 4.82 5.09 5.29 5.51 6.67 Table 1: Performance comparison of different training pipelines. Our framework significantly outperforms all competing baselines. The top-performing model, Ours, utilizes direct reinforcement learning, bypassing the SFT stage. Best results in each block are in bold. The second-best results in each block are underlined. Model first predicts the users current turn sales intent. This intent is used to retrieve relevant recommended response template from our pre-built library. Finally, this turn-specific guidance is combined with the system prompt and the full dialogue history to assemble dynamic system prompt. This prompt steers the model to generate response that is strategically aligned with the immediate conversational goal."
        },
        {
            "title": "4 Experiments\nThis section presents a series of experiments designed to\nevaluate the effectiveness of our proposed AI-Salesman\nframework. We first detail the experimental setup, includ-\ning the datasets, models, and evaluation protocols. We then\npresent the main results comparing our full method against\nseveral baselines. Finally, through extensive ablation stud-\nies, scalability analysis, and human evaluations, we validate\nthe contributions of the key components of our framework.",
            "content": "Datasets We utilize two datasets with distinct roles in our experiments: TeleSalesCorpus (Syn-Data): To ensure the reproducibility and openness of our research, we introduce this synthetic dataset, which will be made publicly available. Constructed as described in Section 3.1, it contains 2,000 high-fidelity, multi-turn dialogues. Model SFT GRPO Inference Strategy Baseline SFT-only GRPO w/ SFT Ours Few-shot Few-shot DOGA DOGA Table 2: Configurations for the different models in our experiments. The base model for all versions is Qwen2.5-7BInstruct. indicates the stage was applied, while indicates it was skipped. Real-world Tele-sales Dataset (Real-Data): This is our dataset for large-scale training. It consists of over 8,000 real-world tele-sales dialogues. This proprietary dataset reflects the complexities of authentic sales conversations, including significant conversational noise and diversity. It is instrumental for assessing our models performance and scalability in realistic application setting."
        },
        {
            "title": "4.1 Experimental Setup\nModels and Baselines Our main experiment is based on\nthe Qwen2.5-7B-Instruct model (Qwen et al. 2025). As de-\ntailed training and inference configuration in Table 2. We es-",
            "content": "(a) Effect of Rbayes on Syn-Data. (b) Advantage in strategic capabilities. (c) Performance scaling with model size. Figure 4: Key experimental results. (a) Bayesian reward (Rbayes) stably raises the upper bound of the semantic similarity reward. (b) DOGA shows decisive advantages in complex, strategic capabilities. (c) Our methods performance scales effectively, with the 32B model offering an optimal trade-off. tablish performance reference using the original Baseline and standard Supervised Fine-Tuning SFT-only model. Our primary contribution is Ours, which applies the GRPO algorithm with the reward function we designed directly to the baseline. To investigate whether SFT is necessary step for effective preference alignment, we also train GRPO w/ SFT model by applying GRPO after the SFT stage. Evaluation Metrics As detailed in Section 2.2, we use the LLM-as-a-Judge paradigm with GPT-4 as the evaluator. Each dialogue turn is scored from 1 to 10 across seven metrics. The final score for each of the six core sales capabilities is the arithmetic mean of seven metrics."
        },
        {
            "title": "4.2 Main Results",
            "content": "The comprehensive performance evaluation, detailed in Table 1, empirically substantiates the remarkable efficacy of our proposed training paradigm. Our final model, denoted Ours, establishes new state-of-the-art, achieving dominant scores across the vast majority of capabilities and dimensions evaluated. Our analysis reveals three principal findings: Finding 1: Domain-specific SFT establishes robust but limited performance baseline. The results indicate mixed but overall positive effect from SFT. This confirms its role as preliminary adaptation stage. While SFT led to significant gains in areas like Business Analysis (6.49 6.78) and Objection Handling (4.77 5.24), its impact on more complex skills was limited. For example, the score for Role-playing grew minimally from 5.54 to 5.66. This demonstrates that SFT is effective at mimicking explicit patterns but struggles with tasks requiring deeper strategic generalization. Finding 2: SFT creates performance bottleneck for reinforcement learning. Our experiments show that applying reinforcement learning to an SFT-initialized model (GRPO w/ SFT) offers negligible performance gain over the SFT model alone, with the overall mean score across all capabilities only increasing minimally from 5.69 (SFT-only) to 5.77 (GRPO w/ SFT). We conclude that SFT, by forcing the model to mimic noisy and suboptimal dataset, traps its policy in narrow, flawed space. This severely restricts RLs ability to explore and discover superior strategies, resulting in final policy that fails to meaningfully diverge from the flawed behaviors learned during SFT. The model thus adheres to rules but lacks conversational richness. Finding 3: Direct RL optimization without SFT unlocks superior performance. In stark contrast, optimizing base model directly with our GRPO reward signal yields holistically superior model, boosting the overall mean score from the Baselines 5.46 to 6.49a significant 18.9% increase. By being liberated from the constraints of imitating potentially suboptimal reference corpus, the model learns to internalize the underlying business logic and knowledge directly from rewards. This approach achieves high performance across all dimensionsexcelling not only in Richness (Res.) and User Satisfaction (Use.) but also maintaining strong Guideline Adherence (Gui.), proving its more effective path to developing capable and adaptive sales model."
        },
        {
            "title": "4.3 Ablation Studies\nTo evaluate the specific contributions of our proposed com-\nponents, we conducted a series of ablation studies. These\nexperiments are designed to isolate and quantify the impact\nof our reward functions and DOGA.",
            "content": "Quantitative Analysis of Reward Components We first investigated the individual importance of the key signals in our composite reward function. To do this, we trained two ablated versions of our model: GRPO w/o Rbayes: The model was trained without the Bayesian-Supervised Reasoning Reward, removing the explicit supervision on the internal thought process. GRPO w/o Rsem: The model was trained without the Semantic Similarity Reward, removing the direct pressure to align the final answer with the expert reference. Model Version Mean Score Comparison Pair Win (%) Tie (%) Loss (%) GRPO w/o Rbayes GRPO w/o Rsem Ours 6.15 6.39 6.49 Ours vs. Baseline Ours vs. SFT-only SFT-only vs. Baseline 88.5 75.1 68.7 7.2 17.6 21. 4.3 7.3 9.9 Table 3: Ablation study of reward components. The LLMas-a-Judge calculates the mean score across all evaluation capabilities. Table 4: A/B test results based on head-to-head human preference evaluations. As shown in Table 3, the results clearly demonstrate the criticality of both components. Removing the Bayesian reward (Rbayes) led to 5.2% drop in the mean score, while removing the semantic reward (Rsem) caused 1.5% decrease. This confirms that both reward signals are essential for guiding the model. Rsem directly optimizes for output quality, while Rbayes ensures the underlying reasoning is sound, which indirectly but powerfully contributes to the generation of high-quality and reliable responses. Visualizing the Effect of Bayesian Reward To visualize the effect of our most novel component, the Bayesian reward, we plotted the training-time semantic similarity reward on our synthesized dataset, TeleSalesCorpus (SynData). As shown in Figure 4a, the model trained with Rbayes converges to higher semantic similarity ceiling steadily. This suggests that by penalizing illogical thought processes, the Bayesian reward acts as an internal verifier, preventing the model from exploring ineffective generation paths and steering it more directly toward producing answers that are semantically aligned with expert behavior. The reward demonstrates similar effect on challenging real-world dataset, as detailed in the appendix G. Effectiveness of DOGA comparative analysis of our DOGA framework against static prompt on six sales capabilities reveals two key findings (Figure 4b): Finding 1: DOGA excels in complex tasks. It achieved significant performance gains in Business Analysis (+4.9%), Objection Handling (+11.1%), and Operational Guidance (+14.7%). This performance boost is driven by its ability to dynamically adapt, drawing from library of expert templates to deliver more detailed and accurate contextual guidance in real-time, surpassing the limitations of static prompts. Finding 2: trade-off exists between strategic precision and conversational naturalness.The static prompt performed marginally better in Role-playing and Idlechat Rejection. DOGAs template injection, while precise, can sound formulaic. For simple tasks, the static prompts direct rules are more efficient than DOGAs complex retrieval cycle. In conclusion, DOGA is specialized instrument, not universal upgrade. Its primary value is enhancing strategic reasoning and procedural adherence in complex, goaloriented dialogues, making it indispensable for developing sophisticated AI-Salesman."
        },
        {
            "title": "4.4 Scalability Analysis\nTo systematically evaluate the scalability of our proposed\nmethod, we conducted a scaling experiment using the\nQwen2.5-Instruct series of models, which includes variants\nwith 7B, 14B, 32B, and 72B parameters. Each model was\ntrained and subsequently evaluated on our curated Real-Data\nset. The results are shown in Figure 4c. We observed a non-\nlinear performance trend with several key findings (detailed\nexperimental settings are provided in Appendix I):\n• Marginal Gain: Scaling from 7B to 14B yields only a",
            "content": "minor improvement. Peak Performance: The 32B model achieves significantly higher score of 7.17, marking the peak performance across all tested scales. Diminishing Returns: Further scaling to 72B leads to slight performance drop. These findings indicate that the 32B model offers the optimal capacity for our task, effectively leveraging our proposed frameworks."
        },
        {
            "title": "4.5 Human Evaluation (A/B Test)\nTo assess real-world performance, we conducted a blind A/B\ntest with 30 front-line sales professionals. These experts,\nchosen for their deep understanding of sales strategies and\nreal-world business interactions, role-played as clients and\nengaged in hundreds of sales conversations with three AI\nmodels: our AI-Salesman, a strong SFT-only variant, and a\nBaseline. They then voted on paired responses, evaluating\nthem on persuasiveness and professionalism.",
            "content": "The results in Table 4 establish clear performance hierarchy: Ours SFT-only > Baseline. Our full model was preferred in 88.5% of matchups against the baseline and 75.1% against the strong SFT-only model. Notably, this performance ranking aligns with the results from our offline evaluations in Table 1, where GPT-4 served as the judge. This quantitative strength was echoed in qualitative feedback, where evaluators praised our model for its richer, more varied language and more natural user experience, confirming its practical value in real-world scenarios."
        },
        {
            "title": "5 Conclusion\nThis paper introduces AI-Salesman, an end-to-end frame-\nwork designed to address the limitations of Large Lan-\nguage Models in professional telemarketing scenarios. Our\ncore innovations include a Bayesian-supervised reinforce-\nment learning algorithm to optimize sales dialogue strate-\ngies directly, and the Dynamic Outline-Guided Agent mech-\nanism for flexible, real-time conversation management.",
            "content": "We also constructed and released the first real-worldgrounded telemarketing dataset, TeleSalesCorpus, for this task. Extensive automated and human evaluations demonstrate that our approach significantly outperforms baseline models in generating persuasive and business-compliant dialogue. In summary, this work provides systematic methodology and practical resources for building more effective and reliable goal-oriented persuasive AI."
        },
        {
            "title": "6 Acknowledgments\nfor",
            "content": "insightWe sincerely thank the reviewers ful comments and valuable suggestions. This work was supported by National Key R&D Program of China (2024YFC3308000), the Natural Science Foundation of China (No. 62476265, 62306303, 62506354), the Basic Research Program of ISCAS (Grant No. ISCAS-ZD-202401). their References Agarwal, R.; Singh, A.; Zhang, L. M.; Bohnet, B.; Rosias, L.; Chan, S. C.; Zhang, B.; Faust, A.; and Larochelle, H. 2024. Many-shot In-Context Learning. In ICML 2024 Workshop on In-Context Learning. Ahearne, M.; Mathieu, J.; and Rapp, A. 2005. To empower or not to empower your sales force? An empirical examination of the influence of leadership empowerment behavior on customer satisfaction and performance. Journal of Applied psychology, 90(5): 945. Atanasova, P.; Camburu, O.-M.; Lioma, C.; Lukasiewicz, T.; Simonsen, J. G.; and Augenstein, I. 2023. Faithfulness Tests for Natural Language Explanations. In Rogers, A.; Boyd-Graber, J.; and Okazaki, N., eds., Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), 283294. Toronto, Canada: Association for Computational Linguistics. Brown, T.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J. D.; Dhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell, A.; Agarwal, S.; Herbert-Voss, A.; Krueger, G.; Henighan, T.; Child, R.; Ramesh, A.; Ziegler, D.; Wu, J.; Winter, C.; Hesse, C.; Chen, M.; Sigler, E.; Litwin, M.; Gray, S.; Chess, B.; Clark, J.; Berner, C.; McCandlish, S.; Radford, A.; Sutskever, I.; and Amodei, D. 2020. Language Models are Few-Shot Learners. In Larochelle, H.; Ranzato, M.; Hadsell, R.; Balcan, M.; and Lin, H., eds., Advances in Neural Information Processing Systems, volume 33, 18771901. Curran Associates, Inc. Chan, C.-M.; Chen, W.; Su, Y.; Yu, J.; Xue, W.; Zhang, S.; Fu, J.; and Liu, Z. 2024. ChatEval: Towards Better LLM-based Evaluators through Multi-Agent Debate. In The Twelfth International Conference on Learning Representations. Chen, J.; Guan, X.; Yuan, Q.; Mo, G.; Zhou, W.; Lu, Y.; Lin, H.; He, B.; Sun, L.; and Han, X. 2025a. ConsistentChat: Building Skeleton-Guided Consistent Multi-Turn Dialogues for Large Language Models from Scratch. In The 2025 Conference on Empirical Methods in Natural Language Processing. Chen, Y.; Liu, S.; Lyu, Y.; Zhang, C.; Shi, J.; and Xu, T. 2025b. Xiangqi-R1: Enhancing Spatial Strategic Reasoning in LLMs for Chinese Chess via Reinforcement Learning. arXiv:2507.12215. Chen, Y.; Lyu, Y.; Liu, S.; Zhang, C.; Lv, J.; and Xu, T. 2025c. Think Wider, Detect Sharper: Reinforced Reference Coverage for Document-Level Self-Contradiction Detection. In Christodoulopoulos, C.; Chakraborty, T.; Rose, C.; and Peng, V., eds., Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, 12731288. Suzhou, China: Association for Computational Linguistics. ISBN 979-8-89176-332-6. Chung, W.; Cahyawijaya, S.; Wilie, B.; Lovenia, H.; and Fung, P. 2023. InstructTODS: Large Language Models for End-to-End Task-Oriented Dialogue Systems. In Chen, K.; and Ku, L.-W., eds., Proceedings of the Second Workshop on Natural Language Interfaces, 121. Bali, Indonesia: Association for Computational Linguistics. DeepSeek-AI; Guo, D.; Yang, D.; Zhang, H.; Song, J.; Zhang, R.; Xu, R.; Zhu, Q.; Ma, S.; Wang, P.; Bi, X.; Zhang, X.; Yu, X.; Wu, Y.; Wu, Z. F.; Gou, Z.; Shao, Z.; Li, Z.; Gao, Z.; Liu, A.; Xue, B.; Wang, B.; Wu, B.; Feng, B.; Lu, C.; Zhao, C.; Deng, C.; Zhang, C.; Ruan, C.; Dai, D.; Chen, D.; Ji, D.; Li, E.; Lin, F.; Dai, F.; Luo, F.; Hao, G.; Chen, G.; Li, G.; Zhang, H.; Bao, H.; Xu, H.; Wang, H.; Ding, H.; Xin, H.; Gao, H.; Qu, H.; Li, H.; Guo, J.; Li, J.; Wang, J.; Chen, J.; Yuan, J.; Qiu, J.; Li, J.; Cai, J. L.; Ni, J.; Liang, J.; Chen, J.; Dong, K.; Hu, K.; Gao, K.; Guan, K.; Huang, K.; Yu, K.; Wang, L.; Zhang, L.; Zhao, L.; Wang, L.; Zhang, L.; Xu, L.; Xia, L.; Zhang, M.; Zhang, M.; Tang, M.; Li, M.; Wang, M.; Li, M.; Tian, N.; Huang, P.; Zhang, P.; Wang, Q.; Chen, Q.; Du, Q.; Ge, R.; Zhang, R.; Pan, R.; Wang, R.; Chen, R. J.; Jin, R. L.; Chen, R.; Lu, S.; Zhou, S.; Chen, S.; Ye, S.; Wang, S.; Yu, S.; Zhou, S.; Pan, S.; Li, S. S.; Zhou, S.; Wu, S.; Ye, S.; Yun, T.; Pei, T.; Sun, T.; Wang, T.; Zeng, W.; Zhao, W.; Liu, W.; Liang, W.; Gao, W.; Yu, W.; Zhang, W.; Xiao, W. L.; An, W.; Liu, X.; Wang, X.; Chen, X.; Nie, X.; Cheng, X.; Liu, X.; Xie, X.; Liu, X.; Yang, X.; Li, X.; Su, X.; Lin, X.; Li, X. Q.; Jin, X.; Shen, X.; Chen, X.; Sun, X.; Wang, X.; Song, X.; Zhou, X.; Wang, X.; Shan, X.; Li, Y. K.; Wang, Y. Q.; Wei, Y. X.; Zhang, Y.; Xu, Y.; Li, Y.; Zhao, Y.; Sun, Y.; Wang, Y.; Yu, Y.; Zhang, Y.; Shi, Y.; Xiong, Y.; He, Y.; Piao, Y.; Wang, Y.; Tan, Y.; Ma, Y.; Liu, Y.; Guo, Y.; Ou, Y.; Wang, Y.; Gong, Y.; Zou, Y.; He, Y.; Xiong, Y.; Luo, Y.; You, Y.; Liu, Y.; Zhou, Y.; Zhu, Y. X.; Xu, Y.; Huang, Y.; Li, Y.; Zheng, Y.; Zhu, Y.; Ma, Y.; Tang, Y.; Zha, Y.; Yan, Y.; Ren, Z. Z.; Ren, Z.; Sha, Z.; Fu, Z.; Xu, Z.; Xie, Z.; Zhang, Z.; Hao, Z.; Ma, Z.; Yan, Z.; Wu, Z.; Gu, Z.; Zhu, Z.; Liu, Z.; Li, Z.; Xie, Z.; Song, Z.; Pan, Z.; Huang, Z.; Xu, Z.; Zhang, Z.; and Zhang, Z. 2025. DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning. arXiv:2501.12948. Dong, W.; Chen, S.; and Yang, Y. 2025. ProTOD: Proactive Task-oriented Dialogue System Based on Large Language In Rambow, O.; Wanner, L.; Apidianaki, M.; AlModel. Khalifa, H.; Eugenio, B. D.; and Schockaert, S., eds., Proceedings of the 31st International Conference on Computational Linguistics, 91479164. Abu Dhabi, UAE: Association for Computational Linguistics. Feng, Y.; Lu, Z.; Liu, B.; Zhan, L.; and Wu, X.-M. 2023. Towards LLM-driven Dialogue State Tracking. In Bouamor, H.; Pino, J.; and Bali, K., eds., Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, 739755. Singapore: Association for Computational Linguistics. Friedman, D.; Panigrahi, A.; and Chen, D. 2025. Representing Rule-based Chatbots with Transformers. In Chiruzzo, L.; Ritter, A.; and Wang, L., eds., Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), 31553180. Albuquerque, New Mexico: Association for Computational Linguistics. ISBN 979-8-89176-189-6. Fu, Y.; Peng, H.; Khot, T.; and Lapata, M. 2023. Improving Language Model Negotiation with Self-Play and In-Context Learning from AI Feedback. arXiv:2305.10142. Gao, J.; Galley, M.; and Li, L. 2018a. Neural Approaches to Conversational AI. In Artzi, Y.; and Eisenstein, J., eds., Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics: Tutorial Abstracts, 27. Melbourne, Australia: Association for Computational Linguistics. Gao, J.; Galley, M.; and Li, L. 2018b. Neural approaches In The 41st international ACM SIto conversational AI. GIR conference on research & development in information retrieval, 13711374. Ham, D.; Lee, J.-G.; Jang, Y.; and Kim, K.-E. 2020. End-toEnd Neural Pipeline for Goal-Oriented Dialogue Systems using GPT-2. In Jurafsky, D.; Chai, J.; Schluter, N.; and Tetreault, J., eds., Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, 583592. Online: Association for Computational Linguistics. He, H.; Chen, D.; Balakrishnan, A.; and Liang, P. 2018. Decoupling Strategy and Generation in Negotiation Dialogues. In Riloff, E.; Chiang, D.; Hockenmaier, J.; and Tsujii, J., eds., Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, 23332343. Brussels, Belgium: Association for Computational Linguistics. Holman, D. 2002. Employee wellbeing in call centres. Human Resource Management Journal, 12: 35 50. Hosseini-Asl, E.; McCann, B.; Wu, C.-S.; Yavuz, S.; and Socher, R. 2020. Simple Language Model for TaskOriented Dialogue. In Advances in Neural Information Processing Systems, volume 33, 2017920191. Kirkpatrick, J.; Pascanu, R.; Rabinowitz, N.; Veness, J.; Desjardins, G.; Rusu, A. A.; Milan, K.; Quan, J.; Ramalho, T.; Grabska-Barwinska, A.; et al. 2017. Overcoming catastrophic forgetting in neural networks. Proceedings of the national academy of sciences, 114(13): 35213526. Li, H.; Ding, L.; Fang, M.; and Tao, D. 2024. Revisiting Catastrophic Forgetting in Large Language Model Tuning. In Al-Onaizan, Y.; Bansal, M.; and Chen, Y.-N., eds., Findings of the Association for Computational Linguistics: EMNLP 2024, 42974308. Miami, Florida, USA: Association for Computational Linguistics. Lin, C.-Y. 2004. ROUGE: Package for Automatic Evaluation of Summaries. In Text Summarization Branches Out, 7481. Barcelona, Spain: Association for Computational Linguistics. Lin, L.; Lin, Z.; Zeng, Z.; and Ji, R. 2025. Speculative Decoding Reimagined for Multimodal Large Language Models. arXiv:2505.14260. Liu, C.-W.; Lowe, R.; Serban, I.; Noseworthy, M.; Charlin, L.; and Pineau, J. 2016. How NOT To Evaluate Your Dialogue System: An Empirical Study of Unsupervised Evaluation Metrics for Dialogue Response Generation. In Su, J.; Duh, K.; and Carreras, X., eds., Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, 21222132. Austin, Texas: Association for Computational Linguistics. Liu, N. F.; Lin, K.; Hewitt, J.; Paranjape, A.; Bevilacqua, M.; Petroni, F.; and Liang, P. 2024. Lost in the Middle: How Language Models Use Long Contexts. Transactions of the Association for Computational Linguistics, 12: 157173. Lloyd, A. 2020. Efficiency, productivity and targets: The gap between ideology and reality in the call centre. Critical Sociology, 46(1): 8396. Luo, Y.; Yang, Z.; Meng, F.; Li, Y.; Zhou, J.; and Zhang, Y. 2025. An Empirical Study of Catastrophic Forgetting in Large Language Models During Continual Fine-tuning. arXiv:2308.08747. Maynez, J.; Narayan, S.; Bohnet, B.; and McDonald, R. 2020. On Faithfulness and Factuality in Abstractive Summarization. In Jurafsky, D.; Chai, J.; Schluter, N.; and Tetreault, J., eds., Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, 19061919. Online: Association for Computational Linguistics. OpenAI. 2024. Learning to reason with LLMs. Pan, M. Z.; Cemri, M.; Agrawal, L. A.; Yang, S.; Chopra, B.; Tiwari, R.; Keutzer, K.; Parameswaran, A.; Ramchandran, K.; Klein, D.; Gonzalez, J. E.; Zaharia, M.; and Stoica, I. 2025. Why Do Multiagent Systems Fail? In ICLR 2025 Workshop on Building Trust in Language Models and Applications. Papineni, K.; Roukos, S.; Ward, T.; and Zhu, W.-J. 2002. Bleu: Method for Automatic Evaluation of Machine Translation. In Isabelle, P.; Charniak, E.; and Lin, D., eds., Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, 311318. Philadelphia, Pennsylvania, USA: Association for Computational Linguistics. Qin, L.; Pan, W.; Chen, Q.; Liao, L.; Yu, Z.; Zhang, Y.; Che, W.; and Li, M. 2023. End-to-end Task-oriented Dialogue: Survey of Tasks, Methods, and Future Directions. In Bouamor, H.; Pino, J.; and Bali, K., eds., Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, 59255941. Singapore: Association for Computational Linguistics. Qwen; :; Yang, A.; Yang, B.; Zhang, B.; Hui, B.; Zheng, B.; Yu, B.; Li, C.; Liu, D.; Huang, F.; Wei, H.; Lin, H.; Yang, Volume 1, Long Papers, 438449. Valencia, Spain: Association for Computational Linguistics. Xu, H.-D.; Mao, X.-L.; Yang, P.; Sun, F.; and Huang, H. 2024. Rethinking Task-Oriented Dialogue Systems: From Complex Modularity to Zero-Shot Autonomous Agent. In Ku, L.-W.; Martins, A.; and Srikumar, V., eds., Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 27482763. Bangkok, Thailand: Association for Computational Linguistics. Zheng, L.; Chiang, W.-L.; Sheng, Y.; Zhuang, S.; Wu, Z.; Zhuang, Y.; Lin, Z.; Li, Z.; Li, D.; Xing, E.; Zhang, H.; Gonzalez, J. E.; and Stoica, I. 2023. Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena. In Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track. J.; Tu, J.; Zhang, J.; Yang, J.; Yang, J.; Zhou, J.; Lin, J.; Dang, K.; Lu, K.; Bao, K.; Yang, K.; Yu, L.; Li, M.; Xue, M.; Zhang, P.; Zhu, Q.; Men, R.; Lin, R.; Li, T.; Tang, T.; Xia, T.; Ren, X.; Ren, X.; Fan, Y.; Su, Y.; Zhang, Y.; Wan, Y.; Liu, Y.; Cui, Z.; Zhang, Z.; and Qiu, Z. 2025. Qwen2.5 Technical Report. arXiv:2412.15115. Rawte, V.; Chakraborty, S.; Pathak, A.; Sarkar, A.; Tonmoy, S. T. I.; Chadha, A.; Sheth, A.; and Das, A. 2023. The Troubling Emergence of Hallucination in Large Language Models - An Extensive Definition, Quantification, and Prescriptive Remediations. In Bouamor, H.; Pino, J.; and Bali, K., eds., Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, 25412573. Singapore: Association for Computational Linguistics. Roller, S.; Dinan, E.; Goyal, N.; Ju, D.; Williamson, M.; Liu, Y.; Xu, J.; Ott, M.; Smith, E. M.; Boureau, Y.-L.; and Weston, J. 2021. Recipes for Building an Open-Domain Chatbot. In Merlo, P.; Tiedemann, J.; and Tsarfaty, R., eds., Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, 300325. Online: Association for Computational Linguistics. Schulman, J.; Wolski, F.; Dhariwal, P.; Radford, A.; and Klimov, O. 2017. Proximal Policy Optimization Algorithms. arXiv:1707.06347. Shao, Z.; Wang, P.; Zhu, Q.; Xu, R.; Song, J.; Bi, X.; Zhang, H.; Zhang, M.; Li, Y. K.; Wu, Y.; and Guo, D. 2024. DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models. arXiv:2402.03300. Thakur, A. S.; Choudhary, K.; Ramayapally, V. S.; Vaidyanathan, S.; and Hupkes, D. 2025. Judging the Judges: Evaluating Alignment and Vulnerabilities in LLMsas-Judges. arXiv:2406.12624. Valmeekam, K.; Marquez, M.; Sreedharan, S.; and Kambhampati, S. 2023. On the Planning Abilities of Large Language Models - Critical Investigation. In Thirty-seventh Conference on Neural Information Processing Systems. Wang, P.; Li, L.; Chen, L.; Cai, Z.; Zhu, D.; Lin, B.; Cao, Y.; Kong, L.; Liu, Q.; Liu, T.; and Sui, Z. 2024. Large Language Models are not Fair Evaluators. In Ku, L.-W.; Martins, A.; and Srikumar, V., eds., Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 94409450. Bangkok, Thailand: Association for Computational Linguistics. Wang, X.; Shi, W.; Kim, R.; Oh, Y.; Yang, S.; Zhang, J.; and Yu, Z. 2019. Persuasion for Good: Towards Personalized Persuasive Dialogue System for Social Good. In Korhonen, A.; Traum, D.; and M`arquez, L., eds., Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, 56355649. Florence, Italy: Association for Computational Linguistics. Wen, T.-H.; Vandyke, D.; Mrkˇsic, N.; Gaˇsic, M.; RojasBarahona, L. M.; Su, P.-H.; Ultes, S.; and Young, S. 2017. Network-based End-to-End Trainable Task-oriented Dialogue System. In Lapata, M.; Blunsom, P.; and Koller, A., eds., Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Related Work Traditional telephone sales have long faced severe challenges such as high labor costs, high employee turnover rates, and bottlenecks in conversion efficiency (Holman 2002; Ahearne, Mathieu, and Rapp 2005; Lloyd 2020). These inherent business pain points provide clear motivation and broad application prospects for the intervention of conversational AI technology. Early research in conversational AI primarily focused on Task-Oriented Dialogue Systems (TODS) (Gao, Galley, and Li 2018b; Qin et al. 2023). TODS typically employ modular pipeline architecture, including components such as Natural Language Understanding, Dialogue State Tracking, Dialogue Policy, and Natural Language Generation. This architecture demonstrates high reliability in handling dialogues with clear goals and fixed processes (e.g., booking tickets, querying the weather). However, the rigidity of its design, high costs for domain extension, and vulnerability to unexpected user inputs make it difficult to meet the flexibility and persuasive skills required for telephone sales (Wen et al. 2017; Feng et al. 2023). With the advent of Large Language Models, end-to-end generative dialogue systems have become the mainstream paradigm (Chung et al. 2023; Dong, Chen, and Yang 2025). To adapt general-purpose LLMs to specific domains, the main technical paths are divided into Fine-tuning and In-Context Learning. Fine-tuning can deeply inject domain-specific knowledge into the model by updating its parameters on domain data, but this process is associated with high computational and time costs, and carries the risk of catastrophic forgetting, which may impair the models original general capabilities (Kirkpatrick et al. 2017; Luo et al. 2025; Li et al. 2024). In contrast, In-Context Learning guides the model by providing task examples in the prompt, offering greater flexibility and cost-effectiveness, but the stability and depth of its knowledge injection are significantly affected by the context window length and the choice of examples, making it difficult to ensure consistency in long-process tasks (Brown et al. 2020; Liu et al. 2024; Agarwal et al. 2024). RL has promoted the success of reasoning models (OpenAI 2024; DeepSeekAI et al. 2025; Chen et al. 2025b), but its application has mainly focused on tasks with closed-form solutions like code and mathematics. How to utilize reinforcement learning to optimize multi-turn interaction strategies in Persuasive Dialogue remains an under-explored research gap. The evaluation of generative AI dialogue systems is also key challenge. Traditional automatic metrics based on word overlap (Papineni et al. 2002; Lin 2004) have been shown to have very low correlation with human judgments of open-ended dialogue quality (Liu et al. 2016).To this end, academia and industry have begun to explore new evaluation paradigms represented by LLM-as-a-Judge(Zheng et al. 2023; Chan et al. 2024). This method utilizes powerful LLM as judge to score and evaluate the responses generated by models. Although this method shows efficiency advantages in automated evaluation, its own bias issues, the stability of evaluation results, and their consistency with real human judgments have also attracted extensive attention and in-depth research (Wang et al. 2024; Thakur et al. 2025). This highlights the necessity of establishing more reliable and comprehensive evaluation system tailored to specific tasks, such as sales conversion rates. Detailed Evaluation Framework Components This appendix provides detailed descriptions of the core components of our evaluation framework introduced in Section 2.2. B.1 Core Sales Capabilities Our framework identifies six core capabilities essential for successful telemarketing interactions. These capabilities ensure holistic evaluation of the model performance. Role-playing: This assesses the models ability to consistently maintain predefined persona, such as an experienced and professional account manager. The evaluation focuses on whether the models tone, language, and conversational focus align with the specified role throughout the dialogue. Business Analysis: This capability measures the models proficiency in leveraging user-specific data to deliver personalized and persuasive sales pitch. key aspect is the models ability to ground its analysis strictly within the provided context, making relevant connections between the users business status and the proposed promotional activity without hallucinating information. Activity Introduction: This evaluates the clarity, accuracy, and appeal of the models presentation of the sales activity. The agent must effectively communicate all critical information, including the activitys rules, validity period, and participation methods, ensuring the user can fully comprehend the offer. Idle-chat Rejection: In telemarketing, maintaining focus is crucial. This capability assesses the models skill in politely declining to engage in conversations that deviate from the sales objective. successful model should gracefully redirect the dialogue back to the promotional activity, reinforcing its professional role and the calls purpose. Objection Handling: This measures the models effectiveness in addressing and resolving user inquiries and objections. This includes clarifying ambiguities about the promotion (e.g., duration, calculating rewards) and articulating the value proposition to alleviate the users concerns. Operational Guidance: This capability evaluates the models ability to provide clear, actionable instructions that guide the user to locate and participate in the activity. This is critical final step to convert interest into action. Metric Description"
        },
        {
            "title": "Guideline Adherence\nFactual Correctness\nLogical Coherence\nUser Need Fulfillment\nResponse Richness\nSafety\nCompleteness",
            "content": "Conformance to predefined sales rules, policies, and ethical guidelines. Correctness of all presented information against the reference context. Clarity, sound reasoning, consistency, and contextual relevance of the response. Effectiveness in addressing the customers explicit and implicit needs. Diversity, and the informativeness of the agents responses, avoiding repetition. Absence of false promises, misleading content, or other potentially harmful content. Coverage of all critical information points and standard operating procedures required for the dialogue turn. Table 5: Multi-dimensional Evaluation Metrics. B.2 Multi-dimensional Evaluation Metrics To provide fine-grained and consistent assessment across all capabilities, we evaluate each of the models responses using rubric of seven qualitative metrics. These metrics ensure that our evaluation is not only comprehensive but also deeply rooted in the practical requirements of successful sales interaction. As shown in Table 5, the seven metrics are: Guideline Adherence, Factual Correctness, Logical Coherence, User Need Fulfillment, Response Richness, Safety, and Completeness. This appendix provides comprehensive description of the three main stages of our data construction pipeline."
        },
        {
            "title": "C Detailed Data Construction Methodology",
            "content": "C.1 Stage 1: Asset Distillation and Scenario Synthesis To ensure TeleSalesCorpus is grounded in reality, we began with seed collection of anonymized, real-world sales conversations. Instead of using this data directly, we performed structured analysis to distill reusable components. and State-Conditioned Dialogue Flow Modeling structelemarketture, we first manually annotated our ing flow: Opening Business Analysis Promotion Introduction UI Guidance Ascertain Intent & Handle Objections Polite Closing. For each turn in these dialogues, we created an interaction chunk consisting of (User Utterance, Agent Response) pair and tagged each chunk with its corresponding dialogue state. Each chunks User Utterance was then embedded and stored in vector database, creating state-conditioned index for targeted retrieval during simulation. to model seed collection of real dialogues the canonical Indexing To conversational enforce logical Scenario Synthesis Using insights from the real-world data, we synthesized diverse set of dynamic business scenarios. We created pool of 5 distinct promotional campaigns. For each dialogue to be generated, random combination of 1-to-3 campaigns was sampled from this pool. We then used GPT-4 to author detailed knowledge base for each promotion, guided by structured templates. This ensures every dialogue is grounded in unique, complex, and logically consistent set of business constraints. Below is an example of knowledge base entry. - [Promotion Name] Flash Recharge Bonus - [Objective] To encourage users to increase their advertising budget by offering immediate value. - [Eligibility Criteria] Users who have been online for less than 90 days. - [Pricing Tiers] Recharge 50/100, receive 10/25 bonus coupon. - [Operational Rules] The bonus coupon is valid for 30 days and can be used for Keyword Bidding and Homepage Banner ads only. The coupon cannot be used to purchase other services or exchanged for cash. Limit one bonus per user during the campaign period. Figure 5: Knowledge Base Entry Example C.2 Stage 2: State-Aware Three-Agent Dialogue Simulation We designed an LLM-mediated simulation framework involving three distinct GPT-4-powered agents: Sales Agent, User Agent, and Dialogue Manager. (See Appendix for the detailed prompt of each agent). The simulation is initiated by the User Agent. The framework then enters turn-by-turn generation loop. Each cycle of the loop is driven by the users reply and proceeds through the following five steps to generate the subsequent Sales Agent response: 1. User Response Generation: The RESPONSE text from the Sales Agents previous turn is sent to the User Agent. The User Agent, guided by its independent persona and the dialogue history, formulates and delivers its reply. 2. LLM-Based State Adjudication: Immediately following the users reply, the Dialogue Manager LLM receives the full context: the current state from the previous turn, the Sales Agents PROPOSED NEXT STATE, and the User Agents actual response. It analyzes this information to make final, authoritative judgment on the true state of the conversation, which becomes the new current state. 3. State-Conditioned Retrieval: With the dialogue state now finalized for the current turn, the Dialogue Manager takes the users latest utterance and queries the vector database. Crucially, this search is filtered to only include interaction chunks tagged with the newly adjudicated current state. 4. Dynamic Prompt Assembly: The Dialogue Manager assembles new, context-rich prompt for the Sales Agent. This prompt includes the full dialogue history and is dynamically augmented with the retrieved real-world example, which serves as style and strategy guide for that specific turn. 5. Sales Agent Response Generation: The Dialogue Manager sends the complete prompt to the Sales Agent LLM. The Sales Agent processes this input and generates its action in the structured format: [RESPONSE]: <Your response to the user> and [PROPOSED NEXT STATE]: <The dialogue state you intend to transition to>. The RESPONSE generated in the final step is then delivered back to the User Agent, initiating the next cycle of the loop. C.3 Stage 3: Quality Assurance and Refinement multi-faceted quality assurance process was implemented. First, all synthesized business scenarios and rules underwent manual review by domain experts to confirm their plausibility. After generation, we applied automated scripts to filter an initial corpus of 2500 dialogues based on several criteria: Dialogues with fewer than 4 turns were discarded. Dialogues with high n-gram overlap between consecutive agent turns were removed. Dialogues containing unreplaced placeholder strings were filtered. Dialogues where the agent failed to mention the keywords of the sampled promotions were discarded as off-task. Finally, for each distinct promotion type, we randomly sampled 20 full dialogues for manual review, guided by rubric assessing coherence, realism, and strict factual faithfulness. After all filtering stages, we obtained our final, high-fidelity dataset of 2000 multi-turn conversations. Detailed Formulation of GRPO This section provides the detailed mathematical formulation for the Group Relative Policy Optimization algorithm referenced in the main text. GRPO adapts the Proximal Policy Optimization (PPO) framework (Schulman et al. 2017). Its key innovation is to estimate the advantage function by normalizing the rewards obtained from group of parallel rollouts. This approach circumvents the need for an explicit value model, thereby eliminating the associated training overhead common in standard PPO implementations. The advantage function A(i) in GRPO is defined as the standardized measure of the i-th samples reward, R(i), within its group. This is formalized as: A(i) = R(i) EjU (1,G)[R(j)] (cid:113) VjU (1,G)[R(j)] + ϵ (7) Here, E[] and V[] denote the empirical mean and variance over the set of rewards {R(j)}G j=1 from the rollouts, and ϵ is small constant for numerical stability. This group-normalized advantage is then used to optimize the final objective function, which incorporates the clipped surrogate objective from PPO and KL-divergence penalty term to regularize policy updates: JGRP O(θ) = qP (Q),{oi}G (cid:32) (cid:40) i=1πθold (Oq) 1 G (cid:88) i=1 1 oi oi (cid:88) t=1 min πθ(oi,tqi, oi,<t) πθold(oi,tqi, oi,<t) A(i), clip (cid:18) πθ(oi,tqi, oi,<t) πθold (oi,tqi, oi,<t) , 1 ϵ, 1 + ϵ (cid:33) (cid:19) A(i) βDKL (cid:2)πθπref (cid:41) (8) (cid:3) Theoretical Derivation of the Bayesian-Supervised Reasoning Reward This appendix elaborates on the theoretical foundations and step-by-step derivation of the Bayesian-Supervised Reasoning reward (Rbayes), as defined in Equation 3 of the main text. E.1 Bayes Theorem: Theoretical Background Bayes theorem is fundamental in probability theory that describes how to update the probability of hypothesis based on new evidence. Its mathematical form is as follows: (HE) = (EH)P (H) (E) (9) where: (HE) is the posterior probability: The probability of the hypothesis being true after observing the evidence E. This is the updated belief we aim to find. (EH) is the likelihood: The probability of observing the evidence given that the hypothesis is true. It measures how well the hypothesis explains the evidence. (H) is the prior probability: The initial probability of the hypothesis being true, before considering any evidence. It represents our prior belief in H. (E) is the marginal likelihood of evidence: The total probability of observing the evidence E. The core idea of Bayes theorem is that the posterior is proportional to the likelihood times the prior. It provides mathematically rigorous framework for updating our beliefs from prior state in light of new evidence. E.2 Applying Bayes Theorem to Reasoning Generation In our task, we map the components of Bayes theorem as follows: Hypothesis is the model-generated reasoning chain ht. We hypothesize that this is good and effective reasoning process. Evidence is the given reference answer . We use this evidence to evaluate the quality of our hypothesis (the reasoning chain). Our objective is to find an optimal reasoning chain, hoptimal answer . This is precisely maximum posteriori estimation problem: , that maximizes the posterior probability given the reference Substituting our variables into Bayes theorem yields: hoptimal = arg max ht (T htA ) (T htA ) = (A ht)P (T ht) (A ) (10) (11) When maximizing this expression with respect to ht, the denominator (A is constant for all candidate reasoning chains. Therefore, we can omit it from the optimization objective: ) is the probability of the reference answer, which arg max ht (T htA ) = arg max ht (A ht)P (T ht) (12) The term on the right-hand side, (A maximize this joint probability. ht)P (T ht), is the joint probability (T ht, ). high-quality reasoning chain should For computational convenience and numerical stability (to avoid underflow from multiplying many small probabilities), we typically optimize in log-space. Since the logarithm is monotonically increasing function, maximizing positive value is equivalent to maximizing its logarithm: arg max ht log (T ht, ) = arg max ht (log (T ht) + log (A ht)) (13) Thus, we have successfully transformed the MAP problem into one of maximizing the log-joint probability. We define our reward Rbayes as this log-joint probability, which naturally decomposes into two meaningful components: Rbayes(T ht, ) = log (T ht) (cid:125) (cid:123)(cid:122) Log-Prior (cid:124) + log (A (cid:123)(cid:122) Log-Likelihood T ht) (cid:125) (cid:124) (14) These two components are estimated by the language model πθ itself and are finally autoregressively decomposed to arrive at the computable form presented in Equation 3 of the main text. This part provides detailed description of the two stages of the DOGA framework."
        },
        {
            "title": "F DOGA Framework Implementation Details",
            "content": "F.1 Offline Stage: Structured Script Library Construction The construction of our structured script library involves three-step process designed to distill best practices from historical data into reusable and efficient resource. Data Collection and Annotation: We begin with corpus of historical telemarketing dialogues with high conversion rates. Using GPT-4, we programmatically classify each turn according to predefined set of dialogue intents and annotate the user-specific information utilized (e.g., recent recharge status). High-Quality Script Extraction: We use GPT-4 to extract concise, persuasive, and generalizable scripts from the annotated dialogues that directly contribute to achieving the annotated intent. Template Generation via Clustering and Summarization: The extracted scripts are refined into reusable templates. We first encode scripts into vectors using Qwen3-Embedding-0.6B and group them using greedy clustering algorithm (cosine similarity > 0.8). Then, we employ GPT-4 to summarize each cluster into generic template with standardized placeholders. The final output is structured library where templates are indexed by dialogue intent for efficient retrieval. F.2 Online Stage: Real-time Dialogue Management Details Constrained Dialogue Intent Classification Before generating response, lightweight classifier predicts the most appropriate dialogue intent. Model: We fine-tune Qwen2.5-7B model as our intent classifier, which takes the conversation history and previous intents as input. Intent Transition Rules: We define finite-state machine that dictates valid transitions between intents. This constrains the classifiers prediction to valid subset of intents based on the conversations history, significantly improving accuracy and coherence. Dynamic Prompt Assembly Once the intent for the current turn It is determined, the framework assembles tailored system prompt Pt. The prompt is formally composed as: Pt = Pstatic(Ht1) D(It, ) (15) where Pstatic is base prompt containing the models core persona and the full dialogue history up to turn 1 (Ht1), denotes concatenation, and D(It, ) is the dynamic prompt component. This dynamic part is function of the predicted intent It and the user profile . The assembly of D(It, ) involves: 1. Template Retrieval: Based on the predicted intent It, corresponding templates (instructions, key points, reminders) are retrieved from the script library. 2. Personalization: Placeholders within the retrieved template are populated with the users specific information from their profile . The fully assembled prompt Pt is then passed to the model. Performance on Real-World Data To further validate the robustness of our proposed Bayesian reward, we replicated the ablation study on our held-out Realworld Tele-sales Dataset. This dataset, derived from anonymized expert conversations, is inherently noisier and more complex than the synthetic data. As illustrated in Figure 6, the performance trends are consistent with our primary findings. Although the absolute reward scores are naturally lower due to the increased difficulty of the dataset, the model trained with Rbayes again demonstrates markedly more stable learning curve and achieves higher final convergence point. This confirms that the benefits of supervising the internal thought process via Rbayes are not limited to controlled, synthetic scenarios but also translate effectively to the challenges of real-world conversational data. Figure 6: Performance validation on the Real-world Tele-sales Dataset. The stabilizing effect of Rbayes remains consistent, even on more complex, non-synthetic data."
        },
        {
            "title": "H Agent Prompts",
            "content": "### Your Persona You are the owner of \"The Corner Bistro\". You are busy and practical. You are open to good ideas but are very careful with your budget because your business is new. You are currently worried about the large number of competing Italian restaurants in your neighborhood. ### Your Task Respond naturally to the sales agent. Raise objections based on your persona, especially concerning cost and effectiveness. Figure 7: User Agent Prompt Example ### Task Analyze the conversation snippet and determine the true dialogue state. The Sales Agent attempted to move the conversation to [PROPOSED NEXT STATE]. Based on the [USER RESPONSE], did the transition succeed? Choose the most accurate next state from the available list. ### Available States - Opening - Business Analysis - Promotion Introduction - UI Guidance - Ascertain Intent & Handle Objections - Polite Closing ### Context Current State: Business Analysis Agents Proposed Next State: Promotion Introduction Users Actual Response: Wait, before that, have another question about my business analysis. You said my click-through rate was low. What can do about that specifically?\" ### Output Business Analysis Figure 8: Dialogue Manager Prompt Example ### Role and Task You are senior sales consultant. You are professional, patient, and an expert in helping new restaurant owners succeed. Your task is to generate response to the users last message. After crafting your response, you must also determine the most logical next state for the conversation from the available options. Your response should naturally lead the conversation into the state you propose. ### Available Dialogue States - Opening - Business Analysis - Promotion Introduction - UI Guidance - Ascertain Intent & Handle Objections - Polite Closing ### Current Dialogue State Business Analysis ### Dialogue History User: \"Business has been bit slow since we opened. There are lot of other Italian places around here, so its hard to get noticed.\" ### Style Guidance for THIS TURN Emulate the style and strategy of the AGENT in the following real-world example, which was retrieved because it is highly relevant to the users last message and the current Business Analysis stage: User: \"We just opened, so things are still bit slow.\" AGENT: \"Understood. Thats very common for new shops. Have you had chance to look at your customer traffic data in the app yet? That can give us good baseline.\" ### Domain Knowledge for THIS CALL You must strictly adhere to the following information. Do not mention promotions the user is not eligible for. Promotion 1: \"Flash Recharge Bonus\"** - Objective: To encourage users to increase their advertising budget by offering immediate value. - Eligibility Criteria: Users who have been online for less than 90 days and have an average daily ad spend of less than $10. - Pricing Tiers: Recharge $50/$100, receive 10/25 bonus coupon.\" - Operational Rules: \"The bonus coupon is valid for 30 days and can be used for Keyword Bidding and Homepage Banner ads only. Limit one bonus per user.\" Promotion 2: \"New Customer Welcome Offer\" - Objective: To help new users attract their first set of customers with compelling discount. - Eligibility Criteria: Users who have been online for less than 30 days. - Offer Details: \"The platform will sponsor 20% off your first order coupon for your store. The cost is fully covered by the platform for the first 50 redemptions. This offer is displayed prominently to users browsing your area.\" - OperationalRules: \"The offer runs for 14 days after activation. No cost to the user.\" ### User Profile - business name: \"The Corner Bistro\" - category: \"Italian Restaurant\" - time since onboarding: \"15 days\" - recent ad spend:** \"$5\" - synthesized pain point: \"High competition in the area; struggling to stand out.\" ### Behavioral Guardrails - You must not invent any features, prices, or rules not explicitly listed in the Domain Knowledge Base. - If you do not know the answer to question, state that you will find out and get back to them. - Maintain polite, empathetic, and helpful tone. Do not be pushy. ### Output Format You must generate your output in the following JSON format, and nothing else: { \"RESPONSE\": \"<Your response to the user>\", \"PROPOSED NEXT STATE\": \"<Your choice for the next dialogue state from the available list>\" } Figure 9: System Prompt for Sales Agent Example (Turn-Specific)"
        },
        {
            "title": "I Experimental Settings",
            "content": "Parameter 7B 14B 32B 72B Training Configuration Precision Epochs Num Generations Max Completion Length Reward Weights Global Batch Size Learning Rate (LR) Warmup Ratio DeepSpeed ZeRO Stage Hardware Configuration Num GPUs(80 GB) Resource Utilization Peak GPU Memory Usage Total Training Time (h) BF16 2 4 128 1,1,5,7 256 5 106 0.1 2 BF16 2 4 128 1,1,5,7 256 5 106 0.1 3 BF16 2 4 128 1,1,5,7 256 5 105 0.1 3 BF16 2 4 128 1,1,5,7 256 5 106 0.1 8 95% 4 8 95% 12 32 95% 32 85% 26 Table 6: Detailed hyperparameters and resource utilization for scalability experiments."
        }
    ],
    "affiliations": [
        "Chinese Information Processing Laboratory, Institute of Software, Chinese Academy of Sciences",
        "University of Chinese Academy of Sciences"
    ]
}