{
    "paper_title": "ProtoGCD: Unified and Unbiased Prototype Learning for Generalized Category Discovery",
    "authors": [
        "Shijie Ma",
        "Fei Zhu",
        "Xu-Yao Zhang",
        "Cheng-Lin Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Generalized category discovery (GCD) is a pragmatic but underexplored problem, which requires models to automatically cluster and discover novel categories by leveraging the labeled samples from old classes. The challenge is that unlabeled data contain both old and new classes. Early works leveraging pseudo-labeling with parametric classifiers handle old and new classes separately, which brings about imbalanced accuracy between them. Recent methods employing contrastive learning neglect potential positives and are decoupled from the clustering objective, leading to biased representations and sub-optimal results. To address these issues, we introduce a unified and unbiased prototype learning framework, namely ProtoGCD, wherein old and new classes are modeled with joint prototypes and unified learning objectives, {enabling unified modeling between old and new classes}. Specifically, we propose a dual-level adaptive pseudo-labeling mechanism to mitigate confirmation bias, together with two regularization terms to collectively help learn more suitable representations for GCD. Moreover, for practical considerations, we devise a criterion to estimate the number of new classes. Furthermore, we extend ProtoGCD to detect unseen outliers, achieving task-level unification. Comprehensive experiments show that ProtoGCD achieves state-of-the-art performance on both generic and fine-grained datasets. The code is available at https://github.com/mashijie1028/ProtoGCD."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 ] . [ 1 5 5 7 3 0 . 4 0 5 2 : r IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 1 ProtoGCD: Unified and Unbiased Prototype Learning for Generalized Category Discovery Shijie Ma, Fei Zhu, Xu-Yao Zhang, Senior Member, IEEE, and Cheng-Lin Liu, Fellow, IEEE AbstractGeneralized category discovery (GCD) is pragmatic but underexplored problem, which requires models to automatically cluster and discover novel categories by leveraging the labeled samples from old classes. The challenge is that unlabeled data contain both old and new classes. Early works leveraging pseudo-labeling with parametric classifiers handle old and new classes separately, which brings about imbalanced accuracy between them. Recent methods employing contrastive learning neglect potential positives and are decoupled from the clustering objective, leading to biased representations and sub-optimal results. To address these issues, we introduce unified and unbiased prototype learning framework, namely ProtoGCD, wherein old and new classes are modeled with joint prototypes and unified learning objectives, enabling unified modeling between old and new classes. Specifically, we propose dual-level adaptive pseudo-labeling mechanism to mitigate confirmation bias, together with two regularization terms to collectively help learn more suitable representations for GCD. Moreover, for practical considerations, we devise criterion to estimate the number of new classes. Furthermore, we extend ProtoGCD to detect unseen outliers, achieving task-level unification. Comprehensive experiments show that ProtoGCD achieves state-of-the-art performance on both generic and fine-grained datasets. Index TermsGeneralized Category Discovery, Open-World Learning, Semi-Supervised Learning, Prototype Learning."
        },
        {
            "title": "H UMANS are capable of discovering and acquiring novel",
            "content": "concepts based on what they have learned [1], [2], [3]. Consider that kid has been taught to recognize some species (e.g., cat, panda, car) and gradually grasp some general knowledge, i.e., what constitutes class. Then, the kid could cluster some tiger images together and regard them as novel category even without learning them before, as shown in Fig. 1. Accordingly, it is important to empower such ability to deep learning and make it more applicable in the open-world [3], [4], [5], [6], where samples from new classes might emerge and models are expected to discover them by transferring the knowledge from old classes. Recently, novel category discovery (NCD) [1], [2], [7], [8], [9], [10], [11] has been introduced to solve the aforementioned problem. Formally, NCD aims to automatically cluster the unlabeled novel classes by leveraging the knowledge learned from old classes in the labeled dataset. It assumes that unlabeled data exclusively comprises samples from novel categories, which often fails to hold in reality. By relaxing such strong assumption, Vaze et al. [12] extended NCD to more pragmatic setting, called generalized category This work has been supported by the National Science and Technology Major Project (2022ZD0116500), National Natural Science Foundation of China (62222609, 62076236), CAS Project for Young Scientists in Basic Research (YSBR-083), Key Research Program of Frontier Sciences of CAS (ZDBS-LY-7004), and the InnoHK program. (Corresponding author: Xu-Yao Zhang.) Shijie Ma, Xu-Yao Zhang and Cheng-Lin Liu are with the State Key Laboratory of Multimodal Artificial Intelligence Systems, Institute of Automation, Chinese Academy of Sciences, 95 Zhongguancun East Road, Beijing 100190, P.R. China, and also with the School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing 100049, China. Email: mashijie2021@ia.ac.cn, {xyz, liucl}@nlpr.ia.ac.cn. Fei Zhu is with the Centre for Artificial Intelligence and Robotics, Hong Kong Institute of Science and Innovation, Chinese Academy of Sciences, Hong Kong 999077, P.R. China. Email: zhfei2018@gmail.com. Code is available at https://github.com/mashijie1028/ProtoGCD. Fig. 1: Generalized category discovery. Given dataset with labeled data from old classes and unlabeled data from both old and novel categories. The objective is to classify old classes and cluster new categories in the unlabeled data. discovery (GCD). In GCD, images from unlabeled data could contain both old and new classes. In this paper, we tackle the task of GCD [12], [13], [14], [15] as illustrated in Fig. 1, which is challenging openworld [3], [5], [6] setting in that models need to simultaneously discover novel categories and recognize old classes coexisting in the unlabeled data. Pioneer works [12], [13] resort to the supervised [16] and unsupervised contrastive learning [17] on labeled data and unlabeled data, respectively. And non-parametric semi-supervised K-means [12], [18] is employed upon the learned features for clustering. However, contrastive learning alone ignores underlying positives and is susceptible to class collision [19]. Furthermore, pure contrastive learning is essentially decoupled with the clustering objective of GCD, leading to biased representations and sub-optimal performance. Another line of works [2], [20] use pseudo-labels and handle old and novel classes with separate classification heads and learning objectives. These methods tend to be biased toward old classes [12], and bring about IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 2 Fig. 2: The unified and unbiased characteristics of ProtoGCD, which contribute to addressing the issues of prior methods. imbalanced accuracies between old and new classes. The problems of preceding methods are summarized in Fig. 2. To solve the issues above, we propose unified and unbiased Prototype Learning framework for Generalized Category Discovery (ProtoGCD), which handles old and novel categories jointly in shared feature space with the same set of learnable prototypes. There are two key insights to solve the issues of prior methods: (1) The first is the unification between old and new classes (Fig. 2 (a)). We model old and new classes with joint classifier and unified learning objectives, which helps alleviate the imbalanced performance of prior parametric methods [2], [20], as shown in Fig. 3 (a) and (b). (2) Secondly, the model is equipped with parametric prototypical classifier and self-trained with pseudo-labels, which aligns more closely with the clustering objective and learns more suitable representations for GCD (Fig. 2 (c)) than contrastive learning-based methods [12], [13], [14], [15]. Specifically, considering the challenging annotation conditions in GCD, we propose dual-level adaptive pseudolabeling (DAPL) mechanism. The model adaptively adjusts both the type and proportion of pseudo-labels assigned to unlabeled samples, according to the samples confidence and the models performance. DAPL ensures efficient and stable self-training while effectively circumventing confirmation bias [21] (Fig. 2 (d)). Additionally, two regularization terms (Fig. 2 (c)) are further proposed to avoid trivial solutions of clustering and learn better features. Besides, we propose novel criterion that simultaneously considers the feature space and the classification performance of old classes to precisely estimate the number of new classes, enabling our method to manage the more challenging situation where the number of novel categories is unknown. As whole, the feature extractor and learnable prototypes are trained together in an end-to-end manner, making ProtoGCD learn efficiently and achieve remarkable performance. Furthermore, beyond GCD, we explore the unification at the task level (Fig. 2 (b)), and extend ProtoGCD to detect unseen outliers. As in Fig. 3 (c), ProtoGCD could classify both the old classes and the previously discovered new classes, as well as detect unseen outliers, which makes it potentially unified open-world classifier. Our main contributions are summarized as follows: We propose ProtoGCD, unified and unbiased framework for the task of GCD, which effectively addresses the issues of imbalanced performance and biased representations in previous methods. The unified modeling helps ProtoGCD achieve balanced accuracy between old and new classes, and we propose dual-level adaptive pseudo-labeling and regularizations to learn unbiased representations. We devise Prototype Score to estimate the number of novel classes, making our method more practical. At the task level, we extend ProtoGCD to detect outliers from unseen classes, and achieve the unification of multiple tasks. Experiments on generic and fine-grained datasets show that ProtoGCD outperforms previous state-ofthe-art methods by large margin and Prototype Score obtains more accurate class number estimation. The remainder is organized as follows: Section 2 shows related works. Section 3 introduces the proposed ProtoGCD. Section 4 presents the class number estimation algorithm and Section 5 extends ProtoGCD to detect unseen outliers. Section 6 provides comprehensive experiments and Section 7 concludes the paper and outlines future works."
        },
        {
            "title": "2 RELATED WORKS\n2.1 Novel Category Discovery",
            "content": "Novel category discovery (NCD) is initially formulated as deep transfer clustering [23] problem. The core spirit is to leverage the knowledge learned from labeled classes to cluster unlabeled data from novel categories. AutoNovel [2], [7] is seminal work involving three steps. Models are firstly pre-trained via self-supervision and then fine-tuned on labeled datasets. Finally, models transfer knowledge from labeled data to unlabeled data through rank statistics [7], [10]. UNO [20] proposed unified objective and assigned pseudolabels with swapped prediction [24], while OpenMix [8] and NCL [9] further explored the relationship between labeled and unlabeled data. 2.2 Generalized Category Discovery Vaze et al. [12] relaxed the assumption in NCD that all unlabeled data comes from novel classes and formalized more pragmatic task called generalized category discovery (GCD). We categorize existing methods into two groups. (1) Contrastive learning-based methods with non-parametric classifiers. Pioneering works [12], [13] employed contrastive learning followed by non-parametric semi-supervised Kmeans clustering [12], [18]. Subsequent works explore more underlying relationships. Zhao et al. [14] extended prototypical contrastive learning [25] to an EM-like learning framework. Pu et al. [15] proposed dynamic conceptional contrastive learning, which alternates between conception estimation and conceptional representation learning. In these IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 3 Fig. 3: Unified prototype learning framework. (a) Previous GCD methods [7], [20], [22] with parametric classifiers employ distinct classification heads or training objectives for old and new classes, while (b) ProtoGCD models old and new classes in shared feature space with unified set of prototypes (i.e., classifier) and adopts unified learning objectives across old and new classes. (c) During inference, ProtoGCD could classify both the old and the newly discovered classes. Moreover, it could also be extended to reject unseen outliers, which makes ProtoGCD general-purpose open-world classifier. methods, feature representation learning is decoupled with and not optimal for subsequent clustering. (2) Pseudo-labelingbased methods with parametric-classifiers, like adapted methods from NCD [2], [20]. These methods implement separate classification heads on old and new classes, leading to imbalanced performance, and the predictions are easily biased to old classes. More recently, some works enhance the performance of GCD by exploiting instance-wise neighbors [26] and complementary textual modality [27]. While others extend GCD to more learning paradigms [28], [29] and scenarios [30], [31]. To address the respective problems of the two types of methods, we propose to unify old and novel classes modeling with learnable prototypes and devise proper pseudo-labeling mechanism to circumvent confirmation bias. Moreover, regarding that most methods assume the number of novel categories is known a-prior, only few [2], [12], [14] tackle the estimation issue. We also propose to estimate the class number precisely to make GCD more applicable. Here, we summarize the differences between ProtoGCD and prior works. (1) Compared with non-parametric methods like [12], [14] with contrastive learning, ProtoGCD explicitly learns parametric classifier with discriminative self-training. (2) Compared with the recent parametric-based SimGCD [32], ProtoGCD incorporates generative modeling, and the prototypes represent class-wise distributions, so ProtoGCD could be viewed as hybrid model, while SimGCD is purely discriminative model. Considering the characteristics of the GCD task, we further incorporate dual-level adaptivity into pseudo-labeling and propose separation regularization. 2.3 Out-of-Distribution Detection Out-of-distribution (OOD) detection [33], [34], [35], [36], [37] aims to classify samples from known classes and reject unseen samples outside of the training classes. Conventionally, each sample is assigned score. If the score is higher than predefined threshold, then it is recognized as in-distribution (ID) and classified into one of the known classes, or detected as OOD and rejected. Post-hoc methods aim to devise score functions [33], [38], [39] to increase the separability between ID and OOD instances without training the models. Several works resort to self-supervised learning [40], [41] and logit normalization [42] to train models that inherently excel at OOD rejection. Others explicitly employ auxiliary outliers [43]. OOD detection only requires rejecting OOD samples without any further clustering on them."
        },
        {
            "title": "3 THE PROPOSED METHOD: PROTOGCD",
            "content": "Motivation and Overview. As depicted in Fig. 2, we aim to address the issues of imbalanced performance and biased representations of prior methods. To maintain the balance between old and new classes, we propose to employ unified prototypical classifier and feature space for them (Section 3.1.2). To acquire unbiased and suitable representations for GCD, we utilize contrastive learning for basic representations (Section 3.2). More importantly, we propose an adaptive pseudo-labeling mechanism that dynamically adapts the types and proportions of the pseudo-labels, considering the samples confidence and the models performance (Section 3.3), which helps mitigate confirmation bias. Furthermore, two regularizations (Section 3.4) help avoid trivial solutions and improve inter-class separation, thereby collectively refining the representations. Overall, ProtoGCD is an end-to-end training method, achieving unified learning objectives between old and new classes and aligning better with the clustering objectives of GCD (Section 3.5). The overall pipeline of ProtoGCD is illustrated in Fig. 4. IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 4 Fig. 4: The proposed method ProtoGCD. Left: Overview of ProtoGCD. The blue, purple and orange backgrounds indicate the projection, feature and probability space, respectively. The yellow font represents learning objectives. Right: Dual-Level Adaptive Pseudo-Labeling (DAPL). We adaptively assign hard pseudo-labels to top r% samples by confidence while soft ones for the others, and the ratio r% adaptively ramps up (blue font). ProtoGCD could be trained end-to-end. 3.1 Preliminaries 3.1.1 Problem Formulation and Notation = { old D (xl = ="
        },
        {
            "title": "Y\nC",
            "content": "old in i, yl i) old, and (xu ) consisting of both old classes new, i.e., Formally, given partially-labeled dataset where i=1 } { from the old classes1, i.e., = u, denotes the labeled data j=1 } denotes the unlabeled data with its underlying label old and novel space = new. The objective of GCD classes new and classify is to simultaneously cluster samples from l. The samples from number of old classes Kold = can be obtained directly l, while the number of novel classes Knew = from new is always known a-prior in the literature [12], [13]. We also present an algorithm to estimate Knew in Section 4. = Kold + Knew is the total number of classes. Let ) ) is the projection head. denote the feature extractor, and ϕ( zi = (xi) is the d-dimensional feature representation of the i-th sample xi. hi = ϕ(zi) is in the dh-dimensional projection space for contrastive learning. with the prior knowledge in D old ( 3.1.2 Principled Modeling of ProtoGCD = Unified Feature Space and Prototypes (Classifiers). We adopt ℓ2-normalized d-dimensional hyperspherical feature space, which is compatible with contrastive learning [17], [44] and has less bias between classes. To realize unified modeling of old and new classes, we assign the same set of learnable c=1 where = Kold +Knew, each class prototypes } with one prototype µc. Both zi and µc are ℓ2-normalized in the feature space, and prototypes could be updated on-the-fly. Generative Modeling. ProtoGCD models both old and novel classes jointly in shared d-dimensional hypersphere, i.e. (d 1)-sphere, and each class-wise prototype µc formalizes von MisesFisher (vMF) distribution [45] with the probability density of the i-th sample in the c-th class as follows: µc { pvMF(zi; µc, τ ) = Cp(1/τ ) exp(µ zi/τ ), = 1, 2, , K, (1) 1. In this paper, old classes and labeled classes are synonymous and both refer to the classes that appear in Dl. zi, τ ), where τ is the temperature and κ = 1/τ is the concentration parameter [45] of vMF with Cp(κ) = (2π)p/2Ip/21(κ) and Iv denotes the first kind of Bessel function at order v. The prototype µc is mean direction in vMF. Then we could draw the posterior probability of sample xi belonging to class k: κp/21 p(y = kzi, τ ) = pvMF(zi; µk, τ ) c=1 pvMF(zi; µc, τ ) (cid:80)K = zi/τ ) exp(µ c=1 exp(µ zi/τ ) . (cid:80)K (2) In Eq. (2), logits are computed via cosine similarity between features and class-wise prototypes, and the posterior probability prediction vector p(zi, τ ) RK : p(zi, τ ) = p(y = 1 , p(y = zK, τ ) . (3) (cid:0) The generative modeling with prototypes is more suitable to the open-world and reduces open-space risk [4] as validated in [35], [36], [37]. In this paper, we generalize prototype learning to the more pragmatic setting of GCD, where we model unlabeled new classes with prototypes as well. (cid:1) 3.2 Contrastive Learning To maintain fundamental representations, we employ supervised contrastive learning [16] on and unsupervised contrastive learning (i.e., self-supervised contrastive learning named SimCLR) [17] on respectively, within the projection space, following the convention in the literature [12], [13]. Specifically, given two views (random augmentations) of the input xi and , the unsupervised in mini-batch contrastive learning loss: B con = log iB exp(h 1[j=i] exp(h i/τc) hj/τc) , (4) (cid:88) where 1[] denotes the indicator function and equals to 1 (cid:80) when the condition is true else 0, τc denotes the temperature in contrastive learning. The supervised contrastive learning [16] on labeled data in is: Ll con = 1 Bl (cid:88) iBl 1 (i) (cid:88) qN (i) log hq/τc) exp(h 1[j=i] exp(h hj/τc) (cid:80) , (5) IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE denotes labeled subset of (i) denotes where positive samples with the same label as xi. Then, we combine them and draw the overall contrastive learning objective: and B where λsup is the weight of supervised component. λsup) con + λsupL con, Lcon = (1 (6) 3.3 Dual-Level Adaptive Pseudo-Labeling GCD is semi-supervised setting but subject to more stringent labeling conditions, i.e., unlabeled data contain new classes. Parametric classifiers are supposed to consider both old and novel classes when assigning pseudo-labels. As result, they are more susceptible to confirmation bias. As for pseudo-labels, learning solely on hard pseudo-labels [21], [46], i.e., one-hot targets, is prone to bias accumulation due to overconfidence in incorrect information, particularly during early training stages when the classifier is less performant. Conversely, relying entirely on soft pseudo-labels [47], [48], [49], which are less confident, could hinder model training due to less informative targets. Therefore, it is essential to consider both types of pseudo-labels simultaneously. Thus, we pose question: What constitutes suitable pseudolabels for GCD? Based on the discussions above, the crux of this question is to choose the suitable type of pseudolabels, i.e., soft or one-hot, for each sample and determine the ratio of the two types. We provide two aspects to determine the pseudo-labels: (1) The confidence of samples. Samples confidence varies based on their distribution in the feature space. Those close to the decision boundary exhibit lower confidence, and assigning overly confident hard pseudo-labels to these samples could introduce bias. (2) The models capabilities. During early training phases, the model has relatively weak classification performance and is prone to confirmation bias, which is not readily corrected by the model itself. As training progresses, the model becomes stronger, facilitating the generation of high-quality pseudo-labels. Considering the above aspects, we propose dual-level adaptive pseudo-labeling (DAPL) mechanism. The primary philosophy is to adaptively assign pseudolabels to unlabeled data based on the samples confidence across different training samples and models capability across various training phases. In this way, ProtoGCD is capable of training models efficiently while preventing potential bias. exponential of the cosine similarity between zi and the top-1 prototype and the one with the top-2 prototype: proto_conf(zi) = exp(µ t1(i)zi/τ )/ exp(µ t2(i)zi/τ ). (8) Intuitively, Eq. (8) indicates that the closer zi to the top-1 prototype µt1(i) compared with the top-2 prototype µt2(i), the higher the confidence of zi. Prototype confidence only involves the two most similar prototypes, which is relatively more robust and stable with less noise than using all the prototypes for confidence estimation, e.g., maximum softmax probability [33], [50] (MSP), i.e., maxk p(y = zi, τ ), regarding that large number of unlabeled samples could bring potential bias, especially in early training stages. Moreover, the range of prototype confidence is broader than MSP, enhancing the distinctiveness among samples. Assign Hard or Soft Pseudo-Labels Based on Confidence. The pseudo-label of each sample q(zi) is determined by its confidence. If sample has high confidence, it might be far from the decision boundary and more likely belongs to class argmaxk p(y = zi, τ ), and we assign onehot pseudo-label, which accelerates training with more informative targets. If the confidence is low, hard pseudolabels could easily bring erroneous information, so we choose soft labels instead. Concretely, hard or one-hot pseudo-labels are employed when confidence is above certain threshold δ, otherwise soft labels. p(zi, τbase) is the predictive vector in Eq. (3), then the adaptive pseudo-label of the i-th sample is: q(zi) RK = (cid:40) (cid:16) (cid:17) one_hot p(zi, τbase) , if proto_conf(zi) δ, p(zi, τsharp), if proto_conf(zi) < δ. (9) Here, δ > 1, one_hot( ) denotes the one-hot operation, where the output is one at the index of the maximum input value, and zero for other indices. τbase and τsharp are temperature in the original prediction and pseudo-labels. Temperature controls the hardness/certainty of the pseudo-labels, and lower τ indicates more certain pseudo-labels. In Eq. (9), τbase > τsharp, i.e., for samples with confidence less than δ, we still assign sharpened pseudo-labels p(zi, τsharp) than the original prediction, which encourages the model to gradually make more certain predictions, this sharpening mechanism is of vital importance to steadily enhance the model, which is validated in Section 6.3. The hard pseudo-label could be viewed as special case of the soft one with τ 0. 3.3.1 Level-1: Adaptivity across Training Samples 3.3.2 Level-2: Adaptivity across Training Phases We propose to assign pseudo-labels flexibly based on the confidence of samples, which could help mitigate bias from overconfident pseudo-labels while preventing slow training from overly ambiguous ones. Let µt1(i), µt2(i) denote the top1 and top-2 prototypes of sample zi, having the maximum and second maximum cosine similarities with zi respectively: t1(i) = argmax c=1,2, ,K µ zi, t2(i) = argmax c=1,2, ,K c=t1(i) µ zi, (7) where = Kold+Knew. Here, we define confidence, namely prototype confidence, in Definition 1. Definition 1 (Prototype Confidence of Each Sample). The prototype confidence of sample zi is defined as the ratio of the From an orthogonal perspective, the models capabilities vary during training. Initially, models are weak and tend to produce biased pseudo-labels, so more soft pseudo-labels are suggested. As training progresses, the model gradually learns to distinguish between different categories. As consequence, we would place greater trust in its predictions and reduce the threshold δ in Eq. (9). However, directly determining the threshold is non-trivial. Here, we propose more reasonable approach, in which we set the proportion of unlabeled samples to which we assign hard labels. At epoch e, we present one-hot pseudo-labels to the top r% unlabeled samples with the highest confidence, while soft labels for the left. And δ could be implicitly expressed by -th highest confidence of all unlabeled the r/100 IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE samples. For the proportion of samples assigned with hard pseudo-labels, we adopt linear ramp-up function: r(e) = eramp 100%, 100%, if 0 eramp, if > eramp, (10) where r(e) [0%, 100%] is function of training epochs. In practice, there is no need to explicitly compute δ. At epoch e, we could select the top r(e)% of samples with the highest confidence and assign one-hot labels, while softly sharpened % of samples. As in labels for the remaining Eq. (10), the ratio of hard pseudo-labels r(e) grows linearly from the beginning to the eramp-th epoch, then all are hard pseudo-labels in later epochs. r(e) 100 (cid:0) (cid:1) 3.3.3 Cross-view Prediction with Pseudo-Labels We perform pseudo-labeling on all the training data, and propose to learn with cross-view prediction [24] as follows: Ldapl = 1 2 (cid:88) ℓ(q i, pi) + ℓ(qi, i) (cid:17) , (11) iB (cid:16) where ℓ(q, p) = q(k) log p(k) denotes cross-entropy and we simplify q(zi) and p(zi, τbase) as qi and pi respectively. The superscript indicates the k-th entry. In Eq. (11), two views provide pseudo-labels for each other, like swapped prediction [24], which implicitly implements consistency regularization [51]. (cid:80) 3.3.4 Theoretical Analysis We provide the theoretical analysis of the DAPL mechanism. GCD could be viewed as open-world semi-supervised learning (SSL) [22], where unlabeled data contains new classes, so it also conforms to the basic assumptions of SSL. Assumption 1 (Cluster Assumption [52]). Samples in the same cluster (high-density region) are expected to have the same label. Proposition 1 (Entropy Minimization [53] in SSL). Under Assumption 1, entropy minimization on unlabeled data helps ensure that classes are well-separated. Entropy minimization [53] could help push unlabeled data to high-density areas away from boundaries, which decreases class overlap and improves inter-class separation. Proposition 2 (Pseudo-labeling in SSL). Pseudo-labeling [46] implicitly performs entropy minimization on unlabeled data. Generally, learning with pseudo-labels ˆy on unlabeled data xu could be expressed as minimizing the cross-entropy i.e., between pseudo-labels and the model predictions, (f (xu), ˆy). Regardless of whether the pseudo-labels are hard [46] or soft [54], they are invariably more confident with lower entropy than the models predictions, consequently, pseudo-labeling encourages the model to predict more confidently and minimize the entropy on unlabeled data. Let R(f ) = E(x,y)L R(f ) = (f (xi), yi) and (cid:98) (f (x), y) denote the true risk of the classification model . The empirical risk could be Rl(f ) + Rl(f ) = decomposed as Ru(f ) = 1 1 (f (xj), ˆyi) are (cid:98) (cid:98) empirical risk on labeled and unlabeled data. The error of hard pseudo-labels ˆyj = argmaxc (xj)[c] with threshold τpl Ru(f ), where j=1 i=1 (cid:80) (cid:80) (cid:98) (cid:98) could be written as errpl = 1 Then we have the theorem [55] below: j= 1[f (xj )[ˆyj ]τpl] (cid:80) 6 1[ˆyj =yj ]. Theorem 1 (Performance Gap of Pseudo-labeling Meth- ) is Lℓ-Lipschitz conods [55]). Suppose the loss function ℓ( ϵ, and for tinuous and bounded by B. For some ϵ > 0, if errpl any δ > 0, with probability at least 1 δ, we have: R( ˆf ) R(f ) 2KBϵ + 4KLℓRN (F) + 2KB (cid:114) log(2/δ) 2N , (12) R ( ) is the expected Rademacher complexity [56] and where = + denotes the total number of training samples, = argminf R(f ) and ˆf = argminf R(f ) denote the R(f ), respectively. minimizer of true risk R(f ) and empirical risk From Theorem 1, the performance of ˆf depends on the error of pseudo-labels and the number of training samples. Lower errpl leads to better generalization performance. (cid:98) (cid:98) To build strong classifier, we have to balance between Proposition 1 and Theorem 1. On the one hand, we are supposed to encourage the model to output confident predictions. On the other hand, we should still avoid overconfidence in pseudo-labels, which could bring about severe errors and confirmation bias [21], and it is more obvious in GCD owing to its stricter labeling conditions. In ProtoGCD, we propose DAPL to balance them. Specifically, we progressively provide the model with more confident pseudo-labels as the models performance improves. To realize this objective, We achieve adaptivity on two levels: (1) We assign hard labels for more confident samples while soft labels for others. (2) The ratio of samples for hard labels increases gradually. In this way, DAPL helps achieve efficient training while circumventing bias. Assumption 2 (Consistency Regularization [51]). The models predictions remain consistent over some slight perturbations. ProtoGCD adopts cross-view prediction (Eq. (11)), which ensures consistency across various augmentations and enhances the models robustness and generalization ability. 3.4 Regularization 3.4.1 Avoiding Trivial Solutions GCD is essentially transfer clustering task [23], which is susceptible to trivial solutions [24], [57] where most of the samples in are allocated to one or small number of clusters. Early works employ equipartition constraints [24], which do not always hold for long-tailed data, and others resort to heuristics [57]. In ProtoGCD, we adopt marginal entropy maximization [58] as follows: Lentropy = H(p) = k=1 (cid:88) p(k) log p(k), (13) iB p(zi, τbase) + p(z (cid:16) p(k) log p(k) denotes entropy, and where H(p) = = 1 i, τbase) denotes marginal 2B probability distribution over two views. Lentropy encourages (cid:17) to predict across different categories as evenly as possible as whole. We also provide an orthogonal perspective of Eq. (13) in Theorem 2. The proof is in the Appendix. (cid:80) (cid:80) IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 7 Theorem 2. Marginal entropy maximization to incorporating prior distribution Lentropy is equivalent across categories, where is uniform distribution. U Advantages of Entropy Regularization. The advantages of Lentropy are two-fold. Firstly, it is flexible soft regularization term. One could choose the proper weight and even specific prior distribution according to the characteristics of the downstream datasets, instead of imposing equipartition constraints [24] in all cases. Secondly, Lentropy is differentiable and could be learned end-to-end, which is effective without any alternative optimization [24]. 3.4.2 Inter-Class Separation Learning with pseudo-labels (Section 3.3) improves intraclass compactness. It is also important to promote inter-class separation for better classification. To this end, we explicitly , i.e., decrease the increase the distances among prototypes similarities between each pair of prototypes, and obtain the inter-class separation regularization term as below: Lsep ="
        },
        {
            "title": "1\nK",
            "content": "K i=1 (cid:88) log 1 j=1,j=i (cid:88) exp(µ µj/τsep), (14) τsep is the temperature. The overall regularization is: Lreg = λentropyLentropy + λsepLsep, (15) where λentropy and λsep are weights of two terms. 3.5 Overall Learning Objective For labeled data ground-truth labels on both of the views: l, ProtoGCD directly learns from the Lsup = 1 2 iBl (cid:16) (cid:88) ℓ(yl i, pi) + ℓ(yl i, i) (cid:17) . (16) u, is: λsup) The integrated classification loss, i.e., learning with groundtruth labels on and pseudo-labels on Lcls = (1 Ldapl + λsupLsup, λsup) denote the weights of supervised where λsup and (1 and unsupervised components, which is the same as Eq. (6). By integrating the learning objectives in Section 3.2 Lreg in Section 3.4, i.e., Eq. (15), we could obtain the overall learning objective: Lcls in Eq. (17) and Lcon in Eq. (6), (17) = (18) Lreg. Lcon + Lcls + End-to-end Training. Each term in Eq. (18) is differential. ) and projec- , feature extractor The learnable prototypes tion head ϕ( ) could be updated collectively in an end-to-end manner. Consequently, ProtoGCD is an efficient framework without any alternating optimization or EM-like operations like [14], [15]. It also flexibly mitigates confirmation bias and learns appropriate and unbiased representations for GCD. ( E"
        },
        {
            "title": "4 ESTIMATING THE NUMBER OF CATEGORIES",
            "content": "D = In the literature of GCD, most methods assume the number of new categories Knew is known a-prior, which is unrealistic. It is important to estimate Knew given the whole training data u. Vaze et al. [12] propose to run K-means [18] with various Knew, and choose the one corresponding on to the maximum clustering accuracy on the labeled data as an estimation of Knew, namely Max-Acc. However, only considering accuracy neglects latent information in feature space and leads to degraded results. In this paper, we propose to simultaneously exploit the accuracy of labeled Knew and data and feature information of all data. Let Knew denote the estimated and ground truth number of new Knew. We train ProtoGCD models with classes. various numbers of classes, i.e., total number of prototypes = Kold + (cid:101) (cid:101)K = (cid:101) µc { } (cid:101)K c=1, and devise the following two proxies. (cid:101) Accuracy Score. ProtoGCD adopts the parametric classifier, so we could directly compute old classes accuracy on without clustering and Hungarian algorithm [59] as below: accScore = 1 1 iDl (cid:88) yi=argmaxc p(y=czi,τ ) , (19) (cid:2) entropy in Eq. (13) encourages uniform predictions, if are assigned Knew > Knew, some samples from outside of old prototypes, leading to lower accScore. (cid:101) Centroid Score. The centroids, i.e., mean features, of could be computed in the following two ways: old in old (cid:3) ck = ck = 1 zi, zi, l (cid:88)iDk 1 u (cid:88)iDk (xi, yi) = 1, 2, = 1, 2, , Kold, (20) , Kold, (21) = { l, yi = } = where denotes the labeled samples assigned to the prototypes of old classes based u, yi = on ground-truth labels yi, and denotes the unlabeled samples assigned to the pro- } totypes of old classes based on the models predictions zi, τ ). Similarly, due to the effect yi = argmaxc p(y = new in of are assigned to old classes, in this case, the divergence (cid:101) between ck in old classes becomes larger, resulting in lower centrScore: Knew < Knew, more samples from and the corresponding ck entropy, if (xi) { D (cid:101) centrScore = ck ck u, Kold k=1 (cid:89) (22) Prototype Score as Combination of Two Scores. As mentioned Knew > Knew, accScore becomes lower, above, when Knew < Knew, centrScore becomes lower, which when motivates us to integrate them and propose Prototype Score by incorporating both accuracy and centroids divergence: (cid:101) (cid:101) protoScore( Knew) = accScore centrScore. (23) In both cases, protoScore is small. We choose the that maximizes protoScore as an estimator of Knew: (cid:101)"
        },
        {
            "title": "Knew",
            "content": "K new = argmax protoScore( Knew). (cid:101) (24) (cid:101)Knew (cid:101) (cid:101) IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 8 Algorithm 1 Prototype Score for Class Number Estimation TABLE 1: The statistics of three generic datasets and three Input: Training dataset = Dl Du. Input: Number of old classes Kold. Input: Maximum range of new classes number max new. 1: Initialize the left and right boundary Ka = 0, Kb = max new. 2: while Ka < Kb do Kc1 1 3: Train ProtoGCD with (Kold + Kc1 ) and (Kold + Kc2 ) 4: prototypes on for 3 epochs and compute protoScore pc1 and pc2 , respectively, as described in Eq. (23). 2 (Ka + Kb), Kc2 1 2 (Ka + Kb) + 1. else pa pc2 . if pc1 < pc2 then Ka Kc2 , 5: 6: 7: 8: end if 9: 10: end while Output: Estimated number of new class (cid:101)K Kb Kc1 , pb pc1 . new = Ka. fine-grained datasets . The number of instances of both ), as well labeled and unlabeled data is shown ( , = Kold + Knew). as the number of classes ( = Kold, Y Y Labeled Dl Unlabeled Du Datasets CIFAR10 [61] CIFAR100 [61] ImageNet-100 [62] CUB [63] Stanford Cars (SCars) [64] FGVC-Aircraft (Aircraft) [65] Herbarium19 (Herb) [66] Dl Yl Du Yu 12,500 20,000 31,860 1,498 2,000 1,666 8, 5 80 50 100 98 50 341 37,500 30,000 95,255 4,496 6,144 5,001 25,356 10 100 100 200 196 100 { (cid:101) µc (cid:101)K ="
        },
        {
            "title": "K and set the prototypes",
            "content": "Overall Pipeline. We train ProtoGCD with various class (cid:101)K numbers c=1. Models } are trained using the overall objectives in Section 3.5 for only 3 epochs, then we compute protoScore and estimate the novel classes number as in Eq. (24). This low-epochtraining avoids low distinguishable accScore due to the and ensures fast estimation. We employ overfitting to to further accelerate the binary search to iterate over algorithm. The whole process is shown in Algorithm 1. Our algorithm requires approximately O(log max new) epochs. After the acquisition of new, we could use the estimated number to instantiate prototypes and train models for GCD with the proposed method in Section 3. (cid:101) (cid:101) C old and"
        },
        {
            "title": "5 EXTENDING TO DETECT UNSEEN OUTLIERS\nOnce trained on partially labeled old classes\nbeled new classes",
            "content": "old and unlanew, the model can classify samples from new during testing. However, in practical scenarios, test samples outside of new could emerge after the models deployment, we refer to them as outliers or unseen novel categories, and denote them as out, see Fig. 3 (c). Since the model has not seen out during training, it is essential to detect them during inference, rather than irresponsibly classifying them into one of the categories in new, which is important in safety-critical circumstances [60] and often overlooked in GCD [12]. old old C C old old and cluster new, but also to reject new as in-distribution (ID) and In this paper, we extend ProtoGCD to not only classify out. Herein, we out as outrefer to of-distribution (OOD). In other words, we extend ProtoGCD to the task of OOD detection [33]. Following the common practice, we assign each sample confidence score S(x), indicating its normality. Given pre-defined threshold δood, if S(x) δood, then is recognized as ID, otherwise, is detected as OOD and rejected. Because ProtoGCD adopts the parametric classifier, it could easily obtain the predictive probability, as in Eq. (2). We propose to employ the posthoc score functions for OOD detection, like MSP [33] and Energy [38], these methods are independent of ProtoGCDs training, thus could be directly integrated into our method for OOD detection, e.g., S(x) = maxk p(y = z, τ ) for MSP. By contrast, methods [12], [13], [15] using contrastive learning could not directly obtain posterior probabilities. We propose to firstly run K-means [18] on training data and obtain the cluster centroids for ID classes, which are then used to compute probabilities similar to Eq. (2)."
        },
        {
            "title": "6 EXPERIMENTS\n6.1 Experimental Setup",
            "content": "C Datasets. we conduct experiments on generic recognition datasets: CIFAR10 [61], CIFAR100 [61] and ImageNet100 [62], as well as more challenging fine-grained datasets in Semantic Shift Benchmark [67]: CUB [63], Stanford Cars (SCars) [64], FGVC-Aircraft (Aircraft) [65] and Herbarium19 (Herb) [66]. Following the canonical setting in the literature of GCD [12], [13], [15], in each dataset, we sample subset of all classes as old classes old, the remaining classes are novel classes new. Half of the instances in old classes from the original training data are drawn to form labeled data l, while all the remaining data from the original training u. We summarize the set constitute the unlabeled dataset datasets statistics in Table 1. The original test data in each dataset serves as the validation set for model selection. GCD follows the transductive setting [12], i.e., the model is trained on u. Evaluation Protocol. GCD is essentially clustering problem, we evaluate the performance following [12]. At test time, we measure the clustering accuracy (ACC) of the models predictions yi given the ground-truth labels yi: and evaluated on D ACC = max ωΩ(Yu)"
        },
        {
            "title": "1\nM",
            "content": "M 1 yi = ω(yi) , (25) i=1 (cid:88) (cid:8) (cid:9) where = denotes the total number of unlabeled samples, and Ω( u) represents the set of all permutations that match the prediction to the ground-truth labels. We find the optimal permutation by the Hungarian algorithm [59], which is performed only once across both new on all the unlabeled data [12]. The ACC in Eq. (25) reflects the overall clustering performance on the entire unlabeled u, namely All, we further report the clustering dataset accuracy for samples from the old classes old subset and u, namely Old and New the new classes respectively. The Old and New results are evaluated after the Hungarian assignment is computed. new subset in old and C C Implementation Details. For fair comparisons, we follow prior arts [12], [13], [15] and train our method with ViTB/16 backbone [68] pre-trained with DINO [49], and the IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 9 TABLE 2: Main results on generic image classification datasets, where denotes the reproduced results."
        },
        {
            "title": "Methods",
            "content": "K-means [18] RankStats+ [2] UNO+ [20] ORCA [22] GCD [12] XCon [13] DCCL [15] GPC [14] SimGCD [32]"
        },
        {
            "title": "All",
            "content": "83.6 46.8 68.6 81.8 91.5 96.0 96.3 92.2 97.1 CIFAR10 CIFAR100 ImageNet-"
        },
        {
            "title": "Old",
            "content": "85.7 19.2 98.3 86.2 97.9 97.3 96.5 98.2 95."
        },
        {
            "title": "New",
            "content": "82.5 60.5 53.8 79.6 88.2 95.4 96.9 89.1 98."
        },
        {
            "title": "All",
            "content": "52.0 58.2 69.5 69.0 73.0 74.2 75.3 77.9 80."
        },
        {
            "title": "Old",
            "content": "52.2 77.6 80.6 77.4 76.2 81.2 76.8 85.0 81."
        },
        {
            "title": "New",
            "content": "50.8 19.3 47.2 52.0 66.5 60.3 70.2 63.0 77."
        },
        {
            "title": "All",
            "content": "72.7 37.1 70.3 73.5 74.1 77.6 80.5 76.9 83."
        },
        {
            "title": "Old",
            "content": "75.5 61.6 95.0 92.6 89.8 93.5 90.5 94.3 93."
        },
        {
            "title": "New",
            "content": "71.3 24.8 57.9 63.9 66.3 69.7 76.2 71.0 77.9 ProtoGCD (ours) 97.30.0 95.30.2 98.20.1 81.90.2 82.90.0 80.00.4 84.00.6 92.20.9 79.91.3 TABLE 3: Main results on fine-grained image classification datasets, where denotes the reproduced results. Methods K-means [18] RankStats+ [2] UNO+ [20] ORCA [22] GCD [12] XCon [13] DCCL [15] GPC [14] SimGCD [32] All 34.3 33.3 35.1 35.3 51.3 52.1 63.5 55.4 60.3 CUB Stanford Cars FGVC-Aircraft Herbarium19 Old 38.9 51.6 49.0 45.6 56.6 54.3 60.8 58.2 65.6 New 32.1 24.2 28.1 30.2 48.7 51.0 64.9 53.1 57.7 All 12.8 28.3 35.5 31.9 39.0 40.5 43.1 42.8 53.8 Old 10.6 61.8 70.5 42.2 57.6 58.8 55.7 59.2 71.9 New 13.8 12.1 18.6 26.9 29.9 31.7 36.2 32.8 45.0 All 16.0 26.9 40.3 31.6 45.0 47.7 46.3 54.2 Old 14.4 36.4 56.4 32.0 41.1 44.4 42.5 59.1 New All 16.8 22.2 32.2 31.4 46.9 49.4 47.9 51. 13.0 27.4 28.3 24.6 35.4 38.1 44.0 Old 12.2 55.8 53.7 26.5 51.0 58.3 58.0 New 13.4 12.8 14.7 23.7 27.0 27.3 36.4 ProtoGCD (ours) 63.20.1 68.50.5 60.50.2 53.80.4 73.70.6 44.20.6 56.80.4 62.50.8 53.90.9 44.50.3 59.40.5 36.50. final transformer block is fine-tuned. We use the output [CLS] token as feature representation zi. All the methods are trained for 200 epochs with batch size of 128, and models are selected on the validation set for evaluation. The feature and projection space dimensions are 768 and 65,536, as in [12]. The initial learning rate is 0.1 and decayed with cosine annealed schedule. As for the hyper-parameters, the weight of the supervised component λsup is 0.35. λentropy and λsep is set to be 2 and 0.1 respectively. τbase = τsep = 0.1, and τsharp = 0.05. The ramp-up stage contains eramp = 100 epochs with linear schedule as in Eq. (10). All experiments are conducted on NVIDIA RTX A6000 GPUs. 6.2 Generalized Category Discovery Performance 6.2.1 Comparison with State-of-the-Arts We compare our method with naive K-means [18], strong baselines [2], [20] derived from NCD and competitive GCD methods [12], [13], [22] DCCL [15], GPC [14] and state-ofthe-art (SOTA) SimGCD [32] and µGCD [69]. We report the std), results of our method averaged over 5 runs (mean while for other methods, official results from original papers are reported. The experimental results on generic and finegrained image datasets are shown in Table 2 and Table 3, respectively. ProtoGCD outperforms previous SOTA methods by large margin. ProtoGCD consistency achieves remarkable performance. For example, on CIFAR100, ProtoGCD achieves 1.8% gains on All classes and 2.2% on New classes, as in Table 2. For fine-grained datasets in Table 3, our method outperforms DCCL [15] by 7.0% on SCars. The results indicate that ProtoGCD learns better representations from the pseudolabeling mechanism and parametric prototypes. ProtoGCD provides more balanced accuracy between old and novel classes. The significant issue addressed by ProtoGCD is the imbalanced performance between old and new classes, especially for parametric classifier-based methods [2], [20]. On ImageNet-100, although UNO+ [20] achieves the best Old accuracy, it suffers from severely imbalanced performance (37.1% gap between Old and New). By contrast, our method achieves more balanced results (12.3%). similar trend could be observed in other datasets. These results show that ProtoGCD benefits from its unified modeling and learning objectives between old and new classes to obtain balanced accuracy. 6.2.2 Inductive Evaluation Canonical GCD follows transductive evaluation [12], [13], [15], i.e., models are tested on the unlabeled part of training data. In this paper, we generalize to the inductive evaluation, where we evaluate the trained models on separate and unseen test datasets. The results are shown in Table 4. Compared with the transductive results, contrastive learningbased methods GCD [12] and XCon [13] have degraded performance. The reason is that these methods use semisupervised K-means for transductive evaluation, however, there are no labeled data at hand for inductive settings, and unsupervised K-means results in unstable clusters. Our method utilizes parametric classifier and does not rely at inference time. Consequently, ProtoGCD achieves on better generalization performance under inductive settings IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE TABLE 4: Inductive evaluation on four datasets. Values in () indicate the performance gap compared with transductive evaluations, i.e., generalization errors. 10 TABLE 6: Estimating the number of total classes in the unlabeled data on generic and fine-grained datasets. Here, GT denotes the ground truth. Datasets GT GCD [12] XCon [13] DCCL [15] Ours CIFAR10 10 CIFAR100 100 100 INCUB SCars Herb 200 196 683 9 100 109 231 230 520 8 97 109 236 206 - 14 146 129 172 192 - 10 100 106 211 205 603 Datasets GCD XCon Ours CIFAR100 75.4 (0.8 ) 81.1 (0.1 ) 82.5 (0.4 ) Old New 60.0 (6.5 ) 51.5 (8.8 ) 78.0 (2.0 ) ImageNet-100 87.3 (2.5 ) 91.4 (2.1 ) 92.8 (0.3 ) Old New 65.4 (0.9 ) 64.4 (5.3 ) 78.4 (1.5 ) SCars Aircraft 52.0 (5.6 ) 53.8 (5.0 ) 68.9 (0.3 ) Old New 26.6 (3.3 ) 27.4 (4.3 ) 41.2 (0.0 ) 40.1 (1.1 ) 43.8 (0.6 ) 62.1 (0.4 ) Old New 41.1 (5.8 ) 43.2 (6.2 ) 53.7 (0.2 ) TABLE 5: Comparison results using DINO and DINOv2 initialized backbone. Bold and underline denote the best and the second best values. Method CUB Stanford Cars FGVC Aircraft All Old New All Old New All Old New DINO (a) CIFAR10. (b) ImageNet-100. SimGCD [32] µGCD [69] 60.3 65.6 57.7 53.8 71.9 45.0 54.2 59.1 51.8 65.7 68.0 64.6 56.5 68.1 50.9 53.8 55.4 53.0 63.2 68.5 60.5 53.8 73.7 44.2 56.8 62.5 53.9 ProtoGCD (ours) ProtoGCD+ (ours) 66.3 68.9 65.0 58.8 75.1 51.2 59.5 62.0 58.3 Fig. 5: Results on different scores for class number estimation on CIFAR10 (a) and ImageNet-100 (b), and the ground-truth classes numbers are 10 and 100, respectively. DINOv2 SimGCD [32] µGCD [69] 71.5 78.1 68.3 71.5 81.9 66.6 63.9 69.9 60.9 74.0 75.9 73.1 76.1 91.1 68.9 66.3 68.7 65.1 ProtoGCD (ours) 74.9 80.1 72.3 75.8 88.7 69.5 69.4 75.9 66.2 ProtoGCD+ (ours) 75.7 81.5 72.9 77.6 90.5 71.5 71.1 76.3 68.5 as in Table 4. For instance, the performance degradation of our method on Aircraft is 0.2%, less than 6.2% of XCon [13]. 6.2.3 Evaluation under Other Training Configurations To comprehensively evaluate our method, we conduct experiments under different training configurations. From the model perspective, we consider more recent DINOv2 [70] for enhanced initializations. From the training techniques perspective, recent work µGCD [69] builds upon SimGCD and further utilizes FixMatch [71]-like techniques, including the exponential moving average of the teacher model and misaligned data augmentations for teacher and student models. µGCD also employs the model trained in [12] for initialization. These techniques are complementary to ProtoGCD. Thus, we seamlessly incorporate the three techniques into ProtoGCD and name the upgraded method as ProtoGCD+. Results under these training configurations are shown in Table 5. Our method outperforms SimGCD for both DINO and DINOv2, and the upgraded version ProtoGCD+ achieves the SOTA performance. 6.2.4 Finding the Number of Classes For GCD [12], [13], most methods assume the number of new classes is known. To relax this restriction, we present Prototype Score for class number estimation in Algorithm 1. We compare our method with GCD [12] Xcon [13] and DCCL [15] in Table 6. Prototype Score consistently achieves more precise estimation results. The reason is that we further consider information in the feature space beyond accuracy and grasp more latent characteristics. TABLE 7: Class number estimation results across different training epochs. Here, GT denotes the ground truth. (cid:101) Datasets GT C100 IN-100 CUB Herb 100 100 200 683 # Training Epochs 1 89 90 180 2 98 97 205 595 3 100 106 211 603 4 101 109 218 5 109 113 221 670 6 120 119 229 701 7 117 119 235 To demonstrate the validity of our method, we illustrate the trend of changes in two scores of Prototype Score. grows, Fig. 5 demonstrates that as the estimated number centrScore increases while accScore decreases. This is consistent with the analysis in Section 4. As result, we as the estimation when the combination value of select centrScore and accScore is the largest. (cid:101) (cid:101) Training epochs for class number estimation. Algorithm 1 requires repeatedly training the model for several epochs. We conduct experiments of class number estimation with different epochs in Table 7. If the number of training epochs is insufficient, the model is very weak on labeled classes, resulting in unreliable accScore. Conversely, if the number of epochs is large, the model tends to overfit the labeled data, resulting in indistinguishable accScore. Then, centrScore assumes greater significance. As result, the , as in Table 7. By default, method tends to predict larger we choose to train 3 epochs for all datasets. 6.3 Ablation Studies (cid:101) Ablations on the main components. Here we validate the effectiveness of main training objectives, including contrastive learning Ldapl (Section 3.3), entropy regularization Lentropy (Section 3.4.1) Lsep (Section 3.4.2). In Table 8, and separation regularization (a) is the baseline where only supervised classification Lsup Lcon (Section 3.2), DAPL mechanism IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 11 TABLE 8: Main ablation studies on the learning objectives. ID (a) (b) (c) (d) (e) (f) (g)"
        },
        {
            "title": "ProtoSep\nLsep",
            "content": "CIFAR"
        },
        {
            "title": "All Old New All Old New",
            "content": "61.2 64.0 65.8 30.1 66.4 79.7 81.9 79.4 73.4 71.9 44.0 71.8 79.6 82.9 24.6 45.3 53.7 2.2 55.7 79.9 80.0 30.7 33.6 36.1 20.8 38.0 54.4 56.8 39.9 33.1 36.2 42.5 38.4 56.6 62.5 26.2 33.9 36.0 10.0 37.8 53.3 53. (a) CIFAR100. (b) Aircraft. Fig. 6: Detailed ablations on DAPL. Fig. 7: Detailed ablations of eramp on CIFAR100 and Aircraft. in Eq. (16) is employed. (b) shows slight improvement over (a), which implies that contrastive learning ensures fundamental representations. Comparing (b) and (c), DAPL improves overall performance, especially for New accuracy, highlighting the effectiveness of self-training with pseudolabeling. Comparing (b) and (d), introducing Lentropy alone leads to collapsed performance. The reason is that blindly avoiding trivial solutions without the guidance of DAPL for pseudo-labeling brings about meaningless outcomes. In contrast, as in (f), the concurrent presence of DAPL and entropy regularization ensure significant performance gains, for instance, (f) outperforms (b) by 23.5% and 19.4% on Old and New classes of Aircraft, which highlights the importance of both DAPL mechanism and avoidance of trivial solutions in GCD. In (e), removing Lentropy severely degrades the performance compared with (g) due to the trivial solutions. Besides, explicitly separating clusters via Lsep further enhances the performance, with 2.2% and 2.4% improvements on two datasets. Detailed ablations on DAPL. We conduct ablations on our pseudo-labeling mechanism, including sharpening in soft pseudo-labels (PL), the combination of soft and hard PL and the adaptive ramp-up ratio of hard PL. In Fig. 6, the overall trends are similar across (a) and (b). Sharpening helps models produce more confident outputs, and the absence of sharpening impedes self-training, leading to significant 20%. Models are susceptible to performance decline, i.e., confirmation bias without soft PL, while without hard PL, the training is hindered due to less informative PL. Overall, soft PL has more significant impact on the results. We also remove the adaptive ratio and fix the ratio of hard to soft PL at 1 : 1, and the overall accuracy is roughly 2% lower than the full DAPL, which underscores the importance of adaptivity according to the models capabilities. Detailed ablations on eramp. In the proposed DAPL, the proportion of samples assigned with hard pseudo-labels increases linearly from 0 to 100% during the first eramp epochs, as in Eq. (10). Here, we conduct detailed ablation on the rampup epochs eramp across 0, 25, 50, 75, 100, 125, 150, 175, 200 on Fig. 8: All accuracy across various class splits. Fig. 9: Detailed ablations of λentropy on Herb19. CIFAR100 and Aircraft. Results are shown in Fig. 7. The optimal value of eramp is around 125 for both datasets, and we could observe that the accuracy remains stable and high when eramp ranges within [75, 150], showcasing the robustness of our method. Evaluation with various old/new class splits. We further evaluate different methods across various class splits on CIFAR100 where Knew ranges from 1 to 99. Fig. 8 illustrates All accuracy and indicates that our method is more robust when very few classes are labeled, and consistently outperforms the competitors across various class splits. In-depth analysis of Lentropy on Herb dataset. Although the Lentropy has implicitly imposed the entropy regularization assumption of uniform distribution on the dataset, which IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 12 TABLE 9: OOD detection performance on CIFAR100. TABLE 11: OOD detection performance with other scores. Dtest out FPR95 AUROC GCD XCon ProtoGCD GCD XCon ProtoGCD OOD Scores CIFAR100 ImageNetFPR95 AUROC FPR95 AUROC Texture SVHN Places365 29.92 47.80 49.36 TinyImageNet 59.08 71.16 69.15 71.97 LSUN iSUN CIFAR10 42.81 51.54 69.20 60.88 63.40 65.0 68.53 Mean 56.92 60.20 31.31 50.65 56.17 58.93 60.89 64.03 63.53 55.07 92.99 90.54 86.68 84.62 83.42 82.87 77.59 90.74 90.89 81.31 84.00 84.68 83.97 78. 85.53 84.82 93.90 91.21 84.20 85.94 87.07 84.51 80.18 86.72 MSP [33] MLS [39] Energy [38] 55.07 54.90 54. 86.72 86.85 88.57 45.21 44.40 42.74 88.95 89.24 90.07 TABLE 12: Performance degradation under different levels of corruption severity on CIFAR100-C of our method. Level All Old New TABLE 10: OOD detection performance on ImageNet-100. Dtest out FPR95 AUROC GCD XCon ProtoGCD GCD XCon ProtoGCD 46.62 Texture 66.37 Places365 70.30 iNaturalist ImageNet-O 63.47 OpenImage-O 64.34 39.79 67.82 69.87 61.70 60.84 Mean 62.22 60. 21.75 56.47 52.29 48.91 46.64 45.21 91.60 87.00 86.28 85.75 86.56 93.70 86.88 86.29 87.23 88.43 87.44 88. 94.60 85.09 87.72 87.89 89.45 88.95 36.2%). In summary, Lentropy (44.5% might conflict with the long-tailed Herb, we conduct detailed ablations and argue that Lentropy is still relatively applicable regularization in GCD. As Fig. 9 shows, the results indicate huge degradation in the absence of marginal entropy maximization 29.4%). Even when imposing small weight, e.g., 0.1, there is notable enhancement (29.4% Lentropy is indispensable. The reason Lentropy is soft regularization rather than the is that (1) rigid constraints like the equipartition in [20], we could choose appropriate λentropy to balance between avoiding trivial solutions and preventing conflicts with the actual Lentropy directly acts on the models dataset distribution. (2) predicted marginal probabilities, which may not strictly align with the ratio of samples from new and old classes predicted by the model. The latter corresponds to the actual distribution of the dataset. More details are shown in the Appendix. 6.4 OOD Detection Performance In this section, we extend ProtoGCD to OOD detection scenarios, as described in Section 5, and compare its rejection ability of unseen classes with GCD methods [12], [13]. Experimental Setup. For CIFAR100 as ID dataset, test OOD datasets are Texture [72], SVHN [73], Places365 [74], TinyImageNet, LSUN [75], iSUN [76] and CIFAR10. For ImageNet-100 as ID dataset, test OOD datasets are Texture [72], Places365 [74], iNaturalist [77], ImageNet-O [78], OpenImage-O [79]. Following the convention [33], [39], we use AUROC and FPR95 to measure OOD detection. We treat out) new) as positives, and OOD classes ( ID classes ( as negatives. More details are shown in the Appendix. old Comparative Results. As discussed in Section 5, ProtoGCD could obtain posterior probabilities with the learned prototypes (Eq. (2)), for non-parametric methods [12], [13], we firstly run K-means on the training set of GCD and employ the cluster centroids of new to get predictive probabilities. For fair comparisons, we use MSP [33] as the score function, and conduct OOD detection on CIFAR100  (Table 9)  and ImageNet-100  (Table 10)  . ProtoGCD demonstrates stronger OOD detection capability, e.g., on CIFAR100, old 0 1 2 3 4 5 81. 80.0 82.9 72.4 ( 9.5 ) 73.4 ( 9.5 ) 68.2 (11.8 ) 62.4 (17.6 ) 66.9 (16.0 ) 66.0 (15.9 ) 57.1 (22.9 ) 60.7 (22.2 ) 60.0 (21.9 ) 51.7 (28.3 ) 54.3 (28.6 ) 53.8 (28.1 ) 41.8 (38.2 ) 43.8 (39.1 ) 43.4 (38.5 ) it achieves 1.85% lower FPR95 and 1.19% higher AUROC compared to GCD [12]. OOD Detection with Other Score Functions. We also conduct OOD detection of ProtoGCD under different OOD scores, including max logit score (MLS) [39] and Energy [38]. MLS explores logits in the feature space, namely the similarity to prototypes, maxc µ zi, while Energy aligns better with the density of data [38]. Consequently, MLS and Energy outperform the MSP baseline, as validated in Table 11. 6.5 Further Analysis 6.5.1 Category Discovery under Covariate-Shifts Existing GCD works predominantly assume the data distribution is invariant. However, samples inevitably undergo covariate-shifts [80] in the ever-changing environments. The model is still required to robustly discover distributionshifted novel categories. In this paper, we evaluate the performance of GCD [12], XCon [13] and our ProtoGCD under distribution shifts. Experimental Setup. We directly use models trained in standard GCD settings, i.e., CIFAR10/100, which are then evaluated on the corrupted datasets, i.e., CIFAR10/100-C [81]. It is worth noting that in corrupted test data, there are only covariate-shifts without semantic-shifts. The evaluation dataset contains 15 types of synthetic corruptions, with 5 levels of severity for each, resulting in 75 distinct corruptions. The corruptions include noise, weather changes and digital operations (see the horizontal axis of Fig. 10). Experimental Results. Comparative results at severity 1 of three methods are shown in Fig. 10. ProtoGCD consistently outperforms GCD and XCon, for instance, regarding snow and jpeg of CIFAR100-C, our method achieves 7.19% and 7.60% higher All accuracy over XCon. Besides, we implement our methods on 5 levels of severity. As Table 12 reveals, at lower levels of severity, performance degradation for new classes is more significant than for old classes, but at higher levels of severity, the decrease is similar for both. 6.5.2 Cluster Characteristics To further quantitatively evaluate the learned feature representations of GCD, we present the following two metrics of IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 13 (a) CIFAR-10-C. (b) CIFAR-100-C. Fig. 10: All accuracy (%) on distribution shift scenarios. CIFAR-10-C and CIFAR-100-C are corruption datasets that contain 15 types of corruption, each with 5 levels of severity. Results here are at severity 1. TABLE 13: Cluster metrics (intra-class compactness inter-class separation ) on CIFAR100 (a) and CUB (b). and Methods Compactness All Old New GCD 0.66 0.67 0.63 XCon 0.71 0.72 0.67 DCCL 0.75 0.76 0.70 GPC 0.70 0.71 0.66 0.80 0.79 0.81 Ours Separation Methods Compactness All Old New Separation 0.20 0.13 0.11 0.15 0.01 GCD 0.76 0.78 0.75 XCon 0.77 0.78 0.77 DCCL 0.78 0.79 0.78 GPC 0.75 0.76 0.73 0.79 0.80 0.79 Ours 0.17 0.17 0.13 0.15 0.12 (a) CIFAR100. (b) CUB. Fig. 11: Cluster metrics w/ and w/o prototype separation (left) and interloss class separation Lsep, including intra-class compactness (right). intra-class compactness and inter-class separation: compactness"
        },
        {
            "title": "1\nK",
            "content": "= separation ="
        },
        {
            "title": "1\nK",
            "content": "K k=1 (cid:88) 1 test (cid:88)iDk 1 test 1 µ zi, (26) µ µj, (27) } test (cid:80) iDk test test = j=1,j=i (cid:88) Dtest, yi = i=1 (cid:88) where = Kold + Knew denotes total number of classes, is the i-th classes of the test (xi, yi) { dataset, µk = 1 zi is the ℓ2-normalized mean Dk feature of class k. Due to the cosine similarity in Eq. (26) and Eq. (27), greater intra-class compactness and inter-class separation lead to higher compactness and lower separation. We compute compactness and separation in the test dataset, as shown in Table 13. Regarding the two metrics, ProtoGCD outperforms its competitors on both CIFAR100 and CUB. Take CUB as an instance, ProtoGCD improves compactness from 0.77 of XCon [13] to 0.79, and decreases separation from 0.17 to 0.12. The results demonstrate that ProtoGCD learns better representations with greater intraclass compactness and inter-class separation. We further validate the effectiveness of prototype separation loss. As Fig. 11 shows, while it also implicitly increases intra-class compactness. Lsep explicitly enhances inter-class separation, 6.6 Qualitative Visualization and Analysis In this section, we provide visualizations of the feature space (Section 6.6.1) and the attention map (Section 6.6.2) to qualitatively verify the effectiveness and superiority of our method. Our method could also retrieve samples with prototypes (Section 6.6.3). 6.6.1 Visualizations of the Feature Space We first show feature space visualizations of three methods: pre-trained DINO [49], the classical approach GCD [12] and our ProtoGCD using t-SNE [82]. Visualizations on CIFAR10 [61] are illustrated in Fig. 12. ProtoGCD improves intra-class compactness and effectively helps mitigate confirmation bias. The DAPL module in Eq. (9) with the parametric prototypical classifier gradually assigns high-quality pseudo-labels, encouraging samples to move toward their associated prototypes, leading to compact clusters. By contrast, GCD [12] with non-parametric classifier resorts to pure contrastive learning, which performs instance discrimination [17] and treats any two samples as negative pairs, even if they belong to the same class. As result, it suffers from the class collision issue, resulting in dispersed and sparse clusters. For example, in Fig. 12, the clusters of ship and horse in GCD are dispersed, while in our methods are more compact. Overall, the class-wise prototypes help place each cluster in reasonable locations, and ProtoGCD benefits from the synergy of the proposed pseudo-labeling mechanism and learning objectives. ProtoGCD further improves inter-class separation. The protoLsep explicitly pushes the prototypes far type separation loss away from each other, which improves inter-class separation. As Fig. 12 shows, deer and horse in GCD [12] tend to overlap and become intertwined, posing challenges to distinguishing among them, while our method achieves clear cluster boundaries and separated clusters. 6.6.2 Visualizations of the Attention Map We visualize the attention mechanism of the ViT backbone pre-trained with DINO, fine-tuned with Xcon [13] and our ProtoGCD in Fig. 13. Specifically, self-attention maps of [CLS] token over three heads in the last layer are displayed. We conduct experiments on Stanford Cars [64] and CUB [63]. The regions with the top attention values are highlighted in red, and deeper red indicates higher attention values. IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 14 Fig. 12: Visualizations of the feature space on CIFAR10. Features of old classes are depicted in cool colors (e.g., while novel categories in warm colors (e.g., provides improved inter-class separation and intra-class compactness. ) ). Additionally, the learnable prototypes are denoted as . Our method , , , , , , Fig. 13: Visualizations attention maps. For Stanford Cars (top), Tesla (old) and Audi (new) are shown. For CUB (bottom), Yellow_Breasted_Chat (old) and Pacific_Loon (new) are displayed. Please zoom in for more details. ProtoGCD produces attended regions with greater concentration and alleviates the spurious correlation regions. Overall, in Fig. 13, the attention maps of the pre-trained DINO are relatively sparse and dispersed. For instance, attended regions of Pacific_Loon distribute across different locations. Even worse, DINO attends to spurious correlation background areas, like surroundings near the car (head 1 and 3 of Tesla), tree branches (head 1 of Yellow_Breasted_Chat) and water (three heads of Pacific_Loon). By contrast, ProtoGCD could greatly mitigate the spurious correlation and focus on core regions to discern classes in fine-grained manner, like cars logo (head 1 of Tesla and Audi) and birds eyes (head 1 of Yellow_Breasted_Chat) and beaks (head 2 of Pacific_Loon). Additionally, the attention areas of each head in ProtoGCD are more concentrated and precise. ProtoGCD effectively transfers the classification capabilities from old classes to novel categories. In GCD, models are expected to learn the classification criterion, i.e., what constitutes class and how to discern different classes, on labeled classes, and transfer the knowledge to novel categories. The results in Fig. 13 effectively substantiate this point. Specifically, car logos are one of the most salient areas for car classification. Models learn to attend to the car logo on Tesla (head 1 of ProtoGCD) from old classes, and manage to attend to car Fig. 14: Sample retrieval on CUB. The three most typical and least typical are shown for each class. logos of the new class Audi (head 1). One could also observe similar phenomenon in birds eyes in Fig. 13. 6.6.3 Sample Retrieval via Typicality Intuitively, the learnable prototypes of ProtoGCD capture the stereotype or template of each category, allowing us to explore an additional functionality: sample retrieval via typically. We define typicality as follows: typicality(zi) = µ yi zi, (28) IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 15 is the learned prototype of (28), we extract the yi-th where µyi class. Based on Eq. typical and least typical samples of Black_capped_Vireo, White_crowned_Sparrow and Slaty_backed_Gull, as depicted in Fig. 14. In the first row, three typical images (left) contain distinctive features, e.g., head and eyes. In contrast, indistinctive images (right) display that the vireos body is partially obscured by human hands. the most"
        },
        {
            "title": "7 CONCLUSION AND FUTURE WORKS",
            "content": "In this paper, we propose novel framework called ProtoGCD to provide unified modeling and learn suitable representations for the task of generalized category discovery (GCD). ProtoGCD is characterized by its unification and unbiased features, as shown in Fig. 2. The unification is manifested on two levels: (1) Unified modeling of old and new classes (Fig. 2 (a)). ProtoGCD employs joint prototypes and unified learning objectives for both old and new classes. (2) Task-level Unification (Fig. 2 (b)). ProtoGCD could classify old classes, cluster new classes and detect unseen outliers, making it unified classifier in the open-world. Regarding the unbiased properties, there are also two dimensions: (1) ProtoGCD adopts parametric classifier and DAPL, which aligns closely with the clustering objectives of GCD, together with two regularizations collectively learn suitable and less biased representations for GCD (Fig. 2 (c)). (2) Our method flexibly assigns pseudo-labels to reduce the confirmation bias of incorrect pseudo-labels (Fig. 2 (d)). In general, these two characteristics allow ProtoGCD to achieve balanced and remarkable performance for both old and new classes. Besides, this paper introduces novel method for estimating the number of new classes, considering both features and accuracy, enabling ProtoGCD to handle more realistic settings when the number of novel categories is unknown. Furthermore, we extend ProtoGCD to detect unseen categories, and achieve task-level unification. To validate the effectiveness of ProtoGCD, we conduct comprehensive experiments, including experiments on generic and finegrained datasets, ablations and extended OOD detection. We also thoroughly analyze the advantages of ProtoGCD in broad scenarios, e.g., visualization of feature spaces and attention mechanisms, and corruption-shift cases. We further highlight the capability for typical sample retrieval. ProtoGCD is an initial exploration oriented to handling scenarios involving various types of semantic-shift categories [3], [67], including unlabeled novel categories and unseen outliers. We hope this work can inspire further research on versatile open-world classifiers and tackle more challenging settings, including filtering out outliers [83] in training data, continual category discovery [84], [85] requiring incrementally identifying novel categories while overcoming catastrophic forgetting. In both scenarios, OOD detection contributes to the discovery of new classes. Besides, future works could also design more suitable methods for long-tailed distributions in GCD and calibrate the confidence for both old and new classes. Beyond classification tasks, category and knowledge discovery can also be further applied to semantic segmentation [86], [87] and multimodal learning [88], [89], [90], [91]."
        },
        {
            "title": "REFERENCES",
            "content": "[1] C. Troisemaine, V. Lemaire, S. Gosselin, A. Reiffers-Masson, J. Flocon-Cholet, and S. Vaton, Novel class discovery: an introduction and key concepts, arXiv preprint arXiv:2302.12028, 2023. [2] K. Han, S.-A. Rebuffi, S. Ehrhardt, A. Vedaldi, and A. Zisserman, Autonovel: Automatically discovering and learning novel visual categories, IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 44, no. 10, pp. 67676781, 2022. [3] F. Zhu, S. Ma, Z. Cheng, X.-Y. Zhang, Z. Zhang, and C.-L. Liu, Open-world machine learning: review and new outlooks, arXiv preprint arXiv:2403.01759, 2024. [4] C. Geng, S.-J. Huang, and S. Chen, Recent advances in open set recognition: survey, IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 43, no. 10, pp. 36143631, 2021. [5] M. Salehi, H. Mirzaei, D. Hendrycks, Y. Li, M. H. Rohban, and M. Sabokrou, unified survey on anomaly, novelty, open-set, and out of-distribution detection: Solutions and future challenges, Transactions on Machine Learning Research, 2022. [6] P. Zhao, J.-W. Shan, Y.-J. Zhang, and Z.-H. Zhou, Exploratory machine learning with unknown unknowns, Artificial Intelligence, vol. 327, p. 104059, 2024. [7] K. Han, S.-A. Rebuffi, S. Ehrhardt, A. Vedaldi, and A. Zisserman, Automatically discovering and learning new visual categories with ranking statistics, in International Conference on Learning Representations, 2020. [8] Z. Zhong, L. Zhu, Z. Luo, S. Li, Y. Yang, and N. Sebe, Openmix: Reviving known knowledge for discovering novel visual categories in an open world, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021, pp. 94629470. [9] Z. Zhong, E. Fini, S. Roy, Z. Luo, E. Ricci, and N. Sebe, Neighborhood contrastive learning for novel class discovery, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2021, pp. 10 86710 875. [10] B. Zhao and K. Han, Novel visual category discovery with dual ranking statistics and mutual knowledge distillation, in Conference on Neural Information Processing Systems (NeurIPS), 2021. [11] W. Li, Z. Fan, J. Huo, and Y. Gao, Modeling inter-class and intraclass constraints in novel class discovery, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2023, pp. 34493458. [12] S. Vaze, K. Han, A. Vedaldi, and A. Zisserman, Generalized category discovery, in IEEE Conference on Computer Vision and Pattern Recognition, 2022. [13] Y. Fei, Z. Zhao, S. Yang, and B. Zhao, Xcon: Learning with experts for fine-grained category discovery, in British Machine Vision Conference (BMVC), 2022. [14] B. Zhao, X. Wen, and K. Han, Learning semi-supervised gaussian mixture models for generalized category discovery, in Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), October 2023, pp. 16 62316 633. [15] N. Pu, Z. Zhong, and N. Sebe, Dynamic conceptional contrastive learning for generalized category discovery, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023, pp. 75797588. [16] P. Khosla, P. Teterwak, C. Wang, A. Sarna, Y. Tian, P. Isola, A. Maschinot, C. Liu, and D. Krishnan, Supervised contrastive learning, in Advances in Neural Information Processing Systems, H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, and H. Lin, Eds., vol. 33. Curran Associates, Inc., 2020, pp. 18 66118 673. [17] T. Chen, S. Kornblith, M. Norouzi, and G. Hinton, simple framework for contrastive learning of visual representations, in Proceedings of the 37th International Conference on Machine Learning, ser. Proceedings of Machine Learning Research, H. D. III and A. Singh, Eds., vol. 119. PMLR, 1318 Jul 2020, pp. 15971607. [18] D. Arthur and S. Vassilvitskii, K-means++ the advantages of careful seeding, in Proceedings of the eighteenth annual ACM-SIAM symposium on Discrete algorithms, 2007, pp. 10271035. [19] M. Zheng, F. Wang, S. You, C. Qian, C. Zhang, X. Wang, and C. Xu, Weakly supervised contrastive learning, in Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), October 2021, pp. 10 04210 051. [20] E. Fini, E. Sangineto, S. Lathuilière, Z. Zhong, M. Nabi, and E. Ricci, unified objective for novel class discovery, in Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), October 2021, pp. 92849292. IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 16 [21] E. Arazo, D. Ortego, P. Albert, N. E. OConnor, and K. McGuinness, Pseudo-labeling and confirmation bias in deep semi-supervised learning, in 2020 International Joint Conference on Neural Networks (IJCNN). IEEE, 2020. [22] K. Cao, M. Brbic, and J. Leskovec, Open-world semi-supervised learning, in International Conference on Learning Representations, 2022. [23] K. Han, A. Vedaldi, and A. Zisserman, Learning to discover novel visual categories via deep transfer clustering, in Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), October 2019. [24] M. Caron, I. Misra, J. Mairal, P. Goyal, P. Bojanowski, and A. Joulin, Unsupervised learning of visual features by contrasting cluster assignments, Advances in neural information processing systems, vol. 33, pp. 99129924, 2020. [25] J. Li, P. Zhou, C. Xiong, and S. Hoi, Prototypical contrastive learning of unsupervised representations, in International Conference on Learning Representations, 2021. [26] F. Yang, N. Pu, W. Li, Z. Luo, S. Li, N. Sebe, and Z. Zhong, Learning to distinguish samples for generalized category discovery, in European Conference on Computer Vision. Springer, 2024, pp. 105 122. [27] H. Zheng, N. Pu, W. Li, N. Sebe, and Z. Zhong, Textual knowledge matters: Cross-modality co-teaching for generalized visual class discovery, in European Conference on Computer Vision. Springer, 2024, pp. 4158. [28] S. Ma, F. Zhu, Z. Zhong, X.-Y. Zhang, and C.-L. Liu, Active generalized category discovery, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024, pp. 16 89016 900. [29] N. Pu, W. Li, X. Ji, Y. Qin, N. Sebe, and Z. Zhong, Federated generalized category discovery, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024, pp. 28 74128 750. [30] H. Zheng, N. Pu, W. Li, N. Sebe, and Z. Zhong, Prototypical hash encoding for on-the-fly fine-grained category discovery, in The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. [31] Y. Liu, Y. Cai, Q. Jia, B. Qiu, W. Wang, and N. Pu, Novel class discovery for ultra-fine-grained visual categorization, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024, pp. 17 67917 688. [32] X. Wen, B. Zhao, and X. Qi, Parametric classification for generalized category discovery: baseline study, in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2023, pp. 16 59016 600. [33] D. Hendrycks and K. Gimpel, baseline for detecting misclassified and out-of-distribution examples in neural networks, in International Conference on Learning Representations, 2016. [34] S. Ma, F. Zhu, Z. Cheng, and X.-Y. Zhang, Towards trustworthy dataset distillation, Pattern Recognition, vol. 157, p. 110875, 2025. [35] H.-M. Yang, X.-Y. Zhang, F. Yin, Q. Yang, and C.-L. Liu, Convolutional prototype network for open set recognition, IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 44, no. 5, pp. 23582370, 2020. [36] G. Chen, P. Peng, X. Wang, and Y. Tian, Adversarial reciprocal points learning for open set recognition, IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 44, no. 11, pp. 8065 8081, 2021. [37] H. Huang, Y. Wang, Q. Hu, and M.-M. Cheng, Class-specific semantic reconstruction for open set recognition, IEEE transactions on pattern analysis and machine intelligence, vol. 45, no. 4, pp. 4214 4228, 2022. [38] W. Liu, X. Wang, J. Owens, and Y. Li, Energy-based out-ofdistribution detection, Advances in neural information processing systems, vol. 33, pp. 21 46421 475, 2020. [39] D. Hendrycks, S. Basart, M. Mazeika, A. Zou, J. Kwon, M. Mostajabi, J. Steinhardt, and D. Song, Scaling out-of-distribution detection for real-world settings, in International Conference on Machine Learning. PMLR, 2022, pp. 87598773. [40] D. Hendrycks, M. Mazeika, S. Kadavath, and D. Song, Using self-supervised learning can improve model robustness and uncertainty, Advances in neural information processing systems, vol. 32, 2019. [41] J. Tack, S. Mo, J. Jeong, and J. Shin, Csi: Novelty detection via contrastive learning on distributionally shifted instances, Advances in neural information processing systems, vol. 33, pp. 11 83911 852, 2020. [42] H. Wei, R. Xie, H. Cheng, L. Feng, B. An, and Y. Li, Mitigating neural network overconfidence with logit normalization, in International Conference on Machine Learning. PMLR, 2022, pp. 23 63123 644. [43] D. Hendrycks, M. Mazeika, and T. Dietterich, Deep anomaly detection with outlier exposure, in International Conference on Learning Representations, 2018. [44] T. Wang and P. Isola, Understanding contrastive representation learning through alignment and uniformity on the hypersphere, in International Conference on Machine Learning. PMLR, 2020, pp. 99299939. [45] K. V. Mardia, P. E. Jupp, and K. Mardia, Directional statistics. Wiley Online Library, vol. 2. [46] D.-H. Lee et al., Pseudo-label: The simple and efficient semisupervised learning method for deep neural networks, in Workshop on challenges in representation learning, ICML, vol. 3, no. 2. Atlanta, 2013, p. 896. [47] J. Xie, R. Girshick, and A. Farhadi, Unsupervised deep embedding for clustering analysis, in International conference on machine learning. PMLR, 2016, pp. 478487. [48] K. Nigam and R. Ghani, Analyzing the effectiveness and applicability of co-training, in Proceedings of the ninth international conference on Information and knowledge management, 2000, pp. 8693. [49] M. Caron, H. Touvron, I. Misra, H. Jégou, J. Mairal, P. Bojanowski, and A. Joulin, Emerging properties in self-supervised vision transformers, in Proceedings of the IEEE/CVF international conference on computer vision, 2021, pp. 96509660. [50] C. Guo, G. Pleiss, Y. Sun, and K. Q. Weinberger, On calibration of modern neural networks, in International conference on machine learning. PMLR, 2017, pp. 13211330. [51] P. Bachman, O. Alsharif, and D. Precup, Learning with pseudoensembles, Advances in neural information processing systems, vol. 27, 2014. [52] O. Chapelle, B. Scholkopf, and A. Zien, Semi-supervised learning (chapelle, o. et al., eds.; 2006)[book reviews], IEEE Transactions on Neural Networks, vol. 20, no. 3, pp. 542542, 2009. [53] Y. Grandvalet and Y. Bengio, Semi-supervised learning by entropy minimization, Advances in neural information processing systems, vol. 17, 2004. [54] D. Berthelot, N. Carlini, I. Goodfellow, N. Papernot, A. Oliver, and C. A. Raffel, Mixmatch: holistic approach to semi-supervised learning, Advances in neural information processing systems, vol. 32, 2019. [55] M.-K. Xie, J.-H. Xiao, H.-Z. Liu, G. Niu, M. Sugiyama, and S.- J. Huang, Class-distribution-aware pseudo-labeling for semisupervised multi-label learning, in Thirty-seventh Conference on Neural Information Processing Systems, 2023. [56] M. Mohri, A. Rostamizadeh, and A. Talwalkar, Foundations of machine learning. MIT press, 2018. [57] M. Caron, P. Bojanowski, A. Joulin, and M. Douze, Deep clustering for unsupervised learning of visual features, in Proceedings of the European conference on computer vision (ECCV), 2018, pp. 132149. [58] W. Hu, T. Miyato, S. Tokui, E. Matsumoto, and M. Sugiyama, Learning discrete representations via information maximizing selfaugmented training, in International conference on machine learning. PMLR, 2017, pp. 15581567. [59] H. W. Kuhn, The hungarian method for the assignment problem, Naval research logistics quarterly, vol. 2, no. 1-2, pp. 8397, 1955. [60] K. Chitta, A. Prakash, B. Jaeger, Z. Yu, K. Renz, and A. Geiger, Transfuser: Imitation with transformer-based sensor fusion for autonomous driving, IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 45, no. 11, pp. 12 87812 895, 2023. [61] A. Krizhevsky, G. Hinton et al., Learning multiple layers of features from tiny images, 2009. [62] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, Imagenet: large-scale hierarchical image database, in 2009 IEEE conference on computer vision and pattern recognition. Ieee, 2009, pp. 248255. [63] C. Wah, S. Branson, P. Welinder, P. Perona, and S. Belongie, The caltech-ucsd birds-200-2011 dataset, 2011. [64] J. Krause, M. Stark, J. Deng, and L. Fei-Fei, 3d object representations for fine-grained categorization, in Proceedings of the IEEE international conference on computer vision workshops, 2013, pp. 554 561. IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 17 [86] X.-J. Wu, R. Zhang, J. Qin, S. Ma, and C.-L. Liu, Wps-sam: Towards weakly-supervised part segmentation with foundation models, in European Conference on Computer Vision. Springer, 2024, pp. 314333. [87] W. Liu, F. Zhu, S. Ma, and C.-L. Liu, MSPE: Multi-scale patch embedding prompts vision transformers to any resolution, in The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. [88] Y. Guo, S. Ma, H. Su, Z. Wang, Y. Zhao, W. Zou, S. Sun, and Y. Zheng, Dual mean-teacher: An unbiased semi-supervised framework for audio-visual source localization, Advances in Neural Information Processing Systems, vol. 36, pp. 48 63948 661, 2023. [89] Y. Guo, S. Ma, Y. Zhao, H. Su, and W. Zou, Cross pseudo-labeling for semi-supervised audio-visual source localization, in ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2024, pp. 83568360. [90] Y. Guo, S. Sun, S. Ma, K. Zheng, X. Bao, S. Ma, W. Zou, and Y. Zheng, Crossmae: Cross-modality masked autoencoders for region-aware audio-visual pre-training, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024, pp. 26 72126 731. [91] Y. Guo, S. Ma, S. Ma, X. Bao, C.-W. Xie, K. Zheng, T. Weng, S. Sun, Y. Zheng, and W. Zou, Aligned better, listen better for audio-visual large language models, in The Thirteenth International Conference on Learning Representations, 2025. [92] M. Cuturi, Sinkhorn distances: Lightspeed computation of optimal transport, Advances in neural information processing systems, vol. 26, 2013. [65] S. Maji, E. Rahtu, J. Kannala, M. Blaschko, and A. Vedaldi, Fine-grained visual classification of aircraft, arXiv preprint arXiv:1306.5151, 2013. [66] K. C. Tan, Y. Liu, B. Ambrose, M. Tulig, and S. Belongie, The herbarium challenge 2019 dataset, arXiv preprint arXiv:1906.05372, 2019. [67] S. Vaze, K. Han, A. Vedaldi, and A. Zisserman, Open-set recognition: good closed-set classifier is all you need, in International Conference on Learning Representations, 2022. [68] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit, and N. Houlsby, An image is worth 16x16 words: Transformers for image recognition at scale, in International Conference on Learning Representations, 2021. [69] S. Vaze, A. Vedaldi, and A. Zisserman, No representation rules them all in category discovery, in Advances in Neural Information Processing Systems, vol. 36, 2023, pp. 19 96219 989. [70] M. Oquab, T. Darcet, T. Moutakanni, H. V. Vo, M. Szafraniec, V. Khalidov, P. Fernandez, D. Haziza, F. Massa, A. El-Nouby, R. Howes, P.-Y. Huang, H. Xu, V. Sharma, S.-W. Li, W. Galuba, M. Rabbat, M. Assran, N. Ballas, G. Synnaeve, I. Misra, H. Jegou, J. Mairal, P. Labatut, A. Joulin, and P. Bojanowski, Dinov2: Learning robust visual features without supervision, 2023. [71] K. Sohn, D. Berthelot, N. Carlini, Z. Zhang, H. Zhang, C. A. Raffel, E. D. Cubuk, A. Kurakin, and C.-L. Li, Fixmatch: Simplifying semisupervised learning with consistency and confidence, Advances in neural information processing systems, vol. 33, pp. 596608, 2020. [72] M. Cimpoi, S. Maji, I. Kokkinos, S. Mohamed, and A. Vedaldi, Describing textures in the wild, in Proceedings of the IEEE conference on computer vision and pattern recognition, 2014, pp. 36063613. [73] Y. Netzer, T. Wang, A. Coates, A. Bissacco, B. Wu, and A. Y. Ng, Reading digits in natural images with unsupervised feature learning, in NIPS Workshop on Deep Learning and Unsupervised Feature Learning 2011, 2011. [74] B. Zhou, A. Lapedriza, A. Khosla, A. Oliva, and A. Torralba, Places: 10 million image database for scene recognition, IEEE transactions on pattern analysis and machine intelligence, vol. 40, no. 6, pp. 14521464, 2017. [75] F. Yu, Y. Zhang, S. Song, A. Seff, and J. Xiao, Lsun: Construction of large-scale image dataset using deep learning with humans in the loop, arXiv preprint arXiv:1506.03365, 2015. [76] P. Xu, K. A. Ehinger, Y. Zhang, A. Finkelstein, S. R. Kulkarni, and J. Xiao, Turkergaze: Crowdsourcing saliency with webcam based eye tracking, arXiv preprint arXiv:1504.06755, 2015. [77] G. Van Horn, O. Mac Aodha, Y. Song, Y. Cui, C. Sun, A. Shepard, H. Adam, P. Perona, and S. Belongie, The inaturalist species classification and detection dataset, in Proceedings of the IEEE conference on computer vision and pattern recognition, 2018, pp. 8769 8778. [78] D. Hendrycks, K. Zhao, S. Basart, J. Steinhardt, and D. Song, Natural adversarial examples, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021, pp. 15 26215 271. [79] H. Wang, Z. Li, L. Feng, and W. Zhang, Vim: Out-of-distribution with virtual-logit matching, in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2022, pp. 49214930. [80] J. Li, E. Chen, Z. Ding, L. Zhu, K. Lu, and H. T. Shen, Maximum density divergence for domain adaptation, IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 43, no. 11, pp. 3918 3930, 2021. [81] D. Hendrycks and T. Dietterich, Benchmarking neural network robustness to common corruptions and perturbations, in International Conference on Learning Representations, 2019. [82] L. Van der Maaten and G. Hinton, Visualizing data using t-sne. Journal of machine learning research, vol. 9, no. 11, 2008. [83] X. Zhang, J. Jiang, Y. Feng, Z.-F. Wu, X. Zhao, H. Wan, M. Tang, R. Jin, and Y. Gao, Grow and merge: unified framework for continuous categories discovery, Advances in Neural Information Processing Systems, vol. 35, pp. 27 45527 468, 2022. [84] S. Roy, M. Liu, Z. Zhong, N. Sebe, and E. Ricci, Class-incremental novel class discovery, in European Conference on Computer Vision. Springer, 2022, pp. 317333. [85] S. Ma, F. Zhu, Z. Zhong, W. Liu, X.-Y. Zhang, and C.-L. Liu, Happy: debiased learning framework for continual generalized category discovery, in The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE APPENDIX Overview. This is the appendix for the paper entitled ProtoGCD: Unified and Unbiased Prototype Learning for Generalized Category Discovery. In the material, Section provides the proof of Theorem 2 of the main text. Section presents the experimental details. Section demonstrates the relationship of several related task settings. Section elaborates on the evaluation metrics of GCD and OOD detection. Section gives more detailed experimental results, including OOD detection, sensitivity analysis and visualizations. An in-depth analysis of entropy regularization is included in Section F. Section presents comprehensive comparison between SimGCD and our method. Finally, more discussion about the inter-class separation loss is provided in Section H. PROOF OF THEOREM 2 Theorem 2. Marginal entropy maximization to incorporating prior distribution Lentropy is equivalent across categories, where is uniform distribution. Proof. We firstly draw the KullbackLeibler (KL) divergence between the marginal distribution and as: KL(p K ) = p(k) log p(k) (k) = H(p) + log K. (29) k=1 (cid:88) In Eq. (29), log is constant. Thus, maximizing the entropy is equivalent to minimizing the KL divergence between , i.e., incorporating uniform distribution as prior. and EXPERIMENTAL DETAILS Training Hyper-parameters. For fair comparison, the basic training hyper-parameters follow prior methods [12], [13], [15]. We provide the list of basic training hyper-parameters in Table 14, and the specific hyper-parameters of ProtoGCD are shown in Table 15. Additionally, the temperature in prototype confidence is τsharp. And we set λentropy = 2 for most datasets, while λentropy = 1 for CIFAR10 [61] and Aircraft [65]. Data Augmentations. Following the common practice of GCD [12], [13], [15], we resize input images to 224 224. We adopt conventional random augmentations for two views, including RandomCrop, RandomHorizontalFlip and ColorJitter. RELATIONSHIP WITH RELATED SETTINGS We clarify the relationship between GCD and related fields. (1) Semi-Supervised Learning. GCD extends SSL to the openworld, where unlabeled data contain samples from new classes, while in SSL, labeled and unlabeled data share the same classes. (2) Unsupervised Clustering. GCD could be viewed as deep transfer clustering [23]. The underlying principle is to transfer the knowledge from labeled classes to cluster unlabeled novel categories. In contrast, without any prior knowledge, unsupervised clustering [47], [57] suffers from poor representation and ambiguity in the classification criterion. For example, models tend to face the dilemma of whether to group red flowers and red birds together or red flowers and blue flowers into the same cluster. In GCD, models grasp the prior knowledge and implicit cluster TABLE 14: Basic training hyper-parameters. 18 Hyper-parameters train epochs batch size initial learning rate feature_dim projection_dim dh supervised weight λsup"
        },
        {
            "title": "Value",
            "content": "200 128 0.1 768 65,536 0.35 TABLE 15: Specific hyper-parameters of ProtoGCD. Params λentropy λsep τc τbase τsharp τsep eramp Description weight of entropy regularization weight of prototype separation temperature of contrastive learning temperature of predictions temperature of sharpened soft labels temperature of prototype separation ramp-up epochs Value 1 or 2 0.1 0.07 0.1 0.05 0.1 100 criterion in labeled data, as result, models could obtain desired outcomes. (3) OOD Detection. Both GCD and OOD detection consider open-set samples. OOD detection only needs to detect unseen samples, while GCD further requires the clustering of the new classes. (4) Novel Category Discovery. GCD relaxes the assumption of NCD that unlabeled data exclusively come from novel classes. In GCD, unlabeled data contain samples from both old and novel classes. To conclude, GCD is more challenging and pragmatic task."
        },
        {
            "title": "D EVALUATION METRICS",
            "content": "D.1 Generalized Category Discovery GCD is essentially clustering task, especially for novel classes. As described in the main text, during evaluation, we measure the clustering accuracy (ACC) of the models predictions yi given the ground-truth labels yi: ACC = max pΩ(Yu)"
        },
        {
            "title": "1\nM",
            "content": "M i=1 (cid:88) 1 yi = p(yi) , (30) (cid:8) (cid:9) D where = is the total number of unlabeled samples, and Ω( u) represents the set of all permutations that map the prediction to the ground-truth labels. We provide All, Old and New accuracy for all data, data from groundtruth old classes, and data from ground-truth new classes, respectively. Eq. (30) is achieved by the Hungarian algorithm. Note that we only perform Eq. (30) once on all the test data, and after acquiring Ω( ), we then calculate All, Old and New separately. This is canonical in GCD [12], [13], [15]. D.2 Out-of-Distribution Detection For Out-of-distribution (OOD) detection, we treat indistribution (ID) samples as positives while OOD samples as negatives. In our experiments, the number ratio of ID to OOD samples is set to 1 : 1. FPR95. FPR95 is short for false positive rate at 95% true positive rate. It could be interpreted as the probability that negative sample (OOD) is misperceived as positive (ID) IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE TABLE 16: Performance of our method in the setting of unknown class numbers. Values in () indicate the performance gap compared with known class number scenarios. Datasets CIFAR-100 ImageNet-100 CUB Scars 81.9 (0.0 ) All 82.9 (0.0 ) Old New 80.0 (0.0 ) 84.8 (0.8 ) 90.9 (1.3 ) 81.8 (1.9 ) 61.4 (1.8 ) 52.7 (0.1 ) 66.2 (2.3 ) 71.1 (1.6 ) 58.8 (1.7 ) 43.8 (0.6 ) 19 E.4 More Visualization Results In the main text, we provide the feature visualization of the CIFAR10 dataset. Here, we further visualize the features of the CUB dataset with more classes. ProtoGCD could obtain feature representations with improved intra-class compactness and inter-class separation. In Fig. 16, the clusters of Long_tailed_Jaeger, Tennessee_Warbler and Loggerhead_Shrike in GCD are dispersed, while in our methods are more compact. Besides, classes like White_crowned_Sparrow, Tree_Sparrow and European_Goldfinch in GCD [12] tend to overlap and become intertwined, posing challenges to distinguish among them, while our method achieves clear cluster boundaries and separated clusters. Fig. 15: Performance over various weights of λentropy and λsep. IN-DEPTH DISCUSSION OF ENTROPY REGULARIZATION ON HERB when 95% of ID samples are correctly accepted, i.e., the true positive rate is 95%. AUROC. AUROC is short for Area Under the Receiver Operating Characteristic curve, which depicts the true positive rate (TPR) of ID against the false positive rate of OOD by varying the threshold. AUROC could be interpreted as the probability that we assign higher OOD score to positive sample than to negative sample. AUROC is the thresholdindependent metric. AUPR. AUPR is the Area under the Precision-Recall curve, which shows the precision and recall against each other. AUPR-IN means that we treat ID as the positive. AUPR is also threshold-independent metric. Although entropy regularization Lentropy has implicitly imposed the assumption of uniform distribution on the dataset, which might conflict with the long-tailed distributions for Herb. We have conducted detailed sensitivity analysis of Lentropy on the Herb dataset, as in Fig. 9 of the main text. Overall, despite the Herb dataset being longtailed dataset, the results indicate huge degradation in the Lentropy (44.5% absence of marginal entropy maximization 29.4%, as shown in 0.0 of Fig. 9). Even when imposing small weight, e.g., 0.1, there is notable performance en36.2%). In summary, the most suitable hancement (29.4% weight λentropy is approximately 2.0. From the experimental results, we argue that Lentropy is still relatively applicable regularization in GCD. Some explanations are discussed as follows:"
        },
        {
            "title": "E MORE EXPERIMENTAL RESULTS",
            "content": "E.1 Evaluation of GCD without Prior Class Numbers We also conduct experiments in the scenarios without prior class numbers. Specifically, we train ProtoGCD with the (Table 6 in the main text) by Prototype Score, as estimated shown in Table 16. (cid:101) E.2 Sensitivity of regularization weights To further explore the effects of the two regularization terms, we test sensitivity regarding their weights in Fig. 15. For Lentropy, the optimal value is 2.0. Too large values hamper the learning of DAPL, leading to decreased performance. For Lsep, the optimal value is 0.1. Overall, Lentropy has greater impact than Lsep. E.3 Detailed OOD Experimental Results We provide detailed OOD results in Table 17 and Table 18, including the standard derivation and the AUPR-IN metric. As Table 17 shows, ProtoGCD consistently outperforms other counterparts for OOD detection. Lentropy is soft regularization rather than the hard constraint. It is noteworthy that Lentropy is essentially different from the hard constraint, e.g., UNO [20] that rigidly follows equipartition constraints via the Sinkhorn-Knopp algorithm [92]. The hard constraint could drastically damage the result. For example, UNO has very weak performance on Herb in Table 3 of the main paper. In comparison, by incorporating Lentropy as differential part of the overall learning objective, we could adjust the weight λentropy to balance its influence. If Lentropy is completely discarded via λentropy = 0, the model could be restricted to trivial solutions, leading to significant performance degradation. Conversely, if λentropy is too large, it contradicts the long-tailed distribution of Herb. As result, we could choose proper λentropy to obtain desirable results, for example, For old and new classes, there is gap between the models marginal probabilities and the models p(c) predicted classes ˆy. Formally, let pold = p(c) denote the predicted probaand pnew = bilities for old and new classes, both are scalars and pold + pnew = 1. Then let rold and rnew denote the proportions of samples that the model classified as old and new classes, i.e., rold = 1 i=1 1(ˆyi <= old), rnew = 1 i=1 1(ˆyi > old) and rold + rnew = 2.0 in Fig. 9. cCnew cCold (cid:80) (cid:80) (cid:80) (cid:80) IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 20 TABLE 17: OOD detection performance on different OOD datasets of CIFAR100 (in-distribution). Dtest out FPR95 AUROC AUPR-IN GCD XCon ProtoGCD GCD XCon ProtoGCD GCD XCon ProtoGCD Texture SVHN Places365 29.921.12 42.810.71 31.310.91 92.990.23 90.740.24 93.900.19 98.440.05 97.950.06 98.730.04 47.800.93 51.540.72 50.651.05 90.540.22 90.890.17 91.210.21 98.010.06 98.200.04 98.230.05 49.361.28 69.200.54 56.170.75 86.680.48 81.310.41 84.200.36 96.760.14 95.410.15 96.260.09 TinyImageNet 59.081.26 60.881.30 58.931.00 84.620.42 84.000.35 85.940.33 96.410.14 96.260.08 96.810.08 71.160.92 63.400.81 60.891.21 83.420.20 84.680.35 87.070.21 96.410.04 96.610.10 97.160.06 69.150.69 65.020.99 64.031.10 82.870.27 83.970.32 84.510.45 96.150.07 96.440.09 96.420.12 71.970.73 68.531.18 63.531.12 77.590.32 78.130.45 80.180.33 94.470.09 94.580.13 95.030.10 LSUN iSUN CIFAR Mean 56.92 60.20 55.07 85.53 84. 86.72 96.66 96.49 96.95 TABLE 18: OOD detection performance on different OOD datasets of ImageNet-100 (in-distribution). Dtest out FPR95 AUROC AUPR-IN GCD XCon ProtoGCD GCD XCon ProtoGCD GCD XCon ProtoGCD 46.621.27 39.790.98 21.751.64 91.600.20 93.700.21 94.600.38 98.370.05 98.610.05 98.750.14 Texture 66.371.60 67.821.54 56.471.65 87.000.43 86.880.29 85.090.46 97.500.11 97.480.06 96.670.13 Places365 70.301.45 69.871.57 52.291.28 86.280.49 86.290.31 87.720.57 97.280.11 97.200.06 97.310.16 iNaturalist ImageNet-O 63.470.90 61.700.76 48.910.98 85.750.39 87.230.40 87.890.46 97.020.12 97.100.11 97.270.10 OpenImage-O 64.341.32 60.841.02 46.641.10 86.560.45 88.430.27 89.450.55 97.350.12 97.170.06 97.590.20 Mean 62.22 60.00 45.21 87. 88.51 88.95 97.50 97.51 97.52 Fig. 16: Visualizations of the feature space on CUB. Features of old classes are depicted in cool colors (e.g., novel categories in warm colors (e.g., improved inter-class separation and intra-class compactness. ) while ). Additionally, the learnable prototypes are denoted as . Our method provides , , , , , , zi) denotes the pre1. Here ˆyi = arg maxk p(y = dicted class of the i-th sample. Due to the confidence gap between old and new classes, the model generally exhibits higher confidence in old classes (because old classes are partially labeled while new classes are fully unlabeled). Consequently, there exists disparity between rold and pold, so as to rnew and pnew. The entropy regularization Lentropy is directly applied to pold and pnew, while the actual long-tailed distribution is associated with rold and rnew. To sum up, considering the confidence gap between old and new classes and the weak confidence calibration performance in GCD, employing maximum entropy constraint remains relatively suitable approach. Similar findings have been reported in recent work [85]. We believe that addressing the gap between old and new classes and reducing the disparity between and will be valuable open problem in GCD. DETAILED COMPARISON WITH SIMGCD SimGCD [32] is recent parametric-based GCD method. Here, we provide comprehensive comparison between SimGCD and our ProtoGCD. (a) Differences in the model structure design. About prototypical classifier. Although both SimGCD and ProtoGCD utilize prototypes, there is significant distinction in the meaning of the term prototype. SimGCD refers to its classifier as prototypical classifier merely due to implementing ℓ2 normalization and omitting the bias term upon conventional classifier. So there is no fundamental difference from the traditional classifier. Overall, SimGCD can be regarded as purely discriminative model. By contrast, the prototypes in our ProtoGCD represent the class-wise probability distributions (Eq. (1) in the main text), i.e., von MisesFisher (vMF) distribution [45]. It is form of generative modeling. Then, we derive the posterior predictive probabilities in Eq. (2). The learning mechanism incorporates both discriminative learning with pseudo-labels and generative learning with inter-class separation loss Lsep (in Eq. (14)) and prototype confidence. Overall, ProtoGCD is hybrid model that combines both generative and discriminative modeling. Moreover, contrastive learning [17], [16] is versatile technique that has been widely adopted in the IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 21 TABLE 19: The summarized differences between SimGCD and ProtoGCD from various perspectives. Perspectives Modeling Prototypes Pseudo-Labeling Regularization Extensions SimGCD ProtoGCD Purely Discriminative ℓ2-normed Classifier Self-distillation Hybrid = Generative + Discriminative Class-wise Distribution Dual-level Adaptive Pseudo-Labeling Entropy Maximization Entropy Maximization + Inter-class Separation N/A Class Number Estimation + OOD Detection (c) Other contribution of ProtoGCD. SimGCD solely focuses on the GCD task. In contrast, we provide theoretical analysis for our ProtoGCD, and we further devise method to estimate the number of classes and extend ProtoGCD to detect OOD samples. To conclude, we summarize the differences between these two methods in Table 19. MORE DISCUSSION ABOUT INTER-CLASS SEPARATION REGULARIZATION (m, n) (Eq. (6) in the DCCL Although the dispersion loss paper [15]) and our inter-class separation loss Lsep (Eq. (14) in our paper) share similar goal, our approach is generally more efficient and stable. Specifically, DCCL [15] is nonparametric method following the EM-like framework, where the class-wise conception representations (analogous to prototypes in ProtoGCD) are non-learnable and updated via the exponential moving average (EMA). In each iteration, DCCL requires sampling multiple instances for each conception label, averaging their features, and subsequently computing the dispersion loss. This sampling and averaging process is inefficient, and if the number of samples per class is insufficient, it may lack representativeness, leading to instability. Additionally, DCCL relies on the threshold τ to filter the conception pairs with high uncertainty. Tuning this hyper-parameter might increase the experimental burden. By contrast, our method directly applies separation loss Lsep in Eq. (14) of to learnable prototypes c=1 (see } our paper), which requires no sampling and averaging Lsep enables endprocess and is computationally simple. to-end training, making it highly efficient. Furthermore, the learnable prototypes in our method effectively represent each class, eliminating issues about insufficient representation due to limited samples, thereby ensuring the stability of ProtoGCD. µc { literature of GCD [12], [15], [14], [32], which helps ensure basic feature representations, so we follow their common practice in our method. (b) Differences in the loss function design. About regularization terms. ProtoGCD primarily comprises two regularization terms, i.e., marginal entropy maximization Lentropy and inter-class (protoLsep is our Lsep. Here, type) separation regularization main novelty. Similar to contrastive learning, entropy regularization Lentropy is also commonly employed in the literature of GCD [32], [69], which helps to alleviate trivial solutions in clustering. However, many previous methods, including SimGCD, rely solely on entropy maximization as constraint, and neglect the constraints within the feature space, resulting in less separable clusters. To overcome this issue, we propose to explicitly decrease inter-class overlapping via the separation regularization Lsep. In this way, ProtoGCD could obtain more suitable representations for GCD and remarkable accuracy for both old and new classes. Besides, the prototype separation loss Lsep aligns with our generative modeling, which helps reduce the overlap between distributions of different classes and makes them more separable. About the pseudo-labeling mechanism. The crossview prediction is general framework, while the design of pseudo-labels within this framework is of vital importance. In this regard, our ProtoGCD have significant differences from SimGCD. Specifically, SimGCD simply employs the off-the-shelf selfdistillation borrowed from DINO [49], which fails to consider the specific characteristics of GCD, resulting in suboptimal performance. In this task, there is an inherent imbalance in labeling conditions between old and new classes, leading to an obvious confidence gap among samples. As result, the informativeness for pseudo-labeling varies remarkably among different samples. Besides, at early training stages, the models capabilities are relatively weak and could bring larger noise to pseudo-labels compared with later training stages, so the optimal configuration for pseudo-labels is continuously evolving. These issues motivate us to propose dual-level adaptive pseudo-labeling (DAPL) in ProtoGCD. Our method is specifically designed to consider varying confidence levels among samples and varying model capabilities across learning stages, and could effectively mitigate biases while achieving efficient self-learning."
        }
    ],
    "affiliations": [
        "Centre for Artificial Intelligence and Robotics, Hong Kong Institute of Science and Innovation, Chinese Academy of Sciences, Hong Kong 999077, P.R. China",
        "School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing 100049, China",
        "State Key Laboratory of Multimodal Artificial Intelligence Systems, Institute of Automation, Chinese Academy of Sciences, 95 Zhongguancun East Road, Beijing 100190, P.R. China"
    ]
}