{
    "paper_title": "Internalizing Meta-Experience into Memory for Guided Reinforcement Learning in Large Language Models",
    "authors": [
        "Shiting Huang",
        "Zecheng Li",
        "Yu Zeng",
        "Qingnan Ren",
        "Zhen Fang",
        "Qisheng Su",
        "Kou Shi",
        "Lin Chen",
        "Zehui Chen",
        "Feng Zhao"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as an effective approach for enhancing the reasoning capabilities of Large Language Models (LLMs). Despite its efficacy, RLVR faces a meta-learning bottleneck: it lacks mechanisms for error attribution and experience internalization intrinsic to the human learning cycle beyond practice and verification, thereby limiting fine-grained credit assignment and reusable knowledge formation. We term such reusable knowledge representations derived from past errors as meta-experience. Based on this insight, we propose Meta-Experience Learning (MEL), a novel framework that incorporates self-distilled meta-experience into the model's parametric memory. Building upon standard RLVR, we introduce an additional design that leverages the LLM's self-verification capability to conduct contrastive analysis on paired correct and incorrect trajectories, identify the precise bifurcation points where reasoning errors arise, and summarize them into generalizable meta-experience. The meta-experience is further internalized into the LLM's parametric memory by minimizing the negative log-likelihood, which induces a language-modeled reward signal that bridges correct and incorrect reasoning trajectories and facilitates effective knowledge reuse. Experimental results demonstrate that MEL achieves consistent improvements on benchmarks, yielding 3.92%--4.73% Pass@1 gains across varying model sizes."
        },
        {
            "title": "Start",
            "content": "Internalizing Meta-Experience into Memory for Guided Reinforcement Learning in Large Language Models Shiting Huang1 Zecheng Li1 Yu Zeng1 Qingnan Ren1 Zhen Fang1 Qisheng Su1 Kou Shi1 Lin Chen1 Zehui Chen1 Feng Zhao1(cid:66) 1University of Science and Technology of China (cid:66): Corresponding Author 6 2 0 2 0 1 ] . [ 1 4 2 2 0 1 . 2 0 6 2 : r Abstract Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as an effective approach for enhancing the reasoning capabilities of Large Language Models (LLMs). Despite its efficacy, RLVR faces meta-learning bottleneck: it lacks mechanisms for error attribution and experience internalization intrinsic to the human learning cycle beyond practice and verification, thereby limiting fine-grained credit assignment and reusable knowledge formation. We term such reusable knowledge representations derived from past errors as meta-experience. Based on this insight, we propose Meta-Experience Learning (MEL), novel framework that incorporates selfdistilled meta-experience into the models parametric memory. Building upon standard RLVR, we introduce an additional design that leverages the LLMs self-verification capability to conduct contrastive analysis on paired correct and incorrect trajectories, identify the precise bifurcation points where reasoning errors arise, and summarize them into generalizable meta-experience. The meta-experience is further internalized into the LLMs parametric memory by minimizing the negative log-likelihood, which induces language-modeled reward signal that bridges correct and incorrect reasoning trajectories and facilitates effective knowledge reuse. Experimental results demonstrate that MEL achieves consistent improvements on benchmarks, yielding 3.92% 4.73% Pass@1 gains across varying model sizes. 1. Introduction Reinforcement Learning (RL) has emerged as pivotal paradigm for enhancing the reasoning capabilities of Large Language Models (LLMs) on complex tasks, such as mathematics, programming, and logic reasoning (Shao et al., 2024; Chen et al., 2025; Zeng et al., 2025a; Wang et al., 1 2025; Zeng et al., 2025b; 2026; Huang et al., 2026). By leveraging feedback signals obtained from interaction with the task environment, RL enables LLMs to move beyond passive imitation learning toward goal-directed reasoning and action (Schulman et al., 2017; Ouyang et al., 2022; Wulfmeier et al., 2024). Furthermore, by replacing learned reward models with programmatically verifiable signals, Reinforcement Learning with Verifiable Rewards (RLVR) eliminates the need for expensive human annotations and mitigates reward hacking, thereby enabling models to explore problem-solving strategies more effectively, which has contributed to its growing attention (Lambert et al., 2024). However, RLVR still faces fundamental bottleneck regarding the granularity and utilization of learning signals. From meta-learning perspective, the human learning cycle involves three critical components: practice and verification, error attribution, and experience internalization. While RLVR primarily drives policy updates through practice and verification, it overlooks the critical stages of error attribution and experience internalization, both of which are essential for fine-grained credit assignment and the formation of reusable knowledge (Wu et al., 2025; Zhang et al., 2025a). In other words, RLVR is largely limited to assessing the overall quality of entire trajectories, while struggling to reason about fine-grained knowledge at the level of intermediate steps (Xie et al., 2025). Although RL approaches (Lightman et al., 2023; Khalifa et al., 2025) employing Process Reward Models (PRMs) to provide dense learning signals attempt to mitigate this limitation, their reliance on trained proxies makes them inherently susceptible to reward hacking (Cheng et al., 2025; Guo et al., 2025), and poses fundamental tension with the RLVR paradigm, which is centered on programmatically verifiable rewards. Recently, growing number of studies have explored integrating experience learning within the RLVR framework to address the above challenge. Early attempts, such as StepHint (Zhang et al., 2025b) utilizes experience as hints to elicit superior reasoning paths from the original problems, treating these trajectories as off-policy migration signals. Preprint RLVR, which relies on coarse-grained outcome rewards and treats correct and incorrect trajectories independently, by explicitly connecting them via meta-experiences. Hence, this process can be viewed as language-modeled processlevel reward signal, providing continuous and fine-grained guidance for improving reasoning capability. To further enhance stability and effectiveness during RLVR training, we propose empirical validation via replay, which uses metaexperiences as auxiliary in-context hints to assess their contribution to output accuracy. Meta-experiences that pass validation are integrated via negative log-likelihood minimization, while those that fail validation are excluded. In summary, our main contributions are as follows: We propose MEL, novel framework that integrates self-distilled meta-experience with reinforcement learning, addressing the limitations of standard RLVR in error attribution and experience internalization by embedding these meta-experiences directly into the parametric memory of LLMs. We validate the effectiveness of MEL through extensive experiments on five challenging mathematical reasoning benchmarks across multiple LLM scales (4B, 8B, and 14B). Compared with both the vanilla GRPO baseline and the corresponding base models, MEL consistently improves performance across Pass@1, Avg@8, and Pass@8 metrics. Empirical results confirm that MEL seamlessly integrates with diverse paradigms (e.g., RFT, GRPO, REINFORCE++) to reshape reasoning patterns and elevate performance ceilings. Notably, these benefits exhibit strong scalability, becoming increasingly pronounced as model size expands. 2. Related Work Reinforcement Learning with Verifiable Rewards. Reinforcement Learning with Verifiable Rewards (RLVR) leverages rule-based validators to provide deterministic feedback on models self-generated solutions (Lambert et al., 2024). Extensive research has systematically investigated RLVR, exploring how this paradigm improves the performance of complex reasoning (Jaech et al., 2024; Guo et al., 2025; Liu et al., 2025; Zhang et al., 2025c). The pioneering framework Group Relative Policy Optimization (GRPO) (Shao et al., 2024) estimates advantages via groupwise relative comparisons, eliminating the need for separate value model. Building on this base method, recent studies have introduced range of algorithmic variants to improve training stability and efficiency. For instance, REINFORCE++ (Hu, 2025) enhances stability through global advantage normalization; DAPO (Yu et al., 2025) mitigates Figure 1. Paradigm comparison between standard RLVR and MEL, where MEL extends RLVR with an explicit knowledge-level learning loop. However, the resulting off-policy deviation in response distribution can compromise optimization stability, undermining the theoretical benefits of on-policy reinforcement learning. To alleviate such instability, Scaf-GRPO (Zhang et al., 2025d) leverages superior models to generate multi-level knowledge-intensive experience, injecting them as on-policy prefixes for policy updates. Yet, while effective in teaching models to reason within specific experience-augmented distributions, such prefixes are unavailable during inference, inducing severe distributional mismatch, thereby limiting performance gains. Critically, despite their differences, these approaches utilize retrieved experience primarily as external hints. While these strategies effectively elicit better reasoning paths during training, the resulting learning signals remain predominantly at the trajectory-level, yielding superficial corrections rather than intrinsic cognitive enhancements. Building on this insight, we introduce the concept of metaexperience, elevating experience learning from trajectorylevel instances to knowledge-level representations. Through contrastive analysis on paired correct and incorrect trajectories, we pinpoint the bifurcation points underlying reasoning failures and abstracts them into reusable metaexperiences. Accordingly, we propose Meta-Experience Learning (MEL), framework explicitly designed to enable knowledge-level internalization and reuse of metaexperiences. During training phase, MEL leverages metaexperiences to inject generalizable insights via selfdistillation mechanism, and internalizes them by minimizing the negative log-likelihood in the models parametric memory. As shown in Figure 1, MEL differs from standard 2 Preprint Figure 2. Overview of Meta-Experience Learning (MEL), which constructs meta-experiences from contrastive pairs via abstraction and validation, thereby introducing an explicit knowledge-level learning loop on top of standard RLVR. entropy collapse and improves reward utilization via relaxed clipping and dynamic sampling; and GSPO (Zheng et al., 2025) reduces gradient estimation variance with sequencelevel clipping. Despite these algorithmic advancements, fundamental limitation persists: current RLVR methods predominantly rely on outcome-level rewards. This failure to assign fine-grained credit to specific knowledge points prevents the construction of reusable knowledge formation, fundamentally constraining the development of systematic and generalizable reasoning capabilities. Experience Learning. Recent studies have increasingly recognized that leveraging various forms of experience can substantially enhance LLM reasoning capabilities. One prominent line of research lies in test-time scaling methods, which store experience in external memory pools. For example, SpeedupLLM (Pan & Zhao, 2025) appends memories of previously reasoning traces as experience to accelerate inference, while Training Free GRPO (Cai et al., 2025) and ReasoningBank (Ouyang et al., 2025) distill accumulated experience into structured memory entries for retrieval-based augmentation. However, these approaches rely on evergrowing external memory, preventing the experience from being truly internalized and thus failing to substantively enhance intrinsic reasoning capabilities. Complementarily, another stream of research integrates experience directly into RL training as guiding signals. Methods such as Scaf-GRPO (Zhang et al., 2025d) and StepHint (Zhang et al., 2025b) employ external models to generate experiential hints, which are injected as prefixes or migration signals, to guide the policy toward higher-quality trajectories. Similarly, approaches like LUFFY (Yan et al., 2025) and SRFT (Fu et al., 2025) incorporate expert solution traces as additional experience. Despite improving exploration efficiency, these methods primarily induce trajectory-level imitation. Consequently, models become proficient at following specific patterns but fail to develop the meta-cognitive understanding required for establishing reusable knowledge structures. 3. Meta-Experience Learning Human learning follows recurrent cognitive cycle consisting of practice and verification, error attribution, and experience internalization, which in turn informs subsequent practice. Motivated by this cognitive process, we define meta-experience for LLMs as generalizable and reusable knowledge derived from accumulated reasoning trials, capturing both underlying knowledge concepts and common failure modes. Building on this notion, we propose MetaExperience Learning (MEL), framework operating within the RLVR paradigm and expressly designed to internalize such self-distilled, knowledge-level insights into the models parametric memory. As illustrated in Figure 2, we first formalize the model exploration stage in RLVR (3.1), then present the details of the Meta-Experience construction (3.2). Finally, we describe the internalization mechanism (3.3) for consolidating these insights into parametric memory, followed by the joint training objective for policy optimization (3.4). 3.1. Explorative Rollout and Verifiable Feedback Mirroring the practice and check phase in human learning, the RLVR framework engages the model in exploring po3 Preprint tential solutions for reasoning tasks, while the environment serves as deterministic verifier that provides verifiable feedback on the final answers. As mastering complex logic typically requires traversing the solution space through multiple attempts, we simulate this stochastic exploration by adopting the group rollout formulation from Group Relative Policy Optimization (GRPO) (Shao et al., 2024). For fine-grained comparison within each pair, each trajectory can be formatted as reasoning chain = (s1, s2, . . . , sL, a), where each st denotes an atomic reasoning step and indicates the final answer. Since both trajectories originate from the same context, they typically share correct reasoning path until critical divergence step occurs. Formally, given query sampled from the dataset D, the policy model πθ performs stochastic exploration over the solution space and generates group of independent reasoning trajectories = {y1, y2, . . . , yG}. rule-based verifier then evaluates each trajectory using verification function (), which compares the extracted final answer from yi against the ground-truth answer and assigns binary outcome reward: ri = I(cid:2)V (yi, y)(cid:3) {0, 1}. (1) This process partitions the generated group into two distinct subsets: the set of correct trajectories + = {yi ri = 1} and the set of incorrect trajectories = {yi ri = 0}. The coexistence of + and under the same prompt distribution suggests that the model is capable of solving the task, while producing diverse reasoning trajectories. For our method, such diversity constitutes beneficial property and serves dual role. On the one hand, it supplies the variance necessary for effective policy updates in standard RLVR. On the other hand, it enables the extraction of knowledgelevel meta-expression through systematic contrast between correct and incorrect reasoning outcomes. 3.2. Meta-Experience Construction Prior studies (Xie et al., 2025; Khalifa et al., 2025; Huang et al., 2025) have shown that effective learning does not arise from merely knowing that final answer is incorrect, but rather from identifying the specific bifurcation point at which the reasoning process deviates from the correct trajectory, critical cognitive process known as error attribution. Building on this insight, we leverage pairs of correct and incorrect trajectories to localize reasoning errors and distill such bifurcation points into explicit meta-experiences. Locating the Bifurcation Point. To extract knowledgelevel learning signals from the exploration results, we focus on identifying the bifurcation points where the reasoning logic diverges into an erroneous path. With the exploration results partitioned into + and by the verifier, we construct set of contrastive pairs Px = {(y+, y) y+ +, } for each query x, whose contrast naturally exposes the specific errors in the reasoning process. Such contrastive analysis requires the presence of both positive and negative trajectories; accordingly, we only consider gradient-informative samples with non-empty + and . Given deterministic verification signals and full access to the reasoning chains, identifying the bifurcation point can be viewed as discriminative task that is easier than reasoning from scratch (Saunders et al., 2022; Swamy et al., 2025). Motivated by this observation, we task the policy model with analyzing each contrastive pair to identify the reasoning bifurcation point s: πθ (cid:0) I, x, y+, y(cid:1). (2) Where denotes structured instruction guiding introspective analysis. Deep Diagnosis and Abstraction. Identifying the bifurcation point localizes where the reasoning fails, serving as the raw material for subsequent learning. Anchored at s, the policy model conducts deep diagnostic analysis to contrast the strategic choices underlying the successful and failed trajectories. Specifically, the model examines the local reasoning context around to pinpoint the root cause of failure, such as violated assumptions, erroneous sub-goals, overlooked constraints, or the misuse of specific principles. Complementarily, it inspects the successful trajectory to uncover the mechanisms that prevented such pitfalls, including precise knowledge application, explicit constraint verification, coherent knowledge representations, or emergent self-correction behaviors. By jointly synthesizing these perspectives, the model distills the structural divergence between the correct and incorrect logic, crystallizing it into explicit knowledge. Formally, we model this diagnostic process as generating critique that encapsulates the error attribution, the comparative strategic gap, and the corresponding corrective principle: πθ (cid:0) I, x, y+, y, s(cid:1). (3) To ensure generalization, it is imperative for the model to distill instance-specific critiques into abstract heuristics capable of guiding future reasoning. This abstraction mechanism systematically strips away context-dependent variables, mapping the concrete logic of success and failure onto generalized space of preconditions and responses. Structurally, such heuristics synthesize abstract problem categorization with the corresponding reasoning principles, encompassing the essential knowledge points, theoretical theorems, and decision criteria. Crucially, they also demarcate error-prone boundaries, explicitly highlighting potential pitfalls or latent constraints associated with the specific 4 Preprint problem class. We define the extraction of this heuristic knowledge as generation process conditioned on the full critique context: πθ (cid:0) I, x, y+, y, s, C(cid:1). (4) Finally, we consolidate these components into unified Meta-Experience tuple M, which elevates experience learning from trajectory-level instances to knowledge-level representations. = (cid:0)s, C, H(cid:1). (5) This formulation enables meta-experiences to be reused across tasks that share analogous reasoning structures, serving as fine-grained learning signal. By applying the extraction process across distinct contrastive pairs for query x, we construct candidate pool of meta-experiences DM = {(x, y+ i=1, where denotes the total number of meta-experiences derived from x, and (y+ , ) represents the specific contrastive pair used to derive Mi. , Mi)}N , Empirical Validation via Replay. Closing the cognitive loop requires re-instantiating theoretical insights derived from past failures in future problem-solving to assess their validity. We recognize that the raw meta-experience may still suffer from intrinsic hallucinations or causal misalignment. To mitigate this, we conduct empirical verification by incorporating the extracted tuple as short-term working memory into the prompt, thereby guiding the model to reattempt the original query x. This procedure tests whether the injected meta-experience can effectively steer the model away from the previously identified bifurcation point and toward correct reasoning trajectory. We retain meta-experience only if the corresponding replay trajectory yval πθ( x, M) satisfies the verifier by producing the correct answer. DM = (cid:8)(x, y+, y, M) DM (cid:12) (cid:12) I(cid:2)V (yval, y) = 1(cid:3)(cid:9) . (6) Consequently, this empirical validation preserves only highquality meta-experiences for integration into parametric long-term memory, guaranteeing the reliability of the supervision signals used in the subsequent optimization phase. 3.3. Internalization Mechanism The verified meta-experiences DM constitute highquality reservoir of reasoning guidance. However, treating these insights solely as retrieval-augmented memory imposes substantial computational burden during the inference forward pass, as it necessitates processing elongated contexts for every query. To overcome this limitation, we propose to transfer these insights from the transient context window to the models parametric memory. Unlike the finite context buffer, the model parameters offer virtually unlimited capacity for accumulating diverse meta-experiences, allowing the policy to internalize vast amounts of reasoning patterns without incurring inference-time latency. We establish this internalization process as self-distilled paradigm, where the model learns from its own verified experiences. Specifically, we employ fine-tuning based on the token-averaged negative log-likelihood (NLL) objective to compile the meta-experiences into the policys weights. Formally, given the retrospective context Cretro = [I, x, y+, y], the internalization loss is defined as: LNLL(θ) = (x,y+,y,M)DM (cid:104) 1 M (cid:88) t=1 log πθ(M Cretro, <t) = (cid:34) xD, {yi}G i=1πθold (x) (y+,y,M)T (x,{yi}G i=1) (cid:104) 1 M (cid:88) t=1 log πθ(M Cretro, <t) (cid:105) (cid:35) (cid:105) (7) where () represents the stochastic construction process detailed in 3.2. Based on this formulation, the internalization process can be viewed as specialized sampling form within the RLVR framework. By inverting the loss, we define the MetaExperience Return RMEL as the expected log-likelihood over the stochastically constructed verification set: RMEL = (cid:34) (y+,y,M)T (x,{yi}G i=1) 1 log πθ(M Cretro, <t) (cid:35) . (8) (cid:88) t=1 3.4. Joint Training Objective To simultaneously encourage solution exploration and consolidate the internalized meta-experiences, achieving dual optimization across trajectory-level behaviors and knowledge-level representations, we train the policy model πθ using joint optimization objective. To simultaneously encourage solution exploration and consolidate the internalized meta-experiences, achieving dual optimization across trajectory-level behaviors and knowledge-level representations, we train the policy model πθ using joint optimization objective. This objective synergizes the RLVR signal derived from diverse explorative rollouts with the supervised signal distilled from high-quality meta-experiences: (θ) = JRLVR(θ) + JMEL(θ). (9) We adopt GRPO (Shao et al., 2024) as the RLVR component and compute group-normalized advantages by standardizing 5 Preprint Table 1. Main Results Comparison. Comparison of Pass@1, Avg@8, and Pass@8 accuracy (%) across different model scales. The best results within each model scale are marked in bold. AIME 2024 AIME 2025 AMC"
        },
        {
            "title": "Method",
            "content": "Pass@1 Avg@8 Pass@8 Pass@1 Avg@8 Pass@8 Pass@1 Avg@8 Pass@ Qwen3-4B-Base Baseline GRPO MEL 13.33 13.33 20.00 Qwen3-8B-Base Baseline GRPO MEL 6.67 16.67 30.00 Qwen3-14B-Base 13.33 Baseline 30.00 GRPO 33.33 MEL 9.90 18.33 20. 10.00 24.58 25.42 10.83 35.41 35.83 30.00 30.00 33.00 26.67 43.33 60.00 36.67 56.67 60.00 10.00 6.67 16. 13.33 20.00 23.33 6.66 33.33 36.67 6.56 17.50 18.33 15.00 20.83 23.33 9.58 24.17 30.00 23.33 30.00 33. 33.33 36.67 36.67 33.33 43.33 46.67 45.00 57.50 60.00 65.00 67.50 70.00 60.00 75.00 82.50 42.73 58.13 60. 52.50 69.06 70.31 51.25 75.94 82.81 72.50 85.00 87.50 87.50 87.50 90.00 82.50 95.00 95.00 MATH"
        },
        {
            "title": "Method",
            "content": "Pass@1 Avg@8 Pass@8 Pass@1 Avg@8 Pass@8 Pass@1 Avg@8 Pass@ Qwen3-4B-Base Baseline GRPO MEL 74.20 81.80 82.20 Qwen3-8B-Base Baseline GRPO MEL 77.00 84.40 86.60 Qwen3-14B-Base 80.80 Baseline 85.00 GRPO 90.80 MEL 65.74 82.20 82. 73.40 86.28 86.70 74.15 88.35 90.80 89.60 93.00 93.80 91.40 95.40 96.20 93.60 96.40 97.20 39.17 48.51 48. 44.51 53.56 54.01 45.25 58.16 61.87 35.37 48.46 49.48 39.41 54.60 55.60 40.50 58.46 60.90 60.38 67.21 69. 64.09 73.74 73.00 65.58 74.78 75.82 36.34 41.56 45.48 41.30 48.43 52.79 41.21 56.30 61.03 32.06 44.92 46. 38.06 51.07 52.27 37.26 56.47 60.07 55.16 61.04 63.41 60.60 67.33 71.17 62.34 73.24 74.94 rewards within the sampled group and broadcast them to each token. Let yi,t denote the t-th token in trajectory yi and yi,<t, the corresponding prefix. Substituting the definition of RMEL from Eq. 8, the joint objective in Eq. 9 is explicitly expanded as: (θ) =E xD, {yi}G (x) i=1πθold yi (cid:88) min t= (cid:104) 1 (cid:88) i=1 1 yi (cid:16) ρi,t(θ) ˆAi,t, (10) clip(cid:0)ρi,t(θ), 1 ϵ, 1 + ϵ(cid:1) ˆAi,t (cid:17) + RMEL (cid:105) . Although derived from log-likelihood objective, its optimization gradient is mathematically equivalent to policy gradient update where the reward signal is constant positive scalar. Consequently, the total objective (θ) can be unified as maximizing the expected cumulative return of hybrid reward function. In this unified view, the metaexperiences function as dense process reward model. Unlike the sparse outcome rewards in standard RLVR that only evaluate the final correctness, RMEL provides explicit, step-by-step reinforcement for the reasoning process itself. This ensures that the model not only pursues correct outcomes via broad exploration but is also continuously shaped by the dense supervision of its own successful cognitive patterns, effectively bridging the gap between trajectory-level search and token-level knowledge encoding. 4. Experiments Datasets. We train our model on the DAPO-Math-17k dataset (Yu et al., 2025) and evaluate it on five challenging mathematical reasoning benchmarks: AIME24, AIME25, AMC23 (Li et al., 2024), MATH500 (Hendrycks et al., Preprint 2021), and OlympiadBench (He et al., 2024). Setups. All reinforcement learning training is conducted using the VERL framework (Sheng et al., 2024) on 8H20 GPUs, with Math-Verify providing rule-based outcome verification. During training, we sample 8 responses per prompt at temperature of 1.0 with batch size of 128. Optimization uses learning rate of 1 106 and mini-batch size of 128. For evaluation, we report Pass@1 at temperature 0, and Avg@8 and Pass@8 at temperature 0.6. Models and Baselines. To demonstrate the general applicability of MEL, we conduct experiments across diverse range of model scales, including Qwen3-4B-Base, Qwen38B-Base, and Qwen3-14B-Base (Yang et al., 2025). We adopt GRPO (Shao et al., 2024) as the base reinforcement learning algorithm for MEL, and thus perform direct and controlled comparison between the vanilla GRPO and our meta-experience learning approach. 4.1. Experimental Results As shown in Table 1, MEL achieves consistent and significant improvements over vanilla GRPO and the basemodel across multiple benchmarks and model scales. We report three complementary metrics: Pass@1 reflects one-shot reliability, Avg@8 measures the average performance over 8 samples, and Pass@8 reports the best-of-8 success rate. First, the gains in Pass@1 demonstrate that MEL substantially enhances the models confidence in following correct reasoning paths. Across all model scales, it achieves consistent improvement of 3.924.73% over the strong GRPO baseline. This indicates that MEL effectively internalizes the explored insights into the models parametric memory. By consolidating these successful reasoning patterns, the model generates high-confidence solutions, markedly reducing the need for extensive test-time sampling. This reliability is further corroborated by the gains in Avg@8, which reveal that MEL significantly enhances reasoning consistency and output stability. High performance on this metric supports our hypothesis that internalized meta-experiences function as intrinsic process-level guidance, continuously steering the generation toward valid logic and effectively reducing variance across sampled outputs. Finally, the sustained gains in Pass@8 suggest that learning from metaexperience does not harm exploration; instead, it expands the reachable solution space and raises the upper bound of best-of-k performance. 4.2. Training Dynamics and Convergence Analysis To understand the mechanisms driving the performance gains under MEL, we monitored the training dynamics and validation performance in Figures 3 and 68. 7 Figure 3. Training curves comparing GRPO and MEL. Vanilla GRPO methods often struggle to obtain positive reinforcement in the early stages, particularly when initial performance is low, due to the sparsity of outcome-based rewards. As illustrated in the training curve, vanilla GRPO exhibits relatively slow ascent during the initial phase. In contrast, MEL demonstrates sharp, rapid trajectory growth immediately from the onset of training. This acceleration is attributed to the internalized meta-experience return, RMEL. By functioning as dense, language-modeling process reward, RMEL continuously provides informative gradient signals for every reasoning step, even when successful trajectories yielding positive reinforcement are scarce. Beyond sample efficiency, MEL achieves consistently higher performance upper bound. The training curves show that the average reward of MEL consistently surpasses that of vanilla GRPO throughout the entire training process. Crucially, the downstream validation trajectories reveal that even as performance growth begins to plateau in the later stages, MEL maintains distinct and sustained advantage over the baseline. This phenomenon demonstrates that the internalization of meta-experiences empowers the model to effectively navigate and explore more complex, longhorizon solutions that remain inaccessible to the baseline. 4.3. How Meta-Experience Shapes Reasoning Patterns To investigate how MEL shapes the models cognitive processes beyond numerical metrics, we conduct qualitative analysis comparing the reasoning trajectories of MEL and the baseline GRPO model, as visualized in Figure 4. distinct behavioral divergence is observed from the onset of the solution. While the GRPO baseline tends to prioritize immediate execution through direct numerical operations, MEL adopts structured preparatory strategy by explicitly outlining relevant theorems and formulas. Although the direct approach may appear efficient for simple queries, it increases the susceptibility to errors in complex tasks due to the lack of holistic view of problem constraints. Notably, MEL exhibits an emergent cognitive behavior. When applying specific theorems, it spontaneously activates internalized bitter lessons as endogenous safeguards to regulate its actions. These active signals effectively reduce reasoning drift by encouraging earlier constraint checking Preprint Figure 4. Case study comparing GRPO and MEL, with visualization of meta-experience in early stage. and consistent self-correction when the model enters uncertain regions. 4.4. Generality Across Learning Paradigms Figure 5. Impact of meta-experience across different training methods, including Rejection Sampling Fine-Tuning (RFT) and REINFORCE++. ME denotes Meta-Experience. 4.5. Scalability Analysis As indicated by the training curves in Figure 3, the method exhibits distinct positive scaling law: the performance margin between MEL and the baseline widens significantly as the model size increases. This phenomenon consistently extends to downstream validation benchmarks. We attribute this effect to the quality of self-generated supervision, which is inherently bounded by the models intrinsic capability. As shown in Figure 9, the 14B model achieves significantly higher yield rate of valid meta-experiences than its smaller counterparts. While limited-capacity models introduce noise due to imprecise error attribution, larger models benefit from stronger self-verification, enabling the distillation of high-quality heuristics that provide more accurate gradient signals and fully realize the potential of our framework. To demonstrate the versatility of meta-experience, we integrated it into RFT and REINFORCE++ using the Qwen-8BBase model as the backbone and the same training set in our experiments. As shown in Figure 5, while vanilla RFT often suffers from rote memorization and tends to overfit to specific samples in this training set, the incorporation of meta-experiences introduces robust reasoning heuristics. This allows the model to internalize the underlying logic rather than merely imitating specific answers, thereby effectively mitigating overfitting and enhancing generalization to unseen test sets. Similarly, applying meta-experiences to REINFORCE++ significantly raises the performance ceiling on benchmarks. This confirms that the benefit of internalized meta-experiences is universal enhancement, not limited to the GRPO framework. 5. Conclusion In this paper, we introduced MEL, novel framework designed to overcome the meta-learning bottleneck in standard RLVR by transforming instance-specific failure patterns into reusable cognitive assets. Unlike traditional methods that rely solely on outcome-oriented rewards, MEL empowers models to perform granular error attribution, distilling specific failure modes into natural language heuristicstermed Meta-Experiences. By internalizing these experiences into parametric memory, our approach bridges the gap between verifying solution and understanding the underlying reasoning logic. Extensive empirical evaluations confirm that MEL consistently boosts mathematical reasoning across diverse model scales. 8 Preprint"
        },
        {
            "title": "Impact Statement",
            "content": "This paper presents research aimed at advancing the field of reinforcement learning. While our work may have broader societal implications, we do not identify any specific impacts that require particular attention at this stage."
        },
        {
            "title": "References",
            "content": "Cai, Y., Cai, S., Shi, Y., Xu, Z., Chen, L., Qin, Y., Tan, X., Li, G., Li, Z., Lin, H., et al. Training-free group relative policy optimization. arXiv preprint arXiv:2510.08191, 2025. Chen, J., He, Q., Yuan, S., Chen, A., Cai, Z., Dai, W., Yu, H., Yu, Q., Li, X., Chen, J., et al. Enigmata: Scaling logical reasoning in large language models with synthetic verifiable puzzles. arXiv preprint arXiv:2505.19914, 2025. Cheng, J., Xiong, G., Qiao, R., Li, L., Guo, C., Wang, J., Lv, Y., and Wang, F.-Y. Stop summation: Min-form credit assignment is all process reward model needs for reasoning. arXiv preprint arXiv:2504.15275, 2025. Fu, Y., Chen, T., Chai, J., Wang, X., Tu, S., Yin, G., Lin, W., Zhang, Q., Zhu, Y., and Zhao, D. Srft: single-stage method with supervised and reinforcement fine-tuning for reasoning. arXiv preprint arXiv:2506.19767, 2025. Guo, D., Yang, D., Zhang, H., Song, J., Zhang, R., Xu, R., Zhu, Q., Ma, S., Wang, P., Bi, X., et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. He, C., Luo, R., Bai, Y., Hu, S., Thai, Z., Shen, J., Hu, J., Han, X., Huang, Y., Zhang, Y., et al. Olympiadbench: challenging benchmark for promoting agi with olympiadlevel bilingual multimodal scientific problems. In Proceedings of the Association for Computational Linguistics, pp. 38283850, 2024. Hendrycks, D., Burns, C., Kadavath, S., Arora, A., Basart, S., Tang, E., Song, D., and Steinhardt, J. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv: 2103.03874, 2021. Hu, J. Reinforce++: simple and efficient approach arXiv preprint for aligning large language models. arXiv:2501.03262, 2025. Huang, S., Fang, Z., Chen, Z., Yuan, S., Ye, J., Zeng, Y., Chen, L., Mao, Q., and Zhao, F. Critictool: Evaluating self-critique capabilities of large language models in toolcalling error scenarios. arXiv preprint arXiv:2506.13977, 2025. Huang, W., Zeng, Y., Wang, Q., Fang, Z., Cao, S., Chu, Z., Yin, Q., Chen, S., Yin, Z., Chen, L., et al. Visiondeepresearch: Incentivizing deepresearch capability in arXiv preprint multimodal large language models. arXiv:2601.22060, 2026. Jaech, A., Kalai, A., Lerer, A., Richardson, A., El-Kishky, A., Low, A., Helyar, A., Madry, A., Beutel, A., Carney, A., et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. Khalifa, M., Agarwal, R., Logeswaran, L., Kim, J., Peng, H., Lee, M., Lee, H., and Wang, L. Process reward models that think. arXiv preprint arXiv:2504.16828, 2025. Lambert, N., Morrison, J., Pyatkin, V., Huang, S., Ivison, H., Brahman, F., Miranda, L. J. V., Liu, A., Dziri, N., Lyu, S., et al. Tulu 3: Pushing frontiers in open language model post-training. arXiv preprint arXiv:2411.15124, 2024. Li, J., Beeching, E., Tunstall, L., Lipkin, B., Soletskyi, R., Huang, S., Rasul, K., Yu, L., Jiang, A. Q., Shen, Z., et al. Numinamath: The largest public dataset in ai4maths with 860k pairs of competition math problems and solutions. Hugging Face repository, 13(9):9, 2024. Lightman, H., Kosaraju, V., Burda, Y., Edwards, H., Baker, B., Lee, T., Leike, J., Schulman, J., Sutskever, I., and Cobbe, K. Lets verify step by step. In Proceedings of the International Conference on Learning Representations, 2023. Liu, K., Yang, D., Qian, Z., Yin, W., Wang, Y., Li, H., Liu, J., Zhai, P., Liu, Y., and Zhang, L. Reinforcement learning meets large language models: survey of advancements and applications across the llm lifecycle. arXiv preprint arXiv:2509.16679, 2025. Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35:2773027744, 2022. Ouyang, S., Yan, J., Hsu, I., Chen, Y., Jiang, K., Wang, Z., Han, R., Le, L. T., Daruki, S., Tang, X., et al. Reasoningbank: Scaling agent self-evolving with reasoning memory. arXiv preprint arXiv:2509.25140, 2025. Pan, B. and Zhao, L. Can past experience accelerate llm reasoning? arXiv preprint arXiv:2505.20643, 2025. Saunders, W., Yeh, C., Wu, J., Bills, S., Ouyang, L., Ward, J., and Leike, J. Self-critiquing models for assisting human evaluators. arXiv preprint arXiv:2206.05802, 2022. 9 Preprint Zeng, Y., Qi, Y., Zhao, Y., Bao, X., Chen, L., Chen, Z., Huang, S., Zhao, J., and Zhao, F. Enhancing large visionlanguage models with ultra-detailed image caption generation. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pp. 2670326729, 2025b. Zeng, Y., Huang, W., Fang, Z., Chen, S., Shen, Y., Cai, Y., Wang, X., Yin, Z., Chen, L., Chen, Z., et al. Visiondeepresearch benchmark: Rethinking visual and textual search for multimodal large language models. arXiv preprint arXiv:2602.02185, 2026. Zhang, K., Chen, X., Liu, B., Xue, T., Liao, Z., Liu, Z., Wang, X., Ning, Y., Chen, Z., Fu, X., et al. Agent learning via early experience. arXiv preprint arXiv:2510.08558, 2025a. Zhang, K., Lv, A., Li, J., Wang, Y., Wang, F., Hu, H., and Yan, R. Stephint: Multi-level stepwise hints enhance reinforcement learning to reason. arXiv preprint arXiv:2507.02841, 2025b. Zhang, K., Zuo, Y., He, B., Sun, Y., Liu, R., Jiang, C., Fan, Y., Tian, K., Jia, G., Li, P., et al. survey of reinforcement learning for large reasoning models. arXiv preprint arXiv:2509.08827, 2025c. Zhang, X., Wu, S., Zhu, Y., Tan, H., Yu, S., He, Z., and Jia, J. Scaf-grpo: Scaffolded group relative policy optimization for enhancing llm reasoning. arXiv preprint arXiv:2510.19807, 2025d. Zheng, C., Liu, S., Li, M., Chen, X.-H., Yu, B., Gao, C., Dang, K., Liu, Y., Men, R., Yang, A., et al. Group sequence policy optimization. arXiv preprint arXiv:2507.18071, 2025. Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. Shao, Z., Wang, P., Zhu, Q., Xu, R., Song, J., Bi, X., Zhang, H., Zhang, M., Li, Y., Wu, Y., et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Sheng, G., Zhang, C., Ye, Z., Wu, X., Zhang, W., Zhang, R., Peng, Y., Lin, H., and Wu, C. Hybridflow: flexible and efficient rlhf framework. arXiv preprint arXiv: 2409.19256, 2024. Swamy, G., Choudhury, S., Sun, W., Wu, Z. S., and Bagnell, J. A. All roads lead to likelihood: The value of reinforcement learning in fine-tuning. arXiv preprint arXiv:2503.01067, 2025. Wang, Q., Ding, R., Zeng, Y., Chen, Z., Chen, L., Wang, S., Xie, P., Huang, F., and Zhao, F. Vrag-rl: Empower vision-perception-based rag for visually rich information understanding via iterative reasoning with reinforcement learning. arXiv preprint arXiv:2505.22019, 2025. Wu, R., Wang, X., Mei, J., Cai, P., Fu, D., Yang, C., Wen, L., Yang, X., Shen, Y., Wang, Y., et al. Evolver: Self-evolving llm agents through an experience-driven lifecycle. arXiv preprint arXiv:2510.16079, 2025. Wulfmeier, M., Bloesch, M., Vieillard, N., Ahuja, A., Bornschein, J., Huang, S., Sokolov, A., Barnes, M., Desjardins, G., Bewley, A., et al. Imitating language via scalable inverse reinforcement learning. Advances in Neural Information Processing Systems, 37:9071490735, 2024. Xie, G., Shi, Y., Tian, H., Yao, T., and Zhang, X. Capo: Towards enhancing llm reasoning through verifiable generative credit assignment. arXiv e-prints, pp. arXiv2508, 2025. Yan, J., Li, Y., Hu, Z., Wang, Z., Cui, G., Qu, X., Cheng, Y., and Zhang, Y. Learning to reason under off-policy guidance. arXiv preprint arXiv:2504.14945, 2025. Yang, A., Li, A., Yang, B., Zhang, B., Hui, B., Zheng, B., Yu, B., Gao, C., Huang, C., Lv, C., et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. Yu, Q., Zhang, Z., Zhu, R., Yuan, Y., Zuo, X., Yue, Y., Dai, W., Fan, T., Liu, G., Liu, L., et al. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476, 2025. Zeng, Y., Huang, W., Huang, S., Bao, X., Qi, Y., Zhao, Y., Wang, Q., Chen, L., Chen, Z., Chen, H., et al. Agentic jigsaw interaction learning for enhancing visual perception and reasoning in vision-language models. arXiv preprint arXiv:2510.01304, 2025a. 10 A. Result of Performance Evolution Preprint As illustrated in Figures 6, 7, and 8, we visualize the performance evolution of models with different scales (Qwen3-4B-Base, Qwen3-8B-Base, and Qwen3-14B-Base) across multiple benchmarks throughout training. It can be observed that MEL consistently outperforms standard GRPO in terms of average performance on all benchmarks. Figure 6. Performance evolution of GRPO and MEL on Qwen3-4B-Base across training steps on multiple benchmarks. Figure 7. Performance evolution of GRPO and MEL on Qwen3-8B-Base across training steps on multiple benchmarks. 11 Preprint Figure 8. Performance evolution of GRPO and MEL on Qwen3-14B-Base across training steps on multiple benchmarks. B. Retention Ratio of Meta-Experience Through empirical validation via replay, MEL is able to collect high-quality meta-experiences. To examine the utilization of collected meta-experiences, Figure 9 reports the retention ratio of meta-experiences after empirical validation throughout training. We observe that the retention ratio consistently increases with model scale, indicating that larger models are more effective at abstracting high-quality knowledge into meta-experiences, thereby achieving higher retention. Figure 9. Dynamics of the retention ratio of MEL across different model scales. 12 C. Prompt Template Preprint We use the same prompt template for all models. Details of the prompts used for meta-experience construction and for empirical validation via replay are shown below. Meta-Experience Prompt You are Meta-Cognitive Reasoning Analyst specializing in self-reflection, error root-cause analysis, and the extraction of generalizable heuristics. You are provided with multiple solution trajectories for the same problem. Note that the labels Correct or Incorrect apply to the final answer, but the reasoning process itself may contain twists and turns. Your task is to conduct deep comparative autopsy of the thinking processes. You must identify the structural differences in cognition that led to success or failure, and synthesize these into abstract principles for future use. Core Analysis Requirements: 1. Deep Dive into Correct Trajectories (Resilience & Robustness Analysis): Scenario (Self-Correction): If you find the reasoning contains initial errors or uncertainties, look for moments of self-correction. What triggered the realization? What structural insight allowed the reasoning to pivot back to the right track? Scenario (Flawless Execution): When every step of the reasoning is correct from the start, identify the Foundational Immunity. What specific definition, clear knowledge representation, or disciplined step-by-step verification prevented this Agent from falling into the traps that the Incorrect Agent fell into? Goal: Extract the specific logic validation technique or robust mental representation that saved the solution. 2. Deep Dive into Incorrect Trajectories (Vulnerability Analysis): You must identify not only where the math/logic went wrong, but also why the reasoning drifted. Identify: The Bifurcation Point where correct start turned into hallucination or logic gap. Analyze: The latent cognitive defect (e.g., concept conflation, rigid mindset, overlooking edge cases, intuitive bias) that caused the error. Identify: What specific knowledge point or constraint was violated? 3. Comparative Synthesis: Contrast the Solutions and Decision Boundaries. Why did the successful trajectory avoid the trap that the failed one fell into? What structural insight did the winner have that the loser missed? (e.g., The winner treated the problem as geometric issue, while the loser got stuck in algebra). 4. Strict Generalization Constraint: Forbidden: Do NOT mention the specific numbers, variables, or exact answer of the current problem in your Heuristics or Reflective Summary. Required: Convert specific lessons into abstract heuristics (e.g., instead of The integral of x2 is. . . , use When integrating polynomial functions. . . ). Formulate them as conditionally triggered rules (If. . . Then. . . , When dealing with [Concept X]. . . should. . . ). Output Format (Strict Adherence Required) 1. Failure Resolution Path & Error Pattern Recognition (Mandatory for incorrect samples) Failure Point: Identify the exact step where logic diverged. Did it start correctly? Where did the drift happen? Latent Cognitive Pattern: Reveal the deep-seated reasoning defect. Was it bias? missing prerequisite? misunderstanding of the prompts intent? Do not list surface-level calculation errors. 2. Analysis of Success Factors (Mandatory for correct samples) 13 Preprint Reasoning Pivot (If applicable): If the path involved self-correction, describe the moment of realization and the strategy used to fix it. Robustness Factor (If flawless): If the path was perfect, explain the fundamental concept or structural approach that effectively immunized the reasoning against common errors. Reason for Effectiveness: Why did this perspective work? What fundamental logic did it satisfy? 3. First-Person Reflective Summary (Mandatory) Write meta-cognitive reflection from the first-person perspective (I). Review: Briefly review the thinking process differences. Insight: Discuss the specific knowledge point or cognitive habit that was critical. Action: Explain how you will restructure your approach to avoid the identified Internal Reasoning Defects in the future. Focus on the How of thinking, not the What of the answer. 4. Subject Heuristics (Internalized Experience) (Mandatory) [Pattern Name]: If [abstract condition] occurs, then [abstract action]. . . [Pattern Name]: When dealing with [concept type], must strictly verify [constraint]. . . (Note: These must be applicable to *future* problems of similar class, completely stripped of this problems specifics.) Here the question and the corresponding solutions. <question> {question} </question> Solution 1: <answer> {error ans} </answer> <judge> Incorrect </judge> Solution 2: <answer> {correct ans} </answer> <judge> Correct </judge> Empirical Validation Prompt Prior study has provided some internal reference information relevant to this question, including the key approaches, steps, and reasoning needed for correct solution; the typical reasoning biases, logical flaws, or pitfalls that appear in incorrect solutions; and various heuristic insights on how to complete this problem more effectively. {experience} Now, please fully internalize this information as your own experience, then independently think through the problem in detail and produce complete answer. Note: You must perform full, in-depth reasoning internally and arrive at the final answer while making full use of the information above. Answer the following question: {question}"
        }
    ],
    "affiliations": [
        "University of Science and Technology of China"
    ]
}