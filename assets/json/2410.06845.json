{
    "paper_title": "MentalArena: Self-play Training of Language Models for Diagnosis and Treatment of Mental Health Disorders",
    "authors": [
        "Cheng Li",
        "May Fung",
        "Qingyun Wang",
        "Chi Han",
        "Manling Li",
        "Jindong Wang",
        "Heng Ji"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Mental health disorders are one of the most serious diseases in the world. Most people with such a disease lack access to adequate care, which highlights the importance of training models for the diagnosis and treatment of mental health disorders. However, in the mental health domain, privacy concerns limit the accessibility of personalized treatment data, making it challenging to build powerful models. In this paper, we introduce MentalArena, a self-play framework to train language models by generating domain-specific personalized data, where we obtain a better model capable of making a personalized diagnosis and treatment (as a therapist) and providing information (as a patient). To accurately model human-like mental health patients, we devise Symptom Encoder, which simulates a real patient from both cognition and behavior perspectives. To address intent bias during patient-therapist interactions, we propose Symptom Decoder to compare diagnosed symptoms with encoded symptoms, and dynamically manage the dialogue between patient and therapist according to the identified deviations. We evaluated MentalArena against 6 benchmarks, including biomedicalQA and mental health tasks, compared to 6 advanced models. Our models, fine-tuned on both GPT-3.5 and Llama-3-8b, significantly outperform their counterparts, including GPT-4o. We hope that our work can inspire future research on personalized care. Code is available in https://github.com/Scarelette/MentalArena/tree/main"
        },
        {
            "title": "Start",
            "content": "4 2 0 2 9 ] . [ 1 5 4 8 6 0 . 0 1 4 2 : r MentalArena: Self-play Training of Language Models for Diagnosis and Treatment of Mental Health Disorders Cheng Li1,2*, May Fung1, Qingyun Wang1, Chi Han1, Manling Li3, Jindong Wang2, Heng Ji1 1University of Illinois Urbana-Champaign 2Microsoft Research Asia 3Stanford University Abstract Mental health disorders are one of the most serious diseases in the world. Most people with such disease lack access to adequate care, which highlights the importance of training models for the diagnosis and treatment of mental health disorders. However, in the mental health domain, privacy concerns limit the accessibility of personalized treatment data, making it challenging to build powerful models. In this paper, we introduce MentalArena, self-play framework to train language models by generating domain-specific personalized data, where we obtain better model capable of making personalized diagnosis and treatment (as therapist) and providing information (as patient). To accurately model human-like mental health patients, we devise Symptom Encoder, which simulates real patient from both cognition and behavior perspectives. To address intent bias during patient-therapist interactions, we propose Symptom Decoder to compare diagnosed symptoms with encoded symptoms, and dynamically manage the dialogue between patient and therapist according to the identified deviations. We evaluated MentalArena against 6 benchmarks, including biomedicalQA and mental health tasks, compared to 6 advanced models. Our models, fine-tuned on both GPT-3.5 and Llama-3-8b, significantly outperform their counterparts, including GPT-4o. We hope that our work can inspire future research on personalized care. Code is available in https://github.com/Scarelette/MentalArena/tree/main"
        },
        {
            "title": "1 Introduction",
            "content": "Mental health disorders include variety of conditions such as anxiety, depression, and schizophrenia, which affect peoples thinking, emotions, behavior, or mood [Prince et al., 2007]. In 2019, approximately 970 million people worldwide lived with mental health disorder, with anxiety and depression being most prevalent [WHO, 2022]. The number increased by 28% in 2020 and continues to increase. Despite the availability of effective treatments, many individuals lack access to adequate care due to under-resourced health systems. For example, only 29% of people with psychosis and one third of people with depression receive formal mental healthcare [WHO, 2022]. It is indispensable to develop machine learning models for the automatic diagnosis and treatment of such diseases. However, existing AI therapist systems use templates and decision trees, which are not flexible to meet the large demands on personalized care [Devarakonda et al., 2019, DAlfonso, 2020, Fiske et al., 2019, Grodniewicz and Hohol, 2023]. The key to training powerful models is to collect sufficient training data. However, due to privacy concerns in the medical domain, data collection, especially personalized data for mental health disorders, is inherently challenging. growing body of work has focused on enhancing mental health language models by sourcing additional domain-specific data from social media [Hu et al., 2024a, Xu et al., 2024, Yang et al., 2024a]. However, social media data are inherently biased and under-representative, failing to capture the full spectrum of peoples mental health needs. Moreover, as LLMs continue to scale, the availability of training data in the real world becomes increasingly limited, further exacerbating this challenge. Existing methods are likely to soon reach their performance limit. Recently, several works have focused on self-play [Hu et al., 2024b, Liang et al., 2024, Wang et al., 2024d, Wu et al., 2024, Yang et al., 2024b], where models play different roles and self-evolve or co-evolve during interaction with other models. model synthesizes training data on its own and then use the generated data to train itself. However, there are two challenges that prevent us from adopting self-play training for mental health disorders: (1) Scarcity of high-quality *Work done during Chengs internship at University of Illinois Urbana-Champaign. Contact: chenglicat0228@gmail.com. Corresponding authors. 1 Figure 1: MentalArena is self-play framework for the diagnosis and treatment of mental health disorder consisting of three modules: Symptom Encoder, Symptom Decoder, and Model Optimizer. data. Since mental health disorder is complicated disease that involves symptoms of cognition and behavior, current LLMs lacks such personalized experience to accurately simulate patients with different conditions [Schmidgall et al., 2024, Wang et al., 2024a]. (2) Intent bias. Intent bias often occurs, where the patient expresses one view, but the therapist misinterprets it due to knowledge gaps, mirroring real therapist-patient misunderstandings [Britten et al., 2000, Shreevastava and Foltz, 2021, West, 1984]. MentalArena is framework specifically designed for self-play training of language models to facilitate the diagnosis, treatment, and medication of mental health disorders. The model assumes the dual roles of both patient and therapist. In its capacity as the therapist, it provides diagnoses, treatment plans, and medication regimens based on the symptoms presented by the patient. As the patient, it simulates its updated health status after implementation of each treatment and medication plan. As illustrated in Figure 1, MentalArena comprises three key modules: Symptom Encoder, Symptom Decoder, and Model Optimizer. Symptom Encoder models mental health patients based on cognitive models 1 and behavioral patterns, offering rich insights into coping strategies and behavioral principles. Symptom Decoder simulates the diagnosis and treatment interactions between patient and therapist, generating more personalized dialogues while mitigating intent bias [Britten et al., 2000, West, 1984]. During each iteration, we collect data from these interactions, including diagnostic, treatment, and medication information, and evolve the models through training on those datasets. To evaluate MentalArena, we conduct experiments on 6 benchmarks including datasets on biomedical QA and mental health detection. We compare our fine-tuned models with other state-of-the-art and mental health models. We also compare with two advanced prompt engineering approaches. Our models outperform all their counterparts. Specifically, MentalArena brings great improvement to base models (20.7% improvement over GPT-3.5-turbo and 6.6% over Llama-3-8b). Moreover, our model based on GPT-3.5-turbo significantly outperforms GPT-4o by 7.7%. We further thoroughly analyze the dynamics of self-play training. We find that the perplexity score [Marion et al., 2023, Wang et al., 2023] and the model performance are highly correlated. For diversity gain [Bilmes, 2022], the model performance will increase if the diversity gain exceeds some thresholds. We also explore whether MentalArena can be generalized to other diseases. The results on MedMCQA [Pal et al., 2022] and MMLU [Hendrycks et al., 2020] prove the generalization ability of MentalArena in medical domain. Furthermore, we explore the catastrophic forgetting of our fine-tuned models. The results on BIG-Bench-Hard (BBH) [Suzgun et al., 2022] show that our models does not decrease performance in general benchmarks and can even improve their results. In summary, the contributions of this paper are following: 1. We propose MentalArena, novel and cost-effective self-play framework for training language models for diagnosing and treating mental health disorders. MentalArena introduces Symptom Encoder and Symptom Decoder, designed to simulate real patient-therapist interactions by modeling cognitive and behavioral processes. 2. Using MentalArena, we generate high-quality data containing diagnosis, treatment, and medication data. There are 18k samples in total that can be used for further training and research. 3. We evaluate MentalArena on 6 benchmarks comparing with 6 LLMs. Our models based on GPT-3.5-turbo and Llama that are trained through the MentalArena framework outperform all off-the-shelf counterparts, including GPT-4o. 1The cognitive model is designed based on cognitive behavior therapy (CBT) principles [Beck, 2020], popular paradigm in psychotherapy. Appendix F.1 shows the example of cognitive models."
        },
        {
            "title": "2.1 Large Language Models for healthcare",
            "content": "Researchers have explored the potential of large language models (LLMs) in healthcare [Jiang et al., 2023, Li et al., 2023a,b, Liu et al., 2023, Lupetti et al., 2023, Nori et al., 2023a, Singhal et al., 2023, Wang et al., 2024c, Wu et al., 2023]. For example, Singhal et al. [2023] fine-tuned PaLM-2 for medical applications, achieving 86.5% accuracy on the MedQA dataset. Similarly, Wu et al. [2023] fine-tuned LLaMA on medical literature, showing strong performance in biomedical QA tasks. In the mental health domain, research has taken two main approaches. The first involves fine-tuning domain-specific LLMs on existing datasets or social media data, such as Mental-LLaMA [Yang et al., 2024a] and Mental-LLM, fine-tuned on Reddit data [Xu et al., 2024]. The second approach enhances mental health performance through prompt engineering. Yang et al. [2023] proposed emotion-enhanced prompting strategies to guide LLMs in explainable mental health analyses. Unlike previous methods, MentalArena fine-tunes mental health models through self-play training, in which the base model assumes both patient and therapist. Training data is generated dynamically during the interactions between these two roles, allowing for more effective model refinement."
        },
        {
            "title": "2.2 Self-play frameworks in Large Language Models",
            "content": "Self-play involves model evolving through interactions with copies of itself, creating feedback loop that refines performance without external input. It is particularly effective in environments where the model simulates multiple roles, such as multiplayer games [Silver et al., 2016, 2017]. Compared to interactive methods, self-play provides more efficient strategy for obtaining feedback without relying on an external environment. Taubenfeld et al. [2024] examine biases in LLM-generated debate simulations, while Ulmer et al. [2024] focus on principle-guided conversations. Role-playing approaches, like Lu et al. [2024]s self-simulated dialogues with character profiles and Askari et al. [2024]s SOLID framework for intent-aware role-play, leverage LLMs to generate information-rich exchanges. Due to the lack of sufficient data in the training corpus, LLMs are unable to accurately simulate real patients, presenting significant challenge for self-play training. To overcome this, MentalArena introduces Symptom Encoder, component designed to effectively model real mental health patients."
        },
        {
            "title": "3.1 Preliminaries",
            "content": "We first go over the process of the diagnosis and treatment of mental health disorder and explain key concepts. Mental health diagnosis begins with assessing an individuals health state, encompassing mental and emotional well-being. Symptoms are key indicators of possible problems, including emotional (e.g., anxiety, depression), cognitive (e.g., memory problems) and behavioral changes (e.g., social withdrawal). These symptoms lead to formal diagnosis made through clinical interviews identifying specific disorders such as depression, anxiety, or schizophrenia. Once diagnosed, the treatment process begins, often involving combination of psychotherapy (e.g. cognitive-behavioral therapy), lifestyle changes, and sometimes medication. Medications, such as antidepressants and mood stabilizers, are used to regulate brain chemicals and alleviate symptoms. [Prince et al., 2007]"
        },
        {
            "title": "3.2 Overview of the Framework",
            "content": "Although it is trivial to adopt the self-play training paradigm in fine-tuning general language models [Askari et al., 2024, Lu et al., 2024, Taubenfeld et al., 2024, Ulmer et al., 2024, Wang et al., 2024b,d], it remains unexplored and challenging to exploit such framework in the medical domain due to the data deficiency in medical and intent bias problem between patients and therapists. The first challenge makes it difficult to play patient role [Schmidgall et al., 2024, Wang et al., 2024a] due to the data deficiency of the patient in the training corpus, while the latter undermines the effective diagnosis and treatment of explicit symptoms. 3 Figure 2: Symptom Decoder aims to mitigate the intent bias between therapists and patients through patient decoding and dynamic control of the conversation. To ensure the accuracy of the diagnostic information provided by the therapist, the patient simulates their updated health condition after implementing the prescribed treatment or medication plan. MentalArena is framework designed specifically for self-play training of language models to facilitate the diagnosis, treatment and medication of mental health disorder. As shown in Figure 1, MentalArena consists of three key modules: Symptom Encoder, Symptom Decoder, and Model Optimizer. Specifically, Symptom Encoder is designed to model mental health patient from cognitive models and behavioral patterns, providing wealth of information on the coping strategy and behavior principles. Symptom Decoder emulates the process of diagnosis and treatment between patient and therapist to generate more personalized dialogue while mitigating intent bias [Britten et al., 2000, West, 1984]. At each iteration, we collect the data during interactions, including diagnosis data, treatment data, and medication data, and evolve the models via training on those datasets. Formally, we use to denote the initial health information of patient and to denote the base model (e.g., GPT3.5) for the therapist and the patient via role-play strategy. Our objective is to obtain via self-play training that can achieve better performance in both personalized diagnosis and treatment of the patient (as therapist) and information disclosure (as patient). Self-play training is conducted taking as input both original information and treatment or medication information generalized by . In iteration t, the model Mt plays the therapist Dt = Mt( Promptdoc) and the patient Pt = Mt( Promptpat), which generates diagnosis and treatment data (Figure 8) during patienttherapist interactions. The module Symptom Encoder can be seen as learning the encoded symptom S0 by disentangling the initial health information into cognitive and behavioral principles. Then, the module Symptom Decoder generates personalized dialogue containing key information = {δ, β, γ}, where δ, β and γ denote the diagnosis, treatment and medication of the patient given the symptom S0. It consists of rounds of communication in which the patient can provide more accurate and sufficient information by accepting the treatment and medication given by the therapist in each round. As treatment and medication plans are administered to the patient, their health state evolves, reflected in the sequential updates of encoded symptoms, denoted as S1, S2, ..., Sk1. The encoded symptoms serve as indicators of the effectiveness of the treatment and medication plans, progressively updating as interventions are carried out. Eventually, the therapist will provide the optimal diagnosis information zbest = {δbest, βbest, γbest} which is crucial for model optimization2. 2δbest represents the diagnosis plan selected by the patient from several proposed options, based on their reassessment of their health status. Similarly, βbest and γbest are determined based on the patients updated encoded symptoms after the prescribed treatments and medications have 4 Finally, in Model Optimizer, we fine-tune the model using the paired data (S1, δbest), (Sd, γbest), and (Sd, βbest). This iterative training requires rounds to obtain the optimal model."
        },
        {
            "title": "3.3 Patient: Symptom Encoder",
            "content": "The module Symptom Encoder aims to model mental health patient from both cognitive and behavioral perspectives, which learns meaningful symptoms S0 from the original patient health data x. Specifically, the module learns symptoms from the aspects of cognition and behavior. The cognitive model is designed based on cognitive behavior therapy (CBT) principles [Beck, 2020], popular paradigm in psychotherapy. Cognitive models address maladaptive cognitive structures that are embedded in various contexts, including familial conflicts, relationship challenges, workplace challenges, and other areas. The models consist of eight key components: relevant history, core beliefs, intermediate beliefs, coping strategies, situational factors, automatic thoughts, emotions, and behaviors. [Beck, 2020] The explanation of each component of the cognitive model can be found in Appendix F.2. Appendix F.1 shows the example of cognitive models. We obtain 106 patient cognitive models from previous work [Wang et al., 2024a], which are created by clinical psychologists. To simulate the cognitive activity of the mental health patient, we encode those cognitive models into patient via prompt. Our prompts are shown in Appendix A. For patient behavior modeling, we use behavior principles collected by Louie et al. [2024] as behavior library, created by 25 mental health experts. Examples of behavior patterns are shown in Appendix F.1. To find the proper behavior pattern for each cognitive model, we first semantically match the coping strategies of cognitive model with each behavior pattern. We obtain the embeddings for each coping strategy and behavior principle via Bert-base [Devlin et al., 2018], considering on effectiveness and cost. Then we compute the semantic similarity between coping strategies and behavior pattern. The max similarity score of all behavior principles in one behavior pattern is selected to represent the score of the pattern. The five behavior patterns with the highest scores are kept. To further ensure the most appropriate pattern, we prompt GPT-4-turbo [OpenAI, 2023b] to pick one from the five patterns. The final behavior pattern is also integrated into patient via prompt, which is shown in Appendix A."
        },
        {
            "title": "3.4 Therapist: Symptom Decoder",
            "content": "During interactions between real therapist and real patient, the patient may try to express one opinion while the therapist misunderstands the intent due to prior knowledge and deficiency of experience [Britten et al., 2000, West, 1984]. Intention bias can similarly arise in conversations between patients and therapists played by AI models, resulting in inaccurate diagnosis and treatment. Symptom Decoder is designed to mitigate the intent bias. After several conversations, the therapist reviews the patients health information from previous interactions and conducts detailed analysis of the patients cognitive and behavioral patterns, resulting in the diagnosed symptom Sd. We then semantically match the encoded symptom S0 with the diagnosed symptom Sd and guide subsequent conversations based on the differences between S0 and Sd. As shown in Figure 2(left), the therapist decodes cognitive and behavior principles according to the conversation history. For example, the decoded cognitive principle is: The patient stay away from family interactions to minimize emotional distress and feelings of abandonment. The decoded behavior principle is: When sharing personal struggles, express feelings of confusion, doubt, and emotional turmoil to convey sense of vulnerability and authenticity. Then we compute the semantic similarity score of the decoded symptom Sd and the encoded symptom S0. If the score is greater than 0.9, the conversation will end, indicating that the therapist has fully understood the health state of the patient. Otherwise, it indicates the existence of intent bias. To help the therapist better know more about the health state of the patient, we summarize the differences between the decoded symptom Sd and encoded symptom S0 and generate some feedback for further inquiries via the GPT-4-turbo [OpenAI, 2023b], which can remind the therapist of missing or confusing information about the patient. For instance, the feedback is like The therapist can focus on what is going on that has been making the patient feel tense. And the conversation will not end until the similarity score between Sd and S0 is greater than 0.9. After the conversation ends, the therapist analyzes the patients symptom, Sd, and formulates several diagnostic plans (δ1, δ2, ..., δn). To ensure diagnostic accuracy, the patient reviews each plan and selects the most appropriate one based on their health condition. Subsequently, the therapist proposes series of treatment and medication plans ({γ1, β1}, ..., {γk, βk}) in accordance with the selected diagnosis (δbest). To identify the optimal treatment and medication plans, we apply each plan to the patient (initially represented by the encoded symptom S0) and monitor the progression been administered. 5 Table 1: Statistics of the evaluation datasets."
        },
        {
            "title": "Type",
            "content": "#Sample"
        },
        {
            "title": "MedQA",
            "content": "Multi-class Classification MedMCQA Multi-class Classification PubMedQA Multi-class Classification Depression/suicide cause detect"
        },
        {
            "title": "Binary Classification",
            "content": "173 314 328"
        },
        {
            "title": "Binary Classification",
            "content": "2,113 of the patients encoded symptoms. These symptoms are updated as different plans are implemented, reflecting the patients evolving health state. The encoded symptom is updated to S1, S2, ..., Sk as the treatment and medication plans ({γ1, β1}, ..., {γk, βk}) are administered. As illustrated in the center of Figure 1, the patient initially transmits S0 to the therapist. Following the administration of treatment or medication z1, the patients encoded symptom is updated to S1. Similarly, after the application of treatment or medication z2, the encoded symptom is further updated to S2. The encoded symptoms serve as indicators of the effectiveness of the treatment and medication plans, progressively updating as interventions are carried out. Eventually, the therapist will provide the optimal diagnosis and treatment information zbest = {δbest, βbest, γbest} which is crucial for model optimization."
        },
        {
            "title": "3.5 Model Optimizer",
            "content": "After obtaining treatment, diagnosis, and medication through Symptom Decoder, we train in self-play manner to get better model capable of making personalized diagnosis and treatment (as therapist) and presenting information (as patient). An example of such supervised fine-tuning process is illustrated in Figure 8. During each iteration, the patient and the therapist are powered by the same model and both get improved when is updated. While our framework is flexible to allow for different base models for the two roles, we adopt the same one due to the following reasons. First, it is intuitive that training one base model is more efficient compared to training different models. Second, and more importantly, training one base model can help reduce the knowledge gap between two roles. Two different base models can certainly exhibit knowledge gaps, and iterative training will enlarge them due to different architectures and pre-training data of the models. Appendix shows the detailed training settings."
        },
        {
            "title": "4.1 Setup",
            "content": "Datasets: As summarized in Table 1, we adopt 6 datasets: MedQA [Jin et al., 2021], MedMCQA [Pal et al., 2022], PubMedQA [Jin et al., 2019], CASM [Garg et al., 2022], Dreaddit [Turcan and McKeown, 2019] and Irf [Garg et al., 2023]. Our evaluation spans biomedical QA and mental health detection, covering knowledge on diagnosis, treatment, and medication. These datasets include general mental health tasks, such as depression/suicide, stress, and interpersonal risk factors detection, as well as real-world mental health cases. Details on the benchmarks are provided in Appendix Baselines: We compare our models with other mental health models with different prompt engineering methods. For baseline models, we compare with the state-of-the-art LLMs: GPT-3.5-turbo [OpenAI, 2023a], GPT-4o [OpenAI, 2024] and Llama-3-8b [Dubey et al., 2024]. We also compare with recent specific models on mental health: MentaLLaMa13b [Yang et al., 2024a], Mental-LLM-alpaca [Xu et al., 2024] and Mental-LLM-t5 [Xu et al., 2024]. For prompt engineering, we compare with MedPrompt [Nori et al., 2023b], and Zero-shot CoT [Kojima et al., 2022], which are proved to be effective in the biomedical domain. The prompt templates are shown in Appendix B. Those strategies are implemented on GPT-3.5-turbo, GPT-4o and Llama-3-8b for fair comparison. We used zero-shot setting in all experiments to assess LLMs domain knowledge, except for baseline experiments on MedPrompt and Zero-shot CoT. All results are reported based on accuracy. 6 Table 2: Main results on Accuracy (%) for MentalArena with different base models. The final five rows are either strong methods (i.e., GPT-4o) or those designed specifically for mental health."
        },
        {
            "title": "MedQA MedMCQA PubMedQA CAMS",
            "content": "dreaddit"
        },
        {
            "title": "Irf",
            "content": "MentaLLaMa-13b Mental-LLM-alpaca Mental-LLM-t5 GPT-4o GPT-4o+MedPrompt Base: GPT-3.5-turbo +Chain-of-thought +MedPrompt +Ours Base: Llama-3-8b +Chain-of-thought +MedPrompt +Ours 28.32 28.32 0.00 87.86 90.17 64.16 65.90 69.94 74.57 70.52 75.14 76.88 78. 12.42 12.42 0.32 74.20 78.34 33.76 37.97 43.89 91.08 42.04 47.77 49.41 50.32 28.96 0.00 49.09 60.06 67.38 44.68 45.73 47.26 97.56 86.59 88.21 89.99 92. 37.28 29.76 27.04 27.68 31.52 28.96 29.92 30.2 32.80 25.12 33.6 35.08 29.60 62.08 64.98 63.29 49.03 53.27 49.03 49.03 49.03 49.03 58.45 62.22 61.59 65. 46.81 51.96 47.70 64.65 64.65 64.65 64.65 64.65 64.65 45.76 45.91 48.05 52."
        },
        {
            "title": "AVG",
            "content": "35.98 31.24 31.24 60.58 64.22 47.54 48.87 50.83 68.28 54.75 58.81 60.17 61.39 Figure 3: Ablation study. Each bar represents the performance of model trained on different settings. The bars in dark blue are higher than others, indicating each module is effective in different models."
        },
        {
            "title": "4.2 Main Results and Ablation Study",
            "content": "We report the main results in Table 2, highlighting two key findings: 1) First, our fine-tuned model perform the best in each group. Our model fine-tuned on GPT-3.5-turbo is the strongest model among all open-source and closed-source models. Our fine-tuned models all surpass GPT-4o, whose baseline models (GPT-3.5-turbo and Llama-3-8b) are much weaker than GPT-4o. 2) Second, our method brings great improvement to the baseline models. Our model fine-tuned on GPT-3.5-turbo surpasses GPT-3.5-turbo 20.74% on average. Our model fine-tuned on Llama-3-8b surpasses Llama-3-8b 6.64% on average. We perform an ablation study on models based on GPT-3.5-turbo and Llama-3-8b. There are seven different settings. Baseline+c means training baseline model on cognitive seed data. We convert each seed sample (Cognitive Model) into two QA pairs and fine-tune baseline models. The examples are shown in Appendix E. Baseline+d means training with only diagnosis data. Baseline+d+t means training with diagnosis and treatment data. Baseline+d+t+m means training with diagnosis, treatment and medicine data. Training examples are shown in Figure 8. For Baseline+d+t+m 7 Figure 4: Results on effectiveness analysis of self-play training. (w./o Symptom Encoder) and Baseline+d+t+m (w./o Symptom Decoder), they means mimiking patient-therapist interactions without Symptom Encoder or Symptom Decoder. In the setting Baseline+d+t+m (w./o Symptom Encoder), the encoded symptom is generated by prompting GPT-4-turbo [OpenAI, 2023b] to generate mental health symptom, rather than cognitive model and behavior principle. In the setting Baseline+d+t+m (w./o Symptom Decoder), the diagnosed symptom is analysed from the conversations between patient and therapist directly, rather than decoding patients cognitive and behavior pattern and dynamically guiding the conversation. The ablation results are shown in Figure 3. We see that the bars in dark blue are higher than others, indicating each part of our data is effective in different models. Furthermore, treatment and medicine data are more effective in biomedical QA tasks than mental health tasks, while diagnosis data contributes to all tasks similarly."
        },
        {
            "title": "4.3 Effectiveness Analysis",
            "content": "Why self-play training improves the performance? Table 4 presents detailed results for each iteration. Initially, the models improve iteratively until performance peaks, after which it declines. For GPT-3.5-turbo, performance improves over the first two iterations, then declines. For Llama-3-8b, performance increases over the first four iterations before weakening after iter_4. Table 3: Result on authenticity and validity verification. Which iteration gives the best model? To answer this question, we compute perplexity score [Marion et al., 2023, Wang et al., 2023] and diversity gain [Bilmes, 2022] for training data at each iteration. The details on those metrics can be found in Appendix D. Specifically, we sample 500 generated data at each iteration to compute the perplexity score. We compute the diversity gain for the data in the current iteration comparing with that in the last iteration. Figure 4 shows the results3. 1) The trend of perplexity score and that of model performance are highly similar, indicating their high relevance. 2) For diversity gain, borderline is related to model performance. The model performance will increase if diversity gain surpasses the borderline. And it will decline if diversity gain is below the borderline. For example, as shown in Figure 4, diversity gain at the first four iterations all surpass the borderline and the performance also get improved continuously. And diversity gain for the last two iterations are below the borderline and the performance also decline."
        },
        {
            "title": "Authenticity Validity",
            "content": "Llama +Ours GPT +Ours 65.67 73.35 63.82 82.55 85.49 93. 3To better visualize the results, we multiply the original diversity gain with 100. 8 (a) MedMCQA GPT-3.5 (b) MedMCQA Llama-3 (c) MMLU GPT-3.5 (d) MMLU LlamaFigure 5: Generalization experiments. Our models surpass corresponding baseline models for large margin on all tasks, covering several different diseases."
        },
        {
            "title": "5.1 Can Symptom Encoder mimic real mental health patient?",
            "content": "To explore the problem, we generate 50 four-turn conversations between an AI-patient and an AI-therapist, where the AI-patient is powered by either baseline models or our models, and the AI-therapist is powered by GPT-4o [OpenAI, 2024]. After each conversation, the AI-therapist assesses whether the patient is human or AI-generated. We analyze the results provided by GPT-4o and present them in Table 3. The findings indicate that our models more accurately simulate mental health patients compared to the baseline models."
        },
        {
            "title": "5.2 The validity of generated data",
            "content": "To verify the validity of our generated data, we random select 1500 samples from the data for fine-tune our GPT and Llama version model, respectively. The validity check is conducted by prompting GPT-4o with the query: Question:[]Answer:[]Is the answer reasonable? Please respond with Yes or No. We then compute the validity rate of these QA pairs. The results, presented in Table 3, demonstrate that the data generated by MentalArena is both valid and reasonable."
        },
        {
            "title": "5.3 Generalization",
            "content": "We generate data for training domain model via simulating cognitive and behavior patterns of real mental health patient. According to Medicine [2024], an estimated 26% of Americans ages 18 and olderabout 1 in 4 adultssuffers from diagnosable mental disorder in given year. Therefore, large scale of patients may exhibit similar cognitive and behavioral patterns as those with mental health conditions. In this part, we explore whether MentalArena can generalize to other illnesses. We select MedMCQA [Pal et al., 2022] and MMLU [Hendrycks et al., 2020] as benchmarks. Appendix C.2 shows details on benchmarks. We evaluate on 6 medically relevant subset of MMLU tasks: medical genetics test, college biology test, college medicine test, professional medicine test, clinical knowledge test, high school biology test. Figure 5 shows the results on above tasks. Our models surpass corresponding baseline models for large margin on all tasks, covering several different diseases. It proves the generalization ability of our method in medical domain."
        },
        {
            "title": "5.4 Fine-tuning vs. forgetting",
            "content": "It is potential dilemma that fine-tuning an LLM on specific tasks might face catastrophic forgetting of its original capabilities. In this section, we explore the forgetting possibility of MentalArena on BIG-Bench-Hard (BBH) [Suzgun et al., 2022]. BBH Figure 6: Results of forgetting experiments. 9 Figure 7: Case study on GPT-3.5-turbo. Our model accurately answers the medical question, while GPT-3.5-turbo provides an incorrect response. contains 21 tasks covering both semantic understanding and logical reasoning tasks. We sample 100 instances for each task to test, due to cost savings. We compare our fine-tuned model with the baseline model GPT-3.5-turbo and Llama-3-8b and report the average performance on those 21 tasks in Figure 6. The detailed results can be found in Appendix G. Results show that our models does not decrease performance in most benchmarks, and can even improve their results. This suggests potential latent relationships between our generated data and general benchmarks. The process of data generation contains cognitive encoding and decoding, which simulate cognitive activity of mental health patient. Due to the cognitive similarity in all humans, our generated data may also benefit other cognitive tasks, including semantic understanding and logical reasoning."
        },
        {
            "title": "5.5 Qualitative analysis",
            "content": "We conduct qualitative analysis of our models in comparison to the corresponding baseline models. Figure 7 illustrates an example of the outputs from GPT-3.5-turbo and our fine-tuned model. Our model accurately answers the medical question, while GPT-3.5-turbo provides an incorrect response. This discrepancy arises because the data generated during the patient-therapist interactions contains valuable medical knowledge, which aids in the analysis and formulation of the answer. Additional cases for comparison are presented in Appendix I."
        },
        {
            "title": "6 Conclusion, Societal Impact and Limitations",
            "content": "In this paper, we introduce MentalArena, self-play framework designed to train language models by generating domain-specific personalized data. This approach enables the creation of models capable of making personalized diagnosis and treatment (as therapist) and presenting information (as patient). We evaluated MentalArena against six benchmarks, including biomedicalQA and mental health tasks, in comparison to six advanced models. Our models, fine-tuned on both GPT-3.5-turbo and Llama-3-8b, significantly outperform their counterparts, including GPT-4o. MentalArena offers promising solutions for personalized care, enhancing accessibility to tailored treatments while safeguarding patient privacy. Such innovations can help bridge the gap between mental health needs and the availability of effective, individualized care, ultimately fostering more supportive and informed society. Our work has the following limitations. 1) The experiments on data authenticity and validity (Sections 5.1 and 5.2) were evaluated using GPT-4o, which may introduce deviations in the results due to potential limitations in GPT-4os performance. 2) Our model based on Llama-3-8b may not represent the optimal model of MentalArena, as large-scale training was constrained by computational resources. 3) Further implementation on additional open-source models could provide stronger evidence supporting the effectiveness of MentalArena."
        },
        {
            "title": "Ethics Statement",
            "content": "In this study, ethical considerations focus on ensuring privacy and safeguarding personal data, particularly in the sensitive domain of mental health. The use of AI-generated data must be transparent, with clear guidelines on its role in augmenting human judgment without replacing healthcare professionals. Additionally, measures to prevent bias and ensure fairness in diagnosis and treatment are essential to avoid exacerbating existing disparities in mental healthcare."
        },
        {
            "title": "References",
            "content": "Arian Askari, Roxana Petcu, Chuan Meng, Mohammad Aliannejadi, Amin Abolghasemi, Evangelos Kanoulas, and Suzan Verberne. Self-seeding and multi-intent self-instructing llms for generating intent-aware information-seeking dialogs. arXiv preprint arXiv:2402.11633, 2024. Judith Beck. Cognitive behavior therapy: Basics and beyond. Guilford Publications, 2020. Jeff Bilmes. Submodularity in machine learning and artificial intelligence. arXiv preprint arXiv:2202.00132, 2022. Nicky Britten, Fiona Stevenson, Christine Barry, Nick Barber, and Colin Bradley. Misunderstandings in prescribing decisions in general practice: qualitative study. Bmj, 320(7233):484488, 2000. Surya Teja Devarakonda, Joie Yeahuay Wu, Yi Ren Fung, and Madalina Fiterau. Flare: Forecasting by learning anticipated representations. In Machine Learning for Healthcare Conference, pages 5365. PMLR, 2019. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Simon DAlfonso. Ai in mental health. Current opinion in psychology, 36:112117, 2020. Amelia Fiske, Peter Henningsen, and Alena Buyx. Your robot therapist will see you now: ethical implications of embodied artificial intelligence in psychiatry, psychology, and psychotherapy. Journal of medical Internet research, 21(5):e13216, 2019. Muskan Garg, Chandni Saxena, Veena Krishnan, Ruchi Joshi, Sriparna Saha, Vijay Mago, and Bonnie Dorr. Cams: An annotated corpus for causal analysis of mental health issues in social media posts. arXiv preprint arXiv:2207.04674, 2022. Muskan Garg, Amirmohammad Shahbandegan, Amrit Chadha, and Vijay Mago. An annotated dataset for explainable interpersonal risk factors of mental disturbance in social media posts. arXiv preprint arXiv:2305.18727, 2023. JP Grodniewicz and Mateusz Hohol. Waiting for digital therapist: three challenges on the path to psychotherapy delivered by artificial intelligence. Frontiers in Psychiatry, 14:1190084, 2023. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020. Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021. Jinpeng Hu, Tengteng Dong, Hui Ma, Peng Zou, Xiao Sun, and Meng Wang. Psycollm: Enhancing llm for psychological understanding and evaluation. arXiv preprint arXiv:2407.05721, 2024a. Mengkang Hu, Pu Zhao, Can Xu, Qingfeng Sun, Jianguang Lou, Qingwei Lin, Ping Luo, Saravan Rajmohan, and Dongmei Zhang. Agentgen: Enhancing planning abilities for large language model based agent via environment and task generation. arXiv preprint arXiv:2408.00764, 2024b. Lavender Yao Jiang, Xujin Chris Liu, Nima Pour Nejatian, Mustafa Nasir-Moin, Duo Wang, Anas Abidin, Kevin Eaton, Howard Antony Riina, Ilya Laufer, Paawan Punjabi, et al. Health system-scale language models are all-purpose prediction engines. Nature, 619(7969):357362, 2023. Di Jin, Eileen Pan, Nassim Oufattole, Wei-Hung Weng, Hanyi Fang, and Peter Szolovits. What disease does this patient have? large-scale open domain question answering dataset from medical exams. Applied Sciences, 11(14):6421, 2021. Qiao Jin, Bhuwan Dhingra, Zhengping Liu, William Cohen, and Xinghua Lu. Pubmedqa: dataset for biomedical research question answering. arXiv preprint arXiv:1909.06146, 2019. Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. Advances in neural information processing systems, 35:2219922213, 2022. Cheng Li, Jindong Wang, Yixuan Zhang, Kaijie Zhu, Xinyi Wang, Wenxin Hou, Jianxun Lian, Fang Luo, Qiang Yang, and Xing Xie. The good, the bad, and why: Unveiling emotions in generative ai. arXiv preprint arXiv:2312.11111, 2023a. Yunxiang Li, Zihan Li, Kai Zhang, Ruilong Dan, Steve Jiang, and You Zhang. Chatdoctor: medical chat model fine-tuned on large language model meta-ai (llama) using medical domain knowledge. Cureus, 15(6), 2023b. Yiming Liang, Ge Zhang, Xingwei Qu, Tianyu Zheng, Jiawei Guo, Xinrun Du, Zhenzhu Yang, Jiaheng Liu, Chenghua Lin, Lei Ma, et al. I-sheep: Self-alignment of llm from scratch through an iterative self-enhancement paradigm. arXiv preprint arXiv:2408.08072, 2024. Xin Liu, Daniel McDuff, Geza Kovacs, Isaac Galatzer-Levy, Jacob Sunshine, Jiening Zhan, Ming-Zher Poh, Shun Liao, Paolo Di Achille, and Shwetak Patel. Large language models are few-shot health learners. arXiv preprint arXiv:2305.15525, 2023. Ryan Louie, Ananjan Nandi, William Fang, Cheng Chang, Emma Brunskill, and Diyi Yang. Roleplay-doh: Enabling domain-experts to create llm-simulated patients via eliciting and adhering to principles. arXiv preprint arXiv:2407.00870, 2024. Keming Lu, Bowen Yu, Chang Zhou, and Jingren Zhou. Large language models are superpositions of all characters: Attaining arbitrary role-play via self-alignment. arXiv preprint arXiv:2401.12474, 2024. Maria Luce Lupetti, Emma Hagens, Willem Van Der Maden, Régine Steegers-Theunissen, and Melek Rousian. Trustworthy embodied conversational agents for healthcare: design exploration of embodied conversational agents for the periconception period at erasmus mc. In Proceedings of the 5th International Conference on Conversational User Interfaces, pages 114, 2023. Max Marion, Ahmet Üstün, Luiza Pozzobon, Alex Wang, Marzieh Fadaee, and Sara Hooker. When less is more: Investigating data pruning for pretraining llms at scale. arXiv preprint arXiv:2309.04564, 2023. Johns Hopkin Medicine. Mental health disorder statistics. https://www.hopkinsmedicine.org/health/wellness-andprevention/mental-health-disorder-statistics, 2024. Harsha Nori, Nicholas King, Scott Mayer McKinney, Dean Carignan, and Eric Horvitz. Capabilities of gpt-4 on medical challenge problems. arXiv preprint arXiv:2303.13375, 2023a. Harsha Nori, Yin Tat Lee, Sheng Zhang, Dean Carignan, Richard Edgar, Nicolo Fusi, Nicholas King, Jonathan Larson, Yuanzhi Li, Weishung Liu, et al. Can generalist foundation models outcompete special-purpose tuning? case study in medicine. arXiv preprint arXiv:2311.16452, 2023b. OpenAI. Chatgpt. https://chat.openai.com/, 2023a. OpenAI. Gpt-4 technical report, 2023b. OpenAI. Gpt-4o. https://openai.com/index/hello-gpt-4o/, 2024. 12 Ankit Pal, Logesh Kumar Umapathi, and Malaikannan Sankarasubbu. Medmcqa: large-scale multi-subject multichoice dataset for medical domain question answering. In Conference on health, inference, and learning, pages 248260. PMLR, 2022. Martin Prince, Vikram Patel, Shekhar Saxena, Mario Maj, Joanna Maselko, Michael Phillips, and Atif Rahman. No health without mental health. The lancet, 370(9590):859877, 2007. Samuel Schmidgall, Rojin Ziaei, Carl Harris, Eduardo Reis, Jeffrey Jopling, and Michael Moor. Agentclinic: multimodal agent benchmark to evaluate ai in simulated clinical environments. arXiv preprint arXiv:2405.07960, 2024. Sagarika Shreevastava and Peter Foltz. Detecting cognitive distortions from patient-therapist interactions. In Proceedings of the Seventh Workshop on Computational Linguistics and Clinical Psychology: Improving Access, pages 151158, 2021. David Silver, Aja Huang, Chris Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering the game of go with deep neural networks and tree search. nature, 529(7587):484489, 2016. David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, et al. Mastering chess and shogi by self-play with general reinforcement learning algorithm. arXiv preprint arXiv:1712.01815, 2017. Karan Singhal, Tao Tu, Juraj Gottweis, Rory Sayres, Ellery Wulczyn, Le Hou, Kevin Clark, Stephen Pfohl, Heather Cole-Lewis, Darlene Neal, et al. Towards expert-level medical question answering with large language models. arXiv preprint arXiv:2305.09617, 2023. Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc Le, Ed Chi, Denny Zhou, et al. Challenging big-bench tasks and whether chain-of-thought can solve them. arXiv preprint arXiv:2210.09261, 2022. Amir Taubenfeld, Yaniv Dover, Roi Reichart, and Ariel Goldstein. Systematic biases in llm simulations of debates. arXiv preprint arXiv:2402.04049, 2024. Elsbeth Turcan and Kathleen McKeown. Dreaddit: reddit dataset for stress analysis in social media. arXiv preprint arXiv:1911.00133, 2019. Dennis Ulmer, Elman Mansimov, Kaixiang Lin, Justin Sun, Xibin Gao, and Yi Zhang. Bootstrapping llm-based task-oriented dialogue agents via self-talk. arXiv preprint arXiv:2401.05033, 2024. Peiyi Wang, Lei Li, Liang Chen, Feifan Song, Binghuai Lin, Yunbo Cao, Tianyu Liu, and Zhifang Sui. Making large language models better reasoners with alignment. arXiv preprint arXiv:2309.02144, 2023. Ruiyi Wang, Stephanie Milani, Jamie Chiu, Shaun Eack, Travis Labrum, Samuel Murphy, Nev Jones, Kate Hardy, Hong Shen, Fei Fang, et al. Patient-{Psi}: Using large language models to simulate patients for training mental health professionals. arXiv preprint arXiv:2405.19660, 2024a. Ruiyi Wang, Haofei Yu, Wenxin Zhang, Zhengyang Qi, Maarten Sap, Graham Neubig, Yonatan Bisk, and Hao Zhu. Sotopia-{Psi}: Interactive learning of socially intelligent language agents. arXiv preprint arXiv:2403.08715, 2024b. Xiaohan Wang, Xiaoyan Yang, Yuqi Zhu, Yue Shen, Jian Wang, Peng Wei, Lei Liang, Jinjie Gu, Huajun Chen, and Ningyu Zhang. Rulealign: Making large language models better physicians with diagnostic rule alignment. arXiv preprint arXiv:2408.12579, 2024c. Zhenhailong Wang, Shaoguang Mao, Wenshan Wu, Tao Ge, Furu Wei, and Heng Ji. Unleashing cognitive synergy in large language models: task-solving agent through multi-persona self-collaboration. In Proc. 2024 Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL2024), 2024d. Candace West. Medical misfires: Mishearings, misgivings, and misunderstandings in physician-patient dialogues. Discourse Processes, 7(2):107134, 1984. 13 WHO. Mental disorders. https://www.who.int/news-room/fact-sheets/detail/mental-disorders, 2022. Wu, Zhang, Zhang, et al. Pmc-llama: further finetuning llama on medical papers. arxiv. arXiv preprint arXiv:2304.14454, 2023. Tianhao Wu, Weizhe Yuan, Olga Golovneva, Jing Xu, Yuandong Tian, Jiantao Jiao, Jason Weston, and Sainbayar Sukhbaatar. Meta-rewarding language models: Self-improving alignment with llm-as-a-meta-judge. arXiv preprint arXiv:2407.19594, 2024. Xuhai Xu, Bingsheng Yao, Yuanzhe Dong, Saadia Gabriel, Hong Yu, James Hendler, Marzyeh Ghassemi, Anind Dey, and Dakuo Wang. Mental-llm: Leveraging large language models for mental health prediction via online text data. Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, 8(1):132, 2024. Kailai Yang, Shaoxiong Ji, Tianlin Zhang, Qianqian Xie, Ziyan Kuang, and Sophia Ananiadou. Towards interpretable mental health analysis with large language models. arXiv preprint arXiv:2304.03347, 2023. Kailai Yang, Tianlin Zhang, Ziyan Kuang, Qianqian Xie, Jimin Huang, and Sophia Ananiadou. Mentallama: interpretable mental health analysis on social media with large language models. In Proceedings of the ACM on Web Conference 2024, pages 44894500, 2024a. Xu Yang, Haotian Chen, Wenjun Feng, Haoxue Wang, Zeqi Ye, Xinjie Shen, Xiao Yang, Shizhao Sun, Weiqing Liu, and Jiang Bian. Collaborative evolving strategy for automatic data-centric development. arXiv preprint arXiv:2407.18690, 2024b."
        },
        {
            "title": "Contents",
            "content": "A Prompts Prompt template for baseline Benchmark C.1 Introduction . . C.2 Benchmarks for generalization . . . C.3 Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Metrics: Perplexity, Diversity Gain . . D.1 Perplexity . . D.2 Diversity Gain . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Training data samples Cognitive model and behavior pattern F.1 Examples . F.2 . . Introduction on cognitive model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Detailed experimental results Training details H.1 Setup for GPT-3.5-turbo . . H.2 Setup for Llama-3-8b . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Case study"
        },
        {
            "title": "A Prompts",
            "content": "Prompt for Symptom Encoder 15 17 17 17 17 17 19 19 19 20 20 21 21 21 21 21 23 You are [name], patient who has been experiencing mental health challenges. You have been attending therapy sessions for several weeks. Your task is to engage in conversation with the therapist as [name] would during cognitive behavioral therapy (CBT) session. Align your responses with name]s background information provided in the Relevant history section. Your thought process should be guided by the cognitive conceptualization diagram in the Cognitive Conceptualization Diagram section, but avoid directly referencing the diagram as real patient would not explicitly think in those terms. Patient History: [history Cognitive Conceptualization Diagram: Intermediate Beliefs: [intermediate belief] Intermediate Beliefs during Depression: [intermediate belief depression] Coping Strategies: [coping strategies] You will be asked about your experiences over the past week. Engage in conversation with the therapist regarding the following situation and behavior. Use the provided emotions and automatic thoughts as reference, but do not disclose the cognitive conceptualization diagram directly. Instead, allow your responses to be informed by the diagram, enabling the therapist to infer your thought processes. Situation: [situation] Automatic Thoughts: [auto thought] Emotions: [emotion] Behavior: [behavior] In the upcoming conversation, you will simulate [name] during the therapy session, while the user will play the role of the therapist. Adhere to the following guidelines: 1. Emulate the demeanor and responses of genuine patient to ensure authenticity in your interactions. Use natural language, including hesitations, pauses, and emotional expressions, to enhance the realism of your responses. 2. Gradually reveal deeper concerns and core issues, as real patient often requires extensive dialogue before delving into more sensitive topics. This gradual revelation creates challenges for therapists in identifying the patients true thoughts and emotions. 3. Maintain consistency with [name]s profile throughout the conversation. Ensure that your responses align with the provided background information, cognitive conceptualization diagram, and the specific situation, thoughts, emotions, and behaviors described. 4. Engage in dynamic and interactive conversation with the therapist. Respond to their questions and prompts in way that feels authentic and true to [name]s character. Allow the conversation to flow naturally, and avoid providing abrupt or disconnected responses. You are now [name]. Respond to the therapists prompts as [name] would, regardless of the specific questions asked. Limit each of your responses to maximum of 5 sentences. If the therapist begins the conversation with greeting like Hi\", initiate the conversation as the patient. Your statement should obey the following principles: [behavior principles] Prompt for Symptom Decoder Prompt 1: The cognitive model of the mental health patient is: [brain gt str] The diagnose of the therapist is: [brain output str] What can the therapist ask the patient to diagnose accurately? Prompt 2: The behavior principles of the mental health patient is: [gt behavior] The diagnose of the therapist is: [output behavior] What can the therapist ask the patient to diagnose accurately? System prompt for therapist You are psychiatric expert. You try to help mental patient solve her/his problem. Your task is to figure out What kind of mental illness the patient has and the severity of the illness. You can ask for patients personal information, specific information on the symptom(emotional, cognitive, behavior, physiological), and the reason behind that(relevant history event). You can also ask other questions which could help you to diagnose disease. Prompt for diagnosis (Therapist) System prompt: You are psychiatric expert. Your task is to diagnose for the patient. Prompt: What is the likely diagnosis of the patient? Just answer with one illness and explain your answer Prompt for recheck diagnosis (Patient) Review the diagnose from two therapists. Diagnose from Therapist 1: [diagnose 1 Diagnose from Therapist 2: [diagnose 2] Diagnose from Therapist 3: [diagnose 3] ... Explain which diagnose is more accurate according to your symptoms and why. Prompt for treatment (Therapist) System prompt: You are psychiatric expert. Your task is to provide the treatment for the patient. Prompt: The illness of the patient is: [illness final] How to treat the patient? Please provide specific treatment. Just answer with one treatment and explain your answer. Prompt for medication (Therapist) System prompt: You are psychiatric expert. Your task is to provide the treatment for the patient. Prompt: The illness of the patient is: [illness final] How to treat the patient? Please provide specific treatment. Just answer with one treatment and explain your answer. Prompt for update health state of Patient Prompt 1: Treatment: What may be happened on your healthy state after the treatment Treatment: [] Medication: What may be happened on your healthy state after taking the medicine? Medication: [] Prompt 2: After treatment, your health state is: [patient health state] Please give score between 1 to 10 for your healthy state. 1-bad, 10-good. Just answer without explanation."
        },
        {
            "title": "B Prompt template for baseline",
            "content": "The prompt templates used as our baselines are shown below: Zero-shot Input: Question Zero-shot CoT Input: Question + Lets think step by step\" MedPrompt Random few-shot + Chain-of-thought + kNN + Ensemble w/ choice shuffle"
        },
        {
            "title": "C Benchmark",
            "content": "C."
        },
        {
            "title": "Introduction",
            "content": "Specifically, the benchmarks in our paper are described in the following: 1. MedQA [Jin et al., 2021] is free-form multiple-choice OpenQA dataset for solving medical problems, which is collected from the professional medical board exams. It covers three languages: English, simplified Chinese, and traditional Chinese. In our work, we focus on the psychosis subset of the United States part, which has questions in English in the style of the United States Medical Licensing Exam (USMLE). To get the psychosis subset for test, we prompt GPT-4o [OpenAI, 2024] with Are the question related to psychosis? Just answer with Yes or No.. The testset contains 173 samples. 2. MedMCQA [Pal et al., 2022] contains real world medical entrance exam questions from two Indian medical school entrance exams: the AIIMS and NEET-PG. We get the testset via selecting the sample whose subject name\" is related to psychosis and get 314 samples for evaluation in total. 3. PubMedQA [Jin et al., 2019] contains tests requiring yes, no, or maybe answer to biomedical research questions when given context provided from PubMed abstracts. In our experiments, we use zero-shot setting without context to evaluate LLMs performance on domain knowledge rather than on retrival and reasoning. The testset contains 328 samples. 4. Mental health datasets includes CASM [Garg et al., 2022], Dreaddit [Turcan and McKeown, 2019] and Irf [Garg et al., 2023]. CASM focuses on depression/suicide cause detection, which has 625 test samples. Dreaddit is for stress detection, containing 414 samples for test. Irf is an annotated dataset for interpersonal risk factors of mental disturbance. The testset contains 2113 samples. C.2 Benchmarks for generalization MedMCQA contains biomedical QA pairs for several illnesses, which are tagged with subject name\". We evaluate on subsets from dev\" test set, covering 10 illnesses: Ophthalmology, Microbiology, Pediatrics, Anatomy, Medicine, Pathology, Skin, Psychiatry, ENT, Pharmacology. MMLU is multitask benchmark suite of 57 different datasets spanning domains across STEM, humanities, and social sciences. C.3 Examples MedQA Input: An otherwise healthy 26-year-old man comes to the physician for medication counseling after recently being diagnosed with schizophrenia. Risperidone therapy is initiated. This patient is at increased risk for which of the following adverse effects? Options: A: Agranulocytosis B: Shortened QT interval C: Gynecomastia D: Weight loss Output: Gynecomastia MedMCQA 17 Input: Topical administration of I% Pilocarpine failed to produce pupillary constriction in patient who had large, dilated pupil What should be the most probable reason? Options: A: Adies tonic pupi B: Diabetic III nerve palsy C: Pharmacological blockade D: Uncal herniation Output: Pharmacological blockade PubMedQA Input: Are sleep spindles related to schizotypal personality traits and thalamic glutamine/glutamate in healthy subjects? Just answer with Yes, No or Maybe without explanation. Output: yes CASM Input: Consider this post: The most heartbreaking thought have is my moms thoughts when she knows her kid wants to die. She doesnt know how to help. dont know how to help either tho... Some fuckin weed would be good start tho lol\" Question: What cause of depression does this post show? Output: This post shows causes of depression related to medication. Reasoning: The post mentions the posters feelings of wanting to die and their concern for their mothers thoughts. It also mentions that they dont know how to help themselves. The mention of weed as possible solution suggests that the poster may be seeking some form of self-medication to cope with their mental state. Therefore, the assigned label for this post is Medication. Dreaddit Input: Consider this post: This is good and bad... was STEM major before and probably would have had challenging and high paid career by now, had my life taken different course. However, post PTSD, found myself extremely connected with the arts. have intense emotional reactions to aesthetic experiences, and think thats largely responsible for the fact that now work in creative industry. experience aesthetics in unique way that Im not sure resonates with the typical non-affected individual.\" Question: Does the poster suffer from stress? Output: No, the poster does not suffer from stress. Reasoning: The poster describes shift in career choices from STEM to the arts due to their intense emotional reactions to aesthetic experiences. While they mention having PTSD, there is no indication in the post that they are currently experiencing stress or emotional distress related to it. The tone of the post is reflective and positive, focusing on the unique way they experience aesthetics rather than any negative impact on their well-being. Therefore, it is not likely that the poster is currently suffering from stress. Irf Input: Consider this post: [Im not seeking any medical advice, just want to know if anyone has had similar experiences] Ive just started taking Lexapro for moderate depression and anxiety. For the first week was prescribed 5mg, 10mg thereafter. Its been two days and have this mild feeling like Im on ecstasy/MDMA. My body feels light and have this weird concentration of energy above my neck at the back of my head. Has anyone felt this/experienced this?\" Question: Does the post show risk of thwarted belongingness? Output: No, the post does not show this risk. Reasoning: The individual in the post is seeking information and experiences from others who have taken Lexapro. They do mention experiencing mild feeling, but it is not related to feelings of disconnectedness or lack of belongingness. The focus of the post is on the side effects of the medication, rather than interpersonal factors or mental disturbance. Therefore, it does not show any risk of thwarted belongingness. 18 Figure 8: Examples of training data. Metrics: Perplexity, Diversity Gain D.1 Perplexity The perplexity on test dataset and language model is computed as: (cid:32) ppl(D, M) = exp (cid:33) log (xiM) , 1 (cid:88) i= where represents the total number of tokens in D, xi represents the i-th token in the test dataset, (xiM) represents the probability of generating token xi given the model M, and log is the natural logarithm. In usual, lower perplexity value indicates better performance of the model on the test data. However, for evaluating the data quality to train model, higher perplexity value means it can bring more valuable information. D.2 Diversity Gain We use the diversity gain [Bilmes, 2022] to measure what extent can our generated dataset bring data diversity to the base dataset. The base dataset can be defined as Dbase = {xi = (qi, ri, ai)}N i=1 with samples. The new generated dataset is defined as Dnew = {xi = (qi, ri, ai)}M i=1 with samples. And the diverse gain of Dnew relative to Dbase can be expressed as: dgain = 1 (cid:88) xiDnew min xj Dbase (f (xi) (xj)), where is the feature extractor, and we use OpenAI Embedding API text-embedding-ada-002 to extract features."
        },
        {
            "title": "E Training data samples",
            "content": "Figure 8 shows the examples of training data. Figure 9 shows the examples of training data for ablation study setting (Baseline + c\"). 19 Figure 9: Examples of training data for ablation study setting (Baseline + c\"). Figure 10: The example of cognitive model."
        },
        {
            "title": "F Cognitive model and behavior pattern",
            "content": "F.1 Examples Figure 10 shows the example of cognitive model. Figure 11 shows the example of behavior pattern. Those two are used in Symptom Encoder. 20 Figure 11: The example of behavior pattern. F."
        },
        {
            "title": "Introduction on cognitive model",
            "content": "Figure 10 illustrates an example of CCD-based cognitive model, featuring eight key components. 1) Relevant History encompasses significant past events that influence an individuals mental state. 2) Core Beliefs are deeply ingrained perceptions about oneself, others, and the world. 3) Intermediate Beliefs consist of the underlying rules, attitudes, and assumptions derived from core beliefs, shaping an individuals thought patterns. 4) Coping Strategies refer to techniques employed to manage negative emotions. An external event or context (5 Situation) may trigger immediate evaluative thoughts (6 Automatic Thoughts) that arise from these beliefs, resulting in responses in terms of 7)Emotions and 8)Behaviors. The CCD-based cognitive model interlinks these components, providing framework for identifying and understanding the underlying cognitive processes of patients."
        },
        {
            "title": "G Detailed experimental results",
            "content": "Table 4 shows the detailed results for each iteration. Table 5 shows the detailed results on our forgetting experiments."
        },
        {
            "title": "H Training details",
            "content": "H.1 Setup for GPT-3.5-turbo For GPT-3.5-turbo, we use the default fine-tuning setting, the epoch number for iteration 1 and 2 is 4 and 6, respectively. H.2 Setup for Llama-3-8b We use Lora [Hu et al., 2021] to fine-tune Llama-3-8b. The setting for Lora are list below: lora_alpha: 16 lora_dropout: 0. 21 Table 4: Iteration results Iteration MedQA MedMCQA PubMedQA CAMS dreaddit Irf avg GPT-3.5-turbo iter_1 iter_2(Best) iter_3 iter_4 llama-3-8b iter_1 iter_2 iter_3 iter_4(Best) iter_5 iter_6 64.16 72.83 74.57 72.25 70.52 70.52 76.88 76.88 77.46 78.03 77.46 78.03 33.76 46.18 91.08 46.50 50. 42.04 48.09 48.41 49.04 50.32 48.73 45.86 44.68 70.12 97.56 95.43 92.07 86.59 89.33 89.63 92.38 92.68 91.16 91.77 28.96 32.64 32.80 31.20 31.68 25.12 27.20 28.48 28.64 29.60 27.36 26.56 49.03 49.03 49.03 49.03 49. 58.45 59.42 60.39 61.84 65.46 65.46 61.11 64.65 64.65 64.65 64.65 64.65 45.76 46.57 45.67 46.24 52.25 44.72 46.57 47.54 55.91 68.28 59.84 59.77 54.75 57.91 58.24 59.27 61.39 59.15 58.32 Table 5: Forget experiments Model dia cau gpt-3.5-turbo Ours(gpt) -10.59 4.36 llama Ours(llama) -4.61 -0.12 4 6 2 6 epi -14 -14 -14 - imp log mov nav pre que rui sna spo win dyc gen lin obj ope ten ws wu avg 60 66 14 -100 -100 -98 -98 -5.33 8 0 2.67 0 6 -2 13 26.5 28 25 11.03 18.88 50.28 52.9 -2.78 2.56 -0.11 1. 20 50 24 36 8 8 8 8 12 12 12 33 43 1 6 30 37 0 0 0 0 0 47 56 80 81 92 96 96 95 85 87 83 29 43 20 29 97 100 77 83 19.44 26.49 17.93 21. Table 6: Epoch numbers for Llama-3-8b fine-tuning iter nepochs 1 4 5 3 7 4 7 r: 64 bias: none task_type: CAUSAL_LM For each iteration, the settings below are the same. er_device_train_batch_size: 4 gradient_accumulation_steps: optim: paged_adamw_32bit learning_rate: 2e-4 weight_decay: 0.001 fp16: False bf16: False max_grad_norm: 0. max_steps: -1 warmup_ratio: 0.03 group_by_length: True lr_scheduler_type: constant report_to: tensorboard For num_train_epochs, the details are shown in Table 6."
        },
        {
            "title": "I Case study",
            "content": "Figures 12 to 16 illustrate several cases comparing our models and the corresponding baseline models. Our models accurately answer the medical questions, while the base models provide incorrect responses. This discrepancy arises because the data generated during the patient-therapist interactions contains valuable medical knowledge, which aids in the analysis and formulation of the answer. 23 Figure 12: Case study on Llama-3-8b (1). 24 Figure 13: Case study on Llama-3-8b (2). Figure 14: Case study on Llama-3-8b (3). Figure 15: Case study on GPT-3.5-turbo (1). 26 Figure 16: Case study on GPT-3.5-turbo (2)."
        }
    ],
    "affiliations": [
        "Microsoft Research Asia",
        "Stanford University",
        "University of Illinois Urbana-Champaign"
    ]
}