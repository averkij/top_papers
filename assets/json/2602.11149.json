{
    "paper_title": "Data Repetition Beats Data Scaling in Long-CoT Supervised Fine-Tuning",
    "authors": [
        "Dawid J. Kopiczko",
        "Sagar Vaze",
        "Tijmen Blankevoort",
        "Yuki M. Asano"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Supervised fine-tuning (SFT) on chain-of-thought data is an essential post-training step for reasoning language models. Standard machine learning intuition suggests that training with more unique training samples yields better generalization. Counterintuitively, we show that SFT benefits from repetition: under a fixed update budget, training for more epochs on smaller datasets outperforms single-epoch training on larger datasets. On AIME'24/25 and GPQA benchmarks, Olmo3-7B trained for 128 epochs on 400 samples outperforms the equivalent 1 epoch on 51200 samples by 12-26 percentage points, with no additional catastrophic forgetting. We find that training token accuracy reliably signals when repetition has saturated; improvements from additional epochs plateau at full memorization, a pattern consistent across all settings. These findings provide a practical approach for reasoning SFT, where scaling epochs with token accuracy as a stopping criterion can replace expensive undirected data scaling. We pose the repetition advantage, where full memorization coincides with improved generalization, as a new open problem for the community in understanding the training dynamics of large language models."
        },
        {
            "title": "Start",
            "content": "Data Repetition Beats Data Scaling in Long-CoT Supervised Fine-Tuning Dawid J. Kopiczko 1 Sagar Vaze 2 Tijmen Blankevoort 3 Yuki M. Asano 1 6 2 0 2 1 1 ] . [ 1 9 4 1 1 1 . 2 0 6 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Supervised fine-tuning (SFT) on chain-of-thought data is an essential post-training step for reasoning language models. Standard machine learning intuition suggests that training with more unique training samples yields better generalization. Counterintuitively, we show that SFT benefits from repetition: under fixed update budget, training for more epochs on smaller datasets outperforms single-epoch training on larger datasets. On AIME24/25 and GPQA benchmarks, Olmo37B trained for 128 epochs on 400 samples outperforms the equivalent 1 epoch on 51200 samples by 1226 percentage points, with no additional catastrophic forgetting. We find that training token accuracy reliably signals when repetition has saturated; improvements from additional epochs plateau at full memorization, pattern consistent across all settings. These findings provide practical approach for reasoning SFT, where scaling epochs with token accuracy as stopping criterion can replace expensive undirected data scaling. We pose the repetition advantage, where full memorization coincides with improved generalization, as new open problem for the community in understanding the training dynamics of large language models. Code is available at: https://github.com/ dkopi/data-repetition. 1. Introduction Modern language model training proceeds through distinct stages: pretraining on internet-scale data to acquire world knowledge, mid-training on curated corpora to extend capabilities, and post-training to shape model behavior (Guo et al., 2025; Team OLMo, 2025; Yang et al., 2025). For reasoning-focused models, post-training typically begins with supervised fine-tuning (SFT) on long Chain-ofThought (CoT) demonstrations, often distilled from 1University of Technology Nuremberg 2Mistral AI 3NVIDIA. Correspondence to: <dj.kopiczko@gmail.com>. Preprint. February 12, 2026. 1 Figure 1. Illustration of our approach to supervised fine-tuning in modern LLM training pipeline. Instead of maximizing dataset size and training for few epochs, we train for many epochs on small random subset of SFT data, substantially reducing compute while improving downstream reasoning performance. more capable models, where reasoning traces can span thousands of tokens before reaching final answer. This SFT step, analogous to behavioral cloning in reinforcement learning (Osa et al., 2018), primes the model for subsequent stages such as reinforcement learning from human feedback (Ouyang et al., 2022) or reinforcement learning with verifiable rewards (Guo et al., 2025; Shao et al., 2024). Unlike pretraining data, which can be scraped at scale from the web, high-quality long-CoT demonstrations require either expensive human annotation or careful distillation from larger models, including generation, filtering, and validation of long reasoning traces. As result, the question of how to best utilize limited SFT data is practically important. The common assumption in machine learning would suggest that training with more unique training samples yields better generalization. Under i.i.d. sampling, each new example provides independent information about the data distribution, and generalization bounds in statistical learning theory typically improve with dataset size. This principle manifests practically throughout the field data augmentation techniques are widely used to artificially expand effective dataset size when real data is limited (Hernandez-Garcıa & Konig, 2018; Shorten & Khoshgoftaar, 2019), and the success of large language models has been attributed in significant part to training on ever-larger unique corpora. Following this logic, modern post-training pipelines employ millions of SFT samples (Team OLMo, 2025). In this paper, we show that this might not only be suboptimal, but that, actually, reverse pattern can be observed for the SFT stage for pretrained LLM, see Figure 1. Under Data Repetition Beats Data Scaling in Long-CoT Supervised Fine-Tuning Figure 2. Scaling epochs versus scaling data for Olmo3-7B trained on long-CoT SFT data, averaged across AIME24, AIME25, and GPQA benchmarks. Each diagonal represents fixed update budget, where epochs samples is constant. Within any diagonal, moving toward fewer samples and more epochs consistently improves accuracy and pass@n, with gains diminishing around 3264 epochs. Termination rate correlates strongly with accuracy and may be primary driver of performance gains, as models that fail to terminate cannot produce final answer. fixed update budget, training for more epochs on smaller datasets outperforms training on larger datasets. The gains are not marginal. The performance and termination rates, i.e., the models ability to successfully conclude reasoning with final answer, both scale with epoch count and saturate together, suggesting that sufficient repetition of the same data is required for models to fully internalize the demonstrated reasoning structure. We find that this convergence is tightly linked to training set memorization. Performance improvements plateau once models achieve near-perfect next-token prediction accuracy on the training data, even as validation loss continues to rise. This relationship holds across all models we test, all benchmarks, and different training datasets, making train token accuracy practical stopping criterion for scaling epochs. Despite this apparent overfitting, we observe no additional catastrophic forgetting compared to single-epoch training on large datasets. Our main contributions are: Phenomenon. We demonstrate that under fixed update budget, scaling epochs on smaller datasets substantially outperforms scaling unique samples. Dynamics. We identify training token accuracy as reliable stopping criterion for epoch scaling, with performance gains plateauing once models reach full memorization, and we show that multi-epoch training on small datasets causes no additional catastrophic forgetting compared to large datasets. as teacher model size in distillation and the correctness of data samples, affect the repetition advantage. While we provide practical heuristic for exploiting the repetition advantage, we pose explaining this phenomenon in long-CoT SFT as novel, open problem for the community. 2. Scaling Epochs on Fixed Update Budget To investigate whether data repetition can substitute for data scaling in supervised fine-tuning, we conduct controlled experiments varying the number of epochs and unique samples while holding total gradient updates, and all other parameters, constant. We train base checkpoints of two recent language models on chain-of-thought data and evaluate on challenging reasoning benchmarks. 2.1. Preliminaries Supervised fine-tuning adapts pretrained language model to target behaviors by training on demonstration data. Given input-output pairs (x, y) where = (y1, . . . , yT ) is target sequence, SFT minimizes the cross-entropy loss over nexttoken predictions: L(θ) = (cid:88) t=1 log pθ(yt x, y<t) (1) In practice, the loss is typically masked to exclude input tokens, applying only to the response. Factors. We show how training data properties, such Throughout this work, we use update budget to denote the 2 Data Repetition Beats Data Scaling in Long-CoT Supervised Fine-Tuning Figure 3. The repetition advantage is consistent across models, benchmarks, and evaluation metrics. Heatmaps show normalized scores for Olmo3-7B (top) and Qwen3-8B (bottom) on AIME24, AIME25, and GPQA, evaluated with both Accuracy@n and Pass@n. Each diagonal corresponds to fixed update budget (epochs samples), and in all settings, performance improves when moving along diagonal toward fewer samples and more epochs. total number of gradient updates during training, which for batch size one is equal to the number of epochs multiplied by the number of unique samples. Comparing configurations at equal update budgets isolates the effect of data repetition from differences in total optimization steps. 2.2. Experimental Setup Models. We use the Qwen3-4B, Qwen3-8B (Yang et al., 2025), and Olmo3-7B (Team OLMo, 2025) base models. These are pretrained checkpoints prior to any instruction tuning, providing clean starting point for studying SFT dynamics. For training and evaluation, we use the default chat template for each model. Dataset. We use the Dolci SFT 7B1 dataset from the Olmo3 post-training pipeline, which contains distilled longCoT demonstrations spanning math, coding, precise instruction following, and general conversation (Team OLMo, 2025). We apply several filters: we keep only the first conversation turn, retain samples containing complete reasoning traces (verified by presence of <think> and </think> tags), and remove samples exceeding 10k tokens when tokenized with the Olmo tokenizer. From the filtered data, we randomly sample nested training splits of increasing size: 200, 400, 800, 1.6k, 3.2k, 6.4k, 12.8k, 25.6k, 51.2k samples, constructed so that each smaller split is subset of the next larger one. We hold out 1000 random samples as validation set for analysis. 1https://huggingface.co/datasets/allenai/ Dolci-Think-SFT-7B Evaluation. We evaluate on three challenging reasoning benchmarks: AIME 2024, AIME 2025, and GPQA. AIME (AIME, 2025) is mathematical reasoning benchmark consisting of 30 competition problems per year, requiring multistep reasoning across algebra, geometry, number theory, and combinatorics; each answer is an integer from 0 to 999. GPQA (Rein et al., 2024) is graduate-level multiplechoice benchmark with expert-written questions in biology, physics, and chemistry, where the model must reason through the problem before selecting from four options. For each problem in these benchmarks, we append an instruction requesting the final answer in boxed{} format for straightforward extraction. We report three metrics: Acc@n, the accuracy averaged over independent generations per problem; Pass@n, the fraction of problems solved in at least one of attempts; and Termination, the fraction of generations that conclude with an end-of-sequence token rather than being truncated. We sample up to 30k tokens per generation to accommodate extended reasoning traces. For AIME we generate 16 responses per problem, while for GPQA, 4 responses due to its larger test set. We use recommended sampling parameters from each models technical report and vLLM (Kwon et al., 2023) for efficient inference. Training. We load models in bfloat16, use Unsloth optimized kernels (Daniel Han & team, 2023), and the 8-bit Adam optimizer (Dettmers et al., 2022) with cosine learning rate schedule. Warmup is set to 10% of the total update budget for each run. We use batch size of one, following recent findings that small batch sizes achieve equal or better per-token performance (Marek et al., 2025). We mask 3 Data Repetition Beats Data Scaling in Long-CoT Supervised Fine-Tuning Table 1. Performance at fixed update budget of = 51, 200 gradient updates, showing configurations up to 16 epochs. All rows within each model use equivalent update budget but vary the epochs-to-samples ratio. For all three models, 16 epochs on 3,200 samples substantially outperforms 1 epoch on 51,200 samples across all benchmarks. Model Epochs Samples GPQA AIME24 AIME25 Avg@4 Pass@4 Avg@ Pass@16 Avg@16 Pass@16 7 - 3 l 8 - 3 Q 4 - 3 Q 1 2 4 8 16 1 2 4 8 16 1 2 4 8 16 51.2k 25.6k 12.8k 6.4k 3.2k 51.2k 25.6k 12.8k 6.4k 3.2k 51.2k 25.6k 12.8k 6.4k 3.2k 11.5 14.8 20.2 29.7 34.0 21.6 25.4 35.6 41.9 51.0 13.1 21.1 29.7 40.5 39.3 23.7 29.3 38.9 51.5 62.1 38.9 42.4 56.6 61.6 72. 29.8 38.9 52.5 64.1 68.7 17.7 28.1 33.3 44.4 42.3 12.3 14.0 20.6 30.6 30.6 6.5 13.5 18.1 23.3 19.2 46.7 66.7 73.3 73.3 80.0 36.7 46.7 66.7 70.0 76. 26.7 36.7 36.7 43.3 43.3 22.3 24.8 29.2 35.4 39.2 11.2 17.3 20.0 27.7 31.2 5.6 14.0 17.7 23.5 18.8 50.0 46.7 50.0 66.7 63.3 30.0 43.3 40.0 56.7 63. 30.0 36.7 40.0 43.3 40.0 the input prompt and compute cross-entropy loss only on response tokens. We conduct learning rate sweep for each model using 1 epoch on 51,200 samples and select the bestperforming rate based on benchmark accuracy, then use that learning rate for all subsequent runs. Each configuration is run on single H100 94GB GPU for up to 24 hours. Experimental grid. We train models across dataset sizes from 200 to 51,200 samples and epoch counts from 1 to 256, subject to maximum update budget of 51,200. For example, the 200-sample split is trained for up to 256 epochs, while the 51,200-sample split is trained for only 1 epoch. Each configuration is trained independently from the base checkpoint with its own warmup and learning rate schedule, rather than evaluating intermediate checkpoints from single extended run. This design ensures that for any given update budget, we can compare multiple configurations trading off epochs against unique samples. 2.3. Results Figure 2 presents heatmaps of accuracy, pass rate, and termination rate across all combinations of epochs and dataset sizes for Olmo3-7B, averaged over benchmarks. Figure 3 presents normalized scores for each benchmark separately, for Olmo3-7B and Qwen3-8B models. Table 1 provides detailed per-benchmark results for all models at fixed update budget of 51,200 gradient updates, showing training runs up to 16 epochs. Figures with all configurations can be found in Appendix B. Fewer unique samples repeated more times yields substantially better performance than training on more data for fewer epochs. For example, at budget of 51,200 updates, Olmo3-7B trained for 32 epochs on 1,600 samples reaches an average 39% accuracy across benchmarks, compared to 17% for single epoch on 51,200 samples. The same pattern appears across benchmarks and models: on Figure 3 all top performances are clearly in the top part of the samplesepochs pyramid. The gains diminish around 3264 epochs, suggesting saturation point beyond which additional repetition provides limited benefit. We investigate this saturation in Sec. 4.1 3. Impact of Training Data The previous experiments establish the repetition advantage on general-purpose SFT dataset spanning diverse domains, but whether this phenomenon depends on properties of the training data remains unclear. In this section, we vary data characteristics while keeping the model fixed to Olmo3-7B. We construct math-focused datasets by distilling long chainof-thought solutions from various Qwen3 models. We use problems from the NuminaMath-TIR2 dataset (Li et al., 2024) as prompts and generate solutions using reasoning checkpoints of Qwen3-0.6B and Qwen3-8B as teacher models. We split each distilled dataset into nested subsets from 200 to 25,600 samples and train across the same epoch2https://huggingface.co/datasets/AI-MO/ We can see clear and consistent pattern: NuminaMath-TIR 4 Data Repetition Beats Data Scaling in Long-CoT Supervised Fine-Tuning Table 2. Impact of teacher model size on the repetition advantage. Olmo3-7B is trained on math data distilled from Qwen3-0.6B and Qwen3-8B teachers, with results averaged across AIME24, AIME25, and GPQA. The repetition advantage persists for both teachers. With the weaker 0.6B teacher, increasing the update budget from 6.4k to 25.6k leads to lower peak performance, echoing the degradation observed in weak-to-strong generalization. Teacher Budget Ep Samples Avg@n Pass@n 6 . 0 - 3 Q 8 - 3 Q 6.4k 6.4k 6.4k 6.4k 6.4k 6.4k 25.6k 25.6k 25.6k 25.6k 25.6k 25.6k 6.4k 6.4k 6.4k 6.4k 6.4k 6.4k 25.6k 25.6k 25.6k 25.6k 25.6k 25.6k 1 2 4 8 16 1 2 4 8 16 32 1 2 4 8 16 32 1 2 4 8 16 32 6.4k 3.2k 1.6k 800 400 200 25.6k 12.8k 6.4k 3.2k 1.6k 800 6.4k 3.2k 1.6k 800 400 25.6k 12.8k 6.4k 3.2k 1.6k 800 2.2 4.8 6.9 11.4 20.6 21.6 3.8 5.4 6.5 10.4 20.8 20.2 10.6 17.3 22.1 25.4 26.1 24.5 13.3 18.5 24.1 33.3 35.5 30.1 13.7 19.4 22.3 34.1 46.5 54. 16.7 19.1 25.9 29.8 49.2 49.5 36.1 39.7 51.0 53.6 55.0 51.0 36.5 40.0 49.9 64.9 66.6 63.0 sample grid as before. 3.1. Teacher Model Quality. Table 2 compares results when training on data distilled from the 0.6B and 8B teachers, for budgets of = 25, 600 and = 6, 400. The repetition advantage persists in both settings, with epoch scaling improving performance more reliably than data scaling regardless of teacher size. The interaction between epochs and data differs between teachers, however. With the smaller 0.6B teacher, the average performance degrades with additional samples; the highest average pass rate for = 6,400 is 54.0%, while for = 25,600 its 49.5%. This pattern echoes findings in weak-to-strong generalization (Burns et al., 2024), where student models trained on weaker teacher data can initially exceed teacher performance but degrade with prolonged exposure. With the larger 8B teacher, the pattern is similar to the previous experiments on the Dolci SFT 7B dataset. The model reaches higher absolute performance after sufficient number of epochs, and the performance improves when 5 Table 3. Training on incorrect reasoning traces does not harm performance. Olmo3-7B is trained on positive and negative trajectories distilled from Qwen3-8B, with fixed update budget of = 6.4k. The repetition advantage holds regardless of trajectory correctness. Surprisingly, training on negatives often matches or exceeds training on positives, with higher peak performance on GPQA and AIME24. and denote Accuracy@n and Pass@n respectively, while Ep stands for the number of epochs. Ep Subset GPQA AIME24 AIME25 A@4 P@4 A@16 P@16 A@16 P@16 2 4 8 Neg. Pos. Neg. Pos. Neg. Pos. Neg. Pos."
        },
        {
            "title": "32 Neg.\nPos.",
            "content": "4.5 13.1 9.1 3.0 6.6 17.7 6.2 15.2 11.9 26.8 10.4 25.3 18.7 42.4 19.1 43.4 29.3 54.0 23.4 51.5 28.5 55.6 16.4 41. 15.0 13.3 22.1 20.2 30.2 27.3 35.0 36.9 40.0 37.3 35.0 38. 56.7 33.3 66.7 60.0 73.3 66.7 73.3 80.0 80.0 80.0 70.0 76. 13.3 15.0 19.8 19.4 27.5 26.0 30.8 29.2 33.3 34.2 31.2 27. 36.7 40.0 46.7 46.7 50.0 46.7 66.7 53.3 66.7 63.3 70.0 53. scaling up the number of data samples. In this case, the highest average pass rate for = 6,400 is 55.0%, while for = 25,600 its 66.6%. These results suggest that: Teacher quality determines whether data scaling remains beneficial, while the repetition advantage itself is robust to teacher choice. 3.2. Negative Trajectories If the repetition advantage depends on learning from correct reasoning, training on incorrect traces should degrade performance or exhibit different scaling dynamics. We define negative trajectories as chain-of-thought samples where the models final answer is incorrect. To test this, we take the data distilled from the Qwen3-8B teacher and partition it by correctness of the final answer. Samples with correct answers form the positive set; those with incorrect answers form the negative set. We construct nested splits from 200 to 6,400 samples for each and train Olmo3-7B across the same epoch-sample grid. From Table 3 we find that: Training on negative trajectories does not degrade performance. The epoch scaling advantage persists with the same pattern as before. Moreover, the top performance on AIME24 and GPQA is on-par and even slightly higher when training on Data Repetition Beats Data Scaling in Long-CoT Supervised Fine-Tuning Figure 4. Relationship between training set memorization and downstream performance for Olmo3-7B. Points are colored by epoch count; within each epoch group, variation reflects different dataset sizes. Token accuracy on train set increases primarily with epochs rather than total updates. Across all benchmarks, performance gains plateau once models approach full memorization, suggesting that token accuracy can serve as stopping criterion for epoch scaling. The initial token accuracy of the base model is marked with the vertical line. negatives than positives, reaching 40.0% versus 38.8% on AIME24 and 29.3% versus 23.4% on GPQA. One possible explanation is that negative trajectories come from harder problems where the teacher failed, and exposure to difficult reasoning attempts benefits the student even when the final answer is wrong. 4. Probing the Repetition Advantage Having demonstrated that the repetition advantage is robust across models, benchmarks, and training data sources, we now attempt to understand what drives this phenomenon. We return to the Olmo3-7B models trained on the Dolci dataset from Section 2 and examine several training dynamics, including memorization, termination behavior, and classical overfitting metrics, searching for signals that might explain why epoch scaling outperforms data scaling. While we identify correlates of improved performance, we do not find definitive causal mechanism. We present these observations as empirical characterizations that may guide future investigation into the underlying causes. 4.1. Memorization signals convergence. We first investigate training set memorization as potential indicator of convergence. During SFT, we measure token accuracy on fixed 200-sample training subset, computing the fraction of response tokens where the models top nexttoken prediction matches the target. Figure 4 plots this metric against downstream accuracy for Olmo3-7B. Token accuracy increases primarily with epoch count rather than total gradient updates: models trained for 16 epochs achieve near-perfect memorization regardless of whether they see 200 or 3,200 unique samples. Across all three benchmarks, performance improvements plateau once models approach full memorization. Table 4 shows this pattern across all three models, revealing that the smaller model memorizes faster and peaks at lower epoch counts, possibly due to higher optimal learning rate than larger models. This relationship suggests practical stopping criterion for epoch scaling: Saturation of token accuracy on training data marks convergence. 4.2. Termination correlates with performance. notable pattern in Figure 2 is the strong correlation between termination rate and accuracy. Single-epoch models terminate only 24% of generations, while 32-epoch models approach the rate of 89%. This correlation likely reflects causal relationship, where models that fail to terminate cannot produce final answer, directly limiting their measured accuracy. The increase in termination rate with epoch count suggests that: Repeated exposure helps models internalize not just the reasoning patterns but also the structural convention of concluding long reasoning chains. This behavioral convergence appears to require sufficient repetition, as even models trained on 51,200 unique samples fail to reliably terminate when trained for only one epoch. 4.3. Overfitting paradox. natural concern with multi-epoch training is overfitting. Figure 5 examines this for Olmo3-7B. As epochs increase, train loss approaches zero while validation loss rises substantially. We also measure prediction entropy on the validation set, defined as the average token-level entropy = (cid:80) pi log pi of the models output distribution. This metric decreases with epoch count, indicating the model Data Repetition Beats Data Scaling in Long-CoT Supervised Fine-Tuning Figure 5. Training dynamics for Olmo3-7B showing the relationship between loss, entropy, and downstream performance averaged over AIME24, AIME25, and GPQA. Points are colored by epoch count; within each group, variation reflects dataset size. As epochs increase, train loss approaches zero while validation loss rises, the classical signature of overfitting in terms of the train-validation gap. Prediction entropy also decreases, showing increased model confidence in predictions that diverge from the validation distribution. Despite these indicators, downstream accuracy improves with epoch count. Vertical lines mark base model metrics. grows more confident in predictions that diverge from the validation distribution. By standard metrics, the model is overfitting. Yet, downstream accuracy improves monotonically with epoch count, suggesting that: However: Epoch scaling leads to less catastrophic forgetting than data scaling. Validation loss on held-out SFT data is not reliable metric for reasoning performance. Combined with the large improvement in reasoning accuracy, epoch scaling offers strictly better tradeoff. One interpretation is that multi-epoch training elicits latent capabilities already present in the pretrained model, rather than teaching genuinely new skills. The model becomes confident in its own reasoning patterns, which differ from the validation trajectories but nonetheless transfer to heldout benchmarks. This view aligns with recent work on entropy minimization in fine-tuning (Agarwal et al., 2025) and suggests that SFT may function more as capability elicitation than capability acquisition. 4.4. Catastrophic Forgetting Beyond overfitting, multi-epoch training on small datasets risks catastrophic forgetting, where the model may lose general capabilities while specializing to the narrow training distribution. To evaluate this, we measure performance on MMLU (Hendrycks et al., 2021), broad knowledge benchmark spanning 57 subjects. Unlike our reasoning benchmarks, MMLU is evaluated by comparing the models probability assignments to answer choices rather than generating full responses. We use 5-shot prompting following the standard protocol. Figure 6 compares two training strategies matched by total gradient updates: scaling epochs on fixed 200-sample dataset versus scaling dataset size with single epoch. Both approaches cause some forgetting relative to the base model, as expected when fine-tuning on domain-specific data. 5. Related Work Data repetition and scaling laws in pretraining. Scaling laws for language model pretraining characterize how validation loss improves predictably with increased model size, total training tokens, and compute (Kaplan et al., 2020; Hoffmann et al., 2022). While these laws are agnostic to whether tokens are unique or repeated, they have commonly been interpreted as motivating the heuristic that, when available, additional fresh data is preferable to revisiting the same corpus. More directly, recent work studies pretraining in dataconstrained regimes where training necessarily becomes multi-epoch. Muennighoff et al. (2023) propose dataconstrained scaling laws that explicitly model the decreasing marginal value of repeated tokens, and empirically find that repeating fixed corpus for small number of epochs (on the order of few passes) can be nearly as effective for loss as training on equivalently-sized fresh tokens, while the returns from further repetition decay sharply. Relatedly, recent work on diffusion language models shows that, in data-constrained pretraining regimes, extensive data repetition can be beneficial, with diffusion objectives extracting substantially more value per unique token than autoregressive training (Ni et al., 2025). Our work contrasts with this pretraining-focused literature 7 Data Repetition Beats Data Scaling in Long-CoT Supervised Fine-Tuning Table 4. Relationship between training set memorization and average downstream performance at fixed update budget of = 51,200. Token accuracy measures the fraction of response tokens where the models top prediction matches the training target. Performance improves with epoch count until models reach full memorization, after which gains plateau or degrade. Model Epochs Acc@n Pass@n Token Acc. 7 - 3 O 8 - 3 Q 4 - 3 Q 1 2 4 8 16 32 64 128 1 2 4 8 16 32 64 128 1 2 4 8 16 32 64 17.2 22.6 27.6 36.5 38.5 38.8 38.9 38.4 15.0 18.9 25.4 33.4 37.6 36.2 34.4 30.4 8.4 16.2 21.8 29.1 25.7 22.9 18.7 12.4 40.1 47.5 54.1 63.8 68.5 73.7 69.0 71.5 35.2 44.1 54.4 62.8 70.9 61.6 62.1 61.4 28.8 37.4 43.1 50.3 50.7 49.7 40.3 37. 75.7 78.8 84.1 92.2 98.0 100.0 100.0 100.0 76.6 79.4 84.2 91.6 97.7 100.0 100.0 100.0 76.6 81.5 88.5 96.1 99.9 100.0 100.0 100.0 by showing that the avoid repetition heuristic does not transfer to supervised fine-tuning on long chain-of-thought data; on the contrary, repetition substantially improves convergence and downstream performance. Multi-epoch SFT in post-training practice. Although single-pass training is often treated as the default in instruction tuning, many recent training pipelines perform supervised fine-tuning for multiple epochs as part of post-training, often without isolating epoch count as studied variable. Examples include: 1) Olmo 3 reports training on SFT data, consisting of over 2M samples, for two epochs (Team OLMo, 2025). 2) DeepSeek-R1 similarly includes an SFT phase that fine-tunes its base model for 2-3 epochs on large curated set prior to reinforcement learning (Guo et al., 2025). 3) Llama-3 trains SFT for multiple epochs (at Meta AI, 2024). 4) LIMO trains for 15 epochs on curated reasoning set (Ye et al., 2025), while 5) Muennighoff et al. (2025) train an instruct model on long-CoT data for 5 epochs. Across these releases, epoch counts are typically presented as recipe details rather than as ablated design choices. Our work provides controlled, compute-matched comparison of epoch scaling versus unique-data scaling in long-CoT SFT, showing that multi-epoch training can be strictly better strategy Figure 6. Catastrophic forgetting under epoch scaling versus data scaling for Olmo3-7B. Multi-epoch training on 200 samples is compared against single-epoch training on increasingly large datasets, matched by total update steps. Both approaches exhibit forgetting as measured by MMLU accuracy, with epoch scaling causing less degradation. Combined with the large improvement in reasoning accuracy, measured on AIME24/25 and GPQA benchmarks, epoch scaling offers strictly better tradeoff. even when additional training tokens are available. Memorization, overfitting, and training dynamics. Classic results in deep learning challenge the view that memorization necessarily harms generalization. Arpit et al. (2017) show that deep networks tend to learn simple patterns before memorizing noise. For language modeling specifically, Tirumala et al. (2022) study exact memorization throughout training and characterize how memorization depends on model size, dataset size, and optimization choices. Complementing these empirical findings, Feldman (2019) provides theoretical perspective arguing that memorization can be necessary for generalization on long-tailed data distributions. We connect to this literature by showing that, in long-CoT supervised fine-tuning, downstream gains from repetition saturate when the model reaches near-perfect token-level accuracy on the training demonstrations. 6. Conclusion We show that supervised fine-tuning on long chain-ofthought data can defy standard machine learning intuition. Data Repetition Beats Data Scaling in Long-CoT Supervised Fine-Tuning Under fixed update budget, training for more epochs on smaller datasets substantially outperforms training on larger datasets, and this repetition advantage holds across models, benchmarks, and training data sources studied in this work. Despite its robustness, the mechanism underlying the repetition advantage remains poorly understood. While training token accuracy provides practical stopping signal for epoch scaling, the optimal dataset size is dataand model-dependent, and principled criteria for selecting it priori remain elusive. We argue that explaining why memorization under repetition improves generalization in reasoning SFT is an important open problem. More broadly, our results suggest that both epoch count and dataset size should be treated as first-class decision variables in reasoning SFT, rather than defaulting to single-epoch training on the largest available dataset."
        },
        {
            "title": "References",
            "content": "Agarwal, S., Zhang, Z., Yuan, L., Han, J., and Peng, H. The unreasonable effectiveness of entropy minimizaIn The Thirty-ninth Annual tion in LLM reasoning. Conference on Neural Information Processing Systems, 2025. URL https://openreview.net/forum? id=UfFTBEsLgI. AIME. Aime problems and solutions, 2025. URL https://artofproblemsolving.com/wiki/ index.php/AIME_Problems_and_Solutions. Arpit, D. et al. closer look at memorization in deep networks, 2017. URL https://arxiv.org/abs/ 1706.05394. at Meta AI, L. T. The llama 3 herd of models, 2024. URL https://arxiv.org/abs/2407.21783. Burns, C., Izmailov, P., Kirchner, J. H., Baker, B., Gao, L., Aschenbrenner, L., Chen, Y., Ecoffet, A., Joglekar, M., Leike, J., Sutskever, I., and Wu, J. Weak-to-strong generalization: eliciting strong capabilities with weak In Proceedings of the 41st International supervision. Conference on Machine Learning, ICML24. JMLR.org, 2024. Daniel Han, M. H. and team, U. Unsloth, 2023. URL http://github.com/unslothai/unsloth. Dettmers, T., Lewis, M., Shleifer, S., and Zettlemoyer, L. 8-bit optimizers via block-wise quantization. 9th International Conference on Learning Representations, ICLR, 2022. Guo, D., Yang, D., Zhang, H., Song, J., Wang, P., et al. Deepseek-r1 incentivizes reasoning in LLMs through reinforcement learning. Nature, 645:633638, 2025. doi: 10.1038/s41586-025-09422-z. Hendrycks, D. et al. Measuring massive multitask language understanding. In International Conference on Learning Representations (ICLR), 2021. URL https://arxiv. org/abs/2009.03300. Hernandez-Garcıa, A. and Konig, P. Data augmenCoRR, tation instead of explicit abs/1806.03852, 2018. URL http://arxiv.org/ abs/1806.03852. regularization. Hoffmann, J. et al. Training compute-optimal large language models, 2022. URL https://arxiv.org/ abs/2203.15556. Kaplan, J. et al. Scaling laws for neural language models, 2020. URL https://arxiv.org/abs/2001. 08361. Kwon, W., Li, Z., Zhuang, S., Sheng, Y., Zheng, L., Yu, C. H., Gonzalez, J. E., Zhang, H., and Stoica, I. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023. Li, J., Beeching, E., Tunstall, L., Lipkin, B., Soletskyi, R., Huang, S. C., Rasul, K., Yu, L., Jiang, A., Shen, Z., Qin, Z., Dong, B., Zhou, L., Fleureau, Y., Lample, G., and Polu, S. Numinamath tir. [https://huggingface.co/AI-MO/ NuminaMath-TIR](https://github.com/ project-numina/aimo-progress-prize/ blob/main/report/numina_dataset.pdf), 2024. Marek, M., Lotfi, S., Somasundaram, A., Wilson, A. G., and Goldblum, M. Small batch size training for language models: When vanilla SGD works, and why gradient accumulation is wasteful. In The Thirty-ninth Annual Conference on Neural Information Processing Systems, 2025. URL https://openreview.net/forum? id=52Ehpe0Lu5. Muennighoff, N., Yang, Z., Shi, W., Li, X. L., Fei-Fei, L., Hajishirzi, H., Zettlemoyer, L., Liang, P., Cand`es, E., and Hashimoto, T. s1: Simple test-time scaling, 2025. URL https://arxiv.org/abs/2501.19393. Muennighoff, N. et al. Scaling data-constrained language models, 2023. URL https://arxiv.org/abs/ 2305.16264. Feldman, V. Does learning require memorization? short tale about long tail, 2019. URL https://arxiv. org/abs/1906.05271. Ni, J., Liu, Q., Dou, L., Du, C., Wang, Z., Yan, H., Pang, T., and Shieh, M. Q. Diffusion language models are super data learners. arXiv preprint arXiv:2511.03276, 2025. 9 Data Repetition Beats Data Scaling in Long-CoT Supervised Fine-Tuning Osa, T., Pajarinen, J., Neumann, G., Bagnell, J. A., Abbeel, P., and Peters, J. An algorithmic perspective on imitation learning. In Foundations and Trends in Robotics, volume 7, pp. 1179. Now Publishers, 2018. Ouyang, L. et al. Training language models to follow instructions with human feedback, 2022. URL https: //arxiv.org/abs/2203.02155. Rein, D. et al. GPQA: graduate-level google-proof Q&A benchmark. In Proceedings of the First Conference on Language Modeling (COLM), 2024. URL https:// openreview.net/forum?id=Ti67584b98. Shao, Z., Wang, P., Zhu, Q., Xu, R., Song, J., Bi, X., Zhang, H., Zhang, M., Li, Y. K., Wu, Y., and Guo, D. Deepseekmath: Pushing the limits of mathematical reasoning in open language models, 2024. URL https://arxiv.org/abs/2402.03300. Shorten, C. and Khoshgoftaar, T. M. survey on Image Data Augmentation for Deep Learning. Journal of Big Data, 6(1):60, July 2019. ISSN 2196-1115. doi: 10.1186/ s40537-019-0197-0. Team OLMo. Olmo 3, 2025. URL https://arxiv. org/abs/2512.13961. Tirumala, K. et al. Memorization without overfitting: Analyzing the training dynamics of large language models, 2022. URL https://arxiv.org/abs/2205. 10770. Yang, A. et al. Qwen3 technical report, 2025. URL https: //arxiv.org/abs/2505.09388. Ye, Y., Huang, Z., Xiao, Y., Chern, E., Xia, S., and Liu, P. LIMO: Less is more for reasoning, 2025. URL https: //arxiv.org/abs/2502.03387. 10 Data Repetition Beats Data Scaling in Long-CoT Supervised Fine-Tuning A. Hyperparameters Hyperparameter GPUs Optimizer Weight Decay Beta1 Beta2 Grad. Norm Clipping Batch Size LR Scheduler Warmup Steps RNG Seed Value 1 H100 94GB per run 8-bit Adam 0.0 0.9 0.999 1.0 1 Cosine 10% of all updates 42 Olmo3 7B Qwen3 8B Qwen3 4B 2e-5 3e-5 2e-"
        },
        {
            "title": "Learning Rate",
            "content": "B. Full Results. B.1. Dolci Dataset Figures 79 show results on the Dolci dataset for three model backbones. Across all models, training for more epochs on smaller datasets consistently outperforms training on larger datasets for fewer epochs. Performance gains saturate once models approach full memorization, mirroring the convergence behavior discussed in Section 4.1. Figure 7. Dolci dataset results for Olmo3-7B. Scaling epochs on smaller datasets yields higher downstream accuracy than scaling the number of unique samples under fixed update budget. 11 Data Repetition Beats Data Scaling in Long-CoT Supervised Fine-Tuning Figure 8. Results for distillation from Qwen3-8B teacher. Stronger teachers increase overall performance but do not eliminate the repetition advantage. Figure 9. Dolci dataset results for Qwen3-8B. The repetition advantage persists across dataset sizes, with gains plateauing at higher epoch counts. 12 Data Repetition Beats Data Scaling in Long-CoT Supervised Fine-Tuning B.2. Qwen3 Distills Figures 10 and 11 examine the effect of teacher model size in distillation. While stronger teachers improve absolute performance, the repetition advantage remains robust: multi-epoch training on smaller distilled datasets consistently outperforms scaling unique samples. Figure 10. Results for distillation from Qwen3-0.6B teacher. Despite weaker teacher signals, repetition continues to improve downstream accuracy. 13 Data Repetition Beats Data Scaling in Long-CoT Supervised Fine-Tuning Figure 11. Results for distillation from Qwen3-8B teacher. Stronger teachers increase overall performance but do not eliminate the repetition advantage. B.3. Qwen3 8B Distill; Pos. vs Neg. Figures 12 and 13 separate distilled samples by correctness. The repetition advantage is substantially stronger when training on correct reasoning traces, while incorrect samples reduce overall performance and weaken the gains from repetition. 14 Data Repetition Beats Data Scaling in Long-CoT Supervised Fine-Tuning Figure 12. Results using only correct (positive) distilled samples from Qwen3-8B teacher. Repetition yields consistent gains until memorization saturates. Figure 13. Results using incorrect (negative) distilled samples from Qwen3-8B teacher. Overall performance is lower, and the repetition advantage is substantially diminished."
        }
    ],
    "affiliations": [
        "Mistral AI",
        "NVIDIA",
        "University of Technology Nuremberg"
    ]
}