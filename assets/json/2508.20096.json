{
    "paper_title": "CODA: Coordinating the Cerebrum and Cerebellum for a Dual-Brain Computer Use Agent with Decoupled Reinforcement Learning",
    "authors": [
        "Zeyi Sun",
        "Yuhang Cao",
        "Jianze Liang",
        "Qiushi Sun",
        "Ziyu Liu",
        "Zhixiong Zhang",
        "Yuhang Zang",
        "Xiaoyi Dong",
        "Kai Chen",
        "Dahua Lin",
        "Jiaqi Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Autonomous agents for Graphical User Interfaces (GUIs) face significant challenges in specialized domains such as scientific computing, where both long-horizon planning and precise execution are required. Existing approaches suffer from a trade-off: generalist agents excel at planning but perform poorly in execution, while specialized agents demonstrate the opposite weakness. Recent compositional frameworks attempt to bridge this gap by combining a planner and an actor, but they are typically static and non-trainable, which prevents adaptation from experience. This is a critical limitation given the scarcity of high-quality data in scientific domains. To address these limitations, we introduce CODA, a novel and trainable compositional framework that integrates a generalist planner (Cerebrum) with a specialist executor (Cerebellum), trained via a dedicated two-stage pipeline. In the first stage, Specialization, we apply a decoupled GRPO approach to train an expert planner for each scientific application individually, bootstrapping from a small set of task trajectories. In the second stage, Generalization, we aggregate all successful trajectories from the specialized experts to build a consolidated dataset, which is then used for supervised fine-tuning of the final planner. This equips CODA with both robust execution and cross-domain generalization. Evaluated on four challenging applications from the ScienceBoard benchmark, CODA significantly outperforms baselines and establishes a new state of the art among open-source models."
        },
        {
            "title": "Start",
            "content": "CODA: COORDINATING THE CEREBRUM AND CEREBELLUM FOR DUAL-BRAIN COMPUTER USE AGENT WITH DECOUPLED REINFORCEMENT LEARNING. Zeyi Sun1,2, Yuhang Cao2, Jianze Liang2, Qiushi Sun4, Ziyu Liu1,2 Zhixiong Zhang1,2 Yuhang Zang2, Xiaoyi Dong2,3, Kai Chen2, Dahua Lin2,3, Jiaqi Wang2 1Shanghai Jiao Tong University 3The Chinese University of Hong Kong 4The University of Hong Kong 2Shanghai AI Laboratory 5 2 0 2 7 ] . [ 1 6 9 0 0 2 . 8 0 5 2 : r a"
        },
        {
            "title": "ABSTRACT",
            "content": "Autonomous agents for Graphical User Interfaces (GUIs) face significant challenges in specialized domains like scientific computing, require both long-horizon planning and precise, fine-grained execution. Existing approaches suffer from trade-off: generalist agents excel at planning but falter in execution, while specialized agents show the opposite weakness. While recent compositional frameworks attempt to bridge this gap by combining planner and an actor, they are typically static and non-trainable, preventing adaptation from experiencea critical limitation given the scarcity of high-quality data in scientific domains. To address these limitations, we introduce CODA, novel and trainable compositional framework that synergizes generalist planner (Cerebrum) with specialist executor (Cerebellum), trained with dedicated two-stage training pipeline. The first stage, Specialization, employs decoupled GRPO approach to train an expert planner for each scientific application individually, bootstrapping from small set of initial task trajectories. The second stage, Generalization, aggregates all successful trajectories from all specialized experts. This consolidated, highquality dataset is then used to perform supervised fine-tuning (SFT) on the final planner, equipping it with the robust, cross-domain capabilities of generalist. Evaluated on four challenging applications from the ScienceBoard benchmark, our framework significantly outperforms the baseline and establishes new stateof-the-art (SOTA) among open-source models. Our models and code are available at https://github.com/OpenIXCLab/CODA."
        },
        {
            "title": "INTRODUCTION",
            "content": "(Anthropic, 2024; OpenAI, 2025; Qin Autonomous agents for Graphical User Interfaces (GUIs) et al., 2025; Lin et al., 2024; Wu et al., 2024b; Hong et al., 2023) promise to automate wide range of digital tasks (Zhou et al., 2023; Xie et al., 2024). However, their application in specialized domains such as scientific computing and engineering analysis remains highly challenging (Sun et al., 2025a). These environments pose two primary difficulties: first, their interfaces are highly complex, requiring precise and fine-grained actions; second, the problems they address are intrinsically complicated, demanding long-horizon planning to achieve effective solutions. Effective agency for computer task automation in these domains requires both high-level planning and low-level execution as well as domain knowledge. However, current models exhibit clear trade-off. Generalist models like Qwen2.5-VL (Bai et al., 2025) provide robust planning capabilities but often struggle with the precise grounding needed for reliable execution. Conversely, specialized agents (Wu et al., 2024b; 2025; Xie et al., 2025) like UI-Tars (Qin et al., 2025) are highly proficient in execution, yet their capacity for complex, high-level planning is more constrained. To bridge this gap, natural approach has been to develop compositional frameworks that explicitly decouple planning from execution, effectively pairing generalist cerebrum with specialist Corresponding Authors. Equal contribution"
        },
        {
            "title": "Technical Report",
            "content": "Figure 1: Overall architecture of the proposed learnable PlannerExecutor framework. Analogous to the relationship between the cerebrum and the cerebellum in the human brain, the Planner (cerebrum) generates high-level thoughts based on the history and screenshots, while the Executor (cerebellum) executes concrete GUI actions accordingly. cerebellum (Agashe et al., 2024; 2025). While promising, these pioneering approaches are fundamentally limited. They are typically static and non-trainable, relying on powerful, often closedsource models as their core planner. This design introduces significant drawbacks: it compromises transparency and replicability, and most critically, prevents the agent from learning and adapting through experience. This architectural decoupling is not merely an engineering convenience but is inspired by the functional architecture of the human brain (illustrated in Fig.1). The specialization of high-level planning (the Cerebrum) and low-level motor control (the Cerebellum) is key aspect of human intelligence. Crucially, these structures exhibit different learning patterns: the Cerebellum, once mature, provides stable and broadly applicable motor skills that require infrequent updates Ito (2000). In contrast, the Cerebrum continuously adapts its strategies based on the nuances of new tasks and environments Demarin & Morovic (2014); Hallett (2005). This biological parallel motivates our core hypothesis: an effective agent should pair stable, proficient grounding model with dynamic planner that is specialized for different software domains through targeted, experience-driven learning. To realize this vision, we propose trainable compositional framework that integrates Qwen2.5VL Bai et al. (2025) as the planner (cerebrum) and UI-Tars-1.5 Qin et al. (2025) as the executor (cerebellum). Unlike prompting-based systems that rely on proprietary closed-source planners, our framework makes the planner itself learnable through interaction with software environments mediated by static executor. Concretely, the executor provides stable, software-agnostic grounding for low-level GUI actions, while the planner, by leveraging this reliable interface, can gradually acquire domain-specific knowledge and improve its high-level planning strategies. In contrast to end-to-end training of full agent, which requires massive amounts of specialized data and costly retraining of both perception and execution modules, our decoupled approach is substantially more data efficient: only the planner is optimized for domain adaptation, while the executor remains fixed as generalpurpose grounder that already possesses strong generalization ability after massive pretraining for grounding purposes. This design reduces reliance on curated trajectories, lowers training cost, and ensures controllable adaptation. To train the planner effectively under this cerebrumcerebellum separation, we avoid the need for costly human-labeled trajectories. Instead, we leverage judging system built from open-source models to automatically provide dense reward signals, combined with autonomous interaction with scientific software environments through the static executor. This setup enables the planner to gradually acquire domain-specific planning ability with zero human effort. Furthermore, by distributing the interaction process across multiple software environments in parallelcoordinated by central masterwe can significantly accelerate reinforcement learning. This strategy not only makes the training process more efficient but also echoes our brain-inspired design: the cerebellum-like executor delivers stable grounding, while the cerebrum-like planner continually adapts through experience."
        },
        {
            "title": "Technical Report",
            "content": "We validate our framework on four typical scientific software applications from the ScienceBoard benchmark (Sun et al., 2025a). Experiments show that our method not only significantly improves the baseline performance (Cerebrum: Qwen2.5-32B-VL, Cerebellum: UI-Tars-1.5) but also establishes new state-of-the-art (SOTA) among open-source models, confirming its effectiveness."
        },
        {
            "title": "2 RELATED WORKS",
            "content": "Reinforcement Learning for LVLMs. Training for LLMs and LVLMs (Touvron et al., 2023; Grattafiori et al., 2024; Liu et al., 2023a; Bai et al., 2025; Wang et al., 2024; Xing et al., 2025; Sun et al., 2024d;c; Ding et al., 2025) has progressed from data-intensive Supervised Fine-Tuning (SFT) (Liu et al., 2023a; Wei et al., 2022) towards Reinforcement Learning (RL). Algorithms like Group Relative Policy Optimization (GRPO) (Guo et al., 2025; Shao et al., 2024) have proven effective for reasoning tasks, moving beyond earlier single-turn RLHF applications (Ouyang et al., 2022; Ziegler et al., 2019; Rafailov et al., 2023). However, applying RL to complex agentic tasks (Bai et al., 2024; Qi et al., 2024; Zhou et al., 2024; Zhai et al., 2024; Carta et al., 2023) is challenging. Prevailing methods train monolithic agents end-to-end, often requiring co-trained critic models (Schulman et al., 2015) or preference-based optimization like DPO (Rafailov et al., 2023; Putta et al., 2024; Qin et al., 2025), which problematically entangles the distinct skills of planning and execution. In contrast, our work employs decoupled reinforcement learning strategy: the high-level planner is optimized via environmental interaction while the execution model remains fixed. We adapt GRPO by computing rewards from the final action and backpropagating the advantage exclusively through planning tokens. This targeted optimization stably enhances strategic planning, distinguishing our method from prior works that train dedicated critic models (Bai et al., 2024; Qi et al., 2024) or use filtered behavior cloning (Pan et al., 2024; Chen et al., 2020). Computer Use Agent. Fueled by advancements in Large Vision-Language Models (LVLMs) (Touvron et al., 2023; Grattafiori et al., 2024; Liu et al., 2023a; Bai et al., 2025; Wang et al., 2024), new generation of agents capable of operating computers via multi-modal inputs is emerging (Hu et al., 2024b; Hong et al., 2024; Cheng et al., 2024; Nguyen et al., 2024; Lin et al., 2024; Sun et al., 2024b). Whether processing structured text and code (Qi et al., 2024; Putta et al., 2024; Lai et al., 2024; Sun et al., 2024a; Nakano et al., 2021) or screenshots (Hong et al., 2023; Lin et al., 2024; Wu et al., 2024b; OpenAI, 2025), these agents face an inherent dichotomy analogous to human cognition: the tension between high-level strategic planning and precise, low-level action execution. This has motivated the development of compositional frameworks that decouple these responsibilities (Agashe et al., 2024; 2025; Liu et al., 2023b; Zhang et al., 2025; Song et al., 2025). However, significant portion of this research relies on static, non-trainable systems that orchestrate powerful, often proprietary models (Anthropic, 2024; OpenAI, 2025; Google DeepMind, 2025; Yan et al., 2023; He et al., 2024; Zhang et al., 2024; Wang et al., 2023; Wu et al., 2024a) as their core planner. This design fundamentally prevents the agent from adapting through experiencea critical flaw for mastering novel software where interaction data is scarce. Our work charts different course by exploring reinforcement fine-tuning of the planner. By enabling the planner to learn specialized domain knowledge through direct software interaction via fixed execution model, our strategy achieves robust performance on unfamiliar applications."
        },
        {
            "title": "3 METHOD",
            "content": "3.1 PROBLEM FORMULATION We formally define the task of autonomous GUI operation for software workflows as Partially Observable Markov Decision Process (POMDP). Each task is initiated with natural language instruction from the task space G. At each timestep t, the agent perceives the latent environment state st through visual observation ot Ω, consisting of screenshot of the user interface. The agents behavior is governed by policy π, instantiated by large vision-language model, which synthesizes an action program at A. The action space consists of precisely parameterized pyautogui scripts, where precision in arguments (e.g., coordinates) is critical for execution. The policy generates this action based on the initial instruction and the history of interactions: at = π(g, (o1, a1, . . . , at1, ot))"
        },
        {
            "title": "Technical Report",
            "content": "Figure 2: Overall training process of the proposed PlannerExecutor framework. The Planner generates high-level thoughts based on the history and screenshots, while the Executor executes concrete GUI actions accordingly. During training, the rewards are calculated from a(i) and applied to p(i) to calculate loss. This sequential process induces state trajectory τ = (s0, s1, . . . , sT ) with the maximum time step . task is considered successful if the final state sT satisfies the predefined goal condition specified in G. 3.2 MODEL ARCHITECTURE To address the inherent trade-off in monolithic models, which struggle to balance long-horizon planning with precise action grounding, we propose composite agent architecture that structures the decision-making process into Planner-Executor framework. This design decouples the task into two distinct yet collaborative modules: high-level Planner responsible for strategic thinking and low-level Executor for concrete action execution. Planner The Planner is instantiated from the Qwen2.5-VL (Bai et al., 2025) model. Its primary responsibility is to analyze the tasks progress and formulate high-level, explicit plan pt for each step. Specifically, at each timestep t, the Planner receives the interaction history up to the previous step mt1 = (p1, a1, . . . , pt1, at1), the current visual observation ot, and the preceding observation ot1. The output is structured thought, denoted as pt, which outlines the immediate objective and explicitly identifies the target UI elements for interaction. The process can be summarized as: pt = Planner(mt1, ot1, ot) Executor The Executor employs UI-TARS-1.5 (Qin et al., 2025) model. Its role is to translate the Planners abstract thought pt into precise, executable action. The Executor is provided with the same historical and visual context as the Planner (mt1, ot1, and ot), but is critically augmented with the Planners newly generated thought pt. Its output is low-level GUI action at in the form of pyautogui command, such as click(x, y). The Executors operation is defined as: at = Executor(mt1, ot1, ot, pt) 3.3 TRAINING PIPELINE Our training methodology employs two-stage curriculum designed for initial specialization followed by broad generalization. 3.3.1 STAGE 1: SPECIALIZATION VIA DECOUPLED REINFORCEMENT LEARNING The primary objective of this initial training stage is to enhance the agents specialized performance on individual software applications. Through empirical analysis, we observed that the Executor exhibits strong generalization capabilities, accurately translating well-structured plans into executable actions. However, the Planner module emerged as the primary bottleneck, often struggling to formulate effective high-level strategies. To address this, we adopt decoupled training strategy that focuses reinforcement learning exclusively on the Planner (πθ = Planner). This targeted approach allows us to refine the agents strategic reasoning without altering the already competent Executor."
        },
        {
            "title": "Technical Report",
            "content": "Since the initial Planner is relatively weak and generates limited number of successful trajectories, standard reinforcement learning methods can be inefficient. Therefore, we adapt the Group Relative Policy Optimization (GRPO) framework (Guo et al., 2025; Shao et al., 2024), which is particularly effective in such scenarios. GRPO can derive meaningful learning signal by comparing the relative quality of different outputs, even when most of them are suboptimal. The training process for given task unfolds as follows. Given the current state and interaction history, the Planner first generates group of candidate plans. Subsequently, the fixed Executor takes each plan as input and produces corresponding low-level action. To generate fine-grained learning signal, we compute reward for each plan by comparing its resulting action a(i) to the labeled positive action aT (details of labeling process are in Sec.3.4 ). Our composite reward function assesses both the correctness of the action type and the precision of its parameters: r(i) = r(a(i), aT ) = (cid:16) (cid:17) type(a(i)) = type(aT ) + rdist(a(i), aT ), (1) Here, the indicator function I() provides binary reward for selecting the correct type of action (e.g., click vs. type). The term rdist(a(i), aT ) offers continuous reward based on the parametric similarity between the predicted and ground-truth actions, such as L1 distance for coordinates or IoU for bounding boxes. These distance-based rewards are normalized to [0, 1] to ensure consistent scaling. Once the rewards are calculated, they are used to derive relative advantage A(i) for each plan, which is then fed into the GRPO loss function to update the Planner policy: A(i) = r(i) mean({r(j)}G std({r(j)}G j=1) j=1) , = 1, , G. The GRPO loss is formulated as follows: LGRPO(πθ) = (s,I)D,{a(i)}G i=1πref(s,I) (cid:34) 1 (cid:88) i=1 1 p(i) p(i) (cid:88) (cid:110) t=1 min (cid:16) (θ)A(i), clip(r(i) r(i) (θ), 1 ϵ, 1 + ϵ)A(i)(cid:17) β D(i,t) KL (πθπref) (cid:35) (cid:111) (2) (3) , where ri,t(θ) = πθ(p(i)s, I) πθref(p(i)s, I) and Di,t KL(πθ, πref) = πref(p(i)s, I) πθ(p(i)s, I) 1 log πref(p(i)s, I) πθ(p(i)s, I) . Consistent with the approach in (Shao et al., 2024; Guo et al., 2025), this advantage is applied across all reasoning tokens in the plan p(i), encouraging the model to develop more robust and free-form planning capabilities. 3.3.2 STAGE 2: GENERALIZATION VIA AGGREGATED SUPERVISED FINE-TUNING We adopt the specialist-to-generalist paradigm proposed in Sun et al. (2025b), where generalist model is trained by leveraging multiple specialist models as teachers. We observe that directly applying reinforcement learning across all software leads to suboptimal performance. To address this, we first train four specialist models using the methods described in Sec. 3.3.1. These specialists are then employed to generate new trajectories for each software, which serve as supervision for training generalist model. After learning from the four software-specific teachers, the resulting generalist not only surpasses its teachers in performance, but also demonstrates stronger reasoning and reflection abilities during planning, as well as broader domain knowledge across different software."
        },
        {
            "title": "3.4 AUTO EXPLORATION PIPELINE.",
            "content": "Auto Task Generation. We employ Qwen2.5-72B (Wang et al., 2024) as the task generator to produce high-level tasks. Specifically, small set of real human-instructed tasks on each software is provided as input, together with the prompt shown in Fig. 5. The agent then repeatedly executes these tasks to collect diverse set of interaction trajectories, which are subsequently filtered by judge system to retain only trajectories with positive actions for training. Judge System for Providing Reward Signals. Our judge system labels the positive actions aT within an agents trajectory when performing task. Given full trajectory = {o0, a0, . . . , ofinal}, the judge takes the complete sequence of screenshot observations (o1, o2, . . . , on) as input and outputs three signals: Correctness, Redundant, and First Error Step, using the detailed prompt shown in Fig. 6. trajectory is considered clean and successful when Correctness is True and both Redundant and First Error Step are empty. In this case, all actions in the trajectory are labeled as aT . We present detailed evaluation of the judges precision and discuss approaches for improving it in Sec. 4.2. Distributed Virtual Machine System. Task execution is the most time-consuming step in our pipeline, so we developed lightweight distributed system to accelerate large-scale trajectory curation. As illustrated in Fig. 3b, the system follows an HTTP-based masterclient architecture: the master node manages dynamic task queue, monitors execution progress, and aggregates results, while multiple client nodes execute tasks in parallel within isolated virtual machine environments. This design enables efficient scaling to hundreds of concurrent environments, substantially reducing the time required to collect successful trajectories and making the framework well-suited for large-scale training and evaluation. (a) Specialist-to-Generalist strategy. (b) Distributed VM System. Figure 3: Exploration pipeline for training support."
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "4.1 AGENT PERFORMANCE EVALUATION. Our planner-executor approach is based on Qwen2.5VL-32B (Bai et al., 2025) serve as planner and UI-TARS-1.5-7B (Qin et al., 2025) serve as executor. We use method proposed in Sec.3.4 to generate high level tasks for each software from ScienceBoard (Sun et al., 2025a). through decoupled reinforcement learning proposed in Sec.3.3.1. During Training, the reward signal is provided by our judge system evaluated in Sec.4.2. Our training is based on OpenRLHF Hu et al. (2024a). As reported in Tab.1, our evaluation is done on four GUI centric software from ScienceBoard Sun et al. (2025a). We also report other planner-executor decoupled approaches. This first-stage reinforcement learning approach lead to significant performance gain compared to baseline. In second stage, we use these specialist planner serve as teachers to teach generalist planner. This new model is also initialized from Qwen2.5VL-32B and perform supervised fine-tuning on 0.77K trajectories from teacher models labeled by our judge system. As shown in tab.1, this new model surpass the performance of the ensemble of individual specialist, showing improved reasoning and planning abilities. This result demonstrates the effectiveness of our specialist-to-generalist strategy."
        },
        {
            "title": "Technical Report",
            "content": "Figure 4: Case studies. Qwen2.5-VL-72B Bai et al. (2025) struggles with precise grounding, whereas UI-TARS-1.5 Qin et al. (2025), though specialized, fails to generalize to out-of-distribution software. 4.2 TOWARDS PRECISE JUDGING SYSTEM Our reinforcement learning framework heavily relies on accurate judgments of agent trajectories to provide reliable reward signals. In this section, we present detailed evaluation of our judge model, which demonstrates improved precision in decision making. Settings. We conduct experiments on two sources of trajectories. (1) AgentRewardBench (L`u et al., 2025), benchmark designed specifically for judge evaluation. (2) trajectory dataset we collected from ScienceBoard (Sun et al., 2025a). We run Qwen2.5-VL-72B (Bai et al., 2025) on ScienceBoard tasks and extract 377 labeled trajectories, which are then used as inputs to our judge model. This setup allows us to quantitatively assess the judges ability to discriminate between successful and failed executions. We report Precision and Recall as our primary metrics. For voting-"
        },
        {
            "title": "Technical Report",
            "content": "Metrics Model Success Rate () Algebra Biochem GIS Astron Overall Average@1 GPT-4o (OpenAI, 2023) Claude-3.7-Sonnet (Anthropic, 2025) Gemini-2.0-Flash (Team et al., 2023) GPT4oUGround-V1-7B (Gou et al., 2024) GPT4oOS-Atlas-Pro-7B (Wu et al., 2024b) GPT4oUI-TARS-72B (Qin et al., 2025) Qwen2.5-VL-72B (Bai et al., 2025) InternVL3-78B (Zhu et al., 2025) UI-TARS-1.5-7B (Qin et al., 2025) 3.23% 9.67% 6.45% 0.00% 6.25% 3.23% 22.58% 6.45% 12.90% 0.00% 37.93% 3.45% 3.45% 10.34% 10.34% 27.59% 3.45% 13.79% 0.00% 2.94% 2.94% 0.00% 0.0% 5.88% 5.88% 0.00% 0.00% 0.00% 0.81% 6.06% 14.15% 4.73% 6.06% 1.62% 3.03% 4.92% 3.03% 6.06% 6.38% 9.09% 12.94% 2.69% 0.00% 8.19% 6.06% Average@8 Pass@8 Qwen2.5-VL-32B (Bai et al., 2025) UI-TARS-1.5-7B (Qin et al., 2025) CODA (Stage-1)* CODA (Stage-2) Qwen2.5-VL-32B (Bai et al., 2025) UI-TARS-1.5-7B (Qin et al., 2025) CODA (Stage-1)* CODA (Stage-2) 7.57% 4.55% 1.47% 10.48% 3.03% 5.14% 0.80% 6.49% 13.71% 9.85% 14.39% 7.72% 20.16% 32.23% 14.71% 17.05% 21.04% 13.79% 10.24% 26.29% 8.82% 9.09% 19.49% 31.03% 29.03% 24.14% 5.88% 12.12% 15.36% 19.35% 41.94% 44.83% 23.53% 18.18% 32.12% 48.39% 51.72% 29.41% 30.30% 39.96% Table 1: Success rates of various models on ScienceBoard (Sun et al., 2025a). Proprietary models and open-sourced models based methods are highlighted with purple and green backgrounds, respectively. *Indicates specialist agents trained separately for each software with ensembled results. Table 2: Evaluation of different judge methods on AgentRewardBench (L`u et al., 2025) and ScienceBoard (Sun et al., 2025a). Method AgentRewardBench (L`u et al., 2025) ScienceBoard (Sun et al., 2025a) Precision Recall Precision Recall Qwen2.5-VL-72B-single 72B-GUI-Judge 72B-voting@4 72B-voting@4 w/ multi-res 72B-voting@4 Ensemble 64.5 73.5 76.1 78.9 81.2 83.4 79.0 79.5 77.4 76.8 41.5 43.7 58.6 65.7 69.5 80.1 80.1 75.3 77.9 74.2 based strategies, we adopt sampling temperature of = 1.0 and nucleus sampling probability of top = 0.6 over 4 independent inference runs. Results. As summarized in Table 2, our evaluations reveal three effective strategies for improving precision, building upon difference description fine-tuning (Sun et al., 2025b): 1. Voting. Instead of single query, we prompt the model multiple times with high randomness (T = 1.0, top = 0.6). trajectory is only deemed successful if all votes agree, which significantly reduces false positives. 2. Multi-resolution inputs. Trajectories often include long sequences of high-resolution screenshots. We observe that using mixture of resolutions across voting rounds is beneficial: low-resolution images help capture global execution dynamics, while high-resolution images aid in detecting finegrained correctness. In practice, we first apply low-resolution inputs to quickly filter out failures, thereby improving both precision and efficiency. 3. Model ensembling. In addition to the fine-tuned judge model (see Sup. A), we find that ensembling two models within the voting strategy further enhances precision. Across both ScienceBoard Sun et al. (2025a) and AgentRewardBench L`u et al. (2025), we observe consistent progression: the fine-tuned model (72B-GUI-Judge) primarily improves recall, while voting substantially increases precision; multi-resolution inputs add further gains, and ensembling achieves the best balance with the highest precision while maintaining competitive recall. This consistent trend across benchmarks highlights the robustness and generality of our proposed strategies. With methods proposed in . This judge system provide high quality reward signal for the planner to perform RL to improve reasoning ability and learning software domain knowledge."
        },
        {
            "title": "5 CONCLUSION",
            "content": "We presented trainable PlannerExecutor disentangled framework for GUI agents, inspired by the division of labor between the cerebrum and cerebellum. By coupling fixed executor (UI-Tars-1.5) with fine-tunable planner (Qwen2.5-VL), and supporting it with robust judging system, GRPObased exploration, and distributed data generation pipeline, our approach effectively addresses the challenges of complex interfaces and long-horizon planning. Experiments on ScienceBoard applications demonstrate substantial improvements over strong baselines, establishing new opensource state-of-the-art. These results highlight the importance of combining stable execution with adaptive planning, and open promising directions for extending our framework to richer multi-modal feedback, broader professional domains, and continual learning for long-term adaptability."
        },
        {
            "title": "REFERENCES",
            "content": "Saaket Agashe, Jiuzhou Han, Shuyu Gan, Jiachen Yang, Ang Li, and Xin Eric Wang. Agent s: An open agentic framework that uses computers like human. arXiv preprint arXiv:2410.08164, 2024. Saaket Agashe, Kyle Wong, Vincent Tu, Jiachen Yang, Ang Li, and Xin Eric Wang. Agent s2: compositional generalist-specialist framework for computer use agents. arXiv preprint arXiv:2504.00906, 2025. Anthropic. Claude computer use. 2024. URL https://www.anthropic.com/news/ 3-5-models-and-computer-use. Anthropic. Claudes extended thinking. 2025. URL https://www.anthropic.com/ research/visible-extended-thinking. Hao Bai, Yifei Zhou, Jiayi Pan, Mert Cemri, Alane Suhr, Sergey Levine, and Aviral Kumar. Digirl: Training in-the-wild device-control agents with autonomous reinforcement learning. Advances in Neural Information Processing Systems, 37:1246112495, 2024. Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. Thomas Carta, Clement Romac, Thomas Wolf, Sylvain Lamprier, Olivier Sigaud, and Pierre-Yves Oudeyer. Grounding large language models in interactive environments with online reinforcement learning. In International Conference on Machine Learning, pp. 36763713. PMLR, 2023. Xinyue Chen, Zijian Zhou, Zheng Wang, Che Wang, Yanqiu Wu, and Keith Ross. Bail: Bestaction imitation learning for batch deep reinforcement learning. Advances in Neural Information Processing Systems, 33:1835318363, 2020. Kanzhi Cheng, Qiushi Sun, Yougang Chu, Fangzhi Xu, Yantao Li, Jianbing Zhang, and Zhiyong Wu. Seeclick: Harnessing gui grounding for advanced visual gui agents. arXiv preprint arXiv:2401.10935, 2024. Vida Demarin and Sandra Morovic. Neuroplasticity. Periodicum biologorum, 116(2):209211, 2014. Shengyuan Ding, Shenxi Wu, Xiangyu Zhao, Yuhang Zang, Haodong Duan, Xiaoyi Dong, Pan Zhang, Yuhang Cao, Dahua Lin, and Jiaqi Wang. Mm-ifengine: Towards multimodal instruction following. arXiv preprint arXiv:2504.07957, 2025. Google DeepMind. Gemini 2.5 Pro Preview (03-25). https://deepmind.google/ technologies/gemini, 2025. Boyu Gou, Ruohan Wang, Boyuan Zheng, Yanan Xie, Cheng Chang, Yiheng Shu, Huan Sun, and Yu Su. Navigating the digital world as humans do: Universal visual grounding for gui agents. arXiv preprint arXiv:2410.05243, 2024."
        },
        {
            "title": "Technical Report",
            "content": "Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Mark Hallett. Neuroplasticity and rehabilitation. Journal of rehabilitation research and development, 42(4):R17, 2005. Hongliang He, Wenlin Yao, Kaixin Ma, Wenhao Yu, Yong Dai, Hongming Zhang, Zhenzhong Lan, and Dong Yu. Webvoyager: Building an end-to-end web agent with large multimodal models. arXiv preprint arXiv:2401.13919, 2024. Wenyi Hong, Weihan Wang, Qingsong Lv, Jiazheng Xu, Wenmeng Yu, Junhui Ji, Yan Wang, Zihan Wang, Yuxuan Zhang, Juanzi Li, Bin Xu, Yuxiao Dong, Ming Ding, and Jie Tang. Cogagent: visual language model for GUI agents. CoRR, abs/2312.08914, 2023. doi: 10.48550/ARXIV. 2312.08914. URL https://doi.org/10.48550/arXiv.2312.08914. Wenyi Hong, Weihan Wang, Qingsong Lv, Jiazheng Xu, Wenmeng Yu, Junhui Ji, Yan Wang, Zihan Wang, Yuxiao Dong, Ming Ding, et al. Cogagent: visual language model for gui agents. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1428114290, 2024. Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. ICLR, 1(2):3, 2022. Jian Hu, Xibin Wu, Zilin Zhu, Xianyu, Weixun Wang, Dehao Zhang, and Yu Cao. Openrlhf: An easy-to-use, scalable and high-performance rlhf framework. arXiv preprint arXiv:2405.11143, 2024a. Xueyu Hu, Tao Xiong, Biao Yi, Zishu Wei, Ruixuan Xiao, Yurun Chen, Jiasheng Ye, Meiling Tao, Xiangxin Zhou, Ziyu Zhao, et al. Os agents: survey on mllm-based agents for general computing devices use, 2024b. Masao Ito. Mechanisms of motor learning in the cerebellum. Brain research, 886(1-2):237245, 2000. Hanyu Lai, Xiao Liu, Iat Long Iong, Shuntian Yao, Yuxuan Chen, Pengbo Shen, Hao Yu, Hanchen Zhang, Xiaohan Zhang, Yuxiao Dong, et al. Autowebglm: large language model-based web navigating agent. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pp. 52955306, 2024. Kevin Qinghong Lin, Linjie Li, Difei Gao, Zhengyuan Yang, Shiwei Wu, Zechen Bai, Weixian Lei, Lijuan Wang, and Mike Zheng Shou. Showui: One vision-language-action model for gui visual agent. arXiv preprint arXiv:2411.17465, 2024. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36:3489234916, 2023a. Zhiwei Liu, Weiran Yao, Jianguo Zhang, Le Xue, Shelby Heinecke, Rithesh Murthy, Yihao Feng, Zeyuan Chen, Juan Carlos Niebles, Devansh Arpit, et al. Bolaa: Benchmarking and orchestrating llm-augmented autonomous agents. arXiv preprint arXiv:2308.05960, 2023b. Xing Han L`u, Amirhossein Kazemnejad, Nicholas Meade, Arkil Patel, Dongchan Shin, Alejandra Zambrano, Karolina Stanczak, Peter Shaw, Christopher Pal, and Siva Reddy. AgenarXiv preprint trewardbench: Evaluating automatic evaluations of web agent trajectories. arXiv:2504.08942, 2025. Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al. Webgpt: Browser-assisted question-answering with human feedback. arXiv preprint arXiv:2112.09332, 2021."
        },
        {
            "title": "Technical Report",
            "content": "Dang Nguyen, Jian Chen, Yu Wang, Gang Wu, Namyong Park, Zhengmian Hu, Hanjia Lyu, Junda Wu, Ryan Aponte, Yu Xia, et al. Gui agents: survey. arXiv preprint arXiv:2412.13501, 2024. OpenAI. GPT-4 technical report. CoRR, abs/2303.08774, 2023. doi: 10.48550/ARXIV.2303.08774. URL https://doi.org/10.48550/arXiv.2303.08774. OpenAI. Operator. 2025. URL https://openai.com/research/operator. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35: 2773027744, 2022. Jiayi Pan, Yichi Zhang, Nicholas Tomlin, Yifei Zhou, Sergey Levine, and Alane Suhr. Autonomous evaluation and refinement of digital agents. arXiv preprint arXiv:2404.06474, 2024. Pranav Putta, Edmund Mills, Naman Garg, Sumeet Motwani, Chelsea Finn, Divyansh Garg, and Rafael Rafailov. Agent q: Advanced reasoning and learning for autonomous ai agents. arXiv preprint arXiv:2408.07199, 2024. Zehan Qi, Xiao Liu, Iat Long Iong, Hanyu Lai, Xueqiao Sun, Wenyi Zhao, Yu Yang, Xinyue Yang, Jiadai Sun, Shuntian Yao, et al. Webrl: Training llm web agents via self-evolving online curriculum reinforcement learning. arXiv preprint arXiv:2411.02337, 2024. Yujia Qin, Yining Ye, Junjie Fang, Haoming Wang, Shihao Liang, Shizuo Tian, Junda Zhang, Jiahao Li, Yunxin Li, Shijue Huang, Wanjun Zhong, Kuanye Li, Jiale Yang, Yu Miao, Woyu Lin, Longxiang Liu, Xu Jiang, Qianli Ma, Jingyu Li, Xiaojun Xiao, Kai Cai, Chuang Li, Yaowei Zheng, Chaolin Jin, Chen Li, Xiao Zhou, Minchao Wang, Haoli Chen, Zhaojian Li, Haihua Yang, Haifeng Liu, Feng Lin, Tao Peng, Xin Liu, and Guang Shi. UI-TARS: pioneering automated GUI interaction with native agents. CoRR, abs/2501.12326, 2025. doi: 10.48550/ARXIV.2501.12326. URL https://doi.org/10.48550/arXiv.2501.12326. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. Advances in Neural Information Processing Systems, 36:5372853741, 2023. John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. HigharXiv preprint dimensional continuous control using generalized advantage estimation. arXiv:1506.02438, 2015. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Linxin Song, Yutong Dai, Viraj Prabhu, Jieyu Zhang, Taiwei Shi, Li Li, Junnan Li, Silvio Savarese, Zeyuan Chen, Jieyu Zhao, et al. Coact-1: Computer-using agents with coding as actions. arXiv preprint arXiv:2508.03923, 2025. Qiushi Sun, Zhirui Chen, Fangzhi Xu, Kanzhi Cheng, Chang Ma, Zhangyue Yin, Jianing Wang, Chengcheng Han, Renyu Zhu, Shuai Yuan, et al. survey of neural code intelligence: Paradigms, advances and beyond. arXiv preprint arXiv:2403.14734, 2024a. Qiushi Sun, Kanzhi Cheng, Zichen Ding, Chuanyang Jin, Yian Wang, Fangzhi Xu, Zhenyu Wu, Chengyou Jia, Liheng Chen, Zhoumianze Liu, et al. Os-genesis: Automating gui agent trajectory construction via reverse task synthesis. arXiv preprint arXiv:2412.19723, 2024b. Qiushi Sun, Zhoumianze Liu, Chang Ma, Zichen Ding, Fangzhi Xu, Zhangyue Yin, Haiteng Zhao, Zhenyu Wu, Kanzhi Cheng, Zhaoyang Liu, et al. Scienceboard: Evaluating multimodal autonomous agents in realistic scientific workflows. arXiv preprint arXiv:2505.19897, 2025a. Zeyi Sun, Ziyang Chu, Pan Zhang, Tong Wu, Xiaoyi Dong, Yuhang Zang, Yuanjun Xiong, Dahua Lin, and Jiaqi Wang. X-prompt: Towards universal in-context image generation in autoregressive vision language foundation models, 2024c. URL https://arxiv.org/abs/ 2412.01824."
        },
        {
            "title": "Technical Report",
            "content": "Zeyi Sun, Tong Wu, Pan Zhang, Yuhang Zang, Xiaoyi Dong, Yuanjun Xiong, Dahua Lin, and Jiaqi Wang. Bootstrap3d: Improving 3d content creation with synthetic data. arXiv e-prints, pp. arXiv 2406, 2024d. Zeyi Sun, Ziyu Liu, Yuhang Zang, Yuhang Cao, Xiaoyi Dong, Tong Wu, Dahua Lin, and Jiaqi Wang. Seagent: Self-evolving computer use agent with autonomous learning from experience. arXiv preprint arXiv:2508.04700, 2025b. Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. Voyager: An open-ended embodied agent with large language models. arXiv preprint arXiv:2305.16291, 2023. Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. Qianhui Wu, Kanzhi Cheng, Rui Yang, Chaoyun Zhang, Jianwei Yang, Huiqiang Jiang, Jian Mu, Baolin Peng, Bo Qiao, Reuben Tan, et al. Gui-actor: Coordinate-free visual grounding for gui agents. arXiv preprint arXiv:2506.03143, 2025. Zhiyong Wu, Chengcheng Han, Zichen Ding, Zhenmin Weng, Zhoumianze Liu, Shunyu Yao, Tao Yu, and Lingpeng Kong. Os-copilot: Towards generalist computer agents with self-improvement. arXiv preprint arXiv:2402.07456, 2024a. Zhiyong Wu, Zhenyu Wu, Fangzhi Xu, Yian Wang, Qiushi Sun, Chengyou Jia, Kanzhi Cheng, Zichen Ding, Liheng Chen, Paul Pu Liang, et al. Os-atlas: foundation action model for generalist gui agents. arXiv preprint arXiv:2410.23218, 2024b. Tianbao Xie, Danyang Zhang, Jixuan Chen, Xiaochuan Li, Siheng Zhao, Ruisheng Cao, Toh Hua, Zhoujun Cheng, Dongchan Shin, Fangyu Lei, et al. Osworld: Benchmarking multimodal agents for open-ended tasks in real computer environments. Advances in Neural Information Processing Systems, 37:5204052094, 2024. Tianbao Xie, Jiaqi Deng, Xiaochuan Li, Junlin Yang, Haoyuan Wu, Jixuan Chen, Wenjing Hu, Xinyuan Wang, Yuhui Xu, Zekun Wang, et al. Scaling computer-use grounding via user interface decomposition and synthesis. arXiv preprint arXiv:2505.13227, 2025. Long Xing, Qidong Huang, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao, Jinsong Li, Shuangrui Ding, Weiming Zhang, Nenghai Yu, et al. Scalecap: Inference-time scalable image captioning via dual-modality debiasing. arXiv preprint arXiv:2506.19848, 2025. An Yan, Zhengyuan Yang, Wanrong Zhu, Kevin Lin, Linjie Li, Jianfeng Wang, Jianwei Yang, Yiwu Zhong, Julian McAuley, Jianfeng Gao, et al. Gpt-4v in wonderland: Large multimodal models for zero-shot smartphone gui navigation. arXiv preprint arXiv:2311.07562, 2023. Simon Zhai, Hao Bai, Zipeng Lin, Jiayi Pan, Peter Tong, Yifei Zhou, Alane Suhr, Saining Xie, Yann LeCun, Yi Ma, et al. Fine-tuning large vision-language models as decision-making agents via reinforcement learning. Advances in neural information processing systems, 37:110935110971, 2024."
        },
        {
            "title": "Technical Report",
            "content": "Chi Zhang, Zhao Yang, Jiaxuan Liu, Yanda Li, Yucheng Han, Xin Chen, Zebiao Huang, Bin Fu, and Gang Yu. Appagent: Multimodal agents as smartphone users. In Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems, pp. 120, 2025. Jiwen Zhang, Jihao Wu, Yihua Teng, Minghui Liao, Nuo Xu, Xiao Xiao, Zhongyu Wei, and arXiv preprint Duyu Tang. Android in the zoo: Chain-of-action-thought for gui agents. arXiv:2403.02713, 2024. Shuyan Zhou, Frank Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Tianyue Ou, Yonatan Bisk, Daniel Fried, et al. Webarena: realistic web environment for building autonomous agents. arXiv preprint arXiv:2307.13854, 2023. Yifei Zhou, Andrea Zanette, Jiayi Pan, Sergey Levine, and Aviral Kumar. Archer: Training language model agents via hierarchical multi-turn rl. arXiv preprint arXiv:2402.19446, 2024. Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Hao Tian, Yuchen Duan, Weijie Su, Jie Shao, et al. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models. arXiv preprint arXiv:2504.10479, 2025. Daniel Ziegler, Nisan Stiennon, Jeffrey Wu, Tom Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. Fine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593, 2019."
        },
        {
            "title": "Technical Report",
            "content": "A JUDGE MODEL FINE-TUNING DETAILS Inspired by Sun et al. (2025b), we adopt fine-tuning approach to obtain strong judge model. We scale up the model to Qwen2.5-VL-72B Bai et al. (2025), and use dataset comprising 4.7K labeled judgment samples. These trajectories are generated by Qwen2.5-VL and Gemini-2.0-Pro on WebArena Zhou et al. (2023), UI-TARS-1.5 Qin et al. (2025), and GPT-4o OpenAI (2023) on OSWorld Qin et al. (2025). Judgments are provided by GPT-4o and Gemini2.5-Pro Google DeepMind (2025), with detailed captions for each screenshot frame during agent execution. The judgments are further filtered, retaining only those that align with verified ground-truth results. Additionally, change description data is incorporated inspired by SEAgent Sun et al. (2025b). Training is conducted on 32 A100 GPUs for 370 steps, using LoRA Hu et al. (2022) with rank of 8. The resulting model, trained on OSWorld trajectories, generalizes well to AgentRewardBench L`u et al. (2025) and ScienceBoard Sun et al. (2025a). This fine-tuned model is referred to as 72B-GUIJudge in Table 2, and demonstrates improved precision on two out-of-domain benchmarks. When further ensembled with the original 72B base model, it achieves even higher precision, providing more accurate reward signalscrucial for effective reinforcement learning of the planner agent. PROMPT DETAILS. We provide detailed prompt for task generator in Fig.5 and judge system in Fig.6. Detailed prompt for planner agent is in Fig.7. Prompt we used for executor agent aligns with UI-TARS Qin et al. (2025) official code."
        },
        {
            "title": "C VIRTUAL MACHINE SYSTEM DETAILS",
            "content": "We utilized local cluster consisting of 15 servers to collect interaction trajectories. Among these, 13 servers were equipped with AMD EPYC 7742 processors, and 2 servers were equipped with Intel i9-13900K CPUs paired with NVIDIA GeForce RTX 4090 GPUs to support software with high graphical computing demands, such as ChimeraX. Using VMware Workstation Pro, we ran 4 to 8 independent virtual machines concurrently on each server to execute tasks in parallel."
        },
        {
            "title": "Technical Report",
            "content": "Figure 5: Detailed prompt for task generation."
        },
        {
            "title": "Technical Report",
            "content": "Figure 6: Detailed prompt for the judge model. Text in gray all task type based on whether finish given task or answer the question from user."
        },
        {
            "title": "Technical Report",
            "content": "Figure 7: Detailed prompt for the planner agent."
        }
    ],
    "affiliations": [
        "Shanghai AI Laboratory",
        "Shanghai Jiao Tong University",
        "The Chinese University of Hong Kong",
        "The University of Hong Kong"
    ]
}