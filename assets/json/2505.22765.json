{
    "paper_title": "StressTest: Can YOUR Speech LM Handle the Stress?",
    "authors": [
        "Iddo Yosha",
        "Gallil Maimon",
        "Yossi Adi"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Sentence stress refers to emphasis, placed on specific words within a spoken utterance to highlight or contrast an idea, or to introduce new information. It is often used to imply an underlying intention that is not explicitly stated. Recent advances in speech-aware language models (SLMs) have enabled direct processing of audio, allowing models to bypass transcription and access the full richness of the speech signal and perform audio reasoning tasks such as spoken question answering. Despite the crucial role of sentence stress in shaping meaning and speaker intent, it remains largely overlooked in evaluation and development of such models. In this work, we address this gap by introducing StressTest, a benchmark specifically designed to evaluate a model's ability to distinguish between interpretations of spoken sentences based on the stress pattern. We assess the performance of several leading SLMs and find that, despite their overall capabilities, they perform poorly on such tasks. To overcome this limitation, we propose a novel synthetic data generation pipeline, and create Stress17k, a training set that simulates change of meaning implied by stress variation. Then, we empirically show that optimizing models with this synthetic dataset aligns well with real-world recordings and enables effective finetuning of SLMs. Results suggest, that our finetuned model, StresSLM, significantly outperforms existing models on both sentence stress reasoning and detection tasks. Code, models, data, and audio samples - pages.cs.huji.ac.il/adiyoss-lab/stresstest."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 2 ] . [ 1 5 6 7 2 2 . 5 0 5 2 : r StressTest: Can YOUR Speech LM Handle the Stress? Iddo Yosha Gallil Maimon Yossi Adi"
        },
        {
            "title": "School of Computer Science and Engineering\nThe Hebrew University of Jerusalem",
            "content": "iddo.yosha@mail.huji.ac.il"
        },
        {
            "title": "Abstract",
            "content": "Sentence stress refers to emphasis, placed on specific words within spoken utterance to highlight or contrast an idea, or to introduce new information. It is often used to imply an underlying intention that is not explicitly stated. Recent advances in speech-aware language models (SLMs) have enabled direct processing of audio, allowing models to bypass transcription and access the full richness of the speech signal and perform audio reasoning tasks such as spoken question answering. Despite the crucial role of sentence stress in shaping meaning and speaker intent, it remains largely overlooked in evaluation and development of such models. In this work, we address this gap by introducing StressTest, benchmark specifically designed to evaluate models ability to distinguish between interpretations of spoken sentences based on the stress pattern. We assess the performance of several leading SLMs and find that, despite their overall capabilities, they perform poorly on such tasks. To overcome this limitation, we propose novel synthetic data generation pipeline, and create Stress-17k, training set that simulates change of meaning implied by stress variation. Then, we empirically show that optimizing models with this synthetic dataset aligns well with real-world recordings and enables effective finetuning of SLMs. Results suggest, that our finetuned model, StresSLM, significantly outperforms existing models on both sentence stress reasoning and detection tasks. Code, models, data, and audio samples - https://pages.cs.huji.ac.il/adiyoss-lab/stresstest."
        },
        {
            "title": "Introduction",
            "content": "Large language models (LLMs) have revolutionized language processing, enabling new forms of human-computer interaction [Minaee et al., 2025; Grattafiori et al., 2024]. As the field advanced, researchers have started exploring the integration of other modalities into LLMs, aiming to enrich their understanding of the world [Carolan et al., 2024; Liu et al., 2023]. In particular, the incorporation of speech and audio into LLMs has gained notable traction, equipping models with the ability to listen, speak, and reason about audio [Arora et al., 2025; Ghosh et al., 2025; Maimon et al., 2025a]. An elementary approach of integrating speech into LLMs follows cascade paradigm [Ji et al., 2024], where audio is first transcribed by an automatic speech recognition (ASR) system, and the resulting text is then processed by language model. While effective to some extent, this approach falls short of capturing the full expressive range of spoken language. Speech carries rich paralinguistic cues such as emotion, speaker identity, and prosodic characteristics including pitch (f0), loudness, duration, timbre and rhythm, that are often lost in transcription [Wilson and Wharton, 2006; Van Heuven, 2018]. One key aspect of prosody is sentence stress, which refers to the emphasis placed on particular words or phrases within sentence to highlight an idea or to contrast another [Bolinger, 1972]. For example, Preprint. Under review. Figure 1: StressTest provides samples that can be understood differently based on different stress patterns. We consider both sentence stress detection (SSD) and sentence stress reasoning (SSR). StresSLM detects the stress and reasons about the intended meaning. consider the sentence didnt say she stole the money. Depending on which word is stressed, the sentence can express dramatically different meanings, despite its written-form remaining unchanged. Recent developments in Speech-aware LMs (SLMs) aim to address these limitations by enabling models to process audio directly, bypassing the need for explicit transcription and allowing them to access the full range of acoustic information [Arora et al., 2025]. State-of-the-art models in this space have demonstrated impressive performance across wide range of tasks, including audio question answering, automatic speech recognition, and emotion recognition [Chu et al., 2024; Tang et al., 2024]. Nevertheless, sentence stress has received limited attention in the evaluation and development of these models, despite its vital role in expressing the speakers intent and meaning. We argue that interpreting sentence stress requires the listener to reason about the intended meaning based on stress placement, which can often be inferred even without explicit context. In this work, we address this gap by introducing StressTest, comprehensive benchmark designed to evaluate models ability to distinguish spoken sentence meanings based on different stress patterns. We then evaluate leading SLMs on our benchmark to quantify their capacity for stress based reasoning. Additionally, we introduce novel synthetic dataset generation pipeline, which produces the Stress-17k dataset. We empirically show that finetuning model using Stress-17k leads to enhanced ability to detect and model sentence stress on real-world recordings. Through extensive empirical evaluation and ablation studies, we demonstrate that our finetuned model, StresSLM, significantly outperforms existing models in both stress detection and sentence stress reasoning, with minimal performance degradation on its original tasks. Our contributions: (i) We propose StressTest, novel benchmark for evaluating sentence stress understanding in SLMs; (ii) We analyze the performance of several leading models on this benchmark, and show that they fail to detect sentence stress and to reason about the underlying meaning, based on the stress pattern; (iii) Lastly, we propose synthetic data generation pipeline and demonstrate its effectiveness by finetuning leading SLM."
        },
        {
            "title": "2 Background",
            "content": "Theoretical views of sentence stress. As described by Ladd [2008], theoretical accounts of sentence stress, can be broadly divided into two main perspectives. The first is the phonological view, which defines the notion of normal stress as the default prosodic pattern in an utterance. This view is formalized in the Nuclear Stress Rule [Chomsky and Halle, 1968], which assumes that sentence stress is determined by syntactic structure and does not necessarily convey special semantic intent. The second, views stress as semantic phenomenon that conveys the speakers intention. According to this perspective, stress can be flexibly assigned to any word in sentence to highlight its relative importance in the discourse. Such stress can signal new information, contrast, focus, emotional emphasis or other special meanings. The semantic view suggests that stress is context-dependent rather than purely syntactically derived [Bolinger, 1972]. In this work, we focus on this view of sentence stress, by testing whether the meaning intended by the speaker can be inferred by SLMs. Sentence stress can be roughly categorized into four, non-mutually exclusive, main categories: (i) Contrastive Stress: Used to mark an element in opposition to another, whether explicit or implicit. [Bolinger, 1961; Dretske, 1972; Boer, 1979]; (ii) Emphatic Stress: Used to amplify or diminish the intensity of concept, often conveying emotion or perceived magnitude [Szwedek, 1986]; (iii) 2 Table 1: Examples of sentence stress categories from StressTest. Stress Type Description Stressed speech (intention) Contrastive Emphatic Demonstrates contrast with another option. didnt take your book. (vs. someone else) didnt take your book. (vs. something else) Amplifies or diminishes the intensity of concept. They loved how you treated her dog. (You really exceeded their expectations) New-Information Marks surprising or novel content in the discourse. Hes actually moving to New York. (surprising since its far from his current home) Focus General-purpose mechanism for highlighting key elements. enjoy the taste of espresso at sunrise. (Its about that particular time) New Information Stress: Used to signal that word or phrase introduces new, potentially surprising information to the discourse [Bolinger, 1958; Szwedek, 1986]; (iv) Focus Stress: general-purpose mechanism for marking the most relevant element in response to discourse needs [Szwedek, 1986; Ladd, 2008]. Examples and descriptions of the different categories can be found in Table 1. Realization of sentence stress in speech. complementary line of research has focused on how stress is realized in speech signal, rather than on its discourse-level function. These studies investigate the acoustic aspects that contribute to the production and perception of stress. The most widely agreed-upon prosodic features associated with stress are pitch (f0), loudness, duration and vowel quality. Among which, those contributing most to detection of sentence stress found to be intensity (loudness), duration and pitch [Van Heuven, 2018; Silipo and Greenberg, 2000]."
        },
        {
            "title": "3 StressTest: Benchmarking sentence stress understanding",
            "content": "We present StressTest, benchmark aimed at evaluating stress understanding in SLMs. We focus on two main tasks: (i) Sentence Stress Reasoning (SSR), evaluates whether an SLM can infer the speakers intended meaning from the speech alone. This task goes beyond recognizing what was said, it requires understanding how it was said, focusing on how stress influences the interpretation of an utterance; and (ii) Sentence Stress Detection (SSD), evaluates the models ability to identify stressed word or words in an utterance, given the ground truth transcription. The detection task is simpler and focuses on the models ability to identify the stressed words while alleviating the requirement to recognize the transcription. This provides controlled assessment of the models sensitivity to acoustic prominence. Note that, in contrast to SSR which is novel task, SSD is already established in the literature. We include it as complementary task to SSR. 3.1 Dataset StressTest is single-speaker dataset (recorded by professional actor) comprising 101 unique texts, each recorded with at least two distinct sentence stress patterns, yielding different underlying interpretations of the same utterance. Specifically, 85 sentences have 2 different interpretations, while 16 have 3. Of the interpretations - 170 have single stressed word, 43 contain 2 stressed words and 5 include 3 distinct stressed words. In total, the number of audio samples is 218. All audio recordings are sampled of 48kHz. To match SLMs requirements, audio recordings were down-sampled to 16kHz. More details can be found in Appendix A. Text samples were curated through rigorous manual process. Annotators who are fluent in English were instructed to suggest sentences that could have at least two different meanings based on stressed word or words. We provide annotators with several examples from literature for inspiration, and they could use semi-automatic tools for proposing new examples (e.g. LLMs), but had to manually write and verify the samples. We also request they do not use exact templates of the examples, even if they use similar stress types. These samples were verified for agreement with at least one further annotator. In cases where the stressed words were agreed upon, but the text description of the meaning had to be refined this was done by third annotator. Finally, annotators were requested to alter textual interpretations of examples that do not adhere to the stressed speech and to mark samples where the audio is corrupted for removal. Illustrative examples of the recorded texts, each featuring different 3 stressed words and interpretations, are provided in Table 1. breakdown of the various stress patterns in StressTest, along with their frequencies, is shown in Figure 3. Overall, we define StressTest as = {洧논1, ..., 洧논洧녵}, where 洧녵 = 218. Each sample 洧논 consists of: 洧논 = (洧녩, 洧노, 洧, 洧냢, 洧녳), where: (i) 洧노 is the samples transcription; (ii) 洧 is binary vector of the stressed words of the transcription 洧노, where 1 marks word as stressed; (iii) 洧냢 is set of two possible interpretations of the transcription 洧노, both possible given different stress pattern; (iv) 洧녳 洧냢 marks the correct interpretation; and finally, (v) 洧녩 is the speech sample. 3.2 Evaluation procedure Given sample 洧논, we evaluate models ability to perform both stress reasoning and stress detection by constructing task-specific prompts. The complete set of prompts used for evaluation is detailed in Appendix D.1. For the sentence stress reasoning (SSR) task, the model is provided with the audio only and is instructed to select the most likely intended meaning from the set of possible interpretations. Model performance is measured using accuracy. As prompt adherence varies significantly across models, we follow the LLM-as-a-judge [Gu et al., 2025] approach, using gpt-4o [Hurst et al., 2024], to interpret the output of the model. This judgment is then used to compute the final evaluation metric consistently for all evaluated models. Given model M, dataset D, prompt and judge the SSR accuracy is measured as follows: SSR洧녩洧녫洧녫 (M, D) = 1 (洧녩,洧노 ,洧, 洧냢,洧녳) I{J (M (洧녩, ( 洧냢))) = 洧녳}. (1) Similarly, for the sentence stress detection (SSD) task, the model receives both the audio 洧녩 and the ground truth transcription 洧노 and is instructed to identify the stressed word(s) in the utterance. Then, judge interprets the models output and we compute precision, recall, and F1 scores based on the predicted stress positions. 3.3 Benchmarking results Equipped with method to evaluate sentence stress modeling, we benchmark leading SLMs on StressTest. We consider Qwen2Audio-7B-Instruct [Chu et al., 2024], SALMONN [Tang et al., 2024], LLaMA-Omni [Fang et al., 2025], Phi-4-multimodal-instruct [Microsoft et al., 2025], and gpt-4o-audio [Hurst et al., 2024]. In addition, we conduct human study to validate that our core SSR task is relatively easy and straightforward for human listeners. We randomly sample 100 samples from StressTest, sufficient to estimate human-level performance. We ask 16 annotators to answer the same multiple-choice questions. Each sample is independently labeled by three annotators. We report both overall accuracy and majority-vote accuracy of three annotators in Table 2 (bottom cell). The annotation system and protocol can be viewed on Appendix A, Figure 4. Results suggest that leading SLMs struggle to infer the intended meaning conveyed through stress patterns, achieving near-random performance on StressTest. In contrast, human annotators perform near perfectly under majority vote accuracy and overall accuracy, 96.0% and 92.6% respectively."
        },
        {
            "title": "4 Synthetic data generation",
            "content": "We present synthetic data generation pipeline, used to create Stress-17k, training set aimed at enhancing performance on sentence stress understanding tasks. The core idea is that, once data is generated with sufficient diversity and quality, finetuning SLMs on this dataset will generalize to real-world recordings. The data generation methodology is illustrated in Figure 2 and is divided into four main components: (i) Text sample generation; (ii) Stressed speech synthesis; (iii) Stress verification; (iv) Training task definition. Overall, the training set amounts to 17K audio samples, of which 4.5K are automatically verified, additional data statistics are available in Appendix B.1. Text sample generation. We start by generating the texts, stress patterns, and their interpretations. Note that not all texts can have different meanings based on emphasis, thus we explicitly create them 4 Table 2: SSR performance on StressTest with different inputs - ground truth transcriptions, stress labels, and the raw audio. We also report human performance and majority vote of three annotators. Category Model Text LLM gpt-4o [Hurst et al., 2024] gpt-4o-mini [Hurst et al., 2024] Llama-3.1-8B-Instruct [Grattafiori et al., 2024] Qwen2-7B-Instruct [Yang et al., 2024a] Qwen-7B-Chat [Bai and et al., 2023] Cascade Pipeline WhiStress Llama-3.1-8B-Instruct WhiStress Qwen2-7B-Instruct WhiStress Qwen-7B-Chat SLM gpt-4o-audio [Hurst et al., 2024] Qwen2Audio-7B-Instruct [Chu et al., 2024] SALMONN [Tang et al., 2024] LLaMA-Omni [Fang et al., 2025] Phi-4-multimodal-instruct [Microsoft et al., 2025] StresSLM (ours) Human Annotators Trans. Stress Audio SSR 86.2 79.3 73.3 67.8 61.4 66.9 63.7 55.5 58.7 56.4 56.8 53.6 53.2 81.6 92.6 (96.0) to that end. The text samples are generated through sequential agentic process, with gpt-4o [Hurst et al., 2024] as the agent, and using the CrewAI framework1. This sequential process is comprised of two parts: (i) Create sentence samples that can be understood differently according to the stressed words. The instruction is conditioned on domain, topic and sentence type from fixed list. The reason for using such variables is to improve sample diversity, since we find that the agent is prone to repetitions. Using the notations from Section 3.2, after the first part we get two samples that share the same transcription: 洧논1 = (洧노, 洧1, 洧녬1), 洧논2 = (洧노, 洧2, 洧녬2) where for 洧녰 {1, 2}, 洧녬洧녰 describes the underlying meaning of each interpretation imposed by the different stress pattern, 洧멇롐; (ii) As 洧녬 can be lengthy, we aim at create shorter summarized version of it. Given the input from the previous part, we ask the agent to create set of possible answers 洧냢, each summarizing the corresponding interpretations description 洧녬. Hence, 洧냢 can seen as concise version of 洧녬. Then, we end with two samples of the form 洧논 = (洧노, 洧, 洧녬, 洧냢, 洧녳) where 洧녳 洧냢 is the target answer out of two possible answers corresponding to 洧노. By that, we can ask model what is the speakers intention while providing two feasible answers. Prompts and metadata can be seen in Appendix D.4.1. Stressed speech synthesis. Given text samples with markers of which words to stress to convey the desired meaning, we use the OpenAI text-to-speech2 to generate expressive speech. We find that marking the stressed words with enclosing asterisks leads to these words being stressed when synthesized. Our preliminary results suggest, this approach leads to more natural voice compared to editing prosodic features directly. For each textual stress pattern, we generate two audio samples using randomly selected male or female speakers. This results in four audio samples per transcription 洧노: two reflecting one stress pattern and two reflecting different one. Finally, each sample is of the form 洧논 = (洧녩, 洧노, 洧, 洧녬, 洧냢, 洧녳) where 洧녩 marks the synthesized speech. Stress verification. Despite using an expressive TTS, in preliminary studies we observe frequent inconsistencies in stress synthesis, i.e. stressing unwanted words or missing desired words. Hence, we leverage the WhiStress model [Yosha et al., 2025] for data filtering. We predict transcriptions and stressed words for each utterance and filter samples generated with inaccurate sentence stress. This verification step allows us to create higher-quality curated subset of the data which we find to improve model performance, especially with curriculum setup (see Section 5.4). The WhiStress verifier uses the Whisper [Radford et al., 2022] speech recognition model as backbone. It is equipped with an additional detection head to predict binary stress targets for each token predicted by the backbone model. Thus, enriching the model-predicted transcriptions with stress predictions, without affecting the original performance. The additional stress detection head is constructed from 1https://www.crewai.com 2https://platform.openai.com/docs/guides/text-to-speech 5 Figure 2: An illustrative example of the synthetic training data generation process. an added decoder block that cross-attends the input from the backbones decoder and encoder, and then uses an MLP classifier to predict the stress label per transcribed token. Training task definition. We aim to improve SLM performance on SSR and SSD. To this end, each sample 洧논 = (洧녩, 洧노, 洧, 洧녬, 洧냢, 洧녳) is utilized in four tasks, with corresponding prompt: (i) Sentence stress detection: The model is requested to identify the stressed words 洧, given the ground truth transcription 洧노; (ii) End-to-end reasoning: The model is asked to choose the most likely underlying meaning out of 洧냢 according to the audio, directly responding the correct answer 洧녳 洧냢; (iii) Elaborate reasoning: Similar to the above, this task aims to train the model on SSR. However, the model is instructed to first elaborate on the interpretations meaning and then choose the most likely answer. In practice, we use the description 洧녬 as the models elaboration prefix before the final answer 洧녳; (iv) Cascade reasoning: This task targets both SSD and SSR by requesting the model to output the transcription 洧노 with the emphasized words 洧 and then output the correct answer 洧녳. Notice, for both (iii) and (iv) we do take loss over the prefix as well. We hypothesize that this variation allows the model to better connect sentence stress and implied underlying meaning. The training prompts are given in Appendix D.3."
        },
        {
            "title": "5 Experiments and results",
            "content": "We study the efficacy of our synthetic training data, Stress-17k, on SSR and SSD. We further conduct analysis to ablate the impact of pipeline components - stress verifier, trained encoder, training tasks. We fine-tune Qwen2Audio-7B-Instruct [Chu et al., 2024] using LoRA adapters [Hu et al., 2021] on the query and value projections with the synthetic training dataset generated by our proposed pipeline. To prevent overfitting to stress-focused tasks, we also add samples from the original tasks which the base model was trained on, namely, LibriLight [Kahn et al., 2020] for automatic-speech-recognition (ASR) and MELD [Poria et al., 2019] for speech-emotion-recognition (SER). We ensure that the total audio duration of these auxiliary tasks approximately matches our verified training subset. During training, we employ staged training approach, where the model is first finetuned on the full dataset (both verified and unverified) for one epoch, and then finetuned on the smaller, high-quality subset for another epoch. We find this two-stage strategy effective in balancing the performance on both SSR and SSD tasks. Hyperparameters and implementation details are reported in Appendix B.2. 5.1 Sentence stress detection One may argue that an essential prerequisite for understanding the meaning conveyed by sentence stress is the ability to accurately detect it. For this purpose, we start by evaluating the ability of SLMs on the task of SSD. We evaluate models both on StressTest and Expresso [Nguyen et al., 2023] benchmarks. To align with previous works [Yosha et al., 2025; de Seyssel et al., 2024] we use only samples that include at least one stressed word and speaker IDs ex01 and ex02. Results in Tab. 3 show that current SLMs struggle with detecting stress while StresSLM significantly outperforms them. Table 3: Performance on stress detection across two datasets. We compare the results of StresSLM to other SLMs, and also to task-specific model, WhiStress, which is trained solely for this task. Dataset Model Precision Recall F1 Expresso StressTest WhiStress gpt-4o-audio SALMONN LLaMA-Omni Phi-4-multimodal-instruct Qwen2Audio-7B-Instruct StresSLM (ours) WhiStress gpt-4o-audio SALMONN LLaMA-Omni Phi-4-multimodal-instruct Qwen2Audio-7B-Instruct StresSLM (ours) 57. 23.6 13.2 18.7 22.5 34.2 51.8 88.8 33.1 19.1 24.1 19.9 24.6 89.6 86.3 66.1 45.5 58.2 37.5 30.6 68.6 88. 52.1 29.5 47.6 32.8 46.2 83.3 68.9 34.7 20.5 28.3 28.2 32.3 59.1 88.5 40.5 23.2 32.0 24.7 32.1 86.4 5.2 Sentence stress reasoning We evaluate model performance on SSR by systematically testing different input configurations in both speech-aware and text-only LMs. Since SLMs are an extension of text LMs, we analyze the scenario where both the transcription and stressed words are available for the text LM to base its answer on. We consider three primary settings: (i) an oracle configuration in which both ground truth transcription and stressed words are supplied to text LM; (ii) cascade pipeline in which these inputs are predicted by WhiStress, simulating speech-only, end-to-end inference process; and (iii) fully end-to-end setting, where SLMs receive only the raw audio and directly predict the underlying meaning. Results are summarized in Table 2, and additional settings tested are available in Table 9. The proposed model demonstrates strong results on the SSR task, exceeding the evaluated SLMs and Cascade models that receive audio only as input by large margin, 81.6 vs. 66.9 and 58.7 for the best cascade and SLM respectively. This demonstrates the effectiveness of Stress-17k in optimizing the model to understand the intended meaning implied by spoken stress. In assessing sentence stress understanding in text-based LLMs, we observe that, when given oracle labels, these models outperform their speech-based counterparts on the SSR task. Moreover, the results indicate that performance tends to correlate with the models overall language capabilitiesas reflected by their scores on standard text benchmarkswhere open-source 7B models lag behind proprietary models like gpt-4o-mini and gpt-4o [Hurst et al., 2024]. Finally, as expected, compared to the oracle configuration, when transcription and stress are predicted the SSR results degrade due to prediction errors, for instance 73.3 vs. 66.9 for the Llama3.1 version. Despite error propagation, the cascade approach outperforms current end-to-end SLMs (with the exception of the proposed method). This implies that stress information remains underutilized when derived directly from raw audio, thereby highlighting the effectiveness of our pipeline in explicitly extracting and leveraging prosodic cues for reasoning. 5.3 Effect on original tasks To assess whether our proposed approach introduces trade-offs with existing capabilities, we evaluate StresSLM on original tasks used to train the base model. Specifically, we test ASR and SER. This analyzes if our stress-focused training interferes with the SLMs abilities on basic speech understanding tasks. Results are reported in Table 4. We observe minor degradation in ASR performance compared to the Qwen2Audio-7B-Instruct model. However, the results remain strong, indicating that the model retains its speech recognition ability. Interestingly, on the SER task, we even see modest improvement. These findings suggest that our multitask training setup does not inherently conflict with the original objectives, demonstrating the potential to enrich SLMs with sentence stress reasoning capabilities, without compromising on established tasks. 7 Table 4: Results on fundamental speech understanding tasks. Model ASR (WER) SER dev-clean dev-other test-clean test-other MELD Qwen2Audio-7B Qwen2Audio-7B-Instruct StresSLM (ours) 1.70 2.30 2.70 3.68 4.64 4. 1.73 2.31 2.46 4.01 4.92 5.50 54.6 26.4 57.2 5.4 Ablation study In the following experiments we analyze our design choices regarding the two-stage training procedure, architecture trainable components and the tasks we used during training of StresSLM. Models are trained solely on Stress-17k without the ASR and SER related data. Additionally, all models are trained under the same conditions with compute budget equivalent to 5 epochs on the verified data subset, the number of steps is smaller when training tasks are removed. Staged training introduces balance. We first evaluate the efficacy of the WhiStress verifier. Results are presented in Table 5. We train LMs on all stress-related tasks using frozen speech encoder and assess performance on StressTest under different training strategies. Results show that training on the verified subset improves performance on SSR, suggesting that the data selected by WhiStress is beneficial for stress reasoning. However, this comes at the cost of reduced performance on the SSD task, likely due to reduced data diversity or quantity. In contrast, the staged (curriculum setup) training approach, in which the model is first trained on the full dataset and then fine-tuned on the filtered subset, achieves better balance, improving SSD performance while slightly reducing SSR. Table 5: Effect of using WhiStress verifier on model performance. Verifier # Training Samples SSD SSR Precision Recall F1 Accuracy 4K 17K 17K 4K 87.3 87.4 88. 76.3 81.9 83.7 81.4 84.5 85.9 79.3 76.6 78.4 Speech encoder captures stress. We then assess whether stress related information must be explicitly extracted from the audio encoder of StresSLM, or if the fine-tuned model can rely solely on the frozen speech representations from the original model. As shown in the first and last rows of Table 6, the model benefits significantly from training the encoder on both SSR and SSD tasks. This finding is consistent with previous works [Yosha et al., 2025; Pasad et al., 2022] which demonstrated that prosodic features are encoded in different layers of speech representation models, thus, finetuning the encoder enables extracting this stress-related information. Moreover, the results indicate that SLMs finetuned on the SSD task can outperform existing stress detection models such as WhiStress. As shown in Table 3, WhiStress achieves an F1 score of 88.5 on StressTest, while as seen in Table 6, StresSLM that was trained solely on stress detection-oriented tasks achieves higher F1 score of 90.5. Training tasks balance each other. We also analyze the effect of the different tasks introduced in Section 4 on performance on StressTest. To do so, we conduct an ablation study in which we remove one training task at time while always retaining our primary end-to-end reasoning task. Results shown in Table 6, reveal that no combination yields the best performance across all metrics. However, including all task variants produces the second-best results for both SSR and SSD, suggesting that diverse task mixture leads to more balanced fine-tuning. Interestingly, removing the elaborated explanation task results in substantial drop in SSR performance, while improving SSD results. This may be due to SSD-oriented data becoming more dominant in the training data, indicating trade-off in task emphasis that affects model specialization."
        },
        {
            "title": "6 Related work",
            "content": "Sentence stress modeling. Existing works define the task of sentence stress detection, usually as word-level binary classification. Modeling approaches attempted to directly detect stress from speech 8 Table 6: Ablating the different training design choices of StresSLM on stress modeling capabilities, while using staged-training strategy. Train Setup Training Tasks StressTest Performance Train Enc. Stress Cascade Elaborated SSD SSR Det. Reasoning Exp. Precision Recall F1 Accuracy 86.6 45.5 90.4 94. 92.4 84.1 86.3 83.3 87.0 86.3 85.3 59.9 86.7 90. 89.3 78.8 84.4 82.5 78.4 83.0 [Silipo and Greenberg, 2000; Mishra et al., 2012] or include additional grammatical and contextual information to facilitate detection [Lin et al., 2020; Lee et al., 2016]. Most modern methods use powerful speech representations from pre-trained models [de Seyssel et al., 2024; Yosha et al., 2025]. Another research effort was developing expressive TTS models that can generate emphasized speech [Stephenson et al., 2022]. Other work investigated whether text LLMs understand the implications of emphasized words in the context of discourse [Lin and Lee, 2024]. Speech-aware language models. In the past years, integration of speech and audio to existing LLMs has demonstrated impressive abilities on speech tasks. Most such models follow similar approach [Arora et al., 2025]: (i) use pretrained LLM; (ii) encode the speech to latent representation with pretrained encoder; (iii) project the representations to the LLM embedding space. These speech-conditioned LMs are trained on speech related tasks, showing impressive results. Qwen2Audio [Chu et al., 2024] use Whisper encoder Radford et al. [2022] for feature representation and Qwen-7B [Bai and et al., 2023] as the LLM backbone. SALMONN [Tang et al., 2024] further extend this by also including BEATs encoder [Chen et al., 2022] to encode non-speech sounds. Other models such as LLaMA-Omni [Fang et al., 2025] and mini-omni [Xie and Wu, 2024] introduce the ability to generate speech. Phi4-multimodal further extends the capabilities beyond two modalities, allowing the model to consume vision data in addition to text and audio [Microsoft et al., 2025]. Related benchmarks. Speech-aware LM evaluation includes ASR [Ardila et al., 2020; Panayotov et al., 2015], SER [Ma et al., 2024], audio question answering [Lipping et al., 2022], sound classification [Gong et al., 2022], speech-to-text-translation [Wang et al., 2020]. With the progress of SLMs as universal speech processing systems, several benchmarks have been suggested covering range of speech and audio tasks, including prosodic elements [Yang et al., 2024b; Wang et al., 2025; wen Yang et al., 2021; Maimon et al., 2025b]. However, sentence stress is notably absent from such evaluations. For stress detection, few targeted benchmarks have been proposed. Some rely on synthetic data or re-purpose existing emotion datasets [Yosha et al., 2025; de Seyssel et al., 2024] such as Expresso [de Seyssel et al., 2024] as labeled data. Another work includes crowd-sourced annotations to assess the presence of stress in spoken utterances [Morrison et al., 2023]. Recent work by de Seyssel et al. [2024] introduced framework for assessing whether speech-to-speech translation models preserve stress. Despite these efforts, there remains lack of benchmarks designed to evaluate the ability of models to reason about meaning derived from sentence stress in spoken language."
        },
        {
            "title": "7 Conclusion",
            "content": "In this work, we introduced StressTest, benchmark for evaluating sentence stress in SLMs. We consider two tasks: Sentence Stress Reasoning and Sentence Stress Detection, which assess models ability to capture meaning and speaker intent through spoken prosodic cues. These tasks highlight the important yet underexplored role of stress placement in shaping the meaning of spoken language, particularly in the context of speech-aware LMs. We then proposed novel and automatic synthetic data generation pipeline tailored to stress modeling, by which creating Stress-17k. Finetuning SLMs on Stress-17k yields strong performance gains. Our model, StresSLM, significantly outperforms existing baselines on both SSR and SSD, while preserving performance on core tasks like ASR and SER. comprehensive ablation study further shows the efficacy of our approach. 9 Despite these contributions, our work is currently limited to English. How well stress-based reasoning generalizes across languages and accents remains an open question. Nonetheless, our findings highlight the value of prosody in speech-language modeling and suggest promising directions for future work in speech understanding and generation that better reflect the nuances of human communication."
        },
        {
            "title": "References",
            "content": "Ardila, R., Branson, M., Davis, K., Henretty, M., Kohler, M., Meyer, J., Morais, R., Saunders, L., Tyers, F. M., and Weber, G. (2020). Common voice: massively-multilingual speech corpus. Arora, S., Chang, K.-W., Chien, C.-M., Peng, Y., Wu, H., Adi, Y., Dupoux, E., Lee, H.-Y., Livescu, K., and Watanabe, S. (2025). On the landscape of spoken language models: comprehensive survey. Bai, J. and et al., S. B. (2023). Qwen technical report. Boer, S. E. (1979). Meaning and contrastive stress. The Philosophical Review, 88(2):263298. Bolinger, D. (1972). Accent is predictable (if youre mind-reader). Language, 48(3):633644. Bolinger, D. L. (1958). Stress and information. American Speech, 33(1):520. Bolinger, D. L. (1961). Contrastive accent and contrastive stress. Language, 37(1):8396. Carolan, K., Fennelly, L., and Smeaton, A. F. (2024). review of multi-modal large language and vision models. Chen, S., Wu, Y., Wang, C., Liu, S., Tompkins, D., Chen, Z., and Wei, F. (2022). Beats: Audio pre-training with acoustic tokenizers. Chomsky, N. and Halle, M. (1968). The sound pattern of english. Chu, Y., Xu, J., Yang, Q., Wei, H., Wei, X., Guo, Z., Leng, Y., Lv, Y., He, J., Lin, J., Zhou, C., and Zhou, J. (2024). Qwen2-audio technical report. de Seyssel, M., DAvirro, A., Williams, A., and Dupoux, E. (2024). Emphassess : prosodic benchmark on assessing emphasis transfer in speech-to-speech models. Dretske, F. I. (1972). Contrastive statements. The Philosophical Review, 81(4):411437. Fang, Q., Guo, S., Zhou, Y., Ma, Z., Zhang, S., and Feng, Y. (2025). Llama-omni: Seamless speech interaction with large language models. Ghosh, S., Kong, Z., Kumar, S., Sakshi, S., Kim, J., Ping, W., Valle, R., Manocha, D., and Catanzaro, B. (2025). Audio flamingo 2: An audio-language model with long-audio understanding and expert reasoning abilities. Gong, Y., Yu, J., and Glass, J. (2022). Vocalsound: dataset for improving human vocal sounds recognition. In ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE. Grattafiori, A. et al. (2024). The llama 3 herd of models. arXiv preprint arXiv:2407.21783. Gu, J., Jiang, X., Shi, Z., Tan, H., Zhai, X., Xu, C., Li, W., Shen, Y., Ma, S., Liu, H., Wang, S., Zhang, K., Wang, Y., Gao, W., Ni, L., and Guo, J. (2025). survey on llm-as-a-judge. Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., and Chen, W. (2021). Lora: Low-rank adaptation of large language models. Hurst, A., Lerer, A., Goucher, A. P., Perelman, A., Ramesh, A., Clark, A., Ostrow, A., Welihinda, A., Hayes, A., Radford, A., et al. (2024). Gpt-4o system card. arXiv preprint arXiv:2410.21276. Ji, S., Chen, Y., Fang, M., Zuo, J., Lu, J., Wang, H., Jiang, Z., Zhou, L., Liu, S., Cheng, X., et al. (2024). Wavchat: survey of spoken dialogue models. arXiv preprint arXiv:2411.13577. Kahn, J., Riviere, M., Zheng, W., Kharitonov, E., Xu, Q., Mazare, P., Karadayi, J., Liptchinsky, V., Collobert, R., Fuegen, C., Likhomanenko, T., Synnaeve, G., Joulin, A., Mohamed, A., and Dupoux, E. (2020). Libri-light: benchmark for asr with limited or no supervision. In ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), page 76697673. IEEE. Kalajdzievski, D. (2023). rank stabilization scaling factor for fine-tuning with lora. Ladd, D. R. (2008). Intonational Phonology. Cambridge Studies in Linguistics. Cambridge University Press, 2 edition. Lee, G., Lee, H.-Y., Song, J., Kim, B., Kang, S., Lee, J., and Hwang, H. (2016). Automatic sentence stress feedback for non-native english learners. Computer Speech & Language, 41. Lin, B., Wang, L., Feng, X., and Zhang, J. (2020). Joint detection of sentence stress and phrase boundary for prosody. In Interspeech. Lin, G.-T. and Lee, H.-y. (2024). Can LLMs understand the implication of emphasized sentences in dialogue? In Al-Onaizan, Y., Bansal, M., and Chen, Y.-N., editors, Findings of the Association for Computational Linguistics: EMNLP 2024, pages 1339113401, Miami, Florida, USA. Association for Computational Linguistics. Lipping, S., Sudarsanam, P., Drossos, K., and Virtanen, T. (2022). Clotho-aqa: crowdsourced dataset for audio question answering. Liu, H., Li, C., Wu, Q., and Lee, Y. J. (2023). Visual instruction tuning. Ma, Z., Chen, M., Zhang, H., Zheng, Z., Chen, W., Li, X., Ye, J., Chen, X., and Hain, T. (2024). Emobox: Multilingual multi-corpus speech emotion recognition toolkit and benchmark. Maimon, G., Hassid, M., Roth, A., and Adi, Y. (2025a). Scaling analysis of interleaved speech-text language models. arXiv preprint arXiv:2504.02398. Maimon, G., Roth, A., and Adi, Y. (2025b). Salmon: suite for acoustic language model evaluation. Microsoft, :, Abouelenin, A., Ashfaq, A., Atkinson, A., Awadalla, H., Bach, N., Bao, J., Benhaim, A., Cai, M., Chaudhary, V., Chen, C., Chen, D., Chen, D., Chen, J., Chen, W., Chen, Y.-C., ling Chen, Y., Dai, Q., Dai, X., Fan, R., Gao, M., Gao, M., Garg, A., Goswami, A., Hao, J., Hendy, A., Hu, Y., Jin, X., Khademi, M., Kim, D., Kim, Y. J., Lee, G., Li, J., Li, Y., Liang, C., Lin, X., Lin, Z., Liu, M., Liu, Y., Lopez, G., Luo, C., Madan, P., Mazalov, V., Mitra, A., Mousavi, A., Nguyen, A., Pan, J., Perez-Becker, D., Platin, J., Portet, T., Qiu, K., Ren, B., Ren, L., Roy, S., Shang, N., Shen, Y., Singhal, S., Som, S., Song, X., Sych, T., Vaddamanu, P., Wang, S., Wang, Y., Wang, Z., Wu, H., Xu, H., Xu, W., Yang, Y., Yang, Z., Yu, D., Zabir, I., Zhang, J., Zhang, L. L., Zhang, Y., and Zhou, X. (2025). Phi-4-mini technical report: Compact yet powerful multimodal language models via mixture-of-loras. Minaee, S., Mikolov, T., Nikzad, N., Chenaghlu, M., Socher, R., Amatriain, X., and Gao, J. (2025). Large language models: survey. Mishra, T., Sridhar, V. R., and Conkie, A. (2012). Word prominence detection using robust yet simple prosodic features. In Interspeech 2012, pages 18641867. Morrison, M., Pawar, P., Pruyne, N., Cole, J., and Pardo, B. (2023). Crowdsourced and automatic speech prominence estimation. Nguyen, T. A., Hsu, W.-N., DAvirro, A., Shi, B., Gat, I., Fazel-Zarani, M., Remez, T., Copet, J., Synnaeve, G., Hassid, M., Kreuk, F., Adi, Y., and Dupoux, E. (2023). Expresso: benchmark and analysis of discrete expressive speech resynthesis. Panayotov, V., Chen, G., Povey, D., and Khudanpur, S. (2015). Librispeech: An asr corpus based on public domain audio books. In 2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 52065210. Pasad, A., Chou, J.-C., and Livescu, K. (2022). Layer-wise analysis of self-supervised speech representation model. 11 Poria, S., Hazarika, D., Majumder, N., Naik, G., Cambria, E., and Mihalcea, R. (2019). Meld: multimodal multi-party dataset for emotion recognition in conversations. Radford, A., Kim, J. W., Xu, T., Brockman, G., McLeavey, C., and Sutskever, I. (2022). Robust speech recognition via large-scale weak supervision. Silipo, R. and Greenberg, S. (2000). Prosodic stress revisited: Reassessing the role of fundamental frequency. Stephenson, B., Besacier, L., Girin, L., and Hueber, T. (2022). Bert, can he predict contrastive focus? predicting and controlling prominence in neural tts using language model. Szwedek, A. (1986). Linguistic Analysis of Sentence Stress. [Gunter Narr Verlag, Tubingen]. Tang, C., Yu, W., Sun, G., Chen, X., Tan, T., Li, W., Lu, L., Ma, Z., and Zhang, C. (2024). Salmonn: Towards generic hearing abilities for large language models. Van Heuven, V. (2018). Acoustic Correlates and Perceptual Cues of Word and Sentence Stress: Theories, Methods and Data, pages 1559. Wang, B., Zou, X., Lin, G., Sun, S., Liu, Z., Zhang, W., Liu, Z., Aw, A., and Chen, N. F. (2025). Audiobench: universal benchmark for audio large language models. Wang, C., Wu, A., and Pino, J. (2020). Covost 2 and massively multilingual speech-to-text translation. wen Yang, S., Chi, P.-H., Chuang, Y.-S., Lai, C.-I. J., Lakhotia, K., Lin, Y. Y., Liu, A. T., Shi, J., Chang, X., Lin, G.-T., Huang, T.-H., Tseng, W.-C., tik Lee, K., Liu, D.-R., Huang, Z., Dong, S., Li, S.-W., Watanabe, S., Mohamed, A., and yi Lee, H. (2021). Superb: Speech processing universal performance benchmark. Wilson, D. and Wharton, T. (2006). Relevance and prosody. Journal of Pragmatics, 38(10):15591579. Special Issue: Prosody and Pragmatics. Xie, Z. and Wu, C. (2024). Mini-omni: Language models can hear, talk while thinking in streaming. Yang, A., Yang, B., Hui, B., Zheng, B., Yu, B., Zhou, C., Li, C., Li, C., Liu, D., Huang, F., Dong, G., Wei, H., Lin, H., Tang, J., Wang, J., Yang, J., Tu, J., Zhang, J., Ma, J., Yang, J., Xu, J., Zhou, J., Bai, J., He, J., Lin, J., Dang, K., Lu, K., Chen, K., Yang, K., Li, M., Xue, M., Ni, N., Zhang, P., Wang, P., Peng, R., Men, R., Gao, R., Lin, R., Wang, S., Bai, S., Tan, S., Zhu, T., Li, T., Liu, T., Ge, W., Deng, X., Zhou, X., Ren, X., Zhang, X., Wei, X., Ren, X., Liu, X., Fan, Y., Yao, Y., Zhang, Y., Wan, Y., Chu, Y., Liu, Y., Cui, Z., Zhang, Z., Guo, Z., and Fan, Z. (2024a). Qwen2 technical report. Yang, Q., Xu, J., Liu, W., Chu, Y., Jiang, Z., Zhou, X., Leng, Y., Lv, Y., Zhao, Z., Zhou, C., and Zhou, J. (2024b). Air-bench: Benchmarking large audio-language models via generative comprehension. Yosha, I., Shteyman, D., and Adi, Y. (2025). Whistress: Enriching transcriptions with sentence stress detection."
        },
        {
            "title": "A Benchmark information",
            "content": "Recording information. All audio samples were recorded using Shure-SM7 microphone over RME-800 sound card in professional and acoustically treated studio. All samples were recorded in English by professional actor, who got paid more than 4 times the minimum wage. Figure 3: Categorization of sentence stress types in StressTest. Types of sentence stress in StressTest. Based on the stress types presented in Section 2, we coarsely categorize and illustrate the different types of sentence stress represented in the StressTest samples in Figure 3. As definitions of sentence stress are fluid and not strictly defined, categorization inaccuracies may be present. Human evaluation. Human annotators where requested to fill forms with 15 samples to evaluate. The form is illustrated in Figure 4."
        },
        {
            "title": "B Training data and procedure",
            "content": "B.1 Stress-17k information Table 7 features statistics about the synthetically generated dataset before and after the use of the WhiStress verifier. The number of samples in the table is then multiplied by the number of task templates presented in Section 4 resulting in 17K samples. Table 7: Dataset statistics before and after verifying with WhiStress. Statistic Full Dataset Verified Dataset # Samples (audio) # Unique Interpretations # Unique Transcriptions # Transcriptions with 2 Interpretations Gender Distribution Female Male 4400 2200 1100 1100 2122 2278 1311 931 731 200 597 13 Figure 4: Human evaluation annotation view. B.2 Training procedure During training, gradients are computed only over the models final answers. Hyperparameters for all models were chosen using grid search, taking the model to reach the best results on the validation set. Final model. The final model was trained for 2440 steps using cosine learning rate scheduler with warm-up ratio of 5%. In the first 1952 steps, we use the entire data set (stage 1), followed by continued training in the verified subset for the remaining steps (stage 2), while preserving the internal state of the scheduler. The model was finetuned on single NVIDIA L40S GPU. Training hyperparameters are summarized in Table 8. Ablation models. When trained on the verified only and the full datasets the checkpoint reaching the best performance on the validation set is chosen, while in the staged training the best checkpoint in the second stage is considered."
        },
        {
            "title": "C Additional Results",
            "content": "C.1 Extended Inputs In this section we analyze the impact of performance on providing SLMs with the audio as well as ground truth stress and optionally the ground truth transcription. This can be seen as extending table 2, helping understand if the SLMs utilize the audio input, and how much of the error has to do with transcription performance. The prompts used for this analysis are provided in Appendix D.2 and the results are provided in Table 9. 14 Table 8: Training hyperparameters and LoRA configuration. Hyperparameter Learning Rate Batch Size (per device) Gradient Accumulation Steps Value 7e-5 8 2 LoRA Configuration [Hu et al., 2021] Rank (洧) LoRA Alpha Use rslora [Kalajdzievski, 2023] Target Modules LoRA Dropout 16 32 True q_proj, v_proj 0.1 Table 9: Performance on SSR the task of StressTest using different input configurations. The input column demonstrates the information provided to the model in addition to our tasks question. This extends upon table 2. Model SLM Trans. Input Stress Audio SSR gpt-4o-audio SALMONN LLaMA-Omni Phi-4-multimodal-instruct Qwen2-Audio-7B-Instruct StresSLM (ours) gpt-4o-audio SALMONN LLaMA-Omni Phi-4-multimodal-instruct Qwen2-Audio-7B-Instruct StresSLM (ours) Human - - - 87.1 59.6 70.1 55.5 60.5 84.4 83.9 66.5 62.3 51.3 59.1 81. 92.6 (96.0) In these experiments, we test two scenarios: (i) An oracle provides the SLM with ground truth transcription and ground truth stress in addition to the provided audio; and, (ii) An oracle provides solely the ground truth stress while relying on the ability of the SLM to recognize the spoken utterance from the audio. Results suggest that generally, SLMs slightly improve their results the more information is provided to the model. An exception to this is SALMONN which shows better results when it is only provided the stress signal as opposed to stress and transcription. gpt-4o-audio [Hurst et al., 2024] shows significant performance preservation and even improvement when stress signal is given to it, compared to its text-only counterpart that as shown in Table 2 yields 86.2 on SSR. Our proposed model StresSLM that results with 81.6 using the audio signal alone, generally preserves its capabilities on SSR in both scenarios. It also demonstrates improvement in the oracle scenario when both transcription and stress are provided, perhaps due to the slight decrease in transcription abilities shown in Table 4."
        },
        {
            "title": "D Prompts",
            "content": "D.1 Benchmarking D.1.1 Sentence stress reasoning Stress Reasoning Evaluation. The following prompt is the one used to query speech-aware language models to evaluate the performance on the proposed end-to-end sentence stress reasoning task. Speech-Aware LM SSR Prompt [audio] Out of the following answers, according to the speakers stressed words, what is most likely the underlying intention of the speaker? 1. [answer 1] 2. [answer 2] Answer: Reasoning LM Judge. For the LLM-as-judge model we used the following prompt. LLM-as-judge SSR Prompt You are very good <system prompt> You are Speech-LM evaluator that helps evaluating models that have trouble in outputting correct schema for an answer. at outputting the correct schema according to the instructions. INSTRUCTIONS: Given prompt with question and possible answers that the Speech-LM received, and the output the model emmited, you are required to output the Speech-LM answer in fixed format. * The output should be aligned with the Speech-LM output, and should not include any additional information or context. * The output should be JSON object with single key \"answer\" and value that is the number of the correct answer according to the output of the Speech-LM. * The answer should be an integer, either 1 or 2. EXAMPLE 1: INPUT TO Speech-LM: According to the intonation of the speaker, what is more probable? 1. Yesterday, someone did not inform the speaker about the meeting. 2. Someone did not inform the speaker about the meeting that occurred yesterday. OUTPUT FROM Speech-LM: Someone did not inform the speaker about the meeting that occurred yesterday. than option 1. YOUR EXPECTED JSON OUTPUT: {\"answer\": 2} EXAMPLE 2: INPUT TO Speech-LM: Question: speaker, what is more probable? 1. Yesterday, someone did not inform the speaker about the meeting. 2. Someone did not inform the speaker about the meeting that occurred yesterday. OUTPUT FROM Speech-LM: Answer: the speaker about the meeting. YOUR EXPECTED JSON OUTPUT: {\"answer\": <user prompt> INPUT TO Speech-LM: [input prompt] OUTPUT FROM Speech-LM: [speech lm output] Therefore, option 2 is more probable According to the intonation of the Yesterday, someone did not inform Possible answers: 1. 1} 16 YOUR EXPECTED JSON OUTPUT: D.1.2 Sentence stress detection Stress Detection Evaluation. We use the following prompt to asses the 洧녡洧녡洧냥 task. Speech-Aware LM SSD Prompt [audio] The speaker said \"[transcription]\". According to the audio, what words did the speaker stress? Answer format: Answer: [stressed_word_1, ...] Detection LM Judge. For the LLM-as-judge model we used the following prompt. LLM-as-judge SSD Prompt <system prompt> You are Speech-LM evaluator that helps evaluating models that have trouble in outputting correct schema for an answer. You are very good at outputting the correct schema according to the instructions. INSTRUCTIONS: Given prompt with question that the Speech-LM received, and the output the model emitted, you are required to output the Speech-LM answer in fixed format. * The output should be aligned with the Speech-LM output, and should not include any additional information or context. * The output should be JSON object with single key \"answer\" and value that is list of words according to the output of the Speech-LM. * The answer should be list of strings. * If the model mistakenly outputs two or more words as single word, you should split them into separate words. EXAMPLE: INPUT TO Speech-LM: The speaker said \"What lovely day we have\". According to the audio, what words did the speaker stress? Answer format: Answer: OUTPUT FROM Speech-LM: The speaker stressed: YOUR EXPECTED JSON OUTPUT: {\"answer\": [\"lovely\", \"we\", \"have\"]} <user prompt> INPUT TO Speech-LM: [input prompt] OUTPUT FROM Speech-LM: [speech lm output] YOUR EXPECTED JSON OUTPUT: [\"lovely\", \"we have\"]. [stressed_word_1, ...] D.2 Reasoning analysis prompts Transcription and stress as input. We used the following prompt to evaluate whether additional context helps sentence stress understanding. In case the evaluated model is an LM, the audio placeholder was not used. Sentence Stress Reasoning - stress and transcription input [Audio] Question: 17 \"[transcription]\", and stressed the words: Given that speaker said: [stressed words]. Out of the following answers, what is most likely the underlying intention of the speaker? Possible answers: 1. [answer 1] 2. [answer 2] Answer: Stress as input. The prompt evaluates whether only the stressed words helps sentence stress understanding, since ASR is fundamental task for speech-aware LMs. Sentence Stress Reasoning - stress input [Audio] Question: Out of the following answers, given that the speaker stressed the words: [stressed words]. What is most likely the underlying intention of the speaker? Possible answers: 1. [answer 1] 2. [answer 2] Answer: D.3 Training prompts End-to-end task. The following prompts guide the model to precisely choose the speakers intended meaning based on stressed words. End to end reasoning [Audio] Out of the following answers, according to the speakers stressed words, what is most likely the underlying intention of the speaker? 1. [answer 1] 2. [answer 2] Answer: Expected Answer Format [answer label]. [correct answer] Elaborated answer task. These prompts require the model to first explain its reasoning and then answer. Elaborated Answer Prompt [Audio] According to the speakers stressed words, what is the speakers underlying intention? 1. [answer 1] 2. [answer 2] Elaborate, then answer in the following way: correct_answer\" \"answer_number. Expected Answer Format [description]. Therefore, the correct answer is: [correct answer] [answer label]. 18 Cascade reasoning task. These prompts encourage the model to reason based on the stressed words and transcription before answering. Stress Detection Reasoning Prompt [Audio] The speaker stressed some words. communicate? 1. [answer 1] 2. [answer 2] Think about the transcription and the stressed words. this: \"answer_number. correct_answer\" What is the speaker trying to Then, answer like Expected Answer Format (Format 7) The speaker said \"[transcription]\" and emphasized \"[stressed words]\". Therefore, the correct answer is: [correct answer] [answer label]. Stress detection task. This prompt focuses only on detecting which words were emphasized in the speech. Stress detection prompt [Audio] The speaker said \"[transcription]\". According to the audio, what words did the speaker stress? Answer format: Answer: [stressed_word_1, ...] Expected Answer Format [stressed words] D.4 Data generation pipeline D.4.1 Text sample generation Metadata. Each textual example is one of 10 sentence types: statement, question, command, exclamation, request, suggestion, invitation, offer, opinion and warning. Additionally, we prompt gpt-4o [Hurst et al., 2024] to form diverse list of domains and topics for the generation of our synthetic dataset. domain and topic are injected into the agent generation prompt out of the following list of 32 domains and 110 topics corresponding to the domains. Domains and topics Art & Culture: -The Renaissance Period -Street Art and Graffiti -The Influence of Jazz Music -The Role of Film in Society -Modern Architecture Trends -Art as Political Protest -The Influence of Surrealism -Cultural Appropriation in Fashion -Folk Music and Tradition -The Globalization of Art Markets -The Role of Museums in Preserving Culture -The Influence of Classical Music on Modern Genres -Digital Art and Its Rising Popularity -The Evolution of Dance Across Cultures -The Impact of the Internet on Art Consumption Business: -Startup Culture and Innovation -Corporate Social Responsibility -Leadership Styles and Management -The Gig Economy Economics: -Income Inequality -Global Trade Wars -Cryptocurrency and Digital Economy -The Future of Work and Automation Education: -Online Learning Platforms -Special Education Needs -The Role of Arts in Education -STEM Education for Girls Engineering: -Renewable Energy Engineering -Aerospace Innovations -Civil Engineering and Urban Planning -Robotics and Automation Environment: -Ocean Pollution and Marine Life -Conservation of Endangered Species -Urban Green Spaces -Sustainable Agriculture Practices Food & Culinary Arts: -The Science of Baking -Global Cuisine Trends -Farm-to-Table Movement -Veganism and Plant-Based Diets Health & Medicine: -Mental Health Awareness -The Rise of Telemedicine -Nutrition and Lifestyle Diseases -Vaccine Development Processes History: -The Industrial Revolution -Ancient Civilizations and Their Contributions -World Wars and Their Consequences -The History of Human Rights Law: -Intellectual Property Rights -The Justice System and Reforms -International Law and Human Rights -Cyber Law and Internet Regulations Literature: -Dystopian Novels and Society -The Impact of Shakespeare -The Evolution of Poetry -Modern Graphic Novels Philosophy: -Existentialism and Modern Thought -The Ethics of Artificial Intelligence -Eastern vs. -The Philosophy of Science Politics: -Globalization and Nationalism Western Philosophical Traditions 20 -Election Systems and Voter Rights -The Role of the United Nations -Immigration Policies and Refugee Crisis Psychology: -Cognitive Behavioral Therapy -The Psychology of Social Media -Child Development Stages -Emotional Intelligence in Leadership -The Impact of Stress on Health Religion: -Comparative Religion Studies -Secularism and Society -Rituals and Traditions -The Role of Religion in Politics Science: -CRISPR and Genetic Engineering -Climate Change and Its Impact on Biodiversity -Space Exploration and Mars Colonization -Nanotechnology in Medicine Sociology: -Urbanization and Its Challenges -The Dynamics of Family Structures -Gender Roles in Society -The Sociology of Religion Sports: -The Science of Sports Performance -Gender Equality in Sports Technology: -Artificial Intelligence and Ethics -Quantum Computing Advancements Travel & Tourism: -Eco-Tourism and Sustainability -Cultural Heritage Sites Anthropology: -Cultural Practices of Hunter-Gatherer Societies -The Evolution of Human Speech and Language Astronomy: -The Birth and Death of Stars -The Formation of Black Holes Cryptography: -The Evolution of Cryptographic Algorithms -Quantum Cryptography and Secure Communication Fashion: -The Rise of Fast Fashion and Its Environmental Impact -Fashion Icons Who Changed History Gaming: -The Psychology of Video Game Addiction -The Rise of Esports and Competitive Gaming Geopolitics: -The Role of Natural Resources in International Conflict -Geopolitical Impacts of Climate Change Mathematics: -Chaos Theory and its Applications -The Role of Mathematics in Predictive Modeling Performing Arts: -The Evolution of Contemporary Dance -The Role of Theater in Political Activism Photography: 21 -The Evolution of Documentary Photography -The Role of Photography in Social Movements Space Exploration: -The Privatization of Space Travel -The Role of Satellites in Modern Communication Visual Arts: -The Impact of Technology on Visual Arts -The Role of Photography in Modern Art Urban Studies: -The Role of Public Transportation in Urban Growth -Urbanization and Its Impact on Housing Text sample generation. The agentic process used to create the textual examples given the metadata will be open-sourced with the full yml configurations and code."
        },
        {
            "title": "E Broader impact",
            "content": "As with any advancement in language models, our work carries the potential for both positive and unintended consequences. By equipping models with the ability to interpret the meaning conveyed through stress patterns in speech, we aim to foster more efficient and nuanced communication systems. Such capabilities may particularly benefit populations who rely heavily on prosody for meaning - such as individuals with hearing disabilities, language learners, etc and ultimately contribute to more accessible and context-aware AI systems."
        }
    ],
    "affiliations": [
        "The Hebrew University of Jerusalem"
    ]
}