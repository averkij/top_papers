{
    "paper_title": "OmniCreator: Self-Supervised Unified Generation with Universal Editing",
    "authors": [
        "Haodong Chen",
        "Lan Wang",
        "Harry Yang",
        "Ser-Nam Lim"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce OmniCreator, a novel framework that can conduct text-prompted unified (image+video) generation as well as editing all in one place. OmniCreator acquires generative and universal editing capabilities in a self-supervised manner, taking original text-video pairs as conditions while utilizing the same video as a denoising target to learn the semantic correspondence between video and text. During inference, when presented with a text prompt and a video, OmniCreator is capable of generating a target that is faithful to both, achieving a universal editing effect that is unconstrained as opposed to existing editing work that primarily focuses on certain editing types or relies on additional controls (e.g., structural conditions, attention features, or DDIM inversion). On the other hand, when presented with a text prompt only, OmniCreator becomes generative, producing high-quality video as a result of the semantic correspondence learned. Importantly, we found that the same capabilities extend to images as is, making OmniCreator a truly unified framework. Further, due to the lack of existing generative video editing benchmarks, we introduce the OmniBench-99 dataset, designed to evaluate the performance of generative video editing models comprehensively. Extensive experiments demonstrate that OmniCreator exhibits substantial superiority over all other models."
        },
        {
            "title": "Start",
            "content": "OmniCreator: Self-Supervised Unified Generation with Universal Editing Haodong Chen1,2 Lan Wang4 Harry Yang1,2 Ser-Nam Lim1,3 1Everlyn AI 2HKUST 3UCF 4MSU Project page:https://haroldchen19.github.io/OmniCreator-Page/ 4 2 0 2 3 ] . [ 1 4 1 1 2 0 . 2 1 4 2 : r Figure 1. OmniCreator enables universal text-guided video editing across four distinct editing types (top) and eight different scenarios (bottom). For comprehensive definitions of the editing types and scenarios, please refer to App. A."
        },
        {
            "title": "Abstract",
            "content": "We introduce OmniCreator, novel framework that can conduct text-prompted unified (image+video) generation as well as editing all in one place. OmniCreator acquires generative and universal editing capabilities in self-supervised manner, taking original text-video pairs as conditions while utilizing the same video as denoising target to learn the semantic correspondence between video and text. During inference, when presented with text prompt and video, OmniCreator is capable of generating target that is faithful to both, achieving universal editing effect that is unconstrained as opposed to existing editing work that primarily focuses on certain editing types or relies on additional controls (e.g., structural conditions, attention features, or DDIM inversion). On the other hand, when presented with text prompt only, OmniCreator becomes generative, producing high-quality video as result of the semantic correspondence learned. Importantly, we found that the same capabilities extend to images as is, making OmniCreator truly unified framework. Further, due to the lack of existing generative video editing benchmarks, we introduce the OmniBench-99 dataset, designed to evaluate the performance of generative video editing models 1 comprehensively. Extensive experiments demonstrate that OmniCreator exhibits substantial superiority over all other models. 1. Introduction Generative artificial intelligence has opened up new possibilities for creative content generation, allowing for seamless creation and modification. Key developments in textto-image (T2I) generation [89, 92, 94] have laid the foundation for innovative editing techniques, enabling text-based manipulation of images [7, 38, 73] and videos [2, 59, 114]. Controllable video editing faces the challenge of maintaining cross-frame consistency. Frame-by-frame editing approaches like utilizing IP2P [7] often produce inconsistencies [109]. Keyframe-based methods improve coherence by editing select frames and interpolating others [2, 8, 30, 51, 57, 59, 63, 68, 80, 85, 109], but still suffer from low editing quality and restricted scope. To enhance editing precision, Tune-A-Videos [114] one-video tuning paradigm advanced fine-tuning on specific videos, but often struggles with generalization [10, 68, 97, 101, 133 135]. Structural conditions (e.g., edge, depth maps) [2, 22, 26, 59, 63, 65, 110, 120, 123, 134], attention feature [1, 27, 55, 61, 64, 68, 108, 113], or DDIM inversionbased [22, 30, 52, 61, 66, 68, 97, 104, 114, 132] methods can further refine edits but reduce model flexibility. Additionally, the IP2P paradigm promotes generating video-textvideo pairs for training [18, 86], though dataset creation remains resource-intensive.* All the above observations motivate us to explore more elegant and generalizable way for text-based generative video editing. To this end, we introduce OmniCreator, framework for learning the intrinsic semantic correspondence between original text-video pairs. In this setup, text provides textual semantics while video delivers visual semantics. By conditioning the denoising process on both text and video embeddings while at the same time using the exact same video as denoising target, the model learns to associate these semantics in structured manner. Very importantly, we note that this approach is self-supervised, serving to avoid tedious and large-scale annotation costs, and is yet surprisingly effective. Intuitively, this makes sense since in principle the model, when given the original text-video pair, should generate something close to the same video. Further, once the semantic correspondence is learned, the model can interpret any text and video conditions and generate the correct target video, thus achieving truly universal editing (see Fig. 1) without being restricted to certain editing types. To implement OmniCreator, we introduce key architectural innovations. Here we seek to solve two key chal- *For comprehensive editing capability survey, please refer to Tab. 5 in App. A. Figure 2. OmniCreator samples for text-to-video generation. OmniCreator not only enables universal video editing but also generates high-quality videos from text prompts. lenges, namely that (1) aligning text and video is non-trivial due to the added temporal complexity, and (2) effectively integrating video embeddings into the denoising network (i.e., U-Net [93]) in way that allows the network to interpret video in conjunction with text embeddings better is also not straightforward. We propose to overcome (1) and (2) as follows. Specifically, to preserve alignment between CLIPs [88] visual and textual embeddings while adapting it to video inputs, we introduce an adapter into the CLIP image encoder to capture temporal dynamics. To accommodate the new denoising condition in U-Net, we employ query transformer for enhanced multimodal fusion inspired by [119]. Additionally, to reduce computational complexity, we apply low-rank adaption (LoRA) [44] to both spatial and temporal layers (see Fig. 3). These components enable OmniCreator to internalize the semantic relationship between text and video, preserving global visual semantics while introducing controlled edits via local textual semantics (i.e., the delta text prompts). number of pleasant surprises were also discovered along the way that turned OmniCreator into truly unified (image+video) framework for both editing and generation. Through our self-supervised semantic correspondence learning, we found that OmniCreator can also achieve highquality text-to-video as well as text-to-image generation using only full-sentence prompts, where the text provides the global textual semantics in the absence of video reference (see Fig. 2). We further discover that OmniCreator had also acquired generative image editing capability without any further training. In summary, our contributions are as follows: We propose OmniCreator, unified framework enabling both universal editing and generation in self-supervised manner. To our knowledge, it marks pioneering leap in text-guided controllable image and video generation; We explore adopting the same video of denoising target as We call our target text the delta prompt to distinguish it from the widely used full-sentence prompt (see the bottom of Fig. 5 for examples). 2 Figure 3. Overview of OmniCreator. During training, the original video also serves as condition of the denoising process. To enable temporal information understanding, we incorporate an adapter. Additionally, we utilize query transformer to effectively align video and text embeddings, which aids in the denoising process. For computational efficiency, LoRAs are integrated into the denoising U-Net. During inference, OmniCreator enables universal video editing by adopting reference video alongside an editing text prompt. condition for the first time, effectively learning the semantic correspondence between text and video, enabling more flexible and versatile editing capabilities; We also introduce OmniBench-99, comprehensive evaluation benchmark that not only assesses various editing types but also uniquely evaluates editing scenarios, setting new standard for video editing research. Existing generative video editing benchmarks [26, 83, 115] primarily focus on four types of edits (see the top of Fig. 1) but overlook the evaluation of editing scenarios. OmniBench-99 dataset addresses this gap, which includes 99 diverse videos, each with four prompts for the four editing types. Furthermore, the videos are evenly distributed across three categories: human/animal, environment, and object. Each video is also paired with three to four prompts representing different editing scenarios; Extensive experiments show OmniCreators superior performance over state-of-the-art methods, highlighting its potential to advance the field of controllable generation. 2. Related Work Diffusion-based Text-to-Video Generation. Recent advancements in diffusion models (DMs) [40, 100] have showcased their impressive ability in T2I generation [78, 89, 92, 94]. To extend this success to video, the first video diffusion model (VDM) [42] was introduced, utilizing space-time factorized U-Net [93] for effective lowresolution video modeling in pixel space. Following this, Imagen-Video [41] developed cascaded DMs employing vprediction to generate high-definition videos. To enhance training efficiency, subsequent research [5, 36, 107, 111] has focused on transferring T2I concepts to T2V generation [28, 69, 98], along with the development of VDMs in latent or hybrid pixel-latent spaces. Many studies [59, 72, 110, 120] have incorporated various controls to achieve more precise generation in specific scenarios, though often with additional supervision. However, achieving generalizable T2V generation [4, 11, 15, 62, 71] has proven more effective through training with raw text-video pairs or joint training with additional text-image pairs. Building on their success, we further explore the potential of raw text-video pairs for more generalizable and controllable video and image generation. Diffusion-based Text-guided Video Editing. Video editing can be viewed as controllable video generation, with reference video serving as condition during inference. Drawing inspiration from image editing [7, 21, 73], several studies [2, 9, 26, 59, 63, 113, 125] employ off-the-shelf image-to-image (I2I) models to edit video frames directly or edit keyframes first to ensure temporal consistency. The introduction of one-video tuning by TAV [114] has prompted research [8, 54, 68, 80, 130, 133] into fine-tuning T2V models for specific video structures, although this may limit usability and practical applications (see Tab. 5). To improve editing precision, many studies introduce additional control mechanisms [103], categorized into three types: 1) Structural conditions, which utilize elements like human pose [72, 104, 132], optical flow [52, 65, 125], depth 3 maps [56, 110, 120], and masks [45, 59, 134]; 2) Attention features [64, 85, 113, 126], where hidden features are manipulated for editing, as seen in Video-P2Ps [68] dualbranch framework and TokenFlows [30] spatiotemporal attention; and 3) DDIM inversion, which provides structural guidance during inference [8, 67, 85, 114, 121]. While these methods offer promising approaches to video editing, their reliance on additional controls can limit the models versatility and precision in specific edits. Moreover, many existing models [17, 56, 75, 90] primarily enable overall editing rather than precise editing, e.g., selectively editing foregrounds or backgrounds. In this work, we propose more elegant pipeline to achieve universal video and image editing and generation by learning the semantic correspondence within original text-video pairs in self-supervised manner. 3. Method 3.1. Preliminary: Video Diffusion Models Diffusion Models (DMs) [40, 100] are probabilistic models that gradually corrupt data x0 pdata(x) into Gaussian noises xT (0, I) through forward process, and learn to reverse this process by denoising. The forward process q(xtx0, t), defined over timesteps, progressively adds noise to the data sample x0 to yield xt through parameterization trick. The denoising process pθ(xt1xt, t) denoises the noisy input xt to obtain cleaner data xt1 through denoising network ϵθ (xt, t), which is supervised by: min θ Et,xpdata,ϵN (0,I)ϵ ϵθ (xt; c, t) 2 2, (1) where ϵ refers to the sampled ground truth noise, θ represents the learnable parameters, and indicates possible conditions. Once trained, the model generates denoised data x0 by iteratively denoising random noise xT . Latent Diffusion Models (LDMs) [41, 92] improve the computational efficiency by working in learned compact latent space instead of the original pixel space. For given video RL3HW , we first encode it into latent representation = E(x) via an autoencoder E, where RLChw. The diffusion process zt = p(z0, t) and denoising process zt = pθ(zt1, c, t) are then performed in the latent space. Finally, the generated videos are obtained through the decoder ˆx = D(z). 3.2. OmniCreator The primary objective of our work is to enable unified (image+video) generation and universal editing. While prior arts have made significant strides in controlled editing within specific scenarios by introducing additional controls, achieving more generalizable remains more complex endeavor. To tackle this, OmniCreator adopts the original video as new denoising condition, coupled with the origiFigure 4. Illustration of the alignment between video and text embeddings. We utilize Euclidean Distance (left, blue) and Cosine Similarity (right, red) to evaluate the impact of the Adapter (Ada.) and Query Transformer (Query) on the embedding alignment. nal text caption, to harness the semantic correspondence between both in self-supervised manner, as shown in Fig. 3. Video Embedding: New Condition. Unlike previous works that impose explicit structural constraints (e.g., depth maps [26, 110], edges [9, 132], etc.) for additional supervision, which can limit the flexibility of video editing, we introduce the same video of the denoising target as new denoising condition. This allows the model to learn the videos structure at semantic level through self-supervised training, jointly conditioning on both video and text. To achieve this, we leverage CLIPs [88] powerful encoders that have been pretrained with large amount of data to achieve text-image alignment. The tricky part, however, lies in how to encode videos with these encoders. naive approach is to apply temporal aggregation (e.g., average pooling) on per-frame features, but this often leads to information loss in temporal dynamics. Although using video ViT [3] could capture temporal information more effectively, it would be computationally costly [112]. To address this, we insert lightweight adapter into the CLIP image encoder to handle temporal aspects of video with minimal computational overhead. This adapter, as illustrated in Fig.3, is implemented as depth-wise 3D convolutional layer (DWConv3D)[25], inspired by [81]. The adapted feature Xada is computed as: Xada = + (DWConv3D(XWdown))Wup, (2) where initially represents the patch and positional embedding feature of input video x, () is the activation function, and Wdown, Wup are the downand up-projection layers, respectively. Additionally, inspired by image-to-video (I2V) works [11, 119] that project image embedding into text-aligned space, we adopt similar idea for video embedding to improve cross-modal compatibility. Specifically, we use the full set of video tokens Evid = {ei}K i=1 from the last transformer layer, rather than relying solely on the global semantic token ecls [96, 128], to ensure comprehensive visual representation. To align video and text embeddings within the denoising network, we employ query transformer [49, 50] 4 Figure 6. Statistics of OmniBench-99. Building upon this foundation, we explore how global visual semantics from the reference video and textual semantics interact. Specifically, as shown in the third block in Fig. 5, full-sentence prompts for global textual semantics may clash with the reference videos global visual semantics due to inherent ambiguities. Conversely, delta prompts focus on local textual semantics, aligning effectively with the videos global visual structures. This demonstrates the effectiveness of our semantic correspondence strategy: aligning text and video at complementary levels enables high-quality generation and editing, adapting seamlessly to diverse input types under self-supervised conditions. Spatiotemporal Low-Rank Adaptations. To reduce the computational cost of training the video model, we inject LoRAs [44] into both the spatial and temporal layers of the denoising U-Net (see Fig. 3 right side), common strategy in generation tasks [4, 31, 133]. Particularly, we do not modify the cross-attention layers to preserve the learning of text-video semantic correspondence. Instead, LoRAs are applied to the spatial and temporal self-attention layers as well as the feed-forward networks, updating correlations across both dimensions. Formally, LoRA updates the weight matrix via low-rank factorization: = W0 + = W0 + BA, (5) where W0 Rdk represents the original weights, and Rdr and Rrk are the low-rank factors, with much smaller than and k. This enables efficient training and fine-tuning of our video editing model. Figure 5. Illustration of semantic correspondence. Top: Reference video and its original caption. Middle: Results using only one condition. Bottom: Effects of full sentence vs. delta prompt. consisting of stacked cross-attention and feed-forward networks. This enables the U-Net to interpret video embeddings effectively within the denoising process. Fig. 4 further illustrates how text and video embeddings are progressively aligned, making the full OmniCreator achieve better performance. For video examples, please see Fig. 11. Text-Video Semantic Correspondence. After projecting the video condition and text condition into intermediate representations Evid and Etxt, dual-path cross-attention layer is introduced to incorporate these conditions into the UNets intermediate layers: Softmax( QK txt )Vtxt + Softmax( QK vid )Vvid, (3) where the queries derive from an intermediate U-Net representation Elat via = W(i) Elat. The keys and values for text and video conditions are calculated as follows: (cid:40) Ktxt = W(i) Kvid = W(i) Etxt, Vtxt = W(i) Evid, Vvid = W(i) Etxt Evid . (4) Inference and Applications. The classifier-free guidance [39] is naturally extended to multi-conditional during inference, i.e., text condition ctxt and video condition cvid: Here, W(i) , W(i) , W(i) are learnable projection matrices. Fig. 5 illustrates how semantic correspondence operates in our framework. The middle block shows results with only one condition applied from the first block: text-only generation adheres to the caption, but the inherent ambiguity in textual semantics often fails to capture specific video details, limiting effective global visual modeling. In contrast, video-only generation reconstructs visual elements more accurately, as the reference video supplies comprehensive global visual semantics. ϵθ(zt; t, cvid, ctxt) ϵθ(zt; t, , ) + wvid + wtxt (cid:0)ϵθ(zt; t, cvid, ) ϵθ(zt; t, , )(cid:1) (cid:0)ϵθ(zt; t, cvid, ctxt) ϵθ(zt; t, cvid, )(cid:1) (6) where wvid and wtxt denote the guidance scales for video and text conditions, respectively. OmniCreator enables unified text-based generation and editing of images and videos. ❶ Video: For editing, we set wvid and wtxt to specific values, allowing for semantic 5 Figure 7. Video editing comparison with baselines. We follow the baselines prompt setting but only show delta prompts here. Due to space constraints, we only compare editing scenarios with TokenFlow and CCEdit, for complete comparisons, please refer to App. G.2. composition of the reference video and delta prompt. For generation, we use only full-sentence prompts to provide global semantics, setting wvid = 0 to exclude the reference video. ❷ Image: Recognizing that an image is single frame from video, we construct the reference video using identical reference images and sample the initial frame as the output image for both editing and generation. 4. OmniBench-99 Overview. Generative video editing has emerged as rapidly growing research area, yet it still lacks comprehensive benchmark, potentially hindering its technical advancement. Although the recently introduced BalanceCC [26] organizes videos into four categories, i.e., humans, animals, objects, and landscapes, it only evaluates four editing types following the LOVEU-TGVE-2023 [115] benchmark. This limited scope overlooks the importance of assessing editing scenarios. To this end, we present OmniBench-99, new benchmark of 99 videos with text prompts that evaluate both editing types and scenarios, providing more comprehensive benchmark for generative video editing evaluation. Establishment and Statistics. We collected 99 opentheir suitability for nonlicense videos, selected for stigmatizing and legal modifications. These videos range in length from 2 to 20 seconds, with frame rate of about 30 FPS. The videos are evenly distributed across three categories: Human/Animal, Environment, and Object, with In addition to generating four 33 videos per category. editing-type prompts for each video [26], we tasked GPT4V [79] to create category-specific prompts tailored to different editing scenarios (see Fig. 1 for examples). Notably, we provide two kinds of prompts, namely full sentence and delta caption, for convenient use. Afterward, we conducted thorough manual inspection to ensure the quality of these prompts. We present the statistical distribution of OmniBench-99 in Fig. 6. We hope this new benchmark will better address gaps in previous research and provide reliable standard for generative video editing. More details and visual examples are available in App. D. 5. Experiment In this section, we will describe in detail how we evaluate OmniCreator. As mentioned, OmniCreator is capable of both video and image universal editing and generation. The next few sections will demonstrate the superiority of OmniCreator in all four tasks. Figure 9. Qualitative results of OmniCreator T2V samples. Figure 8. Image editing comparison with baselines. 5.1. Implementation Details Training and Dataset. OmniCreator is developed based on the I2V model DynamiCrafter [119] and the T2I model Stable-Diffusion-v2.1 [92]. We adopt the newly proposed high-quality T2V dataset, OpenVidHD-0.4M [77]. Our OmniCreator is fine-tuned based on [119] for 12K iterations, sampling 16 frames per video with dynamic FPS, with resolution 320 576. We employ batch size of 32 and learning rate of 1 105. Video Evaluation. ❶ Automatic Metrics: For editing, we use PickScore [60] to evaluate the average alignment between all frames of the output video and the corresponding edited prompt. We also apply CLIP Frame (Frame Consistency) [88] to assess the average cosine similarity between CLIP image embeddings on all frames. For generation, we compute Frechet Video Distance (FVD) [105] on the UCF-101 [102] and CLIP Similarity [88] on the MSR-VTT [122]. ❷ User Study: We utilize mean opinion score (MOS) as our metric and focus on four aspects: ① Text Alignment, ② Temporal Consistency, ③ Structure Alignment (for editing only) and ④ Overall Quality. ❸ Benchmarks: For editing, We evaluate video editing using our newly constructed OmniBench-99 dataset, covering both editing types and scenarios. For generation, we evaluate on UCF-101 and MSR-VTT benchmarks. ❹ Baselines: For editing, we Figure 10. Qualitative results of OmniCreator T2I samples. compare OmniCreator with state-of-the-art methods: TuneA-Video [114], Pix2Video [8], ControlVideo [132], TokenFlow [30], InsV2V [18], Video-P2P [68], and CCEdit [26]. For generation, we adopt LaVie [111], CogVideo (En) [43], CogVideoX 5B [127], and HiGen [87] as baselines. We follow the default settings officially. Detail information please refer to App. E. Image Evaluation. ❶ Benchmark and Metrics: For editing, we adopt EditEval [48], comprehensive image editing evaluation benchmark with LMM score as metrics covering seven types of editing (i.e., addition, replacement, removal, background, style, texture, action). For generation, we evaluate via compositionality on T2I-CompBench benchmark [47]. ❷ Baselines: For editing, we compare OmniCreator with Null-Text [74], Disentangle [116], IP2P [7], Imagic [58], ProxEdit [32], LEDITS++ [6], and InstructDiffusion [29]. For generation, we employ SD-v2 [91], GORS [47], SDXL [84], and PixArt-α [14]. 5.2. Qualitative Evaluation Due to limitations in most PDF readers, videos may not render correctly. Therefore, we present small number of frames from each video and highly recommend readers visit our video demo for full video demonstrations. ❶ Editing: We first demonstrate video editing examples of our OmniCreator in Fig. 1, illustrating its universal editing capabilities. OmniCreator consistently performs well across four editing types and ten scenarios, spanning three video content categories. To further demonstrate OmniCreators efTable 1. Quantitative comparison with text-guided video editing methods on our OmniBench-99. User study includes text alignment (Align.), temporal consistency (Temp.), structure alignment (Stru.), and overall quality (Overall). We also conduct quantitative evaluation on the BalanceCC [26] and LOVEU-TGVE-2023 [115] benchmarks in App. G.1. Editing Type Eidting Scenario Method Automatic User Study Automatic User Study CLIP Frame Pick Score Align. Temp. Stru. Overall CLIP Frame Pick Score Align. Temp. Stru. Overall Tune-A-Video [114] ICCV23 Pix2Video [8] ICCV23 ControlVideo [132] ICLR24 TokenFlow [30] ICLR24 InsV2V [18] ICLR24 Video-P2P [68] CVPR24 CCEdit [26] CVPR24 OmniCreator (Ours) 0.931 0.929 0.949 0.948 0.914 0.930 0.932 0.962 0.205 0.203 0.210 0.208 0.208 0.198 0.210 0.212 3.07 3.60 2.93 2.73 2.13 3.13 1.73 4.47 2.87 3.20 2.27 3.33 2.20 3.27 2.53 4. 3.13 3.27 2.40 2.80 2.33 3.20 2.27 4.07 3.20 3.33 2.40 3.07 2.47 3.00 2.20 4.33 0.929 0.927 0.950 0.951 0.911 0.928 0.935 0.966 0.198 0.199 0.203 0.200 0.198 0.189 0.204 0.216 3.33 3.93 1.80 3.07 1.73 3.13 1.53 4.07 2.93 3.53 1.87 3.07 1.93 3.20 2.53 4. 3.00 3.27 2.13 2.93 1.87 3.13 2.20 4.20 3.13 3.47 2.27 3.13 2.00 3.07 2.20 4.00 Table 2. Quantitative comparison with text-guided image editing methods. Details regarding metrics are available in App. E.3. Method Addition Background Mean Devia. Mean Devia. Mean Devia. Mean Devia. Mean Devia. Mean Devia. Mean Devia. Replacement Removal Texture Action Style Null-Text [74] CVPR23 Disentangle [116] CVPR23 InstructPix2Pix [7] CVPR23 Imagic [58] CVPR23 ProxEdit [32] WACV24 LEDITS++ [6] CVPR24 InstructDiffusion [29] CVPR24 OmniCreator (Ours) - 6.14 6.88 7.80 7.06 6.74 7.59 7.63 - 1.74 2.31 1.27 1.53 1.72 1.89 1.79 8.15 7.66 5.00 7.22 7.53 7.41 6.55 8.49 0.80 1.41 1.95 1.65 1.63 1.86 1.46 0.96 - - - - 7.75 8.65 7.48 8.33 - - - - 1.26 1.29 1.68 1. 7.28 - 6.51 - 6.35 6.91 - 7.40 1.17 - 2.49 - 0.78 0.97 - 0.81 7.16 - 8.21 7.26 6.80 6.86 7.41 8.22 0.90 - 0.40 1.25 1.07 1.20 0.66 0.45 6.82 6.78 6.10 5.57 - - 7.13 6.99 1.44 1.07 1.41 1.49 - - 1.83 1. - - - 6.97 - - - 7.53 - - - 0.80 - - - 1.11 Table 3. Quantitative comparison with T2V generation models using FVD on UCF-101 and CLIPSIM on MSR-VTT. Table 4. Quantitative comparison with T2I generation models on T2I-CompBench."
        },
        {
            "title": "Method",
            "content": "LaVie [111] arXiv23 CogVideo (En) [43] ICLR23 CogVideoX 5B [127] arXiv24 HiGen [87] CVPR24 OmniCreator (Ours)"
        },
        {
            "title": "User Study",
            "content": "FVD CLIPSIM Align. Temp. Overall 526.30 701.59 - 497.21 332.42 0.2949 0.2631 - 0.2947 0.3102 3.33 2.00 4.13 3.20 4.13 3.20 1.87 4.07 2.87 4.07 3.93 1.87 4.07 3.93 4.20 fectiveness, we compare state-of-the-art video and image editing models, as shown in Fig. 7 and Fig. 8. All these results confirm the universal and faithful editing capability of our OmniCreator. ❷ Generation: In addition to its universal editing capabilities, our OmniCreator can also achieve highquality text-to-video and -image generation. As demonstrated in Fig. 2 and Fig. 9, OmniCreator is able to generate wide variety of temporally consistent photorealistic videos that align with the textual prompt only. Fig. 10 also clearly demonstrates OmniCreators superior T2I generation capability. We provide more qualitative results in App. G.2. We have also conducted additional ablations on the importance of the adapter and query transformer (see Fig. 3) as well as the effect of LoRA ranks and the multimodal guidance scales. Due to space constraints, these results are deferred to App. (Fig. 11, Fig. 12, and Fig. 13). 5.3. Quantitative Evaluation User studies and automatic metrics serve equally important roles in generative works. Our user study evaluates four key aspects: Text Alignment, Temporal Consistency, Structural Alignment (for editing only), and Overall Quality. Feedback was gathered from 15 volunteers who rated each aspect on five-point scale (15). ❶ Editing: Building on our newly developed OmniBench-99 benchmark, we conduct video quantitative comparisons with baseline methods using"
        },
        {
            "title": "Object Relationship",
            "content": "Color Shape Texture Spatial Non-Spatial Stable v2 [91] CVPR22 0.5065 0.6603 GORS [47] NeurIPS23 SDXL [84] ICLR24 0.6369 PixArt-α [14] CVPR24 0.6886 0.6792 OmniCreator (Ours) 0.4221 0.4785 0.5408 0.5582 0.5621 0.4922 0.6287 0.5637 0.7044 0.7103 0.1342 0.1815 0.2032 0.2082 0.2039 0.3096 0.3193 0.3110 0.3179 0. both automatic metrics (i.e., CLIP Frame and PickScore) and user studies. We also conduct image comparisons using the LMM score following EditEval [48] benchmark. As shown in Tab. 1 and Tab. 2, OmniCreator outperforms baseline approaches without requiring additional controls, further validating its flexibility and generalizability. ❷ Generation: We also quantitatively assess the performance of our OmniCreator T2V generation using FVD and CLIPSIM, and T2I generation using compositionality score. As illustrated in Tab. 3 and Tab. 4, our method demonstrates superior performance, further establishing OmniCreator as robust baseline for both generation and editing future works. Detailed information on user study is in App. E.2. 6. Conclusion This paper presents OmniCreator, self-supervised framework for both high-quality text-to-video generation and universal text-guided video editing, covering diverse editing types and scenarios. The key sight of this work is leveraging the original video as novel denoising condition, learning the semantic correspondence between video and text. To address the current gap in evaluating generative video editing methods, we introduce the meticulously curated OmniBench-99 benchmark, designed for comprehensive evaluation across both editing types and scenarios. This paper further discovers that OmniCreator can be extended to the image modality, as is, for both generative and editing tasks. We hope that our efforts will advance research in the field of universal content editing. More discussions about limitations are available in App. F."
        },
        {
            "title": "References",
            "content": "[1] Jianhong Bai, Tianyu He, Yuchi Wang, Junliang Guo, Haoji Hu, Zuozhu Liu, and Jiang Bian. Uniedit: unified tuningfree framework for video motion and appearance editing. arXiv preprint arXiv:2402.13185, 2024. 2, 16 [2] Omer Bar-Tal, Dolev Ofri-Amar, Rafail Fridman, Yoni Kasten, and Tali Dekel. Text2live: Text-driven layered image and video editing. In European conference on computer vision, pages 707723. Springer, 2022. 2, 3, 16 [3] Gedas Bertasius, Heng Wang, and Lorenzo Torresani. Is space-time attention all you need for video understanding? In ICML, page 4, 2021. 4 [4] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. 3, 5 [5] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis. Align your latents: High-resolution video syntheIn Proceedings of the sis with latent diffusion models. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2256322575, 2023. [6] Manuel Brack, Felix Friedrich, Katharia Kornmeier, Linoy Tsaban, Patrick Schramowski, Kristian Kersting, and Apolinario Passos. Ledits++: Limitless image editing using text-to-image models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 88618870, 2024. 7, 8 [7] Tim Brooks, Aleksander Holynski, and Alexei Efros. Instructpix2pix: Learning to follow image editing instrucIn Proceedings of the IEEE/CVF Conference on tions. Computer Vision and Pattern Recognition, pages 18392 18402, 2023. 2, 3, 7, 8, 15, 16, 17, 19 [8] Duygu Ceylan, Chun-Hao Huang, and Niloy Mitra. In ProPix2video: Video editing using image diffusion. ceedings of the IEEE/CVF International Conference on Computer Vision, pages 2320623217, 2023. 2, 3, 4, 7, 8, 16, 19, 20 [9] Wenhao Chai, Xun Guo, Gaoang Wang, and Yan Lu. Stablevideo: Text-driven consistency-aware diffusion video In Proceedings of the IEEE/CVF International editing. Conference on Computer Vision, pages 2304023050, 2023. 3, 4, 16 [10] Shao-Yu Chang, Hwann-Tzong Chen, and Tyng-Luh Liu. Diffusionatlas: High-fidelity consistent diffusion video editing. arXiv preprint arXiv:2312.03772, 2023. 2, 16 [11] Haoxin Chen, Menghan Xia, Yingqing He, Yong Zhang, Xiaodong Cun, Shaoshu Yang, Jinbo Xing, Yaofang Liu, Qifeng Chen, Xintao Wang, et al. Videocrafter1: Open diffusion models for high-quality video generation. arXiv preprint arXiv:2310.19512, 2023. 3, [12] Haodong Chen, Haojian Huang, Junhao Dong, Mingzhe Zheng, and Dian Shao. Finecliper: Multi-modal finegrained clip for dynamic facial expression recognition with In Proceedings of the 32nd ACM International adapters. Conference on Multimedia, pages 23012310, 2024. 18 [13] Haodong Chen, Yongle Huang, Haojian Huang, Xiangsheng Ge, and Dian Shao. Gaussianvton: 3d human virtual try-on via multi-stage gaussian splatting editing with image prompting. arXiv preprint arXiv:2405.07472, 2024. 18 [14] Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, and Zhenguo Li. Pixart-α: Fast training of diffusion transformer for photorealistic text-to-image synthesis, 2023. 7, 8 [15] Shoufa Chen, Mengmeng Xu, Jiawei Ren, Yuren Cong, Sen He, Yanping Xie, Animesh Sinha, Ping Luo, Tao Xiang, and Juan-Manuel Perez-Rua. Gentron: Diffusion transformers for image and video generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 64416451, 2024. 3 [16] Ting-Hsuan Chen, Jiewen Chan, Hau-Shiang Shiu, ShihHan Yen, Chang-Han Yeh, and Yu-Lun Liu. Narcan: Natural refined canonical image with integration of diffusion prior for video editing. arXiv preprint arXiv:2406.06523, 2024. 16 [17] Weifeng Chen, Yatai Ji, Jie Wu, Hefeng Wu, Pan Xie, Jiashi Li, Xin Xia, Xuefeng Xiao, and Liang Lin. Control-avideo: Controllable text-to-video generation with diffusion models. arXiv preprint arXiv:2305.13840, 2023. 4, 16 [18] Jiaxin Cheng, Tianjun Xiao, and Tong He. Consistent arXiv video-to-video transfer using synthetic dataset. preprint arXiv:2311.00213, 2023. 2, 7, 8, 16, 19, 20 [19] Ernie Chu, Shuo-Yen Lin, and Jun-Cheng Chen. Video controlnet: Towards temporally consistent synthetic-to-real video translation using conditional image diffusion models. arXiv preprint arXiv:2305.19193, 2023. 16 [20] Yuren Cong, Mengmeng Xu, Christian Simon, Shoufa Chen, Jiawei Ren, Yanping Xie, Juan-Manuel Perez-Rua, Bodo Rosenhahn, Tao Xiang, and Sen He. Flatten: optical flow-guided attention for consistent text-to-video editing. arXiv preprint arXiv:2310.05922, 2023. [21] Guillaume Couairon, Jakob Verbeek, Holger Schwenk, and Matthieu Cord. Diffedit: Diffusion-based semanarXiv preprint tic image editing with mask guidance. arXiv:2210.11427, 2022. 3 [22] Paul Couairon, Clement Rambour, Jean-Emmanuel Haugeard, and Nicolas Thome. Videdit: Zero-shot and spatially aware text-driven video editing. Transactions on Machine Learning Research, 2023. 2, 16 [23] Patrick Esser, Johnathan Chiu, Parmida Atighehchian, Jonathan Granskog, and Anastasis Germanidis. Structure and content-guided video synthesis with diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 73467356, 2023. 16 [24] Xiang Fan, Anand Bhattad, and Ranjay Krishna. Localized semantic video editing with Videoshop: 9 noise-extrapolated diffusion inversion. arXiv:2403.14617, 2024. 16 arXiv preprint fidelity video generation with arbitrary lengths. preprint arXiv:2211.13221, 2(3):4, 2022. 3 arXiv [25] Christoph Feichtenhofer. X3d: Expanding architectures In Proceedings of the for efficient video recognition. IEEE/CVF conference on computer vision and pattern recognition, pages 203213, 2020. 4 [26] Ruoyu Feng, Wenming Weng, Yanhui Wang, Yuhui Yuan, Jianmin Bao, Chong Luo, Zhibo Chen, and Baining Guo. Ccedit: Creative and controllable video editing via diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6712 6722, 2024. 2, 3, 4, 6, 7, 8, 16, 18, 19, 20, 21 [27] Yutang Feng, Sicheng Gao, Yuxiang Bao, Xiaodi Wang, Shumin Han, Juan Zhang, Baochang Zhang, and Angela Yao. Wave: Warping ddim inversion features for zero-shot text-to-video editing. 2, 16 [28] Songwei Ge, Seungjun Nah, Guilin Liu, Tyler Poon, Andrew Tao, Bryan Catanzaro, David Jacobs, Jia-Bin Huang, Ming-Yu Liu, and Yogesh Balaji. Preserve your own correlation: noise prior for video diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2293022941, 2023. 3 [29] Zigang Geng, Binxin Yang, Tiankai Hang, Chen Li, Shuyang Gu, Ting Zhang, Jianmin Bao, Zheng Zhang, Houqiang Li, Han Hu, et al. Instructdiffusion: generalist modeling interface for vision tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1270912720, 2024. 7, 8 [30] Michal Geyer, Omer Bar-Tal, Shai Bagon, and Tali Dekel. Tokenflow: Consistent diffusion features for consistent video editing. arXiv preprint arXiv:2307.10373, 2023. 2, 4, 7, 8, 16, 19, [31] Yuwei Guo, Ceyuan Yang, Anyi Rao, Zhengyang Liang, Yaohui Wang, Yu Qiao, Maneesh Agrawala, Dahua Lin, and Bo Dai. Animatediff: Animate your personalized textto-image diffusion models without specific tuning. arXiv preprint arXiv:2307.04725, 2023. 5 [32] Ligong Han, Song Wen, Qi Chen, Zhixing Zhang, Kunpeng Song, Mengwei Ren, Ruijiang Gao, Anastasis Stathopoulos, Xiaoxiao He, Yuxiao Chen, et al. Proxedit: Improving tuning-free real image editing with proximal guidance. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 42914301, 2024. 7, 8 [33] Yue Han, Junwei Zhu, Keke He, Xu Chen, Yanhao Ge, Wei Li, Xiangtai Li, Jiangning Zhang, Chengjie Wang, and Yong Liu. Face adapter for pre-trained diffusion models with fine-grained id and attribute control. arXiv preprint arXiv:2405.12970, 2024. 21 [34] Hao He, Yinghao Xu, Yuwei Guo, Gordon Wetzstein, Bo Dai, Hongsheng Li, and Ceyuan Yang. Cameractrl: Enabling camera control for text-to-video generation. arXiv preprint arXiv:2404.02101, 2024. 20 [35] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770778, 2016. 18 [37] Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). arXiv preprint arXiv:1606.08415, 2016. 17 [38] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Prompt-to-prompt image editing with cross attention control. arXiv preprint arXiv:2208.01626, 2022. 2, 17, 19 [39] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. 5 [40] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. 3, 4 [41] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik Kingma, Ben Poole, Mohammad Norouzi, David Fleet, et al. Imagen video: High definition video generation with diffusion models. arXiv preprint arXiv:2210.02303, 2022. 3, 4 [42] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David Fleet. Video diffusion models. Advances in Neural Information Processing Systems, 35:86338646, 2022. 3 [43] Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, and Jie Tang. Cogvideo: Large-scale pretraining for text-to-video generation via transformers. arXiv preprint arXiv:2205.15868, 2022. 7, 8 [44] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021. 2, [45] Zhihao Hu and Dong Xu. motion-guided video-to-video translation framework by arXiv preprint using diffusion model with controlnet. arXiv:2307.14073, 2023. 4, 16, 19 Videocontrolnet: [46] Haojian Huang, Xiaozhennn Qiao, Zhuo Chen, Haodong Chen, Bingyu Li, Zhe Sun, Mulin Chen, and Xuelong Li. Crest: Cross-modal resonance through evidential deep learning for enhanced zero-shot learning. In Proceedings of the 32nd ACM International Conference on Multimedia, pages 51815190, 2024. 18 [47] Kaiyi Huang, Kaiyue Sun, Enze Xie, Zhenguo Li, and Xihui Liu. T2i-compbench: comprehensive benchmark for open-world compositional text-to-image generation. arXiv preprint arXiv: 2307.06350, 2023. 7, 8 [49] Andrew Jaegle, [48] Yi Huang, Jiancheng Huang, Yifan Liu, Mingfu Yan, Jiaxi Lv, Jianzhuang Liu, Wei Xiong, He Zhang, Shifeng Chen, and Liangliang Cao. Diffusion model-based image editing: survey. arXiv preprint arXiv:2402.17525, 2024. 7, 8, 20 Jean-Baptiste Alayrac, Carl Doersch, Catalin Ionescu, David Ding, Skanda Koppula, Daniel Zoran, Andrew Brock, Evan Shelhamer, et al. Perceiver io: general architecture for In International Conference structured inputs & outputs. on Learning Representations. 4 Sebastian Borgeaud, [36] Yingqing He, Tianyu Yang, Yong Zhang, Ying Shan, and Qifeng Chen. Latent video diffusion models for high- [50] Andrew Jaegle, Felix Gimeno, Andy Brock, Oriol Vinyals, Andrew Zisserman, and Joao Carreira. Perceiver: General 10 perception with iterative attention. In International conference on machine learning, pages 46514664. PMLR, 2021. [51] Ondˇrej Jamriˇska, ˇSarka Sochorova, Ondˇrej Texler, Michal Lukaˇc, Jakub Fiˇser, Jingwan Lu, Eli Shechtman, and Daniel S`ykora. Stylizing video by example. ACM Transactions on Graphics (TOG), 38(4):111, 2019. 2 [52] Hyeonho Jeong and Jong Chul Ye. Ground-a-video: Zeroshot grounded video editing using text-to-image diffusion models. arXiv preprint arXiv:2310.01107, 2023. 2, 3, 16 [53] Hyeonho Jeong, Jinho Chang, Geon Yeong Park, and Jong Chul Ye. Dreammotion: Space-time self-similarity score distillation for zero-shot video editing. arXiv preprint arXiv:2403.12002, 2024. 16 [54] Hyeonho Jeong, Geon Yeong Park, and Jong Chul Ye. Vmc: Video motion customization using temporal attention adaption for text-to-video diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 92129221, 2024. 3, 16 [55] Kumara Kahatapitiya, Adil Karjauv, Davide Abati, Fatih Porikli, Yuki Asano, and Amirhossein Habibian. Objectcentric diffusion for efficient video editing. arXiv preprint arXiv:2401.05735, 2024. 2, 16 [56] Ozgur Kara, Bariscan Kurtkaya, Hidir Yesiltepe, James Rehg, and Pinar Yanardag. Rave: Randomized noise shuffling for fast and consistent video editing with diffusion In Proceedings of the IEEE/CVF Conference on models. Computer Vision and Pattern Recognition, pages 6507 6516, 2024. 4, [57] Yoni Kasten, Dolev Ofri, Oliver Wang, and Tali Dekel. Layered neural atlases for consistent video editing. ACM Transactions on Graphics (TOG), 40(6):112, 2021. 2 [58] Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen Chang, Tali Dekel, Inbar Mosseri, and Michal Irani. Imagic: Text-based real image editing with diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 60076017, 2023. 7, 8 [59] Levon Khachatryan, Andranik Movsisyan, Vahram Tadevosyan, Roberto Henschel, Zhangyang Wang, Shant Navasardyan, and Humphrey Shi. Text2video-zero: Textto-image diffusion models are zero-shot video generators. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1595415964, 2023. 2, 3, 4, 16 [60] Yuval Kirstain, Adam Polyak, Uriel Singer, Shahbuland Matiana, Joe Penna, and Omer Levy. Pick-a-pic: An open dataset of user preferences for text-to-image generation. Advances in Neural Information Processing Systems, 36: 3665236663, 2023. 7 [61] Max Ku, Cong Wei, Weiming Ren, Huan Yang, and Wenhu Chen. Anyv2v: plug-and-play framework for any videoto-video editing tasks. arXiv preprint arXiv:2403.14468, 2024. 2, 16 [62] PKU-Yuan Lab and Tuzhan AI etc. Open-sora-plan, 2024. [63] Yao-Chih Lee, Ji-Ze Genevieve Jang, Yi-Ting Chen, Elizabeth Qiu, and Jia-Bin Huang. Shape-aware text-driven layered video editing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1431714326, 2023. 2, 3, 16 [64] Xirui Li, Chao Ma, Xiaokang Yang, and Ming-Hsuan Yang. Vidtome: Video token merging for zero-shot video editing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 74867495, 2024. 2, 4, 16 [65] Feng Liang, Bichen Wu, Jialiang Wang, Licheng Yu, Kunpeng Li, Yinan Zhao, Ishan Misra, Jia-Bin Huang, Peizhao Zhang, Peter Vajda, et al. Flowvid: Taming imperfect optical flows for consistent video-to-video synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 82078216, 2024. 2, 3, 16 [66] Jun Hao Liew, Hanshu Yan, Jianfeng Zhang, Zhongcong Xu, and Jiashi Feng. Magicedit: High-fidelity arXiv preprint and temporally coherent video editing. arXiv:2308.14749, 2023. 2, 16 [67] Pengyang Ling, Jiazi Bu, Pan Zhang, Xiaoyi Dong, Yuhang Zang, Tong Wu, Huaian Chen, Jiaqi Wang, and Yi Jin. Motionclone: Training-free motion cloning for controllable video generation. arXiv preprint arXiv:2406.05338, 2024. 4, 16 [68] Shaoteng Liu, Yuechen Zhang, Wenbo Li, Zhe Lin, and Jiaya Jia. Video-p2p: Video editing with cross-attention control. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 85998608, 2024. 2, 3, 4, 7, 8, 16, 19, [69] Zhengxiong Luo, Dayou Chen, Yingya Zhang, Yan Huang, Liang Wang, Yujun Shen, Deli Zhao, Jingren Zhou, and Tieniu Tan. Videofusion: Decomposed diffusion modarXiv preprint els for high-quality video generation. arXiv:2303.08320, 2023. 3 [70] Kaijing Ma, Haojian Huang, Jin Chen, Haodong Chen, Pengliang Ji, Xianghao Zang, Han Fang, Chao Ban, Hao Sun, Mulin Chen, et al. Beyond uncertainty: Evidential deep learning for robust video temporal grounding. arXiv preprint arXiv:2408.16272, 2024. 18 [71] Xin Ma, Yaohui Wang, Gengyun Jia, Xinyuan Chen, Ziwei Liu, Yuan-Fang Li, Cunjian Chen, and Yu Qiao. Latte: Latent diffusion transformer for video generation. arXiv preprint arXiv:2401.03048, 2024. 3 [72] Yue Ma, Yingqing He, Xiaodong Cun, Xintao Wang, Siran Chen, Xiu Li, and Qifeng Chen. Follow your pose: Poseguided text-to-video generation using pose-free videos. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 41174125, 2024. 3, 16 [73] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit: Guided image synthesis and editing with stochastic differential equations. arXiv preprint arXiv:2108.01073, 2021. 2, 3, 19 [74] Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Null-text inversion for editing real imIn Proceedings of ages using guided diffusion models. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 60386047, 2023. 7, 11 [75] Eyal Molad, Eliahu Horwitz, Dani Valevski, Alex Rav Acha, Yossi Matias, Yael Pritch, Yaniv Leviathan, and Yedid Hoshen. Dreamix: Video diffusion models are general video editors. arXiv preprint arXiv:2302.01329, 2023. 4, 16 [76] Chong Mou, Xintao Wang, Liangbin Xie, Yanze Wu, Jian Zhang, Zhongang Qi, and Ying Shan. T2i-adapter: Learning adapters to dig out more controllable ability for text-toimage diffusion models. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 42964304, 2024. 19 [77] Kepan Nan, Rui Xie, Penghao Zhou, Tiehan Fan, Zhenheng Yang, Zhijie Chen, Xiang Li, Jian Yang, and Ying Tai. Openvid-1m: large-scale high-quality dataset for text-tovideo generation. arXiv preprint arXiv:2407.02371, 2024. 7, 17 [78] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. arXiv preprint arXiv:2112.10741, 2021. 3 [79] OpenAI. Gpt-4 system card. https://openai.com/ index/gpt4vsystemcard/, 2024. Accessed: 2024-08-03. 6, 18, [80] Hao Ouyang, Qiuyu Wang, Yuxi Xiao, Qingyan Bai, Juntao Zhang, Kecheng Zheng, Xiaowei Zhou, Qifeng Chen, and Yujun Shen. Codef: Content deformation fields for temporally consistent video processing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 80898099, 2024. 2, 3, 16 [81] Junting Pan, Ziyi Lin, Xiatian Zhu, Jing Shao, and Hongsheng Li. St-adapter: Parameter-efficient image-to-video transfer learning. Advances in Neural Information Processing Systems, 35:2646226477, 2022. 4 [82] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 4195 4205, 2023. 18 [83] Federico Perazzi, Jordi Pont-Tuset, Brian McWilliams, Luc Van Gool, Markus Gross, and Alexander Sorkine-Hornung. benchmark dataset and evaluation methodology for video In Proceedings of the IEEE conferobject segmentation. ence on computer vision and pattern recognition, pages 724732, 2016. 3 [84] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion modarXiv preprint els for high-resolution image synthesis. arXiv:2307.01952, 2023. 7, 8 [85] Chenyang Qi, Xiaodong Cun, Yong Zhang, Chenyang Lei, Xintao Wang, Ying Shan, and Qifeng Chen. Fatezero: Fusing attentions for zero-shot text-based video editing. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1593215942, 2023. 2, 4, 16 [86] Bosheng Qin, Juncheng Li, Siliang Tang, Tat-Seng Chua, and Yueting Zhuang. Instructvid2vid: Controllable video editing with natural language instructions. arXiv preprint arXiv:2305.12328, 2023. 2, [87] Zhiwu Qing, Shiwei Zhang, Jiayu Wang, Xiang Wang, Yujie Wei, Yingya Zhang, Changxin Gao, and Nong Sang. Hierarchical spatio-temporal decoupling for text-to-video In Proceedings of the IEEE/CVF Conference generation. on Computer Vision and Pattern Recognition, pages 6635 6645, 2024. 7, 8 [88] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language superIn International conference on machine learning, vision. pages 87488763. PMLR, 2021. 2, 4, 7 [89] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey text-conditional arXiv preprint Chu, and Mark Chen. image generation with clip latents. arXiv:2204.06125, 1(2):3, 2022. 2,"
        },
        {
            "title": "Hierarchical",
            "content": "[90] Yixuan Ren, Yang Zhou, Jimei Yang, Jing Shi, Difan Liu, Feng Liu, Mingi Kwon, and Abhinav Shrivastava. Customize-a-video: One-shot motion customizaarXiv preprint tion of text-to-video diffusion models. arXiv:2402.14780, 2024. 4, 16 [91] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1068410695, 2022. 7, 8 [92] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. 2, 3, 4, 7, 17 [93] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. Unet: Convolutional networks for biomedical image segmentation. In Medical image computing and computer-assisted interventionMICCAI 2015: 18th international conference, Munich, Germany, October 5-9, 2015, proceedings, part III 18, pages 234241. Springer, 2015. 2, 3 [94] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in neural information processing systems, 35:3647936494, 2022. 2, 3 [95] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, Patrick Schramowski, Srivatsa Kundurthy, Katherine Crowson, Ludwig Schmidt, Robert Kaczmarczyk, and Jenia Jitsev. LAION-5b: An open large-scale dataset for In Thirtytraining next generation image-text models. sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2022. 17 [96] Jing Shi, Wei Xiong, Zhe Lin, and Hyun Joon Jung. Instantbooth: Personalized text-to-image generation without test-time finetuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 85438552, 2024. 4 [97] Chaehun Shin, Heeseung Kim, Che Hyun Lee, Sang-gil Lee, and Sungroh Yoon. Edit-a-video: Single video editing with object-aware consistency. In Asian Conference on Machine Learning, pages 12151230. PMLR, 2024. 2, 16 [98] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, et al. Make-a-video: Text-to-video generation without text-video data. arXiv preprint arXiv:2209.14792, 2022. 3 [99] Uriel Singer, Amit Zohar, Yuval Kirstain, Shelly Sheynin, Adam Polyak, Devi Parikh, and Yaniv Taigman. Video editing via factorized diffusion distillation. arXiv preprint arXiv:2403.09334, 2024. 16 [100] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In International conference on machine learning, pages 22562265. PMLR, 2015. 3, 4 [101] Yeji Song, Wonsik Shin, Junsoo Lee, Jeesoo Kim, and Nojun Kwak. Save: Protagonist diversification with structure agnostic video editing. arXiv preprint arXiv:2312.02503, 2023. 2, 16 [102] Soomro. Ucf101: dataset of 101 human actions classes from videos in the wild. arXiv preprint arXiv:1212.0402, 2012. 7 [103] Wenhao Sun, Rong-Cheng Tu, Jingyi Liao, and Dacheng Tao. Diffusion model-based video editing: survey. arXiv preprint arXiv:2407.07111, 2024. 3, [104] Shuyuan Tu, Qi Dai, Zhi-Qi Cheng, Han Hu, Xintong Han, Zuxuan Wu, and Yu-Gang Jiang. Motioneditor: Editing video motion via content-aware diffusion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 78827891, 2024. 2, 3, 16 [105] Thomas Unterthiner, Sjoerd Van Steenkiste, Karol Kurach, Raphael Marinier, Marcin Michalski, and Sylvain Gelly. Towards accurate generative models of video: new metric & challenges. arXiv preprint arXiv:1812.01717, 2018. 7 [106] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in neural information processing systems, 30, 2017. 17 [107] Jiuniu Wang, Hangjie Yuan, Dayou Chen, Yingya Zhang, Xiang Wang, and Shiwei Zhang. Modelscope text-to-video technical report. arXiv preprint arXiv:2308.06571, 2023. 3 [108] Jiangshan Wang, Yue Ma, Jiayi Guo, Yicheng Xiao, Gao Huang, and Xiu Li. Cove: Unleashing the diffusion feature correspondence for consistent video editing. arXiv preprint arXiv:2406.08850, 2024. 2, 16 [109] Wen Wang, Yan Jiang, Kangyang Xie, Zide Liu, Hao Chen, Yue Cao, Xinlong Wang, and Chunhua Shen. Zero-shot video editing using off-the-shelf image diffusion models. arXiv preprint arXiv:2303.17599, 2023. 2, 16, 19 [110] Xiang Wang, Hangjie Yuan, Shiwei Zhang, Dayou Chen, Jiuniu Wang, Yingya Zhang, Yujun Shen, Deli Zhao, and Jingren Zhou. Videocomposer: Compositional video synthesis with motion controllability. Advances in Neural Information Processing Systems, 36, 2024. 2, 3, 4, 16 [111] Yaohui Wang, Xinyuan Chen, Xin Ma, Shangchen Zhou, Ziqi Huang, Yi Wang, Ceyuan Yang, Yinan He, Jiashuo Yu, Peiqing Yang, et al. Lavie: High-quality video generation with cascaded latent diffusion models. arXiv preprint arXiv:2309.15103, 2023. 3, 7, [112] Yi Wang, Yinan He, Yizhuo Li, Kunchang Li, Jiashuo Yu, Xin Ma, Xinhao Li, Guo Chen, Xinyuan Chen, Yaohui Wang, et al. Internvid: large-scale video-text dataset for multimodal understanding and generation. arXiv preprint arXiv:2307.06942, 2023. 4 [113] Bichen Wu, Ching-Yao Chuang, Xiaoyan Wang, Yichen Jia, Kapil Krishnakumar, Tong Xiao, Feng Liang, Licheng Yu, and Peter Vajda. Fairy: Fast parallelized instructionIn Proceedings of the guided video-to-video synthesis. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 82618270, 2024. 2, 3, 4, 16 [114] Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Stan Weixian Lei, Yuchao Gu, Yufei Shi, Wynne Hsu, Ying Shan, Xiaohu Qie, and Mike Zheng Shou. Tune-a-video: One-shot tuning of image diffusion models for text-to-video generation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 76237633, 2023. 2, 3, 4, 7, 8, 15, 16, 17, 19, 20 [115] Jay Zhangjie Wu, Xiuyu Li, Difei Gao, Zhen Dong, Jinbin Bai, Aishani Singh, Xiaoyu Xiang, Youzeng Li, Zuwei Huang, Yuanxi Sun, Rui He, Feng Hu, Junhua Hu, Hai Huang, Hanyu Zhu, Xu Cheng, Jie Tang, Mike Zheng Shou, Kurt Keutzer, and Forrest Iandola. Cvpr 2023 text guided video editing competition, 2023. 3, 6, 8, 18, 21 [116] Qiucheng Wu, Yujian Liu, Handong Zhao, Ajinkya Kale, Trung Bui, Tong Yu, Zhe Lin, Yang Zhang, and Shiyu Chang. Uncovering the disentanglement capability in In Proceedings of the text-to-image diffusion models. IEEE/CVF conference on computer vision and pattern recognition, pages 19001910, 2023. 7, 8 [117] Ruiqi Wu, Liangyu Chen, Tong Yang, Chunle Guo, Chongyi Li, and Xiangyu Zhang. Lamp: Learn motion pattern for few-shot-based video generation. arXiv preprint arXiv:2310.10769, 2023. [118] Zeqi Xiao, Yifan Zhou, Shuai Yang, and Xingang Pan. Video diffusion models are training-free motion interpreter and controller. arXiv preprint arXiv:2405.14864, 2024. 20 [119] Jinbo Xing, Menghan Xia, Yong Zhang, Haoxin Chen, Xintao Wang, Tien-Tsin Wong, and Ying Shan. Dynamicrafter: Animating open-domain images with video diffusion priors. arXiv preprint arXiv:2310.12190, 2023. 2, 4, 7, 17 [120] Jinbo Xing, Menghan Xia, Yuxin Liu, Yuechen Zhang, Yong Zhang, Yingqing He, Hanyuan Liu, Haoxin Chen, Xiaodong Cun, Xintao Wang, et al. Make-your-video: Customized video generation using textual and structural guidIEEE Transactions on Visualization and Computer ance. Graphics, 2024. 2, 3, 4, 16 [121] Zhen Xing, Qi Dai, Han Hu, Zuxuan Wu, and Yu-Gang Jiang. Simda: Simple diffusion adapter for efficient video In Proceedings of the IEEE/CVF Conference generation. on Computer Vision and Pattern Recognition, pages 7827 7839, 2024. 4, 16 13 [135] Xiaojing Zhong, Xinyi Huang, Xiaofeng Yang, Guosheng Lin, and Qingyao Wu. Deco: Decoupled human-centered arXiv diffusion video editing with motion consistency. preprint arXiv:2408.07481, 2024. 2, 16 [122] Jun Xu, Tao Mei, Ting Yao, and Yong Rui. Msr-vtt: large video description dataset for bridging video and language. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 52885296, 2016. 7 [123] Wilson Yan, Andrew Brown, Pieter Abbeel, Rohit Girdhar, and Samaneh Azadi. Motion-conditioned image animation for video editing. arXiv preprint arXiv:2311.18827, 2023. 2, [124] Yibo Yan, Haomin Wen, Siru Zhong, Wei Chen, Haodong Chen, Qingsong Wen, Roger Zimmermann, and Yuxuan Liang. Urbanclip: Learning text-enhanced urban region profiling with contrastive language-image pretraining from the web. In Proceedings of the ACM on Web Conference 2024, pages 40064017, 2024. 18 [125] Shuai Yang, Yifan Zhou, Ziwei Liu, and Chen Change Loy. Rerender video: Zero-shot text-guided video-toIn SIGGRAPH Asia 2023 Conference video translation. Papers, pages 111, 2023. 3, 16, 20 [126] Shuai Yang, Yifan Zhou, Ziwei Liu, and Chen Change Loy. Fresco: Spatial-temporal correspondence for zeroIn Proceedings of the IEEE/CVF shot video translation. Conference on Computer Vision and Pattern Recognition, pages 87038712, 2024. 4, 16 [127] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-tovideo diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. 7, 8 [128] Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ipadapter: Text compatible image prompt adapter for text-toimage diffusion models. arXiv preprint arXiv:2308.06721, 2023. 4 [129] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 38363847, 2023. [130] Zicheng Zhang, Bonan Li, Xuecheng Nie, Congying Han, Tiande Guo, and Luoqi Liu. Towards consistent video editing with text-to-image diffusion models. Advances in Neural Information Processing Systems, 36, 2024. 3, 16 [131] Haoyu Zhao, Tianyi Lu, Jiaxi Gu, Xing Zhang, Qingping Zheng, Zuxuan Wu, Hang Xu, and Yu-Gang Jiang. Magdiff: Multi-alignment diffusion for high-fidelity video generation and editing. 16 [132] Min Zhao, Rongzhen Wang, Fan Bao, Chongxuan Li, and Jun Zhu. Controlvideo: Conditional control for oneshot text-driven video editing and beyond. arXiv preprint arXiv:2305.17098, 2023. 2, 3, 4, 7, 8, 16, 19, 20 [133] Rui Zhao, Yuchao Gu, Jay Zhangjie Wu, David Junhao Zhang, Jiawei Liu, Weijia Wu, Jussi Keppo, and Mike Zheng Shou. Motiondirector: Motion customizaarXiv preprint tion of text-to-video diffusion models. arXiv:2310.08465, 2023. 2, 3, 5, 15, 16 [134] Yuyang Zhao, Enze Xie, Lanqing Hong, Zhenguo Li, and Gim Hee Lee. Make-a-protagonist: Generic video arXiv preprint editing with an ensemble of experts. arXiv:2305.08850, 2023. 2, 4,"
        },
        {
            "title": "Contents",
            "content": "A. Editing Capabilities Overview B. Ablation Studies B.1. Training Paradigm. . B.2. Inference Paradigm. . . . . C. Implementation Details . C.1. Network Architecture C.2. Training and Inference . D. OmniBench-99 . D.1. Overview . D.2. Human Inspection . . . . . . . . . E. Additional Evaluation Details E.1. Video Baselines . . . E.2. User Study . E.3. Image Editing Metrics . . . . . . . . F. More Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . F.1. Limitations and Future Work . . F.2. Ethical Implications . . . . G. More Results G.1. Quantitative . G.2. Qualitative . . . . . . . . . . . . . . . . . . . 15 17 17 17 17 17 17 18 18 18 19 19 20 20 20 21 21 21 21 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . The videos in the main paper and appendix can be viewed with our Video Demo. A. Editing Capabilities Overview To facilitate clearer understanding of the current advancements in text-based video editing, we perform comprehensive evaluation and review, as illustrated in Tab. 5, structured around the following aspects: Tune & I2I. The Tune refers to techniques like onevideo tuning, as seen in Tune-A-Video [114], or few-video tuning in MotionDirector [133]. While this method helps models learn the specific structure of given videos, it tends to limit the models generalizability and flexibility in practical applications; The I2I method involves using imageto-image models, e.g., IP2P [7], to edit individual video frames. Although this allows for precise edits at the image level, it introduces challenges like increased computational cost, potential inconsistencies, and uncertainty in the editing process. Additional Control. We categorize the additional control in video editing into three main types: ❶ Condition (Cond.): Widely used to constrain the structure of the edited video, this approach incorporates elements, e.g., optical flow, depth maps, masks, edges, etc.; ❷ Attention Features 15 (Feat.): This enhances control during inference by injecting hidden features from reference video to guide the structure of the edited video. It can be divided into two types: inversion-based features and motion-based features [103]; ❸ DDIM Inversion (DDIM): In this technique, the inverted noise from reference video is progressively denoised during inference, ensuring consistency between the edited and reference videos. While these additional control strategies have significantly advanced controllable video generation/editing, it is important to acknowledge that the introduction of explicit controls can impose certain limitations on the models flexibility in editing scenarios. Editing Type. Following [103], we classify video editing into four distinct types, as shown at the top of Fig. 1 in the main content. Foreground (Fore.) and Background (Back.) editing refer to modifying only specific parts of the foreground or background while keeping other regions as unchanged as possible. Overall/Style editing involves altering the entire video. It is worth noting that even when certain methods target specific regions, they often result in broader changes to the entire video. Thus we classify these models as overall editing as well. Composite (Comp.) editing requires more complex modifications, such as editing both the foreground and style while keeping the background as consistent as possible. Editing Scenario. To further evaluate the models performance, we categorized the video editing scenarios into three main groups: ❶ Human/Animal: 1) Appearance (App.) editing involves modifying the physical appearance of person or animal, classified as foreground editing. 2) Motion (Mo.) or Pose editing focuses on action changes, such as altering actions from walking to running.; ❷ Object: 1) Addition (Add.) and Removal (Rem.) editing refer to inserting or deleting specific objects in the video. 2) Replacement (Rep.) editing involves substituting one object for another, also classified under foreground editing; ❸ Environment: 1) Weather (Wea.) editing involves composite editing, such as adding rainfall to reference video featuring seascape. This task not only requires adding raindrop effects but also capturing ripples in the water. 2) Time editing is form of overall editing, adjusting the video based on prompts related to different seasons or times of day, while maintaining the original video style (e.g., realistic video should not be transformed into an animated style). 3) Background (Back.) editing aligns with the typical background editing type. These eight categories are designed to assess various application scenarios for text-based video editing models. While this list may not be exhaustive, we plan to further expand it in future work. Table 5. Editing Capability Overview. Tune: One-shot or few-shot tuning-based; I2I: Image editing model (e.g., InstructPix2Pix [7]) assisted; Additional Control: Cond.: Condition={Ma=Mask, Mo=Motion vector, E=Edge, O=Optical flow, C=Canny, H=HED boundary, D=Depth, P=Pose, S=Sketch, SI=Style Image, B=Bounding box, A=Atlas}, Feat.: Attention feature injection during inference, DDIM: DDIM inversion-assisted; Editing Type: Fore.: Foreground, Back.: Background, Comp.: Composite, Overall: only for overall editing, e.g., style; Editing Scenario: App.: Appearance, Mo.: Motion, Add.: Addition, Rem.: Removal, Rep.: Replacement, Wea.: Weather, Back.: Background. Note: since many methods are not open source, we only evaluate this type of model through the results shown in its paper/page. Method Tune I2I Additional Control Editing Type Human/Animal Object Environment Editing Scenario Cond. Feat. DDIM Fore. Back. Comp. Overall App. Mo./Pose Add. Rem. Rep. Wea. Time Back. InstuctVid2Vid [86] ICME24 InsV2V [18] ICLR Control-A-Video [17] arXiv23 Video ControlNet [19] arXiv23 VideoControlNet [45] arXiv23 Dreamix [75] arXiv23 Vid2Vid-Zero [109] arXiv23 Fate-Zero [85] ICCV23 Pix2Video [8] ICCV23 EI2 [130] NeurIPS23 RAV [125] SIGGRAPH Asia23 MotionClone [67] arXiv24 Make-Your-Video [120] TVCG24 FLATTEN [20] ICLR24 Follow-Your-Pose [72] AAAI24 FreSCo [126] CVPR24 FlowVid [65] CVPR24 RAVE [56] CVPR24 CoDeF [80] CVPR24 VMC [54] CVPR24 SimDA [121] CVPR24 LAMP [117] CVPR24 CusAV [90] ECCV24 MotionDirector [133] ECCV24 NeRCan [16] NeurIPS24 Text2LIVE [2] ECCV22 MoCA [123] arXiv23 DiffusionAtlas [10] arXiv23 Make-A-Prota. [134] arXiv23 MagicEdit [66] arXiv23 VidEdit [22] TMLR23 STL [63] CVPR23 T2V-Zero [59] ICCV23 Tune-A-Video [114] ICCV23 Gen-1 [23] ICCV23 StableVideo [9] ICCV23 VideoComposer [110] NeurIPS23 UniEdit [1] arXiv24 AnyV2V [61] arXiv24 Edit-A-Video [97] ACML24 TokenFlow [30] ICLR24 Ground-A-Video [52] ICLR24 ControlVideo [132] ICLR24 CCEdit [26] CVPR24 Fairy [113] CVPR24 Video-P2P [68] CVPR24 MotionEditor [104] CVPR24 VidToMe [64] CVPR24 SAVE [101] ECCV24 EVE [99] ECCV24 WAVE [27] ECCV24 DeCo [135] ECCV24 Videoshop [24] ECCV24 OCD [55] ECCV24 DreamMotion [53] ECCV24 MagDiff [131] ECCV24 COVE [108] NeurIPS24 OmniCreator (Ours) Video-Video Pair-based Text-Video Pair-based Only Style/Overall Editing C/H/D O/D O+Ma/D/C E+O P O+D Mo A D+Ma D/P A+H+Ma A+Ma Ma D+Ma A+D+C D/S/Ma/Mo/SI D+O+B C/H/D/P D/S P+Ma Ma O+Ma D+A+P Ma Ma Ma Ma Diverse Editing Video-Video Pair vs. Text-Video Pair-based. While text-video pair datasets are widely used for text-to-video generation and editing, two works [18, 86] utilize videovideo data for training. These datasets consist of reference video, human-instruction-like text prompt, and an edited video. Although this approach has seen some suc16 Figure 11. Ablation on video condition modeling. Ada. indicates the adapter, and Query is the query transformer. cess at the image level [7], video datasets present greater challenges in terms of resource consumption and quality, since they rely on off-the-shelf video or image editing models (e.g., TAV [114] and P2P [38]) to create video pairs. Similar to our OmniCreator, these works also do not employ additional explicit structural control. However, their reliance on tightly paired training videos and the limited information provided by the text prompts further constrain the performance of their models. B. Ablation Studies B.1. Training Paradigm. ❶ Condition modeling: As shown in Fig. 4, we illustrate how video embeddings align with text embeddings through the adapter and query transformer. Fig. 11 further demonstrates the editing results, indicating that both components enhance the alignment of video and text for more accurate editing. Notably, with the adapter, the model can still largely reconstruct the reference videos background structure, underscoring the importance of video-text alignment, especially in temporal modeling. ❷ LoRA ranks: We also examine the impact of LoRA ranks on model performance for T2V generation, shown in Fig. 12. Increasing the rank within reasonable range (e.g., 32 to 64) enhances text adherence (e.g., choppy). However, overly low or high ranks (e.g., 2 or 256) weaken prompt adherence significantly. These results suggest that appropriately tuned LoRA ranks can boost generative capability while maintaining computational efficiency. B.2. Inference Paradigm. An appropriate inference paradigm in generative models is often as critical as the training process. As discussed in Sec. 3.2  (Fig. 5)  , the use of delta prompts allows for more 17 Figure 12. Ablation on LoRA ranks. LoRA with different ranks exhibits different learning comprehension abilities precise modeling of local textual semantics for the target area, thereby avoiding conflicts between the global textual semantics of full sentence and the global visual semantics provided by the reference video. Additionally, we analyze the impact of our two multimodal classifier-free guidance scales in Fig. 13. Increasing each scale strengthens the respective control over the editing results. Notably, beyond the obvious control over facial regions, increasing the video condition scale wvid further enhances the preservation of the spatial structure of the reference videos background, as shown by the red bounding box. In practice, the ideal editing effect depends on the user preferences, making it beneficial to adjust the scales according to different examples for optimal results. C. Implementation Details C.1. Network Architecture We implement our OmniCreator framework on DynamiCrafter [119], pre-trained image-to-video (I2V) generation model and T2I model Stable-Diffusion-v2.1 [92]. Notably, our method of using the original video as new denoising condition can be extended to other I2V or T2V diffusion models that incorporate cross-attention mechanisms for text conditioning. The VQ-VAE [106] is used for patchwise frame encoding and CLIP-ViT-H-14 [95] for text and visual embedding. Following [119], we incorporate combination of sinusoidal functions and several fully connected (FC) layers activated by SiLU [37] as the FPS embedding layer. This embedding is added to the timestep embedding for further refinement. C.2. Training and Inference We fine-tune our OmniCreator on the OpenVidHD-0.4M dataset [77] using four NVIDIA H20 GPUs for 12K iterations with batch size of 32. This setup is considered resource-efficient given the complexity of training videoto-video diffusion models. The default LoRA rank is set to 32, and corresponding ablation results for different values Figure 13. Ablation on multimodal guidance scales. wtxt controls consistency with the edit instruction, while wvid controls the similarity with reference video. are provided in Fig. 12. During inference, we used DDIM deterministic sampling with 50 steps. The multimodal classifier-free guidance scales, wvid and wtxt, are set to 1.5 and 7.5, respectively, in most cases. Further details on these settings and their impact are also included in Fig. 13. D. OmniBenchD.1. Overview Despite the rapid development of computer vision [12, 13, 35, 46, 70, 82, 124], the evaluation of video editing still focuses only on editing types [26, 115]. To comprehensively evaluate the performance of generative text-guided video editing models, we introduce OmniBench-99, benchmark that enables both editing types and scenarios evaluation. Our dataset consists of 99 high-quality, open-license videos across three categories: Human/Animal, Environment, and Object. For each video, we first create four editing-type prompts, i.e., Foreground, Background, Composite, and Style/Overall. Then, based on the videos category, we generate specific editing-scenario prompts. To accommodate the variety of prompts accepted by existing models, we further design two types of text prompt, i.e., full description and delta caption. To reduce the burden of manual data annotation, we leverage GPT-4V(ision) [79] to automatically generate these prompts following the instructions shown in Fig. 14. D.2. Human Inspection Human inspection plays pivotal role in our OmniBench-99 since LLMs often experience some hallucinations. In video editing, while the range of potential edits is vast, we aim for reasonable and appropriate prompts. During the refinement Figure 14. Text Prompt Demonstration. process, we focus on ensuring that the prompt aligns with the video content and whether the resulting edited video adheres to real-world physics. For example, consider video of car driving on road with the prompt to change the road into cracked canyon. Editing only the road would be ideal, but driving the car across canyon is unrealistic. Ensuring such physical consistency is key aspect of our approach: Editing Type Foreground: Ensure that the prompt clearly specifies the foreground object and that the edits (e.g., changes in color, shape, or pose) are both reasonable and actionable. The foreground should be distinct from the background and other video elements to avoid ambiguity. Background: The prompt should clearly suggest modifications to the background, such as changing the setting, adjusting depth of field, or altering the environments style. Verify that the target for editing is appropriate, such as focusing on natural background rather than the primary subject. Style: Ensure that the prompt proposes changes to the videos overall or localized style, such as color tone, 18 filters, or texture, and that these changes are consistent with the video content. brief introduction of each model, we recommend referring to Tab. 5 for concise, clear summary. Composite: The prompt should involve multiple (at least two) elements, ensuring the edits work together cohesively without conflict. Check if all parts of the composite edit are feasible and correspond to identifiable points in the video. Editing Scenario Human/Animal ① Appearance: Evaluate whether the prompt clearly specifies the visual characteristics of person or animal, such as clothing, skin tone, or hairstyle. Assess whether the requested edits are appropriate and relevant to the main subject of the video. ② Motion/Pose: Ensure the prompt addresses the movement within the video. The description should be clear, and the requested motion changes should maintain the flow and coherence of the video. Object ① Addition: Ensure the prompt clearly specifies the object or element to be added and that it integrates naturally into the existing scene. ② Removal: Verify that the prompts request for object removal aligns with the actual video content and that the removal does not disrupt the scenes continuity. ③ Replacement: Check that the prompt provides clear instructions for object replacement and that the new object is appropriate for the videos context. Environment ① Weather: Ensure the prompt provides clear editing instructions for weather elements, such as rain, sunshine, or snow, and that these changes are realistically reflected in the videos background. ② Time: Verify whether the prompt specifies timerelated scene transitions, like day turning to night or seasonal shifts, and assess whether these changes are implemented logically. ③ Background: Ensure the prompt suggests appropriate modifications to the videos background, such as switching urban settings to natural landscapes, while maintaining consistency with the videos main theme. We provide examples of OmniBench-99 in Fig. 16. The full benchmark will be released with our code after the paper notification. E. Additional Evaluation Details E.1. Video Baselines We compare our OmniCreator with state-of-the-art generative video editing models: ControlVideo [132], Tune-AVideo [114], Pix2Video [8], TokenFlow [30], InsV2V [18], Video-P2P [68], and CCEdit [26]. Before delving into 19 ControlVideo [132] utilizes ControlNet [45] with structural guidance to maintain consistency from reference video clips. It enhances appearance coherence by incorporating full cross-frame interaction within self-attention modules. An interleaved frame is also smoother to minimize flickering through frame interpolation. DDIM inversion is employed during inference. Tune-A-Video [114] introduces sparse attention mechanism to preserve temporal coherence and fin-tuning the network parameters through one-video tuning. It has been treated as de facto baseline in the text-guided video editing domain. DDIM inversion is applied during inference. Pix2Video [8] edits an anchor frame using structureguided diffusion model (i.e., SDEdit [73]) based on text prompts. These edits are then propagated to subsequent frames through self-attention feature injection during the diffusion models denoising process, adjusting each frames latent code. DDIM inversion ensures the videos structure is preserved. TokenFlow [30] enhances video editing by utilizing interframe feature similarity. It processes sampled keyframes using global spatiotemporal self-attention mechanism instead of traditional spatial self-attention. To ensure consistency in the diffusion feature space, it identifies the nearest neighbors from adjacent keyframes for each token and employs DDIM inversion for structural consistency. InsV2V [18] adapts the IP2P [7] paradigm for video editing. The process begins with the construction of prompts, utilizing T2V model combined with P2P techniques to generate both the reference and edited videos based on human-like instructions. This approach specifically targets the editing of long videos and introduces motion compensation method to minimize visible boundaries between chunks. Video-P2P [68] extends the P2P framework [38] into video editing. Similar to Vid2Vid-Zero [109], it employs cross-attention map injection, null-text inversion, and DDIM inversion. It replaces spatial self-attention layers with sparse-causal attention, introduces temporal attention layers, and implements one-video tuning to learn temporal consistency. CCEdit [26] uses trident architecture featuring an inflated T2I U-Net and two hyper-networks [76, 129] for structure and appearance. It begins by editing an anchor frame with the I2I model, which extracts pyramid features from the appearance hyper-network. These features are added to the U-Net encoder for video frame propagation [76]. Meanwhile, the second hyper-network extracts structural features, which are integrated into the denoising U-Nets decoder to provide structural guidance [129]. E.2. User Study In the context of text-guided video editing, it is essential to assess results not only through automated metrics but also through human subjective evaluation. To achieve this, we conducted comprehensive user study to gather human preferences, comparing our method with recent state-ofthe-art models using the mean opinion score (MOS) and direct comparisons. We developed user-friendly interface and collected scoring results from 15 volunteers, who evaluated various indicators of the edited videos. Participants rated the videos based on four key aspects: ① Text Alignment: whether the edited video accurately achieves the intended meaning of the target prompt; ② Temporal Consistency: whether the video maintains coherence over time; ③ Structure Alignment: whether the edited video better retains the structure of the reference video; and ④ Overall Quality: the reflection of the subjective overall rating of the edited video. The interface is demonstrated in Fig. 17. E.3. Image Editing Metrics In our quantitative evaluation of image editing, we employ EditEval [48], comprehensive benchmark tailored to assess general diffusion model-based image editing across seven distinct tasks: Addition, Replacement, Removal, Background, Style, Texture, and Action, as illustrated in Fig. 8. Following the methodology in [48], we use the LMM Score as the evaluation metric, which leverages large multimodal models (LMM), (GPT-4V [79] in our work), to measure editing performance. Given reference image Iref with its text description tref , and an edited image Iedit accompanied by the editing prompt tedit and an editing instruction tinst, the evaluation considers four critical factors: ① Editing Accuracy: Measures how precisely the edited image Iedit aligns with the specified editing prompt tedit and instruction tinst. ② Contextual Preservation: Evaluates the extent to which the edited image Iedit preserves the context of the reference image Iref that should remain unchanged. ③ Visual Quality: Assesses the overall quality of the edited image Itgt, considering aspects such as resolution, absence of artifacts, color accuracy, sharpness, and more. ④ Logical Realism: Examines the logical realism of the edited image Itgt, ensuring adherence to natural laws such as lighting consistency and texture continuity. Each of these factors produces sub-score, ranging from 1 to 10, denoted as Sacc, Spre, Squa, and Sreal. The overall score SLMM is then calculated using the following formula: SLMM = 0.4Sacc + 0.3Spre + 0.2Squa + 0.1Sreal. (7) 20 Table 6. Additional Quantitative Comparison on LOVEUTGVE-2023 and BalanceCC benchmarks, which only focus on editing-type evaluations. Method ControlVideo [132] arXiv23 Tune-A-Video [114] ICCV23 Pix2Video [8] ICCV23 RAV [125] SIGGRAPH Asia24 TokenFlow [30] ICLR24 InsV2V [18] ICLR24 Video-P2P [68] CVPR24 CCEdit [26] CVPR24 OmniCreator (Ours) LOVEU BalanceCC CLIP 0.930 0.924 0.916 0.909 0.940 0.911 0.935 - 0.958 Pick 0.201 0.204 0.201 0.196 0.205 0.208 0.201 - 0.209 CLIP 0.950 0.937 0.939 0.928 0.949 - - 0.936 0. Pick 0.210 0.206 0.208 0.201 0.210 - - 0.213 0.214 F. More Discussion F.1. Limitations and Future Work leveraging self-supervised paradigm, OmniCreator, achieves highly universal and superior editing and generation performance using only the original text-video training data. This means it addresses many of the limitations seen in existing methods. However, we still observe some limitations, including: Large Motion Change: Since OmniCreator relies on semantic information for conditional generation, it may struggle to capture the temporal dynamics and finegrained details of large motion changes, particularly in cases involving high-speed movement or complex posture shifts. This is due to the inherent limitations of static video embeddings and text embeddings in capturing such temporal information. promising solution could be to leverage the motion features from the reference video more effectively while ensuring they are not overly constrained by the original contents structure, as exemplified in MOFT [118]. Camera Motion Editing: OmniCreators framework primarily models semantic information from the video content, while camera motion is less semantically related to the content itself. Camera motion involves complex spatial transitions and shot continuity, which are difficult to express through textual descriptions alone. As result, OmniCreator may struggle to capture and generate subtle variations in camera motion, making the generated camera transitions appear unnatural or disjointed. One possible solution to achieve better camera control or editing is to explicitly use camera motion as an additional condition [34]a direction we leave for future work. Facial Modeling: The limitations in facial modeling primarily arise from OmniCreators reliance on semantic information to model the relationship between video and text. Facial details (such as expressions, skin textures, and micro-expressions) are often fine-grained and highly individualized, and such details usually go beyond the level of abstraction captured by high-level semantic information. Figure 15. Image editing comparison. While text can describe broad facial features (e.g., smile or anger), it may not provide the precise details required for high-fidelity facial generation. This leads to suboptimal facial reconstruction during editing, as the model cannot accurately recreate the intricate textures and expressions based solely on semantic guidance. feasible solution would be to incorporate face-specific representations as conditions or use auxiliary ID control models to enhance facial detail generation [33]. Beyond addressing current limitations, future work will further enhance the editing and generation performance within our current unified (image+video) framework. This effort will explore additional modalities, aiming to make our framework both more unified and universal. Furthermore, we plan to incorporate physical information and constraints into the model, which can significantly benefit both generation and editing tasks. F.2. Ethical Implications OmniCreator is developed as self-supervised framework for research only. It may still raise important ethical considerations, particularly around content manipulation. The ability to generate and edit high-quality videos can potentially be misused for creating misleading or harmful content. To mitigate this risk, we recommend incorporating safeguards such as adding watermarks to edited videos to ensure transparency and authenticity. Additionally, guidelines on responsible use should be established, emphasizing its application in ethical and creative contexts, such as educational, artistic, or research-based scenarios, while discouraging its use for deceptive or harmful purposes. G. More Results G.1. Quantitative To further assess the effectiveness of our OmniCreator, we conducted additional quantitative comparisons with stateof-the-art methods using existing popular benchmarks, specifically LOVEU-TGVE-2023 [115] and BalanceCC [26]. As demonstrated in Tab. 6, OmniCreator consistently outperforms the baseline models. Since these two benchmarks only focus on evaluating editing types, we encourage future research to further evaluate editing scenarios using our proposed OmniBench-99 benchmark (see Tab. 1 for experiments). We believe this will provide more comprehensive assessment of model performance and generalizability. G.2. Qualitative Comparisons. ❶ Video: In addition to Fig. 7 in the main content, we present further comparisons of video editingtype in Fig. 21 to Fig. 24, as well as comparisons of editingscenario in Fig. 25 to Fig. 32. Consistent with the observations detailed in the main paper, most methods effectively handle the four types of editing. However, they exhibit certain limitations in specific editing scenarios, which aligns with the findings summarized in our survey Tab. 5. ❷ Image: Similarly, beyond Fig. 8 in the main text, we perform additional image editing comparisons in Fig. 15, further validating the universal editing capabilities of our OmniCreator framework. Gallery of our results. We demonstrate more T2I and T2V results by our OmniCreator in Fig. 18, Fig. 19, and Fig. 20. We provide the video result in our Video Demo. 22 Figure 16. Examples of OmniBench-99 benchmark. The delta prompts are highlighted in yellow-brown. 23 Figure 17. Demonstration of our user study interface. Here we demonstrate one complete sample. 24 Figure 18. Gallery of OmniCreators text-to-image generation results. Figure 19. Gallery of OmniCreators text-to-video generation results. 25 Figure 20. Gallery of OmniCreators text-to-video generation results. Figure 21. Video editing comparison: Editing-Type-Foreground. Figure 22. Video editing comparison: Editing-Type-Background. 27 Figure 23. Video editing comparison: Editing-Type-Style. Figure 24. Video editing comparison: Editing-Type-Composite. Figure 25. Video editing comparison: Editing-Scenario-Environment-Weather. Figure 26. Video editing comparison: Editing-Scenario-Environment-Time. 29 Figure 27. Video editing comparison: Editing-Scenario-Environment-Background. Figure 28. Video editing comparison: Editing-Scenario-Object-Addition. Figure 29. Video editing comparison: Editing-Scenario-Object-Removal. Figure 30. Video editing comparison: Editing-Scenario-Object-Replacement. 31 Figure 31. Video editing comparison: Editing-Scenario-Human-Appearance. Figure 32. Video editing comparison: Editing-Scenario-Human-Motion."
        }
    ],
    "affiliations": [
        "Everlyn AI",
        "HKUST",
        "MSU",
        "UCF"
    ]
}