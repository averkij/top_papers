{
    "paper_title": "R-Zero: Self-Evolving Reasoning LLM from Zero Data",
    "authors": [
        "Chengsong Huang",
        "Wenhao Yu",
        "Xiaoyang Wang",
        "Hongming Zhang",
        "Zongxia Li",
        "Ruosen Li",
        "Jiaxin Huang",
        "Haitao Mi",
        "Dong Yu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Self-evolving Large Language Models (LLMs) offer a scalable path toward super-intelligence by autonomously generating, refining, and learning from their own experiences. However, existing methods for training such models still rely heavily on vast human-curated tasks and labels, typically via fine-tuning or reinforcement learning, which poses a fundamental bottleneck to advancing AI systems toward capabilities beyond human intelligence. To overcome this limitation, we introduce R-Zero, a fully autonomous framework that generates its own training data from scratch. Starting from a single base LLM, R-Zero initializes two independent models with distinct roles, a Challenger and a Solver. These models are optimized separately and co-evolve through interaction: the Challenger is rewarded for proposing tasks near the edge of the Solver capability, and the Solver is rewarded for solving increasingly challenging tasks posed by the Challenger. This process yields a targeted, self-improving curriculum without any pre-existing tasks and labels. Empirically, R-Zero substantially improves reasoning capability across different backbone LLMs, e.g., boosting the Qwen3-4B-Base by +6.49 on math-reasoning benchmarks and +7.54 on general-domain reasoning benchmarks."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 ] . [ 1 4 0 0 5 0 . 8 0 5 2 : r Technical Report R-Zero: Self-Evolving Reasoning LLM from Zero Data Chengsong Huang1,2(cid:66) ,Wenhao Yu1(cid:66) , Xiaoyang Wang1,Hongming Zhang1, Zongxia Li1,3, Ruosen Li1,4, Jiaxin Huang2, Haitao Mi1, Dong Yu1 1Tencent AI Seattle Lab, 2Washington University in St. Louis, 3University of Maryland, College Park, 4The University of Texas at Dallas chengsong@wustl.edu; wenhaowyu@global.tencent.com"
        },
        {
            "title": "Abstract",
            "content": "Self-evolving Large Language Models (LLMs) offer scalable path toward superintelligence by autonomously generating, refining, and learning from their own experiences. However, existing methods for training such models still rely heavily on vast human curated tasks and labels, typically via fine-tuning or reinforcement learning, which poses fundamental bottleneck to advancing AI systems toward capabilities beyond human intelligence. To overcome this limitation, we introduce R-Zero, fully autonomous framework that generates its own training data from scratch. Starting from single base LLM, R-Zero initializes two independent models with distinct roles Challenger and Solver. These models are optimized separately and co-evolve through interaction: the Challenger is rewarded for proposing tasks near the edge of the Solvers capability, and the Solver is rewarded for solving increasingly challenging tasks posed by the Challenger. This process yields targeted, self-improving curriculum without any pre-existing tasks and labels. Empirically, R-Zero substantially improves reasoning capability across different backbone LLMs, e.g., boosting the Qwen3-4B-Base by +6.49 on math reasoning benchmarks, and +7.54 on general-domain reasoning benchmarks. Code: https://github.com/Chengsong-Huang/R-Zero. Figure 1: (Left): R-Zero employs co-evolutionary loop between Challenger and Solver. (Right): R-Zero achieves strong benchmark gains without any pre-existing tasks or human labels."
        },
        {
            "title": "Introduction",
            "content": "Self-evolving Large Language Models (LLMs) represent promising frontier for advancing language intelligence. By autonomously generating, refining, and learning from their own experiences, these 1 Technical Report models provide scalable pathway toward artificial superintelligence (Tao et al., 2024). critical requirement for training such self-evolving LLMs is access to large volumes of expertly curated tasks and labels, which serve as supervision signals for fine-tuning or reinforcement learning with verifiable rewards (RLVR) (Shao et al., 2024; DeepSeek-AI et al., 2025). However, relying on human annotators to create these tasks and labels is not only costly, labor-intensive, and difficult to scale, but also presents fundamental bottleneck to advancing AI systems toward capabilities that could eventually surpass human intelligence (Su et al., 2025; Zhao et al., 2025a). To reduce dependence on human-curated data, self-generated and label-free methods have been proposed to eliminate the need for explicit supervision. In particular, label-free RL derives reward signals directly from the models own outputs, such as sequence-level confidence scores (Li et al., 2025a; Prabhudesai et al., 2025; Huang et al., 2025) and output entropy (Agarwal et al., 2025; Cheng et al., 2025). However, despite removing the need for explicit labels, label-free methods still relies on pre-existing corpus of tasks, which limits its scalability in truly self-evolving settings. On the other side, self-challenging approaches train LLMs on tasks generated by the models themselves (Zhou et al., 2025; Wang et al., 2025a; Zhao et al., 2025a), While promising, many of these methods rely on external code executors to ensure that the synthesized tasks are both feasible and verifiable. However, in domains that lack an explicit verification oracle, such as open-ended reasoning, ensuring the quality and correctness of self-generated data remains significant challenge. In this paper, we propose R-Zero, framework for training reasoning LLMs that can self-evolve from zero external data. In R-Zero, single base model is initialized with two roles Challenger and Solver that are independently optimized but co-evolve throughout the RL process. During co-evolving, the Challenger is rewarded for generating tasks targeted to be at the edge of Solvers current abilities, while the Solver is rewarded for solving increasingly challenging tasks posed by the Challenger. Framework details are provided in Section 3, but briefly, in the Challenger training phase, the Challenger is trained via Group Relative Policy Optimization (GRPO) (Shao et al., 2024) to generate difficult questions. The reward signal is derived from the uncertainty for the frozen Solver, which is measured by the self-consistency of its multiple generated answers. In the Solver training phase, the Solver is fine-tuned with GRPO on filtered set of these challenging questions generated by the now-frozen Challenger, using the pseudo-labels voted by itself. This entire process repeats, creating self-evolving cycle that operates without any human intervention. Our experiments demonstrate that R-Zero is model-agnostic framework, consistently and iteratively improving the reasoning abilities of different backbone LLMs. For example, Qwen3-4B-Base models average score on math benchmarks increased by significant +6.49 points after three iterations of self-evolution. Moreover, the reasoning skills learned through our math-focused questions can generalize to complex general-domain tasks, with models trained using R-Zero showing significant improvements on general domain reasoning benchmarks like MMLU-Pro (Wang et al., 2024) and SuperGPQA (Du et al., 2025). Our further analysis finds that R-Zero can act as mid-training method, as models first improved by our method achieve higher performance after fine-tuned on labeled data. In addition, we provide an in-depth analysis that validates our frameworks components, demonstrates its synergy with supervised fine-tuning, and characterizes the co-evolutionary dynamics to identify both strengths and limitations, offering insights for future research."
        },
        {
            "title": "2 Preliminaries",
            "content": "Our work builds upon recent advancements in reinforcement learning for fine-tuning large language models. We briefly review two key methodologies that are relevant to our framework. 2.1 Group Relative Policy Optimization Group Relative Policy Optimization (GRPO) (Shao et al., 2024) is reinforcement learning algorithm that fine-tunes policy LLM πθ without separate, learned value function. Its core idea is to normalize rewards based on the performance of other responses generated from the same prompt. 2 Technical Report For given prompt p, policy LLM πθold generates group of complete responses {x1, . . . , xG}. Each response xi is evaluated to receive single scalar reward ri. The rewards across the group are then normalized using z-score to compute response-level advantage: where εnorm is small constant added for numerical stability. ˆAi = ri mean(r1, . . . , rG) std(r1, . . . , rG) + εnorm , Policy Update. The policy is updated using clipped surrogate objective, similar to PPO, to ensure stable training. The objective, regularized by KL-divergence penalty to constrain policy drift, is: LGRPO(θ) = 1 i=1 min (cid:16) πθ (xi) (xi) πθold ˆAi, clip(cid:0) πθ (xi) (xi) , 1 ϵ, 1 + ϵ(cid:1) ˆAi πθold (cid:17) + β KL(cid:0)πθ πθold ). Maximizing the negative of this loss encourages the policy to increase the probability of generating responses with positive relative advantages, while the KL term, controlled by β, limits divergence from the previous policy. 2.2 Reinforcement Learning with Verifiable Rewards Reinforcement Learning with Verifiable Rewards (RLVR) (Lambert et al., 2024) is paradigm for finetuning models in domains where response quality can be deterministically verified. This approach relies on rule-based verifier : {0, 1} that assigns binary reward to each generation xi: ri = v(xi) = (cid:40) 1, if xi satisfies task-specific correctness check, 0, otherwise. This reward structure is especially effective for tasks like math, code generation with clear correctness criteria, and serves as the foundation for the reward mechanism in our Solver training."
        },
        {
            "title": "3 Method",
            "content": "3.1 Overview We propose R-Zero, fully automated framework featuring Challenger and Solver, both initialized from the same base LLM. The framework operates in an iterative loop. We illustrate the main framework in Figure 2. First, the Challenger (Qθ) is trained with Group Relative Policy Optimization (GRPO) to generate synthetic questions that are challenging for the current Solver (Sec. 3.2). training dataset of question-answer pairs is then constructed from these synthetic questions using filtering strategy and majority-vote mechanism (Sec. 3.3). Next, the Solver (Sϕ) is fine-tuned on this new dataset, also using GRPO (Sec. 3.4). This iterative process allows the Challenger and Solver to co-evolve, leading to progressively more capable Solver. The entire framework is self-supervised, requiring no human intervention. 3.2 Challenger Training The Challenger, Qθ, is an autoregressive language model trained to generate challenging questions. We train Qθ using the GRPO algorithm detailed in Sec. 2. The core of this process lies in designing reward function that accurately captures the desired properties of good question. This final scalar reward, ri, is then used in the GRPO advantage calculation. We focus on generating questions specifically within the domain of mathematics, as it provides convenient and selfcontained setting for our framework; the objective nature of mathematical answers allows for the straightforward generation of pseudo-labels via majority voting, without the need for external verification environments like code executors. 3 Technical Report Figure 2: An overview of our R-Zero framework, which illustrates the co-evolution of the Challenger and the Solver. Top: In the Challenger training phase, the Challenger is trained via GRPO to generate difficult questions. The reward signal is derived from the uncertainty for the frozen Solver, which is measured by the self-consistency of its multiple generated answers. Bottom: In the Solver training phase, the Solver is fine-tuned with GRPO on filtered set of these challenging questions generated by the now-frozen Challenger, using the pseudo-labels voted by itself. Uncertainty Reward. To guide the Challenger toward producing challenging yet solvable questions, we first define an uncertainty score. For generated question x, we query the current Solver Sϕ for responses {y1, . . . , ym}. The most frequent response is treated as the pseudo-label y(x), and 1{yj = y(x)}. The uncertainty we compute the Solvers empirical accuracy as ˆp(x; Sϕ) = 1 reward is then defined as: j=1 runcertainty(x; ϕ) = 1 2 (cid:12) (cid:12) (cid:12) ˆp(x; Sϕ) 2 (cid:12) (cid:12) (cid:12) This function incentivizes questions where the Solver is maximally uncertain (accuracy approaches 50%). We provide theoretical motivation for this reward function in Sec. 3.5. Repetition Penalty. To encourage diversity within training batch , we introduce repetition penalty. We could use any similarity metric, but in our case, we specifically use the BLEU score for faster computation, as this calculation must be performed numerous times during the rollout process. We compute pairwise distances using BLEU score similarity, dij = 1 BLEU(xi, xj), and group questions where dij < τBLEU into clusters = {C1, . . . , CK}. The penalty for question xi in cluster Ck is proportional to its relative size: rrep(xi) = λ Ck where is the batch size and λ is scaling factor. In our experiments, we set λ = 1. The implementation details are shown in Appendix A.4. Format Check Penalty. critical first step in the reward pipeline is structural format check to verify that each generated question is correctly enclosed within <question> and </question> tags. If the output does not adhere to this required structure, it is immediately assigned final reward of 0, and no further reward signals are computed. Composite Reward and Policy Update. For all questions that pass the format check, we calculate composite reward. The final scalar reward ri for each valid question xi combines signals for uncertainty and repetition: ri = max(cid:0)0, runcertainty(xi; ϕ) rrep(xi)(cid:1) With these rewards {r1, . . . , rG} for batch of generated questions, we compute the advantage ˆAi for each question and update the Challengers policy Qθ by minimizing the GRPO loss LGRPO(θ). 4 Technical Report 3.3 Solver Dataset Construction After updating the Challenger, we use it to generate new, curated dataset to train the Solver. This process acts as curriculum generator. We first sample large pool of candidate questions from the Challengers policy, xi Qθ( p0). For each question, we obtain answers from the current Solver, determine the pseudo-label yi via majority vote, and calculate the empirical correctness ˆpi. question-answer pair (xi, yi) is added to the training set only if its correctness falls within an informative band, ˆpi 1 2 δ. This filtering step discards tasks that are either too easy or too hard. While the primary goal of this filtering is to discard tasks that are too easy or too hard, it also serves as an implicit quality control mechanism. Since our pseudo-labels are derived from majority vote, very low empirical correctness ˆpi often indicates that the question itself is ambiguous, ill-posed, or that the resulting pseudo-label is unreliable. By filtering out these low-consistency items, our method simultaneously improves the quality and the uncertainty calibration of the training data. 3.4 Solver Training The Solver, Sϕ, is then fine-tuned on the curated dataset of challenging problems S. We also use GRPO for this stage, but with simpler, verifiable reward signal. For given question xi with its pseudo-label yi, the Solver generates batch of answers, each assigned binary reward rj: (cid:40) rj = if xj is identical to the pseudo-label yi, 1, 0, otherwise. This verifiable reward is used to compute the advantage ˆAj, and the Solvers policy Sϕ is subsequently updated by minimizing the GRPO loss LGRPO(ϕ). This process enhances the Solvers ability to correctly answer the difficult questions generated by its co-evolving Challenger. 3.5 Theoretical Analysis In this section, we provide theoretical motivation for our uncertainty reward function, runcertainty 1 2 ˆp(x; Sϕ) 1 2 , which is maximized when the Solvers success probability, ˆp, is 50%. Our analysis is grounded in recent work that formally establishes that the most efficient training occurs when learner is exposed to tasks at the frontier of its capabilities (Shi et al., 2025a; Bae et al., 2025). The core insight from these studies is that the learning potential of the current Solver, with policy Sϕ, can be quantified by the KL divergence to an optimal policy S. This divergence, DKL(SϕS), is lower-bounded by the variance of the Solvers reward. For the binary reward signal used in our framework, the success probability is ˆp. This leads to the specific lower bound: DKL(SϕS) ˆp(1 ˆp) 2β2 where β is the temperature parameter controlling entropy regularization. The right-hand side of the inequality, which is proportional to the reward variance, is maximized precisely when ˆp = 0.5. Therefore, by designing the Challengers reward to incentivize questions that push the current Solver towards this point of maximum uncertainty, our framework is theoretically motivated to generate maximally efficient curriculum in each iteration of the co-evolutionary process."
        },
        {
            "title": "4 Experiments",
            "content": "4.1 Experiments Setting 4.1.1 Models We employ the Qwen3-4B-Base (Yang et al., 2025) and Qwen3-8B-Base models to assess the impact of scale within single architectural family. Second, to ensure our approach is effective on distinct 5 Technical Report lineage, we utilize the OctoThinker-3B and OctoThinker-8B models (Wang et al., 2025b).This choice is particularly relevant as Wang et al. (2025b) reported that applying RL training directly to Llama models yielded suboptimal results. As the OctoThinker series is continually trained from the Llama3.1 models (Dubey et al., 2024), this comprehensive selection allows us to test our framework across different foundational architectures Qwen vs. Llama. 4.1.2 Evaluation Benchmark We assess our framework on comprehensive suite of benchmarks. Although the question-generator prompt for our method is primarily focused on mathematical problem-solving, key objective of our evaluation is to explore whether the resulting improvements in reasoning ability can generalize to other domains. Therefore, our evaluation is divided into two main categories. Mathematical Reasoning. We use seven challenging benchmarks: AMC, Minerva (Lewkowycz et al., 2022), MATH-500 (Hendrycks et al., 2021b), GSM8K (Cobbe et al., 2021), Olympiad-Bench (He et al., 2024), AIME-2024, and AIME-2025. For these tasks, where answers can be complex, we employ GPT-4o as programmatic judge to semantically verify the correctness of the final answer against the ground truth. For the highly AMC and AIME benchmarks, we report the mean@32 metric. For all other math benchmarks, we report accuracy based on greedy decoding. General Domain Reasoning. To test for the generalization of reasoning ability, we evaluate on the following challenging benchmarks: MMLU-Pro (Wang et al., 2024): An enhanced version of the MMLU (Hendrycks et al., 2021a) benchmark, featuring more challenging suite of multi-task questions designed to provide stricter evaluation of language model capabilities. SuperGPQA (Du et al., 2025): large-scale benchmark focused on graduate-level reasoning. It comprises questions across 285 distinct disciplines that have been verified as unsearchable on the web, thereby isolating true reasoning ability from simple knowledge recall. BBEH (shoaa kazemi et al., 2025): This benchmark builds upon the foundation of BIG-Bench Hard (Suzgun et al., 2023) by incorporating new selection of tasks specifically engineered to be more difficult, thus providing more accurate measure of complex reasoning skills. For this category, we follow the experimental setup, prompts, and evaluation codes from (Ma et al., 2025), reporting Exact Match (EM) accuracy obtained via greedy decoding. 4.1.3 Training Details Our entire framework is implemented based on the EasyR1 codebase (Zheng et al., 2025). In each iteration of the R-Zero co-evolutionary loop, we follow specific set of hyperparameters. The Challenger (Qθ) first generates candidate pool of = 8, 000 questions. To construct the training dataset for the Solver, these questions are filtered based on consistency. For each candidate question, we sample = 10 answers from the current Solver (Sϕ). question is retained for the training set only if the number of answers matching the majority-vote pseudo-label is between 3 and 7, inclusive (δ = 0.25). This numerical range is consistent with the methodology used in previous research (Zhang & Zuo, 2025; Li et al., 2025b; Bercovich et al., 2025). When training the Challenger, the uncertainty reward r(x; ϕ) is calculated by sampling = 10 responses from the Solver. For the intra-batch repetition penalty, we set the clustering distance threshold to τBLEU = 0.5. Further implementation details and prompts can be found in Appendix A. 4.2 Results in Mathematical Reasoning The comprehensive results of our experiments are presented in Table 1. The findings confirm that our proposed framework, R-Zero, is highly effective, model-agnostic method for enhancing the performance of language models on mathematical tasks across different architectures and scales. 6 Technical Report Table 1: Comprehensive results on mathematical reasoning benchmarks. We compare each base model against Base Challenger baseline (where the Solver is trained on questions from an untrained Challenger) and our iterative method, R-Zero. The peak performance achieved during each models training process is highlighted in bold. Model Name AVG AMC Minerva MATH GSM8K Olympiad AIME25 AIME24 Qwen3-4B-Base Base Model Base Challenger R-Zero (Iter 1) R-Zero (Iter 2) R-Zero (Iter 3) Qwen3-8B-Base Base Model Base Challenger R-Zero (Iter 1) R-Zero (Iter 2) R-Zero (Iter 3) OctoThinker-3B Base Model Base Challenger R-Zero (Iter 1) R-Zero (Iter 2) R-Zero (Iter 3) OctoThinker-8B Base Model Base Challenger R-Zero (Iter 1) R-Zero (Iter 2) R-Zero (Iter 3) 42.58 44.36 48.06 48.44 49.07 49.18 51.87 53.39 53.84 54.69 26.64 27.51 27.76 28.20 29. 36.41 36.98 37.80 38.23 38.52 45.70 45.00 51.56 52.50 57.27 51.95 60.70 61.56 61.56 61.67 17.19 20.19 20.39 24.06 27.03 32.11 29.30 32.97 32.58 34.03 38.24 45.22 51.47 51.47 52. 50.00 57.72 59.93 59.93 60.66 24.26 24.63 25.74 25.37 27.57 41.91 42.28 45.22 48.53 48.22 68.20 72.80 78.60 79.80 79.60 78.00 81.60 82.00 82.00 82.00 55.00 54.60 54.60 54.80 54. 65.20 66.20 65.60 67.20 68.80 87.79 87.87 91.28 91.66 92.12 89.08 92.56 93.71 93.93 94.09 73.69 74.98 75.51 74.45 74.98 86.96 88.10 86.96 87.11 87.19 41.04 41.19 43.85 44.30 44. 44.74 46.44 48.00 48.30 48.89 16.15 15.70 16.30 17.48 18.22 26.52 27.56 28.44 27.26 27.56 6.15 7.29 9.17 4.27 4.27 16.67 13.44 14.17 17.60 19.17 0.21 0.10 0.10 0.00 3. 1.56 1.04 1.98 0.00 0.42 10.94 11.15 10.52 15.10 12.71 13.85 10.62 14.37 13.54 16.35 0.00 2.40 1.67 1.25 0.00 0.62 4.38 3.44 4.90 3.44 Our iterative training process consistently and substantially improves upon the performance of the base models. This holds true for large models like Qwen3-8B-Base, where three iterations of R-Zero raise the average performance from baseline of 49.18 to 54.69, significant gain of +5.51 points. Similarly, on the smaller OctoThinker-3B, our method improves the average score from 26.64 to 29.32 (+2.68 points), demonstrating the broad applicability of our self-supervised training loop. This improvement is progressive, with the results showing clear trend of performance gains across iterations. For instance, the Qwen3-8B-Base models average score climbs from base performance of 49.18 to 53.39 (Iter 1) and ultimately reaches 54.69 (Iter 3). similar monotonic improvement is observed on OctoThinker-3B, which progresses from its base score of 26.64 to 29.32 after three iterations. This consistent growth underscores the benefits of the co-evolutionary dynamic, where the progressively more capable Solver learns from an increasingly challenging curriculum. The critical role of the Challengers RL-based training is validated by the immediate performance leap from the Base Challenger to the first iteration of R-Zero. On Qwen3-8B-Base, this first iteration provides +1.52 point gain over the baseline, and the improvement is even more pronounced on Qwen3-4B-Base at +3.7 points. This confirms that the intelligent curriculum generated by the RL-trained Challenger is significantly more effective than that of non-trained generator. 4.3 Results in General Reasoning Previous work has demonstrated that training language models on reasoning-intensive domains, such as mathematics, can lead to improvements in general-domain capabilities (Huan et al., 2025). 7 Technical Report Table 2: Results on general-domain reasoning benchmarks. The table compares the Base Model, Zero-Shot Challenger baseline, and our iterative R-Zero. The peak performance achieved during each models training process is highlighted in bold. Model Name Overall AVG MATH AVG SuperGPQA MMLU-Pro BBEH Qwen3-4B-Base Base Model Base Challenger R-Zero (Iter 1) R-Zero (Iter 2) R-Zero (Iter 3) Qwen3-8B-Base Base Model Base Challenger R-Zero (Iter 1) R-Zero (Iter 2) R-Zero (Iter 3) OctoThinker-3B Base Model Base Challenger R-Zero (Iter 1) R-Zero (Iter 2) R-Zero (Iter 3) OctoThinker-8B Base Model Base Challenger R-Zero (Iter 1) R-Zero (Iter 2) R-Zero (Iter 3) 27.10 30.83 34.27 34.92 34.64 34.49 36.43 37.93 38.45 38.73 12.27 14.41 14.93 15.11 15.67 16.81 25.08 26.44 26.77 26.88 42.58 44.36 48.06 48.44 49. 49.18 51.87 53.39 53.84 54.69 26.64 27.51 27.76 28.20 29.32 32.11 36.41 37.80 38.23 38.52 20.88 24.77 27.92 27.72 27.55 28.33 30.12 31.26 31.58 31.38 10.09 11.19 12.21 12.43 12. 13.26 16.99 19.15 19.27 19.82 37.38 47.59 51.69 53.75 51.53 51.80 54.14 57.17 58.20 58.23 10.87 14.53 15.72 16.08 16.71 20.21 41.46 42.05 41.34 40.92 7.57 6.59 9.42 9.76 10. 8.63 9.60 9.91 10.20 10.60 1.46 4.40 4.05 3.74 4.20 1.64 5.46 6.77 8.25 8.25 key question, however, is whether this generalization effect still holds when the training curriculum is not human-labeled, but entirely self-generated through R-Zero. As shown in Table 2, this transfer of skills is evident across all tested models. For instance, three iterations of our math-focused training improve the average general-domain score of Qwen3-8B-Base by +3.81 points and OctoThinker-3B by +3.65 points. This generalization also extends to the key performance patterns observed in the mathematical results, with progressive iterative gains. This confirms that our method does not merely teach domain-specific knowledge, but enhances the models underlying capabilities in way that successfully generalizes across domains."
        },
        {
            "title": "5 Analysis",
            "content": "In this section, we conduct series of in-depth analyses to better understand the behavior and effectiveness of our R-Zero framework. To ensure consistency, all analytical experiments presented here were conducted on the Qwen3-4B-Base model, unless explicitly stated otherwise. 5.1 Ablation Study To isolate the contribution of each key component within our R-Zero framework, we conduct comprehensive ablation study on the Qwen3-4B-Base model. We specifically investigate the importance of three critical modules by disabling them one at time and observing the impact on performance. The results are summarized in Table 3. 8 Technical Report As shown in the table, removing any core components leads to significant degradation in performance. The largest drop occurs when we disable the Challengers reinforcement learning (w/o RL-Challenger), with the Math and General average scores decreasing by 3.7 and 4.1 points, respectively. This result highlighting the importance of our co-evolutionary curriculum generation process. Similarly, removing the Repetition Penalty also harms performance, indicating that generating diverse set of questions is crucial for effective Solver training. Table 3: Ablation study results on the Qwen3-4BBase model. w/o RL-Challenger: Disables GRPO training for the Challenger. w/o Filtering: Disables the difficulty-based curriculum filtering. w/o Rep. Penalty: Removes the repetition penalty from the Challengers reward. Method R-Zero (full) Ablations Math AVG General AVG 48. 30.41 w/o RL-Challenger w/o Rep. Penalty w/o Filtering 44.36 45.76 47.35 26.32 27.56 24.26 Finally, disabling the Task Filtering module results in notable performance drop, particularly on the general-domain average, which falls by over 6 points. As discussed in Section 3.3, this filtering serves dual purpose: it calibrates the curriculums difficulty and acts as an implicit quality control mechanism by removing questions with low answer consistency. Without this filter, the Solver is trained on noisy and poorly curated dataset that likely includes ambiguous or ill-posed questions, which harms its ability to learn robustly. 5.2 Evolution of Question Difficulty and Data Accuracy Table 4: Performance and data accuracy analysis. The highlighted column represents the true accuracy of the self-generated pseudo-labels for each question set. Performance of Evaluated Model (vs. Ground Truth) Base Model Solver (Iter 1) Solver (Iter 2) Solver (Iter 3) Pseudo-Label Acc. DIter 1 DIter 2 DIter 3 48.0 52.5 44.0 59.0 53.0 47.0 57.0 51.5 45.0 61.0 53.5 50. 79.0% 69.0% 63.0% To understand the co-evolutionary dynamic, we analyzed how the Challengers generated questions and their corresponding pseudo-labels change across iterations. We sampled 200 questions from the Challengers policy after each of the first three training iterations, creating three distinct test sets: DIter 1, DIter 2, and DIter 3. For this analysis, we assumed the external oracle model, GPT-4o, to be perfect annotator, providing the ground truth answers for all generated questions. The evaluation was conducted as follows: the performance of our internal models was measured against these GPT-4o ground truth answers. The score reported for GPT-4o itself, however, reflects the true accuracy of our self-generated pseudo-labels by comparing the pseudo label against the ground truth from the oracle(GPT-4o). The results on the filtered dataset are summarized in Table 4. This analysis reveals multi-faceted dynamic. The first finding is that the questions generated by the Challenger become progressively more difficult. This is directly evidenced by evaluating fixed model against the evolving question sets. For instance, the performance of the static Solver (Iter 1), when measured against the consistent GPT-4o ground truth, drops from 59.0% on Iteration 1 questions to 47.0% on Iteration 3 questions. This confirms that the Challenger is successfully increasing the intrinsic difficulty of its curriculum. The second finding, revealed by the highlighted column, pertains to the true accuracy of the self-generated dataset. Unfortunately, while the accuracy of the pseudo-labels is initially high at 79.0%, it systematically drops to 63.0% by the third iteration. This trend indicates that as the system generates more difficult problems, the Solvers majority vote becomes less reliable source for ground truth. This decline in data quality is critical trade-off and potential bottleneck for the frameworks ultimate performance. 9 Technical Report Finally, despite this drop in absolute label accuracy, the frameworks internal reward mechanism functions precisely as designed. The scores on the tables diagonal show how each Solver performs on questions from its contemporary Challenger. The Solver (Iter 2) achieves 51.5% and the Solver (Iter 3) achieves 50.5% on their respective question sets. This demonstrates that the Challenger successfully calibrates the question difficulty to match the Solvers evolving capabilities, consistently targeting the 50% success rate that our reward function incentivizes. 5.3 Synergy with Supervised Data To analyze the utility of our framework in scenarios where labeled dataset is available, we measure the synergy between R-Zero and traditional supervised fine-tuning using labeled datasets1. The GRPO settings for this experiment were kept identical to our main experiments. We first establish supervised baseline by finetuning the base model directly on the labeled data. For this process, we employ GRPO, an approach similar to Zero-RL (Zeng et al., 2025). We then apply our R-Zero framework, where at the end of each co-evolutionary iteration, the resulting checkpoint is also fine-tuned on the same labeled dataset. The results show that our method provides significant additional gains. As highlighted in Figure 3, this represents gain of +2.35 points over the direct training baseline. This finding confirms that R-Zero is not redundant with labeled data; instead, it acts as powerful performance amplifier. The co-evolutionary process enables the model to better leverage the supervised information and achieve performance levels unattainable by standard fine-tuning alone. Figure 3: Performance of R-Zero when combined with supervised fine-tuning. The dashed line represents the baseline of fine-tuning the base model on labelled data alone, showing that our iterative method provides better initialization."
        },
        {
            "title": "6 Related Work",
            "content": "6.1 Label-Free Reinforcement Learning significant trend in recent research is Label-Free Reinforcement Learning, which aims to improve LLM reasoning without human-annotated data. Many such methods use the models own outputs as reward signal. This includes leveraging sequence-level confidence (Li et al., 2025a; Prabhudesai et al., 2025), the consistency of answers derived from varied reasoning paths (Zhang et al., 2025a; Zuo et al., 2025; Zhang et al., 2025b), minimizing the output entropy (Agarwal et al., 2025; Cheng et al., 2025), or even random (Shao et al., 2025) or negative reward (Zhu et al., 2025). These signals are often used within self-training loops where models fine-tune on their own most plausible solutions (Shafayat et al., 2025; Zhao et al., 2025b). While these methods all rely on pre-existing set of unlabeled problems, R-Zero removes the need for any seed dataset. 6.2 Self-Play in Large Language Models The paradigm of self-play, where models take on dual roles to create self-improvement loop, has recently been adapted to improve language models without human data. This approach has been particularly fruitful in verifiable domains like code generation, where Coder agents program is verified by Tester agents unit tests (Lin et al., 2025; Wang et al., 2025a; Pourcel et al., 2025). More advanced frameworks push autonomy further by learning to generate the problems themselves, creating an adaptive curriculum from small seed of examples or from scratch (Zhao et al., 2025a; Li et al., 2025c; Zhou et al., 2025; Fang et al., 2025). Our work distinguishes itself by extending this 1https://huggingface.co/datasets/hiyouga/math12k Technical Report paradigm to general reasoning domains that lack such verifiable environments, instead learning from reward signal derived from the models own internal consistency. 6.3 Reinforcement Learning with Verifiable Rewards (RLVR) Reinforcement Learning with Verifiable Rewards (RLVR) has been widely adopted as versatile paradigm for enhancing LLMs across multitude of tasks. Its effectiveness is demonstrated in diverse applications such as relation extraction (Dai et al., 2025), interactive GUI navigation (Shi et al., 2025b), search-engine utilization (Jin et al., 2025), and multi-modal alignment (Zhan et al., 2025).While early implementations relied on rule-based verifiers, recent work has begun to explore more sophisticated, model-based verifiers(Ma et al., 2025; Li et al., 2025b)."
        },
        {
            "title": "7 Conclusion and Future Work",
            "content": "In this paper, we introduced R-Zero, fully autonomous self-evolving framework that overcomes data dependency by having Challenger and Solver co-evolve to create self-generating curriculum. Our experiments demonstrate that R-Zero significantly improves LLMs reasoning capability on multiple domains. Future work could further focus on improving efficiency, exploring more robust labeling techniques, and expanding R-Zero to new domains. It is crucial to note, however, that the core mechanism of R-Zero is currently suited for domains where correctness can be objectively determined. Extending this self-evolutionary paradigm to open-ended generative tasks, such as creative writing or dialogue, where evaluation is subjective, remains significant hurdle for future research. We believe R-Zero is significant step towards creating truly self-evolving LLMs."
        },
        {
            "title": "References",
            "content": "Shivam Agarwal, Zimin Zhang, Lifan Yuan, Jiawei Han, and Hao Peng. The unreasonable effectiveness of entropy minimization in llm reasoning. ArXiv preprint, abs/2505.15134, 2025. Sanghwan Bae, Jiwoo Hong, Min Young Lee, Hanbyul Kim, JeongYeon Nam, et al. Online difficulty filtering for reasoning oriented reinforcement learning. ArXiv preprint, abs/2504.03380, 2025. Akhiad Bercovich, Itay Levy, Izik Golan, Mohammad Dabbah, Ran El-Yaniv, et al. Llama-nemotron: Efficient reasoning models. ArXiv preprint, abs/2505.00949, 2025. Daixuan Cheng, Shaohan Huang, Xuekai Zhu, Bo Dai, Wayne Xin Zhao, et al. Reasoning with exploration: An entropy perspective. ArXiv preprint, abs/2506.14758, 2025. Karl Cobbe, Vineet Kosaraju, Mo Bavarian, Mark Chen, Heewoo Jun, et al. Training verifiers to solve math word problems. ArXiv preprint, abs/2110.14168, 2021. Runpeng Dai, Tong Zheng, Run Yang, and Hongtu Zhu. R1-re: Cross-domain relationship extraction with rlvr. ArXiv preprint, abs/2507.04642, 2025. DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Jun-Mei Song, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. ArXiv preprint, abs/2501.12948, 2025. Xinrun Du, Yifan Yao, Kaijing Ma, Bingli Wang, Tianyu Zheng, et al. Supergpqa: Scaling llm evaluation across 285 graduate disciplines. ArXiv preprint, abs/2502.14739, 2025. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, et al. The llama 3 herd of models. ArXiv preprint, abs/2407.21783, 2024. Wenkai Fang, Shunyu Liu, Yang Zhou, Kongcheng Zhang, Tongya Zheng, et al. Serl: Self-play reinforcement learning for large language models with limited data. ArXiv preprint, abs/2505.20347, 2025. Technical Report Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, et al. Olympiadbench: challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems. In Annual Meeting of the Association for Computational Linguistics, 2024. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. In Proc. of ICLR, 2021a. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, et al. Measuring mathematical problem solving with the math dataset. ArXiv preprint, abs/2103.03874, 2021b. Maggie Huan, Yuetai Li, Tuney Zheng, Xiaoyu Xu, Seungone Kim, et al. Does math reasoning improve general llm capabilities? understanding transferability of llm reasoning. 2025. Chengsong Huang, Langlin Huang, Jixuan Leng, Jiacheng Liu, and Jiaxin Huang. Efficient test-time scaling via self-calibration, 2025. Bowen Jin, Hansi Zeng, Zhenrui Yue, Dong Wang, Hamed Zamani, and Jiawei Han. Search-r1: Training llms to reason and leverage search engines with reinforcement learning. ArXiv preprint, abs/2503.09516, 2025. Nathan Lambert, Jacob Daniel Morrison, Valentina Pyatkin, Shengyi Huang, Hamish Ivison, et al. ulu 3: Pushing frontiers in open language model post-training. ArXiv preprint, abs/2411.15124, 2024. Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay V. Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, and Vedant Misra. Solving quantitative reasoning problems with language models. In Sanmi Koyejo, S. Mohamed, A. Agarwal, Danielle Belgrave, K. Cho, and A. Oh (eds.), Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022, 2022. Pengyi Li, Matvey Skripkin, Alexander Zubrey, Andrey Kuznetsov, and Ivan V. Oseledets. Confidence is all you need: Few-shot rl fine-tuning of language models. ArXiv preprint, abs/2506.06395, 2025a. Zongxia Li, Yapei Chang, Yuhang Zhou, Xiyang Wu, Zichao Liang, Yoo Yeon Sung, and Jordan Lee Boyd-Graber. Semantically-aware rewards for open-ended r1 training in free-form generation, 2025b. Zongxia Li, Xiyang Wu, Guangyao Shi, Yubin Qin, Hongyang Du, Tianyi Zhou, Dinesh Manocha, and Jordan Lee Boyd-Graber. Videohallu: Evaluating and mitigating multi-modal hallucinations on synthetic video understanding, 2025c. Zi Lin, Sheng Shen, Jingbo Shang, Jason Weston, and Yixin Nie. Learning to solve and verify: self-play framework for code and test generation. ArXiv preprint, abs/2502.14948, 2025. Xueguang Ma, Qian Liu, Dongfu Jiang, Ge Zhang, Zejun Ma, et al. General-reasoner: Advancing llm reasoning across all domains. ArXiv preprint, abs/2505.14652, 2025. Julien Pourcel, Cedric Colas, and Pierre-Yves Oudeyer. Self-improving language models for evolutionary program synthesis: case study on arc-agi. 2025. Mihir Prabhudesai, Lili Chen, Alex Ippoliti, Katerina Fragkiadaki, Hao Liu, et al. Maximizing confidence alone improves reasoning. ArXiv preprint, abs/2505.22660, 2025. Sheikh Shafayat, Fahim Tajwar, Ruslan Salakhutdinov, Jeff Schneider, and Andrea Zanette. Can large reasoning models self-train? ArXiv preprint, abs/2505.21444, 2025. Rulin Shao, Shuyue Stella Li, Rui Xin, Scott Geng, Yiping Wang, et al. Spurious rewards: Rethinking training signals in rlvr, 2025. 12 Technical Report Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Jun-Mei Song, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. ArXiv preprint, abs/2402.03300, 2024. Taiwei Shi, Yiyang Wu, Linxin Song, Tianyi Zhou, and Jieyu Zhao. Efficient reinforcement finetuning via adaptive curriculum learning. ArXiv preprint, abs/2504.05520, 2025a. Yucheng Shi, Wenhao Yu, Zaitang Li, Yonglin Wang, Hongming Zhang, et al. Mobilegui-rl: Advancing mobile gui agent through reinforcement learning in online environment. 2025b. Mehrangiz shoaa kazemi, Bahare Fatemi, Hritik Bansal, John Palowitch, Chrysovalantis Anastasiou, et al. Big-bench extra hard. In Annual Meeting of the Association for Computational Linguistics, 2025. Yi Su, Dian Yu, Linfeng Song, Juntao Li, Haitao Mi, et al. Crossing the reward bridge: Expanding rl with verifiable rewards across diverse domains. ArXiv preprint, abs/2503.23829, 2025. Mirac Suzgun, Nathan Scales, Nathanael Scharli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc Le, Ed Chi, Denny Zhou, and Jason Wei. Challenging BIG-bench tasks and whether chain-of-thought can solve them. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (eds.), Findings of the Association for Computational Linguistics: ACL 2023, pp. 1300313051, 2023. doi: 10.18653/v1/2023.findings-acl.824. Zhengwei Tao, Ting-En Lin, Xiancai Chen, Hangyu Li, Yuchuan Wu, et al. survey on self-evolution of large language models. ArXiv preprint, abs/2404.14387, 2024. Yinjie Wang, Ling Yang, Ye Tian, Ke Shen, and Mengdi Wang. Co-evolving llm coder and unit tester via reinforcement learning. ArXiv preprint, abs/2506.03136, 2025a. Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, Tianle Li, Max Ku, Kai Wang, Alex Zhuang, Rongqi Fan, Xiang Yue, and Wenhu Chen. Mmlu-pro: more robust and challenging multi-task language understanding benchmark. In Amir Globersons, Lester Mackey, Danielle Belgrave, Angela Fan, Ulrich Paquet, Jakub M. Tomczak, and Cheng Zhang (eds.), Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024, 2024. Zengzhi Wang, Fan Zhou, Xuefeng Li, and Pengfei Liu. Octothinker: Mid-training incentivizes reinforcement learning scaling. ArXiv preprint, abs/2506.20512, 2025b. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, et al. Qwen3 technical report. ArXiv preprint, abs/2505.09388, 2025. Weihao Zeng, Yuzhen Huang, Qian Liu, Wei Liu, Keqing He, et al. Simplerl-zoo: Investigating and taming zero reinforcement learning for open base models in the wild. ArXiv preprint, abs/2503.18892, 2025. Yufei Zhan, Yousong Zhu, Shurong Zheng, Hongyin Zhao, Fan Yang, Ming Tang, and Jinqiao Wang. Vision-r1: Evolving human-free alignment in large vision-language models via vision-guided reinforcement learning. ArXiv preprint, abs/2503.18013, 2025. Jixiao Zhang and Chunsheng Zuo. Grpo-lead: difficulty-aware reinforcement learning approach for concise mathematical reasoning in language models. ArXiv preprint, abs/2504.09696, 2025. Kongcheng Zhang, Qi Yao, Shunyu Liu, Yingjie Wang, Baisheng Lai, et al. Consistent paths lead to truth: Self-rewarding reinforcement learning for llm reasoning. ArXiv preprint, abs/2506.08745, 2025a. Qingyang Zhang, Haitao Wu, Changqing Zhang, Peilin Zhao, and Yatao Bian. Right question is already half the answer: Fully unsupervised llm reasoning incentivization. ArXiv preprint, abs/2504.05812, 2025b. 13 Technical Report Andrew Zhao, Yiran Wu, Yang Yue, Tong Wu, Quentin Xu, et al. Absolute zero: Reinforced self-play reasoning with zero data. ArXiv preprint, abs/2505.03335, 2025a. Xuandong Zhao, Zhewei Kang, Aosong Feng, Sergey Levine, and Dawn Xiaodong Song. Learning to reason without external rewards. ArXiv preprint, abs/2505.19590, 2025b. Yaowei Zheng, Junting Lu, Shenzhi Wang, Zhangchi Feng, Dongdong Kuang, et al. Easyr1: An efficient, scalable, multi-modality rl training framework. https://github.com/hiyouga/EasyR1, 2025. Yifei Zhou, Sergey Levine, Jason E. Weston, Xian Li, and Sainbayar Sukhbaatar. Self-challenging language model agents. ArXiv preprint, abs/2506.01716, 2025. Xinyu Zhu, Mengzhou Xia, Zhepei Wei, Wei-Lin Chen, Danqi Chen, et al. The surprising effectiveness of negative reinforcement in llm reasoning. ArXiv preprint, abs/2506.01347, 2025. Yuxin Zuo, Kaiyan Zhang, Shang Qu, Li Sheng, Xuekai Zhu, et al. Ttrl: Test-time reinforcement learning. ArXiv preprint, abs/2504.16084, 2025. Technical Report"
        },
        {
            "title": "A Experiment Details",
            "content": "A.1 Training Hyperparameter This section summarizes the most critical algorithmic hyperparameters for the Solver and Challenger training stages. All experiments were conducted using BFloat16 (BF16) mixed precision and FlashAttention 2. A.1.1 Solver Training Global Batch Size: 128 Learning Rate: 1 106 Weight Decay: 1 10 KL Penalty Coefficient (λKL): 1 102 Max Steps: 15 Number of Rollouts: 5 Rollout Temperature: 1.0 Rollout Top-p: 0.99 A.1.2 Challenger Training Global Batch Size: 128 Learning Rate: 1 106 Weight Decay: 1 102 KL Penalty Coefficient (λKL): 1 102 Max Steps: 5 Number of Rollouts: Rollout Temperature: 1.0 Rollout Top-p: 0.99 A.2 Prompt Templates This section presents the exact prompt templates used for the solver and challenger models. Solver Prompt Template System Message: Please reason step by step, and put your final answer within boxed{}. User Message: {problem statement} Note: {problem statement} is placeholder for the actual math problem. 15 Technical Report Challenger Prompt Template System Message: You are an expert competition-math problem setter. FIRST, in your private scratch-pad, think step-by-step to design brand-new, non-trivial problem. The problem could come from any field of mathematics, including but not limited to algebra, geometry, number theory, combinatorics, prealgebra, probability, statistics, and calculus. Aim for difficulty such that fewer than 30% of advanced high-school students could solve it. Avoid re-using textbook cliches or famous contest problems. THEN, without revealing any of your private thoughts, output exactly the following two blocks: <question> {The full problem statement on one or more lines} </question> boxed{final answer} Do NOT output anything elseno explanations, no extra markup. User Message: Generate one new, challenging reasoning question now. Remember to format the output exactly as instructed. A.3 GPT-4o Judge Prompt To programmatically evaluate the correctness of answers on mathematical benchmarks where the final answer can be complex (e.g., simplified expressions), we use GPT-4o as judge. The exact prompt and configuration used for this evaluation are detailed below. Configuration for GPT-4o as Judge Model: gpt-4o Temperature: 0.1 System Message: You are math answer checker. User Message Template: Hi, there is an answer: {answer}, and the ground truth answer is: {response}, please check whether the answer is correct or not, and return the **only** Yes or No. Note: {answer} is placeholder for the model-generated solution, and {response} is the groundtruth answer from the benchmark. A.4 Repetition Penalty Implementation To encourage the Challenger to generate diverse set of questions within each batch, we apply repetition penalty, rrep. This penalty is designed to disincentivize the model from producing semantically similar questions in the same batch. The implementation is multi-step process based on clustering questions by their BLEU score similarity. 16 Technical Report 1. Pairwise Distance Calculation via BLEU Score First, we compute pairwise distance matrix for all questions in batch. The distance dij between any two questions, xi and xj, is defined as one minus their BLEU score: dij = 1 BLEU(xi, xj) For this calculation, we specifically use the sentence bleu function from the NLTK library (nltk.translate.bleu score). To ensure numerical stability, especially for shorter questions with limited n-gram overlap, we employ its first smoothing function, SmoothingFunction().method1. The questions are tokenized for the BLEU calculation by splitting on whitespace; no further text normalization, such as lowercasing or punctuation removal, is performed. 2. Agglomerative Clustering With the pairwise distance matrix computed, we then group similar questions using agglomerative hierarchical clustering. This step is performed using the Clustering implementation from the scikit-learn library. The clustering algorithm is configured with the following key parameters: Metric: Set to precomputed, indicating that we provide our custom BLEU-based distance matrix instead of having the algorithm compute distances. Linkage: Set to average. This method defines the distance between two clusters as the average of the distances between all pairs of questions across the two clusters. 3. Final Penalty Calculation Once each question in the batch is assigned to cluster, the repetition penalty rrep(xi) for given question xi is determined by the relative size of the cluster Ck to which it belongs. The penalty is calculated as: rrep(xi) = Ck Here, Ck represents the number of questions in cluster Ck, and is the total number of questions in the batch (i.e., the batch size)."
        }
    ],
    "affiliations": [
        "Tencent AI Seattle Lab",
        "The University of Texas at Dallas",
        "University of Maryland, College Park",
        "Washington University in St. Louis"
    ]
}