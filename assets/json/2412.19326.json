{
    "paper_title": "Task Preference Optimization: Improving Multimodal Large Language Models with Vision Task Alignment",
    "authors": [
        "Ziang Yan",
        "Zhilin Li",
        "Yinan He",
        "Chenting Wang",
        "Kunchang Li",
        "Xinhao Li",
        "Xiangyu Zeng",
        "Zilei Wang",
        "Yali Wang",
        "Yu Qiao",
        "Limin Wang",
        "Yi Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Current multimodal large language models (MLLMs) struggle with fine-grained or precise understanding of visuals though they give comprehensive perception and reasoning in a spectrum of vision applications. Recent studies either develop tool-using or unify specific visual tasks into the autoregressive framework, often at the expense of overall multimodal performance. To address this issue and enhance MLLMs with visual tasks in a scalable fashion, we propose Task Preference Optimization (TPO), a novel method that utilizes differentiable task preferences derived from typical fine-grained visual tasks. TPO introduces learnable task tokens that establish connections between multiple task-specific heads and the MLLM. By leveraging rich visual labels during training, TPO significantly enhances the MLLM's multimodal capabilities and task-specific performance. Through multi-task co-training within TPO, we observe synergistic benefits that elevate individual task performance beyond what is achievable through single-task training methodologies. Our instantiation of this approach with VideoChat and LLaVA demonstrates an overall 14.6% improvement in multimodal performance compared to baseline models. Additionally, MLLM-TPO demonstrates robust zero-shot capabilities across various tasks, performing comparably to state-of-the-art supervised models. The code will be released at https://github.com/OpenGVLab/TPO"
        },
        {
            "title": "Start",
            "content": "4 2 0 2 6 2 ] . [ 1 6 2 3 9 1 . 2 1 4 2 : r Task Preference Optimization: Improving Multimodal Large Language Models with Vision Task Alignment Ziang Yan2,1, Zhilin Li3,1, Yinan He1 Chenting Wang4,1, Kunchang Li5,1, Xinhao Li6,1, Xiangyu Zeng6,1 Zilei Wang3, Yali Wang5,1,Yu Qiao1, Limin Wang6,1, Yi Wang 1Shanghai AI Laboratory 2Zhejiang University 3University of Science and Technology of China 4Shanghai Jiao Tong University 5Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences 6Nanjing University https://github.com/OpenGVLab/TPO"
        },
        {
            "title": "Abstract",
            "content": "Current multimodal large language models (MLLMs) struggle with fine-grained or precise understanding of visuals though they give comprehensive perception and reasoning in spectrum of vision applications. Recent studies either develop tool-using or unify specific visual tasks into the autoregressive framework, often at the expense of overall multimodal performance. To address this issue and enhance MLLMs with visual tasks in scalable fashion, we propose Task Preference Optimization (TPO), novel method that utilizes differentiable task preferences derived from typical fine-grained visual tasks. TPO introduces learnable task tokens that establish connections between multiple task-specific heads and the MLLM. By leveraging rich visual labels during training, TPO significantly enhances the MLLMs multimodal capabilities and task-specific performance. Through multi-task co-training within TPO, we observe synergistic benefits that elevate individual task performance beyond what is achievable through single-task training methodologies. Our instantiation of this approach with VideoChat and LLaVA demonstrates an overall 14.6% improvement in multimodal performance compared to baseline models. Additionally, MLLM-TPO demonstrates robust zero-shot capabilities across various tasks, performing comparably to state-of-the-art supervised models. 1. Introduction Multimodal large language models (MLLMs) show impressive visual perception and reasoning capabilities, with applications in personal assistants [64, 71], embodied systems [20, 91], and scientific discovery [78], among others [6, 13, 58]. Considering the growing expectations of users *Equal contribution. Corresponding author. Figure 1. TPO uses differentiable task preferences from dense visual supervisions via task-specific heads to enhance MLLMs in fine-grained understanding. in more accurate and detailed perception, taking the shell game as an example, further improving the generality of MLLM requires fine-grained knowledge representation beyond words, e.g. MLLM implicitly embeds how to track keys indicated by users. Existing studies address the enhancement of perceptual granularity in MLLMs by focusing on specific visual tasks (e.g. temporal grounding, segmentation, tracking) via MLLMs. They usually fine-tune MLLMs on more task data in text format or enable MLLMs to activate the corresponding task heads. Shikra [8] applies MLLM to localization tasks, transforms object coordinates into dialogue formats, and learns them autoregressively. Meanwhile, TimeChat [72] formulates event timestamps as text for autoregressive prediction, thereby endowing MLLMs with temporal grounding. They do improve specific task performance significantly, while at the cost of multimodal performance more or less. This is counterintuitive as seminal research proves different visual tasks are correlated and training them together often yields mutual gains [32, 50, 75]. We conjecture that the presentation of different tasks influences 1 this training and that the conflict arises from the learning discrepancies between discrete textual tokens and visually dense predictions. Our experiments in Section 4 validate that decoupled representation design can effectively address this issue. To enhance the multimodal capabilities of MLLMs, we explore optimization methods to meet multiple visual task requirements in an end-to-end manner. We propose task preference optimization (TPO), incorporating visual task knowledge into the MLLMs by jointly maximizing the likelihoods of visual task estimations and multimodal dialogue. Inspired by direct preference optimization [67] (DPO) and related methods, we treat visual task annotations as human preferences in particular demands, as shown in Figure 2. DPO aids LLM (or MLLM) better in better aligning with human preferences through binary classification that directs the model to generate responses that people prefer. Similarly, TPO enhances MLLMs visual sensing capabilities through differentiable task optimizations that guide MLLMs to yield dense predictions closely resembling human perception. To achieve this, TPO concretizes MLLMspecific visual perceptions into corresponding task tokens, disentangled from MLLM representation. Then it fine-tunes these task tokens and then updates the MLLM accordingly. Specifically, TPO appends visual task heads to the partial output of the MLLM, using several learnable task tokens as inputs for these corresponding heads. During training, TPO enables the MLLM to first distinguish and activate the appropriate task tokens based on user instructions. Subsequently, TPO jointly trains the task tokens and their corresponding heads to enhance the MLLMs understanding of visual tasks. Finally, TPO trains the entire modelincluding task tokens and headson both multimodal and visual-specific task data, promoting the perception and reasoning capabilities of the MLLM. Additionally, we note that multi-task co-training yields greater improvements than single-task training. TPO demonstrates scalability across various MLLM approaches, encompassing wide range of visual task categories and data quantities. We validate the effectiveness of TPO within widely used MLLMs, such as LLaVA [51] and VideoChat [46], as detailed in Section 4.3. By fine-tuning these open-source MLLMs with TPO, we enhance their visual understanding capabilities and improve dialogue performance. Additionally, we explore several key spatiotemporal and spatial perception tasks, including spatial grounding, tracking, and temporal grounding. Our findings indicate that these tasks can mutually enhance each others performance, particularly contributing to the improvement of multimodal dialogue capabilities. Our contributions can be summarized as: We propose new training method for multimodal large language models, referred to as Task Preference OptiFigure 2. Comparison of Learning Method. solid line indicates data flow, and dotted line represents feedback. denote modules that are frozen and unfrozen. and mization (TPO). This method leverages supervised information from visual task-specific data to optimize the MLLM through the corresponding heads, resulting in significantly enhanced multimodal perception and reasoning performance. Specifically, TPO achieves an average improvement of 14.6% across multiple image and video multimodal benchmarks [22, 41, 48, 60, 117]. TPO effectively equips MLLM with the capability to address several key visual perception tasks through the introduced task heads. MLLM-TPO achieves comparable performance in spatial grounding, moment retrieval, highlight detection, referring segmentation, and tracking comparable to expert models across various benchmarks. TPO demonstrates scalability across various employed MLLMs, task heads, and scales of task data. We validate the effectiveness of TPO in multiple mainstream MLLMs, such as VideoChat2 [48] and LLaVA [42, 51]. Notably, multi-task joint training based on TPO enhances both multimodal performance and individual visual task, with improvements becoming increasingly significant as additional appropriate heads are introduced. Furthermore, the performance of the MLLM and task heads improves when scaling task data. 2. Related Work Vision Foundation Model. Vision foundation models [47, 75, 77, 80, 81, 83, 8789, 96, 99, 116] are designed to be adaptable for various downstream tasks through extensive pre-training on large-scale, diverse datasets. VideoPrism [116] achieves SOTA results on various video tasks by combining video-text contrastive learning and video token reconstruction using dataset of public and proprietary videos. InternVideo2 [89] utilize masked reconstruction, cross-modal contrastive learning, and next-token prediction to enhance the models perceptiveness, semantic understanding, and reasoning capabilities. Based on vision foundation models, some studies [44, 55, 86, 119, 120] intend to incorporate downstream task heads into this framework and expect end-to-end task train2 Figure 3. Overall Pipeline of TPO. The architecture of Task Preference Optimization (TPO) consists of four main components: (1) vision encoder, (2) connector, (3) large language model, and (4) series of visual task heads. Differently colored flame symbols indicate which components are unfrozen at various stages of the training process. ing. Unified-IO [55] builds model that can support diverse set of tasks across vision and language with little to no need for task-specific customizations and parameters. Uni-Perceivers [44, 119, 120] formulate different tasks to find the maximum likelihood target for each input through the representation similarity regardless of their modality. Nevertheless, these generalist models are limited to predefined tasks and cannot support flexible, open-ended task customization based on language instructions. Q-former in the MLLM to enhance the perception of temporal information. For P2E methods [4, 38, 84, 94, 109], MLLM compresses visual information and inputs it into the downstream decoder, which outputs the prediction results. LISA [38], NExT-Chat [109] and VideoLISA [4] introduce SAM [36] as segmentation tool based on MLLM, using special token as prompt to connect MLLM and SAM. VisionLLM v2 [94] designs multiple routing tokens and superlink queries to bridge MLLM with multiple decoders. Multimodal Large Language Model. The effective understanding and reasoning of LMMs have attracted the attention of many researchers. Limited to its input modality, researchers expand the visual capabilities of LLM, leading to MLLMs. Seminal works, such as BLIP-2 [45], LLaVA [51], and mPLUG-Owl [101], introduce image captioning and visual question answering based on LLM by visual instruction-tuning data. Some video-based MLLMs have been proposed, such as VideoChat [46], VideoChatGPT [57], and Video-LLaMA [112], which enable LLM to gain video understanding capabilities by encoding multiple video frames and using video instruction data. Vanilla MLLMs have achieved impressive results in visual captioning and question answering, but barely address fine-grained visual tasks, such as segmentation and temporal grounding, with precise estimations. To address this challenge, MLLMs usually take one of the three pipelines: pixel-to-sequence (P2S), and pixel-to-embedding (P2E). For P2S methods [8, 72, 79, 90, 102, 108], MLLM directly outputs the prediction results in text form. TimeChat [72] introduces time-aware frame encoder and sliding video Alignment in MLLM. Aligning MLLM with human preferences or values is crucial for MLLMs development. Recent works [76, 106, 118] introduce alignment approaches to MLLM, including proximal policy optimization (PPO) [73] and direct preference optimization (DPO) [67], as shown in Figure 2. They usually exploit proprietary models like GPT4-V to build visual preference datasets and then tune MLLMs using PPO or DPO, accordingly. LlavaRLHF [76] incorporates PPO into MLLM framework Llava, argumented by image captions or question-answers (QA), while Zhou et al. [118] and Zhang et al. [114] give DPO implementations for MLLMs where visual preference data are created by GPT-4V and other open-sourced MLLMs. RLHF-V [106] collects dense human preference in segmentation and enhances MLLMs with DPO. 3. Method Task Preference Optimization (TPO) aims to enable MLLM master classical visual perceptions (such as tracking, temporal grounding, etc) for better task generalization. Many multimodal reasoning tasks require precise vi3 sual cues for accurate and reliable responses. As given in Figure 3, MLLM-TPO has typical multimodal model (consisting of vision encoder E, vision-language connector C, and large language model G) and task preference model with series of visual task heads {Hi}i=1...n (n denotes the task head number). These heads connect with MLLM using the embeddings {ei}i=1...n (ei = G(vi) R1C) transformed from the learnable task-specific tokens {vi}i=1...n (vi R1C) via MLLM. TPO employs local-to-global training scheme, first adapting task heads to the MLLM and then training them Specifically, MLLM starts to recognize jointly. {vi}i=1...n by updating according to user instruction, then we tune {vi}i=1...n and {Hi}i=1...n for adapting visual heads to . Finally, we train both and {Hi}i=1...n together. During inference, MLLM-TPO can respond to users queries with visuals with text, and produce structured visual outputs (like masks, timestamps, trajectories, etc) when users ask (e.g. yielding time results for find when the birthday party starts). We detail how to form MLLM-TPO structurally and train it in the following. 3.1. Task Preference Models The task preference model (TPM) contains series of task tokens {vi}i=1...n and heads {Hi}i=1...n. Before TPM works, the attached MLLM generates special tokens indicating task types from the input queries. Then TPM dynamically calls the task token vj based on the special token, transforms it to the task embedding ej via the LLM (the last hidden embedding), and feeds ej to the corresponding visual task head {Hj} for specific task predictions. Considering existing MLLMs demonstrate remarkable capabilities in common object and scene recognition, yet struggle to accurately locate things or actions, the employed task heads mainly focus on spatiotemporal localization and tracking. Specifically, we give three fundamental task types for compensating mainstream MLLMs gaps from expert models in visual perceptions: 1) region head, 2) temporal head, and 3) mask head. Their architectures are given blew. Region Head. two-layer multilayer perceptron (MLP) with ReLU activation is employed for the region head. It takes embeddings from LLM and regresses them to the bounding box coordinates for the spatial grounding task. Temporal Head. This head is composed of video encoder, text encoder, and temporal grounding model for moment retrieval and highlight detection. The original video and text query are input to the video and text encoder for their features respectively. Then we append the temporal task embedding after the text features. The new text features and video features output the start and end time of the corresponding moment of the query and the highlight score through the temporal grounding model. Mask Head. Pixel-level tasks pose significant challenges for MLLMs due to the lack of pixel-level output capabilities in MLLMs. To this end, we introduce specialized mask head, utilizing the image encoder, mask decoder, and memory bank components of SAM2 [69], replacing the prompt encoder with single MLP layer called the mask adapter. Specifically, the image features extracted by the image encoder and the text features corresponding to the mask task embedding from LLM are fed into the mask decoder to produce the final mask. Most known discriminative vision tasks can be addressed by one or combination of the aforementioned three task heads. TPM builds the architectural foundations for leveraging existing vision annotations to enhance MLLMs. 3.2. Task Preference Optimization TPO improves MLLMs with extra supervision from visual task heads by back-propagating gradients from heads to update MLLMs using visual task data. It enables the language model in MLLM to discriminate specific task types when users demand (task assignment). Then, TPO trains TPM via compact task representations ei (task optimization) from vi. Lastly, we train and together, tuning for the refined spatiotemporal perception according to task preferences from . Its optimization objective is given as: = Lmllm+Lassign(G(Tq), s) (cid:125) (cid:123)(cid:122) Task Assignment (cid:124) + (cid:88) i=1 Ltask(Ai, Hi(G(vi))) (cid:123)(cid:122) (cid:125) (cid:124) Task Optimization , (1) where Tq and denote user query (some contain specific task needs like tracking) in the form of text token sequence and the grounding truth task indicating token, respectively. Ai is the task annotation of the input visual X, so it could be number tuple for describing regional localization and area or mask for delineating object shape and position. Here (X, Tq, Ta, Ai) stands for an input data tuple to MLLM-TPO. Usually, we have (X, Tq, Ta) for typical MLLM training while (X, Tq, Ai) for task token and head training. Lassign is the cross-entropy loss, and Ltaski varies according to the task and are usually regressionor classification-related losses. To train MLLM-TPO, we propose 3-stage local-toglobal training scheme. Stage 1 learns to identify the task type based on user queries. In stage 2, we train task heads along with their corresponding task tokens, respectively. Lastly, we co-train task heads with MLLM by both task data and multimodal conversation data. Our 3-stage training strategy mitigates the risk of degrading the MLLMs general abilities. We describe them as: Stage 1: Task Assignment. We design variety of dialogue templates for different visual tasks to perform task recognition instruction tuning for LLM. The LLM is trained using LoRA [28] in this stage."
        },
        {
            "title": "Segmentation",
            "content": "114.6K MeViS [18], SAMv2 [69]"
        },
        {
            "title": "Spatial\nGrounding",
            "content": "116.5K 540.0K ActivityNet [7], TACoS [70], QVHighlight [39], DiDeMo [27], QuerYD [63], HiREST [107], NLQ [25] Allseeing-V2 [85], Visual Genome [37], RefCOCO [103], RefCOCO+ [103], RefCOCOg [59]"
        },
        {
            "title": "Conversation",
            "content": "3M YouCook2 [17], ActivityNet [7], VideoChat2-IT [48], ShareGPT-4o [14], LLaVA-Hound-DPO [113], ShareGPT4V [10] Table 1. Overview of Datasets Used in TPO for Various Tasks. Stage 2: Vision Task Training. We integrate task tokens and task-specific heads into the model. By training on taskspecific data. The model equips the capacity with finegrained perception and aligns between the task head and the MLLM. The region head, temporal head, mask adapter, and corresponding task tokens are trained respectively. Similarly, the LLM is updated with LoRA. Stage 3: Multi-task Training. We tune the entire model on the mixed corpus combining multimodal conversations and specific task data. The synergistic training allows gradients from the heads to MLLM, supervised by human annotations in vision tasks. The vision encoder, connector, task tokens, region head, temporal head, mask adapter, and LLM (with LoRA) are joint-trained in this stage. 4. Experiment We give the implementation and training & testing specifics of our TPO, and then show its results and ablations. Implementation Details. We employ VideoChat2 [48], video-based MLLM, as the primary framework in experiments. For its configuration, we employ UMT-L [47] as the vision encoder, Q-former in BERTbase [35] as the connector, and Mistral-7B[31] as the language model (LLM). Regarding the three task heads, the Region Head is initialized randomly. The Temporal Head utilizes CGDETR [61], with parameters also initialized randomly. The video features input to the temporal head are extracted from the pre-trained InternVideo2 [89], while query features are extracted using the LLM [15]. The Mask Head employs SAM2 [69] and is initialized with its pre-trained weights. Since SAM2 is originally designed solely for tracking, we incorporate two-layer MLP to encode positioning prompts for spatiotemporal grounding. Additionally, to enable the MLLM to handle the spatial locations provided by the user, we utilize another two-layer MLP to encode the spatial input into the MLLM. Our model is trained on variety of visual task datasets and conversation datasets, as shown in Table 1. More training details are provided in the Appendix. Benchmarks. We evaluate our given TPO on both general image/video understanding benchmarks (mainly in multiple choice form) and specific visual tasks (e.g. grounding, tracking, and so on). Specifically, we run our model along with other approaches on MVBench, VideoMME, NExTGQA, MLVU, MMIU, and SEED-Bench2. MVBench [48] is designed to evaluate multi-modal fine-grained video understanding tasks for clips (lasting around 8 to 16 seconds), consisting of 20 video tasks relying heavily on temporal perception that cannot be addressed by single-frame analysis. Video-MME [22] is for evaluating MLLMs in both perception and reasoning across varying lengths of videos, annotated by humans. NExT-GQA [95] builds on NExTQA by introducing timestamps that are crucial for understanding questions and determining answers. This requires MLLM to perform multi-step reasoning and emphasize deeper understanding of both visual and textual content. MLVU [117] is constructed from wide variety of long videos, with lengths ranging from 3 minutes to 2 hours, and includes nine distinct evaluation tasks. MMIU [60] is comprehensive evaluation suite designed to assess LVLMs across wide range of multi-image tasks. It encompasses 7 types of multi-image relationships, 52 tasks, 77K images, and 11K meticulously curated multiple-choice questions. SEED2-Bench2 [41] is comprehensive benchmark for evaluating MLLMs, featuring 24K multiple-choice questions with human annotations. It spans 27 evaluation dimensions, assessing both text and image generation. Concerning visual tasks, we test spatial grounding, moment retrieval, highlight detection, tracking, and referring segmentation, including 7 benchmarks and the results from several corresponding state-of-the-art expert models. 4.1. General Understanding Evaluation Multimodal Video Understanding. TPO improves its baseline (VideoChat2) on several video understanding benchmarks with notable margin. As shown in Table 2, VideoChat-TPO, using 7B LLM and only 16 input frames, achieves 66.8 average score on MVBench [48], increasing by 6.4% over VideoChat2 and exceeding the performance of ST-LLM [52] which uses 64 frames. Considering MVBench focuses on characterizing subtle temporal variations involving fine-grained action types, action order, moving direction, and so on, TPOs seamless integration of detailed video understanding (like spatiotemporal grounding and tracking) into its optimized MLLM makes it effectively handle these video reasoning tasks. On VideoMME [22], VideoChat-TPO outperforms comparable models, achieving 9.3% improvement over VideoChat2 and demonstrating significant gains across short and medium-length videos, regardless of subtitle availability. On MLVU [117], VideoChat-TPO achieves M-AVG score of 54.7, surpassing VideoChat2 by 10.2%."
        },
        {
            "title": "Frames",
            "content": "MVBench [48] VideoMME [22] MLVU [117]"
        },
        {
            "title": "Long",
            "content": "TimeChat [72] Video-LLAVA [49] ShareGPT4Video [11] LLaVA-Next-Video [115] ST-LLM [52] PLLaVA-34B [97] Chat-UniVi [33] VideoChat2 (baseline) [48] VideoChat-TPO 7B 7B 7B 7B 7B 34B 7B 7B 7B 96 8 16 16 64 16 64"
        },
        {
            "title": "AVG",
            "content": "38.5 43.0 51.2 44.0 54.9 58.1 40.8 60.4 w/o s. 34.3 41.1 39.9 38.0 37.9 40.0 40.6 39.5 w/ s. 36.9 41.9 43.6 40.8 42.3 35.0 45.9 43.8 66.8 (+6.4) 48.8 (+9.3) 53.8 (+10.0) w/o s. w/ s. w/o s. w/ s. w/o s. w/ s. M-AVG 39.1 46.9 48.3 44.6 45.7 47.2 45.7 48.3 58. 43.1 47.3 53.6 47.4 48.4 36.2 51.2 52.8 64.9 31.8 38.7 36.3 37.7 36.8 38.2 41.3 37.0 46.7 33.9 40.4 39.3 39.4 41.4 35.9 47.3 39.4 50. 32.1 37.8 35.0 31.9 31.3 34.7 39.1 33.2 41.0 33.6 37.9 37.9 35.6 36.9 32.9 43.4 39.2 46.4 30.9 47.3 46.4 39.3 - 53.6 - 44.5 54.7 (+10.2) Table 2. Performance on Multimodal Video Understanding. We compare our model to others using LLMs of the same generation or 16-frame input. w/o s. indicates without subtitle, while s. indicates with subtitle. M-AVG refers to the mean average of MLVU. This confirms the effectiveness of TPOs enhanced perceptions of long-form video evaluations. These results across three benchmarks demonstrate the significant advancements in multimodal video understanding achieved through TPO. Grounded Video QA. Table 3 shows that VideoChatTPO outperforms other models, achieving superior accuracy (Acc) and intersection over union (IoU) scores in NExT-GQA [95]. Its intersection over prediction (IoP) scores are comparable to those of LLoVi [110], which employs large, closed-source commercial models like GPT4 [1]. The high Acc@IoP score not only reflects the enhanced capability of TPO-optimized VideoChat in effectively understanding and interpreting video content but also demonstrates its robustness in handling complex reasoning scenarios. Furthermore, the higher Acc@GQA score indicates that the model successfully integrates fine-grained temporal grounding with complex reasoning tasks, enabling it to accurately provide temporal clues necessary for inferring answers. The TPO training framework significantly enhances the reasoning capabilities of the baseline model, allowing it to overcome the limitations of traditional QAtrained models in identifying and localizing temporal elements within video data. This enhancement positions MLLM-TPO with stronger competitive edge in the field of video reasoning. Multimodal Multi-image Understanding. To explore TPOs effect on multi-image understanding, we test it on the MMIU and SEED-Bench2, as shown in Table 14. On MMIU, VideoChat-TPO achieved an overall score of 40.2, 5.2% improvement over VideoChat2. Besides temporal changes, MMIU evaluates models spatial sensing and semantic relations in scenes. On SEED-Bench2, across 27 performance indicators, VideoChat-TPO achieves 41.7% improvement on average performance compared to videochat2. VideoChat-TPOs superior performance on multi-image understanding compared to LLaVA-Interleave and InternVL1.5-Chat demonstrates that TPOs vision enhancements improve the MLLMs spatial perception and image understanding. 4.2. Vision Task Evaluation Moment Retrieval. Moment Retrieval is to locate the target segments in video based on language query. Table 5 and Table 6 compare the zero-shot and fine-tuned moment retrieval performance of VideoChat-TPO against other expert models and MLLMs. In zero-shot settings on the Charades-STA dataset [24], VideoChat-TPO achieves R@1 (IoU=0.5) score of 40.2, surpassing the previous stateof-the-art (SOTA) end-to-end trained MLLM, ChatVTG [66], and the expert model, UniVTG [50]. This demonstrates VideoChat-TPOs ability to accurately locate video moments corresponding to given text queries, thereby enhancing the practical applicability of MLLMs. detection Highlight Detection. Highlight generates salient scores for emphasizing frames based on the input language query. We compare the fine-tuning highlight detection performance of VideoChat-TPO with that of other expert models and MLLMs, as shown in Table 6. VideoChat-TPO surpasses notably the previous state-of-the-art MLLM (such as TimeChat [72]) on both Charades-STA and QVHighlight. For expert models like QD-DETR and UniVTG, VideoChat-TPO beats them on Charades-STA non-trivially and achieves comparable performance on QVHighlight. This also demonstrates the progress in extending MLLMs to broad temporal tasks. Spatial Grounding. To verify the fine-grained localization ability of the model, we run the spatial grounding task which inputs textual descriptions into the model and gets the corresponding bounding box on RefCOCO [103]. As shown in Table 7, we compare VideoChat-TPO with pixel-to-sequence models, i.e., VisionLLM-H [84], pixelto-embedding methods, i.e., NExT-Chat [109] and expert models, i.e., G-DINO [53]. We outperform the pixel-toembedding methods by using only simple task head and"
        },
        {
            "title": "Model",
            "content": "Acc@IoP Acc@GQA mIoP IoP@0.3 IoP@0.5 mIoU IoU@0.3 IoU@0."
        },
        {
            "title": "Model",
            "content": "MMIU [60] SEED2I [41] SEED2M [41] VIOLETv2 [23] SeViLA [105] LangRepo [34] FrozenBiLM NG+ [100] VideoStreaming [65] LLoVi [110] HawkEye [90] VideoChat-TPO 54.9 72.5 59.6 73.8 57.4 65.9 - 77.7 12.8 16.6 17.1 17.5 17.8 24.3 - 25.5 23.6 29.5 31.3 24.2 32.2 37.3 - 35.6 25.1 34.7 - 28.5 - - - 47.5 23.3 22.9 28.7 23.7 31.0 36.9 - 32.8 3.1 21.7 18.5 9.6 19.3 20.0 25.7 27.7 4.3 29.2 - 13.5 - - 37. 41.2 1.3 13.8 12.2 6.1 13.3 15.3 19.5 23.4 LLaVA-v1.5 [51] ShareGPT4V [10] OpenFlamingo [2] LLaVA-Interleave [43] VideoChat2 [48] VideoChatGPT [57] InternLM-XComposer [19] 19.2 18.5 22.3 32.4 35.0 - 21.9 58.3 - 36.6 - 26.5 38.3 65. 39.2 - 43.5 - 27.6 49.8 49.8 VideoChat-TPO 40.2 (+5.2) 67.3 (+40.8) 70.0 (+42.4) Table 3. Performance on Grounded QA. Table 4. Performance on Image Understanding."
        },
        {
            "title": "Model",
            "content": "UniVTG [50] VideoChat2 [48] VTimeLLM [29] TimeChat [72] HawkEye [90] ChatVTG [66] VideoChat-TPO Charades-STA [24] R@0.3 R@0.5 R@0.7 mIoU"
        },
        {
            "title": "Model",
            "content": "Charades-STA [24] QVHighlight [39]"
        },
        {
            "title": "Methods",
            "content": "R@0.3 R@0.5 R@0.7 mIoU mAP HIT@1 RefCOCO [103] val testA testB 44. 38.0 51.0 - 50.6 52.7 58.3 25.2 14.3 27.5 32.2 31.4 33.0 40.2 10. 3.8 11.4 13.4 14.5 15.9 18.4 27.1 24.6 31.2 - 33.7 34.9 38.1 M-DETR [39] QD-DETR [62] UniVTG [50] TimeChat [72] HawkEye [90] VideoChat-TPO 65.8 - 72.6 - 72.5 77.0 52.1 57.3 60. 46.7 58.3 65.0 30.6 32.6 38.6 23.7 28.8 40.7 45.5 - 52. - - 55.0 35.7 38.9 40.5 21.7 - 38.8 55.6 62.4 66. 37.9 - 66.2 MAttNet [104] OFA-L [82] G-DINO-L [53] VisionLLM-H [84] Shikra-7B [8] NExT-Chat-7B [109] VideoChat-TPO 76.4 80.0 90. - 87.0 85.5 85.9 80.4 83.7 93.2 86.7 90.6 90.0 90.8 69.3 76.4 88. - 80.2 77.9 81.3 Table 5. Zero-Shot Performance on Moment Retrieval.Gray means no LLM. Table 6. Fine-tuning Performance on Moment Retrieval and Highlight Detection. Gray means no LLM. Spatial Grounding Table 7. Task. with refined decoder. achieve comparable performance to pixel-to-sequence models fine-tuned on large amount of spatial grounding data, as well as to specialized models. 4.3. Ablation Studies In this section, we analyze the effectiveness of the key components of TPO and evaluate its scalability. Tracking. In the tracking task, the model receives the object coordinates from the first frame and outputs the coordinates for the remaining frames in the video. We evaluate the VideoChat-TPO on the mainstream tracking benchmarks LAOST [21] and GOT-10k [30], as shown in Table 8. In zero-shot testing, VideoChat-TPO ranks among the best of all MLLMs, even surpassing some fine-tuned expert models remarkably with around 10% increase on success rate, such as SiamFC[5] and ATOM [16]. With the TPO training method, the model is optimized from mask sequences that hard to be represented by language, allowing the MLLM to achieve strong motion characterization for multimodal generalization in highly dynamic and occluded scenes. Extending to Other MLLMs. To evaluate the effectiveness of the TPO method on different MLLMs, we apply TPO to the LLaVA (LLaVA-OneVision [42]) in addition to VideoChat. In LLaVA, short-term fine-grained video understanding is weakness. Despite the conversation data used in LLaVA-OneVision being very similar to, or even more extensive than, what we employ, there remains significant room for improvement in its performance on short-term fine-grained understanding tasks. As shown in Table 10, after optimization through TPO, LLaVA not only gains capabilities that it originally lacked but also achieves an 8.1% improvement on MVBench, demonstrating the universality of the TPO method. Referring Segmentation. The referring segmentation task requires the model to output the mask sequence corresponding to the specified prompt. This capability is not designed or learned in the original VideoChat2 or SAM2, but it can be activated in VideoChat-TPO. We compare the referring segmentation performance of VideoChat-TPO with other expert models and MLLMs in Tabel 9. The zero shot capability of VideoChat-TPO is comparable to the finetuning capabilities of other expert models, i.e. ReferFormer [93], showing its notable open-world video segmentation performance. Through TPO training, MLLM effectively optimizes its understanding in both tracking and segmenting objects indicated by users. It further enables the model to excel in pixel-level tasks, offering perceptual advantages to it in practical applications like robotic control. Task Preference Model vs. Textualized Task Data. We compare the TPO method with the approach of textualizing task data and training it in an autoregressive manner, which is straightforward and efficient method for MLLMs to learn specific tasks. We use the same data as the TPO method and convert the data related to the task head into conversation format. Due to the limitations of textual representation, much data must be input in more discrete manner. For instance, in tracking supervision data, we follow Merlin [102] by converting masks into sequence of spatial coordinates to serve as both the models input and output. As shown in Table 11, MLLM-TPO achieves 2.1% performance improvement on MVBench [48] compared to MLLMs trained with textualized task data. This demonstrates that the task head in the TPO method en7 LaSOT [21] GOT-10k [30]"
        },
        {
            "title": "Pnorm",
            "content": "P"
        },
        {
            "title": "Overlap",
            "content": "SR0.5 SR0."
        },
        {
            "title": "Method",
            "content": "Ref-YouTube-VOS [74] &F MeViS [18] &F F"
        },
        {
            "title": "Model",
            "content": "SiamFC [5] ATOM [16] SiamRPN++ [40] SiamFC++ [98] LLaVA-1.5 [51] Merlin [102] VideoChat-TPO 33.6 51.5 49.6 54.4 19.4 39.8 69. 42.0 - 56.9 62.3 16.5 40.2 80.1 33.9 - 49.1 54.7 12.8 38.1 76. 34.8 55.6 51.8 59.5 23.5 51.4 70.6 35.3 63.4 61.8 69.5 20.2 55.9 79. 9.8 40.2 32.5 47.9 9.7 42.8 66.0 Table 8. Performance on Tracking Benchmarks. ReferFormer [93] OnlineRefer [92] LISA [38] VideoLISA [4] VideoChat-TPO 62.9 62.9 52.6 63.7 63.9 61.3 61.0 52.1 61. 52.3 64.6 64.7 53.0 63.7 75.4 31.0 - - 44. 29.8 - - 41.3 32.2 - - 47.6 47.0 42. 51.3 Table 9. Performance on Referring Segmentation. TPO is evaluated in zero-shot on Ref-YouTube."
        },
        {
            "title": "MVBench",
            "content": "LLaVA-OV [42] LLaVA-OV-TPO 64.8 (+8.1) 56.7 Table 10. Applying TPO on LLaVA-OV. TPO significantly enhances the ability to understand fine-grained video details."
        },
        {
            "title": "Model",
            "content": "Charades-STA [24]"
        },
        {
            "title": "MVBench",
            "content": "R@0.3 R@0.5 R@0.7 mIoU"
        },
        {
            "title": "AVG",
            "content": "VideoChat-TPO w/o reasoning data replace by simple head textualized task data 58.3 56.4 31.5 33.7 40.2 38.3 17.8 18.6 18.4 17.1 6.1 6.2 38.1 35.6 15.4 16. 66.8 66.1 65.8 64.7 Table 11. Ablation of Reasoning Data and Head Performance. R@0.5 Acc@0.5 &F Exp. 1 2 3 4 5 30.2 - - - 36.7 40. - 77.3 - 80.2 81.6 82.0 - - 55.1 58.3 61.4 63.9 Table 12. Impact of TPO Components and Data. T, R, M, and denote temporal head, region head, mask head, and conversation data respectively. R1@0.5 means R1@0.5 in Charades-STA, Acc@0.5 represents the mean of Acc@0.5 in all COCO datasets, &F means &F in Ref-YouTube-VOS. ables more effective utilization of the original supervision signals compared to the next-token prediction approach, which inevitably incurs information loss when converting fine-grained tasks into textual annotations. The next-token prediction method cannot truly capture the nuanced information present in videos, while TPO allows the MLLM to learn perceptual information beyond conversational data. Impact of Task Head Performance on TPO. stronger task head is expected to enhance the multimodal capabilities of TPO more effectively than weaker one. To further explore the impact of the task head on model performance, we replaced the temporal head from CG-DETR [61] with simple two-layer MLP. As shown in Table 11, the simple temporal head exhibits significant performance decline compared to CG-DETR on the corresponding tasks. However, it still achieves 1.1% improvement compared to the model without TPO. In contrast to the simpler head, the welldesigned head can provide more accurate expert knowledge through its architecture and pre-trained task preference weights, enabling better utilization of the data. Impact of Data Scaling. Enhancing model performance can be significantly achieved by incorporating additional In our approach, we expand the existing conversadata. 8 tion data by integrating two reasoning datasets [7, 17] which offer valuable temporal information. With the addition of these datasets, we observe significant enhancement in the capabilities of the temporal head in Table 11. Furthermore, as shown in Table 12, incorporating conversation data further improves performance across various tasks. Overall, increasing the amount of conversation and fine-grained task data leads to noticeable improvements in model performance, demonstrating the effectiveness of data scaling in training multimodal models. Synergistic Gains from Co-training on Visual Tasks. Table 12 presents the impact of incorporating different visual tasks into TPO on performance. This indicates that the collaborative learning of visual tasks facilitates the transfer of knowledge, ultimately leading to better performance across the board. Overall, these results confirm that cotraining not only benefits individual tasks but also creates synergistic effect that enhances overall capabilities in visual understanding. 5. Conclusions This paper introduces Task Preference Optimization (TPO), which significantly enhances the overall multimodal performance of MLLMs by enhancing their precise understanding. TPO achieves this by integrating differentiable task preferences derived from fine-grained visual tasks. It introduces learnable task tokens that serve as bridges between multiple task-specific heads and the MLLM. Through the joint optimization of these task tokens, heads, and the MLLM, TPO leads to substantial improvement in multimodal dialogue capabilities and performance across various visual tasks. Our results demonstrate the effectiveness of TPO in scaling MLLMs with task-specific data and seamlessly integrating them with existing expert vision models. We believe this work clarifies the prerequisites for fusing MLLMs with models and data from the pre-large model era, effectively bridging the gap between expert and generalist models, as well as between generation and understanding. Limitations. Currently, TPO focuses exclusively on discriminative visual tasks, overlooking generative ones. Meanwhile, the framework is supervised by human annotations, which neglects potentially valuable unsupervised or self-supervised learning approaches such as contrastive learning [12, 26]. This limitation inherently restricts the scalability of TPO in terms of both task diversity and data requirements. While we demonstrate TPOs potential for enhancing MLLMs through increased task-specific data, comprehensive investigation into the broader impact of this scaling remains crucial area for future research."
        },
        {
            "title": "References",
            "content": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. 6 [2] Anas Awadalla, Irena Gao, Josh Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe, Yonatan Bitton, Samir Gadre, Shiori Sagawa, Jenia Jitsev, Simon Kornblith, Pang Wei Koh, Gabriel Ilharco, Mitchell Wortsman, and Ludwig Schmidt. Openflamingo: An opensource framework for training large autoregressive visionlanguage models. arXiv preprint arXiv:2308.01390, 2023. 7, 14 [3] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei arXiv preprint Huang, et al. Qwen technical report. arXiv:2309.16609, 2023. 14 [4] Zechen Bai, Tong He, Haiyang Mei, Pichao Wang, Ziteng Gao, Joya Chen, Lei Liu, Zheng Zhang, and Mike Zheng Shou. One token to seg them all: Language instructed reasoning segmentation in videos. arXiv preprint arXiv:2409.19603, 2024. 3, 8 [5] Luca Bertinetto, Jack Valmadre, Joao Henriques, Andrea Vedaldi, and Philip HS Torr. Fully-convolutional siamese In ECCV, pages 850865, networks for object tracking. 2016. 7, 8 [6] Jake Bruce, Michael Dennis, Ashley Edwards, Jack Parker-Holder, Yuge Shi, Edward Hughes, Matthew Lai, Aditi Mavalankar, Richie Steigerwald, Chris Apps, et al. Genie: Generative interactive environments. In Forty-first International Conference on Machine Learning, 2024. 1 [7] Fabian Caba Heilbron, Victor Escorcia, Bernard Ghanem, and Juan Carlos Niebles. Activitynet: large-scale video In CVPR, benchmark for human activity understanding. pages 961970, 2015. 5, 8, 15, [8] Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang, Shikra: Unleashing multiarXiv preprint Feng Zhu, and Rui Zhao. modal llms referential dialogue magic. arXiv:2306.15195, 2023. 1, 3, 7 [9] Lin Chen, Jisong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua Lin. Sharegpt4v: Improving large multi-modal models with better captions. arXiv preprint arXiv:2311.12793, 2023. 15 [10] Lin Chen, Jisong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua Lin. Sharegpt4v: Improving large multi-modal models with better captions. In ECCV, 2024. 5, 7, 14, 16 [11] Lin Chen, Xilin Wei, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Bin Lin, Zhenyu Tang, Li Yuan, Yu Qiao, Dahua Lin, Feng Zhao, and Jiaqi Wang. Sharegpt4video: Improving video understanding and generation with better captions. arXiv preprint arXiv:2406.04325, 2024. 6, 14 [12] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. simple framework for contrastive learning of visual representations. In ICML, pages 15971607. PMLR, 2020. 9 [13] Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, Ji Ma, Jiaqi Wang, Xiaoyi Dong, Hang Yan, Hewei Guo, Conghui He, Botian Shi, Zhenjiang Jin, Chao Xu, Bin Wang, Xingjian Wei, Wei Li, Wenjian Zhang, Bo Zhang, Pinlong Cai, Licheng Wen, Xiangchao Yan, Min Dou, Lewei Lu, Xizhou Zhu, Tong Lu, Dahua Lin, Yu Qiao, Jifeng Dai, and Wenhai Wang. How far are we to gpt4v? closing the gap to commercial multimodal models with open-source suites. CoRR, abs/2404.16821, 2024. 1 [14] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. In CVPR, pages 2418524198, 2024. 5, 14, 15, 16 [15] Yiming Cui, Ziqing Yang, and Xin Yao. Efficient and effective text encoding for chinese llama and alpaca. arXiv preprint arXiv:2304.08177, 2023. 5, 15 [16] Martin Danelljan, Goutam Bhat, Fahad Shahbaz Khan, and Michael Felsberg. Atom: Accurate tracking by overlap maximization. In CVPR, pages 46604669, 2019. 7, 8 [17] Pradipto Das, Chenliang Xu, Richard Doell, and Jason Corso. thousand frames in just few words: Lingual description of videos through latent topics and sparse object stitching. In CVPR, pages 26342641, 2013. 5, 8, 15 [18] Henghui Ding, Chang Liu, Shuting He, Xudong Jiang, and Chen Change Loy. MeViS: large-scale benchmark for In ICCV, video segmentation with motion expressions. 2023. 5, 8, 15, 16 [19] Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao, Bin Wang, Linke Ouyang, Xilin Wei, Songyang Zhang, Haodong Duan, Maosong Cao, et al. Internlm-xcomposer2: Mastering free-form text-image composition and comprehension in vision-language large model. arXiv preprint arXiv:2401.16420, 2024. 7, [20] Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, et al. Palm-e: An embodied multimodal language model. arXiv preprint arXiv:2303.03378, 2023. 1 [21] Heng Fan, Liting Lin, Fan Yang, Peng Chu, Ge Deng, Sijia Yu, Hexin Bai, Yong Xu, Chunyuan Liao, and Haibin Ling. Lasot: high-quality benchmark for large-scale single object tracking. In CVPR, pages 53745383, 2019. 7, 8 [22] Chaoyou Fu, Yuhan Dai, Yondong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, et al. Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis. arXiv preprint arXiv:2405.21075, 2024. 2, 5, 6 [23] Tsu-Jui Fu, Linjie Li, Zhe Gan, Kevin Lin, William Yang Wang, Lijuan Wang, and Zicheng Liu. An empirical study of end-to-end video-language transformers with masked visual modeling. In CVPR, 2023. 7 [24] Jiyang Gao, Chen Sun, Zhenheng Yang, and Ram Nevatia. Tall: Temporal activity localization via language query. In ICCV, pages 52675275, 2017. 6, 7, 8, [25] Kristen Grauman, Andrew Westbury, Eugene Byrne, Zachary Chavis, Antonino Furnari, Rohit Girdhar, Jackson Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, et al. Ego4d: Around the world in 3,000 hours of egocentric video. In CVPR, pages 1899519012, 2022. 5, 15, 16 [26] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. In CVPR, pages 97299738, 2020. 9 [27] Lisa Anne Hendricks, Oliver Wang, Eli Shechtman, Josef Sivic, Trevor Darrell, and Bryan Russell. Localizing moments in video with natural language. In ICCV, 2017. 5, 15, 16 [28] Edward Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Lowrank adaptation of large language models. In ICLR, 2022. 4, 15 [29] Bin Huang, Xin Wang, Hong Chen, Zihan Song, and Wenwu Zhu. Vtimellm: Empower llm to grasp video moments. In CVPR, 2024. 7 [30] Lianghua Huang, Xin Zhao, and Kaiqi Huang. Got-10k: large high-diversity benchmark for generic object tracking in the wild. TPAMI, 43(5):15621577, 2019. 7, 8 [31] Albert Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023. [32] Fan Jiang and Zilei Wang. Sparse sharing relation network for panoptic driving perception. In ACMMM, pages 800 808, 2023. 1 [33] Peng Jin, Ryuichi Takanobu, Wancai Zhang, Xiaochun Cao, and Li Yuan. Chat-univi: Unified visual representation empowers large language models with image and video understanding. In CVPR, pages 1370013710, 2024. 6 [34] Kumara Kahatapitiya, Kanchana Ranasinghe, Jongwoo Park, and Michael Ryoo. Language repository for long arXiv preprint arXiv:2403.14622, video understanding. 2024. 7 [35] Jacob Devlin Ming-Wei Chang Kenton and Lee Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In naacL-HLT, page 2. Minneapolis, Minnesota, 2019. 5 [36] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, et al. Segment anything. In ICCV, pages 40154026, 2023. 3 [37] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David Shamma, et al. Visual genome: Connecting language and vision using crowdsourced dense image annotations. IJCV, 123:3273, 2017. 5, 15 [38] Xin Lai, Zhuotao Tian, Yukang Chen, Yanwei Li, Yuhui Yuan, Shu Liu, and Jiaya Jia. Lisa: Reasoning segmentation via large language model. In CVPR, pages 95799589, 2024. 3, 8 [39] Jie Lei, Tamara Berg, and Mohit Bansal. Detecting moments and highlights in videos via natural language queries. NeurIPS, 34:1184611858, 2021. 5, 7, 14, 15 [40] Bo Li, Wei Wu, Qiang Wang, Fangyi Zhang, Junliang Xing, and Junjie Yan. SiamRPN++: Evolution of siamese visual tracking with very deep networks. In CVPR, pages 4282 4291, 2019. 8 [41] Bohao Li, Yuying Ge, Yixiao Ge, Guangzhi Wang, Rui Wang, Ruimao Zhang, and Ying Shan. Seed-bench: BenchIn CVPR, marking multimodal large language models. pages 1329913308, 2024. 2, 5, 7 [42] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. 2, 7, [43] Feng Li, Renrui Zhang, Hao Zhang, Yuanhan Zhang, Bo Li, Wei Li, Zejun Ma, and Chunyuan Li. Llava-next-interleave: Tackling multi-image, video, and 3d in large multimodal models. arXiv preprint arXiv:2407.07895, 2024. 7, 14 [44] Hao Li, Jinguo Zhu, Xiaohu Jiang, Xizhou Zhu, Hongsheng Li, Chun Yuan, Xiaohua Wang, Yu Qiao, Xiaogang Wang, Wenhai Wang, et al. Uni-perceiver v2: generalist model for large-scale vision and vision-language tasks. In CVPR, pages 26912700, 2023. 2, 3 [45] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In ICML, pages 1973019742. PMLR, 2023. 3 [46] KunChang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai Wang, Ping Luo, Yali Wang, Limin Wang, and Yu Qiao. Videochat: Chat-centric video understanding. arXiv preprint arXiv:2305.06355, 2023. 2, 3, 14 [47] Kunchang Li, Yali Wang, Yizhuo Li, Yi Wang, Yinan He, Limin Wang, and Yu Qiao. Unmasked teacher: Towards training-efficient video foundation models. In ICCV, pages 1994819960, 2023. 2, 5 [48] Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, et al. Mvbench: comprehensive multi-modal video understanding benchmark. In CVPR, pages 2219522206, 2024. 2, 5, 6, 7, 14, 15, 16 [49] Bin Lin, Bin Zhu, Yang Ye, Munan Ning, Peng Jin, and Li Yuan. Video-llava: Learning united visual represenarXiv preprint tation by alignment before projection. arXiv:2311.10122, 2023. 6, [50] Kevin Qinghong Lin, Pengchuan Zhang, Joya Chen, Shraman Pramanick, Difei Gao, Alex Jinpeng Wang, Rui Yan, and Mike Zheng Shou. Univtg: Towards unified videolanguage temporal grounding. In ICCV, pages 27942804, 2023. 1, 6, 7 10 [51] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In NeurIPS, 2024. 2, 3, 7, 8, 14 [52] Ruyang Liu, Chen Li, Haoran Tang, Yixiao Ge, Ying Shan, and Ge Li. St-llm: Large language models are effective temporal learners. In ECCV, 2024. 5, 6, 14 [53] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun Zhu, et al. Grounding dino: Marrying dino with grounded pre-training for open-set object detection. In ECCV, 2024. 6, 7 [54] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. 15 [55] Jiasen Lu, Christopher Clark, Rowan Zellers, Roozbeh Mottaghi, and Aniruddha Kembhavi. Unified-io: unified model for vision, language, and multi-modal tasks. In ICLR, 2022. 2, [56] Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Khan. Videogpt+: Integrating image and video encoders for enhanced video understanding. arXiv preprint arXiv:2406.09418, 2024. 14 [57] Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Shahbaz Khan. Video-chatgpt: Towards detailed video understanding via large vision and language models. In ACL, 2024. 3, 7, 14 [58] Kit-Kay Mak, Yi-Hang Wong, and Mallikarjuna Rao Pichika. Artificial intelligence in drug discovery and development. Drug discovery and evaluation: safety and pharmacokinetic assays, pages 14611498, 2024. 1 [59] Junhua Mao, Jonathan Huang, Alexander Toshev, Oana Camburu, Alan Yuille, and Kevin Murphy. Generation and comprehension of unambiguous object descriptions. In CVPR, pages 1120, 2016. 5, 15 [60] Fanqing Meng, Jin Wang, Chuanhao Li, Quanfeng Lu, Hao Tian, Jiaqi Liao, Xizhou Zhu, Jifeng Dai, Yu Qiao, Ping Luo, et al. Mmiu: Multimodal multi-image understanding for evaluating large vision-language models. arXiv preprint arXiv:2408.02718, 2024. 2, 5, 7, 14 [61] WonJun Moon, Sangeek Hyun, SuBeen Lee, and JaePil Heo. Correlation-guided query-dependency calibration in video representation learning for temporal grounding. arXiv preprint arXiv:2311.08835, 2023. 5, 8, [62] WonJun Moon, Sangeek Hyun, SangUk Park, Dongchan Park, and Jae-Pil Heo. Query-dependent video representation for moment retrieval and highlight detection. In CVPR, pages 2302323033, 2023. 7 [63] Andreea-Maria Oncescu, Joao Henriques, Yang Liu, Andrew Zisserman, and Samuel Albanie. Queryd: video In dataset with high-quality text and audio narrations. ICASSP, pages 22652269. IEEE, 2021. 5, 15, 16 [64] OpenAI. Gpt-4o. https://openai.com/index/ hello-gpt-4o/, 2024. 1 [65] Rui Qian, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Shuangrui Ding, Dahua Lin, and Jiaqi Wang. Streaming long video understanding with large language models. arXiv preprint arXiv:2405.16009, 2024. 7 [66] Mengxue Qu, Xiaodong Chen, Wu Liu, Alicia Li, and Yao Zhao. Chatvtg: Video temporal grounding via chat with In CVPR, pages video dialogue large language models. 18471856, 2024. 6, [67] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. NeurIPS, 36, 2024. 2, 3 [68] Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters. In SIGKDD, pages 35053506, 2020. 15 [69] Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman Radle, Chloe Rolland, Laura Gustafson, Eric Mintun, Junting Pan, Kalyan Vasudev Alwala, Nicolas Carion, ChaoYuan Wu, Ross Girshick, Piotr Dollar, and Christoph Feichtenhofer. Sam 2: Segment anything in images and videos. arXiv preprint arXiv:2408.00714, 2024. 4, 5, 15, 16 [70] Michaela Regneri, Marcus Rohrbach, Dominikus Wetzel, Stefan Thater, Bernt Schiele, and Manfred Pinkal. Grounding action descriptions in videos. Transactions of the Association for Computational Linguistics, 1:2536, 2013. 5, 15, 16 [71] Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy P. Lillicrap, Jean-Baptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, Ioannis Antonoglou, Rohan Anil, Sebastian Borgeaud, Andrew M. Dai, Katie Millican, Ethan Dyer, Mia Glaese, Thibault Sottiaux, Benjamin Lee, Fabio Viola, Malcolm Reynolds, Yuanzhong Xu, James Molloy, Jilin Chen, Michael Isard, Paul Barham, Tom Hennigan, Ross McIlroy, Melvin Johnson, Johan Schalkwyk, Eli Collins, Eliza Rutherford, Erica Moreira, Kareem Ayoub, Megha Goel, Clemens Meyer, Gregory Thornton, Zhen Yang, Henryk Michalewski, Zaheer Abbas, Nathan Schucher, Ankesh Anand, Richard Ives, James Keeling, Karel Lenc, Salem Haykal, Siamak Shakeri, Pranav Shyam, Aakanksha Chowdhery, Roman Ring, Stephen Spencer, Eren Sezener, and et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. CoRR, abs/2403.05530, 2024. 1 [72] Shuhuai Ren, Linli Yao, Shicheng Li, Xu Sun, and Lu Hou. Timechat: time-sensitive multimodal large language model for long video understanding. CVPR, abs/2312.02051, 2024. 1, 3, 6, 7, [73] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. 3 [74] Seonguk Seo, Joon-Young Lee, and Bohyung Han. Urvos: Unified referring video object segmentation network with large-scale benchmark. In ECCV, pages 208223. Springer, 2020. 8 [75] Jing Shao, Siyu Chen, Yangguang Li, Kun Wang, Zhenfei Yin, Yinan He, Jianing Teng, Qinghong Sun, Mengya Gao, Jihao Liu, et al. Intern: new learning paradigm towards 11 general vision. arXiv preprint arXiv:2111.08687, 2021. 1, 2 [76] Zhiqing Sun, Sheng Shen, Shengcao Cao, Haotian Liu, Chunyuan Li, Yikang Shen, Chuang Gan, Liang-Yan Gui, Yu-Xiong Wang, Yiming Yang, et al. Aligning large multimodal models with factually augmented rlhf. arXiv preprint arXiv:2309.14525, 2023. [77] Zhan Tong, Yibing Song, Jue Wang, and Limin Wang. Videomae: Masked autoencoders are data-efficient learners In NeurIPS, pages for self-supervised video pre-training. 1007810093, 2022. 2 [78] Hanchen Wang, Tianfan Fu, Yuanqi Du, Wenhao Gao, Kexin Huang, Ziming Liu, Payal Chandak, Shengchao Liu, Peter Van Katwyk, Andreea Deac, et al. Scientific discovery in the age of artificial intelligence. Nature, 620(7972): 4760, 2023. 1 [79] Haibo Wang, Zhiyang Xu, Yu Cheng, Shizhe Diao, Yufan Zhou, Yixin Cao, Qifan Wang, Weifeng Ge, and Lifu Huang. Grounded-videollm: Sharpening fine-grained temporal grounding in video large language models. arXiv preprint arXiv:2410.03290, 2024. 3 [80] Limin Wang, Yuanjun Xiong, Zhe Wang, Yu Qiao, Dahua Lin, Xiaoou Tang, and Luc Van Gool. Temporal segment networks: Towards good practices for deep action recognition. In ECCV, pages 2036. Springer, 2016. 2 [81] Limin Wang, Bingkun Huang, Zhiyu Zhao, Zhan Tong, Yinan He, Yi Wang, Yali Wang, and Yu Qiao. Videomae v2: Scaling video masked autoencoders with dual masking. In CVPR, 2023. 2 [82] Peng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai, Zhikang Li, Jianxin Ma, Chang Zhou, Jingren Zhou, and Hongxia Yang. Ofa: Unifying architectures, tasks, and modalities through simple sequence-to-sequence learning framework. In ICML, pages 2331823340. PMLR, 2022. 7 [83] Rui Wang, Dongdong Chen, Zuxuan Wu, Yinpeng Chen, Xiyang Dai, Mengchen Liu, Lu Yuan, and Yu-Gang Jiang. Masked video distillation: Rethinking masked feature modeling for self-supervised video representation learning. In CVPR, pages 63126322, 2023. [84] Wenhai Wang, Zhe Chen, Xiaokang Chen, Jiannan Wu, Xizhou Zhu, Gang Zeng, Ping Luo, Tong Lu, Jie Zhou, Yu Qiao, et al. Visionllm: Large language model is also an open-ended decoder for vision-centric tasks. In NeurIPS, 2024. 3, 6, 7 [85] Weiyun Wang, Yiming Ren, Haowen Luo, Tiantong Li, Chenxiang Yan, Zhe Chen, Wenhai Wang, Qingyun Li, Lewei Lu, Xizhou Zhu, et al. The all-seeing project v2: Towards general relation comprehension of the open world. arXiv preprint arXiv:2402.19474, 2024. 5, 15, 16 [86] Xinlong Wang, Xiaosong Zhang, Yue Cao, Wen Wang, Chunhua Shen, and Tiejun Huang. Seggpt: Towards segmenting everything in context. In ICCV, pages 11301140, 2023. 2 [87] Yi Wang, Kunchang Li, Yizhuo Li, Yinan He, Bingkun Huang, Zhiyu Zhao, Hongjie Zhang, Jilan Xu, Yi Liu, Zun Wang, et al. Internvideo: General video foundation models via generative and discriminative learning. arXiv preprint arXiv:2212.03191, 2022. 2 [88] Yi Wang, Yinan He, Yizhuo Li, Kunchang Li, Jiashuo Yu, Xin Ma, Xinhao Li, Guo Chen, Xinyuan Chen, Yaohui Wang, et al. Internvid: large-scale video-text dataset for multimodal understanding and generation. In ICLR, 2024. [89] Yi Wang, Kunchang Li, Xinhao Li, Jiashuo Yu, Yinan He, Guo Chen, Baoqi Pei, Rongkun Zheng, Jilan Xu, Zun Wang, et al. Internvideo2: Scaling video foundation models for multimodal video understanding. In ECCV, 2024. 2, 5, 15 [90] Yueqian Wang, Xiaojun Meng, Jianxin Liang, Yuxuan Wang, Qun Liu, and Dongyan Zhao. Hawkeye: Training video-text llms for grounding text in videos. arXiv preprint arXiv:2403.10228, 2024. 3, [91] Zun Wang, Jialu Li, Yicong Hong, Yi Wang, Qi Wu, Mohit Bansal, Stephen Gould, Hao Tan, and Yu Qiao. Scaling data generation in vision-and-language navigation. In ICCV, pages 1200912020, 2023. 1 [92] Dongming Wu, Tiancai Wang, Yuang Zhang, Xiangyu Zhang, and Jianbing Shen. Onlinerefer: simple online baseline for referring video object segmentation. In ICCV, pages 27612770, 2023. 8 [93] Jiannan Wu, Yi Jiang, Peize Sun, Zehuan Yuan, and Ping Luo. Language as queries for referring video object segmentation. In CVPR, pages 49744984, 2022. 7, 8 [94] Jiannan Wu, Muyan Zhong, Sen Xing, Zeqiang Lai, Zhaoyang Liu, Wenhai Wang, Zhe Chen, Xizhou Zhu, Lewei Lu, Tong Lu, et al. Visionllm v2: An end-to-end generalist multimodal large language model for hundreds of vision-language tasks. arXiv preprint arXiv:2406.08394, 2024. 3 [95] Junbin Xiao, Angela Yao, Yicong Li, and Tat-Seng Chua. Can trust your answer? visually grounded video question answering. In CVPR, 2024. 5, 6 [96] Hu Xu, Gargi Ghosh, Po-Yao Huang, Dmytro Okhonko, Armen Aghajanyan, Florian Metze, Luke Zettlemoyer, and Christoph Feichtenhofer. Videoclip: Contrastive pretraining for zero-shot video-text understanding. In EMNLP, pages 67876800, 2021. [97] Lin Xu, Yilin Zhao, Daquan Zhou, Zhijie Lin, See-Kiong Ng, and Jiashi Feng. Pllava : Parameter-free llava extension from images to videos for video dense captioning. CoRR, abs/2404.16994, 2024. 6, 14 [98] Yinda Xu et al. Siamfc++: Towards robust and accurate visual tracking with target estimation guidelines. In AAAI, pages 140148, 2020. 8 [99] Shen Yan, Tao Zhu, Zirui Wang, Yuan Cao, Mi Zhang, Soham Ghosh, Yonghui Wu, and Jiahui Yu. Video-text modeling with zero-shot transfer from contrastive captioners. ArXiv, abs/2212.04979, 2022. 2 [100] Antoine Yang, Antoine Miech, Josef Sivic, Ivan Laptev, and Cordelia Schmid. Zero-shot video question answering via frozen bidirectional language models. In NeurIPS, 2022. 7 [101] Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, Chaoya Jiang, Chenliang Li, Yuanhong Xu, Hehong Chen, Junfeng Tian, Qian Qi, Ji Zhang, and Fei Huang. mplug-owl: Modularization empowers large language models with multimodality, 2023. 3 12 [102] En Yu, Liang Zhao, Yana Wei, Jinrong Yang, Dongming Wu, Lingyu Kong, Haoran Wei, Tiancai Wang, Zheng Ge, Xiangyu Zhang, et al. Merlin: Empowering multimodal In ECCV, pages 425443. llms with foresight minds. Springer, 2025. 3, 7, [103] Licheng Yu, Patrick Poirson, Shan Yang, Alexander Berg, and Tamara Berg. Modeling context in referring expressions. In ECCV, pages 6985. Springer, 2016. 5, 6, 7, 15 [104] Licheng Yu, Zhe Lin, Xiaohui Shen, Jimei Yang, Xin Lu, Mohit Bansal, and Tamara Berg. Mattnet: Modular attention network for referring expression comprehension. In CVPR, pages 13071315, 2018. 7 [105] Shoubin Yu, Jaemin Cho, Prateek Yadav, and Mohit Bansal. Self-chained image-language model for video localization and question answering. In NeurIPS, 2023. 7 [106] Tianyu Yu, Yuan Yao, Haoye Zhang, Taiwen He, Yifeng Han, Ganqu Cui, Jinyi Hu, Zhiyuan Liu, Hai-Tao Zheng, Maosong Sun, et al. Rlhf-v: Towards trustworthy mllms via behavior alignment from fine-grained correctional human feedback. In CVPR, pages 1380713816, 2024. 3 [107] Abhay Zala, Jaemin Cho, Satwik Kottur, Xilun Chen, Barlas Oguz, Yashar Mehdad, and Mohit Bansal. Hierarchical video-moment retrieval and step-captioning. In CVPR, pages 2305623065, 2023. 5, 15, 16 [108] Xiangyu Zeng, Kunchang Li, Chenting Wang, Xinhao Li, Tianxiang Jiang, Ziang Yan, Songze Li, Yansong Shi, Zhengrong Yue, Yi Wang, et al. Timesuite: Improving mllms for long video understanding via grounded tuning. arXiv preprint arXiv:2410.19702, 2024. 3 [109] Ao Zhang, Liming Zhao, Chen-Wei Xie, Yun Zheng, Wei Ji, and Tat-Seng Chua. Next-chat: An lmm for chat, detection and segmentation. arXiv preprint arXiv:2311.04498, 2023. 3, 6, [110] Ce Zhang, Taixi Lu, Md Mohaiminul Islam, Ziyang Wang, Shoubin Yu, Mohit Bansal, and Gedas Bertasius. simple llm framework for long-range video question-answering. arXiv preprint arXiv:2312.17235, 2023. 6, 7 [111] Hang Zhang, Xin Li, and Lidong Bing. Video-llama: An instruction-tuned audio-visual language model for video understanding. arXiv preprint arXiv:2306.02858, 2023. 14 [112] Hang Zhang, Xin Li, and Lidong Bing. Video-llama: An instruction-tuned audio-visual language model for video understanding. In EMNLP, pages 543553, 2023. 3 [113] Ruohong Zhang, Liangke Gui, Zhiqing Sun, Yihao Feng, Keyang Xu, Yuanhan Zhang, Di Fu, Chunyuan Li, Alexander Hauptmann, Yonatan Bisk, et al. Direct preference optimization of video large multimodal models from language model reward. arXiv preprint arXiv:2404.01258, 2024. 5, 15, 16 [114] Yanzhe Zhang, Ruiyi Zhang, Jiuxiang Gu, Yufan Zhou, Nedim Lipka, Diyi Yang, and Tong Sun. Llavar: Enhanced visual instruction tuning for text-rich image understanding. arXiv preprint arXiv:2306.17107, 2023. 3 [115] Yuanhan Zhang, Bo Li, haotian Liu, Yong jae Lee, Liangke Gui, Di Fu, Jiashi Feng, Ziwei Liu, and Chunyuan Li. Llava-next: strong zero-shot video understanding model, 2024. 6 [116] Long Zhao, Nitesh Bharadwaj Gundavarapu, Liangzhe Yuan, Hao Zhou, Shen Yan, Jennifer Sun, Luke Friedman, Rui Qian, Tobias Weyand, Yue Zhao, et al. Videoprism: foundational visual encoder for video understanding. In ICML, 2024. 2 [117] Junjie Zhou, Yan Shu, Bo Zhao, Boya Wu, Shitao Xiao, Xi Yang, Yongping Xiong, Bo Zhang, Tiejun Huang, and Zheng Liu. Mlvu: comprehensive benchmark for multi-task long video understanding. arXiv preprint arXiv:2406.04264, 2024. 2, 5, [118] Yiyang Zhou, Chenhang Cui, Rafael Rafailov, Chelsea Finn, and Huaxiu Yao. Aligning modalities in vision large language models via preference fine-tuning. In ICLR Workshop on Reliable and Responsible Foundation Models, 2024. 3 [119] Jinguo Zhu, Xizhou Zhu, Wenhai Wang, Xiaohua Wang, Hongsheng Li, Xiaogang Wang, and Jifeng Dai. Uniperceiver-moe: Learning sparse generalist models with conditional moes. In NeurIPS, pages 26642678, 2022. 2, 3 [120] Xizhou Zhu, Jinguo Zhu, Hao Li, Xiaoshi Wu, Hongsheng Li, Xiaohua Wang, and Jifeng Dai. Uni-perceiver: Pretraining unified architecture for generic perception for zeroIn CVPR, pages 1680416815, shot and few-shot tasks. 2022. 2,"
        },
        {
            "title": "Appendix",
            "content": "1. Detailed Results MVbench. We present the detailed performance of MVBench in Table 13, VideoChat-TPO achieves an average score of 66.8, increasing by 6.4 points based on VideoChat2. It gets superior performance among MLLMs with the same number of input frames and LLMs of comparable model scale. In Action Localization, temporal labels in the VideoChat2-Textualized-Task are defined as text. While the model demonstrates strong capabilities in zeroshot temporal grounding, converting the task into QA problem does not improve performance. However, by optimizing with TPO, the model can benefit from original label supervision, resulting in corresponding performance enhancements. Also, Its superior performance is particularly evident in tasks that require moment-based perception and reasoning, including Action Sequence (AS), Action Localization (AL) and Action Prediction (AP), with scores of 84.0 (+7.5%), 55.0 (+10%), and 69.5 (+13.5%) respectively. This demonstrates the excellent potential of TPO in sophisticated video understanding tasks. MMIU. The results are shown in Table 14. VideoChatTPO shows significant improvement over VideoChat2, achieving an overall score of 40.2 (a 5.2-point increase). Compared with VideoChat2, Our model has achieved clear improvements in Causality Reasoning (CR), Visually"
        },
        {
            "title": "Model",
            "content": "Avg. AS AP VideoChatGPT [57] VideoLLaMA [111] VideoChat [46] TimeChat [72] Video-LLaVA [49] P-LLaVA-7B [97] ShareGPT4Video [11] ST-LLM [52] VideoGPT+ [56] VideoChat2 [48] VideoChat2-textualized-task VideoChat-TPO 32.7 34.1 35.5 38.5 43.0 46.6 51.2 54.9 58.7 60.4 64.8 66.8 23.5 27.5 33.5 40.5 46.0 58.0 49.5 66.0 69.0 75.5 26.0 25.5 26.5 36.0 42.5 49.0 39.5 53.5 60.0 58.0 AA 62.0 51.0 56.0 61.0 56.5 55.5 79.5 84.0 83.0 83.5 FA 22.5 29.0 33.5 32.5 39.0 41.0 40.0 44.0 48.5 50.5 76.5 56.0 88. 52.5 84.0 69.5 87.5 52.0 UA 26.5 39.0 40.5 53.0 53.5 61.0 54.5 58.5 66.5 60.5 77.0 77.0 OE 54.0 48.0 53.0 53.5 53.0 56.0 82.5 80.5 85.5 87.5 92. OI"
        },
        {
            "title": "OS MD",
            "content": "AL ST"
        },
        {
            "title": "AC MC MA",
            "content": "SC FP CO EN ER CI 28.0 40.5 40.5 41.5 48.0 61.0 54.5 73.5 75.5 74.5 40.0 38.0 30.0 29.0 41.0 36.0 32.5 38.5 36.0 45.0 74.0 41.0 23.0 22.5 25.5 19.5 29.0 23.5 50.5 42.5 44.0 47.5 50. 20.0 22.5 27.0 26.5 31.5 26.0 41.5 31.0 34.0 44.0 31.0 43.0 48.5 66.5 82.5 82.0 84.5 86.5 89.5 82.5 30.5 34.0 35.0 34.0 45.0 39.5 35.5 36.5 39.5 37.0 45.0 87.0 47. 25.5 22.5 20.5 20.0 26.0 42.0 62.5 56.5 71.0 64.5 74.0 39.5 32.5 42.5 43.5 53.0 52.0 75.0 78.5 90.5 87.5 48.5 45.5 46.0 42.0 41.5 45.0 51.0 43.0 45.0 51.0 29.0 32.5 26.5 36.5 33.5 42.0 25.5 44.5 53.0 66.5 33.0 40.0 41.0 36.0 41.5 53.5 46.5 46.5 50.0 47. 29.5 30.0 23.5 29.0 27.5 30.5 28.5 34.5 29.5 35.0 26.0 21.0 23.5 35.0 38.5 48.0 39.0 41.5 44.0 37.0 89.0 48.0 85.0 45. 34.0 58.5 35.5 37.0 36.0 35.0 31.5 31.0 51.5 58.5 60.0 72.5 73.0 92.0 81. 40.5 42.5 55.0 89.0 47.5 68. 89.0 58.0 87.0 57.5 27.0 60. 72.0 Table 13. Results on MVBench Multi-choice Question Answering."
        },
        {
            "title": "EVQA HE",
            "content": "I2IR MIC IQASC ICSC ISTE ITRSC MAR MR S2IR STD STS JPS AQA 3DE 3DOD 3DOT 3DPE 3DSR 3DQA PT"
        },
        {
            "title": "GAR MVU MEV NIP",
            "content": "T2IR VR HR PR TL TO"
        },
        {
            "title": "VidCap",
            "content": "RPM SOT 3DCR 3DIR OpenFlamingo [2] 22.3 XComposer2 [19] 21.9 Qwen-chat [3] LLaVA-v1.5 [51] 15.9 19.2 ShareGPT4V [10] 18.5 LLaVA-interleave [43] 32.4 InternVL1.5-chat [14] 37.4 VideoChat2 [48] VideoChat-TPO 35. 40.2 25.5 25.0 24.0 55.0 20.5 29.0 14.1 24.5 16.4 26. 29.5 43.0 63.7 90.5 46.8 54.0 73.3 59.0 25.8 21.5 21.0 35. 2.5 23.0 4.2 17.5 5.0 19.0 24.8 34.0 31.0 35.5 27.5 42. 24.3 39.5 24.6 25.5 10.8 42.5 13.3 18.0 13.7 40.0 10.8 42. 26.3 49.0 22.6 56.5 31.6 59.0 37.0 56.5 21.6 25.0 5.8 22. 2.5 6.0 5.8 15.0 6.2 7.5 23.2 29.5 20.3 23.5 23.6 23. 24.6 27.5 25.0 14.5 0.0 2.5 9.9 6.0 1.9 21.5 9.0 14. 26.4 32.0 16.3 31.0 25.6 30.5 26.5 29.5 28.2 13.5 0.0 19. 5.9 6.0 6.9 4.0 2.7 7.5 25.1 26.0 28.3 24.5 28.8 23. 26.9 21.0 34.5 15.5 34.2 20.0 31.2 32.0 27.3 26.0 34.2 31. 48.8 30.0 63.2 53.0 45.3 44.5 45.0 59.0 49.0 27.5 24.0 8. 23.8 9.0 35.0 7.5 28.5 7.0 49.8 21.5 38.5 26.0 54.3 26. 69.5 25.0 14.5 4.0 14.5 15.5 10.5 13.5 6.5 26.5 4.5 29. 23.5 42.0 21.0 40.0 20.5 44.0 20.5 44.0 19.0 25.5 2.5 45. 19.5 17.0 12.5 17.5 10.5 18.0 25.0 47.5 28.0 49.0 25.5 36. 23.5 48.5 13.5 23.0 23.0 0.0 12.5 15.5 12.5 5.0 3.5 5. 28.0 22.5 26.5 25.5 25.5 25.0 29.5 27.5 22.5 7.0 63.5 0. 41.0 3.5 53.0 4.5 57.0 1.5 57.0 14.0 82.5 15.5 64.0 18. 83.0 14.5 17.5 22.1 19.0 20.6 5.5 40.2 10.0 25.6 4.0 28. 21.5 23.6 20.5 59.3 21.0 38.6 21.0 73.4 26.0 3.0 26.0 0. 13.5 15.8 25.5 27.1 12.5 23.3 33.0 32.3 31.5 43.6 31.0 44. 31.0 44.4 39.0 1.5 14.5 16.5 29.5 16.5 66.5 8.5 55.5 9. 63.5 17.5 6.0 19.5 31.5 21.0 92.5 23.5 49.0 26.5 31.0 0. 45.0 16.5 43.0 8.0 44.5 3.0 54.5 28.5 45.5 22.5 50.0 26. 49.5 27.5 20.0 22.0 9.5 7.0 3.0 22.5 19.0 4.0 13.5 7. 25.0 23.0 26.5 23.5 21.0 24.0 29.5 24.5 27.5 35.0 28.5 0. 12.0 17.5 3.5 6.0 5.0 6.0 26.0 17.5 29.5 15.0 31.0 13. 30.0 7.5 10.0 17.0 31.5 4.5 10.0 13.0 2.5 6.0 5.0 2. 24.0 3.0 29.5 33.5 30.5 0.0 24.5 0.0 13.5 28.5 59.5 0. 52.5 14.5 23.5 14.5 26.0 8.0 27.0 31.0 85.0 28.0 73.0 28. 88.0 24.0 16.5 20.5 44.0 33.5 18.5 14.0 36.5 29.5 38.0 27. 49.5 36.0 65.0 39.0 51.0 43.0 67.5 38.5 30.0 23.5 30.0 63. 16.5 8.0 12.0 66.0 14.0 65.5 29.0 79.0 32.0 71.0 31.5 65. 34.5 67.0 20.0 11.5 4.5 1.5 2.5 3.0 16.5 2.0 15.5 0. 23.0 15.0 23.5 9.5 23.5 11.5 29.5 11.5 18.7 31.0 15.5 38. 3.6 8.5 6.7 35.0 10.9 44.0 25.4 60.5 29.0 46.5 21.8 58. 36.8 58.5 24.5 25.0 12.0 42.0 5.5 1.5 7.0 34.5 6.0 36. 27.5 34.5 18.5 50.5 24.0 36.0 24.5 47.0 22.5 23.5 66.0 33. 47.0 0.5 28.0 28.5 25.0 31.0 32.5 42.5 89.0 39.5 81.5 35. 94.5 40.5 Table 14. Quantitative results of MMIU [60]. Accuracy is the metric, and the Overall score is computed across all tasks."
        },
        {
            "title": "Model",
            "content": "Charades-STA [24]"
        },
        {
            "title": "MVBench",
            "content": "R@0.3 R@0.5 R@0.7 mIoU VideoChat-TPO"
        },
        {
            "title": "Only QVHighlight",
            "content": "58.3 54.8 40.2 34.6 18.4 15.1 38.1 35."
        },
        {
            "title": "AVG",
            "content": "66.8 66.5 evaluations emphasize the accuracy of responses to specific questions. After optimization with TPO, the model has significantly improved its instruction following, leading to higher success rate. Table 15. Ablation task datasets. Grounded Reasoning (VGR), Multiple Image Captioning (MIC), Spot the Difference (STD), General Action Recognition (GAR), Temporal Localization (TL), Video Captioning (VidCap), Multiview Action Recognition (MAR), Image Captioning with Spatial Context (ICSC), and Egocentric Video Question Answering (EVQA), with scores of 73.0 (+26.5%), 69.5 (+15.2%), 83.0 (+19%), 92.5 (+61%), 88.0 (+15%), 94.5 (+13.0%), 73.4 (+35.8%), 48.5 (+12%) and 59.0 (14.5%), respectively. Among them, we suppose the improvement of TL capability comes from the optimization of our temporal head, and the improvement of VGR, STD, MAR and ICSC capabilities comes from the optimization of our region head and mask head. The enhancements observed in captioning, specifically in metrics such as MIC, IC, and VidCap, indicate an improvement of TPO to capture detailed visuals. Meanwhile, we find that the improvement in multi-image capabilities stems from enhanced instruction comprehension. Compared with video assessments, which primarily consist of multiple-choice questions, multi-image How Scaling Task Data Affect MLLMs. We perform an ablation experiment on the dataset of stage 2 to evaluate the impact of the task training data on the model performance. Specifically, we reduce the number of temporal grounding datasets from six to one (QVHighlight [39]). As shown in Table 15, using only one dataset leads to slightly worse conversational performance (-0.3%) on MVBench and significantly poorer expert task performance (-5.6%) on CharadesSTA R@0.5, when compared to employing multiple temporal grounding datasets for training the temporal task head. Notably, this approach remains more effective than training after textualizing the task data in QA tasks like MVBench. This finding indicates that scaling task data gives notable performance improvements in both multimodal and specific vision tasks. Various datasets are necessary for effectively enhancing TPOs dialogue capabilities and achieving zeroshot generalization to fine-grained visual tasks. Stage 2 Config Frozen Vision Enc. LR Frozen Connector LR 1e-4 Temporal Head LR 1e-4 Region Head LR Frozen Mask Head LR 1e-4 Mask Adapter LR 2e-4 Temporal Token LR 1e-4 Region Token LR 1e-4 Mask Token LR 2e-5 LLM LoRA LR Cosine Decay LR Schedule AdamW [54] Optimizer 0.02 Weight Decay 2242 Input Resolution 16 Input Frames 16 LLM LoRA Rank 32 LLM LoRA Alpha 0.2 Warmup Ratio 64/128/128 Total Batch Size Epoch 25/3/1 Numerical Precision DeepSpeed bf16 [68] DeepSpeed bf16 [68] DeepSpeed bf16 [68] DeepSpeed bf16 [68] Stage 3 w/o Con. 2e-5 2e-5 2e-5 2e-5 Frozen 2e-5 2e-5 2e-5 2e-5 2e-5 Cosine Decay AdamW [54] 0.02 2242 16 16 32 0.2 256 1 Stage 1 Frozen Frozen - - - - - - - 2e-5 Cosine Decay AdamW [54] 0.02 2242 16 16 32 0.2 128 1 Stage 3 2e-5 2e-5 2e-5 2e-5 Frozen 2e-5 2e-5 2e-5 2e-5 2e-5 Cosine Decay AdamW [54] 0.02 2242 16 16 32 0.2 256 3 Table 16. Training Settings of VideoChat-TPO. Con. means conversation data and LR means learning rate."
        },
        {
            "title": "Datasets",
            "content": "Stage 1 Stage 2 Stage"
        },
        {
            "title": "Temporal Grounding\nSpatial Grounding\nSegmentation\nTemporal Reasoning\nConversation",
            "content": "50K 50K 50K 116.5K 540.0K 114.6K 7.5K 400K 116.5K 40K 3M DiDeMo [27], QuerYD [63] RefCOCO [103], RefCOCOg [103], RefCOCO+ [59] SAMv2 [69], MeViS [18] DiDeMo [27], QuerYD [63], HiRest [107], ActivityNet [7], TACoS [70], NLQ [25] AS-V2 [85], Visual Genome [37], RefCOCO [103], RefCOCO+ [103], RefCOCOg [59] SAMv2 [69], MeViS [18] QVHighlight [39] AS-V2 [85], Visual Genome [37], RefCOCO [103], RefCOCO+ [103], RefCOCOg [59] MeViS [18], SAMv2 [69] YouCook2 [17], ActivityNet [7] VideoChat2-IT [48], ShareGPT-4o [14], LLaVA-Hound-DPO [113], ShareGPT4V [9] Table 17. Training Datasets. The temporal grounding includes two subtasks: moment retrieval and highlight detection. 2. Training and Data Details Table 16 and 17 lists the detailed training configurations and data of VideoChat-TPO in different stages. In each stage, the model is parametrized from the weights from the previous stage and continues training. Settings of Stage 1. The LLM is equipped with LoRA [28] for saving computational memory, using LoRA rank of 16 and an alpha of 32. Only the LoRA is trained for efficiency. We adopt the AdamW optimizer [54] with the peak learning rate of 2e-5 and use cosine weight decay. The training involves total batch size of 128 across 32 A100 GPUs. Since the purpose of stage 1 is to make MLLM identify tasks, we only use small amount of data in this stage and adopt LLM loss so that LLM can generate task-specific tokens. For each task, we train the LLM with 50k examples to recognize the task. For training data, we use DiDeMo [27] and QuerYD [63] for temporal grounding task, RefCOCO [103], RefCOCOg [103] and RefCOCO+ [103] for spatial grounding task, and SAMv2 [69], MeViS [18] for segmentation task. In stage 2, we add the task heads (i.e. Settings of Stage 2. temporal head, region head, and mask head) and learnable task tokens (temporal token, region token, and mask token). The objective of the second training stage is to learn the task head with preliminary functional capabilities. Therefore, we train LLM, task head and task token at this stage, and freeze vision encoder and connector. In stage 2, the region head and token are trained with learning rate of 2e-5 using cosine learning rate scheduler. We use two-layer MLP with MSE loss as the region head to train from scratch. For training data, we use AS-V2 [85], Visual Genome [37], RefCOCO [103], RefCOCOg [103], RefCOCO+ [103] for one epoch with batch size of 128. We use learning rate of 1e-4 for the temporal head and 2e-4 for the temporal token in stage 2. The tempo15 Tracking. Some tracking results are shown in Figure 6. Users need to locate the target (e.g. bounding box coordinates) in the first frame of the input video. The visualizations show that when the target object is partially occluded in the video, it can still be tracked. Even if the target object is out of the cameras view, VideoChat-TPO can still follow it when it appears in subsequent frames. Moment Retrieval and Highlight Detection. The moment retrieval and highlight detection are illustrated in Figure 7. VideoChat-TPO can target events based on the users questions, and perform moment retrieval and highlight detection on the target events. Multimodal Video Understanding. The multimodal video understanding visualizations are shown in Figure 8. VideoChat-TPO achieves decent results in fine-grained action description, spatial description, and video captioning. ral head is the same as CG-DETR [61] in structure, but we use the pre-trained InternVideo2 [89] to extract video features, while query features are extracted using the ChineseLlama-Alpaca [15]. We use the same loss function in CGDETR. We train the model on DiDeMo [27], QuerYD [63], HiRest [107], ActivityNet [7], TACoS [70], NLQ [25] for 25 epochs with total batch size of 64. For the mask head, we use the pre-trained SAM2 [69] model, replacing the prompt encoder of SAM2 with single MLP layer (mask adapter). During learning, only the mask token and adapter are trained with learning rate of 2e-5. We use MeViS [18] , SAMv2 [69] for three epochs in this stage with batch size of 128. We supplement the training data by adding expanded ASv2 [85] (we convert images into videos) to this stage. Settings of Stage 3. The third training stage aims to strengthen the models conversational ability using TPO. This stage is divided into two parts. The first part involves training on combined dataset of all tasks. The second part uses dataset combining both task and conversation data. For conversatation data, we use VideoChat2IT [48], ShareGPT-4o [14], LLaVA-Hound-DPO [113], ShareGPT4V [10] for instruction finetuning. We adopt peak learning rate of 2e-5 for all the model in this stage and use total batch size of 128. Template Details. To support the proper invocation of task-specific decoders, we construct series of instruction templates for different tasks and use them as instruction tuning data for MLLM. We comprehensively list all the instruction templates below, in Table 18, 19, 20, and 21. 3. Qualitative Results We evaluate VideoChat-TPO on various visual perception tasks and display the visualizations from Figure 4 to Figure Figure 7. In addition, we also show the results of multimodal video understanding in Figure 8. Spatial Grounding. In Figure 4, we show the spatial grounding visualizations. VideoChat-TPO finds the target object from the description of natural language. It accurately locates the target among multiple similar objects, even if the target is occluded or in the background area. Referring Segmentation. We show the visualizations of the referring segmentation in Figure 5. VideoChat-TPO can delinear the target object in the video according to user input in complex scenes. Furthermore, VideoChat-TPO can separate the target object from multiple objects of the same kind according to the description of appearance or action characteristics indicated by the user. 16 1. Localize the visual content described by the given textual query query in the video, and output the start and end timestamps in seconds. 2. Detect and report the start and end timestamps of the video segment that semantically matches the given textual query query. 3. Locate and describe the visual content mentioned in the text query query within the video, including timestamps. 4. The given natural language query query is semantically aligned with video moment, please give the start time and end time of the video moment. 5. Find the video segment that corresponds to the given textual query query and determine its start and end seconds. Table 18. Instructions for Temporal Grounding. 1. Track the object in the video using box with initial coordinates track box. 2. Use bounding box with coordinates track box to follow the movement of the moving object in the visual input. 3. Given an initial bounding box with coordinates track box, track the motion of the target object in the sequence of frames. 4. Starting from the box defined by the coordinates track box, monitor the movement of the object in the video. 5. Utilizing the initial box specified by the coordinates track box, continuously track and update the location of the object in the video stream. 6. Given video with an object of interest enclosed in bounding box with coordinates track box, generate sequence of bounding boxes that track the objects movement. 7. With an initial box defined by track box, trace the objects trajectory by generating sequence of bounding boxes that follow the objects movement in the visual input. 8. Apply an object tracking algorithm to video, starting with bounding box defined by track box. 9. Given video and an initial bounding box defined by track box, track the movement of the object within the video. 10. Starting from an initial box defined by track box, track the movement of the object in the visual input. Table 19. Instructions for Tracking. 1. Where is expr? 2. Can you find expr? 3. Can you detect expr? 4. Can you locate expr? 5. Please find expr. 6. Please detect expr? 7. Please locate expr? 8. Find expr. 9. Detect expr? 10. Locate expr? 1. Please give the motion path of obj in the video over time. 2. Show the tracking trajectory of objs movement through the scene in the video. 3. Please generate motion path of objs movement in the video, highlighting its tracking trajectory. 4. Show the tracking trajectory of obj. 5. Generate objs tracking trajectory. 6. Visualize the tracking trajectory of obj in the video. 7. Please generate visual representation of objs movement in the video, highlighting its tracking trajectory. Table 20. Instructions for Spatial Grounding. Table 21. Instructions for Referring Segmentation. 17 Figure 4. Qualitative Results of Spatial Grounding. 18 Figure 5. Qualitative Results of Referring Segmentation. 19 Figure 6. Qualitative Results of Tracking. 20 Figure 7. Qualitative Results of Moment Retrieval and Highlight Detection. The orange curve represents the saliency score, the blue interval represents the time interval predicted by the model, and the green interval represents the ground truth. 21 Figure 8. Qualitative Results of Multimodal Video Understanding."
        }
    ],
    "affiliations": [
        "Nanjing University",
        "Shanghai AI Laboratory",
        "Shanghai Jiao Tong University",
        "Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences",
        "University of Science and Technology of China",
        "Zhejiang University"
    ]
}