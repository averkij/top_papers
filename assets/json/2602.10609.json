{
    "paper_title": "Online Causal Kalman Filtering for Stable and Effective Policy Optimization",
    "authors": [
        "Shuo He",
        "Lang Feng",
        "Xin Cheng",
        "Lei Feng",
        "Bo An"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Reinforcement learning for large language models suffers from high-variance token-level importance sampling (IS) ratios, which would destabilize policy optimization at scale. To improve stability, recent methods typically use a fixed sequence-level IS ratio for all tokens in a sequence or adjust each token's IS ratio separately, thereby neglecting temporal off-policy derivation across tokens in a sequence. In this paper, we first empirically identify that local off-policy deviation is structurally inconsistent at the token level, which may distort policy-gradient updates across adjacent tokens and lead to training collapse. To address the issue, we propose Online Causal Kalman Filtering for stable and effective Policy Optimization (KPO). Concretely, we model the desired IS ratio as a latent state that evolves across tokens and apply a Kalman filter to update this state online and autoregressively based on the states of past tokens, regardless of future tokens. The resulting filtered IS ratios preserve token-wise local structure-aware variation while strongly smoothing noise spikes, yielding more stable and effective policy updates. Experimentally, KPO achieves superior results on challenging math reasoning datasets compared with state-of-the-art counterparts."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 1 1 ] . [ 1 9 0 6 0 1 . 2 0 6 2 : r a"
        },
        {
            "title": "Online Causal Kalman Filtering for Stable and Effective Policy Optimization",
            "content": "Shuo He 1 Lang Feng 1 Xin Cheng 1 Lei Feng 2 Bo An"
        },
        {
            "title": "Abstract",
            "content": "Reinforcement learning for large language models suffers from high-variance token-level importance sampling (IS) ratios, which would destabilize policy optimization at scale. To improve stability, recent methods typically use fixed sequencelevel IS ratio for all tokens in sequence or adjust each tokens IS ratio separately, thereby neglecting temporal off-policy derivation across tokens in sequence. In this paper, we first empirically identify that local off-policy deviation is structurally inconsistent at the token level, which may distort policy-gradient updates across adjacent tokens and lead to training collapse. To address the issue, we propose Online Causal Kalman Filtering for stable and effective Policy Optimization (KPO). Concretely, we model the desired IS ratio as latent state that evolves across tokens and apply Kalman filter to update this state online and autoregressively based on the states of past tokens, regardless of future tokens. The resulting filtered IS ratios preserve token-wise local structureaware variation while strongly smoothing noise spikes, yielding more stable and effective policy updates. Experimentally, KPO achieves superior results on challenging math reasoning datasets compared with state-of-the-art counterparts. Figure 1. Illustration of local structural off-policy patterns. Raw token-level importance-sampling (IS) ratios (blue) exhibit high local variance and structural inconsistency, whereas sequencelevel IS ratio (purple) is globally smooth but obscures withinsequence structure. Off-policy frequency increases over the sequence (window-wise statistics), off-policy runs are short-lived (run-length distribution), and token states switch frequently, suggesting weak local coherence. Token-level Kalman filtering (red) yields locally smoothed yet structurally consistent IS ratios. 1. Introduction Reinforcement learning (RL) has become key method for advancing large language models (LLMs) beyond the limits of pretraining (OpenAI, 2024; Yang et al., 2025a; DeepSeekAI, 2025). With large-scale RL, e.g., GRPO (DeepSeekAI, 2025), LLMs can acquire substantially stronger longhorizon reasoning ability, enabling solutions to challenging tasks across diverse domains, e.g., code generation (Lyu et al., 2025), information retrieval (Zhang et al., 2025c), software engineering (Qian et al., 2024), and open-ended 1Nanyang Technological University, Singapore. Email: shuohe123@gmail.com 2Southeast University, China. Correspondence to: <>. Preprint. February 12, 2026. device control (Tan et al., 2025; Bai et al., 2025). Despite these gains, recent research reveals that GRPO-style optimization can become unstable at scale, with high policygradient variance and entropy collapse (Cui et al., 2025). Moreover, instability is often worsened in off-policy settings (e.g., mini-batch updates), where the policy gradient additionally depends on an importance-sampling (IS) ratio between the updated policy and the behavior (old) policy, potentially amplifying policy-gradient variance (Zheng et al., 2025a). In practice, the IS ratio can be further destabilized by Mixture-of-Experts routing discontinuities (Zheng et al., 2025b), train-inference mismatches (Liu et al., 2025a), and inconsistent numerical precision (Qi et al., 2025). Consequently, reducing the variance of the IS ratio is critical for stabilizing large-scale policy optimization (Zheng et al., 2025a). To this end, recent methods typically re1 Online Causal Kalman Filtering for Stable and Effective Policy Optimization place token-wise ratios with fixed sequence-level IS ratio, e.g., GSPO (Zheng et al., 2025b) and GMPO (Zhao et al., 2025), or separately adjust token-wise IS ratios via soft gating functions (Gao et al., 2025) or flipping the IS ratios of positive-advantage tokens (Wang et al., 2025b). However, they totally neglect the temporal structure of off-policy derivation across tokens within sequence, which is important for stable and effective policy optimization. Intuitively, off-policy deviation across tokens within sequence should exhibit global heterogeneity but local homogeneity. For instance, in complex mathematical reasoning, single response typically comprises multiple distinct reasoning paths, which may differ substantially in off-policy deviation, i.e., global heterogeneity. Yet within specific reasoning path, adjacent tokens share similar local semantics and should therefore display coherent, slowly varying off-policy deviation, i.e., local homogeneity. Motivated by this intuition, in this paper, we pioneer new temporal perspective on token-wise IS ratios and examine the patterns of off-policy derivation. Specifically, we perform GRPO on Qwen3-4B and record corresponding token-wise IS ratios. The deatiled experimental setting is shown in Appendix B.1. We design three complementary statistics for off-policy tokens: (i) window-wise occurrence frequency, (ii) run-lengths (consecutive occurrences), and (iii) switching frequency (categorized into up when ratio > 1 and down when ratio < 1). As shown in Figure 1, window-wise occurrence frequency increases along the sequence, suggesting that more off-policy tokens occurs in the later part of the sequence. Meanwhile, off-policy tokens are short-lived (low run-lengths), and states switch frequently (e.g., switch frequency 0.41), indicating weak local structural coherence. Overall, these observations point to key phenomenon: local off-policy deviation is structurally inconsistent at the token level. The phenomenon counters our earlier intuition of local homogeneity and could distort the updates of policy gradients across adjacent tokens and ultimately results in the training collapse of GRPO. To address this issue, we propose Online Causal Kalman Filtering for stable and effective Policy Optimization (KPO), structure-aware autoregressive smoothing method for tokenwise IS ratios. Our key idea is to model the token-wise IS ratio as time series within state-space framework: process model captures smooth latent dynamics across adjacent tokens, while an observation model accounts for noise in the computed ratios. We then apply an online Kalman filter to produce token-wise filtered ratio using only the information from past and current tokens. The estimated IS ratio of the current token will consider the local past tokens IS ratios, thereby preserving the local off-policy structure. In this way, the Kalman-filtered IS ratio is both numerically smoothed and structurally coherent. As shown in Figure 1, the Kalman-filtered IS ratio (red) increases the token run-lengths and decreases the token switching, which enables more stable and more effective policy optimization. Extensive experiments on six challenging mathematical reasoning tasks validate the stability and effectiveness of KPO compared with current state-of-the-art methods. Our main contributions are: An empirical revealing for local off-policy patterns. We empirically reveal the structural inconsistency of local off-policy derivation in token-level IS ratios, which could degrade the policy optimization. novel policy optimization method: KPO. We propose causal Kalman filtering over token ratios to smoothing local fluctuations while preserving within-sequence local coherent structure, improving stability and effectiveness under off-policy updates. Strong and stable empirical results. KPO consistently improves performance on math-reasoning benchmarks over state-of-the-art baselines, while simultaneously enhancing training stability. 2. Related Work Post-training for LLMs. Supervised fine-tuning (Wei et al., 2022; Sanh et al., 2022) adapts pretrained LLMs to instruction following via demonstrations. In contrast, RL from human feedback (RLHF) (Stiennon et al., 2020; Ouyang et al., 2022; Bai et al., 2022; Ethayarajh et al., 2024; Garg et al., 2025) optimizes preference reward model with KL regularization to reference policy. TRPO (Schulman et al., 2015a) imposes an explicit KL constraint, while PPO (Schulman et al., 2017) uses clipped surrogate and remains the default for scalable RLHF. Recent post-training reduces critic and on-policy overhead. DPO (Rafailov et al., 2023) yields closed-form objective for KL-regularized preference learning, replacing RL with supervised optimization. GRPO (Shao et al., 2024; 2025) further removes the critic by using group-relative normalization over multiple samples per prompt, reducing memory while retaining PPOstyle (Zheng et al., 2023; Touvron et al., 2023) stability. Beyond single-turn alignment, RL has also been extended to long-horizon agentic settings (Feng et al., 2025; Zhou et al., 2023; Zeng et al., 2023; Zhai et al., 2024; Yao et al., 2023; Zhang et al., 2024; Chen et al., 2025a) Advanced variants of group-based RL. Recent groupbased RL methods extend GRPO-style objectives by refining how learning signals are estimated, normalized, and utilized. first line of work focuses on improving advantage estimation and group weighting to correct biased or degenerate updates. Dr. GRPO (Liu et al., 2025b) identifies estimator-driven collapse in R1-ZERO-like trainOnline Causal Kalman Filtering for Stable and Effective Policy Optimization ing, while GVPO (Zhang et al., 2025a) derives principled group weights from KL-constrained reward maximization to achieve better variance control. Building on more reliable advantage estimates (Yang et al., 2025c), subsequent work addresses instability introduced by normalization under bounded or multi-reward supervision. BNPO (Xiao et al., 2025) applies Beta-based normalization to stabilize gradients on bounded rewards, whereas GDPO (Liu et al., 2026) shows that shared normalization can collapse advantages in multi-reward settings and resolves this issue through reward-decoupled normalization. As group-based methods are increasingly applied to long-chain-of-thought rollouts, several studies target training stability and efficiency at scale. DAPO (Yu et al., 2025) and Stable-GRPO (Dai et al., 2025) introduce practical stabilizers such as decoupled clipping (Yang et al., 2025b) and staged truncation to mitigate length-induced collapse, while CPPO (Lin et al., 2025) and ARM (Wu et al., 2025) improve sample efficiency by pruning low-contribution completions or adapting reasoning formats on per-instance basis. When external rewards become unreliable or unavailable, recent work explores uncertainty-aware updates. SEED (Chen et al., 2025b) and RIGHT (Zhang et al., 2025b) leverage model uncertainty for reweighting or intrinsic supervision, whereas minimalist analyses (Xiong et al., 2025) and KRPO (Wang et al., 2025a) clarify the role of filtering effects and smoothing in group-based advantage estimation. IS-ratio-oriented RL. To achieve stable policy optimization at scale, GMPO (Zhao et al., 2025) and GSPO (Zheng et al., 2025b) use fixed sequence-level ratios for all tokens within sequence. In contrast, SAPO (Gao et al., 2025) introduces adaptive clipping via soft gating functions to continuously modulate token-wise ratios based on their magnitudes. Besides, ASPO (Wang et al., 2025b) employs asymmetric ratio handling to tighten the update on risky high-ratio tokens while retaining learning signal elsewhere. Notably, we take new temporal view for token-wise IS ratios within sequence, and perform structure-aware smoothing for stable and effective policy optimization. 3. Preliminaries In this section, we introduce the necessary notions for policy optimization. We consider dataset of prompt-response pairs (x, y), where the response = [y1, y2, . . . , yT ] with tokens is generated autoregressively by large language model parameterized by θ, i.e., policy πθ : πθ(y x) = (cid:81)T t=1 πθ(yt x, y<t). prompt-response pair (x, y) can be assigned score s(x, y) [0, 1] by verifier R. Proximal policy optimization (PPO). We optimize new policy πθ using samples collected from an old policy πθold . For each token yt, we define the token-level importancesampling (IS) ratio rt = πθ(ytx,y<t) πθold (ytx,y<t) , which corrects for the distribution shift between the behavior policy and the updated policy. PPO uses token-level advantage estimate At, typically obtained from learned value function (e.g., via GAE (Schulman et al., 2015b)). The clipped surrogate objective of PPO is (cid:34) JPPO = E"
        },
        {
            "title": "1\nT",
            "content": "T (cid:88) t=1 (cid:16) min rt At, At (cid:35) (cid:17) , (1) = clip(cid:0)rt, 1 ϵ, 1 + ϵ(cid:1) and ϵ controls the clipping where range. The clipping operation limits overly large policy updates by capping the effective ratio used in the objective. Group relative policy optimization (GRPO). GRPO follows the same clipped form, but avoids an explicit Instead, for each prompt x, it samples value model. group of responses {yi}G i=1 from πθold, evaluates each response with verifier score s(, ), and constructs sequence-level relative advantage. This advantage is then shared across all tokens within the corresponding response: (cid:98)Ai,t = s(xi,yi)mean(cid:0){s(xi,yi)}G std(cid:0){s(xi,yi)}G (cid:1) i=1 i=1 (cid:1) . The resulting objective of GRPO is JGRPO = (cid:34) 1 (cid:88) i=1 1 (cid:88) t=1 min (cid:16) ri,t (cid:98)Ai,t, i,t (cid:98)Ai,t (cid:35) (cid:17) , (2) Token-level policy gradient in GRPO. To better understand how the IS ratio affects the policy gradient, we formulate the policy gradient of the GRPO objective for an individual token: ri,t θ log πθ(yi,t x, yi,<t) (cid:98)Ai,t. (3) In the on-policy setting, the IS ratio ri,t is equal to 1 for all tokens, while in the off-policy setting, the IS ratio could vary among the clipping bounds, which could have negligible impact on the policy gradient, such as amplifying the covariance effect of tokens with high probability and high advantages (Cui et al., 2025). Hence, GRPO could be unstable for policy optimization at scale (Zheng et al., 2025a), especially under the off-policy optimization. IS ratios. To address Sequence-level issue, GSPO (Zheng et al., 2025b) and GMPO (Zhao et al., 2025) directly use fixed sequence-level IS ratio (stopped gradient) for each token in sequence, by averaging token-wise IS ratios across the entire sequence: the ri = (cid:32) (cid:89) t=1 (cid:33) 1 (cid:32) ri,t = exp 1 (cid:88) t=1 (cid:33) log ri,t . (4) This practice reduces variance by totally smoothing tokenwise fluctuations into single scalar, regardless of the withinsequence off-policy heterogeneity. In contrast, we focus on local structure-aware smoothing for token-wise IS ratios. 3 Online Causal Kalman Filtering for Stable and Effective Policy Optimization 4. Method: Online Causal Kalman Filtering In this section, we will first clarify our motivation and then provide detailed introduction to our proposed method, as shown in Figure 2, which mainly includes three processes to autoregressively filter the IS ratio. 4.1. Motivation As discussed above, using sequence-level importance sampling (IS) ratios is an effective method for stabilizing policy optimization at scale. However, this stability assumes that all tokens in sequence have the same degree of off-policy derivation, which is commonly violated in policy optimization. For instance, in mathematical reasoning tasks, the response generally includes different reasoning steps in chain-of-thought, along which the updated policy may only drift from the behavior policy on certain reasoning steps but maintain on-policy on others. In contrast, our intuition is that although off-policy degrees can vary across semantic segments, they should exhibit local coherence within segment because neighboring tokens are usually semantically consistent. It is therefore implausible for the policy to alternate sharply or frequently relative to the behavior policy at the token-to-token level. These considerations motivate us to design method that can not only smooth the IS ratio but also preserve the local coherent structure. For this purpose, we treat per-token IS ratios as structured but noisy time series over token positions and seek an online estimate of the latent, locally coherent off-policy signal using only past and current tokens, corresponding to the autoregressive generation of tokens. 4.2. Kalman filtering for token-wise IS ratios Formally, we treat {zi,t}T t=1 as left-to-right observed time series for the sequence xi including tokens. In particular, it is associated with an underlying latent smoothed IS ratio ρi,t. Notably, we consider the causal Kalman filtering in the log space of the IS ratios zi,t = logri,t due to numerical stability. Our objective is to estimate the latent IS ratio ρi,t by Kalman filtering, which is both locally smoothed and structurally coherent. Next, we will first introduce the Kalman state-space model, and then detail the filtering process, including three processes. Kalman state-space model. To characterize the variation of the latent (observed) IS ratio, given each sequence including tokens (we omit the subscript for simplicity in the next), we model the latent and observed IS ratio (Grewal, 2025) respectively: (cid:40) ρt = ρt1 + ηt, zt = ρt + ϵt, ηt (0, Q), ϵt (0, ). (5) 4 Figure 2. Causal Kalman filtering. The filter alternates prediction, adaptive gain computation, and update to produce smoothed estimate ˆρtt and its uncertainty Ptt from streaming observations zt, using process noise and observation noise . where ρt is the latent desired IS ratio and Q/V is the process/observation noise variance.Q and govern the Kalman filters smoothing tracking trade-off. is the process-noise variance, controlling how quickly the latent IS ratio may evolve across tokens. larger permits faster drift and improves responsiveness to real local shifts, while smaller imposes stronger temporal smoothness at the cost of potential lag. In contrast, is the observation-noise variance, reflecting the reliability of the observed IS ratio zt. larger down-weights noisy observations and suppresses isolated spikes and rapid switching, whereas smaller makes the estimate follow zt more closely. Together, and determine the Kalman gain and thus balance denoising against fidelity to true off-policy dynamics in KPO. ① Prediction step. To capture the intuition that the offpolicy degree may evolve gradually across token positions, we employ naive random-walk dynamics with processnoise variance controlling how quickly the latent signal is allowed to drift. Formally, let ˆρtt and Ptt denote the posterior mean and variance of ρt given observations z1:t. Similarly, ˆρtt1 and Ptt1 denote the one-step-ahead predictive quantities given z1:t1. We initialize with prior (ˆρ00, P00) (e.g., ˆρ00 = 0 and P00 = P0). ˆρ00 = 0 indicates the IS ratio of the first token is on-policy (1). The prediction step will only use the observed IS ratio of past tokens, regardless of future tokens, corresponding to the autoregressive generation of tokens: (cid:40) ˆρtt1 = ˆρt1t1, Ptt1 = Pt1t1 + Q. (6) ② Kalman gain. After predicting the posterior mean and variance of ρt, the next step is to predict the Kalman gain, which is the key for Kalman filtering. The Kalman gain determines how the filter trades off the current observation against the prior prediction when estimating the latent IS ratio at token position t. Given the one-step prediction (ˆρtt1, Ptt1), we first compute the innovation (residual) δt = zt ˆρtt1, (7) Online Causal Kalman Filtering for Stable and Effective Policy Optimization which measures the discrepancy between the observed logratio and the predicted latent value. The Kalman gain is then Kt = Ptt1 Ptt1 + , (8) By construction, Kt [0, 1] in the scalar case and acts as an adaptive step size for correcting the prediction with the new measurement. When the predicted uncertainty Ptt1 is large (or the measurement noise is small), Kt increases and the filter places more weight on zt, allowing rapid adaptation to real changes in off-policy degree. Conversely, when is large, Kt decreases and the update relies more on the temporal prior, thereby suppressing isolated spikes and high-frequency fluctuations in zt. In our setting, this adaptive weighting is crucial, since it preserves persistent, structure-aware shifts in token-wise off-policy dynamics while attenuating system-induced noise in the observed ratios. ③ Update step. Given the innovation δt and the Kalman gain Kt, we incorporate the new observation by performing linear correction of the one-step prediction: (cid:40) ˆρtt = ˆρtt1 + Kt δt, Ptt = (1 Kt) Ptt1. (9) The first line performs an innovation-based correction: Kt serves as an adaptive step size that controls how far the estimate moves from the prior ˆρtt1 toward the measurement zt. Larger Kt makes the filter track genuine changes more quickly, while smaller Kt favors the temporally smoothed prior and suppresses transient fluctuations. The second line updates uncertainty: incorporating zt reduces the posterior variance by factor (1 Kt), with higher gains implying greater information gain. In our setting, this recursion stabilizes token-wise weighting by damping isolated spikes (large δt with small Kt) while allowing persistent deviations across neighboring tokens to accumulate into smooth, coherent shifts in ˆρtt. 4.3. The policy optimization objective of KPO After filtering in log space, we map the latent estimate back to ratio space by exponentiation: (cid:101)rt = exp(ˆρtt). We then replace the raw token-wise IS ratio rt in GRPO-style objectives with the filtered ratio (cid:101)rt. Let (cid:98)At denote token-shaped sequence-level advantage. The resulting KPO objective is JKPO = (cid:88) i=1 (cid:88) i=1 (cid:34) (cid:34) 1 = clip(cid:0) 1 E 1 1 (cid:88) t=1 (cid:88) t=1 min (cid:16) (cid:101)ri,t (cid:98)Ai,t, (cid:101)r i,t (cid:98)Ai,t (cid:35) (cid:101)ri,t (cid:98)Ai,t , (cid:35) (cid:17) , (10) (cid:101)rt, 1 ϵ, 1 + ϵ+(cid:1) and ϵ(ϵ+) is the where (cid:101)r clipping threshold. In particular, the first line corresponds to the clipped surrogate, while the second line recovers the unclipped variant. Compared with sequence-level aggregation (Eq. (4)), KPO preserves token-wise heterogeneity in off-policy degree by maintaining per-token ratios, yet substantially reduces the variance through causal filtering. As result, KPO inherits the fine-grained credit assignment of token-level reweighting while improving optimization stability. The full procedure is summarized in Algorithm 1 in Appendix A.1. 5. Experiments In this section, we describe the evaluation setting, baselines, and implementation details for assessing KPO on challenging mathematical reasoning benchmarks. 5.1. Setup We use Qwen3-4B (Yang et al., 2025a) for evaluation. Unless stated otherwise, all methods share the same model initialization, tokenizer, and inference pipeline. For RL fine-tuning, we adopt the supervised RL training corpus from DAPO (Yu et al., 2025), which contains diverse mathematical problems paired with verifiable solutions and corresponding reward signals. Evaluation benchmarks and metrics. We follow the standard protocol for math-reasoning evaluation and report avg@16 and pass@16 on five competitive benchmarks: AIME24, AIME25, AMC23, MATH500 (Hendrycks et al., 2021), and OlympiadBench. For each problem, we sample 16 candidate solutions and compute: (i) pass@16, the fraction of problems for which at least one sample is correct, and (ii) avg@16, the mean accuracy across the 16 samples. The benchmarks are: AIME24, AIME 2024 problems that emphasize contest-style multi-step reasoning with short numeric answers; AIME25, AIME 2025 problems with the same answer format and comparable difficulty; AMC23, AMC 2023 multiple-choice problems targeting intermediate contest mathematics; MATH500, 500-problem subset of MATH covering broad topics with multi-step reasoning and solution supervision; and OlympiadBench, Olympiad-level problems designed to stress high-difficulty reasoning and proof-oriented skills. Baselines. We compare KPO with token-level GRPO and representative sequence-level ratio stabilization methods, including GMPO and GSPO. All methods are evaluated under the same inference protocol, model backbone, and decoding budget to ensure controlled comparison. Implementation details. For fair comparison, we adopt the same training configuration for all methods: training batch size of 32, minibatch size of 8 (yielding four offpolicy minibatches), and group size of 8. we set the Kalman filter parameters to = 1106(V = 1) for KPO5 Online Causal Kalman Filtering for Stable and Effective Policy Optimization Table 1. Main results on challenging math reasoning tasks. Columns are benchmarks and rows are methods. We report avg@16 and pass@16. Higher is better. Best results are in bold and second-best are underlined. KPO achieves overall superior performance. Method GRPO GMPO GSPO KPO-unclipped KPO-clipped AIME24 AIME25 AMC23 MATH Minerva Olympiad avg@16 pass@16 avg@16 pass@16 avg@16 pass@16 avg@16 pass@16 avg@16 pass@16 avg@16 pass@16 27.29 30.83 32.70 34.79 37.91 53.33 50.00 60. 66.67 63.33 23.12 27.50 29.16 33.75 36.87 43.33 46.66 50.00 50.00 60.00 73.43 76.56 75. 80.00 87.50 92.50 87.50 95.00 95.00 95.00 85.66 86.62 87.41 88.24 89.42 92.80 93.40 94. 93.00 94.80 38.67 38.48 37.17 39.15 38.23 50.36 50.73 47.79 50.36 48.16 48.60 49.27 51. 52.31 54.06 62.25 60.92 63.00 62.55 66.27 Figure 3. Training dynamics of KPO-clipped over optimization steps. From left to right: mean episodic reward, policy entropy, PPO clip fraction, and policy gradient loss. Solid lines denote the average across runs, and the shaded bands indicate variability across runs. clipped and = 1 104(V = 1) for KPO-unclipped respectively. The clipping parameters ϵ and ϵ+ are set to 0.0003 and 0.0004 for KPO-clipped. All methods use the same sampling strategy and generate 16 candidate solutions per problem for computing avg@16 and pass@16. The detailed setting is shown in Appendix B.2. 5.2. Experimental results KPO achieves overall superior performance. Table 1 summarizes avg@16 and pass@16 on six math benchmarks. Overall, KPO consistently improves over strong baselines: GRPO, GMPO, and GSPO, demonstrating the effectiveness of Kalman filtering. In particular, KPO-clipped achieves the best avg@16 on five benchmarks: AIME24 (37.91), AIME25 (36.87), AMC23 (87.50), MATH500 (89.42), and Olympiad (54.06), and also attains the best pass@16 on AIME25 (60.00), MATH500 (94.80), and Olympiad (66.27). On AMC23, KPO matches the top pass@16 (95.00, tied with GSPO) while substantially improving avg@16. Meanwhile, KPO-unclipped attains the best pass@16 on AIME24 (66.67) and the best avg@16 on Minerva (39.15). Minerva is the only benchmark where KPO does not lead in pass@16. GMPO is slightly higher (50.73 vs. 50.36), while KPO remains competitive and improves avg@16. Overall, these experimental results indicate that KPO achieves consistent gains across benchmarks, which definitely validates its superiority. KPO improves most on the challenging benchmarks. The gains are most pronounced on AIME24 and AIME25, where long-horizon reasoning and compounding errors make stable optimization especially critical. Compared with the strongest baseline GSPO, KPO-clipped improves AIME24 from 32.70/60.00 to 37.91/63.33 in avg@16/pass@16, and KPO-unclipped further raises pass@16 to 66.67. On AIME25, KPO-clipped yields larger margin, improving GSPO from 29.16/50.00 to 36.87/60.00. These results suggest that smoothing tokenwise IS ratios while preserving local structure is particularly beneficial for challenging multi-step reasoning. KPO-clipped is generally stronger than KPO-unclipped. Across benchmarks, the clipped variant consistently outperforms the unclipped one, with the largest gains on AIME24, AIME25, and AMC23 (Minerva being the main exception). potential explanation is that Kalman filtering smooths ratio noise but does not fully remove truly extreme, structurally off-policy tokens. PPO-style clipping can still suppress the influence of these tokens, effectively preventing locally coherent off-policy segments from dominating the update. This behavior contrasts with standard GRPO, where clipped events tend to be sporadic and locally incoherent, and with GSPO (GMPO), where clipping is driven by global sequence-level ratio. These results suggest that combining KPO with clipping yields better bias-stability tradeoff than filtering alone. Training dynamics. We also show the training dynamics of frou methods over optimization steps in Figure 3. The metrics are Reward means, Entropy, Clip fraction, and Policy gradient loss. The specific meanings of these metrics are shown in Appendix B.3. From Figure 3, we can summarize the following observations: Reward mean. All curves rise rapidly in the first 6 Online Causal Kalman Filtering for Stable and Effective Policy Optimization Table 2. Comparison of token-level dynamics before and after Kalman filtering (KF). Reported are the token proportion of Up/Down/On, the mean run-length for each token state, the overall token switching frequency, the low-frequency ratio (LFR), and the global and windowed local variance. All reported values are averaged over all samples. Metrics Token proportion Up Down On Token run-length Down On Up Token Switch frequency Low frequency ratio Variance Glob. var. Win. loc. var. Before KF 0.25 After KF 0.35 0.22 0.43 1.64 0.53 3.53 0.22 119.95 135.12 35. 1.57 0.43 (high) 0.01 (low) 0.12 (low) 0.98 (high) 0.19 1e-4 0.15 1e-5 steps. Afterwards, KPO continues to improve and finishes with the highest reward. GSPO and GMPO peak mid-training and then plateau. GRPO diverges after 200 steps and steadily degrades, ending far below the others. Overall, GRPO is clearly unstable. GSPO (GMPO) are stable but saturate early. KPO is both stable and effective throughout training. Entropy. GRPO collapses to near-zero entropy early, indicating near-deterministic behavior and curtailed exploration. In contrast, KPO and GMPO maintain relatively high entropy with mild fluctuations, while GSPO stabilizes at lower but non-zero level. This suggests KPO mitigates entropy collapse and better preserves exploration. Clip fraction. The clip fractions of KPO and GSPO increases sharply early and then stabilize. In contrast, GMPO and GRPO always exhibit near-zero clip fractions (around 0.0015). This is because their clipping bounds are set to be larger than the former. Notably, KPO clips fewer tokens than GSPO. This is because the Kalman-filtered IS ratios are locally structurally coherent, and only part of the tokens within sequence could be clipped, while the sequence-level IS ratios could result in the overall clipping towards the whole sequence. Policy gradient loss. GRPO shows large, noisy oscillations with high variance, consistent with unstable optimization. The other methods remain small policy loss with low variability, indicating more controlled and stable updates. 5.3. Distribution of Kalman-filtered IS ratios In this section, we will further analyze the distribution of the Kalman-filtered IS ratio as shown in Table 2 in terms of (i) the fraction of tokens assigned to each state (Up, Down, On), (ii) how long each state persists once it appears (run-length), (iii) how often the state changes between adjacent tokens (switch frequency), and (iv) frequencyand variance-based measures that reflect whether the sequence is dominated by fast fluctuations or slow trends. The detailed calculation of these metrics is shown in Appendix B.2. Token proportion. This metric measures how often each token state occurs in the sequence. Before filtering, the distribution is dominated by the On state (0.53), with Up and Down appearing at similar rates (Up: 0.25, Down: 0.22). After Kalman filtering, the mass shifts away from On (0.22) and toward the two directional states (Up: 0.35 and Down: 0.43). This shift could not be viewed as undesirable for policy optimization. Although the proportion of on-policy tokens decreases, their local off-policy deviation is structurally smooth and coherent as shown in the token run-length and switching-frequency statistics. More detailed analysis is shown in Appendix C.1. Token run-length. Run-length is the average number of consecutive tokens that stay in the same state. It captures temporal persistence: short run-lengths imply fragmented, noisy state assignments, while long run-lengths imply stable segments. Before Kalman filtering, all states had short runs (Up: 1.64, Down: 1.57, On: 3.53), suggesting frequent fragmentation and many short-lived excursions. After Kalman filtering, run-lengths increase dramatically (Up: 119.95, Down: 135.12, On: 35.11). The filtered sequence, therefore, forms long contiguous blocks, especially for Up and Down, which is consistent with Kalman filtering enforcing temporal coherence across neighboring tokens. Token switch frequency. Switch frequency measures how often the state changes between adjacent tokens (normalized by sequence length). It is direct measure of discontinuity or mixing among states: higher values mean frequent alternation, lower values mean fewer boundaries and more clustering. Before Kalman filtering, the switch frequency is high (0.43), indicating that the state changes roughly every few tokens. After Kalman filtering, it drops sharply to 0.01, meaning that boundaries between states become rare and state assignments are strongly clustered. This supports the observation from run-length: Kalman filtering greatly reduces short-range alternation and merges nearby tokens into consistent segments. Frequency-domain view. Compared to the view of temporal structure, we follow the view of the frequency domain to analyze the IS ratio. We define the low-frequency ratio to measure how much of the signal energy lies in low-frequency components. Low LFR implies rapid, highfrequency fluctuations, while high LFR implies slow variations over long spans of tokens. Before Kalman filtering, LFR is very small (0.12), consistent with signal dominated 7 Online Causal Kalman Filtering for Stable and Effective Policy Optimization shows stable improvement throughout training. Q/V = 1e 4 performs slightly worse but follows similar trend. In contrast, Q/V = 1e 2 results in significantly lower rewards and exhibits performance degradation after midtraining, indicating that weak filtering introduces excessive noise into the ratio signal. Clip faction. On the other hand, the clip fraction reflects the magnitude of policy updates. Q/V = 1e 6 maintains moderate and stable clip fraction, suggesting balanced updates. Q/V = 1e 4 produces the highest clip fraction, indicating more aggressive updates that may risk instability. Q/V = 1e 2 leads to relatively low clip fractions, which is somewhat counterfactual. We conjecture that this may be because, in this case, the Kalman-filtered IS ratios may suffer from some structural estimation deviation. Overall, the results highlight clear trade-off between stability and effectiveness. With stronger filtering (smaller Q/R), the token-level ratios exhibit greater temporal coherence and training becomes more stable, leading to higher training rewards. By contrast, excessively large Q/R weakens the filter, allowing high-frequency noise to leak back into the estimates, which degrades learning dynamics and final performance. In practice, and should be tuned to reflect prior knowledge about ratio variability. For example, when using MoE models or when traininference mismatch arises across rollout and inference engines, the underlying IS ratios may fluctuate more rapidly, and increasing helps the filter track these faster variations. Conversely, when off-policy deviation is mild, smaller is preferable to impose slower, stronger smoothing. 6. Conclusion We pioneer new temporal view for token-wise importancesampling ratios and empirically reveal practical counterfactual mode in off-policy GRPO training: token-level importance-sampling ratios exhibit weak local coherence of off-policy deviation, which could amplify gradient variance, distort the policy-gradient updates of adjacent tokens and thus result in optimization instability. We propose KPO, causal Kalman filter over within-sequence ratios that treats ratio estimation as online state-space inference. KPO suppresses noisy spikes while retaining the structural coherence of importance-sampling ratios, thereby improving both stability and effectiveness. Experiments on challenging mathreasoning benchmarks the superiority of KPO, while remaining lightweight and compatible with existing pipelines. Since KPO needs to process the sequential token autoregressively, it is difficult to parallelize the Kalman filtering procedure like the autoregressive generation of tokens. In the future, it is also interesting to design parallelized Kalman filtering algorithms. Figure 4. Effect of the Kalman filter noise ratio Q/V on training dynamics. From left to right, we report the mean episodic reward and the PPO clip fraction. Solid lines show the mean over multiple runs, and shaded regions indicate standard deviation. by high-frequency changes. After KF, LFR rises to 0.98, showing that the filtered ratio sequence is almost entirely explained by low-frequency components. This aligns with the role of Kalman filtering as temporal filter that suppresses fast oscillations and retains slow trends. The details of frequency-domain anlysis is shown in Appendix C.2. Global and local variance. Variance quantifies the magnitude of fluctuations in the ratio signal. The global variance measures variability over the full sequence, while the windowed local variance measures variability within short windows, capturing local instability. We can see that before Kalman filtering, both are non-trivial (global: 0.19; local: 0.15), indicating substantial fluctuations both overall and within short spans. After Kalman filtering, both variances drop to near zero (1e 4 and 1e 5), implying that the filtered IS ratio is nearly constant at both global and local scales. Together with the higher LFR and the reduced switch frequency, this confirms that Kalman filtering strongly dampens local volatility and produces much more stable token-level IS ratios. Overall, KPO transforms the token dynamics from rapidly switching, high-frequency pattern into long, coherent segments with minimal local variation. This distribution shift of the IS ratios will straighten the policy gradients of local tokens within sequence and thus achieves the stable and effective policy optimization. 5.4. Parameter analysis In this section, we will analyze the effect of different values of Q/V in KPO, which controls the strength of temporal smoothing. We set three settings: Q/V equals to 1e 6, 1e 4, and 1e 2. Empirically, smaller Q/V enforces stronger smoothing and higher temporal coherence, while larger Q/V makes the filter more responsive to short-term fluctuations. The experimental results, in terms of mean training reward and clip fractions, are shown in Figure 4. Reward mean. From Figure 4, we can see that smaller Q/V values consistently achieve higher rewards. The setting Q/V = 1e 6 yields the best final performance and 8 Online Causal Kalman Filtering for Stable and Effective Policy Optimization"
        },
        {
            "title": "References",
            "content": "Bai, H., Zhou, Y., Li, L. E., Levine, S., and Kumar, A. Digiq: Learning q-value functions for training device-control agents. arXiv preprint arXiv:2502.15760, 2025. Bai, Y., Jones, A., Ndousse, K., Askell, A., Chen, A., DasSarma, N., Drain, D., Fort, S., Ganguli, D., Henighan, T., et al. Training helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862, 2022. Chen, K., Cusumano-Towner, M., Huval, B., Petrenko, A., Hamburger, J., Koltun, V., and Krahenbuhl, P. Reinforcement learning for long-horizon interactive llm agents. ArXiv preprint arXiv:2502.01600, 2025a. Chen, M., Chen, G., Wang, W., and Yang, Y. Seed-grpo: Semantic entropy enhanced grpo for uncertainty-aware policy optimization. arXiv preprint arXiv:2505.12346, 2025b. Cui, G., Zhang, Y., Chen, J., Yuan, L., Wang, Z., Zuo, Y., Li, H., Fan, Y., Chen, H., Chen, W., et al. The entropy mechanism of reinforcement learning for reasoning language models. arXiv preprint arXiv:2505.22617, 2025. Dai, M., Liu, S., and Si, Q. Stable reinforcement learning for efficient reasoning. arXiv preprint arXiv:2505.18086, 2025. DeepSeek-AI. DeepSeek-R1: Incentivizing reasoning capability in LLMs via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. URL https:// arxiv.org/abs/2501.12948. Ethayarajh, K., Xu, W., Muennighoff, N., Jurafsky, D., and Kiela, D. Model alignment as prospect theoretic optimization. In International Conference on Machine Learning, 2024. Hendrycks, D., Burns, C., Kadavath, S., Arora, A., Basart, S., Tang, E., Song, D., and Steinhardt, J. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021. Lin, Z., Lin, M., Xie, Y., and Ji, R. Cppo: Accelerating the training of group relative policy optimization-based reasoning models. arXiv preprint arXiv:2503.22342, 2025. Liu, J., Li, Y., Fu, Y., Wang, J., Liu, Q., and Shen, Y. When speed kills stability: Demystifying rl collapse from the training-inference mismatch. Notion Blog, 2025a. Liu, S.-Y., Dong, X., Lu, X., Diao, S., Belcak, P., Liu, M., Chen, M.-H., Yin, H., Wang, Y.-C. F., Cheng, K.-T., et al. Gdpo: Group reward-decoupled normalization policy optimization for multi-reward rl optimization. arXiv preprint arXiv:2601.05242, 2026. Liu, Z., Chen, C., Li, W., Qi, P., Pang, T., Du, C., Lee, W. S., and Lin, M. Understanding r1-zero-like training: critical perspective. arXiv preprint arXiv:2503.20783, 2025b. Lyu, Z., Huang, J., Deng, Y., Hoi, S., and An, B. Lets revise step-by-step: unified local search framework for code generation with llms. arXiv preprint arXiv:2508.07434, 2025. Learning with OpenAI. https://openai.com/index/ LLMs. learning-to-reason-with-llms/, September 2024. Accessed: 2026-01-15. reason to Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:2773027744, 2022. Feng, L., Xue, Z., Liu, T., and An, B. Group-in-group policy optimization for llm agent training. arXiv preprint arXiv:2505.10978, 2025. Qi, P., Liu, Z., Zhou, X., Pang, T., Du, C., Lee, W. S., and Lin, M. Defeating the training-inference mismatch via fp16. arXiv preprint arXiv:2510.26788, 2025. Gao, C., Zheng, C., Chen, X.-H., Dang, K., Liu, S., Yu, B., Yang, A., Bai, S., Zhou, J., and Lin, J. Soft adaptive policy optimization. arXiv preprint arXiv:2511.20347, 2025. Garg, S., Singh, A., Singh, S., and Chopra, P. Ipo: Your language model is secretly preference classifier. arXiv preprint arXiv:2502.16182, 2025. Grewal, M. S. Kalman filtering. In International encyclopedia of statistical science, pp. 12851289. Springer, 2025. Qian, C., Liu, W., Liu, H., Chen, N., Dang, Y., Li, J., Yang, C., Chen, W., Su, Y., Cong, X., et al. ChatDev: Communicative agents for software development. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1517415186, 2024. Rafailov, R., Sharma, A., Mitchell, E., Manning, C. D., Ermon, S., and Finn, C. Direct preference optimization: Your language model is secretly reward model. Advances in neural information processing systems, 36: 5372853741, 2023. 9 Online Causal Kalman Filtering for Stable and Effective Policy Optimization Sanh, V., Webson, A., Raffel, C., Bach, S. H., Sutawika, L., Alyafeai, Z., Chaffin, A., Stiegler, A., Le Scao, T., Raja, A., et al. Multitask prompted training enables zeroshot task generalization. In International Conference on Learning Representations, 2022. Schulman, J., Levine, S., Abbeel, P., Jordan, M., and Moritz, In International P. Trust region policy optimization. conference on machine learning, pp. 18891897. PMLR, 2015a. Schulman, J., Moritz, P., Levine, S., Jordan, M., and Abbeel, P. High-dimensional continuous control using generalized advantage estimation. arXiv preprint arXiv:1506.02438, 2015b. Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. Shao, Z., Wang, P., Zhu, Q., Xu, R., Song, J., Bi, X., Zhang, H., Zhang, M., Li, Y., Wu, Y., et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Shao, Z., Luo, Y., Lu, C., Ren, Z., Hu, J., Ye, T., Gou, Z., Ma, S., and Zhang, X. Deepseekmath-v2: Towards self-verifiable mathematical reasoning. arXiv preprint arXiv:2511.22570, 2025. Stiennon, N., Ouyang, L., Wu, J., Ziegler, D., Lowe, R., Voss, C., Radford, A., Amodei, D., and Christiano, P. F. Learning to summarize with human feedback. Advances in neural information processing systems, 33:30083021, 2020. Tan, W., Zhang, W., Xu, X., Xia, H., Ding, Z., Li, B., Zhou, B., Yue, J., Jiang, J., Li, Y., et al. Cradle: Empowering foundation agents towards general computer control. In International Conference on Machine Learning, 2025. Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et al. Llama 2: Open foundation and finetuned chat models. arXiv preprint arXiv:2307.09288, 2023. Wang, H., Ma, C., Reid, I., and Yaqub, M. Kalman filter enhanced grpo for reinforcement learning-based language model reasoning. arXiv preprint arXiv:2505.07527, 2025a. Wang, J., Liu, R., Lin, L., Hu, W., Li, X., Zhang, F., Zhou, G., and Gai, K. Aspo: Asymmetric importance sampling policy optimization. arXiv preprint arXiv:2510.06062, 2025b. Wei, J., Bosma, M., Zhao, V. Y., Guu, K., Yu, A. W., Lester, B., Du, N., Dai, A. M., and Le, Q. V. Finetuned language models are zero-shot learners. In International Conference on Learning Representations, 2022. Wu, S., Xie, J., Zhang, Y., Chen, A., Zhang, K., Su, Y., and Xiao, Y. Arm: Adaptive reasoning model. arXiv preprint arXiv:2505.20258, 2025. Xiao, C., Zhang, M., and Cao, Y. Bnpo: Beta normalization policy optimization. arXiv preprint arXiv:2506.02864, 2025. Xiong, W., Yao, J., Xu, Y., Pang, B., Wang, L., Sahoo, D., Li, J., Jiang, N., Zhang, T., Xiong, C., et al. minimalist approach to llm reasoning: from rejection sampling to reinforce. arXiv preprint arXiv:2504.11343, 2025. Yang, A., Li, A., Yang, B., Zhang, B., Hui, B., Zheng, B., Yu, B., Gao, C., Huang, C., Lv, C., et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025a. Yang, S., Dou, C., Guo, P., Lu, K., Ju, Q., Deng, F., and Xin, R. Dcpo: Dynamic clipping policy optimization. arXiv preprint arXiv:2509.02333, 2025b. Yang, Z., Luo, X., Wang, Z., Han, D., He, Z., Li, D., and Xu, Y. Do not let low-probability tokens over-dominate in rl for llms. arXiv preprint arXiv:2505.12929, 2025c. Yao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan, K. R., and Cao, Y. ReAct: Synergizing reasoning In The Eleventh Inand acting in language models. ternational Conference on Learning Representations, 2023. URL https://openreview.net/forum? id=WE_vluYUL-X. Yu, Q., Zhang, Z., Zhu, R., Yuan, Y., Zuo, X., Yue, Y., Fan, T., Liu, G., Liu, L., Liu, X., et al. DAPO: An open-source LLM reinforcement learning system at scale. arXiv preprint arXiv:2503.14476, 2025. Zeng, A., Liu, M., Lu, R., Wang, B., Liu, X., Dong, Y., and Tang, J. AgentTuning: Enabling generalized agent abilities for LLMs. ArXiv preprint arXiv:2310.12823, 2023. Zhai, S., Bai, H., Lin, Z., Pan, J., Tong, P., Zhou, Y., Suhr, A., Xie, S., LeCun, Y., Ma, Y., et al. Fine-tuning large vision-language models as decision-making agents via reinforcement learning. In Proceedings of the Advances in Neural Information Processing Systems, volume 37, pp. 110935110971, 2024. Zhang, K., Li, J., Li, G., Shi, X., and Jin, Z. CodeAgent: Enhancing code generation with tool-integrated agent systems for real-world repo-level coding challenges. In Proceedings of the Annual Meeting of the Association for Computational Linguistics, pp. 1364313658, 2024. 10 Online Causal Kalman Filtering for Stable and Effective Policy Optimization Zhang, K., Hong, Y., Bao, J., Jiang, H., Song, Y., Hong, D., and Xiong, H. Gvpo: Group variance policy optimization for large language model post-training. arXiv preprint arXiv:2504.19599, 2025a. Zhang, Q., Wu, H., Zhang, C., Zhao, P., and Bian, Y. Right question is already half the answer: Fully unsupervised llm reasoning incentivization. arXiv preprint arXiv:2504.05812, 2025b. Zhang, W., Zeng, L., Xiao, Y., Li, Y., Cui, C., Zhao, Y., Hu, R., Liu, Y., Zhou, Y., and An, B. Agentorchestra: Orchestrating hierarchical multi-agent intelligence with the tool-environment-agent (tea) protocol. arXiv preprint arXiv:2506.12508, 2025c. Zhao, Y., Liu, Y., Liu, J., Chen, J., Wu, X., Hao, Y., Lv, T., Huang, S., Cui, L., Ye, Q., et al. Geometric-mean policy optimization. arXiv preprint arXiv:2507.20673, 2025. Zheng, C., Dang, K., Yu, B., Li, M., Jiang, H., Lin, J., Liu, Y., Lin, H., Wu, C., Hu, F., et al. Stabilizing reinforcement learning with llms: Formulation and practices. arXiv preprint arXiv:2512.01374, 2025a. Zheng, C., Liu, S., Li, M., Chen, X.-H., Yu, B., Gao, C., Dang, K., Liu, Y., Men, R., Yang, A., et al. Group sequence policy optimization. arXiv preprint arXiv:2507.18071, 2025b. Zheng, R., Dou, S., Gao, S., Hua, Y., Shen, W., Wang, B., Liu, Y., Jin, S., Liu, Q., Zhou, Y., et al. Secrets of rlhf in large language models part i: Ppo. arXiv preprint arXiv:2307.04964, 2023. Zhou, A., Yan, K., Shlapentokh-Rothman, M., Wang, H., and Wang, Y.-X. Language agent tree search unifies reasoning acting and planning in language models. arXiv preprint arXiv:2310.04406, 2023. 11 Online Causal Kalman Filtering for Stable and Effective Policy Optimization Algorithm 1 KPO: Online Causal Kalman Filtering for Stable and Effective Policy Optimization Require: Behavior policy πold, updated policy πθ, initial posterior, process/measurement noises (Q, R), trajectory (s1:T , a1:T ) Ensure: The policy loss JKPO 1: Raw log-ratio computation: 2: zt log πθ(at st) log πold(at st) {token/action-level log IS ratio} 3: Kalman filtering in log space. {1D local-level model: ρt = ρt1 + ηt, zt = ρt + ϵt} 4: for = 1 to do Predict: 5: ˆρtt1 ˆρt1t1 6: Ptt1 Pt1t1 + {Eq. (6)} 7: Innovation and gain: 8: δt zt ˆρtt1 9: St Ptt1 + 10: 11: Kt Ptt1/St Update: 12: ˆρtt ˆρtt1 + Ktδt 13: Ptt (1 Kt) Ptt1 {Eq. (9)} 14: 15: end for 16: Back to ratio space. 17: for = 1 to do (cid:101)rt exp(cid:0)ˆρtt 18: 19: end for 20: Objective. Compute the policy loss JKPO using {(cid:101)rt}T t=1 according to Eq. (10). (cid:1) {filtered IS ratio} A. Algorithm A.1. The pseudo-code of KPO Algorithm 1 summarizes the training procedure of KPO at the trajectory level. The method first computes token/action-wise log importance ratios zt = log πθ(at st) log πold(at st), which serve as noisy observations of an underlying, smoothly varying log-ratio process. one-dimensional Kalman filter is then applied in log space, using the process noise and measurement noise to control the strength of temporal smoothing. Specifically, the prediction step propagates the posterior mean and variance of the latent log-ratio, while the update step corrects the prediction using the innovation δt and the Kalman gain Kt. The filtered estimates are finally mapped back to ratio space via exponentiation, yielding (cid:101)rt = exp(ˆρtt), which is subsequently used to evaluate the KPO objective in Eq. (10). This design preserves the standard importance-ratio form while reducing high-frequency variability that otherwise amplifies gradient noise in policy optimization. B. The experimental setting B.1. The experimental setting for Figure 1 To explore off-policy patterns, we run GRPO on Qwen3-4B and record token-level importance sampling (IS) ratios for 960 samples at the 100-th training step, following the original GRPO setup. We use window size of 50 and maximum sequence length of 4096 tokens. The training batch size is 32, and the mini-batch size is 8, resulting in four mini-batches per batch. Notably, only the first mini-batch is on-policy, and the remaining three mini-batches are off-policy (we only use off-policy samples). Other experimental setting is same to the main experiment setting shown in B.2. Window-wise off-policy frequency analysis. Using non-overlapping windows of length 50 (up to 80 windows per sample), we first restrict each sequence to valid positions with padding mask = True. The valid prefix of length nw 50 is reshaped into [nw, 50]. For each window, we count up = (cid:80) I[log ratio > 0] and down = (cid:80) I[log ratio < 0], and define the off-policy frequency as (up + down)/50. Finally, for each window index, we aggregate frequencies across all samples that contain that window using nanmean and nanvar to produce mean/variance curves for plotting. 12 Online Causal Kalman Filtering for Stable and Effective Policy Optimization Run-length computation. Run-lengths are computed per sample on valid tokens only (padding mask = True). Each valid position is first assigned type ct {up, down, on} (by the sign of log ratiot or predefined ratio bins). For each type, the valid sequence is scanned left-to-right to record lengths of maximal contiguous runs: counter ℓ is incremented while ct remains unchanged, and when the type differs the current ℓ (if ℓ > 0) is appended and then reset; the final run is appended if nonzero. Switch-frequency computation. Switch frequency is computed in as follows: (i) restrict to valid tokens with padding mask = True and map each position to type st {1, 0, 1} (down, on, up); (ii) partition the valid type sequence into non-overlapping windows of length 50; (iii) for each window of length L, count adjacent type changes (cid:80)L1 I[wj = wj1] and normalize by 1 to obtain window-level switch rate; (iv) report the sample-level switch j=1 frequency as the mean over windows, falling back to the full valid sequence if fewer than 50 tokens are available; (v) if the valid length is 1, the switch frequency is set to 0. B.2. The details of experiments For the Math task, uniform hyperparameters are employed across all methods. The maximum response lengths are set to 4096 tokens, respectively. The actor learning rate is fixed at 1 106. We employ group-based rollouts with group size of 8. binary rule-based reward function is used (1 for success, 0 for failure). The batch sizes for training and evaluation are 32 and 64, respectively. The loss aggregation mode issequence-mean-token-mean. During evaluation, we use nucleus sampling with top = 1.0 and temperature of 1.0. For comparing methods, we use the suggested parameters in their papers. Specifically, the clipping parameter ϵ = 0.2 in GRPO and ϵ = 0, 4 in GMPO. The clipping parameter ϵ = 0.0003 and ϵ+ = 0.0004 in GSPO. All experiments are conducted on NVIDIA H100 GPUs. B.3. The meanings of training metrics Policy Gradient Loss: Primary optimization signal. Smooth, gradual decrease suggests stable learning; sharp spikes or large magnitudes indicate overly aggressive updates and potential instability. Policy Gradient Clip Fraction: Fraction of updates affected by clipping. Moderate levels suggest controlled steps; persistently high fractions imply frequent extreme updates and unstable optimization dynamics. Entropy: Tracks policy stochasticity. Adequate entropy supports exploration and prevents premature collapse; rapid entropy decay indicates mode collapse or over-confident generation, while excessively high entropy can slow convergence. Mean Reward: Average return per rollout. smooth upward trend reflects effective learning; abrupt drops typically signal instability or reward hacking. C. Experimental analysis C.1. Token proportion Token-type assignment is based on the IS ratio. Before Kalman filtering, we treat tokens with an on-policy ratio = 1 as on-policy, tokens with > 1 as up-policy, and tokens with < 1 as down-policy. After Kalman filtering, the IS ratios are smoothed, so tokens with exactly = 1 become rare. We therefore define token as on-policy if [1 ϵ, 1 + ϵ+], with ϵ = 0.0003 and ϵ+ = 0.0004. Tokens with > 1 + ϵ+ and < 1 ϵ are classified as up-policy and down-policy, respectively. As shown in Table 2, Kalman filtering reduces the measured fraction of on-policy tokens and increases the fraction of off-policy tokens. This shift should not be viewed as undesirable for policy optimization. It largely reflects the tighter on-policy band after smoothing, while the resulting off-policy ratios are no longer dominated by noisy spikes but become structurally coherent. Consistently, the token run-length and switching-frequency statistics indicate that local off-policy deviations remain smooth and stable, suggesting that the induced token-type distribution is well-behaved. C.2. The frequency-domain analysis Low-frequency ratio. Beyond time-domain characterizations of temporal structure, we study the IS ratio sequence in the frequency domain. Let {rt}T 1 t=0 denote the token-level ratio sequence (in our implementation, rt may refer to the log-ratio), and let = 1 xt = rt be the mean-centered signal, so that the DC component does not dominate t=0 rt, (cid:80)T 1 13 Online Causal Kalman Filtering for Stable and Effective Policy Optimization the energy accounting. Define the discrete Fourier transform (DFT) Xk = (cid:80)T 1 = 0, 1, . . . , 1, and the associated spectral power (periodogram) Pk = Xk2. Given cutoff index kc {0, . . . , /2}, we define the low-frequency band as the union of symmetric bins t=0 xt ej 2π kt, KLF = {0, 1, . . . , kc} {T kc, . . . , 1}, which ensures that the low-frequency energy is counted consistently for real-valued sequences. We then define the low-frequency ratio (LFR) by LFR(kc) = (cid:80) kKLF 1 (cid:80) k=0 Pk . Pk By construction, LFR(kc) [0, 1] quantifies the fraction of total signal energy residing below the cutoff frequency ωc = 2πkc/T : small values indicate energy concentrated in high-frequency components (rapid token-to-token fluctuations), whereas large values indicate predominantly low-frequency variation over longer token spans. In our measurements, the unfiltered ratio sequence yields low LFR of 0.12, consistent with spectrum dominated by high-frequency energy. After applying Kalman filtering, the LFR increases to 0.98, implying that the filtered sequence is almost entirely explained by low-frequency components. This behavior is consistent with the role of the Kalman filter as temporal denoiser: it suppresses fast oscillations while preserving slowly varying structure."
        }
    ],
    "affiliations": [
        "Nanyang Technological University, Singapore",
        "Southeast University, China"
    ]
}