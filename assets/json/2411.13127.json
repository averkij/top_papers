{
    "paper_title": "Adapting Vision Foundation Models for Robust Cloud Segmentation in Remote Sensing Images",
    "authors": [
        "Xuechao Zou",
        "Shun Zhang",
        "Kai Li",
        "Shiying Wang",
        "Junliang Xing",
        "Lei Jin",
        "Congyan Lang",
        "Pin Tao"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Cloud segmentation is a critical challenge in remote sensing image interpretation, as its accuracy directly impacts the effectiveness of subsequent data processing and analysis. Recently, vision foundation models (VFM) have demonstrated powerful generalization capabilities across various visual tasks. In this paper, we present a parameter-efficient adaptive approach, termed Cloud-Adapter, designed to enhance the accuracy and robustness of cloud segmentation. Our method leverages a VFM pretrained on general domain data, which remains frozen, eliminating the need for additional training. Cloud-Adapter incorporates a lightweight spatial perception module that initially utilizes a convolutional neural network (ConvNet) to extract dense spatial representations. These multi-scale features are then aggregated and serve as contextual inputs to an adapting module, which modulates the frozen transformer layers within the VFM. Experimental results demonstrate that the Cloud-Adapter approach, utilizing only 0.6% of the trainable parameters of the frozen backbone, achieves substantial performance gains. Cloud-Adapter consistently attains state-of-the-art (SOTA) performance across a wide variety of cloud segmentation datasets from multiple satellite sources, sensor series, data processing levels, land cover scenarios, and annotation granularities. We have released the source code and pretrained models at https://github.com/XavierJiezou/Cloud-Adapter to support further research."
        },
        {
            "title": "Start",
            "content": "Xuechao Zou, Student Member, IEEE, Shun Zhang, Kai Li, Student Member, IEEE, Shiying Wang, Junliang Xing, Senior Member, IEEE, Lei Jin, Congyan Lang, Member, IEEE, and Pin Tao, Member, IEEE 1 4 2 0 2 0 2 ] . [ 1 7 2 1 3 1 . 1 1 4 2 : r AbstractCloud segmentation is critical challenge in remote sensing image interpretation, as its accuracy directly impacts the effectiveness of subsequent data processing and analysis. Recently, vision foundation models (VFM) have demonstrated powerful generalization capabilities across various visual tasks. In this paper, we present parameter-efficient adaptive approach, termed Cloud-Adapter, designed to enhance the accuracy and robustness of cloud segmentation. Our method leverages VFM pretrained on general domain data, which remains frozen, training. Cloud-Adapter eliminating the need for additional incorporates lightweight spatial perception module that initially utilizes convolutional neural network (ConvNet) to extract dense spatial representations. These multi-scale features are then aggregated and serve as contextual inputs to an adapting module, which modulates the frozen transformer layers within the VFM. Experimental results demonstrate that the Cloud-Adapter approach, utilizing only 0.6% of the trainable parameters of the frozen backbone, achieves substantial performance gains. CloudAdapter consistently attains state-of-the-art (SOTA) performance across wide variety of cloud segmentation datasets from multiple satellite sources, sensor series, data processing levels, land cover scenarios, and annotation granularities. We have released the source code and pretrained models at https://github. com/XavierJiezou/Cloud-Adapter to support further research. Index TermsCloud segmentation, vision foundation models, domain adaptation, fine-tuning, remote sensing image processing. I. INTRODUCTION LOUD segmentation, also known as cloud detection, plays critical role in the interpretation of remote sensing images, with wide applications in climate monitoring [1], [2], environmental protection, disaster warning, and land use [3]. The variations and characteristics of clouds significantly influence the quality of remote sensing images and subsequent analysis results. Therefore, accurate cloud segmentation is essential for extracting meaningful geospatial information and Xuechao Zou, Shun Zhang and Kai Li have contributed equally to this work. Corresponding authors: Congyan Lang and Pin Tao. Xuechao Zou and Congyan Lang are with the Key Lab of Big Data & Artificial Intelligence in Transportation (Ministry of Education), School of Computer Science & Technology, Beijing Jiaotong University, Beijing, China (e-mail: xuechaozou@foxmail.com; cylang@bjtu.edu.cn). Kai Li, Junliang Xing, and Pin Tao are with the Department of Computer Science and Technology, Tsinghua University, Beijing, China. Pin Tao is also with the Key Laboratory of Pervasive Computing, Ministry of Education (e-mail: jlxing@tsinghua.edu.cn; tsinghua.kaili@gmail.com; taopin@tsinghua.edu.cn). Shun Zhang and Shiying Wang are with the School of Computer Technology and Application, Qinghai University, Xining, China (e-mail: xiaoshun3238@gmail.com; wangshiying.qhu@foxmail.com). Lei Jin is with the School of Electronic Engineering, Beijing University of Posts and Telecommunications, Beijing, China (e-mail: jinlei@bupt.edu.cn). Fig. 1: Comparison between (a) the Previous Method and (b) our Cloud-Adapter approach. Unlike (a), where the entire network (e.g., typical encoder-decoder architecture) is fully trainable, resulting in large number of parameters and an increased risk of overfitting, (b) our Cloud-Adapter approach leverages frozen vision foundation model (VFM) combined with lightweight, trainable adapter. This design preserves generalization and adaptability, enabling efficient learning for cloud segmentation tasks. The frozen VFM extracts generic visual features, while the parameter-efficient adapter facilitates effective transfer learning across remote sensing scenes. ensuring reliable data processing. However, due to the diversity of cloud shapes, textures, and sizes, coupled with the complexity of imaging conditions, traditional cloud segmentation methods [4][7] often fail to achieve stable high-accuracy results in dynamic remote sensing images. Consequently, developing more robust and adaptable cloud segmentation method has become key research focus in the field of remote sensing. With the rapid development of deep learning [8], convolutional neural network (ConvNet [9][11])-based cloud segmentation methods [12][18] have made remarkable progress, particularly due to their ability to automatically extract highlevel features that enhance the accuracy of cloud shape, edge detection, and texture analysis. For instance, MFCNN [14] combines multi-scale convolutional features to enable finegrained cloud detection, while DABNet [18] introduces the deformable context feature pyramid module to improve the adaptive modeling capability of multi-scale features for highaccuracy cloud detection. Despite these advancements, many methods still face challenges, including strong dependence on training data, high model complexity, and limited adaptability to diverse remote sensing scenes. Meanwhile, vision foundation models (VFM) [19][23], which learn task-agnostic representations from large-scale datasets [24], have demonstrated impressive performance across various applications, such as image classification [25], object detection [26], and segmentation [27][29]. Transformer architectures like ViT [19] and Swin Transformer [20] have become dominant in the VFM landscape, showing superior performance. However, the potential of VFMs for cloud segmentation remains largely unexplored. Existing cloud segmentation methods [30], [31] primarily rely on complex neural network architectures, most of which require extensive retraining or fine-tuning on large labeled datasets. Although these methods achieve good performance, they suffer from several limitations. First, they are computationally expensive and require large amount of labeled data, which is often impractical for real-world applications. Second, traditional models are prone to overfitting, particularly when dealing with small datasets or high-resolution remote sensing images. Third, most existing methods [16], [17], [30][34] have barely leveraged the power of VFM for cloud segmentation, missing the opportunity to exploit their robust transfer learning capabilities with minimal additional parameters. To address these challenges, we propose novel cloud segmentation method based on vision foundation models, named Cloud-Adapter (see Fig. 1). This approach freezes the pretrained VFM backbone and incorporates lightweight adaptive module, requiring minimal additional parameters to enhance cloud segmentation accuracy and robustness. Specifically, we design spatial perception module that employs convolutional neural networks (ConvNets) to extract dense spatial features and aggregate them into multi-scale contextual information. This information is then passed into the adaptive module, which modulates the frozen transformer layers of the VFM to efficiently perform cloud segmentation. Experimental results demonstrate that Cloud-Adapter achieves significant performance improvements with only 0.6% of the trainable parameters of the backbone across various popular datasets. The contributions of this work are summarized as follows: 1) We propose Cloud-Adapter, simple but powerful parameter-efficient fine-tuning method for cloud segmentation that freezes the backbone of VFMs and finetunes only small fraction of the frozen parameters. 2) We introduce two lightweight components: the spatial perception module and the adapting module, which modulate the features of the VFM, enabling adaptation from general domain to remote sensing domain. 3) We demonstrate that our method consistently achieves state-of-the-art performance on four datasets (six subsets) including multiple satellite sources, sensor types, and land cover scenarios, highlighting its robustness towards remote sensing cloud segmentation tasks. 2 II. RELATED WORK A. Cloud Segmentation Early cloud segmentation methods primarily relied on the spectral and statistical features of images, using techniques like thresholding, multispectral fusion, and edge detection to distinguish clouds from other surface elements. For instance, CFMask [6], [7] employs decision trees to label pixels and projects estimated cloud heights onto the ground to generate cloud shadow mask. HOT [4], [5] introduces transformation optimized for detecting cloud and haze distributions in Landsat images, amplifying haze variations via spectral sensitivity. However, these traditional methods are highly sensitive to cloud shapes, lighting conditions, and atmospheric environments, resulting in suboptimal performance in complex remote sensing scenarios. With advancements in deep learning, ConvNet-based cloud segmentation methods have gained significant attention due to their ability to automatically extract high-level features, enhancing the accuracy of cloud shape, texture, and edge detection. Examples of innovative model architectures include MSCFF [12], which combines multiscale convolutional features to facilitate cloud detection. DBNet [31] employs hybrid model integrating Transformers and ConvNets to capture semantic and spatial details, thereby reducing false detections and omissions in cloud segmentation. HRCloudNet [34] adopts hierarchical integration approach to preserve cloud texture in high-resolution images, while MCDNet [30] achieves effective thin cloud segmentation through multi-scale feature fusion and auxiliary cloud removal. For models designed for specific scenarios and training needs, CDNetv1 [16] leverages feature pyramid and boundary refinement block to extract cloud masks from lowresolution thumbnails. CDNetv2 [17] improves performance in mixed cloud-snow scenes using adaptive feature fusion and high-level semantic guidance. SCNN [32] applies shallow ConvNet with filters of varying sizes to minimize model training costs. Additionally, TransMCD [35] uses pseudo-labels derived from spectral features, enabling weakly supervised cloud detection and enhancing model generalization. Despite these advancements, achieving robust segmentation performance across diverse remote sensing images remains significant challenge. Furthermore, most methods continue to rely on traditional network architectures, such as UNet [36], ResNet [37], FPN [38], and DeepLab [39], with limited exploration of vision foundation models for cloud segmentation. B. Vision Foundation Models Learning task-agnostic pre-trained representations [40], [41] language prohas become standard practice in natural cessing. Inspired by this paradigm, substantial progress has been achieved in exploring vision foundation models (VFMs). Typically, these models are trained on large-scale datasets to learn general-purpose representations, achieving remarkable performance across diverse applications, including image classification, object detection, and segmentation. Transformer architectures [19], [20] dominate the landscape of vision foundation models, demonstrating superior performance. CLIP [42] aligns text and image modalities in latent space through 3 Fig. 2: Detailed network architecture of the proposed Cloud-Adapter method, consisting of the spatial perception and adapting modules. The spatial perception module uses ConvNet blocks to extract dense spatial features, which are aggregated into multi-scale context and fed to the adapting module. The adapting module modulates the frozen transformer layers in the VFM. contrastive learning. MAE [21] learns robust representations via self-supervised masked modeling. SAM [22] incorporates interactive prompts to capture general concepts of objects, while DINOv2 [23] employs self-distillation to learn generic semantic features. These models effectively capture complex visual patterns and exhibit strong capabilities for few-shot and even zero-shot transfer, allowing them to adapt well to new tasks. Given the diversity and complexity of remote sensing images, exploring vision foundation models with strong generalization is essential for achieving robust cloud segmentation. C. Transfer Learning and Domain Adaptation In resource-constrained environments or when dealing with large-scale data, training models from scratch is often impractical and costly. Transfer learning [43] enables adapting models trained on large datasets to specific tasks. This approach typically involves fine-tuning pre-trained models on smaller, domain-specific datasets, allowing the model to retain general knowledge while learning domain-specific characteristics. Domain adaptation [44] further enhances this process by aligning the feature distributions between the source domain (where the model was initially trained) and the target domain (the new application environment), thus maximizing task performance. Adapters exemplify the integration of transfer learning and domain adaptation, offering generally plug-and-play approach across various tasks. Initially introduced in natural language processing (NLP) [45], chapters have rapidly evolved. In NLP tasks, LoRA [46] injects trainable low-rank decomposition matrices into each layer of the Transformer architecture, enabling large language models to adapt to specific domains. In multimodal generation tasks, IP-Adapter [47] is an effective adapter for implementing image prompting functions in pre-trained text-to-image diffusion models. T2IAdapter [48] introduces lightweight adapter to modulate the intensity of internal knowledge and external control signals in text-to-image models. For visual segmentation tasks, ViTAdapter [49] integrates image-related inductive biases into pre-trained ViT models, improving dense prediction performance. SAM-Adapter [50] introduces simple adapter that incorporates domain-specific information or visual prompts into SAM [22], enhancing performance in specific scenarios. Therefore, developing an efficient adapter shows great potential for improving the robustness of cloud segmentation by leveraging the strengths of vision foundation models. III. METHOD In this section, we introduce simple but powerful finetuning method called Cloud-Adapter (as shown in Fig. 2) for cloud segmentation by freezing the vision foundation model (VFM) and incorporating lightweight adapting module. The method consists of two key components: spatial perception and adapting modules. The spatial perception module extracts multi-scale spatial features using multiple ConvNet blocks and then aggregates them into dense context. The adapting modules modulate the frozen VFM layers with the context, enabling effective transfer learning for cloud segmentation. 4 the VFM. The cross-attention mechanism calculates the relationships between aggregated dense features and the features extracted by the VFM, producing the adapted features. The cross-attention computation is expressed by: = Softmax (cid:32) (cid:33) , QKT = V, (2) (3) (4)"
        },
        {
            "title": "Fatt",
            "content": "j = M(O) + Ft j, where Rd, Rd, and Rd are the query, key, and value matrices, respectively. The Softmax operation computes the attention weights S, which are then multiplied with the value matrix to obtain the output O. Finally, after the mapping operation (denoted as M), the adapted features Fatt are obtained with residual connection, where denotes the j-th feature from the frozen VFM transformer layers. The mapping operation is performed via multi-layer perceptron (MLP) with the low-rank [46] design without any activation, aimed to achieve feature projection with minimal parameter cost. The mapping process can be expressed as: M(z) = W2 (W1 z), (5) where W1 Rdr and W2 Rrd are the weight matrices of the fully connected layers, is the input feature, and is the rank for dimensionality reduction. The smaller the value of r, the fewer trainable parameters the adapting module contains. IV. EXPERIMENTS A. Datasets We conducted experiments on four popular cloud segmentation datasets, comprising six subsets, from different satellite sources: CloudSEN12 High [56], L8 Biome [57], GF12MS WHU [35], and HRC WHU [12], as summarized in Table I. Specifically, CloudSEN12 High contains two data processing levels, L1C and L2A, while GF12MS WHU consists of data captured by two satellite sensors, GF1 and GF2. Notably, in typical binary cloud segmentation datasets, thin and thick clouds are grouped into single category, cloud, while cloud shadows are excluded from the annotations. 1) CloudSEN12 High: [56] CloudSEN12 is global dataset designed for semantic understanding of clouds and their shadows in Sentinel-2 imagery. It addresses the longstanding challenge of accurately characterizing clouds and cloud shadows in the earth observation community. This dataset comprises 49,400 image patches, including Sentinel2 Level-1C and Level-2A multispectral data, Sentinel-1 synthetic aperture radar data, auxiliary remote sensing products, and meticulously crafted annotations identifying thick clouds, thin clouds, and cloud shadows. CloudSEN12 stands out due to its diverse annotations, scene variability, broad geographic coverage, and metadata complexity, making it an essential resource for benchmarking and improving cloud detection methodologies. Here, we select its high-quality version with four classes of fine-grained annotations for our experiments. Fig. 3: Schematic diagram of the proposed adapting module. A. Spatial Perception Module The spatial perception module (SPM) extracts multi-scale spatial features from the input remote sensing images. First, the input image goes through stem module for preliminary feature extraction. The stem consists of two depthwise separable convolutions [51]. The first convolution uses 11 kernel to map the input images channels to larger feature space, primarily for channel-wise information fusion. The second convolution uses 3 3 kernel to extract spatial features and capture local spatial information. Let the input image be RCHW , where is the number of channels, and and are the height and width of the image. The output feature map after stem is Fstem RCHW , where is the mapped number of channels, and and are the feature map dimensions, which remain the same as the input image. After the stem, the input features are processed by multiple ConvNet blocks, each of which extracts spatial features at different scales, resulting in series of feature maps Fc RCiHW , where denotes different scales. To aggregate these multi-scale feature maps into unified scale, we use an aggregator, which applies adaptive average pooling to normalize the feature maps to the same size. The aggregator is parameter-free operation, meaning it does not contain any trainable parameters, making it zero-parameter operation. The aggregation of multi-scale features is expressed as: Fagg = (cid:88) i=1 Pavg(Fc , output size = Fc k.shape[2 :]), (1) where Fc are the feature maps at different scales, and Pavg is the adaptive average pooling operation that pools the feature maps to the smallest size of the final feature map Fc k. B. Adapting Module The adapting module, as referred to in Fig. 3, utilizes the cross-attention mechanisms [52][55] to adjust the frozen VFMs Transformer layers. Specifically, it takes the context Fagg from the spatial perception module and interacts with the frozen VFM features, thereby modulating the output of TABLE I: Summary of information for four popular cloud segmentation datasets. - indicates non-existence or not mentioned. Dataset Name Satellite Source Ground Resolution Training Set Validation Set Testing Set Patch Size Classes CloudSEN12 High [56] L8 Biome [57] GF12MS WHU [35] HRC WHU [12] Sentinel 2 Landsat 8 Gaofen 1, 2 Google Earth 10m 30m 8m & 4m 0.5m-15m 8,490 7,931 20,697 120 535 2,643 11,645 - 975 2,643 - 30 512512 512512 256256 256256 4 4 2 2 2) L8 Biome: 3) GF12MS WHU: [57] The L8 Biome dataset is designed to facilitate comparing and validating cloud detection algorithms for operational Landsat data products. This dataset encompasses various scenario types, including Urban, Barren, Forest, Shrubland, Grass/Cropland, Snow/Ice, Wetlands, and Water. Such variety of scene types enables models to learn more diverse features, handle complex surface backgrounds, and enhance the accuracy of cloud removal tasks across different terrains. The dataset includes manually generated cloud masks to support the validation of cloud cover assessment algorithms, enabling precise computation of cloud cover percentages for each scene. These characteristics make the dataset an essential resource for cloud segmentation. Here, we cropped each tile into 512 512 patches and removed images that did not contain any targets of interest. The remaining data were then split into training, validation, and test sets in ratio of 6:2:2. [35] The GFMS WHU dataset consists of 8-meter Gaofen1-MS and 4-meter Gaofen2-MS images. Gaofen1 and Gaofen2 belong to the same satellite series but are equipped with different sensors, offering varying spatial resolutions to enhance the models generalization capability. These images were collected from diverse regions of China between June 2014 and December 2020, improving the datasets ability to support robust model training. To standardize input dimensions, all images were uniformly cropped and padded to size of 256 256 patches. The dataset is split into training set and test set with ratio of 7:3, providing balanced framework for model development and evaluation. [12] HRC WHU is high-resolution cloud cover validation dataset created by the SENDIMAGE Lab at Wuhan University. It comprises 150 high-resolution images acquired from Google Earth with RGB channels and resolutions varying from 0.5 to 15 meters. These images have been expertly digitized with two mask annotations: clear sky and clouds. Following the original paper, We uniform cut to 256 256 patch size, 120 for training, and 30 for testing. 4) HRC WHU: B. Implementation Details To ensure experimental reproducibility, this section provides comprehensive overview of the hyperparameter configurations used during training and the evaluation metrics applied to assess the experimental results. All experiments were conducted on workstation equipped with NVIDIA GeForce RTX 3090 GPUs with 24 GB of memory. The operating system was Ubuntu 22.04, and the neural networks were implemented in PyTorch 2.4.1 with CUDA 11.8. The experiments were performed within the MMSegmentation [58] framework (version 1.2.2), sound semantic segmentation toolbox based on PyTorch. This framework offers an intuitive interface and variety of pre-built models, facilitating streamlined experimentation. All segmentation heads across the models used in this study were based on Mask2Former [59], flexible and efficient architecture tailored for image segmentation tasks. Mask2Formers unified approach to segmentation and detection tasks contributed to consistent and reliable performance across the different models evaluated in these experiments. 1) Training Settings: We employed custom learning rate schedule to optimize model performance. The schedule began with warm-up phase, utilizing linear scheduler that increased the learning rate from 1 106 over the first 1,000 iterations. This was followed by PolyLR scheduler with an eta minimum of 0.0 and power of 0.9, applied from iteration 1,000 to 40,000. The training loop was configured to run for maximum of 40,000 iterations, with validation conducted at intervals of every 4,000 steps. To ensure reproducibility, we set the random seed to 42 and used the AdamW optimizer [60] with an initial learning rate of 1 104 and weight decay of 0.05. The batch size during training was set to 4, and parameters were initialized using the kaiming normal [61]. 2) Evaluation Metrics: To comprehensively evaluate cloud segmentation performance, we adapt several widely recognized metrics: Intersection over Union (IoU), Accuracy (Acc), and Dice Coefficient (Dice), all reported as percentages (%). Furthermore, we provided the proposed methods trainable parameters and their proportion relative to the frozen VFM backbone, denoted as *Params. The IoU quantifies the overlap between the predicted and the ground truth masks: IoUi = Predi GTi Predi GTi = Pi Pi + Pi + Ni , (6) where Pi, Pi, and Ni represent the true positives, false positives, and false negatives for class i, respectively. The Acc represents the pixel-wise classification accuracy: Acci = Pi + Ni Pi + Ni + Pi + Ni , (7) where Ni represents the true negatives for class i. The Dice measures the similarity between the predicted and ground truth masks, and it is equivalent to the F1-score metric: Dicei = 2Predi GTi Predi + GTi = 2T Pi 2T Pi + Pi + Ni . (8) The metrics mAcc, mIoU, and mDice represent the mean into the values computed for each class, offering insight models performance at the overall accuracy (aAcc) evaluates the models classification performance across the entire dataset. However, aAcc often overlooks minority classes, such as cloud shadow and thin cloud, potentially overestimating the models effectiveness. the class level. In contrast, significant discrepancy between aAcc and mAcc may highlight class imbalance, where dominant classes skew results. To address this, we calculate the average of these metrics across all courses for cloud segmentation, ensuring balanced evaluation by considering both per-class accuracy and overall pixel-wise accuracy. These metrics are all positive indicators, with higher values signifying better performance, simplifying interpretation, and facilitating model comparison. C. Ablation Studies and Analysis This section presents series of comprehensive ablation studies to evaluate the effectiveness of key components in our proposed model using the CloudSEN12 High L1C dataset. Specifically, we investigate the impact of (1) different variants of the VFM backbone, (2) the ConvNet architectures employed in the SPM, (3) the dimensions of interactive context, (4) the low-rank MLP in the adapting module, (5) the importance of the aggregator, (6) the interaction frequency within the adapting module, and (7) the presence or absence of each component in the Cloud-Adapter. These studies provide detailed analysis of how each component contributes to cloud segmentation performance, ultimately highlighting the strengths and advantages of our proposed Cloud-Adapter. TABLE II: Cloud segmentation performance comparison of different VFMs backbone with their multiple variants. Backbone Variant mIoU mAcc aAcc mDice *Params (M) SAM [22] DINOv2 [23] Base Large Huge Small Base Large 72.53 73.36 73.70 68.12 71.95 74. 83.29 89.46 83.90 89.84 84.37 89.95 79.37 87.50 83.02 89.22 84.79 90.19 83.22 83.85 84.07 79.84 82.82 84.46 0.92 (1.1%) 1.83 (0.6%) 2.63 (0.4%) 0.77 (3.4%) 0.92 (1.1%) 1.82 (0.6%) 1) Variants of VFMs Backbone: To evaluate the impact of different vision foundation models and their variants on our methods performance, we conducted comprehensive experiments using SAM [22] and DINOv2 [23] as backbones. Table II compares three available model variants of them. We observed that as the model scale increased, both SAM and DINOv2 exhibited consistent performance improvements. Among the SAM variants, the Huge version performed the best, achieving 73.70% mIoU, 84.37% mAcc, 89.95% aAcc, and 84.07% mDice. In contrast, DINOv2 (Large) demonstrated superior performance across all metrics, reaching 74.18% mIoU, 84.79% mAcc, 90.19% aAcc, and 84.46% mDice. More importantly, DINOv2 (Large) was significantly more computationally efficient, requiring only 11 hours and 8 minutes of training time and 14,558M of GPU memory. In comparison, the SAM (Huge) variant took 19 hours and 9 minutes to train, with GPU memory usage of 24,892M, which was substantially higher than DINOv2 (Large). These results indicate that DINOv2 (Large) outperforms SAM (Huge) across all metrics while being significantly more efficient in terms of training time and memory usage. Therefore, DINOv2 (Large) emerges as the optimal backbone for our method, striking an excellent balance between performance and computational efficiency for cloud segmentation. 6 TABLE III: Comparison of cloud segmentation performance and model parameters for different ConvNet architectures. ConvNet Block mIoU mAcc aAcc mDice *Params (M) Transformer-Like [62] Pure-ConvNet [55] 74.08 74. 84.95 84.79 90.09 90.19 84.39 84.46 1.81 (0.6%) 1.82 (0.6%) 2) ConvNet Architectures of SPM: We investigate the impact of different ConvNet architectures on our methods performance, as shown in Table III. Specifically, we compare two types of ConvNet designs: Transformer-Like architecture inspired by [62] and Pure-ConvNet structure from [55]. The Pure-ConvNet [55] architecture achieves slightly better overall performance with 74.18% mIoU, 90.19% aAcc, and 84.46% mDice, while the Transformer-Like [62] design shows marginally higher mAcc at 84.95%. Both architectures maintain similar parameter efficiency, with approximately 1.8M. The comparable performance between these two architectures suggests that both designs are viable options for our method. However, considering the slightly higher performance in most metrics and the simpler structure, we adopt the PureConvNet architecture as our default choice. 3) Dimension of Interactive Context: We conduct experiments to explore the impact of different dimension settings on the interactive context extracted by the SPM. As illustrated in Fig. 4, we evaluate four configurations: 16, 32, 64, and 128. The results show that the models performance does not necessarily improve with larger dimensions. Starting from dimension 16 with mIoU of 73.72%, we observe slight improvement to 73.77% when increasing to dimension 32. The peak performance is achieved at dimension 64 with mIoU of 74.18%. However, further increasing the dimension to 128 leads to slight performance degradation (0.44% dropping). trainable parameters grows steadily with the dimension size. The model requires 1.60M parameters at dimension 16, gradually increasing to 1.65M at dimension 32, 1.82M at dimension 64, and significantly jumping to 2.49M at dimension 128. Based on these observations, we identify dimension 64 as the optimal choice, offering the best trade-off between performance and efficiency. the number of Meanwhile, Fig. 4: Ablation study of different dimension settings on the interactive context extracted by the spatial perception module. This superior performance can be attributed to the strategys ability to effectively combine and utilize information from different feature scales. Based on these results, we adopt the Aggregated feature output strategy in our final model design, as it provides the most effective use of dense multi-scale feature information without introducing additional parameters. TABLE V: Comparison of performance at different interaction frequencies (N ) between the Adapter and Transformer layers. 4 8 12 24 mIoU mAcc 73.79 73.76 73.75 74.18 84.22 84.46 84.46 84.79 aAcc 90.10 89.99 89.99 90.19 mDice *Params (M) 84.17 84.15 84.13 84.46 0.47 (0.2%) 0.74 (0.2%) 1.02 (0.3%) 1.82 (0.6%) 6) Interaction Frequency of Adapting Module: In this ablation study, we explore the effect of varying the interaction frequency (N ) between the Adapter and Transformer layers on the models performance. The parameter indicates how frequently the Adapter interacts with the Transformer layers, with larger values corresponding to more frequent interactions. Table presents the results of this experiment, evaluating different values of : 4, 8, 12, and 24. Specifically, when is set to 4, the Adapter interacts with the Transformer layers at indices [0, 6, 12, 18]. The 8 and 12 represent interactions at every 3rd and 2nd layer starting from index 0, respectively. The results indicate that the optimal performance is achieved when is set to 24, where the model attains 74.18% mIoU, 84.79% mAcc, 90.19% aAcc, and 84.46% mDice. This configuration outperforms all others, particularly in terms of mIoU and mAcc. Notably, the increase in interaction frequency from 4 to 24 leads to improvements in the models ability to capture complex spatial dependencies across layers, significantly boosting the performance metrics. In contrast, when the interaction frequency is lower (N = 4, 8, or 12), performance remains relatively stable but does not reach the peak achieved by the 24-interaction configuration. Furthermore, while the number of parameters increases with higher values of , the increase is minimal (only 0.2-0.6% in parameters), suggesting that the additional interactions do not incur significant computational overhead, thanks to the parameter-efficient LoRA-MLP design. These results suggest that allowing the Adapter to interact with more Transformer layers improves model performance, with the 24-interaction configuration representing sweet spot for this balance. TABLE VI: Study of each component in the Cloud-Adapter. Spatial Perception Module Stem ConvNet Block Aggregator Adapting Module mIoU *Params (M) 73.10 73.12 73.20 73.37 74. 0.00 3e-4 0.20 0.20 1.82 7) Overall Components in the Cloud-Adapter: To evaluate the contribution of each component in our model, we conducted an ablation study by systematically removing or modifying specific modules. The results are presented in Table Fig. 5: Study of the low-rank MLP in the adapting module. 4) Low-Rank MLP of Adapting Module: We investigate the influence of different rank settings on our models performance and parameter efficiency. The rank dimension controls the size of the low-rank matrices used to approximate the weight updates in the MLP of the adapting module, which helps to reduce the number of trainable parameters. As shown in Figure 5, we examine four rank settings: 4, 8, 16, and 32. The experimental results reveal an interesting relationship between rank size and model performance. With rank of 4, the model achieves mIoU of 73.84% while requiring only 0.61M trainable parameters. Increasing the rank to 8 improves performance to 74.03% with 1.02M parameters. The peak performance is reached at rank 16 with mIoU of 74.18%, which requires 1.82M parameters. Further increasing the rank to 32 leads to slight performance degradation. Based on these observations, we choose rank 16 as our optimal setting. This finding aligns with LoRAs design principle [46] of achieving efficient adaptation through low-rank approximation. TABLE IV: Cloud segmentation performance comparison of context output strategies for the spatial perception module. Feature Type mIoU mAcc aAcc mDice *Params (M) Single-Scale Multi-Scale Aggregated 74.05 74.10 74. 84.45 84.70 84.79 90.19 90.15 90.19 84.34 84.40 84.46 1.82 (0.6%) 1.82 (0.6%) 1.82 (0.6%) 5) Importance of the Aggregator: We explore three context output strategies for our SPM: Single-Scale, Multi-Scale, and Aggregated approaches. Table IV presents the comparative results of these strategies while maintaining the same parameter count of 1.82M (0.6% of total parameters). The Single-Scale strategy, which directly utilizes the final feature output from the last ConvNet block, achieves baseline performance with 74.05% mIoU, 84.45% mAcc, and 84.34% mDice. The Multi-Scale approach, leveraging multiple scale features without fusion, shows slight improvements with 74.10% mIoU, 84.70% mAcc, and 84.40% mDice. The Aggregated strategy, which fuses multiple scale features into single output with parameter-free operation, demonstrates the best overall performance. It achieves 74.18% mIoU, 84.79% mAcc, and 84.46% mDice, while maintaining the same aAcc (90.19%) as the Single-Scale approach. VI, showing the performance in terms of mIoU and parameter count for different configurations. Notably, in configurations without the adapting module, the features output by the spatial perception module are concatenated with the features from the VFM backbone before being passed to the head for prediction. In the baseline model, without any components from the spatial perception module or adapting module, the features from the VFM backbone were directly passed to the head for segmentation prediction. This configuration achieved mIoU of 73.10%. When the stem module was added, the mIoU increased slightly to 73.12%, with negligible parameter increase (3 104M). Including the stem module helps in early spatial feature projection and extraction, providing the model with rudimentary spatial awareness information. Further improvement was seen when the ConvNet Block and the stem module were added to the model. This resulted in mIoU of 73.20% and added 0.20M parameters. The ConvNet Block enhances the models ability to extract more complex spatial features, contributing to slight performance boost. When the aggregator module was incorporated, the mIoU rose to 73.37%, with no additional parameters, indicating that the aggregator effectively combines multi-scale features to improve the models overall cloud segmentation performance. Finally, with the inclusion of the adapting module, the full model achieved the highest mIoU of 74.18% and total of 1.82M parameters. The adapting module enables fine-tuned integration of the spatial and backbone features, significantly enhancing the models performance despite increased parameters. This demonstrates the crucial role of the adapting module in optimizing the feature fusion process. In conclusion, the ablation study confirms that each component contributes incrementally to the models performance. The concatenation of spatial and backbone features, followed by successive refinement and adaptation through the added modules, substantially improved the segmentation performance in special domains. D. Comparison with State-of-the-Art Methods We compare the performance of our proposed Cloud-Adapter with several state-of-the-art cloud segmentation methods across different datasets: HRC WHU, GF12MS WHU GF1, GF12MS WHU GF2, CloudSEN12 High L1C, and CloudSEN12 High L2A. These datasets include both binary (HRC WHU, GF1, GF2) and multi-class (L1C, L2A) cloud segmentation. The performance comparison, as shown in Tables VII, VIII, IX, X, and XI, highlights Cloud-Adapters robustness in both binary and multi-class cloud segmentation. 1) Binary-Class Cloud Segmentation: a) Quantitative Comparison: For binary-class cloud segmentation datasets, Cloud-Adapter consistently demonstrates superior performance across all evaluation metrics (aAcc, mIoU, mAcc, and mDice). On the HRC WHU dataset (Table VII), Cloud-Adapter achieves the highest scores with an aAcc of 94.50%, mIoU of 89.05%, mAcc of 93.74%, and mDice of 94.19%, significantly outperforming the secondbest method, HRCloudNet, which only achieves mIoU of 83.44%. On the GF12MS WHU GF1 dataset (Table VIII), Cloud-Adapter attains an aAcc of 98.92%, mIoU of 92.55%, 8 TABLE VII: Cloud segmentation performance comparison of different methods on the HRC WHU dataset. Method MCDNet [30] SCNN [32] KappaMask [33] CDNetv2 [17] DBNet [31] CDNetv1 [16] RSAM-Seg [63] UNetMobv2 [56] HRCloudNet [34] Cloud-Adapter (Ours) aAcc 75.14 74.51 84.73 89.71 90.11 89.88 92.07 92.13 92.93 94.50 mIoU mAcc mDice 53.50 57.22 67.48 76.75 77.78 77.79 80.90 79.91 83.44 89.05 68.91 81.27 80.30 87.46 88.80 89.93 88.72 85.61 92.39 93. 67.96 72.31 79.74 86.46 87.17 87.20 89.16 88.45 90.79 94.19 TABLE VIII: Cloud segmentation performance comparison of different methods on the GF12MS WHU GF1 dataset. Method SCNN [32] CDNetv1 [16] CDNetv2 [17] MCDNet [30] RSAM-Seg [63] UNetMobv2 [56] DBNet [31] HRCloudNet [34] KappaMask [33] Cloud-Adapter (Ours) aAcc 97.18 96.81 97.55 97.55 98.36 98.82 98.73 98.80 98.91 98. mIoU mAcc mDice 81.68 81.82 84.93 85.16 89.26 91.71 91.36 91.86 92.42 92.55 87.21 92.75 92.96 93.97 94.70 93.99 95.19 95.82 95.05 95.99 89.13 89.27 91.36 91.51 94.09 95.53 95.33 95.62 95.94 96. TABLE IX: Cloud segmentation performance comparison of different methods on the GF12MS WHU GF2 dataset. Method KappaMask [33] HRCloudNet [34] SCNN [32] CDNetv1 [16] MCDNet [30] DBNet [31] CDNetv2 [17] UNetMobv2 [56] Cloud-Adapter (Ours) aAcc 90.30 91.46 91.99 92.42 92.30 92.61 92.63 93.22 94.04 mIoU mAcc mDice 72.00 75.57 76.99 78.20 78.36 78.68 78.84 80.44 83.02 77.64 80.95 82.06 83.07 83.95 83.39 83.67 84.86 87.54 82.57 85.29 86.32 87.17 87.31 87.50 87.61 88.70 90.40 mAcc of 95.99%, and mDice of 96.02%, surpassing KappaMask, which achieved slightly lower mIoU of 92.42%. On the GF12MS WHU GF2 dataset (Table IX), Cloud-Adapter maintains its leading performance with an aAcc of 94.04%, mIoU of 83.02%, mAcc of 87.54%, and mDice of 90.40%, showing its robustness in handling challenging satellite data. b) Visualization Comparison: Fig. 6 compares the visualized segmentation results for three binary-class datasets. Cloud-Adapter stands out for its ability to accurately capture cloud boundaries and reduce over-segmentation issues, which are common in other methods like DBNet and HRCloudNet. The results clearly illustrate its superiority in segmenting clear sky and cloud regions, even under complex conditions. 2) Multi-Class Cloud Segmentation: a) Quantitative Comparison: Cloud-Adapter achieves state-of-the-art performance on multi-class cloud segmentation datasets, which include four categories: Clear Sky (CRS), Thin Clouds (TNC), Thick Clouds (TKC), and Cloud Shadows (CDS). On the CloudSEN12 High L1C dataset (Table X), Cloud-Adapter achieves mIoU of 74.18%, with 9 Fig. 6: Comparison of visualized segmentation results of different models on the coarse-grained cloud segmentation dataset."
        },
        {
            "title": "Thick Cloud",
            "content": "TABLE X: Cloud segmentation performance comparison of different methods on the CloudSEN12 High L1C dataset. Method IoU CRS TNC TKC CDS 56.39 00.00 34.60 00.00 SCNN [32] 76.84 14.75 73.51 00.00 KappaMask [33] 68.39 13.02 65.26 32.52 MCDNet [30] 80.48 36.50 79.60 44.83 CDNetv1 [16] 85.20 43.02 81.40 52.44 DBNet [31] 84.98 43.37 80.67 53.40 CDNetv2 [17] 86.01 45.88 83.54 57.61 HRCloudNet [34] UNetMobv2 [56] 88.51 50.12 84.79 63.17 Cloud-Adapter (Ours) 89.19 56.15 85.46 65. mIoU 22.75 41.27 44.80 60.35 65.52 65.60 68.26 71.65 74.18 Acc CRS TNC TKC CDS mAcc Dice CRS TNC TKC CDS mDice aAcc 89.69 00.00 45.08 00.00 33.69 97.54 27.78 77.73 00.00 50.76 77.85 18.39 84.49 59.20 59.99 90.60 50.19 88.34 60.72 72.46 95.56 53.15 89.40 62.47 75.15 94.80 53.13 89.32 65.34 75.65 94.49 59.47 91.39 67.19 78.13 95.77 61.87 91.44 75.13 81.05 94.08 75.23 91.72 78.15 84.79 72.11 00.00 51.41 00.00 30.88 86.91 25.70 84.73 00.00 49.33 81.23 23.04 78.98 49.08 58.08 89.19 53.48 88.64 61.91 73.30 92.01 60.16 89.75 68.80 77.68 91.88 60.50 89.30 69.62 77.83 92.48 62.90 91.03 73.10 79.88 93.90 66.78 91.77 77.43 82.47 94.29 71.91 92.16 79.47 84. 60.19 76.27 72.68 83.48 86.83 86.68 87.86 89.52 90.19 TABLE XI: Cloud segmentation performance comparison of different methods on the CloudSEN12 High L2A dataset. Method IoU CRS TNC TKC CDS SCNN [32] 62.20 00.00 52.74 00.08 KappaMask [33] 79.58 23.35 78.20 00.00 MCDNet [30] 72.46 14.96 67.04 31.62 CDNetv1 [16] 82.58 38.61 78.72 49.64 DBNet [31] 85.42 42.76 80.80 53.62 CDNetv2 [17] 85.35 44.01 80.85 54.00 HRCloudNet [34] 87.24 44.18 82.78 59.20 87.76 47.38 84.16 62.13 UNetMobv2 [56] Cloud-Adapter (Ours) 88.99 55.04 85.31 65.64 mIoU 28.76 45.28 40.18 62.39 65.65 66.05 68.35 70.36 73.75 Acc CRS TNC TKC CDS 85.73 00.00 76.19 00.08 97.24 38.05 86.58 00.00 85.76 21.09 85.29 42.01 92.73 52.65 88.12 60.50 94.83 53.53 90.27 63.45 95.65 56.37 88.06 63.56 95.41 51.00 93.17 69.83 95.98 59.60 90.94 71.75 94.02 74.93 91.48 77.41 mAcc 40.50 55.47 55.94 73.50 75.52 75.91 77.35 79.57 84.46 Dice CRS TNC TKC CDS 76.70 00.00 69.06 00.16 88.63 37.85 87.77 00.00 84.03 26.02 80.27 48.04 90.46 55.71 88.09 66.35 92.14 59.90 89.38 69.81 92.09 61.12 89.41 70.13 93.19 61.28 90.58 74.37 93.48 64.29 91.40 76.64 94.17 71.00 92.07 79.26 mDice aAcc 36.48 53.56 53.07 75.15 77.81 78.19 79.85 81.45 84.13 67.14 79.60 69.21 84.74 86.82 86.88 88.35 88.96 89.99 Fig. 7: Comparison of visualized segmentation results of different models on the CloudSEN12 High L1C dataset."
        },
        {
            "title": "Cloud Shadow",
            "content": "IoUs of 89.19% (CRS), 85.46% (TKC), 56.15% (TNC), and 65.93% (CDS). It outperforms methods like UNetMobv2 and HRCloudNet, which show lower scores, particularly in challenging classes such as TNC and CDS. On the CloudSEN12 High L2A dataset (Table XI), Cloud-Adapter achieves mIoU of 73.75%, with IoUs of 88.99% (CRS), 10 TABLE XII: Cloud segmentation performance comparison of different methods on the L8 Biome dataset. Method IoU CRS TNC TKC CDS 65.83 08.40 60.97 00.21 MCDNet [30] 66.30 00.00 63.24 00.00 SCNN [32] 60.69 19.63 56.79 01.21 CDNetv1 [16] 77.72 22.63 68.12 00.00 KappaMask [33] 79.18 28.35 80.46 03.05 UNetMobv2 [56] 73.81 24.85 75.85 00.00 CDNetv2 [17] 72.41 30.89 70.76 00.00 HRCloudNet [34] 80.19 38.18 82.37 04.90 DBNet [31] Cloud-Adapter (Ours) 85.81 44.41 70.75 29.13 mIoU 33.85 32.38 34.58 42.12 47.76 43.63 43.51 51.41 57.53 Acc CRS TNC TKC CDS 73.80 10.26 95.01 00.22 81.11 00.00 95.12 00.00 88.19 25.51 67.43 01.22 93.08 32.87 80.96 00.00 93.62 34.63 93.59 03.31 90.18 31.95 89.81 00.00 85.50 43.70 85.88 00.00 96.51 49.34 87.89 05.03 90.63 78.76 75.35 32.65 mAcc 44.82 44.06 45.59 51.73 56.29 52.98 53.77 59.70 69.34 Dice CRS TNC TKC CDS 79.40 15.49 75.75 00.41 79.73 00.00 77.48 00.00 75.54 32.81 72.44 02.40 87.45 36.91 81.04 00.00 88.37 44.18 89.16 05.92 84.93 39.81 86.27 00.00 83.99 47.20 82.88 00.00 89.00 55.25 90.34 09.34 92.36 61.51 82.87 45.12 mDice aAcc 42.76 39.30 45.80 51.35 56.91 52.75 53.52 60.99 70.46 69.75 71.22 68.16 76.63 82.00 78.56 77.04 83.62 81. TABLE XIII: Comparison of cloud segmentation performance across different land cover types in the L8 Biome dataset. Scene Method mIoU mAcc mDice aAcc Scene Method mIoU mAcc mDice aAcc Grass/Crops Urban Wetlands Snow/Ice KappaMask [33] CDNetv1 [16] CDNetv2 [17] HRCloudNet [34] MCDNet [30] SCNN [32] DBNet [31] UNetMobv2 [56] Cloud-Adapter (Ours) KappaMask [33] CDNetv1 [16] CDNetv2 [17] HRCloudNet [34] MCDNet [30] SCNN [32] DBNet [31] UNetMobv2 [56] Cloud-Adapter (Ours) KappaMask [33] CDNetv1 [16] CDNetv2 [17] HRCloudNet [34] MCDNet [30] SCNN [32] DBNet [31] UNetMobv2 [56] Cloud-Adapter (Ours) KappaMask [33] CDNetv1 [16] CDNetv2 [17] HRCloudNet [34] MCDNet [30] SCNN [32] DBNet [31] UNetMobv2 [56] Cloud-Adapter (Ours) 36.29 39.99 55.17 50.47 28.92 43.81 59.08 49.46 41. 48.83 46.30 56.27 56.47 31.90 48.63 59.68 50.24 51.76 40.38 34.50 40.54 41.48 26.23 34.38 49.79 41.54 50.16 24.41 20.93 24.99 22.20 10.43 12.43 29.72 23.50 32.01 62.25 59.01 71.84 66.21 69.85 70.48 73.85 74.78 63.10 73.39 64.64 74.55 71.57 70.68 71.55 76.73 79.90 76.84 61.72 56.45 59.62 57.89 55.86 55.90 67.54 64.58 74. 43.24 38.00 41.18 36.91 25.64 23.33 47.25 41.99 56.99 42.17 44.22 59.23 56.19 32.92 46.86 63.31 54.39 48.12 55.55 51.03 61.74 62.85 36.48 52.05 65.25 55.39 58.36 47.59 39.69 46.71 47.93 30.75 37.76 56.52 47.82 57.09 29.37 23.82 28.51 25.41 13.30 14.37 34.04 27.96 36.25 76.56 73.04 86.78 81.99 81.61 83.13 89.85 88.21 72. 86.78 81.44 89.13 87.99 83.87 85.66 92.14 91.59 86.27 75.12 66.57 74.78 76.38 68.51 67.42 83.53 77.59 86.08 56.74 44.90 55.37 47.58 29.59 24.70 67.69 56.48 73.15 Barren Forest Shrubland Water KappaMask [33] CDNetv1 [16] CDNetv2 [17] HRCloudNet [34] MCDNet [30] SCNN [32] DBNet [31] UNetMobv2 [56] Cloud-Adapter (Ours) KappaMask [33] CDNetv1 [16] CDNetv2 [17] HRCloudNet [34] MCDNet [30] SCNN [32] DBNet [31] UNetMobv2 [56] Cloud-Adapter (Ours) KappaMask [33] CDNetv1 [16] CDNetv2 [17] HRCloudNet [34] MCDNet [30] SCNN [32] DBNet [31] UNetMobv2 [56] Cloud-Adapter (Ours) KappaMask [33] CDNetv1 [16] CDNetv2 [17] HRCloudNet [34] MCDNet [30] SCNN [32] DBNet [31] UNetMobv2 [56] Cloud-Adapter (Ours) 38.36 31.50 41.24 44.33 26.56 36.93 47.41 43.66 48. 38.32 33.86 41.94 42.93 30.05 37.62 44.93 45.23 48.09 40.87 32.70 42.04 41.68 28.66 37.11 46.49 41.58 51.18 42.42 44.43 48.72 46.74 29.43 41.82 52.42 49.58 46.43 60.64 48.05 59.24 60.53 60.52 60.42 65.13 67.80 72.35 57.00 48.50 58.62 58.30 57.39 55.44 62.40 64.89 68.76 59.71 49.79 59.48 57.22 57.34 56.93 63.41 63.24 72. 64.40 61.84 64.17 60.46 60.95 61.56 67.85 70.25 67.75 45.31 36.15 46.03 50.19 31.34 40.55 53.05 49.67 56.45 44.46 38.67 47.87 49.64 35.23 41.39 50.81 51.17 56.25 48.28 38.89 48.51 48.98 34.20 41.61 53.76 48.19 59.19 49.36 49.20 54.10 52.96 33.81 45.51 58.29 55.37 53.26 76.96 59.96 76.33 77.51 73.00 73.35 80.38 84.01 83. 80.07 71.81 81.34 81.76 77.71 77.92 85.22 86.89 82.30 77.26 61.35 76.94 76.79 71.57 71.25 80.88 79.79 87.42 81.19 82.83 85.07 82.62 79.71 80.86 87.15 88.24 82.05 85.31% (TKC), 55.04% (TNC), and 65.64% (CDS). Its mDice score of 84.13% further highlights its balanced performance across all categories. On the L8 Biome dataset (Tables XII and XIII), Cloud-Adapter attains the highest mIoU (57.53%), mAcc (69.34%), mDice (70.46%), and aAcc (81.79%). It excels in challenging categories like Snow/Ice (32.01%) and Shrubland (51.18%), consistently outperforming competitors such as DBNet and UNetMobv2 across diverse land scenes. b) Visualization Comparison: Fig. 7 and 8 present Cloud-Adapters segmentation results on multi-class datasets (CloudSEN12 High L1C and CloudSEN12 High L2A), accurately distinguishing thick clouds, thin clouds, and cloud shadows in complex conditions. The figures also demonstrate its performance across varying cloud cover levels: Low (less than 35%), Medium (3565%), and High (over 65%). These results highlight Cloud-Adapters robustness and adaptability to diverse atmospheric scenarios. Fig. 9 highlights Cloud-Adapters segmentation performance on the L8 Biome dataset, demonstrating superior generalization across diverse biomes, such as barren, forest, and snow/ice regions. These results further emphasize the models capability to adapt to varying environmental conditions while maintaining high accuracy and precision, solidifying its effectiveness for complex cloud segmentation tasks in remote sensing applications. In conclusion, across both binary-class and multi-class datasets, Cloud-Adapter consistently outperforms state-of-the11 Fig. 8: Comparison of visualized segmentation results of different models on the CloudSEN12 High L2A dataset."
        },
        {
            "title": "Cloud Shadow",
            "content": "Fig. 9: Comparison of visualized segmentation results of different models on the L8 Biome dataset. art methods. Its high aAcc, mIoU, mAcc, and mDice scores demonstrate its capability to effectively handle diverse cloud segmentation tasks. The visual comparisons further validate its robustness and adaptability, making it reliable algorithm for cloud detection in remote sensing applications. V. CONCLUSION In this paper, we introduced Cloud-Adapter, parameterefficient fine-tuning method for cloud segmentation in remote sensing images. By leveraging frozen VFM and adding lightweight adapter, Cloud-Adapter reduces the number of trainable parameters while achieving SOTA performance across various datasets. Our approach offers an effective solution for robust cloud segmentation without requiring extensive retraining, making it simultaneously efficient and adaptable. Limitations. The performance of Cloud-Adapter depends heavily on the VFM, and the large parameters of VFMs make it difficult to deploy on edge devices with limited resources."
        },
        {
            "title": "REFERENCES",
            "content": "[22] A. Kirillov, E. Mintun, and N. e. a. Ravi, Segment anything, in ICCV, [1] C. E. Bulgin, R. I. Maidment, D. Ghent, and C. J. Merchant, Stability of cloud detection methods for land surface temperature (lst) climate data records (cdrs), Remote Sensing of Environment, vol. 315, p. 114440, 2024. [2] C.-C. Young, Y.-C. Cheng, M.-A. Lee, and J.-H. Wu, Accurate reconstruction of satellite-derived sst under cloud and cloud-free areas using physically-informed machine learning approach, Remote Sensing of Environment, vol. 313, p. 114339, 2024. [3] Z. Zhu and C. E. Woodcock, Automated cloud, cloud shadow, and snow detection in multitemporal landsat data: An algorithm designed specifically for monitoring land cover change, Remote Sensing of Environment, vol. 152, pp. 217234, 2014. [4] Y. Zhang, B. Guindon, and J. Cihlar, An image transform to characterize and compensate for spatial variations in thin cloud contamination of landsat images, Remote Sensing of Environment, vol. 82, no. 2, pp. 173187, 2002. [5] S. Chen, X. Chen, J. Chen, P. Jia, X. Cao, and C. Liu, An iterative haze optimized transformation for automatic cloud/haze detection of landsat imagery, IEEE Transactions on Geoscience and Remote Sensing, vol. 54, no. 5, pp. 26822694, 2016. [6] Z. Zhu, S. Wang, and C. E. Woodcock, Improvement and expansion of the fmask algorithm: cloud, cloud shadow, and snow detection for landsats 47, 8, and sentinel 2 images, Remote Sensing of Environment, vol. 159, pp. 269277, 2015. [7] S. Foga, P. L. Scaramuzza, S. Guo, Z. Zhu, R. D. Dilley, T. Beckmann, G. L. Schmidt, J. L. Dwyer, M. Joseph Hughes, and B. Laue, Cloud detection algorithm comparison and validation for operational landsat data products, Remote Sensing of Environment, vol. 194, pp. 379390, 2017. [8] Y. LeCun, Y. Bengio, and G. Hinton, Deep learning, Nature, vol. 521, no. 7553, pp. 436444, 2015. [9] Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner, Gradient-based learning applied to document recognition, Proceedings of the IEEE, vol. 86, no. 11, pp. 22782324, 1998. [10] A. Krizhevsky, I. Sutskever, and G. E. Hinton, Imagenet classification with deep convolutional neural networks, in NeurIPS, F. Pereira, C. Burges, L. Bottou, and K. Weinberger, Eds., vol. 25, 2012. [11] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich, Going deeper with convolutions, in CVPR, 2015. [12] Z. Li, H. Shen, Q. Cheng, Y. Liu, S. You, and Z. He, Deep learning based cloud detection for medium and high resolution remote sensing images of different sensors, ISPRS Journal of Photogrammetry and Remote Sensing, vol. 150, pp. 197212, 2019. [13] J. H. Jeppesen, R. H. Jacobsen, F. Inceoglu, and T. S. Toftegaard, cloud detection algorithm for satellite imagery based on deep learning, Remote Sensing of Environment, vol. 229, pp. 247259, 2019. [14] Z. Shao, Y. Pan, C. Diao, and J. Cai, Cloud detection in remote sensing images based on multiscale features-convolutional neural network, IEEE Transactions on Geoscience and Remote Sensing, vol. 57, no. 6, pp. 40624076, 2019. [15] M. Shi, F. Xie, Y. Zi, and J. Yin, Cloud detection of remote sensing images by deep learning, in 2016 IEEE International Geoscience and Remote Sensing Symposium (IGARSS), 2016, pp. 701704. [16] J. Yang, J. Guo, H. Yue, Z. Liu, H. Hu, and K. Li, Cdnet: Cnnbased cloud detection for remote sensing imagery, IEEE Transactions on Geoscience and Remote Sensing, vol. 57, no. 8, pp. 61956211, 2019. [17] J. Guo, J. Yang, H. Yue, H. Tan, C. Hou, and K. Li, Cdnetv2: Cnnbased cloud detection for remote sensing imagery with cloud-snow coexistence, IEEE Transactions on Geoscience and Remote Sensing, vol. 59, no. 1, pp. 700713, 2021. [18] Q. He, X. Sun, Z. Yan, and K. Fu, Dabnet: Deformable contextual and boundary-weighted network for cloud detection in remote sensing images, IEEE Transactions on Geoscience and Remote Sensing, vol. 60, pp. 116, 2022. [19] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly et al., An image is worth 16x16 words: Transformers for image recognition at scale, in ICLR, 2020. [20] Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, and B. Guo, Swin transformer: Hierarchical vision transformer using shifted windows, in ICCV, 2021, pp. 10 01210 022. [21] K. He, X. Chen, S. Xie, Y. Li, P. Dollar, and R. Girshick, Masked autoencoders are scalable vision learners, in CVPR, 2022, pp. 16 000 16 009. 2023, pp. 40154026. [23] M. Oquab, T. Darcet, and T. e. a. Moutakanni, DINOv2: Learning Robust Visual Features without Supervision, Transactions on Machine Learning Research Journal, pp. 131, 2024. [24] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, Imagenet: large-scale hierarchical image database, in CVPR, 2009, pp. 248255. [25] C.-F. R. Chen, Q. Fan, and R. Panda, Crossvit: Cross-attention multiscale vision transformer for image classification, in ICCV, October 2021, pp. 357366. [26] H. Chen, P. Wei, G. Guo, and S. Gao, Sam-cod: Sam-guided unified framework for weakly-supervised camouflaged object detection, in ECCV, 2025, pp. 315331. [27] X. He, Y. Zhou, J. Zhao, D. Zhang, R. Yao, and Y. Xue, Swin transformer embedding unet for remote sensing image semantic segmentation, IEEE Transactions on Geoscience and Remote Sensing, vol. 60, pp. 115, 2022. [28] F. Li, H. Zhang, H. Xu, S. Liu, L. Zhang, L. M. Ni, and H.-Y. Shum, Mask dino: Towards unified transformer-based framework for object detection and segmentation, in CVPR, June 2023, pp. 30413050. [29] G. Li, H. Zheng, D. Liu, C. Wang, B. Su, and C. Zheng, Semmae: Semantic-guided masking for learning masked autoencoders, in NeurIPS, vol. 35, 2022, pp. 14 29014 302. [30] J. Dong, Y. Wang, Y. Yang, M. Yang, and J. Chen, Mcdnet: Multilevel cloud detection network for remote sensing images based on dualperspective change-guided and multi-scale feature fusion, International Journal of Applied Earth Observation and Geoinformation, vol. 129, p. 103820, 2024. [31] C. Lu, M. Xia, M. Qian, and B. Chen, Dual-branch network for cloud and cloud shadow segmentation, IEEE Transactions on Geoscience and Remote Sensing, vol. 60, pp. 112, 2022. [32] D. Chai, J. Huang, M. Wu, X. Yang, and R. Wang, Remote sensing image cloud detection using shallow convolutional neural network, ISPRS Journal of Photogrammetry and Remote Sensing, vol. 209, pp. 6684, 2024. [33] M. Domnich, I. Sunter, H. Trofimov, O. Wold, F. Harun, A. Kostiukhin, M. Jarveoja, M. Veske, T. Tamm, K. Voormansik et al., Kappamask: Ai-based cloudmask processor for sentinel-2, Remote Sensing, vol. 13, no. 20, p. 4100, 2021. [34] J. Li, T. Xue, J. Zhao, J. Ge, Y. Min, W. Su, and K. Zhan, Highresolution cloud detection network, Journal of Electronic Imaging, vol. 33, no. 4, pp. 043 027043 027, 2024. [35] S. Zhu, Z. Li, and H. Shen, Transferring deep models for cloud detection in multisensor images via weakly supervised learning, IEEE Transactions on Geoscience and Remote Sensing, vol. 62, pp. 118, 2024. [36] O. Ronneberger, P. Fischer, and T. Brox, U-net: Convolutional networks for biomedical image segmentation, in MICCAI, 2015, pp. 234241. [37] K. He, X. Zhang, S. Ren, and J. Sun, Deep residual learning for image recognition, in CVPR, 2016, pp. 770778. [38] T.-Y. Lin, P. Dollar, R. Girshick, K. He, B. Hariharan, and S. Belongie, Feature pyramid networks for object detection, in CVPR, 2017, pp. 936944. [39] L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. L. Yuille, Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs, IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 40, no. 4, pp. 834848, 2018. [40] J. D. M.-W. C. Kenton and L. K. Toutanova, Bert: Pre-training of deep bidirectional transformers for language understanding, in NAACL, vol. 1, 2019, p. 2. [41] T. Brown, B. Mann, N. Ryder, and et al., Language models are few-shot learners, in NeurIPS, vol. 33, 2020, pp. 18771901. [42] A. Radford, J. W. Kim, and C. e. a. Hallacy, Learning transferable visual models from natural language supervision, in ICML, vol. 139, 2021, pp. 87488763. [43] S. J. Pan and Q. Yang, survey on transfer learning, IEEE Transactions on knowledge and data engineering, vol. 22, no. 10, pp. 1345 1359, 2009. [44] Y. Ganin and V. Lempitsky, Unsupervised domain adaptation by backpropagation, in ICML, vol. 37, 2015, pp. 11801189. [45] N. Houlsby, A. Giurgiu, S. Jastrzebski, B. Morrone, Q. De Laroussilhe, A. Gesmundo, M. Attariyan, and S. Gelly, Parameter-efficient transfer learning for nlp, in ICML, 2019, pp. 27902799. [46] E. J. Hu, yelong shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and W. Chen, LoRA: Low-rank adaptation of large language models, in ICLR, 2022, pp. 113. [47] H. Ye, J. Zhang, S. Liu, X. Han, and W. Yang, Ip-adapter: Text compatible image prompt adapter for text-to-image diffusion models, 2023. [48] C. Mou, X. Wang, L. Xie, Y. Wu, J. Zhang, Z. Qi, and Y. Shan, T2iadapter: Learning adapters to dig out more controllable ability for textto-image diffusion models, in AAAI, vol. 38, no. 5, 2024, pp. 4296 4304. [49] Z. Chen, Y. Duan, W. Wang, J. He, T. Lu, J. Dai, and Y. Qiao, Vision transformer adapter for dense predictions, in ICLR, 2023, pp. 120. [50] T. Chen, L. Zhu, C. Deng, R. Cao, Y. Wang, S. Zhang, Z. Li, L. Sun, Y. Zang, and P. Mao, Sam-adapter: Adapting segment anything in underperformed scenes, in ICCV, 2023, pp. 33673375. [51] F. Chollet, Xception: Deep learning with depthwise separable convolutions, in CVPR, 2017, pp. 18001807. [52] H. Lin, X. Cheng, X. Wu, and D. Shen, Cat: Cross attention in vision transformer, in ICME, 2022, pp. 16. [53] Z. Huang, X. Wang, L. Huang, C. Huang, Y. Wei, and W. Liu, Ccnet: Criss-cross attention for semantic segmentation, in ICCV, October 2019. [54] O. Petit, N. Thome, C. Rambour, L. Themyr, T. Collins, and L. Soler, U-net transformer: Self and cross attention for medical image segmentation, in MLMI, 2021, pp. 267276. [55] X. Zou, K. Li, J. Xing, P. Tao, and Y. Cui, PMAA: progressive multiscale attention autoencoder model for high-performance cloud removal from multi-temporal satellite imagery, in ECAI, vol. 372, 2023, pp. 31653172. [56] C. Aybar, L. Ysuhuaylas, J. Loja, K. Gonzales, F. Herrera, L. Bautista, R. Yali, A. Flores, L. Diaz, N. Cuenca et al., Cloudsen12, global dataset for semantic understanding of cloud and cloud shadow in sentinel-2, Scientific data, vol. 9, no. 1, p. 782, 2022. [57] S. Foga, P. L. Scaramuzza, S. Guo, Z. Zhu, R. D. Dilley, T. Beckmann, G. L. Schmidt, J. L. Dwyer, M. Joseph Hughes, and B. Laue, Cloud detection algorithm comparison and validation for operational landsat data products, Remote Sensing of Environment, vol. 194, pp. 379390, 2017. [58] M. Contributors, MMSegmentation: Openmmlab semantic segmentation toolbox and benchmark, 2020. [59] B. Cheng, I. Misra, A. G. Schwing, A. Kirillov, and R. Girdhar, Masked-attention mask transformer for universal image segmentation, in CVPR, 2022, pp. 12901299. [60] I. Loshchilov and F. Hutter, Decoupled weight decay regularization, in ICLR, 2018. [61] K. He, X. Zhang, S. Ren, and J. Sun, Delving deep into rectifiers: Surpassing human-level performance on imagenet classification, in ICCV, 2015, pp. 10261034. [62] Z. Liu, H. Mao, C.-Y. Wu, C. Feichtenhofer, T. Darrell, and S. Xie, convnet for the 2020s, in CVPR, 2022, pp. 11 97611 986. [63] J. Zhang, X. Yang, R. Jiang, W. Shao, and L. Zhang, Rsam-seg: sam-based approach with prior knowledge integration for remote sensing image semantic segmentation, 2024. Xuechao Zou received the B.E. degree in 2021 and the M.S. degree in 2024 from the School of Computer Technology and Application at Qinghai University, Xining, China. He is currently pursuing Ph.D. degree with the School of Computer Science and Technology at Beijing Jiaotong University. His research interests focus on computer vision, particularly in the area of remote sensing image processing. Shun Zhang is pursuing the B.E. degree with the School of Computer Technology and Application at Qinghai University, Xining, China. His research interests primarily focus on computer vision, particularly in the area of remote sensing image processing. 13 Kai Li received his B.E. degree from the School of Computer Technology and Application at Qinghai University, Xining, China, in 2020, and his M.S. degree from the Department of Computer Science and Technology at Tsinghua University, Beijing, China, in 2024. He is pursuing Ph.D. in the Department of Computer Science and Technology at Tsinghua University. His research interests focus primarily on speech separation and audio-visual learning. Shiying Wang received the B.E. degree from Jilin Agricultural University, Changchun, China, in 2015, and the M.S. degree from the School of Computer Technology and Application, Qinghai University, Xining, China, in 2018. She is currently pursuing Ph.D. degree at Qinghai University. Her research interests mainly include remote sensing image processing, particularly in grassland informatics. Junliang Xing received his dual B.S. degrees in computer science and mathematics from Xian Jiaotong University, Shaanxi, China, in 2007, and the Ph.D. degree in Computer Science and Technology from Tsinghua University, Beijing, China, in 2012. He is currently Professor at the Department of Computer Science and Technology, Tsinghua University. He has published over 150 peer-reviewed papers with more than 17,000 citations from Google Scholar. His current research interest focuses on computer vision and gaming problems related to single/multiple agent learning and human-computer interactive learning. Lei Jin is currently an associate research fellow in the Beijing University of Posts and Telecommunications (BUPT), Beijing, China. Before that, he graduated from the same university with Ph.D. degree in 2020. He received bachelors degree in the BUPT in 2015. His research interest includes machine learning and pattern recognition, focusing on 6D and human pose estimation. Moreover, he concentrated on network security and analyzed traffic security while pursuing his Ph.D. degree. Congyan Lang received the Ph.D. degree from the Beijing Key Laboratory of Traffic Data Analysis and Mining, School of Computer and Information Technology, Beijing Jiaotong University, Beijing, China, in 2006. She was Visiting Professor with the Department of Electrical and Computer Engineering, National University of Singapore, Singapore, from 2010 to 2011. From 2014 to 2015, she was Visiting Professor with the Department of Computer Science, University of Rochester, Rochester, NY, USA. She is currently Professor with the School of Computer and Information Technology, Beijing Jiaotong University. She has published more than 80 research articles in various journals and refereed conferences. Her research areas include computer vision and machine learning. Pin Tao received his B.S. degree and Ph.D. in computer science and technology from Tsinghua University, Beijing, China, in 1997 and 2002. He is currently an Associate Professor at the Department of Computer Science and Technology, Tsinghua University, Beijing, China. He is also the vice director of the Key Laboratory of Pervasive Computing, Ministry of Education. Dr. Tao has published more than 80 papers and over 10 patents. His current research interests mainly focus on human-AI hybrid intelligence and multimedia-embedded processing."
        }
    ],
    "affiliations": [
        "Department of Computer Science and Technology, Tsinghua University, Beijing, China",
        "Key Lab of Big Data & Artificial Intelligence in Transportation (Ministry of Education), School of Computer Science & Technology, Beijing Jiaotong University, Beijing, China",
        "Key Laboratory of Pervasive Computing, Ministry of Education",
        "School of Computer Technology and Application, Qinghai University, Xining, China",
        "School of Electronic Engineering, Beijing University of Posts and Telecommunications, Beijing, China"
    ]
}