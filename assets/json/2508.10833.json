{
    "paper_title": "UI-Venus Technical Report: Building High-performance UI Agents with RFT",
    "authors": [
        "Zhangxuan Gu",
        "Zhengwen Zeng",
        "Zhenyu Xu",
        "Xingran Zhou",
        "Shuheng Shen",
        "Yunfei Liu",
        "Beitong Zhou",
        "Changhua Meng",
        "Tianyu Xia",
        "Weizhi Chen",
        "Yue Wen",
        "Jingya Dou",
        "Fei Tang",
        "Jinzhen Lin",
        "Yulin Liu",
        "Zhenlin Guo",
        "Yichen Gong",
        "Heng Jia",
        "Changlong Gao",
        "Yuan Guo",
        "Yong Deng",
        "Zhenyu Guo",
        "Liang Chen",
        "Weiqiang Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We present UI-Venus, a native UI agent that takes only screenshots as input based on a multimodal large language model. UI-Venus achieves SOTA performance on both UI grounding and navigation tasks using only several hundred thousand high-quality training samples through reinforcement finetune (RFT) based on Qwen2.5-VL. Specifically, the 7B and 72B variants of UI-Venus obtain 94.1% / 50.8% and 95.3% / 61.9% on the standard grounding benchmarks, i.e., Screenspot-V2 / Pro, surpassing the previous SOTA baselines including open-source GTA1 and closed-source UI-TARS-1.5.To show UI-Venus's summary and planing ability, we also evaluate it on the AndroidWorld, an online UI navigation arena, on which our 7B and 72B variants achieve 49.1% and 65.9% success rate, also beating existing models.To achieve this, we introduce carefully designed reward functions for both UI grounding and navigation tasks and corresponding efficient data cleaning strategies.To further boost navigation performance, we propose Self-Evolving Trajectory History Alignment \\& Sparse Action Enhancement that refine historical reasoning traces and balances the distribution of sparse but critical actions, leading to more coherent planning and better generalization in complex UI tasks. Our contributions include the publish of SOTA open-source UI agents, comprehensive data cleaning protocols and a novel self-evolving framework for improving navigation performance, which encourage further research and development in the community. Code is available at https://github.com/antgroup/UI-Venus."
        },
        {
            "title": "Start",
            "content": "UI-Venus Technical Report: Building High-performance UI Agents with RFT Zhangxuan Gu, Zhengwen Zeng, Zhenyu Xu, Xingran Zhou, Shuheng Shen,, Yunfei Liu, Beitong Zhou, Changhua Meng, Tianyu Xia, Weizhi Chen, Yue Wen, Jingya Dou, Fei Tang, Jinzhen Lin, Yulin Liu, Zhenlin Guo, Yichen Gong, Heng Jia, Changlong Gao, Yuan Guo, Yong Deng, Zhenyu Guo, Liang Chen, Weiqiang Wang Ant Group We present UI-Venus, native UI agent that takes only screenshots as input based on multimodal large language model. UI-Venus achieves SOTA performance on both UI grounding and navigation tasks using only several hundred thousand high-quality training samples through reinforcement finetune (RFT) based on Qwen2.5-VL. Specifically, the 7B and 72B variants of UI-Venus obtain 94.1% / 50.8% and 95.3% / 61.9% on the standard grounding benchmarks, i.e., Screenspot-V2 / Pro, surpassing the previous SOTA baselines including open-source GTA1 and closed-source UI-TARS-1.5. To show UI-Venuss summary and planing ability, we also evaluate it on the AndroidWorld, an online UI navigation arena, on which our 7B and 72B variants achieve 49.1% and 65.9% success rate, also beating existing models. To achieve this, we introduce carefully designed reward functions for both UI grounding and navigation tasks and corresponding efficient data cleaning strategies. To further boost navigation performance, we propose Self-Evolving Trajectory History Alignment & Sparse Action Enhancement that refine historical reasoning traces and balances the distribution of sparse but critical actions, leading to more coherent planning and better generalization in complex UI tasks. Our contributions include the publish of SOTA opensource UI agents, comprehensive data cleaning protocols and novel self-evolving framework for improving navigation performance, which encourage further research and development in the community. Code: https://github.com/antgroup/UI-Venus 5 2 0 A 4 1 ] . [ 1 3 3 8 0 1 . 8 0 5 2 : r Figure 1 UI-Venus achieves SOTA performance across multiple UI grounding and navigation benchmarks. *Equal contribution. Corresponding author: Shuheng Shen(shuheng.ssh@antgroup.com)."
        },
        {
            "title": "1 Introduction",
            "content": "Recent studies of multimodal large language models (MLLMs) Bai et al. (2023); Anthropic (2024); Wang et al. (2024d); Bai et al. (2025); Zhu et al. (2025); Zhipu-AI (2025) have contributed significantly to the advancements and developments of UI agents Hu et al. (2024); Gao et al. (2024); Wang et al. (2024e); Nguyen et al. (2024); Zhang et al. (2024a). In particular, many early approaches, e.g., CogAgent Hong et al. (2024) and UI-TARS Qin et al. (2025), directly leverage extensive open-source and private datasets through pretraining and supervised fine-tuning (SFT) to achieve commendable performance in UI agent tasks. Specifically, these pretrained and SFT approaches treat the complete UI task traces, state observations, and current actions as plain texts for token-level supervised learning. Although SFT is effective in many generation tasks with strong instruction-following ability, sometimes it is not appropriate in discriminative tasks like UI agent. For example, in UI grounding tasks, predicted point is considered correct as long as it falls within the ground-truth bounding box. In contrast, SFT assigns widely-used cross-entropy loss penalty to any predicted points within and without the box except the center point. Moreover, the data collection and cleaning workflow during pretraining is time consuming and requires lot of human work. Inspired by the emergence of DeepSeek-R1 DeepSeek-AI (2025) and its innovative Group Relative Policy Optimization (GRPO) algorithm Shao et al. (2024), recent researchers focus on reinforcement finetune (RFT) paradigms for discriminative tasks such as math and code. RFT generally needs fewer training data, yet has better generalization abilities compared to SFT, as exemplified by Chen et al. (2025). As result, many approaches similar to UI-R1 Lu et al. (2025b) have also achieved satisfactory results on UI grounding benchmarks by adapting and modifying the reward functions of VLM-R1 Shen et al. (2025). For example, GUI-G1 Zhou et al. (2025), GUI-G2 Tang et al. (2025a), Phi-Ground Zhang et al. (2025a) and GTA1 Yang et al. (2025) mainly focus on UI grounding benchmarks (ScreenSpotv2 Wu et al. (2024) and ScreenSpotpro Li et al. (2025b)) with different data collection and ratios. Some tricks like think / no-think and input message construction are also discussed in their reports. On the other hand, GUI-R1 Luo et al. (2025a), InfiGUI-R1 Liu et al. (2025) and UI-R1 Lu et al. (2025b) extend GRPO to UI offline navigation benchmarks (Andorid Control Li et al. (2024) and GUI-Odyssey Lu et al. (2025a)) by adapting action type, format and the point rewards with predefined action spaces. Despite these advancements, current R1-like UI agents exhibit three critical limitations. First, although some attempts are made by UI-R1Lu et al. (2025b) and InfiGUI-R1 Liu et al. (2025) in offline UI navigation tasks, their summary, memory and planning ability are still not enough for online tasks like AndroidWorld Rawles et al. (2025) when given user goal with complex and changeful interactive environment. Secondly, one important factor for training UI agents is the data quality. According to our observations, approximately half of open-source UI data contain noise, while only few methods have considered the data cleaning and selection strategies. Thirdly, existing implementations mainly focus on small MLLMs (e.g., 3B/7B parameters), neglecting the potential of large-scale models (e.g., 72B) in RFT training. Although GTA1 has developed 72B grounding model, no further attempts are made on the end-to-end UI navigation tasks without addtional planner like GPT4o OpenAI (2024). This constraint results in performance gaps compared to state-of-the-art large-scale models like UI-TARS-1.5 Seed (2025b) and SeedVL-1.5 Seed (2025a) on UI agent benchmarks. Moreover, the evaluation for UI agents on many benchmarks suffers from severe challenges due to different input prompts as well as unreleased hyper-parameters and dataset settings. For example, some results from the official papers can not be reproduced according to Yang et al. (2025); Zhang et al. (2025b) and some github issues. 2 To address the first challenge mentioned above, we design Self-Evolving Trajectory History Alignment & Sparse Action Enhancement framework. This method addresses two critical limitations in existing UI navigation agents: (1) misaligned historical reasoning traces and (2) insufficient learning of rare but pivotal actions. For the first issue, we iteratively refine the thoughtaction histories between training epochs, aligning the form and level of detail of the historical reasoning with the agents evolving decision-making patterns. This produces more coherent and informative context for predicting subsequent steps, which in turn improves planning accuracy. For the second, we selectively re-sample trajectories containing sparse actions, constructing multiple historical context variants that lead to the same low-frequency operation. This balanced sampling increases the models exposure to infrequent yet crucial skills, enhancing its ability to generalize in complex and dynamic UI task scenarios. To acquire high-quality UI data, we implement three-stage processing pipeline to tackle the second problem: (1) Data Filtering includes unifying scroll directions, filtering out trajectories with inconsistencies, and resampling trajectories based on their categories. (2) Trace Reconstruction refers to modifying the information-retrieval traces with specific answers inserted. (3) Iteratively Trace Generation: we develop data generation framework by using UI-Venus-Navi to predict and record trajectories, with comprehensive quality filtering strategies to select high-quality traces for iterative training. During the training, we use about 107k/350k high-quality training samples filtered by ourselves for UI grounding/navigation task, respectively. More details will be introduced in the next section. In this report, we develop and publicly release the UI-Venus series, comprising UI-Venus-Ground7B/72B and UI-Venus-Navi-7B/72B, all trained with GRPO on the Qwen2.5-VL model Bai et al. (2025). We also open-source the evaluation codes of grounding as well as the prompts and postprocessing scripts of navigation to enhance accessibility and ease of use for researchers. Experimental results exhibit that UI-Venus outperforms all existing UI agents, showing strong performance with about 350k self-constructed high quality dataset. Our model achieves new SOTA performance on five UI grounding benchmarks including ScreenSpot-V2 Wu et al. (2024), ScreenSpot-Pro Li et al. (2025b), OSWorld-G Xie et al. (2025), UI-Vision Nayak et al. (2025) and CA-GUI Zhang et al. (2025b). The 7B and 72B variants of UI-Venus obtain 94.1% / 50.8% and 95.3% / 61.9% on the Screenspot-V2 / Pro, surpassing previous SOTA baselines including open-source GTA1 and closed-source UI-TARS-1.5. For UI navigation, we evaluate UI-Venus on both offline and online UI navigation benchmarks. Our model achieves SOTA performance on AndroidWorld Rawles et al. (2025), where 7B and 72B variants achieve 49.1% and 65.9% success rate, while obtaining comparable results on AndroidControl Li et al. (2024) and GUI-Odyssey Lu et al. (2025a) benchmarks. There are two motivations for publishing the grounding and navigation models separately. (1) More Efficient Grounding Model: According to our experiments, explicitly prompting the model to output its thinking process significantly enhances its observation and planning capabilities in UI navigation tasks. However, UI-Venus achieves approximate performance on UI grounding tasks with and without thinking. As no-think mode for grounding outputs only several location tokens, its more efficient in inference for real-world applications. (2) Reward Conflicts: If we attempt to train an agent using both grounding and navigation tasks, feasible approach is to convert grounding data into single-step click actions in the navigation task. However, using these two reward mechanisms simultaneously can lead to unstable training, ultimately degrading performance on both tasks. We summarize our main contributions as follows: 3 To mitigate historical reasoning misalignment and augment the learning of rare but pivotal actions, we design Self-Evolving Trajectory History Alignment & Sparse Action Enhancement framework, which in turn boosts navigation performance in complex UI scenarios. We conduct comprehensive study of the quality of UI data, and propose data cleaning and selection strategy to improve training data quality for both grounding and navigation. We develop and open-source UI-Venus, SOTA UI agent for both grounding and navigation tasks with carefully designed reward functions, whose 7B and 72B variants obtaining 94.1% / 50.8% and 95.3% / 61.9% on Screenspot-V2 / Pro, and 49.1% and 65.9% on AndroidWorld, demonstrating the effectiveness of model scaling up for GRPO in UI-agent tasks."
        },
        {
            "title": "2.1 UI Grounding",
            "content": "UI Grounding focuses on localizing and identifying UI elements based on natural language instructions, serving as the foundation of automated UI interaction. Traditional approaches have relied on Supervised Fine-Tuning to train models on labeled UI datasets Cheng et al. (2024); Lin et al. (2024); Xu et al. (2024); Wu et al. (2024); Gu et al. (2023); Gou et al. (2024); Wang et al. (2024c). However, these methods face two primary limitations: (1) poor generalization in out-of-distribution scenarios where UI layouts or visual styles differ from training data, and (2) high costs associated with acquiring large-scale annotated datasets for diverse UI environments. Recent advances have shifted toward reinforcement learning-based fine-tuning inspired by the DeepSeek-R1 paradigm. Early works like UI-R1 Lu et al. (2025b) and GUI-R1 Luo et al. (2025a) introduced binary hit-or-miss rewards for task completion, while InfiGUI-R1 Liu et al. (2025) proposed two-stage training combining offline pretraining with online RL. GUI-G1 Zhou et al. (2025) further refined the reward design with box-size-based rewards for improved spatial precision. The latest methods, including SE-GUI Yuan et al. (2025), LPO Tang et al. (2025c), and GUI-G2 Tang et al. (2025a), employ continuous reward mechanisms that provide fine-grained feedback throughout the grounding process."
        },
        {
            "title": "2.2 UI Agents",
            "content": "UI Agent Framework. The UI agent framework leverages collaborative agent systems to handle complex GUI automation tasks through task decomposition and specialization. Mobile-Agent Wang et al. (2024b,a, 2025b) addresses mobile device automation by introducing planning-decisionreflection architecture, where specialized agents handle task progress tracking, action execution, and operation verification respectively, while maintaining memory unit for context preservation across interactions. Cradle Tan et al. (2024) presents modular framework with six core componentsinformation gathering, self-reflection, task inference, skill curation, action planning, and memoryenabling agents to tackle both video game control and software manipulation through adaptive learning and skill reuse. Agent-S Agashe et al. (2024) implements role-specific agents that specialize in distinct UI interaction patterns, improving task execution through modular decisionmaking. DroidRun dro (2025) focuses on Android automation by leveraging Accessibility Services to access structured UI hierarchies rather than pixel-based approaches, enabling reliable interaction with native mobile applications . These frameworks illustrate that agent specialization, in conjunction with structured communication protocols, can effectively address diverse UI scenarios. However, this capability comes at the cost of increased system complexity and computational overhead required to 4 coordinate multiple agents. Native UI Agent. Native UI agents Hong et al. (2024); Qin et al. (2025); Zhang et al. (2025b,a); Feng et al. (2025); Bai et al. (2024); Humphreys et al. (2022) represent paradigm shift toward unified and end-to-end systems that directly learn to interact with graphical interfaces without requiring multiple specialized components. CogAgent Hong et al. (2024) and UI-TARS Qin et al. (2025) pioneered this approach through large-scale training on diverse UI interaction data, enabling the model to develop comprehensive understanding of GUI patterns across different platforms and applications. By training on millions of UI screenshots paired with action sequences, UI-TARS demonstrated that single model could effectively handle various tasks. Following this success, the native UI agent introduced architectural improvements that enhance the models ability to process high-resolution screenshots while maintaining efficient action prediction, particularly excelling in scenarios with dynamic UI updates and real-time feedback requirements. AgentCPM-GUI Zhang et al. (2025b) took different approach by focusing on mobile-specific optimizations, incorporating touch gesture understanding and mobile UI design patterns into its training process, resulting in superior performance on mobile platforms with reduced latency. Meanwhile, Phi-Ground Zhang et al. (2025a) advanced the field by integrating sophisticated vision-language alignment techniques, enabling more robust grounding of natural language instructions to visual UI elements through cross-modal attention mechanisms. These native agents benefit from their streamlined architecture and ability to learn complex UI interaction patterns directly from data, though they face challenges in data efficiency and often require substantial computational resources for training on diverse UI environments."
        },
        {
            "title": "3 Methodology",
            "content": "Many prior works have successfully verified that Reinforcement Fine-Tune (RFT) based on Group Relative Policy Optimization (GRPO) algorithm is suitable for UI grounding, where the MLLM answers response that includes the predicted boxes given corresponding objective descriptions. In this work, we further extend GRPO to UI navigation task and prove its effectiveness in UI Agents training compared to merely SFT. We first present the preliminaries of GRPO, followed by comprehensive discussion of the data pipelines, reward design, and learning framework in UI grounding and UI navigation respectively."
        },
        {
            "title": "3.1 Preliminaries",
            "content": "GRPO enhances training stability by estimating baselines through relative rewards within groups rather than using separate critic model. For each training question Q, GRPO samples rollouts {o1, o2, . . . , oG} from the current policy πθ and computes their corresponding rewards {r1, r2, . . . , rG}. The core idea lies in normalizing rewards within each group to obtain advantages: ˆAi = rimean({r1,r2,...,rG}) JGRPO(πθ) = . The policy is then optimized by maximizing the objective: std({r1,r2,...,rG}) qQ,{oi}G i=1πθold (q)"
        },
        {
            "title": "1\nG",
            "content": "G (cid:88) i=1 1 oi oi (cid:88) t=1 (cid:26) min (cid:20) πθ(oi,tq, oi,<t) πθold (oi,tq, oi,<t) ˆAi, clip (cid:16) πθ(oi,tq, oi,<t) πθold (oi,tq, oi,<t) , 1ϵ, 1+ϵ (cid:17) ˆAi βDKL [πθ πref ] (cid:21)(cid:27) , (1) where ri mean({r1, r2, rG}) std({r1, r2, rG}) where the clipping mechanism prevents excessive policy updates, ϵ controls the clipping range, and the KL divergence term with coefficient β constrains the policy from diverging excessively from the reference model πref. ˆAi = , 5 Figure 2 Executions on typical grounding and navigation tasks of UI-Venus. a) The instruction and the screenshot are needed for UI-Venus-Ground to output the corresponding coordinates; b) For navigation tasks, historical context (thought-action pairs) are essential for UI-Venus-Navi, which will generate the thinking content and model action. The historical context will be updated after each step finished."
        },
        {
            "title": "3.2.1 Data Collection",
            "content": "Preliminarily, we collect UI grounding instances from existing public datasets, including Widget Captioning Cheng et al. (2024), UI RefExp Bai et al. (2021), SeeClick Cheng et al. (2024), ShowUI Lin et al. (2024), and OmniAct Kapoor et al. (2024). They are extracted from different platforms such as mobile, desktop and web to ensure the diversity of training data. As shown in Table 1, we collect about 627k open-source grounding samples and use about 107k for training after carefully data cleaning."
        },
        {
            "title": "Widget Captioning UI RefExp",
            "content": "SeeClick-Web"
        },
        {
            "title": "Sampled training set",
            "content": "# Samples 34k 17k 325k 110k 141k 107k Table 1 GUI grounding data composition. Our training dataset comprises 107k samples from five complementary sources."
        },
        {
            "title": "3.2.2 Data Cleaning",
            "content": "However, according to our observation, approximately 40% of the current open-source grounding data contain significant noise issues, including prompt ambiguity and box shifts. possible cause is that many web and mobile datasets utilize HTML and A11Y (Accessibility) source code, respectively. Although these source codes contain the bounding boxes and their corresponding descriptive instructions required for grounding tasks, some boxes may have mismatched offsets after page rendering. In severe cases, all boxes in one entire image may shift simultaneously in one certain 6 direction. Additionally, due to nested elements in UI components from source code, bounding boxes and instructions may fail to maintain one-to-one correspondence, with many-to-one and one-to-many mappings potentially affecting model training. Since RFT training requires high-quality clean data, we employ manual inspection to ensure the correctness of box-instruction pairs. Specifically, given the original open source datasets (e.g., Seeclick), which are large in scale but contain many redundant or simple samples, we first perform downsampling to create subset by removing the repeated prompts. Then we manually filter out ambiguous prompts, relocate offset boxes, and rephrase unmeaningful instructions. After these steps, we obtain about 107k high-quality training samples, which is shown in Table 1. Failed attempts for automatic grounding data cleaning include: (1) Using open-source models to perform the above filtering and rewriting operations; (2) RFT with hard samples selected by rejection sampling strategies. Although these approaches failed to work, they provide some insight for the UI-agent community."
        },
        {
            "title": "3.2.3 Reward Function",
            "content": "In the UI grounding task, we only use two reward functions, i.e., point-in-box and format reward. Although many approaches Lu et al. (2025b); Tang et al. (2025a); Luo et al. (2025a) use intersectionover-union (IoU) or other smooth rewards, the results are similar according to our experiments. Specifically, the reward function is composed of two components formally: Format Reward. We first check whether the predicted answer string conforms to predefined syntax. Valid answers receive base reward to ensure the model to generate executable and parsable instructions. Point-in-box Reward. Given screenshot and the instruction, the model must predict bounding box that localizes the element, where (xc, yc) denote the box center. Assume that the ground truth is annotated as [x1, y1, x2, y2], then Rpoin-in-box = (cid:40) 1 0 otherwise. if x1 xc x2 and y1 yc y2, Total Reward. Combining all components, the final action-wise reward is computed as: where w1 and w2 control the relative importance of format correctness and location precision. = Rformat w1 + Rpoin-in-box w2, (2) (3)"
        },
        {
            "title": "3.3.1 Data Collection",
            "content": "For UI navigation tasks, we first collect traces from open-source navigation datasets of GUI Odyssey Lu et al. (2025a), Aguvis-Aitz Xu et al. (2024), AndroidControl Li et al. (2024), Aitw Rawles et al. (2023a), and Amex Kapoor et al. (2024) as shown in Table 2. Meanwhile, grounding data plays critical role in improving click operation accuracy for navigation tasks, so we also integrate grounding data into the training process. In addition, to strengthen the models ability to handle long-chain traces and capabilities in cross-lingual generalization, we build custom annotation"
        },
        {
            "title": "Dataset",
            "content": "#Samples #Traces"
        },
        {
            "title": "Single Step",
            "content": "GUI-O AITZ AC Aitw Amex Navi* GD* VQA* 114k 7k 371k 5k 107k - 84k 13k 39k 3k 20k * 14k 2k 42k -"
        },
        {
            "title": "Sampled training set",
            "content": "350k * Table 2 Basic statistics for our collected navigation training data. Note that AC means Android Control dataset while the GUI-O represents GUI-Odyssey. The datasets marked with * means are collected by ourselves. Among these data, we select 350k high-quality samples as our training set. platform that provides approximately 20k samples from several popular Chinese mobile APPs (notated Navi* in Table 2). Besides, unified action space is critical for the integration of data from different sources, enabling the agent to focus on learning actions without being confused by diverse definitions. As shown in Table 3, we follow UI-TARS but modify its action space (e.g., redefine CallUser action for information-retrieval) to better suit the existing open-source navigation training data."
        },
        {
            "title": "Definition",
            "content": "Click at coordinates (x, y). Drag from (x1, y1) to (x2, y2). Click(box=(x, y)) Drag(start=(x1, y1), end=(x2, y2)) Scroll(start=(x1, y1), end=(x2, y2), direction=) Scroll from (x1, y1) to (x2, y2) with specified direction. Type(content=) Launch(app=) Wait() Finished(content=) CallUser(content=) LongPress(box=(x, y)) PressBack() PressHome() PressEnter() PressRecent() Type the specified content. Launch the specified app. Wait for loading. Finish the task, with optional information. Conclude the answer for information-retrieval. Long press at coordinates (x, y). Press the back button. Press the home button. Press the enter button. Press the recent button. Table 3 All actions and their definitions used in UI-Venus. We unify the action space and map all the actions in the existing open-source dataset to this space."
        },
        {
            "title": "3.3.2 Data Pipeline",
            "content": "As noisy data are detrimental to the training process of the model, we develop pipeline to build clean training set that can properly guide the model to learn various instructions, which includes three stages: (1) Data Filtering to reintegrate existing data, (2) Trace Reconstruction to modify existing traces, and (3) Iteratively Trace Generation to produce more high-quality traces beyond existing data. After data cleaning, we finally obtain about 350K samples for UI navigation training. Data Filtering. In this stage, we preliminarily filter the collected data by removing overly short traces and standardizing the direction definition of scroll operations among different datasets. In addition, there are some inconsistencies between actions and tasks, e.g., there are traces that may contain more or less operations to complete tasks, or even not follow the requirements of tasks. To filter out these invalid traces, we utilize MLLM to summarize each action and integrate the summaries to obtain an overall description for each trace, which could be compared with the original task. Finally, we categorize the traces based on the apps and subtasks they involve and resample 8 them to ensure the diversity of the training data, preventing the model from overfitting to specific scenarios without sufficient exploration on diverse tasks. Trace Reconstruction. Among existing datasets, the information retrieval tasks Rawles et al. (2025), an important category of UI navigation tasks, often lack an explicit answer in the final step of the traces. For example, when users make request of What is the weather today? or What is the total price in my shopping cart now?, they usually expect an answer from the agent, rather than just being navigated to some pages and finding the answer by themselves. To bridge this gap, we select traces of information retrieval tasks from filtered data, and then adopt the MLLM to generate corresponding answers based on the last screenshots of these traces. Finally, we reconstruct the original trace by inserting CallUser step with generated answer before the Finish step, requiring the agent to report the final answer before ending an information retrieval task. Iteratively Trace Generation. Besides utilizing existing open-source trajectory data, we also design an automated framework to iteratively generate high-quality traces built upon the virtual cloud environment, which contains dozens of available mobiles. We adopt our well-trained UI-Venus model to generate trajectories on real-world tasks including hand-designed and MLLM-generated instructions to ensure their diversity in Chinese and English mobile applications. The noisy and invalid traces would be discarded by combining following steps: (1) Rule-based filtering: empirical rules are defined to remove typical errors (e.g., short trajectory with abnomal exit, the repeating invalid actions); (2) ORM-based filtering: we trained an outcome reward model (ORM) regarding the whole trajectory as single example to score on the generated traces and remove those with low scores; (3) Annotator-based filtering: Annotators are required to strictly select correct traces from the remaining ones. Also, to learn from failures, the fault actions would be fixed by annotators and added to the train set with its valid trace prefix. The above process helps iteratively optimize our model for complex real-world scenarios."
        },
        {
            "title": "3.3.3 Trajectory History Alignment and Sparse Action Enhancement",
            "content": "Accurate historical context is critical for successful task execution in dynamic UI navigation planning. By leveraging historical information, the agent model can understand the steps already taken, perceive the state transitions Qin et al. (2025); Chae et al. (2025); Sun et al. (2025c), and engage in self-reflection Shinn et al. (2023); Wu et al. (2025a); Li et al. (2025c). In our approach, the historical context is provided as thought-action pairs, where the thought represents the reasoning process behind taking particular action at each step, and the action is the actual executed operation. Specifically, when predicting the n-th step, historical context from the previous 1 steps is formulated as: Hn1 = (cid:2)(t1, a1), (t2, a2), . . . , (tn1, an1)(cid:3). (4) The context Hn1, together with the task description and the current UI screenshot, forms the input to the agent model for the n-th step prediction. However, in practice, the historical thoughts (t1, . . . , tn1) are often not well aligned with the agent models intrinsic reasoning capability Li et al. (2025a), due to inconsistencies in style, detail, and abstraction level across data. This mismatch reduces the utility of Hn1, sometimes causing confusion in decision-making. To address this issue, we propose self-evolving Tao et al. (2024) history alignment mechanism that 9 refines historical thoughts between training epochs, progressively aligning the reasoning traces (i.e., the thought sequence in Hn1) with the models evolving decision patterns. By dynamically adjusting the trajectory history to better reflect the models current reasoning behavior, the mechanism provides more coherent and consistent historical context, resulting in sustained improvements in navigation performance. Additionally, we observe that the action distribution in the training data significantly impacts the model performance Qi et al. (2025). Particularly for sparse actions (e.g., LongPress), the agent model often struggles to learn these actions effectively due to their low frequency in the data. In certain complex trajectories, these actions can be pivotal, and missing them may cause the entire task to fail. To overcome the limitation, our method also enhances the sampling of sparse actions, allowing the agent to better acquire and generalize these rare but critical skills. The sampling strategy helps the model develop more transferable understanding of both the UI states and the planned actions, which improves its robustness in complex and dynamic task scenarios. To elaborate on these enhancements, we now provide detailed description of the two key components of our approach. Figure 3 illustrates the overall framework of the proposed Self-Evolving Trajectory History Alignment & Enhancement method. Figure 3 The overview of the proposed Self-Evolving Trajectory History Alignment & Enhancement process, applied between training epochs. The process consists of two key components: 1) Trajectory History Alignment refines the historical context for each trajectory step. The model executes multiple rollouts to generate candidate thoughtaction pairs, applying an Action Exact Match filter to retain only those whose predicted actions match ground-truth actions. The corresponding thoughts are collected in pool and subsequently replace the original thoughts in the historical context, creating an optimized trajectory history for the next training epoch. 2) Sparse Action Enhancement focuses on samples with sparse actions. Multiple variants are constructed by combining different rollout generated thoughts that lead to the same sparse action, effectively increasing the representation of these sparse but critical operations in the training distribution. Trajectory History Alignment. In our framework, the agent model leverages historical context composed of thoughtaction pairs from previous steps, where each thought describes the reasoning process behind the corresponding action. Explicitly incorporating the past chain-of-thought (CoT) Wei et al. (2023) in this historical context can improve planning quality in sequential decisionmaking tasks by revealing the latent reasoning behind each action. However, existing UI navigation datasets with high-quality CoT labels are both scarce and heterogeneous. These annotations are collected from diverse sources, including crowd-sourced workers Rawles et al. (2023b), expert 10 demonstrations Zhang et al. (2024b), and synthetic generation Sun et al. (2025b), which leads to substantial variation in sequence length, linguistic style, and level of reasoning detail. As historical thoughts play crucial role in guiding the agent models planning, cross-source variability can introduce inconsistencies in its perception for the navigation history, ultimately degrading decision quality. In particular, shorter thought sequences may miss critical intermediate observations, while overly verbose ones may obscure key decision-making milestones. To address these challenges, we introduce self-evolving trajectory history alignment mechanism that iteratively refines historical thoughts during training. The detailed procedure is presented in Algorithm 1. After each training epoch, we perform global trajectory refinement by re-inferring reasoning traces with the current model. Let = {Tk}K k=1 denote the dataset. Each trajectory Tk is sequence of steps {(xk,n, Hk,n1)}Nk n=1, where xk,n is the UI state (screenshot) at step n, Hk,n1 = (cid:2)(tk,1, ak,1), . . . , (tk,n1, ak,n1)(cid:3) is the historical context from the previous 1 steps. For each step n, given (xk,n, Hk,n1), we perform rollouts: {(t(r) k,n, a(r) k,n)}R r=1 pθ(t, xk,n, Hk,n1). (5) We then filter the results to keep only those whose predicted action matches the ground-truth action: Ck,n = { t(r) k,n a(r) k,n = a(gt) k,n }. (6) All candidates in Ck,n form the thought pool C. For the n-th step, the thoughts in all previous 1 steps are refined by selecting replacement from the corresponding thought pool: k,i = (cid:40) Select(Ck,i), Ck,i = , otherwise, tk,i, = 1, . . . , 1, (7) where Select() denotes selection policy (e.g., choosing the candidate with length closest to target length). k,n1)(cid:3) is then used as input in k,1, a(gt) The updated historical context the next training epoch. This refinement is applied after every epoch, aligning the reasoning traces with the agents evolving policy. This self-evolving process enhances the agent models planning, leading to more robust and coherent navigation performance. k,n1 = (cid:2)(t k,1 ), . . . , (t k,n1, a(gt) Sparse Action Enhancement. The distribution of actions in the training data is often imbalanced (see Figure 4). Common actions such as Click and Scroll appear frequently, while rare actions such as LongPress are much less represented. This imbalance makes it difficult for the model to learn sparse actions effectively. However, these sparse actions often play critical role in complex task completion, and insufficient handling of such actions results in incomplete or erroneous navigation plans. To address this problem, we design sampling strategy that increases learning for sparse actions. During trajectory history alignment, the agent produces thought pool Cn at each step via rollouts. For step (xn, Hn1) involving sparse action (e.g., an {LongPress, CallUser, . . . }), we create historical variants by combining thoughts from the pools. Specifically, for each previous step n1, an1)(cid:3). m=1 Ci and construct (m) = 1, . . . , 1, we sample {t(m) }M n1 = (cid:2)(t(m) , a1), . . . , (t(m) 1 Algorithm 1 Self-Evolving Trajectory History Alignment Require: Dataset = {Tk}K k=1; each trajectory Tk is sequence of steps {(xk,n, Hk,n1)}Nk n=1; ground-truth actions {ak,1:Nk}; model pθ(t, x, H); rollout count Ensure: Updated historical thoughts {tk,1:Nk} 1: for each training epoch do 2: for each trajectory Tk do Initialize thought pools {Ck,i }Nk i=1 for = 1 to Nk do 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: Hk,n1 [(tk,1, ak,1), . . . , (tk,n1, ak,n1)] for = 1 to do k,n, a(r) k,n = a(gt) k,n then Ck,n Ck,n {t(r) k,n) pθ(t, xk,n, Hk,n1) (t(r) if a(r) k,n} end if end for collect candidates for every step rollouts for step action exact-match add to thought pool end for for = 1 to Nk do if Ck,i = then k,i SELECT(Ck,i) end if end for for = 1 to Nk do Hk,n1 [(t end for k,1, a(gt) k,1 ), . . . , (t k,n1, a(gt) k,n1)] select historical thoughts after collecting pools select policy update historical context for next epoch end for 21: 22: end for Conceptually, the augmented set corresponds to the Cartesian product C1 Cn1. This increases both the diversity and frequency of reasoning patterns that lead to sparse actions. For common actions, we do not perform this augmentation. By focusing only on sparse actions, we enhance the models ability to learn these rare behaviors, leading to more accurate navigation reasoning."
        },
        {
            "title": "3.3.4 Action-wise Reward Function",
            "content": "The rule-based reward function is key component in reinforcement learning, especially for tasks where outputs can be verified against well-defined rules. In visual grounding, for example, the reward is often based on the intersection-over-union between the predicted and the ground-truth bounding boxes Shen et al. (2025). While effective for coarse-level correctness, such rewards provide limited guidance for fine-grained UI manipulation. In UI navigation tasks, the agent model is required to select the correct action type (e.g., Click, Scroll) and generate precise action parameters (e.g., target coordinates or input text). This requirement becomes particularly important in complex interfaces, where even small parameter errors can cause task failure. We propose an action-wise reward function tailored for the navigation task, which assesses the output action along multiple dimensions, namely format, action type, coordinate, and content, to provide detailed feedback. It consists of four components: 12 Figure 4 Action type distribution in our mobile training data, showing long-tailed profile with several low-frequency (sparse) actions. Format Reward. We apply format reward to train the model to generate reasoning and action outputs that follow predefined template. This design is motivated by the fact that the reasoning process provides useful historical context for subsequent decision-making. The format reward, denoted as Rformat, checks whether the model output contains the required XML-style tags in the correct order, with the reasoning block enclosed in <think> followed by the action block enclosed in <action> DeepSeek-AI (2025); Qian et al. (2025). Action Type Reward. We adopt the action type reward, Rtype, which compares the predicted action type with the ground-truth action type. The model receives reward of 1 for match and 0 otherwise. This provides direct and interpretable metric to encourage accurate action type prediction Lu et al. (2025b). Coordinate Reward. For actions involving spatial positioning (e.g., Click, Scroll), we adopt coordinate reward, Rcoord, that depends on the pixel-level distance between the predicted and groundtruth coordinates. Prior methods often assess correctness by checking whether this distance is within fixed threshold, which can be somewhat rigid and insufficient for distinguishing near-misses from large deviations. To address this, we introduce stepwise reward strategy that grants higher scores for predictions closer to the target, while still awarding partial credit for reasonably accurate locations. This design encourages the model to incrementally improve its coordinate parameter localization. For point-targeting actions (i.e., Click, LongPress), we adopt stepwise coordinate reward based on the pixel-level distance between the predicted and the ground-truth coordinates: Rcoord = α, 0.5α, 0, < δ2, δ2 < δ1, otherwise. (8) Here, δ1 and δ2 (δ2 < δ1) are distance thresholds that assign higher scores to more accurate predictions, while still granting partial credit for reasonably close locations. 13 For scrolling actions, we incorporate both spatial accuracy at the start and end positions as well as the correctness of the scrolling direction. The reward Rscroll is defined as: Rscroll = 1.5β if dstart, dend < δ3 and direction match, if dstart < δ3 and direction match, β 0.5β if dstart < δ3 or direction match, 0 otherwise. (9) Here, dstart and dend denote the pixel-level distances between the predicted and ground-truth start and end coordinates, respectively, and δ3 is the spatial distance threshold for scroll evaluation. The direction refers to the scrolling orientation (i.e., up, down, left, or right), and direction match indicates that the predicted orientation exactly matches the ground-truth. Content Reward. For actions involving text input (e.g., Type), we compute the reward Rcontent using the token-level F1-score between the predicted and ground-truth text: Rcontent = (cid:40) γ 0 if F1-score 0.5, otherwise. (10) Here, the F1 score captures the precision-recall balance of textual overlap between the prediction and the reference. Total Reward. Combining all components, the final action-wise reward is computed as: = Rformat w1 + (Rtype + Rcoord + Rcontent) w2, where w1 and w2 control the relative importance of structural correctness and action parameter precision. (11)"
        },
        {
            "title": "4 Experiments",
            "content": "4."
        },
        {
            "title": "Implementation Details",
            "content": "We summarize the training hyperparameters in Table 4 and provide the implementation details in the following section."
        },
        {
            "title": "Hyperparameter",
            "content": "rollout batch_size βKL freeze_vit epoch UI-Venus-Ground-7B UI-Venus-Ground-72B UI-Venus-Navi-7B UI-Venus-Navi-72B 8 10 8 8 128 128 256 512 4e-3 4e-3 1e-3 1e-"
        },
        {
            "title": "False\nFalse\nTrue\nTrue",
            "content": "10 1 10 1 Table 4 Training hyperparameter settings used in the experiments of UI-Venus. Grounding. For UI grounding, we train our model with the above-mentioned clean data based on Qwen2.5-VL Bai et al. (2025). Specifically, we set the learning rate as 4 107 and the rollout sample 8, 10 for 7B, 72B model, respectively. The global batch size is 128 and the models are trained with 128 ppu-gpus with the EasyR1 framework Zheng et al. (2025). It takes 1.5 days, 10 days to train 7B, 72B model for about one epoch, respectively. To fully reproduce our benchmark results, we provide our evaluation prompt for grounding in Appendix A.1. 14 Models Closed-source Models GPT-4o (OpenAI, 2024) UI-TARS-1.5 (Seed, 2025b) Seed1.5-VL (Seed, 2025a) General Open-source Models Qwen2.5-VL-7B* (Bai et al., 2025) Qwen2.5-VL-72B* (Bai et al., 2025) GUI-specific Models (SFT) SeeClick-9.6B (Cheng et al., 2024) ShowUI-2B Lin et al. (2024) UGround-7B (Gou et al., 2024) OS-Atlas-7B (Wu et al., 2024) Aguvis-7B (Xu et al., 2024) UI-TARS-7B (Qin et al., 2025) UI-TARS-72B (Qin et al., 2025) JEDI-7B (Xie et al., 2025) GUI-Actor-7B (Wu et al., 2025b) OpenCUA-7B (Wang et al., 2025a) OpenCUA-32B (Wang et al., 2025a) GUI-specific Models (RL) UI-R1-E-3B (Lu et al., 2025b) SE-GUI-7B (Yuan et al., 2025) LPO (Tang et al., 2025c) GUI-G2-7B (Tang et al., 2025a) Phi-Ground-7B-16C-DPO (Zhang et al., 2025a) GTA1-7B (Yang et al., 2025) GTA1-72B (Yang et al., 2025) Ours UI-Venus-Ground-7B UI-Venus-Ground-72B Mobile Desktop Web Text Icon/Widget Text Icon/Widget Text Icon/Widget 26.6 - - 98.3 97.6 78.4 92.1 75.1 95.2 89.3 96.9 94.8 96.9 97.6 - - 98.2 - 97.9 - 96.5 99.0 99.3 99.0 99. 24.2 - - 85.3 88.6 50.7 75.4 84.5 75.8 68.7 89.1 86.3 87.2 88.2 - - 83.9 - 82.9 - 62.0 88.6 92.4 90.0 93.8 24.2 - - 88.7 92.3 70.1 78.9 85.1 90.7 80.6 95.4 91.2 95.9 96.9 - - 94.8 - 95.9 - 90.2 94.9 97.4 97.0 95.9 19.3 - - 58.6 86. 29.3 78.9 61.4 63.6 67.9 85.0 87.9 87.9 85.7 - - 75.0 - 86.4 - 76.4 89.3 89.3 90.7 90.0 12.8 - - 92.7 91.9 55.2 84.2 84.6 90.6 89.3 93.6 91.5 94.4 93.2 - - 93.2 - 95.6 - 93.6 92.3 95.3 96.2 96.2 11.8 - - 81.8 85.2 32.5 61.1 71.9 77.3 70.0 85.2 87.7 84.2 86.7 - - 83.7 - 84.2 - 75.9 86.7 91. 88.7 92.6 Avg 20.1 94.2 95.2 87.7 90.7 55.1 77.3 76.3 84.1 80.5 91.6 90.3 91.7 92.1 92.3 93.4 89.5 90.3 90.5 93.3 83.8 92.4 94. 94.1 95.3 Table 5 Performance comparison on ScreenSpot-V2 dataset. Our UI-Venus-72B achieves state-of-the-art performance, outperforming all baseline methods across mobile, desktop, and web platforms. Note that models with * are reproduced and the means trained from UI-TARS-1.5-7B. Navigation. Based on the filtered navigation training data, the training is conducted with learning rate of 4 107 and rollout sample size of 8. The global batch size is set to 256 and 512 for the 7B and 72B models, respectively, with training conducted on 256 and 512 PPU-GPUs. Training one epoch takes approximately 1 day for the 7B model and 8.5 days for the 72B model. In addition, we provide the user prompt for the navigation task in Appendix A.2, where the model is constrained to output its reasoning and conclusions along with the predicted actions in specified format."
        },
        {
            "title": "4.2 Grounding Benchmarks",
            "content": "We evaluate UI-Venus on five comprehensive GUI grounding benchmarks to assess its ability to associate natural language instructions with corresponding GUI elements, including ScreenSpotV2 Wu et al. (2024), ScreenSpot-Pro Li et al. (2025b), OSWorld-G Xie et al. (2025), UI-Vision Nayak et al. (2025) and CA-GUI Zhang et al. (2025b). During the evaluation, we follow the standard protocol Cheng et al. (2024); Lin et al. (2024), i.e., prediction is considered correct when the center of predicted box falls within the ground truth bounding box. In the experiments, we compare UI-Venus models against various state-of-the-art baselines across different model categories: (1) Closed-source Models: UI-TARS-1.5 Seed (2025b), Seed1.5- (2) VL Seed (2025a), GPT-4o OpenAI (2024), and Claude Computer Use Anthropic (2024). 15 CAD Dev Creative Scientific Office OS Avg. Text Icon Text Icon Text Icon Text Icon Text Icon Text Icon Text Icon Avg. Model Closed-source Models GPT-4o (OpenAI, 2024) Claude Computer Use (Anthropic, 2024) UI-TARS-1.5 (Seed, 2025b) Seed1.5-VL (Seed, 2025a) General Open-source Models Qwen2.5-VL-7B* (Bai et al., 2025) Qwen2.5-VL-72B* (Bai et al., 2025) GUI-specific Models (SFT) SeeClick-9.6B (Cheng et al., 2024) FOCUS-2B (Tang et al., 2025b) CogAgent-18B (Hong et al., 2024) Aria-UI (Yang et al., 2024) OS-Atlas-7B (Wu et al., 2024) ShowUI-2B (Lin et al., 2024) UGround-7B (Gou et al., 2024) UGround-V1-7B (Gou et al., 2024) UI-TARS-7B (Qin et al., 2025) UI-TARS-72B (Qin et al., 2025) JEDI-7B (Xie et al., 2025) GUI-Actor-7B (Wu et al., 2025b) OpenCUA-7B (Wang et al., 2025a) OpenCUA-32B (Wang et al., 2025a) 2.0 14.5 - - 0.0 3.7 - - 1.3 22.0 - - 0.0 3.9 - - 1.0 25.9 - - 0.0 3.4 - - 2.1 33.9 - - 0.0 15.8 - - 16.8 46.8 1.6 54.8 15.6 65.6 4.1 16.6 35.9 49.3 7.7 63.1 19.6 78.5 7.3 34.5 0.6 0.0 2.5 22.8 3.1 7.6 14.9 3.1 7.1 16.2 1.6 7.6 33.1 4.7 12.2 16.9 0.0 2.5 26.6 1.6 14.2 51.9 1.2 15.8 20.8 58.4 9.4 18.8 12.5 62.9 38.0 14.1 42.9 - - - - - - - - - 0.0 1.7 0.7 0.0 1.4 1.4 2.1 2.8 12.4 17.2 11.0 - - - 3.5 0.0 1.0 25.0 1.7 23.7 22.2 0.0 9.6 27.1 2.1 23.7 37.5 2.8 28.8 13.2 0.0 9.1 31.9 2.8 27.3 57.6 9.7 47.5 50.0 63.9 9.1 57.1 15.4 64.6 50.0 11.9 72.9 - - - - - - - - - GUI-specific Models (RL) 37.1 12.5 46.1 UI-R1-E-3B (Lu et al., 2025b) 23.9 49.4 6.3 GUI-R1-7B (Luo et al., 2025a) 33.0 14.1 51.3 InfiGUI-R1-3B (Liu et al., 2025) 39.6 50.7 9.4 GUI-G1-3B (Zhou et al., 2025) 51.3 42.2 68.2 SE-GUI-7B (Yuan et al., 2025) Phi-Ground-7B-16C-DPO (Zhang et al., 2025a) 26.9 17.2 70.8 GUI-G2-7B (Tang et al., 2025a) 55.8 12.5 68.8 UI-TARS-1.5-7B (Seed, 2025b) - GTA1-7B (Yang et al., 2025) 53.3 17.2 66.9 56.9 28.1 GTA1-72B (Yang et al., 2025) 6.9 4.8 12.4 10.3 19.3 16.7 17.2 - 20.7 79.9 33.1 - - 56.9 4.2 41.9 55.6 8.4 38.9 44.9 58.3 7.0 36.6 11.9 61.8 75.0 9.1 57.6 56.6 13.3 58.0 57.1 15.4 77.1 - 62.6 18.2 76.4 73.2 20.3 81.9 - - 1.1 0.0 0.0 30.1 16.3 11.0 - - - - - - 0.0 4.5 - - 1.3 23.4 - - 0.0 7.1 - - 0.8 17.1 61.6 60.9 52.5 20.8 37.4 26.8 6.7 79.1 47.2 66.4 29.2 67.3 25.0 51.2 38.9 7.1 0.0 3.9 0.8 2.0 4.0 2.6 2.8 8.1 1.8 19.8 12.0 17.1 28.1 10.8 25.0 45. 1.1 0.0 2.8 0.0 1.1 13.3 2.5 17.8 7.7 23.2 7.7 0.0 5.6 0.0 13.0 11.3 0.0 4.7 1.9 20.3 18.9 4.5 27.1 5.7 33.9 7.7 2.2 15.3 10.3 7.5 16.5 0.0 31.6 11.3 17.8 60.5 13.2 38.3 31.1 7.9 63.3 20.8 30.8 16.9 47.8 16.2 35.7 63.3 26.4 42.1 15.7 50.9 17.6 38.1 75.1 47.2 33.6 16.9 52.6 18.2 39.5 44.6 - 50.0 - 55.3 - - - - - - - - - - - - - - - - - - - - 33.5 - 65.0 26.4 32.7 10.1 58.7 26.4 42.1 16.9 65.5 28.3 43.9 12.4 49.1 14.1 35.7 67.2 32.1 23.5 10.6 49.5 16.8 37.1 78.5 43.4 49.5 25.8 63.5 21.0 47.3 76.4 44.0 55.1 25.8 56.4 21.8 43.2 74.0 32.7 57.9 21.3 64.7 19.6 47.5 49.6 - 82.5 50.9 48.6 25.9 65.5 25.2 50.1 49.1 73.8 39.1 74.5 32.5 58.4 85.3 - - - - - 0.0 7.1 1.8 6.4 7.3 7.3 2.7 14.5 31.8 20.9 25.5 - - - 21.8 11.8 20.0 30.0 28.2 29.1 24.5 - 31.8 38.2 Ours UI-Venus-Ground-7B UI-Venus-Ground-72B 60.4 21.9 74.7 66.5 29.7 84.4 24.1 33.1 63.1 14.7 76.4 73.2 30.8 84.7 31.8 42.7 75.7 41.5 49.5 22.5 67.1 24.3 50.8 83.1 60.4 75.7 36.0 77.4 36.8 61.9 Table 6 Performance comparison of different agent models across various task categories based on Text, Icon, and Average scores on ScreenSpot-Pro. Note that models with * are reproduced and the means trained from UI-TARS-1.5-7B. General Open-source Models: Qwen2.5-VL-7B/72B Bai et al. (2025) and InternVL2.5 Chen et al. (2024). (3) GUI-specific SFT Models: UI-TARS-7B/72B Qin et al. (2025), OS-Atlas-7B Wu et al. (2024), Jedi-7B Xie et al. (2025), and GUI-Actor-7B Wu et al. (2025b). (4) GUI RL Models: GTA1-7B/72B Yang et al. (2025), SE-GUI-7B Yuan et al. (2025), and UI-TARS-1.5-7B Seed (2025b). Methods marked with * indicate our own reproductions, some of which achieve superior performance compared to their original reported results. ScreenSpot-V2. As general GUI grounding benchmark across mobile, web and desktop platforms with text and icon/widget elements, this benchmark tests the basic grounding capabilities in daily GUI-related scenarios. From Table 5, it can be observed that though previous models have already obtained high scores, UI-Venus-Ground-72B further pushes the performance boundary with 95.3% average score and exhibits superior grounding ability in diverse scenarios of daily life. Notably, UIVenus-Ground-7B, with significantly fewer parameters, can still achieve 94.1% and outperform larger models like UI-TARS-72B or OpenCUA-32B, which only score 90.3% and 93.4%, respectively. 16 Models Closed-source Models Operator (OpenAI, 2025) Gemini-2.5-Pro (Team, 2024) Seed1.5-VL (Seed, 2025a) Open-source Models OS-Atlas-7B (Wu et al., 2024) Qwen2.5-VL-7B (Bai et al., 2025) Qwen2.5-VL-72B (Bai et al., 2025) UGround-7B (Gou et al., 2024) Aguvis-7B (Xu et al., 2024) Jedi-7B (Xie et al., 2025) UI-TARS-7B (Qin et al., 2025) UI-TARS-1.5-7B (Seed, 2025b) UI-TARS-72B (Qin et al., 2025) GTA1-7B (Yang et al., 2025) GTA1-72B (Yang et al., 2025) OpenCUA-7B (Wang et al., 2025a) OpenCUA-32B (Wang et al., 2025a) Ours UI-Venus-Ground-7B UI-Venus-Ground-72B Text Matching Element Recognition Layout Understanding Fine-grained Manipulation Refusal Avg 51.3 59.8 73.9 44.1 45.6 52.6 51.3 55.9 65.9 60.2 52.6 69.4 63.2 57.9 - - 74.6 82.1 42.4 45.5 66.7 29.4 32.7 74.6 40.3 41.2 55.5 51.8 75.4 60.6 82.1 76.9 - - 60.5 71.2 46.6 49.0 69.6 35.2 41.9 74.7 43.5 43.9 57.7 54.9 72.4 62.9 74.2 77.3 - - 61.5 70.7 31.5 33.6 47.0 16.8 18.1 55.3 24.8 28.2 46.9 35.6 66.7 45.6 70.5 66.7 - - 45.5 64.4 - 38.9 18.5 7.4 - - - - 7.4 - - - - - - - - - 40.6 45.2 62.9 27.7 31.4 62.2 36.4 38.7 54.1 47.5 64.2 57.1 67.7 66.7 55.3 59. 58.8 70.4 Table 7 Performance comparison on OS-World-G grounding dataset. Our UI-Venus-72B achieves state-of-the-art performance, outperforming all baseline methods across different settings. ScreenSpot-Pro. With high-resolution professional software interfaces, including CAD, development, creative, scientific, office, and OS applications, the difficulty of element grounding is greatly increased. Nevertheless, as shown in Table 6, UI-Venus-Ground-72B can achieve comprehensive leadership with 61.9% score. Compared to text elements, we can find in professional software, icon elements are much harder to detect and locate due to their diverse and small shapes, which requires the models understanding of GUI layouts and fine-grained coordinate perception, while UI-Venus-Ground-72B still makes breakthrough in icon grounding for software of Creative, Scientific and Office categories, with improvements over previous optimal model GTA1-72B of 10.5%, 4.5% and 9.5%, respectively. OSWorld-G. This benchmark is composed of fine-grained tasks from real computer environment with diverse capabilities including text matching, element recognition, layout understanding, fine-grained manipulation and refusal handling. The result in Table 7 demonstrates our models exceptional performance in such tasks, where 72B model achieves 70.4% score, substantially surpassing strong baselines like UI-TARS-1.5-7B and GTA1 series. UI-Vision. UI-Vision provides fine-grained evaluation of computer-using agents in real-world desktop environments, serving as comprehensive, license-permissive benchmark. In Table 8, we can observe UI-Venus-Ground-72B establishes new state-of-the-art performance across three task categories, while UI-Venus-Ground-7B performs on par with previous best model Phi-Ground7B-16C-DPO, indicating our model has made significant progress in grounding capability under real-world environments. CA-GUI. We also adopt multilingual evaluation with realistic Chinese mobile applications, featuring Fun2Point and Text2Point tasks, and the results are exhibited in Table 9. In this benchmark, our model demonstrates strong cross-lingual generalization capabilities in non-English GUI environments, markedly exceeding AgentCPM-GUI by 5% in Fun2Point task and by 9.4% in Text2Point 17 Models Basic Functional Spatial Avg Closed-source Models GPT-4o (OpenAI, 2024) Claude-3.7-Sonnet (Anthropic, 2024) Open-source Models Qwen2.5-VL-7B (Bai et al., 2025) SeeClick (Cheng et al., 2024) UGround-V1-7B (Gou et al., 2024) OS-Atlas-7B (Wu et al., 2024) UI-TARS-7B (Qin et al., 2025) UI-TARS-1.5-7B (Seed, 2025b) UI-TARS-72B (Qin et al., 2025) Phi-Ground-7B-16C-DPO (Zhang et al., 2025a) Ours UI-Venus-Ground-7B UI-Venus-Ground-72B 1.6 9.5 1.2 9.4 15.4 12.2 20.1 28.8 31.4 36.8 36.1 45. 1.5 7.7 0.8 4.7 17.1 11.2 24.3 27.5 30.5 37.1 32.8 42.3 1.0 7.6 0.5 2.1 6.3 3.7 8.4 10.7 14.7 7.6 11.9 23. 1.38 8.3 0.9 5.4 12.9 9.0 17.6 22.3 25.5 27.2 26.5 36.8 Table 8 Performance comparison on UI-Vision grounding dataset. Our UI-Venus-Ground-72B achieves state-of-the-art performance, outperforming all baseline methods across different settings. Models Fun2Point Text2Point Avg Closed-source Models GPT-4o (OpenAI, 2024) GPT-4o grounding (OpenAI, 2024) Open-source Models Intern2.5-VL-8B (Chen et al., 2024) Intern2.5-VL-26B (Chen et al., 2024) Qwen2.5-VL-7B (Bai et al., 2025) OS-Genesis-7B (Sun et al., 2025a) OS-Atlas-7B (Wu et al., 2024) Aguvis-7B (Xu et al., 2024) UI-TARS-7B (Qin et al., 2025) AgentCPM-GUI (Zhang et al., 2025b) Ours UI-Venus-Ground-7B UI-Venus-Ground-72B 22.1 44. 17.2 14.8 59.8 8.3 53.6 60.8 56.8 79.1 83.3 84.1 19.9 44.0 24.2 16.6 59.3 5.8 60.7 76.5 66.7 76.5 83.2 85.9 21.0 44. 20.7 15.7 59.6 7.1 57.2 68.7 61.8 77.8 83.3 85.0 Table 9 Performance comparison on CA-GUI grounding dataset. Our UI-Venus-72B achieves state-of-the-art performance, outperforming all baseline methods across different settings. tasks, and even UI-Venus-Ground-7B shows great advantages. In summary, our evaluation reveals several key insights: (1) Consistent SOTA Performance: UI-Venus achieves new state-of-the-art results across all benchmarks, with UI-Venus-Ground-72B reaching 95.3% on ScreenSpot-V2, 61.9% on ScreenSpot-Pro, and 85.0% on CA-GUI, significantly outperforming previous best models including GTA1-72B (94.8%, 58.4%, -) and UI-TARS-1.5 (95.2%, 61.6%, -). (2) Superior Fine-grained Capabilities: On OSWorld-G, our model demonstrates exceptional performance in fine-grained manipulation tasks, substantially surpassing strong baselines like UI-TARS-1.5-7B and GTA1 series. (3) Robust Multilingual Grounding: UI-Venus shows substantial improvements on the Chinese benchmark CA-GUI, achieving 83.2% average accuracy and demonstrating strong cross-lingual generalization capabilities in non-English GUI environments. Models With Planner A11y Tree Screenshot Success Rate(pass@1) Closed-source Models GPT-4o (OpenAI, 2024) ScaleTrack (Huang et al., 2025) SeedVL-1.5 (Seed, 2025a) UI-TARS-1.5 (Seed, 2025b) Open-source Models GUI-Critic-R1-7B (Wanyan et al., 2025) Qwen2.5-VL-72B (Bai et al., 2025) UGround (Gou et al., 2024) Aria-UI (Yang et al., 2024) UI-TARS-72B (Qin et al., 2025) GLM-4.5v (Zhipu-AI, 2025) Ours UI-Venus-Navi-7B UI-Venus-Navi-72B (cid:36) (cid:36) (cid:36) (cid:36) (cid:36) (cid:36) (cid:34) (cid:34) (cid:36) (cid:36) (cid:36) (cid:36) (cid:34) (cid:34) (cid:34) (cid:36) (cid:34) (cid:36) (cid:36) (cid:36) (cid:36) (cid:36) (cid:36) (cid:36) (cid:36) (cid:36) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) 30.6 44.0 62.1 64. 27.6 35.0 44.0 44.8 46.6 57.0 49.1 65.9 Table 10 Performance comparison on AndroidWorld for end-to-end models. Our UI-Venus-Navi-72B achieves state-of-the-art performance, outperforming all baseline methods across different settings."
        },
        {
            "title": "4.3 Navigation Benchmarks",
            "content": "We evaluate UI-Venus on three widely-adopted benchmarks: AndroidWorld Rawles et al. (2025), AndroidControl Li et al. (2024), and GUI-Odyssey Lu et al. (2025a) datasets to assess its multi-step decision-making capabilities across diverse mobile interface scenarios. Methods marked with * indicate our own reproductions, some of which achieve superior performance compared to their original reported results. Online Benchmark. For real-time multi-step decision-making evaluation, we adopt AndroidWorld, dynamic benchmark requiring continuous interaction with live mobile applications. This framework assesses the models ability to adapt strategies based on real-time feedback and maintain task coherence across extended interaction sequences. From Table 10, we can see UI-Venus is implemented without relying on additional planner or A11y tree and is able to conduct end-to-end UI navigation purely based on screenshots, which ensures strong generalization capability in real-time interactive scenarios. In AndroidWorld benchmark, UI-Venus-Navi-72B gains score of 65.9% success rate in one-time evaluation, surpassing UI-TARS-1.5 (64.2%), and UI-Venus-Navi-7B also succeeds with 49.1% rate to outperform UI-TARS-72B (46.6%), demonstrating the effectiveness of UI-Venus. Offline Benchmark. Besides, we employ two established offline benchmarks: AndroidControl and GUI-Odyssey. These static evaluation scenarios assess the models fundamental UI understanding, task decomposition, and action planning capabilities in controlled environments. With evaluation results in Table 11, where Low/High categories in AndroidControl represent the low/high level instruction the agent is prompted with, respectively, we can observe that UI-Venus produces comparable results with previous optimal methods. Notably, in AndroidControl-High evaluation, UI-Venus performs best in both type accuracy and step success rate, indicating that UI-Venus possesses stronger planning and summary capability with high-level task instruction. In summary, our evaluation reveals several key insights: (1) SOTA Online Navigation Performance: UI-Venus achieves new state-of-the-art end-to-end model results on AndroidWorld, with UI-VenusNavi-72B reaching 65.9% success rate, significantly outperforming previous best models including 19 Models Closed-source Models GPT-4o (OpenAI, 2024) Open-source Models Qwen2.5-VL-7B (Bai et al., 2025) SeeClick (Cheng et al., 2024) OS-Atlas-7B (Wu et al., 2024) Aguvis-7B (Xu et al., 2024) Aguvis-72B (Xu et al., 2024) OS-Genesis-7B (Xu et al., 2024) UI-TARS-7B (Qin et al., 2025) UI-TARS-72B (Qin et al., 2025) GUI-R1-7B (Luo et al., 2025a) NaviMaster-7B (Luo et al., 2025b) UI-AGILE-7B (Lian et al., 2025) AgentCPM-GUI (Zhang et al., 2025b) Ours UI-Venus-Navi-7B UI-Venus-Navi-72B AndroidControl-Low AndroidControl-High GUI-Odyssey Type Acc. Step SR Type Acc. Step SR Type Acc. Step SR 74.3 94.1 93.0 93.6 - - 90.7 98.0 98.1 85.2 85.6 87.7 94.4 97.1 96.7 19.4 85.0 75.0 85.2 80.5 84.4 74.2 90.8 91.3 66.5 69.9 77.6 90.2 92.4 92. 66.3 75.1 82.9 85.2 - - 66.2 83.7 85.2 71.6 72.9 80.1 77.7 86.5 85.9 20.8 62.9 59.1 71.2 61.5 66.4 44.5 72.5 74.7 51.7 54.0 60.6 69.2 76.1 77. 34.3 59.5 71.0 84.5 - - - 94.6 95.4 65.5 - - 90.0 87.3 87.2 3.3 46.3 53.9 62.0 - - 87.0 88.6 38.8 - - 75. 71.5 72.4 Table 11 Performance comparison on offline UI navigation datasets including AndroidControl and GUI-Odyssey. UI-TARS-72B (46.6%) and UI-TARS-1.5 (64.2%). We also show some case studies in Sec. B.2 and open-source the all evaluation traces of AndroidWorld to better show the effectiveness of our UI-Venus. (2) Comparable Offline Navigation Performance: On AndroidControl and GUIOdyssey, our UI-Venus also achieves comparable results with previous SOTA methods. Note that UI-Venus obtains best performance on AndroidControl-High, reflecting its remarkable generalization performance of planning and summary in long traces."
        },
        {
            "title": "5 Future Work",
            "content": "Although the GRPO algorithm has proven to be more effective post-training strategy for UI agents than conventional supervised fine-tuning, certain challenges remain. One notable issue is the hallucination gap between the models internal reasoning (think) and its final response (answer), which can lead to incorrect or inconsistent behavior in navigation tasks. Another observation is that even humans may struggle to operate unfamiliar applications without prior exposure. This suggests that large-scale pretraining on trajectories from diverse UI agents can equip models with richer prior knowledge of various applications and to enhance their adaptability. Thus, future research may explore integrating pretraining with curated UI-agent traces, refining action generation through enhanced reasoning alignment, and incorporating domain-specific priors. Such advancements would help mitigate these issues and enable broader deployment of UI agents across diverse and dynamic computing environments."
        },
        {
            "title": "References",
            "content": "Droidrun. https://github.com/droidrun/droidrun, 2025. Saaket Agashe, Jiuzhou Han, Shuyu Gan, Jiachen Yang, Ang Li, and Xin Eric Wang. Agent s: An open agentic framework that uses computers like human. arXiv preprint arXiv:2410.08164, 2024. Anthropic. Claude computer use. Available at: https://www.anthropic.com/news/developing-computer-use, 2024. Chongyang Bai, Xiaoxue Zang, Ying Xu, Srinivas Sunkara, Abhinav Rastogi, Jindong Chen, and Blaise Aguera Arcas. Uibert: Learning generic multimodal representations for ui understanding, 2021. https://arxiv.org/abs/2107.13731. Hao Bai, Yifei Zhou, Mert Cemri, Jiayi Pan, Alane Suhr, Sergey Levine, and Aviral Kumar. Digirl: Training in-the-wild device-control agents with autonomous reinforcement learning, 2024. https://arxiv.org/abs/2406.11896. Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: versatile vision-language model for understanding, localization, text reading, and beyond, 2023. https://arxiv.org/abs/2308. 12966. Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin. Qwen2.5-vl technical report, 2025. https://arxiv.org/abs/2502.13923. Hyungjoo Chae, Namyoung Kim, Kai Tzu iunn Ong, Minju Gwak, Gwanwoo Song, Jihoon Kim, Sunghwan Kim, Dongha Lee, and Jinyoung Yeo. Web agents with world models: Learning and leveraging environment dynamics in web navigation. In The Thirteenth International Conference on Learning Representations, 2025. https://openreview.net/forum?id=moWiYJuSGF. Hardy Chen, Haoqin Tu, Fali Wang, Hui Liu, Xianfeng Tang, Xinya Du, Yuyin Zhou, and Cihang Xie. Sft or rl? an early investigation into training r1-like reasoning large vision-language models, 2025. https://arxiv.org/abs/2504.11468. Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, et al. Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling. arXiv preprint arXiv:2412.05271, 2024. Kanzhi Cheng, Qiushi Sun, Yougang Chu, Fangzhi Xu, Yantao Li, Jianbing Zhang, and Zhiyong Wu. Seeclick: Harnessing gui grounding for advanced visual gui agents, 2024. https://arxiv.org/abs/2401.10935. DeepSeek-AI. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. https://arxiv.org/abs/ 2501.12948. Lang Feng, Weihao Tan, Zhiyi Lyu, Longtao Zheng, Haiyang Xu, Ming Yan, Fei Huang, and Bo An. Towards efficient online tuning of vlm agents via counterfactual soft reinforcement learning, 2025. https://arxiv.org/abs/2505.03792. Minghe Gao, Wendong Bu, Bingchen Miao, Yang Wu, Yunfei Li, Juncheng Li, Siliang Tang, Qi Wu, Yueting Zhuang, and Meng Wang. Generalist virtual agents: survey on autonomous agents across digital platforms. arXiv preprint arXiv:2411.10943, 2024. Boyu Gou, Ruohan Wang, Boyuan Zheng, Yanan Xie, Cheng Chang, Yiheng Shu, Huan Sun, and Yu Su. Navigating the digital world as humans do: Universal visual grounding for gui agents, 2024. https://arxiv.org/abs/2410.05243. Zhangxuan Gu, Zhuoer Xu, Haoxing Chen, Jun Lan, Changhua Meng, and Weiqiang Wang. Mobile user interface element detection via adaptively prompt tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1115511164, 2023. Wenyi Hong, Weihan Wang, Qingsong Lv, Jiazheng Xu, Wenmeng Yu, Junhui Ji, Yan Wang, Zihan Wang, Yuxuan Zhang, Juanzi Li, Bin Xu, Yuxiao Dong, Ming Ding, and Jie Tang. Cogagent: visual language model for gui agents, 2024. https: //arxiv.org/abs/2312.08914. Xueyu Hu, Tao Xiong, Biao Yi, Zishu Wei, Ruixuan Xiao, Yurun Chen, Jiasheng Ye, Meiling Tao, Xiangxin Zhou, Ziyu Zhao, et al. Os agents: survey on mllm-based agents for computer, phone and browser use, 2024. Jing Huang, Zhixiong Zeng, Wenkang Han, Yufeng Zhong, Liming Zheng, Shuai Fu, Jingyuan Chen, and Lin Ma. Scaletrack: Scaling and back-tracking automated gui agents. arXiv preprint arXiv:2505.00416, 2025. Peter Humphreys, David Raposo, Tobias Pohlen, Gregory Thornton, Rachita Chhaparia, Alistair Muldal, Josh Abramson, Petko Georgiev, Adam Santoro, and Timothy Lillicrap. data-driven approach for learning to control computers. In International Conference on Machine Learning, pages 94669482. PMLR, 2022. 21 Raghav Kapoor, Yash Parag Butala, Melisa Russak, Jing Yu Koh, Kiran Kamble, Waseem Alshikh, and Ruslan Salakhutdinov. Omniact: dataset and benchmark for enabling multimodal generalist autonomous agents for desktop and web, 2024. https: //arxiv.org/abs/2402.17553. Cheryl Li, Tianyuan Xu, and Yiwen Guo. Reasoning-as-logic-units: Scaling test-time reasoning in large language models through logic unit alignment, 2025a. https://arxiv.org/abs/2502.07803. Kaixin Li, Ziyang Meng, Hongzhan Lin, Ziyang Luo, Yuchen Tian, Jing Ma, Zhiyong Huang, and Tat-Seng Chua. Screenspot-pro: Gui grounding for professional high-resolution computer use, 2025b. Ning Li, Xiangmou Qu, Jiamu Zhou, Jun Wang, Muning Wen, Kounianhua Du, Xingyu Lou, Qiuying Peng, Jun Wang, and Weinan Zhang. Mobileuse: gui agent with hierarchical reflection for autonomous mobile operation. arXiv preprint arXiv:2507.16853, 2025c. https://arxiv.org/abs/2507.16853. Wei Li, William Bishop, Alice Li, Chris Rawles, Folawiyo Campbell-Ajala, Divya Tyamagundlu, and Oriana Riva. On the effects of data scale on ui control agents, 2024. https://arxiv.org/abs/2406.03679. Shuquan Lian, Yuhang Wu, Jia Ma, Zihan Song, Bingqi Chen, Xiawu Zheng, and Hui Li. Ui-agile: Advancing gui agents with effective reinforcement learning and precise inference-time grounding, 2025. https://arxiv.org/abs/2507.22025. Kevin Qinghong Lin, Linjie Li, Difei Gao, Zhengyuan Yang, Shiwei Wu, Zechen Bai, Weixian Lei, Lijuan Wang, and Mike Zheng Shou. Showui: One vision-language-action model for gui visual agent, 2024. https://arxiv.org/abs/2411.17465. Yuhang Liu, Pengxiang Li, Congkai Xie, Xavier Hu, Xiaotian Han, Shengyu Zhang, Hongxia Yang, and Fei Wu. Infigui-r1: Advancing multimodal gui agents from reactive actors to deliberative reasoners. 2025. https://arxiv.org/abs/2504.14239. Quanfeng Lu, Wenqi Shao, Zitao Liu, Lingxiao Du, Fanqing Meng, Boxuan Li, Botong Chen, Siyuan Huang, Kaipeng Zhang, and Ping Luo. Guiodyssey: comprehensive dataset for cross-app gui navigation on mobile devices, 2025a. https://arxiv.org/ abs/2406.08451. Zhengxi Lu, Yuxiang Chai, Yaxuan Guo, Xi Yin, Liang Liu, Hao Wang, Han Xiao, Shuai Ren, Guanjing Xiong, and Hongsheng Li. Uir1: Enhancing efficient action prediction of gui agents by reinforcement learning. 2025b. https://arxiv.org/abs/2503.21620. Run Luo, Lu Wang, Wanwei He, and Xiaobo Xia. Gui-r1 : generalist r1-style vision-language action model for gui agents. 2025a. https://arxiv.org/abs/2504.10458. Zhihao Luo, Wentao Yan abd Jingyu Gong, Min Wang, Zhizhong Zhang, Xuhong Wang, Yuan Xie, and Xin Tan. Navimaster: Learning unified policy for gui and embodied navigation tasks, 2025b. https://arxiv.org/abs/2508.02046. Shravan Nayak, Xiangru Jian, Kevin Qinghong Lin, Juan Rodriguez, Montek Kalsi, Rabiul Awal, Nicolas Chapados, Tamer Özsu, Aishwarya Agrawal, David Vazquez, et al. Ui-vision: desktop-centric gui benchmark for visual perception and interaction. arXiv preprint arXiv:2503.15661, 2025. Dang Nguyen, Jian Chen, Yu Wang, Gang Wu, Namyong Park, Zhengmian Hu, Hanjia Lyu, Junda Wu, Ryan Aponte, Yu Xia, et al. Gui agents: survey. arXiv preprint arXiv:2412.13501, 2024. OpenAI. Introducing gpt-4o. Available at: https://openai.com/index/hello-gpt-4o, 2024. OpenAI. Introducing operator. Available at: https://openai.com/index/introducing-operator/, 2025. Zehan Qi, Xiao Liu, Iat Long Iong, Hanyu Lai, Xueqiao Sun, Wenyi Zhao, Yu Yang, Xinyue Yang, Jiadai Sun, Shuntian Yao, Tianjie Zhang, Wei Xu, Jie Tang, and Yuxiao Dong. Webrl: Training llm web agents via self-evolving online curriculum reinforcement learning, 2025. https://arxiv.org/abs/2411.02337. Cheng Qian, Emre Can Acikgoz, Qi He, Hongru Wang, Xiusi Chen, Dilek Hakkani-Tür, Gokhan Tur, and Heng Ji. Toolrl: Reward is all tool learning needs, 2025. https://arxiv.org/abs/2504.13958. Yujia Qin, Yining Ye, Junjie Fang, Haoming Wang, Shihao Liang, Shizuo Tian, Junda Zhang, Jiahao Li, Yunxin Li, Shijue Huang, Wanjun Zhong, Kuanye Li, Jiale Yang, Yu Miao, Woyu Lin, Longxiang Liu, Xu Jiang, Qianli Ma, Jingyu Li, Xiaojun Xiao, Kai Cai, Chuang Li, Yaowei Zheng, Chaolin Jin, Chen Li, Xiao Zhou, Minchao Wang, Haoli Chen, Zhaojian Li, Haihua Yang, Haifeng Liu, Feng Lin, Tao Peng, Xin Liu, and Guang Shi. Ui-tars: Pioneering automated gui interaction with native agents, 2025. https://arxiv.org/abs/2501.12326. Christopher Rawles, Alice Li, Daniel Rodriguez, Oriana Riva, and Timothy Lillicrap. Androidinthewild: large-scale dataset for android device control. Advances in Neural Information Processing Systems, 36:5970859728, 2023a. Christopher Rawles, Alice Li, Daniel Rodriguez, Oriana Riva, and Timothy Lillicrap. Android in the wild: large-scale dataset for android device control, 2023b. https://arxiv.org/abs/2307.10088. 22 Christopher Rawles, Sarah Clinckemaillie, Yifan Chang, Jonathan Waltz, Gabrielle Lau, Marybeth Fair, Alice Li, William Bishop, Wei Li, Folawiyo Campbell-Ajala, Daniel Toyama, Robert Berry, Divya Tyamagundlu, Timothy Lillicrap, and Oriana Riva. Androidworld: dynamic benchmarking environment for autonomous agents, 2025. https://arxiv.org/abs/2405.14573. ByteDance Seed. Seed1.5-vl technical report. arXiv preprint arXiv:2505.07062, 2025a. ByteDance Seed. Ui-tars-1.5. https://seed-tars.com/1.5, 2025b. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models, 2024. https://arxiv.org/ abs/2402.03300. Haozhan Shen, Peng Liu, Jingcheng Li, Chunxin Fang, Yibo Ma, Jiajia Liao, Qiaoli Shen, Zilun Zhang, Kangjia Zhao, Qianqian Zhang, Ruochen Xu, and Tiancheng Zhao. Vlm-r1: stable and generalizable r1-style large vision-language model, 2025. https://arxiv.org/abs/2504.07615. Noah Shinn, Federico Cassano, Edward Berman, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. Reflexion: Language agents with verbal reinforcement learning, 2023. Qiushi Sun, Kanzhi Cheng, Zichen Ding, Chuanyang Jin, Yian Wang, Fangzhi Xu, Zhenyu Wu, Chengyou Jia, Liheng Chen, Zhoumianze Liu, Ben Kao, Guohao Li, Junxian He, Yu Qiao, and Zhiyong Wu. Os-genesis: Automating gui agent trajectory construction via reverse task synthesis, 2025a. https://arxiv.org/abs/2412.19723. Qiushi Sun, Kanzhi Cheng, Zichen Ding, Chuanyang Jin, Yian Wang, Fangzhi Xu, Zhenyu Wu, Chengyou Jia, Liheng Chen, Zhoumianze Liu, Ben Kao, Guohao Li, Junxian He, Yu Qiao, and Zhiyong Wu. Os-genesis: Automating gui agent trajectory construction via reverse task synthesis, 2025b. https://arxiv.org/abs/2412.19723. Yuchen Sun, Shanhui Zhao, Tao Yu, Hao Wen, Samith Va, Mengwei Xu, Yuanchun Li, and Chongyang Zhang. Gui-xplore: Empowering generalizable gui agents with one exploration, 2025c. https://arxiv.org/abs/2503.17709. Weihao Tan, Wentao Zhang, Xinrun Xu, Haochong Xia, Ziluo Ding, Boyu Li, Bohan Zhou, Junpeng Yue, Jiechuan Jiang, Yewen Li, et al. Cradle: Empowering foundation agents towards general computer control. arXiv preprint arXiv:2403.03186, 2024. Fei Tang, Zhangxuan Gu, Zhengxi Lu, Xuyang Liu, Shuheng Shen, Changhua Meng, Wen Wang, Wenqi Zhang, Yongliang Shen, Weiming Lu, Jun Xiao, and Yueting Zhuang. Gui-g2: Gaussian reward modeling for gui grounding, 2025a. https: //arxiv.org/abs/2507.15846. Fei Tang, Yongliang Shen, Hang Zhang, Siqi Chen, Guiyang Hou, Wenqi Zhang, Wenqiao Zhang, Kaitao Song, Weiming Lu, and Yueting Zhuang. Think twice, click once: Enhancing gui grounding via fast and slow systems. 2025b. https://arxiv.org/abs/ 2503.06470. Jiaqi Tang, Yu Xia, Yi-Feng Wu, Yuwei Hu, Yuhui Chen, Qing-Guo Chen, Xiaogang Xu, Xiangyu Wu, Hao Lu, Yanqing Ma, Shiyin Lu, and Qifeng Chen. Lpo: Towards accurate gui agent interaction via location preference optimization, 2025c. https: //arxiv.org/abs/2506.09373. Zhengwei Tao, Ting-En Lin, Xiancai Chen, Hangyu Li, Yuchuan Wu, Yongbin Li, Zhi Jin, Fei Huang, Dacheng Tao, and Jingren Zhou. survey on self-evolution of large language models, 2024. https://arxiv.org/abs/2404.14387. Gemini Team. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context, 2024. https://arxiv.org/ abs/2403.05530. Junyang Wang, Haiyang Xu, Haitao Jia, Xi Zhang, Ming Yan, Weizhou Shen, Ji Zhang, Fei Huang, and Jitao Sang. Mobile-agent-v2: Mobile device operation assistant with effective navigation via multi-agent collaboration, 2024a. https://arxiv.org/abs/2406. 01014. Junyang Wang, Haiyang Xu, Jiabo Ye, Ming Yan, Weizhou Shen, Ji Zhang, Fei Huang, and Jitao Sang. Mobile-agent: Autonomous multi-modal mobile device agent with visual perception. arXiv preprint arXiv:2401.16158, 2024b. Ke Wang, Tianyu Xia, Zhangxuan Gu, Yi Zhao, Shuheng Shen, Changhua Meng, Weiqiang Wang, and Ke Xu. E-ant: large-scale dataset for efficient automatic gui navigation, 2024c. https://arxiv.org/abs/2406.14250. Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, and Junyang Lin. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution, 2024d. https://arxiv.org/abs/2409.12191. Shuai Wang, Weiwen Liu, Jingxuan Chen, Yuqi Zhou, Weinan Gan, Xingshan Zeng, Yuhan Che, Shuai Yu, Xinlong Hao, Kun Shao, et al. Gui agents with foundation models: comprehensive survey. arXiv preprint arXiv:2411.04890, 2024e. 23 Xinyuan Wang, Bowen Wang, Dunjie Lu, Junlin Yang, Tianbao Xie, Junli Wang, Jiaqi Deng, Xiaole Guo, Yiheng Xu, Chen Henry Wu, Zhennan Shen, Zhuokai Li, Ryan Li, Xiaochuan Li, Junda Chen, Boyuan Zheng, Peihang Li, Fangyu Lei, Ruisheng Cao, Yeqiao Fu, Dongchan Shin, Martin Shin, Jiarui Hu, Yuyan Wang, Jixuan Chen, Yuxiao Ye, Danyang Zhang, Dikang Du, Hao Hu, Huarong Chen, Zaida Zhou, Yipu Wang, Heng Wang, Diyi Yang, Victor Zhong, Flood Sung, Y. Charles, Zhilin Yang, and Tao Yu. Opencua: Open foundations for computer-use agents, 2025a. https://arxiv.org/abs/2508.09123. Zhenhailong Wang, Haiyang Xu, Junyang Wang, Xi Zhang, Ming Yan, Ji Zhang, Fei Huang, and Heng Ji. Mobile-agent-e: Self-evolving mobile assistant for complex tasks, 2025b. https://arxiv.org/abs/2501.11733. Yuyang Wanyan, Xi Zhang, Haiyang Xu, Haowei Liu, Junyang Wang, Jiabo Ye, Yutong Kou, Ming Yan, Fei Huang, Xiaoshan Yang, et al. Look before you leap: gui-critic-r1 model for pre-operative error diagnosis in gui automation. arXiv preprint arXiv:2506.04614, 2025. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. Chain-ofthought prompting elicits reasoning in large language models, 2023. https://arxiv.org/abs/2201.11903. Penghao Wu, Shengnan Ma, Bo Wang, Jiaheng Yu, Lewei Lu, and Ziwei Liu. Gui-reflection: Empowering multimodal gui models with self-reflection behavior. arXiv preprint arXiv:2506.08012, 2025a. Qianhui Wu, Kanzhi Cheng, Rui Yang, Chaoyun Zhang, Jianwei Yang, Huiqiang Jiang, Jian Mu, Baolin Peng, Bo Qiao, Reuben Tan, et al. Gui-actor: Coordinate-free visual grounding for gui agents. arXiv preprint arXiv:2506.03143, 2025b. Zhiyong Wu, Zhenyu Wu, Fangzhi Xu, Yian Wang, Qiushi Sun, Chengyou Jia, Kanzhi Cheng, Zichen Ding, Liheng Chen, Paul Pu Liang, and Yu Qiao. Os-atlas: foundation action model for generalist gui agents, 2024. https://arxiv.org/abs/2410.23218. Tianbao Xie, Jiaqi Deng, Xiaochuan Li, Junlin Yang, Haoyuan Wu, Jixuan Chen, Wenjing Hu, Xinyuan Wang, Yuhui Xu, Zekun Wang, Yiheng Xu, Junli Wang, Doyen Sahoo, Tao Yu, and Caiming Xiong. Scaling computer-use grounding via user interface decomposition and synthesis, 2025. https://arxiv.org/abs/2505.13227. Yiheng Xu, Zekun Wang, Junli Wang, Dunjie Lu, Tianbao Xie, Amrita Saha, Doyen Sahoo, Tao Yu, and Caiming Xiong. Aguvis: Unified pure vision agents for autonomous gui interaction. 2024. https://arxiv.org/abs/2412.04454. Yan Yang, Dongxu Li, Yutong Dai, Yuhao Yang, Ziyang Luo, Zirui Zhao, Zhiyuan Hu, Junzhe Huang, Amrita Saha, Zeyuan Chen, Ran Xu, Liyuan Pan, Caiming Xiong, and Junnan Li. Gta1: Gui test-time scaling agent, 2025. https://arxiv.org/abs/2507.05791. Yuhao Yang, Yue Wang, Dongxu Li, Ziyang Luo, Bei Chen, Chao Huang, and Junnan Li. Aria-ui: Visual grounding for gui instructions, 2024. https://arxiv.org/abs/2412.16256. Xinbin Yuan, Jian Zhang, Kaixin Li, Zhuoxuan Cai, Lujian Yao, Jie Chen, Enguang Wang, Qibin Hou, Jinwei Chen, Peng-Tao Jiang, and Bo Li. Enhancing visual grounding for gui agents via self-evolutionary reinforcement learning. 2025. https: //arxiv.org/abs/2505.12370. Chaoyun Zhang, Shilin He, Jiaxu Qian, Bowen Li, Liqun Li, Si Qin, Yu Kang, Minghua Ma, Guyue Liu, Qingwei Lin, et al. Large language model-brained gui agents: survey. arXiv preprint arXiv:2411.18279, 2024a. Jiwen Zhang, Jihao Wu, Yihua Teng, Minghui Liao, Nuo Xu, Xiao Xiao, Zhongyu Wei, and Duyu Tang. Android in the zoo: Chain-of-action-thought for gui agents, 2024b. https://arxiv.org/abs/2403.02713. Miaosen Zhang, Ziqiang Xu, Jialiang Zhu, Qi Dai, Kai Qiu, Yifan Yang, Chong Luo, Tianyi Chen, Justin Wagle, Tim Franklin, and Baining Guo. Phi-ground tech report: Advancing perception in gui grounding, 2025a. https://arxiv.org/abs/2507.23779. Zhong Zhang, Yaxi Lu, Yikun Fu, Yupeng Huo, Shenzhi Yang, Yesai Wu, Han Si, Xin Cong, Haotian Chen, Yankai Lin, Jie Xie, Wei Zhou, Wang Xu, Yuanheng Zhang, Zhou Su, Zhongwu Zhai, Xiaoming Liu, Yudong Mei, Jianming Xu, Hongyan Tian, Chongyi Wang, Chi Chen, Yuan Yao, Zhiyuan Liu, and Maosong Sun. Agentcpm-gui: Building mobile-use agents with reinforcement fine-tuning, 2025b. https://arxiv.org/abs/2506.01391. Yaowei Zheng, Junting Lu, Shenzhi Wang, Zhangchi Feng, Dongdong Kuang, and Yuwen Xiong. Easyr1: An efficient, scalable, multi-modality rl training framework. https://github.com/hiyouga/EasyR1, 2025. Zhipu-AI. Glm-4.5v. Available at: https://docs.z.ai/guides/vlm/glm-4.5v, 2025. Yuqi Zhou, Sunhao Dai, Shuai Wang, Kaiwen Zhou, Qinglin Jia, and Jun Xu. Gui-g1: Understanding r1-zero-like training for visual grounding in gui agents. 2025. https://arxiv.org/abs/2505.15810. Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Hao Tian, Yuchen Duan, Weijie Su, Jie Shao, Zhangwei Gao, Erfei Cui, Xuehui Wang, Yue Cao, Yangzhou Liu, Xingguang Wei, Hongjie Zhang, Haomin Wang, Weiye Xu, Hao Li, Jiahao Wang, Nianchen Deng, Songze Li, Yinan He, Tan Jiang, Jiapeng Luo, Yi Wang, Conghui He, Botian Shi, Xingcheng Zhang, Wenqi Shao, Junjun He, Yingtong Xiong, Wenwen Qu, Peng Sun, Penglong Jiao, Han Lv, Lijun Wu, Kaipeng Zhang, 24 Huipeng Deng, Jiaye Ge, Kai Chen, Limin Wang, Min Dou, Lewei Lu, Xizhou Zhu, Tong Lu, Dahua Lin, Yu Qiao, Jifeng Dai, and Wenhai Wang. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models, 2025. https://arxiv.org/abs/2504.10479."
        },
        {
            "title": "A Prompt Templates",
            "content": "A.1 Grounding"
        },
        {
            "title": "Grounding Prompt",
            "content": "Outline the position corresponding to the instruction: {problem}. The output should be only [x1,y1,x2,y2]. A.2 Navigation"
        },
        {
            "title": "Navigation Prompt",
            "content": "**You are GUI Agent**. Your task is to analyze given user task, review current screenshot and previous actions, and determine the next action to complete the task. ### User Task {problem} ### Previous Actions {history} ### Available Actions You may execute one of the following functions: Click(box=(x1, y1)) Drag(start=(x1, y1), end=(x2, y2)) Scroll(start=(x1, y1), end=(x2, y2), direction=down/up/right/left) Type(content=) Launch(app=) Wait() Finished(content=) CallUser(content=) LongPress(box=(x1, y1)) PressBack() PressHome() PressEnter() PressRecent() ### Instruction - Make sure you understand the task goal to avoid wrong actions. - Make sure you carefully examine the current screenshot. Sometimes the summarized history might not be reliable, over-claiming some effects. - For requests that are questions (or chat messages), remember to use the CallUser action to reply to user explicitly before finishing! Then, after you have replied, use the Finished action if the goal is achieved. - Consider exploring the screen by using the scroll action with different directions to reveal additional content. - To copy some text: first select the exact text you want to copy, which usually also brings up the text selection bar, then click the copy button in bar. - To paste text into text box, first long press the text box, then usually the text selection bar will appear with paste button in it. - You first think about the reasoning process in the mind, then provide the action. The reasoning and action are enclosed in <think></think> and <action></action> tags respectively. After providing action, summarize your action in <conclusion></conclusion> tags."
        },
        {
            "title": "B Qualitative Examples",
            "content": "B.1 Grounding We show several grounding examples across desktop professional software, desktop system, website and mobile in Figure 5-10. Figure 5 Grounding example for desktop professional software Blender. The instruction is \"Increase axis\" and the bounding box result is displayed as dotted red box 27 Figure 6 Grounding example for Excel. The instruction is \"Redo\" and the bounding box result is displayed as dotted red box. Figure 7 Grounding example for common linux system. The instruction is \"Change the terminal encoding to the legacy GBK\" and the bounding box result is displayed as dotted red box. 28 Figure 8 Grounding example for the website Gitlab. The instruction is \"Add new one\" and the bounding box result is displayed as dotted red box. Figure 9 Grounding example for shopping website. The instruction is \"Switch to customers\" and the bounding box result is displayed as dotted red box. 29 Figure 10 Grounding examples for the mobile system with Chinese instructions. The bounding box results are displayed as dotted red boxes. B.2 Navigation We present several UI navigation case studies in Figures 12 and 13 from the AndroidWorld dataset. To demonstrate UI-Venuss cross-lingual capabilities, we additionally provide navigation trace (Figure 11) showcasing its performance on Chinese language application with Chinese interface elements. 30 Figure 11 One UI-Venus navigation trace on Chinese application with Chinese goal description. Interaction points are highlighted using red circular markers. 31 Figure 12 One trace of UI-Venus on the task named ContactsNewContactDraft in AndroidWorld. We mark the clicking points with red circles and observe that UI-Venus successfully create the draft without hitting the Save, which shows the models powerful navigation generalization and strong instruction-following ability. Figure 13 One trace of UI-Venus on the task named MarkorDeleteAllNotes in AndroidWorld. We can observe that UI-Venus successfully achieves the goal and has the reflection ability in Step 3. However, there also exists the conflict between think and action in Step 5, remaining as future work about how to solve MLLMs hallucination."
        }
    ],
    "affiliations": [
        "Ant Group"
    ]
}