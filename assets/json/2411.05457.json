{
    "paper_title": "Improving the detection of technical debt in Java source code with an enriched dataset",
    "authors": [
        "Nam Le Hai",
        "Anh M. T. Bui",
        "Phuong T. Nguyen",
        "Davide Di Ruscio",
        "Rick Kazman"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Technical debt (TD) is a term used to describe the additional work and costs that emerge when developers have opted for a quick and easy solution to a problem, rather than a more effective and well-designed, but time-consuming approach. Self-Admitted Technical Debts (SATDs) are a specific type of technical debts that developers intentionally document and acknowledge, typically via textual comments. While these self-admitted comments are a useful tool for identifying technical debts, most of the existing approaches focus on capturing crucial tokens associated with various categories of TD, neglecting the rich information embedded within the source code itself. Recent research has focused on detecting SATDs by analyzing comments embedded in source code, and there has been little work dealing with technical debts contained in the source code. To fill such a gap, in this study, through the analysis of comments and their associated source code from 974 Java projects hosted in the Stack corpus, we curated the first ever dataset of TD identified by code comments, coupled with its associated source code. Through an empirical evaluation, we found out that the comments of the resulting dataset help enhance the prediction performance of state-of-the-art SATD detection models. More importantly, including the classified source code significantly improves the accuracy in predicting various types of technical debt. In this respect, our work is two-fold: (i) We believe that our dataset will catalyze future work in the domain, inspiring various research issues related to the recognition of technical debt; (ii) The proposed classifiers may serve as baselines for other studies on the detection of TD by means of the curated dataset."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 8 ] . [ 1 7 5 4 5 0 . 1 1 4 2 : r TRANSACTIONS ON SOFTWARE ENGINEERING, VOL. XX, NO. YY"
        },
        {
            "title": "Improving the detection of technical debt in Java\nsource code with an enriched dataset",
            "content": "Nam Le Hai, Anh M. T. Bui, Phuong T. Nguyen, Davide Di Ruscio, and Rick Kazman AbstractTechnical debt (TD) is term used to describe the additional work and costs that emerge when developers have opted for quick and easy solution to problem, rather than more effective and well-designed, but time-consuming approach. Self-Admitted Technical Debts (SATDs) are specific type of technical debts that developers intentionally document and acknowledge, typically via textual comments. While these self-admitted comments are useful tool for identifying technical debts, most of the existing approaches focus on capturing crucial tokens associated with various categories of TD, neglecting the rich information embedded within the source code itself. Recent research has focused on detecting SATDs by analyzing comments embedded in source code, and there has been little work dealing with technical debts contained in the source code. To fill such gap, in this study, through the analysis of comments and their associated source code from 974 Java projects hosted in the Stack corpus, we curated the first ever dataset of TD identified by code comments, coupled with its associated source code. Through an empirical evaluation, we found out that the comments of the resulting dataset help enhance the prediction performance of state-of-the-art SATD detection models. More importantly, including the classified source code significantly improves the accuracy in predicting various types of technical debt. In this respect, our work is two-fold: (i) We believe that our dataset will catalyze future work in the domain, inspiring various research issues related to the recognition of technical debt; (ii) The proposed classifiers may serve as baselines for other studies on the detection of TD by means of the curated dataset. Index TermsTechnical debt, mining software repositories, source code."
        },
        {
            "title": "1 INTRODUCTION",
            "content": "The concept of technical debt was originally introduced by Cunningham [15] to represent the liabilities that arise when developers make sub-optimal technical decisions, either intentionally or unintentionally during the software development life-cycle in their rush to market. Various factors can lead to the accumulation of technical debt, including deadline pressures, existing low-quality code, misaligned incentives, and poor software processes, among others [6]. Previous studies have shown that developers often underestimate the consequences of such debts, which can degrade the quality of the source code, increase bug rates, and slow development velocity [72, 78]. Identifying the code that contains technical debt is crucial to rational development process, as this allows developers to fix the most important issues, the ones that are slowing the project down. Given the significant impact of technical debt on software maintenance and quality assurance, substantial research has been conducted to identify and analyze technical debt in software engineering [3, 8, 20, 21, 38]. In 2015, da Silva Maldonado et al. [42] introduced their seminal work on detecting self-admitted technical debt (SATD) from comments embedded in source code. In particular, the authors manually classified set of code comments to identify various types of technical debt. Afterward, they also developed various models to detect SATD from their dataset [16]. Significant research on the recognition of SATD N. Le and A. M. T. Bui are with Hanoi University of Science and Technology, Vietnam. Email: {namlh, anhbtm}@soict.hust.edu.vn P.T. Nguyen and D. Di Ruscio are with Università degli Studi dellAquila, 67100-LAquila, Italy.E-mail: {phuong.nguyen, davide.diruscio}@univaq.it R. Kazman is with University of Hawaii, USA. Email: kazman@hawaii.edu Corresponding author: Anh M. T. Bui. has flourished since then [18, 55]. Recently, there have been various approaches proposed to recognize technical debt contained in different project artifacts. Among others, Li et al. [37] conceived an approach to identify SATD from four independent sources, i.e., source code comments, commit messages, pull requests, and issue tracking systems. Tan et al. [64] manually curated dataset of TD manifested in 3,000 issues. An evaluation of the collected dataset showed that there is positive correlation between the number of TD items identified and mentioned as resolved in issue trackers and the number of debt items paid back in the source code. The majority of research thus far has mined TD from textual sources, e.g., comments [42], issues [37, 64], or pull requests [37]. In this respect, SATD detection tools rely heavily on text to function. If no technical debt is reported in text, debt might exist, but these tools would fail to identify it. To add insult to injury, even when there are comments, many of them and the corresponding code are not coherent, i.e., comments may be out of date and incorrectly reflect what is actually contained in the associated code [14]. This may happen because developers forget to update their comments after they made changes to the code [28, 49]. To the best of our knowledge, there has been no attempt to create largescale datasets of technical debt directly contained in source code. But we see need for this type of data, to reduce the dependence on textual comments in detecting TD, thus vastly enhancing the contexts in which debt may be detected. To address these challenges, in this paper we propose pipeline for the enrichment of technical debt data. In particular, our methodology involves extracting SATD comments in conjunction with corresponding source code units. We have devised method to identify Java source code that possibly contains technical debt, creating our initial corpus. Then we TRANSACTIONS ON SOFTWARE ENGINEERING, VOL. XX, NO. YY 2 manually classified five categories of technical debt in this corpus. In addition, we developed machine learning based tool to detect technical debt contained in textual comments and source code. By means of an empirical evaluation, we demonstrated that the curated dataset has the potential to advance state-of-the-art research in the domain, paving the way for completely new way of identifying TD. Using this dataset, we have addressed the following research questions (RQs): RQ1: Do the manually classified comments contribute to an improvement in the detection of SATD? We augmented the dataset collected by da Silva Maldonado et al. [42] with our newly classified comments to yield combined dataset. Afterward, we ran four machine learning models, i.e., BERT, RoBERTa, UniXCoder, CodeBERT, on both datasets to compare the prediction performance of these models. This aims to determine whether the new comments are useful in predicting technical debt. RQ2: Does the inclusion of source code help to enhance the detection of technical debt? We enriched the input data in RQ1 with source code and fine-tuned the four aforementioned machine learning models to identify technical debt in code. This RQ aims to investigate whether the enriched dataset (consisting of classified comments and corresponding source code) is beneficial to the detection of technical debt. RQ3: What is the accuracy of different pre-trained models when detecting TD solely from source code? Using various machine learning models, we ran experiments on the collected dataset to investigate the accuracy of these models in classifying debt contained solely in source code. With this RQ we investigate to what extent existing deep learning algorithms are able to detect TD from code, thus inspiring future research in this direction. Contributions. In summary, our paper makes the following contributions: comprehensive pipeline from data extraction to labeling, aimed at improving labeling efficiency by selecting informative examples to enrich the existing corpus. Given sufficient resources and computational capabilities, this pipeline can be iteratively executed to continuously improve the quality of the dataset. datasetnamed TESOROcurated for detecting technical debt within source code. In addition to existing corpora, TESORO offers an additional and important feature, i.e., source code that contains debt. This facilitates the exploration of broader range of scenarios to advance the detection of technical debt. We propose novel approaches that integrate source code information to enhance SATD detection. Additionally, we conduct comprehensive study on effectively utilizing this context by examining the impact of code context length provided to the model. An empirical study on the curated dataset to evaluate the extent to which it contributes to the detection of technical debt contained in source code. With this evaluation, we attempt to lay the foundations of new method to identify technical debt, focusing on debts that exist in source code. The replication package including the curated dataset and the source code implementation has been published online to foster future research.1 Structure. Section 2 provides some background on technical debt, as well as the related work. Afterward, Section 3 presents in detail the proposed pipeline to curate the dataset. Section 4 describes the resulting datasets. In Section 5 we present an empirical study on the usage of the resulting dataset to evaluate its effect in the detection of technical debt. Section 6 provides some discussions on the findings, as well as highlights the threats to validity of the results. In Section 7, we review the related work on the detection of technical debt from different types of input data. Finally, Section 8 sketches future work and concludes the paper."
        },
        {
            "title": "2 BACKGROUND",
            "content": "In this section, we review different types of SATD and provide an overview of pretrained language models. 2.1 Self-admitted technical debt (SATD) SATD is the technical debt that is expressly admitted by developer through comments embedded in source code, issue trackers [36], commit messages, or pull requests [37]. da Silva Maldonado et al. [42] identified five types of SATD, i.e., DESIGN, DEFECT, DOCUMENTATION, REQUIREMENT/IMPLEMENTATION, and TESTING. This categorization allows for more insightful descriptions and deeper understanding of the non-optimal solution options taken. This section takes various examples to explain these SATD categories. DESIGN. Comments of this type indicate that there is problem with the design of the code, i.e., comments about misplaced code, lack of abstraction, long methods, poor implementation, workarounds, or temporary solutions. To illustrate, consider the following examples. C1: // TODO: - This method is too complex, lets break it up C2: // hate this so much even before start writing it. // Re-initializing global in place where no-one will see it just // feels wrong. Oh well, here goes. C3: //quick & dirty, to make nested mapped p-sets work: C4: //I cant get my head around this; is encoding treatment needed here? In C1, the developer said that the method is complex and should be broken up. This is related to the existing design, and the creator of the code signaled this so that other developers could tackle the issue later on. In C2 the developer complained about the fact that re-initializing global variable in an obscure location is not the right thing to do. This is actually design issue, and it needs to be fixed. C3 implies that the code is makeshift solution, i.e., it is suboptimal, but still works in the given context. And in C4 the developer wondered aloud if the encoding treatment was really necessary. 1. https://github.com/NamCyan/tesoro TRANSACTIONS ON SOFTWARE ENGINEERING, VOL. XX, NO. YY DEFECT. In this category, the authors state that part of the code does not have the expected behavior, i.e., there is lingering defect in the code as shown in the examples below. C17: // TODO - need lot more tests C18: // TODO enable some proper tests!! C5: // Bug in above method C6: // WARNING: the OutputStream version of this doesnt work! C7: // the following stuff did not work and dont know why! C8: // POTENTIAL FLAW: Use password directly in PasswordAuthentication() C5 explicitly points out that there is bug detected in the given method. This is clear case of defect, and it should be fixed soon. With C6, warning is given, marking OutputStream as malfunctioning API call in the current context. In C7 the developer warned that the code did not work, thereby admitting that they had no idea why this had happened. Eventually, C8 signals disclosure of sensitive information, which possibly poses security threat. DOCUMENTATION. In this type of debt authors express that there is no proper documentation supporting some part of the system. We consider the following examples. C9: // FIXME This function needs documentation C10: // TODO Document the reason for this C11: // @return DOCUMENT ME! C12: // TODO(saurabh): Explain reload scenario here All the four comments, i.e., C9, C10, C11, and C12 clearly state that documentation is needed in the containing projects; C12 is more specific, stressing that it is necessary to explain concrete method. REQUIREMENT or IMPLEMENTATION. Requirement or implementation debt comments express incompleteness of the functionality in the method, class, or program. Here are some examples. C13: //TODO no methods yet for getClassname C19: // TODO(lwhite): Better tests C20: // TODO figure out how to test this. All the examples in this category indicate that some project members knew that these areas of the code were inadequately tested. Especially, by C20, it is highly probable that the developers had not tested the code at all. In this work, we utilized SATD comments as means to locate source code that possibly contains technical debt. 2.2 Pretrained Language Models Language models (LMs) are foundational component in natural language processing (NLP) that have significantly advanced over the past decade. Recently, LMs have been powered by neural networks and trained on large text corpora, being able to capture both the syntactic and semantic aspects of languages more effectively. These models commonly follow the pre-training and fine-tuning paradigm [81]. During the pre-training phase, models are trained on largescale unlabeled corpora using task-agnostic objectives such as word prediction, resulting in the development of pre-trained language models (PLMs). PLMs are then fine-tuned to adapt to various downstream tasks. Early PLMs [56] were mostly based on Recurrent Neural Networks (RNNs) and their variants, such as long short-term memory (LSTM) [33] and gated recurrent units (GRU) [10]. However, these approaches were computationally inefficient due to limitations in parallel processing, reducing scalability when training with extensive datasets and large model sizes. With the introduction of the Transformer architecture [68] and its self-attention mechanism, significantly more parallelization became possible as compared to RNNs. This advancement enables efficient pre-training of large language models on extensive datasets using multiple GPUs. Various transformer-based PLMs have achieved state-of-the-art performance across wide range of tasks [17, 31, 39, 52, 65]. Given the superior performance of transformer-based PLMs, which have also been explored in the context of SATD detection [18, 58, 59], our study concentrates on utilizing these models. C14: //TODO no method for newInstance using reverseclassloader 2.2.1 Transformer architecture C15: /*TODO: The copy function is not yet * completely implemented - so we will * have some exceptions here and there.*/ C16: //TODO Find way to re-send the message. Starting with //TODO, C13 signals the missing implementation for getClassname. Similarly, C14 indicates the case where the newInstance method is incomplete. In C15, the developer admitted that the copy function had not been fully implemented, and will throw some exceptions. C16 advises developers to look for suitable method to re-send the messages. TESTING. These comments signal the need for the creation or improvement of the current set of tests. This section provides an overview of the Transformer architecture, emphasizing key components and elements [68]. Encoder-Decoder architecture: The Transformer architecture, initially designed for machine translation problems, features both an encoder and decoder. The encoder consists of stack of six identical layers, each containing two sub-layers: multi-head self-attention and position-wise feed-forward neural network. Similarly, the decoder is structured with six identical layers, but in addition to the two sub-layers found in the encoder, it includes third sub-layer that applies multi-head attention over the encoders output. In addition, the decoder uses masked matrix in the attention layer to prevent attending to future positions in the input sequence, ensuring that the model only considers previously generated tokens during training. TRANSACTIONS ON SOFTWARE ENGINEERING, VOL. XX, NO. YY 4 Multi-head self-attention mechanism: The attention mechanism operates by mapping query and collection of key-value pairs to an output. The output is obtained by computing weighted sum of the values, with the weights (or attention scores) derived from compatibility function that measures the alignment between the query and each corresponding key. Instead of utilizing single attention mechanism with keys, values, and queries of dimensionality dmodel, it has been found advantageous to project the queries, keys, and values into dimensions dq, dk and dv, respectively, through distinct learned linear projections (multi-head). Positional encoding: This technique is introduced to integrate information regarding the relative or absolute positions of tokens within the sequence. Specifically, the Transformer model employs absolute positional encoding by utilizing sine and cosine functions to represent token positions. 2.2.2 Types of PLMs Based on the neural architectures of Transformer-based PLMs, we categorize the models into three main groups, as also outlined in existing work [44]. Encoder-based PLMs: This type of model utilizes the Transformer Encoder and builds network by stacking multiple layers. These models were initially developed for language understanding tasks, such as text classification, where the objective is to predict class label for given input text. The pre-training stage of these models typically involves corrupting given sentence in some way (e.g., by masking random words) and then training the model to identify or reconstruct the original sentence. To tackle downstream task such as sentence classification or named entity recognition, these models are fine-tuned on task-specific data, and this involves substituting the LM head (the word prediction layer), with classification head. BERT [17], prominent encoder-based model, has inspired the development of several variants, such as RoBERTa [39] and ALBERT [31], which have demonstrated substantial improvements across various understanding tasks. Bidirectional Encoder Representations from Transformers (BERT) [17] is among the most widely adopted encoder-based PLMs. During pretraining, BERT leverages two objectives: masked language modeling (MLM) and next sentence prediction (NSP). In MLM, random tokens within sentence are masked, and the model is trained to predict these masked tokens using the context of the surrounding words. Meanwhile, NSP trains BERT to comprehend the relationship between two sentences by predicting whether one sentence logically follows the other. RoBERTa [39] extends BERT by improving its robustness through refined model design choices and training strategies. These enhancements include adjusting some key hyperparameters, eliminating the NSP objective, and training with larger batch size and learning rate. ALBERT [31] introduces two parameter reduction techniques to reduce memory consumption and enhance the training speed of BERT. Encoder-Decoder-based PLMs: This neural architecture is primarily designed for sequence-to-sequence tasks, including machine translation, text summarization, and dialogue generation. These models integrate both the encoder and decoder modules of the Transformer, where the encoder processes the input sequence into continuous representations that capture contextual information, and the decoder sequentially generates the output sequence based on these representations. T5 [52] and BART [35] are two prominent EncoderDecoder-based PLMs that have demonstrated exceptional performance in sequence-to-sequence tasks. The Text-to-Text Transfer Transformer (T5) model [52] advances the field of transfer learning in NLP by proposing unified framework that reformulates all text-based language tasks into text-to-text format. BART [35] utilizes standard sequence-to-sequence model architecture augmented with denoising strategy, in which the input text is intentionally corrupted using various noising functions such as token masking, document rotation, or sentence permutation. The model is then trained to reconstruct the original text from the corrupted input. Decoder-based PLMs: In these models, the attention layers at each stage are restricted to attending only to preceding words in the sentence, characterizing them as unidirectional or auto-regressive models. The pre-training process generally involves predicting the next word (or token) in the sequence. Consequently, decoder-based models are particularly effective for text generation tasks. The GPT [7, 50, 51] and LLaMA [65, 66] families have developed several powerful foundational models that utilize the Transformers decoder architecture. These models are pre-trained on extensive datasets comprising trillions of tokens and enhance the architecture through various techniques, such as employing the SwiGLU activation function instead of ReLU, incorporating rotary positional embeddings in place of absolute positional embeddings, and utilizing root-meansquared layer normalization instead of the standard layer normalization. Large Language Models (LLMs) primarily refer to Transformer-based PLMs characterized by their extensive architecture, containing billions of parameters. These models are primarily inspired by decoder-based architectures, forming the foundation for the development of more advanced LLMs. LLMs are considerably larger in size, exhibiting superior language understanding and generation capabilities compared to small-scale PLMs. Some notable LLMs include GPT-4 [1], LLaMA-2 [66], PaLM [11], and FLAN [74]. Based on the data utilized for the pre-training stage, we categorize PLMs into two groups. NL-based PLMs: This is class of models primarily trained on extensive natural language text corpora [7, 17, 31, 35, 39, 50, 51, 52, 65, 66]. These models leverage vast amounts of textual data to learn rich linguistic representations, making them highly effective for wide range of NLP tasks, such as text classification, sentiment analysis, and question answering. Code-based PLMs: These are specialized models designed to understand and generate programming code [22, 23, 24, 40, 46, 54, 70, 71, 75]. Typically, these models are initialized from NL-based PLMs and further trained on large corpora of source code from various programming languages. The datasets are collected from rich code sources, including GitHub and Stack Overflow. These models demonstrate TRANSACTIONS ON SOFTWARE ENGINEERING, VOL. XX, NO. YY 5 Fig. 1: An Overview of the TESORO Creation Pipeline. exceptional performance across various code-related tasks, including code summarization, code translation, bug detection, technical debt detection, and code generation."
        },
        {
            "title": "3 PROPOSED METHODOLOGY",
            "content": "TABLE 1: Input data information across phases. Phase #File #Function #Comment Raw files (The Stack-Java) Code Parser Tool SATD Detection Tool Annotation process 26M 2M - 999 - - - 1,255 - - 3.6M 4.981 In this section, we describe our proposed approach to constructing the TESORO dataset. We outline the steps of our processing pipeline as follows. 3.1 Source Data We reused the benchmark dataset proposed by Maldonado et al. [42] to train an SATD classifier. The pre-trained SATD classifier was then employed to detect SATD comments from open-source projects within the Stack corpus [29]. We then created an approach to localize and annotate code snippets following SATD comments from opensource projects in the Stack corpus. We detail the annotation process at the end of this section. Specifically, we invited seven Masters students of Computer Science to verify the SATD comments and their associated source code snippets. The objective was to assign technical debt (TD) label to the source code. TESORO facilitates the detection of technical debt (TD) not only in comments but also through an additional feature: the source code. For SATD tasks, TESORO offers additional code context, rather than relying solely on comments as in previous studies [16, 42, 58]. To construct the dataset for TD detection in source code, we employed information from SATD comments to identify specific categories of debt within the code. While in prior work the identification of TD was based solely on comments, in this work we identify where TD appears in the source code, without accompanying comments. The data collection process is illustrated in Figure 1. The pipeline consists of four major components: Code Parser Tool to extract functions and comments from Java files, an SATD Detection Tool to identify TD types in comments, Sampling Strategy to select high-quality samples, and an Annotation Process to assign TD type to chosen comments. We initially opted for GITHUB as our data retrieval source. However, due to rate limit constraints of the GITHUB API,2 we adopted an alternative dataset: The Stack [29], which has been acknowledged as the most extensive publicly available source code dataset, boasting permissive license and substantial size of 3TB. The Stack contains samples from 358 programming languages. In this work, we focus on detecting TD in Java source code. This subset of The Stack yielded dataset of 26M raw files. Due to constraints in storage and computational resources associated with the code parser and SATD Detection tools, we restricted ourselves to analyzing just 2M of these files. Table 1 shows the statistics for the dataset across each phase. When considering identifying TD at the file level, large files present challenges for developers in localizing the sections of code harboring TD. Our goal, then, was to reduce this scope, focusing on identifying TD within the context of function blocks in the source code, as depicted in Figure 2. 3.2 Code Parser Tool As our emphasis is on detecting TD at the function level, we needed to parse code files from The Stack into individual functions. Since comments serve as the primary annotations for identifying TD within function, we extracted collection of functions from each file, each containing set of comments. We leverage Toolkit3a tool introduced in our previous work [43], which relies on Tree-sitter4to parse source code into 2. https://docs.github.com/en/rest?apiVersion=2022-11-28 3. https://github.com/FSoft-AI4Code/CodeText-parser 4. https://tree-sitter.github.io/tree-sitter/ TRANSACTIONS ON SOFTWARE ENGINEERING, VOL. XX, NO. YY Fig. 2: Extraction of comments and functions. Abstract Syntax Tree (AST) representations, enabling the extraction of functions. Subsequently, we extracted series of comments associated with each function. Comments belonging to function are defined as those located within the body of the function, and the initial comment preceding the functions definition. Developers commonly break down long comments into several lines. The AST classifies distinct lines of comments as individual block nodes; we re-classify consecutive comment lines as single comment. For example, in Figure 2, there are four blocks of comments within the addModuleForVoiceCall function: three blocks contained within the function and one outside (highlighted in green). The three blocks within the function are consecutive, so we group them to form single comment. As result, the function addModuleForVoiceCall contains two comment statements. If the single comment block in Fig. 2 includes TD, there there are two different data points. The comments extracted from each function are then annotated following previous research on SATD detection [16, 37, 42, 60]. The labeled information of comments then serves as the ground truth for assigning TD in the function after removing all comments. 3.3 SATD Detection Tool Since there is large number of functions and comments, the annotation process requires considerable human labor. Thus, it becomes crucial to choose subset for annotation purposes. Previous studies [16, 42] showed that the majority of extracted comments do not include TD, with over 90% of comments not implying TD. Therefore, randomly selecting examples for annotation might yield numerous comments that do not contribute to the TD identification process. Identify comments containing TD: To address this challenge, we developed TD detection tool to identify comments containing TD within the corpus from Section 3.2. In particular, we constructed neural model to determine whether comment contains TD or not. In fact, there have been various SATD detection techniques [18, 55], but we decided to develop tailored tool on top of pre-trained models as means to validate thei effectiveness in detecting SATD. The detection tool is binary classifier, in which all comments containing TD are classified into the positive class, while the rest are assigned to the negative class. Since comments are predominantly in natural language text format, we built the tool using the RoBERTa architecture [39]. Since the tool needed to process substantial volume of comments, we employed the base version of the model, with 128 million parameters, to balance performance and speed. The Maldonado-62K [16] dataset was leveraged for fine-tuning. The dataset includes more than 62,000 comments, of which 6.5% were identified as containing debt, and categorized into 5 different types of TD, and the majority were identified as non-SATD. We grouped the five classes into the positive class to train the neural classifier using binary cross-entropy loss. Before fine-tuning, we performed simple cleaning process to convert comments to lowercase, removing comment delimiters such as //, /*, */, and eliminating duplicates. We conducted training for 10 epochs with learning rate set to 2e 5, and held back 10% of the data as validation set. Subsequently, the trained model is employed to scan through approximately 40 million comments to seek out those containing TD. Consequently, over 1.6 million comments were identified as potentially implying TD. Detecting TD types of comments: After acquiring the candidate comments, we classified them into five types (design, implementation, defect, test, and documentation) following da Silva Maldonado el at. [16]. Initially, we intend to employ this information to guide annotators, thereby mitigating their TRANSACTIONS ON SOFTWARE ENGINEERING, VOL. XX, NO. YY entropyθ (x) = i=1 Pθ(y = ix) log(Pθ(y = ix)) (1) This score indicates the confidence level of the trained model regarding particular example: low entropy score signifies high confidence in the models prediction, whereas high score indicates uncertainty. To enhance the existing datasets, we selected difficult examples for annotation. Combining the predictions from multiple TD classifiers, we devised strategy to acquire subset of functions for annotation, as outlined in Algorithm 1. We constructed candidate set comprising examples that imply more than one TD type using our five binary classifiers, along with examples exhibiting high uncertainty scores. We selected subset of comments and corresponding functions from this set for annotation. After acquiring the set of functions, we extracted list of comments Ci corresponding to each function. These comments were subsequently relabeled and they serve as the primary information for defining TD types within the code functions. Algorithm 1 Sampling informative subset for annotation Input Number of samples for selection. List of TD types. No. comments extracted from detection tool. Extracted triplet (comment, function, category prediction list) set: {di = (ci, fi, Pi) = 1, }."
        },
        {
            "title": "M Dictionary of classifiers corresponding to each TD",
            "content": "type key: {X: Classifier-X C}. Output Set of functions for annotation {di di D, Pi > 1} Comments that are predicted to contain more than 2 types of TD. UnSc list() for dj in D/Q do Pj[0] θ θM[p] entropyθ UnSc.add(s) Calculate using Equation 1 (cj) end for sID argsort(UnSc, desc = True) ˆQ {dj dj D/Q, topQ (sID)} ({fi di ˆQ}) random_samplingn return Fig. 3: Overlap categories ratio from multiple binary classifiers prediction on comment. workload. However, this information may have biased the annotators. Hence, we utilized this information to investigate TD types that are frequently misunderstood by the models capabilities, thus pinpointing examples worthy of annotation (Section 3.4). Instead of employing multiclass classification to detect TD types within comments, we constructed binary classifier for each type. For instance, when considering TD types X, Classifier-X is developed to distinguish whether comment contains TD type or not. Similar to identifying comments containing TD, we designate training examples containing TD type as the positive class, while the remainder are categorized as the negative class. Consequently, we created five classifiers and each of the 1.6 million extracted comments was analyzed by these five classifiers to obtain pseudo-categories. Since these classifiers work independently, comment can be categorized into more than one class. Figure 3 depicts the overlap categories predicted within single comment, demonstrating the similarity between the two types of TD. It is shown that design and implementation are the two categories most often confused, finding consistent with prior studies [37] that tried to merge these categories. However, we maintained these categories separately for more fine-grained evaluation in our dataset. 3.4 Sampling Strategy The volume of detected comments (1.6M) is still very large, and resource-intensive for human relabeling. Therefore, we employed technique to select useful samples for annotation. Our objective is not only the selection of examples to explore the detection of TD at the function level but also the identification of comments that could enrich existing datasets. Following existing literature [9, 13, 57, 61], we utilized uncertainty scores to identify challenging instances for annotation, employing the Entropy score. Consider an instance x, model with parameter θ and classes, the uncertainty score of the model on sample is as follows: 3.5 Data Annotation Process 3.5.1 Annotation Group We formed an annotation team by hiring 7 final-year university students, each specializing in Software Engineering as their primary field of study. Their background allowed them to comprehend complex technical concepts and effectively apply this knowledge to the TD annotation process, thereby providing high-quality and contextually accurate data labels. To further enhance their capabilities, we conducted comprehensive training session tailored to the specific requirements of the labeling process. These sessions introduced TRANSACTIONS ON SOFTWARE ENGINEERING, VOL. XX, NO. YY 8 TABLE 2: Annotation Assessment. Phase Number of comments Raw Agreement IAA 1 2 1,400 3,680 56.18 92.77 37.00 45.29 the annotators to the field of TD and provided detailed explanations of various TD types, as specified in Section 2. By equipping the annotators with thorough understanding of the task, we aimed to minimize errors and improve the overall quality of the labeling process. 3.5.2 Labeling process Each annotator is provided detailed guidelines that serve as reference throughout the annotation process. These guidelines include standardized procedures, examples of correctly labeled data, and common pitfalls to avoid. By adhering to these guidelines, the annotators better maintain consistency and reliability across the dataset. Following da Silva Maldonado et al. [42], we developed tool for the labeling process. However, diverging from the conventional approach of displaying only comments, we also included the corresponding code function as reference for annotators. Annotators were asked to review both the comments and the corresponding code for labeling. Moreover, we limited the labeling process to our five specific TD types: design, defect, documentation, implementation, and test debts, in addition to non-SATD. The students were given comment and corresponding code, and they had to read both and make decision. In case, there is no TD contained in the code, so the corresponding label non-SATD was given. Additionally, we conducted Cross-checking and Label Auditing to enhance the quality of the labeling process. Specifically, regarding data sample, the labeling process is outlined as follows. 1) Annotator assignment: For each comment, two annotators were randomly chosen for labeling. 2) Cross-checking: We collected the labels assigned to each example by the two annotators and made comparison. If there was disagreement between the labeling results, we moved to the Label Auditing step; otherwise, the example was included in the final dataset. 3) Label Auditing: We asked the two annotators to discuss their labeling, and reach an eventual consensus. As shown in Table 2, the labeling process was conducted in two phases. In the first phase, 1,400 comments were selected for annotation by seven annotators. Aiming for reliability, every comment was labelled by two students, i.e., each annotator was assigned 400 comments. This phase helps the students familiarize themselves with the labeling task, and establish uniform conventions for the labeling process. Subsequently, in the second phase, there were 3,680 different comments, and each of them was independently evaluated by two students. This resulted in total of 7,360 comments for the labeling process. To guarantee the reliability of the labeling process, we assessed it using two consensus metrics: Raw Agreement and Inter-Annotator Agreement. 1) Raw Agreement: refers to the count of items for which both annotators assign identical labels, expressed as percentage of the total items annotated [4]. TABLE 3: The comparison between popular SATD benchmarks and TESORO. NL-sample refers to data in natural language text format, such as comments, pull requests, issues, and commit messages. Dataset #Code-sample #NL-sample % TD NL-samples #Repo Maldonado-62K [16] 4Source-SATD [37] SATD in [58] TESORO - - - 1, 62,566 95,455 146,583 4,981 6.5 8.5 3.4 31.1 10 103 503 Fig. 4: Category distribution in TESOROcomment. Left: distribution of TD categories within comments containing SATD. Right: percentage of comments that contain versus those that do not contain SATD. 2) Inter-Annotator Agreement (IAA): Cohens Kappa coefficient [19, 32] was applied to quantify the agreement or consistency between different annotators. Table 2 presents the annotation scores across two phases. In the initial phase, as the annotators were becoming acquainted with the task, there is relatively low agreement of 37%, referring to Fair agreement. However, following reviews and discussions, the agreement strength improved significantly. In the second phase, the Raw Agreement increased by over 35%, and the IAA improved by over 8%, resulting in Moderate agreement strength. This demonstrates the reliability of our labeling process and the overall quality of the dataset. Following the labeling of TD types for the comments, we aligned the labeled comments with their corresponding code functions, thus obtaining multi-TD type information for an entire code function."
        },
        {
            "title": "4 DATA CHARACTERISTICS\nTo support detecting technical debt in both comments and\ncode, we constructed two datasets.",
            "content": "4.1 Dataset for SATD-Related Tasks We created TESOROcomment, where comments serve as the input source, to support SATD-related tasks, including the identification, classification, and detection of TD. These tasks are structured as multi-class classification problems, with details in Section 5.1. In addition, unlike existing datasets, each comment is associated with its corresponding code, providing richer context for investigation and analysis. Figure 4 presents the statistics of the TESOROcomment dataset, which contains 5,000 labeled comments across six TRANSACTIONS ON SOFTWARE ENGINEERING, VOL. XX, NO. YY 9 demonstrate that the enhanced dataset improves the performance of existing SATD classification models. RQ2 seeks to assess the significance of source code in detecting SATD comments. We use the function containing the comments as supplementary features to aid in identifying SATD comments. Lastly, we assess the effectiveness of various models in identifying technical debts from source code without relying on SATD comments. In RQ3 our goal is to utilize the curated dataset to detect the presence of different types of technical debts within source code. 5.1 RQ1: Do the manually classified comments contribute to an improvement in the detection of SATD? Motivation: Our primary focus in this RQ is to assess whether the newly identified SATD comments from the Stack corpus contribute positively to the performance of SATD comment detection. Our objective is to enhance the state-ofthe-art benchmark dataset introduced by da Silva Maldonado et al. [16], henceforth referred to as Maldonado-62K. This dataset comprises 62,566 comments, of which 4,071 (approximately 6.5%) encompass one of five categories of technical debt. While the datasets volume is relatively limited, making it challenging to apply deep learning models, it is crucial to increase the diversity of data samples within each category to enhance the training performance of SATD detection models. Approach: For this RQ, our objective is to examine whether manually classified comments from the TESOROcomment dataset improve the detection of SATD comments. Following previous studies [16, 53, 55, 59], we employed cross-project experimental approach. The benchmark dataset Maldonado-62K comprises 10 projects, divided into 10 validation folds. We used one fold (one project) for testing and the remaining nine folds (nine other projects) for training. To prevent data leakage, we removed duplicate entries from the entire dataset before splitting it, resulting in 38,269 samples. We then analyzed the impact of incorporating TESORO during training, denoted as +TESORO, compared to using only the state-of-the-art dataset, Maldonado-62K, across three scenarios: SATD-Identification, SATD-Classification and SATD-Detection. 1) SATD Identification (S1): This task involves determining the presence of technical debt in given comment. 2) SATD Classification (S2): In this task we categorize comments identified as containing SATD into one of five distinct categories. 3) SATD Detection (S3): This task combines both identification and classification, categorizing each comment into one of six groups: five for the types of technical debt and one for non-SATD. For each scenario, we used the same test set while training the model separately with the Maldonado-62K and with +TESORO datasets. Research in the field of SATD detection has highlighted the promising results of PLMs [18, 58, 59], thus we also employed these models to detect SATD comments. Specifically, we experimented with BERT [17], RoBERTa [39], CodeBERT [22], UniXCoder [23] and GraphCodeBERT [24]. PLMs are employed as comment encoders, followed by fully connected network dedicated to downstream tasks such as identification, classification, and Fig. 5: Statistics of TESOROcode. Left: Distribution of the number of comments per function. Right: Distribution of the number of TD types within function. categories: the five TD types and non-SATD. Consistent with previous studies [16, 37, 42], design and implementation debts constitute the majority, with fewer entries for test and documentation, reflecting the real-world distribution. On the other hand, comments with SATD represent significant portion of the dataset, i.e., 31.1%, which is considerably higher than the proportions in previous datasets  (Table 3)  . This underscores the effectiveness of our SATD detection tool in identifying SATD comments, which helps to mitigate the imbalance between SATD and non-SATD comments. Table 3 shows that our dataset is sourced from 974 repositories. Compared to existing studies TESOROcomment is derived from more diverse range of sources, capturing wider variety of commenting and coding styles. 4.2 Dataset for TD detection in Source Code Next we introduce dataset named TESOROcode, to support detecting technical debt in source code without relying on natural language comments. Unlike comments, function can contain multiple types of technical debt; hence, we formulate this scenario as multi-label classification problem. Specifically, we exclude comments within the function and consider TD types that indicate intrinsic issues in the source code: design, implementation, defect, and test. As result, TESOROcode presents challenge for detecting these types of TD within code function. Table 3 highlights that no existing dataset has addressed this crucial scenario, underscoring the significance of TESOROcode. TESOROcode includes 1,255 Java functions from 974 projects. Figure 5 indicates that the average function contains two comments. More than 1,000 functions (86.9%) contain only single type of TD. Only 7 functions contain three types of TD, and none encompass all types. This suggests that TD is mostly homogeneous within function. Furthermore, although non-SATD comments are the majority, only small portion of the functions exhibit no type of TD. This is because we specifically focused on selecting functions with TD comments to facilitate more efficient labeling, as outlined in Section 3.4."
        },
        {
            "title": "5 EXPERIMENTAL RESULTS",
            "content": "We now discuss the experimental results and answer the research questions outlined in Section 1. For RQ1 we first TRANSACTIONS ON SOFTWARE ENGINEERING, VOL. XX, NO. YY 10 TABLE 4: The performance (F1-score) of five PLMs across three tasks when trained on the Maldonado-62K dataset and further enhanced with the additional TESORO dataset. Model BERT RoBERTa UniXCoder CodeBERT GraphCodeBERT Identification Classification Detection Maldonado-62K +TESORO () Maldonado-62K +TESORO () Maldonado-62K +TESORO () 87.96 89.06 88.38 88.74 89.94 88.99 (1.17%) 89.96 (1.01%) 88.42 (0.05%) 90.06 (1.49%) 90.12 (0.20%) 51.42 53.91 54.82 57.50 58. 55.10 (7.16%) 55.32 (2.62%) 54.99 (0.31%) 63.70 (10.78%) 60.44 (4.21%) 45.99 46.13 50.94 53.79 49.12 49.64 (7.94%) 52.86 (14.59%) 52.11 (2.30%) 55.60 (3.36%) 56.87 (15.78%) detection, as previously described. All models are fine-tuned for 10 epochs using batch size of 32 and learning rate of 1e 5. Given the significant class imbalance in the dataset, the F1 score to was used to evaluate performance. Result: Table 4 depicts the performance of five different PLMs across three aforementioned scenarios S1, S2 and S3. We see that incorporating TESORO during the training phase consistently boosts the performance of all PLMs across all tasks. Specifically, the identification of SATD comments from the entire set of comments demonstrates an improvement ranging from 0.05% to 1.49% across all models when the training phase is supplemented with the TESORO dataset, as opposed to relying solely on the Maldonado-62K dataset. In this scenario, the dataset demonstrates considerable imbalance, with non-SATD data making up more than 90% of the total. The improvement highlights the effectiveness of the TESORO comment in mitigating the data imbalance issue, and this is significant because real-world scenarios will have similar imbalance. In the context of SATD detection, BERT and RoBERTa exhibited lower performance compared to the other three code-based PLMs. However, training with +TESORO considerably improves the performance of these two models, leading to comparable results across all the models. For instance, applying +TESORO during training improved BERTs performance by 7.94% and RoBERTas by 14.59% in detecting various types of SATD comments. similar trend is observed in other scenarios, where all models show enhanced performance when utilizing +TESORO. Specifically, the performance in classifying the five SATD types increases by approximately 0.31% to 10.78% for all PLM models. Additionally, the results indicate that CodeBERT and its variant, GraphCodeBERT, achieve the highest performance across all scenarios, highlighting the advantage of code-based PLMs for the detection of SATD comments. Finally, we conducted an in-depth analysis of the results from 10 large-scale open source projects across all three scenarios. In this setup, each project serves as test fold, while the PLMs are trained on the remaining 9 projects. For illustrative purposes, we present the findings for CodeBERT and RoBERTa, as depicted in Figure 6. Overall, it is evident that training the models with +TESORO results in improved performance across all projects. In particular, in the S1 task, the performance improved across all tested projects for both models, highlighting the enhanced training sets effectiveness in addressing the imbalance issue in the original dataset. Furthermore, 7 out of the 10 folds show an increase in F1-score when training CodeBERT on classification and Fig. 6: An in-depth analysis of CodeBERT and RoBERTa performance across three scenarios for 10 projects. detection tasks with +TESORO, while RoBERTa exhibits significant improvement in 9 out of 10 test sets for the SATD detection task. Answer to RQ1 There is an improvement in the prediction performance when TESORO is incorporated into the training, validating the efficacy of our data pipeline in selecting informative samples, and proving robust annotation process. CodeBERT and GraphCodeBERT consistently achieve superior performance and notable improvements across all three tasks, highlighting the advantages of employing code-based PLMs for SATD comment detection. 5.2 RQ2: Does the inclusion of source code help to enhance the detection of technical debt? Motivation: As mentioned earlier, existing work on TD detection has primarily focused on comments and other textual artifacts such as commits and issues [16, 36, 37, 42, 55] while overlooking the content of source code. However, certain features of source code can indicate the presence of some types of technical debt, such as design or implementation. For instance, design technical debt may be introduced in antipattern source code where developers have neglected specific design principles [76]. Recent studies have established link between code smells and technical debt [67]. This highlights the potential of source code to provide meaningful features TRANSACTIONS ON SOFTWARE ENGINEERING, VOL. XX, NO. YY TABLE 5: Performance (F1-score) comparison of various models on SATD detection using only comments versus incorporating additional source code. The subscripts accompanying the numerical results indicate the number of context lines that produced the best outcomes for each model, with ff representing the use of the full function. Model Comment only Comment + Code StrConcat CodeAtt Ensemble RoBERTa CodeBERT UniXCoder GraphCodeBERT 68.28 72.75 70.62 71.39 69.112 (1.22%) 76.30ff (4.88%) 71.5420 (1.30%) 74.862 (4.86%) 69.452 (1.71%) 74.80ff (2.82%) 71.46ff (1.19%) 72.75ff (1.91%) 69.92 76.55 72.22 75.25 for the detection of technical debt. Furthermore, during the annotation process, it was observed that annotators were more proficient in identifying technical debt when utilizing source code information as reference, rather than relying exclusively on comments. Thus, our objective is to explore the effectiveness of leveraging source code for the SATD detection task. Approach: We assess the effectiveness of different models in detecting SATD comments by comparing their performance when using only comments with that when incorporating both comments and source code. We used TESOROcomment for these experiments. To ensure comprehensive evaluation, we designed the experiment based on the following considerations. Model versatility: Our goal is to explore the impact of incorporating source code in various models to demonstrate the effectiveness of this approach. As such, we apply the proposed method to four PLMs: RoBERTa, CodeBERT, UniXCoder and GraphCodeBERT. BERT has been excluded because it showed performance similar to or lower than its variant, RoBERTa as can be seen in Table 4. Integration techniques to combine source code and comments: We explore the effectiveness of two distinct methods for combining source code and comments. Specifically, both the source code and comments are tokenized into separate sequences of tokens, which are then integrated using two different strategies. (a) String Concatenation (StrConcat): Two sequences of tokens are concatenated then passed into pre-trained models. This approach is typically employed in prior studies for text classification tasks [17, 31, 39]. (b) Code Attention (CodeAtt): The source code and comments are processed independently using pre-trained encoder, resulting in an embedding vector for each token. After that, we calculate the attention score for each code token in relation to the embedding representation of the comment tokens. Let GM represent the embedding matrix for code tokens from the source code, and HN represent the embedding matrix for comment tokens, where M, are the number of code and comment tokens, respectively, is the size of the embedding vector. The final embedding that combines both source code and comment is obtained by taking the dot product of the codes attention matrix with the comment embedding, as shown below. = sof tmax(G ) classif ication_emb = 11 Code context length: Based on our observations during the labeling process, annotators did not need to review the entire code to determine the type of technical debt (TD) associated with comment; they only needed to scan the nearby code. With this in mind, we investigated the impact of varying the length of the code context. Specifically, we assessed the effect of using the surrounding code by including 2, 10, and 20 lines, as well as the entire function. Figure 2 illustrates an example of utilizing code context that includes 2 lines of code. Result: Table 5 presents the performance of the four PLMs in detecting SATD comments, comparing the outcomes of using only comments versus combining comments with code context. The analysis of the results is based on the three previously mentioned aspects. Model versatility: The results shows that incorporating code context significantly improves the performance across all evaluated models and integration approaches. Notably, CodeBERT achieves the highest accuracy when using comments alone and exhibits the greatest improvement, attaining an F1-score of 76.3% with the addition of code context. These findings highlight the value of integrating source code information for detecting SATD, boosting the effectiveness of already high-performing models. Furthermore, the improvements observed across different models further illustrate the robustness and the adaptability of this approach in identifying SATD. Integration techniques: Overall, both proposed methods improve performance across all models, with StrConcat demonstrating greater effectiveness than CodeAtt except in the case of RoBERTa. Specifically, StrConcat enhances the performance of CodeBERT and GraphCodeBERT by over 4.88% and 4.86%, respectively. In contrast, the improvements achieved with CodeAtt are more moderate, ranging between 1.19% and 2.82%. The superior results of StrConcat highlight the ability of Transformer-based models to effectively process multi-modal inputs through their self-attention mechanism, capability that has been demonstrated in various downstream tasks [69, 73, 83]. Code context length: Table 5 shows that the F1-Score of four models is significantly improved when using either two lines of code context or the entire function code. The tendency to favor two surrounding code lines during prediction aligns with human intuition, which relies on local context for annotation. Moreover, incorporating the entire function code highlights the capability of these models to leverage global context. This context could enhance the performance by enabling models to identify relevant code snippets across the function, offering more nuanced understanding of the functions structure and semantic, thereby potentially improving the reliability of the models. While employing code context generally demonstrates improvements over using comments alone, varying the context length might impact model performance differTRANSACTIONS ON SOFTWARE ENGINEERING, VOL. XX, NO. YY 12 ently. Consequently, we employ an ensemble approach to combine model predictions across different code context lengths. Specifically, for each model majority voting mechanism was applied to produce the final prediction. Each model was configured with varying code context lengths, including 2, 10, 20 lines and the entire function code, considering CodeAtt as the input concatenation approach for RoBERTa and StrConcat for the other models. As shown in the last column of Table 5, this ensemble approach achieves the highest performance across all four models, underscoring the advantage of leveraging multiple code context lengths for identifying SATD comments. Answer to RQ2 Incorporating comments with source code information results in performance improvements across various models compared to using comments alone, highlighting the robustness, versatility, and adaptability of this approach in detecting SATD. The proposed methods, StrConcat and CodeAtt, effectively utilize the source code context and enhance the performance across all evaluated models. This paves the way for future research with the ultimate aim of further improving the prediction. When comment and code context are combined as input, an optimal performance is achieved with 2 surrounding code lines or by including the entire code function. Combining various scopes demonstrates the effective contribution of each scope, highlighting the potential of multi-code scope strategies in improving SATD detection. 5.3 RQ3: What is the accuracy of different pre-trained models when detecting TD solely from source code? Motivation: As mentioned earlier, technical debt is frequently identified through textual content, such as comments or issue reports. However, when such debt is not explicitly documented, existing tools are unable to detect it, despite its presence in the code. Moreover, many comments become outdated or inconsistent with the actual code, as developers often fail to update comments after modifying the code. This discrepancy between comments and code introduces significant blind spot for tools that rely solely on textual indicators, limiting their ability to accurately detect technical debt. Therefore, there is pressing need for more advanced approaches that surpass textual cues to effectively identify and manage technical debt within code bases. Approach: We designed scenario where TD detection relies solely on source code. Specifically, we constrained the scope to the function level for practical application, as analyzing the entire file is lengthy and challenging for users to segment after detecting TD. We utilized TESOROcode for our experiments, addressing it as multi-label classification problem. In order to extract information from source code, we have investigated different PLMs, categorized into three architectures as detailed in Section 2.2.2. Encoder-based PLMs: Language models that are based on Transformer architecture utilizing the Encoder layer. TABLE 6: Performance of different PLMs on TD detection using TESOROcode. Model Model size EM Encoder-based PLMs CodeBERT [22] UniXCoder [23] GraphCodeBERT [24] RoBERTa [39] ALBERT [31] 125M 125M 125M 125M 11.8M 38.28 38.12 39.38 35.37 39.32 43.47 42.58 44.21 38.22 41.99 Encoder-Decoder-based PLMs PLBART [2] Codet5 [71] CodeT5+ [70] 140M 220M 220M 36.85 32.66 37.91 39.90 35.41 41.96 Decoder-based PLMs (LLMs) TinyLlama [79] DeepSeek-Coder [82] OpenCodeInterpreter [25] phi-2 [54] starcoder2 [40] CodeLlama [54] Magicoder [75] 1.03B 1.28B 1.35B 2.78B 3.03B 6.74B 6.74B 37.05 42.52 38.16 37.92 35.37 34.14 39.14 40.05 46.19 41.76 41.57 41.77 38.16 42.49 Encoder-Decoder-based PLMs: Language models, built on top of Transformer architecture, that leverage both the Encoder and Decoder layers. Decoder-based PLMs: Language models that use Decoder layer of Transformer architecture, trained with Causal Language Modeling. The models, characterized by large number of parameters (in billions), are commonly referred as Large Language Models (LLMs). We conducted experiments on 16 models, including 5 Encoder-based PLMs, 3 Encoder-Decoder-based PLMs, and 8 Decoder-based PLMs. Since only code is used as input, and Sections 5.1 and 5.2 demonstrated the superior performance of code-based PLMs, we experiment with models primarily pre-trained on coding corpora. The language model head layer is replaced with linear classification head during finetuning on the downstream task. All models are fine-tuned for 10 epochs using batch size of 32 and learning rate of 1e5. For models with more than 6 billion parameters, we utilize LoRA [27] with learning rate of 1e 3 for fine-tuning due to resource constraints. For decoder-base PLMs, we apply template for input presented in the online appendix, and use the embedding of final tokens as the representation fed into the classification head. We randomly split TESORO into 10 folds for cross-validation and report the average Exact Match (EM) and F1-score across these folds for each model. Result: Table 6 shows the experimental results, with bold text indicating the highest score, while underlined scores representing the runner-up. DeepSeek-Coder achieves the best performance with an F1-score of 46.19% marking an improvement of 4.98% over the second highest one, i.e., GraphCodeBERT getting 44.21%. Though previous studies indicated limited adaptability of LLMs to classification tasks [63, 80], these results highlight the potential of such models. However, models containing the Decoder module generally exhibit lower performance compared to EncoderTRANSACTIONS ON SOFTWARE ENGINEERING, VOL. XX, NO. YY 13 based models. Figure 7 supports this observation, as the three models following DeepSeek-Coder are all Encoderbased. Several factors can account for this observation. Firstly, Encoder-Decoder and Decoder-based models are pretrained on generation tasks, which may result in suboptimal performance on classification tasks due to the lack of task-specific optimization. Secondly, some studies [5, 34] showed that Decoder-based models are less effective for text representation or embeddings due to their causal attention mechanism, which limits the models ability to learn robust representations. Hence, the embedding information before the classification head is not sufficiently rich, leading to suboptimal performance. Though Encoder-based models experience slight performance drop compared to that of DeepSeek-Coder, they achieve this with significantly fewer parametersaround 90% lessoffering more practical approach to TD detection using source code. Fig. 7: F1-score of various PLMs on TESOROcode across different model sizes, types, and pretraining datasets. denotes NL-based PLMs; represents code-based PLMs. Figure 7 further highlights the superior performance of code-based PLMs compared to NL-based PLMs when considering models of comparable size. For example, within the group of models containing 100M to 200M parameters, GraphCodeBERT achieves the highest F1-score of 44.21%. Similarly, models with sizes around 1B and 3B parameters also exhibit the best performance with two code-based PLMs, DeepSeek-Coder and StarCoder2, respectively. This further reinforces the superiority of code-based PLMs in this scenario. However, the performance of all models remains below 50% in both EM and F1-score, indicating the need for more advanced approaches and further improvements. Answer to RQ3 DeepSeek-Coder achieves the highest accuracy on TESOROcode. In contrast, Encoder-based models exhibit slight performance drop but with substantial reduction in parameters, making them more practical in real-world scenarios. Code-based PLMs show superiority in the detection of TD from source code. However, their performance remains below 50% in both EM and F1-score. This indeed highlights the need for more advanced methods and further research."
        },
        {
            "title": "6 DISCUSSION",
            "content": "We now discuss possible impacts of our work, and highlight the threats to validity of the findings. 6.1 Implications Our work has the following implications: Unlike existing approaches, which rely solely on textual data such as issue reports, comments, commits to detect TD, we propose using source code as means to facilitate such the detection. This may have significant implications in practice, as source code and the associated text might not be coherent, and using only source code helps us capture the intrinsic debt, without relying on the presence of any accompanying textual data. With the curated datasets, we expect to lay the foundations for new method to detect TD. This may be beneficial to industry, as software companies could make use of our datasets to train tailored machine learning models to recognize TD directly from their source code. This would help them save time and effort, thereby increasing the overall productivity. The results in RQ3 show that the PLMs we considered achieve mediocre results when detecting TD in source code. This implies that there is still room for improvement: more advanced detection models are needed. We anticipate that LLMs could be an eventual solution to this problem, as they have been trained with huge amount of data including source code, with the potential to better capture the intrinsic features of TD contained in source code. This, however, needs further refinement and empirical evidence and is part of our future work. The curated dataset is expected to advance research in technical debt detection from source code, and holds the potential to facilitate the identification of other software artifacts, such as code smells. 6.2 Threats to validity We see the following threats to validity related to this research: Internal validity. This threat is related to the confounding factors that might impact the validity of the evaluation results. The dataset that we used to train the classifier to look for additional SATD comments could cause the engine to harvest false positives if it is not properly curated. To mitigate this threat, we used the preexisting Maldonado-62K dataset, which was carefully classified, and has been utilized in various studies. When conducting the user study, we tried to avoid any bias by involving seven Computer Science students with significant programming experience in the manual evaluation step. In addition, the results of each student were then double checked by another student to resolve any conflicts and to increase the reliability of the results. External validity. This threat concerns the generalizability of our findings. We used SATD comments extracted from projects to train classifier to locate Java source code containing TD. While it seems reasonable to assume that this method could also be used to identify TD TRANSACTIONS ON SOFTWARE ENGINEERING, VOL. XX, NO. YY 14 contained in other programming languages, we have not shown that. In this paper we managed to validate our hypothesis only on code written in Java. Therefore future work is required to further validate the generalizability of our pipeline to other languages. [58] demonstrated that ALBERT [31] and RoBERTa [39] significantly outperform traditional machine learning and CNN-based methods in identifying and classifying SATD categories in the language. Furthermore, Sheikhari et al. [59] illustrated the superior performance of the LLM model Flan-T5 [12] across three SATD-related tasks."
        },
        {
            "title": "7 RELATED WORK",
            "content": "This section reviews related studies that address the problem of technical debt (TD). It also highlights various datasets, approaches, and methods developed to identify and detect TD across different software systems. Technical debt detection datasets: The availability of datasets is critical for advancing research in technical debt related tasks, providing empirical evidence to validate the effectiveness of detection techniques. Large-scale studies of TD frequently focus on specific category, with SelfAdmitted Technical Debt (SATD) being prevalent area of focus. Maldonado et al. [16, 42] presented widely utilized dataset, comprising more than 62,000 comments from 10 Java projects, classified into five types of TD and non-SATD. Instead of using only comment, Li et al. [37] introduced the dataset by investigating four different sources: code comments, commit messages, pull requests, and issue tracking systems. In addition, they merged code debt and design debt into single category due to their high similarity. To explore programming languages beyond Java, Sharma et al. [58] introduced dataset for examining SATD in the language, which includes over 140,000 samples and expands the number of TD types to 12. Furthermore, several datasets [30, 41, 45, 47] have been introduced to address code smells, which is related issue to TD. SATD detection techniques: Early methods for identifying and detecting SATD relied on rule-based approaches that searched for matching keywords or phrases [26, 62]. For example, the Matches Task Annotation Tags (MAT) [26] method demonstrated that simply matching set of commonly used task annotation tags, such as TODO, FIXME, HACK, and XXX, can achieve significant performance in identifying SATD. In PENTACET, Sridharan et al. [62] built upon the 64 SATD detection patterns initially introduced by Potdar and Shihab [48]. By leveraging the Sense2Vec tool, they expanded the set to 1,041 patterns, significantly increasing the scope for identifying SATD. More advanced methods have been proposed using machine learning approaches [16, 42, 55], where classifiers like maximum entropy and Naive Bayes multinomial classifiers are trained to detect various types of TD. Recently, supervised deep learning methods have been introduced, demonstrating superior performance compared to rule-based and traditional machine learning approaches. Li et al. [37] introduced method called MT-Text-CNN, which utilizes convolutional neural network combined with multi-task learning to detect SATD across multiple sources. Meanwhile, Yu et al. [77] proposed method utilizing bidirectional long short-term memory (BiLSTM) networks with an attention mechanism and balanced crossentropy loss function to mitigate the imbalance problem in SATD identification. Besides, several approaches have utilized pretrained language models, achieving state-ofthe-art performance in SATD-related tasks. Sharma et al."
        },
        {
            "title": "8 CONCLUSIONS AND FUTURE WORK\nIn this work we created a pipeline to augment existing\ndatasets for investigating SATD. By means of an empirical\nevaluation, we demonstrate that the pipeline improves both\nthe quality and the scope of data used in SATD research\nby employing a selection strategy that identifies informa-\ntive examples, thereby minimizing the manual labeling\neffort. Moreover, our study highlights the effectiveness\nof integrating additional source code context into SATD\ndetection. This strategy improves both the accuracy and\nrobustness of existing models. Our study is the first to\nprovide a comprehensive analysis of how to efficiently\nleverage code context by examining different scopes sur-\nrounding comments. Additionally, we propose two effective\nmethods to incorporate this contextual information and an\nensemble approach combining multi-scope results to achieve\nsuperior performance in detecting SATD in code comments.\nFurthermore, we conducted extensive experiments using a\nlarge number of models, diverse in size, architecture, and\nknowledge domain, on a novel scenario–detecting technical\ndebt in source code. These experiments provide valuable\ninsights into the performance and adaptability of various\nmodels in this context.",
            "content": "We see this as just the first step in program of research. In our future research we can further improve the context for detecting technical debt in source code by incorporating additional information such as execution outputs, test coverage, and runtime metrics. These enhancements could lead to more accurate and comprehensive technical debt detection methods, which could benefit software maintenance and quality assurance processes. ACKNOWLEDGMENTS This work has been partially supported by the EMELIOT national research project, which has been funded by the MUR under the PRIN 2020 program (Contract 2020W3A5FY). The work has been also partially supported by the European UnionNextGenerationEU through the Italian Ministry of University and Research, Projects PRIN 2022 PNRR FRINGE: context-aware FaiRness engineerING in complex software systEms grant n. P2022553SL. We acknowledge the Italian PRIN 2022 project TRex-SE: Trustworthy Recommenders for Software Engineers, grant n. 2022LKJWHC. Our research is also funded by Hanoi University of Science and Technology (HUST), Vietnam under project number T2023PC-002. REFERENCES [1] J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. Almeida, J. Altenschmidt, S. Altman, S. Anadkat et al., Gpt-4 technical report, arXiv preprint arXiv:2303.08774, 2023. TRANSACTIONS ON SOFTWARE ENGINEERING, VOL. XX, NO. YY [2] W. U. Ahmad, S. Chakraborty, B. Ray, and K.-W. Chang, Unified pre-training for program understanding and generation, arXiv preprint arXiv:2103.06333, 2021. [3] N. S. Alves, T. S. Mendes, M. G. De Mendonça, R. O. Spínola, F. Shull, and C. Seaman, Identification and management of technical debt: systematic mapping study, Information and Software Technology, vol. 70, pp. 100121, 2016. [4] R. Artstein, Inter-annotator agreement, Handbook of linguistic annotation, pp. 297313, 2017. [5] P. BehnamGhader, V. Adlakha, M. Mosbach, D. Bahdanau, N. Chapados, and S. Reddy, Llm2vec: Large language models are secretly powerful text encoders, arXiv preprint arXiv:2404.05961, 2024. [6] S. Bellomo, R. L. Nord, I. Ozkaya, and M. Popeck, Got technical debt? surfacing elusive technical debt in issue trackers, in Proceedings of the 13th international conference on mining software repositories, 2016, pp. 327338. [7] T. B. Brown, Language models are few-shot learners, arXiv preprint arXiv:2005.14165, 2020. [8] Y. Cai and R. Kazman, Software design analysis and technical debt management based on design rule theory, Information and Software Technology, vol. 164, 2024. [9] K. Chitta, J. M. Álvarez, E. Haussmann, and C. Farabet, Training data subset search with ensemble active learning, IEEE Transactions on Intelligent Transportation Systems, vol. 23, no. 9, pp. 14 74114 752, 2021. [10] K. Cho, B. Van Merriënboer, D. Bahdanau, and Y. Bengio, On the properties of neural machine translation: Encoder-decoder approaches, arXiv preprint arXiv:1409.1259, 2014. [11] A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts, P. Barham, H. W. Chung, C. Sutton, S. Gehrmann et al., Palm: Scaling language modeling with pathways, Journal of Machine Learning Research, vol. 24, no. 240, pp. 1113, 2023. [12] H. W. Chung, L. Hou, S. Longpre, B. Zoph, Y. Tay, W. Fedus, Y. Li, X. Wang, M. Dehghani, S. Brahma et al., Scaling instruction-finetuned language models, Journal of Machine Learning Research, vol. 25, no. 70, pp. 153, 2024. [13] C. Coleman, C. Yeh, S. Mussmann, B. Mirzasoleiman, P. Bailis, P. Liang, J. Leskovec, and M. Zaharia, Selection via proxy: Efficient data selection for deep learning, arXiv preprint arXiv:1906.11829, 2019. [14] A. Corazza, V. Maggio, and G. Scanniello, Coherence of comments and method implementations: dataset J., and an empirical vol. 26, no. 2, pp. 751777, 2018. [Online]. Available: https://doi.org/10.1007/s11219-016-9347-1 investigation, Softw. Qual. [15] W. Cunningham, The wycash portfolio management system, ACM Sigplan Oops Messenger, vol. 4, no. 2, pp. 2930, 1992. [16] E. da Silva Maldonado, E. Shihab, and N. Tsantalis, Using natural language processing to automatically detect self-admitted technical debt, IEEE Transactions on Software Engineering, vol. 43, no. 11, pp. 10441062, 2017. [17] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, Bert: Pre-training of deep bidirectional transformers for language understanding, arXiv preprint arXiv:1810.04805, 2018. [18] A. Di Salle, A. Rota, P. T. Nguyen, D. Di Ruscio, F. A. Fontana, and I. Sala, Pilot: Synergy between text processing and neural networks to detect self-admitted technical debt, in 2022 IEEE/ACM International Conference on Technical Debt (TechDebt), 2022, pp. 4145. [19] K. E. Emam, Benchmarking kappa: Interrater agreement in software process assessments, Empirical Software Engineering, vol. 4, pp. 113133, 1999. [20] N. Ernst, J. Delange, and R. Kazman, Technical debt in practicehow to find it and fix it. MIT Press, 2021. [21] N. A. Ernst, S. Bellomo, I. Ozkaya, R. L. Nord, and I. Gorton, Measure it? manage it? ignore it? software practitioners and technical debt, in Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering, 2015, pp. 5060. [22] Z. Feng, D. Guo, D. Tang, N. Duan, X. Feng, M. Gong, L. Shou, B. Qin, T. Liu, D. Jiang, and M. Zhou, Codebert: pre-trained model for programming and natural languages, in Findings of the Association for Computational Linguistics: EMNLP 2020, Online Event, 16-20 November 2020, ser. Findings of ACL, vol. EMNLP 2020. Association for Computational Linguistics, 2020, pp. 15361547. [23] D. Guo, S. Lu, N. Duan, Y. Wang, M. Zhou, and J. Yin, Unixcoder: Unified cross-modal pre-training for code representation, in Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022, S. Muresan, P. Nakov, and A. Villavicencio, Eds. Association for Computational Linguistics, 2022, pp. 72127225. [24] D. Guo, S. Ren, S. Lu, Z. Feng, D. Tang, S. Liu, L. Zhou, N. Duan, A. Svyatkovskiy, S. Fu et al., Graphcodebert: Pre-training code representations with data flow, arXiv preprint arXiv:2009.08366, 2020. [25] D. Guo, Q. Zhu, D. Yang, Z. Xie, K. Dong, W. Zhang, G. Chen, X. Bi, Y. Wu, Y. Li et al., Deepseek-coder: When the large language model meets programmingthe rise of code intelligence, arXiv preprint arXiv:2401.14196, 2024. [26] Z. Guo, S. Liu, J. Liu, Y. Li, L. Chen, H. Lu, and Y. Zhou, How far have we progressed in identifying self-admitted technical debts? comprehensive empirical study, ACM Transactions on Software Engineering and Methodology (TOSEM), vol. 30, no. 4, pp. 156, 2021. [27] E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and W. Chen, Lora: Low-rank adaptation of large language models, arXiv preprint arXiv:2106.09685, 2021. [28] M. D. Igbomezie, P. T. Nguyen, and D. Di Ruscio, When simplicity meets effectiveness: Detecting code comments coherence with word embeddings and lstm, ser. EASE 24. New York, NY, USA: Association for Computing Machinery, 2024, p. 411416. [Online]. Available: https://doi.org/10.1145/3661167.3661187 [29] D. Kocetkov, R. Li, L. B. Allal, J. Li, C. Mou, C. M. Ferrandis, Y. Jernite, M. Mitchell, S. Hughes, T. Wolf et al., The stack: 3 tb of permissively licensed source code, arXiv preprint arXiv:2211.15533, 2022. TRANSACTIONS ON SOFTWARE ENGINEERING, VOL. XX, NO. YY 16 [30] A. Kovaˇcevic, N. Luburic, J. Slivka, S. Prokic, K.-G. Grujic, D. Vidakovic, and G. Sladic, Automatic detection of code smells using metrics and codet5 embeddings: case study in c#, Neural Computing and Applications, vol. 36, no. 16, pp. 92039220, 2024. [31] Z. Lan, M. Chen, S. Goodman, K. Gimpel, P. Sharma, and R. Soricut, Albert: lite bert for self-supervised learning of language representations, arXiv preprint arXiv:1909.11942, 2019. [32] J. R. Landis and G. G. Koch, The measurement of observer agreement for categorical data, biometrics, pp. 159174, 1977. [33] V. Le Quoc et al., Sequence to sequence learning with neural networks, Advances in neural information processing systems, vol. 27, pp. 31043112, 2014. [34] C. Lee, R. Roy, M. Xu, J. Raiman, M. Shoeybi, B. Catanzaro, and W. Ping, Nv-embed: Improved techniques for training llms as generalist embedding models, arXiv preprint arXiv:2405.17428, 2024. [35] M. Lewis, Bart: Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension, arXiv preprint arXiv:1910.13461, 2019. [36] Y. Li, M. Soliman, and P. Avgeriou, Identification and remediation of self-admitted technical debt in issue trackers, in 2020 46th Euromicro Conference on Software Engineering and Advanced Applications (SEAA), 2020, pp. 495503. [37] , Automatic identification of self-admitted technical debt from four different sources, Empirical Software Engineering, vol. 28, no. 3, p. 65, 2023. [38] Z. Li, P. Avgeriou, and P. Liang, systematic mapping study on technical debt and its management, Journal of Systems and Software, vol. 101, pp. 193220, 2015. [39] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer, and V. Stoyanov, Roberta: robustly optimized bert pretraining approach, arXiv preprint arXiv:1907.11692, 2019. [40] A. Lozhkov, R. Li, L. B. Allal, F. Cassano, J. LamyPoirier, N. Tazi, A. Tang, D. Pykhtar, J. Liu, Y. Wei et al., Starcoder 2 and the stack v2: The next generation, arXiv preprint arXiv:2402.19173, 2024. [41] L. Madeyski and T. Lewowski, Mlcq: Industry-relevant code smell data set, in Proceedings of the 24th International Conference on Evaluation and Assessment in Software Engineering, 2020, pp. 342347. [42] E. d. S. Maldonado and E. Shihab, Detecting and quantifying different types of self-admitted technical debt, in 2015 IEEE 7Th international workshop on managing technical debt (MTD). IEEE, 2015, pp. 915. [43] D. N. Manh, N. L. Hai, A. T. V. Dau, A. M. Nguyen, K. Nghiem, J. Guo, and N. D. Q. Bui, The vault: comprehensive multilingual dataset for advancing code understanding and generation, in Findings of the Association for Computational Linguistics: EMNLP 2023, Singapore, December 6-10, 2023, H. Bouamor, J. Pino, and K. Bali, Eds. Association for Computational Linguistics, 2023, pp. 47634788. [44] S. Minaee, T. Mikolov, N. Nikzad, M. Chenaghlu, R. Socher, X. Amatriain, and J. Gao, Large language models: survey, arXiv preprint arXiv:2402.06196, 2024. [45] H. Nandani, M. Saad, and T. Sharma, Dacosa manually annotated dataset of code smells, in 2023 IEEE/ACM 20th International Conference on Mining Software Repositories (MSR). IEEE, 2023, pp. 446450. [46] E. Nijkamp, B. Pang, H. Hayashi, L. Tu, H. Wang, Y. Zhou, S. Savarese, and C. Xiong, Codegen: An open large language model for code with multi-turn program synthesis, arXiv preprint arXiv:2203.13474, 2022. [47] F. Palomba, D. Di Nucci, M. Tufano, G. Bavota, R. Oliveto, D. Poshyvanyk, and A. De Lucia, Landfill: An open dataset of code smells with public evaluation, in 2015 IEEE/ACM 12th Working Conference on Mining Software Repositories. IEEE, 2015, pp. 482485. [48] A. Potdar and E. Shihab, An exploratory study on self-admitted technical debt, in 2014 IEEE International Conference on Software Maintenance and Evolution. IEEE, 2014, pp. 91100. [49] F. Rabbi and M. S. Siddik, Detecting code comment inconsistency using siamese recurrent network, in Proceedings of the 28th international conference on program comprehension, 2020, pp. 371375. [50] A. Radford, Improving language understanding by generative pre-training, 2018. [51] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever et al., Language models are unsupervised multitask learners, OpenAI blog, vol. 1, no. 8, p. 9, 2019. [52] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J. Liu, Exploring the limits of transfer learning with unified text-to-text transformer, Journal of machine learning research, vol. 21, no. 140, pp. 167, 2020. [53] X. Ren, Z. Xing, X. Xia, D. Lo, X. Wang, and J. Grundy, Neural network-based detection of self-admitted technical debt: From performance to explainability, ACM transactions on software engineering and methodology (TOSEM), vol. 28, no. 3, pp. 145, 2019. [54] B. Roziere, J. Gehring, F. Gloeckle, S. Sootla, I. Gat, X. E. Tan, Y. Adi, J. Liu, T. Remez, J. Rapin et al., Code llama: Open foundation models for code, arXiv preprint arXiv:2308.12950, 2023. [55] I. Sala, A. Tommasel, and F. Arcelli Fontana, Debthunter: machine learning-based approach for detecting self-admitted technical debt, in Proceedings of the 25th International Conference on Evaluation and Assessment in Software Engineering, 2021, pp. 278283. [56] J. Sarzynska-Wawer, A. Wawer, A. Pawlak, J. Szymanowska, I. Stefaniak, M. Jarkiewicz, and L. Okruszek, Detecting formal thought disorder by deep contextualized word representations, Psychiatry Research, vol. 304, p. 114135, 2021. [57] B. Settles, Active learning literature survey, 2009. [58] R. Sharma, R. Shahbazi, F. H. Fard, Z. Codabux, and M. Vidoni, Self-admitted technical debt in r: detection and causes, Automated Software Engineering, vol. 29, no. 2, p. 53, 2022. [59] M. S. Sheikhaei, Y. Tian, S. Wang, and B. Xu, An empirical study on the effectiveness of large language models for satd identification and classification, arXiv preprint arXiv:2405.06806, 2024. TRANSACTIONS ON SOFTWARE ENGINEERING, VOL. XX, NO. YY 17 Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering, 2020, pp. 349360. [74] J. Wei, M. Bosma, V. Y. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du, A. M. Dai, and Q. V. Le, Finetuned language models are zero-shot learners, arXiv preprint arXiv:2109.01652, 2021. [75] Y. Wei, Z. Wang, J. Liu, Y. Ding, and L. Zhang, Magicoder: Source code is all you need, arXiv preprint arXiv:2312.02120, 2023. [76] L. Xiao, R. Kazman, Y. Cai, R. Mo, and Q. Feng, Detecting the locations and predicting the costs of compound architectural debts, IEEE Trans. Software Engineering, vol. 48, no. 9, pp. 36863715, September 2022. [77] D. Yu, L. Wang, X. Chen, and J. Chen, Using bilstm with attention mechanism to automatically detect selfadmitted technical debt, Frontiers of Computer Science, vol. 15, no. 4, p. 154208, 2021. [78] N. Zazworka, M. A. Shaw, F. Shull, and C. Seaman, Investigating the impact of design debt on software quality, in Proceedings of the 2nd workshop on managing technical debt, 2011, pp. 1723. [79] P. Zhang, G. Zeng, T. Wang, and W. Lu, Tinyllama: An open-source small language model, arXiv preprint arXiv:2401.02385, 2024. [80] Y. Zhang, M. Wang, C. Ren, Q. Li, P. Tiwari, B. Wang, and J. Qin, Pushing the limit of llm capacity for text classification, arXiv preprint arXiv:2402.07470, 2024. [81] W. X. Zhao, K. Zhou, J. Li, T. Tang, X. Wang, Y. Hou, Y. Min, B. Zhang, J. Zhang, Z. Dong et al., survey of large language models, arXiv preprint arXiv:2303.18223, 2023. [82] T. Zheng, G. Zhang, T. Shen, X. Liu, B. Y. Lin, J. Fu, W. Chen, and X. Yue, Opencodeinterpreter: Integrating code generation with execution and refinement, arXiv preprint arXiv:2402.14658, 2024. [83] Y. Zhou, S. Yang, X. Chen, Z. Zhang, and J. Pei, Qtc4so: Automatic question title completion for stack overflow, in 2023 IEEE/ACM 31st International Conference on Program Comprehension (ICPC). IEEE, 2023, pp. 112. [60] G. Sierra, E. Shihab, and Y. Kamei, survey of selfadmitted technical debt, Journal of Systems and Software, vol. 152, pp. 7082, 2019. [61] B. Sorscher, R. Geirhos, S. Shekhar, S. Ganguli, and A. Morcos, Beyond neural scaling laws: beating power law scaling via data pruning, Advances in Neural Information Processing Systems, vol. 35, pp. 19 52319 536, 2022. [62] M. Sridharan, L. Rantala, and M. Mäntylä, Pentacet data-23 million contextual code comments and 250,000 satd comments, in 2023 IEEE/ACM 20th International Conference on Mining Software Repositories (MSR). IEEE, 2023, pp. 412416. [63] X. Sun, X. Li, J. Li, F. Wu, S. Guo, T. Zhang, and G. Wang, Text classification via large language models, in Findings of the Association for Computational Linguistics: EMNLP 2023, H. Bouamor, J. Pino, and K. Bali, Eds. Singapore: Association for Computational Linguistics, Dec. 2023, pp. 89909005. [Online]. Available: https: //aclanthology.org/2023.findings-emnlp. technical debt [64] J. Tan, D. Feitosa, and P. Avgeriou, The lifecycle of that manifests in both source code and issue trackers, Information and Software Technology, vol. 159, p. 107216, 2023. [Online]. Available: https://www.sciencedirect.com/science/ article/pii/S0950584923000708 [65] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozière, N. Goyal, E. Hambro, F. Azhar et al., Llama: Open and efficient foundation language models, arXiv preprint arXiv:2302.13971, 2023. [66] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale et al., Llama 2: Open foundation and fine-tuned chat models, arXiv preprint arXiv:2307.09288, 2023. [67] E. Tuna, C. Seaman, and E. Tüzün, Do code reviews lead to fewer code smells? Journal of Systems and Software, vol. 215, p. 112101, 2024. [Online]. Available: https://www.sciencedirect.com/ science/article/pii/S0164121224001468 [68] A. Vaswani, Attention is all you need, Advances in Neural Information Processing Systems, 2017. [69] D. Wang, Z. Jia, S. Li, Y. Yu, Y. Xiong, W. Dong, and X. Liao, Bridging pre-trained models and downstream tasks for source code understanding, in Proceedings of the 44th International Conference on Software Engineering, 2022, pp. 287298. [70] Y. Wang, H. Le, A. D. Gotmare, N. D. Bui, J. Li, and S. C. Hoi, Codet5+: Open code large language models for code understanding and generation, arXiv preprint arXiv:2305.07922, 2023. [71] Y. Wang, W. Wang, S. Joty, and S. C. Hoi, Codet5: Identifier-aware unified pre-trained encoder-decoder models for code understanding and generation, arXiv preprint arXiv:2109.00859, 2021. [72] S. Wehaibi, E. Shihab, and L. Guerrouj, Examining the impact of self-admitted technical debt on software quality, in 2016 IEEE 23Rd international conference on software analysis, evolution, and reengineering (SANER), vol. 1. IEEE, 2016, pp. 179188. [73] B. Wei, Y. Li, G. Li, X. Xia, and Z. Jin, Retrieve and refine: exemplar-based neural comment generation, in"
        }
    ],
    "affiliations": []
}