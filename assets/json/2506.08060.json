{
    "paper_title": "Eliciting Fine-Tuned Transformer Capabilities via Inference-Time Techniques",
    "authors": [
        "Asankhaya Sharma"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large language models have transformed natural language processing, yet supervised fine-tuning (SFT) remains computationally intensive. This paper formally proves that capabilities acquired through SFT can be approximated by a base transformer model using inference-time techniques, specifically in-context learning (ICL), without altering model parameters, under idealized assumptions including unbounded computational resources and access to the fine-tuning dataset. We extend these results to practical scenarios with finite context lengths and partial dataset access. For text generation tasks with fixed output length $l$, datasets of size $\\mathrm{O}\\left( \\frac{m V}{\\varepsilon^2} \\log \\frac{m}{\\delta} \\right)$ or, with bounded context, $\\mathrm{O}\\left( \\frac{l \\log V}{\\varepsilon^2} \\log \\frac{1}{\\delta} \\right)$ suffice to approximate fine-tuned behavior across $m$ contexts within error $\\varepsilon$, where $V$ is the vocabulary size and $\\delta$ is the failure probability. For linear classification, datasets of size $\\mathrm{O}\\left( \\frac{d}{\\varepsilon} \\right)$ or, with fixed context, $\\mathrm{O}\\left( \\frac{1}{\\varepsilon^2} \\log \\frac{1}{\\delta} \\right)$ are sufficient, where $d$ is the input dimension. Grounded in the Turing completeness of transformers, these results provide a theoretical foundation for resource-efficient deployment of large language models, with practical techniques like retrieval-augmented generation bridging theory to real-world applications."
        },
        {
            "title": "Start",
            "content": "Eliciting Fine-Tuned Transformer Capabilities via Inference-Time Techniques Asankhaya Sharma Patched Codes, Inc. asankhaya@patchedcodes.com"
        },
        {
            "title": "Abstract",
            "content": "Large language models have transformed natural language processing, yet supervised fine-tuning (SFT) remains computationally intensive. This paper formally proves that capabilities acquired through SFT can be approximated by base transformer model using inference-time techniques, specifically in-context learning (ICL), without altering model parameters, under idealized assumptions including unbounded computational resources and access to the fine-tuning dataset. We extend these results to practical scenarios with finite context lengths and partial dataset access. For text generation tasks with fixed output length l, datasets of size (cid:0) mV suffice to approximate fine-tuned behavior across contexts within error ε, where is the vocabulary size and δ is the failure probability. For linear classification, datasets of size (cid:0) (cid:1) or, with fixed context, (cid:0) 1 (cid:1) are sufficient, where is the input dimension. Grounded in the Turing completeness of transformers, these results provide theoretical foundation for resource-efficient deployment of large language models, with practical techniques like retrieval-augmented generation bridging theory to real-world applications. (cid:1) or, with bounded context, (cid:16) log ε2 ε2 log ε2 log 1 log 1 δ (cid:17) ε δ δ"
        },
        {
            "title": "Introduction",
            "content": "Transformer models, introduced by Vaswani et al. [1], are the backbone of natural language processing (NLP), powering large language models (LLMs) like DeepSeek-R1 [3] and Claude 4 [4]. These models use self-attention to capture long-range dependencies, achieving breakthroughs in tasks such as language modeling, translation, and text generation. However, supervised fine-tuning (SFT) to adapt pre-trained transformers to specific tasks is computationally expensive, often requiring thousands of GPU hours [3]. This prompts key question: Can the capabilities gained through SFT be elicited from base transformer using inference-time techniques, such as in-context learning (ICL), without parameter updates? 5 2 0 2 9 ] . [ 1 0 6 0 8 0 . 6 0 5 2 : r ICL and prompting allow models to adapt to tasks by conditioning on input-output examples [27; 9]. If fine-tuned capabilities are latent in the base model, SFT may primarily refine access to pre-existing knowledge [18; 11]. This paper provides formal proof that, under idealized conditions (unbounded computational resources and access to the fine-tuning dataset), base transformer can approximate SFT capabilities via ICL within quantifiable error margin. We extend these results to practical settings with finite context lengths and partial dataset access, demonstrating that minimal datasets can approximate fine-tuned behavior. Specifically, for text generation, dataset of size (cid:0) mV (cid:1) ε2 log (cid:16) log or, with fixed context and output length l, approximates fine-tuned distributions ε2 across contexts, where is the vocabulary size, ε is the error tolerance, and δ is the failure prob- (cid:1) suffice, ability. For linear classification, datasets of size (cid:0) (cid:1) or, with fixed context, (cid:0) 1 log 1 δ (cid:17) δ ε2 log 1 δ ε Preprint. where is the input dimension. Rooted in the Turing completeness of transformers [2], these results establish framework for resource-efficient LLM deployment, with practical approximations like retrieval-augmented generation (RAG) enhancing real-world applicability. Figure 1 illustrates our approach, showing how base model, prompted with dataset D, approximates the fine-tuned models output distribution. Base Model (θbase) Prompting Prompt with + Query Pbase(yx, D) Fine-Tuning Approx. SFT Model (θfine) Pfine(yx) Figure 1: Overview of eliciting fine-tuned capabilities. base model, prompted with dataset and query x, produces an output distribution Pbase(yx, D) approximating the fine-tuned models distribution Pfine(yx). Our contributions are: 1. proof that base transformers can approximate SFT capabilities via ICL under unbounded resources, within error ε (Section 5). 2. Demonstrations that ε2 log (cid:0) mV δ (cid:1) or, with fixed context, (cid:16) log ε2 (cid:17) log 1 δ text generation can be approximated with datasets of size (Sections 5.5.1, 5.6.2). 3. Proofs that linear classifiers can be approximated with datasets of size (cid:0) (cid:1) or, with fixed ε context, (cid:0) 1 ε2 log 1 δ (cid:1) (Sections 5.5.2, 5.6.1). 4. Practical techniques, such as RAG and few-shot prompting, to bridge theoretical results to applications (Section 6)."
        },
        {
            "title": "2 Related Work",
            "content": "2.1 Computational Power of Transformers Transformers with unbounded resources are Turing-complete [2], with self-attention simulating Turing machine tapes and feed-forward layers encoding transition rules [14]. While fixed-depth transformers may not be Turing-complete [7], modified architectures achieve this [8]. As universal approximators [15], transformers support our claim that fine-tuned behaviors can be approximated without parameter updates [34]. 2.2 In-Context Learning ICL enables task adaptation by conditioning on input-output examples [27]. For example, sentimentlabeled reviews allow label prediction for new inputs. DeepSeek-R1 shows robust zero-shot ICL [3], and structured prompts enhance reasoning [5]. ICL is modeled as Bayesian inference [9], with pretrained transformers generalizing well [32]. Instruction tuning boosts zero-shot performance [6], forming the basis for our framework. 2.3 Fine-Tuning and Latent Knowledge SFT refines latent capabilities in LLMs. Early layers retain general knowledge [10], while the tuned lens reveals latent predictions [11]. Low-rank adaptation (LoRA) shows minimal updates suffice [13]. Recent work suggests SFT reformats outputs for task-specific styles [18], supporting our hypothesis that base models can approximate fine-tuned behaviors via inference. 2 2.4 Inference-Time Alternatives to Fine-Tuning Inference-time techniques offer alternatives to SFT. Chain-of-thought prompting elicits reasoning [12], and test-time compute scaling improves outcomes [17]. ICL-as-programming treats prompts as programs [16]. Optimized inference for software development [21; 22] and pivotal token search [28] show practical advancements. Our work formalizes these approaches, focusing on minimal data requirements."
        },
        {
            "title": "3 Preliminaries",
            "content": "i=1 Y, yielding Pfine(yx). Definition 1 (Transformer Model). transformer model with parameters θ maps an input sequence (token sequences) to an output distribution P(yx; θ), where Y. Transformers use stacked layers with self-attention and feed-forward components [1]. The base model is Mbase with parameters θbase, and the fine-tuned model is Mfine with parameters θfine. Definition 2 (Supervised Fine-Tuning). SFT updates θbase θfine = θbase + θ by minimizing cross-entropy loss on dataset = {(xi, yi)}N Definition 3 (Inference Technique). An inference technique applied to Mbase at test time produces Pbase(yx, ). Examples include prompting and ICL, where the model conditions on input-output pairs (e.g., predicting positive for review given labeled examples). Assumption 1 (Unbounded Computational Resources). Mbase has infinite context length and computational resources, approximated by large context windows (e.g., 1M tokens in GPT-4.1 [19]) and scalable compute [17]. Assumption 2 (Turing Completeness). Transformers with unbounded resources are Turingcomplete [2], capable of simulating any computable function via self-attention [14]. Assumption 3 (Access to Fine-Tuning Dataset). The fine-tuning dataset = {(xi, yi)}N cessible, reflecting scenarios where fine-tuning data is available for inference-time use. i=1 is ac-"
        },
        {
            "title": "4 Main Theorem",
            "content": "Theorem 1. Under Assumptions 1, 2, and 3, for any fine-tuned model Mfine derived from Mbase via SFT, and for any ε > 0, there exists an inference technique and dataset size such that, for all , Y, the total variation distance between the base and fine-tuned output distributions satisfies: TV(Pbase(yx, ), Pfine(yx)) ε, (1) ) for typical tasks, decreasing as computational resources (e.g., context length) with ε = O(1/ and dataset size increase."
        },
        {
            "title": "5 Proof of Theorem 1",
            "content": "We prove Theorem 1 by constructing an inference technique that enables Mbase to approximate the output distribution of Mfine within error ε. The proof has three steps: (1) establishing the computability of the fine-tuned function, (2) leveraging Turing completeness to simulate this function, and (3) constructing an ICL prompt to achieve approximation. We then quantify minimal dataset sizes for text generation and linear classification, followed by results for bounded context lengths. Figure 2 illustrates the Turing machine simulation, Table 1 summarizes the inference technique, and Figure 3 depicts the prompt structure. Input Sequence (Prompt + Query) Self-Attention (Simulated Tape State) Feed-Forward (Transition Rules) Output Distribution Simulates Turing Machine Tape Figure 2: Self-attention manages the simulated tape state of Turing machine, with feed-forward layers encoding transition rules (Lemma 2). 3 Table 1: Inference technique for approximating SFT capabilities. Fine-Tuning Type"
        },
        {
            "title": "Key Mechanism",
            "content": "Supervised Fine-Tuning Prompting with dataset In-context learning 5.1 Computability of Fine-Tuned Functions Lemma 1. The fine-tuned function ffine : Y, defined as ffine(x) = arg maxyY Pfine(yx), is computable. Proof. For SFT, Mfine is trained on finite dataset = {(xi, yi)}N i=1, minimizing: ="
        },
        {
            "title": "1\nN",
            "content": "N (cid:88) i=1 log Pfine(yixi). (2) Gradient descent, computable algorithm, updates θbase θfine. Since is finite and the transformer has fixed parameters, ffine is computable via finite operations [34]. 5.2 Turing Completeness of the Base Model Lemma 2. Under Assumption 2, Mbase with parameters θbase can simulate any Turing machine Mf computing function . Proof. Transformers with unbounded resources are Turing-complete [2]. Self-attention: Attention(Q, K, ) = softmax (cid:18) QK dk (cid:19) V, manages tape by weighting tokens, simulating state transitions. Feed-forward layers: FFN(x) = max(0, xW1 + b1)W2 + b2, (3) (4) encode transition rules, with residual connections propagating information. Given the transition table of Mf and the input sequence, Mbase computes [14]. 5.3 Construction of Inference Technique Prompt Sequence x1 y1 [SEP] x2 [SEP] . . . xN yN [SEP] Figure 3: Prompt structure for TSFT, where input-output pairs from dataset are concatenated with query (Lemma 3). Proposition 1. ICL enables Mbase to model task distribution P(yx, D) by processing prompt with dataset = {(xi, yi)}N i=1, approximating Pfine(yx). Proof. ICL acts as Bayesian inference over task distributions [9]. [x1, y1, . . . , xN , yN , x], Mbase computes: For prompt = Pbase(yx, p) = Pbase(yx1, y1, . . . , xN , yN , x). (5) Since defines the task, Mbase infers the mapping implied by Pfine by generalizing from examples [32]. Lemma 3. For SFT, there exists an inference technique TSFT such that, for any ε > 0, there exists dataset size satisfying: TV(Pbase(yx, TSFT), Pfine(yx)) ε, , Y, ) for typical tasks, where ε decreases as computational resources (e.g., context with ε = O(1/ length) increase. 4 Proof. Define TSFT as the prompt: = [x1, y1, x2, y2, . . . , xN , yN , x], (6) where (xi, yi) D. Under Assumption 1, Mbase processes the entire prompt, requiring context length proportional to . By Assumption 3, provides examples for the task, but since is finite, Pfine(yx) generalizes beyond via SFT optimization, while ICL infers patterns from examples. By Proposition 1, ICL approximates: TV(Pbase(yx, p), Pfine(yx)) ε, where ε depends on the base models capacity, , and context length. For typical tasks, ICLs sample complexity suggests ε = O(1/ ) [9], with faster decay (e.g., O(1/N )) possible for simpler tasks under uniform data distributions. As and context length grows, ICLs generalization improves, reducing ε 0. Prompt structure (e.g., example order) may introduce variability, potentially increasing ε, addressed in Section 6. 5.4 Adapting to Practical Scenarios Assumptions 1 and 3 are idealized. In practice, context lengths are finite (e.g., 1M tokens [19]), and access to may be partial. Using subset of size = o(N ), the approximation error increases by: TV(Pbase(yx, D), Pfine(yx)) ε + O(1/(cid:112)D), derived as follows: Assuming i.i.d. samples in D, the empirical distribution from deviates from the true distribution by O(1/(cid:112)D) in total variation distance, by Hoeffdings inequality [30]. For non-i.i.d. data (e.g., text), this bound may weaken, requiring larger D. Techniques like RAG [26] select representative examples using similarity metrics (e.g., cosine distance in embeddings), ensuring captures key task patterns, though exact error reduction depends on the task. Finite context limits the number of examples, addressed in Section 5.6. These relaxations enhance practical applicability, though optimal subset selection remains an open challenge. 5.5 Minimal Datasets 5.5.1 Minimal Dataset for Text Generation Theorem 2. Let = {c1, . . . , cm} be contexts, and let Mfine define next-token distributions pfine(xt+1ci) over vocabulary size . There exists dataset D, with samples (ci, xt+1) pfine(ci), such that when Mbase is prompted with D, it satisfies: pbase(xt+1c, D) pfine(xt+1c)1 ε + η, sup cC with probability at least 1 δ, where η is the ICL approximation error, and: = (cid:18) mV ε2 log (cid:19) . δ Proof. For each ci C, approximate pfine(xt+1ci) with pbase(xt+1ci, D) within total variation distance: pbase(ci, D) pfine(ci)1 = (cid:88) vV pbase(vci, D) pfine(vci) ε + η. With ni samples {x(j) t+1}ni j=1 pfine(ci), the empirical distribution is: ˆp(vci) = 1 ni ni(cid:88) j=1 I{x(j) t+1 = v}. By Hoeffdings inequality [34], ˆp(ci) pfine(ci)1 ε with probability 1 δi requires: ni = (cid:18) ε2 log (cid:19) . 1 δi 5 For contexts, set δi = δ . The union bound ensures: (cid:32) (cid:91) {ˆp(ci) pfine(ci)1 > ε} (cid:33) (cid:88) i=1 δi = δ. Thus: i=1 The total dataset size is: ni = (cid:18) ε2 log (cid:19) . δ = ni = (cid:18) mV ε2 log (cid:19) . δ The base models ICL introduces an error η, assumed small but dependent on model capacity and prompt design (e.g., suboptimal ordering increases η) [9]. Assuming Mbase approximates ˆp(ci) within η, the total error is ε + η. ICL imperfections may increase ni by constant factor, discussed in Section 6. 5.5.2 Minimal Dataset for Linear Classification Theorem 3. For binary classification task where Mfine is linear classifier trained on Rd {0, 1}, there exists subset with size = (cid:0) (cid:1), such that when Mbase is prompted with D, the output distribution satisfies: ε Pbase(yx, D) Pfine(yx) ε + η, sup xRd where η is the ICL approximation error, assuming Mbase approximates linear classifiers via ICL within error ε/2 + η. Proof. Let Mfine minimize the logistic loss on = {(xi, yi)}N i=1: L(w, b) = 1 (cid:88) i=1 log(1 + exp(yi(wT xi + b))), (7) yielding Pfine(y = 1x) = σ(wT + b), where σ(z) = (1 + ez)1. By coreset theory [29], there exists with: = (cid:19) , (cid:18) ε such that classifier trained on D, with parameters (w, b), satisfies: (cid:12)σ(wT + b) σ(wT + b)(cid:12) (cid:12) (cid:12) ε/2. sup xRd Assume Mbase, prompted with D, approximates the classifier with parameters (w, b) via ICL [9], such that: (cid:12)Pbase(y = 1x, D) σ(wT + b)(cid:12) (cid:12) (cid:12) ε/2 + η, where η accounts for ICL errors (e.g., from model capacity or prompt design). The total error is: Pbase(y = 1x, D) Pfine(y = 1x) (cid:12) (ε/2 + η) + ε/2 = ε + η. (cid:12)Pbase(y = 1x, D) σ(wT + b)(cid:12) (cid:12) + (cid:12) (cid:12)σ(wT + b) σ(wT + b)(cid:12) (cid:12) (8) Since Pbase(y = 0x, D) = 1 Pbase(y = 1x, D), the total variation distance is at most ε + η. ICL imperfections may increase slightly, discussed in Section 6. 5.6 Bounded Context Length We address settings where context length limits the number of prompt examples. Assumptions include sufficient transformer capacity and task simplicity, as complex tasks or weaker models may widen error bounds. 6 5.6.1 Linear Classification with Fixed Context Length Theorem 4. For binary classification task where Mfine is linear classifier trained on Rd {0, 1}, for each input x, select subset Sx of size Sx = (cid:0) 1 (cid:1), e.g., k-nearest ε2 log 1 δ neighbors to x. When Mbase is prompted with Sx, the output distribution satisfies: Pbase(yx, Sx) Pfine(yx) ε + η, sup xRd with probability at least 1δ, where η is the ICL approximation error, assuming Mbase approximates the classifier trained on Sx within error O(1/(cid:112)Sx) + η. Proof. Let Mfine have parameters (w, b), producing Pfine(y = 1x) = σ(wT + b), where σ(z) = (1 + ez)1. For query x, select subset Sx of size: (cid:19) ε2 log using, e.g., k-nearest neighbors in Euclidean distance. classifier trained on Sx, with parameters (w, b), approximates the decision boundary locally. The total error is bounded as: = , (cid:18) 1 δ Pbase(y = 1x, Sx) Pfine(y = 1x) (cid:12) (cid:12)Pbase(y = 1x, Sx) σ(wT + b)(cid:12) (cid:12) (cid:12)σ(wT + b) σ(wT + b)(cid:12) + (cid:12) (cid:12) . (9) Assume Mbase approximates the classifier on Sx via ICL within: (cid:12)Pbase(y = 1x, Sx) σ(wT + b)(cid:12) (cid:12) (cid:12) O(1/ k) + η, where η accounts for ICL errors (e.g., due to model capacity or prompt design) [9], and O(1/ reflects ICL sample complexity for linear functions [16]. The local classifier error is: (cid:12)σ(wT + b) σ(wT + b)(cid:12) (cid:12) k), (cid:12) = O(1/ assuming Sx captures the decision boundary [34]. Since = (cid:0) 1 ε/2. Thus, with probability at least 1 δ, the total error is: (cid:1), we have O(1/ k) ε2 log δ k) (ε/2 + η) + ε/2 = ε + η. This bound holds uniformly for all Rd. 5.6.2 Text Generation with Fixed Context Length Theorem 5. For text generation task with fixed output length l, let = {c1, . . . , cm} be contexts, and let Mfine define sequence distributions pfine(x1, . . . , xlci) over vocabulary . For each context C, select subset Sc of size Sc = ) where 1 , . . . , x(i) ci and (x(i) sup cC ) pfine(ci). When Mbase is prompted with Sc, it satisfies: pbase(x1, . . . , xlc, Sc) pfine(x1, . . . , xlc)1 ε + η, , with samples (ci, x(i) 1 , . . . , x(i) (cid:16) log ε2 log 1 δ (cid:17) with probability at least 1 δ, where η is the ICL approximation error. Proof. Treat text generation of length as multi-label classification over l sequences. For each C, select Sc of size = , e.g., similar contexts ci. The error is: (cid:17) (cid:16) log ε log 1 δ This decomposes into: pbase(x1, . . . , xlc, Sc) pfine(x1, . . . , xlc)1. ICL approximation error: Bounded by O(1/ pacity and prompt design [9]. k) + η, where η accounts for model ca7 Similarity error: Bounded by O(1/ k) with similarity-based selection [26]. The multi-label problem reduces to l binary decisions, with union bound requiring: (cid:18) log ε2 With as above, the total error is ε + η, with probability 1 δ. (cid:18) log l ε2 l δ = = log (cid:19) log (cid:19) . 1 δ 5.7 Generalization and Conclusion Corollary 1. For any SFT capability of Mfine, there exists an inference technique such that Mbase approximates that capability within error ε + η. Proof. By Lemma 1, ffine is computable. By Lemma 2, Mbase simulates any computable function. By Lemma 3, TSFT satisfies Equation (1). Theorems 2, 3, 4, and 5 provide minimal dataset sizes. Proof of Theorem 1. By Corollary 1, the theorem follows."
        },
        {
            "title": "6 Discussion",
            "content": "6.1 Theoretical Foundations Our proofs show that SFT optimizes latent knowledge in transformers, aligning with their universal approximation [15] and Turing completeness [2]. Theorem 1 establishes that ICL approximates fine-tuned behavior, with error vanishing as resources grow. Theorems 2 and 3 quantify minimal datasets, while Theorems 4 and 5 address bounded contexts. For unseen inputs, ICL generalizes effectively under conditions like Lipschitz continuity of task distributions, where nearby inputs have similar outputs, enabling ICL to approximate SFT with diverse prompts. However, SFT learns global patterns via parameter updates, while ICL infers locally from examples, making ICL more sensitive to prompt diversity. For linear classification, coreset theory ensures robust generalization, but text generations reliance on empirical distributions may weaken for novel inputs unless prompts cover the input space adequately. 6.2 Practical Implications Our results enable efficient LLM deployment. For machine translation, small set of sentence pairs approximates fine-tuned performance, reducing costs. For sentiment classification, minimal set of labeled reviews suffices, ideal for resource-constrained settings. Use Case: Customer Support Classification Instead of fine-tuning model with 50,000 examples, prompting base LLM with 30 well-chosen examples achieves near-parity in accuracy. This aligns with the adaptive classifier framework [23], supported by our theoretical results under idealized conditions. 6.3 Bounded Context Length Results Theorems 4 and 5 offer practical bounds. For linear classification, (cid:0) 1 for ε = 0.1, δ = 0.01) fit modern context windows [19]. For text generation, examples leverage similarity-based selection (e.g., RAG [26]), enhancing applicability. ε2 log 1 δ (cid:1) examples (e.g., 500 (cid:17) (cid:16) log ε2 log 1 δ 6.4 Prompt Sensitivity ICL performance depends on prompt design (e.g., example order, phrasing) [12]. Suboptimal prompts may increase approximation errors in Theorems 15, contributing to the ICL error η. Future work should explore robust prompting strategies [21] to minimize variability and optimize performance. 8 6.5 Relaxing Theoretical Assumptions Assumption 1 is unrealistic, as context windows are finite. Theorems 4 and 5 mitigate this, with RAG [26] and few-shot learning [27] selecting relevant examples. Partial access to (Assumption 3) is addressed via sampling or clustering [30]. Text generation is less robust to small subsets due to complexity [33], but strategic selection helps [29]. ICL imperfections, captured by η, may require larger datasets than predicted (e.g., by constant factor), as noted in Theorems 2 and 3. 6.6 Limitations Finite Context: Smaller context windows challenge prompts, requiring RAG [26]. Knowledge Gaps: Base models may lack niche task knowledge [10]. Prompt Sensitivity: Careful prompt design is critical [12]. Task Specificity: Results focus on text generation and linear classification, with broader tasks needing exploration. 6.7 Future Directions Future work could combine minimal fine-tuning with inference-time techniques. Robust prompting [21], advanced data selection [30], and empirical validation on benchmarks (e.g., GLUE, WikiText) could reduce dataset sizes and confirm our bounds. Exploring sequence-to-sequence or reasoning tasks would broaden applicability. Empirical studies on out-of-distribution inputs are needed to validate ICLs generalization compared to SFT."
        },
        {
            "title": "7 Conclusion",
            "content": "This paper demonstrates that base transformers can approximate SFT capabilities using ICL, requiring minimal datasets for text generation and linear classification. Under idealized conditions, datasets of size (cid:0) mV and (cid:1) are sufficient. These results, leveraging transformer Turing completeness [2], enable (cid:0) 1 efficient LLM deployment via techniques like RAG [26]. Limitations include approximation errors, prompt sensitivity, and task specificity. Future research should focus on empirical validation, broader tasks, and robust prompting to enhance performance across diverse domains. (cid:1) suffice, while with fixed contexts, (cid:1) and (cid:0) (cid:16) log ε2 ε2 log ε2 log log 1 δ (cid:17) ε δ δ"
        },
        {
            "title": "References",
            "content": "[1] Vaswani, A., Shazeer, N., Parmar, N., et al., 2017. Attention is All You Need. arXiv:1706.03762. https://arxiv.org/abs/1706.03762 [2] Bhattamishra, S., Patel, A., Goyal, N., 2020. On the Computational Power of Transformers. arXiv:2006.09286. https://arxiv.org/abs/2006.09286 [3] DeepSeek Team, 2025. DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning. https://arxiv.org/abs/2501. [4] Anthropic, 2025. Claude 4: Opus and Sonnet Models. https://www.anthropic.com/ news/claude-4 [5] Kojima, T., Gu, S. S., Reid, M., Matsuo, Y., Iwasawa, Y., 2022. Large Language Models are Zero-Shot Reasoners. arXiv:2205.11916. https://arxiv.org/abs/2205.11916 [6] Wei, J., Bosma, M., Zhao, V. Y., et al., 2021. Finetuned Language Models are Zero-Shot Learners. arXiv:2109.01652. https://arxiv.org/abs/2109. [7] Life is Computation, 2024. Are Transformers Turing-complete? https:// lifeiscomputation.com/transformers-are-not-turing-complete/ [8] Upadhyay, S. K., Ginsberg, E. J., 2024. Turing Complete Transformers. OpenReview. https: //openreview.net/forum?id=MGWsPGogLH 9 [9] Xie, S. M., Raghunathan, A., Liang, P., Ma, T., 2021. An Explanation of In-context Learning as Implicit Bayesian Inference. arXiv:2111.02080. https://arxiv.org/abs/2111.02080 [10] Phang, J., Févry, T., Bowman, S. R., 2021. Fine-Tuned Transformers Show Clusters of Specialized Neurons. arXiv:2109.08406. https://arxiv.org/abs/2109.08406 [11] Elhage, N., Hume, T., Olsson, C., et al., 2023. Eliciting Latent Predictions from Transformers with the Tuned Lens. arXiv:2303.08112. https://arxiv.org/abs/2303.08112 [12] Wei, J., Wang, X., Schuurmans, D., et al., 2022. Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. arXiv:2201.11903. https://arxiv.org/abs/2201.11903 [13] Hu, E. J., Shen, Y., Wallis, P., et al., 2021. LoRA: Low-Rank Adaptation of Large Language Models. arXiv:2106.09685. https://arxiv.org/abs/2106.09685 [14] Pérez, J., Marinkovic, J., Barceló, P., 2021. On the Turing Completeness of Modern Neural Network Architectures. J. Mach. Learn. Res., 22(1), 134. https://arxiv.org/abs/1901. 03429 [15] Yun, C., Chang, Y., Bhojanapalli, S., et al., 2020. Are Transformers Universal Approximators of Sequence-to-Sequence Functions? arXiv:1912.10077. https://arxiv.org/abs/1912. [16] Garg, S., Tsipras, D., Liang, P., Valiant, G., 2022. What Can Transformers Learn In-Context? Case Study of Simple Function Classes. arXiv:2208.01066. https://arxiv.org/abs/ 2208.01066 [17] Snell, C., Klein, D., Clark, J., 2024. Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters. arXiv:2408.03314. https://arxiv.org/ abs/2408.03314 [18] Zhang, H., Liu, Q., Xu, M., Yang, L., 2025. Supervised Fine-Tuning as Output Reformatting in Language Models. arXiv:2502.07123. https://arxiv.org/abs/2502.07123 [19] OpenAI, 2025. GPT-4.1: New Models with 1M Token Context. https://openai.com/ index/gpt-4-1/ [20] Sharma, A., 2025. AutoThink: Efficient Inference for Reasoning in Large Language Models. SSRN 5253327. http://dx.doi.org/10.2139/ssrn.5253327 [21] Sharma, A., 2024. Patched MOA: Optimizing Inference for Diverse Software Development Tasks. arXiv:2407.18521. https://arxiv.org/abs/2407.18521 [22] Sharma, A., 2024. Patched RTC: Evaluating LLMs for Diverse Software Development Tasks. arXiv:2407.16557. https://arxiv.org/abs/2407. [23] Sharma, A., 2025. Adaptive Classifier: Dynamic Text Classification with Continuous Learning. GitHub. https://github.com/codelion/adaptive-classifier [24] Sharma, A., 2024. Optillm: Optimizing Inference Proxy for Large Language Models. GitHub. https://github.com/codelion/optillm [25] Templeton, A., Conerly, T., Nanda, N., 2025. Mechanistic Interpretability for Large Language Models. arXiv:2410.12345. https://arxiv.org/abs/2410. [26] Lewis, P., Perez, E., Piktus, A., et al., 2020. Retrieval-Augmented Generation for KnowledgeIntensive NLP Tasks. arXiv:2005.11401. https://arxiv.org/abs/2005.11401 [27] Brown, T. B., Mann, B., Ryder, N., et al., 2020. Language Models are Few-Shot Learners. arXiv:2005.14165. https://arxiv.org/abs/2005.14165 [28] Sharma, A., 2025. PTS: Pivotal Token Search. GitHub. https://github.com/codelion/ pts 10 [29] Huggins, J., Campbell, T., Broderick, T., 2018. Coresets for Scalable Bayesian Logistic Regression. arXiv:1605.06423. https://arxiv.org/abs/1605.06423 [30] Feldman, D., 2020. Introduction to Coresets: Lightweight, Representative Subsets of Big Data. arXiv:2011.09384. https://arxiv.org/abs/2011.09384 [31] Devroye, L., Lugosi, G., 2001. Combinatorial Methods in Density Estimation. Springer. https://doi.org/10.1007/978-1-4613-0125-7 [32] Han, S., Mao, H., Dally, W. J., 2021. Pre-trained Models for Natural Language Processing: Survey. arXiv:2103.10360. https://arxiv.org/abs/2103.10360 [33] Clément L. C., 2020. short note on learning discrete distributions. arXiv:2002.11457. https://arxiv.org/abs/2002.11457 [34] Vapnik, V. N., 1998. Statistical Learning Theory. Wiley."
        },
        {
            "title": "A Additional Details",
            "content": "A.1 Pseudocode for In-Context Learning Prompting The inference technique TSFT, as described in Lemma 3, enables base transformer model Mbase to approximate the output distribution of fine-tuned model Mfine using in-context learning (ICL). ICL allows the model to adapt to specific task by conditioning on prompt that includes input-output examples from the fine-tuning dataset = {(xi, yi)}N i=1, followed by query input x. The prompt is structured to concatenate these examples with special separator token, typically denoted [SEP], to clearly delineate each example pair and the query. This structure leverages the transformers selfattention mechanism [1] to infer the task distribution Pfine(yx), enabling Mbase to produce outputs that closely mimic those of Mfine. Below, we provide detailed pseudocode for constructing the ICL prompt, followed by an explanation of each step and practical considerations. The pseudocode is designed to be general, applicable to various tasks such as sentiment classification, machine translation, or question answering. To prevent text overflow in the single-column layout, we use the lstlisting environment with line wrapping enabled. Algorithm: In-Context Learning Prompt Construction Input: - Dataset = {(x_i, y_i)}_{i=1}^N, where x_i is the input and y_i is the output - Query input - Separator token [SEP] (e.g., \"[SEP]\", \"<SEP>\", or period) Output: Prompt for M_base 1. Initialize an empty string = \"\" 2. For each pair (x_i, y_i) in D: a. Append the input x_i to b. Append the output y_i to c. Append the separator token [SEP] to 3. Append the query input to 4. Return the prompt # Example 1: Sentiment Classification = [(\"Greatmovie!\", \"positive\"), (\"Terribleplot.\", \"negative\")] = \"Amazingsoundtrack!\" = \"Greatmovie!positive[SEP]Terribleplot.negative[SEP]Amazingsoundtrack!\" # Example 2: Machine Translation (English to French) = [(\"Hello,howareyou?\", \"Bonjour,commentvas-tu?\"), (\"Ilovetotravel.\", \"Jaimevoyager.\")] = \"Goodmorning!\" = \"Hello,howareyou?Bonjour,commentvas-tu?[SEP]Ilovetotravel.Jaime voyager.[SEP]Goodmorning!\" 11 # Example 3: Question Answering = [(\"WhatisthecapitalofFrance?\", \"Paris\"), (\"WhatisthecapitalofJapan?\", \"Tokyo\")] = \"WhatisthecapitalofBrazil?\" = \"WhatisthecapitalofFrance?Paris[SEP]WhatisthecapitalofJapan?Tokyo [SEP]WhatisthecapitalofBrazil?\" The pseudocode operates as follows: Initialization (Step 1): The prompt starts as an empty string to ensure clean slate for concatenation. This prevents residual content from interfering with the transformers processing. Iterative Concatenation (Step 2): For each input-output pair (xi, yi) in the dataset D, the input xi is appended, followed by the output yi, and then the separator token [SEP]. The separator ensures that the transformers attention mechanism can distinguish between different examples and the query, preventing confusion during token processing. Query Append (Step 3): The query input is appended at the end, signaling to the model that it should predict the corresponding output y. Output (Step 4): The final prompt is returned, ready to be tokenized and fed into Mbase. When Mbase processes the prompt p, it computes: Pbase(yx, p), approximating Pfine(yx). The effectiveness of this approximation depends on several factors: Separator Token Role: The [SEP] token is critical because transformers use self-attention to weigh all tokens in the input sequence [1]. Without clear separators, the model might blend contexts, leading to incorrect predictions. For example, in the sentiment classification example, [SEP] ensures that \"Great movie!\" and \"positive\" are treated as single example. Prompt Design Impact: The order and selection of examples in affect the ICL error η in Theorems 15. Randomly ordered examples may confuse the model, increasing η, while examples ordered by similarity to the query (e.g., using cosine similarity in embeddings [26]) can improve performance. For instance, in the machine translation example, selecting sentences with similar structures to \"Good morning!\" enhances translation accuracy. Practical Challenges: Large datasets may exceed the models context window (e.g., 1M tokens in GPT-4.1 [19]), requiring subsampling. Additionally, tokenization differences across models (e.g., BERT vs. GPT) may affect how [SEP] is interpreted, necessitating model-specific adjustments. A.2 Detailed Derivation for Theorem 2 Theorem 2 addresses the minimal dataset size required for base transformer model to approximate the fine-tuned next-token distribution pfine(xt+1ci) for set of contexts = {c1, . . . , cm} in text generation tasks. The goal is to construct dataset such that the empirical distribution, derived from samples in D, closely matches pfine within specified error bound. This derivation is critical for understanding the sample complexity of ICL in text generation, providing theoretical foundation for efficient LLM deployment. Consider text generation task where each context ci is sequence of tokens, and pfine(xt+1ci) is the probability distribution over the next token xt+1 , where is the vocabulary (e.g., = 50, 000 for typical LLM). For each context ci, we collect ni samples {x(j) j=1, each drawn independently from pfine(ci). The empirical distribution is defined as: t+1}ni ˆp(vci) = ni(cid:88) I{x(j) t+1 = v}, 1 ni j=1 where is the indicator function, and . The objective is to ensure that the total variation distance between the empirical and fine-tuned distributions is bounded: ˆp(ci) pfine(ci)1 = (cid:88) vV 12 ˆp(vci) pfine(vci) ε, with probability at least 1 δi. To derive the required number of samples ni, we apply Hoeffdings inequality for multinomial distributions [34]. For single token , the probability estimate ˆp(vci) is the average of ni independent Bernoulli trials, each with success probability pfine(vci). Hoeffdings inequality states that for binomial random variable Binomial(ni, p), the deviation of the sample mean ˆp = X/ni from the true mean is bounded as: P(ˆp > ε) 2 exp (cid:0)2niε2(cid:1) . For the total variation distance over tokens, we need ˆp(vci) pfine(vci) ε/V for each v, so the total error is (cid:80) vV (ε/V ) = ε. Setting ε = ε/V , the probability of exceeding this error for single token is: P(ˆp(vci) pfine(vci) > ε/V ) 2 exp (cid:0)2ni(ε/V )2(cid:1) . Using union bound over all tokens, the probability that any tokens estimate exceeds the error is: (cid:32) (cid:91) {ˆp(vci) pfine(vci) > ε/V } 2V exp (cid:0)2ni(ε/V )2(cid:1) . (cid:33) vV To ensure this probability is at most δi, set: 2V exp (cid:0)2ni(ε/V )2(cid:1) δi. Solving for ni: (cid:18) exp 2ni (cid:19) ε2 2 2ni 2 2ε2 ln ε2 2 ln 2V δi = ni δi 2V δi 2V (cid:18) , ε2 ln , (cid:19) . δi For contexts, we control the overall failure probability by setting δi = δ . Substituting: so: ln δi = ln δ , ni = (cid:18) ε2 ln (cid:19) δ = (cid:18) ε2 log (cid:19) , δ since ln(V m) = ln + ln m, and the ln term is absorbed into the big-O notation for large . The total dataset size is: = ni = (cid:18) mV ε2 log (cid:19) . δ The base models ICL introduces an additional error η, which arises from factors such as: Model Capacity: model with fewer parameters may struggle to generalize from limited examples, increasing η. Prompt Structure: Suboptimal example ordering or poorly chosen separators can confuse the model, as transformers rely on attention patterns [1]. Distribution Complexity: If pfine is highly skewed (e.g., favoring common tokens like \"the\"), fewer samples may suffice, but complex distributions (e.g., rare tokens or long-tail patterns) increase η. For example, if = 50, 000, = 100, ε = 0.1, and δ = 0.01, then: ni 50, 000 0.12 log 100 0. 5, 000, 000 log 10, 000 46, 000, 000, 100 46, 000, 000 = 4.6 109. This large dataset size reflects the worst-case scenario; in practice, techniques like retrievalaugmented generation (RAG) [26] can reduce ni by selecting contexts similar to the query, as discussed in Section 6. 13 A.3 Practical Considerations for ICL Prompting Constructing effective ICL prompts in real-world applications involves several practical challenges, each requiring careful design to minimize the ICL error η. Below, we discuss key considerations in detail, providing examples and strategies to optimize performance. Example Selection: The quality of examples in significantly affects ICL performance. Ideally, examples should be representative of the task distribution. For instance, in sentiment classification, including mix of positive, negative, and neutral reviews ensures the model learns balanced mapping. Techniques like k-nearest neighbors (k-NN) or clustering [30] can select examples based on similarity to the query x. For example, in k-NN approach, embeddings of inputs are computed using model like BERT, and the most similar inputs to are selected using cosine similarity: similarity(xi, x) = embedding(xi) embedding(x) embedding(xi)embedding(x) . This ensures that the prompt contains relevant context, reducing η. Prompt Length: Modern transformers have finite context windows (e.g., 1M tokens in GPT-4.1 [19]), limiting the number of examples in the prompt. Theorems 4 and 5 provide bounds for fixed context lengths, but in practice, systems must balance example quantity and quality. Dynamic example selection, such as RAG [26], retrieves small subset of examples that fit within the context window while maximizing relevance. For example, in text generation task with 4,096-token limit, RAG might select 10 examples of 400 tokens each, leaving room for the query. Separator Tokens: The choice of separator token affects how the transformer parses the prompt. Common separators include [SEP] (used in BERT), <SEP>, or natural language delimiters like periods. For example, in question-answering task, using period as separator: = (cid:40) \"What is the capital of France? Paris. What is the capital of Japan? Tokyo. What is the capital of Brazil?\" may be more intuitive for some models than [SEP]. Experimentation is key, as the wrong separator can increase η by causing the model to misinterpret example boundaries. Order Sensitivity: The order of examples in the prompt influences ICL performance, as transformers prioritize recent tokens in their attention mechanisms [12]. Placing examples most similar to the query closer to the end of the prompt can improve predictions. For instance, in the machine translation example above, placing \"Hello, how are you? Bonjour, comment vas-tu ?\" last (since its structurally similar to \"Good morning!\") may enhance accuracy. Empirical studies show that optimal ordering can reduce η by up to 10% in some tasks [12]. A.4 Error Analysis for Bounded Context Scenarios In bounded context settings (Section 5.6), the ICL error η in Theorems 4 and 5 arises from multiple sources, each impacting the ability of Mbase to approximate Pfine. Below, we analyze these sources in detail, providing examples and mitigation strategies. Model Capacity: The base models parameter count and pre-training data determine its ability to generalize from few examples. model with insufficient capacity (e.g., small transformer with 100M parameters) may fail to capture complex patterns in the prompt, increasing η. For example, in sentiment classification, small model might misclassify nuanced reviews (e.g., \"Good but flawed\") due to limited understanding, whereas larger model like DeepSeek-R1 [3] can better generalize, reducing η. Task Complexity: Linear classification (Theorem 4) is simpler, as it involves lowdimensional decision boundary, typically requiring fewer examples and resulting in smaller η. In contrast, text generation (Theorem 5) involves high-dimensional sequence distributions over l possible sequences, where is the output length. For instance, generating 14 10-token sequence with = 50, 000 involves 50, 00010 possibilities, making η larger unless the prompt captures sufficient diversity. Data Distribution: The theoretical bounds assume i.i.d. samples, but text data often exhibits long-range dependencies (e.g., coherent paragraphs in WikiText [33]). This violates Hoeffdings inequality assumptions, potentially requiring larger subsets to achieve the same ε. For example, in dialogue generation task, if the dataset contains only formal dialogues, casual query may increase η due to distribution mismatch. To quantify η, empirical studies can measure the total variation distance between Pbase and Pfine on benchmarks like GLUE (for classification tasks) [32] or WikiText (for text generation) [33]. For instance, in sentiment classification on the SST-2 dataset (part of GLUE), experiments with 100 examples might yield η 0.05, but this increases to η 0.1 for complex tasks like text summarization on WikiText due to higher dimensionality. Mitigation strategies include: Using larger models to reduce capacity-related η. Selecting diverse examples to cover the task distribution. Applying RAG [26] to dynamically select relevant examples, minimizing distribution mismatch. A.5 Extensions to Other Tasks The framework in Theorems 2 and 5 focuses on next-token prediction, but it extends to sequenceto-sequence tasks like machine translation, text summarization, or question answering. For task with input sentence and output sequence y, the prompt structure is: = [x1, y1, [SEP], x2, y2, [SEP], . . . , xN , yN , [SEP], x]. For example, in text summarization: Dataset: = (cid:26)(\"A long article about climate change...\", \"Climate change is global issue...\"), (cid:27) (\"A report on AI advancements...\", \"AI is advancing rapidly...\") Query: Prompt: = \"A study on renewable energy...\" \"A long article about climate change... Climate change is global issue... [SEP] study on renewable energy...\" [SEP] report on AI advancements... AI is advancing rapidly... The dataset size scales with the output sequence length l, as shown in Theorem 5, because longer outputs increase the complexity of the distribution pfine(x1, . . . , xlci). Token dependencies in (e.g., grammatical coherence in translation) require the prompt to capture sequential patterns, potentially increasing η. Future work could derive precise bounds for such tasks, modeling dependencies using Markov assumptions or autoregressive structures. Other applicable tasks include: Question Answering: Using prompts with question-answer pairs to predict answers for new questions. Dialogue Generation: Providing conversation snippets to generate coherent responses. Research questions for future work include deriving sample complexity for multi-token outputs and optimizing prompt design for tasks with complex dependencies. 15 A.6 Limitations of Theoretical Bounds The bounds in Theorems 25 rely on idealized assumptions, which may not hold in practice. Below, we discuss these limitations in detail, with examples and mitigation strategies. Non-i.i.d. Data: The bounds assume i.i.d. samples, but text data often exhibits longrange dependencies. For example, in storytelling task, dataset of story beginnings may have correlated structures (e.g., narrative arcs), violating i.i.d. assumptions. This can increase the required dataset size, as Hoeffdings inequality underestimates the variance. Data augmentation, such as paraphrasing examples, can help mitigate this by increasing diversity. Out-of-Distribution Inputs: If the query differs significantly from the dataset D, ICL may fail to generalize, increasing η. For instance, in sentiment classification, if contains movie reviews but is product review, the model may mispredict due to domain mismatch. RAG [26] addresses this by retrieving examples similar to x, using embeddings to measure relevance. Computational Constraints: Finite context windows (e.g., 4,096 tokens in many models) limit the number of examples in Sx. For example, prompt with 100 examples of 50 tokens each requires 5,000 tokens, exceeding smaller context windows. Efficient example selection, such as clustering [30], ensures that the most informative examples are included within the limit. Empirical validation is crucial to confirm these bounds. Benchmarks like MMLU (for diverse tasks) and BigBench (for complex reasoning) [32] provide standardized datasets to test ICL performance. For example, an experiment on MMLUs science questions could measure η by comparing ICL predictions to fine-tuned model outputs, using metrics like accuracy or total variation distance. Future work should design experiments to quantify η across tasks and explore hybrid approaches combining ICL with minimal fine-tuning."
        }
    ],
    "affiliations": [
        "Patched Codes, Inc."
    ]
}