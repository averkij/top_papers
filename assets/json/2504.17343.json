{
    "paper_title": "TimeChat-Online: 80% Visual Tokens are Naturally Redundant in Streaming Videos",
    "authors": [
        "Linli Yao",
        "Yicheng Li",
        "Yuancheng Wei",
        "Lei Li",
        "Shuhuai Ren",
        "Yuanxin Liu",
        "Kun Ouyang",
        "Lean Wang",
        "Shicheng Li",
        "Sida Li",
        "Lingpeng Kong",
        "Qi Liu",
        "Yuanxing Zhang",
        "Xu Sun"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The rapid growth of online video platforms, particularly live streaming services, has created an urgent need for real-time video understanding systems. These systems must process continuous video streams and respond to user queries instantaneously, presenting unique challenges for current Video Large Language Models (VideoLLMs). While existing VideoLLMs excel at processing complete videos, they face significant limitations in streaming scenarios due to their inability to handle dense, redundant frames efficiently. We introduce TimeChat-Online, a novel online VideoLLM that revolutionizes real-time video interaction. At its core lies our innovative Differential Token Drop (DTD) module, which addresses the fundamental challenge of visual redundancy in streaming videos. Drawing inspiration from human visual perception's Change Blindness phenomenon, DTD preserves meaningful temporal changes while filtering out static, redundant content between frames. Remarkably, our experiments demonstrate that DTD achieves an 82.8% reduction in video tokens while maintaining 98% performance on StreamingBench, revealing that over 80% of visual content in streaming videos is naturally redundant without requiring language guidance. To enable seamless real-time interaction, we present TimeChat-Online-139K, a comprehensive streaming video dataset featuring diverse interaction patterns including backward-tracing, current-perception, and future-responding scenarios. TimeChat-Online's unique Proactive Response capability, naturally achieved through continuous monitoring of video scene transitions via DTD, sets it apart from conventional approaches. Our extensive evaluation demonstrates TimeChat-Online's superior performance on streaming benchmarks (StreamingBench and OvOBench) and maintaining competitive results on long-form video tasks such as Video-MME and MLVU."
        },
        {
            "title": "Start",
            "content": "TimeChat-Online: 80% Visual Tokens are Naturally Redundant in Streaming Videos Linli Yao, Yicheng Li, Yuancheng Wei, Lean Wang Yuanxin Liu Kun Ouyang Lei Li Shicheng Li Shuhuai Ren Sida Li Lingpeng Kong Qi Liu Yuanxing Zhang Xu Sun Peking University, South China University of Technology, The University of Hong Kong, Kuaishou Technology https://timechat-online.github.io 5 2 0 2 4 2 ] . [ 1 3 4 3 7 1 . 4 0 5 2 : r Figure 1: This paper presents TimeChat-Online for efficient Streaming Video Understanding [6]. Its core design is the Differential Token Dropping (DTD) module that selectively preserves only significant temporal changes across video streams. The DTD eliminates 82.8% of redundant video tokens without any user-query guidance, while achieving 1.76 speedup in response latency and maintaining over 98% of original accuracy. Furthermore, it naturally monitors video scene transitions, facilitating online Proactive Responding. Abstract The rapid growth of online video platforms, particularly live streaming services, has created an urgent need for real-time video understanding systems. These systems must process continuous video streams and respond to user queries instantaneously, presenting unique challenges for current Video Large Language Models (VideoLLMs). While existing VideoLLMs excel at processing complete videos, they face significant limitations in streaming scenarios due to their inability to handle dense, redundant frames efficiently. We introduce TimeChat-Online, novel online VideoLLM that revolutionizes real-time video interaction. At its core lies our innovative Differential Token Drop (DTD) module, which addresses the fundamental challenge of visual redundancy in streaming videos. Drawing inspiration from human visual perceptions Change Blindness phenomenon, DTD preserves meaningful temporal changes while filtering out static, redundant content between frames. Remarkably, our experiments demonstrate that DTD achieves an 82.8% reduction *Equal contribution. # linliyao@stu.pku.edu.cn. 1 in video tokens while maintaining 98% performance on StreamingBench, revealing that over 80% of visual content in streaming videos is naturally redundant without requiring language guidance. To enable seamless real-time interaction, we present TimeChat-Online-139K, comprehensive streaming video dataset featuring diverse interaction patterns including backward-tracing, current-perception, and futureresponding scenarios. TimeChat-Onlines unique Proactive Response capability, naturally achieved through continuous monitoring of video scene transitions via DTD, sets it apart from conventional approaches. Our extensive evaluation demonstrates TimeChat-Onlines superior performance on streaming benchmarks (StreamingBench and OvOBench) and maintaining competitive results on long-form video tasks such as Video-MME and MLVU. Notably, when integrated with Qwen2.5VL-7B, DTD achieves 5.7-point accuracy improvement on the challenging VideoMME subset containing videos of 30-60 minutes, while reducing video tokens by 84.6%. Our work establishes new paradigm for efficient streaming video understanding and reveals the potential of leveraging natural video redundancy for future VideoLLMs development. Conference17, July 2017, Washington, DC, USA L. Yao, Y. Li, Y. Wei, L. Li, et al. CCS Concepts Computing methodologies Natural language generation. Keywords Video Large Language Models, Streaming Video Understanding, Video Token Pruning"
        },
        {
            "title": "1\nThe proliferation of online video platforms (e.g., live broadcasts)\nand real-world applications (e.g., domestic robots and surveillance\nsystems) has driven researchers to focus on online video understand-\ning with continuous streams, commonly referred to as Streaming\nVideoQA [6, 12, 44, 62, 75, 78]. In streaming video scenarios, video\nassistants must continuously process incoming frames while simulta-\nneously enabling real-time user interaction, remaining responsive to\nuser queries posed at any moment.",
            "content": "Streaming video tasks introduce two fundamental challenges: Firstly, Long-form High-redundant Video Context: successive video frames are received at high frame rates (typically 1-10 FPS in real-world applications [6]), with neighboring frames exhibiting substantial similarity in backgrounds and static objects. Additionally, video streams are potentially infinite, creating extensive temporal context that must be maintained across the timeline. Secondly, Realtime Interaction with Proactive Responding: streaming video tasks involve backward tracing, current-time perception, and forward responding. When presented with user query at specific moment, VideoLLM must efficiently access past and current visual context to generate immediate responses with minimal latency. For questions requiring future visual cues not yet available, the model must possess Proactive Responding capabilities to automatically trigger responses at prospective timestamp when relevant visual cues become available. Despite recent advancements in Video Large Language Models (VideoLLMs) [3, 5, 26, 27, 35, 46, 73, 78, 80], these models struggle with online video understanding. They are primarily designed for offline video processing, where they receive and process entire videos at once. In online scenarios, they cannot implement proactive responses and face difficulties with long-form high-redundancy video streams: short-context VideoLLMs that uniformly sample sparse video frames [34, 35, 80], such as 32 or 64 frames, suffer from significant visual context loss; conversely, long-context VideoLLMs that densely sample video frames at 1 FPS (frame-per-second), such as Qwen2.5VL-7B [5], incur substantial response delays when processing computationally intensive video tokens. Recent approaches have proposed Average Pooling [5, 24] or Resampler-based mechanisms [4, 11, 31, 46, 74] to compress redundancy in long-form videos. However, these compression methods impose fixed number of tokens per frame, failing to adapt to the variable redundancy inherent in dynamic video streams. Concurrently, language-guided approaches [11, 28, 48] prove inefficient for streaming scenarios, as they necessitate reprocessing of all historical dense frames whenever new user query is received. To address these challenges, this paper presents TimeChat-Online, novel online VideoLLM that efficiently enables real-time interaction with streaming video content. To tackle the long-form and high-redundancy challenges of video streams, we propose Differential Token Drop (DTD) mechanism. Inspired by human visual perception phenomena such as Change Blindness [47, 49], our approach mimics how the human visual system processes continuous video streams: not by capturing every detail in each frame, but by selectively focusing on salient spatial-temporal changes while filtering out static, redundant content. As Figure 1 illustrates, our DTD mechanism adaptively preserves only the changed visual tokens between successive frames from joint spatial-temporal perspective. This approach significantly reduces video token count by 82.8% in streaming videos purely at the visual level without requiring any textual information. Notably, it maintains VideoQA accuracy at comparable levels to full-token processing, indicating that more than 80% of the visual context in video streams is naturally redundant. While DTD effectively addresses the challenge of processing long-form high-redundant video streams, we must also tackle the real-time interaction requirements of online video understanding. To this end, we introduce new training dataset TimeChat-Online-139K that addresses the scarcity of data specifically designed for streaming VideoQA. We collect long-form videos averaging 11.1 minutes in length and utilize GPT-4o [41] to annotate them with diverse streaming VideoQA pairs that encompass backward tracing, currenttime understanding, and forward responding task types. To enable proactive responding for future-oriented questions, we construct negative samples in which the questions cannot be answered using the currently available streaming video content. For future-responding question interactions, TimeChat-Online is designed to be triggered at video scene transition timestamps, generating new responses with the updated video context. As Figure 1 illustrates, scene transitions are naturally indicated by frames with few tokens dropped, signifying significant visual differences from previous frames. To summarize, our contributions are three-fold: I) We propose TimeChat-Online with novel Differential Token Drop module that significantly reduces spatial-temporal redundancy in streaming videos (eliminating 82.8% of tokens, achieving 1.76 speed-up while maintaining 98% accuracy on StreamingBench). The DTD can also be seamlessly integrated with general VideoLLMs such as Qwen2.5VL to substantially enhance its efficiency in long video tasks (reducing 87.2% of video tokens and yielding 5.6 absolute accuracy gain on the VideoMME long set). II) We develop new instruction-tuning dataset, TimeChat-Online-139K, specifically designed to facilitate more flexible streaming VideoQA interactions. III) Experimental results demonstrate that TimeChat-Online achieves state-of-the-art performance on streaming video benchmarks, including StreamingBench (58.0) and OVO-Bench (47.6). Furthermore, it maintains competitive performance on general long-form video understanding tasks such as VideoMME (63.3), MLVU (65.4), and LongVideoBench (57.7)."
        },
        {
            "title": "2 Related Work\nStreaming Video Understanding. Streaming video understanding,\nwhich processes continuously updating video frames, was first intro-\nduced by VideoLLM-online [6]. Recent advancements follow two\nprimary directions. The first focuses on efficient encoding of dense\nvideo streams. Memory-bank-based approaches like VideoStream-\ning [44], Flash-VStream [75], StreamChat [61], and VideoChat-\nonline [21] utilize dynamic memory banks to retain informative",
            "content": "2 TimeChat-Online: 80% Visual Tokens are Naturally Redundant in Streaming Videos video tokens. ReKV [12] and Inf-MLLM [39] optimize KV Cache management for video context, while VideoLLM-MoD [60] employs Mixture-of-Depth to reduce token count. Most of these methods require language guidance from user queries to select relevant video content. In contrast, our proposed DTD reduces video tokens by over 80% purely-visually as an early step to lighten the computational burden of video tokens before language model processing. The second direction enhances real-time interaction experiences in streaming scenarios. IXC2.5-Omni [77] and Qwen2.5-Omni [62] incorporate audio modality, MMDuet [57] refines video-text duet interaction formats, and Dispider [43] introduces disentangled Perception-Decision-Reaction paradigm. Our work differs by implementing novel proactive response paradigm that leverages our synthetic streaming training dataset TimeChat-Online-139K and is intelligently triggered by scene transitions naturally revealed by DTDs token drop ratio curves. Efficient Video Token Pruning. Recent advancements in long video processing have led to diverse approaches for video token pruning [28, 38, 45, 6669]. branch of methods [28, 46, 54, 63, 66] compress frames or clips to fixed number, disregarding the dynamic visual redundancy inherent in different videos. For instance, LLamavid [32] represents each frame with fixed two tokens. To address the limitations, several methods [22, 45, 53, 68] design adaptive token merging in either spatial or temporal dimensions. While they offer improved flexibility, they blur the vanilla spatial-temporal positions which will hurt the fine-grained video perception such as spatial localization, action ordering, or temporal counting task. Furthermore, selection-based methods without merging [58] primarily consider spatial redundancy while neglecting temporal redundancy. In contrast, our DTD adaptively drops video tokens via dynamic temporal redundancy while preserving the related spatial-temporal positions. Another category of approaches [7, 48, 81] leverages language guidance through user queries or vision-language cross-attention mechanisms for token pruning. However, these language-guided methods are inefficient for streaming scenarios, as they require reprocessing all historical frames for each new user query. This significantly increases computational burden and introduces response delays, making them impractical for real-world applications. In contrast, our DTD efficiently processes video streams by calculating redundancy only for newly-arriving frames with faster speed."
        },
        {
            "title": "3 TimeChat-Online Framework\nThis paper addresses the streaming video question-answering (Stream-\ning VideoQA) task [33, 61] and introduces an online video assis-\ntant, TimeChat-Online, capable of providing real-time responses to\nuser queries posed at specific timestamps within video streams. In\nSection 3.1, we first formulate the Streaming VideoQA task and\ndistinguish it from offline video understanding tasks. Then, in Sec-\ntion 3.2, we present the design of the Differential Token Drop, which\neliminates over 80% of video tokens by retaining significant temporal\nchanges while removing static redundancy. Next, we introduce the\ncuration process of TimeChat-Online-139K for training streaming\nvideo tasks in Section 3.3 and describe the training strategy and\ninference procedure in Section 3.4.",
            "content": "Conference17, July 2017, Washington, DC, USA"
        },
        {
            "title": "3.2.1 Patchify and Encoding. We use a ViT [13] to split each video\nframe 𝑓𝑡 into a sequence of visual patches P𝑡 = [𝑝1, 𝑝2, ..., 𝑝𝐻 ×𝑊 ]\nand encode the related spatial tokens as V𝑡 = [𝑣1, 𝑣2, ..., 𝑣𝐻 ×𝑊 ],\nwhere 𝐻 and 𝑊 denotes the maximum position index in height and\nwidth respectively. For video streams F1:𝑡 received at timestamp 𝑡,\nwe can get the temporal sequence of patches P1:𝑡 and the related\nvisual tokens V1:𝑡 .",
            "content": "3 Conference17, July 2017, Washington, DC, USA L. Yao, Y. Li, Y. Wei, L. Li, et al. Figure 2: The core of TimeChat-Online lies in the Differential Token Dropping (DTD) design for efficiently encoding video streams. DTD captures significant temporal changes through three steps: (a) patchifying and encoding dense video frames, (b) calculating static redundancy between temporally-consecutive and spatially-identical video tokens, (c) dropping temporally-redundant video tokens while preserving the (temporal, height, width) positions of remaining tokens. DTD dynamically eliminates visual redundancy in the temporal dimension, yielding an adaptive drop ratio for each frame. During Real-Time Interaction, frames with low drop ratios in the timeline indicate video scene transitions, triggering TimeChat-Online to achieve Proactive Responding at these scene-oriented timestamps."
        },
        {
            "title": "3.2.2 Static Redundancy Calculation. Intuitively, if temporally\nconsecutive frames (𝑓𝑡 −1, 𝑓𝑡 ) are visually similar, we determine\nthe latter frame 𝑓𝑡 as a redundant frame, because it contains the\nsame static visual content as the former frame 𝑓𝑡 −1. From a more\nfine-grained perspective, we can formulate static redundancy by\ncomparing temporally-consecutive patches P or visual tokens V at\npixel-level and feature-level, respectively.\nPixel-level Redundancy. Inspired by the RLT [10] method, we\nselect two temporally consecutive and spatially identical patches\n(𝑝𝑡 −1\nℎ𝑤), where ℎ and 𝑤 are the height and width indices in the\nℎ𝑤\nspatial dimension. Next, we calculate the pixel similarity between\nthem using the L1 distance [1] as:",
            "content": ", 𝑝𝑡 (1) (cid:13) (cid:13)1 < 𝜏𝑝𝑖𝑥𝑒𝑙 ℎ𝑤 , 𝑝𝑡 ℎ𝑤) = (cid:13) 𝑆𝑖𝑚(𝑝𝑡 1 ℎ𝑤 𝑝𝑡 ℎ𝑤 (cid:13)𝑝𝑡 1 If the selected two patches are visually similar, they will have close pixel values, leading to small L1 distance. We define hyperparameter 𝜏𝑝𝑖𝑥𝑒𝑙 to determine the threshold of pixel redundancy. Feature-level Redundancy. Besides pixel-level redundancy, we can also calculate the feature-level visual redundancy. The cosine similarity [51] between temporally-consecutive spatially-aligned visual tokens 𝑣𝑡 1 ℎ𝑤 is defined as: ℎ𝑤 and 𝑣𝑡 𝑆𝑖𝑚(𝑣𝑡 1 ℎ𝑤 , 𝑣𝑡 ℎ𝑤) = ℎ𝑤 𝑣𝑡 𝑣𝑡 1 ℎ𝑤 (cid:13) (cid:13) 𝑣𝑡 (cid:13) (cid:13) ℎ𝑤 (cid:13) (cid:13)2 (cid:13) 𝑣𝑡 1 (cid:13) ℎ𝑤 (cid:13) (cid:13) (cid:13) (cid:13)2 > 𝜏𝑓 𝑒𝑎𝑡 (2) If the selected two visual tokens are visually similar, they will have similar feature embeddings, leading to large cosine similarity. We define hyperparameter 𝜏𝑓 𝑒𝑎𝑡 to determine the threshold of feature-level redundancy. The values of 𝜏𝑝𝑖𝑥𝑒𝑙 and 𝜏𝑓 𝑒𝑎𝑡 can control the overall drop ratio of video tokens. They measure how different two temporally consecutive patches/tokens are, depending only on the inherent nature of videos. Empirically, we verify that the 𝜏 values are consistent across different datasets. For example, 𝜏𝑓 𝑒𝑎𝑡 = 0.25 corresponds to around 85% of video tokens being dropped across three datasets, while 𝜏𝑓 𝑒𝑎𝑡 = 0.5 corresponds to around 45%. Notably, unlike feature-level token pruning approaches [7] that operate inside LLMs, our pixel-level and feature-level calculations are purely visionbased strategies that operate before the Large Language Model (LLM) [14, 65] module, which achieves greater efficiency."
        },
        {
            "title": "3.2.3 Position-aware Token Dropping. Based on the calculated\nsimilarity between token (or patch) pairs, we identify redundant\ntokens (or patches) in the Current frame 𝑓𝑡 and generate a binary",
            "content": "4 TimeChat-Online: 80% Visual Tokens are Naturally Redundant in Streaming Videos Conference17, July 2017, Washington, DC, USA Table 1: Performance comparison on StreamingBench focusing on Real-Time Visual Understanding tasks. Real-Time Visual Understanding encompasses Object Perception (OP), Causal Reasoning (CR), Clips Summarization (CS), Attribute Perception (ATP), Event Understanding (EU), Text-Rich Understanding (TR), Prospective Reasoning (PR), Spatial Understanding (SU), Action Perception (ACP), and Counting (CT). VTokens(%) represents the percentage of video tokens remaining after dropping, where 100% indicates no dropping, and 82.6% signifies an 82.6% reduction in video tokens. The bold values indicate the best performance and underlined values indicate the second best. #Frames VTokens(%) OP CR CS"
        },
        {
            "title": "ATP",
            "content": "EU TR PR SU"
        },
        {
            "title": "ACP",
            "content": "CT"
        },
        {
            "title": "Human",
            "content": "Gemini 1.5 pro [18] GPT-4o [41] Claude 3.5 Sonnet [2] Video-LLaMA2-7B [9] VILA-1.5-8B [34] Video-CCAM-14B [16] LongVA-7B [79] InternVL-V2-8B [8] Kangaroo-7B [36] LLaVA-NeXT-Video-32B [35] MiniCPM-V-2.6-8B [19] LLaVA-OneVision-7B [24] Qwen2.5-VL-7B [5] Flash-VStream-7B [75] VideoLLM-online-8B [6] Dispider-7B [43] [CVPR 2025] TimeChat-Online-7B TimeChat-Online-7B TimeChat-Online-7B - 1 fps 64 32 14 96 128 16 64 64 32 32 1 fps - 2 fps 1 fps 1 fps 1 fps 1 fps - - - - - - - - - - - - - 89.47 92.00 93.60 91.47 95.65 92. 88.00 88.75 89.74 91.30 91."
        },
        {
            "title": "Proprietary MLLMs",
            "content": "79.02 77.11 73.33 80.47 80.47 80.47 83.54 83.91 84.09 79.67 76.47 82.02 80.00 70.19 75.39 84.74 83.80 79. 77.78 66.67 61.11 64.23 62.19 61.79 71.95 69.12 69.32 48.70 49.22 43.09 75.69 73.28 72.44 Open-source Offline VideoLLMs 55.86 53.68 56.40 70.03 68.12 71.12 78.20 71.93 80.38 78.32 55.47 49.22 57.81 63.28 60.94 84.38 70.31 71.09 74.22 80.47 57.41 70.98 65.30 61.20 69.40 70.66 73.82 77.92 76.03 78.86 58.17 56.86 62.75 70.92 77.12 73.20 76.80 75.82 80.72 80.45 Open-source Online VideoLLMs - - - 44.2% 82.6% 25.89 39.07 74.92 80.76 79.13 43.57 40.06 75.53 79.69 81.25 24.91 34.49 74.10 80.76 78.86 23.87 31.05 73.08 83.33 80.77 52.80 53.42 64.60 62.73 67.70 67.08 63.35 64.60 72.67 76.73 27.33 45.96 74.44 74.84 70. 43.61 53.89 51.40 59.50 62.93 61.68 69.78 65.73 71.65 78.50 13.08 32.40 59.92 78.82 77.26 39.81 54.63 42.59 61.11 59.26 56.48 57.41 70.37 67.59 79.63 18.52 31.48 76.14 78.70 77.78 42.68 48.78 47.97 53.66 53.25 55.69 56.10 56.10 65.45 63.41 25.20 34.16 62.91 64.23 67. 45.61 50.14 49.58 54.67 54.96 62.04 64.31 62.32 65.72 66.19 23.87 42.49 62.16 68.75 66.19 35.23 17.62 31.61 34.72 56.48 38.86 38.86 53.37 45.08 53.19 48.70 27.89 45.80 57.98 53.72 49.52 52.32 53.96 59.96 63.72 64.60 66.96 67.44 71.12 73.68 23.23 35.99 67.63 75.28 73. 100% 80.22 82.03 79.50 83.33 76. 78.50 78.70 64.63 69.60 57.98 75. mask 𝑀𝑡 drop that indicates whether each token (or patch) should be dropped or preserved. For feature-level dropping, we directly identify which visual tokens in V𝑡 to drop. For pixel-level dropping, we map the dropped patch indices to their corresponding visual tokens, leveraging the one-to-one correspondence maintained by the ViT encoder between input patches and output visual tokens. We then eliminate redundant static tokens from the current frame 𝑓𝑡 by applying the binary mask 𝑀𝑡 drop to the visual tokens V𝑡 and related position embeddings P𝑡 : (cid:101)V𝑡 = V𝑡 𝑀𝑡 drop , (cid:101)P𝑡 = P𝑡 𝑀𝑡 drop (3) Spatial-Temporal Position Reserving. After dropping the marked redundant tokens, the vanilla spatial and temporal position relations of the remaining tokens will be disrupted. This disruption would harm fine-grained spatial/temporal perception tasks such as Spatial Localization [25] or Action Recognition [76]. To preserve the relative spatial-temporal positions, we leverage Multi-modal Rotary Position Embedding (M-ROPE) derived from QwenVL-series models [5, 54]. M-ROPE indexes the 3D {temporal, height, width} position 𝑝 = {𝑡, ℎ, 𝑤 } of each video token. We calculate the 3D M-ROPE positions before dropping to record the original spatial-temporal structure of video tokens. When dropping visual tokens, we simultaneously drop their M-ROPE position embeddings as shown in the equation 3. As depicted in Figure 2, the remaining visual tokens (cid:101)V𝑡 are aligned with their vanilla 3D position embeddings calculated before dropping. Finally, we can parallelize the position-aware token dropping operation for all temporal-consecutive frame pairs in F1:𝑡 and get the final dropped video token sequence (cid:101)V1:𝑡 across the timeline. Discussion: Advantages of Differential Token Dropping. DTD is purely vision-based approach to reduce video tokens by preserving significant temporal changes across video frames. Compared with existing token pruning methods [39, 48, 53, 61, 63], DTD offers three key advantages: (1) video-aware dynamic pruning that adaptively reduces video tokens from holistic video perspective, well-suited for both high-speed and slow-motion videos; (2) positional reservation that maintains the fine-grained spatial-temporal positions of retained tokens; and (3) streaming-friendly operation that calculates visual redundancy only for newly incoming frames without re-processing historical video content. Moreover, DTD is orthogonal to memorybased [75] or KV-Cache Retrieval streaming methods [12, 44], serving as an initial efficient step to reduce the computational video burden and being complementary to these approaches."
        },
        {
            "title": "3.3 TimeChat-Online-139K Collection\nTo better apply the DTD design, we introduce TimeChat-Online-\n139K, a comprehensive synthetic streaming VideoQA dataset specif-\nically designed for training online-VideoLLMs. Existing works [6,",
            "content": "5 Conference17, July 2017, Washington, DC, USA L. Yao, Y. Li, Y. Wei, L. Li, et al. Table 2: Evaluation results on OVO-Bench [30] comprising three categories: i) Real-Time Visual Perception (OCR: Optical Character Recognition, ACR: Action Recognition, ATR: Attribute Recognition, STU: Spatial Understanding, FPD: Future Prediction, OJR: Object Recognition), ii) Backward Tracing (EPM: Episodic Memory, ASI: Action Sequence Identification, HLD: Hallucination Detection), and iii) Forward Active Responding (REC: Repetition Event Count, SSR: Sequential Steps Recognition, CRR: Clues Reveal Responding)."
        },
        {
            "title": "Model",
            "content": "#Frames Real-Time Visual Perception"
        },
        {
            "title": "Forward Active Responding",
            "content": "OCR ACR ATR STU FPD OJR Avg. EPM ASI HLD Avg. REC SSR CRR Avg."
        },
        {
            "title": "Human Agents",
            "content": "- 94.0 92.6 94.8 92.7 91. 94.0 93.2 92.6 93.0 91.4 92. 95.5 89.7 93.6 92."
        },
        {
            "title": "Proprietary Multimodal Models",
            "content": "Gemini 1.5 Pro GPT-4o LLaVA-NeXT-Video-7B LLaVA-OneVision-7B Qwen2-VL-7B InternVL-V2-8B LongVU-7B 1fps 64 64 64 64 64 1fps Flash-VStream-7B VideoLLM-online-8B TimeChat-Online-7B 1fps (44.6%) TimeChat-Online-7B 1fps (84.8%) 1fps 2fps TimeChat-Online-7B 1fps (100%) 87.3 69.1 67.0 65.1 80.2 65.5 54.5 50.0 68.3 68. 67.4 63.7 70.8 63.6 68.6 49.8 75.7 71.0 52.7 55.4 62.3 58. 35.5 27.6 74.2 73.2 61.7 59.4 57.2 53.4 Open-source Offline VideoLLMs 69.8 67.1 69.1 68.5 55. 25.5 8.1 74.5 69.8 75.2 59.6 58.7 53.2 58.7 49.5 32.1 23.9 48.6 48.6 46.8 66.4 69.8 63.8 69.0 59. 29.3 12.1 68.1 64.7 50.6 49.4 50.6 44.9 48.3 72.3 71.3 66.3 67.3 68.3 61.4 60.3 60.9 56.0 63.0 63.3 62.8 60.7 60.7 57.4 51.2 52.5 44.4 43.1 43. Open-source Online Video-LLMs 33.7 14.0 48.3 44.9 29.7 45.5 69.3 68.3 28.8 21.2 59.8 55.4 29.9 20.8 61.4 58.6 36.4 22.2 56.9 53. 64.2 58.8 66.9 61.5 66.2 33.8 18.8 64.9 62.8 9.7 23.7 34.4 27.4 9.1 5.9 12.2 11.8 9.1 70.7 47. 69.3 61.4 61.9 55.9 59.5 9. 41.7 45.0 48.6 44.0 39.5 25.4 17.7 44.5 42.0 41.7 34.1 24.8 30.1 25.8 16.6 5.4 - 31.8 32.5 67.6 66.9 65.7 57.6 69. 67.3 - 38.5 36.5 60.8 60.8 50.8 52.9 60.0 60.0 - 40.0 40.0 31.6 38.5 40. 54.2 50.9 48.9 45.4 48.5 44.2 - 36.8 36.4 36."
        },
        {
            "title": "Overall",
            "content": "Avg. 92.8 65.3 58.6 53.1 52.9 52.7 50.1 48.5 33.2 - 47.6 (+14.4) 45.6 (+12.4) 46. 21, 46] convert dense video narrations [6] or timestamp-related tasks [37] into streaming dialogue datasets. However, these transformed data samples are limited in question-answer diversity and fail to mimic real-world interactions. Instead, our dataset encompasses diverse online tasks across backward tracing, real-time perception, and forward active responding to facilitate more flexible Real-time Interaction. Specifically, we collect long-range and visually informative videos, annotate scene-oriented dense captions, and produce diverse streaming question-answer samples using GPT-4o [41]. Step 1: Visually Informative Video Collection. The quality of video content is crucial for the Streaming VideoQA task. Videos with monotonous or static scenes provide limited information for multiple-turn real-time interactions. To address this, we collect visually informative videos characterized by diverse scene changes. First, we source long-form videos [46] and segment them into successive scenes using PySceneDetect1. We then extract key frames from each scene segment and eliminate redundant frames based on visual similarity measured by DINO-v2 [42]. To ensure sufficient visual diversity and information richness, we select videos containing more than 5 distinct scenes. Step 2: Scene-oriented Detailed Caption Generation. Following the collection of videos and extraction of key frames, we employ GPT-4o [41] to generate scene-oriented dense captions for each frame. These comprehensive descriptions encompass both static and dynamic visual elements, including shot types, object appearances and actions, environmental and background variations, and camera movements. To enhance the models ability to discern scene transitions, we provide preceding frame caption as contextual information. Step 3: Streaming VideoQA Generation. We generate streaming VideoQA samples by providing the dense captions of the current scene and the previous scenes as context. We utilize GPT-4o to generate diverse question-answer pairs, including Temporal-enhanced, 1https://www.scenedetect.com/ 6 Backward Tracing, Real-Time Visual Perception, and Forward Active Responding QAs following previous works[30, 33]. Step 4: Negative Samples for Future-Response Training. To facilitate the training of proactive responding capabilities, we construct negative samples that intentionally cannot be answered using the currently available streaming video content. We carefully select irrelevant video frames before the question timestamp and generate corresponding answers labeled as unanswerable. Data Statistics. We collect 11,043 visually informative videos ranging from 5 minutes to several hours in length, with an average duration of 11.1 minutes per video. From each video, we extract an average of 87.8 key frames with approximately 7.14 seconds between consecutive frames. Each key frame is annotated with detailed description averaging 176 words. Based on the dense descriptions, we generate 139K question-answer pairs. Details regarding data statistics, task types, and GPT-4o prompts are provided in the Appendix."
        },
        {
            "title": "3.4 Training and Real-Time Inference\nWe implement TimeChat-Online based on the long-context Qwen2.5-\nVL [5] architecture to support high-FPS dense frame processing. We\nincorporate DTD to efficiently reduce visual redundancy in video\nstreams. Our model is trained on a combination of our streaming\ndataset TimeChat-Online-139K and offline video understanding\ndatasets (LLaVA-Video-178K [80], Tarsier2 [70] and VideoChat-\nFlash [29]) to ensure robust performance. During training, we densely\nsample frames at 1 FPS to simulate streaming video input with a\nmaximum sequence length of N frames. For DTD, we apply token\ndropping with 50% probability to each training batch.",
            "content": "During inference, TimeChat-Online processes video frames at 1 FPS and uses DTD to drop approximately 85% of video tokens. first-in-first-out memory bank stores the slimmed historical video tokens. It is worth noting that this memory design is not the emphasis of this paper, and can be replaced with alternatives [12, 75]. TimeChat-Online: 80% Visual Tokens are Naturally Redundant in Streaming Videos Table 3: Results on offline long video benchmarks. We report the accuracy on the MLVU [82], LongVideoBench [59] and VideoMME(w/o subtitles) [17]. indicates the reproduced results."
        },
        {
            "title": "Model",
            "content": "#Frames"
        },
        {
            "title": "VideoMME",
            "content": "overall long"
        },
        {
            "title": "Video Length",
            "content": "- 3120 min 8 sec60 min 160 min 3060 min Open-Source Offline VideoLLMs LLaMA-VID-7B MovieChat-7B LLaVA-Next-Video-7B VideoChat2-7B LongVA-7B Kangaroo-7B Video-CCAM-14B VideoXL-7B Qwen2.5-VL-7B Qwen2.5-VL-7B w/ DTD Qwen2.5-VL-7B w/ DTD 1fps 2048 32 16 128 64 96 128 1fps (100%) 1fps (46.2%) 1fps (84.6%) 33.2 25.8 - 47.9 56.3 61.0 63.1 64.9 66.9 68.6 68.8 Open-source Online VideoLLMs Dispider-7B [CVPR 2025] VideoChat-Online-8B [CVPR 2025] 1fps 2fps TimeChat-Online-7B TimeChat-Online-7B TimeChat-Online-7B 1fps (100%) 1fps (46.3%) 1fps (85.0%) 61.7 - 62.6 62.9 65.4 - - 43.5 39.3 - 54.2 - - 61.5 61.6 59.3 - - 55.4 57.1 57.7 - 38.2 46.6 39.5 52.6 56.0 53.2 55.5 63.2 63.4 64.9 57.2 52.8 62.4 63.3 62.5 - 33.4 - 33.2 46.2 46.6 46.7 49.2 50.4 51.9 56.1 - 44. 48.4 52.4 49.2 Conference17, July 2017, Washington, DC, USA Table 4: Ablation study of different token dropping strategies on StreamingBench (Real-Time Visual Understanding). For fair comparison, all methods are evaluated with Qwen2.5VL-7B (Vanilla) without supervised fine-tuning (SFT). Δ represents accuracy retention rate (%) relative to the full token setting. Method w/o SFT StreamingBench (3s24min) Δ Vanilla, Avg 22K Video Tokens per Video (1 fps) Qwen2.5VL-7B 73.7 100% Drop 44.1%, Avg 12K Video Tokens per Video + VisionZip [CVPR 2025] + Pixel-level drop + Feature-level drop (frame-aware) + Feature-level drop (video-aware) Drop 82.5%, Avg 4K Tokens per Video + VisionZip [CVPR 2025] + Pixel-level drop + Feature-level drop (frame-aware) + Feature-level drop (video-aware) 72.3 72.8 73.0 73.4 68.8 68.8 72.0 72.0 98.1% 98.8% 99.1% 99.6% 93.4% 93.4% 97.7% 97.7% Proactive Response with Trigger Time. For Future-responding questions, the model must decide the optimal timestamps to generate proactive response. We define these decision time points as Trigger times, which always occur when the video transitions to new scene [43]. These Trigger times can be effectively monitored through the token drop ratio curve along the timeline, as illustrated in Figure 2 (bottom). The valleys in this curve indicate moments when the current frame content changes significantly from previous frames, revealing scene transition. We implement proactive response by generating responses at each trigger time using the most recent video content. Through training with the negative samples in TimeChat-Online139K (Section 3.3), the model learns to generate an unanswerable response when the available video content is insufficient at particular trigger time, effectively instructing the system to wait for the next trigger time to respond again. 4 Experiments 4.1 We implement our model using the Qwen2.5VL 7B architecture. For training, we sample frames at 1 FPS with maximum sequence length of 64 frames for streaming VideoQA samples. We configure the model with maximum input resolution of 448448 pixels, threshold parameters 𝜏𝑓 𝑒𝑎𝑡 = 0.7, batch size 128, and learning rate 1e-5. Our training procedure combines both offline and online datasets, including subsets of LLaVA-Video-178K (100K samples), Tarsier2 (100K samples), VideoChat-Flash (3K samples) and TimeChat-Online-139K, among which negative samples with the unanswerable label are 20K. We keep the vision encoder frozen while fine-tuning the full-parameter projector and language model for 1 epoch. All experiments are performed on 8A800 80G GPUs. The hard drop ratio to determine Trigger Time is 60%. During inference, we maintain the 1 FPS frame processing rate while extending the maximum frame length to 1016. We use feature-level dropping for all experiments by default since it performs consistently better than"
        },
        {
            "title": "Implementation Details",
            "content": "7 pixel-level dropping as shown in Figure 1. We set 𝜏𝑓 𝑒𝑎𝑡 = 0.25/0.5 corresponding to around 85% / 45% token dropping rate by default. For real-time interaction, the most recent 6K slimmed video tokens after dropping are preserved in first-in-first-out memory bank, introducing maximum latency of 2 seconds for responding."
        },
        {
            "title": "4.2 Results on Streaming Video Benchmarks\nWe first evaluate our model’s performance on two streaming VideoQA\nbenchmarks: StreamingBench [33] and OVO-Bench [30]. For these\nevaluations, VideoLLMs processes the historical video content\nreceived before the Current timestamp, which represents the moment\nwhen a user question is posed.\nStreamingBench. Table 1 shows that TimeChat-Online achieves\nstate-of-the-art performance with a score of 75.28 on the Real-time\nVisual Understanding subtask of StreamingBench, representing a 7.65\nimprovement over the recent online model Dispider-7B [43] scored\n67.63. This demonstrates that TimeChat-Online effectively combines\nthe superior VideoQA capabilities of offline VideoLLMs with the\nreal-time streaming inference capabilities of online VideoLLMs.\nMoreover, this 75.28 score surpasses both the best offline VideoLLM\nQwen2.5VL-7B (73.68) and proprietary models including GPT-4o\n(73.28) and Claude-3.5-Sonnet (72.44).",
            "content": "When compared to Qwen2.5VL-7B with 1fps full token inputting, TimeChat-Online achieves superior performance (75.28 vs. 73.68) while requiring 44.2% fewer video tokens. Even at an extreme token dropping ratio of 82.8%, TimeChat-Online maintains comparable results to the full token setting of Qwen2.5VL-7B (73.64 vs. 73.68). These findings highlight the substantial redundancy in 1 fps video streams and the effectiveness of our DTD approach. OVO-Bench. Table 2 presents the results on OVO-Bench, which comprehensively evaluates Backward Tracing and Forward Active Responding capabilities across 12 diverse subtasks. TimeChat-Online substantially outperforms existing online VideoLLMs, achieving Conference17, July 2017, Washington, DC, USA L. Yao, Y. Li, Y. Wei, L. Li, et al. Table 5: Impact of training datasets on StreamingBench (RealTime Visual Understanding) performance across different token drop ratios. Ours refers to our proposed dataset TimeChatOnline-139K."
        },
        {
            "title": "Token Drop Ratio",
            "content": "44% 82.6% Qwen2.5-VL-7B (Vanilla) + LLaVA-Video100K + LLaVA-Video100K + Tarsier2129K + LLaVA-Video100K + Tarsier2129K + VideoChat-Flash3K + Above Offline Datasets + Ours 73.4 73.4 73.5 74.0 75.3 72.0 72.3 72.5 72.8 73.6 final score of 47.6, which represents significant 14.4-point absolute improvement over Flash-VStream [75] and VideoLLM-online [6]. Notably, with 84.8% of video tokens dropped, TimeChat-Online maintains robust performance with score of 45.6. Case Study. Figure 4 visualizes proactive responding case of TimeChat-Online. When user proposes question What specifically did the woman in red do? that can also be answered by future moments, TimeChat-Online proactively generates responses at future trigger times (i.e., the video scene transition timestamps), which are indicated by frames with low token drop ratios."
        },
        {
            "title": "4.4 Ablation Study\nEffectiveness of DTD Design. We compare different token dropping\nmethods in Table 4. VisionZip [68] represents a similar purely-vision\nspatial token selection method, while pixel-level and feature-level\ndropping approaches are introduced in this paper (Section 3.2.2).\nZero-shot results with Qwen2.5-VL-7B demonstrate that Feature-\nlevel (video-aware) Dropping, i.e., the final design of DTD, achieves\nthe best performance under the same dropping ratio. The frame-aware",
            "content": "Figure 3: Video redundancy of different video length on VideoMME [17]. dropping approach applies fixed dropping ratio to each frame, in contrast to the video-aware approach that dynamically selects tokens across the entire video. These results reveal that joint spatio-temporal dynamic token pruning proves most effective. Effectiveness of TimeChat-Online-139K dataset. Table 5 demonstrates the impact of our training strategy that combines online and offline datasets. The results clearly show that integrating TimeChatOnline-139K with existing offline VideoQA datasets significantly enhances streaming performance."
        },
        {
            "title": "6 Conclusion\nThis paper introduces TimeChat-Online, a novel online VideoLLM\nthat incorporates Differential Token Dropping, a simple yet effi-\ncient approach addressing long-form high-redundancy challenges in",
            "content": "8 TimeChat-Online: 80% Visual Tokens are Naturally Redundant in Streaming Videos Conference17, July 2017, Washington, DC, USA Figure 4: Case study of TimeChat-Online on StreamingBench. When user proposes question What specifically did the woman in red do? that can also be answered by the future moments, TimeChat-Online will proactively generate responses at the future trigger time (i.e., the video scene transition timestamps), which are indicated by the frames with low token drop ratios. streaming video understanding. Comprehensive experiments demonstrate that TimeChat-Online achieves state-of-the-art performance on streaming video benchmarks while eliminating up to 82.8% of visual tokens. Additionally, our analysis reveals that reducing temporal redundancy is particularly critical for hours-long videos, which can eliminate over 95% of tokens via the DTD strategy without performance degradation. For real-time interaction in streaming scenarios, we also propose synthetic TimeChat-Online-139K to endow TimeChat-Online with backward-tracing, real-time perception, and proactive future-responding capabilities. This work not only highlights the substantial redundancy in video streams but also establishes promising direction for computationally efficient video encoding in streaming VideoQA tasks. References [1] Charu Aggarwal, Alexander Hinneburg, and Daniel Keim. 2001. On the surprising behavior of distance metrics in high dimensional space. In International conference on database theory. Springer, 420434. [2] Anthropic. 2024. Claude 3.5 Sonnet. https://www.anthropic.com/news/claude-35-sonnet [3] Kirolos Ataallah, Xiaoqian Shen, Eslam Abdelrahman, Essam Sleiman, Deyao Zhu, Jian Ding, and Mohamed Elhoseiny. 2024. MiniGPT4-Video: Advancing Multimodal LLMs for Video Understanding with Interleaved Visual-Textual Tokens. ArXiv preprint abs/2404.03413 (2024). [4] Jinze Bai, Shuai Bai, Shusheng Yang, Shĳie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. 2023. Qwen-VL: Frontier Large Vision-Language Model with Versatile Abilities. ArXiv preprint abs/2308.12966 (2023). [5] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shĳie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin. 2025. Qwen2.5-VL Technical Report. arXiv preprint arXiv:2502.13923 (2025). Conference17, July 2017, Washington, DC, USA L. Yao, Y. Li, Y. Wei, L. Li, et al. [6] Joya Chen, Zhaoyang Lv, Shiwei Wu, Kevin Qinghong Lin, Chenan Song, Difei Gao, Jia-Wei Liu, Ziteng Gao, Dongxing Mao, and Mike Zheng Shou. 2024. Videollm-online: Online video large language model for streaming video. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 1840718418. [7] Liang Chen, Haozhe Zhao, Tianyu Liu, Shuai Bai, Junyang Lin, Chang Zhou, and Baobao Chang. 2024. An Image is Worth 1/2 Tokens After Layer 2: Plug-and-Play Inference Acceleration for Large Vision-Language Models. arXiv:2403.06764 [cs.CV] [8] Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, Ji Ma, Jiaqi Wang, Xiaoyi Dong, Hang Yan, Hewei Guo, Conghui He, Botian Shi, Zhenjiang Jin, Chao Xu, Bin Wang, Xingjian Wei, Wei Li, Wenjian Zhang, Bo Zhang, Pinlong Cai, Licheng Wen, Xiangchao Yan, Min Dou, Lewei Lu, Xizhou Zhu, Tong Lu, Dahua Lin, Yu Qiao, Jifeng Dai, and Wenhai Wang. 2024. How Far Are We to GPT-4V? Closing the Gap to Commercial Multimodal Models with Open-Source Suites. arXiv:2404.16821 [cs.CV] https://arxiv.org/abs/2404.16821 [9] Zesen Cheng, Sicong Leng, Hang Zhang, Yifei Xin, Xin Li, Guanzheng Chen, Yongxin Zhu, Wenqi Zhang, Ziyang Luo, Deli Zhao, et al. 2024. Videollama 2: Advancing spatial-temporal modeling and audio understanding in video-llms. arXiv preprint arXiv:2406.07476 (2024). [10] Rohan Choudhury, Guanglei Zhu, Sihan Liu, Koichiro Niinuma, Kris Kitani, and László Jeni. 2024. Dont Look Twice: Faster Video Transformers with Run-Length Tokenization. Advances in Neural Information Processing Systems 37 (2024), 2812728149. [11] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven C. H. Hoi. 2023. InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning. In Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine (Eds.). [12] Shangzhe Di, Zhelun Yu, Guanghao Zhang, Haoyuan Li, Tao Zhong, Hao Cheng, Bolin Li, Wanggui He, Fangxun Shu, and Hao Jiang. 2025. Streaming video question-answering with in-context video kv-cache retrieval. arXiv preprint arXiv:2503.00540 (2025). [13] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Heigold, Gelly, et al. 2020. An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. In International Conference on Learning Representations. [14] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurélien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Rozière, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip Radenovic, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Graeme Nail, Grégoire Mialon, Guan Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov, Imanol Arrieta Ibarra, Isabel M. Kloumann, Ishan Misra, Ivan Evtimov, Jade Copet, Jaewon Lee, Jan Geffert, Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasuden Alwala, Kartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heafield, Kevin Stone, and et al. 2024. The Llama 3 Herd of Models. ArXiv preprint abs/2407.21783 (2024). [15] Bernard Ghanem Fabian Caba Heilbron, Victor Escorcia and Juan Carlos Niebles. 2015. ActivityNet: Large-Scale Video Benchmark for Human Activity Understanding. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 961970. [16] Jiajun Fei, Dian Li, Zhidong Deng, Zekun Wang, Gang Liu, and Hui Wang. 2024. Video-ccam: Enhancing video-language understanding with causal cross-attention masks for short and long videos. arXiv preprint arXiv:2408.14023 (2024). [17] Chaoyou Fu, Yuhan Dai, Yondong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, et al. 2024. Video-MME: The First-Ever Comprehensive Evaluation Benchmark of Multi-modal LLMs in Video Analysis. ArXiv preprint abs/2405.21075 (2024). [18] Gemini Team. 2024. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. ArXiv preprint abs/2403.05530 (2024). [19] Shengding Hu, Yuge Tu, Xu Han, Chaoqun He, Ganqu Cui, Xiang Long, Zhi Zheng, Yewei Fang, Yuxiang Huang, Weilin Zhao, et al. 2024. Minicpm: Unveiling the potential of small language models with scalable training strategies. arXiv preprint arXiv:2404.06395 (2024). 10 [20] Gabriel Huang, Bo Pang, Zhenhai Zhu, Clara Rivera, and Radu Soricut. 2020. Multimodal Pretraining for Dense Video Captioning. arXiv:2011.11760 [cs.CV] https://arxiv.org/abs/2011.11760 [21] Zhenpeng Huang, Xinhao Li, Jiaqi Li, Jing Wang, Xiangyu Zeng, Cheng Liang, Tao Wu, Xi Chen, Liang Li, and Limin Wang. 2024. Online video understanding: comprehensive benchmark and memory-augmented method. arXiv preprint arXiv:2501.00584 (2024). [22] Peng Jin, Ryuichi Takanobu, Wancai Zhang, Xiaochun Cao, and Li Yuan. 2024. Chat-univi: Unified visual representation empowers large language models with image and video understanding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 1370013710. [23] Jie Lei, Tamara L. Berg, and Mohit Bansal. 2021. QVHighlights: Detecting Moments and Highlights in Videos via Natural Language Queries. arXiv:2107.09609 [cs.CV] https://arxiv.org/abs/2107.09609 [24] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. 2024. LLaVA-OneVision: Easy Visual Task Transfer. ArXiv preprint abs/2408.03326 (2024). [25] Hongyu Li, Jinyu Chen, Ziyu Wei, Shaofei Huang, Tianrui Hui, Jialin Gao, Xiaoming Wei, and Si Liu. 2025. LLaVA-ST: Multimodal Large Language Model for Fine-Grained Spatial-Temporal Understanding. arXiv preprint arXiv:2501.08282 (2025). [26] Kunchang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai Wang, Ping Luo, Yali Wang, Limin Wang, and Yu Qiao. 2023. VideoChat: Chat-Centric Video Understanding. ArXiv preprint abs/2305.06355 (2023). [27] Lei Li, Yuanxin Liu, Linli Yao, Peiyuan Zhang, Chenxin An, Lean Wang, Xu Sun, Lingpeng Kong, and Qi Liu. 2025. Temporal Reasoning Transfer from Text to Video. In ICLR 2025. OpenReview.net. https://openreview.net/forum?id=sHAvMp5J4R [28] Xinhao Li, Yi Wang, Jiashuo Yu, Xiangyu Zeng, Yuhan Zhu, Haian Huang, Jianfei Gao, Kunchang Li, Yinan He, Chenting Wang, et al. 2024. Videochatflash: Hierarchical compression for long-context video modeling. arXiv preprint arXiv:2501.00574 (2024). [29] Xinhao Li, Yi Wang, Jiashuo Yu, Xiangyu Zeng, Yuhan Zhu, Haian Huang, Jianfei Gao, Kunchang Li, Yinan He, Chenting Wang, et al. 2024. Videochatflash: Hierarchical compression for long-context video modeling. arXiv preprint arXiv:2501.00574 (2024). [30] Yifei Li, Junbo Niu, Ziyang Miao, Chunjiang Ge, Yuanhang Zhou, Qihao He, Xiaoyi Dong, Haodong Duan, Shuangrui Ding, Rui Qian, Pan Zhang, Yuhang Zang, Yuhang Cao, Conghui He, and Jiaqi Wang. 2025. OVO-Bench: How Far is Your Video-LLMs from Real-World Online Video Understanding? arXiv:2501.05510 [cs.CV] https://arxiv.org/abs/2501.05510 [31] Yanwei Li, Chengyao Wang, and Jiaya Jia. 2024. Llama-vid: An image is worth 2 tokens in large language models. In European Conference on Computer Vision. Springer, 323340. [32] Yanwei Li, Chengyao Wang, and Jiaya Jia. 2024. Llama-vid: An image is worth 2 tokens in large language models. In European Conference on Computer Vision. Springer, 323340. [33] Junming Lin, Zheng Fang, Chi Chen, Zihao Wan, Fuwen Luo, Peng Li, Yang Liu, and Maosong Sun. 2024. StreamingBench: Assessing the Gap for MLLMs to Achieve Streaming Video Understanding. arXiv preprint arXiv:2411.03628 (2024). [34] Ji Lin, Hongxu Yin, Wei Ping, Pavlo Molchanov, Mohammad Shoeybi, and Song Han. 2024. Vila: On pre-training for visual language models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2668926699. [35] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. 2024. LLaVA-NeXT: Improved reasoning, OCR, and world knowledge. [36] Jiajun Liu, Yibing Wang, Hanghang Ma, Xiaoping Wu, Xiaoqi Ma, xiaoming Wei, Jianbin Jiao, Enhua Wu, and Jie Hu. 2024. Kangaroo: Powerful Video-Language Model Supporting Long-context Video Input. arXiv preprint arXiv:2408.15542 (2024). [37] Ye Liu, Zongyang Ma, Zhongang Qi, Yang Wu, Ying Shan, and Chang Chen. 2024. Et bench: Towards open-ended event-level video-language understanding. Advances in Neural Information Processing Systems 37 (2024), 3207632110. [38] Zhihang Liu, Chen-Wei Xie, Pandeng Li, Liming Zhao, Longxiang Tang, Yun Zheng, Chuanbin Liu, and Hongtao Xie. 2025. Hybrid-Level Instruction Injection for Video Token Compression in Multi-modal Large Language Models. arXiv preprint arXiv:2503.16036 (2025). [39] Zhenyu Ning, Jieru Zhao, Qihao Jin, Wenchao Ding, and Minyi Guo. 2024. InfMLLM: Efficient streaming inference of multimodal large language models on single GPU. arXiv preprint arXiv:2409.09086 (2024). [40] Andreea-Maria Oncescu, Joao Henriques, Yang Liu, Andrew Zisserman, and Samuel Albanie. 2021. Queryd: video dataset with high-quality text and audio narrations. In ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 22652269. [41] OpenAI. 2024. GPT-4o System Card. [42] Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. 2023. Dinov2: Learning robust visual features without supervision. TimeChat-Online: 80% Visual Tokens are Naturally Redundant in Streaming Videos Conference17, July 2017, Washington, DC, USA arXiv preprint arXiv:2304.07193 (2023). [43] Rui Qian, Shuangrui Ding, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao, Dahua Lin, and Jiaqi Wang. 2025. Dispider: Enabling Video LLMs with Active Real-Time Interaction via Disentangled Perception, Decision, and Reaction. arXiv preprint arXiv:2501.03218 (2025). [44] Rui Qian, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Shuangrui Ding, Dahua Lin, and Jiaqi Wang. 2024. Streaming long video understanding with large language models. Advances in Neural Information Processing Systems 37 (2024), 119336119360. [45] Shuhuai Ren, Sishuo Chen, Shicheng Li, Xu Sun, and Lu Hou. 2023. TESTA: Temporal-Spatial Token Aggregation for Long-form Video-Language Understanding. ArXiv abs/2310.19060 (2023). [46] Shuhuai Ren, Linli Yao, Shicheng Li, Xu Sun, and Lu Hou. 2024. Timechat: time-sensitive multimodal large language model for long video understanding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 1431314323. [47] Ronald A. Rensink. 2002. Change detection. Annual Review of Psychology 53 (2002), 245277. [48] Xiaoqian Shen, Yunyang Xiong, Changsheng Zhao, Lemeng Wu, Jun Chen, Chenchen Zhu, Zechun Liu, Fanyi Xiao, Balakrishnan Varadarajan, Florian Bordes, et al. 2024. Longvu: Spatiotemporal adaptive compression for long video-language understanding. arXiv preprint arXiv:2410.17434 (2024). [49] Daniel Simons and Ronald Rensink. 2005. Change blindness: Past, present, and future. Trends in cognitive sciences 9, 1 (2005), 1620. [50] Yale Song, Jordi Vallmitjana, Amanda Stent, and Alejandro Jaimes. 2015. TVSum: Summarizing web videos using titles. In 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 51795187. doi:10.1109/CVPR.2015.7299154 [51] Harald Steck, Chaitanya Ekanadham, and Nathan Kallus. 2024. Is cosine-similarity of embeddings really about similarity?. In Companion Proceedings of the ACM Web Conference 2024. 887890. [52] Yansong Tang, Dajun Ding, Yongming Rao, Yu Zheng, Danyang Zhang, Lili Zhao, Jiwen Lu, and Jie Zhou. 2019. COIN: Large-scale Dataset for Comprehensive Instructional Video Analysis. arXiv:1903.02874 [cs.CV] https://arxiv.org/abs/ 1903.02874 [53] Keda Tao, Can Qin, Haoxuan You, Yang Sui, and Huan Wang. 2024. DyCoke: Dynamic Compression of Tokens for Fast Video Large Language Models. arXiv preprint arXiv:2411.15024 (2024). [54] Peng Wang, Shuai Bai, Sinan Tan, Shĳie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, and Junyang Lin. 2024. Qwen2-VL: Enhancing Vision-Language Models Perception of the World at Any Resolution. arXiv preprint arXiv:2409.12191 (2024). [55] Weiying Wang, Jieting Chen, and Qin Jin. 2020. VideoIC: Video Interactive Comments Dataset and Multimodal Multitask Learning for Comments Generation. In Proceedings of the 28th ACM International Conference on Multimedia (Seattle, WA, USA) (MM 20). Association for Computing Machinery, New York, NY, USA, 25992607. doi:10.1145/3394171.3413890 [56] Weiying Wang, Yongcheng Wang, Shizhe Chen, and Qin Jin. 2019. YouMakeup: Large-Scale Domain-Specific Multimodal Dataset for Fine-Grained Semantic Comprehension. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-ĲCNLP). 51365146. [57] Yueqian Wang, Xiaojun Meng, Yuxuan Wang, Jianxin Liang, Jiansheng Wei, Huishuai Zhang, and Dongyan Zhao. 2024. VideoLLM Knows When to Speak: Enhancing Time-Sensitive Video Comprehension with Video-Text Duet Interaction Format. arXiv:2411.17991 [cs.CV] https://arxiv.org/abs/2411. [58] Zichen Wen, Yifeng Gao, Shaobo Wang, Junyuan Zhang, Qintong Zhang, Weĳia Li, Conghui He, and Linfeng Zhang. 2025. Stop Looking for Important Tokens in Multimodal Language Models: Duplication Matters More. arXiv preprint arXiv:2502.11494 (2025). [59] Haoning Wu, Dongxu Li, Bei Chen, and Junnan Li. 2024. LongVideoBench: Benchmark for Long-context Interleaved Video-Language Understanding. ArXiv preprint abs/2407.15754 (2024). [60] Shiwei Wu, Joya Chen, Kevin Qinghong Lin, Qimeng Wang, Yan Gao, Qianli Xu, Tong Xu, Yao Hu, Enhong Chen, and Mike Zheng Shou. 2024. Videollm-mod: Efficient video-language streaming with mixture-of-depths vision computation. Advances in Neural Information Processing Systems 37 (2024), 109922109947. [61] Haomiao Xiong, Zongxin Yang, Jiazuo Yu, Yunzhi Zhuge, Lu Zhang, Jiawen Zhu, and Huchuan Lu. 2025. Streaming Video Understanding and Multi-round Interaction with Memory-enhanced Knowledge. arXiv preprint arXiv:2501.13468 (2025). [62] Jin Xu, Zhifang Guo, Jinzheng He, Hangrui Hu, Ting He, Shuai Bai, Keqin Chen, Jialin Wang, Yang Fan, Kai Dang, et al. 2025. Qwen2. 5-Omni Technical Report. arXiv preprint arXiv:2503.20215 (2025). [63] Mingze Xu, Mingfei Gao, Zhe Gan, Hong-You Chen, Zhengfeng Lai, Haiming Gang, Kai Kang, and Afshin Dehghan. 2024. Slowfast-llava: strong training-free baseline for video large language models. arXiv preprint arXiv:2407.15841 (2024). [64] Hongwei Xue, Tiankai Hang, Yanhong Zeng, Yuchong Sun, Bei Liu, Huan Yang, Jianlong Fu, and Baining Guo. 2022. Advancing High-Resolution Video-Language Representation with Large-Scale Video Transcriptions. arXiv:2111.10337 [cs.CV] https://arxiv.org/abs/2111.10337 [65] An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang, Jialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jin Xu, Jingren Zhou, Jinze Bai, Jinzheng He, Junyang Lin, Kai Dang, Keming Lu, Keqin Chen, Kexin Yang, Mei Li, Mingfeng Xue, Na Ni, Pei Zhang, Peng Wang, Ru Peng, Rui Men, Ruize Gao, Runji Lin, Shĳie Wang, Shuai Bai, Sinan Tan, Tianhang Zhu, Tianhao Li, Tianyu Liu, Wenbin Ge, Xiaodong Deng, Xiaohuan Zhou, Xingzhang Ren, Xinyu Zhang, Xipin Wei, Xuancheng Ren, Yang Fan, Yang Yao, Yichang Zhang, Yu Wan, Yunfei Chu, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zhihao Fan. 2024. Qwen2 Technical Report. ArXiv preprint abs/2407.10671 (2024). [66] Chenyu Yang, Xuan Dong, Xizhou Zhu, Weĳie Su, Jiahao Wang, Hao Tian, Zhe Chen, Wenhai Wang, Lewei Lu, and Jifeng Dai. 2024. PVC: Progressive Visual Token Compression for Unified Image and Video Processing in Large Vision-Language Models. arXiv preprint arXiv:2412.09613 (2024). [67] Cheng Yang, Yang Sui, Jinqi Xiao, Lingyi Huang, Yu Gong, Chendi Li, Jinghua Yan, Yu Bai, Ponnuswamy Sadayappan, Xia Hu, et al. 2025. TopV: Compatible Token Pruning with Inference Time Optimization for Fast and Low-Memory Multimodal Vision Language Model. arXiv preprint arXiv:2503.18278 (2025). [68] Senqiao Yang, Yukang Chen, Zhuotao Tian, Chengyao Wang, Jingyao Li, Bei Yu, and Jiaya Jia. 2024. Visionzip: Longer is better but not necessary in vision language models. arXiv preprint arXiv:2412.04467 (2024). [69] Linli Yao, Lei Li, Shuhuai Ren, Lean Wang, Yuanxin Liu, Xu Sun, and Lu Hou. 2024. Deco: Decoupling token compression from semantic abstraction in multimodal large language models. arXiv preprint arXiv:2405.20985 (2024). [70] Liping Yuan, Jiawei Wang, Haomiao Sun, Yuchen Zhang, and Yuan Lin. 2025. Tarsier2: Advancing Large Vision-Language Models from Detailed Video Description to Comprehensive Video Understanding. arXiv preprint arXiv:2501.07888 (2025). [71] Zihao Yue, Qi Zhang, Anwen Hu, Liang Zhang, Ziheng Wang, and Qin Jin. 2023. Movie101: New Movie Understanding Benchmark. arXiv:2305.12140 [cs.CV] https://arxiv.org/abs/2305.12140 [72] Abhay Zala, Jaemin Cho, Satwik Kottur, Xilun Chen, Barlas Oğuz, Yashar Mehdad, and Mohit Bansal. 2023. Hierarchical Video-Moment Retrieval and Step-Captioning. In CVPR. [73] Boqiang Zhang, Kehan Li, Zesen Cheng, Zhiqiang Hu, Yuqian Yuan, Guanzheng Chen, Sicong Leng, Yuming Jiang, Hang Zhang, Xin Li, et al. 2025. VideoLLaMA 3: Frontier Multimodal Foundation Models for Image and Video Understanding. arXiv preprint arXiv:2501.13106 (2025). [74] Hang Zhang, Xin Li, and Lidong Bing. 2023. Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, Yansong Feng and Els Lefever (Eds.). 543553. doi:10.18653/ v1/2023.emnlp-demo.49 [75] Haoji Zhang, Yiqin Wang, Yansong Tang, Yong Liu, Jiashi Feng, Jifeng Dai, and Xiaojie Jin. 2024. Flash-vstream: Memory-based real-time understanding for long video streams. arXiv preprint arXiv:2406.08085 (2024). [76] Jianrui Zhang, Mu Cai, and Yong Jae Lee. 2024. Vinoground: Scrutinizing LMMs over Dense Temporal Reasoning with Short Videos. arXiv preprint arXiv:2410.02763 (2024). [77] Pan Zhang, Xiaoyi Dong, Yuhang Cao, Yuhang Zang, Rui Qian, Xilin Wei, Lin Chen, Yifei Li, Junbo Niu, Shuangrui Ding, et al. 2024. Internlm-xcomposer2. 5-omnilive: comprehensive multimodal system for long-term streaming video and audio interactions. arXiv preprint arXiv:2412.09596 (2024). [78] Pan Zhang, Xiaoyi Dong, Yuhang Cao, Yuhang Zang, Rui Qian, Xilin Wei, Lin Chen, Yifei Li, Junbo Niu, Shuangrui Ding, Qipeng Guo, Haodong Duan, Xin Chen, Han Lv, Zheng Nie, Min Zhang, Bin Wang, Wenwei Zhang, Xinyue Zhang, Jiaye Ge, Wei Li, Jingwen Li, Zhongying Tu, Conghui He, Xingcheng Zhang, Kai Chen, Yu Qiao, Dahua Lin, and Jiaqi Wang. 2024. InternLM-XComposer2.5OmniLive: Comprehensive Multimodal System for Long-term Streaming Video and Audio Interactions. arXiv preprint arXiv:2412.09596 (2024). [79] Peiyuan Zhang, Kaichen Zhang, Bo Li, Guangtao Zeng, Jingkang Yang, Yuanhan Zhang, Ziyue Wang, Haoran Tan, Chunyuan Li, and Ziwei Liu. 2024. Long Context Transfer from Language to Vision. ArXiv preprint abs/2406.16852 (2024). [80] Yuanhan Zhang, Jinming Wu, Wei Li, Bo Li, Zejun Ma, Ziwei Liu, and Chunyuan Li. 2024. Video Instruction Tuning With Synthetic Data. arXiv:2410.02713 [cs.CV] https://arxiv.org/abs/2410.02713 [81] Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin Zheng, Ruisi Cai, Zhao Song, Yuandong Tian, Christopher Ré, Clark Barrett, et al. 2023. H2o: Heavy-hitter oracle for efficient generative inference of large language models. Advances in Neural Information Processing Systems 36 (2023), 3466134710. [82] Junjie Zhou, Yan Shu, Bo Zhao, Boya Wu, Shitao Xiao, Xi Yang, Yongping Xiong, Bo Zhang, Tiejun Huang, and Zheng Liu. 2024. MLVU: Comprehensive Benchmark for Multi-Task Long Video Understanding. ArXiv preprint abs/2406.04264 (2024). Conference17, July 2017, Washington, DC, USA L. Yao, Y. Li, Y. Wei, L. Li, et al. [83] Luowei Zhou, Chenliang Xu, and Jason Corso. 2018. Towards Automatic Learning of Procedures From Web Instructional Videos. In AAAI Conference on Artificial Intelligence. 75907598. https://www.aaai.org/ocs/index.php/AAAI/ AAAI18/paper/view/17344 Table 6: Consistency of drop ratios (%) across different datasets with the same 𝜏𝑓 𝑒𝑎𝑡 values, demonstrating that our method captures intrinsic video redundancy independent of dataset characteristics. Appendix In the appendix, we provide more results and analysis and summarize them as follows: In Section A.1, we report extensive experimental results. In Section A.2, we present the training hyperparameter tables. In Section A.3, we present details of TimeChat-Online-139K. In Section A.4, we introduce diverse visualization cases for the Differential Token Drop design. A.1 More Experimental Results StreamingBench Full-set Results. Table 11 presents the comprehensive results across all three categories of StreamingBench. TimeChatOnline achieves state-of-the-art performance among open-source models with an overall score of 58.11, outperforming the recent online model Dispider-7B [43] by 4.99 points (58.11 vs. 53.12). While proprietary model Gemini 1.5 pro leads with 67.07, TimeChatOnline surpasses Claude-3.5-Sonnet (57.68) and all open-source offline VideoLLMs. For Omni-Source and Contextual Understanding, TimeChat-Online scores 37.80 and 35.30 respectively, consistently surpassing Dispider-7B. Most importantly, with 82.6% token reduction, TimeChat-Online maintains 97.3% of its original performance (56.56 vs. 58.11), confirming that most visual tokens in streaming videos are redundant while our approach preserves essential information for comprehensive understanding. Selection of Hyperparameters 𝜏. The values of 𝜏𝑝𝑖𝑥𝑒𝑙 and 𝜏𝑓 𝑒𝑎𝑡 control the overall drop ratio of video tokens by dynamically measuring the visual difference between temporally consecutive frames. This approach identifies redundancy based solely on the videos inherent temporal characteristics, without requiring language guidance. Remarkably, we observe that the relationship between 𝜏 values and drop ratios remains consistent across diverse datasets, as shown in Table 6. For example, 𝜏𝑓 𝑒𝑎𝑡 = 0.25 consistently yields approximately 85% token reduction across StreamingBench, OVOBench, LongVideoBench, and MLVU, while 𝜏𝑓 𝑒𝑎𝑡 = 0.5 results in approximately 45%-55% reduction. More 𝜏-dropratio correspondence can also be found in Figure 1 (top right) in the main paper. This consistency demonstrates that visual redundancy is an intrinsic property of videos regardless of content type or task domain. Based on our comprehensive experiments, we conclude that over 80% of visual information in long-form videos is naturally redundant. Therefore, we recommend using 𝜏𝑓 𝑒𝑎𝑡 = 0.25 as the optimal setting for video token dropping, as it maintains high performance while significantly reducing computational requirements. Breakdown Analysis of Fine-grained subtasks on MLVU. Table 8 shows the performance breakdown on fine-grained subtasks of MLVU. Remarkably, even with 89.5% token reduction, our model not only maintains but improves performance across multiple challenging fine-grained visual understanding tasks compared to the full token baseline. The needle-in-haystack task (NQA) improves from 80.0 to 82.0, plot reading (PQA) from 66.4 to 68.8, temporal ordering (AO) 𝜏𝑓 𝑒𝑎𝑡 0.5 0.25 Drop Ratio (%) StreamingBench OVOBench LongVideoBench MLVU 44.1% 82.6% 44.6% 84.8% 56.5% 85.1% 46.3% 84.7% Hyper-parameter"
        },
        {
            "title": "Model Training",
            "content": "Max Context Length Batch Size Learning Rate Warmup Ratio Training epoch1 LR Scheduler Type FPS=1.0 448*448 128 64 14x14 80 8192 152064 64 64 8 11264 128 1e-5 0.05 1 Cosine Table 7: Training hyper-parameters for TimeChat-Online. from 40.9 to 49.4, and counting (AC) from 34.0 to 42.2, resulting in an overall improvement from 62.0 to 64.1. These results demonstrate that DTD effectively preserves essential visual details while discarding redundant information. Despite dropping nearly 90% of tokens, the model maintains the capability to recognize small objects (needle task), interpret plots, track temporal relationships, and count objects accurately. This is enabled by our spatial-temporal aware design, which preserves the original videos spatiotemporal structure even after significant token reduction. The improved performance on the counting task (AC) is particularly noteworthy, showing an 8.2 point gain, which confirms that our approach selectively retains detailed visual information critical for complex analysis tasks. A.2 Training Hyperparameter We provide detailed hyper-parameters in Table 7. 12 TimeChat-Online: 80% Visual Tokens are Naturally Redundant in Streaming Videos Conference17, July 2017, Washington, DC, USA with diverse scenes have less redundancy, while monotonous videos with static scenes display high redundancy. Figure 11 demonstrates how different hyperparameter 𝜏 values lead to different levels of redundancy control. Figure 8 and Figure 9 illustrate the Trigger Time naturally monitored in the drop ratio-timeline curve, where valleys of low drop ratio reveal video scene transitions. TimeChatOnline leverages this scene change detection without requiring additional perception modules as in Dispider [43], thereby reducing response delay. Figure 10 shows case study of TimeChat-Online on StreamingBench with specific drop ratio curve. When user proposes question What specifically did the woman in red do? that can also be answered by the future moments, TimeChat-Online will proactively generate responses at the future trigger time (i.e., the video scene transition timestamps), which are indicated by the frames (165 seconds, 196 seconds and 230 seconds) with low token drop ratios. Figure 12 shows the average drop ratio across different video durations. Since temporal redundancy is inherent to video, it has little correlation with video length. The analysis reveals that spatial-related tasks may have more visual redundancy, while temporal-reasoning tasks typically have less visual redundancy as they require more informative visual content for reasoning and response. Figure 13 presents the average drop ratio across different video subtask types. Table 8: Breakdown analysis on fine-grained subtasks of MLVU. Our approach with feature-level token dropping (89.5% reduction) improves performance across all subtasks. indicates lower than human performance, while indicates comparable or better than human performance. Drop Ratio (%) NeedleQA PlotQA) Action Order Action Counting M-Avg No drop 𝜏𝑓 𝑒𝑎𝑡 = 0.5 𝜏𝑓 𝑒𝑎𝑡 = 0.25 Δ 100% 46.4% 89.5% - 80.0 80.8 82.0 +2.0 66.4 67.7 68.8 +2.4 40.9 44.0 49.4 +8. 34.0 32.0 42.2 +8.2 62.0 61.9 64.1 +2.1 Figure 5: Distribution of video durations across the 11,043 videos in our dataset. The minimum video length in our dataset is 5 minutes. A.3 Dataset Statistics Our dataset comprises 11,043 videos sampled from 12 publicly available source datasets, with an average duration of 11.1 minutes per video. The composition of the video sources and the distribution of video durations are shown in Table 9 and Figure 5, respectively. Based on these videos, we generate total of 139K question-answer (QA) pairs, following the procedure outlined in main Section 3.3. These QA pairs are categorized into four high-level typesTemporalenhanced, Backward Tracing, Real-Time Visual Perception, and Forward Active Respondingencompassing eleven fine-grained subcategories. Representative prompt examples for each subtask are provided in Table 10, while the prompt templates for Step 2: Sceneoriented Detailed Caption Generation and Step 3: Streaming QA Pair Construction are shown in Table 12 and Table 13, respectively. A.4 Diverse Visualization Cases We visualize qualitative results in the following figures: Figure 6 and Figure 7: Visualization of Feature-level token dropping and Pixel-level token dropping. These demonstrate that DTD performs video-aware dynamic pruning based on specific video content. With identical 𝜏 hyperparameters, different frames within the same video exhibit varying drop ratios due to adaptive temporal redundancy. Different videos also show significant redundancy differences: informative videos 13 Conference17, July 2017, Washington, DC, USA L. Yao, Y. Li, Y. Wei, L. Li, et al."
        },
        {
            "title": "Dataset",
            "content": "#Videos"
        },
        {
            "title": "Dataset",
            "content": "#Videos"
        },
        {
            "title": "Dataset",
            "content": "#Videos COIN [52] HD-VILA [64] ViTT [20] VideoIC [55] 151 695 2000 2649 QV-Highlights [23] YouCook2 [83] QuerYD [40] Movie101 [71] 1778 710 566 202 ActivityNet [15] TVSum [50] YouMakeup [56] HiREST [72] 12 10"
        },
        {
            "title": "Total",
            "content": "11,043 Table 9: Source datasets and number of unique videos sampled from each. Dataset names link to their respective original publications."
        },
        {
            "title": "Event order",
            "content": "What is the correct order of events in this video? Temporal-enhanced"
        },
        {
            "title": "Key Attribute change",
            "content": "What are the key changes in the main characters attire from the beginning to the end of the video?"
        },
        {
            "title": "Camera transitions",
            "content": "How does the camera angle change between the beginning and middle frames?"
        },
        {
            "title": "Event relationship",
            "content": "Which event directly leads to [Cui Hu] aiming his gun at an adversary?"
        },
        {
            "title": "Key Frame Focus",
            "content": "Which frame captures the progression from indoor reflection to outdoor contemplation for Grandma Li Ailian?"
        },
        {
            "title": "Backward Tracing",
            "content": "Action Sequence Identification Which action occurred first in the makeup tutorial?"
        },
        {
            "title": "Episodic Memory",
            "content": "Which event occurred before the individual pointed towards the forest?"
        },
        {
            "title": "Hallucination Detection",
            "content": "Did the presenter ever apply green eyeshadow during the tutorial? Real-Time Visual Perception"
        },
        {
            "title": "Future Prediction",
            "content": "What is likely to happen next after the presenter shows the highlighter stick?"
        },
        {
            "title": "Sequential Steps Recognition",
            "content": "What are the steps involved in applying bronzer as shown in the tutorial?"
        },
        {
            "title": "Clues Reveal Responding",
            "content": "What accessory was visible when the person was blending the contour product? Table 10: Task taxonomy and corresponding prompt examples. (a) Feature-level: 𝜏𝑓 𝑒𝑎𝑡 = 0.4, drop ratio = 58.3% (b) Pixel-level: 𝜏𝑝𝑖𝑥𝑒𝑙 = 0.1 Figure 6: Visualization of (a) Feature-level token dropping and (b) Pixel-level token dropping for the video case 752 from StreamingBench. TimeChat-Online: 80% Visual Tokens are Naturally Redundant in Streaming Videos Conference17, July 2017, Washington, DC, USA Table 11: Performance comparison on StreamingBench full set including three categories: Real-Time Visual Understanding, OmniSource Understanding and Contextual Understanding. Model Params Frames OP CR CS ATP EU TR PR SU ACP CT All ER SCU SD MA All ACU MCU SQA PO All Real-Time Visual Understanding Omni-Source Understanding Contextual Understanding Overall - 89.47 92.00 93.60 91.47 95.65 92. 88.00 88.75 89.74 91.30 91.46 88. 88.24 93.60 90.27 90.26 88.80 90. 95.00 100 93.55 91.66 1 fps 64 20 79.02 77.11 80. 80.47 80.47 77.34 83.54 83.91 82.02 79.67 76.47 81.73 80.00 70.19 72.33 84.74 83.80 75.39 77.78 66.67 61. 64.23 62.19 61.79 71.95 69.12 69.32 48.70 49.22 43.09 75.69 73.28 72.44 46.80 41.20 31.60 39.60 37.20 34. 74.90 43.60 32.80 80.00 56.00 48.80 60.22 44.50 36.80 51.41 41.20 38.40 40.73 38.40 34.80 54.80 32.80 34. 45.10 56.86 64.71 48.73 38.70 37.70 Proprietary MLLMs 80.38 75.20 71.93 78.20 68.12 71.12 70.03 53.68 56.40 55.86 25.89 39.07 74.92 80.76 79.13 74.22 82.81 71.09 70.31 60.94 84.38 63.28 49.22 57.81 55. 43.57 40.06 75.53 79.69 81.25 76.03 73.19 77.92 73.82 69.40 70.66 61.20 70.98 65.30 57.41 24.91 34.49 74.10 80.76 78.86 80.72 77.45 75.82 76.80 77.12 73.20 70.92 56.86 62.75 58.17 23.87 31.05 73.08 83.33 80.77 72.67 68.32 64.60 63.35 67.70 67.08 62.73 53.42 64.60 52. 27.33 45.96 74.44 74.84 70.44 71.65 71.03 65.73 69.78 62.93 61.68 59.50 53.89 51.40 43.61 13.08 32.40 59.92 78.82 77.26 Open-Source Video MLLMs 67.59 72.22 70.37 57.41 59.26 56.48 61.11 54.63 42.59 39.81 18.52 31.48 76.14 78.70 77. 65.45 61.19 56.10 56.10 53.25 55.69 53.66 48.78 47.97 42.68 65.72 61.47 62.32 64.31 54.96 62.04 54.67 50.14 49.58 45.61 45.08 46.11 53.37 38.86 56.48 38.86 34.72 17.62 31.61 35.23 Streaming MLLMs 25.20 34.16 62.91 64.23 67.07 23.87 42.49 62.16 68.75 66. 48.70 27.89 45.80 57.98 53.72 71.12 69.04 67.44 66.96 63.72 64.60 59.96 52.32 53.96 49.52 23.23 35.99 67.63 75.28 73.64 40.80 41.20 40.80 37.69 37.60 37.60 39.60 41.60 33.60 30.40 25.91 31.20 35.46 41.60 40.00 37.20 22.00 24.00 24.80 26.40 31.20 32.40 26.40 22.00 32. 24.90 26.51 25.26 29.60 32.80 33.60 32.80 34.00 34.40 37.20 28.80 28.00 28.40 28.40 30.40 25.60 24.10 38.57 34.80 36.80 44.80 43.60 41.20 42.80 42.00 39.20 41.60 36.00 34.80 36.00 28.40 32.00 43.34 52.00 52.00 38.40 34.90 35.00 34.90 35.80 34.20 35.40 33.10 29.70 32. 26.00 28.45 35.66 39.50 40.40 35.60 31.20 34.00 29.20 32.00 32.80 32.80 26.80 27.60 24.80 24.80 24.19 39.62 41.20 38.40 36.00 26.00 31.60 30.40 31.20 26.40 29.60 34.00 24.40 26.80 25.20 29.20 27.65 30.40 26.80 27.27 39.60 41.92 35.35 32.32 33.84 30.30 23.23 16.67 18. 26.80 30.80 34.80 42.80 40.40 29.55 22.73 22.22 18.18 40.91 16.00 15.91 17.65 22.73 0.00 1.96 3.92 25.34 18.80 14.40 32.74 31.66 34.97 30.79 32.42 30.06 29.95 27.35 22.88 21.93 24.12 26.55 33.61 33.30 30.00 67.07 60.15 57. 56.36 54.14 53.85 52.77 51.40 51.10 48.66 43.20 42.53 40.40 24.04 32.48 53.12 58.00 56.56 58.11 Human Gemini 1.5 pro GPT-4o Claude 3.5 Sonnet LLaVA-OneVision Qwen2-VL MiniCPM-V 2.6 LLaVA-NeXT-Video InternVL-V2 Kangaroo LongVA VILA-1.5 Video-CCAM Video-LLaMA - - - - 7B 7B 8B 32B 8B 7B 7B 8B 14B 7B 32 0.2-1 fps 32 64 16 64 128 14 96 32 7B Flash-VStream 8B VideoLLM-online 7B Dispider TimeChat-Online-7B 7B TimeChat-Online-7B 7B - 2 fps 1 fps 1 fps (44.2%) 1 fps (82.6%) TimeChat-Online-7B 7B 1fps (100%) 80.22 82.03 79.50 83. 76.10 78.50 78.70 64.63 69.60 57. 75.36 38.40 26.80 35.60 50.40 37. 39.20 31.60 41.60 28.80 35.30 (a) Feature-level: 𝜏𝑓 𝑒𝑎𝑡 = 0.4, drop ratio = 89.5% (b) Pixel-level: 𝜏𝑝𝑖𝑥𝑒𝑙 = 0.1 Figure 7: Visualization of (a) Feature-level token dropping and (b) Pixel-level token dropping for the video case 671 from StreamingBench. 15 Conference17, July 2017, Washington, DC, USA L. Yao, Y. Li, Y. Wei, L. Li, et al. (a) Scene Transition Point w/ Trigger Time (b) Drop Ratio - Timeline Curve Figure 8: Visualization of monitored Trigger Time via drop ratio curve. The colored highlighted frames correspond to trigger times that reveal video scene transitions. Our model utilizes temporal patch size of 2. (a) Scene Transition Point w/ Trigger Time (b) Drop Ratio - Timeline Curve Figure 9: Visualization of monitored Trigger Time via drop ratio curve. The colored highlighted frames correspond to trigger times that reveal video scene transitions. Our model utilizes temporal patch size of 2. TimeChat-Online: 80% Visual Tokens are Naturally Redundant in Streaming Videos Conference17, July 2017, Washington, DC, USA Figure 10: Case study of TimeChat-Online on StreamingBench with drop ratio curve. When user proposes question What specifically did the woman in red do? that can also be answered by the future moments, TimeChat-Online will proactively generate responses at the future trigger time (i.e., the video scene transition timestamps), which are indicated by the frames with low token drop ratios. (a) Pixel-level: 𝜏𝑝𝑖𝑥𝑒𝑙 = 0.01 (b) Pixel-level: 𝜏𝑝𝑖𝑥𝑒𝑙 = 0.05 (c) Pixel-level: 𝜏𝑝𝑖𝑥𝑒𝑙 = 0. (d) Feature-level: 𝜏𝑓 𝑒𝑎𝑡 = 0.7 (e) Feature-level: 𝜏𝑓 𝑒𝑎𝑡 = 0.6 (f) Feature-level: 𝜏𝑓 𝑒𝑎𝑡 = 0.5 Figure 11: Comprehensive visualization of feature-level and pixel-level token dropping with varying threshold values (𝜏𝑓 𝑒𝑎𝑡 ) for the same video case. 17 Conference17, July 2017, Washington, DC, USA L. Yao, Y. Li, Y. Wei, L. Li, et al. (a) StreamingBench (b) VideoMME Figure 12: Average token drop ratio across different video durations using the same threshold parameter 𝜏. (a) StreamingBench (b) VideoMME Figure 13: Average token drop ratio across different task types using same hyperparameter 𝜏 settings. 18 TimeChat-Online: 80% Visual Tokens are Naturally Redundant in Streaming Videos Conference17, July 2017, Washington, DC, USA GPT-4 Prompt for Detailed Video Frame Caption Generation: You are an advanced AI visual assistant tasked with describing frames extracted from video clip. When provided with frame, describe it in detailed and accurate terms, focusing on both static and dynamic elements visible within, as well as the shot type, object appearances and actions, environment and background variations, and camera movements. While your primary focus should be the current frame, you may reference the provided caption of the preceding frame to describe any relevant relationships between the two frames, particularly regarding camera movements, which often require contrasts between frames. Ensure your description reflects only the contents that can be determined in the current frame without analysis or speculation. Do not include any elements of this prompt in your response. Your caption should be in narrative format, avoiding list-like itemizations. Begin with \"This frame\". Heres an output example: This frame shows an arrangement of six small white bowls placed on dark granite countertop. Each bowl contains different ingredients key to the cooking recipe. Starting from the top left and moving clockwise, the first bowl has white powder which seems like cornstarch. hand is seen above this bowl, with finger pointing towards it, indicating either an instruction or choice selection. The top middle bowl is empty, noticeably different from the rest. The top right bowl is filled with what appears to be brown sugar, granulated and slightly heaped. In the bottom row, the bowl on the left is also empty, mirroring the top middle bowl. The bottom middle bowl contains light yellow-brown liquid, which could be oil or prepared marinade. The bottom right bowl is filled with dark liquid, possibly soy sauce. The lighting in this frame is uniform, ensuring all the details and texture of the ingredients are clearly visible. No background elements distract from the central subject. This static shot is focused on presenting the different ingredients required, providing clear visual reference for the cooking process. The current frame is uploaded as .jpg file, and heres some relevant information: <video_meta_information> <video_name> <video_id> <video_subtitle> <video_category> <video_duration> </video_meta_information> The caption of the preceding frame is provided below: <caption_of_preceding_frame> Table 12: The prompt template used for scene-oriented key frame caption generation. 19 Conference17, July 2017, Washington, DC, USA L. Yao, Y. Li, Y. Wei, L. Li, et al. GPT-4 Prompt for Streaming VideoQA Generation: Given the following video frame descriptions, generate 5 different question-answer pairs that focus on temporal understanding and visual perception. These questions should cover the following cognitive abilities: 1. Episodic Memory: Questions about where objects were located before events or recalling details from earlier frames 2. Action Sequence Identification: Questions about what actions occurred before others or the order of events 3. Hallucination Detection: Questions requiring verification of visual elements that actually appeared in the video 4. Future Prediction: Questions predicting what might happen next based on the video context 5. Sequential Steps Recognition: Questions about step-by-step processes shown in the video 6. Clues Reveal Responding: Questions requiring attention to specific details or clues spread across frames Video Description: {description} Question Requirements: 1. Create multiple-choice questions (A, B, C, format) with one correct answer 2. Include at least one question from each of the six cognitive abilities listed above 3. Ensure questions require temporal understanding across frames 4. Include questions that require connecting information from different parts of the video 5. Create at least 2-3 questions that specifically require comparing frames with long distances between them CRITICAL REQUIREMENTS: 1. NEVER reference specific frame numbers/indices in questions or answers (e.g., DO NOT say \"In Frame 201\" or \"From Frame 10 to Frame 20\") 2. Instead, describe the content and events directly (e.g., \"When the person was in the kitchen\" or \"After picking up the cup\") 3. You MAY reference timestamps if needed (e.g., \"At 1:20 in the video\") 4. Questions should focus on the content and temporal relationships, not on arbitrary frame numbering Target Format: Please format your response as JSON object using the following structure: [ \"question\": <question>, \"options\": <options>, \"answer\": <answer>, \"answer_text\": <answer_text>, \"category\": <category>, \"rationale\": <rationale>, \"rationale_frames\": <rationale_frames> { } ] Ensure that your response includes 5 multiple-choice question-answer pairs in this JSON format, with each pair addressing different cognitive abilities from the six categories listed. **The specific frame numbers (index) or timestamps should NOT appear in the questions or options themselves.** Table 13: The prompt template used for generating streaming question-answer pairs."
        }
    ],
    "affiliations": [
        "Kuaishou Technology",
        "Peking University",
        "South China University of Technology",
        "The University of Hong Kong"
    ]
}