{
    "paper_title": "ComposeAnything: Composite Object Priors for Text-to-Image Generation",
    "authors": [
        "Zeeshan Khan",
        "Shizhe Chen",
        "Cordelia Schmid"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Generating images from text involving complex and novel object arrangements remains a significant challenge for current text-to-image (T2I) models. Although prior layout-based methods improve object arrangements using spatial constraints with 2D layouts, they often struggle to capture 3D positioning and sacrifice quality and coherence. In this work, we introduce ComposeAnything, a novel framework for improving compositional image generation without retraining existing T2I models. Our approach first leverages the chain-of-thought reasoning abilities of LLMs to produce 2.5D semantic layouts from text, consisting of 2D object bounding boxes enriched with depth information and detailed captions. Based on this layout, we generate a spatial and depth aware coarse composite of objects that captures the intended composition, serving as a strong and interpretable prior that replaces stochastic noise initialization in diffusion-based T2I models. This prior guides the denoising process through object prior reinforcement and spatial-controlled denoising, enabling seamless generation of compositional objects and coherent backgrounds, while allowing refinement of inaccurate priors. ComposeAnything outperforms state-of-the-art methods on the T2I-CompBench and NSR-1K benchmarks for prompts with 2D/3D spatial arrangements, high object counts, and surreal compositions. Human evaluations further demonstrate that our model generates high-quality images with compositions that faithfully reflect the text."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 3 ] . [ 1 6 8 0 4 2 . 5 0 5 2 : r ComposeAnything: Composite Object Priors for Text-to-Image Generation Zeeshan Khan Shizhe Chen Cordelia Schmid Inria, École normale supérieure, CNRS, PSL Research University https://zeeshank95.github.io/composeanything/ca.html Figure 1: The proposed ComposeAnything framework enables text-to-image generation for complex compositions involving surreal spatial relationships and high object counts. It enhances both visual quality and faithfulness to the input text compared to diffusion-based models (e.g., SD3 [11], FLUX [3]) and 2D layout conditioned models (e.g., RPG [56] and CreatiLayout [58])."
        },
        {
            "title": "Abstract",
            "content": "Generating images from text involving complex and novel object arrangements remains significant challenge for current text-to-image (T2I) models. Although prior layout-based methods improve object arrangements using spatial constraints with 2D layouts, they often struggle to capture 3D positioning and sacrifice quality and coherence. In this work, we introduce ComposeAnything, novel framework for improving compositional image generation without retraining existing T2I models. Our approach first leverages the chain-of-thought reasoning abilities of LLMs to produce 2.5D semantic layouts from text, consisting of 2D object bounding boxes enriched with depth information and detailed captions. Based on this layout, we generate spatial and depth aware coarse composite of objects that captures the intended composition, serving as strong and interpretable prior that replaces stochastic noise initialization in diffusion-based T2I models. This prior guides the denoising process through object prior reinforcement and spatial-controlled denoising, enabling seamless generation of compositional objects and coherent backgrounds, while allowing refinement of inaccurate priors. ComposeAnything outperforms state-of-the-art methods on the T2I-CompBench and NSR-1K benchmarks for prompts with 2D/3D spatial arrangements, high object counts, and surreal compositions. Human evaluations further demonstrate that our model generates high-quality images with compositions that faithfully reflect the text. Preprint. Under review."
        },
        {
            "title": "Introduction",
            "content": "Text-to-image (T2I) models, particularly diffusion-based ones such as SDXL [45], SD3 [11] and Flux [3], have achieved remarkable success in generating individual concepts with high fidelity. However, they struggle with complex object compositions [21], especially novel arrangements that deviate from their training distribution, often resulting in unnatural mixing of objects, incorrect 2D/3D spatial positioning, and inaccurate object counts, as shown in Figure 1. To improve compositional T2I generation, layout control has emerged as key strategy [13, 29, 41]. This approach leverages various 2D layout representations, such as bounding boxes [29] and blobs [41], typically generated by large language models (LLMs) [42] from textual prompts. Layoutcontrolled T2I methods can be broadly categorized into training-based and training-free approaches. Training-based methods [58, 29, 59, 52] incorporate layout conditioning by training adapter modules over pretrained T2I models. While they provide strong spatial control, they require extensive training and often degrade image coherence and quality [60] due to rigid layout constraints, as seen with CreatiLayout [58] in Figure 1. In contrast, training-free methods mainly apply inference-time layout control [56] or latent optimization [10], to guide spatial aware generation, such as manipulating regionto-text cross-attention maps [4, 10] or employing region-wise denoising [56]. These methods better preserve image quality but offer weaker control, making it difficult to follow complex compositional instructions such as RPG [56] in Figure 1. Furthermore, all existing training-free methods rely solely on coarse 2D layouts, which not only lack 3D spatial relationships but also fail to visually represent object appearance, limiting their effectiveness in guiding T2I generation. To address these limitations, we propose ComposeAnything, novel inference-time T2I framework that enhances compositional generation without requiring model retraining. Our approach leverages LLMs with chain-of-thought reasoning to decompose the input text into 2.5D semantic layout comprising objects and background. Each object is represented by caption, 2D bounding box, and depth value to reflect relative 3D spatial relations. We then synthesize coarse image by generating individual object images from their captions with existing T2I models [11] and arranging them spatially according to this layout. This results in composite object prior that captures object appearance, count, size and 2.5D spatial positioning. To effectively utilize it we replace random noise initialization with noisy object prior and propose prior-guided diffusion module that combines object prior reinforcement and spatial-controlled denoising. The prior reinforcement preserves the influence of foreground object priors in early diffusion steps, while automatically generating background to enable coherent generation. The spatial-controlled denoising further strengthens the spatial arrangement of the composite prior via mask-guided attention in early diffusion steps where global structure is determined. After these initial steps, we switch to standard diffusion to refine the image quality and coherence, achieving both faithful composition and high visual fidelity. ComposeAnything outperforms state-of-the-art methods on two challenging compositional T2I benchmarks: T2I-CompBench [21] and NSR-1K [13], under automatic metrics. It achieves both high image quality and strong faithfulness to input text, as further validated by human evaluations. Ablation studies highlight the effectiveness of the composite object prior and prior-guided diffusion. To summarize, our contributions are as follows: (i) We introduce ComposeAnything, training-free framework that enhances the compositional capabilities of existing diffusion-based T2I models by generating structured 2.5D semantic layouts and composite object priors from text using LLMs with chain-of-thought reasoning. (ii) We propose prior-guided diffusion method that integrates composite object priors into the denoising process via object prior reinforcement and spatially-controlled denoising in initial diffusion steps to balance faithful object compositions and high-quality image synthesis. (iii) ComposeAnything enables interpretable and robust image generation, and outperforms the state of the art on two compositional T2I benchmarks, especially for surreal compositions. We will publicly release our code."
        },
        {
            "title": "2 Related Works",
            "content": "Compositional generation aims to produce images that are faithfully aligned with complex texts [21, 60, 22, 25, 52, 13, 56, 53, 9, 31]. While diffusion models [45, 11, 3] have demonstrated strong 2 Figure 2: The ComposeAnything framework, which enhances text-to-image diffusion models e.g. SD3-M [11] with layouts and composite object priors for complex compositional generation. generative capabilities, they still struggle with compositional generation. To address this, prior work in this area can be broadly categorized into two main directions. Training-based methods fine-tune pretrained diffusion models [45, 11, 3, 5, 6, 43] on large-scale datasets to improve semantic alignments [7, 57]. Some approaches introduce additional training objectives, such as grounding loss [53] or text-to-image alignment [23], to strengthen compositional accuracy, while others enrich semantic features by using LLM-generated information [19]. To incorporate explicit spatial control, training-based layout-controlled methods train adapter modules to inject new conditioning for spatial control [29, 59, 52, 14, 57, 58, 32, 61, 62, 15, 40, 27] such as bounding boxes, segmentation masks, and keypoints. Despite their effectiveness, these methods face two key limitations: a) large-scale training is required to learn object compositions comprehensively; and b) hard conditioning often degrades image quality as indicated in [60]. Training-free methods provide an alternative by manipulating diffusion models at inference time, without requiring retraining. Inspired by text-based image editing methods [18, 50, 51] which manipulate attention maps for fine-grained region-level control, many of training-free approaches operate by adjusting text embeddings or cross-attention activations to influence object placement and composition [4, 12, 39, 49, 30, 46, 34, 1, 16]. Within this category, training-free layout-controlled methods steer generation using layout guidance without additional training. These methods often use LLMs [42] to generate 2D layouts of object bounding boxes, and then modulate cross-attention to emphasize regions corresponding to input boxes [60, 55, 10, 24, 36, 9, 8, 22, 44]. RPG [56] and MuLan [28] introduce region-wise diffusion, which decomposes the image latent into spatial regions and performs localized denoising. However, controlling cross-attention alone is often insufficient for handling complex multi-object compositions. Another class of training-free approaches focuses on initial noise search and optimization, motivated by the observation that the initial noise can significantly influence the generation outcome. Several works have proposed inference-time noise search and optimization [35, 17], which involve generating multiple candidates and selecting the best one using heuristics based on attention dynamics [17] or external verifiers [35]. While promising, these methods tend to be computationally expensive and unreliable when generating highly complex or out-of-distribution compositions. To overcome these limitations, we propose to directly generate noise in the form of composite object priors coarse RGB images representing the scene layout, semantics and appearance. This is inspired by image editing methods [38, 2, 37] that use noisy initialization and image inversion. However, we generate object priors based on LLMs chain-of-thought reasoning for noise initialization and propose novel prior-guided diffusion method in the image generation process."
        },
        {
            "title": "3 Proposed Method",
            "content": "As illustrated in Figure 2, our ComposeAnything framework consists of three key components for compositional text-to-image generation: 1) LLM Planning (Section 3.1): We employ LLMs to transform the input prompt into structured 2.5D semantic layout, including object captions, bounding boxes and relative depths; 2) Composite Object Prior (Section 3.2): Based on the layout, we generate coarse composite image that serves as strong semantic and spatial prior for guiding image synthesis; and 3) Prior Guided Diffusion (Section 3.3): We iteratively initialize noises with the object prior and apply spatially-controlled self-attention to preserve structure in early denoising steps. 3 3.1 LLM Planning Recent advancements in LLMs have demonstrated their effectiveness in generating high-quality scene layouts from textual descriptions [13, 56, 20]. Hence, we harness GPT-4.1 [42] to produce structured 2.5D semantic layout from the original text. The layout includes the following elements: Object captions {yoi}K i=1 that describe size, orientation and appearance for each identified object; Bounding boxes {boxi}K i=1 that specify 2D spatial configuration for each object; Depth values {depthi}K i=1 that reflect relative depth orders for each object to support 3D-aware composition; Background caption ybg describing the background scene; and Compositional caption ybase which is concise summary of the entire image. This process involves several key steps for chain-of-thought reasoning, as illustrated in Figure 3. More details are provided in section of the appendix. Figure 3: Chain-of-thought LLM planning for generating 2.5D semantic layouts from text. 3.2 Composite Object Prior 2.5D position-aware composite image generation. Given the isolated object captions from LLM, we first generate individual objects using Stable Diffusion-3 Medium (SD3-M) [11]. Next, we use referring expression segmentation model Hyperseg [54] to extract objects {oi}K i=1 along with their segmentation masks {mi}K i=1. Each object and its corresponding mask are resized to fit within the designated bounding box generated from the LLM according to scaling factor scalei. Objects are then composited in depth-aware order, where objects with smaller depth values are placed above those with larger depth values, thereby establishing occlusion-correct layering in the final scene. This process is formulated as follows: i, op, mp = Compose({o = Resize(oi, mi, scalei), i}, {m (2) Finally, all objects are composited on sized canvas, denoted as op. Its corresponding composited mask is denoted as mp. Figure 2 shows an example of the composite image and mask. The composition of all objects forms the foreground, and the rest is considered the background. i}, {boxi}, {depthi}). (1) Initializing object prior for diffusion-based models. Our work builds upon existing T2I diffusion models, aiming to enhance its ability to generate images with complex object compositions. Our method is compatible with both denoising diffusion probabilistic models like SDXL [45] and recent flow-matching based models like SD3-M [11]. The core idea of diffusion models is to learn generative process by simulating and then reversing gradual noising procedure. Given an image x0 from the real data distribution p(x), the forward process transforms x0 into xT (0, I) through predefined noise schedule: xt = α(t)x0 + σ(t)z, (0, I), (3) where [0, ] indexes the diffusion timestep. denoising network ϵ(θ) is trained to predict the added noise at each step in the forward process. During inference, image generation starts from pure Gaussian noise xT and denoises it back to x0 via the reverse process, which is an ordinary differential model (ODE) on time [T, 0] guided by the noise prediction network ϵ(θ): xtt xt ϵ(θ)(xt, t) t. (4) Our method is inspired by the fact that the reverse ODE can be solved from any (0, ) [38]. Instead of starting from pure Gaussian noise at = , we initialize the process with noisy object prior at an intermediate timestep tp < , providing stronger starting point for generation. 4 Specifically, we follow latent diffusion models [47, 45, 11, 3] where the denoising is applied on the latent space. We use the above composite image op to generate an initial noise in the latent space. The image op is first encoded through Variational Autoencoder (VAE) to get the prior latent. Then, we apply the forward process from Eq (3) at high noise timestep tp to get the latent object prior: zop = VAE(op), ˆzop tp = α(tp)zop + σ(tp)z, (0, I). (5) (6) Since the background in op is empty, we avoid conditioning the generation process on the uninformative background region of the latent ˆzop tp . To achieve this, we use the mask mp to reinitialize the background with pure Gaussian noise: tp = ˆzop zop tp mp + zbg (1 mp), (7) where zbg (0, I) and indicates element multiplication. This ensures that only the object regions are guided by prior, while the background remains free to be generated based on the caption. The reverse process still starts from = , but uses the composite object prior zop tp as initialization. 3.3 Prior-guided Diffusion We propose two mechanisms to incorporate the guidance from the composite object prior in the denoising process. Figure 4 illustrates the prior-guided diffusion method. Figure 4: Overview of prior-guided diffusion. The spatialcontrolled denoising is applied for each aligned text and region pair to strengthen spatial control, and we further re-inject the object prior zop tp into predicted zt1 to reinforce the prior. Object prior reinforcement. To prevent excessive corruption of the foreground object prior, we initialized the foreground time tp, while with noise at the background is still initialized with pure Gaussian noise at . However, during denoising from = , this mismatch in noise levels leads to inaccurate noise predictions for the foreground region, which potentially distort its semantics and structure. To address this, we propose novel foreground prior reinforcement algorithm. During the denoising steps from to tp, we repeatedly restore the original object prior in the foreground regions to protect them from degradation. Specifically, we overwrite the foreground region in the current latent zt1 with the initial object prior, while retaining the denoised background: zt1 zop tp mp + zt1 (1 mp). (8) This iterative replacement ensures that the semantic integrity and spatial structure of the object prior are preserved throughout the early diffusion steps. At the same time, the background is progressively refined in the presence of fixed foreground, allowing for coherent integration between the two. Once the latent reaches time tp, both foreground and background are aligned in terms of noise level and the global structure becomes stable. From this point onward, denoising proceeds without any additional intervention, allowing for natural refinement and generative flexibility. Notably, decreasing tp strengthens the object prior while reducing generative flexibility. Spatial-controlled denoising. To further enhance object-level spatial control in T2I generation, we propose spatial-controlled attention mechanism that explicitly strengthens the alignment between between specific image regions and their corresponding region textual descriptions. Our method builds on Multi-Modal Diffusion Transformers, dual-stream architecture used in Stable Diffusion 3 [11], which processes text and image modalities in parallel. In addition to the base prompt embeddings ybase, we introduce set of object prompt embeddings {yoi}K i=1 and one background prompt embedding ybg. These are independently processed by the text stream, while the image stream receives only the latent image embeddings. 5 Table 1: State-of-the-art comparison on the T2I-CompBench and NSR-1k benchmarks. NSR-1K T2I-CompBench 2D-Spatial Count 3D-Spatial Complex Spatial Count"
        },
        {
            "title": "Method",
            "content": "LayoutGPT [13] CreatiLayout[58]"
        },
        {
            "title": "45.81\nSD-v1\nSD3-M 47.36",
            "content": "SD-v1 [47] 12.46 SD-v1 Attend-Excite v2 [4] 14.55 SD-v2 SDXL [45] 21.33 SDXL RealCompo [60] SDXL 31.73 SD3-M [11] SD3-M 31.32 FLUX-Schnell [3] 26.13 FLUX RPG1 [56] 40.26 SDXL 31.51 Inference-scale [35] FLUX ComposeAnything (Ours) SD3-M 48.24 60.27 62.15 44.61 47.67 49.88 65.92 60.22 60.58 56.39 67.89 68.21 68.85 47.12 49.43 59.51 50.93 77.16 34. 30.80 34.01 32.37 37.71 37.03 36.53 38.10 38.66 60.6 59.8 16.89 26.86 31.57 - 44.43 39.29 52.24 - 63.80 55.6 63.4 31.45 39.41 30.62 - 44.61 55.97 39.81 - 59.36 During the self-attention, the image latent zt is split into two latents: 1) base latent zbase object-background latent zob zob , and 2) an i=1 and background mask mbg, we segment into separate objects and background latents: . Given object masks {mi}K {zoi }K {zbg i=1 = Segment(zob } = Segment(zob , {mi}K , mbg). i=1), (9) (10) Each object latent zoi through Joint Self-Attention (JSA) module: and its corresponding prompt embedding yoi are concatenated and passed = [(Wqy yoi qi = [(Wky yoi ki = [(Wvy yoi vi ); (Wqz zoi )], ); (Wkz zoi )], ); (Wvz zoi )], t)(ki (qi t) ) vi t, [yoi , zoi ] Softmax( (11) (12) (13) (14) where Wqy, Wky, Wvy are linear projects for the prompt embeddings, and Wqz, Wkz, Wvz for the image latent. For simplicity, we reuse the same notation for the input and output of the transformer layer. This spatial-controlled self-attention is applied at each transformer layer, enabling precise control over object placement and appearance while preserving global visual consistency. The same mechanism is applied to the background: [ybg ). The original base attention is , zbase applied on the base prompt and the base latent embeddings [ybase t ] JSA(ybg ] JSA(ybase , zbg , zbg , zbase ). After the last transformer layer, the object and background latents are denoised from 1. The updated object latents {zoi t1 using the segmentation masks. t1 are then composed back into zob i=1 and background latent zbg t1}K Finally, we merge the base latent and object-background latent with weighted sum: zt1 = zbase t1 ratiobase + zob t1 (1 ratiobase). (15) It balances global coherence from the base latent and fine-grained spatial control from the objectbackground latent. We apply the spatial control for the initial Nsc denoising steps."
        },
        {
            "title": "4 Experiments",
            "content": "4.1 Experimental Setup 1We run RPG using the released code and report the reproduced results on all categories of the two benchmarks. 6 Figure 5: State-of-the-art comparison against SD3-M [11], FLUX [3], RPG [56], and Creatilayout [58], on complex surreal prompts. Evaluation benchmarks. We evaluate our method on T2I-CompBench [21] and NSR-1k [13] datasets. They contain prompts rich in spatial, 3D, numeric and generally complex and often surreal compositions. We evaluate our method on four categories from T2I-CompBench: 2D Spatial, Numeracy (Count), 3D Spatial, and Complex, each containing 300 prompts. For NSR-1K, we report results on the Spatial (283 prompts) and Count (672 prompts) categories. Evaluation metrics. For the 2D-spatial and numeracy categories, we follow the standard evaluation protocols from T2I-CompBench and NSR-1k. Object detectors UniDet [63] for T2I-CompBench and GLIP [26] for NSR-1k are first used to identify objects in the generated images. The metric penalizes missing objects, incorrect counts, and spatial errors, with the latter measured by the center-point distance for each specified 2D relation. For the 3D-spatial category, the original T2I-CompBench metric relies on depth estimation and bounding box detection, which we found inaccurate and overly punitive. To address this, we introduce an MLLM-based metric using GPT4.1 [42]. The model is prompted to first identify all required objects and then assess their 3D spatial relations. Scores are assigned as follows: 0 if objects are missing or 3D relations are wrong, 1 if all objects are present but the 3D relations are ambiguous, and 2 if everything is correct. We normalize the total score to 0100 scale and average over all examples. Full details are provided in section of the appendix. For the complex category, we adopt the 3-in-1 metric from T2I-CompBench, which averages the CLIP similarity score, spatial accuracy (via object detection), and BLIP-VQA accuracy. This composite score better aligns with human judgment. Implementation details. We use GPT-4.1 [42] for LLM planning and SD3-Medium (SD3-M) [11] as the base diffusion model with Flow matching Euler discrete scheduler. We fix total 28 steps for denoising. We also perform experiments with SDXL as the base model, in section of the appendix. The generation process is controlled by two key hyper-parameters: (1) tp The time at which noise is sampled and applied to the prior image in the forward diffusion. As tp goes from (T to 0), prior strength increases, which increases faithfulness while reducing generative flexibility. (2) Nsc The number of steps for spatially controlled denoising. higher value enforces stronger spatial control. 7 The two hyper-parameters enable highly controllable generation and can be tuned to balance the composition performance and image quality, as demonstrated in section of the appendix. For the experiments in this section, we sample tp corresponding to high noise of 91.3% from the Flow matching schedule and set Nsc = 3 steps. 4.2 Comparison with State-of-the-Art Methods Compared methods. We compare our method against both training-based and inference-only approaches. Training-based methods include layout-to-image models with box conditioning such as Gligen [29] and CreatiLayout [58]. The inference-only methods include general pretrained T2I models (SDv1 [47], SDXL [45], SD3-M [11], and FLUX [3]), layout-guided training-free approaches (RPG [56] and RealCompo [60]), and noise search method inference time scaling [35]. Quantitative results. Table 1 presents the evaluation results on the T2I-CompBench and NSR-1K benchmarks. Our method outperforms all prior approaches across all categories on T2I-CompBench by significant margin. Compared to the base model SD3-M, ComposeAnything achieves absolute gains of 16.9% on 2D-Spatial, 7.9% on Count, 27.7% on 3D-Spatial, and 0.9% on Complex. On NSR-1K, we also outperform SD3-M with improvements of absolute 19.0% and 14.7% on the Spatial and Count categories, respectively. Our method surpasses all state-of-the-art methods except being slightly worse than CreatiLayout [58] in the Count category. Qualitative results. Figure 5 compares our method against state-of-the-art approaches on challenging compositional prompts. Our method consistently adheres to the input prompt, generating coherent and realistic images. In contrast, models such as SD3-M [11], FLUX [3], and RPG [56] often fail to follow the prompt. For instance, all three baselines struggle with placing chicken on balloon and frequently fail to maintain the correct object count. As object count increases, SD3-M and FLUX tend to cartoonify\" the output, sacrificing realism. CreatiLayout [58], fine-tuned layout-conditioned model, performs slightly better in maintaining spatial relations when provided with bounding box inputs. However, as discussed in RealCompo [60], hard box-conditioning methods, while strong in compositional accuracy, often compromise image quality. These methods are inherently limited by the spatial distributions seen during training and struggle to generalize to unusual or surreal layouts. In contrast, by leveraging object priors and integrating them into the denoising process, our method achieves better balance between compositional fidelity and visual quality. Human evaluations. To further assess generation quality, we conduct human evaluations on three categories in T2I-CompBench: 2D-, 3DSpatial and Count. We compare ComposeAnything against two state-of-theart methods: RPG [56] (inferenceonly) and CreatiLayout [58] (trainingbased). For each category, we randomly sample 30 prompts and perform pairwise comparisons, resulting in 180 unique image comparisons (30 prompts 3 categories 2 baselines). Human raters were instructed to evaluate based on both prompt alignment and image quality (see appendix for details). Five raters participated in the evaluation, with 30% of images overlapping across raters to measure inter-annotator agreement. The average agreement scores were around 80% for all categories. As shown in Figure 6, our method significantly outperforms both methods across the three categories. Figure 6: Human evaluations against RPG and CreatiLayout. 4.3 Ablations Object prior quality. The correctness of object priors is crucial for high-quality image generation. We use the same metrics for evaluating the final image to evaluate the generated composite object prior. As shown in Table 2, the performance of the object priors is closely correlated with that of the final generated images. Our method achieves state-of-the-art results even at the prior Table 2: Performance of Object Priors (OP) and the corresponding Final Image (FI) on T2I-Compbench. Spatial Numeracy 3D Complex OP 45.79 48.24 FI 68.06 68.21 86.71 77.16 35.04 38. 8 Figure 7: LLM generated object prior and their corresponding final image generation. stage, demonstrating the effectiveness and robustness of the LLM-driven pipeline for automatic prior generation. Notably, our prior-guided diffusion is applied only during the initial denoising steps. This design preserves the structural benefits of the prior while allowing subsequent steps to introduce generative flexibility, which helps correct inaccuracies in the initial composite. As illustrated in Figure 7(b), the final images can resolve issues present in the priors such as missing elements, incorrect orientation, size, and overall incoherence of the composite. This improvement is also reflected in the quantitative results in Table 2, where the final generations outperform the priors across all categories except in 3D. Although the standard diffusion step improves image quality and coherence, it sometimes compromises the correctness of 3D spatial relationships for difficult compositions, due to lack of explicit 3D guidance in the diffusion process, as seen in Figure 7(c). However composition correctness and image quality/coherence can be easily balanced by tuning the hyper-parameters as shown in section of the appendix. More detailed human evaluations on the quality of prior image and the corresponding generation is presented in section of the appendix. Impact of object prior reinforcement and Prior Reinforce Spatial Control Spatial Numeracy Table 3: spatial-controlled denoising on T2I-CompBench. Prior-guided diffusion. Table 3 shows the contribution of object prior reinforcement and spatial-controlled denoising in the prior-guided diffusion process. Each component significantly enhances performance over the base SD3-M model [11]. Specifically, the object prior reinforcement yields absolute gains of over 14 and 6% in spatial and numeric categories, respectively, while spatial-controlled denoising improves performance by over absolute 12 and 4%. When combined, these components further boost results, demonstrating their complementary roles in achieving precise and controlled text-to-image generation. 60.22 66.08 64.42 68.21 31.32 45.33 43.56 48."
        },
        {
            "title": "5 Conclusion",
            "content": "In this work, we introduce ComposeAnything, novel inference-time framework for compositional text-to-image generation that leverages object-level guidance derived from LLM-generated 2.5D semantic layouts. By introducing composite object prior for structured initialization and prior-guided diffusion, our approach enables precise object placement and robust semantic grounding without any additional training. ComposeAnything achieves state-of-the-art performance on T2I-CompBench and NSR-1K, effectively balancing image quality and prompt fidelity even under complex or surreal scenarios. Our results highlight the potential of LLM-driven reasoning and composite prior guidance in advancing compositional T2I generation. Limitations. The main failure mode of our approach are errors of the LLM for spatial layout generation, as shown in Table 2. While our approach is robust to some of these failures (Fig. 7(b)), completely incorrect object prior can not be recovered (Fig. 7(d)). Future work could improve the 9 layout generation with LLMs by fine-tuning the models for this task or by adding explicit 3D scene information. In very rare cases, we observe failure even though the object prior is correct ((Fig. 7(c)). This is due to missing 3D knowledge in the diffusion model and requires further improvement of the base model."
        },
        {
            "title": "References",
            "content": "[1] Aishwarya Agarwal, Srikrishna Karanam, Joseph, Apoorv Saxena, Koustava Goswami, and Balaji Vasan Srinivasan. A-star: Test-time attention segregation and retention for text-to-image synthesis. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 22832293, October 2023. [2] Omri Avrahami, Dani Lischinski, and Ohad Fried. Blended diffusion for text-driven editing of natural images. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022. [3] Black Forest Labs. FLUX. https://blackforestlabs.ai, 2024. [4] Hila Chefer, Yuval Alaluf, Yael Vinker, Lior Wolf, and Daniel Cohen-Or. Attend-and-excite: Attentionbased semantic guidance for text-to-image diffusion models. ACM Trans. Graph., 2023. [5] Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, and Zhenguo Li. Pixart-α: Fast training of diffusion transformer for photorealistic text-to-image synthesis, 2023. URL https://arxiv.org/abs/2310.00426. [6] Junsong Chen, Yue Wu, Simian Luo, Enze Xie, Sayak Paul, Ping Luo, Hang Zhao, and Zhenguo Li. Pixart-δ: Fast and controllable image generation with latent consistency models, 2024. URL https: //arxiv.org/abs/2401.05252. [7] Kai Chen, Enze Xie, Zhe Chen, Yibo Wang, Lanqing HONG, Zhenguo Li, and Dit-Yan Yeung. Geodiffusion: Text-prompted geometric control for object detection data generation. In The Twelfth International Conference on Learning Representations, 2024. [8] Minghao Chen, Iro Laina, and Andrea Vedaldi. Training-free layout control with cross-attention guidance. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), 2024. [9] Guillaume Couairon, Marlène Careil, Matthieu Cord, Stéphane Lathuilière, and Jakob Verbeek. Zeroshot spatial layout conditioning for text-to-image diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2023. [10] Omer Dahary, Or Patashnik, Kfir Aberman, and Daniel Cohen-Or. Be yourself: Bounded attention for multi-subject text-to-image generation. In European Conference on Computer Vision (ECCV), 2024. [11] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, Dustin Podell, Tim Dockhorn, Zion English, and Robin In International Rombach. Scaling rectified flow transformers for high-resolution image synthesis. Conference on Machine Learning (ICML), 2024. [12] Weixi Feng, Xuehai He, Tsu-Jui Fu, Varun Jampani, Arjun Reddy Akula, Pradyumna Narayana, Sugato Basu, Xin Eric Wang, and William Yang Wang. Training-free structured diffusion guidance for compositional text-to-image synthesis. In The Eleventh International Conference on Learning Representations, 2023. [13] Weixi Feng, Wanrong Zhu, Tsu-Jui Fu, Varun Jampani, Arjun Reddy Akula, Xuehai He, Basu, Xin Eric Wang, and William Yang Wang. LayoutGPT: Compositional visual planning and generation with large language models. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. [14] Yutong Feng, Biao Gong, Di Chen, Yujun Shen, Yu Liu, and Jingren Zhou. Ranni: Taming text-to-image diffusion for accurate instruction following. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024. [15] Hanan Gani, Shariq Farooq Bhat, Muzammal Naseer, Salman Khan, and Peter Wonka. LLM blueprint: Enabling text-to-image generation with complex and detailed prompts. In The Twelfth International Conference on Learning Representations, 2024. [16] Biao Gong, Siteng Huang, Yutong Feng, Shiwei Zhang, Yuyuan Li, and Yu Liu. Check locate rectify: training-free layout calibration system for text-to-image generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 66246634, June 2024. [17] Xiefan Guo, Jinlin Liu, Miaomiao Cui, Jiankai Li, Hongyu Yang, and Di Huang. Initno: Boosting text-toimage diffusion models via initial noise optimization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024. [18] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-or. Prompt-toprompt image editing with cross-attention control. In The Eleventh International Conference on Learning Representations, 2023. [19] Xiwei Hu, Rui Wang, Yixiao Fang, Bin Fu, Pei Cheng, and Gang Yu. Ella: Equip diffusion models with llm for enhanced semantic alignment. CoRR, 2024. [20] Ziniu Hu, Ahmet Iscen, Aashi Jain, Thomas Kipf, Yisong Yue, David Ross, Cordelia Schmid, and Alireza Fathi. Scenecraft: An LLM agent for synthesizing 3D scenes as blender code. In Forty-first International Conference on Machine Learning, 2024. [21] Kaiyi Huang, Kaiyue Sun, Enze Xie, Zhenguo Li, and Xihui Liu. T2I-compBench: comprehensive benchmark for open-world compositional text-to-image generation. Advances in Neural Information Processing Systems, 36:7872378747, 2023. [22] Vikram Jamwal and Ramaneswaran S. Composite diffusion: whole >= sparts. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), 2024. [23] Dongzhi Jiang, Guanglu Song, Xiaoshi Wu, Renrui Zhang, Dazhong Shen, Zhuofan Zong, Yu Liu, and Hongsheng Li. Comat: Aligning text-to-image diffusion model with image-to-text concept matching. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. [24] Yunji Kim, Jiyoung Lee, Jin-Hwa Kim, Jung-Woo Ha, and Jun-Yan Zhu. Dense text-to-image generation with attention modulation. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2023. [25] Baiqi Li, Zhiqiu Lin, Deepak Pathak, Jiayao Li, Yixin Fei, Kewen Wu, Xide Xia, Pengchuan Zhang, Graham Neubig, and Deva Ramanan. Evaluating and improving compositional text-to-visual generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, 2024. [26] Liunian Harold Li, Pengchuan Zhang, Haotian Zhang, Jianwei Yang, Chunyuan Li, Yiwu Zhong, Lijuan Wang, Lu Yuan, Lei Zhang, Jenq-Neng Hwang, Kai-Wei Chang, and Jianfeng Gao. Grounded languageIn Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern image pre-training. Recognition (CVPR), 2022. [27] Ming Li, Taojiannan Yang, Huafeng Kuang, Jie Wu, Zhaoning Wang, Xuefeng Xiao, and Chen Chen. Controlnet ++: Improving conditional controls with efficient consistency feedback. In European Conference on Computer Vision (ECCV), 2024. [28] Sen Li, Ruochen Wang, Cho-Jui Hsieh, Minhao Cheng, and Tianyi Zhou. Mulan: Multimodal-llm agent for progressive multi-object diffusion. arXiv preprint arXiv:2402.12741, 2024. [29] Yuheng Li, Haotian Liu, Qingyang Wu, Fangzhou Mu, Jianwei Yang, Jianfeng Gao, Chunyuan Li, and Yong Jae Lee. Gligen: Open-set grounded text-to-image generation. In Conference on Computer Vision and Pattern Recognition (CVPR), 2023. [30] Yumeng Li, Margret Keuper, Dan Zhang, and Anna Khoreva. Divide & bind your attention for improved generative semantic nursing. In BMVC, 2023. [31] Long Lian, Boyi Li, Adam Yala, and Trevor Darrell. LLM-grounded diffusion: Enhancing prompt understanding of text-to-image diffusion models with large language models. Transactions on Machine Learning Research, 2024. ISSN 2835-8856. [32] Han Lin, Jaemin Cho, Abhay Zala, and Mohit Bansal. Ctrl-adapter: An efficient and versatile framework for adapting diverse controls to any diffusion model. In The Thirteenth International Conference on Learning Representations, 2025. [33] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. [34] Nan Liu, Shuang Li, Yilun Du, Antonio Torralba, and Joshua B. Tenenbaum. Compositional visual generation with composable diffusion models. In European Conference on Computer Vision (ECCV), 2022. [35] Nanye Ma, Shangyuan Tong, Haolin Jia, Hexiang Hu, Yu-Chuan Su, Mingda Zhang, Xuan Yang, Yandong Li, Tommi Jaakkola, Xuhui Jia, and Saining Xie. Inference-time scaling for diffusion models beyond scaling denoising steps. arXiv preprint arXiv:2501.09732, 2025. [36] Wan-Duo Kurt Ma, Avisek Lahiri, J.P. Lewis, Thomas Leung, and W. Bastiaan Kleijn. Directed diffusion: direct control of object placement through attention guidance. In Association for the Advancement of Artificial Intelligence (AAAI), 2024. [37] Jiafeng Mao, Xueting Wang, and Kiyoharu Aizawa. Guided image synthesis via initial image editing in diffusion model. In Proceedings of the 31st ACM International Conference on Multimedia, 2023. [38] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. In International SDEdit: Guided image synthesis and editing with stochastic differential equations. Conference on Learning Representations (ICLR), 2022. [39] Tuna Han Salih Meral, Enis Simsar, Federico Tombari, and Pinar Yanardag. Conform: Contrast is all you need for high-fidelity text-to-image diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024. [40] Chong Mou, Xintao Wang, Liangbin Xie, Yanze Wu, Jian Zhang, Zhongang Qi, and Ying Shan. T2iadapter: learning adapters to dig out more controllable ability for text-to-image diffusion models. In Proceedings of the Thirty-Eighth AAAI Conference on Artificial Intelligence, 2024. [41] Weili Nie, Sifei Liu, Morteza Mardani, Chao Liu, Benjamin Eckart, and Arash Vahdat. Compositional text-to-image generation with dense blob representations. In Forty-first International Conference on Machine Learning, 2024. [42] OpenAI. GPT-4.1. https://openai.com/index/gpt-4-1/, 2025. [43] Pablo Pernias, Dominic Rampas, Mats L. Richter, Christopher J. Pal, and Marc Aubreville. Wuerstchen: An efficient architecture for large-scale text-to-image diffusion models, 2023. URL https://arxiv. org/abs/2306.00637. [44] Quynh Phung, Songwei Ge, and Jia-Bin Huang. Grounded text-to-image synthesis with attention refocusing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024. [45] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis, 2023. URL https://arxiv.org/abs/2307.01952. [46] Royi Rassin, Eran Hirsch, Daniel Glickman, Shauli Ravfogel, Yoav Goldberg, and Gal Chechik. Linguistic binding in diffusion models: Enhancing attribute correspondence through attention map alignment. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. [47] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In Conference on Computer Vision and Pattern Recognition (CVPR), 2022. [48] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In International Conference on Learning Representations (ICLR), 2021. [49] Maria Mihaela Trusca, Wolf Nuyts, Jonathan Thomm, Robert Honig, Thomas Hofmann, Tinne Tuytelaars, and Marie-Francine Moens. Object-attribute binding in text-to-image generation: Evaluation and control. arXiv preprint arXiv:2404.13766, 2024. [50] Narek Tumanyan, Michal Geyer, Shai Bagon, and Tali Dekel. Plug-and-play diffusion features for textdriven image-to-image translation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023. [51] Kai Wang, Fei Yang, Shiqi Yang, Muhammad Atif Butt, and Joost van de Weijer. Dynamic prompt learning: Addressing cross-attention leakage for text-based image editing. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. [52] Xudong Wang, Trevor Darrell, Sai Saketh Rambhatla, Rohit Girdhar, and Ishan Misra. Instancediffusion: Instance-level control for image generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024. [53] Zirui Wang, Zhizhou Sha, Zheng Ding, Yilin Wang, and Zhuowen Tu. Tokencompose: Text-to-image diffusion with token-level supervision. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024. 12 [54] Cong Wei, Yujie Zhong, Haoxian Tan, Yong Liu, Zheng Zhao, Jie Hu, and Yujiu Yang. Hyperseg: Towards universal visual segmentation with large language model, 2024. URL https://arxiv.org/abs/2411. 17606. [55] Jinheng Xie, Yuexiang Li, Yawen Huang, Haozhe Liu, Wentian Zhang, Yefeng Zheng, and Mike Zheng Shou. Boxdiff: Text-to-image synthesis with training-free box-constrained diffusion. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2023. [56] Ling Yang, Zhaochen Yu, Chenlin Meng, Minkai Xu, Stefano Ermon, and Bin CUI. Mastering textto-image diffusion: Recaptioning, planning, and generating with multimodal LLMs. In International Conference on Machine Learning (ICML), 2024. [57] Zhengyuan Yang, Jianfeng Wang, Zhe Gan, Linjie Li, Kevin Lin, Chenfei Wu, Nan Duan, Zicheng Liu, Ce Liu, Michael Zeng, and Lijuan Wang. Reco: Region-controlled text-to-image generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023. [58] Hui Zhang, Dexiang Hong, Tingwei Gao, Yitong Wang, Jie Shao, Xinglong Wu, Zuxuan Wu, and Yu-Gang Jiang. Creatilayout: Siamese multimodal diffusion transformer for creative layout-to-image generation. arXiv preprint arXiv:2412.03859, 2024. [59] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In International Conference on Computer Vision (ICCV), 2023. [60] Xinchen Zhang, Ling Yang, YaQi Cai, Zhaochen Yu, Kai-Ni Wang, xie jiake, Ye Tian, Minkai Xu, Yong Tang, Yujiu Yang, and Bin CUI. Realcompo: Balancing realism and compositionality improves textto-image diffusion models. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. [61] Yibo Zhao, Liang Peng, Yang Yang, Zekai Luo, Hengjia Li, Yao Chen, Wei Zhao, Qinglin Lu, Wei Liu, and Boxi Wu. Local conditional controlling for text-to-image diffusion models. CoRR, abs/2312.08768, 2023. [62] Guangcong Zheng, Xianpan Zhou, Xuewei Li, Zhongang Qi, Ying Shan, and Xi Li. Layoutdiffusion: Controllable diffusion model for layout-to-image generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023. [63] Xingyi Zhou, Vladlen Koltun, and Philipp Krähenbühl. Simple multi-dataset detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022."
        },
        {
            "title": "Appendix",
            "content": "In the appendix, we first discuss the broader impact of our work in Section A. Section analyzes the impact of two key hyper-parameters in our framework. Section provides more detailed results of the correctness of object prior and final images, including human evaluation and qualitative examples. Section shows the result of our ComposeAnything method with SDXL as the base model. Section introduces the human evaluation details for state-of-the-art comparison. Section presents the LLM prompt for 2.5D semantic layout generation and illustrates an output example. Finally, Section details the LLM prompt for evaluating the 3D-Spatial category."
        },
        {
            "title": "A Broader Impact",
            "content": "This work proposes novel text-to-image (T2I) framework for compositional image generation, advancing interpretability, faithfulness and controllability of deep generative models in image generation from complex textual descriptions. This has potential applications in creative design, digital content creation, education and human-computer interaction to serve as an assistive tool for artists and designers. It can also benefit robotics in generating synthetic datasets. Since our method focuses on improving the compositional capabilities of existing diffusion models without training, we do not foresee significant additional ethical risks associated with this work compared to the base models. These T2I base models come with significant misinformation risks (deepfakes). Furthermore, copyright infringement and biases need to be carefully checked when training them. Impact of Key Hyper-parameters We analyze the effect of two key hyper-parameters tp and Nsc, as discussed in Section 4.1 of the main paper. tp is the time at which noise is sampled and applied to the prior image in the forward diffusion. It controls the object prior reinforcement strength (OPR). Lower values denote stronger priors. Nsc is the number of steps for spatially controlled denoising. It controls the spatial-controlled denoising strength (SCD). Larger values result in stronger control. Figure 8 presents object priors and corresponding generated images for two text prompts, under varying tp and Nsc values. In the first row of each example, Nsc is fixed at 3 to isolate the impact of OPR with different tp. At low OPR strength, the final image fails to preserve the appearance and semantics of the prior, for example, the butterfly appears in front of the cup not on top of it, and the number of clocks and microwaves is incorrect. As the strength of OPR increases, objects semantics and appearance such as color, shape and number are more strongly retained. However, excessive reinforcement reduces generative flexibility, leading to over-constrained and less natural outputs. In the second row of each example, tp is fixed at 0.91 to examine the effect of Nsc. Low SCD strength leads to limited spatial control, with objects leaking in background (extra cup), objects getting merged (both dogs merged), and incorrect object counts. As we increase the SCD strength, object positions and sizes from the prior are more faithfully preserved in the final image. However, too strong spatial control results in low-quality compositions such as rigid placements, incoherent scene, floating objects similar to training-based box-conditioned methods [58]. Therefore, in our experiments, we set tp = 0.91 and Nsc = 3 to strike balance between faithful prompt adherence, generative flexibility, and overall scene coherence. Both hyper-parameters are beneficial and complementary to reliably produces correct spatial relations, accurate object counts and high-quality images. 14 Figure 8: Effect of Object Prior Reinforcement and Spatial-Controlled Denoising. Increasing either strength enhances appearance fidelity and spatial precision, but reduces generative flexibility."
        },
        {
            "title": "C Detailed Evaluation of Object Priors and Corresponding Final Images",
            "content": "To better assess the quality of object priors and their influence on final image generation, we categorize results into four combinations: i) correct prior correct image, ii) correct prior incorrect image, iii) incorrect prior correct image, iv) incorrect prior incorrect image. We conduct human evaluation using 30 samples, i.e., pairs of the prior and final image, across the 2D-Spatial, 3D-Spatial, and Numeracy categories in T2I-Compbench. For sample annotators perform 15 Figure 9: Human evaluation results on the correctness of prior and final image pairs on the three categories of T2I-Compbench dataset. 4-way classification task, judging the correctness of both the prior and the resulting image. The annotation interface is illustrated in Figure 10. Figure 9 presents the results of the human evaluation. As seen in the bar plots for the 2D and 3D-Spatial categories, the majority of samples fall into the correct-prior and correct-image category. In contrast, the Numeracy category shows higher occurrence of correct-prior but incorrect-image cases. This is primarily due to the increasing number of objects, where relative object sizes in the prior affect its realism and quality. While our models generative flexibility helps correct such visual artifacts during image synthesis, it often does so at the expense of count accuracy, leading to mismatches in object quantities, as illustrated in Figure 10 (bottom). Moreover, as the number of objects increases, the priors tend to become less coherent overall, resulting in greater frequency of incorrect-prior and incorrect-image cases. Figure 11 - 14 provide more examples of the object priors and corresponding final images on T2ICompbench dataset, including 2D-spatial, 3D-spatial, non-spatial, numeracy and complex prompt categories. 16 Figure 10: Labeling interface for evaluating object prior and the final image. 17 Figure 11: Object prior and the corresponding generation for 2D-Spatial compositions from T2Icompbench Figure 12: Object prior and the corresponding generation for Complex compositions from T2ICompbench. 19 Figure 13: Object prior and the corresponding generation for 3D-Spatial compositions from T2Icompbench 20 Figure 14: Object prior and the corresponding generation for Numeracy compositions from T2Icompbench Table 4: Evaluating our framework on T2I-Compbench for different base models SDXL (a score based diffusion model), and SD3-M (a flow matching based diffusion model). Model SDXL SD3-M ComposeAnything(ours) ComposeAnything(ours) Base Model SDXL SD3-M SDXL SD3-M 2D-Spatial Numeracy 3D-Spatial Complex 21.33 31.32 44.64 48. 49.88 60.22 57.22 68.21 47.12 49.43 71.03 77.16 32.37 37.71 36.20 38."
        },
        {
            "title": "D Additional results on DDIM based SDXL as a base model",
            "content": "In our main experiments, we adopt Stable Diffusion 3 (SD3) as the base model. SD3 is built on the flow matching framework [33], which replaces the traditional score-based training objective with learned velocity field that enables more stable and efficient generative dynamics. For sampling, it utilizes the Flow Matching Euler (FME) discrete sampler, enabling faster image generation with fewer steps. Architecturally, SD3 incorporates the MM-DiT (Multimodal DiT) Transformer [11] with 2-way, joint self attention, allowing for improved scalability and better handling of complex conditioning signals. In this section, we further report results on T2I-Compbench, using SDXL [45] prior diffusion model trained with the conventional score-based denoising objective. SDXL employs DDIM (Denoising Diffusion Implicit Models) [48] sampling and incorporates U-Net backbone with unidirectional cross-attention layers for conditioning on text prompts. Our model enhances the performance of both SDXL and SD3 base architectures by large margin."
        },
        {
            "title": "E Labeling interface for human evaluations",
            "content": "Figure 15 shows the human evaluation instructions and interface. The instructions focuses on prioritizing both correctness to prompt focusing on 2D-3D spatial relations and object count. Also to select images with higher quality. As can be seen in the \"six horses\" image, both images have 6 horses, but the first image has better quality. Figure 15: Labeling interface for human evaluations."
        },
        {
            "title": "F LLM Planning Instructions",
            "content": "Figures 16 - 20 show the detailed instructions for LLM planning. Figure 21 shows the output from the LLM for an example alongside the prior and final image. Figure 16: Instructions for LLM planning (to be continued). 24 Figure 17: Instructions for LLM planning (to be continued). Figure 18: Instructions for LLM planning (to be continued). 26 Figure 19: Instructions for LLM planning (to be continued). 27 Figure 20: Instructions for LLM planning. Figure 21: LLM planning for object prior generation, with final generated image. 29 3D metric evaluation with LLM Figure 22 presents the detailed instructions given to the LLM for evaluating 3D-spatial relations. Figure 22: LLM instructions for evaluating 3D-spatial relations."
        }
    ],
    "affiliations": [
        "Inria, École normale supérieure, CNRS, PSL Research University"
    ]
}