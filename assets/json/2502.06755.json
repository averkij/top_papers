{
    "paper_title": "Sparse Autoencoders for Scientifically Rigorous Interpretation of Vision Models",
    "authors": [
        "Samuel Stevens",
        "Wei-Lun Chao",
        "Tanya Berger-Wolf",
        "Yu Su"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "To truly understand vision models, we must not only interpret their learned features but also validate these interpretations through controlled experiments. Current approaches either provide interpretable features without the ability to test their causal influence, or enable model editing without interpretable controls. We present a unified framework using sparse autoencoders (SAEs) that bridges this gap, allowing us to discover human-interpretable visual features and precisely manipulate them to test hypotheses about model behavior. By applying our method to state-of-the-art vision models, we reveal key differences in the semantic abstractions learned by models with different pre-training objectives. We then demonstrate the practical usage of our framework through controlled interventions across multiple vision tasks. We show that SAEs can reliably identify and manipulate interpretable visual features without model re-training, providing a powerful tool for understanding and controlling vision model behavior. We provide code, demos and models on our project website: https://osu-nlp-group.github.io/SAE-V."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 1 ] . [ 1 5 5 7 6 0 . 2 0 5 2 : r a"
        },
        {
            "title": "Samuel Stevens\nThe Ohio State University",
            "content": "Wei-Lun Chao The Ohio State University Tanya Berger-Wolf The Ohio State University"
        },
        {
            "title": "Yu Su\nThe Ohio State University",
            "content": "stevens.994@osu.edu chao.209@osu.edu berger-wolf.1@osu.edu su.809@osu.edu"
        },
        {
            "title": "Abstract",
            "content": "To truly understand vision models, we must not only interpret their learned features but also validate these interpretations through controlled experiments. Current approaches either provide interpretable features without the ability to test their causal influence, or enable model editing without interpretable controls. We present unified framework using sparse autoencoders (SAEs) that bridges this gap, allowing us to discover human-interpretable visual features and precisely manipulate them to test hypotheses about model behavior. By applying our method to state-of-the-art vision models, we reveal key differences in the semantic abstractions learned by models with different pre-training objectives. We then demonstrate the practical usage of our framework through controlled interventions across multiple vision tasks. We show that SAEs can reliably identify and manipulate interpretable visual features without model re-training, providing powerful tool for understanding and controlling vision model behavior1."
        },
        {
            "title": "1 Introduction",
            "content": "We do not have knowledge of thing until we have grasped its why, that is to say, its cause. Aristotle Understanding deep neural networks requires more than passive observationit demands the ability to test hypotheses through controlled intervention. This mirrors the scientific method itself: true understanding emerges not from mere observation, but from our ability to make and test predictions through controlled experiments. Biologists did not truly understand how genes control traits until they could manipulate DNA and observe the effects. Similarly, understanding vision models requires not just observing behavior, but systematically testing explanations through controlled experiments. Applying the scientific method to understanding vision models requires three key capabilities. First, we need observable features that correspond to human-interpretable concepts like textures, objects, or abstract properties that appear consistently across images. Biologists need measurable markers of gene expression; we need reliable ways to identify specific visual concepts within our models. Second, we must be able to precisely manipulate these features to test hypotheses about their causal rolelike geneticists knockout experiments 1Project website: https://osu-nlp-group.github.io/SAE-V 1 Observe Hypothesis Experiment We observe the DNA sequence of blue jay (Cyanocitta cristata), bird with distinct blue wings. We hypothesize that these genes cause the blue coloration. We edit the gene (gene knockout) to validate that the hypothesized genes do cause blue coloration. We observe CUB-2011-trained ViT correctly predict blue jay. Grad-CAM hypothesizes the highlighted pixels are integral to the ViTs prediction. How can we empirically validate GradCAMs hypothesis? Genomics Grad-CAM SAEs We observe CUB-2011-trained ViT correctly predict blue jay. We manually choose to inspect the birds wing. Our SAE finds similar patches from images not necessarily of blue jays; we hypothesize this blue feathers feature is integral to the ViTs prediction. We suppress blue feathers and observe changed behavior: the ViT predicts Clarks nutcracker, similar species besides no coloration (examples above). Figure 1: We compare the scientific method when applied to genomics and deep learning model interpretation. Grad-CAM (Selvaraju et al., 2017) leverages saliency maps to produce hypothetical explanations for model predictions, but provides no natural way to experimentally validate the hypothesis. In comparison, our proposed use of sparse autoencoders (SAEs) for vision models naturally enables experimental validation via feature suppression. See Section 5 for more examples of experimental validation. to validate gene function. Finally, methods must work with existing models, just as biological techniques must work with existing organisms rather than requiring genetic redesign. The scientific method advances understanding through systematic cycle: observing phenomena, forming hypotheses about their causes, and validating these hypotheses through controlled experiments (Poincar√©, 1914; Popper, 1959). This method drove discoveries from genetics to behavioral psychologyMendel observed patterns in pea plants and tested inheritance theories through careful breeding, while Pavlovs observation of dogs salivating before feeding led to controlled experiments demonstrating learned associations between stimuli. This sequence of scientific discoveryfrom observation to hypothesis to experimental validation provides proven template that, surprisingly, has not been systematically applied to understanding vision models. Interpretable features enable us to form meaningful hypotheses from observations. Precise control mechanisms let us design experiments to test these hypotheses. Compatibility with existing models allows us to validate our findings on models of interest. The scientific method breaks down if any component is missing: observations become untestable, experiments become uninterpretable, or validation becomes impossible. Current approaches to understanding vision models fall short of these requirements, preventing rigorous scientific investigation. Feature visualization (Erhan et al., 2009; Mordvintsev et al., 2015a; Olah et al., 2017) methods provide interpretable observations but no mechanism to validate if these observations actually drive model behavior (Geirhos et al., 2024). Adversarial examples (Szegedy, 2013; Goodfellow et al., 2014) enable manipulation but offer no interpretable explanation for how they work. Network dissection (Bau et al., 2017; Hernandez et al., 2021; Kalibhat et al., 2023) attempts to map individual neurons to semantic concepts, but struggles with distributed representations where concepts span multiple neurons or single neurons encode multiple concepts. This makes it difficult to reliably identify and manipulate specific semantic 2 Figure 2: Sparse autoencoders (SAEs) trained on pre-trained ViT activations discover wide spread of features across both visual patterns and semantic structures. We show eight different features from an SAE trained on ImageNet-1K activations from CLIP-trained ViT-B/16. features. This fragmentationwhere methods either enable interpretation without control or control without interpretationundermines our ability to build reliable understanding of these systems We demonstrate that sparse autoencoders (SAEs; Makhzani & Frey 2013; Subramanian et al. 2018; Huben et al. 2024) enable this kind of rigorous scientific investigation. SAEs transform entangled activation vectors into higher-dimensional sparse representations, in which each nonzero element likely corresponds to distinct semantic concept. In the case of bird classification, one sparse dimension might indicate blue plumage. Because the SAE is trained to reconstruct the original activations, each element explicitly corresponds to particular direction in the original dense activation space. This direct mapping allows us to selectively modify that featuresuppressing the blue signalto observe consequent shifts in model predictions. This unification of interpretation with controlled intervention completes the cycle of observation, hypothesis, and experiment (see Figure 1), and we validate its effectiveness through experiments across multiple core contributions. First, we show how SAEs enable systematic observation of learned features, revealing fundamental differences between models like CLIP (Radford et al., 2021) and DINOv2 (Oquab et al., 2023) (Section 4). We discover that CLIP learns to recognize country-specific visual patterns, from architectural landmarks to sporting events (Section 4.1) and style-agnostic representations (Section 4.2), suggesting that language supervision leads to rich world knowledge that purely visual training cannot achieve. Just as comparative biology reveals how different environments shape evolution (Grant & Grant, 2006; Losos, 2011; Brawand et al., 2014), our analysis shows how different training objectives lead models to develop qualitatively different internal representations. Second, we validate our interpretations through controlled experiments across multiple tasks. When models detect features corresponding to bird markings, we confirm their causal role by showing that modifying them predictably changes species classification (Section 5.1). Similarly, with semantic segmentation, we show that identified features enable precise, targeted manipulation: we can suppress specific semantic concepts (like sand or grass) while preserving all other scene elements, demonstrating both the semantic meaning of our features and their independence from unrelated concepts (Section 5.2). We achieve this through multiple interactive dashboards, enabling readers to readily explore these features. 2https://OSU-NLP-Group.github.io/SAE-V#demos 3 Third, we provide public, extensible codebase that works with any vision transformer without modification, enabling broad scientific investigation of modern vision models. We demonstrate this flexibility through fine-grained classification and semantic segmentation experiments, with future updates planned. Through these contributions, we show that rigorous understanding of vision models requires both interpretation and experimental validation, and that SAEs provide natural concrete implementation of such framework."
        },
        {
            "title": "2 Related Work",
            "content": "Our work connects to three main ideas: methods for interpreting vision models, methods for controlling model behavior, and sparse autoencoder development. We discuss each in relation to our approach."
        },
        {
            "title": "2.1 Interpretability Methods",
            "content": "Feature visualization methods (Simonyan, 2013; Zeiler, 2014; Mordvintsev et al., 2015b; Olah et al., 2017) reveal interpretable concepts through synthetic images. While these approaches demonstrate that models learn meaningful features, the generated images can be unrealistic or misleading, and the methods cannot validate if these visualizations actually drive model behavior. In contrast, our SAE approach identifies features through real image examples and enables direct testing of their causal influence. Network dissection (Bau et al., 2017; Zhou et al., 2018; Hernandez et al., 2022; Ghiasi et al., 2022) attempts to map individual neurons to semantic concepts using labeled datasets. However, this approach struggles with distributed representations where concepts span multiple neurons or single neurons encode multiple concepts (polysemanticity). The sparse features learned by our SAEs naturally decompose these distributed representations into interpretable components. Concept bottleneck models (CBMs; Ghorbani et al. 2019; Koh et al. 2020), prototype-based approaches (Chen et al., 2019; Nauta et al., 2021; Donnelly et al., 2022; Willard et al., 2024), and prompting-based methods (Chowdhury et al., 2025; Paul et al., 2024) explicitly incorporate interpretable concepts into model architecture. While powerful, these methods require model retraining and cannot analyze existing architectures. SAEs can be applied to any pre-trained vision transformer. Follow-up work addresses these shortcomings: Yuksekgonul et al. (2023) convert pre-trained models to CBMS; Schrodi et al. (2024); Tan et al. (2024) develop CBMs with open vocabularies rather than fixed concept set. In contrast to these works, SAEs reveal models intrinsic knowledge in task-agnostic manner by decomposing dense activations into sparse, monosemantic features without any retraining. This plug-and-play approach not only faithfully captures the vision models internal representations but also enables precise interventions, making SAEs more flexible tool for validating model interpretation. Testing with Concept Activation Vectors (TCAV) (Kim et al., 2018) attempts to connect human concepts to model behavior by identifying directions in activation space that correlate with human-specified concepts (like striped or furry). While TCAV can reveal correlations between activation patterns and semantic concepts, it cannot validate if these concepts actually drive model decisions. This highlights fundamental limitation of correlation-based interpretation methods: showing that activations correlate with human concepts does not prove those concepts cause model behavior. Our SAE approach moves beyond correlation by enabling controlled experimentswe can actively suppress or enhance specific features and observe the effects on model behavior, providing causal evidence for our interpretations. 2.2 Related Control Methods Adversarial examples (Szegedy, 2013; Goodfellow et al., 2014; Moosavi-Dezfooli et al., 2016) demonstrate that small, carefully crafted perturbations can dramatically change model predictions. Later work extended this to universal perturbations (Moosavi-Dezfooli et al., 2017) and model reprogramming (Elsayed et al., 2018). However, these perturbations are typically uninterpretablewe cannot understand why they work. Our SAE-based interventions provide interpretable control by manipulating specific semantic features. 4 Figure 3: Given picture and set of highlighted patches, we find exemplar images by (1) getting ViT activations for each patch, (2) computing sparse representation for each highlighted patch (Eqs. (1) and (2)), (3) summing over sparse representations, (4) choosing the top features by activation magnitude and (5) finding existing images that maximize these features. Model editing methods for language models (Meng et al., 2022; 2023) enable precise modification of model behavior by directly editing weights or activations. While successful for language tasks, these approaches have not been extended to vision models. Our work provides the first general framework for interpretable editing of vision model behavior. Recent counterfactual explanation methods (Goyal et al., 2019) attempt to identify minimal input changes that alter predictions. While related, these approaches focus on individual examples rather than discovering and manipulating general features as our method does. CBMs (Koh et al., 2020) enable editing but only in the last concept layer without spatial resolution. In contrast, our approach allows for localized manipulation, making it widely applicable to various vision tasks. 2.3 Sparse Autoencoders Makhzani & Frey (2013; 2015) apply k-sparse autoencoders to learn improved image representations. Subramanian et al. (2018) apply k-sparse autoencoders to static word embeddings (word2vec (Mikolov et al., 2013) and GloVe (Pennington et al., 2014)) to improve interpretability. Zhang et al. (2019); Yun et al. (2021); Bricken et al. (2023); Templeton et al. (2024b); Gao et al. (2024) apply sparse autoencoders to transformer-based language model activations and find highly interpretable features. We extend these ideas to vision, showing that sparsity can reveal interpretable features in visual as well as linguistic domains."
        },
        {
            "title": "3 Methodology",
            "content": "3.1 Observations: Model Behaviors Vision models demonstrate diverse capabilities across multiple tasks. Understanding these systems requires explanations that satisfy three key criteria: they must be human-interpretable, testable through controlled experiments, and enable precise intervention in model behavior. 3.2 Hypotheses: SAE-Generated Explanations Sparse autoencoders generate testable hypotheses by decomposing dense activation vectors into sparse feature vectors. Given an d-dimensional activation vector Rd from an intermediate layer of vision transformer, an SAE maps to sparse representation (x) (Eqs. (1) and (2)) and reconstructs the original input (Eq. (3)). We use ReLU autoencoders (Bricken et al., 2023; Templeton et al., 2024b): = Wenc(x bdec) + benc (x) = ReLU(h) ÀÜx = Wdecf (x) + bdec 5 (1) (2) (3) where Wenc Rnd, benc Rn, Wdec Rdn and bdec Rd. The training objective minimizes reconstruction error while encouraging sparsity: L(Œ∏) = ÀÜx2 2 + ŒªS(f (x)) (4) where Œª controls the sparsity penalty and measures sparsity (L1 norm for training, L0 for model selection). We train on randomly sampled patch activation vectors from the residual stream of layer in vision transformer. Following prior work (Templeton et al., 2024b), we subtract the mean activation vector and normalize activation vectors to unit norm before training. Detailed reproduction instructions are available. Given pre-trained ViT like CLIP or DINOv2, an image, and one or more patches of interest, we leverage trained SAE to find similar examples (Figure 3)."
        },
        {
            "title": "3.3 Experiments: Testing Through Control",
            "content": "We validate SAE-proposed explanations through general intervention framework that leverages the common pipeline of vision tasks: an image is first converted into d-dimensional activation vectors in Rpd (e.g., from vision transformer), then these activation vectors are mapped by task-specific decoder to produce outputs in the task-specific output space O. For instance, in semantic segmentation each patchs activation vector Rd is fed to decoder head that assigns pixel-level segmentation labels, and these pixel labels are assembled into the final segmentation map. Given task-specific decoder : Rpd that maps from activations vectors to per-patch class predictions, our intervention process proceeds in six steps: 1. Encode and reconstruct: (x) = ReLU(Wenc(x bdec) + benc) and ÀÜx = Wdecf (x) + bdec. 2. Calculate reconstruction error (Templeton et al., 2024b): = ÀÜx. 3. Modify individual values of (x) to get (x). 4. Reconstruct modified activations: ÀÜx = Wdecf (x) + bdec. 5. Add back error: = + ÀÜx. 6. Compare outputs M(x) versus M(x). In plain language: We start by converting an image into set of activation vectorsone per patchthat capture the ViTs internal representation. Then, we encode these vectors into sparse representation that highlights the key features, while also tracking the small differences (errors) between the sparse form and the original. Next, we deliberately tweak the sparse representation to modify feature of interest. After reconstructing the modified activation vectors (and adding back the previously captured details), we pass them through the task-specific decoder (e.g., for classification, segmentation, or another vision task) to see how the output changes. By comparing the original and altered outputs, we can determine whether and how the targeted feature influences the models behavior."
        },
        {
            "title": "4 SAE-Enabled Analysis of Vision Models",
            "content": "Sparse autoencoders (SAEs) provide powerful new lens for understanding and comparing vision models. By decomposing dense activation vectors into interpretable features, SAEs enable systematic analysis of what different architectures learn and how their training objectives shape their internal representations. We demonstrate that even simple manual inspection of top-activating images for SAE-discovered features can reveal fundamental differences between models that would be difficult to detect through other methods. While prior work has used techniques like TCAV (Kim et al., 2018) to probe for semantic concepts in vision models, these methods typically test for pre-defined concepts and rely on supervised concept datasets. In contrast, our SAE-based approach discovers interpretable features directly from model activations without requiring concept supervision. We train SAEs on intermediate layer activations following the procedure detailed in Section 3, then analyze the highest-activating image patches for each learned feature as in Figure 3. This straightforward process reveals both specific features (e.g., feature that fires on dental imagery) and model-level patterns (e.g., one model consistently learning more abstract features than another). 6 (a) CLIP-24K/6909: Brazil (b) Not Brazil (c) DINOv2-24K/ (d) ImageNet-1K Exemplars Figure 4: CLIP learns robust cultural visual features. Top Left (a): Brazil feature (CLIP-24K/6909) responds to distinctive Brazilian imagery including Rio de Janeiros urban landscape, the national flag, and the iconic sidewalk tile pattern of Copacabana Beach Top Right (b): CLIP-24K/6909 does not respond to other South American symbols like Machu Picchu or the Argentinian flag. Bottom Left (c): We search DINOv2s SAE for similar Brazil feature and find that DINOv2-24K/9823 fires on Brazilian imagery. Bottom Right (d): However, maximally activating ImageNet-1K examples for DINOv2-24K/9823 are of lamps, convincing us that DINOv2-24K/9823 does not reliably detect Brazilian cultural symbols. We refer to individual SAE features using standardized format MODEL-WIDTH/INDEX, where MODEL identifies the vision transformer the SAE was trained on (e.g., CLIP or DINOv2), WIDTH indicates the number of features in the SAE (e.g., 24K for 24,576), and INDEX uniquely identifies the specific feature. For example, CLIP-24K/20652 refers to feature 20,652 from 24,576-dimensional SAE trained on CLIP activations. 4.1 Language Supervision Enables Cultural Understanding We analyze SAE features and find that CLIP learns remarkably robust representations of cultural and geographic conceptsa capability that appears absent in DINOv2s purely visual representations. This finding demonstrates how language supervision enables learning of abstract cultural features that persist across diverse visual manifestations. Consider CLIPs representation of country-specific visual features. We find individual SAE features that consistently activate on imagery associated with specific countries, while remaining inactive on visually similar but culturally distinct images (Figure 4). For instance, CLIP reliably detects German visual elements across architectural landmarks (like the Brandenburg Gate), sports imagery (German national team uniforms), and cultural symbols (Oktoberfest celebrations). Crucially, this feature remains inactive on other European architectural landmarks or sporting events, suggesting it captures genuine cultural associations rather than just visual similarities. Similarly, we find features that activate on distinctly Brazilian imagery, spanning Rio de Janeiros urban landscape, the national flag, coastal scenes, and cultural celebrations. These features show selective activation, responding strongly to Brazilian content while remaining inactive on visually similar scenes from other South American locations. This selective response pattern suggests CLIP has learned to recognize and group culturally related visual elements, even when they share few low-level visual features. See Figure B1 for additional examples of this phenomena. 7 CLIP-24K/20652 ImageNet-1K Exemplars DINOv2-24K/9672 ImageNet-1K Exemplars Figure 5: CLIP learns unified representations of abstract concepts that persist across visual styles. Highlighted patches indicate feature activation strength. Upper Left: We find CLIP SAE feature (CLIP-24K/20652) that consistently activates on accidents or crashes: car accidents, plane crashes, cartoon depictions of crashes and generally damaged metal. Upper Right: Two exemplar images from ImageNet-1K for feature CLIP-24K/20652. Lower Left: We probe an SAE trained on DINOv2 activations. DINOv2-24K/9762 is the closest feature, but does not reliably fire on all the examples. Lower Right: Two exemplar images from ImageNet-1K for feature DINOv2-24K/9762 clarifies that it does not match the semantic concept of crash. In contrast, when we analyze DINOv2s features, we find no comparable country-specific representations. This fundamental difference reveals how language supervision guides CLIP to learn culturally meaningful visual abstractions that pure visual training does not discover. 4.2 Language Supervision Induces Semantic Abstraction Beyond cultural concepts, CLIP learns abstract semantic features that persist across visual stylesa capability absent in DINOv2revealing how language supervision enables human-like semantic concept formation. For example, we discover an SAE feature (CLIP-24K/20652) that activates on accident scenes across photographs of car accidents, crashed planes and cartoon depictions of crashes (Figure 5). This feature fires strongly whether processing news photograph of car accident or stylized illustration of collision suggesting CLIP has learned truly abstract representation of accident that transcends visual style. In contrast, DINOv2s features fragment these examples across multiple low-level visual patterns. While it learns features that detect specific visual aspects of accidents (like crumpled metal in photographs), no single feature captures the semantic concept across different visual styles. This suggests that without language supervision, DINOv2 lacks the learning signal needed to unite these diverse visual presentations into single abstract concept. We hypothesize that CLIPs language supervision provides explicit signals to group visually distinct but semantically related images, while DINOv2s purely visual training offers no such bridge across visual styles. The ability to form such abstract semantic concepts, independent of visual style, represents fundamental difference in how these models process and represent visual information. 4.3 Implications for Vision Model Selection Recent work has demonstrated empirical benefits from combining different vision encoders in vision-language models (Liu et al., 2024b;a; Lu et al., 2024). Tong et al. (2024b) constructed challenging dataset for VLMs using only CLIP as visual encoder. Cambrian-1 (Tong et al., 2024a) found improved performance when incorporating both CLIP and DINOv2 encoders, and Jiang et al. (2023) systematically examined the distinct contributions of different visual encoders in VLMs. However, the mechanistic basis for these improvements has remained unclear. 8 Figure 6: Demonstrating the scientific method for understanding vision model behavior using sparse autoencoders (SAEs). Left: We observe that CLIP predicts Blue Jay. Upper Middle: We select the birds wing in the input image; the SAE proposes hypothesis that the most salient feature is blue feathers via exemplar images. Lower Middle: We validate this hypothesis through controlled intervention by suppressing the identified blue feathers feature in the models activation space. Right: we observe change in behavior: the predicted class shifts away from Blue Jay towards Clark Nutcracker, similar bird besides the lack of blue plumage. This three-step process of observation, hypothesis formation, and experimental validation enables systematic investigation of how vision models process visual information. Our SAE-driven analysis provides possible explanation for these empirical findings. The distinct feature patterns we observeCLIPs abstract semantic concepts versus DINOv2s style-specific featuressuggest these models develop complementary rather than redundant internal representations. When CLIP learns to recognize accidents across various visual styles, or country-specific features across diverse contexts, it develops abstractions that may help with high-level semantic understanding. Meanwhile, DINOv2s more granular features could provide detailed visual information that complements CLIPs abstract representations. Rather than treating encoders as interchangeable components to be evaluated purely on benchmark performance, practitioners might choose encoders based on characterized feature patterns."
        },
        {
            "title": "5 Validating Hypotheses of Vision Model Behavior",
            "content": "Understanding how vision models process information requires proving that our explanations accurately capture the models behavior. While prior work has demonstrated interpretable features or model control in isolation, validating explanations requires showing both that discovered features are meaningful and that we can precisely manipulate them. We demonstrate that SAE-derived features enable reliable control across two complementary visual understanding tasksclassification and segmentationproviding strong evidence that our interpretations faithfully capture model behavior. Vision models process information across multiple levels of abstraction, from local visual features to global semantic reasoning. Standard evaluation approaches focus on single level of processing, limiting their ability to validate feature explanations. We overcome this by strategically selecting tasks that test different aspects of visual understanding: classification validates precise control over visual attributes and segmentation demonstrates spatial coherence of identified features, For each task, we first train or utilize an existing task-specific prediction head. We then perform controlled feature interventions using our SAE and compare model behavior before and after intervention. Success 9 Figure 7: Far Left: We train linear head to predict semantic segmentation classes for each patch. Middle Left: We choose all sand-filled patches in the input image to inspect. Middle Right: Our SAE proposes exemplar images for the maximally activating sparse dimensions, as in Section 5.1, suggesting that DINOv2 is learning sand feature. Far Right: We suppress the sand feature in not just the selected patches, but all patches. We modify all activation vectors and pass them to DINOv2s final transformer layer followed by our trained linear segmentation head. We see that the head predicts earth, ground and water for the former sand patches. Both classes are good second choices if sand is unavailable. Notably, other patches are not meaningfully affected, demonstrating the pseudo-orthogonality of the SAEs learned feature vectors. across these diverse challenges, which we demonstrate through extensive qualitative results, provides strong evidence that our method identifies meaningful and faithful features. 5.1 Image Classification Precisely manipulating individual visual features is essential for validating our understanding of how vision models make decisions. While prior work identifies features through post-hoc analysis, proving causal relationships requires demonstrating that controlled feature manipulation yields predictable output changes. Using fine-grained bird classification as testbed, we show that SAE-derived features enable precise control: we can manipulate specific visual attributes like beak shape or plumage color while preserving other traits, producing predictable changes in classification outputs that validate our feature-level explanations of model behavior. We train sparse autoencoder on activations from layer 11 of CLIP-pretrained ViT-B/16 (Radford et al., 2021), using the complete ImageNet-1K dataset (Russakovsky et al., 2015) to ensure broad feature coverage. The SAE uses 32 expansion factor (24,576 features) to capture rich vocabulary of visual concepts. Through an interactive exploration interface, we identify interpretable features by examining patches that maximally activate specific SAE features. For example, selecting the blue feathers of blue jay (Cyanocitta cristata) reveals SAE features that consistently activate on similar colorations across the dataset. To validate the proposed explanation of these features, we can manipulate them by adjusting their values in the SAEs latent space. When we reduce the blue feathers feature in blue jay image, the models prediction shifts from blue jay to Clarks nutcracker (Nucifraga columbiana)a semantically meaningful change that aligns with ornithological knowledge (Figure 6). See Appendix for SAE details, linear classifier details, and additional qualitative examples. 5.2 Semantic Segmentation Moving beyond single-feature control requires proving that discovered features can be manipulated independentlya key challenge that traditional interpretability methods struggle to address. When features are entangled, attempts to modify one aspect of an image often produce unintended changes in others, making precise intervention impossible. Through semantic segmentation, we demonstrate that SAE features form pseudo-orthogonal basis for the models representational spacewhile not mathematically perpendicular, these features are functionally independent in practice. When we suppress semantic concepts like sand or grass, we observe consistent changes in targeted regions while leaving other predictions intact, demonstrating this functional independence. 10 We train linear probe on vision model patch-level features on the ADE20K semantic segmentation dataset (Zhou et al., 2017) following the setup in Oquab et al. (2023). Specifically, given an image-level representation tensor Rppd, we train linear probe to predict low-resolution logit map, which is up-sampled to full resolution. We achieve 35.1 mIoU on the ADE20K validation set. While this method falls short of state-of-the-art methods, it is sufficient testbed for studying feature control. Specific training details are are available in Appendix D. We train sparse autoencoder on ImageNet-1K activations from layer 11 of DINOv2-pretrained ViT-B/16 following the procedure described in Section 3. To validate our methods ability to manipulate semantic features, we developed an interactive interface that allows precise control over individual SAE features. Users can select specific image patches and modify their feature activations in both positive and negative directions, enabling targeted manipulation of semantic concepts. This approach provides several advantages over automated evaluation metrics: it allows exploration of feature interactions, demonstrates the spatial coherence of manipulations, and reveals how features compose to form higher-level concepts. The interface makes it possible to test hypotheses about learned features through direct experimentationfor instance, we can verify that sand features truly capture sand-like textures by suppressing them and observing consistent changes across diverse images, as shown in Figure 7."
        },
        {
            "title": "6 Conclusion",
            "content": "We demonstrate that sparse autoencoders enable both hypothesis formation and controlled testing for vision models, enabling both interpretation of learned features and precise control over model behavior. Our work reveals fundamental insights about representation learning in vision models. We show that language supervision guides CLIP toward human-interpretable abstractions that generalize across visual styles, while purely visual training in DINOv2 produces more granular, style-specific features. This finding suggests that achieving human-like visual abstraction may require bridging between different modalities or forms of supervision. Through controlled experiments across classification, segmentation, and image-text tasks, we validate that SAE-discovered features capture genuine causal relationships in model behavior rather than mere correlations. The ability to reliably manipulate these features while maintaining semantic coherence demonstrates that vision transformers learn decomposable, interpretable representations even without explicit supervision. However, significant challenges remain. Current methods for identifying manipulable features still require manual exploration, and the relationship between feature interventions and model behavior becomes increasingly complex for higher-level tasks. Future work should focus on automating feature discovery, understanding feature interactions across different model architectures, and extending reliable control to more sophisticated visual reasoning tasks. More broadly, our results suggest that combining interpretation with control may be essential for developing truly trustworthy AI systems. Just as scientific understanding emerges from our ability to both explain and manipulate natural phenomena, meaningful understanding of neural networks requires tools that unite explanation with experimentation. We believe SAEs provide promising foundation for building such tools."
        },
        {
            "title": "Acknowledgment",
            "content": "We thank colleagues from the Imageomics Institute and the OSU NLP group for their constructive feedback. This research is supported by National Science Foundation OAC-2118240. We are thankful for the generous support of the computational resources by the Ohio Supercomputer Center."
        },
        {
            "title": "References",
            "content": "Bau, D., Zhou, B., Khosla, A., Oliva, A., and Torralba, A. Network dissection: Quantifying interpretability of deep visual representations. In 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 33193327, 2017. doi: 10.1109/CVPR.2017.354. 2, 4 Brawand, D., Wagner, C. E., Li, Y. I., Malinsky, M., Keller, I., Fan, S., Simakov, O., Ng, A. Y., Lim, Z. W., Bezault, E., et al. The genomic substrate for adaptive radiation in african cichlid fish. Nature, 513(7518): 375381, 2014. 3 Bricken, T., Templeton, A., Batson, J., Chen, B., Jermyn, A., Conerly, T., Turner, N., Anil, C., Denison, C., Askell, A., Lasenby, R., Wu, Y., Kravec, S., Schiefer, N., Maxwell, T., Joseph, N., Hatfield-Dodds, Z., Tamkin, A., Nguyen, K., McLean, B., Burke, J. E., Hume, T., Carter, S., Henighan, T., and Olah, C. Towards monosemanticity: Decomposing language models with dictionary learning. Transformer Circuits Thread, 2023. https://transformer-circuits.pub/2023/monosemantic-features/index.html. 5 Chen, C., Li, O., Tao, D., Barnett, A., Rudin, C., and Su, J. K. This looks like that: deep learning for interpretable image recognition. Advances in neural information processing systems, 32, 2019. 4 Chowdhury, A., Paul, D., Mai, Z., Gu, J., Zhang, Z., Mehrab, K. S., Campolongo, E. G., Rubenstein, D., Stewart, C. V., Karpatne, A., et al. Prompt-cam: simpler interpretable transformer for fine-grained analysis. arXiv preprint arXiv:2501.09333, 2025. Donnelly, J., Barnett, A. J., and Chen, C. Deformable protopnet: An interpretable image classifier using deformable prototypes. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1026510275, June 2022. 4 Elsayed, G. F., Goodfellow, I., and Sohl-Dickstein, J. Adversarial reprogramming of neural networks. arXiv preprint arXiv:1806.11146, 2018. 4 Erhan, D., Bengio, Y., Courville, A., and Vincent, P. Visualizing higher-layer features of deep network. University of Montreal, 1341(3):1, 2009. 2 Gao, L., la Tour, T. D., Tillman, H., Goh, G., Troll, R., Radford, A., Sutskever, I., Leike, J., and Wu, J. Scaling and evaluating sparse autoencoders. arXiv preprint arXiv:2406.04093, 2024. 5 Geirhos, R., Zimmermann, R. S., Bilodeau, B., Brendel, W., and Kim, B. Dont trust your eyes: on the (un)reliability of feature visualizations. In Forty-first International Conference on Machine Learning, 2024. URL https://openreview.net/forum?id=s0Jvdolv2I. 2 Ghiasi, A., Kazemi, H., Borgnia, E., Reich, S., Shu, M., Goldblum, M., Wilson, A. G., and Goldstein, T. What do vision transformers learn? visual exploration. arXiv preprint arXiv:2212.06727, 2022. 4 Ghorbani, A., Wexler, J., Zou, J. Y., and Kim, B. Towards automatic concept-based explanations. In Wallach, H., Larochelle, H., Beygelzimer, A., d'Alch√©-Buc, F., Fox, E., and Garnett, R. (eds.), Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019. URL https://proceedings. neurips.cc/paper_files/paper/2019/file/77d2afcb31f6493e350fca61764efb9a-Paper.pdf. 4 Goodfellow, I. J., Shlens, J., and Szegedy, C. Explaining and harnessing adversarial examples. arXiv preprint arXiv:1412.6572, 2014. 2, 4 Goyal, Y., Wu, Z., Ernst, J., Batra, D., Parikh, D., and Lee, S. Counterfactual visual explanations. In Chaudhuri, K. and Salakhutdinov, R. (eds.), Proceedings of the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pp. 23762384. PMLR, 0915 Jun 2019. URL https://proceedings.mlr.press/v97/goyal19a.html. 5 Grant, P. R. and Grant, B. R. Evolution of character displacement in Darwins finches. Science, 313(5784): 224226, 2006. 3 12 Hernandez, E., Schwettmann, S., Bau, D., Bagashvili, T., Torralba, A., and Andreas, J. Natural language descriptions of deep visual features. In International Conference on Learning Representations, 2021. Hernandez, E., Schwettmann, S., Bau, D., Bagashvili, T., Torralba, A., and Andreas, J. Natural language descriptions of deep visual features. In International Conference on Learning Representations, 2022. URL https://arxiv.org/abs/2201.11114. 4 Huben, R., Cunningham, H., Smith, L. R., Ewart, A., and Sharkey, L. Sparse autoencoders find highly interpretable features in language models. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=F76bwRSLeK. 3 Jiang, D., Liu, Y., Liu, S., Zhao, J., Zhang, H., Gao, Z., Zhang, X., Li, J., and Xiong, H. From clip to dino: Visual encoders shout in multi-modal large language models. arXiv preprint arXiv:2310.08825, 2023. 8 Kalibhat, N., Bhardwaj, S., Bruss, C. B., Firooz, H., Sanjabi, M., and Feizi, S. Identifying interpretable subspaces in image representations. In International Conference on Machine Learning, pp. 1562315638. PMLR, 2023. 2 Kim, B., Wattenberg, M., Gilmer, J., Cai, C., Wexler, J., Viegas, F., et al. Interpretability beyond feature attribution: Quantitative testing with concept activation vectors (tcav). In International conference on machine learning, pp. 26682677. PMLR, 2018. 4, 6 Koh, P. W., Nguyen, T., Tang, Y. S., Mussmann, S., Pierson, E., Kim, B., and Liang, P. Concept bottleneck models. In III, H. D. and Singh, A. (eds.), Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pp. 53385348. PMLR, 1318 Jul 2020. URL https://proceedings.mlr.press/v119/koh20a.html. 4, Liu, H., Li, C., Li, Y., and Lee, Y. J. Improved baselines with visual instruction tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2629626306, 2024a. 8 Liu, H., Li, C., Wu, Q., and Lee, Y. J. Visual instruction tuning. Advances in neural information processing systems, 36, 2024b. 8 Loshchilov, I. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. 17, 21 Losos, J. B. Lizards in an evolutionary tree: ecology and adaptive radiation of anoles, volume 10. Univ of California Press, 2011. 3 Lu, H., Liu, W., Zhang, B., Wang, B., Dong, K., Liu, B., Sun, J., Ren, T., Li, Z., Yang, H., et al. Deepseek-vl: towards real-world vision-language understanding. arXiv preprint arXiv:2403.05525, 2024. 8 Makhzani, A. and Frey, B. K-sparse autoencoders. arXiv preprint arXiv:1312.5663, 2013. 3, 5 Makhzani, A. and Frey, B. J. Winner-take-all autoencoders. Advances in neural information processing systems, 28, 2015. Meng, K., Bau, D., Andonian, A., and Belinkov, Y. Locating and editing factual associations in gpt. Advances in Neural Information Processing Systems, 35:1735917372, 2022. 5 Meng, K., Sharma, A. S., Andonian, A. J., Belinkov, Y., and Bau, D. Mass-editing memory in transformer. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview. net/forum?id=MkbcAHIYgyS. 5 Mikolov, T., Sutskever, I., Chen, K., Corrado, G. S., and Dean, J. Distributed representations of words and phrases and their compositionality. Advances in neural information processing systems, 26, 2013. 5 Moosavi-Dezfooli, S.-M., Fawzi, A., and Frossard, P. Deepfool: simple and accurate method to fool deep neural networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 25742582, 2016. 13 Moosavi-Dezfooli, S.-M., Fawzi, A., Fawzi, O., and Frossard, P. Universal adversarial perturbations. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 17651773, 2017. 4 Mordvintsev, A., Olah, C., and Tyka, M. Deepdream-a code example for visualizing neural networks. Google Research, 2(5), 2015a. 2 Mordvintsev, A., Olah, C., and Tyka, M. Inceptionism: Going deeper into neural networks. Google research blog, 20(14):5, 2015b. Nauta, M., Jutte, A., Provoost, J., and Seifert, C. This Looks Like That, Because ... Explaining Prototypes for Interpretable Image Recognition, pp. 441456. Springer International Publishing, 2021. ISBN 9783030937362. doi: 10.1007/978-3-030-93736-2_34. URL http://dx.doi.org/10.1007/978-3-030-93736-2_34. 4 Olah, C., Mordvintsev, A., and Schubert, L. Feature visualization. Distill, 2017. doi: 10.23915/distill.00007. https://distill.pub/2017/feature-visualization. 2, 4 Oquab, M., Darcet, T., Moutakanni, T., Vo, H. V., Szafraniec, M., Khalidov, V., Fernandez, P., Haziza, D., Massa, F., El-Nouby, A., Howes, R., Huang, P.-Y., Xu, H., Sharma, V., Li, S.-W., Galuba, W., Rabbat, M., Assran, M., Ballas, N., Synnaeve, G., Misra, I., Jegou, H., Mairal, J., Labatut, P., Joulin, A., and Bojanowski, P. Dinov2: Learning robust visual features without supervision, 2023. 3, 11, 21 Paul, D., Chowdhury, A., Xiong, X., Chang, F.-J., Carlyn, D., Stevens, S., Provost, K. L., Karpatne, A., Carstens, B., Rubenstein, D., et al. simple interpretable transformer for fine-grained image classification and analysis. In International Conference on Learning Representations, 2024. 4 Pennington, J., Socher, R., and Manning, C. D. Glove: Global vectors for word representation. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pp. 15321543, 2014. Poincar√©, H. Science and Method. Thomas Nelson, London, 1914. 2 Popper, K. The Logic of Scientific Discovery. Julius Springer, Hutchinson & Co, Berlin, 1959. 2 Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pp. 87488763. PMLR, 2021. 3, 10, 17 Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein, M., et al. Imagenet large scale visual recognition challenge. International journal of computer vision, 115:211252, 2015. 10 Schrodi, S., Schur, J., Argus, M., and Brox, T. Concept bottleneck models without predefined concepts. arXiv preprint arXiv:2407.03921, 2024. Selvaraju, R. R., Cogswell, M., Das, A., Vedantam, R., Parikh, D., and Batra, D. Grad-cam: Visual explanations from deep networks via gradient-based localization. In Proceedings of the IEEE international conference on computer vision, pp. 618626, 2017. 2 Simonyan, K. Deep inside convolutional networks: Visualising image classification models and saliency maps. arXiv preprint arXiv:1312.6034, 2013. 4 Subramanian, A., Pruthi, D., Jhamtani, H., Berg-Kirkpatrick, T., and Hovy, E. Spine: Sparse interpretable neural embeddings. In Proceedings of the AAAI conference on artificial intelligence, volume 32, 2018. 3, 5 Szegedy, C. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013. 2, 4 Tan, A., Zhou, F., and Chen, H. Explain via any concept: Concept bottleneck model with open vocabulary concepts. In European Conference on Computer Vision, pp. 123138. Springer, 2024. 4 14 Templeton, A., Conerly, T., Marcus, J., and Henighan, T. Update on dictionary learning improvements. Transformer Circuits Thread, 2024a. URL https://transformer-circuits.pub/2024/march-update/ index.html#dl-update. 16 Templeton, A., Conerly, T., Marcus, J., Lindsey, J., Bricken, T., Chen, B., Pearce, A., Citro, C., Ameisen, E., Jones, A., Cunningham, H., Turner, N. L., McDougall, C., MacDiarmid, M., Freeman, C. D., Sumers, T. R., Rees, E., Batson, J., Jermyn, A., Carter, S., Olah, C., and Henighan, T. Scaling monosemanticity: Extracting interpretable features from claude 3 sonnet. Transformer Circuits Thread, 2024b. URL https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html. 5, 6, 16 Tong, S., Brown, E., Wu, P., Woo, S., Middepogu, M., Akula, S. C., Yang, J., Yang, S., Iyer, A., Pan, X., et al. Cambrian-1: fully open, vision-centric exploration of multimodal llms. arXiv preprint arXiv:2406.16860, 2024a. 8 Tong, S., Liu, Z., Zhai, Y., Ma, Y., LeCun, Y., and Xie, S. Eyes wide shut? exploring the visual shortcomings In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern of multimodal llms. Recognition, pp. 95689578, 2024b. 8 Wah, C., Branson, S., Welinder, P., Perona, P., and Belongie, S. Caltech-ucsd birds-200-2011. Technical Report CNS-TR-2011-001, California Institute of Technology, 2011. 17 Willard, F., Moffett, L., Mokel, E., Donnelly, J., Guo, S., Yang, J., Kim, G., Barnett, A. J., and Rudin, C. This looks better than that: Better interpretable models with protopnext. arXiv preprint arXiv:2406.14675, 2024. 4 Yuksekgonul, M., Wang, M., and Zou, J. Post-hoc concept bottleneck models. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=nA5AZ8CEyow. 4 Yun, Z., Chen, Y., Olshausen, B. A., and LeCun, Y. Transformer visualization via dictionary learning: contextualized embedding as linear superposition of transformer factors. arXiv preprint arXiv:2103.15949, 2021. Zeiler, M. Visualizing and understanding convolutional networks. In European conference on computer vision/arXiv, volume 1311, 2014. 4 Zhang, J., Chen, Y., Cheung, B., and Olshausen, B. A. Word embedding visualization via dictionary learning. arXiv preprint arXiv:1910.03833, 2019. 5 Zhou, B., Zhao, H., Puig, X., Fidler, S., Barriuso, A., and Torralba, A. Scene parsing through ade20k dataset. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 633641, 2017. 11, 21 Zhou, B., Bau, D., Oliva, A., and Torralba, A. Interpreting deep visual representations via network dissection. IEEE transactions on pattern analysis and machine intelligence, 41(9):21312145, 2018. 4 15 Hyperparameter Value Hidden Width Sparsity Coefficient Œª Sparsity Coefficient Warmup Batch Size Learning Rate Œ∑ Learning Rate Warmup 24,576 (32 expansion) {4 104, 8 104, 1.6 103} 500 steps 16,384 {3 104, 1 103, 3 103} 500 steps Table A1: Hyperparameters for training SAEs. Sparsity coefficient Œª and learning rate Œ∑ are chosen qualitatively by inspecting discovered features."
        },
        {
            "title": "Appendices",
            "content": "We provide additional details omitted in the main text: 1. Appendix A: SAE Training Details (for Section 3.2) 2. Appendix B: Additional Examples (for Section 4) 3. Appendix C: Classification Details (for Section 5.1) 4. Appendix D: Classification Details (for Section 5.2)"
        },
        {
            "title": "A SAE Training Details",
            "content": "Our training code is publicly available, along with instructions to train your own SAEs for ViTs. Below, we describe the technical details necessary to re-implement our work. We save all patch-level activation vectors for given dataset from layer of pre-trained ViT to disk, stored in sharded format in 32-bit floating points. Future work should explore lower-precision storage. In practice, we explore 12-layer ViTs and record activations from layer 11 after all normalization. We then train newly initialized SAE on 100M activations. Wenc and Wdec are initialized using PyTorchs kaiming_uniform_ initialization and benc is zero-initialized. bdec is initialized as the mean of 524,288 random samples from the dataset, as recommended by prior work (Templeton et al., 2024b;a). We use mean squared error ÀÜx x2 2 sparsity coefficient Œª as our sparsity loss, and track L0 throughout training. as our reconstruction loss, L1 length of (x) scaled by the current The learning rate Œ∑ and sparsity coefficient Œª are linearly scaled from 0 to their maximum over 500 steps each and remain at their maximum for the duration of training. Columns of Wdec are normalized to unit length after every gradient update, and gradients parallel to the columns are removed before gradient updates."
        },
        {
            "title": "B Additional Examples",
            "content": "Additional examples of CLIP learning cultural features are in Figure B1."
        },
        {
            "title": "C Classification Details",
            "content": "Detailed instructions for reproducing our results using our public codebase are available on the web. We use an SAE trained on 100M CLIP ViT-B/16 activations from layer 11 on ImageNet-1Ks training split of 1.2M images following the procedure in Appendix A. 16 CLIP-24K/7622: United States of America Not USA CLIP-24K/13871: Germany Not Germany Figure B1: Additional examples of cultural features learned by CLIP. Top: CLIP-24K/7622 responds to symbolism from the United States of America, including portrait of George Washington, but not to portrait of King Louis XIV of France. Bottom: CLIP-24K/13871 activates strongly on the Brandenburg Gate and other German symbols, but not on visually similar flags like the Belgian flag. C.1 Task-Specific Decoder Training (Image Classification) For image classification, we train linear probe on the [CLS] token representations extracted from CLIPpretrained ViT-B/16 model (Radford et al., 2021) using the CUB-2011 dataset (Wah et al., 2011). The following details describe our experimental setup to ensure exact reproducibility. Data Preprocessing. Each input image is first resized so that its shorter side is 256 pixels, followed by center crop to obtain 224224 image. The resulting image is then normalized using the standard ImageNet mean and standard deviation for RGB channels. No additional data augmentation is applied. Feature Extraction. We use CLIP-pretrained ViT-B/16 model. From the final layer of the ViT, we extract the [CLS] token representation. The backbone is kept frozen during training, and no modifications are made to the extracted features. Classification Head. The classification head is linear layer mapping the [CLS] tokens feature vector to logits corresponding to the 200 classes in the CUB-2011 dataset. Training Details. We train the linear probe on the CUB-2011 training set using the AdamW (Loshchilov, 2017) optimizer for 20 epochs with batch size of 512. We performed hyperparameter sweeps over the learning rate with values in {105, 104, 103, 102, 101, 1.0} and over weight decay with values in {0.1, 0.3, 1.0, 3.0, 10.0}. Based on validation accuracy, we selected learning rate of 103 and weight decay of 0.1. The CLIP backbone remains frozen during this process. The trained linear probe achieves final accuracy of 79.9% on the CUB-2011 validation set. C.2 Additional Examples We provide additional examples of observing classification predictions, using SAEs to form hypothesis explaining model behavior, and experimentally validating said hypothesis through feature suppression in Figures C2 to C4. Each figure links to live, web-based demo where readers can complete the observe, hypothesize, experiment sequence themselves. 17 Figure C2: Tropical Kingbirds have distinctive yellow chest. When we suppress yellow feathers feature, our linear classifier predicts Gray Kingbird, similar species but with gray chest. This example is available at https://osu-nlp-group.github.io/SAE-V/demos/classification?example= 18 Figure C3: Canada Warblers have distinctive black necklace on the chest. CLIP-24K/20376 fires on similar patterns; when we suppress this feature, the linear classifier predicts Wilson Warbler, similar species without the distinctive black necklace. This example is available at https://osu-nlp-group.github.io/SAE-V/ demos/classification?example=1129 19 Figure C4: Purple finches have bright red coloration on the head and neck area; when we suppress CLIP-24K/10273, which appears to be red feathers feature, our classifier predicts Field Sparrow, which has similar wing banding but no red coloration. This example is available at https://osu-nlp-group. github.io/SAE-V/demos/classification?example="
        },
        {
            "title": "D Semantic Segmentation Details",
            "content": "Detailed instructions for reproducing our results using our public codebase are available on the web. D.1 Task-Specific Decoder Training (Semantic Segmentation) For semantic segmentation, we train single linear segmentation head on features extracted from frozen DINOv2-pretrained ViT-B/14 model (Oquab et al., 2023) using the ADE20K dataset (Zhou et al., 2017). We aim to map patch-level features to 151 semantic class logits. The following details describe our experimental setup to ensure exact reproducibility. Data Preprocessing. Each image is first resized so that its shorter side is 256 pixels, followed by center crop to obtain 224224 image. The cropped images are normalized using the standard ImageNet mean and standard deviation for RGB channels. No additional data augmentation is applied. Feature Extraction. We use DINOv2-pretrained ViT-B/14 with 224224 images, which results in 16 16 grid of 14 14 pixel patches. The final outputs from the ViT (after all normalization layers) are used as features. We exclude the [CLS] token and any register tokens, retaining only the patch tokens, thereby producing feature tensor of shape 16 16 d, where = 768 is the ViTs output feature dimension. Segmentation Head. The segmentation head is linear layer applied independently to each patch token. This layer maps the d-dimensional feature vector to 151-dimensional logit vector, corresponding to the 151 semantic classes. For visualization purposes, we perform simple upsampling by replicating each patch prediction to cover the corresponding 14 14 pixel block. For quantitative evaluation (computing mIoU), the 16 16 logit map is bilinearly interpolated to the full image resolution. Training Details. We train the segmentation head using the standard cross entropy loss. The DINOv2 ViT-B/14 backbone remains frozen during training, so that only the segmentation head is updated. We use AdamW (Loshchilov, 2017) with batch size of 1,024 over 400 epochs. We performed hyperparameter sweeps over the learning rate with values in {3 105, 1 104, 3 104, 1 103, 3 103, 1 102} and over weight decay values in {1 101, 1 103, 1 105}. No learning rate schedule is applied; the chosen learning rate is kept constant throughout training. The segmentation head is initialized with PyTorchs default initialization. Evaluation. For evaluation, the predicted 16 16 logit maps are upsampled to the full image resolution using bilinear interpolation before computing the mean Intersection-over-Union (mIoU) with the ground-truth segmentation masks. Our final segmentation head achieves an mIoU of 35.1 on the ADE20K validation set. D.2 Additional Examples We provide additional examples of observing segmentation predictions, using SAEs to form hypothesis explaining model behavior, and experimentally validating said hypothesis through feature suppression in Figures D5 to D8. These additional examples further support the idea of SAEs discovering features that are pseduo-orthogonal. Each figure links to live, web-based demo where readers can complete the observe, hypothesize, experiment sequence themselves. 21 Figure D5: DINOv2 correctly identifies the framed painting. We find that DINOv2-24K/16446 fires for paintings, and that suppressing this feature removes the painting without meaningfully affecting other parts of the image, despite modifying all patches. This example is available at https://osu-nlp-group.github. io/SAE-V/demos/semseg?example=1633. Figure D6: We find that feature DINOv2-24K/5876 fires for toilets, and that suppressing this feature removes the toilet without meaningfully affecting other parts of the image, despite modifying all patches. This example is available at https://osu-nlp-group.github.io/SAE-V/demos/semseg?example=1099. Figure D7: After suppressing bed feature (DINOv2-24K/18834), the segmentation head predicts pillow for the pillows and table for the bed spread. This examples is available at https://osu-nlp-group. github.io/SAE-V/demos/semseg?example=1117. Figure D8: We remove all cars from the scene by suppressing car-like feature (DINOv2-24K/7235). Notably, the van on the right of the image is also removed. This examples is available at https://osu-nlp-group. github.io/SAE-V/demos/semseg?example=1852."
        }
    ],
    "affiliations": [
        "The Ohio State University"
    ]
}