{
    "paper_title": "GazeGen: Gaze-Driven User Interaction for Visual Content Generation",
    "authors": [
        "He-Yen Hsieh",
        "Ziyun Li",
        "Sai Qian Zhang",
        "Wei-Te Mark Ting",
        "Kao-Den Chang",
        "Barbara De Salvo",
        "Chiao Liu",
        "H. T. Kung"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We present GazeGen, a user interaction system that generates visual content (images and videos) for locations indicated by the user's eye gaze. GazeGen allows intuitive manipulation of visual content by targeting regions of interest with gaze. Using advanced techniques in object detection and generative AI, GazeGen performs gaze-controlled image adding/deleting, repositioning, and surface material changes of image objects, and converts static images into videos. Central to GazeGen is the DFT Gaze (Distilled and Fine-Tuned Gaze) agent, an ultra-lightweight model with only 281K parameters, performing accurate real-time gaze predictions tailored to individual users' eyes on small edge devices. GazeGen is the first system to combine visual content generation with real-time gaze estimation, made possible exclusively by DFT Gaze. This real-time gaze estimation enables various visual content generation tasks, all controlled by the user's gaze. The input for DFT Gaze is the user's eye images, while the inputs for visual content generation are the user's view and the predicted gaze point from DFT Gaze. To achieve efficient gaze predictions, we derive the small model from a large model (10x larger) via novel knowledge distillation and personal adaptation techniques. We integrate knowledge distillation with a masked autoencoder, developing a compact yet powerful gaze estimation model. This model is further fine-tuned with Adapters, enabling highly accurate and personalized gaze predictions with minimal user input. DFT Gaze ensures low-latency and precise gaze tracking, supporting a wide range of gaze-driven tasks. We validate the performance of DFT Gaze on AEA and OpenEDS2020 benchmarks, demonstrating low angular gaze error and low latency on the edge device (Raspberry Pi 4). Furthermore, we describe applications of GazeGen, illustrating its versatility and effectiveness in various usage scenarios."
        },
        {
            "title": "Start",
            "content": "GazeGen: Gaze-Driven User Interaction for Visual Content Generation He-Yen Hsieh1, Ziyun Li2, Sai Qian Zhang2,3, Wei-Te Mark Ting1, Kao-Den Chang1, Barbara De Salvo2, Chiao Liu2, H. T. Kung1 2Reality Labs Research, Meta 3New York University 1Harvard University 4 2 0 2 ] . [ 1 5 3 3 4 0 . 1 1 4 2 : r Figure 1: GazeGen. (1) Users View: Overview of the users view, setting the context for gaze estimation (input: users eye images) and visual editing (inputs: users view and predicted gaze point). (2) Real-Time Gaze Estimation: The DFT Gaze Agent (281KB storage) predicts the users gaze point (green) aligned with the ground-truth gaze (red). (3) Gaze-Driven Visual Content Generation/Detection: Predicted gaze is used for editing ( ) based on the users focus ( ). GazeGen sets new standard for gaze-driven visual content generation, enhancing user experience and positioning users as visual creators. ) objects, or creating animations ( ) objects, detecting ( Abstract We present GazeGen, user interaction system that generates visual content (images and videos) for locations indicated by the users eye gaze. GazeGen allows intuitive manipulation of visual content by targeting regions of interest with gaze. Using advanced techniques in object detection and generative AI, GazeGen performs gaze-controlled image adding/deleting, repositioning, and surface material changes of image objects, and converts static images into videos. Central to GazeGen is the DFT Gaze (Distilled and Fine-Tuned Gaze) agent, an ultra-lightweight model with only 281K parameters, performing accurate real-time gaze predictions tailored to individual users eyes on small edge devices. GazeGen is the first system to combine visual content generation with real-time gaze estimation, made possible exclusively by DFT Gaze. This real-time gaze estimation enables various visual content generation tasks, all controlled by the users gaze. The input for DFT Gaze is the users eye images, while the inputs for visual content generation are the users view and the predicted gaze point from DFT Gaze. To achieve efficient gaze predictions, we derive the small model from large model (10x larger) via novel knowledge distillation and personal adaptation techniques. We integrate knowledge distillation with masked autoencoder, developing compact yet powerful gaze estimation model. This model is further finetuned with Adapters, enabling highly accurate and personalized gaze predictions with minimal user input. DFT Gaze ensures low-latency and precise gaze tracking, supporting wide range of gaze-driven tasks in AR environments. Leveraging these precise gaze predictions, GazeGen facilitates visual content generation through diffusion processes, allowing users to intuitively manipulate visual content by targeting regions with their gaze. Additionally, it enables real-time object detection by focusing on specific regions indicated by the users gaze, improving responsiveness. We validate the performance of DFT Gaze on AEA and OpenEDS2020 benchmarks, demonstrating low angular gaze error and low latency on the edge device (Raspberry Pi 4). Furthermore, we describe applications of GazeGen, illustrating its versatility and effectiveness in various usage scenarios."
        },
        {
            "title": "Introduction",
            "content": "Recent advancements in visual content editing interfaces have highlighted the need for systems that are both intuitive and accessible. Traditional methods often rely on physical manipulation, which can be limiting, especially for individuals with physical disabilities. To address this, we introduce GazeGen, system leveraging eye gaze for hands-free interaction, enhancing user engagement and accessibility beyond conventional augmented reality (AR) environments. By utilizing natural human behaviorwhere gaze directs attention and guides actionsGazeGen provides straightforward interface for managing and interacting with digital content. This approach capitalizes on instinctual behaviors, such as looking and seeing, to simplify complex operations, making GazeGen more user-friendly and widely accessible. Consider designer adjusting visual elements in digital design platform. Traditionally, this task requires manual adjustments, which can be cumbersome and time-consuming. With GazeGen, the designer simply looks at the elements Figure 2: Extended applications of gaze-driven interaction with GazeGen. (1) Real-Time Gaze Estimation: Continuous tracking of eye movements for precise gaze estimation. (2) Gaze-Driven Detection: Detecting and identifying objects based on where the user is looking. (3) Gaze-Driven Image Editing: Dynamic editing tasks such as Addition (adding objects based on the users gaze), Deletion/Replacement (removing or replacing objects based on the users gaze), Reposition (move objects by first gazing at the initial position, then the new position), and Material Transfer (change an objects style or texture by first gazing at reference object, then applying the style to the target object). (4) Gaze-Driven Video Generation: Creating and manipulating video content driven by the users gaze. they want to adjust. The system interprets these gaze points as commands, enabling immediate and precise edits. Realtime eye interaction is crucial as it allows for seamless and intuitive control, and since everyone has different eye shapes and movements (He et al. 2019; Krafka et al. 2016; Zhang et al. 2018; Yu, Liu, and Odobez 2019; Park et al. 2019; Linden, Sjostrand, and Prouti`ere 2019; Liu et al. 2018, 2021a; Chen and Shi 2020; Liu et al. 2024), personalization is essential for accuracy. This capability not only accelerates the creative process but also makes it more inclusive, allowing anyone to express their creativity regardless of physical capabilities. At the core of GazeGen is the DFT Gaze (Distilled and Fine-Tuned Gaze) agent, an ultra-lightweight gaze estimation model designed for real-time, accurate predictions tailored to individual users. DFT Gaze captures gaze points in real time for both object retrieval and visual content manipulation. Integrating gaze estimation technology into visual content generation applications presents unique challenges, which GazeGen addresses through effective personalization for accurate gaze prediction and lightweight design. The DFT Gaze agent is designed to be adaptable and efficient, requiring minimal computational resources for real-time interactions. It learns from general gaze patterns and supports easy personalization with just few user-specific samples. With only 281K parameters, DFT Gaze is very compact, achieving performance comparable to larger models while operating 2x faster on edge devices (e.g., the Raspberry Pi 4). The lightweight and real-time capabilities of DFT Gaze enable direct manipulation of objects through eye gaze. This allows users to interact with digital content naturally and intuitively, enabling hands-free interactions in AR environments. We demonstrate the broad applications of GazeGen in Fig. 2. With advanced object detection and generative AI methods, GazeGen extends the functionality of eye gaze from simple tracking to dynamic visual content manipulation. Users can perform complex tasks such as adding, deleting, repositioning elements, and even transforming static images into videos, all through their gaze. This capability makes visual content creation accessible to everyone, regardless of physical limitations, and enhances the creative process with seamless, unobtrusive interface. To support these advanced functionalities, we begin by developing compact gaze estimation model through knowledge distillation. This process preserves the teacher models knowledge while significantly reducing computational complexity by reconstructing the teachers features using selfsupervised learning. To achieve accurate gaze estimation, we integrate Adapters into this model, allowing it to learn diverse gaze patterns and personalize predictions for individual users. With this robust gaze estimation foundation, GazeGen extends its capabilities to real-time object detection by leveraging gaze points to focus on specific regions of the image, retrieving object categories and bounding boxes. For visual content generation, GazeGen uses gaze as natural command for dynamic image editing and video creation, enabling intuitive operations such as addition, deletion, repositioning, and material transfer. This comprehensive approach allows users to seamlessly manipulate visual content through their gaze, setting new standard for accessibility and efficiency in the field. GazeGen offers new standard in gaze-driven visual content generation with the following key contributions: 1. Use of Eye Gaze for Visual Content Manipulation: We are the first to propose using eye gaze for comprehensive visual content generation and editing, such as adding, deleting, repositioning elements, material transfer, and generating videos. Additionally, GazeGen can detect and interact with objects based on where the user is looking, offering hands-free and intuitive interface for content manipulation. 2. Compact and Efficient Gaze Model: We developed the DFT Gaze agent, highly compact gaze estimation model with only 281K parameters, created through knowledge distillation coupled with masked autoencoder. Our model leverages self-supervised learning techniques to reconstruct input images and teacher network features, effectively capturing the teachers knowledge. Despite its compact size, the student model shows minimal performance drop compared to the teacher model and achieves 2x faster performance on the edge device. 3. Enhanced User Experience: GazeGen leverages natural human behaviors, providing seamless and intuitive interface for visual content manipulation. By personalizing gaze estimation with minimal samples, our system adapts to individual users, ensuring high accuracy and ease of use. 4. Broad Application Scope: We demonstrate the wide applicability of GazeGen in various scenarios. Fig. 2 illustrates the diverse potential applications of our system 1, showcasing its versatility and effectiveness."
        },
        {
            "title": "2 Preliminary\nThis section details the key components of GazeGen:\nKnowledge Distillation (KD), Adapters, and Stable Diffu-\nsion (SD). These components are foundational for advanc-\ning gaze-driven interaction. The DFT Gaze model, designed\nfor precise gaze estimation, employs KD and Adapters to\nachieve high accuracy. Integrated with SD, the DFT Gaze\nmodel constitutes the core of GazeGen, facilitating sophisti-\ncated visual editing and interaction capabilities.\nKnowledge Distillation (KD): Knowledge Distillation\ntransfers knowledge from a large, complex neural network\n(the teacher) to a smaller, more efficient one (the student).\nThis process allows the student model to perform nearly\nas well as the teacher with significantly less computational\npower. In our system, feature-based knowledge distillation\nis employed to enhance the student model by aligning its\nvisual processing abilities with those of the teacher model.\nThis alignment involves minimizing the discrepancies in\nhow both models process and interpret visual information,",
            "content": "1Text can be converted through voice. ensuring that the student model not only retains but effectively utilizes the high-level insights learned by the teacher. Adapters: Adapters are compact modules added to pretrained neural networks to enable fine-tuning for specific tasks without the need to retrain the entire model. By applying simple transformation: featurenew = featureoriginal + Adapter(featureoriginal), where featureoriginal represents the feature vector produced by the standard layers of the model, and Adapter() is the function implemented by the adapter module. Adapters adjust the models output, enhancing its task-specific performance while preserving the original network architecture. This method is efficient, leveraging pre-trained weights that already encode valuable general knowledge, thus avoiding the costly process of training from scratch. In the DFT Gaze model, adapters are introduced post knowledge distillation to fine-tune generic and personalized gaze patterns. This adaptation significantly enhances gaze estimation accuracy by tailoring the model to individual user characteristics. Stable Diffusion (SD): Stable Diffusion (SD) serves as generative engine to transform textual descriptions into visual content, specifically Text-to-Image (T2I) and Text-toVideo (T2V), valued for its flexibility and strong community support. It begins by encoding an image into latent representation z0 = E(x0) within pre-trained autoencoders latent space. The transformation process involves modifying z0 through series of diffusion steps: zt = αtz0 + ϵ (0, I), 1 αtϵ, for each step t, where αt controls the noise level. The denoising model θ() works to reverse these additions and restore the image using the textual prompt and the text encoder τ (). The network architecture of θ() features U-Net structure optimized for various resolution levels, integrating ResNet blocks, spatial self-attention, and cross-attention mechanisms to respond adaptively to the textual prompts in the image synthesis. Leveraging prior knowledge from generative models, GazeGen generates and edits high-quality visual content directed by user gaze, operating without the need for dataset finetuning. By interpreting gaze points as commands for precise edits, this method simplifies the intricate and labor-intensive nature of graphic design tasks. GazeGen accelerates the creative process and enhances inclusivity, allowing anyone to express their creativity regardless of physical capabilities."
        },
        {
            "title": "3 GazeGen\nGazeGen enhances user interaction by leveraging eye-gaze\nto generate and edit visual content. As shown in Fig. 3, the\nsystem integrates a gaze estimation agent with visual content\ngeneration techniques, dynamically adapting to the user’s\ngaze patterns. First, in Sec. 3.1, we reduce the larger, com-\nplex ConvNeXt V2 Atto (ConvNeXt V2-A) network into a\nmore compact yet effective model capable of capturing es-\nsential visual details. Next, in Sec. 3.2, we enhance this com-\npact model, now referred to as DFT Gaze, with Adapters to",
            "content": "Figure 3: Gaze-driven visual content generation. This diagram shows the process starting from the users eye, where the gaze estimation agent determines the gaze point. The gaze point is used to get the editing region, which can be toggled to use either box or mask. The T2I (Textto-Image) and T2V (Text-to-Video) modules then generate visual content based on the selected editing region. The On/Off switches indicate whether the box or mask is used for gaze-driven editing. better align with individual gaze patterns. Finally, in Sec. 3.3 and 3.4, we utilize gaze predictions from the real-time gaze estimation model to dynamically detect objects and generate and modify visual content. Detailed explanations of the training and operational mechanisms of GazeGen are provided in Sec. 3.5."
        },
        {
            "title": "3.1 Self-Supervised Compact Model Distillation",
            "content": "Efficient gaze estimation is fundamental for GazeGen, given the computationally intensive tasks of visual content generation and object retrieval. These tasks necessitate an exceptionally fast and precise gaze estimation model to minimize overall latency. To address this, we developed compact model that effectively balances speed and precision, essential for facilitating smooth user interactions. Using the ConvNeXt V2-A (Woo et al. 2023) framework, known for its high performance in image classification and low overhead, we applied knowledge distillation to create student model. This streamlined version of the more complex teacher model (ConvNeXt V2-A) maintains the ability to process complex visual information effectively. The student model adopts the architecture of the teacher but with reduced complexity by reducing the channel dimensions to one-fourth in each ConvNeXt V2 Block, as depicted in Fig. 4. In the knowledge distillation phase, the student model processes masked input images from ImageNet-1K (Deng et al. 2009), aiming to reconstruct both the original images and the teachers intermediate features . This approach allows the student model to emulate the teachers deep understanding of visual data, aligning with how the teacher perceives and interprets these images. We specifically focus on reconstructing high-level features in the last two stages (l-th stage, where {3, 4}) of the ConvNeXt V2-A, while the first stage uses the same weights as the teacher. This setup ensures that the student model builds on the same fundamental knowledge, allowing it to develop and process abstract concepts similarly. The dual reconstruction tasks, aligning on how data is represented and perceived, help the student model closely match the teachers advanced capabilities, even with partial inputs. Each reconstruction task is handled by distinct ConvNeXt V2 Block (Woo et al. 2023) acting as decoder, tailored to manage both image and feature reconstructions efficiently. To reconstruct the intermediate features from the teacher network, we express the decoder Ψ(z) with an input as: Ψ(z) = FC(z + Conv11(GRN(GELU(ˆz)))) where ˆz = Conv1 1(LN(DConv7 7(z))). We align the students features, , at the same stage using this decoder. The reconstruction loss, which considers both the input image and intermediate features, is defined as: , with those of the teacher, Lrecon = 1 ϕ(XK) (cid:88) (Xk ˆXk)2+ kK 1 ϕ(f l,K) (cid:88) γ l{3,4} (f l,k Ψ(f l,k))2, (cid:88) kK (1) where represents the set of masked pixels in both the original images and the corresponding feature maps. ϕ() denotes the count of these pixels in each context, and γ = 0.5 balances the loss components between image and feature reconstruction."
        },
        {
            "title": "3.2 Gaze Estimation Interpreting with Adapters\nTo achieve accurate gaze estimation tailored to individual\nusers, we enhance the streamlined model developed through\nknowledge distillation by integrating Adapters, transform-\ning it into the DFT Gaze model. This adaptation serves two\nkey purposes: 1) to learn from a comprehensive dataset that\ncaptures a wide range of gaze patterns from various partici-\npants, and 2) to tailor the model to the unique gaze dynamics\nof each user, which is critical due to individual variations in\neye anatomy and gaze behavior.\nGeneralized Gaze Estimation. In the generalized phase,\nthe DFT Gaze model uses Adapters, each consisting of\ntwo fully-connected (FC) layers with BatchNorm (BN) and\nLeakyReLU (LReLU) activation, to learn gaze variations.\nThese Adapters are specifically fine-tuned to improve re-\nsponsiveness to varied gaze data, while the rest of the model\nremains unchanged to leverage existing visual knowledge.\nThe training involves a generalized dataset (DG) contain-\ning gaze data from all participants, which is clustered into\n15 groups using K-means to address imbalances in gaze di-\nrection distributions. This clustered generalized dataset (G)\nensures that the model learns from a balanced and compre-\nhensive representation of diverse gaze behaviors, facilitating\na more uniform adaptation to different gaze patterns.",
            "content": "Figure 4: Self-supervised distillation for compact model. Using ConvNeXt V2-A (Woo et al. 2023) as the teacher network, we create downsized student network. The first stage of the student model inherits weights from the teacher, while stages 2 to 4 reduce the channel dimensions to onefourth. Distinct decoders are used to reconstruct both input images and the teachers intermediate features. The student processes masked inputs, allowing it to emulate the teachers deep understanding of visual data and align with how the teacher perceives and interprets these images. For simplicity, the diagram only illustrates the reconstruction of the teachers features to emulate knowledge. To adaptively adjust gaze features within the DFT Gaze model, Adapters are applied to modify internal features in each ConvNeXt V2 Block. The transformation is defined as: Adapter(f ) = FCup(LReLU(BN(FCdown(f )))) Here, denotes the features from the final Convolutional layer of each block. The FCdown layer initially compresses these features to quarter of their original dimension, isolating the most crucial attributes. This compression simplifies the feature space, enhancing focus during learning. Subsequently, the FCup layer restores the features to their original dimensions, allowing the model to integrate these refined features while maintaining the overall structure of the feature space. Personalized Gaze Estimation. Following the generalized phase, personalization is essential to adapt the model to each users unique gaze dynamics, considering individual differences in eye anatomy and behavior. The personalization phase focuses on fine-tuning the Adapters in the final stage of the DFT Gaze model. This fine-tuning uses personalized dataset (DP ) comprising only five personal eye gaze images per participant. To prevent overfitting and maintain the models generalization capabilities, known as avoiding model forgetting (Lange et al. 2019; Ruiz et al. 2023; Park et al. 2019; Schneider and Vlachos 2021; Liu et al. 2024), we reintroduce subset of the clustered generalized dataset (G) during this phase. This approach preserves the models robustness across diverse gaze patterns and enhances its precision for personalized gaze estimation, resulting in high accuracy for individual-specific scenarios. Table 3 presents the low angular gaze error achieved with DFT Gaze on both the AEA and OpenEDS2020 datasets."
        },
        {
            "title": "3.4 Gaze-Driven Visual Content Generation\nBeyond simply recognizing objects users are looking at, we\nask: Can we create and edit visual content using just our\neyes? GazeGen enables dynamic visual content generation\nand editing, leveraging gaze as a natural command. This\nmakes the process more efficient and closely aligned with\nuser intentions. GazeGen incorporates both gaze-driven im-\nage editing and video generation, utilizing two key diffusion\nprocesses: forward diffusion, which transforms the input im-\nage into noisy latent layers for preparation, and reverse dif-\nfusion, which refines these layers and integrates the modifi-\ncations. For clarity, latent layers result from forward diffu-\nsion, while refinement and integration are done via reverse\ndiffusion.",
            "content": "Gaze-Driven Image Editing We introduce gaze-driven operations such as Addition, Deletion/Replacement, Repositioning, and Material Transfer, facilitating intuitive editing of visual content, gaining insights from recent advancements in image editing. By leveraging gaze to obtain object masks and bounding boxes through FastSAM or SAM, and Multimodal Large Language Models (MLLM) respectively, we achieve accurate and user-aligned editing. Addition. To incorporate new objects based on the users gaze, we use MLLM (e.g., LLaVA (Liu et al. 2023)) to suggest the bounding box from the users gaze point. diffusion model then generates the object within this specified area, followed by segmentation (e.g., SAM (Kirillov et al. 2023)) to create mask that isolates its latent representation. This masked latent representation is subsequently blended with the latent space of the original image, ensuring the new object integrates naturally with the existing visual content. Deletion/Replacement. For deletion, FastSAM (Zhao et al. 2023) retrieves the object mask within the gaze-specified region. The latent layers corresponding to this region are removed and replaced with Gaussian noise. reverse diffusion process then regenerates these areas to ensure coherent image. For replacement, new object can be added to the removed area using the addition process. Repositioning. Furthermore, we can perform repositioning by aligning objects with new positions determined by tracking multiple gaze points. To facilitate this, FastSAM retrieves the object mask to define the boundaries. The segmented object is then shifted in the latent space to its new position, while the latent layers at the original position are removed and replaced with Gaussian noise. reverse diffusion process then regenerates these areas to ensure coherent background, while also refining the object boundary and seamlessly integrating it into its new location. Material Transfer. This process begins with using eye gaze to extract an object as material exemplar, which is encoded for use in diffusion model. The encoded features are integrated into the diffusion model via cross-attention layers for additional generation conditions. Concurrently, the users viewed image is converted into depth map, and another tracked gaze is used to obtain an object mask. These elements provide geometry and illumination guidance. The depth-based diffusion model offers geometric guidance, while the diffusion model uses these features and the object mask to transfer material properties onto the target object, preserving other attributes. Gaze-Driven Video Generation We extend Text-toVideo (T2V) models, by transforming users viewed image into animation. Using gaze coupled with LLaVA to suggest bounding boxes and add animated objects, we edit and animate visual content based on user gaze. This integration enables intuitive and dynamic video creation, where the users gaze directs the animation process, allowing for interactive video generation. Addition. To incorporate animated objects into T2V model using gaze, we utilize combination of backward and forward diffusion processes. Initially, the users viewed image is encoded into spatio-temporal latent codes, z1:f 0 , using pre-trained T2V model with repeated frames in the animation. For the region indicated by the bounding box, this area is replaced with Gaussian noise to prepare it for new content. The diffusion process is represented as: z1:f = αtz1:f 0 + 1 αtϵ1:f , ϵ (0, I), 0 where z1:f denotes the initial spatio-temporal latent codes, αt controls the noise level at time step t, and ϵ represents Gaussian noise. Subsequently, reverse diffusion is employed to generate animated objects that blend with the spatio-temporal latents. This process ensures that the newly introduced animated content integrates naturally into the existing visual context, resulting in cohesive and dynamic animation. Replacement. For replacement, the process begins by retrieving an object mask within the gaze-specified region using FastSAM. The latent layers corresponding to this region are then removed and replaced with Gaussian noise. reverse diffusion process regenerates these areas to ensure coherent frames. Subsequently, new animated object is added to the removed area using the addition process described above."
        },
        {
            "title": "3.5 GazeGen in Practice",
            "content": "All experiments were conducted on desktop with an Intel Core i9-13900K CPU and an Nvidia GeForce RTX 4090 GPU. Self-Supervised Compact Model Distillation. We perform knowledge distillation through self-supervised learning on the ImageNet-1K dataset (Deng et al. 2009). ConvNeXt V2 Atto (Woo et al. 2023) serves as the teacher network, utilizing the officially released checkpoint2. The reconstruction loss is calculated using Eq. (1). Gaze Estimation Interpreting with Adapters. We use L1 loss to minimize gaze prediction errors and report mean angular gaze error following prior studies (Palmero et al. 2020; Zhang et al. 2020; Cai et al. 2023). In the generalized phase, generalized dataset (DG) is clustered into 15 groups using K-means to balance gaze direction distributions, forming the clustered dataset (G). In the personalized phase, personalized dataset (DP ) with 5 personal eye gaze images per participant is supplemented by subset of to avoid model forgetting. Gaze-Driven Visual Content Generation/Detection. We leverage advanced models to achieve training-free gazedriven visual content generation and detection, enabling intuitive user interactions. For image editing, objects are added based on bounding boxes suggested by LLaVA (Liu et al. 2023), while FastSAM (Zhao et al. 2023) retrieves object masks for removal. For material transfer, we utilize diffusion model combined with geometric guidance techniques to ensure accurate transformations. The T2V diffusion model generates animations from gaze-driven inputs, enabling dynamic content creation. Additionally, YOLOv9 (Wang, Yeh, and Liao 2024) identifies and classifies objects within the scene, facilitating gaze-driven object detection."
        },
        {
            "title": "4 Experiments",
            "content": "2 https://github.com/facebookresearch/ConvNeXt-V"
        },
        {
            "title": "4.3 Gaze Estimation Latency on Edge Device\nTo enable real-time gaze estimation for quick eye interaction\nand enhance user experience, which is crucial for subsequent",
            "content": "Model #param tunable #param MPIIGaze MPIIFaceGaze AEA OpenEDS2020 GazeNet (Zhang et al. 2019) RT-Gene (Fischer, Chang, and Demiris 2018) GazeTR-Hybrid (Cheng and Lu 2022) ConvNeXt V2-A DFT Gaze 90.24M 31.67M 11.42M 3.6M 281K 90.24M 31.67M 11.42M 191.7K 14.43K 5.70 4.61 - 5.30 6. 5.76 4.66 4.00 4.29 5.17 3.01 2.03 1.71 1.94 2.14 7.51 6.02 5.44 6.90 7.82 Table 1: Comparison of state-of-the-art methods for generalized gaze estimation using within-dataset evaluation. To ensure fair comparison, we reimplement these methods and apply the same K-means clustering with 15 groups as DFT Gaze during training. We follow the original hyperparameter settings specified in these methods. Model #param tunable #param MPIIGaze MPIIFaceGaze AEA OpenEDS GazeNet (Zhang et al. 2019) RT-Gene (Fischer, Chang, and Demiris 2018) GazeTR-Hybrid (Cheng and Lu 2022) PNP-GA (Liu et al. 2021b) RUDA (Bao et al. 2022) TPGaze (Liu et al. 2024) ConvNeXt V2-A DFT Gaze 90.24M 31.67M 11.42M 119.5M 12.20M 11.82M 3.6M 281K 90.24M 31.67M 11.42M 116.9M 12.20M 125K 191.7K 14.43K 5.39 - - - - - 5.49 6.61 - 3.27 3.04 6.91 6.86 6.30 4.60 5.35 4.16 2.38 2.05 - - - 2.32 2. 6.57 4.80 3.43 - - - 5.36 5.80 Table 2: Comparison of state-of-the-art methods for personalized gaze estimation using within-dataset evaluation. To ensure fair comparison, we reimplement these methods and apply the same K-means clustering with 15 groups as DFT Gaze during training. We follow the original hyperparameter settings specified in these methods. The symbol represents source-free unsupervised domain adaptation (UDA) methods. Model #param AEA OpenEDS2020 Generalized Gaze Estimation ConvNeXt V2-A (Teacher) DFT Gaze (Student) Personalized Gaze Estimation ConvNeXt V2-A (Teacher) DFT Gaze (Student) 3.6M 281K 3.6M 281K 1.94 2.14 2.32 2.60 6.90 7.82 5.36 5.80 Table 3: Generalized and personalized gaze Estimation results. The teacher model, ConvNeXt V2-A, with 3.6M parameters, excels in both generalization and personalization, achieving superior performance across all datasets. The student model, DFT Gaze, with only 281K parameters, shows minimal performance drop, maintaining competitive levels in both settings. Despite its compact size, the student model provides robust gaze estimation within streamlined framework, demonstrating its efficiency and effectiveness. visual content generation, we tested the latency of two models, ConvNeXt V2-A (teacher) and DFT Gaze (student), on Raspberry Pi 4 with 8GB RAM. This widely-used edge device demonstrates the feasibility of deploying our model in real-world scenarios with limited computational resources. Using input eye images from the AEA dataset, we evaluated each model on 1,000 images. As shown in Fig. 8, ConvNeXt V2-A exhibits an average latency of 928.84 milliseconds (ms), while DFT Gaze reduces this to an average latency of 426.66 ms, making it more suitable for real-time applications on edge devices. Despite this latency reduction, DFT Gaze only shows minor performance drop, as indicated in Table 3. In knowledge distillation (KD), we streamline the student model design while retaining rich visual knowledge from the teacher model. This process allows DFT Gaze to achieve significant latency reduction without substantial loss in accuracy, making it practical solution for real-time gaze estimation on edge devices."
        },
        {
            "title": "4.4 Qualitative Results",
            "content": "In this section, we demonstrate the diverse applications of GazeGen, including real-time gaze estimation, gaze-driven detection, gaze-driven image editing, and gaze-driven video generation. Real-Time Gaze Estimation and Detection. We begin with real-time gaze estimation and gaze-driven object detection as shown in Fig. 5. The first row displays the captured users eye movements. The second row presents eye tracking in real time on the left, while the right side illustrates how the system performs gaze-driven object detection, identifying one or multiple items based on the users gaze. Gaze-Driven Image Editing. Next, we present results from various gaze-driven image editing tasks as shown in Fig. 6. Addition: The first row shows how objects like lantern, basket, or photo are added to the scene based on where the user looks, enhancing the environment interactively. Deletion/Replacement: In the second row, objects are replaced or model relies on visible features such as the iris and pupil, which are not available when the eyes are closed. Considering previous eye images as hints could help avoid this limitation. Visual Content Generation Limitation. Despite the effectiveness of gaze-driven visual content generation, the system still faces limitations. This figure, Fig. 10, illustrates that the replaced objects do not accurately reflect the original objects 3D angle or orientation, causing visual inconsistencies. Enhancing the system to incorporate 3D modeling and perspective correction techniques could improve the accuracy of object replacements, potentially aligning them more closely with the original 3D angles and orientations. Additionally, implementing algorithms that address depth and spatial relationships could further refine the visual coherence of the generated content."
        },
        {
            "title": "6 Related Work\nKnowledge Distillation is an effective compression tech-\nnique that reduces model size by transferring knowledge\nfrom a deep network (teacher) to a lightweight network (stu-\ndent), enhancing inference speed while maintaining robust\nperformance. Knowledge distillation can be categorized into\nlogit distillation (Zhang, Xiang, and Lu 2018; Furlanello\net al. 2018; Cho and Hariharan 2019; Mirzadeh et al. 2020;\nZhao et al. 2022) and intermediate representation distilla-\ntion (Romero et al. 2015; Kim, Park, and Kwak 2018; Heo\net al. 2019a,b; Tian, Krishnan, and Isola 2020; Bai et al.\n2023). Our method focuses on the latter, minimizing the dif-\nference between features from the teacher and student net-\nworks. FitNets (Romero et al. 2015) pioneered this approach\nby distilling intermediate representations. CRD (Tian, Kr-\nishnan, and Isola 2020) uses contrastive learning to trans-\nfer structural data representations, while DMAE (Bai et al.\n2023) minimizes the distance between intermediate fea-\ntures using distinct architectures for teacher and student net-\nworks. Unlike DMAE, our method downsizes the teacher\nnetwork’s architecture to create the student network and\ntransfers weights to its early stages, preserving detailed in-\nformation. We then reconstruct the teacher network’s fea-\ntures through decoders, ensuring the student model retains\nhigh-level insights learned by the teacher.\nPersonalized Gaze Estimation (He et al. 2019; Krafka et al.\n2016; Zhang et al. 2018; Yu, Liu, and Odobez 2019; Park\net al. 2019; Lind´en, Sj¨ostrand, and Prouti`ere 2019; Liu et al.\n2018, 2021a; Chen and Shi 2020; Liu et al. 2024) tailors\ngaze predictions to individual variations using a minimal set\nof personal gaze images, typically referred to as calibration\npoints. This personalization enables precise mapping of gaze\npredictions to an individual’s unique gaze patterns. In con-\ntrast, person-independent gaze models (referred to as gen-\neralized models in this paper) often yield low accuracies,\nexhibiting significant variability and person-dependent bi-\nases. For instance, SAGE (He et al. 2019) employs an un-\nsupervised personalization approach for 2D gaze estima-\ntion, using unlabeled facial images and requiring at most\nfive calibration points. Liu et al. (Liu et al. 2018, 2021a)\ntrain a convolutional neural network to capture gaze differ-",
            "content": "(Eye Tracking) (Object Detection) Figure 5: Qualitative results on AEA dataset. First row: users eye. Second row: eye tracking (left) and gaze-driven object detection (right). Predicted gaze (green), ground-truth gaze (red). Best viewed in Acrobat Reader; click images to play animations. removed, such as swapping out items for curtain, aquarium, or galaxy. This demonstrates the systems ability to dynamically transform the visual context. Reposition: The third row illustrates repositioning, where objects like wall decoration are moved to new locations, such as the upper left corner, books to the lower left corner, or phone moved upward, all guided by the users gaze. Material Transfer: The final row demonstrates changing the material style of objects based on the users gaze. For instance, the style of the first object seen by the user is applied to the next object they look at. Examples include applying polished wood texture to fridge, woven wicker to washing machine, or polished metal to chopping board. These changes reflect how gaze can influence the aesthetic and functional attributes of objects. Gaze-Driven Video Generation. Lastly, we demonstrate gaze-driven video generation in Fig 7, where static objects are replaced with other animated objects based on the users gaze. This application highlights the dynamic and interactive nature of the system, making scenes more engaging as the users focus changes."
        },
        {
            "title": "5 Limitations\nReal-Time Gaze Estimation Limitation. Despite DFT\nGaze achieving accurate gaze predictions, it faces challenges\nunder certain scenarios. These challenges primarily arise\nfrom: (1) Lighting Conditions: Eye images often exhibit\nbright spots or glare due to reflective surfaces caused by\nlighting (see Fig. 9, (a)). This can confuse the gaze estima-\ntion model, leading to errors in the predicted gaze. Imple-\nmenting image preprocessing methods to remove glare and\nreflections could help mitigate this issue. (2) Closed Eyes:\nWhen the user’s eyes are closed, the gaze estimation model\ncannot provide accurate predictions (see Fig. 9, (b)). The",
            "content": "Figure 6: Qualitative results for gaze-driven image editing. The tasks include: Addition (first row): Adding objects like lantern, basket, or photo. Deletion/Replacement (second row): Replacing objects with items like curtain, aquarium, or galaxy. Reposition (third row): Moving objects such as wall decoration to the upper left corner, books to the lower left corner, or phone upward. Material Transfer (last row): Changing an objects style, such as polished wood to the fridge, woven wicker to the washing machine, or polished metal to the chopping board. All edits are based on the users gaze. serene river flows gently with sparkling waves, with stones visible under the water night sky filled with twinkling stars vibrant aquarium with fish swimming gracefully Figure 7: Qualitative results for gaze-driven video generation. Objects are replaced based on users gaze with animated objects. Best viewed in Acrobat Reader; click images to play animations. Zoom in for better view. Figure 8: Model latency comparison on Raspberry Pi 4. The figure compares the latency of two gaze estimation models: ConvNeXt V2-A (Teacher) and DFT Gaze (Student). ConvNeXt V2-A shows latency of 928.84 ms, while DFT Gaze reduces latency to 426.66 ms, demonstrating its efficiency for real-time applications on edge devices. ences between pairs of eye images, which is then used to predict the gaze direction for new eye sample based on inferred differences. TPGaze (Liu et al. 2024) enhances personalization efficiency by updating small set of parameters, termed prompts, while keeping the network backbone fixed and employing meta-learning to optimize these prompts for adaptation."
        },
        {
            "title": "7 Conclusion\nThis paper introduces GazeGen, a hands-free system for vi-\nsual content generation using eye gaze, enhancing user en-\ngagement and accessibility in AR environments. At its core\nis the DFT Gaze agent, an ultra-lightweight model with\n281K parameters, delivering real-time, accurate gaze pre-\ndictions. It elevates eye gaze from basic tracking to dy-\nnamic visual manipulation, enabling tasks like adding, delet-\ning, repositioning elements, material transfer, and convert-\ning static images into videos. We developed a compact gaze\nestimation model using knowledge distillation and a masked\nautoencoder, refined with Adapters for precise, personalized\ngaze predictions. These predictions allow GazeGen to fa-\ncilitate intuitive visual content manipulation and real-time\nobject detection by targeting regions of interest indicated\nby the user’s gaze, thus enhancing responsiveness and the\ncreative process. Overall, GazeGen sets a new standard for\ngaze-driven visual content generation, positioning users as",
            "content": "(a) Lighting conditions (b) Closed eyes Figure 9: Real-time gaze estimation limitations. The figure illustrates the DFT Gazes limitations, showing deviations between predicted gaze (green) and ground-truth gaze (red) due to lighting conditions (left) and closed eyes (right). The top row shows users eye images, while the bottom row visualizes the resultant gaze discrepancies. active creators and broadening the scope of gaze-driven interfaces. References Bai, Y.; Wang, Z.; Xiao, J.; Wei, C.; Wang, H.; Yuille, A. L.; Zhou, Y.; and Xie, C. 2023. Masked Autoencoders Enable Efficient Knowledge Distillers. In CVPR, 2425624265. Bao, Y.; Liu, Y.; Wang, H.; and Lu, F. 2022. Generalizing Gaze Estimation with Rotation Consistency. In CVPR, 41974206. Cai, X.; Zeng, J.; Shan, S.; and Chen, X. 2023. SourceFree Adaptive Gaze Estimation by Uncertainty Reduction. In CVPR, 2203522045. Chen, Z.; and Shi, B. E. 2020. Offset Calibration for Appearance-Based Gaze Estimation via Gaze Decomposition. In WACV, 259268. Figure 10: Visual content generation limitation. This figure illustrates the limitations of gaze-driven visual content generation. The replaced objects do not accurately reflect the original objects 3D angle or orientation, causing visual inconsistencies. Cheng, Y.; and Lu, F. 2022. Gaze Estimation using Transformer. In ICPR, 33413347. Cho, J. H.; and Hariharan, B. 2019. On the Efficacy of Knowledge Distillation. In ICCV, 47934801. Deng, J.; Dong, W.; Socher, R.; Li, L.; Li, K.; and Fei-Fei, L. 2009. ImageNet: large-scale hierarchical image database. In CVPR, 248255. Fischer, T.; Chang, H. J.; and Demiris, Y. 2018. RT-GENE: Real-Time Eye Gaze Estimation in Natural Environments. In ECCV, volume 11214 of Lecture Notes in Computer Science, 339357. Furlanello, T.; Lipton, Z. C.; Tschannen, M.; Itti, L.; and Anandkumar, A. 2018. Born-Again Neural Networks. In ICML, 16021611. He, J.; Pham, K.; Valliappan, N.; Xu, P.; Roberts, C.; Lagun, D.; and Navalpakkam, V. 2019. On-Device Few-Shot Personalization for Real-Time Gaze Estimation. In ICCVW, 11491158. Heo, B.; Kim, J.; Yun, S.; Park, H.; Kwak, N.; and Choi, J. Y. 2019a. Comprehensive Overhaul of Feature Distillation. In ICCV, 19211930. Heo, B.; Lee, M.; Yun, S.; and Choi, J. Y. 2019b. Knowledge Transfer via Distillation of Activation Boundaries Formed by Hidden Neurons. In AAAI, 37793787. Jocher, G.; Chaurasia, A.; and Qiu, J. 2023. YOLO by Ultralytics. https://github.com/ultralytics/ultralytics. Kim, J.; Park, S.; and Kwak, N. 2018. Paraphrasing Complex Network: Network Compression via Factor Transfer. In NeurIPS, 27652774. Kirillov, A.; Mintun, E.; Ravi, N.; Mao, H.; Rolland, C.; Gustafson, L.; Xiao, T.; Whitehead, S.; Berg, A. C.; Lo, W.; Dollar, P.; and Girshick, R. B. 2023. Segment Anything. In ICCV, 39924003. Krafka, K.; Khosla, A.; Kellnhofer, P.; Kannan, H.; Bhandarkar, S. M.; Matusik, W.; and Torralba, A. 2016. Eye Tracking for Everyone. In CVPR, 21762184. Lange, M. D.; Aljundi, R.; Masana, M.; Parisot, S.; Jia, X.; Leonardis, A.; Slabaugh, G. G.; and Tuytelaars, T. 2019. Continual learning: comparative study on how to defy forgetting in classification tasks. Linden, E.; Sjostrand, J.; and Prouti`ere, A. 2019. Learning to Personalize in Appearance-Based Gaze Tracking. In ICCVW, 11401148. Liu, G.; Yu, Y.; Mora, K. A. F.; and Odobez, J. 2018. Differential Approach for Gaze Estimation with Calibration. In BMVC, 235. BMVA Press. Liu, G.; Yu, Y.; Mora, K. A. F.; and Odobez, J. 2021a. Differential Approach for Gaze Estimation. IEEE TPAMI, 43(3): 10921099. Liu, H.; Li, C.; Wu, Q.; and Lee, Y. J. 2023. Visual Instruction Tuning. In NeurIPS. Liu, H.; Qi, J.; Li, Z.; Hassanpour, M.; Wang, Y.; Plataniotis, K. N.; and Yu, Y. 2024. Test-Time Personalization with Meta Prompt for Gaze Estimation. In AAAI, 36213629. Liu, Y.; Liu, R.; Wang, H.; and Lu, F. 2021b. Generalizing Gaze Estimation with Outlier-guided Collaborative Adaptation. In ICCV, 38153824. Lv, Z.; Charron, N.; Moulon, P.; Gamino, A.; Peng, C.; Sweeney, C.; Miller, E.; Tang, H.; Meissner, J.; Dong, J.; Somasundaram, K.; Pesqueira, L.; Schwesinger, M.; Parkhi, O. M.; Gu, Q.; De Nardi, R.; Cheng, S.; Saarinen, S.; Baiyya, V.; Zou, Y.; Newcombe, R. A.; Engel, J. J.; Pan, X.; and Ren, C. Y. 2024. Aria Everyday Activities Dataset. arXiv:2402.13349. Mirzadeh, S.; Farajtabar, M.; Li, A.; Levine, N.; Matsukawa, A.; and Ghasemzadeh, H. 2020. Improved Knowledge Distillation via Teacher Assistant. In AAAI, 51915198. Palmero, C.; Sharma, A.; Behrendt, K.; Krishnakumar, K.; Komogortsev, O. V.; and Talathi, S. S. 2020. OpenEDS2020: Open Eyes Dataset. Park, S.; Mello, S. D.; Molchanov, P.; Iqbal, U.; Hilliges, O.; and Kautz, J. 2019. Few-Shot Adaptive Gaze Estimation. In ICCV, 93679376. Park, S.; Spurr, A.; and Hilliges, O. 2018. Deep Pictorial Gaze Estimation. In ECCV, 741757. Romero, A.; Ballas, N.; Kahou, S. E.; Chassang, A.; Gatta, C.; and Bengio, Y. 2015. FitNets: Hints for Thin Deep Nets. In ICLR. Ruiz, N.; Li, Y.; Jampani, V.; Pritch, Y.; Rubinstein, M.; and Aberman, K. 2023. DreamBooth: Fine Tuning Textto-Image Diffusion Models for Subject-Driven Generation. In CVPR, 2250022510. Schneider, J.; and Vlachos, M. 2021. Personalization of Deep Learning. Proceedings of the 3rd International Data Science ConferenceiDSC2020, 8996. Tian, Y.; Krishnan, D.; and Isola, P. 2020. Contrastive Representation Distillation. In ICLR. Wang, C.; Yeh, I.; and Liao, H. M. 2024. YOLOv9: Learning What You Want to Learn Using Programmable Gradient Information. Woo, S.; Debnath, S.; Hu, R.; Chen, X.; Liu, Z.; Kweon, I. S.; and Xie, S. 2023. ConvNeXt V2: Co-designing and In CVPR, Scaling ConvNets with Masked Autoencoders. 1613316142. Yu, Y.; Liu, G.; and Odobez, J. 2019. Improving Few-Shot User-Specific Gaze Adaptation via Gaze Redirection Synthesis. In CVPR, 1193711946. Zhang, X.; Huang, M. X.; Sugano, Y.; and Bulling, A. 2018. Training Person-Specific Gaze Estimators from User Interactions with Multiple Devices. In CHI, 624. Zhang, X.; Park, S.; Beeler, T.; Bradley, D.; Tang, S.; and Hilliges, O. 2020. ETH-XGaze: Large Scale Dataset for Gaze Estimation Under Extreme Head Pose and Gaze Variation. In ECCV, volume 12350, 365381. Zhang, X.; Sugano, Y.; Fritz, M.; and Bulling, A. 2019. MPIIGaze: Real-World Dataset and Deep AppearanceBased Gaze Estimation. TPAMI, 41(1): 162175. Zhang, Y.; Xiang, T.; and Lu, H. 2018. Deep Mutual Learning. In CVPR, 43204328. Zhao, B.; Cui, Q.; Song, R.; Qiu, Y.; and Liang, J. 2022. Decoupled Knowledge Distillation. In CVPR, 1194311952. Zhao, X.; Ding, W.; An, Y.; Du, Y.; Yu, T.; Li, M.; Tang, M.; and Wang, J. 2023. Fast Segment Anything."
        }
    ],
    "affiliations": [
        "Reality Labs Research, Meta",
        "New York University",
        "Harvard University"
    ]
}