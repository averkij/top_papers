{
    "paper_title": "Motion 3-to-4: 3D Motion Reconstruction for 4D Synthesis",
    "authors": [
        "Hongyuan Chen",
        "Xingyu Chen",
        "Youjia Zhang",
        "Zexiang Xu",
        "Anpei Chen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We present Motion 3-to-4, a feed-forward framework for synthesising high-quality 4D dynamic objects from a single monocular video and an optional 3D reference mesh. While recent advances have significantly improved 2D, video, and 3D content generation, 4D synthesis remains difficult due to limited training data and the inherent ambiguity of recovering geometry and motion from a monocular viewpoint. Motion 3-to-4 addresses these challenges by decomposing 4D synthesis into static 3D shape generation and motion reconstruction. Using a canonical reference mesh, our model learns a compact motion latent representation and predicts per-frame vertex trajectories to recover complete, temporally coherent geometry. A scalable frame-wise transformer further enables robustness to varying sequence lengths. Evaluations on both standard benchmarks and a new dataset with accurate ground-truth geometry show that Motion 3-to-4 delivers superior fidelity and spatial consistency compared to prior work. Project page is available at https://motion3-to-4.github.io/."
        },
        {
            "title": "Start",
            "content": "Motion 3-to-4: 3D Motion Reconstruction for 4D Synthesis Hongyuan Chen1 Xingyu Chen1 Youjia Zhang1,2 Zexiang Xu3 Anpei Chen1 1Westlake University 2HUST 3Hillbot 6 2 0 2 0 2 ] . [ 1 3 5 2 4 1 . 1 0 6 2 : r Figure 1. From single glance, Motion 3-to-4 unfolds: weaving time, shape, and movement into living 4D reality."
        },
        {
            "title": "Abstract",
            "content": "tency compared to prior work. Project page is available at https://motion3-to-4.github.io/. We present Motion 3-to-4, feed-forward framework for synthesising high-quality 4D dynamic objects from single monocular video and an optional 3D reference mesh. While recent advances have significantly improved 2D, video, and 3D content generation, 4D synthesis remains difficult due to limited training data and the inherent ambiguity of recovering geometry and motion from monocular viewpoint. Motion 3-to-4 addresses these challenges by decomposing 4D synthesis into static 3D shape generation and motion reconstruction. Using canonical reference mesh, our model learns compact motion latent representation and predicts per-frame vertex trajectories to recover complete, temporally coherent geometry. scalable frame-wise transformer further enables robustness to varying sequence lengths. Evaluations on both standard benchmarks and new dataset with accurate ground-truth geometry show that Motion 3-to-4 delivers superior fidelity and spatial consis1. Introduction The creation of high-fidelity 4D assetswhich comprehensively capture both the static shape and the dynamic motion of an object over timeis critical and highly soughtafter capability in fields like virtual reality, cinematography, robotics, and simulation. Recent advancements in 2D image, video and 3D content synthesis have revolutionized the computer graphics and computer vision communities, achieving high fidelity through scalable datasets and learning frameworks. However, 4D reconstruction and generation remain significantly more challenging due to the larger solution space and the inherent complexity of modeling spatial-temporal evolution. To address these challenges, recent research has advanced along several directions. One line of work [2, 29, 42, 82, 89, 101, 112] first generates multi-view videos from text or single-image inputs and then reconstructs 4D assets using dynamic NeRF [53] or Gaussian Splatting [30]. While conceptually straightforward, these pipelines rely on lengthy per-instance optimization and are fundamentally constrained by the view inconsistencies inherited from 2D generative models. Another line of work adopts pretrained 3D generative priors as foundation for 4D synthesis. For example, V2M4 [7] produces per-frame meshes followed by iterative mesh alignment to recover coherent temporal structure. However, similar to multi-view reconstruction methods, the optimization remains slow and susceptible to temporal artifacts. Alternatively, several approaches such as GVFD [100] and AnimateAnyMesh [86] build motion latent spaces via VAEs and apply the predicted motion to an initial geometry in feed-forward manner. These methods are efficient and can perform inference in seconds. However, VAE-based modeling typically requires large-scale and diverse training data to form well-structured latent distribution. When trained solely on the limited and narrow 4D datasets, these models struggle to capture complex motion patterns and exhibit poor generalization. Given the scarcity of high-quality 4D training data, our central idea is to reformulate 4D generation as combination of 3D shape generation and motion reconstruction. In this work, we introduce Motion 3-to-4, feed-forward framework that synthesizes 4D dynamic objects from single monocular video and an optional reference mesh. To efficiently tackle this inherently ill-posed problem, we decompose 4D generation into two more tractable components: static 3D shape encoding and dynamic motion reconstruction. Our key insight is to leverage static mesh, either provided or generated, as stable reference geometry, and to estimate per-frame 3D motion flow relative to this canonical state. Given monocular video and an optional initial mesh as input, Motion 3-to-4 first performs motion latent learning, jointly encoding the static object shape and the video context into compact motion representation. Based on this representation, motion decoder predicts vertex trajectories for queries sampled from the reference mesh, enabling accurate recovery of complete geometry and temporally coherent motion throughout the entire sequence. We highlight our contributions as follows: feed-forward 4D synthesis framework. We propose novel feed-forward pipeline that reformulates 4D object synthesis as motion reconstruction using only monocular video guidance, achieving strong generalization despite the scarcity of high-quality 4D training data. scalable architecture. We design frame-wise transformer architecture that is robust to input meshes of varying resolution and supports flexible processing of videos of arbitrary length. Benchmarks. Existing motion reconstruction and 4D generation benchmarks typically provide only multi-view renderings without accurate 3D geometry, making rendering alignment and motion evaluation difficult. To address this limitation, we introduce new motion-80 benchmark with ground-truth motion, realistic renderings, and geometry. We demonstrate the effectiveness of Motion 3-to-4 on both this new dataset and widely used benchmark [29]. 2. Related work 3D Object Generation. Early progress in 3D generation largely relied on GAN-based models [5, 6, 18, 55, 61], allows for fast object crafting for specific category. Subsequently, optimization-driven methods distill 2D generative priors, typically guided by CLIP scores [27, 90] or Score Distillation Sampling (SDS) [44, 57, 71, 74, 80, 97], or use multi-view generation followed by reconstruction [17, 26, 34, 65, 70, 91]. While conceptually simple, these methods are often time-consuming and struggle to maintain consistent geometry and appearance across views. Later methods [48, 50, 58, 64] reduce this issue through multi-view fine-tuning on large 3D datasets [12, 13], though reconstruction is still needed to obtain 3D representation. With the rapid expansion of high-quality 3D data [12, 13], recent work shifts toward direct 3D generation using diffusion-transformers that output explicit 3D representations. These models either tokenize 3D shapes into diffusable latent sets by encoding unstructured point clouds into unordered latent vectors [32, 35, 99, 104, 110], or adopt voxel-based structured latents that can be decoded into explicit 3D representations [41, 81, 84, 87]. In particular, Hunyuan3D 2.0 [110] achieves high-quality generation of 3D assets with rich geometry and appearance, forming strong foundation for our 4D asset creation. 4D Reconstruction. Structure-from-Motion (SfM) [1, 62, 66] and Simultaneous Localization and Mapping (SLAM) [11, 54, 72] have long been the foundation for 3D structure and camera pose estimation. Although effective, these approaches often struggle with dynamic sequences, leading to performance degeneracy. To improve robustness in dynamic scenarios, recent work has demonstrated progress toward general dynamic reconstruction by integrating semantic segmentation [22, 98], optical flows [79, 108], geometric constraints [31, 40, 43, 51, 52, 106], generative priors [23, 63]. Alternatively, point-map-based approaches [10, 77, 102] such as CUT3R [77] extend the feedforward 3D foundation model DUSt3R [78] to dynamic scenes by fine-tuning with dynamic datasets. This yields feedforward 4D reconstruction and can be further scaled to longer sequences [9, 85, 113], novel-view synthesis [33, 37, 76, 93], and 3D tracking [15, 46, 49, 88]. Despite recent progress, both reconstruction-based paradigms remain inherently non-generative: they cannot hallucinate geometry for occluded or unseen regions, often resulting in incomplete surfaces or missing structures. 4D Generation. Similar to 3D object generation, early 4D approaches rely on 2D generative priors from multi-view or video models [107], or introduce intermediate rigging structures [3, 14, 21, 47, 68, 69, 92, 103] with skinning-based animation [3, 8, 19, 20, 25, 39, 67, 83]. Score Distillation Sampling (SDS) [57] is also widely adopted to optimize 4D representations from multi-view or video diffusion models [2, 4, 29, 36, 38, 45, 109, 112], but often introduces artifacts. More recent works use generated multi-view videos as explicit supervision [28, 42, 82, 89, 94, 95, 101], yet still require slow per-instance optimization. L4GM [60] enables faster feed-forward 4D regression from generated multiview data, but its performance remains limited by scarce 4D data and view-inconsistent 2D priors. Current 4D generation methods primarily adopt twostage pipeline that extends powerful 3D generative models into the temporal dimension. Notable, V2M4 [7] generates meshes for each frame via pre-trained 3D generative models [87, 110], followed by post-processing step to align these meshes to form 4D mesh. This generate-then-align strategy is slow and prone to topology drift due to independently conditioned frame generations. To improve temporal consistency, ShapeGen4D [96] fine-tunes 3D generator [35] on 4D data and alleviates the misalignment issue, but still needs time-consuming post-processing. Parallel efforts such as GVFD [100] animate Trellis-based [87] 3D Gaussians via latent diffusion model that learns global temporal changes, enabling motion generation conditioned on monocular videos. However, since the training depends on rendering supervision of the scarcity of 4D data, it yields weak geometry and 3D structure. In contrast, we take an orthogonal path by combining the strengths of both 3D generation and reconstruction: we reformulate 4D generation as combination of 3D shape synthesis and motion reconstruction. By generating the entire object and taking motion synthesis as an alignment problem between surface points and video pixels, our method enables efficient motion representation learning and feedforward prediction without requiring the post-processing alignment steps. 3. Motion 3-to-4 In this paper, we aim to efficiently craft 4D assets that encompass complete shape and motion. Our key idea is to decompose the ill-posed 4D generation problem into static shape generation and dynamic motion reconstruction, enabling the recovery of complete motion flow and geometry, including both visible and unseen surfaces. To this end, our method takes monocular video as input and, optionally, an existing mesh asset of the first frame. Method Solution FF Motion Mesh Retarget FPS Consistent4D [29] SV4D [89] L4GM [60] DreamMesh4D [38] MV Gen. + 3D Rec. 3D Gen. + 4D Align V2M4 [7] 3D Gen. + 4D Align ShapeGen4D [96] GVFD [100] MV Gen. + 3D Rec. MV Gen. + 3D Rec. MV Gen. + 3D Rec. 3D Gen. + Motion Gen. Motion 3-to-4 3D Gen. + Motion Rec. 0.1 0.1 7.8 0.1 0.1 0.1 0.8 6.5 Table 1. An overview of 4D synthesis methods from monocular video. FF denotes feed-forward. FPS is averaged over 512 frames. To address the ill-pose Video-to-4D problem, early pipelines generate multi-view images or videos but suffer from view inconsistency. Following frame-wise 3D generative models avoid this issue yet require time-consuming 4D alignment. Motion-generationbased methods animate 3D generation, but their generalizability is fundamentally constrained by the limited availability of 4D training data. We incorporate static generation and motion reconstruction to learn local surface-to-pixel correspondences for efficient novel shapes and complex motions. If such mesh is unavailable, we generate one from the initial frame using pretrained 3D generative model [110]. We then estimate per-frame 3D motion flow relative to the first video frame, yielding temporally consistent 4D assets that encapsulate both shape and motion in their entirety, as shown in Figure 2. Our framework consists of two main components: 1) motion latent learning that encodes the static mesh and video frames into compact representation (Section 3.1); and 2) motion decoding that regresses per-frame point locations from queries sampled on the static mesh (Section 3.2). 3.1. Motion Latent Learning In the following, we will introduce simple yet efficient representation learning framework for 3D motion. Geometric Features. To efficiently capture 3D geometry, we first encode the reference mesh {V, F, R} with vertices V, faces F, and texture into compact latent representation. The mesh can either be user-provided or lifted directly from the video using recent image-to3D generation techniques [87, 110]. To encode shape and appearance, we uniformly sample surface points X0 = (xi, ni, ci)N i=1, where xi R3 is the 3D coordinate, ni R3 is the surface normal, and ci R3 denotes the RGB color. Our shape encoder is inspired by 3DShape2VecSet [99], and we compress the sampled points into compact 1D latent representation by performing cross attention to aggregate shape information. Specifically, we employ learnable query set RKC of fixed length K, where each query token is C-dimensional latent vector. These query tokens act as anchors that attend to and gather information from their neighboring shape samples in X0, producing the shape latent representation. ZX0 = CrossSelfAttn(A, PointEmb(X0)) (1) Figure 2. An overview of our Motion 3-to-4 framework for 4D synthesis. At the core of the framework is motionlatent learning module consisting of geometry encoder and video encoder, which jointly process the input video and sampled points. The resulting latent tokens are decoded into frame-wise 3D motion flow relative to the first video frame, producing temporally consistent 4D assets. where PointEmb(X0) : R9 RC maps the point label into high-dimensional positional embedding using an MLP. The aggregated tokens are further refined through few layers of self-attention transformer blocks to exchange context. This process embeds the mesh geometry into lowdimensional latent space, yielding shape latent ZX0 RKC that retains the essential geometric and semantic structure required for motion reconstruction. Modulation with Video Features. Next, we aggregate the geometric token ZX0 with the spatial-temporal video sequence to obtain motion representation. Specifically, we take monocular video RT HW 3 with frames as input and extract patch-level features using pretrained DINOv2 [56] encoder. These semantic features facilitate robust correspondence matching and strong generalization across diverse frames, which is crucial for maintaining consistent motion throughout the sequence. Additionally, we inject temporal embeddings into the patch tokens to make them explicitly aware of frame ordering. To reconstruct the motion for each frame, straightforward approach is to represent the entire motion as single 1D latent sequence and decode frame-wise tokens using positional and temporal queries. While this design is efficient for motions with fixed length, it cannot naturally handle motion sequences of arbitrary duration. To address this limitation, we propose to append the global shape token ZX0 to each frame token as the frame-wise motion representation, enabling flexible input lengths, as shown in Figure 2. Furthermore, we add reference positional token to explicitly distinguish the reference frame from the others, ensuring that the attention mechanism can properly leverage the reference information during propagation. With this design, we derive the final latent representation Zt for each frame t, which jointly encodes the shared geometric structure and frame-specific motion information. To distinguish motion features across frames, inspired by VGGT [75], we adopt an Alternating-Attention architecture. Let Z(0) R(K+P )C denote the initial aggregated shape and visual latent representation for frame t. For alternating attention blocks, the updates are defined as: 0 , . . . , Z(ℓ 1 2 ) (Global update) [Z(ℓ 1 2 ) (Frame-wise update) = FrameAttn(Z(ℓ 1 2 ) Z(ℓ) 1 ]=GlobalAttn(Z(ℓ1) 0 , . . . , Z(ℓ1) 1 ) ), = 0, . . . , 1 (2) After blocks, we obtain the final motion-aware repre- . We take the first sentation for each frame as Zt = Z(L) tokens as per-frame motion representation. 3.2. Motion decoding With the learned shape and motion latent representations, we decode them into explicit per-frame 3D point flows. Rather than predicting the full shape independently per4 instance [96] or attribute offsets per-frame [100], we predict per-frame motion flows relative to the reference shape, which preserves surface correspondences and ensures temporal consistency over long sequences. To achieve this, we adopt cross-attention decoder: we take set of points sampled from the reference mesh as queries and predict their positions at each time step. Specifically, we resample points from the reference mesh ˆP0 = {(xi, ni, ci)}M i=1. These points are embedded using the same PointEmb module employed in shape encoder (Section 3.1). The motion decoder then predicts the per-frame positions independently: ˆXt = MotionDecoder( ˆX0, Zt) (3) where Zt is the motion-aware latent for frame t. This design enables motion prediction at any spatial location and arbitrary time step, providing flexible and fully feed-forward 4D reconstruction framework. The decoded point features are subsequently processed through shared fully connected layer to predict the final 3D coordinates. 3.3. Training We train Motion 3-to-4 with straightforward direct supervision by minimizing the mean squared error (MSE) between the predicted and ground-truth point positions, ="
        },
        {
            "title": "1\nM × T",
            "content": "M (cid:88) (cid:88) i=1 t=1 ˆXi Xi t2 2 (4) Here, Xt and ˆXt denote the ground-truth and predicted point positions, respectively. To capture continuous motion fields, we densely sample points during training. This dense supervision encourages the model to learn fine-grained shape correspondences and ensures coherent motion across the entire mesh. 4. Experiments 4.1. Implementation Details Dataset. We curate high-quality animation dataset by filtering 16,000 objects from pool of approximately 50,000 models sourced from Objaverse [13] and ObjaverseXL [12]. Our filtering policy excludes objects with simplistic geometry (e.g., cubes, spheres) and employs Iterative Closest Point (ICP) analysis to discard sequences exhibiting trivial motion. To ensure scale consistency, we normalize each object to fit within bounding cube defined by the dimensions [0.5, 0.5]. For the video data, we render each asset at resolution of 256 256 from fixed viewpoints with uniformly sampled azimuth angles. Both the curated assets and their renderings will be publicly released. 5 Training Strategy. Our model is trained on 12-frame sequences. For each mesh, we sample = 4096 points as the shape input, which are encoded into = 64 shape latents. These latents are processed through = 16 transformer blocks to produce motion latents for decoding, and = 4096 for densely sampled ground-truth points. Training is conducted with total batch size of 256 using 8 H100 GPUs, using learning rate of 4 104. The model is trained with 60, 000 steps in roughly 1.5 days. 4.2. Evaluation Evaluation Datasets. We evaluate our method on two datasets. (1) We collect held-out set of 80 subjects from Objaverse, termed Motion-80, featuring objects with rich textures and diverse motion, which contains 64 short sequences and 16 long sequences exceeding 128 frames rendered from four orthogonal views. (2) The Consistent4D benchmark [29], which includes 7 videos of 32 frames each. Since ground-truth meshes are not available for the Consistent4D dataset, we report rendering-based metrics computed at four target novel views following the evaluation protocol in Consistent4D. Baselines. We conduct comprehensive comparisons with state-of-the-art video-to-4D methods. Our baselines include feedforward approaches that predict 3D Gaussians, i.e., L4GM [60] and GVFD [100]. We also evaluate against the optimization-based method V2M4 [7], which first generates 3D mesh for each frame and then performs temporal alignment to obtain 4D mesh. Metrics. For geometric evaluation, we follow the protocol of Shape2VecSet [99] to compute the Chamfer Distance (CD) and F-Score. This involves sampling 50,000 points from each mesh surface for comparison. However, due to the inherent ambiguity of 4D synthesis from monocular view, the scale and orientation of the generated meshes may not align with the ground truth. To address this, we apply an Iterative Closest Point (ICP) algorithm for alignment prior to evaluation. Specifically, we register the first frame of the generated sequence to the ground-truth mesh to estimate rigid transformation (rotation, scale, and translation), which is then applied to all subsequent frames in the sequence. For Gaussian-based methods, we extract the centers of all Gaussians as the surface point set and uniformly sample 50,000 points from it to ensure fair comparison. For appearance evaluation, we render the textured mesh from target viewpoints. For the Motion-80 evaluation set, we use the front view as input and the remaining 3 views for eval. For the Consistent4D dataset, we follow its original camera settings. We adopt LPIPS [105], CLIP [59], FVD [73], and DreamSim [16] to assess overall quality and temporal consistency, which are widely used in video-to-4D tasks. We adopt the V2M4 [7] evaluation protocol and furShort Sequence Long Sequence Geometry Appearance Geometry Appearance Method CD F-Score LPIPS CLIP FVD DreamSim CD F-Score LPIPS CLIP FVD DreamSim L4GM [60] 0.3561 GVFD [100] 0. V2M4 [7] Ours Ours w/m 0.3437 0.1113 0.0437 0.1269 0. 0.2318 0.3171 0.6774 0.1487 0.1664 0.1769 0. 0.8182 1120.67 0.7933 1414.21 0.8080 0.8428 1516. 1175.89 0.0921 0.9251 497.43 0.1941 0. 0.1974 0.1682 0.0614 0.3648 0.0997 0.1467 0. 1070.72 OOM 0.3719 0.1495 0.0929 OOM 0.1652 0. 0.4322 OOM 0.2031 0.1688 OOM OOM 0.7872 0. 1534.16 1264.36 0.1057 0.9224 673.03 0. OOM 0.2292 0.1967 0.0781 Table 2. Quantitative evaluation on our Motion-80 set. Results are reported for both short and long sequences. Ours w/m denotes our method initialized with the ground-truth static mesh from the first frame. Thanks to the disentangled mesh representation and sceneflowbased motion modeling, our approach capable of transforming artist-created static 3D meshes into fully dynamic 4D sequences. Method LPIPS CLIP L4GM [60] GVFD [100] V2M4 [7] Ours 0.1468 0.1789 0.1611 0.1455 0.8457 0.8278 0.8482 0.8609 FVD 1207.79 1340.78 1471.58 1260. DreamSim 0.1830 0.2009 0.1832 0.1691 Table 3. Quantitative evaluation on Consist4D benchmark. We evaluate rendering performance across 7 test cases, each containing 32 frames, rendered from 4 target novel views. ther extend it by assessing performance from novel views instead of the input viewpoint. This provides more comprehensive evaluation, as it avoids cases where method appears satisfactory from the input view but exhibits artifacts from other views. Quantitative Evaluation. On geometry. As demonstrated in Table 2 and Figure 3, our method achieves superior 4D geometric accuracy compared to all baselines, as indicated by consistent gain across both CD and F-Score metrics. Among Gaussian-based methods, L4GM fails to recover accurate geometry. Because the 3DGS representation is tailored for novel-view synthesis, the predicted Gaussians are not constrained to lie exactly on the surface [24], and the resulting point clouds exhibit floating artifacts, as shown in Figure 3. GVFD can generate reasonable surface point clouds. However, it struggles to accurately reconstruct the motion of these points over time, leading to degraded overall performance. The optimizationbased method V2M4 refines the generated 3D mesh from each frame and reconstructs plausible surface, thereby outperforming Gaussian-based approaches. Nonetheless, it still suffers from temporal inconsistency, leading to flickering and physically implausible spatial motion, resulting in low CD scores. In contrast, we animate the generated explicit 3D mesh with the reconstructed scene flow, producing temporally coherent motion through alignment between the mesh and video observations and thus achieving high geometric accuracy. This advantage is particularly evident in Ours w/m, which drives ground-truth static mesh from the first frame using our reconstructed motion and significantly outperforms all baselines, highlighting the fidelity Figure 3. Geometric comparison on the Consistent4D benchmark [29]. Through spatially consistent motion reconstruction, we obtain plausible and high-quality 3D geometry. of our motion reconstruction. Notably, Motion 3-to-4 is the only approach capable of converting artist-created static 3D meshes into dynamic 4D sequences, owing to its disentangled mesh representation and scene-flowbased motion modeling. On appearance. As shown in Table 2 and Table 3, our approach achieves better 3D content fidelity and consistency, quantitatively outperforms baselines in CLIP and DreamSim metrics. Note that L4GM is trained and evaluated on the orthogonal view, thus its rendering is biased to the evaluation protocol and may have more advantages than our method on the specific rendering perspective. However, when viewed from non-orthogonal novel viewpoints, L4GM exhibits noticeable ghosting artifacts, whereas our method continues to produce plausible and stable results. We invite the reviewer to our supplemental material for more results. Qualitative Evaluation. Figure 4 further illustrates the benefits of our reformulation. L4GM suffers from error accumulation during multi-view generation, resulting in ghosting artifacts when viewed from angles different from the input views. GVFD generates jittery, temporally inconsistent motions due to limitations in its VAE-based motion modeling that relies on large datasets to learn motion latent distribution, leading to weak generalization and discontinuous appearance in the Gaussian during movement. V2M4, 6 Figure 4. Qualitative Comparisons. We compare our method with strong baselines including GVFD [100], L4GM [60], and V2M4 [7] on our proposed Motion-80 benchmark. For fair evaluation, we render the generated 4D assets from all methods into an orthogonal novel view. Our approach produces more temporally coherent and structurally consistent motion. We invite reviewers to consult the supplemental material for animation visualization. relying on per-frame optimization, generates plausible results from the input view, but suffers from spatial discontinuities when observed from other viewpoints, failing to capture true motion. In contrast, our method combines strong pretrained 3D generator with feed-forward, generalizable motion reconstruction. This design preserves both spatial and temporal coherence, produces smooth, physically plausible motion, and achieves superior visual fidelity. 4.3. More Results Wild4D. We show few in-the-wild testing samples Figure 5. To handle out-of-domain inputs, we first apply BiRefNet [111] to automatically remove background regions on per-frame basis. We then generate an initial 3D shape using the first video frame with Hunyuan2.0 [110]. Thanks to the strong visual features from the DINO encoder and the robust geometry encoder, our method generalizes well to in-the-wild video inputs, including both real-world footage and generated animation sequences. Motion Transfer. Although our model is trained using paired videos and corresponding 3D subjects, we observe that it inherently generalizes to transferring the motion from an input video to 3D object with different shape and appearance. As shown in Figure 6, we feed the dragon video into the DINO encoder and use the chicken and robotdragon meshes as inputs to the geometry encoder. Remarkably, our method successfully transfers the neck, body and leg motions from the video to these new target objects. For shapes with geometric differences, our method is still able to successfully transfer the leg movement from the source to the target model. 7 Figure 5. In-the-Wild Video-to-4D Synthesis. Our method generalizes to diverse in-the-wild inputs, including real-world videos (top row) and generated animations (bottom row). By formulating motion reconstruction as surface-to-pixel alignment, we achieve robust local correspondence reasoning across varied shapes and motion patterns. Figure 6. Motion Transfer Example. By disentangling 4D synthesis into 3D mesh generation and motion reconstruction, our framework can animate static articulated objects with motion retargeted from videos of different sources. 5. Conclusion We presented 4D synthesis pipeline that decomposes the inherently ill-posed 4D generation problem into two tractable components: 3D shape generation and motion reconstruction. This decomposition offers several key advantages. Efficiency: Off-the-shelf 3D generative models can be directly reused for high-quality shape synthesis, while the motion branch remains lightweight, substantially reducing the scale requirements for 4D training data. Generalizability: By formulating motion reconstruction as an alignment task between surface points and video pixels, our method performs robust local correspondence reasoning, enabling strong generalization to both synthesized and Figure 7. Failure cases. (A) Vertex sticking in challenging cases. (B) Initial mesh topology not able to adapt to later motion. real-world shapes as well as diverse motion patterns. Flexibility: Our framework also supports lifting static articulated objects into dynamic 4D driven by video conditions, including motion retargeting from entirely different sources. Limitations. Our method exhibits several limitations. First, the geometry encoder operates on dense point cloud without explicitly modeling the mesh topology. As result, when different parts of the object are not clearly separated in the reference mesh, the model may produce vertex sticking artifacts, as illustrated in Figure 7(A). Second, when reconstructing motion from monocular video, our pipeline relies on the mesh generated from the first frame as reference geometry. This makes it difficult to accommodate topology changes that occur in later frames, leading to failure cases, as shown in Figure 7(B)."
        },
        {
            "title": "Acknowledgments",
            "content": "We would like to thank the members of Inception3D Lab for their helpful discussions, and Isabella Liu for sharing the Blender scripts used in rendering the teaser. This work is done with the sponsorship of TeleAI."
        },
        {
            "title": "References",
            "content": "[1] Sameer Agarwal, Yasutaka Furukawa, Noah Snavely, Ian Simon, Brian Curless, Steven Seitz, and Richard Szeliski. Building rome in day. ACM Communications, 2011. 2 [2] Sherwin Bahmani, Ivan Skorokhodov, Victor Rong, Gordon Wetzstein, Leonidas Guibas, Peter Wonka, Sergey Tulyakov, Jeong Joon Park, Andrea Tagliasacchi, and David Lindell. 4d-fy: Text-to-4d generation using hybrid score distillation sampling. In CVPR, 2024. 1, 3 [3] Ilya Baran and Jovan Popovic. Automatic rigging and animation of 3d characters. ACM Trans. on Graphics, 2007. 3 [4] Yarin Bekor, Gal Michael Harari, Or Perel, and Or Litany. Gaussian see, gaussian do: Semantic 3d motion transfer In SIGGRAPH Asia, pages 110, from multiview video. 2025. 3 [5] Eric Chan, Connor Lin, Matthew Chan, Koki Nagano, Boxiao Pan, Shalini De Mello, Orazio Gallo, Leonidas Guibas, Jonathan Tremblay, Sameh Khamis, et al. Efficient geometry-aware 3d generative adversarial networks. In CVPR, 2022. 2 [6] Anpei Chen, Ruiyang Liu, Ling Xie, Zhang Chen, Hao Su, and Jingyi Yu. Sofgan: portrait image generator with dynamic styling. ACM Trans. on Graphics, 2022. 2 [7] Jianqi Chen, Biao Zhang, Xiangjun Tang, and Peter Wonka. V2m4: 4d mesh animation reconstruction from single monocular video. In ICCV, 2025. 2, 3, 5, 6, [8] Ling-Hao Chen, Yuhong Zhang, Zixin Yin, Zhiyang Dou, Xin Chen, Jingbo Wang, Taku Komura, and Lei Zhang. Motion2motion: Cross-topology motion transfer with sparse correspondence. In SIGGRAPH Asia, 2025. 3 [9] Xingyu Chen, Yue Chen, Yuliang Xiu, Andreas Geiger, and Anpei Chen. Ttt3r: 3d reconstruction as test-time training. arXiv preprint arXiv:2509.26645, 2025. 2 [10] Xingyu Chen, Yue Chen, Yuliang Xiu, Andreas Geiger, and Anpei Chen. Easi3r: Estimating disentangled motion from dust3r without training. In ICCV, 2025. 2 [11] Andrew Davison, Ian Reid, Nicholas Molton, and Olivier Stasse. Monoslam: Real-time single camera slam. PAMI, 2007. 2 [12] Matt Deitke, Ruoshi Liu, Matthew Wallingford, Huong Ngo, Oscar Michel, Aditya Kusupati, Alan Fan, Christian Laforte, Vikram Voleti, Samir Yitzhak Gadre, et al. Objaverse-xl: universe of 10m+ 3d objects. NeurIPS, 2023. 2, 5 [13] Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs, Oscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana Ehsani, Aniruddha Kembhavi, and Ali Farhadi. Objaverse: universe of annotated 3d objects. In CVPR, 2023. 2, 5 [14] Yufan Deng, Yuhao Zhang, Chen Geng, Shangzhe Wu, and Jiajun Wu. Anymate: dataset and baselines for learning 3d object rigging. In SIGGRAPH, 2025. 3 [15] Haiwen Feng, Junyi Zhang, Qianqian Wang, Yufei Ye, Pengcheng Yu, Michael Black, Trevor Darrell, and Angjoo Kanazawa. St4rtrack: Simultaneous 4d reconstruction and tracking in the world. In ICCV, 2025. 2 [16] Stephanie Fu, Netanel Tamir, Shobhita Sundaram, Lucy Chai, Richard Zhang, Tali Dekel, and Phillip Isola. Dreamsim: learning new dimensions of human visual similarity using synthetic data. NeurIPS, 2023. 5 [17] Gege Gao, Weiyang Liu, Anpei Chen, Andreas Geiger, and Bernhard Scholkopf. Graphdreamer: Compositional 3d scene synthesis from scene graphs. In CVPR, 2024. 2 [18] Jun Gao, Tianchang Shen, Zian Wang, Wenzheng Chen, Kangxue Yin, Daiqing Li, Or Litany, Zan Gojcic, and Sanja Fidler. Get3d: generative model of high quality 3d textured shapes learned from images. NeurIPS, 2022. 2 [19] Zhiyang Guo, Jinxu Xiang, Kai Ma, Wengang Zhou, Houqiang Li, and Ran Zhang. Make-it-animatable: An efficient framework for authoring animation-ready 3d characters. In CVPR, 2025. 3 [20] Zhiyang Guo, Ori Zhang, Jax Xiang, Alan Zhao, Wengang Zhou, and Houqiang Li. Make-it-poseable: Feed-forward latent posing model for 3d humanoid character animation. arXiv preprint arXiv:2512.16767, 2025. 3 [21] Guangzhao He, Chen Geng, Shangzhe Wu, and Jiajun Wu. Category-agnostic neural object rigging. In CVPR, 2025. 3 [22] Kaiming He, Georgia Gkioxari, Piotr Dollar, and Ross Girshick. Mask R-CNN. In ICCV, 2017. 2 [23] Wenbo Hu, Xiangjun Gao, Xiaoyu Li, Sijie Zhao, Xiaodong Cun, Yong Zhang, Long Quan, and Ying Shan. Depthcrafter: Generating consistent long depth sequences for open-world videos. In CVPR, 2025. 2 [24] Binbin Huang, Zehao Yu, Anpei Chen, Andreas Geiger, and Shenghua Gao. 2d gaussian splatting for geometrically accurate radiance fields. In SIGGRAPH, 2024. 6 [25] Zehuan Huang, Haoran Feng, Yangtian Sun, Yuanchen Guo, Yanpei Cao, and Lu Sheng. Animax: Animating the inanimate in 3d with joint video-pose diffusion models. arXiv preprint arXiv:2506.19851, 2025. 3 [26] Zehuan Huang, Yuan-Chen Guo, Haoran Wang, Ran Yi, Lizhuang Ma, Yan-Pei Cao, and Lu Sheng. Mv-adapter: Multi-view consistent image generation made easy. In ICCV, 2025. 2 [27] Ajay Jain, Ben Mildenhall, Jonathan Barron, Pieter Abbeel, and Ben Poole. Zero-shot text-guided object generation with dream fields. In CVPR, 2022. [28] Yanqin Jiang, Chaohui Yu, Chenjie Cao, Fan Wang, Weiming Hu, and Jin Gao. Animate3d: Animating any 3d model with multi-view video diffusion. NeurIPS, 2024. 3 [29] Yanqin Jiang, Li Zhang, Jin Gao, Weimin Hu, and Yao Yao. Consistent4d: Consistent 360 dynamic object generation from monocular video. In ICLR, 2024. 1, 2, 3, 5, 6 [30] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkuhler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM Trans. on Graphics, 2023. 2 [31] Johannes Kopf, Xuejian Rong, and Jia-Bin Huang. Robust consistent video depth estimation. In CVPR, 2021. 2 [32] Zeqiang Lai, Yunfei Zhao, Haolin Liu, Zibo Zhao, Qingxiang Lin, Huiwen Shi, Xianghui Yang, Mingxin Yang, Shuhui Yang, Yifei Feng, et al. Hunyuan3d 2.5: Towards high-fidelity 3d assets generation with ultimate details. arXiv preprint arXiv:2506.16504, 2025. 2 [33] Jiahui Lei, Yijia Weng, Adam Harley, Leonidas Guibas, and Kostas Daniilidis. Mosca: Dynamic gaussian fusion from casual videos via 4d motion scaffolds. In CVPR, 2025. 2 [34] Jiahao Li, Hao Tan, Kai Zhang, Zexiang Xu, Fujun Luan, Yinghao Xu, Yicong Hong, Kalyan Sunkavalli, Greg Shakhnarovich, and Sai Bi. Instant3d: Fast text-to-3d with sparse-view generation and large reconstruction model. In ICLR, 2024. 2 [35] Weiyu Li, Xuanyang Zhang, Zheng Sun, Di Qi, Hao Li, Wei Cheng, Weiwei Cai, Shihao Wu, Jiarui Liu, Zihao Wang, et al. Step1x-3d: Towards high-fidelity and controllable generation of textured 3d assets. arXiv preprint arXiv:2505.07747, 2025. 2, [36] Xuan Li, Qianli Ma, Tsung-Yi Lin, Yongxin Chen, Chenfanfu Jiang, Ming-Yu Liu, and Donglai Xiang. Articulated kinematics distillation from video diffusion models. In CVPR, 2025. 3 [37] Zhengqi Li, Simon Niklaus, Noah Snavely, and Oliver Wang. Neural scene flow fields for space-time view synthesis of dynamic scenes. In CVPR, 2021. 2 [38] Zhiqi Li, Yiming Chen, and Peidong Liu. Dreammesh4d: Video-to-4d generation with sparse-controlled gaussianmesh hybrid representation. NeurIPS, 2024. 3 [39] Zizhang Li, Dor Litvak, Ruining Li, Yunzhi Zhang, Tomas Jakab, Christian Rupprecht, Shangzhe Wu, Andrea Vedaldi, and Jiajun Wu. Learning the 3d fauna of the web. In CVPR, 2024. 3 [40] Zhengqi Li, Richard Tucker, Forrester Cole, Qianqian Wang, Linyi Jin, Vickie Ye, Angjoo Kanazawa, Aleksander Holynski, and Noah Snavely. MegaSaM: accurate, fast, and robust structure and motion from casual dynamic videos. In CVPR, 2025. 2 [41] Zhihao Li, Yufei Wang, Heliang Zheng, Yihao Luo, and Bihan Wen. Sparc3d: Sparse representation and construction for high-resolution 3d shapes modeling. arXiv preprint arXiv:2505.14521, 2025. [42] Hanwen Liang, Yuyang Yin, Dejia Xu, Hanxue Liang, Zhangyang Wang, Konstantinos Plataniotis, Yao Zhao, and Yunchao Wei. Diffusion4d: Fast spatial-temporal consistent 4d generation via video diffusion models. arXiv preprint arXiv:2405.16645, 2024. 2, 3 [43] Ting-Hsuan Liao, Haowen Liu, Yiran Xu, Songwei Ge, Gengshan Yang, and Jia-Bin Huang. Pad3r: Pose-aware dynamic 3d reconstruction from casual videos. In SIGGRAPH Asia, pages 111, 2025. 2 [44] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa, Xiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler, Ming-Yu Liu, and Tsung-Yi Lin. Magic3d: Highresolution text-to-3d content creation. In CVPR, 2023. 2 [45] Huan Ling, Seung Wook Kim, Antonio Torralba, Sanja Fidler, and Karsten Kreis. Align your gaussians: Text-to-4d with dynamic 3d gaussians and composed diffusion models. In CVPR, 2024. 3 [46] Isabella Liu, Hao Su, and Xiaolong Wang. Dynamic gaussians mesh: Consistent mesh reconstruction from dynamic scenes. arXiv preprint arXiv:2404.12379, 2024. [47] Isabella Liu, Zhan Xu, Wang Yifan, Hao Tan, Zexiang Xu, Xiaolong Wang, Hao Su, and Zifan Shi. Riganything: Template-free autoregressive rigging for diverse 3d assets. ACM Trans. on Graphics, 2025. 3 [48] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tokmakov, Sergey Zakharov, and Carl Vondrick. Zero-1-to-3: Zero-shot one image to 3d object. In ICCV, 2023. 2 [49] Xinhang Liu, Yuxi Xiao, Donny Chen, Jiashi Feng, YuWing Tai, Chi-Keung Tang, and Bingyi Kang. Trace anything: Representing any video in 4d via trajectory fields. arXiv preprint arXiv:2510.13802, 2025. 2 [50] Yuan Liu, Cheng Lin, Zijiao Zeng, Xiaoxiao Long, Lingjie Liu, Taku Komura, and Wenping Wang. Syncdreamer: Generating multiview-consistent images from single-view image. In ICLR, 2024. 2 [51] Xuan Luo, Jia-Bin Huang, Richard Szeliski, Kevin Matzen, and Johannes Kopf. Consistent video depth estimation. ACM Trans. on Graphics, 2020. 2 [52] Ziqiao Ma, Xuweiyi Chen, Shoubin Yu, Sai Bi, Kai Zhang, Chen Ziwen, Sihan Xu, Jianing Yang, Zexiang Xu, Kalyan Sunkavalli, et al. 4d-lrm: Large space-time reconstruction model from and to any view at any time. arXiv preprint arXiv:2506.18890, 2025. 2 [53] Ben Mildenhall, Pratul Srinivasan, Matthew Tancik, Jonathan Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. Communications of the ACM, 2021. [54] Raul Mur-Artal, Jose Maria Martinez Montiel, and Juan Tardos. Orb-slam: versatile and accurate monocular slam system. IEEE transactions on robotics, 2015. 2 [55] Michael Niemeyer and Andreas Geiger. Giraffe: Representing scenes as compositional generative neural feature fields. In CVPR, 2021. 2 [56] Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. Transactions on Machine Learning Research. 4, 1 [57] Ben Poole, Ajay Jain, Jonathan Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d diffusion. In ICLR, 2023. 2, 3 [58] Lingteng Qiu, Guanying Chen, Xiaodong Gu, Qi Zuo, Mutian Xu, Yushuang Wu, Weihao Yuan, Zilong Dong, Liefeng Bo, and Xiaoguang Han. Richdreamer: generalizable normal-depth diffusion model for detail richness in text-to-3d. In CVPR, 2024. 2 [59] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In ICCV, 2021. 5 [60] Jiawei Ren, Cheng Xie, Ashkan Mirzaei, Karsten Kreis, Ziwei Liu, Antonio Torralba, Sanja Fidler, Seung Wook Kim, Huan Ling, et al. L4gm: Large 4d gaussian reconstruction model. NeurIPS, 2024. 3, 5, 6, 7, 2 [61] Katja Schwarz, Yiyi Liao, Michael Niemeyer, and Andreas Geiger. Graf: Generative radiance fields for 3d-aware image synthesis. NeurIPS, 2020. 2 [62] Johannes Lutz Schonberger and Jan-Michael Frahm. Structure-from-motion revisited. In CVPR, 2016. 2 [63] Jiahao Shao, Yuanbo Yang, Hongyu Zhou, Youmin Zhang, Yujun Shen, Vitor Guizilini, Yue Wang, Matteo Poggi, and Yiyi Liao. Learning temporally consistent video depth from video diffusion priors. In CVPR, 2025. 2 [64] Ruoxi Shi, Hansheng Chen, Zhuoyang Zhang, Minghua Liu, Chao Xu, Xinyue Wei, Linghao Chen, Chong Zeng, single image to consisand Hao Su. arXiv preprint tent multi-view diffusion base model. arXiv:2310.15110, 2023. 2 Zero123++: [65] Yichun Shi, Peng Wang, Jianglong Ye, Mai Long, Kejie Li, and Xiao Yang. Mvdream: Multi-view diffusion for 3d generation. In ICLR, 2024. [66] Noah Snavely, Steven Seitz, and Richard Szeliski. Photo tourism: exploring photo collections in 3d. In SIGGRAPH. 2006. 2 [67] Chaoyue Song, Xiu Li, Fan Yang, Zhongcong Xu, Jiacheng Wei, Fayao Liu, Jiashi Feng, Guosheng Lin, and Jianfeng Zhang. Puppeteer: Rig and animate your 3d models. NeurIPS, 2025. 3 [68] Chaoyue Song, Jianfeng Zhang, Xiu Li, Fan Yang, Yiwen Chen, Zhongcong Xu, Jun Hao Liew, Xiaoyang Guo, Fayao Liu, Jiashi Feng, et al. Magicarticulate: Make your 3d models articulation-ready. In CVPR, 2025. 3 [69] Keqiang Sun, Dor Litvak, Yunzhi Zhang, Hongsheng Li, Jiajun Wu, and Shangzhe Wu. Ponymation: Learning articulated 3d animal motions from unlabeled online videos. In ECCV, 2024. 3 [70] Jiaxiang Tang, Zhaoxi Chen, Xiaokang Chen, Tengfei Wang, Gang Zeng, and Ziwei Liu. Lgm: Large multi-view gaussian model for high-resolution 3d content creation. In ECCV, 2024. 2 [71] Jiaxiang Tang, Jiawei Ren, Hang Zhou, Ziwei Liu, and Gang Zeng. Dreamgaussian: Generative gaussian splatting for efficient 3d content creation. In ICLR, 2024. [72] Zachary Teed and Jia Deng. Droid-slam: Deep visual slam for monocular, stereo, and rgb-d cameras. NeurIPS, 2021. 2 [73] Thomas Unterthiner, Sjoerd Van Steenkiste, Karol Kurach, Raphael Marinier, Marcin Michalski, and Sylvain Gelly. Fvd: new metric for video generation. 5 [74] Haochen Wang, Xiaodan Du, Jiahao Li, Raymond Yeh, and Greg Shakhnarovich. Score jacobian chaining: Lifting pretrained 2d diffusion models for 3d generation. In CVPR, 2023. 2 [75] Jianyuan Wang, Minghao Chen, Nikita Karaev, Andrea Vedaldi, Christian Rupprecht, and David Novotny. Vggt: Visual geometry grounded transformer. In CVPR, 2025. 4, 1 [76] Qianqian Wang, Vickie Ye, Hang Gao, Weijia Zeng, Jake Austin, Zhengqi Li, and Angjoo Kanazawa. Shape of motion: 4d reconstruction from single video. In ICCV, 2025. 2 [77] Qianqian Wang, Yifei Zhang, Aleksander Holynski, Alexei A. Efros, and Angjoo Kanazawa. Continuous 3d perception model with persistent state. In CVPR, 2025. [78] Shuzhe Wang, Vincent Leroy, Yohann Cabon, Boris Chidlovskii, and Jerome Revaud. DUSt3R: geometric 3d vision made easy. In CVPR, 2024. 2 [79] Yihan Wang, Lahav Lipson, and Jia Deng. Sea-raft: Simple, efficient, accurate raft for optical flow. In ECCV, 2024. 2 [80] Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan Li, Hang Su, and Jun Zhu. Prolificdreamer: Highfidelity and diverse text-to-3d generation with variational score distillation. NeurIPS, 2023. 2 [81] Guanjun Wu, Jiemin Fang, Chen Yang, Sikuang Li, Taoran Yi, Jia Lu, Zanwei Zhou, Jiazhong Cen, Lingxi Xie, Xiaopeng Zhang, et al. Unilat3d: Geometry-appearance unified latents for single-stage 3d generation. arXiv preprint arXiv:2509.25079, 2025. 2 [82] Rundi Wu, Ruiqi Gao, Ben Poole, Alex Trevithick, Changxi Zheng, Jonathan Barron, and Aleksander Holynski. Cat4d: Create anything in 4d with multi-view video diffusion models. In CVPR, 2025. 2, [83] Shangzhe Wu, Ruining Li, Tomas Jakab, Christian Rupprecht, and Andrea Vedaldi. Magicpony: Learning articulated 3d animals in the wild. In CVPR, 2023. 3 [84] Shuang Wu, Youtian Lin, Feihu Zhang, Yifei Zeng, Yikang Yang, Yajie Bao, Jiachen Qian, Siyu Zhu, Xun Cao, Philip Torr, et al. Direct3d-s2: Gigascale 3d generation made easy with spatial sparse attention. arXiv preprint arXiv:2505.17412, 2025. 2 [85] Yuqi Wu, Wenzhao Zheng, Jie Zhou, and Jiwen Lu. Point3r: Streaming 3d reconstruction with explicit spatial pointer memory. arXiv preprint arXiv:2507.02863, 2025. 2 [86] Zijie Wu, Chaohui Yu, Fan Wang, and Xiang Bai. Animateanymesh: feed-forward 4d foundation model for text-driven universal mesh animation. In ICCV, 2025. 2 [87] Jianfeng Xiang, Zelong Lv, Sicheng Xu, Yu Deng, Ruicheng Wang, Bowen Zhang, Dong Chen, Xin Tong, and Jiaolong Yang. Structured 3d latents for scalable and versatile 3d generation. In CVPR, 2025. 2, 3 [88] Yuxi Xiao, Jianyuan Wang, Nan Xue, Nikita Karaev, Yuri Makarov, Bingyi Kang, Xing Zhu, Hujun Bao, Yujun Shen, and Xiaowei Zhou. Spatialtrackerv2: 3d point tracking made easy. In ICCV, 2025. 2 [89] Yiming Xie, Chun-Han Yao, Vikram Voleti, Huaizu Jiang, and Varun Jampani. Sv4d: Dynamic 3d content generation with multi-frame and multi-view consistency. In ICLR, 2025. 2, 3 [90] Jiale Xu, Xintao Wang, Weihao Cheng, Yan-Pei Cao, Ying Shan, Xiaohu Qie, and Shenghua Gao. Dream3d: Zero-shot text-to-3d synthesis using 3d shape prior and text-to-image diffusion models. In CVPR, 2023. [91] Jiale Xu, Weihao Cheng, Yiming Gao, Xintao Wang, Shenghua Gao, and Ying Shan. Instantmesh: Efficient 3d 11 [106] Zhoutong Zhang, Forrester Cole, Zhengqi Li, Noah Snavely, Michael Rubinstein, and William T. Freeman. Structure and motion from casual videos. In ECCV, 2022. 2 [107] Mingrui Zhao, Sauradip Nag, Kai Wang, Aditya Vora, Guangda Ji, Peter Chun, Ali Mahdavi-Amiri, and Hao Zhang. Advances in 4d representation: Geometry, motion, and interaction. arXiv preprint arXiv:2510.19255, 2025. 3 [108] Wang Zhao, Shaohui Liu, Hengkai Guo, Wenping Wang, and Yong-Jin Liu. Particlesfm: Exploiting dense point trajectories for localizing moving cameras in the wild. In ECCV, 2022. 2 [109] Yuyang Zhao, Zhiwen Yan, Enze Xie, Lanqing Hong, Zhenguo Li, and Gim Hee Lee. Animate124: AnimatarXiv preprint ing one image to 4d dynamic scene. arXiv:2311.14603, 2023. 3 [110] Zibo Zhao, Zeqiang Lai, Qingxiang Lin, Yunfei Zhao, Haolin Liu, Shuhui Yang, Yifei Feng, Mingxin Yang, Sheng Zhang, Xianghui Yang, et al. Hunyuan3d 2.0: Scaling diffusion models for high resolution textured 3d assets generation. arXiv preprint arXiv:2501.12202, 2025. 2, 3, 7, [111] Peng Zheng, Dehong Gao, Deng-Ping Fan, Li Liu, Jorma Laaksonen, Wanli Ouyang, and Nicu Sebe. Bilateral reference for high-resolution dichotomous image segmentation. CAAI AIR, 3:112, 2024. 7 [112] Yufeng Zheng, Xueting Li, Koki Nagano, Sifei Liu, Otmar Hilliges, and Shalini De Mello. unified approach for textand image-guided 4d scene generation. In CVPR, 2024. 2, 3 [113] Dong Zhuo, Wenzhao Zheng, Jiahe Guo, Yuqi Wu, Jie Zhou, and Jiwen Lu. Streaming 4d visual geometry transformer. arXiv preprint arXiv:2507.11539, 2025. 2 mesh generation from single image with sparse-view large reconstruction models. arXiv preprint arXiv:2404.07191, 2024. 2 [92] Zhan Xu, Yang Zhou, Evangelos Kalogerakis, Chris Landreth, and Karan Singh. Rignet: Neural rigging for articulated characters. ACM Trans. on Graphics, 2020. 3 [93] Ziyi Yang, Xinyu Gao, Wen Zhou, Shaohui Jiao, Yuqing Zhang, and Xiaogang Jin. Deformable 3d gaussians for high-fidelity monocular dynamic scene reconstruction. In CVPR, 2024. 2 [94] Zeyu Yang, Zijie Pan, Chun Gu, and Li Zhang. Diffusion2: Dynamic 3d content generation via score composition of video and multi-view diffusion models. In ICLR, 2025. [95] Chun-Han Yao, Yiming Xie, Vikram Voleti, Huaizu Jiang, and Varun Jampani. Sv4d 2.0: Enhancing spatio-temporal consistency in multi-view video diffusion for high-quality 4d generation. In ICCV, 2025. 3 [96] Jiraphon Yenphraphai, Ashkan Mirzaei, Jianqi Chen, Jiaxu Zou, Sergey Tulyakov, Raymond Yeh, Peter Wonka, and Chaoyang Wang. Shapegen4d: Towards high qualarXiv preprint ity 4d shape generation from videos. arXiv:2510.06208, 2025. 3, 5 [97] Taoran Yi, Jiemin Fang, Junjie Wang, Guanjun Wu, Lingxi Xie, Xiaopeng Zhang, Wenyu Liu, Qi Tian, and Xinggang Wang. Gaussiandreamer: Fast generation from text to 3d gaussians by bridging 2d and 3d diffusion models. In CVPR, 2024. 2 [98] Chao Yu, Zuxin Liu, Xin-Jun Liu, Fugui Xie, Yi Yang, Qi Wei, and Fei Qiao. DS-SLAM: semantic visual SLAM towards dynamic environments. In IROS, 2018. 2 [99] Biao Zhang, Jiapeng Tang, Matthias Niessner, and Peter Wonka. 3dshape2vecset: 3d shape representation for neural fields and generative diffusion models. ACM Trans. on Graphics, 2023. 2, 3, 5, 1 [100] Bowen Zhang, Sicheng Xu, Chuxin Wang, Jiaolong Yang, Feng Zhao, Dong Chen, and Baining Guo. Gaussian variation field diffusion for high-fidelity video-to-4d synthesis. In ICCV, 2025. 2, 3, 5, 6, [101] Haiyu Zhang, Xinyuan Chen, Yaohui Wang, Xihui Liu, Yunhong Wang, and Yu Qiao. 4diffusion: Multi-view video diffusion model for 4d generation. NeurIPS, 2024. 2, 3 [102] Junyi Zhang, Charles Herrmann, Junhwa Hur, Varun Jampani, Trevor Darrell, Forrester Cole, Deqing Sun, and Ming-Hsuan Yang. MonST3R: simple approach for estimating geometry in the presence of motion. In ICLR, 2025. 2 [103] Jia-Peng Zhang, Cheng-Feng Pu, Meng-Hao Guo, Yan-Pei Cao, and Shi-Min Hu. One model to rig them all: Diverse skeleton rigging with unirig. ACM Trans. on Graphics, 2025. 3 [104] Longwen Zhang, Ziyu Wang, Qixuan Zhang, Qiwei Qiu, Anqi Pang, Haoran Jiang, Wei Yang, Lan Xu, and Jingyi Yu. Clay: controllable large-scale generative model for creating high-quality 3d assets. ACM Trans. on Graphics, 2024. 2 [105] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In CVPR, 2018. 5 Motion 3-to-4: 3D Motion Reconstruction for 4D Synthesis"
        },
        {
            "title": "Supplementary Material",
            "content": "A. Implementation Details A.1. Training and Inference Details Network Architecture. Our network architecture consists of two primary components: shape encoder and motion latent block. For the shape encoder, we adopt the architecture from 3DShape2VecSet [99], which provides strong representational capacity and high compression efficiency. Specifically, we employ point cloud encoder to extract features from the canonical shape representation. The encoder takes = 4096 sampled points as input. First, the point coordinates are projected through Fourier feature embedding layer to obtain position embeddings. These embeddings are then concatenated with surface normals and color attributes, and subsequently projected to the model dimension = 768 via linear layer. To aggregate point features into compact representation, we employ QK-Norm crossattention block with 64 learnable query tokens. The resulting tokens are further processed through 4 transformer layers with self-attention mechanisms to capture inter-point relationships, ultimately producing semantically rich canonical shape representation R64768. For video encoding, we adopt frozen DINOv2-ViTB/14 model [56] to extract spatial features from each frame. Input videos are resized to 224 224 resolution and processed frame-by-frame through the DINOv2 encoder, which extracts 256 patch tokens per frame with dimension 768. For video with frames, we obtain image tokens RT 256768. These tokens are combined with fixed positional embeddings generated using sinusoidal encoding over both temporal and spatial dimensions. To further aggregate spatio-temporal information from the video, we process these embeddings through Motion 3-to-4 blocks. Following the design of VGGT [75], we adopt an alternating global-frame attention architecture consisting of 16 layers (8 global and 8 frame). Both global and frame attention layers use QK-normalization. This alternating design effectively captures both spatial and temporal dependencies while maintaining computational efficiency. To predict per-point motion trajectories, we extract 64 motion tokens from the processed video sequence. For each point in = 4096 output point clouds, we construct query using its position, normal, and color through the same embedding layer used in the shape encoder. We then apply QK-Norm cross-attention layer where the per-point queries attend to the extracted motion tokens from the corresponding frame. This cross-attention mechanism produces perpoint features RM 768 that encode motion information. Finally, two-layer MLP with GELU activation decodes these features into motion trajectories Xt RM 3 for time t. During inference, when we need to animate mesh vertices, we process them in chunks of 4096 to maintain memory efficiency. For temporal processing, we adopt chunk size of 256 frames. When dealing with videos exceeding 256 frames, we use sliding window approach with stride of 255 frames, where each window consists of the first frame concatenated with 255 subsequent frames to maintain temporal consistency. Training Configuration. We employ the AdamW optimizer with learning rate η = 4104, β1 = 0.9, β2 = 0.95, and weight decay 0.05. The learning rate follows cosine annealing schedule with 1000 warm-up steps. We train for 60,000 parameter update steps with gradient clipping at norm 1.0. To reduce memory consumption and accelerate training, we apply gradient checkpointing, FlashAttentionv2 in the xFormers library, and mixed precision strategy with BF16. Training Data and Strategy. For training data, we use videos at 256256 resolution with black backgrounds. The point clouds are uniformly sampled on the mesh surface, and crucially, we sample points at consistent barycentric coordinates within each face across all frames. This ensures temporal correspondence between points across different frames, enabling us to track each points trajectory and supervise the model using MSE loss. For the training strategy, we train on 12-frame sequences and apply temporal data augmentation. Specifically, we randomly select starting frame and then sample 12 consecutive frames with stride intervals of 1, 2, or 4 frames. This augmentation strategy enables the model to handle different initial poses and learn to predict larger motion displacements across varying temporal scales. Inference with video. For cases where only video input is provided without any mesh, we leverage Hunyuan 2.0 [110] to generate mesh based on the first frame. It is worth noting that the directly generated mesh is non-watertight and includes texture. To address the watertight issue, we apply vertex mapping technique to convert it into watertight mesh while preserving the original texture, which is essential for subsequent video-driven animation. For cases where mesh is already provided, we directly drive the mesh using generated or existing videos. A.2. Evaluation Details We utilize the official release code for evaluation. For our held-out dataset, we use the front view as the input view and the remaining three orthogonal views for evaluation. 1 We exclude the front view from evaluation because including it would be unfair to generation-based methods, since L4GM [60] can perform lossless reconstruction of the input view (i.e., the front view). In the Table 2, OOM refers to out of memory. For the GVFD [100], the official released code does not provide scripts for processing long sequences; it only includes scripts for single-video inference. Consequently, when the sequence length exceeds 128 frames, our machine encounters the out-of-memory (OOM) problem, preventing us from obtaining quantitative results for this method. A.3. Ablation Study We conduct an ablation study to explore different model architecture choices. Due to limited training resources, we train each model variant for 30,000 steps under identical settings. To evaluate the trajectory prediction capability of each variant, we report the mean squared error (MSE) of point trajectories over time on the held-out dataset in Table 4, which provides more direct measure of the models ability to track accurate motion trajectories. rame Attn Global Attn Ref oken Rec MSE 0.0055 0.0033 0.0021 0.0018 Table 4. Ablation studies of the modules of Motion 3-to-4. Rec MSE denotes the squared error averaged across time steps within the [0.5, 0.5] bounding box. B. More Results We provide several categories of additional visualization results below. We also include local video webpage in the supplementary materials to better present the results. B.1. More Results from Synthesis Video As illustrated in Figure 8, our model demonstrates strong generalization capability across diverse test cases from synthetic videos, achieving high-quality results that showcase its ability to handle various object types and motion patterns. B.2. More Results from Real-World Video As illustrated in Figure 9, despite being trained exclusively on synthetic data, our model generalizes well to real-world videos. This demonstrates the models robustness and its ability to bridge the synthetic-to-real domain gap, highlighting the effectiveness of our method. B.3. Results of Existing 3D Model As illustrated in Figure 10, our model has strong practical value as it can extend existing static 3D assets to dynamic 4D content. This capability enables flexible application scenarios, such as animating existing 3D models from various Figure 8. Additional visual results from synthesis video. Figure 9. Additional visual results from real-world video. sources, thereby significantly broadening the potential use cases of our method. Figure 10. Results of existing 3D model condition on generated video. B.4. Visual Comparison on Consist4D The Consist4D dataset [29] provides input views from various angles that differ from the frontal views in our heldout dataset, which better demonstrates the models robust2 Figure 11. More results from the held-out objaverse dataset. ness to different viewpoints. This is especially relevant for method L4GM, which is designed to work well under orthogonal views. When the input view is not orthogonal, L4GMs multi-view diffusion module fails to generate consistent results across views, leading to severe ghosting artifacts as shown in Figure 12. B.5. More Results from the Held-Out Dataset As illustrated in Figure 11, we present additional results on our held-out dataset to demonstrate that our model achieves superior visual fidelity and temporal motion coherence. These results further validate the effectiveness of our approach in generating high-quality 4D content with both realistic appearance and consistent motion over time. 3 Figure 12. Visual comparison with SOTA methods on Consist4D."
        }
    ],
    "affiliations": [
        "HUST",
        "Hillbot",
        "Westlake University"
    ]
}