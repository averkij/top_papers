{
    "paper_title": "CrispEdit: Low-Curvature Projections for Scalable Non-Destructive LLM Editing",
    "authors": [
        "Zarif Ikram",
        "Arad Firouzkouhi",
        "Stephen Tu",
        "Mahdi Soltanolkotabi",
        "Paria Rashidinejad"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "A central challenge in large language model (LLM) editing is capability preservation: methods that successfully change targeted behavior can quietly game the editing proxy and corrupt general capabilities, producing degenerate behaviors reminiscent of proxy/reward hacking. We present CrispEdit, a scalable and principled second-order editing algorithm that treats capability preservation as an explicit constraint, unifying and generalizing several existing editing approaches. CrispEdit formulates editing as constrained optimization and enforces the constraint by projecting edit updates onto the low-curvature subspace of the capability-loss landscape. At the crux of CrispEdit is expressing capability constraint via Bregman divergence, whose quadratic form yields the Gauss-Newton Hessian exactly and even when the base model is not trained to convergence. We make this second-order procedure efficient at the LLM scale using Kronecker-factored approximate curvature (K-FAC) and a novel matrix-free projector that exploits Kronecker structure to avoid constructing massive projection matrices. Across standard model-editing benchmarks, CrispEdit achieves high edit success while keeping capability degradation below 1% on average across datasets, significantly improving over prior editors."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 7 1 ] . [ 1 3 2 8 5 1 . 2 0 6 2 : r CrispEdit: Low-Curvature Projections for Scalable Non-Destructive LLM Editing Zarif Ikram, Arad Firouzkouhi, Stephen Tu, Mahdi Soltanolkotabi, Paria Rashidinejad {zikram,firouzko,stephen.tu,soltanol,paria.rashidinejad}@usc.edu University of Southern California central challenge in large language model (LLM) editing is capability preservation: methods that successfully change targeted behavior can quietly game the editing proxy and corrupt general capabilities, producing degenerate behaviors reminiscent of proxy/reward hacking. We present CrispEdit, scalable and principled second-order editing algorithm that treats capability preservation as an explicit constraint, unifying and generalizing several existing editing approaches. CrispEdit formulates editing as constrained optimization and enforces the constraint by projecting edit updates onto the low-curvature subspace of the capability-loss landscape. At the crux of CrispEdit is expressing capability constraint via Bregman divergence, whose quadratic form yields the GaussNewton Hessian exactly and even when the base model is not trained to convergence. We make this second-order procedure efficient at the LLM scale using Kronecker-factored approximate curvature (K-FAC) and novel matrix-free projector that exploits Kronecker structure to avoid constructing massive projection matrices. Across standard model-editing benchmarks, CrispEdit achieves high edit success while keeping capability degradation below 1% on average across datasets, significantly improving over prior editors. Date: February 17, 2026 Website: https://crispedit.github.io Code: https://github.com/zarifikram/CrispEdit"
        },
        {
            "title": "1 Introduction",
            "content": "Large language models (LLMs) are rapidly becoming shared backbone for knowledge work, spanning search and question answering (Gao et al., 2023b; Lewis et al., 2020), science (Jumper et al., 2021), software development (Chen, 2021), decision support (Lopez-Lira and Tang, 2023), and education (Kasneci et al., 2023). Yet every day, facts shift, new discoveries land, products ship, hallucinations or unsafe behaviors are uncovered, quickly making the models stale. Retraining from scratch is the cleanest way to absorb this drift, but it is also the most expensive and slowest. Model editing (Sinitsin et al., 2020; De Cao et al., 2021; Mitchell et al., 2022b; Wang et al., 2024c) offers practical alternative: apply targeted updates to correct fact, insert new knowledge, remove unsafe behavior, personalize the style while leaving everything else intact. In many cases, edits may appear to succeed while quietly degrading broader capabilities reminiscent of reward/proxy hacking (Gao et al., 2023a). This degradation can manifest as brittle reasoning, weaker instructionfollowing, or even broken fluency. In response, prior work 1 Figure 1 Comparison overview of CrispEdit. CrispEdit achieves strong edit reliability and generality, with and without QA context, while preserving broad base capabilities (MMLU, GSM8K, IFEval, ARC-C, TruthfulQA) on LLaMA-3-8B-Instruct. Figure 2 Geometric interpretation of CrispEdit compared to baseline editing strategies. Top left: Standard gradient descent effectively minimizes edit loss but moves perpendicular to the capability contours, resulting in high capability loss (degradation). Top right: Projecting onto the nullspace of activation covariance is overly conservative; it preserves representations but restricts the update too heavily to successfully optimize the edit loss. Bottom: CrispEdit projects the update onto the low-curvature subspace of the capability loss. This allows changes in representations to satisfy the edit while moving along the valley of the landscape to maintain general model capabilities. has introduced heuristic guardrails: restrict updates to small set of parameters (Hu et al., 2022; Yu et al., 2024), localize where the knowledge lives, (Meng et al., 2022; Yang et al., 2025b; Gu et al., 2025) or constrain representation changes (e.g., via subject-centric knowledge vectors) (Meng et al., 2023; Fang et al., 2025). Despite improvements, these methods tend to bake in strong assumptions about edit structure (e.g., explicit subjects/entities) and impose constraints in parameter or representation space that are only indirectly tied to capability preservation, resulting in poor editpreservation trade-off. Indeed, editors built on such constraints still perform poorly when tested in the wild under natural autoregressive generation, despite looking strong under unrealistic teacher-forced evaluation that scaffolds the ground-truth prefix and target length (Yang et al., 2025a). In this paper, we adopt first-principles formulation of model editing: an edit should reduce an edit loss while leaving broader capabilities effectively unchanged. Accordingly, we pose editing as constrained optimization problem that seeks to minimize the edit loss subject to negligible changes in capability loss, measured on designated capability set via distance metric (Section 2). Standard approaches that replace the constraint with soft penalty typically require nontrivial tuning and can be prohibitively expensive when the capability set is far larger than the edit set. This motivates us to ask: How to enforce capability preservation directly, without turning editing into full retraining? To address this, we introduce CrispEdit (Curvature-Restricted In-Situ Parameter Editing), scalable non-destructive editor, built around the following core ideas. 1. Preserving capabilities with low-curvature projections. core idea behind CrispEdit is that not all parameter directions are equally important for preserving models capabilities. Recent work shows that the curvature of the pretrained loss landscape can be characterized by the Hessian, which is observed to be highly anisotropic: sharp in small number of directions and flat in others (Sagun et al., 2017; Oymak et al., 2 2019; Ghorbani et al., 2019; Kalra et al., 2026). CrispEdit exploits this structure by projecting updates into low-curvature subspaces of Hessian, effectively hiding parameter movement where capabilities are minimally affected (see Figure 2 and Section 3.1). 2. Avoiding base model convergence requirement with Bregman constraint. quadratic approximation based on the standard Hessianwhich instantiates our formulation with Euclidean distancerequires assuming that the base model is trained to (near)-convergence, which is rarely satisfied in practice for modern large networks. We resolve this by measuring capability preservation using Bregman divergence. This choice yields quadratic form expressed exactly in terms of the Gauss-Newton Hessian (GNH), even when the base model is not trained to convergence, avoiding stationarity assumptions. 3. Representation constraints as restrictive special case. Our Bregman-GNH formulation also sheds light on several successful prior heuristics. We prove (see Proposition 1) popular editors such as AlphaEdit (Fang et al., 2025) and Adam-NSCL (Wang et al., 2021) solve an approximate special case of our framework, but do so within far more restrictive and lower-dimensional subspaces, leading to worse capability preservation-edit tradeoff (Figure 1). 4. Scalable matrix-free low-curvature projectors. The remaining challenge is scale: how can we efficiently compute and store curvature information for billion-parameter transformers? CrispEdit addresses this with two key ideas: (a) The resulting GNH is amenable to accurate approximations via Kronecker-factored approximate curvature (Martens and Grosse, 2015, K-FAC), which we leverage to enable efficient computation of the low-curvature projection matrix. (b) Instead of explicitly constructing low-curvature projection matrix, we introduce (Section 3.3) matrixfree projector that exploits the Kronecker eigen-structure: rotate gradients into factored eigenbasis, mask high-curvature components, and rotate back. This makes constraint-aware second-order editing feasible and enables precomputing capability curvature statistics once and reusing them across many future edits, amortizing cost and enabling batch and sequential editing. Experimental results. We evaluate CrispEdit in both smalland large-scale regimes. In controlled small-scale experiments on image classification (MNIST 7 FashionMNIST), where calculating exact curvature is feasible, we show that Hessian low-curvature projections yield the strongest capability preservation, and that K-FAC closely tracks this behavior cheaply. We then scale CrispEdit to edit LLMs (e.g., LLaMA-3-8B-Instruct and Qwen-2.5-1.5B-Instruct) and evaluate them as used in practice: edits should be reliable in standalone autoregressive generations, generalize across semantically equivalent in-scope queries, and remain local, preserving out-of-scope knowledge and broad skills such as reasoning, instruction-following, and truthfulness. We further test our method in both batch editing, where many edits are applied at once, and sequential editing, where batches of edits are applied to the model sequentially. Across settings, CrispEdit consistently improves the editcapability trade-off, achieving strong edit success while substantially reducing capability degradation, with modest compute and storage requirements."
        },
        {
            "title": "2 The Model editing problem",
            "content": "Let fθ : 7 denote model with parameters θ Θ Rp, mapping inputs to outputs Y. Model editing seeks to update pretrained (base) model fθ0 with initial parameters θ0, using provided edit target pair (x, y) Y, while preserving the existing capabilities of the base model. We formalize this as follows. Let Dcap = {(xi, yi)}n be reference dataset that serves as proxy for capabilities we wish to preserve, an exemplar of the domains on which the model should continue to perform well. We formulate capability preservation through the empirical loss i=1 Lcap(θ; Dcap) := 1 X i=1 ℓ (fθ(xi), yi), 1Using cached curvature, 3000 edits with our method takes six minutes on an NVIDIA A40 GPU. 3 where ℓ(ˆy, y) is task-appropriate loss (e.g., cross entropy). Preserving capabilities then means keeping be the edit L(θ; Dcap) close to its pre-edit value, i.e., L(θ; Dcap) L(θ0; Dcap). Let Dedit = {(xi, yi)}T dataset containing the desired edit pairs. We write Ledit(θ; Dedit) to denote an edit loss, such as the negative log-likelihood of edit outputs. Using the language of constrained optimization, natural optimization problem that expresses our desire to minimize edit loss subject to preserving capabilities is the following:2 i= min θΘ Ledit(θ) s.t. (Lcap(θ), Lcap(θ0)) ε, (1) where d(, ) is measure of distance, such as the difference between the two loss values or the Bregman divergence, and ε is small tolerance value. The above formulation is general, unifying and extending many existing model editing frameworks as we discuss in Section 5. While Problem (1) rigorously expresses our desired intent for model editing, actually solving (1), especially at LLM scale, is challenging due to the hard constraint. We note that we focus on the constrained formulation above in lieu of the standard Lagrangian relaxation to (1), namely minθΘ Ledit(θ) + λd (Lcap(θ), Lcap(θ0)). This is due to the fact that in typical operating regimes (the number of reference pairs) far exceeds (the number of edits), and the computational overhead of gradient-based optimization on the unconstrained problem can be non-trivial. We avoid this complexity by considering an alternative approach to approximating (1) based on low-curvature projections."
        },
        {
            "title": "3 CrispEdit: Curvature-Restricted In-Situ Parameter Editing",
            "content": "We now present CrispEdit for solving Problem (1). The key idea is to edit only along directions that are locally safe for maintaining capabilities as informed by the constraint. We start in Section 3.1 with simple instantiation of CrispEdit under the standard capability loss difference and derive principled curvature-restricted model-editing algorithm. Then, in Section 3.2, we leverage Bregman divergences to derive practical editing approach that scales to billion-parameter LLMs. For what follows, we assume that both the maps ˆy 7 ℓ(ˆy, y) and θ 7 fθ(x) are twice continuously differentiable over their respective domains. This immediately holds for architectures with smooth activation functions such as GeLU/SwiGLU. Furthermore, this assumption can readily be relaxed to functions that are twice differentiable except on measure zero set, such as architectures with ReLU activations; for simplicity of exposition, we omit these details."
        },
        {
            "title": "3.1 Preserving capabilities with low-curvature updates",
            "content": "We first consider the distance measure to be the standard distance d(a, b) = b. Furthermore, in this section, we assume that the base parameters θ0 are local minima of the capabilities loss Lcap(θ); we remove this assumption in Section 3.2 by using different distance measure. Applying second-order Taylor expansion θLcap(θ0) is the to the constraint in (1) yields Lcap(θ) Lcap(θ0) 1 2 Hessian of the capability loss function evaluated at θ0, and the first term in Taylor expansion is zero because θLcap(θ0) = 0. Under this setting, (1) can be approximated by optimizing the following quadratically constrained optimization problem: (θ θ0)Hcap(θ θ0), where Hcap := 2 min θΘ Ledit(θ) s.t. (θ θ0)Hcap(θ θ0) ε. (2) In the deep learning literature, it is well-understood that in typical overparameterized settings, the Hessian Hcap at the end of training is usually low-rank (Sagun et al., 2017; Oymak et al., 2019; Ghorbani et al., 2019). Thus, the ellipsoidal constraint in (2) offers many parameter directions around θ0 of low-curvature, where the capability loss Lcap remains (approximately) invariant. These low-curvature directions enable the optimization (2) to decrease the edit loss Ledit, while limiting loss of capabilities. Furthermore, compared to the Lagrange relaxation objective, the quadratic constraint offers several key advantages: (a) Strict control of capability loss: The ellipsoidal constraint can be enforced via projected gradient or trust-region methods, enabling strict control of tolerated capability degradation; we discuss this shortly. 2We will drop the dependency of the capabilities and edit losses on datasets Dedit and Dcap when it is clear from the context. 4 (b) Scalability to billion-parameter models: The second-order relaxation of the constraint forms the foundation for efficiently scaling our approach to billion-parameter LLMs leveraging Bregman divergences (cf. Section 3.2). (c) Pre-computation: The curvature model Hcap can be precomputed once and reused across many subsequent edits, amortizing cost and enabling sequential and online interventions (cf. Section 3.4). Projected low-curvature gradient descent. We can enforce the constraint in (2) by ensuring that the weight changes θ = θ θ0 are in the (approximate) null-space of the Hessian Hcap, i.e., Hcapθ 0 which is equivalent to θ Null(Hcap). sufficient condition to enforce the constraint during gradient descent is projecting the gradients to the approximate null-space of Hcap at every gradient step. Let Hcap = ΣU be the eigen-decomposition of Hcap, where Σ = diag(σ1, . . . , σp) and σ1 . . . σp 0 (recall that θ0 is locally optimal). We construct low-curvature projector by discarding the top eigenspace. Concretely, given an energy threshold γ (0, 1), let := min{r [p] Pr i=1 σi γ} denote the minimum index capturing γ-fraction of the eigenspectrum. Then, the orthogonal projection to the remaining directions U>k := [uk+1 . . . up] can be computed as: i=1 σi/Pp gproj = PγθLedit(θt), where Pγ := U>kU >k. (3) Intuitively, the projection in (3) removes the components of the edit gradient that point in the directions where capability loss is sensitive. We will refer to the subspace spanned by U>k as the γ-approximate nullspace."
        },
        {
            "title": "3.2 Gauss-Newton constraint via Bregman divergence\nIn Section 3.1 and deriving (2), we assumed that ∇θLcap(θ0) = 0. However, in training neural networks,\nespecially LLMs, one typically does not train the network to convergence, to avoid overfitting. Moreover, the\ncapability loss can only be viewed as a mere proxy to the pretraining loss. To avoid relying on the linear term\nvanishing, we instantiate CrispEdit using a Bregman divergence that is always first-order flat at θ0.\nDefinition 1 (Bregman divergence). For a pair (x, y) and loss ℓ(·, ·), define the Bregman divergence:",
            "content": "dBreg ℓ,y (fθ(x),fθ0 (x)) := ℓ(fθ(x), y) ℓ(fθ0 (x), y) ℓ(fθ0 (x), y), fθ(x) fθ0 (x) . With this definition, we now consider distance defined as d(Lcap(θ), Lcap(θ0)) := 1 (xi)). key property of Bregman divergence is that in the second-order Taylor approximation, the gradient is zero for any fixed θ, resulting in the following (cf. Appendix B): (fθ(xi), fθ0 i=1 dBreg ℓ,yi Pn dBreg ℓ (Lcap(θ), Lcap(θ0)) 1 2 (θ θ0)Gcap(θ θ0), where Gcap is the Gauss-Newton Hessian (GNH, also referred to as the Generalized Gauss-Newton), defined (cid:2)J HˆyJ (cid:3). Here, = θfθ(x) is the networks parameter-output Jacobian, and Hˆy = 2 as Gcap := EDcap ˆyℓ is the Hessian of the loss with respect to the networks outputs, with the expectation taken empirically over the dataset Dcap. Importantly, Gcap is well-behaved for overparameterized and partially trained networks, and lends itself to reliable and scalable approximations which we explore below. Connections to existing model editing methods. It turns out many existing heuristic model editing methods can be viewed as solving the problem (2) via conservative approximations of the quadratic constraint, and with more restrictive assumptions. For example, the popular AlphaEdit technique (Fang et al., 2025) (and related methods like Adam-NSCL (Wang et al., 2021)) can be viewed as solving the following approximate optimization problem: min θ Ledit(θ) s.t. θ θ0 Null(Kcap). (4) Here, matrix Kcap is constructed from the so-called knowledge vectors for particular MLP layer, used for preserving capabilities in certain domains of interest. We show that AlphaEdit solves special, more restrictive problem compared to our approach; the proof can be found in Appendix C. 5 cap cap cap := [a1 l1, . . . , an l1 ) Null(Gl ] be the layer-input activations on the capability dataset, and Gl ). Proposition 1 (AlphaEdit is more conservative). Fix an MLP layer and consider updating only the weights of layer l. Let Kl be the GNH. Then, Null(Kl Unlike AlphaEdits representation-level restriction via Kcap, our method preserves capabilities through loss curvatures via Gcap. Furthermore, our approach can update multiple layers simultaneously, whereas AlphaEdit edits one layer at time; consequently, direct comparison requires matching the edited parameter subset. Proposition 1 shows that even if we artificially restrict our method to single layer l, the feasible update subspace defined by the corresponding layerwise GNH is superset of AlphaEdits layerwise subspace. We emphasize that this constraint of the form can be significantly more restrictive than our approach. In particular, Null(Kcap) can be subspace of much smaller dimension than the nullspace of the GNH. Furthermore, in contrast to the knowledge matrix, in practice, the GNH is known to be flat in many directions, e.g., due to network overparameterization (Sagun et al., 2017; Oymak et al., 2019). Therefore, the constraint in AlphaEdit can be significantly more restrictive, leading to worse tradeoff between preserving prior capabilities and applying the new edits, as evidenced by our comparative analysis in Section 4.2. cap Result: Representational constraint is restrictive special case of our formulation We prove that heuristic methods like AlphaEdit enforce updates within the nullspace of layer inputs (Kcap), which is strict subset of the nullspace of the loss curvature (Gcap) utilized by our method. Consequently, AlphaEdit solves significantly more constrained optimization problem, limiting its accessible parameter space and resulting in worse tradeoff between editing efficacy and capability preservation."
        },
        {
            "title": "3.3 K-FAC for scalable, matrix-free projections\nThe remaining obstacle is scale: Gcap is expensive to compute and represent as a matrix. To address this,\nwe approximate Gcap with Kronecker-Factored Approximate Curvature (K-FAC)3 (Martens and Grosse,\n2015; George et al., 2018). At a high level, K-FAC approximates Gcap as a block-diagonal matrix, i.e.,\nGcap ≈ blkdiag(G1\n) for a network with L layers. To describe each block-diagonal approximation,\nsuppose that layer l of an MLP computes its outputs as follows: sl = Wlal−1 and al = ϕl(sl), where\nal−1 ∈ Rdin are input activations, Wl ∈ Rdout×din are layer weights (including any bias terms), and sl ∈ Rdout\nare layer pre-activations. Let gl = ∇sl\nlog p(ˆy | x) denote the pseudo-gradients of preactivations. Then, the\nK-FAC approximation of GNH for layer l is given by:",
            "content": "cap, . . . , GL cap Gl cap (cid:2)al1a l1 (cid:3) (cid:2)glg (cid:3) := Al1 Sl. (5) Here, Al1 and Sl are uncentered covariance matrices of the activations and preactivation pseudo-gradients, respectively, with the expectation taken with respect to the capabilities dataset Dcap. This reduces the ) to O(d2 per-layer storage requirements from O(d2 in ) memory. + d2 ind out out for the γ-approximate nullspace of G(l) Matrix-free projections without forming (l) γ . Even with K-FAC approximations in place, explicitly materializing projector matrix (l) cap is memory-prohibitive. Thus, γ we now describe an efficient method to project onto the γ-approximate nullspace that does not require explicitly forming (l) γ . The key idea behind our approach is the fact that the eigenvalues/eigenvectors of Kronecker product are simply the product of the eigenvalues/eigenvectors of and . Specifically, let Al1 = UinΛinU denote the respective eigendecompositions of in Al1 and Sl1. We show in Appendix D, for weight-gradient Ql = Wl Ledit(θ), the projected gradient Qproj γ vec(Ql)) can be written as: and Sl1 = UoutΛoutU out = mat(P (l) Qproj = Uout (cid:16) (U outQlUin) (cid:17) in , (6) (cid:3) is binary mask where denotes the Hadamard (entry-wise) matrix product and Mij = 1(cid:2)λout that selects low-curvature components of the Kronecker matrix; λγ denotes the largest eigenvalue associated λγ λin 3While K-FAC approximates the Fisher information matrix, for many models, such as the transformers with softmax output and cross-entropy loss, it is equivalent to the GNH (Martens, 2020). 6 Algorithm 1 CrispEdit Require: θ0, Dcap, Dedit, number of epochs E. Ensure: Edited model parameters θ. 1: Compute K-FAC factors (Al1, Sl) for all finetuned layers on L(θ; Dcap); cache (l) out, (l) in , and projection mask (l) for each layer (computed via SVD). 2: Initialize parameters θ θ0. 3: for = 1 to do 4: 5: 6: for each minibatch Dedit do Compute gradient Ql for each fine-tuned layer l. Project gradient to Qproj Update parameters θ using Qproj (cf. Equation (6)) . 7: 8: 9: end for end for with the γ-approximate nullspace of ℓ γ projector, further reducing the storage requirement from O(d2 projection in hand, we are ready to define CrispEdit, presented in Algorithm 1. . Using this formula, one never needs to form the dindout dindout + dindout). With this ) to O(d2 in + d2 ind2 out out Result: K-FAC enables scalable matrix-free curvature projection. Directly storing the GNH or its projector is memory-prohibitive. We overcome this by approximating the GNH with K-FAC and deriving matrix-free projection update. By exploiting the Kronecker structure, we project gradients using only the eigendecompositions of the smaller factor matrices and S, avoiding the materialization of the full high-dimensional projector. This reduces memory complexity from O(d ), enabling CrispEdit to scale efficiently. + d2 ind2 out ) to O(d2 in out"
        },
        {
            "title": "3.4 Sequential editing via online projection updates",
            "content": "Up to this point, we have described CrispEdit in batch editing setting, where we assume all the edits Dedit are gathered at once, and the base model is updated to incorporate all the edits. complementary setting is one of sequential editing, where edits (single instances or batches) arrive over time and the model is updated from fθ0 in successive rounds. Here, at every round k, the goal is to preserve both the base capabilities and the earlier edits in rounds 1 to 1 applied to the model. This setting is closely connected to continual (or lifelong) learning (De Lange et al., 2021; Shi et al., 2025) and inherits its core failure mode catastrophic forgetting. Batch editing can be viewed as breadth-first, integrating many edits at once, whereas sequential editing is depth-first, repeatedly revising the model as the new edit data arrive (Yang et al., 2025b). to fθ1 , . . . , fθK Concretely, consider sequence of edit data that arrives over time in chunks: D(1) algorithm at every round sets Dedit = . naïve , and approximately solves problem (1) using Algorithm 1. edit, . . . , D(K) edit i=1D(i) edit edit, . . . , D(K) Algorithm 2 CrispEdit-Seq Require: θ0, Dcap, edits D(1) Ensure: Edited models θ1, . . . , θK (updated sequentially). 1: Compute K-FAC factors {A(l1), S(l)} on L(θ; Dcap). 2: Initialize {A(l1) 3: for = 1 to do 4: acc} {A(l1) cap , S(l) , S(l) cap}. edit acc . Solve (1) for θk with edit loss L(θ; D(k) edit by {A(l1) Compute K-FAC factors {A(l1) Aggregate K-FAC factors {A(l1) edit,k, S(l) , S(l) acc} (cf. Algorithm 1). , S(l) acc acc 5: 6: 7: end for edit,k} for D(k) acc} with {A(l1) . edit,k, S(l) edit ), using layer-wise γ-approximate nullspace projections induced edit,k} via streaming averages. 7 However, this naïve approach must keep all edits around, which can be infeasible and/or impractical for large or privacy-sensitive settings (Yao et al., 2023). To address these issues, we develop an algorithm (Algorithm 2) which sequentially maintains the requisite statistics to implement γ-approximate nullspace projection. The key idea behind Algorithm 2 is that the Al1 and Sl factors from K-FAC (cf. (5)) are memory-efficient sufficient statistics to summarize the approximate nullspace of the capability loss and the previous edit losses. By updating these statistics online after each round k, we can simultaneously minimize L(θ; D(k) edit ) while treating both capabilities and the existing edit losses as hard constraints."
        },
        {
            "title": "4.1 Comparison of various second-order constraints",
            "content": "To understand the effect of various second-order constraints on capability preservation in model editing, we consider simple setting where calculating the Hessian of the model is tractable. Since this is prohibitive for large LLMs, we use LeNet-5 (LeCun et al., 2002) as representative model. We pre-train the model to 99% test accuracy on the MNIST dataset (LeCun, 1998) and fine-tune it on the Fashion-MNIST dataset (Xiao et al., 2017). In this setting, we treat the MNIST loss as the capabilities objective, and the Fashion-MNIST loss as the edit objective. For the fine-tuning phase, we first compute the γapproximate nullspace projector of the Hessian of the pretrain loss, applying projected gradient descent (PGD) to fine-tune one hidden-layer MLP, as described in Section 3.1. To address the inaccuracy of the projector caused by parameter drift, we recalculate the γapproximate nullspace projector every time parameter changes more than 25%. To understand the trade-off curve between pre-train and fine-tune test accuracy, we sweep over range of energy threshold γ = 110k with 10 , 7]. We then compare this algorithm against runk [ 1 ning PGD onto four alternative approximate nullspaces: (a) activation covariance (cf. Adam-NSCL (Wang et al., 2021)), (b) Gauss-Newton Hessian, (c) K-FAC (Martens and Grosse, 2015), and (d) eigenvalue-corrected K-FAC (EK-FAC) (George et al., 2018). Figure 3 Tradeoff between pre-training accuracy (capability preservation) and post-training performance (edit efficacy) for different nullspace projection methods. We fine-tune LeNet-5 model pre-trained on MNIST on Fashion-MNIST in the γ-approximate nullspace of the embeddings (Adam-NSCL) Hessian along with Hessian approximations Gauss-Newton Hessian, K-FAC, and EK-FAC (CrispEdit), over range of energy thresholds γ. Our results, which illustrate the trade-off between pretrain and fine-tune performance for both the Hessianbased algorithm and the four alternatives (a)-(d), are shown in Figure 3. We highlight three findings: (i) Projecting gradient updates onto the γ-approximate nullspace of the Hessian provides an effective strategy for improving fine-tune accuracy on Fashion-MNIST while maintaining base MNIST performance. (ii) The GNH approach yields trade-off curve that is quite competitive with the Hessian approach, illustrating the efficacy of the Bregman constraint. This, however, is not the case with the activation covariance used by Adam-NSCL. (iii) Both K-FAC and EK-FAC approximate the performance of the GNH approach reasonably well. The last point (iii) is promising, as it suggests that using K-FAC when we are unable to compute the full Hessian (e.g., LLMs) is viable approach as we demonstrate next."
        },
        {
            "title": "4.2 Large-scale LLM evaluations",
            "content": "We now study scaling CrispEdit to billion-parameter LLMs, predominately focusing on LLaMA-3-8B-Instruct. We investigate the following: (i) How well can we edit the model? (ii) Do the edits generalize for different 8 Table 1 Comparison of CrispEdit with existing methods on editing LLaMA-3-8B-Instruct. Rel and Gen denote reliability and generalization. We edit 3,000 samples from three datasets, evaluate edits with WILD, and measure base capability on five benchmarks. Values that are best or within 5% of best are in bold. Data Method QA Context No Context Edited Capabilities Base Capabilities LLaMA-3-8B-Instruct MEMIT AlphaEdit Adam-NSCL LocBF-FT UltraEdit MEND FT FT Sequential LoRA LoRA Sequential CrispEdit CrispEdit-Seq LLaMA-3-8B-Instruct MEMIT AlphaEdit Adam-NSCL LocBF-FT UltraEdit MEND FT FT Sequential LoRA LoRA Sequential CrispEdit CrispEdit-Seq LLaMA-3-8B-Instruct MEMIT AlphaEdit Adam-NSCL LocBF-FT UltraEdit MEND FT FT Sequential LoRA LoRA Sequential CrispEdit CrispEdit-Seq Rel 2.1 0.1 70.1 16.6 69.5 20.0 0.0 46.8 3.6 9.1 4.4 80.5 71. 1.2 0.0 74.9 19.1 61.1 18.1 0.0 12.3 19.1 13.2 6.5 79.4 66.5 9.3 0.0 72.9 13.6 50.4 59.2 0.0 23.3 13.4 30.0 20.9 77.0 66.7 Gen 1.7 0.0 60.6 15.5 59.7 16.3 0.0 43.1 3.5 7.4 4.0 69.0 62.9 1.0 0.0 57.0 8.5 41.6 12.4 0.0 6.0 10.6 8.3 4.8 55.9 43.8 9.1 0.0 66.8 13.6 46.7 54.8 0.0 23.2 12.6 25.8 18.7 70.2 59. Rel 2.9 0.1 48.1 1.9 25.2 22.7 0.0 9.9 0.9 18.7 1.3 57.4 72.8 0.3 0.0 50.5 1.7 10.9 10.2 0.0 1.6 1.3 9.5 1.6 38.4 39.1 16.4 0.0 73.9 3.4 16.7 55.4 0.0 4.2 1.8 27.0 7.9 28.4 40.8 Gen MMLU IFEval TruthfulQA ARC-C GSM8K 2.1 0.1 39.4 2.0 22.1 17.4 0.0 8.3 1.2 7.2 0.9 50.9 60. 0.6 0.0 44.1 1.8 13.3 9.3 0.0 2.2 2.2 2.7 2.0 32.4 29.2 16.1 0.0 68.3 3.4 15.7 52.0 0.0 4.3 1.5 15.7 7.3 30.5 38.6 69.5 22.9 52.7 69.2 69.5 69.3 22.9 69.3 68.8 67.8 67.3 69.5 67.8 69.5 24.6 47.4 68.6 69.4 69.2 22.9 67.4 33.4 68.2 67.3 69.3 67.9 69.5 24.6 58.5 69.2 69.2 69.3 22.9 69.5 68.1 67.8 67.8 69.3 69.2 69.3 0.0 47.7 29.6 70.1 72.5 18.2 45.0 19.4 70.8 64.6 67.9 70. 69.3 18.6 32.9 22.8 65.0 68.6 18.2 22.7 20.4 68.8 62.4 67.5 68.5 69.3 13.6 61.6 45.3 73.2 67.7 18.2 49.4 34.5 70.7 73.8 70.5 68.8 50.7 51.3 46.3 50.8 51.6 51.8 0.0 48.7 52.8 52.0 56.0 50.5 53.6 50.7 49.6 41.5 57.1 51.3 49.2 0.0 50.4 51.3 53.4 53.9 49.5 56.6 50.7 52.3 50.2 50.2 52.0 52.4 0.0 49.2 51.8 55.4 54.4 51.8 50.4 58.0 23.5 40.5 42.0 54.0 54.5 26.0 43.0 40.5 56.0 47.0 55.0 52. 58.0 21.0 40.5 39.5 52.5 52.0 26.0 40.0 31.5 53.0 40.0 54.0 54.0 58.0 23.5 50.5 42.5 55.5 53.5 26.0 42.5 43.0 48.0 48.0 55.0 53.0 73.5 0.0 45.5 39.5 75.5 73.0 0.0 50.0 6.5 71.0 67.0 76.0 74.0 73.5 0.0 37.5 16.5 74.0 74.0 0.0 18.0 0.0 71.0 71.0 76.5 73.0 73.5 0.0 58.0 39.0 73.5 74.5 0.0 59.0 29.5 75.0 71.0 74.0 73.0 Z a t C d k Time 9h 27m 7h 19m 29m 19s 22m 15s 3m 23s 58m 20s 4m 32s 9m 17s 47m 24s 3h 12m 4m 6s 43m 36s 7h 30m 5h 56m 24m 9s 14m 40s 3m 9s 17m 42s 4m 12s 6m 45s 51m 34s 2h 16m 3m 17s 34m 39s 10h 42m 7h 37m 30m 45s 15m 47s 3m 15s 38m 36s 5m 12s 10m 13s 58m 42s 4h 54m 6m 29s 38m 47s contexts? (iii) To what extent can we preserve the model capabilities? Datasets, metrics, and evaluation. We edit the base model on 3000 samples of three standard model editing datasets: ZsRE (Levy et al., 2017), CounterFact (Meng et al., 2022), and WikiBigEdit (Thede et al., 2025). We report two standard edit metrics (De Cao et al., 2021; Yang et al., 2025a): reliability (or efficacy) asks whether the edited model produces an acceptable answer to given edit query, and generalization asks whether the effects of an edit extend to semantically related contexts. All three datasets contain rewrite prompts for efficacy evaluation, and paraphrased prompts for generalization evaluation. To measure capability degradation, we benchmark edited and base models on diverse tasks: MMLU (Hendrycks et al., 2020), IFEval (Zhou et al., 2023), TruthfulQA (Lin et al., 2022), ARC-Challenge (Clark et al., 2018), and GSM8k (Cobbe et al., 2021). An edited LM should apply the edits in conversational manner and across different contexts. Yet, due to the computational costs, prior works (Fang et al., 2025; Gu et al., 2025) typically use likelihood-based, 9 teacher-forced evaluation that leak both content and length of the ground truth, leading to overestimated performance (Yang et al., 2025a). To better capture realistic editing behavior, we follow the WILD evaluation protocol (Yang et al., 2025a) that combines context-guided autoregressive decoding of LLM responses with LLM-as-a-judge evaluation. We adopt WILD with EasyEdit (Wang et al., 2024b), evaluating prompts both with and without QA context. While we do not anticipate any real-world carry-over, we include teacher-forced evaluations in Table 3 (Appendix) for completeness. Method and baselines. We edit the base model with CrispEdit by first computing K-FAC caches on Wikipedia samples for five MLP down-projection layers, and then fine-tuning them with PGD in the γapproximate nullspace of caches (cf. Algorithm 1). We compare against range of baselines. MEMIT (Meng et al., 2023) and AlphaEdit (Fang et al., 2025) follow the locate-then-edit paradigm; Adam-NSCL (Wang et al., 2021) performs PGD in the feature covariance nullspace; UltraEdit (Gu et al., 2025) leverages sensitivity analysis with online statistics; MEND (Mitchell et al., 2022a) uses hypernetwork to predict parameter changes, FT and LoRA (Hu et al., 2022; Zhang et al., 2023) performs standard and low-rank fine-tuning, respectively; and LocBF-FT (Yang et al., 2025b) constrains fine-tuning to single, hyperparameter-tuned layer. For more details about the evaluation and baselines, see Appendix E. Key results. We report our key results in Table 1. Across all datasets, we find two consistent patterns. First, aggressive editing approachesincluding MEMIT, MEND, FT, and Adam-NSCLexhibit substantial degradation. While these methods perform well under teacher-forced evaluation (cf. Table 3, Appendix), the degraded base capabilities adversely affect their editing performance under autoregressive decoding (cf. Appendix F). Second, conservative editing strategies, which restrict updates to limited parameter subspaces, better preserve base capabilities but lead to suboptimal edited capabilities. AlphaEdit remains strong baseline of this class, yet it degrades the models base capabilities due to its limited nullspace estimate, in addition to needing additional subject-centric In comparison, CrispEdit representations. consistently tops editing performance while preserving the base capabilities nearly intact. Furthermore, it remains computationally efficient (cf. Figure 4), as it only augments standard fine-tuning with PGD. Figure 4 Runtime comparison of CrispEdit with other methods. We apply number of model editing methods to edit LLaMA-38B-Instruct on 3,000 ZsRE samples and measure the wall-clock time for execution. Result: CrispEdit achieves superior editing performance while preserving base model capabilities. Prior editing methods often trade capability preservation for edit quality. Approaches like FT and AdamNSCL can lead to substantial degradation under autoregressive decoding, while conservative methods such as AlphaEdit require pushing the energy threshold so low that the resulting nullspace becomes loose approximation, thus improving edits at the cost of base capabilities. CrispEdit consistently yields better trade-off and achieves high edit performance with nearly intact base capabilities, all the while maintaining computational efficiency via projected gradient descent fine-tuning. Ablations. We now discuss key findings from ablation experiments; results are provided in Appendix G. (i) Robustness to energy threshold γ. We vary the threshold γ from 0.5 to 0.99. Table 8 shows that CrispEdits base capability preservation is reasonably robust to the threshold, even with γ as small as 0.5. (ii) Sensitivity to the size of capability dataset n. We vary = Dcap from 10 to 100,000. Surprisingly, as Table 7 shows, CrispEdit stays robust across range of dataset sizes, maintaining strong base capability even with as few as 100 samples. This suggests CrispEdit requires only small cache to be effective. This raises question: are capability dataset needed at all? To validate the importance of capability dataset, we run standard finetuning with no projections (i.e., = 0). As Figure 5 shows, while CrispEdit is 10 Figure 5 Effect of capability dataset size on editing performance and base capability preservation. We edit LLaMA-3-8B-Instruct on 3,000 ZsRE samples using CrispEdit for range of and measure the editing performance and base capability preservation. robust to n, lack of projection yields detrimental effect on capability preservation. Furthermore, capability preservation can improve edit performance in autoregressive evaluation through maintaining fluency and reliable instruction-following during generation. (iii) Scaling to larger datasets. We increase the size of the the edit dataset, using up to 10,000 ZsRE samples. As Table 4 shows, CrispEdit scales robustly from 3,000 to 10,000 edits. In contrast, the baselines degrade or plateau at larger scales due to sequential editing, restrictive layer choices, or limited adaptation capacity. Notably, while LocBF-FT performs competitively at 3k edits, its performance drops significantly at 10k edits. This degradation stems from its restriction to single layer, which lacks the representational capacity required to manage larger-scale knowledge updates. (iv) Sensitivity to model families. We use CrispEdit to apply 3,000 ZsRE edits to Qwen-2.5-1.5B-Instruct, and compare it against strong baselines. As Table 5 shows, our method retains its advantages, achieving strong editing performances while retaining base capabilities. Takeaway: CrispEdit is robust to hyperparameter choices and scales to large-scale editing. Our ablations demonstrate that CrispEdit is highly resilient to variations in the energy threshold γ and remains effective with minimal capability cache (as few as 100 samples), though the projection mechanism itself remains essential. Unlike baselines that face capacity bottlenecks at scale (e.g., LocBF-FT), CrispEdit maintains performance up to 10,000 edits and generalizes effectively across different model architectures like Qwen-2.5-1.5B-Instruct. Sequential editing with CrispEdit-Seq. Table 1 shows that CrispEdit-Seq matches the strength of Figure 6 Consequence of scaling the number of edits up to 10,000. We edit LLaMA-3-8B-Instruct on 3,000 and 10,000 ZsRE samples using several model editing methods and measure their reliability and generality with QA context. Here, darker hue corresponds to larger editing samples. Figure 7 Evolution of CrispEditSeq performance. CrispEdit-Seq shows stronger editing performance whilst retaining previous edits. 11 CrispEdit in sequential editing. CrispEdit-Seq also reasonably matches the sequential editing performance of AlphaEdit (the strongest competitor), while retaining base capabilities nearly intact and operating 8 faster. Figure 7 shows that CrispEdit-Seq retains previously edited knowledge despite being depth-first fine-tuning method, challenging previous assumptions that depth-first methods are ill-suited for sequential model editing (Yang et al., 2025b)."
        },
        {
            "title": "5 Related work",
            "content": "Memory-based approaches employ additional memory components to store edits outside its parameters. These components can be in the form of axillary models (Dong et al., 2022; Mitchell et al., 2022b; Hartvigsen et al., 2023; Wang et al., 2024a), in-context learning (Wang et al., 2024a, WISE), low-rank adapters (Yu et al., 2024, MELO), or retrieval-based alignment (Jiang et al., 2024, LTE). Compared to these methods, CrispEdit does not assume any data, memory, or architectural augmentations for inference. Locate-then-edit based approaches aim to locate set of parameters responsible for undesired behavior and edit them. They rely on the assumption that feed-forward networks contain the knowledge in models (Geva et al., 2021, 2022; Dai et al., 2022) and precisely edit the neurons responsible for particular information. They often assume structures in the dataset such as subject or entity (Meng et al., 2022, 2023; Gupta et al., 2024; Fang et al., 2025; Pan et al., 2025) and relations (Dai et al., 2022). An exception to these is Gu et al. (2025, UltraEdit), which uses representations of the last token for its localization calculation. In contrast, CrispEdit does not assume any edit structure and does not require locating specific parameters. Hypernet-based approaches treat predicting parameters shifts as meta-learning problem and learns separate network to solve the problem. These methods take the underlying optimization problem of locatethen-edit methods and uses an hypernetwork to predict the parameter shifts, such as Mitchell et al. (2022a, MEND) solving the optimization speed of Meng et al. (2022, ROME) and Tan et al. (2024, MALMEN) solving the least squares problem of Meng et al. (2023, MEMIT). Recently, Li et al. (2025, RLEdit) treats the dual optimization problem of model stability and edit quality by treating the hypernetwork as reinforcement learning (RL) agent. Compared to these methods, CrispEdit has no additional network for predicting parameters shifts. Constrained fine-tuning approaches perform GD-based finetuning with additional constraints such as weight decay (Rawat et al., 2021, FT-L), null-space projection (Wang et al., 2021, Adam-NSCL), promptmasking (Zhang et al., 2024, FT-M), low-rank update (Yu et al., 2024, MELO) or strict layer choice (Yang et al., 2025b, LocBF-FT). CrispEdit builds on this line by combining FT-M with PGD, deriving the projection from constrained-optimization view of capability preservation leveraging the loss curvature. In this way, CrispEdit aims to reduce the amount of manual strictness (e.g., highly restrictive layer choices or aggressive update limitations) sometimes required for constrained fine-tuning baselines, while retaining the simplicity and scalability of standard fine-tuning. Closest to our method is Adam-NSCL, which applies PGD in the null space of activation covariances. We show that Adam-NSCL is special, more conservative case (Proposition 1) and CrispEdit empirically outperforms it. Continual learning (CL) is closely related to model editing that studies sequential updates while mitigating catastrophic forgetting. Existing methods broadly fall into three categories: regularization-based methods aim to preserve relevant parameters (Zenke et al., 2017), replay-based methods aim to efficiently replay past memories during training (Shin et al., 2017; Rebuffi et al., 2017), and architecture-based methods adjust model architecture on the fly (Rusu et al., 2016). Relevant to our work are curvature aware methods, most notably elastic weight consolidation (Kirkpatrick et al., 2017, EWC), which estimates old task curvature with the Fisher and adds it as penalty alongside standard loss to minimize curvature change. Relatedly, Li et al. (2024, HALRP) performs automatic rank selection with Hessian information of the loss w.r.t base weights and low rank perturbation on the weights to obtain task weights. Recently, (Gupta et al., 2024) unify different CL methods under single Bregman divergence-based objective. In comparison, CrispEdit avoids per-step auxiliary loss calculation and scales to LLM editing."
        },
        {
            "title": "6 Conclusion and future work",
            "content": "We formulate model editing as quadratically constrained optimization problem, introducing CrispEdit and its sequential variant as scalable approaches for editing billion-parameter LLMs while preserving capabilities. Our method leverages GaussNewton Hessian eigenspaces, induced by Bregman divergence constraint, to identify low-curvature directions where the capabilities loss is nearly invariant. We use K-FAC to design efficient projection onto these nullspaces, making CrispEdit practical at LLM scale. Our work opens up several exciting avenues for future work. The first direction is exploring the use of CrispEdit in other applications, such as safety (e.g., editing out harmful generation and/or hallucinations) and personalization (e.g., changing response style to suit user preferences). Another interesting direction is to utilize CrispEdit for learning interpretable models, e.g., training models to minimize some notion of model complexity such as weight sparsity, feature disentanglement, etc., subject to maintaining model capabilities. Finally, on the algorithmic side, alternative techniques for non-linear constrained optimization, such as trust-region and sequential quadratic programming methods, could enable CrispEdit to take larger, more aggressive fine-tuning steps leading to further improvements on edit capabilities while preserving base capabilities."
        },
        {
            "title": "References",
            "content": "Mark Chen. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? Try ARC, the AI2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. Damai Dai, Li Dong, Yaru Hao, Zhifang Sui, Baobao Chang, and Furu Wei. Knowledge neurons in pretrained transformers. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 84938502, 2022. Nicola De Cao, Wilker Aziz, and Ivan Titov. Editing factual knowledge in language models. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, 2021. Matthias De Lange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu Jia, Aleš Leonardis, Gregory Slabaugh, and Tinne Tuytelaars. continual learning survey: Defying forgetting in classification tasks. IEEE transactions on pattern analysis and machine intelligence, 44(7):33663385, 2021. Qingxiu Dong, Damai Dai, Yifan Song, Jingjing Xu, Zhifang Sui, and Lei Li. Calibrating factual knowledge in pretrained language models. In Findings of the Association for Computational Linguistics: EMNLP 2022, pages 59375947, 2022. Junfeng Fang, Houcheng Jiang, Kun Wang, Yunshan Ma, Jie Shi, Xiang Wang, Xiangnan He, and Tat-Seng Chua. Alphaedit: Null-space constrained model editing for language models. In The Thirteenth International Conference on Learning Representations, 2025. https://openreview.net/forum?id=HvSytvg3Jh. Leo Gao, John Schulman, and Jacob Hilton. Scaling laws for reward model overoptimization. In International Conference on Machine Learning, pages 1083510866. PMLR, 2023a. Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noach, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. The language model evaluation harness, 07 2024. https://zenodo.org/records/12608602. Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yixin Dai, Jiawei Sun, Haofen Wang, and Haofen Wang. Retrieval-augmented generation for large language models: survey. arXiv preprint arXiv:2312.10997, 2(1), 2023b. Thomas George, César Laurent, Xavier Bouthillier, Nicolas Ballas, and Pascal Vincent. Fast approximate natural gradient descent in Kronecker factored eigenbasis. Advances in neural information processing systems, 31, 2018. 13 Mor Geva, Roei Schuster, Jonathan Berant, and Omer Levy. Transformer feed-forward layers are key-value memories. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 54845495, 2021. Mor Geva, Avi Caciularu, Kevin Wang, and Yoav Goldberg. Transformer feed-forward layers build predictions by promoting concepts in the vocabulary space. In Proceedings of the 2022 conference on empirical methods in natural language processing, pages 3045, 2022. Behrooz Ghorbani, Shankar Krishnan, and Ying Xiao. An investigation into neural net optimization via Hessian eigenvalue density. In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, Proceedings of the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pages 22322241. PMLR, 0915 Jun 2019. Xiaojie Gu, Guangxu Chen, Jungang Li, Jia-Chen Gu, Xuming Hu, and Kai Zhang. Ultraedit: Training-, subject-, and memory-free lifelong editing in large language models. arXiv preprint arXiv:2505.14679, 2025. Akshat Gupta, Dev Sajnani, and Gopala Anumanchipalli. unified framework for model editing. In Findings of the Association for Computational Linguistics: EMNLP 2024, pages 1540315418, 2024. Tom Hartvigsen, Swami Sankaranarayanan, Hamid Palangi, Yoon Kim, and Marzyeh Ghassemi. Aging with grace: Lifelong model editing with discrete key-value adaptors. Advances in Neural Information Processing Systems, 36: 4793447959, 2023. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020. Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. LoRA: Low-rank adaptation of large language models. ICLR, 1(2):3, 2022. Yuxin Jiang, Yufei Wang, Chuhan Wu, Wanjun Zhong, Xingshan Zeng, Jiahui Gao, Liangyou Li, Xin Jiang, Lifeng Shang, Ruiming Tang, et al. Learning to edit: Aligning LLMs with knowledge editing. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 46894705, 2024. John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger, Kathryn Tunyasuvunakool, Russ Bates, Augustin Žídek, Anna Potapenko, et al. Highly accurate protein structure prediction with alphafold. nature, 596(7873):583589, 2021. Dayal Singh Kalra, Jean-Christophe Gagnon-Audet, Andrey Gromov, Ishita Mediratta, Kelvin Niu, Alexander Miller, and Michael Shvartsman. scalable measure of loss landscape curvature for analyzing the training dynamics of LLMs. arXiv preprint arXiv:2601.16979, 2026. Enkelejda Kasneci, Kathrin Seßler, Stefan Küchemann, Maria Bannert, Daryna Dementieva, Frank Fischer, Urs Gasser, Georg Groh, Stephan Günnemann, Eyke Hüllermeier, et al. Chatgpt for good? On opportunities and challenges of large language models for education. Learning and individual differences, 103:102274, 2023. James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcoming catastrophic forgetting in neural networks. Proceedings of the national academy of sciences, 114(13):35213526, 2017. Yann LeCun. The MNIST database of handwritten digits. http://yann. lecun. com/exdb/mnist/, 1998. Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):22782324, 2002. Omer Levy, Minjoon Seo, Eunsol Choi, and Luke Zettlemoyer. Zero-shot relation extraction via reading comprehension. In Roger Levy and Lucia Specia, editors, Proceedings of the 21st Conference on Computational Natural Language Learning (CoNLL 2017), pages 333342, Vancouver, Canada, August 2017. Association for Computational Linguistics. doi: 10.18653/v1/K17-1034. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al. Retrieval-augmented generation for knowledge-intensive NLP tasks. Advances in neural information processing systems, 33:94599474, 2020. Jiaqi Li, Yuanhao Lai, Rui Wang, Changjian Shui, Sabyasachi Sahoo, Charles Ling, Shichun Yang, Boyu Wang, Christian Gagné, and Fan Zhou. Hessian aware low-rank perturbation for order-robust continual learning. IEEE Transactions on Knowledge and Data Engineering, 36(11):63856396, 2024. 14 Zherui Li, Houcheng Jiang, Hao Chen, Baolong Bi, Zhenhong Zhou, Fei Sun, Junfeng Fang, and Xiang Wang. Reinforced lifelong editing for language models. In Forty-second International Conference on Machine Learning, 2025. https://openreview.net/forum?id=1jUXprrfcb. Stephanie Lin, Jacob Hilton, and Owain Evans. TruthfulQA: Measuring how models mimic human falsehoods. In Proceedings of the 60th annual meeting of the association for computational linguistics (volume 1: long papers), pages 32143252, 2022. Alejandro Lopez-Lira and Yuehua Tang. Can ChatGPT forecast stock price movements? Return predictability and large language models. Return Predictability and Large Language Models (April 6, 2023), 2023. James Martens. New insights and perspectives on the natural gradient method. Journal of Machine Learning Research, 21(146):176, 2020. http://jmlr.org/papers/v21/17-678.html. James Martens and Roger Grosse. Optimizing neural networks with Kronecker-factored approximate curvature. In International conference on machine learning, pages 24082417. PMLR, 2015. Kevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov. Locating and editing factual associations in GPT. Advances in neural information processing systems, 35:1735917372, 2022. Kevin Meng, Arnab Sen Sharma, Alex Andonian, Yonatan Belinkov, and David Bau. Mass-editing memory in transformer. In The Eleventh International Conference on Learning Representations, 2023. https://openreview. net/forum?id=MkbcAHIYgyS. Eric Mitchell, Charles Lin, Antoine Bosselut, Chelsea Finn, and Christopher Manning. Fast model editing at scale. In International Conference on Learning Representations, 2022a. https://openreview.net/forum?id=0DcZxeWfOPt. Eric Mitchell, Charles Lin, Antoine Bosselut, Christopher Manning, and Chelsea Finn. Memory-based model editing at scale. In International Conference on Machine Learning, pages 1581715831. PMLR, 2022b. Samet Oymak, Zalan Fabian, Mingchen Li, and Mahdi Soltanolkotabi. Generalization guarantees for neural networks via harnessing the low-rank structure of the Jacobian. arXiv preprint arXiv:1906.05392, 2019. Haowen Pan, Xiaozhi Wang, Yixin Cao, Zenglin Shi, Xun Yang, Juanzi Li, and Meng Wang. Precise localization of memories: fine-grained neuron-level knowledge editing technique for LLMs. In The Thirteenth International Conference on Learning Representations, 2025. https://openreview.net/forum?id=5xP1HDvpXI. Ankit Singh Rawat, Chen Zhu, Daliang Li, Felix Yu, Manzil Zaheer, Sanjiv Kumar, and Srinadh Bhojanapalli. Modifying memories in transformer models. In International conference on machine learning (ICML), volume 2020, 2021. Sylvestre-Alvise Rebuffi, Alexander Kolesnikov, Georg Sperl, and Christoph Lampert. icarl: Incremental classifier and representation learning. In Proceedings of the IEEE conference on Computer Vision and Pattern Recognition, pages 20012010, 2017. Andrei Rusu, Neil Rabinowitz, Guillaume Desjardins, Hubert Soyer, James Kirkpatrick, Koray Kavukcuoglu, Razvan Pascanu, and Raia Hadsell. Progressive neural networks. arXiv preprint arXiv:1606.04671, 2016. Levent Sagun, Utku Evci, Ugur Guney, Yann Dauphin, and Leon Bottou. Empirical analysis of the Hessian of over-parametrized neural networks. arXiv preprint arXiv:1706.04454, 2017. Haizhou Shi, Zihao Xu, Hengyi Wang, Weiyi Qin, Wenyuan Wang, Yibin Wang, Zifeng Wang, Sayna Ebrahimi, and Hao Wang. Continual learning of large language models: comprehensive survey. ACM Computing Surveys, 58(5): 142, 2025. Hanul Shin, Jung Kwon Lee, Jaehong Kim, and Jiwon Kim. Continual learning with deep generative replay. Advances in neural information processing systems, 30, 2017. Anton Sinitsin, Vsevolod Plokhotnyuk, Dmitry Pyrkin, Sergei Popov, and Artem Babenko. Editable neural networks. In International Conference on Learning Representations, 2020. Chenmien Tan, Ge Zhang, and Jie Fu. Massive editing for large language models via meta learning. In The Twelfth International Conference on Learning Representations, 2024. https://openreview.net/forum?id=L6L1CJQ2PE. Lukas Thede, Karsten Roth, Matthias Bethge, Zeynep Akata, and Thomas Hartvigsen. WikiBigEdit: Understanding the limits of lifelong knowledge editing in LLMs. In Aarti Singh, Maryam Fazel, Daniel Hsu, Simon Lacoste-Julien, Felix Berkenkamp, Tegan Maharaj, Kiri Wagstaff, and Jerry Zhu, editors, Proceedings of the 42nd International 15 Conference on Machine Learning, volume 267 of Proceedings of Machine Learning Research, pages 5932659354. PMLR, 1319 Jul 2025. Peng Wang, Zexi Li, Ningyu Zhang, Ziwen Xu, Yunzhi Yao, Yong Jiang, Pengjun Xie, Fei Huang, and Huajun Chen. WISE: Rethinking the knowledge memory for lifelong model editing of large language models. Advances in Neural Information Processing Systems, 37:5376453797, 2024a. Peng Wang, Ningyu Zhang, Bozhong Tian, Zekun Xi, Yunzhi Yao, Ziwen Xu, Mengru Wang, Shengyu Mao, Xiaohan Wang, Siyuan Cheng, et al. Easyedit: An easy-to-use knowledge editing framework for large language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations), pages 8293, 2024b. Shipeng Wang, Xiaorong Li, Jian Sun, and Zongben Xu. Training networks in null space of feature covariance for continual learning. In Proceedings of the IEEE/CVF conference on Computer Vision and Pattern Recognition, pages 184193, 2021. Song Wang, Yaochen Zhu, Haochen Liu, Zaiyi Zheng, Chen Chen, and Jundong Li. Knowledge editing for large language models: survey. ACM Computing Surveys, 57(3):137, 2024c. Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: novel image dataset for benchmarking machine learning algorithms. arXiv preprint arXiv:1708.07747, 2017. Wanli Yang, Fei Sun, Jiajun Tan, Xinyu Ma, Qi Cao, Dawei Yin, Huawei Shen, and Xueqi Cheng. The mirage of model editing: Revisiting evaluation in the wild. In Wanxiang Che, Joyce Nabende, Ekaterina Shutova, and Mohammad Taher Pilehvar, editors, Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1533615354, Vienna, Austria, July 2025a. Association for Computational Linguistics. ISBN 979-8-89176-251-0. doi: 10.18653/v1/2025.acl-long.745. https://aclanthology.org/2025. acl-long.745/. Wanli Yang, Fei Sun, Rui Tang, Hongyu Zang, Du Su, Qi Cao, Jingang Wang, Huawei Shen, and Xueqi Cheng. Fine-tuning done right in model editing. In Socially Responsible and Trustworthy Foundation Models at NeurIPS 2025, 2025b. https://openreview.net/forum?id=lJFPwtobkG. Yunzhi Yao, Peng Wang, Bozhong Tian, Siyuan Cheng, Zhoubo Li, Shumin Deng, Huajun Chen, and Ningyu Zhang. Editing large language models: Problems, methods, and opportunities. In The 2023 Conference on Empirical Methods in Natural Language Processing, 2023. https://openreview.net/forum?id=NZZB3UGcd8. Lang Yu, Qin Chen, Jie Zhou, and Liang He. MELO: Enhancing model editing with neuron-indexed dynamic LoRA. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 1944919457, 2024. Friedemann Zenke, Ben Poole, and Surya Ganguli. Continual learning through synaptic intelligence. In International conference on machine learning, pages 39873995. PMLR, 2017. Ningyu Zhang, Yunzhi Yao, Bozhong Tian, Peng Wang, Shumin Deng, Mengru Wang, Zekun Xi, Shengyu Mao, Jintian Zhang, Yuansheng Ni, et al. comprehensive study of knowledge editing for large language models. arXiv preprint arXiv:2401.01286, 2024. Qingru Zhang, Minshuo Chen, Alexander Bukharin, Pengcheng He, Yu Cheng, Weizhu Chen, and Tuo Zhao. Adaptive In The Eleventh International Conference on Learning budget allocation for parameter-efficient fine-tuning. Representations, 2023. https://openreview.net/forum?id=lq62uWRJjiY. Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi Luan, Denny Zhou, and Le Hou. Instruction-following evaluation for large language models. arXiv preprint arXiv:2311.07911, 2023."
        },
        {
            "title": "A Notation",
            "content": "General notations. We use bold lowercase letters (e.g., θ) for vectors and bold uppercase letters (e.g., H) for matrices. For matrix , Null(M ) denotes its null space. The identity matrix is denoted by I. For vectors u, v, u, denotes the standard inner product. The operator denotes the Hadamard (element-wise) ϕ(zi) to denote the product. We denote sets by calligraphy letters e.g., . We write ED[ϕ(z)] = 1 empirical expectation of function ϕ(z) using the dataset = {zi}n . denotes the Kronecker product. For subspace Rd, PS Rdd denotes the orthogonal projection onto S. Models and parameters. Let fθ : denote parametric model with parameters θ Θ Rp. The pretrained (base) model parameters are denoted by θ0. We write θ := θ θ0 for parameter updates. Datasets. We distinguish between: (i) capability dataset Dcap = {(xi, yi)}n to be preserved, and (ii) an edit dataset Dedit = {(xi, yi)}T Losses and objectives. Let ℓ(ˆy, y) denote task-appropriate loss (e.g., cross-entropy). The empirical capability loss is , specifying desired edits. Typically . , used as proxy for behaviors i=1 i= i=1 Lcap(θ) = ℓ(fθ(xi), yi), 1 i=1 and Ledit(θ) denotes the edit loss evaluated on Dedit. We measure deviations in capability loss using distance function d(, ), including absolute loss differences and Bregman divergences. Second-order quantities. We denote by Hcap := 2 θLcap(θ0) the Hessian of the capability loss at the base model parameters. When using Bregman divergences, the quadratic approximation is governed by the GaussNewton Hessian (GNH), Gcap := E(x,y)Dcap (cid:2)J HˆyJ (cid:3) , where = θfθ(x) is the parameteroutput Jacobian and Hˆy = 2 to model outputs. ˆyℓ is the Hessian of the loss with respect Low-curvature subspaces. Let {Hcap, Gcap} admit an eigendecomposition = ΣU , with eigenvalues σ1 σp 0. For threshold γ (0, 1), we define as the smallest index such that i= σi (cid:30) i=1 σi γ. The γ-approximate nullspace is spanned by U>k = [uk+1, . . . , up], and the corresponding projector is Pγ := U>kU >k. Layerwise notation and K-FAC. For an MLP layer ℓ, we denote input activations by aℓ1, weights by Wℓ Rdoutdin , and pre-activation pseudo-gradients by gℓ. Under the K-FAC approximation, the layerwise GNH block is approximated as G(ℓ) cap Aℓ1 Sℓ, where Aℓ1 = E[aℓ1a Operators. We use vec() and mat() to denote vectorization and reshaping operators between matrix and vector forms. ] and Sℓ = E[gℓg ℓ ℓ1 ]."
        },
        {
            "title": "B Proof of Bregman divergence quadratic form",
            "content": "The following lemma computes second order approximation to Bregman divergence associated with loss function ℓ. Proposition 2 (Quadratic Approximation of Bregman Divergence). Fix an input and parameters θ0 Rp. Assume fθ(x) : Rm is 2 in θ and ℓ : Rm is convex and 2. Define the Bregman divergence Dℓ(a, b) = ℓ(a) ℓ(b) ℓ(b), b. (7) Denote the Jacobian by (θ) := θfθ(x) Rmp and the output Hessian by Hℓ(u) := 2 Then, there exists ρ > 0 such that for all θ with θ ρ uℓ(u) Rmm. Dℓ(fθ0+θ(x), fθ (x)) = θh 1 2 (θ0)Hℓ(fθ0 (x))J (θ0) θ + o(θ2). Proof. By the chain rule, θDℓ(fθ(x), fθ0 (x)) = J(θ)(cid:16) ℓ(fθ(x)) ℓ(fθ0 (x)) (cid:17) , which evaluates to zero at θ = θ0. Differentiating again gives the following decomposition: 2 θDℓ(fθ(x), fθ0 (x)) = J(θ)Hℓ(fθ(x))J (θ) + (cid:16) j=1 [aDℓ(a, fθ0 (x))] (cid:17) (cid:12) (cid:12) (cid:12)a=fθ (x) 2 θ [fθ(x)]j. At θ = θ0, aDℓ(fθ0 Therefore, (x), fθ0 (x)) = 0 and thus the second term in the above equation evaluates to zero. θDℓ(fθ(x), fθ0 (x)) (cid:12) (cid:12) (cid:12)θ=θ0 = J(θ0)Hℓ(fθ0 (x))J (θ0). Thus, by the second order Taylor approximation of Dℓ(fθ(x), fθ (x)) around θ0, we conclude Dℓ(fθ0+θ(x), fθ0 (x)) = θh 1 2 (θ0)Hℓ(fθ (x))J (θ0) θ + o(θ2). 18 Proof of Proposition 1 Throughout the proof, we drop the dependency on layer ℓ for notation simplicity. We show that any vectors that belong to the null space of Kcap also belongs to the null space of Gcap. We interpret Wℓ Null(Kℓ ) as the constraint WℓKℓ ) vec(Wℓ) = 0 under column-wise vectorization). We ) Idout keep all network parameters fixed except the layer-ℓ weight matrix Rdoutdin . Define the parameter-space representation of layer-ℓ weights and updates by := vec(W ) Rdoutdin , and := vec(W ) Rdoutdin. Define the downstream map : Rdout Rm to be the function that takes the layer pre-activation sℓ at layer ℓ (with all other parameters held fixed) to the network output. Thus, for each capability example [n], = 0 (equivalently, ((Kℓ cap cap cap Let Jf (sℓ) := sℓf (sℓ) Rmdout denote the Jacobian of at sℓ. By the chain rule, yi(W ) = (W ai ℓ ) Rm. yi(W0) = Jf (W0ai ℓ1 ) (W ai ℓ1 ) (cid:12) (cid:12) (cid:12)W =W0 . The map 7 ai ℓ1 is linear, and its Jacobian under = vec(W ) is (W ai ℓ1 ) = Idout (ai ℓ1 ), so the per-example Jacobian with respect to can be written as Ji := yi(W0) = Jf (W0ai ℓ1 ) (Idout (ai ℓ1 )). Now let Null(Kcap), i.e. ai ℓ = 0 for all [n]. Using the identity (Idout x) = for any Rdin , we obtain (Idout (ai ℓ )) = ai ℓ1 = 0 [n], and hence Jiw = 0 for all i. By definition, the layer GaussNewton Hessian for the capability objective has the form where each Hi 0. Therefore, for any vector v, Gcap = i=1 Hi Ji, vGcapv = i=1 (Jiv)Hi(Jiv), so if Jiv = 0 for all then vGcapv = 0, which implies Gcapv = 0 since Gcap 0. Applying this with = and using Jiw = 0 for all i, we conclude Gcapw = 0, i.e. Null(Gcap). 19 Proof of matrix-free projection Proposition 3. Let RnAnA , RnB nB be two positive semi-definite matrices, := denote the Kronecker product, and let RnAnB . Let τ : R0 7 {0, 1} denote any predicate function, and define the following subspace: := span{u RnAnB the pair (λ, u) is an eigenvalue/vector pair of with τ (λ) = 1}. We have that: mat(PS vec(X)) = UA (cid:0)(U XUB) (cid:1) , (8) where = UAdiag(λA,1, . . . , λA,nA are the eigen-decompositions of A, respectively, and RnAnB with Mij = τ (λA,i λB,j) is the mask matrix corresponding to the predicate function τ . and = UBdiag(λB,1, . . . , λB,nB )U )U Before we give the proof, we remark that mat : RnAnB 7 RnAnB above is understood to be the functional inverse of vec : RnAnB 7 RnAnB , i.e., mat(vec(X)) = for any RnAnB . Proof. Let us order the columns of UA (resp. UB) as uA,i (resp. uB,j). From basic properties of Kronecker products, the eigenvalues and eigenvectors of are given by λA,iλB,j and uB,j uA,i, with [nA] and [nB]. Therefore, PS can be written as: PS = nA,nBX i,j=1 τ (λA,iλB,j)(uB,ju B,j uA,iu A,i ). Hence, using the identity vec(F XG) = (G ) vec(X) for any size-conforming , X, G, PS vec(X) = = nA,nBX i,j= nA,nBX i,j=1 = vec τ (λA,iλB,j)(uB,ju B,j uA,iu A,i ) vec(X) τ (λA,iλB,j) vec(uA,iu A,iXuB,ju B,j ) τ (λA,iλB,j)u A,iXuB,j uA,iu B,j nA,nBX i,j=1 = vec (cid:0)UA (cid:0)(U XUB) (cid:1) (cid:1) . Hence the claim follows by taking mat() on each side."
        },
        {
            "title": "E Additional details on LLM experiments",
            "content": "Base capability evaluation. We evaluate the base capabilities of edited models using the lm-evaluationharness (Gao et al., 2024). We benchmark performance on diverse set of standard reasoning and knowledge tasks, including IFEval, TruthfulQA (MC2), MMLU (5-shot), GSM8K with chain-of-thought prompting (8-shot), and ARC-Challenge (25-shot). For each task, we evaluate 200 examples, applying the chat template and multi-turn few-shot formatting. Editing performance evaluation. We use EasyEdit (Wang et al., 2024b) for evaluation. Except for Table 3 where we perform teacher-forcing, we follow WILD (Yang et al., 2025a) protocol for evaluation. For No Context, we use the dataset questions as is. For QA Context, That is, we contextualize prompt by appending the template Please answer the question: nnQ: {question}nA:, and autoregressively generate up to 40 tokens using predefined stop tokens [., n, eos]. We evaluate the generated outputs with gpt-4o-mini (see Figure 8 for the exact prompt). Prompt for LLM-as-a-Judge Your job is to look at question, gold target, and predicted answer, and then assign grade of either [\"CORRECT\", \"INCORRECT\"]. The following are examples of CORRECT predicted answers. Question: What are the names of Barack Obamas children? Gold target: Malia Obama and Sasha Obama Predicted answer 1: sasha and malia obama Predicted answer 2: Malia and Sasha Obama are the names of Barack Obamas children. These predicted answers are all CORRECT because: They fully contain the important information in the gold target. They do not contain any information that contradicts the gold target. The following are examples of INCORRECT predicted answers. Question: What are the names of Barack Obamas children? Gold target: Malia and Sasha Predicted answer 1: Malia. Predicted answer 2: Malia, Sasha, and Susan. Predicted answer 3: Malia and Sasha, Malia and Sasha, Malia and Sasha, Malia and Sasha (repeated answer) These predicted answers are all INCORRECT because: factual statement in the answer contradicts the gold target or contains repeated content. Here is sample. Simply reply with either CORRECT or INCORRECT. Question: {question} Gold target: {target} Predicted answer: {predicted_answer} According to the gold target, please grade the predicted answer of this question as one of: A: CORRECT B: INCORRECT Just return the letters or B, with no text around it. Figure 8 The complete prompt used to employ an LLM as judge. The judge provides binary assessments (correct or incorrect) based on given question, gold target answer, and predicted answer. CrispEdit implementation. For experiments reported in Table 1, CrispEdit uses (n, γ) = (10, 000, 0.9) for CounterFact and WikiBigEdit and (n, γ) = (10, 000, 0.7) for ZsRE, while CrispEdit-Seq uses (n, γ) = 21 Table 2 Default hyperparameters used for CrispEdit and CrispEdit-Seq."
        },
        {
            "title": "Hyperparameter",
            "content": "Editing layers (LLaMA-3-8B-Instruct) Editing layers (Qwen-2.5-1.5B-Instruct) Number of steps Early stopping Batch size Chunk size (CrispEdit-Seq) Learning rate (Adam)"
        },
        {
            "title": "Value",
            "content": "{19, 20, 21, 22, 23} {4, 5, 6} 25 0.01 32 100 5 104 (30, 0.999) for ZsRE and CounterFact and (n, γ) = (200, 0.995) for WikiBigEdit. For ZsRE10k experiment reported in Table 4, CrispEdit uses (n, γ) = (1, 000, 0.7). For our Qwen-2.5-1.5B-Instruct implementation Table 5, CrispEdit uses (n, γ) = (1000, 0.7) for ZsRE and (n, γ) = (1000, 0.9) for Counterfact and WikiBigEdit, while CrispEdit-Seq uses (n, γ) = (30, 0.995). All other hyperparameters are kept fixed across experiments and follow Table 2. Non-trivial K-FAC implementation for CrispEdit-Seq. We now discuss one non-trivial design choice made in our implementation. We found that masking prompt tokens for K-FAC calculation (mirroring the fine-tuning setup) yielded suboptimal performance, even with larger number of tokens  (Table 6)  . Instead, in our K-FAC calculation for edit samples, we calculate the next token prediction loss over the entire prompttarget sequence. While we are not sure about the underlying cause of this behavior, we suspect that it arises from our relaxed assumption of token independence during K-FAC calculation. Baseline implementation. All our baselines follow the code and hyperparameters provided by the EasyEdit framwork. Such hyperparameters come from the original authors of respective baselines that tuned their method for LLaMA-3-8B-Instruct."
        },
        {
            "title": "F Qualitative case study",
            "content": "Model Editing Case Study 1 Editing Prompt What voice type does Marina Rebeka have? Edit Target mezzo-srano"
        },
        {
            "title": "Generation Output",
            "content": "Adam-NSCL mezzo-srano-srano-srano-srano-srano-srano-srano-srano-srano-srano-srano-srano-sranosrano-srano-srano LocBFFT mezzo-oprano AlphaEdit mezzo-soprano UltraEdit mezzo soprano FT mezzo-srano-srano-srano-srano-srano-srano-srano-srano-srano-srano-srano-srano-sranosrano-srano-srano CrispEdit mezzo-srano Model Editing Case Study 2 Editing Prompt What is the status of Cebu flowerpecker? Edit Target endangered species"
        },
        {
            "title": "Generation Output",
            "content": "Adam-NSCL endangered species Data Deficient species endangered species endangered species Data Deficient species endangered species endangered species endangered species endangered species endangered species endangered species endangered species endangered species endangered species endangered species endangered species endangered species endangered species endangered species endangered species endangered species endangered species endangered species LocBFFT endangered species AlphaEdit endangered UltraEdit critically endangered species FT endangered species species endangered species endangered species endangered species endangered species endangered species endangered species endangered species endangered species endangered species endangered species endangered species endangered species endangered species endangered species endangered species endangered species endangered species endangered species endangered species endangered species endangered species endangered species endangered species endangered CrispEdit endangered species 23 Table 3 Comparison of CrispEdit with existing methods on editing LLaMA-3-8B-Instruct in the teacher-forcing evaluation pipeline. Rel, gen, Spec denote reliability, generality, and specificity, respectively. We perform model editing on 3,000 samples of three representative datasets and evaluate editing performance and base performance following teacher-forcing setup of (Meng et al., 2023, 2022; Fang et al., 2025). Results that are the highest or within 5% of the highest results are highlighted in bold. Method Llama 3 8B Instruct MEMIT AlphaEdit Adam-NSCL LocBF-FT UltraEdit MEND FT FT Sequential LoRA LoRA Sequential CrispEdit CrispEdit-Seq ZsRE CounterFact WikiBigEdit Rel 25.7 0.0 86.7 98.8 99.1 61.9 0.0 99.1 79.7 93.4 36.8 99.1 98.3 Gen 25.1 0.0 77.8 92.4 91.1 57.3 0.0 93.1 76.6 60.6 32.7 92.1 91.4 Spec 37.8 0.0 32.4 22.1 34.5 31.5 0.1 22.9 16.8 30.9 21.7 32.3 30.2 Rel 0.9 0.0 94.3 99.5 99.7 28.0 0.0 99.7 78.6 93.8 20.9 99.8 99.5 Gen Spec 1.2 0.0 72.0 81.5 72.7 18.7 0.0 82.0 59.8 17.8 10.4 73.0 62. 89.4 49.4 69.1 47.7 44.6 51.4 0.0 47.9 51.6 42.9 57.0 55.2 52.3 Rel 34.0 0.5 95.0 99.7 99.9 87.4 0.0 99.8 93.5 99.3 70.2 99.9 99.9 Gen 34.8 0.5 89.0 97.5 96.8 84.7 0.0 97.6 90.5 82.4 65.4 97.1 96.7 Spec 32.8 0.0 42.0 36.4 42.8 47.8 0.0 36.3 29.0 44.4 36.0 44.7 39."
        },
        {
            "title": "G Additional tables",
            "content": "Table 4 Influence of scaling to larger editing dataset. Rel and Gen denote reliability and generality, respectively. We perform model editing on 10,000 samples of ZsRE and evaluate editing performance with WILD framework and base performance with five representative benchmarks. Results that are the highest or within 5% of the highest results are highlighted in bold. Data Method QA Context No Context Edited Capabilities Base Capabilities 0 s LLaMA-3-8B-Instruct LocBF-FT UltraEdit Adam-NSCL AlphaEdit CrispEdit Rel 2.0 53.5 20.1 1.2 0.3 77.4 Gen 1.5 47.7 16.7 1.1 0.2 68. Rel 2.9 11.5 12.6 0.4 0.1 31.1 Gen MMLU IFEval TruthfulQA ARC-C GSM8K 2.1 11.6 10.4 0.7 0.0 28.9 69.5 68.0 67.9 68.2 22.8 68.5 69.3 67.6 68.9 14.8 20.9 69. 50.7 50.7 49.8 54.0 53.9 50.2 58.0 50.0 46.0 35.0 22.0 52.0 73.5 73.0 73.0 2.0 0.0 71.0 24 Table 5 Comparison of CrispEdit with existing methods on editing Qwen-2.5-1.5B-Instruct. Rel and gen denote reliability and generality, respectively. We perform model editing on 3,000 samples of ZsRE and evaluate editing performance with WILD framework and base performance with five representative benchmarks. Results that are the highest or within 5% of the highest results are highlighted in bold. Data Model QA Context No Context Edited Capabilities Base Capabilities Qwen 2.5 1.5B FT LocBF-FT AlphaEdit UltraEdit Adam-NSCL CrispEdit (Batch) CrispEdit (Seq) Qwen 2.5 1.5B FT LocBF-FT AlphaEdit UltraEdit Adam-NSCL CrispEdit (Batch) CrispEdit (Seq) Qwen 2.5 1.5B FT LocBF-FT AlphaEdit UltraEdit Adam-NSCL CrispEdit (Batch) CrispEdit (Seq) Rel 3.5 35.4 71.4 7.2 11.3 62.6 77.8 55.5 2.0 22.3 58.2 22.6 10.8 5.9 63.3 64.6 8.4 59.5 76.8 0.7 27.5 31.9 62.9 53.4 Gen 4.0 29.6 52.9 4.3 9.8 50.5 61.0 40. 1.8 28.4 32.6 14.1 8.5 4.9 34.4 41.8 8.6 50.4 61.9 0.7 25.8 28.4 52.0 43.3 Rel 2.3 32.2 38.0 6.2 18.2 21.4 52.6 77.7 0.9 8.9 46.8 31.2 14.4 3.4 67.0 60.3 7.0 42.2 66.0 1.5 53.2 11.6 57.3 83. Gen MMLU IFEval TruthfulQA ARC-C GSM8K 2.0 25.5 30.6 4.2 11.8 15.3 44.0 51.6 0.7 14.2 21.5 16.8 5.9 1.5 29.9 27.9 6.4 37.0 55.2 1.3 45.5 10.4 46.3 60.0 61.9 50.0 59.6 24.9 62.3 59.3 57.8 59.3 61.9 34.8 59.3 24.4 62.4 60.5 61.5 58. 61.9 54.3 60.4 24.4 62.5 62.3 61.2 59.9 48.3 24.8 42.0 12.4 47.7 38.0 32.8 39.5 48.3 15.1 39.0 12.9 41.9 18.5 40.5 39.9 48.3 30.7 34.8 13.2 41.4 36.2 38.7 34.0 50.9 49.8 54.6 44.7 52.1 46.0 46.4 46.0 50.9 45.7 46.6 46.8 44.8 48.3 47.3 47. 50.9 46.2 46.1 48.9 44.5 46.4 47.0 47.9 52.0 34.5 44.0 21.5 50.0 44.0 42.0 42.0 52.0 23.5 40.5 19.0 41.5 36.0 44.0 43.0 52.0 39.5 43.5 23.5 44.5 41.5 45.5 44.5 58.0 35.5 54.0 2.0 54.0 32.0 58.5 59.0 58.0 6.5 56.0 1.5 62.0 4.5 58.5 58. 58.0 52.0 58.0 1.0 60.0 33.0 58.5 55.0 Z F n i E W i Table 6 Effect of prompt masking during K-FAC calculation. Even with larger number of tokens for computing K-FAC, prompt masking leads to suboptimal performance with CrispEdit-Seq. Method CrispEdit (chunk size = 100) CrispEdit (chunk size = 500, prompt masking) Rel 71.1 12 25 Table 7 Influence of the size of capability dataset on editing performances and base capability preservation. Across range of n, we set γ = 0.9 for CrispEdit, perform model editing on 3,000 samples of ZsRE, and evaluate editing performance with WILD framework and base performance with five representative benchmarks. Results that are the highest or within 5% of the highest results are highlighted in bold. CrispEdit remains robust across wide range of n. Highlighted model represents data used in Table 1. Data Sample Size QA Context No Context Edited Capabilities Base Capabilities LLaMA-3-8B-Instruct No Projection (FT) = 10 = 50 = 100 = 500 = 1000 = 10000 = 50000 = 100000 Rel 2.1 46.8 53.6 69.8 74.2 78.4 75.9 71.2 71.0 69.9 Gen 1.7 43.1 48.5 62.9 66.0 65.9 63.9 57.9 57.3 55.5 Rel 2.9 9.9 10.6 24.9 35.8 54.4 48.8 48.0 47.3 54.2 Gen MMLU IFEval TruthfulQA ARC-C GSM8K 2.1 8.3 9.3 24.5 31.4 47.2 41.3 40.3 39.9 43.8 69.5 69.3 69.1 69.3 69.4 69.5 69.4 69.4 69.2 69.3 69.3 45.0 48.8 68.3 68.1 72.3 72.3 68.4 68.9 68.3 50.7 48.7 50.8 51.8 50.4 51.5 50.4 50.3 50.2 50.1 58.0 43.0 42.5 53.0 52.0 54.5 54.0 59.5 57.0 56.5 73.5 50.0 57.5 74.0 75.0 75.0 74.5 73.0 75.5 72. s Table 8 Influence of energy threshold γ on editing performances and base capability preservation. Across range of γ, we set = 10, 000 for CrispEdit, perform model editing on 3,000 samples of three representative datasets, and evaluate editing performance with WILD framework and base performance with five representative benchmarks. Results that are the highest or within 5% of the highest results are highlighted in bold. CrispEdit remains robust across wide range of γ. Data Energy threshold QA Context No Context Edited Capabilities Base Capabilities s a t C d k i LLaMA-3-8B-Instruct CrispEdit (γ = 0.5) CrispEdit (γ = 0.6) CrispEdit (γ = 0.7) CrispEdit (γ = 0.8) CrispEdit (γ = 0.9) CrispEdit (γ = 0.95) CrispEdit (γ = 0.99) LLaMA-3-8B-Instruct CrispEdit (γ = 0.5) CrispEdit (γ = 0.6) CrispEdit (γ = 0.7) CrispEdit (γ = 0.8) CrispEdit (γ = 0.9) CrispEdit (γ = 0.95) CrispEdit (γ = 0.99) LLaMA-3-8B-Instruct CrispEdit (γ = 0.5) CrispEdit (γ = 0.6) CrispEdit (γ = 0.7) CrispEdit (γ = 0.8) CrispEdit (γ = 0.9) CrispEdit (γ = 0.95) CrispEdit (γ = 0.99) Rel 2.1 77.4 77.8 80.5 80.3 71.2 62.7 37.8 1.2 45.5 65.5 75.7 79.2 79.4 72.0 51.6 9.3 62.6 66.5 76.2 77.2 77.0 76.9 67.6 Gen 1.7 68.3 67.8 69.0 68.3 57.9 48.5 28.8 1.0 31.5 48.7 57.3 57.4 55.9 47.5 27.7 9.1 58.7 60.8 69.2 72.1 70.2 68.9 57.2 Rel 2.9 43.4 56.0 57.4 52.3 48.0 38.5 35.3 0.3 5.7 9.8 15.5 21.4 38.4 46.3 45. 16.4 14.3 17.4 26.3 21.2 28.4 23.4 34.4 Gen MMLU IFEval TruthfulQA ARC-C GSM8K 69.5 69.5 69.5 69.5 69.2 69.4 69.4 69.4 69.5 68.5 69.7 69.4 69.5 69.3 69.4 69.4 69.5 69.0 69.3 69.2 69.4 69.3 69.2 69.3 69.3 67.8 70.2 67.9 66.7 68.4 68.4 68. 69.3 50.4 63.2 66.8 69.4 67.5 67.8 68.2 69.3 68.8 68.2 68.8 69.1 70.5 62.6 62.5 50.7 50.5 51.0 50.5 50.1 50.3 50.4 51.1 50.7 51.4 52.4 51.7 49.8 49.5 50.3 51.7 50.7 50.6 51.4 51.1 50.4 51.8 51.2 52.6 58.0 52.0 53.5 55.0 56.0 59.5 56.0 57. 58.0 50.0 55.5 55.0 54.5 54.0 57.0 56.0 58.0 55.0 53.0 54.0 55.0 55.0 57.5 58.0 73.5 77.5 75.5 76.0 77.0 73.0 76.0 73.0 73.5 43.5 75.0 72.5 73.5 76.5 74.0 76.5 73.5 72.5 75.0 76.5 76.5 74.0 74.5 70.5 2.1 39.1 48.0 50.9 46.0 40.3 31.7 27. 0.6 7.2 14.3 20.4 25.8 32.4 33.0 26.8 16.1 14.9 19.1 27.8 24.4 30.5 27.3 32."
        }
    ],
    "affiliations": [
        "University of Southern California"
    ]
}