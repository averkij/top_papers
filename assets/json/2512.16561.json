{
    "paper_title": "N3D-VLM: Native 3D Grounding Enables Accurate Spatial Reasoning in Vision-Language Models",
    "authors": [
        "Yuxin Wang",
        "Lei Ke",
        "Boqiang Zhang",
        "Tianyuan Qu",
        "Hanxun Yu",
        "Zhenpeng Huang",
        "Meng Yu",
        "Dan Xu",
        "Dong Yu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "While current multimodal models can answer questions based on 2D images, they lack intrinsic 3D object perception, limiting their ability to comprehend spatial relationships and depth cues in 3D scenes. In this work, we propose N3D-VLM, a novel unified framework that seamlessly integrates native 3D object perception with 3D-aware visual reasoning, enabling both precise 3D grounding and interpretable spatial understanding. Unlike conventional end-to-end models that directly predict answers from RGB/RGB-D inputs, our approach equips the model with native 3D object perception capabilities, enabling it to directly localize objects in 3D space based on textual descriptions. Building upon accurate 3D object localization, the model further performs explicit reasoning in 3D, achieving more interpretable and structured spatial understanding. To support robust training for these capabilities, we develop a scalable data construction pipeline that leverages depth estimation to lift large-scale 2D annotations into 3D space, significantly increasing the diversity and coverage for 3D object grounding data, yielding over six times larger than the largest existing single-image 3D detection dataset. Moreover, the pipeline generates spatial question-answering datasets that target chain-of-thought (CoT) reasoning in 3D, facilitating joint training for both 3D object localization and 3D spatial reasoning. Experimental results demonstrate that our unified framework not only achieves state-of-the-art performance on 3D grounding tasks, but also consistently surpasses existing methods in 3D spatial reasoning in vision-language model."
        },
        {
            "title": "Start",
            "content": "N3D-VLM: Native 3D Grounding Enables Accurate Spatial Reasoning in Vision-Language Models Yuxin Wang1,2* Lei Ke2 Boqiang Zhang2 Tianyuan Qu2,3 Hanxun Yu2,4 Zhenpeng Huang2,5 Meng Yu2 Dan Xu1 Dong Yu2 1HKUST 2Tencent AI Lab 3CUHK 4ZJU 5NJU Project Page: https://n3d-vlm.github.io 5 2 0 2 8 1 ] . [ 1 1 6 5 6 1 . 2 1 5 2 : r Figure 1. Our unified vision-language model N3D-VLM performs native 3D grounding and subsequent spatial reasoning and answering. Given an RGB image and the corresponding text question, the model is capable of predicting 3D bounding boxes for specified objects and explicitly reasoning about spatial relations in 3D space."
        },
        {
            "title": "Abstract",
            "content": "While current multimodal models can answer questions based on 2D images, they lack intrinsic 3D object perception, limiting their ability to comprehend spatial relationships and depth cues in 3D scenes. In this work, we propose N3D-VLM, novel unified framework that seamlessly integrates native 3D object perception with 3Daware visual reasoning, enabling both precise 3D grounding and interpretable spatial understanding. Unlike conventional end-to-end models that directly predict answers from RGB/RGB-D inputs, our approach equips the model with native 3D object perception capabilities, enabling it to directly localize objects in 3D space based on textual descriptions. Building upon accurate 3D object localization, the model further performs explicit reasoning in 3D, achieving more interpretable and structured spatial understanding. To support robust training for these capabilities, we develop scalable data construction pipeline that leverages depth estimation to lift large-scale 2D annotations into 3D space, significantly increasing the diversity and coverage for 3D object grounding data, yielding over six times larger than the largest existing single-image 3D detection dataset. Moreover, the pipeline generates spatial questionanswering datasets that target chain-of-thought (CoT) reasoning in 3D, facilitating joint training for both 3D object localization and 3D spatial reasoning. Experimental results demonstrate that our unified framework not only achieves state-of-the-art performance on 3D grounding tasks, but also consistently surpasses existing methods in 3D spatial reasoning in vision-language model. 1. Introduction Recent vision-language models (VLMs) [2, 11, 19, 28] have expanded beyond text-only understanding to handle diverse multimodal tasks such as image and video analysis, OCR, and visual reasoning. However, real-world applications often demand deeper grasp of 3D structure and spatial relationships, which current VLMs largely lack. Effective 3D spatial reasoning requires accurate object-level perception in 3D space; without it, models struggle to infer spatial configurations or reason about physical environments. Advancing toward truly multimodal intelligence therefore requires moving beyond 2D language-centric perception toward robust 3D spatial ability to perceive, ground, and reason about the 3D world from visual inputs. Specialized VLMs enhance 3D spatial understanding capability through diverse input modalities and architectural designs. Some models integrate external perception models [24, 26] to obtain auxiliary object information, such as *Work done during an internship at Tencent AI Lab. Corresponding author. 2D/3D bounding boxes or segmentation masks [8, 10, 34]. Others assume that 3D object bounding boxes or spatial layouts are provided in advance [15, 23, 39]. Alternatively, recent approaches have explored using VLMs to directly localize objects in point clouds [1, 22]. However, these methods typically focus on object detection in constrained scenes with limited object categories, and do not support explicit spatial reasoning. Although these approaches have advanced specific aspects of 3D spatial understanding, they either depend heavily on external modules or predefined spatial information, or remain confined to narrow perception tasks, which makes it challenging to generalize and integrate them into unified vision-language systems. Based on these observations, we argue that 3D spatial understanding could be decomposed into two core abilities: 3D object localization and subsequent 3D spatial reasoning. This perspective motivates our design, where explicit 3D object perception serves as critical foundation for spatial reasoning. By first detecting objects in 3D space, models can reason more effectively over structured representations such as 3D bounding boxes, enhancing both the accuracy and interpretability of the reasoning process. To this end, we propose N3D-VLM, unified visionlanguage model that integrates 3D detection, grounding, and CoT reasoning. The model is equipped with inherent and generalizable 3D object perception, allowing it to accurately localize objects and capture depth cues in physical space. Building on these 3D perception results, N3D-VLM performs spatial reasoning taskssuch as computing interobject distances based on 3D coordinates or inferring relative sizes from 3D bounding box dimensions, as shown in Fig. 1. Recent works like [14, 21] utilized 3D coordinates or 2D grounding data to aid spatial understanding. However, our approach fundamentally differs: our model explicitly predicts comprehensive 3D bounding boxes, enabling more generalizable and interpretable spatial reasoning. key challenge in developing general 3D object perception lies in the scarcity of large-scale 3D training data. Existing datasets [4, 37], typically captured from indoor [3, 25] or autonomous driving scenes [5, 9], suffer from limited diversity, small scale, and narrow category coverage. In contrast, large-scale 2D detection datasets [13, 16, 27] provide richer scene variety and class diversity. To overcome 3D data scarcity, we leverage depth and camera estimation model [32] to lift 2D annotations into 3D space, generating abundant 3D detection and grounding data. We further construct spatial QA datasets to supervise CoT-based spatial reasoning. Another challenge is ensuring consistent real-world scale and camera geometry in 3D outputs; we address this by using the same depth model during data construction and introducing depth-aware positional encoding to inject explicit depth cues. Jointly training on these heterogeneous sources enables our model to achieve robust 3D localization and effective CoT spatial reasoning grounded in explicit 3D perception. In summary, we present comprehensive solution for 3D grounded reasoning by introducing model with native 3D obejct localization capabilities, marking significant advancement in 3D visual understanding. Our main contributions are threefold: We propose unified model that takes RGB-D input and features native 3D object detection/grounding abilities, enabling spatial reasoning based on detection outcomes. We design data construction pipeline that leverages depth estimation to convert large-scale 2D annotations into 3D space, addressing the shortage of diverse and high-volume 3D object perception training data. We establish spatial reasoning benchmark with explicit reasoning process, covering both single-object and multiobject scenarios across wide variety of question types. Our model consistently surpasses existing methods in both object localization and spatial reasoning metrics, demonstrating state-of-the-art effectiveness and generalization. We will release code, checkpoints, and datasets upon acceptance. 2. Related Work 2.1. VLM for 3D Spatial Understanding Recently, an increasing number of approaches aim to extend general-purpose VLMs [2, 11, 19, 28] with 3D spatial understanding, by leveraging 3D point clouds, video [7, 23, 31, 35, 39] or RGB/RGB-D inputs [6, 8, 10, 40]. For example, GPT4Scene [23] generates object-marked images from point clouds, and uses them along BEV (birds-eye view) images, to perform 3D captioning and question answering in reconstructed scenes. Think-in-Space [35] supports broader range of spatial questions, including multiplechoice route planning and relative distance reasoning based on video inputs. However, these methods often rely on additional 3D information or object-level annotations, and are typically constrained to limited indoor environments. Other approaches focus on inferring spatial relations from 2D inputs. SpatialVLM [6] enables spatial question answering (e.g., left, right, front, behind) from single RGB image. SpatialRGPT further extends this capability to region-level reasoning, allowing spatial understanding between specified image regions. Nevertheless, these methods generally lack explicit 3D spatial understanding, which either rely on black-box end-to-end reasoning to produce answers, or requires external modules for region localization. 2.2. VLM for 3D Object Localization Other 3D visual grounding approaches can localize object positions in 3D space based on point cloud or video input [15, 18, 20, 34]. For example, VLM-Grounder [34] performs 2D segmentation on selected video frames, followed by multi-view matching and ensemble projection to localize objects in 3D. SeeGround [15] supports 3D visual grounding but assumes known object positions after selecting specific view. These methods typically rely on external segmentation tools or require additional objectlevel information. In contrast, our approach explicitly localizes 3D objects and outputs 3D bounding boxes. SpatialLM [22] also performs 3D grounding from point clouds and produces 3D bounding boxes. However, it is limited to indoor scenes with small set of object categories and lacks spatial reasoning capabilities beyond grounding. SpatialReasoner [21] enhances spatial understanding by estimating object position and orientation. However, it operates in constrained scenarios, does not capture object size, and demonstrates limited generalization. By contrast, our method generalizes well across diverse scenes and outputs full 3D bounding boxes with complete object dimensions and positions, supporting both 3D detection, 3D grounding and downstream spatial reasoning. 3. The Proposed Framework: N3D-VLM Given an RGB image and its corresponding monocular depth map, represented as (I, D), we aim to train visionlanguage model (VLM) that takes RGB-D input and outputs 3D object detection and grounding results in the form of explicit 3D bounding box in the camera coordinate system. Furthermore, the model can leverage these grounded 3D objects to perform spatial reasoning and answer spatial understanding questions. The depth map can be easily obtained using monocular depth estimation models [32, 36]. Our framework consists of two main components: 3D data construction and model design with training and evaluation, as illustrated in Fig. 2 and Fig. 3, respectively. As shown in Fig. 2, we begin by lifting 2D annotations from existing large-scale, category-rich 2D detection and grounding datasets into 3D domain, forming diverse repository of 3D detection annotations. Based on this 3D repository, we further construct datasets for 3D detection, 3D grounding, and 3D spatial reasoning QA, which are used for both training and evaluation. Fig. 3a depicts the architecture of our model, which accepts RGB-D inputs. For grounding tasks, the model predicts structured language descriptions that correspond explicitly to the 3D bounding boxes of objects. For spatial reasoning tasks, the model performs spatial reasoning explicitly over grounded 3D objects to answer spatial understanding questions. 3.1. 3D Data Generation 3D Detection Annotation Repository. As shown in Fig. 2, we construct large-scale 3D detection repository by lifting existing 2D detection datasets into 3D. Starting from images annotated with 2D bounding boxes, we first obtain Figure 2. Illustration of our data construction pipeline. We first lift annotations from existing 2D detection datasets with diverse object categories into 3D space, resulting in large-scale and category-rich 3D detection annotation repository. Based on this repository, we generate data for 3D detection, 3D grounding, and 3D spatial reasoning QA tasks. [24] and estimate monocular object segmentation masks depth for each image [32]. The resulting depth maps are back-projected to generate point clouds in the camera space. By combining these point clouds with category and segmentation labels, we derive 3D bounding boxes for each object. During this lifting process, we apply rule-based filters to automatically remove outlier points from invalid depth values and discard implausible boxes that are excessively large or small. Leveraging the scale and category richness of 2D datasets, our constructed 3D corpus inherits these advantages, and provides strong foundation for boosting VLMs 3D localization capabilities. As illustrated in Fig. 2b, our dataset contains 2.78 million samples, which substantially more than existing single-image 3D detection datasets such as Omni3D (234K) [4] and DetAny3D (450K) [37]. Our dataset is constructed from three major sources: COCO [17], OpenImages [13], and Objects365 [27]. COCO and Objects365 offer high-quality 2D annotations across dozens to hundreds of object categories. For OpenImages, where the average number of boxes per image is low, we apply RAM [38] to re-detect objects, producing open-vocabulary 2D detections that are subsequently lifted into 3D. Generation of 3D Localization Data. After constructing the 3D detection annotation repository, we further generate corresponding 3D detection and grounding QA pairs to facilitate training VLMs for 3D object localization. Specifically, we represent each 3D bounding box using structured language format, shown in Fig. 3c. Similar to [1, 22], each box is encoded as: bbox(id, class, u, v, z, sx, sy, sz) where id and class denotes the object identifier and category. (u, v) is the 2D projection of the 3D center on the image plane, and is its depth. x, y, and represent the box dimensions along the three spatial axes. Note that, given known camera intrinsics, (u, v, z) and (x, y, z) are interconvertible via deterministic projection. For 3D detection QA, answers are directly derived from the structured annotations. For 3D grounding QA, we adopt two strategies. First, we select objects uniquely identifiable by category (i.e., categories that appear only once) and convert them into grounding questions. This extends to cases where multiple distinct categories are each uniquely identifiable. When multiple instances of the same category appear (see Fig. 2), we formulate questions like Locate all the boys in the image. For objects not easily described by category, we Figure 3. Illustration of our model design and quantitative comparison. (a) Overview of our model architecture and the cascaded spatial reasoning process. (b) Quantitative comparison showing that our model outperforms existing methods. (c) Definition of structured language representation for 3D bounding boxes. either use referring expressions from existing datasets [12], or refer to them by rendering their 2D bounding boxes on the image. This process yields diverse set of 3D detection and grounding QA pairs, supporting effective training of VLMs for 3D object localization. Generation of 3D Spatial Reasoning Data. We further construct 3D spatial reasoning questions and explicit reasoning answers based on the 3D detection annotation repository. As shown in Fig. 2, we randomly sample objects from an image and apply predefined question templates, e.g., asking for the clock direction between two objects. For each question, we generate deterministic reasoning process and answer based on the 3D bounding boxes, which is then rephrased using an LLM to improve naturalness. Following [8], we design both open-ended and numerically grounded questions. Specifically, we adopt question types similar to SpatialRGPT, including comparisons of relative scale (e.g., wider/narrower, taller/shorter), spatial relations (e.g., above/below, left/right, front/behind), absolute distances between objects, clock directions, and object dimenIn addition, we also extend sions (e.g., height, width). to multi-object reasoning involving three or more objects. For example, we ask about relative distances among three objects or spatial configurations among dozen objects, as illustrated in Fig. 1. All answers include deterministic numerical computations and interpretable reasoning steps, In the example grounded in 3D object bounding boxes. from Fig. 2, the question asks for the clock direction of the boy relative to the stroller. The reasoning process is first generated based on predefined template: given the 3D bounding boxes of both objects, we compute the vector from the stroller to the boy on the xz-plane, calculate its angle with respect to the positive x-axis, and convert the angle into clock position. This explicit geometric reasoning is embedded in the answer chain, enabling more interpretable and intuitive explanations. More question templates and task definitions are provided in the Appendix. 3.2. Model Architecture 3D-aware Visual Encoding. To ensure that the 3D bounding boxes predicted by our VLM are in real-world scale and aligned with an existing coordinate system, we adopt the depth estimation model [32], which predicts both depth maps and camera intrinsics. We then use the predicted depth as an additional input to our model. This guarantees that all predicted 3D bounding boxes are expressed in metric scale and aligned with the coordinate system defined by [32]. As illustrated in Fig. 3, we design 3D-aware visual encoding pipeline to incorporate geometric information into the vision-language model. Given an RGB image RHW 3, its corresponding depth map RHW , and camera intrinsics intr R33. We first back-project each pixel to 3D point in the camera coordinate system: RHW 3: Pij = Dij intr1 uj vi 1 , (1) Table 1. Comparison between our proposed N3D-Bench and SpatialRGPT-Bench [8]."
        },
        {
            "title": "Comparision",
            "content": "SpatialRGPT-Bench [8] N3D-Bench where (uj, vi) are the pixel coordinates. This yields dense point cloud RHW 3, which is then downsampled to ˆP Rhw3 to match the spatial resolution of the image features Fimg Rhwc extracted by the vision encoder. To inject spatial information, each 3D coordinate (x, y, z) ˆP is encoded using sinusoidal positional encoding. For each axis α {x, y, z}, we compute: PE(α, 2i) = sin PE(α, 2i + 1) = cos (cid:16) (cid:16) α 100002i/c α 100002i/c (cid:17) (cid:17) , , (2) for = 0, 1, . . . , obtained by summing the encodings across all three axes: 2 1. The final coordinate embedding is ecoord = (cid:88) PE(k). k{x,y,z} (3) We then add the coordinate embedding to the image features to obtain the fused representation as follows: Fimg = Fimg + ecoord. (4) The fused feature map Fimg, which encodes both visual and spatial cues, is then passed to the language model along with the prompt tokens for autoregressive prediction. Training Strategy and Inference Pipeline. Our model is based on Qwen2.5-VL [2] and trained in two stages. In the first stage, we train the model for 3D object localization using the dataset described in Sec. 3.1. In the second stage, we train the model for grounding-based 3D spatial reasoning using mixture of 3D spatial reasoning data and subset of the localization data. All parameters of the encoder and the language model are learnable throughout both stages. At inference time, our model supports two usage modes. The first, illustrated in Fig. 1, allows users to ask spatialrelated questions directly. The model automatically decomposes the query into two steps: 3D object grounding, followed by spatial reasoning based on the grounding results. In the second mode, shown in Fig. 3, users can explicitly request 3D grounding first, then issue follow-up questions according to the grounding output. In both cases, reasoning is performed conditionally on the grounding results. 3.3. N3D-Bench Given the narrow range of scenes and object categories, current benchmarks fall short in representing the complexity of real-world 3D spatial understanding. Based on our 3D spatial data generation pipeline, we manually curated 1,200 open-ended and 800 numerically questions with CoT # Questions # Object Categories Objects / Question View Change CoT Reasoning 1406 88 {1,2} 2000 264 {1,2,3,>3} reasoning to construct N3D-Bench. As shown in Tab. 1, our benchmark significantly extends SpatialRGPT-Bench 1 in both object category coverage and question complexity. N3D-Bench includes references to 264 object categories, which is three times more than SpatialRGPT-Bench. It also introduces questions involving spatial relations among three or more objects, as well as viewpoint-shifted reasoning (e.g., from the opposite view), which are not considered in SpatialRGPT-Bench. Additionally, we introduce explicit CoT reasoning grounded in 3D object bounding box, offering interpretable intermediate steps beyond direct answers. These enhancements make N3D-Bench more comprehensive and challenging benchmark for evaluating 3D spatial reasoning over both single and multiple objects. 4. Experiments 4.1. Experimental Setup Dataset. As described in Sec. 3.2, we train our model on 3D object localization and spatial reasoning data derived from OpenImages, Objects365, and COCO. For 3D spatial reasoning, we evaluate on three benchmarks: our proposed N3D-Bench, SpatialRGPT-Bench [8] (1,404 openended and numerical questions), and CV-Bench-3D [30] (1,200 multiple-choice questions). For 3D grounding, we evaluate on the RefCOCO series [12] , along with an additional test set constructed from Objects365. Metrics. For spatial reasoning, we report the accuracy. For open-ended questions, we use GPT-4o [11] in an LLM-as-ajudge setup to access correctness. For numerical questions, we extract predicted values via string matching and apply 25% tolerance, following [8]. For all methods evaluated on SpatialRGPT-Bench, except SpatialRGPT itself, object references are provided via bounding boxes drawn on the image. For 3D grounding, to ensure fair comparison across varying depth scales and box types, we compute projected IoU and projected center offset by projecting predicted 3D bounding box onto the image plane and comparing it with ground-truth 2D boxes. We also try to align the predicted boxes to the depth of the ground-truth boxes and compute 3D IoU and 3D center offset. To mitigate the alignment noise, 3D metrics are reported on sampled subset. Table 2. Quantitative comparison on spatial reasoning benchmarks. Our N3D-VLM consistently outperforms baseline methods across all three spatial reasoning benchmarks, achieving state-of-the-art performance on open-ended, numerical, and multiple-choice questions."
        },
        {
            "title": "Method",
            "content": "Closed-source GPT-4o [11] Gemini-2.5-Flash [28] Open-source Qwen2.5-VL-7B [2] Qwen3-VL-8B [29] SpatialLadder-3B [14] SpatialReasoner-7B [21] SpatialRGPT-VILA-1.5-8B [8]"
        },
        {
            "title": "Ours",
            "content": "N3D-VLM-3B N3D-VLM-7B N3D-Bench SpatialRGPT-Bench [8] CV-Bench-3D [30] Open-ended Numerical Open-ended Numerical Multi-choice 63.5 64.2 55.0 66.3 48.9 54.8 63.1 77.0 89.7 27.8 36.7 22.5 36.3 18.1 27.4 50.4 90.1 92. 76.3 82.4 74.4 89.2 55.9 63.2 92.7 80.5 95.7 55.8 42.2 38.2 40.7 26.5 33.7 62.7 73.3 78. 72.4 86.0 75.8 91.3 74.9 80.3 63.3 96.3 93.3 Table 3. Quantitative comparison of 3D grounding using IoU and center offset of projected 3D bounding boxes. Our N3D-VLM achieves the best performance on both projected IoU and offset, demonstrating our superior accuracy in localizing objects in 3D space."
        },
        {
            "title": "Method",
            "content": "Refcoco [12] Refcoco+ [12] Refcocog [12] Objects365 [27] Proj. IoU Proj. Offset Proj. IoU Proj. Offset Proj. IoU Proj. Offset Proj. IoU Proj. Offset Qwen3-VL-8B [29] Qwen3-VL-30B-A3B [29] Our N3D-VLM-7B 0.37 0.38 0.59 0.16 0.14 0. 0.34 0.36 0.53 0.26 0.16 0.10 0.36 0.38 0. 0.14 0.13 0.08 0.28 0.28 0.61 0.12 0.13 0. Table 4. Quantitative comparison of 3D bounding boxes. Our N3D-VLM achieves the best results across 3D IoU and 3D offset, demonstrating superior performance in 3D object localization. Table 5. Ablation studies on model design. Variants (3) and (4), which adopt our model design shown in Fig. 3(a), achieve the best 3D detection performance, when compared with other variants."
        },
        {
            "title": "Method",
            "content": "Refcoco/+/g [12] 3D IoU 3D Offset Qwen3-VL-8B [29] Qwen3-VL-30B-A3B [29] Our N3D-VLM-7B 0.20 0. 0.48 1.88 1.86 0.36 4.2. Main Results 3D Spatial QA. Tab. 2 compares the accuracy of our method with baseline approaches across three benchmarks. Our model achieves the highest accuracy on all benchmarks, demonstrating that our native 3D grounding significantly improves performance on 3D spatial reasoning tasks. Compared to the base model Qwen2.5-VL-7B, our 7B model shows substantial gains, especially on numerical questions, which indicates that grounding-based reasoning enhances the models quantitative understanding beyond standard QA capabilities. While Qwen3-VL improves over Qwen2.5-VL in spatial reasoning, its numerical reasoning remains limited, with only 36.3% and 40.7% accuracy. In contrast, our model achieves 92.1% and 78.0% on the same tasks, demonstrating significantly stronger numerical reasoning. Although SpatialRGPT performs well on numerical questions in N3D-Bench, achieving 50.4% which outperforms Qwen3-VL, but still far below our models 92.1%. 3D Object Grounding. Tab. 3, Tab. 4 and Fig. 4 present the quantitative and qualitative comparisons of 3D ground-"
        },
        {
            "title": "Method",
            "content": "F1@0.25 P@0.25 R@0.25 (0) SpatialLM [22]-340K (1) 3B-340K-nodepth (2) 3B-340K-cameraxy (3) 3B-340K-imageuv (4) 3B-1.7M-imageuv 2. 9.4 10.8 12.8 22.9 2.3 10.9 11.6 13.6 24.3 2. 9.4 10.7 12.9 22.9 ing performance between our model and Qwen3-VL [29]. As shown in Tab. 3 and 4, our method consistently outperforms Qwen3-VL in both projected 2D metrics and aligned 3D bounding box evaluation, demonstrating stronger 3D grounding capabilities. Fig. 4 further shows that our model accurately localize objects in 3D space, producing precise 3D bounding boxes across diverse scenarios, including indoor and outdoor environments. We provide more grounding results comparison and analysis in the Appendix. 4.3. Ablation Study on Model Design We conduct ablation studies of the 3D detection task on Objects365 dataset to evaluate the effectiveness of our model design. Tab. 5 reports the 3D IoU results on validation set of 5,565 images covering 341 object classes. Row (0) shows that the SpatialLM [22] architecture, which combines point cloud encoder [33] and an LLM [2], performs poorly on our more diverse point cloud data, indicating limited generalization. Variants (1) and (2) are based on our Figure 4. Qualitative comparison of 3D grounding capability with Qwen3-VL-8B [29]. Compared to Qwen3-VL-8B, our N3DVLM generates 3D bounding boxes that more accurately close to the ground truth, reflecting stronger 3D understanding and localization precision. In the visualization, green boxes represent ground truth 3D bounding boxes, and red boxes indicate models predictions. model architecture, with (1) removing depth input and (2) using depth but directly predicting 3D coordinates in the camera coordinate system. Variants (3) and (4) follow our full design (Fig. 3a), which takes depth input and predicts 2D pixel-space coordinates shown in Fig. 3c. Comparing (1) and (3), we can observe that incorporating depth input improves 3D detection accuracy, increasing the F1 score from 9.4 to 12.8. Comparing (2) and (3), we can find that predicting the center coordinates in pixel space outperforms camera-space prediction, likely because the base model is pretrained with 2D perception data, which is naturally aligned with the image-space representations. These results suggest that using depth input and predicting pixelspace coordinates, is more effective than excluding depth or directly predicting in the camera coordinate system. Finally, scaling the training set from 340K to 1.7M samples in (4) leads to substantial improvements, demonstrating the effectiveness of our large-scale 3D data generation pipeline. 4.4. 3D Grounding Helps Spatial Reasoning We design two experiments to demonstrate that native 3D grounding explicitly improves spatial reasoning. First, we feed the grounding results from our model into Qwen3-VL, prompting it to reason based on our 3D grounding output. Table 6. Ablation studies on the effectiveness of 3D grounding. Our intermediate 3D grounding results can improve Qwen3-VLs performance, while training our model directly on QA data leads to degraded results. These findings demonstrate that our unified N3D-VLM, which first performing grounding, then spatial reasoning, effectively enhances the spatial understanding."
        },
        {
            "title": "Method",
            "content": "N3D-open N3D-num Qwen3-VL-8B [29] (direct answer) Qwen3-VL-8B [29] (ground. given) Improvement (%) Our QAonly-7B Our N3D-VLM-7B 66.3 71.3 +7.5% 80.6 89.7 36.3 54.6 +50.4% 62.4 92.1 As shown in Tab. 6, Qwen3-VL achieves higher accuracy with the grounding results compared to answering directly, indicating that 3D grounding enhances spatial reasoning. However, its performance still lags behind our 7B model, suggesting that our model performs strong spatial reasoning based on 3D grounding. In the second experiment, we use the same architecture but train it end-to-end for question answering without separating grounding and reasoning. This setup underperforms our full model, confirming that explicitly decomposing the task into grounding and reasoning leads to better performance under same architecture. 5. Conclusion"
        },
        {
            "title": "References",
            "content": "We present unified framework N3D-VLM that bridges 3D object perception and spatial reasoning within single model. By enabling native 3D grounding and explicit 3Daware reasoning, our approach offers both accurate localization and interpretable spatial understanding. Supported by scalable data construction pipeline that projects 2D annotations into 3D space and facilitates the creation of explicit reasoning datasets, our model demonstrates reasonable generalization in 3D grounding and provides structured foundation for spatial reasoning. Extensive experiments validate the effectiveness of our framework, which achieves strong performance in both 3D grounding and 3D spatial reasoning among existing vision-language models. [1] Armen Avetisyan, Christopher Xie, Henry Howard-Jenkins, Tsun-Yi Yang, Samir Aroudj, Suvam Patra, Fuyang Zhang, Duncan Frost, Luke Holland, Campbell Orme, et al. Scenescript: Reconstructing scenes with an autoregressive structured language model. In ECCV, 2024. 2, 4 [2] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. 2, 3, 6, 7 [3] Gilad Baruch, Zhuoyuan Chen, Afshin Dehghan, Tal Dimry, Yuri Feigin, Peter Fu, Thomas Gebauer, Brandon Joffe, Daniel Kurz, Arik Schwartz, et al. Arkitscenes: diverse real-world dataset for 3d indoor scene understanding using mobile rgb-d data. arXiv preprint arXiv:2111.08897, 2021. 2 [4] Garrick Brazil, Abhinav Kumar, Julian Straub, Nikhila Ravi, Justin Johnson, and Georgia Gkioxari. Omni3d: large benchmark and model for 3d object detection in the wild. In CVPR, 2023. 2, 4 [5] Holger Caesar, Varun Bankiti, Alex Lang, Sourabh Vora, Venice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan, Giancarlo Baldan, and Oscar Beijbom. nuscenes: multimodal dataset for autonomous driving. In CVPR, 2020. 2 [6] Boyuan Chen, Zhuo Xu, Sean Kirmani, Brain Ichter, Dorsa Sadigh, Leonidas Guibas, and Fei Xia. Spatialvlm: Endowing vision-language models with spatial reasoning capabilities. In CVPR, 2024. [7] Sijin Chen, Xin Chen, Chi Zhang, Mingsheng Li, Gang Yu, Hao Fei, Hongyuan Zhu, Jiayuan Fan, and Tao Chen. Ll3da: Visual interactive instruction tuning for omni-3d understanding reasoning and planning. In CVPR, 2024. 3 [8] An-Chieh Cheng, Hongxu Yin, Yang Fu, Qiushan Guo, Ruihan Yang, Jan Kautz, Xiaolong Wang, and Sifei Liu. Spatialrgpt: Grounded spatial reasoning in vision-language models. In NeurIPS, 2024. 2, 3, 5, 6, 7 [9] Andreas Geiger, Philip Lenz, Christoph Stiller, and Raquel Urtasun. Vision meets robotics: The kitti dataset. In IJRR, 2013. 2 [10] Peiqi He, Zhenhao Zhang, Yixiang Zhang, Xiongjun Zhao, and Shaoliang Peng. Spatial-ormllm: Improve spatial relation understanding in the operating room with multimodal large language model. arXiv preprint arXiv:2508.08199, 2025. 2, 3 [11] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. 2, 3, 6, 7, 5 [12] Sahar Kazemzadeh, Vicente Ordonez, Mark Matten, and Tamara Berg. ReferItGame: Referring to objects in photographs of natural scenes. In EMNLP, 2014. 5, 6, 7 [13] Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Uijlings, Ivan Krasin, Jordi Pont-Tuset, Shahab Kamali, Stefan Popov, Matteo Malloci, Alexander Kolesnikov, et al. The open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale. In IJCV, 2020. 2, [14] Hongxing Li, Dingming Li, Zixuan Wang, Yuchen Yan, Hang Wu, Wenqi Zhang, Yongliang Shen, Weiming Lu, Jun Xiao, and Yueting Zhuang. Spatialladder: Progressive training for spatial reasoning in vision-language models. arXiv preprint arXiv:2510.08531, 2025. 2, 7 [15] Rong Li, Shijie Li, Lingdong Kong, Xulei Yang, and Junwei Liang. Seeground: See and ground for zero-shot openvocabulary 3d visual grounding. In CVPR, 2025. 2, 3 [16] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and Lawrence Zitnick. Microsoft coco: Common objects in context. In ECCV, 2014. 2 [17] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and Lawrence In Zitnick. Microsoft coco: Common objects in context. ECCV, 2014. 4 [18] Dingning Liu, Xiaomeng Dong, Renrui Zhang, Xu Luo, Peng Gao, Xiaoshui Huang, Yongshun Gong, and Zhihui Wang. 3daxiesprompts: Unleashing the 3d spatial task capabilities of gpt-4v. arXiv preprint arXiv:2312.09738, 2023. 3 [19] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In NeurIPS, 2023. 2, [20] Zhenyang Liu, Yikai Wang, Sixiao Zheng, Tongying Pan, Longfei Liang, Yanwei Fu, and Xiangyang Xue. Reasongrounder: Lvlm-guided hierarchical feature splatting for open-vocabulary 3d visual grounding and reasoning. In CVPR, 2025. 3 [21] Wufei Ma, Yu-Cheng Chou, Qihao Liu, Xingrui Wang, Celso de Melo, Jianwen Xie, and Alan Yuille. Spatialreasoner: Towards explicit and generalizable 3d spatial reasoning. arXiv preprint arXiv:2504.20024, 2025. 2, 3, 7, 6 [22] Yongsen Mao, Junhao Zhong, Chuan Fang, Jia Zheng, Rui Tang, Hao Zhu, Ping Tan, and Zihan Zhou. Spatiallm: Training large language models for structured indoor modeling. arXiv preprint arXiv:2506.07491, 2025. 2, 3, 4, 7 [23] Zhangyang Qi, Zhixiong Zhang, Ye Fang, Jiaqi Wang, and Hengshuang Zhao. Gpt4scene: Understand 3d scenes from videos with vision-language models. arXiv preprint arXiv:2501.01428, 2025. 2, 3 [24] Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman Radle, Chloe Rolland, Laura Gustafson, et al. Sam 2: arXiv preprint Segment anything in images and videos. arXiv:2408.00714, 2024. 2, 4 [25] Mike Roberts, Jason Ramapuram, Anurag Ranjan, Atulit Kumar, Miguel Angel Bautista, Nathan Paczan, Russ Webb, and Joshua Susskind. Hypersim: photorealistic synthetic dataset for holistic indoor scene understanding. In ICCV, 2021. [26] Jonas Schult, Francis Engelmann, Alexander Hermans, Or Litany, Siyu Tang, and Bastian Leibe. Mask3d: Mask transformer for 3d semantic instance segmentation. arXiv preprint arXiv:2210.03105, 2022. 2 [27] Shuai Shao, Zeming Li, Tianyuan Zhang, Chao Peng, Gang Yu, Xiangyu Zhang, Jing Li, and Jian Sun. Objects365: large-scale, high-quality dataset for object detection. In ICCV, 2019. 2, 4, 7 [28] Gemini Team, Petko Georgiev, Ving Ian Lei, Ryan Burnell, Libin Bai, Anmol Gulati, Garrett Tanzer, Damien Vincent, Zhufeng Pan, Shibo Wang, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. 2, 3, 7 [29] Qwen Team. Qwen3-vl github repository. https:// github.com/QwenLM/Qwen3VL, 2025. 7, 8, 2, 3, 4, 5 [30] Shengbang Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Manoj Middepogu, Sai Charitha Akula, Jihan Yang, Shusheng Yang, Adithya Iyer, Xichen Pan, Austin Wang, Rob Fergus, Yann LeCun, and Saining Xie. Cambrian-1: fully open, vision-centric exploration of multimodal llms. arXiv preprint arXiv:2406.16860, 2024. 6, 7 [31] Haochen Wang, Yucheng Zhao, Tiancai Wang, Haoqiang Fan, Xiangyu Zhang, and Zhaoxiang Zhang. Ross3d: Reconstructive visual instruction tuning with 3d-awareness. arXiv preprint arXiv:2504.01901, 2025. 3 [32] Ruicheng Wang, Sicheng Xu, Yue Dong, Yu Deng, Jianfeng Xiang, Zelong Lv, Guangzhong Sun, Xin Tong, and Jiaolong Yang. Moge-2: Accurate monocular geometry with metric scale and sharp details. arXiv preprint arXiv:2507.02546, 2025. 2, 3, 4, [33] Xiaoyang Wu, Daniel DeTone, Duncan Frost, Tianwei Shen, Chris Xie, Nan Yang, Jakob Engel, Richard Newcombe, Hengshuang Zhao, and Julian Straub. Sonata: Selfsupervised learning of reliable point representations. In CVPR, 2025. 7 [34] Runsen Xu, Zhiwei Huang, Tai Wang, Yilun Chen, JiangVlm-grounder: vlm arXiv preprint miao Pang, and Dahua Lin. agent for zero-shot 3d visual grounding. arXiv:2410.13860, 2024. 2, 3 [35] Jihan Yang, Shusheng Yang, Anjali Gupta, Rilyn Han, Li Fei-Fei, and Saining Xie. Thinking in space: How multimodal large language models see, remember, and recall spaces. In CVPR, 2025. 3 [36] Lihe Yang, Bingyi Kang, Zilong Huang, Zhen Zhao, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. Depth anything v2. In NeurIPS, 2024. 3 [37] Hanxue Zhang, Haoran Jiang, Qingsong Yao, Yanan Sun, Renrui Zhang, Hao Zhao, Hongyang Li, Hongzi Zhu, and Zetong Yang. Detect anything 3d in the wild. arXiv preprint arXiv:2504.07958, 2025. 2, [38] Youcai Zhang, Xinyu Huang, Jinyu Ma, Zhaoyang Li, Zhaochuan Luo, Yanchun Xie, Yuzhuo Qin, Tong Luo, Yaqian Li, Shilong Liu, et al. Recognize anything: strong image tagging model. In CVPR, 2024. 4 [39] Duo Zheng, Shijia Huang, and Liwei Wang. Video-3d llm: Learning position-aware video representation for 3d scene understanding. In CVPR, 2025. 2, 3 [40] Chenming Zhu, Tai Wang, Wenwei Zhang, Jiangmiao Pang, and Xihui Liu. Llava-3d: simple yet effective pathway to empowering lmms with 3d-awareness. arXiv preprint arXiv:2409.18125, 2024. 3 N3D-VLM: Native 3D Grounding Enables Accurate Spatial Reasoning in Vision-Language Models"
        },
        {
            "title": "Supplementary Material",
            "content": "In this supplementary material, we provide more additional experiments in Sec. A. We also present video demo for more qualitative results in Sec. B. We illustrate the data distribution of our proposed N3D-Bench in Sec. C. A. Additional Experiments A.1. 3D Grounding Comparison We present qualitative comparison of the native 3D grounding capability of our N3D-VLM with two baseline methods, SpatialLM [22] and Qwen3-VL-8B [29], as shown in Fig. 5 and Fig. 6. SpatialLM is vision-language model designed for 3D grounding in the indoor scenes using point cloud input. We generate point clouds for SpatialLM by combining the input image with depth maps obtained via [32]. For Qwen3-VL-8B [29], we apply the depth alignment strategy described in the main paper to align its predictions with our ground truth. Fig. 5 shows two indoor scene examples. In the first, our method accurately localizes the pillows, while both baselines either detect only subset of objects or exhibit inaccurate spatial prediction. In the second scene, our method also provides precise localization of the washing machines. Fig. 6 presents two diverse outdoor scenes. SpatialLM fails in these cases, as it is limited to predefined set of indoor categories. In both examples, our N3D-VLM outperforms Qwen3VL-8B for detection, demonstrating superior native 3D grounding capabilities, which are essential for reliable spatial reasoning. We also consider SpatialReasoner [21], related method capable of predicting 3D object centers. However, it does not support explicit 3D bounding box output, limiting its capacity for comprehensive spatial perception. In our evaluation, we observed that even when prompted to output object centers, the results were inconsistent and often failed to follow coherent 3D coordinate format. A.2. 3D Spatial Reasoning Comparison We present qualitative comparison of the 3D spatial reasoning capabilities of our N3D-VLM with GPT-4o [11], Qwen3-VL-8B [29], SpatialRGPT [8], and SpatialReasoner [21], as shown in Fig. 7 and Fig. 8. Fig. 7 compares our N3D-VLM with GPT-4o and Qwen3-VL-8B. In Example 1, both GPT-4o and Qwen3-VL fail to answer the question correctly due to incorrect reasoning about the change in viewpoint. In contrast, our N3D-VLM provides the correct answer by leveraging accurate 3D grounding and reasoning over the grounded results. In Example 2, GPT-4o and Qwen3-VL-8B overly rely on commonsense priors while neglecting visual cues. Our method, empowered by native 3D grounding, directly localizes the bounding box to answer the question accurately. Fig. 8 compares our method with SpatialRGPT and SpatialReasoner on questions involving relative distance comparison and depth comparison among three objects. SpatialRGPT either provides incorrect answers or misunderstands the question entirely. Although SpatialReasoner has the potential to reason using 3D coordinates, it fails to complete the reasoning process. For instance, in Example 1, it incorrectly attempts to calculate absolute distances for depth-ordering question, resulting in rigid and ultimately incorrect reasoning. In contrast, our N3D-VLM not only accurately localizes 3D bounding boxes of objects but also reliably performs reasoning over grounded results, demonstrating its effectiveness in 3D spatial understanding tasks. As supplement to Table 2 in the main paper, Tab. 7 and Tab. 8 report accuracy scores across different question types. Our N3D-VLM consistently outperforms our base model Qwen2.5VL [2], confirming that native 3D grounding significantly enhances the models ability to understand various forms of spatial questions. A.3. Failure Cases We present two failure cases of 3D grounding in Fig. 9. In the first example, ducks reflection on the water surface (highlighted by green circle) is mistakenly identified as real object, suggesting that our model could benefit from better understanding of specular reflections. In the second example, although N3D-VLM successfully detects 30 jellyfish with accurate 3D bounding boxes, several jellyfish are still missed, as indicated in the green circle. These cases highlight that our method, while effective, still has room for improvement. Enhancing the 3D grounding capabilities may further boost overall performance in 3D spatial understanding tasks. B. Video Demo As supplement to Sec. A, we have included video demo to showcase the qualitative results in video format. C. N3D-Bench Details C.1. Distribution Summary In Table 1 of the main paper, we compare our N3D-Bench with SpatialRGPT-Bench, showing that N3D-Bench covers Figure 5. Qualitative comparison of 3D grounding capability with SpatialLM [22] and Qwen3-VL-8B [29] in indoor scenes. our N3D-VLM accurately localizes objects such as pillows and washing machines, while baselines either miss objects or exhibit inaccurate prediction. In the visualization, green boxes represent ground truth 3D bounding boxes, and red boxes indicate models predictions. broader range of object categories and question types. As supplement to Table 1, we further present the distribution of question types and object categories in N3DBench. Specifically, Fig. 11 shows the distribution of quesFigure 6. Qualitative comparison of 3D grounding capability with SpatialLM [22] and Qwen3-VL-8B [29] in outdoor scenes. Our N3D-VLM outperforms Qwen3-VL-8B and SpatialLM, highlighting its superior native 3D grounding capability for reliable spatial reasoning. In the visualization, green boxes represent ground truth 3D bounding boxes, and red boxes indicate models predictions. tion types. The dataset contains 2,000 questions grouped into 11 major categories, reflecting both high diversity and balanced coverage. For example, the questions include reasoning about relative distance comparison and depth comparison among three or more objects. For relative spatial relations such as left/right, we also incorporate variations in viewpoint, including phrasing like from the opposite direction of the camera. Fig. 10 illustrates the distribution of object categories involved in the questions. N3D-Bench includes 264 commonly encountered indoor and outdoor object classes, derived from the Objects365 [27] dataset. Figure 7. Qualitative comparison of 3D spatial reasoning capability with GPT-4o [11] and Qwen3-VL-8B [29]. Our N3D-VLM outperforms GPT-4o and Qwen3-VL-8B by leveraging accurate 3D grounding and 3D spatial reasoning. Figure 8. Qualitative comparison of 3D spatial reasoning capability with SpatialRGPT [8] and SpatialReasoner [21]. N3D-VLM outperforms SpatialRGPT and SpatialReasoner by accurately interpreting the question and reasoning over explicit 3D bounding boxes. Table 7. Detailed quantitative comparison on spatial reasoning benchmark N3D-Bench. Dark green indicates the highest accuracy, while light green denotes the second-highest accuracy. Category Method Open-ended Numerical N3D-Bench left/right front/behind wide/thin tall/short big/small relative dis. depth comp. width/height distance ver./hor. dis. direction Closed-source GPT-4o [11] Gemini-2.5-Flash [28] Open-source Qwen2.5-VL-7B [2] Qwen3-VL-8B [29] SpatialLadder-3B [14] SpatialReasoner-7B [21] SpatialRGPT-VILA-1.5-8B [8] Ours N3D-VLM-3B N3D-VLM-7B 61.50 63.00 51.50 71.50 45.50 38.50 50. 97.50 95.00 51.00 52.00 45.00 48.00 50.50 51.50 48.00 95.50 93.00 65.33 62.31 58.00 62.50 52.50 62.50 78. 45.50 90.00 62.00 62.00 54.00 63.50 47.00 58.50 69.00 66.50 85.50 68.50 74.00 61.00 71.50 52.00 59.50 75. 62.50 80.00 70.00 70.00 61.97 61.97 52.11 64.79 61.43 92.96 92.96 74.22 73.44 58.14 66.67 41.09 54.26 54. 93.80 95.35 24.50 36.00 12.50 27.50 8.00 16.00 34.00 75.00 82.50 14.00 20.00 15.00 33.00 16.00 36.00 39. 96.00 96.50 14.00 24.50 18.50 21.50 17.00 16.50 36.00 93.00 93.00 25.50 25.00 17.00 27.50 12.00 13.50 62. 83.50 87.00 Table 8. Detailed quantitative comparison on spatial reasoning benchmark SpatialRGPT-Bench. Dark green indicates the highest accuracy, while light green denotes the second-highest accuracy."
        },
        {
            "title": "Method",
            "content": "Open-ended"
        },
        {
            "title": "Numerical",
            "content": "SpatialRGPT-Bench below/above left/right big/small tall/short wide/thin behind/front distance hor. dis. ver. dis. width height direction Closed-source GPT-4o [11] Gemini-2.5-Flash [28] Open-source Qwen2.5-VL-7B [2] Qwen3-VL-8B [29] SpatialLadder-3B [14] SpatialReasoner-7B [21] SpatialRGPT-VILA-1.5-8B [8]"
        },
        {
            "title": "Ours",
            "content": "N3D-VLM-3B N3D-VLM-7B 88.33 85.83 76.67 93.33 58.33 68.33 99.17 97.50 100.0 78.10 92.23 89.52 96.19 56.19 48.57 100. 100.0 99.05 82.08 88.68 71.70 88.68 55.66 68.87 84.90 75.47 94.34 74.11 81.25 71.43 84.82 54.46 66.07 89. 73.21 92.86 68.27 79.81 73.08 88.46 55.77 76.92 91.34 39.42 92.31 65.45 67.27 64.55 83.64 54.55 50.0 90. 94.55 95.45 30.30 12.06 21.15 29.73 24.32 37.16 45.9 72.30 81.08 42.31 12.93 28.74 26.23 21.31 34.43 68. 78.69 85.25 53.12 18.18 21.62 29.25 35.85 17.92 56.6 83.96 88.68 52.24 64.55 35.29 45.80 21.80 34.62 48. 48.12 54.17 71.70 77.24 49.50 55.64 33.08 35.34 61.7 69.70 64.29 65.42 72.64 57.94 59.05 23.71 42.53 95. 93.46 96.26 Figure 9. Failure cases of our native 3D grounding. N3D-VLM misidentifies specular reflection and misses several objects, suggesting room for improvement in handling reflections and dense object scenes. Figure 10. Distribution of object classes in N3D-Bench. Figure 11. Distribution of question types in N3D-Bench."
        }
    ],
    "affiliations": [
        "CUHK",
        "HKUST",
        "NJU",
        "Tencent AI Lab",
        "ZJU"
    ]
}