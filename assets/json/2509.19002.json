{
    "paper_title": "VIR-Bench: Evaluating Geospatial and Temporal Understanding of MLLMs via Travel Video Itinerary Reconstruction",
    "authors": [
        "Hao Wang",
        "Eiki Murata",
        "Lingfang Zhang",
        "Ayako Sato",
        "So Fukuda",
        "Ziqi Yin",
        "Wentao Hu",
        "Keisuke Nakao",
        "Yusuke Nakamura",
        "Sebastian Zwirner",
        "Yi-Chia Chen",
        "Hiroyuki Otomo",
        "Hiroki Ouchi",
        "Daisuke Kawahara"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advances in multimodal large language models (MLLMs) have significantly enhanced video understanding capabilities, opening new possibilities for practical applications. Yet current video benchmarks focus largely on indoor scenes or short-range outdoor activities, leaving the challenges associated with long-distance travel largely unexplored. Mastering extended geospatial-temporal trajectories is critical for next-generation MLLMs, underpinning real-world tasks such as embodied-AI planning and navigation. To bridge this gap, we present VIR-Bench, a novel benchmark consisting of 200 travel videos that frames itinerary reconstruction as a challenging task designed to evaluate and push forward MLLMs' geospatial-temporal intelligence. Experimental results reveal that state-of-the-art MLLMs, including proprietary ones, struggle to achieve high scores, underscoring the difficulty of handling videos that span extended spatial and temporal scales. Moreover, we conduct an in-depth case study in which we develop a prototype travel-planning agent that leverages the insights gained from VIR-Bench. The agent's markedly improved itinerary recommendations verify that our evaluation protocol not only benchmarks models effectively but also translates into concrete performance gains in user-facing applications."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 2 ] . [ 1 2 0 0 9 1 . 9 0 5 2 : r VIR-Bench: Evaluating Geospatial and Temporal Understanding of MLLMs via Travel Video Itinerary Reconstruction Hao Wang 1 Eiki Murata 2,3 Lingfang Zhang1 Ayako Sato2 So Fukuda1 Ziqi Yin1 Wentao Hu1 Keisuke Nakao1 Yusuke Nakamura1 Sebastian Zwirner1 Yi-Chia Chen1 Hiroyuki Otomo2 Hiroki Ouchi4,2 Daisuke Kawahara 1Waseda University 2CyberAgent, Inc. 3AI Shift, Inc. 4Nara Institute of Science and Technology https://github.com/nlp-waseda/VIR-Bench"
        },
        {
            "title": "Abstract",
            "content": "Recent advances in multimodal large language models (MLLMs) have significantly enhanced video understanding capabilities, opening new possibilities for practical applications. Yet current video benchmarks focus largely on indoor scenes or short-range outdoor activities, leaving the challenges associated with long-distance travel largely unexplored. Mastering extended geospatial-temporal trajectories is critical for next-generation MLLMs, underpinning real-world tasks such as embodied-AI planning and navigation. To bridge this gap, we present VIR-Bench, novel benchmark consisting of 200 travel videos that frames itinerary reconstruction as challenging task designed to evaluate and push forward MLLMs geospatial-temporal intelligence. Experimental results reveal that state-of-the-art MLLMs, including proprietary ones, struggle to achieve high scores, underscoring the difficulty of handling videos that span extended spatial and temporal scales. Moreover, we conduct an in-depth case study in which we develop prototype travel-planning agent that leverages the insights gained from VIR-Bench. The agents markedly improved itinerary recommendations verify that our evaluation protocol not only benchmarks models effectively but also translates into concrete performance gains in user-facing applications."
        },
        {
            "title": "Introduction",
            "content": "Recent advances in multimodal large language models (MLLMs) Liu et al. (2023); Li et al. (2024a); Lin et al. (2024); OpenAI (2024a) have improved remarkable capabilities in video understanding. Lately, research attention has shifted toward evaluating the spatial and temporal reasoning abilities of MLLMs, prompting the proposal of new benchmarks Grauman et al. (2022); Chandrasegaran et al. (2024); Jia et al. (2024); Yang et al. (2025); Lin et al. (2025). However, existing benchmarks primarily focus on micro-scale scenarios, such as indoor scenes or short-range outdoor activities, leaving macro-scale geospatial scenarios, namely, long-distance travel activities involving multi-day footage across multiple cities, largely unexplored. We argue that long-horizon geospatial-temporal Equal contribution. Correspondence to Hao Wang (conan1024hao@akane.waseda.jp) . Preprint. Figure 1: Overview of VIR-Bench. Given an input travel video (Top), we reconstruct visiting order graph (Right) whose nodes are visited locations (prefectures, cities, and POIs) and whose edges capture both temporal transitions and geographic containment among the locations. The itinerary visualization (Left) omits the second stop at Atami Station for visual clarity. The video frames are adopted from https://www.youtube.com/watch?v=6aJ4CZfn9c8. reasoning is essential for next-generation MLLMs, as numerous real-world applications, such as embodied AI planning, navigation, and autonomous driving, heavily rely on these capabilities. To address this gap, we introduce VIR-Bench, benchmark to evaluate long-range geospatialtemporal understanding via itinerary reconstruction from travel vlog videos. The core output is directed visiting order graph Yamamoto et al. (2025): nodes represent locations at three granularities (prefecture, city, and point of interest (POI)) and edges represent two relations, inclusion for spatial hierarchy and transition for temporal adjacency. By decomposing the task into two sub-tasks: (1) node prediction, identifying all locations visited; and (2) edge prediction, inferring geographic inclusion relations and temporal transition relations among visited locations, VIR-Bench enables separate evaluation of geospatial and temporal intelligence. Because the footage is mostly egocentric or selfie-style, models must construct holistic understanding from partial views, which further stresses geospatialtemporal reasoning. The dataset comprises 200 travel vlogs filmed across Japan, major inbound tourism destination, each accompanied by manually annotated and double-reviewed visiting order graph. Through extensive experiments on state-of-the-art open-weight and proprietary MLLMs, we observe persistent challenge in geospatial and temporal understanding. Particularly, open-weight models suffer from insufficient geographic knowledge and limited capability for long-context reasoning. Although proprietary models achieve better performance, they still struggle on POI node prediction and transition edge prediction, which remain major bottlenecks. Ablations further reveal that more visual context (frames), greater reasoning effort, and access to audio each provide consistent gains. Collectively, these findings highlight key obstacles that need to be addressed to advance geospatialtemporal applications in real-world settings. In addition, we develop prototype travel-planning agent that generates travel plans directly from videos and their corresponding visiting order graphs. Results from crowdsourcing and automatic evaluations indicate that the itinerary, represented by the POI list, is essential for producing logically sound and feasible plans, underscoring the importance of itinerary reconstruction. Meanwhile, the video provides rich, nuanced context that enhances the attractiveness of travel plan. setting 2 that uses both the itinerary and the video leverages these complementary strengths, highlighting promising approach to generating high-quality travel plans from multimedia sources. These findings validate that our benchmark not only effectively evaluates models but also pushes forward practical user-facing applications."
        },
        {
            "title": "2.1 Video Benchmarks",
            "content": "With the rapid advancement of MLLMs, recent video understanding benchmarks have increasingly prioritized evaluating models spatial and temporal reasoning capabilities. For instance, Ego4D Grauman et al. (2022) facilitates the evaluation of models comprehension of past and future events by utilizing curated temporal data, while HourVideo Chandrasegaran et al. (2024) examines the performance of models in understanding extended-duration video content. VSI-Bench Yang et al. (2025) assesses models ability to infer 3D scene layouts from 2D video inputs, while OST-Bench Lin et al. (2025) evaluates spatial-temporal understanding by requiring models to explore and interpret information within 3D space. CityGuessr Kulkarni et al. (2024) introduces video-based benchmark for assessing geo-localization using driving videos, while UrbanVideo-Bench Zhao et al. (2025) targets the embodied cognitive abilities of MLLMs within urban 3D environments using drone-collected footage. Nevertheless, most existing benchmarks primarily feature indoor scenarios or short-distance outdoor movements, lacking extensive long-distance traversal, such as inter-city journeys. Consequently, these benchmarks are insufficient for thoroughly evaluating the geospatial-temporal intelligence of MLLMs. In contrast, VIR-Bench specifically addresses this gap by comprehensively assessing video understanding capabilities across extended spatial (e.g., from Tokyo to Osaka, Hokkaido to Kyoto) and temporal (spanning multiple days) scales. 2."
        },
        {
            "title": "Itinerary Extraction",
            "content": "Researchers in natural language processing have substantially studied the task of extracting travel trajectories from text Drymonas and Pfoser (2010); Kaushik et al. (2017); Haris and Gan (2021); Yamamoto et al. (2025). representative study by Yamamoto et al. (2025) introduces visiting order graph designed to capture relationships among visited locations and provides benchmark dataset for training and evaluating itinerary extraction models. In multimodal settings, Pang et al. (2011) proposes framework aimed at summarizing travelogues by integrating text and images from blogs. More recently, Rosa (2024) leverages MLLMs to perform structured entity extraction from travel videos, while Zhuang et al. (2024) tackles the inverse problem by generating vlogs with diverse travel scenes. This study advances this line of work by providing, to our knowledge, the first systematic investigation into extracting and reconstructing itineraries directly from videos, establishing new benchmark for video-centric geospatial and temporal understanding. 2."
        },
        {
            "title": "Itinerary Generation",
            "content": "The complexity of manual trip planning has driven research into automated itinerary generation. Initial approaches were often based on optimization problems like the Tourist Trip Design Problem and classic machine learning Gavalas et al. (2014); Chen et al. (2016); He et al. (2019); Carrillo et al. (2023). More recently, Large Language Models (LLMs) have enabled more sophisticated and flexible frameworks Chen et al. (2024); Xie et al. (2024). Current research trends include developing novel reasoning paradigms Gui et al. (2025), creating hybrid systems that combine LLMs with classical planners de la Rosa et al. (2024), and enhancing personalization through user model integration and interactive feedback Singh et al. (2024); Chen et al. (2024); Otaki and Baba (2025). To rigorously evaluate these methods, various benchmarks have been developed. Notable studies include real-world planning Xie et al. (2024), fine-grained spatio-temporal planning Chaudhuri et al. (2025), and assessing personalization Singh et al. (2024). Existing benchmarks and generation methods primarily use text data such as user preferences and travel logs as input. In contrast, this research uses travel videos as input, aiming to reconstruct the itinerary based on their content. 3 Figure 2: Example of visiting order graph. Inclusion edges represent containment relationships, flowing from larger geographical area to smaller one. Transition edges indicate chronological movement between distinct locations at the same hierarchical level."
        },
        {
            "title": "3 Dataset Construction",
            "content": "VIR-Bench comprises 200 travel videos filmed across Japan, each paired with corresponding visiting order graph that captures the itinerary depicted in the video. In this section, we present the construction process of VIR-Bench. We begin by defining the visiting order graph, followed by detailed description of our data annotation procedure."
        },
        {
            "title": "3.1 Visiting Order Graph",
            "content": "We adopt and refine the definition of visiting order graph introduced by Yamamoto et al. (2025). simplified example is shown in Figure 2. visiting order graph is hierarchical directed graph with four node types: Root node: the starting node of the graph. Prefecture node: the highest-level administrative division (e.g., Tokyo, Osaka, Aichi). City node: municipality within prefecture, including Tokyos special wards, cities, towns, and villages. POI node: specific named location (point of interest), such as landmarks, tourist attractions, stations, restaurants, cafes, stores, parks, or museums. The graph includes two edge types: Inclusion edge: directed edge representing containment of one location within another. This edge flows from the larger geographical area to the smaller one. Prefecture City (e.g., Aichi Nagoya). City POI (e.g., Nagoya Nagoya Station). POI sub-POI (e.g., Nagoya Station Ippudo - Nagoya Station Branch). Transition edge: directed edge representing movement between two distinct locations at the same hierarchical level; it indicates the chronological flow of travel. Between prefectures (e.g., Tokyo Aichi). Between cities within the same prefecture (e.g., Taito City Chiyoda City). Between POIs within the same city (e.g., Nagoya Station MIRAI TOWER). To prevent cycles in the graph, we treat multiple visits to the same location as distinct nodes. Following Yamamoto et al. (2025), we also introduce special Overlap edge to handle POIs that are geographically overlapping but cannot be represented through inclusion edges."
        },
        {
            "title": "3.2 Data Annotation",
            "content": "Identifying locations visited in travel videos is similar to playing GeoGuessr2: annotators infer places from visual cues. We recruited 10 Japan-based annotators, each tasked with collecting 20 YouTube travel videos filmed in Japan. The videos could be narrated in English or Japanese. Annotators were asked to identify all visited POIs in each video. We define visit as when the POI appears in the video and it is clear that the videographer visited the facility. For every POI, they recorded the start and end times within the video and provided the corresponding Google Maps URL. When location could not be uniquely identified (e.g., cafe shown without its name), they entered the placeholder UNKNOWN and recorded the POI category (e.g., cat_cafe). After annotation, we retrieved detailed information of each POI including name, address and categories using Google Places API. Using the timestamped POIs, we then constructed visiting order graph for each video (e.g., Figure 2). We also conducted quality check of the generated graphs and corrected errors by rerunning the retrieval step manually when POIs were incorrectly annotated or matched. This pipeline yielded VIR-Bench, dataset of 200 travel videos paired with their corresponding visiting order graphs. In total, 3,689 POIs were identified across 43 of Japans 47 prefectures. Detailed annotation guidelines and dataset statistics are provided in Appendix A."
        },
        {
            "title": "4.1 Task Definition",
            "content": "We aim to generate visiting order graphs directly from videos with MLLMs. However, our preliminary experiments revealed that this end-to-end approach is too difficult for current models. To address this, we decompose the task into two sub-tasks: node prediction and edge prediction. We describe each of these tasks in the following. Node Prediction This task evaluates models geospatial understanding, akin to playing GeoGuessr. Given video, MLLMs are asked to return all visited locations in three JSON lists (prefectures, cities, and POIs). For each POI, the model must also predict its category. Edge Prediction Given video and all visited locations (gold labels, shuffled), MLLMs are asked to predict all inclusion and transition edges that constitute the videos visiting order graph. The output should be JSON list of tuples formatted as <source, target, edge_type>. Inclusion edge prediction evaluates models geospatial knowledge, while transition edge prediction assesses their temporal understanding. We omit overlap edges in this task due to their low frequency."
        },
        {
            "title": "4.2 Benchmark Models",
            "content": "We evaluate the performance of mainstream MLLMs on VIR-Bench, including both open-weight models (VideoLLaMA3 Zhang et al. (2025), LLaVA-Video Zhang et al. (2024)), InternVL3 Zhu et al. (2025), Qwen2.5-VL Bai et al. (2025)) and proprietary models (GPT-4.1 OpenAI (2025a), o4mini OpenAI (2025b), Gemini-2.5-Flash and Pro Comanici et al. (2025a)). All models are evaluated in zero-shot setting. We use as many input frames as permitted by each models interface or pretraining setup; only the Gemini models accept audio input, and full details appear in Appendix B.1."
        },
        {
            "title": "4.3 Evaluation Metrics",
            "content": "We evaluate models using macro-averaged precision, recall, and F1 across both node and edge prediction. For prefecture and city nodes, prediction is considered correct only if it exactly matches the gold labels surface name. For POIs, we apply lightweight sequence-matching algorithm: predictions with similarity score above 0.7 (high similarity) are treated as correct; predictions with score above 0.5 (moderate similarity) are also accepted if the predicted POI category matches the gold category; all others are treated incorrect. For inclusion and transition edges, prediction is counted as correct only when the tuple <source, target, edge_type> exactly matches the gold tuple. 2https://www.geoguessr.com"
        },
        {
            "title": "Model",
            "content": "Open-weight VideoLLaMA3-7B LLaVA-Video-7B LLaVA-Video-72B InternVL3-8B InternVL3-38B InternVL3-78B Qwen2.5-VL-7B Qwen2.5-VL-32B Qwen2.5-VL-72B Proprietary GPT-4.1 o4-mini Gemini-2.5-Flash Gemini-2.5-Pro"
        },
        {
            "title": "Prefecture\nPrecision Recall",
            "content": "F"
        },
        {
            "title": "Precision Recall",
            "content": "F"
        },
        {
            "title": "Precision Recall",
            "content": "F"
        },
        {
            "title": "POI",
            "content": "29.8 15.3 17.8 20.1 48.6 58.2 46.9 74.7 86.2 91.2 90.3 88.7 89.7 41.6 13.7 24.7 17.3 46.3 60.5 45.1 70.6 73.6 85.9 85.6 85.5 89.0 31.6 13.2 18.5 17.9 45.7 56.7 44.5 69.7 77.2 86.5 86.1 85.1 87. 19.1 4.8 10.6 8.8 29.8 41.1 30.7 53.6 65.4 75.9 71.3 74.3 73.4 14.4 6.5 10.4 8.0 19.6 33.7 25.3 38.1 43.8 62.6 59.0 63.7 68.2 14.7 4.9 9.1 7.2 21.8 33.5 25.3 41.2 49.0 66.0 62.3 65.6 68. 24.0 10.8 10.2 10.4 19.4 30.4 27.5 37.4 52.3 61.0 63.1 57.0 51.8 10.3 4.7 7.9 4.5 11.5 14.8 16.8 26.1 26.6 51.0 44.8 50.4 58.1 13.7 6.0 7.8 6.0 13.6 19.0 19.8 29.2 33.9 53.6 50.4 51.5 52. OVR F1 14.6 5.9 8.5 6.8 16.4 22.8 21.9 33.0 38.1 57.0 53.9 55.3 57.4 Table 1: Evaluation results on node prediction. OVR abbrev for Overall. The open-weights and proprietary models with the highest and second-highest overall average scores are highlighted with bright green and light green marks."
        },
        {
            "title": "Model",
            "content": "Open-weight VideoLLaMA3-7B LLaVA-Video-7B LLaVA-Video-72B InternVL3-8B InternVL3-38B InternVL3-78B Qwen2.5-VL-7B Qwen2.5-VL-32B Qwen2.5-VL-72B Proprietary GPT-4.1 o4-mini Gemini-2.5-Flash Gemini-2.5-Pro"
        },
        {
            "title": "Inclusion\nPrecision Recall",
            "content": "39.8 22.7 66.3 48.3 64.7 74.4 32.0 66.6 76.5 78.3 86.0 83.1 94.8 31.1 20.7 60.3 45.2 59.7 67.5 28.6 61.3 69.2 75.9 79.0 74.9 87.6 F1 33.4 21.5 62.5 46.0 61.9 70.6 29.6 63.5 72. 76.5 82.0 78.5 90."
        },
        {
            "title": "Transition\nPrecision Recall",
            "content": "2.5 1.7 10.3 4.9 15.6 20.7 1.5 23.5 32.6 34.2 40.9 42.8 66.4 1.2 0.9 8.6 2.4 12.3 11.8 1.3 15.5 18.5 36.2 41.0 42.4 68.0 OVR F1 23.9 15.4 42.4 31.2 41.8 48.9 18.0 44.6 52. 58.8 64.9 63.4 80.7 F1 1.4 1.1 8.4 2.5 12.9 13.4 1.3 16.5 20.8 34.4 40.5 41.7 66.8 Table 2: Evaluation results on edge prediction. Figure 3: Overall results of topperforming models."
        },
        {
            "title": "4.4 Main Results",
            "content": "We present the evaluation results for node prediction in Table 1 and for edge prediction in Table 2. The overall scores are computed as weighted averages across different node and edge types, with weights proportional to the number of elements in each task category. We also provide detailed error analysis in Appendix B.2. Overall Performance Across all five task categories, open-weight models continue to underperform proprietary models. The strongest open model, Qwen2.5-VL-72B, comes close to proprietary performance on the easier categories (prefecture node prediction and inclusion edge prediction), but substantial gaps remain on the harder categories (POI node prediction and transition edge prediction). Other open models perform markedly worse: the LLaVA-Video series and InternVL3-8B achieve only single-digit F1 in city and POI node prediction, and five of the nine models also remain in single digits on transition edge prediction. In the proprietary models, Gemini-2.5-Pro is the top performer, especially on edge prediction, yet its F1 scores for city/POI node and transition edge prediction remain around 60. Taken together, these findings indicate that VIR-Bench is highly challenging for current MLLMs and highlight persistent limitations in geospatial and temporal understanding. Weak Results on Transition Edge Prediction Across the tables, transition edge prediction is the most challenging task. Video-LLaMA3-7B, LLaVA-Video-7B, and Qwen2.5-VL-7B score only around 1, close to random guessing. plausible factor is the limited number of input frames (e.g.,"
        },
        {
            "title": "Model",
            "content": "Setting Node (Prefecture/City/POI) Edge (Inclusion/Transition)"
        },
        {
            "title": "Frames",
            "content": "GPT-4.1 Reasoning o4-mini"
        },
        {
            "title": "Audio",
            "content": "Gemini2.5-Flash 64 128 256 low medium high 85.8 / 62.5 / 39.6 85.4 / 64.0 / 52.9 86.5 / 66.0 / 53.6 86.8 / 62.0 / 49.1 86.1 / 62.3 / 50.4 86.4 / 63.3 / 51.2 85.1 / 65.6 / 51.5 82.5 / 64.0 / 50. 76.6 / 27.6 78.8 / 33.5 76.5 / 34.4 77.8 / 30.0 82.0 / 40.5 83.2 / 43.8 78.5 / 41.7 82.6 / 22.3 Table 3: Ablation results across frame count, reasoning effort, and audio usage. Node = Prefecture / City / POI nodes, and Edge = Inclusion / Transition edges. All reported scores are F1 values. for the LLaVA-Video and InternVL3 series); however, models with larger budgets (180 for VideoLLaMA3 and 256 for Qwen2.5-VL) still struggle, suggesting the issue is not solely input length. Inspecting outputs from lower-performing models reveals two recurrent failure modes: (i) The model sometimes misinterprets the task, including the definitions of inclusion and transition edges; although it produces valid JSON, it yields nonsensical tuples such as <Tokyo, Shibuya, transition>. (ii) Transition edges are constrained to connect locations at the same hierarchical level; for POIs, edges are permitted only between POIs within the same city. Models often ignore this constraint and predict cross-city transitions, for example linking POI in Tokyo to one in Osaka. These factors render the task an even more challenging test of temporal reasoning. Impact of Model Size When comparing models of different sizes within the same families (LLaVAVideo, InternVL3, Qwen2.5-VL), we observe steady, scale-driven gains on node prediction, whereas edge prediction shows sharp jump; for example, transition F1 improves by about 16 from Qwen2.5VL-7B to Qwen2.5-VL-72B. This pattern reflects the task demands: node prediction is localized, single-point task that primarily relies on geospatial knowledge encoded in the models, whereas edge prediction requires holistic view of the itinerary and thus benefits more from larger models with stronger long-context and temporal reasoning. An exception is LLaVA-Video-72B, which shows minimal improvement over LLaVA-Video-7B in POI node prediction. This is likely due to the limited geographic coverage in the LLaVA-OneVision training data Li et al. (2024b). In contrast, models like Qwen2.5-VL demonstrate strong geo-localization capabilities; in our internal evaluations, Qwen2.5-VL was able to accurately predict POI coordinates, indicating extensive pretraining on geographic data. Thinking Models Performance Among the evaluated models, o4-mini and Gemini-2.5-Pro are the only ones that perform explicit thinking at inference. Although neither has an available nonthinking counterpart, we use GPT-4.1 and Gemini-2.5-Flash as proximate baselines to gauge what thinking contributes on VIR-Bench. On node prediction, the gains from thinking are limited: for POI nodes, o4-mini achieves higher precision but lower recall, while Gemini-2.5-Pro shows the opposite trend, suggesting different thinking strategies between OpenAI and Google. In contrast, for edge prediction, enabling deliberate thinking yields large gains for both models, especially Gemini-2.5-Pro, indicating that temporal understanding demands more complex reasoning. We presume that Geminis advantage stems from its use of audio, which supplies continuous, fine-grained temporal cues that sparsely sampled frames cannot provide, highlighting the need for truly multimodal modeling. To further validate the impact of reasoning and audio usage, we conduct additional ablation studies in the following section."
        },
        {
            "title": "4.5 Ablation Study",
            "content": "We conduct additional ablations varying the number of input frames, reasoning effort, and audio usage; results are reported in Table 3. Increasing the number of input frames consistently improves GPT-4.1s overall performance. In particular, the model limited to 64 frames performs poorly on POI node and transition edge prediction, suggesting that for videos in our benchmark, at least 128 frames (1 frame every 14s) are minimum requirement for reliable temporal reasoning. Higher reasoning effort (i.e., longer thinking) leads o4-mini to better performance, especially on transition 7 edge prediction, confirming our earlier observation that the task requires high-level, long-context reasoning. Removing audio from Gemini-2.5-Flash yields worse results across most categories, with nearly 50% drop on transition edge prediction. This confirms that audio is essential for temporal understanding, as it offers finer and more continuous granularity than the video stream (sampled at 1 fps), likely supporting more consistent long-context reasoning."
        },
        {
            "title": "5 Travel-planning Agent",
            "content": "After watching travel vlog, an animation, or movie, many fans go on pilgrimage: visiting the featured locations in the same order as they appear. An automatically generated tranvel plan derived from the video and its visiting order graph would greatly streamline this process. In this section, we construct MLLM-based agents which aim to provide travel plans given the videos. The purposes of this experiment are (1) to demonstrate the importance of the itinerary reconstruction for this application and (2) to explore the feasibility of generating travel plans from videos, capability not substantially addressed in prior work."
        },
        {
            "title": "5.1 Task Definition",
            "content": "Input The agent system takes list of POIs, video and planning constraints as input. While this list of POIs could be the output of the node prediction task, we use the POI list from the video annotation in this experiment to isolate the evaluation from model performance. The video provides richer information for the planning process. The constraints consist of the number of companions, travel duration and travel budget inspired by previous work Xie et al. (2024). Output The output is travel plan formatted in Markdown. It includes basic information such as the destination, duration, and budget. core component is detailed day-by-day itinerary, specifying schedule of activities, visiting times for each POI, and transportation methods. This itinerary is supplemented with POI details and other rich information extracted from the provided video and/or the search results. Furthermore, the plan provides practical recommendations, including specific restaurants and accommodations with relevant details like price and ratings. 5."
        },
        {
            "title": "Implementation of the Agent System",
            "content": "We implement the system as multi-agent framework coordinated by an autonomous orchestrator. This central component is responsible for dynamically determining the execution order of agents, managing the shared state of them. The framework comprises five specialized agents, each tasked with specific function: Plan Agent constructs the day-by-day schedule, optimizing time allocation based on the users budget and constraints. Google Maps Agent retrieves POI details. Route Agent finds the routes between POIs given the list of POIs. Accommodation Agent finds suitable lodging that fits the budget and is optimally located relative to the planned activities. Summary Agent integrates the outputs from all other agents to generate unified final report, including complete travel plan and budget breakdown. Each agent can use tools that are appropriate for its purpose, including Google Maps API-based tools and browser-based tools. Further implementation details are shown in Appendix C.1."
        },
        {
            "title": "5.3.1 System Setup",
            "content": "To verify the importance of the itinerary reconstruction step, the core task of our benchmark, we prepare 3 input settings: list of POIs only (POI), video only (Video) and both of them (P+V). Constraints are always provided as input in all settings. The backbone models of the orchestrator and all agents are fixed as Gemini-2.5-Pro. For the reproducibility, the temperature is set as zero. 8 (a) Attraction Task (b) Feasibility Task (c) Density Task (d) Alignment Task Figure 4: Crowdsourcing results of the agent system."
        },
        {
            "title": "5.3.2 Evaluation Setup",
            "content": "We sample 20 pairs of videos and their corresponding annotated graphs as input. Agents under the three settings then generate travel plans for each pair, resulting in 60 plans for evaluation. We qualitatively evaluate the generated plans with crowdsourcing3. Since the videos are filmed across Japan, we hire Japanese-speaking crowdworkers and translate the plans into Japanese. Each plan is evaluated by five workers. The evaluation consists of four tasks: assessing the plans attractiveness (Attraction), verifying the feasibility of the transportation information (Feasibility), judging the suitability of the number of POIs (Density), and determining the plans consistency with the video (Alignment). We also evaluate the systems POIs selection. Using GPT-4o OpenAI (2024b), we extract the POIs mentioned in the generated plans and treat them as selected. We then compare selected versus unselected POIs in terms of their on-screen duration in the video and their Google Maps ratings."
        },
        {
            "title": "5.4 Results and Discussion",
            "content": "Crowdsourcing Evaluation The crowdsourcing results are summarized in Figure 4 and Table 4. These results indicate that the P+V setting yielded the most attractive travel plans, achieving the highest average score of 3.73 in the Attraction task  (Table 4)  . This suggests that while list of POIs provides solid foundation, the rich information from the videosuch as the atmosphere of place or specific activities shownis crucial for creating more appealing plan. The Alignment task reveals the critical challenge of POI extraction from video (Figure 4d). The video-only setting produced the most polarized results: while it achieved the highest proportion of mostly aligned or aligned plans (41%), it also generated the largest share of plans deemed 3https://crowdsourcing.yahoo.co.jp"
        },
        {
            "title": "Mean Score",
            "content": "Transportation (%)"
        },
        {
            "title": "Input",
            "content": "Attract Density Relevance (1-5) (1-5) (1-4)"
        },
        {
            "title": "Has Info Feasible",
            "content": "3.58 3.46 3.73 POI Video P+V For Density: 1=too little, 3=just right, 5=too much. 2.96 3.07 3.13 2.34 2.22 2.08 93.0 80.0 89.0 88.2 87.5 87. Table 4: Crowdsourcing results by system configuration."
        },
        {
            "title": "Input",
            "content": "Duration (seconds) Google Rating (1-5) Selected Unselected Selected Unselected POI Video P+V 58.4 68.6 76.3 36.8 34.4 34. 21.6 34.2 41.7 4.29 4.26 4.25 57.2 Total : < 0.05, : < 0.01, : < 0.001 4.25 32. 24.6 4.17 4.19 4.19 4.17 0.12 0.07 0.06 0.08 Table 5: Comparison of statistics for selected vs. unselected POIs by system configuration. denotes the difference (selected minus unselected). In the Total row, selected refers to POIs chosen in at least one setting, and unselected refers to POIs never chosen in any setting. completely unrelated (31%). This instability suggests that the agents success is highly dependent on the initial, error-prone step of identifying POIs from raw video. failure in this stage, as evidenced by the low node prediction F1-scores  (Table 1)  , leads directly to final plan that is misaligned with the videos content. This underscores that robust itinerary reconstruction is foundational prerequisite for generating contextually relevant and reliable travel plans. In terms of practicality, all settings generated feasible plans. The POI-only setting was most reliable in providing transportation information, while the video-based settings were slightly better at creating plan with just right density of activities (Figure 4c). The video-only setting had the highest proportion of plans lacking transportation information (22%, Figure 4b) among the three settings. Analysis of POI Selection Table 5 provides deeper insights into the agents underlying POI selection strategy. For all input settings, the agent showed strong and statistically significant tendency to select POIs that were featured for longer duration in the video. This effect was most pronounced in the P+V setting ( = +41.7, < 0.001), suggesting that the combination of POI list and video context enables the agent to most effectively identify and prioritize key locations. Furthermore, the agent also showed preference for POIs with higher Google Maps ratings, although this effect was less pronounced than that of video duration. These findings indicate that the agent intelligently synthesizes signals from both the video (visual prominence) and external knowledge sources (user ratings) to make its planning decisions."
        },
        {
            "title": "6 Conclusion",
            "content": "We presented VIR-Bench, video understanding benchmark designed to evaluate long-range geospatialtemporal reasoning through itinerary reconstruction, utilizing visiting order graphs constructed from 200 travel videos. By decomposing the task into node and edge prediction, we revealed persistent weaknesses in state-of-the-art MLLMs: open-weight models consistently lag behind proprietary ones, and transition edges remain the primary bottleneck. Our prototype travel-planning agent further illustrated the practical value of VIR-Bench, demonstrating that combining POI lists with videos generates the most appealing travel plans, particularly emphasizing visually salient and highly rated POIs. In summary, VIR-Bench provides both rigorous benchmark and practical foundation for advancing MLLMs toward video-grounded geospatialtemporal understanding in real-world travel planning scenarios. Looking ahead, we intend to expand our dataset by increasing geographic and 10 linguistic diversity, incorporating broader range of filming styles, and enhancing annotation richness. Furthermore, we aim to explore advanced agent systems capable of referencing multiple videos simultaneously, enabling the generation of more comprehensive and engaging travel plans."
        },
        {
            "title": "References",
            "content": "Liu, H., Li, C., Wu, Q. and Lee, Y.J. Visual instruction tuning, 2023. URL https://arxiv.org/ abs/2304.08485. Li, F., Zhang, R., Zhang, H., Zhang, Y., Li, B., Li, W., Ma, Z. and Li, C. Llava-next-interleave: Tackling multi-image, video, and 3d in large multimodal models, 2024a. URL https://arxiv. org/abs/2407.07895. Lin, J., Yin, H., Ping, W., Molchanov, P., Shoeybi, M. and Han, S. Vila: On pre-training for visual language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 2668926699, June 2024. OpenAI. Gpt-4 technical report, 2024a. URL https://arxiv.org/abs/2303.08774. Grauman, K. et al. Ego4d: Around the world in 3,000 hours of egocentric video, 2022. URL https://arxiv.org/abs/2110.07058. Chandrasegaran, K., Gupta, A., Hadzic, L.M., Kota, T., He, J., Eyzaguirre, C., Durante, Z., Li, M., Wu, J. and Fei-Fei, L. Hourvideo: 1-hour video-language understanding. Advances in Neural Information Processing Systems, 37:5316853197, 2024. Jia, B., Chen, Y., Yu, H., Wang, Y., Niu, X., Liu, T., Li, Q. and Huang, S. Sceneverse: Scaling 3d vision-language learning for grounded scene understanding, 2024. URL https://arxiv.org/ abs/2401.09340. Yang, J., Yang, S., Gupta, A.W., Han, R., Fei-Fei, L. and Xie, S. Thinking in space: How multimodal large language models see, remember, and recall spaces. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1063210643, 2025. Lin, J., Zhu, C., Xu, R., Mao, X., Liu, X., Wang, T. and Pang, J. Ost-bench: Evaluating the capabilities of mllms in online spatio-temporal scene understanding. arXiv preprint arXiv:2507.07984, 2025. Yamamoto, A., Otomo, H., Ouchi, H., Higashiyama, S., Teranishi, H., Shindo, H. and Watanabe, T. Graph-structured trajectory extraction from travelogues. In Che, W., Nabende, J., Shutova, E. and Pilehvar, M.T., editors, Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1411614132, Vienna, Austria, July 2025. Association for Computational Linguistics. ISBN 979-8-89176-251-0. URL https:// aclanthology.org/2025.acl-long.690/. Kulkarni, P.P., Nayak, G.K. and Shah, M. Cityguessr: City-level video geo-localization on global scale, 2024. URL https://arxiv.org/abs/2411.06344. Zhao, B., Fang, J., Dai, Z., Wang, Z., Zha, J., Zhang, W., Gao, C., Wang, Y., Cui, J., Chen, X. and Li, Y. Urbanvideo-bench: Benchmarking vision-language models on embodied intelligence with video data in urban spaces, 2025. URL https://arxiv.org/abs/2503.06157. Drymonas, E. and Pfoser, D. Geospatial route extraction from texts. In Proceedings of the 1st ACM SIGSPATIAL International Workshop on Data Mining for Geoinformatics, DMG 10, page 2937, New York, NY, USA, 2010. Association for Computing Machinery. ISBN 9781450304306. doi: 10.1145/1869890.1869894. URL https://doi.org/10.1145/1869890.1869894. Kaushik, D., Gupta, S., Raju, C., Dias, R. and Ghosh, S. Making travel smarter: Extracting travel information from email itineraries using named entity recognition. In Mitkov, R. and Angelova, G., editors, Proceedings of the International Conference Recent Advances in Natural Language Processing, RANLP 2017, pages 354362, Varna, Bulgaria, September 2017. INCOMA Ltd. doi: 10.26615/978-954-452-049-6_047. URL https://aclanthology.org/R17-1047/. Haris, E. and Gan, K.H. Extraction and visualization of tourist attraction semantics from travel blogs. ISPRS International Journal of Geo-Information, 10(10), 2021. ISSN 2220-9964. doi: 10.3390/ijgi10100710. URL https://www.mdpi.com/2220-9964/10/10/710. Pang, Y., Hao, Q., Yuan, Y., Hu, T., Cai, R. and Zhang, L. Summarizing tourist destinations by mining user-generated travelogues and photos. Computer Vision and Image Understanding, 115 (3):352363, 2011. ISSN 1077-3142. doi: https://doi.org/10.1016/j.cviu.2010.10.010. URL https://www.sciencedirect.com/science/article/pii/S1077314210002419. Special issue on Feature-Oriented Image and Video Computing for Extracting Contexts and Semantics. Rosa, K.D. Structured entity extraction from travel videos using vision-language models. In Neidhardt, J., Kuflik, T., Livne, A. and Zanker, M., editors, Proceedings of the Workshop on Recommenders in Tourism co-located with the 18th ACM Conference on Recommender Systems (RecSys 2024), Bari, Italy, September 18, 2024, volume 3886 of CEUR Workshop Proceedings, pages 2330. CEUR-WS.org, 2024. URL https://ceur-ws.org/Vol-3886/paper3.pdf. Zhuang, S., Li, K., Chen, X., Wang, Y., Liu, Z., Qiao, Y. and Wang, Y. Vlogger: Make your dream vlog, 2024. URL https://arxiv.org/abs/2401.09414. Gavalas, D., Konstantopoulos, C., Mastakas, K. and Pantziou, G. survey on algorithmic approaches for solving tourist trip design problems. Journal of Heuristics, 20(3):291328, jun 2014. ISSN 1572-9397. doi: 10.1007/s10732-014-9242-5. URL https://doi.org/10.1007/ s10732-014-9242-5. Chen, D., Ong, C. and Xie, L. Learning points and routes to recommend trajectories. In CIKM 2016 - Proceedings of the 2016 ACM Conference on Information and Knowledge Management, International Conference on Information and Knowledge Management, Proceedings, pages 2227 2232, United States, October 2016. Association for Computing Machinery (ACM). doi: 10.1145/ 2983323.2983672. Publisher Copyright: 2016 ACM.; 25th ACM International Conference on Information and Knowledge Management, CIKM 2016 ; Conference date: 24-10-2016 Through 28-10-2016. He, J., Qi, J. and Ramamohanarao, K. joint context-aware embedding for trip recommendations. In 2019 IEEE 35th International Conference on Data Engineering (ICDE), pages 292303, 2019. doi: 10.1109/ICDE.2019.00034. Carrillo, J., Beltran, V., Sebastia, L. and Onaindia, E. Smarttur+eco: conversational recommender system for tourism, 07 2023. Chen, A., Ge, X., Fu, Z., Xiao, Y. and Chen, J. Travelagent: An ai assistant for personalized travel planning, 2024. URL https://arxiv.org/abs/2409.08069. Xie, J., Zhang, K., Chen, J., Zhu, T., Lou, R., Tian, Y., Xiao, Y. and Su, Y. Travelplanner: benchmark for real-world planning with language agents. In Forty-first International Conference on Machine Learning, 2024. Gui, R., Wang, Z., Wang, J., Ma, C., Zhen, H., Yuan, M., HAO, J., Lian, D., Chen, E. and Wu, F. Hypertree planning: Enhancing LLM reasoning via hierarchical thinking. In Forty-second International Conference on Machine Learning, 2025. URL https://openreview.net/forum? id=45he3Ri6JP. de la Rosa, T., Gopalakrishnan, S., Pozanco, A., Zeng, Z. and Borrajo, D. Trip-pal: Travel planning with guarantees by combining large language models and automated planners, 2024. URL https://arxiv.org/abs/2406.10196. Singh, H., Verma, N., Wang, Y., Bharadwaj, M., Fashandi, H., Ferreira, K. and Lee, C. Personal large language model agents: case study on tailored travel planning. In Dernoncourt, F., PreotiucPietro, D. and Shimorina, A., editors, Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing: Industry Track, pages 486514, Miami, Florida, US, November 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.emnlp-industry.37. URL https://aclanthology.org/2024.emnlp-industry.37/. 12 Otaki, K. and Baba, Y. Travel itinerary recommendation using interaction-based augmented data. Expert Systems with Applications, 269:126294, 2025. ISSN 0957-4174. doi: https://doi.org/ 10.1016/j.eswa.2024.126294. URL https://www.sciencedirect.com/science/article/ pii/S0957417424031610. Chaudhuri, S., Purkar, P., Raghav, R., Mallick, S., Gupta, M., Jana, A. and Ghosh, S. TripCraft: Benchmark for Spatio-Temporally Fine Grained Travel Planning, 2025. URL https://arxiv. org/abs/2502.20508. Zhang, B., Li, K., Cheng, Z., Hu, Z., Yuan, Y., Chen, G., Leng, S., Jiang, Y., Zhang, H., Li, X., Jin, P., Zhang, W., Wang, F., Bing, L. and Zhao, D. Videollama 3: Frontier multimodal foundation models for image and video understanding, 2025. URL https://arxiv.org/abs/2501.13106. Zhang, Y., Wu, J., Li, W., Li, B., Ma, Z., Liu, Z. and Li, C. Video instruction tuning with synthetic data, 2024. URL https://arxiv.org/abs/2410.02713. Zhu, J. et al. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models, 2025. URL https://arxiv.org/abs/2504.10479. Bai, S. et al. Qwen2.5-vl technical report, 2025. URL https://arxiv.org/abs/2502.13923. OpenAI. Introducing gpt-4.1 in the api. System card / technical report, OpenAI, April 2025a. URL https://openai.com/index/gpt-4-1/. Release of GPT-4.1 flagship, mini, and nano models with improved instruction-following, long-context capabilities, and lower latency and cost compared to GPT-4o and GPT-4.5. report, OpenAI, April Openai o3 and openai o4-mini OpenAI. nical 2221c875-02dc-4789-800b-e7758f3722c1/o3-and-o4-mini-system-card.pdf. Launch of reasoning models with full tool capabilities including web browsing, Python execution, image and file analysis, image generation, canvas, automations, file search, and memory. techURL https://cdn.openai.com/pdf/ System card / system card. 2025b. Comanici, G., Bieber, E., Schaekermann, M., Pasupat, I., Sachdeva, N., Dhillon, I., Blistein, M., Ram, O., Zhang, D., Rosen, E., Marris, L., Petulla, S., Gaffney, C., Aharoni, A., Lintz, N., Pais, T.C., Jacobsson, H., Szpektor, I., Jiang, N.J. et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities, 2025a. URL https://arxiv.org/abs/2507.06261. Li, B., Zhang, Y., Guo, D., Zhang, R., Li, F., Zhang, H., Zhang, K., Zhang, P., Li, Y., Liu, Z. and Li, C. Llava-onevision: Easy visual task transfer, 2024b. URL https://arxiv.org/abs/2408. 03326. OpenAI. Gpt-4o system card, 2024b. URL https://arxiv.org/abs/2410.21276. Zhang, J., Gould, S. and Ben-Shabat, I. VidatANU CVML video annotation tool. https: //github.com/anucvml/vidat, 2020. Comanici, G. et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities, 2025b."
        },
        {
            "title": "A Benchmark Details",
            "content": "A.1 Annotation Guidelines This section presents the complete annotation guidelines used in constructing VIR-Bench. A.1."
        },
        {
            "title": "Introduction",
            "content": "This project constructs video understanding benchmark to evaluate multimodal LLMs temporal and spatial grounding capabilities. Concretely, the task is to input travel vlog into the MLLM and have it infer the subjects movement trajectory (e.g., Haneda Airport Shinjuku Station McDonalds Seibu-Shinjuku Ekimae). To enable trajectory analysis and evaluation, this annotation collects the set of POIs visited in each video. The overall workflow consists of two stages: (1) selecting videos, and (2) annotating trajectories. A.1.2 Video Selection Please search YouTube for videos that satisfy all of the following required criteria: 1. The video is travel vlog set within Japan. Videos that include leaving Japan (e.g., trip to Korea) are not allowed. 2. The spoken language in the video is the language specified by the authors (Japanese or English). 3. Apart from the intro/outro, the video progresses in chronological order. Narratives that jump back to earlier scenes are not allowed. 4. The subjects do not split into multiple groups acting in parallel (no scenes where different places are visited simultaneously). 5. It is tourism-oriented, visiting multiple POIs, rather than simple neighborhood stroll. 6. The video does not continuously overlay POI names as on-screen captions (short captions for few seconds are acceptable). 7. The preferred duration is 1020 minutes, and at most 30 minutes. In addition to the required criteria, please aim to satisfy the following desirable criteria where possible: 1. Geographic diversity across the set of videos assigned to single annotator (e.g., avoid selecting ten videos all in Tokyo). 2. Prefer videos that span multiple prefectures. Example videos (for reference): https://www.youtube.com/watch?v=f-t3IFu-U7U https://www.youtube.com/watch?v=VjWr1NYVGPc https://www.youtube.com/watch?v=CuEOnd5cAA8 https://www.youtube.com/watch?v=xqbiBTgcaiA A.1.3 Trajectory Annotation Definition of POI In this project, POI is defined as place registered on Google Maps. Exclude locations with extremely few reviews (0 or near 0), and places unrelated to tourism (e.g., station-front public restroom). Annotation Tool We use Vidat Zhang et al. (2020), browser-based video annotation tool. Before starting, we strongly recommend watching the official 7-minute tutorial video. Procedure Annotate the video trajectory as follows: 1. Access Vidat in your browser: https://vidat2.davidz.cn/#/ 2. Load the downloaded video. 3. For each POI visit, use the timeline to select the start and end time and create new segment. 14 4. Copy the corresponding Google Maps URL for that POI into the segments description. Example: https://maps.app.goo.gl/DT4ENgf8dUopWNUK6. 5. Move on to the next POI and repeat steps 34. 6. When finished, save the annotation as <videoID>.json. A.1.4 FAQ 1. POIs with the Same Name Q: There are multiple entries named Sakuragicho Station. Which one? A: Choose the entry with the larger number of reviews. 2. Hierarchical Structure of POIs Q: What if POI contains another POI? (e.g., Ghibli Park is within Expo 2005 Aichi Commemorative Park.) A: Annotate all levels of the hierarchy. The parent POIs start/end times must enclose the child POIs times. If the parent times are (x1, y1) and the child times are (x2, y2), ensure x1 < x2 < y2 < y1 ."
        },
        {
            "title": "2.2 Maximum granularity of a POI.\nQ: Does Okinawa count as a POI?\nA: Okinawa, Kokusai-dori, and Minatomirai are areas on Google Maps (no review counts) and should not\nbe annotated. However, Kokusai-dori Shopping Street and Yokohama Chinatown are POIs (with reviews)\nand should be annotated.",
            "content": "3. Concurrent Visits Q: What if multiple POIs are visited simultaneously? A: Create multiple segments with the same start/end times. 4. Unknown Venue Names Q: They visit cafe, but cannot identify which one. A: Leave the URL blank and write UNKNOWN + the POI category. If you are unsure of the category, please consult. This is last resort; keep the number of UNKNOWN entries as low as possible. POI category list: Google Maps Platform Place Type (Table A). Examples: UNKNOWN cafe, UNKNOWN ramen_restaurant, UNKNOWN hotel. 5. Chain Stores with Unknown Branch Q: Its Starbucks, but cant tell which branch. A: Pick one plausible branch and paste its URL, then append , branch unknown. Example: https://maps.app.goo.gl/uSybfW2HRjJwrfrg8, branch unknown 6. POIs Missing from Google Maps Q: rural shop may not exist as POI on Google Maps. A: Leave the URL blank and write <shop name> + <category>, analogous to the UNKNOWN handling. 7. Closed or Relocated Venues Q: The venue visited in the video has since closed or moved. A: If it is closed but still exists as POI entry on Google Maps, annotate as usual. For relocations, annotate according to the current location as listed on Google Maps. 8. Intros/Outros Q: The intro (or outro) shows all POIs. Annotate them? A: Annotate only the main content of the video. 15 9. Start/End Time Definitions Q: How do we define the start and end times? A: The start time is when the subject physically arrives at the POI; the end time is when they leave. Q: How precise must we be at hard cuts between scenes? A: Do not annotate with frame-level strictness; reasonable precision is sufficient. 10. Definition of Visit Q: Do they need to enter the facility to count as visit? A: If they reach the front of the facility and it is visibly captured in the frame, count it as visit. Distant landmarks merely visible on the horizon do not count. A.2 Statistics VIR-Bench comprises 200 travel videos filmed across Japan, each paired with corresponding visiting order graph that captures the itinerary depicted in the video. Of these, 100 videos are narrated in English, while the rest are in Japanese. Detailed statistics are shown in Table 6. Figure 5 presents key dataset statistics, while Figure 6 visualizes the geographical distribution of POIs across Japan. Language Japanese English All # POIs / video Min Mean SD Max Sum Min Video Duration (s) Max Mean SD 7 7 7 15.4 6.4 21.4 13.1 18.4 10.7 36 75 1,544 2,145 3,689 624.4 433.4 433.4 1061.2 293.6 1059.5 299.3 1060.3 295.7 1900.9 1707.6 1900.9 Sum 106,115 105,945 212, POI Duration (s) Min Mean SD Max 1.0 0.4 0.4 62.1 78.1 42.4 55.2 50.7 66.5 857.3 658.2 857.3 Sum 95,849 90,702 186, # Unique Prefectures Min Mean SD Max 1 1 1 1.4 0.7 1.7 0.9 1.5 0.8 4 5 5 Table 6: Statistics of VIR-Bench. (a) Distribution of unique prefectures per video (b) Distribution of POI count per video (c) Distribution of POI duration Figure 5: Statistical distributions of the dataset. (a) shows the distribution of unique prefectures covered per video, with most videos spanning 1-2 prefectures. (b) presents the distribution of POI count per video, ranging from 7 to 75 POIs with mean of 18.4. (c) displays the distribution of POI durations in minutes, showing right-skewed distribution with most POIs lasting under 2 minutes. A.3 License and Access We release the VIR-Bench dataset strictly for research purposes, in compliance with Article 30-4 (Use for Non-Enjoyment Purposes) and Article 47-5 (Minor Use in Information Analysis Services) of the Japanese Copyright Act. Commercial use of any kind is strictly prohibited. The dataset may not be redistributed on servers outside Japan or under alternative licenses. Figure 6: Geographical distribution of POIs across Japan, showing an even spread from Hokkaido in the northeast to Okinawa in the southwest. Blue points indicate POIs from English-speaking videos, while red points represent POIs from Japanese-speaking videos."
        },
        {
            "title": "B Experiment Details",
            "content": "B.1 Model Cards We list the models and their corresponding parameters used in this paper in Table 7. Model VideoLLaMA3-7B LLaVA-Video-7B LLaVA-Video-72B InternVL3-8B InternVL3-38B InternVL3-78B Qwen2.5-VL-7B Qwen2.5-VL-32B Qwen2.5-VL-72B GPT-4.1 o4-mini Gemini-2.5-Flash Gemini-2.5-Pro Max Frames 180 64 64 64 64 64 256 256 256 256 256 1 fps 1 fps"
        },
        {
            "title": "Thinking Budget Audio Input Reference",
            "content": "- - - - - - - - - - 32768 - 32768 - - - - - - - - - - - 1 kbps 1 kbps Zhang et al. (2025) Zhang et al. (2024) same as above Zhu et al. (2025) same as above same as above Bai et al. (2025) same as above same as above OpenAI (2025a) OpenAI (2025b) Comanici et al. (2025b) same as above Table 7: Models used in this paper. Identifier / Repository DAMO-NLP-SG/VideoLLaMA3-7B lmms-lab/LLaVA-Video-7B-Qwen2 lmms-lab/LLaVA-Video-72B-Qwen2 OpenGVLab/InternVL3-8B OpenGVLab/InternVL3-38B OpenGVLab/InternVL3-78B Qwen/Qwen2.5-VL-7B-Instruct Qwen/Qwen2.5-VL-32B-Instruct Qwen/Qwen2.5-VL-72B-Instruct gpt-4.1-2025-04-14 o4-mini-2025-04-16 gemini-2.5-flash gemini-2.5-pro B.2 Error Analysis We categorize model errors into three types: (1) Prompt Analysis Error, where the model misinterprets the task or fails to follow instructions; (2) Geographic Knowledge Error, reflecting limited geographic knowledge (i.e., the ability to play GeoGuessr) encoded in the model; (3) Temporal Reasoning Error, stemming from flawed geospatial-temporal reasoning based on the video input. Prompt analysis errors can arise in both node and edge prediction tasks, while geographic knowledge errors are primarily observed in node prediction, and temporal reasoning errors occur exclusively in edge prediction. B.2.1 Prompt Analysis Error Node Prediction Figure 7 is an output example from LLaVA-Video-7B for the video at https: //www.youtube.com/watch?v=2guZrrVMGfI. The model simply copied the example output provided in the input prompt  (Table 9)  , without attempting to predict the actual visited locations in the video. We believe this behavior is due to the relatively long prompt, which may have hindered the models ability to follow instructions effectively. Figure 7: Example of prompt analysis error made by the model during the node prediction task. 18 Figure 8 shows another example from LLaVA-Video-7B using the video at https://www.youtube. com/watch?v=nzUKUdEHWAc. The video is vlog filmed in Nikko, Tochigi, but the model listed numerous prefectures, cities, and POIs, resembling random guessing. Figure 8: Another example of prompt analysis error made by the model during the node prediction task. Edge Prediction Figure 9 presents an example from InternVL3-8B using the video at https: //www.youtube.com/watch?v=1YWj6ll-tJI. The model appeared to misinterpret the concept of sub-POI, failing to capture the inclusion edge between Owakudani and Owakudani Stationa relation that should be identifiable even without geographic knowledge. Additionally, its grasp of transition edges was insufficient: it generated five outgoing transition edges from Owakudani Station, completely disregarding the videos chronological order. Figure 9: Example of prompt analysis error made by the model during the edge prediction task. 19 B.2.2 Geographic Knowledge Error Figure 10 shows an example from Qwen2.5-VL-7B using the video at https://www.youtube. com/watch?v=x37h_iPtWiI. Although the model correctly identified the prefecture as Okinawa, it failed to predict the correct cities, returning only Naha, the most well-known city in Okinawa but not visited in the video. Its POI predictions were also poor, with fewer than half of the locations correctly identified. These results suggest that the model has limited understanding of geographic knowledge. Figure 10: Example of geographic knowledge error. Figure 11 presents another example from Gemini-2.5-Flash using the video at https://www. youtube.com/watch?v=RCpg1DHX-7s. While the predicted POIs are largely accurate, some cities featured in the videosuch as Chiyoda City and Shinjuku Citywere omitted. Interestingly, the model correctly identified Nabazo Shinjuku Sanchome Store but failed to associate it with Shinjuku City, suggesting that even strong proprietary models still exhibit limitations in geographic reasoning. B.2.3 Temporal Reasoning Error Figure 12 presents an example from InternVL3-78B using the video at https://www.youtube. com/watch?v=kBErL9FMce4. The video features multiple visited locations (e.g., HOTEL ORIGO HAKATA and LAMP LIGHT BOOKS HOTEL), but most of these visits are ignored by the model. If predicted correctly, the model-predicted graph in Figure 12 would contain many circles, as we visualize each visited location as single node. This suggests that the model failed to form holistic view of the itinerary, instead constructing the graph in piecemeal manneradding edges one by one without considering the overall structure. Figure 13 presents another example from o4-mini using the video at https://www.youtube.com/ watch?v=0Vjufc6rhH8. When the video contains large number of POIs, the overall flow of the model-predicted graph becomes difficult to follow, indicating that even strong reasoning models like o4-mini still struggle with limited long-context and temporal reasoning capabilities. While we attempted to obtain the thinking summary provided by OpenAI, the information was too sparse to yield meaningful insights (thinking summary for the generation process of Figure 13 is shown in Table 8). We contend that the development of open-weight reasoning MLLMs is urgently needed, and VIR-Bench offers challenging benchmark for evaluating their capabilities. Figure 11: Another example of geographic knowledge error. **Planning visit to Numazu** want to explore Nakamise Shopping Street in Numazu. Ill stroll through the street and check out Marusan bookstore. Then, Ill look for the Yasushi Inoues Mother and Child statue nearby. Next, Ill consider which landmark to visit maybe Onari Bridge? After that, Ill visit Agetsuchi Asahi Inari Shrine, enjoying the river view. Finally, Ill head back to the station, catch the bus to Izu Mito Sea Paradise, and pass the rental car location! Table 8: Thinking summary of o4-mini during the generation process for Figure 13. 21 Figure 12: Example of temporal reasoning error. Due to the output format, multiple visited locations in the model-predicted graph appear as single node, unlike the multiple nodes in the ground-truth graph. Figure 13: Another example of temporal reasoning error. 22 B.3 Prompt Templates The prompts used for node and edge prediction are shown in Table 9 and Table 10, respectively. --------------------------------------------------------------------------------------------------------------------------------------------------------------- ### **Overview** **Role**: You are an expert geo-location AI with specialization in Japanese geography and travel analysis. **Objective**: Analyze the provided video of trip in Japan and identify all visited locations. **Input**: video file depicting travel within Japan. ### **Location Definitions** 1. **Prefecture ()**: The highest-level administrative division of Japan, such as Tokyo, Osaka, Hiroshima,Hokkaido, etc. 2. **City (23)**: The municipality within prefecture. This include Tokyos 23 Specific Wards like Shibuya and Minato, cities like Yokohama and Sapporo, towns like Karuizawa and Hakone, and villages like Hakuba and Shirakawa. 3. **Point of Interest (POI)**: specific, named location. This includes landmarks, tourist attractions, stations, restaurants, cafes, stores, parks, museums, etc. ### **Instructions & Rules** 1. **No Duplicates**: Each unique prefecture, city, and POI should appear only once in its respective list. 2. **Maximum Specificity**: Always use the most specific and complete name identifiable. For businesses, include the branch name if visible (e.g., \"Starbucks Shibuya Tsutaya\" not just \"Starbucks\"). 3. **Verification over Guesswork**: Only include locations that can be confidently identified from clear visual or auditory cues in the video (e.g., signs, logos, building architecture, announcements). Do not guess or infer locations that are not explicitly evidenced. 4. **Minimum POI Unit**: Do not list integrated facilities within POI (e.g., museums gift shop). However, you must list separate businesses located inside larger commercial complex, such as specific caf within shopping mall or train station. 5. **Maximum POI Unit**: Do not include general geographic areas, districts, or neighborhoods which are not single, managed entities (e.g., \"Minato Mirai,\" \"Kokusai Dori\"). 6. **POI Formatting**: Strictly adhere to the \"Name (category)\" format for all entries in the points_of_interest list. You must select the single most appropriate category for each POI from the curated list below. **POI Category List**: [adventure_sports_center, airport, american_restaurant, amusement_center, amusement_park, aquarium, arena, art_gallery, ... ### **Output Format** Generate single JSON object conforming to the schema below. Ensure the lists are populated according to the rules above. **JSON Schema**: json { \"prefectures\": [\"list of prefectures\"], \"cities\": [\"list of cities\"], \"points_of_interest\": [\"list of points of interest\"] } **Example Output**: For video showing person arriving at Kansai International Airport, taking train to Osaka, seeing the Glico Running Man sign, and then eating at Dotonbori restaurant, the output should be: json { \"prefectures\": [ \"Osaka\" ], \"cities\": [ \"Izumisano\", \"Osaka\" ], \"points_of_interest\": [ \"Kansai International Airport (airport)\", \"Namba Station (train_station)\", \"Glico Running Man Sign (tourist_attraction)\", \"Kukuru Dotonbori (japanese_restaurant)\" ] } --------------------------------------------------------------------------------------------------------------------------------------------------------------- Table 9: Prompt for node prediction. 23 ------------------------------------------------------------------------------------------------------------------------------------------------------------------------- ### **Overview** **Role**: You are an expert geo-location AI with specialization in Japanese geography and travel analysis. **Objective**: Analyze the provided video of trip in Japan. Based on the sequence of visited locations, generate directed graph representing the visiting trajectory. The graph should distinguish between hierarchical containment (e.g., landmark within city) and sequential travel (e.g., moving from one landmark to another). **Input**: video file depicting travel within Japan and all visited locations. ### **Input** The input for visited locations is JSON object, as shown below. Crucially, the lists are **not** in chronological order. json {{ input_json }} ### **Edge Definitions** 1. **Inclusion Edge**: directed edge representing containment of one location within another. This edge flows from the larger geographical area to the smaller one. - Example: - From prefecture to city: Osaka (prefecture) Osaka (city) - From city to point of interest (POI): Shibuya Shibuya Station - From POI to sub-POI: AEON MALL Zama AEON Cinema Zama 2. **Transition Edge**: directed edge representing movement between two distinct locations at the same hierarchical level. This edge indicates the chronological flow of travel. - Example: - Between prefectures: Osaka Hiroshima - Between cities (within the same prefecture): Shibuya Shinjuku - Between POIs (within the same city): Tokyo Tower Odaiba Marine Park ### **Instructions & Rules** 1. **Inclusion Edges**: - For every city in the input, create an inclusion edge from its corresponding prefecture node. - For every POI in the input, create an inclusion edge from its corresponding city node. - For every sub-POI (if applicable) within POI, create an inclusion edge from the POI node to the sub-POI node. 2. **Transition Edges**: - Create transition edges between consecutively visited locations at the same hierarchical level. - transition from POI in City to POI in City breaks the POI-level transition chain. The graph must instead show transition edge from City to City B. - Similarly, transition from city in Prefecture to city in Prefecture is represented by transition edge from Prefecture to Prefecture B. 3. **Location Useage**: - Please make sure to use only the exact location names provided in the inputdo not alter, add to, or infer any names beyond what is given. 4. **Unkonwn POIs**: - For POIs which can not be identified with certainty, we provide placeholders in the format \"Unknown (category)\" (e.g., Unknown (restaurant)). Use these placeholders when creating edges. ### **Output Format** Generate single JSON object conforming to the schema below. This object will represent the final directed graph with all nodes and edges correctly populated according to the rules. **JSON Schema**: json { \"edges\": [ { \"source\": \"string\", \"target\": \"string\", \"type\": \"inclusion transition\" } ] } **Example Output**: For video showing person arriving at Kansai International Airport, taking train to Osaka, seeing the Glico Running Man sign, and then eating at Dotonbori restaurant, the output should be: json { \"edges\": [ { \"source\": \"Osaka\", \"target\": \"Izumisano\", \"type\": \"inclusion\" }, ... { \"source\": \"Glico Running Man Sign\", \"target\": \"Kukuru Dotonbori\", \"type\": \"transition\" } ] } ------------------------------------------------------------------------------------------------------------------------------------------------------------------------- Table 10: Prompt for edge prediction."
        },
        {
            "title": "C Travel Agent Details",
            "content": "C."
        },
        {
            "title": "Implementation Details",
            "content": "We implemented the travel plan agent system using LangChain4. The system consists of one orchestrator, five specific agents and some tools. The sequence diagram of this system is shown in Figure 14. As tools, following external services are used: Google Places API: For detailed POI information and accommodation information. Google Routes API: For route search (TRANSIT mode is unavailable in Japan). BrowserUse5: For fallback of the route search by automated GUI-based browser operation. We randomly set the input constraints as below: Headcount: 1 to 4 Duration: 2 days to 4 days. Budget per day per person: 70 USD to 500 USD. C.2 Evaluation Details We have evaluated generated plans using crowdsourcing on Yahoo! Crowdsourcing6. The instructions to crowdworkers and the screen where the workers execute the tasks are shown in Table 12, 13, 14, 15 and Figure 15, respectively. The shown instructions are translated into English while they are originally in Japanese. To ensure the quality of the evaluation, we included check questions with obvious correct answers and only accepted responses from users who answered them correctly. One check question was assigned to each worker. In the alignment task, we asked the workers to watch videos and then answer the questions, as shown in Figure 4d. The detailed crowdsourcing results corresponding to Figure 4 in the main paper are shown in Table 11. Attraction Density Feasibility Alignment Setting Very Attr. Attr. Neutral Not Attr. Not Attr. At All Too Many Slightly Many Just Right Slightly Few Too Few Navigable Not Navigable No Info Matches Mostly Matches Partially Matches No Match POI Video POI + Video 18.0 17.0 20.0 41.0 35.0 47.0 28.0 32.0 22.0 7.0 9.0 8. 6.0 7.0 3.0 2.0 3.0 4.0 24.0 27.0 26.0 48.0 45.0 53.0 20.0 24.0 13.0 6.0 1.0 4. 82.0 70.0 78.0 11.0 10.0 11.0 7.0 20.0 11.0 10.0 12.0 3.0 25.0 29.0 24.0 54.0 28.0 51. 11.0 31.0 22.0 Table 11: Aggregated analysis of user evaluation ratings across different settings (all values in %) C.3 Examples of the Generated Plans Actual plans generated for each agent setting are shown in Sections C.3.1, C.3.2, and C.3.3. The input video used for these examples is: https://www.youtube.com/watch?v=P60ee5-wQic. For visibility, the plans were converted from Markdown to LaTeX using rule-based script. The generated plans highlight the appeal of POIs and provide practical information such as prices and transportation options. The input video is filmed in Kyoto and Nara, however, Section C.3.2 contains plan in Tokyo. As mentioned in the main paper, the video-only setting sometimes generates completely unrelated itineraries. 4https://www.langchain.com 5https://github.com/browser-use 6https://crowdsourcing.yahoo.co.jp 25 Figure 14: The sequence diagram of the agent system. 26 --------------------------------------------------------------------------------------------------------------------------------------------------------------- Travel Plan Appeal Evaluation Task"
        },
        {
            "title": "Task Overview",
            "content": "Read the travel plans generated by AI and evaluate them on 5-point scale based on whether they are appealing enough to make you want to visit."
        },
        {
            "title": "Evaluation Criteria",
            "content": "Evaluate the overall appeal of the plan, taking into consideration the appeal of the destinations and the specificity of the descriptions. Rating 5: Very appealing Criteria: The ideal plan. The combination of destinations is excellent, and the plan is full of specific and appealing information, such asLets try the local specialty . It is original, and you feel strongly that you want to travel according to this plan. Rating 4: Attractive Criteria: well-made, good plan. The destinations are attractive, and the itinerary flows smoothly. The information is specific and sufficient, and many people will be satisfied with the content. Rating 3: Average Criteria: Gives the impression of list of typical tourist destinations. Not bad, but nothing special. The information is mostly abstract, such as visit XX, and the content is commonplace. Rating 2: Somewhat unattractive Criteria: Not very appealing plan. There are questions about the choice of places to visit, and the information is not specific enough. It is difficult to imagine how enjoyable the trip will be. Rating 1: Not attractive at all Criteria: It is difficult to understand why this plan was chosen. The combination of places to visit is incongruous, and there is very little information, making it completely unattractive. Points to note when evaluating Attractiveness of destinations: Are the selected locations attractive as travel themes or destinations? Are there any suggestions other than the usual tourist spots, such as hidden gems? Specificity of information: The appeal can vary greatly depending on whether it simply says lunch or fresh seafood bowl lunch at XX market. Plan structure: Is the theme of the trip (e.g., gourmet, scenic views, relaxation) consistent, and does it flow smoothly? Excitement: Does reading the plan overall make you feel like This sounds fun! or want to go!? Response Method Read the entire travel plan and select the option that best applies from the choices below. If you are unsure, consider whether you would choose this plan if you were spending your own money and time. --------------------------------------------------------------------------------------------------------------------------------------------------------------- Table 12: The crowdsourcing instruction for the attraction task. 27 --------------------------------------------------------------------------------------------------------------------------------------------------------------- Evaluate the appropriateness of transportation methods in the itinerary Task overview Read the travel plan generated by AI and evaluate whether the means of transportation used to travel between locations are realistic and appropriate."
        },
        {
            "title": "Evaluation criteria",
            "content": "Check whether the travel times and means of transportation listed in the plan match the actual geography and traffic conditions, and select the most appropriate option from the following three choices. 1. No transportation means specified Criteria: No information is provided on how to travel (e.g., walking, train, bus, car). Specific example: [10:00] Tokyo Station [11:00] Senso-ji Templethe method of transportation to the next destination is not specified. 2. Travel is not possible Criteria: The transportation method or travel time provided is unrealistic, making travel impossible or extremely difficult. Specific example: Travel time is too short: 30 minutes from Tokyo Station to Osaka Station by Shinkansen Inappropriate transportation method: Walk from Okinawa Main Island to Miyakojima Physically impossible: 5 minutes on foot from Shibuya to Skytree 3. Actual travel is possible Criteria: The transportation method and travel time provided are realistic and travel is considered possible without undue difficulty. Specific examples: 9:00Shinjuku Station (approx. 7 minutes by JR Yamanote Line) 9:10 approx.Harajuku Station [1:00 p.m.] Kinkaku-ji Temple (approx. 40 minutes by city bus) [approx. 1:40 p.m.] Kiyomizu-dera Temple Evaluation points Imagine the map: Get rough idea of the distance between locations. Reasonable travel time: Consider waiting times for trains and buses, as well as transfer times, and determine whether the travel time provided is feasible. Regional characteristics: The appropriateness of transportation methods may vary depending on whether the area is an urban area (with well-developed transportation networks) or rural area (where car is essential, etc.). Answering method Read the transportation sections of each travel plan and select the most appropriate option from 1 to 3. If you are unsure, consider whether you would choose this transportation method or think it is feasible within the given time frame. --------------------------------------------------------------------------------------------------------------------------------------------------------------- Table 13: The crowdsourcing instruction for the feasibility task. 28 --------------------------------------------------------------------------------------------------------------------------------------------------------------- Travel Plan Event Density Evaluation Task"
        },
        {
            "title": "Task Overview",
            "content": "Read the travel plan generated by AI and evaluate whether the number of activities per day (event density) is appropriate on 5-point scale."
        },
        {
            "title": "Evaluation Criteria",
            "content": "Event density: The degree to which sightseeing, meals, transportation, and activities are packed into day. Evaluation 1: Too few Criteria: Only 1-2 locations visited per day, too much free time, not enough activities relative to travel time Rating 2: Slightly insufficient Criteria: Could use few more activities, either the morning or afternoon is too empty Rating 3: Just right Criteria: Reasonable pace for sightseeing, adequate rest time, sufficient time at each location Rating 4: Slightly excessive Criteria: May feel rushed, limited rest time, suitable for those with good physical stamina Rating 5: Too much Criteria: Clearly overloaded, insufficient time at each location, may be tiring due to travel alone Notes when evaluating Consider the number of travelers and the number of days (specified at the beginning of the plan) Include travel time as part of the activities Ensure that meal times are appropriately allocated Evaluate based on whether it is realistically feasible. Evaluation examples Example of Rating 1: two-day trip with hotel check-in only in the afternoon on the first day, and one sightseeing spot in the morning on the second day. Example of Rating 3: Departure at 9 AM, visit 2-3 sightseeing spots at reasonable pace, with time for lunch and dinner, and return to the hotel around 8 PM. Example of Evaluation 5: Depart at 7 AM, visit 10 or more tourist spots, stay at each location for about 15 minutes, eat meals while traveling, and schedule until 11 PM. Answering Method Read each travel plan and select number from 1 to 5. If you are unsure, consider whether you would actually want to travel according to the plan. --------------------------------------------------------------------------------------------------------------------------------------------------------------- Table 14: The crowdsourcing instruction for the density task. --------------------------------------------------------------------------------------------------------------------------------------------------------------- Watch the vlog and determine whether the itinerary has been accurately reproduced."
        },
        {
            "title": "Task Overview",
            "content": "Watch the travel vlog (video blog) and evaluate on 4-point scale whether the itinerary generated by AI accurately reproduces the content of the video. You may watch the video at double speed."
        },
        {
            "title": "Evaluation Criteria",
            "content": "Check how accurately the places visited, meals, activities, etc. introduced in the video are reflected in the itinerary, and select the most appropriate option from the following four choices. 1. The itinerary is completely unrelated to the video Criteria: The places and activities listed in the itinerary are completely different from the content of the video. Specific example: The video is about trip to Kyoto, but the itinerary is for trip to Okinawa. None of the tourist spots featured in the video are included in the itinerary. 2. The itinerary is partially related to the video but mostly unrelated Criteria: The itinerary includes one or two locations featured in the video, but overall it is completely different itinerary. Specific example: Only Kiyomizu Temple, which appeared in the Kyoto trip video, is included in the itinerary, but the other destinations and order are completely different. 3. The itinerary is basically the same as the video, but includes some parts that are not related to the video Criteria: Most of the itinerary follows the content of the video, but there are additional places that are not shown in the video, or parts of the video are missing. Specific example: The order of visits in the video (ABC) is reproduced, but place called that is not in the video has been added to the itinerary. The itinerary covers all the major tourist spots visited in the video, but caf that was visited is missing. 4. Itinerary that follows the video Criteria: The itinerary faithfully reproduces the content of the video, including the places visited, the order of visits, and the meals. Specific example: The sequence introduced in the video (Breakfast at Caf Sightseeing at Temple Street food at Shopping Street) is listed exactly as it appears in the itinerary. Points to note when evaluating Order of visits: It is important that the places visited are correct. Minor differences in order are not major issue. Check that the specific names of shops and dishes mentioned in the video are reflected in the itinerary. There is no need to recreate every action in the video (e.g., stopping at convenience store, getting slightly lost). As long as the main events are covered, it is acceptable. Answering Method Compare the vlog and itinerary, then select the option that best matches from 1 to 4. If you are unsure, use the following criterion: Can you relive the trip from the video by following this itinerary? --------------------------------------------------------------------------------------------------------------------------------------------------------------- Table 15: The crowdsourcing instruction for alignment task. 30 (a) Attraction Task (b) Feasibility Task (c) Density Task (d) Alignment Task Figure 15: Crowdsourcing screen examples. [Actual Generated Plan] is substituted with the actual generated plans. 31 C.3.1 Generated Plan by POI-only Setting"
        },
        {
            "title": "Travel Guide",
            "content": "Destination: N/A Travel Dates: 2 days Group Size: 2 people Budget: $1964.00 USD Your Unforgettable 2-Day Kyoto & Nara Adventure Welcome to whirlwind journey through ancient traditions and serene nature! This itinerary is crafted to give you and your travel partner deep, immersive experience, balancing iconic sights with authentic local flavors. Weve based your adventure in Kyoto, the cultural soul of Japan, with magical day trip to Nara to meet its famous, friendly deer. Get ready to create some incredible memories together. This plan focuses on Kyoto and Nara because, with just two days, it allows for rich and relaxed experience without the rush of long-distance travel. The incredible sights of Tokyo, like Shibuya Crossing and Senso-ji Temple, are amazing but would require separate trip to be fully appreciated. Weve prioritized depth over distance to make your short trip truly special. Day 1: Kyotos Spiritual Peaks & Culinary Heart Theme: day of contrasts, from spiritual mountain hike through thousands of vermilion gates to sensory explosion in Kyotos most famous food market. Morning (8:00 AM 12:00 PM): Fushimi Inari Taisha Why its must-see: With an importance score of 91.8 and over 79,000 reviews, Fushimi Inari is an unmissable Kyoto icon, famous for the Senbon Torii, mesmerizing network of thousands of vibrant red gates. What to Expect: Dedicated to Inari, the Shinto god of rice and business. The path is hike, not just stroll. The lower sections are crowded, but the upper trails are peaceful and atmospheric. Recommended Duration: 2.53 hours. Optimal Timing: Arrive by 8:00 AM for fewer crowds and stunning photos. Special Considerations: Comfortable walking shoes are essential. Consider turning around at the Yotsutsuji intersection if pressed for time. Lunch (12:30 PM 1:30 PM): Authentic Udon Near the Shrine Restaurant: Kendonya Address: 25-13 Fukakusa Ichinotsubocho, Fushimi Ward, Kyoto Cuisine: Udon Noodles Price Range: Budget-friendly ($) Why it Complements the Itinerary: Close to the shrine, this cozy spot serves handmade udon and is loved by locals. Reservation: Not required, but expect short wait during peak times. Afternoon (2:00 PM 5:00 PM): Nishiki Market & Teramachi Arcade Transportation: Take the Keihan Main Line to Gion-Shijo Station (10 mins, $2 USD), then walk 5 minutes to the market. Why its must-see: Known as \"Kyotos Kitchen\", Nishiki Market is five-block arcade packed with food stalls, traditional goods, and vibrant atmosphere. What to Expect: Sample everything from candied octopus to fresh sashimi. Explore nearby Teramachi for modern boutiques and souvenirs. Recommended Duration: 23 hours. Dinner (6:30 PM): Taste of Modern Kyoto Restaurant: Cross Hotel Kyoto Restaurant & Bar Cuisine: Japanese-Italian Fusion Price Range: Moderate-High ($$$) Why it Complements the Itinerary: Conveniently located in your hotel, it offers stylish yet relaxing end to the day. Reservation: Recommended; ask the hotel concierge. Day 2: Day with the Deer in Ancient Nara Theme: delightful and easy day trip to Japans first permanent capital, where sacred deer roam among ancient temples and tranquil paths. Morning (9:00 AM 1:00 PM): Journey to Nara & Todai-ji Temple Transportation: JR Nara Line Rapid train from Kyoto Station to JR Nara Station (45 mins, $6 USD one-way). Activity: Nara Park: Meet the friendly bowing deer, roam the park, and enjoy nature. Explore Todai-ji: Visit the Great Buddha inside the worlds largest wooden structure. Cost: Nara Park is free; Todai-ji Temple: 600 ($5 USD). Lunch (1:00 PM 2:00 PM): Award-Winning Burgers Restaurant: SAKURA BURGER Address: 38-1 Higashimuki Kitamachi, Nara Cuisine: American Gourmet Burgers Price Range: Moderate ($$) Why it Complements the Itinerary: Highly-rated and nice change from traditional Japanese meals. 32 Afternoon (2:00 PM 4:30 PM): Serene Shrines & Gardens Explore: Stroll to Kasuga Taisha Shrine through tranquil lantern-lined forest path. Return: Head back to JR Nara Station and return to Kyoto. Dinner (7:00 PM): Crispy Tonkatsu in Nara or Kyoto Restaurant: Marukatsu Nara Honten (or nearby Kyoto options) Cuisine: Tonkatsu (Pork Cutlet) Price Range: Budget-friendly ($) Why it Complements the Itinerary: hearty and satisfying way to end the day before returning to your hotel."
        },
        {
            "title": "Accommodation Recommendation",
            "content": "Hotel: Cross Hotel Kyoto Address: 71-1 Daikokucho, Nakagyo Ward, Kyoto Rating: 4.6 stars (1,796 reviews) Why its Perfect for You: Combines comfort, design, and prime location. Great for couples, near Nishiki Market and Gion. Recommended Room: Standard or Superior King Room Budget Breakdown (for 2 People) Total Budget: $1964.00 USD Estimated Total Cost: $1964.00 USD Utilization: 100% Detailed Breakdown: Accommodation: $N/A Transportation: $N/A Activities: $N/A Food: $N/A Practical Travel Tips for Two Get an IC Card: Use Suica or ICOCA for effortless train and bus travel. Share and Sample: Share snacks at Nishiki Market to try more without getting full. Pack Smart: Layer clothing and wear comfortable shoes. Cash is Still King: Many local shops and food stalls only accept yen. Embrace the Quiet: Respectful silence in shrines enhances the experience. Enjoy every moment of your incredible Japanese adventure! 33 C.3.2 Generated Plan by video-only Setting Travel Guide Destination: N/A Travel Dates: 2 days Group Size: 2 people Budget: $1964.00 USD Your Ultimate 2-Day Tokyo Adventure: Comprehensive Guide Welcome to Tokyo! Get ready for whirlwind 48-hour journey that perfectly balances the electric, trend-setting energy of modern Tokyo with its timeless traditions and unparalleled culinary scene. This itinerary is designed for two people, focusing on shared experiences, incredible food, and efficient travelall inspired by the dynamic flow and visual highlights of your travel video. The vibe of this trip is energetic, youthful, and deeply immersive. Well move from the effortlessly cool vintage shops of Shimokitazawa to the dazzling neon canyons of Shinjuku, and from the bustling chaos of world-famous fish market to the serene grounds of an ancient temple. Its trip of delightful contrasts, perfect for travelers who want to see, taste, and feel the very best of what Tokyo has to offer. Day-by-Day Itinerary Day 1: The Heartbeat of Modern Tokyo Fashion, Culture & Neon Nights Morning (9:00 AM 1:00 PM): Vintage Hunting in Shimokitazawa Why this first? Begin in the effortlessly cool, low-rise neighborhood of Shimokitazawaknown for its maze of alleyways filled with vintage stores, quirky cafes, and record shops. What to Expect: Wander the narrow streets and discover hidden gems like New York Joe Exchange or Flamingo. Duration: 4 hours. Cost: Free to explore; transportation approx. $3$4 per person. Lunch (1:00 PM): Soul-Warming Soup Curry Restaurant: Rojiura Curry Samurai (Shimokitazawa) Address: 3-31-14 Kitazawa, Setagaya City, Tokyo Why here? hearty, healthy, and trendy Hokkaido-style curry perfect for the area. What to Order: \"Chicken and 20 Kinds of Vegetables\"customizable and photogenic. Cost: Approx. $30$40 USD for two. Afternoon (2:30 PM 6:00 PM): The Shibuya Spectacle Transportation: 5-minute ride on Keio Inokashira Line to Shibuya Station. Activities: 1. Shibuya Scramble Crossing Cross once, then view from Starbucks Tsutaya. 2. Hachiko Statue Iconic meeting spot. 3. Puppy Cafe Break Relax with adorable dogs. Cost: Puppy Cafe: approx. $30$40 USD for two. Evening (6:00 PM onward): Shinjukus Dazzling Nightscape Transportation: JR Yamanote Line to Shinjuku. Activities: 1. 3D Calico Cat Billboard at Cross Shinjuku Vision. 2. Omoide Yokocho (Memory Lane) Lantern-lit alley with yakitori stalls. Dinner (7:30 PM): Authentic Yakitori Restaurant: Tachan () Address: 1-2-8 Nishishinjuku, Shinjuku City, Tokyo What to Order: Momo (chicken thigh), Tsukune (meatball), Negima (chicken & leek), with draft beer. Cost: Approx. $80$100 USD for two, including drinks. Day 2: Tradition, Food Markets & Refined Flavors Morning (9:00 AM 12:00 PM): Tsukiji Outer Market Why this first? Experience the energy and variety of Japans iconic food market. What to Expect: Share snacks like tuna sashimi, tamagoyaki, grilled scallops, and strawberry daifuku. Cost: Approx. $80$100 USD for two. Lunch (12:30 PM): Zeitaku Don at Tsujihan Address: 3-1-15 Nihonbashi, Chuo City, Tokyo What to Order: \"Zeitaku Don\" sashimi bowl turned ochazuke with sea bream broth. Cost: Approx. $70$80 USD for two. Afternoon (2:30 PM 5:00 PM): Asakusas Cultural Heart Transportation: Toei Asakusa Line to Asakusa. Activities: 1. Senso-ji Temple Participate in traditional rituals. 2. Nakamise-dori Traditional souvenirs and street snacks. Cost: Free. Late Afternoon (5:00 PM 6:30 PM): Tokyo Character Street Transportation: Ginza Line to Tokyo Station. What to Expect: Browse anime shops and Gachapon machines. Cost: Free. Dinner (7:30 PM): Elegant Udon at Ginza Sato Yosuke Address: 7-2-19 Ginza, Chuo City, Tokyo What to Order: \"Seiro\" (cold Inaniwa udon) with soy and sesame miso sauces + tempura. Cost: Approx. $120$140 USD for two."
        },
        {
            "title": "Accommodation Recommendation",
            "content": "Hotel: OMO5 Tokyo Gotanda by Hoshino Resorts Address: 8-4-13 Nishigotanda, Shinagawa City, Tokyo Why It Works: Prime Location: On JR Yamanote & Toei Asakusa Linesideal for both days. Modern Comfort: Stylish, smart, and cozy rooms by trusted Japanese brand. Great Value: Reasonable pricing compared to Shinjuku or Ginza. Room: Standard Double or Twin Room. Cost: Approx. $250/night (total $500 for 2 nights). Comprehensive Budget Breakdown (2 People, 2 Days) Total Budget: $1964.00 USD Estimated Total Cost: $1964.00 USD Budget Utilization: 100% Accommodation: $500 Food: Approx. $420 Activities: Approx. $70 Transportation: Approx. $60 Miscellaneous/Souvenirs: $200 Contingency: $ Practical Tips for Your Group of Two Get Suica or Pasmo Card for convenient transit and payments. Rent pocket Wi-Fi or ensure mobile data is available. Share dishes to try more varietyespecially at markets and izakayas. Designate meeting point in case of separation (e.g., Hachiko statue). Carry cashespecially for stalls and traditional shops. Pack comfortable shoes and dress in layers. 35 C.3.3 Generated Plan by P+V Setting Travel Guide Destination: N/A Travel Dates: 2 days Group Size: 2 people Budget: $1964.00 USD Your Unforgettable 2-Day Kyoto Adventure Welcome to Kyoto! Get ready for whirlwind 48 hours that perfectly balances serene nature, iconic temples, and world-class cuisine. This itinerary is designed for two people, focusing on creating seamless and enriching experience that minimizes travel time and maximizes immersion. Drawing inspiration from real-world travel footage, this plan isnt just about seeing the sightsits about feeling the ancient heart of Japan. You have generous budget of $1,964, which allows for comfort, convenience, and few splurges without any financial stress. Well be using mix of efficient public transport and strategic taxi rides to make the most of every moment. Note on Traveling as Pair: Kyoto is fantastic city for two. It allows for intimate moments, whether youre sharing quiet cup of tea or navigating the bustling Nishiki Market. This plan includes moments for both shared discovery and quiet reflection. Coordinating is simple; just ensure your phones are charged and you have portable Wi-Fi device or local SIMs to stay connected. Day 1: Majestic Nature and Golden Pavilions Theme: From the wild, natural beauty of Arashiyamas river gorge to the meticulously crafted Zen gardens of Kyotos northern temples. Morning (8:00 AM 1:00 PM): Arashiyama by River and Foot Hozugawa River Boat Ride: From JR Kyoto Station, take the Sagano Line to Kameoka Station ( 30 mins, $2.80 USD/person). The boat ride lasts 2 hours and is the scenic highlight of the morning. Cost: 4100 ($27.50 USD). Arashiyama Bamboo Grove & Togetsukyo Bridge: Walk through the famous bamboo forest ( 45 min) and cross the historic bridge ( 30 min). Afternoon (1:00 PM 5:00 PM): Kaiseki Lunch & Zen Temples Shoraian: Address: 15 Sagakamenoocho, Ukyo Ward Cuisine: Tofu Kaiseki ($$$) Cost: $120 USD for two Reservation required. Kinkaku-ji (Golden Pavilion): Taxi from Arashiyama ( 25 mins, $27 USD). Entry: 500 ( $3.35 USD) Duration: 1 hour Ryoan-ji Temple: Walk 15 min from Kinkaku-ji Entry: 600 ( $4.00 USD) Evening (7:00 PM): Yakitori Dinner Restaurant: Kushikura Honten Cuisine: High-end Yakitori ($$) Estimated Cost: $80 USD for two Day 2: Sacred Gates and Historic Geishas Theme: deep dive into Kyotos spiritual legacy and geisha culture. Morning (7:30 AM 12:30 PM): Fushimi Inari & Kiyomizu-dera Fushimi Inari Taisha: Start early ( 7:30 AM) Free entry Optional hike to Yotsutsuji intersection ( 45 min). Kiyomizu-dera: Take Keihan Line to Kiyomizu-Gojo Station Entry: 400 ( $2.70 USD) Afternoon (12:30 PM 5:00 PM): Historic Streets & Philosophers Path Lunch at Gyukatsu Kyoto Katsugyu: Address: 6-chome-583 Gojobashihigashi, Higashiyama Ward Cuisine: Gyukatsu ($$) Cost: $40 USD for two 36 Nanzen-ji & Philosophers Path: Explore Zen temple grounds (free) Optional sub-temples: $8 USD/person Evening (5:00 PM late): Gion District & Farewell Dinner Explore Gion: Hanami-koji and Shirakawa Canal Dinner at Teppan Tavern Tenamonya: Address: 537-2 Gionmachi Minamigawa, B1F Cuisine: Teppanyaki / Steakhouse ($$) Cost: $110 USD for two Excluded POIs: Why We Skipped Them Nijo Castle: Geographically isolated from the itinerary. Ginkaku-ji: Too far from Nanzen-ji to include both."
        },
        {
            "title": "Accommodation Recommendation",
            "content": "Hotel: Cross Hotel Kyoto Address: 71-1 Daikokucho, Nakagyo Ward, Kyoto Rating: 4.6 stars (1,800 reviews) Why its perfect: Excellent location near Gion and Nishiki Market Room for 2: Standard King or Twin Room Estimated Cost: $250 per night Comprehensive Budget Breakdown (for 2 People) Total Budget: $1964.00 USD Estimated Total Cost: $1964.00 USD Utilization: 100% Cost Breakdown: Accommodation: $500 Transportation: $100 Activities: $300 Food: $1064 Practical Travel Tips for Kyoto Cash is King: Many places are cash-only. Get Suica/ICOCA Card: For easy travel. Pack Light Layers: Weather can change quickly. Respectful Photography: Be mindful in Gion and temples. Comfortable Shoes: Youll walk lot. Portable Wi-Fi or SIM: Stay connected easily. Enjoy every moment of your unforgettable Kyoto journey!"
        }
    ],
    "affiliations": [
        "AI Shift, Inc.",
        "CyberAgent, Inc.",
        "Nara Institute of Science and Technology",
        "Waseda University"
    ]
}