{
    "paper_title": "Pangea: A Fully Open Multilingual Multimodal LLM for 39 Languages",
    "authors": [
        "Xiang Yue",
        "Yueqi Song",
        "Akari Asai",
        "Seungone Kim",
        "Jean de Dieu Nyandwi",
        "Simran Khanuja",
        "Anjali Kantharuban",
        "Lintang Sutawika",
        "Sathyanarayanan Ramamoorthy",
        "Graham Neubig"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Despite recent advances in multimodal large language models (MLLMs), their development has predominantly focused on English- and western-centric datasets and tasks, leaving most of the world's languages and diverse cultural contexts underrepresented. This paper introduces Pangea, a multilingual multimodal LLM trained on PangeaIns, a diverse 6M instruction dataset spanning 39 languages. PangeaIns features: 1) high-quality English instructions, 2) carefully machine-translated instructions, and 3) culturally relevant multimodal tasks to ensure cross-cultural coverage. To rigorously assess models' capabilities, we introduce PangeaBench, a holistic evaluation suite encompassing 14 datasets covering 47 languages. Results show that Pangea significantly outperforms existing open-source models in multilingual settings and diverse cultural contexts. Ablation studies further reveal the importance of English data proportions, language popularity, and the number of multimodal training samples on overall performance. We fully open-source our data, code, and trained checkpoints, to facilitate the development of inclusive and robust multilingual MLLMs, promoting equity and accessibility across a broader linguistic and cultural spectrum."
        },
        {
            "title": "Start",
            "content": "Preprint - Work in Progress PANGEA: FULLY OPEN MULTILINGUAL MULTIMODAL LLM FOR 39 LANGUAGES Xiang Yue, Yueqi Song, Akari Asai, Seungone Kim, Jean de Dieu Nyandwi, Simran Khanuja, Anjali Kantharuban, Lintang Sutawika, Sathyanarayanan Ramamoorthy, Graham Neubig {xyue2,yueqis,gneubig}@cs.cmu.edu"
        },
        {
            "title": "Carnegie Mellon University",
            "content": "https://neulab.github.io/Pangea/"
        },
        {
            "title": "ABSTRACT",
            "content": "Despite recent advances in multimodal large language models (MLLMs), their development has predominantly focused on Englishand western-centric datasets and tasks, leaving most of the worlds languages and diverse cultural contexts underrepresented. This paper introduces PANGEA, multilingual multimodal LLM trained on PANGEAINS, diverse 6M instruction dataset spanning 39 languages. PANGEAINS features: 1) high-quality English instructions, 2) carefully machine-translated instructions, and 3) culturally relevant multimodal tasks to ensure cross-cultural coverage. To rigorously assess models capabilities, we introduce PANGEABENCH, holistic evaluation suite encompassing 14 datasets covering 47 languages. Results show that PANGEA significantly outperforms existing open-source models in multilingual settings and diverse cultural contexts. Ablation studies further reveal the importance of English data proportions, language popularity, and the number of multimodal training samples on overall performance. We fully open-source our data, code, and trained checkpoints, to facilitate the development of inclusive and robust multilingual MLLMs, promoting equity and accessibility across broader linguistic and cultural spectrum. 4 2 0 2 1 2 ] . [ 1 3 5 1 6 1 . 0 1 4 2 : r Figure 1: Overview of the aggregate performance of various multimodal LLMs on PANGEABENCH. Our PANGEA-7B demonstrates comparable performance to SoTA open-source models in English settings, while significantly outperforming them in multilingual scenarios. Equal Contributions. 1 Preprint - Work in Progress Figure 2: Statistics of our PANGEAINS, which comprises 6M multimodal instructions in 39 languages. PANGEAINS includes general instructions, document and chart question answering, captioning, domain-specific, culturally relevant, and text-only instructions. To address the data scarcity of multilingual multimodal instructions, we employ three strategies: 1) translating high-quality English instructions into multiple languages, filtering culturally relevant images; 2) generating culturally aware instructions, and 3) collecting and re-purposing existing open-source multimodal datasets."
        },
        {
            "title": "INTRODUCTION",
            "content": "Multimodal large language models (MLLMs) (Liu et al., 2023b; Deitke et al., 2024; OpenAI, 2024b; Team et al., 2023) have demonstrated impressive capabilities in tasks such as image captioning, visual question answering, and complex reasoning (Yue et al., 2024a;b). Despite this rapid progress the overwhelming focus on Englishand in their reasoning capabilities, critical flaw persists: western-centric training datasets and evaluation benchmarks (Liu et al., 2021; Song et al., 2023). This homogeneity results in lack of representation for the vast majority of the worlds languages and diverse cultural contexts (Yu et al., 2022). Consequently, models predominately trained on such data exhibit: (a) diminished performance in multilingual settings (Blasi et al., 2022) with poor tokenization further leading to higher inference costs (Ahia et al., 2023); (b) generate outputs misaligned with the socio-cultural norms of underrepresented languages (AlKhamissi et al., 2024); and (c) lack the ability to recognize objects from geographically diverse regions (Ramaswamy et al., 2024) or rare objects belonging to the long-tail (Gupta et al., 2019). With the increased adoption of these models into real-world applications across the globe, theres an urgent need to develop multilingual MLLMs that equitably serve diverse set of users. Few efforts have been made to develop multilingual MLLMs (Geigle et al., 2024; Rasheed et al., 2025), however, their performance still exhibits inequalities across languages and lacks evaluation of cultural understanding. In this paper, we address how to train and evaluate multilingual MLLMs that are culturally inclusive, using limited open-source resources, tackling four major challenges (Yu et al., 2022): 1) Data scarcity: high-quality multilingual multimodal data is scarce, especially in low-resource languages, which makes it difficult to create large-scale training data; 2) Cultural nuances: visual interpretations are context-dependent and vary across cultures (Ramaswamy et al., 2023; Khanuja et al., 2024); 3) Catastrophic forgetting: training on many languages or modalities often results in suboptimal performance on some subsets and require careful balancing; 4) Evaluation complexity: developing an evaluation suite that accurately measures performance across languages and cultures requires substantial resources and expertise. To tackle these challenges, we introduce PANGEA, an open-sourced multilingual MLLM designed to bridge linguistic and cultural gaps in visual understanding tasks. PANGEA is trained on PANGEAINS (Figure 2), high-quality multilingual multimodal instruction tuning dataset comprising 6 million samples in 39 typologically diverse languages. PANGEAINS combines existing open-source resources with newly created instructions focused on multicultural understanding. We curate high-quality English instructions, carefully translate them, and adapt them for multilingual con2 Preprint - Work in Progress texts. To address Western-centric biases in visual representations, we source images from LAIONMulti (Schuhmann et al., 2022), which includes images from various countries and captions in multiple languages. However, LAION-Multi contains many images that are not culturally representative of the countrys speaking population. Additionally, the associated alt text is often short, noisy, and lacks sufficient detail. To combat these issues, we develop multi-cultural multilingual multimodal instruction generation pipeline. This pipeline leverages an LLM (Dubey et al., 2024) to score and filter images based on cultural informativeness. We then enhance the remaining data by generating detailed descriptions and creating complex instructions that combine culturally relevant tasks with general multilingual scenarios. This approach improves the models cultural understanding while maintaining robust multilingual performance. To evaluate PANGEAs capabilities, we present PANGEABENCH, comprehensive multilingual and multimodal evaluation suite comprising five multimodal and three text-based tasks across 14 datasets in 47 languages. PANGEABENCH assesses MLLMs performance on open-domain multimodal chat, image captioning, cultural understanding, multimodal reasoning, and text-only tasks including question answering and complex math reasoning. key highlight of PANGEABENCH is the introduction of xChat, human-crafted benchmark designed to evaluate open-ended, information-seeking multimodal conversations. xChat employs fine-grained evaluation pipeline where human annotators annotate both reference answers and scoring rubrics for each query. An LLM then uses these rubrics to score the models predictions on 1-5 scale. This approach offers more precise assessment of MLLM performance, addressing limitations of coarse LLM-as-Judge methods (Zheng et al., 2023). Additionally, we introduce xMMMU, translated version of MMMU (Yue et al., 2024a), testing college-level multimodal reasoning across seven languages. Together, these components provide robust, nuanced evaluation of PANGEAs cross-lingual and cross-cultural capabilities. Our results demonstrate PANGEA abilities in both English and multilingual scenarios, significantly outperforming existing open-source MLLMs across PANGEABENCH datasets, surpassing the best open MLLMs by 7.3 points on English tasks and 10.8 points on multilingual tasks on average. Notably, PANGEA excels in multilingual and multi-cultural understanding, as evidenced by its performance on xChat and CVQA benchmarks. PANGEA also matches or outperforms state-of-the art proprietary LLMs, namely Gemini-1.5-Pro and GPT4o, on several tasks. However, some performance gaps remain in multimodal chat and complex reasoning, shedding light on the need for further improvements in open multimodal LLMs. We discuss key insights from PANGEAs training, including the scaling effect of instructions, the role of English data, and the impact of languagespecific training proportions on performance. Additionally, we explore preliminary methods to improve multilingual OCR. We fully open-source PANGEAINS, PANGEABENCH, PANGEA models, and code, enabling advances in culturally inclusive MLLMs across diverse languages."
        },
        {
            "title": "2 PANGEAINS: MULTILINGUAL MULTIMODAL INSTRUCTION TUNING",
            "content": "Creating truly multilingual, multicultural MLLM presents unique challenges. We developed PANGEAINS, diverse and high-quality dataset specifically designed for instruction tuning. Comprising 6 million samples in 39 languages, PANGEAINS was curated with focus on linguistic and cultural diversity. We implemented three key strategies to ensure comprehensive coverage, each addressing the specific hurdles encountered in multilingual multimodal learning. Figure 2 shows the details of our PANGEAINSs distribution. 2.1 MACHINE TRANSLATED INSTRUCTIONS The scarcity of human-annotated multilingual multimodal training datasets presents significant challenge. To address this, we primarily adopt machine translation as practical and scalable solution to extend our dataset beyond English. While human annotation is ideal, it is resource-intensive and impractical for covering wide range of languages. Constructing High-quality Pool of English Instructions from Existing Sources. We first collect high-quality set of English multimodal instructions, which serve as the foundation for translation into other languages. These instructions span wide range of visual understanding tasks, including general visual instructions and conversations (Tong et al., 2024; Liu et al., 2024), visual reasoning, captioning, and chart question answering (Masry et al., 2022). Besides, we also added text-only 3 Preprint - Work in Progress Figure 3: Overview of multicultural understanding instructions data generation pipeline. high-quality English instructions, covering general instructions (Teknium, 2023), code (Zheng et al., 2024), and math (Li et al., 2024c). Figure 2 shows the statistics of our translated datasets. By leveraging existing English instructions, we ensured comprehensive coverage of visual interpretation and text instruction following tasks in English, preparing pool of high-quality data for translation. Translation Model Selection. To expand the English instructions to other languages, we initially experimented with strong open-source machine translation models, such as NLLB-3B (NLLB Team, 2024). However, we found that these models struggled with complex instruction-following scenarios and context-switching tasks, particularly in specialized domains like code generation and mathematical reasoning. For example, in code-related tasks, the model failed to recognize and correctly translate programming language keywords, significantly reducing the quality of the instructions. Based on these limitations, we shifted to using the proprietary Gemini 1.5 Pro model, which shows slightly better performance in small-scale human evaluations compared with GPT4o. Post-Processing Translated Data. Even with high-quality translations, inconsistencies arose. To resolve issues such as mismatched conversation turns or missing candidates in multiple-choice questions, we developed post-processing pipeline. This pipeline automatically corrected these errors or directly dropped the examples, ensuring that all translated instructions remained consistent. Overall, we found the Gemini 1.5 Pro translation to be satisfactoryproviding fast, cost-effective alternative to human annotation, especially for scaling across multiple languages. However, we acknowledge that machine translation still has some limitations, particularly in handling nuanced contexts and cultural subtleties. 2.2 MULTICULTURAL UNDERSTANDING INSTRUCTIONS While machine translation enables us to scale across multiple languages, data translated from English is still Anglo-centric in its coverage of cultural concepts (Yu et al., 2022). To address this, we developed pipeline focused on creating instructions for multicultural understanding. Both visual and textual elements can convey deep cultural significance, and our goal is to design dataset that allows models to not only recognize these nuances but also respond appropriately across various cultural contexts. The pipeline of creating multicultural understanding instructions is shown in Figure 3. Curation of Culturally Diverse Images. To ensure that our dataset captures wide array of cultural contexts, we began by sampling 10 million images from the LAION-Multi dataset (Schuhmann et al., 2022), which includes images and short alt texts from diverse languages and regions. filtering process was proposed to guarantee both the quality and cultural relevance of the images. Heuristic Filtering: We implemented automatic filtering based on several key criteria: Image Size, Aspect Ratio, Text Length, NSFW content, Offensive Text, Deduplication, and CLIP Score (used to assess the alignment between the image and its textual description). This helped remove lowquality or inappropriate images and ensured the remaining dataset adhered to quality standards. LLM Scoring: To further refine the dataset, we employed the Llama-3.1-8B-Instruct model (Dubey et al., 2024) to evaluate the quality, subjects, and cultural relevance of the accompanying text descriptions (alt text) for each image. The model was instructed to perform the following tasks: 1) Evaluate Text Quality: The alt text was rated on scale from 1 to 5 based on how well it described the corresponding image, assuming the model could not access the image itself. Alt 4 Preprint - Work in Progress text scoring below 4 was removed. 2) Subject Classification: The model assigned subject or category to the alt text based on its content. 3) Country/Region Classification: The model determined whether the alt text was closely related to specific countrys culture. Images classified as no specific country (approximately 60% of the dataset) were excluded to ensure we focused on culturally identifiable content. The full LLM scoring prompt is included in Appendix B. Avoiding Overrepresentation: To maintain balanced representation, we downsampled images from frequently occurring subjects, such as objects, materials, and clothing, to avoid skewing the dataset toward specific topics or regions. Additionally, an accessibility check was conducted, which removed around 30% of the remaining samples due to image download or other issues. Ultimately, we curated final set of 1 million high-quality, culturally specific images that form the foundation of our dataset. Captioning of Multicultural Images with Different Languages. To provide context and enhance the models ability to interpret the images accurately, we regenerated more detailed caption using Gemini 1.5 Pro based on high-quality original text. Each image was accompanied by caption written in the language corresponding to its cultural origin. This step was crucial as writing captions in the native language of the images cultural context adds layer of authenticity and helps the model learn language-specific nuances. Generating Multilingual and Cross-Cultural Instructions. For each image, we generated captions in the corresponding native language using Gemini 1.5 Pro. However, our approach was not just about using capable model. The filtered high-quality alt text played critical role in enriching the data, as it often contained culturally specific and contextually important information that would otherwise be absent from the images alone. For example, in the Appendix Figure 9, with high-quality alt text, model can incorporate details such as President and CEO of The Walt Disney Company standing in front of model of Shanghai Disneyland, which adds significant context that may not be immediately evident from the image. This additional layer of information helps the model generate captions that better capture the cultural and contextual nuances. After recaptioning, we generated multilingual instructions based on the detailed captions with Gemini 1.5 Pro. Instead of only prompting the model to generate random instructions, we did careful prompt engineering where we first came up with 13 task types (e.g., Information Seeking, Coding & Debugging, Critical Reasoning, Cultural Interpretation, etc.). Then for each image, up to two QA pairs were created, representing different instruction types to ensure diverse set of interactions. This approach ensures that the model not only recognizes these visual elements but also responds appropriately across varied linguistic and different instruction contexts. The captioning and instruction generation prompts are included in Appendix B. 2.3 CURATING EXISTING MULTILINGUAL INSTRUCTIONS To further enrich PANGEAINS, we conducted an extensive survey of the available multilingual multimodal literature and datasets, including those hosted on HuggingFace. As result, we incorporated several high-quality, open-source datasets into our PANGEAINS mixture. These include Chinese ALLaVA-4V (Chen et al., 2024), Viet Document and OCR QA (Doan et al., 2024), Llava Chinese (LinkSoul-AI, 2023), Llava Medical Chinese Instruction (BUAA, 2023), LLaVA-JapaneseInstruct (Toshi456, 2023), MTVQA (Tang et al., 2024), Japanese STAIR Captions (Yoshikawa et al., 2017), Russian GQA (Belopolskih & Spirin, 2024), French Doc-VQA (Sonagu & Sola, 2024), and French Table-VQA (Agonnoude & Delestre, 2024). Each of these datasets brings unique linguistic and cultural perspectives to the mix, covering wide range of languages and task types. 2.4 DATASET STATISTICS By combining these three methods, we created PANGEAINS as comprehensive dataset that addresses the major challenges in building multilingual multimodal models: data scarcity, linguistic diversity, and cultural nuance. The datasets balanced distribution across languages and tasks supports the development of more sophisticated LLMs that can handle complex visual and textual content in multilingual, multicultural context. Language and Task Distribution: PANGEAINS features an extensive and balanced distribution of languages, tasks, and cultural contexts (as shown in Figure 2). We empirically keep the final lan5 Preprint - Work in Progress Category Tasks Multimodal Multimodal Chat Captioning Cultural Understanding Multilingual VQA Reasoning (Multi-subject) QA Datasets Forms xChatBench Long M-LlavaBench Long Size 400 600 Languages zh,en,hi,id,ja,rw,ko,es ar,bn,zh,fr,hi,ja,ru,es,ur,en XM100 CVQA MaRVL xGQA MaXM xMMMU M3Exam TyDiQA Long MC Short Short MC 3.6K 36 languages 21K 6K 77K 2K en,zh,ko,mn,ja,id,jv,min,su, id,sw,ta,tr,zh en,de,pt,ru,id,bn,ko,zh hi,th,zh,fr,en,iw,ro Short/MC 3K 3K MC en,ar,fr,hi,id,ja,pt en,zh,it,pt,vi,th,af Short Long MC MC Open 5.1K 18K ar,ru,bn,te,fi,sw,ko,id,en ar,en,fr,de,hi,id,iw,ja,pt,ro,tr 197K ar,bn,de,es,fr,hi,id,it,ja,ko,pt,sw,yo,zh en,ar,es,eu,hi,id,my,ru,sw,te,zh 21K bn,de,en,es,fr,ja,ru,sw,te,th,zh 3K Text-only Translation FLORES-Sub Reasoning (Multi-subject, Commonsense, Math) MMMLU XStoryCloze MGSM Figure 4: Overview of PANGEABENCH, which contains 5 multimodal and 3 text tasks covering 14 datasets (including two newly curated xChatBench and xMMMU datasets). The table provides details about the datasets, while the figure shows evaluation examples from five different multimodal eval tasks in our PANGEABENCH. guage ratio of English to Multilingual as 40%:60% as we found significant portion of English data plays an important role in cross-lingual transfer. See more discussions about the ratio in section 5 and Figure 6. The inclusion of diverse multimodal instructions ensures that the model develops deeper understanding of varied linguistic and cultural environments. Examples of training samples from different languages and categories are provided in Appendix G. The comprehensive nature of PANGEAINS lays solid foundation for training PANGEA, enabling it to become truly multilingual, multicultural multimodal LLM, capable of understanding and interacting effectively with users from diverse linguistic and cultural backgrounds."
        },
        {
            "title": "3 PANGEABENCH: EVALUATION OF MULTILINGUAL MULTIMODAL MODELS",
            "content": "3.1 OVERVIEW OF PANGEABENCH To assess the capabilities of PANGEA across variety of languages, cultures, and task types, we have developed PANGEABENCH, comprehensive multilingual and multimodal evaluation suite. PANGEABENCH integrates diverse benchmarks that encompass both multimodal and text-only tasks, enabling holistic evaluation of PANGEAs performance in cross-lingual, cross-cultural, and multimodal contexts. Each task within PANGEABENCH is designed to probe specific aspects of PANGEAs proficiency, ensuring robust testing across wide range of scenarios. 3.2 MULTIMODAL TASKS The multimodal tasks in PANGEABENCH are categorized as follows: Multimodal Chat, Captioning, Cultural Understanding, Multilingual Visual Question Answering (VQA), and Multi-Subject Rea6 Preprint - Work in Progress soning. These tasks incorporate datasets from PANGEABENCH to ensure comprehensive testing of PANGEAs multimodal capabilities. The overview and examples of PANGEABENCH from each task are shown in Figure 4. Multimodal Chat. This task tests the models ability to engage in natural, dynamic real-world conversations involving both text and images. In this category, Multilingual LlavaBench (Rasheed et al., 2025) (M-LlavaBench for short) stands as the only benchmark for evaluating multilingual long-form generation capabilities from MLLMs. Moreover, following the evaluation pipeline from Zheng et al. (2023) and Liu et al. (2023b), M-LlavaBench uses coarse-grained evaluation criteria (i.e., Please rate the helpfulness, relevance, accuracy, level of details of their responses.). Prior works have shown that employing such coarse-grained evaluation criteria might lead to automatic evaluation results that diverge from how humans would evaluate them (Ye et al., 2023; Kim et al., 2023; Lee et al., 2024; Kim et al., 2024a;b). To assess our baselines based on more accurate evaluation pipeline with fine-grained evaluation criteria on diverse scenarios, we additionally annotate new multilingual multimodal generation benchmark called the xChatBench. An additional explanation of the annotation process of xChatBench is included in Appendix E. For the multimodal chat scenarios, we found that many English-centric models tend to respond in English regardless of the query language. This behavior is problematic, as it undermines the fundamental capability of multilingual model, which should ideally respond in the language of the query. To address this, we implemented strict evaluation criterion where such responses were penalized and assigned score of 0. We believe this is crucial, as users may not understand English, and failing to respond in the appropriate language can hinder effective communication and user experience. Captioning. The XM3600 (Thapliyal et al., 2022) dataset was developed to evaluate models capability in multilingual image captioning. This dataset contains images paired with captions in 36 different languages. The original datasets often contain similar images and captions. To address this, we clustered the images based on their captions and manually selected 100 representative images from these clusters. This approach enhances the diversity of the dataset while also accelerating the evaluation process. We call the dataset XM100. Cultural Understanding. To assess the models ability to reason about and understand culturally diverse visual content, we use the CVQA (Romero et al., 2024) and MaRVL (Liu et al., 2021) datasets. These datasets are designed to test the models performance in reasoning tasks involving culturally relevant imagery and concepts across multiple languages. Multilingual VQA. This task measures the models proficiency in answering questions about images across multiple languages. The xGQA (Pfeiffer et al., 2022b) and MaXM (Changpinyo et al., 2022) datasets provide diverse range of visual question-answering challenges in several languages and scripts, addressing cross-lingual visual understanding. Multi-Subject Reasoning. The xMMMU and M3Exam (Zhang et al., 2023) datasets are used to evaluate the models reasoning abilities across different academic subjects. xMMMU is machinetranslated version of MMMU validation questions, which focuses on multimodal reasoning in multiple subjects. We randomly sample 300 questions from MMMU (Yue et al., 2024a) validation set and employ GPT-4o for the six languages translation. M3Exam challenges the model with complex, real-world educational questions requiring both textual and visual comprehension. Further details on how we ensure the quality of translations for xMMMU, as well as detailed description of other datasets, can be found in Appendix D. 3.3 TEXT-ONLY MULTILINGUAL DATASETS While multimodal tasks are critical for evaluating the holistic capabilities of models like PANGEA, text-only multilingual tasks provide an equally essential dimension to assess. Most existing multimodal evaluations tend to overlook the importance of text-only evaluation, especially across diverse languages. Including text-only tasks in PANGEABENCH allows us to examine whether the model can perform well in scenarios that require deep linguistic understanding without the aid of visual context, highlighting its performance as foundation model. We include three tasks QA, Translation, and Reasoning covering five datasets for the text-only evaluations in PANGEABENCH. Specifically, we include TydiQA (Clark et al., 2020) to test the models ability to answer questions across 11 typologically diverse languages. We adopt the FLORES (NLLB Team, 2024) dataset 7 Preprint - Work in Progress Models AVG (all) Multimodal Chat Cultural Understanding xChatBench M-LlavaBench CVQA MaRVL Gemini-1.5-Pro GPT4o Llava-1.5-7B Llava-Next-7B Phi-3.5-Vision Cambrian-8B Llava-OV-7B Molmo-7B-D Llama3.2-11B PaliGemma-3B PALO-7B mBLIP mT0-XL mBLIP BLOOMZ PANGEA-7B (Ours) over SoTA Open Models Gemini-1.5-Pro GPT4o Llava-1.5-7B Llava-Next-7B Phi-3.5-Vision Cambrian-8B Llava-OV-7B Molmo-7B-D Llama3.2-11B PaliGemma-3B PALO-7B mBLIP mT0-XL mBLIP BLOOMZ PANGEA-7B (Ours) over Best Open Model en 67.1 68.6 45.4 51.1 54.0 50.9 59.5 55.4 57. 37.3 46.3 35.1 36.1 59.9 +0.4 mul 62.5 64.6 28.4 32.7 35.0 36.4 41.3 34.1 41.9 25.8 32.2 29.8 30. 52.7 +10.8 en 67.0 71.0 28.5 40.5 38.5 27.5 51.0 49.5 49.0 6.0 27.0 2.5 4.0 46.0 -3. mul 54.4 64.4 11.8 18.9 13.2 11.3 28.5 21.1 27.8 3.5 11.8 0.5 1.6 35.6 +7.1 en 103.4 104.6 mul 106.6 100.4 66.1 78.9 70.8 78.4 89.7 95.9 93.9 32.1 68.9 32.7 43.5 40.8 50.7 58.0 61.8 55.3 13.8 58. 31.9 71.2 28.2 41.0 84.2 -11.7 89.5 +18.3 en 75.9 79.1 48.9 55.7 56.3 59.7 65.2 59.4 70. 52.9 50.9 40.5 44.9 64.4 -5.8 mul 75.7 79.4 36.5 42.6 42.3 47.5 53.7 48.3 61.4 42.9 39.2 37.5 36. 57.2 -4.2 en 76.4 81.4 56.2 62.8 72.1 75.4 72.7 65.3 64.5 56.5 63.3 67.3 62.3 mul 72.0 82.1 53.7 50.9 56.5 61.8 57.5 54.9 58.1 52.2 54.2 66.7 58.6 87.0 +11.6 79.0 +12.3 Captioning Short VQA Multi-subject Reasoning XM100 xGQA MaXM xMMMU M3Exam en 27.6 27.7 28.6 29.3 30.2 20.6 30.6 22.1 27.6 18.7 30.4 31.9 22.5 30.4 -0. mul 19.1 19.1 1.1 9.4 5.2 9.9 7.0 9.1 4.5 0.8 0.8 3.1 10.3 14.2 +3.9 en 54.2 55.8 62.0 64.8 64.7 64.6 64.4 51.5 55.6 59.7 60.5 44.2 43.3 64.7 -0.1 mul 48.7 51. 30.6 37.8 38.4 39.8 48.2 43.0 45.4 30.5 37.8 39.9 36.9 60.2 +12.0 en 56.4 60.7 49.8 54.9 55.3 55.3 54.9 52.9 55. 47.9 51.4 44.7 44.7 55.3 0.0 mul 63.5 65.4 20.4 21.4 25.0 28.7 34.8 37.5 43.9 19.9 16.3 36.8 24. 53.2 +9.3 en 65.8 69.1 36.2 36.7 42.6 41.8 46.3 44.5 46.5 26.3 33.1 29.3 29.2 45.7 -0. mul 57.7 58.3 31.5 34.3 38.8 33.2 41.0 40.4 41.4 25.2 30.5 30.4 30.8 43.7 +2.3 en 77.4 68.0 32.3 36.5 55.8 34.7 60.4 57.1 51.8 36.0 30.8 22.8 30.3 61.4 +1.0 mul 64.7 61. 29 28.4 37.2 33.4 45.8 39.1 36.6 25.6 27.8 25 29.5 42.1 -3.7 Table 1: Overall performance on the multilingual multimodal benchmarks in PANGEABENCH. The best-performing open model on each dataset is in bold and the second best is underlined. to assess machine translation performance across multiple languages. We sample 11 languages (denoted as FLORES-Sub). We use MMMLU (OpenAI, 2024a), human-translated version of MMLU to test the general language understanding of models. We use XStoryCloze (Lin et al., 2021) and MGSM (Shi et al., 2022) to test the models commonsense and mathematical reasoning ability in multilingual contexts respectively."
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "4.1 EXPERIMENTAL SETUP We train PANGEA on PANGEAINS, our multilingual multimodal dataset comprising 6 million samples across 39 languages. The model is based on LLaVA-Next architecture (Liu et al., 2024) with Qwen2-7B-Instruct (Yang et al., 2024) as the language model backbone. We employ learning rate of 2e-5, batch size of 512, coupled with cosine decay schedule with 0.03 warmup steps. We train the model for 1 epoch. 8 Preprint - Work in Progress Models Vicuna-1.5-7B Qwen2-7B-Instruct Llava-1.5-7B Llava-Next-7B Phi-3.5-Vision PALO-7B PANGEA-7B (Ours) AVG (all) en 52.1 66.6 53.1 54.0 60.7 52. 72.8 mul 38.7 54.5 39.0 38.9 41.7 37.5 54.3 FLORES-Sub enx xen 55.6 61.8 54.7 54.8 28.5 52.9 60.7 42.4 46.0 41.5 41.4 32.5 40.4 44. TyDiQA XStoryCloze MGSM MMMLU en 59.7 72. 66.8 68.3 75.9 69.4 73.7 mul 52.7 71.2 52.8 52.1 51.3 50.8 66. en 78.1 80.3 79.1 79.1 77.9 77.4 79.1 mul 57.4 61. 57.6 57.1 54.8 57.2 61.2 en 17.6 48.8 14.8 15.6 59.2 13.6 82. mul 6.4 40.4 7.6 7.5 33.1 5.8 47.4 en 49.5 70. 50.2 52.1 62.0 46.7 68.4 mul 34.7 53.1 35.7 36.5 36.7 33.4 52. Table 2: Overall performance on text-only multilingual benchmarks in PANGEABENCH. For evaluation, we compare PANGEA against several state-of-the-art open source baselines, including English-centric models Llava-1.5-7B (Liu et al., 2023a), Llava-Next-7B (Liu et al., 2024), Phi3.5-Vision (Abdin et al., 2024), Cambrian-8B (Tong et al., 2024), Llava-OV-7B (Li et al., 2024b), Molmo-7B-D (Deitke et al., 2024) Llama3.2-11B (Dubey et al., 2024) and multilingual models PaliGemma-3B (Beyer et al., 2024), PALO-7B (Rasheed et al., 2025), mBLIP mT0-XL and mBLIP BLOOMZ (Geigle et al., 2024). We also consider two text-only LLMs baselines Vicuna-1.57B (Zheng et al., 2023) and Qwen2-7B-Instruct (Yang et al., 2024), which are the backbones of Llava-Next and our PANGEA respectively. We integrate our multimodal tasks in PANGEABENCH into lmms-eval (Li et al., 2024a),1 multimodal evaluation package that supports many English multimodal benchmarks. We use lm-evaluation-harness (Biderman et al., 2024)2 to evaluate text-only tasks. We follow the original paper for their best models prompts in different tasks, and mostly reproduce their original numbers on datasets reported in the original papers. 4.2 MULTILINGUAL MULTIMODAL RESULTS The results in Table 1 provide clear insights into the strengths and remaining challenges of PANGEA7B in multilingual and multimodal tasks. Key observations from the evaluation include: Superior English and Multilingual Performance: PANGEA-7B outperforms existing open-source models across both English and multilingual tasks. While concurrent multimodal models such as Molmo (Deitke et al., 2024) or Llama 3.2 show strong performance on English datasets, they struggle in multilingual evaluation settings. Particularly in multilingual subsets like xChatBench, MLlavaBench, and MaRVL, it has achieved substantial gains, highlighting its effectiveness in both cross-lingual and cross-cultural contexts. Balanced Cross-Language Capabilities: Unlike many models that exhibit significant drop in performance when moving from English to multilingual tasks, PANGEA-7B is relatively consistent. For instance, in Multimodal Chat tasks, the performance gap between English and multilingual remains relatively small, indicating its ability to handle multiple languages effectively. Challenges Compared to Proprietary Models: While PANGEA-7B leads in open-source models, some gaps remain when compared to closed-source models like GPT4o. Additionally, though PANGEA-7B narrows the gap between English and multilingual performance, there is still room for improvement in fully closing this divide across all tasks. 4.3 MULTILINGUAL TEXT-ONLY RESULTS We further evaluate our model in text-only scenarios in Table 2. Interesting findings include: Best Text Performance Among Multimodal LLMs: PANGEA-7B demonstrates the strongest performance among all multimodal LLMs in the text-only tasks consistently outperforming baselines like Llava-Next-7B. This highlights that, despite being trained as multimodal model, PANGEA-7B maintains superior text understanding and reasoning capabilities compared to other MLLMs. 1https://github.com/EvolvingLMMs-Lab/lmms-eval 2https://github.com/EleutherAI/lm-evaluation-harness Preprint - Work in Progress Maintained Performance from its Text Backbone. PANGEA-7B generally maintains or sees slight drops in performance on most text-only benchmarks compared with its text backbone Qwen2-7BInstruct. Notably, the model shows significant improvement in MGSM. This improvement is directly attributable to the inclusion of math-related instructions in PANGEAINS, which enhances the models capability to handle complex multilingual reasoning and mathematical tasks."
        },
        {
            "title": "5 DISCUSSION",
            "content": "Finally, we explore the implications of our findings and their potential impact on future developments in the field. We examine the scaling effects of instruction quantity, the persistent role of English data, the relationship between training sample proportions and performance, qualitative examples of model behavior, and challenges in multilingual OCR. Through this discussion, we aim to provide comprehensive understanding of our model and chart course for future advancements. Scaling Effect of Number of Instructions. Understanding how the quantity of instructions affects model performance is crucial for optimizing training strategies and resource allocation. Figure 5 reveals clear scaling effect related to the number of instructions used during training. Performance improvements were consistent as we increased the number of multilingual instructions in PANGEAINS, for both English and multilingual performance. This demonstrates the necessity of scaling multilingual multimodal instruction tuning. Figure 5: Scaling effect of training samples on English and multilingual scores across datasets. Role of English Data. In multilingual scenarios, English data plays pivotal role in cross-lingual transfer. To investigate this, we sampled 500K examples from the translated data described in subsection 2.1, ensuring consistent data distribution. We varied the ratio of English data while keeping the total number of training samples fixed at 500K. For the 17 multilingual languages in the translated subset, we evenly distributed the number of samples across languages. As shown in Figure 6 , English performance generally improves as the percentage of English data increases. More surprisingly, using no English data (full multilingual data) results in relatively lower multilingual performance. As we introduce more English data, multilingual performance improves, peaking at 38.7% with 40% English. However, performance drops sharply when English data reaches 100%. This suggests that English data aids cross-lingual transfer, however, over-reliance on it harms multilingual performance. Figure 6: Impact of English training data proportion on English vs. multilingual performance. Figure 7: The relationship between training sample size (relative to English) and performance (relative to English) of different languages across four datasets. 10 Preprint - Work in Progress How does the proportion of training samples in language affect downstream performance? An interesting question to ask is whether the downstream task performance is correlated with the number of training samples. Our analysis in Figure 7 revealed nuanced relationship between training sample proportion and downstream performance. While there is general positive correlation, the impact varies significantly across languages and tasks. For widely spoken languages with rich linguistic resources, we observed near-linear relationship. However, for low-resource languages, even small increase in proportion yielded disproportionately large performance gains. Interestingly, we also noted instances of positive transfer between typologically similar languages. These findings suggest that strategic allocation of training samples, considering both language prevalence and linguistic similarities, can optimize overall model performance. Multimodal Chat in the Wild. One important application of MLLMs is to answer users queries in the wild. Here, we show the outputs of PANGEA for the multimodal chat queries from our xChatBench. As shown in Appendix Figure 10, 11, 12, 13, 14, 15, PANGEA successfully interprets the figures in different tasks and generates fluent and readable in certain languages. These qualitative examples further demonstrate the remarkable visual understanding ability of PANGEA in multilingual contexts. On the other hand, we also identified few bad cases shown in Figure 16, 17. Despite generating relevant responses to the queries, the model does not capture the key details of the images due to the lack of knowledge, which points out potential improvement directions in the future. Preliminary Explorations of Multilingual OCR. Multilingual OCR emerged as particularly challenging aspect of PANGEAs functionality. We made efforts to enhance its multilingual OCR capabilities. Specifically, we constructed dataset of 500K multilingual OCR instructions spanning 10 languages, with 50K examples per language, sourced from web user interfaces. Webpages naturally serve as image-rich environments containing abundant text, and by capturing screenshots of websites from various countries in different languages, we were able to gather substantial number of OCR images. Further details on the construction of the multilingual OCR training dataset are available in Appendix I. Figure 8: preliminary exploration of multilingual OCR. We employed the same model architecture as PANGEA but trained it exclusively on these OCR images, reserving portion of the data as test set. As shown in Figure 8 , the results indicate that improving multilingual OCR performance is feasible with an increase in training samples. However, the OCR accuracy for non-Latin scripts (e.g., Chinese, Japanese, and Korean) remains lower than for Latin-based languages. Looking ahead, we aim to further expand the multilingual OCR training dataset to include more languages and integrate this data into PANGEAINS."
        },
        {
            "title": "6 CONCLUSION",
            "content": "In this paper, we introduced PANGEA, novel multilingual multimodal large language model designed to bridge linguistic and cultural gaps in visual understanding tasks. By leveraging PANGEAINS, our newly curated 6M multilingual multimodal instruction data samples, we demonstrated significant improvements in cross-lingual and cross-cultural understanding across 39 typologically diverse languages. Our comprehensive evaluation using PANGEABENCH revealed PANGEAs superior performance compared to existing open-source models, particularly in tasks requiring nuanced cultural interpretation. We also highlight ongoing challenges in areas such as low-resource language support and multilingual OCR. We fully open-source PANGEA, PANGEAINS, and PANGEABENCH to facilitate future research to build open and inclusive MLLMs."
        },
        {
            "title": "ACKNOWLEDGMENTS",
            "content": "This work was supported in part by the Carnegie Bosch Institute Fellowship and grant from DSTA Singapore. The training is supported by the CMU FLAME Center. The authors would like to thank CMU NeuLab colleagues for their constructive comments. The authors would like to thank Google Gemini credits for data construction and evaluation. 11 Preprint - Work in Progress"
        },
        {
            "title": "REFERENCES",
            "content": "Marah Abdin, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harkirat Behl, et al. Phi-3 technical report: highly capable language model locally on your phone. ArXiv preprint, abs/2404.14219, 2024. URL https://arxiv.org/abs/2404.14219. Tom Agonnoude and Cyrile Delestre. Table vqa dataset, 2024. URL https://huggingface. co/datasets/cmarkea/table-vqa. Orevaoghene Ahia, Sachin Kumar, Hila Gonen, Jungo Kasai, David Mortensen, Noah Smith, and Yulia Tsvetkov. Do all languages cost the same? tokenization in the era of commercial language models. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Proc. of EMNLP, pp. 99049923, 2023. URL https://aclanthology.org/2023.emnlp-main.614.pdf. Badr AlKhamissi, Muhammad ElNokrashy, Mai Alkhamissi, and Mona Diab. Investigating cultural alignment of large language models. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Proc. of ACL, pp. 1240412422, 2024. URL https://aclanthology.org/2024. acl-long.671.pdf. Daniil Belopolskih and Egor Spirin. Gqa-ru, 2024. URL https://huggingface.co/ datasets/deepvk/GQA-ru. Lucas Beyer, Andreas Steiner, Andre Susano Pinto, Alexander Kolesnikov, Xiao Wang, Daniel Salz, Maxim Neumann, Ibrahim Alabdulmohsin, Michael Tschannen, Emanuele Bugliarello, Thomas Unterthiner, Daniel Keysers, Skanda Koppula, Fangyu Liu, Adam Grycner, Alexey Gritsenko, Neil Houlsby, Manoj Kumar, Keran Rong, Julian Eisenschlos, Rishabh Kabra, Matthias Bauer, Matko Boˇsnjak, Xi Chen, Matthias Minderer, Paul Voigtlaender, Ioana Bica, Ivana Balazevic, Joan Puigcerver, Pinelopi Papalampidi, Olivier Henaff, Xi Xiong, Radu Soricut, Jeremiah Harmsen, and Xiaohua Zhai. PaliGemma: versatile 3B VLM for transfer. ArXiv preprint, abs/2407.07726, 2024. URL https://arxiv.org/abs/2407.07726. Stella Biderman, Hailey Schoelkopf, Lintang Sutawika, Leo Gao, Jonathan Tow, Baber Abbasi, Alham Fikri Aji, Pawan Sasanka Ammanamanchi, Sidney Black, Jordan Clive, et al. Lessons from the trenches on reproducible evaluation of language models. ArXiv preprint, abs/2405.14782, 2024. URL https://arxiv.org/abs/2405.14782. Damian Blasi, Antonios Anastasopoulos, and Graham Neubig. Systematic inequalities in language technology performance across the worlds languages. In Proc. of ACL, pp. 54865505, Dublin, Ireland, 2022. Association for Computational Linguistics. URL https://aclanthology. org/2022.acl-long.376. BUAA. Chinese-llava-med, 2023. URL https://huggingface.co/BUAADreamer/ Chinese-LLaVA-Med-7B. Accessed: 2024-10-01. Soravit Changpinyo, Linting Xue, Michal Yarom, Ashish Thapliyal, Idan Szpektor, Julien Amelot, Xi Chen, and Radu Soricut. Maxm: Towards multilingual visual question answering. ArXiv preprint, abs/2209.05401, 2022. URL https://arxiv.org/abs/2209.05401. Guiming Hardy Chen, Shunian Chen, Ruifei Zhang, Junying Chen, Xiangbo Wu, Zhiyi Zhang, Zhihong Chen, Jianquan Li, Xiang Wan, and Benyou Wang. Allava: Harnessing gpt4v-synthesized data for lite vision-language model. ArXiv preprint, abs/2402.11684, 2024. URL https: //arxiv.org/abs/2402.11684. Xi Chen, Josip Djolonga, Piotr Padlewski, Basil Mustafa, Soravit Changpinyo, Jialin Wu, Carlos Riquelme Ruiz, Sebastian Goodman, Xiao Wang, Yi Tay, et al. Pali-x: On scaling up multilingual vision and language model. ArXiv preprint, abs/2305.18565, 2023. URL https: //arxiv.org/abs/2305.18565. Jonathan H. Clark, Eunsol Choi, Michael Collins, Dan Garrette, Tom Kwiatkowski, Vitaly Nikolaev, and Jennimaria Palomaki. TyDi QA: benchmark for information-seeking question answering in typologically diverse languages. Transactions of the Association for Computational Linguistics, 8:454470, 2020. URL https://aclanthology.org/2020.tacl-1.30. Preprint - Work in Progress Matt Deitke, Christopher Clark, Sangho Lee, Rohun Tripathi, Yue Yang, Jae Sung Park, Mohammadreza Salehi, Niklas Muennighoff, Kyle Lo, Luca Soldaini, et al. Molmo and pixmo: Open weights and open data for state-of-the-art multimodal models. ArXiv preprint, abs/2409.17146, 2024. URL https://arxiv.org/abs/2409.17146. Khang Doan, Bao Huynh, Dung Hoang, Thuc Pham, Nhat Pham, Quan Nguyen, Bang Vo, and Suong Hoang. Vintern-1b: An efficient multimodal large language model for vietnamese. ArXiv preprint, abs/2408.12480, 2024. URL https://arxiv.org/abs/2408. 12480. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. ArXiv preprint, abs/2407.21783, 2024. URL https://arxiv.org/abs/2407.21783. Gregor Geigle, Abhay Jain, Radu Timofte, and Goran Glavaˇs. mBLIP: Efficient bootstrapping of multilingual vision-LLMs. In Jing Gu, Tsu-Jui (Ray) Fu, Drew Hudson, Asli Celikyilmaz, and William Wang (eds.), Proceedings of the 3rd Workshop on Advances in Language and Vision Research (ALVR), pp. 725, 2024. URL https://aclanthology.org/2024.alvr-1. 2.pdf. Agrim Gupta, Piotr Dollar, and Ross B. Girshick. LVIS: dataset for large vocabulary instance segmentation. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019, pp. 53565364. Computer Vision Foundation / IEEE, 2019. URL http://openaccess.thecvf.com/content_CVPR_2019/html/ Gupta_LVIS_A_Dataset_for_Large_Vocabulary_Instance_Segmentation_ CVPR_2019_paper.html. Felix Hamborg, Norman Meuschke, Corinna Breitinger, and Bela Gipp. news-please: generic In Proceedings of the 15th International Symposium of Informanews crawler and extractor. tion Science, pp. 218223, 2017. URL https://edoc.hu-berlin.de/server/api/ core/bitstreams/b83362d0-0fb5-4b65-8370-b31f187223a4/content. Seungju Han, Junhyeok Kim, Jack Hessel, Liwei Jiang, Jiwan Chung, Yejin Son, Yejin Choi, and Youngjae Yu. Reading books is great, but not if you are driving! visually grounded reasoning about defeasible commonsense norms. ArXiv preprint, abs/2310.10418, 2023. URL https: //arxiv.org/abs/2310.10418. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. In Proc. of ICLR. OpenReview.net, 2021. URL https://openreview.net/forum?id=d7KBjmI3GmQ. Jack Hessel, Ana Marasovic, Jena Hwang, Lillian Lee, Jeff Da, Rowan Zellers, Robert Mankoff, and Yejin Choi. Do androids laugh at electric sheep? humor understanding benchmarks In Proc. of ACL, pp. 688714, 2023. URL https: from the new yorker caption contest. //aclanthology.org/2023.acl-long.41.pdf. Aashi Jain, Mandy Guo, Krishna Srinivasan, Ting Chen, Sneha Kudugunta, Chao Jia, Yinfei Yang, and Jason Baldridge. Mural: multimodal, multitask retrieval across languages. ArXiv preprint, abs/2109.05125, 2021. URL https://arxiv.org/abs/2109.05125. Simran Khanuja, Sathyanarayanan Ramamoorthy, Yueqi Song, and Graham Neubig. An image speaks thousand words, but can everyone listen? on translating images for cultural relevance. arXiv preprint arXiv:2404.01247, 2024. Geewook Kim, Teakgyu Hong, Moonbin Yim, JeongYeon Nam, Jinyoung Park, Jinyeong Yim, Wonseok Hwang, Sangdoo Yun, Dongyoon Han, and Seunghyun Park. Ocr-free document unIn European Conference on Computer Vision (ECCV), 2022. URL derstanding transformer. https://arxiv.org/abs/2111.15664. Seungone Kim, Jamin Shin, Yejin Cho, Joel Jang, Shayne Longpre, Hwaran Lee, Sangdoo Yun, Seongjin Shin, Sungdong Kim, James Thorne, et al. Prometheus: Inducing fine-grained evaluation capability in language models. In The Twelfth International Conference on Learning Representations, 2023. URL https://arxiv.org/abs/2310.08491. 13 Preprint - Work in Progress Seungone Kim, Juyoung Suk, Ji Yong Cho, Shayne Longpre, Chaeeun Kim, Dongkeun Yoon, Guijin Son, Yejin Cho, Sheikh Shafayat, Jinheon Baek, et al. The biggen bench: principled benchmark for fine-grained evaluation of language models with language models. ArXiv preprint, abs/2406.05761, 2024a. URL https://arxiv.org/abs/2406.05761. Seungone Kim, Juyoung Suk, Shayne Longpre, Bill Yuchen Lin, Jamin Shin, Sean Welleck, Graham Neubig, Moontae Lee, Kyungjae Lee, and Minjoon Seo. Prometheus 2: An open source language model specialized in evaluating other language models. ArXiv preprint, abs/2405.01535, 2024b. URL https://arxiv.org/abs/2405.01535. Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David Shamma, et al. Visual genome: Connecting language and vision using crowdsourced dense image annotations. International journal of computer vision, 123:3273, 2017. URL https://arxiv.org/abs/1602.07332. Seongyun Lee, Seungone Kim, Sue Hyun Park, Geewook Kim, and Minjoon Seo. PrometheusviArXiv preprint, sion: Vision-language model as judge for fine-grained evaluation. abs/2401.06591, 2024. URL https://arxiv.org/abs/2401.06591. Bo Li, Peiyuan Zhang, Kaichen Zhang, Fanyi Pu, Xinrun Du, Yuhao Dong, Haotian Liu, Yuanhan Zhang, Ge Zhang, Chunyuan Li, and Ziwei Liu. Lmms-eval: Accelerating the development of large multimoal models, 2024a. URL https://lmms-lab.github.io/ lmms-eval-blog/lmms-eval-0.1/. Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024b. Jia Li, Edward Beeching, Lewis Tunstall, Ben Lipkin, Roman Soletskyi, Shengyi Costa Huang, Kashif Rasul, Longhui Yu, Albert Jiang, Ziju Shen, Zihan Qin, Bin Dong, Li Zhou, Yann Fleureau, Guillaume Lample, and Stanislas Polu. Numinamath. https://huggingface. co/AI-MO/NuminaMath-CoT, 2024c. https://github.com/project-numina/ aimo-progress-prize/blob/main/report/numina_dataset.pdf. Xi Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu Wang, Shuohui Chen, Daniel Simig, Myle Ott, Naman Goyal, Shruti Bhosale, Jingfei Du, Ramakanth Pasunuru, Sam Shleifer, Punit Singh Koura, Vishrav Chaudhary, Brian OHoro, Jeff Wang, Luke Zettlemoyer, Zornitsa Kozareva, Mona Diab, Veselin Stoyanov, and Xian Li. Few-shot learning with multilingual language models, 2021. URL https://arxiv.org/abs/2112.10668. LinkSoul-AI. Chinese-llava, 2023. URL https://huggingface.co/spaces/LinkSoul/ Chinese-LLaVa. Accessed: 2024-10-01. Fangyu Liu, Emanuele Bugliarello, Edoardo Maria Ponti, Siva Reddy, Nigel Collier, and Desmond In Proc. of EMNLP, pp. Elliott. Visually grounded reasoning across languages and cultures. 1046710485, Online and Punta Cana, Dominican Republic, 2021. Association for Computational Linguistics. URL https://aclanthology.org/2021.emnlp-main.818. Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning, 2023a. URL https://arxiv.org/abs/2310.03744. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In NeurIPS, 2023b. URL https://arxiv.org/abs/2304.08485. Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Improved reasoning, ocr, and world knowledge, 2024. URL https://arxiv. org/pdf/2401.13601. Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, KaiWei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. In The Twelfth International Conference on Learning Representations, 2024. URL https://arxiv.org/abs/2310.02255. 14 Preprint - Work in Progress Ahmed Masry, Xuan Long Do, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. ChartQA: benchmark for question answering about charts with visual and logical reasoning. In Findings of the Association for Computational Linguistics: ACL 2022, pp. 22632279, Dublin, Ireland, 2022. Association for Computational Linguistics. URL https://aclanthology.org/2022. findings-acl.177. Minheng Ni, Haoyang Huang, Lin Su, Edward Cui, Taroon Bharti, Lijuan Wang, Dongdong Zhang, and Nan Duan. M3P: learning universal representations via multitask multilingual multimodal pre-training. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2021, virtual, June 19-25, 2021, pp. 39773986. Computer Vision Foundation / IEEE, 2021. https://openaccess.thecvf.com/content/CVPR2021/html/Ni_M3P_ URL Learning_Universal_Representations_via_Multitask_Multilingual_ Multimodal_Pre-Training_CVPR_2021_paper.html. NLLB Team. Scaling neural machine translation to 200 languages. Nature, 630(8018):841, 2024. URL https://www.nature.com/articles/s41586-024-07335-x. OpenAI. Mmmlu dataset, 2024a. URL https://huggingface.co/datasets/openai/ MMMLU. Accessed: 2024-10-01. OpenAI. Hello gpt4-o. https://openai.com/index/hello-gpt-4o/, 2024b. URL https://openai. com/index/hello-gpt-4o/. Jonas Pfeiffer, Gregor Geigle, Aishwarya Kamath, Jan-Martin Steitz, Stefan Roth, Ivan Vulic, In Findings of the Asand Iryna Gurevych. xGQA: Cross-lingual visual question answering. sociation for Computational Linguistics: ACL 2022, pp. 24972511, Dublin, Ireland, 2022a. Association for Computational Linguistics. URL https://aclanthology.org/2022. findings-acl.196. Jonas Pfeiffer, Gregor Geigle, Aishwarya Kamath, Jan-Martin Steitz, Stefan Roth, Ivan Vulic, In Findings of the Asand Iryna Gurevych. xGQA: Cross-lingual visual question answering. sociation for Computational Linguistics: ACL 2022, pp. 24972511, Dublin, Ireland, 2022b. Association for Computational Linguistics. URL https://aclanthology.org/2022. findings-acl.196. Jonas Pfeiffer, Francesco Piccinno, Massimo Nicosia, Xinyi Wang, Machel Reid, and Sebastian Ruder. mmt5: Modular multilingual pre-training solves source language hallucinations. ArXiv preprint, abs/2305.14224, 2023. URL https://arxiv.org/abs/2305.14224. Vikram V. Ramaswamy, Sing Yu Lin, Dora Zhao, Aaron Adcock, Laurens van der Maaten, Deepti Ghadiyaram, and Olga Russakovsky. Geode: geographically diverse evaluation dataset for object recognition. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine (eds.), Advances in Neural Information Processing Systems, volume 36, pp. 6612766137, 2023. https://proceedings.neurips.cc/paper_files/paper/2023/file/ URL d08b6801f24dda81199079a3371d77f9-Paper-Datasets_and_Benchmarks. pdf. Vikram Ramaswamy, Sing Yu Lin, Dora Zhao, Aaron Adcock, Laurens van der Maaten, Deepti Ghadiyaram, and Olga Russakovsky. Geode: geographically diverse evaluation dataset for object recognition. Advances in Neural Information Processing Systems, 36, 2024. https://proceedings.neurips.cc/paper_files/paper/2023/file/ URL d08b6801f24dda81199079a3371d77f9-Paper-Datasets_and_Benchmarks. pdf. Hanoona Rasheed, Muhammad Maaz, Abdelrahman Shaker, Salman Khan, Hisham Cholakal, Rao M. Anwer, Tim Baldwin, Michael Felsberg, and Fahad S. Khan. Palo: large multilingual multimodal language model. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV 2025), 2025. URL https://arxiv.org/abs/2402.14818. Nils Reimers and Iryna Gurevych. Sentence-BERT: Sentence embeddings using Siamese BERTnetworks. In Proc. of EMNLP, pp. 39823992, Hong Kong, China, 2019. Association for Computational Linguistics. URL https://aclanthology.org/D19-1410. 15 Preprint - Work in Progress David Romero, Chenyang Lyu, Haryo Akbarianto Wibowo, Teresa Lynn, Injy Hamed, Aditya Nanda Kishore, Aishik Mandal, Alina Dragonetti, Artem Abzaliev, Atnafu Lambebo Tonja, et al. Cvqa: Culturally-diverse multilingual visual question answering benchmark. ArXiv preprint, abs/2406.05967, 2024. URL https://arxiv.org/abs/2406.05967. Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. Advances in Neural Information Processing Systems, 35:2527825294, 2022. URL https://arxiv.org/abs/ 2210.08402. Bin Shan, Yaqian Han, Weichong Yin, Shuohuan Wang, Yu Sun, Hao Tian, Hua Wu, and Haifeng Wang. Ernie-unix2: unified cross-lingual cross-modal framework for understanding and generation. ArXiv preprint, abs/2211.04861, 2022. URL https://arxiv.org/abs/2211. 04861. Freda Shi, Mirac Suzgun, Markus Freitag, Xuezhi Wang, Suraj Srivats, Soroush Vosoughi, Hyung Won Chung, Yi Tay, Sebastian Ruder, Denny Zhou, et al. Language models are multilingual chain-of-thought reasoners. In The Eleventh International Conference on Learning Representations, 2022. URL https://arxiv.org/abs/2210.03057. Loıc Sokoudjou Sonagu and Yoann Sola. Docvqa dataset, 2024. URL https://huggingface. co/datasets/cmarkea/doc-vqa. Yueqi Song, Simran Khanuja, Pengfei Liu, Fahim Faisal, Alissa Ostapenko, Genta Winata, Alham Fikri Aji, Samuel Cahyawijaya, Yulia Tsvetkov, Antonios Anastasopoulos, and Graham Neubig. GlobalBench: benchmark for global progress in natural language processing. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Proc. of EMNLP, pp. 1415714171, Singapore, 2023. Association for Computational Linguistics. URL https://aclanthology. org/2023.emnlp-main.875. Jingqun Tang, Qi Liu, Yongjie Ye, Jinghui Lu, Shu Wei, Chunhui Lin, Wanqing Li, Mohamad Fitri Faiz Bin Mahmood, Hao Feng, Zhen Zhao, et al. Mtvqa: Benchmarking multilingual text-centric visual question answering. ArXiv preprint, abs/2405.11985, 2024. URL https://arxiv. org/abs/2405.11985. Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, et al. Gemini: family of highly capable multimodal models. ArXiv preprint, abs/2312.11805, 2023. URL https://arxiv. org/abs/2312.11805. Teknium. Openhermes 2.5: An open dataset of synthetic data for generalist llm assistants, 2023. Ashish V. Thapliyal, Jordi Pont Tuset, Xi Chen, and Radu Soricut. Crossmodal-3600: masIn Proc. of EMNLP, pp. 715729, Abu sively multilingual multimodal evaluation dataset. Dhabi, United Arab Emirates, 2022. Association for Computational Linguistics. URL https: //aclanthology.org/2022.emnlp-main.45. Shengbang Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Manoj Middepogu, Sai Charitha Akula, Jihan Yang, Shusheng Yang, Adithya Iyer, Xichen Pan, et al. Cambrian-1: fully open, vision-centric exploration of multimodal llms. ArXiv preprint, abs/2406.16860, 2024. URL https://arxiv.org/abs/2406.16860. Toshi456. Llava-jp-instruct-108k dataset, 2023. Accessed: 2024-10-01. Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, and Colin Raffel. mT5: massively multilingual pre-trained text-to-text transformer. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 483498, Online, 2021. Association for Computational Linguistics. URL https://aclanthology.org/2021.naacl-main. 41. 16 Preprint - Work in Progress An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, et al. Qwen2 technical report. ArXiv preprint, abs/2407.10671, 2024. URL https://arxiv.org/abs/2407.10671. Seonghyeon Ye, Doyoung Kim, Sungdong Kim, Hyeonbin Hwang, Seungone Kim, Yongrae Jo, James Thorne, Juho Kim, and Minjoon Seo. Flask: Fine-grained language model evaluation based on alignment skill sets. ArXiv preprint, abs/2307.10928, 2023. URL https://arxiv. org/abs/2307.10928. Yuya Yoshikawa, Yutaro Shigeto, and Akikazu Takeuchi. STAIR captions: Constructing largeIn Proc. of ACL, pp. 417421, Vancouver, Canada, scale Japanese image caption dataset. 2017. Association for Computational Linguistics. URL https://aclanthology.org/ P17-2066. Xinyan Yu, Trina Chatterjee, Akari Asai, Junjie Hu, and Eunsol Choi. Beyond counting datasets: survey of multilingual dataset construction and necessary resources. In Findings of the Association for Computational Linguistics: EMNLP 2022, pp. 37253743, Abu Dhabi, United Arab Emirates, 2022. Association for Computational Linguistics. URL https://aclanthology. org/2022.findings-emnlp.273. Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 95569567, 2024a. URL https: //arxiv.org/abs/2311.16502. Xiang Yue, Tianyu Zheng, Yuansheng Ni, Yubo Wang, Kai Zhang, Shengbang Tong, Yuxuan Sun, Ming Yin, Botao Yu, Ge Zhang, et al. Mmmu-pro: more robust multi-discipline multimodal understanding benchmark. ArXiv preprint, abs/2409.02813, 2024b. URL https://arxiv. org/abs/2409.02813. Yan Zeng, Wangchunshu Zhou, Ao Luo, Ziming Cheng, and Xinsong Zhang. Cross-view lanArXiv preprint, guage modeling: Towards unified cross-lingual cross-modal pre-training. abs/2206.00621, 2022. URL https://arxiv.org/abs/2206.00621. Wenxuan Zhang, Mahani Aljunied, Chang Gao, Yew Ken Chia, and Lidong Bing. M3exam: multilingual, multimodal, multilevel benchmark for examining large language models. Advances in Neural Information Processing Systems, 36:54845505, 2023. URL https://arxiv.org/ abs/2306.05179. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural Information Processing Systems, 36:4659546623, 2023. URL https://arxiv.org/abs/2306.05685. Tianyu Zheng, Ge Zhang, Tianhao Shen, Xueling Liu, Bill Yuchen Lin, Jie Fu, Wenhu Chen, and Xiang Yue. Opencodeinterpreter: Integrating code generation with execution and refinement. ArXiv preprint, abs/2402.14658, 2024. URL https://arxiv.org/abs/2402.14658. 17 Preprint - Work in Progress"
        },
        {
            "title": "Table of Contents in Appendix",
            "content": "A Related Work Prompts used in the data construction Recaptioning Example from LAION-Cultural Datasets used in PANGEABENCH 19 23 24 D.1 Multimodal Datasets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24 D.2 Text-Only Multilingual Datasets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24 Explanation of xChatBench Qualitative Examples from xChatBench Training Examples 26 27 35 G.1 Machine Translated Instructions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35 G.2 Multicultural Understanding Instructions . . . . . . . . . . . . . . . . . . . . . . . . . 35 Breakdown Results of Different Languages on PANGEABENCH 44 H.1 xChat . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . H.2 Multilingual LLaVABench . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44 H.3 CVQA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44 H.4 MaRVL . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44 H.5 XM100 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44 H.6 xGQA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45 H.7 MAXM . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . H.8 xMMMU . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47 H.9 M3Exam . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47 H.10 TyDiQA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47 H.11 XStoryCloze . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47 H.12 MGSM . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47 H.13 MMMLU . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Preliminary Exploration of Constructing Multilingual OCR Instructions 48 18 Preprint - Work in Progress"
        },
        {
            "title": "A RELATED WORK",
            "content": "Visual Instruction Tuning. Visual instruction tuning is key technique for enhancing multimodal large language models by aligning visual inputs with textual instructions to improve understanding and generation tasks Liu et al. (2023b). Traditionally, these instructions are built using Englishlanguage data from visual question answering and other datasets Liu et al. (2023b;a; 2024); Tong et al. (2024); Beyer et al. (2024). Researchers often supplement this with synthetic instruction tuning data, generating large volumes of instructional pairs to possibly cover multiple languages too Geigle et al. (2024). However, these instruction-tuning datasets have mostly been task-focused and lack conversational capabilities. Further, while translation gives lends to multilingual capabilities, the data remains to be culturally homogeneous. By curating multilingual and multicultural instruction tuning data across various task types, our model is designed to intuitively understand and engage with users from diverse demographics. Multilingual Multimodal LLMs. Multilingual MLLMs have evolved from dual-encoder-based models, only capable of understanding and reasoning Ni et al. (2021); Zeng et al. (2022); Jain et al. (2021), to encoder-decoder models capable of multilingual text generation as well Shan et al. (2022); Chen et al. (2023); Geigle et al. (2024). Despite their advancements, these models have remained focused on conventional tasks such as VQA and image captioning. Moreover, most efforts have centered around training with multilingual text, while little attention has been given to curating culturally diverse image datasets. Even for text, despite the focus on multilinguality, few attempts have been made to reflect cultural diversity in instructions and captions. As result, these models tend to reflect Western-centric bias. By selecting culturally diverse images from LAION and intentionally integrating this diversity into our instructions and captions, our model aims to serve wide range of users in an inclusive and equitable manner. 19 Preprint - Work in Progress"
        },
        {
            "title": "B PROMPTS USED IN THE DATA CONSTRUCTION",
            "content": "In this appendix, we will list the detailed prompts we used when constructing cultural understanding instruction tuning data described in subsection 2.2. Cultural Images LLM Scoring Prompt You are given an [Alt Text] associated with an image from the web. [Alt Text]: {Alt Text} Your goal is to: 1. Evaluate Text Quality: Rate the following alt text on scale from 1 to 5 based on its quality in describing the image, assuming the model does not have access to the image: 1 (Very Low Quality): Alt text is vague, irrelevant, misleading, or uses placeholders (e.g., file names). 2 (Low Quality): Alt text is overly simplistic, generic, or provides minimal useful information. 3 (Moderate Quality): Alt text is somewhat descriptive but lacks detail or relevance, with possible redundancy or ambiguity. 4 (High Quality): Alt text is descriptive, clear, concise, and provides sufficient information to understand the images content. 5 (Very High Quality): Alt text is highly specific, detailed, and relevant, with clear description that conveys all key aspects of the image. 2. Subject Classification: Assign subject/category to the alt text based on its content. Choose from the following categories: Vehicles and Transportation Cooking and Food People and Everyday Life Sports and Recreation Plants and Animals Objects, Materials, and Clothing Brands and Products Geography, Buildings, and Landmarks Tradition, Art, and History Public Figure and Pop-Culture Others 3. Country/Region Classification: Decide if the alt text is closely related to specific countrys culture. For example, if the alt text says, Tokyo Skytree Photo in March with beautiful cherry blossoms, its strongly related to Japan. If the alt text is not specifically about certain culture or country, you can say No specific country. Even if the alt text is written in their official language, it doesnt mean the caption is specifically about the country (e.g., product page caption is often unlikely to be country-specific). Output: Provide the final result in the following structured format: 1. Text Quality Score (1-5): 2. Subject Category: 3. Country/Region: Only generate the final result without any additional descriptions or explanations. 20 Preprint - Work in Progress Image Recaption Prompts We randomly select one recaption prompt from the following: PROMPT 1: Please describe the image in detail in {language}. The image might be related to the country: {country}. The topic might be related to: {category}. The previous short caption of the image is {text}. PROMPT 2: Analyze this image and provide comprehensive description in {language}. Consider that it may be associated with {country} and the theme could be related to {category}. If there is cultural significance, please include it. brief previous description was: {text}. PROMPT 3: In {language}, give detailed description of what you see in this image. Keep in mind it might be connected to {country} and the subject could be about {category}. If there are culturally relevant details, please include them. An earlier short description stated: {text}. PROMPT 4: Examine this image closely and describe its contents in {language} in more structured way. The image might have connection to {country} and could be about {category}. previous concise caption mentioned: {text}. PROMPT 5: Using {language}, provide an in-depth and structured description of this image. It may be related to {country} and the topic could be associated with {category}. prior brief description was given as: {text}. 21 Preprint - Work in Progress Instruction Generation Prompt Task: Generate two instruction-response pair based on the visual content of an image. Choose two task types from the list below to guide the rewriting process: Coding & Debugging Information Seeking Creative Writing Critical Reasoning Planning & Strategy Mathematical Thinking Text Revision & Editing Data Analysis Role Playing & Scenarios Brainstorming & Ideation Advice Seeking & Problem-Solving Learning & Understanding Cultural Interpretation Guidelines: Instruction: Select two different task types from the list above. Make sure the instruction prompts an interpretation or analysis directly tied to what can be visually observed in the image, not just general reasoning. The instruction should require response that uses details from the image. Avoid generic instructions that can be answered without visual information. Response: Provide very detailed and structured response that reflects clear understanding of the implied visual information. Offer multiple perspectives, deep analysis, or step-by-step explanations where applicable. Avoid general responses that could be inferred without observing the image. Responses must rely on interpreting the visual content. Content: Instructions should be varied, challenging, and explore different advanced aspects of the visual scene. Responses must showcase deep understanding of the images visual context, using thoughtful insights where applicable. Output: Provide the output in JSON format with three keys: task type, instruction and response. Ensure the instruction and response do not mention based on caption but instead, refer to the image or simply avoid reference to any external description. Do not include additional text or explanations beyond what is required. Provide both the instruction and response in {language} but task type in English. Caption: {caption} 22 Preprint - Work in Progress RECAPTIONING EXAMPLE FROM LAION-CULTURAL Figure 9: An example from LAION-Cultural illustrating why the filtered informative alt text helps generate more informative caption. With the high-quality alt text, the model incorporates important details like President and CEO of The Walt Disney Company standing in front of model of Shanghai Disneyland into the generated caption. 23 Preprint - Work in Progress"
        },
        {
            "title": "D DATASETS USED IN PANGEABENCH",
            "content": "To comprehensively assess the capabilities of PANGEA across diverse languages, cultures, and task types, we developed PANGEABENCH. We list the details of each dataset included in the PANGEABENCH. D.1 MULTIMODAL DATASETS xGQA (Pfeiffer et al., 2022a): cross-lingual visual question-answering dataset featuring 9,666 questions in eight languages covering five scripts. The dataset includes 300 unique images from Visual Genome (Krishna et al., 2017). xGQA tests the models ability to understand and reason about visual content across multiple languages. MaXM (Changpinyo et al., 2022): VQA dataset in seven languages and five scripts, with questions and answers in the same language. Images are culturally matched to the target language regions. MaXM specifically addresses the challenge of cultural diversity in multimodal understanding. MaRVL (Liu et al., 2021): Multicultural Reasoning over Vision and Language dataset in five languages and three scripts, featuring 4,914 culturally diverse images matched to respective languages. MaRVL focuses on evaluating models ability to reason about culturally diverse visual concepts. XM100 (Thapliyal et al., 2022): We create subset of 3600 instances (100 instances per language) from the original XM100 dataset, large multilingual image captioning dataset comprising 36 languages, with 261,375 captions for 100 unique images per language, culturally matched to each language. XM100 evaluates models ability to generate culturally appropriate captions across wide range of languages. For sampling, we select 100 instances per language, ensuring that all languages share the same set of images for their respective 100 instances. To ensure diversity within our sample, we use Sentence-BERT (Reimers & Gurevych, 2019) to cluster the 3600 English instances from the original dataset into 100 groups, and then select one instance from each group. This method ensures that the sampled instances are as diverse as possible. We evaluate models on this new sample of 3600 instances, which allows for more time-efficient evaluation while still accurately reflecting the multilingual capabilities of models in diverse contexts. M3Exam (Zhang et al., 2023): novel benchmark sourced from real and official human exam questions, featuring 12,317 questions in 9 languages across three educational levels. Approximately 23% of the questions require image processing. M3Exam tests the models ability to handle complex, multi-step reasoning tasks in an educational context. xMMMU: MMMU contains multimodal questions from college-level materials across six disciplines and 30 subjects. The dataset features 183 subfields and 30 diverse image types, including charts, diagrams, and chemical structures. We sample 300 questions from the original MMMU validation set and translate them using GPT-4o into xx languages. To ensure the quality, we translated each sampled question multiple times and then back-translated it to English. We select the translation with the highest BLEU score. xMMMU evaluates the models capacity to understand and reason about specialized academic content across languages and modalities. D.2 TEXT-ONLY MULTILINGUAL DATASETS TyDiQA (Clark et al., 2020): question answering dataset covering 11 typologically diverse languages with 204K question-answer pairs. Questions are written by native speakers without seeing the answers, ensuring realistic information-seeking task. TyDiQA is designed to test linguistic diversity and avoid translation artifacts. FLORES (NLLB Team, 2024): machine translation benchmark for 200 languages, including many low-resource languages. It consists of 3,001 sentences from 842 web articles, divided into dev, devtest, and test splits. FLORES-200 includes translations from multiple pivot languages and provides script alternatives for some languages, making it comprehensive test of translation capabilities. MMMLU (OpenAI, 2024a): human-translated version of MMLU (Hendrycks et al., 2021), covering 57 subjects across STEM, humanities, social sciences, and more. It ranges in difficulty 24 Preprint - Work in Progress from elementary to advanced professional levels, testing both world knowledge and problemsolving ability in zero-shot and few-shot setting across multiple languages. MGSM (Shi et al., 2022): Multilingual Grade School Math Benchmark, featuring 250 gradeschool math problems translated into 10 languages. Based on GSM8K, it requires multi-step reasoning and tests the models ability to solve complex mathematical word problems across languages. This diverse set of datasets in PANGEABENCH allows for comprehensive evaluation of PANGEAs capabilities across various languages, cultures, modalities, and task types, providing holistic assessment of its performance in multilingual and multimodal contexts. Preprint - Work in Progress"
        },
        {
            "title": "E EXPLANATION OF XCHATBENCH",
            "content": "divide art explanation, Task Category We first bar chart interpretation, ocr, image humor understanding, science figure explanation, unusual images. graph interpretation, The task categories are inspired by existing papers that do not use free-form generation format (Lu et al., 2024; Yue et al., 2024a; Han et al., 2023; Hessel et al., 2023; Kim et al., 2022). task figurative speech explanation, 10 defeasible reasoning, categories, namely iq test, into Construction Procedure To annotate the instances, we mainly follow the procedure of Kim et al. (2024a). Two human annotators first hand-crafted the instances by searching through appropriate images for the task and then hand-crafting each component of the instance. As our motivation for fine-grained evaluation, each instance consists of not only an instruction, reference answer, but also unique evaluation criteria tailored to each instance (e.g., Does the response effectively explain the humor in the image based on the juxtaposition of characters portrayal in different scenarios?) and description for each score between 1 and 5 (e.g., score4 description: The response understands the juxtaposition and relates it to the humor involving machine learning models, but may miss some nuances or the related aspect of the humor). During the annotation process, we asked the annotators to not copy-and-paste results from LLM services like ChatGPT or directly from the web. Then, we hire four additional annotators to assess the quality of the instances. Each participant to asked to grade if each instance (1) fits into the devised task category, (2) if the quality of the reference answer is good enough, and (3) if the score rubric is suitable to assess the response. We iteratively ask the annotators who made the instances to revise them if the instance does not satisfy all three criteria. The resulting dataset consists of 50 instructions, reference answers, and evaluation criteria with corresponding score rubric. Translation Procedure To assess the multilingual generation capabilities of MLLMs, we translate the hand-crafted 50 instances into 6 different languages, namely Chinese, Hindi, Indonesian, Japanese, Korean, and Spanish. We first use GPT-4o-2024-08-06 to translate the instruction and reference answer of each instance with naive prompt, Translate the following sentences into {target language}. Sentences: {sentences}. Then, the coauthors who are native speakers of each language reviewed the instances and made adjustments if the translated results were unnatural. Evaluation Pipeline Similar to prior works employing LLM-as-a-Judge, we use GPT-4o-202408-06 as the judge model and prompt it in direct assessment manner. As input, the judge model is given the instruction, the models response, the reference answer, the evaluation criteria, and the descriptions for each score. As output, the judge generates verbal feedback and an integer score between 1 and 5. For this procedure, we use the prometheus-eval library (Kim et al., 2024b) and employ their default hyperparameter setting for evaluation. Lastly, the final score is acquired by averaging the results across the 50 instances for each language. Note that in the main result and breakdown result tables, we normalize the score from 1-5 to 0-100 by (score 1) 25. Also, for postprocessing, we use langdetect 3 to identify whether the response is written in the given language and change the score to 1 when it is written in different language, phenomenon called language hallucination (Xue et al., 2021; Pfeiffer et al., 2023). 3https://pypi.org/project/langdetect/ 26 Preprint - Work in Progress"
        },
        {
            "title": "F QUALITATIVE EXAMPLES FROM XCHATBENCH",
            "content": "We show some interesting qualitative examples from xChatBench, including good cases in Figure 10, Figure 11, Figure 12, Figure 13, Figure 14, Figure 15, as well as bad cases in Figure 16, Figure 17. The examples included the scoring rubric, query, response from our PANGEA, reference answer, and LLM-as-Judge feedback. Figure 10: An xChat example from Japanese subset and our model PANGEAs response. The English part of the instruction, response, and reference answer is additionally added only on the figure to help to understand and was not given nor generated by PANGEA. 27 Preprint - Work in Progress Figure 11: An xChat example from Hindi subset and our model PANGEAs response. The English part of the instruction, response, and reference answer is additionally added only on the figure to help to understand and was not given nor generated by PANGEA. Preprint - Work in Progress Figure 12: An xChat example from the Korean subset and our model PANGEAs response. The English part of the instruction, response, and reference answer is additionally added only on the figure to help to understand and was not given nor generated by PANGEA. 29 Preprint - Work in Progress Figure 13: An xChat example from the Indonesian subset and our model PANGEAs response. The English part of the instruction, response, and reference answer is additionally added only on the figure to help to understand and was not given nor generated by PANGEA. Preprint - Work in Progress Figure 14: An xChat example from Spanish subset and our model PANGEAs response. The English part of the instruction, response, and reference answer is additionally added only on the figure to help to understand and was not given nor generated by PANGEA. 31 Preprint - Work in Progress Figure 15: An xChat example from the Chinese subset and our model PANGEAs response. The English part of the instruction, response, and reference answer is additionally added only on the figure to help to understand and was not given nor generated by PANGEA. Preprint - Work in Progress Figure 16: An xChat example from the Chinese subset and our model PANGEAs response. The English part of the instruction, response, and reference answer is additionally added only on the figure to help to understand and was not given nor generated by PANGEA. 33 Preprint - Work in Progress Figure 17: An xChat example from the Korean subset and our model PANGEAs response. The English part of the instruction, response, and reference answer is additionally added only on the figure to help to understand and was not given nor generated by PANGEA. Preprint - Work in Progress"
        },
        {
            "title": "G TRAINING EXAMPLES",
            "content": "G.1 MACHINE TRANSLATED INSTRUCTIONS We include few machine-translated training examples from PANGEAINS in Figure 18, Figure 19, Figure 20, Figure 21, Figure 22, Figure 23. G.2 MULTICULTURAL UNDERSTANDING INSTRUCTIONS We include few multicultural understanding instructions from PANGEAINS in Figure 24, Figure 25, Figure 26, Figure 27. 35 Preprint - Work in Progress Figure 18: Translated Task: An example from the Cambrian dataset where it discusses the concept of hourly wages based on given prompt in Russian. Figure 19: Multimodal Translated Task: An example from the ALLaVa-LAION dataset where the GPT model answers prompt in Turkish regarding the maximum time displayed on digital timer. 36 Preprint - Work in Progress Figure 20: Multimodal Translated Task: An example from the ShareGPT-4v dataset where the model describes an image of wine rack in Thai, detailing its structure and the arrangement of wine bottles. Figure 21: Text-only Translated Task: An example from the OpenHermes2.5 dataset translated into Japanese. 37 Preprint - Work in Progress Figure 22: Text-only Translated Task: An example from the NumininaMath dataset translated into Spanish. 38 Preprint - Work in Progress Figure 23: Text-only Translated Task: An example from the Code-Feedback dataset translated into German. 39 Preprint - Work in Progress Figure 24: Multicultural Understanding: An example from the LAION-Culture dataset where the GPT model describes and analyzes the Petrobras logo in Portuguese. 40 Preprint - Work in Progress Figure 25: Multicultural Understanding: Example from the LAION-Culture dataset where the model interprets the meaning of Japanese character and its representation in calligraphy, described in Bulgarian. 41 Preprint - Work in Progress Figure 26: Multicultural Understanding: LAION-Culture sample in Hebrew. 42 Preprint - Work in Progress Figure 27: Multicultural Understanding: LAION-Culture sample in Tamil. 43 Preprint - Work in Progress"
        },
        {
            "title": "H BREAKDOWN RESULTS OF DIFFERENT LANGUAGES ON PANGEABENCH",
            "content": "H.1 XCHAT We show the performance of different models performance on the xChat benchmark in Table 3. Models Gemini-1.5-Pro GPT4o Llava-1.5-7B Llava-Next-7B Phi-3.5-Vision Cambrian-8B Llava-OV-7B Molmo-7B-D Llama3.2-11B PaliGemma-3B PALO-7B mBLIP mT0-XL mBLIP BLOOMZ-7B PANGEA-7B (Ours) English Multi Spanish Hindi Indonesian Japanese Korean Chinese 71.0 67.0 22.5 40.5 38.5 27.5 51.0 49.5 49.0 6.0 27.0 2.5 4. 46.0 65.6 65.1 16.7 20.4 21.1 15.8 33.1 34.7 31.3 3.8 16.2 0.5 1.7 35.8 66.0 66.0 22.5 33.0 37.0 22.5 45.5 45.0 42.5 4.5 23.0 0.0 2. 43.5 62.0 64.0 3.5 1.5 11.5 4.0 6.5 19.5 19.5 0.5 3.0 0.0 2.5 23.5 65.5 65.0 18.0 19.0 10.5 20.0 42.0 36.5 45.0 6.5 19.0 0.5 2. 34.5 68.0 66.5 23.0 25.0 31.0 20.0 36.5 36.0 26.0 6.5 20.0 2.0 0.0 39.0 66.5 67.5 12.0 15.0 12.5 10.5 26.0 35.0 21.0 2.0 13.5 0.5 0. 33.5 65.5 61.5 21.0 29.0 24.0 18.0 42.0 46.0 43.0 3.0 18.5 0.0 3.0 40.5 Table 3: Comparison of models on the xChat dataset across different languages. H.2 MULTILINGUAL LLAVABENCH We show the performance of different models performance on the Multilingual LLaVABench benchmark in Table 4. Models Gemini-1.5-Pro GPT4o Llava-1.5-7B Llava-Next-7B Phi-3.5-Vision Cambrian-8B Llava-OV-7B Molmo-7B-D Llama3.2-11B PaliGemma-3B PALO-7B mBLIP mT0-XL mBLIP BLOOMZ-7B PANGEA-7B (Ours) English Multi Arabic Bengali Chinese French Hindi Japanese Russian Spanish Urdu 103.4 104.6 106.6 100.4 112.9 98.3 117.1 111. 104.1 96.5 115.5 101.1 106.2 99.7 118.1 104.0 66.1 78.9 70.8 78.4 89.7 95.9 93.9 32.1 68.9 32.7 43.5 84. 40.8 50.7 58.0 61.8 55.3 13.8 58.2 31.9 71.2 28.2 41.0 89.5 26.4 24.9 50.1 54.1 45.5 10.1 39.4 37.3 79.1 33.7 48.1 91.0 11.9 11.2 35.1 35.4 33.8 4.2 48.1 38.2 54.6 26.2 44.1 94. 50.7 72.8 69.2 80.9 90.0 0.3 47.2 29.1 71.5 3.6 30.6 94.4 63.8 91.4 86.0 87.3 89.4 59.6 85.6 30.0 83.9 39.8 53.3 93.8 23.2 18.0 35.9 44.2 35.3 5.5 67.8 35.8 61.9 26.9 39.1 84. 70.0 70.1 63.0 64.4 70.3 6.0 53.7 33.4 66.6 26.8 29.8 92.8 95.7 88.5 46.5 71.8 67.6 76.4 44.7 8.7 68.5 26.1 80.9 34.1 38.1 91.2 88.2 100. 101.6 102.5 59.2 82.9 75.6 90.3 75.5 29.5 77.8 32.3 74.4 36.9 51.5 87.4 15.4 13.4 39.3 23.3 13.3 0.0 35.3 25.1 68.2 26.0 34.0 75.5 Table 4: Comparison of models on the Multilingual LLaVABench benchmark across different languages. H.3 CVQA We show the performance of different models performance on the CVQA benchmark in Table 5 and Table 6. H.4 MARVL We show the performance of different models performance on the MaRVL benchmark in Table 7. H.5 XM100 We show the performance of different models performance on the XM100 benchmark in Table 8. 44 Preprint - Work in Progress Models ar-es br-pt bu-bg ch-es ch-zh co-es ec-es eg-ar et-am et-or Llava-1.5-7B Llava-Next-7B Phi-3.5-Vision Cambrian-8B Llava-OV-7B Molmo-7B-D Llama3.2-11B PaliGemma-3B PALO-7B mBLIP BLOOMZ-7B mBLIP mT0-XL PANGEA-7B (Ours) 37.8 52.5 54.0 59.6 64.5 61.1 69.1 48.7 50.9 45.3 40.8 68.3 51.1 62.3 57.2 60.6 69.7 69.0 74.6 53.9 56.7 51.4 44.4 72.9 35.6 41.5 36.9 42.0 49.6 54.9 64.2 39.1 36.7 30.5 38.0 53.9 42.4 59.0 57.7 64.5 67.1 60.7 70.5 53.4 55.1 45.3 44.9 70.5 44.4 51.1 51.1 59.5 69.1 66.2 73.6 53.7 45.3 51.1 39.9 74. 50.6 54.8 52.3 57.7 66.8 58.5 69.3 50.6 48.5 46.9 41.9 64.7 48.6 50.8 50.1 56.1 65.5 54.9 66.9 45.3 46.4 44.8 42.5 63.5 31.5 33.5 38.4 40.9 47.8 56.7 68.5 40.4 28.6 35.9 31.0 49.3 Models fr-br in-bn in-ta in-te ind-id ind-jv ind-mi ind-sv Llava-1.5-7B Llava-Next-7B Phi-3.5-Vision Cambrian-8B Llava-OV-7B Molmo-7B-D Llama3.2-11B PaliGemma-3B PALO-7B mBLIP BLOOMZ-7B mBLIP mT0-XL PANGEA-7B (Ours) 29.4 27.4 29.3 31.6 34.3 44.2 49.4 29.9 29.1 26.7 23.5 34.6 31.1 31.1 39.0 47.2 56.3 61.9 76.9 46.2 37.8 41.9 36.4 59.1 29.8 28.8 40.0 38.1 43.9 61.2 80.4 46.0 31.2 40.0 44.2 51.9 28.0 28.0 36.8 44.0 46.5 58.5 80.5 43.5 25.0 42.0 39.0 54.5 41.7 42.2 45.0 50.2 58.0 52.9 65.8 45.4 41.3 41.9 37.4 62. 32.0 38.7 38.2 43.8 45.8 53.9 60.6 41.4 32.3 35.4 37.4 49.5 32.7 40.2 38.2 39.4 45.4 54.6 68.9 39.8 32.3 35.1 34.7 47.8 33.5 35.5 30.8 45.5 40.5 55.0 64.0 33.0 32.0 32.0 31.0 53.0 27.8 29.5 27.8 27.8 32.5 58.1 68.4 24.8 19.2 23.9 35.9 36.3 ir-ir 42.6 42.6 39.6 47.9 50.6 64.4 76.4 34.4 42.9 29.4 35.3 56. 31.8 36.9 32.2 25.7 41.1 60.7 63.1 28.0 32.7 25.7 26.6 35.5 ja-jp 37.4 32.5 39.7 40.9 49.8 42.9 54.2 43.3 30.5 31.0 30.0 48.3 Models ke-sw ma-my me-es mo-mg ni-ig no-ng pk-ur ph-fi ro-ro ru-ru 43.8 46.8 41.4 47.3 56.2 57.6 70.0 39.9 39.4 31.5 32.0 58. 47.0 52.3 47.4 52.0 60.3 63.6 76.8 50.3 46.0 46.0 43.7 64.6 51.0 53.5 50.5 61.5 75.5 61.5 74.5 53.5 47.0 34.0 42.0 74.0 Llava-1.5-7B Llava-Next-7B Phi-3.5-Vision Cambrian-8B Llava-OV-7B Molmo-7B-D Llama3.2-11B PaliGemma-3B PALO-7B mBLIP BLOOMZ-7B mBLIP mT0-XL PANGEA-7B (Ours) 34.4 46.2 46.0 50.5 46.5 73.3 79.1 44.0 35.9 37.0 45.1 64.1 42.2 45.7 45.1 52.1 55.6 54.6 72.1 44.1 42.5 42.5 40.6 59.7 42.4 51.4 46.3 56.7 59.4 53.6 66.6 47.4 44.3 44.8 44.9 62. 26.9 33.3 31.9 34.6 35.9 51.9 54.5 29.2 28.8 28.8 29.2 42.3 Models rw-ki sg-zh sk-ko sp-es Llava-1.5-7B Llava-Next-7B Phi-3.5-Vision Cambrian-8B Llava-OV-7B Molmo-7B-D Llama3.2-11B PaliGemma-3B PALO-7B mBLIP BLOOMZ-7B mBLIP mT0-XL PANGEA-7B (Ours) 31.1 34.5 31.1 31.9 35.3 57.4 57.9 27.2 28.9 29.4 33.2 35.7 44.3 44.8 43.9 54.7 70.3 69.3 80.7 48.6 45.8 47.6 36.8 65.6 44.5 43.4 55.2 54.5 65.2 65.2 73.8 61.0 44.5 33.1 38.3 70.7 56.9 63.5 62.4 70.4 79.9 70.1 81.4 60.1 64.8 56.6 53.5 72.6 34.5 35.0 33.3 36.0 33.5 53.0 61.5 32.0 29.5 33.0 30.5 46. sr-si 24.9 29.8 28.0 36.4 31.6 68.0 72.4 31.6 28.0 28.0 31.1 39.1 47.5 56.9 50.0 53.5 62.5 54.8 66.9 52.2 49.2 49.2 42.8 64.5 26.4 36.6 35.2 48.6 58.3 67.1 78.7 44.9 44.4 47.7 40.3 66.2 ur-es macro 37.8 41.0 43.3 45.7 47.3 50.8 52.4 39.4 39.4 39.4 39.1 49. 38.7 42.6 42.4 47.5 53.8 59.4 70.1 43.0 39.3 36.9 37.6 57.2 Table 5: Comparison of models on CVQA across different country-language pairs (in local languages). Includes Macro-Acc. H.6 XGQA We show the performance of different models performance on the xGQA benchmark in Table 9. H.7 MAXM We show the performance of different models performance on the MAXM benchmark in Table 10. 45 Preprint - Work in Progress Models ar-es br-pt bu-bg ch-es ch-zh co-es ec-es eg-ar et-am et-or Llava-1.5-7B Llava-Next-7B Phi-3.5-Vision Cambrian-8B Llava-OV-7B Molmo-7B-D Llama3.2-11B PaliGemma-3B PALO-7B mBLIP mT0-XL mBLIP BLOOMZ-7B PANGEA-7B (Ours) 56.2 53.9 59.2 57.7 63.0 57.7 66.8 51.7 50.2 38.1 46.0 67.2 61.6 61.3 61.9 66.5 73.9 65.8 72.9 59.5 57.0 45.4 51.4 72.9 52.3 50.9 54.9 56.1 59.3 45.6 54.4 49.3 48.8 39.1 41.5 60.1 60.2 59.8 64.1 65.4 65.8 63.7 72.6 51.7 53.4 42.7 44.4 68.8 54.0 58.8 58.2 64.3 68.8 68.5 72.0 54.9 52.1 43.7 48.9 67. 55.6 60.2 59.3 59.3 65.1 57.3 66.4 54.8 51.9 41.1 49.0 64.7 55.5 52.8 57.5 60.2 63.3 55.0 65.2 47.2 53.0 40.9 45.0 61.6 50.2 54.7 50.7 56.7 62.1 43.8 56.7 51.2 48.3 42.9 45.3 59.1 Models fr-br in-bn in-ta in-te ind-id ind-jv ind-mi ind-sv Llava-1.5-7B Llava-Next-7B Phi-3.5-Vision Cambrian-8B Llava-OV-7B Molmo-7B-D Llama3.2-11B PaliGemma-3B PALO-7B mBLIP mT0-XL mBLIP BLOOMZ-7B PANGEA-7B (Ours) 37.3 37.5 41.7 40.7 44.2 29.6 36.3 37.3 36.8 30.4 34.6 45.2 52.1 60.8 58.7 68.5 69.6 47.9 62.9 59.1 52.4 43.0 43.4 67.1 61.4 61.4 60.5 65.6 72.0 36.4 66.4 66.0 53.5 46.0 52.6 71.0 63.5 60.5 60.0 63.0 70.5 41.5 66.5 62.5 56.5 41.0 49.5 68.0 47.8 48.5 51.7 55.1 59.0 50.5 63.6 49.3 45.1 38.1 41.0 60. 50.8 48.1 45.5 50.2 55.9 45.1 48.8 48.1 45.8 39.1 44.8 57.2 49.0 51.4 51.4 58.2 59.4 43.4 58.2 43.4 44.2 38.6 38.2 56.9 44.0 49.0 47.5 56.0 58.5 39.5 54.0 46.0 42.0 32.5 30.5 56.0 51.3 52.9 54.7 60.3 59.8 31.6 41.9 52.6 47.0 34.2 38.9 60.7 ir-ir 61.3 66.6 62.6 66.6 76.4 43.6 57.4 58.3 55.6 37.4 42.3 72. 53.3 58.9 58.4 56.5 59.3 38.8 32.2 51.4 52.3 42.1 46.3 56.0 ja-jp 41.9 40.9 41.4 42.4 47.3 44.8 58.1 44.8 37.4 34.0 36.5 45.8 Models ke-sw ma-my me-es mo-mg ni-ig no-ng pk-ur ph-fi ro-ro ru-ru 52.7 56.7 57.6 60.1 64.0 43.3 66.0 48.8 52.7 36.9 36.9 64. 55.6 62.6 61.9 66.6 72.5 52.0 75.5 60.9 55.0 43.7 50.3 71.9 59.0 58.5 58.5 61.5 72.5 63.5 74.5 56.0 53.5 41.0 44.0 68.5 Llava-1.5-7B Llava-Next-7B Phi-3.5-Vision Cambrian-8B Llava-OV-7B Molmo-7B-D Llama3.2-11B PaliGemma-3B PALO-7B mBLIP mT0-XL mBLIP BLOOMZ-7B PANGEA-7B (Ours) 68.9 71.1 72.9 74.4 79.1 47.6 61.5 59.7 65.9 50.2 54.6 77.2 52.1 54.9 57.1 61.9 65.1 51.7 69.2 54.9 49.2 41.6 45.7 62.5 47.9 51.1 46.3 56.7 63.2 55.1 64.7 51.7 53.4 34.7 39.3 61. 45.8 44.2 50.7 48.7 52.6 35.9 41.0 43.4 42.9 33.9 38.1 52.9 Models rw-ki sg-zh sk-ko sp-es Llava-1.5-7B Llava-Next-7B Phi-3.5-Vision Cambrian-8B Llava-OV-7B Molmo-7B-D Llama3.2-11B PaliGemma-3B PALO-7B mBLIP mT0-XL mBLIP BLOOMZ-7B PANGEA-7B (Ours) 51.1 52.8 52.3 56.2 55.7 34.9 40.4 44.7 51.9 38.3 45.1 56.6 60.8 62.3 59.4 66.0 73.6 66.0 73.6 59.4 56.1 43.9 53.8 71.7 56.9 60.0 66.5 63.1 67.9 56.9 73.1 58.3 55.9 41.4 46.9 66.6 66.0 67.6 66.7 71.7 80.2 66.7 83.3 61.0 62.9 51.9 58.5 75.2 51.0 53.0 53.0 56.5 57.5 36.0 39.5 46.0 49.0 39.5 45.0 59. sr-si 58.7 59.1 61.3 63.1 72.9 31.6 51.1 62.2 54.2 48.0 46.7 70.6 58.5 57.2 56.2 60.5 64.2 49.2 65.9 55.2 54.5 43.1 47.2 64.9 63.9 67.1 60.6 73.1 75.0 46.8 65.7 67.6 60.6 45.4 60.6 72.2 ur-es macro 42.5 38.7 46.3 47.0 48.9 44.8 56.2 40.6 42.2 34.9 34.0 52. 54.2 55.7 56.3 59.7 65.2 48.3 61.2 52.9 50.9 40.5 44.9 64.4 Table 6: Comparison of models on CVQA across different country-language pairs (in English). Includes Macro-Acc. 46 Preprint - Work in Progress Models English Multi Indonesian Swahili Tamil Turkish Chinese GPT4o Gemini-1.5-Pro Llava-1.5-7B Llava-Next-7B Phi-3.5-Vision Cambrian-8B Llava-OV-7B Molmo-7B-D Llama3.2-11B PaliGemma-3b PALO-7B mBLIP mT0-XL mBLIP BLOOMZ-7B PANGEA-7B 81.8 76.4 56.2 62.8 72.1 75.4 72.7 65.3 64.5 56.5 63.3 67.3 62.3 87.0 82.3 72.0 53.7 50.9 56.5 61.8 57.5 54.9 58.1 52.2 54.2 66.7 58.6 79. 81.9 71.2 56.1 52.2 58.6 64.7 60.9 61.1 62.7 53.4 58.3 64.9 59.1 81.3 80.8 67.8 49.8 50.6 51.4 53.6 51.2 49.6 52.4 49.6 50.6 64.8 56.2 75. 80.2 70.0 49.7 50.5 52.0 56.7 51.9 49.6 54.0 50.5 51.9 69.7 60.3 69.4 86.4 75.4 55.4 50.4 58.6 65.2 63.5 52.2 61.6 56.3 54.9 68.1 57.7 84. 82.1 75.8 57.5 50.6 61.7 68.9 60.0 62.2 59.5 51.3 55.3 65.9 59.7 84.3 Table 7: Comparison of models on the MaRVL dataset across different languages. H.8 XMMMU We show the performance of different models performance on the xMMMU benchmark in Table 11. H.9 M3EXAM We show the performance of different models performance on the M3Exam benchmark in Table 12. H.10 TYDIQA We show the performance of different models performance on the TyDiQA benchmark in Table 13. H.11 XSTORYCLOZE We show the performance of different models performance on the XStoryCloze benchmark in Table 14. H.12 MGSM We show the performance of different models performance on the MGSM benchmark in Table 15. H.13 MMMLU We show the performance of different models performance on the MMMLU benchmark in Table 16. 47 Preprint - Work in Progress"
        },
        {
            "title": "INSTRUCTIONS",
            "content": "Optical Character Recognition (OCR) is critical capability for multimodal LLMs, enabling them to interpret and process textual information embedded within images. However, most existing OCR training datasets are predominantly English-centric, which limits the models performance in nonEnglish contexts. To address this gap, we have curated comprehensive set of 500K multilingual OCR training samples from web user interfaces. We utilize URLs from the CC-News-Multilingual4dataset (Hamborg et al., 2017) to obtain diverse set of multilingual web pages. Using Playwright5, we render each website and automatically capture screenshots under various device settings and resolutions to achieve wide range of image dimensions and aspect ratios. Each screenshot includes red bounding box that highlights specific element targeted for OCR extraction. We focus on ten languages for this dataset: English, Chinese, Japanese, Korean, Indonesian, Hindi, Spanish, French, Portuguese, and Arabic. We totally have 1M samples (50K for each language). 4https://huggingface.co/datasets/intfloat/multilingual cc news 5https://github.com/microsoft/playwright 48 Preprint - Work in Progress Models English Multi Arabic Bengali Czech Danish German Greek Gemini-1.5-Pro GPT4o Llava-1.5-7B Llava-Next-7B Phi-3.5-Vision Cambrian-8B Llava-OV-7B Molmo-7B-D Llama3.2-11B PaliGemma-3B PALO-7B mBLIP mT0-XL mBLIP BLOOMZ PANGEA-7B (Ours) 27.6 27.7 28.6 29.3 30.2 20.6 30.6 22.1 27.6 18.7 30.4 31.9 22.5 30.8 19.1 19.1 1.1 9.4 5.2 9.9 7.0 9.1 4.5 0.8 0.8 3.1 10.3 14.2 1.7 15.8 0.0 5.6 0.4 1.4 0.2 5.4 0.0 0.0 0.0 3.2 9.5 18.1 7.5 13.5 0.0 0.1 2.4 6.6 0.6 7.9 0.0 0.0 0.0 1.6 6.4 16.4 25.9 21.1 2.1 12.1 16.6 7.4 5.2 5.7 1.5 1.1 2.0 3.7 11.5 16. 32.8 25.3 1.0 15.7 16.2 15.1 16.8 13.8 11.8 3.1 1.0 2.1 15.9 20.7 27.6 19.3 3.1 14.4 0.0 15.5 14.0 12.2 4.6 2.7 2.7 2.9 14.5 20.6 5.0 21.1 0.0 4.2 20.7 4.4 0.4 4.2 1.2 0.0 0.0 3.1 10.9 11.2 Models Spanish Persian Finnish Filipino French Hebrew Hindi Croatian Gemini-1.5-Pro GPT4o Llava-1.5-7B Llava-Next-7B Phi-3.5-Vision Cambrian-8B Llava-OV-7B Molmo-7B-D Llama3.2-11B PaliGemma-3B PALO-7B mBLIP mT0-XL mBLIP BLOOMZ PANGEA-7B (Ours) 39.5 28.3 3.7 23.6 20.7 18.6 24.9 19.8 10.2 0.7 1.5 8.3 18.9 26.2 4.2 26.6 0.0 9.4 0.0 9.6 3.8 11.3 0.0 0.0 0.0 5.5 13.8 19.3 29.0 13.1 0.4 5.5 1.0 5.1 1.5 3.1 2.4 0.1 0.4 1.7 4.8 3.8 28.7 26.4 1.1 9.3 1.7 19.6 4.2 13.0 8.4 0.1 0.9 2.8 7.7 18.9 42.4 23.1 2.0 23.0 21.2 18.3 22.0 19.8 12.0 0.6 2.1 6.4 19.1 26.7 4.3 20.4 0.1 2.7 0.3 5.8 0.0 8.3 0.0 0.0 0.0 4.0 7.5 18. 2.2 17.0 0.0 10.2 0.0 6.8 4.4 9.4 0.2 0.0 0.0 1.8 10.1 17.4 33.8 19.4 0.3 7.5 0.5 7.2 7.2 6.9 0.7 1.3 0.2 0.9 3.2 10.8 Models Hungarian Indonesian Italian Japanese Korean Maori Dutch Norwegian Gemini-1.5-Pro GPT4o Llava-1.5-7B Llava-Next-7B Phi-3.5-Vision Cambrian-8B Llava-OV-7B Molmo-7B-D Llama3.2-11B PaliGemma-3B PALO-7B mBLIP mT0-XL mBLIP BLOOMZ PANGEA-7B (Ours) 37.2 21.8 3.3 9.3 3.4 6.6 3.6 3.5 12.7 2.0 3.4 2.8 11.8 7. 55.4 28.4 0.9 14.7 3.2 15.7 16.4 17.2 1.2 0.2 1.1 6.0 16.0 27.9 27.6 21.0 4.3 17.6 17.5 15.5 12.8 17.8 16.0 1.8 3.2 2.8 16.5 22.9 1.2 0.0 0.0 4.2 1.6 7.2 0.6 5.2 0.0 0.0 0.0 0.3 0.0 2.1 8.2 11.1 0.0 5.2 0.3 2.0 0.0 2.4 0.0 0.0 0.0 2.1 4.5 8.1 3.8 26.8 0.2 9.2 0.2 3.2 1.7 7.5 9.3 4.0 0.1 1.5 0.1 0.7 27.7 26.4 2.9 23.8 17.2 20.3 24.7 15.7 22.0 2.6 3.5 3.4 18.2 26. 36.7 24.7 3.7 16.3 14.1 16.0 13.9 13.8 1.1 2.3 0.7 3.1 14.5 24.9 Models Polish Portuguese Quechua Romanian Russian Swedish Swahili Telugu Gemini-1.5-Pro GPT4o Llava-1.5-7B Llava-Next-7B Phi-3.5-Vision Cambrian-8B Llava-OV-7B Molmo-7B-D Llama3.2-11B PaliGemma-3B PALO-7B mBLIP mT0-XL mBLIP BLOOMZ PANGEA-7B (Ours) 35.5 22.2 0.8 13.5 1.0 9.3 7.4 8.2 1.0 0.9 0.8 3.5 11.8 16.2 35.7 28.0 2.5 21.3 21.0 17.5 24.6 16.2 18.6 1.3 1.7 5.8 16.5 28.1 0.7 4.4 0.0 0.0 0.4 0.0 0.0 0.6 0.0 0.1 0.0 0.2 0.1 0. 31.2 19.1 1.6 11.5 3.2 13.4 6.8 11.6 10.1 0.8 1.1 2.3 13.7 21.4 32.4 20.7 0.5 13.5 0.7 11.3 5.5 12.3 0.6 0.0 0.5 3.1 14.5 20.9 37.8 26.0 2.0 16.0 12.5 17.9 15.0 14.1 7.4 2.0 0.9 3.7 14.5 19.4 10.7 20.0 0.1 3.2 0.4 3.7 2.0 3.8 5.8 0.0 0.2 3.8 8.4 18.7 0.0 12.5 0.0 0.0 0.0 2.3 0.0 0.4 0.0 0.0 0.0 2.7 3.0 0.1 Models Thai Turkish Ukrainian Vietnamese Chinese Gemini-1.5-Pro GPT4o Llava-1.5-7B Llava-Next-7B Phi-3.5-Vision Cambrian-8B Llava-OV-7B Molmo-7B-D Llama3.2-11B PaliGemma-3B PALO-7B mBLIP mT0-XL mBLIP BLOOMZ PANGEA-7B (Ours) 0.0 0.0 0.0 0.0 0.5 0.4 0.0 0.0 0.0 0.5 0.2 0.0 0.5 0.0 0.9 17.6 0.0 0.0 1.9 9.3 0.0 0.0 0.0 0.0 0.0 3.9 1.9 0. 0.0 16.9 0.0 0.3 0.0 5.9 0.0 0.0 0.0 0.0 0.0 2.0 0.0 0.3 0.0 30.9 0.0 0.0 2.2 17.8 0.0 0.0 0.0 0.2 0.1 7.1 2.2 0.0 0.9 0.4 0.0 6.3 0.0 11.3 2.9 0.0 2.9 0.0 0.0 0.0 0.0 4.9 Table 8: Comparison of models on the XM100 dataset across different languages. 49 Preprint - Work in Progress Models Gemini-1.5-Pro GPT4o Llava-1.5-7B Llava-Next-7B Phi-3.5-Vision Cambrian-8B Llava-OV-7B Molmo-7B-D Llama3.2-11B PaliGemma-3B PALO-7B mBLIP mT0-XL mBLIP BLOOMZ-7B PANGEA-7B (Ours) English Multi Bengali German Indonesian Korean Portuguese Russian Chinese 54.2 55.8 62.0 64.8 64.7 64.6 64.4 51.5 55.6 59.7 60.5 44.2 43.3 64.7 48.7 51.0 30.7 37.8 38.4 39.8 48.2 43.0 45.4 30.5 37.8 39.9 36.9 60. 49.4 49.4 15.6 11.5 7.7 32.3 41.8 25.6 42.9 13.3 42.2 39.1 37.7 58.9 50.2 52.6 28.4 41.5 51.4 44.6 49.2 45.9 46.7 44.5 39.1 41.1 36.3 61. 48.6 50.4 33.4 37.3 36.0 36.0 48.8 44.9 46.2 21.3 36.8 39.1 39.3 60.1 46.4 51.0 38.2 42.5 36.3 43.6 45.3 44.2 44.5 22.8 41.7 39.7 28.5 58. 51.2 52.2 27.5 39.8 49.6 41.6 52.4 46.5 46.5 34.7 31.7 40.7 40.7 61.8 44.8 50.0 33.1 43.5 46.2 44.2 54.0 45.6 44.7 35.8 27.0 40.2 36.6 60. 50.2 51.4 38.4 48.2 41.4 36.2 45.9 48.1 46.1 41.2 46.5 39.4 39.1 59.6 Table 9: Comparison of models on the xGQA dataset across different languages Models Gemini-1.5-Pro GPT4o Llava-1.5-7B Llava-Next-7B Phi-3.5-Vision Cambrian-8B Llava-OV-7B Molmo-7B-D Llama3.2-11B PaliGemma-3B PALO-7B mBLIP mT0-XL mBLIP BLOOMZ-7B PANGEA-7B (Ours) English Multi French Hindi Hebrew Romanian Thai Chinese 56.4 60.7 49.8 54.9 55.3 55.3 54.9 52.9 55.3 47.9 51.4 44.7 44. 48.6 63.5 65.4 20.4 21.4 25.0 28.7 34.8 37.5 43.9 19.9 16.3 36.8 24.8 34.3 60.2 59.8 32.2 33.7 38.3 41.7 37.9 45.5 48.1 8.0 33.7 36.0 33. 36.4 66.5 68.8 17.3 16.2 31.9 23.8 31.9 33.5 50.4 36.5 15.8 42.7 47.3 40.4 65.7 70.0 12.9 10.7 17.5 17.1 17.8 30.7 41.8 19.3 12.1 28.9 8. 36.4 57.4 61.3 15.1 15.5 10.9 32.0 30.2 28.9 36.6 13.4 11.3 30.3 16.9 33.1 73.9 76.5 17.2 18.3 24.3 25.7 53.0 46.3 56.7 31.3 14.6 56.3 9. 36.2 57.4 56.3 27.8 33.9 27.4 31.8 37.9 40.4 30.0 10.8 10.5 26.4 33.2 23.1 Table 10: Comparison of models on the MAXM dataset across different languages. Models English Multi Arabic French Hindi Indonesian Japanese Portuguese Gemini-1.5-Pro (0801) GPT4o (0513) Llava-1.5-7B Llava-Next-7B Phi-3.5-Vision Cambrian-8B Llava-OV-7B Molmo-7B-D Llama3.2-11B PaliGemma-3B PALO-7B mBLIP mT0-XL mBLIP BLOOMZ-7B PANGEA-7B (Ours) 65.8 69.1 36.2 36.7 42.6 41.8 46.3 42.9 39.2 26.3 33.1 29.3 29.2 45.7 57.7 58.3 31.5 34.3 38.8 33.2 41.0 40.4 34.0 25.2 30.5 30.4 30.8 43. 57.7 56.7 29.5 30.5 35.6 32.6 41.6 40.6 33.6 29.2 30.5 30.2 28.5 42.3 58.1 58.1 34.9 35.6 44.0 34.6 43.0 42.6 39.6 23.8 33.2 33.2 33.9 45. 55.5 58.1 27.5 30.9 30.9 30.9 34.7 32.6 32.3 21.6 28.9 28.2 27.8 41.6 60.2 59.9 31.6 37.0 36.7 31.3 43.4 40.7 36.7 24.2 34.0 26.9 33.3 46. 55.0 58.0 32.0 34.9 37.9 33.5 40.1 43.9 29.0 24.5 27.1 31.6 31.6 40.5 59.6 58.9 33.7 37.0 47.8 36.0 43.4 42.1 33.0 27.6 33.3 32.3 29.6 46. Table 11: Comparison of models on the xMMMU dataset across different languages. 50 Preprint - Work in Progress Models Gemini-1.5-Pro GPT4o Llava-1.5-7B Llava-Next-7B Phi-3.5-Vision Cambrian-8B Llava-OV-7B Molmo-7B-D Llama3.2-11B PaliGemma-3B PALO-7B mBLIP mT0-XL mBLIP BLOOMZ-7B PANGEA-7B (Ours) English Multi Afrikaans Chinese Italian Portuguese Thai Vietnamese 77.4 68.0 32.3 36.5 55.8 34.7 60.4 57.1 51.8 36.0 30.8 22.8 30.3 61. 64.7 61.0 29.0 28.4 37.2 33.4 45.8 39.1 36.6 25.6 27.8 25.0 29.5 42.1 80.4 73.0 28.2 28.2 44.2 36.8 50.3 35.6 42.3 26.4 31.9 16.0 28.2 52. 74.1 68.0 24.3 25.4 40.8 34.2 58.0 56.4 46.4 24.7 22.1 25.6 29.8 49.2 76.3 67.0 40.1 37.8 51.4 45.2 57.2 49.4 45.8 32.2 36.9 33.7 37.3 54. 61.8 58.0 28.2 27.0 40.3 30.3 43.8 40.2 28.4 24.3 32.3 21.2 28.3 43.3 49.9 52.0 23.7 23.7 25.2 28.9 30.9 27.4 26.4 27.2 22.7 22.4 22.9 32. 46.0 48.3 29.3 28.4 21.6 25.0 34.5 25.9 30.2 19.0 20.7 31.0 30.2 19.8 Table 12: Comparison of models on the M3Exam dataset across different languages. Models English Multi Arabic Bengali Finnish Indonesian Korean Russian Swahili Telugu Vicuna-1.5-7B Qwen2-7B-Instruct Llava-1.5-7B Llava-Next-7B Phi-3.5-Vision PALO-7B PANGEA-7B (Ours) 59.7 72. 66.8 68.3 75.9 69.4 73.7 52.7 71.2 52.8 52.1 51.3 50.8 66.0 32.3 67. 61.8 64.5 63.1 60.9 55.5 68.1 75.9 33.4 24.9 24.8 46.0 65.3 63.0 67. 60.2 63.0 57.3 61.8 66.3 72.6 78.0 72.8 74.3 70.6 70.6 74.5 58.8 64. 63.3 61.9 60.2 56.8 69.4 57.6 67.2 55.0 58.4 57.5 56.7 60.1 51.3 75. 55.0 53.1 48.7 42.5 76.6 18.1 73.8 20.6 17.0 28.3 10.8 60.0 Table 13: Comparison of models on the TyDiQA dataset across different languages. Models English Multi Arabic Spanish Basque Hindi Ind. Burmese Russian Swahili Telugu Chinese Vicuna-1.5-7B Qwen2-7B-Instruct Llava-1.5-7B Llava-Next-7B Phi-3.5-Vision PALO-7B PANGEA-7B (Ours) 78.1 80.3 79.1 79.1 77.9 77.4 79.1 57.4 61. 57.6 57.1 54.8 57.2 61.2 52.7 64.0 52.7 51.7 53.7 56.5 60.5 69.4 71. 69.2 68.8 67.2 68.4 67.8 50.8 51.6 50.9 50.3 50.4 49.8 50.0 54.5 59. 54.9 54.5 54.9 58.6 61.8 61.0 68.5 62.6 62.0 51.7 58.5 66.4 48.4 50. 49.0 46.7 47.8 47.4 48.7 66.5 72.7 65.9 65.5 61.3 65.6 69.4 52.1 53. 51.7 52.1 49.3 51.2 58.9 54.5 55.3 55.8 55.2 52.5 53.1 60.4 63.5 72. 63.9 63.8 59.5 62.8 68.2 Table 14: Comparison of models on the XStoryCloze dataset across different languages. Models English Multi Bengali German Spanish French Japanese Russian Swahili Telugu Thai Chinese Vicuna-1.5-7B Qwen2-7B-Instruct Llava-1.5-7B Llava-Next-7B Phi-3.5-Vision PALO-7B PANGEA-7B (Ours) 17.6 48. 14.8 15.6 59.2 13.6 82.0 6.4 40.4 7.6 7.5 33.1 5.8 47.3 0.0 0. 0.0 0.0 0.0 0.0 0.0 14.4 67.2 15.2 13.6 64.0 11.6 68.4 9.6 67. 10.8 13.2 59.6 9.6 74.8 14.4 68.8 18.0 16.0 58.0 13.2 63.2 2.8 11. 2.8 1.6 20.0 1.6 22.0 10.8 71.2 11.2 12.8 54.0 8.8 68.0 3.6 10. 0.4 2.0 4.0 0.4 54.0 0.0 2.4 0.0 0.0 0.0 0.0 5.6 2.0 45. 1.6 1.6 18.8 0.0 49.6 14.8 59.2 15.6 14.0 52.4 12.4 68.0 Table 15: Comparison of models on the MGSM dataset across different languages. 51 Preprint - Work in Progress Models English Multi Arabic Bengali Portuguese Chinese French German Vicuna-1.5-7B Qwen2-7B-Instruct Llava-1.5-7B Llava-Next-7B Phi-3.5-Vision PALO-7B PANGEA-7B (Ours) 49.5 70.1 50.2 52.1 62.0 46.7 68.4 34.7 53.1 34.9 35.6 39.1 32.6 52. 30.3 51.0 29.7 30.0 34.9 30.3 49.3 28.5 43.4 28.5 28.8 27.9 29.5 44. 39.6 60.7 40.3 40.7 47.6 36.0 58.9 36.9 63.8 36.8 37.3 41.5 34.2 60. 40.4 61.5 40.1 41.4 49.2 36.9 58.9 39.8 57.7 39.8 41.4 45.8 35.8 56. Models Hindi Indonesian Italian Japanese Korean Spanish Swahili Yoruba Vicuna-1.5-7B Qwen2-7B-Instruct Llava-1.5-7B Llava-Next-7B Phi-3.5-Vision PALO-7B PANGEA-7B (Ours) 29.8 45. 29.2 29.6 32.9 29.6 45.7 36.5 57.1 37.1 37.5 38.3 33.7 55.4 39.5 60. 41.0 41.2 47.0 36.4 58.8 35.9 58.0 35.1 36.0 40.0 32.7 55.3 34.1 54. 34.1 34.2 36.6 30.6 52.7 40.3 61.9 41.6 42.7 49.6 37.0 59.7 27.9 36. 28.0 28.5 28.9 26.4 42.8 26.8 31.8 27.3 28.7 27.8 27.1 31.3 Table 16: Comparison of models on the MMMLU dataset across different languages."
        }
    ],
    "affiliations": []
}