{
    "paper_title": "Teaching Pretrained Language Models to Think Deeper with Retrofitted Recurrence",
    "authors": [
        "Sean McLeish",
        "Ang Li",
        "John Kirchenbauer",
        "Dayal Singh Kalra",
        "Brian R. Bartoldson",
        "Bhavya Kailkhura",
        "Avi Schwarzschild",
        "Jonas Geiping",
        "Tom Goldstein",
        "Micah Goldblum"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advances in depth-recurrent language models show that recurrence can decouple train-time compute and parameter count from test-time compute. In this work, we study how to convert existing pretrained non-recurrent language models into depth-recurrent models. We find that using a curriculum of recurrences to increase the effective depth of the model over the course of training preserves performance while reducing total computational cost. In our experiments, on mathematics, we observe that converting pretrained models to recurrent ones results in better performance at a given compute budget than simply post-training the original non-recurrent language model."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 1 ] . [ 1 4 8 3 7 0 . 1 1 5 2 : r a"
        },
        {
            "title": "TEACHING PRETRAINED LANGUAGE MODELS TO\nTHINK DEEPER WITH RETROFITTED RECURRENCE",
            "content": "Sean McLeish1, Ang Li2, John Kirchenbauer1, Dayal Singh Kalra1, Brian R. Bartoldson3, Bhavya Kailkhura3, Avi Schwarzschild4, Jonas Geiping5, Tom Goldstein1, Micah Goldblum6 1 University of Maryland, 2 New York University, 3 Lawrence Livermore National Laboratory, 4 University of North Carolina, 5 ELLIS Institute Tübingen, Max Planck Institute for Intelligent Systems, Tübingen AI Center, 6 Columbia University"
        },
        {
            "title": "ABSTRACT",
            "content": "Recent advances in depth-recurrent language models show that recurrence can decouple train-time compute and parameter count from test-time compute. In this work, we study how to convert existing pretrained non-recurrent language models into depth-recurrent models. We find that using curriculum of recurrences to increase the effective depth of the model over the course of training preserves performance while reducing total computational cost. In our experiments, on mathematics, we observe that converting pretrained models to recurrent ones results in better performance at given compute budget than simply post-training the original non-recurrent language model. Code: github.com/mcleish7/retrofitting-recurrence Models: huggingface.co/collections/tomg-group-umd/retrofitting-recurrence"
        },
        {
            "title": "INTRODUCTION",
            "content": "Test-time compute scaling refers to the use of additional computation during inference to improve model outputs. By decoupling computation intensity from model size, test-time compute scaling achieves superior benchmark scores without requiring more model parameters or additional pretraining. The mainstream paradigm for test-time scaling involves generating many tokens, either in chain-of-thought traces or by generating many candidate solutions and choosing the best (Snell et al., 2024; Guo et al., 2025). An emerging alternative paradigm for test-time scaling leverages depth-recurrence, by which language model can simply recur layers for more iterations to expend more compute. Depthrecurrence has the advantage that increasing compute does not increase memory consumption or context size during inference. Moreover, not requiring the model to verbalize thoughts as tokens may allow for more complex reasoning to happen within the latent space where there is higher information bandwidth. Finally, recurrent networks can be trained on standard data sources and do not require training with bespoke reasoning traces in the domain of interest. Correspondence to: smcleish@umd.edu. Figure 1: We take layers from pretrained language models and recur core block. We take early layers to form the prelude and later layers to form the recurrent block and coda, removing the layers in between. After each recurrence, we concatenate the output of the prelude with the output of the recurrent block (or random noise at time zero) and apply linear adapter. 1 Geiping et al. (2025) pretrain depth-recurrent transformer from scratch on 800 billion tokens at substantial cost. Although their model can reuse parameters at test time to scale up compute and improve performance, their work also uses large number of recurrent iterations during training, which significantly slows down training compared to fixed depth model with the same parameter count. This inspires us to focus on the training efficiency of depth-recurrent models. In this work, we study fast procedures for converting fixed depth models into depth-recurrent models through continued pretraining, visualized in Figure 1. Because transformer models include residual connections (He et al., 2015) that write updates back into the same residual stream, transformer layers operate in shared representation space (Elhage et al., 2021). This makes it possible to loop block of layers from pretrained language models by feeding the output of the block back into itself as input. By training model while it operates in this looped mode, the model learns to exploit recurrence to improve performance. Our main experiments demonstrate that TinyLlama-1.1B-intermediate-step1431k-3T (Zhang et al., 2024b), OLMo-2-0425-1B (OLMo et al., 2024) and Llama-3.2-1B (Grattafiori et al., 2024) can be converted into depth-recurrent transformers. We view post training fixed depth models with recurrence as simple addition to the training pipeline, similar to how one would extend the context length during the later stages of pretraining (Grattafiori et al., 2024). We observe that doing so improves performance on reasoning tasks that are known to differentially benefit from additional test-time compute (Geiping et al., 2025). We focus on two efficiency goals. First, we want the model initialized from pretrained weights to outperform model trained from scratch on per-training-FLOP basis. Since parameters are both added to and removed from the original model when converting it into depth-recurrent one, this knowledge transfer goal is non-trivial. We show in Figure 2 that initializing depth-recurrent model from Llama-3.2-1B weights strongly outperforms the randomly initialized model in terms of loss and benchmark accuracy per training FLOP spent. Second, we want the performance of the pre-trained model to increase after conversion to recurrent form. We find that with well-formed data curriculum, recurrence results in an increase in accuracy on math tasks while maintaining high accuracy on broad suite of language modeling benchmarks (see Figure 8 and Table 1). Overall, we show that retrofitting recurrence into pretrained language models is an efficient way to train performant depth-recurrent models. In summary, our contributions are as follows: 1. We show that initializing parameters of recurrent models from those of pretrained fixed depth model is significantly more efficient than using random initialization (Figure 2). 2. We propose curriculum over recurrent depths, slowly increasing the average number of recurrent iterations during training to maintain performance while improving training speed (Figure 3). 3. We show that, using Common Crawl math data, we can convert TinyLlama, OLMo, and Llama models into recurrent models that achieve better GSM8K and MATH performance than base models (Figures 5 and 6). 4. Since we remove layers when converting fixed depth models to recurrent ones, we find that introducing healing period with minimal distribution shift helps recover basic language modeling performance before switching to task-specific data to further refine the depth-recurrent models reasoning performance (Figure 8 and Table 1)."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Recurrent models. It has been shown that universal transformers based on recurrence are Turing-complete (Dehghani et al., 2018). Recurrent transformers with weight shared layers but fixed layer repetition count have been studied in detail (Lan et al., 2019; Takase & Kiyono, 2021; Fan et al., 2024; Bae et al., 2024; Gao et al., 2024; Ng & Wang, 2024; Csordás et al., 2024; McLeish et al., 2024; Saunshi et al., 2025; Zeng et al., 2025). Adaptive-depth mechanisms have been studied with the specific goal of increasing computation efficiency (Graves, 2016; Elbayad et al., 2019; Schwarzschild et al., 2021; Bansal et al., 2022). more advanced class of recurrent transformer can utilize an internal mechanism to exit after data-dependent number of recurrences (Geiping et al., 2025; Aleksandrov et al., 2025; Chen et al., 2025; Bae et al., 2025). Raposo et al. (2024) propose mixture of depths models which adaptively route tokens through or around each transformer block. Mohtashami et al. (2023) augment mixture of depths with weight sharing, extended by Bae et al. (2025) with adaptive exiting to further increase efficiency. 2 Model surgery. There is rich literature on methods for making post-hoc changes to model architecture and size (Chen et al., 2015; Wei et al., 2016). Li et al. (2025) finetune looped models initialized from the GPT-2 (Brown et al., 2020) and OPT (Zhang et al., 2022) checkpoints finding small gains from finetuning and looping under-trained models on multiple choice benchmarks over the base checkpoints. In particularly relevant prior work, Bae et al. (2024) study converting pretrained transformer language models into recurrent models using just 2 or 3 recursions. Notably, the authors maintain the same shape as the base model and require low rank adapters (Hu et al., 2022) to recover performance of the base model. Bae et al. (2024) also find that recurring more leads to performance decreases in the post-trained model which means that their approach cannot effectively leverage additional compute at test time. Unlike Bae et al. (2024), our approach does not require distillation or auxiliary adapters and does benefit from additional test time computation. Finally, in concurrent work Koishekenov et al. (2025) convert OLMo (OLMo et al., 2024) models into depth recurrent models. They also use prelude, recurrent block, and coda structures but do not use input injection, keep all parameters when converting the model, and train with fixed number of recurrences. While they do demonstrate modest performance improvements they do not present their results in terms of training or inference compute making the degree of cost-benefit afforded by their method difficult to discern. Latent reasoning. Wang et al. (2025) introduce the Hierarchical Reasoning Model (HRM); an architecture designed to better align with certain anthropomorphic biases for compositional intelligence. However, ARC Prize Team (2025) performs further ablations on the HRM architecture and finds only the main recurrence is needed for reasoning performance, reducing the HRM to model similar to that of Geiping et al. (2025) without the ability to extrapolate in recurrence. We begin our own research by re-purposing aspects of the pretraining recipe developed by Geiping et al. (2025) to train large recurrent language model from scratch; the first work to establish that latent reasoning as scalable, alternate approach for pretraining transformer language models. We detail how our architecture and training recipe is derived from theirs more formally in Section 3. We provide an extended discussion of other related work in Section A."
        },
        {
            "title": "3 EXPERIMENTAL SETUP",
            "content": "Model Definition. Using the same notation as Geiping et al. (2025), here we define the structure of the class of recurrent models we study. We define as the prelude, as the recurrent block and as the coda; each of which is set of unique transformer blocks with the embeddings included in and unembeddings in C. We visualize the architecture in Figure 1 (right). begins with linear adapter which takes the concatenation of si and e, hence if the width of the model is the linear layer goes from 2h to h. Given vocabulary set , for an input sequence and number of recurrences r, the model output distribution is defined as follows. relude = (x) Recurrent Block s0 (0, σ2)nh, si = R(e, si1) for {1, . . . , r} Coda = C(sr) Geiping et al. (2025) use scalable initialization (Takase et al., 2023) for their Huginn-0125 model. Such schemes allow model shape to be altered whilst maintaining training stability. We also use this random initialization when training from scratch. To allow for adaptive recurrence at test time, Geiping et al. (2025) sample from Poisson-Lognormal distribution with mean of 32 at each training step. They also employ truncated backpropagation procedure, only propagating gradients through at most the last 8 passes through R. This reduces training time and allows for very large values of without exhausting GPU memory. When we say model is trained with train recurrence = k, this means that the mean of the Poisson-Lognormal distribution is equal to during training. We note that the prelude parameters are still updated as the skip connection to the recurrent block allows for gradient propagation from the output. Model Surgery. Similar to Geiping et al. (2025), we use tuple notation to define the number of transformer layers in each of the prelude, recurrent block, and coda. For example, (2, 4, 2) means there are 2 transformer layers in the prelude, 4 in the recurrent block, and 2 in the coda. To improve efficiency at large numbers of test recurrences, we do not use every layer from the pretrained model when adapting it into depth-recurrent model. We find that selecting the early layers for the prelude 3 and later layers for the recurrent block and coda performs best (see Appendix Figures 12 and 13). For example, if the model we are using has 22 layers and we take (4, 8, 4) configuration. This corresponds to selecting layers [0, 1, 2, 3], [10, 11, 12, 13, 14, 15, 16, 17], [18, 19, 20, 21]; we use this selection for our (4, 8, 4) TinyLlama based models. We visualize our methodology in Figure 1. We detail the exact parameter counts and layers taken from pretrained models for our recurrent models in Section E. In Appendix Figure 13, we compare to the ShortGPT pruning method (Men et al., 2024) to select layers to drop from the parent model when forming the recurrent model. We find our selection to be better for depth-recurrent model post-training. We also compare to taking all layers from TinyLlama to form (6, 10, 6) model and to (2, 4, 2) TinyLlama model in Appendix Figure 14. We inherit the conventions of the models we are converting. Specifically, Geiping et al. (2025) use normalizations four times in each decoder block and additionally use the final norm before the coda; we reduce to two norms in each decoder block and remove the dual use of the final layer norm. We also use grouped-query attention (Ainslie et al., 2023), train all models with context length of 1024, and do not weight-tie the embedding and unembedding layers. We present additional technical training details in Section B. We emphasize that although two of models we analyze in this paper share the llama name they are different models, trained independently. The two models are different shapes, with TinyLlama being 6 layers deeper than Llama, but narrower (smaller residual stream) as they both contain approximately 1 billion parameters. TinyLlama uses the Llama-2 vocabulary, whereas Llama-3 uses vocabulary over 4 larger. Finally, TinyLlama is trained with next token prediction cross entropy loss from random initialization for 3 trillion tokens, whereas Llama is initialized by pruning Llama-3.1-8B and then using logit level distillation from Llama-3.1-8B and Llama-3.1-70B for 9 trillion tokens (Meta, 2024). Furthermore, the OLMo models use QK-norm and post-normalization scheme unlike the llama models which use pre-normalization scheme and do not use QK-norm. Calculating Training Cost. For recurrent model, the number of unique parameters refers to the number of distinct, trainable parameters in the model without double counting parameters that are shared across recurrences; we simply use the term parameters in this paper1. One can also consider the effective parameters of recurrent model by including repetitions across recurrences. However, for clarity, throughout the rest of the work we quantify the size of recurrent model evaluated at different depths in terms of Floating Point Operations (FLOPs) rather than describing parameter re-use. In other words, increasing the number of iterations performed by the recurrent block increases the amount of computation invested while number of actual parameters in the model remains fixed. When calculating training FLOPs for standard fixed depth transformers, we use the approximation FLOPs = 6N (Kaplan et al., 2020), where is non-embedding parameters and is number of training tokens. However, recurrent models require different rule. As we only backpropagate through at most the last 8 iterations of the recurrent block, we split the effective parameter count (N ) into two parts: N1 which includes all parameters with gradients recorded and N2 which includes all parameters that are used in the forward pass without gradients. We calculate N1 and N2 using the mean number of recurrences during training. This gives LOP = (6N1 + 2N2)D for our recurrent models."
        },
        {
            "title": "4 TRAINING RECURRENT LANGUAGE MODELS",
            "content": "Our main experimental results are presented in four subsections. In Section 4.1, we find that pretrained initialization outperforms random initialization in terms of loss and benchmark performance. Then, in Section 4.2, we use curriculum to schedule the mean of the Poisson-Lognormal distribution, showing that this can reduce training costs without negatively impacting loss. In Section 4.3, we show that depth-recurrent post-training is more efficient than training non-recurrent models for math problems. Finally, in Section 4.4, we demonstrate that with good data curriculum, depth-recurrent models can be good general language models in addition to achieving higher accuracy on grade school math problems despite having fewer parameters. 1We also exclude embedding and unembedding parameters in this count. 4 Figure 2: Initializing from pretrained Llama layers gives significant advantage in loss and benchmark accuracy. Left: Loss over training step for 120 billion tokens for models initialized from Llama-3.2-1B layers and randomly (Takase et al., 2023). Although starting higher, the model initialized from Llama weights achieves lower losses consistently than the model initialized randomly. Right: Zero shot accuracy on Hellaswag (Zellers et al., 2019) over training step for recurrences [1, 2, 4, 8, 16, 32]. We see the Llama based model (blue) achieves higher accuracy quicker and leverages recurrence effectively from early training steps. We record accuracy over recurrence for suite of language modeling benchmarks in Appendix Table 2. 4.1 EFFICIENTLY INITIALIZING RECURRENT TRANSFORMERS We begin by demonstrating that using pretrained initialization outperforms random initialization for depth-recurrent models. We train two models for approximately 120 billion tokens on FineWeb-Edu (Penedo et al., 2024) data with mean number of recurrences of 32. Figure 2 visualizes the training loss and Hellaswag accuracy curves over training for (2, 4, 2) model initialized from Llama-3.2-1B and from random initialization, following Takase et al. (2023). On the left, we see the initialization from pretrained Llama layers yields large efficiency gain in terms of loss. On the right, we show that the model initialized from pretrained Llama layers achieves higher benchmark accuracy earlier on Hellaswag (Zellers et al., 2019). By training step 1000, the Llama initialized model is already leveraging recurrence to increase accuracy, unlike the random initialization for which all recurrences are achieving just over random accuracy. In Appendix Table 2, we show the accuracy at 28, 000 steps for both models over multiple recurrence levels on suite of language modeling benchmarks, finding that initializing from pretrained Llama weights causes significant increase in accuracy in all cases. In Section C.1, we also present additional experiments including cooldown for 12 billion additional tokens. Extrapolating the loss curves in log-linear space suggests it would take at least approximately 950 billion tokens for these loss curves to intersect (see Appendix Figure 9). It is likely that this is an underestimate of the true number of tokens required for the models to achieve loss parity as the curves are not perfectly log-linear at the end of our data. 4.2 SCHEDULING RECURRENCES Using truncated backpropagation means the forward pass for our recurrent models consumes larger share of runtime than it would for non-recurrent model. Hence, reducing the time spent on the forward pass for our models has large impact on training time. With this insight, we explore an efficient curriculum which schedules the mean of the Poisson-Lognormal distribution. This curriculum is analogous to the gradual stacking technique (Gong et al., 2019; Reddi et al., 2023; Saunshi et al., 2024; Du et al., 2024) which increases the depth of non-recurrent model by duplicating layers within the model during training and then training them independently. We visualize our curricula in Appendix Figure 16. Figure 3 measures the validation loss on one million tokens taken every 1000 training steps for (2, 4, 2) models initialized from Llama layers. This is the same as in Figure 2 but for shorter time horizon of 48 hours on 4 MI300A GPUs which equates to approximately 1e20 FLOPs. In Figure 3 (left), we see that linearly scheduling the recurrent depth has small positive impact on the validation loss as function of steps. Furthermore, on the right, we see that linearly scheduling 5 Figure 3: Scheduling the mean of the depth distribution is efficient in terms of both data and compute. We report validation loss over multiple recurrent depths in terms on steps (i.e. data) on the left and in terms of FLOPs on the right. We see that linearly scheduling the number of recurrences up to the final mean (32) over long period of training decreases the validation loss, hence the curriculum is both data and compute efficient. Alternative length curricula and more test recurrent depths are shown in Appendix Figure 17. greatly improves the efficiency in terms of loss improvement as function of FLOPs spent during training. In Section C.2, we show that scheduling the maximum backpropagation depth over training is better in terms of FLOPs but worse in terms of steps, and therefore less efficient overall than scheduling the mean depth but still valuable when trying to reach the lowest possible loss in given period of time. Finally, in Appendix Figure 19, we show that more efficient curricula where we schedule according to 1-sqrt function (visulaized in Figure 16) are as good as linear curricula for TinyLlama. 4.3 HOW TO RETROFIT RECURRENCE Next, we investigate how to efficiently retrofit depth-recurrence into pretrained non-recurrent transformers. First, we find Muon to be better optimizer than AdamW when training recurrent models in Section 4.3.1. In Section 4.3.2, we analyze TinyLlama, OLMo and Llama models. In all cases, under the same training FLOP budget, depth-recurrent models with fewer parameters can achieve higher accuracy on math problems than the non-recurrent parent model. We extend these results for TinyLlama, OLMo and Llama in Section C.3. Figure 4: Muon improves over AdamW when training recurrent models. Left: Loss vs. step for multiple training runs on the same data order with different optimizers, using learning rate of 5e5 for AdamW and 0.001 for Muon. Muon is the most stable and achieves the lowest loss for recurrent models. Note, the AdamW line ends early as the loss spikes and becomes NaN. Right: Loss (smoothed over 50 steps) vs. step for AdamW and Muon. For the non-recurrent TinyLlama model there is minimal difference between optimizers."
        },
        {
            "title": "4.3.1 OPTIMIZATION",
            "content": "We begin by initializing models from TinyLlama-1.1B-intermediate-step-1431k-3T. We consider (4, 8, 4) TinyLlama recurrent configuration, dropping out 6 layers (layers 4, 5, 6, 7, 8 and 9, using 0-indexing) from the original model. This yields approximately 700 million remaining parameters in this recurrent model, 72.7% of the parameters in the parent non-recurrent TinyLlama model. We also compare our (4, 8, 4) to (7, 8, 7) model in Figure 15, finding that removing the layers is efficient, even from the prelude and coda. Full parameter counts are provided in Section E. In Figure 4 (left), the Muon optimizer improves over AdamW for our recurrent models as it achieves lower loss and removes loss spikes during training. For the non-recurrent TinyLlama models, the difference is much less pronounced, but we still see small gain using the Muon optimizer. We smooth the loss over 50 steps to make this more visible in the plot. In Figure 4, we also compare to the variant of AdamW which is used by Geiping et al. (2025), and we refer to this variant as AdamW*. AdamW* modifies AdamW by including update clipping, removing the ε constant (Wortsman et al., 2023; Everett et al., 2024), and using different decoupling method than the PyTorch AdamW implementation (Schaipp, 2024). In subsequent experiments, we optimize all models with Muon. 4.3.2 RECURRENT MODELS ARE EFFICIENT TO TRAIN In our next set of experiments, while we continue training our (4, 8, 4) TinyLlama configuration, we build another set of models initialized from the weights of OLMo-2-0425-1B. For OLMo, we construct (4, 6, 4) configurations, removing 2 layers (layers 4 and 5 with 0 indexing) from the pretrained model. This leaves approximately 900 million remaining parameters in the recurrent model, which equates to 87.5% of the pretrained models parameters. Figure 5: Recurrence improves reasoning on GSM8K for TinyLlama, even when controlling for FLOPs. We train (4, 8, 4) and non-recurrent models for approximately 50 billion tokens of Nemotron-CC-Math-v1 data. Left: We plot accuracy over the number of FLOPs used during training. We see that recurrent models, trained with scheduling, can efficiently outperform the non-recurrent baseline. Right: We plot accuracy over the number of recurrences for inference. We see the recurrent models are competitive with the fixed depth baseline and can outperform it by using more recurrences and therefore more FLOPs. We plot each individual models accuracy over training step and recurrence in full in Section C.3.1, including for training recurrence 8 and 32. Evaluations on the final checkpoint over tasks shown in Table 1 are in Appendix Table 3. We also provide identical experiments for OLMo and Llama in Section C.3. train models on approximately 50 billion tokens of In Figures 5 and 6, we Nemotron-CC-Math-v1-4plus (Mahabadi et al., 2025) data, with 1-sqrt curriculum for the first 75% of training and constant mean recurrences thereafter, and evaluate on GSM8K (Cobbe et al., 2021) and MATH (Hendrycks et al., 2021), for TinyLlama and OLMo, respectively. For our GSM8K and MATH evaluations, we use single shot example in context when evaluating. For GSM8K, we record the flexible extract accuracy to avoid formatting of the answer being confounder, and for MATH we use the Minerva (Lewkowycz et al., 2022) criteria with Math-Verify (Kydlíˇcek, 2025). Controlling for training FLOPs, both Figures 5 and 6 (left) show that it is efficient to train recurrent models. The depth-recurrent models achieve comparable performance to the non-recurrent baseline when evaluated at smaller training budgets but continue to improve as more compute is invested while accuracy for the non-recurrent model plateaus. We emphasize that all of 7 these experiments utilize the same training dataset presented in the same order. The differences in curve length come from the additional FLOPs required to train the recurrent models (which require more FLOPs per parameter) for the same number of steps. The end of each line shown in Figures 5 and 6 (left) corresponds to the exact same number of tokens seen for each model. For each recurrent model we visualize the accuracy using the test recurrence equal to the mean training recurrence. In Figures 5 and 6 (right), we plot accuracy against number of recurrences used during inference for the models at the end of training. We see that recurrent models improve performance over the non-recurrent baseline significantly when utilizing more test-time compute. Moreover, combining this with Appendix Figures 22 and 33, we conclude that recurrent models are competitive on per-FLOP basis for inference despite containing fewer trainable parameters at any FLOPs count. Overall, depth-recurrent models are able to leverage compute to achieve higher overall performance with fewer parameters than their non-recurrent counterparts. We construct our final set of models from the weights of Llama-3.2-1B. For Llama, we construct (4, 6, 4) configurations, removing 2 layers (layers 4 and 5 with 0 indexing) from the pretrained model. In Figure 7, we visualize the final checkpoint accuracy for TinyLlama, Llama and OLMo on both GSM8K and MATH, seeing gains in all cases using recurrence. We provide full visualizations of accuracy over train recurrences 4, 8, 16 and 32 for both training and inference on GSM8K and MATH for all three model families in Section C.3. While our results are generally congruous, we do note some differences in the results across datasets and models. For GSM8K, we see that larger training recurrences lead to larger performance improvement on per FLOP basis for OLMo and Llama, however this trend does not hold for TinyLlama (see Figure 20). For both tasks, we see that the accuracy achieved by the Llama and OLMo models is higher than that of the TinyLlama based models. This suggests that using stronger set of pretrained weights transfers additional knowledge to the final depth-recurrent model. Figure 6: Recurrence efficiently improves reasoning on MATH for OLMo. Left: We plot accuracy over the number of FLOPs used during training. We see that recurrent models, trained with scheduling, can efficiently outperform the non-recurrent baseline when trained on the same tokens. Right: We plot accuracy over the number of recurrences used for inference. We see the recurrent models are competitive with the fixed depth baseline (green horizontal line) and can outperform it by using more recurrences and therefore more FLOPs. We plot each individual models accuracy over training and recurrence in full in Section C.3.2, including for training recurrence 8 and 32. Evaluations on the final checkpoint over tasks shown in Table 1 are in Appendix Table 4.We also provide identical experiments for TinyLlama and Llama in Section C.3. 4.4 DATA MIXTURES In previous experiments, we observe that training strictly on math data slightly degrades performance on non-reasoning based evaluations such as Hellaswag, Arc and OpenbookQA (see Appendix Tables 3, 4 and 5). To address this degradation, we train on an even mix of FineWeb-Edu (Penedo et al., 2024), Nemotron-Pretraining-SFT-v1-General (NVIDIA et al., 2025), and Nemotron-Pretraining-SFT-v1Math (NVIDIA et al., 2025). We also specifically remove rows in the Nemotron-Pretraining-SFT-v1 dataset generated by reasoning models trained with reinforcement learning (e.g., DeepSeek-R1 (Guo et al., 2025)), as well as the userassistant tags. 8 Figure 7: Recurrent models achieve higher final checkpoint accuracy on GSM8K and MATH. We plot the final checkpoint accuracy on GSM8K and MATH for the non-recurrent baseline and multiple training recurrences for TinyLlama, Llama and OLMo, using test recurrence 32 for all recurrent models. Full accuracies including recurrences 1, 2, 4, 8 and 16 can be seen in Tables 3, 4 and 5. In our first experiment on data mixtures, we train (4, 8, 4) TinyLlama models for 26 billion tokens on an even mix of the three datasets; we call this single phase training. Since we remove layers during recurrent retrofitting, we hypothesize that the depth-recurrent models must first recover their basic language modeling abilities before they can efficiently learn the high-quality Nemotron-PretrainingSFT-v1 data. To test this hypothesis, we then construct simple two phase training procedure involving an initial healing period followed by phase of high-quality training. In two phase training, we train for 26 billion tokens of FineWeb-Edu followed by the same data as seen in the single phase training, totaling 52 billion tokens. For our recurrent models, we use linear curriculum for 25% of total steps. We note it is common to heal models after pruning to regain language modeling performance (Yang et al., 2024; Men et al., 2024). In Figure 8, we visualize accuracy on Arc-Challenge over training for the 26 billion tokens on the combination of FineWeb-Edu, Nemotron-Pretraining-SFT-v1-General, and Nemotron-PretrainingSFT-v1-Math data, i.e. the secondary phase after healing for the two phase approach. We see that under single phase training, when we directly train on the mix of all three datasets, the final recurrent model is worse than the non-recurrent model. Next, we observe that during two phase training, the non-recurrent model sees small increase in accuracy over single phase training. Intuitively, this could be explained by the fact that the initial model is already trained for 3 trillion tokens of web text and as there is no model surgery performed on the non-recurrent baseline, there is nothing to explicitly heal. However, for our depth-recurrent model, two phase training provides an increase of 5% on Arc-C, demonstrating that the initial 26 billion token healing period is effective in helping the model to regain basic language modeling abilities. Our results demonstrate that data curriculum designed to minimize initial distribution shift after model surgery helps depth-recurrent models maintain basic language modeling performance while improving on math problems. In Table 1, we measure zero-shot accuracy on Arc-Easy (Clark et al., 2018), Arc-Challenge (Clark et al., 2018), Hellaswag (HS) (Zellers et al., 2019), Winogrande (WG) (Sakaguchi et al., 2021), MMLU (Hendrycks et al., 2020), PIQA (Bisk et al., 2020), and OpenbookQA (OBQA) (Mihaylov et al., 2018). We see that the depth-recurrent model achieves high scores across all benchmarks only outperformed by the non-recurrent model on MMLU by less than one standard error. We include the Huginn-0125 evaluations from Geiping et al. (2025), comparisons to the base TinyLlama, as well as evaluations using more test recurrences for our models in Appendix Table 6. Our models are competitive with the much larger Huginn-0125 model released by Geiping et al. (2025) achieving an MMLU score over 12% higher and GSM8K performance 10% higher than their published evaluation results. Overall, we find depth-recurrent models can improve performance on math tasks while improving or maintaining performance across broad range of language modeling benchmarks despite having fewer unique trainable parameters. 9 Figure 8: High quality data and curricula improve recurrent model performance on nonreasoning benchmarks. Left: We plot accuracy on Arc-Challenge over training for the 26 billion tokens on FineWeb-Edu and Nemotron-SFT data, i.e. after healing for two phase training. We see the training accuracy of the non-recurrent model does not differ significantly between single or two phase training. For the depth-recurrent model, two phase training outperforms single phase by 5% suggesting the healing period helps the model recover language modeling ability after surgery. Right: Accuracy over multiple recurrences at the end of training. We see the depth-recurrent model with two phase training can use recurrence to extend its accuracy to 37.7% by utilizing more FLOPs during inference. We repeat our Arc-Challenge accuracies in Table 1 for clarity at test recurrences 1 and 32. Table 1: High quality data and curricula improve recurrent model performance across benchmarks. We see that depth-recurrence achieves better accuracy on non-math when using two phase training and confirm that the depth-recurrent models improve as function of test-time recurrence. On the other hand, for the non-recurrent baseline we see single phase and two phase training perform similarly. Full results in Appendix Table 6, including test recurrences 2, 4, 8 and 16. Test Rec Arc-E Arc-C HS WG MMLU PIQA OBQA GSM8K MATH 1 32 1 50.0 52.7 52.7 65.2 4,8,4 (Train Recurrence=4) - Single Phase 31.6 32.7 50.8 58.2 58.0 61. 35.7 39.4 69.3 71.4 38.8 38.6 4,8,4 (Train Recurrence=4) - Two Phase 31.6 37.7 51.5 60. 56.7 60.5 36.2 44.8 71.0 73.6 39.4 40.0 TinyLlama-1.1b-3T Static Depth - Single Phase 25.6 52. 26.5 51.2 8.8 14.5 9.7 14.2 61.2 35.2 58. 60.5 45.1 71.4 39.2 46.2 14. TinyLlama-1.1b-3T Static Depth - Two Phase 62.5 36.5 60.3 59.6 44. 72.9 39.4 45.2 12."
        },
        {
            "title": "5 DISCUSSION",
            "content": "Our work demonstrates that depth-recurrent language models are parameter efficient and highlights their flexibility in decoupling train-time and test-time compute. However, investing more FLOPs per parameter makes the training process for depth-recurrent models more expensive. Our work makes significant progress towards ameliorating this issue by leveraging pretrained initializations, recurrence scheduling during training, and data curriculum. Here we identify several promising avenues for future work. One unsolved problem is how to most effectively build depth-recurrent models that can recur deeper at test time to solve harder problems than were seen during training. related goal is how to imbue recurrent models with native adaptivity that automatically assigns the right amount of compute (recurrence) to given problem based on how difficult it is. Such built-in stopping criteria would in principle allow models to think deeply on hard prob10 lems while solving easy problems quickly. Figure 12 and Figure 13 present our search process on selecting which layers to keep and which ones to discard, but future work could identify more optimal method for layer choice during model surgery. While our experiments are at the 1B parameter and 50B token scales, more experimentation is required to verify that our method generalizes to much larger model and data scales. Finally, we primarily focus on strengthening models mathematical capabilities via depth-recurrence, and future work should extend this to other reasoning-intensive domains."
        },
        {
            "title": "ACKNOWLEDGMENTS",
            "content": "This work was made possible by DARPA TIAMAT and the NSF TRAILS Institute (2229885). Additional support was provided by awards from Capital One Bank, Open Philanthropy, and the Center For AI and Responsible Financial Innovation. Prepared by LLNL under Contract DE-AC52-07NA27344 and supported by the LLNL-LDRD Program under Project No. 24-ERD-010 (LLNL-CONF-2012308). This manuscript has been authored by Lawrence Livermore National Security, LLC under Contract No. DE-AC52-07NA27344 with the U.S. Department of Energy. The United States Government retains, and the publisher, by accepting the article for publication, acknowledges that the United States Government retains non-exclusive, paid-up, irrevocable, world-wide license to publish or reproduce the published form of this manuscript, or allow others to do so, for United States Government purposes."
        },
        {
            "title": "REFERENCES",
            "content": "Joshua Ainslie, James Lee-Thorp, Michiel De Jong, Yury Zemlyanskiy, Federico Lebrón, and Sumit Sanghai. Gqa: Training generalized multi-query transformer models from multi-head checkpoints. arXiv preprint arXiv:2305.13245, 2023. Preslav Aleksandrov, Meghdad Kurmanji, Fernando Garcia Redondo, David OShea, William Shen, Alex Iacob, Lorenzo Sani, Xinchi Qiu, Nicola Cancedda, and Nicholas Lane. Abbie: Autoregressive block-based iterative encoder for efficient sequence modeling. arXiv preprint arXiv:2507.08567, 2025. S-I Amari. Learning patterns and pattern sequences by self-organizing nets of threshold elements. IEEE Transactions on computers, 100(11):11971206, 1972. AMD. AMD Instinct MI300A Accelerators, December 2023. URL https://www.amd.com/ en/products/accelerators/instinct/mi300/mi300a.html. Cem Anil, Ashwini Pokle, Kaiqu Liang, Johannes Treutlein, Yuhuai Wu, Shaojie Bai, Zico Kolter, and Roger Grosse. Path independent equilibrium models can better exploit test-time computation. Advances in Neural Information Processing Systems, 35:77967809, 2022. ARC Prize Team. The hidden drivers of hrms performance on arc-agi, August 2025. URL https: //arcprize.org/blog/hrm-analysis. Sangmin Bae, Adam Fisch, Hrayr Harutyunyan, Ziwei Ji, Seungyeon Kim, and Tal Schuster. Relaxed recursive transformers: Effective parameter sharing with layer-wise lora. arXiv preprint arXiv:2410.20672, 2024. Sangmin Bae, Yujin Kim, Reza Bayat, Sungnyun Kim, Jiyoun Ha, Tal Schuster, Adam Fisch, Hrayr Harutyunyan, Ziwei Ji, Aaron Courville, et al. Mixture-of-recursions: Learning dynamic recursive depths for adaptive token-level computation. arXiv preprint arXiv:2507.10524, 2025. Arpit Bansal, Avi Schwarzschild, Eitan Borgnia, Zeyad Emam, Furong Huang, Micah Goldblum, and Tom Goldstein. End-to-end algorithm synthesis with recurrent networks: Extrapolation without overthinking. Advances in Neural Information Processing Systems, 35:2023220242, 2022. Jay Bear, Adam Prugel-Bennett, and Jonathon Hare. Rethinking deep thinking: Stable learning of algorithms using lipschitz constraints. Advances in Neural Information Processing Systems, 37: 9702797052, 2024. Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about physical commonsense in natural language. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pp. 74327439, 2020. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901, 2020. Tianqi Chen, Ian J. Goodfellow, and Jonathon Shlens. Net2net: Accelerating learning via knowledge transfer. CoRR, abs/1511.05641, 2015. Yilong Chen, Junyuan Shang, Zhenyu Zhang, Yanxi Xie, Jiawei Sheng, Tingwen Liu, Shuohuan Wang, Yu Sun, Hua Wu, and Haifeng Wang. Inner thinking transformer: Leveraging dynamic depth scaling to foster adaptive internal thinking. arXiv preprint arXiv:2502.13842, 2025. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. 12 Róbert Csordás, Kazuki Irie, Jürgen Schmidhuber, Christopher Potts, and Christopher D. Manning. Moeut: Mixture-of-experts universal transformers. arXiv preprint arXiv:2305.13245, 2024. Róbert Csordás, Christopher Manning, and Christopher Potts. Do language models use their depth efficiently? arXiv preprint arXiv:2505.13898, 2025. Tri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning. arXiv preprint arXiv:2307.08691, 2023. Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and Łukasz Kaiser. Universal transformers. arXiv preprint arXiv:1807.03819, 2018. Wenyu Du, Tongxu Luo, Zihan Qiu, Zeyu Huang, Yikang Shen, Reynold Cheng, Yike Guo, and Jie Fu. Stacking your transformers: closer look at model growth for efficient llm pre-training. Advances in Neural Information Processing Systems, 37:1049110540, 2024. Maha Elbayad, Jiatao Gu, Edouard Grave, and Michael Auli. Depth-adaptive transformer. arXiv preprint arXiv:1910.10073, 2019. Nelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Nova DasSarma, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish, and Chris Olah. mathematical framework for transformer circuits. Transformer Circuits Thread, 2021. https://transformer-circuits.pub/2021/framework/index.html. Katie Everett, Lechao Xiao, Mitchell Wortsman, Alexander Alemi, Roman Novak, Peter Liu, Izzeddin Gur, Jascha Sohl-Dickstein, Leslie Pack Kaelbling, Jaehoon Lee, et al. Scaling exponents across parameterizations and optimizers. arXiv preprint arXiv:2407.05872, 2024. Ying Fan, Yilun Du, Kannan Ramchandran, and Kangwook Lee. Looped transformers for length generalization. arXiv preprint arXiv:2409.15647, 2024. Yihang Gao, Chuanyang Zheng, Enze Xie, Han Shi, Tianyang Hu, Yu Li, Michael Ng, Zhenguo Li, and Zhaoqiang Liu. Algoformer: An efficient transformer framework with algorithmic structures. arXiv preprint arXiv:2402.13572, 2024. Khashayar Gatmiry, Nikunj Saunshi, Sashank Reddi, Stefanie Jegelka, and Sanjiv Kumar. Can looped transformers learn to implement multi-step gradient descent for in-context learning? arXiv preprint arXiv:2410.08292, 2024. Jonas Geiping and Tom Goldstein. Cramming: Training language model on single gpu in one day. In International Conference on Machine Learning, pp. 1111711143. PMLR, 2023. Jonas Geiping, Sean McLeish, Neel Jain, John Kirchenbauer, Siddharth Singh, Brian Bartoldson, Bhavya Kailkhura, Abhinav Bhatele, and Tom Goldstein. Scaling up test-time compute with latent reasoning: recurrent depth approach. arXiv preprint arXiv:2502.05171, 2025. Felix Gers and Jürgen Schmidhuber. Recurrent nets that time and count. In Proceedings of the IEEE-INNS-ENNS International Joint Conference on Neural Networks. IJCNN 2000. Neural Computing: New Challenges and Perspectives for the New Millennium, volume 3, pp. 189194. IEEE, 2000. Angeliki Giannou, Shashank Rajput, Jy-yong Sohn, Kangwook Lee, Jason Lee, and Dimitris Papailiopoulos. Looped transformers as programmable computers. In International Conference on Machine Learning, pp. 1139811442. PMLR, 2023. Alexi Gladstone, Ganesh Nanduru, Md Mofijul Islam, Peixuan Han, Hyeonjeong Ha, Aman Chadha, Yilun Du, Heng Ji, Jundong Li, and Tariq Iqbal. Energy-based transformers are scalable learners and thinkers. arXiv preprint arXiv:2507.02092, 2025. Linyuan Gong, Di He, Zhuohan Li, Tao Qin, Liwei Wang, and Tieyan Liu. Efficient training of bert by progressively stacking. In International conference on machine learning, pp. 23372346. PMLR, 2019. 13 Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Alex Graves. Adaptive computation time for recurrent neural networks. arXiv preprint arXiv:1603.08983, 2016. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Shibo Hao, Sainbayar Sukhbaatar, DiJia Su, Xian Li, Zhiting Hu, Jason Weston, and Yuandong Tian. Training large language models to reason in continuous latent space. arXiv preprint arXiv:2412.06769, 2024. Kaiming He, X. Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 770778, 2015. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and arXiv preprint Jacob Steinhardt. Measuring massive multitask language understanding. arXiv:2009.03300, 2020. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. NeurIPS, 2021. John Hopfield. Neural networks and physical systems with emergent collective computational abilities. Proceedings of the national academy of sciences, 79(8):25542558, 1982. Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. ICLR, 1(2):3, 2022. Jared Kaplan, Sam McCandlish, Tom Henighan, Tom Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. Jungo Kasai, Hao Peng, Yizhe Zhang, Dani Yogatama, Gabriel Ilharco, Nikolaos Pappas, Yi Mao, Weizhu Chen, and Noah A. Smith. Finetuning pretrained transformers into rnns. arXiv preprint arXiv:2103.13076, 2021. Yeskendir Koishekenov, Aldo Lipani, and Nicola Cancedda. Encode, think, decode: Scaling test-time reasoning with recursive latent thoughts. arXiv preprint arXiv:2510.07358, 2025. Hynek Kydlíˇcek. Math-Verify: Math Verification Library, 2025. URL https://github.com/ huggingface/math-verify. Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. Albert: lite bert for self-supervised learning of language representations. arXiv preprint arXiv:1909.11942, 2019. Yann LeCun and Fu Jie Huang. Loss functions for discriminative training of energy-based models. In International Conference on Artificial Intelligence and Statistics, 2005. Yoav Levine, Noam Wies, Or Sharir, Hofit Bata, and Amnon Shashua. The depth-to-width interplay in self-attention. arXiv preprint arXiv:2006.12467, 2020. Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, et al. Solving quantitative reasoning problems with language models. Advances in neural information processing systems, 35:38433857, 2022. Guanghao Li, Wenhao Jiang, Li Shen, Ming Tang, and Chun Yuan. Zero token-driven deep thinking in llms: Unlocking the full potential of existing parameters via cyclic refinement. arXiv preprint arXiv:2502.12214, 2025. 14 Rabeeh Karimi Mahabadi, Sanjeev Satheesh, Shrimai Prabhumoye, Mostofa Patwary, Mohammad Shoeybi, and Bryan Catanzaro. Nemotron-cc-math: 133 billion-token-scale high quality math pretraining dataset. arXiv preprint arXiv:2508.15096, 2025. Sean McLeish, Arpit Bansal, Alex Stein, Neel Jain, John Kirchenbauer, Brian Bartoldson, Bhavya Kailkhura, Abhinav Bhatele, Jonas Geiping, Avi Schwarzschild, et al. Transformers can do arithmetic with the right embeddings. Advances in Neural Information Processing Systems, 37: 108012108041, 2024. Sean McLeish, John Kirchenbauer, David Yu Miller, Siddharth Singh, Abhinav Bhatele, Micah Goldblum, Ashwinee Panda, and Tom Goldstein. Gemstones: model suite for multi-faceted scaling laws. arXiv preprint arXiv:2502.06857, 2025. Xin Men, Mingyu Xu, Qingyu Zhang, Bingning Wang, Hongyu Lin, Yaojie Lu, Xianpei Han, and Weipeng Chen. Shortgpt: Layers in large language models are more redundant than you expect. arXiv preprint arXiv:2403.03853, 2024. Jean Mercat, Igor Vasiljevic, Sedrick Keh, Kushal Arora, Achal Dave, Adrien Gaidon, and Thomas Kollar. Linearizing large language models. arXiv preprint arXiv:2405.06640, 2024. William Merrill and Ashish Sabharwal. little depth goes long way: The expressive power of log-depth transformers. arXiv preprint arXiv:2503.03961, 2025. William Merrill, Ashish Sabharwal, and Noah Smith. Saturated transformers are constant-depth threshold circuits. Transactions of the Association for Computational Linguistics, 10:843856, 2022. Meta. Llama 3.2: Revolutionizing edge 2024. able models, llama-3-2-connect-2024-vision-edge-mobile-devices. September ai URL and vision with open, customizhttps://ai.meta.com/blog/ Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can suit of armor conduct electricity? new dataset for open book question answering. arXiv preprint arXiv:1809.02789, 2018. Tomáš Mikolov, Stefan Kombrink, Lukáš Burget, Jan ˇCernock`y, and Sanjeev Khudanpur. Extensions of recurrent neural network language model. In 2011 IEEE international conference on acoustics, speech and signal processing (ICASSP), pp. 55285531. IEEE, 2011. Amirkeivan Mohtashami, Matteo Pagliardini, and Martin Jaggi. Cotformer: chain-of-thought driven architecture with budget-adaptive computation cost at inference. arXiv preprint arXiv:2310.10845, 2023. Kei-Sing Ng and Qingchen Wang. Loop neural networks for parameter sharing. arXiv preprint arXiv:2409.14199, 2024. NVIDIA, :, Aarti Basant, Abhijit Khairnar, Abhijit Paithankar, Abhinav Khattar, Adithya Renduchintala, Aditya Malte, Akhiad Bercovich, Akshay Hazare, Alejandra Rico, Aleksander Ficek, Alex Kondratenko, Alex Shaposhnikov, Alexander Bukharin, Ali Taghibakhshi, Amelia Barton, Ameya Sunil Mahabaleshwarkar, Amy Shen, Andrew Tao, Ann Guan, Anna Shors, Anubhav Mandarwal, Arham Mehta, Arun Venkatesan, Ashton Sharabiani, Ashwath Aithal, Ashwin Poojary, Ayush Dattagupta, Balaram Buddharaju, Banghua Zhu, Barnaby Simkin, Bilal Kartal, Bita Darvish Rouhani, Bobby Chen, Boris Ginsburg, Brandon Norick, Brian Yu, Bryan Catanzaro, Charles Wang, Charlie Truong, Chetan Mungekar, Chintan Patel, Chris Alexiuk, Christian Munley, Christopher Parisien, Dan Su, Daniel Afrimi, Daniel Korzekwa, Daniel Rohrer, Daria Gitman, David Mosallanezhad, Deepak Narayanan, Dima Rekesh, Dina Yared, Dmytro Pykhtar, Dong Ahn, Duncan Riach, Eileen Long, Elliott Ning, Eric Chung, Erick Galinkin, Evelina Bakhturina, Gargi Prasad, Gerald Shen, Haifeng Qian, Haim Elisha, Harsh Sharma, Hayley Ross, Helen Ngo, Herman Sahota, Hexin Wang, Hoo Chang Shin, Hua Huang, Iain Cunningham, Igor Gitman, Ivan Moshkov, Jaehun Jung, Jan Kautz, Jane Polak Scowcroft, Jared Casper, Jian Zhang, Jiaqi Zeng, Jimmy Zhang, Jinze Xue, Jocelyn Huang, Joey Conway, John Kamalu, Jonathan Cohen, Joseph Jennings, Julien Veron Vialard, Junkeun Yi, Jupinder Parmar, Kari Briski, Katherine Cheung, Katherine Luna, Keith Wyss, Keshav Santhanam, Kezhi Kong, Krzysztof Pawelec, Kumar Anik, Kunlun Li, Kushan Ahmadian, Lawrence McAfee, Laya Sleiman, Leon Derczynski, Luis Vega, Maer Rodrigues de Melo, Makesh Narsimhan Sreedhar, Marcin Chochowski, Mark Cai, Markus Kliegl, Marta StepniewskaDziubinska, Matvei Novikov, Mehrzad Samadi, Meredith Price, Meriem Boubdir, Michael Boone, Michael Evans, Michal Bien, Michal Zawalski, Miguel Martinez, Mike Chrzanowski, Mohammad Shoeybi, Mostofa Patwary, Namit Dhameja, Nave Assaf, Negar Habibi, Nidhi Bhatia, Nikki Pope, Nima Tajbakhsh, Nirmal Kumar Juluru, Oleg Rybakov, Oleksii Hrinchuk, Oleksii Kuchaiev, Oluwatobi Olabiyi, Pablo Ribalta, Padmavathy Subramanian, Parth Chadha, Pavlo Molchanov, Peter Dykas, Peter Jin, Piotr Bialecki, Piotr Januszewski, Pradeep Thalasta, Prashant Gaikwad, Prasoon Varshney, Pritam Gundecha, Przemek Tredak, Rabeeh Karimi Mahabadi, Rajen Patel, Ran El-Yaniv, Ranjit Rajan, Ria Cheruvu, Rima Shahbazyan, Ritika Borkar, Ritu Gala, Roger Waleffe, Ruoxi Zhang, Russell J. Hewett, Ryan Prenger, Sahil Jain, Samuel Kriman, Sanjeev Satheesh, Saori Kaji, Sarah Yurick, Saurav Muralidharan, Sean Narenthiran, Seonmyeong Bak, Sepehr Sameni, Seungju Han, Shanmugam Ramasamy, Shaona Ghosh, Sharath Turuvekere Sreenivas, Shelby Thomas, Shizhe Diao, Shreya Gopal, Shrimai Prabhumoye, Shubham Toshniwal, Shuoyang Ding, Siddharth Singh, Siddhartha Jain, Somshubra Majumdar, Soumye Singhal, Stefania Alborghetti, Syeda Nahida Akter, Terry Kong, Tim Moon, Tomasz Hliwiak, Tomer Asida, Tony Wang, Tugrul Konuk, Twinkle Vashishth, Tyler Poon, Udi Karpas, Vahid Noroozi, Venkat Srinivasan, Vijay Korthikanti, Vikram Fugro, Vineeth Kalluru, Vitaly Kurin, Vitaly Lavrukhin, Wasi Uddin Ahmad, Wei Du, Wonmin Byeon, Ximing Lu, Xin Dong, Yashaswi Karnati, Yejin Choi, Yian Zhang, Ying Lin, Yonggan Fu, Yoshi Suhara, Zhen Dong, Zhiyu Li, Zhongbo Zhu, and Zijia Chen. Nvidia nemotron nano 2: An accurate and efficient hybrid mamba-transformer reasoning model, 2025. URL https://arxiv.org/abs/2508.14444. Team OLMo, Pete Walsh, Luca Soldaini, Dirk Groeneveld, Kyle Lo, Shane Arora, Akshita Bhagia, Yuling Gu, Shengyi Huang, Matt Jordan, et al. 2 olmo 2 furious. arXiv preprint arXiv:2501.00656, 2024. Guilherme Penedo, Hynek Kydlíˇcek, Anton Lozhkov, Margaret Mitchell, Colin Raffel, Leandro Von Werra, Thomas Wolf, et al. The fineweb datasets: Decanting the web for the finest text data at scale. Advances in Neural Information Processing Systems, 37:3081130849, 2024. David Raposo, Sam Ritter, Blake Richards, Timothy Lillicrap, Peter Conway Humphreys, and Adam Santoro. Mixture-of-depths: Dynamically allocating compute in transformer-based language models. arXiv preprint arXiv:2404.02258, 2024. Sashank Reddi, Sobhan Miryoosefi, Stefani Karp, Shankar Krishnan, Satyen Kale, Seungyeon Kim, and Sanjiv Kumar. Efficient training of language models using few-shot learning. In International Conference on Machine Learning, pp. 1455314568. PMLR, 2023. Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale. Communications of the ACM, 64(9):99106, 2021. Nikunj Saunshi, Stefani Karp, Shankar Krishnan, Sobhan Miryoosefi, Sashank Jakkam Reddi, and Sanjiv Kumar. On the inductive bias of stacking towards improving reasoning. Advances in Neural Information Processing Systems, 37:7143771464, 2024. Nikunj Saunshi, Nishanth Dikkala, Zhiyuan Li, Sanjiv Kumar, and Sashank Reddi. Reasoning with latent thoughts: On the power of looped transformers. arXiv preprint arXiv:2502.17416, 2025. Fabian Schaipp. How to jointly tune learning rate and weight decay for AdamW. https:// fabian-sp.github.io/posts/2024/02/decoupling/, 2024. Avi Schwarzschild. Deep Thinking Systems: Logical Extrapolation With Recurrent Neural Networks. PhD thesis, University of Maryland, College Park, 2023. Avi Schwarzschild, Eitan Borgnia, Arjun Gupta, Furong Huang, Uzi Vishkin, Micah Goldblum, and Tom Goldstein. Can you learn an algorithm? generalizing from easy to hard problems with recurrent networks. Advances in Neural Information Processing Systems, 34:66956706, 2021. Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling llm test-time compute optimally can be more effective than scaling model parameters. arXiv preprint arXiv:2408.03314, 2024. 16 Ilya Sutskever, Geoffrey Hinton, and Graham Taylor. The recurrent temporal restricted boltzmann machine. Advances in neural information processing systems, 21, 2008. Sho Takase and Shun Kiyono. Lessons on parameter sharing across layers in transformers. arXiv preprint arXiv:2104.06022, 2021. Sho Takase, Shun Kiyono, Sosuke Kobayashi, and Jun Suzuki. Spike no more: Stabilizing the pre-training of large language models. arXiv preprint arXiv:2312.16903, 2023. Johannes von Oswald, Nino Scherrer, Seijin Kobayashi, Luca Versari, Songlin Yang, Maximilian Schlegel, Kaitlin Maile, Yanick Schimpf, Oliver Sieberling, Alexander Meulemans, et al. Mesanet: Sequence modeling by locally optimal test-time training. arXiv preprint arXiv:2506.05233, 2025. Guan Wang, Jin Li, Yuhao Sun, Xing Chen, Changling Liu, Yue Wu, Meng Lu, Sen Song, and Yasin Abbasi Yadkori. Hierarchical reasoning model. arXiv preprint arXiv:2506.21734, 2025. Junxiong Wang, Daniele Paliotta, Avner May, Alexander Rush, and Tri Dao. The mamba in the llama: Distilling and accelerating hybrid models. Advances in Neural Information Processing Systems, 37:6243262457, 2024. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. Tao Wei, Changhu Wang, Yong Rui, and Chang Wen Chen. Network morphism. In International conference on machine learning, pp. 564572. PMLR, 2016. Ronald Williams and Jing Peng. An efficient gradient-based algorithm for on-line training of recurrent network trajectories. Neural computation, 2(4):490501, 1990. Mitchell Wortsman, Tim Dettmers, Luke Zettlemoyer, Ari Morcos, Ali Farhadi, and Ludwig Schmidt. Stable and low-precision training for large-scale vision-language models. Advances in Neural Information Processing Systems, 36:1027110298, 2023. Liu Yang, Kangwook Lee, Robert Nowak, and Dimitris Papailiopoulos. Looped transformers are better at learning learning algorithms. arXiv preprint arXiv:2311.12424, 2023. Yifei Yang, Zouying Cao, and Hai Zhao. Laco: Large language model pruning via layer collapse. arXiv preprint arXiv:2402.11187, 2024. Pedram Zamirai, Jian Zhang, Christopher Aberger, and Christopher De Sa. Revisiting bfloat16 training. arXiv preprint arXiv:2010.06192, 2020. Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can machine really finish your sentence? arXiv preprint arXiv:1905.07830, 2019. Boyi Zeng, Shixiang Song, Siyuan Huang, Yixuan Wang, He Li, Ziwei He, Xinbing Wang, Zhiyu Li, and Zhouhan Lin. Pretraining language models to ponder in continuous space. arXiv preprint arXiv:2505.20674, 2025. Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lucas Beyer. Scaling vision transformers. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1210412113, 2022. Michael Zhang, Simran Arora, Rahul Chalamala, Alan Wu, Benjamin Frederick Spector, Aaryan Singhal, Krithik Ramesh, and Christopher Re. Lolcats: On low-rank linearizing of large language models. arXiv preprint arXiv:2410.10254, 2024a. Peiyuan Zhang, Guangtao Zeng, Tianduo Wang, and Wei Lu. Tinyllama: An open-source small language model, 2024b. Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068, 2022. 17 Rui-Jie Zhu, Tianhao Peng, Tianhao Cheng, Xingwei Qu, Jinfa Huang, Dawei Zhu, Hao Wang, Kaiwen Xue, Xuanliang Zhang, Yong Shan, et al. survey on latent reasoning. arXiv preprint arXiv:2507.06203, 2025. Jingwei Zuo, Maksim Velikanov, Ilyas Chahed, Younes Belkada, Dhia Eddine Rhayem, Guillaume Kunsch, Hakim Hacid, Hamza Yous, Brahim Farhat, Ibrahim Khadraoui, et al. Falcon-h1: family of hybrid-head language models redefining efficiency and performance. arXiv preprint arXiv:2507.22448, 2025."
        },
        {
            "title": "A EXTENDED RELATED WORKS",
            "content": "The field of methods for leveraging adaptive test-time computation with architectural modifications (e.g. von Oswald et al., 2025) and additional training methodologies (e.g. Guo et al., 2025) is vast and we refer the reader to Zhu et al. (2025) for detailed survey. Recurrent models have been cornerstone of machine learning for many years (Amari, 1972; Hopfield, 1982; Gers & Schmidhuber, 2000; Sutskever et al., 2008). depth-recurrent architectures can all be viewed as learning the gradient of an energy based model (LeCun & Huang, 2005). Gladstone et al. (2025) show energy based models can be scaled effectively. Recurrent mechanisms are shown to learn generalizable solutions to problems using ResNet (He et al., 2015) based architectures (Schwarzschild et al., 2021; Bansal et al., 2022; Anil et al., 2022; Schwarzschild, 2023; Bear et al., 2024). Yang et al. (2023); Giannou et al. (2023); Gatmiry et al. (2024) and Fan et al. (2024) study the potential theoretical benefits of recurrence at small scales. Many works study the impact of depth for transformers both theoretically and practically (Levine et al., 2020; Merrill et al., 2022; McLeish et al., 2025; Zuo et al., 2025; Merrill & Sabharwal, 2025; Csordás et al., 2025), it is still an open question how recurrent depth impacts the performance of transformers. Saunshi et al. (2025) demonstrate the power of recurrence by showing chain of thought (Wei et al., 2022) steps can be implicitly simulated in latent space using recurrence. Similar to latent thinking is continuous chain of thought (Hao et al., 2024), finetuning method to add recurrent behavior to pretrained language models, but training is limited as it requires sequential computations. Prior work on model surgery has heavily studied converting pretrained transformer language models into linear complexity attention models (Kasai et al., 2021; Zhang et al., 2024a; Mercat et al., 2024; Wang et al., 2024)."
        },
        {
            "title": "B ADDITIONAL TECHNICAL DETAILS",
            "content": "Optimization Similarly to Geiping et al. (2025), we train all models with truncated backpropagation (Williams & Peng, 1990; Mikolov et al., 2011), only recording gradients for at most the last 8 uses of the recurrent block. We train in bfloat16 mixed precision (Zamirai et al., 2020), with Flash Attention (Dao, 2023) and compile the model when training. Notably, to compile the model at scale we observe repeating the prebuilt inductor cache on each individual node removes deadlock errors and improves speed. We train all models on AMD MI300A accelerators (AMD, 2023), using distributed data parallel training. We use warmup-stable-decay learning rate scheduler (Zhai et al., 2022; Geiping & Goldstein, 2023), adjusting the warmup and decay periods to be appropriate for each experiment. We optimize with the official implementation of Muon2. Muon shards the Newton-Schulz calculations between all accelerators and then communicates them, overcoming some of the efficiency degradations compared to Adam. Combined with the fact that the models we are optimizing are smaller language models, we do not observe degradation in step time when using Muon. 2https://github.com/KellerJordan/Muon"
        },
        {
            "title": "C ADDITIONAL EXPERIMENTS",
            "content": "C.1 MODEL SURGERY ABLATIONS In Figure 9, we perform linear extrapolation of the loss curves shown in Figure 2, seeing the extrapolations intersect at approximately 950 billion tokens. We note this is more than likely an underestimate as there is still curvature in the loss curves. In Figure 10, we continue training the models from Figure 2, cooling the learning rate down over an additional 12 billion tokens. In Figure 11, we vary the emb_scale hyperparameter used by Geiping et al. (2025). Ours is using the emb_scale from the Huginn-0125 model, where as the line for Geiping et al. (2025) has been adjusted for this specific model shape. We see negligible difference. In Table 2, we extend Figure 2 with additional test recurrences for other language modelling tasks. Figure 9: Training loss for models initialized from Llama layers and Randomly. Here we extend Figure 2 including the linear extrapolations in log-log space. We note this is more than likely an underestimate of the point of intersection as there is still curvature in the loss curves. Figure 10: Training loss for models initialized from Llama layers and Randomly. Here, we extend Figure 2 by including cooldown for 12b additional tokens, taking this to total of 132b tokens. 19 Figure 11: Training loss for models initialized Randomly with different embedding scales over 120 billion tokens. We follow Geiping et al. (2025) when initializing models with scaled embeddings. However, we also ablate how much the scale impacts by using the same embedding scale from Huginn-0125 in this much smaller model, we find there to be minimal impact. Table 2: Initializing from pretrained model weights yields consistent gains across benchmarks. We evaluate our models trained for 120 billion tokens in zero shot setting seeing clear advantages to initializing from pretrained weights. Test Recurrence Arc-E Arc-C HS WG MMLU PIQA OBQA Random 25 25 50 25 50 25 Takase init 23.4 22.4 26.7 29.4 30.0 29. 23.8 26.6 30.8 34.0 35.1 35.0 28.6 31.6 37.8 45.4 47.8 48.3 Llama init 34.8 41.2 49.4 54.9 55.4 55.6 50.5 50.2 48.8 53.3 53.7 54.3 51.3 51.4 53.2 55.6 55.7 56. 36.1 41.2 50.7 54.5 55.8 56.1 41.6 48.4 54.5 59.2 60.2 60.4 22.9 23.0 23.4 24.4 24.8 25.0 22.9 23.2 24.0 25.4 25.3 25.3 55.2 58.3 63.4 67.9 68.7 68.9 62.5 65.9 69.7 72.3 73.1 72. 26.6 28.4 31.2 35.8 36.6 36.8 27.2 30.6 35.4 38.4 38.4 38.6 1 2 4 8 16 32 1 2 4 8 16 32 C.1.1 WHICH LAYERS TO TAKE? In Figure 12, we perform small search over which layers to select when forming recurrent model and removing layers. In Figure 13, we compare the layers we found to be optimal to dropping the least impactful layers using the ShortGPT method (Men et al., 2024). We find for training depth-recurrent models our selection is better. In Figure 14, we show additional results for models with more varied shapes. In Figure 15, we extend the prelude and coda to leverage all layers of the parent model, this yields only negligible improvement for the increased FLOPs. 20 Figure 12: We ablate which layers to select from Llama-1b. We measure the training loss on Fineweb-Edu with different layer selections from Llama-1b. We find taking early layers for the prelude, and later layers for the recurrent block and coda to be best. Figure 13: Comparison to prior methods for decreasing depth. We use the ShortGPT pruning method proposed by Men et al. (2024) to decrease the depth of the TinyLlama model. We train two non-recurrent models with this pruning method, reducing TinyLlamas depth to 8 and 16. We also train (4, 8, 4) model using our layer selection (See Table 9 and model using the layers prescribed by ShortGPT. We train on the nemotron dataset for approximately 25 billion tokens and find our layer selection to be better in terms of loss. 21 Figure 14: We ablate different layer selections and architectural choices for TinyLlama. We show the accuracy on evaluations at 32 recurrences after training on Fineweb-Edu. We ablate (2, 4, 2), (4, 8, 4) and (6, 10, 6) models with (6, 10, 6) keeping all of the layers of the depth 22 TinyLlama model. It is clear to see that increasing the number of layers in the recurrent block allows for the model to achieve higher accuracy, consistently beating the fixed depth model. However, having larger recurrent block does significantly increase the FLOPs used by the model. We also ablate swapping the linear adapter used by Geiping et al. (2025) and in our main results for an addition adapter (Add). We find that although training loss is higher the evaluation accuracy is approximately the same. Figure 15: Extending the prelude and coda leads to minimal performance improvements. We train (4, 8, 4) and (7, 8, 7) TinyLlama models with 25% linear curriculum. We see that adding an additional 6 layers leads to minimal final performance improvement for the additional FLOPs used. Left: Performance over training FLOPs on GSM8K. Right: Performance over training FLOPs on MATH. 22 C.2 SCHEDULING RECURRENCES ABLATIONS In Figure 16, we visually show the values our curriculum takes, looking like staircase from 1 to the maximum value over the curriculum period. In Figure 17, we extend Figure 3, showing more curriculum lengths and more test recurrences. In Figure 18, we show the result of scheduling the backpropagation depth over training. Figure 16: Visualization of our curriculum over training steps. We visualize curriculum with 3125 steps over training period of 6250 steps with final mean recurrence of 32. We show both linear and 1-sqrt schedules. flinear(tgt_depth, current_step) = ceil(tgt_depth (current_step/num_warmup_steps)) f1-sqrt(tgt_depth, current_step) = ceil(tgt_depth(1sqrt(1current_step/num_warmup_steps))) In the legend we include the number of recurrences used during the curriculum period, seeing the 1-sqrt schedule uses fewer recurrences. Figure 17: Scheduling the mean of the depth distribution is efficient in terms of data and compute. We extend Figure 3, showing more curriculum lengths on the left and more test recurrences on the right. We see the same as in Figure 3, that it is efficient in terms of data (steps) and compute to schedule the mean of the depth distribution. Figure 18: Validation loss of models with schedules maximum backpropagation depth. We see that scheduling the maximum backpropagation is efficient in terms of FLOPs spent but does lead to worse models in terms of steps. Figure 19: 1-sqrt vs. linear curricula for TinyLlama. We see there is little separating the curricula on per FLOP basis and therefor choose 75% 1-sqrt for our experiments in Figures 5 and 6 as it efficient while spending 25% of training at the maximum mean number of recurrences. Left: We plot accuracy over FLOPs for GSM8K. Right: We plot accuracy over FLOPs for MATH. C.3 HOW TO RETROFIT RECURRENCE ABLATIONS C.3.1 TINYLLAMA In Figure 20 we extend Figure 5, showing more train recurrences. In Figure 22, we plot Right of Figures 5 and 20 with an effective parameters x-axis, which can be viewed as proportional to FLOPs required for inference. In Figures 23 and 24 we show the GSM8K accuracy over training step for train recurrences 4, 8, 16 and 32. In Figure 21, we show evaluation results over FLOPs for MATH. In Figure 25, we plot Right of Figure 21 with an effective parameters x-axis, which can be viewed as proportional to FLOPs required for inference. In Figures 26 and 27 we show the MATH accuracy over training step for train recurrences 4, 8, 16 and 32. In Table 3, we show broad range of evaluations for the models in Figures 5 and 20. 24 Table 3: Final step accuracy for models shown in Figure 5 on broad range of evaluations. We also include TinyLlama-1.1b-3T Hugging Face which is our evaluations of the TinyLlama-1.1b-3T model downloaded from Hugging Face, i.e. the step 0 accuracy of the non-recurrent TinyLlama model. Test Rec Arc-E Arc-C HS WG MMLU PIQA OBQA GSM8K MATH 1 2 4 8 16 32 1 2 4 8 16 1 2 4 8 16 32 1 2 4 8 16 32 54.5 57.8 58.5 58.6 58.5 58.6 51.6 55.5 57.1 57.5 57.9 58.0 50.6 54.7 56.5 56.8 57.1 57.2 46.8 53.3 57.6 58.4 58.6 58. 33.2 34.8 34.6 34.0 33.8 33.8 32.8 34.3 34.2 34.1 34.3 34.4 31.3 34.3 35.8 35.6 35.6 35.4 28.7 32.1 33.8 35.0 35.6 35.6 4,8,4 (Train Recurrence=4) 39.7 42.3 42.9 43.2 43.2 43. 55.2 54.8 55.3 54.6 54.8 54.9 28.5 30.1 32.1 32.0 32.0 32.0 64.0 65.3 65.7 65.8 65.9 65.9 4,8,4 (Train Recurrence=8) 38.9 42.1 43.2 43.4 43.5 43.5 51.8 55.0 56.4 56.9 56.7 57. 26.8 31.2 34.0 34.1 34.1 34.1 64.3 65.7 66.1 66.2 66.4 66.2 4,8,4 (Train Recurrence=16) 39.4 42.7 44.6 44.8 44.9 44.9 52.4 54.6 55.2 55.7 56.3 56.2 25.0 30.2 34.0 35.0 34.9 34. 61.5 65.0 65.5 66.0 65.9 65.9 4,8,4 (Train Recurrence=32) 37.5 41.8 44.5 44.9 45.0 45.1 50.9 54.4 57.5 59.0 58.2 57.6 25.4 28.6 32.8 35.2 34.8 34.6 62.6 64.7 65.6 66.5 66.3 66. TinyLlama Non-Recurrent 32.4 33.6 33.4 33.6 33.6 33.6 32.0 32.0 33.2 32.6 32.8 32.8 32.4 32.2 34.4 34.4 34.2 34.2 31.6 31.8 33.4 32.4 32.0 32.2 17.6 32.4 38.0 37.7 37.7 37. 15.2 34.3 40.5 40.9 40.9 41.0 11.4 32.4 42.2 43.9 43.4 43.7 5.6 24.2 39.7 45.3 44.7 45.0 20.6 24.9 25.9 25.9 26.1 26.0 16.8 24.9 26.5 26.1 25.9 25.9 14.2 23.3 28.9 28.8 29.0 29. 9.1 19.1 25.6 27.2 26.9 27.1 57.5 34.9 45.3 55.8 33. 68.8 32.8 26.6 24.0 TinyLlama-1.1b-3T Hugging Face 55. 31.0 59.1 58.9 25.4 73.0 35. 1.6 2.3 25 Figure 20: Recurrence efficiently improves reasoning on GSM8K for TinyLlama. We train (4, 8, 4) and non-recurrent models for approximately 50 billion tokens of Nemotron-CC-Math-v1 data, extending Figure 5. Left: We plot accuracy over the number of FLOPs used during training. We see that recurrent models can efficiently outperform the non-recurrent baseline. Right: We plot accuracy over the number of recurrences for inference. We see the recurrent models are competitive with the fixed depth baseline and can outperform it by using more FLOPs. We plot each individual models accuracy over training and recurrence in full in Figure 23 and Figure 24. Evaluations on the final checkpoint over tasks shown in Table 1 are in Appendix Table 3. Figure 21: Recurrence efficiently improves reasoning on MATH for TinyLlama. Left: We plot accuracy over the number of FLOPs used during training. We see that recurrent models can efficiently outperform the non-recurrent baseline. Right: We plot accuracy over the number of recurrences for inference. We see the recurrent models are competitive with the fixed depth baseline and can outperform it by using more FLOPs. We plot each individual models accuracy over training and recurrence in full in Figure 26 and Figure 27. Evaluations on the final checkpoint over tasks shown in Table 1 are in Appendix Table 3. Figure 22: Recurrent models are competitive in terms of inference FLOPs for GSM8K. This is the same data as in Right of Figures 5 and 20 but replotted with an effective parameters x-axis, which can be viewed as proportional to FLOPs required for inference. Figure 23: Recurrence efficiently improves reasoning. Left: GSM8K accuracy over training step for train recurrence equal to 4 model. Right: GSM8K accuracy over training step for train recurrence equal to 8 model. Figure 24: Recurrence efficiently improves reasoning. Left: GSM8K accuracy over training step for train recurrence equal to 16 model. Right: GSM8K accuracy over training step for train recurrence equal to 32 model. 27 Figure 25: Recurrent models are competitive in terms of inference FLOPs for MATH. This is the same data as in 21 but replotted with an effective parameters x-axis, which can be viewed as proportional to FLOPs required for inference. Figure 26: Recurrence efficiently improves reasoning. Left: MATH accuracy over training step for train recurrence equal to 4 model. Right: MATH accuracy over training step for train recurrence equal to 8 model. Figure 27: Recurrence efficiently improves reasoning. Left: MATH accuracy over training step for train recurrence equal to 16 model. Right: MATH accuracy over training step for train recurrence equal to 32 model. 28 C.3.2 OLMO In Figure 28 we show evaluation results for OLMo on GSM8k. In Figure 30, we plot Right of Figure 28 with an effective parameters x-axis, which can be viewed as proportional to FLOPs required for inference. In Figures 31 and 32 we show the GSM8K accuracy over training step for train recurrences 4, 8, 16 and 32. In Figure 29, we extend Figure 6, showing more training recurrences. In Figure 33, we plot Right of Figure 29 with an effective parameters x-axis, which can be viewed as proportional to FLOPs required for inference. In Figures 34 and 35 we show the MATH accuracy over training step for train recurrences 4, 8, 16 and 32. In Table 4, we show broad range of evaluations for the models in Figure 28. Figure 28: Recurrence efficiently improves reasoning on GSM8K for OLMo. We train (4, 8, 4) and non-recurrent models for approximately 50 billion tokens of Nemotron-CC-Math-v1 data. Left: We plot accuracy over the number of FLOPs used during training. We see that recurrent models can efficiently outperform the non-recurrent baseline. Right: We plot accuracy over the number of recurrences for inference. We see the recurrent models are competitive with the fixed depth baseline and can outperform it by using more FLOPs. We plot each individual models accuracy over training and recurrence in full in Figure 31 and Figure 32. Evaluations on the final checkpoint over tasks shown in Table 1 are in Appendix Table 4. 29 Table 4: Final step accuracy for models shown in Figure 6 on broad range of evaluations. We also include OLMo-2-0425-1B-step1907359 Hugging Face which is our evaluations of the OLMo2-0425-1B-step1907359 model downloaded from Hugging Face, i.e. the step 0 accuracy of the non-recurrent OLMo model. Test Rec Arc-E Arc-C HS WG MMLU PIQA OBQA GSM8K MATH 1 2 4 8 16 32 1 2 4 8 16 1 2 4 8 16 32 1 2 4 8 16 32 61.6 63.8 63.8 63.7 63.6 63.6 60.9 64.0 64.8 65.0 65.0 65.0 57.6 62.3 64.1 65.2 65.2 65.2 56.9 61.7 65.2 66.0 66.1 66. 36.3 37.7 37.4 37.4 37.3 37.3 37.0 39.1 39.6 39.7 39.4 39.4 35.6 38.7 40.2 39.5 39.8 39.8 33.2 37.8 38.9 39.8 40.2 40.2 4,6,4 (Train Recurrence=4) 46.4 48.3 48.8 49.0 49.0 49. 56.8 58.2 57.7 57.2 57.1 57.2 36.4 37.9 38.3 38.2 38.1 38.1 68.4 69.5 69.6 69.9 70.0 70.0 4,6,4 (Train Recurrence=8) 45.9 48.4 49.2 49.4 49.5 49.5 55.8 58.4 59.9 59.3 59.4 59. 35.2 37.5 39.0 39.2 39.1 39.1 69.2 69.7 70.3 70.6 70.5 70.5 4,6,4 (Train Recurrence=16) 45.3 48.7 49.8 49.8 49.8 49.8 56.0 59.3 58.6 58.6 58.0 58.0 33.7 37.1 39.4 39.9 39.8 39. 68.1 67.8 69.4 69.8 70.0 70.0 4,6,4 (Train Recurrence=32) 44.4 47.9 49.4 49.6 49.8 49.8 54.1 55.5 59.2 58.2 57.8 57.5 30.9 36.5 39.3 40.4 40.5 40.5 66.8 68.1 68.8 69.6 69.9 70. Olmo-2 Non-Recurrent 33.6 35.6 36.2 35.8 35.8 35.8 32.0 34.4 34.4 34.4 34.2 34.2 36.8 34.0 34.2 34.6 34.8 34.8 33.4 34.2 33.2 34.4 34.4 34.4 24.3 37.4 38.7 39.5 39.4 40. 20.5 37.6 43.1 44.4 43.6 44.6 18.0 36.2 46.6 48.4 48.7 48.3 10.4 30.6 44.6 48.6 49.7 51.6 22.7 27.0 27.8 27.6 27.7 27.6 19.3 25.9 27.6 28.2 28.4 28.1 17.0 26.0 28.2 28.9 28.7 29. 11.2 22.0 28.1 29.4 29.1 29.3 65.2 40.8 50.7 60.0 40. 70.0 35.4 35.3 25.1 OLMo-2-0425-1B-step1907359 Hugging Face 67. 39.2 67.0 65.3 24.6 76.3 39. 3.6 3.4 30 Figure 29: Recurrence efficiently improves reasoning on MATH for OLMo. We train (4, 8, 4) and non-recurrent models for approximately 50 billion tokens of Nemotron-CC-Math-v1 data, extending Figure 6. Left: We plot accuracy over the number of FLOPs used during training. We see that recurrent models can efficiently outperform the non-recurrent baseline. Right: We plot accuracy over the number of recurrences for inference. We see the recurrent models are competitive with the fixed depth baseline and can outperform it by using more FLOPs. We plot each individual models accuracy over training and recurrence in full in Figure 34 and Figure 35. Evaluations on the final checkpoint over tasks shown in Table 1 are in Appendix Table 4. Figure 30: Recurrent models are competitive in terms of inference FLOPs for GSM8K. This is the same data as in 28 but replotted with an effective parameters x-axis, which can be viewed as proportional to FLOPs required for inference. Figure 31: Recurrence efficiently improves reasoning. Left: GSM8K accuracy over training step for train recurrence equal to 4 model. Right: GSM8K accuracy over training step for train recurrence equal to 8 model. 31 Figure 32: Recurrence efficiently improves reasoning. Left: GSM8K accuracy over training step for train recurrence equal to 16 model. Right: GSM8K accuracy over training step for train recurrence equal to 32 model. Figure 33: Recurrent models are competitive in terms of inference FLOPs for MATH. This is the same data as in 29 but replotted with an effective parameters x-axis, which can be viewed as proportional to FLOPs required for inference. Figure 34: Recurrence efficiently improves reasoning. Left: MATH accuracy over training step for train recurrence equal to 4 model. Right: MATH accuracy over training step for train recurrence equal to 8 model. 32 Figure 35: Recurrence efficiently improves reasoning. Left: MATH accuracy over training step for train recurrence equal to 16 model. Right: MATH accuracy over training step for train recurrence equal to 32 model. 33 C.3.3 LLAMA We build another set of models initialized from the weights of Llama-3.2-1B. For Llama, we construct (4, 6, 4) configurations, removing 2 layers (layers 4 and 5 with 0 indexing) from the pretrained model. This leaves approximately 850 million remaining parameters in the recurrent model, which equates to 87.5% of the pretrained models parameters. In Figure 36 we show evaluation results for Llama on GSM8k. In Figure 38, we plot Right of Figure 36 with an effective parameters x-axis, which can be viewed as proportional to FLOPs required for inference. In Figures 39 and 40 we show the GSM8K accuracy over training step for train recurrences 4, 8, 16 and 32. In Figure 37, we show evaluation results for Llama on MATH. In Figure 41, we plot Right of Figure 37 with an effective parameters x-axis, which can be viewed as proportional to FLOPs required for inference. In Figures 42 and 43 we show the MATH accuracy over training step for train recurrences 4, 8, 16 and 32. In Table 5, we show broad range of evaluations for the models in Figure 36. Figure 36: Recurrence efficiently improves reasoning on GSM8K for Llama. We train (4, 6, 4) and non-recurrent models for approximately 50 billion tokens of Nemotron-CC-Math-v1 data. Left: We plot accuracy over the number of FLOPs used during training. We see that recurrent models can efficiently outperform the non-recurrent baseline. Right: We plot accuracy over the number of recurrences for inference. We see the recurrent models are competitive with the fixed depth baseline and can outperform it by using more FLOPs. We plot each individual models accuracy over training and recurrence in full in Figure 39 and Figure 40. Evaluations on the final checkpoint over tasks shown in Table 1 are in Appendix Table 5. 34 Table 5: Final step accuracy for models shown in Figure 36 on broad range of evaluations. We also include Llama-3.2-1B Hugging Face which is our evaluations of the Llama-3.2-1B model downloaded from Hugging Face, i.e. the step 0 accuracy of the non-recurrent Llama model. Test Rec Arc-E Arc-C HS WG MMLU PIQA OBQA GSM8K MATH 1 2 4 8 16 32 1 2 4 8 16 1 2 4 8 16 32 1 2 4 8 16 32 57.7 60.9 60.4 60.3 60.2 60.2 55.3 59.6 61.0 61.4 61.4 61.4 55.2 60.6 61.8 61.9 61.9 61.9 53.1 58.2 61.2 61.5 61.4 61. 33.1 35.6 36.3 36.1 36.2 36.2 34.8 36.6 37.0 37.3 37.0 36.9 33.9 36.3 36.6 36.4 36.7 36.7 32.4 35.2 36.4 36.9 36.6 36.8 4,6,4 (Train Recurrence=4) 42.6 44.2 44.6 44.7 44.7 44. 54.5 54.1 55.8 55.2 55.2 55.2 33.3 35.3 36.1 36.1 36.1 36.1 65.6 66.3 67.2 67.5 67.5 67.5 4,6,4 (Train Recurrence=8) 42.4 44.9 45.5 45.6 45.7 45.7 53.5 55.0 54.6 53.9 54.4 54. 33.8 36.1 36.9 36.7 36.7 36.7 66.6 67.7 67.4 67.5 67.6 67.6 4,6,4 (Train Recurrence=16) 41.7 44.6 45.9 45.8 45.8 45.8 51.0 55.3 57.5 57.2 57.2 57.1 31.5 34.8 36.9 37.0 37.0 37. 65.4 67.1 67.0 67.0 66.9 66.9 4,6,4 (Train Recurrence=32) 40.2 44.4 46.4 46.2 46.2 46.2 50.2 52.7 56.4 57.1 57.0 56.8 28.2 34.4 37.9 38.5 38.5 38.4 64.9 66.5 67.2 67.3 67.5 67. Llama Non-Recurrent 33.6 33.0 33.2 33.0 33.0 33.0 32.2 33.2 34.4 34.4 34.2 34.0 34.4 33.4 34.2 33.8 34.0 34.0 31.2 31.0 31.4 31.2 31.6 31.6 26.4 36.2 39.4 40.1 40.1 40. 21.2 36.5 43.4 44.3 44.7 44.7 13.1 31.9 42.2 45.3 45.4 45.3 6.4 25.5 44.6 48.1 49.1 49.4 24.4 28.5 29.0 29.0 28.9 28.9 21.6 28.7 29.9 29.3 29.6 29.6 16.5 26.1 28.8 28.7 28.6 28. 10.5 23.0 30.9 31.3 31.6 31.5 62.6 38.2 45.8 57.1 38. 68.4 33.4 37.1 27.4 Llama-3.2-1B Hugging Face 61. 36.9 64.2 60.9 38.6 74.9 37. 4.9 4.3 35 Figure 37: Recurrence efficiently improves reasoning on MATH for Llama. We train (4, 6, 4) and non-recurrent models for approximately 50 billion tokens of Nemotron-CC-Math-v1 data. Left: We plot accuracy over the number of FLOPs used during training. We see that recurrent models can efficiently outperform the non-recurrent baseline. Right: We plot accuracy over the number of recurrences for inference. We see the recurrent models are competitive with the fixed depth baseline and can outperform it by using more FLOPs. We plot each individual models accuracy over training and recurrence in full in Figure 42 and Figure 43. Evaluations on the final checkpoint over tasks shown in Table 1 are in Appendix Table 5. Figure 38: Recurrent models are competitive in terms of inference FLOPs for GSM8K. This is the same data as in Right of Figure 36 but replotted with an effective parameters x-axis, which can be viewed as proportional to FLOPs required for inference. Figure 39: Recurrence efficiently improves reasoning. Left: GSM8K accuracy over training step for train recurrence equal to 4 model. Right: GSM8K accuracy over training step for train recurrence equal to 8 model. 36 Figure 40: Recurrence efficiently improves reasoning. Left: GSM8K accuracy over training step for train recurrence equal to 16 model. Right: GSM8K accuracy over training step for train recurrence equal to 32 model. Figure 41: Recurrent models are competitive in terms of inference FLOPs for MATH. This is the same data as in 37 but replotted with an effective parameters x-axis, which can be viewed as proportional to FLOPs required for inference. Figure 42: Recurrence efficiently improves reasoning. Left: MATH accuracy over training step for train recurrence equal to 4 model. Right: MATH accuracy over training step for train recurrence equal to 8 model. 37 Figure 43: Recurrence efficiently improves reasoning. Left: MATH accuracy over training step for train recurrence equal to 16 model. Right: MATH accuracy over training step for train recurrence equal to 32 model. 38 C.4 DATA MIXTURES In Table 6, we extend Table 1, including more test recurrences. We also include the Huginn-0125 evaluations conducted and published by Geiping et al. (2025) for comparison. Table 6: High quality data and curricula improve recurrent model performance across benchmarks. We see that the depth-recurrent models increases in accuracy over recurrence and achieves better accuracy when using two phrase training. For the non-recurrent baseline we see single phase training slightly outperforms two phrase training. This table extends Table 1. ** We note our context restricted and without chat template evaluations would more than likely decrease performance of Huginn-0125, hence we do not reevaluate the model under our conditions and instead state the best accuracies released by Geiping et al. (2025). We note that this model has over 4 as many parameters as our (4, 8, 4) models. Test Rec Arc-E Arc-C HS WG MMLU PIQA OBQA GSM8K MATH 25 25 50 25 50"
        },
        {
            "title": "Random",
            "content": "1 2 4 8 16 32 1 2 4 8 16 32 50.0 52.3 53.3 52.5 52.7 52.7 52.7 59.3 63.8 65.2 65.2 65.2 4,8,4 (Train Recurrence=4) - Single Phase 31.6 31.9 32.8 32.4 32.8 32. 31.6 34.8 36.9 37.4 37.7 37.7 50.8 55.8 57.7 58.1 58.2 58.2 58.0 60.5 60.9 60.8 61.0 61.1 35.7 39.2 39.6 39.5 39.4 39.4 69.3 70.9 71.3 71.2 71.2 71.4 38.8 38.8 39.0 38.6 38.6 38. 4,8,4 (Train Recurrence=4) - Two Phase 51.5 57.3 60.0 60.3 60.4 60.4 56.7 58.6 58.7 59.9 60.2 60.5 36.2 41.3 44.3 44.7 44.8 44.8 71.0 71.3 73.5 73.7 73.6 73.6 39.4 41.0 40.6 40.0 40.0 40. TinyLlama-1.1b-3T Static Depth - Single Phase 25.6 44.1 51.2 51.8 51.9 52.0 26.5 44.6 51.7 52.0 51.4 51.2 8.8 13.8 14.5 14.1 14.4 14.5 9.7 12.3 13.6 14.3 14.3 14.2 61. 35.2 58.9 60.5 45.1 71.4 39. 46.2 14.4 TinyLlama-1.1b-3T Static Depth - Two Phase 62.5 36.5 60. 59.6 44.4 72.9 39.4 45.2 12. TinyLlama-1.1b-3T (Zhang et al., 2024b) 55.7 31.0 59.1 58.9 25. 73.0 35.0 1.6 2.3 Huginn-0125** 3.5b parameters (Geiping et al., 2025) 1 34.9 69.9 24.1 38.2 29.3 65.2 49.4 59.4 23.6 31.4 55.3 76. 26.8 38.8 0.0 42.08 0.8 12."
        },
        {
            "title": "D HYPERPARAMETERS",
            "content": "We use learning rate of 5e5 for AdamW and 0.001 for Muon with weight decay of 1e4. We clip all gradients at 1. We use microbatch size of 8, global batch size of 1024 using 8 nodes of 4 AMD MI300A GPUs (AMD, 2023) by default. For the experiments shown in Section 4.1 and Section C.1 we use global batch size of 4096 on 64 nodes. For experiments shown in Section 4.2 and Section C.2 we use global batch size of 512 on 1 node. When using AdamW*, we use the same values as Geiping et al. (2025) for all hyper parameters which are not learning rate or weight decay."
        },
        {
            "title": "E PARAMETER COUNTS",
            "content": "In Table 7, we give exact parameter counts for non recurrent models. In Table 8, we give exact parameter counts for recurrent models. In Table 9, we detail the layers we take from the pretrained models to form our depth-recurrent models. Table 7: Exact parameter counts for non-recurrent models."
        },
        {
            "title": "Body",
            "content": "TinyLlama-1.1B-intermediate-step-1431k-3T 131, 072, 000 525, 336, 576 411, 041, 792 Llama-3.2-1B (untied) OLMo-2-0425-1B 968, 976, 384 973, 146, 112 1, 073, 874, 944 Model Name Table 8: Exact parameter counts for depth-recurrent models. Rec Block Embeddings Prelude Body Coda TinyLlama (2, 4, 2) TinyLlama 4, 8, 4) TinyLlama (6, 10, 6) Llama (4, 6, 4) OLMo (4, 6, 4) 131, 072, 000 131, 072, 000 131, 072, 000 525, 336, 576 411, 041, 792 486, 572, 032 704, 708, 608 968, 974, 336 851, 501, 056 939, 638, 121, 643, 008 176, 177, 152 264, 265, 728 243, 286, 016 268, 468, 224 243, 286, 016 352, 354, 304 440, 442, 880 364, 929, 024 402, 702, 336 121, 643, 008 176, 177, 152 264, 265, 728 243, 286, 016 268, 468, 224 Table 9: Layers taken from original non-recurrent models to form depth-recurrent models. Model Name Rec Block Prelude Body TinyLlama (2, 4, 2) TinyLlama 4, 8, 4) TinyLlama (6, 10, 6) Llama (4, 6, 4) OLMo (4, 6, 4) [0, 1] [0, 1, 2, 3] [0, 1, 2, 3, 4, 5] [0, 1, 2, 3] [0, 1, 2, 3] [16, 17, 18, 19] [10, 11, 12, 13, 14, 15, 16, 17] [6, 7, 8, 9, 10, 11, 12, 13, 14, 15] [6, 7, 8, 9, 10, 11] [6, 7, 8, 9, 10, 11] [20, 21] [18, 19, 20, 21] [16, 17, 18, 19, 20, 21] [12, 13, 14, 15] [12, 13, 14, 15]"
        }
    ],
    "affiliations": [
        "Columbia University",
        "ELLIS Institute Tübingen",
        "Lawrence Livermore National Laboratory",
        "Max Planck Institute for Intelligent Systems",
        "New York University",
        "Tübingen AI Center",
        "University of Maryland",
        "University of North Carolina"
    ]
}