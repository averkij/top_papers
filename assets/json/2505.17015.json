{
    "paper_title": "Multi-SpatialMLLM: Multi-Frame Spatial Understanding with Multi-Modal Large Language Models",
    "authors": [
        "Runsen Xu",
        "Weiyao Wang",
        "Hao Tang",
        "Xingyu Chen",
        "Xiaodong Wang",
        "Fu-Jen Chu",
        "Dahua Lin",
        "Matt Feiszli",
        "Kevin J. Liang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Multi-modal large language models (MLLMs) have rapidly advanced in visual tasks, yet their spatial understanding remains limited to single images, leaving them ill-suited for robotics and other real-world applications that require multi-frame reasoning. In this paper, we propose a framework to equip MLLMs with robust multi-frame spatial understanding by integrating depth perception, visual correspondence, and dynamic perception. Central to our approach is the MultiSPA dataset, a novel, large-scale collection of more than 27 million samples spanning diverse 3D and 4D scenes. Alongside MultiSPA, we introduce a comprehensive benchmark that tests a wide spectrum of spatial tasks under uniform metrics. Our resulting model, Multi-SpatialMLLM, achieves significant gains over baselines and proprietary systems, demonstrating scalable, generalizable multi-frame reasoning. We further observe multi-task benefits and early indications of emergent capabilities in challenging scenarios, and showcase how our model can serve as a multi-frame reward annotator for robotics."
        },
        {
            "title": "Start",
            "content": "Multi-SpatialMLLM: Multi-Frame Spatial Understanding with MultiModal Large Language Models Runsen Xu1,2, Weiyao Wang1, Hao Tang1, Xingyu Chen1, Xiaodong Wang1, Fu-Jen Chu1, Dahua Lin2, Matt Feiszli1, Kevin J. Liang1 1FAIR, Meta, 2The Chinese University of Hong Kong Work done at Meta Multi-modal large language models (MLLMs) have rapidly advanced in visual tasks, yet their spatial understanding remains limited to single images, leaving them ill-suited for robotics and other real-world applications that require multi-frame reasoning. In this paper, we propose framework to equip MLLMs with robust multi-frame spatial understanding by integrating depth perception, visual correspondence, and dynamic perception. Central to our approach is the MultiSPA dataset, novel, large-scale collection of more than 27 million samples spanning diverse 3D and 4D scenes. Alongside MultiSPA, we introduce comprehensive benchmark that tests wide spectrum of spatial tasks under uniform metrics. Our resulting model, Multi-SpatialMLLM, achieves significant gains over baselines and proprietary systems, demonstrating scalable, generalizable multi-frame reasoning. We further observe multi-task benefits and early indications of emergent capabilities in challenging scenarios, and showcase how our model can serve as multi-frame reward annotator for robotics. Date: May 23, 2025 Correspondence: runsxu@gmail.com Project Page: https://runsenxu.com/projects/Multi-SpatialMLLM Figure 1. We present Multi-SpatialMLLM, model capable of multi-frame spatial understanding, capability overlooked by previous spatial understanding research. Multi-SpatialMLLM can support different types of input referencing and outputs for various tasks. 1. Introduction Recent years have witnessed tremendous advances in multi-modal large language models (MLLMs), which have evolved into versatile AI assistants capable of wide array of visual tasks [17, 28, 36, 50, 61]. Despite these strides, deploying such assistants as mere brains in jar within digital platforms limits their potential. Instead, there is growing push to integrate MLLMs directly into real-world applications, such as robotics [7, 37] and autonomous vehicles [59], to facilitate natural interactions with the environment. This shift imposes requirement for human-like spatial understanding. However, current MLLMs often struggle with surprisingly basic spatial understandingeven confusing left and right [58]. Previous works [9, 14] attribute these deficiencies pri1 Table 1. Comparison of spatial understanding datasets. Our MultiSPA is the first large-scale dataset for multi-frame spatial understanding, with diverse referencing and output formats. We generate 27M samples here and can scale further if needed."
        },
        {
            "title": "Split",
            "content": "Multi-Frames GT Annotation"
        },
        {
            "title": "Output",
            "content": "# Images # QAs BLINK [21] UniQA-3D [70] Q-Spatial [39] VSR [41] SpatialVLM [9] SpatialRGPT [14]"
        },
        {
            "title": "MultiSPA",
            "content": "eval eval eval train, eval train, eval train, eval train, eval Ø Ø Ø Ø Only eval Only eval dot dot, semantic semantic semantic semantic mask MCQ MCQ scalar true/false qual., MCQ, scalar qual., scalar 877 2450 271 11K 10M 1M 572 2450 271 11K 2B 8.7M dot, coord., semantic qual., MCQ, scalar, coord., vec. 1.1M 27M+ marily to shortage of specialized training data, and addresses it by incorporating spatial data into model training, leading to notable improvements. However, these works focus on single-image scenarios, thus restricting the models perception to static field-of-view without any dynamic information. We instead aim for more comprehensive spatial understanding, enabling MLLMs to reason across multiple images. Inspired by the long-standing Structure-fromMotion problem [25] from 3D computer vision, we focus on integrating three fundamental capabilities into MLLMs: (1) depth perception, to infer relative distances and threedimensional structures (2) visual correspondence, to match overlapping regions across images for consistent scene association, and (3) dynamic perception, to perceive selfmovement (camera motion) and object motion. The challenge in achieving this goal is the scarcity of suitable training data. Because manual annotation at the required scale can be expensive, prior works [9, 14] have resorted to single-view data from in-the-wild images [35], relying on off-the-shelf modules such as monocular depth estimators [27] and open-vocabulary object detectors [55]. However, this approach often produces noisy annotations. Moreover, our objective is to collect multi-frame spatial data, which requires both spatial and temporal alignmentan open challenge [64, 67] in unstructured in-thewild images. Consequently, we leverage existing annotated 3D [16] and 4D [31, 34, 51] datasets for data collection. We develop data engine that samples image pairs with uniform overlap distribution, then backprojects spatially and temporally aligned point clouds to establish pixel correspondences. Leveraging these correspondences in addition to camera movement and projection information, we create high quality questionanswer pairs via diverse, LLMgenerated templates. In contrast to previous methods that rely on semantic labels [9] or object masks [14] for referencing, our framework supports multiple modalities, including visual point annotations, pixel coordinates, and semantic labels, thus broadening potential downstream applications. The collected data encompasses both qualitative and quantitative spatial information, ranging from text to scalar, 2D pixel locations, and 3D displacement vectors. In total, we curate dataset named MultiSPA consisting of more than 27 million samples, which we use to train our Multi-SpatialMLLM, as illustrated in Fig. 1. To the best of our knowledge, MultiSPA is the first large-scale dataset dedicated to multi-frame spatial understanding. Alongside the dataset, we introduce novel MultiSPA benchmark to evaluate multi-frame spatial reasoning in MLLMs, covering diverse tasks and output formats under unified metric. Extensive experiments show that Multi-SpatialMLLM significantly outperforms both base models and proprietary systems, exhibiting scalable and generalizable multi-frame spatial understanding. We further observe that multi-task training on MultiSPA offers notable advantages, and identify preliminary signs of emergent behavior in more challenging spatial tasks. Lastly, we demonstrate our models potential as multi-frame reward annotator for robotics. 2. Related Work Multi-modal large language models. We refer to multimodal large language models (MLLMs) or vision-language models (VLMs) as large language models (LLMs) extended to handle image inputs [4, 12, 17, 22, 24, 40, 42, 46, 69], typically by incorporating an image encoder [13, 29, 52] converting images into tokens, which are then projected into the LLMs [3, 61, 63] text latent space and processed alongside text tokens. Training MLLMs uses the same language modeling objective as text-only LLMs, generally following two-stage paradigm. First, pre-trained image encoder is aligned with pre-trained LLMs latent representations, either by fine-tuning projector [17, 42, 69] or by jointly training the entire model from scratch [1, 19, 28]. Second, part or all of the model is fine-tuned with highquality instruction-following data [10, 15, 43] to enable user-guided responses. Thanks to Internet-scale imagetext data, e.g. captioning or OCR corpora [56, 57], MLLMs have demonstrated remarkable performance on tasks like image captioning and multi-modal dialogue. However, these training data lack sufficient spatial annotations, resulting in deficient spatial understanding. We address this limitation by further fine-tuning existing MLLMs on newly collected spatial data. Notably, we preserve the original model architecture to maintain the wide range of capabilities and application scenarios derived from large-scale pre-training. 2 Spatial understanding benchmarks for MLLMs. Researchers have explored deploying MLLMs on real-world platforms such as robotics [7, 19, 32, 37] and autonomous vehicles [59], applications requiring human-like spatial understanding to perceive and interact with the environment effectively. However, spatial understanding is complex, fundamental ability that is difficult to define formally, so researchers have introduced various benchmarks targeting different aspects of spatial reasoning for evaluation purposes. Most prior works focus on single-image spatial understanding, primarily assessing inter-object spatial relations [20, 30, 39, 41, 58, 62, 68] or spatial recognition inspired by cognitive science [53]. Some benchmarks extend beyond single images: BLINK [21] and UniQA-3D [70] evaluate spatial relationships across image pairs, while VSIBench [66] introduces video-based scene-level spatial reasoning. Though these share some similar tasks with ours, such as qualitative camera movement estimation and keypoint matching, our proposed benchmark includes additional tasks like object movement perception, and supporting more diverse input and output formats, instead of just the limited multiple-choice format. Improving MLLMs for spatial understanding. Existing benchmarks highlight the limitations of MLLMs in spatial understanding, prompting several recent works [6, 8, 9, 14, 48, 60]. However, most of these focus on single-image understanding. SpatialVLM [9] was the first to identify the lack of spatial training data as key limitation, demonstrating significant improvements by fine-tuning MLLMs on curated spatial dataset. SpatialRGPT [14] extended this approach by introducing mask-based reference and incorporating depth images. SpatialPIN [48] explored an alternative strategy, avoiding model fine-tuning and instead leveraging specialized perception models to extract spatial information for MLLMs. Unlike prior efforts, we focus on enabling MLLMs to reason across multiple images for spatial understanding by fine-tuning them on our newly collected dataset. Concurrently, SAT [54] also explores multi-frame spatial reasoning, but it relies on simulated data, potentially introducing sim-to-real gap. In contrast, our dataset is significantly larger, derived from real-world images across diverse scenarios, and covers broader range of spatial reasoning tasks. See Tab. 1 for comparison of our dataset with other popular spatial datasets and benchmarks. 3. MultiSPA Dataset and Benchmark in In this section, we introduce our MultiSPA dataset Sec. 3.1, describe the data generation pipeline in Sec. 3.2, and present the MultiSPA benchmark in Sec. 3.3. 3.1. MultiSPA Dataset Tasks definitions. We aim to equip MLLMs with the ability to integrate multiple images for spatial understanding. Building on the three fundamental capabilies discussed in Sec. 1, we introduce the following five tasks to generate training data: 1) Depth perception, 2) Visual correspondence, 3) Camera movement perception, 4) Object movement perception, and 5) Object size perception. Figure 1 shows examples of these five tasks. Referencing and output types. As summarized in Tab. 1, we reference specific pixels or objects in spatial QA data using visual dots (points) or semantic labels, as opposed to masks[14] to avoid additional dependency on segmentation module [33]. Additionally, we introduce pixel coordinates as straightforward referencing method that preserves the original images without requiring extra annotations. Beyond referencing, most existing datasets constrain spatial tasks to multiple-choice formats or limit outputs to qualitative or scalar quantitative answers. We broaden these restrictions by incorporating diverse quantitative outputs such as pixel coordinates and displacement vectors. Detailed descriptions of each task are provided as follows, with examples of each in the supplementary material. Depth perception. We divide this task into two subtasks: direct depth estimation and depth comparison. In the first task, single pixel is specified in an image, and the model must estimate its depth. In the second, two pixels are specified, and the model must identify the one closer to the camera. Both subtasks support referencing pixels either via visually annotated dots or pixel coordinates. Visual correspondence. Given two images and pixel location in the first image, the model must identify the corresponding pixel with the same 3D position in the second image, either qualitatively or quantitatively. In the qualitative version, the pixel is annotated visually in both images, and an additional three pixels in the second image are labeled to form multiple-choice question. The models goal is to pick the correct label. In the quantitative version, only the pixel coordinates are specified, and the model must output the corresponding coordinates in the second image. Camera movement perception. Given two images, the model must estimate the relative movement of the camera from the first view to the second, including both translation and orientation. We define multiple output levels, In the simplest variant, the from coarse to fine-grained. model must only identify the cameras movement direction along three translational axes: right or left, forward or backward, and up or down, as well as its rotation direction in two axes: rotating left or right and tilting up or down. more challenging variant requires the model to estimate scalar values of the overall translation distance or rotation angle. Finally, the most detailed form requires predicting the cameras displacement vector. In total, we have nine question types for this category. Object movement perception. Given two images and pixel location on specific object (or object part) in the first 3 image, the model estimates the pixels overall translation distance or, at finer level, the pixels displacement vector with respect to the first view. The camera may remain still or move during capture, and pixel referencing can be done either via visual annotations or pixel coordinates. Object size perception. Given several images of target object, the model estimates the objects height, width, and length. We treat this as higher level of spatial understanding compared with the previous four tasks. The model must integrate information across all images to infer the objects size. We use semantic labels to refer to the object. 3.2. MultiSPA Data Generation Data Format. strategies [12, 17, 43], we format our data as QA pairs as: Following common MLLM fine-tuning User: <image>...<image>{description}{question} Assistant: {answer} We use GPT-4o[50] to generate diverse templates for task descriptions, questions, and answers. Please refer to the supplementary material for detailed templates for each task. To facilitate answer extraction, we enclose the answer in backticks (). For numerical answers of metric length, we use millimeters as the unit and round to the nearest integer. For pixel coordinates, we normalize the values to maintain compatibility with varying image resolutions as follows: xnorm = (cid:106) (cid:107) 1000 , ynorm = (cid:107) 1000 (cid:106) (1) where x, are the original pixel coordinates, and W, are the width and height of the image, respectively. Source datasets. We leverage existing annotated scene datasets for high-quality data collection. Specifically, we use the 4D datasets Aria Digital Twin (ADT)[51] and Panoptic Studio (PStudio)[31], with 3D tracking annotations from the TAPVid3D [34] dataset for the object movement perception task, and the 3D dataset ScanNet [16] for other tasks. Our data generation pipeline can be used for other datasets as long as they have the same spatial annotations. Further details are in the supplementary material. 3.2.1. Static Scene Data Generation Visible points calculation. For each scene, ScanNet [16] provides reconstructed point cloud Pscene = {pW }, where each point pW = (X, Y, Z)T is in world coordinates. Each RGB image Ii has depth map Di, an extrinsic matrix Ei (camera to world), and an intrinsic matrix Ki. We transform and project each point pW onto Ii via: = (Ei)1 pC (cid:20)pW (cid:21) , = Ki pC [2] . pC pC pC [0] [1] [2] (2) 4 Figure 2. Overlap ratio calculation of image pairs. We maintain all visible points of image i, denoted as Pi, by selecting those whose projected coordinates (u, v) lie within the image bounds and are not occluded: 0 < pC [2] < Di(u, v). (3) Depth perception data generation. To create depth perception data, we randomly sample images for each scene. For each image Ii, we sample one or two visible points from Pi, record their 2D coordinates (u, v) and corresponding depth pC [2], and fill in the templates to construct QA pairs. Image pairs sampling. Although depth estimation is performed on single images, we also require image pairs with overlapping regions to construct multi-frame spatial understanding data. For each scene, we define the overlap ratio between two images as the IoU of their visible points: Overlap(i, j) = Pi Pj Pi Pj . (4) We only consider image pairs with overlap ratios between 6% and 35%, as ratios outside this range indicate either too little or too much shared content. Figure 2 visualizes the calculation of the overlap ratio. Notably, the overlap ratio exhibits long-tailed distribution, where most pairs have low overlap. We do not use all image pairs, and to achieve balanced sampling, we divide the overlapping pairs into bins based on their overlap ratios. We then evenly allocate target number of samples among these bins while prioritizing bins with fewer samples. For each task, we sample image pairs with different random seeds to ensure diversities. More details are in the supplementary material. Visual correspondence data generation. For an sampled image pair (Ii, Ij), we randomly select one point from their co-visible points Pi Pj and use its projected pixel coordinates in both images to construct the QA pair. Camera movement perception data generation. In the ScanNet[16] dataset, the camera coordinate system is dej, zi j, yi = E1 > 0, and zi j[0 : 3,3], and its norm, di fined with its origin at the top-left of the image, where the x-axis points to the right, the y-axis points downward, and the z-axis points forward. For an image pair (Ii, Ij), we compute the relative camera pose with respect to the first Ej R44. The translation component image as Ei is given by the displacement vector (di j)T = [xi j] = Ei j, represents the overall translation distance. When xi > 0, yi > 0, we label the camera motion as right, down, and forward, respectively; otherwise, the movement is considered left, up, or backward. To determine orientation, we measure rotation angles around the gravity direction and the tilt relative to the ground plane (details in the supplementary material). Finally, we format all these spatial parameters into QA templates to construct the camera movement data. Object size perception data generation. For this task, we require set of images that not only share overlapping regions but also jointly cover the entire target object. To ensure that the model learns to reason across all images, only the complete image set should cover the objects full dimensions, while no proper subset does. To achieve this, we propose BFS-based minimum-coverage-set search algorithm that iteratively explores image combinations with early pruning. For each object in ScanNet[16], we use the size of the target objects 3D bounding box as its height, width, and length and combine these with the searched image sets to construct QA pairs. More details are in the supplementary material. 3.2.2. Dynamic Data Generation t=1, camera extrinsics {Et}T TAPVid3D [34] provides temporally aligned point cloud tracking sequences {Pt}T t=1, along with the corresponding video frames {It}T t=1, and intrinsics {Kt}T t=1 for the ADT[51] and PStudio datasets[31]. We use these datasets to construct object movement perception QA pairs. We randomly select one point from the tracked sequences, then choose two images (Ii, Ij) to form the image pair. Similar to the camera movement data generation procedure, we compute each points displacement vector and translation distance between these two frames using the camera extrinsics. To ensure diversity, we adopt two additional modules described as follows to sample the points and image pairs (more in supplementary material). Rigid body segmentation. Point clouds from TAPVid3D typically belong to the same object or local region, but different parts may be unevenly represented (see Fig. 3). For instance, moving human often has more points on the torso than on the arms. Random sampling yields distribution skewed toward the dominant body part, which follows single movement pattern. Thus, we devise clusteringbased rigid body segmentation method to group the point clouds according to inter-point distance changes over time, and sample each group separately to enhance diversity. Figure 3. Visualization of rigid body segmentation results. Image pairs sampling. Given selected point that appears in frames, one could form up to (T 1) image pairs. However, similar to ScanNet, these pairs exhibit long-tailed distribution of motion magnitudes. We therefore bin the image pairs by the object translation distances and perform balanced sampling for each bin, ensuring diversity across small and large displacements. 2 3.3. MultiSPA Benchmark Using both ScanNet [16] and TAPVid3D [34], we employ our proposed data generation pipeline to create over 27M QA samples from 1.1M unique images. For each subtask in MultiSPA, we hold out 300 samples as evaluation sets, resulting in total of 7,800 benchmark samples. We ensure that the images in the benchmark come from scenes or scenarios distinct from those in the training split. Evaluation metric. Our MultiSPA benchmark supports diverse answer formats. The required answer format is specified in the question, and regular expression is used to extract the answer from model responses. Accuracy is calculated using task-specific criteria. For qualitative and multiple-choice answers, exact string matching is used. For scalar and vector responses, if the L2 norm of the error is within 20% of the ground truths L2 norm, the answer is correct. For pixel coordinates, prediction is correct if within 5% image width pixels of the ground truth. 4. Experimental Results 4.1. Multi-Frame Spatial Understanding We evaluate the multi-frame spatial understanding of our Multi-SpatialMLLM using our proposed MultiSPA benchmark and discuss the scalability of our model. Implementation details. Our preliminary studies show that InternVL2 [61] exhibits stronger instruction-following capabilities than other popular MLLMs (e.g., LLaVAOneVision [36], VILA [40]). Hence, we adopt the 8B InternVL2 model as our base, fine-tuning it on the MultiSPA training split. Specifically, we employ LoRA [26] with rank = 16 to update the LLM backbone, while freezing the Table 2. Evaluation on the MultiSPA benchmark. Our Multi-SpatialMLLM significantly outperforms baselines across both qualitative and quantitative subtasks, demonstrating an average 36% gain and surpassing even larger proprietary models. Multi-SpatialMLLM InternVL-8B InternVL-13B InternVL-26B Claude-3.5 Gemini-2.0 GPT-4o"
        },
        {
            "title": "Average",
            "content": "56.11 (+35.68)"
        },
        {
            "title": "Camera Orientation",
            "content": "74.00 (+24.50) 75.33 (+71.99) 49.00 (+47.33) 90.00 (+56.67)"
        },
        {
            "title": "Direction\nDegree",
            "content": "90.83 (+42.66) 45.50 (+42.16)"
        },
        {
            "title": "Direction\nDistance\nVector",
            "content": "85.89 (+33.56) 42.33 (+28.00) 18.00 (+17.67)"
        },
        {
            "title": "Distance\nVector",
            "content": "40.42 (+31.58) 12.92 (+10.42)"
        },
        {
            "title": "Object Perception",
            "content": "20.43 49.50 3.34 1.67 33.33 48.17 3.34 52.33 14.33 0.33 8.84 2. 21.47 51.83 1.34 2.33 44.00 47.67 2.00 47.55 14.33 0.00 8.09 4. 21.36 50.50 2.50 1.67 44.00 49.34 5.17 50.22 13.00 0.67 8.75 3. 27.50 30.31 28.87 38.17 34.84 1.33 54.67 62.17 10. 55.11 16.33 0.33 8.50 1.92 57.00 28.67 5.67 73.00 62.17 16.34 51.89 14.00 0. 9.42 2.33 54.84 22.50 2.00 67.67 58.84 17.50 54.78 13.67 0.00 8.92 5."
        },
        {
            "title": "Size",
            "content": "49.11 (+21.66) 27.45 34.45 26.89 46.11 42. 40.44 image encoder and projection layer. We use cosine learning rate scheduler with lr = 4 105 and the AdamW [45] optimizer. Due to computational constraints, we train on subset of MultiSPA (3M QA samples) for one epoch, mixed with 60K general image-based instruction-following samples to preserve the base models original abilities. The training is conducted on 24 nodes of 832G V100 GPUs with batch size of 192, taking 50 hours to complete. Baselines. We include the official versions of InternVL2 [61] with different model sizes as baselines to investigate how our proposed training data improves Additionally, we evaluate three popperformance. ular proprietary models, including Claude-3.5-Sonnet20241022 [2], Gemini2.0-Flash [18], and GPT-4o20241120 [50], as representative models to highlight the limitations of multi-frame spatial understanding even in SOTA MLLMs. Since these baselines often either refuse to answer certain questions or produce responses failing to adhere to the required answer format, we employ additional prompts to encourage them to provide answers or guess values when uncertain. We also use GPT-4 for post-processing to ensure their outputs conform to the prescribed answer format and can be extracted for evaluation accordingly. MultiSPA benchmark. Table 2 summarizes model accuracy on our MultiSPA benchmark, grouping similar subtasks for clarity (e.g., Direction under Camera Translation refers to predicting the cameras movement along three axes). We observe that most existing MLLMs have limited multi-frame spatial understanding ability, performing slightly above random (about 5060% accuracy) on qualitative tasks such as depth comparison, camera orientaFigure 4. Scalability of Multi-SpatialMLLM. tion, and camera translation direction. Even worse, they effectively fail entirely on tasks requiring quantitative outputs such as coordinate-based visual correspondence and camera or object movement vectors. By contrast, our Multi-SpatialMLLM significantly improves performance across all tasks, achieving an average 36% gain over the base model. On relatively easier qualitative tasks, it gets 8090% accuracy (compared to about 50% for the base model) and outperforms all proprietary models. Even on challenging tasks like predicting camera movement vectors, our model attains 18% accuracy, whereas all other baselines remain near zero. It is notable that our model has only 8B parameters, which is likely far fewer than those of closed-source models. Yet, with the MultiSPA dataset, it matches or even exceeds their performance, validating the effectiveness of our proposed data. Scalability of Multi-SpatialMLLM. Certain tasks like es6 timating the cameras displacement vector remain significant challenge even for Multi-SpatialMLLM. This is perhaps unsurprising, given their high difficulty and unsolved nature despite being longstanding task in 3D computer vision. We hypothesize that the lower performance we observe is in part because of higher data requirements of these tasks due to their challenging nature. To verify this, we investigate whether the multi-frame spatial understanding ability of Multi-SpatialMLLM is scalable. We select the challenging camera movement vector prediction task as case study and gradually increase the training data from 0.5M to 2.5M QA samples, fine-tuning different sizes of the InternVL model. (Figure 4). We observe consistent improvements by adding more data and increasing model capacity. With 2.5M samples, the 26B variant achieves around 44% accuracy, compared with the base models 0.67%. These findings encouragingly suggest that further scaling up training data and model capacity holds promise for even more powerful spatial understanding. 4.2. Generalization of Multi-SpatialMLLM We study the generalization ability of Multi-SpatialMLLM by evaluating it on the held-out external benchmarks and on standard VQA benchmarks. We also demonstrate the multitask benefits introduced by our MultiSPA data. BLINK benchmark. To verify whether our models learned multi-frame spatial understanding generalizes to other datasets outside of our fine-tuning data, we perform zero-shot evaluation on BLINK [21], diverse benchmark for assessing MLLM perception (Tab. 3). We focus on four splits relevant to spatial reasoning: Visual Correspondence (V.C.), Relative Depth (R.D.), Multi-View Reasoning (M.V.), and Spatial Reasoning (S.R.). Note that our model never sees BLINK images during fine-tuning, and BLINKs image resolutions and distributions differ from our training data. We find that all baselines fail on the Multi-View Reasoning task, and the InternVL base models especially struggle with Visual Correspondence. In contrast, our Multi-SpatialMLLM achieves almost 90% accuracy on these tasks and delivers an average 26.4% improvement over the base model, even outperforming several proprietary models. This result demonstrates that the multiframe spatial understanding learned by our model is transferable across datasets. We do not observe gains on the Spatial Reasoning task, possibly because this task focuses on topological position relations between two objects within single image, which differs significantly from our multiframe training data geared toward integrating spatial cues from multiple viewpoints. Standard VQA benchmarks. We evaluate our MultiSpatialMLLM on several popular standard VQA benchmarks, as shown in Tab. 4. These benchmarks target various MLLM capabilities, such as general perception Table 3. Evaluation on the BLINK[21] benchmark."
        },
        {
            "title": "Model",
            "content": "Avg. V.C. R.D. M.V. S.R."
        },
        {
            "title": "75.7\nGemini-2.0\n67.7\nClaude-3.5\n73.8\nGPT-4o\n57.9\nInternVL-8B\n64.0\nInternVL-13B\n61.4\nInternVL-26B\nMulti-SpatialMLLM 84.3",
            "content": "88.4 74.4 84.9 39.0 52.3 47.1 89.5 83.9 63.7 71.0 71.8 71.0 78.2 79.8 42.9 54.1 53.4 49.6 49.6 44.4 94.7 83.9 75.5 81.8 76.2 85.3 79.7 74.8 Table 4. Evaluation on standard VQA benchmarks."
        },
        {
            "title": "POPE VizWiz OCRVQA MathVista MMStar CCBench",
            "content": "InternVL-8B 84.5 85.3 Ours 33.2 30.7 42.7 42.7 58.5 57.6 61.1 59.7 77.3 75. (POPE [38] and VizWiz [23]), optical character recognition (OCRVQA [49]), reasoning (MathVista [47] and MMStar [11]), and Chinese VQA (CCBench [44]). The results show rough parity across the benchmarks, indicating that our model retains most of its original standard VQA proficiency and can be used as general-purpose MLLM, without being overfit to just multi-frame spatial reasoning. Table 5. Model performance w./wo. multi-task training."
        },
        {
            "title": "Single Task\nMultiple Tasks",
            "content": "9.30 18.00 (+8.70) 17.50 22.04 (+4.56) Multi-task generalization and synergy. While each of the tasks proposed in Sec. 3.1 are focused on narrower subgoal, ultimately the aim is to collectively improve multiframe spatial understanding; we thus prefer that our training data has synergistic generalization effects, as opposed to balancing potentially antagonistic tasks individually. We observe that this is indeed the case by comparing training on just the 500K samples from the camera-movement subset (without any other task data) versus the full training set of 3M samples: we observe that the additional data from the additional tasks indeed increases the accuracy on camera movement questions from 9.3% to 18.0%. We further compare two training configurations for object movement: (1) dataset of 400K object movement samples alone, and (2) the same 400K object movement samples plus 400K additional samples from camera movement, visual correspondence, and depth estimation. The average accuracy on object movement subtasks increases from 17.5% to 22.04% with the additional data, as shown in Tab. 5. Importantly, these extra 400K samples only involve ScanNet [16] images, whereas the object movement data originate from PStudio [31] and ADT [51], and the two sets do not share question types or data sources. This improvement demonstrates the spatial understanding learned from different datasets and task types can transfer, highlighting an ad7 Figure 5. Demonstrations of Multi-SpatialMLLM in zero-shot robotics tasks. Our model accurately identifies static objects and predicts movement distances, aligning with the ground truth. It exhibits potential for novel applications like multi-frame reward annotation. ditional scalability dimension beyond merely data volume and model capacitynamely, task diversity. Table 6. Multiple-choice visual correspondence accuracy. Model Size Encoder Size LLM Size Acc. (Baseline v.s. Hard) 4.3. Emergence of spatial understanding We have shown that our models multi-frame spatial understanding is scalable (Fig. 4 and Tab. 5). However, we also investigate whether certain spatial reasoning abilities only appear in sufficiently large models, mirroring the emergent phenomena observed in text-based LLMs [65]. We explore this through our multiple-choice visual correspondence task for preliminary study. By default, when generating distractor pixels in the second image, we pick them randomly; we denote this as Easy, as distractors may be quite far from the answer. For more challenging scenario, we deliberately select distractors near the correct pixel, thus requiring higher discriminative power from the model (Hard version). We train various sizes of the base models on these Hard samples and then test on the Easy samples, to gauge whether they can effectively learn from the Hard data. Table 6 shows that only the 26B variant improves over the base model, whereas both the 8B and 13B models (the latter equipped with larger 6B vision encoder) fail to learn effectively from the Hard samples and even show reduced performance. As reference, training the 8B model on the same number of Easy samples yields 93.33% accuracy on the test set. These findings suggest that learning difficult spatial tasks may require sufficiently large model capacitypotentially pointing to an emergent aspect of multi-frame spatial understanding. We leave deeper investigation of this interesting phenomenon to future work. 4.4. Demonstrations of Multi-SpatialMLLM In Fig. 1, we demonstrate our Multi-SpatialMLLMs multiview spatial understanding. We further test its real-world performance on newly collected images of robot arm 8B 13B 26B 300M 6B 6B 7B 7B 20B 33.3 v.s. 25.67 44.0 v.s. 42.67 44.0 v.s. 82.33 stacking cubes. These robot scenes are out-of-distribution because our training set does not include any robotic scenario. As shown in Fig. 5, when asked about the movement of static blue cube, GPT-4o and the base model respond incorrectly, while ours accurately identifies no movement. Multi-Frame reward annotator. Prior works [9, 14] have shown that MLLMs with spatial understanding can act as reward annotators in robot learning, but they only handle single-frame inputs. In contrast, our model supports multiframe tasks such as perceiving object movement across consecutive frames. In Fig. 5, we provide Frame-0 and subsequent frames (Frame-1 to Frame-5), then query our model about the objects displacement. Our model successfully estimates an increasing trend in movement distances, aligning with the ground truth. Though the predicted values are not exact (due to differing resolutions and domains), these results underscore our models generalization ability and highlight potential novel applications as reward annotator or evaluator for robot tasks involving multi-frame spatial understanding, such as move the object by meters. 5. Conclusion In this work, we extend MLLMs spatial understanding to multiple frames, capability overlooked in previous research. We develop data generation pipeline that produces the first large-scale dataset and benchmark, MultiSPA, dedicated to this goal. Our extensive experiments demonstrate the effectiveness, scalability, and generalization of the 8 proposed Multi-SpatialMLLM, revealing key observations such as multi-task benefits and emergent behaviors in challenging spatial tasks. The model also opens up new applications, including acting as multi-frame reward annotator. We discuss limitations in the supplementary material."
        },
        {
            "title": "Appendix",
            "content": "A. MultiSPA Data Samples B. MultiSPA Data Templates C. Details of Source Datasets D. Image Pairs Sampling E. Rotation Angles F. BFS-Based Minimum-Coverage-Set Search G. Clustering-Based Rigid Body Segmentation H. Limitations A. MultiSPA Data Samples 9 9 9 9 10 10 10 Our MultiSPA dataset has 26 subtasks in total. Each task with an example is shown from Fig. 6 to Fig. 14. B. MultiSPA Data Templates Due to paper length limits, we only show part of the templates in Listing 4. Other templates are similar to those shown in this supplementary material. Please refer to our website for the complete lists of templates. C. Details of Source Datasets ScanNet. ScanNet [16] is an RGB-D dataset containing more than 1,500 indoor scans. Each scan provides reconstructed point clouds, 3D camera poses, camera intrinsics, depth maps, and 3D instance and semantic segmentation masks. Our data generation pipeline utilizes all these annotations, though segmentation masks are optional if object perception data is not required. PStudio. The CMU Panoptic Studio dataset [31] comprises 65 sequences (5.5 hours total) of multiple people interacting with one another or with objects, captured within light stage. It offers multi-view images, 3D body skeletons, and facial landmark annotations. ADT. Aria Digital Twin [51] is an egocentric video dataset recorded with Aria glasses. It contains 200 sequences of real-world indoor activities, each with precise 6DoF camera poses, 3D human poses, 2D image segmentations, and depth maps, as well as digital twin environment. TAPVid3D. TAPVid3D is dataset for tracking 3D points in space. It provides temporal 3D point tracking, constructed from PStudio, ADT, and DriveTrack [5]. It leverages official annotations to produce temporally aligned 3D point sequences, along with camera pose sequences and intrinsics. We use these annotations for our data generation. Note that we exclude the DriveTrack split because its camera poses are insufficiently accurate. D. Image Pairs Sampling To ensure balanced selection of image pairs based on their overlap ratio, we adopt the procedure as follows. First, we separate pairs with zero overlap and randomly sample predefined number of them. Next, we partition all nonzerooverlap pairs into bins according to their overlap ratio. We then distribute the sampling quota across bins in proportion to the number of bins, sorting them by bin size in ascending order to prevent smaller bins from being overshadowed by larger ones. Finally, we either sample or exhaust each bin, carrying over any unused quota to subsequent bins. This step balances pairs of different overlap levels, mitigating issues caused by long-tail distributions. The main content of the full algorithm is shown in Listing 1. E. Rotation Angles Beyond translation, we estimate two orientation angles: yaw and pitch. We do not model roll, as it typically remains small in real-world use cases (e.g., autonomous vehicles, robotics, wearable devices). Formally, let R44 be the camera pose in world coordinate, which has the zaxis aligned with the gravity direction, and its upper-left 3 3 submatrix: = E[0:3, 0:3]. (5) We then extract yaw and pitch by focusing on the cameras forward (i.e., z-) axis: zfwd = 0 0 . 1 (6) Yaw is defined as the angle of this rotated z-axis in the horizontal plane, measured around the gravity axis: yaw = arctan 2(cid:0)zfwd[1], zfwd[0](cid:1) 180 π . Pitch is the angle of zfwd relative to the ground plane: pitch = arcsin (cid:16) zfwd[2] zfwd (cid:17) 180 π . (7) (8) With these two angles, we can determine the camera rotates left or rotates right, and tilt up or titl down. F. BFS-Based Minimum-Coverage-Set Search To ensure that an objects full dimensions are captured across multiple images, we develop breadth-first search (BFS) algorithm that identifies minimal sets of images whose combined coverage meets each dimensions size requirement. In particular, for each axis (height, width, length), we track which subset of object points are visible per image. If the difference between the minimum and maximum coordinates of all selected points along that axis meets target threshold (based on the objects 3D bounding box size), we consider it covered. Our BFS proceeds in two phases at each iteration: 1. Phase A: Coverage check. We examine the current sets and mark any that fully cover the object on the chosen axis. These sets are recorded as minimal, and any set that is superset of previously found minimal set is pruned. 2. Phase B: Expansion. We expand the remaining (uncovered) sets to the next level by appending additional images, while pruning those that cannot possibly achieve coverage in deeper levels. This process continues until either no further expansion is possible or the maximum number of images is reached. The final result is collection of minimal sets that together span the objects relevant dimension. Although we include pruning steps, the search still becomes expensive when considering sets of three or more images. Hence, we only use two images for object size perception in our data. Listing 2 shows simplified implementation. G. Clustering-Based Rigid Body Segmentation In TAPVid3D, all points in sequence often belong to the same object or scene region, but they can be unevenly distributed (e.g., human torso versus arms). To sample diverse motion patterns, we segment the point cloud into multiple rigid bodies, each undergoing distinct motion. Our method accumulates inter-point distance changes over time and applies hierarchical clustering to identify coherent groups. We also filter groups with too less points to avoid noise. Listing 3 is simplified code snippet. H. Limitations Despite our focus on enabling multi-frame spatial understanding, most of our experiments employ only two-view scenarios. However, our data generation pipeline naturally extends to additional frames. Future work may explore scaling beyond pairs of images, leveraging more views for enhanced spatial reasoning. Another limitation is that although we observe signs of the emergent phenomenon, further investigation is required to clarify what exact spatial abilities drive such emergence. 10 1 2 4 5 6 7 8 10 11 12 13 14 16 17 18 19 20 22 23 24 25 26 28 29 30 31 32 34 35 36 37 38 40 41 42 43 44 46 47 48 49 50 52 53 54 55 56 58 59 def sample_dataframe(df, all_overlap_samples, non_overlap_samples, overlap_min=0, overlap_max=100, interval=1): # 1) Sample pairs with overlap == 0 non_overlap_df = df[df[\"overlap\"] == 0].copy() sampled_non_overlap_df = (non_overlap_df if len(non_overlap_df) <= non_overlap_samples else non_overlap_df.sample(n=non_overlap_samples)) # 2) Partition the remaining pairs (overlap != 0) into bins remaining_df = df[df[\"overlap\"] != 0].copy() bins = np.arange(overlap_min, overlap_max + interval, interval) remaining_df[\"overlap_group\"] = pd.cut(remaining_df[\"overlap\"], bins=bins, include_lowest=True) remaining_df.dropna(subset=[\"overlap_group\"], inplace=True) bin_groups = [] for ovlp_bin, group_df in remaining_df.groupby(\"overlap_group\"): bin_groups.append((ovlp_bin, group_df)) if not bin_groups: final_df = sampled_non_overlap_df.copy() final_df.drop(columns=[\"overlap_group\"], errors=\"ignore\", inplace=True) return final_df # 3) Distribute all_overlap_samples evenly across bins = len(bin_groups) base_quota = all_overlap_samples // remainder = all_overlap_samples % bin_quotas = [base_quota] * for in range(remainder): bin_quotas[i] += # 4) Sort bins by size (ascending) and sample bin_data = [] for i, (ovlp_bin, group_df) in enumerate(bin_groups): bin_data.append({ \"group_df\": group_df, \"quota\": bin_quotas[i], \"size\": len(group_df) }) bin_data.sort(key=lambda x: x[\"size\"]) sampled_df = pd.DataFrame() leftover = 0 for info in bin_data: group, quota, size = info[\"group_df\"], info[\"quota\"], info[\"size\"] current = quota + leftover if size <= current: sampled_df = pd.concat([sampled_df, group], ignore_index=True) leftover = current - size else: sampled_df = pd.concat([sampled_df, group.sample(n=current)], ignore_index=True) leftover = 0 if leftover > 0: print(f\"Warning: leftover {leftover} samples not used.\") # 5) Combine sampled bins with zero-overlap samples final_df = pd.concat([sampled_df, sampled_non_overlap_df], ignore_index=True) final_df.drop(columns=[\"overlap_group\"], errors=\"ignore\", inplace=True) return final_df Listing 1. The image pairs sampling algorithm for static scene data 11 1 2 4 5 6 7 8 10 11 12 13 14 16 17 18 19 20 22 23 24 25 26 28 29 30 31 32 34 35 36 37 38 40 def compute_coverage(points, mask, axis): \"\"\"Returns the min-to-max spread along axis for points indicated by mask.\"\"\" if not mask.any(): return 0.0 coords = points[mask][:, axis] return coords.max() - coords.min() def covers_dimension(coverage, target_dim, tol): \"\"\"Checks if coverage is within tolerance of the target dimension.\"\"\" return abs(coverage - target_dim) <= tol * target_dim def bfs_min_coverage(images, visibility, points, obj_mask, axis, target_dim, tol, max_k=2): \"\"\" Finds minimal image sets up to size max_k that meet coverage criteria along axis. images is list of candidate frames, visibility maps frame->boolean mask, obj_mask indicates the object points in points. \"\"\" # Prepare BFS queue: each item is (set_of_images, combined_mask, last_idx) queue = [] for i, img in enumerate(images): mask_i = visibility[img] & obj_mask queue.append(([img], mask_i, i)) solutions = [] = 1 while <= max_k and queue: next_level = [] for combo, comb_mask, last_idx in queue: cov = compute_coverage(points, comb_mask, axis) if covers_dimension(cov, target_dim, tol): solutions.append(combo) elif < max_k: # Expand only if we have not reached max_k for in range(last_idx + 1, len(images)): mask_j = visibility[images[j]] & obj_mask next_mask = comb_mask mask_j next_level.append((combo + [images[j]], next_mask, j)) queue = next_level += 1 return solutions Listing 2. Simplifed version of BFS-based minimum-coverage-set search with pruning. 12 1 2 3 5 6 7 8 9 11 12 13 14 15 17 18 19 20 21 23 24 25 26 27 29 30 31 def smooth_distance_changes(dist_t, dist_prev, smooth_factor=0.01): \"\"\"Zeroes out small distance changes to reduce noise.\"\"\" diff = np.abs(dist_t - dist_prev) return np.where(diff > smooth_factor, diff, 0) def rigid_body_segmentation(points, thr=0.1, smooth_factor=0.01): \"\"\" points: Shape (T, N, 3), with time steps & points. thr: Threshold for clustering distance. smooth_factor: Ignored small changes. Returns: list of groups, each group is list of point indices. \"\"\" T, N, _ = points.shape cum_loss = np.zeros((N, N)) # Accumulate distance changes over time for in range(1, T): dist_t = squareform(pdist(points[t])) dist_prev = squareform(pdist(points[t - 1])) cum_loss += smooth_distance_changes(dist_t, dist_prev, smooth_factor) # Hierarchical clustering = linkage(squareform(cum_loss), method=\"average\") labels = fcluster(Z, thr, criterion=\"distance\") # Group points by label groups = [] for label_id in range(1, labels.max() + 1): group = np.where(labels == label_id)[0].tolist() groups.append(group) return groups Listing 3. Rigid body segmentation with smoothing and hierarchical clustering. 13 1 2 4 5 6 7 8 10 11 12 13 14 16 17 18 19 20 22 23 24 25 26 28 29 30 31 32 34 35 36 37 38 40 41 42 43 44 46 47 48 49 50 52 53 # Depth Estimation-Dot TASK_DESCRIPTION = [ \"<image>nGiven an image with an annotated point, complete the question-answer task.\", ] TEMPLATES = { \"questions\": [ \"What is the depth of the annotated point in the image (in mm)?\", ], \"answers\": [ \"The depth of the annotated point is {depth} mm.\", ] } # Visual Correspondence Multiple-Choice TASK_DESCRIPTION = [ \"Image-1: <image>nImage-2: <image>nGiven these two images, find the corresponding points between them.\", ] TEMPLATES = { \"questions\": [ \"Which point labeled A, B, C, or in Image-2 corresponds to the circle point in Image-1? Please answer with the correct label from Image-2.\", ], \"answers\": [ \"The correct point is labeled {correct_label}.\", ] } # Object Perception TASK_DESCRIPTION = [ \"Assume the scene remains unchanged. Your task is to determine the spatial properties based on the images. You need to integrate and analyze information from all provided images to get the answer.\", ] QUESTION_TEMPLATES = [ \"What is the {dimension} (in millimeters) of the {object_category} itself commonly visible in these images?\", ] ANSWER_TEMPLATES = [ \"The {dimension} is approximately {value_mm} millimeters.\", ] # Object Movement-Coordinate-Distance TASK_DESCRIPTION = [ \"Image-1: <image>nImage-2: <image>nGiven two images, analyze the movements of objects in the images and the cameras that captured them. The movement should be relative to the first image. Note that the objects in the images and the camera may or may not have moved.\", ] QUESTION_TEMPLATES = [ \"How far did the point at [ {x1} , {y1} ] in Image-1 travel between the two shots? The coordinates [ , ] are normalized to 0-1 and scaled by 1000, with [ 0 , 0 ] at the top-left corner. The x-axis represents the width, and the y-axis represents the height.\" ] ANSWER_TEMPATES = [ \"The point traveled total of {total_distance} mm.\", ] Listing 4. Part of the templates used by MultiSPA dataset 14 Figure 6. Data samples of depth perception. 15 Figure 7. Data samples of visual correspondence. Figure 8. Data samples of camera movement-translation direction. 16 Figure 9. Data samples of camera movement-orientation direction. Figure 10. Data samples of camera movement-orientation degree. 17 Figure 11. Data samples of camera movement-translation distance and vector. 18 Figure 12. Data samples of object perception. 19 Figure 13. Data samples of object movement-distance. 20 Figure 14. Data samples of object movement-vector."
        },
        {
            "title": "References",
            "content": "[1] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: In NeurIPS, visual language model for few-shot learning. 2022. 2 [2] anthropic. https : / / www . anthropic . com / news / claude - 3 - 5 - sonnet, 2024. 6 Claude 3.5 sonnet. [3] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang Zhu. Qwen technical report. arXiv:2309.16609, 2023. 2 [4] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: versatile vision-language model for understanding, localization, text reading, and beyond. arXiv:2308.12966, 2023. 2 [5] Arjun Balasingam, Joseph Chandler, Chenning Li, Zhoutong Zhang, and Hari Balakrishnan. Drivetrack: benchmark for In CVPR, long-range point tracking in real-world videos. 2024. [6] Mahtab Bigverdi, Zelun Luo, Cheng-Yu Hsieh, Ethan Shen, Dongping Chen, Linda Shapiro, and Ranjay Krishna. Perception tokens enhance visual reasoning in multimodal language models. arXiv:2412.03548, 2024. 3 [7] Kevin Black, Noah Brown, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai, Lachy Groom, Karol Hausman, Brian Ichter, Szymon Jakubczak, Tim Jones, Liyiming Ke, Sergey Levine, Adrian Li-Bell, Mohith Mothukuri, Suraj Nair, Karl Pertsch, Lucy Xiaoyang Shi, James Tanner, Quan Vuong, Anna Walling, Haohuan Wang, and Ury Zhilinsky. Pi0: vision-languagehttps : action flow model for general robot control. //physicalintelligence.company/blog/pi0, 2024. 1, 3 [8] Wenxiao Cai, Iaroslav Ponomarenko, Jianhao Yuan, Xiaoqi Li, Wankou Yang, Hao Dong, and Bo Zhao. Spatialbot: Precise spatial understanding with vision language models. In ICRA, 2025. 3 [9] Boyuan Chen, Zhuo Xu, Sean Kirmani, Brain Ichter, Dorsa Sadigh, Leonidas Guibas, and Fei Xia. Spatialvlm: Endowing vision-language models with spatial reasoning capabilities. In CVPR, 2024. 1, 2, 3, 8 [10] Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua Lin. Sharegpt4v: Improving large multi-modal models with better captions. In ECCV, 2024. 2 [11] Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Jiaqi Wang, Yu Qiao, Dahua Lin, et al. Are we on the right way for evaluating large vision-language models? arXiv:2403.20330, 2024. 7 [12] Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, et al. How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites. Science China Information Sciences, 2024. 2, 4 [13] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. In CVPR, 2024. 2 [14] An-Chieh Cheng, Hongxu Yin, Yang Fu, Qiushan Guo, Ruihan Yang, Jan Kautz, Xiaolong Wang, and Sifei Liu. Spatialrgpt: Grounded spatial reasoning in vision-language models. In NeurIPS, 2024. 1, 2, 3, 8 [15] Erfei Cui, Yinan He, Zheng Ma, Zhe Chen, Hao Tian, Weiyun Wang, Kunchang Li, Yi Wang, Wenhai Wang, Xizhou Zhu, Lewei Lu, Tong Lu, Yali Wang, Limin Wang, https : / / Yu Qiao, and Jifeng Dai. sharegpt4o.github.io/, 2024. 2 sharegpt4o. [16] Angela Dai, Angel Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and Matthias Nießner. Scannet: Richly-annotated 3d reconstructions of indoor scenes. In CVPR, 2017. 2, 4, 5, 7, [17] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. Instructblip: Towards generalpurpose vision-language models with instruction tuning. NeurIPS, 20243. 1, 2, 4 [18] Google Deepmind. Gemini 2.0: our new ai model for the agentic era. https://blog.google/technology/ google - deepmind / google - gemini - ai - update - december - 2024 / ceo - message, 2024. 6 [19] Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, et al. Palm-e: An embodied multimodal language model. arXiv:2303.03378, 2023. 2, 3 [20] Mengfei Du, Binhao Wu, Zejun Li, Xuanjing Huang, and Zhongyu Wei. Embspatial-bench: Benchmarking spatial understanding for embodied tasks with large vision-language models. arXiv:2406.05756, 2024. 3 [21] Xingyu Fu, Yushi Hu, Bangzheng Li, Yu Feng, Haoyu Wang, Xudong Lin, Dan Roth, Noah Smith, Wei-Chiu Ma, and Ranjay Krishna. Blink: Multimodal large language models can see but not perceive. In ECCV, 2024. 2, 3, 7 [22] Tao Gong, Chengqi Lyu, Shilong Zhang, Yudong Wang, Miao Zheng, Qian Zhao, Kuikun Liu, Wenwei Zhang, Ping Luo, and Kai Chen. Multimodal-gpt: vision and language model for dialogue with humans. arXiv:2305.04790, 2023. [23] Danna Gurari, Qing Li, Abigale Stangl, Anhong Guo, Chi Lin, Kristen Grauman, Jiebo Luo, and Jeffrey Bigham. Vizwiz grand challenge: Answering visual questions from blind people. In CVPR, 2018. 7 [24] Jiaming Han, Renrui Zhang, Wenqi Shao, Peng Gao, Peng Xu, Han Xiao, Kaipeng Zhang, Chris Liu, Song Wen, Ziyu Guo, et al. Imagebind-llm: Multi-modality instruction tuning. arXiv:2309.03905, 2023. 2 [25] R. Hartley and A. Zisserman. Multiple View Geometry in Computer Vision. Cambridge University Press, 2003. 2 [26] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. ICLR, 2022. 5 [27] Mu Hu, Wei Yin, Chi Zhang, Zhipeng Cai, Xiaoxiao Long, Hao Chen, Kaixuan Wang, Gang Yu, Chunhua Shen, and Shaojie Shen. Metric3d v2: versatile monocular geometric foundation model for zero-shot metric depth and surface normal estimation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2024. 2 [28] Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma, Tengchao Lv, Lei Cui, Owais Khan Mohammed, Qiang Liu, et al. Language is not all you need: Aligning perception with language models. arXiv:2302.14045, 2023. 1, [29] Gabriel Ilharco, Mitchell Wortsman, Ross Wightman, Cade Gordon, Nicholas Carlini, Rohan Taori, Achal Dave, Vaishaal Shankar, Hongseok Namkoong, John Miller, Hannaneh Hajishirzi, Ali Farhadi, and Ludwig Schmidt. Openclip, 2021. 2 [30] Justin Johnson, Bharath Hariharan, Laurens Van Der Maaten, Li Fei-Fei, Lawrence Zitnick, and Ross Girshick. Clevr: diagnostic dataset for compositional language and elementary visual reasoning. In CVPR, 2017. 3 [31] Hanbyul Joo, Hao Liu, Lei Tan, Lin Gui, Bart Nabbe, Iain Matthews, Takeo Kanade, Shohei Nobuhara, and Yaser Sheikh. Panoptic studio: massively multiview system for social motion capture. In Proceedings of the IEEE international conference on computer vision, 2015. 2, 4, 5, 7, 9 [32] Moo Jin Kim, Karl Pertsch, Siddharth Karamcheti, Ted Xiao, Ashwin Balakrishna, Suraj Nair, Rafael Rafailov, Ethan Foster, Grace Lam, Pannag Sanketi, et al. Openvla: An opensource vision-language-action model. In CoRL, 2024. 3 [33] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, et al. Segment anything. In ICCV, 2023. [34] Skanda Koppula, Ignacio Rocco, Yi Yang, Joe Heyward, Joao Carreira, Andrew Zisserman, Gabriel Brostow, and Carl Doersch. Tapvid-3d: benchmark for tracking any point in 3d. arXiv preprint arXiv:2407.05921, 2024. 2, 4, 5 [35] Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Uijlings, Ivan Krasin, Jordi Pont-Tuset, Shahab Kamali, Stefan Popov, Matteo Malloci, Alexander Kolesnikov, et al. The open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale. IJCV, 2020. 2 23 [36] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, et al. Llava-onevision: Easy visual task transfer. arXiv:2408.03326, 2024. 1, 5 [37] Xinghang Li, Peiyan Li, Minghuan Liu, Dong Wang, Jirong Liu, Bingyi Kang, Xiao Ma, Tao Kong, Hanbo Zhang, and Huaping Liu. Towards generalist robot policies: What matters in building vision-language-action models. arXiv preprint arXiv:2412.14058, 2024. 1, 3 [38] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, and Ji-Rong Wen. Evaluating object hallucination in large vision-language models. In EMNLP, 2023. [39] Yuan-Hong Liao, Rafid Mahmood, Sanja Fidler, and David Acuna. Reasoning paths with reference objects elicit quantitative spatial reasoning in large vision-language models. arXiv:2409.09788, 2024. 2, 3 [40] Ji Lin, Hongxu Yin, Wei Ping, Pavlo Molchanov, Mohammad Shoeybi, and Song Han. Vila: On pre-training for visual language models. In CVPR, 2024. 2, 5 [41] Fangyu Liu, Guy Emerson, and Nigel Collier. Visual spatial reasoning. TACL, 2023. 2, 3 [42] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In NeurIPS, 2023. [43] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In CVPR, 2024. 2, 4 [44] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, Kai Chen, and Dahua Lin. Mmbench: Is your multi-modal model an all-around player? arXiv:2307.06281, 2023. 7 [45] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. ICLR, 2019. 6 [46] Haoyu Lu, Wen Liu, Bo Zhang, Bingxuan Wang, Kai Dong, Bo Liu, Jingxiang Sun, Tongzheng Ren, Zhuoshu Li, Hao Yang, et al. Deepseek-vl: towards real-world visionlanguage understanding. arXiv:2403.05525, 2024. 2 [47] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematIn ical reasoning of foundation models in visual contexts. ICLR, 2024. 7 [48] Chenyang Ma, Kai Lu, Ta-Ying Cheng, Niki Trigoni, and Andrew Markham. Spatialpin: Enhancing spatial reasoning capabilities of vision-language models through prompting and interacting 3d priors. In NeurIPS, 2024. [49] Anand Mishra, Shashank Shekhar, Ajeet Kumar Singh, and Anirban Chakraborty. Ocr-vqa: Visual question answering by reading text in images. In ICDAR, 2019. 7 [50] OpenAI. Gpt-4o. https://openai.com/index/ hello-gpt-4o/, 2024. 1, 4, 6 [51] Xiaqing Pan, Nicholas Charron, Yongqian Yang, Scott Peters, Thomas Whelan, Chen Kong, Omkar Parkhi, Richard Newcombe, and Yuheng Carl Ren. Aria digital twin: new benchmark dataset for egocentric 3d machine perception. In ICCV, 2023. 2, 4, 5, 7, 9 [66] Jihan Yang, Shusheng Yang, Anjali Gupta, Rilyn Han, Li Fei-Fei, and Saining Xie. Thinking in space: How multimodal large language models see, remember, and recall spaces. arXiv:2412.14171, 2024. 3 [67] Jianing Yang, Alexander Sax, Kevin Liang, Mikael Henaff, Hao Tang, Ang Cao, Joyce Chai, Franziska Meier, and Matt Feiszli. Fast3r: Towards 3d reconstruction of 1000+ images in one forward pass. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2025. [68] Wenyu Zhang, Wei En Ng, Lixin Ma, Yuwen Wang, Jungqi Zhao, Boyang Li, and Lu Wang. Sphere: hierarchical evaluation on spatial perception and reasoning for visionlanguage models. arXiv preprint arXiv:2412.12693, 2024. 3 [69] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Enhancing visionlanguage understanding with advanced large language models. arXiv:2304.10592, 2023. 2 Minigpt-4: [70] Yiming Zuo, Karhan Kayan, Maggie Wang, Kevin Jeon, Jia Deng, and Thomas Griffiths. Towards foundation modarXiv preprint els for 3d vision: How close are we? arXiv:2410.10799, 2024. 2, 3 [52] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In ICML, 2021. 2 [53] Santhosh Kumar Ramakrishnan, Erik Wijmans, Philipp Kraehenbuehl, and Vladlen Koltun. Does spatial cognition emerge in frontier models? arXiv:2410.06468, 2024. 3 [54] Arijit Ray, Jiafei Duan, Reuben Tan, Dina Bashkirova, Rose Hendrix, Kiana Ehsani, Aniruddha Kembhavi, Bryan Plummer, Ranjay Krishna, Kuo-Hao Zeng, et al. Sat: Spatial aptitude training for multimodal language models. arXiv preprint arXiv:2412.07755, 2024. [55] Tianhe Ren, Qing Jiang, Shilong Liu, Zhaoyang Zeng, Wenlong Liu, Han Gao, Hongjie Huang, Zhengyu Ma, Xiaoke Jiang, Yihao Chen, et al. Grounding dino 1.5: Advance the edge of open-set object detection, 2024. 2 [56] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. NeurIPS, 2022. 2 [57] Christoph Schuhmann, Andreas Kopf, Richard Vencu, Theo Coombes, and Romain Beaumont. Laion coco: 600m synthetic captions from laion2b-en, 2022. 2 [58] Fatemeh Shiri, Xiao-Yu Guo, Mona Far, Xin Yu, Reza Haf, and Yuan-Fang Li. An empirical analysis on spatial reasonIn EMNLP, ing capabilities of large multimodal models. 2024. 1, 3 [59] Chonghao Sima, Katrin Renz, Kashyap Chitta, Li Chen, Hanxue Zhang, Chengen Xie, Jens Beißwenger, Ping Luo, Andreas Geiger, and Hongyang Li. Drivelm: Driving with graph visual question answering. In ECCV, 2024. 1, 3 [60] Chan Hee Song, Valts Blukis, Jonathan Tremblay, Stephen Tyree, Yu Su, and Stan Birchfield. Robospatial: Teaching spatial understanding to 2d and 3d vision-language models for robotics. arXiv:2411.16537, 2024. 3 [61] InternLM Team. Internlm: multilingual language model https : / / with progressively enhanced capabilities. github.com/InternLM/InternLM, 2023. 1, 2, 5, [62] Shengbang Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Manoj Middepogu, Sai Charitha Akula, Jihan Yang, Shusheng Yang, Adithya Iyer, Xichen Pan, et al. Cambrian1: fully open, vision-centric exploration of multimodal llms. arXiv preprint arXiv:2406.16860, 2024. 3 [63] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv:2302.13971, 2023. 2 [64] Qianqian Wang, Yifei Zhang, Aleksander Holynski, Alexei Efros, and Angjoo Kanazawa. Continuous 3d arXiv preprint perception model with persistent state. arXiv:2501.12387, 2025. 2 [65] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. Emergent abilities of large language models. arXiv:2206.07682, 2022."
        }
    ],
    "affiliations": [
        "FAIR, Meta",
        "The Chinese University of Hong Kong"
    ]
}