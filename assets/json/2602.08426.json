{
    "paper_title": "Prism: Spectral-Aware Block-Sparse Attention",
    "authors": [
        "Xinghao Wang",
        "Pengyu Wang",
        "Xiaoran Liu",
        "Fangxu Liu",
        "Jason Chu",
        "Kai Song",
        "Xipeng Qiu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Block-sparse attention is promising for accelerating long-context LLM pre-filling, yet identifying relevant blocks efficiently remains a bottleneck. Existing methods typically employ coarse-grained attention as a proxy for block importance estimation, but often resort to expensive token-level searching or scoring, resulting in significant selection overhead. In this work, we trace the inaccuracy of standard coarse-grained attention via mean pooling to a theoretical root cause: the interaction between mean pooling and Rotary Positional Embeddings (RoPE). We prove that mean pooling acts as a low-pass filter that induces destructive interference in high-frequency dimensions, effectively creating a \"blind spot\" for local positional information (e.g., slash patterns). To address this, we introduce Prism, a training-free spectral-aware approach that decomposes block selection into high-frequency and low-frequency branches. By applying energy-based temperature calibration, Prism restores the attenuated positional signals directly from pooled representations, enabling block importance estimation using purely block-level operations, thereby improving efficiency. Extensive evaluations confirm that Prism maintains accuracy parity with full attention while delivering up to $\\mathbf{5.1\\times}$ speedup."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 9 ] . [ 1 6 2 4 8 0 . 2 0 6 2 : r Prism: Spectral-Aware Block-Sparse Attention Xinghao Wang1,4 Pengyu Wang1,4 Xiaoran Liu1,2,4 Fangxu Liu3 Jason Chu3 Kai Song3 Xipeng Qiu1,2,4, 1Fudan University 2Shanghai Innovation Institute 3ByteDance Inc. 4OpenMOSS Team"
        },
        {
            "title": "Abstract",
            "content": "Block-sparse attention is promising for accelerating long-context LLM pre-filling, yet identifying relevant blocks efficiently remains bottleneck. Existing methods typically employ coarse-grained attention as proxy for block importance estimation, but often resort to expensive token-level searching or scoring, resulting in significant selection overhead. In this work, we trace the inaccuracy of standard coarse-grained attention via mean pooling to theoretical root cause: the interaction between mean pooling and Rotary Positional Embeddings (RoPE). We prove that mean pooling acts as low-pass filter that induces destructive interference in high-frequency dimensions, effectively creating \"blind spot\" for local positional information (e.g., slash patterns). To address this, we introduce Prism, training-free spectral-aware approach that decomposes block selection into high-frequency and lowfrequency branches. By applying energy-based temperature calibration, Prism restores the attenuated positional signals directly from pooled representations, enabling block importance estimation using purely block-level operations, thereby improving efficiency. Extensive evaluations confirm that Prism maintains accuracy parity with full attention while delivering up to 5.1 speedup. Repository: https://github.com/xinghaow99/prism Correspondence: xinghaowang22@m.fudan.edu.cn,xpqiu@fudan.edu.cn"
        },
        {
            "title": "Introduction",
            "content": "The capacity to process extensive contexts is defining characteristic of modern Large Language Models (LLMs), unlocking applications ranging from repository-level code understanding to hour-long video understanding [1, 2]. However, handling such long contexts is non-trivial, as the self-attention mechanism scales quadratically with sequence length [3], resulting in massive computational intensity during the tokenparallel pre-filling phase and bottlenecking practical deployment. To mitigate this, block-sparse attention has emerged as promising solution, approximating full attention by computing only subset of relevant blocks. The efficacy of this approach hinges on block importance estimation: efficiently identifying relevant blocks without full computation. Standard training-free methods typically employ mean pooling [4, 5] as coarse-grained proxy. However, this proxy is often inaccurate, forcing state-of-the-art methods to rely on expensive heuristic search and token-level verification to maintain performance. This creates fundamental trade-off: the heavy estimation overhead often negates the sparsity gains, causing these methods to underperform highly optimized full attention implementations (e.g., FlashAttention [6]) at moderate sequence lengths. 1 Figure 1 Spectral Disentanglement of Attention Patterns. We visualize the attention score matrices computed using different spectral bands of RoPE. (Left) Low-Frequency Band: Captures global semantic dependencies (e.g., block-sparse patterns / vertical lines), acting as the semantic backbone. (Middle) High-Frequency Band: Strictly encodes fine-grained relative locality (e.g., slash lines), which is critical for local coherence. (Right) Full Spectrum: The superposition of both patterns. In this work, we trace the inaccuracy of standard coarse-grained attention to theoretical root cause: the spectral interaction between mean pooling and Rotary Positional Embeddings (RoPE) [7]. As illustrated in Figure 1, the spectral heterogeneity of RoPE naturally disentangles attention into distinct structural patterns: high-frequency dimensions strictly encode fine-grained relative positions, while low-frequency dimensions capture global semantic dependencies, manifesting as divergent sparse patterns. However, we mathematically prove that mean pooling acts as Low-Pass Filter. In high-frequency dimensions, the rapid rotation of RoPE vectors induces destructive interference during aggregation, causing the signal magnitude to collapse. This phenomenon creates spectral Blind Spot that effectively erases fine-grained positional information (e.g., slash patterns) from the pooled representation, explaining why standard methods struggle to maintain local coherence without expensive corrections. To address this, we introduce Prism, spectral-aware framework that disentangles block importance estimation into two parallel branches. Instead of treating embeddings as monolithic vectors, Prism explicitly separates the attenuated high-frequency band from the robust low-frequency band. By applying novel energy-based temperature calibration, Prism restores the attenuated positional signals from pooled representations. This design enables Prism to perform precise importance estimation using exclusively block-level operations, eliminating the selection bottleneck common in prior works. We evaluate Prism with diverse long-context capabilities, ranging from language modeling (PG19 [8]), long-context understanding (LongBench [1]), long-context retrieval (RULER [9]), and video understanding (VideoMME [10] & LongVideoBench [2]). Experiments demonstrate that Prism closely matches the accuracy of full attention while delivering substantial speedups compared to FlashAttention and state-of-the-art sparse attention methods. Our contributions are summarized as follows: Theoretical Insight: We identify mean pooling as low-pass filter under RoPE, revealing the Blind Spot responsible for the failure of standard block importance estimation. Methodology: We propose Prism, training-free framework utilizing dual-band scoring and energybased calibration to explicitly preserve high-frequency positional information without token-level overhead. SOTA Efficiency: Prism achieves state-of-the-art accuracy-speedup trade-offs, delivering up to 5 speedup at 128K tokens while outperforming baselines in latency across all sequence lengths."
        },
        {
            "title": "2 Related Work",
            "content": "Block-Sparse Attention The quadratic computational complexity of the self-attention mechanism [3] poses significant bottleneck for processing long contexts in modern LLMs. Fortunately, as result of the softmax operation, learned attention matrices often exhibit highly sparse patterns; that is, small subset of tokens 2 accounts for the majority of the attention mass, providing an opportunity to reduce computational overhead. Early sparse attention approaches relied on static sparse patterns, such as fixed sliding windows [11], dilated windows [12], or global \"sink\" tokens [13] to maintain local coherence and stability. However, static patterns often fail to capture long-range dependencies scattered arbitrarily across the sequence (the \"needle in haystack\" problem). Consequently, recent research has shifted toward dynamic sparse attention, where the attention pattern is determined adaptively based on the input. To implement this efficiently on hardware, block-sparse approaches partition the sequence into fixed-size blocks (e.g., 128128). This design naturally aligns with the tiling mechanism of FlashAttention [6], which decomposes computation into contiguous blocks for I/O awareness. By restricting the dense computation and online accumulation to selected subset of block pairs, this granularity allows for optimized GPU kernels (e.g., via Triton or CUDA) while significantly reducing the number of FLOPs during the compute-bound pre-filling stage. Block Importance Estimation The central challenge in dynamic block-sparse attention is block importance estimation: identifying which Key blocks are relevant to given Query block without incurring the quadratic cost of the full attention matrix. In the scope of pre-filling, existing training-free approaches typically rely on coarse-grained proxies combined with heuristic pattern matching. Methods such as MInference [4] and FlexPrefill [5] employ offline or online search strategies to classify attention heads into pre-defined categories (e.g., Vertical Slash or Block-Sparse). Consequently, they adopt divergent estimation techniques, utilizing coarse-level attention for semantic retrieval heads while falling back to selection against certain patterns. Other works aim for unified estimation metric. SpargeAttention [14] adopts coarse-level attention for all heads while enforcing blocks with low intra-block similarity. XAttention [15] introduces an antidiagonal scoring mechanism to capture both block-sparse and vertical-slash patterns, while PBS-Attn [16] utilizes token permutation to cluster critical tokens for better separability. However, these methods typically involve additional token-level operations, which significantly degrade block selection efficiency, particularly at moderate sequence lengths where the selection overhead outweighs the sparsity gains."
        },
        {
            "title": "3.1 Preliminaries\nCoarse-grained Attention Block-sparse attention requires a block mask M to determine if a block pair\n(u, v) should be computed. For efficient estimation of M, a typical approach is to compute a coarse-grained\nattention matrix ¯S. Formally, let Q, K, V ∈ RL×d denote the query, key, and value matrices, where L is the\nB ⌉ blocks, where B is\nsequence length and d is the head dimension. The sequence is partitioned into N = ⌈ L\nthe block size. For the u-th query block and v-th key block, let Iu and Iv denote the sets of token indices\nbelonging to each block, respectively. Coarse-grained attention typically compresses each block into a single\nrepresentative vector using mean pooling:",
            "content": "qu = 1 iIu qi, kv = 1 jIv kj (1) Let Q, RN be the matrices formed by stacking these pooled vectors. Then the coarse-grained attention matrix is computed as: = softmax (cid:19) (cid:18) K (2) Finally, top-k or top-p selection is applied to to generate the binary mask {0, 1}N . Spectral Structure of RoPE Modern large language models (LLMs) [1720] typically employ rotary positional embeddings (RoPE) [7] to inject positional information. RoPE rotates feature pairs in the complex plane. Let x(j) denote the j-th feature pair of vector at position n, represented as complex number. The embedding 3 is rotated by an angle dependent on the position and frequency θj: = x(j) x(j) nope einθj (3) Crucially, the rotation frequencies are defined as geometric sequence decaying across the feature dimension index {0, . . . , d/2 1}: θj = b2j/d where is the base (e.g. 1M for Qwen3). This definition creates Spectral Heterogeneity [21] across the embedding dimensions: (4) High-Frequency Band (j 0): Dimensions with low indices possess large θj, resulting in rapid rotation. These dimensions encode fine-grained, relative positional information (e.g., local context). Low-Frequency Band (j d/2): Dimensions with high indices possess θj 0, resulting in negligible rotation over long distances. These dimensions behave similarly to absolute embeddings, primarily encoding global semantic content. This spectral distribution implies that linear operations applied across the sequence dimension, such as the mean pooling defined in Eq. 1, will exhibit frequency-dependent behaviors, phenomenon we analyze in the following section. Sparse Patterns of Attention Extensive empirical analysis [4, 5, 13] reveals that attention matrices in pretrained LLMs are not uniformly sparse but exhibit distinct structural characteristics, most notably the vertical slash patterns and block-sparse patterns. Prior works typically treat these patterns as mutually exclusive properties of specific attention heads, employing heuristic classifiers to assign distinct estimation strategies [4, 5]. Although Xu et al. [15] attempted to capture both patterns via unified antidiagonal scoring mechanism, their approach still incurs additional token-level operations, resulting in significant selection overhead at long sequence lengths. We challenge this head-level dichotomy. We posit that these patterns are not spatially separated across heads but are instead spectrally disentangled within individual heads. As visualized in Figure 1, the high-frequency spectral bands of RoPE (low indices) strictly encode relative locality (slash patterns), while the low-frequency bands (high indices) capture global semantic dependencies (block-sparse patterns). This spectral observation motivates our frequency-decomposed approach."
        },
        {
            "title": "3.2 Mean Pooling as a Low-Pass Filter",
            "content": "To facilitate efficient block importance estimation, mean pooling(Eq. 1) serves as common technique to compress block into single representative vector. In this section, we theoretically analyze the impact of mean pooling with the consideration of RoPE, which explains why existing methods had to resort to token-level operations for accurate block importance estimation. Geometric Summation of Mean Pooling Consider the j-th frequency pair of the query vector. Under RoPE, the embedding at position can be decomposed into content component c(j) and positional rotation einθj . Assuming the semantic content c(j) remains relatively stable within the local context of block (a standard assumption for adopting mean pooling), applying the mean pooling over block of size starting at position n0 can be formulated as geometric series summation: q(j) c(j) B1 k= ei(n0+k)θj = c(j)ein0θj B1 ! eikθj k= } {z Geometric Sum (5) Spectral Attenuation The magnitude of this pooled vector dictates the signal strength available for dot-product retrieval. By evaluating the geometric sum, we derive the Spectral Attenuation Factor λj(B), defined as the 4 Figure 2 Spectral attenuation factor λj(B) with block size = 128 and head dimension = 128. ratio of the pooled vectors magnitude to the original vectors magnitude: λj(B) qj = (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) 1 B1 n= einθj (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) = 1 (cid:12) (cid:12) (cid:12) (cid:12) sin(Bθj/2) sin(θj/2) (cid:12) (cid:12) (cid:12) (cid:12) For small frequencies, this function converges to the normalized sinc function: λj(B) (cid:12) (cid:12) (cid:12) (cid:12) sinc (cid:18) Bθj 2π (cid:19)(cid:12) (cid:12) (cid:12) (cid:12) (6) (7) detailed derivation is provided in Appendix A. This derivation mathematically reveals that mean pooling functions as Low-Pass Filter: Destructive Interference (λj 0): In the high-frequency band where the block size covers full rotation periods (Bθj 2πk), the vectors sum to near-zero. For standard block size = 128, this creates Blind Spot in the first 30 dimensions (for Base 1M), effectively erasing local positional structures. Constructive Interference (λj 1): In the low-frequency band where θj 0, the rotations are negligible, and the signal magnitude is fully preserved. We quantify this effect using standard setting with block size = 128 and head dimension = 128, considering RoPE bases = 106 (Qwen3) and = 5 105 (LLaMa 3.1), as visualized in Figure 2. Taking Qwen3 as an example, destructive interference reaches its peak (λj 0) when the total rotation Bθj = 2π. We solve for the corresponding feature dimension index 2j: b2j/d = 2π = 2j = ln(B/2π) ln (8) Substituting the values yields cutoff dimension of 2j 28. Based on this derivation, the spectrum in Figure 2 divides into three distinct regimes: The Dead Zone (0 2j 30): The signal magnitude is effectively zero due to full phase cancellation. The Transition Zone (30 2j 60): The signal begins to recover but remains heavily attenuated (λ < 1). The Semantic Zone (2j > 60): The signal magnitude is fully preserved, capturing global semantic information. This analysis theoretically justifies why standard coarse-grained attention is blind to fine-grained positional structures encoded in the high-frequency band. Figure 3 Comparison of Query RMS norms before and after pooling. Left (Token-level): While the Semantic Zone (blue) holds the highest energy, the Dead Zone (green) maintains robust magnitude (RMS 1.0), confirming that high-frequency dimensions are actively utilized by the pre-trained model. Right (Block-pooled): After pooling, energy in the Dead Zone collapses to near-zero due to destructive interference, while the Semantic Zone preserves its magnitude."
        },
        {
            "title": "3.3 Energy Analysis",
            "content": "To verify whether the theoretical attenuation derived in Section 3.2 manifests in actual model representations, we analyze the spectral energy distribution using Qwen3-8B. We measure the RMS norms of the query vectors before and after mean pooling across the three spectral zones defined in Figure 2. Ideally, if pooling were lossless, the block-level RMS should mirror the token-level RMS. However, Figure 3 reveals distinct Spectral Divergence: At the token level (Left), the Dead Zone maintains robust magnitude (RMS 1.0), confirming that high-frequency positional features are intrinsically significant to the pre-trained model. In contrast, the block-pooled representation (Right) exhibits dramatic Energy Collapse in the Dead Zone (RMS 0.1), empirically validating that mean pooling acts as low-pass filter that suppresses local positional information. Crucially, the RMS of the Semantic Zone consistently surpasses the Full spectrum. This intrinsic divergence is significantly exacerbated post-pooling, as the Full vector is further diluted by the dead weight of attenuated high-frequency dimensions. This widened energy gap necessitates the frequency-dependent calibration proposed next."
        },
        {
            "title": "3.4 Prism: Spectral-Aware Block-Sparse Attention\nTo resolve the spectral bias identified above, we propose Prism, a framework that decomposes block selection\ninto two spectral branches based on their characteristics. The overall procedure is summarized in Figure 4\nand consists of two core components: (1) Dual-Band Block Importance Estimation, which explicitly isolates\nthe high-frequency and low-frequency bands to avoid signal interference during aggregation; and (2)\nEnergy-Based Temperature Calibration, which derives branch-specific temperatures from spectral energy\ndistributions, restores the logit magnitudes without any hyperparameter tuning. Crucially, this design enables\nPrism to perform estimation using exclusively block-level operations, minimizing selection overhead.\nDual-Band Block Importance Estimation To best preserve information from both spectral bands, we propose\na dual-band block importance estimation strategy that avoids interference between the two bands.",
            "content": "Let Q, RLd denote the input query and key matrices. We explicitly isolate the High-Frequency Band by slicing the first dhigh dimensions, yielding Qhigh, Khigh RLdhigh. Similarly, we slice the last dlow dimensions to form the Low-Frequency Band, Qlow, Klow RLdlow . Subsequently, mean pooling with block size is applied to the high-frequency and low-frequency bands independently, obtaining Qhigh, Khigh RN dhigh and Qlow, Klow RN dlow , where = . With the pooled representations, we compute the coarse-grained importance scores for each spectral band {high, low}. Furthermore, to account for the distinct spectral energy densities caused by attenuation (as observed in Figure 3), we introduce 6 branch-specific temperature scaling factors τhigh and τlow: Sz = softmax (cid:18) Qz dz τz (cid:19) , for {high, low} (9) Based on the probability distributions Shigh and Slow, we generate binary block masks Mhigh and Mlow by selecting the top-p cumulative probability mass for each query block. The final block-sparse mask is obtained by the union of these branch-specific selections: = Mhigh Mlow (10) Energy-Based Temperature Calibration To align the logit magnitude of the individual spectral bands to the scale of the full spectrum, we derive the branch-specific temperatures τz based on the spectral energy distribution. We employ RMS norm to represent the spectral energy density of pooled matrix RN d, where xu2 RMS( X) = . Consider attention logits Lf ull = ( Qf ull d. Since the dot ull)/ product accumulates magnitude across dimensions, the scale of these logits follows: PN u=1 1 Lf ull RMS( Qf ull)RMS( Kf ull) (11) Similarly, for spectral branch using subspace dimension dz, the uncalibrated logits Lz scale as: Lz dz RMS( Qz)RMS( Kz) (12) To restore the signal strength of the partial branch to the baseline level (i.e., Lz/τz Lf ull), we derive the calibration factor: τz dz RMS( Qz) RMS( Qf ull) RMS( Kz) RMS( Kf ull) (13)"
        },
        {
            "title": "4 Experiments",
            "content": "def prism(Q, K, d_h, d_l, B, p): # Setup dimensions bs, h, L, = Q.shape = // # 1. Pooling & Slicing Qb, Kb = pool(Q, B), pool(K, B) Qh, Ql = Qb[..., :d_h], Qb[..., -d_l:] Kh, Kl = Kb[..., :d_h], Kb[..., -d_l:] # 2. RMS Calculation rq, rk = rms(Qb), rms(Kb) rq_h, rk_h = rms(Qh), rms(Kh) rq_l, rk_l = rms(Ql), rms(Kl) # 3. Calibration (Eq. 13) th = sqrt(d_h/d) * (rq_h/rq) * (rk_h/rk) tl = sqrt(d_l/d) * (rq_l/rq) * (rk_l/rk) # 4. Dual-Band Scoring scale_h = sqrt(d_h) * th scale_l = sqrt(d_l) * tl logits = empty(bs, h, 2N, N) logits[..., :N, :]=(Qh @ Kh.T) / scale_h logits[..., N:, :]=(Ql @ Kl.T) / scale_l # 5. Selection = softmax(logits, dim=-1) Mh, Ml = top_p(P, p).split(N, dim=-2) return Mh Ml Figure 4 PyTorch-style implementation of Prism. Prism exclusively uses block-level operations for best efficiency. See Appendix for top_p implementation."
        },
        {
            "title": "4.1 Setup\nBenchmarks, Models & Baselines To evaluate the versatility and robustness of Prism, we conduct experiments\nacross four categories of long-context tasks: (1) Language Modeling using PG19 [8]; (2) Long-Context\nUnderstanding using LongBench [1]; (3) Long-Context Retrieval using RULER [9]; and (4) Video Under-\nstanding using VideoMME [10] and LongVideoBench [2]. We employ state-of-the-art models including\nLlama-3.1-8B-Instruct (128K) [17] and the Qwen3-8B [18]. Notably, for Qwen3-8B, we apply YaRN [22]\nextrapolation to extend the context from 32K to 128K. For multimodal tasks, we utilize Qwen3-VL-8B [23].\nThis selection specifically enables us to verify Prism’s generalization to RoPE variants, including YaRN, and In-\nterleaved M-RoPE. We compare Prism with FlashAttention-2 [24] (full attention baseline), and state-of-the-art\ntraining-free dynamic block-sparse methods: MInference [4], FlexPrefill [5], and XAttention [15]. To ensure\nfair comparison, we use the official recommended configurations for all baselines. Details in Appendix C.",
            "content": "7 Figure 5 Language modeling performance on PG19. We compare the Perplexity Degradation (PPL, solid lines, left axis) and Speedup (bars, right axis) across sequence lengths. Prism achieves double win: it shows no perplexity degradation (sticking to the 0 line) while delivering the highest speedup (5.1 at 128K), significantly outperforming baselines that trade off accuracy for speed or suffer from high selection overhead. Implementation Details For Prism, we use block size = 128 based on the trade-off analysis in Appendix D. Guided by the spectral analysis in Figure 2, we configure the spectral bands as dhigh = 64 and dlow = 96. This configuration ensures robust signal coverage by overlapping the transition zone, while strictly aligning dimension sizes with multiples of 32 to maximize Tensor Core throughput on GPUs. For Top-P selection, we use threshold = 0.95 for Llama-3.1-8B-Instruct and = 0.93 for Qwen models to balance the trade-off between efficiency and accuracy. For importance estimation and block-sparse attention, we implement custom Triton kernels for best efficiency. Table 1 Performance comparison on LongBench. Method Single-Doc QA Multi-Doc QA Summarization Few-shot Learning Code Synthetic Avg. Full MInference FlexPrefill XAttention Prism Full MInference FlexPrefill XAttention Prism 47.51 47.42 46.13 45.89 47.09 47.1 46.9 43.77 44.49 46.47 43.28 42.54 41.49 41.56 42.13 40.45 40.39 39.31 40.09 40.08 Llama-3.1-8B 25.9 25.85 25.85 26.18 26 Qwen-3-8B 24.07 24.07 23.99 24.12 24.01 45.92 45.58 46.63 45.86 46. 56.69 55.74 57.33 57.27 58.36 18.01 17.84 17.68 19.24 18.72 1.65 1.61 1.87 1.29 1.64 68.18 67.6 25.61 59.32 66.15 67 66.33 50.5 65.67 64.17 41.47 41.14 33.90 39.68 41. 39.49 39.18 36.13 38.82 39."
        },
        {
            "title": "4.2 Main Results\nLanguage Modeling We evaluate the modeling capability on long-context sequences using the PG19\nbenchmark. Figure 5 visualizes the scalability of Prism compared to baselines, plotting Perplexity Degradation\n(∆PPL) and Speedup. Notably, Prism demonstrates superior robustness, maintaining a perplexity virtually\nidentical to the Full Attention baseline (∆PPL ≈ 0) across all context lengths. In contrast, baselines like\nMInference and FlexPrefill suffer from significant perplexity degradation as sequence length increases,\nespecially at 128K. While XAttention achieves high fidelity comparable to Prism, it is bottlenecked by\nsignificant estimation overhead. This becomes critical at extreme lengths: at 128K, XAttention is limited to a\n3.0× speedup, whereas Prism achieves 5.1×. Consequently, Prism achieves a double win, delivering the\nhighest speedup while simultaneously maintaining the perplexity of full attention.",
            "content": "8 Long-Context Understanding Table 1 presents the evaluation results on LongBench. Prism demonstrates exceptional robustness, achieving average scores of 41.08 on Llama-3.1-8B-Instruct and 39.12 on Qwen-3-8B, showing negligible degradation (< 0.4%) compared to the full attention baseline. While MInference achieves similar accuracy, it relies on fixed budget strategy that, at the moderate sequence lengths of LongBench (< 16K), often results in selecting nearly all tokens. Consequently, it degenerates to full attention while incurring additional estimation overhead, failing to provide meaningful sparsity. In contrast to other sparse baselines, Prism significantly outperforms FlexPrefill and XAttention on average for both models. Notably, Prism even slightly outperforms full attention on specific tasks (e.g., 58.36 vs. 56.69 on Qwen-3 Few-shot). We attribute this gain to the explicit preservation of high-frequency positional signals. By recovering the fine-grained relative structure essential for Induction Heads [25], Prism enhances the models ability to perform in-context pattern copying. Furthermore, unlike full attention, Prism filters out irrelevant semantic blocks, effectively denoising the context for these position-sensitive heads. Table 2 Performance comparison on RULER. Method Full MInference FlexPrefill XAttention Prism Full MInference FlexPrefill XAttention Prism 4K 95.42 95.43 93.8 95.17 95.28 95.01 95.08 90.89 94.55 94.84 8K 94.38 94.46 92.44 94.3 94.47 92.35 92.37 87.61 91.03 90. 16K Llama-3.1-8B 93.38 93.42 93.28 93.28 92.48 32K 87.98 87.22 87.92 89.06 87.67 Qwen-3-8B(YaRN) 90.04 89.67 87.82 87.91 87.69 87.24 86.01 85.58 84.37 86.88 64K 84.72 83.07 84.74 82.31 82.59 79.93 76.53 78.27 77.73 78.58 128K 77.77 71.04 72.41 70.52 72.75 75.09 70.36 73.42 72.01 72.65 Avg. 88.94 87.44 87.43 87.44 87.54 86.61 85.00 83.93 84.60 85.27 Long-Context Retrieval Table 2 reports the evaluation results on RULER. As shown in the table, all methods show comparable performance with their configured threshold parameters. However, it is crucial to note that Prism achieves this parity using exclusively block-level operations in semantic retrieval. In contrast, baselines like MInference and FlexPrefill rely on token-level estimation using the last query block, heuristic that is inherently advantageous for RULERs format, where the query is typically positioned at the end. Despite not being explicitly optimized for such structure, Prisms Low-Frequency Branch successfully handles these retrieval tasks, validating that our spectral calibration preserves sufficient semantic recall. Notably, the robust results on the YaRN-extrapolated Qwen3-8B demonstrate Prisms generalizability to RoPE variants without requiring additional adaptations. Table 3 Performance comparison on long video understanding tasks with Qwen3-VL-8B. Video Understanding To assess the generalizability of Prism to multimodal scenarios, we evaluate performance on VideoMME and LongVideoBench using Qwen3-VL8B. As shown in Table 3, Prism outperforms existing approaches on both benchmarks, achieving performance comparable to the full attention baseline. Crucially, in the Long split of VideoMME, where video durations range from 30 minutes to 1 hour (spanning 54K to 107K tokens), Prism surpasses the full attention baseline (64.00 vs. 63.11). We attribute this to the denoising effect of sparse attention, which effectively filters out irrelevant visual tokens, allowing the model to focus on the most salient visual"
        },
        {
            "title": "Full\nMInference\nFlexPrefill\nXAttention\nPrism",
            "content": "63.11 62.44 62.67 63.44 64.00 65.00 61.48 64.10 64.25 64.25 71.22 70.63 70.34 70.81 71.22 70.67 70.00 70.67 69.78 70."
        },
        {
            "title": "Long Overall Overall",
            "content": "79.89 79.44 77.67 79.22 79.00 Short Med."
        },
        {
            "title": "LVB",
            "content": "9 Figure 6 Efficiency comparison on Llama-3.1-8B-Instruct with an H100 GPU. We report pre-filling latency (bars, left axis) and speedup relative to FlashAttention-2 (lines, right axis). Shaded areas represent the block importance estimation time. Figure 7 Estimation overhead comparison. The upper and lower panels illustrate the time and memory overhead of block importance estimation, respectively. information. These results also confirm the generalization of Prism to other multimodal RoPE variants (i.e., Interleaved M-RoPE [23]), demonstrating its robustness."
        },
        {
            "title": "4.4 Ablation Studies\nSpectral Division We analyze the impact of different spectral band configurations on the Perplexity-Density\ntrade-off in Figure 8 with the following findings:",
            "content": "Mean Pooling is indeed Low-Pass Filter: Using only the low-frequency band (i.e., dlow = 96, dhigh = 0) exhibits nearly identical behavior to directly using the full dimension, even lower than the full dimension case, indicating that high-frequency components are acting only as noise in mean pooling block importance estimation. Necessity of Transition Zone in High-Frequency Band: Restricting the high-frequency band to the 10 Figure 8 Perplexity vs. Density with various dimension division strategies at 32K length. Figure 9 Effect of Energy-Based Temperature Calibration. theoretical dead zone (dhigh = 32) yields suboptimal performance. This confirms that within the dead zone, positional signals are effectively erased by destructive interference. Consequently, attempting to align and calibrate this subspace only amplifies background noise, causing severe performance degradation. Extending the branch to dhigh = 64 is thus critical to capture the recovering signals in the transition zone for effective restoration. Robustness of Overlapping: While the aggressive semantic slicing (dlow = 64) appears promising at low densities, it exhibits performance instability (a U-shaped curve) at higher densities. We attribute this to the exclusion of the transition zone (d [32, 64]). By extending to dhigh = 96 (red), we create spectral overlap where the transition zone is covered by both branches. This design is crucial because the transition band, having moderate energy, acts as spectral regularizer for the low-frequency branch: it moderates the energy density to prevent over-calibrated temperatures while ensuring signal continuity between positional and semantic regimes. Effect of Energy-Based Temperature Calibration We validate the necessity of our derived calibration formula by comparing the PPL-Density trade-off against baseline with fixed temperature (τlow = τhigh = 1.0). As shown in Figure 9, the calibrated configuration consistently dominates the uncalibrated one, pushing the Pareto frontier significantly towards better efficiency. Without calibration, the high-frequency logits remain attenuated, resulting in flattened softmax distribution (high entropy). Consequently, the adaptive Top-P policy fails to distinguish weak positional signals from background noise, forcing it to select large number of irrelevant blocks, leading to an inefficient density inflation. In contrast, our calibration restores the logit magnitude, effectively sharpening the distribution to capture salient information within limited density budget."
        },
        {
            "title": "5 Conclusion",
            "content": "In this work, we identified the spectral attenuation induced by mean pooling under RoPE as the theoretical bottleneck for efficient block importance estimation. To address this, we introduced Prism, training-free framework that explicitly preserves high-frequency information via dual-band scoring and energy-based calibration. By enabling precise selection using exclusively block-level operations, Prism achieves 5 speedup at 128K context while maintaining performance parity with full attention, offering robust and scalable solution for long-context and multimodal LLMs."
        },
        {
            "title": "References",
            "content": "[1] Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li. Longbench: bilingual, multitask benchmark for long context understanding, 2024. URL https://arxiv.org/abs/2308.14508. [2] Haoning Wu, Dongxu Li, Bei Chen, and Junnan Li. Longvideobench: benchmark for long-context interleaved video-language understanding, 2024. URL https://arxiv.org/abs/2407.15754. [3] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need, 2023. URL https://arxiv.org/abs/1706.03762. [4] Huiqiang Jiang, Yucheng Li, Chengruidong Zhang, Qianhui Wu, Xufang Luo, Surin Ahn, Zhenhua Han, Amir H. Abdi, Dongsheng Li, Chin-Yew Lin, Yuqing Yang, and Lili Qiu. Minference 1.0: Accelerating pre-filling for long-context llms via dynamic sparse attention, 2024. URL https://arxiv.org/abs/2407.02490. [5] Xunhao Lai, Jianqiao Lu, Yao Luo, Yiyuan Ma, and Xun Zhou. Flexprefill: context-aware sparse attention mechanism for efficient long-sequence inference, 2025. URL https://arxiv.org/abs/2502.20766. [6] Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. Flashattention: fast and memory-efficient exact attention with io-awareness. In Proceedings of the 36th International Conference on Neural Information Processing Systems, NIPS 22, Red Hook, NY, USA, 2022. Curran Associates Inc. ISBN 9781713871088. [7] Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding, 2023. URL https://arxiv.org/abs/2104.09864. [8] Jack W. Rae, Anna Potapenko, Siddhant M. Jayakumar, and Timothy P. Lillicrap. Compressive transformers for long-range sequence modelling, 2019. URL https://arxiv.org/abs/1911.05507. [9] Cheng-Ping Hsieh, Simeng Sun, Samuel Kriman, Shantanu Acharya, Dima Rekesh, Fei Jia, Yang Zhang, and Boris Ginsburg. Ruler: Whats the real context size of your long-context language models?, 2024. URL https: //arxiv.org/abs/2404.06654. [10] Chaoyou Fu, Yuhan Dai, Yongdong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, Peixian Chen, Yanwei Li, Shaohui Lin, Sirui Zhao, Ke Li, Tong Xu, Xiawu Zheng, Enhong Chen, Caifeng Shan, Ran He, and Xing Sun. Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis, 2025. URL https://arxiv.org/abs/2405.21075. [11] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse transformers, 2019. URL https://arxiv.org/abs/1904.10509. [12] Iz Beltagy, Matthew E. Peters, and Arman Cohan. Longformer: The long-document transformer, 2020. URL https://arxiv.org/abs/2004.05150. [13] Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming language models with attention sinks, 2024. URL https://arxiv.org/abs/2309.17453. [14] Jintao Zhang, Chendong Xiang, Haofeng Huang, Jia Wei, Haocheng Xi, Jun Zhu, and Jianfei Chen. Spargeattention: Accurate and training-free sparse attention accelerating any model inference, 2025. URL https://arxiv.org/abs/ 2502.18137. [15] Ruyi Xu, Guangxuan Xiao, Haofeng Huang, Junxian Guo, and Song Han. Xattention: Block sparse attention with antidiagonal scoring, 2025. URL https://arxiv.org/abs/2503.16428. [16] Xinghao Wang, Pengyu Wang, Dong Zhang, Chenkun Tan, Shaojun Zhou, Zhaoxiang Liu, Shiguo Lian, Fangxu Liu, Kai Song, and Xipeng Qiu. Sparser block-sparse attention via token permutation, 2025. URL https://arxiv.org/ abs/2510.21270. [17] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv e-prints, pages arXiv2407, 2024. [18] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao Ge, Haoran Wei, Huan Lin, 12 Jialong Tang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jing Zhou, Jingren Zhou, Junyang Lin, Kai Dang, Keqin Bao, Kexin Yang, Le Yu, Lianghao Deng, Mei Li, Mingfeng Xue, Mingze Li, Pei Zhang, Peng Wang, Qin Zhu, Rui Men, Ruize Gao, Shixuan Liu, Shuang Luo, Tianhao Li, Tianyi Tang, Wenbiao Yin, Xingzhang Ren, Xinyu Wang, Xinyu Zhang, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yinger Zhang, Yu Wan, Yuqiong Liu, Zekun Wang, Zeyu Cui, Zhenru Zhang, Zhipeng Zhou, and Zihan Qiu. Qwen3 technical report, 2025. URL https://arxiv.org/abs/2505.09388. [19] Aohan Zeng, Xin Lv, Qinkai Zheng, Zhenyu Hou, Bin Chen, Chengxing Xie, Cunxiang Wang, Da Yin, Hao Zeng, Jiajie Zhang, et al. Glm-4.5: Agentic, reasoning, and coding (arc) foundation models. arXiv preprint arXiv:2508.06471, 2025. [20] Team Olmo, :, Allyson Ettinger, Amanda Bertsch, Bailey Kuehl, David Graham, David Heineman, Dirk Groeneveld, Faeze Brahman, Finbarr Timbers, Hamish Ivison, Jacob Morrison, Jake Poznanski, Kyle Lo, Luca Soldaini, Matt Jordan, Mayee Chen, Michael Noukhovitch, Nathan Lambert, Pete Walsh, Pradeep Dasigi, Robert Berry, Saumya Malik, Saurabh Shah, Scott Geng, Shane Arora, Shashank Gupta, Taira Anderson, Teng Xiao, Tyler Murray, Tyler Romero, Victoria Graf, Akari Asai, Akshita Bhagia, Alexander Wettig, Alisa Liu, Aman Rangapur, Chloe Anastasiades, Costa Huang, Dustin Schwenk, Harsh Trivedi, Ian Magnusson, Jaron Lochner, Jiacheng Liu, Lester James V. Miranda, Maarten Sap, Malia Morgan, Michael Schmitz, Michal Guerquin, Michael Wilson, Regan Huff, Ronan Le Bras, Rui Xin, Rulin Shao, Sam Skjonsberg, Shannon Zejiang Shen, Shuyue Stella Li, Tucker Wilde, Valentina Pyatkin, Will Merrill, Yapei Chang, Yuling Gu, Zhiyuan Zeng, Ashish Sabharwal, Luke Zettlemoyer, Pang Wei Koh, Ali Farhadi, Noah A. Smith, and Hannaneh Hajishirzi. Olmo 3, 2025. URL https://arxiv.org/abs/2512.13961. [21] Xiaoran Liu, Hang Yan, Shuo Zhang, Chenxin An, Xipeng Qiu, and Dahua Lin. Scaling laws of rope-based extrapolation, 2024. URL https://arxiv.org/abs/2310.05209. [22] Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. Yarn: Efficient context window extension of large language models, 2023. URL https://arxiv.org/abs/2309.00071. [23] Shuai Bai, Yuxuan Cai, Ruizhe Chen, Keqin Chen, Xionghui Chen, Zesen Cheng, Lianghao Deng, Wei Ding, Chang Gao, Chunjiang Ge, Wenbin Ge, Zhifang Guo, Qidong Huang, Jie Huang, Fei Huang, Binyuan Hui, Shutong Jiang, Zhaohai Li, Mingsheng Li, Mei Li, Kaixin Li, Zicheng Lin, Junyang Lin, Xuejing Liu, Jiawei Liu, Chenglong Liu, Yang Liu, Dayiheng Liu, Shixuan Liu, Dunjie Lu, Ruilin Luo, Chenxu Lv, Rui Men, Lingchen Meng, Xuancheng Ren, Xingzhang Ren, Sibo Song, Yuchong Sun, Jun Tang, Jianhong Tu, Jianqiang Wan, Peng Wang, Pengfei Wang, Qiuyue Wang, Yuxuan Wang, Tianbao Xie, Yiheng Xu, Haiyang Xu, Jin Xu, Zhibo Yang, Mingkun Yang, Jianxin Yang, An Yang, Bowen Yu, Fei Zhang, Hang Zhang, Xi Zhang, Bo Zheng, Humen Zhong, Jingren Zhou, Fan Zhou, Jing Zhou, Yuanzhi Zhu, and Ke Zhu. Qwen3-vl technical report, 2025. URL https://arxiv.org/abs/2511.21631. [24] Tri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning, 2023. URL https: //arxiv.org/abs/2307.08691. [25] Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Scott Johnston, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish, and Chris Olah. In-context learning and induction heads, 2022. URL https://arxiv.org/abs/2209.11895."
        },
        {
            "title": "A Derivation of Spectral Attenuation Factor",
            "content": "In this section, we provide the detailed derivation of the spectral attenuation factor λj(B) introduced in Eq. 6 and its convergence to the sinc function in Eq. 7. A.1 Setup and Geometric Summation Consider the j-th frequency component of the query vector under Rotary Positional Embeddings (RoPE). We model the embedding at position as complex number: = c(j) einθj q(j) (14) where c(j) represents the semantic content (magnitude and initial phase) and θj is the rotation frequency. To isolate the effect of pooling on positional information, we assume the semantic content c(j) is locally stationary (constant) within the pooling window. The mean pooling operation over block of size (indexed locally from = 0 to 1) yields the pooled vector q(j): q(j) = 1 B1 k=0 c(j) ei(n0+k)θj = c(j)ein0θj B1 k=0 eikθj (15) where n0 is the start position of the block. The term = PB1 Applying the summation formula for finite geometric series: k=0 (eiθj )k is geometric series with ratio = eiθj . = 1 (eiθj )B 1 eiθj = 1 eiBθj 1 eiθj (16) A.2 Magnitude Calculation (The Dirichlet Kernel) We define the attenuation factor λj(B) as the ratio of the magnitude of the pooled vector to the magnitude of the original content c(j). Note that the phase term ein0θj = 1 and thus does not affect the magnitude. λj(B) q(j) c(j) = 1 = 1 (cid:12) (cid:12) (cid:12) (cid:12) 1 eiBθj 1 eiθj (cid:12) (cid:12) (cid:12) (cid:12) (17) To simplify the magnitude of the complex fraction, we utilize the half-angle identity 1 eiϕ = eiϕ/2(eiϕ/2 eiϕ/2) = 2i sin(ϕ/2) = 2 sin(ϕ/2). Applying this to both the numerator (ϕ = Bθj) and the denominator (ϕ = θj): λj(B) = 1 2 sin(Bθj/2) 2 sin(θj/2) = 1 sin(Bθj/2) sin(θj/2) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (18) This function is known as the normalized Dirichlet kernel, which describes the diffraction pattern of discrete periodic lattice. A.3 Sinc Approximation The RoPE frequencies are defined as θj = b2j/d. For dimensions away from 0, the frequency θj decays exponentially and becomes very small (θj 1). We apply the small-angle approximation sin(x) to the 14 denominator term1: Substituting this into the expression for λj(B): sin(θj/2) θj 2 λj(B)"
        },
        {
            "title": "1\nB",
            "content": "(cid:12) (cid:12) (cid:12) (cid:12) sin(Bθj/2) θj/2 (cid:12) (cid:12) (cid:12) (cid:12) We rearrange the terms to match the form of the normalized sinc function, defined as sinc(u) sin(πu) πu λj(B) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) sin( Bθj 2 ) Bθj 2 (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) Let πu = Bθj 2 , which implies = Bθj 2π . Substituting yields the final approximation: λj(B) (cid:12) (cid:12) (cid:12) (cid:12) sinc (cid:18) Bθj 2π (cid:19)(cid:12) (cid:12) (cid:12) (cid:12) (19) (20) : (21) (22) This derivation confirms that mean pooling acts as rectangular window filter in the signal domain, leading to the sinc-shaped spectral response shown in Figure 2. Top-P Block Selection Figure 10 provides the PyTorch-style implementation of the Top-P selection process used in Prism. The function takes block-level probabilities as input and sorts the key blocks for each query block based on relevance. Subsequently, it selects the minimal set of blocks required for the cumulative probability to exceed the threshold p. Finally, the original spatial order is restored via scatter operation. def top_p(probs, p): # 1. Sort probabilities sorted_probs, sorted_indices = sort(probs, descending=True, dim=-1) # 2. Compute cumulative probabilities cumulative_probs = cumsum(sorted_probs, dim=-1) # 3. Thresholding sorted_mask = (cumulative_probs - sorted_probs) < threshold # 4. Scatter to restore order mask = zeros_like(logits) mask.scatter_(dim=-1, index=sorted_indices, src=sorted_mask) return mask Figure 10 PyTorch-style implementation of the Top-P block selection."
        },
        {
            "title": "C Experimental Setup Details",
            "content": "C.1 Datasets We provide detailed descriptions of the benchmarks used in our evaluation: 1The small-angle approximation sin(x) holds due to the exponential decay of RoPE frequencies θj = b2j/d. Taking Qwen3 (b = 106, = 128) as an instance, the frequency drops to θ10 0.11 by the 10th dimension pair. At this point, the relative error is already < 0.2%. Thus, for the vast majority of the spectrum (j > 10), θj is sufficiently small to make the sinc model analytically exact. 15 PG19 [8]: standard benchmark consisting of full-length books, used to evaluate the models ability to model long-range dependencies via perplexity. LongBench [1]: bilingual, multi-task benchmark consisting of 21 datasets across 6 task categories in both English and Chinese, designed to measure broader understanding capabilities. RULER [9]: synthetic benchmark designed to measure the retrieval capability of long-context language models. Video Benchmarks: VideoMME [10] and LongVideoBench [2]. We use max pixels of 327680 for each frame and 1 frame per second for video sampling, which translate to approximately 107K tokens per hour. C.2 Baselines Configuration We compare Prism with the following baselines using their official implementations: MInference: method employing offline search to classify attention heads into pre-defined heuristic patterns for subsequent block importance estimation. We use the recommended Vertical-Slash pattern configurations. FlexPrefill: An approach utilizing online search to dynamically switch between static patterns and mean-pooling based estimation depending on input contexts. We adopt γ = 0.95, τ = 0.1 following the original paper. XAttention: unified method introducing antidiagonal scoring to capture both geometric and semantic patterns without explicit head classification. We use threshold = 0.9 and stride = 8 following the original paper."
        },
        {
            "title": "D Effect of Block Size",
            "content": "Figure 11 Effect of Block Size B. The upper panel illustrates the perplexity at various densities with context length of 128K using Llama-3.1-8B-Instruct. The lower panels illustrates the estimation time at various sequence lengths. 16 Theoretically, smaller block size enhances the Signal-to-Noise Ratio (SNR) by reducing spectral attenuation, but quadratically increases the estimation overhead due to the larger number of blocks (N = L/B). Figure 11 empirically validates this trade-off. In terms of accuracy (upper panel), finer granularity (B = 64) consistently yields better performance, even outperforming the full attention baseline due to effective noise filtering. = 128 closely follows this trend, matching full attention at reasonable densities. However, in terms of efficiency (lower panel), the estimation latency for = 64 rises sharply, reaching 22 ms at 128K. Although this is still faster than many existing baselines (Figure 7), it is more than double the overhead of = 128 ( 9 ms). Consequently, we select = 128 for the main experiments, as good compromise between accuracy and efficiency."
        }
    ],
    "affiliations": [
        "ByteDance Inc.",
        "Fudan University",
        "OpenMOSS Team",
        "Shanghai Innovation Institute"
    ]
}