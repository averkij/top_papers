{
    "paper_title": "Youtu-LLM: Unlocking the Native Agentic Potential for Lightweight Large Language Models",
    "authors": [
        "Junru Lu",
        "Jiarui Qin",
        "Lingfeng Qiao",
        "Yinghui Li",
        "Xinyi Dai",
        "Bo Ke",
        "Jianfeng He",
        "Ruizhi Qiao",
        "Di Yin",
        "Xing Sun",
        "Yunsheng Wu",
        "Yinsong Liu",
        "Shuangyin Liu",
        "Mingkong Tang",
        "Haodong Lin",
        "Jiayi Kuang",
        "Fanxu Meng",
        "Xiaojuan Tang",
        "Yunjia Xi",
        "Junjie Huang",
        "Haotong Yang",
        "Zhenyi Shen",
        "Yangning Li",
        "Qianwen Zhang",
        "Yifei Yu",
        "Siyu An",
        "Junnan Dong",
        "Qiufeng Wang",
        "Jie Wang",
        "Keyu Chen",
        "Wei Wen",
        "Taian Guo",
        "Zhifeng Shen",
        "Daohai Yu",
        "Jiahao Li",
        "Ke Li",
        "Zongyi Li",
        "Xiaoyu Tan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce Youtu-LLM, a lightweight yet powerful language model that harmonizes high computational efficiency with native agentic intelligence. Unlike typical small models that rely on distillation, Youtu-LLM (1.96B) is pre-trained from scratch to systematically cultivate reasoning and planning capabilities. The key technical advancements are as follows: (1) Compact Architecture with Long-Context Support: Built on a dense Multi-Latent Attention (MLA) architecture with a novel STEM-oriented vocabulary, Youtu-LLM supports a 128k context window. This design enables robust long-context reasoning and state tracking within a minimal memory footprint, making it ideal for long-horizon agent and reasoning tasks. (2) Principled \"Commonsense-STEM-Agent\" Curriculum: We curated a massive corpus of approximately 11T tokens and implemented a multi-stage training strategy. By progressively shifting the pre-training data distribution from general commonsense to complex STEM and agentic tasks, we ensure the model acquires deep cognitive abilities rather than superficial alignment. (3) Scalable Agentic Mid-training: Specifically for the agentic mid-training, we employ diverse data construction schemes to synthesize rich and varied trajectories across math, coding, and tool-use domains. This high-quality data enables the model to internalize planning and reflection behaviors effectively. Extensive evaluations show that Youtu-LLM sets a new state-of-the-art for sub-2B LLMs. On general benchmarks, it achieves competitive performance against larger models, while on agent-specific tasks, it significantly surpasses existing SOTA baselines, demonstrating that lightweight models can possess strong intrinsic agentic capabilities."
        },
        {
            "title": "Start",
            "content": "Youtu-LLM Technical Report Youtu-LLM: Unlocking the Native Agentic Potential for Lightweight Large Language Models Youtu-LLM Team We introduce Youtu-LLM, lightweight yet powerful language model that harmonizes high computational efficiency with native agentic intelligence. Unlike typical small models that rely on distillation, Youtu-LLM (1.96B) is pre-trained from scratch to systematically cultivate reasoning and planning capabilities. The key technical advancements are as follows: (1) Compact Architecture with Long-Context Support: Built on dense Multi-Latent Attention (MLA) architecture with novel STEM-oriented vocabulary, Youtu-LLM supports 128k context window. This design enables robust long-context reasoning and state tracking within minimal memory footprint, making it ideal for long-horizon agent and reasoning tasks. (2) Principled \"Commonsense-STEM-Agent\" Curriculum We curated massive corpus of approximately 11T tokens and implemented multi-stage training strategy. By progressively shifting the pre-training data distribution from general commonsense to complex STEM and agentic tasks, we ensure the model acquires deep cognitive abilities rather than superficial alignment. (3) Scalable Agentic Mid-training: Specifically for the agentic mid-training, we employ diverse data construction schemes to synthesize rich and varied trajectories across math, coding, and tool-use domains. This high-quality data enables the model to internalize planning and reflection behaviors effectively. Extensive evaluations show that Youtu-LLM sets new stateof-the-art for sub-2B LLMs. On general benchmarks, it achieves competitive performance against larger models, while on agent-specific tasks, it significantly surpasses existing SOTA baselines, demonstrating that lightweight models can possess strong intrinsic agentic capabilities. Code: https://github.com/TencentCloudADP/youtu-tip/youtu-llm Model: https://huggingface.co/collections/tencent/youtu 5 2 0 D 1 3 ] . [ 1 8 1 6 4 2 . 2 1 5 2 : r Figure 1. Parameterperformance scaling of base and instruct models on agentic benchmarks. The trend line represents the desired agent performance with the smallest possible number of parameters, among which Youtu-LLM stands out as lightweight yet strong performer. 1 Youtu-LLM Technical Report Figure 2. Multi-field general capability comparison of similarly sized models. Youtu-LLM shows balanced and competitive profile, highlighting its general-purpose performance potential under limited parameter budgets."
        },
        {
            "title": "1 Introduction",
            "content": "Large Language Models (LLMs) have made remarkable progress in recent years, steadily advancing toward Artificial General Intelligence (AGI) [Hendrycks et al., 2025]. By leveraging massive multi-domain corpora, growing parameter scales, and continuously evolving training paradigms, modern LLMs have demonstrated strong capabilities in reasoning, problem solving, and decision making [Guo et al., 2025, Yang et al., 2025]. Notably, recent reasoning-oriented models have achieved phenomenal performance on challenging agentic benchmarks, highlighting the effectiveness of large-scale training in eliciting complex cognitive behaviors [Plaat et al., 2025]. Despite these successes, current progress is tightly coupled with parameter scaling. State-of-the-art models typically rely on tens or hundreds of billions of parameters, incurring substantial computational, financial, and environmental costs during both training and deployment [Belcak et al., 2025]. These constraints significantly limit accessibility and real-world research and applicability, especially in latency-sensitive or resource-constrained settings. As result, there is renewed interest in developing lightweight large language models that retain strong general-purpose and reasoning capabilities while remaining practical for deployment. More well-known open-source model series are beginning to release models with fewer than 7B, and even fewer than 2B parameter models [Hu et al., 2024, Bakouch et al., 2025]. Existing approaches for improving small models largely rely on distillation, instruction tuning, or architectural simplification [Sharma and Mehta, 2025]. Although effective to some extent, these methods primarily align output behavior rather than systematically cultivating underlying cognitive capabilities. Consequently, lightweight models often lack robustness, generalization, and planning competence. Urgently, with the rapid emergence of complex tasks such as deep research, coding, and tool-augmented workflows, the above mentioned limitations of lightweight LLMs become even more pronounced in realworld agentic scenarios. The general consensus of the community indicates that effective agents require not only strong language understanding but also intrinsic capabilities for planning, tool execution, state perception, and feedback-driven reflection [Luo et al., 2025]. Therefore, recent work has begun exploring these native agentic capabilities in language models, shifting away from purely external agent frameworks 2 Youtu-LLM Technical Report toward internalized reasoning and interaction behaviors [Zeng et al., 2025]. In particular, trajectory-based training and continual pre-training on structured interaction data have shown promise in enhancing planning, reasoning, and tool-use abilities [Li et al., 2025]. Nevertheless, existing studies leave critical open question unanswered: Can lightweight LLMs acquire strong agentic capabilities through pre-training, rather than postaugmentation, such as post-training or agentic frameworks? In this work, we claim that strong agentic performance in lightweight LLMs is achievable when agentoriented signals are injected early and systematically through an agentic pre-training process. Specifically, we introduce Youtu-LLM, 2B-sized lightweight open-source model designed to balance compactness with robust general and agentic performance. Our approach integrates innovations in tokenizer design, data allocation, and multi-stage learning strategies under STEMand agent-centric training principle. Specifically, we propose series of scalable frameworks for constructing high-quality agentic trajectory data for pre-training. These frameworks cover broad range of capabilities, including reasoning, reflection, and planning, across diverse domains such as mathematics, coding, deep research, and general tool use. Through the proposed data pipeline, we obtain over 200B tokens of high-quality agentic trajectory data, providing fuel for our agentic pre-training. Youtu-LLM significantly outperforms existing state-of-the-art models of similar scale across both generalpurpose (Figure 2) and agentic benchmarks (Figure 1), and in several settings, rivals substantially larger models. Beyond performance gains, our analyses provide the first systematic evidence that agentic pretraining can unlock agent potential in lightweight LLMs, revealing phenomena such as scalable growth of agent capabilities. We summarize the main contributions, highlights, and insights of Youtu-LLM as follows: Lightweight Agentic LLM. We introduce Youtu-LLM, lightweight open-source language model that significantly outperforms state-of-the-art models of similar or even larger size in agent benchmarks. Native Agentic Capability Induction. We propose principled training paradigm that enhances native agentic capabilities through innovations in tokenizer design, data allocation, and multi-stage learning, guided by an agent-centric philosophy. Scalable Agentic Trajectory Construction. We present series of scalable frameworks for constructing high-quality agentic trajectory data, spanning reasoning, reflection and planning abilities, across multiple domains such as mathematics, coding, deep research and general tool use. Empirical Insights. We provide the first systematic analysis of agentic pre-training in lightweight LLMs, revealing the scalable growth of agentic abilities."
        },
        {
            "title": "2 Pre-Training Data",
            "content": "The scale and quality of the data determine the potential of LLMs. In this section, we introduce the data recipe of Youtu-LLMs pre-training. We report on two aspects: conventional general pre-training data and intricate trajectory pre-training data. The former accounts for over 95% of the total pre-training corpus, so we focus on controlling the overall quality and composition of this part; while the latter focuses on collecting complete, verifiable and high-quality agent execution trajectories, covering five categories: Agentic-CoT, Math, Code, Deep Research, and Tool-use."
        },
        {
            "title": "2.1 General Pre-Training Data",
            "content": "We firstly collected over 10T raw tokens from various sources, with English as the primary language and Chinese as the secondary focus. After deduplication, filtering and decontamination, we retained 8.7T raw 3 Youtu-LLM Technical Report tokens. The high-quality Chinese and English web pages and encyclopedic knowledge totaled 6.1T tokens, accounting for over 70% of the raw corpus. As aforementioned, STEM and coding capabilities were the key focus of Youtu-LLMs pre-training. Therefore, in the rest data, we compiled 700B tokens of Chinese and English STEM corpora, covering key disciplines such as mathematics, physics, chemistry, biology, and medicine; as for coding, we collected up to 1,400B tokens of source data. We also synthesized an additional 500B tokens of STEM and code corpora, including but not limited to explanations of STEM knowledge points, notebook documents, PDF summaries, paper interpretations, and code explanations in various forms. Finally, based on quality, we up-sampled certain high-quality STEM and code data. This expanded the 8.7T raw data into 10.64T-token pool for common pre-training data. Considering the risk of uneven quality and domain coverage bias in large-scale pre-training data, we adopted solution involving multi-dimensional data classification and quality scoring model for filtering. We first developed set of classification principles, including 10 quality assessment criteria and 11 domain classification criteria (comprising 46 sub-domains), covering core scenarios such as programming, mathematics, healthcare, and finance. Using semi-automated process, we employed Deepseek-R1 [Liu et al., 2025] for initial label annotation. During the annotation process, samples were randomly selected for manual review. Only when the consistency between the labels generated by the Deepseek-R1 and the manual annotations reached over 95%, the current batch of annotated data would be officially included in the training set. Through multiple iterations and optimizations, we obtained approximately 600K instances of English and Chinese data with evenly distributed domain and quality labels. Finally, we trained fast classification and scoring quality model based on Qwen3-1.7B [Yang et al., 2025], achieving over 95% accuracy of both domain classification and quality scoring, and showing high consistency with manual review. small-scale experiments proved that 80B high-quality data selected by the filtering model obtained even better performance than the original 100B raw data with only 50% training steps. Based on this classification and scoring model, we performed preliminary classification and quality assessment of the original corpus, retaining only those parts with an average evaluation score above 8.5 across all 10 criteria. Based on this, we binned the remaining data by length interval, then performed heuristic rule-based filtering, and MinHash-LSH-based [Broder, 1997, Har-Peled et al., 2012] deduplication within each bin. We included several heuristic rules such as 13-gram duplicate detection, toxicity scoring, and code keyword filtering. In addition, we introduced the AhoCorasick algorithm [Aho and Corasick, 1975] to benchmark and decontaminate the STEM and code corpus, minimizing the risk of test dataset leakage."
        },
        {
            "title": "2.2 Agentic Trajectory Data",
            "content": "This section details our approach to constructing diverse agentic trajectory data. We constructed 200B tokens of high-quality trajectory data, including 25B Agentic-CoT trajectories, 20B mathematical trajectories, 70B code execution trajectories, 60B Deep Research trajectories, and 25B other trajectories (e.g., tool using, function calling, and planning)."
        },
        {
            "title": "2.2.1 Agentic-CoT Trajectory",
            "content": "Motivation and Goal The reasoning data utilized in LLMs has progressed through multiple stages, including chain-of-thought (CoT) [Wei et al., 2022] and slow thinking enhanced with reinforcement learning techniques [Shao et al., 2024]. Currently, thinking models that encapsulate reasoning steps within <think> tags are prevalent. Although extended chains of thought significantly improve the accuracy of final answers, such reasoning traces are often plagued by redundancy and repetitive expressions. Directly learning from these raw reasoning processes may compromise both logical coherence and natural language fluency. Nevertheless, the information contained within these traces remains valuable. We argue that by refining 4 Youtu-LLM Technical Report Figure 3. Above: Vanilla long CoT enables thorough response preparation, while it can also tend to overthinking and unnecessary repetition. Below: Our agentic thinking paradigm implements defined reasoning architecture that guides models through sequential steps of analysis, plan, action, reflection and summary. This disciplined process fosters the development of agentic capabilities. Components Description Analysis Plan Action Reflection Summary This segment contains the initial breakdown and examination of the given problem. It includes identifying key components, constraints, and the core question to be answered. This section outlines the strategic steps devised to solve the problem based on the preceding analysis. It serves as blueprint for the subsequent actions. This part documents the concrete implementation of the planned steps, leading to an initial result or output. This phase captures an evaluation of the execution step and its outcome. It involves checking for errors, considering alternative approaches, or assessing the results validity and completeness. The final segment provides concise synthesis of the entire reasoning trajectory, distilling the key findings, the final answer, or the ultimate conclusion. Table 1. Agentic-CoT Data Components and Description. such content, it is possible to remove inefficient expressions while preserving the essential logical structure. Data Construction Strategy Inspired by agentic workflowswhich typically involve planning to break down problems, step-by-step actions, execution attempts, reflection, and final summarizationwe propose structured thinking paradigm called Agentic-CoT and construct corresponding dataset. This approach represents refined method for organizing reasoning processes and supports the advancement of agentic thinking paradigms. In detail, raw reasoning data are transformed into an Agentic-CoT trajectory composed of five distinct phases (Figure 3). The original, often linear or monolithic, CoT is decomposed and reorganized into the following sequential segments, each encapsulated within its own XML-style tag, shown in Table 1. Our data construction methodology employs stratified rewriting approach, utilizing LLMs to systematically generate and refine the dataset. The process unfolds in multiple tiers. (1) Reasoning and Generation: Initially, foundational reasoning model processes input queries to perform analysis and produce initial responses. This stage focuses on core logical derivation and answer formulation. (2) Curation and Extraction: The generated outputs undergo rigorous filtering and verification process to ensure the correctness. Subsequently, the targeted segments are extracted from the structured tripletscomprising the original query, the models internal reasoning chain, and the final response. (3) Synthesis and Assembly: The refined segments are then intelligently synthesized and concatenated. This assembly forms our proprietary Agentic-CoT Data, specialized corpus designed to capture explicit reasoning pathways. 5 Youtu-LLM Technical Report Ability Level Atomic Ability Description Symbol Recognition Identifying mathematical symbols and structural patterns from textual or visual inputs Module Action Basic Knowledge and Computation Concept Understanding Understanding and distinguishing mathematical Action concepts Complex Reasoning and Application Computation Execution Performing arithmetic, algebraic, calculus, and Action linear algebra operations Spatial Perception Reasoning over geometric objects, spatial relations, and quantitative measurements Action Formal Mathematical Language Translating natural language mathematical statements into formal representations Action Deduction and Induction Reasoning Deriving conclusions through deductive or inductive reasoning given known premises Action Counter-proof and Construction Solving problems via goal-driven reasoning strategies Action Modeling Transformation Converting real-world or textual scenarios into mathematical models with variables, constraints, and objectives Action Theorem Application Mathematical Meta-cognition Self-Reflection Identifying applicable theorems or lemmas, verifying conditions, and applying them correctly Feedback Diagnosing errors in reasoning trajectories and proposing corrective strategies Feedback New Knowledge Acquisition Learning novel definitions or theorems and transferring them to unseen mathematical problems Feedback Table 2. Atomic ability taxonomy for mathematical reasoning and its correspondence to agent modules. Data Statistical Information Through this multi-layered pipeline, we have constructed comprehensive Agentic-CoT dataset spanning 25B tokens. Its scope encompasses diverse domains such as industry, mathematics, STEM, and Coding. Experimental results demonstrate that Agentic-CoT Data significantly enhance the agentic thought of the model, leading to improved performance on agentic benchmarks. The corresponding results are provided in Appendix B."
        },
        {
            "title": "2.2.2 Math Trajectory",
            "content": "Motivation and Goal Mathematical reasoning tasks have long served as cornerstone for evaluating model performance in complex reasoning scenarios, due to their objectively verifiable ground-truth answers and coverage of broad difficulty spectrum. They are also widely regarded as the critical benchmarks for assessing the cognitive capabilities of Large Language Models or agents [Shao et al., 2025]. Although mathematical reasoning trajectories lack explicit environmental feedback signals commonly found in code execution or search-based trajectories, they exhibit highly generalizable and abstract agentic behavior patterns [Liu et al., 2025]. These patterns naturally capture essential agent behaviors such as planning, execution, and reflection, making mathematical trajectories particularly well-suited for pre-training models with robust cognitive strategies [Gao et al., 2025, Zhao et al., 2025]. Motivated by this observation, we incorporate mathematical reasoning trajectories into our trajectory pre-training pipeline. We aim to construct trajectories that model diverse agentic behaviors and fundamental reasoning abilities in clear, systematic, and comprehensive manner, enabling large language models to acquire interpretable, stable, and highquality mathematical reasoning behaviors during pre-training. 6 Youtu-LLM Technical Report Figure 4. The illustration of the designed math agent framework for math trajectory construction. Trajectory Construction Strategy To meet the demand for large-scale, high-quality mathematical trajectories, we require an agent framework that organically integrates agentic behaviors with mathematical reasoning abilities. With the rapid progress of LLM-based agents in complex tasks, various mathematical agents have been proposed. Some approaches enhance problem-solving accuracy through tool integration [Gou et al., 2024], or focus on specific mathematical tasks, such as mathematical modeling [Liu et al., 2025] or formal theorem proving [Shen et al., 2025]. However, most existing mathematical agents are adapted from general-purpose agent frameworks and fail to tightly couple planning, execution, and reflection with the fundamental abilities specific to mathematics, lacking systematic analysis of the various atomic capabilities in mathematical reasoning. Although some works are trying to establish structured mathematical skills for mathematical reasoning [Zhang et al., 2025], they have not yet integrated these systematic mathematical capability units with the agent framework. As result, the model only learns limited agent mathematical reasoning behavior patterns from this type of trajectory data, making it difficult to effectively improve the comprehensive understanding of reasoning capabilities. The Atomic Thinking [Kuang et al., 2025] systematically decouples the atomic abilities involved in mathematical reasoning and analyzes their interactions, which aligns closely with the modular interactions in agent architectures. Inspired by this insight, we design mathematical agent framework based on atomic ability to synthesize high-quality trajectories that comprehensively cover diverse agent behaviors and mathematical skills. Specifically, we construct three-level atomic ability hierarchy that decomposes mathematical reasoning into 11 independent and irreducible atomic abilities. The first level captures basic knowledge understanding and computation, the second level models complex reasoning and application abilities, and the third level represents higher-level mathematical metacognition. These atomic abilities can be naturally mapped to different agent modules and seamlessly integrated into planning, action, and feedback processes. The detailed ability taxonomy is summarized in Table 2. We adopt classical planningactionreflection loop in our agent design, illustrated as Figure 4. Given mathematical problem, the planning module first analyzes the problem structure and identifies the required atomic abilities, further decomposing them into executable sub-tasks with specified order, inputs, and expected outputs. The agent then enters the action stage, executing each atomic task step by step. After each execution, the feedback module evaluates the problem-solving state, returning feedback signals to the planning module for dynamic adjustment. Upon completion, we assess both answer correctness and trajectory quality, resulting in large corpus of high-quality correct mathematical trajectories for pre-training. Data Statistical Information For trajectory synthesis, we apply problems used during the annealing stage as queries, primarily consisting of high-difficulty computation, application, and reasoning tasks, which alighs with the K12 and mathematics competition level. Based on these problem sets and our proposed mathematical agent framework, we synthesize and filter total of 1,380,327 trajectories, comprising approximately 20B tokens. During pre-training, trajectories with lengths up to 32k tokens and those between 32k and 128k tokens are used separately. 7 Youtu-LLM Technical Report Figure 5. The Synthesis pipeline of Code Trajectories."
        },
        {
            "title": "2.2.3 Code Trajectory",
            "content": "Data Statistical Information The continual pre-training dataset for code scenario is composed of two primary components: (1) key atomic capabilities required for real-world code-related problem solving; (2) end-to-end agent trajectories, which encompass issue-resolving tasks in software engineering scenarios and other real-world tasks involving code tools. We construct 70B tokens of code trajectory data. Atomic Code Capabilities The Atomic Code Capabilities dataset covers the following core atomic capabilities essential for autonomous code agents: Proactive Exploration and Localization: Autonomously exploring the repository to locate specific variables, functions, or files, guided by ambiguous diagnostic logs or feature descriptions. Context-Aware Generation and Completion: Completing critical code segments given extensive repository-level context. Patch Generation and Editing: Generating precise modifications or patches based on localized faulty files and specific issue descriptions. Testing and Verification: Evaluating the correctness of action trajectories or localizing failure points within sequence of steps. Environment Comprehension: The ability to interpret or predict signals from the execution environment. Following the work of CWM, we synthesized function-level and repository-level variable tracing data, followed by diverse rewriting-based data augmentation [Copet et al., 2025]. Self Reflection: Executing single or multi-turn self-correction of generated code in response to environmental feedback. End-to-End Code Agent Trajectories To efficiently scale the end-to-end code agent trajectories, our approach focuses on three dimensions: (1) Scaling Tasks by creating diverse environments and new task instances; (2) Scaling Contexts by using wide variety of long-tail repositories through verifiable static tasks; and (3) Scaling Actions by branching from key editing and testing steps to reuse both successful and failed trajectories. The overall pipeline is illustrated in Figure 5. Scaling Tasks. To broaden the scope of task environments, we leverage multiple open-source sandboxed environments, including SWE-gym [Pan et al., 2024], SWE-smith [Yang et al., 2025], and SWE-rebench [Badertdinov et al., 2025]. Beyond utilizing existing instances, we extend these environments with new synthesized instances following SWE-smith [Yang et al., 2025]. To decouple model performance from specific scaffolding or toolset dependencies, we generate diverse issue-resolving trajectories using various scaffolds such as Mini-SWE-agent, SWE-gym, OpenHands, and R2E-gym. The toolsets employed encompass bash, search, 8 Youtu-LLM Technical Report (a) Successful Trajectories. (b) Failed Trajectories. Figure 6. Trajectory branching strategy for code agent trajectories. We identify Editing and Testing as Critical Actions that determine problem-solving success. For successful paths, we expand variations at each critical step; for failed paths, we generate single branch at the initial critical action to repurpose data effectively while mitigating error propagation. file_editor, str_replace_editor, submit, and finish. Notably, we prioritize bash-only trajectories, as bash represents the fundamental primitive from which more complex tools are abstracted. Furthermore, we extend our task domain to other real-world scenarios, including data science, front-end generation, game development, and specialized agents like ExcelAgent. Scaling Contexts. significant bottleneck in software engineering (SE) tasks is the high cost of constructing complete Docker environments and corresponding test suites. While automated pipelines exist, their nonnegligible failure rates necessitate manual verification [Yang et al., 2025], creating bottleneck for efficient scaling. To mitigate this, we construct environments based on repositories that lack executable images. For these, we design tasks that require static evaluation rather than dynamic testing, such as fault localization and refactoring. In these scenarios, agents interact with the environment using search or other bash tools to identify specific faulty files or perform global renaming of variables and functions. While these tasks may be simpler than resolving complex GitHub issues, they are highly scalable and verifiable. This approach exposes the model to vast diversity of codebases, fostering capabilities in proactive exploration, dynamic repository comprehension, and precise localization. Scaling Actions. Integrating trajectories into the pre-training phase poses two challenges: (1) pure behavioral cloning (BC) may lead to premature collapse of the action space, capping the models potential [Ross et al., 2011]; and (2) reusing failed trajectories might introduce suboptimal performance. However, discarding failed trajectories constitutes significant loss of data. We identify two Critical Actions that largely determine problem-solving success: Editing (e.g., , file_editor or sed via bash) and Testing (e.g., , pytest or unittest). We extract the context preceding these actions and generate branched trajectories as follows: Successful Trajectories: We generate alternative branches at every critical action point to enrich the models action space. Failed Trajectories: We generate single branch at the first critical action and truncate the sequence thereafter. Trajectories devoid of critical actions are filtered out. To prevent the propagation of errors from unverified branches, we limit this expansion to single-step rollout and single variation per action. Additionally, we insert an evaluation step following the branched action to assess the state of task completion. Two representative examples of this branching strategy are provided in Figure 6. This strategy enables the effective reuse of both successful and failed trajectories while mitigating overfitting and enhancing the models performance ceiling. 9 Youtu-LLM Technical Report Figure 7. The trajectory synthesis pipeline of the closed-ended DR."
        },
        {
            "title": "2.2.4 Deep Research Trajectory",
            "content": "Motivation and Goal Deep Research (DR) has emerged as pivotal application scenario for autonomous agents, representing significant leap from simple information retrieval to complex, self-directed knowledge discovery. In this scenario, agents are required to navigate intricate environments, verify heterogeneous information, and synthesize findings into coherent outputs. However, the development of such capable agents is currently hindered by scarcity of high-quality process data that captures the full spectrum of DR behaviors. Thus, our primary goal is to bridge this gap by constructing large-scale, diverse DR trajectory and behavior dataset which empower models with necessary skills to handle the complexity of real-world research tasks with high precision and depth. In the this scenario, we classify DR tasks into two broad categories: closed-ended tasks, e.g., multi-hop QA which typically possess well-defined, verifiable answers, and open-ended tasks, e.g., generating comprehensive research report or scientific paper. For the two categories, we design scalable trajectory synthesis pipelines tailored to their specific nature to ensure high-quality data production. Furthermore, we augment these complete trajectories with specialized datasets targeting atomic capabilities, serving as an enhancement to reinforce the fundamental skills essential for effective research. Closed-ended Trajectory Synthesis For the closed-ended setting, we primarily construct multi-hop QA trajectories. As shown in Figure 7, the overall data synthesis process proceeds as follows: First, we extract multi-hop QA pairs from seed corpora with high knowledge density. These seed sources include knowledgecentric web pages, multidisciplinary question banks, textbooks, and academic papers. Next, we evaluate the quality of the extracted QA pairs, filtering out low-quality examples. Once high-quality QA pairs are obtained, we synthesize trajectories using various frameworks, incorporating multiple strategies to enhance trajectory diversity. Given the verifiability of closed-ended tasks, we not only retain successful trajectories that lead to correct answers but also make use of failed trajectories through further processing. The detailed process is listed as follows: QA Generation. To construct large-scale, high-quality QA trajectories, we primarily leverage high knowledgedensity content from both pre-training and mid-training corpora as seed sources. To ensure content diversity, the seed data used for QA generation goes beyond typical encyclopedic resources and includes domainspecific materials across multiple disciplines, such as textbooks, academic papers, and professional question banks. From these knowledge-rich sources, we extract large number of entity-knowledge pairs, which serve as the foundation for generating multi-hop QA data. To ensure the quality of the QA pairs, we further apply an LLM-as-a-judge approach to score each QA instance across several dimensions, including factual accuracy, question clarity, answer uniqueness, and multi-hop reasoning consistency. As result of this pipeline, we obtain large-scale dataset consisting of millions of high-quality QA pairs. Trajectory Diversification. To enhance the diversity of synthesized trajectories, we employ multiple agent frameworks (e.g., , Youtu-Agent [Youtu, 2025], WebDancer [Wu et al., 2025]) and search APIs during the generation process. To further increase trajectory variability, we introduce perturbations to the search results. Specifically, at certain steps, we randomly mask specific sources (e.g., , Wikipedia) or suppress the top-K Youtu-LLM Technical Report Figure 8. The trajectory synthesis pipeline of the open-ended DR report generation. ranked results. These perturbations increase the difficulty of the QA task, often resulting in longer and more complex trajectories. Moreover, the increased diversity in search results leads to greater contextual variety within the trajectories. Such data contributes to improving the robustness and generalization ability of the trained agent models. Failed Trajectories Utilization. Failed trajectories also contain valuable information, and they are not entirely erroneous at every step. Incorporating such trajectories during pre-training is an important strategy for scaling up the amount of usable data. For all trajectories identified as incorrect in answering the given question, we append trajectory analysis segment. This analysis explicitly indicates that the trajectory fails to produce correct answer and provides step-by-step examination of the reasoning process, culminating in summary of the reasons behind the incorrect response. Open-ended Trajectory Synthesis For open-ended DR tasks, where the primary objective is to generate comprehensive research reports, we employ dual-pronged strategy combining forward and inverse trajectory synthesis to effectively scale data production. This hybrid approach allows us to simultaneously simulate the authentic, iterative research workflow through forward generation, while ensuring high data quality and factual grounding by reconstructing search paths backwards from expert-level documents via inverse synthesis. Forward Trajectory Synthesis. This approach constructs trajectory data in forward manner following the common deep research process. To generate large volume of high-quality data, we need to consider: (1) how to obtain substantial number of meaningful queries for each scenario; (2) how to ensure sufficiently thorough research; and (3) how to produce professional report writing that meets specific style requirements. To this end, we have developed comprehensive data synthesizing pipeline, which includes the following stages, as shown in Figure 8. (1) Acquisition of questions: Research and analysis question sources are obtained from three scenarios: a) Extract proprietary terms from encyclopedic corpora; b) Extract relevant knowledge or generate research topics and projects from academic textbook and paper corpora; c) Acquire basic company information from known financial reports. These three categories represent typical scenarios for research and analysis, ensuring the generation of valuable trajectories. (2) Thinking twice framework: When applying the existing DeepResearch framework for report research, there is often tendency toward insufficient investigation, as the model may mistakenly assume that it has already gathered sufficient material to answer the given question. However, for report writing tasks, broad exploration is essential to capture as complete and relevant information as possible regarding the research topic. To address this, we propose \"think twice\" method. This approach requires the model, when it believes it is ready to generate the report, to engage in systematic reconsideration. Specifically, it first summarizes all currently obtained information, and then sequentially evaluates each section to determine whether further 11 Youtu-LLM Technical Report Figure 9. The inverse trajectory synthesis pipeline of the open-ended DR. exploration is still possible. This process continues until every aspect that requires elaboration is thoroughly researched. Through this mechanism, all potentially relevant information can be comprehensively gathered. (3) Report generation Module: This module is designed to synthesize the final report from the gathered information. Since data retrieved via the web search API is often fragmented, generating report strictly following the initial research path can introduce bias, as earlier findings may be overlooked. To solve this, the module first consolidates and summarizes all the information recorded in information library, then creates structured outline to guide the narrative. Each section is systematically developed using the compiled material, ensuring balanced coverage. Finally, the complete report is refined for consistency and style. Inverse Trajectory Synthesis. Since it is difficult to directly evaluate the quality of forward-generated content in open-ended scenarios, we propose leveraging existing high-quality content to generate trajectories via inverse synthesis. In inverse trajectory synthesis, we generate the intermediate trajectory steps back from the final outcome. This approach offers several advantages. First, there is large volume of readily available high-quality open-ended content, such as academic papers, research reports, and legal documents, which can serve as reliable targets for inverse synthesis. Second, the outcomes of inverse trajectory synthesis are deterministic; by starting from verified high-quality content, we can ensure that the final outputs of the trajectories are of high quality. The main challenge lies in ensuring that the inversely generated trajectories contain complete and coherent chain of context that is sufficient to reconstruct the target content. To address the challenges of inverse trajectory synthesis, we carefully select suitable domains, focusing primarily on academic papers and legal decision documents. In these domains, citations in academic texts and references to specific statutes in legal decisions serve as inherently reliable relevance signals. These citations reflect expert-level judgment in identifying highly pertinent sources from vast body of academic literature or legal documents. As shown in Figure 9, the core of result-based inverse trajectory synthesis lies in reconstructing the context that leads to the final output. We leverage the reference relationships captured in graph structures to identify truly relevant referenced content. Based on this content, we generate multi-turn search queries that, through interaction with search APIs, gradually approximate the cited materials. Search results that is on the reference graph are treated as verified, correct search trajectories. Finally, we aggregate these Search API calls and refine them to produce coherent and high-quality trajectory data. Using this approach, we construct over 20B tokens of open-ended trajectories with an average length around 32K. Atomic Capability of DR Besides trajectories, we also decompose the DR process into the following critical atomic capabilities. Explicitly reinforcing these individual skills is also important for building robust foundation that sustains complex, multi-step research. Plan for solving problems: Used to break down problems, determine the relevant web search APIs to call, and formulate the next steps for the deep research plan. Self-Reflection: Analyzes the progress or correctness of the current state, providing improvements or determining the next steps. 12 Youtu-LLM Technical Report Summary: Summarizes the results of searches and visited web content, as well as consolidates the information that has already been obtained. Reading Comprehension: Identifies relevant information from large amount of context and integrates it to draw conclusions for each turn. To effectively enhance these capabilities, we have specifically constructed dedicated set of datasets as follows, allowing the agent to better master the core skills required for deep research. Tool Parsing Data. We utilized the authentic responses from the search and web page visit APIs, and employed an LLM to rewrite the inputs and outputs for analysis, examining what was accomplished and what conclusions can be drawn. By doing so, we can gain deeper understanding of the operational mechanisms of the core tools for deep research, as well as the processes of parsing, summarizing, and planning next steps based on the searched content. Trajectory Understanding Data. complete trajectory captures the full process of solving problem. We claim that such trajectories contain more than just procedural stepsthey encapsulate deep-seated problemsolving methodologies and experiential insights. To extract and articulate this implicit knowledge, we employ LLM-powered rewriting to summarize and review the entire trajectory. This process involves interpreting key moments within the trajectory, analyzing critical decision points, and drawing meaningful conclusions. We further examine the reasoning process and outcomes to assess their validity and explore potential optimizations. By doing so, we aim to explicitly capture the underlying methodologies embedded in the trajectory, thereby enhancing the models capabilities in both summarization and reflection reasoning. Reading Comprehension Data. specialized dataset focused on reading comprehension is constructed. large number of contexts from corpus is collected first. For each context, we generated accurate questionanswer pairs. These three-tuples (context, question, answer) formed the foundational data for reading comprehension. To further augment the complexity of the reading comprehension tasks, the context from each three-tuple was mixed with other, unrelated contexts. This process resulted in composite contexts that contained both relevant and irrelevant information. This methodology yielded the finalized reading comprehension dataset. Data Statistical Information Through the combination of closed-ended, open-ended synthesis trajectories (incorporating both forward and inverse construction methodologies) and atomic deep research capabilities data, comprehensive DR dataset of 60B tokens has been developed. This corpus encompasses wide range of prevalent DR scenarios, thereby ensuring coverage of critical capability dimensions."
        },
        {
            "title": "2.2.5 Tool-use and Planning Trajectory",
            "content": "Motivation and Goal Tool-use and strategic planning ability determines how the LLM interacts with the environment, moving beyond static internal reasoning to dynamic, goal-oriented problem solving. In fact, in the several types of major agentic trajectory data mentioned above (e.g., , coding and deep research trajectories), tool calling and task planning have naturally become an integral part of them. Additionally, we compiled more tool invocation and task planning trajectory data that is not limited to specific domain, and incorporated the high-quality portions into the trajectory pre-training phase of Youtu-LLM. Trajectory Construction Strategy Our approach to synthesizing 25B-token tool-use and planning data is formalized as the following four steps in Figure 10: (1) Collecting atomic tools; (2) Synthesizing multi-turn trajectories; (3) Verifying trajectory quality; (4) Expanding negative samples. 13 Youtu-LLM Technical Report Figure 10. Our trajectory construction strategy for synthesizing tool-use and planning data. Atomic Tool Collection. We first constructed mixed skill library from three perspectives: breadth, depth, and dependencies. In terms of breadth, referencing the tool collection methods in existing work such as Toucan-1.5M [Xu et al., 2025] and FunReason-MT [Xu et al., 2025], we collected thousands of diverse tools, including APIs, MCPs, and protocols; in terms of depth, our skill library includes simple actions such as weather and date search, and also covers professional tools such as text2sql and code interpreters. Furthermore, we retained the direct dependencies between skills as much as possible (e.g., , retrieve the table search over the table modify the table). We performed verifiable checks and parameter corrections on the above atomic skill library to ensure the completeness of each tool, finally generating tool graph. Multi-turn Trajectory Synthesis. Based on the tool graph, we synthesized multi-turn tool-use trajectory data with planning. We first randomly walked through the tool graph to generate valid tool execution chain that conforms to the dependencies between tool calls. This is equivalent to constructing outlinelevel prompt information for the LLM. Based on this execution chain, two different LLMs were used for adversarial generation, playing the roles of User and Assistant, simulating the generation of multi-turn agentic trajectories that follow the entire tool execution chain. During synthesis, we randomly inserted additional rules on the User side, such as vague statements, repeated requests, spelling errors, and personality simulation. Finally, based on the complete trajectory and tool execution chain, new LLM, defined as Expert, summarized the trajectory and generated planning data. Data Quality Verification. After obtaining the above trajectory data, we performed three-step quality check. The first step is format checking, which involves verifying the correctness of JSON and XML formats within the tool calls, ensuring that the called tools actually exist, and that the parameters used are valid and complete. The second step is tool call feasibility checking; we locally re-executed calling tests on data that used coding and mathematical executors. The third step is quality checking, where an LLM comprehensively judges the quality of each trajectory. Negative Trajectory Augmentation. From the validated trajectories, we randomly selected 5% for negative augmentation, using strategies such as deleting some available tools, modifying the User models statements, inserting topic shifts, and then asked the User and Assistant LLMs to rewrite the trajectories."
        },
        {
            "title": "3 Pre-training of Youtu-LLM",
            "content": "In this section, we will report on the pre-training of Youtu-LLM. We first describe the architecture and hyperparameter configuration, including the acquisition of the STEM-oriented tokenizer and the adoption of the dense MLA architecture. Secondly, we present the complete multi-stage pre-training recipe. 14 Youtu-LLM Technical Report"
        },
        {
            "title": "3.1.1 Tokenizer",
            "content": "Tokenizer Llama3 Qwen3 Mix Multi-stage (Youtu-LLM) Benchmark Performance Commonsense STEM Coding Compression Rate Vocab Size 34.3 32.3 34.9 35.0 19.7 21.5 20.6 23.1 30.8 30. 29.2 30.6 1.00 1.10 1.13 1.15 128,256 151,936 128,256 128,256 Table 3. We compare our tokenizer with two mainstream paradigms: Qwen3s tokenizer and Llama3s tokenizer. \"Mix\" indicates conventional training strategy, that is, directly training tokenizer over pre-training data mix-up. \"Multi-stage\" refers to our multi-stage training method, which is ultimately adopted by Youtu-LLM. Compression ratio refers to the relative number of tokens produced by other tokenizers compared to the number of tokens produced by the Llama3 tokenizer on the same corpus, with the latter serving as the baseline (1.0). Our tokenizer is byte-level BPE (BBPE) tokenizer [Sennrich et al., 2016, Wang et al., 2020] trained using modified HuggingFace Tokenizers training pipeline provided by superBPE [Liu et al., 2025]. We adopt stricter pre-tokenization scheme. Specifically, we segment all Chinese characters, Japanese kana, Hangul letters, and CJK punctuation into four standalone units, preventing them from merging with each other or with other parts of the text. For English, we largely follow the pre-tokenization design used in GPT-4o: we allow single punctuation mark or space-prefixed capital letter to be followed by sequence of lowercase letters, optionally with suffixes such as s, etc. For numeric tokenization, we avoid any multi-digit tokens and retain only the ten atomic digit tokens 09 [Yang et al., 2025, Ding et al., 2025]. This pre-tokenization design largely prevents tokens from spanning unrelated semantic units and, in particular, mitigates the issue of noisy tokens that arises in mixed ChineseEnglish text or Chinese text with whitespace. We start from the o200k vocabulary as our base. Because Zhang et al. [2025] have pointed out that some Chinese tokens in o200k are polluted, we retain only the first 100k tokens that contain solely ASCII characters, together with 1k additional tokens (not Chinese) to preserve basic multilingual and emoji coverage. We remove from o200k all tokens that contain at least one Chinese character, as well as tokens that are invalid under our new pre-tokenization scheme. The resulting 101k tokens constitute our base vocabulary. We continue training Chinese-specific tokens using our multi-domain Chinese corpus. We observe that Chinese token learning is strongly influenced by domain-specific vocabulary: for example, legal and patent corpora contain many technical terms whose frequencies are much higher than those of everyday words, which biases tokenizer training. To address this, we rebalance the corpus mixture to suppress the abnormally high frequency of such domain-specific terms. In addition, we remove Chinese tokens longer than four characters that tend to appear frequently only in few highly specific scenarios. At this stage, the vocabulary is expanded from 101k to 121k tokens. Given the strong demand for reasoning performance in current models, for Youtu-LLM, we also take reasoning-oriented scenarios into account at the tokenizer design stage. To this end, we further augment the vocabulary with specialized tokens for code and mathematical/technical content. Based on the collected Chinese and English reasoning corpora, we add 4k and 3k tokens, respectively. This multi-stage training strategy helps ensure balance between basic language ability and domain-specific reasoning capacity. After adding 256 reserved tokens, our final vocabulary size is 128,256, consistent with the Llama3 series [Grattafiori et al., 2024]. Compared with baselines such as the Qwen [Bai et al., 2023] tokenizer, our tokenizer achieves about 5% improvement in tokenization efficiency on general pre-training data, and this gain further increases to around 10% on reasoning-oriented data, as shown by the compression rate in Table 3. Using different tokenizers, we pre-trained multiple models from scratch with the same 1B GQA skeleton on 80B tokens, demonstrating that our multi-stage tokenizer performs best on Commonsense and STEM tasks. 15 Youtu-LLM Technical Report Model GQA-1B MLA-1B Chinese Generative Multi Choice English Generative Multi Choice Wiki PPL () 6.0 7.2 35.6 37. 17.8 18.5 49.6 50.7 16.5 15.4 Table 4. We compared GQA and MLA by pre-training 1B-parameter models from scratch on 500B tokens. We evaluated both models based on seven Chinese benchmarks and fifteen English benchmarks, and found that MLA-1B averagely outperformed GQA in all formats and languages tested. Model Param. Layers Hidden Size Attention Q/KV heads Max Len. Vocab Size Qwen3-1.7B SmoLM3 3B MiniCPM3-4B 1.70B 3.08B 4.07B Youtu-LLM 2B 1.96B 28 36 32 2,048 2,048 2,560 2,048 GQA GQA MLA MLA 16/8 16/4 40/ 16/16 40,960 65,536 32,768 151,936 128,256 73,448 131,072 128,256 Table 5. Architecture comparison between existing lightweight LLM and our Youtu-LLM."
        },
        {
            "title": "3.1.2 Dense MLA",
            "content": "As lightweight LLM designed for on-device scenarios, we plan to adopt the dense Multi-Latent Attention (MLA) [Liu et al., 2024] architecture from the very beginning of Youtu-LLMs development. On the one hand, for on-device scenarios, the Mixture-of-Experts (MoE) [Cai et al., 2025] architecture doesnt offer significant speed advantages compared to the vanilla dense architecture, as they requires more frequent I/O operations [Yi et al., 2023]; on the other hand, MLA introduces low-rank compression of the KV Cache and larger intermediate projection matrices [Liu et al., 2024], which helps improve the expressiveness and inference performance of the attention mechanism with constrained model parameters [Bai et al., 2025, Liu et al., 2025], surpassing vanilla GQA. Our experiments have also validated this  (Table 4)  . In Table 5, we list the main features of Youtu-LLM and other similar-size designs. We take Qwen3-1.7B [Yang et al., 2025], SmolLM3-3B [Bakouch et al., 2025] and MiniCPM3-4B [Hu et al., 2024] as the typical representations of lightweight LLMs. We use standard architecture with 32 layers and vocabulary size of 128,256. The MLA configuration of Youtu-LLM remains consistent with DeepSeek-V3 [Liu et al., 2025], whose KV lora rank dimension is 512, lora rank dimension is 1,536, QK nope head dimension is 128, QK rope head dimension is 64, and head dimension is 128. Therefore, with the input and output embeddings tied, Youtu-LLM has total of 1.96B parameters."
        },
        {
            "title": "3.2 Multi-stage Pre-training",
            "content": "Given that Youtu-LLM incorporated MLA, we follow the scaling laws introduced in DeepSeek-V3 to guide hyperparameter selection. Considering the training scale of 2B parameters and 10.84T tokens of data, we adopt an optimal global batch size of 12M tokens and learning rate of 4e-4. To improve training stability, we design an initializer range value that is coupled with the hidden dimension: for non-embedding weights, 2 5d , where denotes the hidden dimension; for embedding the initialized standard deviation is set to weights, the initialized standard deviation is twice that of the non-embedding weights. (cid:113) We demonstrate our entire pre-training recipe in Figure 11. Youtu-LLMs pre-training is divided into four stages: Commonsense Pre-training (Stage 1), STEMand Coding-centric Pre-training (Stage 2), General Mid-training (Stage 3), and Agentic Mid-training (Stage 4). The entire pre-training strategy follows the 16 Youtu-LLM Technical Report Figure 11. The pre-training recipe for Youtu-LLM. At the top, we illustrate the variations in the data recipe from Stage 1 to Stage 4, and it can be clearly observed that the average quality of the data gradually improves as the stages move forward. At the bottom, we draw the learning rate scheduler aligned with the data recipe. \"Commonsense-STEM-Agent\" design principle, which aligns with Jerome Bruners Spiral Curriculum theory [Bruner, 1960] and David Ausubels Progressive Differentiation principle [Ausubel, 1960]. In Stage 1, we consume 8.16T tokens for pre-training from scratch with sequence length of 8,192, with web pages and encyclopedia data accounting for 75%. Regarding the learning rate, we follow the paradigm of most LLM pre-training work [Liu et al., 2025, Bakouch et al., 2025, Walsh et al., 2024]. We warm up the model for 2000 steps. Next, in Stage 2, we significantly increase the proportion of STEM and Coding data to 60% while maintaining the maximum learning rate. Stage 3 is decaying and long context extension phase (8k -> 32k -> 128k), where the learning rate is reduced from the peak of 4e-4 to 4e-5. In this stage, the proportion of STEM and Coding data is further slightly increased, and Youtu-LLM gains more stable STEM, Coding, and long-context capabilities. Finally, in Stage 4, we adjust the decaying data to be primarily agentic trajectories (approximately 60%), and further decay the learning rate to 1e-7, with specific training strategies applied (reach Section 5.3 for details). Notably, we observed that training agentic trajectory data after general long-context training led to larger performance gains. This may be because the long-context model can better capture cross-segment key information in trajectory tasks."
        },
        {
            "title": "4 Post-training of Youtu-LLM",
            "content": "In this section, we introduce the post-training of Youtu-LLM, which mainly consists of Supervised FineTuning and Reinforcement Learning."
        },
        {
            "title": "4.1 Supervised Fine-Tuning",
            "content": "Supervised Fine-Tuning (SFT) serves as the critical alignment phase where the pre-trained model transitions from probabilistic next-token predictor to an instruction-following assistant. In this work, we implement rigorous SFT framework, which consists of two main parts: high-quality data engineering and decoupled two-stage training strategy. 17 Youtu-LLM Technical Report"
        },
        {
            "title": "4.1.1 Data Engineering",
            "content": "The quality of instruction-tuning data is the primary determinant of model performance [Guha et al., 2025, Bakouch et al., 2025]. In this subsection, we outline our systematic approach to data collection, reasoning answer construction, and multi-stage data cleaning. The overall data processing pipeline is illustrated in Figure 12. Figure 12. Overview of our data engineering workflow for high-quality Supervised Fine-Tuning. Data Collection and Construction We begin by curating comprehensive and heterogeneous dataset designed to ensure broad distribution of knowledge and robust reasoning capabilities. To achieve this, we first collect data from diverse array of sources, categorized as follows: (1) Mathematics: Ranging from elementary arithmetic to collegiate-level calculus and competition mathematics [Moshkov et al., 2025]. (2) Code: Containing high-quality repositories and competitive coding datasets [Ahmad et al., 2025]. (3) Scientific Reasoning: Encompassing data in scientific fields such as physics, chemistry, biology, engineering physics, etc. (4) Agentic Data: Covering trajectories involving precise function calls, deep research workflows (requiring multi-step information retrieval and summary), and code execution environments for dynamic task completion. (5) General Knowledge QA: Covering diverse professional fields, such as Agriculture, Forestry, Animal Husbandry, Fishery, Law, Psychology and so on. (6) Instruction Following: Containing diverse set of complex constraints and formatting requirements. (7) Role-Playing: Dialogues involving specific character profiles, historical figures, and fictional entities. (8) Creative Writing: Including poetry, storytelling, narrative generation, scriptwriting, and rhetorical rewriting. (9) Multi-turn Dialogue: Dialogue involving sequential context dependency and dynamic intent evolution. (10) Safety & Alignment: Containing refusal examples for toxic queries, bias mitigation, and adherence to ethical guidelines. Followed by the collection of open-source data and proprietary corpora, we enrich the samples with explicit long Chain-of-Thought (CoT) reasoning processes. We categorize this reasoning answer construction pipeline into two scenarios: (1) Queries without Answers: For open-ended questions or prompts lacking reference answers, we utilize advanced LLMs [Liu et al., 2025, Yang et al., 2025] to generate comprehensive responses that include both the reasoning process and the final answer. (2) Queries with Answers: We first evaluate the quality of the existing answers. For high-quality answers, we employ few-shot prompting strategy with the advanced LLMs to reconstruct the reasoning contents that logically lead to the existing gold-standard answer, ensuring the thought process aligns with the correct conclusion. For low-quality answers that are deemed factually incorrect, we discard it and again utilized the modern LLMs to regenerate the entire response (Reasoning + Answer) from scratch. In this way, we successfully construct high-quality initial SFT dataset enriched with detailed reasoning paths. 18 Youtu-LLM Technical Report Multi-stage Data Cleaning The process of generating answers via LLM APIs would inevitably introduce noise (e.g., API return errors and generation failures). Such noise could degrade model performance, thus, we implement comprehensive multi-stage cleaning pipeline to ensure high data quality. Heuristic Filtering: We first apply set of rule-based filters to remove low-quality samples. This includes detecting and removing: (1) Truncation errors: Instances where the text ends mid-sentence or lacks valid termination token. (2) Mixed-language noise: Segments containing unintended garbled character encodings that do not align with the target language. (3) Repetition loops: Text exhibiting pathological repetition of phrases, which often indicates generation failure in the source data. Model-Based Quality Scoring: To go beyond surface-level cleaning, we utilize state-of-the-art teacher models to act as judges. These teacher models score the samples based on three dimensions, including the logical coherence of the reasoning process, the fluency of the text, and the factual accuracy of the final answer. Samples falling below calibrated confidence threshold are automatically discarded to prevent adverse effects on the model training. De-contamination: To ensure that the models performance on benchmarks is not result of memorization or leakage, we perform de-contamination process against our evaluation benchmarks. Specifically, we utilize 32-gram exact matching to identify and remove any training samples that overlap with test sets. The remaining high-quality samples constitute our final training dataset."
        },
        {
            "title": "4.1.2 Two-Stage Supervised Fine-Tuning",
            "content": "Many recent studies [Bakouch et al., 2025, Yang et al., 2025] advocate that distilling reasoning capabilities should precede the broad acquisition of general knowledge. Consequently, we adopt two-stage SFT strategy to progressively elicit and refine the models abilities. This strategy is designed to first \"cold-start\" the models reasoning engines and subsequently refine its general versatility. Stage I: Reasoning SFT The primary goal of this initial stage is to elicit and solidify the models latent reasoning potential. As suggested by recent studies [Yang et al., 2025], focusing exclusively on logic-dense data would facilitate the emergence of complex problem-solving skills by allowing the model to learn the structure of argumentation and step-by-step deduction without the interference of varied stylistic constraints. In this stage, the training distribution is strategically skewed toward intensive intellectual tasks to develop the models higher-order reasoning abilities. Specifically, we curate logic-dense dataset from our training corpus, including: Mathematics (40%), Programming Code (30%), Scientific Reasoning (20%), and Agentic Tasks (10%). The rationale behind this composition is that these domains inherently require step-by-step deduction, thereby forcing the model to learn the internal representations essential for deep thinking. We fine-tune the pre-trained base model utilizing standard autoregressive cross-entropy loss. The training supervision is concurrently applied to both the intermediate reasoning trajectories and the final outputs. Finally, this training phase significantly strengthens the models chain-of-thought reasoning and structured response capabilities, providing robust foundation for subsequent capability expansions. Stage II: General SFT Following the reasoning enhancement, the second stage focuses on generalization and versatility. The objective is to equip the model with broad world knowledge and the ability to handle diverse user intents while preserving the reasoning capabilities acquired in Stage I. In this stage, we expand the training corpus to encompass the full diversity of our collected data as detailed in Section 4.1.1. Crucially, to mitigate catastrophic forgetting of the reasoning skills, we integrate the training subsets from Stage into this comprehensive mix. This strategy ensures that the model maintains the reasoning capability when aligning with wide array of general user instructions. Youtu-LLM Technical Report Furthermore, to accommodate tasks that do not require complex reasoning, we introduce simple yet effective strategy to induce dual-mode (\"think\" and \"non-think\") capability. Specifically, we process the training data by stripping the intermediate reasoning content to treat them as non-thinking samples. And we train the model on these non-thinking samples along with original thinking samples. In this way, we successfully enabled the model to switch between thinking mode (generating explicit thought processes) and non-thinking mode (direct answering) by conditioning the model on specific control tokens. This approach allows developers to control reasoning behaviors, while also reducing the complexity of deploying separate models for thinking and non-thinking tasks. We hypothesize that further refinement in the construction of non-reasoning data could yield superior performance, presenting direction for future research."
        },
        {
            "title": "4.2.1 Tasks and Verifiers",
            "content": "Math Mathematical data can typically be validated using objective criteria, making it highly effective for strengthening the foundational reasoning capabilities of models [Guo et al., 2025, He et al., 2025]. In the context of mathematical reasoning tasks, we have compiled large-scale dataset comprising competitionlevel math problems, spanning broad spectrum of difficultyfrom elementary to advanced academic levelsand including content in both Chinese and English. Given the substantial overlap often present across multiple data sources, we performed rigorous deduplication using embedding-based similarity measures to ensure data uniqueness and quality. To enhance verifiability, we systematically reformulated problem instructions so that model outputs adhere to structured formats that enable efficient automated validation. For example, we required answers to be explicitly presented within boxed{} notation or to compute multiple variables according to predefined formulaic patterns. We intentionally excluded multiplechoice questions, as their constrained answer space increases the risk of reward hacking and undermines genuine reasoning. To improve generalization in general-purpose applications, we augmented the dataset with practical mathematical reasoning problems, including tabular comprehension and date calculation. Code Our competitive programming dataset was developed through synthetic generation and licensed content, supplemented by problems with permissive intellectual property rights. To adapt these data for reinforcement learning, we implemented two distinct expansion strategies. The primary strategy involves autonomous code generation, where model-produced solutions are validated through robust, scalable code execution environment. To guarantee the integrity of this validation process, we employ an iterative verification mechanism that cross-references standard solutions with test suites. The secondary strategy focuses on program execution simulation, tasking the model with inferring outputs from inputs (or the inverse) [Li et al., 2025]. This paradigm is predicated on the belief that mastering input/output mapping fosters deeper structural and logical understanding of the source code. Complex Instructions We have collected diverse set of over 100 foundational instructions covering tasks such as text comprehension, role-playing, and creative writing. We employ hybrid rewards approach to validate this data. For simple tasks, we use rule-based code to verify constraints like word count and paragraph structure. For more complex instructions, we develop detailed rubrics and provide reference gold answers, utilizing an LLM-as-a-judge framework for scoring [Gunjal et al., 2025]. Safety We developed specialized safe reward model trained on datasets containing harmful and misleading instructions. In terms of safety preferences, we prioritize informative guidance over categorical refusal. For instance, when encountering queries such as bomb making, the model is trained to explain the inherent legal and safety risks and pivot the conversation toward constructive topics, rather than issuing standard cannot answer this disclaimer. Furthermore, we curated suite of adversarial datasetsincluding tech20 Youtu-LLM Technical Report niques such as story continuation and role-playingto perform safety red-teaming and enhance the models robustness against jailbreaking attempts. General Rewards To ensure the stability of the training process and prevent model degradation, we implement suite of global validation strategies designed to safeguard against common failure modes. These strategies include: Thinking Format Preservation In complex scenarios such as multi-turn dialogues, tool using, or long reasoning tasks, models often fail to maintain required structures (e.g., , <think>...</think>). Such omissions lead to degradation in output structuredness. We employ rule-based verification to ensure the completeness of these cognitive markers, penalizing outputs with fragmented or missing formats. Language Consistency Optimization techniques like Clip-Higher [Yu et al., 2025] can inadvertently induce linguistic drift, resulting in erratic code-switching (e.g., , mixing Chinese and English) or, in extreme cases, the generation of gibberish that triggers training collapse. To mitigate this, we pre-label the target language for each data entry. During reward calculation, we strip non-textual elements (e.g., , code snippets and mathematical formulas) and verify that the dominant language of the remaining text aligns with the target. Samples exhibiting linguistic inconsistency are penalized to maintain cross-lingual purity. Repetition Detection To improve response diversity and prevent the model from collapsing into cyclic patterns, we implement detection mechanism that monitors high-confidence n-gram frequencies. Given sequence = {id0, id1, . . . , idL1} and its transition probabilities = {p1, p2, . . . , pL1}, we define multiset of \"confident n-grams\" G. An n-tuple is recorded only when its generation probability pi exceeds confidence threshold p, = {(idin+1, . . . , idi) pi > p, 1 < L} Let C(g) denote the frequency of an n-gram within G. We define unique(G) as the set of distinct ngram types present in G, such that each unique pattern is counted only once regardless of its frequency. We isolate the subset of recurring patterns Gdup where C(g) > 1. The repetition ratio is calculated as, = unique(G),C(g)>1 C(g) By identifying sequences where > r, this method serves as trigger to suppress the likelihood of the model selecting repetitive patterns. This intervention forces the generator to explore alternative token paths, increasing the semantic diversity and structural variance of the generated response."
        },
        {
            "title": "4.2.2 Training Dynamics",
            "content": "Ensuring training-inference consistency is critical for maintaining the on-policy characteristics of reinforcement learning algorithms, which in turn facilitates superior training stability and enables longer optimization horizons [Yao et al., 2025, Liu et al., 2025]. We have implemented several refinements to address this challenge, as detailed below. FP16 Precision Discrepancies often arise between the training and inference stages due to the use of different operators or hardware kernels. These inconsistencies can cause the training policy to deviate significantly from the behavior observed during rollout. As illustrated in Figure 13(a), when employing BF16 precision, the prediction probability error between training and inference grows progressively as iterations increase. This divergence introduces substantial noise into the optimization process, leading to performance plateaus. This is evident in Figure 13(b) and (c), where benchmarks for mathematical and coding stagnate 21 Youtu-LLM Technical Report Figure 13. Training Dynamics with BF16/FP16 precision and consistent sampling. The reported accuracy metrics are derived from internal mathematical and coding benchmarks. after approximately 50 training steps. In contrast, our findings align with prior research [Qi et al., 2025] suggesting that FP16 precision is more effective than BF16 at maintaining training-inference consistency. As shown in Figure 13(a), the probability gap between train and rollout is markedly narrower when using FP16. This reduction in numerical drift allows for more precise gradient updates. The empirical results in Figure 13(b) and (c) validate that by adopting FP16, the model avoids early performance saturation and achieves superior results on both mathematical and coding benchmarks compared to the BF16 baseline. Consistent Sampling We define train policy  as the policy currently being optimized and rollout policy rollout used for fast sampling. For given prompt q, the KL metric K(q) measures the divergence between the training and rollout policies. K(q) = arollout(q) (cid:20) (aq) rollout(aq) 1 log (cid:21) (aq) rollout(aq) To ensure training stability, we introduce consistent sampling based on GRPO [Shao et al., 2024]. group of outputs Oq = {o1, . . . , oG} generated for prompt is admitted for training if and only if K(q) < , where  is hard threshold (e.g., ,  = 0.01). Groups exceeding this threshold are discarded prior to gradient computation, ensuring the model only learns from prompts where the policy drift is constrained. The CF-GRPO objective is formulated over the filtered distribution D, where every batch of size is guaranteed to satisfy the drift constraint. The objective JCFGRPO() is defined as: JCFGRPO() = qP(QK(q)<),{oi}G i=1 rollout (cid:34)"
        },
        {
            "title": "1\nG",
            "content": "G i=1 min (cid:0)i Ai, clip(i, 1 , 1 + ) Ai (cid:1) (cid:35) with the importance sampling ratio i and the advantage Ai. The empirical results in Figure 13(b) and (c) demonstrate that our approach significantly enhances training stability."
        },
        {
            "title": "5 Experimental Results",
            "content": "We present the results of our model evaluation on diverse array of benchmarks, spanning multiple domains, to comprehensively assess its capabilities. The following sections provide detailed breakdown of its performance, involving general and agent benchmarks on our base and instruct models, respectively."
        },
        {
            "title": "5.1 General Evaluation",
            "content": "22 Youtu-LLM Technical Report"
        },
        {
            "title": "5.1.1 General Evaluation of Base Model",
            "content": "We evaluate the general capabilities of the base model across four dimensions: Commonsense knowledge, STEM, Coding, and Long Context. The benchmarks involved are as follows: Commonsense knowledge. To assess broad world knowledge and reasoning in both English and Chinese, we utilize MLQA-Zh [Lewis et al., 2020], MMLU-ProX-Zh [Xuan et al., 2025], and MMLU-Pro [Wang et al., 2024]. These benchmarks focus on evaluating world knowledge and provide insights into the models ability to handle both fact-based and conceptual questions. The suffix \"-Zh\" indicates that the evaluation is restricted to the Chinese subset. For MMLU-Pro and MMLU-ProX-Zh, we conduct 5-shot evaluations and report Exact Match (EM) accuracy. For MLQA-Zh, we use 3-shot setting and also report EM. STEM. Mathematical ability is evaluated using suite of datasets, including GSM8K [Cobbe et al., 2021] and MGSM-Zh [Shi et al., 2022] for grade-school math, MATH [Hendrycks et al., 2021] for competition-level problems, and Big-Bench-Hard (BBH) [Suzgun et al., 2023] for complex logical reasoning. Additionally, we include GPQA [Rein et al., 2024] for expert-level scientific QA, and Humanitys Last Exam (HLE) [Phan et al., 2025] for human-like exam-style math questions. It is worth noting that GPQA and HLE are far more difficult to directly test with small LLMs. Therefore, we choose to evaluate with multi-choice (-MC) paradigm. Unlike GPQA, HLE includes both multi-choice and generative exams, therefore we only retrieve the multi-choice subset for evaluation. In terms of evaluation settings, we use 8-shot for GSM8K and MGSM-Zh, and report strict match scores for both. We use 4-shot, 3-shot, 5-shot and 3-shot for MATH, BBH, GPQA-MC and HLE-MC respectively, following most conventions. EM is reported on MATH and BBH, while two multi-choice benchmarks are evaluated with normed accuracy. Coding. Coding proficiency is measured using MBPP [Austin et al., 2021] and MBPP+ [Liu et al., 2023] for basic programming tasks, HumanEval [Chen, 2021] and HumanEval+ [Liu et al., 2023] for functional code generation, LiveCodeBench v6 [Jain et al., 2024] for latest coding scenarios, CRUXEval [Gu et al., 2024] for atomic coding, and RepoBench [Liu et al., 2024] for repository-level code understanding and generation. We report pass@1 success rate for all coding benchmarks except RepoBench, which is suggested to be evaluated with EM. MBPP and MBPP+ adopt 3-shot test, HumanEval and HumanEval+ use 0-shot, LiveCodeBench v6 takes 3-shot, and CRUXEval uses 1-shot. Its worth noting that the 1-shot setting in CRUXEval refers to the examples built into the prompt in its official code1. Long-Context. The models ability over long context is evaluated using LongBench v2 [Bai et al., 2025] and needle-in-a-haystack (NIAH) testing [Hsieh et al., 2024]. Specifically, we adapt LongBench v2 into set of 3-shot multiple-choice questions designed to assess base model capabilities, and the overall accuracy is reported. For NIAH evaluation, we use the HELMET suite [Yen et al., 2024], which incorporates the RULER dataset [Hsieh et al., 2024] as its testbed for NIAH, the average recall rate is reported. We evaluate the models in Table 6 based on the open-source evaluation framework lm-evaluation-harness2, and using the version of Transformers recommended in their own configurations. Overall, in terms of general capabilities, Youtu-LLM 2B Base significantly outperforms the similar sized baselines, and achieves competitive results to the larger Qwen3-4B Base. 1https://github.com/facebookresearch/cruxeval/blob/main/prompts.py 2https://github.com/EleutherAI/lm-evaluation-harness 23 Youtu-LLM Technical Report Qwen3 1.7B Base SmolLM3 3B Base Gemma3 4B Base Qwen3 4B Base Llama3.1 8B Base Youtu-LLM 2B Base 34.9 38.1 32.5 68.2 57.1 28.1 53.0 30.4 10.7 55.6 71.0 49.9 41.3 5.1 40.6 21. 28.0 79.8 35.3 38.0 26.7 67.3 40.7 40.8 60.0 26.6 3.1 51.0 66.1 34.8 28.1 2.9 42.1 21.8 28.8 75.0 29.4 40.1 24. 38.5 33.0 24.4 51.6 28.6 8.0 45.8 61.9 36.6 28.1 2.9 39.7 23.0 26.6 99.5 46.1 47.2 45.2 80.8 69.7 44.8 70.8 37.8 15.0 67.5 80.8 57.6 49.9 6.9 54.8 25. 25.8 83.0 36.2 43.0 25.4 47.8 35.9 21.5 62.9 30.1 11.5 49.4 62.7 36.0 28.1 3.4 42.3 25.2 27.8 99.8 48.4 43.5 40. 77.6 68.9 44.4 60.0 33.3 17.4 66.6 81.8 64.6 57.3 9.7 55.9 22.7 27.2 98.8 Benchmark Commonsense MMLU-Pro (5-shot) MLQA-Zh (3-shot) MMLU-ProX-Zh (5-shot) STEM GSM8K (8-shot) MGSM-Zh (8-shot) MATH (4-shot) BBH (3-shot) GPQA-MC (5-shot) HLE-MC (3-shot) Coding MBPP (3-shot) MBPP+ (3-shot) HumanEval (0-shot) HumanEval+ (0-shot) LiveCodeBench v6 (3-shot) CRUXEval (1-shot) RepoBench (3-shot) Long Context LongBench v2 (3-shot) NIAH Table 6. General benchmark performance of similar-sized base models. The top performance is highlighted in bold, and the second-place performance is marked with an underline."
        },
        {
            "title": "5.1.2 General Evaluation of Instruct Model",
            "content": "For the instruct models, we evaluate them from four perspectives: Commonsense Knowledge, Instruction Following & Text Reasoning, STEM, and Coding. The benchmarks involved are described as follows: Commonsense Knowledge. We evaluate the models foundational knowledge using MMLU-Redux [Gema et al., 2025] and MMLU-Pro [Wang et al., 2024]. The accuracy score is reported. Instruction Following & Text Reasoning. To assess how well models adhere to complex constraints and reason through linguistic tasks, we utilize IFEval [Zhou et al., 2023], DROP [Dua et al., 2019], and MUSR [Sprague et al., 2023]. IFEval focuses on objective instruction following (e.g., , formatting and keyword constraints), DROP tests discrete reasoning over paragraphs requiring mathematical operations on text, and MUSR evaluates the models ability to perform long-chain logical deduction in narrative contexts. As for the evaluation metrics, we utilize the strict-prompt accuracy for IFEval, the F1 score for DROP, and the accuracy score for MUSR. STEM. Mathematical and scientific proficiency is measured across spectrum of difficulty. We include MATH-500 [Lightman et al., 2023] for competition-level mathematics and AIME (2024 and 2025 sets) [AIME, 2025] for elite-level mathematical problem solving. For complex logical reasoning, we use BBH [Suzgun et al., 2023]. Furthermore, we evaluate expert-level scientific knowledge using GPQA-Diamond [Rein et al., 2024]. When evaluating STEM benchmarks, we adopt zero-shot settings, except for BBH, where we employ 3-shot setup. For BBH, we report the accuracy score. And for other benchmarks, we report the pass@1 metric. Specifically, we generate 4, 16, 16, and 8 responses per question for MATH-500, AIME 24, AIME 25, and 24 Youtu-LLM Technical Report Benchmark DeepSeek-R1Distill-Qwen 1.5B Qwen3 1.7B SmolLM3 3B Qwen3 4B DeepSeek-R1Distill-Llama 8B Youtu-LLM 2B Commonsense Knowledge Reasoning MMLU-Redux (0-shot) MMLU-Pro (5-shot) 53.0 36.5 Instruction Following & Text Reasoning IFEval (0-shot) DROP (3-shot) MUSR (0-shot) STEM MATH-500 (0-shot) AIME 24 (0-shot) AIME 25 (0-shot) GPQA-Diamond (0-shot) BBH (3-shot) Coding HumanEval (0-shot) HumanEval+ (0-shot) MBPP (0-shot) MBPP+ (0-shot) LiveCodeBench v6 (0-shot) 29.4 41.3 43.8 84.8 30.2 23.1 33.6 31. 64.0 59.5 51.5 44.2 19.8 74.1 54.9 70.4 72.5 56.6 89.8 44.2 37.1 36.9 69.1 84.8 76.2 80.5 67.7 30.7 75.6 53. 60.4 72.0 54.1 91.8 46.7 34.2 43.8 76.3 79.9 74.7 66.7 56.7 30.8 83.8 69.1 83.6 82.9 60.5 95.0 73.3 64.2 55.2 87. 95.4 87.8 92.3 77.6 48.5 78.1 57.5 34.6 73.1 59.7 90.8 52.5 34.4 45.5 77.8 88.1 82.5 73.9 61.0 36.8 75.8 61. 81.2 86.7 57.4 93.7 65.4 49.8 48.0 77.5 95.9 89.0 85.0 71.7 43.7 Table 7. Comprehensive comparison across various benchmarks of similar-sized instruct models. Models are evaluated on commonsense knowledge, instruction following & text reasoning, stem, and coding capabilities. The top performance is highlighted in bold, and the second-place performance is marked with an underline. GPQA-Diamond respectively, to obtain an unbiased estimate of the pass@1 score. Coding. We measure coding proficiency using standard functional benchmarks and real-world scenarios. This includes HumanEval [Chen, 2021] and MBPP [Austin et al., 2021], along with their rigorous \"+\" versions [Liu et al., 2023]. Additionally, we use LiveCodeBench v6 [Jain et al., 2024] to evaluate the models performance on the latest competitive programming problems. When evaluating coding benchmarks, we adopt zero-shot settings and default to report the pass@1 metric. Here, for HumanEval/HumanEval+ and MBPP/MBPP+, we generate 4 responses for each question. And 16 responses are generated for LiveCodeBench v6 benchmark. We evaluated the instruct models based on open-source evaluation frameworks, including evalscope3 and evalplus4. Here, the majority of the benchmarks were conducted using evalscope, with the exception of HumanEval, MBPP, HumanEval+, and MBPP+, which were assessed using the official evalplus repository. For all listed models, they are evaluated in thinking mode (generating explicit thought processes). We utilized sampling temperature of 1.0, top-p value of 0.95, top-k value of 20, and presence penalty of 1.5. Besides, we set the maximum output length to 32, 768 tokens to ensure sufficient thinking space. We compare our Youtu-LLM 2B instruct model with DeepSeek-R1-Distill-Qwen 1.5B, Qwen3 1.7B, SmolLM3 3B, and DeepSeek-R1-Distill-Llama 8B. The evaluation results are shown in Table 7. The Youtu-LLM 2B model outperforms similarly sized baselines (such as Qwen3-1.7B and SmolLM3 3B) and even exceeds the larger DeepSeek-R1-Distill-Llama 8B model in most capabilities. Furthermore, it exhibits competitive performance against Qwen3 4B, particularly in coding, instruction following and text reasoning (e.g., , HumanEval and DROP). These results underscore the efficacy of the proposed method in developing high-performance, light-weight LLMs. 3https://github.com/modelscope/evalscope 4https://github.com/evalplus/evalplus 25 Youtu-LLM Technical Report"
        },
        {
            "title": "5.2.1 Agentic Evaluation of Base Model",
            "content": "To evaluate the agent-oriented capability of the base model during the agentic pre-training, we use APTBench [Qin et al., 2025] as the benchmark5. APTBench serves as benchmark specifically designed to evaluate agent capabilities of base models. It acts as proxy metric for agent ability without requiring additional post-training or end-to-end evaluation. APTBench adopts combination of few-shot multiple-choice question (MCQ), text completion (TC) and true/false questions, making it suitable for efficient assessment during the pre-training/mid-training phase. Specifically, APTBench includes data from four domains: code, deep research, tool, and math. Code. This scenario evaluates the models ability to handle complex coding tasks across the Environment Setup and Issue Fixing. To comprehensively assess the models potential as an coding agent, the benchmark measures two core agent capabilities: planning and action. Planning measures whether the base model knows which step should be executed next while action measures its ability to write correct bash commands. We further test other important atomic capabilities of coding tasks, including error handling, bug localization and fix/test patch identification. Details of the APTBench-Code are shown in Table 8. Task Type Task Definition Evaluation Mechanism # Instances Planning Stepwise environment setup strategies or SWE debugging plans. MCQ: Distinguish the correct plans from the wrong ones. Ground-truth plans are extracted from real successful trajectories. Action Atomic Execution of terminal commands based on prior context. TC: Write the exact next bash command based on the given trajectory context. Focuses on core engineering tasks like error handling, bug localization and fix/test patch identification. MCQ: Select correct fixes/code snippet from options including irrelevant or failed negative choices. 680 1,325 1,722 Table 8. Overview of APTBench-Code tasks. Task Type Task Definition Evaluation Mechanism Planning Stepwise decision-making for information seeking/organizing report generation. Action Synthesizes gathered information into outputs, from short answers to detailed reports. MCQ: Choose correct plans verified by ground-truth agent trajectories from the wrong plans. TC: Generate short answers from trajectories. Atomic Checks citation accuracy and factual grounding of statements. MCQ: Distinguish supported statements from unsupported distractors. # Instances 1,329 564 362 Table 9. Overview of APTBench-DR tasks. DeepResearch (DR). This scenario assesses the models ability to synthesize information and navigate the web to answer Closed-ended (concise fact-seeking) and Open-ended (comprehensive report generation) questions. Similar to the coding scenario, the benchmark measures two core agent capabilities: planning and action. Other critical atomic abilities such as accurate citation and factual grounding are also tested. The detailed explanation is shown in Table 9. 5https://github.com/TencentYoutuResearch/APTBench 26 Youtu-LLM Technical Report Task Type Task Definition Evaluation Mechanism # Instances Planning Action Identify the most critical atomic ability required to solve given mathematical problem. MCQ: Select one essential atomic ability from five candidates. Execute atomic computational subtasks involved in mathematical reasoning, emphasizing precise symbolic and numerical manipulation. MCQ: Solve calculation problems with distractors generated via operator, fraction, and numeric perturbations from ground truth. 1, 1,101 Feedback Perform trajectory-level verification by judging whether full solution process is correct. T/F: Determine the correctness of solution trajectories from given problems. 1,198 Table 10. Overview of APTBench-math Tasks. Task Type Task Definition Evaluation Mechanism # Instances Tool-Select Select the correct tool call from APIs based on the given context. Tool-Param Determine the corresponding parameters that need to be passed to the tool function. MCQ: Select the correct tool name. 894 TC: Write the parameters name and value. 2,103 Table 11. Overview of APTBench-tool tasks. Math. Most existing mathematical reasoning benchmarks emphasize end-to-end answer correctness, offering limited insight into the reasoning process and providing little support for evaluating agentic capabilities. In contrast, we evaluates mathematical reasoning through three orthogonal agentic dimensions: planning, action, and feedback. This design enables fine-grained, capability-oriented assessment beyond final-answer accuracy, as details shown in Table 10. Tool. Since general tool-use capability is core competency of agents, we design an additional evaluation task to assess this ability in base models, as an addition to Code, DR, and Math tasks. Specifically, we adapt AceBench [Chen et al., 2025] and BFCL V4 [Patil et al., 2025] into two types of problems: The first type presents given context and asks the model to select the appropriate tool from predefined list, formulated as multiple-choice question (MCQ). The second type provides both the context and the selected tool, and requires the base model to generate the corresponding tool parameters as text completion task (TC). Details of the APT-Tool are shown in Table 11. We use APT-Bench to evaluate Youtu-LLM 2B Base and series of similarly sized LLMs. The comparison results are shown in Table 12. Youtu-LLM 2B bases agentic capabilities are close to those of Qwen3 4B base across all dimensions, significantly outperforming other models of similar size. Category Code Deep Research Math Tool Qwen3 1.7B Base SmolLM3 3B Base Gemma3 4B Base Qwen3 4B Base Llama3.1 8B Base Youtu-LLM 2B Base 25.1 28.5 59.9 56. 24.3 27.2 60.7 59.1 32.8 36.4 59.8 61.7 41.9 40.5 70.5 65.8 23.6 30.0 60.1 64.1 37.9 38.6 68.0 64.2 Table 12. APTBench performance of Youtu-LLM 2B Base and other similarly-sized base models. 27 Youtu-LLM Technical Report Benchmark Qwen3 1.7B SmolLM3 3B Qwen3 4B Youtu-LLM 2B Deep Research GAIA xbench Code SWE-Bench-Verified EnConda-Bench Tool BFCL V3 2-Bench 11.4 11. 0.6 10.8 55.5 2.6 11.7 13.9 7.2 3.5 31.5 9.7 25.5 18. 5.7 16.1 61.7 10.9 33.9 19.5 17.7 21.5 58.0 15.0 Table 13. Agent benchmark performance of similarly-sized SOTA instruct models."
        },
        {
            "title": "5.2.2 Agentic Evaluation of Instruct Model",
            "content": "Deep Research (DR). We test the instruct models on two established deep research benchmarks: GAIA [Mialon et al., 2023] and xbench [Chen et al., 2025]. Our methodology integrates standard ReAct framework [Yao et al., 2022] for each candidate model. Evaluation of both tasks was performed using the LLM-as-a-Judge paradigm, with Pass@1 as the primary metric. We adopted the evaluation protocol and implementation details of WebDancer [Wu et al., 2025], ensuring all models were assessed in thinking mode. To enhance reliability, each test case was executed four times, and the average accuracy was reported. Code. For the coding agentic capability, we use two benchmarks: SWE-Bench-Verified [Jimenez et al., 2023] and EnConda-Bench [Kuang et al., 2025]. For SWE-Bench-Verified, we use R2E-Gym [Jain et al., 2025] and Openhands [Wang et al., 2024] scaffold with the maximum step of 50 for evaluation. Across all models, we employ the non-thinking mode and report the average resolve rate across 8 trials. For EnConda-Bench, we test models in non-thinking mode and report the accuracy of the fix suggestion in environment setup tasks. Tool. We evaluate instruct models on two established Tool benchmarks: BFCL V3 [Patil et al., 2025] and 2-bench [Barres et al., 2025]. For BFCL V3, all models are evaluated with the FC format in the thinking mode. And we utilize the following sampling parameters to generate responses, i.e., temperature = 1.0, top-p = 0.95, top-k = 20, and presence penalty = 1.5. For 2-bench, we use the non-thinking mode of the compared models with other generation parameters being kept the same with BFCL testing. The evaluation results are presented in Table 13. Youtu-LLM 2B achieves superior performance compared to the best baseline on most benchmarks, with significant improvements. It is important to note that, to date, no suitable agentic mathematical benchmark exists to effectively evaluate the corresponding capabilities of instruction-tuned models. This remains an open challenge in the field. Additionally, Gemma3 4B and Llama3.1 8B do not natively support tool-calling mechanism, which precludes their evaluation on the Deep Research and Tool tasks."
        },
        {
            "title": "5.3 Analysis of Agentic Mid-Training",
            "content": "As described in Section 3.2, we use the model after general mid-training (Stage 3) as the starting point to conduct native agentic mid-training on Youtu-LLM. The models performance is evaluated using the average APTBench score to analyze scaling behavior and training effectiveness. We further compare the post-trained 28 Youtu-LLM Technical Report Figure 14. The impact of training data scaling on APTBench performance. models with and without agentic mid-training on the critical agent benchmarks. Mid-Training Scaling. As shown in Figure 14, we observe clear logarithmic trend between the number of training tokens and model agentic performance. significant improvement occurs within the first 34B tokens (approximately 20% of the total), indicating that agentic behaviors can be efficiently acquired through agentic mid-training. Moreover, agentic mid-training yields consistent performance gains across the entire 340B-token training budget, achieving an overall improvement of more than 6%. These results validate the effectiveness of our agentic mid-training paradigm. Benchmark Youtu-LLM 2B w/o AMT Youtu-LLM 2B Relative Ratio GAIA xbench SWE-Bench-Verified EnConda-Bench BFCL V3 2-Bench 31.1 18.0 12.4 19.8 59.3 12.5 33.9 19.5 17.7 21.5 58.0 15. +2.8 +1.5 +5.3 +1.7 -1.3 +2.5 +9.0% +8.3% +42.7% +8.6% -2.2% +20.0% Table 14. Performance of post-trained models with and without agentic mid-training (AMT). To systematically investigate the impact of individual trajectory data in Section 2.2 on overall model training, we conduct dedicated ablation study focusing exclusively on separate trajectory pre-training of each category, including Appendix for Agentic-CoT trajectory, Appendix for math trajectory, Appendix for deep research trajectory, Appendix for code trajectory and Appendix for tool-use trajectory. Specifically, we take the base model after completing the 8k-stage (Stage 2) training as the starting point for all ablation experiments. Agentic Performance Analysis. The improvements brought by the agentic mid-training stage on APTBench also effectively transfer to downstream post-training performance. We evaluate models with and without agentic mid-training under aligned post-training settings, as described in Section 4, and compare their performance across multiple agent-specific evaluation benchmarks, which are widely adopted in Deep Research, software engineering, and tool-use evaluation tasks. As shown in Table 14, the base model equipped with agentic mid-training demonstrates significant performance improvements on most of the benchmarks. The average relative improvement rate of the six agent benchmarks is 14.4%, which aligns perfectly with the 13.7% improvement on APTBench as shown in Figure 14. 29 Youtu-LLM Technical Report We further conduct pass@k evaluation on SWE-Bench-Verified as an example to demonstrate the effectiveness of AMT. As illustrated in Figure 15, the illusion of AMT yields consistent and substantial performance uplift across all values. Specifically, at = 1, the AMT-enhanced model achieves resolve rate of 17.7%, relative improvement of 42.7% over the 12.4% baseline. This margin widens as increases, reaching 33.4% at = 8 compared to 26.2% for the version without AMT. Notably, the 95% confidence intervals for these two variants remain largely non-overlapping, particularly at higher values, signifying that the performance gains are statistically significant and not attributable to sampling variance. AMT endows the model with better long-horizon planning and error-correction priors, resulting in model that is not only more accurate on the first attempt but scales more effectively with repeated sampling. Figure 15. Pass@k comparison on SWE-Bench-Verified with Openhands scaffold across different models (with 95% confidence intervals smeared)."
        },
        {
            "title": "6 Conclusion",
            "content": "In this work, we introduce Youtu-LLM, lightweight 1.96B-parameter language model that effectively unlocks native agentic potential. It successfully balances computational efficiency with deep reasoning capabilities, proving highly effective for complex agent tasks. Most notably, the integration of our scalable Agentic Mid-training paradigm enables the model to internalize planning and reflection behaviors. Extensive evaluations confirm that Youtu-LLM sets new state-of-the-art for sub-2B models. Remarkably, it even surpasses larger LLMs like SmolLM3-3B and Qwen3-4B on challenging agentic benchmarks, demonstrating robust agentic capabilities in domains including code, deep research and math. Despite promising, we acknowledge certain limitations when compared to larger foundation models, which guide our future research roadmap: First, constrained by computational resources, gap in agentic capabilities remains compared to large proprietary LLMs. We aim to bridge this by evolving our model into world model that simulates execution dynamics to ground reasoning in robust understanding of the environment. Second, model efficiency remains challenge, as long reasoning trajectories inevitably increase inference latency. Future work will explore more efficient model architecture, such as Diffusion LLMs, to optimize inference speed. Third, our current scope is confined to text-based environments. We intend to extend our model to multimodal scenarios, equipping the model with native omni-modal perception for complex real-world tasks. 30 Youtu-LLM Technical Report"
        },
        {
            "title": "References",
            "content": "[1] Dan Hendrycks, Dawn Song, Christian Szegedy, Honglak Lee, Yarin Gal, Erik Brynjolfsson, Sharon Li, Andy Zou, Lionel Levine, Bo Han, et al. definition of agi. arXiv preprint arXiv:2510.18212, 2025. [2] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [3] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report, 2025. [4] Aske Plaat, Max van Duijn, Niki van Stein, Mike Preuss, Peter van der Putten, and Kees Joost Batenburg. Agentic large language models, survey. arXiv preprint arXiv:2503.23037, 2025. [5] Peter Belcak, Greg Heinrich, Shizhe Diao, Yonggan Fu, Xin Dong, Saurav Muralidharan, Yingyan Celine Lin, and Pavlo Molchanov. Small language models are the future of agentic ai. arXiv preprint arXiv:2506.02153, 2025. [6] Shengding Hu, Yuge Tu, Xu Han, Chaoqun He, Ganqu Cui, Xiang Long, Zhi Zheng, Yewei Fang, Yuxiang Huang, Weilin Zhao, et al. Minicpm: Unveiling the potential of small language models with scalable training strategies. arXiv preprint arXiv:2404.06395, 2024. [7] Elie Bakouch, Loubna Ben Allal, Anton Lozhkov, Nouamane Tazi, Lewis Tunstall, Carlos Miguel Patio, Edward Beeching, Aymeric Roucher, Aksel Joonas Reedi, Quentin Galloudec, Kashif Rasul, Nathan Habib, Clmentine Fourrier, Hynek Kydlicek, Guilherme Penedo, Hugo Larcher, Mathieu Morlon, Vaibhav Srivastav, Joshua Lochner, Xuan-Son Nguyen, Colin Raffel, Leandro von Werra, and Thomas Wolf. SmolLM3: smol, multilingual, long-context reasoner. https://huggingface.co/blog/ smollm3, 2025. [8] Raghav Sharma and Manan Mehta. Small language models for agentic systems: survey of architectures, capabilities, and deployment trade offs. arXiv preprint arXiv:2510.03847, 2025. [9] Junyu Luo, Weizhi Zhang, Ye Yuan, Yusheng Zhao, Junwei Yang, Yiyang Gu, Bohan Wu, Binqi Chen, Ziyue Qiao, Qingqing Long, et al. Large language model agent: survey on methodology, applications and challenges. arXiv preprint arXiv:2503.21460, 2025. [10] Aohan Zeng, Xin Lv, Qinkai Zheng, Zhenyu Hou, Bin Chen, Chengxing Xie, Cunxiang Wang, Da Yin, Hao Zeng, Jiajie Zhang, et al. Glm-4.5: Agentic, reasoning, and coding (arc) foundation models. arXiv preprint arXiv:2508.06471, 2025. [11] Baixuan Li, Bo Zhang, Dingchu Zhang, Fei Huang, Guangyu Li, Guoxin Chen, Huifeng Yin, Jialong Wu, Jingren Zhou, et al. Tongyi deepresearch technical report. arXiv preprint arXiv:2510.24701, 2025. [12] Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, and et al. Deepseek-v3 technical report, 2025. URL https://arxiv.org/abs/2412.19437. [13] Andrei Z. Broder. On the resemblance and containment of documents. Proceedings. Compression and Complexity of SEQUENCES 1997 (Cat. No.97TB100171), pages 2129, 1997. URL https://api. semanticscholar.org/CorpusID:11748509. [14] Sariel Har-Peled, Piotr Indyk, and Rajeev Motwani. Approximate nearest neighbor: Towards removing the curse of dimensionality. Theory Comput., 8:321350, 2012. URL https://api.semanticscholar. org/CorpusID:6110572. [15] Alfred Aho and Margaret Corasick. Efficient string matching: an aid to bibliographic search. Communications of the ACM, 18(6):333340, 1975. 31 Youtu-LLM Technical Report [16] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. [17] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. [18] Zhihong Shao, Yuxiang Luo, Chengda Lu, ZZ Ren, Jiewen Hu, Tian Ye, Zhibin Gou, Shirong Ma, and Xiaokang Zhang. Deepseekmath-v2: Towards self-verifiable mathematical reasoning. arXiv preprint arXiv:2511.22570, 2025. [19] Xianyang Liu, Yilin Liu, Shuai Wang, Hao Cheng, Andrew Estornell, Yuzhi Zhao, and Jiaheng Wei. Agenticmath: Enhancing llm reasoning via agentic-based math data generation. arXiv preprint arXiv:2510.19361, 2025. [20] Songyang Gao, Yuzhe Gu, Zijian Wu, Lingkai Kong, Wenwei Zhang, Zhongrui Cai, Fan Zheng, Tianyou Ma, Junhao Shen, Haiteng Zhao, et al. Long-horizon reasoning agent for olympiad-level mathematical problem solving. arXiv preprint arXiv:2512.10739, 2025. [21] Haiteng Zhao, Junhao Shen, Yiming Zhang, Songyang Gao, Kuikun Liu, Tianyou Ma, Fan Zheng, Dahua Lin, Wenwei Zhang, and Kai Chen. Achieving olympia-level geometry large language model agent via complexity boosting reinforcement learning. arXiv preprint arXiv:2512.10534, 2025. [22] Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu Yang, Minlie Huang, Nan Duan, and Weizhu Chen. Tora: tool-integrated reasoning agent for mathematical problem solving. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. URL https://openreview.net/forum?id=Ep0TtjVoap. [23] Fan Liu, Zherui Yang, Cancheng Liu, Tianrui Song, Xiaofeng Gao, and Hao Liu. Mm-agent: LLM as agents for real-world mathematical modeling problem. CoRR, abs/2505.14148, 2025. doi: 10.48550/ ARXIV.2505.14148. URL https://doi.org/10.48550/arXiv.2505.14148. [24] Ziju Shen, Naohao Huang, Fanyi Yang, Yutong Wang, Guoxiong Gao, Tianyi Xu, Jiedong Jiang, Wanyi He, Pu Yang, Mengzhou Sun, Haocheng Ju, Peihao Wu, Bryan Dai, and Bin Dong. Real-prover: Retrieval augmented lean prover for mathematical reasoning. CoRR, abs/2505.20613, 2025. doi: 10.48550/ARXIV.2505.20613. URL https://doi.org/10.48550/arXiv.2505.20613. [25] Yue Zhang, Jiaxin Zhang, Qiuyu Ren, Tahsin Saffat, Xiaoxuan Liu, Zitong Yang, Banghua Zhu, and Yi Ma. GAUSS: benchmarking structured mathematical skills for large language models. CoRR, abs/2509.18122, 2025. doi: 10.48550/ARXIV.2509.18122. URL https://doi.org/10.48550/arXiv.2509.18122. [26] Jiayi Kuang, Haojing Huang, Yinghui Li, Xinnian Liang, Zhikun Xu, Yangning Li, Xiaoyu Tan, Chao Qu, Meishan Zhang, Ying Shen, and Philip S. Yu. Atomic thinking of LLMs: Decoupling and exploring mathematical reasoning abilities. In The Thirty-ninth Annual Conference on Neural Information Processing Systems, 2025. URL https://openreview.net/forum?id=iBFfb6bGOz. [27] Jade Copet, Quentin Carbonneaux, Gal Cohen, Jonas Gehring, Jacob Kahn, Jannik Kossen, Felix Kreuk, Emily McMilin, Michel Meyer, Yuxiang Wei, et al. Cwm: An open-weights llm for research on code generation with world models. arXiv preprint arXiv:2510.02387, 2025. [28] Jiayi Pan, Xingyao Wang, Graham Neubig, Navdeep Jaitly, Heng Ji, Alane Suhr, and Yizhe Zhang. Training software engineering agents and verifiers with swe-gym. arXiv preprint arXiv:2412.21139, 2024. [29] John Yang, Kilian Lieret, Carlos Jimenez, Alexander Wettig, Kabir Khandpur, Yanzhe Zhang, Binyuan Hui, Ofir Press, Ludwig Schmidt, and Diyi Yang. Swe-smith: Scaling data for software engineering agents. arXiv preprint arXiv:2504.21798, 2025. 32 Youtu-LLM Technical Report [30] Ibragim Badertdinov, Alexander Golubev, Maksim Nekrashevich, Anton Shevtsov, Simon Karasik, Andrei Andriushchenko, Maria Trofimova, Daria Litvintseva, and Boris Yangel. Swe-rebench: An automated pipeline for task collection and decontaminated evaluation of software engineering agents. arXiv preprint arXiv:2505.20411, 2025. [31] Stphane Ross, Geoffrey Gordon, and Drew Bagnell. reduction of imitation learning and structured prediction to no-regret online learning. In Proceedings of the fourteenth international conference on artificial intelligence and statistics, pages 627635. JMLR Workshop and Conference Proceedings, 2011. [32] Tencent Youtu. Youtu-agent: simple yet powerful agent framework. https://github.com/ TencentCloudADP/youtu-agent, 2025. [33] Jialong Wu, Baixuan Li, Runnan Fang, Wenbiao Yin, Liwen Zhang, Zhengwei Tao, Dingchu Zhang, Zekun Xi, Yong Jiang, Pengjun Xie, Fei Huang, and Jingren Zhou. Webdancer: Towards autonomous information seeking agency. ArXiv, abs/2505.22648, 2025. URL https://api.semanticscholar. org/CorpusID:278959248. [34] Zhangchen Xu, Adriana Meza Soria, Shawn Tan, Anurag Roy, Ashish Sunil Agrawal, Radha Poovendran, and Rameswar Panda. Toucan: Synthesizing 1.5 tool-agentic data from real-world mcp environments. arXiv preprint arXiv:2510.01179, 2025. [35] Zengzhuang Xu, Bingguang Hao, Zechuan Wang, Yuntao Wen, Xinyi Xu, Yang Liu, Long Chen, Dong Wang, Maolin Wang, Tong Zhao, et al. Funreason-mt technical report: Advanced data synthesis solution for real-world multi-turn tool-use. arXiv preprint arXiv:2510.24645, 2025. [36] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with subword units. In Proceedings of the 54th annual meeting of the association for computational linguistics (volume 1: long papers), pages 17151725, 2016. [37] Changhan Wang, Kyunghyun Cho, and Jiatao Gu. Neural machine translation with byte-level subwords. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pages 91549160, 2020. [38] Alisa Liu, Jonathan Hayase, Valentin Hofmann, Sewoong Oh, Noah Smith, and Yejin Choi. SuperBPE: Space travel for language models. In Second Conference on Language Modeling, 2025. URL https: //arxiv.org/abs/2503.13423. [39] Haotong Yang, Yi Hu, Shijia Kang, Zhouchen Lin, and Muhan Zhang. Number cookbook: Number understanding of language models and how to improve it. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?id=BWS5gVjgeY. [40] Mucong Ding, Sean Michael McLeish, Kazem Meidani, Igor Melnyk, Nam Nguyen, Bayan Bruss, and Furong Huang. You only train once: Efficient tokenizer selection for arithmetic in language models. In Tokenization Workshop, 2025. URL https://openreview.net/forum?id=syvdPYbdTI. [41] Qingjie Zhang, Di Wang, Haoting Qian, Liu Yan, Tianwei Zhang, Ke Xu, Qi Li, Minlie Huang, Hewu Li, and Han Qiu. Speculating llms chinese training data pollution from their tokens. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pages 2612426144, 2025. [42] Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, Amy Yang, Angela Fan, Anirudh Goyal, and et al. The llama 3 herd of models, 2024. URL https://arxiv.org/abs/2407.21783. [43] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, and et al. Qwen technical report, 2023. URL https://arxiv.org/abs/2309.16609. [44] Aixin Liu, Bei Feng, Bin Wang, Bingxuan Wang, Bo Liu, Chenggang Zhao, Chengqi Dengr, Chong Ruan, Damai Dai, Daya Guo, Dejian Yang, Deli Chen, Dongjie Ji, and et al. Deepseek-v2: strong, economical, and efficient mixture-of-experts language model, 2024. URL https://arxiv.org/abs/ 2405.04434. 33 Youtu-LLM Technical Report [45] Weilin Cai, Juyong Jiang, Fan Wang, Jing Tang, Sunghun Kim, and Jiayi Huang. survey on mixture of experts in large language models, 2025. URL https://arxiv.org/abs/2407.06204. [46] Rongjie Yi, Liwei Guo, Shiyun Wei, Ao Zhou, Shangguang Wang, and Mengwei Xu. Edgemoe: Fast on-device inference of moe-based large language models. arXiv preprint arXiv:2308.14352, 2023. [47] Yifan Bai, Yiping Bao, Guanduo Chen, Jiahao Chen, Ningxin Chen, Ruijue Chen, Yanru Chen, Yuankun Chen, Yutian Chen, Zhuofu Chen, Jialei Cui, and et al. Kimi k2: Open agentic intelligence, 2025. URL https://arxiv.org/abs/2507.20534. [48] Jerome Bruner. The process of education, harvard, univ. Press, Cambridge, Mass, 1960. [49] David Ausubel. The use of advance organizers in the learning and retention of meaningful verbal material. Journal of educational psychology, 51(5):267, 1960. [50] Pete Walsh, Luca Soldaini, Dirk Groeneveld, Kyle Lo, Shane Arora, Akshita Bhagia, Yuling Gu, Shengyi Huang, Matt Jordan, et al. 2 olmo 2 furious. arXiv preprint arXiv:2501.00656, 2024. [51] Etash Guha, Ryan Marten, Sedrick Keh, Negin Raoof, Georgios Smyrnis, Hritik Bansal, Marianna Nezhurina, Jean Mercat, Trung Vu, Zayne Sprague, Ashima Suvarna, Benjamin Feuer, Liangyu Chen, Zaid Khan, Eric Frankel, Sachin Grover, Caroline Choi, Niklas Muennighoff, Shiye Su, Wanjia Zhao, John Yang, Shreyas Pimpalgaonkar, Kartik Sharma, Charlie Cheng-Jie Ji, Yichuan Deng, Sarah Pratt, Vivek Ramanujan, Jon Saad-Falcon, Jeffrey Li, Achal Dave, Alon Albalak, Kushal Arora, Blake Wulfe, Chinmay Hegde, Greg Durrett, Sewoong Oh, Mohit Bansal, Saadia Gabriel, Aditya Grover, Kai-Wei Chang, Vaishaal Shankar, Aaron Gokaslan, Mike A. Merrill, Tatsunori Hashimoto, Yejin Choi, Jenia Jitsev, Reinhard Heckel, Maheswaran Sathiamoorthy, Alexandros G. Dimakis, and Ludwig Schmidt. Openthoughts: Data recipes for reasoning models, 2025. URL https://arxiv.org/abs/2506. 04178. [52] Ivan Moshkov, Darragh Hanley, Ivan Sorokin, Shubham Toshniwal, Christof Henkel, Benedikt Schifferer, Wei Du, and Igor Gitman. Aimo-2 winning solution: Building state-of-the-art mathematical reasoning models with openmathreasoning dataset. arXiv preprint arXiv:2504.16891, 2025. [53] Wasi Uddin Ahmad, Sean Narenthiran, Somshubra Majumdar, Aleksander Ficek, Siddhartha Jain, Jocelyn Huang, Vahid Noroozi, and Boris Ginsburg. Opencodereasoning: Advancing data distillation for competitive coding. arXiv preprint arXiv:2504.01943, 2025. [54] Aixin Liu, Aoxue Mei, Bangcai Lin, Bing Xue, Bingxuan Wang, Bingzheng Xu, Bochao Wu, Bowei Zhang, Chaofan Lin, Chen Dong, et al. Deepseek-v3. 2: Pushing the frontier of open large language models. arXiv preprint arXiv:2512.02556, 2025. [55] Chen Yang, Guangyue Peng, Jiaying Zhu, Ran Le, Ruixiang Feng, Tao Zhang, Wei Ruan, Xiaoqi Liu, Xiaoxue Cheng, Xiyun Xu, et al. Nanbeige4-3b technical report: Exploring the frontier of small language models. arXiv preprint arXiv:2512.06266, 2025. [56] Jujie He, Jiacai Liu, Chris Yuhao Liu, Rui Yan, Chaojie Wang, Peng Cheng, Xiaoyu Zhang, Fuxiang Zhang, Jiacheng Xu, Wei Shen, et al. Skywork open reasoner 1 technical report. arXiv preprint arXiv:2505.22312, 2025. [57] Junlong Li, Daya Guo, Dejian Yang, Runxin Xu, Yu Wu, and Junxian He. Codei/o: Condensing reasoning patterns via code input-output prediction. arXiv preprint arXiv:2502.07316, 2025. [58] Anisha Gunjal, Anthony Wang, Elaine Lau, Vaskar Nath, Yunzhong He, Bing Liu, and Sean Hendryx. Rubrics as rewards: Reinforcement learning beyond verifiable domains. arXiv preprint arXiv:2507.17746, 2025. [59] Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Weinan Dai, Tiantian Fan, Gaohong Liu, Lingjun Liu, et al. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476, 2025. 34 Youtu-LLM Technical Report [60] Feng Yao, Liyuan Liu, Dinghuai Zhang, Chengyu Dong, Jingbo Shang, and Jianfeng Gao. Your efficient rl framework secretly brings you off-policy rl training, August 2025. URL https://fengyao.notion. site/off-policy-rl. [61] Jiacai Liu, Yingru Li, Yuqian Fu, Jiawei Wang, Qian Liu, and Yu Shen. When speed kills stability: Demystifying RL collapse from the training-inference mismatch, September 2025. URL https:// richardli.xyz/rl-collapse. [62] Penghui Qi, Zichen Liu, Xiangxin Zhou, Tianyu Pang, Chao Du, Wee Sun Lee, and Min Lin. Defeating the training-inference mismatch via fp16. arXiv preprint arXiv:2510.26788, 2025. [63] Patrick Lewis, Barlas Oguz, Ruty Rinott, Sebastian Riedel, and Holger Schwenk. Mlqa: Evaluating cross-lingual extractive question answering. In Proceedings of the 58th annual meeting of the association for computational linguistics, pages 73157330, 2020. [64] Weihao Xuan, Rui Yang, Heli Qi, Qingcheng Zeng, Yunze Xiao, Aosong Feng, Dairui Liu, Yun Xing, Junjue Wang, Fan Gao, et al. Mmlu-prox: multilingual benchmark for advanced large language model evaluation. arXiv preprint arXiv:2503.10497, 2025. [65] Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, et al. Mmlu-pro: more robust and challenging multi-task language understanding benchmark. Advances in Neural Information Processing Systems, 37:9526695290, 2024. [66] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. [67] Freda Shi, Mirac Suzgun, Markus Freitag, Xuezhi Wang, Suraj Srivats, Soroush Vosoughi, Hyung Won Chung, Yi Tay, Sebastian Ruder, Denny Zhou, et al. Language models are multilingual chain-of-thought reasoners. arXiv preprint arXiv:2210.03057, 2022. [68] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021. [69] Mirac Suzgun, Nathan Scales, Nathanael Schrli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc Le, Ed Chi, Denny Zhou, et al. Challenging big-bench tasks and whether chain-of-thought can solve them. In Findings of the Association for Computational Linguistics: ACL 2023, pages 1300313051, 2023. [70] David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel Bowman. Gpqa: graduate-level google-proof q&a benchmark. In First Conference on Language Modeling, 2024. [71] Long Phan, Alice Gatti, Ziwen Han, Nathaniel Li, Josephina Hu, Hugh Zhang, Chen Bo Calvin Zhang, Mohamed Shaaban, John Ling, Sean Shi, et al. Humanitys last exam. arXiv preprint arXiv:2501.14249, 2025. [72] Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021. [73] Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and Lingming Zhang. Is your code generated by chatgpt really correct? rigorous evaluation of large language models for code generation. Advances in Neural Information Processing Systems, 36:2155821572, 2023. [74] Mark Chen. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. 35 Youtu-LLM Technical Report [75] Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando Solar-Lezama, Koushik Sen, and Ion Stoica. Livecodebench: Holistic and contamination free evaluation of large language models for code. arXiv preprint arXiv:2403.07974, 2024. [76] Alex Gu, Baptiste Rozire, Hugh Leather, Armando Solar-Lezama, Gabriel Synnaeve, and Sida I. Wang. Cruxeval: benchmark for code reasoning, understanding and execution. arXiv preprint arXiv:2401.03065, 2024. [77] Tianyang Liu, Canwen Xu, and Julian McAuley. Repobench: Benchmarking repository-level code auto-completion systems, 2024. URL https://arxiv.org/abs/2306.03091. [78] Yushi Bai, Shangqing Tu, Jiajie Zhang, Hao Peng, Xiaozhi Wang, Xin Lv, Shulin Cao, Jiazheng Xu, Lei Hou, Yuxiao Dong, et al. Longbench v2: Towards deeper understanding and reasoning on realistic long-context multitasks. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 36393664, 2025. [79] Cheng-Ping Hsieh, Simeng Sun, Samuel Kriman, Shantanu Acharya, Dima Rekesh, Fei Jia, Yang Zhang, and Boris Ginsburg. Ruler: Whats the real context size of your long-context language models? arXiv preprint arXiv:2404.06654, 2024. [80] Howard Yen, Tianyu Gao, Minmin Hou, Ke Ding, Daniel Fleischer, Peter Izsak, Moshe Wasserblat, and Danqi Chen. Helmet: How to evaluate long-context language models effectively and thoroughly. arXiv preprint arXiv:2410.02694, 2024. [81] Aryo Pradipta Gema, Joshua Ong Jun Leang, Giwon Hong, Alessio Devoto, Alberto Carlo Maria Mancino, Rohit Saxena, Xuanli He, Yu Zhao, Xiaotang Du, Mohammad Reza Ghasemi Madani, et al. Are we done with mmlu? In Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 50695096, 2025. [82] Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi Luan, Denny Zhou, and Le Hou. Instruction-following evaluation for large language models, 2023. URL https://arxiv. org/abs/2311.07911. [83] Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and Matt Gardner. Drop: reading comprehension benchmark requiring discrete reasoning over paragraphs. arXiv preprint arXiv:1903.00161, 2019. [84] Zayne Sprague, Xi Ye, Kaj Bostrom, Swarat Chaudhuri, and Greg Durrett. Musr: Testing the limits of chain-of-thought with multistep soft reasoning. arXiv preprint arXiv:2310.16049, 2023. [85] Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step. In The Twelfth International Conference on Learning Representations, 2023. [86] AIME. Aime problems and solutions. https://artofproblemsolving.com/wiki/index.php/ AIME_Problems_and_Solutions, 2025. [87] Jiarui Qin, Yunjia Xi, Junjie Huang, Renting Rui, Di Yin, Weiwen Liu, Yong Yu, Weinan Zhang, and Xing Sun. Aptbench: Benchmarking agentic potential of base llms during pre-training. arXiv preprint arXiv:2510.24397, 2025. [88] Chen Chen, Xinlong Hao, Weiwen Liu, Xu Huang, Xingshan Zeng, Shuai Yu, Dexun Li, Shuai Wang, Weinan Gan, Yuefeng Huang, et al. Acebench: Who wins the match point in tool usage? arXiv preprint arXiv:2501.12851, 2025. [89] Shishir G. Patil, Huanzhi Mao, Charlie Cheng-Jie Ji, Fanjia Yan, Vishnu Suresh, Ion Stoica, and Joseph E. Gonzalez. The berkeley function calling leaderboard (bfcl): From tool use to agentic evaluation of large language models. In Forty-second International Conference on Machine Learning, 2025. 36 Youtu-LLM Technical Report [90] Grgoire Mialon, Clmentine Fourrier, Thomas Wolf, Yann LeCun, and Thomas Scialom. Gaia: benchmark for general ai assistants. In The Twelfth International Conference on Learning Representations, 2023. [91] Kaiyuan Chen, Yixin Ren, Yang Liu, Xiaobo Hu, Haotong Tian, Tianbao Xie, Fangfu Liu, Haoye Zhang, Hongzhang Liu, Yuan Gong, et al. xbench: Tracking agents productivity scaling with profession-aligned real-world evaluations. arXiv preprint arXiv:2506.13651, 2025. [92] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. In The eleventh international conference on learning representations, 2022. [93] Carlos Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik arXiv preprint Narasimhan. Swe-bench: Can language models resolve real-world github issues? arXiv:2310.06770, 2023. [94] Jiayi Kuang, Yinghui Li, Xin Zhang, Yangning Li, Di Yin, Xing Sun, Ying Shen, and Philip Yu. Processlevel trajectory evaluation for environment configuration in software engineering agents. arXiv preprint arXiv:2510.25694, 2025. [95] Naman Jain, Jaskirat Singh, Manish Shetty, Tianjun Zhang, Liang Zheng, Koushik Sen, and Ion Stoica. R2e-gym: Procedural environment generation and hybrid verifiers for scaling open-weights swe agents. In Second Conference on Language Modeling, 2025. [96] Xingyao Wang, Boxuan Li, Yufan Song, Frank Xu, Xiangru Tang, Mingchen Zhuge, Jiayi Pan, Yueqi Song, Bowen Li, Jaskirat Singh, et al. Openhands: An open platform for ai software developers as generalist agents. arXiv preprint arXiv:2407.16741, 2024. [97] Victor Barres, Honghua Dong, Soham Ray, Xujie Si, and Karthik Narasimhan. 2-bench: Evaluating conversational agents in dual-control environment, 2025. URL https://arxiv.org/abs/2506. 07982. Youtu-LLM Technical Report"
        },
        {
            "title": "A Contributions and Acknowledgments",
            "content": "We would like to express our sincere gratitude to all contributors, including those not listed in the paper, for their invaluable support and efforts. The contributors within each group are listed in no particular order. Core Contributors Junru Lu Jiarui Qin Lingfeng Qiao Yinghui Li Xinyi Dai Bo Ke Jianfeng He Ruizhi Qiao Di Yin Xing Sun Yunsheng Wu Contributors Yinsong Liu Shuangyin Liu Mingkong Tang Haodong Lin Jiayi Kuang Fanxu Meng Xiaojuan Tang Yunjia Xi Junjie Huang Haotong Yang Zhenyi Shen Yangning Li Qianwen Zhang Yifei Yu Siyu An Junnan Dong Qiufeng Wang Jie Wang Keyu Chen Wei Wen Taian Guo Zhifeng Shen Daohai Yu Jiahao Li Ke Li Zongyi Li Xiaoyu Tan Youtu-LLM Team The Impact of Agentic-CoT Trajectory Data Figure 16. Ablation experimental results of agentic-CoT trajectory data on APT-Math. Ablation Study of Agentic-CoT Trajectory Data To verify the specific contribution of our proposed Agentic-CoT trajectory data, we conducted an ablation study comparing the performance of the Baseline model, which is trained without any agentic trajectory data, against the model with mixed training on Agentic-CoT trajectory data. We evaluated the models across three key dimensions of APT-Math: Planning, Feedback, and Action, alongside the overall average performance (AVG). For reference, the performance of the base models (Qwen3-1.7B-Base and Qwen3-4B-Base) is also included to contextualize the improvements. As illustrated in the Figure 16, the integration of Agentic-CoT Trajectory Data yields significant performance Corresponding author: winfredsun@tencent.com 38 Youtu-LLM Technical Report improvement. The most notable enhancement is observed in the Planning capability, where the score surged from 65.1% (Baseline) to 82.9%, surpassing even the larger base models. While there is slight fluctuation in the Feedback metric, the Action metric showed steady increase from 66.1% to 67.8%. Consequently, the overall average score increased from 65.3% to 69.0%, resulting in net performance gain of 3.7%. The substantial uplift in the Planning dimension indicates that the agentic-CoT Trajectory Data effectively enhances the models ability to decompose complex tasks and formulate logical execution steps. By incorporating reflection trajectories, the model learns to \"think before acting,\" which significantly improves the quality of its initial plans. This strong planning capability acts as the primary driver for the overall performance, leading to the observed 3.7% increase in the average score. This confirms that equipping the model with reflective reasoning paths is crucial for solving complex tasks that require long-horizon planning."
        },
        {
            "title": "C The Impact of Math Trajectory Data",
            "content": "In the ablation studies of the math trajectory, the starting baseline is further trained using 40B tokens of 32k-length general-purpose data (including portion of general trajectory data) and 3.3B tokens of 32k-length mathematical trajectory data. All training settings are kept identical to those used in the full trajectory pre-training, ensuring fair comparison. We design three categories of ablation experiments to analyze: (1) the effect of different training strategies, (2) the influence of trajectory data scaling. C.1 Different Masking Strategies Analysis To avoid the impact of numerous repetitive system prompts and instructions in the trajectory, we mask the trajectory during training. We first examine how different masking strategies applied to trajectory training affect model performance. Beyond the baseline model, we explore several training configurations, including: (i) training on general data without masking, (ii) mixed training on general-purpose data and mathematical trajectory data without masking, and (iii) mixed training with masking applied to both general-purpose and mathematical trajectory data. In the masking-based setups, we conduct universal masking strategy to maintain consistency as much as possible with trajectory data from other domains. All fields except those with the \"role\":\"assistant\" are masked. We further investigate whether fully masking the user query introduces additional effects. Several important observations can be drawn from these results, as shown in Figure 17. First, incorporating mathematical trajectory data consistently improves overall performance compared to the baseline, confirming the effectiveness of trajectory-level supervision for enhancing agentic mathematical reasoning. Second, applying masking strategies yields substantially larger gains than unmasked training, particularly on Planning and Feedback tasks. This suggests that masking non-assistant fields encourages the model to internalize reasoning patterns and solution trajectories. In contrast, the gains on Action tasks are more moderate, implying that computational execution benefits less from trajectory abstraction than high-level reasoning and verification. Finally, we observe that whether the user query is fully masked or not has only marginal impact on average performance. This suggests that the primary benefit of masking arises from different agentic actions, while preserving or removing the query itself plays secondary role. Overall, these findings highlight masking-aware trajectory pre-training as key design choice for effectively injecting agentic reasoning capabilities into lightweight language models. 39 Youtu-LLM Technical Report Figure 17. Ablation experimental results for different mask training strategies of math trajectories. Figure 18. Ablation experiments results of mathematical trajectory training at different up-sampling rates. C.2 Math Trajectory Scaling Analysis After exploring the optimal masking strategy, we further investigate the scaling behavior of mathematical trajectory data. Starting from the 3.3B-token mathematical trajectory dataset used in the ablation stage, we progressively upsample the data by factors of 1, 2, 3, 4, and 5, resulting in maximum of 16.5B tokens of mathematical trajectories. These trajectories are mixed with fixed 40B-token general-purpose dataset and trained under the same configuration as in previous experiments. From the results in Figure 18, several notable observations emerge from this scaling study. First, increasing the amount of mathematical trajectory data leads to consistent and substantial performance gains over the baseline models, validating the effectiveness of trajectory-level supervision for strengthening agentic mathematical reasoning. Even single upsampling (1) yields clear improvement, particularly in Planning, suggesting that mathematical trajectories rapidly inject high-level structural reasoning patterns into the model. Performance continues to improve as the trajectory scale increases from 1 to 3, which indicates scaling lawlike behavior in which larger volumes of trajectory data progressively enhance the models Youtu-LLM Technical Report ability to plan solution strategies and verify multi-step reasoning processes. In addition, the results reveal clear performance saturation point. While the 5 configuration slightly improves planning accuracy, it leads to degradation in feedback and action performance. This suggests that excessive upsampling of domainspecific trajectory data may introduce distributional imbalance, potentially overwhelming general-purpose knowledge and harming cross-capability robustness."
        },
        {
            "title": "D The Impact of Deep Research Trajectory Data",
            "content": "D.1 Different Masking Strategies Analysis In deep research scenarios, trajectory data often contains significant noise and redundancy, particularly in the form of repetitive system prompts and verbose search engine responses. Blindly learning these segments can negatively impact the models instruction-following and reasoning capabilities by introducing distribution shifts or encouraging the generation of non-informative tokens. Therefore, defining an optimal masking strategy is crucial before formal training. We investigate three distinct masking configurations to determine the most effective approach: Baseline + DR data without mask: Training on Deep Research (DR) data with full-text loss calculation. Baseline + DR data with mask (tool response): Masking the search engine/tool outputs, which are not calculated into the generation loss. Baseline + DR data with mask (tool response/system/query): comprehensive masking strategy that excludes tool responses, system prompts, and user queries, focusing the loss exclusively on the models internal reasoning and action generation. Figure 19. Ablation experimental results for different mask training strategies of deep research. The performance of these strategies was evaluated across three key dimensions: Planning, Action, and Atomic capabilities, along with an average score on APT-DR. The results are illustrated in the Figure 19. Training without masking yields mixed results. While it improves the Action score (36.8%) compared to 41 Youtu-LLM Technical Report the Baseline (30.9%), it causes significant regression in Planning capabilities, dropping from the Baselines 35.7% to 33.7%. This suggests that forcing the model to predict noisy search results and repetitive prompts interferes with its high-level planning logic. By masking the tool responses, we observe an immediate improvement. This strategy outperforms the unmasked approach in both Planning (35.2%) and Action (38.6%). Notably, the Action score sees substantial leap, surpassing the Qwen3-1.7B-Base model (36.3%), indicating that the model learns to utilize tools better when it is not penalized for failing to predict the tools output perfectly. The most aggressive masking strategy (masking tool response, system, and query) achieves the best overall performance. It yields the highest AVG score of 29.6%, significantly outperforming the Baseline (26.2%) and the unmasked variant (27.5%). Crucially, this is the only strategy that achieves substantial gain in the Atomic metric (15.8%) compared to the Baseline (14.3%) and the Base model (14.8%). The experimental data clearly demonstrate that selective loss calculation is essential for leveraging Deep Research trajectory data effectively. Noise Reduction. The drop in Planning performance for the unmasked model confirms that verbose search results act as noise. When the model attempts to learn these unpredictable or redundant sequences, its ability to structure complex reasoning paths degrades. Focus on Reasoning. The comprehensive masking strategy forces the model to focus solely on the \"Thought\" and \"Action\" componentsthe actual reasoning trace. By masking the system prompts and user queries, we prevent the model from overfitting to specific prompt templates, while masking tool responses prevents the hallucination of search results. Consequently, the strategy of masking tool responses, system prompts, and user queries is adopted as the standard for subsequent training, as it maximizes the extraction of reasoning capabilities from the trajectory data while minimizing the interference of environmental noise. D.2 Ablation Study of Deep Research Trajectory Data To evaluate the contribution of our proposed deep research trajectory data to model performance, we conducted an ablation study using the APT-DR evaluation set. We compare Qwen3-1.7B-Base, Qwen3-4BBase, and several Youtu-LLM models with different data setups. We introduced the data in two stages: first by incorporating the Deep Research (DR) Trajectory Data alone, and subsequently by combining it with other Trajectory Data to observe potential synergistic effects. As illustrated in the Figure 20, the inclusion of Deep Research Trajectory Data yields clear performance gain. Impact of DR Data. Adding the DR trajectory data to the Baseline results in tangible improvement in the overall score, raising the AVG from 27.5% to 30.9%. Notably, the Action metric sees significant boost, increasing from 33.9% to 41.5%, surpassing even the Qwen3-4B-Base in this specific dimension. Synergy with Other Trajectory Data. The further addition of other trajectory data triggers substantial leap in performance. The Planning metric exhibits dramatic increase, surging to 65.3%, which is nearly identical to the 65.7% achieved by the significantly larger Qwen3-4B-Base. Consequently, the final AVG score reaches 38.6%, narrowing the performance gap with the 4B model to less than 2 percentage points. The experimental results indicate two critical findings. First, the Deep Research Trajectory Data is highly effective in enhancing the models execution capabilities, as evidenced by the sharp rise in the Action metric. Second, there is strong mutual promotion (synergy) between the DR data and other trajectory datasets. While DR data solidifies the models action execution, the supplementary trajectory data appears to unlock the models reasoning and planning potential. This combination allows our 1.7B parameter model to achieve 42 Youtu-LLM Technical Report Figure 20. Ablation experimental results of deep research trajectory data on APT-DR. comprehensive performance level (AVG=38.6%) that closely approximates that of the Qwen3-4B-Base (AVG=40.5%), demonstrating the high efficiency and quality of our data strategy."
        },
        {
            "title": "E The Impact of Code Trajectory Data",
            "content": "Figure 21. Ablation experimental results for different mask training strategies code trajectory data (EnvSetup). E.1 Different Masking Strategies Analysis We first conduct preliminary study using small subset of code trajectory data based on the baseline model to investigate the effects of masking non-assistant turns. As shown in Figure 21 and Figure 22, the 43 Youtu-LLM Technical Report Figure 22. Ablation experimental results for different mask training strategies of code trajectory data (IssueFix). performance gap is marginal on both EnvSetup and Issuefix subset. Learning from supervised signals from non-assistant turns facilitates the acquisition of code-centric world knowledge, thereby improving the models ability to interpret environment feedback. However, this approach may introduce distributional inconsistencies or incorporate stochastic noise and redundant patterns, which could potentially undermine training stability. To maintain methodological alignment with our other trajectory experiments, we opted to mask non-assistant content in our final experimental setup. Nevertheless, developing more nuanced approach to learning from non-assistant segments that leverages environmental insights without compromising data quality is still promising avenue for future exploration. Figure 23. Ablation experimental results of code trajectory data on APT-Code-EnvSetup. E.2 Ablation Study of Code Trajectory Data To further investigate the impact of code trajectory data on agentic code capabilities, we conduct an ablation study based on the 128k baseline. Figure 23 and Figure 24 illustrates the results of the EnvSetup subset and IssueFix subset, respectively. Youtu-LLM Technical Report Figure 24. Ablation experimental results of code trajectory data on APT-Code-IssueFix. In the EnvSetup subset, the Qwen3-4B-Base model exhibits clear superiority, particularly in Planning (71.8%) and ErrorHandle (61.9%). This suggests that the larger parameter count of the 4B model provides stronger foundational capability for logical sequencing and recovering from setup exceptions. significant observation can be made regarding the Youtu-LLM 2B. The \"Baseline\" starts at modest 20.4% AVG. However, the integration of code trajectories (+ code traj) boosts the average to 39.3%, and further adding other trajectories (+ other traj) raises it to 42.0%. While Youtu-LLM lags behind Qwen3-4B in Planning, it shows competitive performance in Action and ErrorHandle after data enhancement, indicating that domainspecific trajectories significantly narrow the gap between 2B model and larger 4B model. Unlike the EnvSetup subset, the IssueFix results show much tighter competition. Notably, the Youtu-LLM 2B (with trajectories) actually outperform Qwen3-4B-Base in the AVG metric (35.5% vs. 33.9%). The most striking disparity is found in the Tool category. The Youtu-LLM variants (with code trajectories) achieve an impressive 58.1%, significantly outstripping both Qwen3-1.7B (17.8%) and Qwen3-4B (47.7%). This suggests that the trajectory-based training specifically enhances the models ability to interface with external debugging and diagnostic tools. Across Locate, Fix_Patch, and Test_Patch, the performance remains relatively stable across all models, hovering between 24% and 29%, indicating that while \"fixing\" the code is shared challenge for small LLMs, \"tooling\" and \"planning\" are the primary differentiators for overall success. While Qwen3-4B benefits from its larger scale to lead in general planning and complex environment reasoning, the Youtu-LLM 2B demonstrates that specialized trajectory data can allow smaller model to achieve SOTA performance in technical workflows like issue fixing. The incremental gains in the Youtu-LLM series confirm that the quality and variety of training trajectories (code vs. others) are critical for \"Action\" and \"Tool\" oriented tasks, which are more execution-heavy than purely generative. Qwen3-4B remains the most robust \"all-rounder\" for environment setup, but for developers focusing on automated bug fixing and tool integration, the enhanced Youtu-LLM 2B offers more efficient and higher-performing alternative. The Impact of Tool-use Trajectory Data 45 Youtu-LLM Technical Report F.1 Different Masking Strategies Analysis In Figures 25 and 26, we report preliminary experiments on tool-use trajectory data, following the experimental setup adopted in the ablation studies of other types of trajectory data. We first examine the impact of masking strategy, comparing the effects of agentic mid-training with and without masks (on all non-assistant input) at 100% sampling. The results in Figure 25 shows that the difference between the two is marginal, with the version using masks showing slight improvement of 0.4% in the average APTBench score. Figure 25. Ablation experimental results of tool-use trajectory data, concerning masking and up-sampling strategy. The metrics reported are the sub-scores of four evaluation dimensions in APTBench, as well as the overall average score. F.2 Ablation Study of Tool-use Trajectory Data Regarding the sampling rate, we experimented with sampling rates 100% (1), 200% (2), and 300% (3). Under the same training base and unified experimental settings, Figure 25 shows that the 1 sampling rate is more suitable for tool-use and planning trajectory data. When the sampling rate increased to 2, the correlated metrics for deep research improved, but significantly decreased at 3. In terms of mathematics, using more tool-related data consistently improved the metrics. However, for the evaluation of code and tool, increasing the sampling rate continuously impaired the models agentic capabilities. Therefore, we ultimately chose the 1 sampling rate for tool-use and planning trajectory data. How can we verify the effectiveness of agentic mid-training? We designed set of small-scale experiments. As shown in Figure 26, we performed mini SFT using 5B data based on Qwen3-1.7B-Base, the original pre-stage3 Youtu-LLM baseline, and the pre-stage3 Youtu-LLM baseline after agentic mid-training. This SFT data included small amount of format instruction following data that is necessary for GAIA testing, as well as the vast majority of general thinking or non-thinking SFT examples. We conducted tests on BFCL V3 [89], SWE-Bench-Verified [93], and GAIA [90], and the results showed that the SFT performance of the base model after agentic mid-training was significantly better than other comparison models (e.g, over 6% gains in SWE-Bench-Verified, and 4% improvement in GAIA)."
        },
        {
            "title": "G Case Study",
            "content": "46 Youtu-LLM Technical Report Figure 26. Ablation experimental results of mini SFT over less-agentic and agentic base models. G.1 Case of Math"
        },
        {
            "title": "Analysis",
            "content": "This case highlights our models agentic mathematical reasoning ability beyond end-to-end problem solving. Instead of performing naive forward derivation, the model first identifies the core mathematical ambiguity (the definition of number ring) and actively switches to counterexampledriven strategy. By constructing minimal counterexamples such as Z[ 3), the model demonstrates strong capabilities in planning atomic abilities, self-verification, and robust logical action, avoiding common reasoning traps caused by imprecise terminology. 3] and Q( Question: Please judge whether the following statement is true or false: The smallest number ring containing 3 is the real number ring. Step Analysis: Agentic Problem Understanding Before attempting any algebraic manipulation, we performs global semantic analysis of the problem. It identifies two key properties that fundamentally shape the solution strategy: The phrase smallest number ring implies minimality constraint, where direct forward derivation toward is unlikely to be optimal. The correctness of the statement depends on the existence of strictly smaller algebraic structures containing 3. Rather than proceeding with routine symbolic computation, we concludes that: counterexample-oriented reasoning trajectory is the most effective strategy. Step Planning: Strategy Selection 47 Youtu-LLM Technical Report Based on the analysis, we formulates high-level plan: Avoid proving that contains Actively search for minimal algebraic structure that already contains 3. 3, as this is trivial and non-informative. Use the existence of such structure as direct falsification of the statement. Planning Decision: Adopt counterexample-first strategy by constructing smaller candidate rings or fields. Step Feedback: Definition Verification Before execution, we performs an internal consistency check on the problem definition: The term number ring is ambiguous and may refer to either ring or field. The final judgment must therefore be valid under all reasonable interpretations. This feedback step prevents overfitting to single definition and ensures robustness of the reasoning trajectory. Step Action: Counterexample Construction (Ring) Under the ring interpretation, we constructs the minimal subring of generated by Z[ 3] = {a + 3 a, Z}. 3: This structure: Contains 3, Is closed under addition and multiplication, Is proper subset of R. Thus, cannot be the smallest number ring containing 3. Step Feedback: Cross-Interpretation Validation To ensure robustness, we further verifies the statement under the field interpretation: The smallest field containing This field is also strictly smaller than R. 3 is Q( 3). The counterexample remains valid across interpretations, confirming the correctness of the falsification. Step Action: Final Judgment By explicitly constructing minimal algebraic counterexamples and validating them under multiple definitions, we concludes: The statement is false. This trajectory demonstrates that our model conduct full agentic loop involving analysis, planning, feedback, and action, rather than mere step-by-step computation. 48 Youtu-LLM Technical Report G.2 Case of Deep Research"
        },
        {
            "title": "Analysis",
            "content": "The agent locate and parse the official 2011 Bulgarian census PDF to extract precise data points, rather than relying on potentially inaccurate search snippets. Most notably, the agent exhibited adaptive reasoning and error recovery. When tool call failed due to syntax error during the calculation phase, the agent did not loop or hallucinate; instead, it immediately switched strategies to perform the arithmetic internally. This seamless transition from tool-use to internal reasoning ensured the final answer was both accurate and delivered efficiently. Question: Take the gender split from the 2011 Bulgarian census about those who have completed tertiary education. Subtract the smaller number from the larger number, then return the difference in thousands of women. So if there were 30.1 thousand more men, youd give \"30.1\". Step 0 Think: ...It seems like mathematical calculation based on demographic data from the 2011 Bulgarian census about those who have completed tertiary education. However, the question doesnt provide specific numbers or data that can directly calculate from. Let me search for information about the 2011 Bulgarian census and its data on tertiary education completion to understand what numbers might need. Action: search({\"query\": [\"2011 Bulgarian census tertiary education gender split men women\"]}) Search Results: [1] 2011 POPULATION CENSUS Main Results: Women have completed tertiary education significantly more often than the men and at the census moment 791.8 thousand women (22.3%) and ... Source: [URL 1] [2] 2011 BULGARIA NATIONAL REPORT: Population structure by gender and age. As of 1 February 2011 the female population was 3 777 999 (51.3%), while males amounted to 3 586 571 persons (48.7 ... Source: [URL 2] [3] Census 2021: Every fourth person in Bulgaria is ...: The statistics institute said that the gender gap in tertiary education had increased compared with the census 10 years earlier ... Source: [URL 3] ... Step 1 Think: Let me visit the official Bulgarian National Statistical Institute website to get the specific data on tertiary education completion by gender from the 2011 census. Action: visit({\"url\": [\"[URL 1]\"], \"goal\": \"Find specific data on tertiary education completion by gender from the 2011 Bulgarian census, particularly the numbers for men and women who have completed tertiary education\"}) 49 Youtu-LLM Technical Report Visit Results: Webpage Summary: According to the 2011 Bulgarian census, the gender data regarding tertiary education completion is: The total number of people with tertiary education degree is 1,348,700 (19.6% of the total population). Of these, 791,800 are women (22.3% of the female population) and 556,900 are men (16.7% of the male population). Relevant Webpage Link: [URL 1] Step 2 Great! found the specific numbers from the 2011 Bulgarian census: Women with tertiary education: 791,800; Men with tertiary education: 556,900. Now need to calculate the difference: Subtract the smaller number (556,900) from the larger number (791,800); Then return the difference in thousands of women. Let me calculate this: Action: search({\"query\": \"791800 - 556900 difference in thousands women\"}) Search Results: An error occurred while running the tool. Please try again. Error: Invalid JSON input for tool search: 1 validation error for search_args. Query input should be valid list [type=list _type, input_value=791800 - 556900 difference in thousands women, input _type=str]. For further information visit [URL 4]. Step 3 Think: Let me try the calculation manually: - Women: 791,800 - Men: 556,900 - Difference: 791,800 - 556,900 = 234,900 - In thousands: 234,900 1,000 = 234.9 So the difference in thousands of women is 234.9. Let me verify this calculation and provide the final answer. Answer: According to the 2011 Bulgarian census data on tertiary education completion: - Number of women with tertiary education: 791,800 - Number of men with tertiary education: 556,900 The difference is calculated as follows: - Larger number (women): 791,800 - Smaller number (men): 556,900 - Difference: 791,800 - 556,900 = 234,900 - In thousands: 234,900 1,000 = 234.9. Therefore, there were 234.9 thousand more women who have completed tertiary education than men in Bulgaria according to the 2011 census. Youtu-LLM Technical Report G.3 Case of Agentic Coding"
        },
        {
            "title": "Analysis",
            "content": "capabilities the models instance To demonstrate sphinx-doc__sphinx-7889 from SWE-bench Verified. This instance is derived from real-world issue within the Sphinx open-source Python repository. This trajectory illustrates high-level Python debugging. The model identifies rare edge case where string concatenation assumes the input is always string, whereas it can be TypeVar object in generic classes. It shows the models ability to navigate large documentation frameworks and fix issues at the intersection of static analysis and runtime metadata. coding, we in agentic select Question: Autodoc extensions mock file throws TypeError for generic-typed classes. **Describe the bug** When building the docs for generically-typed class, TypeError is thrown as Autodocs mock._make_subclass attempts to concatenate str to TypeVar. See the attached log: [sphinx-err325ndteh.log](https://github.com/sphinx-doc/sphinx/files/4842672/sphinx-err-325ndteh.log) **To Reproduce** $ git https://github.com/perrygoy/screenpy.git $ cd screenpy/docs $ python -m venv env $ source env/bin/activate $ pip install sphinx pyhamcrest selenium typing_extensions $ make html Observe the docs command fails with TypeError. **Expected behavior** Docs can still be built when generics are involved. **Your project** https://github.com/perrygoy/screenpy.git **Environment info** - OS: Mac 10.15.5 (19F101) - Python version: 3.7.7 - Sphinx version: 3.1.1 - Sphinx extensions: sphinx.ext.autodoc, sphinx.ext.intersphinx, sphinx.ext.coverage, sphinx.ext.ifconfig, sphinx.ext.napoleon **Additional context** This might just be me not knowing how to make Sphinx-friendly generic typing, if thats the case please let me know! Step 0 Ill help you implement the necessary changes to fix the issue with the autodoc extensions mock file throwing TypeError for generic-typed classes. Lets follow the steps systematically. ## Step 1: Explore the codebase to locate and understand the code relevant to the issue First, lets explore the repository structure to understand what were working with: Action: <function=execute_bash> <parameter=command>find /testbed -type -name \"*.py\" grep -E \"(autodocmock)\" head -20</parameter> </function> 51 Youtu-LLM Technical Report Observation: Exit code: 0 Execution output of [execute_bash]: [STDOUT] /testbed/sphinx/ext/autodoc/mock.py /testbed/sphinx/ext/autodoc/type_comment.py /testbed/sphinx/ext/autodoc/typehints.py /testbed/sphinx/ext/autodoc/importer.py /testbed/sphinx/ext/autodoc/__init__.py /testbed/sphinx/ext/autodoc/directive.py /testbed/tests/test_ext_autodoc_autofunction.py /testbed/tests/test_ext_autodoc_configs.py /testbed/tests/test_ext_autodoc_private_members.py /testbed/tests/test_ext_autodoc_mock.py /testbed/tests/test_ext_autodoc_events.py /testbed/tests/test_ext_autodoc.py /testbed/tests/roots/test-warnings/autodoc_fodder.py /testbed/tests/roots/test-root/autodoc_target.py /testbed/tests/roots/test-ext-autosummary-mock_imports/conf.py /testbed/tests/roots/test-ext-autosummary-mock_imports/foo.py /testbed/tests/roots/test-ext-autodoc/bug2437/autodoc_dummy_foo.py /testbed/tests/roots/test-ext-autodoc/bug2437/__init__.py /testbed/tests/roots/test-ext-autodoc/autodoc_dummy_bar.py /testbed/tests/roots/test-ext-autodoc/conf.py [STDERR] Step 1 Lets first examine the mock.py file mentioned in the issue, as it seems to be the source of the problem: Action: <function=str_replace_editor> <parameter=command>view</parameter> <parameter=path>/testbed/sphinx/ext/autodoc/mock.py</parameter> </function> 52 Youtu-LLM Technical Report Observation: Execution output of [str_replace_editor]: Heres the result of running cat -n on the file: /testbed/sphinx/ext/autodoc/mock.py: 1 \"\"\" 2 sphinx.ext.autodoc.mock 3 4 5 mock for autodoc 6 7 :copyright: Copyright 2007-2020 by the Sphinx team, see AUTHORS. 8 :license: BSD, see LICENSE for details. 9 \"\"\" 10 11 import contextlib 12 import os 13 import sys 14 from importlib.abc import Loader, MetaPathFinder 15 from importlib.machinery import ModuleSpec 16 from types import FunctionType, MethodType, ModuleType 17 from typing import Any, Generator, Iterator, List, Sequence, Tuple, Union 18 19 from sphinx.util import ... [Omitted for brevity] ... invalidate_caches(self) -> None: 130 \"\"\"Invalidate mocked modules on sys.modules.\"\"\" 131 for modname in self.mocked_modules: 132 sys.modules.pop(modname, None) 133 134 135 @contextlib.contextmanager 136 def mock(modnames: List[str]) -> Generator[None, None, None]: 137 \"\"\"Insert mock modules during context:: 138 139 with mock([target.module.name]): 140 # mock modules are enabled here 141 ... 142 \"\"\" 143 try: 144 finder = MockFinder(modnames) 145 sys.meta_path.insert(0, finder) 146 yield 147 finally: 148 sys.meta_path.remove(finder) 149 finder.invalidate_caches() [Agent performed 1 intermediate operations] Step 3 Based on my analysis of the code, can see that the issue occurs in the _make_subclass function in /testbed/sphinx/ext/autodoc/mock.py. The problem is that when dealing with generic-typed classes, the function tries to concatenate string to TypeVar, which causes TypeError. Lets first create reproduction script to confirm the issue: 53 Youtu-LLM Technical Report Action: <function=str_replace_editor> <parameter=command>create</parameter> <parameter=path>/testbed/reproduce_issue.py</parameter> <parameter=file_text>#!/usr/bin/env python3 \"\"\" Script to reproduce the TypeError when building docs for generic-typed classes. \"\"\" import sys from typing import TypeVar, Generic, List from sphinx.ext.autodoc.mock import _make_subclass def test_generic_class(): \"\"\"Test creating mock subclass for generic class.\"\"\" # Define generic class with TypeVar = TypeVar(T) class MyGeneric(Generic[T]): def __init__(self, value: T) -> None: self.value = value def get_value(self) -> T: return self.value try: # This should trigger the TypeError mock_class = _make_subclass(MyGenericMock, test_module, MyGeneric) print(\"SUCCESS: No TypeError occurred\") return True except TypeError as e: print(f\"ERROR: TypeError occurred: {e}\") return False if __name__ == \"__main__\": success = test_generic_class() sys.exit(0 if success else 1)</parameter> </function> Observation: Execution output of [str_replace_editor]: File created at /testbed/reproduce_issue.py. Heres the result of running cat -n on / testbed/reproduce_issue.py: 1 #!/usr/bin/env python3 2 \"\"\" 3 Script to reproduce the TypeError when building docs for generic-typed classes. 4 \"\"\" 5 6 import sys 7 from typing import TypeVar, Generic, List 8 from sphinx.ext.autodoc.mock import _make_subclass ... [Omitted for brevity] ... 31 if __name__ == \"__main__\": 32 success = test_generic_class() 33 sys.exit(0 if success else 1) Review the file and make sure that it is as expected. Edit the file if necessary. [Agent performed 7 intermediate operations] 54 Youtu-LLM Technical Report Step 11 Great! Now weve successfully reproduced the issue. The error occurs when we try to concatenate string to TypeVar. The second test case (test_typevar_in_name) now triggers the error as expected. ## Root Cause Analysis The issue is in the _make_subclass function in /testbed/sphinx/ext/autodoc/mock.py at line 73: python attrs = {__module__: module, __display_name__: module + . + name} When name is TypeVar (which is type, not string), the expression module + . + name fails because you cant concatenate string to TypeVar. ## Potential Solutions **Check if name is string before concatenation**: Add type check to ensure name is 1. string before concatenating. **Convert name to string if its not**: If name is not string, convert it to string representa2. tion. 3. **Use safer concatenation method**: Use str(name) to ensure we always have string. Lets evaluate these options: **Check if name is string before concatenation**: This is the most direct approach and would 1. be safe. 2. **Convert name to string if its not**: This would work but might be less efficient. 3. **Use safer concatenation method**: This is essentially the same as option 2. The most elegant and safe solution is to check if name is string before concatenation, or better yet, use str(name) to ensure we always have string. ## Implementation Lets implement the fix by modifying the _make_subclass function: 55 Youtu-LLM Technical Report Action: <function=str_replace_editor> <parameter=command>str_replace</parameter> <parameter=path>/testbed/sphinx/ext/autodoc/mock.py</parameter> <parameter=old_str>def _make_subclass(name: str, module: str, superclass: Any = _MockObject, attributes: Any = None) -> Any: attrs = {__module__: module, __display_name__: module + . + name} attrs.update(attributes or {}) return type(name, (superclass,), attrs)</parameter> <parameter=new_str>def _make_subclass(name: str, module: str, superclass: Any = _MockObject, attributes: Any = None) -> Any: # Ensure name is string for concatenation display_name = module + . + str(name) if not isinstance(name, str) else module + . + name attrs = {__module__: module, __display_name__: display_name} attrs.update(attributes or {}) return type(name, (superclass,), attrs)</parameter> </function> Observation: Execution output of [str_replace_editor]: The file /testbed/sphinx/ext/autodoc/mock.py has been edited. Heres the result of running cat -n on snippet of /testbed/sphinx/ext/autodoc/mock.py: 67 def __repr__(self) -> str: 68 return self.__display_name__ 69 70 71 def _make_subclass(name: str, module: str, superclass: Any = _MockObject, 72 attributes: Any = None) -> Any: 73 # Ensure name is string for concatenation 74 display_name = module + . + str(name) if not isinstance(name, str) else module + . + name 75 attrs = {__module__: module, __display_name__: display_name} 76 attrs.update(attributes or {}) 77 78 return type(name, (superclass,), attrs) 79 80 81 class _MockModule(ModuleType): 82 \"\"\"Used by autodoc_mock_imports.\"\"\" Review the changes and make sure they are as expected. Edit the file again if necessary. [Agent performed 15 intermediate operations] Step 27 56 Youtu-LLM Technical Report ## Summary have successfully implemented fix for the TypeError that occurred when building docs for generically-typed classes using the Autodoc extensions mock file. ### Root Cause The issue was in the _make_subclass function in /testbed/sphinx/ext/autodoc/mock.py. When creating mock subclasses for generic-typed classes, the function would attempt to concatenate string to TypeVar (which is type, not string), causing TypeError. ### Solution modified the _make_subclass function to safely handle non-string values for the name parameter by converting them to strings before using them in the type() constructor: python def _make_subclass(name: str, module: str, superclass: Any = _MockObject, attributes: Any = None) -> Any: # Ensure name is string for concatenation and type() constructor name_str = str(name) if not isinstance(name, str) else name display_name = module + . + name_str attrs = {__module__: module, __display_name__: display_name} attrs.update(attributes or {}) return type(name_str, (superclass,), attrs) ### Changes Made 1. Modified /testbed/sphinx/ext/autodoc/mock.py to safely convert non-string name values to strings before using them 2. Verified the fix works with the original reproduction case 3. Tested various edge cases including generic classes, TypeVars, List, Dict, Optional, and nested generics 4. Confirmed all existing tests in test_ext_autodoc_mock.py still pass ### Verification - The original reproduction script now works without errors - All edge case tests pass - Existing unit tests for the mock extension continue to pass - The fix is minimal and targeted, addressing only the specific issue without breaking existing functionality The fix ensures that Autodoc can build documentation for generically-typed classes without throwing TypeError, resolving the issue described in the GitHub issue. Action: <function=submit> </function> Observation: <<< Finished >>>"
        }
    ],
    "affiliations": [
        "Tencent",
        "Youtu-LLM Team"
    ]
}