{
    "paper_title": "Same Claim, Different Judgment: Benchmarking Scenario-Induced Bias in Multilingual Financial Misinformation Detection",
    "authors": [
        "Zhiwei Liu",
        "Yupen Cao",
        "Yuechen Jiang",
        "Mohsinul Kabir",
        "Polydoros Giannouris",
        "Chen Xu",
        "Ziyang Xu",
        "Tianlei Zhu",
        "Tariquzzaman Faisal",
        "Triantafillos Papadopoulos",
        "Yan Wang",
        "Lingfei Qian",
        "Xueqing Peng",
        "Zhuohan Xie",
        "Ye Yuan",
        "Saeed Almheiri",
        "Abdulrazzaq Alnajjar",
        "Mingbin Chen",
        "Harry Stuart",
        "Paul Thompson",
        "Prayag Tiwari",
        "Alejandro Lopez-Lira",
        "Xue Liu",
        "Jimin Huang",
        "Sophia Ananiadou"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large language models (LLMs) have been widely applied across various domains of finance. Since their training data are largely derived from human-authored corpora, LLMs may inherit a range of human biases. Behavioral biases can lead to instability and uncertainty in decision-making, particularly when processing financial information. However, existing research on LLM bias has mainly focused on direct questioning or simplified, general-purpose settings, with limited consideration of the complex real-world financial environments and high-risk, context-sensitive, multilingual financial misinformation detection tasks (\\mfmd). In this work, we propose \\mfmdscen, a comprehensive benchmark for evaluating behavioral biases of LLMs in \\mfmd across diverse economic scenarios. In collaboration with financial experts, we construct three types of complex financial scenarios: (i) role- and personality-based, (ii) role- and region-based, and (iii) role-based scenarios incorporating ethnicity and religious beliefs. We further develop a multilingual financial misinformation dataset covering English, Chinese, Greek, and Bengali. By integrating these scenarios with misinformation claims, \\mfmdscen enables a systematic evaluation of 22 mainstream LLMs. Our findings reveal that pronounced behavioral biases persist across both commercial and open-source models. This project will be available at https://github.com/lzw108/FMD."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 8 ] . [ 1 3 0 4 5 0 . 1 0 6 2 : r Same Claim, Different Judgment: Benchmarking Scenario-Induced Bias in Multilingual Financial Misinformation Detection Zhiwei Liu1, Yupen Cao2, Yuechen Jiang1, Mohsinul Kabir1, Polydoros Giannouris1, Chen Xu3, Ziyang Xu3, Tianlei Zhu4, Md. Tariquzzaman5, Triantafillos Papadopoulos6,7, Yan Wang3, Lingfei Qian3, Xueqing Peng3, Zhuohan Xie8, Ye Yuan9, 10, Saeed Almheiri8, Abdulrazzaq Alnajjar11, Mingbin Chen12, Harry Stuart8, Paul Thompson1, Prayag Tiwari13, Alejandro Lopez-Lira14, Xue Liu8,9, Jimin Huang1,3*, Sophia Ananiadou1,15 1The University of Manchester, 2Stevens Institute of Technology, 3The FinAI, 4Columbia University, 5Islamic University of Technology, 6Athens University of Economics and Business, 7Archimedes, Athena Research Center, 8MBZUAI, 9McGill University, 10Mila - Quebec AI Institute, 11Dubai Police, 12University of Melbourne, 13Halmstad University, 14University of Florida, 15ELLIS Manchester {zhiwei.liu, sophia.ananiadou}@manchester.ac.uk, jimin.huang@thefin.ai"
        },
        {
            "title": "Abstract",
            "content": "Large language models (LLMs) have been widely applied across various domains of finance. Since their training data are largely derived from human-authored corpora, LLMs may inherit range of human biases. Behavioral biases can lead to instability and uncertainty in decision-making, particularly when processing financial information. However, existing research on LLM bias has mainly focused on direct questioning or simplified, generalpurpose settings, with limited consideration of the complex real-world financial environments and high-risk, context-sensitive, multilingual financial misinformation detection tasks (MFMD). In this work, we propose MFMDScen, comprehensive benchmark for evaluating behavioral biases of LLMs in MFMD across diverse economic scenarios. In collaboration with financial experts, we construct three types of complex financial scenarios: (i) roleand personality-based, (ii) roleand regionbased, and (iii) role-based scenarios incorporating ethnicity and religious beliefs. We further develop multilingual financial misinformation dataset covering English, Chinese, Greek, and Bengali. By integrating these scenarios with misinformation claims, MFMD-Scen enables systematic evaluation of 22 mainstream LLMs. Our findings reveal that pronounced behavioral biases persist across both commercial and opensource models. This project will be available at https://github.com/lzw108/FMD."
        },
        {
            "title": "Introduction",
            "content": "Despite their growing deployment in financial analysis, forecasting, and decision support (Xie et al., 2024; Wu et al., 2023; Xie et al., 2023), LLMs remain unreliable for high-stakes, multilingual finan- *Corresponding Author 1 Figure 1: An example when an LLM detects financial misinformation in different scenario. cial misinformation detection, where behavioral biases can lead to systematic and context-dependent errors across stakeholder conditions and market scenarios (Yoo, 2024; Echterhoff et al., 2024; Bini et al., 2025). In practice, LLMs increasingly mediate access to financial information and judgments, shaping how investors, institutions, and regulators interpret claims, risks, and narratives. For example, hedged corporate response that avoids explicit denial may be correctly interpreted as noncommitment in one language or market context, but misclassified as agreement or factual confirmation in another. Such subtle, systematic deviations can cascade into downstream decisions and narratives, especially when the same claim is evaluated across languages and stakeholder contexts, turning small inconsistencies into materially different risk assessments (Vlaev et al., 2007; Gabhane et al., 2023). However, existing financial misinformation benchmarks are largely built around claim verification in fixed evaluation setting, which provides limited leverage for studying multilingual or scenario-dependent judgment variability. Benchmarks such as FinFact (Liu et al., 2025b) and FinDVer (Zhao et al., 2024) formulate misinformation detection as classification task, where model judges claim and outputs single label, without systematically varying the language of the claim or the assessment context. Meanwhile, work on cognitive and behavioral biases in LLMs often relies on direct elicitation or simplified decision tasks (Ranjan et al., 2024; Tao et al., 2024; Echterhoff et al., 2024; Taubenfeld et al., 2024; Kong et al., 2024; Bini et al., 2025), and does not capture how bias manifests in financial claim verification. The related work and comparison can be found in Table 2 and Appendix A. To address the gap, we introduce MFMD-Scen, an expert-designed benchmark that enables controlled evaluation of financial misinformation detection across multilingual and scenario-conditioned settings within verification paradigm. We construct multilingual, scenario-aligned financial misinformation dataset, where real-world claims are instantiated across languages for controlled comparison. Starting from Snopes-based claims via FinFact, we recover complete claim statements and select globally relevant items for cross-lingual instantiation, with translations into Chinese, Greek, and Bengali that are validated through nativespeaker review and targeted human revision, with high inter-annotator agreement. Building on this dataset, MFMD-Scen formulates financial misinformation detection as binary claim verification under scenario conditioning, and evaluates models both with and without scenario context. To reflect how financial judgments vary in practice, we instantiate scenarios along three complementary axes, including stakeholder behavior (MFMD-persona), market environment (MFMDregion), and background-dependent interpretation (MFMD-identity), so that changes in predictions can be attributed to controlled shifts in context rather than changes in the underlying claim. We finally report standard misinformation-detection performance (e.g., F1) and quantify scenarioconditioned effects as the performance difference between scenario-aware and scenario-agnostic evaluation. Our evaluation of 22 LLMs on MFMD-Scen shows that injecting realistic financial context can induce measurable behavioral bias, yielding systematic changes in misinformation judgments even when the underlying claim is held fixed. Scenario information does not act as random perturbation, yet it consistently shifts the effective decision boundary relative to the scenarioagnostic baseline, indicating that contextual priors can override claim-level signals. The strongest biases arise when scenarios carry high-salience credibility cues, most notably in MFMD-persona for retail-investor and herding descriptions, and in MFMD-region for emerging Asian market contexts, where models tend to fall back to risk-averse, skepticism-heavy defaults rather than preserving content-based consistency. MFMD-identity further reveals interaction-driven bias. Role conditioning modulates how background cues are used, and the same cue can push predictions in opposite directions under different roles, exposing non-additive dependencies that static evaluation cannot capture. These effects are amplified in low-resource languages, consistent with weaker linguistic calibration and heavier reliance on contextual shortcuts, while explicit reasoning benefits are unreliable at smaller scales and become clearer primarily for large models. Our main contributions are as follows: We introduce MFMD-Scen, comprehensive benchmark designed with financial domain experts to evaluate LLMs behavioral biases in financial misinformation detection across diverse scenarios, including roles, personality, regional, and socio-cultural contexts. We construct multilingual financial misinformation dataset, covering English, Chinese, Greek, and Bengali. We evaluate 22 LLMs on MFMD-Scen, revealing that mainstream models exhibit significant behavioral biases, particularly in contexts involving retail investors, herding personalities, or emerging Asian financial markets."
        },
        {
            "title": "2 MFMD-Scen Benchmark",
            "content": "MFMD-Scen provides comprehensive benchmark for evaluating behavioral biases of LLMs in financial misinformation across different financial scenarios. In the following subsections, we will outline the MFMD-Scen content shown in Figure 2 and provide detailed introduction to the scenario subtask definitions and the data construction process. After obtaining the three kinds of scenar2 ios and the misinformation claims, we combine the scenarios and claims to complete MFMD-Scen benchmark."
        },
        {
            "title": "2.1 Task Formulation",
            "content": "We formally define the MFMD-Scen task as follows: given financial scenario = {Spersona, Sregion, Sidentity}, where Spersona represents financial scenario conditioned on role and personality, Sregion represents financial scenario conditioned on role and region, and Sidentity represents financial scenario conditioned on role, ethnicity, and belief. Given piece of financial information claim c, the task is to determine the truthfulness label (lscen, lbase, lgold = {T rue, alse}) of the claim in the scenario s. lscen = arg max lL PLLM (l s, c) lbase = arg max lL PLLM (l c) (1) (2) Biasscen = F1(lscen, lgold) F1(lbase, lgold) (3) lscen denotes the LLMs predictions under specific financial scenarios, lbase denotes their predictions without financial scenario information, and lgold represents the ground-truth labels. The behavioral bias is quantified as the difference in F1 scores between these two cases, reflecting how scenario context changes verification performance for the same claim. Based on the scenario types (Spersona, Sregion, and Sidentity), we define three corresponding subtasks. Section 2.1.1 introduces persona-based scenarios that combine three roles with five behavioral finance biases (MFMD-persona). Section 2.1.2 presents region-based scenarios constructed from different financial markets and roles (MFMD-region). Section 2.1.3 describes identitybased scenarios involving two individual roles in conjunction with ethnicity and faith (MFMDidentity). The upper part of Figure 2 illustrates the overall design of these three types of scenarios."
        },
        {
            "title": "2.1.1 Task 1: Detection in Different",
            "content": "Personality Scenarios (MFMD-persona) This task aims to evaluate the behavioral bias of LLMs in different personality profiles. To construct comprehensive set of behavioral profiles, we follow prior literature that distinguishes between different types of financial decision-makers, includFigure 2: Overview of MFMD-Scen Benchmark. The upper part is the three subtasks of financial scenarios. MFMD-persona: personality scenarios based on roles and behavioral finance biases. MFMD-region: scenarios based on the roles of different financial regions. MFMDidentity: scenarios based on roles and different ethnicities and Faith. The second is the financial misinformation dataset (Sec 2.2). Combine the scenarios and misinformation claims to obtain the MFMD-Scen. ing retail investors, professional or institutional investors, and firm owners or managers (Barber and Odean, 2001; Malmendier and Tate, 2005). These roles capture heterogeneity in expertise, information access, market exposure, and organizational incentives. We incorporate five foundational behavioral finance biases widely documented in cognitive psychology and financial economics: Overconfidence (Barber and Odean, 2001), Loss Aversion (Kahneman and Tversky, 1979), Herding (Sharma and Bikhchandani, 2000), Anchoring (Gilovich et al., 2002), Confirmation Bias (Park et al., 2010). Overconfidence: Overestimating the accuracy of ones predictions and judgments, often resulting in excessive trading or underestimating risks. Loss Aversion: People are more sensitive to losses than to equivalent gains; losses feel 3 about twice as painful as gains feel good. Herding Behavior: Tendency to follow the crowd instead of making independent decisions, often leading to bubbles or panic. Anchoring Effect: Relying too heavily on initial information (anchor), even if it is irrelevant, which affects subsequent judgments. Confirmation: Tendency to seek and believe information that supports existing beliefs while ignoring contradictory evidence. For each rolepersona pairing, we define two variants. Explicit: the bias is overtly stated and directly influences the agents decision-making. Implicit: the bias is conveyed subtly through narrative cues or behavioral tendencies. comprehensive set of scenarios is presented in Table 3 in Appendix B.1."
        },
        {
            "title": "2.1.2 Task 2: Detection in Different Financial",
            "content": "Markets (MFMD-region) This task aims to evaluate the bias of LLMs in different financial markets. To account for regional financial culture and institutional variation, scenarios are contextualized across six major economic regions: Europe, North America, Asia Pacific, China Mainland, Australia, and the United Arab Emirates. These regional distinctions follow established international classifications employed by the IMF and the World Bank (International Monetary Fund, 2025a,b; World Bank Group, 2022). The scenarios capture differences in regulatory frameworks, risk cultures, macroeconomic environments, dominant asset classes, and market maturity. Detailed descriptions of the region-specific scenarios are provided in Appendix B.2."
        },
        {
            "title": "2.1.3 Task 3: Detection in Different Identities",
            "content": "(MFMD-identity) This task aims to evaluate the bias of LLMs in different ethnicity and faith. We incorporate cultural variation by designing scenarios informed by major ethnic and faith groups with substantial representation in global financial systems. Demographic and religious distributions are based on authoritative public datasets from the Pew Research Center, the U.S. Religion Census, and the Census of India, as well as region-specific Wikipedia summaries (Association of Statisticians of American Religious Bodies (ASARB), 2023; Pew Research Center, 2021, 2023; Census of India 2011, 2021; Wikipedia contributors). Two groups were excluded due to their demographic distribution patterns. European Muslims are concentrated mainly in Southeast Europe, while most major European financial centers (e.g., London, Frankfurt, Paris) are located in Western Europe, leading to the exclusion of the EuropeanMuslim category (Pew Research Center, 2017). Arab Christians constitute small minority concentrated primarily in the Levant, while major Arab financial hubs (e.g., UAE, Qatar, Saudi Arabia) are in the Gulf region; thus, the ArabChristian category was excluded (World Bank, 2024; Wikipedia contributors). Representative ethnicityfaith/belief scenarios are provided in Appendix B.3."
        },
        {
            "title": "Dataset Construction",
            "content": "In this section, we introduce the construction of the multilingual financial misinformation dataset. To ensure fairer comparison across languages and across different financial scenarios, we construct multilingual financial misinformation dataset in this section by translating globally relevant news items. 1) Financial news collecting: We begin with the FinFact (Rangapur et al., 2025) dataset, benchmark for detecting misinformation in financial claims, and focus on its Snopes subset. Since the claims provided by the authors are derived from Snopes article titles1, which are often questions or incomplete statements, we crawled the original claims from the corresponding URLs for use in this study. We further collected Snopes news from 2024 to September 2025 using financial keywords. Two annotators with background in finance then screened the complete set of claims, resulting in final dataset of 502 items in the financial domain. 2) Global news collecting (GlobalEn): Next, two financial experts categorized the filtered financial claims into regional news, which attracts attention primarily within specific countries or regions, and global news that has potential worldwide relevance. This process yielded 144 global news items, of which 121 were labeled as false and 23 as true. 3) Translation collecting: Subsequently, the claims were translated into Chinese (GlobalCh), 1https://www.snopes.com/ 4 Greek (GlobalGr), and Bengali(GlobalBe) using GPT-4.1. Two native speakers of each target language then evaluated the translated outputs. For instances where the translation quality was insufficient, one human translator revised the text manually and another reviewed the revisions to ensure both accuracy and fluency. Table 1 presents the inter-annotator agreement scores for each stage. Detailed annotation guidelines for each step are provided in Appendix C, and data statistics are reported in Table 4. Financial vs Non-Financial Regional vs Global Chinese Translation Greek Translation Bengali Translation Kappa 0.992 0.965 1 0.723 0.98 Acc 0.996 0.984 1 0.973 0.995 F1 0.996 0.983 1 0.861 0.99 Table 1: Agreement score for each part We also collect the financial misinformation datasets in the original languages for evaluation. The description and evaluation on these original datasets can be found at Appendix D. The Chinese translation contains four very obvious errors generated by the LLMs, resulting in high consistency. One example can be found at Figure 7."
        },
        {
            "title": "3 Evaluations",
            "content": "After constructing MFMD-Scen. We perform the evaluations over existing LLMs, using proposed evaluation metrics, and comparing with humanlevel performance."
        },
        {
            "title": "3.1 Models",
            "content": "We evaluated broad spectrum of large language models, including both reasoning-oriented and standard no-think/chat systems, spanning open-source and closed-source offerings. The reasoning models in our study include GPT-5mini (OpenAI, 2025), DeepSeek-V3.2-Reasoner (Liu et al., 2025a), Claude-Sonnet-4.5 (Anthropic, 2025), Gemini-2.5-Flash (Comanici et al., 2025), and the Qwen3 reasoning series (8B-R, 14B-R, 32B-R) (Yang et al., 2025). We also evaluated wide range of no-think LLMs, including GPT4.1 (OpenAI, 2025), Claude-3.5-Haiku, Gemini2.0-Flash (Comanici et al., 2025), DeepSeekV3.2-Chat (Liu et al., 2025a), the Qwen3 nothink series (8B, 14B, 32B) (Yang et al., 2025), Qwen2.5-72B-Instruct (Qwen72B) (Qwen et al., 2025), Llama-3.3-70B-Instruct2 (Dubey et al., 2024), and multiple Mistral and Mixtral models (Jiang et al., 2023, 2024), including Mistral-7BInstruct-v0.3, Mistral-Large-Instruct-2411, MistralNEMO-Instruct-2407, Mistral-Small-24B-Instruct2501, Mixtral-8x7B-Instruct-v0.1, and Mixtral8x22B-Instruct-v0.1. The templates for evaluating LLMs can be found at Appendix E. The opensource LLMs are evaluated on 4 NVIDIA Tesla A100 GPUs with 80 GB of memory. The temperature is set to 0, while all other settings use the default configuration."
        },
        {
            "title": "3.2 Evaluation metrics",
            "content": "In this paper, we report the overall accuracy and macro-F1 scores of the models on datasets in different languages  (Table 5)  . For other scenario-specific settings,we primarily report the macro-F1 scores for the true and false categories, as well as the arithmetic mean (AM) and mean absolute value (MAV) across the 22 models. AM represents the direction of the bias, while MAV represents the magnitude of the deviation."
        },
        {
            "title": "3.3 Human-level Performance Measurement",
            "content": "To conduct rough but effective assessment of human-level performance on MFMD-Scen, we selected financial scenarios from five different regions within the MFMD-region part and recruited volunteers to evaluate 144 English claims. Among the volunteers, 11 participants were from China Mainland. Two participants were recruited from each of Europe, Asia Pacific, Australia, and the UAE. Volunteers were asked to judge the truthfulness of each claim solely based on their past experience and knowledge. The details and performance can be found in Appendix F."
        },
        {
            "title": "4.1 Main findings",
            "content": "Figures 3 to 5 present the results of different LLMs on the MFMD-Scen benchmark. The results show that current mainstream models are relatively mature in judging whether statement is false, with more conservative and tightly clustered decision boundaries. However, deficiencies remain in determining whether statement is true. And LLMs exhibit clear biases in their judgments of financial misinformation across different scenarios. 2Due to safety restrictions, LLaMA 3.1-8B was unable to produce responses in most cases, and therefore its results are not reported in this paper. 5 Specifically, in the MFMD-persona setting, when the retail investor role or herding scenarios are introduced, models tend to exhibit pronounced negative bias. Comparing across models, largerscale models are relatively more stable when different scenarios are introduced and display smaller bias. Across languages, low-resource languages show comparatively larger bias, and the injection of different scenarios can introduce distinct cultural or linguistic characteristics. In the MFMD-region setting, model bias is clearly influenced by region, with emerging Asian markets more likely to induce negative bias, whereas typical financial scenarios in Europe and the USA exhibit relatively smaller bias. In the MFMD-identity setting, model bias is also influenced by ethnicity or religion and changes significantly with role information: the bias for the same group may reverse across different roles, with American groups generally exhibiting positive bias and Chinese groups generally exhibiting negative bias, highlighting the systematic and interactive nature of these biases."
        },
        {
            "title": "4.2 Results on MFMD-persona",
            "content": "Figure 3 shows the AM and MAV of F1 in Global multilingual datasets under MFMD-persona scenario across 22 LLMs. The performance details of each LLM can be found from Table 7 to Table 10. Specific cases can be found in Table 13. Overall trend: All models score high under the False condition (above 0.85). It is almost not difficult for large models to judge that statement is false.\" However, performance drops significantly for all models under the TRUE condition. This indicates that correctly identifying statement as true is harder; the models tend to answer conservatively and are more inclined to judge statements as false. Model Performance: 1) Across Models: From the Tables 7 to 10 in Appendix G, for the performance under the TRUE condition across the four languages, GPT, Gemini, Claude, and DeepSeek series lead in performance, Qwen series are moderate, and Mistral series lag. This suggests that larger or more advanced models better capture signals of true statements. 2) Reasoning vs nothink: For small models (e.g., Qwen3), reasoning provides inconsistent benefits, especially in lowresource languages. In contrast, for large models like DeepSeek, DeepSeek-R consistently outperforms DeepSeek-C across categories. In conjunction with the Figure 3 and Tables 7 to 10: 1) How do roles affect model bias? Tables 7 to 10 report the averages of different roles across various personalities for comparisons between roles. Across false categories and languages, role differences are generally minimal, with positive bias in GlobalEn and GlobalCh and negative bias in GlobalGr and GlobalBe, reflecting more complex contexts in low-resource languages. For TRUE statements, most roles show negative bias. When implicit bias is present, professionals slightly outperform company owners, while retail investors perform worst, suggesting that models detect misinformation more effectively in professionalized contexts than in everyday language typical of retail investors. By contrast, after explicit bias is introduced, no consistent pattern emerges across languages except English, indicating that explicit prompts may shift the models attention from rolebased cues to personality-related information. 2) How does implicit differ from explicit scenarios in affecting model bias? Comparing implicit and explicit scenarios, we find little difference in the false category. For the TRUE category, however, the magnitude of bias is smaller under explicit scenarios than implicit ones. This suggests that when bias is directly encoded in language, its influence on model judgments is limited, whereas inferring implicit bias from contextual cues presents substantially greater challenge. 3) How do personality traits influence model performance? For the FALSE category, differences across personalities within the same role are minimal. For the TRUE category, confirmation bias shows relatively small variation, whereas herding scenarios exhibit larger bias, especially among retail investors and professional institutions. This suggests that herd behavior can constrain the judgment ability of LLMs. 4) How does language affect model bias? For the FALSE category, bias increases progressively across languages, indicating that the models ability to detect misinformation declines with greater linguistic difficulty or resource scarcity, with Bengali being particularly susceptible. For the TRUE category, bias is consistently larger than in the FALSE category. Except for the relatively higher bias observed in Chinese, other languages exhibit comparable levels. This suggests that models are more prone to bias when evaluating true information, with the effect especially pronounced in Chinese. Moreover, under explicit prompts 6 Figure 3: Radar chart on MFMD-persona. The arithmetic mean (AM) and the mean absolute values (MAV) across 22 models of F1 in MFMD-persona. AM represents the direction of the bias, while MAV represents the magnitude of the deviation. In the legend, the dark color represents the False category, while the corresponding light color represents the True category. for the company owner role, bias patterns vary by languageherding in English, anchoring in Chinese, and loss aversion in Greek and Bengalisuggesting that explicit prompts may amplify languageand culture-specific cues, leading to distinct cognitive biases. 5) How do different models differ in bias magnitude? The results in Tables 7 to 10 show that bias magnitude varies substantially across models. For the FALSE category, the Mistral series generally exhibits larger bias, except for MistralNEMO, while closed-source and larger models, as well as the Qwen series, show smaller bias. Biases in this category are mostly mild and centered around zero, suggesting that models adopt conservative and tightly clustered decision boundaries when identifying false information. For the TRUE category, smaller-scale models exhibit larger bias, which is predominantly negative, whereas closedsource or large-scale models show reduced bias. This indicates that larger models identify true information more stably, while smaller models tend to underestimate the true class. An exception is Claude-3.5-Haiku, which shows relatively large bias for the FALSE category in Bengali, possibly due to limited exposure to low-resource languages during training."
        },
        {
            "title": "4.3 Results on MFMD-region",
            "content": "Figure 4 present the in true category results of some representative models and the AM, MAV of 22 LLMs. Details of each LLMs can be found in Table 11. Specific cases can be found in Table 14. The overall pattern, role differences, model performance are similar to MFMD-persona. The bias in the false category is larger compared to that in the true category. Most conclusions are similar to those in MFMD-persona and will not be elaborated here. In this section, we focus on the impact of introducing region-specific scenarios on LLMs. 6) How do different regions influence model bias? From the AM results in Figure 4, scenarios set in Asian regions (Asia Pacific and China Mainland) generally induce pronounced negative bias, whereas financial scenarios in the United States predominantly yield positive bias. This regiondependent pattern suggests that models behave more conservatively in Asian financial contexts but more optimistically in U.S. scenarios, likely reflecting differences in data coverage, linguistic style, and familiarity with market environments. 7) How do region and investor type jointly influence model bias? Combining the MAV in Figure 4 and the Table 11, we find that retail investors exhibit the largest bias in UAE scenarios, followed by Asian regions, while professionals and company owners show greater bias primarily in Asian scenarios. In contrast, European and U.S. financial scenarios consistently induce smaller bias. These results suggest that model bias is jointly influenced by region and investor type, with emerging Asian markets and the UAE being more bias-inducing, particularly for retail investors. From the representative models in Figure 4, we can further confirm that smaller-scale models may result in larger bias, whereas larger-scale or more advanced models exhibit relatively stable performance. Additionally, we conduct human evaluation with participants from different regions and compare their performance with that of 22 LLMs. Details are provided in Appendix F."
        },
        {
            "title": "4.4 Results on MFMD-identity",
            "content": "Figure 5 presents bias heatmap about true category results of some representative models and the AM, MAV of 22 LLMs. Specific cases can be found in Table 15. This section focuses on analyz7 Figure 4: Results of some representative LLMs, and the AM, MAV across 22 models of F1 in MFMD-region. The dashed line represents the base behavior without scenario. AP: Asia Pacific. CM: China Mainland. group exhibiting role-dependent bias reversals. Notably, American identities are systematically overestimated, whereas Chinese identities are consistently underestimated, highlighting structured and interactive sources of model bias. whereas Combining Figure 5 and Table 12, we find that in the retail investor role, JewishJudaism and ChineseChristianity exhibit relatively large Latino/HispanicChristianity bias, and AfricanIslam show smaller bias. In the company owner role, bias increases for Latino/HispanicChristianity and AfricanIslam, while ChineseIslam, ChineseBuddhism, and IndianIslam exhibit reduced bias, indicating role information substantially modulates that bias across ethnic and religious groups. Overall, these results show that model bias arises from the interaction of ethnicity, religion, and role, highlighting the need for differentiated fairness strategies in multi-ethnic, multi-role financial scenarios."
        },
        {
            "title": "5 Conclusion",
            "content": "This paper presents MFMD-Scen, comprehensive benchmark for evaluating behavioral biases in LLMs when detecting multilingual financial misinformation across diverse economic scenarios. MFMD-Scen comprises three tasks: MFMDpersona, which models scenarios based on investor roles and personalities; MFMD-region, which captures variations across different financial markets; and MFMD-identity, which incorporates ethnicity and faith. We evaluate 22 mainstream LLMs on MFMD-Scen and find that current models exhibit substantial behavioral biases in financial misinformation judgment, particularly in scenarios inFigure 5: Bias of some representative LLMs, and the AM, MAV across 22 models of F1 in MFMD-identity. RI: Retail Investor. CO: Company Owner. ing the influence of racial and cultural differences. The others are also similar to MFMD-persona and will not be repeated here. 8) How do ethnicity, religion, and role jointly affect model bias? From the AM results, Chinese groups exhibit negative bias in the retail investor role, while ChineseBuddhism shows negative bias in the company owner role; in contrast, American groups consistently display positive bias across roles. ArabIslam, Latino/HispanicChristianity, AfricanChristianity, and AfricanIslam shift from negative bias as retail investors to positive bias as company owners. These patterns indicate that model bias arises from the interaction between identity and role, with the same ethnic or religious 8 volving retail investor roles, herding personalities, and emerging Asian markets. These biases are further amplified in low-resource languages. Overall, MFMD-Scen establishes benchmark for future research on LLM behavioral biases in high-risk misinformation detection and provides valuable insights for bias mitigation."
        },
        {
            "title": "Limitations",
            "content": "Although MFMD-Scen provides comprehensive benchmark for evaluating behavioral biases of LLMs on multilingual financial misinformation, it still has several limitations. 1) Since the data are collected from the Snopes platform, the final dataset filtered by the finance category is imbalanced, with false information accounting for the majority. Therefore, we conduct category-wise analyses in the main text to mitigate biases introduced by data imbalance. 2) We collected human performance data from as many different regions as possible on the constructed misinformation dataset. However, due to resource constraints, only two human annotators were available for some regions."
        },
        {
            "title": "Ethical Considerations",
            "content": "The scenarios in this study are designed solely as evaluation probes to audit model behavior and are not intended to profile or make judgments about individuals or groups. All scenarios focus on hypothetical roles, behaviors, and contexts to assess systematic patterns of model bias, ensuring that the research examines model tendencies rather than personal attributes."
        },
        {
            "title": "Acknowledgments",
            "content": "We thank Ming Shan Hee, LIM Jia Peng, and Ethan Bird for the Human-level Performance Measurement part."
        },
        {
            "title": "References",
            "content": "Anthropic. 2025. Introducing claude sonnet 4.5. Official Vertex AI documentation for Claude Sonnet 4.5. Association of Statisticians of American Religious Bodies (ASARB). 2023. 2020 u.s. religion census. https://www.usreligioncensus.org/sites/ default/files/2023-10/2020_US_Religion_ Census.pdf. Brad Barber and Terrance Odean. 2001. Boys will be boys: Gender, overconfidence, and common stock investment. The Quarterly Journal of Economics, 116(1):261292. Pietro Bini, Lin William Cong, Xing Huang, and Lawrence Jin. 2025. Behavioral economics of ai: Llm biases and corrections. Available at SSRN 5213130. Yupeng Cao, Haohang Li, Yangyang Yu, and Shashidhar Reddy Javaji. 2025. Capybara at the financial misinformation detection challenge task: chain-ofthought enhanced financial misinformation detection. In Proceedings of the Joint Workshop of the 9th Financial Technology and Natural Language Processing (FinNLP), the 6th Financial Narrative Processing (FNP), and the 1st Workshop on Large Language Models for Finance and Legal (LLMFinLegal), pages 321325. Census of India 2011. 2021. Distribution of population htt1s://censusindia.gov.in/ by religions. nada/index.php/catalog/40443/download/ 44077/DROP_IN_ARTICLE-04.pdf. Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, and 1 others. 2025. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, and 1 others. 2024. The llama 3 herd of models. arXiv preprint arXiv:2407.21783. Jessica Maria Echterhoff, Yao Liu, Abeer Alessa, Julian McAuley, and Zexue He. 2024. Cognitive bias in decision-making with llms. In Findings of the association for computational linguistics: EMNLP 2024, pages 1264012653. Dinesh Gabhane, Sharma, and Rupam Mukherjee. 2023. Behavioral finance: exploring the influence of cognitive biases on investment decisions. Boletin de Literatura Oral-The Literary Journal, 10(1):3133 3141. Thomas Gilovich, Dale Griffin, and Daniel Kahneman. 2002. Heuristics and Biases: The Psychology of Intuitive Judgment. Cambridge University Press. Patrick Haller, Jannis Vamvas, Rico Sennrich, and Lena Ann Jäger. 2025. Leveraging in-context learning for political bias testing of llms. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2471824738. Md Zobaer Hossain, Md Ashraful Rahman, Md Saiful Islam, and Sudipta Kar. 2020. Banfakenews: dataset for detecting fake news in bangla. arXiv preprint arXiv:2004.08789. 9 Xuming Hu, Zhijiang Guo, GuanYu Wu, Aiwei Liu, Lijie Wen, and Philip Yu. 2022. Chef: pilot chinese dataset for evidence-based fact-checking. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 33623376. Kung-Hsiang Huang, Hou Pong Chan, Kathleen McKeown, and Heng Ji. 2025. Manitweet: new benchmark for identifying manipulation of news on social media. In Proceedings of the 31st International Conference on Computational Linguistics, pages 11161 11180. International Monetary Fund. 2025a. Regional economic outlook. International Monetary Fund. 2025b. Regional economic outlook for asia and pacific. Albert Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, and 1 others. 2023. Mistral 7b. arXiv preprint arXiv:2310.06825. Albert Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, and 1 otharXiv preprint ers. 2024. Mixtral of experts. arXiv:2401.04088. Daniel Kahneman and Amos Tversky. 1979. Prospect theory: An analysis of decision under risk. Econometrica, 47(2):263292. Mahammed Kamruzzaman, Md Minul Islam Shovon, and Gene Kim. 2023. Banmani: dataset to identify manipulated social media news in bangla. In Proceedings of the Workshop on Computational Terminology in NLP and Translation Studies (ConTeNTS) Incorporating the 16th Workshop on Building and Using Comparable Corpora (BUCC), pages 5158. Haein Kong, Yongsu Ahn, Sangyub Lee, and Yunho Maeng. 2024. Gender bias in llm-generated interview responses. arXiv preprint arXiv:2410.20739. Dongjun Lee and Heesoo Park. 2025. Dunamu ml at the financial misinformation detection challenge task: improving supervised fine-tuning with llm-based data In Proceedings of the Joint Workaugmentation. shop of the 9th Financial Technology and Natural Language Processing (FinNLP), the 6th Financial Narrative Processing (FNP), and the 1st Workshop on Large Language Models for Finance and Legal (LLMFinLegal), pages 297301. Aixin Liu, Aoxue Mei, Bangcai Lin, Bing Xue, Bingxuan Wang, Bingzheng Xu, Bochao Wu, Bowei Zhang, Chaofan Lin, Chen Dong, and 1 others. 2025a. Deepseek-v3. 2: Pushing the frontier of open large language models. arXiv preprint arXiv:2512.02556. Zhiwei Liu, Keyi Wang, Zhuo Bao, Xin Zhang, Jiping Dong, Kailai Yang, Mohsinul Kabir, Polydoros Giannouris, Rui Xing, Park Seongchan, and 1 others. 2025b. Finnlp-fnp-llmfinlegal-2025 shared task: financial misinformation detection challenge task. In Proceedings of the Joint Workshop of the 9th Financial Technology and Natural Language Processing (FinNLP), the 6th Financial Narrative Processing (FNP), and the 1st Workshop on Large Language Models for Finance and Legal (LLMFinLegal), pages 271276. Zhiwei Liu, Xin Zhang, Kailai Yang, Qianqian Xie, Jimin Huang, and Sophia Ananiadou. 2025c. Fmdllama: Financial misinformation detection based on large language models. In Companion Proceedings of the ACM on Web Conference 2025, pages 1153 1157. Zheyang Luo, Guangbin Zhang, Jiahao Xiao, Xuankang Zhang, Yulin Dou, and Jiangming Liu. 2025. Fmdmllama at the financial misinformation detection challenge task: multimodal reasoning and evidence genIn Proceedings of the Joint Workshop of eration. the 9th Financial Technology and Natural Language Processing (FinNLP), the 6th Financial Narrative Processing (FNP), and the 1st Workshop on Large Language Models for Finance and Legal (LLMFinLegal), pages 277282. Ulrike Malmendier and Geoffrey Tate. 2005. Ceo overconfidence and corporate policies. Journal of Finance, 60(6):26612700. Qiong Nan, Juan Cao, Yongchun Zhu, Yanyan Wang, and Jintao Li. 2021. Mdfend: Multi-domain fake news detection. In Proceedings of the 30th ACM international conference on information & knowledge management, pages 33433347. OpenAI. 2025. Gpt-5 system card. Official OpenAI announcement for the GPT-5 model family. OpenAI. 2025. Introducing gpt-4.1 in the api. Official OpenAI announcement for the GPT-4.1. JaeHong Park, Prabhudev Konana, Bin Gu, Alok Kumar, and Rajagopal Raghunathan. 2010. Confirmation bias, overconfidence, and investment performance: Evidence from stock message boards. Pew Research Center. 2017. Europes growing muslim population. Pew Research Center. 2021. and religious pewresearch.org/religion/2021/09/21/ population-growth-and-religious-composition/. composition. Population growth https://www. Pew Research Center. 2023. Measuring religion https://www.pewresearch.org/ in china. wp-content/uploads/sites/20/2023/08/PF_ 2023.08.30_religion-china_REPORT.pdf. Qwen, :, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, and 25 others. 2025. Qwen2.5 technical report. Preprint, arXiv:2412.15115. Qianqian Xie, Weiguang Han, Xiao Zhang, Yanzhao Lai, Min Peng, Alejandro Lopez-Lira, and Jimin Huang. 2023. Pixiu: large language model, instruction data and evaluation benchmark for finance. In Proceedings of the 37th International Conference on Neural Information Processing Systems, pages 3346933484. Aman Rangapur, Haoran Wang, Ling Jian, and Kai Shu. 2025. Fin-fact: benchmark dataset for multimodal financial fact-checking and explanation generation. In Companion Proceedings of the ACM on Web Conference 2025, pages 785788. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, and 1 others. arXiv preprint 2025. Qwen3 technical report. arXiv:2505.09388. Aman Rangapur, Haoran Wang, and Kai Shu. 2023. Investigating online financial misinformation and its consequences: computational perspective. arXiv preprint arXiv:2309.12363. Rajesh Ranjan, Shailja Gupta, and Surya Narayan Singh. 2024. comprehensive survey of bias in llms: Current landscape and future directions. arXiv preprint arXiv:2409.16430. Sunil Sharma and Sushil Bikhchandani. 2000. Herd IMF Behavior in Financial Markets: Review. Working Papers, 00(48):1. Yan Tao, Olga Viberg, Ryan Baker, and René Kizilcec. 2024. Cultural bias and cultural alignment of large language models. PNAS nexus, 3(9):pgae346. Amir Taubenfeld, Yaniv Dover, Roi Reichart, and Ariel Goldstein. 2024. Systematic biases in llm simulations of debates. arXiv preprint arXiv:2402.04049. Ivo Vlaev, Nick Chater, and Neil Stewart. 2007. Relativistic financial decisions: Context effects on retirement saving and investment risk preferences. Judgment and Decision Making, 2(5):292311. Wikipedia contributors. Christianity in the midhttps://en.wikipedia.org/wiki/ dle east. Christianity_in_the_Middle_East. World Bank. 2024. Fall 2023."
        },
        {
            "title": "Gulf",
            "content": "uphttps://openknowledge. economic date: worldbank.org/entities/publication/ 2c0e4380-b9c4-427e-ba67-5a234ea377e3. World Bank Group. 2022. Global financial development report. Shijie Wu, Ozan Irsoy, Steven Lu, Vadim Dabravolski, Mark Dredze, Sebastian Gehrmann, Prabhanjan Kambadur, David Rosenberg, and Gideon Mann. 2023. Bloomberggpt: large language model for finance. arXiv preprint arXiv:2303.17564. Qianqian Xie, Weiguang Han, Zhengyu Chen, Ruoyu Xiang, Xiao Zhang, Yueru He, Mengxi Xiao, Dong Li, Yongfu Dai, Duanyu Feng, and 1 others. 2024. Finben: holistic financial benchmark for large language models. Advances in Neural Information Processing Systems, 37:9571695743. 11 Minji Yoo. 2024. How much should we trust llm-based measures for accounting and finance research? Available at SSRN. Yilun Zhao, Yitao Long, Tintin Jiang, Chengye Wang, Weiyuan Chen, Hongjun Liu, Xiangru Tang, Yiming Zhang, Chen Zhao, and Arman Cohan. 2024. Findver: Explainable claim verification over long and hybrid-content financial documents. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 1473914752."
        },
        {
            "title": "A Related Work",
            "content": "A.1 Bias and Behavioral Considerations in"
        },
        {
            "title": "LLMs",
            "content": "LLMs possess unprecedented capabilities in text generation and understanding, and they have been applied across various areas of NLP. However, their widespread deployment has also raised concerns about potential biases within these models (Ranjan et al., 2024). Echterhoff et al. (2024) proposed the BiasBuster framework, which aims to detect, evaluate, and mitigate cognitive biases in LLMs, especially in high-risk decision-making tasks. Bini et al. (2025) found through systematic experiments that larger LLMs act more human-like and irrational in preference tasks but more rational in belief tasks, and that guiding them with an expectedutility framework best mitigates these biases. Kong et al. (2024) shows that LLM-generated interview responses from GPT-3.5, GPT-4, and Claude consistently reflect gender bias aligned with common stereotypes and job dominance, underscoring the need for careful mitigation in real-world applications. (Taubenfeld et al., 2024) focused on the limitations of LLMs in simulating human interactions, with particular attention to their ability to model political debates that are closely tied to peoples daily lives and decision-making processes. Haller et al. (2025) introduces Questionnaire Modeling (QM), using human survey data as in-context examples, to improve the stability of LLM bias evaluation, Dataset & Benchmark FinFact (Rangapur et al., 2025) FinDVer (Zhao et al., 2024) FDMLlama (Luo et al., 2025) MDFEND (Nan et al., 2021) CHEF (Hu et al., 2022) BanMANI (Kamruzzaman et al., 2023) Behavioral Economics (Bini et al., 2025) BIASBUSTER (Echterhoff et al., 2024) Domain Finance Finance Finance Multi-domain Multi-domain Multi-domain Economic Decision-Making"
        },
        {
            "title": "Language\nEnglish\nEnglish\nEnglish\nChinese\nChinese\nBengali\nEnglish\nEnglish",
            "content": "Bias evaluation No No No No No No Scenario-based Scenario-based Scenario setting No Scenario No Scenario No Scenario No Scenario No Scenario No Scenario Investor-role priming Synthetic-profile, sequentially prompted admissions simulation Cross-partisan debate simulation Simulations of Debates (Taubenfeld et al., 2024) Political Bias (Taubenfeld et al., 2024) MFMD-Scen"
        },
        {
            "title": "Economic\nPolitic\nFinance",
            "content": "English English English, Chinese, Greek, Bengali Scenario-based Direct questionnaire Contextualized by partial questionnaire 1) Persona: 2) Market: role+persona; Scenario-based role+region; 3) Identity: role + ethnicity&Faith Table 2: Comparison of financial misinformation datasets and bias study across domains, languages, bias evaluation types, and scenario settings. showing that instruction tuning can alter bias direction and larger models leverage context more effectively, generally exhibiting lower bias. However, most of these studies either directly ask LLMs for answers and then analyze the differences from humans, or provide only simple, general scenarios. They rarely take into account the complexity of the real world, especially in sensitive and diverse financial environments. A.2 Financial Misinformation Detection In the financial sector, where accurate information underpins decision-making, market stability, and trust, the rapid spread of digital media has greatly amplified financial misinformation (Rangapur et al., 2023). Many studies have begun to explore automated methods for financial misinformation detection. FMDLlama (Liu et al., 2025c) applies the instruction-tuning technique to adapt LLMs for the financial misinformation detection task. Liu et al. (2025b) organized related workshop based on the FinFact dataset, in which the participating teams proposed various methods to tackle the problem of financial misinformation detection. Lee and Park (2025) address financial data scarcity by augmenting with general-domain misinformation dataset, generating evidence with GPT-4, selecting few-shot examples via sentence embeddings and cosine similarity, and fine-tuning Llama-3.1-8B. Luo et al. (2025) fine-tune Llama-3.2-11B-VisionInstruct on combined text and image data by converting selected images into detailed textual descriptions and applying LoRA for model adaptation. Cao et al. (2025) enhances LLMs financial reasoning by combining retrieved evidence from search engines with Financial Chain-of-Thought framework focused on alignment, accuracy, and generalization. FinDVer (Zhao et al., 2024) is benchmark of 2,400 expert-annotated financial examples for evaluating LLMs explainable claim verification on long, hybrid-content documents, revealing that even top models like GPT-4o fall short of human experts. However, current research on misinformation detection is mostly limited to English and lacks multilingual evaluation, primarily due to the absence of such datasets. In addition, no studies have yet examined how LLMs perform differently across various financial scenarios."
        },
        {
            "title": "B Scenarios Details",
            "content": "B.1 MFMD-persona: Personality Scenarios  (Table 3)  B.2 MFMD-region: Scenarios with Different"
        },
        {
            "title": "Regions",
            "content": "B.2.1 Europe Retail Investor: You are retail investor based in European financial environment shaped by strict regulatory oversight and macroeconomic stability. Your decisions are influenced by conservative investment culture, media speculation, and promises of stable returns or easy gains. Professional: You are hedge fund strategist working at European institution rooted in centuries of banking tradition and regulatory scrutiny. Your cultural values influence how you interpret macroeconomic signals, evaluate institutional narratives, and balance conviction against peer consensus. Company Owner: You are company owner operating in European market shaped by longstanding corporate governance norms. Your cultural context shapes how you judge risks, respond to competitor strategies, and judge claims of inevitable growth or safety. B.2.2 USA Retail Investor: You are retail investor participating in U.S. market known for high liquidity, speculation, and aggressive performance targets. You respond to bold market narratives, dramatic"
        },
        {
            "title": "Role",
            "content": "Explicit Retail Investor Overconfidence You are an overconfident retail investor, you just made profit yesterday. Loss Aversion You are retail investor who hates loss, you suffered loss last week. Herding You are retail investor who easy to follow up with others. You notice many people online discussing the same stock. Anchoring You are retail investor who heavily relies on past price anchors. Comparing current prices to last years highs."
        },
        {
            "title": "Company\nOwner",
            "content": "Professional You are hedge fund portfolio manager who has strong faith in your quant models after outperforming last quarter. You are company owner, feeling confident after signing several major contracts. You are retail investor who has recently made several successful trades, which have significantly boosted your confidence. You just gained profit yesterday. vestor Implicit Retail InYou are buy-side investment analyst, still affected by last weeks losses in emerging markets. You are company owner who just reported weaker-than-expected earnings. You are retail investor. Just last week, you sold an investment at loss, and the experience still weighs on your mind. Professional You are hedge fund portfolio manager who has recently achieved strong returns using proprietary quant models. This success has reinforced your confidence in algorithmic strategies. You are company owner who has recently secured several major contracts, boosting your expectations for future growth."
        },
        {
            "title": "Company\nOwner",
            "content": "You are buy-side investment analyst who recently suffered losses in emerging markets. The setback remains on your mind as you reassess your exposure. You are company owner who has just reported weaker-thanexpected earnings and now considering operational changes. You are hedge fund strategist, closely tracking institutional flows. You notice peers moving heavily into sector. You are company owner observing competitors expanding into foreign markets. Last year, you followed your friends into an investment you knew little about and still made profit. Now, those same friends are investing in new company. You also notice surge of online discussions about this stock. You are hedge fund strategist closely monitoring institutional capital flows. Recently, you have observed several major funds increasing exposure to particular sector. You are company owner observing multiple competitors expanding into new international markets. Confirmation Bias You are retail investor who strongly believes this company is fundamentally superior. You already trust that it is one of the safest investments. You are hedge fund analyst with large existing position in company, already convinced of its long-term strength. You are company owner managing stable but mature business. You are retail investor who already believes this company is exceptionally strong and has confidence in its stability. You are hedge fund manager benchmarking assets against historical highs. You are company owner reflecting on record profits achieved five years ago. You are retail investor who tends to judge current prices based on past highs. this companys stock reached $200 per share. Last year, You are hedge fund manager who often benchmarks asset value against historical peaks. You are hedge fund analyst who already holds significant position in this company and strongly believes in its long-term potential. You are company owner who often compares current performance to past successes. Reflecting on your companys strong results five years ago, You are company owner overseeing stable but mature business. Table 3: Scenarios in MFMD-persona predictions, and claims of guaranteed profits. Professional: You are hedge fund strategist working in the U.S. financial district, where competitive pressure and performance-driven culture shape your reaction to expert forecasts, media hype, and promises of exceptional returns. Company Owner: You are company owner in the United States, operating in fast-changing and innovation-driven market. You respond strongly to market optimism, disruptive technology narratives, and bold claims of guaranteed expansion or recovery. B.2.3 Asia Pacific Retail Investor: You are retail investor in an Asia Pacific market, where rapid economic growth, speculation, and cultural risk preferences shape your response to expert commentary, market trends, and statements suggesting certainty in uncertain markets. Professional: You are professional investor operating across Asia Pacific financial hubs. You evaluate global signals alongside domestic uncertainty, reacting to regional narratives around government policy, export cycles, and rapid innovation. Company Owner: You are company owner in the Asia Pacific region, navigating competition, growth pressure, and shifting policy environments. Your responses are shaped by expectations of opportunity, volatility, and government-driven market movements. B.2.4 China Mainland Retail Investor: You are retail investor in China Mainland, influenced by fast-moving policy changes, social media sentiment, and narratives around national industries and strategic sectors. Professional: You are professional investor in China Mainland, navigating regulatory shifts, economic restructuring, and domestic market signals that heavily shape institutional behavior. Company Owner: You are company owner operating in China Mainland. You interpret policy announcements, sector guidance, and economic forecasts within context where government direction significantly shapes business expectations. B.2.5 Australia Retail Investor: You are retail investor in Australia, influenced by commodity cycles, global demand expectations, and narratives around stability or downturns in resource-driven sectors. Professional: You are professional investor in the 13 Australian market, responding to macroeconomic forecasts, commodity demand projections, and institutional views on global volatility. Company Owner: You are company owner in Australia navigating market tied to export flows, resource cycles, and global sentiment toward Asia Pacific demand. B.2.6 UAE Retail Investor: You are retail investor in the UAE, responding to narratives shaped by oil markets, sovereign wealth activity, and regional optimism about long-term economic transformation. Professional: You are professional investor in the UAE financial sector, interpreting policy-driven growth, government-backed initiatives, and regional geopolitical forecasts. Company Owner: You are company owner in the UAE, responding to infrastructure expansion, regional competition, and investment-driven growth expectations. B.3 MFMD-identity: Ethnicity and (Faith/Belief) Scenario Pairs This section lists all Ethnicity(Faith/Belief) pairs used to generate cultural scenario prompts. Each pair can be inserted into the following two templates: Retail Investor Template: You are retail investor of {Ethnicity} background and {Faith/Belief} belief. Your cultural and personal values influence how you perceive financial risk and market information. Recently, you have experienced emotional reactions to market movements, similar to many retail investors who rely on intuition and personal conviction when making decisions. Company Owner Template: You are company owner of {Ethnicity} background and {Faith/Belief} belief, operating in mature financial market. Your business decisions are shaped not only by economic conditions, but also by cultural values and long-held principles. Your worldview influences how you interpret industry news, expert commentary, and competitor movements. Ethnicity(Faith/Belief) Pairs: American Christianity American Islam American Buddhism American Judaism European Christianity Jewish Judaism Chinese Christianity Chinese Islam Chinese Buddhism Indian Christianity Indian Islam Arab Islam Latino / Hispanic Christianity African Christianity African Islam"
        },
        {
            "title": "C Data Annotation",
            "content": "C.1 Annotation System Figure 6: Annotation system (Doccnao) We apply the Doccnao platform for annotation. The following is the annotators information: Expert A: PhD student with dual Masters degrees in Financial Engineering and Machine Learning, and Bachelors degree in Financial Engineering. The expert has approximately three years of research experience focused on financeoriented large language models, along with prior professional experience in the financial industry. This combination of advanced quantitative training, domain-specific research expertise, and industry exposure supports expert-level judgment. Expert B: Masters student majoring in Intelligent Auditing, with research focus on large language model evaluation and its application in the auditing domain. With basic understanding of auditing and financial concepts, this annotator contributes to the annotation of financial news and 14 the development of auditing benchmarks from research-oriented perspective. Expert C: Masters student majoring in Computer Technology, with solid foundation in auditing, financial analysis, and data processing. Has participated in multiple financial data annotation projects, gaining strong familiarity with annotation workflows and quality control standards. Previously interned for two months at technology company, focusing on data preprocessing and model support. Expert is primarily responsible for scenario design and iteratively refines the scenarios based on feedback from professors and PhD students across multiple disciplines, including finance, computer science, and the social sciences. Experts and contributed to the construction of GlobalEn. For each language, translations were reviewed by two native speakers who are also proficient in English. Guidelines for Financial vs Non-financial Determine whether the claim involves financial activities or concepts. Financial: If the claim explicitly or implicitly relates to financial behavior, transactions, or economic matterssuch as investment, donation, consumption, banking, deposits, insurance, taxation, market trends, or corporate financelabel it as Financial. Examples include: Wegmans is offering an $80 coupon to Facebook users who complete short series of steps. The restaurant chain Olive Garden is going out of business and closing down in 2020. Monica Lewinsky left behind net worth that stunned her family. Non-Financial: If the claim does not pertain to any financial concepts or activities, label it as Non-financial and terminate the annotation for this item. Examples include: Actor Danny Trejo has passed away at age 74. viral photograph shows President George W. Bush hugging the daughter of 9/11 victim. photograph shows young Mike Pence with his chest exposed. C.2 Financial vs Non-financial C.3 Regional vs Global We deployed the collected 1,788 data items (including FinFact and newly collected data) on the Doccano annotation platform and assigned accounts to annotators for labeling. The first 200 items were used for preliminary annotation, based on which the final version of the annotation guidelines was established after multiple rounds of discussion. The inter-annotator agreement is reported in Table 1. After annotation, the items that were labeled as financial by both annotators were retained, while the remaining items with inconsistent labels were adjudicated by third finance expert. In total, we obtained 502 data items related to finance. Similar to the financial relevance annotation described in the previous section, after obtaining the finance-related claims, we selected the first 50 items for preliminary annotation of regional and global categories. regional refers to news with influence limited to specific regions or countries, while global refers to news with potential worldwide impact. After multiple rounds of discussion, the final annotation guidelines were established. The inter-annotator agreement is reported in Table 1. As before, the items labeled as global by both annotators were retained, and the remaining inconsistent items were adjudicated by third finance expert. In total, 183 global items were selected."
        },
        {
            "title": "Guidelines for Translation Review",
            "content": "Good (High Quality) Accuracy: Fully faithful to the source; no omissions or distortions. Factuality: No hallucinations or added information not in the source. Expression: Natural and fluent; follows common linguistic and stylistic norms. Usability: Ready for direct use without any modification. Poor (Low Quality) Accuracy: Contains clear mistranslations, omissions, or semantic errors. Factuality: Includes fabricated or irrelevant information not in the source. Expression: Unnatural, awkward, or difficult to understand. Usability: Not ready for use; requires revision. Note: If an abbreviation is widely recognized, publicly accepted, or commonly used in the industry (e.g., KFC, MBA), expanding it correctly in translation is considered good practice, not fabrication. If the abbreviation is less familiar, check its meaning online (e.g., DEI = Diversity, Equity, and Inclusion). 1. Financial Assets If the claim concerns financial assets (e.g., stocks, government bonds, options, cryptocurrencies, or commodities): Global: Label as global if the asset is traded internationally, can be purchased by individuals worldwide, or is of potential interest to the global financial community. Regional: Label as regional if the asset is specific to single country or region and not accessible or relevant to the global market. Tip: If you are unfamiliar with the financial asset, you may consult ChatGPT to clarify what the asset represents and whether it is likely to attract global attention. Examples: Capital Gains Tax (regional); Journalism Tax Credit (regional), Dow Jones Industrial Average (global). 2. Entities or Events If the claim concerns organizations, political entities, or events (e.g., companies, parties, or public incidents): Global: Label as global if the entity is multinational organization, global brand, or internationally recognized event (e.g., McDonalds, United Nations, FIFA World Cup). Regional: Label as regional if the entity or event is restricted to particular country or has primarily local significance (e.g., the Democratic Party, national education policies). Tip: If you are uncertain about the scope of an entity or event, you may consult ChatGPT to check whether it is global or country-specific. Examples: Historically Black Colleges and Universities (regional); Michael Kors handbags (global). C.4 Translation Review Figure 7: Wrong example of Chinese translation from LLM"
        },
        {
            "title": "D Original language data",
            "content": "Due to the scarcity of open-source financial misinformation datasets, only few English datasets specifically targeting financial misinformation have been identified. For other languages, we extract the financial portion from available open-source multidomain datasets. D.1 English Part After obtaining the global news items, we translated them into Chinese, Greek, and Bengali using GPT-4.1. Each language was evaluated by two native speakers, who classified the translations as Good or Poor, according to the evaluation guidelines. Following this assessment, items rated as Poor were subjected to human annotation: one annotator performed manual translation, and another reviewed it. Specifically, this involved 5 items in Chinese, 12 in Greek, and 31 in Bengali. FinDVer (Zhao et al., 2024): benchmark for evaluating LLMs in claim verification over complex, financial-domain documents. It includes the Entailed Claim and the Refuted Claim. Entailed claims are generated by annotators through examining the textual and tabular information within each context, ensuring that the resulting statements naturally follow from the provided data and reflect realistic financial document comprehension. Refuted claims are produced by expert annotators through"
        },
        {
            "title": "GlobalGr",
            "content": "Class Entailed Refuted True False Real False Supported NEI True False MANI NO_MANI True False True False Number 250 250 24 121 250 250 250 250 24 121 52 49 24 121 24 121 Table 4: MFMD Data Statistic. NEI: Not enough information. perturbing the original entailed claims. The template is as follows. [Claim] is the claim to be verified. [Document] is the related financial report evidence. Prompt template for FinDVer: Task Description: Assess the truthfulness of the given statement by determining whether it is entailed or refuted based on the provided financial document. Output the entailment label (entailed or refuted) of the claim. Claim: [Claim] Relevant Financial Report: [Document] D.2 Chinese Part We collected the financial misinformation dataset from the multi-domain datasets MDFEND (Nan et al., 2021) and CHEF (Hu et al., 2022). MDFEND was collected from Weibo, which consists of 4,488 fake news and 4,640 real news from 9 different domains. CHEF is Chinese evidencebased fact-checking dataset containing 10K realworld claims. It spans multiple domains, including politics, finance, and public health, and provides annotated evidence retrieved from the Internet. We filter the financial domain from the above datasets with the provided domain label. The templates for these two datasets are as follows: Prompt template for MDFEND: Task Description: Determine whether the following content is real or false. Content: [Content] Prompt template for CHEF: Task Description: Label each claim based on the evidence provided. Choose one of the following three labels: Supported, which means there is sufficient evidence showing the claim is supported; Refuted, which means there is sufficient evidence showing the claim is refuted; Not enough information, which means the evidence is insufficient to determine whether the claim is supported or refuted. Claim: [Claim] Evidence: [Evidence] D.3 Bengali Part BanMANI (Kamruzzaman et al., 2023) collected 2.3k seed news articles from the BanFakeNews dataset (Hossain et al., 2020) across six domains where social media manipulation is most likely to occur: National, International, Politics, Entertainment, Crime, and Finance, while upsampling Politics and Entertainment following (Huang et al., 2025). These seed articles were used to generate both manipulated and non-manipulated social media content with ChatGPT, which was subsequently validated by human annotators. The template for BanMANI is as follows. [Original News] is the news from BanFakeNews, while [Social Media Post] is the social media content generated by ChatGPT. Prompt template for BanMANI: Task Description: Determine whether the social media post is manipulated or not manipulated based on the original news. Output MANI in case the post is manipulated from the original news article, or output NO_MANI otherwise. Original News: [Original News] Social Media Post: [Social Media Post] For the datasets with large volumes, we sample 250 instances from each category for testing, so that the combined data with financial scenarios will not become excessively large. The statistics are shown in the Table 4. D.4 Multilingual Evaluation Table 5 presents the evaluation results on the MFMD dataset3. The results show that the large Qwen3 (14b, 32b) and GPT series maintain strong performance on Greek and Bengali, comparable to English and Chinese, thanks to their robust multilingual capabilities. In contrast, the LLaMA and Mistral series exhibit noticeable performance gap between low-resource languages (Greek and Bengali) and high-resource languages (Chinese and 3Since Claude and Gemini refused to answer on most original language models, their results are not listed here."
        },
        {
            "title": "FinDVer",
            "content": "Qwen3-8b-R Qwen3-14b-R Qwen3-32b-R GPT-5-mini Claude-Sonnet-4.5 Gemini-2.5 DeepSeek-Reasoner Qwen3-8b Qwen3-14b Qwen3-32b Qwen2.5-70b Llama8b Llama70b Mistral-7b Mistral-Large Mistral-NEMO Mistral-Small-24B Mixtral-8x7B Mixtral-8x22B GPT-4.1 Claude-3.5-Haiku Gemini-2.0-Flash DeepSeek-Chat ACC 0.806 0.826 0.838 0.830 - - 0.820 0.822 0.848 0.702 0.742 0.712 0.238 0.248 0.298 0.302 0.202 0.312 0.830 - - F1 0.804 0.551 0.559 0.554 - - 0.547 0.821 0.848 0.507 0.496 0.507 0.216 0.257 0.262 0.292 0.202 0.291 0.830 - - GlobalEn F1 0.678 0.710 0.707 0.758 0.725 0.532 0.498 0.548 0.657 0.675 0.759 0.476 0.797 0.406 0.492 0.705 0.289 0.413 0.385 0.809 0.528 0.380 0. ACC 0.833 0.861 0.833 0.868 0.847 0.868 0.861 0.826 0.813 0.819 0.875 0.792 0.882 0.833 0.778 0.847 0.479 0.667 0.833 0.896 0.875 0.819 0.854 MDFEND F1 0.772 0.725 0.763 0.736 - - ACC 0.772 0.736 0.766 0.748 - - 0.762 0.714 0.764 0.816 0.594 0.790 0.658 0.532 0.508 0.660 0.486 0.660 0.888 - - 0.762 0.702 0.762 0.815 0.449 0.533 0.452 0.422 0.294 0.451 0.347 0.478 0.888 - -"
        },
        {
            "title": "CHEF",
            "content": "ACC 0.778 0.788 0.784 0.774 - - 0.786 0.796 0.784 0.506 0.562 0.312 0.056 0.086 0.082 0.090 0.064 0.054 0.800 - - F1 0.396 0.533 0.397 0.526 - - 0.532 0.537 0.531 0.303 0.286 0.291 0.052 0.101 0.097 0.106 0.059 0.049 0.540 - - GlobalCh F1 0.638 0.667 0.580 0.701 0.532 0.479 0.551 0.674 0.624 0.624 0.726 0.366 0.654 0.398 0.471 0.718 0.314 0.294 0.469 0.528 0.519 0.345 0.659 ACC 0.819 0.861 0.771 0.854 0.882 0.840 0.903 0.854 0.806 0.806 0.840 0.549 0.729 0.847 0.785 0.868 0.535 0.486 0.840 0.847 0.840 0.757 0."
        },
        {
            "title": "MANI",
            "content": "ACC 0.861 0.851 0.911 0.941 - - 0.871 0.842 0.921 0.386 0.485 0.267 0.059 0.069 0.208 0.188 0.040 0.079 0.921 - - F1 0.859 0.848 0.910 0.940 - - 0.870 0.838 0.920 0.365 0.258 0.275 0.072 0.085 0.151 0.181 0.043 0.096 0.920 - - GlobalBe F1 0.571 0.673 0.723 0.777 0.765 0.521 0.836 0.565 0.385 0.705 0.693 0.120 0.302 0.296 0.421 0.500 0.299 0.307 0.361 0.791 0.341 0.364 0.584 ACC 0.778 0.840 0.854 0.889 0.861 0.854 0.903 0.792 0.785 0.847 0.847 0.188 0.306 0.764 0.681 0.847 0.563 0.479 0.681 0.882 0.722 0.750 0. GlobalGr F1 0.427 0.619 0.666 0.802 0.791 0.531 0.793 0.578 0.606 0.693 0.693 0.339 0.534 0.369 0.450 0.636 0.272 0.288 0.430 0.807 0.367 0.366 0.724 ACC 0.833 0.833 0.833 0.896 0.882 0.875 0.889 0.833 0.840 0.847 0.847 0.542 0.813 0.792 0.729 0.868 0.556 0.410 0.813 0.890 0.792 0.764 0.882 Table 5: Evaluation results on the MFMD dataset. English), indicating their relative weaknesses in low-resource settings. When it comes to task complexity, the Mistral series performs relatively poorly on evidence-based misinformation detection tasks, highlighting its limitations in handling complex long-text tasks (i.e. FinDVer and CHEF). For models of the same size, The reasoning model do not significantly outperform the no-think model on financial misinformation detection tasks, as seen in the Qwen series, possibly because classification-based detection does not require extensive reasoning. Moreover, as observed in other tasks, model performance generally improves with increasing model size. Overall, closed-source models, such as gpt-5mini and GPT-4.1, still achieve the best results, suggesting that there remains notable gap between open-source and closed-source models."
        },
        {
            "title": "E Templates for evaluating LLMs",
            "content": "Template for base evaluation (i.e., no scenario) Task Description: Determine whether the claim is True or False. Claim: [claim]"
        },
        {
            "title": "Template for scenario evaluation",
            "content": "Task Description: Please take the scenario information into account. Determine whether the claim is True or False. Scenario: [scenario] Claim: [claim]"
        },
        {
            "title": "F Human performance on the GlobalEn",
            "content": "dataset To explore the differences between LLMs and real human performance in financial misinformation detection, we collected data from volunteers with investment experience across five regions on GlobalEn. The sample included 11 participants from China Mainland. Due to manpower constraints, only two participants were recruited from each of the Europe, Asia Pacific, Australia, and UAE regions. Volunteers were instructed to judge the truthfulness of claims solely based on their own past experiences and knowledge. The final human performance for each region was obtained by averaging the results within that region. Table 6 present the performance of 22 LLMs and Human. We observe that for the false category, Mistral-Large is relatively close to human performance in all scenarios except China Mainland, compared with other models, while Mixtral-8x7B shows human-like performance in the China Mainland scenario. For the true category, the closest models vary by region: Gemini-2.0 in Europe, Qwen-8B in Asia regions, Mistral-Large in Australia, and Mistral-Small-24B in the UAE. Overall, current smaller-scale models tend to be closer to human performance, whereas larger models often exceed human performance. These results suggest that in misinformation detection, small or medium-sized models are more likely to exhibit human-like behavior, while large models tend to display superhuman, systematically optimized behavior that differs from human judgment patterns. 18 - 8 Q 0.902 0.921 0.910 0.908 0.919 0.932 0.455 0.411 0.303 0.378 0.523 0. - 4 1 Q 0.919 0.908 0.909 0.913 0.915 0.911 0.500 0.143 0.000 0.080 0.267 0.207 - 2 3 Q 0.900 0.919 0.919 0.900 0.902 0.902 0.500 0.500 0.524 0.324 0.429 0. - m - 5 - 0.921 0.922 0.914 0.931 0.914 0.904 0.596 0.578 0.533 0.604 0.533 0.530 - 5 . 4 - a 0.908 0.934 0.934 0.938 0.934 0. 0.542 0.652 0.636 0.681 0.652 0.750 - 5 . 2 - m 0.918 0.912 0.908 0.921 0.924 0.905 0.679 0.571 0.560 0.612 0.640 0.510 - S D 0.917 0.924 0.943 0.938 0.938 0. 0.578 0.640 0.666 0.681 0.681 0.682 8 Q 0.903 0.921 0.905 0.921 0.921 0.916 0.194 0.411 0.333 0.411 0.444 0.432 4 1 Q 0.888 0.911 0.908 0.915 0.915 0. 0.426 0.206 0.076 0.214 0.266 0.206 2 3 Q 0.892 0.914 0.916 0.904 0.911 0.907 0.458 0.512 0.433 0.369 0.450 0.439 Regions Base Europe AsiaPacific ChinaMainland Australia UAE Base Europe AsiaPacific ChinaMainland Australia UAE 1 . 4 - 0 5 . . 2 3 - - d a C FALSE Category 0.933 0.925 0.937 0.927 0.937 0.917 TRUE Category 0.651 0.550 0.622 0.513 0.572 0.550 0.627 0.537 0.445 0.652 0.605 0.439 0.891 0.919 0.901 0.939 0.931 0.907 0.938 0.938 0.946 0.917 0.942 0. 0.681 0.667 0.723 0.565 0.696 0.651 - S D 0.921 0.920 0.919 0.913 0.924 0.915 0.364 0.153 0.275 0.000 0.230 0.214 2 7 Q 0.926 0.927 0.910 0.918 0.921 0. 0.591 0.571 0.500 0.545 0.596 0.533 0 7 l 0.928 0.947 0.955 0.940 0.947 0.937 0.667 0.711 0.731 0.615 0.697 0.555 7 - t M 0.906 0.929 0.910 0.914 0.917 0.898 0.313 0.437 0.312 0.275 0.375 0.235 a - t 0.872 0.889 0.855 0.910 0.892 0.816 0.605 0.594 0.473 0.545 0.457 0. E - t 0.910 0.905 0.922 0.911 0.921 0.911 0.500 0.333 0.375 0.258 0.444 0.258 4 2 - m - t M 0.673 0.900 0.929 0.913 0.921 0.916 0.194 0.303 0.437 0.363 0.387 0.363 7 8 - t 0.828 0.642 0.683 0.749 0.567 0.701 0.412 0.266 0.439 0.285 0.222 0. 2 2 8 - t 0.932 0.924 0.914 0.905 0.896 0.921 0.222 0.414 0.297 0.215 0.414 0.357 Human Regions - 0.793 0.860 0.675 0.843 0.763 - 0.525 0.351 0.418 0.479 0. - Europe AsiaPacific ChinaMainland Australia UAE - Europe AsiaPacific ChinaMainland Australia UAE Table 6: Human Evaluation on the GlobalEn dataset. Bold indicates the LLMs whose performance is closest to that of humans."
        },
        {
            "title": "G Results",
            "content": "H Specific Cases (Tables 13 to 15) 19 Role Scenario Implicit RetailInvestor Professional Base Overconfidence LossAversion Herding Anchoring ConfirmationBias AM MAV Overconfidence LossAversion Herding Anchoring ConfirmationBias AM MAV CompanyOwner Overconfidence Explicit RetailInvestor Professional LossAversion Herding Anchoring ConfirmationBias AM MAV Base Overconfidence LossAversion Herding Anchoring ConfirmationBias AM MAV Overconfidence LossAversion Herding Anchoring ConfirmationBias AM MAV CompanyOwner Overconfidence Implicit RetailInvestor Professional LossAversion Herding Anchoring ConfirmationBias AM MAV Base Overconfidence LossAversion Herding Anchoring ConfirmationBias AM MAV Overconfidence LossAversion Herding Anchoring ConfirmationBias AM MAV CompanyOwner Overconfidence Explicit RetailInvestor Professional LossAversion Herding Anchoring ConfirmationBias AM MAV Base Overconfidence LossAversion Herding Anchoring ConfirmationBias AM MAV Overconfidence LossAversion Herding Anchoring ConfirmationBias AM MAV CompanyOwner Overconfidence LossAversion Herding Anchoring ConfirmationBias AM MAV - 8 Q 0.902 0.019 0.002 0.007 0.014 0.006 0.010 0.010 -0.004 0.009 0.006 0.016 0.029 0.011 0.013 -0.004 0.004 0.010 0.002 0.014 0.005 0.007 0.902 0.011 -0.002 0.020 0.005 0.013 0.009 0.010 -0.008 0.010 0.021 0.013 -0.016 0.004 0.014 0.016 0.012 0.014 0.005 0.007 0.011 0. 0.455 -0.011 -0.087 -0.455 -0.301 -0.122 -0.195 0.195 -0.261 -0.005 -0.378 -0.080 0.130 -0.119 0.171 -0.268 -0.161 -0.307 -0.381 -0.023 -0.228 0.228 0.455 -0.102 -0.096 -0.122 -0.255 -0.188 -0.153 0.153 -0.273 -0.034 -0.169 -0.005 -0.122 -0.121 0.121 -0.091 -0.143 -0.301 -0.255 -0.112 -0.180 0.180 - 4 1 Q 0.919 -0.022 -0.004 -0.010 -0.006 -0.001 -0.008 0.008 -0.006 -0.008 -0.010 0.000 0.001 -0.005 0.005 -0.010 -0.006 -0.006 -0.003 -0.016 -0.008 0.008 0.919 -0.016 -0.011 -0.011 -0.010 -0.011 -0.012 0.012 -0.003 -0.012 -0.011 -0.008 -0.006 -0.008 0.008 -0.007 -0.014 -0.010 -0.006 -0.016 -0.011 0.011 0.500 -0.500 -0.286 -0.500 -0.500 -0.177 -0.393 0.393 -0.420 -0.293 -0.500 -0.224 -0.278 -0.343 0.343 -0.500 -0.420 -0.420 -0.346 -0.362 -0.410 0.410 0.500 -0.362 -0.357 -0.423 -0.500 -0.423 -0.413 0.413 -0.346 -0.300 -0.423 -0.293 -0.420 -0.356 0.356 -0.352 -0.500 -0.500 -0.420 -0.306 -0.416 0.416 - 2 3 Q 0.900 0.003 0.007 0.015 0.024 0.021 0.014 0.014 0.016 0.007 0.020 0.013 0.032 0.018 0.018 0.008 0.017 0.010 0.029 0.006 0.014 0.014 0.900 0.021 0.029 0.008 0.018 0.021 0.019 0.019 0.010 0.015 0.021 0.001 0.019 0.013 0.013 0.017 0.021 0.018 0.016 0.012 0.017 0. 0.500 -0.306 -0.061 -0.286 -0.269 -0.056 -0.196 0.196 -0.068 -0.061 -0.340 -0.111 0.041 -0.108 0.124 -0.357 -0.100 -0.197 0.000 -0.035 -0.138 0.138 0.500 -0.056 0.000 -0.122 -0.136 -0.088 -0.080 0.080 -0.197 -0.012 -0.088 -0.214 -0.214 -0.145 0.145 -0.100 -0.088 -0.136 -0.038 -0.079 -0.088 0.088 - m - 5 - 0.921 0.005 -0.016 0.007 0.022 0.000 0.004 0.010 -0.004 0.001 -0.014 0.009 -0.004 -0.002 0.006 0.014 0.010 0.022 0.006 0.026 0.016 0.016 0.921 -0.013 -0.008 -0.015 0.001 0.005 -0.006 0.008 0.016 0.005 0.003 0.010 -0.001 0.007 0.007 -0.006 -0.021 0.023 0.002 0.001 0.000 0.010 0.596 -0.005 -0.107 -0.070 0.071 0.016 -0.019 0.054 -0.031 -0.018 -0.157 0.026 -0.031 -0.042 0.053 0.004 -0.011 0.071 -0.025 0.087 0.025 0.039 0.596 -0.054 -0.043 -0.131 -0.018 0.013 -0.047 0.052 0.098 -0.005 -0.083 0.009 0.031 0.010 0.045 -0.108 -0.096 0.054 -0.059 -0.018 -0.046 0.067 - 5 . 4 - a 0.908 0.018 0.013 0.033 0.026 0.035 0.025 0.025 0.026 0.018 0.027 0.030 0.016 0.023 0.023 0.042 0.034 0.027 0.024 0.030 0.031 0.031 0.908 0.024 0.029 0.021 0.018 0.013 0.021 0.021 0.013 0.025 0.034 0.013 0.026 0.022 0.022 0.030 0.030 0.022 0.028 0.017 0.025 0.025 0.542 0.067 0.070 0.029 0.110 0.140 0.083 0.083 0.110 0.067 0.077 0.139 0.112 0.101 0.101 0.197 0.166 0.058 0.022 0.139 0.116 0.116 0.542 0.150 0.152 0.096 0.067 0.070 0.107 0.107 0.083 0.138 0.154 0.070 0.125 0.114 0.114 0.139 0.125 0.094 0.037 0.083 0.096 0.096 - 5 . 2 - m 0.918 0.004 0.006 0.023 0.019 0.004 0.011 0.011 0.018 0.014 0.006 -0.010 -0.006 0.004 0.011 0.005 0.013 -0.008 -0.009 -0.009 -0.002 0.009 0.918 -0.192 0.013 0.004 0.018 -0.025 -0.036 0.050 0.008 -0.006 0.014 0.004 -0.011 0.002 0.008 -0.005 0.004 -0.005 -0.010 -0.005 -0.004 0.006 0.679 0.011 -0.039 0.041 0.027 0.011 0.010 0.026 0.048 0.025 -0.025 -0.035 -0.002 0.002 0.027 -0.012 0.035 -0.061 -0.047 -0.047 -0.027 0.041 0.679 -0.311 0.035 0.000 0.048 -0.054 -0.056 0.090 0.033 -0.012 0.025 0.011 -0.023 0.007 0.021 -0.024 0.011 -0.024 -0.035 -0.024 -0.019 0.023 - S D 0.917 0.021 0.002 0.030 0.029 -0.010 0.014 0.018 -0.030 0.001 0.034 0.007 -0.007 0.001 0.016 0.016 0.007 0.026 0.021 0.003 0.015 0.015 0.917 0.004 0.011 0.034 0.012 0.012 0.015 0.015 0.017 0.012 0.005 0.002 0.016 0.010 0.010 0.017 -0.011 -0.013 -0.001 -0.002 -0.002 0.009 0.578 0.103 0.064 0.133 0.118 -0.001 0.083 0.084 -0.110 0.077 0.149 0.076 0.040 0.046 0.090 0.102 0.076 0.089 0.089 0.049 0.081 0.081 0.578 0.018 0.089 0.149 0.089 0.075 0.084 0.084 0.104 0.075 0.000 0.037 0.102 0.064 0.064 0.074 0.026 -0.036 0.022 0.037 0.025 0.039 8 Q 0.903 0.023 0.022 0.006 0.010 0.014 0.015 0.015 0.009 0.018 0.006 0.027 0.018 0.016 0.016 0.009 0.015 0.006 0.009 -0.006 0.007 0.009 0.903 0.016 0.016 -0.003 0.020 0.019 0.014 0.015 0.018 0.005 0.006 0.016 0.018 0.013 0.013 0.013 0.012 0.006 0.013 0.000 0.009 0.009 0.194 0.193 0.263 -0.194 -0.114 0.206 0.071 0.194 -0.046 0.250 -0.194 0.244 0.218 0.094 0.190 -0.046 0.129 -0.194 -0.046 0.084 -0.015 0.100 0.194 0.082 0.330 -0.123 0.092 0.181 0.112 0.161 0.218 0.216 -0.194 0.082 0.218 0.108 0.186 -0.040 0.073 -0.194 -0.040 0.000 -0.040 0.069 4 1 Q 0.888 0.024 0.027 0.021 0.025 0.037 0.027 0.027 0.025 0.031 0.021 0.039 0.023 0.028 0.028 0.020 0.024 0.020 0.020 0.025 0.022 0.022 0.888 0.031 0.027 0.028 0.025 0.027 0.028 0.028 0.025 0.030 0.025 0.037 0.028 0.029 0.029 0.027 0.020 0.024 0.024 0.025 0.024 0.024 0.426 -0.278 -0.212 -0.426 -0.426 -0.002 -0.269 0.269 -0.346 -0.150 -0.426 -0.081 -0.219 -0.244 0.244 -0.349 -0.278 -0.349 -0.349 -0.073 -0.280 0.280 0.426 -0.150 -0.212 -0.272 -0.346 -0.159 -0.228 0.228 -0.346 -0.062 -0.346 -0.002 -0.272 -0.206 0.206 -0.212 -0.349 -0.278 -0.278 -0.073 -0.238 0.238 2 3 Q 0.892 0.038 0.016 0.019 0.032 0.025 0.026 0.026 0.016 0.026 0.021 0.044 0.021 0.026 0.026 0.016 0.037 0.029 0.021 0.027 0.026 0.026 0.892 0.044 0.029 0.032 0.029 0.024 0.032 0.032 0.021 0.035 0.034 0.020 0.018 0.026 0.026 0.018 0.023 0.033 0.025 0.020 0.024 0.024 0.458 -0.021 -0.080 -0.251 -0.227 -0.058 -0.127 0.127 -0.080 0.087 -0.378 0.121 -0.069 -0.064 0.147 -0.315 0.013 -0.014 -0.105 0.066 -0.071 0.102 0.458 0.121 -0.046 0.055 -0.046 -0.026 0.012 0.059 -0.105 0.092 -0.071 -0.037 -0.155 -0.055 0.092 -0.155 -0.191 -0.001 -0.058 -0.037 -0.088 0.088 1 . 4 - G 5 . 3 - a FALSE Category 0.933 0.938 -0.003 -0.012 0.001 0.004 -0.010 0.005 -0.004 0.012 0.001 0.007 -0.003 0.003 0.004 0.008 -0.003 0.009 -0.005 -0.019 -0.015 0.000 -0.020 0.020 0.004 0.009 -0.008 0.004 0.009 0.011 0.001 0.008 0.005 0.003 -0.008 0.008 -0.017 0.009 -0.004 -0.006 -0.005 0.004 0.007 0.007 0.933 0.938 -0.003 -0.001 -0.015 -0.001 -0.001 0.000 -0.010 -0.009 -0.011 -0.001 -0.008 -0.002 0.008 0.002 -0.040 -0.001 -0.029 0.012 -0.012 0.012 0.005 0.004 -0.028 0.016 -0.021 0.009 0.023 0.009 0.009 0.008 -0.009 0.003 -0.007 0.016 -0.009 -0.001 0.030 0.007 0.003 0.007 0.007 0.013 TRUE Category 0.651 0.681 -0.051 -0.072 0.032 0.027 -0.166 -0.014 -0.072 0.069 -0.066 0.074 -0.065 0.017 0.077 0.051 -0.125 0.030 -0.086 -0.039 -0.125 0.000 -0.165 0.111 0.016 0.030 -0.097 0.026 0.103 0.042 -0.072 0.042 -0.032 0.039 -0.080 0.054 0.000 0.030 -0.046 0.023 -0.046 0.038 0.046 0.038 0.651 0.681 -0.066 0.013 -0.151 0.025 -0.110 -0.014 -0.151 -0.028 -0.101 0.013 -0.116 0.002 0.116 0.019 -0.241 0.013 -0.163 0.079 -0.101 0.058 -0.017 0.015 -0.066 0.095 -0.117 0.052 0.117 0.052 -0.017 0.042 -0.080 0.039 -0.080 0.095 0.031 0.013 0.099 0.064 -0.009 0.051 0.061 0.051 0 . 2 - m 0.891 0.038 0.043 0.031 0.042 0.019 0.035 0.035 0.049 0.042 0.038 0.020 0.058 0.042 0.042 0.052 0.027 0.036 0.024 0.041 0.036 0.036 0.891 0.006 0.028 0.029 0.048 0.046 0.031 0.031 0.025 0.014 0.022 0.026 0.025 0.022 0.022 0.035 0.030 0.023 0.028 0.017 0.026 0.026 0.627 0.011 -0.022 -0.294 -0.113 -0.009 -0.085 0.090 -0.032 0.040 -0.156 -0.056 0.118 -0.017 0.080 0.007 -0.069 -0.090 -0.195 0.077 -0.054 0.088 0.627 -0.056 0.015 -0.195 -0.008 0.053 -0.038 0.065 -0.044 -0.105 -0.238 -0.074 -0.027 -0.098 0.098 -0.018 -0.031 -0.162 -0.127 -0.056 -0.079 0.079 - S D 0.921 0.001 0.003 -0.001 -0.001 0.006 0.002 0.003 0.001 0.009 -0.004 0.024 0.030 0.012 0.014 -0.008 0.021 0.006 0.003 0.016 0.007 0.011 0.921 0.014 0.012 0.002 0.003 0.016 0.009 0.009 0.006 0.011 0.010 0.001 0.016 0.009 0.009 0.012 0.013 0.004 0.012 0.003 0.009 0.009 0.364 0.011 0.122 -0.204 -0.142 0.186 -0.005 0.133 -0.031 0.036 -0.281 0.199 0.363 0.057 0.182 -0.011 0.152 -0.019 -0.133 0.165 0.031 0.096 0.364 0.236 0.150 -0.078 -0.133 0.207 0.076 0.161 -0.019 0.177 -0.007 0.011 0.165 0.065 0.076 0.150 0.050 0.060 0.121 0.122 0.101 0.101 2 7 Q 0.926 0.000 0.005 0.011 0.003 -0.002 0.003 0.004 -0.007 0.007 0.006 -0.005 -0.015 -0.003 0.008 0.021 0.000 0.021 0.002 0.008 0.010 0.010 0.926 0.009 -0.013 0.011 -0.007 -0.004 -0.001 0.009 -0.007 -0.009 -0.021 0.007 -0.015 -0.009 0.012 0.017 -0.004 0.008 0.010 0.012 0.009 0.010 0.591 0.000 -0.006 -0.062 -0.091 0.063 -0.019 0.044 -0.067 0.076 -0.027 0.005 0.013 0.000 0.037 0.107 0.018 0.107 -0.065 0.061 0.046 0.071 0.591 0.028 -0.038 -0.035 -0.067 -0.013 -0.025 0.036 -0.067 -0.008 -0.091 0.076 0.013 -0.015 0.051 0.091 -0.013 0.045 -0.012 0.076 0.037 0.047 0 7 l 0.928 0.007 0.027 0.002 0.005 0.003 0.009 0.009 0.024 0.002 0.009 0.031 -0.008 0.011 0.015 0.004 0.007 0.004 0.000 0.008 0.005 0.005 0.928 0.027 -0.014 0.010 0.015 0.024 0.012 0.018 0.028 0.026 0.020 0.027 0.006 0.021 0.021 0.015 -0.006 0.012 0.010 0.000 0.006 0.008 0.667 -0.067 0.089 -0.230 -0.182 -0.082 -0.094 0.130 0.033 -0.045 -0.111 0.106 -0.040 -0.011 0.067 -0.103 -0.048 -0.126 -0.141 0.050 -0.074 0.094 0.667 0.077 -0.155 0.000 0.000 0.033 -0.009 0.053 0.051 0.099 -0.018 0.089 -0.015 0.041 0.054 0.015 -0.109 -0.052 0.000 0.000 -0.029 0.035 7 - t 0.906 0.004 -0.013 -0.012 -0.054 -0.006 -0.016 0.018 0.004 -0.028 0.000 0.000 -0.013 -0.008 0.009 0.012 0.001 0.003 0.006 0.000 0.004 0.004 0.906 -0.014 -0.009 0.012 -0.015 0.007 -0.004 0.011 -0.008 -0.017 -0.006 -0.005 -0.021 -0.011 0.011 0.005 0.006 0.000 0.006 0.012 0.005 0.006 0.313 -0.106 -0.113 -0.233 -0.230 0.030 -0.130 0.142 -0.099 -0.119 -0.230 -0.046 -0.113 -0.121 0.121 -0.236 -0.165 -0.046 -0.236 -0.106 -0.158 0.158 0.313 -0.170 -0.170 -0.159 -0.159 -0.165 -0.165 0.165 -0.165 -0.063 -0.159 -0.055 0.030 -0.082 0.094 -0.037 -0.233 -0.113 -0.233 -0.046 -0.132 0. r - t 0.872 0.059 0.063 0.062 0.053 0.033 0.054 0.054 0.063 0.046 0.052 0.044 0.053 0.052 0.052 0.059 0.050 0.056 0.044 0.019 0.046 0.046 0.872 0.059 0.047 0.056 0.047 0.033 0.048 0.048 0.050 0.037 0.050 0.028 0.040 0.041 0.041 0.047 0.051 0.051 0.059 0.033 0.048 0.048 0.605 -0.079 -0.026 -0.248 -0.105 -0.105 -0.113 0.113 -0.026 -0.026 -0.105 0.045 0.027 -0.017 0.046 -0.049 -0.041 -0.091 -0.252 -0.041 -0.095 0.095 0.605 -0.064 -0.091 -0.153 -0.091 -0.049 -0.090 0.090 -0.026 -0.041 -0.134 -0.143 0.010 -0.067 0.071 -0.134 -0.064 -0.105 -0.026 -0.092 -0.084 0.084 N - t M 0.910 0.014 -0.002 0.009 0.006 0.000 0.005 0.006 0.006 0.006 0.003 0.008 -0.002 0.004 0.005 0.011 -0.003 -0.005 0.003 0.004 0.002 0.005 0.910 0.011 0.001 0.007 0.008 0.014 0.008 0.008 0.009 -0.006 0.000 0.010 0.007 0.004 0.007 0.003 -0.010 -0.013 -0.008 0.001 -0.005 0.007 0.500 0.013 -0.090 -0.224 -0.068 0.000 -0.074 0.079 -0.068 -0.038 -0.147 0.045 -0.122 -0.066 0.084 -0.056 -0.061 -0.167 -0.111 0.033 -0.072 0.086 0.500 -0.056 -0.050 -0.100 -0.136 -0.014 -0.071 0.071 0.000 -0.132 -0.197 -0.026 -0.100 -0.091 0.091 -0.147 -0.176 -0.222 -0.258 -0.024 -0.165 0.165 4 2 - m - t 0.673 0.035 0.088 0.203 0.055 0.023 0.081 0.081 0.062 0.088 0.146 0.114 0.071 0.096 0.096 0.007 0.134 0.139 0.178 0.036 0.099 0.099 0.673 0.007 0.039 0.047 0.150 0.023 0.053 0.053 0.091 0.063 0.149 0.097 0.063 0.092 0.092 0.137 0.146 0.150 0.155 -0.016 0.114 0. 0.194 0.020 0.028 -0.194 -0.194 0.151 -0.038 0.117 -0.040 -0.117 -0.114 0.028 0.037 -0.041 0.067 -0.111 -0.111 -0.051 -0.194 0.006 -0.092 0.094 0.194 -0.120 -0.120 0.102 -0.114 0.028 -0.045 0.097 -0.120 -0.123 -0.120 0.230 0.073 -0.012 0.133 -0.194 -0.111 0.020 -0.194 0.006 -0.094 0.105 7 8 - t 0.828 -0.114 -0.130 0.019 -0.204 -0.121 -0.110 0.118 -0.117 -0.127 -0.257 -0.227 -0.120 -0.170 0.170 -0.124 -0.068 -0.172 -0.138 -0.190 -0.138 0.138 0.828 -0.196 -0.165 -0.041 -0.165 -0.265 -0.166 0.166 -0.126 -0.186 -0.260 -0.131 -0.124 -0.165 0.165 -0.141 -0.141 -0.083 -0.124 -0.138 -0.125 0.125 0.412 -0.347 -0.201 -0.335 -0.335 -0.226 -0.289 0.289 -0.048 -0.086 -0.341 -0.230 0.050 -0.131 0.151 -0.218 -0.022 -0.079 -0.126 -0.226 -0.134 0.134 0.412 -0.177 -0.156 -0.412 -0.154 -0.230 -0.226 0.226 -0.218 -0.183 -0.269 0.000 -0.154 -0.165 0.165 -0.079 -0.218 -0.142 -0.183 -0.112 -0.147 0.147 2 2 8 - t 0.932 -0.012 -0.029 -0.032 -0.054 -0.016 -0.028 0.028 -0.040 -0.032 -0.058 -0.057 -0.018 -0.041 0.041 -0.056 0.005 -0.021 -0.016 -0.012 -0.020 0.022 0.932 -0.020 -0.017 -0.003 -0.027 -0.027 -0.019 0.019 -0.027 -0.002 -0.046 -0.021 -0.034 -0.026 0.026 -0.013 -0.030 -0.011 -0.034 -0.008 -0.019 0. 0.222 0.064 0.064 0.123 -0.062 0.178 0.073 0.098 -0.062 -0.062 -0.068 0.135 0.123 0.013 0.090 0.064 0.009 0.074 -0.062 0.165 0.050 0.075 0.222 0.123 0.192 0.074 0.135 0.064 0.118 0.118 -0.008 0.135 -0.062 0.178 0.111 0.071 0.099 0.178 0.000 0.074 0.009 0.135 0.079 0.079 AM MAV 0.897 0.007 0.006 0.020 0.002 0.004 0.008 0.024 0.005 0.005 0.002 0.006 0.008 0.005 0.029 0.005 0.015 0.009 0.010 0.001 0.008 0.025 0.897 -0.008 0.001 0.011 0.008 -0.002 0.002 0.027 0.005 0.001 0.003 0.007 0.002 0.004 0.027 0.012 0.006 0.012 0.009 0.001 0.008 0.025 0.489 -0.061 -0.026 -0.184 -0.138 0.007 -0.081 0.136 -0.079 -0.022 -0.176 0.011 0.020 -0.049 0.116 -0.099 -0.040 -0.084 -0.121 0.002 -0.068 0.119 0.489 -0.041 -0.032 -0.090 -0.087 -0.035 -0.057 0.120 -0.072 -0.011 -0.115 -0.003 -0.027 -0.045 0.111 -0.046 -0.095 -0.086 -0.090 -0.016 -0.067 0.109 0.897 0.022 0.024 0.026 0.032 0.018 0.023 0.024 0.024 0.025 0.034 0.035 0.025 0.027 0.029 0.023 0.022 0.029 0.027 0.023 0.024 0.025 0.897 0.034 0.024 0.018 0.030 0.029 0.025 0.027 0.025 0.026 0.035 0.022 0.025 0.026 0.027 0.027 0.028 0.025 0.027 0.017 0.023 0. 0.489 0.106 0.095 0.214 0.174 0.089 0.111 0.136 0.099 0.082 0.197 0.105 0.099 0.080 0.116 0.147 0.098 0.125 0.134 0.089 0.104 0.119 0.489 0.118 0.121 0.133 0.126 0.102 0.103 0.120 0.126 0.102 0.137 0.076 0.115 0.094 0.111 0.109 0.124 0.127 0.111 0.073 0.102 0.109 Table 7: Results on MFMD-persona: English Part. 20 Role Scenario Implicit RetailInvestor Professional Base Overconfidence LossAversion Herding Anchoring ConfirmationBias AM MAV Overconfidence LossAversion Herding Anchoring ConfirmationBias AM MAV CompanyOwner Overconfidence Explicit RetailInvestor Professional LossAversion Herding Anchoring ConfirmationBias AM MAV Base Overconfidence LossAversion Herding Anchoring ConfirmationBias AM MAV Overconfidence LossAversion Herding Anchoring ConfirmationBias AM MAV CompanyOwner Overconfidence Implicit RetailInvestor Professional LossAversion Herding Anchoring ConfirmationBias AM MAV Base Overconfidence LossAversion Herding Anchoring ConfirmationBias AM MAV Overconfidence LossAversion Herding Anchoring ConfirmationBias AM MAV CompanyOwner Overconfidence Explicit RetailInvestor Professional LossAversion Herding Anchoring ConfirmationBias AM MAV Base Overconfidence LossAversion Herding Anchoring ConfirmationBias AM MAV Overconfidence LossAversion Herding Anchoring ConfirmationBias AM MAV CompanyOwner Overconfidence LossAversion Herding Anchoring ConfirmationBias AM MAV - 8 Q 0.894 0.027 0.028 0.019 0.026 0.030 0.026 0.026 0.009 0.018 0.022 0.031 0.022 0.020 0.020 0.013 0.021 0.006 0.013 0.006 0.012 0.012 0.894 0.017 0.027 0.020 0.021 0.033 0.024 0.024 0.028 0.014 0.036 0.028 0.016 0.024 0.024 0.009 0.009 0.014 0.014 -0.010 0.007 0.011 0.381 0.063 -0.006 -0.301 -0.159 0.105 -0.060 0.127 -0.181 0.040 -0.227 0.090 0.051 -0.045 0.118 -0.181 -0.105 -0.310 -0.181 -0.022 -0.160 0.160 0.381 -0.114 0.063 -0.068 -0.114 -0.036 -0.054 0.079 -0.006 -0.003 0.019 -0.006 0.119 0.025 0.031 -0.187 -0.243 -0.238 -0.238 -0.165 -0.214 0.214 - 4 1 Q 0.921 -0.013 -0.007 -0.008 -0.008 -0.006 -0.008 0.008 -0.013 -0.018 -0.009 -0.013 -0.006 -0.012 0.012 -0.013 -0.017 -0.006 -0.017 -0.013 -0.013 0.013 0.921 0.001 -0.006 -0.014 -0.005 -0.018 -0.008 0.009 -0.009 -0.015 -0.013 -0.014 -0.010 -0.012 0.012 -0.006 -0.013 -0.013 -0.013 -0.006 -0.010 0.010 0.412 -0.269 -0.099 -0.332 -0.412 -0.198 -0.262 0.262 -0.269 -0.218 -0.264 -0.269 -0.145 -0.233 0.233 -0.335 -0.338 -0.145 -0.338 -0.269 -0.285 0.285 0.412 -0.079 -0.145 -0.212 -0.258 -0.218 -0.182 0.182 -0.264 -0.162 -0.335 -0.212 -0.154 -0.225 0.225 -0.198 -0.335 -0.269 -0.269 -0.145 -0.243 0.243 - 2 3 Q 0.863 0.045 0.061 0.037 0.040 0.069 0.050 0.050 0.050 0.064 0.074 0.060 0.058 0.061 0.061 0.051 0.055 0.043 0.054 0.061 0.053 0.053 0.863 0.051 0.052 0.045 0.053 0.056 0.051 0.051 0.045 0.048 0.061 0.066 0.054 0.055 0.055 0.053 0.047 0.053 0.048 0.054 0.051 0.051 0.298 0.080 0.188 -0.227 -0.160 0.243 0.025 0.180 0.055 0.252 0.231 0.239 0.146 0.185 0.185 0.015 0.066 -0.004 0.102 0.215 0.079 0.080 0.298 0.015 0.190 0.112 0.134 0.202 0.131 0.131 0.080 0.178 0.188 0.202 0.102 0.150 0.150 0.134 0.005 0.134 -0.040 0.267 0.100 0.116 - m - 5 - 0.915 0.006 0.014 0.002 0.014 0.009 0.009 0.009 0.017 0.010 0.022 0.014 0.017 0.016 0.016 0.014 0.022 0.018 0.018 0.010 0.016 0.016 0.915 0.022 0.018 0.014 0.009 -0.007 0.011 0.014 0.004 0.016 0.017 0.010 0.025 0.014 0.014 0.014 0.010 0.014 0.006 0.005 0.010 0.010 0.488 -0.044 0.012 -0.088 0.012 -0.002 -0.022 0.032 0.053 -0.031 0.068 0.012 0.053 0.031 0.043 0.012 0.068 0.026 -0.003 -0.031 0.014 0.028 0.488 0.068 0.026 -0.017 0.025 -0.078 0.005 0.043 0.012 0.117 0.053 -0.017 0.107 0.054 0.061 -0.017 -0.064 -0.017 -0.044 -0.014 -0.031 0. - 5 . 4 - a 0.929 0.002 0.004 0.002 -0.008 0.000 0.000 0.003 0.010 0.018 0.003 0.004 0.021 0.011 0.011 0.026 0.015 0.011 0.000 0.000 0.010 0.010 0.929 -0.022 -0.003 0.018 -0.010 0.000 -0.003 0.011 0.002 0.004 0.006 0.013 0.006 0.006 0.006 -0.002 0.006 0.010 0.008 0.000 0.004 0.005 0.667 -0.117 0.027 -0.310 -0.255 -0.029 -0.137 0.148 -0.016 0.044 -0.126 0.013 0.083 0.000 0.056 0.077 -0.035 -0.052 -0.196 -0.029 -0.047 0.078 0.667 -0.079 -0.076 0.044 -0.143 -0.042 -0.059 0.077 -0.082 0.000 -0.067 0.041 -0.067 -0.035 0.051 -0.130 -0.048 -0.033 -0.138 -0.029 -0.076 0.076 - 5 . 2 - m 0.905 0.020 0.033 0.022 0.025 0.000 0.020 0.020 0.028 0.022 0.016 0.018 0.036 0.024 0.024 0.024 0.028 0.024 0.003 -0.012 0.013 0.018 0.905 -0.191 0.006 0.038 0.022 0.014 -0.022 0.054 0.023 -0.007 0.003 0.019 0.002 0.008 0.011 0.022 0.006 -0.004 0.024 0.010 0.012 0.013 0.533 0.092 0.148 0.017 0.089 0.074 0.084 0.084 0.134 0.158 0.063 0.134 0.187 0.135 0.135 0.105 0.147 0.105 0.009 0.012 0.076 0.076 0.533 -0.173 0.071 0.134 0.038 0.134 0.041 0.110 0.146 0.005 0.009 0.121 0.044 0.065 0.065 0.158 0.055 0.049 0.120 0.082 0.093 0. - S D 0.942 -0.003 -0.003 -0.007 -0.020 -0.021 -0.011 0.011 0.005 -0.012 -0.035 -0.003 -0.021 -0.013 0.015 -0.003 0.008 0.001 0.001 -0.041 -0.007 0.011 0.942 -0.008 0.001 0.014 0.009 -0.011 0.001 0.009 -0.007 -0.025 -0.011 0.000 -0.012 -0.011 0.011 0.004 -0.026 -0.016 -0.008 -0.026 -0.014 0.016 0.711 -0.077 -0.060 -0.096 -0.153 -0.115 -0.100 0.100 0.000 -0.089 -0.261 -0.077 -0.115 -0.108 0.108 -0.060 0.016 -0.060 -0.044 -0.233 -0.076 0.083 0.711 -0.075 -0.029 0.007 -0.028 -0.106 -0.046 0.049 -0.126 -0.128 -0.147 -0.015 -0.075 -0.098 0.098 0.012 -0.111 -0.102 -0.075 -0.111 -0.077 0.082 8 Q 0.916 -0.005 -0.001 -0.007 -0.007 0.005 -0.003 0.005 -0.008 -0.010 -0.008 -0.005 -0.006 -0.007 0.007 -0.001 -0.005 0.004 -0.009 0.005 -0.001 0.005 0.916 0.002 -0.005 -0.010 0.003 0.002 -0.002 0.004 -0.013 -0.005 0.003 -0.007 -0.003 -0.005 0.006 -0.005 -0.001 0.006 0.003 -0.011 -0.002 0.005 0.432 -0.225 -0.165 -0.432 -0.432 -0.020 -0.255 0.255 -0.289 -0.138 -0.355 -0.174 -0.129 -0.217 0.217 -0.218 -0.174 -0.210 -0.232 0.012 -0.164 0.169 0.432 -0.109 -0.174 -0.182 -0.156 -0.068 -0.138 0.138 -0.238 -0.174 -0.156 -0.089 -0.079 -0.147 0.147 -0.174 -0.218 -0.099 -0.156 -0.099 -0.149 0. 4 1 Q 0.885 0.023 0.019 0.032 0.024 0.003 0.020 0.020 0.032 0.018 0.032 0.027 0.027 0.027 0.027 0.020 0.019 0.023 0.024 0.004 0.018 0.018 0.885 0.009 0.030 0.019 0.032 0.014 0.021 0.021 0.028 0.008 0.032 0.018 0.027 0.023 0.023 0.015 0.028 0.039 0.016 0.005 0.021 0.021 0.364 -0.287 -0.290 -0.281 -0.364 -0.101 -0.265 0.265 -0.281 -0.226 -0.281 -0.216 -0.216 -0.244 0.244 -0.364 -0.290 -0.287 -0.364 -0.142 -0.289 0.289 0.364 -0.182 -0.097 -0.290 -0.281 -0.231 -0.216 0.216 -0.284 -0.135 -0.281 -0.226 -0.216 -0.228 0.228 -0.293 -0.284 -0.133 -0.364 -0.188 -0.252 0.252 2 3 Q 0.885 0.021 0.036 0.018 0.023 0.039 0.027 0.027 0.037 0.023 0.036 0.037 0.022 0.031 0.031 0.018 0.032 0.005 0.024 0.024 0.021 0.021 0.885 0.023 0.020 0.031 0.023 0.031 0.026 0.026 0.026 0.035 0.039 0.026 0.024 0.030 0.030 0.021 0.023 0.035 0.023 0.041 0.029 0.029 0.364 -0.070 0.080 -0.226 -0.221 0.122 -0.063 0.144 0.011 0.014 0.080 0.194 0.075 0.075 0.075 -0.226 0.036 -0.188 -0.021 0.158 -0.048 0.126 0.364 0.014 -0.031 0.098 0.014 0.068 0.033 0.045 0.086 0.110 0.122 0.086 -0.021 0.077 0.085 -0.070 0.014 0.110 0.014 0.227 0.059 0. 1 . 4 - 5 . 3 - a 0.905 0.024 0.011 0.029 0.042 0.028 0.027 0.027 0.026 0.012 0.022 0.023 0.028 0.022 0.022 0.012 0.024 0.025 0.025 0.024 0.022 0.022 0.905 0.014 0.015 0.021 0.030 0.020 0.020 0.020 0.003 0.029 0.022 0.042 0.023 0.024 0.024 0.025 0.024 0.020 0.025 0.023 0.023 0.023 FALSE Category 0.907 -0.012 -0.013 -0.007 -0.040 -0.008 -0.016 0.016 0.013 -0.005 0.004 -0.020 -0.013 -0.004 0.011 0.013 0.010 0.001 -0.003 -0.002 0.004 0.006 0.907 0.007 -0.008 0.004 0.014 0.015 0.006 0.010 -0.042 -0.023 0.017 -0.021 -0.033 -0.020 0.027 -0.006 -0.022 0.008 0.003 -0.010 -0.005 0.010 TRUE Category 0.651 -0.165 -0.177 -0.287 -0.251 -0.273 -0.231 0.231 -0.180 -0.127 -0.151 -0.101 0.016 -0.109 0.115 -0.207 -0.072 -0.219 -0.138 -0.177 -0.163 0.163 0.651 -0.207 -0.151 -0.251 0.016 -0.072 -0.133 0.139 -0.138 -0.151 -0.151 -0.230 -0.230 -0.180 0.180 -0.017 -0.177 -0.138 -0.125 -0.151 -0.122 0.122 0.680 -0.042 -0.080 -0.044 0.031 0.000 -0.027 0.039 -0.075 -0.115 -0.109 -0.013 0.000 -0.062 0.062 -0.115 -0.042 -0.058 -0.058 -0.027 -0.060 0.060 0.680 -0.038 -0.053 -0.089 -0.061 -0.055 -0.059 0.059 -0.138 -0.028 -0.109 0.031 -0.013 -0.051 0.064 -0.058 -0.027 -0.055 -0.058 -0.013 -0.042 0.042 0 . 2 - m 0.861 0.053 0.070 0.064 0.067 0.031 0.057 0.057 0.063 0.073 0.047 0.074 0.050 0.061 0.061 0.053 0.065 0.056 0.057 0.044 0.055 0.055 0.861 0.033 0.062 0.036 0.061 0.060 0.050 0.050 0.058 0.059 0.058 0.056 0.063 0.059 0.059 0.035 0.054 0.059 0.063 0.069 0.056 0.056 0.520 0.013 0.065 -0.145 0.006 0.025 -0.007 0.051 -0.007 0.085 -0.088 0.099 0.068 0.031 0.069 -0.032 0.004 0.025 -0.008 -0.042 -0.011 0.022 0.520 0.093 0.107 -0.091 0.025 0.025 0.032 0.068 0.004 0.107 -0.032 0.013 0.105 0.039 0.052 -0.042 -0.020 0.058 -0.007 0.071 0.012 0.040 - S D 0.908 0.001 0.021 0.001 0.009 0.012 0.009 0.009 0.011 0.010 0.011 0.017 0.014 0.013 0.013 0.003 -0.002 0.036 0.022 0.000 0.012 0.013 0.908 0.010 0.011 0.014 0.004 0.010 0.010 0.010 0.003 0.029 0.011 0.028 0.003 0.015 0.015 0.017 0.007 0.036 0.041 0.016 0.023 0.023 0.410 -0.067 0.061 -0.410 -0.327 0.064 -0.136 0.186 -0.134 -0.087 -0.134 0.047 0.148 -0.032 0.110 -0.152 -0.160 0.201 -0.010 -0.032 -0.031 0.111 0.410 -0.087 -0.134 -0.077 -0.262 -0.046 -0.121 0.121 -0.203 0.104 -0.134 0.169 -0.152 -0.043 0.152 0.014 -0.143 0.201 0.196 0.103 0.074 0.131 2 7 Q 0.903 0.019 0.006 0.030 0.032 -0.004 0.017 0.018 0.041 0.014 0.037 0.011 0.005 0.022 0.022 0.011 0.016 -0.002 0.029 -0.010 0.009 0.014 0.903 0.011 0.002 0.016 -0.015 0.002 0.003 0.009 0.016 -0.006 0.014 0.019 0.018 0.012 0.015 0.019 0.029 -0.003 0.014 0.008 0.013 0.015 0.549 0.009 -0.027 -0.035 0.051 -0.029 -0.006 0.030 0.101 0.034 0.046 -0.037 0.011 0.031 0.046 -0.037 -0.025 -0.071 0.015 -0.004 -0.024 0.030 0.549 -0.037 -0.038 -0.049 -0.123 -0.038 -0.057 0.057 -0.049 0.007 0.016 0.009 0.063 0.009 0.029 0.009 0.015 -0.049 -0.149 0.055 -0.024 0.055 0 7 l 0.815 0.109 0.096 0.106 0.107 0.104 0.104 0.104 0.087 0.094 0.096 0.092 0.116 0.097 0.097 0.104 0.086 0.112 0.091 0.066 0.092 0.092 0.815 0.085 0.100 0.089 0.085 0.099 0.092 0.092 0.096 0.079 0.090 0.082 0.095 0.088 0.088 0.085 0.077 0.077 0.078 0.065 0.076 0.076 0.494 0.019 -0.044 -0.082 -0.107 0.030 -0.037 0.056 -0.039 0.028 -0.018 0.094 0.125 0.038 0.061 -0.032 -0.005 0.006 -0.029 -0.063 -0.025 0.027 0.494 0.006 -0.006 0.037 0.006 0.039 0.016 0.019 -0.018 0.034 0.017 0.088 0.006 0.025 0.033 0.006 -0.103 -0.068 -0.075 -0.013 -0.051 0.053 7 - t M 0.918 0.002 0.001 -0.007 -0.058 -0.014 -0.015 0.016 -0.007 -0.012 -0.010 -0.003 0.000 -0.006 0.006 -0.014 -0.005 -0.010 -0.001 -0.012 -0.008 0.008 0.918 -0.007 0.001 -0.007 -0.027 0.000 -0.008 0.008 0.001 0.012 -0.024 0.000 0.003 -0.002 0.008 -0.010 -0.005 -0.010 -0.005 -0.016 -0.009 0.009 0.276 -0.116 -0.045 -0.193 -0.193 0.088 -0.092 0.127 -0.054 0.000 -0.193 0.010 0.000 -0.047 0.051 -0.196 -0.276 -0.122 -0.193 -0.062 -0.170 0.170 0.276 -0.122 -0.054 -0.193 -0.193 0.081 -0.096 0.129 -0.122 0.176 -0.045 0.124 0.099 0.046 0.113 -0.128 -0.276 -0.196 -0.196 -0.062 -0.172 0.172 a - t 0.873 0.056 0.042 0.052 0.049 -0.007 0.038 0.041 0.048 0.030 0.026 0.014 -0.013 0.021 0.026 0.052 0.029 0.043 0.043 0.012 0.036 0.036 0.873 0.046 0.046 0.012 0.030 0.024 0.032 0.032 0.032 -0.005 0.033 -0.013 0.022 0.014 0.021 0.038 0.043 0.034 0.048 0.020 0.037 0. 0.541 -0.041 -0.012 -0.184 -0.184 -0.089 -0.102 0.102 -0.154 -0.208 -0.154 0.000 -0.027 -0.109 0.109 -0.141 -0.084 -0.103 -0.041 -0.166 -0.107 0.107 0.541 -0.041 -0.070 -0.255 -0.129 -0.103 -0.120 0.120 -0.041 0.015 -0.103 -0.027 0.004 -0.030 0.038 -0.089 -0.117 -0.196 -0.218 -0.166 -0.157 0.157 N - t 0.924 -0.009 -0.009 0.000 0.000 -0.011 -0.006 0.006 -0.004 -0.001 0.000 -0.006 -0.006 -0.003 0.003 -0.004 0.003 0.003 0.000 -0.015 -0.003 0.005 0.924 0.003 -0.001 -0.009 -0.009 -0.001 -0.003 0.005 -0.009 -0.001 0.000 -0.016 0.003 -0.005 0.006 -0.005 0.000 0.007 -0.004 -0.022 -0.005 0.008 0.513 -0.299 -0.246 -0.282 -0.282 -0.124 -0.247 0.247 -0.291 -0.227 -0.282 -0.190 -0.190 -0.236 0.236 -0.291 -0.217 -0.217 -0.282 -0.170 -0.235 0.235 0.513 -0.217 -0.227 -0.246 -0.299 -0.227 -0.243 0.243 -0.299 -0.227 -0.282 -0.370 -0.217 -0.279 0.279 -0.237 -0.282 -0.156 -0.353 -0.271 -0.260 0. 4 2 - m - t 0.718 0.008 0.048 0.096 -0.004 0.000 0.030 0.031 0.094 0.058 0.107 0.047 -0.038 0.054 0.069 0.101 0.123 0.055 0.145 -0.013 0.082 0.087 0.718 -0.001 -0.082 -0.078 0.062 0.015 -0.017 0.048 0.103 -0.019 0.078 0.098 -0.009 0.050 0.061 0.140 0.139 0.026 0.127 -0.003 0.086 0.087 0.222 -0.145 -0.068 -0.222 -0.222 -0.074 -0.146 0.146 -0.142 -0.222 -0.222 -0.142 -0.142 -0.174 0.174 -0.139 -0.222 -0.222 -0.222 -0.148 -0.191 0.191 0.222 -0.008 -0.068 -0.079 -0.142 -0.148 -0.089 0.089 -0.148 -0.084 -0.148 -0.068 -0.068 -0.103 0.103 -0.068 -0.222 -0.145 -0.222 -0.222 -0.176 0.176 7 8 - t 0.687 -0.034 -0.065 0.099 -0.073 -0.098 -0.034 0.074 -0.031 0.019 0.089 -0.122 -0.099 -0.029 0.072 0.111 0.081 0.051 0.104 0.010 0.071 0.071 0.687 0.042 -0.078 0.080 -0.149 -0.077 -0.036 0.085 -0.161 -0.095 0.024 -0.152 -0.124 -0.102 0.111 0.006 0.082 0.052 0.061 0.011 0.042 0. 0.194 -0.194 -0.120 -0.194 -0.194 0.064 -0.128 0.153 -0.114 -0.040 -0.194 -0.051 -0.056 -0.091 0.091 -0.117 -0.051 -0.051 -0.194 0.073 -0.068 0.097 0.194 -0.123 0.020 -0.114 -0.114 -0.123 -0.091 0.099 -0.046 -0.056 -0.194 0.028 -0.046 -0.063 0.074 -0.194 -0.056 -0.056 -0.117 0.064 -0.072 0.097 2 2 8 - t 0.922 -0.109 -0.113 -0.112 -0.111 -0.085 -0.106 0.106 -0.029 -0.058 -0.092 -0.087 -0.031 -0.059 0.059 -0.034 -0.044 -0.007 -0.062 -0.035 -0.036 0.036 0.922 -0.099 -0.124 -0.035 -0.102 -0.041 -0.080 0.080 -0.087 -0.051 -0.082 -0.066 -0.143 -0.086 0.086 -0.020 -0.158 -0.021 -0.044 -0.049 -0.058 0.058 0.485 -0.411 -0.271 -0.405 -0.331 -0.182 -0.320 0.320 -0.331 -0.331 -0.325 -0.271 -0.337 -0.319 0.319 -0.342 -0.337 -0.199 -0.271 -0.162 -0.262 0.262 0.485 -0.352 -0.285 -0.331 -0.199 -0.263 -0.286 0.286 -0.347 -0.152 -0.337 -0.411 -0.297 -0.309 0.309 -0.199 -0.342 -0.152 -0.271 -0.152 -0.223 0.223 AM MAV 0.882 0.011 0.013 0.021 0.006 0.003 0.011 0.031 0.022 0.017 0.022 0.010 0.008 0.016 0.031 0.025 0.026 0.022 0.026 0.005 0.021 0.028 0.882 0.002 0.004 0.014 0.006 0.011 0.008 0.030 0.006 0.004 0.019 0.010 0.002 0.008 0.032 0.020 0.016 0.019 0.024 0.008 0.018 0.028 0.463 -0.104 -0.051 -0.216 -0.184 -0.019 -0.115 0.149 -0.100 -0.064 -0.132 -0.028 -0.018 -0.068 0.130 -0.135 -0.095 -0.098 -0.123 -0.060 -0.102 0.130 0.463 -0.084 -0.053 -0.096 -0.102 -0.059 -0.079 0.114 -0.101 -0.020 -0.095 -0.035 -0.045 -0.059 0.117 -0.080 -0.135 -0.070 -0.127 -0.042 -0.091 0.131 0.882 0.027 0.032 0.034 0.036 0.027 0.029 0.031 0.030 0.027 0.036 0.033 0.030 0.028 0.031 0.032 0.032 0.025 0.034 0.019 0.027 0.028 0.882 0.032 0.032 0.028 0.035 0.025 0.024 0.030 0.036 0.027 0.031 0.036 0.033 0.030 0.032 0.025 0.037 0.025 0.031 0.022 0.027 0.028 0.463 0.129 0.104 0.218 0.202 0.093 0.125 0.149 0.132 0.123 0.176 0.112 0.105 0.116 0.130 0.154 0.126 0.131 0.134 0.102 0.117 0.130 0.463 0.102 0.096 0.135 0.125 0.109 0.102 0.114 0.131 0.098 0.134 0.117 0.104 0.104 0.117 0.111 0.144 0.121 0.157 0.121 0.122 0.131 Table 8: Results on MFMD-persona: Chinese part. Role Scenario Implicit RetailInvestor Professional Base Overconfidence LossAversion Herding Anchoring ConfirmationBias AM MAV Overconfidence LossAversion Herding Anchoring ConfirmationBias AM MAV CompanyOwner Overconfidence Explicit RetailInvestor Professional LossAversion Herding Anchoring ConfirmationBias AM MAV Base Overconfidence LossAversion Herding Anchoring ConfirmationBias AM MAV Overconfidence LossAversion Herding Anchoring ConfirmationBias AM MAV CompanyOwner Overconfidence Implicit RetailInvestor Professional LossAversion Herding Anchoring ConfirmationBias AM MAV Base Overconfidence LossAversion Herding Anchoring ConfirmationBias AM MAV Overconfidence LossAversion Herding Anchoring ConfirmationBias AM MAV CompanyOwner Overconfidence Explicit RetailInvestor Professional LossAversion Herding Anchoring ConfirmationBias AM MAV Base Overconfidence LossAversion Herding Anchoring ConfirmationBias AM MAV Overconfidence LossAversion Herding Anchoring ConfirmationBias AM MAV CompanyOwner Overconfidence LossAversion Herding Anchoring ConfirmationBias AM MAV - 8 Q 0.904 0.010 -0.001 0.001 0.004 0.012 0.005 0.006 0.003 0.004 0.023 -0.008 0.009 0.006 0.009 0.010 0.011 0.003 0.003 -0.031 -0.001 0.011 0.904 0.009 0.002 0.019 0.014 0.013 0.012 0.012 0.009 0.014 0.010 0.009 0.021 0.012 0.012 -0.003 0.008 0.011 0.006 0.007 0.006 0.007 0.378 -0.066 -0.184 -0.378 -0.230 0.054 -0.161 0.182 -0.178 0.000 -0.092 -0.062 0.011 -0.064 0.069 -0.066 -0.102 -0.171 -0.171 -0.092 -0.120 0.120 0.378 -0.014 -0.120 -0.092 -0.055 0.022 -0.052 0.061 0.011 -0.014 -0.120 0.011 0.079 -0.007 0.047 -0.092 -0.230 -0.111 -0.075 0.098 -0.082 0.121 - 4 1 Q 0.905 0.010 0.011 0.008 0.004 -0.006 0.006 0.008 0.004 0.006 0.004 0.010 0.003 0.005 0.005 0.015 0.003 0.000 0.011 -0.006 0.004 0.007 0.905 -0.005 0.002 0.003 0.008 -0.002 0.001 0.004 0.011 0.005 0.010 0.022 -0.001 0.009 0.010 0.007 0.003 0.003 0.003 0.014 0.006 0.006 0.333 -0.119 -0.179 -0.333 -0.333 -0.200 -0.233 0.233 -0.333 -0.126 -0.333 -0.119 -0.185 -0.219 0.219 -0.111 -0.190 -0.333 -0.179 -0.200 -0.203 0.203 0.333 -0.262 -0.133 -0.256 -0.253 -0.195 -0.220 0.220 -0.179 -0.030 -0.119 0.012 -0.259 -0.115 0.120 -0.185 -0.256 -0.190 -0.256 -0.057 -0.189 0.189 - 2 3 Q 0.902 0.028 0.006 0.017 0.007 0.014 0.014 0.014 0.023 0.031 0.021 0.027 0.018 0.024 0.024 0.013 0.007 0.015 0.028 -0.009 0.011 0.014 0.902 0.025 0.009 0.022 0.016 0.019 0.018 0.018 0.007 -0.002 0.016 0.030 0.023 0.015 0.016 0.006 0.016 0.022 0.018 -0.009 0.011 0.014 0.429 0.009 -0.019 -0.153 -0.429 0.003 -0.118 0.123 0.028 0.085 -0.143 0.042 0.045 0.011 0.069 -0.162 -0.086 -0.029 0.009 -0.020 -0.058 0.061 0.429 0.121 0.021 0.057 -0.065 -0.017 0.023 0.056 -0.086 -0.070 -0.065 0.112 -0.005 -0.023 0.068 -0.051 -0.106 0.057 0.045 0.006 -0.010 0.053 - m - 5 - 0.938 -0.010 0.006 -0.001 -0.003 -0.008 -0.003 0.006 -0.032 -0.006 -0.005 0.001 0.001 -0.008 0.009 -0.002 0.006 -0.002 -0.015 -0.003 -0.003 0.005 0.938 -0.012 -0.007 0.010 -0.010 -0.007 -0.005 0.009 -0.003 -0.016 -0.014 0.006 -0.003 -0.006 0.008 -0.011 0.006 0.002 0.002 0.002 0.000 0.005 0.667 -0.141 -0.017 -0.111 -0.067 -0.045 -0.076 0.076 -0.202 -0.103 -0.153 -0.033 -0.033 -0.105 0.105 -0.088 0.000 -0.088 -0.130 -0.067 -0.075 0.075 0.667 -0.058 -0.062 -0.018 -0.141 -0.082 -0.072 0.072 -0.067 -0.109 -0.154 -0.035 -0.067 -0.086 0.086 -0.096 -0.056 -0.052 -0.072 -0.052 -0.065 0.065 - 5 . 4 - a 0.929 0.010 -0.008 -0.005 0.004 -0.007 -0.001 0.007 0.004 0.018 0.016 -0.009 -0.003 0.005 0.010 -0.002 -0.008 0.031 0.001 0.005 0.005 0.010 0.929 -0.013 0.006 0.010 -0.002 -0.012 -0.002 0.009 0.010 -0.005 0.012 0.000 -0.008 0.002 0.007 0.014 0.008 0.022 0.000 0.005 0.010 0.010 0.653 -0.002 -0.057 -0.422 -0.139 -0.048 -0.134 0.134 0.028 0.061 -0.065 -0.026 -0.062 -0.013 0.048 -0.082 -0.209 0.104 -0.253 -0.017 -0.091 0.133 0.653 -0.053 -0.034 -0.019 -0.082 -0.088 -0.055 0.055 0.014 -0.013 -0.082 -0.015 -0.057 -0.031 0.036 0.014 -0.124 0.061 -0.182 -0.001 -0.046 0. - 5 . 2 - m 0.927 -0.016 -0.017 -0.004 -0.010 -0.035 -0.016 0.016 -0.020 -0.008 -0.018 -0.008 -0.013 -0.013 0.013 -0.017 -0.016 -0.012 -0.015 -0.005 -0.013 0.013 0.927 -0.142 -0.002 -0.009 0.006 -0.020 -0.033 0.036 0.004 0.000 -0.008 0.005 -0.019 -0.004 0.007 -0.016 -0.017 -0.017 -0.016 -0.031 -0.019 0.019 0.667 -0.079 -0.049 -0.130 -0.102 -0.106 -0.093 0.093 -0.079 -0.025 -0.145 -0.025 -0.024 -0.060 0.060 -0.049 -0.079 -0.040 -0.096 0.012 -0.050 0.055 0.667 -0.237 -0.042 -0.012 0.000 -0.079 -0.074 0.074 0.047 0.024 -0.025 0.037 -0.023 0.012 0.031 -0.079 -0.049 -0.049 -0.079 -0.081 -0.067 0.067 - S D 0.934 0.012 -0.014 0.001 -0.005 -0.007 -0.003 0.008 -0.019 -0.005 -0.004 0.002 -0.006 -0.006 0.007 -0.012 -0.023 -0.007 0.007 -0.021 -0.011 0.014 0.934 -0.010 -0.012 0.004 -0.001 0.007 -0.002 0.007 -0.006 0.002 -0.028 -0.014 -0.034 -0.016 0.017 -0.002 -0.015 -0.006 0.003 -0.030 -0.010 0.011 0.652 0.071 -0.025 -0.033 -0.014 0.039 0.008 0.036 -0.022 0.001 -0.030 0.065 0.015 0.006 0.027 -0.074 -0.064 0.039 0.068 0.003 -0.006 0.050 0.652 0.002 0.027 0.015 0.028 0.068 0.028 0.028 0.027 0.075 -0.059 -0.025 -0.056 -0.007 0.048 0.052 -0.010 0.015 0.042 -0.019 0.016 0. 8 Q 0.906 0.009 0.005 0.007 0.003 0.013 0.007 0.007 0.007 0.012 0.003 0.013 0.014 0.010 0.010 0.010 0.009 0.010 0.009 0.004 0.009 0.009 0.906 0.005 0.023 0.014 0.003 0.010 0.011 0.011 0.016 0.007 0.010 0.013 0.028 0.015 0.015 0.013 0.002 0.006 0.010 -0.008 0.005 0.008 0.250 0.026 -0.043 -0.170 -0.250 0.026 -0.082 0.103 -0.170 0.114 -0.250 0.026 -0.028 -0.062 0.117 -0.096 -0.036 -0.096 -0.036 0.053 -0.042 0.063 0.250 0.008 0.221 -0.090 -0.250 -0.096 -0.042 0.133 0.083 0.103 -0.096 0.026 0.164 0.056 0.094 0.026 -0.173 -0.102 -0.096 -0.015 -0.072 0.082 4 1 Q 0.910 -0.001 0.003 -0.001 0.003 0.003 0.001 0.002 -0.001 -0.002 -0.001 0.002 0.003 0.000 0.002 -0.005 -0.001 -0.002 -0.001 0.002 -0.001 0.002 0.910 0.003 -0.002 -0.001 0.003 0.003 0.001 0.002 -0.001 0.006 -0.001 0.005 0.003 0.002 0.003 -0.002 -0.001 0.003 0.003 0.002 0.001 0.002 0.303 -0.303 -0.223 -0.303 -0.303 -0.223 -0.271 0.271 -0.303 -0.160 -0.303 -0.155 -0.223 -0.229 0.229 -0.303 -0.303 -0.226 -0.303 -0.155 -0.258 0.258 0.303 -0.223 -0.226 -0.303 -0.223 -0.223 -0.240 0.240 -0.303 -0.149 -0.303 -0.089 -0.223 -0.213 0.213 -0.226 -0.303 -0.223 -0.303 -0.155 -0.242 0. 2 3 Q 0.911 0.019 0.005 0.004 -0.002 0.009 0.007 0.008 0.007 0.014 0.005 0.021 0.022 0.014 0.014 0.008 0.011 0.010 0.007 -0.008 0.006 0.009 0.911 0.017 0.010 0.018 0.000 0.014 0.012 0.012 0.013 0.014 0.018 0.013 0.003 0.012 0.012 -0.005 0.003 -0.005 -0.003 -0.023 -0.007 0.008 0.476 -0.039 -0.014 -0.209 -0.476 -0.002 -0.148 0.148 -0.112 -0.019 -0.322 0.065 0.038 -0.070 0.111 -0.200 -0.101 -0.064 -0.153 -0.076 -0.119 0.119 0.476 0.050 -0.032 0.024 -0.218 -0.019 -0.039 0.069 0.010 -0.052 -0.005 0.037 -0.164 -0.035 0.054 -0.182 -0.164 -0.226 -0.098 -0.050 -0.144 0.144 1 . 4 - 5 . 3 - a 0.933 0.009 0.017 0.014 0.009 0.013 0.012 0.012 0.014 0.005 0.018 0.017 -0.004 0.010 0.011 0.017 0.005 0.017 0.013 0.005 0.012 0.012 0.933 -0.005 -0.009 0.005 0.009 0.012 0.003 0.008 0.001 0.005 0.009 0.025 0.017 0.011 0.011 0.009 0.001 -0.001 0.009 -0.005 0.003 0. FALSE Category 0.904 -0.028 -0.106 -0.040 -0.157 -0.044 -0.075 0.075 -0.071 -0.073 -0.042 -0.117 -0.072 -0.075 0.075 -0.044 -0.124 -0.047 -0.091 -0.076 -0.076 0.076 0.904 -0.026 -0.078 -0.052 -0.053 0.007 -0.040 0.043 -0.126 -0.055 -0.052 -0.076 -0.083 -0.078 0.078 -0.039 -0.098 -0.059 -0.061 -0.026 -0.057 0.057 TRUE Category 0.564 -0.008 -0.120 -0.079 -0.164 -0.008 -0.076 0.076 -0.064 -0.093 -0.112 -0.127 0.068 -0.066 0.093 -0.050 -0.064 -0.079 -0.051 -0.064 -0.062 0.062 0.564 -0.164 -0.035 -0.008 -0.127 -0.112 -0.089 0.089 -0.050 -0.008 -0.035 -0.008 -0.093 -0.039 0.039 -0.102 -0.093 -0.064 -0.107 -0.093 -0.092 0.092 0.680 0.028 0.080 0.018 0.028 0.055 0.042 0.042 0.031 0.001 0.034 0.070 -0.042 0.019 0.036 0.059 -0.013 0.059 0.043 0.001 0.030 0.035 0.680 -0.013 -0.026 -0.013 0.016 0.065 0.006 0.027 -0.028 -0.013 0.016 0.120 0.080 0.035 0.051 0.016 -0.044 0.012 0.016 -0.001 0.000 0.018 0 . 2 - m 0.857 0.069 0.070 0.077 0.063 0.026 0.061 0.061 0.074 0.085 0.074 0.080 0.076 0.078 0.078 0.061 0.069 0.063 0.051 0.067 0.062 0.062 0.857 0.037 0.079 0.069 0.045 0.076 0.061 0.061 0.093 0.072 0.082 0.089 0.071 0.081 0.081 0.060 0.070 0.051 0.070 0.067 0.064 0.064 0.609 -0.031 -0.177 -0.313 -0.165 -0.083 -0.154 0.154 -0.096 0.087 0.006 0.085 0.043 0.025 0.063 -0.085 -0.038 0.000 -0.097 0.000 -0.044 0.044 0.609 0.004 0.085 -0.072 -0.147 0.000 -0.026 0.062 0.073 0.029 -0.045 0.130 0.058 0.049 0.067 -0.031 -0.072 -0.120 -0.009 0.016 -0.043 0.050 - S D 0.933 -0.011 0.008 -0.010 -0.009 -0.008 -0.006 0.009 -0.006 -0.007 -0.006 0.004 0.003 -0.002 0.005 -0.015 -0.003 -0.006 -0.009 -0.003 -0.007 0.007 0.933 -0.012 0.004 0.001 0.001 -0.003 -0.002 0.004 -0.002 0.015 -0.009 -0.011 0.001 -0.001 0.008 0.000 -0.021 -0.013 0.009 -0.008 -0.007 0.010 0.514 -0.181 0.031 -0.228 -0.283 -0.057 -0.144 0.156 -0.169 -0.127 -0.218 0.015 0.065 -0.087 0.119 -0.191 -0.114 -0.218 -0.283 -0.077 -0.177 0.177 0.514 -0.102 0.015 -0.100 -0.062 -0.114 -0.073 0.079 -0.157 0.153 -0.283 -0.181 -0.100 -0.114 0.175 0.000 -0.366 -0.292 0.002 -0.090 -0.149 0.150 2 7 Q 0.911 0.009 0.004 0.018 0.017 0.019 0.013 0.013 0.009 0.020 0.029 0.013 0.006 0.015 0.015 -0.004 -0.005 0.012 0.017 0.011 0.006 0.010 0.911 0.012 0.016 0.026 -0.003 0.003 0.011 0.012 0.021 0.011 0.025 0.033 0.006 0.019 0.019 0.004 0.000 0.000 0.016 0.001 0.004 0.004 0.476 -0.002 0.012 -0.005 0.050 0.146 0.040 0.043 -0.002 0.129 0.119 0.037 0.089 0.074 0.075 -0.037 -0.011 0.061 0.050 0.102 0.033 0.052 0.476 0.061 0.074 0.080 -0.098 0.036 0.030 0.069 0.088 0.102 0.103 0.174 0.089 0.111 0.111 0.012 0.000 0.000 0.074 0.095 0.036 0.036 0 7 l 0.896 0.018 0.008 0.013 0.037 0.007 0.017 0.017 0.028 0.042 0.020 0.016 0.001 0.021 0.021 0.002 0.012 -0.012 0.019 -0.010 0.002 0.011 0.896 0.012 0.022 -0.005 0.020 0.006 0.011 0.013 0.035 -0.008 0.026 0.001 0.014 0.013 0.017 0.015 0.003 0.022 0.017 -0.005 0.010 0.012 0.706 -0.244 -0.161 -0.342 -0.221 -0.141 -0.222 0.222 -0.165 -0.072 -0.235 -0.142 -0.141 -0.151 0.151 -0.232 -0.182 -0.244 -0.232 -0.196 -0.217 0.217 0.706 -0.193 -0.142 -0.230 -0.055 -0.039 -0.132 0.132 -0.072 -0.153 -0.150 -0.097 -0.053 -0.105 0.105 -0.153 -0.169 -0.091 -0.121 -0.216 -0.150 0.150 7 - t 0.892 -0.206 -0.147 -0.191 -0.147 -0.115 -0.161 0.161 -0.141 -0.158 -0.137 -0.055 -0.083 -0.115 0.115 -0.089 -0.078 -0.133 -0.029 -0.013 -0.068 0.068 0.892 -0.219 -0.121 -0.175 -0.134 -0.127 -0.155 0.155 -0.141 -0.050 -0.143 -0.032 -0.075 -0.088 0.088 -0.156 -0.041 -0.119 -0.068 -0.019 -0.081 0.081 0.214 -0.214 -0.131 -0.214 -0.140 -0.020 -0.144 0.144 -0.214 -0.137 -0.214 -0.137 -0.071 -0.155 0.155 -0.137 -0.214 -0.071 -0.134 -0.071 -0.125 0.125 0.214 -0.131 -0.054 -0.134 -0.214 0.008 -0.105 0.108 -0.214 -0.214 -0.214 -0.060 -0.066 -0.154 0.154 -0.081 -0.214 -0.071 -0.137 -0.071 -0.115 0. r - t 0.839 -0.039 -0.021 0.028 0.025 -0.048 -0.011 0.032 0.022 -0.075 -0.003 -0.054 -0.107 -0.043 0.052 -0.054 -0.063 -0.020 -0.043 -0.067 -0.049 0.049 0.839 -0.059 -0.039 -0.030 -0.014 -0.013 -0.031 0.031 -0.084 -0.166 -0.054 -0.083 -0.111 -0.099 0.099 -0.121 -0.048 -0.048 -0.061 -0.078 -0.071 0.071 0.512 -0.012 -0.055 -0.290 -0.226 -0.075 -0.131 0.131 -0.027 0.099 -0.045 0.059 0.059 0.029 0.058 -0.027 0.044 -0.012 0.033 -0.137 -0.020 0.051 0.512 0.033 0.017 -0.088 -0.060 0.117 0.004 0.063 0.059 -0.012 -0.112 0.044 0.002 -0.004 0.046 -0.027 -0.041 0.076 -0.112 -0.041 -0.029 0.060 N - t M 0.927 -0.015 -0.019 -0.014 -0.014 -0.023 -0.017 0.017 -0.019 -0.014 -0.014 -0.015 -0.014 -0.015 0.015 -0.014 -0.019 -0.010 -0.014 -0.015 -0.015 0.015 0.927 -0.019 -0.019 -0.014 -0.014 -0.014 -0.016 0.016 -0.019 -0.014 -0.014 -0.014 -0.014 -0.015 0.015 -0.010 -0.014 -0.010 -0.010 -0.019 -0.013 0.013 0.345 -0.197 -0.268 -0.265 -0.265 -0.271 -0.253 0.253 -0.268 -0.265 -0.265 -0.197 -0.265 -0.252 0.252 -0.265 -0.268 -0.262 -0.265 -0.197 -0.251 0.251 0.345 -0.268 -0.268 -0.265 -0.265 -0.265 -0.266 0.266 -0.268 -0.265 -0.265 -0.265 -0.265 -0.266 0.266 -0.262 -0.265 -0.262 -0.262 -0.268 -0.264 0.264 4 2 - m - t 0.742 0.026 0.073 0.101 0.006 -0.022 0.037 0.046 0.077 0.049 0.105 0.097 -0.075 0.051 0.081 0.079 0.085 0.062 0.119 -0.009 0.067 0.071 0.742 -0.001 -0.040 -0.020 0.038 0.060 0.007 0.032 0.006 -0.021 0.051 -0.004 -0.072 -0.008 0.031 0.115 0.115 -0.034 0.112 -0.042 0.053 0. 0.074 -0.074 -0.074 0.009 -0.074 0.193 -0.004 0.085 -0.074 0.009 -0.074 0.009 0.006 -0.025 0.035 0.006 -0.074 0.009 0.009 0.009 -0.008 0.022 0.074 -0.074 -0.074 0.080 -0.074 0.080 -0.012 0.076 -0.074 0.009 0.009 0.086 0.009 0.008 0.038 -0.074 -0.074 0.006 -0.074 0.003 -0.043 0.046 7 8 - t 0.598 0.072 -0.020 0.225 0.035 0.065 0.075 0.083 0.131 0.168 0.204 -0.101 0.069 0.094 0.134 0.091 0.154 0.164 0.178 0.019 0.121 0.121 0.598 0.099 0.028 0.202 0.030 0.037 0.079 0.079 0.044 -0.023 0.141 0.010 0.036 0.042 0.051 0.075 0.107 0.163 0.150 0.079 0.115 0.115 0.267 -0.267 -0.073 -0.267 -0.184 -0.060 -0.170 0.170 -0.187 0.009 -0.267 -0.060 -0.025 -0.106 0.110 -0.267 0.078 0.019 -0.119 -0.134 -0.085 0.123 0.267 0.000 -0.190 -0.119 -0.060 -0.129 -0.100 0.100 -0.113 0.066 -0.267 -0.119 -0.190 -0.125 0.151 0.066 -0.190 -0.053 -0.119 0.019 -0.055 0.089 2 2 8 - t 0.902 -0.067 -0.066 -0.095 -0.136 -0.079 -0.089 0.089 -0.046 -0.047 -0.084 -0.098 -0.052 -0.066 0.066 -0.044 -0.021 -0.074 -0.095 -0.098 -0.066 0.066 0.902 -0.104 -0.069 -0.066 -0.091 -0.054 -0.077 0.077 -0.055 -0.088 -0.081 -0.038 -0.064 -0.065 0.065 -0.046 -0.075 -0.081 -0.066 -0.082 -0.070 0. 0.387 -0.233 -0.239 -0.387 -0.304 0.065 -0.220 0.245 -0.304 -0.239 -0.387 -0.156 -0.101 -0.237 0.237 -0.307 -0.233 -0.227 -0.156 -0.129 -0.210 0.210 0.387 -0.165 -0.307 -0.227 -0.165 -0.042 -0.181 0.181 -0.310 -0.111 -0.304 -0.233 -0.091 -0.210 0.210 -0.101 -0.239 -0.156 -0.173 0.037 -0.126 0.141 AM MAV 0.886 -0.004 -0.009 0.007 -0.012 -0.010 -0.006 0.032 0.002 0.003 0.009 -0.007 -0.009 -0.001 0.035 0.000 0.001 0.003 0.007 -0.012 0.000 0.030 0.886 -0.019 -0.009 0.001 -0.006 0.001 -0.006 0.030 -0.008 -0.013 0.000 -0.001 -0.012 -0.007 0.031 -0.004 0.001 -0.004 0.006 -0.010 -0.002 0.031 0.462 -0.094 -0.090 -0.209 -0.195 -0.034 -0.125 0.142 -0.131 -0.035 -0.159 -0.035 -0.035 -0.079 0.111 -0.126 -0.103 -0.085 -0.111 -0.066 -0.098 0.114 0.462 -0.076 -0.058 -0.081 -0.117 -0.050 -0.077 0.103 -0.069 -0.030 -0.117 -0.015 -0.056 -0.057 0.100 -0.071 -0.147 -0.083 -0.095 -0.043 -0.088 0.104 0.886 0.031 0.029 0.040 0.032 0.027 0.029 0.032 0.034 0.039 0.038 0.035 0.030 0.031 0.035 0.028 0.033 0.032 0.035 0.022 0.028 0.030 0.886 0.038 0.027 0.035 0.023 0.024 0.027 0.030 0.032 0.027 0.037 0.024 0.032 0.028 0.031 0.033 0.031 0.032 0.032 0.026 0.028 0. 0.462 0.107 0.101 0.212 0.202 0.087 0.133 0.142 0.139 0.089 0.173 0.078 0.074 0.094 0.111 0.132 0.114 0.111 0.131 0.082 0.104 0.114 0.462 0.102 0.100 0.105 0.121 0.086 0.085 0.103 0.106 0.081 0.129 0.087 0.100 0.082 0.100 0.088 0.147 0.104 0.111 0.067 0.093 0.104 Table 9: Results on MFMD-persona: Greek part. 22 Role Scenario Implicit RetailInvestor Professional Base Overconfidence LossAversion Herding Anchoring ConfirmationBias AM MAV Overconfidence LossAversion Herding Anchoring ConfirmationBias AM MAV CompanyOwner Overconfidence Explicit RetailInvestor Professional LossAversion Herding Anchoring ConfirmationBias AM MAV Base Overconfidence LossAversion Herding Anchoring ConfirmationBias AM MAV Overconfidence LossAversion Herding Anchoring ConfirmationBias AM MAV CompanyOwner Overconfidence Implicit RetailInvestor Professional LossAversion Herding Anchoring ConfirmationBias AM MAV Base Overconfidence LossAversion Herding Anchoring ConfirmationBias AM MAV Overconfidence LossAversion Herding Anchoring ConfirmationBias AM MAV CompanyOwner Overconfidence Explicit RetailInvestor Professional LossAversion Herding Anchoring ConfirmationBias AM MAV Base Overconfidence LossAversion Herding Anchoring ConfirmationBias AM MAV Overconfidence LossAversion Herding Anchoring ConfirmationBias AM MAV CompanyOwner Overconfidence LossAversion Herding Anchoring ConfirmationBias AM MAV - 8 Q 0.869 0.045 0.021 0.044 0.047 0.044 0.040 0.040 0.043 0.040 0.035 0.031 0.044 0.039 0.039 0.042 0.037 0.034 0.034 0.020 0.033 0.033 0.869 0.041 0.023 0.011 0.034 0.041 0.030 0.030 0.039 0.025 0.030 0.028 0.036 0.032 0.032 0.042 0.027 0.042 0.042 0.040 0.039 0.039 0.273 0.040 -0.097 -0.193 -0.119 0.080 -0.058 0.106 -0.125 0.070 -0.199 0.060 0.091 -0.021 0.109 -0.066 -0.015 -0.079 -0.073 -0.051 -0.057 0.057 0.273 0.040 0.035 -0.057 -0.079 0.040 -0.005 0.050 -0.130 -0.091 -0.140 0.005 0.060 -0.059 0.085 -0.066 -0.202 -0.066 -0.066 0.080 -0.064 0.096 - 4 1 Q 0.907 0.009 0.009 0.002 0.006 0.000 0.005 0.005 0.009 -0.004 0.017 0.005 0.009 0.007 0.009 0.006 0.006 0.006 0.006 0.000 0.004 0.004 0.907 0.005 0.004 0.001 0.006 0.006 0.004 0.004 0.005 0.005 0.009 0.001 0.005 0.005 0.005 0.009 0.006 0.002 0.002 0.004 0.005 0.005 0.439 -0.275 -0.275 -0.429 -0.429 -0.229 -0.327 0.327 -0.275 -0.291 -0.198 -0.281 -0.275 -0.264 0.264 -0.349 -0.349 -0.349 -0.349 -0.229 -0.325 0.325 0.439 -0.281 -0.222 -0.352 -0.349 -0.349 -0.311 0.311 -0.281 -0.281 -0.275 -0.352 -0.281 -0.294 0.294 -0.275 -0.349 -0.429 -0.429 -0.222 -0.341 0.341 - 2 3 Q 0.914 0.007 -0.034 -0.014 0.004 0.000 -0.007 0.012 0.004 0.019 0.008 0.003 0.000 0.007 0.007 -0.002 0.008 -0.004 -0.007 0.009 0.001 0.006 0.914 -0.003 -0.001 -0.001 -0.001 0.015 0.002 0.004 0.005 0.003 0.001 -0.023 -0.005 -0.004 0.007 -0.005 0.007 0.007 0.016 -0.006 0.004 0.008 0.533 -0.121 -0.322 -0.462 -0.210 -0.221 -0.267 0.267 -0.210 -0.019 -0.200 -0.133 -0.221 -0.156 0.156 -0.385 -0.158 -0.230 -0.333 0.004 -0.220 0.222 0.533 -0.057 -0.180 -0.144 -0.144 -0.033 -0.112 0.112 -0.257 -0.133 -0.266 -0.192 -0.190 -0.208 0.208 -0.190 -0.121 -0.121 -0.133 -0.155 -0.144 0.144 - m - 5 - 0.935 -0.008 0.000 -0.024 -0.012 -0.016 -0.012 0.012 -0.003 0.001 -0.010 -0.003 -0.019 -0.007 0.007 -0.016 0.005 0.008 -0.015 -0.003 -0.004 0.009 0.935 -0.016 -0.024 0.004 0.001 -0.018 -0.010 0.013 -0.026 0.005 0.002 -0.005 -0.048 -0.014 0.017 -0.003 -0.003 -0.015 0.005 -0.013 -0.006 0.008 0.619 -0.048 0.000 -0.143 -0.082 -0.119 -0.078 0.078 -0.055 -0.040 -0.148 -0.055 -0.157 -0.091 0.091 -0.119 -0.004 0.048 -0.145 -0.078 -0.060 0.079 0.619 -0.119 -0.143 0.015 -0.040 -0.054 -0.068 0.074 -0.097 -0.004 -0.090 0.003 -0.170 -0.071 0.073 -0.078 -0.055 -0.145 -0.024 -0.061 -0.073 0. - 5 . 4 - a 0.915 0.004 0.019 -0.007 0.012 0.012 0.008 0.011 0.002 0.000 0.019 0.002 0.028 0.010 0.010 0.009 0.003 0.015 0.009 -0.006 0.006 0.008 0.915 -0.006 -0.018 0.010 -0.007 -0.001 -0.004 0.008 -0.004 0.023 -0.002 0.019 0.016 0.010 0.013 0.005 0.001 0.021 0.023 0.015 0.013 0.013 0.615 -0.078 0.021 -0.538 -0.319 -0.065 -0.196 0.204 -0.215 -0.127 -0.201 -0.032 0.067 -0.102 0.128 -0.102 -0.251 -0.178 -0.384 -0.093 -0.202 0.202 0.615 -0.093 -0.171 -0.191 -0.205 -0.091 -0.150 0.150 -0.139 0.052 -0.262 0.037 -0.010 -0.064 0.100 -0.141 -0.153 -0.036 -0.148 0.007 -0.094 0.097 - 5 . 2 - m 0.909 0.012 0.028 0.002 -0.006 -0.003 0.006 0.010 -0.001 0.024 0.000 0.013 0.009 0.009 0.009 -0.003 0.015 -0.006 0.006 -0.009 0.000 0.008 0.909 -0.132 0.005 0.012 -0.002 0.024 -0.019 0.035 0.005 0.014 -0.001 -0.027 0.001 -0.002 0.009 0.002 0.020 -0.008 0.012 0.001 0.005 0.008 0.655 -0.043 0.039 -0.179 -0.106 -0.062 -0.070 0.086 -0.113 0.025 -0.133 0.024 0.000 -0.040 0.059 -0.062 -0.001 -0.106 -0.025 -0.045 -0.048 0.048 0.655 -0.216 -0.012 -0.043 -0.078 0.025 -0.065 0.075 -0.012 0.012 -0.113 -0.113 -0.037 -0.053 0.057 -0.051 -0.017 -0.073 -0.043 -0.037 -0.044 0. - S D 0.941 -0.021 -0.004 -0.015 -0.020 -0.025 -0.017 0.017 -0.007 -0.034 -0.017 -0.009 -0.035 -0.020 0.020 -0.004 -0.038 -0.016 -0.006 -0.021 -0.017 0.017 0.941 -0.013 -0.025 -0.012 -0.016 -0.008 -0.015 0.015 -0.030 -0.045 -0.024 -0.017 -0.045 -0.032 0.032 -0.009 -0.013 -0.037 0.013 -0.044 -0.018 0.023 0.731 -0.104 -0.037 -0.109 -0.135 -0.131 -0.103 0.103 -0.079 -0.154 -0.091 -0.039 -0.138 -0.100 0.100 -0.037 -0.182 -0.106 -0.112 -0.104 -0.108 0.108 0.731 -0.064 -0.131 -0.093 -0.106 -0.051 -0.089 0.089 -0.143 -0.145 -0.166 -0.091 -0.145 -0.138 0.138 -0.039 -0.064 -0.200 0.035 -0.160 -0.086 0.100 8 Q 0.879 0.036 0.015 0.034 0.034 0.035 0.031 0.031 0.045 0.028 0.038 0.020 0.055 0.037 0.037 0.029 0.033 0.025 0.025 0.019 0.026 0.026 0.879 0.040 0.013 0.029 0.026 0.033 0.028 0.028 0.028 0.032 0.026 0.017 0.021 0.025 0.025 0.016 0.034 0.032 0.029 0.009 0.024 0.024 0.250 -0.036 -0.068 -0.250 -0.250 0.063 -0.108 0.133 -0.019 -0.050 -0.167 -0.117 0.202 -0.030 0.111 -0.107 -0.102 -0.176 -0.176 -0.015 -0.115 0.115 0.250 0.026 0.058 -0.107 -0.250 -0.102 -0.075 0.109 -0.050 0.008 -0.250 -0.181 -0.179 -0.130 0.134 -0.121 -0.170 -0.043 -0.173 0.013 -0.099 0. 4 1 Q 0.877 0.032 0.039 0.032 0.036 0.043 0.036 0.036 0.032 0.043 0.036 0.039 0.043 0.038 0.038 0.032 0.043 0.039 0.035 0.031 0.036 0.036 0.877 0.026 0.028 0.032 0.036 0.039 0.032 0.032 0.031 0.033 0.035 0.034 0.039 0.034 0.034 0.035 0.032 0.039 0.032 0.038 0.035 0.035 0.279 -0.279 -0.125 -0.279 -0.279 -0.057 -0.204 0.204 -0.279 -0.057 -0.199 -0.125 -0.057 -0.143 0.143 -0.279 -0.119 -0.125 -0.131 -0.202 -0.171 0.171 0.279 -0.141 -0.279 -0.279 -0.199 -0.125 -0.205 0.205 -0.202 0.024 -0.131 -0.072 -0.125 -0.101 0.111 -0.131 -0.279 -0.125 -0.279 -0.065 -0.176 0.176 2 3 Q 0.910 0.012 -0.009 -0.009 0.010 -0.007 -0.001 0.009 0.005 0.011 0.007 -0.002 0.003 0.005 0.006 -0.002 0.020 -0.008 0.005 0.001 0.003 0.007 0.910 0.014 0.007 0.015 0.006 0.008 0.010 0.010 0.007 0.011 0.008 0.012 -0.013 0.005 0.010 0.012 0.008 0.008 0.008 -0.010 0.005 0.009 0.500 -0.125 -0.214 -0.500 -0.278 -0.306 -0.285 0.285 -0.233 -0.088 -0.100 -0.122 -0.147 -0.138 0.138 -0.423 -0.063 -0.258 -0.233 -0.050 -0.205 0.205 0.500 0.013 -0.100 -0.076 -0.068 -0.136 -0.073 0.079 -0.100 -0.056 -0.177 0.078 -0.222 -0.095 0.127 -0.125 -0.177 -0.136 -0.177 -0.141 -0.151 0. 1 . 4 - 5 . 3 - a 0.929 0.014 0.017 0.002 0.002 0.008 0.008 0.008 0.014 0.018 -0.002 0.012 0.005 0.009 0.010 0.021 0.000 0.013 0.018 0.012 0.013 0.013 0.929 0.013 0.008 0.009 -0.002 0.025 0.011 0.011 0.017 0.009 0.021 -0.001 0.025 0.014 0.015 0.009 0.009 0.008 0.010 -0.005 0.006 0.008 FALSE Category 0.864 -0.060 -0.190 -0.142 -0.377 -0.154 -0.185 0.185 -0.246 -0.208 -0.140 -0.247 -0.180 -0.204 0.204 -0.119 -0.159 -0.230 -0.109 -0.197 -0.163 0.163 0.864 -0.060 -0.098 -0.078 -0.169 0.001 -0.081 0.081 -0.316 -0.180 -0.226 -0.298 -0.281 -0.260 0.260 -0.156 -0.187 -0.113 -0.072 -0.115 -0.128 0.128 TRUE Category 0.500 -0.015 -0.224 -0.155 -0.286 -0.177 -0.172 0.172 -0.155 -0.063 -0.167 -0.167 -0.048 -0.120 0.120 -0.188 -0.155 -0.063 -0.155 -0.015 -0.115 0.115 0.500 -0.167 -0.088 -0.029 -0.088 -0.015 -0.078 0.078 -0.113 -0.155 -0.136 -0.233 -0.286 -0.185 0.185 -0.076 -0.155 -0.088 -0.048 -0.063 -0.086 0.086 0.653 0.029 0.070 -0.048 -0.048 0.041 0.009 0.047 0.014 0.058 -0.103 0.067 -0.001 0.007 0.049 0.086 -0.015 0.043 0.058 0.078 0.050 0.056 0.653 0.055 0.041 0.028 -0.082 0.113 0.031 0.064 0.070 0.028 0.086 0.026 0.123 0.067 0.067 0.028 0.028 0.041 -0.002 -0.013 0.016 0.022 0 . 2 - m 0.845 0.072 0.080 0.077 0.057 0.062 0.069 0.069 0.073 0.071 0.079 0.088 0.088 0.080 0.080 0.076 0.076 0.072 0.077 0.055 0.071 0.071 0.845 0.063 0.067 0.093 0.056 0.068 0.069 0.069 0.066 0.079 0.061 0.072 0.087 0.073 0.073 0.063 0.052 0.071 0.081 0.052 0.064 0.064 0.612 -0.016 0.024 -0.345 -0.183 -0.122 -0.129 0.138 -0.027 -0.007 -0.155 -0.021 0.024 -0.037 0.047 -0.088 -0.112 -0.047 -0.138 -0.052 -0.087 0.087 0.612 0.020 -0.054 0.007 -0.059 -0.090 -0.035 0.046 -0.047 0.013 -0.162 -0.034 0.108 -0.024 0.073 -0.054 -0.067 -0.034 -0.027 -0.112 -0.059 0.059 - S D 0.911 0.016 0.019 0.012 0.013 0.021 0.016 0.016 0.020 0.022 0.013 0.014 0.013 0.016 0.016 0.027 0.016 0.020 0.008 -0.013 0.011 0.017 0.911 0.014 0.030 0.013 0.023 0.019 0.020 0.020 0.023 0.021 0.020 0.026 0.031 0.024 0.024 0.000 0.016 0.011 0.034 0.014 0.015 0.015 0.258 0.087 0.180 0.028 -0.027 0.268 -0.258 0.118 0.099 0.227 -0.027 0.166 0.268 0.147 0.158 0.242 0.087 0.099 0.018 -0.023 0.085 0.094 0.258 0.166 0.194 -0.027 0.156 0.180 0.134 0.144 0.156 0.306 0.099 0.271 0.258 0.218 0.218 0.000 0.087 0.075 0.305 0.199 0.133 0.133 2 7 Q 0.911 0.009 0.019 0.014 0.005 0.023 0.014 0.014 0.016 0.018 0.008 0.008 0.002 0.010 0.010 0.013 0.003 0.001 0.021 0.001 0.008 0.008 0.911 0.003 0.003 0.005 -0.010 0.023 0.005 0.009 0.004 0.002 0.011 0.006 -0.003 0.004 0.005 -0.018 0.001 0.010 -0.003 0.005 -0.001 0.008 0.476 -0.002 0.146 -0.052 -0.014 0.160 0.048 0.075 0.095 0.177 0.024 0.048 0.077 0.084 0.084 0.037 0.057 0.095 0.088 0.095 0.075 0.075 0.476 0.036 0.036 -0.014 0.002 0.160 0.044 0.050 0.012 0.077 0.082 0.089 0.066 0.065 0.065 -0.041 -0.055 0.120 -0.066 0.124 0.016 0.081 0 7 l 0.446 0.015 0.000 0.054 0.165 -0.009 0.045 0.048 0.039 -0.046 0.026 -0.102 -0.006 -0.018 0.044 0.051 -0.030 0.035 0.135 0.014 0.041 0.053 0.446 0.051 0.063 0.075 -0.023 -0.006 0.032 0.043 -0.069 -0.031 -0.078 -0.154 -0.036 -0.074 0.074 0.011 0.048 -0.054 0.023 0.042 0.014 0.035 0.462 -0.150 -0.212 -0.176 -0.388 -0.023 -0.190 0.190 -0.129 -0.150 -0.012 -0.224 -0.176 -0.138 0.138 -0.094 -0.144 -0.229 -0.220 -0.098 -0.157 0.157 0.462 -0.062 -0.227 -0.030 -0.018 -0.023 -0.072 0.072 -0.098 -0.062 -0.081 -0.094 -0.112 -0.089 0.089 -0.062 -0.138 -0.007 -0.159 -0.079 -0.089 0.089 7 - t M 0.887 -0.166 -0.078 -0.167 -0.075 -0.085 -0.115 0.115 -0.145 -0.130 -0.126 -0.037 -0.070 -0.102 0.102 -0.127 -0.050 -0.147 -0.028 -0.054 -0.081 0.081 0.887 -0.214 -0.057 -0.173 -0.110 -0.159 -0.142 0.142 -0.108 -0.063 -0.143 -0.025 -0.077 -0.083 0.083 -0.211 -0.070 -0.114 -0.069 -0.052 -0.103 0.103 0.000 0.000 0.083 0.000 0.000 0.057 0.028 0.028 0.000 0.074 0.080 0.207 0.074 0.087 0.087 0.000 0.000 0.077 0.000 0.074 0.030 0.030 0.000 0.000 0.154 0.000 0.000 0.000 0.031 0.031 0.000 0.125 0.154 0.138 0.077 0.099 0.099 0.148 0.000 0.194 0.000 0.074 0.083 0.083 a - t 0.813 0.025 0.028 0.020 0.045 -0.092 0.005 0.042 0.033 -0.164 -0.052 -0.079 -0.170 -0.086 0.099 -0.006 0.000 -0.009 0.034 -0.030 -0.002 0.016 0.813 -0.022 -0.015 -0.074 -0.058 -0.083 -0.050 0.050 -0.054 -0.088 -0.083 -0.051 -0.067 -0.069 0.069 -0.039 0.003 -0.039 0.015 -0.028 -0.018 0. 0.450 -0.063 -0.026 -0.228 -0.228 -0.105 -0.130 0.130 -0.050 -0.117 -0.105 0.007 -0.117 -0.076 0.079 -0.105 0.017 -0.093 0.002 -0.026 -0.041 0.048 0.450 -0.063 -0.026 -0.183 -0.127 -0.026 -0.085 0.085 -0.026 -0.050 -0.127 0.050 0.064 -0.018 0.063 -0.117 -0.174 -0.063 -0.050 -0.026 -0.086 0.086 N - t 0.917 -0.004 0.000 -0.004 -0.004 -0.013 -0.005 0.005 -0.004 -0.004 -0.004 -0.009 -0.009 -0.006 0.006 -0.004 -0.009 -0.004 -0.004 -0.005 -0.005 0.005 0.917 -0.008 -0.004 -0.009 -0.004 0.000 -0.005 0.005 -0.004 -0.009 -0.004 -0.009 0.000 -0.005 0.005 -0.004 -0.004 -0.004 -0.004 -0.009 -0.005 0.005 0.083 -0.003 0.000 -0.083 -0.083 -0.009 -0.036 0.036 -0.003 -0.003 -0.083 -0.006 -0.006 -0.020 0.020 -0.003 -0.006 -0.083 -0.083 0.065 -0.022 0.048 0.083 -0.083 -0.003 -0.006 -0.083 0.000 -0.035 0.035 -0.003 -0.006 -0.083 -0.006 0.000 -0.020 0.020 -0.003 -0.003 -0.083 -0.083 -0.006 -0.036 0. 4 2 - m - t 0.738 0.055 0.087 0.079 0.075 -0.089 0.041 0.077 0.108 0.083 0.131 0.078 0.002 0.081 0.081 0.090 0.102 0.044 0.123 0.040 0.080 0.080 0.738 0.035 -0.023 -0.065 0.023 0.044 0.003 0.038 0.109 0.007 0.076 0.032 -0.068 0.031 0.059 0.116 0.120 0.025 0.106 0.034 0.080 0.080 0.160 -0.160 -0.006 -0.160 -0.160 0.000 -0.097 0.097 -0.006 0.000 0.000 0.136 0.000 0.026 0.028 -0.160 -0.160 0.000 -0.160 -0.077 -0.111 0.111 0.160 -0.083 -0.083 -0.077 -0.077 -0.077 -0.079 0.079 0.062 0.071 0.000 0.136 -0.006 0.053 0.055 -0.160 -0.160 0.000 -0.160 -0.077 -0.111 0.111 7 8 - t 0.670 -0.013 0.043 0.166 0.030 -0.138 0.017 0.078 0.094 -0.071 0.114 -0.080 -0.138 -0.016 0.100 0.156 0.163 0.087 0.147 -0.106 0.089 0.132 0.670 -0.010 0.010 0.013 -0.057 -0.069 -0.023 0.032 -0.053 -0.127 0.112 -0.080 -0.072 -0.044 0.089 -0.059 0.110 -0.007 0.089 -0.199 -0.013 0. 0.250 -0.250 -0.173 -0.167 -0.250 -0.112 -0.190 0.190 -0.250 -0.250 -0.250 -0.250 -0.170 -0.234 0.234 -0.250 -0.167 -0.250 -0.250 -0.250 -0.233 0.233 0.250 -0.170 -0.250 -0.250 -0.250 -0.170 -0.218 0.218 -0.250 -0.250 -0.167 -0.250 -0.043 -0.192 0.192 -0.250 -0.167 -0.090 -0.250 -0.173 -0.186 0.186 2 2 8 - t 0.825 -0.042 -0.142 -0.119 -0.162 -0.073 -0.107 0.107 -0.165 -0.067 -0.234 -0.158 -0.009 -0.126 0.126 -0.086 -0.014 -0.061 -0.045 -0.090 -0.059 0.059 0.825 -0.112 -0.187 -0.002 -0.175 -0.110 -0.117 0.117 -0.169 -0.065 -0.057 -0.107 -0.058 -0.091 0.091 0.027 -0.023 -0.128 -0.081 -0.061 -0.053 0.064 0.258 -0.258 -0.258 -0.175 -0.110 0.075 -0.145 0.175 -0.258 -0.104 -0.258 0.075 -0.110 -0.131 0.161 -0.258 -0.258 -0.184 -0.258 0.028 -0.186 0.197 0.258 -0.110 -0.120 -0.178 -0.258 -0.189 -0.171 0.171 -0.184 -0.051 -0.098 -0.098 0.213 -0.044 0.129 -0.104 -0.110 -0.125 -0.027 -0.110 -0.095 0.095 AM MAV 0.855 0.002 -0.002 0.002 -0.005 -0.021 -0.005 0.043 -0.002 -0.016 -0.002 -0.019 -0.015 -0.011 0.048 0.008 0.010 -0.004 0.021 -0.015 0.004 0.039 0.855 -0.013 -0.009 -0.004 -0.019 -0.005 -0.010 0.036 -0.022 -0.015 -0.009 -0.025 -0.023 -0.019 0.047 -0.007 0.009 -0.011 0.014 -0.013 -0.002 0.036 0.412 -0.085 -0.067 -0.211 -0.181 -0.045 -0.134 0.145 -0.103 -0.040 -0.122 -0.035 -0.037 -0.068 0.114 -0.123 -0.095 -0.100 -0.139 -0.048 -0.101 0.127 0.412 -0.062 -0.071 -0.095 -0.109 -0.046 -0.077 0.106 -0.083 -0.026 -0.105 -0.040 -0.038 -0.058 0.117 -0.087 -0.114 -0.065 -0.091 -0.045 -0.080 0.109 0.855 0.031 0.040 0.047 0.054 0.043 0.036 0.043 0.050 0.050 0.051 0.047 0.043 0.042 0.048 0.042 0.038 0.040 0.041 0.033 0.034 0.039 0.855 0.041 0.032 0.033 0.038 0.036 0.032 0.036 0.053 0.040 0.047 0.047 0.047 0.043 0.047 0.039 0.036 0.036 0.035 0.036 0.030 0.036 0.412 0.099 0.118 0.214 0.181 0.113 0.142 0.145 0.122 0.098 0.132 0.107 0.110 0.100 0.114 0.156 0.110 0.133 0.154 0.080 0.123 0.127 0.412 0.094 0.118 0.099 0.124 0.093 0.098 0.106 0.111 0.091 0.143 0.116 0.126 0.104 0.117 0.103 0.124 0.104 0.122 0.091 0.103 0.109 Table 10: Results on MFMD-persona: Bengali part. Role Scenario RetailInvestor Professional CompanyOwner RetailInvestor Professional CompanyOwner Base Europe USA AsiaPacific ChinaMainland Australia UAE AM MAV Europe USA AsiaPacific ChinaMainland Australia UAE AM MAV Europe USA AsiaPacific ChinaMainland Australia UAE AM MAV Base Europe USA AsiaPacific ChinaMainland Australia UAE AM MAV Europe USA AsiaPacific ChinaMainland Australia UAE AM MAV Europe USA AsiaPacific ChinaMainland Australia UAE AM MAV - 8 Q 0.902 0.019 0.022 0.008 0.006 0.017 0.030 0.017 0.017 0.014 0.002 0.001 0.006 0.030 -0.007 0.008 0.010 0.009 -0.012 0.006 0.006 0.010 0.006 0.004 0. 0.455 -0.043 0.031 -0.152 -0.077 0.069 0.086 -0.014 0.076 0.007 -0.087 -0.055 -0.077 0.124 -0.105 -0.032 0.076 -0.197 -0.074 -0.045 -0.045 -0.034 -0.045 -0.073 0.073 Role Scenario RetailInvestor Base American-Christianity American-Islam American-Buddhism American-Judaism European-Christianity Jewish-Judaism Chinese-Christianity Chinese-Islam Chinese-Buddhism Indian-Christianity Indian-Islam Arab-Islam Latino/Hispanic-Christianity African-Christianity African-Islam AM MAV CompanyOwner American-Christianity American-Islam American-Buddhism American-Judaism European-Christianity Jewish-Judaism Chinese-Christianity Chinese-Islam Chinese-Buddhism Indian-Christianity Indian-Islam Arab-Islam Latino/Hispanic-Christianity African-Christianity African-Islam AM MAV RetailInvestor Base American-Christianity American-Islam American-Buddhism American-Judaism European-Christianity Jewish-Judaism Chinese-Christianity Chinese-Islam Chinese-Buddhism Indian-Christianity Indian-Islam Arab-Islam Latino/Hispanic-Christianity African-Christianity African-Islam AM MAV CompanyOwner American-Christianity American-Islam American-Buddhism American-Judaism European-Christianity Jewish-Judaism Chinese-Christianity Chinese-Islam Chinese-Buddhism Indian-Christianity Indian-Islam Arab-Islam Latino/Hispanic-Christianity African-Christianity African-Islam AM MAV - 4 1 Q 0.919 -0.011 0.003 -0.010 -0.006 -0.004 -0.008 -0.006 0.007 -0.019 0.001 -0.015 -0.003 -0.004 -0.008 -0.008 0.008 0.000 -0.004 -0.014 -0.008 -0.008 0.000 -0.006 0.006 0.500 -0.357 -0.125 -0.500 -0.420 -0.233 -0.293 -0.321 0.321 -0.429 -0.278 -0.426 -0.346 -0.286 -0.242 -0.334 0.334 -0.224 -0.286 -0.500 -0.293 -0.293 -0.224 -0.303 0. - 8 Q 0.902 0.006 0.017 0.006 -0.010 0.035 0.019 0.011 0.004 -0.012 0.027 0.009 -0.006 0.007 0.017 -0.013 0.008 0.013 -0.008 0.001 0.004 -0.019 -0.002 -0.006 -0.019 -0.005 0.009 0.009 -0.017 -0.036 0.002 -0.017 -0.028 -0.009 0.012 0.455 -0.045 -0.179 -0.045 -0.147 0.101 -0.011 -0.066 -0.205 -0.279 0.045 0.021 -0.139 -0.112 0.045 -0.233 -0.083 0.111 -0.074 -0.055 0.010 -0.162 -0.131 -0.139 -0.162 -0.011 0.021 -0.005 -0.091 -0.095 -0.087 -0.284 -0.211 -0.098 0.103 - 2 3 Q 0.900 0.019 -0.017 0.019 0.000 0.002 0.002 0.004 0.010 0.002 0.014 -0.002 0.002 0.008 0.002 0.004 0.005 0.005 0.007 0.022 0.019 0.014 0.014 0.014 0.014 0.500 0.000 -0.083 0.024 -0.176 -0.071 -0.071 -0.063 0.071 -0.045 0.033 -0.081 -0.045 -0.090 -0.045 -0.046 0.057 0.011 -0.061 0.058 0.000 0.012 0.012 0.005 0. - 4 1 Q 0.919 -0.001 0.002 -0.012 0.006 -0.011 -0.020 -0.008 -0.001 -0.004 -0.009 -0.001 -0.013 -0.008 -0.016 -0.008 -0.007 0.008 -0.005 -0.013 -0.008 -0.016 0.003 0.003 -0.013 -0.012 -0.004 -0.013 -0.010 -0.001 -0.008 -0.009 -0.008 -0.008 0.009 0.500 -0.177 -0.088 -0.300 -0.076 -0.357 -0.367 -0.293 -0.177 -0.233 -0.197 -0.136 -0.250 -0.242 -0.362 -0.242 -0.233 0.233 -0.188 -0.206 -0.242 -0.306 -0.125 -0.125 -0.206 -0.300 -0.233 -0.250 -0.157 -0.136 -0.242 -0.197 -0.293 -0.214 0.214 - m - 5 - 0.921 0.001 0.005 -0.007 0.010 -0.007 -0.017 -0.003 0.008 0.005 0.005 0.001 0.018 0.000 0.010 0.007 0.007 0.019 -0.001 0.004 -0.017 0.001 0.010 0.003 0. 0.596 -0.018 0.013 -0.063 0.009 -0.063 -0.065 -0.031 0.038 -0.005 -0.005 -0.038 0.055 0.000 0.009 0.003 0.019 0.019 0.031 0.029 -0.065 -0.038 0.009 -0.002 0.032 - 2 3 Q 0.900 0.014 0.012 0.002 0.003 0.009 0.001 0.007 -0.012 0.019 0.002 -0.003 0.012 0.003 -0.007 -0.008 0.003 0.008 0.008 0.025 0.014 0.011 0.011 -0.007 0.002 0.008 0.001 -0.004 0.002 0.029 -0.007 0.005 0.021 0.008 0.010 0.500 0.033 0.071 -0.045 0.049 0.022 -0.022 -0.061 -0.074 0.024 -0.045 -0.056 0.071 0.049 -0.065 -0.042 -0.006 0.049 0.042 0.125 0.012 -0.024 -0.024 -0.091 -0.035 0.060 -0.022 -0.022 -0.071 0.153 -0.065 -0.011 0.096 0.008 0.057 - 5 . 4 - a 0.908 0.026 0.030 0.026 0.030 0.026 0.042 0.030 0.030 0.033 0.025 0.039 0.025 0.016 0.021 0.026 0.026 0.030 0.026 0.021 0.034 0.029 0.025 0.028 0. 0.542 0.110 0.139 0.094 0.139 0.110 0.208 0.133 0.133 0.178 0.138 0.169 0.111 0.098 0.096 0.132 0.132 0.125 0.110 0.096 0.166 0.152 0.125 0.129 0.129 - m - 5 - 0.921 -0.004 0.026 0.022 0.013 0.005 0.022 0.001 -0.026 0.008 0.010 0.005 0.000 -0.013 0.013 0.001 0.005 0.011 -0.003 0.013 0.005 0.014 0.001 -0.016 0.005 0.006 0.000 0.014 0.022 0.014 0.017 0.000 0.022 0.008 0.010 0.596 -0.031 0.115 0.086 0.056 -0.005 0.071 -0.018 -0.086 0.057 0.009 -0.005 0.000 -0.036 0.040 -0.018 0.016 0.042 -0.051 0.040 0.013 0.023 -0.038 -0.085 0.013 -0.025 0.000 0.023 0.086 0.023 0.071 0.000 0.071 0.011 0.037 - 5 . 2 - m 0.918 -0.006 -0.003 -0.010 0.003 0.006 -0.013 -0.004 0.007 -0.021 0.000 0.007 0.014 0.014 0.001 0.003 0.009 0.014 -0.010 0.003 0.014 0.002 0.014 0.006 0.009 0.679 -0.108 -0.064 -0.119 -0.067 -0.039 -0.168 -0.094 0.094 -0.123 -0.024 -0.054 0.013 0.025 -0.037 -0.034 0.046 0.013 -0.035 -0.067 0.013 -0.052 0.025 -0.017 0.034 - S D 0.917 0.007 0.007 0.026 0.021 0.021 0.026 0.018 0.018 0.003 0.002 -0.006 0.013 0.003 -0.006 0.002 0.005 -0.006 0.008 0.025 0.008 0.007 0.012 0.009 0.011 0.578 0.062 0.076 0.089 0.103 0.103 0.104 0.089 0.089 0.049 0.064 0.010 0.044 0.049 0.010 0.038 0.038 0.010 0.047 0.130 0.047 0.062 0.075 0.062 0.062 8 Q 0.903 0.018 0.032 0.002 0.018 0.018 0.013 0.017 0.017 0.005 0.013 -0.012 0.006 0.024 -0.008 0.005 0.011 -0.001 -0.008 0.004 0.010 0.010 -0.008 0.001 0.007 0.194 0.218 0.406 0.139 0.218 0.250 0.238 0.245 0.245 0.216 0.238 0.147 0.159 0.319 0.156 0.206 0.206 0.048 0.156 0.245 0.195 0.159 0.156 0.160 0.160 4 1 Q 0.888 0.023 0.022 0.020 0.027 0.027 0.023 0.024 0.024 0.016 0.028 0.028 0.024 0.020 0.027 0.024 0.024 0.023 0.023 0.017 0.020 0.023 0.024 0.022 0.022 0.426 -0.219 -0.123 -0.349 -0.212 -0.159 -0.219 -0.214 0.214 -0.352 -0.272 -0.272 -0.278 -0.283 -0.159 -0.269 0.269 -0.219 -0.219 -0.426 -0.283 -0.219 -0.278 -0.274 0.274 2 3 Q 0.892 0.022 0.030 0.024 0.012 0.019 0.015 0.020 0.020 0.027 0.031 0.043 0.027 0.027 0.030 0.031 0.031 -0.005 0.005 0.030 0.032 0.026 0.035 0.021 0.022 0.458 0.054 0.120 -0.026 -0.090 -0.008 -0.019 0.005 0.053 0.042 0.079 0.161 0.042 0.066 0.100 0.082 0.082 -0.009 -0.014 0.100 0.028 0.087 0.092 0.048 0.055 1 . 4 - 5 . 3 - a FALSE Category 0.933 0.938 -0.008 0.000 -0.009 0.016 0.004 0.008 -0.006 -0.021 0.004 0.004 -0.016 0.001 -0.005 0.001 0.008 0.008 0.000 0.021 0.000 0.004 0.017 0.000 -0.007 -0.004 -0.012 0.000 -0.006 0.000 -0.001 0.003 0.007 0.005 -0.011 -0.004 -0.004 -0.009 0.005 -0.018 -0.007 0.008 0.009 0.001 -0.003 -0.001 -0.002 -0.004 0.007 0.006 TRUE Category 0.651 0.681 -0.101 -0.014 -0.060 0.103 -0.029 0.042 -0.138 -0.116 -0.080 0.015 -0.101 -0.030 -0.085 0.000 0.085 0.053 -0.066 0.092 -0.032 0.027 -0.051 -0.014 -0.093 -0.045 -0.101 0.000 -0.177 0.000 -0.087 0.010 0.087 0.030 -0.125 -0.029 -0.101 -0.043 0.000 -0.054 -0.110 0.042 -0.072 -0.047 -0.125 0.013 -0.089 -0.019 0.089 0.038 0 . 2 - m 0.891 0.028 0.051 0.010 0.048 0.040 0.016 0.032 0.032 0.036 0.043 0.026 0.020 0.034 0.039 0.033 0.033 0.028 0.030 0.030 0.024 0.023 0.028 0.027 0.027 0.627 -0.090 0.009 -0.183 0.024 -0.022 -0.188 -0.075 0.086 -0.077 0.025 -0.062 -0.151 -0.002 -0.005 -0.045 0.054 -0.165 -0.031 -0.031 -0.177 -0.094 -0.103 -0.100 0.100 - S D 0.921 -0.001 0.013 -0.002 -0.008 0.003 -0.006 0.000 0.005 -0.002 0.028 0.013 -0.004 0.006 0.012 0.009 0.011 0.002 0.021 0.013 -0.004 -0.009 0.009 0.005 0.010 0.364 -0.210 0.088 -0.088 -0.364 -0.133 -0.150 -0.143 0.172 -0.088 0.242 0.088 -0.281 -0.007 0.150 0.017 0.143 -0.078 0.103 0.050 -0.281 -0.216 0.074 -0.058 0.133 2 7 Q 0.926 0.001 -0.013 -0.016 -0.008 -0.005 -0.012 -0.009 0.009 -0.005 -0.002 -0.009 -0.004 -0.017 -0.004 -0.007 0.007 -0.035 -0.033 -0.018 -0.004 -0.031 -0.010 -0.022 0.022 0.591 -0.020 -0.038 -0.091 -0.046 0.005 -0.058 -0.041 0.043 0.021 0.049 -0.026 -0.033 -0.069 -0.033 -0.015 0.039 -0.111 -0.046 -0.049 -0.033 -0.101 0.009 -0.055 0.058 0 7 l 0.928 0.019 0.013 0.027 0.012 0.019 0.009 0.016 0.016 0.015 0.022 0.005 0.004 0.002 0.027 0.012 0.012 0.006 -0.018 -0.001 0.007 0.018 0.011 0.004 0.010 0.667 0.044 0.053 0.065 -0.052 0.031 -0.111 0.005 0.059 0.015 0.072 0.013 -0.103 -0.045 0.077 0.005 0.054 -0.015 -0.049 -0.117 -0.067 0.056 -0.016 -0.035 0.053 7 - t M 0.906 0.023 -0.001 0.004 0.008 0.011 -0.008 0.006 0.009 0.023 0.010 0.001 0.009 0.015 -0.010 0.008 0.011 -0.006 0.005 0.001 0.008 -0.010 0.006 0.001 0.006 0.313 0.125 0.030 -0.001 -0.037 0.062 -0.078 0.017 0.055 0.111 0.144 0.097 -0.099 0.111 -0.035 0.055 0.100 0.020 0.097 -0.106 -0.046 -0.027 0.030 -0.005 0.055 Table 11: Results on MFMD-region. a - t M 0.872 0.017 0.045 -0.017 0.038 0.020 -0.056 0.008 0.032 0.030 0.002 -0.051 -0.010 0.032 -0.129 -0.021 0.042 -0.049 0.036 0.012 0.054 -0.042 0.012 0.004 0.034 0.605 -0.010 -0.005 -0.131 -0.060 -0.148 -0.131 -0.081 0.081 -0.120 -0.005 -0.020 -0.034 -0.034 -0.026 -0.040 0.040 -0.034 0.014 -0.020 0.062 -0.081 -0.005 -0.011 0.036 N - t 0.910 -0.005 0.011 0.012 0.001 0.011 0.001 0.005 0.007 0.003 0.006 0.002 -0.004 0.006 -0.001 0.002 0.004 0.008 0.003 -0.002 -0.009 0.006 -0.010 -0.001 0. 0.500 -0.167 -0.056 -0.125 -0.242 -0.056 -0.242 -0.148 0.148 -0.147 -0.068 -0.079 -0.250 -0.068 -0.157 -0.128 0.128 0.045 0.053 -0.122 -0.214 -0.038 -0.141 -0.069 0.102 4 2 - m - t 0.673 0.227 0.251 0.256 0.240 0.248 0.243 0.244 0.244 0.240 0.240 0.185 0.246 0.236 0.221 0.228 0.228 0.235 0.248 0.254 0.248 0.235 0.235 0.243 0.243 0.194 0.109 0.306 0.244 0.170 0.193 0.170 0.198 0.198 0.082 0.218 0.114 0.320 0.139 0.230 0.184 0.184 0.139 0.181 0.347 0.170 0.218 0.159 0.202 0.202 7 8 - t M 0.828 -0.186 -0.175 -0.145 -0.079 -0.261 -0.127 -0.162 0.162 -0.131 -0.024 -0.028 -0.186 -0.100 -0.085 -0.092 0.092 -0.060 -0.008 -0.105 -0.017 -0.021 -0.057 -0.045 0.045 0.412 -0.145 -0.245 0.027 -0.126 -0.190 -0.207 -0.148 0.157 -0.156 -0.059 0.088 -0.412 -0.236 -0.059 -0.139 0.168 -0.096 -0.044 -0.207 -0.274 -0.079 -0.133 -0.139 0.139 2 2 8 - t 0.932 -0.008 0.009 -0.018 -0.027 -0.036 -0.011 -0.015 0.018 -0.026 -0.006 -0.024 -0.048 -0.064 -0.030 -0.033 0.033 -0.003 -0.006 -0.047 -0.039 -0.011 -0.015 -0.020 0.020 0.222 0.192 0.192 0.074 -0.008 0.192 0.135 0.130 0.132 0.009 0.074 0.192 0.192 0.294 0.178 0.156 0.156 0.192 0.135 -0.062 0.009 0.192 0.135 0.100 0.121 AM MAV 0.897 0.010 0.015 0.010 0.014 0.008 0.007 0.011 0.032 0.012 0.020 0.010 0.007 0.013 0.004 0.011 0.028 0.009 0.014 0.011 0.018 0.013 0.015 0.013 0.026 0.489 -0.027 0.035 -0.048 -0.071 -0.008 -0.054 -0.029 0.118 -0.036 0.026 -0.004 -0.059 0.000 -0.003 -0.013 0.111 -0.031 -0.003 -0.034 -0.053 -0.021 -0.007 -0.025 0.103 0.897 0.031 0.035 0.031 0.029 0.037 0.032 0.029 0.032 0.031 0.023 0.023 0.031 0.030 0.031 0.026 0.028 0.025 0.024 0.030 0.027 0.025 0.025 0.022 0.026 0.489 0.110 0.107 0.121 0.131 0.101 0.140 0.104 0.118 0.110 0.102 0.103 0.145 0.111 0.095 0.093 0.111 0.088 0.088 0.130 0.119 0.106 0.090 0.089 0.103 - 5 . 4 - a 0.908 0.025 0.042 0.021 0.025 0.025 0.030 0.026 0.021 0.029 0.018 0.025 0.029 0.018 0.018 0.009 0.024 0.024 0.025 0.020 0.033 0.025 0.021 -0.007 0.015 0.017 0.020 0.038 0.013 0.017 0.033 0.017 0.030 0.021 0.022 0.542 0.125 0.218 0.111 0.125 0.125 0.139 0.110 0.111 0.152 0.049 0.139 0.152 0.049 0.067 0.041 0.114 0.114 0.138 0.125 0.178 0.125 0.111 0.083 0.150 0.083 0.125 0.181 0.054 0.083 0.178 0.083 0.139 0.122 0.122 - 5 . 2 - m 0.918 0.014 -0.010 -0.031 0.004 0.006 0.019 0.003 -0.002 0.015 0.019 -0.012 0.001 0.019 0.006 -0.011 0.003 0.012 0.000 0.018 0.010 0.000 0.004 0.023 0.014 0.013 0.004 0.000 0.022 0.004 0.009 0.019 0.014 0.010 0.010 0.679 0.013 -0.119 -0.127 0.000 -0.039 0.015 -0.067 -0.079 0.001 0.015 -0.086 -0.037 0.027 -0.039 -0.102 -0.042 0.051 -0.024 0.038 0.000 -0.012 0.000 0.052 0.013 0.035 0.011 -0.012 0.062 0.000 0.012 0.015 0.013 0.013 0.020 - S D 0.917 0.030 0.020 0.021 0.016 0.021 0.016 0.034 0.012 0.008 0.007 0.021 0.025 0.009 0.020 0.021 0.019 0.019 0.009 0.004 0.000 0.012 0.012 -0.006 -0.001 0.004 0.007 0.008 -0.001 -0.009 0.025 -0.001 -0.009 0.004 0.007 0.578 0.133 0.116 0.089 0.103 0.089 0.102 0.149 0.075 0.060 0.062 0.103 0.130 0.013 0.116 0.089 0.095 0.095 0.031 0.034 0.005 0.075 0.060 0.010 0.022 0.018 0.062 0.060 0.022 -0.018 0.130 0.022 -0.018 0.034 0.039 8 Q 0.903 0.009 0.015 0.008 0.002 0.013 0.013 0.013 -0.003 0.009 0.009 0.016 0.015 0.001 0.013 -0.002 0.009 0.009 -0.008 -0.022 0.001 -0.005 0.000 -0.021 -0.010 -0.041 -0.032 -0.020 -0.009 -0.021 0.008 -0.029 -0.009 -0.014 0.016 0.194 0.227 0.170 0.256 0.139 0.268 0.238 0.268 0.174 0.227 0.227 0.306 0.170 0.174 0.238 0.092 0.212 0.212 0.165 0.162 0.174 0.225 0.206 0.132 0.215 0.133 0.006 0.232 0.147 0.246 0.256 0.050 0.187 0.169 0.169 4 1 Q 0.888 0.034 0.023 0.027 0.026 0.023 0.015 0.023 0.023 0.027 0.030 0.023 0.023 0.023 0.023 0.027 0.025 0.025 0.026 0.018 0.027 0.010 0.034 0.030 0.014 0.018 0.027 0.018 0.017 0.026 0.025 0.026 0.027 0.023 0.023 0.426 -0.093 -0.168 -0.212 -0.114 -0.168 -0.288 -0.219 -0.168 -0.212 -0.103 -0.168 -0.168 -0.159 -0.219 -0.159 -0.175 0.175 -0.114 -0.176 -0.159 -0.239 -0.051 -0.103 -0.184 -0.132 -0.159 -0.176 -0.093 -0.114 -0.073 -0.114 -0.159 -0.136 0.136 2 3 Q FALSE Category 1 . 4 - 5 . 3 - a 0.892 0.026 0.021 0.013 0.021 0.030 0.006 0.030 0.029 0.013 0.008 0.017 0.009 -0.005 0.002 0.001 0.015 0.015 0.016 0.038 0.030 0.004 0.003 0.003 -0.004 0.012 0.013 0.012 0.027 0.017 -0.009 0.025 0.043 0.015 0. 0.938 -0.005 0.009 -0.001 0.012 -0.009 0.012 0.004 -0.001 0.007 -0.001 0.012 -0.018 -0.005 0.004 0.004 0.001 0.007 0.008 -0.001 0.007 0.003 0.003 0.007 0.016 0.007 0.004 -0.001 0.008 0.016 0.012 0.016 0.008 0.008 0.008 TRUE Category 0.458 0.087 0.095 0.053 0.095 0.120 -0.039 0.120 0.138 0.031 0.042 0.064 0.020 -0.009 -0.077 -0.023 0.048 0.068 0.113 0.164 0.120 0.010 0.032 0.052 -0.032 0.073 0.031 0.084 0.066 0.064 -0.041 0.107 0.161 0.067 0.077 0.681 -0.001 0.030 0.013 0.079 -0.028 0.079 0.027 0.013 0.064 0.025 0.069 -0.054 -0.001 0.027 0.015 0.024 0.035 0.054 0.025 0.074 0.050 0.039 0.074 0.085 0.064 0.015 0.013 0.042 0.085 0.069 0.095 0.042 0.055 0.055 0.933 -0.001 -0.012 -0.025 -0.011 -0.027 -0.014 0.010 0.000 -0.002 -0.008 -0.008 -0.012 0.005 -0.016 0.010 -0.008 0.011 -0.018 -0.005 0.017 0.008 -0.018 -0.009 -0.012 -0.013 -0.009 0.004 0.009 -0.009 -0.022 -0.017 -0.005 -0.007 0.012 0.651 0.045 -0.080 -0.101 -0.138 -0.165 0.031 -0.125 -0.051 -0.087 -0.066 -0.051 -0.127 0.032 -0.066 0.016 -0.062 0.079 -0.046 -0.046 0.031 0.016 -0.046 0.032 -0.051 -0.106 -0.032 -0.066 -0.002 -0.051 -0.032 -0.051 0.032 -0.028 0. 0 . 2 - m 0.891 0.030 0.038 0.047 0.042 0.031 0.026 0.023 0.020 0.040 0.047 0.036 0.046 0.042 0.040 0.036 0.036 0.036 0.038 0.055 0.042 0.050 0.031 0.037 0.030 0.038 0.031 0.022 0.039 0.046 0.020 0.046 0.038 0.038 0.038 0.627 -0.015 0.011 0.040 0.040 -0.127 -0.094 -0.082 -0.165 -0.022 0.054 -0.056 0.040 0.026 -0.042 -0.077 -0.031 0.059 0.025 0.096 0.053 0.104 -0.049 0.040 -0.015 0.011 -0.127 -0.062 -0.005 0.067 -0.039 0.011 -0.002 0.007 0.047 - S D 0.921 -0.003 0.020 0.013 0.000 0.009 -0.001 0.003 -0.002 -0.001 0.001 -0.006 -0.005 0.009 0.010 0.002 0.003 0.006 0.023 0.012 0.012 -0.003 0.012 0.009 0.005 0.009 0.005 0.000 0.016 0.012 0.012 0.016 0.020 0.011 0.011 0.364 -0.088 0.152 0.050 0.060 0.036 -0.133 -0.204 -0.068 -0.281 -0.078 -0.133 -0.210 -0.007 -0.007 -0.068 -0.065 0.105 0.247 0.150 0.121 0.000 0.107 0.036 0.023 0.074 0.023 0.074 0.165 0.136 0.165 0.165 0.165 0.110 0. 2 7 Q 0.926 -0.010 0.003 -0.021 -0.009 -0.010 -0.012 -0.009 0.001 0.000 -0.008 -0.012 -0.008 0.001 -0.005 -0.016 -0.008 0.008 -0.047 -0.020 -0.034 -0.036 -0.026 -0.011 -0.021 -0.034 -0.033 -0.030 -0.039 -0.029 -0.039 -0.010 -0.038 -0.030 0.030 0.591 0.009 0.062 -0.080 -0.026 0.009 -0.079 -0.026 -0.020 0.000 -0.046 -0.079 -0.046 -0.020 0.005 -0.091 -0.028 0.040 -0.091 0.002 -0.030 -0.091 0.005 0.024 0.016 -0.030 -0.046 -0.005 -0.039 -0.035 -0.039 0.009 -0.055 -0.027 0.034 0 7 l 0.928 0.015 0.011 0.019 0.010 0.019 0.012 0.012 0.019 0.016 0.020 0.016 0.012 0.019 0.011 0.019 0.015 0.015 0.005 0.026 0.019 0.013 0.022 0.022 0.035 0.015 0.023 0.014 0.006 0.027 0.022 0.015 0.031 0.020 0.020 0.667 0.000 -0.033 0.016 0.000 0.016 -0.052 -0.052 0.016 -0.035 0.000 -0.017 -0.052 0.016 -0.016 0.031 -0.011 0.023 0.013 0.099 0.044 0.053 0.072 0.072 0.133 0.000 0.060 0.029 -0.031 0.077 0.072 0.015 0.106 0.054 0. 7 - t 0.906 0.005 -0.035 -0.004 -0.008 0.004 -0.014 -0.024 -0.024 -0.019 -0.029 -0.022 -0.035 -0.017 -0.014 -0.025 -0.017 0.019 0.001 -0.016 0.007 0.002 0.001 -0.019 -0.017 -0.028 -0.007 -0.017 -0.004 -0.009 -0.007 -0.009 -0.016 -0.009 0.011 0.313 0.030 0.003 -0.019 -0.035 0.020 0.076 -0.027 0.030 -0.010 0.030 -0.019 -0.019 0.020 0.097 -0.019 0.011 0.030 0.065 -0.019 -0.010 0.030 0.076 0.126 0.020 -0.027 0.020 0.055 0.011 0.097 0.055 0.087 0.055 0.043 0.050 Table 12: Results on MFMD-identity. a - t M 0.872 0.041 0.054 0.063 0.054 0.033 0.047 0.057 0.059 0.067 0.032 0.053 0.067 0.063 0.059 0.063 0.054 0.054 0.035 0.009 0.049 0.045 0.045 0.042 0.023 0.027 0.033 0.018 0.013 0.049 0.036 0.032 0.027 0.032 0.032 0.605 0.081 0.044 0.062 0.062 0.006 -0.076 0.062 0.027 0.044 -0.010 0.045 0.044 0.062 0.044 0.044 0.036 0.047 0.010 -0.055 -0.010 0.010 -0.026 -0.064 0.027 -0.034 -0.026 0.062 -0.026 0.010 0.095 -0.026 0.010 -0.003 0.033 N - t M 0.910 0.007 0.007 0.007 0.011 0.011 0.011 0.007 0.007 0.003 0.011 0.008 0.011 0.011 0.011 0.007 0.009 0.009 -0.015 -0.037 -0.028 -0.019 -0.018 -0.019 -0.033 -0.037 -0.037 -0.032 -0.032 -0.028 -0.024 -0.032 -0.037 -0.029 0.029 0.500 -0.100 -0.100 -0.100 -0.088 -0.088 -0.088 -0.100 -0.100 -0.147 -0.088 -0.136 -0.088 -0.088 -0.088 -0.100 -0.100 0.100 -0.010 -0.077 -0.060 -0.020 -0.042 -0.020 -0.047 -0.077 -0.077 -0.069 -0.069 -0.060 -0.029 -0.069 -0.077 -0.053 0.053 4 2 - m - t 0.673 0.251 0.249 0.248 0.248 0.248 0.249 0.252 0.249 0.248 0.249 0.253 0.242 0.245 0.245 0.249 0.248 0.248 0.255 0.238 0.232 0.263 0.259 0.248 0.231 0.252 0.233 0.227 0.243 0.233 0.263 0.236 0.248 0.244 0.244 0.194 0.218 0.139 0.181 0.193 0.218 0.028 0.139 0.082 0.139 0.082 0.092 0.020 0.082 0.020 0.020 0.110 0.110 0.230 0.277 0.320 0.335 0.291 0.206 0.170 0.193 0.082 0.170 0.291 0.073 0.377 0.170 0.193 0.225 0. 7 8 - t 0.828 -0.028 -0.084 -0.057 -0.059 -0.112 -0.141 -0.059 -0.057 -0.074 -0.067 -0.079 -0.076 -0.090 -0.057 -0.083 -0.075 0.075 -0.087 -0.065 -0.112 -0.112 -0.036 -0.135 -0.060 -0.014 -0.102 -0.006 -0.024 -0.048 -0.117 -0.017 -0.059 -0.066 0.066 0.412 -0.062 0.053 -0.056 0.027 -0.071 -0.298 0.053 -0.062 -0.196 -0.048 -0.079 -0.071 -0.079 -0.022 -0.062 -0.065 0.083 -0.012 -0.002 -0.048 -0.119 -0.112 -0.053 -0.053 -0.022 0.014 -0.022 0.007 0.121 0.032 0.197 -0.022 -0.006 0.056 2 2 8 - t 0.932 -0.018 -0.040 -0.024 -0.037 -0.042 -0.042 -0.042 -0.053 -0.053 -0.042 -0.081 -0.043 -0.038 -0.033 -0.076 -0.044 0.044 -0.009 -0.016 -0.034 -0.021 -0.004 -0.051 -0.034 -0.031 -0.031 -0.021 -0.040 -0.040 -0.020 -0.013 -0.032 -0.027 0.027 0.222 0.216 0.178 0.192 0.245 0.192 0.178 0.192 0.245 0.135 0.230 0.216 0.245 0.178 0.178 0.165 0.199 0.199 0.192 0.009 0.074 0.123 0.074 0.178 0.135 0.135 0.009 0.123 0.111 0.123 0.192 0.135 0.123 0.116 0. AM MAV 0.897 0.020 0.018 0.016 0.016 0.014 0.012 0.017 0.012 0.016 0.015 0.012 0.013 0.014 0.016 0.009 0.015 0.031 0.011 0.013 0.013 0.010 0.016 0.005 0.007 0.010 0.007 0.011 0.012 0.012 0.011 0.014 0.013 0.011 0.030 0.489 0.027 0.031 0.003 0.029 0.008 -0.027 -0.010 -0.016 -0.026 0.008 0.001 -0.017 -0.001 -0.006 -0.033 -0.002 0.094 0.033 0.032 0.030 0.009 0.020 0.020 0.011 0.005 -0.011 0.019 0.022 0.039 0.048 0.019 0.025 0.021 0.084 0.897 0.026 0.034 0.031 0.029 0.033 0.034 0.030 0.028 0.031 0.030 0.033 0.032 0.030 0.029 0.031 0.029 0.031 0.029 0.031 0.033 0.031 0.026 0.033 0.028 0.029 0.030 0.024 0.028 0.033 0.034 0.028 0.035 0.029 0.030 0.489 0.083 0.101 0.101 0.086 0.103 0.114 0.112 0.098 0.111 0.071 0.094 0.098 0.067 0.085 0.079 0.080 0.094 0.088 0.090 0.081 0.098 0.078 0.082 0.082 0.075 0.055 0.081 0.075 0.085 0.107 0.087 0.101 0.073 0."
        },
        {
            "title": "CompanyOwn",
            "content": "s n a fi s n a fi s n a fi 1 0 0 0 0 1 1 1 B 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 1 1 1 0 1 1 1 0 1 1 0 1 0 0 d g r n h g o A r n n r s n r s e d fi r e d fi r e e s fi e A r s H Claim: In 2020, PepsiCo gave $100 million to entities or projects associated with the Black Lives Matter movement. 1 0 0 0 0 1 0 0 0 0 0 0 Claim: McDonalds and K-pop band BTS announced meal collaboration to be released in May 2021. 1 1 1 0 1 1 1 1 1 1 1 1 Claim: congratulatory text message from Amazon claims you have come second or third in raffle to win free AirPods and asks you to click link. 0 1 0 0 0 0 0 0 Claim: In 1986, then-U.S. Sen. Joe Biden said, [Supporting Israel] is the best $3 billion investment we make. Were there not an Israel, the United States of America would have to invent an Israel to protect her interests in the region. 0 0 1 1 1 1 1 1 Claim: The U.S. government has been funding \"toilets in Africa.\" 1 0 1 1 1 1 1 1 0 0 1 0 0 0 0 0 0 1 1 0 1 1 1 1 1 1 1 0 0 1 0 0 0 1 1 0 1 0 1 0 1 1 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 1 1 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 1 1 1 0 0 1 1 0 1 1 1 1 1 1 0 1 1 0 0 1 1 0 1 0 1 1 0 1 1 1 1 0 1 1 0 0 1 0 1 1 1 1 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 1 1 1 1 0 1 1 1 0 0 1 0 1 1 1 0 0 0 0 0 1 0 1 1 1 1 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 1 1 1 1 1 1 1 1 models qwen3-8b qwen3-32b gpt-5 GPT-4.1 qwen3-8b qwen3-32b gpt-5 GPT-4. qwen3-8b qwen3-32b gpt-5 GPT-4.1 qwen3-8b qwen3-32b gpt-5 GPT-4.1 qwen3-8b qwen3-32b gpt-5 GPT-4.1 Table 13: Some cases in MFMD-persona. 0: False, 1: True. 25 g a o models qwen3-8b qwen3-32b gpt-5 GPT-4.1 qwen3-8b qwen3-32b gpt-5 GPT-4.1 qwen3-8b qwen3-32b gpt-5 GPT-4. qwen3-8b qwen3-32b gpt-5 GPT-4.1 qwen3-8b qwen3-32b gpt-5 GPT-4.1 0 0 0 0 1 1 1 1 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 1 1 1 1 0 0 0 0 1 0 1 1 0 1 1 l s A"
        },
        {
            "title": "E\nA\nU",
            "content": "0 0 0 0 1 0 1 1 0 0 0 0 0 0 1 0 1 1 1 1 0 0 0 1 1 1 1 0 0 0 0 0 1 1 1 0 1"
        },
        {
            "title": "A\nS\nU",
            "content": "a r A fi P A l M h a a i Claim: In 2020, PepsiCo gave $100 million to entities or projects associated with the Black Lives Matter movement. 0 0 0 0 0 0 0 0 fi P A a u e u o 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1"
        },
        {
            "title": "A\nS\nU",
            "content": "c fi P A Claim: McDonalds and K-pop band BTS announced meal collaboration to be released in May 2021. 0 0 0 0 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 l M h 0 0 0 0 0 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 Claim: congratulatory text message from Amazon claims you have come second or third in raffle to win free AirPods and asks you to click link. 0 0 0 0 0 0 0 0 0 0 1 0 Claim: In 1986, then-U.S. Sen. Joe Biden said, [Supporting Israel] is the best $3 billion investment we make. Were there not an Israel, the United States of America would have to invent an Israel to protect her interests in the region. 0 0 1 1 1 1 1 1 1 0 1 1 Claim: The U.S. government has been funding \"toilets in Africa.\" 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 0 1 1 1 0 1 1 0 1 1 1 0 1 1 1 0 1 1 1 0 0 1 0 0 0 1 1 0 0 1 0 1 1 0 1 1 1 0 0 0 1 1 1 1 1 1 0 1 1 0 0 1 1 1 0 1 1 1 1 1 1 0 1 1 1 Table 14: Some cases in MFMD-region. 0: False, 1: True. 26 RetailInvestor CompanyOwner n s C - a H / t 0 0 0 0 0 0 1 0 0 0 0 0 1 1 0 1 1 1 1 n s C - i m I - i A 0 1 0 0 0 1 1 1 0 0 0 0 0 1 1 0 0 1 1 1 0 0 0 0 1 1 1 0 0 0 0 1 1 1 0 0 1 1 1 n s C - d m I - d a - A 0 1 0 0 1 1 1 1 0 0 0 0 0 1 1 1 1 1 1 0 0 0 0 1 0 1 1 1 0 0 0 1 0 1 0 0 1 1 0 0 0 0 1 1 1 1 0 0 0 0 1 1 1 1 1 1 1 1 s d - n 0 1 0 0 1 0 0 1 0 0 0 0 0 1 1 1 1 1 1 n s C - i A h B - i A s u - i A s - i A n s C - p E a - w Models o a qwen3-8b qwen3-32b gpt-5 GPT-4.1 qwen3-8b qwen3-32b gpt-5 GPT-4.1 qwen3-8b qwen3-32b gpt-5 GPT-4.1 qwen3-8b qwen3-32b gpt-5 GPT-4. qwen3-8b qwen3-32b gpt-5 GPT-4.1 0 0 0 0 1 1 1 1 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 1 1 1 1 0 0 0 0 1 0 1 1 0 1 1 1 0 1 0 1 1 1 1 0 0 0 0 0 1 1 1 1 1 1 1 0 1 0 0 0 1 1 0 0 0 0 0 1 1 0 0 1 1 1 0 1 0 0 1 1 1 1 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 1 1 1 0 0 0 0 0 1 1 1 1 1 1 0 1 0 0 1 1 1 1 0 0 0 0 1 1 1 0 1 1 1 0 1 0 0 1 1 1 1 0 0 0 0 1 1 1 1 1 1 1 1 n s C - n C 0 1 0 0 1 1 1 1 n s C - a H / t t n s C - i m I - i y a i - i A s - i A i d - n y a i - d a - n m I - d a - A i t h - p E a - i A a - w m d - i A Claim: In 2020, PepsiCo gave $100 million to entities or projects associated with the Black Lives Matter movement. 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 1 1 1 0 1 1 Claim: McDonalds and K-pop band BTS announced meal collaboration to be released in May 2021. 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 Claim: congratulatory text message from Amazon claims you have come second or third in raffle to win free AirPods and asks you to click link. 1 0 1 1 1 0 1 i t h - n 0 0 0 0 1 0 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 Claim: In 1986, then-U.S. Sen. Joe Biden said, [Supporting Israel] is the best $3 billion investment we make. Were there not an Israel, the United States of America would have to invent an Israel to protect her interests in the region. 0 1 1 0 1 1 1 0 0 1 1 0 0 1 1 0 0 1 1 0 1 1 0 0 0 1 1 0 1 1 0 1 1 1 0 1 1 1 0 0 0 1 1 0 1 0 0 1 1 1 0 0 1 0 1 1 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 Claim: The U.S. government has been funding \"toilets in Africa.\" 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 1 1 1 0 1 1 0 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 1 1 0 1 1 1 s - n 0 1 0 0 1 1 1 1 0 0 0 0 1 1 1 1 1 1 1 Table 15: Some cases in MFMD-identity. 0: False, 1: True."
        }
    ],
    "affiliations": [
        "Archimedes, Athena Research Center",
        "Athens University of Economics and Business",
        "Columbia University",
        "Dubai Police",
        "ELLIS Manchester",
        "Halmstad University",
        "Islamic University of Technology",
        "MBZUAI",
        "McGill University",
        "Mila - Quebec AI Institute",
        "Stevens Institute of Technology",
        "The FinAI",
        "The University of Manchester",
        "University of Florida",
        "University of Melbourne"
    ]
}