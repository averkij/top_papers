{
    "paper_title": "Perovskite-LLM: Knowledge-Enhanced Large Language Models for Perovskite Solar Cell Research",
    "authors": [
        "Xiang Liu",
        "Penglei Sun",
        "Shuyan Chen",
        "Longhan Zhang",
        "Peijie Dong",
        "Huajie You",
        "Yongqi Zhang",
        "Chang Yan",
        "Xiaowen Chu",
        "Tong-yi Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The rapid advancement of perovskite solar cells (PSCs) has led to an exponential growth in research publications, creating an urgent need for efficient knowledge management and reasoning systems in this domain. We present a comprehensive knowledge-enhanced system for PSCs that integrates three key components. First, we develop Perovskite-KG, a domain-specific knowledge graph constructed from 1,517 research papers, containing 23,789 entities and 22,272 relationships. Second, we create two complementary datasets: Perovskite-Chat, comprising 55,101 high-quality question-answer pairs generated through a novel multi-agent framework, and Perovskite-Reasoning, containing 2,217 carefully curated materials science problems. Third, we introduce two specialized large language models: Perovskite-Chat-LLM for domain-specific knowledge assistance and Perovskite-Reasoning-LLM for scientific reasoning tasks. Experimental results demonstrate that our system significantly outperforms existing models in both domain-specific knowledge retrieval and scientific reasoning tasks, providing researchers with effective tools for literature review, experimental design, and complex problem-solving in PSC research."
        },
        {
            "title": "Start",
            "content": "Perovskite-LLM: Knowledge-Enhanced Large Language Models for Perovskite Solar Cell Research Penglei Sun1, Shuyan Chen1,2 Longhan Zhang1,2 Xiang Liu1, 5 2 0 2 8 1 ] A . [ 1 9 6 6 2 1 . 2 0 5 2 : r Peijie Dong1 Huajie You1 Yongqi Zhang1 Chang Yan1,2 Xiaowen Chu1,2, Tong-yi Zhang1,2, 1The Hong Kong University of Science and Technology (Guangzhou) 2Guangzhou Municipal Key Laboratory of Materials Informatics {xliu886,psun012,schen068}@connect.hkust-gz.edu.cn {lzhang619,pdong212,hyou603}@connect.hkust-gz.edu.cn {yongqizhang,changyan,xwchu,mezhangt}@hkust-gz.edu.cn"
        },
        {
            "title": "Abstract",
            "content": "The rapid advancement of perovskite solar cells (PSCs) has led to an exponential growth in research publications, creating an urgent need for efficient knowledge management and reasoning systems in this domain. We present comprehensive knowledge-enhanced system for PSCs that integrates three key components. First, we develop Perovskite-KG, domainspecific knowledge graph constructed from 1,517 research papers, containing 23,789 entities and 22,272 relationships. Second, we create two complementary datasets: PerovskiteChat, comprising 55,101 high-quality questionanswer pairs generated through novel multiagent framework, and Perovskite-Reasoning, containing 2,217 carefully curated materials science problems. Third, we introduce two specialized large language models: PerovskiteChat-LLM for domain-specific knowledge assistance and Perovskite-Reasoning-LLM for scientific reasoning tasks. Experimental results demonstrate that our system significantly outperforms existing models in both domainspecific knowledge retrieval and scientific reasoning tasks, providing researchers with effective tools for literature review, experimental design, and complex problem-solving in PSC research."
        },
        {
            "title": "Introduction",
            "content": "Perovskite solar cells (PSCs) have emerged as one of the most promising next-generation photovoltaic technologies, achieving remarkable progress with power conversion efficiencies (PCEs) exceeding 27.0% within just over decade (National Renewable Energy Laboratory, 2025; Snaith, 2018; Correa-Baena et al., 2017; Wu et al., 2021; Ang et al., 2022; Sathaye et al., 2011; Bogdanov et al., 2019). The rapid development of PSCs has generated an exponential growth in research publications, Equal Contribution. Corresponding Author. making it increasingly challenging for researchers to efficiently access and utilize the vast amount of knowledge in this field. This challenge is particularly acute given the complex interplay between material composition, fabrication processes, and device structure that characterizes PSC research. Traditional approaches to scientific knowledge management, such as literature reviews and databases, while valuable, are limited in their ability to capture the intricate relationships between different aspects of PSC research (Yang et al., 2024b; Han et al., 2025). Furthermore, existing artificial intelligence systems in materials science typically focus on either specific prediction tasks or general scientific knowledge, lacking the specialized capability to handle the unique characteristics of perovskite solar cell research (Han et al., 2025). This gap highlights the need for an integrated system that can both organize domain knowledge systematically and provide intelligent assistance to researchers. To address these challenges, we present comprehensive knowledge-enhanced system specifically designed for the perovskite solar cell domain, consisting of three key components. First, we develop Perovskite-KG, domain-specific knowledge graph constructed from 1,517 research papers, containing 23,789 entities and 22,272 relationships across manufacturing processes, parameters, and performance metrics. Second, we create multi-agent framework for generating high-quality instruction-tuning data, which not only reduces annotation costs but also ensures high reliability and low hallucination through the synergy of multiple specialized agents and expert guidance. This framework generates two complementary datasets: (1) Perovskite-Chat, an instruction-tuning dataset comprising 55,101 high-quality question-answer pairs generated from 2,214 high-impact papers using novel multi-agent framework, and (2) Perovskite-Reasoning, collection of 2,217 carefully curated materials science problems designed to enhance scientific reasoning capabilities. Third, we introduce two specialized large language models: Perovskite-Chat-LLM for domain-specific knowledge assistance and Perovskite-ReasoningLLM for tackling complex materials science reasoning tasks. Contributions. Our work makes the following key contributions: We construct the first comprehensive knowledge graph for perovskite solar cells, organizing domain knowledge into structured format that captures the relationships between materials, processes, and device performance. We propose an effective multi-agent framework for generating high-quality instructiontuning data, resulting in two specialized datasets: diverse domain-specific dataset covering seven research categories and focused reasoning dataset for enhancing scientific problem-solving capabilities. We develop and evaluate two specialized LLMs for perovskite solar cells that demonstrate superior performance compared to baseline models: one optimized for domainspecific queries and another for scientific reasoning tasks. We provide extensive experimental results showing the effectiveness of our integrated system in supporting various research tasks, from literature review to experimental design and complex problem-solving in materials science."
        },
        {
            "title": "2.1 LLM in Materials Science",
            "content": "The convergence of language modeling and computational materials science has unlocked transformative potential for accelerated discovery. Recent breakthroughs in domain-specific architectures (e.g., hierarchical attention mechanisms (Kononova et al., 2021) and multimodal fusion networks (Swain and Cole, 2016)) have addressed critical challenges in crystal structure prediction (Walker et al., 2021) and phase diagram analysis (Trewartha et al., 2022). As evidenced by the Materials Genome Initiative benchmarks (Tshitoyan et al., 2019), three principal research thrusts have emerged: (1) structured information extraction from heterogeneous corpora, (2) knowledge graph embeddings for composition-property relationships, and (3) neurosymbolic reasoning for synthesis pathway optimization. Building upon these foundations, knowledgeenhanced systems have achieved state-of-theart performance through two complementary paradigms: graph-based approaches employing heterogeneous graph neural networks (HGNNs) now attain 89.7% accuracy on multi-hop material property queries (An et al., 2024), while agent-based frameworks demonstrate 18.7% improvement in autonomous experimental design through chain-ofthought prompting (Zhang et al., 2024a). The fields maturation is further evidenced by systematic resource development: (i) The SciQAG framework (Wan et al., 2024) introduces novel curriculum learning paradigm for generating 120K domain-specific QA pairs, reducing expert annotation requirements by 78%; (ii) Standardized evaluation now spans chemical synthesis (ChemLLMBenchs reaction yield prediction task (Guo et al., 2023)), biomedical applications (MultiMedQAs toxicity prediction challenge (Singhal et al., 2023)), and cross-domain reasoning (SciEvals materialsdevice co-design track (Sun et al., 2023))."
        },
        {
            "title": "2.2 Knowledge Graph in Materials Science",
            "content": "Domain-specific knowledge graphs have evolved into structured semantic frameworks that systematically consolidate heterogeneous multi-source data through machine-readable representations, enabling cross-domain knowledge integration to accelerate discovery pipelines (Pan et al., 2024; Song et al., 2024; Zhu et al., 2022). In materials informatics, current implementations manifest two distinct paradigms: literature-derived systems exemplified by MatKG (Venugopal and Olivetti, 2024) and DISCOMAT (Gupta et al., 2023), which employ NLP and graph techniques to extract material compositions from textual sources, while empirical architectures represented by MatSciKB (Zhang et al., 2024b), Propnet (Mrdjenovich et al., 2020), MekG (Statt et al., 2023), and MOF-KG (An et al., 2022) focus on encoding experimental provenance and computational models through graphbased representations of material lineages. However, these approaches face the challenges that manual curation processes with resource burdens, while existing extraction methods exhibit limited granularity in resolving complex synthesis-processproperty relationships from unstructured text. To address these limitations, we propose an LLMdriven framework specifically optimized for perovskite materials research, featuring hybrid architecture that synergizes domain ontologies with self-supervised relationship extraction, augmented by automated quality control pipelines that enforce materials science constraints. In this section, we collect 1, 517 paper in perovskite domain to build Perovskite-KG and design the automatic knowledge graph construction pipeline including three stages document filtering, knowledge extracting and knowledge graph organization (Mrdjenovich et al., 2020), as shown in Appendix B. 2.3 Reasoning alignment Recent advances in parameter-efficient alignment have witnessed multiple research teams pursuing distinct methodologies to align the performance of o1 (OpenAI, 2024). Contemporary approaches bifurcate along two technical axes: (1) reinforcement learning paradigms exemplified by DeepSeek-R1s adversarial preference optimization (Guo et al., 2025) and K1.5s multi-objective reward shaping (Team et al., 2025), versus (2) supervised finetuning strategies employing distilled datasets at scale ( 104 examples) as demonstrated in (Team, 2025; Xu et al., 2025; Labs, 2025). Notably, S1 (Muennighoff et al., 2025) and LIMO (Ye et al., 2025) operationalize the Superficial Alignment Hypothesis (Zhou et al., 2023a) through curriculumbased sparse fine-tuning, achieving comparable reasoning capabilities with merely 1,000-2,000 carefully curated examples - 92% reduction in annotation costs relative to conventional SFT approaches."
        },
        {
            "title": "3 Perovskite-KG",
            "content": "Document Filtering. Drawing upon expert knowledge, we have developed the schema for perovskite materials. This schema, shown in Appendix Table 5, integrates three ontologies {oi oi schema}: fabrication, parameters, and performance. The fabrication ontology encompasses the procedures and conditions required to synthesize perovskite materials. The parameters ontology defines the ingredients, structural components, and other compositional aspects of the device. The performance ontology is concerned with the efficiency and functional characteristics of perovskite devices. Each ontology oi is further divided into j=1 so(j) , where oi = (cid:83)ni sub-ontologies so(j) and ni represents the number of sub-ontologies within oi. Each sub-ontology so(j) provides domaini specific description, denoted as d(j) , along with corresponding data type, denoted as t(j) , that is relevant to its particular scope. i , t(j) , d(j) For each sub-ontology [so(j) ] (e.g., Coating Parameter - Details about the coating method used in the material deposition process - Float), we create the prompts to query documents = {Dk = 1, . . . , m} using large language model. These prompts facilitate the extraction of relevant information for each sub-ontology. The output D(i,j) filtered is defined as: D(i,j) filtered = {Dk so(j) Dk}, (1) where D(i,j) filtered represents the set of filtered documents containing pertinent details for sub-ontology so(j) across the collection. This approach ensures systematic and efficient retrieval of targeted information for each sub-ontology. Knowledge Extracting. We employ prompt function, denoted as fprompt(), to transform the sub-ontology [so(j) ] into document prompt, represented as fprompt(so(j) ). To extract the potential domain knowledge K, we utilize pre-trained large language model (LLM), expressed as LLM(; θ), under zero-shot setting where the parameters θ remain fixed. The whole pipeline can be formulated as below: , d(j) , d(j) , t(j) , t(j) = search D(i,j) filtered LLM(fprompt(so(j) , d(j) , t(j) ); θ), (2) where the search function search() may involve an argmax operation to identify the highest-scoring output or sampling approach to generate outputs according to the probability distribution specified by the adopted LLM(; θ). After extracting knowledge, we conduct quality control procedures to ensure accuracy and reliability. These procedures include entity disambiguation and relationship deduplication. Entity disambiguation in knowledge graph aims to resolve ambiguity by identifying the unique entity that corresponds to an ambiguous mention, denoted as emention, within subgraph. The objective is to determine distinct entity that accurately represents emention. Relationship deduplication involves identifying and merging redundant relations in the (a) The pipeline of Perovskite-KG construction. (b) The multi-agent framework for perovskite instruction tuning dataset generation. Figure 1: The pipeline of Perovskite-KG construction and multi-agent framework. 1, 1, r, knowledge graph. For instance, given two relations ri = (e1, r, e2) and rj = (e 2), if they convey the same semantic meaningthat is, if (e1, e2) and (e 2) refer to identical entities and the relations and are equivalent. Knowledge Graph Organization. We construct the Perovskite Knowledge Graph (Perovskite-KG) using graph database. The Perovskite-KG consists of 23, 789 entities and 22, 272 relationships. By incorporating citation relationships between papers, we enable our LLM to provide references for its responses, enhancing credibility and reducing hallucination."
        },
        {
            "title": "Instruction Tuning Dataset Generation",
            "content": "In this section, we collect 2, 214 the top level publications papers in perovskite domain and design the instruction tuning dataset including question answering and multiple choice questions, containing 55, 101 instances around 4.4 million tokens, named Perovskite-Chat. Our experiments show that our perovskite instruction tuning dataset can effectively improve the performance of LLMs on perovskite related tasks. Figure 1 (b) illustrates this multi-agent framework for instruction tuning dataset generation. The process begins with expert guidance and academic literature from various sources (including Science, Nature, Elsevier, Springer, arXiv, and others) as inputs. The expert guidance is provided by the domain expert focus on 7 research categories, 21 research questions. Table 1 further expands this classification by presenting 21 specific research questions (Q1-Q21) organized within these seven categories, more details can be found in Appendix D.1. These inputs feed into multi-agent system: (1) an Information Extraction Agent that processes the raw content, (2) Quality Validation Agent that ensures data accuracy and relevance, and (3) Document Summarization Agent that condenses and structures the information. This framework ensures systematic, high-quality data processing through multiple validation and refinement stages. Let = {d1, ..., dn} represent the collection of academic literature from various sources, and = {c1, ..., c7} denote the expert guidance categories with corresponding research questions = {q1, ..., q21}. The multi-agent framework processes these inputs through three specialized agents: Information Extraction: Aextract(di) = {x1, ..., xk} (3) Quality Validation: Avalidate(xj) = (cid:40) 1, if valid 0, otherwise (4) data and baseline model testing. To develop comprehensive solution paths, we employed advanced language models like DeepSeek-R1 (Guo et al., 2025) and O1 (OpenAI, 2024) in multi-step reasoning approach. This methodology involved decomposing complex problems into logical steps, applying key physical and chemical principles, and implementing systematic solution strategies with result validation. The resulting dataset features detailed reasoning chains that demonstrate step-bystep problem-solving processes, making it valuable for training models in scientific reasoning and materials science problem-solving. Training Dataset Figure 2 (a) presents the distribution of question categories in the PerovskiteChat instruction tuning dataset. Device Structure dominates with 43.9% of all questions, followed by Performance Enhancement at 20.4%. Device & Recom. comprises 13.1%, while Metrics accounts for 8.2%. The remaining categories include Stability (9.8%), Materials (2.9%), and Interface (1.7%). Figure 2 (b) displays wordcloud visualization of the datasets most frequent terms, with perovskite solar and solar cell appearing as predominant phrases, reflecting the datasets core focus. Other frequently occurring terms such as device structure, configuration, and stability underscore the key technical aspects addressed. This distribution demonstrates the datasets strong emphasis on device structural aspects, with performance-related queries forming the second largest category. Figure 3 shows the length distribution patterns across categories in our perovskite instruction tuning dataset. All categories display similar characteristics, with sequence lengths predominantly ranging from 100 to 500 tokens and median length of 400 tokens. This analysis informs our model design decisions, particularly regarding sequence length limitations and tokenization strategies. The distributions are independently normalized for each category, enabling direct pattern comparison despite varying sample sizes. Evaluation Dataset For better evaluation, we design the evaluation dataset including multiple choice questions and question answering in the perovskite domain. The evaluation dataset also extract from the top level publications in perovskite domain with our multi-agent framework and extral expert double check. The evaluation dataset contains 1,103 question answering named Perovskite QA and 1,103 multiple choice questions named (a) The distribution of question categories in the instruction tuning dataset. (b) The word cloud of the instruction tuning dataset. Figure 2: The distribution of question categories in the instruction tuning dataset. Document Summarization: Asummarize(Xvalid) = (5) The final instruction tuning dataset is constructed as: = {(qi, yi) qi Q, yi = Asummarize(Avalidate(Aextract(di)))} (6) Next, we introduce Perovskite-Reasoning, collection of 2,217 high-quality questions from materials science textbooks, designed to enhance reasoning capabilities in perovskite and materials science domains. Similar to S1 (Muennighoff et al., 2025) and LIMO (Ye et al., 2025), our dataset maintains data efficiency with fewer than 3,000 instances. The questions were sourced from hundreds of widely-used materials science and engineering textbooks, with focus on perovskite solar cells and fundamental materials science concepts. Our rigorous selection process applied three key criteria: clarity of problem statements, solution completeness, and alignment with core materials science principles. Materials science professors conducted expert assessments to categorize questions by difficulty level, validated through student performance Category Rationale Device Structure Fundamental aspects focusing on high-efficiency (>25% PCE) device architecture and fabrication processes (Q1-Q3) Perf. Enhancement Analysis of problem-solving approaches and strategic choices in highperformance devices (Q4-Q5) Metrics Stability Key performance indicators (VOC, FF, JSC) and their optimization methods (Q6-Q9) Critical stability aspects addressing main degradation pathways: moisture, thermal, and light stability (Q10-Q12) Defect & Recom. Fundamental mechanisms affecting device efficiency through defect passivation and recombination control (Q13-Q14) Interface Materials Interface engineering and charge transport optimization (Q15-Q17) Comprehensive analysis of functional materials and their characteristics in different device components (Q18-Q21) Table 1: Classification of Research Questions in Perovskite Solar Cell Studies tions that LLaMA-3.1-8B-Instruct fails to answer correctly are categorized as Hard. This classification method resulted in 823 Easy questions and 280 Hard questions in our evaluation dataset, providing balanced assessment of model capabilities across different difficulty levels. For evaluate the performance of PerovskiteReasoning, we incorporated Minerva (Lewkowycz et al., 2022) and GPQA Diamond (Rein et al., 2023) as the benchmark. These contains undergraduate and PhD level science questions from Biology, Chemistry and Physics."
        },
        {
            "title": "5.1 Experiment Design",
            "content": "In this section, we conduct the instruction tuning experiments on the Perovskite-Chat and Perovskite-Reasoning dataset. We select the LLaMA-3.1-8B-Instruct (Dubey et al., 2024) and Qwen-2.5-7B-Instruct (Yang et al., 2024a) as the baseline model, and Perovskite-Chat-LLM and Perovskite-Reasoning-LLM are fine-tuned version of LLaMA-3.1-8B-Instruct and Qwen-2.57B-Instruct with Perovskite-Chat and PerovskiteReasoning dataset. For the training process, we use the full parameter fine-tuning method to fine-tune the PerovskiteLLM. The experiment is conducted on the A800 GPU server, with flash attention (Dao, 2024) and mixed precision training for efficient training. For more details about the training process, please refer Figure 3: Distribution of prompt and response lengths across different categories in our dataset. The y-axis represents density (e-3), and the x-axis shows the word count in logarithmic scale. Each categorys distribution is independently normalized. Perovskite MCQ. For question answering, we set the Rouge-L score and LLM-as-a-Judge (Zheng et al., 2023) score as the evaluation metric. In our experiments, we find that the both metrics can effectively measure the quality of question answering and consistency with each other. For multiple choice questions, we set the accuracy as the evaluation metric. And We using LLaMA-3.1-8B-Instruct (Dubey et al., 2024) as the baseline model, the difficulty level of each question is determined by its zero-shot performance. Specifically, if LLaMA-3.1-8B-Instruct can correctly answer question in zero-shot setting (without any task-specific training or prompt engineering), we classify it as an Easy question. Conversely, quesPerovskite QA PPL Rouge-L LLM-Judge Model Perovskite MCQ Easy Hard All Model GPT-3.5-Turbo GPT-4o-Mini GPT-4o - - - 6.77 LLaMA-3.1-8B 4.98 LLaMA-3.1-70B 6.23 Qwen-2.5-7B Qwen-2.5-72B 5.12 Perovskite-Chat-LLM 2.97 11.24 11.90 11.36 13.18 17.38 11.22 10.17 41.25 1.24 1.34 1.41 1.28 1.80 1.39 1.31 2.97 Table 2: Performance of Perovskite-Chat-LLM on Perovskite QA to Appendix E. For the evaluation process, we use the perplexity (PPL), Rouge-L score, and LLM-Judge score to evaluate the performance on the Perovskite QA benchmark, the accuracy to evaluate the performance on the Perovskite MCQ benchmark to evaluate the performance of Perovskite-Chat-LLM. And the pass@1 rate on Minerva and GPQA benchmarks to evaluate the performance on the Perovskite-Reasoning-LLM. All experiments are conducted with zero-shot setting and three times to get the average results."
        },
        {
            "title": "5.2 Results and analysis",
            "content": "Perovskite-Chat-LLM Table 2 presents the evaluation results of various models on the Perovskite QA dataset, comparing three key metrics: perplexity (PPL), Rouge-L score, and LLM-Judge score. The baseline models include GPT-3.5-Turbo, GPT4o-Mini, GPT-4o, LLaMA-3.1-8B. Among these, Perovskite-Chat-LLM demonstrates superior performance across all metrics, achieving perplexity of 2.97, Rouge-L score of 41.25, and an LLMJudge score of 2.97. This represents significant improvement over the baseline LLaMA-3.1-8B model, which scored 6.77, 13.18, and 1.28 respectively. The GPT family of models, while competitive in terms of LLM-Judge scores, showed lower performance in Rouge-L scores compared to Perovskite-Chat-LLM, with GPT-4o achieving 11.36 for Rouge-L and 1.41 for LLM-Judge. With case study in Figure 4, we can see that PerovskiteChat-LLM can generate more accurate and consistent answers compared to other models, and ChatGPT only can offer general and not specific answer which lead to the low performance on Rouge-L and LLM-Judge metrics. Table 3 presents the evaluation results of various models on the Perovskite MCQ dataset, categorized into Easy, Hard, and All difficulty levels. The GPT-3.5-Turbo GPT-4o-Mini GPT-4o 86.63 89.79 91.37 100.00 LLaMA-3.1-8B LLaMA-3.1-70B 93.44 92.22 Qwen-2.5-7B Qwen-2.5-72B 93.07 Perovskite-LLM 95.50 49.29 61.79 65. 0.00 66.43 55.36 64.29 62.86 77.15 82.68 84.68 74.61 86.58 82.86 85.77 87.22 Table 3: Performance of Perovskite-Chat-LLM on Perovskite MCQ. The LLaMA-3.1-8B baseline models performance defines Easy/Hard question categories. models compared include GPT 3.5 Turbo, GPT 4o Mini, GPT 4o, LLaMA-3.1-8B, and PerovskiteChat-LLM. Among these, GPT 4o achieves the highest overall score of 84.68, with scores of 91.37 for Easy and 65.00 for Hard questions. PerovskiteChat-LLM, highlighted in red, shows strong performance with score of 67.86 on Hard questions, the highest in this category, and an overall score of 81.96. LLaMA-3.1-8B scores perfectly on Easy questions but fails on Hard ones, resulting in lower overall score of 74.21. Perovskite-Reasoning-LLM Table 4 presents the evaluation results of Perovskite-ReasoningLLM compared against various baseline models on the GPQA and Minerva benchmarks. In the 7Bscale model category, our Perovskite-ReasoningLLM achieves state-of-the-art performance with remarkable data efficiency, requiring only 2K training examples compared to 800K for R1-Qwen2.57B and 114K for OpenThinker-7B. Our model achieves 43.95 on GPQA and 44.49 on Minerva, significantly outperforming other 7B models. When compared to 32B models, while our GPQA performance shows room for improvement (suggesting GPQAs sensitivity to model size), our Minerva score (44.49) is competitive with larger models like LIMO-32B (44.90) and approaches S1-32B (47.79). This demonstrates that our efficient training approach can achieve strong performance on STEM reasoning tasks even with smaller model architecture."
        },
        {
            "title": "5.3 Case Study",
            "content": "To illustrate the practical advantages of PerovskiteLLM over existing models, Figure 4 presents comparative analysis of responses from PerovskiteChat-LLM and ChatGPT to question about highModel # ex GPQA Minerva Avg o1 o1-preview o1-mini Deepseek-R1 Qwen2.5-32B-Instruct QwQ-32B-preview LIMO-32B* S1-32B* API Models - - - - 32B - - 0.8K 1K 7B R1-Qwen2.5-7B* 800K R1-LLaMA3-8B* 800K OpenThinker-7B* 114K Perovskite Reasoning-LLM 2K 77.30 73.30 60.00 71. 48.00 65.10 66.70 59.60 44.49 19.19 42.90 43.95 - 47.10 - - 41.20 39.00 44.90 47.79 25.25 30.51 41.10 44.49 - 60.20 - - 44.60 52.05 55.80 53.69 34.87 24.85 42.00 44.22 Table 4: We evaluate the performance of PerovskiteReasoning-LLM on the GPQA and Minerva benchmarks. * indicates the results are from our evaluation. # ex = number of examples used for fine-tuning. efficiency perovskite solar cell fabrication. The responses demonstrate clear distinction in the depth and specificity of knowledge provided by each model. For instance, Perovskite-Chat-LLM specifies precise conditions for the HTL preparation (150C for 15 minutes) and details two-step spin-coating procedure (1,000 rpm for 10 seconds, followed by 6,000 rpm for 30 seconds) with specific process modifications like anisole addition. This level of detail reflects Perovskite-Chat-LLMs specialized training on domain-specific literature and its ability to provide practical, implementationready guidance for researchers in the field. And Figure 5 is case study of Perovskite-Chat-LLMs ability to provide detailed and accurate information with references. For more details, please refer to Appendix F."
        },
        {
            "title": "6 Conclusion",
            "content": "In this work, we present comprehensive knowledge-enhanced system for perovskite solar cell research, integrating three key components: (1) Perovskite-KG, domain-specific knowledge graph containing 23,789 entities and 22,272 relationships; (2) multi-agent system for generating two complementary datasets, Perovskite-Chat and Perovskite-Reasoning, designed for domainspecific knowledge assistance and scientific reasoning respectively; and (3) two specialized large language models that demonstrate superior performance in both knowledge retrieval and reasoning tasks. Our experimental results show significant improvements over existing models, with Perovskite-Chat-LLM achieving state-of-theQuestion: How to fabricate the perovskite solar cells those can reach PCE over 25%? Perovskite-Chat-LLM: ... For the hole transport layer (HTL), materials like PEDOT:PSS are commonly used. These materials are spin-coated onto the substrate and annealed at temperature of 150C for 15 minutes. The perovskite precursor solution is then prepared using mixed cation and halide composition, and spin-coated onto the HTL. The spin-coating process involves twostep procedure: first at 1,000 rpm for 10 seconds, followed by 6,000 rpm for 30 seconds, with anisole added during the second step to facilitate crystallization. ... ChatGPT: ... Perovskite Material Quality: The quality of the perovskite material is crucial for achieving high efficiency. Use high-quality perovskite precursors and optimize the fabrication process to ensure uniform and defect-free perovskite films. ... Figure 4: Comparison of responses between PerovskiteLLM and ChatGPT: Perovskite-LLM provides detailed operational steps with specific parameters, while ChatGPT only offers general conceptual guidance. Question: Can you tell me what is CuO? And give me 5 usage in perovskite. Perovskite-Chat-LLM: ... 1. Hole Transport Material: CuO acts as an effective hole transport material (HTM) due to its suitable work function and high hole mobility. It is often used in conjunction with other materials, such as iOx, to enhance the performance of PSCs by facilitating efficient charge extraction [1]. ... References: [1] Inorganic Materials as Hole Selective Contacts and Intermediate Tunnel Junction Layer for Monolithic Perovskite-CIGSe Tandem Solar Cells, Wang, Yajie; Wenisch, Robert; Schlatman ... Figure 5: case study of Perovskite-Chat-LLMs ability to provide detailed and accurate information with references. art performance on domain-specific tasks and Perovskite-Reasoning-LLM showing competitive performance on scientific reasoning benchmarks despite using substantially fewer training examples. The system provides researchers with effective tools for literature review, experimental design, and complex problem-solving in PSC research. This work demonstrates the potential of large language models to accelerate innovation and discovery in perovskite materials science by enabling more efficient knowledge access and reasoning capabilities."
        },
        {
            "title": "Limitations",
            "content": "Despite the promising results, our current system has several limitations that warrant future investigation: Knowledge Coverage: While our knowledge graph covers substantial portion of the PSC literature, it may not capture all emerging research directions and novel experimental techniques. Future work should focus on developing mechanisms for automatic knowledge base expansion and updates to maintain its relevance. Reasoning Depth: Although PerovskiteReasoning-LLM shows promising results, its performance on more complex multi-step reasoning tasks still has room for improvement, particularly when compared to larger models on the GPQA benchmark. Future research will explore advanced reasoning architectures and training strategies. Model Size Trade-offs: While our 7Bparameter models achieve competitive performance, there might be certain complex tasks that benefit from larger model architectures, suggesting potential trade-off between efficiency and capability. Future work will investigate model compression techniques and more efficient architectures. To address these limitations, our future work will focus on three main directions: (1) developing continuous knowledge integration framework that can automatically update the knowledge base with new research findings, (2) enhancing the reasoning capabilities through advanced model architectures and training strategies, and (3) improving the systems practical utility through better validation mechanisms and more efficient model designs. These improvements will make the system more robust, up-to-date, and accessible to the broader research community."
        },
        {
            "title": "Acknowledgments",
            "content": "This work is supported by the GuangzhouHKUST(GZ) Program (No.2023A03J0003)."
        },
        {
            "title": "Joint",
            "content": "Yuan An, Jane Greenberg, Alex Kalinowski, Xintong Zhao, Xiaohua Hu, Fernando J. Uribe-Romo, Kyle Langlois, Jacob Furst, and Diego A. GomezGualdron. 2024. Knowledge graph question answering for materials science (kgqa4mat): Developing natural language interface for metal-organic frameworks knowledge graph (mof-kg) using llm. Preprint, arXiv:2309.11361. Yuan An, Jane Greenberg, Xintong Zhao, Xiaohua Hu, Scott McCLellan, Alex Kalinowski, Fernando Uribe-Romo, Kyle Langlois, Jacob Furst, Diego Gomez-Gualdron, et al. 2022. Building open knowledge graph for metal-organic frameworks (mofIn 28TH ACM. kg): Challenges and case studies. SIGKDD. Conference. on Knowledge Discovery. and Data Mining. Tze-Zhang Ang, Mohamed Salem, Mohamad Kamarol, Himadry Shekhar Das, Mohammad Alhuyi Nazari, and Natarajan Prabaharan. 2022. comprehensive study of renewable energy sources: Classifications, challenges and suggestions. Energy Strategy Reviews, 43:100939. Dmitrii Bogdanov, Javier Farfan, Kristina Sadovskaia, Arman Aghahosseini, Michael Child, Ashish Gulagi, Ayobami Solomon Oyewo, Larissa de Souza Noel Simas Barbosa, and Christian Breyer. 2019. Radical transformation pathway towards sustainable electricity via evolutionary steps. Nature communications, 10(1):116. Juan-Pablo Correa-Baena, Michael Saliba, Tonio Buonassisi, Michael Gratzel, Antonio Abate, Wolfgang Tress, and Anders Hagfeldt. 2017. Promises and challenges of perovskite solar cells. Science, 358(6364):739744. Tri Dao. 2024. FlashAttention-2: Faster attention with better parallelism and work partitioning. In International Conference on Learning Representations (ICLR). Shizhe Diao, Pengcheng Wang, Yong Lin, Rui Pan, Xiang Liu, and Tong Zhang. 2024. Active prompting with chain-of-thought for large language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 13301350, Bangkok, Thailand. Association for Computational Linguistics. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. 2024. The llama 3 herd of models. arXiv preprint arXiv:2407.21783. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. 2025. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948. Taicheng Guo, Kehan Guo, Bozhao Nan, Zhenwen Liang, Zhichun Guo, Nitesh V. Chawla, Olaf Wiest, and Xiangliang Zhang. 2023. What can large language models do in chemistry? comprehensive benchmark on eight tasks. Preprint, arXiv:2305.18365. Tanishq Gupta, Mohd Zaki, Devanshi Khatsuriya, Kausik Hira, NM Anoop Krishnan, et al. 2023. Discomat: Distantly supervised composition extraction from tables in materials science articles. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1346513483. Jiye Han, Keonwoo Park, Shaun Tan, Yana Vaynzof, Jingjing Xue, Eric Wei-Guang Diau, Moungi Bawendi, Jin-Wook Lee, and Il Jeon. 2025. Perovskite solar cells. Nature Reviews Methods Primers, 5(1):3. Sirui Hong, Yizhang Lin, Bang Liu, Bangbang Liu, Binhao Wu, Danyang Li, Jiaqi Chen, Jiayi Zhang, Jinlin Wang, Li Zhang, Lingyao Zhang, Min Yang, Mingchen Zhuge, Taicheng Guo, Tuo Zhou, Wei Tao, Wenyi Wang, Xiangru Tang, Xiangtao Lu, Xiawu Zheng, Xinbing Liang, Yaying Fei, Yuheng Cheng, Zongze Xu, and Chenglin Wu. 2024a. Data interpreter: An LLM agent for data science. CoRR, abs/2402.18679. Sirui Hong, Mingchen Zhuge, Jonathan Chen, Xiawu Zheng, Yuheng Cheng, Jinlin Wang, Ceyao Zhang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou, Chenyu Ran, Lingfeng Xiao, Chenglin Wu, and Jurgen Schmidhuber. 2024b. Metagpt: Meta programming for multi-agent collaborative framework. In ICLR. OpenReview.net. Olga Kononova, Tanjin He, Haoyan Huo, Amalie Trewartha, Elsa Olivetti, and Gerbrand Ceder. 2021. Opportunities and challenges of text mining in materials research. Iscience, 24(3). Bespoke Labs. 2025. Bespoke-stratos: The unreasonable effectiveness of reasoning distillation. Accessed: 2025-01-22. Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, et al. 2022. Solving quantitative reasoning problems with language models. Advances in Neural Information Processing Systems, 35:3843 3857. Boyan Li, Yuyu Luo, Chengliang Chai, Guoliang Li, and Nan Tang. 2024. The dawn of natural language to sql: Are we fully ready? arXiv preprint arXiv:2406.01265. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. 2023. Self-refine: Iterative refinement with selffeedback. In Thirty-seventh Conference on Neural Information Processing Systems. David Mrdjenovich, Matthew Horton, Joseph Montoya, Christian Legaspi, Shyam Dwaraknath, Vahe Tshitoyan, Anubhav Jain, and Kristin Persson. 2020. Propnet: knowledge graph for materials science. Matter, 2(2):464480. Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Cand`es, and Tatsunori Hashimoto. 2025. s1: Simple test-time scaling. arXiv preprint arXiv:2501.19393. National Renewable Energy Laboratory. 2025. Best research-cell efficiency chart. Photovoltaic research, National Renewable Energy Laboratory (NREL). Harsha Nori, Yin Tat Lee, Sheng Zhang, Dean Carignan, Richard Edgar, Nicol`o Fusi, Nicholas King, Jonathan Larson, Yuanzhi Li, Weishung Liu, Renqian Luo, Scott Mayer McKinney, Robert Osazuwa Ness, Hoifung Poon, Tao Qin, Naoto Usuyama, Chris White, and Eric Horvitz. 2023. Can generalist foundation models outcompete special-purpose tuning? case study in medicine. CoRR, abs/2311.16452. OpenAI. 2024. Learning to reason with llms. Shirui Pan, Linhao Luo, Yufei Wang, Chen Chen, Jiapu Wang, and Xindong Wu. 2024. Unifying large language models and knowledge graphs: roadmap. IEEE Transactions on Knowledge and Data Engineering. David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel Bowman. 2023. Gpqa: graduate-level google-proof q&a benchmark. arXiv preprint arXiv:2311.12022. Tal Ridnik, Dedy Kredo, and Itamar Friedman. 2024. Code generation with alphacodium: From prompt engineering to flow engineering. CoRR, abs/2401.08500. Jayant Sathaye, Oswaldo Lucon, Atiq Rahman, John Christensen, Fatima Denton, Junichi Fujino, Garvin Heath, Monirul Mirza, Hugh Rudnick, August Schlaepfer, et al. 2011. Renewable energy in the context of sustainable development. In IPCC special report on renewable energy sources and climate change mitigation. Cambridge University Press. Karan Singhal, Shekoofeh Azizi, Tao Tu, S. Sara Mahdavi, Jason Wei, Hyung Won Chung, Nathan Scales, Ajay Tanwani, Heather Cole-Lewis, Stephen Pfohl, Perry Payne, Martin Seneviratne, Paul Gamble, Chris Kelly, Abubakr Babiker, Nathanael Scharli, Aakanksha Chowdhery, Philip Mansfield, Dina Demner-Fushman, Blaise Aguera Arcas, Dale Webster, Greg S. Corrado, Yossi Matias, Katherine Chou, Juraj Gottweis, Nenad Tomasev, Yun Liu, Alvin Rajkomar, Joelle Barral, Christopher Semturs, Alan Karthikesalingam, and Vivek Natarajan. 2023. Large language models encode clinical knowledge. Nature, 620(7972):172180. Henry Snaith. 2018. Present status and future prospects of perovskite photovoltaics. Nature materials, 17(5):372376. Yaoxian Song, Penglei Sun, Haoyu Liu, Zhixu Li, Wei Song, Yanghua Xiao, and Xiaofang Zhou. 2024. Scene-driven multimodal knowledge graph construction for embodied ai. IEEE Transactions on Knowledge and Data Engineering. Michael Statt, Brian Rohr, Dan Guevarra, Santosh Suram, John Gregoire, et al. 2023. The materials experiment knowledge graph. Digital Discovery, 2(4):909914. Liangtai Sun, Yang Han, Zihan Zhao, Da Ma, Zhennan Shen, Baocai Chen, Lu Chen, and Kai Yu. 2023. Scieval: multi-level large language model evaluation benchmark for scientific research. arXiv preprint arXiv:2308.13149. Matthew Swain and Jacqueline Cole. 2016. Chemdataextractor: toolkit for automated extraction of chemical information from the scientific literature. Journal of chemical information and modeling, 56(10):18941904. Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, Chuning Tang, Congcong Wang, Dehao Zhang, Enming Yuan, Enzhe Lu, Fengxiang Tang, Flood Sung, Guangda Wei, Guokun Lai, Haiqing Guo, Han Zhu, Hao Ding, Hao Hu, Hao Yang, Hao Zhang, Haotian Yao, Haotian Zhao, Haoyu Lu, Haoze Li, Haozhen Yu, Hongcheng Gao, Huabin Zheng, Huan Yuan, Jia Chen, Jianhang Guo, Jianlin Su, Jianzhou Wang, Jie Zhao, Jin Zhang, Jingyuan Liu, Junjie Yan, Junyan Wu, Lidong Shi, Ling Ye, Longhui Yu, Mengnan Dong, Neo Zhang, Ningchen Ma, Qiwei Pan, Qucheng Gong, Shaowei Liu, Shengling Ma, Shupeng Wei, Sihan Cao, Siying Huang, Tao Jiang, Weihao Gao, Weimin Xiong, Weiran He, Weixiao Huang, Wenhao Wu, Wenyang He, Xianghui Wei, Xianqing Jia, Xingzhe Wu, Xinran Xu, Xinxing Zu, Xinyu Zhou, Xuehai Pan, Y. Charles, Yang Li, Yangyang Hu, Yangyang Liu, Yanru Chen, Yejie Wang, Yibo Liu, Yidao Qin, Yifeng Liu, Ying Yang, Yiping Bao, Yulun Du, Yuxin Wu, Yuzhi Wang, Zaida Zhou, Zhaoji Wang, Zhaowei Li, Zhen Zhu, Zheng Zhang, Zhexu Wang, Zhilin Yang, Zhiqi Huang, Zihao Huang, Ziyao Xu, and Zonghan Yang. 2025. Kimi k1.5: Scaling reinforcement learning with llms. Preprint, arXiv:2501.12599. Vahe Tshitoyan, John Dagdelen, Leigh Weston, Alexander Dunn, Ziqin Rong, Olga Kononova, Kristin A. Persson, Gerbrand Ceder, and Anubhav Jain. 2019. Unsupervised word embeddings capture latent knowledge from materials science literature. Nature. Vineeth Venugopal and Elsa Olivetti. 2024. Matkg: An autonomously generated knowledge graph in material science. Scientific Data, 11(1):217. Nicholas Walker, Amalie Trewartha, Haoyan Huo, Sanghoon Lee, Kevin Cruse, John Dagdelen, Alexander Dunn, Kristin Persson, Gerbrand Ceder, and Anubhav Jain. 2021. The impact of domain-specific pre-training on named entity recognition tasks in materials science. Available at SSRN 3950755. Yuwei Wan, Yixuan Liu, Aswathy Ajith, Clara Grazian, Bram Hoex, Wenjie Zhang, Chunyu Kit, Tong Xie, and Ian Foster. 2024. Sciqag: framework for autogenerated science question answering dataset with fine-grained evaluation. Preprint, arXiv:2405.09939. Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. 2023. Voyager: An open-ended embodied agent with large language models. arXiv preprint arXiv:2305.16291. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2022. Self-consistency improves chain of thought reasoning in language models. In The Eleventh International Conference on Learning Representations. Zhenhailong Wang, Shaoguang Mao, Wenshan Wu, Tao Ge, Furu Wei, and Heng Ji. 2024. Unleashing the emergent cognitive synergy in large language models: task-solving agent through multi-persona selfcollaboration. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 257279. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35:2482424837. Tianhao Wu, Zhenzhen Qin, Yanbo Wang, Yongzhen Wu, Wei Chen, Shufang Zhang, Molang Cai, Songyuan Dai, Jing Zhang, Jian Liu, et al. 2021. The main progress of perovskite solar cells in 20202021. Nano-Micro Letters, 13:118. NovaSky Team. 2025. Sky-t1: Fully open-source reasoning model with o1-preview performance in $450 budget. Accessed: 2025-01-09. Yupeng Xie, Yuyu Luo, Guoliang Li, and Nan Tang. 2024. Haichart: Human and AI paired visualization system. Proc. VLDB Endow., 17(11):31783191. Amalie Trewartha, Nicholas Walker, Haoyan Huo, Sanghoon Lee, Kevin Cruse, John Dagdelen, Alexander Dunn, Kristin A. Persson, Gerbrand Ceder, and Anubhav Jain. 2022. Quantifying the advantage of domain-specific pre-training on named entity recognition tasks in materials science. Patterns. Haotian Xu, Xing Wu, Weinong Wang, Zhongzhi Li, Da Zheng, Boyuan Chen, Yi Hu, Shijia Kang, Jiaming Ji, Yingying Zhang, Zhijiang Guo, Yaodong Yang, Muhan Zhang, and Debing Zhang. 2025. Redstar: Does scaling long-cot data unlock better slowreasoning systems? Preprint, arXiv:2501.11284. Andy Zhou, Kai Yan, Michal Shlapentokh-Rothman, Haohan Wang, and Yu-Xiong Wang. 2024. Language agent tree search unifies reasoning, acting, and planning in language models. In Forty-first International Conference on Machine Learning. Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, Susan Zhang, Gargi Ghosh, Mike Lewis, Luke Zettlemoyer, and Omer Levy. 2023a. Preprint, Lima: Less is more for alignment. arXiv:2305.11206. Xuanhe Zhou, Guoliang Li, and Zhiyuan Liu. 2023b. Llm as dba. arXiv preprint arXiv:2308.05481. Xiangru Zhu, Zhixu Li, Xiaodan Wang, Xueyao Jiang, Penglei Sun, Xuwu Wang, Yanghua Xiao, and Nicholas Jing Yuan. 2022. Multi-modal knowledge graph construction and application: survey. IEEE Transactions on Knowledge and Data Engineering, 36(2):715735. Mingchen Zhuge, Haozhe Liu, Francesco Faccio, Dylan Ashley, Robert Csordas, Anand Gopalakrishnan, Abdullah Hamdi, Hasan Abed Al Kader Hammoud, Vincent Herrmann, Kazuki Irie, et al. 2023. Mindstorms in natural language-based societies of mind. arXiv preprint arXiv:2305.17066. Yiheng Xu, SU Hongjin, Chen Xing, Boyu Mi, Qian Liu, Weijia Shi, Binyuan Hui, Fan Zhou, Yitao Liu, Tianbao Xie, et al. 2024. Lemur: Harmonizing natural language and code for language agents. In The Twelfth International Conference on Learning Representations. An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. 2024a. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115. Chuang Yang, Wenjing Hu, Jiale Liu, Chuanzhou Han, Qiaojiao Gao, Anyi Mei, Yinhua Zhou, Fengwan Guo, and Hongwei Han. 2024b. Achievements, challenges, and future prospects for industrialization of perovskite solar cells. Light: Science & Applications, 13(1):227. Yilin Ye, Jianing Hao, Yihan Hou, Zhan Wang, Shishi Xiao, Yuyu Luo, and Wei Zeng. 2024. Generative AI for visualization: State of the art and future directions. Vis. Informatics, 8(1):4366. Yixin Ye, Zhen Huang, Yang Xiao, Ethan Chern, Shijie Xia, and Pengfei Liu. 2025. Limo: Less is more for reasoning. arXiv preprint arXiv:2502.03387. Huan Zhang, Yu Song, Ziyu Hou, Santiago Miret, and Bang Liu. 2024a. HoneyComb: flexible LLMbased agent system for materials science. In Findings of the Association for Computational Linguistics: EMNLP 2024, pages 33693382, Miami, Florida, USA. Association for Computational Linguistics. Huan Zhang, Yu Song, Ziyu Hou, Santiago Miret, and Bang Liu. 2024b. Honeycomb: flexible llm-based agent system for materials science. In Findings of the Association for Computational Linguistics: EMNLP 2024, pages 33693382. Jiayi Zhang, Chuang Zhao, Yihan Zhao, Zhaoyang Yu, Ming He, and Jianping Fan. 2024c. Mobileexperts: dynamic tool-enabled agent team in mobile devices. CoRR, abs/2407.03913. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. 2023. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural Information Processing Systems, 36:4659546623. Li Zhong, Zilong Wang, and Jingbo Shang. 2024a. Debug like human: large language model debugger via verifying runtime execution step by step. In ACL (Findings), pages 851870. Association for Computational Linguistics. Qihuang Zhong, Kang Wang, Ziyang Xu, Juhua Liu, Liang Ding, Bo Du, and Dacheng Tao. 2024b. Achieving 97% on gsm8k: Deeply understanding the problems makes llms perfect reasoners. arXiv preprint arXiv:2404.14963."
        },
        {
            "title": "A Additional Related Work",
            "content": "A.1 Multi-agent systems The landscape of AI system architectures encompasses two distinct paradigms: multi-agent systems and autonomous agents (Zhuge et al., 2023; Hong et al., 2024a; Zhang et al., 2024c; Wang et al., 2023). While autonomous agents rely on independent decision-making capabilities, multi-agent systems excel through structured collaboration between specialized components. The latter approach offers practical advantages by building upon established expertise rather than requiring complex behavioral modeling. Research in multi-agent frameworks has evolved along two primary trajectories. The first focuses on domain-agnostic systems that leverage collective intelligence for general problem-solving (Wei et al., 2022; Diao et al., 2024; Wang et al., 2022; Madaan et al., 2023; Wang et al., 2024). The second pathway explores domain-specific applications, with notable implementations in: Code generation and debugging (Hong et al., 2024b; Ridnik et al., 2024; Zhong et al., 2024a) Data analytics (Xie et al., 2024; Ye et al., 2024; Li et al., 2024; Zhou et al., 2023b) Mathematical reasoning (Zhong et al., 2024b; Xu et al., 2024) Knowledge retrieval (Nori et al., 2023; Zhou et al., 2024) Despite significant progress in identifying effective agent configurations for specific use cases, the field still faces the challenge of developing systematic approaches for new domains. This highlights the importance of research into automated methods for framework design and optimization. Schema in Perovskite-KG Table 5 presents comprehensive schema for the Perovskite-KG, organized into three main ontological categories: Fabrication, Parameters, and Performance. The Fabrication ontology encompasses process-related attributes such as coating parameters, methods, and annealing conditions. The Parameters ontology covers structural and compositional aspects including solvents, device architecture, and additives. The Performance ontology captures various stability metrics and efficiency parameters like thermal stability, light stability, and power conversion efficiency. Each category is further detailed with specific data types and examples to ensure precise knowledge representation. This structured schema enables systematic organization and retrieval of perovskite solar cell information while maintaining data consistency across the knowledge graph."
        },
        {
            "title": "C Prompts",
            "content": "The system employs four specialized agents, each with carefully designed prompts to perform specific tasks in the perovskite solar cell knowledge processing pipeline: 1. Information Extraction Agent  (Table 6)  : Processes research papers using structured set of 20 predefined questions across seven key categories, including device structure, performance enhancement, stability, and materials. The agent returns answers in standardized JSON format, marking unavailable information as Not mentioned to maintain data quality. 2. Verification Agent  (Table 7)  : Validates extracted information by comparing it with source texts, focusing on maintaining accuracy of technical details like numerical values and material names. The agent provides both corrected content and justification for any modifications made. 3. Organization Agent  (Table 8)  : Synthesizes verified information from multiple sources into coherent, topic-focused responses. This agent ensures that complex technical information is presented in logical and accessible manner. 4. LLM-Judge  (Table 9)  : Evaluates response quality across four key criteria: accuracy, completeness, relevance, and clarity. Using 1-5 scoring system, this agent provides detailed assessments and explanations for each criterion, along with an overall evaluation summary."
        },
        {
            "title": "D Instruction Tuning Dataset",
            "content": "D.1 Dataset Statistics The research questions in perovskite solar cell studies are systematically categorized in Tables 1 and 10. Table 1 provides high-level overview of seven major research categories, including Device Structure and Fabrication, Performance Enhancement Strategies, Performance Metrics Improvement, Stability Improvements, Defect and Recombination Ontology Sub-Category Data Type Description Fabrication Coating Parameter Float The specifics of the coating method used in the material deposition process. Method String Different fabrication techniques, involving variations in material deposition. Parameters Performance Annealing Parameter Float Refers to the heating conditions applied to the perovskite, which are essential for crystallization and stability. Solvent String the liquid medium used to dissolve precursors, helping to form uniform perovskite layer Device Structure Additive Thermal Stability Light Stability Moisture Stability Fill Factor Value Open-Circuit Voltage Value Short-Circuit Current Value Power Conversion Efficiency Value Patterned String String String String String The architecture of the device (e.g., layer order, material interfaces) Any additional materials or chemicals The materials ability to withstand heat without degrading How resistant the material is to prolonged exposure to light. The materials resilience against humidity or water exposure. Float measure of the devices maximum power output. Float Float Float The maximum voltage the device can produce under open-circuit conditions. The current density when the circuit is closed. The efficiency with which the device converts sunlight into electricity. Example 5000 rpm, 100µl spin coating 120C, 10min Dimethylformamide (DMF) ITO/SAM/perovskite /C60/BCP/Cu potassium ions >98% of initial efficiency of >24% after 1,500 hours of continuous maximum power point tracking >92% of initial performance for 1,200 hours under the damp-heat test (85C and 85% relative humidity) Initial PCE of control, target-1 and target-2 devices is 21.73%, 24.42% and 24.11%, respectively. Degraded to 78% of initial PCE after 1,500 hours at 555C 0.88 1.2 25 mA/cm2 25 % Table 5: Schema in Perovskite-KG. represent smaller but significant portions of the research focus, with 1,586 (2.9%) and 952 (1.7%) entries respectively. Figure 6 illustrates the length distribution patterns across different categories in our perovskite instruction tuning dataset."
        },
        {
            "title": "E Instruction Tuning",
            "content": "Table 12 presents the training hyperparameters for Perovskite-LLM."
        },
        {
            "title": "G License",
            "content": "GPQA (Rein and Minal., erva (Lewkowycz et al., 2022) are under MIT license. 2023) et Management, Interface and Extraction Layer Enhancements, and Materials Used in Perovskite Solar Cells. Each category is accompanied by detailed rationale explaining its scope and relevance. Table 10 further expands this classification by presenting 21 specific research questions (Q1-Q21) organized within these seven categories. The questions cover wide range of technical aspects, from device architecture optimization and performance enhancement strategies to material characteristics and stability improvements. Each research question is paired with its corresponding technical focus, providing comprehensive framework for understanding the key areas of investigation in highperformance perovskite solar cell research. Table 11 shows the distribution of research categories in perovskite solar cells. Device Structure and Fabrication dominates the field, accounting for 24,198 entries (43.8% of total). Performance Enhancement Strategies represents the second largest category with 11,233 entries (20.3%), followed by Defect and Recombination Management with 7,209 entries (13.0%). Stability Improvements, crucial aspect of perovskite solar cell development, comprises 5,399 entries (9.8%), while Performance Metrics Improvement accounts for 4,527 entries (8.2%). Materials Used in Perovskite Solar Cells and Interface and Extraction Layer Enhancements Figure 6: Distribution of prompt and response lengths across different categories in our dataset (top: prompt, bottom: response). Question: How to fabricate the perovskite solar cells those can reach PCE over 25%? Perovskite-Chat-LLM: To fabricate perovskite solar cells that can achieve power conversion efficiency (PCE) over 25%, several key steps and considerations are essential. First, the fabrication process should start with clean and dry substrate, typically glass or flexible substrates, which are cleaned thoroughly using combination of detergents and solvents. The substrates are then treated with UV ozone to enhance their wetting properties. For the hole transport layer (HTL), materials like PEDOT:PSS are commonly used. These materials are spin-coated onto the substrate and annealed at temperature of 150C for 15 minutes. The perovskite precursor solution is then prepared, typically using mixed cation and halide composition, and spin-coated onto the HTL. The spin-coating process involves two-step procedure: first at 1,000 rpm for 10 seconds, followed by 6,000 rpm for 30 seconds, with anisole added during the second step to facilitate crystallization. After spin-coating, the perovskite films are annealed at 100C for 10 minutes. The electron transport layer (ETL) is then deposited, often using solution of PC61BM in chlorobenzene, which is spin-coated at 1,000 rpm for 30 seconds and annealed at 100C for 10 minutes. Finally, metal electrode, typically gold, is deposited through thermal evaporation. The entire process is conducted in nitrogen-filled glovebox to minimize exposure to moisture. ChatGPT: To fabricate perovskite solar cells with power conversion efficiency (PCE) over 25%, several key factors need to be considered and optimized in the fabrication process. Here are some steps that can help in achieving high-efficiency perovskite solar cells: 1. Perovskite Material Quality: The quality of the perovskite material is crucial for achieving high efficiency. Use high-quality perovskite precursors and optimize the fabrication process to ensure uniform and defect-free perovskite films. 2. Optimal Perovskite Layer Thickness: The thickness of the perovskite layer plays significant role in determining the efficiency of the solar cell. Optimize the thickness of the perovskite layer to maximize light absorption and charge carrier generation. 3. Tuning the Bandgap: By engineering the composition of the perovskite material, you can tune the bandgap to match the solar spectrum for better light absorption and improved efficiency. 4. Interface Engineering: Pay attention to the interfaces between different layers in the solar cell structure. Optimize the interfaces to minimize recombination losses and improve charge carrier transport. 5. Passivation Layers: Use passivation layers to reduce defects and trap states in the perovskite layer, which can improve the overall performance of the solar cell. 6. Electron and Hole Transport Layers: Choose suitable electron and hole transport materials to facilitate efficient charge extraction and transport within the solar cell. Figure 7: full conversation between Perovskite-LLM and ChatGPT, Perovskite-LLM provides detailed operational steps with specific parameters, while ChatGPT only offers general conceptual guidance. Table 6: Prompts for Information Extraction Agent. Prompts for Information Extraction Agent: Answer the following questions based on the provided text. { Device Structure and Fabrication: [ Q1: Summarize the device structures or configurations of the perovskite solar cells those can reach PCE over 25%., Q2: How to prepare the perovskite precursor solutions those can reach PCE over 25%?, Q3: How to fabricate the perovskite solar cells those can reach PCE over 25%? ], Performance Enhancement Strategies: [ Q4: What are problems solved in literatures that report perovskite solar cells those can reach PCE over 25%?, Q5: What are the reasons to choose the strategies that can enhance performance of the perovskite solar cells in literatures that report perovskite solar cells those can reach PCE over 25%? ], Performance Metrics Improvement: [ Q6: How to improve the VOC of perovskite solar cells?, Q7: How to improve the FF of perovskite solar cells?, Q8: How to improve the Jsc of perovskite solar cells? ], Stability Improvements: [ Q9: How to improve the moisture stability of perovskite solar cells?, Q10: How to improve the thermal stability of perovskite solar cells?, Q11: How to improve the illumination or light stability of perovskite solar cells? ], Defect and Recombination Management: [ Q12: How to passivate or reduce defects/traps of perovskite solar cells?, Q13: How to reduce recombination of perovskite solar cells? ], Interface and Extraction Layer Enhancements: [ Q14: How to improve the wettability of the buried interface in perovskite solar cells?, Q15: How to improve the hole extraction ability of HTL in perovskite solar cells?, Q16: How to improve the electron extraction ability of ETL in perovskite solar cells? ], Materials Used in Perovskite Solar Cells: [ Q17: What are the HTL materials used in perovskite solar cells and the common features of them?, Q18: What are the ETL materials used in perovskite solar cells and their features?, Q19: What are the hole blocking layer materials in perovskite solar cells and their features?, Q20: What are the passivation materials used in perovskite solar cells and their common features? ] } Below is the text: {paper text} Response: Return JSON object with the following structure, if the text does not contain the answer, return Not mentioned: { questions: [ question: Q1, answer: Answer to Question 1 question: Q2, answer: Not mentioned { }, { }, ... ] } Table 7: Prompts for Verification Agent. Prompts for Verification Agent: You need to verify the accuracy of the extracted information from perovskite paper. Compare the extracted data with the original text to ensure consistency and correctness. Highlight any discrepancies and fix them. Moreover, maintain the original meaning of the text and the original information, such as numbers and material names. Input: Paragraph {Section name}:{Text of the section} Extracted: {Extracted information} Output: Verified information with notes on any discrepancies or confirmation of accuracy. Please return JSON object with the following structure only return one item: { verified info: { fixed content: The fixed paragraph, reason: The reason for the fix } } Table 8: Prompts for Organization Agent. Prompts for Organization Agent: Your task is to organize the verified information from perovskite paper related to the question: {question}. Below is the information split into paragraphs that answers the question: {answers} Output: The organized and continuous answer to the question. Return JSON object with the following structure: { answer: The organized and continuous answer to the question. } Table 9: Prompts for LLM-Judge. Prompts for LLM-Judge: You are an expert evaluator. Your task is to compare models response to the ground truth answer and provide detailed evaluation. Models response: {model response} Ground truth: {ground truth} Please evaluate the models response based on the following criteria: 1. Accuracy: How factually correct is the models response compared to the ground truth? 2. Completeness: Does the models response cover all the key points mentioned in the ground truth? 3. Relevance: How well does the models response address the implied question or task? 4. Clarity: Is the models response clear and easy to understand? For each criterion, provide score from 1 to 5, where 1 is the lowest and 5 is the highest. Also, provide brief explanation for each score. Finally, give an overall score from 1 to 5 and summary of your evaluation. Format your response as JSON object with the following structure: { accuracy: { score: 0, explanation: }, completeness: { score: 0, explanation: }, relevance: { score: 0, explanation: }, clarity: { score: 0, explanation: }, overall: { score: 0, summary: } } ID Research Question Technical Focus I. Device Structure and Fabrication Q1 Q2 Summarize device structures for PCE >25% Perovskite precursor solution preparation for PCE >25% Fabrication methods for PCE >25% Q3 II. Performance Enhancement Strategies Q4 Problems solved in high-efficiency (>25%) devices Rationale for performance enhancement strategies Q5 III. Performance Metrics Improvement VOC improvement methods Q6 FF improvement methods Q7 JSC improvement methods Q8 Q9 PLQY-iVOC relationship IV. Stability Improvements Q10 Q11 Q"
        },
        {
            "title": "Moisture stability enhancement\nThermal stability enhancement\nLight stability enhancement",
            "content": "V. Defect and Recombination Management Defect/trap passivation methods Q13 Recombination reduction approaches Q14 VI. Interface and Extraction Layer Enhancements Buried interface wettability improvement Q15 HTL hole extraction enhancement Q16 ETL electron extraction enhancement Q17 VII. Materials Used in Perovskite Solar Cells Q18 Q19 Q20 Q"
        },
        {
            "title": "HTL materials and features\nETL materials and features\nHole blocking layer materials\nPassivation materials and features",
            "content": "Device architecture optimization Solution chemistry and processing Manufacturing processes Critical challenges and solutions Strategic approach justification Open-circuit voltage optimization Fill factor enhancement Short-circuit current density optimization Photoluminescence quantum yield correlation Water resistance strategies Temperature tolerance methods Photo-stability improvement"
        },
        {
            "title": "Hole transport materials\nElectron transport materials\nBlocking layer characteristics\nSurface passivation materials",
            "content": "Table 10: Systematic Classification of Research Questions in High-Performance Perovskite Solar Cell Studies Abbreviated Name Full Name Perf. Enhancement Stability Defect & Recom. Device Structure Metrics Materials Interface Performance Enhancement Strategies Stability Improvements Defect and Recombination Management Device Structure and Fabrication Performance Metrics Improvement Materials Used in Perovskite Solar Cells Interface and Extraction Layer Enhancements Count 11,233 5,399 7,209 24,198 4,527 1,586 952 Table 11: Correspondence between abbreviated and full names of research categories in perovskite solar cells Parameter Chat-LLM Reasoning-LLM Learning Rate Batch Size Number of Epochs Optimizer Learning Rate Scheduler Warmup Steps 1e-6 8 3 AdamW Cosine 20 5e-6 32 15 AdamW Cosine 20 Table 12: Training Hyperparameters for Perovskite-Chat-LLM and Perovskite-Reasoning-LLM Question: What are the ETL materials used in perovskite solar cells and the features of them? Perovskite-Chat-LLM: In perovskite solar cells, various electron transport layer (ETL) materials are utilized, each with distinct features. Common ETL materials include: 1. Tin Oxide (SnO2): SnO2 is popular choice for ETLs due to its high electron mobility, low-temperature process ability, and compatibility with flexible substrates. It is often used in n-i-p structured perovskite solar cells. 2. Zinc Oxide (ZnO): ZnO is another widely used ETL material, known for its high electron mobility and low-temperature processing. It is commonly used in both n-i-p and p-i-n structured perovskite solar cells. 3. Tungsten Oxide (WO3): WO3 is utilized as an interlayer between the perovskite layer and the ETL, enhancing charge extraction and improving device stability. It is particularly effective in p-i-n structured devices. 4. Phenyl-C61-Butyric Acid Methyl Ester (PCBM): PCBM is fullerene derivative used as an ETL in p-i-n structured perovskite solar cells. It offers high electron mobility and is compatible with various perovskite compositions. 5. Tin Oxide Doped with Fluorine (F-SnO2): This material is used to enhance the performance of SnO2 ETLs, improving their electron mobility and stability, which leads to higher efficiency in perovskite solar cells. 6. Cesium Lead Halide (CsPbX3): CsPbX3 is employed as wide-bandgap ETL in allinorganic perovskite solar cells, contributing to improved stability and efficiency. ... ChatGPT: ETL (Electron Transport Layer) materials play crucial role in perovskite solar cells by facilitating the efficient extraction and transport of electrons generated by the perovskite layer. Some commonly used ETL materials in perovskite solar cells include: 1. TiO2 (Titanium Dioxide): TiO2 is one of the most widely used ETL materials in perovskite solar cells. It has good electron transport properties, high electron mobility, and is chemically stable. TiO2 can be easily processed into thin films, making it suitable for largescale production of solar cells. 2. SnO2 (Tin Oxide): SnO2 is another popular ETL material in perovskite solar cells. It has high electron mobility, good transparency, and can be deposited at low temperatures. SnO2 films can also be easily doped to improve their conductivity and electron extraction efficiency. ... Figure 8: full conversation between Perovskite-LLM and ChatGPT, Perovskite-LLM provides detailed information about the ETL materials and their features, while ChatGPT only offers general information about the ETL materials. Question: How to improve the thermal stability of perovskite solar cells? Perovskite-Chat-LLM: Improving the thermal stability of perovskite solar cells involves several strategies. One effective approach is the use of thermally stable materials for the charge transporting layer, such as nickel oxide (NiO) and copper thiocyanate (CuSCN), which can enhance the thermal stability of the devices. Additionally, the incorporation of additives like 1,8-diodoctane (DIO) and 1-chloronaphthalene (CN) has been shown to improve the morphology of perovskite films, leading to better thermal stability. Furthermore, the use of thermally stable hole transport materials (HTMs) and electron transport materials (ETMs) is crucial, as they can withstand high temperatures without degrading the perovskite layer. The development of new HTMs, such as poly(3-hexylthiophene) (P3HT), has also contributed to improved thermal stability. Lastly, the introduction of interfacial layers, like graphene oxide, can enhance the thermal stability of the perovskite layer by reducing defects and improving charge transport. ... ChatGPT: Improving the thermal stability of perovskite solar cells is crucial for their long-term performance and reliability. Here are some strategies to enhance their thermal stability: 1. Encapsulation: One of the most effective ways to improve the thermal stability of perovskite solar cells is to encapsulate them with protective materials. Encapsulation helps to shield the cells from environmental factors such as moisture, oxygen, and heat, which can degrade the perovskite material. 2. Improved Perovskite Composition: Researchers are constantly working on developing new perovskite compositions that are more stable at high temperatures. By tweaking the chemical composition of the perovskite material, it is possible to enhance its thermal stability. ... Figure 9: full conversation between Perovskite-LLM and ChatGPT, Perovskite-LLM provides detailed information about the thermal stability of perovskite solar cells, while ChatGPT only offers general information about the thermal stability of perovskite solar cells. Question: Can you tell me what is CuO? And give me 5 usage in perovskite. Perovskite-Chat-LLM: CuO refers to copper(II) oxide, chemical compound that plays significant role in various applications, particularly in perovskite solar cells (PSCs). Here are five key usages of CuO in perovskite technology: 1. Hole Transport Material: CuO acts as an effective hole transport material (HTM) due to its suitable work function and high hole mobility. It is often used in conjunction with other materials, such as iOx, to enhance the performance of PSCs by facilitating efficient charge extraction [1]. 2. Interface Optimization: The use of CuO helps in optimizing the interfaces within perovskite solar cells. By depositing CuO films on top of perovskite layers, it can improve the interfacial properties and reduce recombination losses at these critical junctions[2]. 3. Thermal Stability Enhancement: Copper oxide (CuO) has been shown to provide thermal stability to perovskite materials, which is crucial for maintaining high efficiency during prolonged operation under elevated temperatures. This property makes it an attractive choice for applications where environmental stability is concern [3]. 4. Doping and Passivation: CuO can be utilized as dopant in other materials used in PSCs, such as SnO2, enhancing their electrical properties like conductivity and mobility. Additionally, the doping effects can lead to improved passivation of defects at the perovskite layer, contributing to better performance metrics [4]. 5. Integration with Other Materials: In bifacial solar cells, CuO has been integrated with materials such as carbon nanotubes (CNTs) to create high-performance architectures that benefit from synergistic effects. This integration not only enhances charge transport but also improves the overall efficiency of the devices [5]. References: [1] Inorganic Materials as Hole Selective Contacts and Intermediate Tunnel Junction Layer for Monolithic Perovskite-CIGSe Tandem Solar Cells, Wang, Yajie; Wenisch, Robert; Schlatman [2] Critical Review of Cu-Based Hole Transport Materials for Perovskite Solar Cells: From Theoretical Insights to Experimental Validation, Sun, Qingde; Sadhu, Anupam; Lie, Stener; Wong, Lydia Helena, Advanced Materials, 2024 [3] Processing-Performance Evolution of Perovskite Solar Cells: From Large Grain Polycrystalline Films to Single Crystals, Haque, Md Azimul; Troughton, Joel; Baran, Derya, Advanced Energy Materials, 2020 [4] Air-Processed Perovskite Solar Cells with 25% Efficiency and High Stability Enabled by Crystallization Modulation and Holistic Passivation, Shi, Xiaoyu; Liu, Tianxiao; Dou, Yunjie; Hu, Xiaodong; Liu, Yangyang; Wang, Feifei; Wang, Lingyuan; Ren, Zhijun; Chen, Shangshang, Advanced Materials, 2024 [5] Critical Review of Cu-Based Hole Transport Materials for Perovskite Solar Cells: From Theoretical Insights to Experimental Validation, Sun, Qingde; Sadhu, Anupam; Lie, Stener; Wong, Lydia Helena, Advanced Materials, 2024 Figure 10: case study of Perovskite-Chat-LLMs ability to provide detailed and accurate information with references."
        }
    ],
    "affiliations": [
        "Guangzhou Municipal Key Laboratory of Materials Informatics",
        "The Hong Kong University of Science and Technology (Guangzhou)"
    ]
}