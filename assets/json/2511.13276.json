{
    "paper_title": "Recognition of Abnormal Events in Surveillance Videos using Weakly Supervised Dual-Encoder Models",
    "authors": [
        "Noam Tsfaty",
        "Avishai Weizman",
        "Liav Cohen",
        "Moshe Tshuva",
        "Yehudit Aperstein"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We address the challenge of detecting rare and diverse anomalies in surveillance videos using only video-level supervision. Our dual-backbone framework combines convolutional and transformer representations through top-k pooling, achieving 90.7% area under the curve (AUC) on the UCF-Crime dataset."
        },
        {
            "title": "Start",
            "content": "Recognition of Abnormal Events in Surveillance Videos using Weakly Supervised Dual-Encoder Models Noam Tsfaty* Intelligence Systems Afeka College of Engineering Tel Aviv, Israel Email: Tsfaty.Noam@s.afeka.ac.il ORCID: 0009-0009-5246-8274 Avishai Weizman* School of Electrical and Computer Engineering Ben-Gurion University of the Negev Beersheba, Israel Email: wavishay@post.bgu.ac.il ORCID: 0009-0004-1182-8601 Liav Cohen Intelligence Systems Afeka College of Engineering Tel Aviv, Israel Email: liav.cohen@s.afeka.ac.il ORCID: 0009-0009-2756-8783 Moshe Tshuva Mechanical Engineering Afeka College of Engineering Tel Aviv, Israel Email: moshet@afeka.ac.il ORCID: 0000-0002-0828-5595 Yehudit Aperstein Intelligence Systems Afeka College of Engineering Tel Aviv, Israel Email: apersteiny@afeka.ac.il ORCID: 0000-0002-0828AbstractWe address the challenge of detecting rare and diverse anomalies in surveillance videos using only video-level supervision. Our dual-backbone framework combines convolutional and transformer representations through top-k pooling, achieving 90.7% area under the curve (AUC) on the UCF-Crime dataset. Index TermsAnomaly Detection, Surveillance Videos, Multiple Instance Learning, Temporal Modeling. I. INTRODUCTION Automatically detecting abnormal events in surveillance videos remains challenge because such events are rare. Obtaining precise temporal annotations for large-scale dataset is often impractical due to the high cost of labeling, ambiguity in defining anomaly boundaries, and inconsistencies between annotators. These constraints make weakly supervised learning with video-level labels an efficient alternative for real-world settings. Multiple Instance Learning (MIL) supports this setting by treating each video as bag of temporal segments, where an anomalous label least one segment is abnormal. Early approaches [1] and later frameworks such as MSTAgent-VAD [2] employ MIL with video-level labels to localize anomalies, incorporating temporal smoothness and attention mechanisms. In [3], this paradigm was further improved through Robust Temporal Feature Magnitude (RTFM) learning, which enhances MIL by separating feature magnitudes and incorporating multi-scale temporal modeling to reveal informative abnormal cues. Other perspectives use multi-sequence learning [4] enhances temporal consistency by ranking short sequences of consecutive snippets and refining their labels through transformer-based self-training process, implies that at *These authors contributed equally to this work. while the clustering method in [5] improves stability by encouraging compact grouping of normal patterns and increasing their separation from anomalous ones. In [6] proposed class-agnostic anomaly detection pipeline that leverages Inflated 3D ConvNet (I3D) feature embeddings and threshold-based scoring, providing simpler but less discriminative baseline for surveillance data. In this work, we propose dual-backbone MIL network that combines two complementary video encoders: an I3D model [7] for spatiotemporal motion analysis and transformer-based architecture (TimeSformer) [8] to enrich the overall video representation. The fused outputs of these encoders are processed by lightweight fully connected (FC) layers, and the resulting features are aggregated using top-k pooling as used in [9] to produce video-level predictions. II. PROPOSED METHOD"
        },
        {
            "title": "The proposed method is suitable for",
            "content": "the UCF-Crime dataset [1], where only about 6% of the video durations are longer than ten minutes, while most video durations last only few minutes. Following the approach of [1] and [2], our process begins by considering collection of videos defined as = {v1, v2, . . . , vM }, where represents the total number of videos in the dataset. Each video vm is divided into sequence of segments vm = {um,1, um,2, . . . , um,N }, where denotes the total number of segments in video vm. These segments serve as the fundamental processing units for our encoders. Every video vm is associated with single videolevel label ym {0, 1}, indicating whether the video is normal or anomalous. Each video is uniformly divided into = 32 temporal segments. From each segment um,i, 16 frames are uniformly sampled, forming shorter segment xm,i = 5 2 0 2 7 1 ] . [ 1 6 7 2 3 1 . 1 1 5 2 : r Fig. 1: Illustration of the dual-backbone MIL framework. Each video is divided into 32 temporal segments (vm). From each segment (um,i), 16 frames (xm,i) are uniformly sampled to form shorter segment, which is encoded by I3D (convolutionalbased) and TimeSformer (transformer-based) encoders. The concatenated and ℓ2-normalized features are processed by compact prediction head and aggregated through top-k pooling to produce the final video-level anomaly prediction. m,i, 2 m,i, . . . , 16 {F 1 m,i}. Segments containing fewer than 16 frames are padded by repeating the last frame until reaching the fixed length of 16 frames. This sampling strategy provides an effective balance between computational cost and temporal coverage, and is well-suited to the UCF-Crime dataset [1], where long segments are relatively rare. Each shorter segment is then encoded using two pretrained backbones: the I3D encoder (768-dimensional vector) and the TimeSformer encoder (1024-dimensional vector), both operating on 16frame inputs. The extracted features are ℓ2-normalized and concatenated to form unified feature representation (1792dimensional vector). The objective is to learn function (xm,i; θ) that assigns higher anomaly scores to abnormal segments while producing lower scores for normal ones. Each segment feature fm,i, derived from the corresponding segment xm,i, is then passed through four FC layers that output scalar anomaly score. LeakyReLU activation is applied between layers, producing final score si that indicates the degree of abnormality for the i-th segment. Following the MIL assumption, only some segments in an anomalous video are abnormal, whereas all segments in normal video are regular. To obtain video-level representation, the highest segment scores are aggregated through top-k pooling, formulated as: z(V ) = 1 (cid:88) si, iTopK(s,k) (1) where z(V ) denotes the video-level anomaly logit, ˆy = σ(z(V )) represents the predicted anomaly probability obtained through the sigmoid function σ(), and is hyper-parameter that determines the number of top segments used in the aggregation. All components of the model are jointly optimized using the binary cross-entropy loss. The proposed architecture, illustrated in Figure 1, computes segment-level anomaly scores, then applies top-k pooling to produce the final video prediction. III. EXPERIMENTS AND RESULTS Anomalies in the UCF-Crime dataset [1] are defined as irregular or criminal activities captured in untrimmed surveillance videos covering 13 anomaly categories and normal events. Each video is annotated at the video level, with varying TABLE I: Comparison of anomaly detection methods on the UCF-Crime dataset. Method Ours Zhao et al. [2] Wu et al. [10] Li et al. [4] Tian et al. [3] CLAWS Net+ [5] Zhong et al. [11] Strijbosch [6] Sultani et al. [1] Cho et al. [12] Model Feature fusion + Top-k MIL Transformer (VideoSwin) CLIP + Prompt Learning Multi-Sequence Learning (VideoSwin) I3D Clustering-Aided MIL GCN-LNC (GCN + Classifier) I3D + Clustering C3D + MIL-ranking ITAE + Normalizing Flows AUC (%) 90.7 89.3 88.4 85.6 84.3 84.1 82.1 80.7 75.4 70. durations, scenes, and complexity. As shown in Table I, our novel dual-backbone fusion model achieves the highest AUC, outperforming all competing approaches. The compared approaches span wide range of strategies to tackle this task, including transformer-based models, contrastive languageimage pre-training (CLIP) [13]-based vision-language methods, I3D combined with clustering, and graph-based approaches such as graph convolutional networks (GCNs). Although these approaches tackle anomaly detection from different angles, our dual-encoder model achieves superior AUC performance over all other methods on the UCF-Crime dataset. IV. CONCLUSIONS AND FUTURE WORK This work presented novel architecture for video anomaly detection that integrates I3D and TimeSformer encoders with lightweight FC layers. The proposed framework demonstrated strong performance on the UCF-Crime dataset, achieving high AUC despite being restricted to uniform framesampling strategy due to computational limitations. This design was guided by the assumption that the UCF-Crime dataset rarely contains long continuous videos, making uniform sampling sufficient to capture the relevant temporal dynamics. In future work, we plan to extend the framework toward multiclass anomaly detection and investigate advanced sampling strategies to enhance the temporal representation of the video segments."
        },
        {
            "title": "REFERENCES",
            "content": "[1] W. Sultani, C. Chen, and M. Shah, Real-world anomaly detection in surveillance videos, in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018, pp. 6479 6488. [2] S. Zhao, R. Zhao, Y. Meng, X. Gu, C. Shi, and D. Li, Mstagent-vad: Multi-scale video anomaly detection using time agent mechanism for segments temporal context mining, Expert Systems with Applications, vol. 276, p. 127154, 2025. [3] Y. Tian, G. Pang, Y. Chen, R. Singh, J. W. Verjans, and G. Carneiro, Weakly-supervised video anomaly detection with robust temporal feature magnitude learning, in Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2021, pp. 49754986. [4] S. Li, F. Liu, and L. Jiao, Self-training multi-sequence learning with transformer for weakly supervised video anomaly detection, in Proceedings of the AAAI Conference on Artificial Intelligence (AAAI), vol. 36, no. 2, 2022, pp. 13951403. [5] M. Z. Zaheer, A. Mahmood, M. Astrid, and S.-I. Lee, Clustering assisted weakly supervised learning with normalcy suppression for anomalous event detection, in Computer Vision ECCV 2020, ser. Lecture Notes in Computer Science, vol. 12367. Springer, 2020, pp. 358376. [6] D. Strijbosch, Towards usable crime-based anomaly detection model, Masters thesis, Delft University of Technology, 2024. [7] J. Carreira and A. Zisserman, Quo vadis, action recognition? new model and the kinetics dataset, in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017, pp. 6299 6308. [8] G. Bertasius, H. Wang, and L. Torresani, Is space-time attention all you need for video understanding? in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2021, pp. 66896699. [9] W. Wu, T. Dai, X. Huang, F. Ma, and J. Xiao, Top-k pooling with patch contrastive learning for weakly-supervised semantic segmentation, in Proceedings of the IEEE International Conference on Systems, Man, and Cybernetics (SMC), 2024, pp. 52705275. [10] P. Wu, X. Zhou, G. Pang, Z. Yang, Q. Yan, P. Wang, and Y. Zhang, Weakly supervised video anomaly detection and localization with spatio-temporal prompts, in arXiv preprint arXiv:2408.05905, 2024. [11] J. Zhong, S. Li, T. Kong, C. Liu, Y. Xie, Q. Li, and R. Ji, Graph convolutional label noise cleaner: Train plug-and-play action classifier for anomaly detection, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2019, pp. 1237 1246. [12] M. Cho, T. Kim, W. J. Kim, S. Cho, and S. Lee, Unsupervised video anomaly detection via normalizing flows with implicit latent features, Pattern Recognition, vol. 129, p. 108703, 2022. [13] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, G. Krueger, and I. Sutskever, Learning transferable visual models from natural language supervision, in International Conference on Machine Learning, 2021."
        }
    ],
    "affiliations": [
        "Afeka College of Engineering",
        "Ben-Gurion University of the Negev"
    ]
}