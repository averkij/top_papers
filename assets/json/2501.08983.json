{
    "paper_title": "CityDreamer4D: Compositional Generative Model of Unbounded 4D Cities",
    "authors": [
        "Haozhe Xie",
        "Zhaoxi Chen",
        "Fangzhou Hong",
        "Ziwei Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "3D scene generation has garnered growing attention in recent years and has made significant progress. Generating 4D cities is more challenging than 3D scenes due to the presence of structurally complex, visually diverse objects like buildings and vehicles, and heightened human sensitivity to distortions in urban environments. To tackle these issues, we propose CityDreamer4D, a compositional generative model specifically tailored for generating unbounded 4D cities. Our main insights are 1) 4D city generation should separate dynamic objects (e.g., vehicles) from static scenes (e.g., buildings and roads), and 2) all objects in the 4D scene should be composed of different types of neural fields for buildings, vehicles, and background stuff. Specifically, we propose Traffic Scenario Generator and Unbounded Layout Generator to produce dynamic traffic scenarios and static city layouts using a highly compact BEV representation. Objects in 4D cities are generated by combining stuff-oriented and instance-oriented neural fields for background stuff, buildings, and vehicles. To suit the distinct characteristics of background stuff and instances, the neural fields employ customized generative hash grids and periodic positional embeddings as scene parameterizations. Furthermore, we offer a comprehensive suite of datasets for city generation, including OSM, GoogleEarth, and CityTopia. The OSM dataset provides a variety of real-world city layouts, while the Google Earth and CityTopia datasets deliver large-scale, high-quality city imagery complete with 3D instance annotations. Leveraging its compositional design, CityDreamer4D supports a range of downstream applications, such as instance editing, city stylization, and urban simulation, while delivering state-of-the-art performance in generating realistic 4D cities."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 1 ] . [ 1 3 8 9 8 0 . 1 0 5 2 : r 1 CityDreamer4D: Compositional Generative Model of Unbounded 4D Cities Haozhe Xie, Zhaoxi Chen, Fangzhou Hong, and Ziwei Liu Abstract3D scene generation has garnered growing attention in recent years and has made significant progress. Generating 4D cities is more challenging than 3D scenes due to the presence of structurally complex, visually diverse objects like buildings and vehicles, and heightened human sensitivity to distortions in urban environments. To tackle these issues, we propose CityDreamer4D, compositional generative model specifically tailored for generating unbounded 4D cities. Our main insights are 1) 4D city generation should separate dynamic objects (e.g., vehicles) from static scenes (e.g., buildings and roads), and 2) all objects in the 4D scene should be composed of different types of neural fields for buildings, vehicles, and background stuff. Specifically, we propose Traffic Scenario Generator and Unbounded Layout Generator to produce dynamic traffic scenarios and static city layouts using highly compact BEV representation. Objects in 4D cities are generated by combining stuff-oriented and instance-oriented neural fields for background stuff, buildings, and vehicles. To suit the distinct characteristics of background stuff and instances, the neural fields employ customized generative hash grids and periodic positional embeddings as scene parameterizations. Furthermore, we offer comprehensive suite of datasets for city generation, including OSM, GoogleEarth, and CityTopia. The OSM dataset provides variety of real-world city layouts, while the Google Earth and CityTopia datasets deliver large-scale, high-quality city imagery complete with 3D instance annotations. Leveraging its compositional design, CityDreamer4D supports range of downstream applications, such as instance editing, city stylization, and urban simulation, while delivering state-of-the-art performance in generating realistic 4D cities. Index TermsCity Generation, 4D Generation, Generative Models, NeRF"
        },
        {
            "title": "1 INTRODUCTION",
            "content": "A MID the rise of the metaverse, 3D and 4D asset generation has garnered significant attention. Notable progress has been made in generating 3D objects [1], [2], [3], avatars [4], [5], [6], and scenes [7], [8], [9], as well as 4D objects [10], [11] and avatars [12], [13], [14]. Cities, as one of the most essential assets, are widely used in diverse applications such as urban planning, environmental simulations, and game asset development. Therefore, the challenge of making 3D/4D city development accessible to wider audience, including artists, researchers, and players, becomes both significant and impactful. In recent years, notable advancements have been made in scene generation. Video-based methods [15], [16], [17] generate 3D scenes by producing videos conditioned on input images, but they cannot guarantee temporal consistency. Outpainting-based methods [18], [19], [20] generate 3D scenes through continuous outpainting on RGB and depth images, but they lack compact scene representation, resulting in scenes that are typically small in scale. PCGbased methods [21], [22], [23] create unbounded cities by integrating large language models (LLMs) with procedural content generation (PCG), but the diversity of the generated cities is constrained by the 3D assets employed. 3Daware-GAN-based methods, represented by GANCraft [24] This study is supported by the Ministry of Education, Singapore, under its MOE AcRF Tier 2 (MOE-T2EP20221-0012), NTU NAP, and under the RIE2020 Industry Alignment Fund Industry Collaboration Projects (IAF-ICP) Funding Initiative, as well as cash and in-kind contributions from the industry partner(s). (Corresponding author: Ziwei Liu.) The authors are with S-Lab, Nanyang Technological University, Singapore 637335 (email: haozhe.xie@ntu.edu.sg; zhaoxi001@ntu.edu.sg; fangzhou.hong@ntu.edu.sg; ziwei.liu@ntu.edu.sg) Project page is available at https://haozhexie.com/project/city-dreamer-4d and SceneDreamer [7], use volumetric neural rendering to generate images within 3D scene, leveraging 3D coordinates and corresponding semantic labels. These methods show promising results in generating 3D natural scenes by leveraging pseudo-ground-truth images generated by SPADE [25]. InfiniCity [26] follows similar pipeline for 3D city generation but it is more complex than 3D natural scenes due to the greater appearance variation in buildings and vehicles, unlike the relatively consistent appearance of objects with the same semantic label in natural scenes. This variation reduces the quality of generated buildings and vehicles when all instances within their respective classes are assigned the same semantic label. Generating 4D scenes poses greater challenges than 3D scenes, as existing methods [27], [28], [29], [30] either fail to ensure temporal consistency or are confined to tiny scales. To address these problems, we propose CityDreamer4D, compositional generative model designed for unbounded 4D cities. As shown in Fig. 1, the unbounded 4D city generation framework separates dynamic objects from static scenes. Static scenes are defined by the city layout from Unbounded Layout Generator, arranging elements like roads, highways, vegetation, and buildings, with the capability to extrapolate to unbounded areas. Dynamic objects, such as vehicles, are defined by traffic scenarios generated by Traffic Scenario Generator, which determines their spatial positioning on high-fidelity (HD) maps derived from city layouts. Unlike existing methods that use single module for all objects, CityDreamer4D divides the generation process into three distinct modules: Building Instance Generator for buildings, Vehicle Instance Generator for vehicles, and City Background Generator for background stuff. These generators leverage highly compact birds-eye-view (BEV) scene representation to ensure efficiency and scalability. The scene parameterization is designed to address the unique characteristics of background stuff and instances: background stuff often features similar appearances with irregular textures, while buildings and vehicles display diverse appearances with regular periodic patterns. To handle these variations, we use generative hash grids for the background and apply periodic positional encodings to each instance. We also place buildings in an object-centric coordinate space and vehicles in an object-canonical coordinate space, using specialized methods designed to capture their compact shapes. Compositor combines the rendered background stuff with the building and vehicle instances to create unified image. To improve the realism of our generated cities, we construct suite of datasets, including OSM, GoogleEarth, and CityTopia. The OSM dataset, sourced from OpenStreetMap [31], includes semantic maps and height fields for 80 cities worldwide, covering over 6,000 km2. The semantic maps indicate the locations of roads, buildings, urban greenery, and water bodies, while the height fields primarily represent building heights. The GoogleEarth dataset is realworld dataset collected using Google Earth Studio [32], featuring 400 drone-view orbit trajectories over New York City. It includes 24,000 real-world city images, with 3D semantic annotations for all classes and 3D instance annotations for buildings. The CityTopia dataset is high-quality synthetic dataset spanning 11 cities generated with 3D assets from the Unreal Engine 5 City Sample project [33]. It offers 37,500 high-fidelity street-view and drone-view images, featuring precise 2D and 3D semantic annotations for all classes, along with 3D instance annotations for buildings and vehicles. The contributions are summarized as follows: We propose CityDreamer4D, the first generative model for unbounded 4D cities that disentangles dynamic objects from static scenes and enables instance editing, city stylization, and urban simulation. We introduce stuff-oriented and instance-oriented neural fields to generate background stuff and instances (buildings and vehicles) in 4D scenes, effectively capturing their diversity. We create comprehensive datasets for city generation, using OSM for realistic layouts and Google Earth and CityTopia for detailed city visuals with 3D semantic and instance annotations. The proposed CityDreamer4D demonstrates superior capability in generating unbounded, diverse 4D cities and enables instance-level editing within them. preliminary version of this work, named CityDreamer, has been published in CVPR 2024 [34]. We make several extensions in this work compared to the preliminary version. 1) We evolve CityDreamer into CityDreamer4D, enabling 4D city generation through Traffic Scenario Generator and Vehicle Instance Generator, effectively separating dynamic objects from static scenes. 2) We enhance the highly compact BEV representation by incorporating an additional bottomup height map, enabling the representation of hollow structures in cities, such as highways. 3) We propose Traffic Scenario Generator, which creates high-fidelity maps from city layouts to produce realistic traffic scenarios with vehicles in unbounded cities. 4) We introduce Vehicle Instance Generator, designed to generate vehicle instances within cities using novel scene parameterization method grounded on the canonical feature space. 5) We build the CityTopia dataset, offering nearly 40k high-quality street-view and drone-view images with both 2D and 3D semantic and instance annotations."
        },
        {
            "title": "2 RELATED WORKS",
            "content": "2.1 3D-aware GANs Building on the recent success of 2D GANs [35], [36], various approaches have been introduced to generate 3D content using GANs as well. The core idea is to represent the generated scenes using 3D representation and apply rendering techniques to produce images from various viewpoints, enabling image-level adversarial learning [37]. Early methods use explicit shapes like voxels [38], [39], [40], meshes [41], and 3D primitives [42] to render images from different viewpoints. However, their limited expressiveness and efficiency hinder the synthesis of complex scenes and photorealistic details. NeRF [43], known for producing high-fidelity novel view synthesis, are introduced to 3Daware generative models. Yet, the high computational cost of NeRF-based GANs restricts earlier attempts [44], [45], [46], [47] from generating high-quality images. To address this, many follow-up works [48], [49], [50], [51], [52] avoid rendering NeRFs at high resolution by applying 2D superresolution on low-resolution feature maps, though this compromises 3D consistency. To maintain strict 3D consistency, newer approaches shift to sparser 3D representations, like sparse voxels [53], radiance manifolds [54], and multi-plane images [55], enabling direct high-resolution rendering. Nevertheless, most of these methods are trained on curated datasets for bounded scenes, such as human faces [56], [57], human bodies [58], [59], and objects [60], [61]. 2.2 3D Scene Generation Unlike advanced 2D generative models that mainly focus on individual categories or familiar objects, generating scenelevel content is more challenging due to the vast diversity and complexity of scenes. Earlier methods [15], [16] generate scenes by synthesizing videos, but they lack 3D awareness and fail to ensure 3D consistency. Semantic image synthesis approaches [24], [62] have shown promising results in generating scene-level content by conditioning on pixelwise dense correspondences, like semantic segmentation or depth maps. Several techniques [18], [19], [20] generate 3D scenes by performing inpainting and outpainting on RGB images or feature maps, though most can only interpolate or extrapolate limited distance from the input views and lack true generative capabilities. Significant research has investigated procedural content generation (PCG) for creating natural [21], [63], indoor [64], [65], [66], and urban scenes [23], [67], [68], [69], but the diversity of the generated scenes is limited by the 3D assets used. Recent methods [7], [9], [26] achieve 3D-consistent scenes at an infinite scale through unbounded layout extrapolation. Other approaches [70], [71], [72] focus on indoor scene synthesis, relying on costly 3D datasets [73], [74] or CAD object retrieval [75], [76], [77]. 3 Fig. 1. Overview of CityDreamer4D. 4D city generation is divided into static and dynamic scene generation, conditioned on and Tt, produced by Unbounded Layout Generator and Traffic Scenario Generator, respectively. City Background Generator uses to create background images ˆIG for stuff like roads, vegetation, and the sky, while Building Instance Generator renders the buildings {ˆIBi } within the city. Using Tt, Vehicle Instance Generator generates vehicles {ˆIt } at time step t. Finally, Compositor combines the rendered background, buildings, and vehicles into Vi unified and coherent image ˆIt . Gen., Mod., Cond., BG., BLDG., and VEH. denote Generation, Modulation, Condition, Background, Building, and Vehicle, respectively. 2.3 4D Scene Generation In recent years, representations like D-NeRF [78] and Deformable 3D Gaussians [79] have been proposed for 4D object and human generation. However, 4D scene generation remains in its early stages, as existing representations are not designed for large-scale scene generation. Mainstream approaches typically formulate it as 4D occupancy generation [80], [81] and distillation from video diffusion [27], [28], [29], [30]. However, these methods lack compact representations, restricting the scale of the generated scenes."
        },
        {
            "title": "3 METHOD",
            "content": "along with its corresponding mask MG. Following this, Building Instance Generator (Section 3.4) generates images for building instances {ˆIBi}nB i=1 and their respective masks {MBi}nB i=1, where nB is the number of building instances. For dynamic object generation, the traffic generator (Section 3.2) first creates the traffic scenario Tt for time step t. Then, Vehicle Instance Generator (Section 3.5) produces images for vehicle instances {ˆIt }nV i=1 and their correspondVi }nV ing masks {Mt i=1 based on Tt, where nV denotes the Vi number of vehicles. Finally, Compositor (Section 3.6) merges the rendered background, building instances, and vehicle instances into cohesive image It for time step t. As illustrated in Figure 1, CityDreamer4D decouples unbounded 4D city generation into static scene generation and dynamic object generation. For static scene generation, Unbounded Layout Generator (Section 3.1) creates an arbitrarily large city layout L. City Background Generator (Section 3.3) then produces the background image ˆIG"
        },
        {
            "title": "3.1 Unbounded Layout Generator",
            "content": "City Layout Representation. The city layout defines the locations of static 3D objects within the city, which are grouped into categories such as roads, highways, buildings, vegetation, water areas, and others. Additionally, null class is included to represent empty spaces in the 3D volume. The city layout in CityDreamer4D, represented as 3D volume L, is constructed by extruding pixels from the semantic map SL according to their corresponding values in the height , where HBU field HL = represent the bottom-up heights and the top-down heights, respectively. Specifically, the value of at (i, j, k) is defined as and HTD , HTD (cid:8)HBU (cid:9) L(i, j, k) = (cid:40) SL(i, j) 0 (i, j) HTD (i, j) if HBU otherwise (1) where 0 denotes empty spaces in the 3D volumes. City Layout Generation. Obtaining unbounded city layouts is translated into generating extendable semantic maps and height fields. To achieve this, we design Unbounded Layout Generator based on MaskGIT [82], which naturally supports inpainting and extrapolation. Specifically, we leverage VQVAE [83] to tokenize patches of semantic maps and height fields, encoding them into discrete latent space with codebook = (cid:8)ck ck RdC (cid:9)dK k=1. During inference, the layout tokens are generated autoregressively, and the VQVAE decoder reconstructs pair of semantic map SL and height field HL. Since VQVAE produces fixed-size outputs, we perform image extrapolation to create arbitrarily large layouts. This involves using sliding window with 25% overlap to iteratively predict local layout tokens at each step. Loss Functions. The VQVAE handles the generation of the height field and semantic map as separate tasks, optimized with L1 Loss and Cross-Entropy Loss E, respectively. To enhance the sharpness of the height field near building edges, we incorporate an additional Smoothness Loss [84] ℓVQ = λR ˆHp where ˆHp denote the generated height field and semantic map patches, respectively. Hp are the corresponding ground truth. MaskGITs autoregressive transformer is optimized with reweighted ELBO loss [85]. + λSS( ˆHp L) + λEE(ˆSp and ˆSp and Sp Hp L, Hp L) (2) L, Sp"
        },
        {
            "title": "3.2 Traffic Scenario Generator\nTraffic Scenario Representation. The city layout L defines\nthe static elements of the unbounded city, while the dynamic\naspects are captured by the traffic scenario, represented as\nT = {Tt}nT\nt=1, where nT represents the number of frames.\nSimilar to the city layout L, Tt is likewise derived from the\n(cid:9)\nsemantic map STt and the height field HTt = (cid:8)HBU\n,\nTt\nwhere the semantic map specifies the positions of dynamic\nobjects, and the height field defines their elevations. Specif-\nically, the value of Tt at (i, j, k) is",
            "content": ", HTD Tt Tt(i, j, k) = (cid:40) STt(i, j) 0 (i, j) HTD Tt if HBU Tt otherwise (i, j) (3) where 0 denotes empty spaces in the 3D volumes. Traffic Scenario Generation. The generation of traffic scenario is conceptualized as the frame-by-frame production of semantic maps ST = {STt}nT t=1 and height fields HT = {HTt}nT t=1. To guarantee realistic and continuous placement of dynamic objects, high-fidelity (HD) map is derived from the city layout L. Unlike the city layout, which only specifies the positions of roads and highways, the HD map includes details about lanes, intersections, and 4 traffic signals. Using the generated HD map, an off-theshelf model [86] determines the per-frame bounding boxes of dynamic objects. The corresponding semantic map and height field are generated based on the bounding boxes. HD Map Generation. In HD maps, we adopt the entity definitions from the Waymo Motion dataset [87], which include road edges, road lanes, road lines, stop signs, and traffic lights. Road Edges, representing the boundaries of roads, are generated by applying Canny edge detection [88] to SL and converting the continuous edges into graph structure using vectorization, which involves detecting corner points and connecting them sequentially. Road Lanes, representing the centerlines of lanes where vehicles can travel, are derived by skeletonizing [89] SL to extract road structures and identifying intersections where multiple edges connect. The image is then converted into road centerline graphs using graph-based traversal. The number and positions of the lanes are determined based on road width, and lanes at intersections are connected using Bezier curves. Road Lines, such as solid single white or solid double yellow, are generated according to the positions and attributes of the road lanes. Stop Signs and Traffic Lights are positioned at the intersections, where multiple road lanes converge."
        },
        {
            "title": "3.3 City Background Generator",
            "content": "G D Scene Representation. Following SceneDreamer [7], we adopt birds-eye-view (BEV) representation for its efficiency and expressiveness, particularly suited for unbounded scenes. Unlike GANCraft [24] and InfiniCity [26], which parameterize features at voxel corners, our BEV representation uses feature-free 3D volume constructed from height field and semantic map, as described in Equation 1. Specifically, we extract local window of resolution from the city layout L. This local window LG is generated using the corresponding height field HG and semantic map SG . Scene Parameterization. To achieve generalizable 3D representation learning across various scenes and align content with 3D semantics, it is necessary to parameterize the scene representation into latent space, making adversarial learning easier. For background stuff, we adopt the generative neural hash grid [7] to learn generalizable features across scenes by modeling the hyperspace beyond 3D space. Specifically, we first encode the local scene (HG ) using the global encoder EG to produce the compact scene-level feature fG RdG . , SG fG = EG(HG , SG ) (4) Using learnable neural hash function H, the indexed feature at the 3D position R3 is derived by mapping and fG into hyperspace, specifically R3+dG RN . = H(p, fG) = (cid:16) dG(cid:77) i=1 Gπi 3 (cid:77) j=1 pjπj(cid:17) mod NE (5) where represents the bitwise XOR operation, while πi and πj are distinct large prime numbers. To capture multiscale features, we construct levels of multi-resolution hash grids. Each level contains up to NE entries, with denoting the number of channels in each feature vector. Volumetric Rendering. In the perspective camera model, every pixel in the image is associated with camera ray r(t) = + tv, which originates at the projection center and extends along the direction v. The pixel value C(r) is then computed as an integral along this ray. Note that O() is applied separately to each element of the feature x, with the values normalized to the range [1, 1]. Volumetric Rendering. Unlike the volumetric rendering approach used in City Background Generator, Building Instance Generator incorporates style code to capture the variability in building appearances. The pixel value C(r) is computed through an integration process. 5 (6) C(r) = (cid:90) 0 A(t)c(f r(t) Bi , z, l(r(t)))σ(f r(t) Bi )dt (11) C(r) = (cid:90) 0 (cid:16) A(t)c(f r(t) )dt , l(r(t)))σ(f r(t) (cid:17) ), ds (cid:82) 0 σ(f r(s) where A(t) = exp represents the accumulated transmittance. l(p) denotes the semantic label at the 3D position p. The symbols and σ correspond to the color and volume density, respectively. Loss Function. City Background Generator is optimized with hybrid objective that combines reconstruction loss and adversarial loss. In particular, it uses an L1 loss, perceptual loss [90], and GAN loss [91] as part of this objective. ℓG = λL1 ˆIG IG + λP GP(ˆIG, IG) + λG GG(ˆIG, SG) (7) where IG represents the ground truth background image, while SG corresponds to the perspective-view semantic map obtained by accumulating semantic labels sampled from LG along each ray. The weights for the three losses are denoted by λL1 G. Note that ℓG is only applied to pixels whose semantic labels are classified as background stuff. G, and λG , λP"
        },
        {
            "title": "3.4 Building Instance Generator",
            "content": "B and SBi Scene Representation. Building Instance Generator also employs the BEV scene representation. It extracts local window LBi from the city layout with dimensions D . This window is centered around the , cBi 2D coordinates (cBi ) of the building instance Bi. The height field and semantic map used to construct LBi are represented as HBi , respectively. Since all buildings share the same semantic label in SL, we perform building instantiation by detecting connected components. Notably, real-world building facades and roofs exhibit distinct visual distributions. To capture this, we assign different semantic labels to the facade and roof of each building instance Bi in LBi , with the roof assigned to the top-most voxel layer. All other building instances are excluded from LBi by assigning them value of 0. Scene Parameterization. Unlike City Background Generator, Building Instance Generator employs distinct scene parameterization, encoding the local scene (HBi ) with EB to produce pixel-level features fBi of resolution W , SBi . fBi = EB(HBi , SBi ) (8) For 3D position = (px, py, pz), the corresponding feature Bi is obtained as Bi = O(Concat(fBi(px, py), pz)) (9) where Concat() denotes the concatenation operation. fBi (px, py) RN represents the feature vector corresponding to the coordinates (px, py). O() refers to the positional encoding function adopted in the standard NeRF [43]. O(x) = {sin(2iπx), cos(2iπx)}N 1 (10) i=0 where r(t) = + tv (cid:2)cBi , cBi center the buildings within their local coordinate system. Loss Function. The training of Building Instance Generator relies solely on the GAN loss G, formulated as , which is employed to , 0(cid:3)T ℓB = G(ˆIBi , SBi) where SBi represents the semantic map of the building instance Bi in perspective view, generated similarly to SG. Note that ℓB is only applied to pixels with semantic labels corresponding to the building instance. (12)"
        },
        {
            "title": "3.5 Vehical Instance Generator",
            "content": "x , cVi Tt and SVi are represented as HVi Scene Representation. Vehicle Instance Generator, like Building Instance Generator, leverages the BEV scene representation. It extracts local window TVi from the traffic scenario Tt, with dimensions D , to generate the vehicle instances within the scene. This window is centered around the 2D coordinates (cVi ) of the vehicle instance Vi. The height field and semantic map used to construct TVi Tt , respectively. Unlike buildings, vehicle instances are instantiated during the generation of the traffic scenario. Instances other than Vi are removed from TVi Scene Parameterization. Compared to building instances, vehicle instances demonstrate greater structural regularity, closely tied to their relative positions. For instance, within the same vehicle, the front, rear, and body exhibit distinct appearances, yet these structural features remain consistent across different vehicles. Building on this observation, we propose scene parameterization method based on the canonical feature space. Given 3D position = (px, py, pz), the canonicalized point pC is (cid:18) (cid:105)T(cid:19) by assigning them value of 0. (cid:104) pC = cVi , cVi , cVi (13) , cVi , cVi where cVi represent the center coordinates of the vehicle Vi along the X, Y, and axes, respectively. is the rotation matrix used to normalize the 3D point into the canonical feature space. = cos θ sin θ cos γ sin θ sin γ sin θ cos θ cos γ cos θ sin γ 0 sin γ cos γ (14) where θ (180, 180] denotes the yaw angle, indicating the vehicles heading in the XY-plane relative to the y-axis, while γ (90, 90) represents the pitch angle, with positive or negative values indicating upward or downward tilt relative to the XY-plane. The feature (pC ,t) to the vehicle Vi at time step for pC is derived as corresponding Vi (pC ,t) Vi = O(Concat(f Vi , pC)) (15) TABLE 1 Comparison of Statistics and Properties: GoogleEarth, CityTopia, and Previous Datasets. Only annotated images are counted. Ext. stands for Extendable, indicating whether the dataset can be easily expanded following the current data generation pipeline. 3DM., Sem., and Inst. refer to 3D Model, Semantic, and Instance, respectively. 6 Dataset KITTI [92] Cityscapes [93] AeroScapes [94] nuScenes [95] GTA-V [96] SYNTHIA [97] VEIS [98] MatrixCity [99] HoliCity [100] KITTI-360 [101] UrbanScene3D [102] GoogleEarth #Images (103) #Cities Area (km2) Source Ext. 3DM. Lighting View Type Day Night StreetAerial Dense Annotations 2D Sem. 2D Inst. 3D Sem. 3D Inst. 0.2 25 3.2 93 25 213 61 519 6.3 78 6.1 1 50 - 2 - 1 - 2 1 1 - 1 - - - - - - - 28 20 - 3 Real Real Real Real Synthetic Synthetic Synthetic Synthetic Real Real Real Real CAD CAD Mesh Voxel CityTopia Only the real-world image subset is counted for this dataset. 3.75 36 Synthetic Voxel where Vi scene (HVi Tt RdV is the features extracted from the local , SVi Tt ) using the global encoder EV."
        },
        {
            "title": "4 DATASETS\n4.1 OSM Dataset",
            "content": "f Vi = EV(HVi Tt , SVi Tt ) (16) Volumetric Rendering. The volumetric rendering mirrors Building Instance Generator, using style code to represent the variability in vehicle appearances. The pixel value C(r) is calculated through an integration process as described in Equation 11. The camera ray r(t) is normalized to the canonical feature space following Equation 13. Loss Function. Vehicle Instance Generator is optimized with hybrid objective that integrates reconstruction and adversarial objectives. Specifically, the training process incorporates an L1 loss, perceptual loss P, and GAN loss to balance fidelity and realism. ℓV = λL1 ˆIt Vi It Vi + λP P(ˆIt Vi , It Vi ) + λG G(ˆIt Vi , St Vi ) (17) The OSM dataset, collected from OpenStreetMap [31], includes rasterized semantic maps and height fields for 80 cities across the globe, covering more than 6,000 km2. In the rasterization step, vector data is transformed into images by converting longitude and latitude coordinates into the EPSG:3857 coordinate system at zoom level 18, which gives resolution of approximately 0.597 meters per pixel. As shown in Fig. 2, The segmentation maps use different colors to indicate various elements: red for roads, yellow for buildings, green for urban greenery, cyan for construction areas, and blue for water bodies. The height fields mainly capture building elevations, based on OpenStreetMap data. The heights for roads are set to 4, water bodies at 0, and urban greenery is assigned random heights, generated using Perlin noise [103] within range of 8 to 16 meters. where It Vi denotes the ground truth image of the vehicle instance Vi at time step t, while St Vi is the corresponding perspective-view semantic map, generated in manner similar to SG. The weights of the three losses are represented as λL1 . Note that ℓV is applied exclusively to pixels with semantic labels belonging to the vehicle instance. , and λG , λP"
        },
        {
            "title": "3.6 Compositor",
            "content": "As there are no ground truth images available for the outputs generated by City Background Generator, Building Instance Generator, and Vehicle Instance Generator, training neural networks to combine these images becomes challenging. Consequently, Compositor merges the generated images and their corresponding masks into one unified image. IC = ˆIGMG + nB(cid:88) i=1 ˆIBiMBi + nV(cid:88) i=1 ˆIt Vi"
        },
        {
            "title": "Mt\nVi",
            "content": "(18)"
        },
        {
            "title": "4.2 GoogleEarth Dataset",
            "content": "CityDreamer4D generates each building instance in the city separately to handle the diversity of buildings, which requires dense 3D instance annotations. As shown in Table 1, no existing dataset provides both dense 3D semantic and instance annotations. To address this, we automatically generate dense 3D semantic and building instance annotations for the GoogleEarth dataset by geographically aligning Google Earth and OpenStreetMap using latitude and longitude. Image Collection. The GoogleEarth dataset, collected from Google Earth Studio [32], includes 400 orbit trajectories over the New York City, totaling 24,000 images at 960x540 resolution. As shown in Fig. 2c, orbit radii range from 125 to 813 meters, with altitudes from 112 to 884 meters. Google Earth Studio also provides camera intrinsic and extrinsic parameters for each image. 2D and 3D Annotation. The 3D annotations can be generated by: 1) performing connected components detection 7 Fig. 2. Overview of the OSM and GoogleEarth Datasets. (a) Examples of the 2D and 3D annotations in the GoogleEarth dataset, which can be automatically generated using the OSM dataset. (b) The automatic annotation pipeline can be readily adapted for worldwide cities. (c) The dataset statistics highlight the diverse perspectives in the GoogleEarth dataset. on the OSM semantic map to create the instance map for buildings, while keeping the labels for background stuff unchanged, and 2) generating 3D volumes by extruding the pixels in the instance map based on height values from the OSM dataset. The dense 3D annotations can be used to create 2D annotations by projecting the 3D volumes onto images, leveraging the camera parameters from Google Earth Studio. Fig. 2a shows the 2D and 3D instance annotations in the GoogleEarth dataset, highlighting the efficiency of automated data annotation. Fig. 2b shows how the automated annotation pipeline can be applied to cities worldwide."
        },
        {
            "title": "4.3 CityTopia Dataset",
            "content": "The GoogleEarth dataset provides images with dense 3D semantic and instance annotations but faces three challenges: 1) it lacks street-view images due to suboptimal 3D reconstructions near ground level in Google Earth Studio [32]; 2) its annotations, sourced from OpenStreetMap [31], have some imprecision due to differing data sources; and 3) elevated structures like highways remain unannotated due Fig. 3. Overview of the CityTopia Dataset. (a) The virtual city generation pipeline. Pro.Inst., Sur.Spl, and 3D Inst. Anno. denote Prototype Instantiation, Surface Sampling, and 3D Instance Annotation, respectively. (b) Examples of 2D and 3D annotations in the CityTopia dataset are shown from both daytime and nighttime street-view and aerial-view perspectives, automatically generated during virtual city generation. (c) The dataset statistics highlight the diverse perspectives in both street and aerial views. to missing height data in OpenStreetMap. To address these challenges, we construct the CityTopia dataset, featuring precise 3D dense annotations on high-fidelity day and night images from both street and aerial views. As shown in Table 1, it is the largest dataset to date, offering unparalleled scene diversity and detailed annotations for urban cities. Virtual City Generation. To build the CityTopia dataset, we design 11 virtual cities in Houdini and Unreal Engine, generating 3D annotations and realistic images with controlled lighting conditions. As illustrated in Fig. 3(a), we use diverse, high-quality set of approximately 5,000 3D assets from the CitySample project [33] to procedurally generate city prototype in Houdini1. This city prototype stores the 6D poses of all 3D assets within the city. Through surface sampling, we can assign each 3D point semantic and instance label, and by instantiating the city prototype in Unreal Engine2, we produce fully generated virtual city. Image Collection. Once the virtual city is instantiated in Unreal Engine, camera trajectories are set to generate 3,000 images for cities with buildings and 7,500 for vehicle1. https://www.sidefx.com 2. https://www.unrealengine.com TABLE 2 Quantitative Comparison. The best values are highlighted in bold. Note that InfiniCity is not included in this comparison as it is not open-sourced. Methods FID KID GoogleEarth VBench SGAM [104] PersistentNature [105] SceneDreamer [7] DreamScene4D [106] DimensionX [107] CityDreamer4D (Ours) 277.6 123.8 232.2 - 206.9 96. 0.358 0.109 0.204 - 0.182 0.096 0.691 0.706 0.781 - 0.805 0.834 DE 0.575 0.326 0.153 - - 0.138 CE 239.2 86.37 0.186 - - 0. FID KID CityTopia VBench 330.1 235.3 195.1 288.2 171.4 88.48 0.284 0.215 0.126 0.136 0.070 0.049 0.690 0.713 0.708 0.715 0.815 0. DE 0.571 0.428 0.185 0.199 - 0.150 CE 233.5 127.3 0.162 0.146 - 0.063 only city. Daytime and nighttime scenes are rendered for each trajectory, with sunlight removed to help the network more easily learn lighting consistency during the generation process. To avoid Moire effects, each image is sampled 8x spatially and 32x temporally during rendering. As shown in Fig. 3(c), the CityTopia dataset provides wider range of viewpoints, shown by its broader elevation angles compared to the GoogleEarth dataset, as well as more streetlevel perspectives, evidenced by the large number of images taken at near-zero altitude. 2D and 3D Annotation. Since the precise 3D annotations are natively generated from the virtual city pipeline, once the camera poses are set in Unreal Engine, 2D annotations are produced by projecting the 3D annotations using the given camera poses. Fig. 3(b) highlights the perfect alignment of 2D and 3D instance annotations with both street-view and aerial-view images in the CityTopia dataset. The last row features vehicle-only scene, enhancing vehicle generation learning. The accurate vehicle annotations demonstrate the effectiveness of the pipeline, which can be scaled by adding more 3D assets."
        },
        {
            "title": "5.1 Evaluation Protocols",
            "content": "We evaluate our method by generating 1,024 unique city layouts, each with 20 variations created by randomly sampling the style code z. For each variation, images are rendered at resolution of 960 540 pixels using randomized camera trajectories. Frames from these renderings are randomly selected for evaluation, depending on the specific metrics used. The evaluation metrics are as follows. FID and KID. Frechet Inception Distance (FID) [108] and Kernel Inception Distance (KID) [109] measure image quality. FID and KID are calculated between 15,000 generated frames and 15,000 randomly sampled images from datasets. VBench. VBench [110] provides comprehensive evaluation of video generative models, considering dimensions such as background consistency, motion smoothness, dynamic degree, aesthetic quality, and imaging quality. The VBench score is computed from 150 videos, each consisting of 100 frames rendered at 16 FPS. Depth Error (DE). To assess 3D geometry, DE is evaluated following EG3D [52]. pretrained model [111] generates pseudo ground truth depth maps by accumulating density σ. DE is calculated as the L2 distance between the normalized depth maps, evaluated on 100 frames per method. Camera Error (CE). CE measures multi-view consistency, following SceneDreamer [7]. CE is computed on static 3D scene by comparing the inferred camera trajectory with the one estimated by COLMAP [112]. This metric is calculated on 600 frames rendered from an orbit trajectory and is defined as the scale-invariant normalized L2 distance between the generated and reconstructed camera poses."
        },
        {
            "title": "5.2 Implementation Details",
            "content": "G = 10, λP = 1536, = 3072, = 3072, and = 16, NE = 219, and Hyperparameters Unbounded Layout Generator. The codebook size dK is set to 512, with each code having dimension dC of 512. Height field and semantic map patches are cropped to 512512 and compressed by factor of 16. The loss weights are λR = 10, λS = 10, and λE = 1. City Background Generator. For the GoogleEarth dataset, the local window resolutions are set to = 1536, and = 640. For the CityTopia dataset, they are set to = 2560. The dimension of scene-level features dG is 2. For the generative hash grid, = 8. The prime numbers used in Equation 5 are π1 = 1, π2 = 2654435761, π3 = 805459861, π4 = 3674653429, and π5 = 2097192037. The loss function weights are set to λL1 = 10, and λG = 0.5. Building Instance Generator. For the GoogleEarth dataset, the local window resolutions are set to = 672, and = 640. For the CityTopia dataset, these values are = 2560. The pixel-level features have 63 channels (N = 63), and the dimension Vehicle Instance Generator. The dimension of scene-level features dV is 2. The local window resolutions are set to = 32. The loss function weights are assigned as λL1 = 0.5. Training Details Unbounded Layout Generator. The VQVAE model is trained over 1,250,000 iterations using batch size of 16, an Adam optimizer with β = (0.5, 0.9), and learning rate of 7.2 105. The autoregressive transformer is trained for 250,000 iterations with batch size of 80, an Adam optimizer with β = (0.9, 0.999), and learning rate of 2 104. Stuff and Instance Generators. The City Background Generator, Building Instance Generator, and Vehicle Instance Generator are trained with an Adam optimizer, using β = (0, 0.999) and learning rate of 104. The discriminators use the same optimizer settings with learning rate of 105. = 768, and = 32, and = 10, and λG = 768, = 32, = 672, P is set to 10. = 10, λP 9 Fig. 4. Qualitative Comparison on Google Earth. For SceneDreamer [7] and CityDreamer4D, vehicles are generated using models trained on CityTopia due to the lack of semantic annotations for vehicles in Google Earth. For DimensionX [107], the initial frame is provided by CityDreamer4D. The visual results of InfiniCity [26], provided by the authors, have been zoomed in for better viewing. Pers.Nature stands for PersistentNature [105]. Training runs for 298,500 iterations with batch size of 8, and images are randomly cropped to 192192 resolution."
        },
        {
            "title": "5.3 Main Results",
            "content": "Comparison Methods. We compare CityDreamer4D against several state-of-the-art methods, including SGAM [104], PersistentNature [105], SceneDreamer [7], and InfiniCity [26]. Since no method exists for 4D scene generation, we use DreamScene4D [106] for 4D novel view synthesis and DimensionX [107] for 4D video generation as competitive baselines. To ensure fair comparison, all methods, except for InfiniCity and DimensionX, are retrained using their released code on the GoogleEarth and CityTopia datasets. Since SceneDreamer cannot generate city layouts or traffic scenarios, their inputs are supplied by Unbounded Layout Generator and Traffic Scenario Generator. Additionally, because the GoogleEarth dataset lacks annotations for dynamic objects, vehicles are generated using models trained on the CityTopia dataset to support 4D generation. Qualitative Comparison. Fig. 4 and 5 present qualitative comparisons with the baseline methods on the GoogleEarth and CityTopia datasets, respectively. SGAM faces difficulties in generating realistic results and maintaining multi-view consistency due to the inherent challenges of extrapolating views for complex 4D cities. PersistentNature, which adopts tri-plane representation, also struggles to produce realistic renderings. Both InfiniCity and SceneDreamer use BEV maps as their scene representation, but they still experience significant structural distortions in instance-level objects, such as buildings and vehicles, because all instances are assigned the same semantic label. DreamScene4D cannot directly generate 4D scenes but transforms monocular videos 10 Fig. 5. Qualitative Comparison on CityTopia. The initial frame for DimensionX and the input frames for DreamScene4D are chosen from the dataset. Pers.Nature refers to PersistentNature [105]. Perceptual Quality 4D Realism View Consistency 5 4 3"
        },
        {
            "title": "A M",
            "content": "G s. u fi"
        },
        {
            "title": "I n",
            "content": "n it e r m e e m 4 e n n it D e 4 Fig. 6. User Study on 4D City Generation. All scores are in the range of 5, with 5 indicating the best. Pers.Nature refers to PersistentNature [105]. into 4D scenes by decoupling dynamic objects from the background, yet it struggles to reconstruct their 3D shapes. During the generation of orbit 4D videos, DimensionX exhibited severe distortions and failed to maintain multiview consistency in the results. In comparison, the proposed CityDreamer4D generates more realistic and diverse results compared to all the baselines3. Quantitative Comparison. Table 2 shows the quantitative metrics, where CityDreamer4D outperforms the baselines in FID, KID, and VBench, highlighting its motion smoothness, dynamic degree, and aesthetic quality. Additionally, CityDreamer4D achieves the lowest DE and CE errors, demonstrating accurate 3D geometry, view consistency, and photorealistic image generation. User Study. To better evaluate the multi-view consistency and quality of unbounded 4D city generation, we per3. More results can be found on our project page. 11 Fig. 7. Qualitative Comparison of City Layout Generators. The height map values are normalized to range of [0, 1] by dividing each value by the maximum value within the map. TABLE 3 Quantitative Comparison of Ubounded Layout Generator (UIG). The best values are highlighted in bold. The generated images are centrally cropped to size of 40964096. TABLE 4 Quantitative Comparison of Building Instance Generator Variants. The best values are highlighted in bold. Note that Inst. and Pos.Enc. refer to Instance Labels and Positional Encoding, while and denote Global Encoder and Local Encoder, respectively. Methods IPSM [113] InfinityGAN [114] UIG (Ours) FID 321.47 183.14 124.45 KID 0.502 0.288 0. form user study following CityDreamers protocol [34]. In this survey, 25 volunteers rate each generated city on three aspects: 1) perceptual quality, 2) 4D realism, and 3) view consistency. Ratings are on scale of 1 to 5, with 5 being the highest. As shown in Fig. 6, the proposed CityDreamer4D outperforms the baselines by significant margin."
        },
        {
            "title": "5.4 Ablation Studies",
            "content": "Effectiveness of Unbounded Layout Generator. Unbounded Layout Generator (ULG) is essential for producing unbounded city layouts. To demonstrate the effectiveness of ULG, we evaluate its performance against InfinityGAN [114], which is utilized in InfiniCity, alongside the rule-based city layout generation technique, IPSM [113]. Following InfiniCity [26], we use FID and KID to quantitatively evaluate the quality of the generated layouts. As illustrated in Table 3, ULG achieves the best results in terms of all metrics compared to IPSM and InfinityGAN. The qualitative results shown in Fig. 7 also demonstrate the high quality and diversity of the proposed method. Effectiveness of Building Instance Generator. We highlight the essential role of Building Instance Generator (BIG) in achieving successful unbounded 4D city generation. To validate its effectiveness, we perform an ablation study for BIG. We first compare BIG with two alternative designs: (1) Removing BIG from CityDreamer4D, effectively reverting the model to SceneDreamer, and (2) Generating all buildings simultaneously using BIG without incorporating instance BIG Inst. Encoder Pos.Enc. Evaluation Metrics Hash SinCos FID KID DE CE - - - - 195.1 167.8 196.8 197.9 182.3 88.48 0.126 0.094 0.124 0.132 0.111 0.049 0.185 0.162 0.157 0. 0.165 0.159 0.162 0.152 0.155 0.092 0.150 0.063 Fig. 8. Qualitative Comparison of Building Instance Generator (BIG) Variants. (a) and (b) illustrate the effects of removing BIG and instance labels, respectively. (c)(f) present the results of various scene parameterizations. Note that Enc. is an abbreviation for Encoder. labels. As shown in the first two rows of Table 4 and Fig. 8(a)-(b), both alternative designs result in significant degradation in generation quality, underscoring the importance of BIG and instance labels. Scene parameterization directly impacts the quality of 4D city generation. BIG uses vanilla SinCos positional encoding with pixel-wise features from the local encoder. To demonstrate the effectiveness of the scene parameterization in BIG, we compare BIG with Scene other alternative scene parameterization designs. TABLE 5 Quantitative Comparison of Vehicle Instance Generator Variants. All metrics are computed on the vehicle-only city from the CityTopia dataset. The best values are highlighted in bold. Note that Can. and Pos.Enc. refer to Canonicalization and Positional Encoding, while and denote Global Encoder and Local Encoder, respectively. VIG Can. Encoder L Pos.Enc. Evaluation Metrics Hash SinCos FID KID DE CE - - - - 419.3 273.4 229.2 273.4 142.3 200. 0.576 0.530 0.428 0.521 0.276 0.403 0.364 1.276 0.289 0.966 0.259 0.989 0.265 0.997 0.202 0.824 0.332 1.117 12 Fig. 10. Localized Editing on the Generated Cities. (a) and (c) show vehicle editing results, while (b) and (d) present building editing results. Fig. 9. Qualitative Comparison of Vehicle Instance Generator (VIG) Variants. (a) and (b) illustrate the effects of removing VIG and canonicalization, respectively. (c)(f) present the results of various scene parameterizations. Note that Enc. is an abbreviation for Encoder. parameterization plays critical role in the quality of 4D city generation. BIG leverages vanilla SinCos positional encoding combined with pixel-wise features from the local encoder. To evaluate the effectiveness of BIGs scene parameterization, we compare it with alternative designs. As shown in the last four rows of Table 4 and Fig. 8(c)-(f), using generative hash grid positional encoding results in distorted building facades, while Global Encoders with SinCos encoding introduce repetitive facade patterns. These comparisons emphasize the significance of BIGs well-designed parameterization in achieving realistic and varied results. Effectiveness of Vehicle Instance Generator. Vehicle Instance Generator (VIG) plays critical role in generating vehicles within 4D cities. To validate its effectiveness, we conduct an ablation study on VIG. We compare it with two alternative designs: (1) Removing VIG from CityDreamer4D and treating vehicles as background stuff, allowing City Background Generator to handle their generation, and (2) Generating vehicles without canonicalization, meaning they are not produced in canonical feature space. As shown in the first two rows of Table 5 and Fig. 9(a)-(b), both alternative designs lead to severe distortions in the generated results, highlighting the importance of VIG and canonicalization. Scene parameterization is equally critical in VIG. To validate this, we compare different scene parameterization designs within VIG. Currently, VIG uses vanilla SinCos positional encoding combined with global-level features from the global encoder. In the canonical feature space, combining global-level features with 3D coordinates allows Fig. 11. Text-driven City Stylization with ControlNet. The multi-view consistency is preserved in stylized Minecraft and Cyberpunk cities. the network to better share features across different vehicles, facilitating better convergence. As shown in the last row of Table 5 and Fig. 9(f), using local encoder with SinCos positional encoding, as in BIG, makes learning more challenging, resulting in incomplete vehicle shapes. Similarly, using generative hash grid in VIG leads to structural distortions by complicating the networks ability to associate texture features with 3D coordinates, as illustrated in Fig. 9(c) and 9(e) as well as the 3rd and 5th rows of Table 5."
        },
        {
            "title": "5.5 Applications",
            "content": "Urban Simulator. CityDreamer4D can be powerful urban simulator, capable of generating realistic 4D urban scenes with dynamic objects and detailed environments. Unlike traditional simulators such as CARLA [67], which are limited to predefined, bounded areas, this method supports unbounded urban scenes, creating vast, seamless cityscapes. Furthermore, it can generate both street-view and aerialview perspectives, providing richer variety of scenarios for applications like autonomous driving, urban planning, and virtual reality. Localized Editing. Benefiting from the compositional architecture, CityDreamer4D allows for localized editing on building and vehicle instances. In Fig. 10(a) and 10(c), vehicle positions and styles can be independently modified without affecting other scene elements. Similarly, as shown in Fig. 10(b) and 10(d), building appearances adapt seamlessly to varying heights while maintaining consistent style. This capability facilitates customized scene refinement in post-production. City Stylization. The generated cities can be seamlessly restyled by leveraging ControlNet [115], fine-tuning pretrained models on images created with ControlNet conditioned on HED edges. Fig. 11 shows examples of city styles such as Minecraft and Cyberpunk. These results maintain 13 ping. Lambertian shading accounts for the light direction and surface normal, resulting in uniform lighting across all directions, as shown in Fig. 13(a). Shadow mapping considers light visibility, enabling the simulation of shadows and occlusion from other objects in the scene, as shown in Fig. 13(b). The final relighting effects, with the camera positioned on the left side of the scene, are presented in Fig. 13(c). Limitations. Despite the realistic generation results, CityDreamer4D has some limitations. 1) During the inference process, buildings and vehicles are generated individually, leading to slightly higher computational cost. 2) The current implementation does not account for global illumination and reflections, which are essential for realistic night scenes. As illustrated in Fig. 14, the emitted light from buildings and vehicles does not illuminate the surrounding environment, limiting the realism of the generated cities under such conditions."
        },
        {
            "title": "6 CONCLUSION",
            "content": "In this paper, we introduce CityDreamer4D, generative model tailored for unbounded 4D city generation. Our method simplifies the process by decoupling dynamic objects from static scenes, enabling greater flexibility and realism driven by dynamic traffic scenarios and static city layouts. Objects in the 4D cities are generated using composition of stuff-oriented and instance-oriented neural fields for background stuff, buildings, and vehicles. Additionally, we construct comprehensive suite of datasets, including OSM, GoogleEarth, and CityTopia, which provide real-world city layouts and cityscapes with high-quality 3D annotations. CityDreamer4D achieves state-of-the-art performance in generating large-scale, realistic 4D cities with instance-level editing, leveraging its compositional design to capture urban diversity and unlock new opportunities for research and practical applications in urban simulation."
        },
        {
            "title": "REFERENCES",
            "content": "[4] [3] [2] [1] H. Xie, H. Yao, S. Zhang, S. Zhou, and W. Sun, Pix2Vox++: Multi-scale context-aware 3D object reconstruction from single and multiple images, IJCV, vol. 128, no. 12, pp. 29192935, 2020. J. Tang, J. Ren, H. Zhou, Z. Liu, and G. Zeng, DreamGaussian: Generative Gaussian splatting for efficient 3D content creation, in ICLR, 2024. Z. Chen, J. Tang, Y. Dong, Z. Cao, F. Hong, Y. Lan, T. Wang, H. Xie, T. Wu, S. Saito, L. Pan, D. Lin, and Z. Liu, 3DTopia-XL: High-quality 3D PBR asset generation via primitive diffusion, arXiv:2409.12957, 2024. F. Hong, Z. Chen, Y. Lan, L. Pan, and Z. Liu, EVA3D: compositional 3D human generation from 2D image collections, in ICLR, 2023. Z. Chen, F. Hong, H. Mei, G. Wang, L. Yang, and Z. Liu, PrimDiffusion: Volumetric primitives diffusion for 3D human generation, in NeurIPS, 2023. X. Liu, X. Zhan, J. Tang, Y. Shan, G. Zeng, D. Lin, X. Liu, and Z. Liu, HumanGaussian: Text-driven 3D human generation with Gaussian splatting, in CVPR, 2024. Z. Chen, G. Wang, and Z. Liu, SceneDreamer: Unbounded 3D scene generation from 2D image collections, IEEE TPAMI, vol. 45, no. 12, pp. 15 56215 576, 2023. Z. Wu, Y. Li, H. Yan, T. Shang, W. Sun, S. Wang, R. Cui, W. Liu, H. Sato, H. Li, and P. Ji, BlockFusion: Expandable 3D scene generation using latent tri-plane extrapolation, ACM TOG, vol. 43, no. 4, pp. 43:143:17, 2024. [5] [6] [7] [8] Fig. 12. COLMAP Reconstruction of 600-frame Orbital Videos. The red ring shows the camera positions, and the clear point clouds demonstrate CityDreamer4Ds consistent rendering. Note that Recon. stands for Reconstruction. Fig. 13. Directional Light Relighting Effect. (a) and (b) show the lighting intensity. (c) illustrates the relighting effect. Note that S.M. denotes Shadow Mapping. Fig. 14. Night-view Generation Results. Despite achieving realistic effects, managing global illumination in the generated scenes remains challenge. multiview consistency, enabled by the proposed scene representation and parameterization in CityDreamer4D."
        },
        {
            "title": "5.6 Discussions",
            "content": "View Consistency. To demonstrate CityDreamer4Ds multiview consistent renderings, we use COLMAP [112] for structure-from-motion and dense reconstruction on orbital videos generated using models trained on the GoogleEarth and CityTopia datasets. The video sequence comprises 600 frames at resolution of 960 540, captured from circular camera trajectory orbiting the scene at fixed height, with the camera focused on the center. Reconstruction is performed solely from the images, without specifying camera parameters. As illustrated in Fig. 12, the estimated camera poses closely align with the sampled trajectory, and the resulting point cloud is both dense and well-defined. Relighting. In CityDreamer4D, the generation of background stuff and instances is deliberately decoupled, offering two key benefits: (1) Simplified learning for building instances, vehicle instances, and background stuff, and (2) Enabling localized editing of building and vehicle instances. This approach can be viewed as an inverse rendering process, where CityDreamer4D generates the albedo, normals, and depth of urban scenes. Lighting and shading effects are then computed based on the given lighting conditions. As shown in Fig. 13, the shading effects are divided into two components: Lambertian shading and shadow map- [9] H. Xie, Z. Chen, F. Hong, and Z. Liu, GaussianCity: Generative Gaussian splatting for unbounded 3D city generation, arXiv 2406.06526, 2024. [10] Y. Jiang, L. Zhang, J. Gao, W. Hu, and Y. Yao, Consistent4D: Consistent 360 dynamic object generation from monocular video, in ICLR, 2024. J. Ren, K. Xie, A. Mirzaei, H. Liang, X. Zeng, K. Kreis, Z. Liu, A. Torralba, S. Fidler, S. W. Kim, and H. Ling, L4GM: large 4D Gaussian reconstruction model, in NeurIPS, 2024. [11] [12] Y. Ma, Z. Lin, J. Ji, Y. Fan, X. Sun, and R. Ji, X-Oscar: progressive framework for high-quality text-guided 3D animatable avatar generation, in ICML, 2024. [13] Z. Chai, C. Tang, Y. Wong, and M. S. Kankanhalli, STAR: skeleton-aware text-based 4D avatar generation with in-network motion retargeting, arXiv 2406.04629, 2024. [14] X. Guo, M. Zhang, H. Xie, C. Gu, and Z. Liu, CrowdMoGen: Zero-shot text-driven collective motion generation, arXiv 2407.06188, 2024. [15] A. Liu, A. Makadia, R. Tucker, N. Snavely, V. Jampani, and A. Kanazawa, Infinite Nature: Perpetual view generation of natural scenes from single image, in ICCV, 2021. [16] Z. Li, Q. Wang, N. Snavely, and A. Kanazawa, InfiniteNatureZero: Learning perpetual view generation of natural scenes from single images, in ECCV, 2022. [17] B. Deng, R. Tucker, Z. Li, L. J. Guibas, N. Snavely, and G. Wetzstein, Streetscapes: Large-scale consistent street view generation using autoregressive video diffusion, in SIGGRAPH, 2024. [18] Y. Liang, X. Yang, J. Lin, H. Li, X. Xu, and Y. Chen, LucidDreamer: Towards high-fidelity text-to-3D generation via interval score matching, in CVPR, 2024. J. Shriram, A. Trevithick, L. Liu, and R. Ramamoorthi, RealmDreamer: Text-driven 3D scene generation with inpainting and depth diffusion, arXiv 2404.07199, 2024. [19] [20] H. Yu, H. Duan, C. Herrmann, W. T. Freeman, and J. Wu, WonderWorld: Interactive 3D scene generation from single image, arXiv 2406.09394, 2024. [22] [21] M. Zhou, J. Hou, C. Luo, Y. Wang, Z. Zhang, and J. Peng, SceneX: Procedural controllable large-scale scene generation via largelanguage models, arXiv 2403.15698, 2024. J. Deng, W. Chai, J. Huang, Z. Zhao, Q. Huang, M. Gao, J. Guo, S. Hao, W. Hu, J. Hwang, X. Li, and G. Wang, CityCraft: real crafter for 3D city generation, arXiv 2406.04983, 2024. S. Zhang, M. Zhou, Y. Wang, C. Luo, R. Wang, Y. Li, X. Yin, Z. Zhang, and J. Peng, CityX: Controllable procedural content generation for unbounded 3D cities, arXiv 2407.17572, 2024. [23] [24] Z. Hao, A. Mallya, S. J. Belongie, and M. Liu, GANcraft: Unsupervised 3D neural rendering of minecraft worlds, in ICCV, 2021. [25] T. Park, M. Liu, T. Wang, and J. Zhu, Semantic image synthesis with spatially-adaptive normalization, in CVPR, 2019. [26] C. H. Lin, H. Lee, W. Menapace, M. Chai, A. Siarohin, M. Yang, and S. Tulyakov, InfiniCity: Infinite-scale city synthesis, in ICCV, 2023. S. Bahmani, I. Skorokhodov, V. Rong, G. Wetzstein, L. J. Guibas, P. Wonka, S. Tulyakov, J. J. Park, A. Tagliasacchi, and D. B. Lindell, 4D-fy: Text-to-4D generation using hybrid score distillation sampling, in CVPR, 2024. [27] [28] Y. Zheng, X. Li, K. Nagano, S. Liu, O. Hilliges, and S. D. Mello, unified approach for text-and image-guided 4D scene generation, in CVPR, 2024. [29] D. Xu, H. Liang, N. P. Bhatt, H. Hu, H. Liang, K. N. Plataniotis, and Z. Wang, Comp4D: Llm-guided compositional 4D scene generation, arXiv 2403.16993, 2024. [30] H. Yu, C. Wang, P. Zhuang, W. Menapace, A. Siarohin, J. Cao, L. A. Jeni, S. Tulyakov, and H. Lee, 4Real: Towards photorealistic 4D scene generation via video diffusion models, in NeurIPS, 2024. [31] https://openstreetmap.org. [32] https://earth.google.com/studio. [33] https://www.unrealengine.com/marketplace/en-US/product/ city-sample. [34] H. Xie, Z. Chen, F. Hong, and Z. Liu, CityDreamer: Compositional generative model of unbounded 3D cities, in CVPR, 2024. [35] T. Karras, S. Laine, and T. Aila, style-based generator architecture for generative adversarial networks, IEEE TPAMI, vol. 43, no. 12, pp. 42174228, 2021. 14 [36] A. Melnik, M. Miasayedzenkau, D. Makarovets, D. Pirshtuk, E. Akbulut, D. Holzmann, T. Renusch, G. Reichert, and H. J. Ritter, Face generation and editing with StyleGAN: survey, IEEE TPAMI, vol. 46, no. 5, pp. 35573576, 2024. I. J. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. WardeFarley, S. Ozair, A. C. Courville, and Y. Bengio, Generative adversarial nets, in NIPS, 2014. [37] [38] M. Gadelha, S. Maji, and R. Wang, 3D shape induction from 2D views of multiple objects, in 3DV, 2017. [39] P. Henzler, N. J. Mitra, and T. Ritschel, Escaping platos cave: 3D shape from adversarial rendering, in ICCV, 2019. [40] T. Nguyen-Phuoc, C. Li, L. Theis, C. Richardt, and Y. Yang, Hologan: Unsupervised learning of 3d representations from natural images, in ICCV, 2019. [41] A. Szab o, G. Meishvili, and P. Favaro, Unsupervised generative 3D shape learning from natural images, arXiv 1910.00287, 2019. [42] Y. Liao, K. Schwarz, L. M. Mescheder, and A. Geiger, Towards unsupervised learning of generative models for 3D controllable image synthesis, in CVPR, 2020. [43] B. Mildenhall, P. P. Srinivasan, M. Tancik, J. T. Barron, R. Ramamoorthi, and R. Ng, NeRF: Representing scenes as neural radiance fields for view synthesis, in ECCV, 2020. [44] K. Schwarz, Y. Liao, M. Niemeyer, and A. Geiger, GRAF: generative radiance fields for 3D-aware image synthesis, in NeurIPS, 2020. [45] E. R. Chan, M. Monteiro, P. Kellnhofer, J. Wu, and G. Wetzstein, Pi-GAN: Periodic implicit generative adversarial networks for 3D-aware image synthesis, in CVPR, 2021. [46] T. DeVries, M. A. Bautista, N. Srivastava, G. W. Taylor, and J. M. Susskind, Unconstrained scene generation with locally conditioned radiance fields, in ICCV, 2021. [47] X. Xu, X. Pan, D. Lin, and B. Dai, Generative occupancy fields for 3d surface-aware image synthesis, in NeurIPS, 2021. [48] M. Niemeyer and A. Geiger, GIRAFFE: representing scenes as [49] compositional generative neural feature fields, in CVPR, 2021. J. Gu, L. Liu, P. Wang, and C. Theobalt, StyleNeRF: stylebased 3D aware generator for high-resolution image synthesis, in ICLR, 2022. [50] Y. Xue, Y. Li, K. K. Singh, and Y. J. Lee, GIRAFFE HD: highresolution 3D-aware generative model, in CVPR, 2022. [51] R. Or-El, X. Luo, M. Shan, E. Shechtman, J. Park, and I. Kemelmacher-Shlizerman, StyleSDF: High-resolution 3Dconsistent image and geometry generation, in CVPR, 2022. [52] E. R. Chan, C. Z. Lin, M. A. Chan, K. Nagano, B. Pan, S. D. Mello, O. Gallo, L. J. Guibas, J. Tremblay, S. Khamis, T. Karras, and G. Wetzstein, Efficient geometry-aware 3d generative adversarial networks, in CVPR, 2022. J. [53] K. Schwarz, A. Sauer, M. Niemeyer, Y. Liao, and A. Geiger, VoxGRAF: Fast 3D-aware image synthesis with sparse voxel grids, in NeurIPS, 2022. [54] Y. Deng, J. Yang, J. Xiang, and X. Tong, GRAM: generative radiance manifolds for 3D-aware image generation, in CVPR, 2022. [55] X. Zhao, F. Ma, D. uera, Z. Ren, A. G. Schwing, and A. Colburn, Generative multiplane images: Making 2D GAN 3D-aware, in ECCV, 2022. [56] T. Karras, S. Laine, and T. Aila, style-based generator architecture for generative adversarial networks, in CVPR, 2019. [57] H. Yang, H. Zhu, Y. Wang, M. Huang, Q. Shen, R. Yang, and X. Cao, FaceScape: large-scale high quality 3D face dataset and detailed riggable 3d face prediction, in CVPR, 2020. [58] C. Ionescu, D. Papava, V. Olaru, and C. Sminchisescu, Human3.6M: Large scale datasets and predictive methods for 3D human sensing in natural environments, IEEE TPAMI, vol. 36, no. 7, pp. 13251339, 2014. [59] Z. Cai, D. Ren, A. Zeng, Z. Lin, T. Yu, W. Wang, X. Fan, Y. Gao, Y. Yu, L. Pan, F. Hong, M. Zhang, C. C. Loy, L. Yang, and Z. Liu, HuMMan: Multi-modal 4D human dataset for versatile sensing and modeling, in ECCV, 2022. [60] T. Wu, J. Zhang, X. Fu, Y. Wang, J. Ren, L. Pan, W. Wu, L. Yang, J. Wang, C. Qian, D. Lin, and Z. Liu, OmniObject3D: Largevocabulary 3D object dataset for realistic perception, reconstruction and generation, in CVPR, 2023. [61] M. Deitke, D. Schwenk, J. Salvador, L. Weihs, O. Michel, E. VanderBilt, L. Schmidt, K. Ehsani, A. Kembhavi, and A. Farhadi, Objaverse: universe of annotated 3D objects, in CVPR, 2023. [62] F. Zhan, Y. Yu, R. Wu, J. Zhang, S. Lu, L. Liu, A. Kortylewski, C. Theobalt, and E. P. Xing, Multimodal image synthesis and editing: The generative AI era, IEEE TPAMI, vol. 45, no. 12, pp. 15 09815 119, 2023. [63] A. Raistrick, L. Lipson, Z. Ma, L. Mei, M. Wang, Y. Zuo, K. Kayan, H. Wen, B. Han, Y. Wang, A. Newell, H. Law, A. Goyal, K. Yang, and J. Deng, Infinite photorealistic worlds using procedural generation, in CVPR, 2023. [64] M. Li, A. G. Patil, K. Xu, S. Chaudhuri, O. Khan, A. Shamir, C. Tu, B. Chen, D. Cohen-Or, and H. R. Zhang, GRAINS: generative recursive autoencoders for indoor scenes, ACM TOG, vol. 38, no. 2, pp. 12:112:16, 2019. [65] H. Fu, B. Cai, L. Gao, L. Zhang, J. Wang, C. Li, Q. Zeng, C. Sun, R. Jia, B. Zhao, and H. Zhang, 3D-FRONT: 3D furnished rooms with layouts and semantics, in ICCV, 2021. [66] A. Raistrick, L. Mei, K. Kayan, D. Yan, Y. Zuo, B. Han, H. Wen, M. Parakh, S. Alexandropoulos, L. Lipson, Z. Ma, and J. Deng, Infinigen Indoors: Photorealistic indoor scenes using procedural generation, in CVPR, 2024. [67] A. Dosovitskiy, G. Ros, F. Codevilla, A. M. opez, and V. Koltun, CARLA: an open urban driving simulator, in CoRL, 2017. S. 1909.11512, 2019. I. Nikolenko, Synthetic data for deep learning, arXiv [68] [69] Y. Li, X. Ran, L. Xu, T. Lu, M. Yu, Z. Wang, Y. Xiangli, D. Lin, and B. Dai, Proc-GS: Procedural building generation for city assembly with 3D Gaussians, arXiv 2412.07660, 2024. [70] Y. Jiang, H. S. Koppula, and A. Saxena, Modeling 3D environments through hidden human context, IEEE TPAMI, vol. 38, no. 10, pp. 20402053, 2016. [71] D. Paschalidou, A. Kar, M. Shugrina, K. Kreis, A. Geiger, and S. Fidler, ATISS: autoregressive transformers for indoor scene synthesis, in NeurIPS, 2021. [72] L. Gao, J. Sun, K. Mo, Y. Lai, L. J. Guibas, and J. Yang, SceneHGN: Hierarchical graph networks for 3D indoor scene generation with fine-grained geometry, IEEE TPAMI, vol. 45, no. 7, pp. 89028919, 2023. [74] [73] A. Dai, A. X. Chang, M. Savva, M. Halber, T. A. Funkhouser, and M. Nießner, ScanNet: Richly-annotated 3D reconstructions of indoor scenes, in CVPR, 2017. J. Straub, T. Whelan, L. Ma, Y. Chen, E. Wijmans, S. Green, J. J. Engel, R. Mur-Artal, C. Y. Ren, S. Verma, A. Clarkson, M. Yan, B. Budge, Y. Yan, X. Pan, J. Yon, Y. Zou, K. Leon, N. Carter, J. Briales, T. Gillingham, E. Mueggler, L. Pesqueira, M. Savva, D. Batra, H. M. Strasdat, R. De Nardi, M. Goesele, S. Lovegrove, and R. A. Newcombe, The Replica Dataset: digital replica of indoor spaces, arXiv 1906.05797, 2019. S. Song, F. Yu, A. Zeng, A. X. Chang, M. Savva, and T. A. Funkhouser, Semantic scene completion from single depth image, in CVPR, 2017. [75] [76] H. Fu, R. Jia, L. Gao, M. Gong, B. Zhao, S. J. Maybank, and D. Tao, 3D-FUTURE: 3D furniture shape with texture, IJCV, vol. 129, no. 12, pp. 33133337, 2021. [77] T. Dai, J. Wong, Y. Jiang, C. Wang, C. Gokmen, R. Zhang, J. Wu, and L. Fei-Fei, ACDC: Automated creation of digital cousins for robust policy learning, in CoRL, 2024. [78] A. Pumarola, E. Corona, G. Pons-Moll, and F. Moreno-Noguer, D-NeRF: Neural radiance fields for dynamic scenes, in CVPR, 2021. [79] Z. Yang, X. Gao, W. Zhou, S. Jiao, Y. Zhang, and X. Jin, Deformable 3D Gaussians for high-fidelity monocular dynamic scene reconstruction, in CVPR, 2024. S. Gu, W. Yin, B. Jin, X. Guo, J. Wang, H. Li, Q. Zhang, and X. Long, DOME: Taming diffusion model into high-fidelity controllable occupancy world model, arXiv 2410.10429, 2024. [80] [81] H. Bian, L. Kong, H. Xie, L. Pan, Y. Qiao, and Z. Liu, DynamicCity: Large-scale LiDAR generation from dynamic scenes, arXiv 2410.18084, 2024. [82] H. Chang, H. Zhang, L. Jiang, C. Liu, and W. T. Freeman, MaskGIT: Masked generative image transformer, in CVPR, 2022. [83] A. van den Oord, O. Vinyals, and K. Kavukcuoglu, Neural [84] [85] discrete representation learning, in NIPS, 2017. S. Meister, J. Hur, and S. Roth, UnFlow: Unsupervised learning of optical flow with bidirectional census loss, in AAAI, 2018. S. Bond-Taylor, P. Hessey, H. Sasaki, T. P. Breckon, and C. G. Willcocks, Unleashing transformers: Parallel token prediction 15 [87] with discrete absorbing diffusion for fast high-resolution image generation from vector-quantized codes, in ECCV, 2022. [86] L. Feng, Q. Li, Z. Peng, S. Tan, and B. Zhou, TrafficGen: Learning to generate diverse and realistic traffic scenarios, in ICRA, 2023. S. Ettinger, S. Cheng, B. Caine, C. Liu, H. Zhao, S. Pradhan, Y. Chai, B. Sapp, C. R. Qi, Y. Zhou, Z. Yang, A. Chouard, P. Sun, J. Ngiam, V. Vasudevan, A. McCauley, J. Shlens, and D. Anguelov, Large scale interactive motion forecasting for autonomous driving : The Waymo open motion dataset, in ICCV, 2021. J. F. Canny, computational approach to edge detection, IEEE TPAMI, vol. 8, no. 6, pp. 679698, 1986. [88] [90] [89] T. Y. Zhang and C. Y. Suen, fast parallel algorithm for thinning digital patterns, Communications of the ACM, vol. 27, no. 3, pp. 236239, 1984. J. Johnson, A. Alahi, and L. Fei-Fei, Perceptual losses for realtime style transfer and super-resolution, in ECCV, 2016. J. H. Lim and J. C. Ye, Geometric GAN, arXiv 1705.02894, 2017. [91] [92] A. Geiger, P. Lenz, and R. Urtasun, Are we ready for autonomous driving? the KITTI vision benchmark suite, in CVPR, 2012. [93] M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler, R. Benenson, U. Franke, S. Roth, and B. Schiele, The Cityscapes dataset for semantic urban scene understanding, in CVPR, 2016. I. Nigam, C. Huang, and D. Ramanan, Ensemble knowledge transfer for semantic segmentation, in WACV, 2018. [94] [95] H. Caesar, V. Bankiti, A. H. Lang, S. Vora, V. E. Liong, Q. Xu, A. Krishnan, Y. Pan, G. Baldan, and O. Beijbom, nuScenes: multimodal dataset for autonomous driving, in CVPR, 2020. S. R. Richter, V. Vineet, S. Roth, and V. Koltun, Playing for data: Ground truth from computer games, in ECCV, 2016. [96] [97] G. Ros, L. Sellart, J. Materzynska, D. Vazquez, and A. M. opez, The SYNTHIA dataset: large collection of synthetic images for semantic segmentation of urban scenes, in CVPR, 2016. [98] F. S. Saleh, M. S. Aliakbarian, M. Salzmann, L. Petersson, and J. M. Alvarez, Effective use of synthetic data for urban scene semantic segmentation, in ECCV, 2018. [99] Y. Li, L. Jiang, L. Xu, Y. Xiangli, Z. Wang, D. Lin, and B. Dai, MatrixCity: large-scale city dataset for city-scale neural rendering and beyond, in ICCV, 2023. [100] Y. Zhou, J. Huang, X. Dai, L. Luo, Z. Chen, and Y. Ma, HoliCity: city-scale data platform for learning holistic 3D structures, arXiv 2008.03286, 2020. [101] Y. Liao, J. Xie, and A. Geiger, KITTI-360: novel dataset and benchmarks for urban scene understanding in 2D and 3D, IEEE TPAMI, vol. 45, no. 3, pp. 32923310, 2023. [102] L. Lin, Y. Liu, Y. Hu, X. Yan, K. Xie, and H. Huang, Capturing, reconstructing, and simulating: The UrbanScene3D dataset, in ECCV, 2022. [103] K. Perlin, An image synthesizer, in SIGGRAPH, 1985. [104] Y. Shen, W. Ma, and S. Wang, SGAM: building virtual 3D world through simultaneous generation and mapping, in NeurIPS, 2022. [105] L. Chai, R. Tucker, Z. Li, P. Isola, and N. Snavely, Persistent Nature: generative model of unbounded 3D worlds, in CVPR, 2023. [106] W. Chu, L. Ke, and K. Fragkiadaki, Dreamscene4D: Dynamic multi-object scene generation from monocular videos, in NeurIPS, 2024. [107] W. Sun, S. Chen, F. Liu, Z. Chen, Y. Duan, J. Zhang, and Y. Wang, DimensionX: Create any 3D and 4D scenes from single image with controllable video diffusion, arXiv 2411.04928, 2024. [108] M. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler, and S. Hochreiter, GANs trained by two time-scale update rule converge to local nash equilibrium, in NIPS, 2017. [109] M. Binkowski, D. J. Sutherland, M. Arbel, and A. Gretton, Demystifying MMD GANs, in ICLR, 2018. [110] Z. Huang, Y. He, J. Yu, F. Zhang, C. Si, Y. Jiang, Y. Zhang, T. Wu, Q. Jin, N. Chanpaisit, Y. Wang, X. Chen, L. Wang, D. Lin, Y. Qiao, and Z. Liu, VBench: Comprehensive benchmark suite for video generative models, in CVPR, 2024. [111] R. Ranftl, K. Lasinger, D. Hafner, K. Schindler, and V. Koltun, Towards robust monocular depth estimation: Mixing datasets for zero-shot cross-dataset transfer, IEEE TPAMI, vol. 44, no. 3, pp. 16231637, 2022. [112] J. L. Sch onberger and J. Frahm, Structure-from-motion revisited, in CVPR, 2016. [113] G. Chen, G. Esch, P. Wonka, P. uller, and E. Zhang, Interactive procedural street modeling, ACM TOG, vol. 27, no. 3, p. 103, 2008. [114] C. H. Lin, H. Lee, Y. Cheng, S. Tulyakov, and M. Yang, InfinityGan: Towards infinite-pixel image synthesis, in ICLR, 2022. [115] L. Zhang, A. Rao, and M. Agrawala, Adding conditional control to text-to-image diffusion models, in ICCV, 2023. 16 Haozhe Xie received his Ph.D. from the Harbin Institute of Technology, in 2021. He is currently research fellow at MMLab@NTU, Nanyang Technological University, Singapore. Previously, he served as senior research scientist at Tencent AI Lab from 2021 to 2023. His research interests include computer vision with focus on 3D generation and reconstruction. He has published several papers in CVPR, ICCV, ECCV, ICLR, and IJCV, and serves as reviewer for these journals and conferences. Zhaoxi Chen received the bachelors degree from Tsinghua University, in 2021. He is currently Ph.D. student at MMLab@NTU, Nanyang Technological University, supervised by Prof. Ziwei Liu. He received the AISG PhD Fellowship in 2021. His research interests include inverse rendering and 3D generative models. He has published several papers in CVPR, ICCV, ECCV, ICLR, NeurIPS, TOG, and TPAMI. He also served as reviewer for CVPR, ICCV, NeurIPS, TOG, and IJCV. Fangzhou Hong received B.Eng. degree in software engineering from Tsinghua University, China, in 2020. He is currently working toward PhD degree at MMLab@NTU, Nanyang Technological University. His research interests include computer vision and deep learning. Particularly, he is interested in 3D representation learning. Ziwei Liu is currently Nanyang assistant professor at Nanyang Technological University, Singapore. His research revolves around computer vision, machine learning, and computer graphics. He has published extensively on top-tier conferences and journals in relevant fields, including CVPR, ICCV, ECCV, NeurlPS, ICLR, ICML, TPAMI, TOG, and Nature-Machine Intelligence. He is the recipient of the Microsoft Young Fellowship, Hong Kong PhD Fellowship, ICCV Young Researcher Award, HKSTP Best Paper Award and WAIC Yunfan Award. He serves as an Area Chair of CVPR, ICCV, NeurlPS, and ICLR, as well as an Associate Editor of IJCV."
        }
    ],
    "affiliations": [
        "S-Lab, Nanyang Technological University, Singapore 637335"
    ]
}