{
    "paper_title": "ToolRM: Outcome Reward Models for Tool-Calling Large Language Models",
    "authors": [
        "Mayank Agarwal",
        "Ibrahim Abdelaziz",
        "Kinjal Basu",
        "Merve Unuvar",
        "Luis A. Lastras",
        "Yara Rizk",
        "Pavan Kapanipathi"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "As large language models (LLMs) increasingly interact with external tools, reward modeling for tool use has become a critical yet underexplored area. Existing reward models, trained primarily on natural language outputs, struggle to evaluate tool-based reasoning and execution. To quantify this gap, we introduce FC-RewardBench, the first benchmark designed to systematically assess reward models' performance in tool-calling scenarios. Our analysis shows that current reward models often miss key signals of effective tool use, highlighting the need for domain-specific modeling. To address this, we propose a training framework for outcome-based reward models using data synthesized from permissively licensed, open-weight LLMs. We train models ranging from 1.7B to 14B parameters and evaluate them across seven out-of-domain benchmarks. These models consistently outperform general-purpose baselines, achieving up to 25\\% average improvement in downstream task performance and enabling data-efficient fine-tuning through reward-guided filtering."
        },
        {
            "title": "Start",
            "content": "ToolRM: Outcome Reward Models for Tool-Calling Large Language Models Mayank Agarwal, Ibrahim Abdelaziz, Kinjal Basu, Merve Unuvar, Luis A. Lastras, Yara Rizk, Pavan Kapanipathi IBM Research, USA {mayank.agarwal, ibrahim.abdelaziz1, kinjal.basu}@ibm.com 5 2 0 2 5 1 ] . [ 1 3 6 9 1 1 . 9 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "As large language models (LLMs) increasingly interact with external tools, reward modeling for tool use has become critical yet underexplored area. Existing reward models, trained primarily on natural language outputs, struggle to evaluate tool-based reasoning and execution. To quantify this gap, we introduce FCRewardBench, the first benchmark designed to systematically assess reward models performance in tool-calling scenarios. Our analysis shows that current reward models often miss key signals of effective tool use, highlighting the need for domain-specific modeling. To address this, we propose training framework for outcome-based reward models using data synthesized from permissively licensed, open-weight LLMs. We train models ranging from 1.7B to 14B parameters and evaluate them across seven out-of-domain benchmarks. These models consistently outperform generalpurpose baselines, achieving up to 25% average improvement in downstream task performance and enabling data-efficient fine-tuning through reward-guided filtering."
        },
        {
            "title": "Introduction",
            "content": "Large language models (LLMs) such as GPT-4 (Achiam et al., 2023), Claude, and Gemini (Reid et al., 2024) have rapidly advanced the field of artificial intelligence (AI), demonstrating impressive performance across wide range of natural language processing tasks. These models are capable of generating coherent text, answering complex questions, writing code, and even reasoning over multi-step problems. As LLMs become increasingly central to AI systems deployed in real-world settings, the demand for them to go beyond language generation and interact with external systems and tools has grown substantially. To meet this demand, tool calling has emerged as critical capability that allows LLMs to invoke external functions such as APIs, databases, calculators, and search engines (Prabhakar et al., 2025; Zhang et al., 2024; Abdelaziz et al., 2024; Liu et al., 2024b; Lin et al., 2024). This paradigm shifts the role of LLMs from standalone generators of text to orchestrators of complex workflows, making them more reliable and useful in practical applications such as autonomous agents, virtual assistants, and multi-modal systems. To effectively train and fine-tune LLMs, reward models are core component of reinforcement learning (RL) and preference optimization pipelines (Wang et al., 2023). They provide learned signal that estimates output quality, enabling scalable evaluation without requiring human judgment on every example. Broadly, reward models fall into two categories: process reward models (PRMs) (Lightman et al., 2023), which score intermediate reasoning steps, and outcome reward models (ORMs) (Cobbe et al., 2021), which only evaluate the final answer. Process models offer finer control over reasoning but demand costly, fine-grained annotations. In contrast, outcome models are easier to train and can achieve comparable performance gains (Uesato et al., 2022). Because outcome feedback is naturally aligned with how users assess responses (e.g., this answer is helpful), it is cheaper to collect and more scalable. While outcome models do not guarantee step-bystep reliability, they are often sufficient to guide LLMs toward high-quality outputs, particularly in tasks where intermediate reasoning is difficult to evaluate. Reward modeling for tool calling remains an underexplored area. Existing reward models are typically trained on natural language outputs and are not well-suited to capture the nuances of toolbased reasoning and execution. Currently, there is no dedicated benchmark to systematically evaluate the effectiveness of reward models in tool-calling scenarios. To address this gap, first, we introduce Figure 1: Performance of ToolRM, top reward models from RewardBench, and leading LLMs-as-judges on FC-RewardBench. Note: We abbreviate the model names for conciseness, for example, L3.1-xx corresponds to Llama-3.1-xx; SW-Rew-xx corresponds to SkyWorks-Reward-xx. Full model names can be found in the Appendix. FC-RewardBench1 comprehensive benchmark specifically designed to evaluate reward models on tool-calling tasks. Derived from the Berkeley Function Calling Leaderboard (BFCL) Version 3 (Patil et al., 2025), the dataset consists of 1500 unique user inputs along with both correct and incorrect function calls. Second, we apply several state-ofthe-art general-purpose reward models trained on human preferences for natural language generation to our benchmark (Figure 1). Our analysis reveals that these models often fail to capture key aspects of successful tool use, highlighting the need for domain-specific reward modeling in this area. To address this issue, we introduce ToolRM - collection of specialized ORMs for tool calling. Trained on preference data synthesized from diverse set of open-source function-calling models, ToolRM outperforms much larger reward models and LLMs-as-Judges on FC-RewardBench. In downstream applications, ToolRM demonstrates up to 25% average improvement across multiple benchmarks in Best-of-n setting. These models also enable efficient data filtering, yielding better fine-tuned models with less data. In summary, our contributions are: We introduce FC-RewardBench; the first benchmark to evaluate reward models on tool calling setting with strong correlation to downstream task performance. 1https://huggingface.co/datasets/ibm-research/ fc-reward-bench We propose framework for training tool calling ORM using data generated from permissively-licensed, medium-sized, openweight LLMs. We train multiple reward models (ToolRM) varying in size from 1.7B to 14B in parameters, and extensively evaluate our proposed models on seven out-of-domain benchmarks. We show that our reward models lead to up to 25% improvement on average across all evaluated benchmarks in Best-of-n setting. We also show that our reward models can be utilized for data filtering and lead to finetuned models that are better in performance with significantly less data."
        },
        {
            "title": "2.1 Tool Calling",
            "content": "Tool calling has extended LLMs beyond static knowledge to tasks requiring external retrieval (Schick et al., 2023), reasoning (He-Yueya et al., 2023), orchestration (Jain et al., 2024), and code execution (Gao et al., 2023). Early prompting-based approaches such as ReAct (Yao et al., 2023) inspired refinements for efficiency (Xu et al., 2023), performance (Shinn et al., 2023; Yang et al., 2023), or balanced trade-offs (Crouse et al., 2023). Recent models now provide built-in tool use (Reid et al., 2024; CodeGemma Team et al., 2024; CohereForAI, 2024; AI@Meta, 2024; Jiang et al., 2023) or are fine-tuned for this capability (Qin et al., 2023; Tang et al., 2023; Patil et al., 2023; Abdelaziz et al., 2024). To assess and enhance these capabilities, benchmarks (Guo et al., 2024; Patil et al., 2023), curated datasets (Liu et al., 2024b; Qian et al., 2025b), and autonomous tool construction methods (Qian et al., 2023b,a) have been proposed. 2.2 RL for Tool-Use Alignment Reinforcement Learning (RL) has recently emerged as powerful approach for aligning LLMs with effective tool use. Search-R1 (Jin et al., 2025) trains LLMs to iteratively generate and refine search queries during multi-step reasoning, showing that RL feedback helps balance exploration and precision in retrieval. ToRL (Li et al., 2025) enables base models to autonomously discover tool-use strategies, with reward signals driving emergent behaviors such as strategic invocation and adaptive switching between reasoning modes. ReTool (Feng et al., 2025) further interleaves code execution with natural language reasoning, using outcome feedback to guide when and how tools are invoked, yielding strong gains in mathematical problem solving. Several works explore reward design for tool use. ToolRL (Qian et al., 2025a) provides systematic study, highlighting how reward type, granularity, and temporal dynamics impact alignment. StepTool (Yu et al., 2024) applies step-level reward shaping and policy-gradient optimization, improving performance on complex multi-step tasks. CodeTool (Lu et al., 2025) combines RL with steplevel supervision, encouraging models to reason about intermediate states. SWE-RL (Wei et al., 2025) leverages large-scale software evolution data to optimize reasoning over evolving sequences of actions, demonstrating how RL can capture temporal dependencies in tool use. iTool (Zeng et al., 2025) addresses performance decay from synthetic data through iterative reinforced fine-tuning with Monte Carlo Tree Search, enhancing robustness in complex scenarios. Collectively, these works demonstrate the effectiveness of RL in aligning LLMs toward generalpurpose tool use. However, to the best of our knowledge, none of these methods explicitly employs an ORM that directly evaluates or optimizes the overall quality of an entire sequence of tool interactions."
        },
        {
            "title": "2.3 Reward Modeling",
            "content": "Reward models (RMs) provide scalar preference signals that guide LLMs through preference optimization or RL, and can be broadly divided into ORMs (outcome reward models), which only evaluate the final output, and PRMs (process reward models), which score intermediate reasoning steps. Early verifier-based approaches in the math domain (Cobbe et al., 2021) laid the foundation for ORMs, while later work explicitly contrasted outcomeand process-based supervision for math word problems (Uesato et al., 2022), and developed PRMs that reward coherent stepwise reasoning (Lightman et al., 2023). Despite their promise, PRMs often face robustness and supervision challenges, as highlighted by failed attempts reported in the DeepSeek-R1 work (Guo et al., 2025). In contrast, ORMs have proven more scalable, focusing on final correctness and generalization, with recent advances such as Skywork-Reward (Liu et al., 2024a) demonstrating effective recipes for outcome-based training that achieved state-of-the-art performance on RewardBench (Lambert et al., 2024). While prior work has studied RMs primarily in free-text reasoning and math/code domains, to the best of our knowledge, this is the first work to introduce ORMs for tool calling, where outcomes are defined over tool call sequences."
        },
        {
            "title": "3.1 FC-RewardBench Evaluation Dataset",
            "content": "While several benchmarks have been proposed to evaluate RMs on tasks involving chat, reasoning, safety (Lambert et al., 2024); factuality, instruction following, math (Malik et al., 2025); sensitivity to style and subtle content differences (Liu et al.); and multi-modal judges for image-generation models (Chen et al.), there remains notable gap in the evaluation of RMs for function-calling tasks. To bridge this gap, we propose FC-RewardBench, benchmark dataset specifically designed to evaluate RMs on function-calling tasks. This dataset comprises 1500 unique data points, each containing tool catalog, user query, and the associated correct and incorrect tool calls for given user query. To construct FC-RewardBench, we utilize the single-turn splits of the BFCL-v3 dataset (Yan et al., 2024). The tool catalog, user query, and the correct tool calls in the dataset are directly sourced from BFCL-v3. Incorrect tool calls are generated using pool of 25 permissively licensed models, spanning sizes from 0.5B to 685B parameters. Each model is prompted to generate tool call in response to Error Type Count Incorrect Parameter Value Incorrect Function Name Incorrect number of functions Missing Optional Parameter Missing Required Parameter Incorrect Parameter Type Unexpected Parameter Incorrect output format 650 403 245 78 45 43 21 15 Table 1: Breakdown of errors in the FC-RewardBench dataset. The majority of errors in the dataset are subtle and hard to identify. Figure 2: Representative example from the FCparameter RewardBench player_count is set to an incorrect value. The tool catalog is hidden for brevity. dataset, where the the user query. The outputs are compared against the ground-truth tool calls, and only the incorrect generations are retained. From this pool, we randomly sample one incorrect call per instance to prevent over-representation from any single user query. Finally, 1,500 such examples are randomly selected to form the final dataset. Figure 2 shows representative example from the dataset, where the incorrect tool call sets the player_count parameter to an incorrect value of 5 instead of the correct value 6. Table 1 presents breakdown of error types observed in the dataset. Notably, majority of the incorrect calls involve subtle errors such as incorrect parameter values, missing optional parameters, or an incorrect number of functions, which are nontrivial to detect. These characteristics require the RM to demonstrate deeper understanding of the function-calling task, making FC-RewardBench challenging and discriminative benchmark. Additional details about the benchmark are provided in Appendix A.1. logs, and corresponding ground-truth tool call sequences, we prompt each model to complete the task. Specifically, each model is instructed to generate tool call in response to user query using the tools provided in the dataset. This process simulates real-world inference scenario where the model must decide which tool to invoke and how to structure the call. Once the model-generated tool call is produced, we compare it against the ground-truth tool call specified in the dataset. If the model output deviates from the ground truth, it is retained as an example of an incorrect tool call. Conversely, outputs that exactly match the ground truth are discarded, as they do not contribute to the error-focused training objective. This data generation approach captures the nuance and variability of real-world model behavior, reflecting the kinds of errors that actual models tend to make in practice, including subtle and complex failure modes that may be difficult to anticipate or enumerate manually."
        },
        {
            "title": "3.3 Reward Modeling",
            "content": "To train RMs for function-calling tasks, we need data that contains the following fields: (a) user query, (b) tool catalog list of tools available to the model to answer the user query, (c) the corresponding correct tool call, and (d) one or more incorrect tool calls. To generate this training data, we leverage diverse pool of open-source, permissively licensed, medium-sized models with function-calling capabilities. publicly function-calling datasets, which include user queries, tool cataavailable"
        },
        {
            "title": "Using",
            "content": "A RM aims to capture human preferences between two model outputs. This is typically modeled by the Bradley-Terry model (Bradley and Terry, 1952), which defines the probability that output y+ is preferred over y, given an input as: p(y+ x) = exp(r(x, y+)) exp(r(x, y+)) + exp(r(x, y)) (1) where r(x, y) is scalar reward function assigning relative score to output given input x. Training requires curating dataset of pairwise preferences = {(x, y+, y) : y+ y}, with preferences being obtained through either human annotations (Stiennon et al., 2020; Ouyang et al., 2022) or synthetic generation methods (Pace et al., 2024; Hosseini et al., 2024). The reward function is parameterized by neural network rθ, typically initialized from supervised fine-tuned model with the final layer replaced by linear head. The parameters of rθ are estimated from the dataset using maximum likelihood estimation of the following objective: J(r) = max rθ E(x,y+,y)D[ log(σ(rθ(x, y+) rθ(x, y))] (2) where σ denotes the sigmoid function. In this work, we use reward centering (Eisenstein et al., 2023) to ensure that rewards are zerocentered. This is achieved by adding the following regularization term to the optimization objective: Jreg(r) = J(r) + ηE(x,y+,y)D[ (rθ(x, y+) + rθ(x, y))2] (3) where η is small positive value hyperparameter."
        },
        {
            "title": "4 Experimental Setup",
            "content": "Training Data: To create training data for the RM, we select open-source datasets that cover various aspects of function-calling, such as the APIGen dataset (Liu et al., 2024c) for single-turn interactions, the Schema-Guided Dialogue (SGD) dataset (Rastogi et al., 2020) for multi-turn interactions with tool invocations and responses, and the xlam-irrelevance2 dataset for cases where the system lacks sufficient information to respond to user query. Since these datasets are common training datasets and our primary focus is to elicit representative incorrect behavior from the model, we follow Lin et al. (2024) and obfuscate the data samples to avoid the model regurgitating its training data. We obfuscate the samples by replacing function and parameter names with randomly generated strings and reordering the keys in the function schema. Note that this transformation preserves the semantics of the inputoutput pairs. We then use collection of 11 permissivelylicensed, medium-sized, open-weight models to generate the training data. The pool includes both general-purpose instruction-tuned models with function-calling capabilities and function-calling specific models, with parameter counts ranging from 0.5B to 32B. Specifically, we use the IBM Granite 3.3-Instruct (Granite Team) and Qwen2.5Instruct series (Yang et al., 2025) models along with Granite-20b-function-calling (Abdelaziz et al., 2024), SmolLM2 (Allal et al., 2025), Mistral-7bInstruct-v0.33 and Mistral-Nemo-Instruct-24074. After generating outputs from the model pool and keeping only the incorrect ones, we subsample one incorrect output per input user query to prevent over-representation from user query in the training data. Overall, this results in 180,000 training data samples divided into 85,000 single and multi-turn data each, and 10,000 irrelevance data. The full list of models used to generate the training data, along with few training data samples, is provided in Appendix A.2. Model architecture: We use the Qwen-2.5Instruct models (Team, 2024; Yang et al., 2024) as the base architecture for our RMs. Specifically, we select the 1.5B, 7B, and 14B parameter variants, as they are Apache-2.0 licensed and offer practical balance between size and performance. We initialize the RMs with the instruction-tuned model weights and replace the final language modeling head with linear layer that maps the hidden representation to scalar reward value. The RMs accept the specifications of available functions, conversation history, and the generated tool call as input and produce scalar reward as output (refer to Appendix A.3 for prompt template). We train all RMs for 1 epoch with learning rate set to 1e-6 and cosine learning rate schedule with warmup set to 3% of total steps. We also use reward centering (Eisenstein et al., 2023) with coefficient set to 0.01. 3https://huggingface.co/mistralai/ Mistral-7B-Instruct-v0.3 2https://huggingface.co/datasets/MadeAgents/ 4https://huggingface.co/mistralai/ xlam-irrelevance-7.5k Mistral-Nemo-Instruct-2407 Benchmarks: In addition to evaluating models performance on FC-RewardBench, we also evaluate on variety of function calling benchmarks: Function-Calling Berkeley Leaderboard (BFCL) (Patil et al., 2025) is comprehensive function calling leaderboard that includes dataset of over 4,400 instances. The leaderboard evaluates model performance across single and multi-turn settings and include tasks that vary in difficulty and programming languages. API-Bank (Li et al., 2023) has 314 tool-use dialogues with 753 API calls to assess LLMs capabilities in planning, retrieving, and calling APIs. ToolAlpaca (Tang et al., 2023) test set has 271 tool-use instances spanning 50 distinct categories. Similar to Nexusflow.ai (2023), we used the simulated part of ToolAlpaca which has total of 100 test examples. NexusRaven API Evaluation5 is diverse benchmark for function calling with 318 test examples across 65 APIs, built by mining function definitions, docstrings, and execution contexts from open-source code, and Sealtools (Wu et al., 2024) is large-scale selfinstructed tool-learning dataset generated by ChatGPT, featuring diverse API-like tools with challenging nested, cross-field, and multi-tool calls. Baselines: To evaluate performance on FCRewardBench, we select eight RMs from RewardBench, spanning sizes from 3B to 70B parameters. We chose these models because they score highly on RewardBench and support tools in their chat template, which helps mitigate performance degradation due to prompt variability. In addition to these specialized RMs, we include six LLMs acting as judges, spanning sizes from 70B to 685B parameters. To evaluate tool calling performance, we selected wide range of models from BFCL V3 leaderboard6 focusing on 1) the best performing models like xLAM-27 (Llama-xLAM-2-70b-fc-r, xLAM-2-32b-fc-r, and Llama-xLAM-2-8b-fc-r), and Qwen3-32B8 and 2) small-sized models that 5https://huggingface.co/datasets/Nexusflow/ NexusRaven_API_evaluation 6https://gorilla.cs.berkeley.edu/leaderboard. html 7https://huggingface.co/collections/ Salesforce/xlam-2-67ef5be12949d8dcdae354c4 8https://huggingface.co/Qwen/Qwen3-32B perform well on the leaderboard to measure the extent of which our RM, ToolRM, can help, e.g. xLAM-2-1b-fc-r, xLAM-2-3b-fc-r, Qwen3-0.6B, Qwen3-1.7B, and Qwen3-4B."
        },
        {
            "title": "5 Results",
            "content": "We evaluate our proposed RM to answer the following three research questions (RQ): RQ1: How does ToolRM compare to existing RMs on FC-RewardBench? RQ2: Can ToolRM improve the performance during inference through Best-of-n sampling?, and RQ3: When ToolRM is used for filtering training data, can it lead to improved finetuned models? 5.1 RQ1: FC-RewardBench evaluation We evaluate ToolRM against state-of-the-art RMs from RewardBench (Lambert et al., 2024), as well as leading LLMs used in an LLM-as-a-Judge setting, on the FC-RewardBench dataset. RMs are evaluated by comparing scores assigned to the correct tool call outputs and incorrect tool call outputs for the same input. prediction is counted as correct when the score for the correct tool call exceeds that of the incorrect one. LLMsas-Judges are evaluated with pairwise comparison prompt, where both candidate tool calls are presented and the model is instructed to select the correct one. To avoid position bias, the order of candidates is randomized. Experimental details, including the full prompt template, are provided in Appendix A.4. We show the results in Figure 1 and observe the following: Specialized RMs under-perform on toolcalling tasks. Despite strong results on RewardBench domains, most specialized RMs fail to generalize effectively to the tool-calling domain. While some individual variants achieve higher scores, performance remains inconsistent and generally below state-of-the-art performance. LLMs-as-Judges achieve higher accuracy but are computationally expensive. LLMsas-Judges attain strong performance on FCRewardBench (exceeding 80% across all models), but their large parameter counts impose substantial computational costs. ToolRM achieves the highest accuracy on the benchmark while maintaining efficiency with respect to model size. The ToolRM-14B and ToolRM-7B variants outperform all other generative and sequential classifier models. Notably, even the ToolRM-1.5B variant surpasses the gptoss-120B model, approaching the performance of substantially larger Llama-4 models. Figure 3: Correlation heatmap between performance on FC-RewardBench and downstream accuracy across generator models and benchmarks, showing consistently strong alignment (avg. correlation = 0.84). Correlation with performance on downstream tasks: The primary purpose of FC-RewardBench is to enable quick evaluation of RMs without having to do computationally expensive downstream evaluation. It is thus imperative that performance on FC-RewardBench reflects downstream task performance. To assess this, we select six generator models (Qwen3-1.7B, 8B, 32B, and xLAM-1B, 8b, 70B), 11 RMs (eight RMs from RewardBench and three ToolRM variants), and five benchmarks. For each generator model, RM, and dataset combination, we compute the performance in Best-of-n (n = 32) setting and compute the Pearson correlation coefficient between the Best-of-n performance and RM performance on FC-RewardBench. Results are shown in Figure 3. Overall, we find that FC-RewardBench scores are strongly correlated with downstream task accuracy, with an average correlation of 0.84 across benchmarks and generator models. Across generator models, the average correlation ranges from 0.62 to 0.94, indicating that the alignment between FC-RewardBench and downstream performance is Importantly, this robust across model families. correlation remains stable even at scale: larger models such as Qwen3-32B and xLAM-2-70B continue to exhibit strong agreement between FCRewardBench accuracy and downstream results. Taken together, these findings confirm that FCRewardBench provides reliable and computationally efficient proxy for expensive downstream evaluations. 5.2 RQ2: Best-of-n sampling with ToolRM In this section, we evaluate ToolRM in Best-ofn setting across multiple generator models. For each input, we sample = 32 independent generations from the generator model and use ToolRM to score and select the highest-ranked generation as the final output. Intuitively, stronger RM should more reliably identify the correct tool call, thereby improving task performance. We compare against two baselines: Greedy Decoding and Majority Voting (n = 32), where the most frequently occurring final answer is selected as the final output. For non-BFCL benchmarks, we report the Full Sequence Matching metric (Basu et al., 2025), which checks whether the predicted tool sequenceincluding tool names and argumentvalue pairsexactly matches the gold sequence. For BFCL, we use its native evaluation metrics: AST-based scores for single-turn tasks and statebased/response-based metrics for multi-turn cases. Figure 4 reports average performance across five benchmarks (API-Bank-Level-1, API-Bank-Level2, ToolAlpaca, NexusRaven, and SealTools), while Table 2 presents results on the BFCL-v3 dataset. We summarize the key insights below: Small Language Models (SLMs) benefit the most: Best-of-n sampling with Qwen3-0.6B and ToolRM-14B as the ranker improves accuracy from 39.5% to 64.38% gain of 24.9 points on non-BFCL benchmarks (Figure 4). This performance surpasses that of Qwen3-32B (63.8%) and Llama-xLAM-2-70b-fc-r (63.6%) with greedy decoding. On BFCL-v3, xLAM-2-1B-fc-r with ToolRM-14B improves overall, Non-Live AST, and Live AST accuracies by 3.2, 6.5, and 6.2 points, respectively. Qwen3-1.7B achieves even larger improvements of 5.3, 9.6, and 8.7 points on these metrics  (Table 2)  . SLM + RM can match or surpass larger models: Best-of-n sampling with Qwen3-8B and ToolRM-14B improves non-BFCL benchmark accuracy by 6.8% points to 70.48%, which is 5.6 points higher than the best greedy baseline. On BFCL-v3, the same setup yields gains of 3.3 Figure 4: Performance of the Qwen3 series (top) and xLAM-2 series (bottom) in the Best-of-n (n = 32) setting across five function-calling benchmarks: API-Bank-1, API-Bank-2, NexusRaven, ToolAlpaca, and SealTools. Model RM Overall Acc Non-Live AST Live AST Multi-Turn Acc Qwen3-1.7b Qwen3-4b Qwen3-8b Qwen3-14b Qwen3-32b xLAM-2-1b-fc-r xLAM-2-3b-fc-r Llama-xLAM-2-8b-fc-r xLAM-2-32b-fc-r Greedy ToolRM-7B ToolRM-14B Greedy ToolRM-7B ToolRM-14B Greedy ToolRM-7B ToolRM-14B Greedy ToolRM-7B ToolRM-14B Greedy ToolRM-7B ToolRM-14B Greedy ToolRM-7B ToolRM-14B Greedy ToolRM-7B ToolRM-14B Greedy ToolRM-7B ToolRM-14B Greedy ToolRM-7B ToolRM-14B 55.74 60.07 61.05 62.45 64.87 64.99 64.65 66.31 67.14 68.62 68.29 69. 69.19 68.33 70.61 54.09 56.74 57.28 64.74 66.44 66.66 71.14 72.12 72.52 76.34 76.43 76.54 80.23 89.69 89. 88.69 91.12 91.46 88.90 92.06 92.19 89.25 91.88 92.08 89.33 90.75 92.31 68.98 75.23 75.50 83.06 85.12 84. 84.31 87.08 87.73 89.40 89.81 90.27 71.35 81.13 80.01 80.31 84.31 83.64 80.09 83.35 82.98 82.16 84.16 83. 82.83 84.90 84.23 54.77 60.55 60.92 62.99 65.14 65.51 67.80 73.28 72.46 75.35 77.87 77.42 10.25 12.12 14. 17.75 23.25 22.75 26.38 30.50 31.50 33.75 32.50 34.25 38.25 35.25 39.62 35.12 33.38 34.25 52.75 51.75 51. 67 61.25 61.62 65.25 63.50 63.25 Table 2: Performance of the Qwen3 and xLAM-2 series of models in the Best-of-n (n = 32) setting on BFCL-v3. BFCL V3 ToolAlpaca Nexus API-Bank-1 API-Bank-2 Sealtools AVG Llama-3.1-8B-Instruct Llama-3.1-8B-Instruct (tuned - 16K) Llama-3.1-8B-Instruct (tuned - random 8K) Llama-3.1-8B-Instruct (tuned - best scored 8K) 49.6 54.1 55.2 55. 38.0 43.0 44.0 44.0 64.8 75.5 74.2 72.0 67.9 57.4 49.9 63.7 66.2 63.5 54.1 66.2 37.6 72.7 73.2 73.7 54.0 61.0 58.4 62. Table 3: Finetuning results of Llama-3.1-8B-Instruct on three training subsets: full 16K dataset (tuned 16K), 8K randomly sampled subset (tuned random 8K), and top 8K examples selected via ToolRM (tuned best scored 8K). points on Non-Live AST and 2.9 points on Live AST, exceeding the performance of Qwen3-32B with greedy decoding. RM scale correlates with gain: We present Bestof-n results with ToolRM-1.5B, 7B, and 14B in Figure 4 and results with ToolRM-7B and 14B in Table 2. In nearly all cases, Best-of-n performance correlates with ToolRM size; larger RMs lead to better performance improvements. We do note, however, that the smallest ToolRM-1.5B model provides substantial gains and can be useful in compute-constrained scenarios. Gains are consistent across benchmarks and model families: Although SLMs exhibit the largest improvements, we observe consistent gains across (a) all considered benchmarks, (b) generator model scales ranging from 0.6B to 70B, and (c) both general instruction-tuned and function-callingspecialized models. Diminishing returns for very large models: Improvements for large-scale generators (32B+) are modest. For instance, Llama-xLAM-2-32B-fc-r improves by only 2.1 points on non-BFCL benchmarks and 2.5 points on BFCL Live AST accuracy, suggesting limited additional utility of Bestof-n sampling with very strong base models. In Appendix A.5, we provide breakdown of errors with greedy decoding and with Best-of-n sampling with ToolRM-14B."
        },
        {
            "title": "5.3 RQ3: ToolRM For Data Filtering",
            "content": "In this experiment, we assess the effectiveness of using ToolRM as data filter to construct highquality training dataset for tool-use models. We curate training corpus comprising both single-turn and multi-turn examples drawn from APIGen-MT (Liu et al., 2024c), SealTools (Wu et al., 2024), Glaive V29, and Granite function-calling dataset (Abdelaziz et al., 2024), yielding total of 16K samples. We highlight that the data sources se9https://huggingface.co/datasets/glaiveai/ glaive-function-calling-v2 lected for this experiment have no overlap with ToolRM training data, thus allowing us to test the generalization capabilities of ToolRM. We select Llama-3.1-8B-Instruct (Grattafiori et al., 2024) as the base model and performed LoRA-based finetuning (Hu et al., 2022) to train each variant for 1 epoch with learning rate of 2e-4, LoRA rank of 16, alpha of 32, cosine scheduler, and warmup ratio of 10%. Table 3 compares the performance of the base model with three fine-tuned variants: (1) trained on the full 16K dataset, (2) trained on random 8K subset, and (3) trained on the top 8K samples as ranked by ToolRM-14B. We highlight the following key insights from the results. Fine-tuning consistently improves performance. All fine-tuned models outperform the base model, raising average accuracy from 54.0% to 61.0% when trained on the full dataset. Naive subsampling degrades performance. Training on random 8K subset reduces accuracy to 58.4% 2.6 points below the model finetuned on the entire corpus. This performance degradation reflects the inapplicability of naively subsampling to reduce dataset size since it results in both the inclusion of low-quality samples and the exclusion of high-quality ones. ToolRM based data filtering achieves the best performance. Selecting the top 50% of samples with ToolRM-14B yields the strongest results at 62.5%, surpassing the full-data model while using only half the training corpus. This demonstrates that ToolRM can effectively identify highquality data, enabling superior performance under tighter data budgets. Overall, these results highlight the importance of data quality in fine-tuning tool-use models and show that filtering low-quality samples through reward model can yield superior performance while reducing dataset size."
        },
        {
            "title": "6 Conclusion",
            "content": "In this paper, we presented comprehensive framework for reward modeling in tool-calling scenarios, addressing critical gap in current LLM evaluation pipelines. Our benchmark, FC-RewardBench, enables systematic assessment of reward models on tool-based reasoning. We also presented framework for training outcome RMs that outperform existing significantly larger RMs in the tool calling setting. When used for inference-time scaling, our RMs also improve the performance of generalpurpose and tool calling models by up to 25% on various tool calling benchmarks. Looking ahead, we see several promising directions for advancing reward modeling in this domain. First, moving beyond classification-based RMs to generative verifiers with chain-of-thought reasoning could improve robustness and interpretability. Second, incorporating tool and environment state into training could help models safely recover from execution failures. Finally, bridging outcome and process reward modeling may offer unified framework that balances scalability with fine-grained control over reasoning quality."
        },
        {
            "title": "References",
            "content": "Ibrahim Abdelaziz, Kinjal Basu, Mayank Agarwal, Sadhana Kumaravel, Matthew Stallone, Rameswar Panda, Yara Rizk, GP Bhargav, Maxwell Crouse, Chulaka Gunasekara, et al. 2024. Granite-function calling model: Introducing function calling abilities via multi-task learning of granular tasks. arXiv preprint arXiv:2407.00121. Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774. AI@Meta. 2024. Llama 3 model card. Loubna Ben Allal, Anton Lozhkov, Elie Bakouch, Gabriel Martín Blázquez, Guilherme Penedo, Lewis Tunstall, Andrés Marafioti, Hynek Kydlíˇcek, Agustín Piqueres Lajarín, Vaibhav Srivastav, et al. 2025. Smollm2: When smol goes bigdata-centric training of small language model. arXiv preprint arXiv:2502.02737. Kinjal Basu, Ibrahim Abdelaziz, Kiran Kate, Mayank Agarwal, Maxwell Crouse, Yara Rizk, Kelsey Bradford, Asim Munawar, Sadhana Kumaravel, Saurabh Goyal, Xin Wang, Luis A. Lastras, and Pavan Kapanipathi. 2025. Nestful: benchmark for evaluating llms on nested sequences of api calls. Preprint, arXiv:2409.03797. Ralph Allan Bradley and Milton Terry. 1952. Rank analysis of incomplete block designs: I. the method of paired comparisons. Biometrika, 39(3/4):324 345. Zhaorun Chen, Yichao Du, Zichen Wen, Yiyang Zhou, Chenhang Cui, Zhenzhen Weng, Haoqin Tu, Chaoqi Wang, Zhengwei Tong, Leria HUANG, et al. Mjbench: Is your multimodal reward model really good judge? In ICML 2024 Workshop on Foundation Models in the Wild. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. 2021. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168. CodeGemma Team, Ale Jakse Hartman, Andrea Hu, Christopher A. Choquette-Choo, Heri Zhao, Jane Fine, and Hui. 2024. Codegemma: Open code models based on gemma. CohereForAI. 2024. C4ai command-r: 35 billion parameter generative model for reasoning, summarization, and question answering. Hugging Face Models. Maxwell Crouse, Ibrahim Abdelaziz, Ramon Astudillo, Kinjal Basu, Soham Dan, Sadhana Kumaravel, Achille Fokoue, Pavan Kapanipathi, Salim Roukos, and Luis Lastras. 2023. Formally specifying the highlevel behavior of llm-based agents. arXiv preprint arXiv:2310.08535. Jacob Eisenstein, Chirag Nagpal, Alekh Agarwal, Ahmad Beirami, Alex DAmour, DJ Dvijotham, Adam Fisch, Katherine Heller, Stephen Pfohl, Deepak Ramachandran, et al. 2023. Helping or herding? reward model ensembles mitigate but do not eliminate reward hacking. arXiv preprint arXiv:2312.09244. Jiazhan Feng, Shijue Huang, Xingwei Qu, Ge Zhang, Yujia Qin, Baoquan Zhong, Chengquan Jiang, Jinxin Chi, and Wanjun Zhong. 2025. Retool: Reinforcement learning for strategic tool use in llms. arXiv preprint arXiv:2504.11536. Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Graham Neubig. 2023. Pal: Program-aided language In International Conference on Machine models. Learning, pages 1076410799. PMLR. IBM Granite Team. Granite 3.0 language models. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad AlDahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. 2024. The llama 3 herd of models. arXiv preprint arXiv:2407.21783. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. 2025. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948. Zhicheng Guo, Sijie Cheng, Hao Wang, Shihao Liang, Yujia Qin, Peng Li, Zhiyuan Liu, Maosong Sun, and Yang Liu. 2024. Stabletoolbench: Towards stable large-scale benchmarking on tool learning of large language models. arXiv preprint arXiv:2403.07714. Chris Yuhao Liu, Liang Zeng, Jiacai Liu, Rui Yan, Jujie He, Chaojie Wang, Shuicheng Yan, Yang Liu, and Yahui Zhou. 2024a. Skywork-reward: Bag of tricks for reward modeling in llms. arXiv preprint arXiv:2410.18451. Joy He-Yueya, Gabriel Poesia, Rose Wang, and Noah Goodman. 2023. Solving math word problems by combining language models with symbolic solvers. arXiv preprint arXiv:2304.09102. Arian Hosseini, Xingdi Yuan, Nikolay Malkin, Aaron C. Courville, Alessandro Sordoni, and Rishabh Agarwal. 2024. V-star: Training verifiers for self-taught reasoners. ArXiv, abs/2402.06457. Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. 2022. Lora: Low-rank adaptation of large language models. ICLR, 1(2):3. Arushi Jain, Shubham Paliwal, Monika Sharma, Lovekesh Vig, and Gautam Shroff. 2024. Smartflow: Robotic process automation using llms. arXiv preprint arXiv:2405.12842. Albert Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. 2023. Mistral 7b. arXiv preprint arXiv:2310.06825. Bowen Jin, Hansi Zeng, Zhenrui Yue, Dong Wang, Hamed Zamani, and Jiawei Han. 2025. Searchr1: Training llms to reason and leverage search engines with reinforcement learning. arXiv preprint arXiv:2503.09516. Nathan Lambert, Valentina Pyatkin, Jacob Morrison, LJ Miranda, Bill Yuchen Lin, Khyathi Raghavi Chandu, Nouha Dziri, Sachin Kumar, Tom Zick, Yejin Choi, et al. 2024. Rewardbench: Evaluating reward models for language modeling. CoRR. Minghao Li, Yingxiu Zhao, Bowen Yu, Feifan Song, Hangyu Li, Haiyang Yu, Zhoujun Li, Fei Huang, and Yongbin Li. 2023. Api-bank: comprehensive benchmark for tool-augmented llms. Preprint, arXiv:2304.08244. Xuefeng Li, Haoyang Zou, and Pengfei Liu. 2025. arXiv preprint Torl: Scaling tool-integrated rl. arXiv:2503.23383. Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. 2023. Lets verify step by step. In The Twelfth International Conference on Learning Representations. Qiqiang Lin, Muning Wen, Qiuying Peng, Guanyu Nie, Junwei Liao, Jun Wang, Xiaoyun Mo, Jiamu Zhou, Cheng Cheng, Yin Zhao, et al. 2024. Hammer: Robust function-calling for on-device language models via function masking. arXiv preprint arXiv:2410.04587. Weiwen Liu, Xu Huang, Xingshan Zeng, Xinlong Hao, Shuai Yu, Dexun Li, Shuai Wang, Weinan Gan, Zhengying Liu, Yuanqing Yu, et al. 2024b. Toolace: Winning the points of llm function calling. arXiv preprint arXiv:2409.00920. Yantao Liu, Zijun Yao, Rui Min, Yixin Cao, Lei Hou, and Juanzi Li. Rm-bench: Benchmarking reward models of language models with subtlety and style. In The Thirteenth International Conference on Learning Representations. Zuxin Liu, Thai Hoang, Jianguo Zhang, Ming Zhu, Tian Lan, Juntao Tan, Weiran Yao, Zhiwei Liu, Yihao Feng, Rithesh RN, et al. 2024c. Apigen: Automated pipeline for generating verifiable and diverse function-calling datasets. Advances in Neural Information Processing Systems, 37:5446354482. Yifei Lu, Fanghua Ye, Jian Li, Qiang Gao, Cheng Liu, Haibo Luo, Nan Du, Xiaolong Li, and Feiliang Ren. 2025. Codetool: Enhancing programmatic tool invocation of llms via process supervision. arXiv preprint arXiv:2503.20840. Saumya Malik, Valentina Pyatkin, Sander Land, Jacob Morrison, Noah Smith, Hannaneh Hajishirzi, and Nathan Lambert. 2025. Rewardbench 2: Advancing reward model evaluation. arXiv preprint arXiv:2506.01937. Nexusflow.ai. 2023. Nexusraven-v2: Surpassing gpt-4 for zero-shot function calling. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:2773027744. Alizée Pace, Jonathan Mallinson, Eric Malmi, Sebastian Krause, and Aliaksei Severyn. 2024. West-of-n: Synthetic preferences for self-improving reward models. arXiv e-prints, pages arXiv2401. Shishir Patil, Huanzhi Mao, Fanjia Yan, Charlie Cheng-Jie Ji, Vishnu Suresh, Ion Stoica, and Joseph E. Gonzalez. 2025. The berkeley function calling leaderboard (BFCL): From tool use to agentic evaluation of large language models. In Forty-second International Conference on Machine Learning. Shishir Patil, Tianjun Zhang, Xin Wang, and Joseph Gonzalez. 2023. Gorilla: Large language model connected with massive apis. arXiv preprint arXiv:2305.15334. Akshara Prabhakar, Zuxin Liu, Ming Zhu, Jianguo Zhang, Tulika Awalgaonkar, Shiyu Wang, Zhiwei Liu, Haolin Chen, Thai Hoang, Juan Carlos Niebles, et al. 2025. Apigen-mt: Agentic pipeline for multiturn data generation via simulated agent-human interplay. arXiv preprint arXiv:2504.03601. Cheng Qian, Emre Can Acikgoz, Qi He, Hongru Wang, Xiusi Chen, Dilek Hakkani-Tür, Gokhan Tur, and Heng Ji. 2025a. Toolrl: Reward is all tool learning needs. Preprint, arXiv:2504.13958. Cheng Qian, Emre Can Acikgoz, Hongru Wang, Xiusi Chen, Avirup Sil, Dilek Hakkani-Tür, Gokhan Smart: Self-aware Tur, and Heng Ji. 2025b. agent for tool overuse mitigation. arXiv preprint arXiv:2502.11435. Cheng Qian, Chi Han, Yi Fung, Yujia Qin, Zhiyuan Liu, and Heng Ji. 2023a. Creator: Tool creation for disentangling abstract and concrete reasoning of large language models. arXiv preprint arXiv:2305.14318. Cheng Qian, Chenyan Xiong, Zhenghao Liu, and Zhiyuan Liu. 2023b. Toolink: Linking toolkit creation and using through chain-of-solving on opensource model. arXiv preprint arXiv:2310.05155. Qiaoyu Tang, Ziliang Deng, Hongyu Lin, Xianpei Han, Qiao Liang, and Le Sun. 2023. Toolalpaca: Generalized tool learning for language models with 3000 simulated cases. arXiv preprint arXiv:2306.05301. Qwen Team. 2024. Qwen2.5: party of foundation models. Jonathan Uesato, Nate Kushman, Ramana Kumar, Francis Song, Noah Siegel, Lisa Wang, Antonia Creswell, Geoffrey Irving, and Irina Higgins. 2022. Solving math word problems with processand outcomebased feedback. Preprint, arXiv:2211.14275. Yufei Wang, Wanjun Zhong, Liangyou Li, Fei Mi, Xingshan Zeng, Wenyong Huang, Lifeng Shang, Xin Jiang, and Qun Liu. 2023. Aligning large language models with human: survey. arXiv preprint arXiv:2307.12966. Yuxiang Wei, Olivier Duchenne, Jade Copet, Quentin Carbonneaux, Lingming Zhang, Daniel Fried, Gabriel Synnaeve, Rishabh Singh, and Sida Wang. 2025. Swe-rl: Advancing llm reasoning via reinforcement learning on open software evolution. arXiv preprint arXiv:2502.18449. Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, et al. 2023. Toolllm: Facilitating large language models to master 16000+ real-world apis. arXiv preprint arXiv:2307.16789. Mengsong Wu, Tong Zhu, Han Han, Chuanyuan Tan, Xiang Zhang, and Wenliang Chen. 2024. Seal-tools: Self-instruct tool learning dataset for agent tuning and detailed benchmark. Preprint, arXiv:2405.08355. Abhinav Rastogi, Xiaoxue Zang, Srinivas Sunkara, Raghav Gupta, and Pranav Khaitan. 2020. Towards scalable multi-domain conversational agents: The schema-guided dialogue dataset. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pages 86898696. Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jean-baptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, et al. 2024. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530. Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Eric Hambro, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. 2023. Toolformer: Language models can teach themselves to use tools. Advances in Neural Information Processing Systems, 36:6853968551. Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. 2023. Reflexion: Language agents with verbal reinforcement learning. Advances in Neural Information Processing Systems, 36:86348652. Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul Christiano. 2020. Learning to summarize with human feedback. Advances in neural information processing systems, 33:3008 3021. Binfeng Xu, Zhiyuan Peng, Bowen Lei, Subhabrata Mukherjee, Yuchen Liu, and Dongkuan Xu. 2023. Rewoo: Decoupling reasoning from observations for efficient augmented language models. arXiv preprint arXiv:2305.18323. Fanjia Yan, Huanzhi Mao, Charlie Cheng-Jie Ji, Tianjun Zhang, Shishir G. Patil, Ion Stoica, and Joseph E. Gonzalez. 2024. Berkeley function calling leaderboard. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. 2025. Qwen3 technical report. arXiv preprint arXiv:2505.09388. An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang, Jialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jin Xu, Jingren Zhou, Jinze Bai, Jinzheng He, Junyang Lin, Kai Dang, Keming Lu, Keqin Chen, Kexin Yang, Mei Li, Mingfeng Xue, Na Ni, Pei Zhang, Peng Wang, Ru Peng, Rui Men, Ruize Gao, Runji Lin, Shijie Wang, Shuai Bai, Sinan Tan, Tianhang Zhu, Tianhao Li, Tianyu Liu, Wenbin Ge, Xiaodong Deng, Xiaohuan Zhou, Xingzhang Ren, Xinyu Zhang, Xipin Wei, Xuancheng Ren, Yang Fan, Yang Yao, Yichang Zhang, Yu Wan, Yunfei Chu, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zhihao Fan. 2024. Qwen2 technical report. arXiv preprint arXiv:2407.10671. Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan Azarnasab, Faisal Ahmed, Zicheng Liu, Ce Liu, Michael Zeng, and Lijuan Wang. 2023. Mmreact: Prompting chatgpt for multimodal reasoning and action. arXiv preprint arXiv:2303.11381. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. 2023. React: Synergizing reasoning and acting in language models. In International Conference on Learning Representations (ICLR). Yuanqing Yu, Zhefan Wang, Weizhi Ma, Shuai Wang, Chuhan Wu, Zhiqiang Guo, and Min Zhang. 2024. Steptool: Enhancing multi-step tool usage in llms through step-grained reinforcement learning. arXiv preprint arXiv:2410.07745. Yirong Zeng, Xiao Ding, Yuxian Wang, Weiwen Liu, Wu Ning, Yutai Hou, Xu Huang, Bing Qin, and Ting Liu. 2025. itool: Reinforced fine-tuning with dynamic deficiency calibration for advanced tool use. Preprint, arXiv:2501.09766. Jianguo Zhang, Tian Lan, Ming Zhu, Zuxin Liu, Thai Hoang, Shirley Kokane, Weiran Yao, Juntao Tan, Akshara Prabhakar, Haolin Chen, et al. 2024. xlam: family of large action models to empower ai agent systems. arXiv preprint arXiv:2409.03215."
        },
        {
            "title": "A Appendix",
            "content": "A.1 FC-RewardBench benchmark details The list of models included in FC-RewardBench, along with the number of incorrect tool call output samples per model is provided in Table 4. Model Name Count Qwen/Qwen2.5-0.5B-Instruct Qwen/Qwen2.5-0.5B-Instruct-FC ibm-granite/granite-20b-functioncalling Qwen/Qwen2.5-1.5B-Instruct BitAgent/BitAgent-8B DeepSeek-R1 openbmb/MiniCPM3-4B-FC NovaSky-AI/Sky-T1-32B-Preview Qwen/Qwen2.5-1.5B-Instruct-FC speakleash/Bielik-11B-v2.3-Instruct Qwen/Qwen2.5-14B-Instruct-FC openbmb/MiniCPM3-4B Qwen/Qwen2.5-14B-Instruct Qwen/Qwen2.5-7B-Instruct ZJared/Haha-7B meetkai/functionary-small-v3.1-FC watt-ai/watt-tool-70B Qwen/Qwen2.5-7B-Instruct-FC Qwen/Qwen2.5-32B-Instruct-FC Qwen/Qwen2.5-32B-Instruct meetkai/functionary-medium-v3.1-FC Team-ACE/ToolACE-2-8B Qwen/QwQ-32B-Preview 450 237 112 102 74 64 59 54 52 41 38 38 28 23 22 21 21 18 15 13 11 6 Table 4: Breakdown of errors by models in FCRewardBench A.2 ToolRM Training Data Details We use the following models to generate the training data for ToolRM: ibm-granite/granite-3.3-2b-instruct ibm-granite/granite-3.3-8b-instruct ibm-granite/granite-20b-functioncalling HuggingFaceTB/SmolLM2-1.7B-Instruct Qwen/Qwen2.5-0.5B-Instruct Qwen/Qwen2.5-1.5B-Instruct Qwen/Qwen2.5-7B-Instruct Qwen/Qwen2.5-14B-Instruct Qwen/Qwen2.5-32B-Instruct mistralai/Mistral-7B-Instruct-v0.3 mistralai/Mistral-Nemo-Instruct-"
        },
        {
            "title": "A few samples from the training data are shown",
            "content": "in Figure 5. Figure 5: Data samples from ToolRM training data. Each sample has tool catalog, conversation between the user and assistant, along with the corresponding correct and incorrect tool calls. The top sample is missing one tool call from the Incorrect version, while the Bottom sample is missing parameter from the tool call. A.3 ToolRM prompt The prompt used to train the ToolRM is shown in Listing 1. A.4 FC-RewardBench Experiment Details A.4.1 Model Details We include the following models from RewardBench: Ray2333/GRM-Llama3.2-3B-rewardmodel-ft Skywork/Skywork-Reward-V2-Qwen3-4B Skywork/Skywork-Reward-V2-Qwen3-8B LxzGordon/URM-LLaMa-3.1-8B Skywork/Skywork-Reward-Llama-3.1-8B-v0.2 nicolinho/QRM-Llama3.1-8B-v infly/INF-ORM-Llama3.1-70B Skywork/Skywork-Critic-Llama-3.1-70B We include the following LLMs as Judges: deepseek-ai/DeepSeek-V3 meta-llama/Llama-3.1-405B-Instruct-FP8 meta-llama/Llama-3.3-70B-Instruct meta-llama/Llama-4-Scout-17B-16E meta-llama/Llama-4-Maverick-17B-128E-Instruct openai/gpt-oss-120b A.4.2 LLM-as-Judge Prompt The prompt used to evaluate LLMs-as-Judges on FC-RewardBench is shown in Listing 2. The placeholders {tool-library}, {query}, {response-A}, and {response-B} are replaced with appropriate values. A.5 Error Analysis In this experiment, we evaluate the impact of ToolRM-14B on error reduction in the Best-of-n (n = 32) sampling setting, using the Qwen3-1.7B generator on the single-turn splits of BFCL-v3. As reported in Table 5, ToolRM-14B decreases the total error count from 742 to 573, corresponding to 22.7% relative reduction. The predominant error type, Incorrect Parameter Value, responsible for nearly 42% of greedy decoding errors, is reduced by 28%, indicating that ToolRM is particularly effective at mitigating semantic mis-specification of parameter values. Moreover, errors such as Incorrect Function Name and Wrong Number of Functions are reduced by 37% and 57%, respectively. In contrast, Irrelevance Errors increase from 185 to 210, suggesting that ToolRM tends to favor producing function call outputs even in cases where no valid call can appropriately satisfy the user query. Error type Greedy ToolRM-14B Incorrect Parameter Value Irrelevance error Malformed output syntax Incorrect function name Missing optional parameter Incorrect parameter type Wrong number of functions Total 321 185 86 62 36 31 21 231 210 34 39 26 24 9 573 Table 5: Breakdown of errors with the Qwen3-1.7B model as generator with Greedy decoding and Best-of32 sampling with ToolRM-14B 1 <im_start>system 2 You are provided with user query, catalog of tools available to fulfill that user query, and list of (cid:44) tool calls that use tools available in the catalog to fulfill user request. 3 Your job is to assess whether the tool calls adequately fulfill the user request or not. 4 5 You have the following tools available: 6 7 ```json 8 [ 9 {\"name\": \"diabetes_prediction\", \"description\": \"Predict the likelihood of diabetes type 2 based (cid:44) on person's weight and height.\", \"parameters\": {\"type\": \"dict\", \"properties\": {\" (cid:44) weight\": {\"type\": \"integer\", \"description\": \"Weight of the person in lbs.\"}, \"height\": (cid:44) {\"type\": \"integer\", \"description\": \"Height of the person in inches.\"}, \"activity_level\": (cid:44) {\"type\": \"string\", \"enum\": [\"sedentary\", \"lightly active\", \"moderately active\", \"very (cid:44) active\", \"extra active\"], \"description\": \"Physical activity level of the person.\"}}, \" (cid:44) required\": [\"weight\", \"height\", \"activity_level\"]}} 10 ] 11 ``` 12 13 <im_end> 14 <im_start>user 15 Predict whether person with weight 150lbs and height 5ft 10in who is lightly active will get type 2 (cid:44) diabetes.<im_end> 16 <im_start>assistant 17 ```json 18 [ 19 20 ] 21 ```<im_end> {\"diabetes_prediction\": {\"weight\": 150, \"height\": 68, \"activity_level\": \"lightly active\"}} Listing 1: ToolRM prompt 1 Please act as an impartial judge and evaluate the quality of the responses provided by two AI assistants (cid:44) to the user question displayed below. You should choose the assistant that follows the user's (cid:44) instructions and answers the user's question best. 2 3 You will be given: 4 TOOL SPECIFICATIONS: All the tool specifications including parameters and their descriptions (cid:44) available to the assistant to answer the query. 5 CONVERSATION: Conversation between the user and the assistant 6 ASSISTANT RESPONSES: List of responses from two assistants [[A]] and [[B]]. Each response is (cid:44) sequence of tool calls needed to answer the question. 7 8 When comparing two tool call sequences for the same user query, carefully evaluate both sequences (cid:44) and determine which one better follows the tool specifications and the question requirements. 9 10 Consider the following instructions to compare the assistant responses: 11 Check whether all the tools used are relevant and actually exist. 12 Verify that the tools are called in correct and logical order. 13 Ensure that the correct parameters are used for each tool and that no nonexistent parameters are (cid:44) included. 14 Confirm that parameter values and formats are appropriate based on the question and tool (cid:44) specifications. 15 Make sure all required parameters and data mentioned in the question are included. 16 Look for any extra tools or parameters that are not needed or mentioned in the question. 17 18 Also, follow these general instructions: 19 Begin your evaluation by comparing the two responses (i.e., [[A]] and [[B]]) and provide short (cid:44) explanation. 20 Avoid any position biases and ensure that the order in which the responses were presented does not (cid:44) influence your decision. 21 Do not allow the length of the responses to influence your evaluation. 22 Do not favor certain names of the assistants. 23 Be as objective as possible. 24 After providing your explanation, output your final verdict by strictly following this format: \"[[A]]\" if (cid:44) assistant A's response is better and \"[[B]]\" if assistant B's response is better. 25 STRICTLY follow this output format: [EXPLANATION]n{Short comparison highlighting (cid:44) differences and reasoning.}nn[VERDICT]n{[[A]] or [[B]]}. Place your reasoning after the [ (cid:44) EXPLANATION] tag and your final choice after the [VERDICT] tag. 26 27 # TOOL SPECIFICATIONS 28 {toollibrary} 29 30 # CONVERSATION 31 {query} 32 33 # ASSISTANT RESPONSES: 34 [[A]] {responseA} 35 [[B]] {responseB} 36 37 # ANSWER: Listing 2: LLM-as-Judge Prompt used for FC-RewardBench evaluation"
        }
    ],
    "affiliations": [
        "IBM Research, USA"
    ]
}