{
    "paper_title": "AlayaDB: The Data Foundation for Efficient and Effective Long-context LLM Inference",
    "authors": [
        "Yangshen Deng",
        "Zhengxin You",
        "Long Xiang",
        "Qilong Li",
        "Peiqi Yuan",
        "Zhaoyang Hong",
        "Yitao Zheng",
        "Wanting Li",
        "Runzhong Li",
        "Haotian Liu",
        "Kyriakos Mouratidis",
        "Man Lung Yiu",
        "Huan Li",
        "Qiaomu Shen",
        "Rui Mao",
        "Bo Tang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "AlayaDB is a cutting-edge vector database system natively architected for efficient and effective long-context inference for Large Language Models (LLMs) at AlayaDB AI. Specifically, it decouples the KV cache and attention computation from the LLM inference systems, and encapsulates them into a novel vector database system. For the Model as a Service providers (MaaS), AlayaDB consumes fewer hardware resources and offers higher generation quality for various workloads with different kinds of Service Level Objectives (SLOs), when comparing with the existing alternative solutions (e.g., KV cache disaggregation, retrieval-based sparse attention). The crux of AlayaDB is that it abstracts the attention computation and cache management for LLM inference into a query processing procedure, and optimizes the performance via a native query optimizer. In this work, we demonstrate the effectiveness of AlayaDB via (i) three use cases from our industry partners, and (ii) extensive experimental results on LLM inference benchmarks."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 1 ] . [ 1 6 2 3 0 1 . 4 0 5 2 : r AlayaDB: The Data Foundation for Efficient and Effective Long-context LLM Inference Yangshen Deng*, Zhengxin You*, Long Xiang*, Qilong Li, Peiqi Yuan, Zhaoyang Hong, Yitao Zheng, Wanting Li, Runzhong Li, Haotian Liu, Kyriakos Mouratidis, Man Lung Yiu, Huan Li, Qiaomu Shen, Rui Mao, Bo Tang(cid:66) Corresponding email: research@alayadb.ai Abstract AlayaDB is cutting-edge vector database system natively architected for efficient and effective long-context inference for Large Language Models (LLMs) at AlayaDB AI. Specifically, it decouples the KV cache and attention computation from the LLM inference systems, and encapsulates them into novel vector database system. For the Model as Service providers (MaaS), AlayaDB consumes fewer hardware resources and offers higher generation quality for various workloads with different kinds of Service Level Objectives (SLOs), when compared with the existing alternative solutions (e.g., KV cache disaggregation, retrieval-based sparse attention). The crux of AlayaDB is that it abstracts the attention computation and cache management for LLM inference into query processing procedure, and optimizes the performance via native query optimizer. In this work, we demonstrate the effectiveness of AlayaDB via (i) two use cases from our industry partners, and (ii) extensive experimental results on LLM inference benchmarks. CCS Concepts Information systems Data management systems; Computing methodologies Artificial intelligence. Keywords Vector database, Large language model, Machine learning systems"
        },
        {
            "title": "1 Introduction\nLarge Language Models (LLMs) have been widely used in various\nreal-world applications such as personal assistants [4, 6, 9, 14, 41],\nsearch engines [2, 3, 10, 17], code generators [5, 7, 11, 41] and\ndocument analyzers [33, 44]. Efficient and effective LLM inference\nis an open problem in the industry [12, 20, 25, 50], especially for\nlong-context (e.g., millions of tokens) inference. In particular, the\nperformance of LLM inference systems is evaluated by three metrics:\n(1) inference latency, the end-to-end time cost for user tasks, (2)\ngeneration quality, the capabilities of LLM in various workloads,\nand (3) GPU memory consumption, the used hardware resources\nfor the user tasks.",
            "content": "Many solutions have been proposed to optimize these metrics in long-context LLM inference. They can be classified into three categories: (i) coupled architecture; (ii) KV cache disaggregation; and (iii) retrieval-based sparse attention. vLLM [42], SGLang [69] and HuggingFace transformers [61] are the most widely-used LLM inference systems in (i) coupled architecture. LLM model computation and KV cache management are tightly coupled in these systems. * These authors contributed equally to this work. (cid:66) Corresponding author: Prof. Bo Tang. 1 These systems achieve high generation quality as they use full attention mechanism. Mooncake [51] and LMCache [15, 46] are representative LLM inference systems in (ii) KV cache disaggregation. They store the KV cache of contexts in external storage and reuse them among different LLM inference instances. Thus, the inference latency of these systems is improved as it reuses the KV cache and reduces the expensive computations (e.g., inner product and softmax). Recently, retrieval-based sparse attention solutions have been proposed (e.g., InfLLM [63] and RetrievalAttention [45]) to alleviate the large GPU memory consumption of these systems in both (i) and (ii). The core idea behind them is the sparse attention mechanism, i.e., only subset of critical key and value tokens are selected to perform the attention computation. Unfortunately, existing systems cannot simultaneously optimize the three aforementioned performance metrics, as we will elaborate in Section 3. At AlayaDB.AI, we designed an LLM-native vector database AlayaDB to overcome the limitations of existing LLM inference systems/solutions and enable efficient and effective long-context inference in LLM era. Specifically, for Model as Service (MaaS) [38] providers, the SLOs of different kinds of workloads indicate their requirements for the inference latency. Thus, the core challenge of AlayaDB is solving bi-objective optimization problem, i.e., meet the SLOs of different workloads by consuming less GPU memory and offering higher generation quality simultaneously. The core idea of AlayaDB is to decouple both KV cache and attention computation and to encapsulate them into monolithic vector database. The major benefits of the novel disaggregation level are three-fold. Lightweight LLM Inference System. The cache management and attention computation can be separated from the LLM inference engine, which lightens its burden. Interface Simplification. It simplifies the interface between LLM inference engine and KV cache service by only returning the attention result, instead of the KV cache content. Co-optimization Opportunity. It sheds light on co-optimizing attention computation and KV cache management in monolithic vector database together. At high level, AlayaDBs role in LLM inference is comparable to the role of traditional databases [21, 30, 40, 47, 53, 57] in web applications. Specifically, the LLM application developers only need to pay attention to the logic of their applications while AlayaDB offers efficient long-context management from their developed LLM applications. This is analogous to web application developers focusing on the logic of their applications and leaving efficient data management to traditional relational database. To achieve the above vision, there are three design goals of AlayaDB: (i) ease-to-use, (ii) high generation quality, and (iii) good efficiency. AlayaDB employs novel system architecture and introduces end-to-end optimizations. Firstly, it provides simple-yetpowerful abstractions and APIs, which are compatible with the software ecosystem of LLMs. Secondly, it handles sparse attention computation as vector search query. To improve the generation quality and reduce the memory consumption simultaneously, AlayaDB defines novel query type, i.e., dynamic inner product range query (DIPR), which overcomes the limitations of the traditional top-ùëò query. To accelerate query processing, AlayaDB includes native query optimizer, which selects the best execution plan for efficient vector search. Last but not least, suite of optimization techniques (from algorithm-side to index-side, from computation to storage) has been employed in AlayaDB. Compared to existing LLM inference systems, AlayaDB enjoys low latency, high generation quality, and low resource consumption at the same time from long-context inference. Our experience shows that AlayaDB greatly lowers the cost of hardware resources for handling long contexts and lightens the labor for optimizing the LLM infrastructure. AlayaDB has already been used to support several online LLM services including chatting apps and knowledgebase QA services in our industry partners. To sum up, the technical contributions of AlayaDB are as follows. Novel Decoupling Level for LLM Inference Systems: We classify existing LLM inference solutions into three categories and analyze their limitations to handle the challenges of longcontext LLM inference. Then, we decouple the KV cache and attention computation from the LLM inference system and encapsulate them into novel vector database system. Dynamic Vector Search Query for Sparse Attention: We analyze the internal characteristics of sparse attention in various LLM benchmarks and real-world applications, then propose novel dynamic vector search query, i.e., Dynamic Inner Product Range (DIPR), to capture the dynamic nature of sparse attention, which overcomes the limitation of traditional top-ùëò query. AlayaDB System Architecture and Implementation: We architect and implement AlayaDB for efficient and effective longcontext inference. It consists of user interface, query processing engine, and vector storage engine. AlayaDB has been used in several LLM applications by our industry partners. To the best of our knowledge, it is the first vector database natively built for LLM inference. Extensive Evaluation: We conduct in-depth evaluation of AlayaDB. The results show that it is able to reduce resource consumption and offer better generation quality while guaranteeing the SLOs for various LLM workloads. The remainder of the paper is organized as follows. Section 2 introduces the LLM inference procedure; Sections 3 and 4 present the motivation, design goals and architecture of AlayaDB; Sections 5, 6, and 7 describe AlayaDBs components; Section 8 elaborates two use cases of AlayaDB; Section 9 presents the experimental study results and Section 10 concludes this work. Y. Deng, Z. You, L. Xiang, et al."
        },
        {
            "title": "2 LLM Inference\nA large language model (LLM) is a deep neural language model\nwith billions of parameters. The decoder-only transformer is the\nmost prevalent architecture in LLMs, such as GPT [4], Llama [56],\nand Qwen [41]. Given a well-trained LLM model, the LLM inference\ngenerates the text in response to the user input prompt, as shown\nin Figure 1(a). Actually, it generates the tokens in response text one\nby one. Each token generation is a forward pass of the language\nmodel. The generated token will be appended to the end of the input\nprompt to form the new context. The context is used to generate\nthe next token by following the same forward pass of the language\nmodel. The generation procedure terminates when a special token\n<eot> (end of text) is generated or the generated text reaches the\npredefined maximum length.",
            "content": "Figure 1(b) depicts the major components in the LLM inference procedure. For the input context of LLM, it first breaks down the text into small chunks (i.e., tokens), then turns tokens into numeric representations to capture the meaning via the tokenization and embedding modules. The embeddings of the context are the input of the transformer model, which consists of stack of transformer layers that do all the processing. The output of the transformer is probability scores for what the most likely next token is via the language modeling head. Transformer LLMs include stack of transformer layers, e.g., Llama 3.1 has 32 layers. Each transformer layer processes its inputs and passes the results to the next layer. For each transformer layer, it has two successive modules: (i) selfattention module, and (ii) feed-forward neural network. The feedforward neural network emphasizes the important features to make the output more informative. We next elaborate the core of transformer LLM, i.e., self-attention mechanism, via the illustrated Figures 1(c) and (d). In general, the self-attention mechanism involves two major steps: (i) measuring how relevant each of the previous context tokens is to the current token being processed; and (ii) combining the information from them into single output vector. well-trained LLM has three projection matrices, i.e., query projection matrix ùëæùëÑ , key projection matrix ùëæùêæ and value projection matrix ùëæùëâ , which are used to calculate the attention. In particular, the self-attention mechanism starts by multiplying the input matrix ùëø Rùëõùëë , where ùëõ is the number of input vectors and ùëë is the dimensionality of the embedding vector of each token, by the projection matrices to create three new matrices, i.e., query matrix ùë∏, key matrix ùë≤ and value matrix ùëΩ , as shown in Figure 1(c). These three matrices are the information of the input tokens in three different spaces, which are used to calculate the attention. In recent transformer LLMs (e.g., Llama 3.1), multi-query and multi-head self-attention mechanisms are employed to improve the scalability of larger models. For simplicity, we utilize one self-attention head for illustration in Figure 1(d) as every head of multi-head attention has distinct version of matrices of queries, keys and values, see Figure 1(c). ùëßùëñ ùëó = ùííùëñ ùíåùëá ùëó ùëë ; ùëéùëñ ùëó = softmax(ùëßùëñ ùëó ); ùíêùëñ = ùëñ ùë†=1 ùëéùëñùë† ùíóùë† (1) As shown in Figure 1(d), to generate the ùëñ + 1-th token ùë°ùëñ+1, the self-attention mechanism in each head computes the inner product between the query vector ùííùëñ R1ùëë and the key vector of the past 2 AlayaDB Figure 1: The concepts and illustrations of LLM inference tokens ùíå ùëó where ùëó [1, ùëñ]. The computed product is scaled by ùëë and normalized via Softmax function to derive the attention score ùëéùëñ ùëó . These attention scores multiply with the value vectors ùíóùë† in value matrix ùëΩ to compute the output ùíêùëñ , see Equation (1). LLM Inference Phases. In LLM services, the LLM inference procedure of prompt can be decomposed into two phases: prefill phase and decode phase. Specifically, in the prefill phase the LLM processes all the input tokens in user prompts and generates the first output token. The service level objective (SLO) of the prefill phase in LLM service is its duration, i.e., Time-To-First-Token (TTFT). In the decode phase the LLM sequentially generates the answers. This phase completes when an end-of-sequence token <eot> is generated or when the context reaches specified maximum length. The SLO in the decode phase is Time-Per-Output-Token (TPOT). KV Cache. Recall that the last generated token is appended to the previous context and then input into the LLM for the next token generation. In particular, the new context does another forward pass of the model. Obviously, the performance of the decode phase can be significantly improved by caching the key and value matrices of the previous context (see KV cache in Figure 1(d)) as they do not need to be recomputed. The KV cache is one of the core components in the self-attention mechanism which is widely used in recent LLMs and offers significant speedup of the decode phase. Sparse Attention. The attention calculation in Equation (1) is the computationally expensive part of LLM inference. To make matters worse, the key and value matrices consume large GPU memory space. The sparse attention mechanism has been proposed to improve the efficiency of the attention calculation and reduce the GPU memory consumption during the LLM inference. The intuition of sparse attention is that only small proportion of tokens, not all tokens in previous context, dominates the generation quality/accuracy [45]. For example, only the key vectors and value vectors in KV cache with red rectangles in Figure 1(d) are critical vectors for the high-quality token generation. The computation cost of the attention calculation is significantly reduced as the sparse attention only calculates fixed size of keys (resp. values) in key matrix ùë≤ (resp. value matrix ùëΩ ), instead of all keys and values in both key and value matrices, see Equation (1). Key vectors with high inner product scores relative to the query vector are considered important, as they have high attention scores, significantly contributing to the final output."
        },
        {
            "title": "3 Motivation of AlayaDB\nThe context length of an LLM request becomes very large with the\nrapid development of LLM applications. For example, users may\nask LLM questions about long documents, including understand-\ning academic papers [8], getting legal assistance from law docu-\nments [22, 35], or analyzing financial documents [33]. Chat applica-\ntions [4, 6, 14] utilize the long chat log to produce better responses\nfor users. The AI programming assistants leverage all code in the\nproject to accurately generate code or identify bugs/errors [5, 11].\nThe long-context LLM inference is extremely expensive as its\nself-attention mechanism incurs high memory consumption and\nnumerous computation operations. In particular, it requires ùëÇ (ùëõ)\nmemory to store the KV cache, where ùëõ is the length of the long\ncontext. The compute complexity for the prefill phase is ùëÇ (ùëõ2) due\nto the self-attention computation in Equation (1) that applies to",
            "content": "3 Table 1: LLM inference solutions analysis Existing GPU memory consumption solution ‚ë† Large ‚ë° Large ‚ë¢ Small AlayaDB Small Inference Generation latency High Medium Low quality Good Good Medium Good Solution usability Good Medium Bad Good Figure 2: Summary of LLM inference solutions each input token. Thus, the TTFT of prefill phase is several minutes to tens of minutes when the context length is quite large. For the decode phase, it needs ùëÇ (ùëõ) for each token generation. In practice, it takes about 141.38 GB memory and 6 minutes to answer question about the book Database System Concepts, 7th Edition [52] (495.5K tokens), with bfloat16 version Llama-3-8B model [13] on 2 NVIDIA A800 GPUs (each has 80 GB memory). To reduce the memory consumption and computation cost of long context LLM inference, several research studies have been proposed to reuse the KV cache of the long context and use them to serve different requests from the users. For example, the users may ask various questions about the same book Database System Concepts. Thus, the KV cache of this book can be reused to answer these different questions. The reused KV cache reduces the latency of TTFT in prefill phase significantly, and it becomes de facto standard in LLM inference systems. However, the performance of long-context LLM inference still has lot of room for improvement. We next analyze the existing LLM inference systems/techniques in four dimensions: (i) GPU memory consumption, (ii) inference latency, (iii) generation quality, and (iv) solution usability."
        },
        {
            "title": "3.1 Analysis of Existing Solutions\nIn this section, we classify existing work into three categories: ‚ë†\ncoupled architecture, ‚ë° KV cache disaggregation, and ‚ë¢ Retrieval-\nbased sparse attention mechanism. We introduce the core idea of\neach category and analyze the characteristics of them in detail.\nTable 1 summarizes the analyzed results of existing solutions.",
            "content": "‚ë† Coupled Architecture. It is the widely-used LLM inference system architecture in industry, e.g., vLLM [42], SGLang [69], and transformers [61]. The core idea of the coupled architecture is the LLM model computation and KV cache management are tightly coupled and it processes the user request in holistic manner, as shown in Figure 2(a). It offers good usability with simple user interface and high generation quality. However, it fails to handle long context. The major reasons are: (i) the large GPU memory consumption for KV cache and (ii) the high TTFT in prefill phase 4 Y. Deng, Z. You, L. Xiang, et al. as it reuses the KV cache in coarse manner, e.g., vLLM employs LRU policy to maintain the KV cache in limited GPU memory. ‚ë° KV Cache Disaggregation. As depicted in Figure 2(b), several systems decouple the KV cache into separate storage service and manage it in stateful way. For example, LMCache [15] and Mooncake [51] store the KV cache of long context in external cheap storage (e.g., CPU memory, disk or remote memory) after its prefill phase such that the KV cache can be reused by everyone in the future as it only needs to be loaded into the inference engine. The inference latency of the KV cache disaggregation solutions is slightly lower than the coupled architecture as it reduces the TTFT of prefill phase by reusing KV cache better. The generation quality of it is the same as the coupled solution as both employ full attention mechanism. However, it is not easy to use, as it involves lot of intrusive modifications (i.e., lots of engineering work) to the inference engine. Moreover, the KV cache disaggregation still consumes large GPU memory during the decoding stage. ‚ë¢ Retrieval-based Sparse Attention. Recently, InfLLM [63] and RetrievalAttention [45] use the sparse attention mechanism to alleviate the high GPU memory consumption of these systems in both ‚ë† and ‚ë°. In particular, they only retrieve small subset of keys and values from offloaded KV cache for attention computation, see Figure 2(c). Although these retrieval-based solutions can significantly reduce GPU memory consumption, almost all (if not all) of them are not easy to use as (i) the retrieval algorithm is hard-coded in the underlying specific LLM model and cannot be directly used on other LLM models and (ii) they lack the ability to manage and reuse the long contexts among different requests and inference engines. Moreover, they trade off between memory consumption/inference latency and generation quality. In particular, the generation quality of these methods is determined by the retrieved critical keys and values. However, it is challenging to retrieve all the critical keys and values efficiently. Existing work assumes that the number of critical vectors is fixed (i.e., ùëò) and then retrieves top-ùëò critical keys and values from the offloaded KV cache. This static method cannot achieve the good generation quality of ‚ë† and ‚ë°, as we will elaborate in Section 6. Regarding inference latency, the retrieval-based sparse attention methods introduce extra overhead to identify the critical key and value vectors. However, they gains benefits during the attention computation as only the selected critical keys and values will be used. According to our internal experimental evaluation, there is no clear winner between the extra overhead and the reduced attention computation. Thus, we use in the inference latency column of ‚ë¢, see Table 1."
        },
        {
            "title": "3.2 Design Goals of AlayaDB\nMotivated by the above limitations, we propose a novel architecture\nfor efficient and effective long-context LLM inference by decoupling\nboth the KV cache and sparse attention computation from the\nLLM inference engine. In particular, we architect a vector database\nAlayaDB to manage the offloaded KV cache and compute the sparse\nattention for LLM inference, as illustrated in Figure 2(d). The design\ngoals of AlayaDB are as follows.",
            "content": "G1: Ease-of-use. The first design goal of AlayaDB for long-context LLM inference engine is ease-of-use. Thus, the user interface abstraction of AlayaDB should be simple and compatible with LLM AlayaDB Figure 3: System overview of AlayaDB inference engines. With these abstractions, the LLM developers could use AlayaDB easily for efficient and effective inference in their LLM applications, e.g., analogue to how web developers use traditional database systems in their web applications. G2: High Quality. The second design goal of AlayaDB for longcontext LLM inference engines is to provide high generation quality. As mentioned above, the generation quality is determined by the quality of retrieved critical keys and values. Thus, AlayaDB should offer the capability to identify the critical tokens. G3: Good Efficiency. The third design goal of AlayaDB for longcontext LLM inference engine is good efficiency. Specifically, AlayaDB should achieve higher generation quality and lower memory consumption as much as possible for user-specified SLOs. We are aware that many vector database systems/techniques have been proposed [1, 18, 19, 24, 27, 34, 39, 54, 59, 60, 62], both in academia and industry. However, to the best of our knowledge, none of them are natively designed to support efficient and effective longcontext LLM inference. In subsequent, we introduce the architecture and key components of AlayaDB. As the last row in Table 1 shows, AlayaDB incurs small memory consumption, low inference latency, and high generation quality simultaneously."
        },
        {
            "title": "4 System Overview of AlayaDB\nFigure 3 depicts the overview of AlayaDB we built at AlayaDB.AI.\nIt consists of three components: (i) user interface, (ii) query pro-\ncessing engine, and (iii) vector storage engine. We briefly introduce\neach component in AlayaDB to elaborate on the designs for the\naforementioned three design goals.",
            "content": "User Interface. The top layer of AlayaDB is the user interface component. It abstracts the complex attention computation and KV cache management to offer easy-to-use APIs. Thus, LLM developers can simply leverage efficient and effective long-context LLM inferences by invoking the abstracted APIs in AlayaDB. This is similar to how web developers can build various applications without worrying about the underlying database management system. Specifically, we use two widely-used concepts DB and Session 5 Table 2: AlayaDB APIs DB abstraction and provided APIs DB.create_session(prompts) -> Session, prompts DB.import(prompts, kv_cache) DB.store(session) Session abstraction and provided APIs Session.attention(q, layer) -> Session.update(q, k, v, layer) -> k, in database community to abstract the context and request in the LLM inference procedure. We will introduce the details in Section 5. Query Processing Engine. The middle layer of AlayaDB is the query processing engine, which is essential to achieve high quality and good efficiency goals. It consists of native attention engine and query optimizer. The native attention engine is designed for efficient sparse attention computation in Equation (1). The query optimizer is devised to identify the optimal query processing plan, which efficiently computes the critical tokens. Unlike traditional database query optimizers, the query optimizer in AlayaDB has two major modules: (i) query type module, and (ii) index type module. The query type module includes set of predefined queries (e.g., topùëò) that are used to retrieve the critical tokens from the KV cache. The index type module has set of indices that can be used to accelerate the predefined queries. It is worth pointing out that both query type and index type in the query optimizer are extensible in AlayaDB. The details of this engine are presented in Section 6. Vector Storage Engine. To further improve the efficiency (both memory consumption and inference latency), we equipped AlayaDB with the vector storage engine in the bottom layer. It includes buffer manager and novel vector file system. novel vector data layout scheme is designed in the vector file system, which could be used to improve the data access locality during query processing. The buffer manager manages the buffered blocks of KV cache and supports high-performance keys and values retrieval. We will show the optimizations of the vector storage engine in Section 7.3."
        },
        {
            "title": "5 User Interface\nAlayaDB provides simple and flexible abstractions and easy-to-use\nAPIs for users to import context, reuse context and compute sparse\nattention result for efficient and effective long-context LLM infer-\nence. Two core abstractions in AlayaDB are DB and Session. A DB\nin AlayaDB manages all the contexts, including prompts, KV cache\nand vector indexes, e.g., an analogue of DB instance in traditional\nrelational database systems, which include the schema, tables, and\ndata tuples. In a traditional database system, a database session is\nthe connection established between an application server and a data-\nbase server to enable communication and data retrieval. Inspired by\nit, in AlayaDB, a Session connects the contexts and the running\ninference requests from a user. AlayaDB provides compatible APIs\nwith HuggingFace transformers [61] and flash-attention [31, 32]\nlibrary, which are the de facto standards of LLM inference and\nattention computation. The core APIs provided by AlayaDB are\nsummarized in Table 2. We briefly introduce them as follows.",
            "content": "Y. Deng, Z. You, L. Xiang, et al. (a) Original code using flash-attention and transformers (b) Modified code using AlayaDB with transformers Figure 4: Using AlayaDB APIs for LLM inference DB.create_session(prompts) takes list of prompts as input and returns Session object and the truncated prompts. Given the input prompts, it reuses the longest common prefix with the stored contexts. The reused context is in the Session object. The non-reused part of input prompts are the truncated prompts. DB.import(prompts, kv_cache) imports list of computed contexts to AlayaDB for further reuse. Thus, its inputs are the prompts and KV cache of these contexts. DB.store(session) persists all states in session into reusable context in the database. It takes the session with the corresponding prompts and KV cache as the input. Session.attention(q, layer) -> generates the attention results of one LLM model layer for the session. It accepts the query vectors and the layer id as the input, and returns computed attention output. This API can be used to replace the flash-attention APIs. Session.update(q, k, v, layer) -> k, updates session with the new inputs or generated tokens for one model layer. This API is compatible with DynamicCache.update in huggingface transformers. It provides an option to return the full key and value cache for manual management. Example. With the above APIs, it is easy for users to import context, reuse context and compute sparse attention score for efficient and effective long-context LLM inferences upon various LLM models. Figure 4 shows an illustration example of AlayaDB with HuggingFace transformers, which only changes few lines of code. In particular, Figure 4(a) is the original code. The inference function is the common implementation of using an LLM model (offered by HuggingFace transformers). For model and list of prompts, it creates new DynamicCache to manage the KV cache as the past_key_values. The prompts and past_key_value are inputs of LLM model to generate the next tokens. LlamaAttention.forward 6 Figure 5: The number of selected tokens in different heads Table 3: The number ùëò of required tokens in different tasks Task Qasper Passage R. QMSum 350 250 150 proportion Task 9.67% 2.69% 1.41% LCC HotpotQA TriviaQA 65 200 20 proportion 5.26% 2.19% 0.24% is the implementation of an attention layer in HuggingFace transformers. It first updates the past_key_value with the newly generated key and value matrix, which is now DynamicCache with the full KV cache. Then, it invokes flash_attn_func attention operator on the newly generated query matrix and the full KV cache. Figure 4(b) shows how to use AlayaDB APIs for the above LLM inference procedure. From the application side, users can enjoy the ability to manage and reuse the contexts in AlayaDB by simply replacing DynamicCache with Session, as the pink-colored lines show. Specifically, it calls DB.CreateSession to initialize session and the truncated prompts for the input prompts. The non-reusable parts (truncated prompts) are input to the LLM model together with the Session for further generation. To further leverage the native attention computation from AlayaDB, users only need to modify the LlamaAttention.forward to replace the flash-attention with the Session.attention, see the last highlighted line in Figure 4(b)."
        },
        {
            "title": "6.1 Dynamic Inner Product Range Query (DIPR)\n6.1.1 Limitations of Top-ùëò Query. In sparse attention, a subset of\ncritical key and value vectors are retrieved to approximately com-\npute the attention output. Thus, the effectiveness of the computed\nattention output is determined by the number of retrieved criti-\ncal tokens. The efficiency of LLM inference also depends on the\ncost to retrieve these critical tokens. Almost all (if not all) exist-\ning work [26, 36, 45, 55, 63, 64, 68] utilize top-ùëò query for critical\ntoken retrieval. The value of ùëò is a pre-defined hyper-parameter,\nand it is applied to all attention heads among all layers. It means\nthe top-ùëò query assumes the number of critical tokens is the same\namong different tasks and different heads. However, this assumption\nis probably not true in various LLM applications. We summarize\ntwo crucial observations as follows, which contradict this assump-\ntion. These observations are summarized from the user experiences",
            "content": "AlayaDB of our products industry customers, and we reproduce them in widely-used LLM benchmarks to follow the DeWitt Clause. Observation I: the number of critical tokens significantly varies in different heads. The transformer-based LLM model includes multiple layers and every layer has multiple heads. We conduct an experiment with Llama-3-8B-Instruct-262k model [12] on the KV retrieval dataset in -Bench [67] to investigate how many critical tokens are needed to result in good approximation of the full attention scores. We measure the accuracy of this approximation with the recovery ratio [45], which represents the proportion of the total attention scores accounted for by the attention scores of the selected critical tokens. The red curve in Figure 5 shows the number of tokens needed to achieve recovery ratio of 90% for each head (randomly sampled five heads per layer), which significantly varies among different heads. For example, it needs on average 42,979.85 tokens in layer 0 head 5, which is much larger than the 53.36 tokens in layer 31 head 5. Observation II: different tasks require different number of critical tokens. We conducted experiments on various tasks in LongBench [23] to explore the number of critical tokens for LLM inference in different tasks. These tasks cover key long-text applications including single-doc QA (Qasper), synthetic tasks (Passage Retrieval), multi-doc QA (HotpotQA), summarization (QMSum), code completion (LCC), and fewshot learning (TriviaQA). Table 3 lists the number of tokens ùëò (resp. its proportion to the context length) required for the top-ùëò query based sparse attention to achieve the same accuracy as full attention in these tasks. Obviously, the number of critical tokens varies widely across tasks, ranging from 20 (0.24%) to 350 (9.67%). For the simple tasks (e.g., TriviaQA), they only need few tokens as the answer can be obtained from short paragraph of context. In contrast, complex tasks (e.g., Qasper) require large amount of tokens to understand the whole context then return correct answers. Take-away message. The nature of sparse attention is to use dynamic set of critical tokens to generate high-quality responses (w.r.t. full attention) in different tasks and different heads of the transformer-based LLM models. The traditional top-ùëò query fails to capture the dynamic nature of sparse attention as it uses fixed and static ùëò, which always results in either low generation quality (i.e., retrieving too few critical tokens) or high computation cost (i.e., retrieving too many critical tokens). From Attention to DIPR. To overcome the limitation of the 6.1.2 traditional top-ùëò query with static and fixed ùëò for different heads and tasks, we propose Dynamic Inner-Product Range query (DIPR) to capture the dynamic nature of sparse attention. In particular, DIPR adaptively determines the number of critical tokens in different tasks and heads. We first formally define the critical token considered by DIPR in Definition 1. Definition 1 (Critical token). Give the definition of attention score in Equation (1), considering all key vectors in the key matrix ùë≤ = [ùíå1, , ùíåùëõ], the key ùíå ùëó is critical token for query vector ùííùëñ if and only if ùëéùëñ ùëó ùõº maxùë† [1,ùëõ] (ùëéùëñùë† ), where ùõº is proportion threshold and ranges in [0, 1]. 7 The intuition of DIPR query is finding all tokens which are larger than given proportion ùõº of the token with maximum inner product as all these tokens are critical. We next transform the critical token in Definition 1 to an inner product-based version in Definition 2. Theorem 1 guarantees the correctness of the definition transformation. Interestingly, Definition 2 means the DIPR query explicitly considers the attention computation in Equation (1). Definition 2 (Inner Product-based Critical token). Considering all key vectors in the key matrix ùë≤ = [ùíå1, , ùíåùëõ], the key ùíå ùëó is critical token for query vector ùííùëñ if and only if ùííùëñ ùíåùëá ùëó maxùë† [1,ùëõ] (ùííùëñ ùíåùëá ùë† ) ùõΩ, where ùõΩ = ùëë ùëôùëõ(ùõº). Theorem 1. The critical token in Definition 1 is equivalent to the inner product-based critical token in Definition 2. Proof. ùëéùëñ ùëó ùõº max ùë† [1,ùëõ] (ùëéùëñùë† ) exp(ùëßùëñ ùëó ) ùõº max ùë† [1,ùëõ] ùííùëñ ùíå ùëá ùëó (cid:205)ùëõ exp(ùëßùëñ ùëó ) ùë° =1 exp(ùëßùëñùë° ) ùõº max ùë† [1,ùëõ] (cid:205)ùëõ (exp(ùëßùëñùë† ) ) ùëßùëñ ùëó ln(ùõº ) + max ùë† [1,ùëõ] exp(ùëßùëñùë† ) ùë° =1 exp(ùëßùëñùë° ) (ùëßùëñùë† ) (cid:18) (cid:19) ùëë ln(ùõº ) + max ùë† [1,ùëõ] (ùííùëñ ùíå ùëá ùë† ) The proof completes by setting ùõΩ as ùëë ùëôùëõ (ùõº ). Last, we formally define the novel DIPR query in Definition 3. Definition 3 (Dynamic Inner-Product Range Query, DIPR(ùíí, ùõΩ)). Given key matrix ùë≤ = [ùíå1, , ùíåùëõ], query vector ùííùëñ and parameter ùõΩ 0, the DIPR query returns subset ùíÑùë≤ of ùë≤ , which includes all inner product-based critical tokens. The advantages of our novel DIPR query are three-fold: (i) For given ùõΩ, different numbers of critical tokens will be retrieved by different tasks and heads in DIPR query. Thus, it explicitly considers the dynamic nature of sparse attention; (ii) the input parameter ùõΩ of DIPR query directly considers the critical tokens by the attention score of every key, however, the top-ùëò query utilizes the absolute rank of every keys attention score; and (iii) the core computation of DIPR is the inner product ùííùëñ ùíåùëá ùëó , which does not introduce extra overhead and the optimizations for inner product-based top-ùëò query can be directly adopted. We demonstrate the effectiveness of our novel DIRP query by the experiments in Figures 5 and 6. In particular, the blue curve in Figure 5 shows the number of retrieved critical tokens of DIPR query by setting ùõΩ to 110, which is very close to the number of tokens required to achieve 90% recovery ratio. In Figure 6, we present the obtained results by varying ùõΩ and ùëò in DIPR query and top-ùëò query for Passage R. and LCC tasks, respectively. It confirms the DIPR query achieves higher accuracy with fewer retrieved tokens when compared with top-ùëò query. 6.1.3 DIPR Query Processing. The top-ùëò query processing algorithms efficiently return sized-ùëò set of critical tokens for every query vector by exploiting widely used graph indices on key vectors, e.g., HNSW [48], NSG [37] and RoarGraph [28]. However, they cannot be directly used to process DIPR query as DIPR query returns variable length of critical tokens w.r.t. the maximum inner product value of the query ùííùëñ and key matrix ùë≤ for different tasks and different heads. In this section, we devise the first approximate DIPR query processing algorithm DIPRS. There are two principles Y. Deng, Z. You, L. Xiang, et al. (a) Passage R. (b) LCC (a) ùê∂ ùëô0 (b) Point pruning (c) ùíí ùíåùëá ùíí ÀÜùíÑùëá ùõΩ Figure 6: The number of critical tokens in different tasks Figure 7: Three cases of tryAppend in DIPRS Algorithm 1: DIPRS(ùê∫, ùíí, ùíå0, ùëô0, ùõΩ) Input: Graph ùê∫, query ùíí, start key ùíå0, capacity threshold ùëô0, and ùõΩ Output: Critical token set ùíÑùë≤ ùíÑùëñ the (ùëñ + 1)-th key vector in ùê∂ ùëñ ùëñ + 1 foreach unvisited neighbor ùíå of ùíÑùëñ in ùê∫ do 1 Initialize list ùê∂ with start key vector ùíå0 2 ùëñ 0 3 while ùëñ < ùê∂.capacity() do 4 5 6 7 8 ÀÜùíÑ the closest point to ùíí in ùê∂ 9 return ùíÑùë≤ {ùíÑ ùíÑ ùê∂, ùíí ùíÑùëá ùíí ÀÜùíÑùëá ùõΩ } 10 Procedure tryAppend(ùíí, ùíå, ùõΩ, ùê∂, ùëô0 ): 11 12 13 14 ÀÜùíÑ the closest point to ùíí in ùê∂ Mark ùíå as visited if ùê∂.capacity() ùëô0 or ùíí ùíåùëá ùíí ÀÜùíÑùëá ùõΩ then tryAppend(ùíí, ùíå, ùõΩ, ùê∂, ùëô0 ) ùê∂.append(ùíå) of DIPRS algorithm design: (i) it should explore more points to find the larger inner product value quickly; and (ii) it should reduce non-critical point explorations. Algorithm 1 shows the pseudocode of the DIPRS algorithm, which follows the above two principles. Specifically, it utilizes the widely used graph-based indices as the fundamental building block as they offer high recall and good efficiency for inner product-based vector similarity search. Given an input ùõΩ, the number of returned tokens in the critical token set ùíÑùë≤ is dynamic and unknown in advance, until the token with maximum inner product value is found. The core ideas of DIPRS algorithm are (1) maintaining an unordered candidate list with variable capacity, and (2) progressively reducing the search space with the best-so-far inner product value. The subroutine tryAppend (Line 10) decides whether the given point ùíå should be appended into candidate list or not. We next briefly present how Algorithm 1 achieves both above intuitions with the illustration example in Figure 7. To achieve (i), we set capacity threshold ùëô0. When the list capacity is lower than ùëô0, it explores all points without pruning (see Line 13). As shown in Figure 7(a), 2 is added to the list even though it is not critical. For (ii), after reaching the capacity threshold, it does not append the non-critical points to the list to reduce the search space. Figures 7(b) and (c) show that 3 is pruned and 7 is appended w.r.t. the current maximum inner product value, respectively."
        },
        {
            "title": "6.2 Query Optimizer in AlayaDB\nExcept for the traditional top-ùëò query and our novel proposed DIPR\nquery, we believe other auxiliary queries can be defined to achieve",
            "content": "8 sparse attention, i.e., retrieving subset of critical keys and values for high-quality generation. However, the processing performance of these queries significantly varies among different hardware settings and workload characteristics. Thus, it is crucial to provide query optimizer in AlayaDB, which assists the LLM application developer in choosing the best query type with its underlying index structure. In AlayaDB, we consider three query types (e.g., top-ùëò, DIPR and filter query) and three index types (e.g., coarse-grained index, fine-grained index, and flat index). Interestingly, both query and index types can be extended in AlayaDB for efficient and effective sparse attention. We next introduce the core idea of each index type and analyze their characteristics in Table 4. Coarse-grained index. It groups the adjacent tokens into blocks, where each block is represented by several vectors. It only computes the inner products between query and representative vectors during the retrieval and selects the critical blocks for attention computation. This kind of algorithms includes InfLLM [63], Quest [55] and PQCache [66]. These methods usually require large GPU memory to cache the blocks and they can provide very low latency for LLM inference. Fine-grained index. It builds the traditional vector search indexes on the key-level, e.g., indexing all key vectors by graph (a.k.a., graph indices). It quickly and accurately locates small number of critical tokens in the index, which can be efficiently computed on CPU. However, due to the expensive random memory access during index traversal, it can be slow when the number of used critical tokens is large, e.g., ùëò is large in top-ùëò queries. Flat index. It scans all the keys to find the critical tokens on CPU. Compared to fine-grained indices, it is less efficient when the number of critical tokens are small due to redundant scans. However, it can be more efficient when the number of critical tokens is large due to the sequential memory access. Inspired by the rule-based query optimizer in database systems, AlayaDB implements unified and extensible optimizer to select an optimized query plan (including specified query type and index type) for attention computation. The workflow of the rule-based query optimizer in AlayaDB is shown in Figure 8. It identifies the context length at first. Query to the short contexts will be processed directly with full attention. For the long contexts, if the context involves partial reuse, an attribute filtering predicate containing the length of the reused prefix is applied to the query, as we will introduce in Section 7.1. Then the optimizer identifies GPU memory budget, which is set to the available GPU memory by default and can be manually set by users. If the budget is enough, the query will be processed as top-ùëò queries with coarse-grained indices, i.e., AlayaDB Table 4: Characteristics of index types Index type Coarse Fine Flat Supported query type Top-ùëò, Filter Top-ùëò, Filter, DIPR Top-ùëò, Filter, DIPR GPU memory comsumption Large Small Small Latency small ùëò Low Low Latency large ùëò Low High Medium Medium Figure 8: Rule-based query optimizer in AlayaDB InfLLM [63] in AlayaDB. If the GPU memory budget is limited, the optimizer will choose DIPR query and select the index type based on the layer id. From production environments of LLM inference and experimental evaluation benchmarks (see Figure 5), we observed that the first layer requires large number of tokens to maintain the generation quality. Thus, the optimizer of AlayaDB chooses flat indices for the first layer and uses graph-based DIPRS for the other layers. It is still an open problem in optimizing the sparse attention with different query types and index types. However, query optimization is widely studied in our database community, we hope the researchers in our community can solve it together."
        },
        {
            "title": "7 Performance Optimization in AlayaDB\n7.1 Query Processing Optimization",
            "content": "Window Caching Enhanced DIPR. Window caching retains window of initial and last tokens during LLM inference, which is standard technique in existing sparse attention algorithms [29, 43, 45, 63, 65]. The intuition of window cache is that those tokens usually contribute large attention weights. AlayaDB adopts the window cache mechanism and caches the window in GPU memory. Interestingly, the cached window can be further utilized to further enhance the quality of DIPR query results. Recall that the core challenge of DIPR queries is to correctly identify the key vector with the maximum inner product value. Interestingly, our engineers observed that the key vector with maximum inner product value has large probability in the cached windows. For example, for dataset math_find on the model Llama-3-8B-Instruct-262k, window of 32 (initial) + 32 (last) tokens can cover almost 98% of the key vectors with the maximum inner product values. Motivated by this observation, we enhance DIPRS by taking the maximum inner product values in both the candidate list and the cached window into consideration. It improves the performance of DIPRS by reducing the number of unnecessary tokens explored. Flexible Context Reuse By Attribute Filtering. When new session containing full context is stored in AlayaDB, its vector index can be reused for efficient generation via sparse attention. However, when new session contains only partial prefix of stored context, the index cannot be reused. This is common case in practice. For example, stored context contains book and user As conversation, while the incoming session of user contains the same book but with new questions. The new session only reuses the book, which is partial prefix of the stored context. In these cases, the session has to either be processed with expensive full attention or wait until new index is built on the partial prefix. To address the limitations, AlayaDB supports flexible context reuse, which enables reusing the index of stored context for efficient LLM inference when only prefix of the stored context is reused. The challenge is to retrieve critical tokens only among the subset of tokens that are reused during searching within the full index. Interestingly, the problem can be transformed into well-studied problem in the database community called attribute filtering query by considering the token id as an attribute. The naive approach of attribute filtering is pruning those nodes that do not satisfy the attribute predicate. However, this approach severely disrupts the connectivity of the graph index structure and leads to significant decline in accuracy. We improve DIPRS with similar idea to [49] to solve this problem. During each node exploration, the algorithm traverses both its neighbors and its neighbors neighbors (2-hop neighbors). Subsequently, the candidates that do not meet the filtering predicate are excluded. This strategy enables AlayaDB to achieve broader search scope during the retrieval process, which enjoys high efficiency and good accuracy."
        },
        {
            "title": "7.2 Computation Optimization",
            "content": "Index Construction Acceleration. In AlayaDB, the index of long context is constructed when context is imported by DB.store() and DB.import(). Although this procedure is usually offline, e.g., the book is pre-loaded before the service is launched, the cost is still not negligible. We first analyze the overhead of index construction and then show how to optimize it. The fine-grained index used in AlayaDB is RoarGraph [28], state-of-the-art index for sparse attention [45] due to its ability to handle vector search on OutOf-Distribution (OOD) data. Following RetrievalAttention [45], RoarGraph is constructed for each query head, and the procedure can be divided into two stages: (i) ùíí to ùíå kNN construction, which constructs graph that links each query vector to its exact nearest key vectors, and (ii) connectivity enhancement, which links each vector to its approximate nearest vectors that are produced by an ANNS search on the graph. We observe that the overhead comes from the large number of indices and the slow kNN construction. We devise the following optimizations to reduce the overhead. GQA-based index sharing: GQA is commonly used in state-ofthe-art LLMs [56] to reduce KV cache size. It splits the ‚Ñéùëû query heads into ‚Ñéùëòùë£ groups, where ‚Ñéùëòùë£ is the number of key-value heads and ‚Ñéùëòùë£ < ‚Ñéùëû. Queries heads within the same group will query the same key head, making one copy of KV cache able to be shared among query group. In AlayaDB, we share RoarGraph among query group by sampling query vectors from each query head and merging them into one RoarGraph in the stage of kNN construction. In this way, the graph can still capture the distribution of all query heads while enjoying speedup of ‚Ñéùëû/‚Ñéùëòùë£ times by the reduction in the number of indices. Our experiments show that index sharing 9 only results in 3% loss in top-ùëò recall, and does not affect the generation quality of end-to-end LLM inference. GPU-based kNN construction: The kNN construction [58] can be highly parallel, making it suitable for GPU. We directly use the NVIDIA cuVS library [16] to accelerate its construction on GPU. To reduce the overhead of KV cache transfer, we process one layer at time, which is compute-bound, and overlap it with the asynchronous CPU-GPU transmission in pipeline manner. Late Materialization for Index Updating. For each session, there are new KV caches generated from user inputs and model outputs. It raises design choice about when to physically update them into the context. straightforward solution is inserting the new KV cache to the existing index immediately after new token is generated or input by users. However, it significantly (i) increases the TPOT by the blocked index updating, and (ii) occupies two memory copies by maintaining physical index for every session. To address this problem, AlayaDB adopts late-materialization strategy for index updating that does not affect the SLO. By default, the newly generated KV cache is appended to the local window for retrieval. The session will only be materialized into new physical index when the DB.store() API is explicitly called. This is based on two practical observations that the user prompt and LLM generation following the long context are (i) often short and (ii) often not reused across sessions. Therefore, there is no need to early materialize the newly generated KV cache to the physical index in most cases. Data-centric Attention Engine. AlayaDB is integrated with native attention engine, which is optimized with data-centric computation. Instead of computing attention after gathering the retrieved vectors [63, 66], AlayaDB directly applies attention to the vectors where they reside, and then aggregates the attention results. This data-centric mechanism can reduce the overhead of moving the large KV cache across different computing devices. For example, when most of the context is on CPU and window is cached on the GPU, partial attention of the two parts is computed independently in parallel and aggregated into final attention output. We use the same algorithm as FlashAttention [32] and RetrievalAttention [45] to compute and aggregate the partial attention outputs."
        },
        {
            "title": "7.3 Storage Optimization\nDuring LLM inference, AlayaDB retrieves a specific portion of\nvector data from each attention head of different attention layers to\ngenerate the next token. However, storing all the data in the limited\nCPU is not practical due to the large KV cache size. To efficiently\nmanage and reuse these vector data, we devise a vector file system\nand a purpose-built buffer manager within AlayaDB.",
            "content": "Vector File Systems. The vector file system in AlayaDB is built upon SPDK (Storage Performance Development Kit) to manage multiple vector files on disk in user space. Specifically, each vector file stores the vectors of an attention head in specific layer. These stored vectors are organized into blocks, where vector indices and vector data are stored separately in different types of blocks, and vector index blocks are linked together in graph structure. The benefits of our layout are two-fold: (i) the graph-based structure allows for quick traversal and access to related vectors, and (ii) the vector data can be inserted or deleted without the need for 10 Y. Deng, Z. You, L. Xiang, et al. restructuring the entire file. Furthermore, the system can bypass traditional kernel I/O paths by leveraging SPDK, which significantly reduces latency and improves throughput. Purpose-built Buffer Manager. AlayaDB has purpose-built buffer manager built upon the underlying vector file system, which is designed to efficiently process the frequently used data in memory. It employs the eviction strategy based on the corresponding block types. For example, blocks storing the vector indices for attention heads are more likely to be kept in memory, as these vectors are frequently accessed during inference. In contrast, blocks storing the vector data are only fetched once to calculate the attention score for each token. The specific designs of it minimize redundant I/O operations by avoiding the need to retrieve them from secondary storage repeatedly. Additionally, the buffer manager supports parallel access, enabling efficient processing in multi-threaded environment."
        },
        {
            "title": "8 Use Cases of AlayaDB\nAlayaDB provides easy-to-use interfaces and good performance for\nlong context management and inference. In this section, we present\ntwo LLM applications to demonstrate the use cases of AlayaDB.",
            "content": "Financial Document Analysis. AlayaDB can be used by financial companies to assist in their financial document analysis. These documents are long, including financial statements, audit reports, business plans, etc. Data analysts in the financial company leverage domain-specific LLMs with AlayaDB to analyze large number of financial documents and generate summarizations for their purposes, e.g., the top-10 news of Hong Kong stock market in 2024. The cost and latency of the document analysis service are reduced. Legal Assistant for Question Answering. Law firms can utilize AlayaDB to enhance their intelligent legal assistant service. The major difference between the legal assistant and other LLM applications is that answers to users questions must be precise and accurate, e.g., comply with the rules of the government. The legal documents can be stored as context in AlayaDB. Their domain-specific LLM answers user questions by the stored context to achieve low costs while guaranteeing result accuracy."
        },
        {
            "title": "9 Empirical Evaluation\nIn this section, we conduct experiments to evaluate the end-to-end\nperformance of AlayaDB in long-context LLM serving. In particular,\nwe aim to answer the following two questions:",
            "content": "Q1: Can AlayaDB achieve low latency, high quality, and low resource consumption at the same time for long-context LLM serving? (Section 9.1) Q2: How is the effectiveness of our proposed performance optimizations in AlayaDB? (Section 9.2) Hardware Configuration. We conduct our experiments on server with one NVIDIA L20 GPU (48GB memory) and two Intel XEON GOLD 6542Y CPUs with 48 cores, 96 threads and 512 GB DRAM in total. We use AlayaDB together with HuggingFace Transformers [61] to support LLM inference. We use the bfloat16 version of Llama-3-8B-Instruct-262k [12], the long context variant of state-of-the-art LLM model Llama [56] for inference. The model AlayaDB Table 5: Generation quality of different sparse attention algorithms in -Bench. Each method used the number of [initial+last]+retrieved tokens for attention computation. Methods Full Attention InfLLM StreamingLLM Top100 Top2000 DIPRS Setting [128+4K]+4K tokens [128]+8K tokens [128+512]+100 tokens [128+512]+2K tokens [128+512] tokens, ùõΩ = 50 SLO Retr.KV Retr.P Retr.N Code.D En.MC En.QA En.Sum Math.F Avg. 45.6 43.8 16.9 45.3 46.7 47.0 100.0 100.0 8.5 100.0 100.0 100.0 100.0 100.0 8.5 100.0 100.0 100. 15.1 15.3 14.3 15.2 16.0 16.4 27.4 28.2 27.7 30.0 29.7 30.7 55.9 39.7 41.5 56.3 58.1 58.1 19.1 23.4 16.3 24.6 24.3 24.9 31.0 18.7 14.5 29.7 31.2 32.1 15.8 25.0 3.8 6.6 14.6 14. has 32 layers. Each layer includes 32 query heads and 8 key value heads. Its weights occupy 15.4 GB GPU memory during inference."
        },
        {
            "title": "9.1 End-to-end Performance Evaluation\n9.1.1 TPOT, Quality and GPU Memory Consumption. We compare\nour proposed DIPR query (see Section 6.1) with existing sparse\nattention algorithms and full attention algorithm w.r.t. Time-Per-\nOutput-Token (a.k.a., TPOT, the inference latency per token gen-\neration), quality, and GPU memory consumption in various LLM\ninference workloads.",
            "content": "Tested Workloads. We adopt widely-used long-context benchmark -Bench [67] for overall performance evaluation. Specifically, we use 8 tasks in -Bench including Retr.KV, Retr.P, Retr.N, Code.D, En.MC, En.QA, En.Sum, Math.F. The average input context length of different tasks ranges from 43.9K to 192.6K tokens. In the experiments, the index of the input context is built in advance and we only measure the latency of each token generation (TPOT). We set the SLO of TPOT 0.24s, which is the reading speed of human [70]. Compared Methods. We compare the following methods: Full Attention, which stores the KV cache of full context and computes the full attention on GPU. InfLLM [63], it is coarse-grained algorithm which selects critical tokens in blocks and computes their attention on GPU. StreamingLLM [65], it is an algorithm that keeps window of tokens in GPU memory for attention computation and simply drops the other tokens. Top-ùëò, it is fine-grained algorithm which processes the top-ùëò similarity search with graph-based index on CPU. We follow the RetrievalAttention [45] to use RoarGraph [28] as the index and align the window size. In particular, the parameter ùëò is set as 100 and 2000 to study the performance of difference retrieved critical tokens in our experiments. DIPRS, our proposed DIPR query processing algorithm for sparse attention. It also uses RoarGraph as the index. The window size of DIPRS is the same as that of the top-ùëò query. Result Analysis. Table 5 shows the generation quality of different methods in all 8 tasks of -Bench. The quality score is measured by -Bench. First of all, our proposed DIPRS not only guarantees the SLO, but also achieves the best average generation quality among all the compared methods, as the last column in Table 5 shows. Moreover, it is the overall winner in 7 tasks out of the 8 tested tasks. For full attention, the SLO of TPOT is violated due to the expensive (1) EN.MC (2) EN.QA Figure 9: Generation quality and GPU memory consumption with SLO guarantees O(ùëõ) computation cost even with the KV cache in GPU memory. Compared with full attention, DIPRS can achieve near or even higher quality in all tasks. The result also confirms the superiority of DIPRS against traditional top-ùëò query. Top-ùëò requires retrieving 2000 tokens to achieve similar quality to DIPRS, but fails to meet the SLO because of retrieving too many tokens. The generation quality of top-ùëò = 100 is worse than DIPRS in 6 tasks. In Retr.P and Retr.N, both DIPRS and top-ùëò = 100 have the same performance. To answer Q1, we perform in-depth analysis on two tasks (i.e., EN.MC and EN.QA) w.r.t. the generation quality and GPU memory consumption with user specified SLO. We vary the number of cached tokens for InfLLM and StreamingLLM to investigate the relationship of generation quality and GPU memory consumption. For top-ùëò = 100 and DIPRS, we use the same settings in Table 5. As Figure 9 shows, compared to all the other methods, DIPRS achieves the best generation quality and lowest GPU memory consumption while guaranteeing the SLO of TPOT. Regarding the coarse-grained methods InfLLM and StreamingLLM, large GPU memory is required to achieve higher accuracy, which limits the throughput of online serving and makes them impractical to run on the consumergrade GPU, e.g., NVIDIA GTX4090 (24GB memory). Compared to top-ùëò, the generation quality of DIPRS surpasses top-ùëò due to its ability to identify the dynamic number of critical tokens for efficient sparse attention. 9.1.2 Time-To-First-Token: TTFT. We compare AlayaDB with the state-of-the-art disaggregated KV cache service LMCache [15, 46] to evaluate its ability to reduce the TTFT by efficiently reusing the stored long context in Figure 10. LMCache stores the compressed KV cache of the full context, and supports context reusing by loading the KV cache into GPU. In this experiment, we store the context in CPU memory in advance and measure the time of decoding the first token on this offloaded context as TTFT. Figure 10(a) depicts 11 Y. Deng, Z. You, L. Xiang, et al. (a) TTFT (b) Latency breakdown Figure 10: TTFT of long context reusing (a) Construction time (b) Memory consumption Figure 11: Index construction optimization the experimental results. Firstly, reusing the KV cache is faster than recomputing the expensive prefill stage without reuse. For example, our AlayaDB outperforms the w/o reuse by 2 to 3 orders of magnitude, see the red and black curves in Figure 10(a). Secondly, the TTFT of AlayaDB is 19 to 42 times faster than LMCache, see the red and blue curves in Figure 10(a). By analyzing the breakdown of latency of LMCache and AlayaDB in Figure 10(b), LMCache suffers from the slow KV cache loading, including decompressing and transferring from CPU to GPU. The loading time increases linearly with the context length. Instead of loading the KV cache, AlayaDB can directly decode on the offloaded KV cache with an extremely low latency, thus, resulting to low TTFT for context reuse. This experiment also confirms the analyzed limitation of the existing KV cache disaggregation architecture. In other words, decoupling both attention computation and KV cache from the LLM inference engine as our proposal AlayaDB is new opportunity for our community to develop fast and accurate LLM inference systems, which provides huge optimization space."
        },
        {
            "title": "9.2 Effectiveness of Optimization Techniques\nIndex construction. We conduct an ablation study of our pro-\n9.2.1\nposed optimizations for the RoarGraph construction in Section 7.2.\nIn particular, we set the ratio of sampled queries for each index\nto 40%, which means when building an index, the number of used\nquery vectors is 40% of the key vectors. Figure 11(a) shows the in-\ndex construction time under different context lengths. The baseline\nmethod follows RetrievalAttention, which builds the index on CPU\nand builds one index for each query head, see the black curve. Intro-\nducing GPU to build kNN and employing CPU-GPU pipeline can\ngain a speedup from 3√ó to 15√ó, see the blue curve. Then, by sharing\nthe index in the same query group, index construction time can be\nfurther reduced from 12√ó to 62√ó compared to pure CPU baseline,\nas the red curve shows. Moreover, index sharing also significantly\nreduces memory consumption by reducing the number of indexes.\nAs depicted in Figure 11(b), the index size can be 4√ó smaller than\nthe GPU and CPU baseline without index sharing.",
            "content": "12 Figure 12: Micro-benchmark of filter-based DIPRS Filter-based DIPRS. In Section 7.1, we introduce that AlayaDB 9.2.2 leverages the attribute filtering with DIPRS algorithm to support partial context reuse. In this experiment, we study the effect of this optimization to the generation quality and inference latency. In particular, we conduct micro-benchmark to evaluate the recall and latency of filter-based DIPRS search in the case of partial context reuse. We fix the reused prefix length to 40K, and range the reuse ratio from 100% to 20% by varying the length of the stored context, i.e., the index size. This micro-benchmark uses the KV cache generated by all heads in layer 1 during the En.QA task. The 100% reuse ratio means the stored context is fully reused, in other words, the filter-based DIPRS is the same as the original DIPRS without attribute filtering. Figure 12 shows the measured recall and latency. Firstly, the recall of filter-based DIPRS remains high with different reuse ratios, which guarantees the generation quality with partial context reuse in AlayaDB. Secondly, when searching in larger context with the same prefix length, the latency of filterbased DIPRS increases only slightly . For example, the latency to search in 200K long context is only 1.13 ms higher than it is of 40K long context. Thus, AlayaDB guarantees the inference latency with good generation quality when partial context reuse is enabled."
        },
        {
            "title": "10 Conclusion\nAt AlayaDB.AI, we built AlayaDB for efficient and effective long-\ncontext inference in LLM era. From the architecture perspective,\nAlayaDB decouples the KV cache and attention computation from\nthe LLM inference systems, and encapsulates them into a novel\nvector database system. It optimizes the overall performance by\nco-optimizing attention computation and KV cache management\nin a monolithic manner. Collaborating with the inference engine,\nAlayaDB is able to guarantee the SLO while enjoying low resource\nconsumption and high generation quality for long-context LLM\ninference. The novel architecture poses new challenges and oppor-\ntunities, including (i) implementing different parallelism strategies\nto enable distributed inference, (ii) supporting more LLM inference\nengines like vLLM and SGLang, (iii) improving the query processing\nmethods (or sparse attention algorithms) and query optimizer, (iv)\nleveraging various storage tiers to store the KV cache of contexts,\n(v) utilizing heterogeneous hardware to accelerate the attention\ncomputation, and (vi) designing attention-hybrid architecture for\ngeneral-purpose vector databases. We hope the researchers from\ndifferent communities (e.g., database, machine learning, system)\ncould tackle them together in the future.",
            "content": "AlayaDB References [1] 2024. AlloyDB AI. https://cloud.google.com/alloydb/ai [2] 2024. Amazon Kendra. https://aws.amazon.com/cn/kendra [3] 2024. Bing. https://www.microsoft.com/en-us/bing/apis/llm [4] 2024. ChatGPT. https://chatgpt.com [5] 2024. Cursor. https://www.cursor.com [6] 2024. Deepseek. https://chat.deepseek.com [7] 2024. Deepseek Coder. https://chat.deepseek.com/coder [8] 2024. Explainpaper. https://www.explainpaper.com/ [9] 2024. Gemini. https://gemini.google.com [10] 2024. Generative AI in Search: Let Google do the searching for you. https://blog. google/products/search/generative-ai-google-search-may-2024 [11] 2024. Github Copilot. https://github.com/features/copilot [12] 2024. Gradient AI. Llama-3-8b-instruct-262k. https://huggingface.co/gradientai/ Llama-3-8B-Instruct-262k [13] 2024. Gradient AI. Llama-3-8B-Instruct-Gradient-1048k. https://huggingface.co/ gradientai/Llama-3-8B-Instruct-Gradient-1048k [14] 2024. Kimi. https://kimi.moonshot.cn [15] 2024. LMCache. https://lmcache.ai/ [16] 2024. NVIDIA cuVS. https://github.com/rapidsai/cuvs [17] 2024. Perplexity AI. https://www.perplexity.ai [18] 2024. Pinecone. http://pinecone.io [19] 2024. weaviate: The AI-native database for new generation of software. http: //weaviate.io [20] 01. AI, :, Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge Zhang, Guanwei Zhang, Guoyin Wang, Heng Li, Jiangcheng Zhu, Jianqun Chen, Jing Chang, Kaidong Yu, Peng Liu, Qiang Liu, Shawn Yue, Senbin Yang, Shiming Yang, Wen Xie, Wenhao Huang, Xiaohui Hu, Xiaoyi Ren, Xinyao Niu, Pengcheng Nie, Yanpeng Li, Yuchi Xu, Yudong Liu, Yue Wang, Yuxuan Cai, Zhenyu Gu, Zhiyuan Liu, and Zonghong Dai. 2025. Yi: Open Foundation Models by 01.AI. arXiv:2403.04652 [cs.CL] [21] Morton M. Astrahan, Mike W. Blasgen, Donald D. Chamberlin, Kapali P. Eswaran, Jim Gray, Patricia P. Griffiths, W. Frank King III, Raymond A. Lorie, Paul R. McJones, James W. Mehl, Gianfranco R. Putzolu, Irving L. Traiger, Bradford W. Wade, and Vera Watson. 1976. System R: Relational Approach to Database Management. TODS 1, 2 (1976), 97137. [22] Saleem Ayesha. 2023. LLM for Lawyers, Enrich Your Precedents with the Use of AI. In Data Science Dojo. https://datasciencedojo.com/blog/llm-for-lawyers/ [23] Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li. 2024. LongBench: Bilingual, Multitask Benchmark for Long Context Understanding. In ACL. 31193137. [24] Zheng Bian, Xiao Yan, Jiahao Zhang, Man Lung Yiu, and Bo Tang. 2024. QSRP: Efficient Reverse k-Ranks Query Processing on High-Dimensional Embeddings. In ICDE. 46144627. [25] Zheng Cai, Maosong Cao, Haojiong Chen, Kai Chen, Keyu Chen, Xin Chen, Xun Chen, Zehui Chen, Zhi Chen, Pei Chu, Xiaoyi Dong, Haodong Duan, Qi Fan, Zhaoye Fei, Yang Gao, Jiaye Ge, Chenya Gu, Yuzhe Gu, Tao Gui, Aijia Guo, Qipeng Guo, Conghui He, Yingfan Hu, Ting Huang, Tao Jiang, Penglong Jiao, Zhenjiang Jin, Zhikai Lei, Jiaxing Li, Jingwen Li, Linyang Li, Shuaibin Li, Wei Li, Yining Li, Hongwei Liu, Jiangning Liu, Jiawei Hong, Kaiwen Liu, Kuikun Liu, Xiaoran Liu, Chengqi Lv, Haijun Lv, Kai Lv, Li Ma, Runyuan Ma, Zerun Ma, Wenchang Ning, Linke Ouyang, Jiantao Qiu, Yuan Qu, Fukai Shang, Yunfan Shao, Demin Song, Zifan Song, Zhihao Sui, Peng Sun, Yu Sun, Huanze Tang, Bin Wang, Guoteng Wang, Jiaqi Wang, Jiayu Wang, Rui Wang, Yudong Wang, Ziyi Wang, Xingjian Wei, Qizhen Weng, Fan Wu, Yingtong Xiong, Chao Xu, Ruiliang Xu, Hang Yan, Yirong Yan, Xiaogui Yang, Haochen Ye, Huaiyuan Ying, Jia Yu, Jing Yu, Yuhang Zang, Chuyu Zhang, Li Zhang, Pan Zhang, Peng Zhang, Ruijie Zhang, Shuo Zhang, Songyang Zhang, Wenjian Zhang, Wenwei Zhang, Xingcheng Zhang, Xinyue Zhang, Hui Zhao, Qian Zhao, Xiaomeng Zhao, Fengzhe Zhou, Zaida Zhou, Jingming Zhuo, Yicheng Zou, Xipeng Qiu, Yu Qiao, and Dahua Lin. 2024. InternLM2 Technical Report. arXiv:2403.17297 [cs.CL] [26] Zefan Cai, Yichi Zhang, Bofei Gao, Yuliang Liu, Tianyu Liu, Keming Lu, Wayne Xiong, Yue Dong, Baobao Chang, Junjie Hu, and Wen Xiao. 2024. PyramidKV: Dynamic KV Cache Compression based on Pyramidal Information Funneling. arXiv:2406.02069 [cs.CL] [27] Cheng Chen, Chenzhe Jin, Yunan Zhang, Sasha Podolsky, Chun Wu, SzuPo Wang, Eric Hanson, Zhou Sun, Robert Walzer, and Jianguo Wang. 2024. SingleStore-V: An Integrated Vector Database System in SingleStore. Proc. VLDB Endow. 17, 12 (2024), 37723785. [28] Meng Chen, Kai Zhang, Zhenying He, Yinan Jing, and X. Sean Wang. 2024. RoarGraph: Projected Bipartite Graph for Efficient Cross-Modal Approximate Nearest Neighbor Search. Proc. VLDB Endow. 17, 11 (2024), 27352749. [29] Zhuoming Chen, Ranajoy Sadhukhan, Zihao Ye, Yang Zhou, Jianyu Zhang, Niklas Nolte, Yuandong Tian, Matthijs Douze, Leon Bottou, Zhihao Jia, and Beidi Chen. 2024. MagicPIG: LSH Sampling for Efficient LLM Generation. arXiv:2410.16179 [cs.CL] [30] Benoit Dageville, Thierry Cruanes, Marcin Zukowski, Vadim Antonov, Artin Avanes, Jon Bock, Jonathan Claybaugh, Daniel Engovatov, Martin Hentschel, Jiansheng Huang, Allison W. Lee, Ashish Motivala, Abdul Q. Munir, Steven Pelley, Peter Povinec, Greg Rahn, Spyridon Triantafyllis, and Philipp Unterbrunner. 2016. The Snowflake Elastic Data Warehouse. In SIGMOD. 215226. [31] Tri Dao. 2024. FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning. In ICLR. [32] Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher R√©. 2022. FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness. In NIPS. [33] Gunika Dhingra. 2023. LLMs in Finance: BloombergGPT and FinGPT What You Need to Know. https://12gunika.medium.com/llms-in-finance-bloomberggptand-fingpt-what-you-need-to-know-2fdf3af [34] Matthijs Douze, Alexandr Guzhva, Chengqi Deng, Jeff Johnson, Gergely Szilvasy, Pierre-Emmanuel Mazar√©, Maria Lomeli, Lucas Hosseini, and Herv√© J√©gou. 2025. The Faiss library. arXiv:2401.08281 [cs.LG] [35] Zhiwei Fei, Xiaoyu Shen, Dawei Zhu, Fengzhe Zhou, Zhuo Han, Alan Huang, Songyang Zhang, Kai Chen, Zhixin Yin, Zongwen Shen, Jidong Ge, and Vincent Ng. 2024. LawBench: Benchmarking Legal Knowledge of Large Language Models. In EMNLP. 79337962. [36] Yuan Feng, Junlin Lv, Yukun Cao, Xike Xie, and S. Kevin Zhou. 2025. Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for Efficient LLM Inference. arXiv:2407.11550 [cs.CL] [37] Cong Fu, Chao Xiang, Changxu Wang, and Deng Cai. 2019. Fast Approximate Nearest Neighbor Search With The Navigating Spreading-out Graph. Proc. VLDB Endow. 12, 5 (2019), 461474. [38] Wensheng Gan, Shicheng Wan, and Philip S. Yu. 2023. Model-as-a-Service (MaaS): Survey. In BigData. 46364645. [39] Rentong Guo, Xiaofan Luan, Long Xiang, Xiao Yan, Xiaomeng Yi, Jigao Luo, Qianya Cheng, Weizhi Xu, Jiarui Luo, Frank Liu, Zhenshan Cao, Yanliang Qiao, Ting Wang, Bo Tang, and Charles Xie. 2022. Manu: Cloud Native Vector Database Management System. Proc. VLDB Endow. 15, 12 (2022), 35483561. [40] Dongxu Huang, Qi Liu, Qiu Cui, Zhuhe Fang, Xiaoyu Ma, Fei Xu, Li Shen, Liu Tang, Yuxing Zhou, Menglong Huang, Wan Wei, Cong Liu, Jian Zhang, Jianjun Li, Xuelian Wu, Lingyu Song, Ruoxi Sun, Shuaipeng Yu, Lei Zhao, Nicholas Cameron, Liquan Pei, and Xin Tang. 2020. TiDB: Raft-based HTAP Database. Proc. VLDB Endow. 13, 12 (2020), 30723084. [41] Binyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang, Bowen Yu, Keming Lu, Kai Dang, Yang Fan, Yichang Zhang, An Yang, Rui Men, Fei Huang, Bo Zheng, Yibo Miao, Shanghaoran Quan, Yunlong Feng, Xingzhang Ren, Xuancheng Ren, Jingren Zhou, and Junyang Lin. 2024. Qwen2.5-Coder Technical Report. arXiv:2409.12186 [cs.CL] [42] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. 2023. Efficient Memory Management for Large Language Model Serving with PagedAttention. In SOSP. [43] Yuhong Li, Yingbing Huang, Bowen Yang, Bharat Venkitesh, Acyr Locatelli, Hanchen Ye, Tianle Cai, Patrick Lewis, and Deming Chen. 2024. SnapKV: LLM Knows What You are Looking for Before Generation. In NIPS. [44] Yiming Lin, Madelon Hulsebos, Ruiying Ma, Shreya Shankar, Sepanta Zeigham, Aditya G. Parameswaran, and Eugene Wu. 2024. Towards Accurate and Efficient Document Analytics with Large Language Models. arXiv:2405.04674 [cs.DB] [45] Di Liu, Meng Chen, Baotong Lu, Huiqiang Jiang, Zhenhua Han, Qianxi Zhang, Qi Chen, Chengruidong Zhang, Bailu Ding, Kai Zhang, Chen Chen, Fan Yang, Yuqing Yang, and Lili Qiu. 2024. RetrievalAttention: Accelerating Long-Context LLM Inference via Vector Retrieval. arXiv:2409.10516 [cs.LG] [46] Yuhan Liu, Hanchen Li, Yihua Cheng, Siddhant Ray, Yuyang Huang, Qizheng Zhang, Kuntai Du, Jiayi Yao, Shan Lu, Ganesh Ananthanarayanan, Michael Maire, Henry Hoffmann, Ari Holtzman, and Junchen Jiang. 2024. CacheGen: KV Cache Compression and Streaming for Fast Large Language Model Serving. In SIGCOMM. 3856. [47] Zhenghua Lyu, Huan Hubert Zhang, Gang Xiong, Gang Guo, Haozhou Wang, Jinbao Chen, Asim Praveen, Yu Yang, Xiaoming Gao, Alexandra Wang, Wen Lin, Ashwin Agrawal, Junfeng Yang, Hao Wu, Xiaoliang Li, Feng Guo, Jiang Wu, Jesse Zhang, and Venkatesh Raghavan. 2021. Greenplum: Hybrid Database for Transactional and Analytical Workloads. In SIGMOD. 25302542. [48] Yu Malkov and Dmitry Yashunin. 2018. Efficient and robust approximate nearest neighbor search using hierarchical navigable small world graphs. In IEEE transactions on pattern analysis and machine intelligence. 824836. [49] Liana Patel, Peter Kraft, Carlos Guestrin, and Matei Zaharia. 2024. ACORN: Performant and Predicate-Agnostic Search Over Vector Embeddings and Structured Data. Proc. ACM Manag. Data 2, 3 (2024), 120. [50] Sundar Pichai and Demis Hassabis. 2024. Our next-generation model: Gemini 1.5. https://blog.google/technology/ai/google-gemini-next-generation-modelfebruary-2024/#context-window [51] Ruoyu Qin, Zheming Li, Weiran He, Mingxing Zhang, Yongwei Wu, Weimin Zheng, and Xinran Xu. 2024. Mooncake: KVCache-centric Disaggregated Architecture for LLM Serving. arXiv:2407.00079 [cs.DC] 13 Y. Deng, Z. You, L. Xiang, et al. Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. 2020. Transformers: State-of-the-Art Natural Language Processing. In EMNLP Demos. 3845. [62] Long Xiang, Xiao Yan, Lan Lu, and Bo Tang. 2021. GAIPS: Accelerating maximum inner product search with GPU. In SIGIR. 19201924. [63] Chaojun Xiao, Pengle Zhang, Xu Han, Guangxuan Xiao, Yankai Lin, Zhengyan InfLLM: Training-Free LongZhang, Zhiyuan Liu, and Maosong Sun. 2024. Context Extrapolation for LLMs with an Efficient Context Memory. In NIPS. [64] Guangxuan Xiao, Jiaming Tang, Jingwei Zuo, Junxian Guo, Shang Yang, Haotian Tang, Yao Fu, and Song Han. 2024. DuoAttention: Efficient Long-Context LLM Inference with Retrieval and Streaming Heads. arXiv:2410.10819 [cs.CL] [65] Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. 2024. Efficient Streaming Language Models with Attention Sinks. In ICLR. [66] Hailin Zhang, Xiaodong Ji, Yilin Chen, Fangcheng Fu, Xupeng Miao, Xiaonan Nie, Weipeng Chen, and Bin Cui. 2025. PQCache: Product Quantization-based KVCache for Long Context LLM Inference. arXiv:2407.12820 [cs.CL] [67] Xinrong Zhang, Yingfa Chen, Shengding Hu, Zihang Xu, Junhao Chen, Moo Hao, Xu Han, Zhen Thai, Shuo Wang, Zhiyuan Liu, and Maosong Sun. 2024. Bench: Extending Long Context Evaluation Beyond 100K Tokens. In ACL. 1526215277. [68] Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin Zheng, Ruisi Cai, Zhao Song, Yuandong Tian, Christopher R√©, Clark W. Barrett, Zhangyang Wang, and Beidi Chen. 2023. H2O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models. In NIPS. [69] Lianmin Zheng, Liangsheng Yin, Zhiqiang Xie, Chuyue Sun, Jeff Huang, Cody Hao Yu, Shiyi Cao, Christos Kozyrakis, Ion Stoica, Joseph E. Gonzalez, Clark W. Barrett, and Ying Sheng. 2024. SGLang: Efficient Execution of Structured Language Model Programs. In NIPS. [70] Yinmin Zhong, Shengyu Liu, Junda Chen, Jianbo Hu, Yibo Zhu, Xuanzhe Liu, Xin Jin, and Hao Zhang. 2024. DistServe: Disaggregating Prefill and Decoding for Goodput-optimized Large Language Model Serving. In OSDI. 193210. [52] Avi Silberschatz, Henry F. Korth, and S. Sudarshan. 2020. Database System Concepts, Seventh Edition. [53] Michael Stonebraker and Lawrence A. Rowe. 1986. The Design of Postgres. In SIGMOD. 340355. [55] [54] Suhas Jayaram Subramanya, Devvrit, Rohan Kadekodi, Ravishankar Krishaswamy, and Harsha Vardhan Simhadri. 2019. DiskANN: fast accurate billionpoint nearest neighbor search on single node. In NIPS. Jiaming Tang, Yilong Zhao, Kan Zhu, Guangxuan Xiao, Baris Kasikci, and Song Han. 2024. QUEST: Query-Aware Sparsity for Efficient Long-Context LLM Inference. In ICML. [56] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth√©e Lacroix, Baptiste Rozi√®re, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. 2023. LLaMA: Open and Efficient Foundation Language Models. arXiv:2302.13971 [cs.CL] [57] Alexandre Verbitski, Anurag Gupta, Debanjan Saha, Murali Brahmadesam, Kamal Gupta, Raman Mittal, Sailesh Krishnamurthy, Sandor Maurice, Tengiz Kharatishvili, and Xiaofeng Bao. 2017. Amazon Aurora: Design Considerations for High Throughput Cloud-Native Relational Databases. In SIGMOD. 10411052. [58] Hui Wang, Wan-Lei Zhao, Xiangxiang Zeng, and Jianye Yang. 2021. Fast k-NN [59] Graph Construction by GPU based NN-Descent. In CIKM. 19291938. Jianguo Wang, Xiaomeng Yi, Rentong Guo, Hai Jin, Peng Xu, Shengjun Li, Xiangyu Wang, Xiangzhou Guo, Chengming Li, Xiaohai Xu, Kun Yu, Yuxing Yuan, Yinghao Zou, Jiquan Long, Yudong Cai, Zhenxiang Li, Zhifeng Zhang, Yihua Mo, Jun Gu, Ruiyi Jiang, Yi Wei, and Charles Xie. 2021. Milvus: Purpose-Built Vector Data Management System. In SIGMOD. 26142627. [60] Chuangxian Wei, Bin Wu, Sheng Wang, Renjie Lou, Chaoqun Zhan, Feifei Li, and Yuanzhe Cai. 2020. AnalyticDB-V: Hybrid Analytical Engine Towards Query Fusion for Structured and Unstructured Data. Proc. VLDB Endow. 13, 12 (2020), 31523165. [61] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, R√©mi Louf, Morgan Funtowicz,"
        }
    ],
    "affiliations": [
        "AlayaDB AI"
    ]
}