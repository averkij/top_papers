{
    "paper_title": "Position: Mechanistic Interpretability Should Prioritize Feature Consistency in SAEs",
    "authors": [
        "Xiangchen Song",
        "Aashiq Muhamed",
        "Yujia Zheng",
        "Lingjing Kong",
        "Zeyu Tang",
        "Mona T. Diab",
        "Virginia Smith",
        "Kun Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Sparse Autoencoders (SAEs) are a prominent tool in mechanistic interpretability (MI) for decomposing neural network activations into interpretable features. However, the aspiration to identify a canonical set of features is challenged by the observed inconsistency of learned SAE features across different training runs, undermining the reliability and efficiency of MI research. This position paper argues that mechanistic interpretability should prioritize feature consistency in SAEs -- the reliable convergence to equivalent feature sets across independent runs. We propose using the Pairwise Dictionary Mean Correlation Coefficient (PW-MCC) as a practical metric to operationalize consistency and demonstrate that high levels are achievable (0.80 for TopK SAEs on LLM activations) with appropriate architectural choices. Our contributions include detailing the benefits of prioritizing consistency; providing theoretical grounding and synthetic validation using a model organism, which verifies PW-MCC as a reliable proxy for ground-truth recovery; and extending these findings to real-world LLM data, where high feature consistency strongly correlates with the semantic similarity of learned feature explanations. We call for a community-wide shift towards systematically measuring feature consistency to foster robust cumulative progress in MI."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 2 ] . [ 1 4 5 2 0 2 . 5 0 5 2 : r Position: Mechanistic Interpretability Should Prioritize Feature Consistency in SAEs Xiangchen Song Carnegie Mellon University Aashiq Muhamed Carnegie Mellon University Yujia Zheng Carnegie Mellon University Lingjing Kong Carnegie Mellon University Zeyu Tang Carnegie Mellon University Mona T. Diab Carnegie Mellon University Virginia Smith Carnegie Mellon University Kun Zhang Carnegie Mellon University & MBZUAI"
        },
        {
            "title": "Abstract",
            "content": "Sparse Autoencoders (SAEs) are prominent tool in mechanistic interpretability (MI) for decomposing neural network activations into interpretable features. However, the aspiration to identify canonical set of features is challenged by the observed inconsistency of learned SAE features across different training runs, undermining the reliability and efficiency of MI research. This position paper argues that mechanistic interpretability should prioritize feature consistency in SAEsthe reliable convergence to equivalent feature sets across independent runs. We propose using the Pairwise Dictionary Mean Correlation Coefficient (PW-MCC) as practical metric to operationalize consistency and demonstrate that high levels are achievable (0.80 for TopK SAEs on LLM activations) with appropriate architectural choices. Our contributions include detailing the benefits of prioritizing consistency; providing theoretical grounding and synthetic validation using model organism, which verifies PW-MCC as reliable proxy for ground-truth recovery; and extending these findings to real-world LLM data, where high feature consistency strongly correlates with the semantic similarity of learned feature explanations. We call for community-wide shift towards systematically measuring feature consistency to foster robust cumulative progress in MI."
        },
        {
            "title": "Introduction",
            "content": "Mechanistic Interpretability (MI) seeks to reverse-engineer neural networks into humanunderstandable algorithms [40, 14], with Sparse Autoencoders (SAEs) emerging as prominent tool for decomposing model activations into more interpretable, monosemantic features [6, 11, 19, 43]. The aspiration within the MI community is often to identify canonical set of featuresunique, complete, and atomic units of analysis that faithfully represent the models internal computations [28]. However, significant challenge highlighted by recent work [44, 32, 15, 28, 36] is the observed inconsistency of features learned by SAEs across different training runs, even when using identical data and model architectures. This instability, potentially arising from phenomena like feature splitting [28, 10] or the amortization gap [42], undermines the reliability of derived interpretations, reduces research efficiency, and impacts the trust in findings derived from MI. Equal contribution. Correspondence to xiangchensong@cmu.edu and amuhamed@andrew.cmu.edu. 2Code is available at https://github.com/xiangchensong/sae-feature-consistency. Preprint. Under review. Mechanistic Interpretability Should Prioritize Feature Consistency in SAEs. We argue that the reliable convergence to equivalent feature sets across independent SAE training runs should be elevated from secondary concern to an essential evaluation criterion and an active research priority. We present the Pairwise Dictionary Mean Correlation Coefficient (PW-MCC) as concrete, working example of how consistency can be operationalized. Furthermore, we demonstrate that high levels of consistency are achievable with appropriate architectural and training choices (e.g., with TopK SAEs), and highlight the benefits of such prioritization for scientific rigor and practical utility in MI. Our main contributions are: We advocate for prioritizing feature consistency in MI for SAEs, detailing its benefits for scientific reproducibility, research efficiency, and trustworthiness of interpretations. We propose using PW-MCC as practical metric for operationalizing run-to-run feature consistency (Section 3). We provide theoretical grounding for achieving strong feature consistency by connecting SAEs to established identifiability results in overcomplete sparse dictionary learning. We validate this using synthetic model organism, demonstrating that PW-MCC reliably tracks ground-truth feature recovery (GT-MCC) and that specific SAE architecture (TopK SAE) achieves high consistency ( 0.97) under idealized, matched-capacity conditions (Section 4). We demonstrate empirically on large language model activations that high feature consistency (PW-MCC 0.80 for TopK SAEs) is attainable with appropriate architectural choices and training. Our real-world data experiments reflect findings from the synthetic setting (e.g., frequency-dependent consistency) and critically show that high PW-MCC scores correlate strongly with the semantic similarity of feature explanations (Section 5). Ultimately, this work calls for community-wide shift towards valuing and systematically measuring feature consistency for cumulative progress in understanding the inner workings of complex neural models, and we outline several open questions and future research directions to achieve this."
        },
        {
            "title": "2 Background and Related Work",
            "content": "Sparse Autoencoders for MI. MI aims to reverse-engineer neural networks into humanunderstandable algorithms by identifying and explicating their internal components and computational processes [40, 43, 11]. central challenge in MI is polysemanticity, where individual neurons respond to multiple, unrelated concepts, which obscures straightforward interpretation of model internals [13]. SAEs have emerged as tool to address this challenge by decomposing high-dimensional neural network activations into sparser, higher-dimensional representation that aims to isolate monosemantic features [6, 11]. An SAE comprises an encoder and decoder network. The encoder transforms an input activation vector Rm into sparse latent representation Rdsae (where dsae > establishes an overcomplete dictionary): (x) = σ(Wencx + benc). Here, Wenc and benc are the encoder weights and biases, while σ is non-linear activation function that is sparsity-inducing. The decoder reconstructs the input from (x) as ˆx = Wdecf (x) + bdec, where Wdec and bdec are the decoder weights and biases. SAEs are trained by minimizing loss function that balances reconstruction fidelity with sparsity: L(x) = ˆx2 2 + λS(f (x)), where S() represents sparsity-inducing penalty (e.g., L1 norm) and λ controls the sparsity trade-off. Once trained, SAEs provides decomposition of the input activation as (cid:80)dsae i=1 fi(x)ai, where fi(x) are the sparse feature activations, and ai are dictionary elements corresponding to columns of feature dictionary (i.e., Wdec). Several SAE variants have been proposed to improve feature quality and sparsity control. These include Standard ReLU [6], TopK [18], BatchTopK [7], Gated [46], and JumpReLU [47]. The ultimate aspiration for many researchers employing SAEs is to identify canonical set of features: unique, complete, and atomic units of analysis that faithfully represent the models internal computations [28]. The Challenge of Feature Consistency. Despite their promise, SAEs trained on identical data and architectures but different random initializations often converge to substantially different feature sets [32, 26], with overlap sometimes as low as 30% for Standard SAEs [44]. This inconsistency manifests through several documented phenomena, including feature splitting, where concept representations vary across runs [28], and feature absorption, where general features are usurped by more specific ones [10]. These empirical instabilities stem from fundamental limitations in overcomplete dictionary learning [28]. Despite theoretical advances [48, 12], the gap between idealized assumptions and practical implementations undermines guarantees for unique feature recovery. Existing 2 approaches to address these limitations include Mutual Feature Regularization [32], which forces alignment between concurrently trained SAEs but addresses the effect rather than underlying causes, and Archetypal SAEs [15], which impose geometric constraints that may sacrifice representational power for stability. These challenges have fostered skepticism, with some [44] suggesting that SAE features should be viewed as pragmatically useful decompositions rather than exhaustive, universal sets. Contrary to this prevailing skepticism, our work shows that high feature consistency is attainable through careful architectural and training choices without explicit alignment mechanisms."
        },
        {
            "title": "3 Mechanistic Interpretability should prioritize Feature Consistency in SAEs",
            "content": "A prerequisite for the scientific validity and practical utility of features extracted in SAEs, is their consistencythe reliable convergence to equivalent feature sets across independent training runs given the same data and model architecture. Feature consistency should be elevated from secondary concern to an essential evaluation criterion and an active research priority for the following reasons. Firstly, achieving consistency yields substantial benefits for current MI practices that use SAEs. Improved Scientific Reproducibility: Reproducibility is cornerstone of science. If SAEs produce different feature dictionaries run-to-run [44, 15], feature explanations and discovered circuits become difficult to replicate. Consistency ensures findings are robust, not initialization artifacts, and helps foster cumulative progress. Improved Research Efficiency and Resource Allocation: Significant effort is invested in interpreting SAE features, whether manually or through automated methods [6]. If features are not consistent across training runs, this entire interpretation process: identifying, labeling, and understanding features must be repeated for each new SAE training instance. Consistent features, however, can be reliably matched across instances, allowing interpretations to be reused and incrementally refined, thereby saving substantial researcher time and computational resources. Increased Trust in Explanations from SAEs on Private Data: When SAEs are trained on private data and their dictionaries and feature explanations are shared, feature consistency increases credibility of the explanations. Although the training process (e.g., random initialization) can create features that are artifacts of that specific run, feature consistently emerging across multiple runs is more likely stable abstraction learned from the data distribution than an ephemeral artifact. This measurable robustness to training variability lends greater confidence that the interpretations reflect genuine learned patterns, which is even more important when direct data validation is impossible. Secondly, many current SAE techniques already implicitly assume feature consistency, even if this assumption is not explicitly verified. The long-term MI ambition of identifying canonical units of analysisfeatures that are complete, atomic, and unique [28]requires run-to-run consistency as prerequisite before addressing complexities like atomicity or completeness. Another example of implicit reliance is feature stitching [28], which compares or combines features across different models or SAEs; this process relies on identifiable and stable underlying features. Similarly, downstream applications like model steering [9], unlearning [37], bias removal [34]), and feature ablation assume that the targeted features are well-defined, consistent entities. If these base features are unstable or non-identifiable, the reliability of such applications is severely compromised. Our Proposal: Defining and Measuring Feature Consistency. While prior work has highlighted the challenge of feature inconsistency in SAEs [44, 32, 42], often leading to pessimistic conclusions about achieving stable feature sets, our findings demonstrate that high levels of feature consistency are attainable, with appropriate architectural choices. This motivates renewed focus on consistency as key dimension for evaluation. Our central position is that the MI community should prioritize feature consistency in SAEs. This requires not only acknowledging its importance but also adopting rigorous methods for its quantification. Conceptually, feature consistency implies that two feature dictionaries, and (both Rmdsae with dsae features), learned from independent training runs using same dataset, should capture the same underlying concepts. We formalize this ideal with an empirically tractable notion, Strong Feature Consistency, where the dictionaries are considered equivalent if their feature vectors align up to permutation and individual non-zero scaling factors. That is, for each feature vector ai in A, there should ideally exist corresponding feature vector σ(i) in (where σ is permutation) such that ai = λia σ(i) for some scaling factor λi = 0. More general notions of consistency are detailed in Appendix B. 3 To make this actionable, throughout this paper we adopt commonly used evaluation metric from the independent component analysis literature [24]: the Mean Correlation Coefficient (MCC). This metric directly evaluates permutation and scaling equivalence, making it robust measure of Strong Feature Consistency for dictionary-based features in SAEs. Cosine similarity addresses arbitrary positive scaling, while the use of the absolute value in MCC accounts for feature sign. We present this as concrete, working example of how consistency can be operationalized, although alternative metrics may be more appropriate for other feature types or notions of equivalence. We define general Mean Correlation Coefficient (MCC) between any two feature dictionaries RmdA and RmdB with columns ai and bj respectively. Let = min(dA, dB) and Mn(A, B) be the set of all possible one-to-one matchings of size between the features of and B. The MCC is defined as: MCC(A, B) = 1 max Mn(A,B) (cid:88) (i,j)M ai, bj ai2bj2 . The optimal matching that achieves this maximum is typically found using the Hungarian algorithm. From this general definition, we derive two specific metrics for our evaluations: 1. Pairwise Dictionary Mean Correlation Coefficient (PW-MCC): When comparing two dictionaries and of learned features, both of size dsae, we use PW-MCC(A, A) = MCC(A, A) where = dsae. PW-MCC approaching unity signifies robust convergence to highly similar feature dictionaries across independent training runs. 2. Ground-Truth MCC (GT-MCC): In controlled synthetic environments, where ground-truth dictionary Agt Rmdgt is known, we use GT-MCC(A, Agt) = MCC(A, Agt) to evaluate the recovery quality of learned dictionary Rmdsae, where = min(dsae, dgt). GT-MCC can be used for validating PW-MCC as proxy for consistency. Prioritizing consistency, and employing well-defined metrics such as PW-MCC to quantify it, offers several advantages: (i) it provides an objective measure of run-to-run stability for this key notion of feature equivalence; (ii) it facilitates equitable comparisons across methods and settings; and (iii) it incentivizes the development of techniques that yield more reliable features. In the following sections, we provide evidence from theory, synthetic experiments, and real-world applications to support our position and illustrate both the attainability and the challenges of achieving high feature consistency."
        },
        {
            "title": "4 Evidence from Theoretical Analysis and Synthetic Experiments",
            "content": "4.1 Theoretical Foundations for Feature Consistency SAEs learn to represent input data Rmn through dictionary Rmdsae and corresponding sparse activations Rdsaen, such that AF. Previous work often dismisses non-invertible dictionaries as non-consistent [42, 25], particularly in the overcomplete regime of dictionary learning where dsae > m. However, this overlooks the natural sparsity present in real signals. Drawing inspiration from sparse dictionary learning literature, we show how sparsity enables feature consistency guarantees even in overcomplete settings. We build our analysis on the spark condition [22, 12], which precisely characterizes when unique sparse representations exist: Definition 1 (Spark condition). dictionary Rmdsae satisfies the spark condition at sparsity level if for any two k-sparse vectors , Rdsae, the equality Af = Af implies that = . This condition ensures that distinct k-sparse vectors produce distinct outputs when transformed by the dictionary A. Equivalently, it provides injectivity of the linear map on the set of k-sparse vectors Σk := {f Rdsae : 0 k}. This is precisely the algebraic property needed for uniqueness of sparse representations. We leverage the following result from [22]: (cid:1)2 Theorem 1 (Adapted from [22]). Fix sparsity level k. There exists witness set of = k(cid:0)dsae k-sparse vectors f1, . . . , fn Σk such that for any pair of dictionaries A, Rmdsae satisfying the spark condition, the factorizations = [Af1, . . . , Afn] and = [Af 1, . . . , Af n] with k-sparse codes must coincide up to permutation and scaling of columns: = APD and = D1PF for some permutation matrix and diagonal invertible D. 4 Figure 1: TopK SAE is significantly better than Standard SAE (0.97 vs 0.63) in terms of GT-MCC. Figure 2: GT-MCC and PW-MCC for TopK and Standard SAE. PW-MCC follows the same trend as GT-MCC, both converging to comparable values. Shaded region represents max-min range across seeds. Implications for TopK SAE Feature Consistency. TopK SAEs achieve feature consistency by satisfying the conditions required for unique sparse factorization. Consider TopK SAE with encoder and decoder that enforces exactly k-sparse activations via (cid:55) TopKk(f ). The training objective simultaneously encourages three key properties: (1) Exact k-sparsity by construction of the TopK constraint, which zeros all but the largest coordinates; (2) Zero reconstruction error by minimizing AFF on data containing the witness set from Theorem 1; and (3) The spark condition through what we term the round-trip property E(Af ) = . As we prove in Appendix C, this round-trip property directly implies the spark condition. When these three conditions hold with data coverage meeting the requirements of Theorem 1, any two TopK SAEs trained on the same data must learn dictionaries that are identical up to permutation and scaling, establishing strong feature consistency we introduced in Section 3 even in overcomplete regimes. This explains why TopK SAEs can achieve consistent features: their training objective directly optimizes for the mathematical prerequisites required by the identifiability theorem. Takeaway: SAEs with k-sparsity and minimal reconstruction error satisfy strong feature consistency when the learned dictionary meets the spark condition. 4.2 Synthetic Verification To empirically validate our theoretical analysis, we conduct synthetic experiments comparing two representative SAE variants: TopK SAE and Standard SAE. We show that models designed according to our theoretical criteria achieve consistent feature representations. Following conventions in dictionary learning literature, we generate synthetic data by first sampling ground-truth feature dictionary Agt Rmdgt from standard normal distribution. We can represent all data points as = AgtFgt, where Fgt Rdgtn contains the activations for all data points. For each individual data sample x, we enforce the k-sparse condition by randomly selecting features and setting their values to independent Gaussian samples: = Agt fgt(x), where fgt(x) Rdgt represents single column of Fgt corresponding to data point and contains at most non-zero entries. In this synthetic setting (m = 8, dgt = 16, = 3, = 5e4) we can evaluate the estimated feature dictionary against the ground-truth Agt using GT-MCC. We also conduct additional experiments by training multiple SAEs (5 seeds) with identical data and model architecture but different weight initializations, comparing the PW-MCC curves with the GT-MCC curves. Figure 1 presents the final MCC evaluation results, showing that TopK SAE achieves significantly better consistency than Standard SAE, which confirms our analysis in Theorem 1. More importantly, Figure 2 demonstrates that the empirical PW-MCC values follow the same trend as GT-MCC, achieving comparable final values, suggesting that PW-MCC serves as an effective alternative to GT-MCC when ground truth dictionaries are unavailable. We refer to this setting as the matched regime, where the empirical dictionary size dSAE matches dgt. In all experiments for TopK SAE, the empirical sparsity value used in during training matches the ground truth sparsity. See Appendix for the extended analysis when is misspecified. Takeaway: We observe that Pairwise MCC converges to GT-MCC and strong feature consistency is achieved with TopK SAE in synthetic matched settings. 5 Figure 3: Left: Redundant regime with high GT-MCC but lower PW-MCC due to selection ambiguity. Right: Compressive regime with lower GT-MCC and PWMCC. Max-min range across 5 seeds is shaded. Figure 4: Token frequency in 1M tokens from Pile, showing the Zipfian distribution in real data, with long and sparse tail. 4.3 Synthetic Model Organism for Analyzing Feature Consistency While TopK SAEs can achieve high feature consistency under idealized matched-capacity scenarios, real-world data introduces substantial complexities that degrade this ideal. To show how these complexities affect feature consistency, we develop and analyze synthetic model organism, progressively introducing realistic data characteristics. This allows us to observe how metrics like PW-MCC respond to these challenges providing insights into their continued diagnostic utility. Consistency and Global Capacity Regimes. Fundamental challenges to feature consistency arise even before considering heterogeneous ground-truth distributions, stemming from the relationship between the SAEs dictionary width dsae and ground-truth dictionary width dgt. Using the linear generative model (X = AgtFgt) earlier, where ground-truth (x) are k-sparse (k = 8 in our experiments) and when all ground-truth features are uniformly sampled, we observe distinct behaviors. In globally redundant regime (dsae > dgt), where the SAE has more dictionary features than the ground truth (e.g., dsae = 160, dgt = 80, = 8, = 5e4), it can achieve high alignment with the ground-truth dictionary (GT-MCC 0.95). This suggests learned features accurately represent underlying concepts (Figure 3). However, run-to-run consistency is often significantly lower (PWMCC 0.77). This discrepancy arises from selection ambiguity: with excess capacity, multiple learned dictionary vectors can be comparably good matches for single ground-truth feature, leading to different, yet individually valid, feature sets being learned across runs. The lower PW-MCC here appropriately reflects this reduced stability in feature selection. Conversely, in globally compressive regime (dsae < dgt), where the SAE has insufficient capacity (e.g., dsae = 80, dgt = 800, = 8, = 5e4), both GT-MCC (0.75) and PW-MCC (0.60) are diminished due to the inability to represent all true features (Figure 3). The parallel decline of both metrics indicates their shared sensitivity to fundamental capacity limitations. These global capacity mismatches show that consistency in practice might be harder to achieve, and PW-MCC provides direct measure of this practical stability. Zipfian Feature Frequencies and Non-Uniform Capacity Allocation. primary characteristic of natural language data is the Zipfian (power-law) distribution of underlying feature frequencies (Figure 4)a few features are common, many are rare. We study the effect of this heterogeneity in the globally compressive regime where SAEs operate in, given the vast number of true concepts versus typical dictionary sizes [5]. To model this, we partition our synthetic ground truth features into clusters uniformly, each containing features, and impose an arbitrary ranking on these clusters. Data points are generated by first sampling cluster with probability pi (following Zipfian distribution with exponent α), and then sampling true features from that cluster. Post-training analysis of TopK SAEs trained on this data show that SAEs do not allocate their dictionary capacity uniformly across these clusters. Instead, the effective capacity Di (number of learned SAE features, matched via Hungarian algorithm, corresponding to ground-truth cluster i) is well-approximated by power law: Di = dsae pβ β. Our experiments empirically find β 1.4. Thus, in globally compressive setting (e.g., = 10, = 80 per cluster, total dgt = 800; dsae = 80, = 8), more frequent clusters (higher pi) receive proportionally larger share of the SAEs limited dictionary representation and, as result, exhibit higher GT-MCC scores, indicating better feature recovery for more common concepts (Figure 6). This differential ground-truth recovery suggests that run-to-run consistency would similarly depend on the frequency of features, pattern that feature-level PW-MCC analysis would capture. For additional details see Appendix D. /(cid:80) pj 6 Figure 5: Min activation frequency between matched feature pairs vs. pairwise similarity. Data from twophase Zipfian model (dgt = 5000, dsae = 1000). Feature-level similarity captures the influence of local consistency regimes across the frequency spectrum. Figure 6: Globally Compressive Zipfian Model (α = 1, 10 clusters, dgt = 800, dSAE = 80) shows cluster frequency-dependent GT-MCC (red line, left y-axis) and SAE feature allocation (green line, right y-axis). Emergence of Local Identifiability Regimes and Frequency-Dependent Consistency. This non-uniform capacity allocation driven by Zipfian frequencies means that different ground-truth clusters experience varied effective representational capacity within the same SAE. We define local redundancy factor for each ground-truth cluster (containing true features) as Ri := Di/d. This factor characterizes three distinct local identifiability regimes: Locally Redundant (Ri > 1) where the SAE has allocated more features to this cluster than true underlying features, risking selection ambiguity for this clusters concepts; Locally Matched (Ri 1) where the allocated capacity approximately matches the clusters complexity; and Locally Compressive (Ri < 1) where insufficient capacity allocated for the cluster prevents full recovery. These local regimes coexist within single, even globally compressive SAE. Frequent clusters may become locally redundant or matched, while rare clusters inevitably remain locally compressive. This coexistence translates to frequency-dependent consistency (PW-MCC) of individual learned features. Probing the Full Spectrum of Consistency: Two-Phase Zipfian Model. To more robustly investigate how these local regimes affect the consistency of individual features across wide dynamic range, especially for very rare concepts, we further enhance our model organism. As realworld feature distributions exhibit an extremely long and sparse tail, we employed two-phase feature cluster frequency distribution for 50000 clusters with dgt = 400000: Mandelbrot-Zipf function (g(r; s1, q) = (r + q)s1 , with s1 = 1.05, = 5.0) for common concepts (rank < 40, 000), transitioning to steeper power law (g(r; s2) rs2 , with s2 = 30.0) for the long tail (see Appendix D.4). Training TopK SAE with relatively ample dictionary width (dsae = 1000) on this data reveals clear positive correlation between the minimum activation frequency of matched feature pairs and their inter-run representational similarity as shown in Figure 5. This correlation emerges naturally from the interplay of local regimes: Frequent features are more likely to be in locally redundant or matched regimes. They receive sufficient allocated capacity, leading to stable learning and high GT-MCC. As true feature frequency decreases, the corresponding learned features are more likely to transition into locally compressive regimes. Here, insufficient allocated capacity relative to the conceptual complexity leads to lower inter-run similarity scores. Features in the extreme tail become so deeply locally compressive that they may be learned inconsistently across runs or not at all, resulting in minimal inter-run similarity. This spectrum of varying stability is effectively quantified by analyzing PW-MCC at the individual feature level. Takeaway: Our synthetic model organism validates PW-MCC as robust diagnostic, capturing how global capacity, Zipfian skew, and local identifiability regimes affect feature consistency."
        },
        {
            "title": "5 Evidence from Applications and Large-Scale Validation",
            "content": "5.1 Evaluating Consistency in Real-World Applications Prevailing practices in SAE evaluation prioritize metrics like reconstruction error (L2 loss), Fraction of Variance Explained, and sparsity (L0/L1) [6, 11]. While reconstruction fidelity is one aspect of SAE quality, incorporating feature consistency, quantified by PW-MCC, into the evaluation process offers benefits for both practical model development and the interpretation of learned features. 7 Figure 7: PW-MCC vs. train steps for BatchTopK, Gated, P-Anneal, JumpReLU, Standard, TopK, and Matryoshka BatchTopK SAEs on Pythia-160M activations. Higher PW-MCC indicates greater feature consistency. Figure 8: PW-MCC contribution by feature activation frequency for TopK, Standard, Gated, and JumpReLU SAEs. Bars (left axis) show each bins contribution; solid lines show cumulative contribution. Dashed lines (right axis) show feature distribution across bins. PW-MCC enables more decisive SAE model comparisons and hyperparameter selection where conventional metrics like reconstruction loss prove ambiguous. This advantage is particularly pronounced when controlling feature sparsity. When tuning the L1 coefficient, lower penalties improve reconstruction loss while potentially degrading feature quality. Similarly, in TopK SAEs where reconstruction loss improves monotonically with k, reconstruction loss alone fails to identify optimal sparsity levels for feature quality. PW-MCC addresses this limitation by identifying the underlying trade-off. Insufficient sparsity from excessively large or low L1 penalties causes feature selection ambiguity, while excessive sparsity from small or high L1 penalties leads to inconsistent concept representation. Both extremes degrade GT-MCC and PW-MCC, enabling clear identification of optimal k. We demonstrate how PW-MCC guides this selection in practice (Appendix E). PW-MCC acts as justifiable proxy for ground-truth alignment in unsupervised settings. Its utility stems from observations in our synthetic experiments where PW-MCC strongly correlated with GT-MCC, tracked its progression during training, and served as practical lower bound. Consequently, low PW-MCC across independent training runs suggests that an SAE is unlikely to converge to well-defined, true feature set. This ability to signal robust feature learning, even without ground truth, underpins its use as key evaluation metric in the real-world experiments presented next. 5.2 Training SAEs on LLM Activations: Empirical Consistency Results Experimental Setup. We train SAEs on Pythia-160M model [4], with width 214 on 500 million tokens from monology/pile-uncopyrighted [17], using residual stream activations from layer 8. For each SAE, we performed hyperparameter sweep, selecting the configuration that yielded the highest final PW-MCC across three independent training runs. Further details on the training setup and hyperparameters as well as results for Gemma-2-2B are provided in Appendix F. Table 1: TopK SAE (Pythia-160M): Higher activation frequency features show stronger mean cosine similarity (and lower variance) between matched pairs from independently trained SAEs, indicating greater consistency for more prevalent features. Table 2: TopK SAE (Pythia-160M): Semantic similarity scores (GPT Score, 1-10 scale) for automatically generated explanations of matched feature pairs correlate strongly with the features dictionary vector cosine similarity. GPT-score is averaged over 20 pairs. Act freq/1M tokens Features Similarity Similarity Range Feat pairs GPT Score 0.12.4 2.454.1 54.11.2k 1.2k26.7k 26.7k592.2k 127 2,542 10,013 3,548 0.514 0.280 0.742 0.295 0.837 0.209 0.888 0.157 0.964 0.087 0.06540.1128 0.11280.1947 0.19470.3359 0.33590.5795 0.57950.9999 34 311 975 1,423 13,640 1.71 2.19 3.27 4.12 8.28 8 Overall Consistency and Training Dynamics. Figure 7 shows the evolution of PW-MCC during training for several SAE architectures. We observe the steady increase in PW-MCC over training steps, indicating that as SAEs learn, their feature dictionaries converge and PW-MCC captures the emergence of consistent representations. Among the evaluated architectures, TopK and BatchTopK SAEs achieved the highest PW-MCC scores. The PW-MCC for some architectures, like BatchTopK, has not fully saturated by 2.5 105 training steps, indicating that longer training might yield even higher consistency. The curves also reveal interesting dynamics; for instance, some methods (e.g., BatchTopK) may start with lower consistency but exhibit faster improvement, eventually surpassing others (e.g., Gated SAE), suggesting that different SAEs impose varied structural assumptions [23]. Consistency Across the Feature Frequency Spectrum. The insights from our synthetic model organism, particularly the relationship between feature frequency and consistency, are reflected in these real-data experiments. Table 1 quantifies this: features with higher activation frequencies exhibit markedly stronger inter-run similarity. For example, the rarest features show an average similarity of 0.514, while the most frequent features achieve much higher 0.964. This trend is broadly observed across architectures (see Appendix F.2). This confirms that frequently occurring concepts are generally learned more stably, and PW-MCC reveals this spectrum of consistency. Figure 8 further dissects this relationship by showing the contribution of different feature frequency bins to the overall PW-MCC. For the TopK SAE, we observe relatively symmetric contribution from features across wide range of activation frequencies, with few dead features. In contrast, the Standard SAE exhibits larger proportion of features in the lowest frequency bins which contribute minimally to the cumulative PW-MCC, effectively pulling down its overall consistency. Gated SAEs perform well, approaching TopK SAEs but with slightly larger tail of less active, less consistent features. PW-MCC thus enables nuanced understanding of how different SAEs utilize their dictionary and achieve consistency across the frequency spectrum. Correlation with Semantic Similarity. We find that feature consistency is related to learning semantically related concepts. Table 2 shows that when we matched features between two independent SAE runs, binned these pairs by their cosine similarity, used an automated interpretation pipeline (details in Appendix F.4) to generate explanations for each feature in pair, and used an LLM to rate the semantic similarity of these two explanations (GPT Score), the results show strong positive correlation. Feature pairs with high dictionary vector similarity receive high semantic similarity scores from the LLM, while pairs with low vector similarity are judged as semantically divergent. This validates that PW-MCC and feature-level similarity are indicative of genuine stability in the learned semantic representations, making them valuable for ensuring the reliability of interpretability findings. Takeaway: High PW-MCC is achievable on LLM activations with TopK SAEs. PW-MCC tracks feature consistency, reveals frequency-dependent consistency, and correlates with feature semantic similarity."
        },
        {
            "title": "6 Alternative Views",
            "content": "While we advocate for prioritizing feature consistency in SAEs, we acknowledge and address potential counterarguments from the community. Some argue that SAE feature consistency is fundamentally unachievable, viewing features merely as useful, pragmatic decomposition[44]. This view is bolstered by findings that multiple, incompatible mechanistic explanations can coexist for the same model behavior [36, 31], questioning the existence of single, canonical feature set. While perfect universality for every feature on real data is indeed challenging, our work demonstrates that high levels of consistency are attainable with appropriate methods and evaluation (e.g., TopK SAEs achieving PW-MCC 0.80; Sections 5.2). pragmatic decomposition gains significant scientific utility when its components are demonstrably stable. The focus, therefore, should be on understanding, maximizing, and characterizing this stability. Another perspective is that sufficiently good interpretability can be achieved without demanding perfect feature consistency, and an excessive focus on stability might stifle exploratory research[30, 16]. Initial exploration using single-run features certainly has value. However, for claims aspiring to scientific robustnesssuch as those underpinning causality [35, 20], safety verification [1, 21], or canonical understanding [38, 39]sufficiently good stability must be quantitatively defined 9 and verified. The current degree of instability in many applications is often underestimated [44]. We advocate for establishing measurable baselines for consistency to add rigor for cumulative progress. It is also suggested that the pursuit of low-level feature stability might divert from the arguably more important goal of understanding higher-level conceptual abstractions or circuits [40]. We contend, however, that reliable higher-level understanding requires robust lower-level foundations. If the fundamental feature vocabulary is ill-defined or shifts between runs, any circuits or mechanisms built upon them become inherently suspect and difficult to validate [41]. Stable features are important, dependable anchors for trustworthy compositional analyses and the mapping of learned circuits. Finally, theres view that the field will organically converge on more consistent SAE methods without explicit mandates for consistency benchmarking. While scientific fields do self-correct over time, feature instability remains significant and often under-reported issue [15], even in widely-used methods. An active, concerted push for prioritizing consistency, supported by standardized metrics (like PW-MCC) and benchmarks, can substantially accelerate progress, guiding the community towards more scientifically sound practices."
        },
        {
            "title": "7 Conclusion and Call to Action",
            "content": "Prioritizing feature consistency in SAEs is important for advancing MI towards more robust engineering science. This requires shift in how we develop and evaluate feature extraction methods. We call upon the community to routinely report quantitative consistency scores (e.g., PW-MCC), ideally contextualized by feature frequency, alongside standard metrics, enabling meaningful comparisons. Furthermore, we propose developing standardized benchmarks for consistency, such as challenging synthetic model organisms with known ground-truth features and data heterogeneity. Finally, focused research is needed to deeply understand the determinants of consistency, including the interplay between SAE architecture, optimization, data characteristics, and evaluation metrics. Our work highlights several fertile avenues for future research: (a) designing SAEs for robust consistency across diverse LLM activation statistics (e.g., early vs. late layers) and developing adaptive sparsity mechanisms responsive to local data properties; (b) improving consistency for less frequent yet potentially critical features, and exploring techniques to target specific parts of the feature spectrum; (c) defining broader notions of feature equivalence beyond strong feature consistency (e.g., functional, subspace alignment) and corresponding consistency metrics; (d) understanding how the SAE encoders amortization gap [42] influences dictionary stability and whether encoder improvements or explicit consistency regularizers can enhance it; (e) establishing stronger theoretical guarantees for the consistency of features from modern SAEs under realistic data assumptions. Addressing these challenges and embracing research culture that values and quantifies feature consistency will be pivotal in building more reliable and cumulative science of MI."
        },
        {
            "title": "References",
            "content": "[1] Samir Abdaljalil, Filippo Pallucchini, Andrea Seveso, Hasan Kurban, Fabio Mercorio, and Erchin Serpedin. Safe: sparse autoencoder-based framework for robust query enrichment and hallucination mitigation in llms. arXiv preprint arXiv:2503.03032, 2025. [2] Emmanuel Ameisen, Jack Lindsey, Adam Pearce, Wes Gurnee, Nicholas L. Turner, Brian Chen, Craig Citro, David Abrahams, Shan Carter, Basil Hosmer, Jonathan Marcus, Michael Sklar, Adly Templeton, Trenton Bricken, Callum McDougall, Hoagy Cunningham, Thomas Henighan, Adam Jermyn, Andy Jones, Andrew Persic, Zhenyi Qi, T. Ben Thompson, Sam Zimmerman, Kelley Rivoire, Thomas Conerly, Chris Olah, and Joshua Batson. Circuit tracing: Revealing computational graphs in language models. Transformer Circuits Thread, 2025. [3] Sanjeev Arora, Rong Ge, and Ankur Moitra. New algorithms for learning incoherent and overcomplete dictionaries. In Conference on Learning Theory, pages 779806. PMLR, 2014. [4] Stella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, Kyle OBrien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, et al. Pythia: suite for analyzing large language models across training and scaling. In Proceedings of the 40th International Conference on Machine Learning, ICML23, 2023. 10 [5] Trenton Bricken, Adly Templeton, Joshua Batson, Brian Chen, Adam Jermyn, Tom Conerly, Nicholas L. Turner, Cem Anil, Carson Denison, Amanda Askell, et al. Towards monosemanticity: Decomposing language models with dictionary learning. Transformer Circuits Thread, 2023. [6] Trenton Bricken, Adly Templeton, Joshua Batson, Brian Chen, Adam Jermyn, Tom Conerly, Nicholas Turner, Cem Anil, Carson Denison, Amanda Askell, Robert Lasenby, Yifan Wu, Shauna Kravec, Nicholas Schiefer, Tim Maxwell, Nicholas Joseph, Alex Tamkin, Karina Nguyen, Brayden McLean, Josiah Burke, Tristan Hume, Shan Carter, Tom Henighan, and Chris Olah. Towards monosemanticity: Decomposing language models with dictionary learning. Transformer Circuits Thread, 2023. [7] Bart Bussmann, Patrick Leask, and Neel Nanda. Batchtopk sparse autoencoders. arXiv preprint arXiv:2412.06410, 2024. [8] Bart Bussmann, Noa Nabeshima, Adam Karvonen, and Neel Nanda. Learning multi-level features with matryoshka sparse autoencoders. arXiv preprint arXiv:2503.17547, 2025. [9] Sviatoslav Chalnev, Matthew Siu, and Arthur Conmy. Improving steering vectors by targeting sparse autoencoder features. arXiv preprint arXiv:2411.02193, 2024. [10] David Chanin, James Wilken-Smith, Tomáš Dulka, Hardik Bhatnagar, and Joseph Bloom. is for absorption: Studying feature splitting and absorption in sparse autoencoders. arXiv preprint arXiv:2409.14507, 2024. [11] Hoagy Cunningham, Aidan Ewart, Logan Riggs, Robert Huben, and Lee Sharkey. Sparse autoencoders find highly interpretable features in language models. arXiv preprint arXiv:2309.08600, 2023. [12] David Donoho and Michael Elad. Optimally sparse representation in general (nonorthogonal) dictionaries via l1 minimization. Proceedings of the National Academy of Sciences, 100(5):2197 2202, 2003. [13] Nelson Elhage, Tristan Hume, Catherine Olsson, Nicholas Schiefer, Tom Henighan, Shauna Kravec, Zac Hatfield-Dodds, Robert Lasenby, Dawn Drain, Carol Chen, et al. Toy models of superposition. arXiv preprint arXiv:2209.10652, 2022. [14] Nelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, et al. mathematical framework for transformer circuits. Transformer Circuits Thread, 1(1):12, 2021. [15] Thomas Fel, Ekdeep Singh Lubana, Jacob Prince, Matthew Kowal, Victor Boutin, Isabel Papadimitriou, Binxu Wang, Martin Wattenberg, Demba Ba, and Talia Konkle. Archetypal sae: Adaptive and stable dictionary learning for concept extraction in large vision models. arXiv preprint arXiv:2502.12892, 2025. [16] Timo Freiesleben, Gunnar König, Christoph Molnar, and Alvaro Tejero-Cantero. Scientific inference with interpretable machine learning: Analyzing models to learn about real-world phenomena. Minds and Machines, 34(3):32, 2024. [17] Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et al. The pile: An 800gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027, 2020. [18] Leo Gao, Tom Dupré la Tour, Henk Tillman, Gabriel Goh, Rajan Troll, Alec Radford, Ilya Sutskever, Jan Leike, and Jeffrey Wu. Scaling and evaluating sparse autoencoders. arXiv preprint arXiv:2406.04093, 2024. [19] Leo Gao, Tom Dupre la Tour, Henk Tillman, Gabriel Goh, Rajan Troll, Alec Radford, Ilya Sutskever, Jan Leike, and Jeffrey Wu. Scaling and evaluating sparse autoencoders. In The Thirteenth International Conference on Learning Representations, 2025. 11 [20] Atticus Geiger, Duligur Ibeling, Amir Zur, Maheep Chaudhary, Sonakshi Chauhan, Jing Huang, Aryaman Arora, Zhengxuan Wu, Noah Goodman, Christopher Potts, et al. Causal abstraction: theoretical foundation for mechanistic interpretability. arXiv preprint arXiv:2301.04709, 2023. [21] Ruben Härle, Felix Friedrich, Manuel Brack, Björn Deiseroth, Patrick Schramowski, and Kristian Kersting. Scar: Sparse conditioned autoencoders for concept detection and steering in llms. arXiv preprint arXiv:2411.07122, 2024. [22] Christopher Hillar and Friedrich Sommer. When can dictionary learning uniquely recover sparse data from subsamples? IEEE Transactions on Information Theory, 61(11):62906297, 2015. [23] Sai Sumedh Hindupur, Ekdeep Singh Lubana, Thomas Fel, and Demba Ba. Projecting assumptions: The duality between sparse autoencoders and concept geometry. arXiv preprint arXiv:2503.01822, 2025. [24] Aapo Hyvärinen and Erkki Oja. Independent component analysis: algorithms and applications. Neural networks, 13(4-5):411430, 2000. [25] Shruti Joshi, Andrea Dittadi, Sébastien Lachapelle, and Dhanya Sridhar. Identifiable steering via sparse autoencoding of multi-concept shifts. arXiv preprint arXiv:2502.12179, 2025. [26] Adam Karvonen, Can Rager, Jessica Lin, Curt Tigges, Jacob Bloom, Daniel Chanin, YueTing Lau, Euan Farrell, Arthur Conmy, Callum McDougall, Kolawole Ayonrinde, Martin Wearden, Logan Marks, and Neel Nanda. SAEBench: Comprehensive Benchmark for Sparse Autoencoders. https://www.neuronpedia.org/sae-bench/info, 2024. [27] Adam Karvonen, Benjamin Wright, Can Rager, Rico Angell, Jannik Brinkmann, Logan Smith, Claudio Mayrink Verdun, David Bau, and Samuel Marks. Measuring progress in dictionary learning for language model interpretability with board game models. Advances in Neural Information Processing Systems, 37:8309183118, 2024. [28] Patrick Leask, Bart Bussmann, Michael Pearce, Joseph Bloom, Curt Tigges, Noura Al Moubayed, Lee Sharkey, and Neel Nanda. Sparse autoencoders do not find canonical units of analysis. arXiv preprint arXiv:2502.04878, 2025. [29] Jack Lindsey, Wes Gurnee, Emmanuel Ameisen, Brian Chen, Adam Pearce, Nicholas L. Turner, Craig Citro, David Abrahams, Shan Carter, Basil Hosmer, Jonathan Marcus, Michael Sklar, Adly Templeton, Trenton Bricken, Callum McDougall, Hoagy Cunningham, Thomas Henighan, Adam Jermyn, Andy Jones, Andrew Persic, Zhenyi Qi, T. Ben Thompson, Sam Zimmerman, Kelley Rivoire, Thomas Conerly, Chris Olah, and Joshua Batson. On the biology of large language model. Transformer Circuits Thread, 2025. [30] Zachary Lipton. The mythos of model interpretability: In machine learning, the concept of interpretability is both important and slippery. Queue, 16(3):3157, 2018. [31] Aleksandar Makelov, Georg Lange, and Neel Nanda. Is this the subspace you are looking for? an interpretability illusion for subspace activation patching. arXiv preprint arXiv:2311.17030, 2023. [32] Luke Marks, Alasdair Paren, David Krueger, and Fazl Barez. Enhancing neural network interpretability with feature-aligned sparse autoencoders. arXiv preprint arXiv:2411.01220, 2024. [33] Samuel Marks, Adam Karvonen, and Aaron Mueller. Dictionary learning. https://github. com/saprmarks/dictionary_learning, 2024. [34] Samuel Marks, Can Rager, Eric Michaud, Yonatan Belinkov, David Bau, and Aaron Mueller. Sparse feature circuits: Discovering and editing interpretable causal graphs in language models. arXiv preprint arXiv:2403.19647, 2024. [35] Samuel Marks, Can Rager, Eric Michaud, Yonatan Belinkov, David Bau, and Aaron Mueller. Sparse feature circuits: Discovering and editing interpretable causal graphs in language models. In The Thirteenth International Conference on Learning Representations, 2025. [36] Maxime Méloux, Silviu Maniu, François Portet, and Maxime Peyrard. Everything, everywhere, all at once: Is mechanistic interpretability identifiable? arXiv preprint arXiv:2502.20914, 2025. [37] Aashiq Muhamed, Jacopo Bonato, Mona Diab, and Virginia Smith. Saes can improve unlearning: Dynamic sparse autoencoder guardrails for precision unlearning in llms. arXiv preprint arXiv:2504.08192, 2025. [38] Kyle OBrien, David Majercak, Xavier Fernandes, Richard Edgar, Jingya Chen, Harsha Nori, Dean Carignan, Eric Horvitz, and Forough Poursabzi-Sangde. Steering language model refusal with sparse autoencoders. arXiv preprint arXiv:2411.11296, 2024. [39] Chris Olah. Mechanistic interpretability, variables, and the importance of interpretable bases. Transformer Circuits Thread, 2(4), 2022. [40] Chris Olah, Nick Cammarata, Ludwig Schubert, Gabriel Goh, Michael Petrov, and Shan Carter. Zoom in: An introduction to circuits. Distill, 5(3):e00024001, 2020. [41] Chris Olah, Arvind Satyanarayan, Ian Johnson, Shan Carter, Ludwig Schubert, Katherine Ye, and Alexander Mordvintsev. The building blocks of interpretability. Distill, 3(3):e10, 2018. [42] Charles ONeill, Alim Gumran, and David Klindt. Compute optimal inference and provable amortisation gap in sparse autoencoders. arXiv preprint arXiv:2411.13117, 2024. [43] Mateusz Pach, Shyamgopal Karthik, Quentin Bouniot, Serge Belongie, and Zeynep Akata. Sparse autoencoders learn monosemantic features in vision-language models. arXiv preprint arXiv:2504.02821, 2025. [44] Gonçalo Paulo and Nora Belrose. Sparse autoencoders trained on the same data learn different features. arXiv preprint arXiv:2501.16615, 2025. [45] Gonçalo Paulo, Alex Mallen, Caden Juang, and Nora Belrose. Automatically interpreting millions of features in large language models. arXiv preprint arXiv:2410.13928, 2024. [46] Senthooran Rajamanoharan, Arthur Conmy, Lewis Smith, Tom Lieberum, Vikrant Varma, János Kramár, Rohin Shah, and Neel Nanda. Improving dictionary learning with gated sparse autoencoders. arXiv preprint arXiv:2404.16014, 2024. [47] Senthooran Rajamanoharan, Tom Lieberum, Nicolas Sonnerat, Arthur Conmy, Vikrant Varma, János Kramár, and Neel Nanda. Jumping ahead: Improving reconstruction fidelity with jumprelu sparse autoencoders. arXiv preprint arXiv:2407.14435, 2024. [48] Yuchen Sun and Kejun Huang. Global identifiability of overcomplete dictionary learning via l1 and volume minimization. In The Twelfth International Conference on Learning Representations (ICLR), 2024. [49] Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivière, Mihir Sanjay Kale, Juliette Love, et al. Gemma: Open models based on gemini research and technology. arXiv preprint arXiv:2403.08295, 2024. [50] Kexin Wang and Anna Seigal. Identifiability of overcomplete independent component analysis. arXiv preprint arXiv:2401.14709, 2024."
        },
        {
            "title": "A Additional Related Work",
            "content": "A.1 Sparse Autoencoders for Mechanistic Interpretability This section provides further context on the specific SAE architectures evaluated in our work, complementing the broader discussion of SAEs in Section 2. SAEs aim to learn overcomplete dictionaries that can decompose high-dimensional neural network activations into sparser, potentially more interpretable feature representations. The core principle involves training an autoencoder to reconstruct an input activation while simultaneously encouraging the latent representation (x) to be sparse. Various SAE architectures implement different mechanisms to achieve this sparsity objective. The key architectures employed in our experiments are described below. Standard SAE. The architecture we refer to as Standard SAE is an L1-penalized ReLU SAE that incorporates several contemporary training practices aimed at improving stability and reducing the incidence of inactive (dead) features [6, 33]. distinguishing characteristic of this variant is the application of the L1 penalty to feature activations (x) after they have been scaled by the L2 norm of their corresponding decoder dictionary vectors: λ1 fj(x) aj2, where aj is the j-th column of the decoder matrix. Unlike some earlier L1 SAEs that explicitly constrain decoder column norms to unity during optimization, this approach omits such constraint, integrating the decoder norm directly into the sparsity term. We use Adam optimization and gradient clipping. (cid:80) TopK SAE. TopK SAEs [18] enforce sparsity structurally, rather than through continuous penalty term. For each input, only the features with the highest pre-activation values (typically after ReLU non-linearity) are selected to be active, while all other feature activations are set to zero. The integer directly determines the L0 norm of the feature activation vector. This design obviates the need for tuning an L1 coefficient but introduces as crucial hyperparameter. We do not incorporate additional auxiliary loss terms designed to prevent feature death in this work. BatchTopK SAE. The BatchTopK SAE architecture [7] adapts the TopK principle by enforcing the k-sparsity constraint on average across batch of inputs, rather than strictly on per-sample basis. This is achieved by learning global activation threshold that is dynamically adjusted during training (using an Exponential Moving Average of feature pre-activations) to ensure that, averaged over batch, approximately features are active per input sample. This allows for greater variability in per-sample sparsity while maintaining target average L0 norm. Gated SAE. Gated SAEs [46] are designed to decouple the decision of whether feature activates from the magnitude of that activation. They employ two distinct pathways for processing the input: gate pathway, which produces values (near 0 or 1 via an L1 or similar sparsity penalty on the gate outputs) that determine if each feature should be active, and magnitude pathway, which computes the strength of each feature if it is gated on. The final feature activation is then the element-wise product of the outputs from these two pathways. The rationale behind this design is to allow features to activate with strong magnitudes when relevant, without these magnitudes being directly suppressed by the primary sparsity-inducing penalty, as that penalty is instead applied to the gating mechanism. P-Anneal SAE. This SAE variant [27] modifies L1-penalized SAEs by employing dynamic sparsity penalty based on an Lp p-norm. In this approach, the exponent in the sparsity term λsf (x)ps ps is annealed during the training process. Training typically commences with ps = 1 (equivalent to L1 minimization, which offers convex optimization landscape) and ps is progressively decreased towards target value pend < 1 (e.g., pend = 0.2 in the original work). This annealing schedule aims to first guide the optimization towards good region using the L1 penalty, and then gradually shift towards non-convex objective that more closely approximates L0 sparsity, potentially yielding sparser solutions. To ensure the effective strength of the sparsity penalty remains relatively consistent as ps changes, the scaling coefficient λs is also adaptively adjusted during training based on statistics derived from recent batches of feature activations. JumpReLU SAE. JumpReLU SAEs [47] employ JumpReLU activation function which uses per-feature learnable thresholds, θj. For an input language model activation Rn, the encoder 14 computes pre-activations πj(x) = (Wencx + benc)j for each feature j. The feature activation fj(x) is then given by fj(x) = JumpReLUθj (πj(x)) = πj(x)H(πj(x) θj), where is the Heaviside step function and θj > 0 is the learned threshold for feature j. Sparsity in the feature representation (x) is encouraged by an L0 penalty on the feature activations: for instance, using loss term like λ(f (x)0/Ltarget 0 1)2 to drive the average number of active features towards predefined target Ltarget . The non-differentiable nature of both the JumpReLU (with respect to θj) and the L0 penalty 0 is handled during training using Straight-Through Estimators. This architecture allows the model to learn distinct activation sensitivities for different features, as each θj can be optimized independently. Matryoshka BatchTopK SAE. Matryoshka SAEs [8] introduce hierarchical structure to the learned dictionary. In this paradigm, multiple, nested dictionaries of progressively increasing sizes are trained simultaneously within single model. Features are ordered or grouped, and the training objective is designed to encourage more general or broadly important features to be learned by the smaller, inner dictionaries (analogous to the inner dolls in Matryoshka set). More numerous or specialized features are then captured by the larger, outer dictionaries. This hierarchical approach aims to learn features at multiple levels of granularity and can offer computational efficiencies at inference time if smaller, inner dictionary provides sufficient representational power for given task. The Matryoshka BatchTopK variant evaluated in our study combines this hierarchical dictionary organization with the BatchTopK mechanism for selecting active features. Each architecture comes with its own set of hyperparameters, computational considerations, and characteristic effects on the learned feature space. A.2 Extended Discussion on Dictionary Learning Identifiability The feature consistency challenges observed in SAEs can be understood through the theoretical lens of dictionary learning identifiability. Dictionary learning identifiability addresses fundamental question: under what conditions can we guarantee that learning algorithm will recover the true underlying dictionary (or an equivalent version up to permutation and scaling) from observed data? This question directly parallels our inquiry into when SAEs can consistently learn the same features across different initializations. Dictionary learning can be formalized as the problem of finding dictionary matrix Rmd and sparse coefficient matrix Rdn such that AF, where Rmn represents observed data. In the overcomplete setting (d > m), which is most relevant to SAEs, the problem becomes particularly challenging because infinitely many solutions can potentially fit the data equally well. Several lines of theoretical work establish conditions under which overcomplete dictionary recovery is possible. For example, as discussed in our paper, the Spark condition introduced by [12] states that when spark(A) is sufficiently large relative to the sparsity level, unique recovery becomes possible. Specifically, unique sparse representation is guaranteed when spark(A) > 2k, where is the sparsity level. Building on this foundation, [48] recently established more comprehensive identifiability results for overcomplete dictionary learning. Their work introduces conditions under which global identifiability holds, showing that the identifiability of dictionaries depends on both the structure of the dictionary itself and the generative mechanism for coefficient vectors. key insight from [48] is that traditional dictionary identifiability frameworks rely on verifying two conditions: (1) coefficients are sufficiently diverse to span the full space of possibilities, and (2) dictionaries satisfy appropriate structural conditions such as the Spark condition. When both conditions hold, the dictionary can be uniquely determined up to permutation and scalingexactly the type of consistency we seek in SAE features. The Restricted Isometry Property (RIP) provides another important set of conditions. dictionary matrix satisfies RIP of order with constant δk if (1 δk)x2 2 for all k-sparse vectors x. When RIP holds with sufficiently small δk, consistent recovery becomes possible even in overcomplete regimes [3]. In our discussion, we opted to use the Spark condition due to its clearer connections to the training objectives of SAEs and the simplicity for implementing the condition in the learning algorithm. 2 (1 + δk)x 2 Ax2 We also draw inspiration from the independent component analysis (ICA) literature for defining our evaluation metric MCC, as dictionary learning has deep connections to ICA, particularly in its overcomplete form. ICA assumes that observed signals are linear mixtures of statistically independent source signals and aims to recover both the mixing matrix and the source signals [24], an objective shared with dictionary learning for finding the dictionary and sparse coefficient matrix [50]."
        },
        {
            "title": "B Formal Definitions of Feature Consistency",
            "content": "This appendix provides more formal definitions for the concepts of -Feature Consistency and Strong Feature Consistency, briefly introduced in Section 3. These definitions help to precisely articulate what it means for two sets of learned features to be considered equivalent. Definition 2 (T -Feature Consistency). Let and be two dictionaries (matrices whose columns are feature vectors), each containing features, learned from the same dataset using the same algorithm and hyperparameters but with different random initializations. These dictionaries are said to be -consistent (A A) if there exists permutation σ Sd (the set of all permutations of {1, . . . , d}) and specified transformation such that for all feature indices {1, . . . , d}: ai = (a σ(i)), where a(k) denotes the j-th feature vector (column) of dictionary A(k). The transformation can take various forms. For instance, in some contexts, might represent more complex function if features are not considered atomic or have internal structure. However, for dictionary learning in sparse autoencoders, where features are typically represented by individual vectors (dictionary atoms), more specific and common notion of equivalence is based on permutation and scaling. Definition 3 (Strong Feature Consistency). The dictionaries and from Definition 2 are said to exhibit Strong Feature Consistency if the transformation corresponds to an individual, perfeature scaling. That is, there exists permutation σ Sd and set of non-zero scaling factors λ1, . . . , λd {0} such that for all {1, . . . , d}: ai = λia σ(i). This definition implies that each feature learned in one run has one-to-one correspondent in the other run that points in the same (or exactly opposite, if λi < 0) direction, differing only in magnitude. If feature vectors are constrained to have unit ℓ2-norm (either by explicit normalization during training or as post-processing step before comparison), the scaling factors λi would effectively become 1. The Mean Correlation Coefficient (MCC) metrics used in this papernamely PW-MCC and GT-MCCare designed to measure this Strong Feature Consistency. The use of cosine similarity u, v/(u2v2) inherently accounts for differences in magnitude (norm), and the absolute value handles the sign ambiguity (features pointing in opposite directions are still considered perfectly correlated in direction). We prioritize Strong Feature Consistencyalignment up to permutation and scalingas foundational and empirically tractable starting point. This notion directly connects to identifiability results in dictionary learning and allows for straightforward quantification using metrics like MCC. While other, broader notions of consistency, such as functional equivalence (where features achieve similar outcomes despite different dictionary vectors) or subspace alignment, are undoubtedly important and represent valuable avenues for future research, establishing robust vector-level consistency is critical first step. How the Round-Trip Condition Guarantees Spark in SAEs This section proves that the round-trip property directly implies the spark condition for TopK SAEs. The argument is purely algebraic. C.1 Setting and Notation Throughout this section, we fix sparsity level 1 and denote by Σk := {f Rd : 0 k} the set of k-sparse vectors. SAE Components. Let Rmd be the decoder (dictionary) learned by TopK SAE, and let : Rm Σk be its deterministic TopK encoder. The encoder selects the largest magnitude inner products aj, and returns their signed values, with ties broken lexicographically to ensure is deterministic function. 16 Round-Trip Property. We assume that the encoder-decoder pair (E, A) satisfies the round-trip property if: Σk, E(Af ) = . (1) k-Injectivity and Spark. dictionary is k-injective if , Σk, Af = Af implies = . This is equivalent to the spark condition spark(A) > 2k, where spark(A) is the size of the smallest linearly dependent column set of [12]: is k-injective spark(A) > 2k. C.2 Key Decomposition Lemma The following lemma provides the crucial technical tool for our main result: Lemma 1 (Two-Vector Decomposition). Let Rd {0} with h0 2k. There exist distinct vectors , Σk with disjoint supports such that = . Consequently, if Ah = 0, then Af = Af . Proof. Let = supp(h), so 2k. We can partition into two disjoint sets S1, S2 such that S1, S2 k. This is always possible since 2k. Define vectors , Rd by: fj := (cid:26)hj 0 if S1 if / S1 , := (cid:26)hj 0 if S2 if / . By construction: 1. , Σk since supp(f ) S1 and supp(f ) S2 with S1, S2 2. supp(f ) supp(f ) = S1 S2 = (disjoint supports) 3. = since = 0 implies at least one of S1, S2 is non-empty 4. = by direct verification on each coordinate If Ah = 0, then A(f ) = 0, which immediately gives Af = Af . C.3 Main Result Theorem 2 (Round-Trip Implies k-Injectivity). If the round-trip property (1) holds, then spark(A) > 2k. Equivalently, is k-injective. Proof. We proceed by contradiction. Assume spark(A) 2k. Then there exists non-zero vector Rd with h0 2k such that Ah = 0. By Lemma 1, we can decompose = where , Σk are distinct vectors with disjoint supports, and Af = Af (since Ah = 0). Let := Af = Af denote the common image. Since is deterministic function, E(x) is uniquely determined. However, applying the round-trip property (1) to both representations: E(x) = E(Af ) = and E(x) = E(Af ) = . This implies = , contradicting the fact that and are distinct. Therefore, our assumption spark(A) 2k must be false, which means spark(A) > 2k. 17 C.4 Application to TopK SAE Corollary 1 (Spark Condition for TopK SAEs). Let Rm be training dataset and suppose TopK SAE with learned dictionary Rmd and deterministic Top-k encoder : Rm Σk achieves: 1. Zero reconstruction error: AE(x) = for all 2. Reachability: For every k-sparse vector Σk, there exists such that E(x) = Then the learned dictionary is k-injective: spark(A) > 2k. Proof. Let Σk be arbitrary. By reachability, there exists such that E(x) = . Zero reconstruction error gives: Applying the encoder to both sides: Af = AE(x) = x. E(Af ) = E(x) = . (2) (3) Since this holds for arbitrary Σk, the round-trip property is satisfied. Theorem 2 then immediately implies spark(A) > 2k. C.5 Implications for Feature Consistency Theoretical Guarantee. Corollary 1 establishes that when TopK SAEs achieve zero reconstruction error and reachability on their training data, the learned dictionary satisfies the spark condition. This provides theoretical foundation for feature consistency in TopK SAEs based purely on operational training outcomes. Practical Interpretation. The two conditions serve complementary but distinct roles in ensuring the spark guarantee. Zero reconstruction error prevents encoder collapse by forcing the encoder to distinguish among all training inputsif multiple inputs collapsed to the same sparse code, some could not be perfectly reconstructed by the decoder. Reachability ensures comprehensive coverage by guaranteeing that every possible k-sparse code appears in the training dataset X. This coverage requirement enables the theoretical result to apply universally across all codes in Σk. In practice, exact reachability cannot be verified on finite datasets, so this condition is approximated through diverse training data that provides broad coverage across the feature space. Connection to Identifiability. Our result shows that learning decoding dictionary satisfying the spark condition does not require access to the ground truth, which significantly broadens the scope of traditional identifiability discussions. When the ground truth Agt is assumed to satisfy the spark condition, our result naturally recovers standard identifiability guarantees. More importantly, even in the absence of any ground truth information, our result ensures strong consistency across all learned dictionaries. This is especially valuable for practitioners who cannot reliably make assumptions about the data generation process."
        },
        {
            "title": "D Supplementary Analysis of SAEs trained on Synthetic Data",
            "content": "D.1 Detailed Analysis of Learning Regimes This section provides additional analysis of the redundant and compressive learning regimes introduced in the main text. In all experiments, we maintain constant TopK sparsity parameter = 8, = 5e4. D.1.1 Redundant Regime Analysis In the redundant regime, we set the ground truth dimension dgt = 80 and the SAE dictionary size dsae = 160, creating setting where the SAE has twice the capacity needed to represent all ground truth features (dsae > dgt). 18 Figure 9: Feature Correlation Distribution (dgt = 40, dsae = 160, = 8). Compares similarities of Run 0 features to ground truth (red) and Run 1 features (blue). The substantial overlap in high-similarity regions (purple) demonstrates ambiguity where multiple SAE features are good matches to both ground truth and features learned in other runs, creating selection ambiguity despite high feature quality. Figure 10: Cosine Similarity Decay with Ground Truth (dgt = 40, dsae = 160, = 8). Features are ranked by ground truth similarity for Run 0 (red) and Run 1 (blue). Similarity decays very slowly, remaining high well past rank dgt = 40, indicating that the SAE learns multiple good representations for each ground truth feature, creating selection challenge when comparing across runs. Figures 9 and 10 illustrate key characteristic of the redundant regime: the SAE learns multiple good representations for each ground truth feature. Figure 9 shows substantial overlap between features with high similarity to ground truth and features with high similarity across runs. Figure 10 demonstrates that cosine similarity to ground truth decays very slowly, remaining high well beyond the ground truth dimension. This redundancy creates fundamental selection ambiguity problem when comparing features across different SAE initializations. The large pool of near-equally good candidates for the top-dgt matches makes the optimal feature matching determined by the Hungarian algorithm highly sensitive to small variations between runs. This dictionary instability persists despite high average MCC scores with the ground truth. In essence, while the SAE learns good representations of the underlying features (as evidenced by high GT-MCC), it lacks consistent way to select among multiple valid alternatives, leading to different features being selected across runs and consequently comparatively lower pairwise consistency. As shown in Figure 11, the Mean GT-MCC (Maximum Correlation Coefficient) reaches high values, indicating strong recovery of ground truth features. However, Figure 12 shows that the PW-MCC across different SAE initializations is lower, reflecting the challenge of consistent feature selection despite good ground truth recovery. Intersection Ratio The Intersection Ratio measures the consistency of feature selection across different training runs by quantifying how often the same learned features that match well to ground truth also match well between different runs. For pair of runs (run1, run2), we first find M1GT , the optimal matching between dictionaries A1 and Agt, and define I1GT = {i j, (i, j) M1GT } as the set of feature indices from Run 1 that successfully match to ground truth features. Next, we find M12, the optimal matching between dictionaries A1 and A2, and let 12 be the set of the top dgt feature indices from Run 1 that participate in the highest-scoring similarity pairs (i, k) M12 (if dsae < dgt, we use all dsae indices). The intersection ratio is then computed as R1,2 = I1GT 12 min(dgt,dsae) . We report Ei=j[Ri,j], the expected intersection ratio estimated by averaging over multiple distinct pairs of runs, where higher values indicate reduced selection ambiguity and more consistent feature discovery across training runs. We find that the Intersection Ratio increases over training steps, indicating that SAEs converge toward more consistent feature selection, though perfect consistency remains challenging due to the fundamental ambiguity introduced by excess capacity. 19 Figure 11: Redundant Regime: TopK SAE Mean GT-MCC (across 5 seeds) vs. Training Steps (dgt = 80, dsae = 160, = 8). The GT-MCC reaches high values, indicating strong recovery of ground truth features. Figure 12: Redundant Regime: TopK SAE Mean PW-MCC (across 5 seeds) vs. Training Steps (dgt = 80, dsae = 160, = 8). The PW-MCC reaches lower values than GT-MCC, reflecting the challenge of feature consistency across different SAE initializations due to selection ambiguity. Figure 13: Redundant Regime: TopK SAE Mean Intersection Ratio (across 5 seeds) vs. Training Steps (dgt = 80, dsae = 160, = 8). The Intersection Ratio measures the consistency of feature selection indices across different SAE initializations, with higher values indicating more stable feature recovery. D.1.2 Compressive Regime Analysis In the compressive regime, we set the ground truth dimension dgt = 800 and the SAE dictionary size dsae = 80, creating setting where the SAE has only one-tenth of the capacity needed to represent all ground truth features (dsae < dgt). This capacity limitation forces the SAE to prioritize which features to learn. Figures 14 and 15 illustrate the key characteristics of the compressive regime. Unlike the redundant regime, where feature selection ambiguity was the primary challenge, the compressive regime faces fundamental capacity limitation. Figure 14 shows that both the PW-MCC and the Mean GT-MCC reach significantly lower values compared to the redundant regime (Figure 11), reflecting the inability to recover all ground truth features with limited capacity. D.2 Uniform Partitioning Experiments We analyze how partitioning ground truth features into uniform clusters affects SAE learning dynamics. In these experiments, we maintain constant total number of ground truth features (dgt = 800) while varying the number of clusters they are organized into. The dimension of each cluster is 20 Figure 14: Compressive Regime: TopK SAE Mean GT-MCC (across 5 seeds) vs. Training Steps (dgt = 800, dsae = 80, = 8). The GT-MCC reaches lower values compared to the redundant regime, reflecting the fundamental capacity limitation that prevents complete recovery of all ground truth features. Figure 15: Compressive Regime: TopK SAE Mean PW-MCC (across 5 seeds) vs. Training Steps (dgt = 800, dsae = 80, = 8). The PW-MCC values are also lower compared to the redundant regime, indicating lower overall recovery quality. dgt/num_clusters, resulting in fewer features per cluster as the number of clusters increases. The complete hyperparameter settings for these experiments are presented in Table 3. Table 3: Hyperparameters for Uniform Clustering Experiments Parameter TopK sparsity parameter (k) Activation dimension Dictionary size (dsae) Training examples Training steps Learning rate Learning rate decay factor Learning rate decay steps Warmup steps Minimum learning rate L1 coefficient Batch size Number of seeds Cluster distribution Ground truth dimension (dgt) Number of clusters Cluster dimensions Value 8 20 80 100,000 20,000 0.04 0.1 [20,000] 1,000 1e-05 0.1 4,096 3 uniform 800 varies (1, 10, 50, 100) dgt/num_clusters Table 4: Effect of uniform partitioning of ground truth vectors. As the number of clusters increases while keeping the total number of ground truth vectors constant, mean PW-MCC shows weak but consistent increase (0.621 to 0.665), while ground truth MCC remains stable around 0.74. Clusters Mean GT-MCC Std GT-MCC Mean PW-MCC 1 10 50 100 0.742 0.747 0.740 0.746 0.006 0.002 0.002 0. 0.621 0.634 0.651 0.665 Table 4 presents the results of our uniform partitioning experiments. The key finding is that as we impose additional structure by increasing the number of clusters from 1 to 100 while keeping the total number of ground truth vectors constant, the mean PW-MCC between SAEs increases consistently from 0.621 to 0.665. This suggests that clustered organization of features promotes more consistent feature learning across different SAE initializations. Interestingly, the mean ground truth MCC 21 remains stable around 0.74 across all cluster configurations, indicating that the overall recovery quality of ground truth features is not significantly affected by the clustering structure. D.3 Feature Recovery Across Zipf Distributions This section provides additional experimental results showing how feature recoverability in SAEs varies across different Zipf distributions. We analyzed distributions with exponents α {1.0, 1.1, 1.5, 2.0} to understand how the skewness of ground truth feature cluster probability affects SAE learning dynamics and feature reproducibility. For all experiments in this section, we set the ground truth dimension dgt = 800, SAE dictionary size dsae = 80, and TopK sparsity parameter = 8, placing us in the compressive regime where the SAE must learn compressed representation of the underlying features. The ground truth features are organized into 10 clusters, with 80 ground truth features per cluster, where the probability of each cluster appearing in the data follows Zipf distribution with varying exponents α. Table 5 provides the complete hyperparameter settings used in these experiments. Table 5: Hyperparameters for Zipf Distribution Experiments Parameter TopK sparsity parameter (k) Activation dimension Dictionary size (dsae) Training examples Training steps Learning rate Learning rate decay factor Learning rate decay steps Warmup steps Minimum learning rate L1 coefficient Batch size Number of models trained (seeds) Number of clusters Cluster dimensions Distribution Zipf skew (α) Value 8 20 80 100,000 20,000 0.04 0.1 [20,000] 1,000 1e-05 0.1 4,096 3 10 80 per cluster zipf varies (1.0, 1.1, 1.5, 2.0) D.3.1 Zipf Distribution with α = 1. Figure 16: Left: Capacity allocation model for Zipf distribution with α = 1.0, showing how SAE features are allocated to clusters based on cluster probability. The red curve shows the fitted power law model, following Di pβ where β 1.343. Right: Feature similarity between independently trained SAEs as function of minimum feature activation frequency, with bucketed averages (red) showing positive trend between activation frequency and feature reproducibility. For α = 1.0, we observe moderate skew in cluster probabilities with corresponding power-law allocation of dictionary features. As shown in Figure 16 (left), the SAE capacity allocation via 22 Figure 17: Cluster metrics for Zipf distribution with α = 1.0. Left: Cluster rank vs. probability (blue bars) and MCC scores (red line), showing how feature recovery quality varies with cluster probability. The MCC scores demonstrate positive correlation with cluster probability, with lower-ranked (more probable) clusters achieving better feature recovery. Right: Cluster rank vs. probability (blue bars) and feature allocation (green line), demonstrating how the model allocates dictionary features based on cluster probability, with more frequent clusters receiving proportionally more features. Figure 18: Feature-cluster relationships for Zipf distribution with α = 1.0. Left: Activation-based affinity heatmap showing how features (y-axis, sorted by primary cluster) are activated by different clusters (x-axis, sorted by probability). Brighter colors indicate stronger activation, showing that more frequent cluster features activate more features. Right: Matching-based affinity heatmap showing global assignment of features to clusters using Hungarian matching, with features on y-axis and clusters on x-axis Hungarian matching follows Di pβ with β 1.343, where Di is the number of SAE features allocated to cluster and pi is the clusters probability in the data distribution. This superlinear relationship indicates that more probable clusters receive disproportionately more dictionary features. Figure 16 (right) demonstrates that features with higher activation frequencies also show greater reproducibility across different SAE initializations, indicating that frequently activated features are more robustly learned. The cluster metrics in Figure 17 further support this relationship, showing that more probable clusters achieve better feature recovery quality as measured by MCC scores. The feature-cluster activationbased affinity map in Figure 18 reveal that while features tend to specialize for specific clusters, there is some activation overlap, particularly among the most probable clusters. The activation-based affinity (left) displays more diffuse relationships between features and clusters and exhibits less frequency skew compared to the discrete one-to-one assignments established through Hungarian matching (right), which more strongly favors high-probability clusters. D.3.2 Zipf Distribution with α = 1.1 Increasing the exponent to α = 1.1 creates slightly more skewed distribution. Comparing Figure 19 with Figure 16, we observe steeper power law curve in the capacity allocation model (Di pβ 23 Figure 19: Left: Capacity allocation model for Zipf distribution with α = 1.1, showing how SAE features are allocated to clusters based on cluster probability. Red curve shows fitted power law model with Di pβ where β 1.455. Right: Feature similarity between independently trained SAEs as function of minimum feature activation frequency, with bucketed averages (red) showing positive trend between activation frequency and feature reproducibility. Figure 20: Cluster metrics for Zipf distribution with α = 1.1. Left: Cluster rank vs. probability (blue bars) and MCC scores (red line), showing steeper decline in feature recovery quality for less probable clusters compared to α = 1.0. Right: Cluster rank vs. probability (blue bars) and feature allocation (green line), demonstrating more skewed allocation of dictionary features toward high-probability clusters. Figure 21: Feature-cluster relationships for Zipf distribution with α = 1.1. Left: Activation-based affinity heatmap showing stronger feature-to-cluster specialization. compared to α = 1.0. Right: Matching-based affinity heatmap showing increased skew in feature assignments, with high-probability clusters receiving disproportionately more feature allocations compared to α = 1.0. 24 with β 1.455), indicating that high-probability clusters now receive an even larger share of the dictionary capacity. We see similar positive trend between feature activation frequency and feature reproducibility, with no noticeable increase in the trend. Figure 20 reveals more dramatic drop-off in feature recoverability for lower-probability clusters, and Figure 21 shows increased skew in feature allocation, with high-probability clusters receiving proportionally more features than in the α = 1.0 case. This skew is less pronounced when measured through activation-based affinity (left) compared to the more extreme allocation in the Hungarian matching assignments (right). D.3.3 Zipf Distribution with α = 1.5 Figure 22: Left: Capacity allocation model for Zipf distribution with α = 1.5, showing significantly more skewed allocation of SAE features to clusters. Red curve shows fitted power law model with Di pβ where β 1.35. Right: Feature similarity between independently trained SAEs as function of minimum feature activation frequency showing weak positive trend. Figure 23: Cluster metrics for Zipf distribution with α = 1.5. Left: Cluster rank vs. probability (blue bars) and MCC scores (red line), showing sharp threshold effect where feature recovery quality drops dramatically beyond the highest-probability clusters. Right: Cluster rank vs. probability (blue bars) and feature allocation (green line), demonstrating highly concentrated allocation of dictionary features to the most probable clusters. With α = 1.5, we observe highly skewed distribution where small number of probable clusters dominate. Figure 22 shows that the capacity allocation follows power law with Di pβ where β 1.35, with the majority of dictionary features allocated to the highest-probability clusters. The feature similarity plot shows positive relationship between the feature activation frequency and feature similarity between independently trained SAEs, but the effect is not much stronger than the α = 1 or α = 1.1 case. The cluster metrics in Figure 23 show MCC scores dropping precipitously beyond the most probable clusters. The feature-cluster affinity maps in Figure 24 also show highly specialized features based on frequency but the skew when measured through activation-based affinity is less pronounced as compared to Hungarian matching assignments. 25 Figure 24: Feature-cluster relationships for Zipf distribution with α = 1.5. Left: Activation-based affinity heatmap showing high feature specialization with minimal cross-activation. Right: Matching-based affinity heatmap showing strong one-to-one mapping for high-probability clusters but poor assignment for low-probability clusters. D.3.4 Zipf Distribution with α = 2. Figure 25: Left: Capacity allocation model for Zipf distribution with α = 2.0, showing extreme concentration of SAE features to the highest-probability clusters. Red curve shows fitted power law model with Di pβ where β 1.256. Right: Feature similarity between independently trained SAEs as function of minimum feature activation frequency showing flat to weak positive trend. At α = 2.0, we observe an extremely skewed distribution where handful of clusters dominate the probability distribution. Figure 25 shows that dictionary capacity is allocated according to power law with Di pβ where β 1.256, with capacity concentrated in the highest-probability clusters. The feature similarity plot shows flat to weak positive relationship between the feature activation frequency and feature similarity between independently trained SAEs. The cluster metrics in Figure 26 confirm that only the very top clusters achieve meaningful feature recovery, with the vast majority of clusters poorly represented. D.4 Analysis of Dictionary Size Effects in Two-Phase Distributions To better approximate real language data distributions, we developed two-phase model that combines different power laws. This section examines how dictionary size affects feature learning in this more realistic distribution. We enhanced our model organism with two-phase feature cluster frequency distribution combining Mandelbrot-Zipf function (g(r; s1, q) = (r + q)s1 , s1 = 1.05, = 5.0) for common concepts (r < 40, 000) and steeper power law (g(r; s2) rs2, s2 = 30.0) for the long tail. For all experiments in this section, we set the ground truth dimension dgt = 400000, with 50000 clusters (each cluster represented by 8 ground truth features on average), and maintain constant TopK sparsity parameter = 8, while varying the SAE dictionary size dsae {80, 160, 1000, 10000} to analyze the impact of SAE capacity on feature learning. Table 6 summarizes the hyperparameters used in these experiments. Figure 26: Cluster metrics for Zipf distribution with α = 2.0. Left: Cluster rank vs. probability (blue bars) and MCC scores (red line), showing that the very highest-probability clusters achieve good feature recovery. Right: Cluster rank vs. probability (blue bars) and feature allocation (green line), demonstrating that dictionary features are almost exclusively allocated to the top clusters, with negligible capacity for the long tail. Figure 27: Feature-cluster relationships for Zipf distribution with α = 2.0. Left: Activation-based affinity heatmap showing specialization to high-probability clusters. Right: Matching-based affinity heatmap showing strong assignment for only the highest-probability clusters, with the majority of clusters receiving minimal or no feature representation. Figure 28: Two-phase cluster probability distribution used to approximate real language data. The distribution follows Mandelbrot-Zipf pattern (s1 = 1.05) until rank 40,000, then transitions to steeper power law (s2 = 30.0) capturing the long tail characteristics of natural language. 27 Table 6: Hyperparameters for Two-Phase Distribution Experiments Parameter TopK sparsity parameter (k) Activation dimension Dictionary size (dsae) Training examples Training steps Learning rate Learning rate decay factor Learning rate decay steps Warmup steps Minimum learning rate L1 coefficient Batch size Number of models trained Number of clusters Cluster dimensions First power law exponent (s1) Second power law exponent (s2) Transition rank parameter Value 8 20 varies (80, 160, 1000, 10000) 100,000 20,000 0.04 0.1 [20,000] 1,000 1e-05 0.1 4,096 3 50,000 8 per cluster 1.05 30.0 40,000 5.0 Our two-phase distribution depicted in Figure 28 combines shallow power law for frequent tokens (s1 = 1.05 until rank 40,000) with much steeper power law (s2 = 30.0) for the long tail. This creates realistic approximation of natural language distributions, which exhibit similar two-phase characteristics (Figure 4). Figure 29: Two-phase model with dictionary size 80. Feature reproducibility shows weak positive relationship with activation frequency. Figure 30: Two-phase model with dictionary size 160. The relationship between activation frequency and feature reproducibility remains weak but becomes slightly more pronounced compared to dictionary size 80. Figures 29 through 32 demonstrate how dictionary size affects feature reproducibility across the activation frequency spectrum. Several key trends emerge: 1. With small dictionary sizes (80-160 features), we observe only weak relationship between activation frequency and feature reproducibility. Only the most frequent clusters show consistent reproducibility, indicating severe capacity limitations where the dictionary should prioritize only the dominant clusters. 2. As dictionary size increases to 1000 features, the relationship between activation frequency and reproducibility becomes more pronounced. wider range of moderately frequent features begins to show improved reproducibility, as the increased capacity allows the model to represent more clusters with sufficient local redundancy. 3. At dictionary size 10000, we observe positive relationship between activation frequency and reproducibility across wide frequency range. The substantial increase in capacity 28 Figure 31: Two-phase model with dictionary size 1000. Feature reproducibility shows moderately strong positive correlation with activation frequency especially at higher activation frequencies. Increased model capacity creates sufficient local redundancy for high probability clusters. Figure 32: Two-phase model with dictionary size 10000. With substantially increased capacity, feature reproducibility exhibits strong positive correlation with activation frequency across wide frequency range. Increased model capacity creates sufficient local redundancy for high probability clusters. creates local redundancy for many more clusters, enabling consistent representation of features. These results demonstrate that dictionary size relative to the distribution characteristics plays In particular, the local crucial role in determining which features can be consistently learned. redundancydefined as the ratio of dictionary size to the effective number of clusters above certain frequency thresholddetermines the models ability to learn reproducible features across different frequency bands. Rather than threshold effect, we observe continuum where the strength of correlation between feature frequency and reproducibility increases as more clusters benefit from sufficient local redundancy. These results demonstrate that dictionary size relative to the distribution characteristics plays an important role in determining which features can be consistently learned. In particular, local redundancy defined as the ratio of dictionary capacity allocated to cluster relative to the cluster dimension determines the models ability to learn reproducible features across different frequency bands. With larger dictionaries, more clusters achieve sufficient local redundancy, leading to continuum where the strength of correlation between feature frequency and reproducibility increases as dictionary capacity expands."
        },
        {
            "title": "E Effect of Misspecifying Encoder Sparsity Parameter",
            "content": "In practical applications of TopK SAEs, practitioners must choose the sparsity parameter without knowledge of the true underlying sparsity of the data-generating process. This section investigates how misspecification of relative to the optimal value affects dictionary recovery quality and training stability. Rmdgt having Consider conceptual cluster within the data with ground-truth dictionary unit-norm columns. Let < dgt be the true sparsity of coefficient vectors for signals = s from this cluster. The SAE uses TopK encoder with parameter k. We define the sparsity ratio as ρ := k/s and focus on understanding the asymmetric effects of under-sparsity (ρ < 1) versus over-sparsity (ρ > 1) on feature learning. We conduct experiments in the matched regime where dsae = dgt, generating synthetic data by first sampling the ground-truth dictionary Agt Rmdgt from standard normal distribution with unitnorm columns. For each data point, we randomly select exactly features and set their activations to independent Gaussian samples, yielding coefficient vector fgt(x) Rdgt with non-zero entries, then construct data points as = Agtfgt(x). Our experimental configuration uses input dimension = 8, dictionary sizes dgt = dsae = 40, true sparsity = 8, training samples = 50, 000, TopK parameter range {2, 4, 6, 8, 10, 12, 14, 16}, and 5 independent runs per configuration. We evaluate dictionary recovery using GT-MCC, which measures correlation between learned dictionary 29 and ground-truth Agt using optimal permutation matching, computed over the final 100 training steps and averaged to reduce noise. Figure 33: Effect of Activation Sparsity in TopK SAE in the Matched Regime (dgt = dsae = 40, true = 8). We plot final GT-MCC (averaged over the last 100 steps) vs. k. Performance peaks at = = 8, with underestimating being more harmful than overestimating it. Figure 33 demonstrates that GT-MCC peaks precisely at = = 8, confirming that matching encoder sparsity to true data sparsity yields optimal dictionary recovery. The performance curve exhibits notable asymmetry around the optimal point: under-sparsity (k < s) causes sharp degradation in GT-MCC as decreases below s, while over-sparsity (k > s) leads to more gradual decline as increases beyond s. This asymmetry reflects fundamental differences in how sparsity misspecification affects the optimization process. When < s, the SAE lacks sufficient representational capacity to capture all active features in the ground-truth generating process. This constraint forces the model to either merge multiple ground-truth features into single learned features or systematically ignore some ground-truth features, both resulting in poor dictionary recovery. The sharp performance degradation occurs because the model cannot represent the true complexity of the data-generating process, leading to fundamental representational limitations that cannot be overcome through better optimization. Conversely, when > s, the SAE has excess representational capacity that allows recovery of all true features but may also lead to learning near-duplicate features representing similar directions, increased sensitivity to initialization, and selection ambiguity among competing feature representations. While this reduces run-to-run consistency, the gradual performance decline suggests that over-sparsity is less immediately harmful than under-sparsity, as the model can still capture the essential structure of the data even if it learns redundant or unstable features. These findings have important practical implications. Because practitioners never observe in real applications, measuring PW-MCC across multiple training runs becomes an important diagnostic tool. As demonstrated earlier in this paper, PW-MCC correlates strongly with GT-MCC, especially in the matched regime, providing practical proxy for dictionary quality when ground truth is unavailable. Our observations reveal that when is too low, the SAE lacks sufficient representational capacity, leading to poor feature recovery. When is too high, features may become unstable across runs due to selection ambiguity among near-duplicates, even when reconstruction loss appears acceptable. Therefore, sweeping over values while monitoring PW-MCC provides principled approach to approximate effective sparsity for given dataset and SAE architecture. For practitioners selecting appropriate sparsity parameters, these results suggest that when uncertain about true sparsity, conservative approach favoring slight over-sparsity (ρ 1.21.5) is preferable to under-sparsity, as performance degradation is more gradual and reversible. Practitioners should systematically vary while monitoring PW-MCC to identify regions of high consistency. Warning signs include low PW-MCC coupled with acceptable reconstruction loss, which may indicate oversparsity leading to feature instability, and poor reconstruction performance combined with high PW-MCC, which may indicate under-sparsity with consistent but incomplete feature recovery. These observations raise important theoretical questions for future research. Can we precisely characterize how dictionary learning error scales with the degree of sparsity misspecification under 30 different conditions, potentially leading to theoretical bounds on performance degradation? What is the exact relationship between mutual coherence, sparsity ratio, and feature stability? How do these effects manifest in real LLM representations, where underlying sparsity may vary across different data regimes and semantic contexts? Can we develop methods that adaptively determine or accommodate varying sparsities within datasets to better capture heterogeneous features? theoretical analysis of these questions, combined with further empirical investigation, could substantially advance our understanding of SAE training dynamics and improve feature extraction for mechanistic interpretability."
        },
        {
            "title": "F Supplementary Analysis of SAEs Trained on Language Model Activations",
            "content": "This appendix provides additional experimental details, visualizations, and quantitative results for SAEs trained on activations extracted from LLMs. The primary objective of these experiments is to empirically evaluate the feature consistency and learned characteristics of different SAE architectures when applied to complex, real-world data. In the absence of ground-truth features for LLM activations, feature consistency is primarily assessed by training multiple instances of each SAE configuration (differing only by random initialization seeds) and subsequently quantifying the similarity between their learned dictionaries using PW-MCC. F.1 Experimental Setup for Real Data Analysis F.1.1 General Methodology and Data SAEs of various architectures were trained using activations derived from LLMs processing text from the monology/pile-uncopyrighted dataset. For each specific SAE architecture and hyperparameter set, three independent training runs were conducted using distinct random seeds (random_seeds = [42, 43, 44]) to evaluate consistency. The learned dictionaries from pairs of these runs were compared by first finding an optimal one-to-one matching between their features using the Hungarian algorithm, and then calculating the cosine similarity for each matched pair. The average of these similarities constitutes the PW-MCC for the dictionary. Separately, we plot the individual feature similarity and its correlation with feature activation statistics. All SAEs were trained on 500 million tokens from the source dataset. Common training parameters, consistent across these experiments unless otherwise specified, are detailed in Table 7. Feature activation frequency for given SAE feature is defined as the proportion of input tokens (within representative sample of the training data) on which that feature exhibits non-zero activation. When comparing matched pair of features from two independently trained SAEs (run 1 and run 2), their joint activation behavior is characterized by min(freq_run1, freq_run2). This metric provides conservative estimate of their shared activity level, as feature pair representing truly consistent underlying concept should ideally be active on substantial and largely overlapping set of inputs. Table 7: Common Training Parameters for SAEs on LLM Activations. Parameter Value Dataset for Activations Total Training Tokens SAE Batch Size Warmup Steps Sparsity Warmup Steps Learning Rate Decay Start Number of Random Seeds per Configuration Activation Normalization Autocast Data Type monology/pile-uncopyrighted 5 108 2048 1000 5000 (for L1-based and JumpReLU ) 0.8 (of total training steps) 3 (42, 43, 44) Applied (before SAE input) torch.bfloat16 F.1.2 Configuration for Pythia-160M Experiments The detailed visualizations and quantitative comparisons presented in Section F.2 pertain to SAEs trained on activations extracted from the EleutherAI/pythia-160m-deduped model. These SAEs were trained on the residual stream activations output by layer 8 of the LLM (resid_post_layer_8). 31 Specific parameters for this experimental setup are provided in Table 8 and inspired by the sweep used in [26]. For the specific configurations analyzed from this Pythia-160M setup, the SAE architectures demonstrated varying levels of pairwise dictionary consistency as measured by PW-MCC. TopK SAEs achieved the highest overall performance with PW-MCC of 0.8181 using target of 20. Batch TopK SAEs followed with PW-MCC of 0.7656, also at target of 20. Gated SAEs produced PW-MCC of 0.7370 with sparsity penalty of 0.06. Among the remaining architectures, Matryoshka Batch TopK SAEs achieved 0.6267 at target of 20, P-Anneal SAEs reached 0.6113 with an initial sparsity penalty of 0.025, JumpReLU SAEs attained 0.4947 at target L0 of 40, and Standard SAEs achieved 0.4739 with sparsity penalty of 0.06. All optimal results were observed at training step 244,140. In terms of feature utilization, TopK and Gated SAEs consistently produced fewer dead features (features with very low or zero activation rates across the evaluation data) compared to the Standard and JumpReLU variants, with the L0-constrained architectures (TopK, Batch TopK, JumpReLU, and Matryoshka Batch TopK) demonstrating more controlled sparsity patterns than their L1-penalized counterparts. Table 8: SAE Training Parameters for EleutherAI/pythia-160m-deduped (Layer 8). Parameter Value LLM Model Targeted Layer LLM Activation Dimension (m) LLM Batch Size (for activation generation) LLM Context Length LLM Data Type SAE Dictionary Width SAE Learning Rate Architectures Evaluated EleutherAI/pythia-160m-deduped 8 (residual stream output) 768 32 1024 torch.float32 214 (16,384) 3 104 Standard, TopK, BatchTopK, Gated, P-Anneal, JumpReLU, Matryoshka BatchTopK Sparsity Penalties Sweep (for L1-based architectures): Standard P-Anneal (initial penalty) Gated Target L0s / (for L0-based architectures): [0.012, 0.015, 0.02, 0.03, 0.04, 0.06] [0.006, 0.008, 0.01, 0.015, 0.02, 0.025] [0.012, 0.018, 0.024, 0.04, 0.06, 0.08] [20, 40, 80, 160, 320, 640] F.1.3 Configuration for Gemma-2-2B Experiments Additional experiments were conducted using activations from the google/gemma-2-2B model [49], targeting the residual stream output of layer 12 (resid_post_layer_12). Table 9 documents the pertinent hyperparameters for these larger-scale runs. For the Gemma-2-2B experimental configuration, the SAE architectures exhibited similar performance characteristics in terms of PW-MCC. TopK SAEs achieved the highest pairwise dictionary consistency with PW-MCC of 0.7898 using target of 80. JumpReLU SAEs demonstrated competitive performance with PW-MCC of 0.7405 at target of 40, closely followed by Batch TopK SAEs with PW-MCC of 0.7403 at target of 80. Gated SAEs produced PW-MCC of 0.7033 with sparsity penalty of 0.04. The remaining architectures showed more modest performance levels, with Matryoshka Batch TopK SAEs achieving 0.5842 at target of 80, P-Anneal SAEs reaching 0.5731 with an initial sparsity penalty of 0.025, and Standard SAEs attaining 0.5717 with sparsity penalty of 0.03. The performance patterns observed in the larger Gemma-2-2B model generally maintained the relative ordering established in the Pythia-160M experiments, with L0-constrained architectures consistently outperforming their L1-penalized counterparts in terms of dictionary consistency metrics. F.2 Detailed Analysis of SAEs Trained on Pythia-160M Layer 8 Activations This section presents comparative analysis of feature consistency and activation patterns for four different SAE architectures (TopK, Gated, Standard, JumpReLU) trained on Pythia-160M layer 8 activations. Table 9: SAE Training Parameters for google/gemma-2-2B (Layer 12). Parameter Value LLM Model Targeted Layer LLM Activation Dimension (m) LLM Batch Size (for activation generation) LLM Context Length LLM Data Type SAE Dictionary Width SAE Learning Rate Random Seeds Used Architectures Evaluated google/gemma-2-2B 12 (residual stream output) 2304 4 1024 torch.bfloat16 216 (65,536) 3 104 42, 43 Standard, TopK, BatchTopK, Gated, P-Anneal, JumpReLU, Matryoshka BatchTopK Standard Sparsity Penalties Sweep (for L1-based architectures): Standard P-Anneal (initial penalty) Gated Target L0s / (for L0-based architectures): [0.012, 0.015, 0.02, 0.03, 0.04, 0.06] [0.006, 0.008, 0.01, 0.015, 0.02, 0.025] [0.012, 0.018, 0.024, 0.04, 0.06, 0.08] [20, 40, 80, 160, 320, 640] F.2.1 Cross-Architectural Comparison of Feature Similarity and Activation Patterns Figure 34: Average feature similarity (PW-MCC of matched individual features) versus activation rate for four SAE architectures. The activation rate is defined as min(freq_run1, freq_run2), representing the minimum percentage of tokens activating the feature across two independent runs. Data is from SAEs trained on Pythia-160M layer 8 activations. strong positive correlation is evident, indicating that features with higher shared activation rates are learned more consistently. Figure 35: Log-log plot of feature activation frequency versus feature rank for four SAE architectures trained on Pythia-160M layer 8 activations. All architectures exhibit power-law-like distributions of feature usage, but with distinct slopes and differing prevalence of very low-frequency (potentially dead) features. Notably, TopK and Gated architectures tend to show fewer dead features. general trend observed across SAE architectures is the positive correlation between features activation frequency and its consistency across independent training runs. Figure 34 illustrates this for Standard, TopK, Gated, and JumpReLU SAEs. The x-axis, min(freq_run1, freq_run2), represents condition for shared activity; features satisfying this with higher values (i.e., both are frequently active) exhibit higher pairwise similarity. This suggests that features corresponding to more prevalent patterns in the LLMs activations are more robustly learned. While this correlation is seen in all SAEs, the overall level of consistency varies, with TopK SAEs achieving the highest aggregate PW-MCC. Concurrently, Figure 35 reveals that different SAE architectures induce distinct feature utilization profiles. While all exhibit power-law-like distributions for feature activation frequencies (when features are ranked by frequency), the slopes of these distributions and the number of extremely low-frequency or dead features vary. Architectures such as TopK and Gated SAEs tend to result in 33 fewer dead features compared to Standard and JumpReLU SAEs, indicating potentially more efficient use of their learned dictionaries. F.2.2 Standard SAE Figure 36: Feature similarity analysis for Standard SAE (L1-penalized) trained on Pythia-160M layer 8 activations. The overall PW-MCC for this configuration was approximately 0.4739. Top left: Density map of pairwise feature similarity vs. log minimum activation frequency (min(freq_run1, freq_run2)). Top right: Histogram of pairwise feature similarity scores. Bottom left: Average similarity bucketed by log minimum frequency range. Bottom right: Scatter plot of feature activation frequencies from two runs (freq_run1 vs. freq_run2), colored by their pairwise similarity score. Points along the diagonal represent features with similar activation frequencies in both runs. Figure 37: Average feature similarity versus min(freq_run1, freq_run2) for the Standard SAE. This plot highlights the positive trend: features with higher shared activation levels tend to exhibit greater pairwise similarity, though the overall consistency for this architecture is modest. The Standard SAE architecture, characterized by an L1 penalty on activations, achieved relatively low overall PW-MCC of approximately 0.4739 in these experiments. detailed breakdown of its feature similarity characteristics is provided in Figure 36. The density map (top-left) and the bucketed average similarity (bottom-left) illustrate that while higher minimum activation frequencies correlate with improved similarity, significant portion of features exhibit low similarity. The histogram of similarity scores (top-right) is also peaked at zero, indicating several dead features. The scatter plot of paired frequencies (bottom-right) shows that features matched by the Hungarian algorithm (and thus contributing to the similarity score) often, but not always, possess similar activation frequencies 34 across the two runs. Features lying closer to the diagonal (similar frequencies in both runs) and having higher joint frequencies (top-right of this sub-panel) tend to exhibit higher similarity. Figure 37 isolates and clearly depicts the positive relationship between shared activation frequency and pairwise similarity for this architecture. F.2.3 TopK SAE Figure 38: Feature similarity analysis for TopK SAE trained on Pythia-160M layer 8 activations. This architecture achieved high overall PW-MCC of approximately 0.8188. Panels are analogous to Figure 36. Figure 39: Average feature similarity versus min(freq_run1, freq_run2) for the TopK SAE. This architecture demonstrates both high overall similarity levels and strong positive correlation between shared activation frequency and feature reproducibility. In contrast, the TopK SAE architecture demonstrated superior performance, achieving the highest overall PW-MCC of approximately 0.8188. The analyses in Figure 38 and Figure 39 illustrate this. The density map in Figure 38 (top-left) shows strong concentration of features in the high-similarity, high-shared-frequency region. The histogram of similarity scores (top-right) is markedly skewed towards higher values compared to the Standard SAE, indicating more consistently learned features across the dictionary. The scatter plot of paired frequencies (bottom-right) again suggests that matched features tend to have similar activation rates especially at higher frequencies. Figure 39 clearly shows robust positive correlation between shared activation frequency and high pairwise similarity, underscoring the stability of frequently used features learned by TopK SAEs. 35 F.2.4 Gated SAE Figure 40: Feature similarity analysis for Gated SAE trained on Pythia-160M layer 8 activations, with an overall PW-MCC of approximately 0.7378. Panels are analogous to Figure 36. Figure 41: Average feature similarity versus min(freq_run1, freq_run2) for the Gated SAE. The overall dictionary PW-MCC for this configuration is 0.7378. strong positive correlation is evident between shared activation frequency and individual feature similarity. The Gated SAE architecture yielded an overall PW-MCC of approximately 0.7378, exhibiting good consistency, second only to TopK SAEs in this comparison. Figure 40 and Figure 41 detail its characteristics. The four-panel plot in Figure 40 displays trends consistent with other architectures regarding the relationship between shared frequency and similarity. The distribution of similarity scores (top-right) is more favorable than that of Standard SAEs, leaning towards higher consistency. The paired frequency scatter plot (bottom-right) also indicates that matched features tend to share similar activation levels. Figure 41 confirms the strong positive correlation between min(freq_run1, freq_run2) and feature similarity. F.2.5 JumpReLU SAE The JumpReLU SAE achieved an overall PW-MCC of approximately 0.4957, placing its aggregate consistency on par with Standard SAEs in these experiments. The results are presented in Figure 42 and Figure 43. The general trend of higher similarity for more frequently and jointly active features persists. The scatter plot of paired frequencies in Figure 42 (bottom-right) also suggests that features 36 Figure 42: Feature similarity analysis for JumpReLU SAE trained on Pythia-160M layer 8 activations. The overall PW-MCC was approximately 0.4957. Panels are analogous to Figure 36. Figure 43: Average feature similarity versus min(freq_run1, freq_run2) for the JumpReLU SAE. This plot shows increasing similarity with higher shared activation frequency. The presence of two distinct clusters suggests potential subpopulations of features with differing learning or consistency characteristics within this architecture. matched by the Hungarian algorithm tend to possess similar activation frequencies across runs. Figure 43 shows that JumpReLU also produces several dead features that affect the net consistency. F.2.6 Summary of Real Data Findings The empirical investigations on Pythia-160M activations consistently reveal significant positive correlation between the activation frequency of learned featuresparticularly their shared activation level across independent runs, min(freq_run1, freq_run2)and their inter-run similarity or consistency. This observation holds across diverse SAE architectures. The use of min(freq_run1, freq_run2) as metric for joint activity is justified by the intuition that feature representing stable, underlying concept should be consistently activated by similar set of inputs across different model initializations, thus implying that both individual frequencies should be high for robust match. Furthermore, the scatter plots of paired feature frequencies (e.g., bottom-right panels in the four-panel figures) visually corroborate that features deemed the same by the Hungarian matching process (and thus contributing to similarity scores) indeed tend to exhibit comparable activation frequencies in the respective runs. 37 Among the architectures quantitatively compared, TopK SAEs demonstrated the highest overall dictionary consistency as measured by PW-MCC, followed in order by Gated, JumpReLU, and Standard SAEs. This ranking correlates with qualitative observations regarding feature utilization; TopK and Gated SAEs also tended to produce fewer dead or very sparsely used features, suggesting more effective and stable learning dynamic that leverages greater portion of their dictionary capacity. While the frequency-consistency trend is common thread, different SAE architectures clearly lead to varying aggregate levels of feature consistency and distinct feature activation profiles. The detailed multi-panel visualizations offer more granular understanding of consistency than single global PW-MCC score, by illustrating how stability is distributed across features with different activation characteristics within each architectural type. These findings highlight the interplay of architectural choice and feature activation statistics in determining the reliability and interpretability of features learned by SAEs from real-world LLM activations. F.3 Analysis of SAEs Trained on Gemma-2-2B Layer 12 Activations This section presents results from training SAEs on activations from layer 12 of the Gemma-22B model [49] and analyzes the mean pairwise MCC averaged across two training seeds. Due to computational constraints, we report only the final aggregated pairwise MCC values. Computing the MCC requires solving an assignment problem with complexity O(cid:0)N 3(cid:1), where represents the dictionary size. This becomes particularly challenging for Gemma-2-2B, which employs dictionary four times larger than Pythia-160M (65 536 versus 16 384). training PW-MCC of SAEs trained on 500M tokens from Figure 44 shows the final monology/pile-uncopyrighted. TopK SAE achieves the highest pairwise MCC on this larger model, corroborating our findings on Pythia-160M and supporting our theoretical analysis. Most other SAE variants exhibit performance patterns similar to those observed in Figure 7. One notable exception is JumpReLU, which previously showed relatively poor consistency on Pythia-160M but achieves substantially improved feature consistency on Gemma-2-2B. Despite this improvement, JumpReLU still does not match the consistency levels attained by TopK SAE. JumpReLU is commonly used in extremely large-scale SAEs within the community [2, 29]. Figure 44: Final PW-MCC for BatchTopK, Gated, P-Anneal, JumpReLU, Standard, TopK, and Matryoshka BatchTopK SAEs on Gemma-2-2B activations. Higher PW-MCC indicates greater run-to-run feature consistency. F.4 Qualitative Analysis: Example Feature Pairs across Similarity Buckets To complement the quantitative assessments of feature consistency, this section provides qualitative examination of example feature pairs extracted from 2 SAEs trained on Pythia-160M. We leverage an automated interpretation pipeline [45], to generate natural language explanations for individual 38 SAE features and to measure the functional similarity between pairs of features from independently trained models. The process for generating an explanation for single SAE feature is as follows: first, activations for the feature are recorded across substantial corpus (100,000 diverse text examples from monology/pile-uncopyrighted in our setup). The 10 text segments eliciting the strongest (highest magnitude) activations for that feature are then selected. These top-activating examples are formatted, the top activating tokens are emphasized (surrounded by ), the activation strength of the tokens is shown after each example and presented to an explainer LLM, specifically gpt-4.1-2025-04-14. The LLM is prompted to identify common patterns or semantic concepts within these examples and produce concise natural language interpretation of the features apparent function. For the analysis presented in Table 10, we first matched features between two independently trained SAEs (SAE1 and SAE2). This matching was achieved by computing the pairwise cosine similarity of all their dictionary vectors and subsequently applying the Hungarian algorithm to find an optimal one-to-one correspondence. These matched feature pairs were then categorized into five buckets based on their dictionary vector cosine similarity scores (Bucket 1: low vector similarity; Bucket 5: high vector similarity). To measure the functional similarity of each matched pair, their independently generated explanations were provided to the same gpt-4.1-2025-04-14 model. This model was then prompted to evaluate the semantic resemblance between the two explanations and assign functional similarity score on 1-10 scale. This score is presented as the GPT-Score score in Table 10, alongside the feature indices and their generated explanations. Table 10 shows clear concordance between the quantitative cosine similarity of the feature dictionary vectors and the functional similarity of their roles, as determined by the automated interpretability pipeline. Feature pairs with low vector similarity (e.g., Buckets 1 and 2) typically receive divergent functional explanations and low similarity scores from GPT-4.1. For instance, one feature might be interpreted as activating on LATEX math symbols, while its low-vector-similarity counterpart from another SAE is interpreted as activating on Go/Rust code structures. Conversely, feature pairs exhibiting high vector similarity (e.g., Buckets 4 and 5) frequently obtain near-identical functional explanations and high similarity scores, such as both features being interpreted as responding to phrases indicating future events or specific Wikipedia category tags like births. This analysis reinforces the utility of dictionary vector cosine similarity as meaningful measure of feature consistency. The strong correlation observed suggests that high vector similarity between features learned across different runs reliably indicates the stable learning of semantically coherent and functionally equivalent interpretable units. 39 Table 10: Representative feature pairs from two independently trained SAEs (SAE1 and SAE2) organized by similarity buckets, with corresponding GPT-evaluated functional similarity scores. Buckets represent increasing levels of feature similarity, demonstrating strong correlation between computed similarity measures and functional equivalence. Lower-similarity pairs (Buckets 1-2) exhibit largely unrelated behavioral patterns, while higher-similarity pairs (Buckets 4-5) demonstrate nearly identical semantic functions across both SAEs. Bucket SAE1, SAE GPT-Score SAE1 Feature Explanation SAE2 Feature Explanation 10742, 13528 37, 6034 11993, 9627 16044, 13563 5177, 15030 15430, 4143 13156, 12246 2292, 15215 4789, 3718 15326, 4995 10739, 10630 1203, 2543 9668, 13698 14177, 3256 7237, 7841 8557, 8334 3796, 6569 3161, 11659 2045, 14698 10453, 9653 13974, 1289 9040, 9704 13514, 4930 3430, 6144 3215, 7110 3/10 2/10 3/ 2/10 3/10 4/10 6/10 4/10 4/ 2/10 5/10 3/10 6/10 4/10 4/ 9/10 4/10 6/10 5/10 7/10 Activates on punctuation marks and transition words marking syntactic or discourse boundaries. Activates on LaTeX/math environments and symbols within mathematical markup. Activates on bibliographic reference tokens and citation markers in academic writing. Activates on common 2-4 letter substrings within larger tokens or variable names. Activates on LaTeX math tokens beginning or ending with backslash or angle brackets. Activates on ordinal terms and comparative adjectives marking ordered elements in sequence. Activates on markup-like sequences of repeated or paired symbols used as section separators in code. Activates on forms of the verb \"to be\" used as auxiliary verbs or main verbs. Activates on phrases like \"sounds like\", \"feels like\" expressing resemblance or subjective impressions. Activates on the token \"int\" in various contexts, both as suffix and as programming token. Activates on section separator \"Background {Sec1}\" in scientific writing. Activates at the start of code block bodies after opening braces in Go and Rust. Activates on closing parenthesis and angle bracket sequence at the end of figure captions. Activates on log message prefixes like \"W/\" and \"E/\" in Android logcat output. Activates on file paths or URLs containing delimiter sequences between directory or resource names. Activates on words following punctuation or in enumerated lists, especially suffixes or grammatical constructs. Activates on closing angle brackets that terminate LaTeX-style or math expression delimiters. Activates on the auxiliary verb \"have\" used for forming present perfect constructs. Activates on verbs expressing mental or emotional impact in reactions or realizations. Activates on \"Image\" and variants in variable names, software names, and UI elements. Activates on paired angle brackets used as delimiters or markers in code and technical writing. Activates on tokens containing the sequence \"red\", \"whit\", or \"Reddit\" within longer words. Activates on past-tense passive forms of verbs indicating completion or accomplishment. Activates on delimiters for copyright and license statements in code and documentation. Activates on wordpieces containing \"wor\" and similar substrings within longer words. Activates on code keywords, method names, and identifiers in programming contexts. Activates on scientific nouns denoting materials used in laboratory or industrial contexts. Activates on tokens signifying the achievement or fulfillment of requirements or conditions. Activates on punctuation elements and conjunctions serving as delimiters in complex sentences. Activates on \"War\" in proper nouns and as standalone word in relevant contexts. Activates on mentions of the color \"yellow\" when describing objects or attributes. Activates on \".>\" token in numeric, scientific notation or code fragments. Activates on multi-word phrases with verbs plus prepositions introducing perspectives. Activates on mentions of diseases, particularly cancer and related medical conditions. Activates on tokens related to biometric identification, especially fingerprints and forensics. Activates on the token \"yellow\" as standalone word or within color-related phrases. Activates on the period character when used as decimal point in numerical values. Activates on \"in\" within common prepositional phrases introducing abstract relationships. Activates on scientific terms denoting important issues, processes, or domains in research. Activates on technical nouns denoting identifiers, labels, codes, or tags in specialized domains. 10/10 10/10 9/10 9/ 9/10 Activates on phrases indicating future events or developments with specific timeframes. Activates on \"births\" in Wikipedia-style category tags denoting birth years. Activates on multi-token prepositions and conjunctions, especially with \"of\", \"by\", \"to\". Activates on function definitions in code following parameter lists before function bodies. Activates on \"exclude\", \"excluded\", or \"exclusion\" in procedural or scientific contexts. Activates on future-related phrases indicating when something is expected or planned. Activates on \"births\" within Wikipedia category tags indicating year of birth. Activates on phrases containing prepositions that indicate causes, relationships, or compositions. Activates on opening curly braces that begin function or method bodies in code. Activates on \"exclude\" and related forms in the context of setting boundaries or omission criteria."
        }
    ],
    "affiliations": [
        "Carnegie Mellon University",
        "MBZUAI"
    ]
}