{
    "paper_title": "How Alignment Shrinks the Generative Horizon",
    "authors": [
        "Chenghao Yang",
        "Ari Holtzman"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Despite their impressive capabilities, aligned large language models (LLMs) often generate outputs that lack diversity. What drives this stability in the generation? We investigate this phenomenon through the lens of probability concentration in the model's output distribution. To quantify this concentration, we introduce the Branching Factor (BF) -- a token-invariant measure of the effective number of plausible next steps during generation. Our empirical analysis reveals two key findings: (1) BF often decreases as generation progresses, suggesting that LLMs become more predictable as they generate. (2) alignment tuning substantially sharpens the model's output distribution from the outset, reducing BF by nearly an order of magnitude (e.g., from 12 to 1.2) relative to base models. This stark reduction helps explain why aligned models often appear less sensitive to decoding strategies. Building on this insight, we find this stability has surprising implications for complex reasoning. Aligned Chain-of-Thought (CoT) models (e.g., DeepSeek-distilled models), for instance, leverage this effect; by generating longer reasoning chains, they push generation into later, more deterministic (lower BF) stages, resulting in more stable outputs. We hypothesize that alignment tuning does not fundamentally change a model's behavior, but instead steers it toward stylistic tokens (e.g., \"Sure\") that unlock low-entropy trajectories already present in the base model. This view is supported by nudging experiments, which show that prompting base models with such tokens can similarly reduce BF. Together, our findings establish BF as a powerful diagnostic for understanding and controlling LLM outputs - clarifying how alignment reduces variability, how CoT promotes stable generations, and how base models can be steered away from diversity."
        },
        {
            "title": "Start",
            "content": "Chenghao Yang, Ari Holtzman Department of Computer Science, University of Chicago {chenghao, aholtzman}@uchicago.edu Codebase Website"
        },
        {
            "title": "Abstract",
            "content": "Despite their impressive capabilities, aligned large language models (LLMs) often generate outputs that lack diversity. What drives this stability in the generation? We investigate this phenomenon through the lens of probability concentration in the models output distribution. To quantify this concentration, we introduce the Branching Factor (BF)a token-invariant measure of the effective number of plausible next steps during generation. Our empirical analysis reveals two key findings: (1) BF often decreases as generation progresses, suggesting that LLMs become more predictable as they generate. (2) alignment tuning substantially sharpens the models output distribution from the outset, reducing BF by nearly an order of magnitude (e.g., from 12 to 1.2) relative to base models. This stark reduction helps explain why aligned models often appear less sensitive to decoding strategies. Building on this insight, we find this stability has surprising implications for complex reasoning. Aligned Chain-of-Thought (CoT) models (e.g., DeepSeekdistilled models), for instance, leverage this effect; by generating longer reasoning chains, they push generation into later, more deterministic (lower BF) stages, resulting in more stable outputs. We hypothesize that alignment tuning does not fundamentally change models behavior, but instead steers it toward stylistic tokens (e.g., Sure) that unlock low-entropy trajectories already present in the base model. This view is supported by nudging experiments, which show that prompting base models with such tokens can similarly reduce BF. Together, our findings establish BF as powerful diagnostic for understanding and controlling LLM outputs - clarifying how alignment reduces variability, how CoT promotes stable generations, and how base models can be steered away from diversity. 5 2 0 2 2 2 ] . [ 1 1 7 8 7 1 . 6 0 5 2 : r Figure 1: conceptual illustration of how alignment and CoT influence the generation space of LLMs. While base models begin with high output diversity, alignment tuning sharply concentrates early probability mass, leading to more stable outputs. CoT extends this effect into later positions, flattening output sample variation and reducing sensitivity to decoding. Preprint."
        },
        {
            "title": "Introduction",
            "content": "While alignment tuning improves helpfulness and safety in large language models (LLMs), it often introduces tradeoff: reduced output diversity (Padmakumar and He, 2024; Chakrabarty et al., 2024; Tian et al., 2024; Kirk et al., 2024; Lu et al., 2025) and increased determinism (Saparov and He, 2023; Song et al., 2024; Renze and Guven, 2024; Bigelow et al., 2024; West and Potts, 2025). Our own case study on MMLU (Hendrycks et al., 2021) confirms that aligned models exhibit reduced variance under Chain-of-Thought (CoT) prompting (Wei et al., 2022). These findings suggest potential distributional concentration phenomenon in aligned models i.e., tendency to produce semantically and structurally similar outputs. But how should we conceptualize this concentration? Autoregressive language generation is inherently traversal through branching tree: at each step, the model selects token and expands the sequence along specific branch. This structure, illustrated conceptually in Figure 1, naturally defines space of possible continuations, independent of specific token identities. To analyze how concentrated this space is during generation, we introduce the Branching Factor (BF) as local, average-case measure of LLMs output breadth, offering microscopic lens on how local branching behavior gives rise to global output concentration. To quantify BF, we leverage perplexity, an information-theoretic proxy for the effective size of models output space. Unlike standard perplexity evaluations that are computed over dataset using teacher-forcing, our goal is to estimate the perplexity of the models full output distribution over continuationsi.e., the number of diverse, high-probability sequences it is likely to generate. However, directly computing this quantity is intractable due to the exponential number of possible outputs, especially for long generations. To address this, we leverage the Asymptotic Equipartition Property (AEP) (Shannon, 1948; Mudireddy et al., 2024), which implies that the average log-probability of sufficiently long samples approximates the length-averaged entropy of the underlying distribution. This lets us estimate BF directly from naturally sampled completions, without requiring teacherforcing or exhaustive enumeration. To see what variables control BF, we investigate how BF varies with output length, model size, prompt complexity, and training paradigms. Our findings reveal: ① BF typically declines over the course of generation, indicating that model output becomes increasingly constrained, and thus more predictable, with each successive token. ② Among various factors, alignment tuning (e.g., RLHF) exerts the strongest and most consistent impact, sharply compressing the branching factor by nearly an order of magnitude (e.g., 12 1.2). This pronounced narrowing offers quantitative basis for the reduced output variance and decoding sensitivity observed in aligned models, highlighting key behavioral divergence from their base counterparts. Is this reduction in BF merely reflection of concentrated token probabilities, or does it indicate that the model is actively honing in on narrower target space? To probe this, we conduct resampling experiments, requiring the model to choose an alternative to its top-ranked token at an intermediate generation step. We observe sharp drop in accuracy, suggesting that aligned models not only concentrate probability mass but also commit to specific generative pathways early in the process. Building on these theoretical and empirical insights, we also hypothesize and empirically validate that aligned CoT models exhibit particularly low BF and reduced output variability under majority voting. This stability arises because CoT encourages long reasoning chains, shifting key information to later tokensprecisely where BF tends to be lowest. As result, different completions often converge to similar answers, making CoT natural stabilizer in generation. Why does alignment tuning exert such dominant effect on BF? Inspired by the superficial alignment hypothesis (Zhou et al., 2024) and recent advances in tuning-free alignment (Lin et al., 2023; Fei et al., 2024), we hypothesize that base models already encode low-entropy conditional distributions, and that alignment tuning primarily steers generation toward certain stylistic tokens (e.g., Sure), thereby narrowing the conditional output space. To test this, we replicate nudging experiments (Fei et al., 2024) as form of tuning-free alignment. We find that when conditioning base models on prefixes typically produced by aligned models, BF drops more rapidly than when conditioning on self-generated prefixes. This supports our hypothesis that base models already contain low-entropy subspaces, which alignment surfaces rather than fundamentally reshaping. In summary, by measuring LLM probability concentration via BF ( 4), our contributions are: 2 ① We find that aligned models possess BF of around 1.2, nearly an order of magnitude lower than their base counterparts. This low BF helps explain reduced output diversity and randomness for aligned models. Also, the BF diminishes as generation progresses, reducing the influence of the decoding method further and suggesting LLMs become more predictable as they generate. ( 5) ② Using this framework, we uncover an unexpected source of stability in complex reasoning. We show that aligned CoT models, by generating extended reasoning chains, pushes generation into later, more deterministic (lower BF) regions. This indicates CoT stabilizes generations. ( 6) ③ Perhaps most surprisingly, we find alignment surfaces low-entropy trajectories already latent in base models. Our evidence suggests when conditioning base models on low-probability prefixes typically produced by aligned models, BF drops more rapidly than under self-generated prefixes. This raises important questions about how to achieve alignment while preserving the rich generative capacity of base models. ( 7)"
        },
        {
            "title": "2 Background",
            "content": "Autoregressive Language Models LLMs are typically trained to predict the next token and the probability of output (y1:N x; θ) can be decomposed as: (y1:N x; θ) = ΠN t=1P (yt[x, y1:t1]; θ), where y1:t1 is the output up to position 1, θ is the model parameter, and is the prompt. Each output sample is generated via token-by-token sampling, and the generation of multiple samples naturally forms search tree (Yao et al., 2023; Hao et al., 2023; Wan et al., 2024). Modern LLMs would go through multiple training stages. In this paper, we would use base models to refer to the models trained without alignment tuning techniques (Touvron et al., 2023), including instruction tuning and Reinforcement Learning from Human Feedback (RLHF) (Ouyang et al., 2022; Bai et al., 2022) (e.g., Llama-2-13B (Touvron et al., 2023)) and aligned models to refer to models undergoing these additional fine-tuning stages (e.g., Llama-2-13B-Chat). Decoding Methods as Truncated Sampling Though LLMs are trained with large vocabulary size , in many cases, the desired tokens often concentrate on much smaller set of tokens under distribution (ytx, y1:t1; θ). Common decoding methods (Holtzman et al., 2020; Hewitt et al., 2022) utilize this observation and propose various heuristics to truncate vocabulary as Vt at each step t. The next token is then sampled from the renormalized distribution (yt[x, y1:t1]; θ): (yt[x, y1:t1]; θ) = (cid:40) (ytx,y1:t1;θ) ytVt (ytx,y1:t1;θ) (cid:80) 0 yt Vt o.w. (1) Token-wise Conditional Entropy Since tokens are sampled from the truncated distribution , we use to compute the token-level conditional entropy for given prefix instance y1:t1:1 (Yt[x, y1:t1]; θ) = (cid:88) yt (yt[x, y1:t1]; θ) log (yt[x, y1:t1]; θ) (2) To generalize, we can compute the expected conditional entropy over the distribution of prefix sequences Y1:t1: (Yt[x, Y1:t1]; θ) = Ey1:t1 (Yt[x, y1:t1]; θ)."
        },
        {
            "title": "3 Case Study: Is Decoding Method Still Crucial for Modern LLMs?",
            "content": "Many prevalent decoding methods were introduced before LLMs scaled to billions of parameters and underwent multiple training stages. Additionally, model developers adopt different decoding strategies when reporting LLM capabilities (Touvron et al., 2023; Dubey et al., 2024; Yang et al., 2024; Guo et al., 2025), raising questions about the significance of decoding choices for modern LLMs. To explore this, we benchmark various decoding methods on standard LLM reasoning tasks, extending prior work (Song et al., 2024; Renze and Guven, 2024) to the latest models including DeepSeek-distlled models (Guo et al., 2025), which would generate long CoT before the final answer.2 1The common convention setting 0 log 0 = 0 for entropy computation is followed. 2For Llama-3 series models, in our prior study, we find there is only minor performance difference among Llama-3, Llama-3.1, Llama-3.2, and Llama-3.3. We mainly use Llama-3 in this paper as it includes the most diverse collection of models. 3 Specifically, we evaluate model performance on MMLU-STEM (Hendrycks et al., 2021) under CoT prompting across different temperatures (T = 0.6/1.0) in temperature sampling and truncation thresholds (p = 0.9/1.0) in nucleus sampling (Holtzman et al., 2020). Further implementation details can be found in Appendix A. Models Default (T = 0.6, = 0.9) = 0.6, = 1.0 = 1.0, = 0.9 Min (T = 1.0, = 1.0) Llama-3-70B-Instruct Llama-3-70B DeepSeek-R1-Distill-Llama-8B Llama-3.1-8B-Instruct Llama-3.1-8B 77.60 ( 2.23) 74.00 ( 3.80) 65.70 ( 3.84) 61.50 ( 4.37) 53.50 ( 4.92) Table 1: Experiment Results across decoding methods on STEM subset of MMLU. We follow the common practice of using 5-shot CoT prompting. DefaultMin Default % indicates the maximum relative performance drop when deviating from the default decoding configuration. 77.50 ( 2.60) 72.00 ( 4.38) 62.70 ( 4.14) 57.50 ( 4.92) 47.00 ( 5.21) 78.50 ( 2.09) 78.00 ( 3.52) 66.30 ( 3.51) 63.00 ( 4.01) 54.00 ( 4.61) 75.90 ( 2.85) 63.50 ( 5.02) 59.70 ( 4.65) 50.50 ( 5.34) 37.00 ( 5.48) DefaultMin Default % 3.31 18.59 9.95 19.84 31.48 The results in Table 1 reveal that for aligned models, decoding configurations have limited impact typically around 10% (up to 20%) relative performance changes. Among Llama-3.1-8B models, DeepSeek-distilled Llama-8B (based on Llama-3.1-8B), which is trained to generate long CoT, exhibits the smallest relative performance changes. In contrast, base models exhibit greater sensitivity, with performance varying by up to 31%. Additionally, lowering the temperature (T ) generally improves performance across all models more than adjusting truncation threshold (p), though excessive reduction (e.g., greedy decoding when = 0) may lead to repetition issues (Guo et al., 2025). Based on these observations and findings in existing literature, we propose the following hypotheses: Hypo 1 Aligned models produce tokens with more concentrated distribution than base models (Padmakumar and He, 2024; Bigelow et al., 2024; Lu et al., 2025; West and Potts, 2025). Hypo 2 Larger models have more concentrated distributions compared with smaller models (Ye et al., 2024; Xiong et al., 2024), though may varied by tasks (Lu et al., 2025; West and Potts, 2025). Hypo 3 As LLMs generate more tokens, its next-word prediction probability distribution becomes increasingly concentrated (Tian et al., 2024; Chakrabarty et al., 2024). Researchers often assess probability concentration using token-level metrics such as entropy or log-likelihood. However, these offer only narrow lens on model behavior: they capture local properties but miss the global structure of the output spacehow probability mass is distributed across plausible sequences. This motivates our proposal of the BF as structural measure of generative breadth."
        },
        {
            "title": "4 Measuring Branching Factor",
            "content": "The generative process of language models can be viewed as moving down branching tree, with each token choice selecting path forward. While the full tree spans O(V ) sequences for vocabulary size and sequence length , LLMs concentrate probability mass on far smaller subset. To capture how many options the model seriously considers at each step, we introduce the Branching Factor (BF). Given high-probability leaf sequences, we approximate the tree as balanced B-ary tree, where = 1/N . In this section, we describe how to compute and in practice. 4.1 Intuitive Perspective: Exponentiated Entropy (Perplexity) as Branches We propose to use the exponentiated entropy (perplexity) to quantify : def= exp (H(Y1:N x; θ)). This reflects the effective number of equally probable outcomes with the same total uncertainty (OConnor, 2013).3 Analogously, it is like sampling from fair -sided die, where entropy equals = H(Y1:N x; θ). Thus, B(x; θ) = exp (cid:0) H(Y1:N x; θ)(cid:1) where H(Y1:N x; θ) = (cid:80) 1 (Y1:N x; θ) is the averaged entropy per output token up to position . larger B(x; θ) indicates 1 greater potential for diverse outputs. log 1 3In the field of ecology and information theory (Hill, 1973; Cover, 1999; Jost, 2006), the exponentiated entropy is often used as an diversity index, which is an alternative term to uncertainty. 4 (a) MMLU (b) BBCNewsLatest (c) MMLU (Std) (d) BBCNewsLatest (Std) Figure 2: Empirical verification of AEP for Llama-3-8B-Instruct. (a, b): length-averaged NLL closely tracks length-averaged Entropy. (c, d): Standard deviation of length-averaged NLL diminishes with output length. For short outputs, where its tractable to sample sufficiently many sequences to closely estimate the conditional entropy at each position, we can estimate the BF by computing the conditional entropy at each position and then aggregating as: B(x; θ) exp 1 M (cid:88) (cid:80)y(i) t=1 i=1 H(Yt[x, y(i) y(i) 1:t1]; θ) (3) where H(Yt[x, y(i) 1:t1]; θ) is the entropy of the distribution at position for sample i. 4.2 Practical BF Estimator via Asymptotic Equipartition Property While the above approach works well for short outputs, it becomes challenging for longer sequences, as we can only sample tiny fraction of the exponentially large output space. In such cases, we show that when LLMs generate sufficiently long outputs, the average log-probability of each output sequence will be roughly the same, and can approximate average output entropy well, following the Asymptotic Equipartition Property (AEP) (Shannon, 1948; Breiman, 1957; Cover, 1999). The original AEP proof requires additional assumptions about the generation process, such as that it needs to be stationary and ergodic, often violated by LLMs. But as noted by Mudireddy et al. (2024), these assumptions are unnecessary if we do not require (Y1:N x; θ) to converge to constant: Theorem 4.1 (AEP for LLMs) Given 0 < ϵ < 1, we have: lim (cid:18)(cid:12) (cid:12) (cid:12) (cid:12) 1 log (y1:N x; θ) (Y1:N x; θ) (cid:19) < ϵ = 1 (cid:12) (cid:12) (cid:12) (cid:12) (4) Theorem 4.1 is equivalent to the statement: for sufficiently large , the probability of any length-N high-probability output y1:N under can be approximated as exp (cid:0)N H(Y1:N x; θ)(cid:1), rendering log-probability asymptotically ineffective for distinguishing among them. The proof can be found in Appendix E.4 As an empirical demonstration, we plot the standard deviation of the average negative log-likelihood of Llama-3-8B-Instruct over multiple datasets5 in Figure 2, where we can see that with the increased output length, the difference between length-averaged entropy and negative log-likelihood (NLL) is reduced, and the standard deviation of average NLL also quickly reduces within the first 50 output tokens. Therefore, for long sequences, we can estimate BF using the NLL of sampled sequences as: (cid:32) B(x; θ) exp 1 (cid:88) i=1 1 y(i) log (cid:0)y1:y(i)x; θ(cid:1) (cid:33) (5) This approach allows us to compute BF in sample-efficient way. For task-wise BF, we simply compute it via averaging all instance-wise BF: B(X; θ) = (cid:80) p(x)B(x; θ). 4We provide simplified proof with minor changes to the original proof of Mudireddy et al. (2024). 5For dataset-specific details, we refer readers to Appendix B. 5 (a) Creative StoryGen (b) Random Strings (c) BBCNewsLatest Figure 3: Shrinking BF with output length over various tasks for Llama-3-70B and Llama-370B-Instruct. For better visualization, we compute the exponential moving averaged values of BF with the smoothing factor set as 0.1."
        },
        {
            "title": "5 Benchmarking and Attributing Branch Factors",
            "content": "In this part, we will introduce our BF computation experiment settings, including models, tasks, and the impact factors influencing BF. Models and Sampling We run experiments on models from Llama-2 (Touvron et al., 2023) and Llama-3 (Dubey et al., 2024) families as they are widely-used open-weight model families. For each model family, we include both base and aligned models to investigate how alignment tuning affects BF. We set = 0.9 and = 1.0 to sample outputs to conform with the setting for most datasets. We set = 50 sequences to estimate BF, which yields reliable estimation across datasets in prior studies. For aligned models, we apply the official chat templates to prompts. In addition, we carefully control the lengths of all inputs plus outputs to be within the context window of the models. Tasks We consider variety of tasks covering common application scenarios of LLM generation, including reasoning and open-ended generation: MMLU (Hendrycks et al., 2021) (Reasoning), Cognac (Chen et al., 2022) (Controlled Generation), BBCLatestNews (Li et al., 2024b) (News Generation), and Creative Story Generation (Chakrabarty et al., 2024) (Creative Generation). To test subjective randomness bias (Bigelow et al., 2024), we also prepare synthetic task Random Strings where the prompt is generated via randomly sampled characters. See Appendix for dataset details. Impact Factors (IFs) We consider modulating these factors that may impact BF computations: Prompt Complexity (C), Alignment Tuning (AT {Instruct, Base}), Model Size (S {8B/13B, 70B}), and Model Generation (G {2, 3}). controls the informativeness of the input prompt (e.g., the number of banned words in Cognac, the number of in-context samples in MMLU). Intuitively, providing more information in should make the model more confident in its outputs, resulting in lower BF. Dataset-specific setups for are detailed in Appendix B. AT, S, represent model-wise variations to explore how different configurations of θ affect B(X; θ). 5.1 BF Dynamic in Generation Process Both BF and the output length are functions of the output Y, and BF computation relies on . To avoid confounding effects, we first analyze how BF varies with before intervening IFs In Figure 3, we demonstrate BF trajectories over different output positions by running Llama-3-70B 6 (a) Cognac (b) MMLU (c) BBCNews (d) Creative StoryGen Figure 4: Pareto Analysis of BF across various IFs. AT indicates whether the model is aligned. denotes the prompt complexity. refers to model size, and refers to model generation (Llama-2 vs. Llama-3). Across all settings, alignment tuning has the most pronounced impact on BF. and Llama-3-70B-Instruct on three representative tasks. Specifically, we compute BF over every five output tokens, conditioning on the prompt and all previously generated output tokens.6 As we can see, first, the average BF for the base model ( 12) is roughly ten times higher than the aligned model ( 1.2). Therefore, there are actually very few candidate next-token to be truncated in decoding for the aligned models. This explains why the decoding method would assert weaker effects for aligned models, as we see in 3. Also, in most cases, BF would often drop smoothly as more output tokens are generated. Under the same task, when > 0, different mainly controls the starting point and the rate of decreasing, while in the end, they would converge to roughly the same point. When almost zero knowledge is provided (C = 0), the output will end much earlier compared to > 0 cases. These findings also provide support that the future token generation is gradually becoming predictable and the model may have certain generation plan to follow, resonating with recent observation in interpretability (Pal et al., 2023; Wu et al., 2024; Li et al., 2024a) and inference acceleration (Cai et al., 2024; Welleck et al., 2024). We further examine potential confounds such as prompt likelihood and data contamination in Appendix I, and find they do not fully account for the observed BF reductions. 5.2 Pareto Analysis of BF We perform Pareto analysis to identify the relative influence of all IFs of BF. For each factor Di, we define the unnormalized Impact I(Di) as the average absolute pairwise difference in BF when varying Di while holding other dimensions constant: (cid:80) I(Di) = di,dj Domain(Di),di=dj Avg(B(Di = di)) Avg(B(Di = dj)) Domain(Di) Domain(Di) 1 . (6) Then we normalize it as I(Di) = I(Di) (cid:80) I(Di) . The results, shown in Figure 4, indicate that alignment tuning is the most influential factor affecting BF, surpassing model size, model generation, and prompt complexity by large margin. For tasks with richer inputssuch as MMLU (with more in-context examples) and BBCLatestNews (with more headlines)prompt complexity and model size emerge as the next most impactful factors. In contrast, for open-ended tasks like Cognac and Story Generation, model generation Gparticularly improvements from Llama-2 to Llama-3plays more dominant role. This shift likely reflects gains the use of larger, more diverse datasets in training (Dubey et al., 2024). Intuitively, greater prompt specificity (larger C) reduces BF Curious Case of Prompt Complexity by narrowing the models output space through more informative context. However, our experimental results reveal task-varied effects. As illustrated in Figure 5 for the Cognac task, greater prompt complexity can increase BFpotentially due to the cognitive burden of processing negation or complex linguistic structures. In contrast, for tasks like News Generation, higher generally leads to lower BF, consistent with the expected narrowing of output diversity. Comprehensive task-wise BF results are provided in Appendix D. 6See full results for running other models and tasks in Appendix C. 7 (a) Cognac (b) BBCNewsLatest Figure 5: Task-varied influence of prompt complexity on BF. On Cognac, we see BF increases with increased C, while on BBCNewsLatest, increasing can lead to reduced BF. Model Maj@1 Std Maj@3 Std Maj@8 Std Maj@16 Std BF DeepSeek-R1-Distill-Llama-70B Llama-3-70B-Instruct Llama-3-70B DeepSeek-R1-Distill-Llama-8B Llama-3.1-8B-Instruct Llama-3.1-8B 14.34 16.37 27.78 27.10 31.54 36. 8.29 11.40 19.53 20.91 24.64 29.78 4.99 7.50 13.22 13.93 17.30 20.43 3.21 5.12 9.23 9.14 12.90 14. 1.23 1.28 1.31 1.23 1.31 1.35 Table 2: Majority Voting@K standard deviation on MMLU-STEM with 200 samples. We compute the standard deviation over 100 bootstrapping trials, each using 64 samples per instance. We set = 0.6, = 0.9 to match standard benchmarking settings, differing from = 1.0, = 0.9 setup in 5. Lower temperature concentrates probability mass on fewer tokens, reducing BF and making direct comparisons more difficult. Still, BF remains strong predictor of standard deviation. Figure 6: Resampling from different output positions to assess the effect of interrupting BF reduction. We resample new continuations at the 25th and 200th output token of DeepSeek-Distilled Llama-8B MMLU outputs. Results show substantial performance drops at both positions."
        },
        {
            "title": "6 Application: Variance Reduction and Risks of Mid-Generation Forking",
            "content": "Building on our findings that BF declines over the generation process ( 5.1) and is lower in aligned models ( 5.2), we derive practical implication: aligned CoT models, by starting with low BF and delaying decisive tokens, shrink the output space more aggressively and produce fewer highprobability variants. To test this, we evaluate output variability on MMLU-STEM using 200 samples per model, measuring the standard deviation of Majority@K accuracy for = 1, 3, 8, 16 under temperature = 0.6 and truncation threshold = 0.9. As shown in Table 2, among models with similar capacity, those with lower BFespecially the aligned CoT modelexhibit markedly lower variance. This confirms that BF is reliable predictor of sampling stability. 8 But is this narrowing of BF merely reflection of concentrated token probabilities, or does it reflect deeper commitment to specific generative paths? To examine this, we conduct resampling experiment: at various points in the generation process, we force the model to take different path by replacing the remainder of the output with newly sampled continuation. As shown in Figure 6, performance drops sharply when resampling occurs at later, lower-BF position in the sequence. This suggests that aligned models arent just concentrating probability mass locally, but are actively locking into trajectories, making late-stage deviations more error-prone. In practice, this highlights key application of BF: parallel sampling should be applied early, while BF remains high, to ensure meaningful diversity and avoid quality degradation. (a) Output BF Dynamics (b) Nudging Ratio Histogram Figure 7: Nudging Experiments over Just-Eval-Instruct dataset."
        },
        {
            "title": "7 How does Alignment Tuning Impact BF?",
            "content": "Why does alignment tuning exert such pronounced effect on BF? Building on the superficial alignment hypothesis (Zhou et al., 2024) (Alignment tuning might simply teach base LLMs to select subdistribution of data formats for interacting with users.) and recent work on tuning-free alignment (Lin et al., 2023; Fei et al., 2024), we hypothesize that base models already encode low-entropy conditional distributions. In this view, alignment tuning doesnt reshape generation from scratch, but instead nudge the model toward stylistic tokens (e.g., Sure), thereby narrowing the conditional output distribution. To test this hypothesis, we reproduce the nudging experiments (Fei et al., 2024), over Just-EvalInstruct (Lin et al., 2023) and MMLU datasets. We employ Llama-3-70B for drafting most outputs. However, when the base models Top-1 probability is low, we apply nudging by switching to Llama-38B-Instruct to generate single word. BF was computed as in prior experiments. The results, shown in Figure 7,7 indicate that after most nudging occurs early in the generation process indicating the prefix generated by the nudging model is of low probability. These observations collectively support our hypothesis. Considering that nudging not only reduces BF but also improves aligned model performance on these tasks (Fei et al., 2024), our results highlight the dual effect of alignment training: reducing BF while preserving or even enhancing task performance."
        },
        {
            "title": "8 Related Works",
            "content": "Uncertainty Quantification for LLM Uncertainty quantification (UQ) for LLMs has gained significant attention due to its importance in real-world applications, particularly in high-stakes domains (Desai and Durrett, 2020; Jiang et al., 2021; Wang et al., 2022; Kadavath et al., 2022; Xiong et al., 2024; Ye et al., 2024; Gupta et al., 2024). Existing methods typically address closeddomain tasks such as classification and question-answering, where outputs are discrete and easier to assess. However, as Kuhn et al. (2023) note, these approaches often overlook challenges specific to open-ended generation, such as semantic equivalence across outputs. They introduce semantic entropy to quantify uncertainty in LLM output space by first clustering the sampled output and then quantifying uncertainty over cluster distribution. This method empirically works well in hallucination detection (Farquhar et al., 2024). In this paper, we focus on investigating the probability concentration phenomenon for LLMs. We introduce BF to quantify this concentration, whichapplies broadly across tasks without imposing strong assumptions on output categories. 7We present results on Just-Eval-Instruct in short of space. MMLU results are included in Appendix F. 9 Reduced Diversity in Aligned Models Recent studies have consistently shown that alignment tuning reduces output diversity in language models (Perez et al., 2022; Padmakumar and He, 2024; Chakrabarty et al., 2024; Tian et al., 2024; Kirk et al., 2024; Lu et al., 2025; West and Potts, 2025). Our work aims at connecting reduced diversity with related observations on diminished randomness and robustness in aligned models (Saparov and He, 2023; Song et al., 2024; Renze and Guven, 2024; Bigelow et al., 2024), and proposes unifying explanation: increased probability concentration. Traditional diversity metrics such as n-gram lexical diversity (Li et al., 2016) are sensitive to vocabulary size and output length (Liu et al., 2022; Tevet and Berant, 2021; Guo et al., 2024; Kirk et al., 2024) and cannot work well with most recent long CoT models. In Appendix H, we demonstrate that lexical diversity poorly correlates with BF and fails to robustly measure generation concentration. Our work also resonates with information density research in cognitive science and linguistic theories, and we present short discussion in Appendix G."
        },
        {
            "title": "9 Conclusion",
            "content": "We investigate probability concentration in LLMs through the lens of branching factor (BF). Aligned models exhibit BF values nearly an order of magnitude lower than their base counterparts, with BF declining further during generation. This helps explain their reduced output diversity, low sampling variance, and insensitivity to decoding strategies. We predict and verify that aligned CoT models, due to their especially low BF, produce more stable outputs and mid-generation resampling yields degraded performance by forcing unlikely continuations. Nudging experiments further support our hypothesis that alignment narrows generation not by reshaping the model, but by steering it toward stylistic tokens that activate low-entropy subspaces already present in the base model. Open questions remain: which components of alignment tuning drive these effects, and how does reduced BF shape user interaction in more complex, real-world tasks? Future work may build on our findings to design inference-time decoding strategies that better balance diversity and stability."
        },
        {
            "title": "Acknowledgement",
            "content": "We are grateful for the constructive comments and feedback from Peter West (Stanford & UBC), Zhaoran Wang (Northwestern), Zhiyuan Hu (NUS), Tenghao Huang (USC), Tuhin Chakrabarty (Salesforce & Stony Brook), and Hao Zhu (Stanford) (names are not listed in particular order) and insightful discussions with UChicago C&I community."
        },
        {
            "title": "References",
            "content": "Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862, 2022. Eric Bigelow, Ekdeep Singh Lubana, Robert P. Dick, Hidenori Tanaka, and Tomer Ullman. Incontext learning dynamics with random binary sequences. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=62K7mALO2q. Leo Breiman. The individual ergodic theorem of information theory. The Annals of Mathematical Statistics, 28(3):809811, 1957. Tianle Cai, Yuhong Li, Zhengyang Geng, Hongwu Peng, Jason D. Lee, Deming Chen, and Tri Dao. Medusa: Simple LLM inference acceleration framework with multiple decoding heads. In Forty-first International Conference on Machine Learning, 2024. URL https://openreview. net/forum?id=PEpbUobfJv. Tuhin Chakrabarty, Philippe Laban, Divyansh Agarwal, Smaranda Muresan, and Chien-Sheng Wu. Art or artifice? large language models and the false promise of creativity. In Proceedings of the CHI Conference on Human Factors in Computing Systems, pages 134, 2024. Howard Chen, Huihan Li, Danqi Chen, and Karthik Narasimhan. Cognac: Controllable text generation with language constraints. In arXiv, 2022. 10 Joel Cohen. Markovs inequality and chebyshevs inequality for tail probabilities: sharper image. The American Statistician, 69(1):57, 2015. Thomas Cover. Elements of information theory. John Wiley & Sons, 1999. Shrey Desai and Greg Durrett. Calibration of pre-trained transformers. arXiv preprint arXiv:2003.07892, 2020. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Sebastian Farquhar, Jannik Kossen, Lorenz Kuhn, and Yarin Gal. Detecting hallucinations in large language models using semantic entropy. Nature, 630(8017):625630, 2024. Yu Fei, Yasaman Razeghi, and Sameer Singh. Nudging: Inference-time alignment via model collaboration. arXiv preprint arXiv:2410.09300, 2024. Yao Fu, Litu Ou, Mingyu Chen, Yuhao Wan, Hao Peng, and Tushar Khot. Chain-of-thought hub: continuous effort to measure large language models reasoning performance. arXiv preprint arXiv:2305.17306, 2023. Dmitriy Genzel and Eugene Charniak. Entropy rate constancy in text. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics, pages 199206, 2002. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Yanzhu Guo, Guokan Shang, and Chloé Clavel. Benchmarking linguistic diversity of large language models. arXiv preprint arXiv:2412.10271, 2024. Neha Gupta, Harikrishna Narasimhan, Wittawat Jitkrittum, Ankit Singh Rawat, Aditya Krishna Menon, and Sanjiv Kumar. Language model cascades: Token-level uncertainty and beyond. In The Twelfth International Conference on Learning Representations, 2024. Shibo Hao, Yi Gu, Haodi Ma, Joshua Hong, Zhen Wang, Daisy Wang, and Zhiting Hu. Reasoning with language model is planning with world model. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 81548173, 2023. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=d7KBjmI3GmQ. John Hewitt, Christopher Manning, and Percy Liang. Truncation sampling as language model desmoothing. In Findings of the Association for Computational Linguistics: EMNLP 2022, pages 34143427, 2022. Mark Hill. Diversity and evenness: unifying notation and its consequences. Ecology, 54(2): 427432, 1973. Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of neural text degeneration. In International Conference on Learning Representations, 2020. URL https: //openreview.net/forum?id=rygGQyrFvH. Jaeger and Roger Levy. Speakers optimize information density through syntactic reduction. Advances in neural information processing systems, 19, 2006. Zhengbao Jiang, Jun Araki, Haibo Ding, and Graham Neubig. How can we know when language models know? on the calibration of language models for question answering. Transactions of the Association for Computational Linguistics, 9:962977, 2021. Lou Jost. Entropy and diversity. Oikos, 113(2):363375, 2006. 11 Saurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain, Ethan Perez, Nicholas Schiefer, Zac Hatfield-Dodds, Nova DasSarma, Eli Tran-Johnson, et al. Language models (mostly) know what they know. arXiv preprint arXiv:2207.05221, 2022. Robert Kirk, Ishita Mediratta, Christoforos Nalmpantis, Jelena Luketina, Eric Hambro, Edward Grefenstette, and Roberta Raileanu. Understanding the effects of RLHF on LLM generalisation and diversity. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=PXD3FAVHJT. Lorenz Kuhn, Yarin Gal, and Sebastian Farquhar. Semantic uncertainty: Linguistic invariances for uncertainty estimation in natural language generation. In The Eleventh International Conference on Learning Representations, 2023. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023. Roger Levy. Expectation-based syntactic comprehension. Cognition, 106(3):11261177, 2008. Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao, and William Dolan. diversity-promoting objective function for neural conversation models. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 110119, 2016. Margaret Li, Weijia Shi, Artidoro Pagnoni, Peter West, and Ari Holtzman. Predicting vs. acting: trade-off between world modeling & agent modeling. arXiv preprint arXiv:2407.02446, 2024a. Yucheng Li, Frank Guerin, and Chenghua Lin. Latesteval: Addressing data contamination in language model evaluation through dynamic and time-sensitive test construction. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 1860018607, 2024b. Bill Yuchen Lin, Abhilasha Ravichander, Ximing Lu, Nouha Dziri, Melanie Sclar, Khyathi Chandu, Chandra Bhagavatula, and Yejin Choi. The unlocking spell on base llms: Rethinking alignment via in-context learning. In The Twelfth International Conference on Learning Representations, 2023. Siyang Liu, Sahand Sabour, Yinhe Zheng, Pei Ke, Xiaoyan Zhu, and Minlie Huang. Rethinking and refining the distinct metric. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio, editors, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 762770, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-short.86. URL https://aclanthology.org/2022. acl-short.86/. Ximing Lu, Melanie Sclar, Skyler Hallinan, Niloofar Mireshghallah, Jiacheng Liu, Seungju Han, Allyson Ettinger, Liwei Jiang, Khyathi Chandu, Nouha Dziri, and Yejin Choi. AI as humanitys salieri: Quantifying linguistic creativity of language models via systematic attribution of machine text against web text. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?id=ilOEOIqolQ. Kyle Mahowald, Evelina Fedorenko, Steven Piantadosi, and Edward Gibson. Info/information theory: Speakers choose shorter words in predictive contexts. Cognition, 126(2):313318, 2013. Clara Meister, Tiago Pimentel, Patrick Haller, Lena Jäger, Ryan Cotterell, and Roger Levy. Revisiting the uniform information density hypothesis. arXiv preprint arXiv:2109.11635, 2021. William Merrill and Ashish Sabharwal. The parallelism tradeoff: Limitations of log-precision transformers. Transactions of the Association for Computational Linguistics, 11:531545, 2023. George Miller. Wordnet: lexical database for english. Communications of the ACM, 38(11): 3941, 1995. Avinash Mudireddy, Tyler Bell, and Raghu Mudumbai. Slaves to the law of large numbers: An asymptotic equipartition property for perplexity in generative language models. arXiv preprint arXiv:2405.13798, 2024. 12 OConnor. index, Brendan versity perplexity-as-branching-factor-as-shannon-diversity-index/. 2024-09-28. dibranching https://brenocon.com/blog/2013/01/ Accessed: as URL"
        },
        {
            "title": "Perplexity",
            "content": "shannon factor 2013. as Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:27730 27744, 2022. Vishakh Padmakumar and He He. Does writing with language models reduce content diversity? In The Twelfth International Conference on Learning Representations, 2024. URL https:// openreview.net/forum?id=Feiz5HtCD0. Koyena Pal, Jiuding Sun, Andrew Yuan, Byron Wallace, and David Bau. Future lens: Anticipating subsequent tokens from single hidden state. In Proceedings of the 27th Conference on Computational Natural Language Learning (CoNLL), pages 548560, 2023. Ethan Perez, Saffron Huang, Francis Song, Trevor Cai, Roman Ring, John Aslanides, Amelia Glaese, Nat McAleese, and Geoffrey Irving. Red teaming language models with language models. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 34193448, 2022. Matthew Renze and Erhan Guven. The effect of sampling temperature on problem solving in large language models. arXiv preprint arXiv:2402.05201, 2024. Abulhair Saparov and He He. Language models are greedy reasoners: systematic formal analysis of chain-of-thought. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=qFVVBzXxR2V. Claude Shannon. mathematical theory of communication. The Bell system technical journal, (3):379423, 1948. Weijia Shi, Anirudh Ajith, Mengzhou Xia, Yangsibo Huang, Daogao Liu, Terra Blevins, Danqi Chen, and Luke Zettlemoyer. Detecting pretraining data from large language models. In The Twelfth International Conference on Learning Representations, 2024. Yifan Song, Guoyin Wang, Sujian Li, and Bill Yuchen Lin. The good, the bad, and the greedy: Evaluation of llms should not ignore non-determinism. arXiv preprint arXiv:2407.10457, 2024. Guy Tevet and Jonathan Berant. Evaluating the evaluation of diversity in natural language generation. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 326346, 2021. Yufei Tian, Tenghao Huang, Miri Liu, Derek Jiang, Alexander Spangher, Muhao Chen, Jonathan May, and Nanyun Peng. Are large language models capable of generating human-level narratives? In EMNLP, 2024. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. Vivek Verma, Nicholas Tomlin, and Dan Klein. Revisiting entropy rate constancy in text. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 1553715549, 2023. Ziyu Wan, Xidong Feng, Muning Wen, Stephen Marcus McAleer, Ying Wen, Weinan Zhang, and Jun Wang. Alphazero-like tree-search can guide large language model decoding and training. In Forty-first International Conference on Machine Learning, 2024. Yuxia Wang, Daniel Beck, Timothy Baldwin, and Karin Verspoor. Uncertainty estimation and reduction of pre-trained models for text regression. Transactions of the Association for Computational Linguistics, 10:680696, 2022. 13 Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. In NeurIPS, 2022. Sean Welleck, Amanda Bertsch, Matthew Finlayson, Hailey Schoelkopf, Alex Xie, Graham Neubig, Ilia Kulikov, and Zaid Harchaoui. From decoding to meta-generation: Inference-time algorithms for large language models. Transactions on Machine Learning Research, 2024. ISSN 2835-8856. URL https://openreview.net/forum?id=eskQMcIbMS. Survey Certification. Peter West and Christopher Potts. Base models beat aligned models at randomness and creativity. arXiv preprint arXiv:2505.00047, 2025. Wilson Wu, John Morris, and Lionel Levine. Do language models plan ahead for future tokens? In The First Conference on Language Modeling, 2024. Miao Xiong, Zhiyuan Hu, Xinyang Lu, YIFEI LI, Jie Fu, Junxian He, and Bryan Hooi. Can LLMs express their uncertainty? an empirical evaluation of confidence elicitation in LLMs. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/ forum?id=gjeQKFxFpZ. An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, et al. Qwen2 technical report. arXiv preprint arXiv:2407.10671, 2024. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. Advances in neural information processing systems, 36:1180911822, 2023. Fanghua Ye, Mingming Yang, Jianhui Pang, Longyue Wang, Derek Wong, Emine Yilmaz, Shuming Shi, and Zhaopeng Tu. Benchmarking llms via uncertainty quantification. arXiv preprint arXiv:2401.12794, 2024. Chunting Zhou, Pengfei Liu, Puxin Xu, Srinivasan Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, et al. Lima: Less is more for alignment. Advances in Neural Information Processing Systems, 36, 2024. 14 Figure 8: BF Output Dynamic for Llama-2-families. For better visualization, we compute the exponential moving averaged values of perplexity with the smoothing factor set as 0.1."
        },
        {
            "title": "A Case Study Implementation Details",
            "content": "We use the scripts in Qwen-2.5-Math (Yang et al., 2024) for standard reasoning benchmarks.8 We sample 200 examples from MMLU-STEM and compute the performance numbers under 64 trials and report the average performance. Dataset-Specific Processing For all datasets we used in the paper, we carefully controlled whether the prompt length and expected output length would exceed the models maximum length. MMLU (Hendrycks et al., 2021) is widely-used multiple-choice reasoning question. Unless otherwise explained, we use the full test set of MMLU to avoid potential contamination, following benchmarking settings reported in most LLM technical reports(Touvron et al., 2023; Dubey et al., 2024; Guo et al., 2025). We formulate prompt complexity as the number of in-context samples. For example, = 1 means we only add one in-context sample. For prompting setup and postprocessing details, we follow the standard implementation in Qwen-2.5-Math (Yang et al., 2024). (Chen et al., 2022) is controlled generation task requiring language model not to generate Cognac specified banned words provided in the prompt. We use the WordNet subset (Miller, 1995) of Cognac as this is the only released setting in Cognac paper, where the topic is root node and the constraint 8https://github.com/QwenLM/Qwen2.5-Math/tree/main 15 (a) Creative StoryGen (b) Random Strings (c) BBCNewsLatest (d) MMLU Figure 9: BF Output Dynamic for Llama-3-families. For better visualization, we compute the exponential moving averaged values of perplexity with the smoothing factor set as 0.1. is defined as subtree. We sampled 200 instances using the provided data generation codes in our experiments. To ensure most model generations ended properly in the decoding process, we relax the constraint of maximum decoded tokens from 60 to 512. We use the same prompt templates following their Github repo.9 Creative Story Generation (Chakrabarty et al., 2024) provides the plots and story continuation from both machine and human. We adopt the provided 11 human-written story plots in the original dataset as the prompt. In this task, we set the maximum token = 1024 to ensure the continued story written by LLM can have proper ending. We formulate prompt complexity as providing 25 words in the plot. Random Strings Similar to Bigelow et al. (2024), we sample 200 random strings with length (256, 512) from the tokenizer vocabulary as the prompt. Prompt complexity is formulated by providing 15 tokens in the prompt, ensuring each article contains at least 100 tokens. BBCLatestNews (Li et al., 2024b) is news collection dataset aims at collecting news that is beyond the time cut for training LLMs. Unlike creative story plots, news articles are typically more structured and organized, although headlines can still be surprising. We select news articles from January to July 2024 to minimize data contamination, as the Llama models have knowledge cut-off in late 2023. We formulate prompt complexity as providing 15 words in the plot. 9https://github.com/princeton-nlp/Cognac/tree/main"
        },
        {
            "title": "C Full BF Output Dynamics Investigation",
            "content": "Here we present full task-wise and model-wise BF output dynamic for Llama-2 in Figure 8 and Llama-3 in Figure 9. We can observe the trends as in 5.1: ① The average BF for the base model ( 12) is roughly ten times higher than the aligned model ( 1.2). ② BF would often drop smoothly as more output tokens are generated. Full Task-wise BF Evaluation on Different Prompt Complexity The full task-wise BF evaluation results over different prompt complexity can be found in Figure 10. Here we can see that prompt complexity modulates BF in highly non-consistent ways across models and tasks, and there are no clear monotonic patterns, contradicting the intuition that with more context given, the model should have more confidence in what to generate. (a) Cognac (b) Creative StoryGen (c) Random Strings Figure 10: BF changes with prompt complexity (C) for Different Tasks. We can see prompt complexity affects BF in task-varied way. (d) BBCNewsLatest"
        },
        {
            "title": "E Proof of AEP and other collaries",
            "content": "E.1 Proof of AEP The proof below is simplified version of the proof in (Mudireddy et al., 2024). For more formal measure-theoretical proof, we refer readers to their original papers for more details. We present the simplified proof here mostly for completeness of the paper and to fix some minor issues when deriving bounds. The key observation here is that under current computation architecture, the probability implemented by transformers are log-precision (Merrill and Sabharwal, 2023), and thus log (y1:N x; θ) is bounded (e.g., log (y1:N x; θ) . For the truncated probability (y1:N x; θ), we can essentially only consider the non-zero probability over truncated vocabulary and the same thing holds. Depending on the quantization scheme implemented, examples of include 32, 64, etc. 17 Then, note that (Y1:N x; θ) is actually the first moment of variable log (y1:N x; θ) divided by , if we can further show that the second moment of log (y1:N x; θ) is bounded by some G(N ) which only increases linearly in , by Chebyshev Inequality (Cohen, 2015), we can prove: (cid:18)(cid:12) (cid:12) (cid:12) (cid:12)"
        },
        {
            "title": "1\nN",
            "content": "(cid:12) (cid:12) log (y1:N x; θ) (Y1:N x; θ) (cid:12) (cid:12) (cid:19) ϵ Var(log (y1:N x; θ)) 2ϵ2 G(N ) 2ϵ2 Then taking on both sides, we will see: (cid:18)(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) log (y1:N x; θ) (Y1:N x; θ) (cid:12) (cid:12)"
        },
        {
            "title": "1\nN",
            "content": "(cid:19) ϵ 0 Equivalently: lim (cid:18)(cid:12) (cid:12) (cid:12) (cid:12)"
        },
        {
            "title": "1\nN",
            "content": "log (y1:N x; θ) (Y1:N x; θ) (cid:19) < ϵ = 1 (cid:12) (cid:12) (cid:12) (cid:12) and we can complete the proof. Now lets prove specific choice of G(N ) = 2 upper-bounds Var [log (y1:N x; θ)]. First, we note that (7) (8) (9) log (y1:N x; θ) = (Y1:N x; θ) = (cid:88) t=1 (cid:88) t=1 log (yt[x; y1:t1]; θ) (10) (Yt[x; Y1:t1]; θ) = (cid:88) t=1 Ey1:t1 [H (Yt[x, y1:t1]; θ)] Var [log (yt[x, y1:t1]; θ)] = [log (yt[x, y1:t1]; θ) (Yt[x; Y1:t1]; θ)]2 = E[log2 (yt[x, y1:t1]; θ)] 2 (Yt[x; Y1:t1]; θ) 2 (11) (12) The last line is because log (y1:N x; θ) . Following Mudireddy et al. (2024), we use induction on to prove Var [log (y1:N x; θ)] 2. = 1 trivially holds as Var [log (y1[x]; θ)] 1 2 (directly set = 1 in Equation (12)). Assuming Var [log (y1:N x; θ)] 2 holds for = K, for = + 1, we have: Var [log (y1:K+1x; θ)] = [log (y1:K+1x; θ) (Y1:K+1x; θ)]2 = E[log (y1:Kx; θ) (Y1:Kx; θ) + log (yK+1[x, y1:K]; θ) H(YK+1[x, Y1:K]; θ)]2 = E[C(K)]2 + 2 E[C(K)D(K + 1)] + E[D(K + 1)]2 KM 2 + 2 E[C(K)D(K + 1)] + 2 where: C(K) = log (y1:Kx; θ) (Y1:Kx; θ) D(K + 1) = log (yK+1[x, y<K+1]; θ) H(YK+1[x, Y1:K]; θ) Using the law of iterated expectation, we can compute E[C(K)D(K + 1)] as: E[C(K)D(K + 1)] = Ey1:K E[C(K)D(K + 1)y1:K ] (13) (14) (15) = Ey1:K [C(K)E[D(K + 1)y1:K ]] = Ey1:K (16) (cid:2)C(K)EyK+1y1:K [log (yK+1y1:Kx; θ) H(YK+1[x, y1:K]; θ)](cid:3) (17) (18) Equation (16) is because C(K) is deterministic function when y1:K has been realized. Equation (17) is because H(YK+1y1:K)] = E[log (yK+1y1:Kx; θ)]. Plug Equation (18) in Equation (13), we can obtain Var [log (y1:K+1x; θ)] (K + 1)M 2, which completes the induction. With this specific choice of G(N ) = 2, we can prove Equation (9). = Ey1:K [C(K) 0] ="
        },
        {
            "title": "F Full Nudging Experiment Results",
            "content": "Due to space limits, we put the nudging experiment results for MMLU here. Though on MMLU, nudging does not reduce BF that quickly as over Just-Eval-Instruct, it does bring down BF of base models significantly, which verifies our hypothesis in 7. (a) Just-Eval-Instruct (b) MMLU Figure 11: Output Perplexity Dynamics in Nudging Experiments. (a) Just-Eval-Instruct (b) MMLU Figure 12: Nudging Ratio Histogram."
        },
        {
            "title": "G BF and Information Density",
            "content": "Our BF measure can also be interpreted as capturing the information density that LLMs target to facilitate efficient communication (Genzel and Charniak, 2002; Jaeger and Levy, 2006; Levy, 2008; Mahowald et al., 2013; Meister et al., 2021; Verma et al., 2023). Prior work has leveraged both token-level log-probabilities and entropy rates ( H) as proxies for information density in human and machine communication. In Theorem 4.1, we formalize the connection between these views, showing that BFdefined as the exponentiated entropy ratealigns naturally with this theoretical framework. Unlike prior studies focused primarily on linguistic theory or cognitive science, our work operationalizes this principle at scale across modern LLMs, linking information density to alignment training, decoding dynamics, and output variability in unified analysis."
        },
        {
            "title": "H Lexical Diversity and BF",
            "content": "Following the branching factor (BF) analysis in 2, higher BF suggests greater lexical diversity in finite samples. To examine the relationship between BF and traditional diversity metrics, we compute Distinct-N (Li et al., 2016), incorporating necessary LLM-specific adaptations (Tevet and Berant, 2021; Guo et al., 2024; Kirk et al., 2024). We then conduct correlation analysis between Distinct-N and BF. Figure 13 presents the Signed R2 and Spearman correlation results between Distinct-N and BF on Cognac, MMLU, BBCNewsLatest and Creative Story Generation datasets. We can see there exists no consistent correlation across models; both strong positive and negative correlations are observed, and in some cases, they are entirely uncorrelated (e.g., Llama-3-70B-Instruct on Cognac at Figure 13b). 19 (a) Signed R2(Distinct-1, BF), MaxLength=5 (b) Signed R2(Distinct-1, BF), MaxLength=50 (c) Signed R2(Distinct-2, BF), MaxLength=5 (d) Signed R2(Distinct-2, BF), MaxLength= (e) Signed R2(Distinct-4, BF), MaxLength=5 (f) Signed R2(Distinct-4, BF), MaxLength=50 Figure 13: Correlational Analysis of BF and Distinct-N. We can find there is no consistent correlation between Distinct-N and BF. Additionally, this correlation is sensitive to generation length, as Distinct-N is biased by output length and can be manipulated, as noted by Liu et al. (2022). While the EAD metric (Liu et al., 2022) mitigates this issue, it remains influenced by vocabulary size and is not model-agnostic. Confounder Investigation: Data Contamination potential confounder in our analysis is the influence of data contamination. If prompts closely resemble the training data (including pretraining and alignment tuning, i.e., \"data contamination\"), 20 Figure 14: Signed R2 values heatmap investigating correlation between EBF and Min-K %. smaller BF values would be expected, and vice versa. To evaluate this, we use the Min-K% metric (Shi et al., 2024), which quantifies the overlap between experimental prompts and training data. Following Shi et al. (2024), we set = 20 and compute the average log-likelihood for the minimum K% of tokens. Using these Min-K% values, we perform linear regression with BF to assess their correlation. For each task-model pair, Signed R2 values are reported to indicate the strength and sign (positive or negative) of the correlation. The results of the Min-K% analysis are presented in Figure 14. Significant negative correlations between BF and Min-K% are observed for models such as Llama-3-8B-Instruct, Llama-3-70BInstruct, and Llama-2-70B-Chat across several tasks. Conversely, Llama-3-8B and Llama-2-13B-Chat models exhibit positive correlations. For other models, correlations are notably weaker. Overall, there is no consistent correlation pattern between BF and Min-K% across datasets and models, suggesting that data contamination cannot fully explain our findings."
        },
        {
            "title": "J Computational Resources",
            "content": "We use internal A100/A40 clusters to run our experiments. Each A100 server has at most 500GB memory and 4 A100 GPU cards, while each A40 server has at most 500GB memory and 4 A40 GPU cards. For each individual run, we reserve at most 500 GB memory for 12 hours. The most time-consuming and compute-intensive jobs are for running aligned CoT models to generate long CoT and collect statistics in the generation process. Dataset License Cognac (Chen et al., 2022) MMLU (Hendrycks et al., 2021) BBCNewsLatest (Li et al., 2024b) Creative Story Generation (Chakrabarty et al., 2024) BSD 3-Clause Just-Eval-Instruct (Lin et al., 2023) Chain-of-Thought Hub (Fu et al., 2023) N/A MIT N/A N/A MIT Table 3: Dataset license information"
        },
        {
            "title": "K License",
            "content": "Codes Our implementation heavily relies on vLLM (Kwon et al., 2023) to do LLM inference and statistic collection. vLLM is an open-source LLM inference library adopting Apache-2.0 license. For standard reasoning evaluation, we use open-sourced codes from Qwen-2.5-math (Yang et al., 2024)10 to do preprocessing and postprocessing, which does not provide license for the GitHub repository. 10https://github.com/QwenLM/Qwen2.5-Math 21 Datasets We provide the license information for each dataset we used in Table 3. Models We use Llama-2 (Touvron et al., 2023), Llama-3 (Dubey et al., 2024) and DeepSeekdistilled Llama models (Guo et al., 2025). Llama-2 is licensed under Llama 2 Community License Agreement, and Llama-3 is licensed under META LLAMA 3 COMMUNITY LICENSE AGREEMENT. DeepSeek-distilled Llama models are licensed under MIT License."
        }
    ],
    "affiliations": [
        "Department of Computer Science, University of Chicago"
    ]
}