{
    "paper_title": "Bielik v3 Small: Technical Report",
    "authors": [
        "Krzysztof Ociepa",
        "Łukasz Flis",
        "Remigiusz Kinas",
        "Krzysztof Wróbel",
        "Adrian Gwoździej"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce Bielik v3, a series of parameter-efficient generative text models (1.5B and 4.5B) optimized for Polish language processing. These models demonstrate that smaller, well-optimized architectures can achieve performance comparable to much larger counterparts while requiring substantially fewer computational resources. Our approach incorporates several key innovations: a custom Polish tokenizer (APT4) that significantly improves token efficiency, Weighted Instruction Cross-Entropy Loss to balance learning across instruction types, and Adaptive Learning Rate that dynamically adjusts based on training progress. Trained on a meticulously curated corpus of 292 billion tokens spanning 303 million documents, these models excel across multiple benchmarks, including the Open PL LLM Leaderboard, Complex Polish Text Understanding Benchmark, Polish EQ-Bench, and Polish Medical Leaderboard. The 4.5B parameter model achieves results competitive with models 2-3 times its size, while the 1.5B model delivers strong performance despite its extremely compact profile. These advances establish new benchmarks for parameter-efficient language modeling in less-represented languages, making high-quality Polish language AI more accessible for resource-constrained applications."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 ] . [ 2 0 5 5 2 0 . 5 0 5 2 : r BIELIK V3 SMALL: TECHNICAL REPORT"
        },
        {
            "title": "THE BIELIK LLM TEAM",
            "content": "Krzysztof Ociepa1,4, Łukasz Flis1,2, Remigiusz Kinas1, Krzysztof Wróbel1,3,5, Adrian Gwozdziej1, 2 1SpeakLeash, 2ACK Cyfronet AGH, 3Jagiellonian University, 4Azurro, 5Enelpol"
        },
        {
            "title": "ABSTRACT",
            "content": "We introduce Bielik v3, series of parameter-efficient generative text models (1.5B and 4.5B) optimized for Polish language processing. These models demonstrate that smaller, well-optimized architectures can achieve performance comparable to much larger counterparts while requiring substantially fewer computational resources. Our approach incorporates key innovations, including custom Polish tokenizer (APT4) that significantly improves token efficiency and Adaptive Learning Rate that dynamically adjusts based on training progress. Trained on meticulously curated corpus of 292 billion tokens spanning 303 million documents, these models excel across multiple benchmarks, including the Open PL LLM Leaderboard, Complex Polish Text Understanding Benchmark, Polish EQ-Bench, and Polish Medical Leaderboard. The 4.5B parameter model achieves results competitive with models 2-3 times its size, while the 1.5B model delivers strong performance despite its extremely compact profile. These advances establish new benchmarks for parameter-efficient language modeling in less-represented languages, making high-quality Polish language AI more accessible for resourceconstrained applications."
        },
        {
            "title": "Introduction",
            "content": "The rapid advancement in natural language processing (NLP) has led to the development of increasingly sophisticated language models that can understand and generate human-like text. These models have shown remarkable success in various linguistic tasks across multiple languages. However, the development of high-performing models for lessresourced languages remains significant challenge due to the scarcity of large and diverse datasets and computational resources. Several notable efforts have advanced Polish language modeling in recent years. TRURL 2 Voicelab [2023], collection of fine-tuned Llama 2 models with 7 billion and 13 billion parameters, was trained on approximately 1 million conversational samples. Qra National Information Processing Institute and Gdansk University of Technology [2024] models, comprising continuously pretrained architectures with 1, 7, and 13 billion parameters, leveraged 90 billion tokens of Polish data. More recently, PLLuM, developed by consortium of Polish academic institutions, introduced models ranging from 8 billion to 70 billion parameters, created through continued pretraining of Llama and Mistral models on Polish corpora. While these initiatives have made important strides, they often face limitations in performance, versatility, or accessibility, frequently requiring significantly larger computational resources for comparable performance. Building on our previous work with Bielik 7B v0.1 Ociepa et al. [2024] and Bielik 11B v2 Ociepa et al. [2025], we introduce the Bielik v3 series of generative text models optimized specifically for Polish language processing. These models, with sizes of 1.5B and 4.5B parameters, represent significant advancement in parameter-efficient language modeling. By adopting an innovative approach to model scaling and training, we demonstrate that smaller, well-optimized models can achieve performance comparable to much larger counterparts while requiring substantially fewer computational resources. Our approach introduces several key technical innovations. First, we employ depth up-scaling to adapt Qwen2.5 models Qwen et al. [2025], replacing the original tokenizer with custom-developed Polish tokenizer (APT4) that significantly improves token efficiency for Polish texts. Second, we utilize Adaptive Learning Rate, which dynamically adjusts the Bielik v3 PREPRINT learning rate based on training progress and context length. These techniques, combined with comprehensive training on diverse corpus of 292 billion tokens across 303 million documents, enable the Bielik v3 models to achieve remarkable performance despite their compact size. Our evaluation demonstrates that Bielik v3 models outperform many larger models across various benchmarks, including the Open PL LLM Leaderboard, Complex Polish Text Understanding Benchmark (CPTUB), Polish EQ-Bench, and Polish Medical Leaderboard. Notably, the 4.5B parameter model achieves results competitive with models 2-3 times its size, while the 1.5B model delivers strong performance despite its extremely compact profile. This efficiency makes Bielik v3 particularly valuable for deployment in resource-constrained environments while maintaining high-quality Polish language capabilities. In the following sections, we detail the model architecture and tokenizer design of Bielik v3, describe our comprehensive data preparation methodology, discuss the pre-training and post-training processes, and evaluate the models performance across multiple benchmarks. We also analyze the models limitations and potential biases. Our results demonstrate that Bielik v3 not only advances the state of Polish language understanding but also establishes new benchmarks for parameter-efficient language modeling in less-represented languages."
        },
        {
            "title": "2 Model and Tokenizer",
            "content": "In this section, we introduce the model design and tokenizer, presenting architectural decisions and configurations. resources 2.1 Model Architecture Parameter 1.5B 4.5B Layers Model Dimension Attention Heads Key/Value Heads Head Size Intermediate Size Activation Function Attention Bias MLP Bias Vocabulary Size Positional Embeddings RoPE (θ = 1000000) RoPE (θ = 1000000) Context Length 60 2048 16 2 128 11008 SwiGLU True True 32000 32 1536 12 2 128 8960 SwiGLU True True 32000 32768 8192 Table 1: Architecture details of the 1.5B and 4.5B parameter models. Bielik v3 models are based on the Transformer architecture Vaswani et al. [2017], with key parameters listed in Table 1. The design integrates several advanced techniques to enhance performance and efficiency. Self-attention with causal masks Vaswani et al. [2017] enables the model to assign varying importance to different parts of the input sequence. The causal mask ensures that the model only attends to preceding tokens, preserving the autoregressive property essential for language modeling. Grouped-query attention (GQA) Ainslie et al. [2023] reduces both computational complexity and memory usage while maintaining model quality. It achieves this by using fewer key-value heads than query heads, enabling more efficient handling of long sequences. SwiGLU activation function Dauphin et al. [2016], Shazeer [2020] combines the Swish activation function with Gated Linear Units (GLU), providing better performance and trainability than traditional activation functions such as ReLU. Rotary Positional Embeddings (RoPE) Su et al. [2024] enhance the models ability to capture relative token positions. Compared to absolute positional embeddings, RoPE supports better generalization to longer sequences and improves performance in tasks requiring positional sensitivity. Root Mean Square Layer Normalization (RMSNorm) Jiang et al. [2024] normalizes activations within the network, offering greater training stability and slightly faster computation compared to standard Layer Normalization. Bielik v3 PREPRINT Pre-normalization involves applying layer normalization before the self-attention and feed-forward layers. This improves model convergence and overall performance. The Bielik v3 models, with sizes 1.5B and 4.5B, are adapted from the Qwen2.5 1.5B and 3B models Qwen et al. [2025]. The models were scaled using the Depth Up-Scaling method Kim et al. [2024], the tokenizer was replaced, and further pretraining was conducted, as presented in Figure 1. The decision to build on an existing model rather than developing one from scratch was motivated by the desire to allocate resources efficiently. By focusing on the linguistic adaptation of an already high-performing model, we were able to optimize both time and computational resources. The Qwen2.5 models were selected due to their strong benchmark performance and permissive Apache 2.0 license. Figure 1: Bielik 4.5B v3 model upscaling via Depth Up-Scaling (n = 36, = 8, = 56) with tokenizer replacement and outermost layer duplication. 2.2 Tokenizer Polish English Tokenizer Vocab Size Avg tokens Tokens CpT TpW Tokens CpT TpW APT3 APT4 Mistral v0.1 Qwen2. 31980 32000 32000 151665 480 503 578 499 344 375 747 625 5.22 4.78 2.40 2.87 1.48 1.62 3.22 2.69 615 631 408 3.15 3.07 4.75 5.19 1.93 1.98 1.28 1.17 Table 2: Comparison of token count, characters per token (CpT), and tokens per word (TpW) for the preamble of the Constitution of the Republic of Poland in Polish and English, processed by various tokenizers: APT3 and APT4 (Polish-specific tokenizers), Mistral v0.1 and Qwen2.5 (multilingual tokenizers with limited Polish support). To enhance tokenization efficiency for the Polish language, we replaced the original tokenizer of the Qwen language model with our custom-developed Polish tokenizer, APT. This modification aimed to reduce the number of tokens 3 Bielik A PREPRINT required to represent input and output sequences, thereby enabling the model to handle longer contexts within its attention window and generate outputs more efficiently. Such improvements are particularly beneficial for smaller models, where token budget constraints are more pronounced. One way to assess the effectiveness of the tokenization process is by analyzing the number of tokens generated for given input. lower token count generally indicates more efficient and faster text generation by the language model. The tokenizers used in the Mistral 7B and Qwen2.5 models were not specifically trained for the Polish language. Therefore, we decided to switch to tokenizer trained primarily for Polish, with some support for English. In addition to token count, we also considered how the tokenizer segments textparticularly whether it separates digits, punctuation, and special characters into distinct tokens, which can significantly impact the quality of generated responses. As result, we ultimately chose to use our own APT4 tokenizer, which is successor to the APT3 tokenizer from the Polish APT3 model Ociepa and Azurro Team [2024]. Adapting the model to the new tokenizer necessitated reinitializing the token embedding matrix to accommodate the altered vocabulary. We evaluated several embedding initialization methods: Random Initialization: Assigns random vectors to new tokens, effectively requiring the model to learn embeddings from scratch, which can be inefficient and slow to converge. Frequency-based Vocabulary Transfer (FVT) Yuan et al. [2022]: Initializes embeddings for new tokens by averaging the embeddings of their constituent subword units from the original tokenizer, leveraging frequency information to inform the transfer. Linear Interpolation (aX + b): Applies linear transformation to map embeddings from the source tokenizers space to the target tokenizers space, aiming to preserve relational structures between tokens. WECHSEL Minixhofer et al. [2022]: Utilizes multilingual static word embeddings to identify semantically similar tokens between source and target vocabularies, initializing new token embeddings based on these similarities . FOCUS (Fast Overlapping Token Combinations Using Sparsemax) Dobler and de Melo [2023]: Represents new tokens as sparse combinations of overlapping tokens from the source and target vocabularies, selected based on semantic similarity in an auxiliary embedding space . OFA (One For All) Liu et al. [2023]: Leverages external multilingual word embeddings to initialize unseen subword embeddings, facilitating efficient adaptation of pretrained models to new languages . RAMEN Tran [2020]: Employs alignment techniques, such as bilingual dictionaries or cross-lingual embeddings, to map source language embeddings to target language tokens, aiding in the transfer of pretrained models to new languages. After comparative analysis, we selected the FOCUS method for initializing embeddings corresponding to the APT tokenizer. FOCUSs approach of constructing new token embeddings as sparse combinations of semantically similar overlapping tokens proved effective in preserving the models performance while accommodating the new tokenizer. To assess the efficacy of this tokenizer replacement and embedding initialization, we monitored the initial training loss of the model, providing insights into the immediate impact of these modifications on the models learning dynamics. After tokenizer change we duplicated the outermost layers of the model twice, allowing room for adaptation to the new embeddings. Next, we froze the entire model except for the duplicated layers and the embeddings, and trained it on dataset containing 56 billion tokens. After this initial adaptation, we unfroze the entire model and continued training. We selected the preamble of the Constitution of the Republic of Poland as the benchmark text because it effectively represents the style of Polish formal writing and is also available in an official English translation, enabling meaningful comparison. Table 2 provides detailed comparison of key metrics such as token count, characters per token (CpT), and tokens per word (TpW), demonstrating the relative performance of different tokenizers on both language versions of the preamble."
        },
        {
            "title": "3 Pre-training",
            "content": "The primary objective of the pre-training phase was to enhance the models proficiency in the Polish language, with an emphasis on both accuracy and fluency. To achieve this, we utilized diverse collection of high-quality Polish texts. These materials underwent rigorous preprocessing and thorough quality evaluation to ensure the highest standards of training data, as shown in Tables 4, 5, and 6. 4 Bielik v3 PREPRINT 3.1 Pre-training Data The pre-training of the Bielik v3 models involved constructing novel, diverse, and high-quality dataset composed primarily of Polish-language texts. We leveraged resources from the SpeakLeash project SpeakLeash Team [2024]. Using metadata associated with each documentincluding topical information and various stylometric featureswe selected 294 million documents from different datasets, ensuring both high quality and thematic diversity. These selected texts underwent comprehensive cleaning and quality evaluation, as described in Sections 3.1.1 and 3.1.2. Additionally, we excluded documents where scraping was technically permitted (i.e., not blocked by robots.txt) but where the terms and conditions explicitly prohibited use for training language models. Only documents meeting our stringent quality standards were retained and subsequently tokenized. This meticulous curation resulted in Polish training corpus of 237 billion tokens. To improve the models adaptation to Polish while mitigating catastrophic forgetting Li et al. [2022], Ostapenko et al. [2022], Ibrahim et al. [2024], we supplemented the dataset with English texts from the SlimPajama dataset Soboleva et al. [2023], known for its diversity and quality. To support the models readiness for later training stages, we included the instruction dataset from Section 5.2 as part of the pre-training corpus. Originally intended for supervised fine-tuning (SFT), this data contributed to more seamless and efficient progression into the subsequent phases of training. In total, the final training dataset comprised 292 billion tokens (303 million documents). 3.1.1 Data Cleanup To enhance the quality of the documents, we applied series of heuristics designed to remove corrupted or irrelevant content, anonymize personal data (including physical addresses, email addresses, phone numbers, and URLs), and resolve encoding or formatting issues. These steps produced cleaner, higher-quality texts that were subsequently subjected to further evaluation. 3.1.2 Quality Evaluation To develop the training dataset for text quality evaluation, we manually selected and annotated documents, categorizing them into three quality classes: HIGH, MEDIUM, and LOW. The HIGH class represents superior-quality documents, LOW denotes poor-quality texts, and MEDIUM encompasses documents whose quality is ambiguous, falling between high and low standards. This nuanced classification approach addresses the inherent complexities in assessing textual quality. The Bielik v3 dataset comprises 44 344 training documents, 3 000 test documents, and 1 000 validation documents. In addition to real-world texts, we introduced synthetic samples designed to probe the classifiers sensitivity to wide range of quality degradations. Synthetic Data Categories HIGH-quality synthetics Well-structured and linguistically fluent Markdown documents, demonstrating high factual coherence, clarity of expression, and grammatical correctness in Polish. These examples represent ideal outputs generated under constrained decoding settings. MEDIUM-quality synthetics Passages that are locally fluent and structurally plausible but exhibit moderate issues such as inconsistent tense, mild repetition, or topic drift. They reflect borderline cases between usable and discardable outputs. LOW-quality synthetics Outputs generated under high-temperature sampling (e.g., temperature > 1.1), often formatted correctly in Markdown, but semantically incoherent, factually incorrect, or disorganized. This category also includes examples showing characteristic LLM failures such as looping or hallucinated content despite visually clean structure. Additionally, it covers cases of extremely poor machine translationespecially from languages structurally distant from Polish, such as Chinesewhere the resulting text, while superficially grammatical, is conceptually broken, mistranslated, or entirely nonsensical. These are now reliably detected and classified as low-quality. Furthermore, we have introduced metrics aimed at detecting potentially looped or repetitive content, which sometimes occurred during text generation or editing using the Bielik v2 language model. These metrics effectively identify texts that may appear well-formatted and conceptually strong, but contain signs of excessive repetition or looping, raising concerns about content quality despite their otherwise high surface coherence. 5 Bielik A PREPRINT These synthetic examples serve as both strong positives and strong negatives, enabling the model to move beyond surface-level features and develop greater sensitivity to subtle semantic and structural failures. Stylometric Feature Set To support classification, we designed an expanded set of stylometric features, aimed at capturing surface and deep characteristics of text typical for LLM-generated output. While the Bielik v2 classifier relied on 150 stylometric and Markdown-aware descriptors, Bielik v3 increases this to 200, introducing novel features tailored to detect generation artefacts, degraded machine translation, and formatting inconsistencies. Lexical richness and repetition Features such as unique_word_count, hapax_legomena_ratio, and looping_suspicion help separate repetitive from genuinely varied prose. Diacritic and encoding hygiene Counters of Polish diacritics, the replacement_char_ratio, and related metrics expose corrupted character conversions. Sentenceand line-level structure Ratios of interrogative sentences or single-word lines flag unusual formatting that often correlates with low factual quality. Readability indices The lix and rix scores provide continuous proxies for extremely verbose or overly terse fragments. NER-based coherence The distribution of entity types (person, organisation, location, miscellaneous) helps detect hallucinations or long stretches of proper nouns without context. Morphosyntactic diversity Features measuring variation in case, tense, and mood penalise unnaturally uniform or erratically shifting narratives. Part-of-speechweighted top-word ratios Balances of nouns, verbs, and adjectives among the most frequent tokens reveal content focus versus padding. Feature extraction and modelling We compute these features with an extended StyloMetrix-inspired pipeline Okulska et al. [2023], augmented with Unicode handling and dependency parser for the new descriptors. Among the classifiers tested, XGBoost again achieved the highest macro-F1 on the validation split; detailed feature-importance rankings and ablations are reported in Tables 3 and 4. The performance of the Bielik v3 quality classifier was rigorously evaluated on held-out validation set. The model achieved an overall accuracy of 95% and macro-average F1-score of 0.85, demonstrating strong and balanced performance across all three quality categories. Notably, the classifier reached an F1-score of 0.97 for both the HIGH and LOW classes, indicating excellent precision and recall in distinguishing clearly defined outputs. Despite lower recall in the MEDIUM category (0.51), which reflects its inherently ambiguous nature, the model maintains reliable boundaries between highand low-quality textsan essential requirement for downstream data curation. Quality threshold for corpus construction manual audit of 1000 validation documents confirmed that predicted probability P(HIGH) > 0.50 and P(MEDIUM) > 80 remains an effective cutoff. Documents below this threshold are excluded from the instruction-tuning corpus used in downstream Bielik v3 training. With the larger dataset, purpose-built synthetic adversaries, and richer 200-dimensional feature vector, the Bielik v3 quality classifier more precisely detects both overt and subtle degradations, providing robust filter for large-scale LLM data. Best model configuration. The best-performing model, XGB_RegL1L2_d8_n250_lr008_a02_l05, is an XGBoostClassifier trained with the following hyperparameters: n_estimators = 250, max_depth = 8, learning_rate = 0.08, subsample = 0.75, colsample_bytree = 0.8, reg_alpha = 0.2, reg_lambda = 0.5. The model uses the multi:softprob objective, was trained with eval_metric = mlogloss, and fixed random_state = 42. Label encoding was disabled via use_label_encoder = False. 6 Bielik v3 PREPRINT Model Val F1 (macro) Val F1 (weighted) Val Accuracy XGB_RegL1L2_d8_n250_lr008_a02_l05 XGB_AggressiveSubsample_d8_n300_lr007 XGB_RegL1_d7_n200_lr01_a05 CatBoost XGB_RegL2_d6_n300_lr005_l1 XGBoost_nEstimators500_maxDepth6_lr007_gamma15 XGB_StrongRegCombo_d7_n400_lr003_g2_mcw4_a01_l01 XGBoost_nEstimators400_maxDepth3_lr025_gamma05 XGB_VeryDeep_LowLR_d15_n200_lr005_reg HistGradientBoosting XGBoost_nEstimators180_maxDepth12_lr004_minChild5 XGBoost_nEstimators100_maxDepth6_lr01 XGBoost_nEstimators200_maxDepth8_lr005 LightGBM XGBoost MLP_hidden100_relu_adam EBM MLP_hidden100_50_relu_lbfgs TabNet MLP_hidden50_50_tanh_sgd RandomForest_nEstimators300_maxDepth20_minSamples 0.852303 0.841951 0.841951 0.841948 0.838424 0.836765 0.834315 0.832164 0.830983 0.828799 0.828858 0.823295 0.825671 0.825986 0.817084 0.806844 0.799630 0.794733 0.793772 0.796052 0.777787 0.941526 0.938001 0.938001 0.937998 0.937647 0.936393 0.933820 0.934450 0.934794 0.933364 0.931842 0.931375 0.931104 0.930186 0.927873 0.913972 0.914568 0.915277 0.917838 0.912717 0.914178 Table 3: Comparison of model performance on the validation set 0.946 0.943 0.943 0.943 0.943 0.941 0.939 0.938 0.941 0.938 0.938 0.937 0.937 0.934 0.933 0.915 0.921 0.919 0.920 0.917 0.926 Class LOW MEDIUM HIGH Accuracy Macro avg Weighted avg Precision Recall F1-score Support 0.95 0.79 0.95 0.90 0. 0.98 0.51 0.98 0.95 0.82 0.95 0.97 0.62 0.97 0.85 0.94 461 74 465 1000 1000 Table 4: Classification Report (Validation) Best Quality Classifier Model Class LOW MEDIUM HIGH Accuracy Macro avg Weighted avg Precision Recall F1-score Support 0.94 0.77 0.95 0.89 0.93 0.97 0.47 0.98 0.94 0.81 0.94 0.96 0.58 0. 0.83 0.93 1384 225 1391 3000 3000 3000 Table 5: Classification Report (Test) Best Quality Classifier Model 7 Bielik A PREPRINT Figure 2: Confusion matrix showing test and validation results for the XGBoost classifier. 3.2 Category Classification: Results and Applications This section presents the performance evaluation of the text category classifier developed to automatically assign documents to one of 120 predefined categories. The model was trained and evaluated on substantial dataset derived from Polish texts, with results summarized in Table 7. Dataset and Setup The dataset used for this task comprised total of 58,294 documents. Following standard machine learning practice, the data was partitioned into training set and held-out test set. The training set consisted of 52,464 documents (approximately 90% of the data), while the test set contained 5,830 documents (approximately 10%). stratified splitting strategy was employed during partitioning to ensure that the proportional representation of each of the 120 categories was maintained in both subsets. This is crucial for reliable evaluation, especially given the large number of potentially imbalanced classes. Modeling Approach The core classification model utilized pipeline architecture. Textual data was first processed using CountVectorizer to convert documents into numerical feature vectors based on word counts, limiting the vocabulary to the 15,000 most frequent terms. These counts were subsequently transformed into Term Frequency-Inverse Document Frequency (TF-IDF) representations using TfidfTransformer, capturing the relative importance of words across the corpus. Classification was performed using Linear Support Vector Classifier (LinearSVC), robust and efficient algorithm for high-dimensional text data. To enable probability estimates and enhance performance, the LinearSVC was wrapped within CalibratedClassifierCV using isotonic calibration (method=isotonic) via 3-fold crossvalidation on the training data. Performance Evaluation The final trained pipeline was evaluated on the unseen test set (5,830 documents), demonstrating strong performance across the 120 categories. The overall accuracy achieved was 94.63%, indicating that the vast majority of documents were correctly classified. The high macro-average scores (all above 0.95) are particularly encouraging, suggesting that the classifier performs well across both common and rare categories. Detailed per-class performance metrics, confusion matrix visualizations, and feature importance analyses were generated to examine specific strengths and weaknesses, identifying categories that may present greater classification challenges. 8 Bielik v3 PREPRINT Feature Mean Abs SHAP oovs average_lines stop_word_ratio polish_diacritics_per_word non_alpha_word_fractions rix short_line_ratio_20 avg_paragraph_length lowercase_ratio_md single_word_line_ratio colons_per_sentence not_allowed_chars_ratio special_chars_ratio_md symbol_to_word_ratio duplicate_line_ratio commas_per_sentence blank_lines_ratio polish_diacritics_ratio char_ratio__ emoticons diacritics_std_dev single_char_ratio contextual_word_repetitions_ratio overall_uppercase_ratio avg_dependency_tree_depth short_line_ratio_10 char_ratio_> 0.5726 0.2524 0.2089 0.1851 0.1617 0.1576 0.1524 0.1481 0.1468 0.1449 0.1321 0.1122 0.1003 0.0873 0.0860 0.0828 0.0801 0.0799 0.0776 0.0759 0.0739 0.0704 0.0699 0.0679 0.0679 0.0662 0.0616 Table 6: Top Features by Mean Absolute SHAP Value for quality classification Metric Precision Recall F1-score Support Macro avg Weighted avg 0.9533 0.9471 0.9563 0. 0.9543 0.9461 5830 5830 Table 7: Category Classification Performance on Test Set (N=5,830) Application and Future Directions The primary objective behind developing the category classifier, alongside the quality classifier, is to curate dataset that is both thematically diverse and of the highest textual quality. This dataset is intended for generating synthetic instruction data, leveraging varied and high-quality document collections. Ensuring both thematic breadth and content excellence will enable the creation of robust synthetic instructions, enhancing the effectiveness and generalization capabilities of downstream instruction-following models."
        },
        {
            "title": "4 Synthetic Data Generation for Instruction Tuning",
            "content": "Building on the high-quality category and quality classification pipelines, we introduce structured process for generating synthetic instruction datasets. This process ensures thematic diversity, high textual quality, and strategic reuse of imperfect data. 4.1 Generation of High-Quality, Thematically Balanced QA Instructions The core of the synthetic data generation for QA task process involves creating instructionresponse pairs from documents classified as HIGH quality and evenly distributed across thematic categories. 9 Bielik v3 PREPRINT Figure 3: Distribution of major thematic categories in the Polish text dataset ( 0.9%) Documents meeting the threshold of P(HIGH) > 0.9 under the Bielik v3 quality classifier are selected. Category labels obtained from the thematic classifier ensure that sampling is performed in balanced manner, avoiding overrepresentation of any single domain. From these carefully curated inputs, task-oriented QA instructions are synthesized using controlled prompting techniques, ensuring coverage of wide variety of instruction types, question styles, and domain-specific nuances. Emphasis is placed on factual coherence, clarity, and naturalness of the generated outputs, maintaining strict alignment with the standards of superior-quality human-authored instructions. 4.2 Data Recycling: Improving Imperfect Texts for Inclusion Not all documents initially meet the strict inclusion criteria. Texts assessed as having medium or borderline quality undergo dedicated recycling process before being incorporated into the base training corpus. Specifically, documents categorized as: HIGH-quality texts with quality model confidence between 50% and 70%, MEDIUM-quality texts with high internal confidence scores, are subjected to automated refinement using the Bielik v2.3 model. The recycling stage primarily addresses superficial but systematic defects, including: Spelling errors and typographical mistakes, Formatting irregularities and excessive or missing punctuation, 10 Bielik v3 PREPRINT Category Other Health Politics, Media & News Sport Travel & Tourism Finance Culinary/Food Religion Electronics Beauty IT & Internet Services Automotive Fashion School/Education Home Offices & Municipalities Construction Books & Literature Games Culture & History Cinema, Movies & Series Law & Law Firms Rescue Services Diving Lotteries Bailiff Services Currency Exchange Plumbing Taxi Security Services Postal Services Count 40,337,136 30,432,887 18,066,594 14,308,164 12,168,387 11,182,892 10,537,704 10,499,588 10,403,465 10,337,631 9,707,525 9,637,918 9,481,582 8,826,348 8,810,490 8,332,005 8,233,476 8,013,594 7,418,487 7,250,791 6,984,202 6,945,003 132,048 130,770 129,202 128,669 117,801 107,582 97,234 59,082 52,035 Table 8: Distribution of text categories predicted by the classifier. Note: The \"Other\" category includes texts where the classifier was uncertain and assigned prediction with less than 20% confidence, as shown in Figure 3. Minor OCR artifacts or inconsistencies, while preserving the underlying semantic content of the text. Following refinement, these recycled documents are reassessed for quality. Only those meeting the revised thresholds are incorporated into the base model training datasets. This approach maximizes data utilization efficiency while maintaining high quality standards. 4.3 Summary of the Synthetic Data Curation Strategy The synthetic data curation framework combines two key pillars: Selective instruction generation from top-tier documents, ensuring thematic diversity and linguistic excellence. Strategic data recycling, repairing and upgrading moderately degraded documents to salvage valuable information without compromising quality. By combining rigorous filtering and thematic balancing for instruction generation, and quality-preserving data recycling for the base training corpus, we construct datasets optimized respectively for instruction tuning and foundational model training, ensuring broad coverage and high factual reliability. 11 Bielik v3 PREPRINT"
        },
        {
            "title": "5 Post-training",
            "content": "Upon completing the pre-training phase, we transitioned to the post-training phase, which focused on further improving the models performance across several domains, including coding, mathematics, logical reasoning, and instruction following. 5.1 Supervised Fine-Tuning To better understand human behavior, the first step involves supervised fine-tuning (SFT), where pretrained language model is adapted using dialogue-style datasets consisting of prompts and corresponding replies. The subsequent sections provide detailed overview of the methods applied during this training process. 5.1.1 Masked Tokens We introduced masked token strategy where the loss function is applied selectively to certain parts of the output. Specifically, we masked the user instructions and control tokens from contributing to the loss Shi et al. [2024]. This approach ensures that training focuses exclusively on the meaningful content tokens, avoiding unwanted optimization signals from non-content parts. 5.1.2 Adaptive Learning Rate Since instruction lengths vary widely, the number of tokens influencing the loss computation can fluctuate. To maintain consistent training dynamics, we implemented an adaptive learning rate (ALR) Ociepa et al. [2024], scaling the base learning rate (LR) according to the square root of the ratio between the current batchs token count (T) and reference batch size (BS): (cid:114) BS (1) 5.2 Supervised Fine-Tuning Data ALR = LR The dataset of instructions and dialogues in the Polish language, created for the Bielik 11B v2 release, was cleaned by removing low-quality samples and those containing various errors and defects. Some existing instructions were also regenerated, and completely new instructions were created using the Bielik 11B v2.3 model. The resulting dataset used for training included over 19 million instructions, totaling more than 13 billion tokens. 5.3 Supervised Fine-Tuning Hyperparameters Training employed the AdamW optimizer with parameters β1 = 0.9, β2 = 0.95, and weight decay of 0.05. The learning rate followed cosine decay schedule, beginning at 7 106 and gradually decreasing to 6 107, with warmup phase of 50 iterations. We used global batch size of 128, with each local batch consisting of single sample. Gradient clipping was performed with maximum norm of 1.0, and mixed-precision training was enabled using the bfloat16 format. To improve efficiency, we applied sample packing, combining multiple dataset samples into single sequence until reaching the maximum allowed sequence length. The model was trained for 1.2 epochs with maximum context length of 8,192 tokens. 5.4 Preference Learning 5.4.1 Preference Training Methods In our exploration of post-training methodologies for aligning Bielik v3 model with human preferences, we conducted extensive evaluations of various preference optimization techniques. Building upon established methods such as Direct Preference Optimization (DPO), its penalized variant DPO-P, and Odds Ratio Preference Optimization (ORPO), we introduced and assessed novel approach: Simple Preference Optimization (SimPO). Direct Preference Optimization (DPO) simplifies the reinforcement learning from human feedback (RLHF) paradigm by eliminating the need for an explicit reward model. It directly optimizes the policy to prefer responses aligned with human preferences. 12 Bielik v3 PREPRINT LDPO(πθ; πref) = E(x,yw,yl)D (cid:20) (cid:18) (cid:18) log σ β log πθ(yw x) πref(yw x) log πθ(yl x) πref(yl x) (cid:19)(cid:19)(cid:21) Here, represents the input prompt, yw and yl are the preferred and less preferred responses, respectively, πθ denotes the models policy, πref is the reference policy, β is scaling parameter, and σ is the sigmoid function. DPO addresses the complexity of RLHF by providing stable and computationally efficient alternative that directly incorporates preference data into the training process. DPO with Penalty (DPO-P) extends DPO by introducing penalty term to account for uncertainty in preference data, mitigating overfitting to noisy or ambiguous annotations. This penalization adjusts the loss function to reduce the influence of uncertain samples, enhancing the robustness of the model to imperfect preference data. The DPO-P loss function is formulated as: LDPOP(πθ; πref) = E(x,yw,yl)D (cid:20) (cid:18) (cid:18) log σ β log πθ(yw x) πref(yw x) log (cid:19) πθ(yl x) πref(yl x) (cid:18) λ max 0, log (cid:19)(cid:19)(cid:21) πref(yw x) πθ(yw x) Here, λ is weighting factor for the penalty term. DPO-P addresses the challenge of aligning models without the overhead of additional reference models or complex training procedures. Odds Ratio Preference Optimization (ORPO) ORPO integrates preference alignment into the supervised fine-tuning (SFT) phase by incorporating an odds ratio-based penalty. This approach eliminates the need for separate reference model and simplifies the training pipeline. The ORPO loss function is formulated as: LORPO = LNLL + λ log (cid:19) (cid:18) πθ(yw x) πθ(yl x) Where LNLL is the negative log-likelihood loss, and λ is weighting factor for the odds ratio term. ORPO addresses the challenge of aligning models without the overhead of additional reference models or complex training procedures. Simple Preference Optimization (SimPO) further streamlines preference optimization by utilizing the average logprobability of sequence as an implicit reward, removing the necessity for reference model. It introduces target reward margin γ to enhance the separation between preferred and less preferred responses. The SimPO objective is defined as: LSimPO = E(x,yw,yl)D (cid:20) (cid:18) log σ β (cid:18) 1 yw log πθ(yw x) log πθ(yl x) γ (cid:19)(cid:19)(cid:21) 1 yl Here, denotes the length of the response, ensuring length normalization, and γ serves as the target reward margin. SimPO addresses the computational and memory inefficiencies associated with reference models, offering more efficient and scalable solution for preference alignment. After conducting extensive evaluations across multiple Polish benchmarksincluding we observed that the penalized variant of Direct Preference Optimization (DPO-P) consistently outperformed other methods such as DPO, ORPO, and SimPO. Despite the simplicity and computational efficiency offered by SimPO, DPO-P demonstrated superior alignment with human preferences, particularly in tasks requiring nuanced reasoning and factual accuracy. 5.4.2 Preference Dataset Compared to the previous Bielik 11B v2 release, for the final phase of post-training with reinforcement learning, we introduced several key enhancements to our Polish preference instruction dataset and response generation pipeline. First, we significantly expanded the dataset to 126,000 instructions (only in Polish language), enriching its diversity and complexity. We also broadened the range of alignment categories to include function calling and tool calling tasks, while substantially increasing the volume of instructions focused on reasoning and mathematics. In addition, we incorporated large number of conversational instructions to better reflect realistic interaction patterns. In the generation of preferred and rejected responses, we used broader set of language models, notably including DeepSeek-V3-0324, alongside models used previously. Despite these improvements, we maintain similar approach to instruction creation and curation, combining manual authoring, perturbation-based enhancement, rigorous deduplication, quality evaluation with reward metamodels, and manual inspection. These refinements ensure that the dataset not only grows in scale but also in quality, better supporting downstream alignment training. 13 Bielik v3 PREPRINT 5.4.3 DPO-Positive Hyperparameters For DPO-Positive (DPO-P) training, we set the loss function parameters to β = 0.1 and λ = 2.5, in accordance with best practices aimed at stabilizing preference-based optimization and preserving the quality of preferred outputs. The optimization was carried out using the AdamW algorithm, configured with β1 = 0.9, β2 = 0.95, and no weight decay. fixed learning rate of 5 107 was used, preceded by 50 warmup steps. The total number of training steps amounted to 1,800. Training was performed with global batch size of 64, where each local batch consisted of single example. Gradient clipping was applied with maximum norm of 1.0. Mixed-precision training was employed using the bfloat16 format. 5.5 Reinforcement Learning For Reinforcement Learning (RL) training stage we employed Group Relative Policy Optimization (GRPO) Shao et al. [2024] to fine-tune our Bielik-4.5B-v3 language model, utilizing curated dataset of 12,000 Polish mathematical problems with verifiable solutions (RLVR). This subset was selected from larger corpus of approximately 100,000 Polish-language math problems developed during the Bielik v3 project. The training process was conducted on an 8GPU H100 cluster (Athena - Cyfronet) using the Volcano Engine Reinforcement Learning (VERL) framework Sheng et al. [2024] , which offers scalable and modular support for large language model (LLM) reinforcement learning workflows. GRPO is reinforcement learning algorithm designed to enhance reasoning capabilities in LLMs by addressing limitations found in traditional methods like Proximal Policy Optimization (PPO). Unlike PPO, which relies on separate value function (critic) to estimate the expected return, GRPO eliminates the need for this component, thereby reducing memory consumption and computational complexity. Instead, GRPO evaluates multiple responses generated for the same prompt, computes their rewards using reward model, and calculates the advantage of each response relative to the groups average reward. This group-based advantage estimation allows the model to update its policy by favoring responses that outperform the average, leading to more stable and efficient training. In GRPO, for each input prompt q, the model generates group of responses {a1, a2, . . . , aG}. Each response ai is evaluated using reward function R(q, ai), which assesses the quality of the response. The mean µ and standard deviation σ of the rewards within the group are computed as follows: µ = 1 (cid:88) i=1 R(q, ai), σ = (cid:118) (cid:117) (cid:117) (cid:116) 1 (cid:88) i=1 (R(q, ai) µ) The advantage Ai of each response ai is then calculated by normalizing its reward relative to the groups statistics: Ai = R(q, ai) µ σ This group-relative advantage estimation allows the model to identify which responses are better or worse compared to others in the same group, without requiring an explicit value function. The policy is updated by maximizing the following objective function, which incorporates the advantage and clipping mechanism to ensure stable updates: L(θ) = 1 (cid:88) i=1 min (ri(θ)Ai, clip(ri(θ), 1 ϵ, 1 + ϵ)Ai) Here, ri(θ) = πθ(aiq) the extent of clipping. πθold (aiq) is the probability ratio between the new and old policies, and ϵ is hyperparameter that controls Additionally, GRPO incorporates Kullback-Leibler (KL) divergence penalty to prevent the updated policy from deviating too much from reference policy πref, typically the pre-trained model before fine-tuning. The KL penalty is added to the loss function as follows: Ltotal(θ) = L(θ) β KL[πθ πref] 14 Bielik v3 PREPRINT Where β is coefficient that balances the importance of the KL penalty. By leveraging group-relative advantages and eliminating the need for value function, GRPO offers more efficient and stable approach to fine-tuning LLMs, particularly in tasks that require complex reasoning, such as mathematical problem-solving. The application of GRPO in our training regimen not only improved the models performance on mathematical tasks but also enhanced its capabilities in other reasoning-intensive areas. This suggests that GRPO effectively strengthens the models general reasoning abilities, making it valuable approach for fine-tuning LLMs across diverse domains. The optimization settings included learning rate of 1 106, global batch size of 128 (with local batch size of 16 per GPU). To ensure stable policy updates, KL divergence regularization was applied with coefficient of 0.001, using the low-variance KL loss type. 5.6 Model Merging Similar to the Bielik 11B v2 release, we applied range of merging strategies to refine model quality; however, the leading method throughout the development of Bielik 11B v3 is the linear merging approach. This method served as the primary technique for improving model quality both after Supervised Fine-Tuning (SFT) and after Reinforcement Learning from Human Feedback (RLHF) stages. By consistently employing linear combinations of models at each phase, we ensured stable integration of improvements while maintaining the stylistic and functional consistency of the model outputs."
        },
        {
            "title": "6 Evaluation",
            "content": "We evaluated the Bielik v3 models on several benchmarks to assess their performance across different language understanding and generation tasks, as detailed in the following subsections. The models were evaluated on the following benchmarks: Open PL LLM Leaderboard (Polish) Polish EQ-Bench (Polish) CPTUB Leaderboard (Polish) Polish Medical Leaderboard (Polish) Polish Linguistic and Cultural Competency Benchmark (PLCC) (Polish) Open LLM Leaderboard MixEval Berkeley Function-Calling Leaderboard 6.1 Open PL LLM Leaderboard The Open PL LLM Leaderboard, based on the Open LLM Leaderboard v1 [Beeching et al., 2023a], evaluates models on various NLP tasks, including: sentiment analysis, categorization, short answer question answering, and text classification, but does not test their conversational capabilities [Wróbel et al., 2024, Ociepa et al., 2024]. The leaderboard utilizes the lm-evaluation-harness framework for model evaluation [Gao et al., 2024]. Tasks: polemo2: Sentiment analysis of online consumer reviews across four domains (medicine, hotels, products, university) with four-class labeling (positive, negative, neutral, ambiguous) [Kocon et al., 2019]; metric: accuracy. klej-ner: Named entity recognition in sentences containing single-type entities, classifying into six categories (no entity, place, person, organization, time, geographical name) [Rybak et al., 2020]; metric: accuracy. 8tags: Topic classification of social media headlines into eight categories (film, history, food, medicine, motorization, work, sport, technology) [Dadas et al., 2020]; metric: accuracy. belebele: Machine reading comprehension for question answering [Bandarkar et al., 2024]; metric: accuracy. 15 Bielik v3 PREPRINT dyk: Question answering based on human-annotated pairs from Wikipedias \"Did You Know\" section [Marcinczuk et al., 2013]; metric: binary F1. ppc: Text similarity assessment using manually labeled sentence pairs (exact paraphrases, close paraphrases, non-paraphrases) [Dadas, 2022]; metric: accuracy. psc: Summarization of news articles [Ogrodniczuk and Kopec, 2014]; metric: binary F1. cbd: Text classification for cyberbullying and hate-speech detection [Ptaszynski et al., 2023]; metric: macro F1. polqa: Open-domain question answering from the \"Jeden dziesieciu\" TV show, with and without context (abstractive QA/RAG) [Rybak et al., 2024]; metric: accuracy, levenshtein. poquad: Context-based extractive question answering (QA/RAG) [Tuora et al., 2023]; metric: levenshtein. eqbench: emotional intelligence benchmark [Paech, 2024]; metric: custom. Most of the tasks are multiple-choice tests, which means that the model chooses the correct answer from set of options. They are implemented as two types of tests: Loglikelihood: We choose the highest probability token from the given set, e.g., ABCD. These tests are suitable for base models. Generate: Model generates answer freely. All tasks are evaluated in both 0-shot and 5-shot settings, with the average score across all tasks normalized by baseline scores. It is important to note that PLLuM models are not included in this leaderboard, as they were trained on training portions of the tasks used in the benchmark (except for the Belebele and EQ-Bench tasks), unlike all other models present on the leaderboard, to the best of our knowledge. The authors of the datasets used in the benchmark are primarily PLLuM consortium members. As shown in Table 9, the Bielik-4.5B-v3 model achieves an impressive average score of 54.94, making it competitive with much larger models. This result is particularly noteworthy considering it outperforms several models with significantly more parameters, such as Qwen2.5-7B (53.35), EuroLLM-9B (50.03), and SOLAR-10.7B-v1.0 (47.54). The smaller Bielik-1.5B-v3 model achieves score of 31.48, which is comparable to Qwen2.5-1.5B (31.83) and Llama-3.2-3B (31.89), despite its compact size. The instruction-tuned models demonstrate substantial improvements over their base counterparts. As shown in Table 10, Bielik-4.5B-v3.0-Instruct achieves score of 56.13, outperforming Qwen2.5-7B-Instruct (54.93) and Mistral-NemoInstruct-2407 (55.27) despite having fewer parameters. Most impressively, Bielik-1.5B-v3.0-Instruct scores 41.36, exceeding the performance of Qwen2.5-3B-Instruct (41.23) with approximately half the parameters, and coming close to Phi-4-mini-instruct (43.30) which has more than twice the parameter count. These results demonstrate the effectiveness of our training approach for the Bielik v3 models, which achieve remarkable parameter efficiency and strong performance on Polish language tasks. When considering the performance-to-parameter ratio, both the 1.5B and 4.5B models represent significant advancements in resource-efficient language modeling for Polish. 6.2 Polish EQ-Bench The Polish Emotional Intelligence Benchmark, localized Polish version of the original EQ-Bench Paech [2024], evaluates language models emotional intelligence capabilities across various dimensions of emotional understanding and response. This benchmark assesses models ability to comprehend, interpret, and appropriately respond to emotionally complex situations in Polish language contexts. The results in Table 11 highlight the performance of Bielik v3 models on the emotionally nuanced Polish EQ-Bench. The Bielik-4.5B-v3.0-Instruct achieves score of 53.58, which is particularly impressive for its parameter count. Despite having only 4.8B parameters, it outperforms several much larger models including PLLuM-12B-chat (52.26) and multiple PLLuM models with significantly more parameters. It also performs comparably to EuroLLM-9B-Instruct (54.10) with nearly half the parameters. This efficiency is remarkable when considering the emotional intelligence capabilities achieved with substantially fewer parameters than comparable models. The performance gap between Bielik-4.5B-v3.0-Instruct and larger models like Bielik-11B-v2 variants (approximately 15-18 points difference) reflects the trade-offs between model size and performance, while demonstrating that even compact models can exhibit meaningful emotional intelligence capabilities. 16 Bielik v3 PREPRINT Model Parameters (B) Average Qwen2.5-72B Qwen2.5-32B Qwen-72B Qwen2.5-14B Meta-Llama-3-70B Qwen1.5-72B Meta-Llama-3.1-70B Mixtral-8x22B-v0.1 Mistral-Small-24B-Base-2501 Qwen1.5-32B Bielik-11B-v2 Qwen2.5-7B EuroLLM-9B Qwen-7B SOLAR-10.7B-v1.0 Mistral-Nemo-Base-2407 internlm2-20b Bielik-4.5B-v3 Qwen2.5-3B Meta-Llama-3.1-8B Meta-Llama-3-8B Qwen1.5-72B Mistral-7B-v0.3 Mistral-7B-v0.2 Qwen1.5-7B Bielik-7B-v0.1 Qra-13b Llama-3.2-3B Qwen2.5-1.5B Bielik-1.5B-v3 Qra-7b 72.7 32.8 72.7 14.8 70.6 72.7 70.6 141.0 24.0 32.8 11.2 7.0 9.2 7.0 10.7 12.2 20.0 4.8 3.0 8.0 8.0 72.3 7.0 7.0 7.0 7.2 13.0 3.0 1.5 1.6 7.0 67.38 66.73 66.02 62.71 62.07 61.11 60.87 60.75 59.90 58.71 58.14 53.35 50.03 49.39 47.54 47.28 47.15 45.47 44.59 43.77 43.30 39.51 38.88 38.81 37.92 34.34 33.90 31.89 31.83 31.48 16.60 Table 9: Open PL LLM Leaderboard results for base models (5-shot evaluation) The Bielik-1.5B-v3.0-Instruct model, with just 1.6B parameters, achieves more modest score of 13.88, comparable to PLLuM-12B-nc-instruct (13.11) despite having only about 13% of the parameters. When considering the performance gradient across model sizes, we observe that the Bielik-4.5B-v3.0-Instruct achieves 76% of the performance of our best Bielik-11B-v2.5-Instruct model (72.00) with only 43% of the parameters. This efficient scaling pattern demonstrates the effectiveness of our training approach in balancing performance with computational efficiency across the Bielik model family. These results demonstrate that the specialized training methodologies employed for Bielik v3 models enable them to achieve competitive performance on emotionally nuanced tasks despite their compact size, highlighting the effectiveness of the model architecture and training approach used for Polish language emotional intelligence capabilities. 6.3 Complex Polish Text Understanding Benchmark (CPTUB) The Complex Polish Text Understanding Benchmark (CPTUB) Sowa et al. [2024] is specifically designed to evaluate language models proficiency in interpreting complex Polish texts. Unlike traditional tasks that focus on explicit meaning, CPTUB assesses the models capacity to understand implied meanings and handle cognitively challenging questions. The benchmark comprises two main components: Implicatures: Evaluates models ability to interpret implied meanings, including sarcasm, idiomatic expressions, and phraseological compounds. This component tests sensitivity to nuanced, context-dependent inferences through three subtasks: Sentiment: Correctly identifying the emotional tone beyond literal expressions Language understanding: Interpreting the underlying intentions of text authors Phraseology: Recognizing and explaining fixed or semi-fixed expressions whose meanings cannot be inferred from their individual components 17 Bielik v3 PREPRINT Model Parameters (B) Average Mistral-Large-Instruct-2411 Meta-Llama-3.1-405B-Instruct-FP8 Mistral-Large-Instruct-2407 Qwen2.5-72B-Instruct QwQ-32B-Preview Llama-3.3-70B-Instruct Qwen2-72B-Instruct Bielik-11B-v2.3-Instruct Bielik-11B-v2.2-Instruct Meta-Llama-3.1-70B-Instruct Bielik-11B-v2.1-Instruct Mixtral-8x22B-Instruct-v0.1 Bielik-11B-v2.0-Instruct Meta-Llama-3-70B-Instruct Qwen3-32B Llama-4-Scout-17B-16E-Instruct Bielik-11B-v2.5-Instruct Mistral-Small-24B-Instruct-2501 phi-4 Qwen3-14B Mistral-Small-Instruct-2409 Qwen2.5-32B-Instruct Qwen2.5-14B-Instruct aya-23-35B Bielik-4.5B-v3.0-Instruct Qwen3-8B Qwen3-4B Mistral-Nemo-Instruct-2407 Qwen2.5-7B-Instruct Mistral-7B-Instruct-v0.3 Mistral-7B-Instruct-v0.2 Bielik-7B-Instruct-v0.1 Phi-4-mini-instruct Bielik-1.5B-v3.0-Instruct Qwen2.5-3B-Instruct Qwen3-1.7B Mistral-7B-Instruct-v0.1 Qwen2.5-1.5B-Instruct 123.0 405.0 123.0 72.7 32.8 70.6 72.7 11.2 11.2 70.6 11.2 141.0 11.2 70.6 32.8 109.0 11.2 24.0 14.7 14.8 22.2 32.8 14.8 35.0 4.8 8.2 4.0 12.2 7.6 7.2 7.2 7.2 3.8 1.6 3.1 2.0 7.0 1.5 69.84 69.44 69.11 67.92 67.01 66.40 65.87 65.71 65.57 65.49 65.45 65.23 64.98 64.45 64.24 64.21 63.95 62.97 62.57 62.24 61.41 61.21 59.91 56.37 56.13 55.78 55.49 55.27 54.93 47.74 45.95 44.70 43.30 41.36 41.23 38.34 33.11 31.89 Table 10: Open PL LLM Leaderboard results for instruction-tuned models (5-shot evaluation) Tricky Questions: Assesses models capability to address challenging questions characterized by logical puzzles, semantic ambiguity, logical inconsistencies, absurdity, and humor. This component specifically targets the models reasoning skills and ability to avoid hallucinations when faced with ambiguous or nonsensical queries. As shown in Table 12, the Bielik v3 models demonstrate impressive performance on this challenging benchmark, particularly in relation to their parameter counts. The Bielik-4.5B-v3.0-Instruct model achieves an overall score of 3.38, which is remarkable for model with only 4.8B parameters. This positions it in the same performance tier as significantly larger models, including several with over 10x the parameter count. Several key observations can be made about the Bielik v3 models performance: 1. Exceptional parameter efficiency: Bielik-4.5B-v3.0-Instruct (3.38) outperforms phi-4 (3.30) despite having only about third of the parameters (4.8B vs. 14.7B). It also surpasses all PLLuM models regardless of size, including PLLuM-8x7B variants with nearly 10x more parameters. 2. Strong implicature handling: Bielik-4.5B-v3.0-Instruct shows particularly strong performance in implicatures (3.68), exceeding even some Bielik-11B-v2 variants and models like Mixtral-8x22B-Instruct-v0.1 (3.67). This suggests superior understanding of nuanced Polish linguistic features like idioms and contextual meaning. 18 Bielik v3 PREPRINT Model Parameters (B) Score Mistral-Large-Instruct-2407 Mistral-Large-Instruct-2411 Meta-Llama-3.1-405B-Instruct-FP8 gpt-4o-2024-08-06 gpt-4-turbo-2024-04-09 Mistral-Small-Instruct-2409 Llama-PLLuM-70B-chat Meta-Llama-3.1-70B-Instruct Bielik-11B-v2.5-Instruct Qwen2-72B-Instruct Meta-Llama-3-70B-Instruct gpt-4o-mini-2024-07-18 Qwen2.5-32B-Instruct Bielik-11B-v2.3-Instruct Llama-3.3-70B-Instruct Llama-PLLuM-70B-instruct WizardLM-2-8x22B Qwen2.5-14B-Instruct Bielik-11B-v2.2-Instruct Bielik-11B-v2.0-Instruct glm-4-9b-chat Mistral-Nemo-Instruct-2407 Bielik-11B-v2.1-Instruct EuroLLM-9B-Instruct Bielik-4.5B-v3.0-Instruct PLLuM-12B-chat PLLuM-8x7B-nc-chat Llama-PLLuM-8B-chat Llama-3.2-3B-Instruct PLLuM-8x7B-chat PLLuM-8x7B-nc-instruct PLLuM-8x7B-instruct PLLuM-12B-instruct Qwen2.5-3B-Instruct PLLuM-12B-nc-chat Llama-PLLuM-8B-instruct Qwen2.5-1.5B-Instruct Llama-3.2-1B-Instruct gemma-1.1-2b-it Bielik-1.5B-v3.0-Instruct PLLuM-12B-nc-instruct Models with non-commercial license. 123.0 123.0 405.0 Unknown Unknown 22.2 70.6 70.6 11.2 72.7 70.6 Unknown 32.8 11.2 70.6 70.6 141.0 14.8 11.2 11.2 9.0 12.2 11.2 9.2 4.8 12.2 46.7 8.0 3.2 46.7 46.7 46.7 12.2 3.1 12.2 8.0 1.5 1.2 2.5 1.6 12.2 78.07 77.29 77.23 75.15 74.59 72.85 72.56 72.53 72.00 71.23 71.21 71.15 71.15 70.86 70.73 69.99 69.56 69.17 69.05 68.24 61.79 61.76 60.07 54.10 53.58 52.26 47.29 46.20 46.19 45.22 41.75 39.55 36.21 35.87 35.41 31.59 27.63 17.82 16.47 13.88 13. Table 11: Polish EQ-Bench results for various models. 3. Phraseology strength: The Bielik-4.5B-v3.0-Instruct model scores 3.67 in phraseology, notably higher than many larger models including all Bielik-11B variants. This indicates exceptional ability to understand Polish idiomatic expressions and fixed phrases. 4. Sentiment analysis competence: Both Bielik v3 models perform well in sentiment analysis, with the 1.5B model scoring 3.53 - higher than many larger models including Qwen2.5-3B-Instruct (2.95) despite having half the parameters. 5. Tricky questions challenges: The area with the most room for improvement is in handling tricky questions, where Bielik-4.5B-v3.0-Instruct scores 2.46. This is consistent with the pattern seen across most models, as this category tests complex reasoning abilities that typically benefit from larger model scales. The performance of Bielik-1.5B-v3.0-Instruct is similarly impressive within its parameter class. At just 1.6B parameters, it achieves an overall score of 2.36, outperforming models with substantially more parameters like Phi-4-mini-instruct (2.17 with 3.8B parameters) and performing comparably to Qwen2.5-3B-Instruct (2.50 with 3.1B parameters). Its Bielik v3 PREPRINT Model Params (B) Overall Average Implicatures Average Sentiment Language Understanding Phraseology Tricky Questions DeepSeek-R1 Mistral-Large-Instruct-2411 Qwen2.5-72B-Instruct Mistral-Large-Instruct-2407 Llama-4-Maverick-17B-128E-Instruct-FP8 gemma-3-27b-it Meta-Llama-3-70B-Instruct Qwen2.5-32B-Instruct Llama-4-Scout-17B-16E-Instruct-FP8 Bielik-11B-v2.3-Instruct Bielik-11B-v2.1-Instruct Mixtral-8x22B-Instruct-v0.1 Qwen2.5-14B-Instruct Llama-PLLuM-70B-chat Bielik-11B-v2.5-Instruct Bielik-11B-v2.2-Instruct Bielik-4.5B-v3.0-Instruct Llama-PLLuM-70B-instruct phi-4 Bielik-11B-v2.0-Instruct PLLuM-12B-nc-chat PLLuM-12B-chat PLLuM-8x7B-nc-instruct PLLuM-12B-instruct Qwen2.5-7B-Instruct PLLuM-8x7B-nc-chat Meta-Llama-3.1-8B-Instruct PLLuM-8x7B-instruct PLLuM-8x7B-chat Meta-Llama-3-8B-Instruct Llama-PLLuM-8B-chat Bielik-7B-Instruct-v0.1 Llama-PLLuM-8B-instruct gemma-2-2b-it Qwen2.5-3B-Instruct Bielik-1.5B-v3.0-Instruct Phi-4-mini-instruct Llama-3.2-3B-Instruct EuroLLM-1.7B-Instruct Qwen2.5-1.5B-Instruct Models with non-commercial license. 685.0 123.0 72.7 123.0 402.0 27.4 70.6 32.8 402.0 11.2 11.2 141.0 14.8 70.6 11.2 11.2 4.8 70.6 14.7 11.2 12.2 12.2 46.7 12.2 7.62 46.7 8.0 46.7 46.7 8.0 8.0 7.2 8.0 2.6 3.1 1.6 3.8 3.2 1.7 1.5 4.14 4.00 3.95 3.93 3.93 3.81 3.78 3.75 3.75 3.63 3.61 3.56 3.55 3.53 3.48 3.46 3.38 3.33 3.30 3.26 3.15 3.14 3.11 3.09 3.07 3.03 3.01 3.01 3.01 3.00 2.92 2.88 2.82 2.65 2.50 2.36 2.17 2.00 1.76 1. 4.14 4.10 3.99 4.03 3.99 3.90 3.81 3.80 3.94 3.77 3.66 3.67 3.62 3.63 3.67 3.57 3.68 3.56 3.50 3.61 3.33 3.32 3.56 3.49 3.23 3.44 3.31 3.51 3.41 3.17 3.14 3.13 3.20 2.80 2.73 2.75 2.46 2.26 2.10 2.12 4.49 4.33 4.08 4.23 4.39 3.88 4.13 3.81 4.10 3.97 3.96 3.78 3.91 3.94 4.01 3.72 3.76 3.78 3.72 3.97 3.22 3.32 3.88 3.71 3.56 3.76 3.97 3.59 3.44 3.33 3.13 3.59 3.24 3.40 2.95 3.53 2.69 2.76 2.24 2.79 4.35 3.98 3.97 4.00 4.11 3.79 3.82 3.57 3.81 3.79 3.92 3.68 3.57 3.61 3.86 3.73 3.61 3.63 3.54 3.75 3.23 3.21 3.59 3.17 3.03 3.48 3.38 3.47 3.45 3.15 2.93 3.48 2.90 2.90 2.46 2.33 2.43 2.30 1.79 1.35 3.60 3.99 3.93 3.86 3.48 4.03 3.47 4.04 3.90 3.55 3.47 3.55 3.37 3.35 3.13 3.25 3.67 3.26 3.24 3.13 3.54 3.43 3.22 3.59 3.10 3.08 2.58 3.46 3.35 3.04 3.36 2.32 3.46 2.10 2.80 2.38 2.25 1.72 2.26 2.23 4.12 3.72 3.81 3.65 3.76 3.53 3.71 3.59 3.19 3.22 3.47 3.24 3.34 3.21 2.91 3.12 2.46 2.63 2.72 2.20 2.62 2.59 1.76 1.90 2.58 1.80 2.11 1.51 1.78 2.48 2.25 2.16 1.66 2.21 1.81 1.22 1.30 1.22 0.76 0.66 Table 12: Complex Polish Text Understanding Benchmark (CPTUB) results across different evaluation categories strong sentiment analysis score (3.53) is particularly noteworthy, matching or exceeding many models with 3-7x more parameters. These results highlight the effectiveness of the specialized training methodologies employed for the Bielik v3 models, particularly the focus on Polish-specific data curation and the innovative techniques described in previous sections, as shown in Tables 1 and 2. The models demonstrate that through careful optimization, even relatively small models can achieve competitive performance on complex linguistic tasks that traditionally favor much larger architectures. 6.4 Polish Medical Leaderboard The Polish Medical Leaderboard evaluates language models on Polish Board Certification Examinations (Panstwowy Egzamin Specjalizacyjny, PES) from years 2018-2022. This benchmark assesses models medical knowledge and reasoning capabilities in Polish-language medical context, using datasets from speakleash/PES-2018-2022, which is based on amu-cai/PES-2018-2022 Pokrywka et al. [2024]. 20 Bielik A PREPRINT Model Parameters (B) Average (%) Meta-Llama-3.1-405B-Instruct-FP8 Mistral-Large-Instruct-2407 Qwen2.5-72B-Instruct Meta-Llama-3.1-70B-Instruct Qwen2-72B-Instruct Meta-Llama-3-70B-Instruct Qwen2.5-32B Qwen2.5-32B-Instruct Qwen2.5-14B-Instruct Bielik-11B-v2.5-Instruct GLM-4-9b-chat Mistral-Small-Instruct-2409 Bielik-4.5B-v3.0-Instruct Bielik-11B-v2.3-Instruct Bielik-11B-v2.1-Instruct Bielik-11B-v2.2-Instruct Qwen2.5-7B-Instruct Bielik-11B-v2.0-Instruct Meta-Llama-3.1-8B-Instruct Mistral-Nemo-Instruct-2407 Bielik-11B-v2 Qwen2.5-3B-Instruct Bielik-1.5B-v3.0-Instruct Qwen2.5-1.5B-Instruct Mistral-7B-Instruct-v0.3 Models with non-commercial license. 405.0 123.0 72.7 70.6 72.7 70.6 32.8 32.8 14.8 11.2 9.0 22.2 4.8 11.2 11.2 11.2 7.6 11.2 8.0 12.2 11.2 3.0 1.6 1.5 7.0 69.20 64.28 63.89 61.75 61.35 57.51 55.69 54.52 49.60 44.85 44.54 43.60 43.55 43.26 43.16 43.05 42.69 41.53 40.60 40.36 39.98 37.72 34.63 32.64 31. Table 13: Polish Medical Leaderboard results (5-shot setting) showing model performance on Polish Board Certification Examinations. Bielik v3s performance: medical reasoning capabilities relative to their model size: In the Polish Medical Leaderboard  (Table 13)  , the Bielik v3 models demonstrate impressive Bielik-4.5B-v3.0-Instruct achieves score of 43.55%, which is remarkably close to Bielik-11B-v2.5-Instruct (44.85%) despite having less than half the parameters The smaller Bielik-1.5B-v3.0-Instruct scores 34.63%, outperforming Qwen2.5-1.5B-Instruct (32.64%) and significantly outperforming larger models like Mistral-7B-Instruct-v0.3 (31.24%) Notably, Bielik-4.5B-v3.0-Instruct matches or outperforms several much larger models, including MistralSmall-Instruct-2409 (43.60%) which has 22.2B parameters Performance context: The benchmark highlights several important insights about Bielik v3s medical capabilities: Both Bielik v3 models achieve strong parameter efficiency, with the 4.5B model performing at nearly the same level as models 2-3 times its size The models show effective cross-domain generalization from general Polish language understanding to specialized medical knowledge, despite not having domain-specific medical training The gap between Bielik v3 models and top-performing models like Meta-Llama-3.1-405B-Instruct (69.20%) reflects the expected scaling relationship between model size and specialized domain knowledge These results emphasize the effectiveness of the training methodologies employed for the Bielik v3 models, enabling strong performance on specialized domain knowledge even at smaller parameter counts. This makes the Bielik v3 models particularly valuable for practical applications where computational efficiency must be balanced with domain-specific performance. 6.5 Polish Linguistic and Cultural Competency Benchmark (PLCC) The Polish Linguistic and Cultural Competency Benchmark (PLCC) [Dadas et al., 2025] is specialized evaluation framework that tests language models grasp of Polish cultural knowledge and context. Unlike conventional NLP 21 Bielik v3 PREPRINT assessments, PLCC delves deeper into cultural understanding through 600 carefully designed questions spanning six key domains: history, geography, culture & tradition, art & entertainment, grammar, and vocabulary. The benchmarks questions probe models familiarity with various aspects of Polish heritage, including cultural references, historical milestones, traditional customs, folklore, literary works, and contemporary popular culture. These elements are crucial for achieving authentic language comprehension that goes beyond mere text processing. The questions vary in complexity from widely known information to specialized regional knowledge, presented in both multiple-choice and open-ended formats that demand precise responses with specific facts, dates, names, or concepts. The PLCC results  (Table 14)  reveal several insights about Bielik v3 models cultural competency: Parameter efficiency: Bielik-4.5B-v3.0-Instruct achieves 42.33% on PLCC despite its small size, outperforming several larger models including Qwen-2.5-14B (26.67%) and Phi-4 (29.17%). Similarly, Bielik-1.5B-v3.0-Instruct reaches 27.50%, demonstrating strong performance for its compact parameter count. Size-performance tradeoff: While Bielik v3 models score lower than their larger Bielik v2 counterparts (Bielik11B-v2.2-Instruct at 63.00%), they provide viable alternatives for resource-constrained deployments while maintaining reasonable cultural knowledge capabilities. Competitive small-model performance: Bielik-4.5B-v3.0-Instruct shows competitive results compared to models with similar parameter counts, highlighting the effectiveness of our training methodology in preserving cultural knowledge despite parameter reduction. Progressive improvements: The performance gap between Bielik-1.5B-v3.0-Instruct (27.50%) and Bielik-4.5Bv3.0-Instruct (42.33%) demonstrates the benefits of additional parameters for cultural knowledge retention, while both maintain efficient footprints. 6.6 Open LLM Leaderboard The Open LLM Leaderboard [Beeching et al., 2023b] evaluates models on various English language tasks, providing insights into the models performance across different linguistic challenges. 6.7 MixEval MixEval [Ni et al., 2024] is an English-language benchmark grounded in verified data, created to assess Large Language Models (LLMs) both efficiently and reliably. Its main characteristics include: 1. Built from collection of pre-existing benchmark datasets 2. Demonstrates strong alignment with Chatbot Arena rankings, showing 0.96 correlation 3. Executes locally with minimal overhead, requiring just 6% of the time and cost of MMLU This benchmark offers dependable and fast approach for evaluating LLMs, making it practical choice for continuous performance tracking and model comparison. Results are presented in Table 17. 6.8 Berkeley Function-Calling Leaderboard The Berkeley Function-Calling Leaderboard (BFCL) Yan et al. [2024] is designed to measure the proficiency of language models in accurately invoking functions (tools) using realistic input data. This evaluation is critical for determining how effectively models can interact with APIs and external systemsan essential skill for deploying LLMs in areas such as software engineering, data processing, and automated workflows. The benchmark employs Abstract Syntax Tree (AST) metrics to gauge function call correctness across range of test types: Expert Curated (Non-live) dataset: collection of hand-crafted, static test cases developed by domain experts to assess function calling in controlled environments User Contributed (Live) dataset: Real-time, user-submitted examples that reflect function calling in authentic, dynamic situations 22 Bielik v3 PREPRINT Model Parameters (B) Average Score (%) Gemini-2.5-Pro-Exp-03-25 DeepSeek-R1 DeepSeek-v3-0324 DeepSeek-v3 PLLuM-8x7B-nc-chat Llama-3.1-Tulu-3-405B Bielik-11B-v2.2-Instruct Bielik-11B-v2.3-Instruct GPT-4.1-mini-2025-04-14 Bielik-11B-v2.1-Instruct Llama-3.1-405B PLLuM-12B-nc-chat Llama-PLLuM-70B-chat Llama-4-Maverick Command-A-03-2025 Mistral-Large-2407 PLLuM-8x7B-chat Mistral-Large-2411 WizardLM-2-8x22B Qwen-Max Command-R-Plus-08-2024 Mixtral-8x22B Command-R-Plus-04-2024 Llama-3.3-70B Llama-3.1-70B Gemma-3-27B PLLuM-12B-chat Bielik-7B-Instruct-v0.1 Mistral-Small-3.1-24B-2503 Llama-3.0-70B Gemma-2-27B Bielik-4.5B-v3.0-Instruct Llama-4-Scout EuroLLM-9B Qwen-2.5-72B Mistral-Small-24B-2501 Llama-PLLuM-8B-chat Qwen3-32B Mixtral-8x7B Qwen-2.5-32B Qwen3-14B Gemma-2-9B Phi-4 Bielik-1.5B-v3.0-Instruct Qwen-2.5-14B Qwen3-8B Mistral-Nemo Command-R-7B Llama-3.1-8B Mistral-7B-v0.3 Ministral-8B Qwen-2.5-7B Models with non-commercial license. Unknown 685.0 685.0 685.0 46.7 405.0 11.2 11.2 Unknown 11.2 405.0 12.2 70.6 402.0 111.0 123.0 46.7 123.0 141.0 Unknown Unknown 141.0 Unknown 70.6 70.0 27.4 12.2 7.0 24.0 70.0 27.0 4.8 109.0 9.0 72.7 24.0 8.0 32.8 46.7 32.8 14.8 9.0 14.7 1.6 14.8 8.2 12.2 7.0 8.0 7.2 8.0 7.0 89.50 76.00 71.00 69.17 68.17 63.83 63.00 62.17 62.17 61.00 60.00 59.50 58.50 58.17 56.17 54.17 54.17 52.00 51.50 50.83 50.17 49.83 49.33 48.83 47.83 47.33 47.00 46.67 43.33 43.00 42.67 42.33 41.50 41.00 39.17 39.00 38.50 37.67 35.33 30.50 30.33 29.17 29.17 27.50 26.67 26.00 23.00 22.83 22.67 21.83 20.67 17.67 Table 14: Polish Linguistic and Cultural Competency Benchmark (PLCC) results for open-source models. Closed proprietary models have been excluded from this comparison. Multi-turn interactions: Evaluates the models capability to preserve and utilize conversational context over multiple exchanges 23 Bielik v3 PREPRINT Model AVG arc_challenge hellaswag truthfulqa_mc2 mmlu winogrande gsm8k 66.70 Qwen1.5-14B 65.87 Bielik-11B-v2 Qwen-14B 65.86 Meta-Llama-3-8B 62.62 61.02 Bielik-4.5B-v3 60.97 Mistral-7B-v0.1 60.37 Mistral-7B-v0.2 53.64 Bielik-1.5B-v3 49.98 Bielik-7B-v0.1 56.57 60.58 58.28 60.24 51.19 59.98 60.84 46.93 45.22 81.08 79.84 83.99 82.23 73.01 83.31 83.08 64.28 67. 52.06 46.13 49.43 42.93 45.63 42.15 41.76 42.47 47.16 69.36 63.06 67.70 66.70 61.32 64.16 63.62 55.13 43.20 73.48 77.82 76.80 78.45 71.35 78.37 78.22 63.38 66.85 67.63 67.78 58.98 45.19 63.61 37.83 34.72 49.66 29.49 Table 15: Open LLM Leaderboard results for base models Model AVG arc_challenge hellaswag truthfulqa_mc2 mmlu winogrande gsm8k SOLAR-10.7B-Instruct-v1.0 Phi-3-medium-4k-instruct Bielik-11B-v2.2-Instruct Bielik-11B-v2.3-Instruct Bielik-11B-v2.1-Instruct openchat-3.5-0106-gemma Bielik-11B-v2.0-Instruct Meta-Llama-3-8B-Instruct Mistral-7B-Instruct-v0.2 Bielik-4.5B-v3-Instruct gemma-7b Qwen1.5-32B-Chat Qwen1.5-14B-Chat Bielik-1.5B-v3-Instruct Qwen1.5-7B-Chat Mistral-7B-Instruct-v0.1 Bielik-7B-Instruct-v0.1 74.20 73.45 69.86 69.82 69.82 69.42 68.04 66.87 65.71 64.89 64.29 62.95 62.27 56.64 55.15 54.96 51. 71.08 67.32 59.90 59.30 59.56 64.68 58.62 60.75 63.14 56.06 61.09 66.04 58.70 48.38 55.89 54.52 47.53 88.16 85.76 80.16 80.11 80.20 81.08 78.65 78.55 84.88 73.90 82.47 85.49 82.27 65.03 78.56 75.63 68.91 71.43 57.71 58.34 57.42 59.35 54.93 54.65 51.65 68.26 50.79 44.91 66.95 60.36 42.47 53.54 56.28 46.18 66.21 77.83 64.34 64.57 64.18 64.69 63.71 67.07 60.78 63.66 66.03 74.99 68.57 54.59 61.65 55.38 49.47 83.58 72.69 75.30 76.24 75.06 78.30 76.32 74.51 77.19 71.19 78.45 77.19 73.09 65.35 67.72 73.72 65.51 64.75 79.38 81.12 81.27 80.59 72.86 76.27 68.69 40.03 73.69 52.77 7.05 30.63 62.85 13.57 14.25 29. Table 16: Open LLM Leaderboard results for selected instruction-tuned models Relevance detection: Determines whether the model appropriately triggers function in cases where at least one relevant function should be used. Multiple valid function calls may exist; correctness of arguments is not strictly enforcedonly that relevant function is invoked Irrelevance detection: Tests the models ability to refrain from invoking any functions when none are applicable. Models should either justify why no function is suitable or respond without initiating function call"
        },
        {
            "title": "7 Limitations and Biases",
            "content": "Bielik v3 series of models can produce factually incorrect output, and should not be relied on to produce factually accurate data. Our models were trained on various public datasets. While great efforts have been taken to clear the training data, it is possible that this model can generate lewd, false, biased or otherwise offensive outputs."
        },
        {
            "title": "8 Conclusion",
            "content": "In this technical report, we introduce the Bielik v3 series of generative text models for Polish language processing, including 1.5B and 4.5B parameter variants. These models represent substantial advancement in Polish language AI, offering remarkable parameter efficiency while maintaining strong performance across diverse linguistic tasks, as demonstrated in our comprehensive evaluations in Tables 9, 10, 11, 12, and 13. Key contributions of our work include: 24 Bielik v3 PREPRINT Model MixEval-Hard MixEval Qwen1.5-72B-Chat LLaMA-3-8B-Instruct Bielik-11B-v2.1-Instruct Qwen1.5-32B-Chat Bielik-11B-v2.3-Instruct Bielik-11B-v2.0-Instruct Bielik-11B-v2.2-Instruct Mistral-7B-Instruct-v0.2 Bielik-4.5B-v3-Instruct Bielik-1.5B-v3-Instruct 48.3 45.6 45.0 43.3 43.2 40.2 39.7 36.2 29.6 24.8 84.1 75.0 74.6 81.0 73.0 72.1 72.4 70.0 55.3 46.6 Table 17: MixEval benchmark results comparing Bielik v3 models against other instruction-tuned models Model Non-Live Python Simple AST Non-Live Non-Live Parallel Multiple AST AST Non-Live Parallel Multiple AST Live Live Simple Multiple AST AST Live Parallel AST Live Parallel Multiple AST Open-Mistral-Nemo-2407 (Prompt) Gemma-3-12b-it (Prompt) Open-Mistral-Nemo-2407 (FC) Bielik-11B-v2.5-Instruct (FC) Bielik-4.5B-v3.0-Instruct (FC) Qwen2.5-3B-Instruct (Prompt) Qwen2.5-3B-Instruct (FC) Qwen2.5-1.5B-Instruct (FC) Qwen2.5-1.5B-Instruct (Prompt) Bielik-1.5B-v3.0-Instruct (FC) Bielik-11B-v2.3-Instruct (Prompt) 92.00% 94.00% 91.25% 95.00% 94.00% 91.50% 96.00% 92.25% 89.00% 77.00% 87.50% 93.50% 95.00% 93.50% 97.50% 92.50% 90.50% 92.00% 87.00% 86.00% 85.00% 93.50% 89.50% 90.00% 85.50% 87.50% 82.00% 79.50% 73.50% 81.50% 70.00% 69.50% 47.00% 84.50% 73.00% 85.00% 87.00% 86.00% 79.00% 76.50% 75.50% 66.50% 63.00% 50.00% 87.50% 74.45% 77.91% 87.50% 70.85% 84.88% 75.00% 69.61% 77.13% 77.13% 43.75% 77.21% 70.16% 68.66% 50.00% 56.25% 66.48% 69.77% 62.50% 72.08% 74.03% 50.00% 66.10% 74.03% 70.54% 56.25% 59.26% 61.63% 58.69% 50.00% 43.75% 69.71% 72.87% 66.67% 62.50% 70.83% 66.67% 54.17% 62.50% 45.83% 45.83% 41.67% 41.67% 54.17% Table 18: Comprehensive summary of model performance on the Berkeley Function-Calling Leaderboard subtasks. Bielik models demonstrate strong results across variety of subtasks, excelling especially in Non-Live Python Simple AST and Non-Live Multiple AST categories, while also maintaining consistent outcomes in Live Simple and Multiple AST tasks. 1. Innovative Architectural Decisions: Building upon the Qwen2.5 architecture, we implemented depth upscaling and replaced the tokenizer with our custom APT4 tokenizer optimized for Polish, resulting in more efficient token usage. 2. Data Quality Focus: We developed sophisticated quality classification systems with 95% accuracy to ensure our training corpus consisted of high-quality Polish texts balanced across 120 thematic categories. 3. Training Methodology Innovations: Our techniques include Adaptive Learning Rate, which significantly improved model performance, particularly for Polish-specific linguistic patterns. 4. Impressive Performance Efficiency: The 4.5B parameter model achieves results competitive with models 23 larger across multiple benchmarks, while the 1.5B model delivers strong performance despite its extremely compact profile. This efficiency makes Bielik v3 particularly valuable for deployment in resource-constrained environments while maintaining high-quality Polish language capabilities. 5. Benchmark Excellence: On the Open PL LLM Leaderboard, CPTUB, Polish Medical Benchmark, and EQBench, Bielik v3 models consistently outperform many larger models, demonstrating exceptional efficiency. These models provide powerful foundation for Polish language applications across various domains, from general conversational AI to specialized fields such as medicine and law. By prioritizing parameter efficiency without sacrificing quality, Bielik v3 enables broader deployment on resource-constrained systems while advancing the state of Polish language AI. Future work will focus on further enhancing capabilities for complex reasoning, exploring additional efficiency improvements, and expanding domain-specific knowledge. We believe the Bielik v3 models establish new benchmark for efficient, high-quality language models for less-resourced languages. 25 Bielik v3 PREPRINT"
        },
        {
            "title": "Acknowledgements",
            "content": "We gratefully acknowledge Polish high-performance computing infrastructure PLGrid (HPC Center: ACK Cyfronet AGH) for providing computer facilities and support within computational grant no. PLG/2024/017214 and PLG/2025/018338. The model could not have been created without the commitment and work of the entire SpeakLeash team, whose contribution is invaluable. Thanks to the hard work of many individuals, it was possible to gather large amount of content in Polish and establish collaboration between the open-science SpeakLeash project and the HPC center: ACK Cyfronet AGH. Individuals who contributed to the creation of the model through their commitment to the open-science SpeakLeash project: Sebastian Kondracki, Marek Magrys, Szymon Mazurek, Mieszko Cholewa, Igor Ciuciura, Szymon Baczynski, Jacek Chwiła, Dominika Basaj, Kuba Sołtys, Karol Jezierski, Anna Przybył, Agnieszka Ratajska, Witold Wydmanski, Izabela Babis, Nina Babis, and many other wonderful researchers and enthusiasts of the AI world."
        },
        {
            "title": "References",
            "content": "Voicelab. Trurl 2 models), 2023. URL https://huggingface.co/Voicelab. National Information Processing Institute and Gdansk University of Technology. Qra models, 2024. URL https: //huggingface.co/OPI-PG. Krzysztof Ociepa, Łukasz Flis, Krzysztof Wróbel, Adrian Gwozdziej, and Remigiusz Kinas. Bielik 7b v0.1: polish language model development, insights, and evaluation, 2024. URL https://arxiv.org/abs/2410.18565. Krzysztof Ociepa, Łukasz Flis, Krzysztof Wróbel, Adrian Gwozdziej, and Remigiusz Kinas. Bielik 11b v2 technical report, 2025. URL https://arxiv.org/abs/2505.02410. Qwen, :, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tianyi Tang, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 technical report, 2025. URL https://arxiv.org/abs/2412.15115. Ashish Vaswani, Noam M. Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Neural Information Processing Systems, 2017. URL https: //api.semanticscholar.org/CorpusID:13756489. Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebron, and Sumit Sanghai. GQA: Training generalized multi-query transformer models from multi-head checkpoints. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 48954901, Singapore, December 2023. Association for Computational Linguistics. doi:10.18653/v1/2023.emnlp-main.298. URL https://aclanthology.org/2023.emnlp-main.298. Yann Dauphin, Angela Fan, Michael Auli, and David Grangier. Language modeling with gated convolutional networks. In International Conference on Machine Learning, 2016. URL https://api.semanticscholar.org/CorpusID: 16119010. Noam Shazeer. Glu variants improve transformer, 2020. URL https://arxiv.org/abs/2002.05202. Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. ISSN 09252312. doi:https://doi.org/10.1016/j.neucom.2023.127063. URL https://www.sciencedirect.com/science/ article/pii/S0925231223011864. Zixuan Jiang, Jiaqi Gu, Hanqing Zhu, and David Z. Pan. Pre-rmsnorm and pre-crmsnorm transformers: equivalent and efficient pre-ln transformers. In Proceedings of the 37th International Conference on Neural Information Processing Systems, NIPS 23, Red Hook, NY, USA, 2024. Curran Associates Inc. Sanghoon Kim, Dahyun Kim, Chanjun Park, Wonsung Lee, Wonho Song, Yunsu Kim, Hyeonwoo Kim, Yungi Kim, Hyeonju Lee, Jihoo Kim, Changbae Ahn, Seonghoon Yang, Sukyung Lee, Hyunbyung Park, Gyoungjin Gim, Mikyoung Cha, Hwalsuk Lee, and Sunghun Kim. SOLAR 10.7B: Scaling large language models with simple yet effective depth up-scaling. In Yi Yang, Aida Davani, Avi Sil, and Anoop Kumar, editors, Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 6: Industry Track), pages 2335, Mexico City, Mexico, 2024. Association for Computational Linguistics. URL https://aclanthology.org/2024.naacl-industry.3. 26 Bielik v3 PREPRINT Krzysztof Ociepa and Azurro Team. Introducing apt3-1b-base: Polish language model, 2024. URL https://azurro. pl/apt3-1b-base-en. Accessed: 2024-09-30. X. Yuan, Y. Li, and Y. Liu. Frequency-based vocabulary transfer for efficient tokenizer adaptation in multilingual pretrained models. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing: Industry Track. Association for Computational Linguistics, 2022. Benjamin Minixhofer, Fabian Paischer, and Navid Rekabsaz. Wechsel: Effective initialization of subword embeddings for cross-lingual transfer of monolingual language models. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics, 2022. Konstantin Dobler and Gerard de Melo. Focus: Effective embedding initialization for monolingual specialization of multilingual models. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing (EMNLP 2023). Association for Computational Linguistics, 2023. Yihong Liu, Peiqin Lin, Mingyang Wang, and Hinrich Schütze. Ofa: framework of initializing unseen subword embeddings for efficient large-scale multilingual continued pretraining. arXiv preprint arXiv:2311.08849, 2023. Ke Tran. From english to foreign languages: Transferring pretrained language models. arXiv preprint arXiv:2002.07306, 2020. SpeakLeash Team. Speakleash a.k.a spichlerz!, 2024. URL https://www.speakleash.org. Accessed: 2024-09-30. Dingcheng Li, Zheng Chen, Eunah Cho, Jie Hao, Xiaohu Liu, Xing Fan, and Chenlei Guo. Overcoming catastrophic forgetting during domain adaptation of seq2seq language generation. In North American Chapter of the Association for Computational Linguistics, 2022. URL https://api.semanticscholar.org/CorpusID:249268165. Oleksiy Ostapenko, Timothee Lesort, Pau Rodriguez, Md Rifat Arefin, Arthur Douillard, Irina Rish, and Laurent Charlin. Continual learning with foundation models: An empirical study of latent replay. In Sarath Chandar, Razvan Pascanu, and Doina Precup, editors, Proceedings of The 1st Conference on Lifelong Learning Agents, volume 199 of Proceedings of Machine Learning Research, pages 6091. PMLR, 2224 Aug 2022. URL https: //proceedings.mlr.press/v199/ostapenko22a.html. Adam Ibrahim, Benjamin Thérien, Kshitij Gupta, Mats L. Richter, Quentin Anthony, Timothée Lesort, Eugene Belilovsky, and Irina Rish. Simple and scalable strategies to continually pre-train large language models, 2024. URL https://arxiv.org/abs/2403.08763. Daria Soboleva, Faisal Al-Khateeb, Robert Myers, Jacob Steeves, Joel Hestness, and Nolan Dey. SlimPajama: 627B token cleaned and deduplicated version of RedPajama. https://www.cerebras.net/blog/ slimpajama-a-627b-token-cleaned-and-deduplicated-version-of-redpajama, 6 2023. URL https: //huggingface.co/datasets/cerebras/SlimPajama-627B. Inez Okulska, Daria Stetsenko, Anna Kołos, Agnieszka Karlinska, Kinga Gł abinska, and Adam Nowakowski. Stylometrix: An open-source multilingual tool for representing stylometric vectors, 2023. URL https://arxiv.org/ abs/2309.12810. Zhengyan Shi, Adam X. Yang, Bin Wu, Laurence Aitchison, Emine Yilmaz, and Aldo Lipani. Instruction tuning with loss over instructions, 2024. URL https://arxiv.org/abs/2405.14394. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Mingchuan Zhang, YK Li, Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: flexible and efficient rlhf framework. arXiv preprint arXiv: 2409.19256, 2024. Edward Beeching, Clémentine Fourrier, Nathan Habib, Sheon Han, Nathan Lambert, Nazneen Rajani, Omar Sanseviero, Lewis Tunstall, and Thomas Wolf. Open llm leaderboard (2023-2024). https://huggingface.co/spaces/ open-llm-leaderboard-old/open_llm_leaderboard, 2023a. Krzysztof Wróbel, SpeakLeash Team, and Cyfronet Team. Open pl llm leaderboard. https://huggingface.co/ spaces/speakleash/open_pl_llm_leaderboard, 2024. Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noach, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. framework for few-shot language model evaluation, 07 2024. URL https://zenodo.org/records/12608602. Bielik v3 PREPRINT Jan Kocon, Piotr Miłkowski, and Monika Zasko-Zielinska. Multi-level sentiment analysis of PolEmo 2.0: Extended corpus of multi-domain consumer reviews. In Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL), pages 980991, Hong Kong, China, November 2019. Association for Computational Linguistics. doi:10.18653/v1/K19-1092. URL https://www.aclweb.org/anthology/K19-1092. Piotr Rybak, Robert Mroczkowski, Janusz Tracz, and Ireneusz Gawlik. KLEJ: Comprehensive benchmark for polish language understanding. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 11911201, Online, July 2020. Association for Computational Linguistics. URL https://www.aclweb. org/anthology/2020.acl-main.111. Sławomir Dadas, Michał Perełkiewicz, and Rafał Poswiata. Evaluation of sentence representations in Polish. In Proceedings of the 12th Language Resources and Evaluation Conference, pages 16741680, Marseille, France, May 2020. European Language Resources Association. ISBN 979-10-95546-34-4. URL https://aclanthology.org/ 2020.lrec-1.207. Lucas Bandarkar, Davis Liang, Benjamin Muller, Mikel Artetxe, Satya Narayan Shukla, Donald Husa, Naman Goyal, Abhinandan Krishnan, Luke Zettlemoyer, and Madian Khabsa. The belebele benchmark: parallel reading comprehension dataset in 122 language variants. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 749775, Bangkok, Thailand and virtual meeting, August 2024. Association for Computational Linguistics. URL https://aclanthology.org/2024.acl-long.44. Michał Marcinczuk, Marcin Ptak, Adam Radziszewski, and Maciej Piasecki. Open dataset for development of polish question answering systems. In Proceedings of the 6th Language & Technology Conference: Human Language Technologies as Challenge for Computer Science and Linguistics, Wydawnictwo Poznanskie, Fundacja Uniwersytetu im. Adama Mickiewicza, 2013. Sławomir Dadas. In 2022 IEEE International Conference on Systems, Man, and Cybernetics (SMC), pages 371378, 2022. doi:10.1109/SMC53654.2022.9945218. Training effective neural sentence encoders from automatically mined paraphrases. Maciej Ogrodniczuk and Mateusz Kopec. The Polish Summaries Corpus. In Proceedings of the Ninth International Conference on Language Resources and Evaluation, LREC 2014, 2014. Michal Ptaszynski, Agata Pieciukiewicz, Pawel Dybala, Pawel Skrzek, Kamil Soliwoda, Marcin Fortuna, Gniewosz Leliwa, and Michal Wroczynski. Expert-annotated dataset to study cyberbullying in polish language. Data, 9(1):1, 2023. Piotr Rybak, Piotr Przybyła, and Maciej Ogrodniczuk. PolQA: Polish question answering dataset. In Nicoletta Calzolari, Min-Yen Kan, Veronique Hoste, Alessandro Lenci, Sakriani Sakti, and Nianwen Xue, editors, Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LRECCOLING 2024), pages 1284612855, Torino, Italia, May 2024. ELRA and ICCL. URL https://aclanthology. org/2024.lrec-main.1125. Ryszard Tuora, Aleksandra Zwierzchowska, Natalia Zawadzka-Paluektau, Cezary Klamra, and Łukasz Kobylinski. Poquad-the polish question answering dataset-description and analysis. In Proceedings of the 12th Knowledge Capture Conference 2023, pages 105113, 2023. Samuel J. Paech. Eq-bench: An emotional intelligence benchmark for large language models, 2024. URL https: //arxiv.org/abs/2312.06281. Jan Sowa, Magdalena Krawczyk, Natalia Nadolna, Anna Zielinska, Maria Filipkowska, Agnieszka Kosiak, Marta Kania, Krzysztof Wróbel, Remigiusz Kinas, Szymon Baczynski, SpeakLeash Team, and Cyfronet Team. Complex polish text understanding benchmark. https://huggingface.co/spaces/speakleash/cptu_bench, 2024. Jakub Pokrywka, Jeremi Kaczmarek, and Edward Gorzelanczyk. Gpt-4 passes most of the 297 written polish board certification examinations, 2024. URL https://arxiv.org/abs/2405.01589. Sławomir Dadas, Małgorzata Grebowiec, Michał Perełkiewicz, and Rafał Poswiata. Evaluating polish linguistic and cultural competency in large language models, 2025. URL https://arxiv.org/abs/2503.00995. Edward Beeching, Clémentine Fourrier, Nathan Habib, Sheon Han, Nathan Lambert, Nazneen Rajani, Omar Sanseviero, Lewis Tunstall, and Thomas Wolf. Open llm leaderboard. https://huggingface.co/spaces/ open-llm-leaderboard-old/open_llm_leaderboard, 2023b. Jinjie Ni, Fuzhao Xue, Xiang Yue, Yuntian Deng, Mahir Shah, Kabir Jain, Graham Neubig, and Yang You. Mixeval: Deriving wisdom of the crowd from LLM benchmark mixtures. In Amir Globersons, Lester Mackey, Danielle Belgrave, Angela Fan, Ulrich Paquet, Jakub M. Tomczak, and Cheng Zhang, editors, Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024, 2024. URL http://papers.nips.cc/paper_files/paper/ 2024/hash/b1f34d7b4a03a3d80be8e72eb430dd81-Abstract-Conference.html. 28 Bielik A PREPRINT Fanjia Yan, Huanzhi Mao, Charlie Cheng-Jie Ji, Tianjun Zhang, Shishir G. Patil, Ion Stoica, and Joseph E. Gonzalez. Berkeley function calling leaderboard. https://gorilla.cs.berkeley.edu/blogs/8_berkeley_function_ calling_leaderboard.html, 2024."
        }
    ],
    "affiliations": [
        "ACK Cyfronet AGH",
        "Azurro",
        "Enelpol",
        "Jagiellonian University",
        "SpeakLeash"
    ]
}