{
    "paper_title": "TikZero: Zero-Shot Text-Guided Graphics Program Synthesis",
    "authors": [
        "Jonas Belouadi",
        "Eddy Ilg",
        "Margret Keuper",
        "Hideki Tanaka",
        "Masao Utiyama",
        "Raj Dabre",
        "Steffen Eger",
        "Simone Paolo Ponzetto"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "With the rise of generative AI, synthesizing figures from text captions becomes a compelling application. However, achieving high geometric precision and editability requires representing figures as graphics programs in languages like TikZ, and aligned training data (i.e., graphics programs with captions) remains scarce. Meanwhile, large amounts of unaligned graphics programs and captioned raster images are more readily available. We reconcile these disparate data sources by presenting TikZero, which decouples graphics program generation from text understanding by using image representations as an intermediary bridge. It enables independent training on graphics programs and captioned images and allows for zero-shot text-guided graphics program synthesis during inference. We show that our method substantially outperforms baselines that can only operate with caption-aligned graphics programs. Furthermore, when leveraging caption-aligned graphics programs as a complementary training signal, TikZero matches or exceeds the performance of much larger models, including commercial systems like GPT-4o. Our code, datasets, and select models are publicly available."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 1 ] . [ 2 9 0 5 1 1 . 3 0 5 2 : r TikZero: Zero-Shot Text-Guided Graphics Program Synthesis Jonas Belouadi Eddy Ilg Margret Keuper Raj Dabre Steffen Eger Hideki Tanaka Masao Utiyama Simone Ponzetto University of Mannheim, Germany University of Technology Nuremberg, Germany National Institute of Information and Communications Technology, Japan jonas.belouadi@uni-mannheim.de"
        },
        {
            "title": "Abstract",
            "content": "AutomaTikZv2 TikZero+ With the rise of generative AI, synthesizing figures from text captions becomes compelling application. However, achieving high geometric precision and editability requires representing figures as graphics programs in languages like TikZ, and aligned training data (i.e., graphics programs with captions) remains scarce. Meanwhile, large amounts of unaligned graphics programs and captioned raster images are more readily available. We reconcile these disparate data sources by presenting TikZero, which decouples graphics program generation from text understanding by using image representations as an intermediary bridge. It enables independent training on graphics programs and captioned images and allows for zero-shot text-guided graphics program synthesis during inference. We show that our method substantially outperforms baselines that can only operate with caption-aligned graphics programs. Furthermore, when leveraging caption-aligned graphics programs as complementary training signal, TikZero matches or exceeds the performance of much larger models, including commercial systems like GPT-4o. Our code, datasets, and select models are publicly available.1 1. Introduction Graphics programming languages offer distinct advantages over low-level vector formats (PDF, SVG) or raster image formats by representing visual concepts as high-level programs that preserve semantics, remain human-interpretable, and allow manual editing. These properties are particularly valuable to the scientific research community, where specialized graphics programming languages like TikZ [1] are popular for creating complex figures with high expressivity. However, this comes with steep learning curve, as evidenced 3D contour plot of loss function. multi-layer perceptron with two hidden layers. Gaussian probability density function (blue) with markers showing one standard deviation (red). Figure 1. Qualitative comparison of our TikZero+ model (last two columns) and the end-to-end trained baseline AutomaTikZv2 (LLM; first two columns) on text-guided graphics program synthesis with TikZ. Our method generates outputs that more closely follow the given captions. Example program listings are in Appendix E. by the TEX Stack Exchange2 (TEX.SE), where nearly 10% of questions concern TikZ and make it the most frequently discussed topic on the platform [2, 3]. With recent advances in generative AI, simplifying the creation of graphics programs has become increasingly feasible. Belouadi et al. [2] introduce DeTikZify, an inverse graphics model that generates TikZ programs from images and handdrawn sketches. However, creating these visual inputs stays cumbersome, motivating alternative input modalities such as natural language. While Belouadi et al. [3] propose AutomaTikZ, text-guided synthesis model for TikZ programs trained end-to-end on an aligned caption-program corpus, its performance remains limited (cf. Fig. 1) [4, 5]. 1https://github.com/potamides/DeTikZify 2https://tex.stackexchange.com 1 AutomaTikZ Limited to caption-aligned graphics programs only. Images with Captions Graphics Programs DeTikZify Accesses all graphics programs but without text guidance. TikZero Leverages captioned images and graphics programs independently. Figure 2. Illustration of training data availability for graphics program synthesis. DeTikZify can leverage all graphics programs for training but lacks text guidance, while AutomaTikZ is constrained to the small intersection of captioned graphics programs, resulting in limited performance. Our approach, TikZero, trains independently on both graphics programs and captioned images, enabling more effective use of available data and yielding superior results. We identify insufficient training data as the primary limitation. Unlike inverse graphics models such as DeTikZify, which are inherently self-supervised (trained by being conditioned on compiled representations of their output programs) and can access sufficient training data (cf. Fig. 2), end-toend text-guided models like AutomaTikZ require graphics programs paired with captions, substantially reducing the available data pool (cf. Fig. 2). To address this challenge, we decouple the graphics program generation component from text understanding, enabling independent training on graphics programs and captioned images without requiring paired data (cf. Fig. 2). Our approach first trains an inverse graphics model conditioned on image patch embeddings from vision encoder [6]. We then train an adapter network that generates synthetic image patch embeddings from captions. This adapter training relies solely on captioned images, effectively circumventing resource limitations and enabling zero-shot (in the sense that no aligned caption-program examples are involved in the training process) text-guided graphics program synthesis. We demonstrate that this approach, to which we refer as TikZero, outperforms previous state-of-the-art methods (cf. Fig. 1). Our key contributions are: (i) novel two-stage architecture, TikZero, which addresses the low-resource challenge in text-guided graphics program synthesis by aligning representation spaces rather than relying on aligned data. (ii) The DaTikZv3 dataset, comprising over 450k TikZ graphics programs with approximately 170k captioned samples. Using this dataset, we train both TikZero and AutomaTikZv2 (an updated version of AutomaTikZ) on the same source data, demonstrating that TikZero outperforms AutomaTikZ, AutomaTikZv2, and other end-to-end trained baselines. (iii) An enhanced model, TikZero+, combining TikZero with the end-to-end fine-tuning of AutomaTikZv2, which surpasses larger baselines and matches the performance of commercial models like GPT-4o [7] on key metrics. 2. Related Work Inverse Graphics Program Synthesis Inverse graphics, i.e., synthesizing graphics program to reproduce visual target, represents specialized instance of neural program synthesis [810]. Deep learning models have shown remarkable success in this domain [1113], with Vision-Language Models (VLMs) increasingly gaining prominence [2, 14 16]. While controlled experimental studies often rely on synthetic datasets [12, 14, 1619], real-world applications typically leverage more complex and diverse human-created data [2, 2024], highlighting the importance of data availability. In scientific contexts, TikZ has emerged as popular choice due to its versatility, expressiveness, and widespread adoption in academic circles [2, 2022, 24]. Although these approaches are not tailored to text-guided generation, we incorporate key elements from them into our approach. Text-Guided Graphics Program Synthesis Current textguided approaches to graphics program synthesis remain limited, mainly because of the scarcity of captioned graphics programs outlined in Sec. 1, but also because of the difficulty of generating synthetic data with human-like captions [25, 26]. Researchers interested in this capability currently rely on the emerging capabilities of large commercial models such as GPT-4o [7, 2729], which raises concerns about accessibility, reproducibility, and computational cost [30]. In contrast, related domains like vector graphics generation [23, 3134] and NL2Vis [3539] have shown more progress. Similar to inverse graphics, these fields increasingly incorporate large language models (LLMs) [23, 35, 39]. However, vector graphics approaches typically generate only low-level BÃ©zier curves, limiting output complexity [2, 31, 32], and NL2Vis focuses exclusively on data visualization with restricted set of visualization types [39]. More complex applications, such as generating arbitrary scientific figures from captions with TikZ, remain underexploreda gap we address in this work. Text-to-Image Generation TikZero shares conceptual and architectural similarities with several text-to-image generation methods [4044]. Rodriguez et al. [45, 46] explore generative adversarial networks [47, 48] and diffusion models [49, 50] for scientific figure generation, but these approaches are tied to raster images, which are not ideal for representing scientific figures. Ramesh et al. [42] propose two-stage model with independently trained prior and decoder components to generate raster images from text. Although 2 prior networks resemble our adapters and have been used with inverse graphics models [51], they target global image embeddings containing only abstract information, which degrades performance when used with inverse graphics architectures that work best with patch-level details [52]. In contrast, our adapters specifically operate on patch-level embeddings, and we demonstrate that this improves performance compared to end-to-end trained baselines. 3. The TikZero Model & Architecture As the foundation of TikZero, we first develop state-of-theart inverse graphics model for graphics program synthesis. We then incorporate cross-attention adapter network [54] for text guidance. Fig. 3 provides an overview of our method. The Inverse Graphics Model Due to their demonstrated effectiveness (cf. Sec. 2), we adopt VLM architecture for the inverse graphics model of TikZero. Figure 3 illustrates its inner workings (dashed lines): the model processes rasterized images and autoregressively generates their corresponding programs without involving captions at this stage. The Adapter Network VLMs consist of two primary components: vision encoder that generates image embeddings and text decoder that, in our case, produces graphics programs conditioned on these embeddings. The unidirectional and localized flow of information between these components allows us to inject additional information solely into the vision encoder, thereby influencing the output of the text decoder. In our specific case, this setup enables the introduction of trainable, text-conditioned adapter network trained to mimic the outputs of the original vision encoder. This effectively enables zero-shot generation of graphics programs conditioned on text when these outputs are fed into the decoder. In addition to circumventing the resource limitations discussed in Sec. 1, this architecture has other welcome implications: During adapter training, the text decoder, usually the largest component of the model, does not need to be loaded, resulting in efficient and fast training even with large datasets. Our adapter incorporates lightweight text encoder for embedding captions and introduces newly initialized gated cross-attention layers [55, 56] before each vision encoder layer (cf., Fig. 3). The keys and values derive from the final text encoder representations, while the queries originate from trainable probe used instead of image inputs. The gates ð¾ allow the model to learn at which layers and to what extent information from the text encoder should flow into the vision encoder. Contrary to existing literature, which often employs tanh gates [57] that initialize to zero (indicating no information flow), we find that using sigmoid gates (0.5 at initialization) accelerates training convergence since, in our case, only little information originates from the vision encoder inputs (i.e., the probe)."
        },
        {
            "title": "Source",
            "content": "curated TEX.SE arXiv artificial DaTikZ DaTikZv2 DaTikZv3 1 566 3 646 42 654 30 609 407 851 326 450 2 256 1 958 981 29 238 85 656 1 957 all 117 832 360 456 469 Table 1. Breakdown of the number of unique TikZ graphics in DaTikZv3 compared to its predecessors DaTikZ and DaTikZv2. Qualitative examples can be found in Appendix E. Training Objective Given caption-image dataset for training, we first embed the patches ð ð of image ð using the unmodified vision encoder of our VLM. Subsequently, we incorporate the cross-attention adapter to obtain the modified encoder (cid:98)M, which we then distill on these image patch embeddings conditioned solely on the caption ð¡ and probe Ëð¤ [58]. This leads to the following objective: Ldist = (cid:16) dist 1 ð ð ð Mð ( ð ð ), (cid:98)Mð , Ëð ( ð Ëð¤, ð¡ ) (cid:17) , (1) where dist(ð, ð) represents distance metric. Following common practices in model distillation [59, 60], we experiment with cosine distance and mean squared error. Here, ð denotes the original model parameters that remain fully frozen, while Ëð represents the adapter parameters of which the cross-attention layers and the image probe are trainable. 4. Datasets & Model Training We introduce DaTikZv3, novel dataset of TikZ graphics programs designed to support the training and evaluation of TikZero. Additionally, we train AutomaTikZv2 as directly comparable baseline operating on the same data source. The DaTikZv3 Dataset DaTikZv3 expands upon its predecessors DaTikZ and DaTikZv2 [2, 3], incorporating programs from curated repositories, TEX.SE, arXiv papers, and artificial samples (cf. Tab. 1). While previous versions focused exclusively on TikZ graphics with (v1) or without (v2) captions, DaTikZv3 systematically extracts captions alongside TikZ graphics whenever possible to support our claims. From over 450k instances, fewer than 170k include captions, underscoring the challenges discussed in Sec. 1. Training TikZero TikZeros VLM builds upon DeTikZify [2] by conditioning LLaMA-based text decoder [61] on patch embeddings from SigLIP vision encoder [62]. Specifically, we combine LLaMA3.1 (8B) [56] with SigLIP SoViT (0.4B). Unlike DeTikZify and inspired by the continued ViT pretraining approach of InternVL 1.5 [63], we initialize the vision encoder with weights from the fine-tuned encoder of PaliGemma [64] and increase"
        },
        {
            "title": "Text Encoder",
            "content": "Self-Attn ð . . . . . . Self-Attn 1 ð¾ ð¾ Cross-Attn ð . . . Cross-Attn"
        },
        {
            "title": "Text Decoder",
            "content": "Self-Attn ð . . . Self-Attn 1 Self-Attn . . . Self-Attn ð"
        },
        {
            "title": "Graphics Program",
            "content": "Figure 3. Architecture overview of TikZero during inference. Solid lines represent the standard caption-conditioned path, which flows through the text encoder into the adapter network of TikZero before connecting to the vision encoder. In certain configurations (cf. Sec. 5.2), the caption also feeds into the text decoder (depicted by dotted lines and markers representing shortcuts). The selfand cross-attention layers (yellow) are simplified representations, omitting internal feed-forward layers and residual connections [53]. An exception is the explicit residual connection between the cross-attention and self-attention layers of the vision encoder, visualizing the gating mechanism ð¾ (purple). Additionally, the dashed path illustrates how the inverse graphics model generates graphics programs when conditioned on images. the input resolution to 420 420 pixels. Furthermore, we fully fine-tune the vision encoder alongside the rest of the model instead of freezing it. We train on DaTikZv3 for 5 epochs with learning rate of 5e5 and batch size of 128. TikZeros VLM consistently outperforms DeTikZify, with detailed evaluation results provided in Appendix A. For the adapter network, we initialize with LLaMA3.2 (1B) as the text encoder [56] and leverage ArxivCap [25], dataset comprising 6.4 million scientific caption-image pairs for training. The adapter accounts for 2 billion of TikZeros 10 billion total parameters, with only 400 million being trainable. We train for 3 epochs with learning rate of 1e4 and batch size of 512. We emphasize that this two-stage training process does not access caption-program pairs. However, we demonstrate that incorporating such aligned data in subsequent fine-tuning step (Sec. 5.2) further enhances performance. Training AutomaTikZv2 Similar to its predecessor, AutomaTikZv2 is token-conditioned LLM that uses tokenized captions as conditioning information for graphics prediction (rather than patch embeddings). We initialize AutomaTikZv2 in two different ways: (i) AutomaTikZv2 (LLM), which starts from vanilla LLaMA3.1 (8B) weights, and (ii) AutomaTikZv2 (VLM), which leverages TikZeros trained VLM (minus the vision encoder) to benefit from transfer learning [65] on its larger training corpus. Both variants employ the same hyperparameters as TikZeros VLM but can only utilize the caption-annotated subset of DaTikZv3 for training. Despite having access to less caption-aligned data than TikZeros adapter network, AutomaTikZv2 requires longer training period primarily due to fine-tuning the large decoder (8 billion trainable parameters versus the adapter networks 400 million). Training requires more than two days for AutomaTikZv2 and 1.5 days for TikZeros adapter network when using eight Nvidia A100 40GB GPUs. 5. Experiments Before training models on DaTikZv3, we extract 1k samples from its captioned subset to form our test set. To mitigate data leakage from pretraining to testing, we only include instances created after the cut-off date specified by LLaMA3.2 and ArxivCap. We also employ an ð-gram matching algorithm to avoid cross-contamination with our training split [7]. For all models, the temperature is set to 0.8 and top-p to 0.95. Example outputs are provided in Fig. 1 and Appendix E. Evaluation Metrics The multimodal nature of our task allows for various evaluation metrics in our automatic evaluations. We assess perceptual image similarity between generated outputs and references by computing DreamSim (DSim) [66, 67], which correlates highly with human judgments for scientific images [2]. We also calculate the Kernel Inception Distance (KID) [68] using SigLIP image features, which evaluates the overall quality of generated figures by comparing to the distribution of reference figures. We evaluate caption similarity between generated outputs and reference captions using CLIPScore (CLIP) [69] with SigLIP features. To measure code similarity between generated and reference TikZ programs, we use CrystalBLEU (cBLEU), BLEU variant optimized for code evaluation [70, 71], and TEX Edit Distance (TED) [2], variant of the Extended"
        },
        {
            "title": "Models",
            "content": "IDEFICS 3 (8B) AutomaTikZ (13B) AutomaTikZv2 (VLM) AutomaTikZv2 (LLM) TikZero (MSE) TikZero (Cos) DSim KID 11.426 45.475 46.033 1.294 33.203 38.313 3.491 50.548 5.664 52.024 5.103 52.829 CLIP 14.327 3.955 0.775 15.766 10.583 10.051 cBLEU 0.656 0.386 0.328 0.658 1.723 1.603 TED MTE AVG CLIP Ratio 33.858 66.628 63.175 74.975 63.093 62.24 36.597 0.0 76.985 50.753 82.375 62.307 85.004 66.07 77.831 71.893 65.51 85.599 69.558 85.866 21.595 81.775 79.318 82. 4.851 2.965 0.284 8.002 8.237 7.226 Table 2. System-level scores 100 for TikZero and baselines of comparable size and training setup. Bold and underlined values denote the best and second-best scores for each metric column, respectively. Cell shading illustrates relative score magnitudes. Arrows indicate metric directionality. Overall, TikZero achieves the strongest average performance across metrics. Edit Distance [72] utilizing TEX tokenizer. Since some metrics require that generated programs compile to images, resampling is necessary if the output contains irrecoverable errors. To quantify this, we compute the Mean Token Efficiency (MTE), defined as the 10% winsorized mean of the ratio between the number of tokens in the final TikZ program and the total number of tokens generated to produce that program. For comprehensive view of model performance, we calculate the arithmetic mean (AVG) of all previous metrics. As these metrics operate on different scales, we apply min-max normalization before computing the average. Additionally, some metrics are recomputed with redacted text in the outputs as part of our analysis, cf. Sec. 6.1. 5.1. Comparison against End-to-End Fine-Tuning In our initial experiment, we evaluate the zero-shot performance of TikZero, trained as described in Secs. 3 & 4 using either cosine distance (Cos) or mean squared error (MSE), and compare it against end-to-end trained baselines. Baselines Besides AutomaTikZv2 (LLM & VLM), which we designed as directly comparable baselines, we assess other token-conditioned models of similar and slightly larger sizes trained on TikZ or LATEX. Specifically, we evaluate AutomaTikZ (13B),3 the strongest original AutomaTikZ baseline [3], and the general-purpose chatbot IDEFICS 3 (8B) [21]. Further details are available in Appendix B. Results We present the system-level metric scores in Tab. 2 (Original Text). On average, TikZero, trained with cosine distance, achieves the best performance with an AVG score of 85.599, closely followed by the MSE variant at 85.004. The next best model, AutomaTikZv2 (LLM), scores 82.375, which is 3 percentage points (pp) lower. The remaining models exhibit substantial performance gap, with IDEFICS 3 (8B) and AutomaTikZ (13B) falling behind by approximately 20pp and AutomaTikZv2 (VLM) showing the weakest performance across all metrics, resulting in an AVG score of 3Belouadi et al. [3] refer to this model as CLiMA (13B). 0. The surprisingly poor results of AutomaTikZv2 (VLM) are likely due to catastrophic forgetting [73], as the removal of the vision encoder from TikZeros VLM necessitates reacquisition of conditioning based solely on text. As for individual metrics, our adapter-based models perform particularly well in perceptual image similarity, with TikZero (Cos) outperforming the best baseline, AutomaTikZv2 (LLM), by 3pp on DreamSim. Although AutomaTikZv2 (LLM) outperforms TikZero by 1.5pp on KID, this indicates in this context that such token-conditioned models (compared to those using patch embeddings) capture the general appearance of scientific figures well but fall short in inferring visual specifics from captions. They do, however, have an edge in reproducing text from captions, which we identify as the primary reason for up to 5pp higher CLIPScore, as noted in Sec. 6.1. Regarding code similarity, both TikZero models considerably outperform others on cBLEU. Interestingly, we observe mild inverse correlation between cBLEU and TED. Models conditioned solely on tokenized captions tend to generate shorter, often simplified programs [3], potentially resulting in reduced edit distance to the reference. In terms of efficiency, all models achieve an MTE of 8085, indicating that only 2 out of 10 inferences require resampling. TikZero (Cos) is 3pp more efficient than MSE, while AutomaTikZ (13B), likely benefiting from its larger model size, exceeds it by another 3pp. In summary, training AutomaTikZv2 on top of VLM yields worse performance than training based on vanilla LLaMA3.1, indicating that effective end-to-end training can only leverage the small intersection of graphics programs and images with captions, as illustrated in Fig. 2. However, even without access to this intersection, TikZero surpasses both AutomaTikZ (13B) and AutomaTikZv2 (LLM) on average by being able to train on images with captions independently of graphics programs. Moreover, using loss function based on cosine distance proves more effective than using MSE."
        },
        {
            "title": "Models",
            "content": "Qwen2.5 Coder (32B) GPT-4o TikZero (Cos) + Fine-tuning (i) + Separate Captions (ii) + Weight Resetting (iii) DSim KID CLIP 24.87 5.493 54.473 2.844 31.787 56.464 10.051 5.103 52.829 10.687 53.203 1.794 15.72 2.905 52.983 24.177 1.831 56.295 cBLEU 0.285 0.327 1.603 0.759 0.804 1.988 TED MTE AVG CLIP Ratio 48.911 59.856 41.905 58.511 65.51 71.893 60.931 61.572 55.608 61.32 47.478 59.008 48.593 12.164 79.019 13.32 7.226 14.658 6.512 46.497 8.741 46.326 87.043 11.479 97.269 97.675 82.291 94.851 95.722 93. Table 3. System-level scores 100 for additional baselines and TikZero combined with fine-tuning and token-conditioning. The scores for TikZero (Cos) are replicated from Tab. 2 for convenience. Bold and underlined values denote the best and second-best scores for each metric column, respectively. Cell shading illustrates relative score magnitudes. Arrows indicate metric directionality. Overall, TikZero (Cos) with Weight Resetting (iii) demonstrates the strongest average performance across metrics. 5.2. Combining Adapters with Fine-Tuning In this section, we investigate whether explicitly incorporating the subset of DaTikZv3 that includes captions into the training process of TikZero enhances performance. Our approach involves three incremental stages: (i) We perform light fine-tuning of TikZero (Cos) end-to-end on captionprogram pairs for one epoch with low learning rate of 1e5. Extending the training duration or increasing the learning rate does not yield further performance gains, likely due to the decoder having already reached its saturation point; (ii) Alongside feeding captions into the adapter, we provide them separately to the text decoder in tokenized form (cf., Fig. 3); (iii) Prior to fine-tuning, we reset the decoder to its initial weights to overcome saturation, enabling us to fine-tune using the setup described in Sec. 4, which involves 5 epochs and learning rate of 5e5. Baselines In addition to the baselines in Tab. 2, which remain comparable, we also evaluate larger and commercial models that serve as stronger baselines (cf. Appendix B). Specifically, we assess GPT-4o [7], which has demonstrated strong performance in generating TikZ [3, 27, 29] and Qwen2.5 Coder (32B) [74] as an open-weights model. Results In Tab. 3 (Original Text), all fine-tuning setups of TikZero show considerable improvement over the base version. Approaches (i) and (ii) each enhance performance by over 30pp on AVG, while approach (iii) surpasses them with an improvement of over 70pp, positioning it as the best-performing model on average, even when compared to our new baselines, with GPT-4o being 8pp lower and Qwen2.5 Coder (32B) approximately 40pp lower. Approach (i) demonstrates that direct fine-tuning yields positive effects across nearly all metrics, notably improving MTE by 12pp, TED by 4pp, and KID by 3.5pp. Approach (ii) shows similar trends but, by also incorporating tokenized captions, further improves CLIPScore by 5pp, closing the gap to AutomaTikZv2 (LLM). Interestingly, both (i) and (ii) slightly decrease performance on cBLEU, potentially due to similar reasons discussed in Sec. 5.1. However, the same cannot be said for (iii), which not only achieves the highest score on cBLEU but also ranks as the second-best on TED, trailing only 0.5pp behind GPT-4o and showcasing that it is possible to perform well on both metrics. Additionally, it increases DreamSim by another 3pp and CLIPScore by 8.5pp, competing with the much stronger baselines Qwen2.5 Coder (32B) and GPT-4o. In KID, it even surpasses them by 3.5pp and 1pp, respectively. In summary, fine-tuning TikZero, especially when combined with separate caption input and weight resetting, greatly improves performance. This illustrates that the intersection of graphics programs and images with captions, though small, provides valuable training signal, and best performance can be achieved by making full use of both sets. The best-performing TikZero model even competes with and often surpasses Qwen2.5 Coder (32B) and GPT-4o on several key metrics. Notably, the former model is more than three times larger, and the latter is often estimated at around 1.8 trillion parameters [75], making it 180 times larger. 5.3. Human Evaluation To corroborate our findings from automatic evaluation, we conduct human annotation campaign focusing on two key properties: caption and image similarity. We employ BestWorst Scaling (BWS) [76], comparative annotation method that yields high-quality results even with few annotators [77, 78]. We sample 100 instances from our test set and present annotators with ð-tuples of generated figures, asking them to identify the most and least similar figure to either the reference caption or reference image. This data is then transformed into scores from -1 (poor) to 1 (excellent) by subtracting the proportion of times figure is selected as the best from the proportion of times it is chosen as the worst [79]. For manageable workload, we focus on ð = 4 key models: TikZero (Cos), our best-performing model 6 tween TikZero+ and GPT-4o, appear more pronounced than observed with DreamSim. This discrepancy likely stems from BWS capturing relative preferences rather than absolute performance differences. GPT-4o is selected 20% more often as the best model than TikZero+, and AutomaTikZv2 (LLM) 15% more often as the worst model than TikZero (Cos), creating larger perceived gaps even when qualitative differences may be subtle. The SHR values of 0.68 for caption similarity and 0.76 for image similarity indicate moderate to strong inter-annotator agreement. We also observe correlation between these two tasks, with segment-level ð = 0.62 and system-level ð = 0.8, suggesting that both evaluation dimensions capture related aspects of model performance. GPT-4o emerges as the bestperforming model, aligning with its superior performance on the corresponding automatic metrics, CLIPScore and DSim. Among open-source models, TikZero+ performs best, while AutomaTikZv2 (LLM) ranks lowest overall. 6. Analysis We present comprehensive analysis, investigating the influence of typographic attacks on CLIPScore and examining the effectiveness of our architecture in low-resource settings, both in terms of training data and trainable parameters. 6.1. CLIPScore Limitations & Typographic Attacks known limitation of CLIPScore with text-rich images is its susceptibility to typographic attacks, where scores are disproportionately influenced by string similarity between images and captions [3, 80]. We suspect that token-conditioned models like AutomaTikZv2 (LLM) achieve higher CLIPScore values than models such as TikZero (Cos & MSE) primarily because they tend to visibly copy more substrings from the caption in the output image. To test this hypothesis, we apply the ROT13 substitution cipher [81] to all visible strings in the generated figures and recompute CLIPScore. This basic cipher replaces each letter with the 13th letter after it in the Latin alphabet. While not cryptographically secure, the ratio between the original and recomputed CLIPScore values should indicate the influence of string matching, i.e., higher ratios suggest less copied text and vice versa. Tabs. 2 & 3 (Redacted Text) present the recomputed CLIPScore values and ratios for all evaluated models. The results reveal that most TikZero models, except for fine-tuning approaches (ii) and (iii), which also condition on tokenized captions, have considerably higher ratios (61%78%) compared to strictly token-conditioned models (34%51%), supporting our hypothesis. AutomaTikZ (13B) is an exception, possibly due to its initially low score. Further analysis shows that with redacted text, AutomaTikZv2 (LLM)s CLIPScore performance drops to the same level as TikZero (Cos & MSE), suggesting that string matching is the primary factor in its superior performance rather than producing better visuals Figure 4. Bivariate distributions of BWS scores (higher is better) using kernel density estimation for caption and image similarity. Along the diagonal, TikZero (Cos) achieves higher scores than AutomaTikZv2 (LLM), while TikZero+ and GPT-4o demonstrate superior performance compared to both. from Sec. 5.1; AutomaTikZv2 (LLM), its direct end-to-end trained competitor; GPT-4o, our strongest baseline; and TikZero (Cos) fine-tuned using approach (iii) from Sec. 5.2, our best model overall, henceforth referred to as TikZero+ for convenience. We engage thirteen annotators and obtain six fully annotated sets per task (cf. Appendix for more details). To assess annotator consistency, we calculate the split-half reliability (SHR) [78]. This method randomly divides all annotations into two sets, calculates scores independently, and then determines their correlation using Spearmans ð. Results Fig. 4 presents kernel density estimates for the BWS scores, showing generally consistent rankings with automatic evaluations but revealing notable differences in the magnitude of gaps. For caption similarity, the ranking aligns with CLIPScore evaluations, with TikZero (Cos), AutomaTikZv2 (LLM), TikZero+, and GPT-4o achieving mean scores ð of -0.25, -0.18, 0.03, and 0.4, respectively. Interestingly, humans perceive 40% smaller gap between AutomaTikZv2 (LLM) and TikZero (Cos) than suggested by CLIPScore values, where AutomaTikZv2 (LLM) outperforms TikZero (Cos) by 50%. This indicates humans may evaluate caption similarity differently than CLIPScore (cf. Sec. 6.1). For image similarity, the system order remains consistent with our DreamSim metric, with AutomaTikZv2 (LLM), TikZero (Cos), TikZero+, and GPT-4o achieving ð of -0.26, -0.02, 0.01, and 0.27, respectively. However, the relative gaps between models differ: the separation between AutomaTikZv2 (LLM) and TikZero (Cos), as well as be7 Intv. 100% 1 2 4 8 92.411 87.557 82.254 76."
        },
        {
            "title": "Training Data",
            "content": "50% 77.478 85.249 47.381 40.816 25% 49.967 54.942 32.12 29.774 12.5% 56.055 33.817 37.914 16.25 Table 4. AVG scores for TikZero (Cos) trained on varying fractions of data and intervals of cross-attention layers. Higher scores indicate better performance. Bold and underlined values denote the best and second-best scores for the whole table, respectively. Cell shading illustrates score magnitudes. arguably more difficult task. Nevertheless, reproducing strings is still somewhat desirable. The human oracle of our test set achieves ratio of 50.8%, close to the 47.5% of TikZero+, our best-performing model. In contrast, models like GPT-4o, with lower ratio of 41.9%, may overfit to caption copying, artificially inflating the CLIPScore values. 6.2. Low-Resource Training , 1 4 While our adapters train efficiently on large-scale datasets, we investigate whether such extensive data is necessary for optimal performance. Along the same vein, we examine the impact of reducing the amount of cross-attention layers inserted into the vision encoder. We retrain TikZero (Cos) using varying fractions of the training data (ð {1, 1 , 1 8 }) and insert 2 cross-attention layers at different intervals (ð {1, 2, 4, 8}). Table 4 presents the AVG scores from this parameter grid, with detailed scores in Appendix C. Our findings reveal that utilizing the full dataset and inserting cross-attention at every layer yields the highest average performance, highlighting the benefits of maximizing both variables. Interestingly, the models performance appears more robust to reduction in the number of layers compared to decrease in training data. For instance, training on only 1 8 th of the data leads to substantial performance drop of 36pp, whereas inserting cross-attention layers every 8 layers (resulting in only 3 cross-attention layers in total) causes more modest decline of 16pp. Minimizing both variables leads to the most severe drop of over 75pp. These results validate our training setup while suggesting that incorporating additional data might further enhance performance. Given that ArxivCap extracts figures from only 572k papers, whereas some corpora index over 200 million papers [82], there remains lot of potential for leveraging larger datasets in future work. 7. Conclusion In this work, we demonstrate the potential of TikZero and its variants for generating TikZ graphics programs from captions. Notably, TikZero does not require aligned caption-program pairs in its original formulation but instead aligns representation spaces of unaligned graphics programs and captioned images. This enables our model to leverage substantially more training data compared to end-to-end trained models that operate solely on caption-image pairs (cf. Fig. 2) while maintaining training efficiency. TikZero outperforms strong end-to-end trained baselines, including our independently trained AutomaTikZv2 models, which use the same data pool, excluding instances they cannot process, illustrating the strengths of our approach. When extending the TikZero approach with additional end-to-end training, it also compares favorably to much larger baselines and commercial systems like GPT-4o. While this enhanced approach, TikZero+, is no longer zero-shot by our definition, it remains TikZero model in the sense that it operates on both sets of graphics programs and captioned images, with the added advantage of explicitly utilizing their intersection (cf. Fig. 2). These results demonstrate the benefits of designing architectures around available data and validate the approach of decoupling graphics program generation from text understanding (with optional later reconciliation through TikZero+). Although we demonstrate our method specifically on TikZ, we believe its general principles will inspire future work on related graphics program synthesis tasks. Future Work Beyond scaling up our training data to explore convergence limits (cf. Sec. 6.2), we plan to investigate automatic methods for improving the quality and alignment of caption-image or caption-program pairs. This includes rewriting potentially noisy captions with LLMs and enhancing them with the visual understanding capabilities of VLMs [8385]. We believe our approach to aligning textual and image modalities enables other promising applications for graphics program synthesis, such as editing images in latent space via textual instructions to generate modified graphics programs. Additionally, we intend to explore alternative alignment strategies beyond model distillation, including contrastive learning [86], which has successfully aligned modalities in discriminative models [6, 87, 88]."
        },
        {
            "title": "Limitations",
            "content": "Our evaluations include proprietary systems that operate as black boxes; their training data is unknown, and they offer no guarantees of consistent performance over time. This (i) makes addressing data leakage and cross-contamination impossible and (ii) limits the fairness and reproducibility of our experiments. Nevertheless, even under these unfavorable conditions, our open models remain competitive. Users should be aware, however, that our models may behave unpredictably, and outputs might differ from expectations. Additionally, our models do not include safeguards against potential misuse, e.g., for generating fake scientific content. Regarding licensing of our training data, large portion 8 of the TikZ programs in DaTikZv3 are licensed under permissive terms4 that allow redistribution. The remaining programs are distributed under the arXiv.org perpetual, nonexclusive license, which prohibits redistribution, which is why we exclude them from the public release of DaTikZv3. However, since we open-source our dataset creation scripts, we encourage others to reproduce the full version independently."
        },
        {
            "title": "Acknowledgments",
            "content": "We extend our sincere gratitude to the following individuals (in no particular order) for their valuable contributions: Christian Greisinger, Hour Kaing, Ran Zhang, Tejaswini Medi, Yanran Chen, Sotaro Takeshita, Katharina Prasse, JiWoo Kim, Christoph Leiter, Haiyue Song, and Aida Kostikova. Their assistance with our human evaluation campaign, proofreading, insightful discussions, and constructive feedback has been instrumental to our work. The first author conducted part of this research during an internship at the National Institute of Information and Communications Technology (NICT), Japan. The second to last author is supported by the Federal Ministry of Education and Research (BMBF) via the research grant Metrics4NLG and the German Research Foundation (DFG) via the Heisenberg Grant EG 375/51. We acknowledge computing resources provided by the state of Baden-WÃ¼rttemberg through bwHPC and the German Research Foundation (DFG) through grant INST 35/15971 FUGG. Finally, we thank the OpenMoji project for the open-source icons used throughout this work."
        },
        {
            "title": "References",
            "content": "[1] Till Tantau. The TikZ and PGF Packages, 2023. 1 [2] Jonas Belouadi, Simone Paolo Ponzetto, and Steffen Eger. DeTikZify: Synthesizing graphics programs for scientific figures and sketches with TikZ. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. 1, 2, 3, 4, 15 [3] Jonas Belouadi, Anne Lauscher, and Steffen Eger. AutomaTikZ: Text-guided synthesis of scientific vector graphics with TikZ. In The Twelfth International Conference on Learning Representations, 2024. 1, 3, 5, 6, 7, 15, 16 [4] Leixin Zhang, Yinjie Cheng, Weihe Zhai, Steffen Eger, Jonas Belouadi, Fahimeh Moafian, and Zhixue Zhao. ScImage: How good are multimodal large language models at scientific text-to-image generation? In The Thirteenth International Conference on Learning Representations, 2025. 1, 15 [5] Abhay Zala, Han Lin, Jaemin Cho, and Mohit Bansal. DiagrammerGPT: Generating open-domain, open-platform diagrams via LLM planning. In First Conference on Language Modeling, 2024. 1 [6] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda 4https : / / creativecommons . org / licenses; https : / / opensource.org/license/mit; https://www.gnu.org/licenses/ fdl-1.3.en.html; https://openai.com/policies/terms-of-use Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision, 2021. 2, 8 [7] OpenAI. GPT-4 technical report, 2023. 2, 4, 6 [8] Emilio Parisotto, Abdel rahman Mohamed, Rishabh Singh, Lihong Li, Dengyong Zhou, and Pushmeet Kohli. Neurosymbolic program synthesis. In International Conference on Learning Representations, 2017. 2 [9] Jacob Devlin, Jonathan Uesato, Surya Bhupatiraju, Rishabh Singh, Abdel rahman Mohamed, and Pushmeet Kohli. RobustFill: Neural program learning under noisy I/O. In Proceedings of the 34th International Conference on Machine Learning, pages 990998. PMLR, 2017. [10] Kevin Ellis, Catherine Wong, Maxwell Nye, Mathias SablÃ©Meyer, Lucas Morales, Luke Hewitt, Luc Cary, Armando Solar-Lezama, and Joshua B. Tenenbaum. DreamCoder: bootstrapping inductive program synthesis with wake-sleep library learning. In Proceedings of the 42nd ACM SIGPLAN International Conference on Programming Language Design and Implementation, page 835850, New York, NY, USA, 2021. Association for Computing Machinery. 2 [11] Yaroslav Ganin, Tejas Kulkarni, Igor Babuschkin, S. M. Ali Eslami, and Oriol Vinyals. Synthesizing programs for images using reinforced adversarial learning. In Proceedings of the 35th International Conference on Machine Learning, pages 16661675. PMLR, 2018. 2 [12] Kevin Ellis, Daniel Ritchie, Armando Solar-Lezama, and Josh Tenenbaum. Learning to infer graphics programs from hand-drawn images. In Thirty-second Conference on Neural Information Processing Systems, pages 60626071, 2018. 2 [13] Kevin Ellis, Maxwell Nye, Yewen Pu, Felix Sosa, Josh Tenenbaum, and Armando Solar-Lezama. Write, execute, assess: Program synthesis with REPL. In Advances in Neural Information Processing Systems. Curran Associates, Inc., 2019. 2 [14] Peter Kulits, Haiwen Feng, Weiyang Liu, Victoria Fernandez Abrevaya, and Michael J. Black. Re-thinking inverse graphics with large language models. Transactions on Machine Learning Research, 2024. [15] Wen-Ding Li and Kevin Ellis. Is programming by example solved by LLMs? In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. [16] Shreyas Kapur, Erik Jenner, and Stuart Russell. Diffusion In The Thirteenth on syntax trees for program synthesis. International Conference on Learning Representations, 2025. 2 [17] Gopal Sharma, Rishabh Goyal, Difan Liu, Evangelos Kalogerakis, and Subhransu Maji. CSGNet: Neural shape parser for constructive solid geometry. In 2018 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2018, Salt Lake City, UT, USA, June 18-22, 2018, pages 55155523. Computer Vision Foundation / IEEE Computer Society, 2018. [18] Yonglong Tian, Andrew Luo, Xingyuan Sun, Kevin Ellis, William T. Freeman, Joshua B. Tenenbaum, and Jiajun Wu. Learning to infer and execute 3D shape programs. In International Conference on Learning Representations, 2019. [19] Javier CÃ¡mara, Javier Troya, Lola BurgueÃ±o, and Antonio Vallecillo. On the assessment of generative AI in modeling 9 tasks: an experience report with chatgpt and UML. Softw. Syst. Model., 22(3):781793, 2023. 2 Time? https://hdsr.mitpress.mit.edu/pub/y95zitmz. Harvard Data Science Review, 6(2), 2024. [20] Hugo LaurenÃ§on, Leo Tronchon, Matthieu Cord, and Victor Sanh. What matters when building vision-language models? In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. 2 [21] Hugo LaurenÃ§on, AndrÃ©s Marafioti, Victor Sanh, and LÃ©o Tronchon. Building and better understanding vision-language models: insights and future directions, 2024. 5 [22] Shengbang Tong, Ellis Brown II, Penghao Wu, Sanghyun Woo, Adithya Jairam Iyer, Sai Charitha Akula, Shusheng Yang, Jihan Yang, Manoj Middepogu, Ziteng Wang, Xichen Pan, Rob Fergus, Yann LeCun, and Saining Xie. Cambrian1: fully open, vision-centric exploration of multimodal LLMs. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. 2 [23] Juan A. Rodriguez, Shubham Agarwal, Issam H. Laradji, Pau Rodriguez, David Vazquez, Christopher Pal, and Marco Pedersoli. StarVector: Generating scalable vector graphics code from images, 2023. 2 [24] Haotian Zhang, Mingfei Gao, Zhe Gan, Philipp Dufter, Nina Wenzel, Forrest Huang, Dhruti Shah, Xianzhi Du, Bowen Zhang, Yanghao Li, Sam Dodge, Keen You, Zhen Yang, Aleksei Timofeev, Mingze Xu, Hong-You Chen, Jean-Philippe Fauconnier, Zhengfeng Lai, Haoxuan You, Zirui Wang, Afshin Dehghan, Peter Grasch, and Yinfei Yang. MM1.5: Methods, analysis & insights from multimodal LLM fine-tuning, 2024. [25] Lei Li, Yuqi Wang, Runxin Xu, Peiyi Wang, Xiachong Feng, Lingpeng Kong, and Qi Liu. Multimodal ArXiv: dataset for improving scientific comprehension of large vision-language In Proceedings of the 62nd Annual Meeting of models. the Association for Computational Linguistics (Volume 1: Long Papers), pages 1436914387, Bangkok, Thailand, 2024. Association for Computational Linguistics. 2, 4 [26] Jaeyoung Kim, Jongho Lee, Hong-Jun Choi, Ting-Yao Hsu, Chieh-Yang Huang, Sungchul Kim, Ryan Rossi, Tong Yu, Clyde Lee Giles, Ting-Hao Kenneth Huang, and Sungchul Choi. Multi-LLM collaborative caption generation in scientific documents, 2025. 2 [27] SÃ©bastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, Harsha Nori, Hamid Palangi, Marco Tulio Ribeiro, and Yi Zhang. Sparks of artificial general intelligence: Early experiments with GPT-4, 2023. 2, 6 [28] Pratyusha Sharma, Tamar Rott Shaham, Manel Baradad, Stephanie Fu, Adrian Rodriguez-Munoz, Shivam Duggal, Phillip Isola, and Antonio Torralba. vision check-up for language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1441014419, 2024. [29] Tianjun Zhang, Yi Zhang, Vibhav Vineet, Neel Joshi, and Xin Wang. Controllable text-to-image generation with GPT-4, 2023. 2, 6 [30] Lingjiao Chen, Matei Zaharia, Behavior ChatGPTs How Is and James Zou. Changing Over 10 [31] Sagi Polaczek, Yuval Alaluf, Elad Richardson, Yael Vinker, and Daniel Cohen-Or. NeuralSVG: An implicit representation for text-to-vector generation, 2025. 2 [32] Ronghuan Wu, Wanchao Su, Kede Ma, and Jing Liao. IconShop: Text-guided vector icon synthesis with autoregressive transformers. ACM Trans. Graph., 42(6), 2023. [33] Ajay Jain, Amber Xie, and Pieter Abbeel. VectorFusion: Text-to-SVG by abstracting pixel-based diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 19111920, 2023. [34] Kevin Frans, Lisa B. Soros, and Olaf Witkowski. CLIPDraw: Exploring text-to-drawing synthesis through language-image encoders. In NeurIPS, 2022. 2 [35] Henrik Voigt, Kai Lawonn, and Sina ZarrieÃ. Plots made quickly: An efficient approach for generating visualizations from natural language queries. In Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024), pages 1278712793, Torino, Italia, 2024. ELRA and ICCL. 2 [36] Yuyu Luo, Nan Tang, Guoliang Li, Chengliang Chai, Wenbo Li, and Xuedi Qin. Synthesizing natural language to visualization (nl2vis) benchmarks from NL2SQL benchmarks. In Proceedings of the 2021 International Conference on Management of Data, page 12351247, New York, NY, USA, 2021. Association for Computing Machinery. [37] Jock Mackinlay. Automating the design of graphical presentations of relational information. ACM Trans. Graph., 5(2): 110141, 1986. [38] Steven F. Roth, John Kolojejchick, Joe Mattis, and Jade Goldstein. Interactive graphic design using automatic presentation knowledge. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, page 112117, New York, NY, USA, 1994. Association for Computing Machinery. [39] Yang Wu, Yao Wan, Hongyu Zhang, Yulei Sui, Wucai Wei, Wei Zhao, Guandong Xu, and Hai Jin. Automated data visualization from natural language via large language models: An exploratory study. Proc. ACM Manag. Data, 2(3), 2024. 2 [40] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and BjÃ¶rn Ommer. High-resolution image synthesis with latent diffusion models. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022, New Orleans, LA, USA, June 18-24, 2022, pages 1067410685. IEEE, 2022. 2 [41] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation, 2021. [42] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with CLIP latents, 2022. 2 [43] Ming Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng, Chang Zhou, Da Yin, Junyang Lin, Xu Zou, Zhou Shao, Hongxia Yang, and Jie Tang. CogView: Mastering text-to-image generation via transformers. In Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, pages 1982219835, 2021. [44] Ming Ding, Wendi Zheng, Wenyi Hong, and Jie Tang. CogView2: Faster and better text-to-image generation via hierarchical transformers. In NeurIPS, 2022. 2 [45] Juan A. Rodriguez, David VÃ¡zquez, Issam H. Laradji, Marco Pedersoli, and Pau RodrÃ­guez. FigGen: Text to scientific figure generation. In The First Tiny Papers Track at ICLR 2023, Tiny Papers @ ICLR 2023, Kigali, Rwanda, May 5, 2023. OpenReview.net, 2023. 2 [46] Juan A. Rodriguez, David VÃ¡zquez, Issam H. Laradji, Marco Pedersoli, and Pau RodrÃ­guez. OCR-VQGAN: Taming textwithin-image generation. In IEEE/CVF Winter Conference on Applications of Computer Vision, WACV 2023, Waikoloa, HI, USA, January 2-7, 2023, pages 36783687. IEEE, 2023. 2 [47] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks. Commun. ACM, 63(11):139144, 2020. [48] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1287312883, 2021. 2 [49] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In Proceedings of the 32nd International Conference on Machine Learning, pages 22562265, Lille, France, 2015. PMLR. 2 [50] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In International Conference on Learning Representations, 2021. 2 [51] Yiwei Hu, Paul Guerrero, Milos Hasan, Holly Rushmeier, and Valentin Deschaintre. Generating procedural materials from text or image prompts. In ACM SIGGRAPH 2023 Conference Proceedings, New York, NY, USA, 2023. Association for Computing Machinery. 3 [52] Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing Sun, Tong Xu, and Enhong Chen. survey on multimodal large language models. National Science Review, 11(12):nwae403, 2024. 3 [53] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 770778, 2016. [54] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Å ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems. Curran Associates, Inc., 2017. 3 [55] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob Menick, Sebastian Borgeaud, Andrew Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals, Andrew Zisserman, and Karen Simonyan. Flamingo: visual language model for few-shot learning. In Advances in Neural Information Processing Systems, 2022. 3 [56] Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurelien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Roziere, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, Danny Wyatt, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip Radenovic, Francisco GuzmÃ¡n, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Govind Thattai, Graeme Nail, Gregoire Mialon, Guan Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov, Imanol Arrieta Ibarra, Isabel Kloumann, Ishan Misra, Ivan Evtimov, Jack Zhang, Jade Copet, Jaewon Lee, Jan Geffert, Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasuden Alwala, Karthik Prasad, Kartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heafield, Kevin Stone, Khalid El-Arini, Krithika Iyer, Kshitiz Malik, Kuenley Chiu, Kunal Bhalla, Kushal Lakhotia, Lauren Rantala-Yeary, Laurens van der Maaten, Lawrence Chen, Liang Tan, Liz Jenkins, Louis Martin, Lovish Madaan, Lubo Malo, Lukas Blecher, Lukas Landzaat, Luke de Oliveira, Madeline Muzzi, Mahesh Pasupuleti, Mannat Singh, Manohar Paluri, Marcin Kardas, Maria Tsimpoukelli, Mathew Oldham, Mathieu Rita, Maya Pavlova, Melanie Kambadur, Mike Lewis, Min Si, Mitesh Kumar Singh, Mona Hassan, Naman Goyal, Narjes Torabi, Nikolay Bashlykov, Nikolay Bogoychev, Niladri Chatterji, Ning Zhang, Olivier Duchenne, Onur Ãelebi, Patrick Alrassy, Pengchuan Zhang, Pengwei Li, Petar Vasic, Peter Weng, Prajjwal Bhargava, Pratik Dubal, Praveen Krishnan, Punit Singh Koura, Puxin Xu, Qing He, Qingxiao Dong, Ragavan Srinivasan, Raj Ganapathy, Ramon Calderer, Ricardo Silveira Cabral, Robert Stojnic, Roberta Raileanu, Rohan Maheswari, Rohit Girdhar, Rohit Patel, Romain Sauvestre, Ronnie Polidoro, Roshan Sumbaly, Ross Taylor, Ruan Silva, Rui Hou, Rui Wang, Saghar Hosseini, Sahana Chennabasappa, Sanjay Singh, Sean Bell, Seohyun Sonia Kim, Sergey Edunov, Shaoliang Nie, Sharan Narang, Sharath Raparthy, Sheng Shen, Shengye Wan, Shruti Bhosale, Shun Zhang, Simon Vandenhende, Soumya Batra, Spencer Whitman, Sten Sootla, Stephane Collot, Suchin Gururangan, Sydney Borodinsky, Tamar Herman, Tara Fowler, Tarek Sheasha, Thomas Georgiou, Thomas Scialom, Tobias Speckbacher, Todor Mihaylov, Tong Xiao, Ujjwal Karn, Vedanuj Goswami, Vibhor Gupta, Vignesh Ramanathan, Viktor Kerkez, Vincent Gonguet, Virginie Do, Vish Vogeti, VÃ­tor Albiero, Vladan Petrovic, Weiwei Chu, Wenhan Xiong, Wenyin Fu, Whitney 11 Meers, Xavier Martinet, Xiaodong Wang, Xiaofang Wang, Xiaoqing Ellen Tan, Xide Xia, Xinfeng Xie, Xuchao Jia, Xuewei Wang, Yaelle Goldschlag, Yashesh Gaur, Yasmine Babaei, Yi Wen, Yiwen Song, Yuchen Zhang, Yue Li, Yuning Mao, Zacharie Delpierre Coudert, Zheng Yan, Zhengxing Chen, Zoe Papakipos, Aaditya Singh, Aayushi Srivastava, Abha Jain, Adam Kelsey, Adam Shajnfeld, Adithya Gangidi, Adolfo Victoria, Ahuva Goldstand, Ajay Menon, Ajay Sharma, Alex Boesenberg, Alexei Baevski, Allie Feinstein, Amanda Kallet, Amit Sangani, Amos Teo, Anam Yunus, Andrei Lupu, Andres Alvarado, Andrew Caples, Andrew Gu, Andrew Ho, Andrew Poulton, Andrew Ryan, Ankit Ramchandani, Annie Dong, Annie Franco, Anuj Goyal, Aparajita Saraf, Arkabandhu Chowdhury, Ashley Gabriel, Ashwin Bharambe, Assaf Eisenman, Azadeh Yazdan, Beau James, Ben Maurer, Benjamin Leonhardi, Bernie Huang, Beth Loyd, Beto De Paola, Bhargavi Paranjape, Bing Liu, Bo Wu, Boyu Ni, Braden Hancock, Bram Wasti, Brandon Spence, Brani Stojkovic, Brian Gamido, Britt Montalvo, Carl Parker, Carly Burton, Catalina Mejia, Ce Liu, Changhan Wang, Changkyu Kim, Chao Zhou, Chester Hu, Ching-Hsiang Chu, Chris Cai, Chris Tindal, Christoph Feichtenhofer, Cynthia Gao, Damon Civin, Dana Beaty, Daniel Kreymer, Daniel Li, David Adkins, David Xu, Davide Testuggine, Delia David, Devi Parikh, Diana Liskovich, Didem Foss, Dingkang Wang, Duc Le, Dustin Holland, Edward Dowling, Eissa Jamil, Elaine Montgomery, Eleonora Presani, Emily Hahn, Emily Wood, Eric-Tuan Le, Erik Brinkman, Esteban Arcaute, Evan Dunbar, Evan Smothers, Fei Sun, Felix Kreuk, Feng Tian, Filippos Kokkinos, Firat Ozgenel, Francesco Caggioni, Frank Kanayet, Frank Seide, Gabriela Medina Florez, Gabriella Schwarz, Gada Badeer, Georgia Swee, Gil Halpern, Grant Herman, Grigory Sizov, Guangyi, Zhang, Guna Lakshminarayanan, Hakan Inan, Hamid Shojanazeri, Han Zou, Hannah Wang, Hanwen Zha, Haroun Habeeb, Harrison Rudolph, Helen Suk, Henry Aspegren, Hunter Goldman, Hongyuan Zhan, Ibrahim Damlaj, Igor Molybog, Igor Tufanov, Ilias Leontiadis, Irina-Elena Veliche, Itai Gat, Jake Weissman, James Geboski, James Kohli, Janice Lam, Japhet Asher, Jean-Baptiste Gaya, Jeff Marcus, Jeff Tang, Jennifer Chan, Jenny Zhen, Jeremy Reizenstein, Jeremy Teboul, Jessica Zhong, Jian Jin, Jingyi Yang, Joe Cummings, Jon Carvill, Jon Shepard, Jonathan McPhie, Jonathan Torres, Josh Ginsburg, Junjie Wang, Kai Wu, Kam Hou U, Karan Saxena, Kartikay Khandelwal, Katayoun Zand, Kathy Matosich, Kaushik Veeraraghavan, Kelly Michelena, Keqian Li, Kiran Jagadeesh, Kun Huang, Kunal Chawla, Kyle Huang, Lailin Chen, Lakshya Garg, Lavender A, Leandro Silva, Lee Bell, Lei Zhang, Liangpeng Guo, Licheng Yu, Liron Moshkovich, Luca Wehrstedt, Madian Khabsa, Manav Avalani, Manish Bhatt, Martynas Mankus, Matan Hasson, Matthew Lennie, Matthias Reso, Maxim Groshev, Maxim Naumov, Maya Lathi, Meghan Keneally, Miao Liu, Michael L. Seltzer, Michal Valko, Michelle Restrepo, Mihir Patel, Mik Vyatskov, Mikayel Samvelyan, Mike Clark, Mike Macey, Mike Wang, Miquel Jubert Hermoso, Mo Metanat, Mohammad Rastegari, Munish Bansal, Nandhini Santhanam, Natascha Parks, Natasha White, Navyata Bawa, Nayan Singhal, Nick Egebo, Nicolas Usunier, Nikhil Mehta, Nikolay Pavlovich Laptev, Ning Dong, Norman Cheng, Oleg Chernoguz, Olivia Hart, Omkar Salpekar, Ozlem Kalinli, Parkin Kent, Parth Parekh, Paul Saab, Pavan Balaji, Pedro Rittner, Philip Bontrager, Pierre Roux, Piotr Dollar, Polina Zvyagina, Prashant Ratanchandani, Pritish Yuvraj, Qian Liang, Rachad Alao, Rachel Rodriguez, Rafi Ayub, Raghotham Murthy, Raghu Nayani, Rahul Mitra, Rangaprabhu Parthasarathy, Raymond Li, Rebekkah Hogan, Robin Battey, Rocky Wang, Russ Howes, Ruty Rinott, Sachin Mehta, Sachin Siby, Sai Jayesh Bondu, Samyak Datta, Sara Chugh, Sara Hunt, Sargun Dhillon, Sasha Sidorov, Satadru Pan, Saurabh Mahajan, Saurabh Verma, SeÄ³i Yamamoto, Sharadh Ramaswamy, Shaun Lindsay, Shaun Lindsay, Sheng Feng, Shenghao Lin, Shengxin Cindy Zha, Shishir Patil, Shiva Shankar, Shuqiang Zhang, Shuqiang Zhang, Sinong Wang, Sneha Agarwal, Soji Sajuyigbe, Soumith Chintala, Stephanie Max, Stephen Chen, Steve Kehoe, Steve Satterfield, Sudarshan Govindaprasad, Sumit Gupta, Summer Deng, Sungmin Cho, Sunny Virk, Suraj Subramanian, Sy Choudhury, Sydney Goldman, Tal Remez, Tamar Glaser, Tamara Best, Thilo Koehler, Thomas Robinson, Tianhe Li, Tianjun Zhang, Tim Matthews, Timothy Chou, Tzook Shaked, Varun Vontimitta, Victoria Ajayi, Victoria Montanez, VÄ³ai Mohan, Vinay Satish Kumar, Vishal Mangla, Vlad Ionescu, Vlad Poenaru, Vlad Tiberiu Mihailescu, Vladimir Ivanov, Wei Li, Wenchen Wang, Wenwen Jiang, Wes Bouaziz, Will Constable, Xiaocheng Tang, Xiaojian Wu, Xiaolan Wang, Xilun Wu, Xinbo Gao, Yaniv Kleinman, Yanjun Chen, Ye Hu, Ye Jia, Ye Qi, Yenda Li, Yilin Zhang, Ying Zhang, Yossi Adi, Youngjin Nam, Yu, Wang, Yu Zhao, Yuchen Hao, Yundi Qian, Yunlu Li, Yuzi He, Zach Rait, Zachary DeVito, Zef Rosnbrick, Zhaoduo Wen, Zhenyu Yang, Zhiwei Zhao, and Zhiyu Ma. The llama 3 herd of models, 2024. 3, [57] Sepp Hochreiter and JÃ¼rgen Schmidhuber. Long short-term memory. Neural Comput., 9(8):17351780, 1997. 3 [58] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in neural network, 2015. 3 [59] Chuanguang Yang, Zhulin An, Libo Huang, Junyu Bi, Xinqiang Yu, Han Yang, Boyu Diao, and Yongjun Xu. CLIP-KD: An empirical study of CLIP model distillation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1595215962, 2024. 3 [60] Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. DistilBERT, distilled version of BERT: smaller, faster, cheaper and lighter, 2020. 3 [61] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, TimothÃ©e Lacroix, Baptiste RoziÃ¨re, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. LLaMA: Open and efficient foundation language models, 2023. 3 [62] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 1197511986, 2023. 3 [63] Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, Ji Ma, Jiaqi Wang, Xiaoyi Dong, Hang Yan, Hewei Guo, Conghui He, Botian Shi, Zhenjiang Jin, Chao Xu, Bin 12 Wang, Xingjian Wei, Wei Li, Wenjian Zhang, Bo Zhang, Pinlong Cai, Licheng Wen, Xiangchao Yan, Min Dou, Lewei Lu, Xizhou Zhu, Tong Lu, Dahua Lin, Yu Qiao, Jifeng Dai, and Wenhai Wang. How far are we to GPT-4V? closing the gap to commercial multimodal models with open-source suites, 2024. 3 [64] Lucas Beyer, Andreas Steiner, AndrÃ© Susano Pinto, Alexander Kolesnikov, Xiao Wang, Daniel Salz, Maxim Neumann, Ibrahim Alabdulmohsin, Michael Tschannen, Emanuele Bugliarello, Thomas Unterthiner, Daniel Keysers, Skanda Koppula, Fangyu Liu, Adam Grycner, Alexey Gritsenko, Neil Houlsby, Manoj Kumar, Keran Rong, Julian Eisenschlos, Rishabh Kabra, Matthias Bauer, Matko BoÅ¡njak, Xi Chen, Matthias Minderer, Paul Voigtlaender, Ioana Bica, Ivana Balazevic, Joan Puigcerver, Pinelopi Papalampidi, Olivier Henaff, Xi Xiong, Radu Soricut, Jeremiah Harmsen, and Xiaohua Zhai. PaliGemma: versatile 3B VLM for transfer, 2024. 3 [65] Fuzhen Zhuang, Zhiyuan Qi, Keyu Duan, Dongbo Xi, Yongchun Zhu, Hengshu Zhu, Hui Xiong, and Qing He. comprehensive survey on transfer learning, 2020. 4 [66] Stephanie Fu, Netanel Yakir Tamir, Shobhita Sundaram, Lucy Chai, Richard Zhang, Tali Dekel, and Phillip Isola. DreamSim: Learning new dimensions of human visual similarity using synthetic data. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. 4, 16 [67] Shobhita Sundaram, Stephanie Fu, Lukas Muttenthaler, Netanel Yakir Tamir, Lucy Chai, Simon Kornblith, Trevor Darrell, and Phillip Isola. When does perceptual alignment benefit vision representations? In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. 4, 16 [68] MikoÅaj BiÅkowski, Dougal J. Sutherland, Michael Arbel, and Arthur Gretton. Demystifying MMD GANs. In International Conference on Learning Representations, 2018. 4 [69] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. CLIPScore: reference-free evaluation In Proceedings of the 2021 metric for image captioning. Conference on Empirical Methods in Natural Language Processing, pages 75147528, Online and Punta Cana, Dominican Republic, 2021. Association for Computational Linguistics. 4 [70] Aryaz Eghbali and Michael Pradel. CrystalBLEU: Precisely In Proand efficiently measuring the similarity of code. ceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering, New York, NY, USA, 2023. Association for Computing Machinery. [71] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311 318, Philadelphia, Pennsylvania, USA, 2002. Association for Computational Linguistics. 4 [72] Peter Stanchev, Weiyue Wang, and Hermann Ney. EED: Extended edit distance measure for machine translation. In Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1), pages 514520, Florence, Italy, 2019. Association for Computational Linguistics. 5 [73] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A. Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, Demis Hassabis, Claudia Clopath, Dharshan Kumaran, and Raia Hadsell. Overcoming catastrophic forgetting in neural networks. Proceedings of the National Academy of Sciences, 114(13):35213526, 2017. 5 [74] Binyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang, Bowen Yu, Keming Lu, Kai Dang, Yang Fan, Yichang Zhang, An Yang, Rui Men, Fei Huang, Bo Zheng, Yibo Miao, Shanghaoran Quan, Yunlong Feng, Xingzhang Ren, Xuancheng Ren, Jingren Zhou, and Junyang Lin. Qwen2.5-Coder technical report, 2024. 6 [75] Michael Saxon, Fatima Jahara, Mahsa Khoshnoodi, Yujie Lu, Aditya Sharma, and William Yang Wang. Who evaluates the evaluations? objectively scoring text-to-image prompt coherence metrics with t2IScorescore (TS2). In The Thirtyeighth Annual Conference on Neural Information Processing Systems, 2024. 6 [76] Jordan J. Louviere, Terry N. Flynn, and A. A. J. Marley. Best Worst Scaling: Theory, Methods and Applications. Cambridge University Press, 2015. 6 [77] Svetlana Kiritchenko and Saif M. Mohammad. Capturing reliable fine-grained sentiment associations by crowdsourcing and bestworst scaling. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 811817, San Diego, California, 2016. Association for Computational Linguistics. [78] Svetlana Kiritchenko and Saif Mohammad. Bestworst scaling more reliable than rating scales: case study on sentiment In Proceedings of the 55th Annual intensity annotation. Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 465470, Vancouver, Canada, 2017. Association for Computational Linguistics. 6, 7 [79] Bryan K. Orme. MaxDiff analysis: Simple counting, individual-level logit, and HB. Sawtooth Software Research Paper Series, 2009. 6 [80] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with CLIP latents, 2022. 7 [81] Bruce Schneier. Applied cryptography - protocols, algorithms, and source code in C, 2nd Edition. Wiley, 1996. 7 [82] Kyle Lo, Lucy Lu Wang, Mark Neumann, Rodney Kinney, and Daniel Weld. S2ORC: The semantic scholar open research corpus. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 49694983, Online, 2020. Association for Computational Linguistics. 8 [83] Thao Nguyen, Samir Yitzhak Gadre, Gabriel Ilharco, Sewoong Oh, and Ludwig Schmidt. Improving multimodal datasets with image captioning. In Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2023. 8 [84] Anas Awadalla, Le Xue, Manli Shu, An Yan, Jun Wang, Senthil Purushwalkam, Sheng Shen, Hannah Lee, Oscar Lo, Jae Sung Park, Etash Guha, Silvio Savarese, Ludwig Schmidt, Yejin Choi, Caiming Xiong, and Ran Xu. BLIP3-KALE: Knowledge augmented large-scale dense captions, 2024. 13 [95] Y. Rubner, C. Tomasi, and L.J. Guibas. metric for disIn Sixth tributions with applications to image databases. International Conference on Computer Vision (IEEE Cat. No.98CH36271), pages 5966, 1998. 15 [96] Matt Kusner, Yu Sun, Nicholas Kolkin, and Kilian Weinberger. From word embeddings to document distances. In Proceedings of the 32nd International Conference on Machine Learning, pages 957966, Lille, France, 2015. PMLR. 15 [85] Xianhang Li, Haoqin Tu, Mude Hui, Zeyu Wang, Bingchen Zhao, Junfei Xiao, Sucheng Ren, Jieru Mei, Qing Liu, Huangjie Zheng, Yuyin Zhou, and Cihang Xie. What if we recaption billions of web images with LLaMA-3?, 2024. 8 [86] R. Hadsell, S. Chopra, and Y. LeCun. Dimensionality reduction by learning an invariant mapping. In 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR06), pages 17351742, 2006. 8 [87] Rohit Girdhar, Alaaeldin El-Nouby, Zhuang Liu, Mannat Singh, Kalyan Vasudev Alwala, Armand Joulin, and Ishan Misra. ImageBind: One embedding space to bind them all. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1518015190, 2023. 8 [88] Bin Zhu, Bin Lin, Munan Ning, Yang Yan, Jiaxi Cui, WANG HongFa, Yatian Pang, Wenhao Jiang, Junwu Zhang, Zongwei Li, Cai Wan Zhang, Zhifeng Li, Wei Liu, and Li Yuan. LanguageBind: Extending video-language pretraining to nIn The modality by language-based semantic alignment. Twelfth International Conference on Learning Representations, 2024. 8 [89] Haozhe Zhao, Xiaojian Ma, Liang Chen, Shuzheng Si, Rujie Wu, Kaikai An, Peiyu Yu, Minjia Zhang, Qing Li, and Baobao Chang. UltraEdit: Instruction-based fine-grained image editing at scale. In The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2024. [90] Urbano Lorenzo-Seva and Jos M. F. ten Berge. Tuckers congruence coefficient as meaningful index of factor similarity. Methodology: European Journal of Research Methods for the Behavioral and Social Sciences, 2(2):5764, 2006. 15 [91] Jonas Belouadi and Steffen Eger. UScore: An effective approach to fully unsupervised evaluation metrics for machine translation. In Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, pages 358374, Dubrovnik, Croatia, 2023. Association for Computational Linguistics. 15 [92] Wei Zhao, Maxime Peyrard, Fei Liu, Yang Gao, Christian M. Meyer, and Steffen Eger. MoverScore: Text generation evaluating with contextualized embeddings and earth mover distance. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-Ä²CNLP), pages 563578, Hong Kong, China, 2019. Association for Computational Linguistics. [93] Wei Zhao, Goran GlavaÅ¡, Maxime Peyrard, Yang Gao, Robert West, and Steffen Eger. On the limitations of cross-lingual encoders as exposed by reference-free machine translation evaluation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 16561671, Online, 2020. Association for Computational Linguistics. [94] Yurun Song, Junchen Zhao, and Lucia Specia. SentSim: Crosslingual semantic evaluation of machine translation. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 31433156, Online, 2021. Association for Computational Linguistics. 15 14 A. Supplementary Comparison with DeTikZify Tab. 5 shows in detail how TikZeros inverse graphics model (hereafter referred to as DeTikZifyv2) compares against DeTikZifyDS (7b), previously the best performing DeTikZify model, as evaluated on the test split of DaTikZv3 . DeTikZifyv2 clearly outperforms its predecessor across all evaluated metrics. Below, we briefly outline key differences in training and inference beyond what we described in Sec. 4. For comprehensive description of the foundation on which DeTikZifyv2 builds, we refer to Belouadi et al. [2]. Training Similar to DeTikZify, DeTikZifyv2 employs dense layer as the modality connector between the vision encoder and text decoder. However, for pretraining this layer, we replace the MetaFig dataset [2] with the substantially larger ArxivCap dataset, extracting 1 million (figure, caption, OCR) triplets. During fine-tuning, we randomly substitute inputs with synthetically generated sketches to support handdrawn inputs. To generate these sketches, we fine-tune the image-editing model UltraEdit [89] on dataset of real, human-created scientific sketches [2]. The resulting model, UltraSketch, achieves congruence coefficient (CC) [90] of 0.74 with said sketches, compared to 0.72 for the previous model used with DeTikZify. Additionally, we generate synthetic sketches using traditional image transformations such as random displacement fields. While these sketches exhibit less diversity, they better preserve text rendering and achieve comparable CC of 0.75. Averaging the sketch representations from both methods increases the CC to 0.82, demonstrating their complementary nature. Inference DeTikZify implements Monte Carlo Tree Search-based inference algorithm to iteratively refine outputs. As reward signal ð, it computes the cosine similarity ðcos = cos(pool(ð), pool( ð)) between image patch embeddings ð, ð of input images and compiled outputs via learned pooling function. Since DeTikZifyv2 fully fine-tunes the vision encoder and uses its patch embeddings directly, it cannot compute pooled embeddings in the same way. As an alternative, inspired by popular machine translation metrics [9194], we experiment with computing the Earth Movers Distance (EMD) [95, 96] with image patch embeddings. Given the distance matrix ð«, where ð·ð, ð = cos(ð¥ð, ð¦ ð ), EMD is defined as follows: When correlating reward scores computed as ðcos from DeTikZify and ðEMD = EMD(ð¥ð, ð¦ ð ) from DeTikZifyv2 with human judgments from Belouadi et al. [2], we find that ðEMD enhances correlation with humans (0.456 segmentlevel and 0.911 system-level Spearmans ð), compared to ðcos (0.436 and 0.642, respectively). This demonstrates that DeTikZifyv2 not only supports the inference algorithm but improves upon DeTikZifys capabilities. B. Supplementary Inference Details To instruct general-purpose models to generate TikZ code, we employ consistent prompt across all models (GPT-4o, Qwen2.5 Coder (32B), and IDEFICS 3 (8B)) originally engineered by Zhang et al. [4]. For each figure, we replace the <caption> placeholder with the specific caption: 2 3 4 5 generate according Please scientific following figure requirements: <caption>. Your output should be in TikZ code. Do not include any text other than the TikZ code. the to C. Supplementary Experimental Results Tab. 6 presents detailed evaluation metrics scores for the low-resource training experiments discussed in Sec. 6.2. The results show consistent degradation in performance across all metrics as both the amount of training data and the number of layers decrease, trend effectively captured by the AVG scores also shown in Tab. 4. D. Annotator Demographics Our annotation team consists of thirteen experts with extensive research experience in Machine Learning, Natural Language Processing, or Computer Vision. The team includes one male faculty member, four female PhD students, four male PhD students, and four male researcher scientists from research institute. We deliberately selected expert annotators based on findings by Belouadi et al. [3], which demonstrated that crowd workers often lack the necessary research background to provide reliable annotations for scientific figures. To mitigate potential biases, each annotator received the tuples and items within the tuples in randomized order. EMD(ð, ð) = , E. Additional Examples ð¹ð, ð ð·ð, ð ð¹ð, ð (cid:205) ð ð=1 (cid:205) ð ð=1 ð (cid:205)ð ð=1 (cid:205)ð ð=1 ð ð¹ð, ð ð·ð, ð with min ð­ 0 s.t. ð, ð ð=1 ð=1 (cid:40)(cid:205) ð ð=1 (cid:205)ð ð=1 (2) Figure 5 showcases examples5 from DaTikZv3 with permissive licenses. Additionally, Tab. 7 presents randomly sampled tuples from our human evaluation with the highest ð¹ð, ð = 1 ð ð¹ð, ð = 1 ð , . 5sourced from https://github.com/PetarV- /TikZ, https:// github.com/janosh/tikz, https://tikz.net, and https://arxiv. org"
        },
        {
            "title": "Synthetic Sketches",
            "content": "Models DeTikZifyDS (7b) DeTikZifyv2 DSim KID 0.842 75.46 0.626 80.503 cBLEU 2.953 6.105 TED MTE DSim KID 0.766 56.851 0.751 54.946 84.019 93.326 67.379 74. cBLEU 1.541 3.356 TED MTE 84.401 59.589 93.858 58.32 Table 5. System-level scores 100 for DeTikZifyv2 and DeTikZifyDS (7b) on both reference figures and synthetic sketches generated with UltraSketch from the test split of DaTikZv3. Best scores are in bold, and arrows indicate metric directionality. Note that we compute DreamSim using updated models [67], whereas Belouadi et al. [3] used the original models in their work [66]."
        },
        {
            "title": "Data",
            "content": "100% 100% 100% 100% 50% 50% 50% 50% 25% 25% 25% 25% 12.5% 12.5% 12.5% 12.5% Intv. DSim KID 5.127 5.2 5.688 5.933 5.835 5.103 6.689 6.738 6.055 6.152 7.715 7.764 6.25 7.129 7.031 8.154 52.771 52.311 51.794 51.59 52.106 52.143 50.492 50.093 51.55 51.231 49.859 49.179 50.485 50.152 49.667 48.827 1 2 4 8 1 2 4 8 1 2 4 8 1 2 4 8 CLIP 9.949 9.955 8.886 9.818 8.527 9.315 8.852 7.999 9.12 8.943 7.316 6.495 7.568 6.353 6.474 5.054 cBLEU 1.607 1.484 1.429 1.371 1.454 1.393 1.459 1.379 1.472 1.43 1.41 1.434 1.509 1.275 1.221 1.11 TED MTE AVG 82.292 65.516 82.588 65.473 65.399 83.988 83.679 65.608 83.599 65.605 82.924 65.355 78.456 65.951 78.923 65.963 77.961 66.237 77.566 65.714 79.704 66.128 79.9 66.009 80.816 65.8 81.05 66.045 82.634 65.892 80.738 65.813 92.411 87.557 82.254 76.545 77.478 85.249 47.381 40.816 49.967 54.942 32.12 29.774 56.055 33.817 37.914 16.25 Table 6. System-level scores 100 TikZero (Cos) trained on varying fractions of data and intervals of cross-attention layers. Bold and underlined values denote the best and second-best scores for the whole table, respectively. Cell shading illustrates score magnitudes. Arrows indicate metric directionality. and lowest rated instances highlighted. The results show that AutomaTikZv2 (LLM) and TikZero (Cos) are more frequently selected as the worst models (four and three times, respectively), while TikZero+ and GPT-4o are more often chosen as the best models (both three times), which aligns with our findings in Sec. 5.3. Finally, Fig. 6 illustrates example programs generated by TikZero+ and AutomaTikZv2 (LLM), demonstrating how TikZero+ utilizes advanced TikZ features, whereas AutomaTikZv2 (LLM) employs only basic, simple commands. (a) diagram representing recurrent neural network consisting of several LSTM blocks, processing the input sequence simultaneously forwards and backwards (to exploit both directions of temporal dependence). Contains some rather tight manoeuvering. (b) plot comparing the distribution functions of Bose-Einstein, Boltzmann, and Fermi-Dirac statistics as function of the reduced chemical potential ð½ ( ð ð). This visualiation highlights the differences between the three types of distribution functions, which are used to describe the behavior of particles in different statistical systems. (c) Tree with aligned matrix. probability tree with an aligned matrix listing the possible outcomes, their probabilities and three columns for events described in later tasks. It uses the grahdrawing library and requires LuaLaTeX. (d) Our approach is modified version of meta-seq2seq. transformer decoder (TD) is trained to produce sequence of actions ðð , . . . , ðð ð 1 given query instruction ð¼ ð. The context are demonstrations (ð¼ð , ð´ð ) produced by our generative model. We use transformer encoder-decoder (T) to encode instructions and state ð and transformer encoder (TE) to encode actions. The transformers that process instructions (pink blocks) receive state ð as the input of the encoder. Figure 5. Representative examples from DaTikZv3 (also present in DaTikZ and DaTikZv2), with permissive licenses."
        },
        {
            "title": "Reference",
            "content": "AutomaTikZv"
        },
        {
            "title": "TikZero",
            "content": "TikZero+ GPT-4o An illustration of the reduction from densest ð-subgraph to u-rcp. On the left there is simple undirected graph ðº with single edge. The 2-reduced directed graph of ðº is on the right. Each vertex of ðº is replaced by 2 2 = 4 copies with bidirectional edge connecting any two copies of the same vertex, and an outgoing edge from each copy to the single edge-vertex ð. Domain of dependence: The closed trapezoidal region Î©ð¡ used in the proof of proposition 1, shaded with darker yellow; the domain of dependence of point (0, â) is Î©ð¡ along with the top region shaded with lighter yellow. sketch of iterating ð ( ð¥) = ð¥ 1/ð¥. Points bigger than 1 get sent to point in [0, 1] which then moves to point 1 which moves to [1, 0] which is sent to point 1 and so on. multiline diagram ð· with ð = 6 columns and ð  = 5 rows, with content ð = (5, 4, 3, 1, 0, 0) and bottom row ð(1) (ð·) = (4, 0, 1, 5, 3, 0) ðð. It has weight w(ð·) = wð¥ (ð·) wð¡ (ð·) = ð¥3 ð¥2 3 ð¥4 4 ð¥2 5 ð¥2 6 ð¡2 . Table 7. Alternating rows display randomly selected tuples from the caption and image similarity human evaluation task (cf. Sec. 5.3). The frames of highest and lowest rated instances are highlighted in green and red, respectively. 18 1 usepackage{pgfplots} 2 pgfplotsset{width=7cm,compat=1.8} 3 begin{document} 4 begin{tikzpicture} 5 6 7 9 10 begin{axis}[grid=major,view={210}{30}] addplot3+[domain=-5:5,surf] {exp(-x^2-y^2)*x}; % connect classical points addplot3+[mark=*,only marks,blue,samples at={-4.473,-1.463,1.463,4.473}] {exp(-x^2-y^2)*x}; usepackage{amsmath,amsthm,amssymb,amsfonts} usepackage{tikz} usepackage{pgfplots} usetikzlibrary{arrows.meta} begin{document} begin{tikzpicture} begin{axis}[view={60}{30}] addplot3 [surf] {3*x^2 + y^2}; end{axis} end{tikzpicture} TikZero+ end{document} TikZero+ end{axis} 11 12 end{tikzpicture} 13 end{document} 1 usepackage{amsmath} 2 usepackage{amssymb} 3 usepackage{color} 4 usepackage{tikz} 5 6 begin{document} 7 8 begin{tikzpicture}[scale=0.5] 9 draw [thick,->] (-2,0) -- (2,0); 10 draw [thick,->] (0,-2) -- (0,2); 11 draw [thick] (1.8,0) arc (0:180:1.8); 12 node [right] at (2,0) {$mathrm{Re}(omega)$}; 13 node [above] at (0,2) {$mathrm{Im}(omega)$}; 14 node [right] at (1.8,1) {$Gamma$}; 15 end{tikzpicture} 16 17 end{document} usepackage{amssymb} usepackage{amsmath} usepackage{pgfplots} pgfplotsset{compat=1.14} usepackage{tikz} usetikzlibrary{arrows} begin{document} begin{tikzpicture}[scale=3] draw[fill=green!15] (1,0) -- (2,0) -- (2,2) -- (1,2) -- (1,0); draw[thick] (1,0) -- (1,2); draw[thick] (0,1) -- (2,1); node[below] at (1,0) {$0$}; node[below] at (2,0) {$1$}; node[left] at (1,2) {$1$}; node[left] at (0,1) {$0$}; node[above] at (1.5,1.5) {$gamma$}; draw[->,thick] (0,0) -- (1,0); draw[->,thick] (0,0) -- (0,1); draw[->,thick] (0,0) -- (0.5,0.5); draw[->,thick] (0,0) -- (1.5,0.5); draw[->,thick] (0,0) -- (0.5,1.5); draw[->,thick] (0,0) -- (1.5,1.5); draw[->,thick] (0,0) -- (2,0); draw[->,thick] (0,0) -- (0,2); draw[->,thick] (0,0) -- (1,1); end{tikzpicture} Figure 6. TikZ programs generated by TikZero+ (top) and AutomaTikZv2 (LLM; bottom) corresponding to the figures shown in the first row of Fig. 1 in the same order. AutomaTikZv2 end{document} AutomaTikZv2 19 2 3 4 5 6 8 9 10 11 12 14 1 2 3 4 6 7 8 9 10 12 13 14 15 16 18 19 20 21 22 24 25 26 27"
        }
    ],
    "affiliations": [
        "National Institute of Information and Communications Technology, Japan",
        "University of Mannheim, Germany",
        "University of Technology Nuremberg, Germany"
    ]
}