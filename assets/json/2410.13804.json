{
    "paper_title": "BenTo: Benchmark Task Reduction with In-Context Transferability",
    "authors": [
        "Hongyu Zhao",
        "Ming Li",
        "Lichao Sun",
        "Tianyi Zhou"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Evaluating large language models (LLMs) is costly: it requires the generation and examination of LLM outputs on a large-scale benchmark of various tasks. This paper investigates how to efficiently reduce the tasks used to benchmark LLMs without affecting the evaluation quality. Our study reveals that task transferability and relevance provide critical information to identify the most representative subset of tasks via optimizing a facility location function. We propose a practically efficient metric for estimating the transferability between two tasks via in-context learning (ICL). By analyzing the pairwise transferability, we can reduce tasks in a modern LLM benchmark (e.g., MMLU or FLAN) to 5% while inducing only a <4% difference to the evaluation on the original benchmark. Compared to prior works, our method is training-free, gradient-free, and highly efficient requiring ICL only."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 1 2 ] . [ 3 4 0 8 3 1 . 0 1 4 2 : r BENTO: BENCHMARK TASK REDUCTION WITH IN-CONTEXT TRANSFERABILITY Hongyu Zhao1, Ming Li1, Lichao Sun2, Tianyi Zhou1 1University of Maryland, College Park 2Lehigh University {hongyuz, minglii, tianyi}@umd.edu Project: https://github.com/tianyi-lab/bento"
        },
        {
            "title": "ABSTRACT",
            "content": "Evaluating large language models (LLMs) is costly: it requires the generation and examination of LLM outputs on large-scale benchmark of various tasks. This paper investigates how to efficiently reduce the tasks used to benchmark LLMs without affecting the evaluation quality. Our study reveals that task transferability and relevance provide critical information to identify the most representative subset of tasks via optimizing facility location function. We propose practically efficient metric for estimating the transferability between two tasks via in-context learning (ICL). By analyzing the pairwise transferability, we can reduce tasks in modern LLM benchmark (e.g., MMLU or FLAN) to 5% while inducing only < 4% difference to the evaluation on the original benchmark. Compared to prior works, our method is training-free, gradient-free, and highly efficient requiring ICL only. Figure 1: LEFT: In-context Transferability (ICT) reveals the clusters of benchmark tasks. We apply spectral clustering to ICT (arcs1) between MMLU tasks (nodes), whose color denotes the cluster it belongs to. The discovered clusters are associated with explainable themes. The theme and tasks of each cluster are listed around the chord graph. Only the top-7% arcs with the highest ICT values are shown in the graph, among which intra-cluster arcs are much more than inter-cluster arcs, implying sparse topology captured by ICT. RIGHT: Evaluation accuracy of task reduction methods. Each method selects 3 out of the 57 tasks in MMLU to evaluate 9 LLMs (axes). The plot reports 1 σσ/σ in log-scale where σ and σ are the evaluation metrics on the reduced-benchmark and full-benchmark, respectively. Our method (BENTO-le) achieves 97% evaluation accuracy on average. The grey band reports the random selection baselines meanstandard variation. All baselines are defined in Section 5. Table 2 reports the result when selecting different number of tasks. 1Each arc connects source task with target task and has the same color as the source task."
        },
        {
            "title": "INTRODUCTION",
            "content": "Evaluation of large language models (LLMs) is critical to examining the versatile capability and identifying possible weaknesses/risks of LLMs before deploying them to downstream tasks in practice. However, the development of the LLM benchmark is still an open challenge (Chang et al., 2023; McIntosh et al., 2024): it is usually expensive and heavily relies on human involvement, yet it is still unclear how large the benchmark should be to deliver reliable and consistent evaluation results. In practice, to cover various different application scenarios and diverse skill sets of LLMs, current LLM benchmarks usually attempt to include sufficient test cases drawn from as many tasks as possible (Hendrycks et al., 2021a;b; Wei et al., 2021), e.g., tens to hundreds. Due to the expensive sequential decoding of autoregressive LLMs, larger benchmarks greatly increase the evaluation cost and lead to severe overhead in the development process of LLMs. The substantial costs associated with LLMs drive the need to explore the feasibility of reducing the number of tasks in LLM benchmarks without compromising their evaluative capabilities. Our study in this paper investigates the transferability (Vu et al., 2020; Jiang et al., 2022) between benchmark tasks to discern their relevance and potential overlap. Transferability indicates that skills or knowledge acquired in one task (task-i) can significantly enhance performance in another (task-j). Therefore, model demonstrating strong performance on task-i is likely to perform well on task-j, leveraging the inherent generalization capabilities of LLMs. Given an accurate estimation of the task transferability, we may reduce the tasks required in LLM benchmarking and thus optimize the evaluation efficiency. Existing transferability estimation (Nguyen et al., 2020; Bao et al., 2019; Tan et al., 2021) mainly rely on model finetuning or Fisher information, which is computationally prohibitive for LLMs considering the total number of benchmark tasks and large model size. Hence, this paper aims to design cost-efficient and training-free approach to measure the transferability between different tasks. Motivated by the current progress of in-context learning (ICL) (Brown et al., 2020; Dong et al., 2023), we propose in-context transferability (ICT) as training-free approach tailored for benchmark reduction. Specifically, when applying task-is exemplars as the context for task-js queries, it provides an effective low-cost estimation of the transferability from task-i to task-j. The resulting improvement over task-js zero-shot (non-context) performance reflects the merits of task-is knowledge of task-j. Thorough analysis is conducted towards the transferability matrix computed on all the pairs of tasks from MMLU (Hendrycks et al., 2021a;b), widely adopted LLM benchmark. In the visual representation provided by Figure 1 (LEFT), we observe sparse clustering pattern. This pattern is characterized by concentration of dense interconnections among tasks around the periphery of the circle, with noticeably fewer connections traversing the central area, indicating that the intra-cluster transferability is larger than the inter-cluster transferability. This observation leads to Laplacian Eigenmaps (LE) (Belkin & Niyogi, 2003) embedding of tasks, which is also known as the first step of spectral clustering. Table 1: Evaluation of LLMs on two benchmarks and their BENTO-reduced versions using the same prompts and random seeds2. Previously reported results3are available in Appendix G. Model MMLU(100%) MMLUBENTO(5%) BBH(100%) BBHBENTO(5%) Llama-2-13b Llama-2-7b Llama-3-8b Mistral-7b-v0.3 Phi-2 Phi-3-mini-4k StableLM-2-1.6B TinyLlama Gemma-7b 54.5 46.0 61.7 62.1 56.5 69.5 34.6 24.9 65.2 53.9 49.8 60.2 62.2 56.7 70.0 34.7 25.9 63.4 45.3 37.1 59.1 56.3 58.6 70.2 23.4 25.1 - 49.6 35.4 57.6 56.0 56.7 64.2 20.7 25.1 - 3The evaluation error on BBH is higher due to its higher variance in ICL evaluations. This is also reflected by the larger difference between our evaluation and previously reported results in Appendix G. 3They may use slightly different prompts and distinct random seeds, which are not released. 2 To effectively extract representative subset of tasks that mirrors the full scope of the original benchmark, we propose Benchmark Task reductiOn (BENTO) that formulates the task selection into facility location (FL) problem (Cornuejols et al., 1977). In BENTO, the task similarities are derived either directly from the similarity matrix computed via Laplacian Eigenmaps (LE) or are recalculated within the LE-embedded space. The FL objective was to maximize the similarity between each task in the benchmark and the closest task in the reduced subset. This objective is submodular, allowing us to employ greedy algorithm (Nemhauser et al., 1978) that efficiently achieves high-quality approximate solution. Extensive experiments are conducted to evaluate the effectiveness of BENTO-reduced benchmark by comparing the performance of several widely used LLMs on both the reduced and original benchmarks. Remarkably, as is shown in Figure 1 (RIGHT) and Table 1, the results are highly consistent, even though the reduced benchmark comprises only 5% of the original tasks. This finding underscores the efficiency of our approach. When compared to existing benchmark reduction methods, BENTO not only yields more accurate evaluation results but also significantly lowers the costs associated with transferability estimation. Furthermore, ICT offers substantial potential benefits across wide array of LLM research problems and applications, making it topic of independent interest. Our main contributions can be summarized as: In-Context Transferability (ICT): We harness in-context learning to estimate task transferability ICT does not require any and discover graph and clustering structures of benchmark tasks. finetuning and provides the first scalable transferability metric on LLMs. Benchmark Task Reduction (BENTO): We develop an efficient benchmark reduction approach, which selects representative subset of tasks according to ICT. It can reduce tasks in an LLM benchmark to only 5%, which substantially reduces LLM evaluation costs without hurting the quality."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Task Transferability. Efficient estimation of task transferability has been long-studied problem. Past works mainly use Bayesian optimization (Weiss et al., 2016) and information theory (Bao et al., 2019; Tan et al., 2021). LEEP (Nguyen et al., 2020) proposes estimating via approximately training the model by linear probing on the data of the source tasks and evaluating the target tasks, which resembles our in-context learning approach. Xia et al. (2024) leverages similarity between gradient features obtained by training with LoRA (Hu et al., 2022) as transferability measure, which inspires us to transform the performance feature into similarity matrices. Benchmark Reduction. Dataset reduction for LLM training Li et al. (2024b;a) has been heated area while the reduction for benchmarks is till under-explored. Current benchmark reduction methods can be categorized into two major approaches: selecting tasks from benchmark and selecting examples from single task. Our work falls into the first one. It may seem that the second approach is more robust at least on non-few-shot benchmarks (Perlitz et al., 2024), but we can prove that combining the two approaches always yields better result (see Section 5.2), so these two approaches parallel and we can apply them one by one. Ye et al. (2023) also goes in the first direction and analyzes Big-bench (bench authors, 2023). However, its time-consuming to collect the training data they need, as their methods require performances from different models with various parameters as guidance. Polo et al. (2024) goes in the second direction, making use of item response theory (IRT) models, but share the same drawback. On the contrary, our method only requires the performance of single model as guidance, making it very efficient in data collection. Vivek et al. (2024) is relatively efficient example-selection method, which, similar to us, also draws insight from clustering. Yet, their approach can only predict the ranking instead of the exact performance of models on the whole benchmark."
        },
        {
            "title": "3 TASK TRANSFERABILITY ANALYSIS BY IN-CONTEXT LEARNING",
            "content": "Aiming at proposing cost-efficient benchmark reduction approach, we first analyze the structure of benchmarking tasks by studying the transferability between each pair of tasks. Given source task-i and target task-j, the transferability from task-i to task-j is often measured by training model on task-i and then evaluating it on task-j. However, this approach requires training per task and thus is not computationally scalable to modern LLMs and many tasks. We propose to harness in-context learning to provide training-free estimation of the transferability between tasks. 3 (x(i) , y(i) Compute in-context transferability (ICT) embedding matrix A. To estimate the transferability from source task-i to target task-j, we randomly sample exemplars e(i) 1:L from task-i, where each e(i) ) is an input-output pair for task-i. They are combined with the instruction of task-i p(i) to constitute the context, which is used to query an LLMs answer to each question x(j) of task-j, i.e., LLM([p(i), e(i) 1:L, x(j)]). The performance of such transfer ICL reflects the transferability from task-i to task-j: If task-j shares more similar format, theme/topics, or can benefit more from the knowledge of task-i, then its more likely that the transfer ICL can improve the performance on task-j. To reduce the variance, we can repeat the transfer ICL times with different random seeds and estimate the transferability by their average. To study the structure of multi-task benchmark, we estimate matrix of task transferability for all the pairs of tasks by applying the above operation to each pair. Assuming that we have tasks in total, then we will get an transferability matrix A, where Aij is an estimation of the in-context transferability (ICT) from task-i to task-j, i.e., Aij = 1 nj nj (cid:88) k=1 (cid:16) LLM([p(i), e(i) 1:L, x(j) ]), y(j) (cid:17) , (1) where nj is the total number of input-output pairs we sampled from task-j. s(, ) is an evaluation metric such as an exact match or similarity score. larger s(, ) indicates better transfer-ICLs performance. To further reduce variance, we resample the nj questions multiple times using different random seeds and average the achieved Aij. Since target tasks may differ in difficulty, we normalize by zero-centering each column of A, i.e., Aij Aij 1 (cid:88) i= Aij. (2) In the normalized A, each row can be viewed as an embedding vector for the corresponding task. Spectral clustering. We investigate the graph structure among tasks by applying clustering based on the tasks feature matrix A, which can reveal whether the transferability between intracluster tasks is high (and thus they can be further reduced). Since defines the pairwise transferability on graph of tasks, we choose to use spectral clustering, widely used algorithm for graph cut problems. Given A, spectral clustering computes similarity matrix that is symmetric and non-negative, by applying Euclidean similarity kernel to A: Sij = Eij, Eij = (cid:118) (cid:117) (cid:117) (cid:116) (cid:88) (Aik Ajk)2 (3) k=1 (a) S, induced by ICL embedding A. where is constant to ensure the non-negativeness of S. We discuss the choice of in Section 5.2. With similarity matrix given, spectral clustering can be performed as shown in Figure 1, where each cluster is defined by very clear theme. For example, the red cluster contains all three tasks about history in the MMLU benchmark. The clustering result aligns well with human intuitions, which demonstrates the effectiveness of ICT on capturing the inter-task transferability and redundancy between benchmarking tasks. Note that there are few tasks with counter-intuitive clustering assignments, e.g., professional law is in the biology cluster. This may indicate that our ICT representation captures information inherent to the task structures, which cannot be inferred solely from their names. We will come back to this in Section 5. (b) S, induced by LE embedding A. Figure 2: Similarity matrices. The arcs in the chord graph indicate which tasks are more frequently good source tasks. As weve mentioned in Section 1, this clustering has an interesting structure: The arcs within each cluster are more than those between clusters. This might seem trivial at first by definition of spectral clustering, but note that the arcs come from the ICT feature matrix A, which is not the feature matrix that we directly perform clustering algorithms on. 4 LE embedding. Lets take closer look at the process of spectral clustering. We view the similarity matrix as the adjacency matrix of complete graph. We first compute = Eigenvector1:K(L), = 1 2 SD 2 , = Diag(S1). (4) where is hyperparameter. We then perform the K-Means clustering to the rows of A. As expected, Figure 2 shows that the similarity matrix induced by (Figure 2b) indeed has the same clustering structure as but exhibits stronger block-diagonal pattern than induced by (Figure 2a). This inspires us to view as an alternative task embedding of A: It only preserves and strengthens neighborhood information of A, thus less noisy compared to the original features (Belkin & Niyogi, 2003). The process to transform into is called Laplacian Eigenmaps (LE), so we call LE embedding in this paper. The clustering results motivate us to select tasks that are more important or representative: these tasks exhibit higher transferability to more tasks than others, and each of them may transfer to different subset of tasks. If we can select subset of representative tasks that can cover most of the tasks in benchmark, then we can use the performance on this subset to approximate the performance on the whole benchmark! That is the benchmark-task reduction problem well discuss in the next section."
        },
        {
            "title": "4 BENCHMARK-TASK REDUCTION (BENTO) BY FACILITY LOCATION",
            "content": "Benchmark-task reduction (BENTO) aims to select fixed-size subset of tasks, such that the performance of any model on this subset serves as proxy of the models performance on the whole benchmark. Intuitively, we want to choose the most representative subsets of tasks. In the context of transferability, the representativity of subset of tasks can be measured by their transferability to other tasks in the benchmark. If subset can transfer to the whole benchmark well, then the performance on this subset will be highly correlated to the performance on the whole benchmark. Under the assumption that the task difficulty in benchmark is on similar levels, the model performance on this subset can directly serve as prediction of the performance on the whole benchmark, and this holds for any model. How do we choose such subset? For every target task in the benchmark, if we can always find source task in our subset with sufficiently high transferability to the target, then the subset can transfer to all the tasks in the benchmark. On the other hand, if there exist two or more tasks with high transferability to the same group of tasks, then retaining only one of them suffices to keep the transferability of the representative subset. Inspired by these intuitions, we aim to find subset such that the similarity between each task in the benchmark and its nearest task in the subset is maximized. This formally reduces to optimizing the facility location (FL) function, i.e., arg max X2N ,Xk (X) (cid:88) i=1 max jX Sij. (5) larger (X) corresponds to more representative subset of tasks X. Since this function is submodular, the optimization problem can be solved explicitly and efficiently with greedy algorithm. As an alternate to in Equation (3) (which might be affected by the long-range noises in A), we derive cosine similarity matrix from the LE embedding A: i,j = i, j A j . (6) As shown in Figure 2, shares lot of common properties with S, so we propose variant that replaces in Equation (5) with S. The LE embedding removes some long-range noise and is expected to be more robust in realistic scenarios. We call the original version using BENTO-sim and the LE variant using BENTO-le, where BENTO is an abbreviation of Benchmark Task reductiOn. detailed algorithm is presented in Appendix A. To summarize, our pipeline is: first compute transferability matrix via in-context learning, then compute similarity matrix based on the transferability matrix, and maximize the facility location function defined by the similarity matrix to select subset of representative tasks."
        },
        {
            "title": "5 EXPERIMENT",
            "content": "Benchmarks. We assess our method mainly on two benchmarks: MMLU (Hendrycks et al., 2021a;b) and FLAN (Wei et al., 2021). Additional results on AGIEval English (Zhong et al., 2023) and Big-Bench Hard (Suzgun et al., 2022) can be found in Appendix F. MMLU is question-answering dataset containing 57 tasks with diverse subjects and levels of difficulty, mainly focusing on the knowledge of the language models. All the questions in MMLU are multiple-choice questions with 4 options for each answer. On MMLU, we use accuracy (ACC) as the evaluation metric s(, ) in Equation (1). FLAN is dataset with more diverse forms of tasks, including free-form generation tasks like translation and summarization. Since FLAN is large dataset, we sampled 100 questions from each of its 66 tasks as our whole benchmark. On FLAN, we use response perplexity as the evaluation metric s(, ). We follow widely used prompts for these benchmarks without any re-engineering, more details in Appendix D. Evaluation Metric. In both datasets, we apply all methods to select representative tasks with from 1 to (We pick to be approximately 18% of the tasks, i.e., 10 on MMLU and 12 on FLAN). For each value of k, we calculate the root mean square error (RMSE) of the predicted performance (i.e. performance on the selected tasks) across all the models. To ensure comparability, the RMSE is normalized by the root mean square (RMS) of the ground truth performance, yielding the following normalized RMSE: (cid:118) (cid:117) (cid:117) (cid:116) NRMSE = (cid:80)T ) t=1(σt σ (cid:80)T t=1 σ , (7) where denotes the total number of evaluated models, σt and σ denote the evaluation metrics of model on the reduced-benchmark and the original full-benchmark, respectively. The NRMSE measures the relative error (error rate) of the reduced benchmark in terms of L2 error. For more fine-grained evaluation of each model, please refer to Table 1 and Appendix E. Models. ICL is performed on Llama-2-13B (Touvron et al., 2023) and Llama-2-7B to estimate ICT for MMLU tasks and FLAN tasks, respectively. Since the goal is to find reduced benchmark that can replace the original benchmark to evaluate any models, we compare the benchmark evaluation results on nine popular LLMs, including Llama-2-13B, Llama-2-7B, Gemma-7B (Team et al., 2024), Phi-2 (Javaheripi et al., 2023), Phi-3 (Abdin et al., 2024), StableLM-2-1.6B (Bellagente et al., 2024), Mistral-7b-v0.3 (Jiang et al., 2023) and TinyLlama (Zhang et al., 2024). Baselines. We compare BENTO with the following baselines: random: simple baseline involves randomly selecting tasks. To reduce variance, we sample 1000 sets of random tasks for each and compute the average NRMSE across these samples. GPT4 (OpenAI, 2023): We prompt GPT4 to suggest representative tasks and rank them based solely on the names of the tasks. Given that our evaluation is based on well-established benchmarks, GPT-4 likely has prior knowledge of these tasks and may have encountered them during training. BM25-sim: BM25 (Robertson et al., 2009) is classic measure of text similarity. Here, we calculate the BM25 score between each tasks corpses (including instructions, solutions, etc.) and use it to replace the ICL transferability matrix A. The remaining steps are the same as BENTO-sim. BM25-le: variant of BM25-sim, which uses for the FL problem, just as in BENTO-le. 5.1 MAIN RESULTS Results on the MMLU benchmark are presented in Table 2. As shown, the best of our two methods consistently outperforms other approaches. Notably, Our method can achieve an error rate of 3% with only 3 tasks out of 57, which constitutes approximately 5% of the total number of tasks (5.8% of test samples), significantly surpassing the baseline methods. This demonstrates that the information embedded in the ICL transferability matrix can be effectively utilized for benchmark reduction. deeper examination of the performance of the random and GPT-4 baselines reveals some intriguing patterns. Expectedly, the NRMSE of the random baseline always decreases as increases. In contrast, while the GPT-4 baseline exhibits general downward trend in NRMSE as increases, an anomalous spike occurs at = 7. Upon closer inspection, GPT-4 selects the task professional 6 Table 2: NRMSE on MMLU (lower the better) when selecting tasks for evaluation. Each number is averaged over 9 different models. The standard deviation can be found in Appendix C. Method Best k= k=2 k=3 k=4 k=5 k=6 k= k=8 k=9 random GPT4 BM25-le BM25-sim 0.091 0.034 0.074 0.093 0.029 BENTO-le BENTO-sim 0.026 0.228 0.213 0.074 0. 0.051 0.123 0.163 0.078 0.218 0.188 0.098 0.061 0.144 0.093 0.217 0.177 0.029 0.177 0.130 0.077 0.216 0. 0.049 0.119 0.118 0.056 0.205 0.101 0.045 0.101 0.110 0.034 0.193 0.093 0.031 0.039 0.104 0.160 0.199 0. 0.060 0.079 0.100 0.180 0.213 0.137 0.168 0.039 0.095 0.168 0.209 0.134 0.147 0.029 k= 0.091 0.135 0.176 0.117 0.113 0.026 law as the seventh task, justifying its choice with the reasoning that this task is relevant for understanding societal structures. This decision seems intuitively sound from human perspective, as professional law often deals with governance and social order. However, our clustering result in Figure 1 suggests otherwise: the task professional law is actually placed in the biology cluster, revealing an unexpected underlying connection. This discrepancy highlights key advantage of our approach. The fact that our methods consistently outperform the GPT-4 baseline indicates that our task representations capture more nuanced and accurate relationships between tasks beyond what their names or surface-level associations suggest. Table 3: NRMSE on FLAN (lower the better). is the number of selected tasks. Each number is averaged over 6 different models. The standard deviation can be found in Appendix C. Method random GPT4 BM25-le BM25-sim Best 0.49 0.09 0.51 0.24 BENTO-le 0.07 BENTO-sim 0.04 k=1 1.27 7.48 0.99 7.93 0.55 0. k=2 1.06 3.29 0.76 4.36 0.47 0.76 k=3 0.92 1.86 0.61 2.57 0.22 0. k=4 0.83 1.36 0.57 1.69 0.07 0.10 k=5 0.77 0.89 0.58 1.30 1.44 0. k=6 0.70 0.58 0.65 0.93 1.03 0.30 k=7 0.64 0.61 0.51 0.65 0.86 0. k=8 0.60 0.41 1.61 0.68 0.63 0.25 k=9 0.56 0.26 1.33 0.49 0.58 0. k=10 k=11 k=12 0.53 0.13 1.10 0.36 0.50 0.35 0.51 0.09 1.36 0. 0.40 0.18 0.49 0.44 1.36 0.24 0.29 0.21 Our main results on FLAN are shown in Table 3. On FLAN, our method achieves an error rate of 4% using approximately 5% of the total number of tasks. Note that here the GPT4 baseline performs relatively better compared to its performance on MMLU. This improvement is primarily because the task names in FLAN are more informative they are names of well-known, established datasets such as SST-2. GPT-4 has likely encountered these tasks during its training, enabling it to infer the content of each task without needing to analyze individual examples. Despite this, our methods still outperform it for most values of k. Our methods consistent performance across different benchmarks indicates its potential applicability to wide range of tasks and datasets. The results of the similarity-based methods on both the MMLU and FLAN datasets exhibit consistent pattern: BENTO-le performs well when the value of is sufficiently small, whereas BENTO-sim demonstrates better performance for larger values of k. This pattern also applies to the BM25 baseline; initially, BM25-sim underperforms compared to BM25-le but surpasses it as increases. This trend is clearly illustrated in Figure 3, where we plot the difference () of performance of sim methods and le methods. The underlying reason for this behavior lies in the properties of Laplacian Eigenmaps. As previously discussed, Laplacian Eigenmaps are designed to preserve local neighborhood relationships while discarding long-range information. This characteristic makes them highly effective when selecting small number of tasks, where local similarities are paramount. However, as the number of selected tasks increases, the importance of capturing global structure and long-range relationships becomes more significant. Consequently, methods that consider long-range similarities become more effective for larger values. 5.2 ABLATION STUDY Choice of c. When calculating the similarity matrix from the ICL transferability using Equation (3), we need to choose hyperparameter c. Note that by definition does not influence the results 7 (a) on MMLU. (b) on FLAN. Figure 3: Difference () in NRMSE between (sim) and (le) when used to select different numbers of tasks (x-axis). Larger indicates the le variant produces better reduced benchmark than sim. For both BENTO and BM25, le is better ( 0) for smaller while sim is better ( 0) for larger k. Table 4: Ablation study of similarity metrics: we compare the best NRMSE on different datasets achieved by different metrics: cos cosine similarity, and cheby Chebyshev similarity. Method MMLU AGIEval Eng Big Bench Hard cheby-le cheby-sim cos-le cos-sim BENTO-le BENTO-sim 0.05 0. 0.05 0.03 0.03 0.03 0.07 0.06 0.03 0.09 0.03 0.03 0.09 0. 0.20 0.10 0.05 0.15 of BENTO-sim; however, it does impact the performance of BENTO-le. In our main experiments, we set = 1.5 maxi,j(Eij), ensuring that 0.5 maxi,j(Eij) > 0. This specific choice was empirically validated on the MMLU dataset, where it produced reasonable clustering results. But how optimal is this choice? Could other values of yield better performance? Would this selection generalize effectively to FLAN? To address these questions, we parameterized as = maxi,j(Eij), where is sampled from uniform distribution (1, 50), and generated 1000 random values of t. We then evaluated the performance of BENTO-le under these different values of c. On MMLU, out of the 1000 samples, 191 led to better average performance over compared to our original choice, while 340 achieved better performance on the best k. In contrast, on the FLAN dataset, 506 samples improved the average performance, and 872 samples enhanced the best performance compared to our initial selection of c. These findings suggest that while our choice of is reasonably effective on MMLU, it is less optimal on FLAN. This variability in performance across datasets indicates that more adaptive approach to selecting could be beneficial. In future work, we could explore dynamic strategies for setting c, potentially based on specific dataset characteristics or performance metrics. This approach could lead to more consistent improvements across different benchmarks. Choice of similarity metrics. In our study, we opted for Euclidean similarity when computing the similarity matrix S. An important question arises: how would other similarity metrics, such as cosine similarity or Chebyshev similarity, affect the results? As shown in Table 4, while other metrics like cosine and Chebyshev similarity produced results that were slightly worse than Euclidean similarity, the performance gap was not large, especially between cosine similarity and Euclidean similarity. This suggests that Euclidean similarity may offer slight advantage on the specific benchmarks we evaluate on, but other similarity measures could still be viable alternatives on different benchmarks. Task selection and example selection. While one might argue that selecting representative examples within each task could yield better results while keeping inference costs low, it is important to note 8 that our method can be combined with existing example-selection techniques to further enhance the reduction rate. To evaluate this, we compare two approaches: randomly selecting examples from each task with and without incorporating BENTO. The results, reported in Table 5, demonstrates that BENTO can further reduce the NRMSE of the 5.0% example-selection baseline (Random) by reducing the examples to 2.0% using task selection. When the number of examples per task becomes extremely limited, continuing to reduce examples per task leads to substantial increase in NRMSE, suggesting that task selection offers more robust alternative in such scenarios. Therefore, task selection and example selection are complementary strategies that can be effectively combined to achieve higher reduction rates. Table 5: Example selection with and without BENTO (-sim) on MMLU. Random refers to random selection of examples. Random+BENTO applies Random at first to reduce the examples per task to 5% and then selects subset of tasks by BENTO. It shows that BENTO can further improve example selection and outperforms example selection only. For example, Random+BENTO with 2.0% remaining data achieves lower NRMSE than Random with 5.0% remaining data; Random+BENTO with 0.7% remaining data achieves the same NRMSE as Random with 2.0% remaining data. Selection strategy # Selected Tasks Remaining Examples (%) NRMSE Random Random Random + BENTO Random Random + BENTO 57 57 57 21 5.0% 0.7% 0.7% 2.0% 2.0% 0.029 0.109 0. 0.051 0.026 Facility location v.s. K-medoids clustering. We select FL for its efficiency and precise formation of the task reduction problem. To evaluate this choice, we compare FL with more computationally intensive methods like K-medoid clustering, which can be viewed as K-means where real data points serve as centroids. As illustrated in Figure 4, the choice of ICL embedding plays far more critical role in determining performance than the choice of clustering algorithm. When using the same embedding, both FL and K-medoids produce comparable results; however, FL offers clear advantage in computational efficiency. This makes FL the more practical option for large-scale scenarios without sacrificing performance."
        },
        {
            "title": "6 CONCLUSION",
            "content": "Figure 4: Ablation study on facility location (FL) vs. K-medoids: we report the best NRMSE (lower is better) achieved by each method on MMLU. KM denotes K-medoids. KM-raw, KM-sim and KM-le denote K-medoids on the raw feature matrix A, similarity matrix and respectively. In this study, we demonstrated that large language model (LLM) evaluations can be efficiently conducted with significantly reduced benchmarks, without substantially compromising evaluative accuracy. Utilizing in-context learning to estimate task transferability, our method allows for reduction of up to 95% in the number of tasks, maintaining less than 4% deviation from full benchmark results. This approach not only reduces computational and operational costs but also presents scalable model for rapid LLM assessment. Future work may explore expanding this methodology across different model types and broader task sets to enhance its robustness and applicability in real-world scenarios."
        },
        {
            "title": "7 LIMITATIONS",
            "content": "In this paper, we have focused on achieving cost-efficient benchmark reduction for evaluating large language models (LLMs), which we have demonstrated to be effective through extensive experimentation. However, notable limitation of this approach is that smaller benchmark may 9 inherently be less diverse and potentially more vulnerable to adversarial attacks. We recognize that this limitation represents fundamental trade-off between the efficiency of the evaluation process and the comprehensiveness of the metrics employed."
        },
        {
            "title": "REFERENCES",
            "content": "Marah Abdin, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harkirat Behl, et al. Phi-3 technical report: highly capable language model locally on your phone. arXiv preprint, 2024. URL https://arxiv.org/abs/2404.14219. Yajie Bao, Yang Li, Shao-Lun Huang, Lin Zhang, Lizhong Zheng, Amir Zamir, and Leonidas Guibas. An information-theoretic approach to transferability in task transfer learning. In ICIP, 2019. URL https://ieeexplore.ieee.org/document/8803726. Mikhail Belkin and Partha Niyogi. Laplacian eigenmaps for dimensionality reduction and data representation. Neural computation, 2003. URL https://ieeexplore.ieee.org/ document/6789755. Marco Bellagente, Jonathan Tow, Dakota Mahan, Duy Phung, Maksym Zhuravinskyi, Reshinth Adithyan, James Baicoianu, Ben Brooks, Nathan Cooper, Ashish Datta, et al. Stable lm 2 1.6 technical report. arXiv preprint, 2024. URL https://arxiv.org/abs/2402.17834. BIG bench authors. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. TMLR, 2023. URL https://openreview.net/forum?id= uyTL5Bvosj. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, 2020. URL https://proceedings.neurips.cc/paper_files/paper/ 2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf. Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Linyi Yang, Kaijie Zhu, Hao Chen, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, Wei Ye, Yue Zhang, Yi Chang, Philip S. Yu, Qiang Yang, and Xing Xie. survey on evaluation of large language models, 2023. URL https://arxiv. org/pdf/2307.03109. Gerard Cornuejols, Marshall Fisher, and George L. Nemhauser. On the uncapacitated location problem. In Studies in Integer Programming, volume 1 of Annals of Discrete Mathematics, pp. 163177. Elsevier, 1977. Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, Lei Li, and Zhifang Sui. survey on in-context learning, 2023. URL https://arxiv.org/ pdf/2301.00234. Dan Hendrycks, Collin Burns, Steven Basart, Andrew Critch, Jerry Li, Dawn Song, and Jacob Steinhardt. Aligning ai with shared human values. ICLR, 2021a. URL https://arxiv.org/ abs/2008.02275. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. ICLR, 2021b. URL https: //arxiv.org/abs/2009.03300. Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In ICLR, 2022. URL https://openreview.net/forum?id=nZeVKeeFYf9. 10 Mojan Javaheripi, Sebastien Bubeck, et al. Phi-2: The surprising power of small language models, 2023. URL https://www. microsoft. com/en-us/research/blog/phi-2-the-surprising-power-ofsmall-language-models, 2023. URL https://www.microsoft.com/en-us/research/ blog/phi-2-the-surprising-power-of-small-language-models/. Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lelio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothee Lacroix, and William El Sayed. Mistral 7b, 2023. URL https://arxiv. org/pdf/2310.06825. Junguang Jiang, Yang Shu, Jianmin Wang, and Mingsheng Long. Transferability in deep learning: survey. arXiv preprint, 2022. URL https://arxiv.org/pdf/2201.05867. Guillaume Klein, Yoon Kim, Yuntian Deng, Vincent Nguyen, Jean Senellart, and Alexander M. Rush. Opennmt: Neural machine translation toolkit, 2018. URL https://aclanthology.org/ P17-4012/. Ming Li, Yong Zhang, Shwai He, Zhitao Li, Hongyu Zhao, Jianzong Wang, Ning Cheng, and Tianyi Zhou. Superfiltering: Weak-to-strong data filtering for fast instruction-tuning. In LunWei Ku, Andre Martins, and Vivek Srikumar (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1425514273, Bangkok, Thailand, August 2024a. Association for Computational Linguistics. URL https: //aclanthology.org/2024.acl-long.769. Ming Li, Yong Zhang, Zhitao Li, Jiuhai Chen, Lichang Chen, Ning Cheng, Jianzong Wang, Tianyi Zhou, and Jing Xiao. From quantity to quality: Boosting LLM performance with self-guided data selection for instruction tuning. In Kevin Duh, Helena Gomez, and Steven Bethard (eds.), Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pp. 7595 7628, Mexico City, Mexico, June 2024b. Association for Computational Linguistics. URL https://aclanthology.org/2024.naacl-long.421. Timothy R. McIntosh, Teo Susnjak, Tong Liu, Paul Watters, and Malka N. Halgamuge. Inadequacies of large language model benchmarks in the era of generative artificial intelligence, 2024. URL https://arxiv.org/pdf/2402.09880. G. L. Nemhauser, L. A. Wolsey, and M. L. Fisher. An analysis of approximations for maximizing submodular set functionsi. Math. Program., 14(1):265294, 1978. Cuong Nguyen, Tal Hassner, Matthias Seeger, and Cedric Archambeau. Leep: new measure to evaluate transferability of learned representations. In ICML, 2020. URL https://arxiv. org/abs/2002.12462. OpenAI. Gpt-4 technical report, 2023. URL https://arxiv.org/pdf/2303.08774. Yotam Perlitz, Elron Bandel, Ariel Gera, Ofir Arviv, Liat Ein-Dor, Eyal Shnarch, Noam Slonim, Michal Shmueli-Scheuer, and Leshem Choshen. Efficient benchmarking (of language models). NAACL, 2024. URL https://arxiv.org/abs/2308.11696. Felipe Maia Polo, Lucas Weber, Leshem Choshen, Yuekai Sun, Gongjun Xu, and Mikhail Yurochkin. tinybenchmarks: evaluating llms with fewer examples. arXiv preprint, 2024. URL https: //arxiv.org/abs/2402.14992. Stephen Robertson, Hugo Zaragoza, et al. The probabilistic relevance framework: Bm25 and beyond. Foundations and Trends in Information Retrieval, 2009. URL https://www. nowpublishers.com/article/Details/INR-019. Mirac Suzgun, Nathan Scales, Nathanael Scharli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc Le, Ed Chi, Denny Zhou, et al. Challenging big-bench tasks and whether chain-of-thought can solve them. arXiv preprint, 2022. URL https://arxiv. org/abs/2210.09261. Yang Tan, Yang Li, and Shao-Lun Huang. Otce: transferability metric for cross-domain cross-task representations. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2021. URL https://arxiv.org/abs/2103.13843. Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivi`ere, Mihir Sanjay Kale, Juliette Love, et al. Gemma: Open models based on gemini research and technology. arXiv preprint, 2024. URL https://arxiv.org/ abs/2403.08295. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint, 2023. URL https://arxiv.org/abs/2307. 09288. Rajan Vivek, Kawin Ethayarajh, Diyi Yang, and Douwe Kiela. Anchor points: Benchmarking models with much fewer examples. EACL, 2024. URL https://arxiv.org/abs/2309.08638. Tu Vu, Tong Wang, Tsendsuren Munkhdalai, Alessandro Sordoni, Adam Trischler, Andrew MattarellaMicke, Subhransu Maji, and Mohit Iyyer. Exploring and predicting transferability across nlp tasks. arXiv preprint, 2020. URL https://arxiv.org/pdf/2005.00770. Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew Dai, and Quoc Le. Finetuned language models are zero-shot learners. In ICLR, 2021. URL https://arxiv.org/abs/2109.01652. Karl Weiss, Taghi Khoshgoftaar, and DingDing Wang. survey of transfer learning. Journal of Big data, 2016. URL https://link.springer.com/article/10.1186/ S40537-016-0043-6. Mengzhou Xia, Sadhika Malladi, Suchin Gururangan, Sanjeev Arora, and Danqi Chen. Less: Selecting influential data for targeted instruction tuning. arXiv preprint, 2024. URL https: //arxiv.org/abs/2402.04333. Qinyuan Ye, Harvey Yiyun Fu, Xiang Ren, and Robin Jia. How predictable are large language model capabilities? case study on big-bench. EMNLP Findings, 2023. URL https://arxiv.org/ abs/2305.14947. Peiyuan Zhang, Guangtao Zeng, Tianduo Wang, and Wei Lu. Tinyllama: An open-source small language model. arXiv preprint, 2024. URL https://arxiv.org/abs/2401.02385. Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied, Weizhu Chen, and Nan Duan. Agieval: human-centric benchmark for evaluating foundation models. arXiv preprint arXiv:2304.06364, 2023. URL https://arxiv.org/abs/2304.06364."
        },
        {
            "title": "A ALGORITHM",
            "content": "Our method BENTO is described in Algorithm 1. Algorithm 1 Benchmark Task Reduction (BENTO) = length(Task), p(i) =instruction of Task[i] for i, = 1 do 1: procedure TRANSFERABILITYMATRIX(Task, Model, L, ) and are hyperparameters 2: 3: 4: 5: 6: Estimate ICT from Task[i] to Task[j] random seeds for = 1 do 1:L from source Task[i] Set random seed to Sample exemplars e(i) Sample nj input-output pairs e(j) Estimate ICT Ai,j using Equation (1) 1:nj from target Task[j] end for Average Ai,j over the random seeds end for Normalize the columns of using Equation (2) return 7: 8: 9: 10: 11: 12: 13: 14: end procedure 15: procedure SIMILARITYMATRIX(A, K) Compute the similarity matrix using Equation (3) 16: Compute the Laplacian embedding using Equation (4) 17: Compute the cosine similarity matrix from using Equation (6) 18: return S, 19: 20: end procedure 21: procedure BENCHMARKTASKREDUCTION(S) Maximize Equation (5) by the greedy algorithm 22: Return 23: 24: end procedure Transferability matrix A, hyperparameter can be replaced by S"
        },
        {
            "title": "B EXPERIMENT DETAILS",
            "content": "For our main experiments, we use 4 A100 40G for about 3 days. We use = 5 exemplars and = 10 random seeds. We set to be large value so that we always evaluate on the whole test set. For our main experiments on FLAN, when we normalize E, we also divide it by the standard deviation since the metric we use makes the original one too distorted. Our implementation is based on Klein et al. (2018)."
        },
        {
            "title": "C STANDARD DEVIATIONS OF MAIN RESULTS",
            "content": "The ICL accuracy on MMLU are averaged over 10 random seeds. Since the result is 57 57 matrix, its impossible to list the standard deviation for all entries. The average standard deviation over the 57 57 matrix is 0.017, the standard deviation of the standard deviation is 0.0065. For our main results presented in Table 2 and Table 3, we compute the error bar via bootstrapping. We randomly sample models with replacement and compute NRMSE on the sampled models. This process is repeated 1000 times and we compute the mean and standard deviation of the NRMSE. Results are shown in Table 6 and Table 7."
        },
        {
            "title": "D ICL PROMPTS EXAMPLE",
            "content": "On MMLU, we use the following prompts: 13 Table 6: Error bar of the NRMSE on MMLU. Computed by bootstrapping. kMethod GPT4 BM25-le BM25-sim BENTO-le BENTO-sim 1 2 3 4 5 6 7 8 9 10 0.1930.017 0.0730.006 0.1000.014 0.0820.012 0.0600.010 0.0410.005 0.1520.015 0.1680.015 0.1570.014 0.1260.013 0.0620.015 0.2240.025 0.2200.025 0.2170.023 0.2060.022 0.1930.021 0.1980.021 0.2090.020 0.2050.019 0.1740. 0.3490.024 0.1770.020 0.1470.017 0.1220.016 0.1250.015 0.0890.009 0.1570.014 0.1320.011 0.1290.011 0.1130.010 0.0590.010 0.0900.014 0.0310.009 0.0500.007 0.0450.006 0.0310.006 0.0580.008 0.1590.015 0.1420.013 0.1110.010 0.1300.013 0.0660.010 0.1680.017 0.1130.012 0.0970.010 0.0420.006 0.0770.007 0.0430.007 0.0330.004 0.0290.005 Table 7: Error bar of the NRMSE on FLAN. Computed by bootstrapping kMethod GPT BM25-le BM25-sim BENTO-le BENTO-sim 1 2 3 4 5 6 7 8 9 10 11 12 6.8673.020 3.0211.328 1.7120.752 1.2470.548 0.8140.358 0.5320.234 0.5590.246 0.3770.166 0.2380.105 0.1230.054 0.0870.038 0.4010. 0.9090.400 0.6940.305 0.5590.246 0.5260.232 0.5310.234 0.5940.260 0.4730.208 1.4790.650 1.2230.538 1.0090.444 1.2480.548 1.2490.549 7.2763.199 3.9991.758 2.3601.038 1.5520.682 1.1930.524 0.8500.374 0.5980.263 0.6230.274 0.4540.200 0.3290.144 0.2820.124 0.2170.096 0.5020.220 0.4270.188 0.2020.089 0.0670.030 1.3210.581 0.9480.417 0.7910.348 0.5820.255 0.5370.236 0.4550.200 0.3630.160 0.2650.117 0.8510.374 0.7020.308 0.0330.015 0.0910.041 0.1510.067 0.2720.120 0.2430.107 0.2310.102 0.3070.135 0.3230.142 0.1610.071 0.1910.084 The following are multiple choice questions (with answers) about [Task As subject].nn[Task As exemplars][Task Bs question]nAnswer: An exemplar has the format: [Question]nAnswer: [Answer]nn On FLAN, we use the following prompts: You are helpful AI assistant. Here are some example input-output pairs that you should follow.nn[Task As exemplars]Input:n[Task Bs question]nOutput: An exemplar has the format: Input:n[Question]nOutput: [Answer]nn"
        },
        {
            "title": "E DETAILED PERFORMANCE OF EACH MODEL",
            "content": "The detailed performance of each model is shown in Table 8. Our reduced benchmark works consistently well across different models."
        },
        {
            "title": "F RESULTS ON ADDITIONAL BENCHMARKS",
            "content": "AGIEval (Zhong et al., 2023) is question-answering dataset where the questions mostly come from real human exams. The questions have diverse sources and forms, but are all formatted as multiple-choice questions. We use the 9 English tasks in this dataset and use ACC as the initial transferability measure. The result is shown in Table 9. This benchmark is not ideal for our setting since the number of tasks is too small, but our method still works relatively well comparing to the baselines. Big-Bench Hard (Suzgun et al., 2022) is benchmark with 27 subtasks, including 14 Table 8: Performance of different models on all tasks and selected tasks of MMLU."
        },
        {
            "title": "Selected Tasks",
            "content": "65.20.2 Gemma-7b 54.50.2 Llama-2-13b 46.00.1 Llama-2-7b 61.70.2 Llama-3-8b 62.10.2 Mistral-7b-v0.3 56.50.3 Phi-2 Phi-3-mini-4k 69.50.1 StableLM-2-1.6B 34.60.2 24.90.4 TinyLlama 63.40.5 53.90.2 49.80.3 60.21.8 62.20.4 56.70.3 70.00.6 34.70.7 25.91.6 Table 9: NRMSE on AGIEval English (lower the better). is the number of selected tasks. Each number is averaged over 4 different models. Method Random GPT4 BM25-le BM25-sim Best 0.34 0.07 0.06 0.15 0.03 BENTO-le BENTO-sim 0.03 k=1 0.34 0.48 0.17 0.38 0.38 0.53 k= 1.07 0.32 0.24 0.24 0.08 0.23 k=3 2.08 0.20 0.06 0.28 0.07 0.20 k= 3.07 0.16 0.08 0.38 0.05 0.16 k=5 4.10 0.15 0.11 0.21 0.06 0.03 k= 5.12 0.07 0.15 0.15 0.03 0.08 filling-in-the-blank tasks that require certain natural language response. We require strict match when computing ACC on this dataset. The result is shown in Table 10. On this dataset, we only use the 3 to 5 examples in the training set as the exemplars. Under this extreme few-shot setting, BM25 seems to work slightly better than our methods. Table 10: NRMSE on Big Bench Hard (lower the better). is the number of selected tasks. Each number is averaged over 8 different models. Method Best k=1 k=2 k=3 k=4 k=5 k= k=7 k=8 Random GPT4 BM25-le BM25-sim 0.389 0.072 0.092 0.032 BENTO-le 0.045 BENTO-sim 0.154 0.389 0.540 0.092 0. 0.248 0.780 1.063 0.174 0.353 0.325 0.080 0.392 2.029 0.086 0.133 0.313 0.090 0.333 3.010 0.099 0.114 0. 0.148 0.388 4.012 0.110 0.171 0.186 0.057 0.179 5.002 0.212 0.129 0.119 0.045 0.154 6.004 0.138 0.116 0. 0.070 0.196 7.020 0.072 0.135 0.049 0.080 0."
        },
        {
            "title": "G PUBLICLY REPORTED RESULTS ON MMLU AND BBH",
            "content": "In Table 11, we annotate the publicly reported results for models listed in Table 1. We do not report our assessment of Gemma-7b on BBH because it fails to generate answer in the given format. 15 Table 11: Comparison of full benchmark performance and reduced benchmark performance. The numbers outside brackets are measured by ourselves and the numbers inside are reported by previous works. The difference may come from different prompts / quantization. Model MMLU(100%) MMLUBENTO(5%) BBH(100%) BBHBENTO(5%) Llama-2-13b Llama-2-7b Llama-3-8b Mistral-7b-v0.3 Phi-2 Phi-3-mini-4k StableLM-2-1.6B TinyLlama Gemma-7b 54.5(54.8) 46.0(45.3) 61.7(69.4) 62.1(61.1) 56.5(56.7) 69.5(70.9) 34.6(38.9) 24.9(26.6) 65.2(64.3) 53.9 49.8 60.2 62.2 56.7 70.0 34.7 25.9 63.4 45.3(39.4) 37.1(32.6) 59.1(61.1) 56.3(56.0) 58.6(59.2) 70.2(73.5) 23.4(-) 25.1(29.3) - 49.6 35.4 57.6 56.0 56.7 64.2 20.7 25.1 -"
        }
    ],
    "affiliations": [
        "Lehigh University",
        "University of Maryland, College Park"
    ]
}