{
    "paper_title": "Nemotron Elastic: Towards Efficient Many-in-One Reasoning LLMs",
    "authors": [
        "Ali Taghibakhshi",
        "Sharath Turuvekere Sreenivas",
        "Saurav Muralidharan",
        "Ruisi Cai",
        "Marcin Chochowski",
        "Ameya Sunil Mahabaleshwarkar",
        "Yoshi Suhara",
        "Oluwatobi Olabiyi",
        "Daniel Korzekwa",
        "Mostofa Patwary",
        "Mohammad Shoeybi",
        "Jan Kautz",
        "Bryan Catanzaro",
        "Ashwath Aithal",
        "Nima Tajbakhsh",
        "Pavlo Molchanov"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Training a family of large language models targeting multiple scales and deployment objectives is prohibitively expensive, requiring separate training runs for each different size. Recent work on model compression through pruning and knowledge distillation has reduced this cost; however, this process still incurs hundreds of billions of tokens worth of training cost per compressed model. In this paper, we present Nemotron Elastic, a framework for building reasoning-oriented LLMs, including hybrid Mamba-Attention architectures, that embed multiple nested submodels within a single parent model, each optimized for different deployment configurations and budgets. Each of these submodels shares weights with the parent model and can be extracted zero-shot during deployment without additional training or fine-tuning. We enable this functionality through an end-to-end trained router, tightly coupled to a two-stage training curriculum designed specifically for reasoning models. We additionally introduce group-aware SSM elastification that preserves Mamba's structural constraints, heterogeneous MLP elastification, normalized MSE-based layer importance for improved depth selection, and knowledge distillation enabling simultaneous multi-budget optimization. We apply Nemotron Elastic to the Nemotron Nano V2 12B model, simultaneously producing a 9B and a 6B model using only 110B training tokens; this results in over 360x cost reduction compared to training model families from scratch, and around 7x compared to SoTA compression techniques. Each of the nested models performs on par or better than the SoTA in accuracy. Moreover, unlike other compression methods, the nested capability of our approach allows having a many-in-one reasoning model that has constant deployment memory against the number of models in the family."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 2 ] . [ 1 4 6 6 6 1 . 1 1 5 2 : r 2025-11-21 Nemotron Elastic: Towards Efficient Many-in-One Reasoning LLMs Ali Taghibakhshi*, Sharath Turuvekere Sreenivas*, Saurav Muralidharan*, Ruisi Cai, Marcin Chochowski, Ameya Sunil Mahabaleshwarkar, Yoshi Suhara, Oluwatobi Olabiyi, Daniel Korzekwa, Mostofa Patwary, Mohammad Shoeybi, Jan Kautz, Bryan Catanzaro, Ashwath Aithal, Nima Tajbakhsh, Pavlo Molchanov Abstract: Training family of large language models targeting multiple scales and deployment objectives is prohibitively expensive, requiring separate training runs for each different size. Recent work on model compression through pruning and knowledge distillation has reduced this cost; however, this process still incurs hundreds of billions of tokens worth of training cost per compressed model. In this paper, we present Nemotron Elastic, framework for building reasoning-oriented LLMs, including hybrid Mamba-Attention architectures, that embed multiple nested submodels within single parent model, each optimized for different deployment configurations and budgets. Each of these submodels shares weights with the parent model and can be extracted zero-shot during deployment without additional training or fine-tuning. We enable this functionality through an end-to-end trained router, tightly coupled to two-stage training curriculum designed specifically for reasoning models. We additionally introduce group-aware SSM elastification that preserves Mambas structural constraints, heterogeneous MLP elastification, normalized MSE-based layer importance for improved depth selection, and knowledge distillation enabling simultaneous multi-budget optimization. We apply Nemotron Elastic to the Nemotron Nano V2 12B model, simultaneously producing 9B and 6B model using only 110B training tokens; this results in over 360 cost reduction compared to training model families from scratch, and around 7x compared to SoTA compression techniques. Each of the nested models performs on par or better than the SoTA in accuracy. Moreover, unlike other compression methods, the nested capability of our approach allows having many-in-one reasoning model that has constant deployment memory against the number of models in the family. Models on Hugging Face Nemotron-Elastic"
        },
        {
            "title": "Introduction",
            "content": "Large Language Models (LLMs) have demonstrated remarkable capabilities across diverse natural language tasks [1, 2, 3], achieving state-of-the-art performance through massive parameter scaling. However, this scaling comes at significant cost: training LLM families with multiple model sizeseach targeting different deployment scenariosrequires training each variant from scratch, resulting in prohibitively expensive computational budgets. For instance, the Llama-3.1 family [3] spans 8B, 70B, and 405B parameters, each trained independently on trillions of tokens. This repeated full-scale training not only multiplies infrastructure costs but also limits practitioners ability to efficiently deploy models tailored to specific resource constraints. Recent advances in model compression have sought to address this challenge through structured pruning and knowledge distillation [4, 5]. These methods train only the largest model from scratch, then derive smaller variants through pruning and retraining. While effective, they still require hundreds of billions of training tokens per compressed model, keeping overall training costs high. promising alternative to model compression is elastic or Matryoshka-style nested networks [6, 7]; here, an elastic or nested model is produced either from scratch or after continued training from an existing model - these elastic models have two special properties: (1) sub-networks meeting specific deployment objectives can be extracted from the parent model for free (i.e., without any additional training/fine-tuning), and (2) all sub-networks share the same weights with the parent model. Concurrently, we observe two recent trends that are relevant to the above discussion: the first is the rise of hybrid models that combine attention mechanisms with State Space Models (SSMs) such as Mamba [8, 9]. These hybrid architectures, exemplified by models like Jamba [10], Zamba [11], and Nemotron-H [12], achieve * Equal contribution. Work done during an internship at NVIDIA. 2025 NVIDIA. All rights reserved. Nemotron Elastic: Towards Efficient Many-in-One Reasoning LLMs Figure 1 Left: Accuracy across key reasoning and mathematical benchmarks. The accuracy shown is the average across all benchmarks: MATH-500, AIME-2024, AIME-2025, GPQA, LiveCodeBench v5, and MMLU-Pro. Right: Scaling analysis comparing Nemotron Elastic and Minitron-SSM as model family size grows. Nemotron Elastic maintains constant cost for tokens and deployment memory, while Minitron-SSM scales linearly. superior efficiency through reduced KV cache requirements and linear-time sequence processing while maintaining competitive accuracy. Unfortunately, there is very limited work targeting the elastification and compression of hybrid models [13]. Second is the transition from base and instruct-tuned models to reasoning models. Modern reasoning-capable LLMs generate extended chains of thought to solve complex problems, requiring substantial token budgets for intermediate reasoning steps. This creates fundamental tension: reasoning models demand both architectural flexibility to handle variable computational budgets and the capacity to process long-context sequences where multi-step inference unfolds. Existing compression techniques fail to address this dual requirement, as they neither support elastic deployment across diverse constraints nor optimize for the longcontext reasoning scenarios critical to these models performance. In this work, we present Nemotron Elastic, framework for training hybrid LLMs that simultaneously support multiple deployment configurations via an end-to-end trained router. Our approach produces multiple nested sub-networks at different parameter budgets from single elastic training run, each optimized for reasoning through two-stage curriculum prioritizing long-context capability. We demonstrate that reasoning models require fundamentally different elastic training strategies compared to standard LLMs, with extended-context training (49K tokens) critical for multi-step inference. We achieve up to 40 reduction in training tokens compared to training model families from scratch, while enabling simultaneous training of multiple budgets within the memory footprint of the largest model alone. Our framework achieves this efficiency through: (1) importancebased component ranking establishing architecture priority orderings, (2) frozen teacher knowledge distillation enabling joint sub-network optimization, (3) two-stage curriculum balancing router stabilization with reasoning-specific long-context adaptation, and (4) end-to-end router learning ensuring architecture decisions respond to actual task difficulty rather than post-hoc search heuristics. We validate our approach by training elastic variants of Nemotron NanoV2 12B reasoning model [14], producing both homogeneous and heterogeneous 9B configurations plus 6B variant, all from single training run. We notice that the resulting nested models achieve competitive or superior accuracy compared to independently trained baselines while delivering significantly faster inference. This work provides an efficient path toward democratizing access to high-performance reasoning models across diverse deployment scenarios. This paper makes the following key contributions: First elastic reasoning model: we introduce the first elastic architecture specifically designed for reasoning LLMs, incorporating twostage training with extended-context optimization (49K tokens) critical for multi-step inference. Depth elastification: We add depth reduction to elastification via iterative layer removal guided by normalized MSE to the full models predictionsresulting in more reliable layer ranking than single-shot or perplexity-based methods. Knowledge distillation guided elastification: during elastic training, we treat the nonelastified model as fixed teacher, guiding compression using teacher-aligned signals rather than CE loss alone. This results in elastified variants that more closely track the behavior of the origi2 Nemotron Elastic: Towards Efficient Many-in-One Reasoning LLMs nal model. mensions: Significant training cost reduction: Our approach requires only 110B tokens to derive 6B and 9B variants from 12B parenta 7 reduction compared to NanoV2 Compression (Minitron-SSM) and 360 more efficient than NanoV2 Pretraining from scratch. Memory-efficient multi-budget training: Elastic training with nested weight-sharing requires memory overhead of only the largest model plus router parameters (<2% additional memory), enabling simultaneous training and deployment of multiple sizes without incurring linear increase in memory costs. Heterogeneous elastification: Our routerbased search enables layer-wise heterogeneous configurations (e.g., varying FFN dimensions across layers), whereas previous elastic methods support only homogeneous configurations. This allows for more granular and potentially more optimal model candidate exploration."
        },
        {
            "title": "Methodology",
            "content": "In this section, we describe the core components of Nemotron Elastic: importance estimation to establish component priority rankings, elastic formulation enabling flexible width and depth selection, two-stage training that couples router learning to task-specific constraints, and the dynamic masking implementation that enables efficient multi-budget training. Figure 2 illustrates and overview of the Nemotron Elastic Pipeline. Importance Estimation and Model Preparation Component importance guides the architectural search by identifying which elements contribute most to model performance. We follow an activation-based approach similar to prior work, establishing foundation upon which the router makes selection decisions. Width We employ activation-based importance scoring to rank model components along each width dimension using layer activation magnitudes. For each axisembedding channels, Mamba heads, Mamba head channels, attention heads, and FFN intermediate neuronswe compute importance scores from forward propagation only, keeping this phase lightweight. For embedding channels, we aggregate normalized input activations across the sequence and batch diImportance(ğ‘–) emb = ğµ,ğ¿ LN(ğ‘‹)ğ‘– (1) For FFN neurons, we score based on the output of the first linear layer (the intermediate activations after projection): Importance(ğ‘–) neuron = ğµ,ğ¿ ğ‘‹(Wğ‘– 1)ğ‘‡ (2) where Wğ‘– matrix in the FFN layer. 1 refers to the ğ‘–-th row of the first weight For Mamba components, we extract scores from projection matrix outputs (specifically Wğ‘¥) and apply nested procedures that respect group-aware constraints. First, head channels are scored by aggregating across all heads: ğ‘ ğ‘‘ = ğµ,ğ¿ s:,ğ‘‘ 2 (3) where = LN(ğ‘‹)(Wğ‘¥)ğ‘‡ . Then head-wise scores are computed using top-ranked channels ğ’Ÿtop: ğ‘“â„ = sâ„,ğ’Ÿtop 2 , â„ {1, . . . , ğ‘šâ„} (4) Finally, group-constrained ranking is applied to preserve SSM structure, ensuring heads within each Mamba group ğ’¢ğ‘” are ranked independently. For attention heads, importance is computed from head-wise activation magnitudes aggregated across query projections. Components are then sorted in decreasing order of importance, establishing ranking permutation ğœ(ğ‘¤) that orders components by their contribution to model behavior. This sorted ordering serves as preference structure guiding the routers selection of which components to retain at different compression budgets. Depth Layer importance is estimated iteratively using normalized mean squared error (MSE) between the full models predictions and predictions with specific layers removed. For each layer ğ‘—, we compute: ğ‘ ğ‘— = ğµ,ğ¿(â„³full â„³ğ‘—)2 ğµ,ğ¿ â„³ full (5) where â„³full represents logits from the full model and â„³ğ‘— represents logits with layer ğ‘— ablated. The normalization by the full models energy ensures that importance scores are comparable across different calibration datasets. This yields per-layer importance 3 Nemotron Elastic: Towards Efficient Many-in-One Reasoning LLMs Figure 2 Overview of the Nemotron-Elastic training and deployment pipeline. Training: For each training sample, data flows to both teacher and student models. budget (parameter size: 6B, 9B, or 12B) is selected and passed to the router, which generates differentiable masks for the student model. Knowledge distillation from the model prior to elastification enables simultaneous optimization across all budget variants. Deployment: After training, all models are extracted zero-shot from single elastic checkpoint: the full 12B model and nested sub-networks (9B and 6B) are immediately available without additional fine-tuning or re-training. scores {ğ‘ ğ‘—}ğ‘ 1 ğ‘—=0 that quantify each layers contribution to model predictions. Layers are sorted in decreasing order of importance, yielding depth ranking permutation ğœ(ğ‘‘) that establishes preference order over the layer stack. This ordering ensures that when the router selects target depth, the most critical layersthose with highest normalized MSEare preferentially retained through the binary depth coefficient ğ›¾ğ‘— = 1. This metric-driven approach captures the actual importance structure specific to the model and dataset, enabling principled depth selection during elastic training. Elastic Formulation We build upon nested weight-sharing architecture that enables single hybrid LLM to dynamically adapt across multiple resource constraints. The model architecture can be resized along both width dimensions (embedding size, attention heads, FFN intermediate dimensions, Mamba heads and head channels) and depth (number of layers), enabling instantaneous generation of sub-networks with different parameter budgets without additional fine-tuning. ğ‘’, ğ‘‘ğ‘— ğ‘“ , ğ‘›ğ‘— Elastic Width. For width dimensions, we define set of elastic choices for each component: embedding dimension ğ‘‘ğ‘’, FFN intermediate dimension ğ‘‘ğ‘“ , attention heads ğ‘›â„, Mamba heads ğ‘šâ„, and Mamba head channels ğ‘šğ‘‘. At training time, sub-networks are constructed by selecting values from these dimension ranges according to target budget. For given objective for sub-network (e.g., latency, memory, model size, etc.), the router selects appropriate dimensions (ğ‘‘ğ‘— ğ‘‘) to satisfy that objective. The nested structure ensures that smaller sub-networks always use contiguous subset of the neurons, heads, and channels retained by larger variants, achieved through the importance-based ranking established during model preparation. Specifically, embeddings are selected via â„³emb masking, FFN neurons via â„³ffn, attention heads via â„³attn_head, and Mamba components via â„³mamba, maintaining consistency with the dynamic masking operators defined in the Implementation section. â„, ğ‘šğ‘— â„, ğ‘šğ‘— conElastic Depth. Depth vector through trolled selection ğ›¾ğ‘— = [ğ›¾ğ‘— 0, ğ›¾ğ‘— ğ‘– {0, 1} determines whether layer ğ‘– is active in sub-network ğ‘—. binary ğ‘ 1] where ğ›¾ğ‘— 1, . . . , ğ›¾ğ‘— elasticity is 4 Nemotron Elastic: Towards Efficient Many-in-One Reasoning LLMs Layers with ğ›¾ğ‘— ğ‘– = 0 are bypassed through residual skip connections, maintaining gradient flow while reducing computation. The importance-based layer ranking ensures that critical layers are preferentially retained at lower budgets. Hybrid Architecture Considerations. For hybrid models combining Mamba and attention, the elastic formulation must respect the structural constraints of both components. Mamba layers require group-aware pruning and channel consistency to preserve SSM computation and attention layers require head-wise selection. The router jointly optimizes selections across both layer types and all width dimensions to discover architectures that balance the complementary strengths of Mambas efficient sequence processing and attentions contextual reasoning capabilities. Elastic Training Router Architecture and Design each dynamic dimension For intro- {emb, mamba, attn_head, ffn, depth}, duce dedicated router network that performs architecture search over the target configuration space. Each router consists of two fully connected layers with leaky ReLU activation applied between them. ğ‘˜ we Router Input Representation. The input to router ğ‘˜ is one-hot encoded vector representing the target compression level: u(ğ‘˜) = eâ„“ Rğ‘›targets (6) where eâ„“ is the â„“-th standard basis vector and ğ‘›targets is the number of target model configurations. Router Architecture. Each router is parameterized as: h(ğ‘˜) = LeakyReLU ( W(ğ‘˜) 1 u(ğ‘˜) + b(ğ‘˜) 1 ) (7) 1 Rğ‘‘routerğ‘›targets and b(ğ‘˜) where W(ğ‘˜) 1 Rğ‘‘router are the first layer weights and bias, and ğ‘‘router is the intermediate hidden dimension. The router output is: z(ğ‘˜) = W(ğ‘˜) 2 h(ğ‘˜) + b(ğ‘˜) 2 (8) 2 Rğ‘›(ğ‘˜) where W(ğ‘˜) output dimension ğ‘›(ğ‘˜) tion mode. outğ‘‘router and b(ğ‘˜) out . The out varies by axis and configura2 Rğ‘›(ğ‘˜) Router Output Dimensions. For homogeneous configuration modes where all instances of component type share the same compression ratio: = â„³ ğ‘›(emb) out = â„° ğ‘›(mamba,hom) out ğ‘›(attn,hom) out ğ‘›(ffn,hom) out ğ‘›(depth) out = ğ‘ = ğ’œ = â„± (9) where â„°, â„³, ğ’œ, â„± denote the cardinality of target configuration sets for each dimension. For heterogeneous configuration modes where each layer can independently select its compression ratio: ğ‘›(mamba,het) out ğ‘›(attn,het) out ğ‘›(ffn,het) out = â„³ ğ‘M = ğ’œ ğ‘A = â„± ğ‘F (10) where ğ‘M, ğ‘A, ğ‘F denote the total counts of Mamba, attention, and FFN layers respectively. Embedding remains homogeneous as its channels are globally indexed. Loss Formulation The router outputs are passed through GumbelSoftmax with temperature ğœ to produce soft probability distributions over configuration choices. At each training iteration, we sample from these distributions to obtain relaxed discrete selections that enable gradient flow to the router parameters. Gumbel-Softmax Relaxation. Let z(ğ‘˜) denote the raw logits output by router ğ‘˜. The GumbelSoftmax relaxation is: ğœ‹(ğ‘˜) ğ‘– = ( exp ğ‘— exp ) z(ğ‘˜) ğ‘– +ğ‘”ğ‘– ğœ ( z(ğ‘˜) ğ‘— +ğ‘”ğ‘— ğœ ) (11) where ğ‘”ğ‘– Gumbel(0, 1) are i.i.d. Gumbel noise samples and ğœ > 0 is temperature parameter that is annealed from high values (soft exploration) to low values (sharp decisions) during training. Router Objective Function. The router is jointly trained to optimize resource-aware objective that maps selected configurations to hardware and computational constraints. Let ğ‘ğ‘˜ {1, . . . , ğ‘›(ğ‘˜) out} denote the configuration selected by router ğ‘˜. The resource cost of configuration ğ‘ğ‘˜ is denoted ğ’(ğ‘˜)(ğ‘ğ‘˜), where 5 Nemotron Elastic: Towards Efficient Many-in-One Reasoning LLMs possible cost metrics include parameter count, memory usage (including model parameters, KV cache, Mamba cache, and activations), latency, or throughput. The router loss is: â„’router = ğ’(ğ‘˜)(ğ‘ğ‘˜) ^ğ’(ğ‘˜) (12) where ^ğ’(ğ‘˜) is the target constraint for dimension ğ‘˜. This enables the router to autonomously search through the joint architecture space, balancing multiple objectives and discovering Pareto-optimal configurations. The hybrid approach of combining importance-based sorting with learned router policies is particularly beneficial for hybrid architectures where the interplay between Mambas linear-time properties and attentions expressiveness creates nonobvious accuracy-efficiency trade-offs."
        },
        {
            "title": "Versatile Training Options",
            "content": "The model and router are jointly optimized during training, enabling the architecture search to directly respond to task-specific learning signals. The model parameters are updated to minimize the primary loss, while the router parameters are updated to discover configurations that satisfy resource constraints while maintaining model accuracy. The training framework supports multiple loss formulations, allowing flexible combinations depending on the training regime and available teacher models. Cross Entropy Loss The model can be trained using standard cross-entropy loss over the training corpus without external supervision: â„’CE = E(ğ‘¥,ğ‘¦)ğ’Ÿ [log ğ‘ğœƒ(ğ‘¦ ğ‘¥)] (13) where ğ’Ÿ is the training dataset, ğœƒ represents model parameters, and ğ‘ğœƒ(ğ‘¦ ğ‘¥) is the models predicted probability distribution. This loss can be used independently or combined with other training objectives. Knowledge Distillation Knowledge Distillation (KD) improves model accuracy by transferring knowledge from teacher model. Let ğ‘ğœƒ(ğ‘¥; ğœ ) denote the student models softmax-normalized logits at temperature ğœ , and ğ‘ğœ‘(ğ‘¥; ğœ ) denote the teachers corresponding distribution. The distillation loss using forward KL divergence is: â„’KD = ğ·KL (ğ‘ğœ‘(ğ‘¥; ğœ )ğ‘ğœƒ(ğ‘¥; ğœ )) (14) In this mode, the full-budget Trainable Teacher: model (100% across all dimensions) simultaneously serves as the teacher and is updated during training. Both student and teacher parameters are optimized jointly: â„’teacher = â„’KD(ğœƒstudent, ğœƒteacher) + ğ›¼ â„’CE(ğœƒteacher) (15) where ğœƒteacher corresponds to model parameters with full budget allocation and ğ›¼ > 0 is weighting factor. This enables the teacher to adapt to the training distribution while providing moving supervision targets. The Cross-Entropy loss is added in this case so that the model doesnt collapse to itself during self distillation. Frozen Teacher: In this mode, the teacher model parameters are frozen throughout training and do not receive gradient updates. The teacher can either be the original pre-trained full model or an alternative model architecture: â„’frozen = â„’KD(ğœƒstudent, ğœ‘fixed) (16) where ğœ‘fixed are static teacher parameters. This approach reduces computational overhead and provides stable, consistent supervision throughout training. Mixed Training Modes The framework supports flexible combinations of these losses. For example, training run can employ trainable teachers for initial phases (capturing distribution-specific knowledge) and transition to frozen teachers for final stages (stabilizing convergence). Different sub-models (e.g., elastic variants at different compression levels) can simultaneously use different teacher modes and loss combinations, enabling rich multi-objective training scenarios. Final Optimization Target The joint optimization of the model and router is achieved through combined objective: â„’total = â„’task(ğœƒ) + ğœ† â„’router(ğœ“) (17) where â„’task(ğœƒ) is the primary learning objective (either cross-entropy, knowledge distillation, or their combination), ğœ“ denotes router parameters, and ğœ† > 0 is weighting coefficient that balances task accuracy against resource constraints. The task loss â„’task directly incorporates the chosen supervision signalwhether from standard language modeling, knowledge distillation from teacher, or hybrid of both. Critically, this end-to-end optimization enables the router to make architecture decisions that are aware of the actual training signal (crossentropy, distillation loss, or combined), rather than optimizing purely for zero-shot proxy metrics in posthoc search phases. This tight coupling between NAS and the training objective represents key distinction 6 Nemotron Elastic: Towards Efficient Many-in-One Reasoning LLMs from prior methods such as Minitron and MinitronSSM, which decouple architecture search (performed via importance scoring on frozen checkpoints) from the final training objective. Our approach integrates architecture discovery directly into the learning process, allowing the router to dynamically adapt configurations in response to the loss landscape of the chosen training regime. Two-Stage Training with Curriculum-Based Sampling Multi-budget elastic training requires carefully orchestrated data allocation across budget targets to prevent training imbalance and maintain performance across all sub-networks. This is particularly critical for reasoning models where task complexity demands sophisticated architectural trade-offs. In the Multi-Budget Training Mechanics. multi-budget setting, each training sample is assigned to one of ğ‘›ğ‘ target budgets, and the corresponding router output determines which subset of parameters participates in the forward pass. This requires careful sampling of data across budgets to ensure balanced learning signals for all model variants. The choice of budget distribution directly influences architecture discovery and performance characteristics of the resulting model family. The Role of Extended-Context Training for Reasoning. Standard elastic training approaches optimize for general knowledge recovery and parameter efficiency. However, reasoning tasks impose fundamentally different constraints: complex multi-step inferencefrom mathematical reasoning to code generationrequires substantial token budget for thinking traces and intermediate steps. Short-context training alone is insufficient for developing genuine reasoning capability; the model must adapt its architecture to support extended sequences where reasoning paths unfold. Extended-context training (with sequence length ğ¿2) exposes all elastic variants to problems requiring longer inference chains, forcing the router to discover configurations that maintain coherence and performance across extended contexts. This necessity motivated our two-stage approach: Stage 1 establishes foundational architecture patterns, while Stage 2 enforces reasoning-specific constraints on the final elastic configuration. uniform budget sampling. For ğ‘›ğ‘ target budgets, each training batch receives equal allocation: ğ‘1(ğ‘) = 1 ğ‘›ğ‘ , ğ‘ {1, . . . , ğ‘›ğ‘} (18) Uniform sampling ensures all sub-networks receive balanced training signal during router stabilization, allowing architecture discovery without budget-specific bias. This allocation establishes diverse architectural patterns before reasoning becomes the dominant bottleneck. Stage 2: Curriculum-Based Non-Uniform Sampling (Extended Context). During extendedcontext training (sequence length ğ¿2, total tokens ğ‘‡2), we transition to non-uniform sampling that prioritizes full-budget models. For ğ‘›ğ‘ target budgets with sampling weights {ğ›¼1, ğ›¼2, . . . , ğ›¼ğ‘›ğ‘ } normalized to ğ‘›ğ‘ ğ‘–=1 ğ›¼ğ‘– = 1: ğ‘2(ğ‘) = ğ›¼ğ‘, ğ‘ {1, . . . , ğ‘›ğ‘} (19) The curriculum-based distribution addresses training imbalance observed empirically: uniform sampling in extended-context causes performance degradation in the full model while smaller budgets improve, indicating gradient competition. Non-uniform weighting biases updates toward full-model performancecritical when the full model serves as teacher in frozen distillationwhile still training smaller variants. This approach prioritizes long-context reasoning capability across all sub-networks, with weights typically skewed toward larger budgets to prevent collapse of the largest model. Training Signal Coupling to Architecture Search. The two-stage sampling strategy directly couples multi-budget training to the routers architecture discovery process. During Stage 1, uniform sampling encourages exploration of diverse configurations across budgets. During Stage 2, non-uniform sampling provides stronger gradients for the full model, guiding the router toward configurations that preserve reasoning capability on extended contexts. This coupling ensures that architecture decisions evolve in response to the actual difficulty of training tasks at each stage, rather than being independently determined by importance scores alone. Implementation Stage 1: Uniform Budget Sampling (Short Context). During the initial short-context phase (sequence length ğ¿1, total tokens ğ‘‡1), we employ The elastic architecture is instantiated through structured masking applied to the hybrid MambaAttention-MLP model. Rather than modifying network topology or creating distinct sub-networks, we 7 Nemotron Elastic: Towards Efficient Many-in-One Reasoning LLMs apply dimension-specific binary masks that dynamically select active components. This masking-based approach enables efficient training of multiple budgets simultaneously while maintaining architectural transparency and enabling straightforward deployment of any sub-network without architectural recompilation."
        },
        {
            "title": "Dynamic Model Formulation",
            "content": "We present flexible architecture framework for Nemotron Elastic that enables dynamic adjustment of model dimensions during training through structured masking approach. Our method builds upon the hybrid Mamba-Attention-MLP architecture and extends the elastic training paradigm to support comprehensive width and depth flexibility for hybrid architectures. dynamic model is obtained by making the stack of layers dynamic, and then making each layer type dynamic across different dimensions. If the original LLM is defined as ğ‘¦ = â„’ğ‘ 0(ğ‘¥) = â„’ğ‘—1 (ğ‘¥)), dynamic layer stack is noted 0 as ğ’Ÿ â„’ğ‘ 0 where the operator ğ’Ÿ is applied to each layer and makes it dynamic. For example: 0 (ğ‘¥) where â„’ğ‘— (ğ‘¥)+â„’ğ‘—(â„’ğ‘—1 0 ğ’Ÿ â„’ğ‘— = (ğ’Ÿ â„’ğ‘—) ğ›¾ğ‘— (20) where ğ›¾ğ‘— {0, 1} controls layer retention (depth adaptation) and ğ’Ÿ â„’ğ‘— represents dynamic Mamba, Attention, or MLP layer. The dynamic operator ğ’Ÿ applies dimension-specific binary masks to the output activations of each layer component, enabling selective feature retention (width adaptation): ğ’Ÿ(â„’(ğ‘¥)) = â„’(ğ‘¥) (21) where denotes element-wise multiplication and {0, 1}ğ‘‘ is binary mask vector that determines which dimensions remain active. Depth adaptation is controlled through the binary coefficient vector ğ›¾ = [ğ›¾0, ğ›¾1, . . . , ğ›¾ğ‘ 1], while width adaptation is managed through dimension-specific masks applied within each layer type. Dynamic Embedding Mask Operator. The operator â„³emb applies to any activation or weight matrix with the hidden size ğ‘‘ğ‘’ as one dimension. For matrix Rğ‘‘ğ‘’ğ‘˜, the masked operation is: â„³emb(W) = (Iğ‘’ 1ğ‘˜) (23) where Iğ‘’ {0, 1}ğ‘‘ğ‘’ with Iğ‘’[0 : ğ‘–] = 1 and Iğ‘’[ğ‘– + 1 : ğ‘‘ğ‘’] = 0 for some ğ‘– [0, ğ‘‘ğ‘’], and denotes outer product broadcasting across dimension ğ‘˜. For matrices Rğ‘˜ğ‘‘ğ‘’ , the mask broadcasts similarly: â„³emb(W) = (1ğ‘˜ Iğ‘’). This operator is applied to layer normalization outputs and all weight matrices interfacing with the embedding dimension. Dynamic Mamba Mask Operator. The operator â„³mamba applies to matrices where dimensions derive from Mamba heads ğ‘šâ„ or head channels ğ‘šğ‘‘. For matrix Rğ‘“ (ğ‘šâ„,ğ‘šğ‘‘)ğ‘˜ where ğ‘“ represents dimension function (typically ğ‘“ (ğ‘šâ„, ğ‘šğ‘‘) = ğ‘šâ„ ğ‘šğ‘‘), the masked operation is: â„³mamba(W) = (Iğ‘š 1ğ‘˜) (24) where Iğ‘š {0, 1}ğ‘“ (ğ‘šâ„,ğ‘šğ‘‘) is constructed to satisfy: Iğ‘š[ğœ‘(â„, ğ‘)] = { 1 0 if â„ â„* and ğ‘ ğ‘* otherwise (25) with ğœ‘(â„, ğ‘) mapping head â„ and channel ğ‘ to flat index, â„* [0, ğ‘šâ„] and ğ‘* [0, ğ‘šğ‘‘] defining active dimensions. This construction preserves group-aware permutation structure: for heads â„, â„ ğ’¢ğ‘” belonging to group ğ‘”, Iğ‘š[ğœ‘(â„, )] = Iğ‘š[ğœ‘(â„, )], and maintains head channel consistency: Iğ‘š[ğœ‘(, ğ‘)] is uniform across all heads for each channel ğ‘. Forward Pass. The dynamic Mamba layer processes input through projection matrices following masked layer normalization. First, we apply the embedding mask to the layer norm output: ğ‘¦ln = â„³emb(LN(ğ‘¦)) (26) Dynamic Mamba For Mamba-2 components in the hybrid architecture, we apply group-aware masking following permutationpreserving constraints to maintain structural integrity of state-space computations. The elastic Mamba layer applies the dynamic operator to its output: ğ’Ÿ(Mambaâ„“(ğ‘¦)) = Mambaâ„“(ğ‘¦) mmamba (22) where mmamba {0, 1}ğ‘‘ğ‘’ is the output mask constructed from dynamic embedding and Mambaspecific constraints. Then, projections are computed from the masked normalized input: ğ‘§ = Wğ‘§ ğ‘¦ln, ğµ = Wğµ ğ‘¦ln, ğ¶ = Wğ¶ ğ‘¦ln, ğ‘¥ = Wğ‘¥ ğ‘¦ln, ğ‘‘ğ‘¡ = Wğ‘‘ğ‘¡ ğ‘¦ln (27) where Wğ‘§, Wğ‘¥ R(ğ‘šâ„ğ‘šğ‘‘)ğ‘‘ğ‘’ , Wğµ, Wğ¶ R(ğ‘”ğ‘‘ğ‘ )ğ‘‘ğ‘’ , and Wğ‘‘ğ‘¡ Rğ‘šâ„ğ‘‘ğ‘’ . Here, ğ‘‘ğ‘’ is the embedding dimension, ğ‘šâ„ denotes Mamba heads, ğ‘šğ‘‘ is the head channel dimension, ğ‘” represents the number of Mamba groups, and ğ‘‘ğ‘  is the SSM state dimension. 8 Nemotron Elastic: Towards Efficient Many-in-One Reasoning LLMs We apply the Mamba-specific mask to ğ‘§, ğ‘¥, and ğ‘‘ğ‘¡: Forward Pass. The dynamic attention layer processes input through masked layer normalization: (28) ğ‘¦ln = â„³emb(LN(ğ‘¦)) (36) ğ‘§ â„³mamba(ğ‘§), ğ‘¥ â„³mamba(ğ‘¥), ğ‘‘ğ‘¡ â„³mamba(ğ‘‘ğ‘¡) The intermediate activations ğ‘¥, ğµ, and ğ¶ undergo causal convolution: ^ğ‘¥ = conv1d(ğ‘¥), ^ğµ = conv1d(ğµ), ^ğ¶ = conv1d(ğ¶) (29) where the conv1d operation on ^ğ‘¥ implicitly respects the Mamba mask structure. The selective state-space model update computes: ğ‘¦ = SSM(^ğ‘¥, ^ğµ, ^ğ¶, A, D, ğ‘‘ğ‘¡) (30) Followed by gated RMSNorm and output projection: ğ‘¦pre = Wğ‘‚ RMSNorm(ğ‘¦ silu(ğ‘§)) (31) Projections for query, key, and value are computed as: = â„³attn_head(Wğ‘„) ğ‘¦ln, = â„³attn_head(Wğ¾) ğ‘¦ln, = â„³attn_head(Wğ‘‰ ) ğ‘¦ln (37) where Wğ‘„, Wğ¾, Wğ‘‰ R(ğ‘›â„ğ‘‘â„)ğ‘‘ğ‘’ , with ğ‘›â„ denoting attention heads and ğ‘‘â„ the head dimension. The attention computation follows: Attn = softmax ( QKğ‘‡ ğ‘‘â„ ) (38) where Wğ‘‚ Rğ‘‘ğ‘’(ğ‘šâ„ğ‘šğ‘‘). Followed by output projection: Finally, both dynamic masks are applied to the layer output: ğ‘¦pre = â„³emb(Wğ‘‚) Attn (39) ğ‘¦out = â„³emb(â„³mamba(ğ‘¦pre)) (32) where Wğ‘‚ Rğ‘‘ğ‘’(ğ‘›â„ğ‘‘â„). complete Mamba layer output The ğ’Ÿ(Mambaâ„“(ğ‘¦)) = ğ‘¦out. is thus Dynamic Attention For multi-head attention layers in the hybrid architecture, we apply head-wise and embedding dimension masking to control capacity. The elastic attention layer applies the dynamic operator to its output: ğ’Ÿ(Attnâ„“(ğ‘¦)) = Attnâ„“(ğ‘¦) mattn (33) where mattn {0, 1}ğ‘‘ğ‘’ is the output mask constructed from dynamic embedding and attention head constraints. Dynamic Attention Head Mask Operator. The operator â„³attn_head applies to matrices where one dimension derives from attention heads ğ‘›â„ or head dimension ğ‘‘â„. For matrix Rğ‘“ (ğ‘›â„,ğ‘‘â„)ğ‘˜ where ğ‘“ (ğ‘›â„, ğ‘‘â„) = ğ‘›â„ ğ‘‘â„, the masked operation is: â„³attn_head(W) = (Iğ‘ 1ğ‘˜) (34) where Iğ‘ {0, 1}ğ‘›â„ğ‘‘â„ satisfies: Iğ‘[ğœ“(ğ‘›, ğ‘‘)] = if ğ‘› ğ‘›* and ğ‘‘ ğ‘‘* { 1 0 otherwise (35) Finally, both dynamic masks are applied to the layer output: ğ‘¦out = â„³emb(â„³attn_head(ğ‘¦pre)) (40) The complete attention layer output ğ’Ÿ(Attnâ„“(ğ‘¦)) = ğ‘¦out. is thus Dynamic FFN For feed-forward network layers, we apply masking to both embedding and intermediate dimensions. The elastic FFN layer applies the dynamic operator to its output: ğ’Ÿ(FFNâ„“(ğ‘¦)) = FFNâ„“(ğ‘¦) mffn (41) where mffn {0, 1}ğ‘‘ğ‘’ is the output mask constructed from dynamic embedding and FFN intermediate dimension constraints. Dynamic FFN Mask Operator. The operator â„³ffn applies to matrices where one dimension derives from the FFN intermediate dimension ğ‘‘int. For matrix Rğ‘‘intğ‘˜, the masked operation is: â„³ffn(W) = (Iğ‘“ 1ğ‘˜) (42) with ğœ“(ğ‘›, ğ‘‘) mapping head ğ‘› and head dimension ğ‘‘ to flat index, ğ‘›* [0, ğ‘›â„] and ğ‘‘* [0, ğ‘‘â„] defining active dimensions. where Iğ‘“ {0, 1}ğ‘‘int with Iğ‘“ [0 : ğ‘—] = 1 and Iğ‘“ [ğ‘— + 1 : ğ‘‘int] = 0 for some ğ‘— [0, ğ‘‘int]. For matrices Rğ‘˜ğ‘‘int , the mask broadcasts similarly. 9 Nemotron Elastic: Towards Efficient Many-in-One Reasoning LLMs Forward Pass. The dynamic FFN layer processes input through masked layer normalization: ğ‘¦ln = â„³emb(LN(ğ‘¦)) (43) The first linear transformation with dynamic masking: â„ = â„³ffn(W1) ğ‘¦ln (44) where W1 Rğ‘‘intğ‘‘ğ‘’ and ğ‘‘int is the intermediate dimension. Followed by activation and second linear transformation: (45) ğ‘¦pre = â„³emb(W2) ğœ(â„) where W2 Rğ‘‘ğ‘’ğ‘‘int and ğœ() denotes the activation function. Finally, both dynamic masks are applied to the layer output: ğ‘¦out = â„³emb(â„³ffn(ğ‘¦pre)) (46) The complete FFN layer output is thus ğ’Ÿ(FFNâ„“(ğ‘¦)) = ğ‘¦out. Depth Adaptation. Layer-wise depth adaptation is achieved through selective layer retention controlled by ğ›¾. The set of active layers is: ğ’œ = {ğ‘— ğ›¾ğ‘— = 1, ğ‘— [0, ğ‘ 1]} (47) where ğ’œ = ğ‘target specifies the target model depth. Skipped layers are bypassed via residual connections: ğ‘¦ğ‘—+1 = { ğ‘¦ğ‘— + ğ’Ÿ â„’ğ‘—(ğ‘¦ğ‘—) ğ‘¦ğ‘— if ğ›¾ğ‘— = 1 if ğ›¾ğ‘— = 0 (48) This maintains signal propagation while reducing computation. For hybrid architectures, selective layer retention enables leveraging the complementary strengths of Mamba and attention components at different model scales. Mask Generation ğ‘– , where ğœ‹(ğ‘˜) Mask Generation from Router Output. The router outputs z(ğ‘˜) are processed through GumbelSoftmax to produce relaxed discrete selections. The selected configuration index is determined by ^ğ‘ğ‘˜ = arg maxğ‘– ğœ‹(ğ‘˜) is the Gumbel-Softmax probability distribution. In homogeneous mode, if dimension ğ‘˜ selects configuration index ^ğ‘ğ‘˜, the corresponding target count is ğ‘^ğ‘ğ‘˜ (e.g., number of active embedding channels, depth, or head counts per layer). The binary mask is then constructed by selecting the top ğ‘^ğ‘ğ‘˜ components according to the importancebased ranking ğœ(ğ‘¤) or ğœ(ğ‘‘): I(ğ‘˜) = I[ğœ(ğ‘˜)(ğ‘—) ğ‘^ğ‘ğ‘˜ ], ğ‘— = 1, . . . , size(ğ‘˜) (49) In heterogeneous mode, the router output is reshaped into per-layer selections: z(ğ‘˜) is partitioned into ğ‘X segments of size ğ’³ , where each segment determines the configuration for one layer. Per-layer masks are constructed similarly, allowing each layer to have distinct compression ratios. For depth selection, if the router outputs ğ¿target [1, ğ‘ ], the top ğ¿target layers from the importance ranking ğœ(ğ‘‘) are activated via ğ›¾ğ‘— = 1 for the selected layers. The generated masks are then applied to the dynamic model operators â„³emb, â„³mamba, â„³attn_head, â„³ffn, and depth retention coefficients ğ›¾ as defined in the Dynamic Model Formulation section, enabling the model to dynamically adjust capacity. Mask Integration Strategies. The GumbelSoftmax probabilities provide differentiable signals for router optimization. We support two mask integration modes: Mode 1: Hard Selection via Argmax Logits. The discrete selection is obtained by ^ğ‘–ğ‘˜ = arg maxğ‘– ğœ‹(ğ‘˜) , and hard mask is applied using the corresponding logit: ğ‘– train = z(ğ‘˜) I(ğ‘˜) ^ğ‘–ğ‘˜ I^ğ‘–ğ‘˜ (50) This directly applies the mask from the selected configuration, scaled by its logit magnitude to provide task-relevant gradient signals. Mode 2: Soft Masking via Probabilistic Combination. Alternatively, masks from all candidate configurations are combined proportionally to their probabilities: I(ğ‘˜) train = ğœ‹(ğ‘˜) ğ‘– Iğ‘– ğ‘– (51) During training, this soft mask is applied in the dynamic operators, allowing gradients to flow through all configuration options. At inference time, the discrete mask corresponding to ^ğ‘–ğ‘˜ from the argmax mode is used and the logit, z(ğ‘˜) ^ğ‘–ğ‘˜ is set to 1. Elastic Model Deployment key advantage of the elastic architecture is the ability to extract multiple model variants from single trained checkpoint without requiring separate training or fine-tuning. This is achieved through learned slicing mechanism that leverages the router module trained during the elastic training phase. After training converges, the router has learned optimal budget-aware decisions for every layer and component (attention heads, Mamba, FFN, embeddings). At deployment time, to extract model for any target budget ğµ that was seen during training, we invoke the router with the budget specification. The Nemotron Elastic: Towards Efficient Many-in-One Reasoning LLMs routers learned decisions are used to determine which components should be pruned from the full model. These components are then permanently removed (sliced out) from the checkpoint, effectively extracting nested sub-network that corresponds to the desired parameter count. Formally, given trained full model with parameter set Î˜max and target budget ğµ â„¬ (where â„¬ is the set of budgets used during training), the router â„› produces pruning specification that identifies the parameters to retain. The sliced model parameters are then: Î˜ğµ = {ğœƒ Î˜max : ğœƒ is retained for budget ğµ} This zero-shot slicing operation is computationally negligible and produces an inference-ready model immediately, with no retraining, fine-tuning, or additional distillation required. Crucially, any budget ğµ â„¬whether the largest, smallest, or any intermediate size explored during trainingcan be deployed directly from the single full-model checkpoint. The practical benefit is substantial: practitioners need to deploy and maintain only single full-size model checkpoint, yet at inference time can select any of the trained budget variants on-the-fly without cost. This enables dynamic model selection based on per-request latency or resource constraints. Furthermore, all extracted variants share the same learned representations and architectural decisions, ensuring consistency across the model family and eliminating the need for separate fine-tuning or calibration for each size."
        },
        {
            "title": "Experiments and Results",
            "content": "We evaluate Nemotron Elastic by compressing the NVIDIA Nemotron Nano V2 12B hybrid model [14] across both base and reasoning variants. We simultaneously target two nested models: 9B, and 6B model, representing 25% and 50% compression, respectively. This multi-target setting showcases the Nemotron-Elasticibility of our elastic framework to serve multiple deployment scenarios from single trained model. Experimental Setup Training data. All experiments utilize the same compression data blend that was used to train Nemotron NanoV2 9B (both base and reasoning variants) [14]. This dataset is employed for both importance estimation of network components, and knowledge distillation-based retraining. Using this standardized data blend ensures fair comparison with the Minitron-SSM baseline [15] and maintains consistency across base and reasoning model variants. Evaluation tasks. We evaluate Nemotron-Elastic across comprehensive suite of downstream reasoning and knowledge benchmarks. For general knowledge and language understanding, we use MMLUPro [16] (college-level multiple-choice reasoning) and GPQA [17] (graduate-level science questions). For mathematical and algorithmic reasoning, we employ MATH-500 [18] (pre-calculus through competitionlevel mathematics), AIME-2024 and AIME-2025 [19] (American Invitational Mathematics Examination), and LiveCodeBench v5 [20] (code generation and problem-solving). All evaluations use pass@1 metrics with reasoning enabled, averaging results over 4 to 16 shots as appropriate for each benchmark. This diverse evaluation set allows us to assess the quality-efficiency tradeoff of Nemotron-Elastic against baseline compression methods. Nested compression. We simultaneously train three nested models (6B, 9B, and 12B) from single 12B parent architecture using multi-budget elastic compression. The 12B model serves as the frozen teacher model for knowledge distillation, providing stable supervision signals throughout training. As described in the Two-Stage Training with CurriculumBased Sampling subsection of the Methodology section, training proceeds in two stages: an initial shortcontext phase (sequence length 8192) followed by an extended-context phase (sequence length 49152). Hyperparameters and training setup. For importance estimation (see the Importance Estimation and Model Preparation of the Methodology section), we process 1024 calibration samples with sequence length of 8192. Knowledge distillation training is conducted in two phases: Phase 1 (Short Context): Batch size 1536, sequence length 8192, trained for approximately 65B tokens. Phase 2 (Extended Context): Batch size 512, sequence length 49152, trained for approximately 45B tokens. Model parameters are optimized at learning rate 9e5, while router parameters are optimized at 1e-2. 60-step linear learning rate warmup is applied to both. The Gumbel-Softmax temperature ğœ is initialized at 1.0 and annealed to 0.05. The router weight ğœ† is set to 1.0, and the linear scaling coefficient for router logits is initialized at 1.0 and linearly increased to 10.0. The router intermediate hidden dimension is ğ‘‘router = 256. Budget sampling strategy. For our NemotronElastic family, we target three nested budgets (6B, 11 Nemotron Elastic: Towards Efficient Many-in-One Reasoning LLMs 9B, 12B). During the short-context phase, we employ uniform budget sampling: ğ‘(budget) = 1 3 for each of {6B, 9B, 12B} This ensures that each budget receives an equal training signal, with approximately one-third of each batch assigned to each model variant. In the extended-context phase, we transition to weighted non-uniform sampling to prevent accuracy degradation in larger models: ğ‘(12B) = 0.5, ğ‘(9B) = 0.3, ğ‘(6B) = 0.2 The non-uniform distribution biases training toward the full-budget model, addressing an observed empirical phenomenon: under uniform sampling in extendedcontext training, the 12B models accuracy substantially degrades while the 6B model improves, indicating training imbalance. The adjusted weighting recovers this balance, allowing all model variants to maintain strong performance. Results Our multi-budget elastic compression strategy yields three model variants from single training run, each operating at different parameter budgets while sharing common foundation. As shown in Table 1, the Nemotron-Elastic-12B model achieves performance comparable to NanoV2-12B on most reasoning benchmarks, achieving an average score of 77.41 compared to 77.38 for NanoV2-12B, despite the complexity of simultaneously optimizing three nested budget targets. Notably, the two-stage training approach with adjusted budget sampling prevents accuracy degradation in larger models that would occur under naive uniform sampling. The extended-context phase training (49k sequence length) demonstrates that the router can adapt architecture decisions to support longer contexts while maintaining multi-budget compatibility. The ability to derive three distinct model deployments from single training process provides significant practical advantages: unified model infrastructure can serve heterogeneous hardware constraints and latency requirements through dynamic budget selection without retraining or managing multiple checkpoints. Cost savings. As shown in Tables 2 and 3, Nemotron Elastic achieves substantial reductions in both training token requirements and deployment memory compared to prior compression approaches. These savings become increasingly significant as model family size grows, demonstrating the practical advantages of elastic training over sequential compression methods. Training token efficiency. key advantage of Nemotron Elastic is the elimination of exploratory knowledge distillation runs required by prior methods such as Minitron [4] and Minitron-SSM [15]. These methods perform architecture search by pruning and distilling candidate configurations to identify optimal architectures for each target size, then perform final knowledge distillation with the selected architecture. This two-phase approach incurs substantial token costs that scale linearly with the number of models in the family: each model size requires both exploratory search runs and final distillation. In contrast, Nemotron Elastic performs end-to-end router-guided architecture search during single elastic training run, where all target budgets are optimized simultaneously. The router learns to select optimal configurations for each budget as part of the unified training objective, eliminating the need for separate exploratory runs. Table 2 compares training token requirements for deriving 6B and 9B models from 12B parent. For prior methods like Minitron-SSM, token cost scales as: TokensMinitron(ğ‘›) = ğ‘› (Tokensexplore + TokensKD) (52) In where ğ‘› is the number of target model sizes. contrast, Nemotron Elastic requires: TokensElastic(ğ‘›) = Tokenselastic-KD constant (53) This constant-cost property stems from simultaneous multi-budget optimization: all nested sub-networks share gradient information and are trained together, with marginal overhead for additional target budgets. Deployment memory efficiency. Elastic models with nested weight-sharing provide significant memory advantages for deployment scenarios requiring multiple model sizes. Since all sub-networks share the same parameter space with only routing metadata differentiating them, deploying all budget variants requires memory equivalent to the largest model alone. In contrast, traditional compression methods produce separate checkpoints for each model size, requiring cumulative storage. The memory advantage scales with family size. For prior approaches, memory requirements scale linearly: MemorySeparate(ğ‘›) = ğ‘› ğ‘–=1 Size(Modelğ‘–) (54) For elastic models with nested weight-sharing: MemoryNested(ğ‘›) = Size(Modelmax) + ğœ–router (55) 12 Nemotron Elastic: Towards Efficient Many-in-One Reasoning LLMs Model Nemotron-Elastic-6B Nemotron-Elastic-9B Nemotron-Elastic-12B NanoV2-9B NanoV2-12B QWen3-8B Math-500 AIME-2024 AIME-2025 GPQA LiveCodeBench MMLU-Pro Average 96.50 97.25 97.70 97.30 97.50 96.3 77.64 80.26 83.44 80.89 82.90 75. 68.13 75.42 75.83 71.43 72.50 69.31 53.78 62.50 63.25 63.01 65.28 59.61 60.95 66.82 68.01 67.30 67.61 59.5 66.65 73.45 76.20 73.61 78.47 75.50 70.61 75.95 77.41 75.99 77.38 72.68 Table 1 Multi-budget nested compression results on comprehensive reasoning benchmarks. All three Nemotron-Elastic variants (6B, 9B, 12B) are obtained from single training run with frozen 12B teacher. Nemotron-Elastic-12B achieves competitive performance (77.41) compared to NanoV2-12B baseline (77.38), while simultaneously enabling efficient 9B and 6B deployments. Method Model Sizes Exploratory Final Total NanoV2 Pretraining 6B + 9B NanoV2 Compression (Minitron-SSM) 6B + 9B Nemotron Elastic 6B + 9B 0 480 0 40 40 270 750 110 110 Table 2 Token budget comparison for deriving 6B and 9B models. Nemotron Elastic eliminates exploratory runs and requires only single elastic distillation phase, achieving around 7X token reduction compared to Minitron-SSM (NanoV2 Compression). Note: Token budgets for Minitron-SSM 6B, pretraining 9B, and 6B NanoV2 models are estimated based on the token counts for pretraining and compressing the NanoV2 12B model [14]. Config Models Nemotron Elastic NanoV2 6B + 9B + 12B 9B + 12B Memory 24 GB 42 GB Table 3 Deployment memory comparison (BF16 weights). Despite storing three models, Nemotron Elastic uses 43% less memory than NanoV2s two models. where ğœ–router < 0.02Size(Modelmax) represents router parameter overhead (typically <1 GB). The nested architecture is particularly valuable for edge deployment scenarios where multiple model sizes must be available to handle varying workloads or user-selected quality-latency tradeoffs. sub-models, while extended-context improves the long context reasoning capability of the model, necessary for achieving competitive results on reasoning benchmarks. We evaluate the impact of sampling strategy on downstream performance: The ablation demonstrates that adjusted sampling substantially improves performance for the full-budget model. For instance, on AIME-2025, the 12B model gains 3.54 percentage points with adjusted sampling, while maintaining competitive performance on other budgets. This suggests that multi-budget training requires careful load balancing to prevent negative transfer between budget targets. Effects of Two-Stage Training The necessity of two-stage training is demonstrated through comparisons between Stage 1 (short-context) and Stage 2 (extended-context) performance: The results in Table 4 reveal clear pattern: Stage 2 extended-context training delivers disproportionate improvements on complex reasoning benchmarks (AIME-2025), especially for smaller models. The 6B model gains 19.8% on AIME-2025, while the 12B model gains 4.0%, indicating that smaller models particularly benefit from extended-context adaptation for multi-step reasoning. These gains justify the twostage curriculum: short-context training stabilizes the router and helps initial recovery of the compressed Impact of Budget Sampling Strategy. We investigate the effect of budget sampling distribution through an ablation study comparing uniform budget allocation against our adjusted non-uniform sampling  (Table 5)  . Results demonstrate that uniform sampling leads to performance imbalance during extendedcontext training: the 12B models accuracy degrades significantly on challenging benchmarks, while smaller variants remain competitive. Our adjusted weighting (ğ‘(12B) = 0.5, ğ‘(9B) = 0.3, ğ‘(6B) = 0.2) recovers fullmodel performance by prioritizing gradients toward the largest variant, where reasoning capability demands greater architectural sophistication. The 12B model shows substantial improvements across multiple benchmarks, while smaller variants maintain sta13 Nemotron Elastic: Towards Efficient Many-in-One Reasoning LLMs Model (Benchmark) Performance Stage 1 Stage 2 Absolute Gain Relative Improvement Nemotron-Elastic-6B (Math-500) Nemotron-Elastic-6B (AIME-2025) Nemotron-Elastic-6B (GPQA) Nemotron-Elastic-9B (Math-500) Nemotron-Elastic-9B (AIME-2025) Nemotron-Elastic-9B (GPQA) Nemotron-Elastic-12B (Math-500) Nemotron-Elastic-12B (AIME-2025) Nemotron-Elastic-12B (GPQA) 95.15 56.88 49.12 97.13 68.75 59.43 97.27 72.92 62.50 96.50 68.13 53.78 97.25 75.42 62.50 97.70 75.83 63.25 +1.35 +11.25 +4.66 +0.12 +6.67 +3.07 +0.43 +2.91 +0.75 +1.4% +19.8% +9.5% +0.1% +9.7% +5.2% +0.4% +4.0% +1.2% Table 4 Two-stage training improvements across model sizes and benchmarks. Stage 2 (extended-context) provides substantial gains on reasoning benchmarks, particularly on AIME-2025, where smaller models benefit significantly (6B: +19.8%, 9B: +9.7%). These improvements demonstrate that reasoning tasks require extended-context training to achieve competitive performance, validating the two-stage approach. Model Math-500 AIME-2025 GPQA Nemotron-Elastic-6B Nemotron-Elastic-9B Nemotron-Elastic-12B NanoV2-9B NanoV2-12B Uniform 96.40 97.40 97.33 Adjusted 96.50 97.25 97. Uniform 67.71 75.00 72.29 Adjusted 68.13 75.42 75.83 Uniform 55.30 62.75 61.11 Adjusted 53.78 62.50 63.25 97.30 97.50 71.43 72. 63.01 65.28 Table 5 Budget sampling ablation. Adjusted non-uniform sampling (0.5, 0.3, 0.2 for 12B, 9B, 6B) achieves better balance across model sizes, particularly improving 12B accuracy on challenging benchmarks (AIME-2025: +3.54%, GPQA: +2.14%) compared to uniform sampling. ble performance. This ablation confirms that budgetaware curriculum design is essential for balanced multi-target elastic compression, and that NemotronElastic-12B achieves competitive performance relative to baseline NanoV2 models while enabling zero-shot deployment across all budget variants."
        },
        {
            "title": "Related Work",
            "content": "Model Compression and Pruning. Structured pruning has emerged as powerful technique for LLM compression [21, 22, 5]. Recent work combines pruning with knowledge distillation for accuracy recovery [4], achieving strong results on Transformer models. Group-aware SSM pruning preserves structural constraints critical for sequence modeling while enabling hybrid model compression [23]. Unfortunately, these approaches require separate distillation for each target size. Hybrid SSM-Transformer Models. Hybrid architectures combining Transformers with SSMs have shown promise for efficient long-context modeling [8, 9, 10, 11, 12]. Nemotron-H [12] replaces 92% of attention layers with Mamba2 blocks, achieving 3 inference speedup. Concurrent compression work [15] introduces group-aware Mamba pruning but requires separate distillation per model size. Elastic and Nested Architectures. MatFormer [7] and Flextron [6] pioneered nested weight-sharing for Transformers, training multiple sub-networks simultaneously. Extending the MatFormer methodology to SSMs, MatMamba [13] introduces Matryoshka-style sub-block architecture for Mamba layers. MatFormer introduces MixnMatch heuristics for sub-network selection, while Flextron adds input-adaptive routing for attention and MLP dimensions. However, neither supports: (1) hybrid Mamba-Attention architectures, (2) reasoning-focused two-stage training with extended context, or (3) heterogeneous layer-wise architecture selection via end-to-end learned routing. Googles Gemma 3n [24] recently demonstrated MatFormerstyle nested models with conditional parameter loading, validating the practical deployment value of elastic architectures. Our work extends these foundations to reasoning models and hybrid architectures. Reasoning Model Training. Reasoning-capable LLMs generate extended thought chains for complex problem-solving [25, 26], requiring long-context support for intermediate steps. Prior work on reasoning model optimization focuses on prompting strategies Nemotron Elastic: Towards Efficient Many-in-One Reasoning LLMs or reinforcement learning from reasoning traces [27], but does not address architectural efficiency or elastic deployment. We demonstrate that reasoning models have fundamentally different training requirementsspecifically, extended-context training (49K tokens) is critical for maintaining reasoning performance in compressed variants, requirement not present in standard LLM compression."
        },
        {
            "title": "Conclusions",
            "content": "This paper has presented Nemotron Elastic, the first elastic training framework for reasoning-capable LLMs. We demonstrate that elastic compression of reasoning models requires fundamentally different approaches than standard LLM compression, with extended-context training playing critical role in preserving reasoning performance across deployment scales. Nemotron Elastic achieves strong results: deriving model family from single 12B parent requires only 110B training tokensa 360x reduction versus training from scratch and 7x reduction compared to sequential compression. This efficiency is achieved without compromising accuracy or introducing memory overhead, and having constant memory footprint at deployment against the number of models in the family. During deployment, all the nested submodels can be extracted from the biggest model using zero-shot slicing. Our approach makes elastic reasoning model training practical for organizations with modest computational budgets. Future directions for this work include scaling to larger model families, task-specific architecture selection, dynamic inferencetime routing, and integration with quantization for extreme parameter reduction."
        },
        {
            "title": "Acknowledgments",
            "content": "We would like to thank our colleagues and leaders at NVIDIA for their valuable input and support, including Akhiad Bercovich, Alex Fit-Florea, Joey Conway, Jonah Alben, Jonathan Cohen, Luis Vega, Michael Lightstone, Nave Assaf, Oleksii Kuchaiev, Ran Zilberstein, Terry Kong, Udi Karpas, and Zijia Chen. 15 Nemotron Elastic: Towards Efficient Many-in-One Reasoning LLMs"
        },
        {
            "title": "References",
            "content": "[1] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901, 2020. [2] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models. ArXiv, abs/2307.09288, 2023. [3] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. [4] Saurav Muralidharan, Sharath Turuvekere Sreenivas, Raviraj Joshi, et al. Compact language models via pruning and knowledge distillation. arXiv preprint arXiv:2407.14679, 2024. [5] Mengzhou Xia, Tianyu Gao, Zhiyuan Zeng, and Danqi Chen. Sheared llama: Accelerating language model pre-training via structured pruning. arXiv preprint arXiv:2310.06694, 2023. [6] Ruisi Cai, Saurav Muralidharan, Greg Heinrich, et al. Flextron: Many-in-one flexible large language model. arXiv preprint arXiv:2406.10260, 2024. [7] Sneha Kudugunta, Aditya Kusupati, Tim Dettmers, et al. Matformer: Nested transformer for elastic inference. arXiv preprint arXiv:2310.07707, 2023. [8] Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint arXiv:2312.00752, 2023. [10] Opher Lieber, Barak Lenz, Hofit Bata, et al. Jamba: hybrid transformer-mamba language model. arXiv preprint arXiv:2403.19887, 2024. [11] Paolo Glorioso, Quentin Anthony, and Yury Tokpanov. Zamba: compact 7b ssm hybrid model. arxiv preprint arXiv:2405.16712, 2024. [12] Aaron Blakeman, Aarti Basant, et al. NemotronA family of accurate and efficient hyh: brid mamba-transformer models. arXiv preprint arXiv:2504.03624, 2025. [13] Abhinav Shukla, Sai Vemprala, Aditya Kusupati, and Ashish Kapoor. MatMamba: Matryoshka State Space Model, 2024. [14] NVIDIA Nemotron Nano. Efficient hybrid mambaarXiv preprint reasoning model. transformer arXiv:2508.14444, 2025. [15] Ali Taghibakhshi, Sharath Turuvekere Sreenivas, Saurav Muralidharan, et al. Minitron-SSM: Efficient Hybrid Language Model Compression through arXiv preprint Group-Aware SSM Pruning. arXiv:2504.11409, 2025. [16] Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, et al. Mmlupro: more robust and challenging multi-task language understanding benchmark. arXiv preprint arXiv:2406.01574, 2024. [17] David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel Bowman. Gpqa: graduate-level google-proof q&a benchmark. arXiv preprint arXiv:2311.12022, 2023. [18] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021. [19] Mathematical Association of America. American https:// invitational mathematics examination. www.maa.org/math-competitions/aime, 2024. [20] Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando Solar-Lezama, Koushik Sen, and Ion Stoica. Livecodebench: Holistic and contamination free evaluation of large language models for code. arXiv preprint arXiv:2403.07974, 2024. [21] Xinyin Ma, Gongfan Fang, and Xinchao Wang. Llmpruner: On the structural pruning of large language models. Advances in neural information processing systems, 36, 2023. [9] Tri Dao and Albert Gu. Transformers are ssms: Generalized models and efficient algorithms through structured state space duality. arXiv preprint arXiv:2405.21060, 2024. [22] Saleh Ashkboos, Maximilian Croci, Marcelo Gennari do Nascimento, et al. Slicegpt: Compress large language models by deleting rows and columns. arXiv preprint arXiv:2401.15024, 2024. 16 Nemotron Elastic: Towards Efficient Many-in-One Reasoning LLMs [23] Ali Taghibakhshi, Sharath Turuvekere Sreenivas, Saurav Muralidharan, Marcin Chochowski, Yashaswi Karnati, Raviraj Joshi, Ameya Sunil Mahabaleshwarkar, Zijia Chen, Yoshi Suhara, Oluwatobi Olabiyi, et al. Efficient hybrid language model compression through group-aware ssm pruning. arXiv preprint arXiv:2504.11409, 2025. [24] Google AI. Gemma 3n model overview. https:// ai.google.dev/gemma/docs/gemma-3n, 2024. [25] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35:2482424837, 2022. [26] Shunyu Yao, Dian Yu, Jeffrey Zhao, et al. Tree of thoughts: Deliberate problem solving with large language models. Advances in Neural Information Processing Systems, 36, 2023. [27] Hunter Lightman, Vineet Kosaraju, Yura Burda, et al. Lets verify step by step. arXiv preprint arXiv:2305.20050, 2023."
        }
    ],
    "affiliations": [
        "NVIDIA"
    ]
}