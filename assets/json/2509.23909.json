{
    "paper_title": "EditScore: Unlocking Online RL for Image Editing via High-Fidelity Reward Modeling",
    "authors": [
        "Xin Luo",
        "Jiahao Wang",
        "Chenyuan Wu",
        "Shitao Xiao",
        "Xiyan Jiang",
        "Defu Lian",
        "Jiajun Zhang",
        "Dong Liu",
        "Zheng liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Instruction-guided image editing has achieved remarkable progress, yet current models still face challenges with complex instructions and often require multiple samples to produce a desired result. Reinforcement Learning (RL) offers a promising solution, but its adoption in image editing has been severely hindered by the lack of a high-fidelity, efficient reward signal. In this work, we present a comprehensive methodology to overcome this barrier, centered on the development of a state-of-the-art, specialized reward model. We first introduce EditReward-Bench, a comprehensive benchmark to systematically evaluate reward models on editing quality. Building on this benchmark, we develop EditScore, a series of reward models (7B-72B) for evaluating the quality of instruction-guided image editing. Through meticulous data curation and filtering, EditScore effectively matches the performance of learning proprietary VLMs. Furthermore, coupled with an effective self-ensemble strategy tailored for the generative nature of EditScore, our largest variant even surpasses GPT-5 in the benchmark. We then demonstrate that a high-fidelity reward model is the key to unlocking online RL for image editing. Our experiments show that, while even the largest open-source VLMs fail to provide an effective learning signal, EditScore enables efficient and robust policy optimization. Applying our framework to a strong base model, OmniGen2, results in a final model that shows a substantial and consistent performance uplift. Overall, this work provides the first systematic path from benchmarking to reward modeling to RL training in image editing, showing that a high-fidelity, domain-specialized reward model is the key to unlocking the full potential of RL in this domain."
        },
        {
            "title": "Start",
            "content": "EDITSCORE: UNLOCKING ONLINE RL FOR IMAGE EDITING VIA HIGH-FIDELITY REWARD MODELING Jiahao Wang2,3, Xin Luo1,3, Defu Lian1, Jiajun Zhang2, 1 University of Science and Technology of China 2 Institute of Automation, Chinese Academy of Sciences, 3 Beijing Academy of Artificial Intelligence, 4 Zhejiang University Chenyuan Wu1,3, Dong Liu1, Zheng Liu3 Shitao Xiao3, Xiyan Jiang3,4, 5 2 0 2 8 2 ] . [ 1 9 0 9 3 2 . 9 0 5 2 : r a"
        },
        {
            "title": "ABSTRACT",
            "content": "Instruction-guided image editing has achieved remarkable progress, yet current models still face challenges with complex instructions and often require multiple samples to produce desired result. Reinforcement Learning (RL) offers promising solution, but its adoption in image editing has been severely hindered by the lack of high-fidelity, efficient reward signal. In this work, we present comprehensive methodology to overcome this barrier, centered on the development of state-of-the-art, specialized reward model. We first introduce EditRewardBench, comprehensive benchmark to systematically evaluate reward models on editing quality. Building on this benchmark, we develop EditScore, series of reward models (7B72B) for evaluating the quality of instruction-guided image editing. Through meticulous data curation and filtering, EditScore effectively matches the performance of learning proprietary VLMs. Furthermore, coupled with an effective self-ensemble strategy tailored for the generative nature of EditScore, our largest variant even surpasses GPT-5 in the benchmark. We then demonstrate that high-fidelity reward model is the key to unlocking online RL for image editing. Our experiments show that, while even the largest open-source VLMs fail to provide an effective learning signal, EditScore enables efficient and robust policy optimization. Applying our framework to strong base model, OmniGen2, results in final model that shows substantial and consistent performance uplift. Overall, this work provides the first systematic path from benchmarking to reward modeling to RL training in image editing, showing that high-fidelity, domainspecialized reward model is the key to unlocking the full potential of RL in this domain. Github Link: https://github.com/VectorSpaceLab/EditScore."
        },
        {
            "title": "INTRODUCTION",
            "content": "Recently, Reinforcement Learning (Li, 2017) has demonstrated immense power in advancing large language models and has also shown remarkable success in the text-to-image (T2I) domain. Works like FlowGRPO (Liu et al., 2025a) and DanceGRPO (Xue et al., 2025) have leveraged RL on flowmatching models to significantly enhance T2I generation across multiple dimensions. Despite these successes, the application of RL to the image editing domain remains largely underexplored. We posit that RL holds significant, untapped potential for image editing. By framing editing as dynamic trial-and-error process, policy model can discover novel and more effective editing strategies beyond what is captured in static datasets. Through well-designed reward mechanism, the model can be progressively steered to better align with user intent and achieve deeper, more robust editing capability across diverse scenarios. Despite its theoretical promise, applying online RL to high-resolution image editing remains formidable and largely unsolved challenge (Wei et al., 2025; Wu et al., 2025a; Ahmadi et al., 2025). The primary obstacle lies in the absence of suitable reward function: reliable, efficient, and Equal Contribution. Corresponding Author. 1 scalable oracle that can accurately score the quality of an edit given an instruction. natural consideration would be to employ state-of-the-art, general-purpose Visual Language Models (VLMs) like GPT-5 (OpenAI, 2025) or other proprietary APIs as reward providers. However, this approach is impractical for online RL, with these large VLMs are expensive to query at scale. The second alternative, using powerful open-source VLMs, resolves cost issues. However, our investigations reveal critical performance gap. As we will demonstrate, even Qwen2.5-VL-72B (Bai et al., 2025) fails to provide sufficiently accurate and consistent reward signal to effectively guide the policy. Directly using such models as reward functions often causes unstable training or policy collapse, showing that scale alone cannot replace specialized accuracy. Progress thus hinges on an accurate reward model tailored to image editing, yet the community still lacks powerful open-source optionposing major barrier to advancing RL-based editing. In this work, we argue that the key to unlocking online RL for image editing is the development of specialized, high-fidelity, and efficient reward model. We present systematic approach to build, validate, and deploy such model. Our contributions are organized as follows: First, to enable rigorous and reproducible research, we introduce EditReward-Bench, comprehensive benchmark for evaluating reward models in image editing. It is organized into four main categories, comprising 13 diverse subtasks ranging from simple attribute edits to complex compositional changes. Each entry is evaluated by expert human raters across three key dimensions: prompt following, consistency, and overall quality. All annotations have passed rigorous inter-annotator agreement checks, establishing reliable standard for measuring reward model quality. Second, guided by our benchmark, we develop EditScore, series of powerful reward models (7B72B) for evaluating the quality of instruction-guided editing. Through careful data curation and filtering and inference-time scaling, EditScore sets new state of the art for open-source reward models in this domain, even surpassing the accuracy of leading proprietary VLMs. To validate its practical utility, we first apply EditScore in Best-of-N selection experiment across several state-of-the-art editing models, which confirms that our reward model can effectively improve the performance of diverse editors. We further used EditScore for RL training, resulting in model that shows substantial and consistent performance uplift over its base model. In summary, our key contributions are as follows. We propose EditReward-Bench, new public benchmark for the direct and reliable evaluation of reward models for instruction-guided image editing. We develop and release EditScore, series of state-of-the-art open-source reward models for instruction-based image editing that, through our self-ensembling strategy, surpasses the accuracy of leading proprietary VLMs on this task. We demonstrate the broad, practical utility of EditScore through Best-of-N selection experiment, successfully enhancing the performance of multiple diverse base models. We provide comprehensive analysis of an end-to-end online RL application, identifying key factors for success, including the accuracy and variance of the reward signal."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Reward Models for Image Generation. Reward models provide signals based on specific preferences and can be applied in various scenarios such as automated evaluation and reinforcement learning. Most research has focused on text-to-image tasks, with works like Wu et al. (2023b); Kirstain et al. (2023); Zhang et al. (2024b) fine-tune CLIP-based model with human preference data and Ma et al. (2025); Wang et al. (2025b); Gao et al. (2025) employ VLMs as core component for reward generation. Research on reward models for image editing is relatively underexplored: some works focused on controlled edits based on predefined masks (Ren et al., 2024; Gong et al., 2025). For instruction-guided editing, Zhang et al. (2024a) fine-tunes the BLIP model (Li et al., 2022) with human annotated reward signals for the image generated only by P2P Hertz et al. (2022) and IP2P Brooks et al. (2023), while Chen et al. (2025) leverages CLIP scores to automatically construct training data for training LLaVA-Next-8B Liu et al. (2024) as reward model. However, they used outdated editing models to generate images that needed to be evaluated, or simply used CLIP to annotate and score the images. Meanwhile, they use limited task categories as the prompt pool. The 2 limitations in the narrow scope of task and generation model coverage make them struggle to support online RL for current models. Furthermore, they are not open-source and cannot be used and evaluated by the public. In contrast, our model enables broader evaluation of editing tasks (13 tasks) while supporting SOTA editing models (e.g., Gemini-2.5-image-preview). Moreover, its generative nature enables it to perform inference-time scaling to enhance scoring accuracy (Liu et al., 2025d). Image Reward Model Evaluation. Establishing reliable benchmark is critical as it ensures the objective measurement of reward modeling performance and offers clear guidance for optimization. In previous work, image reward models are evaluated on their accuracy in predicting human preferences for key generative tasks, such as text-to-image generation (Xu et al., 2023; Kirstain et al., 2023; Wu et al., 2023b;a; Ma et al., 2025) and image editing (Ku et al., 2023b; Jiang et al., 2024). Over the past year, generative models such as GPT-Image-1 (Hurst et al., 2024), FLUX-Kontext (Batifol et al., 2025), Qwen-Image-Edit (Wu et al., 2025a) and Nano Banana (Google, 2025) have achieved significant breakthroughs in image editing, particularly in stylization, hybrid edit and text modification. With the growing power of generative models and their expanding editing capabilities, there is an urgent need for reward models that can accurately assess them. This, in turn, necessitates the development of novel and more comprehensive benchmark for evaluating edit rewards."
        },
        {
            "title": "3 EDITREWARD-BENCH",
            "content": "3.1 OVERVIEW EditReward-Bench is benchmark specifically designed for systematic evaluation of reward modeling for image editing. It covers 13 diverse editing tasks and spans 11 heterogeneous editing models for data construction, ranging from open-source baselines to state-of-the-art proprietary models (see Table 1 for comparison). The benchmark is distinguished by the following features. Multi-dimensional Image Evaluation Framework. EditReward-Bench offers three key dimensions for evaluating editing outcomes. These includes Prompt Following (adherence to prompts), consistency (preservation of key visual elements) and overall quality (comparison across all aspects). Comprehensive Model Performance Spectrum. EditReward-Bench incorporates both state-ofthe-art editing models and lower-peforming baselines. It effectively challenges the reward models distinguish ability and validates the reward models scoring performance on current SOTA models. Extensive Task Coverage with Real-world Applicability. EditReward-Bench also covers diverse set of editing tasks that closely align with real-world application scenarios, ensuring authentic and comprehensive evaluation of reward models for image editing. Table 1: Comparison of EditReward-Bench against existing benchmarks, highlighting its superior scale, data sources, and comprehensive task coverage. The order of subtasks within each tuple under Task Coverage corresponds to the order of subtasks listed for each category in Section 3.2. Feature ImagenHub (Ku et al., 2023b) GenAI-Bench (Jiang et al., 2024) EditReward-Bench (ours) General Properties Size Multi-Dimensional Eval. Proprietary Model Data 2,864 Task Coverage (Conceptual Groups) Subject Appearance Scene Advanced (, , ) (, , , ) (, ) (, , , ) 919 3,072 (, , ) (, , , ) (, ) (, , , ) (, , ) (, , , ) (, ) (, , , ) 3.2 CONSTRUCTION PROTOCOL To create robust and comprehensive benchmark, we focused on three key pillars: the diversity of editing tasks, the variety of editing models, and the granularity of our evaluation criteria. 3 First, to ensure comprehensive task coverage, we structure our benchmark into four main categories, comprising 13 distinct subtasks curated from established datasets like GEdit-Bench-EN (Liu et al., 2025b) and ImgEdit-Bench (Ye et al., 2025). These categories are designed to span wide spectrum of complexity. They are: 1) Subject, which includes fundamental tasks like subject addition, subject removal and subject replace; 2) Appearance, covering shape-preserving edits such as color alteration, material modification, style transfer and tone transformation; 3) Scene, which tests understanding of image layout through tasks like background change and extract; and 4) Advanced, the most challenging category, which requires advanced reasoning for tasks like portrait beautification, text modification, motion change and hybrid edit. Second, to populate our benchmark with diverse distribution of edited images, the candidate pool of outputs were generated by diverse array of 11 generative models for data construction: Step1XEdit (Liu et al., 2025b), Step1X-Edit v1.1 (Liu et al., 2025b), Qwen-Image-Edit (Wu et al., 2025a), OmniGen2 (Wu et al., 2025b), FLUX-Kontext-dev (Batifol et al., 2025), FLUX-Kontext-pro (Batifol et al., 2025), Bagel (Deng et al., 2025), MagicBrush (Zhang et al., 2023), Omnigen (Xiao et al., 2025), gpt-image-1 (Hurst et al., 2024), and Gemini-2.5-image-preview (Google, 2025), including both open-source and state-of-the-art proprietary editors. Finally, recognizing that the quality of editing results is not monolithic, we designed multidimensional evaluation scheme for better interpretability as inspired by (Ku et al., 2023a; Liu et al., 2025b). Each editing result is assessed along three distinct axes: Prompt Following (PF): measuring how faithfully the edit executes the given instruction. Consistency (C): assessing the preservation of unedited image regions. Overall Quality (O): providing holistic comparison of edits by accounting for all relevant aspects. 3.3 ANNOTATION PIPELINE To ensure the reliability of EditReward-Bench, we designed rigorous human annotation pipeline. The process was conducted exclusively by experts in generative AI to guarantee data quality and consistency, as illustrated in Figure 1. Before the formal annotation, all raters scored small subset of the entire sample pool to discuss disagreements and collectively established unified scoring guidelines. Then the raters proceeded to the formal annotation of the remaining samples. For each input with five output images randomly sampled from the candidate pool, they were asked to rank them. Our protocol uniquely allowed them to group outputs of similar quality into the same tiers; for instance, ranking of 31245 places output 3 in the top tier, outputs 1 and 2 tied in the second, and 4 and 5 tied in the third. Crucially, each data point was annotated by two raters, and only entries where their rankings were in complete agreement were accepted into the final benchmark, ensuring maximum reliability. These tiered rankings were then decomposed into pairwise preference tuples. Following the 31245 example, this conversion yields preference pairs where higher-tiered item is preferred over lowertiered one, such as (3, 1), (3, 2), (1, 4) and (2, 5). This method correctly excludes comparisons between items within the same tier (e.g., (1, 2)), as they represent equal quality. This process resulted in large-scale, high-fidelity dataset of pairwise comparisons, comprising 944 pairs for prompt following, 890 for consistency, and 1,238 for overall quality, for total of 3,072 preference pairs. detailed overview of our custom-built annotation interface is provided in Appendix F."
        },
        {
            "title": "4 METHOD",
            "content": "4.1 EDIT REWARD 4.1.1 THE APPROACH TO REWARD MODELING Inspired by (Wang et al., 2025b;a; Wu et al., 2025c), we formulate reward modeling as conditional textual generation task. We fine-tune models from the powerful Qwen2.5-VL series (Bai et al., 2025) on standard autoregressive objective to act as specialized evaluators. Our model EditScore takes (Instruction, Input Image, Output Image) as input, (Reasoning, Scalar Score) as output. The chain-of-thought (Wei et al., 2022) process of output not only enhances the models interpretability but also demonstrably improves the accuracy of the final scores. 4 Figure 1: Illustration of the annotation process. Annotators are presented with five candidate output images and are asked to rank them according to three evaluation dimensions. The final ranking is determined through consensus among multiple annotators. For example, 12345 indicates that the first image is preferred over images 2, 3 and 4, which are in turn preferred over image 5. To ensure EditScore can perform comprehensive evaluation of editing tasks, we adopt the VIEScore framework (Ku et al., 2023a), prompting the model in parallel to assess two orthogonal aspects. The first is Semantic Consistency (SC), which evaluates the degree of instruction following and consistency, ensuring specified objects are correctly modified while unmentioned regions remain preserved. The second is Perceptual Quality (PQ), which assesses overall image quality, focusing on photorealism and the absence of artifacts. The final score is the geometric mean of the SC and PQ scores (Sf inal = (cid:112)SSC SP Q), yielding balanced and granular reward signal. 4.1.2 INFERENCE-TIME ENSEMBLING STRATEGY Recent work (Snell et al., 2024; Liu et al., 2025c) has shown that the performance of LLMs can be substantially improved by increasing the computational budget at test time. Building on this insight, we introduce an inference-time ensembling strategy for our generative reward model to systematically boost its evaluation accuracy and enhance reward stability. Our approach is straightforward: for given input triplet = (Instruction, Input Image, Output Image), we perform independent, stochastic forward passes through EditScore, and each pass generates pair (reasoningi, si). Then we aggregate only the scalar scores {s1, s2, . . . , sK} to compute the final ensembled score, Sf inal. An intuitive explanation (Liu et al., 2025c) for this effectiveness is that each of the generated reasonings can be viewed as distinct judgment perspectives. Aggregating the scores derived from these diverse perspectives allows the final reward to more accurately reflect the true quality of an edit, leading to significant scaling effectiveness. 4.2 DATA CONSTRUCTION PIPELINE The training data for reward model and subsequent reinforcement learning are constructed following the similar procedure. For reinforcement learning, the training data only requires the input images and instructions, whereas training the reward model additionally requires the generated outputs paired with their corresponding rewards. The construction process involves three steps as follows. Step I: Images selection and instructions creation. We start by selecting and filtering diverse set of high-quality images as editing inputs. Next, we construct series of reference instructions for various editing tasks. Using the input images and random selected reference instructions as guidance, we prompt Qwen-2.5-VL-72B to generate editing instructions that are task-consistent with the references. After generating large set of image-instruction pairs, we applying K-center greedy algorithm (Sener & Savarese, 2018) to select 1000 semantically diverse samples per task for reward generation. Finally, we constructed 70,000 data samples for training the reward model and 60,000 samples for reinforcement learning training. The data for the reward model requires further annotation as follows. Step II: Candidates output generation. We generate candidates output using 5 distinct editing models random selected from our model pool. 5 Table 2: Benchmark results on EditReward-Bench, reporting both overall pairwise accuracy and fine-grained breakdown across four categories of edit capabilities. Notably, EditScore achieves superior performance even with its compact 7B size. Avg@4 denotes the average score over 4 forward passes. Model GPT-4.1 GPT-5 Gemini-2.5 Metric Pro Qwen2.5-VL 32B 72B 7B EditScore-7B EditScore-32B EditScore-72B Base Avg@4 Base Avg@4 Base Avg@4 Overall Subject Appear. Scene Advanced PF PF PF PF PF 0.673 0.602 0.705 0.615 0.520 0.679 0.673 0.668 0.709 0.763 0.682 0. 0.673 0.556 0.686 0.777 0.669 0.755 0.707 0.538 0.708 0.762 0.714 0.756 0.852 0.741 0.841 0.806 0.687 0. 0.703 0.560 0.722 0.712 0.465 0.765 0.631 0.577 0.700 0.766 0.675 0.693 0.736 0.557 0.724 0.458 0.498 0.540 0.592 0.325 0.376 0.435 0.591 0.432 0.563 0.621 0. 0.414 0.527 0.523 0.590 0.317 0.414 0.394 0.585 0.460 0.570 0.594 0.740 0.422 0.393 0.390 0.573 0.335 0.320 0.416 0.612 0.470 0.514 0.559 0.663 0.433 0.611 0.690 0.744 0.236 0.482 0.429 0.735 0.357 0.673 0.713 0.789 0.541 0.524 0.627 0.536 0.367 0.351 0.488 0.503 0.410 0.553 0.657 0.529 0.722 0.720 0.727 0.691 0.666 0. 0.682 0.730 0.714 0.821 0.835 0.774 0.736 0.693 0.683 0.638 0.556 0.680 0.625 0.473 0.703 0.587 0.591 0. 0.789 0.627 0.764 0.625 0.548 0.631 0.736 0.704 0.733 0.703 0.627 0.754 0.714 0.764 0.710 0.870 0.787 0. 0.717 0.658 0.699 0.635 0.586 0.703 0.612 0.524 0.721 0.584 0.623 0.697 0.788 0.695 0.794 0.625 0.541 0. 0.755 0.735 0.763 0.708 0.639 0.807 0.733 0.778 0.736 0.908 0.797 0.837 0.734 0.733 0.721 Step III: Annotate and filtering. For each candidate output, we use GPT-4.1 to annotate scores. Following VIEScore (Ku et al., 2023a), we score each output on SC and PQ and provide reasons for the assigned scores. Next, we apply filtering to the reward samples. It is conducted from two perspectives: (i) filtering based on group-wise maximum scores to remove unachievable editing tasks, and (ii) filtering by group standard deviation to remove cases with low discriminability."
        },
        {
            "title": "5 REWARD MODEL PERFORMANCE ON EDITREWARD-BENCH",
            "content": "5.1 EXPERIMENTAL SETUP Our final EditScore model is obtained by fine-tuning Qwen2.5-VL with LoRA (Hu et al., 2022) on our curated dataset (see Section 4.2). For evaluation, we adopt the VIEScore (Ku et al., 2024) prompt template with two key modifications: (i) enforcing reasoning-before-scoring format, (ii) expanding the score range from [0, 10] to [0, 25]. Following the formulation of VIEScore, we derive Prompt Following (SP ) and Consistency (SC) from its Semantic Consistency metric SSC, while Overall Quality (O) is directly taken from the final score Sf inal. The detailed prompt templates are shown in Appendix H. 5.2 MAIN RESULTS Results summarized in Table 2 reveal significant performance gap between proprietary models and open-source counterparts. Leading proprietary models such as GPT-4.1, GPT-5 and Gemini-2.5-Pro form distinct upper tier, achieving pairwise accuracies in the 0.7-0.75 range across all dimensions. This confirms their strong, albeit imperfect, zero-shot capabilities for this nuanced task. In particular, we find that VLMs are generally stronger at assessing Prompt Following (PF) than Consistency (C), as the latter requires fine-grained comparisons between the input and output images. In stark contrast, even the largest and most capable open-source models exhibit notable limitations. The Qwen2.5-VL series shows clear scaling trend, yet the 72B-parameter variant still falls short of 0.612 overall accuracy and performs worse than random chance in Consistency judgment. The smaller 7B and 32B models fare even worse, underscoring the inadequacy of off-the-shelf opensource VLMs as reliable reward signals for fine-grained editing tasks. By contrast, our EditScore achieves substantial improvements: the 7B variant surpasses the 10x larger Qwen2.5-VL-72B, while the 72B variant matching the score of GPT-4.1. Moreover, scaling inference-time compute with self-ensemble (Avg@4) further boosts performance across all model sizes, with EditScore-72B establishing the state of the art on EditReward-Bench. Figure 2: Self-ensembling offers superior efficiency-performance trade-off compared to simply scaling model parameters. The colored solid lines show the performance scaling of our 7B, 32B, and 72B models as the number of ensemble passes (K) increases. The gray dashed line connects the single-pass (K=1) performance of these models, serving as baseline for scaling model size alone. The results clearly indicate that scaling the number of forward passes yields significantly higher accuracy gain per unit of computational cost. 5.3 EFFECTIVE INFERENCE-TIME SCALING OF EDITSCORE As shown in Table 2, both scaling model size and inference-time compute improve the performance of EditScore. To further investigate which dimension contributes more effectively, we normalize the comparison by using FLOPs as proxy for inference cost. Figure 2 presents the scaling trends of EditScore across different model sizes: colored solid lines denote inference-time compute scaling, while gray dotted lines indicate parameter scaling. We observe that, across all model sizes, scaling inference-time compute yields greater performance gains than scaling parameters. Moreover, in practical deployment, the self-ensemble strategy allows the profiling stage to be shared within the ensemble group, which improving efficiency further. 5.4 ABLATION STUDY We study two key factors in EditScore design: score range granularity and output format  (Table 3)  . Increasing the target score range generally improves pairwise accuracy for both GPT-4.1 and GPT-5, peaking around [0, 25], while overly large ranges hurt performance due to regression difficulty. In parallel, requiring the model to generate rationale before the numeric score (reasoning + score) consistently outperforms direct scoring, yielding +0.038 accuracy gain for EditScore-7B (0.621 0.659). These findings highlight that both an appropriately chosen score range and reasoning-first output are crucial for maximizing accuracy. Table 3: The effect of score range granularity and reason first on the pairwise accuracy of VLMs on EditReward-Bench. The ranges ([0, x]) indicate the target score space for model outputs. The lightly shaded column highlights our chosen configuration for optimal performance. Method GPT-4.1 GPT-5 EditScore-7B (GPT-4.1) [0, 10] 0.691 0.730 0.605 Score Range [0, 20] [0, 25] Output Format [0, 30] reasoning + score score 0.701 0.760 0.657 0.689 0.705 0.755 0. 0.629 0.705 0.755 0.659 0.695 0.741 0."
        },
        {
            "title": "6 APPLICATION OF EDITSCORE IN IMAGE EDITING",
            "content": "6.1 EXPERIMENTAL SETUP Evaluation Method. To evaluate the effectiveness of EditScore in improving image editing models, we design two experiments. (1) best-of-N selection: EditScore is used as selector, where the editing model generates multiple candidate outputs per input and EditScore chooses the best one. We evaluate the gain on three popular models: OmniGen2 (Wu et al., 2025b), Flux.1-Kontext-dev Batifol et al. (2025) and Qwen-Image-Edit Wu et al. (2025a). (2) Reinforcement learning: EditScore is employed directly as reward model to fine-tune OmniGen2 via an additional RL stage, demonstrating its utility as training signal. We adopt two widely used image editing benchmarksGEdit7 (a) (b) Figure 3: EditScore as superior reward signal for image editing. (a) Using EditScore to select the best sample among multiple outputs effectively improves VIEScore, with OmniGen2 showing the largest gain. (b) Incorporating EditScore into RL training yields stable and significant performance improvements, even surpassing the much larger Qwen2.5-VL-72B. (c) RL training benefits from self-ensembling, which enhances the evaluation accuracy of EditScore across diverse settings. (c) Bench (Liu et al., 2025b) and ImgEdit-Bench (Ye et al., 2025)which cover diverse range of practical editing tasks, to assess the improvements brought by EditScore. For efficiency, we use 7B variant of EditScore for experiments with optional self-ensemble. RL Training. OmniGen2 Wu et al. (2025b) is chosen as the base model due to its strong potential for further improvement, as evidenced in the best-of-N experiments (Section 6.2). We employ Flow-GRPO Liu et al. (2025a) with the following hyperparameters: sampling steps = 20, group size = 12, number of unique prompts = 24, noise level σ = 0.9, and KL weight β = 0.04. We reformulate the standard Flow-GRPO equation to align with OmniGen2s definition; detailed derivations are provided in Appendix and C. 6.2 VALIDATING EDITSCORE UTILITY VIA BEST-OF-N SECTION Before implementing full RL training, we first validate the utility of EditScore in controlled setting. In this experiment, the editing model generates candidate outputs per input of GEdit-Bench, and EditScore selects the best one. This setup directly measures the reward models ability to identify high-quality edits. Figure 3a reports benchmark performance as function of . We evaluate three base models using both single-pass EditScore (solid lines) and the stronger Avg@4 version (dashed lines). The results highlight three key findings: (1) stronger reward models consistently yield better selections, (2) performance gains vary by base model, with Qwen-Image-Edit showing the least improvement due to prior post-training stage (Wu et al., 2025a), and (3) OmniGen2 exhibits the largest absolute potential for improvement, motivating its choice as the base model for subsequent RL experiments. 6.3 MAIN RL RESULTS AND ANALYSIS Having established the utility of EditScore in controlled best-of-N setting and identified OmniGen2 as high-potential candidate, we now proceed to the core of our investigation: leveraging EditScore as direct reward signal for online RL fine-tuning. 6.3.1 UNLOCKING STABLE RL WITH HIGH-FIDELITY REWARD SIGNAL Our experiments reveal that successful RL fine-tuning in the image editing domain is not given; it hinges critically on the quality of the reward signal. The Decisive Role of Reward Model Fidelity. The central thesis of our work is that high-fidelity, specialized reward model is prerequisite for effective RL. Figure 3b provides stark validation of this claim. When attempting to use general-purpose, off-the-shelf VLMs (Qwen2.5-VL series) as reward providers, the training process is highly unstable and the policy model fails to converge, even with the 72B parameter model. This failure highlights crucial insight: raw VLM capabilities do not readily translate into usable reward signal. In stark contrast, our distilled EditScore provides clean, consistent, and informative reward, enabling stable and highly effective learning trajectory. Unlocking Further Gains with Self-Ensembling. Beyond the model itself, the inference-time strategy for deploying the reward model is also impactful. Figure 3c compares the learning curves 8 using single-pass EditScore versus our 4-pass self-ensemble strategy. The ensembled reward signal demonstrably leads to faster convergence and superior final policy performance. This confirms vital connection: improvements in the reward models standalone accuracy (as shown in our main results) directly translate to more efficient and effective downstream RL training."
        },
        {
            "title": "6.4 ANALYSIS OF ANNOTATION SOURCES AND REWARD VARIANCE",
            "content": "Table 4: Comparison of reward models based on annotator source. We analyze reward models (RMs) trained on data from two different annotators: GPT-4.1 and GPT-5. The resulting policys performance is evaluated by both annotators on GEdit-Bench (Liu et al., 2025b). Annotator Component Reward Model Metrics Policy Performance Acc. (O) Score Std. GPT-4.1 Score GPT-5 Score GPT-4.1 (Better for RL) GPT-5 (Worse for RL) Annotator (Itself) (cid:44) EditScore (RM) (cid:44) OmniGen2 (Policy) Annotator (Itself) (cid:44) EditScore (RM) (cid:44) OmniGen2 (Policy) 0.705 0.637 0.755 0.638 3.309 2.868 2.942 2.533 6.375 6.292 5.834 5.768 To understand the role of annotators in shaping RL performance, we trained two distinct EditScore7B reward models on identical data subsets, annotated by GPT-4.1 and GPT-5 separately. While GPT-5 offers superior annotation accuracy, the higher score variance of GPT-4.1 provides more effective learning signal. As shown in Table 4, EditScore trained on GPT-4.1s labels, despite marginal dip in accuracy (0.637 vs. 0.638), inherits substantially higher score standard deviation (2.868 vs. 2.533). This high-variance reward signal leads to demonstrably better policy after RL fine-tuning. Crucially, this improvement is consistent between both GPT-4.1 (6.375 vs. 6.292) and GPT-5 (5.834 vs. 5.768) evaluations, excluding potential evaluation bias from GEdit-Bench (Ye et al., 2025). Our results thus uncover key insight: reward signal variance, rather than absolute annotator accuracy, can be the dominant factor for successful optimization of the RL-based model, corroborating similar findings from Razin et al. (2025). 6.4.1 QUANTIFYING THE FINAL PERFORMANCE GAINS Table 5: Performance comparison of models with RL enhancement, where our EditScore provides substantial gains for OmniGen2 across two widely adopted benchmarks. Model UniPic2-SD3.5M-Kontext UniPic2-SD3.5M-Kontext RL w/ GPT-4.1 OmniGen2 OmniGen2 RL w/ EditScore GEdit-Bench-EN PQ ImgEdit-Bench 6.31 6.55 (+0.24) 3.95 4.04 (+0.09 ) SC 6.72 7.20 7.20 (+0.48) 7.46 (+0.26) 6.68 (+0.40) 6.28 3.40 3.63 (+0.23) Table 5 summarizes the substantial gains achieved by OmniGen2 after RL fine-tuning with EditScore. We also provide visualization of the results before and after RL in the Appendix 5. On GEdit-Bench-EN, RL raises the overall score from 6.28 to 6.68, notable improvement of +0.40, with particularly strong gains in Semantic Control (+0.48). On the more challenging ImgEdit-Bench, we also observe meaningful increase of +0.23. For comparison, we include UniPic2 with EditReward, showing that the improvements achieved by EditScore are both competitive and substantial. These results provide strong evidence that high-fidelity reward model such as EditScore is essential for unlocking the full potential of online RL in image editing."
        },
        {
            "title": "7 CONCLUSION",
            "content": "In this work, we addressed the critical bottleneck for RL in image editing: the lack of reliable reward signal. We established comprehensive benchmark for reward model evaluation. Guided by 9 our benchmark, we developed EditScore, family of specialized generative reward models that deliver robust, high-fidelity signal. Crucially, EditScore is not only efficient for Best-of-N selection but is the key to unlocking stable online RL where previous open-source models fail. By releasing both EditReward-Bench and EditScore, we provide foundational toolkit for future research into RL-based image editing and more nuanced reward modeling."
        },
        {
            "title": "REFERENCES",
            "content": "Saba Ahmadi, Rabiul Awal, Ankur Sikarwar, Amirhossein Kazemnejad, Ge Ya Luo, Juan Rodriguez, Sai Rajeswar, Siva Reddy, Christopher Pal, Benno Krojer, et al. The promise of rl for autoregressive image editing. arXiv preprint arXiv:2508.01119, 2025. Michael S. Albergo, Nicholas M. Boffi, and Eric Vanden-Eijnden. Stochastic interpolants: unifying framework for flows and diffusions. CoRR, abs/2303.08797, 2023. doi: 10.48550/ARXIV. 2303.08797. URL https://doi.org/10.48550/arXiv.2303.08797. Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. Stephen Batifol, Andreas Blattmann, Frederic Boesel, Saksham Consul, Cyril Diagne, Tim Dockhorn, Jack English, Zion English, Patrick Esser, Sumith Kulal, et al. Flux. 1 kontext: Flow matching for in-context image generation and editing in latent space. arXiv e-prints, pp. arXiv2506, 2025. Tim Brooks, Aleksander Holynski, and Alexei Efros. Instructpix2pix: Learning to follow image editing instructions. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1839218402, 2023. Sherry Chen, Yi Wei, Luowei Zhou, and Suren Kumar. Adiee: Automatic dataset creation and scorer for instruction-guided image editing evaluation. arXiv preprint arXiv:2507.07317, 2025. Chaorui Deng, Deyao Zhu, Kunchang Li, Chenhui Gou, Feng Li, Zeyu Wang, Shu Zhong, Weihao Yu, Xiaonan Nie, Ziang Song, et al. Emerging properties in unified multimodal pretraining. arXiv preprint arXiv:2505.14683, 2025. Yu Gao, Lixue Gong, Qiushan Guo, Xiaoxia Hou, Zhichao Lai, Fanshi Li, Liang Li, Xiaochen Lian, Chao Liao, Liyang Liu, et al. Seedream 3.0 technical report. arXiv preprint arXiv:2504.11346, 2025. Yuan Gong, Xionghui Wang, Jie Wu, Shiyin Wang, Yitong Wang, and Xinglong Wu. Onereward: Unified mask-guided image generation via multi-task human preference learning. arXiv preprint arXiv:2508.21066, 2025. Google. Introducing gemini 2.5 flash image. https://developers.googleblog.com/ en/introducing-gemini-2-5-flash-image/, 2025. Accessed: 2025-09-18. Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Prompt-to-prompt image editing with cross attention control. arXiv preprint arXiv:2208.01626, 2022. Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. ICLR, 1(2):3, 2022. Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. Dongfu Jiang, Max Ku, Tianle Li, Yuansheng Ni, Shizhuo Sun, Rongqi Fan, and Wenhu Chen. Genai arena: An open evaluation platform for generative models. Advances in Neural Information Processing Systems, 37:7988979908, 2024. 10 Yuval Kirstain, Adam Polyak, Uriel Singer, Shahbuland Matiana, Joe Penna, and Omer Levy. Picka-pic: An open dataset of user preferences for text-to-image generation. Advances in neural information processing systems, 36:3665236663, 2023. Max Ku, Dongfu Jiang, Cong Wei, Xiang Yue, and Wenhu Chen. Viescore: Towards explainable metrics for conditional image synthesis evaluation. arXiv preprint arXiv:2312.14867, 2023a. Max Ku, Tianle Li, Kai Zhang, Yujie Lu, Xingyu Fu, Wenwen Zhuang, and Wenhu Chen. Imagenhub: Standardizing the evaluation of conditional image generation models. arXiv preprint arXiv:2310.01596, 2023b. Max Ku, Dongfu Jiang, Cong Wei, Xiang Yue, and Wenhu Chen. Viescore: Towards explainable metrics for conditional image synthesis evaluation. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024, pp. 1226812290. Association for Computational Linguistics, 2024. doi: 10.18653/V1/2024. ACL-LONG.663. URL https://doi.org/10.18653/v1/2024.acl-long.663. Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pretraining for unified vision-language understanding and generation. In International conference on machine learning, pp. 1288812900. PMLR, 2022. Yuxi Li. Deep reinforcement learning: An overview. arXiv preprint arXiv:1701.07274, 2017. Yaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maximilian Nickel, and Matthew Le. Flow In The Eleventh International Conference on Learning matching for generative modeling. Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023. URL https://openreview.net/forum?id=PqvMRDCJT9t. Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llavanext: Improved reasoning, ocr, and world knowledge, 2024. Jie Liu, Gongye Liu, Jiajun Liang, Yangguang Li, Jiaheng Liu, Xintao Wang, Pengfei Wan, Di Zhang, and Wanli Ouyang. Flow-grpo: Training flow matching models via online rl. arXiv preprint arXiv:2505.05470, 2025a. Shiyu Liu, Yucheng Han, Peng Xing, Fukun Yin, Rui Wang, Wei Cheng, Jiaqi Liao, Yingming Wang, Honghao Fu, Chunrui Han, et al. Step1x-edit: practical framework for general image editing. arXiv preprint arXiv:2504.17761, 2025b. Zijun Liu, Peiyi Wang, Runxin Xu, Shirong Ma, Chong Ruan, Peng Li, Yang Liu, and Yu Wu. Inference-time scaling for generalist reward modeling, 2025c. URL https://arxiv.org/ abs/2504.02495. Zijun Liu, Peiyi Wang, Runxin Xu, Shirong Ma, Chong Ruan, Peng Li, Yang Liu, and Yu Wu. Inference-time scaling for generalist reward modeling. arXiv preprint arXiv:2504.02495, 2025d. Yuhang Ma, Xiaoshi Wu, Keqiang Sun, and Hongsheng Li. Hpsv3: Towards wide-spectrum human preference score. arXiv preprint arXiv:2508.03789, 2025. OpenAI. Gpt-5 is here. https://openai.com/gpt-5/, 2025. Accessed: 2025-09-18. Noam Razin, Zixuan Wang, Hubert Strauss, Stanley Wei, Jason Lee, and Sanjeev Arora. an optimization perspective. arXiv preprint What makes reward model good teacher? arXiv:2503.15477, 2025. Yuxi Ren, Jie Wu, Yanzuo Lu, Huafeng Kuang, Xin Xia, Xionghui Wang, Qianqian Wang, Yixing Zhu, Pan Xie, Shiyin Wang, et al. Byteedit: Boost, comply and accelerate generative image editing. In European Conference on Computer Vision, pp. 184200. Springer, 2024. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. CoRR, abs/1707.06347, 2017. URL http://arxiv.org/abs/ 1707.06347. 11 Ozan Sener and Silvio Savarese. Active learning for convolutional neural networks: core-set approach. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net, 2018. URL https://openreview.net/forum?id=H1aIuk-RW. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. CoRR, abs/2402.03300, 2024. doi: 10.48550/ARXIV.2402.03300. URL https://doi.org/10.48550/arXiv.2402.03300. Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling llm test-time compute optimally can be more effective than scaling model parameters, 2024. URL https://arxiv.org/ abs/2408.03314. Yang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021. URL https://openreview.net/forum?id= PxTIG12RRHS. Yibin Wang, Zhimin Li, Yuhang Zang, Chunyu Wang, Qinglin Lu, Cheng Jin, and Jiaqi Wang. Unified multimodal chain-of-thought reward model through reinforcement fine-tuning. CoRR, abs/2505.03318, 2025a. doi: 10.48550/ARXIV.2505.03318. URL https://doi.org/10. 48550/arXiv.2505.03318. Yibin Wang, Yuhang Zang, Hao Li, Cheng Jin, and Jiaqi Wang. Unified reward model for multimodal understanding and generation. arXiv preprint arXiv:2503.05236, 2025b. Hongyang Wei, Baixin Xu, Hongbo Liu, Cyrus Wu, Jie Liu, Yi Peng, Peiyu Wang, Zexiang Liu, Jingwen He, Yidan Xietian, et al. Skywork unipic 2.0: Building kontext model with online rl for unified multimodal model. arXiv preprint arXiv:2509.04548, 2025. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models. In Sanmi Koyejo, S. Mohamed, A. Agarwal, Danielle Belgrave, K. Cho, and A. Oh (eds.), Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022, 2022. URL http://papers.nips.cc/paper_files/paper/2022/ hash/9d5609613524ecf4f15af0f7b31abca4-Abstract-Conference.html. Chenfei Wu, Jiahao Li, Jingren Zhou, Junyang Lin, Kaiyuan Gao, Kun Yan, Sheng-ming Yin, Shuai Bai, Xiao Xu, Yilei Chen, et al. Qwen-image technical report. arXiv preprint arXiv:2508.02324, 2025a. Chenyuan Wu, Pengfei Zheng, Ruiran Yan, Shitao Xiao, Xin Luo, Yueze Wang, Wanli Li, Xiyan Jiang, Yexin Liu, Junjie Zhou, et al. Omnigen2: Exploration to advanced multimodal generation. arXiv preprint arXiv:2506.18871, 2025b. Jie Wu, Yu Gao, Zilyu Ye, Ming Li, Liang Li, Hanzhong Guo, Jie Liu, Zeyue Xue, Xiaoxia Hou, Wei Liu, Yan Zeng, and Weilin Huang. Rewarddance: Reward scaling in visual generation, 2025c. URL https://arxiv.org/abs/2509.08826. Xiaoshi Wu, Yiming Hao, Keqiang Sun, Yixiong Chen, Feng Zhu, Rui Zhao, and Hongsheng Li. Human preference score v2: solid benchmark for evaluating human preferences of text-toimage synthesis. arXiv preprint arXiv:2306.09341, 2023a. Xiaoshi Wu, Keqiang Sun, Feng Zhu, Rui Zhao, and Hongsheng Li. Human preference score: Better aligning text-to-image models with human preference. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 20962105, 2023b. Shitao Xiao, Yueze Wang, Junjie Zhou, Huaying Yuan, Xingrun Xing, Ruiran Yan, Chaofan Li, Shuting Wang, Tiejun Huang, and Zheng Liu. Omnigen: Unified image generation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 1329413304, 2025. Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding, Jie Tang, and Yuxiao Dong. Imagereward: Learning and evaluating human preferences for text-to-image generation. Advances in Neural Information Processing Systems, 36:1590315935, 2023. Zeyue Xue, Jie Wu, Yu Gao, Fangyuan Kong, Lingting Zhu, Mengzhao Chen, Zhiheng Liu, Wei Liu, Qiushan Guo, Weilin Huang, et al. Dancegrpo: Unleashing grpo on visual generation. arXiv preprint arXiv:2505.07818, 2025. Yang Ye, Xianyi He, Zongjian Li, Bin Lin, Shenghai Yuan, Zhiyuan Yan, Bohan Hou, and Li Yuan. Imgedit: unified image editing dataset and benchmark. arXiv preprint arXiv:2505.20275, 2025. Kai Zhang, Lingbo Mo, Wenhu Chen, Huan Sun, and Yu Su. Magicbrush: manually annotated dataset for instruction-guided image editing. Advances in Neural Information Processing Systems, 36:3142831449, 2023. Shu Zhang, Xinyi Yang, Yihao Feng, Can Qin, Chia-Chih Chen, Ning Yu, Zeyuan Chen, Huan Wang, Silvio Savarese, Stefano Ermon, et al. Hive: Harnessing human feedback for instructional In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern visual editing. Recognition, pp. 90269036, 2024a. Sixian Zhang, Bohan Wang, Junqiang Wu, Yan Li, Tingting Gao, Di Zhang, and Zhongyuan Wang. Learning multi-dimensional human preference for text-to-image generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 80188027, 2024b. Yaowei Zheng, Richong Zhang, Junhao Zhang, Yanhan Ye, Zheyan Luo, Zhangchi Feng, and Yongqiang Ma. Llamafactory: Unified efficient fine-tuning of 100+ language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations), Bangkok, Thailand, 2024. Association for Computational Linguistics. URL http://arxiv.org/abs/2403.13372. STATEMENT ON THE USE OF LARGE LANGUAGE MODELS (LLMS) In adherence to the ICLR 2026 submission guidelines, this section details the use of Large Language Model (LLM) assistant during the preparation of this manuscript. The LLM, acting as research and writing co-pilot, played significant role in refining the manuscripts structure, language, and presentation. The authors maintained full intellectual control throughout the process and take complete responsibility for all content. The precise role of the LLM can be categorized as follows: Manuscript Writing and Polishing: The authors wrote the initial drafts for all sections of the paper, providing the key technical details, experimental results, and core arguments. The LLM was then used extensively as an interactive writing assistant to: Enhance Clarity and Conciseness: Rephrasing long or complex sentences to improve readability and flow. Improve Academic Tone: Suggesting more formal and professional vocabulary and sentence structures appropriate for top-tier conference submission. Correct Grammar and Syntax: Performing comprehensive proofreading to identify and correct grammatical errors, typos, and awkward phrasing. Suggest Alternative Phrasing: Providing multiple options for expressing single idea to avoid repetitive language). Technical Formalization: For complex mathematical sections, such as the derivation of the SDE from the ODE in Appendix B, the authors provided the core mathematical steps and handwritten notes. The LLM assisted in translating these steps into clear, wellstructured narrative and formatting them professionally in LaTeX. The LLM did not generate novel mathematical proofs but rather helped in their presentation. Throughout this collaborative process, every suggestion and piece of text generated by the LLM was critically reviewed, edited, and approved by the human authors. The authors are solely responsible for the scientific validity, originality, and all claims made in this paper. The LLM is not considered an author."
        },
        {
            "title": "B DERIVATION OF THE SDE FORMULATION FOR FLOW MATCHING",
            "content": "This appendix derives the stochastic differential equation (SDE) underlying policy sampling in our Flow Matching model. Unlike prior work such as Flow-GRPO (Liu et al., 2025a), our base model OmniGen2 defines the probability path from noise at = 0 (x0) to data at = 1 (x1), reversing the convention in some other studies. This leads to different drift term in the resulting SDE. The derivation proceeds in three steps: (i) connecting the deterministic ODE and general SDE via the FokkerPlanck equation, (ii) relating the score term to the learned vector field, and (iii) constructing the discretized transition kernel. B.1 CONNECTING THE PROBABILITY FLOW ODE TO AN SDE Our Flow Matching model (Lipman et al., 2023) is trained to approximate the vector field vt(xt) of probability flow ODE, which deterministically transports samples from noise distribution p0 to data distribution p1:"
        },
        {
            "title": "We wish to find an equivalent SDE of the general form",
            "content": "dxt = vt(xt)dt. dxt = (xt, t)dt + G(t)dWt, (1) (2) that generates the same marginal probability density path pt(x) for all [0, 1]. Here, is the drift term, G(t) is the diffusion coefficient (we assume it is state-independent), and Wt is standard Wiener process. The evolution of the probability density pt(x) is described by the Fokker-Planck equation (Song et al., 2021). For the deterministic ODE in equation 1, it is: For the SDE in equation 2, the Fokker-Planck equation is: pt(x) = [vt(x)pt(x)] . pt(x) = [f (x, t)pt(x)] + 2 (cid:2)G(t)2pt(x)(cid:3) . 1 2 (3) (4) For the two processes to be equivalent (i.e., to share the same pt(x)), the right-hand sides of equation 3 and equation 4 must be equal. This allows us to solve for the SDE drift term (x, t): [vtpt] = [f pt] + [vtpt] = pt (cid:20) 1 2 1 2 (cid:2)(G2pt)(cid:3) (cid:21) G2pt vtpt = pt 1 2 G2pt. Dividing by pt and rearranging yields the expression for the drift: (xt, t) = vt(xt) + G(t)2 log pt(xt). Substituting this back into equation 2, we obtain the equivalent SDE: (cid:20) dxt = vt(xt) + G(t)2 (cid:21) log pt(xt) dt + G(t)dWt. (5) (6) (7) (8) (9) We define that noise at = 0 (x0) and ending with the data sample at = 1 (x1), so equation 9 dont need reverse SDE. This SDEs drift consists of the original ODE vector field plus term proportional to the score function, log pt(xt). B.2 EXPRESSING THE SCORE VIA THE LEARNED VECTOR FIELD The SDE in equation 9 is not yet practical for sampling, as the score function log pt(xt) is unknown. However, for the specific probability path used in Flow Matching, this score can be expressed entirely in terms of the known vector field vt(xt), which is approximated by our trained model vθ.The derivation follows similar approach to that for stochastic interpolants (Albergo et al., 2023). Our derivation begins with the linear interpolation path: xt = (1 t)x0 + tx1, where x0 (0, I) and x1 is data sample. The velocity of this path is defined as the conditional expectation of the instantaneous change: vt(xt) = (cid:21) (cid:20) dxt dt (cid:12) (cid:12) (cid:12)xt ."
        },
        {
            "title": "Since the time derivative is dxt",
            "content": "dt = x1 x0, the vector field becomes: vt(xt) = E[x1xt] E[x0xt]. (10) (11) key identity, derived from Tweedies formula in score-based modeling, connects the marginal score log pt(xt) to the posterior mean of the initial noise sample x0: log pt(xt) = E[x0xt] 1 . (12) Our goal is thus to express E[x0xt] using vt(xt). To do this, we take the conditional expectation of the path definition itself: E[xtxt] = xt = (1 t)E[x0xt] + tE[x1xt]. (13) We now have system of two linear equations ( equation 11 and equation 13) with two unknowns (E[x0xt] and E[x1xt]). We can solve this system for E[x0xt]. Rearranging equation 11 gives E[x1xt] = vt(xt) + E[x0xt]. Substituting this into equation 13: xt = (1 t)E[x0xt] + t(vt(xt) + E[x0xt]) xt = (1 + t)E[x0xt] + tvt(xt) xt = E[x0xt] + tvt(xt). This gives us the desired expression: E[x0xt] = xt tvt(xt). (14) (15) Finally, substituting equation 15 back into the score identity ( equation 12), we arrive at the final, practical expression for the score function: log pt(xt) = xt tvt(xt) 1 . (16) This result allows us to compute the score at any point (xt, t) using only the output of our trained vector field model vθ(xt, t). B.3 FINAL SDE AND DISCRETIZED TRANSITION PROBABILITY We now substitute the practical expression for the score equation 16 into the SDE from equation 9. For simplicity, lets assume constant diffusion G(t) = σ: (cid:20) vt(xt) dxt = σ2 2 xt tvt(xt) 1 (cid:21) dt + σdWt. (17) This is our final SDE, where the drift is expressed entirely in terms of the learned vector field vt (approximated by vθ). Unlike some prior work that requires deriving reverse-time SDE, we can directly use this forward-time SDE for model sampling. To implement this for our RL policy, we discretize equation 17 using the Euler-Maruyama scheme with step size t: xt+t = xt + (cid:20) vθ(xt, t) σ2 2 xt tvθ(xt, t) 1 (cid:21) + σ ϵ, (18) where ϵ (0, I). 15 This defines the stochastic transition probability of our policy, πθ(xt+txt, c). It is Gaussian distribution: with mean and covariance given by: πθ(xt+txt, c) = (xt+t; µ(xt, t), Σ), µ(xt, t) = xt + (cid:20) vθ(xt, t) σ2 2 xt tvθ(xt, t) 1 (cid:21) Σ = σ2t I. (19) (20) (21) This completes the derivation from the deterministic ODE to concrete, sampleable stochastic policy for reinforcement learning."
        },
        {
            "title": "C GRPO ON SDE FLOW MATCHING",
            "content": "With the stochastic policy πθ defined by the SDE in Appendix B, we optimize our generative model using GRPO (Shao et al., 2024). GRPO is efficient online algorithm and lightweight than PPO (Schulman et al., 2017), well-suited for large generative models. The optimization process unfolds over trajectories generated by our SDE-based policy. For each input condition c, we generate group of trajectories. Each trajectory consists of discrete steps, obtained by iteratively applying the Euler-Maruyama update from equation 18 to produce final image xi 1. Our EditScore model then assigns terminal reward ri to each resulting image. The GRPO objective maximizes clipped surrogate function over each step of the trajectories: JGRPO(θ) = ˆEt (cid:104) min (cid:16) ρt(θ) ˆAt, clip(ρt(θ), 1 ϵ, 1 + ϵ) ˆAt (cid:17)(cid:105) , (22) where ϵ is clipping hyperparameter, and the advantage ˆAt is computed based on the terminal rewards within the group. For the final step of the trajectory, the advantage is calculated as: ˆAi = ri mean({r1, . . . , rG}) std({r1, . . . , rG}) , (23) For intermediate timesteps (t < ), the advantages are typically computed using Generalized Advantage Estimation (GAE). Critically, in our SDE framework, the importance ratio ρt(θ) is the ratio of the single-step state transition probabilities defined in equation 19, under the current policy πθ and the old policy πθold that generated the data: ρt(θ) = πθ(xt+txt, c) πθold(xt+txt, c) . (24)"
        },
        {
            "title": "D IMAGES CATEGORIES",
            "content": "Figure 4 illustrates the distribution of input image categories in our editing dataset. Figure 4: Input images categories."
        },
        {
            "title": "E QUALITATIVE RESULTS",
            "content": "In this section, we present qualitative results of OmniGen2 after reinforcement learning. The Figure 5 shows examples across several image editing tasks. These results validate the improvement of editing outcomes achieved by RL training on editing models."
        },
        {
            "title": "F DATA ANNOTATION USER INTERFACE",
            "content": "To ensure the collection of high-quality, fine-grained preference data for EditRewardBench, we developed specialized web-based annotation interface. This section provides visual overview of the interface and its key features, designed to facilitate our multi-dimensional, tiered ranking protocol. The two main components of the interface are shown in Figure 6. As illustrated in Figure 6, the interface presents expert raters with all necessary information for single annotation task. The design is centered around two core principles of our methodology: Multi-Dimensional Evaluation: The lower panel requires annotators to provide three independent rankings for Instruction Following, Consistency, and Overall Quality. This decomposed approach, clearly separated in the UI, ensures that each aspect of the edit is evaluated independently, preventing issues where, for example, visually pleasing but semantically incorrect edit might be unfairly favored. Tiered Ranking Support: key feature of our design is the direct implementation of tiered ranking. As shown in the lower panels input fields, annotators are not forced into strict linear order. Instead, they can group multiple images of perceived equal quality into the same tier using pipe () separator. For instance, ranking of 14523 indicates that output 1 is in the top tier, outputs 4, 5, and 2 are tied in middle tier, and output 3 is in the lowest tier. This method more accurately captures nuanced human judgments and allows for the efficient generation of dense graph of preference pairs from single annotation. The interface also includes progress tracking and quality control features, ensuring smooth and reliable annotation workflow for our expert raters."
        },
        {
            "title": "G DETAILS ON THE EXPERIMENTAL SETUP",
            "content": "This appendix provides detailed specifications for the training of our reward model, EditScore, and for the reinforcement learning of our policy model, OmniGen2-Edit. G.1 REWARD MODEL TRAINING (EDITSCORE) Hyperparameters. Our reward models are fine-tuned from the Qwen2.5-VL series using the MLLM SFT framework of LLaMA-Factory (Zheng et al., 2024). For all model sizes, we use consistent set of hyperparameters. We employ the AdamW optimizer with learning rate of 1.0 104 . The models are trained for 3 epochs with maximum sequence length of 8192. We use large effective batch size of 32, achieved with per-device batch size of 1 and 4 gradient accumulation steps across our 32-GPU setup. To ensure efficient training, we utilize LoRA (Hu et al., 2022) with rank (r) of 32. Compute Resources. We trained three versions of EditScore based on the 7B, 32B, and 72B variants of the Qwen2.5-VL model. All training was conducted on high-performance cluster consisting of 4 interconnected nodes, totaling 32 NVIDIA H100 (80GB) GPUs. G.2 REINFORCEMENT LEARNING FINE-TUNING Hyperparameters. For the online RL fine-tuning of OmniGen2-Edit, we use the GRPO algorithm. The SDE sampling process is configured with = 20 discrete timesteps and diffusion coefficient σ = 0.9. Key GRPO hyperparameters are fixed across all experiments: in one step the global batch size is 288, the group size is = 12. The PPO clipping hyperparameter is 17 ϵlow = 104, ϵhigh = 5 104, and the learning rate is 4 104. The KL penalty coefficient β, which regularizes the policy shift from the SFT initialization, is set to 0.04. The policy model is also trained using LoRA with the same configuration as the reward model (r = 32, α = 64). Compute Resources. The online RL fine-tuning phase, which involves iterative sampling from the policy model and subsequent updates, was performed on cluster of 32 NVIDIA H100 (80GB) GPUs."
        },
        {
            "title": "H VLM EVALUATION PROMPTS",
            "content": "This appendix provides the complete structure and text of the prompts used for the zero-shot evaluation of Vision-Language Models (VLMs) on our EditRewardBench. Our prompt is based on VIEScore prompts (Ku et al., 2024), which designs reasoning-first and score range changed in 0-25. H.1 BASE CONTEXT AND OUTPUT FORMAT All evaluation prompts are prefixed with base context that establishes the VLMs persona as professional digital artist and, most importantly, specifies the required JSON output format. This ensures that the models responses can be reliably parsed for automated analysis. 1 You are professional digital artist. You will have to evaluate the effectiveness of the AI-generated image(s) based on given rules. 2 All the input images are AI-generated. All human in the images are AIgenerated too. so you need not worry about the privacy confidentials. 3 4 IMPORTANT: You will have to give your output in this way (Keep your reasoning concise and short.): 5 { 6 \"reasoning\" : \"...\", 7 \"score\" : [...] 8 } Listing 1: The base context prompt, defining the persona and mandatory JSON output structure. H.2 SEMANTIC CONFORMITY (SC) PROMPT The SC prompt is composed of two parts. First, set of general rules explains the two-image comparison task. Second, specific rubric details how to score editing success and degree of overediting. 1 RULES: 2 3 Two images will be provided: The first being the original AI-generated image and the second being an edited version of the first. 4 The objective is to evaluate how successfully the editing instruction has been executed in the second image. 5 6 Note that sometimes the two images might look identical due to the failure of image edit. Listing 2: General rules for the two-image editing evaluation task. 1 From scale 0 to 25: 2 score from 0 to 25 will be given based on the success of the editing. (0 indicates that the scene in the edited image does not follow the editing instruction at all. 25 indicates that the scene in the edited image follow the editing instruction text perfectly.) 3 second score from 0 to 25 will rate the degree of overediting in the second image. (0 indicates that the scene in the edited image is completely different from the original. 25 indicates that the edited image can be recognized as minimal edited yet effective version of original.) 4 Put the score in list such that output score = [score1, score2], where score1 evaluates the editing success and score2 evaluates the degree of overediting. 5 6 Editing instruction: <instruction> Listing 3: Specific scoring rubric for Semantic Conformity (SC). H.3 PERCEPTUAL QUALITY (PQ) PROMPT The PQ prompt is self-contained, providing both the general rules for single-image quality assessment and the specific rubric for scoring naturalness and artifacts. 1 RULES: 2 3 The image is an AI-generated image. 4 The objective is to evaluate how successfully the image has been generated. 5 6 From scale 0 to 25: 7 score from 0 to 25 will be given based on image naturalness. 8 ( 9 0 indicates that the scene in the image does not look natural at all or give unnatural feeling such as wrong sense of distance, or wrong shadow, or wrong lighting. 25 indicates that the image looks natural. 10 11 ) 12 second score from 0 to 25 will rate the image artifacts. 13 ( 14 0 indicates that the image contains large portion of distortion, or watermark, or scratches, or blurred faces, or unusual body parts, or subjects not harmonized. 25 indicates the image has no artifacts. 15 16 ) 17 Put the score in list such that output score = [naturalness, artifacts] Listing 4: Rules and scoring rubric for Perceptual Quality (PQ). The final prompts sent to the VLM are constructed by concatenating the base context (Listing 1) with the respective rule and rubric sets for the SC (Listings 2 and 3) and PQ (Listing 4) evaluations. This modular design allows for clear and targeted assessment of each evaluation dimension. Figure 5: Qualitative results on image editing task. 20 Figure 6: Screenshots of our custom-built annotation interface for EditRewardBench. (Top) The upper panel presents the expert rater with the visual stimuli: the original input image and set of five candidate edited outputs from various models. (Bottom) The lower panel contains the interactive components. It displays the user instruction (Task Description) and provides three separate input fields for our core evaluation dimensions: Instruction Following, Consistency, and Overall Quality. This panel clearly shows the tiered ranking mechanism, where the pipe symbol () is used to group images of similar quality."
        }
    ],
    "affiliations": [
        "Beijing Academy of Artificial Intelligence",
        "Institute of Automation, Chinese Academy of Sciences",
        "University of Science and Technology of China",
        "Zhejiang University"
    ]
}