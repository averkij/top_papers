{
    "paper_title": "Internal Flow Signatures for Self-Checking and Refinement in LLMs",
    "authors": [
        "Sungheon Jeong",
        "Sanggeon Yun",
        "Ryozo Masukawa",
        "Wenjun Haung",
        "Hanning Chen",
        "Mohsen Imani"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large language models can generate fluent answers that are unfaithful to the provided context, while many safeguards rely on external verification or a separate judge after generation. We introduce \\emph{internal flow signatures} that audit decision formation from depthwise dynamics at a fixed inter-block monitoring boundary. The method stabilizes token-wise motion via bias-centered monitoring, then summarizes trajectories in compact \\emph{moving} readout-aligned subspaces constructed from the top token and its close competitors within each depth window. Neighboring window frames are aligned by an orthogonal transport, yielding depth-comparable transported step lengths, turning angles, and subspace drift summaries that are invariant to within-window basis choices. A lightweight GRU validator trained on these signatures performs self-checking without modifying the base model. Beyond detection, the validator localizes a culprit depth event and enables a targeted refinement: the model rolls back to the culprit token and clamps an abnormal transported step at the identified block while preserving the orthogonal residual. The resulting pipeline provides actionable localization and low-overhead self-checking from internal decision dynamics. \\emph{Code is available at} \\texttt{github.com/EavnJeong/Internal-Flow-Signatures-for-Self-Checking-and-Refinement-in-LLMs}."
        },
        {
            "title": "Start",
            "content": "Internal Flow Signatures for Self-Checking and Refinement in LLMs Sungheon Jeong 1 Sanggeon Yun 1 Ryozo Masukawa 1 Wenjun Haung 1 Hanning Chen 1 Mohsen Imani"
        },
        {
            "title": "Abstract",
            "content": "Large language models can generate fluent answers that are unfaithful to the provided context, while many safeguards rely on external verification or separate judge after generation. We introduce internal flow signatures that audit decision formation from depthwise dynamics at fixed inter-block monitoring boundary. The method stabilizes token-wise motion via biascentered monitoring, then summarizes trajectories in compact moving readout-aligned subspaces constructed from the top token and its close competitors within each depth window. Neighboring window frames are aligned by an orthogonal transport, yielding depth-comparable transported step lengths, turning angles, and subspace drift summaries that are invariant to within-window basis choices. lightweight GRU validator trained on these signatures performs self-checking without modifying the base model. Beyond detection, the validator localizes culprit depth event and enables targeted refinement: the model rolls back to the culprit token and clamps an abnormal transported step at the identified block while preserving the orthogonal residual. The resulting pipeline provides actionable localization and low-overhead self-checking from internal decision dynamics. Code is available on GitHub. 1. Introduction Large language models (LLMs) often produce fluent answers that are locally consistent yet globally incorrect (Huang et al., 2023; Maynez et al., 2020; Lin et al., 2022; Li et al., 2023). In practical deployments, user typically needs to know not only whether an answer is wrong, but also whether the model was internally confident for the right reasons (Geng et al., 2024). Current safeguards lean on external verification, retrieval, or an additional LLM judge 1Department of Computer Science, University of CalSungheon Jeong Correspondence to: Irvine. ifornia, <sungheoj@uci.edu>. Preprint. February 2, 2026. 1 (Nakano et al., 2021; Lewis et al., 2020; Zheng et al., 2023). These approaches add latency and cost, and they react only after the full generation is produced, while the internal process that formed the decision remains largely unobserved (Gao et al., 2023; Kossen et al., 2024; Farquhar et al., 2024). We take different view of inference. Rather than treating an autoregressive transformer as single input-to-output mapping, we interpret generation as depthwise flow of internal states and intermediate readouts (Belrose et al., 2023; Ferrando et al., 2023; Pal et al., 2023). key lens is how logit competition evolves across depth (Belrose et al., 2023; Chuang et al., 2023): as token decision forms, the top token and its close competitors shift in structured ways (Ferrando et al., 2023; Pal et al., 2023). This yields measurable trace of decision formation that supports internal audits and lightweight self-checking (Elhage et al., 2021; Azaria & Mitchell, 2023; Chen et al., 2024a; Ji et al., 2024; Sriramanan et al., 2024). Unlike Logit lens (Elhage et al., 2021), which inspects what layer encodes, we track how the decision trajectory is shaped across depth, enabling localization of atypical motion to specific internal updates. Turning this intuition into reliable measurements faces two obstacles. First, any fixed global coordinate system can be misleading, since readout-relevant directions and what is linearly decodable can shift with depth (Belrose et al., 2023; Tenney et al., 2019; Ethayarajh, 2019). Second, the monitored boundary typically includes normalization with learned gain and bias, and its placement affects depthwise behavior, introducing depth dependent offsets that can contaminate token-wise motion and complicate depth aggregation (Ba et al., 2016; Xiong et al., 2020). Without resolving these issues, depthwise signatures become unstable and difficult to compare across layers, prompts, or models. This can prevent module level attribution of where the trajectory gets steered off course during generation. We address these obstacles by stabilizing depthwise measurements in local moving frame. We monitor the residual stream at fixed boundary and remove depth dependent token shared offsets, so token motion reflects decision dynamics rather than layer specific shifts. Within each depth window, we build compact readout aligned frame from the top token and close competitors, then align neighboring frames orthogonally so step size, turning, and drift remain Submission and Formatting Instructions for ICML 2026 comparable across depth. These signatures enable self checking by training lightweight validator on geometric flow patterns in depthwise decision formation. From flow features extracted during generation, the validator separates reliable and unreliable regimes by detecting atypical transported step length, turning, and subspace drift in moving readout aligned frame, without modifying the base LLM. Across tasks and models, this supervision is stable and learnable, yielding consistent separation between non hallucination and hallucination like behaviors with minimal overhead. Beyond detection, the same signal localizes single depth localized culprit event. We roll back to the token position where it occurs and regenerate while intervening at only one transformer block. The intervention clamps an abnormally large transported step in the readout aligned low dimensional frame while preserving the orthogonal residual component, producing targeted refinement that reduces hallucination without retraining or modifying the base model. 2. Related Work Hallucination detection and self-checking in LLMs. Prior work mitigates hallucination and factual errors by adding external verification, including retrieval augmentation (Lewis et al., 2020; Nakano et al., 2021), post-hoc checking with an additional language model (Zheng et al., 2023; Li et al., 2023), and faithfulness benchmarks (Maynez et al., 2020; Lin et al., 2022). While effective, these approaches act only after generation, add latency and cost, and treat the base model as black box, offering limited access to the internal decision process that produced the output. Logit-based probing and intermediate readouts. Recent work probes transformer representations via intermediate readouts. Logit Lens (Elhage et al., 2021) and refinements such as Tuned Lens (Belrose et al., 2023) decode logits from hidden states across depth to study how token predictions evolve (Elhage et al., 2021; Chuang et al., 2023). Related analyses examine when semantic information becomes linearly decodable (Tenney et al., 2019; Ethayarajh, 2019) and how intermediate representations anticipate final outputs (Ferrando et al., 2023; Pal et al., 2023). While these methods offer layer-wise snapshots, we instead trace token-level decision formation as depthwise trajectory, enabling localization of atypical motion to specific internal updates rather than static representational states. Representation geometry and depth-dependent subspaces. Transformer representations exhibit anisotropy and depth-dependent geometric structure (Ethayarajh, 2019; Li et al., 2020), and task-relevant linear subspaces can shift substantially across layers, limiting single global coordinate system (Tenney et al., 2019; Belrose et al., 2023). Recent analyses further suggest that representational change across depth encodes meaningful computation rather than noise (Pal et al., 2023). Motivated by these findings, we construct readout-aligned subspaces within depth windows and model their drift across depth, treating changes in the local frame as part of the signal rather than nuisance. Internal signals for reliability and lightweight validation. Recent work uses internal signals to assess reliability beyond external verification. Generation-time statistics and hidden-state dynamics can predict errors, overconfidence, or hallucination without modifying the base model (Kadavath et al., 2022; Manakul et al., 2023; Kuhn et al., 2023; Chen et al., 2024a), and related efforts leverage entropy, logit dynamics, or intermediate activations to assess validity or trigger selective abstention (Jiang et al., 2023; Chen et al., 2024b). While effective, these approaches often rely on task-specific heuristics or shallow statistics. We instead extract structured flow signatures that track how decisions evolve across depth and train lightweight validator on these internal dynamics, without altering the base LLM or its decoding procedure. Inference-time refinement via internal interventions. Recent work mitigates hallucination via inference-time interventions on model internals, including activation editing (Wang et al., 2025; Turner et al., 2023), layer-contrast decoding (Chuang et al., 2023), and hidden-state steering in vision-language models (Su et al., 2025; Wu et al., 2025; Liu et al., 2025). Our refinement uses flow signatures to localize sample-specific culprit event in depth and token position, then intervenes only on that update while preserving the orthogonal component, coupling detection and refinement in shared geometric frame. 3. Depthwise Flow Signatures 3.1. Setup, monitoring boundary, and objective This subsection fixes the monitoring boundary and the quantities we record, yielding stable depth-comparable measurements for downstream validation. Model and monitored boundary state. Consider an autoregressive transformer on token sequence x1:n with blocks. For token position {1, . . . , n} and boundary index {0, . . . , B}, let ht,b Rd be the residual stream state observed at fixed inter-block monitoring boundary, chosen once and kept fixed throughout the analysis. For each block {0, . . . , 1}, let ot,b Rd be the total attention contribution (summed over heads) and let mt,b Rd be the MLP contribution. The monitored update across the boundary is ht,b+1 = Nb+1 (cid:0)ht,b + ot,b + mt,b (cid:1), where Nb+1 is the boundary normalization with its learned 2 Submission and Formatting Instructions for ICML 2026 Figure 1. Pipeline overview. (a) Extraction and centering. We read states at fixed monitoring boundary and apply bias centering. (b) Subspace construction. Logit ranking yields top token and competitors, whose directions form window basis via SVD. (c) Geometric view. The depthwise trajectory lies on shell bound, while window frames evolve across depth, inducing subspace drift. affine parameters. All subsequent measurements are derived from states observed at this fixed boundary. Readout and competitor directions. Let RV be the readout to logits over vocabulary of size , and write ℓt,b = ht,b RV . We use only to obtain depth-local competitor directions from the top ranked token and its nearest alternatives. These directions act as task-aligned probes in shared parameter space, enabling depth-wise comparison without enforcing single global subspace. Objective. We monitor depthwise flow signatures without single global readout subspace. Readout-relevant directions vary with depth, so we summarize token motion at the fixed boundary in window-specific readout-aligned frames and treat their drift as signal for validation. Bias centered monitoring. Let βb Rd be the learned affine shift of the boundary normalization at depth b. We monitor the bias-centered state (Fig. 1a) ht,b = ht,b βb. (1) For the depth step + 1, ht,b+1 ht,b = (ht,b+1 ht,b) (βb+1 βb). The offset (βb+1 βb) depends on depth but not on token index, so it acts as token-shared translation at each step. Centering removes this depth-dependent shift from tokenwise motion and yields more stable pooled summaries across tokens. When needed, ℓt,b = (ht,b + βb) with depthonly offset βb. Lemma 3.1 (Shell bound from boundary normalization). Assume the monitored boundary normalization admits the affine form Nb(ut,b) = Γb S(ut,b) + βb, Γb = diag(γb,1, . . . , γb,d), (2) with 0 < γmin γb,i γmax for all and i. Here is the models native normalization at the monitoring boundary (e.g., LayerNorm or RMSNorm). Assume further that on the monitored region the normalization denominator stays away from degeneracy so that cmin S(u)2 cmax d, for constants 0 < cmin cmax. Then, for all (t, b), γmincmin ht,b2 γmaxcmax d. (3) (4) Proof. Appendix A.2. 3.2. Competitor Directions and Moving Subspaces Readout-relevant directions vary with depth, so single global subspace yields unstable summaries. We instead fit windowed k-dimensional subspaces whose frames move with depth. Depth windows. We index blocks by {0, . . . , 1}. Fix window length 1 and stride 1. For 1, define the window start bj = (j 1)s and the jth window Wj = {bj, . . . , bj + 1}, with the last window truncated so that it ends at 1. Let j(b) be the deterministic assignment that maps each block to the most recent window whose start satisfies bj b. This yields unique j(b) for every b. Exact endpoints and the closed-form j(b) are given in Appendix A.3. Readout competitors within window. For each (t, b), let (cid:98)yt,b be the top ranked token under the logit list ℓt,b and let Ct,b be the top competitors excluding (cid:98)yt,b (Appendix A.4). 3 Submission and Formatting Instructions for ICML 2026 For window Wj, we collect competitor difference directions (Fig. 1b) Aj = (cid:110) at,b,y = (cid:98)yt,b wy : Wj, Ct,b (cid:111) . (5) We restrict to token positions permitted by predefined mask (Appendix A.4). Subspace fitting. From Aj in Eq. (5), we sample at most cap directions and stack them as rows into Dj RMj d, where Mj is the number of sampled directions. Let the compact SVD be Dj = (D) with right singular vectors Vj = [vj,1, . . . , vj,d]. We take the window basis as the top right singular vectors, ΣjV Uj = [vj,1, . . . , vj,k] Rdk, Uj = Ik, (6) and re-orthonormalize the extracted columns in finite precision. If Mj = 0, we use deterministic fallback basis. Details of constructing Dj (normalization, capping, and sampling), the re-orthonormalization step, and the deterministic handling of degenerate windows are given in Appendix A.5. Projected coordinates. Using the assigned window basis Uj(b) (Eq. 6) and the bias-centered state ht,b (Eq. 1), define the moving coordinates pt,b = j(b) ht,b Rk. (7) These coordinates feed the transported step, turning, and drift signatures in the next subsection (Fig. 1c). 3.3. Transported Flow Signatures 3.3.1. TRANSPORTED MOTION IN MOVING SUBSPACES We align adjacent window frames with an orthogonal transport and record transported step length, turning, and centered increment in Rk. Window transport. Window bases are only defined up to within-window orthogonal rotation, so window switches can introduce spurious frame changes in Rk. We align consecutive frames with the closest orthogonal map (Fig. 2a) (Fernando et al., 2013): j+1Uj = PjΣjQ , Rjj+1 = PjQ Rkk. Using the window assignment j(b) from Sec. 3.2, define the step-wise transport Rb = (cid:40) Ik, Rj(b)j(b+1), j(b + 1) = j(b), j(b + 1) = j(b). Transported step and turning. Using moving coordinates pt,b (Eq. 7), record the transported increment pt,b = pt,b+1 Rb pt,b Rk, and its step size For directional summaries, use ut,b = pt,b2+ε with εnum > 0 (Appendix B.5), and record the transported turning angle pt,b θt,b = (cid:0)ut,b+1, Rbut,b (cid:1). (9) Centered increment for aggregation. For fixed sample and depth step b, remove token-shared shift by centering {pt,b}t. Let µb Rk be rotation-equivariant robust center over masked token positions (Appendix A.7), and record pc t,b = pt,b µb, t,b = pc sc t,b2. (10) 3.3.2. COMPONENT CONTRIBUTIONS UNDER BOUNDARY NORMALIZATION We record pre-normalization component magnitudes and post-normalization effective updates in the same target frame. Because boundary normalization is nonlinear, we use path-integrated update qt,b to explain the transported increment pt,b, and record the residual mismatch ηt,b. Target frame and pre-normalization magnitudes. For step + 1, use the next-frame basis tgt = Uj(b+1) Rdk, so component projections share the same reference as the transported step at depth b+1. Project pre-normalization components: po )mt,b. Record their magnitudes t,b = (U tgt t,b = (U tgt )ot,b; pm b at,b = po t,b2, mmag t,b = pm t,b2. (11) These quantify injected energy along the target subspace, prior to boundary-normalization reweighting. Path-integrated effective updates in the target frame. Let hraw t,b be the uncentered pre-normalization state and injt,b = ot,b + mt,b. Along the injection path xt,b(α) = hraw t,b + α injt,b for α [0, 1], path integration captures state dependent scaling through the boundary map (Sundararajan et al., 2017). Let Jb+1() denote the Jacobian of the boundary normalization at depth + 1. Record path-integrated channel updates hattn t,b = hmlp t,b = (cid:90) 1 0 (cid:90) 1 0 Jb+1 (cid:0)xt,b(α)(cid:1) ot,b dα, Jb+1 (cid:0)xt,b(α)(cid:1) mt,b dα, evaluate Jb+1(u)v via JVPs (Pearlmutter, 1994), and project to the target frame qattn t,b = (U tgt Combine )hattn t,b , qmlp t,b = (U tgt )hmlp t,b . st,b = pt,b2. (8) qt,b = qattn t,b + qmlp t,b , ct,b = qt,b2. (12) 4 Submission and Formatting Instructions for ICML 2026 Figure 2. Flow signatures from transported subspace trajectories. (a) Aligning moving subspaces. Adjacent window bases Uj and Uj+1 are aligned by an orthogonal transport Rjj+1 from the SVD of j+1Uj, enabling consistent coordinates across depth windows. (b) Projected path decomposition. Boundary-normalized updates are projected onto target window frame, producing attention and MLP contributions (qattn t,b ) and residual term ηt,b. (c) Normalized turning in moving frame. The turning angle θt,b compares the current direction to the transported previous direction on the unit sphere, yielding frame-invariant curvature summary. t,b , qmlp Single-point linearization and residual ratio. singlepoint JVP at α = 1 applied to the full injection gives (cid:0)xt,b(1)(cid:1) injt,b, qend t,b = (U tgt )hend t,b . t,b = Jb+1 hend Record the deviation in space: ηt,b = qt,b qend t,b Rk, ρt,b = ηt,b2 qt,b2 + ε . (13) In all experiments, qt,b uses three-node Simpson rule over α {0, 0.5, 1}, while qend t,b uses single JVP at α = 1 (Appendix B.7). Component turning ratios. Let utgt t,b = ut,b+1. We attribute bending to attention versus MLP by comparing their perpendicular components relative to utgt t,b . For any Rk, t,b , utgt remove its component along utgt t,b . Record per-step ratios t,b : = utgt rattn t,b = t,b )2 (qattn (qt,b)2 + ε , rmlp t,b = (qmlp t,b )2 (qt,b)2 + ε . We aggregate these per-step ratios into token-level summaries using robust masked aggregation over depth (Appendix B.3), yielding stable summaries under outlier steps and variable token lengths (Fig.2c). 3.3.3. SUBSPACE DRIFT AND GAUGE INVARIANCE Subspace drift across windows. Window bases are only identifiable up to within-window orthogonal rotations, so we measure drift with projector-level quantities that depend on the subspace itself (Gong et al., 2012). We record the Grassmann drift, dG(Uj, Uj+1) = UjU where 2 is the spectral norm (Appendix A.8). Uj+1U j+12, We also record state-coupled drift using single deterministic anchor block to the window end), which keeps the cost linear in the number of windows while coupling drift to visited directions: per window (we fix χt,j = (Uj+1U ) ht,b j+1 UjU ht,b 2 + ε 2 , Dt = J1 (cid:88) j=1 χt,j. (14) Appendix A.8 also gives the deterministic anchor rule and simple bounds such as χt,j dG(Uj, Uj+1). Lemma 3.2 (Gauge invariance of transported signatures). Fix window bases {Uj}j with orthonormal columns. For any orthogonal {Qj}j, set = UjQj and apply the same constructions to obtain the transported quantities under j}. Then following are invariant under {Uj} (cid:55) {U {U j}: transported step lengths pt,b2, transported turning angles (cid:0)ut,b+1, Rbut,b (cid:1), any ratio built from Euclidean norms after removing t,b (including the perpendicular the component along utgt channel ratios and their masked depth aggregates), any drift depending only on projectors UjU ing dG and the anchor-coupled drift), (includthe residual ratio ρt,b in Eq. 13. If pc depth, then pc t,b uses rotation-equivariant center in Rk at each t,b2 is also invariant. Rotation-equivariant Submission and Formatting Instructions for ICML 2026 choices include the Euclidean mean and the geometric median; coordinate-wise medians are not. Proof. Fix window-wise basis change = UjQj with orthogonal Qj Rkk. For adjacent windows, write the compact SVD j+1Uj = ΣQ, Rjj+1 = Q."
        },
        {
            "title": "Then",
            "content": "(U j+1(U j+1)U admits compact SVD with = which yields = j+1Uj)Qj j+1P and = Q, jj+1 = (Q) = j+1Rjj+1Qj."
        },
        {
            "title": "Therefore the moving coordinates rotate as",
            "content": "t,b = (U j(b))ht,b = j(b)pt,b, t,b = j(b+1)pt,b. Since Qj(b+1) is orthogonal, it preserves Euclidean norms and angles. This proves invariance of the transported step length pt,b2 and the transported turning angle (cid:0)ut,b+1, Rbut,b (cid:1). Perpendicular ratios are invariant because the target direction and all target-frame projections undergo the same orthogonal rotation, preserving inner products and perpendicular norms. Projector-based drifts are invariant since . ρt,b is invariant because qt,b and qend t,b share the same target-frame rotation. j) = UjU j(U 3.4. Self-checking via Flow Signature Validation We train lightweight validator gϕ that performs selfchecking from the flow signatures in Sec. 3.3. For each sample n, the extractor produces masked depth-andtoken sequence {xn,b,t} together with validity mask Mn. The validator maps the sequence to single score ˆyn = gϕ({xn,b,t}, Mn) [0, 1], used for threshold-based flagging at inference. Supervision is binary, yn {0, 1}, obtained from dataset labels or an external judging protocol applied to the final answer, and gϕ is trained with standard binary classification loss. Signature inputs. Each event vector xn,b,t concatenates the scalar summaries (st,b, sc t,b, θt,b) from Eqs. 8, 9, 10 (at,b, mmag t,b , ct,b, ρt,b) from Eqs. 11, 12, 13, and the tokenlevel drift summary Dt from Eq. 14. All inputs are invariant to within-window basis rotations (Lemma 3.2), so ˆyn is comparable across depths and samples. Validator architecture, masking and packing, optimization, and calibration are deferred to the Appendix B. 3.5. Flow-Guided Refinement Flow signature validation not only flags sample but also localizes culprit event along depth. Let the validator emit Figure 3. Flow-guided single-block refinement. (a) Localizing culprit event (t0, b0), then an intervention at depth b0. (b) Clamp an abnormally large transported step in readout-aligned subspace. per-event scores {zj} with validity indicator {Ij}. We select arg maxj: Ij =1 zj, (t0, b0) = (tj , bj ), roll back the generation to the prefix x1:t0, and regenerate the continuation while intervening only at depth b0; Fig. 3a. We keep the overall decode budget fixed: after rolling back to x1:t0 , we regenerate only the remaining suffix budget, i.e., at most t0 new tokens, so the final continuation length matches the original generation length. The refinement targets the same depth-localized burst pattern that drives detection, an unusually large transported step inside readout-aligned coordinate system; Fig. 3b. At depth b0, let hin, hout Rd denote the transformer block input and output at the monitored boundary for the current decoding step. Here is local readout-aligned basis fitted at the culprit decoding step from the current top token and its competitors, and it is then kept fixed during suffix regeneration (Appendix A.9). Using an orthonormal basis Rdk together with an orthogonal transport Rkk (Appendix A.9), we form pin = hin, pout = hout, = pout Rpin. calibration pass on the rolled back prefix provides reference step scale sref (Appendix B.11). We apply an upper , = clamp with ratio α > 1: λ = min λ p, pout = Rpin + p. Finally, we rewrite only the component of the block output while keeping the orthogonal residual unchanged: 1, α sref p2+ε (cid:16) (cid:17) hout = hout + (pout pout). All other depths remain untouched (Appendix A.9, B.11). 4. Experiments We train lightweight GRU-based validator to self-check from internal flow signatures, predicting whether generated answer is hallucinated relative to the provided context. We evaluate on HaluEval (Li et al., 2023) across four tasks (QA, Dialogue, Summarization, General) and five base LLM 6 Table 1. Hallucination detection performance on HaluEval across LLMs. We report classification Accuracy (%) and AUROC (%). Submission and Formatting Instructions for ICML 2026 Task QA General Summarization Dialogue Qwen2.5 Gemma Accuracy Phi-3 LLaMA3 Mistral Qwen2.5 Gemma 2 72.68 68.02 53.81 50.75 70.05 64.31 54.48 56.02 67.48 68.33 53.82 56.88 64.00 71.41 52.12 47. 61.40 69.50 57.27 54.83 76.45 69.80 58.17 51.83 66.70 65.17 63.32 54.13 AUROC Phi-3 67.57 65.79 59.08 58.12 LLaMA3 Mistral 65.65 67.07 59.05 54.68 61.67 67.01 58.85 54.23 Table 2. Hallucination ratios on HaluEval under refinement applied to hallucination-labeled samples only. Each cell reports Initial / Refine with relative reduction (%). Task Qwen 2.5 Gemma Phi-3 LLaMA3 Mistral QA General Summarization Dialogue 15.25 / 10.95 ( 28.19%) 11.96 / 8.75 ( 26.86%) 40.90 / 40.20 ( 1.71%) 50.95 / 48.10 ( 5.59%) 14.90 / 12.34 ( 17.18%) 6.76 / 5.76 ( 14.79%) 39.80 / 37.10 ( 6.78%) 39.95 / 34.95 ( 12.52%) 12.65 / 6.55 ( 48.22%) 7.64 / 6.31 ( 17.41%) 42.15 / 40.70 ( 3.44%) 51.90 / 46.30 ( 10.79%) 15.10 / 7.70 ( 48.99%) 13.07 / 11.74 ( 10.17%) 45.80 / 44.60 ( 2.62%) 43.60 / 34.30 ( 21.33%) 14.05 / 11.70 ( 16.73%) 10.85 / 10.19 ( 6.08%) 49.70 / 47.25 ( 4.93%) 44.50 / 39.05 ( 12.25%) families (Gemma2 (Team et al., 2024), Phi-3 (Abdin et al., 2024), LLaMA3 (Grattafiori et al., 2024), Qwen2.5 (Hui et al., 2024), Mistral (Jiang et al., 2024); each (task, model) split uses an 8:2 train:test prompt split. For each prompt, we generate one answer, replay the prompt plus realized continuation through the same extractor with generated tokens fed back as fixed inputs (Bengio et al., 2015), and convert boundary traces into masked flowevent sequence with depth as the time axis (Appendix B.8). Binary labels in 0, 1 come from ChatGPT(OpenAI, 2025). The validator outputs score ˆy [0, 1] via mask-aware pooling over depth. We report Accuracy (threshold 0.5) and AUROC, and handle class imbalance by positive-class weighting (Details in Appendix B). 4.1. Hallucination Detection 4.1.1. DETECTING RESULTS Table 1 evaluates whether internal flow signatures alone provide usable signal for hallucination detection. We observe consistent separability on QA and General across model families, with AUROC typically above 0.65, indicating that hallucination often co-occurs with structured depthwise flow changes that lightweight validator can learn. The task gap is informative. QA and General are settings where correctness is frequently determined by contextgrounded verification, so hallucinated generations more often exhibit repeatable depth-localized deviations in transported motion, turning, and readout-aligned drift. In contrast, Summarization and Dialogue are intrinsically harder for the base LLMs, where errors tend to appear as subtle local factual edits or borderline multi-turn inconsistencies; such cases weaken alignment between the external labels and stable internal flow signature, reducing separability. Overall, these results do not claim universal detection across tasks, but establish that internal flow provides practical signal for self-checking in contexts where hallucination manifests as concentrated trajectory deviation. 4.1.2. HALLUCINATION REPORTING In QA, hallucination often manifests as depth-localized burst, where the transported increment pt,b2 spikes within narrow depth band. This yields larger step lengths st,b and higher turning θt,b, with the effective update magnitude ct,b increasing in the same neighborhood; drift summaries Dt also become more concentrated around the burst (Fig. 4a). In General, we observe localized regime change where st,b and θt,b rise together with ct,b and projected magnitudes such as at,b and mmag t,b , often with more concentrated Dt. In both tasks, separability weakens when hallucination stays in late diffuse or accumulation regime, where (st,b, θt,b) and Dt spread over deeper bands and overlap with non hallucination (Fig. 4b). In Summarization, hallucination differs mainly by whether motion concentrates into depth-gated band: pt,b becomes focused within narrow depth neighborhood with increased st,b, θt,b, and ct,b and more localized Dt, while non hallucination more often remains diffuse and lowenergy across later depths (Fig. 4c). When hallucination remains diffuse rather than concentrating, separability weakens. In Dialogue, depth-structured motion provides more stable handle: we observe both high-mass burst regime with elevated st,b, θt,b, ct,b, and concentrated Dt, and low-mass diffuse regime whose profiles overlap with non hallucination. practical summary is the burst depth band and the mixture between burst and diffuse modes, since the diffuse mode is often geometrically similar to non hallucination (Fig. 4d; Appendix C). 4.2. Flow-Guided Refinement Table 2 reports hallucination ratios before and after flowguided refinement, evaluated on samples labeled as hallucination. Refinement reduces hallucination rates most strongly on QA and more moderately on General, with rel7 Submission and Formatting Instructions for ICML Figure 4. Depthwise flow signatures across tasks. Panels contrast non hallucination (top) and hallucination (bottom) by summarizing transported trajectories over depth and tokens across tasks. Color encodes event magnitude and curvature, and annotations mark regimes such as early depth bursts, late diffuse accumulation, and trajectory collapse. ative reductions reaching 48.99% on QA and 26.86% on General in the best cases. The effect is task dependent. Summarization changes remain small across models (1.71%6.78%), while Dialogue shows intermediate reductions (5.59%21.33%). This trend matches the locality of the intervention: targeting single depth is most helpful when the error is driven by localized depth event, while longer-form behaviors that distribute deviations across tokens and depths admit less headroom from single-block correction. We additionally report regeneration-only and random-depth variants in Appendix D.2 to separate pure regeneration effects from depth-specific intervention benefits. 4.3. Runtime Cost and Scalability Refinement keeps the final continuation length fixed at tokens, but may discard short suffix and regenerate it after an intervention point. Let tcur be the number of tokens already generated when the intervention triggers and let t0 tcur be the prefix length we keep fixed. With cached decoding state at t0, the extra decoding cost is exactly the discarded suffix length tcur t0, so the total decoded tokens become +(tcurt0) and the overhead factor is 1+ tcurt0 , which is bounded by 2 in the worst case. If cached restore at t0 is unavailable, the method additionally replays the prefix to reconstruct state, which can add up to another t0 tokens of forward passes per intervention. 5. Discussion Performance depends on base model competence. The signatures are extracted from the same base LLM, so separability is bounded by the strength and consistency of its internal decision dynamics. When the base model handles the task reliably (QA and General in our setting), hallucinations more often align with consistent depth-local regime changes in the tracked signals, yielding stronger separation. For tasks the base model finds harder (Dialogue and Summarization), trajectories are more diffuse and label is ambiguous, weakens alignment between external labels and internal signatures and degrades validator performance. Refinement is first step, not final mechanism. Our refinement uses deliberately simple operator: single-block clamp that shrinks an abnormally large transported step at validator-localized culprit event while preserving the orthogonal residual. The heterogeneous gains across tasks reflect this locality: when the error is driven by depth-localized deviation, targeted correction can improve the trajectory with minimal collateral damage, whereas long-form settings such as Summarization often involve deviations distributed across tokens and depths, leaving less headroom for singlesite intervention. These results position refinement as an initial control primitive; extending it to multiple sites or short staged corrections can be the next direction, and it can naturally guide the choice of intervention locations. 6. Conclusion We presented method that audits how language model forms an answer by monitoring its internal behavior during generation. Using these internal signals, small validator can predict whether an output is likely to be hallucinated without changing the base model, and it can also indicate where the failure emerges inside the model. Building on this localization, we introduce lightweight refinement step that intervenes at single point to reduce hallucinations with minimal additional computation. Overall, internal monitoring offers practical handle for detection and correction, and motivates stronger, more general interventions. 8 Submission and Formatting Instructions for ICML"
        },
        {
            "title": "Acknowledgments",
            "content": "This work was supported in part by the DARPA Young Faculty Award, the National Science Foundation (NSF) under Grants #2127780, #2319198, #2321840, #2312517, and #2235472, #2431561, the Semiconductor Research Corporation (SRC), the Office of Naval Research through the Young Investigator Program Award, and Grants #N0001421-1-2225 and #N00014-22-1-2067, Army Research Office Grant #W911NF2410360. Additionally, support was provided by the Air Force Office of Scientific Research under Award #FA9550-22-1-0253, along with generous gifts from Xilinx and Cisco."
        },
        {
            "title": "Impact Statement",
            "content": "This paper studies depthwise internal flow signatures to detect and characterize hallucination behavior in large language models. The primary intended impact is to improve reliability, auditing, and safety of model outputs in downstream applications by enabling earlier detection and correction of unsupported generations. Potential risks include misuse of internal monitoring signals to optimize deceptive generations, to tune jailbreak style behaviors, or to support surveillance like deployment practices that reduce user autonomy. We mitigate these risks by focusing on model agnostic measurements rather than attack recipes, by reporting limitations and failure cases, and by encouraging responsible disclosure and evaluation under safety guidelines when releasing artifacts."
        },
        {
            "title": "References",
            "content": "Abdin, M., Aneja, J., Awadalla, H., Awadallah, A., Awan, A. A., Bach, N., Bahree, A., Bakhtiari, A., Bao, J., Behl, H., et al. Phi-3 technical report: highly capable language model locally on your phone. arXiv preprint arXiv:2404.14219, 2024. Azaria, A. and Mitchell, T. The internal state of an llm knows when its lying. arXiv preprint arXiv:2304.13734, 2023. Ba, J. L., Kiros, J. R., and Hinton, G. E. Layer normalization. arXiv preprint arXiv:1607.06450, 2016. Belrose, N., Furman, Z., Smith, L., Halawi, D., Ostrovsky, I., McKinney, L., Biderman, S., and Steinhardt, J. Eliciting latent predictions from transformers with the tuned lens. arXiv preprint arXiv:2303.08112, 2023. Chen, C., Liu, K., Chen, Z., Gu, Y., Wu, Y., Tao, M., Fu, Z., and Ye, J. Inside: Llms internal states retain the power of hallucination detection. arXiv preprint arXiv:2402.03744, 2024a. Chen, Y. et al. Rethinking confidence estimation for large arXiv preprint arXiv:2402.XXXX, language models. 2024b. Chuang, Y.-S., Xie, Y., Luo, H., Kim, Y., Glass, J., and He, P. Dola: Decoding by contrasting layers improves arXiv preprint factuality in large language models. arXiv:2309.03883, 2023. Elhage, N., Nanda, N., Olsson, C., Henighan, T., Joseph, N., Mann, B., Askell, A., Bai, Y., Chen, A., Conerly, T., et al. mathematical framework for transformer circuits. Transformer Circuits Thread, 1(1):12, 2021. Ethayarajh, K. How contextual are contextualized word representations? comparing the geometry of bert, elmo, and gpt-2 embeddings. arXiv preprint arXiv:1909.00512, 2019. Farquhar, S., Kossen, J., Kuhn, L., and Gal, Y. Detecting hallucinations in large language models using semantic entropy. Nature, 630(8017):625630, 2024. Fernando, B., Habrard, A., Sebban, M., and Tuytelaars, T. Unsupervised visual domain adaptation using subspace In Proceedings of the IEEE international alignment. conference on computer vision, pp. 29602967, 2013. Ferrando, J., Gallego, G. I., Tsiamas, I., and Costa-Juss`a, M. R. Explaining how transformers use context to build predictions. arXiv preprint arXiv:2305.12535, 2023. Gao, L., Dai, Z., Pasupat, P., Chen, A., Chaganty, A. T., Fan, Y., Zhao, V., Lao, N., Lee, H., Juan, D.-C., et al. Rarr: Researching and revising what language models say, using language models. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1647716508, 2023. Geng, J., Cai, F., Wang, Y., Koeppl, H., Nakov, P., and Gurevych, I. survey of confidence estimation and calibration in large language models. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pp. 65776595, 2024. Bengio, S., Vinyals, O., Jaitly, N., and Shazeer, N. Scheduled sampling for sequence prediction with recurrent neural networks. Advances in neural information processing systems, 28, 2015. Gong, B., Shi, Y., Sha, F., and Grauman, K. Geodesic flow kernel for unsupervised domain adaptation. In 2012 IEEE conference on computer vision and pattern recognition, pp. 20662073. IEEE, 2012. 9 Submission and Formatting Instructions for ICML 2026 Grattafiori, A., Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur, A., Schelten, A., Vaughan, A., et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Huang, L., Yu, W., Ma, W., Zhong, W., Feng, Z., Wang, H., Chen, Q., Peng, W., Feng, X., Qin, B., and Liu, T. survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions, 2023. Hui, B., Yang, J., Cui, Z., Yang, J., Liu, D., Zhang, L., Liu, T., Zhang, J., Yu, B., Lu, K., et al. Qwen2. 5-coder technical report. arXiv preprint arXiv:2409.12186, 2024. Ji, Z., Chen, D., Ishii, E., Cahyawijaya, S., Bang, Y., Wilie, B., and Fung, P. Llm internal states reveal hallucination risk faced with query. arXiv preprint arXiv:2407.03282, 2024. Jiang, A., Sablayrolles, A., Mensch, A., Bamford, C., Chaplot, D., Casas, D., Bressand, F., Lengyel, G., Lample, G., Saulnier, L., et al. Mistral 7b. arxiv 2023. arXiv preprint arXiv:2310.06825, 2024. Jiang, Z. et al. Active retrieval augmented generation. arXiv preprint arXiv:2305.06983, 2023. Kadavath, S., Conerly, T., Askell, A., Henighan, T., Drain, D., Perez, E., Schiefer, N., Hatfield-Dodds, Z., DasSarma, N., Tran-Johnson, E., et al. Language models (mostly) know what they know. arXiv preprint arXiv:2207.05221, 2022. Kossen, J., Han, J., Razzak, M., Schut, L., Malik, S., and Gal, Y. Semantic entropy probes: Robust and cheap hallucination detection in llms. arXiv preprint arXiv:2406.15927, 2024. Kuhn, L., Gal, Y., and Farquhar, S. Semantic uncertainty: Linguistic invariances for uncertainty estimaarXiv preprint tion in natural language generation. arXiv:2302.09664, 2023. Lewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., Goyal, N., Kuttler, H., Lewis, M., Yih, W.-t., Rocktaschel, T., Riedel, S., and Kiela, D. Retrieval-augmented generation for knowledge-intensive NLP tasks, 2020. Li, B., Zhou, H., He, J., Wang, M., Yang, Y., and Li, L. On the sentence embeddings from pre-trained language models. arXiv preprint arXiv:2011.05864, 2020. Li, J., Cheng, X., Zhao, W. X., Nie, J.-Y., and Wen, J.- R. Halueval: large-scale hallucination evaluation benchmark for large language models. arXiv preprint arXiv:2305.11747, 2023. Lin, S., Hilton, J., and Evans, O. Truthfulqa: Measuring how models mimic human falsehoods. In Proceedings of the 60th annual meeting of the association for computational linguistics (volume 1: long papers), pp. 32143252, 2022. Liu, S., Ye, H., and Zou, J. Reducing hallucinations in large vision-language models via latent space steering. In The Thirteenth International Conference on Learning Representations, 2025. Manakul, P., Liusie, A., and Gales, M. Selfcheckgpt: Zeroresource black-box hallucination detection for generative large language models. In Proceedings of the 2023 conference on empirical methods in natural language processing, pp. 90049017, 2023. Maynez, J., Narayan, S., Bohnet, B., and McDonald, R. On faithfulness and factuality in abstractive summarization. arXiv preprint arXiv:2005.00661, 2020. Nakano, R., Hilton, J., Balaji, S., Wu, J., Ouyang, L., Kim, C., Hesse, C., Jain, S., Kosaraju, V., Saunders, W., et al. Webgpt: Browser-assisted question-answering with human feedback, 2021. OpenAI. Chatgpt 4o-mini. https://chat.openai. com/, 2025. Used as automatic evaluator for multimodal hallucination analysis. Pal, K., Sun, J., Yuan, A., Wallace, B. C., and Bau, D. Future lens: Anticipating subsequent tokens from single hidden state. arXiv preprint arXiv:2311.04897, 2023. Pearlmutter, B. A. Fast exact multiplication by the hessian. Neural computation, 6(1):147160, 1994. Sriramanan, G., Bharti, S., Sadasivan, V. S., Saha, S., Kattakinda, P., and Feizi, S. Llm-check: Investigating detection of hallucinations in large language models. Advances in Neural Information Processing Systems, 37: 3418834216, 2024. Su, J., Chen, J., Li, H., Chen, Y., Qing, L., and Zhang, Z. Activation steering decoding: Mitigating hallucination in large vision-language models through bidirectional In Proceedings of the 63rd hidden state intervention. Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1296412974, 2025. Sundararajan, M., Taly, A., and Yan, Q. Axiomatic attribution for deep networks. In International conference on machine learning, pp. 33193328. PMLR, 2017. Team, G., Riviere, M., Pathak, S., Sessa, P. G., Hardin, C., Bhupatiraju, S., Hussenot, L., Mesnard, T., Shahriari, B., Rame, A., et al. Gemma 2: Improving open 10 Submission and Formatting Instructions for ICML language models at practical size. arXiv preprint arXiv:2408.00118, 2024. Tenney, I., Das, D., and Pavlick, E. Bert rediscovers the classical nlp pipeline. arXiv preprint arXiv:1905.05950, 2019. Turner, A. M., Thiergart, L., Leech, G., Udell, D., Vazquez, J. J., Mini, U., and MacDiarmid, M. Steering language models with activation engineering. arXiv preprint arXiv:2308.10248, 2023. Wang, T., Jiao, X., Zhu, Y., Chen, Z., He, Y., Chu, X., Gao, J., Wang, Y., and Ma, L. Adaptive activation steering: tuning-free llm truthfulness improvement method for diverse hallucinations categories. In Proceedings of the ACM on Web Conference 2025, pp. 25622578, 2025. Wu, J., Ding, Y., Liu, G., Xia, T., Huang, Z., Sui, D., Liu, Q., Wu, S., Wang, L., and Tan, T. Sharp: Steering hallucination in lvlms via representation engineering. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pp. 1435714372, 2025. Xiong, R., Yang, Y., He, D., Zheng, K., Zheng, S., Xing, C., Zhang, H., Lan, Y., Wang, L., and Liu, T. On layer normalization in the transformer architecture. In International conference on machine learning, pp. 1052410533. PMLR, 2020. Zheng, L., Chiang, W.-L., Sheng, Y., Zhuang, S., Wu, Z., Zhuang, Y., Lin, Z., Li, Z., Li, D., Xing, E. P., Zhang, H., Gonzalez, J. E., and Stoica, I. Judging LLM-as-a-judge with MT-bench and chatbot arena, 2023. 11 Submission and Formatting Instructions for ICML 2026 A. Technical Appendix A.1. Normalization band constants and sufficient conditions This appendix pins down explicit constants in the norm band cmin simple second moment conditions compatible with common LayerNorm or RMSNorm implementations. S(u)2 cmax used in Lemma 3.1, under Per token normalization notation. For vector Rd, fix ε > 0. LayerNorm case. Write the empirical mean and centered second moment"
        },
        {
            "title": "Use",
            "content": "µ(u) = 1 (cid:88) i=1 ui, s2(u) = 1 (cid:88) i=1 (cid:0)ui µ(u)(cid:1)2 . SLN(u) = µ(u)1 (cid:112)s2(u) + ε . RMSNorm case. Write the (uncentered) second moment Use m2(u) = 1 d (cid:88) i=1 u2 . SRMS(u) = (cid:112)m2(u) + ε . Lemma A.1 (Exact norm identity and variance-to-band reduction (LayerNorm)). For any Rd, SLN(u)2 2 = s2(u) s2(u) + ε . Consequently, if monitored region satisfies two-sided variance condition then SLN(u)2 lies in d-scaled band 0 < vmin s2(u) vmax, cmin SLN(u)2 cmax d, cmin = (cid:114) vmin vmin + ε , cmax = (cid:114) vmax vmax + ε 1. Lemma A.2 (Exact norm identity and moment-to-band reduction (RMSNorm)). For any Rd, Consequently, if monitored region satisfies then SRMS(u)2 lies in d-scaled band SRMS(u)2 2 = m2(u) m2(u) + ε . 0 < vmin m2(u) vmax, cmin SRMS(u)2 cmax d, cmin = (cid:114) vmin vmin + ε , cmax = (cid:114) vmax vmax + ε 1. 12 (15) (16) (17) (18) (19) (20) Proof. For LayerNorm, Submission and Formatting Instructions for ICML 2026 SLN(u)2 2 = (cid:88) i=1 (ui µ(u))2 s2(u) + ε = s2(u) s2(u) + ε , which gives Eq. 15. Under Eq. 16, (cid:55) x/(x + ε) is increasing on (0, ), yielding Eq. 17. For RMSNorm, SRMS(u)2 2 = (cid:88) i=1 u2 m2(u) + ε = m2(u) m2(u) + ε , which gives Eq. 18; Eq. 20 follows from Eq. 19 by the same monotonicity argument. Sufficient conditions supplied by common implementations. Eq. 17 follows on any monitored region where the empirical variance satisfies Eq. 16. Some implementations or custom kernels further enforce this by applying variance floor or clamping: Variance floor. One may replace s2(u) by max{s2(u), vmin} before the square root. This guarantees the lower bound in Eq. 16 for all inputs. Variance clamping. One may replace s2(u) by clip(s2(u), vmin, vmax). This guarantees both inequalities in Eq. 16 for all inputs, so the constants in Eq. 17 hold deterministically. Even without explicit clipping, Eq. 17 holds on any subset of states where the empirical variance stays within [vmin, vmax]. In particular, the upper bound cmax 1 always holds, while the lower bound is controlled by how far the monitored variance stays away from 0 relative to ε. A.2. Proof of Lemma 3.1 Using Eq. (1) and Eq. (2), ht,b = Γb S(ut,b). Since Γb2 γmax and Γ1 ht,b2 γmaxcmax d, which gives Eq. (4). 2 1/γmin, Eq. (3) yields γmincmin A.3. Window indexing and deterministic assignment j(b) This appendix fixes the window list {Wj}J guarantees that the final window ends at block 1, while supporting overlap under stride < L. j=1 and the deterministic mapping j(b) used in Sec. 3.2. The construction Index set. Blocks are indexed by integers {0, 1, . . . , 1}. Window starts and ends with forced final window. Fix integers 1 (window length) and 1 (stride). Define the forced last start index Let be the smallest integer such that (J 1)s blast, equivalently blast = max{0, L}. For {1, . . . , J}, define the start index and the end index The window is the contiguous index set = (cid:109) (cid:108) blast + 1. bj = min{(j 1)s, blast}, ej = min{bj + 1, 1}. Wj = {bj, bj + 1, . . . , ej}. By construction, bJ = blast and eJ = 1, so the final window is always aligned to end at the last block. When L, the final window has length exactly L. When < L, we have blast = 0 and the single window W1 = {0, . . . , 1} covers the full depth axis. 13 Submission and Formatting Instructions for ICML 2026 Deterministic assignment j(b). When < L, block index can belong to multiple windows. We assign each block to the latest started window among those that contain it. For each {0, . . . , 1}, define j(b) = max{j {1, . . . , J} : Wj}. Because WJ contains 1 and windows are contiguous, the set is nonempty for every and the maximum exists and is unique. closed form uses the condition ej: ej bj + 1 bj (L 1). Since bj = min{(j 1)s, blast} is nondecreasing in j, the latest window containing is obtained by taking the largest admissible start, yielding j(b) = min (cid:110) J, (cid:108) max{0, (L 1)} (cid:109) (cid:111) . + 1 This rule reduces to j(b) = min{J, b/s + 1} when is large enough that membership is implied by bj b, and it correctly handles overlap by enforcing Wj(b). Basic properties. The assignment j(b) is nondecreasing in and increments only when crosses new eligible window start under the membership constraint. In particular, for each b, the assigned window satisfies bj(b) ej(b). Step transition logic for transports. The transport selector in Sec. 3.3.1 uses j(b) and j(b + 1). window switch occurs at depth step + 1 exactly when j(b + 1) = j(b). Reference pseudocode. The following pseudocode matches the equations above. # Inputs: B, L, b_last = max(0, - L) = ceil(b_last / s) + 1 for in {1,...,J}: b_j = min((j-1)*s, b_last) e_j = min(b_j + - 1, - 1) W_j = {b_j, ..., e_j} # Deterministic assignment (latest window that contains b): for in {0,...,B-1}: j_of_b = min(J, ceil(max(0, b-(L-1)) / s) + 1) Small worked example. Take = 10, = 4, = 2. Then blast = 6 and = 6/2 + 1 = 4. Starts are bj {0, 2, 4, 6} and endpoints are ej {3, 5, 7, 9}, so W4 = {6, 7, 8, 9} ends at 1. The assignment yields j(b) = 1 for {0, 1, 2, 3}, j(b) = 2 for {4, 5}, j(b) = 3 for {6, 7}, and j(b) = 4 for {8, 9}. A.4. Competitor construction and token mask This appendix fixes the precise construction of the top token (cid:98)yt,b, the competitor set Ct,b used in Eq. 5, and the predefined token mask that restricts which token positions contribute directions to Aj. Logits used for ranking. At each monitored boundary and depth b, we compute logits ℓt,b = ht,b RV . When bias-centering is used, ht,b = ht,b + βb (Eq. 1), so the same logits can be written as ℓt,b = (ht,b + βb). All ranking operations below use these logits and do not apply temperature rescaling, logit clipping, or post-processing. 14 Submission and Formatting Instructions for ICML 2026 Top token and tie breaking. Define the top token index If the argmax is not unique, break ties deterministically by selecting the smallest index: (cid:98)yt,b arg max y{1,...,V } ℓt,b[y]. (cid:98)yt,b = min (cid:110) : ℓt,b[y] = max ℓt,b[y] (cid:111) . Competitor set. Let πt,b be deterministic ordering of token indices obtained by sorting logits in non-increasing order, with ties broken by the same smallest-index rule. Thus, ℓt,b[πt,b(1)] ℓt,b[πt,b(2)] ℓt,b[πt,b(V )]. Then πt,b(1) = (cid:98)yt,b. Fix an integer 1. The competitor set is the next distinct indices after the top token: Ct,b = {πt,b(2), πt,b(3), . . . , πt,b(K + 1)}. (21) If + 1 > , we take all remaining indices. Competitor difference directions. With readout rows {wy}V y=1 and (t, b) fixed, define the competitor directions at,b,y = (cid:98)yt,b wy, Ct,b. These are exactly the directions collected into Aj in Eq. 5. Token mask . For given sequence of length n, define binary mask {0, 1}n, Mt = 1 is eligible for competitor sampling. The masked restriction in the main text means that (t, b) contributes directions only when Mt = 1. The mask is deterministic given the tokenization and sequence metadata, and is chosen to exclude positions whose logits are either not semantically meaningful or are dominated by formatting and control tokens. Default admissible positions. Let be the set of tokenizer special tokens together with any model-specific role or delimiter tokens, and let x1:n be the tokenized prompt-plus-generation sequence. default mask used throughout is (cid:104) Mt = 1 xt / (cid:105) (cid:104) neff (cid:105) . (22) where neff is the effective sequence length after truncation and padding removal. Prompt and response restriction. When prompt prefix is present, we optionally restrict competitor sampling to contiguous token span [tmin, tmax] to avoid control-heavy regions. In that case, Mt Mt 1[tmin tmax]. (23) common choice is to set tmin to the first content token after the last role or delimiter token, and set tmax to the last generated content token before any stop token. Degenerate masked sets. depth. This case is handled by the subspace fitting fallback described in Appendix A.5. If {t : Mt = 1} is empty for sample, then Aj receives no directions from that sample at any A.5. Subspace fitting details and degenerate windows This appendix specifies how the sampled competitor directions in Aj (Eq. 5) are converted into the window matrix Dj and the orthonormal basis Uj (Eq. 6). It also defines deterministic fallback when Aj is empty or numerically degenerate. 15 Submission and Formatting Instructions for ICML 2026 Direction pool for window j. Let Aj = {at,b,y = (cid:98)yt,b wy : Wj, Ct,b, Mt = 1} Rd be the masked competitor direction set for window j. Row normalization and clipping. For each Aj, form normalized direction = a2 + εa , (24) with fixed εa > 0. This removes scale variability induced by readout-row norms and makes the SVD capture dominant angular structure rather than magnitude outliers. Optionally, we discard when a2 < τa for fixed threshold τa > 0, since such directions can be dominated by numerical noise. We choose τa on the same numerical scale as εa. Capping and sampling. Let Nj = Aj be the number of eligible directions for window j. Fix cap cap 1. If Nj cap, use all normalized directions. If Nj > cap, sample subset of size cap without replacement using deterministic seed seed = hash(sample id, j, seed0), with fixed seed0, ensuring reproducibility across runs. Denote the resulting ordered list by (a(1), . . . , a(Mj )), with Mj cap. Window matrix. Stack the sampled normalized directions as rows: Dj = (a(1)) ... (a(Mj )) RMj d. (25) This is the matrix whose right singular vectors define the readout-aligned window subspace. SVD and basis extraction. Compute thin SVD Dj = (D) ΣjV , Σj = diag(σj,1, . . . , σj,r), σj,1 σj,r > 0, where = rank(Dj) min{Mj, d}. Write the right singular vectors as Vj = [vj,1, . . . , vj,d]. Extract the top-k right singular vectors To improve numerical stability, we re-orthonormalize (cid:101)Uj = [vj,1, . . . , vj,k] Rdk. (cid:101)Uj = QjRj (thin QR), Uj = Qj, (26) (27) so that basis with deterministic orthonormal complement as described below. Uj = Ik up to numerical precision. If < k, we extract only the available singular vectors and complete the Degenerate windows and deterministic fallback. window is degenerate if Mj = 0 after masking and filtering. Otherwise, it is degenerate if rank(Dj) = 0. In this case, Dj contains no usable direction signal, so Uj cannot be identified from competitor geometry. We define deterministic fallback basis fb as follows. Fix once global reference orthonormal matrix Rdd. One convenient choice is the identity = Id. Define fb = [g1, . . . , gk], (28) where = [g1, . . . , gd]. Set Uj = fb signatures when competitor geometry is unavailable. for degenerate windows. This choice is deterministic and yields well-defined Submission and Formatting Instructions for ICML 2026 Rank-deficient completion. When 0 < < k, let Uj,1:r be the extracted right singular vectors. Complete to columns by projecting to the orthogonal complement of span(Uj,1:r) and taking the first (k r) directions, followed by thin QR: (cid:98)G = (I Uj,1:rU j,1:r)G, (cid:98)Uj = (cid:2)Uj,1:r (cid:98)G:,1:(kr) (cid:3), Uj = qf( (cid:98)Uj), where qf() denotes the factor of thin QR decomposition. Deterministic reproducibility. All stochasticity can be removed by using the deterministic fallback and deterministic subsampling order. When randomized subsampling is used for cap, the seed must be fixed and recorded, and the same seed must be used across all runs for comparability. A.6. Orthogonal transport, Procrustes optimality, and corner cases This appendix formalizes the orthogonal transport used to align adjacent window frames (Sec. 3.3.1) and states its Procrustes optimality. It also records deterministic choices for corner cases where the overlap between adjacent subspaces is weak. Setup. Let U, Rdk have orthonormal columns, representing two window bases (e.g., = Uj and = Uj+1). Consider the orthogonal group O(k) = {R Rkk : RR = Ik}. Transport definition. Compute the compact SVD = ΣQ, P, Rkk orthogonal, Σ = diag(σ1, . . . , σk), 0 σi 1. Define the transport = O(k). (29) (30) This is the orthogonal factor used in the main text as Rjj+1. Lemma A.3 (Procrustes optimality of the transport). Let U, Rdk have orthonormal columns and let be defined by Eqs. 2930. Then and equivalently Moreover, the minimum value satisfies arg min RO(k) RF , arg max RO(k) tr(cid:0)RV (cid:1). min RO(k) R2 = 2k (cid:88) i=1 σi, where {σi} are the singular values of . Proof. Expand (31) (32) (33) R2 = tr(U ) + tr(RV R) 2 tr(RV ) = 2k 2 tr(RV ), so minimizing the Frobenius distance is equivalent to maximizing tr(RV ) over O(k), giving Eq. 32. Using the SVD = ΣQ, tr(RV ) = tr(RP ΣQ) = tr(cid:0)(P RQ)Σ(cid:1). Let = RQ O(k). Then tr(SΣ) = (cid:80)k = Ik, which corresponds to = = R. Substituting the maximizing trace into the expansion yields Eq. 33. i=1 σi, since Sii 1 and σi 0. Equality is achieved by i=1 σiSii (cid:80)k Interpretation. The transport is the closest orthogonal change of coordinates between the two k-frames in the leastsquares sense. Applying aligns coordinates from the -frame to the -frame while minimizing window-switch-induced rotation artifacts in k-space. 17 Submission and Formatting Instructions for ICML If has distinct singular values and full rank, then is unique. When singular Uniqueness and sign ambiguities. values repeat, the SVD factors P, are not unique, but any valid choice yields transport that attains the same Procrustes optimum in Eq. 31. This non-uniqueness corresponds exactly to the within-subspace gauge freedom and does not affect gauge-invariant signatures. Weak overlap and numerically unstable cases. When adjacent subspaces have weak overlap, some singular values of can be close to 0, and the Procrustes transport = can become numerically unstable. We therefore use deterministic reset rule based on the smallest overlap. Fix threshold τσ (0, 1) and compute the compact SVD in Eq. 29. Let σmin = mini{1,...,k} σi. We define the stabilized transport Rstab = (cid:40) R, σmin τσ, σmin < τσ. Ik, (34) This rule treats weak-overlap window switch as complete frame reset. The resulting discontinuity is captured by the drift metrics in Sec. 3.3.3. In the main text, Rjj+1 refers to by default, with Eq. 34 applied deterministically when overlap is weak. A.7. Rotation-equivariant robust centering in k-space This appendix fixes the per-step center µb Rk used in Eq. 10 and records sufficient conditions under which the centered increment norm pc t,b2 is invariant to within-window basis rotations, as required by Lemma 3.2. Problem statement. For fixed sample and depth step b, let Tb be the set of eligible token indices (those with Mt = 1). Given transported increments {pt,b Rk : Tb}, we seek center map that is robust to outliers and equivariant under rotations in Rk. µ : (Rk)Tb Rk, µb = µ({pt,b}tTb ), Rotation equivariance. center rule µ() is rotation-equivariant if, for every orthogonal O(k), µ({Qvt}t) = Qµ({vt}t). (35) If Eq. 35 holds, then the centered residuals rotate consistently: (Qvt) µ({Qvt}t) = Q(vt µ({vt}t)), so Euclidean norms vt µ()2 are preserved. Default choice: masked coordinate-wise median. We choose µb as the coordinate-wise median over the eligible set: (µb)i = median (cid:16) {(pt,b)i : Tb} (cid:17) , = 1, . . . , k. (36) We use the convention that masked entries are ignored and an all-masked set returns µb = 0. Computation via Weiszfeld iterations. When Tb 1, we compute an approximate geometric median by Weiszfelds algorithm. Initialize µ(0) as the Euclidean mean of {pt,b}tTb . For = 0, 1, . . . , 1, update µ(r+1) = (cid:80) tTb (cid:80) w(r) pt,b w(r) tTb , w(r) = pt,b µ(r) 1 2 + εµ , (37) with fixed εµ > 0 for numerical stability. Set µb = µ(R) return that point, which is valid minimizer. . If any iterate coincides with an input point (within tolerance), we 18 Empty mask. If Tb = , define µb = 0 Rk and skip centering for that step. Submission and Formatting Instructions for ICML 2026 Alternative equivariant centers. The following center rules satisfy the rotation-equivariance property in Eq. 35 because they depend only on Euclidean distances and are invariant under orthogonal changes of coordinates: Euclidean mean: µb = 1 Tb (cid:80) tTb pt,b. Geometric median: any µb arg minu (cid:80) tTb Huber-type radial center: any µb arg minu pt,b u2. (cid:80) tTb ψ(pt,b u2) for radial loss ψ. Distance-based trimmed mean: discard fixed fraction of points farthest from the mean (or another equivariant pilot) using Euclidean distances, then average the remainder. In contrast, the coordinate-wise median (used in our implementation) is robust but is not rotation-equivariant for general O(k). Invariance of centered increment norms. Suppose within-window basis change induces an orthogonal coordinate transform in k-space, so that t,b = Qpt,b for some O(k) and all eligible at step b. If the chosen center rule is rotation-equivariant (Eq. 35), then centering commutes with the rotation: pc t,b = t,b µ({p t,b}t) = Qpt,b Qµ({pt,b}t) = Qpc t,b, and therefore the centered norms are invariant, pc t,b2 = pc t,b2. This is the sufficient condition used in Lemma 3.2. In our experiments we center by the masked coordinate-wise median for robustness; this choice does not satisfy Eq. 35 in general, but it matches the implementation and the reported results. A.8. Drift metric properties and simple bounds This appendix records basic properties of the drift quantities in Sec. 3.3.3. We relate the projector spectral drift to principal angles, and we give simple bounds for the anchor-coupled drift χt,j in Eq. 14. dG(U, ) = V 2 Projectors and principal angles. Let U, Rdk have orthonormal columns, and let PU = and PV = be the associated orthogonal projectors. Let θ1 θk [0, π/2] be the principal angles between span(U ) and span(V ). Equivalently, if has singular values σ1 σk in [0, 1], then Lemma A.4 (Spectral projector drift equals sin(θmax)). With notation above, σi = cos(θi). In particular, 0 dG(U, ) 1, and dG(U, ) = 0 if and only if the two subspaces coincide. PU PV 2 = sin(θmax), θmax = θk. (38) Proof sketch. This is standard identity for orthogonal projectors onto k-dimensional subspaces. One route is to use principal-angle coordinate system in which PU PV decomposes into independent 2 2 blocks with eigenvalues sin(θi), so the spectral norm equals maxi sin(θi). Useful corollaries. The following elementary bounds are often convenient. Frobenius drift: PU PV 2 = 2 (cid:80)k Trace overlap: tr(PU PV ) = V i=1 sin2(θi). = (cid:80)k i=1 cos2(θi). Overlap lower bound: if σmin(U ) α, then dG(U, ) 19 1 α2. Submission and Formatting Instructions for ICML 2026 Anchor-coupled drift and bounds. Recall Eq. 14: χt,j = (Pj+1 Pj) ht,b ht,b 2 + ε 2 , Pj = UjU . We first record deterministic rule for the anchor j , then state simple bounds. Anchor selection. We use the window start as the anchor, = bj, (39) with bj = (j 1)s from Appendix A.3. This rule is deterministic and ensures each anchor belongs to its own window. Lemma A.5 (Simple bounds for χt,j). For any nonzero Rd and any two orthogonal projectors P, Q, (P Q)v2 v2 + ε Q2. Consequently, for χt,j, and also 0 χt,j dG(Uj, Uj+1), χt,j 1. (40) (41) Proof. By submultiplicativity, (P Q)v2 Q2 v2, and since v2/(v2 + ε) 1, the displayed inequality follows. Apply it with = Pj+1 and = Pj to obtain Eq. 40. Eq. 41 follows from Pj+1 Pj2 1 for orthogonal projectors onto equal-dimensional subspaces. Interpretation. The geometry-only drift dG(Uj, Uj+1) measures the worst-case mismatch direction between the two adjacent k-subspaces via the maximum principal angle. The anchor-coupled drift χt,j measures how much of that subspace change is expressed along the actual visited anchor state ht,b , and the bound χt,j dG(Uj, Uj+1) ensures the state-coupled drift cannot exceed the geometry-only drift. A.9. Refinement via Readout-Aligned Step Clamping This appendix defines the single-block refinement operator used after the validator localizes culprit depth event. Implementation choices such as competitor selection, basis caching, calibration span, and stopping are documented separately in Appendix B.11. Target block and intervention position. Given flagged sample, the validator identifies culprit event index and its associated coordinates (t0, b0) = (tj , bj ) as described in Sec. 3.5. Refinement keeps the prefix x1:t0 fixed and regenerates the remaining suffix while modifying the internal update only at block b0. Local readout-aligned subspace at block b0. Let RV be the readout matrix with rows {wy}V y=1. At the monitored boundary of block b0, let hin, hout Rd denote the block input and output at the current decoding position (using the same monitored state convention as in Sec. 3.1). local orthonormal basis Rdk is fitted from readout-row difference directions between the current top token and set of competitors, with denoting the subspace dimension and the number of competitors (Appendix B.11). We assume = Ik. (42) k-space coordinates and transported step. Project the states to k-space (43) To allow distinct bases on the two sides of the block, introduce an orthogonal transport Rkk. The transported step and its norm are pout = hout Rk. pin = hin Rk, = pout pin Rk, = p2. (44) In our default setting we use the same basis on both sides, so = Ik. 20 Submission and Formatting Instructions for ICML 2026 Reference scale from the fixed prefix. calibration pass on the fixed prefix yields robust reference step scale sref > 0 computed from short span of prefix positions. The aggregation rule and span selection are specified in Appendix B.11. Upper clamp and subspace-only rewrite. Given clamp ratio α > 1, define α sref + ε We rewrite only the component in span(U ) and preserve the orthogonal complement: = λ p, λ = min 1, , (cid:16) (cid:17) pout = pin + p. The intervention replaces hout by hout only at block b0 and only at the current decoding position. hout = hout + (cid:0)pout pout(cid:1). (45) (46) Scope. The operator applies only for suffix generation after position t0 and only at single block b0. All other blocks and all earlier prefix positions remain unchanged. B. Implementation Details B.1. Deterministic conventions and numerical constants All runs follow fixed conventions for reproducibility. We fix single random seed per run and apply it consistently to Python, NumPy, and PyTorch. We also use single safeguard constant εnum > 0 in every denominator involving an ℓ2 norm in the flow pipeline and validator features. Boundary normalization constant. The monitored boundary applies the models pretrained normalization module, either LayerNorm or RMSNorm depending on the architecture. We denote its built-in epsilon by εbn and take εbn directly from the pretrained configuration. All statements that use normalization band in Lemma 3.1 and Appendix A.1 refer to this boundary normalization and its εbn. B.2. Token masks and validity priority Each sequence uses two binary masks: the tokenizer attention mask an,t for token validity, and an eligibility mask mn,t that selects positions used in aggregation and centering. The attention mask has strict priority, so any position with an,t = 0 is treated as invalid. All pooling and centering operate only on tokens with an,t = 1 and mn,t = 1. B.3. Robust masked aggregation over depth We aggregate per-step ratio features over depth into token-level summaries using masked median. For each sample and token t, we take the median of {rn,t,b} over depth steps that are valid under the token masks and lie within the effective monitored range (b Bn,eff ). If no valid depth step exists for token, we set its summary to 0. This robust reduction stabilizes summaries under outlier steps and variable effective depths. B.4. Transported increments and robust masked centering For each sample and depth step b, we center transported increments pn,t,b Rk across eligible tokens to remove tokenshared shifts, so pooled motion reflects token-specific deviations. Using the eligible set Tn = {t : an,t = 1, mn,t = 1}, we subtract robust masked center in k-space computed by coordinate-wise median: µn,b[i] = median(cid:0){pn,t,b[i] : Tn}(cid:1), {1, . . . , k}, pn,t,b = pn,t,b µn,b, with the convention µn,b = 0 when Tn = . This is the centering used in all experiments. Note that coordinate-wise medians are robust but are not rotation-equivariant for general within-window orthogonal basis changes; using rotation-equivariant center (e.g., Euclidean mean or geometric median) is drop-in alternative if strict equivariance is desired. B.5. Safe normalization for ratio features All ratio features use shared numerical safeguard εnum > 0 to avoid instability when norms are small. Whenever norm appears in denominator, we use v2 + εnum. This rule is applied to direction normalization in k-space and to all norm-based ratios involving projected updates or residual terms. 21 Submission and Formatting Instructions for ICML 2026 B.6. Moving subspace and competitor hyperparameters Unless noted otherwise, we use window length = 8 and stride = 4 for the moving subspace assignment in Sec. 3.2. For competitor directions and subspace fitting, we use = 32 competitors and fit = 16 basis in the primary detection experiments, matching the refinement setting in Appendix B.11. B.7. Quadrature and Jacobian vector products Path-integrated updates use Jacobian vector products of the boundary normalization map at depth b+1. We compute these products via automatic differentiation without forming explicit Jacobians. For the integral over α [0, 1], we use fixed three-node Simpson-type rule with nodes {0, 0.5, 1}. B.8. Validator inputs Each sample provides depthwise feature grid with token validity. We represent one file payload as xgrid RN Beff F , with samples, Beff monitored depth steps, token length , and feature dimension . Token validity follows the tokenizer attention mask and is applied uniformly across depth steps by masking invalid token positions before any reduction. To form sequence input for the validator, we pad within minibatch to common shape (Bmax, Tmax) and linearize the depthtoken grid into single event axis using fixed index order. This yields an event tensor evt RM LF , = BmaxTmax, with matching binary mask evt valid {0, 1}M indicating valid events after padding. All downstream computations treat masked events as absent. B.9. Validator architecture and masked pooling The validator takes the event sequence evt together with evt valid. We first apply feature-wise LayerNorm to stabilize scale across feature channels, then map each event vector to an embedding through lightweight MLP. GRU processes the resulting event sequence, and the final decision aggregates per-event logits through mask-aware pooling operator that ignores invalid events. We use either max pooling or logsumexp pooling over valid event positions. Pooling is applied after the GRU so that event ordering can influence the hidden dynamics prior to aggregation. Default hyperparameters. Unless noted otherwise, the event encoder uses two-layer MLP with hidden size 256, embedding size de = 128, and dropout 0.1, followed by an output LayerNorm. The GRU uses hidden size dh = 256 with single recurrent layer. B.10. Training and evaluation procedure We train and evaluate validators independently for each (task, base LLM) pair. For each pair, we split the corresponding serialized flow files into train and test with ratio 8:2, and all reported numbers use the held out test split. During both training and testing, we keep only samples whose labels fall in {0, 1}. Batches that contain no remaining binary labeled samples after filtering are skipped and counted. We optimize all validator parameters with AdamW using learning rate 3 105 and weight decay 102 for 300 epochs. Optional gradient clipping uses global norm threshold 1.0. Optional mixed precision uses autocast with GradScaler, controlled by run flag. To address class imbalance, we use weighted binary cross entropy on logits. We scan the training split once, count binary labels, and set the positive class weight to the ratio of negative to positive counts, falling back to weight 1.0 when no positives appear. For evaluation, we convert logits to probabilities with sigmoid. Accuracy uses fixed threshold 0.5. AUROC uses predicted probabilities and binary labels. B.11. Flow guided refinement intervention We apply refinement only to samples labeled as hallucinations (y = 1) under the evaluation protocol used in Table 2. For each selected sample, the extractor provides an event sequence with per-event scores and coordinates (tj, bj) over token 22 Submission and Formatting Instructions for ICML 2026 index and depth. We select the culprit event by masked maximization over valid events and use its coordinates (t0, b0) to determine the intervention token position and the single targeted block. Refinement keeps the prefix x1:t0 fixed and regenerates the remaining suffix while intervening at exactly one transformer block b0. The intervention rewrites only the component inside readout-aligned k-dimensional subspace and preserves the orthogonal complement. We use fixed competitor count and subspace dimension k, and we freeze the competitor set and fitted basis after calibration. We estimate reference step scale sref from the fixed prefix by aggregating transported step norms over bounded span. We set sref to the masked median of p2 over the first Ncal valid prefix steps (default Ncal = 64). During suffix generation, if the transported step norm exceeds an upper band relative to sref , we apply shrink-only update that preserves direction in the transported coordinates. Unless noted otherwise, we use = 32 competitors to fit = 16 readout-aligned basis, estimate sref from up to 64 prefix steps, and use an upper-band clamp ratio α = 1.05. B.12. External judge protocol for hallucination labels We obtain binary hallucination labels from ChatGPT based external judge conditioned on the same context and the base model output. We use fixed prompt template across all tasks and base LLMs, and we use deterministic decoding for the judge. The judge emits labels in {1, 0, 1}, and we keep only labels in {0, 1} for training and evaluation. C. Flow Analyze Details This appendix reports the supporting statistics for Each task on HaluEval, focusing on three views per model: (i) top group dominance (top1 fraction), (ii) group magnitude via Grad times Input group mass, and (iii) hotspot depth distribution. We also summarize label aligned and error mode aligned behaviors. C.1. QA Task This section summarizes how the flow signatures behave on HaluEval QA and why the validator succeeds or fails across model families. The most consistent cue is depth localized gate in the transported coordinates pt,b. Predicted positives tend to collapse into narrow depth band where multiple group masses rise together, while missed hallucinations more often remain in late, diffuse regime that looks similar to non hallucination. Module top1 dominance can shift by label for some models, but it is less reliable than the depth gate and the coupled mass pattern. Overall pattern. Across models, QA predicted positives frequently form single depth gate with strong multi group co activation. Depth gate drives prediction. Predicted positives concentrate at one shallow or late depth index, depending on the base model. TP and FP overlap in magnitude. When the gate activates, group mass often saturates across motion, attention, MLP, competitor, and drift, so and look similar in magnitude. FN looks TN like. Missed hallucinations typically stay in late, diffuse regime where hotspot depth and dominance resemble . C.1.1. QWEN2.5 Key pattern. Predicted positives collapse to an early gate at depth = 1. Within = 1, detected hallucinations deviate from pure motion dominance, with higher attention and drift shares. Missed hallucinations remain motion dominated and resemble . Operating point. Hallucination prevalence: 15.5%. Predicted positive rate: 23.4%. Precision: 38.8%. Recall: 58.4%. Depth localization. The early gate dominates prediction: at = 1 is 100.0% and at = 1 is 99.6%. hotspots spread across mid to late depth with representative peaks at = 18 (12.7% of ), = 19 (11.6%), and = 20 (9.0%). has only small early gate share at = 1 (8.7% of ) and otherwise stays late and diffuse. 23 Submission and Formatting Instructions for ICML 2026 Dominance and mass. Motion top1 drops from 95.4% in = 0 to 87.5% in = 1, while attention rises from 3.1% to 7.2% and drift rises from 0.3% to 2.0%. Within = 1, further reduces motion dominance (motion 79.2%, attention 12.4%, drift 3.4%), whereas stays motion dominant (99.2%), close to (motion 98.8%). Group mass separates mainly by prediction: predicted positives sit near 0.20 across groups, while predicted negatives remain small (T 0.013, 0.024). C.1.2. GEMMA2 Key pattern. strong early gate at depth = 0 snaps predicted positives into single spike. Multi group mass saturates at the gate, producing heavy overlap between and . Module dominance carries little signal. Operating point. Hallucination prevalence: 15.0%. Predicted positive rate: 41.1%. Precision: 22.2%. Recall: 60.8%. Depth localization. The gate dominates prediction: at = 0 is 100.0% and at = 0 is 99.6%. By label, = 0 accounts for 67.9% of = 1 and 44.0% of = 0, so early depth correlates with label but does not isolate it. has weaker early gate share (b = 0 at 18.1% of ) and keeps late tail. Dominance and mass. Top1 dominance stays near motion saturation (motion 99.6% in = 1, 99.5% in = 0). Group mass follows prediction: predicted positives saturate at large values (for example, motion 0.43) while remains smaller (motion 0.046), and closely matches across groups. C.1.3. PHI 3 Key pattern. Phi 3 behaves like depth and token gate. Predicted positives concentrate at depth = 0 and the earliest tokens, while missed hallucinations drift toward later depth and later token hotspots closer to . Operating point. Hallucination prevalence: 13.0%. Predicted positive rate: 63.2%. Precision: 14.8%. Recall: 71.9%. Depth and token localization. Depth gating is absolute for prediction: at = 0 is 100.0% and at = 0 is 100.0%. By label, = 0 covers 84.6% of = 1, while still contains substantial = 0 portion (30.7% of ) and then spreads broadly into later depths. splits, with = 0 at 45.1% of and strong late tail into the high 20s. Token hotspots mirror the gate: predicted positives concentrate at token 0 and 1 (T : token 0 at 55.5%, token 1 at 39.6%; : token 0 at 52.7%, token 1 at 43.4%), while peaks later (token 2 at 25.7%) and also peaks at token 2 (40.8%). Dominance and mass. Module dominance does not separate quadrants, motion top1 is 100.0% in all quadrants. Group mass appears quantized by prediction (predicted positives near 0.20, near 0.09, near 0.061), so the depth and token gate description is more stable than label level dominance. C.1.4. LLAMA3 Key pattern. The decisive gate shifts late. Predicted positives concentrate around sharp spike near depth = 21, and the same spike often appears in , limiting label recoverability from depth alone. Operating point. Hallucination prevalence: 15.1%. Predicted positive rate: 36.6%. Precision: 24.2%. Recall: 58.6%. Depth localization. Predicted positives concentrate at = 21 (T : 62.1% at = 21, : 57.0% at = 21). By label, = 21 accounts for 42.1% of = 1 and 23.7% of = 0, which produces label shift with strong overlap. Dominance and mass. Top1 dominance is saturated for = 1 (motion 100.0%), and = 0 contains only tiny non motion shares (attention 0.41%, competitor 0.06%). Group mass follows prediction, with predicted positives near 0.20 across groups and predicted negatives smaller (T 0.025, 0.043). C.1.5. MISTRAL Key pattern. Mistral forms late depth spike gate in the = 20 to 22 neighborhood. When the spike appears, predicted positives rise, but shares the same spike. Missed hallucinations remain diffuse and like. Operating point. Hallucination prevalence: 14.1%. Predicted positive rate: 51.6%. Precision: 16.9%. Recall: 61.9%. Depth localization. Predicted positives concentrate in late depth: peaks at = 21 (28.7% of ) and = 20 (17.2%), and mirrors the same locations (b = 21 at 30.8% of , = 20 at 18.1%). hotspots are thicker in earlier mid 24 Submission and Formatting Instructions for ICML depth (around = 14 to 18), while spreads across = 16 to 20 without locking to = 21. Dominance and mass. Top1 dominance stays near motion saturation by label (y = 1: motion 98.2%, attention 1.4%; = 0: motion 99.0%, attention 0.41%), and both and are fully motion dominant (motion 100.0%), matching the failure mode where resembles . Group mass follows prediction, with predicted positives near 0.20 and predicted negatives very small (T 0.011, 0.019). C.2. General Task This section summarizes how the flow signatures behave on HaluEval General prompts. The most consistent cue is depth localized regime bend in the transported coordinates pt,b. At narrow depth band, the transported step becomes larger and more curved, and several readout aligned groups rise together, producing composite event rather than single module trigger. Depending on the base model, the description is dominated either by coupled multi group co activation or by hard hotspot depth gate that snaps many predicted positives to an early depth. Across models, error modes follow the same internal regimes: false positives often share the early burst signature, while false negatives more often remain in late, low mass regimes close to true negatives. Overall pattern. Across models, General predicted positives tend to be explained by localized composite event, with two recurring failure modes. Composite burst. narrow depth neighborhood shows joint rise in transported motion and co activation across groups (motion, attention, MLP, competitor, drift), often accompanied by stronger drift concentration. Depth gate. Some models collapse predicted positives into fixed early hotspot depth, so depth location separates prediction more reliably than module decomposition. FP and FN regimes. often inherits the early burst or gate behavior, while often remains late and diffuse, resembling . C.2.1. QWEN2. Key pattern. Hallucination aligns with coupled composite event. Transported motion increases together with attention, MLP, competitor, and drift, so the most stable narrative is joint mass growth rather than any single top1 group. Dominance, magnitude, and depth. Top1 fractions vary sharply across splits, so dominance alone does not summarize behavior. representative contrast shows with large co activation (motion 0.232, attention 0.155, MLP 0.289, competitor 0.271, drift 0.052), while stays uniformly small (motion 0.014, attention 0.008, MLP 0.019, competitor 0.019, drift 0.003), producing large multiplicative gaps across groups (roughly 14 to 19). Hotspots for hallucination related samples pile up in shallow neighborhood around = 4, while non hallucination spreads broadly with nontrivial mid and late depth frequency. C.2.2. GEMMA2 Key pattern. Gemma2 forms hard early gate at = 1 that snaps predicted positives to single hotspot depth. Depth location dominates the description, while module decomposition contributes little. Operating point and depth gate. Predicted positives account for 19.0% of samples, and the predicted positive set is heavily false positive dominated (85.4% of predicted positives are ). Every predicted positive has hotspot fixed at = 1 (T at = 1 is 100.0% and at = 1 is 100.0%). Predicted negatives show broad hotspot spread, with covering the full depth range and placing comparatively more hotspots at later depths. Dominance note. Top1 is almost always motion for all splits, so top1 fractions provide limited leverage. The early depth gate separates prediction rather than ground truth, so early depth alone cannot recover the label. Late depth patterns, especially those concentrated in , should be stated explicitly to avoid overstating separability. C.2.3. PHI 3 Key pattern. Hotspot depth provides the most reliable axis. Predicted positives concentrate at depth 0, while predicted negatives shift late. Missed hallucinations follow the late pattern closer to . 25 Submission and Formatting Instructions for ICML 2026 Operating point and depth structure. Hallucination prevalence is 7.7%, predicted positives account for 35.8%, with precision 13.8% and recall 63.8%. Predicted positives concentrate at depth 0 (62.8% of predicted positives), with secondary accumulation at depth 30 (10.6%). follows the same early gate (depth 0 at 61.4% of ). Predicted negatives concentrate in late depths (mainly = 23 to 28), and follows this late pattern more often, matching the missed detection behavior. Dominance and magnitude note. Top1 dominance stays near motion with small attention minority, and group masses for and appear nearly uniform around 0.2 across groups, so positional localization is more stable than fine grained ratio interpretation. C.2.4. LLAMA3 Key pattern. Both labels occupy late depth, but hallucination shifts earlier and increases the frequency of MLP top1 cases. Error modes align with two regimes: an earlier burst regime associated with and late accumulation regime associated with . Operating point and depth shift. Hallucination prevalence is 13.1%, predicted positives account for 12.8%, with precision 28.7% and recall 28.0%. By depth bands, hallucination allocates more mass to early depth (depth 0 to 7 at 25.4% for = 1 versus 10.5% for = 0), while non hallucination concentrates more strongly in late depth (depth 20 to 30 at 56.8% for = 1 versus 76.4% for = 0). Dominance, magnitude, and regimes. Top1 fractions differ by label: = 1 has motion 81.4% and MLP 18.6%, while = 0 has motion 93.2% and MLP 6.8%, so MLP top1 occurs about 2.7 more often in hallucination. Group mass increases jointly across groups for hallucination, with multiplicative gap of about 2.5 to 2.9 relative to non hallucination. Regimes align with errors: hotspots stay in depth 0 to 19, with depth 0 to 7 taking 60.6% of , while concentrates in depth 20 to 30 (78.8% of ), matching the late, like regime. C.2.5. MISTRAL Key pattern. Mistral shows the clearest bifurcation. Hallucination aligns with an early depth, drift dominant, high mass composite event, while non hallucination aligns with late depth, motion dominant, low mass behavior. inherits the hallucination like signature and inherits the non hallucination like signature. Operating point and dominance. Hallucination prevalence is 11.0%, predicted positives account for 24.6%, with precision 17.3% and recall 38.8%. By label, top1 fractions shift toward drift for hallucination (motion 56.1%, drift 37.8% for = 1) compared with non hallucination (motion 70.4%, drift 28.5% for = 0). By confusion split, predicted positives are drift dominated (T : drift 81.6%, : drift 94.5%), while predicted negatives are motion dominated (T : motion 90.6%, : motion 90.0%). Magnitude and depth. Mass separates sharply: stays small (motion 0.022, drift 0.045), while rises across groups and peaks strongly in drift and competitor (motion 0.197, competitor 0.259, drift 0.415), and shows similar profile (competitor 0.274, drift 0.445). Hotspot depth mirrors the bifurcation. lies entirely in early depth 0 to 10, with depth 0 taking 57.9% and depth 10 taking 26.3%. is also early (depth 0 to 10 at 96.2%, depth 0 at 50.0%). shifts late (depth 20 to 30 at 67.6%, with depth 27 and 28 each at 10.6%), and follows the same late pattern (depth 20 to 30 at 75.0%, with depth 27 and 28 each at 13.3%). Occlusion delta note. Occlusion delta outputs are all zero in the current run, so the conclusions rely on top1 fraction, group mass, and hotspot depth. C.3. Summarization Task This section summarizes Summarization flow patterns on HaluEval. The most informative cue is whether the transported trajectory in the moving readout-aligned frame undergoes depth-localized collapse that concentrates energy into narrow depth neighborhood. In the collapse regime, the transported increment becomes locally large and more curved, and multiple readout-aligned components rise together, producing gate-like event that strongly correlates with predicted positives. The complementary regime is diffuse, low-energy band spread across depth, which dominates predicted negatives and often also contains missed hallucinations. Across models, the key limitation is consistent: the gate can drive the validator commitment, but it does not certify factual correctness, since and frequently share the same collapse regime, while Submission and Formatting Instructions for ICML 2026 hallucinations that remain diffuse overlap with non hallucination and become . Overall pattern. Across models, Summarization is well described by two geometric regimes. Gate or collapse regime. The path compresses into narrow depth slab with joint multi-group activation, yielding strong and overlap inside predicted positives. Diffuse regime. The path remains spread across depth with small projected magnitudes, so often looks -like and separability weakens. C.3.1. QWEN2.5 Key pattern. Prediction is dominated by single depth gate. Predicted positives enter collapse regime with strong joint activation, so and become nearly indistinguishable within the predicted positive set. Operating point in %. Predicted positives are 75.0% of samples. Within predicted positives, is 43.1% and is 56.9%. Gate regime. Predicted positives show co-activation across groups, with mean masses around motion 0.47, attention 0.18, MLP 0.19, competitor 0.15, and drift 0.008. The depth shape collapses tightly, concentrating near depth 14 with weight 0.94 and small secondary support near depth 13 with weight 0.05. This matches sharp fold where entering depth 14 compresses the trajectory into thin slab inside the moving window frame. Diffuse regime. Predicted negatives remain low energy and spread across depth. stays small (for example motion 0.028) and remains small as well (for example motion 0.044). Hotspots do not lock to single depth neighborhood, and resembles under this view. Takeaway. For Qwen2.5 Summarization, the depth-14 gate plus joint activation drives predicted positives, while correctness inside predicted positives remains unresolved because and share the same regime. C.3.2. GEMMA2 Key pattern. Non hallucination is typically smooth and band-like across depth. Hallucination mixes gate subset that collapses and co-activates with diffuse subset that overlaps with non hallucination, naturally producing missed cases. Non hallucination geometry. Trajectories remain broad and smooth across depth without collapsing into single depth neighborhood. Hotspots spread rather than over concentrate, and bending accumulates gradually instead of forming sharp fold. Hallucination mixture. One subset shows depth localized pinch where the trajectory narrows sharply and folds, with local rises across multiple groups. Another subset remains late and diffuse without clear gate, keeping depth and dominance patterns close to non hallucination. The gate subset explains to overlap, while the diffuse subset explains . Takeaway. Gemma2 Summarization is best presented as two regimes, gate driven subset and diffuse subset that overlaps with non hallucination. C.3.3. PHI 3 Key pattern. Phi 3 separates by whether the trajectory enters depth gate that pinches and folds. Without the gate, trajectories remain in thin, low-curvature tube and many remain close to . Diffuse tube regime. For non hallucination and many missed hallucinations, the transported curve stays confined to narrow tube aligned with the dominant motion direction. Curvature remains small and the path progresses without decisive pinch. Gate regime. Typical detected hallucinations enter localized pinch where the ribbon folds, hotspots concentrate near specific depths, and multiple groups rise together around the gate. and overlap because the regime reflects internal dynamics rather than correctness. Takeaway. Phi 3 Summarization reduces to gate versus no gate geometry, and the no gate tube explains why often remains close to . Submission and Formatting Instructions for ICML 2026 C.3.4. LLAMA3 Key pattern. LLaMA3 exhibits mixture. Many non hallucinations remain smooth and dispersed across depth. Hallucinations more often introduce localized gate with joint activation, while diffuse subset lacks clear gate and overlaps with non hallucination. Non hallucination geometry. Trajectories progress with wide band shape. Hotspots remain distributed and do not concentrate into single depth neighborhood. Gate subset and diffuse subset. In the gate subset, energy compresses into narrow depth band, turning increases locally, and multiple groups co-activate in the same neighborhood, including drift and competitor pressure. In the diffuse subset, the trajectory remains spread across depth with no decisive fold, so overlap with non hallucination is strong and missed cases are expected. Takeaway. LLaMA3 Summarization is best stated as gate driven subset plus diffuse subset that overlaps with non hallucination. C.3.5. MISTRAL Key pattern. Mistral contrasts smooth, thin band against localized deformation where step magnitude and turning increase in narrow depth neighborhood. The localized event drives predicted positives and yields to overlap, while diffuse cases remain closer to non hallucination. Non hallucination band. Non hallucination typically forms thin, smooth band in transported coordinates with gradual rotation and no sharp pinch. Localized deformation and diffuse mode. Hallucination introduces localized event where step magnitude spikes and turning increases, producing sharp bend and local concentration. diffuse mode also appears where deformation spreads across depth, making trajectories more similar to non hallucination and increasing ambiguity. Takeaway. Mistral Summarization follows the same regime view. localized deformation drives predicted positives and to overlap, while samples without clear localized event remain diffuse and can become missed cases. C.4. Dialogue Task This section summarizes Dialogue flow patterns on HaluEval. In Dialogue, label-averaged module dominance is frequently saturated and therefore unreliable for interpretation. The most stable axis is mixture of depth-localized regimes in transported coordinates: some samples exhibit high-mass burst concentrated in narrow depth band, while others remain in low-mass diffuse regime that spreads across later depths and overlaps strongly across labels. The burst regime is characterized by locally larger transported steps and sharper bends together with joint rise of readout-aligned components, whereas the diffuse regime shows weaker concentration and small magnitudes. Across models, the decisive factor is not the average top1 group fraction but the depth band of the burst and the mixture proportion between burst and diffuse cases. This explains both successes and failure modes: false positives often share the same burst regime as detected hallucinations, while missed hallucinations often remain in the diffuse regime that resembles non hallucination. Overall pattern. Dialogue is best described by shared two-regime mixture. Burst regime. Hotspots collapse to narrow depth neighborhood, and multiple readout-aligned components rise together, producing strong internal cue that drives predicted positives. Diffuse regime. Hotspots spread to later depths with low mass, creating heavy overlap across labels and persistent source of ambiguity. Label differences typically appear as shifts in regime proportions rather than clean structural separation. C.4.1. QWEN2.5 Key pattern. Qwen2.5 exhibits minority early-burst regime and majority late-diffuse regime, and the late-diffuse regime overlaps strongly across labels. 28 Submission and Formatting Instructions for ICML 2026 Mixture proportions and supporting statistics. Hallucination prevalence is 51.2%. At the label level, top1 dominance is nearly identical for both labels (motion 98%), so module fractions do not separate. Within = 1, the early-burst regime accounts for 12.9% and concentrates at shallow depth (depth 0 to 6 at 83.2%, with = 4 alone at 65.6%). In this regime, motion top1 decreases to 93.1% while attention (3.8%), MLP (2.3%), and competitor (0.8%) appear more often, and group mass rises jointly to near saturation ( 0.20 across groups). The late-diffuse regime accounts for 87.1% of = 1, peaks near = 20, stays motion-dominant (motion 98.8%), and closely matches motion (98.4%). Its magnitude is very small (representatively 0.0068 and 0.0080). The same split appears in = 0, with an early-burst subset at 9.7% that matches the early-burst depth and saturation pattern. C.4.2. GEMMA2 Key pattern. Gemma2 is dominated by two regimes: very late-depth, high-mass explosion regime and low-mass diffuse regime. Both regimes appear in both labels, and label differences are mainly mixture proportions. Operating point and dominance. Hallucination prevalence is 39.8% and the predicted positive rate is 43.0%, with precision 43.4% and recall 46.8%. Top1 dominance is fully saturated to motion in all splits (motion 100%), so dominance does not explain label differences. Mixture structure and localization. Within = 1, the high-mass explosion regime accounts for 46.8% and shows near-saturation mass (motion 0.947 with small dispersion), while hotspot depth shifts to very late neighborhoods with peaks at = 30 (27.3% of this regime) and = 34 (16.8%), plus substantial mass into the high 30s. Token hotspots concentrate early, with tokens 2 to 6 covering 32.0% (token 2 at 8.1%, token 3 at 7.4%, token 5 at 8.4% within this regime). The low-mass diffuse regime accounts for 53.2% of = 1 and overlaps strongly with the non hallucination diffuse regime, with hotspots spread across late teens to early 20s and representative peaks at = 19 and = 23. The same mixture appears in = 0: high-mass late regime at 40.3% that matches the = 1 explosion pattern, and low-mass diffuse regime at 59.7% with hotspots concentrated around = 15 to 23. Occlusion delta note. Occlusion delta outputs are all zero in the current run, so conclusions rely on group mass and hotspot distributions. C.4.3. PHI Key pattern. Phi 3 separates primarily by depth band and magnitude regime, not by module dominance. Both labels mix an early-to-mid regime with co-activated mass and late regime with small mass. Mixture proportions and supporting statistics. Hallucination prevalence is 52.1%. At the label level, motion top1 fractions are nearly identical (90.4% for = 1 and 90.3% for = 0). Within = 1, the early-to-mid regime accounts for 46.8% and places hotspots frequently in depth 0 to 15 with mode at = 14. In this regime, motion top1 drops to 84.3% while attention (9.3%) and MLP (4.7%) increase, and group mass rises jointly near 0.20. The late regime accounts for 53.2% with mode near = 27, and magnitude stays small (roughly 0.056 to 0.059). = 0 shows the same mixture with smaller early share (35.6%) and larger late share (64.4%). C.4.4. LLAMA3 Key pattern. LLaMA3 exhibits an overwhelmingly dominant depth-0 burst regime in both labels. Label differences are therefore primarily proportional rather than structural. Mixture proportions and supporting statistics. Hallucination prevalence is 43.8%. Top1 dominance is motion 100% across labels. For = 1, the burst regime accounts for 87.0% and fixes hotspots at depth 0 (100.0% within the regime). Token hotspots concentrate at token 0 and 1 (token 0 at 84.7%, token 1 at 15.3%), and group mass rises jointly to 0.19 to 0.205. The D. Experiments details D.1. Hallucination Detection Table 3 reports per-class accuracy for the hallucination detector, separating Normal Accuracy on non-hallucinated samples (y=0) and Hallucination Accuracy on hallucinated samples (y=1). The results reveal strong class asymmetries that are 29 Submission and Formatting Instructions for ICML 2026 Table 3. HaluEval per-class accuracy (%) across tasks and model families. We report Normal Accuracy for non-hallucinated samples (y=0) and Hallucination Accuracy for hallucinated samples (y=1). Normal Accuracy Hallucination Accuracy Task QA General Summarization Dialogue Qwen 2.5 Gemma 2 73.08 70.78 40.08 86.43 72.44 89.60 43.50 66.78 Phi-3 58.45 69.73 35.46 63. LLaMA3 Mistral Qwen 2.5 Gemma 2 74.31 73.62 19.46 32.22 62.60 70.89 37.74 64.72 70.49 66.67 71.27 16.09 56.38 39.34 90.23 39.92 Phi-3 60.99 57.97 77.34 50. LLaMA3 Mistral 53.86 58.16 75.55 42.58 62.25 56.78 88.21 73.74 Figure 5. Training curves on HaluEval test data across four tasks. Epoch-wise validator performance over training for QA, General, Summarization, and Dialogue across base model families. largely task dependent, and Fig. 5 visualizes how these task level behaviors emerge over training via the epoch wise performance curves across model families. For QA, the detector remains relatively balanced across classes, with comparable accuracies for y=0 and y=1 across model families, and the QA curves in Fig. 5 rise smoothly and then plateau, indicating stable learnability. For General, performance on y=0 is consistently high, while y=1 accuracy drops sharply for some models, indicating that hallucinated answers in open-ended prompts are harder to separate from fluent but weakly grounded responses; correspondingly, Fig. 5 shows rapid early gains followed by saturation. In contrast, Summarization and Dialogue exhibit pronounced inversions: Summarization shows low y=0 accuracy but high y=1 accuracy, suggesting tendency to over-flag summaries as hallucinated, while the Summarization curves in Fig. 5 remain near chance or improve only modestly, consistent with skewed decision boundary. Dialogue often shows very high y=0 accuracy paired with low y=1 accuracy for several models, indicating missed hallucinations under dialogue-specific ambiguity and history conditioning, and Fig. 5 reflects this with lower plateaus and, for some models, non-monotonic training dynamics. These per-class trends help explain the near-chance overall scores on Summarization and Dialogue in Table 1, where skewed class-wise behavior can mask substantial bias toward one class. D.2. Refinement Comparison Table 4 evaluates whether the gains of our refinement come from what we do (a single-block clamping intervention) or where we do it (the depth selected by flow localization). The Regeneration baseline isolates the effect of simply producing new continuation under the same decoding setup, without any clamping intervention. The Random Depth baseline keeps the clamping intervention identical to ours but applies it at uniformly random transformer depth, testing whether improvements arise from generic perturbation rather than correct depth targeting. Flow Guided applies the same clamping intervention at the depth localized by the flow-signature validator. Across all tasks and model families, flow guidance yields the lowest hallucination ratio, indicating that the localized depth signal is informative. The clearest separation appears in QA: flow-guided refinement consistently outperforms both 30 Submission and Formatting Instructions for ICML 2026 Table 4. Hallucination ratios on HaluEval (%). We report the fraction of generations labeled as hallucinations for each task and model under four settings: Initial (original output), Regeneration (regenerate without any hidden-state intervention), Random Depth (apply the same single-block intervention at randomly chosen depth), and Flow Guided (apply the intervention at the culprit depth localized by the flow-signature validator). Lower is better. Task Model Initial Regenration Random Depth Flow Guided QA General Summarization Dialogue Qwen 2.5 Gemma 2 Phi-3 LLaMA3 Mistral Qwen 2.5 Gemma 2 Phi-3 LLaMA3 Mistral Qwen 2.5 Gemma 2 Phi-3 LLaMA3 Mistral Qwen 2.5 Gemma 2 Phi-3 LLaMA3 Mistral 15.25 14.90 12.65 15.10 14.05 11.96 6.76 7.64 13.07 10. 40.90 39.80 42.15 45.80 49.70 50.95 39.95 51.90 43.60 44.50 12.40 14.00 8.30 10.80 13.24 10.96 6.31 7.56 12.52 10.58 40.70 38.80 41.30 45.60 47.80 49.20 36.55 48.65 39.30 42. 13.40 12.65 7.77 10.50 12.40 10.12 6.55 7.23 12.34 10.42 40.40 38.90 41.60 44.90 48.30 49.90 38.70 48.00 38.10 42.85 10.95 12.34 6.55 7.70 11.70 8.75 5.76 6.31 11.74 10. 40.20 37.10 40.70 44.60 47.25 48.10 34.95 46.30 34.30 39.05 regeneration and random-depth intervention (e.g., Qwen 2.5: 15.2510.95; LLaMA3: 15.107.70; Phi-3: 12.656.55). Regeneration alone provides partial reductions for some models but is notably weaker, while random-depth clamping helps in several cases yet remains reliably worse than targeting the flow-localized depth. For General, improvements are smaller but consistent: flow guidance reduces hallucination ratios for every model, and the gap to random depth remains visible (e.g., Qwen 2.5: 10.128.75). Summarization and Dialogue start from much higher hallucination ratios, and all methods yield modest absolute reductions, yet the same ordering persists: flow-guided is best, regeneration is typically next, and random-depth is weakest. This suggests regime where single-block refinement has limited headroom, while correct depth selection still provides measurable benefit when intervention is effective (e.g., Dialogue LLaMA3: 43.6034.30; Dialogue Gemma 2: 39.9534.95). Overall, the comparison supports two conclusions. First, the gains cannot be explained by re-generating alone, since flowguided refinement consistently improves over regeneration. Second, depth selection is essential: random-depth clamping underperforms flow guidance across the board, confirming that the validators depth localization is key factor for effective single-block refinement."
        }
    ],
    "affiliations": [
        "Department of Computer Science, University of California, Irvine"
    ]
}