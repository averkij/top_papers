{
    "paper_title": "WildGraphBench: Benchmarking GraphRAG with Wild-Source Corpora",
    "authors": [
        "Pengyu Wang",
        "Benfeng Xu",
        "Licheng Zhang",
        "Shaohan Wang",
        "Mingxuan Du",
        "Chiwei Zhu",
        "Zhendong Mao"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Graph-based Retrieval-Augmented Generation (GraphRAG) organizes external knowledge as a hierarchical graph, enabling efficient retrieval and aggregation of scattered evidence across multiple documents. However, many existing benchmarks for GraphRAG rely on short, curated passages as external knowledge, failing to adequately evaluate systems in realistic settings involving long contexts and large-scale heterogeneous documents. To bridge this gap, we introduce WildGraphBench, a benchmark designed to assess GraphRAG performance in the wild. We leverage Wikipedia's unique structure, where cohesive narratives are grounded in long and heterogeneous external reference documents, to construct a benchmark reflecting real-word scenarios. Specifically, we sample articles across 12 top-level topics, using their external references as the retrieval corpus and citation-linked statements as ground truth, resulting in 1,100 questions spanning three levels of complexity: single-fact QA, multi-fact QA, and section-level summarization. Experiments across multiple baselines reveal that current GraphRAG pipelines help on multi-fact aggregation when evidence comes from a moderate number of sources, but this aggregation paradigm may overemphasize high-level statements at the expense of fine-grained details, leading to weaker performance on summarization tasks. Project page:https://github.com/BstWPY/WildGraphBench."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 2 ] . [ 1 3 5 0 2 0 . 2 0 6 2 : r WildGraphBench: Benchmarking GraphRAG with Wild-Source Corpora February 3, Pengyu Wang1, Benfeng Xu1,2, Licheng Zhang1, Shaohan Wang1, Mingxuan Du1, Chiwei Zhu1, Zhendong Mao1 1University of Science and Technology of China, Hefei, China 2Metastone Technology, Beijing, China {wangpengyu, benfeng, zlczlc}@mail.ustc.edu.cn, zdmao@ustc.edu.cn Graph-based Retrieval-Augmented Generation (GraphRAG) organizes external knowledge as hierarchical graph, enabling efficient retrieval and aggregation of scattered evidence across multiple documents. However, many existing benchmarks for GraphRAG rely on short, curated passages as external knowledge, failing to adequately evaluate systems in realistic settings involving long contexts and large-scale heterogeneous documents. To bridge this gap, we introduce WildGraphBench, benchmark designed to assess GraphRAG performance in the wild. We leverage Wikipedias unique structure, where cohesive narratives are grounded in long and heterogeneous external reference documents, to construct benchmark reflecting real-word scenarios. Specifically, we sample articles across 12 top-level topics, using their external references as the retrieval corpus and citation-linked statements as ground truth, resulting in 1,100 questions spanning three levels of complexity: single-fact QA, multi-fact QA, and section-level summarization. Experiments across multiple baselines reveal that current GraphRAG pipelines help on multi-fact aggregation when evidence comes from moderate number of sources, but this aggregation paradigm may overemphasize high-level statements at the expense of fine-grained details, leading to weaker performance on summarization tasks. Project page: https://github.com/BstWPY/WildGraphBench."
        },
        {
            "title": "Introduction",
            "content": "Retrieval-augmented generation (RAG) grounds LLM outputs by retrieving evidence from an external corpus(Lewis et al., 2021), but it may struggle when scattered evidence must be extracted from multiple documents and integrated into coherent answer. Recently, Graph-based RAG (GraphRAG) (Peng et al., 2024) has gained increasing attention as paradigm that builds graph over documents or chunks and performs graph-guided expansion and aggregation for multi-document evidence assembly and long-context reasoning (Zhang et al., 2025). Many GraphRAG methods have been proposed, exploring different graph constructions and retrieval strategies. Microsoft GraphRAG(Edge et al., 2025) builds document-level graphs and supports local-to-global aggregation for query-focused summarization, while LightRAG(Guo et al., 2025) improves evidence coverage by coupling an entity relation graph with vector retrieval and multi-stage expansion. In addition, some works attempt to improve practicality. Fast-GraphRAG(CircleMind-AI, 2024) focuses on efficiency and adopts lightweight graph retrieval pipeline to reduce indexing and querying costs. HippoRAG2(Gutiérrez et al., 2025) further extends evidence access by introducing an external knowledge graph and filtering mechanism. LinearRAG(Zhuang et al., 2025) enhances the practicality by adopting lightweight hierarchy and propagation-style ranking for scalable indexing and retrieval. These strategies enable structured evidence expansion and aggregation, allowing LLMs to address more complex queries. However, current GraphRAG benchmarks still rely on short, curated passages. As result, retrieval and generation in long-context settings with large-scale, heterogeneous document collections remain under-tested. Yet this setting is central to real-world applications and is also where GraphRAG is expected to be most beneficial. In addition, many benchmarks make the task closer to lookup-and-stitch: once few pre-trimmed passages are retrieved, system can answer by simple concatenation or light paraphrasing rather than true multi-document aggregation. Traditional datasets such as HotpotQA (Yang et al., 2018), 2WikiMultiHopQA (Ho et al., 2020), and MultiHop-RAG (Tang and Yang, 2024) Work done during the internship at Metastone. Corresponding author. Preprint. Work in progress. Figure 1: Why we use Wikipedia references as wild evidence. Wikipedia articles are concise summaries with citationlinked statements. The linked reference pages are often long, noisy, and heterogeneous (e.g., news sites, blogs, PDFs, and public reports). This mismatch makes evidence retrieval and verification harder. typically follow this assumption. Even some recent benchmarks extend to longer domain documents, e.g., UltraDomain (Qian et al., 2025), they still assume cleaner document boundaries and less heterogeneous sources than wild corpora. For GraphRAG specifically, few dedicated benchmarks have recently been proposed (Xiao et al., 2025; Xiang et al., 2025), providing controlled protocols and corpora; however, their corpora remain more structured and less heterogeneous than in-the-wild settings. This gap calls for tasks that test reliable aggregation under long contexts and heterogeneous, uncurated sources. To bridge this gap, we introduce WildGraphBench, benchmark that targets the in-the-wild scenario of GraphRAG, where system is expected not only to retrieve evidence from long, heterogeneous corpora, but also to synthesize an answer whose correctness depends on assembling scattered support across multiple sources rather than handful of pre-trimmed passages. We instantiate this setting using Wikipedia: each article provides concise entry with citation-linked statements, while its external reference pages form long, heterogeneous web corpus (Figure 1). We therefore sample Wikipedia articles from 12 top-level topics, using each articles reference pages as the retrieval corpus and treating citation-linked Wikipedia statements as ground-truth facts. We create 1,100+ questions in three types, as shown in Figure 2, spanning single-fact lookup, multi-fact evidence aggregation, and section-level summarization, which together stress the spectrum from precise retrieval to broad factual coverage. Our contributions are as follows: We construct WildGraphBench, dataset based on Wikipedia references that reflects real-world noise and complexity. We design three question types: single-fact, multi-fact, and section-level summary.Then we introduce statement-grounded evaluation method for single-fact, multi-fact, and summary questions. Experiments on multiple methods show that GraphRAG improves multi-fact aggregation but struggles with broad summary tasks. 2 Figure 2: Example instances in WildGraphBench: (1) single-fact questions grounded by single gold statement and one reference, (2) multi-fact questions requiring evidence aggregation across multiple statements/references, and (3) section-level summary questions evaluated at the statement level."
        },
        {
            "title": "2 Related Work",
            "content": "Retrieval-Augmented Generation. Retrieval-augmented generation (RAG) answers query by retrieving related text from an external corpus, then generating based on that text (Lewis et al., 2021). common setup uses retriever (e.g., BM25) plus an LLM reader (Robertson and Zaragoza, 2009; Chen et al., 2017). Prior work shows retrieval can reduce hallucination compared to direct generation, especially when evidence is needed (Shuster et al., 2021). Graph Retrieval-Augmented Generation. Graph retrieval-augmented generation (GraphRAG) builds graph over chunks or documents and retrieves evidence through graph operations, aiming to capture complex dependencies that flat retrieval might miss. Microsoft GraphRAG (Edge et al., 2025) introduces modular pipeline that uses community detection algorithms to generate hierarchical summaries, supporting both local entity queries and global thematic answering. LightRAG (Guo et al., 2025) constructes dual-level entityrelation graph and couples it with vector retrieval, allowing for multi-stage evidence expansion across low-level entities and high-level concepts. Focusing on efficiency, Fast-GraphRAG (CircleMind-AI, 2024) adopts lightweight design with optimized indexing strategies to significantly reduce the computational overhead of graph maintenance. HippoRAG2 (Gutiérrez et al., 2025) draws inspiration from the human brain, introducing an external knowledge graph and memory-style retrieval mechanism powered by Personalized PageRank (PPR) to filter noise and discover multi-hop paths. Furthermore, LinearRAG (Zhuang et al., 2025) targets large-scale scalability by utilizing linear-complexity propagation ranking method, overcoming the bottleneck of traditional graph algorithms. These methods collectively demonstrate that structured graph traversal can significantly enhance evidence aggregation compared to flat baselines. Benchmarks for RAG and GraphRAG. Several benchmarks study retrieval and multi-hop reasoning under different evidence settings. HotpotQA(Yang et al., 2018), 2WikiMultiHopQA(Ho et al., 2020), and MultiHop-RAG(Tang and Yang, 2024) focus on multi-hop question answering. UltraDomain(Qian et al., 2025) evaluates retrieval and generation over longer domain corpora. GraphRAG-Bench(Xiao et al., 2025) provides corpora and protocols tailored to graph-based retrieval. Additionally, Xiang et al. (Xiang et al., 2025) systematically analyze the scenarios where graph structures provide clear benefit over flat retrieval. Despite these advances, there remains gap for benchmark that simultaneously stresses long-context processing, noise robustness, and multi-document aggregation in wild, unstructured web setting."
        },
        {
            "title": "3 WildGraphBench",
            "content": "In this section, we describe how WildGraphBench is constructed. As shown in Figure 3, the framework has three phases. First, we collect reference pages and extract citation-linked statements from Wikipedia leaf sections, producing the Wikipedia gold corpus. Based on the extracted corpus, we then build three types of questions: single-fact, multi-fact, and section-level summary. Finally, we introduce statement-grounded evaluation method. 3 Figure 3: Three-phase workflow of WildGraphBench after data collection. Phase 1:citation-aware statement extraction, producing the Wikipedia gold corpus. Phase 2: design single-fact, multi-fact, and section-level summary questions. Phase 3: evaluate with statement-grounded accuracy and statement-level precision/recall/F1."
        },
        {
            "title": "3.1 Data Collection",
            "content": "We start from 12 high-level Wikipedia topics1 and within each topic select articles with large number of references, as these articles tend to have dense and diverse citation structures. For each article, we collect all reference URLs and fetch the original web pages using jina.ai.2. Crucially, if the original page fails but an archive exists, we use the archived page to ensure data completeness. We keep the raw page text, including boilerplate and noise, to simulate the wild retrieval environment."
        },
        {
            "title": "3.2 Statement Extraction",
            "content": "We use citation-linked Wikipedia statements as the ground-truth factual units for evaluation. .To extract statements, we first split each Wikipedia article into leaf sections using simple regex parser over Wiki markup. Each leaf section has section path (e.g., Donald Trump > Political Career > Impeachments). In each leaf section, we identify sentences containing citation markers. For each sentence, an LLM rewrites it into clean factual statement by removing footnote markers and fixing local coreference issues. We also parse the Wiki markup to retrieve the exact reference URLs."
        },
        {
            "title": "3.3 Wikipedia Gold Corpus Construction",
            "content": "We align the references by matching Wikipedia reference URLs to the crawled pages. If cited sentence is missing any referenced page, we drop it to ensure quality. The resulting Wikipedia gold corpus is organized at the granularity of leaf sections, where each section stores list of triples: = (statement, ref _urls, ref _count) (1) where statement is the normalized factual statement, ref _urls is the set of reference URLs associated with the original cited sentence, and ref _count is the number of references. This corpus serves as the authoritative source of gold statements for evaluation."
        },
        {
            "title": "3.4 Question Design",
            "content": "We design three types of questions on top of the Wikipedia gold corpus. The reference count ref _count attached to each Wikipedia triple determines whether it is used for single-fact or multi-fact question, while leaf sections themselves serve as the basis for summary questions  (Table 1)  . 1Source: Wikipedia:Contents. It lists 13 top-level topics: Culture, Geography, Health, History, Human activities, Mathematics, Nature, People, Philosophy, Religion, Society, Technology, and Reference. We drop Reference. It mainly describes how to write and cite Wikipedia pages, rather than content domain. 2https://jina.ai 4 Table 1: Statistics of Question Types in WildGraphBench. The dataset consists of 1,197 questions distributed across three distinct categories."
        },
        {
            "title": "Question Type",
            "content": "Single-Fact Multi-Fact Summary"
        },
        {
            "title": "Count",
            "content": "667 191 339 1,197 If triple has ref _count = 1, we use it to generate single-fact question. Given the article Single-fact questions. title, the section path, the original source sentence, and the cleaned statement, we prompt an LLM to write question whose answer is exactly that statement (up to minor paraphrasing). The prompt specifically encourages the model to include multiple constraints (e.g., entity, time, and location) and discourages copying long spans from the statement, ensuring that the question is non-trivial but still tightly aligned with the gold statement and its supporting evidence. If triple has ref _count 2, we treat it as candidate for multi-fact question. Intuitively, Multi-fact questions. these statements are supported by multiple references and often describe relationships that span several sources. We again condition an LLM on the article title, section path, and statement to generate question that requires recovering that statement. In addition, we enforce strict multi-reference check: for each such triple, we ask an LLM judge whether any single reference alone is sufficient to support all key facts in the statement, and only keep those triples for which at least two references are jointly required. This yields questions that genuinely test models ability to aggregate evidence from multiple documents. Section-level summary questions. For summary questions, we operate at the level of leaf sections. For given leaf section, we collect all valid triples under that section and deduplicate their statements to obtain the gold statement set S. We then prompt an LLM to generate natural information-seeking question based on the article title and the section path, but not on the section text itself, so that the question is phrased independently of the exact wording of Wikipedia. The expected answer to such question is the set S, i.e., the factual content of the leaf section. During evaluation, systems are required to retrieve and summarize information under noisy, long-context evidence conditions."
        },
        {
            "title": "3.5 Evaluation Metrics",
            "content": "We evaluate systems differently for single-fact/multi-fact questions and for summary questions, but in all cases the gold answer is defined at the level of factual statements. Single-fact and multi-fact accuracy. For single-fact and multi-fact questions, each instance is associated with exactly one gold statement s. Given system answer ˆa and the relevant evidence, an LLM judge decides whether ˆa is factually equivalent to under the evidence. We assign score of 1 if the answer is correct and 0 otherwise, and report accuracy separately for single-fact and multi-fact questions. Statement-level score for summary. For summary questions, the gold answer is the statement set extracted from the corresponding leaf section. Given system output, we run statement extractor to obtain set of predicted statements ˆS = {ˆs1, . . . , ˆsk}. We then define binary matching function Match(s, ˆs) {0, 1}, which returns 1 if ˆs is correct paraphrase of (i.e., it conveys the same fact), and 0 otherwise. Using this match, we compute statement-level recall and precision as: Recall = 1 Match(s, ˆs) max ˆs ˆS Precision = max sS Match(s, ˆs) (cid:88) sS (cid:88) ˆs ˆS 1 ˆS (2) (3) The F1 score is the harmonic mean of precision and recall. This metric directly measures factual coverage (recall) and hallucination rate (precision), while remaining robust to minor paraphrasing."
        },
        {
            "title": "4 Experiments",
            "content": ""
        },
        {
            "title": "4.1 Experimental Settings",
            "content": "Implementation Details For methods requiring pre-chunking, we segment documents with chunk size 1200 tokens and overlap 100 tokens. At retrieval time, we set top_k = 5 for single-fact and multi-fact questions, and top_k = 10 for summary questions. To align generation and graph construction across systems, we use gpt-4o-mini as the default model for graph construction and answering. For evaluation, we use gpt-5-mini as the LLM judge to score single-fact and multi-fact accuracy and to compute statement-level precision/recall/F1 for summary questions. Evaluated Methods We evaluate WildGraphBench on representative flat-RAG and GraphRAG-style baselines. For flat-RAG, we include NaiveRAG (Lewis et al., 2021) and BM25 (Robertson and Zaragoza, 2009). For GraphRAG-style methods, we evaluate Fast-GraphRAG (CircleMind-AI, 2024), Microsoft GraphRAG (local/global) (Edge et al., 2025), LightRAG (hybrid) (Guo et al., 2025), LinearRAG (Zhuang et al., 2025), and HippoRAG2 (Gutiérrez et al., 2025)."
        },
        {
            "title": "4.2 Main Results",
            "content": "Table 2: Main results on WildGraphBench. While graph-based methods show clear advantages on multi-fact questions requiring aggregation, flat baselines like NaiveRAG remain competitive on single-fact retrieval and achieve higher recall on summary tasks due to broader context coverage. Method NaiveRAG BM25 Fast-GraphRAG HippoRAG2 Microsoft GraphRAG(local) Microsoft GraphRAG(global) LightRAG(hybrid) LinearRAG Question Answering Summary Avg. Acc. Single-fact Acc. Multi-fact Acc. Recall Precision 59.79 36.83 33.56 64.33 38.23 54.54 56.76 44.87 66.87 41.38 35.83 71.51 39.43 56.52 61.32 47.53 35.08 20.94 25.65 39.27 34.03 47.64 40.84 35. 13.54 9.38 6.81 11.15 9.82 12.66 12.44 5.81 19.07 19.46 23.48 16.76 12.64 15.13 17.7 29.2 15.84 12.66 10.56 13.39 11.05 13.78 14.61 9. Table 2 reports the overall performance on WildGraphBench and the people subset. We observe consistent pattern across both parts. On the relatively simple single-fact questions, flat retrieval-augmented baselines remain highly competitive. In particular, NaiveRAG achieves strong accuracy, outperforming most graph-based variants on the Novel Dataset; among the GraphRAG-style methods, only HippoRAG2 attains higher single-fact accuracy (71.51 vs. 66.87), suggesting that graph retrieval does not automatically translate into gains when the answer can often be supported by single salient chunk. BM25 is also competitive, and in some cases surpasses several graph methods, indicating that keyword matching remains strong prior for straightforward fact lookup. The advantage of graph-based retrieval becomes more visible on harder tasks. For multi-fact questions, which require aggregating evidence from multiple references, Microsoft GraphRAG(global) achieves the best accuracy(47.64), and several graph variants are comparable to or better than NaiveRAG and BM25. This implies that structured traversal / global context aggregation can help when evidence is scattered and must be combined, while purely flat top-k pipeline is more likely to miss complementary pieces of information. For summary questions, all methods obtain low statement-level scores, highlighting the difficulty of reconstructing leaf sections factual content from long, noisy evidence. Notably, NaiveRAG achieves the highest recall and the best F1 on our Dataset  (Table 2)  . plausible explanation is that summary questions demand broad coverage: re trieving wider variety of raw evidence chunks can directly increase the chance that the generator sees more gold facts, improving recall and thus F1. In contrast, many GraphRAG-style systems introduce additional bottlenecksentity/relation extraction, graph sparsification, neighborhood summarization, and traversal budgetswhich may degrade under web noise and long contexts. When graph construction is imperfect (missing entities/edges) or when traversal/summarization budgets are limited, these methods can fail to scale their evidence gathering to the breadth required by section-level summaries, resulting in lower recall and weaker overall F1 even if they sometimes improve precision via filtering. This suggests that scaling GraphRAG to summary-style tasks under wild evidence requires more robust graph construction and higher-capacity aggregation, rather than relying on graph structure alone. Overall, these results indicate that GraphRAG is not always advantageous on easy questionsit can be more expensive than NaiveRAG or BM25 without clear gains for single-fact lookup. While GraphRAG shows promising Table 3: Results on the people subset of WildGraphBench, compared with human performance. Method NaiveRAG BM25 Fast-GraphRAG HippoRAG2 Microsoft GraphRAG(local) Microsoft GraphRAG(global) LightRAG(hybrid) LinearRAG Human performance Question Answering Summary Ave. Acc. Single-fact Acc. Multi-fact Acc. Recall Precision 65.82 65.2 30.43 64.89 35.16 56.81 74.42 45.26 85.66 76.62 74.03 33.77 72.73 38.96 62.34 80.52 51.95 89. 28.12 34.38 18.75 37.5 21.88 37.5 53.12 21.88 71.88 10.48 5.74 1.48 7.63 4.59 5.52 5.56 1.52 38. 15.29 16.98 22.83 15.69 9.17 14.13 15.69 22.51 12.62 F1 8.03 5.03 1.62 6.14 2.98 5.41 4.73 1. 15.3 improvements on multi-fact aggregation, summary questions remain challenging under noisy long-context evidence, and method design must carefully balance coverage (recall) versus filtering (precision) under fixed retrieval/compute budget."
        },
        {
            "title": "4.3 Graph Analysis",
            "content": "Figure 4 and Table 4 compare graph connectivity across datasets under the same LightRAG construction pipeline. Following Xiang et al. (2025), more organized graph should avoid excessive isolated nodes, because graph-based retrieval (e.g., traversal, community aggregation, or PPR-style propagation) relies on connectivity to reach and combine evidence beyond single chunk. Consistent with this criterion, our graph achieves the highest average degree (3.11) and the lowest proportion of isolated nodes (0.14), indicating denser cross-document links and fewer disconnected components. In contrast, the other datasets are structurally less favorable for graph connectivity under the same pipeline. Figure 4: Graph Quality. We build graphs with LightRAG. Left: the fraction of isolated nodes (lower means better connectivity). Right: the average degree (higher means denser links). Table 4: Graph structural statistics across datasets constructed using the LightRAG pipeline. WildGraphBench exhibits significantly higher max degree and denser connectivity, reflecting the complex, hub-centric nature of wild reference pages. Dataset UltraDomain HotpotQA GraphRAG-Bench Ours Tokens 15.1M 1.4M 1.1M 2.4M Degrees 35284 20762 10638 37246 Max Degree 195 50 155 Nodes 33018 28282 11150 23940 7 UltraDomain is constructed from curated long-context domain corpora (e.g., textbooks and domain documents), where content is organized by chapters/sections with cleaner boundaries and less repeated entity co-occurrence across documents; as result, entity mentions are more local, producing more low-degree or isolated nodes. HotpotQA provides short, paragraph-level Wikipedia evidence for multi-hop QA; such contexts are comparatively compact and often concentrate on answering specific question with small set of supporting paragraphs, leading to weaker global entity sharing and thus sparser graph. GraphRAG-Bench exhibits structural sparsity due to domain-specific characteristics across its subsets. The Novel subset features narrative-style text where entities and events are often distributed with strong locality (e.g., confined to specific scenes or chapters), and coreference-heavy writing dilutes explicit entity overlap during extraction. Similarly, the Medical subsetcomposed largely of biomedical guidelines from high terminological density and distinct, nonnarrative structures. The lack of explicit continuity between independent medical documents isolates entities within their specific contexts, while the extraction ambiguity of specialized terms (e.g., drug names) by general-purpose LLMs further exacerbates graph fragmentation, resulting in lower average degrees and more disconnected components across the benchmark. Finally, Table 4 highlights striking max-degree gap: although the UltraDomain is much larger in total token budget than ours, our graph still exhibits dramatically larger max degree. This suggests the presence of hub entities that are repeatedly linked by many distinct pages, i.e., multiple sources converge on the same entity-centric node. Such hub-andspoke patterns make the benchmark substantially harder for retrieval and generation: systems must aggregate partially overlapping evidence from many documents and synthesize coherent answer, directly stressing cross-document multi-source summarization and reasoningthe key capability that WildGraphBench aims to evaluate."
        },
        {
            "title": "4.4 Human Performance",
            "content": "We recruit domain-knowledgeable annotators (graduate-level or above) and ask them to answer questions under the same evidence constraint as RAG systems. Interestingly, we observe distinct behavior in human responses for summary tasks: human annotators tend to prioritize comprehensive coverage of key facts, attempting to include as many details as possible. While this approach sometimes results in lower precisionas models are often more conservative in their generationthe overall F1 score for human performance remains high (e.g., 15.30 F1 on people subset, see Table 3), effectively serving as strong upper-bound reference for the difficulty of evidence aggregation in our benchmark."
        },
        {
            "title": "4.5 Ablation Study",
            "content": "We further investigate the impact of retrieval budget (top-k) on the performance of summary questions. We conduct an experiment using HippoRAG2 on specific domain subset of our dataset, varying the top-k parameter from 2 to 12. As illustrated in Figure 5, the F1 score exhibits an inverted U-shaped trend: it initially increases as grows, reaching peak at = 8, and subsequently declines as increases further to 12. This trend suggests that an optimal retrieval budget is crucial for balancing recall and precision. When top-k is too small (e.g., < 5), the system fails to retrieve sufficient evidence chunks to cover the broad factual content required for summarization, limiting recall. Conversely, when top-k becomes too large (e.g., > 8), the introduction of excessive irrelevant noise or \"distractor\" chunks overwhelms the generators context window or reasoning capability, leading to hallucinations or loss of focus, which degrades the overall F1. This finding emphasizes that retrieval strategies must be carefully tuned to the corpus size and noise level to maximize performance on complex summary tasks."
        },
        {
            "title": "5 Conclusions",
            "content": "Figure 5: Impact of retrieval budget (top-k) on F1 score for summary questions. F1 increases as grows, then drops when is too large. It peaks at = 8. We introduce WildGraphBench, benchmark designed to evaluate GraphRAG on wild-source corpora. Utilizing the heterogeneous reference pages cited in Wikipedia as the retrieval corpus while grounding statements in the Wikipedia articles themselves, we construct three progressively challenging task types to stress-test retrieval, aggregation, and summarization in uncurated environments. Our experiments reveal that while graph-based retrieval offers limited gains over strong flat baselines for simple, single-fact queries, it demonstrates significant advantages on multi-fact 8 questions requiring cross-document evidence aggregation. Conversely, performance on summarization tasks remains low across all methods in this wild setting, highlighting the critical need for more robust evidence acquisition and synthesis mechanisms in real-world scenarios."
        },
        {
            "title": "Limitations",
            "content": "Our benchmark derives gold statements from Wikipedia, which reflects editorial consensus rather than absolute truth; consequently, the gold set may inherit omissions or inaccuracies from Wikipedia and its citations. In addition, our evaluation relies on LLM-based judgment and statement matching, which may introduce systematic biases (e.g., preference for certain phrasing or verbosity) and may not perfectly mirror unbiased human assessment. These factors should be considered when interpreting absolute scores and fine-grained comparisons between methods."
        },
        {
            "title": "Ethical considerations",
            "content": "Data and intended use. WildGraphBench is constructed from Wikipedia articles and their external reference pages; we use citation-linked Wikipedia statements as gold facts and the cited pages as noisy retrieval corpus. We respect the original sources licenses/terms and do not claim ownership over third-party content; any redistribution or derivatives should comply with the original access conditions. We specify WildGraphBenchs intended use as research-only benchmarking for retrieval robustness and multi-document evidence aggregation, and we discourage non-research uses unless explicitly permitted by the source conditions. Risks. Because the corpus contains long, heterogeneous web pages, it may include noise, bias, outdated claims, toxic language, or inadvertently exposed sensitive information. Accordingly, we recommend standard safety practices (e.g., toxicity/PII filtering when appropriate) and emphasize that WildGraphBench evaluates robustness rather than establishing absolute truth."
        },
        {
            "title": "Acknowledgments",
            "content": "We thank the anonymous reviewers and our colleagues for their helpful feedback and discussions."
        },
        {
            "title": "References",
            "content": "Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. 2017. Reading Wikipedia to answer open-domain questions. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 18701879, Vancouver, Canada. Association for Computational Linguistics. CircleMind-AI. 2024. Fastgraphrag: High-speed graph-based retrieval-augmented generation. https://github.com/ circlemind-ai/fast-graphrag. GitHub repository. Darren Edge, Ha Trinh, Newman Cheng, Joshua Bradley, Alex Chao, Apurva Mody, Steven Truitt, Dasha Metropolitansky, Robert Osazuwa Ness, and Jonathan Larson. 2025. From local to global: graph rag approach to query-focused summarization. Preprint, arXiv:2404.16130. Zirui Guo, Lianghao Xia, Yanhua Yu, Tu Ao, and Chao Huang. 2025. Lightrag: Simple and fast retrieval-augmented generation. Preprint, arXiv:2410.05779. Bernal Jiménez Gutiérrez, Yiheng Shu, Weijian Qi, Sizhe Zhou, and Yu Su. 2025. From rag to memory: Non-parametric continual learning for large language models. Preprint, arXiv:2502.14802. Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara, and Akiko Aizawa. 2020. Constructing multi-hop QA dataset for comprehensive evaluation of reasoning steps. In Proceedings of the 28th International Conference on Computational Linguistics, pages 66096625, Barcelona, Spain (Online). International Committee on Computational Linguistics. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen tau Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela. 2021. Retrieval-augmented generation for knowledge-intensive nlp tasks. Preprint, arXiv:2005.11401. Boci Peng, Yun Zhu, Yongchao Liu, Xiaohe Bo, Haizhou Shi, Chuntao Hong, Yan Zhang, and Siliang Tang. 2024. Graph retrieval-augmented generation: survey. Preprint, arXiv:2408.08921. Hongjin Qian, Zheng Liu, Peitian Zhang, Kelong Mao, Defu Lian, Zhicheng Dou, and Tiejun Huang. 2025. Memorag: Boosting long context processing with global memory-enhanced retrieval augmentation. Preprint, arXiv:2409.05591. Stephen Robertson and Hugo Zaragoza. 2009. The probabilistic relevance framework: Bm25 and beyond. Foundations and Trends in Information Retrieval, 3(4):333389. Kurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela, and Jason Weston. 2021. Retrieval augmentation reduces hallucination in conversation. Preprint, arXiv:2104.07567. Yixuan Tang and Yi Yang. 2024. Multihop-rag: Benchmarking retrieval-augmented generation for multi-hop queries. Preprint, arXiv:2401.15391. Zhishang Xiang, Chuanjie Wu, Qinggang Zhang, Shengyuan Chen, Zijin Hong, Xiao Huang, and Jinsong Su. 2025. When to use graphs in rag: comprehensive analysis for graph retrieval-augmented generation. Preprint, arXiv:2506.05690. Yilin Xiao, Junnan Dong, Chuang Zhou, Su Dong, Qian wen Zhang, Di Yin, Xing Sun, and Xiao Huang. 2025. Graphrag-bench: Challenging domain-specific reasoning for evaluating graph retrieval-augmented generation. Preprint, arXiv:2506.02404. Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W. Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. 2018. Hotpotqa: dataset for diverse, explainable multi-hop question answering. Preprint, arXiv:1809.09600. Qinggang Zhang, Shengyuan Chen, Yuanchen Bei, Zheng Yuan, Huachi Zhou, Zijin Hong, Hao Chen, Yilin Xiao, Chuang Zhou, Junnan Dong, Yi Chang, and Xiao Huang. 2025. survey of graph retrieval-augmented generation for customized large language models. Preprint, arXiv:2501.13958. Luyao Zhuang, Shengyuan Chen, Yilin Xiao, Huachi Zhou, Yujing Zhang, Hao Chen, Qinggang Zhang, and Xiao Huang. 2025. Linearrag: Linear graph retrieval augmented generation on large-scale corpora. Preprint, arXiv:2510.10114."
        },
        {
            "title": "Domain",
            "content": "Single-Fact Multi-Fact"
        },
        {
            "title": "Total",
            "content": "Table 5: Per-domain question counts in WildGraphBench."
        },
        {
            "title": "Total",
            "content": "86 41 76 25 83 21 18 77 46 72 66 56 667 37 24 19 1 13 1 0 32 6 4 21 33 191 32 33 55 10 44 11 10 45 18 30 27 24 155 98 150 36 140 33 28 154 70 106 114 339 1,"
        },
        {
            "title": "B Results on every domain",
            "content": "Method NaiveRAG BM25 Fast-GraphRAG HippoRAG2 Microsoft GraphRAG (local) Microsoft GraphRAG (global) LightRAG (hybrid) LinearRAG Table 6: Culture Question Answering Summary Ave. Acc. Single-fact Acc. Multi-fact Acc. Recall Precision 56.91 28.46 20.32 51.22 41.46 52.04 46.34 29.27 67.44 33.72 22.09 62.79 43.02 52.33 52.33 32.56 32.43 16.22 16.22 24.32 37.84 51.35 32.43 21. 12.69 14.58 5.73 11.02 3.39 6.77 14.25 4.69 24.84 24.95 24.64 19.79 12.96 15.54 21.36 25.33 9.79 10.62 2.48 6.68 1.61 1.80 6.93 4. Method NaiveRAG BM25 Fast-GraphRAG HippoRAG2 Microsoft GraphRAG (local) Microsoft GraphRAG (global) LightRAG (hybrid) LinearRAG Table 7: Geography Question Answering Summary Ave. Acc. Single-fact Acc. Multi-fact Acc. Recall Precision 60.00 35. 44.62 73.85 49.23 52.31 58.47 60.00 70.73 43.90 53.66 85.37 60.98 56.10 60.98 65.85 41.67 20.83 29.17 54.17 29.17 45.83 54.17 50.00 10.92 6. 6.00 8.59 11.48 11.36 15.31 5.71 22.83 14.62 31.52 18.77 16.37 18.68 22.07 38.66 F1 7.89 4.81 6.29 6.28 8.66 7.54 9.61 5. Method NaiveRAG BM25 Fast-GraphRAG HippoRAG2 Microsoft GraphRAG (local) Microsoft GraphRAG (global) LightRAG (hybrid) LinearRAG Table 8: Health Question Answering Summary Ave. Acc. Single-fact Acc. Multi-fact Acc. Recall Precision 49.47 32.63 31.58 62.10 31.58 54.74 52.63 40.00 59.21 39.47 36.84 71.05 34.21 60.53 59.21 43.42 12 10.53 5. 10.53 26.32 21.05 31.58 26.32 26.32 20.34 13.70 10.79 16.59 15.60 20.60 17.73 11.88 16.01 19.79 17.23 14.23 10.53 12.41 14.23 27.70 11.15 8. 6.87 9.12 7.01 10.15 9.90 10.95 Method NaiveRAG BM25 Fast-GraphRAG HippoRAG2 Microsoft GraphRAG (local) Microsoft GraphRAG (global) LightRAG (hybrid) LinearRAG Table 9: History Question Answering Summary Ave. Acc. Single-fact Acc. Multi-fact Acc. Recall Precision 61.54 34.62 34.62 73.08 46.15 65.38 69.23 46.15 60.00 36.00 32.00 72.00 44.00 64.00 68.00 44.00 100.00 0.00 100.00 100.00 100.00 100.00 100.00 100. 2.50 2.50 0.00 3.33 0.00 5.00 0.00 0.00 8.88 23.58 23.35 8.06 17.12 12.53 12.49 29.25 Method NaiveRAG BM Fast-GraphRAG HippoRAG2 Microsoft GraphRAG (local) Microsoft GraphRAG (global) LightRAG (hybrid) LinearRAG Table 10: Human Activities Question Answering Summary Ave. Acc. Single-fact Acc. Multi-fact Acc. Recall Precision 81.25 50.00 36.46 79.17 29.17 59.38 64.58 51.04 85.54 51. 37.35 84.34 30.12 61.45 68.67 50.60 53.85 38.46 30.77 46.15 23.08 46.15 38.46 53.85 12.60 10.38 6.88 13.20 6.99 15.85 9.64 4.26 24.20 20. 20.16 18.90 10.74 19.49 21.61 19.97 F1 2.58 2.14 0.00 2.07 0.00 5.45 0.00 0.00 F1 8.09 6. 5.13 7.39 4.41 9.71 7.42 3.71 Method NaiveRAG BM25 Fast-GraphRAG HippoRAG2 Microsoft GraphRAG (local) Microsoft GraphRAG (global) LightRAG (hybrid) LinearRAG Table 11: Mathematics Question Answering Summary Ave. Acc. Single-fact Acc. Multi-fact Acc. Recall Precision F1 45.46 54.54 50.00 54.54 45.46 54.54 59.09 59.09 42.86 57.14 47.62 57.14 47.62 57.14 61.90 61.90 100.00 0.00 100.00 0.00 0.00 0.00 0.00 0.00 31.82 22.73 36.36 27.27 31.82 39.39 40.91 31.82 27.08 31.36 29.13 32.63 23.46 15.48 20.31 33. 20.60 16.06 24.97 20.22 18.23 19.20 22.32 22.11 Method NaiveRAG BM25 Fast-GraphRAG HippoRAG2 Microsoft GraphRAG (local) Microsoft GraphRAG (global) LightRAG (hybrid) LinearRAG Table 12: Nature Question Answering Summary Ave. Acc. Single-fact Acc. Multi-fact Acc. Recall Precision 50.00 38.89 11.11 44.44 0.00 5.56 27.78 22.22 50.00 38.89 11.11 44.44 0.00 5.56 27.78 22.22 0.00 0. 0.00 0.00 0.00 0.00 0.00 0.00 13.33 8.33 0.00 10.00 0.00 0.00 8.33 0.00 23.86 11.87 17.36 3.61 9.55 7.61 11.56 44.01 Method NaiveRAG BM25 Fast-GraphRAG HippoRAG2 Microsoft GraphRAG (local) Microsoft GraphRAG (global) LightRAG (hybrid) LinearRAG Table 13: People Question Answering Summary Ave. Acc. Single-fact Acc. Multi-fact Acc. Recall Precision 62.38 42.20 29.36 62.39 33.95 55.05 58.71 43. 76.62 51.95 33.77 72.73 38.96 62.34 67.53 51.95 28.12 18.75 18.75 37.50 21.88 37.50 37.50 21.88 10.48 3.63 1.48 7.63 4.59 5.52 5.56 1. 15.29 21.43 22.83 15.69 9.17 14.13 15.69 22.51 Method NaiveRAG BM25 Fast-GraphRAG HippoRAG2 Microsoft GraphRAG (local) Microsoft GraphRAG (global) LightRAG (hybrid) LinearRAG Table 14: Philosophy Question Answering Summary Ave. Acc. Single-fact Acc. Multi-fact Acc. Recall Precision 46.15 26.92 50.00 67.31 46.16 65.38 71.16 50.00 50.00 28.26 47.83 69.57 43.48 63.04 69.57 50.00 16.67 16.67 66.67 50.00 66.67 83.33 83.33 50.00 7.41 1.39 2.96 8.52 5.74 7.13 8.52 1.85 8.69 6.17 12.71 13.98 4.44 8.35 8.73 17. F1 6.50 5.06 0.00 2.50 0.00 0.00 2.22 0.00 F1 8.03 2.82 1.62 6.14 2.98 5.41 4.73 1. F1 2.44 0.79 1.01 3.58 1.73 2.28 4.80 2.56 Method NaiveRAG BM25 Fast-GraphRAG HippoRAG2 Microsoft GraphRAG (local) Microsoft GraphRAG (global) LightRAG (hybrid) LinearRAG Table 15: Religion Question Answering Summary Ave. Acc. Single-fact Acc. Multi-fact Acc. Recall Precision 61.84 39.47 34.21 67.11 27.63 50.00 60.52 46.05 59.72 38.89 30.56 68.06 27.78 47.22 58.33 44. 100.00 50.00 100.00 50.00 25.00 100.00 100.00 75.00 12.33 6.14 3.81 8.87 3.45 8.73 9.48 0.48 15.56 17.66 20.06 19.88 10.12 13.37 19.95 29. F1 6.83 3.33 2.50 5.94 1.74 2.65 5.47 0.69 Method NaiveRAG BM25 Fast-GraphRAG HippoRAG2 Microsoft GraphRAG (local) Microsoft GraphRAG (global) LightRAG (hybrid) LinearRAG Table 16: Society Question Answering Summary Ave. Acc. Single-fact Acc. Multi-fact Acc. Recall Precision F1 62.07 28.73 54.02 68.97 50.57 65.52 66.67 50.57 74.24 33. 59.09 78.79 56.06 71.21 75.76 56.06 23.81 14.29 38.10 38.10 33.33 47.62 38.10 33.33 17.31 8.64 12.41 13.64 17.38 14.78 13.70 8.77 19.22 18. 24.93 13.45 10.12 12.39 14.49 28.99 12.89 5.58 9.55 8.92 7.70 8.02 9.91 10.14 Method NaiveRAG BM25 Fast-GraphRAG HippoRAG2 Microsoft GraphRAG (local) Microsoft GraphRAG (global) LightRAG (hybrid) LinearRAG Table 17: Technology Question Answering Summary Ave. Acc. Single-fact Acc. Multi-fact Acc. Recall Precision F1 45.45 33.33 18.18 48.48 51.52 51.52 39.39 45.45 10.87 6. 2.22 6.91 7.57 8.40 9.55 2.22 24.03 22.38 33.77 18.85 23.76 19.96 21.26 36.39 11.91 5.45 2.94 4.92 7.56 7.10 8.68 3.00 57.30 40. 17.98 59.55 43.82 47.19 43.82 47.19 64.29 44.64 17.86 66.07 39.29 44.64 46.43 48."
        },
        {
            "title": "C Prompts for Data Construction",
            "content": "Question Generation: Single-fact You are constructing question for SINGLE-FACT (supported by one citation) citation-based QA dataset. ARTICLE TITLE: {{WIKI_TITLE}} SECTION PATH: {{SECTION_PATH}} WIKI SENTENCE (with inline citations): {{SENTENCE}} CLEAN FACTUAL STATEMENT (this will be used as the reference answer): {{STATEMENT}} REFERENCE URLS (for context, do NOT quote them explicitly): {{REF_URLS_LIST}} Your task: Write ONE natural-language QUESTION in English or Chinese (depending on the style of the article), such that: The gold answer should be exactly the given STATEMENT (possibly with tiny paraphrasing). The question should contain multiple constraints (e.g. entity + time, quantity + condition, entity + location). If any of these constraints was removed, the question would become under-specified or wrong. The question must be answerable solely from the given statement and sentence. The question should feel natural and non-trivial: Do NOT copy any span of 4 or more consecutive words from the sentence or the statement. Avoid generic patterns like What is X?, Who is Y?, When did happen?. Return JSON ONLY: {\"question\": \"...\"} Figure 6: Prompt for Single-fact question generation Question Generation: Multi-fact You are constructing question for MULTI-FACT (requires several citations together) citation-based QA dataset. ARTICLE TITLE: {{WIKI_TITLE}} SECTION PATH: {{SECTION_PATH}} WIKI SENTENCE (with inline citations): {{SENTENCE}} CLEAN FACTUAL STATEMENT (this will be used as the reference answer): {{STATEMENT}} REFERENCE URLS (for context, do NOT quote them explicitly): {{REF_URLS_LIST}} Your task: Write ONE natural-language QUESTION in English or Chinese (depending on the style of the article), such that: The gold answer should be exactly the given STATEMENT (possibly with tiny paraphrasing). The question should contain multiple constraints (e.g. entity + time, quantity + condition, entity + location). If any of these constraints was removed, the question would become under-specified or wrong. The question must be answerable solely from the given statement and sentence. The question should feel natural and non-trivial: Do NOT copy any span of 4 or more consecutive words from the sentence or the statement. Avoid generic patterns like What is X?, Who is Y?, When did happen?. Return JSON ONLY: {\"question\": \"...\"} Figure 7: Prompt for Multi-fact question generation. 16 Question Generation: Summary You are constructing TOPIC-CENTERED SUMMARY QUESTION for topic. TOPIC PATH (broad -> specific): {{SECTION_PATH}} OPTIONAL BODY EXCERPT (for natural phrasing only): {{BODY_EXCERPT}} GOLD STATEMENTS (facts {{GOLD_STATEMENTS_LIST}} Your task: Write ONE natural-language question that asks for concise, encyclopedic-style overview of the MOST that good answer SHOULD cover; do NOT quote them): SPECIFIC topic (typically the LAST 12 elements of the path). Use the GOLD STATEMENTS only as soft guidance to choose what aspects to emphasize, so that the answer naturally tends to cover those facts. The question must remain strongly anchored to the leaf topic in the path. STRICT constraints: DO NOT mention Wikipedia/article/section/heading or similar meta words. DO NOT copy any span of 4+ consecutive words from any gold statement. Avoid leaking specific factual details from the gold statements in the question (especially exact numbers, exact dates, long proper names, or verbatim event descriptions). You may mention high-level aspects (e.g., history, structure, major components, development, reception) if they align with the leaf topic and the gold statements. 20200 characters. Return JSON ONLY: {\"question\": \"...\"} Figure 8: Prompt for Summary question generation."
        },
        {
            "title": "Summary Question Filtering",
            "content": "You are doing POST-HOC VERIFICATION for citation-based summary dataset. ARTICLE TITLE: {{WIKI_TITLE}} LEAF SECTION TOPIC PATH: {{SECTION_PATH}} Leaf topic (most specific): {{LEAF_TOPIC}} You will be given multiple ITEMS. Each item has: STATEMENT (candidate gold statement) several REFERENCES (content excerpts) Task: For EACH item: keep=true only if the REFERENCES (collectively) contain enough information to support ALL key factual claims in the STATEMENT. keep=false if key facts are missing, contradicted, or the references are irrelevant/noisy. Rules: Use ONLY the given references; ignore outside knowledge. Be fairly strict: if unsure due to missing evidence, set keep=false. # [Items are dynamically inserted here] Return \"summary\":\"brief\"} ONLY: JSON {\"items\":[\"idx\":1,\"keep\":true/false,\"reason\":\"brief\"], Figure 9: Prompt for summary question filtering. Single-fact Question Filtering You are checking whether the provided REFERENCES collectively support Q&A. Q: {{QUESTION}} A: {{ANSWER}} REFERENCES (may include some noise, read holistically): {{REFERENCES_CONTENT_BUNDLE}} Rules: If the references together contain the key facts to justify the answer, return supported=true. If key facts are missing or contradicted, return supported=false. Return JSON ONLY (do NOT explain your reasoning process, be concise): {\"supported\": true/false, \"reason\": \"brief\"} Figure 10: Prompt for Single-fact question filtering. 17 Multi-fact Question Filtering You are given factual STATEMENT (used as the reference answer for QA pair), together with the QUESTION and several reference documents cited from Wikipedia. QUESTION: {{QUESTION}} REFERENCE ANSWER (STATEMENT): {{STATEMENT}} REFERENCES: {{REFERENCES_CONTENT_BUNDLE}} Your task is to judge whether these references are jointly necessary to support the FULL factual content of the STATEMENT. Rules: 1. Consider ONLY the information contained in the given references. Ignore any outside world knowledge. 2. For EACH reference individually, imagine you only had that single reference: If that single reference ALONE already contains enough information to support ALL key factual claims in the STATEMENT (numbers, named entities, relationships, important conditions), then that reference is individually sufficient to justify the STATEMENT. 3. If ANY single reference is individually sufficient, then the multi-reference pattern is NOT truly necessary. In this case, set all_needed = false. 4. Only if NO single reference is individually sufficient (each one misses some essential facts), and you really need to COMBINE at least two references to cover the full STATEMENT, set all_needed = true. Key factual claims means the main facts expressed by the STATEMENT, not minor stylistic details. Return JSON ONLY: {\"all_needed\": true/false, \"reason\": \"brief\"} Figure 11: Prompt for Multi-fact question filtering."
        }
    ],
    "affiliations": [
        "Metastone Technology, Beijing, China",
        "University of Science and Technology of China, Hefei, China"
    ]
}