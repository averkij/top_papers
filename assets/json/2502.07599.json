{
    "paper_title": "DPO-Shift: Shifting the Distribution of Direct Preference Optimization",
    "authors": [
        "Xiliang Yang",
        "Feng Jiang",
        "Qianen Zhang",
        "Lei Zhao",
        "Xiao Li"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Direct Preference Optimization (DPO) and its variants have become increasingly popular for aligning language models with human preferences. These methods aim to teach models to better distinguish between chosen (or preferred) and rejected (or dispreferred) responses. However, prior research has identified that the probability of chosen responses often decreases during training, and this phenomenon is known as likelihood displacement. To tackle this challenge, in this work we introduce \\method to controllably shift the distribution of the chosen probability. Then, we show that \\method exhibits a fundamental trade-off between improving the chosen probability and sacrificing the reward margin, as supported by both theoretical analysis and experimental validation. Furthermore, we demonstrate the superiority of \\method over DPO on downstream tasks such as MT-Bench and a designed win rate experiment. We believe this study shows that the likelihood displacement issue of DPO can be effectively mitigated with a simple, theoretically grounded solution. Our code is available at https://github.com/Meaquadddd/DPO-Shift."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 1 ] . [ 1 9 9 5 7 0 . 2 0 5 2 : r DPO-Shift: Shifting the Distribution of Direct Preference Optimization Xiliang Yang * 1 2 Feng Jiang 1 Qianen Zhang 1 Lei Zhao 3 Xiao Li"
        },
        {
            "title": "Abstract",
            "content": "Direct Preference Optimization (DPO) and its variants have become increasingly popular for aligning language models with human preferences. These methods aim to teach models to better distinguish between chosen (or preferred) and rejected (or dispreferred) responses. However, prior research has identified that the probability of chosen responses often decreases during training, and this phenomenon is known as likelihood displacement. To tackle this challenge, in this work we introduce DPO-Shift to controllably shift the distribution of the chosen probability. Then, we show that DPO-Shift exhibits fundamental tradeoff between improving the chosen probability and sacrificing the reward margin, as supported by both theoretical analysis and experimental validation. Furthermore, we demonstrate the superiority of DPO-Shift over DPO on downstream tasks such as MT-Bench and designed win rate experiment. We believe this study shows that the likelihood displacement issue of DPO can be effectively mitigated with simple, theoretically grounded solution. Our code is available at https:// github.com/Meaquadddd/DPO-Shift. 1. Introduction There has been growing interest in guiding large language models (LLMs) to generate safe and helpful content to align with human values and intentions, or, taken together, preferences. One of the most important methods in this field is known as Reinforcement Learning from Human Feedback (RLHF) (Christiano et al., 2017; Ouyang et al., 2022; Stiennon et al., 2020). However, multi-stage optimization procedure is raised in these methods, which includes the *Most of the work of Xiliang Yang was done when he was with School of Data Science, The Chinese University of Hong Kong, Shenzhen. 1School of Data Science, The Chinese University of Hong Kong, Shenzhen 2School of Mathematics, South China University of Technology 3Institute of Translational Medicine and National Center for Translational Medicine, Shanghai Jiao Tong University. Correspondence to: Xiao Li <lixiao@cuhk.edu.cn>. 1 training of reward model and the policy model to maximize the reward. Such optimization and computational burden make it challenging to use and analyze, despite its ability to improve the quality of generated responses (Bai et al., 2022; Achiam et al., 2023; Touvron et al., 2023). Related Works. Recently, DPO (Rafailov et al., 2023) and its variants (Meng et al., 2024; Azar et al., 2024; Tang et al., 2024; Xu et al., 2024; Ethayarajh et al., 2024; Park et al., 2024) is attracting more and more attention. Given pair of samples (x, yw, yl) from the dataset, where is the prompt, and yw and yl represent the chosen and rejected responses, respectivelyannotated by strong large language models or humansthe loss of DPO is designed to maximize the margin between the reward of the chosen response and the rejected response for the model πθ. Being offline algorithms, its simplicity makes DPO more applicable and stable. The main difference between DPO and RLHF lies in the treatment of reward function. DPO proposes to directly parameterize it with the policy model, therefore eliminating the need of training an extra reward model and largely simplifying the training process. However, it has been reported that both log πθ(ywx) and log πθ(ylx) often decrease simultaneously during the training process of DPO; see, e.g., (Pal et al., 2024; Yuan et al., 2024; Rafailov et al., 2024; Tajwar et al., 2024; Pang et al., 2024; Liu et al., 2024; Razin et al., 2024). There are several names used to describe such phenomenon, and we adopt the term likelihood displacement (Razin et al., 2024) in this work. Though DPO still maximizes the reward margin even with this likelihood displacement issue, it remains unfavorable as it causes an unexpected increase in probabilities for responses that are neither preferred nor dispreferred. Prior work has attributed this phenomenon to limitations in model capacity (Tajwar et al., 2024), the presence of multiple training samples or output tokens (Pal et al., 2024), and the initial SFT phase (Rafailov et al., 2024). Existing studies, such as (Razin et al., 2024), have provided theoretical insights into addressing this gap and proposed solving the likelihood displacement problem by filtering the datasets. Main Contributions. In this paper, we propose DPO-Shift, aiming to solve the likelihood displacement issue of DPO, by adding parameter function (λ) to the rejected reward in the BradleyTerry (BT) model (Bradley & Terry, 1952), DPO-Shift: Shifting the Distribution of Direct Preference Optimization which is detailed in (3). We briefly illustrate in Figure 1 that, by choosing proper (λ) in DPO-Shift, we successfully achieve balance between the distribution of log πθ(ywx) and the reward margin. The first row in Figure 1 represents the SFTed model; since the reward margin is not applicable for it, the right plot is concentrated at 0. The second row corresponds to specific choice of (λ) of our proposed DPO-Shift, where we observe an increased chosen probability compared to DPO (depicted in the last row). This improvement is accompanied by only slight decrease in accuracy of reward margin (i.e., the frequency of r(x, yw) r(x, yl) > 0 on the test set). In fact, we can achieve nearly as high reward margin as that of DPO by choosing (λ) properly; see Section 4. Figure 1. Left: Distribution of log πθ(ywx) and log πθ(ylx). Right: Kernel density estimation (KDE) for the reward margin (r(x, yw) r(x, yl)). The reward accuracy, which is the sample mean of 1{(x, yw, yl)r(x, yw) r(x, yl) > 0} is listed. The three rows are plotted with three models including the SFTed Llama 3-8B, the model trained by one strategy of DPO-Shift, and the model trained by DPO (from top to bottom), separately. The ranges of the y-axis of all subfigures are the same. Our main contributions can be summarized as follows. We propose DPO-Shift to mitigate the likelihood displacement issue of DPO by controllably shifting the distribution of the chosen probability. This is achieved through new parameter function (λ) introduced in 2 DPO-Shift. Our approach is as simple as DPO and does not require modifications to the dataset. We provide theoretical analysis for the proposed DPO-Shift without imposing additional assumptions. The analysis guarantees that DPO-Shift mitigates the likelihood displacement issue while introducing fundamental trade-off. Specifically, our theory reveals that DPO-Shift improves the chosen probability log πθ(ywx) at the cost of reducing the reward margin that DPO seeks to maximize. Furthermore, the tradeoff is explicitly controlled by (λ), offering practical guidance on its selection. Our findings suggest that (λ) should be chosen relatively close to 1 to achieve an improvement in the chosen probability over DPO, while only slightly sacrificing the reward margin. Experimentally, we conduct thorough ablation studies to train the Llama 3-8B and Qwen 2-7B models on the UltraFeedback and Capybara-preferences datasets with fine grained choices of (λ). The experiment results corroborate our analysis, clearly demonstrating that the likelihood displacement issue is largely mitigated by DPO-Shift and the fundamental trade-off exists between the chosen probability and reward margin in DPO-Shift. We use downstream experiments to illustrate the improved performance of DPO-Shift over DPO. In particular, we train the Llama 3-8B and Qwen 2-7B models on the UltraFeedback dataset. The MT-Bench results show that DPO-Shift can outperform DPO. To fully demonstrate the superiority that DPO-Shift can controllably shift the chosen probability, we conduct designed win rate experiment, and the result shows that DPO-Shift can consistently outperform DPO. We believe that our study provides simple yet efficient approach for mitigating the likelihood displacement issue of DPO. 2. DPO-Shift: Formulation and Analysis 2.1. DPO, likelihood displacement, and DPO-Shift Consider the preference dataset Dpref = {(x, yw, yl)}, where is the prompt, yw and yl are the chosen and rejected responses for the prompt x, respectively. Given the prompt x, two responses y1 and y2 are first generated from models or directly taken from static dataset, then the chosen yw and rejected yl are selected from {y1, y2} by human or other strong language models. DPO consists of two steps, including supervised fine-tuning (SFT) and preference optimization. Supervised Fine-tuning (SFT). In this stage, the LLM πθ is trained to maximize the log-likelihood of given with DPO-Shift: Shifting the Distribution of Direct Preference Optimization cross-entropy loss: min θ LSFT(πθ) = E(x,y)DSFT [log πθ(yx)] Here, DSFT = {(x, y)} is the normal prompt-response dataset used for auto-regressive language modeling and πθ(yx) = (cid:81)y i=1 πθ(yix, y1:i1). For convenience, we refer to the model after the SFT stage as the SFTed model. From the first row of Figure 1, we conclude that most responses, including both the chosen and rejected ones, are concentrated around the high likelihood area (i.e., likelihood is close to 0 after taking log) of the distributions of log πθ(ywx) and log πθ(ylx). This phenomenon is reasonable due to the training objective of the SFT stage and the fact that yw and yl are often semantically similar (Tajwar et al., 2024; Hong et al., 2024; Pang et al., 2024). Preference Optimization (PO). In this stage, DPO parameterizes the reward model with the LLM πθ via (cid:18) r(x, y) = β log πθ(yx) πref(yx) (cid:19) + log Z(x) , (1) where Z(x) is the partition function and πref is known as the reference model and usually set to be the SFTed model. Incorporating this reward function into the Bradley-Terry (BT) model, i.e., P(yw > ylx) = σ(r(x, yw) r(x, yl)). Then, by maximizing the log-likelihood of P(yw > ylx), DPO arrives at the following objective function: (cid:20) (cid:18) log σ β log min θ πθ(ywx) πref(ywx) β log πθ(ylx) πref(ylx) (cid:19)(cid:21) . (2) Here, the expectation is taken over the dataset Dpref. The model after the PO state is called POed model. Likelihood Displacement. We compare the distributions of the likelihood log πθ(ywx) and log πθ(ylx) between the SFTed model and the POed model by evaluating them on the test dataset. The result is displayed in Figure 1 (first row for SFTed model and third row for DPO). It is easy to observe that the highest likelihood region for both the chosen and rejected responses are decreased dramatically after DPO. Though some other likelihood regions of the chosen and rejected responses have increased after DPO, the averaged likelihood of log πθ(ywx) and log πθ(ylx) over the entire test set is overall decreasing according to our experiment results, aligning with the likelihood displacement phenomenon observed in the existing literature (Tajwar et al., 2024; Pal et al., 2024; Razin et al., 2024; Pang et al., 2024; Yuan et al., 2024; Liu et al., 2024; Rafailov et al., 2024; Hong et al., 2024). In conclusion, the likelihood displacement occurs not only in the training stage but also in the test dataset, which is counter-intuitive and can be harmful to the models generalization ability. 3 An important factor causing the likelihood displacement issue during the PO stage gives rise to the semantic similarity between the chosen yw and rejected yl pairs in Dpref, as observed in the existing works; see, e.g., (Tajwar et al., 2024; Hong et al., 2024; Razin et al., 2024; Pal et al., 2024). This is indeed implied by the generation process of contemporary preference datasets. For instance, the UltraFeedback dataset (Cui et al., 2024) is generated by using different LLMs to response to the same prompt x, and then it selects yw and yl using GPT4. To demonstrate it, we pick the following examples from UltraFeedback: Q1: ...Select from female and male... Solution: chosen: Female. rejected: Female. Q2: Write the right answer to the question based on... chosen: Dan, the protagonist, got coke out of the cooler. rejected: Dan got coke out of the cooler. This partly explains that the model tends to assign similar probabilities to both responses. In the DPO objective function, it seeks to maximize the margin between the probability of the chosen and rejected responses even if they are semantically similar. Hence, instead of the ideal case where it maximizes the chosen probability while minimizes the rejected one, it often reduces the probability of both responses with similar semantic structures, though their margin is enlarged. This leads to the likelihood displacement issue. Consequently, the model favors responses that are neither chosen nor rejected. DPO-Shift. To address this counter-intuitive and harmful likelihood displacement issue of DPO, we introduce DPOShift in this work. The motivation behind the proposed method is to alleviate the problem caused by the similarity of the chosen and rejected pairs. As we analyzed previously, the chosen probability decreases accordingly when the DPO objective maximizes the margin between two semantically similar responses. Based on this observation, we propose to add real valued function 0 < (λ) < 1 to the reward of the rejected response. This helps the BT model to rank correctly by reducing the confrontation between two semantically similar responses, potentially mitigating the likelihood displacement issue of DPO. Mathematically, our proposed formulation is displayed in the following: (cid:20) (cid:18) log σ β log min θ πθ(ywx) πref (ywx) (λ) β log πθ(ylx) πref (ylx) (cid:19)(cid:21) . (3) DPO-Shift: Shifting the Distribution of Direct Preference Optimization 2.2. Analysis for DPO-Shift be data independent. We analyze the effects of DPO-Shift for two important quantities, including the likelihood of the chosen response log πθ(ywx) and the indicator function of the reward margin 1{(x, yw, yl) log πθ(ywx) πref(ylx) > 0}. The latter reflects the models ability to align with human preferences and is implicitly maximized by DPOs objective. We define the two target functions as follows: ω1(θ) = [log πθ (ywx)] , (cid:26) πref(ywx) log πθ(ylx) (cid:27)(cid:21) (4) (cid:20) ω2(θ) = 1 log πθ(ywx) πref(ywx) log πθ(ylx) πref(ylx) > . (5) The likelihood displacement issue of DPO enlarges ω2 while decreases ω1. To provide an analytical characterization, we alter the discontinuous ω2 and consider its smoothed version πθ(ywx) πref(ywx) πθ(ylx) πref(ylx) ω2(θ) = γ log γ log (cid:19)(cid:21) (cid:18) σ (cid:20) , (6) where γ is the smoothing factor. To compare DPO-Shift with the original DPO, we introduce two functions measuring the gaps between targets after one step of optimization (i.e., updating θt to θt+1) with different objective functions: (cid:12) (cid:12) g1(t + 1) = ω1(θt+1) (cid:12)DPO-Shift (cid:12) (cid:12) g2(t + 1) = ω2(θt+1) (cid:12)DPO-Shift We characterize the two gap functions in the following theorem. (cid:12) (cid:12) (cid:12)DPO (cid:12) (cid:12) (cid:12)DPO ω1(θt+1) ω2(θt+1) (7) . , Theorem 2.1. Given θt and learning rate η and denote (cid:18) c(θ) = γσ (λ) γ log πθ(ylx) πref (ylx) γ log (cid:19) , πθ(ywx) πref (ywx) (cid:19) (cid:18) η1(θ) = ησ log π (ylx) πref (ylx) log π (ywx) πref (ywx) . We have (cid:34) Here, g1(t + 1) = (1 (λ))u1, g2(t + 1) = (1 (λ))u2. (8) u1 = (cid:104) (cid:105) c(θ) θ log πθ (ylx) θ log πθ (ywx) , (cid:104) u2 = (cid:16) θ log πθ (ylx) θ log πθ (ywx) η1(θ) θ log πθ (ylx)2(cid:17)(cid:105) . (9) The proof of Theorem 2.1 is deferred to Appendix B. It is worth mentioning that our derivation in the proof of Theorem 2.1 applies to every single sample, and hence the result in Theorem 2.1 applies to u1 and u2 defined using any specific dataset. We state the results in expectation in order to 4 This theorem explains the pros and cons of DPO-Shift, yielding indications for choosing (λ). We provide detailed discussions below. Fundamental Trade-off. We are interested in characterizing the sign of the two gap functions using (8). To compute u1 and u2 on specific Dpref, we define the following sample-based version: ui 1 = ci(θ) θ log πθ (cid:0)yi ui 2 = η1 (cid:16) (cid:0)yi lxi lxi (cid:1) (cid:1) θ log πθ (cid:0)yi (cid:0)yi (cid:1) wxi (cid:1) wxi θ log πθ (cid:13) θ log πθ 2(cid:17) (cid:1)(cid:13) (cid:13) (cid:0)yi . lxi ui ui (cid:13)θ log πθ Then, we compute the sample average u1 = (cid:80) 1/Dref and u2 = (cid:80) 2/Dref. On the test set of UltraFeedback, when setting γ = 1 and πθ to be the SFTed Llama 3-8B, we obtain that u1 = 4.84 108. For u2, since η1 is bounded, we set it to be 1 and obtain u2 = 4.28 109. In terms of frequency, 71.4% of {ui 2} are negative. These results indicate clear positivity of u1 while clear negativity of u2. Indeed, positivity of u1 is expected due to the semantic similarity between yw and yl in contemporary Dpref. 1} are positive and 81.7% of {ui Since we choose 0 < (λ) < 1, we have 1 (λ) > 0. In this case, g1 > 0 as u1 > 0. This immediately concludes that DPO-Shift improve the chosen probability (or likelihood) log πθ(ywx) compared to DPO, solving the undesired likelihood displacement issue. However, there is an explicit trade-off to achieving this improvement. Since u2 can be negative as shown in our test result, g2 can be negative, leading to reduced reward margin of DPO-Shift compared to DPO. In summary, DPO-Shift improves the chosen probability over DPO, at the cost of decreasing the reward margin. This also yields the indications for choosing (λ), which we describe below. Indications for Choosing (λ). As analyzed previously, the important trade-off exists in DPO-Shift. slightly deeper analysis reveals that smaller (λ) leads to more increase in chosen probability while more severe drop in the reward margin. Thus, this indicates that choosing relatively large (λ), i.e., close to 1, helps to balance both sides. That is, the chosen probability is improved reasonably compared to DPO, in the meanwhile the reward margin of DPO-Shift is only decreased slightly. The balance controlled by (λ) is thoroughly demonstrated by our experiment results in Section 4. For the strategy of choosing of (λ), the first one is to fix it all along the optimization process, i.e., (λ) = λ. We denote this strategy as fixed. We also propose to vary λ DPO-Shift: Shifting the Distribution of Direct Preference Optimization between the minimal λmin and the maximal λmin along with time t. We denote this strategy as (λmin, λmax, t). In this paper, we mainly have linear increase / decrease between λmin < 1 and λmax = 1. Set the maximal iteration steps to be , the detailed strategy for the linear increase version of (λmin, λmax, t) is t/T (λmax λmin) + λmin, while the linearly decreased version is t/T (λmin λmax) + λmax. They are separately denoted as linear increase and linear decrease. Can (λ) be Chosen Larger than 1? By default, we choose (λ) < 1 in DPO-Shift to achieve chosen probability improvement, which is based on the hypothesis that u1 is generally positive. If we encounter the case where u1 < 0, e.g., when most pairs yw and yl are dissimilar to each other, u2 must be negative as well. Interestingly, in this case Theorem 2.1 suggests that DPO-Shift with (λ) > 1 can lead to simultaneous improvements for both the chosen probability and reward margin. However, the event u1 < 0 is likely to be very rare, given the general similarity between yw and yl in existing Dpref. Using (λ) > 1 when u1 > 0 leads to catastrophic results. For instance, we trained Llama 3-8B on UltraFeedback with fixed (λ) = 1.05, and the model quickly exhibited crash behavior, producing unreadable responses. Therefore, choosing (λ) > 1 should be done with great care unless u1 is clearly negative and the overwhelming majority of {ui 1} has negative values. Though choosing (λ) > 1 is highly discouraged, the above analysis provides possible direction to further improve DPO-Shift. Before implementing the PO state, we can first calculate {ui 1} for the entire training set. Then, we assign specific (λi) for each data point, i.e., (λi) < 1 when ui 1 > 0 and (λi) > 1 when ui 1 < 0. Adopting such carefully crafted approach requires significant amount of additional implementation code and falls beyond the scope of this work. We leave it as future work. 3. Experimental Setup We perform our experiment on two models, Llama 38B (AI@Meta, 2024) and Qwen 2-7B (Bai et al., 2023), under base setup. For the SFT stage, we train our models for one epoch on UltraChat-200k dataset (Ding et al., 2023) to obtain an SFT model with learning rate of 2e-5. For Llama 3-8B model, we directly use the off-the-shelf models from (Meng et al., 2024) (princeton-nlp/Llama-3-Base-8B-SFT), which follows the same training strategy. For the Preference optimization stage, we aim to verify the robustness of DPO-Shift. To this end, we perform preference optimization on UltraFeedback dataset (Cui et al., 2024), Capybara-preferences dataset (argilla, 2024). Notice that since the Capybara dataset lacks given test dataset, we randomly divide 5% of the training dataset into test sets for subsequent analysis and observations. We start from the two SFTed models and fine-tune them with these two datasets. As for the training parameter, we mainly follow the experimental detail in (Meng et al., 2024), and train the model for 1 epoch with learning rate of 6e-7. The optimizer we choose is paged Adamw 32bit with the implementation from bitsandbytes. To support our paper, we mainly carry out the following two parts of experiments. Verification Experiment. This part of the experiment is designed to validate the theoretical results presented in Section 2.2. Specifically, we consider three strategies for selecting (λ) in (3): fixed, linear increase, and linear decrease. We design series of ablation studies for each of the strategy by altering the choice of (λ). For the fixed strategy, we perform an ablation study by evaluating fixed (λ) from the range [0.5, 0.55, 0.6, 0.65, 0.75, 0.8, 0.85, 0.9, 0.95]. For the linear increase and linear decrease strategies, we set λmin to values in [0.75, 0.85, 0.95] and fix λmax = 1. We compute the log πθ(ywx), log πθ(ylx), and reward margin for models trained with these (λ) strategies on the test sets of their respective training datasets. To further illustrate the phase transition phenomenon in these probability distributions as (λ) varies from (λ) < 1 to (λ) = 1, we extend the fixed strategy ablation study by including (λ) values of [0.96, 0.97, 0.98, 0.99]. Downstream Performance Evaluation. This experiment is primarily designed to evaluate the general performance of the model trained using our proposed method. For the two SFTed models trained on the UltraFeedback dataset, we evaluate the win rate of DPO-Shift against DPO using 2,000 randomly selected questions from the Capybara dataset and the test prefs split of the UltraFeedback dataset. For the evaluation, we employ the Llama3.3:70binstruct-q4 model provided in the ollama library, 4-bit quantized version of the latest Llama 3.3-Instruct 70B, which delivers performance comparable to Llama 3.1-405B. Computation Environment. The training experiments in this paper were conducted using 8A800 80GB GPUs and 4A100 40GB GPUs, based on the alignment handbook repository (Tunstall et al.). Specifically, all Llama 3 8B-related experiments were performed on 4A100 GPUs, while Qwen 2-7B experiments utilized 8A800 GPUs. For downstream performance evaluation, MT-Bench was assessed using the llm judge submodule from the FastChat (Zheng et al., 2023) repository, and the win rate experiments were conducted with the Ollama repository. Baseline. We compare DPO-Shift using three strategies with the original DPO method. Since it is reported that 5 DPO-Shift: Shifting the Distribution of Direct Preference Optimization Figure 2. Distribution for log πθ(ywx) and log πθ(ylx) on test set split of UltraFeedback for Llama 3-8B trained on UltraFeedback with fixed strategy. Only limited cases of (λ) are listed. For full ablation study, please refer to Appendix A.2. The ranges of the y-axis of all subfigures are the same. Figure 3. Distribution for reward margin and reward accuracy on test set split of UltraFeedback for Llama 3-8B trained on UltraFeedback with fixed strategy. Only limited cases of (λ) are listed. For full ablation study, please refer to Appendix A.2. The ranges of the y-axis of all subfigures are the same. many variants of DPO do not empirically present clear advantage over standard DPO; see, e.g., (Meng et al., 2024), we do not compare our method with other variants of DPO. Additionally, due to the high workload of ablation studies associated with DPO-Shift, the computational resources needed for experiments on other variants are beyond our means. 4. Experimental Results In this section, we present the main results of our experiments, highlighting the performance of DPO-Shift on various benchmarks and ablation studies. 4.1. Experimental Verification for DPO-Shift We report some representative results from our ablation studies for fixed, linear increase and linear decrease. We mainly evaluate them with 2 metrics, including distribution for log πθ(ywx) and log πθ(ylx), and distribution for reward margin. We first study fixed strategy. The two metrics for evaluation, including distributions for log πθ(ywx) and log πθ(ylx), and distribution for reward margin are demonstrated in Figure 2 and Figure 3 separately. The relation between the choice of (λ) and reward accuracy are clearly illustrated in Figure 4. In Figure 2, we observe consistent shift in the distribution of log πθ(ywx) and log πθ(ylx), moving from high-probability regions to lower-probability regions with increasing (λ). This shift is accompanied DPO-Shift: Shifting the Distribution of Direct Preference Optimization Figure 4. Reward accuracy vs different (λ) (fixed strategy) for Llama 3-8B trained on UltraFeedback, where (λ) is selected from [0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95]. Figure 5. The comparison between DPO-Shift with (λ) = 0.99 and DPO on test set split of UltraFeedback for Llama 3-8B trained on UltraFeedback. Left: Distribution for log πθ(ywx). Right: Reward accuracy and distribution for reward margin. by mass transition in the reward margin, shifting from areas less than 0 to areas greater than 0. The extent of this shift largely depends on the value of (λ), regardless of the strategy employed. Specifically, using smaller (λ) leads to higher probabilities for the language model on the chosen answer yw. However, this comes at the cost of decreased reward accuracy and shifted reward margin, as demonstrated in Figure 3. Thus, too smaller (λ) can result in over-fitting to the chosen answer, while losing the contrastive learning ability against the rejected one, reducing the models performance. Fortunately, by choosing relatively larger (λ) that is closer to 1, we observe an increase in reward accuracy with only limited trade-off in the chosen probability. This completely aligns with our analysis proposed in Section 2.2 that larger (λ) helps to balance both metrics. For example, (λ) = 0.9, 0.95 achieves higher probability of the chosen responses while maintaining almost the same reward margin and accuracy. Figure 4 illustrates clear increase of the reward accuracy along with increasing (λ), further corroborating our theory. To further investigate the effect of (λ) on the distribution of log πθ(ywx) and log πθ(ylx) as it approaches 1, we conducted experiments with more fine grains. In the case of fixed, we select new set of fixed (λ)s , including [0.96, 0.97, 0.98, 0.99]. We report the comparison between 7 Figure 6. The comparison between fixed strategy with (λ) = 0.75 and linear decrease with λmin = 0.75 on test set split of UltraFeedback for Llama 3-8B trained on UltraFeedback. Left: Distribution for log πθ(ywx) and log πθ(ylx); Right: Reward accuracy and distribution for reward margin. fixed with (λ) = 0.99 and the original DPO with the distribution of chosen probability and reward margin in the Figure 5. An obvious shift of the distribution log πθ(ywx) can be observed, while the reward accuracy and the distribution of the reward margin remains nearly unchanged. The full experimental results for (λ) [0.96, 0.97, 0.98, 0.99] are reported in Appendix A.4, in which the results are consistent with our analysis. To achieve possibly better trade-off between the two distributions, we conduct experiments on linear increase and linear decrease strategies for choosing (λ). We report the result for linear decrease with λmin = 0.75 in Figure 6. It can be seen that this dynamic (λ) strategy achieves more satisfactory balance between reward accuracy and log πθ(ywx) compared to fixed with (λ) = 0.75. The full experimental results for linear increase and linear decrease strategies are provided in Appendix A.3. Summary. By setting (λ) properly in DPO-Shift, we can achieve controllable trade-off between chosen probability and reward accuracy. 4.2. Downstream Performance We report our MT-bench experiment in Table 1, which contains results of Llama 3-8B and Qwen 2-7B fine-tuned on the UltraFeedback dataset. We observe that although the optimal settings for (λ) vary between models, DPO-Shift achieves consistent comparable and sometimes superior performances compared to DPO. Notice that it is more likely for DPO-Shift to achieve better performance with larger fixed (λ), emphasizing the importance of balancing the trade-off between chosen probability and reward margin. However, since DPO-Shift mainly focuses on better aligning DPO-Shift: Shifting the Distribution of Direct Preference Optimization Table 1. MT-Bench (Zheng et al., 2023) results for Llama 3-8B and Qwen 2-7B trained on UltraFeedback, where GPT-4o is the judge model. The results better than DPO is bolded. Results for fixed, linear increase, and linear decrease are included. (λ) strategy Llama 3-8B Qwen 2-7B SFT DPO fixed 0.5 fixed 0.55 fixed 0.6 fixed 0.65 fixed 0.7 fixed 0.75 fixed 0.8 fixed 0.85 fixed 0.9 fixed 0.95 increase linear 0.75 increase linear 0.85 increase linear 0.95 decrease linear 0.75 decrease linear 0.85 decrease linear 0.95 5.64 6.513 6.118 6.269 6.169 6.314 6.500 6.444 6.731 6.644 6.738 6.444 6.588 6.425 6.519 6.613 6.481 6. 5.88 6.875 6.150 6.369 6.331 6.494 6.581 6.700 6.869 6.775 6.725 6.875 6.775 6.806 7.044 6.742 6.906 6.944 the model with the chosen response, the MT-Bench experiment does not fully capture the models ability to achieve this target. Moreover, when comparing the answers generated by DPO and DPO-Shift, we observe that the model trained with DPO generally generates longer and less meaningful responses, while DPO-Shift tends to produce more concise and high-quality responses. Based on these observations, we adopt perplexity, widely used evaluation metric (Cooper & Scholak, 2024; Chen et al., 1998; Xu et al., 2022) for language models. Perplexity quantifies how well probability model predicts sample of data, essentially measuring the models uncertainty about the data. Lower perplexity indicates that the model is better at predicting the sample. The perplexity of DPO and DPO-Shift trained on UltraFeedback with fixed (λ) = 0.95 is evaluated on the chosen responses from the test split of the UltraFeedback dataset. The results are 4.475 for DPO-Shift and 18.996 for DPO, further demonstrating the potential advantage of DPO-Shift. To fully demonstrate the better alignment of DPO-Shift with the chosen response, we conduct win rate experiment. In this setup, the judge model is provided with the question, the reference answer from the dataset, and the answers generated by DPO-Shift and DPO. The judge model then judges the responses of DPO-Shift and DPO based on their general quality and how close to the reference answer they are. We put the judge prompts in Appendix A.1. We test the win rate using the test split of UltraFeedback and Figure 7. Win rate experiment against DPO using Llama 3-8B trained on the UltraFeedback dataset and tested with questions from the test split of UltraFeedback. Capybara for Llama 3-8B and Qwen 2-7B that is trained on UltraFeedback. Note that in this setting, the test on Capybara is out of distribution. We report the results of Llama 3-8B tested on Ultrafeedback in Figure 7, while the rest of the experiments are deferred to Appendix A.5. One can observe that when (λ) is relatively closer to 1, DPOShift consistently outperforms DPO. This finding aligns with the MT-Bench results listed in Table 1, where DPO-Shift with larger (λ) is more likely to outperforms DPO. These experiment results corroborate our analysis in Section 2.2, highlighting the importance of achieving balance between chosen probability and reward margin. Summary. DPO-Shift outperforms DPO in terms of downstream performance when (λ) is set to achieve balance between chosen probability and reward margin. 5. Conclusion In this work, we proposed DPO-Shift, which controllably shifts the distribution of the chosen probability. Our method guarantees to mitigate the likelihood displacement issue of DPO while introducing fundamental trade-off between the chosen probability and reward margin. By selecting (λ) carefully, the chosen probability can be improved, in the meanwhile the reward margin is only slightly sacrificed. Extensive ablation experiments across various models and datasets confirm the validity of our theoretical findings. To DPO-Shift: Shifting the Distribution of Direct Preference Optimization further demonstrate the advantages of DPO-Shift, we conducted experiments on downstream tasks such as MT-Bench and designed win rate experiment. Clear performance improvements over DPO were observed, highlighting the robustness and effectiveness of our approach. more crafted strategy on the selection of (λ) can be possible direction to further improve DPO-Shift, as we commented at the end of Section 2.2. We leave it as future work."
        },
        {
            "title": "Impact Statement",
            "content": "This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here"
        },
        {
            "title": "References",
            "content": "Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F. L., Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S., et al. GPT-4 technical report. ArXiv, abs/2303.08774, 2023. AI@Meta. Llama 3 model card. URL https://github.com/meta-llama/llama3/ blob/main/MODEL_CARD.md. 2024. argilla. URL argilla/Capybara-Preferences. 2024. Capybara-preferences dataset card. https://huggingface.co/datasets/ Azar, M. G., Guo, Z. D., Piot, B., Munos, R., Rowland, M., Valko, M., and Calandriello, D. general theoretical paradigm to understand learning from human preferences. In International Conference on Artificial Intelligence and Statistics, pp. 44474455. PMLR, 2024. Bai, J., Bai, S., Chu, Y., Cui, Z., Dang, K., Deng, X., Fan, Y., Ge, W., Han, Y., Huang, F., et al. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023. Bai, Y., Jones, A., Ndousse, K., Askell, A., Chen, A., DasSarma, N., Drain, D., Fort, S., Ganguli, D., Henighan, T., et al. Training helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862, 2022. Bradley, R. A. and Terry, M. E. Rank analysis of incomplete block designs: I. the method of paired comparisons. Biometrika, 39:324, 1952. Chen, S. F., Beeferman, D., and Rosenfeld, R. Evaluation metrics for language models. 1998. Christiano, P. F., Leike, J., Brown, T., Martic, M., Legg, S., and Amodei, D. Deep reinforcement learning from human preferences. Advances in neural information processing systems, 30, 2017. 9 Cooper, N. and Scholak, T. Perplexed: Understanding when large language models are confused. arXiv preprint arXiv:2404.06634, 2024. Cui, G., Yuan, L., Ding, N., Yao, G., Zhu, W., Ni, Y., Xie, G., Liu, Z., and Sun, M. UltraFeedback: Boosting language models with high-quality feedback. In ICML, 2024. Ding, N., Chen, Y., Xu, B., Qin, Y., Zheng, Z., Hu, S., Liu, Z., Sun, M., and Zhou, B. Enhancing chat language models by scaling high-quality instructional conversations. In EMNLP, 2023. Ethayarajh, K., Xu, W., Muennighoff, N., Jurafsky, D., and Kiela, D. KTO: Model alignment as prospect theoretic optimization. ArXiv, abs/2402.01306, 2024. Hong, J., Lee, N., and Thorne, J. ORPO: Monolithic preference optimization without reference model. ArXiv, abs/2403.07691, 2024. Liu, Z., Lu, M., Zhang, S., Liu, B., Guo, H., Yang, Y., Blanchet, J., and Wang, Z. Provably mitigating overoptimization in rlhf: Your sft loss is implicitly an adversarial regularizer. arXiv preprint arXiv:2405.16436, 2024. Meng, Y., Xia, M., and Chen, D. Simpo: Simple preference optimization with reference-free reward. arXiv preprint arXiv:2405.14734, 2024. Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C. L., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., Schulman, J., Hilton, J., Kelton, F., Miller, L. E., Simens, M., Askell, A., Welinder, P., Christiano, P. F., Leike, J., and Lowe, R. J. Training language models to follow instructions with human feedback. In NeurIPS, 2022. Pal, A., Karkhanis, D., Dooley, S., Roberts, M., Naidu, S., and White, C. Smaug: Fixing failure modes of preference optimisation with DPO-positive. arXiv preprint arXiv:2402.13228, 2024. Pang, R. Y., Yuan, W., Cho, K., He, H., Sukhbaatar, S., and Weston, J. Iterative reasoning preference optimization. arXiv preprint arXiv:2404.19733, 2024. Park, R., Rafailov, R., Ermon, S., and Finn, C. Disentangling length from quality in direct preference optimization. ArXiv, abs/2403.19159, 2024. Rafailov, R., Sharma, A., Mitchell, E., Ermon, S., Manning, C. D., and Finn, C. Direct preference optimization: Your language model is secretly reward model. In NeurIPS, 2023. Rafailov, R., Hejna, J., Park, R., and Finn, C. From to Q: Your language model is secretly Q-function. arXiv preprint arXiv:2404.12358, 2024. DPO-Shift: Shifting the Distribution of Direct Preference Optimization llm reasoning generalists with preference trees. arXiv preprint arXiv:2404.02078, 2024. Zheng, L., Chiang, W.-L., Sheng, Y., Zhuang, S., Wu, Z., Zhuang, Y., Lin, Z., Li, Z., Li, D., Xing, E., et al. Judging LLM-as-a-judge with MT-Bench and Chatbot Arena. In NeurIPS Datasets and Benchmarks Track, 2023. Razin, N., Malladi, S., Bhaskar, A., Chen, D., Arora, S., and Hanin, B. Unintentional unalignment: Likelihood displacement in direct preference optimization. arXiv preprint arXiv:2410.08847, 2024. Stiennon, N., Ouyang, L., Wu, J., Ziegler, D., Lowe, R., Voss, C., Radford, A., Amodei, D., and Christiano, P. F. Learning to summarize with human feedback. Advances in Neural Information Processing Systems, 33: 30083021, 2020. Tajwar, F., Singh, A., Sharma, A., Rafailov, R., Schneider, J., Xie, T., Ermon, S., Finn, C., and Kumar, A. Preference fine-tuning of llms should leverage suboptimal, on-policy data. arXiv preprint arXiv:2404.14367, 2024. Tang, Y., Guo, Z. D., Zheng, Z., Calandriello, D., Munos, R., Rowland, M., Richemond, P. H., Valko, M., Pires, B. A., and Piot, B. Generalized preference optimization: unified approach to offline alignment. arXiv preprint arXiv:2402.05749, 2024. Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., Bikel, D., Blecher, L., Ferrer, C. C., Chen, M., Cucurull, G., Esiobu, D., Fernandes, J., Fu, J., Fu, W., Fuller, B., Gao, C., Goswami, V., Goyal, N., Hartshorn, A., Hosseini, S., Hou, R., Inan, H., Kardas, M., Kerkez, V., Khabsa, M., Kloumann, I., Korenev, A., Koura, P. S., Lachaux, M.-A., Lavril, T., Lee, J., Liskovich, D., Lu, Y., Mao, Y., Martinet, X., Mihaylov, T., Mishra, P., Molybog, I., Nie, Y., Poulton, A., Reizenstein, J., Rungta, R., Saladi, K., Schelten, A., Silva, R., Smith, E. M., Subramanian, R., Tan, X. E., Tang, B., Taylor, R., Williams, A., Kuan, J. X., Xu, P., Yan, Z., Zarov, I., Zhang, Y., Fan, A., Kambadur, M., Narang, S., Rodriguez, A., Stojnic, R., Edunov, S., and Scialom, T. Llama 2: Open foundation and fine-tuned chat models, 2023. Tunstall, L., Beeching, E., Lambert, N., Rajani, N., Huang, S., Rasul, K., Bartolome, A., M. Rush, A., and Wolf, T. The Alignment Handbook. URL https://github. com/huggingface/alignment-handbook. Xu, F. F., Alon, U., Neubig, G., and Hellendoorn, V. J. systematic evaluation of large language models of code. In Proceedings of the 6th ACM SIGPLAN International Symposium on Machine Programming, pp. 110, 2022. Xu, H., Sharaf, A., Chen, Y., Tan, W., Shen, L., Durme, B. V., Murray, K., and Kim, Y. J. Contrastive preference optimization: Pushing the boundaries of LLM performance in machine translation. ArXiv, abs/2401.08417, 2024. Yuan, L., Cui, G., Wang, H., Ding, N., Wang, X., Deng, J., Shan, B., Chen, H., Xie, R., Lin, Y., et al. Advancing 10 DPO-Shift: Shifting the Distribution of Direct Preference Optimization A. Supplemented Experimental results A.1. Llama 3.3-70B Prompts for Win Rate experiment You are tasked with comparing the responses of two assistants, Assistant and Assistant to users question. Additionally, you will be provided with reference answer to evaluate the quality of the responses from both assistants. Users Question: <question> Reference Answer: <reference answer> Assistant As Response: <response compare> Assistant Bs Response: <response_baseline> First, output A, B, or Tie to indicate your judgment of these two responses. Then, provide one-sentence explanation for your choice. The principles for your judgment should consider the following criteria: 1. Do not judge the quality of the two responses based on their length. 2. Determine which responses meaning is essentially closer to the reference answer. 3. Evaluate the responses based on their helpfulness, relevance, accuracy, depth, and level of detail. 4. For open-ended questions, the reference answer might not be the unique correct answer, and you can take correct and factual alternative responses into account for these types of questions. 5. If the two responses have no essential difference in meaning and correctness, and only differ in wording, format, or length, output Tie. DPO-Shift: Shifting the Distribution of Direct Preference Optimization A.2. Ablation Studies for fixed Figure 8. Distribution for log πθ(ywx) and log πθ(ylx) on test set split of UltraFeedback for Llama 3-8B trained on UltraFeedback, where DPO-Shift uses fixed strategy. The ranges of the y-axis of all subfigures are the same. Figure 9. Distribution for reward margin and reward accuracy on test set split of UltraFeedback for Llama 3-8B trained on UltraFeedback, where DPO-Shift uses fixed strategy. The ranges of the y-axis of all subfigures are the same. 12 DPO-Shift: Shifting the Distribution of Direct Preference Optimization Figure 10. Distribution for log πθ(ywx) and log πθ(ylx) on test set split of Capybara for Llama 3-8B trained on Capybara, where DPO-Shift uses fixed strategy. The ranges of the y-axis of all subfigures are the same. Figure 11. Distribution for reward margin and reward accuracy on test set split of Capybara for Llama 3-8B trained on Capybara, where DPO-Shift uses fixed strategy. The ranges of the y-axis of all subfigures are the same. 13 DPO-Shift: Shifting the Distribution of Direct Preference Optimization Figure 12. Distribution for log πθ(ywx) and log πθ(ylx) on test set split of UltraFeedback for Qwen 2-7B trained on UltraFeedback, where DPO-Shift uses fixed strategy. The ranges of the y-axis of all subfigures are the same. Figure 13. Distribution for reward margin and reward accuracy on test set split of UltraFeedback for Qwen 2-7B trained on UltraFeedback, where DPO-Shift uses fixed strategy. The ranges of the y-axis of all subfigures are the same. 14 DPO-Shift: Shifting the Distribution of Direct Preference Optimization Figure 14. Distribution for log πθ(ywx) and log πθ(ylx) on test set split of Capybara for Qwen 2-7B trained on Capybara, where DPO-Shift uses fixed strategy. The ranges of the y-axis of all subfigures are the same. Figure 15. Distribution for reward margin and reward accuracy on test set split of Capybara for Qwen 2-7B trained on Capybara, where DPO-Shift uses fixed strategy. The ranges of the y-axis of all subfigures are the same. 15 DPO-Shift: Shifting the Distribution of Direct Preference Optimization A.3. Ablation Studies for linear increase and linear decrease Figure 16. Distribution for log πθ(ywx) and log πθ(ylx) on test set split of Ultrafeedback for Llama 3-8B trained on UltraFeedback, where DPO-Shift uses linear increase and linear decrease strategies. The ranges of the y-axis of all subfigures are the same. Figure 17. Distribution for reward margin and reward accuracy on test set split of Ultrafeedback for Llama 3-8B trained on UltraFeedback, where DPO-Shift uses linear increase and linear decrease strategies. The ranges of the y-axis of all subfigures are the same. 16 DPO-Shift: Shifting the Distribution of Direct Preference Optimization Figure 18. Distribution for log πθ(ywx) and log πθ(ylx) on test set split of Capybara for Llama 3-8B trained on Capybara, where DPO-Shift uses linear increase and linear decrease strategies. The ranges of the y-axis of all subfigures are the same. Figure 19. Distribution for reward margin and reward accuracy on test set split of Capybara for Llama 3-8B trained on Capybara, where DPO-Shift uses linear increase and linear decrease strategies. The ranges of the y-axis of all subfigures are the same. 17 DPO-Shift: Shifting the Distribution of Direct Preference Optimization Figure 20. Distribution for log πθ(ywx) and log πθ(ylx) on test set split of UltraFeedback for Qwen 2-7B trained on UltraFeedback, where DPO-Shift uses linear increase and linear decrease strategies. The ranges of the y-axis of all subfigures are the same. Figure 21. Distribution for reward margin and reward accuracy on test set split of UltraFeedback for Qwen 2-7B trained on UltraFeedback, where DPO-Shift uses linear increase and linear decrease strategies. The ranges of the y-axis of all subfigures are the same. DPO-Shift: Shifting the Distribution of Direct Preference Optimization Figure 22. Distribution for log πθ(ywx) and log πθ(ylx) on test set split of Capybara for Qwen 2-7B trained on Capybara, where DPO-Shift uses linear increase and linear decrease strategies. The ranges of the y-axis of all subfigures are the same. Figure 23. Distribution for reward margin and reward accuracy on test set split of Capybara for Qwen 2-7B trained on Capybara, where DPO-Shift uses linear increase and linear decrease strategies. The ranges of the y-axis of all subfigures are the same. 19 DPO-Shift: Shifting the Distribution of Direct Preference Optimization A.4. Ablation Studies for Fine-grained fixed Figure 24. Distribution for log πθ(ywx) and log πθ(ylx) on test set split of UltraFeedback for Llama 3-8B trained on UltraFeedback, where DPO-Shift uses fixed strategy. The ranges of the y-axis of all subfigures are the same. Figure 25. Distribution for reward margin and reward accuracy on test set split of UltraFeedback for Llama 3-8B trained on UltraFeedback, where DPO-Shift uses fixed strategy. The ranges of the y-axis of all subfigures are the same. 20 DPO-Shift: Shifting the Distribution of Direct Preference Optimization A.5. Supplementary Results for Win Rate Experiment Figure 26. Win rate experiment for Llama 3-8B trained on UltraFeedback and tested with questions from the test split of Capybara. Figure 27. Win rate experiment for Qwen 2-7B trained on UltraFeedback and tested with questions from the test split of UltraFeedback. 21 DPO-Shift: Shifting the Distribution of Direct Preference Optimization Figure 28. Win rate experiment for Qwen 2-7B trained on UltraFeedback and tested with questions from the test split of Capybara. 22 DPO-Shift: Shifting the Distribution of Direct Preference Optimization B. Proof of Theorem 2.1 We consider our modified PO loss function: LλDPO(πθ, πref) = E(x,yw,yl)Dpref (cid:20) (cid:18) σ β log πθ(ywx) πref (ywx) (λ) β log (cid:19)(cid:21) πθ(ylx) πref (ylx) where (λ) is real valued function with (λ) < 1. We compute its gradient w.r.t θ: θLλDPO(πθ, πref) = E(x,yw,yl)Dpref := β log πθ(ywx) πref (ywx) (cid:20) σ(u) σ(u) (cid:21) θu (λ) β log πθ(ylx) πref (ylx) notice that σ(x) = σ(x)(1 σ(x)), 1 σ(x) = σ(x)."
        },
        {
            "title": "Then we proceed with the final gradient",
            "content": "θLλDPO(πθ, πref) = E(x,yw,yl)Dpref (cid:20) (cid:18) βσ (λ) β log πθ(ylx) πref (ylx) β log (cid:19) πθ(ywx) πref (ywx) [θ log π (ywx) (λ) θ log π (ylx)]] (10) (11) We then simplify it with the following notation c1 := cθ(λ, yw, yl) = βσ (λ) β log (cid:18) πθ(ylx) πref (ylx) β log (cid:19) πθ(ywx) πref (ywx) c2 = (λ)c1 then we have θLλDPO(πθ, πref) = E(x,yw,yl)Dpref [c1θ log πθ (ywx) c2θ log πθ (ylx)] . Then we upgrade θt+1 with the following: θt+1 θt + ηE(x,yw,yl)Dpref [c1θ log πθt (ywx) c2θ log πθt (ylx)] . We first look into w1(θt) = log πθt (ywx) then then w1(θt+1) = w1(θt) + η (c1θ log πθt (ywx) c2θ log πθt (ylx)) (θ log πθt (ywx)) = w1(θt) + η (cid:16) c1 θ log πθt (ywx) 2 c2θ log πθt (ylx) θ log πθt (ywx) (cid:17) (12) (13) g1(t + 1) = η(c1 c2)θ log π (ylx) θ log π (ywx) We compute θ log π (cid:0)yi turn out to be positive. Consequently, we can choose c2 as small as possible to increase the chosen probability. (cid:1) for the SFTed Llama 3-8B model. In terms of frequency, 71.4% of them θ log π (cid:0)yi wxi lxi (cid:1) However, choosing small c2 can cause performance drop (both in MT-bench Table 1 and reward accuracy). To evaluate the performance of the model, we look into the reward margin: ω2(θt) = (cid:20) (cid:26) log πθt(ywx) πref (ywx) log πθt(ylx) πref (ylx) (cid:27)(cid:21) > To analyze, we alternate it with its smoothed version: ω2(θt) = (cid:20) (cid:18) σ γ log πθt(ywx) πref (ywx) γ log πθt(ylx) πref (ylx) (cid:19)(cid:21) , we abuse notation and use β + as hyper parameter. 23 DPO-Shift: Shifting the Distribution of Direct Preference Optimization Then with first order Taylors expansion for (ω, θ) and (ω, θ). Remark 1. Given the fact that they are using the same expectation E(x,yw,yl)Dpref [], we omit it for the sake of simplicity 2. Given the assumption that η is small enough, we can ignore second order. (cid:19) (cid:18) ω2(θt+1) = ω2(θt) + η(θ2 t+1 θ )σ log (θ log πθt (ywx) θ log π (ylx)) πθt (ylx) πref (ylx) log πθt (ywx) πref (ywx) = ω2 = ω2 + η1 (c1θ log πθt (ywx) c2θ log πθt (ylx)) (θ log πθt (ywx) θ log πθt (ylx)) + η1 (cid:105) c1 θ log πθt (ywx) 2 + c2 θ log πθt (ylx) 2 (c1 + c2) θ log πθt (ylx) θ log πθt (ywx) (cid:104) η1 := ησ (log πθt (ywx) log πθt (ylx)) then we have g2(t + 1) = η1 (c2 c1) θ log π (ylx) 2 + (c1 c2)θ log π (ylx) θ log π (ywx) = η1 (c1 c2) (cid:16) θ log π (ylx) θ log π (ywx) θ log π (ylx) 2(cid:17) (cid:0)yi We compute θ log πθt wxi of frequency, 81.7% of them turn out to be positive. θ log πθt (cid:0)yi lxi (cid:1) (cid:1) (cid:12) (cid:12)θ log πθt (cid:0)yi lxi (cid:1)(cid:12) (cid:12) 2 for the SFTed Llama 3-8B model. In terms which is consistent with our observation that accuracy drop greatly with small c2 , therefore we need to strike balance between the accuracy and chosen probability."
        }
    ],
    "affiliations": [
        "Institute of Translational Medicine and National Center for Translational Medicine, Shanghai Jiao Tong University",
        "School of Data Science, The Chinese University of Hong Kong, Shenzhen",
        "School of Mathematics, South China University of Technology"
    ]
}