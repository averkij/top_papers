{
    "paper_title": "UI-R1: Enhancing Action Prediction of GUI Agents by Reinforcement Learning",
    "authors": [
        "Zhengxi Lu",
        "Yuxiang Chai",
        "Yaxuan Guo",
        "Xi Yin",
        "Liang Liu",
        "Hao Wang",
        "Guanjing Xiong",
        "Hongsheng Li"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The recent DeepSeek-R1 has showcased the emergence of reasoning capabilities in LLMs through reinforcement learning (RL) with rule-based rewards. Building on this idea, we are the first to explore how rule-based RL can enhance the reasoning capabilities of multimodal large language models (MLLMs) for graphic user interface (GUI) action prediction tasks. To this end, we curate a small yet high-quality dataset of 136 challenging tasks, encompassing five common action types on mobile devices. We also introduce a unified rule-based action reward, enabling model optimization via policy-based algorithms such as Group Relative Policy Optimization (GRPO). Experimental results demonstrate that our proposed data-efficient model, UI-R1-3B, achieves substantial improvements on both in-domain (ID) and out-of-domain (OOD) tasks. Specifically, on the ID benchmark AndroidControl, the action type accuracy improves by 15%, while grounding accuracy increases by 10.3%, compared with the base model (i.e. Qwen2.5-VL-3B). On the OOD GUI grounding benchmark ScreenSpot-Pro, our model surpasses the base model by 6.0% and achieves competitive performance with larger models (e.g., OS-Atlas-7B), which are trained via supervised fine-tuning (SFT) on 76K data. These results underscore the potential of rule-based reinforcement learning to advance GUI understanding and control, paving the way for future research in this domain."
        },
        {
            "title": "Start",
            "content": "UI-R1: Enhancing Action Prediction of GUI Agents by Reinforcement Learning Zhengxi Lu1, Yuxiang Chai2, Yaxuan Guo1, Xi Yin1, Liang Liu1, Hao Wang1, Guanjing Xiong1, Hongsheng Li2(cid:66) 1 vivo AI Lab Equal Contribution 2 MMLab @ CUHK Project Lead (cid:66) Corresponding Author"
        },
        {
            "title": "Abstract",
            "content": "The recent DeepSeek-R1 has showcased the emergence of reasoning capabilities in LLMs through reinforcement learning (RL) with rule-based rewards. Building on this idea, we are the first to explore how rule-based RL can enhance the reasoning capabilities of multimodal large language models (MLLMs) for graphic user interface (GUI) action prediction tasks. To this end, we curate small yet high-quality dataset of 136 challenging tasks, encompassing five common action types on mobile devices. We also introduce unified rule-based action reward, enabling model optimization via policy-based algorithms such as Group Relative Policy Optimization (GRPO). Experimental results demonstrate that our proposed data-efficient model, UI-R1-3B, achieves substantial improvements on both in-domain (ID) and out-of-domain (OOD) tasks. Specifically, on the ID benchmark ANDROIDCONTROL, the action type accuracy improves by 15%, while grounding accuracy increases by 10.3%, compared with the base model (i.e. Qwen2.5-VL-3B). On the OOD GUI grounding benchmark ScreenSpot-Pro, our model surpasses the base model by 6.0% and achieves competitive performance with larger models (e.g., OS-Atlas-7B), which are trained via supervised fine-tuning (SFT) on 76K data. These results underscore the potential of rule-based reinforcement learning to advance GUI understanding and control, paving the way for future research in this domain. 5 2 0 2 7 2 ] . [ 1 0 2 6 1 2 . 3 0 5 2 : r Figure 1: Left: Overall performance of UI-R1-3B on both in-domain (i.e., ANDROIDCONTROL) and out-of-domain (i.e., ScreenSpot-Pro, ScreenSpot desktop and web subsets) tasks; Right: Employing reinforcement fine-tuning (RFT), UI-R1-3B achieves performance comparable to SFT models with significantly fewer data and GPU hours. The circle radius indicates the model size."
        },
        {
            "title": "Introduction",
            "content": "Supervised fine-tuning (SFT) has long been the standard training paradigm for large language models (LLMs) and graphic user interface (GUI) agents (Qin et al., 2025; Wu et al., 2024; Hong et al., 2024). However, SFT relies heavily on large-scale, high-quality labeled datasets, leading to prolonged training times and high computational costs. Furthermore, existing open-source VLM-based GUI agents trained using SFT can be criticized for poor performance in out-of-domain (OOD) scenarios (Lu et al., 2024; Chai et al., 2024), limiting their effectiveness and applicability in real-world applications. Rule-based reinforcement learning (RL) or reinforcement fine-tuning (RFT) has recently emerged as an efficient and scalable alternative to SFT for the development of LLMs, which efficiently fine-tune the model with merely dozens to thousands of samples to excel at domain-specific tasks. It uses predefined task-specific reward functions, eliminating the need for costly human annotations. Recent works, such as DeepSeek-R1 (Guo et al., 2025), demonstrate the effectiveness of rule-based RL in mathematical problem solving by evaluating the correctness of the solution, while others (Liu et al., 2025; Wang et al., 2025; Peng et al., 2025; Chen et al., 2025) extend the algorithm to multimodal models, achieving notable improvements in vision-related tasks such as image grounding and object detection. By focusing on measurable objectives, rule-based RL enables practical and versatile model optimization across both textual and multimodal domains, offering significant advantages in terms of efficiency, scalability, and reduced reliance on large datasets. Previous studies targeting traditional vision-related tasks always rely on the traditional Intersection over Union (IoU) metric commonly used for grounding and detection tasks. In this work, we extend the rule-based RL paradigm to new application domain by focusing on GUI action prediction tasks driven by low-level instructions. To achieve this, MLLM generates multiple responses (trajectories) that contain the reasoning tokens and the final answers for each input. Then our proposed reward function evaluates each response and updates the model by policy optimization, such as GRPO (Shao et al., 2024), to improve its reasoning ability. Our reward function contains the action type reward, the action argument reward, along with the common format reward. In detail, (1) the action type reward is determined by whether the predicted action type matches the ground truth; (2) the action argument reward (focused on Click), is evaluated by whether the predicted coordinates fall within the ground truth bounding box; (3) the format reward is evaluated by whether the model provides both the reasoning process and the final answer. This flexible and effective reward mechanism is well aligned with the objectives of general GUI-related tasks, ensuring both accuracy and interpretability in the models performance. Regarding data preparation, we follow Muennighoff et al. (2025) and select just 130+ training mobile samples according to three criterion: difficulty, diversity, and quality, making our method remarkably data-efficient. Experiments demonstrate that UI-R1 achieves significant performance improvements on out-of-domain (OOD) data, including entries from desktop and web platforms, indicating the potential of rule-based RL to tackle complex GUI-related tasks across diverse domains effectively. In summary, our contributions are as follows. We propose UI-R1, which enhances MLLMs reasoning capabilities on GUI action prediction tasks through DeepSeek R1 style reinforcement learning. We design unified rule-based action reward function that effectively aligns with the objectives of common GUI tasks. We utilize the three-stage data selection method and collect only 130+ high-quality training data from the mobile domain. Despite limited data, our model UI-R1-3B achieves notable performance gains on out-of-domain benchmarks, such as those from desktop and web platforms, showcasing adaptability and generalization capability in GUI-related tasks. 2 Figure 2: Overview of UI-R1 training framework. Given GUI screenshot and text instruction from the user, the policy model (i.e., Qwen2.5-VL-3B) generates multiple action planning responses with reasoning. Our proposed rule-based action reward function is then applied, and the policy model is updated using policy gradient optimization algorithm."
        },
        {
            "title": "2 Related Work",
            "content": "2.1 GUI Agents Starting with CogAgent (Hong et al., 2024), researchers have used MLLMs for GUI-related tasks, including device control, task completion, GUI understanding, and more. One line of work, such as the AppAgent series (Zhang et al., 2023; Li et al., 2024b) and the Mobile-Agent series (Wang et al., 2024b;a), integrates commercial generalist models like GPT for planning and prediction tasks. These agents rely heavily on prompt engineering and multi-agent collaboration to execute complex tasks, making them adaptable but dependent on careful manual design for optimal performance. Another branch of research focuses on fine-tuning smaller open-source MLLMs on task-specific GUI datasets (Rawles et al., 2023; Li et al., 2024a; Chai et al., 2024; Gou et al., 2024) to create specialist agents. For example, Chai et al. (2024) enhances agents by incorporating additional functionalities of the GUI element in the Android system, while UGround(Gou et al., 2024) develops special GUI grounding model tailored for precise GUI element localization. Wu et al. (2024) develops foundational model for GUI action prediction. Moving beyond task-specific fine-tuning, UI-TARs (Qin et al., 2025) introduces more comprehensive approach by combining GUI-related pretraining with task-wise reasoning fine-tuning, aiming to better align models with the intricacies of GUI interactions. Despite their differences, all of these existing agents share common reliance on the SFT paradigm. This training approach, while effective, depends heavily on large-scale, high-quality labeled datasets. 2.2 Rule-Based Reinforcement Learning Rule-based reinforcement learning (RL) has recently emerged as an efficient alternative to traditional training paradigms by leveraging predefined rule-based reward functions to guide model behavior. DeepSeek-R1 (Guo et al., 2025) first introduced this approach, using 3 reward functions based on predefined criteria, such as checking whether an LLMs final answer matches the ground truth for math problems. The reward focuses solely on the final results, leaving the reasoning process to be learned by the model itself. Zeng et al. (2025) reproduces the algorithm on models with smaller sizes and illustrates its effectiveness on small language models. Subsequent works (Chen et al., 2025; Shen et al., 2025; Liu et al., 2025; Wang et al., 2025; Peng et al., 2025; Meng et al., 2025), extended the paradigm to multimodal models by designing task-specific rewards for visual tasks, including correct class predictions for image classification and IoU metrics for image grounding and detection. These studies demonstrate the adaptability of rule-based RL for both pure-language and multimodal models. By focusing on task-specific objectives without requiring extensive labeled datasets or human feedback, rule-based RL shows strong potential as scalable and effective training paradigm across diverse tasks."
        },
        {
            "title": "3 Method",
            "content": "UI-R1 is reinforcement learning (RL) training paradigm designed to enhance GUI agents ability to successfully complete low-level instructional tasks. We define low-level instructions as directives that guide the agent to perform actions based on single state (e.g., GUI screenshot), consistent with the definition in ANDROIDCONTROL (Li et al., 2024a). For example, Click the menu icon in the top left corner represents low-level instruction, whereas Create an event for 2 PM tomorrow is high-level instruction. The specifics of the training data selection and reward function design are detailed in the following sections. Figure 2 illustrates the main parts of the framework. 3.1 Preliminary Many rule-based RL works (Guo et al., 2025; Zeng et al., 2025; Liu et al., 2025) adopt the Group Relative Policy Optimization (GRPO) algorithm (Shao et al., 2024) for RL training. GRPO offers an alternative to commonly used Proximal Policy Optimization (PPO) (Schulman et al., 2017) by eliminating the need for critic model. Instead, GRPO directly compares group of candidate responses to determine their relative quality. In GRPO, given task question, the model generates set of potential responses {o1, o2, . . . , oN}. Each response is evaluated by taking the corresponding actions and computing its reward {r1, r2, . . . , rN}. Unlike PPO, which relies on single reward signal and critic to estimate the value function, GRPO normalizes these rewards to calculate the relative advantage of each response. The relative quality Ai of the i-th response is computed as Ai = ri Mean({r1, r2, . . . , rN}) Std({r1, r2, . . . , rN}) , (1) where Mean and Std represent the mean and standard deviation of the rewards, respectively. This normalization step ensures that responses are compared within the context of the group, allowing GRPO to better capture nuanced differences between candidates. Policy updates are further constrained by minimizing the KL divergence between the updated and reference models, ensuring stable RL learning. 3.2 Rule-Based Action Rewards The rule-based reward function introduced by DeepSeek-R1 (Guo et al., 2025) represents foundational step in rule-based RL by simply evaluating whether model predictions exactly match ground-truth answers. This straightforward approach efficiently aligns models with preference alignment algorithms and provides clear optimization signals. For vision-related tasks, works such as VLM-R1 (Shen et al., 2025) and Visual-RFT (Liu et al., 2025) extend this idea by designing task-specific rewards. For image grounding tasks, they compute the IoU between the predicted and ground-truth bounding boxes as the reward. Similarly, for image classification tasks, rewards are determined by checking whether the predicted and ground-truth classes match. In GUI-related tasks, the ability of GUI grounding and understanding is critical requirement for agents. Unlike traditional image grounding tasks, GUI grounding requires agents to identify where specific actions, such as click, should be performed on given GUI screenshot. To address this unique gap, we propose reward function tailored for GUI tasks, as defined in Equation 2: RA = RT + RC + RF , (2) where the predicted action = {T , C} consists of two components: , which represents the action type (e.g., click, swipe), and C, which represents the click coordinate. RF represents the common response format reward. In our tasks, the action space includes Click, Scroll, Back, Open App, Action type reward. and Input Text, covering wide range of common application scenarios in daily life, as inspired by GUIPivot (Wu et al., 2025). The action type reward, denoted as RT , is computed by comparing the predicted action type with the ground truth action type . It assigns reward of 1 if = and 0 otherwise, providing straightforward and effective evaluation mechanism for action type prediction. Coordinate accuracy reward. Through observation, we find that among all action types, the most common action argument error occurs in the mis-prediction of coordinates for the click action when given low-level instruction. To address this issue, we specifically design coordinate accuracy reward. The model is required to output coordinate = [x, y], indicating where the click action should be performed. Given the ground truth bounding box = [x1, y1, x2, y2], the coordinate accuracy reward RC is computed as shown in Equation 3: RC = (cid:26)1 0 if coord in box B, else. (3) Unlike general visual grounding tasks which compute the IoU between the predicted bounding box and the ground truth box, our approach prioritizes action coordinate prediction over element grounding. This focus is more appropriate for GUI agents and better aligns with human intuition, as the ultimate goal is to ensure correct actions are performed rather than merely locating GUI elements. Format reward. During training, we incorporate the widely-used format reward to guide the model in generating its reasoning process and final answer in structured format. This decision is based on our simple experiment that agents producing reasoning processes outperform those directly outputting action predictions by approximately 6%. The reasoning process plays key role in the models self-learning and iterative improvement during reinforcement fine-tuning, while the reward tied to the final answer drives optimization. The format reward, denoted as RF , ensures that the models predictions follow the required HTML tag format, specifically using <think> for the reasoning process and <answer> for the final answer. This structured output not only enhances clarity, but also ensures consistency in the models predictions. Prompt for Training and Inference In this GUI screenshot, want to perform the command instruction. Please provide the action to perform (enumerate in [click, open app, scroll, navigate back, input text]) and the coordinate where the cursor is moved to(integer) if click is performed. Output the thinking process in <think> </think> and final answer in <answer> </answer> tags. The output answer format should be as follows: <think> ... </think> <answer>[action: enum[click, open app, scroll, navigate back, input text], coordinate: [x, y]]</answer>. Please strictly follow the format. 5 3.3 Training Data Selection Compared to SFT, rule-based RL has demonstrated the capability to achieve comparable or even superior performance on mathematical and vision-related tasks using only limited number of training samples (Zeng et al., 2025; Liu et al., 2025). Building on this efficiency and inspired by s1 (Muennighoff et al., 2025), we implement three-stage data selection process to refine open-source GUI-related datasets based on three key principles: Quality, Difficulty, and Diversity. The detailed distribution of the dataset can be found in Appendix A.2. Quality. For refining the click action arguments, we use the mobile subset of ScreenSpot (Cheng et al., 2024) as our initial dataset. ScreenSpot offers clean and wellaligned task-element paired annotations, making it ideal for defining and calculating RC . For other actions, we randomly select 1K episodes from ANDROIDCONTROL (Li et al., 2024a), as it shares similar action space and provides low-level instructions. However, since the element annotations in ANDROIDCONTROL are unfiltered and misaligned, we exclude click action steps and retain the rest. Difficulty. To identify hard samples, we evaluated Qwen2.5-VL-3B on each task instruction by model performance, where sample is labeled hard if the models output does not match the ground truth. We only keep the hard samples among all the data collected. Diversity. We ensure diversity by selecting samples with different action types in ANDROIDCONTROL (e.g., Scroll, Back, Open App, Input Text) and element types in ScreenSpot (e.g. Icon, Text). Rare actions, such as Wait and Long Press, are excluded from ANDROIDCONTROL. After applying these criteria, we finalize high-quality mobile training dataset consisting of 136 samples."
        },
        {
            "title": "4 Experiment",
            "content": "4.1 GUI Grounding Capability We assess the grounding capability of UI-R1 using two benchmarks: ScreenSpot (Cheng et al., 2024) and ScreenSpot-Pro (Li et al., 2025). ScreenSpot evaluates GUI grounding capability across mobile, desktop, and web platforms, while ScreenSpot-Pro focuses on high-resolution professional environments, featuring expert-annotated tasks spanning 23 applications, five industries, and three operating systems. Evaluation results of ScreenSpotV2 (Wu et al., 2024) are in Appendix B. Model Method Model Size Data Size Web Desktop Average Icon Text Icon Text Supervised Fine-tuning SeeClick CogAgent Qwen2.5-VL UGround-V1 AGUVIS SFT SFT SFT SFT SFT 9.6B 18B 3B 7B 7B Zero Shot / Reinforcement Learning Qwen2-VL Qwen2.5-VL UI-R1 ZS ZS RFT 7B 3B 3B 1M - 500 10M 1M 0 0 136 32.5 28.6 63.1 70.4 70.7 25.7 43.2 73.3 55.7 70.4 78.3 80.4 88. 35.2 60.0 85.2 30.0 20.0 46.4 63.6 74.8 54.3 40.0 59.3 72.2 74.2 85.0 82.5 85.7 76.3 80.9 90.2 49.0 51.0 70.1 75.2 80. 46.5 57.1 78.6 Table 1: Grounding accuracy on ScreenSpot. The optimal and the suboptimal results are bolded and underlined, respectively. ZS indicates zero-shot OOD inference and RFT indicates rule-based reinforecement learning. 6 Model Development Creative CAD Scientific Office OS Text Icon Text Icon Text Icon Text Icon Text Icon Text Icon Supervised Fine-tuning SeeClick OS-Atlas-4B ShowUI-2B CogAgent-18B Aria-GUI Qwen2.5-VL-3B* UGround-7B Claude** OS-Atlas-7B 0.6 7.1 16.9 14.9 16.2 15.6 26.6 22.0 33.1 0.0 0.0 1.4 0.7 0.0 0.7 2.1 3.9 1.4 1.0 3.0 9.1 9.6 23.7 13.1 27.3 25.9 28.8 Zero Shot / Reinforcement Fine-tuning Qwen-VL-7B GPT-4o Qwen2-VL-7B Qwen2.5-VL-3B UI-R1-3B 0.0 1.3 2.6 14.9 22.7 0.0 0.0 0.0 2.1 4.1 0.0 1.0 1.5 20.2 27.3 0.0 1.4 0.0 0.0 2.1 2.1 2.8 3.4 2.8 0.0 0.0 0.0 1.4 3. 2.5 2.0 2.5 7.1 7.6 5.6 14.2 14.5 12.2 0.0 2.0 0.5 4.1 11.2 0.0 0.0 0.0 3.1 1.6 3.1 1.6 3.7 4.7 0.0 0.0 0.0 4.7 6.3 3.5 9.0 13.2 22.2 27.1 27.8 31.9 33.9 37.5 0.7 2.1 6.3 34.0 42. 0.0 5.5 7.3 1.8 6.4 8.1 2.7 15.8 7.3 0.0 0.0 0.0 7.3 11.8 1.1 5.1 15.3 13.0 20.3 20.3 31.6 30.1 33.9 0.0 1.1 3.4 22.0 32.2 0.0 3.8 7.5 0.0 1.9 5.7 11.3 16.3 5.7 0.0 0.0 1.9 3.8 11. 2.8 5.6 10.3 5.6 4.7 14.0 17.8 11.0 27.1 0.0 0.0 0.9 6.5 13.1 0.0 0.0 2.2 0.0 0.0 0.0 0.0 4.5 4.5 0.0 0.0 0.0 2.2 4.5 Avg 1.1 3.7 7.7 7.7 11.3 10.8 16.5 17.1 18. 0.1 0.8 1.6 11.8 17.8 Table 2: Accuracy on ScreenSpot-Pro. The optimal and the suboptimal results are bolded and underlined, respectively. * Qwen2.5-VL-3B here is supervised fine-tuned on 500 ScreenSpotmobile data. ** Claude refers to Claude-computer-use. Setting We train the Qwen2.5-VL-3B model on the three-stage selected data (details in Section 3.3) using rule-based RL, naming the resulting model UI-R1-3B. Furthermore, we train the base model using supervised fine-tuning on the entire ScreenSpot mobile set, referring to it as Qwen2.5-VL-3B* in Table 2. For evaluation, an action prediction is considered correct if the predicted click coordinate lies within the ground truth bounding box. Accuracy is computed as the ratio of the correct predictions to the total number of test samples. Analysis Experimental results show that our method significantly improves the GUI grounding capability of the 3B model (+20% on ScreenSpot and +6% on ScreenSpot-Pro from Table 1 and Table 2), surpassing most 7B models on both benchmarks. Additionally, it also achieves performance comparable to the SOTA 7B models (i.e. AGUVIS (Xu et al., 2024) and OS-Atlas (Wu et al., 2024)), which are trained using supervised fine-tuning on substantially larger labeled grounding datasets. Qwen2.5-VL-3B (SFT) in Table 1 demonstrates that supervised fine-tuning (SFT) with limited amount of data (e.g., 500 samples) can effectively improve in-domain performance by tailoring the model to specific tasks. However, the comparison between Qwen2.5-VL3B (ZS) and Qwen2.5-VL-3B (SFT) in Table 2 highlights critical limitation of SFT: its effectiveness significantly diminishes in OOD scenarios. This limitation arises from the dependency of SFT on task-specific labeled data, restricting the models ability to adapt to unseen environments. In contrast, our RL approach not only enhances OOD generalization by focusing on task-specific reward optimization, but also achieves with far fewer training samples, offering scalable and efficient alternative to traditional SFT methods. 4.2 Action Prediction Capability We further evaluate the models ability to predict single-step actions based on low-level instructions. As described in Section 3.3, we test our model on selected subset of ANDROIDCONTROL. The low-level instructions in ANDROIDCONTROL enrich the ScreenSpot benchmark by introducing wider range of action types. Setting The accuracy of the action prediction is evaluated by the accuracies of action type and grounding: (1) The action type accuracy evaluates the match rate between the predicted action types (e.g., click, scroll) and ground truth types; (2) The grounding accuracy focuses specifically on the accuracy of click action argument predictions, similar to Section 4.1. Since ground truth bounding boxes are not consistently available in the 7 Model Method Model size Data size Type Grounding Average Supervised Fine-tuning SeeClick InternVL-2 GUIPivot-Qwen OS-Atlas OS-Atlas SFT SFT SFT SFT SFT 9.6B 4B 7B 4B 7B Zero Shot / Reinforcement Fine-tuning GPT-4o OS-Atlas OS-Atlas Qwen2.5-VL UI-R1 ZS ZS ZS ZS RFT 4B 7B 3B 3B 76K 76K 76K 76K 76K 0 0 0 0 136 93.0 90.9 96.8 91.9 93.6 74.3 64.6 73.0 79.3 94.3 73.4 84.1 75.1 83.8 88.0 38.7 71.2 73.4 72.3 82. 83.2 87.5 86.0 87.8 90.8 56.5 67.9 73.2 75.8 88.5 Table 3: Low-level agent capabilities on ANDROIDCONTROL. The Average column computes the mean of Type and Grounding scores. ANDROIDCONTROL test data, we measure performance by calculating the distance between the predicted and ground truth coordinates. prediction is considered correct if it falls within 14% of the screen size from the ground truth, following the evaluation method of UI-TARS (Qin et al., 2025). Analysis As shown in Table 3, the comparison between UI-R1 and the Qwen2.5-VL (ZS) model highlights the significant benefits of the RL training framework. UI-R1 improves the accuracy of action type prediction by 15% and click element grounding accuracy by 20%, all while using only 136 training data points. Compared with other SFT models, the evaluation results illustrate that UI-R1 not only excels in scenarios with extremely limited training data but also achieves superior type accuracy and grounding performance even than larger models. This underscores the effectiveness of the RL training framework in leveraging small datasets to achieve substantial performance gains, demonstrating its potential as highly data-efficient and scalable approach for training models in resource-constrained environments. 4.3 Performance Factor Study Data Scale In Figure 3 (left), we investigate the relationship between training data size and model performance and compare the two methods of selecting training data from the entire dataset: random selection and select by difficulty (as in Section 3.3). The second method involves selecting the top tasks with the longest reasoning lengths that the Qwen2.5-VL-3B model fails to solve. We find that model performance improves as the training data size increases, but the trend is gradually saturating. Moreover, our select by difficulty method results in significantly better performance than random selection. Reasoning Length In Figure 3 (right), the result reveals that as the reasoning length of the answer increases, the accuracy tends to decrease, suggesting that the questions are getting harder to answer. With reinforcement learning, UI-R1s reasoning ability is significantly enhanced, leading to more pronounced accuracy improvements, especially on more challenging questions. 4.4 Ablation Study Reward Function The design of the reward function plays crucial role in enabling the self-learning capabilities of the model. To evaluate this, we first examine the necessity of the two components of the reward function, action + coord. Specifically, the action reward improves action prediction accuracy, while the coord reward enhances the models ability to ground click elements. Next, we compare this with an alternative reward design, action + bbox, where the coordinate reward RC is replaced by an IoU-based reward RIoU 8 Figure 3: Left: Impact of data selection methods and data size; Right: Study of relation between answering accuracy and reasoning length. Figure 4: Left: Ablation on reward function; Right: Ablation on data selection method. in Equation 2. In this setup, the IoU metric is calculated between the ground truth bounding box and the predicted box, and RIoU assigns value of 1 if IoU > 0.5 and 0 otherwise. Through ablation studies, as shown in Figure 4 (left), we demonstrate the superior effectiveness of RC over RIoU for improving click element grounding. However, we also observe that the action reward does not always positively impact grounding tasks. This is likely because larger action space can introduce ambiguity, making it harder for the model to focus solely on element grounding tasks. These findings highlight the importance of carefully balancing the reward components according to the specific objectives of the task. Data Selection We also examine the impact of different data selection methods, as shown in Figure 4 (right). comparison of three methods across all domains demonstrates that neither random selection nor the use of the entire dataset matches the effectiveness of our three-stage data selection pipeline, indicating that the use of smaller set of high-quality data can lead to higher performance."
        },
        {
            "title": "5 Conclusion",
            "content": "We propose the UI-R1 framework, which extends rule-based reinforcement learning to GUI action prediction tasks, offering scalable alternative to traditional Supervised Fine-Tuning (SFT). We designed novel reward function that evaluates both action type and arguments, enabling efficient learning with reduced task complexity. Using only 130+ training samples from the mobile domain, our approach achieves significant performance improvements and strong generalization to out-of-domain datasets, including desktop and web platforms. The results demonstrate the adaptability, data efficiency, and ability of the rule-based RL to handle specialized tasks effectively."
        },
        {
            "title": "References",
            "content": "Yuxiang Chai, Siyuan Huang, Yazhe Niu, Han Xiao, Liang Liu, Dingyu Zhang, Peng Gao, Shuai Ren, and Hongsheng Li. Amex: Android multi-annotation expo dataset for mobile gui agents. arXiv preprint arXiv:2407.17490, 2024. Liang Chen, Lei Li, Haozhe Zhao, Yifan Song, and Vinci. R1-v: Reinforcing super generalization ability in vision-language models with less than $3. https://github.com/ Deep-Agent/R1-V, 2025. Accessed: 2025-02-02. Kanzhi Cheng, Qiushi Sun, Yougang Chu, Fangzhi Xu, Yantao Li, Jianbing Zhang, and Zhiyong Wu. Seeclick: Harnessing gui grounding for advanced visual gui agents. arXiv preprint arXiv:2401.10935, 2024. Boyu Gou, Ruohan Wang, Boyuan Zheng, Yanan Xie, Cheng Chang, Yiheng Shu, Huan Sun, and Yu Su. Navigating the digital world as humans do: Universal visual grounding for gui agents. arXiv preprint arXiv:2410.05243, 2024. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Wenyi Hong, Weihan Wang, Qingsong Lv, Jiazheng Xu, Wenmeng Yu, Junhui Ji, Yan Wang, Zihan Wang, Yuxiao Dong, Ming Ding, et al. Cogagent: visual language model for gui agents. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1428114290, 2024. Kaixin Li, Ziyang Meng, Hongzhan Lin, Ziyang Luo, Yuchen Tian, Jing Ma, Zhiyong Huang, and Tat-Seng Chua. Screenspot-pro: Gui grounding for professional high-resolution computer use, 2025. Wei Li, William Bishop, Alice Li, Chris Rawles, Folawiyo Campbell-Ajala, Divya Tyamagundlu, and Oriana Riva. On the effects of data scale on computer control agents. arXiv preprint arXiv:2406.03679, 2024a. Yanda Li, Chi Zhang, Wanqi Yang, Bin Fu, Pei Cheng, Xin Chen, Ling Chen, and Yunchao Wei. Appagent v2: Advanced agent for flexible mobile interactions. arXiv preprint arXiv:2408.11824, 2024b. Ziyu Liu, Zeyi Sun, Yuhang Zang, Xiaoyi Dong, Yuhang Cao, Haodong Duan, Dahua arXiv preprint Lin, and Jiaqi Wang. Visual-rft: Visual reinforcement fine-tuning. arXiv:2503.01785, 2025. Yadong Lu, Jianwei Yang, Yelong Shen, and Ahmed Awadallah. Omniparser for pure vision based gui agent. arXiv preprint arXiv:2408.00203, 2024. Fanqing Meng, Lingxiao Du, Zongkai Liu, Zhixiang Zhou, Quanfeng Lu, Daocheng Fu, Botian Shi, Wenhai Wang, Junjun He, Kaipeng Zhang, et al. Mm-eureka: Exploring visual aha moment with rule-based large-scale reinforcement learning. arXiv preprint arXiv:2503.07365, 2025. Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Cand`es, and Tatsunori Hashimoto. s1: Simple test-time scaling, 2025. URL https://arxiv.org/abs/2501.19393. Yingzhe Peng, Gongrui Zhang, Miaosen Zhang, Zhiyuan You, Jie Liu, Qipeng Zhu, Kai Yang, Xingzhong Xu, Xin Geng, and Xu Yang. Lmm-r1: Empowering 3b lmms with strong reasoning abilities through two-stage rule-based rl. arXiv preprint arXiv:2503.07536, 2025. Yujia Qin, Yining Ye, Junjie Fang, Haoming Wang, Shihao Liang, Shizuo Tian, Junda Zhang, Jiahao Li, Yunxin Li, Shijue Huang, et al. Ui-tars: Pioneering automated gui interaction with native agents. arXiv preprint arXiv:2501.12326, 2025. Christopher Rawles, Alice Li, Daniel Rodriguez, Oriana Riva, and Timothy Lillicrap. Androidinthewild: large-scale dataset for android device control. Advances in Neural Information Processing Systems, 36:5970859728, 2023. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Haozhan Shen, Zilun Zhang, Kangjia Zhao, Qianqian Zhang, Ruochen Xu, and Tiancheng Zhao. Vlm-r1: stable and generalizable r1-style large vision-language model. https: //github.com/om-ai-lab/VLM-R1, 2025. Accessed: 2025-02-15. Junyang Wang, Haiyang Xu, Haitao Jia, Xi Zhang, Ming Yan, Weizhou Shen, Ji Zhang, Fei Huang, and Jitao Sang. Mobile-agent-v2: Mobile device operation assistant with effective navigation via multi-agent collaboration. arXiv preprint arXiv:2406.01014, 2024a. Junyang Wang, Haiyang Xu, Jiabo Ye, Ming Yan, Weizhou Shen, Ji Zhang, Fei Huang, and Jitao Sang. Mobile-agent: Autonomous multi-modal mobile device agent with visual perception. arXiv preprint arXiv:2401.16158, 2024b. Weiyun Wang, Zhangwei Gao, Lianjie Chen, Zhe Chen, Jinguo Zhu, Xiangyu Zhao, Yangzhou Liu, Yue Cao, Shenglong Ye, Xizhou Zhu, et al. Visualprm: An effective process reward model for multimodal reasoning. arXiv preprint arXiv:2503.10291, 2025. Zhiyong Wu, Zhenyu Wu, Fangzhi Xu, Yian Wang, Qiushi Sun, Chengyou Jia, Kanzhi Cheng, Zichen Ding, Liheng Chen, Paul Pu Liang, et al. Os-atlas: foundation action model for generalist gui agents. arXiv preprint arXiv:2410.23218, 2024. Zongru Wu, Pengzhou Cheng, Zheng Wu, Tianjie Ju, Zhuosheng Zhang, and Gongshen Liu. Smoothing grounding and reasoning for mllm-powered gui agents with query-oriented pivot tasks, 2025. URL https://arxiv.org/abs/2503.00401. Yiheng Xu, Zekun Wang, Junli Wang, Dunjie Lu, Tianbao Xie, Amrita Saha, Doyen Sahoo, Tao Yu, and Caiming Xiong. Aguvis: Unified pure vision agents for autonomous gui interaction. arXiv preprint arXiv:2412.04454, 2024. Weihao Zeng, Yuzhen Huang, Wei Liu, Keqing He, Qian Liu, Zejun Ma, and Junxian He. 7b model and 8k examples: Emerging reasoning with reinforcement learning is both effective and efficient. https://hkust-nlp.notion.site/simplerl-reason, 2025. Notion Blog. Chi Zhang, Zhao Yang, Jiaxuan Liu, Yucheng Han, Xin Chen, Zebiao Huang, Bin Fu, and Gang Yu. Appagent: Multimodal agents as smartphone users. arXiv preprint arXiv:2312.13771, 2023."
        },
        {
            "title": "A Training",
            "content": "A.1 Setting We configure the hyperparameters as listed in Table 4 and train the base model using 8 NVIDIA 4090 GPUs, completing the training process in approximately 8 hours."
        },
        {
            "title": "Value",
            "content": "lr max pixels num generations num train epochs max prompt length per device train batch size gradient accumulation steps from 9.98e-7 to 0 12845056 8 8 1024 1 2 Table 4: Hyperparameter settings used in the experiments. A.2 Dataset Distribution The distribution of our data selection is listed in Table 5. Trainng dataset Type # Click # Scroll # Input text # Back # Open app # Total UI-R1 Mobile 101 6 4 18 136 Evaluation dataset AndroidControl ScreenSpot* ScreenSpot-pro ID OOD OOD 5074 770 1211 0 0 632 0 0 343 0 0 608 0 0 7868 770 1581 Table 5: Statistics of training and evaluation datasets. * means that we only select subsets Desktop and Web for evaluation. 12 A.3 Visualization Figure 5 illustrates the progression of various variables throughout the training process. Figure 5: UI-R1 training process."
        },
        {
            "title": "B Other Evaluation",
            "content": "B.1 ScreenSpot-V2 We also evaluate the model performance on ScreenSpot-V2 (Wu et al., 2024) and the results are in Table 6. Model GUI specific Size Mobile Web Desktop Icon Text Icon Text Icon Text SeeClick OS-Atlas OS-Atlas UI-TARS Yes Yes Yes Yes Qwen2.5-VL Framework Qwen2.5-VL Qwen2.5-VL UI-R1(Ours) No No Yes 9.6B 4B 7B 2B 3B 7B 3B 50.7 59.7 75.8 79.1 66.8 80.6 84.3 78.4 87.2 95.2 95.2 92.1 95.9 96.2 32.5 63.1 77.3 78.3 46.8 70.0 75. 55.2 85.9 90.6 87.2 72.6 87.2 89.2 29.3 46.4 63.6 68.6 44.3 59.3 63.6 Avg 55.5 71.9 84.1 84. 70.1 72.7 90.7 90.7 83.0 89.2 92.3 70.4 82.6 85.4 Table 6: Grounding accuracy on ScreenSpot-V2. The optimal and the suboptimal results are bolded and underlined, respectively."
        },
        {
            "title": "C Other Ablation",
            "content": "C.1 Training epoches We evaluate the models performance across different training epochs, as shown in Figure 6. Based on the results, we finalize the training at 8 epochs. Figure 6: Accuracy change over rounds. 14 C.2 Max Pixels While adjusting the parameters, we observe that the maximum pixel setting of the image processor plays significant role. If the input image exceeds this maximum pixel value, the smart resize function automatically crops and resizes the image while preserving the original aspect ratio. Mobile images are typically smaller than web or desktop images and often have significantly different aspect ratios. To address this, we implement the algorithm to appropriately rescale the predicted coordinates as shown in Algorithm 1. Algorithm 1 Scale Coordinates Based on Image Resizing = (x, y) : coordinate : input image 1: Input: 2: 3: 4: max pixels : maximum pixel constraint 5: Output: (xscale, yscale) R2 6: (origin width, origin height) I.size 7: (resized height, resized width) smart resize(origin height, origin width, max pixels) smart resize from QwenVL 8: xscale origin width/resized width * 9: yscale origin height/resized height * 10: Output: (xscale, yscale) We also investigate the impact of the maximum pixel value on model performance. Setting this value too high can lead to out-of-memory (OOM) errors during training when processing large images. Conversely, setting it too low may negatively affect the accuracy of prediction results. To better understand this trade-off, we experiment with two different maximum pixel values during training and evaluation, as summarized in Table 7. Based on our analysis, we set the maximum pixel value to 12,845,056 during training, which results in model with improved performance on out-of-domain tasks. For evaluation, we recommend using smaller maximum pixel value to conserve memory. max pixels Train Test Mobile Web Desktop Avg 3211264 3211264 12845056 12845056 3211264 12845056 3211264 91.2 90.8 89.6 90.8 76.1 76.8 78.0 79.6 76.6 76.6 77.8 77.2 82.2 82.3 82.5 83.4 Table 7: Ablation of max pixels in the training and inference."
        },
        {
            "title": "D Case Study",
            "content": "Figure 7 illustrates an example of how UI-R1 trained model can successfully complete the task. Figure 7: An example of use case."
        }
    ],
    "affiliations": [
        "MMLab @ CUHK",
        "vivo AI Lab"
    ]
}