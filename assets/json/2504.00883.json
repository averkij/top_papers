{
    "paper_title": "Improved Visual-Spatial Reasoning via R1-Zero-Like Training",
    "authors": [
        "Zhenyi Liao",
        "Qingsong Xie",
        "Yanhao Zhang",
        "Zijian Kong",
        "Haonan Lu",
        "Zhenyu Yang",
        "Zhijie Deng"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Increasing attention has been placed on improving the reasoning capacities of multi-modal large language models (MLLMs). As the cornerstone for AI agents that function in the physical realm, video-based visual-spatial intelligence (VSI) emerges as one of the most pivotal reasoning capabilities of MLLMs. This work conducts a first, in-depth study on improving the visual-spatial reasoning of MLLMs via R1-Zero-like training. Technically, we first identify that the visual-spatial reasoning capacities of small- to medium-sized Qwen2-VL models cannot be activated via Chain of Thought (CoT) prompts. We then incorporate GRPO training for improved visual-spatial reasoning, using the carefully curated VSI-100k dataset, following DeepSeek-R1-Zero. During the investigation, we identify the necessity to keep the KL penalty (even with a small value) in GRPO. With just 120 GPU hours, our vsGRPO-2B model, fine-tuned from Qwen2-VL-2B, can outperform the base model by 12.1% and surpass GPT-4o. Moreover, our vsGRPO-7B model, fine-tuned from Qwen2-VL-7B, achieves performance comparable to that of the best open-source model LLaVA-NeXT-Video-72B. Additionally, we compare vsGRPO to supervised fine-tuning and direct preference optimization baselines and observe strong performance superiority. The code and dataset will be available soon."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 ] . [ 1 3 8 8 0 0 . 4 0 5 2 : r Improved Visual-Spatial Reasoning via R1-Zero-Like Training Zhenyi Liao1, Qingsong Xie2, Yanhao Zhang2, Zijian Kong1,Haonan Lu2, Zhenyu Yang2, Zhijie Deng1 2OPPO AI Center 1Shanghai Jiao Tong University {l-justice, kong-zijian, zhijied}@sjtu.edu.cn {xieqingsong,zhangyanhao,luhaonan,yangzhenyu}@oppo.com"
        },
        {
            "title": "Abstract",
            "content": "Increasing attention has been placed on improving the reasoning capacities of multi-modal large language models (MLLMs). As the cornerstone for AI agents that function in the physical realm, video-based visual-spatial intelligence (VSI) emerges as one of the most pivotal reasoning capabilities of MLLMs. This work conducts first, in-depth study on improving the visual-spatial reasoning of MLLMs via R1-Zero-like training. Technically, we first identify that the visual-spatial reasoning capacities of smallto mediumsized Qwen2-VL models cannot be activated via Chain of Thought (CoT) prompts. We then incorporate GRPO training for improved visual-spatial reasoning, using the carefully curated VSI-100k dataset, following DeepSeek-R1-Zero. During the investigation, we identify the necessity to keep the KL penalty (even with small value) in GRPO. With just 120 GPU hours, our vsGRPO-2B model, fine-tuned from Qwen2-VL-2B, can outperform the base model by 12.1% and surpass GPT-4o. Moreover, our vsGRPO-7B model, fine-tuned from Qwen2-VL-7B, achieves performance comparable to that of the best open-source model LLaVA-NeXT-Video-72B. Additionally, we compare vsGRPO to supervised fine-tuning and direct preference optimization baselines and observe strong performance superiority. The code and dataset will be available at https://github.com/zhijie-group/R1-Zero-VSI."
        },
        {
            "title": "Introduction",
            "content": "Multimodal Large Language Models (MLLMs) have emerged as significant advancement in AI. They typically accept text, images, and videos as inputs and output textual responses, serving as the foundations for various applications, including multi-modal understanding (Liu et al., 2023; Wang et al., 2024a), visual language agents (Hong et al., 2024; Wang et al., 2024b), autonomous driving (Pan et al., 2024; Wang et al., 2024c), etc. The exhaustive understanding of multi-modal observations hinges on advanced reasoning capabilities, which has spurred growing interest in investigating reasoning mechanisms within MLLMs (Surıs et al., 2023; Xu et al., 2024; Zhang et al., 2025). This trend mirrors concurrent advancements in vanilla LLMs (Yao et al., 2023; Lightman et al., 2023; Wei et al., 2022; Wang et al.). As the foundation for AI agents operating in the physical world, the video-based visual-spatial reasoning stands out as one of the most crucial capacities of MLLMs. Yet, existing models often fall short in this (see VSI-bench (Yang et al., 2024)). This work conducts systematic study on improving the visual-spatial reasoning capacities of MLLMs based on R1-Zero-like training. Focusing on the Qwen2-VL models (Wang et al., 2024a), we first perform an initial study on whether simple reasoning-oriented prompts can activate the visual-spatial reasoning capacities. We investigate various CoT (Wei et al., 2022) strategies, but find that vanilla non-CoT prompts perform the best for smallto medium-sized Qwen2-VL on VSI-bench. This exposes the issue that such models does not have the ability to trade inference FLOPs for improved visual-spatial reasoning. Works done during the internship in OPPO AI Center. Corresponding authors. 1 Backbone Methods Avg Obj. Count Abs. Dist. Qwen2VL-2B Qwen2VL-7B Think-mode 22.9 Observe-mode 21.8 23.3 Vanilla-mode Think-mode 31.3 Observe-mode 32.0 32.2 Vanilla-mode 18.4 16.8 21.4 44.8 29.9 39. Obj. Size 31.5 32.7 32.3 4.3 1.7 3.4 26.1 25.3 19.0 39.6 25.0 25.8 Room Size Rel. Dist. Rel. Dir. Route Plan Appr. Order 17.3 22.7 31.1 23.4 32.0 43.2 28.3 22.9 28.8 27.6 26.7 27. 34.7 30.9 34.6 40.0 32.6 30.9 26.2 26.2 24.7 32.9 36.0 27.8 16.8 18.1 18.9 31.5 24.4 32.6 Table 1: Quantitative comparisons of different prompting strategies on Qwen2-VL-2B and Qwen2-VL-7B on VSI-bench. Following the journey of DeepSeek-R1-Zero (Guo et al., 2025), we decide to improve the visual-spatial reasoning capacities of Qwen2-VL with GRPO (Shao et al., 2024). Considering the scarcity of training data for visual-spatial question answering, we construct videobased question answering dataset of more than 100k samples, VSI-100k, following the protocol of VSI-bench. Specifically, we leverage ScanNet (Dai et al., 2017) to get highfidelity video scans accompanied by meticulous object-level 3D annotations, based on which (question, answer) pairs regarding spatial information can be easily crafted. Following common practice (Guo et al., 2025; Chen et al., 2025), we define the rule-based reward function based on the alignment between the model prediction and the ground-truth answer to perform GRPO. We also include format reward when trying to activate the CoT reasoning behaviour. The GRPO on VSI-100k turns the pretrained Qwen2-VL-2B model into the performant vsGRPO-2B within just 120 GPU hours. We observe that vsGRPO-2B outperforms the base model by 12.1%. The same pipeline also transforms the Qwen2-VL-7B model into vsGRPO-7B, achieving performance similar to that of the best open-source model with 72B parameters. During GRPO training, we have identified the necessity to keep the KL penalty (even with small value) in the training of GRPO and observed phenomena such as reward hacking. We also compare GRPO with supervised fine-tuning (SFT) and direct preference optimization (DPO) (Rafailov et al., 2023), and confirm the superiority of GRPO in improving the spatial-visual reasoning capacities of Qwen2-VL."
        },
        {
            "title": "2 Can Visual-spatial Reasoning Capacities Be Activated by Prompting?",
            "content": "We initiate by evaluating Qwen2-VL on the VSI-bench with various prompting strategies. Concretely, the VSI-bench includes two types of question-answer problems: Numerical Answer (NA), including tasks such as object counting, absolute distance measurement, object size evaluation, and room size assessment; Multiple-Choice Answer (MCA), including tasks related to relative distance, relative direction, route planning, and appearance order. To evaluate the reasoning capacities of Qwen2-VL on this dataset, we consider two CoT prompting strategies: the widely adopted think-mode, where the model first thinks and then replies to the question, and the observe-mode, where the model first observes the input video and then replies. The latter follows human-like pattern and has been explored in related works (Wu et al., 2024). We also include non-CoT vanilla-mode for comparison. Here is summarization of them: Think-mode: Lets think step by step and then answer the question using single word or phrase. Observe-mode: Please observe the video first and then answer the question using single word or phrase. Vanilla-mode: Please answer the question using single word or phrase. 2 Figure 1: Comparison between the vanilla-mode and think-mode predictions. CoT prompting is ineffective for smallto medium-sized Qwen2-VL on VSI-bench. As shown in Table 1, despite longer responses, think-mode and observe-mode underperform the simple vanilla-mode. Namely, smallto medium-sized Qwen2-VL cannot trade inference FLOPs for improved visual-spatial reasoning. We visualize some output examples given by Qwen2-VL-2B in Figure 1. We see that the model can actually understand the instructions for activating thinking, but the final answer is still wrong, the same as that of the vanilla prompting. From the exposed chain of thoughts, we realize that the error may arise from the failure to perceive the sofa in the video."
        },
        {
            "title": "3 R1-Zero-like Training for Visual-spatial Reasoning",
            "content": "Given the above observations, we realize it is necessary to fine-tune the Qwen2-VL models for improved visual-spatial reasoning. Typically, we opt to focus on the GRPO approach (Shao et al., 2024) given its success in building DeepSeek-R1-Zero."
        },
        {
            "title": "3.1 Training Data Construction",
            "content": "We first create video-based question-answering dataset named VSI-100k for visual-spatial It consists of more than 100k samples and follows the VSI-bench protocol. reasoning. Specifically, we utilize ScanNet (Dai et al., 2017) to obtain high-fidelity video scans that come with detailed object-level 3D annotations. With such information, it becomes straightforward to generate (question, answer) pairs related to spatial information. Specifically, we construct questions regarding six topics, including object count, relative direction, relative distance, object size, room size, and absolute distance. We leave the other two topics in VSI-bench, route planning and appearance order, held out. This corresponds to two reasons: 1) (question, answer) pairs of the latter two topics cannot be simply constructed given rarely the static 3D information, which implies that expensive manual annotation can be required; 2) with this, we can test the task generalization ability of the trained models. For the NA type problems, we implement question template similar to that used in (Yang et al., 2024). For the MCA one, we simplify the question format by removing the options. This adjustment enhances the models capacity to recognize entity correspondence instead of simply matching symbols. Some examples are provided in Figure 2. For object count, we construct answers with directly the object labels included in the annotations document, yielding total of 6.4k samples. For absolute distance, we first remove objects that appear multiple times to ensure specification preciseness, and then calculate the distance between geometric centers of various 3D point-cloud objects, obtaining 3 Figure 2: Illustrations of the constructed dataset. 75k samples. For relative distance, we fix one targeted object and compute the absolute distance between it and four other objects to estimate relative distance, yielding 13k samples. For relative direction, we select one object as the front and determine the relative direction of two objects based on their geometric centers of point clouds, getting total of 8k samples. For object size, we leverage the 3D bounding box to compute the longest dimension of the object, yielding total of 13k samples. For room size, we apply the alpha shape algorithm to the total 1.5k scenes, resulting in 1.5k samples."
        },
        {
            "title": "3.2 GRPO",
            "content": "Group Relative Policy Optimization, also abbreviated as GRPO (Shao et al., 2024), is type of reinforcement learning (RL) that eliminates the critic model to reduce training costs. Specifically, group of generated output set {o1, o2, , oG} is sampled for each question from policy model πθold . Then GRPO optimizes the model πθ using the following objective: JGRPO(θ) = qP(Q), {oi}G (Oq) (cid:34) 1 G i=1 min i=1πθold (cid:16) πθ(oi q) (oi q) πθold Ai, clip (cid:16) πθ(oi q) (oi q) πθold , 1 ε, 1 + ε (cid:17) (cid:17) Ai β DKL (cid:0)πθ (cid:13) (cid:13) πref (cid:1) (cid:35) , (1) where ε and β are the clipping hyper-parameter and the coefficient controlling the KullbackLeibler (KL) penalty, respectively, and Ai = rimean({r1,r2,...,rG}) is the computed advantage using the group rewards {r1, r2, , rG}. DKL 1 is the KL divergence. std({r1,r2,...,rG}) (cid:0)πθ πref πθ (oiq) log (cid:16) πref(oiq) πθ (oiq) (cid:1) = πref(oiq) (cid:17) The reward guides the direction of the training process and is crucial. We adhere to (Chen et al., 2025; Meng et al., 2025) of using format rewards and accuracy rewards, but with necessary modifications. Format Reward. Although the CoT prompts are useless for the small-sized Qwen2-VL-2B in inference time, we still wonder if training with them is beneficial for GRPO. As result, following recent progress in the community, we consider three training prompts for GRPO: Think-mode: Please think step by step and enclose your thinking process in <think> </think> tags and then provide the short answer with one or two words or number in <answer> </answer>. 4 Methods Eval. Mode Avg Obj. Count Abs. Dist. Obj. Size Room Size Rel. Dist. Rel. Dir. Route Plan Appr. Order Qwen2-VL-2B + SFT + DPO + vsGRPO-T + vsGRPO-O + vsGRPO-T + vsGRPO-O + vsGRPO-V Qwen2-VL-7B + SFT + DPO + vsGRPO-V IVL2-2B LNV-7B IVL2-40B LNV-72B GPT-4o Gemini-1.5 Pro V V V"
        },
        {
            "title": "V\nV",
            "content": "Open-source 23.3 29.6 23.9 26.1 28.0 29.6 31.2 35.4 32.2 38.1 32.6 40.7 27.4 35.6 36.0 40.9 21.4 29.6 21.7 24.7 26.2 35.0 34.6 53.6 39.4 44.7 39.1 59. 21.8 48.5 34.9 48.9 3.4 23.5 3.7 10.7 16.4 28.2 22.5 29.0 25.0 27.6 25.2 29.6 24.9 14.0 26.9 22.8 32.3 47.4 34.8 37.4 44.8 34.7 44.8 52.7 25.8 46.1 26.5 50. 22.0 47.8 46.5 57.4 31.1 33.5 32.4 36.2 38.2 25.2 33.7 43.4 43.2 50.4 44.2 48.3 35.0 24.2 31.8 35.3 26.7 26.9 27.1 27.3 27.0 28.0 29.4 28.1 32.6 34.0 32.6 35. 33.8 43.5 42.1 42.4 27.7 28.3 28.5 29.5 29.3 38.5 41.8 30.9 30.9 35.7 30.9 35.6 44.2 42.4 32.2 36.7 24.7 28.8 24.2 25.7 24.2 28.5 26.8 26.8 27.8 33.0 29.3 34. 30.5 34.0 34.0 35.0 Close-source 18.9 18.6 18.6 17.9 18.2 18.7 15.8 18.9 32.6 33.4 33.3 31.5 7.1 30.6 39.6 48.6 34.0 48. 46.2 49.6 5.3 28.8 43.8 58.6 38.2 49.4 37.0 46.0 41.3 48. 31.5 42.0 28.5 68.0 Table 2: Quantitative results on VSI-bench. vsGRPO-T, vsGRPO-O, and vsGRPO-V refer to GRPO training on VSI-100k with prompts of think-mode, observe-mode, and vanilla-mode respectively. V, T, and in the Eval. Mode column refer to using vanilla-mode, think-mode, and observe-mode prompts for evaluation, respectively. We also present the best performance of open-source models under the specific model size, like LLaVA-NeXT-Video (Li et al., 2024) (LNV for short) and InternVL2 (Chen et al., 2024) (IVL2 for short), and closesource ones like GPT-4o (Hurst et al., 2024) and Gemini-1.5 Pro (Team et al., 2024). Observe-mode: Please observe carefully and analyze what you see that helps you to solve the question in the video and enclose it in <observe> </observe> tag, and then provide the short answer with one or two words or number in <answer> </answer>. Vanilla-mode: Please provide the short answer with one or two words or number. The format reward quantifies how the responses follow the specified format. It returns score of 1 or 0. Note that such reward is omitted for the vanilla-mode. Accuracy Reward. In the case of non-NA tasks, we employ character matching method to assess accuracy, awarding score of 1 for match and 0 for mismatch. For NA tasks, we develop function that computes the absolute difference between the true value and the predicted one and divides the result by the minimum of the two values. Experimental Settings. Unless specified otherwise, we use Qwen2-VL-2B/7B as the base models due to resource constraints. We employ LoRA (Hu et al., 2022) training with learning rate of 105 for Qwen2-VL-2B and 5 106 for Qwen2-VL-7B. We conduct 14 rollouts per question and set the default sampling temperature to 1. The KL divergence coefficient β is set to 0.0001. 5 Figure 3: Left: the format reward curve of β = 0 and β = 0.0001 during training. Right: the curve of the format reward, the accuracy reward curve of one subtask of VSI-100k, and the total reward curve during GRPO training. 3.3 Results and Analyses"
        },
        {
            "title": "3.3.1 Main Results",
            "content": "Let vsGRPO-T, vsGRPO-O, and vsGRPO-V denote the GRPO training on VSI-100k with prompts of think-mode, observe-mode, and vanilla-mode respectively. We evaluate them with the corresponding test prompts by default. Given the studies in the previous section, we also test the trained models with vanilla-mode prompts. As shown in Table 2, for models based on Qwen-VL-2B, all GRPO fine-tuned models improve over the baseline. Besides, for the models trained with CoT prompting strategies, their CoT test performance outperforms vanilla one. This indicates that GRPO training can effectively enhance the models long reasoning capabilities. Notably, directly applying the vanilla-mode prompting strategy yields the best performance improvements, particularly for NA questions, and even outperforms GPT-4o. We refer to this model as vsGRPO-2B by default. This underscores the conclusion that CoT prompting is ineffective for the small-sized Qwen2-VL-2B on the VSI-bench. In terms of Qwen2-VL-7B, we only tried vsGRPO-V considering the above results. We observe that vsGRPO-V performs the best on two subtasksobject counting and absolute distance. Moreover, the test performance on the Route Plan is also improved, similar to the 2B case. This is possibly because the Route Plan can be divided into sub-tasks that include relative direction, indicating inter-task generalization. With only 7B model size, we note that our model shows performance comparable to that of the leading open-source model, LLaVA-NeXT-Video-72B (Li et al., 2024)."
        },
        {
            "title": "3.3.2 Importance of KL Penalty",
            "content": "The KL penalty term plays crucial role in regulating the divergence between the online policy and the frozen reference one. It avoids the model straying too far from the initial point. While some works (Yu et al., 2025; Meng et al., 2025) advocate for removing the KL penalty to enhance performance, we have observed that doing so can easily lead to training collapse, as illustrated in Figure 3 (left). In contrast, introducing positive β (even very small, such as 0.0001) can effectively address this issue. This may be attributed to the specific nature of VSI reasoning problems."
        },
        {
            "title": "3.3.3 Reward Hacking",
            "content": "During the training, we observed that the model sometimes finds ways to obtain high rewards that do not align with our intentions. One example is that when training with observe-mode, there are some extreme samples in the rollouts, e.g., <think> </think> <answer>xx</answer>. While this format is technically correct, it degrades to missing observation. Observing this, we opt to incorporate length reward function for mitigation. However, we note that some new generations just add extra <think></think> and <answer></answer> tags to exploit the length reward, which does not contribute to meaningful thinking process. So, more reasonable reward functions should be explored."
        },
        {
            "title": "3.3.4 Dynamics of Various Rewards",
            "content": "As shown in Figure 3 (right), during the GRPO training, the format reward converges to 1 quickly in the early stage, while the accuracy reward increases slowly. It seems that there is an upper bound on the accuracy reward. How to address this remains an open problem."
        },
        {
            "title": "3.3.5 Comparison to Other Training Approaches",
            "content": "We also compare our approach with commonly used fine-tuning algorithms, supervised fine-tuning (SFT) and direct preference optimization (DPO) (Rafailov et al., 2023), in Table 2. For SFT, we directly use the constructed VSI-100k for tuning. For DPO, the correct answer is modified to wrong one to serve as the less-preferred answer. As shown, the two approaches both improve over the base model on the VSI-bench, but still lag behind GRPO-V. Besides, the improvement of DPO is minor, which is perhaps because of the sub-optimal preference pair construction."
        },
        {
            "title": "4 Conclusion",
            "content": "In this work, we center on the video-based visual-spatial intelligence of MLLMs. Using Qwen2-VL as the base models, we identify that visual-spatial reasoning capacities of Qwen2VL-2B/7B cannot be activated via CoT prompts. We construct VSI-100k to combat data scarcity and adapt GRPO training. Extensive experiments demonstrate that vsGRPO-2B and vsGRPO-7B outperform models of the same size, highlighting the superiority of the GRPO approach in comparison to SFT and DPO. We also share some findings for future research."
        },
        {
            "title": "References",
            "content": "Liang Chen, Lei Li, Haozhe Zhao, Yifan Song, and Vinci. R1-v: Reinforcing super generalization ability in vision-language models with less than $3. https://github.com/ Deep-Agent/R1-V, 2025. Accessed: 2025-02-02. Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 2418524198, 2024. Angela Dai, Angel Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and Matthias Nießner. Scannet: Richly-annotated 3d reconstructions of indoor scenes. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 58285839, 2017. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Wenyi Hong, Weihan Wang, Qingsong Lv, Jiazheng Xu, Wenmeng Yu, Junhui Ji, Yan Wang, Zihan Wang, Yuxiao Dong, Ming Ding, et al. Cogagent: visual language model for gui agents. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1428114290, 2024. Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. ICLR, 1(2):3, 2022. Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. 7 Feng Li, Renrui Zhang, Hao Zhang, Yuanhan Zhang, Bo Li, Wei Li, Zejun Ma, and Chunyuan Li. Llava-next-interleave: Tackling multi-image, video, and 3d in large multimodal models. arXiv preprint arXiv:2407.07895, 2024. Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step. In The Twelfth International Conference on Learning Representations, 2023. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36:3489234916, 2023. Fanqing Meng, Lingxiao Du, Zongkai Liu, Zhixiang Zhou, Quanfeng Lu, Daocheng Fu, Botian Shi, Wenhai Wang, Junjun He, Kaipeng Zhang, et al. Mm-eureka: Exploring visual aha moment with rule-based large-scale reinforcement learning. arXiv preprint arXiv:2503.07365, 2025. Chenbin Pan, Burhaneddin Yaman, Tommaso Nesti, Abhirup Mallik, Alessandro Allievi, Senem Velipasalar, and Liu Ren. Vlp: Vision language planning for autonomous driving. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1476014769, 2024. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. Advances in Neural Information Processing Systems, 36:5372853741, 2023. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Dıdac Surıs, Sachit Menon, and Carl Vondrick. Vipergpt: Visual inference via python execution for reasoning. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 1188811898, 2023. Gemini Team, Petko Georgiev, Ving Ian Lei, Ryan Burnell, Libin Bai, Anmol Gulati, Garrett Tanzer, Damien Vincent, Zhufeng Pan, Shibo Wang, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024a. Xiaohan Wang, Yuhui Zhang, Orr Zohar, and Serena Yeung-Levy. Videoagent: Long-form video understanding with large language model as agent. In European Conference on Computer Vision, pp. 5876. Springer, 2024b. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. In The Eleventh International Conference on Learning Representations. Yuqi Wang, Jiawei He, Lue Fan, Hongxin Li, Yuntao Chen, and Zhaoxiang Zhang. Driving into the future: Multiview visual forecasting and planning with world model for autonomous driving. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1474914759, 2024c. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. Yixuan Wu, Yizhou Wang, Shixiang Tang, Wenhao Wu, Tong He, Wanli Ouyang, Philip Torr, and Jian Wu. Dettoolchain: new prompting paradigm to unleash detection ability of mllm. In European Conference on Computer Vision, pp. 164182. Springer, 2024. 8 Guowei Xu, Peng Jin, Li Hao, Yibing Song, Lichao Sun, and Li Yuan. Llava-o1: Let vision language models reason step-by-step. arXiv preprint arXiv:2411.10440, 2024. Jihan Yang, Shusheng Yang, Anjali Gupta, Rilyn Han, Li Fei-Fei, and Saining Xie. Thinking in space: How multimodal large language models see, remember, and recall spaces. arXiv preprint arXiv:2412.14171, 2024. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. Advances in neural information processing systems, 36:1180911822, 2023. Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Tiantian Fan, Gaohong Liu, Lingjun Liu, Xin Liu, et al. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476, 2025. Jingyi Zhang, Jiaxing Huang, Huanjin Yao, Shunyu Liu, Xikun Zhang, Shijian Lu, and Dacheng Tao. R1-vl: Learning to reason with multimodal large language models via step-wise group relative policy optimization. arXiv preprint arXiv:2503.12937, 2025."
        }
    ],
    "affiliations": [
        "OPPO AI Center",
        "Shanghai Jiao Tong University"
    ]
}