{
    "paper_title": "Mono4DGS-HDR: High Dynamic Range 4D Gaussian Splatting from Alternating-exposure Monocular Videos",
    "authors": [
        "Jinfeng Liu",
        "Lingtong Kong",
        "Mi Zhou",
        "Jinwen Chen",
        "Dan Xu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce Mono4DGS-HDR, the first system for reconstructing renderable 4D high dynamic range (HDR) scenes from unposed monocular low dynamic range (LDR) videos captured with alternating exposures. To tackle such a challenging problem, we present a unified framework with two-stage optimization approach based on Gaussian Splatting. The first stage learns a video HDR Gaussian representation in orthographic camera coordinate space, eliminating the need for camera poses and enabling robust initial HDR video reconstruction. The second stage transforms video Gaussians into world space and jointly refines the world Gaussians with camera poses. Furthermore, we propose a temporal luminance regularization strategy to enhance the temporal consistency of the HDR appearance. Since our task has not been studied before, we construct a new evaluation benchmark using publicly available datasets for HDR video reconstruction. Extensive experiments demonstrate that Mono4DGS-HDR significantly outperforms alternative solutions adapted from state-of-the-art methods in both rendering quality and speed."
        },
        {
            "title": "Start",
            "content": "MONO4DGS-HDR: HIGH DYNAMIC RANGE 4D GAUSSIAN SPLATTING FROM ALTERNATING-EXPOSURE MONOCULAR VIDEOS Jinfeng Liu1 Lingtong Kong2 Mi Zhou2 1The Hong Kong University of Science and Technology (HKUST) 2vivo Mobile Communication Co., Ltd {jliugk,danxu}@cse.ust.hk Jinwei Chen2 Dan Xu1 {ltkong,zhoumi,jinwei.chen}@vivo.com 5 2 0 2 1 ] . [ 1 9 8 4 8 1 . 0 1 5 2 : r a"
        },
        {
            "title": "ABSTRACT",
            "content": "We introduce Mono4DGS-HDR, the first system for reconstructing renderable 4D high dynamic range (HDR) scenes from unposed monocular low dynamic range (LDR) videos captured with alternating exposures. To tackle such challenging problem, we present unified framework with two-stage optimization approach based on Gaussian Splatting. The first stage learns video HDR Gaussian representation in orthographic camera coordinate space, eliminating the need for camera poses and enabling robust initial HDR video reconstruction. The second stage transforms video Gaussians into world space and jointly refines the world Gaussians with camera poses. Furthermore, we propose temporal luminance regularization strategy to enhance the temporal consistency of the HDR appearance. Since our task has not been studied before, we construct new evaluation benchmark using publicly available datasets for HDR video reconstruction. Extensive experiments demonstrate that Mono4DGS-HDR significantly outperforms alternative solutions adapted from state-of-the-art methods in both rendering quality and speed. The project page for this paper is available at https://liujf1226.github.io/Mono4DGS-HDR."
        },
        {
            "title": "INTRODUCTION",
            "content": "High dynamic range novel view synthesis (HDR NVS) aims to reconstruct and render HDR scenes from multi-view low dynamic range (LDR) images captured at varying exposure levels. By leveraging advanced scene representation techniques, such as Neural Radiance Field (NeRF) (Mildenhall et al., 2020) and 3D Gaussian Splatting (3DGS) (Kerbl et al., 2023), recent HDR NVS methods (Huang et al., 2022; Cai et al., 2024; Liu et al., 2025a; Wu et al., 2024a) have achieved realistic reconstruction and real-time rendering. Most of them are designed for static scenes, while HDRHexplane (Wu et al., 2024a) is the first to investigate on HDR NVS of dynamic scenes. However, it only validates its approach on synthetic scenes with known camera poses in multi-camera setup. In practice, more applicable scenario involves using single handheld camera to capture HDR dynamic scenes in the wild. Therefore, this work focuses on recovering 4D HDR scenes from alternating-exposure monocular LDR videos with unknown camera parameters, as demonstrated in Fig. 1(a). To the best of our knowledge, no existing method has yet explored this challenging task. Current unposed 4D reconstruction systems (Lei et al., 2025; Wang et al., 2025b; Park et al., 2025) for standard monocular videos with unchanged brightness usually leverage 2D priors from vision foundation models, including tracking, depth and optical flow, to provide scene initializations and constraints. In our input case of alternating-exposure video frames, we surprisingly observe that these 2D priors can still be effectively extracted (see in supplementary videos). However, simply extending these 4D reconstruction methods to HDR mode can lead to suboptimal results, as shown in Fig. 1(b). First, the 2D prior knowledge is still noisy and incomplete, leading to coarse and inaccurate scene initializations. Second, the varying brightness in the input video frames makes it infeasible to optimize camera poses through photometric reprojection error as Park et al. (2025). Moreover, the unreliable camera motion can further destabilize the recovery of scene dynamics and 1 (a) Our Mono4DGS-HDR can reconstruct high-quality 4D HDR scenes from unFigure 1: posed monocular LDR videos with alternating exposures. (b) Compared to simply extending SplineGS (Park et al., 2025), MoSca (Lei et al., 2025) and GFlow (Wang et al., 2025b) to HDR mode, our approach achieves significantly better reconstruction quality. geometry, resulting in poor reconstruction quality. Finally, since direct HDR supervision is absent, the recovered HDR scene appearance may be temporally inconsistent and contain color artifacts. To address the above-discussed issues, we present Mono4DGS-HDR, 4D HDR reconstruction system based on Gaussian Splatting, which relies on novel two-stage optimization approach to progressively refine the camera poses and HDR scene representation. In the first stage, inspired by SaV (Sun et al., 2024), we optimize dynamic HDR Gaussians in 3D canonical space with an orthographic camera model, which can eliminate the need for camera poses, making HDR training video reconstruction easier and better. The learned HDR video Gaussian representation gives two advantages: (1) Consistent brightness among reconstructed HDR video frames enables reliable camera pose optimization via photometric reprojection error. (2) These video Gaussians provide good initialization for the subsequent world Gaussian optimization. We can transform the learned video Gaussians into world space by initial camera parameters from bundle adjustment (Lei et al., 2025). The world Gaussian scaling is initialized using the invariance of the projected 2D Gaussian covariance. In the second stage, we jointly optimize camera poses and world Gaussians. The initialization from video Gaussians and the HDR photometric reprojection loss can speed up the convergence and benefit the final reconstruction quality. To further enhance the temporal consistency of HDR scene appearance, we propose temporal luminance regularization strategy, including flow-guided photometric loss aligning per-pixel HDR irradiance between consecutive frames, which ensures the temporal stability of HDR reconstruction and rendering. Since our task setup has not been explored before, we build new evaluation benchmark based on publicly available datasets (Kronander et al., 2014; Froehlich et al., 2014; Kalantari et al., 2013; Chen et al., 2021) for HDR video reconstruction, which contain real-world LDR videos with alternating exposures and synthetic HDR videos. Experiments on the datasets demonstrate that our Mono4DGS-HDR significantly outperforms alternative solutions adapted from existing 4D reconstruction systems in both rendering quality and speed. Our contributions are summarized as follows: We present Mono4DGS-HDR, the first system for reconstructing 4D HDR scenes from unposed monocular LDR videos captured with alternating exposures. We propose unified framework with two-stage optimization procedure that learns video Gaussians in the first stage, transfers the Gaussians to world space, and then optimizes world Gaussians along with camera poses in the second stage. We also conduct temporal luminance regularization to enhance HDR temporal stability. We construct new benchmark for evaluation and show that our approach significantly outperforms adapted alternative solutions. 2 Figure 2: Overview of Mono4DGS-HDR. (a) We infer vision foundation models on the input alternating-exposure video to extract 2D priors, which provide scene initialization and regularization. (b) We propose novel two-stage Gaussian optimization procedure, which includes video Gaussian training in the first stage, world Gaussian fine-tuning in the second stage, and videoto-world Gaussian transformation strategy. The HDR Gaussians are optimized through 2D prior supervision, Gaussian motion regularization, temporal luminance regularization and HDR photometric reprojection loss."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Dynamic Reconstruction and View Synthesis. Dynamic scene reconstruction and view synthesis has experienced great breakthrough by the advent of NeRF and 3DGS. Early dynamic NeRF methods usually represent the 4D scenes by time-varying NeRFs (Park et al., 2023; Gao et al., 2021; Li et al., 2022), or canonical space NeRF with deformation field (Guo et al., 2023; Park et al., 2021a;b; Fang et al., 2022; Xian et al., 2021; Pumarola et al., 2021). Recent methods begin to extend the efficient 3DGS representation to dynamic scenes. They model dynamic contents by learning Gaussian deformation (Yang et al., 2024b; Wu et al., 2024b; Bae et al., 2024), motion trajectories (Li et al., 2024a; Lin et al., 2024; Lee et al., 2024; Yoon et al., 2025), or direct 4D Gaussian primitives (Yang et al., 2024a; Duan et al., 2024). While most of these works take as input synchronized multi-view videos to make problem easier, growing body of works (Liu et al., 2023; Sun et al., 2024; Wang et al., 2025a; Lei et al., 2025; Wang et al., 2025b; Park et al., 2025; Stearns et al., 2024; Liu et al., 2025b) address the more practical and challenging scenario of monocular videos. Among them, RoDynRF (Liu et al., 2023), MoSca (Lei et al., 2025), SplineGS (Park et al., 2025) and GFlow (Wang et al., 2025b) are notable for handling unposed monocular videos, utilizing the 2D prior knowledge of tracking, depth and optical flow. However, they are designed for standard videos with consistent brightness and struggle with the challenges posed by alternating exposures. In this work, we introduce novel framework to explicitly solve on such varying-exposure videos. HDR Novel View Synthesis. Existing HDR NVS approaches generally fall into two categories. The first leverages noisy RAW sensor data (Mildenhall et al., 2022; Wang et al., 2024; Jin et al., 2024; Singh et al., 2024; Li et al., 2024b), which is particularly suited for low-light or nighttime scenarios. This work is more related to the second category, which utilizes multi-exposure and multi-view LDR images for training (Huang et al., 2022; Jun-Seong et al., 2022; Wu et al., 2024a; Cai et al., 2024; Wu et al., 2024c; Liu et al., 2025a). HDR-NeRF (Huang et al., 2022) pioneers the idea of learning HDR radiance fields by regressing HDR irradiance rather than LDR color and introduces tone-mapping MLPs to model the camera response function (CRF). Later works, including HDR-Plenoxels (JunSeong et al., 2022) and HDR-GS (Cai et al., 2024), improve the efficiency by adopting alternative scene representations such as Plenoxels (Fridovich-Keil et al., 2022) and 3DGS. GaussHDR (Liu et al., 2025a) learns unified 3D and 2D local tone mapping for stable and high-quality HDR reconstruction. Casual3DHDR (Gong et al., 2025) applies continuous-time trajectory to jointly optimize camera poses, CRF and exposure times from casually captured videos. However, most of these 3 Figure 3: (a) Our video-world Gaussian Transformation Strategy, including dynamic/static identification, attribute transformation and re-fitting. (b) Example of transformed dynamic/static world Gaussians. (c) Without occlusion handling, the dynamic/static separation is inaccurate. (d) Without 2D covariance invariance (directly inherit scaling), the world Gaussians have unreasonable scales. methods are designed for static scenes. Although HDR-HexPlane (Wu et al., 2024a) extends this task to dynamic scenes with HexPlane (Cao & Johnson, 2023) representation, it requires known camera poses and is validated on multi-camera setting. Unlike it, our work is the first to tackle the more practical setting of unposed monocular videos. HDR Video Reconstruction. HDR videos can be directly collected using dedicated hardware like scanline exposure/ISO (Choi et al., 2017; Heide et al., 2014) and beam splitters (Tocci et al., 2011; McGuire et al., 2007). But they are often impractical due to their sophisticated designs and high cost. Hence, reconstructing HDR videos from alternative-exposure LDR videos is investigated, which aligns neighboring frames to reference frame and then merging the aligned images to an HDR image (Kang et al., 2003; Kalantari et al., 2013). Later deep learning based works (Kalantari & Ramamoorthi, 2019; Chen et al., 2021; Chung & Cho, 2023; Xu et al., 2024) mainly use flow network for alignment and weight network for merging. However, the HDR data for supervised training is rare, limiting their generalization ability. Moreover, these methods can only recover training HDR videos. They can neither synthesize LDR images at new exposure levels nor support NVS. In contrast, our method reconstructs the whole 4D HDR scene in self-supervised manner, enabling novel-view rendering of both HDR videos and LDR videos with controllable exposures."
        },
        {
            "title": "3 METHODOLOGY",
            "content": "In our monocular 4D HDR reconstruction setup, the input is an unposed monocular video consisting of alternating-exposure LDR frames {Lt}Nf t=1, where Nf is the frame number. Consider 2-exposure case, {L1, L3, ...} are captured with short exposure time ts and {L2, L4, ...} with long exposure time tl. Other exposure patterns (e.g., 3-exposure) can be similarly extended. Our goal is to recover the renderable 4D HDR scene along with unknown camera parameters through Gaussian splatting. The overview of our Mono4DGS-HDR is illustrated in Fig. 2. In the following, we first briefly introduce the preliminaries of HDR GS and Gaussian dynamics in Sec. 3.1. Then, we detail each step and component of our system in Sec. 3.2 and Sec. 3.3. 3.1 PRELIMINARIES HDR Gaussian Splatting. HDR Gaussian Splatting (Cai et al., 2024; Liu et al., 2025a) represents an HDR scene using set of anisotropic 3D HDR Gaussians, which is the same as 3DGS (Kerbl et al., 2023) except replacing LDR color [0, 1] with HDR irradiance [0, +). Besides, there is logarithmic-domain tone mapper ϕ to map the HDR irradiance to LDR color at given exposure time t, denoted as = ϕ(log(et)). Details about 3DGS are provided in Sec. A.1.1. Gaussian Dynamics. Unlike those works (Wu et al., 2024b; Yang et al., 2024b; Liu et al., 2025b) that leverage deformation MLPs to model temporal changes and thus slow down rendering speed, we choose to explicitly parameterize the motion of dynamic Gaussians with trajectory functions following Park et al. (2025); Li et al. (2024a), which preserves the rendering speed of 3DGS and enables seamless transformation from video Gaussians to world Gaussians. Specifically, we use the cubic Hermite spline function to represent the position trajectory of each Gaussian (see in Sec. A.1.2), 4 containing Nc control points. For Gaussian rotation, we employ cubic polynomial function to represent quaternion as r(t) = (cid:80)3 j=0 ajtj, where {ajaj R4}3 j=0 are polynomial coefficients. The scaling, opacity, and color of each Gaussian are assumed to be time-invariant for simplicity."
        },
        {
            "title": "3.2 PRIOR PRECOMPUTE",
            "content": "Since monocular 4D reconstruction is highly ill-posed, we leverage the prior knowledge inferred by vision foundation models as Park et al. (2025); Lei et al. (2025) to provide scene initialization and regularization. As shown in Fig. 2(a), we use off-the-shelf foundation models to obtain: (1) Video depth estimations (Hu et al., 2025); (2) Sparse long-term 2D pixel trajectories (Xiao et al., 2024); (3) Per-frame epipolar error maps (Liu et al., 2023) computed from dense optical flow predictions (Teed & Deng, 2020), that can identify the dynamic foreground masks by thresholding. Note that in our input of alternating-exposure LDR frames, we should compute the optical flow between two frames at the same exposure level. For video depth estimation and long-term tracklet prediction, we just feed the whole frame sequence to the models. With these priors, we can also conduct bundle adjustment using static tracklets to obtain initial camera parameters as in MoSca (Lei et al., 2025). 3.3 TWO-STAGE GAUSSIAN OPTIMIZATION Although the 2D priors can be effectively extracted from our input alternating-exposure videos, they are still noisy and insufficient, which results in coarse scene initializations. Therefore, we propose unified framework with novel two-stage Gaussian optimization procedure, which consists of video Gaussian training in the first stage, world Gaussian fine-tuning in the second stage, and transformation strategy from video Gaussians to world Gaussians. 3.3.1 VIDEO HDR GAUSSIANS Inspired by SaV (Sun et al., 2024), we learn set of fully dynamic video HDR Gaussians in orthographic camera coordinate space at the first stage. For 3D point pv = [xv, yv, zv] in this space, (xv, yv) [1, 1]2 is actually the normalized coordinate of its projected pixel position, while zv is the depth. In this representation, we should use an orthographic camera model for rasterization and replace the original projection Jacobian in 3DGS with Jortho. We can lift the track/depth priors to initialize video Gaussians. Please refer to Sec. A.1.3 for more details. In word, video Gaussian representation enables us to treat camera motion and object motion uniformly as the motion of dynamic Gaussians and eliminate the need of camera parameters. Consequently, we can fit LDR training frames and recover HDR training video more efficiently. The resulting HDR video Gaussians provide reliable foundation for subsequent world Gaussian and camera pose refining. 3.3.2 VIDEO-TO-WORLD GAUSSIAN TRANSFORMATION The learned video Gaussians are in pseduo-3D space, which cannot represent the actual 3D geometry in the world. Thus, we design video-to-world Gaussian transformation strategy (see in Fig. 3(a)), to make the initialization from video Gaussians reasonable for further world Gaussian optimization. Dynamic & Static Identification with Occlusion Handling. First, we need to identify whether dynamic video Gaussian is static or dynamic in world space. To this end, we leverage dynamic masks = {Mt}Nf t=1 derived from epipolar error maps, where 1 and 0 in Mt indicate dynamic and static regions, respectively. Concretely, we project each video Gaussian trajectory Gv = {µv = [xv µv t=1 to image plane and count the occurrences Nd when it falls into dynamic regions: ]}Nf , zv , yv Nd = (cid:88)Nf t=1 I[Mt(xv , yv ) (1 ot) = 1], ot = I[zv > (cid:101)Dt(xv , yv )], (1) where I[] is the indicator function, and ot = 1 means the Gaussian is occluded at time (i.e., its depth is larger than the rendered depth (cid:101)Dt). If Nd/Nf is larger than pre-defined threshold (e.g., 0.1), we consider this video Gaussian as dynamic in the world space and static otherwise. Gaussian Position & Rotation Transform. Then, we transform the video Gaussian, with per-frame t=1, into world space using the initial camera intrinsics ˆK and positions {µv extrinsics {[ ˆRt ˆTt]}Nf t=1 obtained from bundle adjustment. Let π ˆK() be the projection function from t=1 and rotations {Rv }Nf }Nf 5 (µv = ˆRtRv ) + ˆTt and Rw camera space to image space with intrinsics ˆK and π1 () be the inverse. We can transform as ˆK = ˆRtπ1 µw and Rw are the Gaussian position and rotation ˆK at time in world space, respectively. For static Gaussian, we simply set the time-independent world position as µw = 1 and rotation as Rw = AvgR({Rw t=1), where AvgR() is the Nf rotation quaternion averaging method proposed by Markley et al. (2007). For dynamic Gaussian, we re-sample control points from the world position trajectory {µw t=1 for spline function initialization and re-fit the polynomial coefficients for rotation quaternion trajectory using least squares. , where µw t=1 µw }Nf }Nf (cid:80)Nf Gaussian Opacity & Color Inheriting. The attributes of opacity and HDR color are directly inherited from the video Gaussians, no matter static or dynamic, since they are intuitively space-invariant. Gaussian Scaling Re-fitting by 2D Covariance Invariance. Note that the scale difference between camera coordinate space and world space should be carefully handled. To obtain initial world Gaussians with rational scales, we propose to re-fit Gaussian scaling based on the invariance of 2D covariance, which is motivated by the fact that the projected 2D Gaussians should have consistent shapes and sizes before and after transformation. The projected 2D covariances of video and world Gaussians at time t, denoted as Σv , can be derived as: and Σw Σv = [JorthoW Rv Sv(JorthoW Rv Sv)]22, Σw = [JW t Rw Sw(JW Rw Sw)]22, (2) where Sv and Sw are the time-invariant scaling matrices of video and world Gaussians, and []22 means skipping the third row and column. For the viewing transformations, we have = (identity matrix) and . Now, we can obtain the initial world Gaussian scaling by solving the optimization problem minSw (cid:80)Nf Σw 2. We apply it to both static and dynamic Gaussians, and solve it using gradient descent, that can converge within 1000 iterations in 1 minute. = ˆR t=1 Σv After above video-to-world Gaussian transformation, we can step into the second stage to refine the world Gaussians together with camera parameters. Due to the appropriate initialization from video Gaussians, the optimization in world space can converge fast and stably. 3.3.3 OPTIMIZATION STRATEGY We render from the fully dynamic video Gaussians in the first stage, and from the union of static and dynamic world Gaussians in the second stage. At each training iteration, we randomly sample query timestamp {1, 2, ..., Nf }. The rendered variables at include the HDR image (cid:101)Ht, the depth map (cid:101)Dt, and the flow/track map (cid:101)Ftt, where is the destination timestamp. To obtain (cid:101)Ftt, we can rasterize the 3D movements of all Gaussians in camera space from and and project to the image plane. The HDR rendering can be then converted to LDR image (cid:101)Lt with tone-mapping MLPs and exposure time. The following descriptions apply to both stages unless otherwise specified. Supervision from 2D Priors. We supervise the scene geometry, appearance and dynamics with input LDR frames and precomputed 2D priors, including LDR RGB loss Lrgb, depth loss Ldep, flow/track loss Ltrack and unit exposure loss Lue. Please refer to Sec. A.1.4 for details. Gaussian Motion Regularization. We also regularize the dynamic Gaussians to ensure smooth and plausible motions as Lei et al. (2025); Sun et al. (2024); Wang et al. (2025a), including as-rigid-as-possible loss Larap, velocity regularization Lvel, and acceleration regularization Lacc (see in Sec. A.1.5 for details). Figure 4: Temporal luminance regularization for temporally consistent HDR appearance. 6 Table 1: Quantitative comparisons on the test frames of Syn-Exp-3 scenes. Metrics are averaged over all scenes. LDR-OE and LDR-NE denote the LDR results with observed and novel exposures, respectively. HDR denotes the HDR results. FPS is measured at 864 480 resolution. We use our initial camera parameters from bundle adjustment as the required camera inputs for GaussHDR (Liu et al., 2025a) and HDR-HexPlane (Wu et al., 2024a). We extend SplineGS (Park et al., 2025) and MoSca (Lei et al., 2025) to HDR mode for fair comparison. LDR-OE LDR-NE HDR Method PSNR SSIM LPIPS PSNR SSIM LPIPS PSNR SSIM LPIPS TAE Training time FPS GaussHDR HDR-HexPlane SplineGS-HDR MoSca-HDR Mono4DGS-HDR (Ours) 29.51 29.26 17.59 34.08 34.75 0.858 0.806 0.661 0.898 0. 0.167 0.186 0.495 0.098 0.086 28.96 28.72 16.60 33.92 34.54 0.863 0.812 0.646 0.910 0.915 0.165 0.189 0.517 0.092 0.081 31.25 29.60 17.82 36.89 37.64 0.891 0.839 0.677 0.952 0. 0.105 0.120 0.447 0.053 0.042 0.089 0.155 1.188 0.059 0.057 1h 1h 1.5h 1.5h 1.5h 51 1 79 82 161 Temporal Luminance Regularization (TLR). The appearance supervision is only from LDR frames, which may cause unstable HDR luminance across time, especially for dynamic scenes, since dynamic Gaussians tend to float around the surfaces of moving objects. The supervision is effective at the times when the corresponding dynamic contents are properly exposed, but it is weak when over/under-exposed. The former case leads to correctly positioned dynamic Gaussians while the latter can result in floaters above dynamic object surfaces. Consequently, the HDR appearance of dynamic contents may vary significantly at different times, as shown in Fig. 4. To address this issue, we propose temporal luminance regularization using flow-guided photometric loss to align per-pixel HDR luminance between consecutive frames. Given two adjacent timestamps 1 and t, we can warp the HDR rendering (cid:101)Ht1 to time using optical flow, generating (cid:101)Ht1t. Then, we can compute the loss as: Ltlr = (cid:12) (cid:12) (cid:12)Vtt1 (cid:101)Ht1t (cid:101)Ht (cid:101)Ht1t + (cid:101)Ht (cid:12) (cid:12) (cid:12)1 , (3) where Vtt1 is valid mask derived from depth order to exclude occluded pixels. By normalizing with (cid:101)Ht1t + (cid:101)Ht, we can eliminate the influence of HDR irradiance scale. At query time t, we choose both 1 and + 1 as the destinations to obtain Ltlr. Since we cannot access optical flow priors between adjacent frames at different exposures, we use the rendered flow maps (cid:101)Ftt1 and (cid:101)Ftt+1(stop gradients) for warping. To this end, we only apply TLR after the ending of Gaussian densification to ensure reliable flow rendering. With TLR, the learned dynamic contents at wellsupervised times can propagate to the poorly-supervised times, leading to temporally consistent HDR appearance, as shown in Fig. 4. HDR Photometric Reprojection Loss. Previous works like SplineGS (Park et al., 2025) employ photometric reprojection loss Lpr (Godard et al., 2019) in monocular videos to optimize depth and camera poses together, which is not suitable for our alternating-exposure inputs. However, we can leverage the recovered HDR training video from the first stage for this purpose. With Lpr (see in Sec. A.1.6), we jointly refine the camera poses and world Gaussians in the second stage. Overall Loss. The overall objective is = λrgbLrgb + λueLue + λdepLdep + λtrackLtrack + λarapLarap + λvelLvel + λaccLacc + λtlrLtlr + λprLpr, where λs are the corresponding weights. Gaussian Densification. We follow the same densification strategy as 3DGS for all Gaussians at both stages. In the second stage, we prune dynamic world Gaussians every certain number of iterations to remove those densified mistakenly. We can project dynamic Gaussian trajectories to camera coordinate space and apply the dynamic/static identification method in Sec. 3.3.2 for filtering. 4 EXPERIMENTS 4.1 EXPERIMENTAL SETTINGS Datasets and Evaluation Metrics. Since our task has never been explored before, we create new evaluation benchmark base on publicly available datasets (Kronander et al., 2014; Froehlich et al., 2014; Kalantari et al., 2013; Chen et al., 2021) for HDR video reconstruction, resulting in 25 dynamic scenes in total. Each scene contains an alternating-exposure video clip with 50-100 frames. We classify them into three categories: (1) Syn-Exp-3: 9 synthetic scenes with 3 exposure levels and 7 Table 2: Quantitative comparisons on the train frames of Real-Exp-2 scenes and the test frames of Real-Exp-3 scenes. Metrics are averaged over all scenes. OE denotes the observed-exposure results. We use our initial camera parameters from bundle adjustment as camera inputs for GaussHDR (Liu et al., 2025a) and HDR-HexPlane (Wu et al., 2024a). We extend GFlow (Wang et al., 2025b), SplineGS (Park et al., 2025) and MoSca (Lei et al., 2025) to HDR mode for fair comparison. Real-Exp-2 (train frames) Real-Exp-3 (test frames) Method LDR-OE PSNR SSIM LPIPS HDR-TAE GaussHDR HDR-HexPlane GFlow-HDR SplineGS-HDR MoSca-HDR Mono4DGS-HDR (Ours) 24.77 29.30 27.71 21.46 30.28 31. 0.895 0.900 0.908 0.721 0.915 0.928 0.106 0.106 0.094 0.334 0.074 0.052 0.073 0.154 0.426 1.282 0.054 0.046 LDR-OE PSNR SSIM LPIPS 24.98 18.49 - 13.64 27.23 27.65 0.858 0.627 - 0.506 0.872 0.876 0.117 0.241 - 0.505 0.084 0.081 HDR-TAE 0.096 0.705 - 0.707 0.076 0.067 Figure 5: HDR visual comparisons on train/test frames. Our method achieves superior quality. HDR GTs; (2) Real-Exp-3: 8 real-world scenes with 3 exposure levels; and (3) Real-Exp-2: 8 realworld scenes with 2 exposure levels. Details about the datasets are given in Sec. A.2.1. For quantitative evaluation, we utilize PSNR (higher is better), SSIM (higher is better), and LPIPS (Zhang et al., 2018) (lower is better) metrics. Following HDR-NeRF (Huang et al., 2022), we quantitatively evaluate HDR results in the µ-law (Kalantari & Ramamoorthi, 2017) domain and qualitatively show HDR results via Photomatix pro. We additionally introduce HDR-TAE (lower is better) metric to measure the temporal stability of rendered HDR videos, which is detailed in Sec. A.2.2. Implementation Details. For prior precomputing, we utilize DepthCrafter (Hu et al., 2025) for video depth estimation, SpatialTracker (Xiao et al., 2024) for track prediction and RAFT (Teed & Deng, 2020) for optical flow estimation. Models are trained for 4K iterations in the first stage and 11K iterations in the second stage. For the camera poses of test frames, we interpolate from the neighboring train frames. Please refer to Sec. A.2.3 for more details. 4.2 PERFORMANCE COMPARISON Baselines. We compare our method with several baselines, which can be categorized into two groups: (1) HDR NVS methods, including GaussHDR (Liu et al., 2025a) for static scenes and HDRHexPlane (Wu et al., 2024a) for dynamic scenes but in multi-camera setting; (2) standard unposed 4D monocular reconstruction methods, including MoSca (Lei et al., 2025), SplineGS (Park et al., 2025) and GFlow (Wang et al., 2025b). For fair comparison, we extend the second group to HDR mode by employing HDR color and applying the same tone-mapper MLPs as ours. Note that GFlow cannot support for time interpolation of Gaussians, so we can only evaluate it on the training frames. For the first group, since they require known camera poses, we use our initial camera parameters as their camera inputs. We train all baseline methods for the same 15K iterations as ours. Quantitative & Qualitative Comparison. The quantitative results of synthetic and real scenes are listed in Table 1 and Table 2, respectively. More comparisons with GFlow on training frames are provided in Appendix  (Table 4)  . We can see that our method outperforms all baseline methods by large margin across all tracks. GaussHDR and HDR-HexPlane perform poorly owing to their incapability of handling dynamic scenes and monocular videos, respectively. GFlow fails to recover HDR scenes since it optimizes per-frame Gaussians independently to overfit the LDR observations. Table 3: Quantitative ablation results on the test frames of Real-Exp-3 and Syn-Exp-3 scenes. V2W denotes the video-to-world Gaussian transformation. All experiments are trained for 15K iteration. All the metrics listed here represent PSNR except HDR-TAE. Method Real-Exp-3 Syn-Exp-3 LDR-OE HDR-TAE LDR-OE LDR-NE HDR HDR-TAE (a) w/o Video Gaussian Initialization (b) w/o Occlusion Handling in V2W (c) w/o 2D Covariance Invariance in V2W (d) w/o HDR Photometric Reprojection Loss (e) w/o Temporal Luminance Regularization (f) Mono4DGS-HDR (Full model) 26.47 27.29 27.34 27.26 27.63 27.65 0.068 0.069 0.068 0.070 0.082 0.067 33.62 34.40 34.31 34.38 34.71 34.75 33.38 34.17 34.21 34.17 34.49 34.54 36.07 37.22 37.25 37.33 37.58 37.64 0.057 0.059 0.057 0.059 0.071 0. SplineGS behaves the worst since it heavily relies on the photometric reprojection loss to recover camera motion, which is infeasible in the presence of alternating-exposure LDR frames. MoSca achieves the second best performance due to its global Gaussian fusion ability, but still inferior to our method. Overall, our Mono4DGS-HDR demonstrates state-of-the-art performance in terms of temporal stability, rendering quality and speed. For qualitative results, we provide HDR visual comparisons of train/test frames in Fig. 5. More examples are exhibited in Appendix  (Fig. 7)  . We also present visual results under fix-view-change-time and fix-time-change-view settings in Appendix (Fig. 8 and Fig. 9). It can be observed that our method generates higher-quality HDR renderings with finer details and fewer artifacts than baselines. 4.3 ABLATION STUDY In this part, we conduct ablation studies to validate the effectiveness of our key designs. Effect of Video Gaussian Initialization. To evaluate the effectiveness of the initialization from video Gaussians, we remove the video Gaussian stage and directly train Gaussians in the world space, where we initialize the world Gaussians with prior track and depth information. As listed in Table 3(a), the PSNR performance drops significantly with more than 1dB. Fig. 6(a) also shows noticeable visual degradation. These demonstrate the importance of our video Gaussian initialization. Compared to track/depth lifting, the video Gaussians provide not only accurate position priors but also meaningful rotation, scaling, opacity and color priors, which greatly facilitate the subsequent world Gaussian optimization. Effect of Occlusion Handling & 2D Covariance Invariance in V2W. We further explore the role of occlusion handling and 2D covariance invariance in our video-to-world Gaussian transformation by ablating them separately. As shown in Fig. 3(b)(c)(d), the occlusion handling provides accurate dynamic/static separation, while 2D covariance invariance ensures that initial world Gaussians have reasonable sizes. Without the former, some static contents are mistakenly treated as dynamic, leading to PSNR performance drop of 0.3dB (Table 3(b)) and blurry background reconstruction especially in the regions that have been occluded (Fig. 6(b)). Without the latter, world Gaussians are not initialized well in size, resulting in sub-optimal results, as indicated in Table 3(c) and Fig. 6(c). Figure 6: Qualitative ablation results. Effect of HDR Photometric Reprojection Loss. To verify the efficacy of the HDR photometric reprojection loss, we experiment by removing it in the second stage. decrease in performance can be observed in Table 3(d) and Fig. 6(d), indicating that the dense supervision from photometric reprojection loss is beneficial for refining camera poses and scene geometry. 9 Effect of Temporal Luminance Regularization. We also investigate the influence of the temporal luminance regularization loss Ltlr by discarding it. We can see in Table 3(e) that although the reconstruction quality (PSNR) is not significantly affected, the temporal stability (TAE) is greatly degraded. This means that Ltlr plays crucial role in stabilizing the appearance variations across frames, leading to more temporally coherent HDR videos. Fig. 6(e) also shows that without Ltlr, the rendered result exhibits noticeable artifacts and noises in dynamic regions."
        },
        {
            "title": "5 CONCLUSION",
            "content": "We present Mono4DGS-HDR, the first system to tackle 4D HDR reconstruction from unposed monocular LDR videos with alternating exposures. Our novel two-stage optimization approach first learns video HDR Gaussian representation in orthographic camera coordinate space, then transforms into world space and refines the world Gaussians along with camera poses. We also propose temporal luminance regularization to ensure the temporal consistency of HDR appearance. Experiments on our new benchmark show superior results over adapted methods in both rendering quality and speed. ETHICS STATEMENT This research methodology does not raise any ethical issues. No human subjects were involved. REPRODUCIBILITY STATEMENT Our Mono4DGS-HDR is built by integrating the publicly available codebase of 3DGS (Kerbl et al., 2023), MoSca (Lei et al., 2025), SaV (Sun et al., 2024), GaussHDR (Liu et al., 2025a), SpatialTracker (Xiao et al., 2024), DepthCrafter (Hu et al., 2025), and RAFT (Teed & Deng, 2020). We create our evaluation benchmark based on the publicly available datasets (Kronander et al., 2014; Froehlich et al., 2014; Kalantari et al., 2013; Chen et al., 2021) for HDR video reconstruction. In this paper, we include comprehensive data preprocessing and implementation details, which greatly facilitate reproducing our work. We will release our code and processed data if this work is accepted. LLM USAGE We use the LLMs including GPT-4 (OpenAI et al., 2024) to help polish the writing of the paper."
        },
        {
            "title": "REFERENCES",
            "content": "Jeongmin Bae, Seoha Kim, Youngsik Yun, Hahyun Lee, Gun Bang, and Youngjung Uh. Pergaussian embedding-based deformation for deformable 3d gaussian splatting. In ECCV, 2024. Yuanhao Cai, Zihao Xiao, Yixun Liang, Yulun Zhang, Xiaokang Yang, Yaoyao Liu, and Alan Yuille. Hdr-gs: Efficient high dynamic range novel view synthesis at 1000x speed via gaussian splatting. In NeurIPS, 2024. Ang Cao and Justin Johnson. Hexplane: fast representation for dynamic scenes. In CVPR, 2023. Guanying Chen, Chaofeng Chen, Shi Guo, Zhetong Liang, Kwan-Yee K. Wong, and Lei Zhang. Hdr video reconstruction: coarse-to-fine network and real-world benchmark dataset. In ICCV, 2021. Inchang Choi, Seung-Hwan Baek, and Min Kim. Reconstructing interlaced high-dynamic-range video using joint learning. IEEE TIP, 26(11):53535366, 2017. Haesoo Chung and Nam Ik Cho. Lan-hdr: Luminance-based alignment network for high dynamic range video reconstruction. In ICCV, 2023. Yuanxing Duan, Fangyin Wei, Qiyu Dai, Yuhang He, Wenzheng Chen, and Baoquan Chen. 4d-rotor gaussian splatting: Towards efficient novel view synthesis for dynamic scenes. In SIGGRAPH, 2024. 10 Jiemin Fang, Taoran Yi, Xinggang Wang, Lingxi Xie, Xiaopeng Zhang, Wenyu Liu, Matthias Nießner, and Qi Tian. Fast dynamic radiance fields with time-aware neural voxels. In SIGGRAPH Asia, 2022. Sara Fridovich-Keil, Alex Yu, Matthew Tancik, Qinhong Chen, Benjamin Recht, and Angjoo Kanazawa. Plenoxels: Radiance fields without neural networks. In CVPR, 2022. Jan Froehlich, Stefan Grandinetti, Bernd Eberhardt, Simon Walter, Andreas Schilling, and Harald Brendel. Creating cinematic wide gamut hdr-video for the evaluation of tone mapping operators and hdr-displays. In Digital Photography X, 2014. Chen Gao, Ayush Saraf, Johannes Kopf, and Jia-Bin Huang. Dynamic view synthesis from dynamic monocular video. In ICCV, 2021. Clément Godard, Oisin Mac Aodha, Michael Firman, and Gabriel J. Brostow. Digging into selfsupervised monocular depth estimation. In CVPR, 2019. Shucheng Gong, Lingzhe Zhao, Wenpu Li, Hong Xie, Yin Zhang, Shiyu Zhao, and Peidong Liu. Casual3dhdr: High dynamic range 3d gaussian splatting from casually-captured videos. In ACM MM, 2025. Xiang Guo, Jiadai Sun, Yuchao Dai, Guanying Chen, Xiaoqing Ye, Xiao Tan, Errui Ding, Yumeng Zhang, and Jingdong Wang. Forward flow for novel view synthesis of dynamic scenes. In ICCV, 2023. Felix Heide, Markus Steinberger, Yun-Ta Tsai, Mushfiqur Rouf, Dawid Paj ak, Dikpal Reddy, Orazio Gallo, Jing Liu, Wolfgang Heidrich, Karen Egiazarian, Jan Kautz, and Kari Pulli. Flexisp: flexible camera image processing framework. ACM TOG, 33(6):113, 2014. Wenbo Hu, Xiangjun Gao, Xiaoyu Li, Sijie Zhao, Xiaodong Cun, Yong Zhang, Long Quan, and Ying Shan. Depthcrafter: Generating consistent long depth sequences for open-world videos. In CVPR, 2025. Xin Huang, Qi Zhang, Ying Feng, Hongdong Li, Xuan Wang, and Qing Wang. Hdr-nerf: High dynamic range neural radiance fields. In CVPR, 2022. Xin Jin, Pengyi Jiao, Zheng-Peng Duan, Xingchao Yang, Chun-Le Guo, Bo Ren, and Chongyi Li. Lighting every darkness with 3dgs: Fast training and real-time rendering for hdr view synthesis. In NeurIPS, 2024. Kim Jun-Seong, Kim Yu-Ji, Moon Ye-Bin, and Tae-Hyun Oh. Hdr-plenoxels: Self-calibrating high dynamic range radiance fields. In ECCV, 2022. Nima Khademi Kalantari and Ravi Ramamoorthi. Deep high dynamic range imaging of dynamic scenes. ACM TOG, 36(4):112, 2017. Nima Khademi Kalantari and Ravi Ramamoorthi. Deep hdr video from sequences with alternating exposures. In Computer Graphics Forum, 2019. Nima Khademi Kalantari, Eli Shechtman, Connelly Barnes, Soheil Darabi, Dan Goldman, and Pradeep Sen. Patch-based high dynamic range video. ACM TOG, 32(6):18, 2013. Sing Bing Kang, Matthew Uyttendaele, Simon Winder, and Richard Szeliski. High dynamic range video. ACM TOG, 22(3):319325, 2003. Bernhard Kerbl, Georgios Kopanas, Thomas Leimkühler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM TOG, 42(4):114, 2023. Diederik Kingma and Jimmy Ba. Adam: method for stochastic optimization. In ICLR, 2015. Joel Kronander, Stefan Gustavson, Gerhard Bonnet, Anders Ynnerman, and Jonas Unger. unified framework for multi-sensor hdr video reconstruction. Signal Processing: Image Communication, 2014. 11 Junoh Lee, ChangYeon Won, Hyunjun Jung, Inhwan Bae, and Hae-Gon Jeon. Fully explicit dynamic guassian splatting. In NeurIPS, 2024. Jiahui Lei, Yijia Weng, Adam W. Harley, Leonidas Guibas, and Kostas Daniilidis. Mosca: Dynamic gaussian fusion from casual videos via 4d motion scaffolds. In CVPR, 2025. Tianye Li, Mira Slavcheva, Michael Zollhoefer, Simon Green, Christoph Lassner, Changil Kim, Tanner Schmidt, Steven Lovegrove, Michael Goesele, Richard Newcombe, and Zhaoyang Lv. Dynamic view synthesis from dynamic monocular video. In CVPR, 2022. Zhan Li, Zhang Chen, Zhong Li, and Yi Xu. Spacetime gaussian feature splatting for real-time dynamic view synthesis. In CVPR, 2024a. Zhihao Li, Yufei Wang, Alex Kot, and Bihan Wen. From chaos to clarity: 3dgs in the dark. In NeurIPS, 2024b. Youtian Lin, Zuozhuo Dai, Siyu Zhu, and Yao Yao. Gaussian-flow: 4d reconstruction with dynamic 3d gaussian particle. In CVPR, 2024. Jinfeng Liu, LingTong Kong, Bo Li, and Dan Xu. Gausshdr: High dynamic range gaussian splatting via learning unified 3d and 2d local tone mapping. In CVPR, 2025a. Qingming Liu, Yuan Liu, Jiepeng Wang, Xianqiang Lyu, Peng Wang, Wenping Wang, and Junhui Hou. Modgs: Dynamic gaussian splatting from casually-captured monocular videos with depth priors. In ICLR, 2025b. Yu-Lun Liu, Chen Gao, Andreas Meuleman, Hung-Yu Tseng, Ayush Saraf, Changil Kim, Yung-Yu Chuang, Johannes Kopf, and Jia-Bin Huang. Robust dynamic radiance fields. In CVPR, 2023. Landis Markley, Yang Cheng, John Crassidis, and Yaakov Oshman. Averaging quaternions. Journal of Guidance, Control, and Dynamics, 30:11931196, 2007. Morgan McGuire, Wojciech Matusik, Hanspeter Pfister, Billy Chen, John Hughes, and Shree Nayar. Optical splitting trees for high-precision monocular imaging. IEEE Computer Graphics and Applications, 27(2):3242, 2007. Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. In ECCV, 2020. Ben Mildenhall, Peter Hedman, Ricardo Martin-Brualla, Pratul P. Srinivasan, and Jonathan T. Barron. Nerf in the dark: High dynamic range view synthesis from noisy raw images. In CVPR, 2022. OpenAI, Josh Achiam, Steven Adler, and et al. Gpt-4 technical report. arXiv:2408.06543, 2024. Jongmin Park, Minh-Quan Viet Bui, Juan Luis Gonzalez Bello, Jaeho Moon, Jihyong Oh, and Munchurl Kim. Splinegs: Robust motion-adaptive spline for real-time dynamic 3d gaussians from monocular video. In CVPR, 2025. Keunhong Park, Utkarsh Sinha, Jonathan T. Barron, Sofien Bouaziz, Dan Goldman, Steven M. Seitz, and Ricardo Martin-Brualla. Nerfies: Deformable neural radiance fields. In ICCV, 2021a. Keunhong Park, Utkarsh Sinha, Peter Hedman, Jonathan T. Barron, Sofien Bouaziz, Dan Goldman, Ricardo Martin-Brualla, and Steven M. Seitz. Hypernerf: higher-dimensional representation for topologically varying neural radiance fields. ACM TOG, 40(6):112, 2021b. Sungheon Park, Minjung Son, Seokhwan Jang, Young Chun Ahn, Ji-Yeon Kim, and Nahyup Kang. Temporal interpolation is all you need for dynamic neural radiance fields. In CVPR, 2023. Adam Paszke et al. Pytorch: An imperative style, high-performance deep learning library. In NeurIPS, 2019. Albert Pumarola, Enric Corona, Gerard Pons-Moll, and Francesc Moreno-Noguer. D-nerf: Neural radiance fields for dynamic scenes. In CVPR, 2021. Shreyas Singh, Aryan Garg, and Kaushik Mitra. Hdrsplat: Gaussian splatting for high dynamic range 3d scene reconstruction from raw images. In BMVC, 2024. Colton Stearns, Adam Harley, Mikaela Uy, Florian Dubost, Federico Tombari, Gordon Wetzstein, and Leonidas Guibas. Dynamic gaussian marbles for novel view synthesis of casual monocular videos. In SIGGRAPH Asia, 2024. Yang-Tian Sun, Yi-Hua Huang, Lin Ma, Xiaoyang Lyu, Yan-Pei Cao, and Xiaojuan Qi. Splatter video: Video gaussian representation for versatile processing. In NeurIPS, 2024. Zachary Teed and Jia Deng. Raft: Recurrent all pairs field transforms for optical flow. In ECCV, 2020. Michael Tocci, Chris Kiser, Nora Tocci, and Pradeep Sen. versatile hdr video production system. ACM TOG, 30(4):110, 2011. Chao Wang, Krzysztof Wolski, Bernhard Kerbl, Ana Serrano, Mojtaba Bemana, Hans-Peter Seidel, Karol Myszkowski, and Thomas Leimkühler. Cinematic gaussians: Real-time hdr radiance fields with depth of field. arXiv:2406.07329, 2024. Qianqian Wang, Vickie Ye, Hang Gao, Weijia Zeng, Jake Austin, Zhengqi Li, and Angjoo Kanazawa. Shape of motion: 4d reconstruction from single video. In ICCV, 2025a. Shizun Wang, Xingyi Yang, Qiuhong Shen, Zhenxiang Jiang, and Xinchao Wang. Gflow: Recovering 4d world from monocular video. In AAAI, 2025b. Zhou Wang, Alan C. Bovik, Hamid R. Sheikh, and Eero P. Simoncelli. Image quality assessment: From error visibility to structural similarity. IEEE TIP, 13(4):600612, 2004. Guanjun Wu, Taoran Yi, Jiemin Fang, Wenyu Liu, and Xinggang Wang. Fast high dynamic range radiance fields for dynamic scenes. In 3DV, 2024a. Guanjun Wu, Taoran Yi, Jiemin Fang, Lingxi Xie, Xiaopeng Zhang, Wei Wei, Wenyu Liu, Qi Tian, In CVPR, and Xinggang Wang. 4d gaussian splatting for real-time dynamic scene rendering. 2024b. Jiahao Wu, Lu Xiao, Rui Peng, Kaiqiang Xiong, and Ronggang Wang. Hdrgs: High dynamic range gaussian splatting. arXiv:2303.08774, 2024c. Renlong Wu, Zhilu Zhang, Mingyang Chen, Xiaopeng Fan, Zifei Yan, and Wangmeng Zuo. Deblur4dgs: 4d gaussian splatting from blurry monocular video. arXiv preprint arXiv:2412.06424, 2024d. Wenqi Xian, Jia-Bin Huang, Johannes Kopf, and Changil Kim. Space-time neural irradiance fields for free-viewpoint video. In CVPR, 2021. Yuxi Xiao, Qianqian Wang, Shangzhan Zhang, Nan Xue, Sida Peng, Yujun Shen, and Xiaowei Zhou. Spatialtracker: Tracking any 2d pixels in 3d space. In CVPR, 2024. Gangwei Xu, Yujin Wang, Jinwei Gu, Tianfan Xue, and Xin Yang. Hdrflow: Real-time hdr video reconstruction with large motions. In CVPR, 2024. Honghui Yang, Di Huang, Wei Yin, Chunhua Shen, Haifeng Liu, Xiaofei He, Binbin Lin, Wanli Ouyang, and Tong He. Depth any video with scalable synthetic data. In ICLR, 2025. Zeyu Yang, Hongye Yang, Zijie Pan, and Li Zhang. Real-time photorealistic dynamic scene representation and rendering with 4d gaussian splatting. In ICLR, 2024a. Ziyi Yang, Xinyu Gao, Wen Zhou, Shaohui Jiao, Yuqing Zhang, and Xiaogang Jin. Deformable 3d gaussians for high-fidelity monocular dynamic scene reconstruction. In CVPR, 2024b. Jihwan Yoon, Sangbeom Han, Jaeseok Oh, and Minsik Lee. SplineGS: Learning smooth trajectories in gaussian splatting for dynamic scene reconstruction. In ICLR, 2025. Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In CVPR, 2018. 13 Figure 7: More HDR visual comparisons on train/test frames. Our method achieves superior quality."
        },
        {
            "title": "A APPENDIX",
            "content": "A.1 ADDITIONAL METHOD DETAILS A.1.1 3D GAUSSIAN SPLATTING 3DGS (Kerbl et al., 2023) represents scene using set of anisotropic 3D Gaussians, each parameterized by its position µ R3, rotation quaternion R4, scaling R3, opacity α [0, 1], and 2 (xµ)Σ1(xµ), color [0, 1]3. The spatial distribution of each Gaussian follows G(x) = 1 where is an arbitrary 3D position and Σ = RSSR is the covariance matrix derived from the rotation matrix (or r) and the scaling matrix (or s). Each 3D Gaussian G(x) is first transformed into 2D Gaussian G(x) on the image plane by evaluating the 2D covariance Σ = [JW ΣW ]22, where is the Jacobian of the affine approximation of the projective transformation, is the viewing transformation matrix and []22 means skipping the third row and column. Then, tile-based rasterizer sorts the 2D Gaussians and performs α-blending: C(u) = (cid:88) ciσi i1 (cid:89) (1 σj), σi = αiG i(u), i= j=1 (4) where denotes the queried pixel position and is the number of sorted 2D Gaussians related to the queried pixel. A.1.2 CUBIC HERMITE SPLINE INTERPOLATION We use cubic Hermite spline function to model the motion of dynamic Gaussian. Given Nc control points {pk}Nc k=1 for cubic Hermite spline, the position k=1 and their corresponding timestamps {tk}Nc 14 Table 4: Quantitative comparisons with GFlow (Wang et al., 2025b) on the train frames of RealExp-3 and Syn-Exp-3 scenes. Metrics are averaged over all scenes. OE denotes observed-exposure results. We extend GFlow to HDR mode for fair comparison. Real-Exp-3 (train frames) Syn-Exp-3 (train frames) Method LDR-OE PSNR SSIM LPIPS HDR-TAE LDR-OE HDR PSNR SSIM LPIPS PSNR SSIM LPIPS HDR-TAE GFlow-HDR Mono4DGS-HDR (Ours) 24.88 30. 0.894 0.913 0.139 0.067 0.770 0.067 30.08 35.18 0.912 0.908 0.115 0. 18.70 37.97 0.674 0.960 0.206 0.040 0.815 0.057 Table 5: Ablation results about sampling interval of the cubic Hermite splines control points on the test frames of Real-Exp-3 and Syn-Exp-3 scenes. All experiments are trained for 15K iteration. All the metrics listed here represent PSNR except HDR-TAE. Sample every Ns frames Real-Exp-3 Syn-Exp-3 LDR-OE HDR-TAE LDR-OE LDR-NE HDR HDR-TAE Ns=1 Ns=2 Ns=4 (Ours) Ns=8 Ns=16 27.73 27.58 27.65 27.26 26.75 0.067 0.067 0.067 0.068 0. 34.86 34.82 34.75 34.16 33.67 34.65 34.60 34.54 33.95 33.47 37.70 37.66 37.64 37.03 36.34 0.057 0.058 0.057 0.058 0.058 of Gaussian at time [tk, tk+1] can be computed as: µ(t) = h00(t)pk + h10(t)(tk+1 tk)mk + h01(t)pk+1 + h11(t)(tk+1 tk)mk+1, = 1 2 where h00(t) = 2t3 3t2 + 1, h10(t) = t3 2t2 + t, h01(t) = 2t3 + 3t2, and h11(t) = t3 t2 are the Hermite basis functions, and mk is the approximated tangent at control point pk. pk pk1 tk tk1 pk+1 pk tk+1 tk tk tk+1 tk , mk = + ), ( (5) A.1.3 VIDEO GAUSSIAN REPRESENTATION Video Gaussian representation optimizes in orthographic camera coordinate space where the videos width, height and depth correspond to the X, and axes, respectively. In this space, we should use an orthographic camera model for rasterization and replace the original projection Jacobian (cid:20)w/2 0 (cid:21) 0 h/2 0 0 in 3DGS with Jortho = , where and are the image resolution. For video Gaussian initialization, we can utilize the precomputed tracking and depth priors. Each tracklet will correspond to video Gaussian. Consider 2D track = {τtτt R2}Nf t=1, with prior video depths = {Dt}Nf t=1, we can lift it to camera coordinate space as video Gaussian trajectory , yv = [xv µv Gv = {µv ) are the normalized pixel coordinates of τt, and zv = Dt(τt) is the depth value at pixel τt. Then, we sample Nc initial control points from Gv for the spline trajectory of this dynamic video Gaussian. For Gaussian rotation quaternion, we simply initialize all polynomial coefficients with [1, 0, 0, 0]. t=1, where (xv ]}Nf , yv , zv A.1.4 SUPERVISION FROM 2D PRIORS LDR RGB Loss. Following 3DGS (Kerbl et al., 2023), we use combination of DSSIM (Wang et al., 2004) and L1 losses to compute the image reconstruction loss between LDR rendering (cid:101)Lt and GT image Lt, denoted as Lrgb = λdDSSIM((cid:101)Lt, Lt) + (1 λd)(cid:101)Lt Lt1, where λd = 0.2. Depth Loss. We include depth loss on the rendered depth map (cid:101)Dt and prior depth map Dt, which can be formulated as Ldep = (cid:101)Dt Dt1. Flow/Track Loss. To distill motion information from 2D priors to 3D Gaussians, we compute the flow/track loss (Lei et al., 2025; Wang et al., 2025a) between the rendered flow/track map (cid:101)Ftt and the prior flow/track map Ftt, derived as Ltrack = Vtt (cid:101)Ftt Ftt1, where Vtt is the valid flow mask or sparse track mask. We randomly choose to supervise with flow or track loss at each iteration. For flow supervision, we set {t Ne} where Ne is the number of exposure 15 Figure 8: HDR visual comparisons under fix-view-change-time setting. levels (note that we only extract prior optical flow between two frames at the same exposure level). For track supervision, we randomly sample from the entire video sequence (except t). Unit Exposure Loss. We follow HDR-NeRF (Huang et al., 2022) to incorporate unit exposure loss on the logarithmic-domain tone mapper ϕ to fix the scale of learned HDR irradiance, enabling HDR quality evaluation, expressed as Lue = ϕ(0) C02 2, where C0 = 0.5 for real scenes and C0 = 0.73 (derived from GT CRF) for synthetic scenes. A.1.5 GAUSSIAN MOTION REGULARIZATION Rigidity Constraint. We enforce the as-rigid-as-possible (or distance preserving) loss (Wang et al., 2025a; Sun et al., 2024) between dynamic Gaussians and their k-nearest neighbors to ensure the local rigidity. For each Gaussian, let µt and µt be its positions at time and t, and Nk(µt) denote the set of k-nearest neighbors of µt, then this loss can be defined as: Larap = dist(µt, Nk(µt)) dist(µt, Nk(µt))2 2, (6) where dist(, ) measures Euclidean distance. We set = 5 in our experiments. Motion Smoothness. We apply the velocity and acceleration smoothness losses Lvel and Lacc to encourage smooth motion of dynamic Gaussians. Considering dynamic Gaussian with position µt and rotation matrix Rt at time t, we can follow MoSca (Lei et al., 2025) to obtain Lvel and Lacc as: Lvel = Nf 1 (cid:88) t=1 µt+1 µt2 + log(RtR1 t+1)F , Lacc = Nf 2 (cid:88) t=1 (µt 2µt+1 + µt+2)2 + (cid:12) (cid:12) log(RtR1 (cid:12) t+1)F log(Rt+1R1 t+2)F (7) (cid:12) (cid:12) (cid:12), where log()F is the Frobenius norm of rotation logarithm (the axis-angle of the rotation). 16 Figure 9: HDR visual comparisons under fix-time-change-view setting. A.1.6 HDR PHOTOMETRIC REPROJECTION LOSS The photometric reprojection loss is widely used in self-supervised monocular depth estimation (Godard et al., 2019) to optimize depths and camera poses together. Given the query timestamp and reference timestamp t, we can compute the reference pixel coordinate utt corresponding to the query pixel coordinate ut as: utt = π ˆK( ˆRt ˆR1 (π1 ˆK (ut, Dt(ut)) ˆTt) + ˆTt), (8) where ˆK is the camera intrinsics, [ ˆRt ˆTt] and [ ˆRt ˆTt] are the camera extrinsics at time and t, Dt(ut) is the depth value at pixel ut, π ˆK() and π1 () are the projection and unprojection funcˆK tions. Then we sample the corresponding HDR color of (cid:101)Ht(ut) in the reference frame by bilinear interpolation, denoted as (cid:101)Ht(utt). In this way, we can obtain the warped HDR image (cid:101)Htt from to and compute the HDR photometric reprojection loss as: Lpr = (cid:12) (cid:12) (cid:12)(1 Mt) (cid:101)Htt (cid:101)Ht (cid:101)Htt + (cid:101)Ht (cid:12) (cid:12) (cid:12)1 , (9) where Mt is the mask indicating dynamic pixels and the (cid:101)Htt + (cid:101)Ht normalization is to eliminate the influence of the scale of HDR values. We sample from {t 1, 2} in the experiments. Note that we use the recovered HDR training frames in the first stage for the computation of Lpr in the second stage, where we stop gradients of the leveraged HDR frames. A.2 ADDITIONAL EXPERIMENTAL SETTINGS A.2.1 DATASETS Syn-Exp-3. Syn-Exp-3 contains 9 synthetic HDR videos. Each video has nearly 100 frames of resolution 864 480. We adopt an alternate frame sampling strategy, where we extract every other Figure 10: Induced 2D/3D tracking by dynamic Gaussian motion in Mono4DGS-HDR. frame from the original sequence. Frames with odd indices are used for training, while frames with even indices are reserved for testing, resulting in an equal 50% split between training and testing data. The test frames are only used for quantitative evaluation and not for training, which are at novel (interpolated) views and times compared to training frames. Since the GT HDR videos are available, we can use them to create alternating-exposure LDR inputs. Specifically, we leverage predefined CRF function as in HDR-NeRF (Huang et al., 2022) to tone-map the HDR frame to LDR frame with given exposure time t, which is formulated as: = (Ht) = ( Ht Ht + 1 1 2.2 , ) (10) where we can derive the unit exposure loss GT as C0 = (1) = 0.73. In this way, we generate LDR frames with 3 (observed) exposure levels for training (LDR-OE). For test frames, we additionally create LDR images at another 2 exposure levels for novel exposure evaluation (LDR-NE). Real-Exp-3. Real-Exp-3 contains 8 real-world alternating-exposure LDR videos with 3 exposure levels. Each video has nearly 50-60 frames of resolution 864 480. We follow the same alternate frame sampling strategy as Syn-Exp-3 to split training and testing frames. The LDR inputs are directly taken from the original sequences. For testing frames, we can only evaluate on the observed exposures (LDR-OE). Real-Exp-2. Real-Exp-2 contains 8 real-world alternating-exposure LDR videos with 2 exposure levels. Each video has nearly 50-60 frames of resolution 864480. Since there are only 2 exposures, it is inconvenient to split training and testing frames using the alternate frame sampling strategy. Therefore, we directly apply the whole video sequence for training and evaluate on all training frames at the observed exposures (LDR-OE). A.2.2 EVALUATION METRICS HDR-TAE. Similar to the temporal alignment error (TAE) in video depth assessment (Yang et al., 2025), we introduce HDR-TAE to measure the temporal consistency of rendered HDR videos in per-pixel manner, defined as: HDR-TAE = 1 2(Nf 1) Nf 1 (cid:88) t=1 (cid:12) (cid:12) (cid:12)Vtt+1 (cid:101)Ht+1t (cid:101)Ht (cid:101)Ht+1t + (cid:101)Ht (cid:12) (cid:12) (cid:12)1 + (cid:12) (cid:12) (cid:12)Vt+1t (cid:101)Htt+1 (cid:101)Ht+1 (cid:101)Htt+1 + (cid:101)Ht+1 (cid:12) (cid:12) (cid:12)1 , (cid:101)Ht+1t = W( (cid:101)Ht+1, Ftt+1), (cid:101)Htt+1 = W( (cid:101)Ht, Ft+1t), (11) where W(, ) means backward warping. The optical flows {Ftt+1, Ft+1t} and visibility masks {Vtt+1, Vt+1t} are extracted between the adjacent HDR frames (cid:101)Ht and (cid:101)Ht+1 (tone-mapped version) using RAFT (Teed & Deng, 2020) model. For real-world scenes, we utilize the rendered HDR video frames. For synthetic scenes, we directly leverage the GT HDR video frames. Lower HDR-TAE indicates better temporal consistency. A.2.3 IMPLEMENTATION DETAILS We process on the original resolution of 864 480 for all videos. We threshold the epipolar error maps by 0.00001 to obtain the dynamic masks. For the cubic Hermite spline trajectory of each 18 dynamic Gaussian, we sample the control points every 4 frames. For HDR Gaussian color, we store 36-dimensional features and compute HDR color via color MLP instead of spherical harmonics in 3DGS for stability. The color and tone-mapper MLPs consist of one hidden layer with 36 and 128 channels respectively. We follow previous works (Huang et al., 2022; Cai et al., 2024; Liu et al., 2025a) to use three different MLPs to model RGB-channel tone mappers independently. Besides, we simply adopt 2D tone mapping described in GaussHDR (Liu et al., 2025a) for stable HDR reconstruction, where we first render the HDR Gaussians to an HDR image and then tone-map it to an LDR image. The loss weights are set as λrgb = 1, λue = 10, λdep = 1, λtrack = 0.01, λarap = 0.01, λvel = 10, λacc = 10, λtlr = 0.1 and λpr = 1. We optimize the first stage (fully dynamic video Gaussians) for 4K iterations, with 3K iterations of Gaussian densification where we densify every 200 steps and reset opacities every 1K steps. The second stage (static and dynamic world Gaussians) is optimized for 11K iterations with 8K iterations of Gaussian densification. For static Gaussians, we densify every 400 steps and reset opacities every 2K steps. For dynamic Gaussians, we densify every 200 steps, reset opacities every 2K steps and remove mistakenly densified Gaussians every 2K steps. In both stages, the temporal luminance regularization loss Ltlr is applied after the end of Gaussian densification. The learning rates of static Gaussian attributes are same as 3DGS (Kerbl et al., 2023). The learning rates of dynamic Gaussian attributes and camera parameters are following MoSca (Lei et al., 2025). The learning rate of tone-mapper and color MLPs is set to 0.0005. All experiments are conducted with Adam optimizer (Kingma & Ba, 2015) using Pytorch (Paszke et al., 2019) on single RTX 3090 GPU. A.3 ADDITIONAL EXPERIMENTAL RESULTS A.3.1 PERFORMANCE COMPARISONS We provide more comparison results with GFlow (Wang et al., 2025b) on training frames of RealExp-3 and Syn-Exp-3 scenes in Table 4. We also present more HDR visual comparisons on train/test frames in Fig. 7, and the results under fix-view-change-time and fix-time-change-view settings in Fig. 8 and Fig. 9, respectively. All these results demonstrate again the superiority of our method. A.3.2 ABLATION STUDY ON CUBIC HERMITE SPLINE TRAJECTORY We additionally conduct ablation study on the number of control points for cubic Hermite spline trajectory. Since different scenes have different video lengths, we explore on the sample interval Ns of control points. As listed in Table 5, the performance starts to degrade when Ns is more than 4. We set Ns = 4 in our experiments as trade-off between performance and storing memory, since smaller Ns means more control points and thus more memory cost. A.3.3 VISUALIZATION OF INDUCED 2D/3D TRACKING We visualize the induced 2D/3D tracking by the dynamic Gaussian motion of Mono4DGS-HDR in Fig. 10. The 2D tracking is obtained by projecting the dynamic Gaussian trajectory to the image plane, while the 3D tracking is directly derived from the dynamic Gaussian trajectory in the world space. The results demonstrate that our method can also induce accurate 2D/3D tracking of dynamic objects in the scene. More results are provided in the supplementary videos. A.4 LIMITATIONS Although our method achieves superior performance on monocular 4D HDR reconstruction, it still has some limitations. First, our method relies on the quality of 2D priors (depth, flow and track) to certain extent. Inaccurate priors may lead to suboptimal results. For example, erroneous depth priors can cause incorrect scene geometry, while inaccurate flow/track priors can misguide the motion of dynamic Gaussians. Besides, the dynamic masks (epipolar error maps) derived from optical flow priors may also not be perfect, which can affect the separation of static and dynamic world Gaussians. Second, our method cannot handle the image blur caused by fast camera/object motion. Luckily, we observe that coping with blurry monocular videos in Gaussian splatting has been discussed in Deblur4DGS (Wu et al., 2024d) and Casual3DHDR (Gong et al., 2025), which provide possibilities to achieve deblurring and HDR reconstruction simultaneously in future."
        }
    ],
    "affiliations": [
        "The Hong Kong University of Science and Technology (HKUST)",
        "vivo Mobile Communication Co., Ltd"
    ]
}