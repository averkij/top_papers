{
    "paper_title": "OmniFusion: Simultaneous Multilingual Multimodal Translations via Modular Fusion",
    "authors": [
        "Sai Koneru",
        "Matthias Huck",
        "Jan Niehues"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "There has been significant progress in open-source text-only translation large language models (LLMs) with better language coverage and quality. However, these models can be only used in cascaded pipelines for speech translation (ST), performing automatic speech recognition first followed by translation. This introduces additional latency, which is particularly critical in simultaneous ST (SimulST), and prevents the model from exploiting multimodal context, such as images, which can aid disambiguation. Pretrained multimodal foundation models (MMFMs) already possess strong perception and reasoning capabilities across multiple modalities, but generally lack the multilingual coverage and specialized translation performance of dedicated translation LLMs. To build an effective multimodal translation system, we propose an end-to-end approach that fuses MMFMs with translation LLMs. We introduce a novel fusion strategy that connects hidden states from multiple layers of a pretrained MMFM to a translation LLM, enabling joint end-to-end training. The resulting model, OmniFusion, built on Omni 2.5-7B as the MMFM and SeedX PPO-7B as the translation LLM, can perform speech-to-text, speech-and-image-to-text, and text-and-image-to-text translation. Experiments demonstrate that OmniFusion effectively leverages both audio and visual inputs, achieves a 1-second latency reduction in SimulST compared to cascaded pipelines and also improves the overall translation quality\\footnote{Code is available at https://github.com/saikoneru/OmniFusion}."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 2 ] . [ 1 4 3 2 0 0 . 2 1 5 2 : r OmniFusion: Simultaneous Multilingual Multimodal Translations via Modular Fusion Sai Koneru1, Matthias Huck2, and Jan Niehues1 1 Karlsruhe Institute of Technology 2 SAP SE, Dietmar-Hopp-Allee 16, 69190 Walldorf, Germany {sai.koneru, jan.niehues}@kit.edu {matthias.huck}@sap.com"
        },
        {
            "title": "Abstract",
            "content": "There has been significant progress in opensource text-only translation large language models (LLMs) with better language coverage and quality. However, these models can be only used in cascaded pipelines for speech translation (ST), performing automatic speech recognition first followed by translation. This introduces additional latency, which is particularly critical in simultaneous ST (SimulST), and prevents the model from exploiting multimodal context, such as images, which can aid disambiguation. Pretrained multimodal foundation models (MMFMs) already possess strong perception and reasoning capabilities across multiple modalities, but generally lack the multilingual coverage and specialized translation performance of dedicated translation LLMs. To build an effective multimodal translation system, we propose an end-to-end approach that fuses MMFMs with translation LLMs. We introduce novel fusion strategy that connects hidden states from multiple layers of pretrained MMFM to translation LLM, enabling joint end-to-end training. The resulting model, OmniFusion, built on Omni 2.57B as the MMFM and SeedX PPO-7B as the translation LLM, can perform speech-to-text, speech-and-image-to-text, and text-and-imageto-text translation. Experiments demonstrate that OmniFusion effectively leverages both audio and visual inputs, achieves 1-second latency reduction in SimulST compared to cascaded pipelines and also improves the overall translation quality1."
        },
        {
            "title": "Introduction",
            "content": "In recent years, research in natural language processing (NLP) has increasingly shifted from unimodal foundation models (Touvron et al., 2023; Grattafiori et al., 2024) to multimodal foundation models (MMFMs) (Abdin et al., 2024; Goel 1Code is available at https://github.com/saikoneru/ OmniFusion 1 et al., 2025; Xu et al., 2025), as many tasks benefit from contextual information derived from multiple sources. Although unimodal systems can perform strongly in certain settings, further progress also requires models to be able to take advantage of additional contextual cues without degrading quality. One representative task that can benefit from such multimodal integration is speech translation (ST), where the goal is to translate spoken input in source language into target language. Over the years, advances in data scale, modeling techniques, and architectural design have led to substantial improvements in ST quality across many language pairs and domains (Ahmad et al., 2024; Agostinelli et al., 2025). However, the use of multimodal contextsuch as visual information from images or presentation slides accompanying speechremains relatively underexplored (Liu and Niehues, 2024; Gaido et al., 2024a; Sinhamahapatra and Niehues, 2025). Although MMFMs are capable of processing multiple modalities jointly, their multilingual translation capabilities are generally weaker than those of specialized translation large language models (LLMs) (Alves et al., 2024; Cheng et al., 2025). As result, directly using MMFMs for ST often yields inferior translation quality compared to approaches that rely on strong text-based translation components (Appendix: Table 4). They also do not cover several languages compared to specialized translation LLMs. Consequently, practical solution is to adopt cascaded pipeline, where automatic speech recognition (ASR) is first performed, followed by machine translation (MT). This modular setup allows each component to be optimized independently and integrated during inference (Ahmad et al., 2024; Agostinelli et al., 2025; Koneru et al., 2025). However, since the translation model operates solely on text, it cannot exploit contextual information from the original speech or visual input. Moreover, the cascaded design introduces Figure 1: Architecture of OmniFusion with supported tasks and inference modes. additional generation steps, leading to increased latency compared to end-to-end (E2E) modelsan issue that is particularly critical for simultaneous ST (SimulST) applications (Agostinelli et al., 2025; Papi et al., 2025a). to how ASR serves as an alignment mechanism for audio. This strategy allows the model to effectively leverage pretrained multimodal representations while receiving direct cross-modal supervision. Therefore, we aim to develop an E2E MMFM that excels in multimodality, multilinguality and efficient for SimulST. This can be achieved by extending the capabilities of translation LLM to handle additional modalities. Few prior works have explored adding audio or image inputs to multilingual LLMs (Ambilduke et al., 2025; Viveiros et al., 2025). However, these approaches typically rely on training from scratch or only using pretrained audio/vision encoders and do not leverage the higher-level representational capabilities of existing MMFMs. Furthermore, training is ineffective as the base LLM needs to learn the modality mapping from scratch. In this work, we propose leveraging pretrained MMFMs to introduce multimodal capabilities into multilingual LLM with E2E architecture. Rather than transferring only vision or audio encoders, we extract and utilize hidden-state representations from multiple layers of the MMFM to capture both the perceptual and reasoning components of multimodal understanding. Specifically, we adapt the translation LLM by incorporating hidden states from the first, middle, and last layers of the MMFM, thereby enabling the transfer of both low-level perception and high-level semantic understanding. To further strengthen cross-modal alignment, we introduce Optical Character Recognition (OCR) as bridge task for the image modality, analogous We use Qwen Omni 2.5 7B (Xu et al., 2025) as the MMFM and SeedX 7B (Cheng et al., 2025) as the translation LLM to validate our approach on SimulST and multimodal MT tasks. We refer to the resulting fused model as OmniFusion. Our main contributions are summarized as follows: We propose gated fusion approach that extracts multimodal embeddings from MMFMs, allowing translation LLMs to fully utilize the pretrained perceptual and interpretive capabilities of MMFMs. We show the benefit of using slides for context and achieve an improvement in latency of approximately 1 second in the SimulST scenario compared to cascaded fine-tuned models while having better quality. (Figure 2). We observe reduction in the major and critical errors for offline ST through selfcascading and effective use of the image context  (Table 2)  . We achieve state-of-the-art performance in the CoMMuTE benchmark for image-text translation highlighting the ability of the model to exploit images  (Table 3)  ."
        },
        {
            "title": "2 Approach",
            "content": "We propose to fuse MMFM with task-specific translation LLM to leverage their complementary strengths and enable E2E training for improved latency. Furthermore, we aim to incorporate not only the perception capabilities of the MMFM but also its multimodal reasoning and understanding. First, we describe the overall architecture and training process of our approach, considering general input and output sequence independent of the specific task. We formally define the information flow, training loss, and the fusion of MMFM layers. Next, we detail the training tasks used to align multimodal representations from both image and audio modalities to the translation LLM. Finally, we discuss the training prompts that enable selfcascading inference mode, allowing the model to enhance translation quality at the cost of increased latency. An overview of the architecture and supported tasks is in Figure 1. 2.1 Architecture Let us denote the specialized translation LLM as MNMT and the MMFM as MMMFM, where both use language model as their backbone. The MNMT is trained for translation via supervised finetuning with the following loss function: LTrans = 1 Ty Ty (cid:88) t=1 log Pθ (cid:0)yt y<t, X(cid:1), (1) = (x1, . . . , xLx), = (y1, . . . , yTy ) where = (x1, . . . , xLx) is the sequence of source tokens of length Lx, and = (y1, . . . , yTy ) is the sequence of target tokens of length Ty. For MMMFM, the image and audio modalities are first processed by modality-specific encoders and then projected to match the embedding dimension of MMMFM, denoted as DMMFM. This projection allows the encoder outputs to be concatenated as sequence of tokens, enabling the MMFM to operate similarly to standard LLM, such as MNMT. To fuse both the perception and higher-level understanding of the MMFM, we hypothesize that lower layers predominantly capture perceptual features, while higher layers encode more abstract reasoning as computation progresses. Accordingly, we extract the first, middle, and last hidden layers from the MMFM, denoted as: = MMFMlayer mid(xt), h(1) = MMFMlayer 1(xt), h(mid) h(last) , h(mid) = MMFMlayer last(xt), , h(last) h(1) RLmDMMFM, = 1, . . . , Lm (2) where Lm is the length of the MMFM hidden states. Directly concatenating these multi-dimensional representations into MNMT poses three challenges. First, the sequence length triples, significantly increasing computational and memory requirements. Second, the relevance of each layer may vary depending on the task, necessitating selective weighting. Third, the embedding dimension of MMMFM may not match that of MNMT. To overcome these challenges, we introduce gated fusion layer that determines the contribution of each hidden state along the sequence length. Each hidden state is multiplied by its corresponding gate values, summed across layers, and then projected through an MLP to align the embedding dimension with that of the translation model: t h(mid) Wgate R33DMMFM, [g1, gmid, glast]t = softmax(cid:0)Wgate h(1) = g1 h(1) mt = MLP(m (cid:1), h(last) + glast h(last) = 1, . . . , Lx (3) where DTrans is the embedding dimension of MNMT and mt is the fused multimodal representation at time step t. + gmid h(mid) t) RLmDTrans, , t The fused multimodal embeddings mt can now be treated like additional token embeddings and concatenated with the MN token embeddings: xt = mt xt, = (x1, . . . , xLm+Lx) Finally, the multimodal translation loss is computed by conditioning on the concatenated sequence while predicting the target tokens: (4) LMM-Trans = 1 Ty t=1 Ty (cid:88) log Pθ (cid:0)yt y<t, X(cid:1) (5) Where is the multimodal input to the translation model. 3 2.2 Training We have shown how MMFM internal representations can be extracted and integrated into our downstream model. However, leveraging these representations effectively also requires training process that explicitly aligns multimodal features so that the LLM can exploit them. The exact alignment procedure depends on the downstream tasks we expect the fused model to perform after fine-tuning. In this work, our goal is to obtain translation model that can utilize both audio and image context. Therefore, we must make the translation LLM context-aware by training to exploit these representations. We consider 3 translation tasks in this work and are defined as follows: Speech Translation (ST): Generate translation in the target language given only audio in the source language. Speech-Image Translation (SIT): Generate translation in the target language given sourcelanguage audio together with an image that provides additional context (e.g., slide corresponding to the spoken segment). Text-Image Translation (TIT): Generate translation in the target language given the source text and an aligned image, such as in caption translation scenarios. While we can train the model to perform translation directly, decomposing the task into intermediate steps can be beneficial. For example, translating English audio to German can be approached by first predicting ASR outputs and then producing the translation by continuing generation conditioned on the ASR hypothesis. This decomposition not only simplifies learning but also provides explicit alignment signals that help map audio representations into the LLMs semantic space. When vision input is availableparticularly for slide-based scenarioswe additionally introduce OCR as bridge task before generating the final translation. In this case, the model must predict both OCR text and the final translation using the same audiovisual representation. This encourages alignment between the modalities and explicitly informs the model that the visual context is meaningful and should be utilized. During training, we decidebased on specified probability for each data sourcewhether an example should stochastically be trained in self-cascading mode and along which modality the cascade should occur. Furthermore, we provide modality-specific prompt to the MMFM for handling different modality combination scenarios. Pseudocode for building the prompt for each task during training is shown in Algorithm 1. We also freeze the MMFM to preserve its pretrained abilities while fine-tuning the translation LLM with LoRA (Hu et al., 2021). We use LoRA for computational benefits and not go Out-OfMemory during fine-tuning."
        },
        {
            "title": "3 Experimental Setup",
            "content": "3.1 Training Data We aim to train the fused model to perform translation tasks across both speech and image modalities. An overview of the data sources is provided in Table 1. For ST, we use subsets of Europarl-ST (IranzoSánchez et al., 2020) and Covost-2 (Wang et al., 2020), sampling across multiple target languages. We limit training data to these subsets due to computational constraints and to demonstrate that full datasets are not strictly necessary. To enable the model to exploit images when translating audio, we use the M3AV corpus (Chen et al., 2024), which contains English scientific talks paired with corresponding slides. The slides also include Paddle-OCR (Cui et al., 2025) predictions, allowing the model to be trained either to predict translations directly or via an intermediate OCR step for better alignment. Since M3AV has limited language coverage, we augment the data by synthetically translating ASR transcripts into additional target languages (M3AVAug). This enables the model to handle ST with slides and supports the self-cascade mode, performing ASR first and then translation. Finally, for TIT, we employ the Multi30k dataset (Elliott et al., 2016), which contains images paired with captions in multiple languages. This task encourages the model to go beyond OCR and to leverage visual content, including objects and scene understanding, for translation. 3.2 Models Several MMFMs have been developed rapidly in recent years, showing significant improvements in both quality and efficiency (Abdin et al., 2024; Team et al., 2025). In our scenario, we require 4 Dataset # Examples Language Modality Task M3AV 122k en Speech, Image ASR, OCR M3AV - Aug 122k Europarl Covost2 260k* zh, fr, de, it, ja, ko, pt, ru, es, vi, ar, cs, hr, da, nl, fi, hu, id, ms, nb, no, pl, ro, tr de, es, fr, it, nl, pl, pt, ro de, ja, tr, zh Coco-Captions 87k cs, de, fr Speech, Image ST, OCR Speech Speech Image ST ST TIT Table 1: Overview of data sources collected for E2E training. * We sample together 260k from the concatenated data uniformly across the language pairs. All the speech and text data are only available with English as the source language. Language code mapping is presented in Table 9. model capable of processing both audio and image modalities, while also supporting cross-modal abilities. Accordingly, we select Qwen Omni 2.5B2 (Xu et al., 2025), due to its strong multimodal capabilities and the potential to incorporate video in future experiments. For the translation LLM, we choose the recently proposed SeedX PPO 7B3 (Cheng et al., 2025), which provides broad language coverage and strong translation performance compared to closed-source alternatives. We use the training data mentioned above and denote the fine-tuned model as OmniFusion for the rest of the paper. While we report results using these specific models, it is important to note that our approach is not limited to these base LLMs. The proposed fusion strategy does not require massive pre-training or fine-tuning resources and can be adapted in more modest computing environments, such as university clusters. The training hyper-parameters are specified in Table 10. For validation, we used Covost-2 validation data and select the checkpoint with the least loss in 20k steps. 3.3 Evaluation: Data & Metrics For evaluating the performance of multimodal Simul-ST systems, we require aligned audio-visual translation data. Additionally, we avoid using M3AV for testing, as it overlaps with our training data, monolingual and we aim to measure the models robustness on unseen domains. Consequently, we use the MCIF test set (Papi et al., 2025b), which contains video and audio recordings of ACL talks in three target languages from English. We report latency with Average Lagging (Ma et al., 2020) and translation quality results on this dataset. For TIT, we focus on test cases where the image is crucial for disambiguating the source sentence. For this, we use the CoMMuTE dataset (Futeral et al., 2023, 2025), which provides small but multilingual targeted TIT test set. We measure translation quality using XCOMETXL (Guerreiro et al., 2024) for ST scenarios, as it offers high-quality evaluation and allows analysis of minor, major, and critical errors. For TIT, we use the COMET4 model (Rei et al., 2022) directly, without providing error spans, to evaluate aligned references."
        },
        {
            "title": "4 Results",
            "content": "Our primary motivation for the proposed approach is to build an E2E ST system that is context-aware, efficient for SimulST and enabled to exploit additional image context. To validate this, we first evaluate the system in simultaneous scenarios by comparing latency and quality. Next, we report quality in offline scenarios and conduct an error analysis to contrast the E2E system with traditional cascaded systems fine-tuned on the same data. We then assess performance in TIT scenarios, benchmarking against other off-the-shelf multimodal translation models. Finally, we perform an ablation study to analyze the contributions of individual layers in the MMFM and present the key findings. We also validate the proposed approach for different model combination (Qwen-Audio (Chu et al., 2023) and Tower-Instruct (Alves et al., 2024)), analyze training with different layers and post-editing in our preliminary experiments and report the results in the Appendixes Table 4 and table 5. 4.1 Simultaneous Speech Translation SimulST system must generate translations incrementally as the source audio arrives, rather than waiting for the entire utterance to finish. The decision of whether to commit token or read more audio is referred to as the policy. Since our E2E system does not natively support simultaneous translation, we adopt the widely used Local Agreement policy (Liu et al., 2020), which commits the longest 2Qwen/Qwen2 Audio.5-Omni-7B 3ByteDance-Seed/Seed-X-PPO-7B 4Unbabel/wmt22-comet-da 5 (a) en de (b) en it (c) en de (d) en it Figure 2: SimulST performance on ende and enit. Average Lag (AL, lower is better) vs. XCOMET-XL quality score (higher is better). Results compare OmniFusion (E2E) against the fine-tuned cascaded baseline and the offline upper bound (with Image). Top figures are computation-unaware and bottom are computation-aware. common prefix observed across consecutive predictions. Concretely, at each step we process the audio in fixed chunk sizes (a hyperparameter) and irreversibly emit the overlapping prefix between the current and previous hypotheses. After committing, subsequent tokens are force-decoded conditioned on the partial translation committed so far. To fairly evaluate the proposed approach, we compare our E2E model OmniFusion against strong cascaded counterpart built from the same pretrained components. We therefore fine-tune this cascaded baseline on the same data using LoRA adapters (Omni + SeedX LoRA FT). The effect of fine-tuning is discussed in more detail in Section 4.2. We experiment with chunk sizes of [1, 1.5, 2, 2.5, 3] seconds, run inference both with and without images, and report computationaware and computation-unaware AL together with XCOMET-XL scores for the en de and en it language pairs in Figure 2. The results reveal several key insights. First, as expected, the offline ST systems (with images) achieve the highest quality in both language pairs due to access to full context. Next, OmniFusion is consistently around 1 second faster than its cascaded counterpart across all chunk sizes while preserving comparable quality in the audio-only setting. When images are provided, OmniFusion delivers both the lowest latency and the highest quality, demonstrating that visual context is substantially more beneficial in the E2E model than in the cascaded baseline allowing it to commit high quality stable outputs earlier. Finally, we also see that in computation-aware settings, adding image causes delay due to processing and causes overhead than only-audio creating trade-off between quality and latency. We attribute the advantage of OmniFusion to two factors: (1) our fine-tuning strategy, which includes an OCR objective that tightly aligns image and audio representations, and (2) the inherent efficiency of the E2E architecture, which requires fewer decoding steps overall. In contrast, the cascaded pipeline with Local Agreement must wait for Category Cascade Systems (ASR MT) Omni + SeedX Config No FT No FT LoRA FT LoRA FT End-to-End (Direct ST) OmniFusion Mid Fusion Mid Fusion Gated Fusion Gated Fusion Gated Fusion + Self-Casade Image XCOMET-XL Minor Major Critical Total 85.88 85.94 86.42 86.59 83.57 85.98 83.98 86.24 86. 1212.3 1191.3 1239.0 1202.3 1114.0 1256.3 1119.3 1255.7 1231.0 773.0 759.3 751.3 766.3 1188.3 749.0 1188.0 736.0 719.0 74.3 74.7 75.0 55.7 72.3 61.7 62.7 57.0 55. 2059.7 2025.3 2065.3 2024.3 2374.7 2067.0 2370.0 2048.7 2005.3 Table 2: Offline ST results averaged across enzh, ende, and enit on the MCIF test. indicates models using image input. Bold indicates best XCOMET-XL and lowest error counts. Language-wise results are additionaly reported in Table 6, 7 and 8. stable ASR hypotheses before translation can even begin, making it slower than the E2E approach. In conclusion, OmniFusion achieves better latency, leverages image context effectively, and has higher quality in SimulST scenarios, making it particularly well-suited for real-world applications such as live presentations and meetings where visual information is readily available. 4.2 Offline Speech Translation Although SimulST is the primary focus of this work, we also evaluate the systems in the offline scenario, where latency is not concern and the goal is to maximize translation quality. In this setting, we can leverage the self-cascade inference mode described in Section 2.2 to further boost performance. To measure the contributions of fine-tuning and visual context, we first evaluate the cascaded baseline in four configurations: base vs. fine-tuned (LoRA FT) models, each tested with and without images. For the E2E OmniFusion model, we additionally train variant that uses only the middlelayer fusion without gating to assess the need for the gating module. We then compare all E2E configurations (mid-only fusion and full gated fusion) both with and without images, and with/without self-cascade inference. Results are averaged across three target languages en {de, it, zh} and reported in Table 2 using XCOMET-XL scores. We further include the detailed XCOMET-XL error categories, hypothesizing that E2E models may sacrifice minor fluency for substantially fewer major and critical errors. The results yield several insights. In the cascaded pipeline, LoRA fine-tuning yields consistent gains (e.g., 85.94 86.59 with images), whereas adding images provides only marginal benefit. In contrast, the E2E OmniFusion model benefits substantially from both gated fusion (outperforming mid-only fusion) and image context showing the need for gating module and additional contextual information. Although the direct E2E translation score remains slightly below the best cascaded system, enabling self-cascade inference closes this gap almost entirely (cascaded LoRA FT: 86.59 vs. E2E gated fusion + self-cascade: 86.57). More importantly, error analysis reveals that our best E2E configuration produces the fewest critical and major errors (e.g., 719 vs. 751.3 for the strongest cascaded baseline). These findings demonstrate that OmniFusion, when equipped with image context and self-cascade inference, matches fine-tuned cascaded system in offline ST quality while offering significantly fewer severe translation errors 4.3 Text-Image Translation Apart from ST, translating captions also relies on image context, where cascaded systems fail due to their inability for disambiguation. To demonstrate this, we evaluate several models on the CoMMuTE test set across multiple language pairs and report COMET scores in Table 3. First, the audio-only Omni 2.5B exhibits the weakest multilingual performance, which reinforces our motivation for integrating it with strong translation backbone. SeedX 7B obtains reasonable scores yet consistently trails all vision-capable models, confirming its limitation in the absence of image context. Among vision-aware systems, both Mid Fusion and Gated Fusion variants of our OmniFusion 14B achieve state-of-the-art or near-SOTA COMET scores, matching or surpassing visiononly models such as ZeroMMT 3.3B (Futeral et al., 2025) and TowerVision 9B (Viveiros et al., 2025) on most language directions. Interestingly, and in contrast to ST, the Mid Fusion variant slightly outperforms Gated Fusion on 7 Model Omni 2.5B SeedX 7B ZeroMMT 3.3B (Futeral et al., 2025) TowerVision 9B (Viveiros et al., 2025) OmniFusion 14B OmniFusion 14B Config en ar en cs en de en fr en ru en zh - - - - Mid Fusion Gated Fusion 78.56 80.39 81.03 83.56 81.94 82.38 79.99 80.31 47.27 82.13 81.74 83. 80.64 86.69 83.54 83.40 83.87 77. 70.85 80.98 80.81 86.62 87.49 87.58 84.97 85.14 84. 84.29 84.97 84.28 85.81 85.69 84.62 85.24 85.15 84. Table 3: COMET scores () on the CoMMuTE dataset. Statistically significant top-performing results per language pair are highlighted in bold. Config distinguishes the two OmniFusion variants (mid-layer fusion vs. gated fusion). CoMMuTE. This suggests that, when the image serves primarily as supplementary context (as in caption translation) rather than the main information source (as in audio for spoken ST), using visual features at the middle layer alone may be sufficient, and the additional flexibility of gating offers no further advantage. Importantly, unlike TowerVision which requires massive multimodal pretraining from scratch, OmniFusion can be built on top of existing strong pretrained components. This makes our approach far more dataand training-efficient without compromising quality, by effectively exploiting the rich imagetext alignment knowledge already present in the MMFM pretrained weights. These results underline the strength of our E2E design on visually grounded translation tasks where image context is essential. 4.4 Contribution of Layers Figure 3: Analysis of the layer contribution from the MMFM in the gating module across modalities. We presented gating module in Section 2.1 for fusing information across layers of the MMFM. To better understand its behavior, we conduct an ablation study by examining the gate values assigned to 8 each MMFM layer during fusion. Specifically, we compute the average gate value across the three layers (first, middle, last), noting that the gate weights for each timestep sum to 1. We run inference on 100 examples from and the MCIF datasetboth speech+image conditionsand on the CoMMuTE dataset for the image-only modality. The resulting average gate values are shown in Figure 3. speech-only We observe that the first and middle layers receive consistently higher contributions, while the last layer contributes only marginally. This suggests that the majority of transferable multimodal processing occurs in the earlier half of the MMFM. Although the weak contribution of the last layer is not entirely clear, we hypothesize that these representations may be more language-specific, whereas the middle layers capture more language-agnostic features (Liu and Niehues, 2025)."
        },
        {
            "title": "5 Related Work",
            "content": "Multimodal MT: Translating captions has been widely explored in the MT field over the last decade (Specia et al., 2016; Elliott et al., 2017; Barrault et al., 2018). Several works focus on enabling NMT models to process images, either by training models from scratch (Vijayan et al., 2024) or by augmenting large pretrained models such as NLLB (Costa-Jussà et al., 2022; Futeral et al., 2025). Most recently, and in parallel to our work, Viveiros et al. (2025) extend Tower with image and video modalities through computationally expensive multistage training pipeline. While these efforts focus primarily on vision, an other line of direction is adding audio to translation LLMs (Ambilduke et al., 2025). smaller line of work tackles the combination of vision and audio, such as Sinhamahapatra and Niehues (2025), which exploits slides and speech for ASR. Building on this trend, there is also increasing interest in translating audio together with video (Anwar et al., 2023; Choi et al., 2024). In contrast to these works, our approach focuses on jointly using images and audio specifically for ST, while remaining efficient in both training and inference. Simultaneous ST: Many real-world scenarios require real-time ST without waiting for the full source utterance. However, most research focuses on training models in offline settings and only later adapting them to SimulST. Such adaptation typically relies on predefined or learned translation policies, such as wait-k (Ma et al., 2019; Elbayad et al., 2020), which assumes fixed token delay before committing to output. Other approaches include Local Agreement (Liu et al., 2020), which selects the most stable prefix across consecutive read steps or attention-based heuristics (Papi et al., 2023). More recently, several works explore leveraging LLMs for SimulST policy learning (Koshkin et al., 2024b,a; Guo et al., 2025; Fu et al., 2025). In our work, we use Local Agreement to validate the effectiveness of our approach and to compare against cascaded baselines. However, our method is compatible in theory with other SimulST policies, and integrating more advanced or LLM-driven strategies is promising direction for future work. Modality Fusion: With the rapid development of multimodal foundation models, key challenge is how to effectively integrate speech and image modalities into text-centric LLMs. Some approaches (Abdin et al., 2024; Xu et al., 2025; Team et al., 2025) process each modality with separate encoders and merge them later within the LLM backbone. In contrast, models such as (Ye et al., 2025) enable early interaction between vision and audio modalities, facilitating better cross-modal alignment. Beyond the timing of modality interaction, ongoing research also investigates the design of modality projectors (Verdini et al., 2024; Gaido et al., 2024b) and the use of continuous versus discrete tokens for non-text modalities (Zhan et al., 2024; Li et al., 2025), balancing the trade-offs between latency and translation quality."
        },
        {
            "title": "6 Conclusion",
            "content": "We proposed novel approach to fuse MMFM with translation LLM, and validated its effectiveness across three key scenarios. First, we showed that the fused model achieves faster inference while maintaining strong translation quality in SimulST settings, even when only partial source audio is available. Second, the model produces fewer major and critical translation errors, demonstrating more reliable linguistic accuracy. Third, we found that the model effectively leverages visual context for both speech and text translation, leading to consistent gains in multimodal settings. Finally, our layer-level analysis reveals that the first and middle MMFM layers contribute most to capturing semantic multimodal information, providing insight into where meaningful cross-modal representations emerge. For future work, our fusion framework can naturally be extended to additional modalities. Since Omni already supports video processing, incorporating video embeddings would enable richer context for translation. Moreover, the fusion strategy is not limited to models for multimodal inputs but future extensions could target multimodal generation, allowing the system to produce not only text but also speech, images, or video as output."
        },
        {
            "title": "Limitations",
            "content": "A primary limitation of our work is that OmniFusion is trained with fixed task-specific prompts across the three supported translation settings, and therefore does not exhibit general instructionfollowing capabilities. It remains unclear whether our fusion strategy can be extended to support more flexible, instruction-driven multimodal behavior. In addition, most of our experiments use English as the source language. Although we report zero-shot results in Table 4, we do not evaluate the OmniFusion model with vision inputs in multilingual zero-shot conditions."
        },
        {
            "title": "References",
            "content": "Marah Abdin, Jyoti Aneja, Harkirat Behl, Sébastien Bubeck, Ronen Eldan, Suriya Gunasekar, Michael Harrison, Russell Hewett, Mojan Javaheripi, Piero Kauffmann, James Lee, Yin Tat Lee, Yuanzhi Li, Weishung Liu, Caio Mendes, Anh Nguyen, Eric Price, Gustavo de Rosa, Olli Saarikivi, and 8 others. 2024. Phi-4 technical report. arXiv [cs.CL]. Victor Agostinelli, Tanel Alumäe, Antonios Anastasopoulos, Luisa Bentivogli, Ondˇrej Bojar, Claudia Borg, Fethi Bougares, Roldano Cattoni, Mauro Cettolo, Lizhong Chen, and 1 others. 2025. Findings of the iwslt 2025 evaluation campaign. In Proceedings of the 22nd International Conference on Spoken Language Translation (IWSLT 2025), pages 412481. 9 Ibrahim Said Ahmad, Antonios Anastasopoulos, Ondˇrej Bojar, Claudia Borg, Marine Carpuat, Roldano Cattoni, Mauro Cettolo, William Chen, Qianqian Dong, Marcello Federico, Barry Haddow, Dávid Javorský, Mateusz Krubinski, Tsz Kin Lam, Xutai Ma, Prashant Mathur, Evgeny Matusov, Chandresh Maurya, John McCrae, and 25 others. 2024. FINDINGS OF THE IWSLT 2024 EVALUATION CAMIn Proceedings of the 21st International PAIGN. Conference on Spoken Language Translation (IWSLT 2024), pages 111, Bangkok, Thailand (in-person and online). Association for Computational Linguistics. Duarte Alves, José Pombal, Nuno Guerreiro, Pedro Martins, João Alves, Amin Farajian, Ben Peters, Ricardo Rei, Patrick Fernandes, Sweta Agrawal, Pierre Colombo, José de Souza, and André Martins. 2024. Tower: An open multilingual large language model for translation-related tasks. arXiv [cs.CL]. Kshitij Ambilduke, Ben Peters, Sonal Sannigrahi, Anil Keshwani, Tsz Kin Lam, Bruno Martins, André Martins, and Marcely Zanon Boito. 2025. From TOWER to SPIRE: Adding the speech modality to translation-specialist LLM. arXiv [cs.CL]. Mohamed Anwar, Bowen Shi, Vedanuj Goswami, WeiNing Hsu, Juan Pino, and Changhan Wang. 2023. Muavic: multilingual audio-visual corpus for robust speech recognition and robust speech-to-text translation. In Proc. Interspeech 2023, pages 4064 4068. Loïc Barrault, Fethi Bougares, Lucia Specia, Chiraag Lala, Desmond Elliott, and Stella Frank. 2018. Findings of the third shared task on multimodal machine translation. In Proceedings of the Third Conference on Machine Translation: Shared Task Papers, pages 304323, Belgium, Brussels. Association for Computational Linguistics. Loïc Barrault, Yu-An Chung, Mariano Cora Meglioli, David Dale, Ning Dong, Paul-Ambroise Duquenne, Hady Elsahar, Hongyu Gong, Kevin Heffernan, John Hoffman, and 1 others. 2023. Seamlessm4t: massively multilingual & multimodal machine translation. arXiv preprint arXiv:2308.11596. Zhe Chen, Heyang Liu, Wenyi Yu, Guangzhi Sun, Hongcheng Liu, Ji Wu, Chao Zhang, Yu Wang, and Yanfeng Wang. 2024. M3AV: multimodal, multigenre, and multipurpose audio-visual academic lecIn Proceedings of the 62nd Annual ture dataset. Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 90419060, Bangkok, Thailand. Association for Computational Linguistics. Shanbo Cheng, Yu Bao, Qian Cao, Luyang Huang, Liyan Kang, Zhicheng Liu, Yu Lu, Wenhao Zhu, Jingwen Chen, Zhichao Huang, Tao Li, Yifu Li, Huiying Lin, Sitong Liu, Ningxin Peng, Shuaijie She, Lu Xu, Nuo Xu, Sen Yang, and 7 others. 2025. Seed-X: Building strong multilingual translation LLM with 7B parameters. arXiv [cs.CL]. Jeongsoo Choi, Se Jin Park, Minsu Kim, and Yong Man Ro. 2024. Av2av: Direct audio-visual speech to audio-visual speech translation with unified audiovisual speech representation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2732527337. Yunfei Chu, Jin Xu, Xiaohuan Zhou, Qian Yang, Shiliang Zhang, Zhijie Yan, Chang Zhou, and Jingren Zhou. 2023. Qwen-audio: Advancing universal audio understanding via unified large-scale audiolanguage models. arXiv preprint arXiv:2311.07919. Marta Costa-Jussà, James Cross, Onur Çelebi, Maha Elbayad, Kenneth Heafield, Kevin Heffernan, Elahe Kalbassi, Janice Lam, Daniel Licht, Jean Maillard, and 1 others. 2022. No language left behind: Scaling human-centered machine translation. arXiv preprint arXiv:2207.04672. Cheng Cui, Ting Sun, Manhui Lin, Tingquan Gao, Yubo Zhang, Jiaxuan Liu, Xueqing Wang, Zelun Zhang, Changda Zhou, Hongen Liu, and 1 others. 2025. Paddleocr 3.0 technical report. arXiv preprint arXiv:2507.05595. Maha Elbayad, Laurent Besacier, and Jakob Verbeek. 2020. Efficient wait-k models for simultaneous machine translation. In Interspeech 2020-Conference of the International Speech Communication Association, pages 14611465. Desmond Elliott, Stella Frank, Loïc Barrault, Fethi Bougares, and Lucia Specia. 2017. Findings of the second shared task on multimodal machine translation and multilingual image description. In Proceedings of the Second Conference on Machine Translation, pages 215233, Copenhagen, Denmark. Association for Computational Linguistics. Desmond Elliott, Stella Frank, Khalil Simaan, and Lucia Specia. 2016. Multi30K: Multilingual EnglishGerman image descriptions. In Proceedings of the 5th Workshop on Vision and Language, pages 70 74, Berlin, Germany. Association for Computational Linguistics. Biao Fu, Donglei Yu, Minpeng Liao, Chengxi Li, Yidong Chen, Kai Fan, and Xiaodong Shi. 2025. Efficient and adaptive simultaneous speech translation with fully unidirectional architecture. arXiv preprint arXiv:2504.11809. Matthieu Futeral, Cordelia Schmid, Ivan Laptev, Benoît Sagot, and Rachel Bawden. 2023. Tackling ambiguity with images: Improved multimodal machine translation and contrastive evaluation. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 53945413, Toronto, Canada. Association for Computational Linguistics. 10 Matthieu Futeral, Cordelia Schmid, Benoît Sagot, and Rachel Bawden. 2025. Towards zero-shot multimodal machine translation. In Findings of the Association for Computational Linguistics: NAACL 2025, pages 761778, Albuquerque, New Mexico. Association for Computational Linguistics. Roman Koshkin, Katsuhito Sudoh, and Satoshi Nakamura. 2024a. LLMs are zero-shot context-aware simultaneous translators. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 11921207, Miami, Florida, USA. Association for Computational Linguistics. Marco Gaido, Sara Papi, Matteo Negri, and Luisa Bentivogli. 2024a. Speech translation with speech foundation models and large language models: What is there and what is missing? arXiv [cs.CL]. Marco Gaido, Sara Papi, Matteo Negri, and Luisa Bentivogli. 2024b. Speech translation with speech foundation models and large language models: What is there and what is missing? In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1476014778. Arushi Goel, Sreyan Ghosh, Jaehyeon Kim, Sonal Kumar, Zhifeng Kong, Sang-Gil Lee, Chao-Han Huck Yang, Ramani Duraiswami, Dinesh Manocha, Rafael Valle, and Bryan Catanzaro. 2025. Audio flamingo 3: Advancing audio intelligence with fully open large audio language models. arXiv [cs.SD]. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad AlDahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, and 542 others. 2024. The llama 3 herd of models. arXiv [cs.AI]. Nuno M. Guerreiro, Ricardo Rei, Daan van Stigt, Luisa Coheur, Pierre Colombo, and André F. T. Martins. 2024. xCOMET: Transparent machine translation evaluation through fine-grained error detection. Transactions of the Association for Computational Linguistics, 12:979995. Shoutao Guo, Xiang Li, Mengge Liu, Wei Chen, and Yang Feng. 2025. Streamuni: Achieving streaming speech translation with unified large speechlanguage model. arXiv preprint arXiv:2507.07803. Edward Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, and 1 others. 2021. Lora: Low-rank adaptation of large language models. In International Conference on Learning Representations. J. Iranzo-Sánchez, J. A. Silvestre-Cerdà, J. Jorge, N. Roselló, A. Giménez, A. Sanchis, J. Civera, and A. Juan. 2020. Europarl-st: multilingual corpus for speech translation of parliamentary debates. In ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 82298233. Sai Koneru, Maike Züfle, Thai-Binh Nguyen, Seymanur Akti, Jan Niehues, and Alexander Waibel. 2025. KITs offline speech translation and instruction following submission for IWSLT 2025. arXiv [cs.CL]. Roman Koshkin, Katsuhito Sudoh, and Satoshi Nakamura. 2024b. TransLLaMa: LLM-based simultaneous translation system. In Findings of the Association for Computational Linguistics: EMNLP 2024, pages 461476, Miami, Florida, USA. Association for Computational Linguistics. Jindong Li, Yali Fu, Jiahong Liu, Linxiao Cao, Wei Ji, Menglin Yang, Irwin King, and Ming-Hsuan Yang. 2025. Discrete tokenization for multimodal arXiv preprint llms: comprehensive survey. arXiv:2507.22920. Danni Liu and Jan Niehues. 2024. Recent highlights in multilingual and multimodal speech translation. In Proceedings of the 21st International Conference on Spoken Language Translation (IWSLT 2024), pages 235253, Stroudsburg, PA, USA. Association for Computational Linguistics. Danni Liu and Jan Niehues. 2025. Middle-layer representation alignment for cross-lingual transfer in fine-tuned LLMs. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 15979 15996, Vienna, Austria. Association for Computational Linguistics. Danni Liu, Gerasimos Spanakis, and Jan Niehues. 2020. Low-latency sequence-to-sequence speech recognition and translation by partial hypothesis selection. In 21st Annual Conference of the International Speech Communication Association, pages 36203624. Mingbo Ma, Liang Huang, Hao Xiong, Renjie Zheng, Kaibo Liu, Baigong Zheng, Chuanqiang Zhang, Zhongjun He, Hairong Liu, Xing Li, Hua Wu, and Haifeng Wang. 2019. STACL: Simultaneous translation with implicit anticipation and controllable latency using prefix-to-prefix framework. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 30253036, Florence, Italy. Association for Computational Linguistics. Xutai Ma, Javad Mohammad Dousti, Changhan Wang, Jiatao Gu, and Juan Pino. 2020. Simuleval: An evaluation toolkit for simultaneous translation. In Proceedings of the EMNLP. Sara Papi, Peter Polák, Dominik Macháˇcek, and Ondˇrej Bojar. 2025a. How real is your real-time simultaneous speech-to-text translation system? Trans. Assoc. Comput. Linguist., 13:281313. Sara Papi, Marco Turchi, and Matteo Negri. 2023. Alignatt: Using attention-based audio-translation alignments as guide for simultaneous speech translation. In Proceedings of INTERSPEECH 2023, pages 3974 3978. 11 Sara Papi, Maike Züfle, Marco Gaido, Beatrice Savoldi, Danni Liu, Ioannis Douros, Luisa Bentivogli, and Jan Niehues. 2025b. Mcif: Multimodal crosslingual instruction-following benchmark from scientific talks. Preprint, arXiv:2507.19634. In Proceedings of the 16th Conference of the Association for Machine Translation in the Americas (Volume 1: Research Track), pages 1428, Chicago, USA. Association for Machine Translation in the Americas. Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever. 2023. Robust speech recognition via large-scale weak supervision. In International conference on machine learning, pages 2849228518. PMLR. Ricardo Rei, José G. C. de Souza, Duarte Alves, Chrysoula Zerva, Ana Farinha, Taisiya Glushkova, Alon Lavie, Luisa Coheur, and André F. T. Martins. 2022. COMET-22: Unbabel-IST 2022 submission for the metrics shared task. In Proceedings of the Seventh Conference on Machine Translation (WMT), pages 578585, Abu Dhabi, United Arab Emirates (Hybrid). Association for Computational Linguistics. Supriti Sinhamahapatra and Jan Niehues. 2025. Do slides help? multi-modal context for automatic transcription of conference talks. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pages 1611116121, Stroudsburg, PA, USA. Association for Computational Linguistics. Lucia Specia, Stella Frank, Khalil Simaan, and Desmond Elliott. 2016. shared task on multimodal machine translation and crosslingual image description. In Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task Papers, pages 543553, Berlin, Germany. Association for Computational Linguistics. Gemma Team, Aishwarya Kamath, Johan Ferret, Shreya Pathak, Nino Vieillard, Ramona Merhej, Sarah Perrin, Tatiana Matejovicova, Alexandre Ramé, Morgane Rivière, Louis Rouillard, Thomas Mesnard, Geoffrey Cideron, Jean bastien Grill, Sabela Ramos, Edouard Yvinec, Michelle Casbon, Etienne Pot, Ivo Penchev, and 197 others. 2025. Gemma 3 technical report. Preprint, arXiv:2503.19786. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, and 49 others. 2023. Llama 2: Open foundation and fine-tuned chat models. arXiv [cs.CL]. Francesco Verdini, Pierfrancesco Melucci, Stefano Perna, Francesco Cariaggi, Marco Gaido, Sara Papi, Szymon Mazurek, Marek Kasztelnik, Luisa Bentivogli, Sébastien Bratières, and 1 others. 2024. How to connect speech foundation models and large language models? what matters and what does not. arXiv preprint arXiv:2409.17044. Vipin Vijayan, Braeden Bowen, Scott Grigsby, Timothy Anderson, and Jeremy Gwinnup. 2024. Adding multimodal capabilities to text-only translation model. André Viveiros, Patrick Fernandes, Saul Santos, Sonal Sannigrahi, Emmanouil Zaranis, Nuno Guerreiro, Amin Farajian, Pierre Colombo, Graham Neubig, and André Martins. 2025. TowerVision: Understanding and improving multilinguality in visionlanguage models. arXiv [cs.LG]. Changhan Wang, Anne Wu, and Juan Pino. 2020. Covost 2: massively multilingual speech-to-text translation corpus. Preprint, arXiv:2007.10310. Jin Xu, Zhifang Guo, Jinzheng He, Hangrui Hu, Ting He, Shuai Bai, Keqin Chen, Jialin Wang, Yang Fan, Kai Dang, Bin Zhang, Xiong Wang, Yunfei Chu, and Junyang Lin. 2025. Qwen2.5-omni technical report. arXiv [cs.CL]. Hanrong Ye, Chao-Han Huck Yang, Arushi Goel, Wei Huang, Ligeng Zhu, Yuanhang Su, Sean Lin, AnChieh Cheng, Zhen Wan, Jinchuan Tian, and 1 others. 2025. Omnivinci: Enhancing architecture and data for omni-modal understanding llm. arXiv preprint arXiv:2510.15870. Jun Zhan, Junqi Dai, Jiasheng Ye, Yunhua Zhou, Dong Zhang, Zhigeng Liu, Xin Zhang, Ruibin Yuan, Ge Zhang, Linyang Li, Hang Yan, Jie Fu, Tao Gui, Tianxiang Sun, Yu-Gang Jiang, and Xipeng Qiu. 2024. AnyGPT: Unified multimodal LLM with discrete sequence modeling. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 9637 9662, Bangkok, Thailand. Association for Computational Linguistics."
        },
        {
            "title": "A Appendix",
            "content": "A.1 Layer Analysis, Zero-shot ability and APE - AudioTower In our preliminary experiments, we focus exclusively on the audio modality and investigate fusing Qwen-Audio 7B (Chu et al., 2023) with TowerInstruct 7B v0.2 (Alves et al., 2024). We train the fused model for ST using the same setup described earlier, but without gating, and we independently fuse the first, middle, and last layers of Qwen-Audio to assess their individual contributions. We then compare this fused model against several baselines. First, we evaluate both the outof-the-box and fine-tuned versions of Qwen-Audio (Qwen2 Audio FT), trained end-to-end on the same ST data. We also include standard cascaded setup using off-the-shelf ASR and MT models. Additionally, we evaluate an Automatic Post-Editing (APE) 12 Model Mode Hypothesis en de en zh zh de es it ja ru fr kr Comet-22 () Qwen2 Audio Qwen2 Audio FT Qwen2 Audio + Tower Tower APE E2E E2E Cascade _ _ _ Cascade APE Qwen2 Audio + Tower 81.24 84.82 85.85 85.96 Baselines Tower-Spire Tower-Spire Seamless-M4T v2 E2E Self-Cascade E2E AudioTower-Zero AudioTower-Mid AudioTower-Last E2E E2E E2E Off-the-shelf ST models _ _ _ 82.77 85.17 85.63 E2E models _ _ _ 85.13 86.02 82.97 78.20 85.57 85.97 85. 81.63 85.73 80.05 83.97 85.03 81.03 AudioTower-Mid AudioTower-Mid Cascade APE Qwen2 Audio + Tower E2E Empty 86.34 85.77 85.99 84.79 E2E + Speech-Aware APE model 73.59 80.22 82.94 82.59 _ _ 70.22 76.62 80.62 54. 83.42 80.39 83.38 84.17 85.36 85.21 _ _ 77.78 83.98 84.86 49.57 85.96 85.27 70.70 67.69 69.26 47. _ _ 70.58 68.71 74.47 50.19 78.26 74.67 71.26 77.09 84.78 70.67 _ _ 78.91 81.64 83.26 52. 84.59 82.71 Table 4: Offline ST results on FLEURS test set. Speech-Aware APE models use the hypothesis from system and perform post-editing. Statistically significant top-performing results per language pair are highlighted in bold. Underlined langauges are not seen during the fusing process as source/target. Model Mode Hypothesis Comet-22 () en de en zh zh de es it ja ru fr kr Qwen2 Audio + Tower Whisper + Tower AudioTower-Mid AudioTower-Mid Cascade Cascade _ _ Cascade APE Qwen2 Audio + Tower Cascade APE Whisper + Tower 85.85 86.71 86.34 87. 85.97 86.31 85.99 86.34 82.94 82.23 83.42 82.81 85.36 86.54 85.96 86.60 69.26 83.56 78.26 84.38 84.78 85.29 84.59 84.95 Whisper ASR _ 3.14 3.15 25.37 2. 7.67 3.28 Character-Error-Rate () Table 5: Analysis of system combination by using Whisper to generate the hypothesis for post-editing on FLEURS test set. Statistically significant top-performing results per language pair are highlighted in bold. Underlined langauges are not seen during the fusing process as source/target. configuration in which Tower corrects translations generated by Qwen-Audio. To contextualize performance, we further compare against other off-the-shelf ST systems such as Tower-Spire (Ambilduke et al., 2025) and Seamless-M4T (Barrault et al., 2023), which provides insight into multilingual generalization. Beyond the main fused model, we also train an APE variant of our fusion architecture that post-edits the outputs of cascaded systems. Unlike traditional APE, this Speech-Aware APE allows the model to use the source audio when correcting errors, enabling more informed revisions. For both APE variants, we generate synthetic APE tuples by running Qwen-Audio on the training data and fine-tune with LoRA. We report results on the FLEURS test set in Table 4, including evaluations on zero-shot language directions not present on either the source or target side during training. First, while fine-tuning improves Qwen2 Audio, it still falls short of the outof-the-box cascaded setup across all languages, underscoring the strength of translation-centric LLMs. Tower-based APE is competitive for seen languages but struggles in harder zero-shot directions, such as frkr. Off-the-shelf end-to-end ST models lag significantly behind cascaded systems as well. Our proposed fused model (AudioTower) with middle-layer fusion achieves the strongest overall performance and even surpasses the cascaded baseline in several languages. Notably, both the first and middle layers show strong transfer to unseen languages, whereas the last layer collapses entirelysupporting the hypothesis that upper layers encode highly language-specific features, making them less suitable for cross-lingual contextual embeddings. Finally, the APE variant of the fused model delivers the best results overall by combining the strong hypotheses of the cascaded pipeline with the ability to exploit the raw audio when correcting errors. We also test this APE model with empty 13 hypotheses and observe performance comparable to AudioTower-Mid, indicating its flexibility for multiple use cases. Given the strong APE results, we additionally explore whether the model can correct outputs from different ASR systems. Using Whisper (Radford et al., 2023) for ASR, we evaluate the fused APE model on Whisper+Tower outputs and report results in Table 5. When Whisper+Tower produce the strongest cascaded baseline, using them as hypotheses likewise yields the highest APE performancehighlighting that Speech-Aware APE can be beneficial not only for E2E setups but also for offline cascaded scenarios. A.2 Prompt Building System Prompt for all modalities You are Qwen, virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech. User Prompt (ST with/without Image) Transcribe the audio using the image for context, and perform OCR on the image for correct spelling of keywords and names. User Prompt (TIT) the following sentence Translate into {tgt_lang} using the image for context. <IMG> refers to the image. Use the image to determine formality, gender, keywords, and other details important for disambiguation: {ex[src]} <IMG> 14 Algorithm 1 Online Prompt building for examples in the training data to enable alignment and selfcascading inference mode. Require: Example ex // SeedX prompt (source text, language tag and reference) 1: src, tgt, img, aud, ocr, lang EXTRACT(ex) 2: suffix LANG(lang) 3: if aud and not img then 4: if RANDOM < 0.1 then Speech only 5: 6: 7: 8: 9: inp en lbl ASR(src)+eos+suffix+tgt+eos English-Audio only in our dataset ASR Self-cascade else inp suffix lbl tgt+eos end if 10: 11: else if img and not aud then if RANDOM < 0.05 then 12: SWAP(src, tgt) suffix English 14: 13: Image only end if srcm MASK(src, prob_words = 0.2, mask_chance = 0.2) Mask 20% source words for 15: 16: 17: 20% of the time to pay attention to Image inp SRC srcm+eos+suffix lbl tgt+eos 18: 19: else if img and aud then 20: if RANDOM < 0.8 then Audio + Image 21: 22: 23: 24: 25: 26: 27: 28: 29: 30: if RANDOM < 0.1 then inp en lbl ASR(src)+eos+suffix+tgt+eos else inp suffix lbl tgt+eos end if else inp OCR lbl ocr+eos+suffix+tgt+eos OCR Self-cascade end if 31: 32: else 33: 34: end if error no modalities // Omni prompt (Multimodal prompt builder) user Transcribe with OCR and image context: img aud 35: if aud and img then 36: 37: else if aud then 38: 39: else 40: 41: end if 42: STORE(user,inp, lbl) user Transcribe audio: aud user Translate with image: img 15 Category Model Name Image XCOMET-XL Minor Major Critical Total Cascade Systems (ASR MT) Omni + SeedX End-to-End (Direct ST) OmniFusion No FT No FT LoRA FT LoRA FT Mid Fusion Mid Fusion Gated Fusion Gated Fusion Gated Fusion + Self-Cascade 89.66 89.49 89.75 90.18 88.44 90.03 88.09 89.90 90. 2172 2115 2209 2135 2126 2256 2133 2217 2191 132 133 128 118 157 104 181 106 94 68 76 67 56 74 68 87 60 2372 2324 2404 2309 2357 2428 2401 2383 2347 Table 6: Offline ST results ende on the MCIF test. indicates models using image input. Bold indicates best XCOMET-XL and lowest error counts. Category Model Name Image XCOMET-XL Minor Major Critical Total Cascade Systems (ASR MT) Omni + SeedX End-to-End (Direct ST) OmniFusion No FT No FT LoRA FT LoRA FT Mid Fusion Mid Fusion Gated Fusion Gated Fusion Gated Fusion + Self-Cascade 86.03 86.11 86.76 86.85 82.80 85.75 84.06 86.79 87.20 850 862 884 868 754 899 745 914 879 932 891 855 855 1065 903 1085 847 51 67 54 55 93 64 56 48 44 1833 1820 1793 1778 1912 1866 1886 1809 1761 Table 7: Offline ST results enit on the MCIF test. indicates models using image input. Bold indicates best XCOMET-XL and lowest error counts. Category Model Name Image XCOMET-XL Minor Major Critical Total Cascade Systems (ASR MT) Omni + SeedX End-to-End (Direct ST) OmniFusion No FT No FT LoRA FT LoRA FT Mid Fusion Mid Fusion Gated Fusion Gated Fusion Gated Fusion + Self-Cascade 81.94 82.23 82.74 82.74 79.47 82.15 79.78 82.03 82.47 615 597 624 604 462 614 480 636 1255 1254 1271 1326 1343 1240 1298 1255 1225 104 81 104 56 50 53 45 63 60 1974 1932 1999 1986 1855 1907 1823 1954 Table 8: Offline ST results enzh on the MCIF test. indicates models using image input. Bold indicates best XCOMET-XL and lowest error counts. 16 Language Code Arabic Chinese Croatian Czech Danish Dutch Finnish French German Hungarian Indonesian Italian Japanese Korean Malay Norwegian Norwegian Bokmål Polish Portuguese Romanian Russian Spanish Turkish Vietnamese ar zh hr cs da nl fi fr de hu id it ja ko ms no nb pl pt ro ru es tr vi Hyperparameter Value Gradient checkpointing Gradient checkpointing kwargs Save steps Evaluation steps Save strategy Evaluation strategy Logging steps Logging strategy Learning rate Train batch size (per device) Eval batch size (per device) Gradient accumulation steps Weight decay Save total limit Max steps bf16 Push to hub Metric for best model Remove unused columns Label names Dataloader num workers DDP find unused parameters True {use_reentrant: False} 1000 1000 steps steps 5 steps 1e-4 4 4 2 0.01 3 20000 True False loss False [\"labels\"] 1 True Table 10: Huggingface trainer hyperparameters used for the model on 4 Nvidia A100 48GB GPUs with DDP training mode. Unspecified hyperparameters are set to default Table 9: Language codes used in paper following Cheng et al. (2025). 17 Image Cascaded FT Output OmniFusion Output Reference Hallo, willkommen zu unserer Präsentation von Deplain, einem neuen Korpus für die Identifizierung deutscher Texte auf Dokumentenund Satzebene. Hallo, willkommen zu unserer Präsentation von DEPLAIN, einem neuen Korpus für die deutsche Texterkennung auf Dokumentebene und auf Satzebene. Herzlich willkommen zu unserer Präsentation von DEPLAIN, einem neuen Korpus für die deutsche Texterkennung auf Dokumentund Satzebene. Table 11: Example illustrating where OmniFusion is better than cascaded variant. Example taken from MCIF segment 1."
        }
    ],
    "affiliations": [
        "Karlsruhe Institute of Technology",
        "SAP SE"
    ]
}