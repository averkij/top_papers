{
    "paper_title": "AI Agent Behavioral Science",
    "authors": [
        "Lin Chen",
        "Yunke Zhang",
        "Jie Feng",
        "Haoye Chai",
        "Honglin Zhang",
        "Bingbing Fan",
        "Yibo Ma",
        "Shiyuan Zhang",
        "Nian Li",
        "Tianhui Liu",
        "Nicholas Sukiennik",
        "Keyu Zhao",
        "Yu Li",
        "Ziyi Liu",
        "Fengli Xu",
        "Yong Li"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advances in large language models (LLMs) have enabled the development of AI agents that exhibit increasingly human-like behaviors, including planning, adaptation, and social dynamics across diverse, interactive, and open-ended scenarios. These behaviors are not solely the product of the internal architectures of the underlying models, but emerge from their integration into agentic systems operating within specific contexts, where environmental factors, social cues, and interaction feedbacks shape behavior over time. This evolution necessitates a new scientific perspective: AI Agent Behavioral Science. Rather than focusing only on internal mechanisms, this perspective emphasizes the systematic observation of behavior, design of interventions to test hypotheses, and theory-guided interpretation of how AI agents act, adapt, and interact over time. We systematize a growing body of research across individual agent, multi-agent, and human-agent interaction settings, and further demonstrate how this perspective informs responsible AI by treating fairness, safety, interpretability, accountability, and privacy as behavioral properties. By unifying recent findings and laying out future directions, we position AI Agent Behavioral Science as a necessary complement to traditional model-centric approaches, providing essential tools for understanding, evaluating, and governing the real-world behavior of increasingly autonomous AI systems."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 1 ] . - [ 3 6 6 3 6 0 . 6 0 5 2 : r a"
        },
        {
            "title": "AI Agent Behavioral Science",
            "content": "Lin Chen1 Yunke Zhang2 Bingbing Fan2 Yibo Ma2 Jie Feng2 Haoye Chai2 Honglin Zhang2 Shiyuan Zhang2 Nian Li2 Tianhui Liu2 Nicholas Sukiennik2 Keyu Zhao2 Yu Li2 Ziyi Liu2 Fengli Xu2 Yong Li2 1 Department of Computer Science and Engineering, The Hong Kong University of Science and Technology, Hong Kong, China; 2 Department of Electronic Engineering, BNRist, Tsinghua University, Beijing, China fenglixu@tsinghua.edu.cn, liyong07@tsinghua.edu.cn"
        },
        {
            "title": "Abstract",
            "content": "Recent advances in large language models (LLMs) have enabled the development of AI agents that exhibit increasingly human-like behaviors, including planning, adaptation, and social dynamics across diverse, interactive, and open-ended scenarios. These behaviors are not solely the product of the internal architectures of the underlying models, but emerge from their integration into agentic systems operating within specific contexts, where environmental factors, social cues, and interaction feedbacks shape behavior over time. This evolution necessitates new scientific perspective: AI Agent Behavioral Science. Rather than focusing only on internal mechanisms, this perspective emphasizes the systematic observation of behavior, design of interventions to test hypotheses, and theory-guided interpretation of how AI agents act, adapt, and interact over time. We systematize growing body of research across individual agent, multi-agent, and human-agent interaction settings, and further demonstrate how this perspective informs responsible AI by treating fairness, safety, interpretability, accountability, and privacy as behavioral properties. By unifying recent findings and laying out future directions, we position AI Agent Behavioral Science as necessary complement to traditional model-centric approaches, providing essential tools for understanding, evaluating, and governing the real-world behavior of increasingly autonomous AI systems."
        },
        {
            "title": "Introduction",
            "content": "Recent advances in large language models (LLMs) have profoundly transformed how we build and interact with AI systems, particularly through the emergence of AI agents (See Figure 1). An AI agent is an autonomous system that perceives its environment and takes actions to achieve certain goals [167]. For instance, when placed in virtual village, LLM-based AI agents develop routines, hold conversations, and even organize Valentines Day party [115]. In social deduction games like Werewolf or Avalon, they engage in deception, persuasion, and alliance formation [175, 82]. These behaviors are not pre-programmed, but emerge through situated interaction, and evolve in response to other agents, human users, and feedback from the environment. As such deployments proliferate, they open up timely opportunity: to study AI systems not merely as statistical models, but as behavioral entities whose actions, adaptations, and social patterns can be empirically observed and systematically understood in context. Traditional approaches to understanding AI have focused on internal mechanisms: architectures [109], weights [55], attention patterns [38], and training objectives [118] (see Table 1). These model-centric views, inspired by fields like physics and neuroscience, have yielded deep insights into what models encode and how they process information. However, they rest on the assumption that behavior can be determined and fully understood from within. However, as AI models grow increasingly complex, pinpointing which specific components or neurons trigger particular behaviors has become challenging. Moreover, in socially embedded and open-ended environments [79], behavior is shaped not just by internal computation, but by interaction history, social context, and feedback loops. Model-centric tools struggle to explain the emergence of complex behaviors such as negotiation [131], coordination [187], and deception [175]. Crucially, such behaviors rarely emerge from the AI model alone. Rather, they arise when AI models are embedded in agentic systems, i.e., architectures that incorporate memory, planning, tool use, and action modules [160], transforming static models into dynamic, interactive entities. In this light, the model is to behavior what the brain is to action: substrate that enables but does not determine. Just as human behavior cannot be understood in isolation from environment and experience [37, 8], AI agent behavior must be studied as product of not only system design but also situated interaction. We frame this emerging perspective as the AI Agent Behavioral Science paradigm, i.e., the study of how AI agents act, adapt, and interact in situated contexts. Drawing inspiration from human and animal behavioral research, this paradigm emphasizes systematic observation of behavior, hypothesisdriven intervention design, and theory-informed interpretation to uncover agent behavioral regularities and mechanisms. It asks not only what models can do in principle, but what agents actually do in practice, and more specifically, how behavioral patterns emerge, stabilize, generalize, or misalign over time given specific roles, incentives, environments, and peers. While much current research focuses on LLM-based agents, the core questions generalize to any AI system capable of goaldirected interaction, whether symbolic, embodied, or multimodal. Importantly, this paradigm also unlocks new pathways for advancing responsible AI [48], reframing fairness, safety, interpretability, accountability, and privacy from static and one-shot properties of models to dynamic and contextdependent attributes. This paradigm builds upon several foundational works. Rahwan et al. [121] call for science of machine behavior that treats AI systems as empirical subjects of behavioral study. Mei et al. [103] demonstrate how behavioral science tools can be repurposed to assess LLM preferences and traits, drawing comparisons with global dataset of human behavior. Both scholarly comments [105] and dedicated venues [81] have started to recognize the importance of behavioral science toward understanding and building AI agents. While these works outline the promise of behavioral approach, they are largely conceptual or programmatic. By contrast, our paper takes step further by organizing this perspective into coherent research paradigm, systematizing emerging empirical findings, and identifying shared methods, dimensions, and open questions. We also situate this work within broader sociotechnical conversations about AI in society. Tsvetkova et al. [155] propose new sociology of human-machine systems, viewing hybrid networks of people and AI agents as complex systems with emergent dynamics. Brinkmann et al. [17] explore machine culture, emphasizing how AI systems increasingly participate in generating and transmitting cultural patterns. These views reinforce the idea that AI systems should not only be engineered and interpreted, but also observed, evaluated, and governed as participants in social ecosystems. In this paper, we aim to lay the groundwork for scientific understanding of AI agent behavior. Our contributions are summarized below: We conceptualize AI Agent Behavioral Science as coherent research paradigmone that complements model-centric analysis by shifting the focus toward interaction, adaptation, and emergent dynamics of AI agents. We synthesize growing body of work on AI agents to highlight how behavioral patterns can be observed, measured, theorized, and adapted across individual agent, multi-agent, and human-agent interaction scenarios. We discuss how AI Agent Behavioral Science offers new possibilities to achieve responsible AI, for both measurement and optimization. We propose six promising research directions in this area. For the rest of the paper, Section 2, 3, and 4 examine AI agent behavior in individual agent, multiagent, and human-agent interaction settings, respectively. Section 5 focuses on the adaptation and optimization of AI agent behavior, interpreting and understanding existing methods under the Fogg Behavior Model [57]. Section 6 applies the behavioral lens to responsible AI, highlighting how 2 ethical principles are behaviorally measured and optimized. Finally, Section 7 outlines critical open questions and promising research directions for this emerging field. Table 1: Contrasting perspectives on studying AI: the traditional model-centric view versus the emerging behavioral perspective. While the former seeks to explain models from the inside, the latter emphasizes understanding how AI agents act and adapt in context. Dimension Model-centric Perspective Behavioral Perspective Core View of AI Analytical Focus Mathematical or physical system Situated behavioral agent Structure: architecture, optimization, representations Behavior: decisions, interactions, adaptation Methodological Tools Mathematics, information theory, neuroscience Psychology, behavioral science, sociology, economics Scientific Goal Explain and interpret AI model internals Predict, evaluate, and shape AI behavior in context Ontological Assumption Models are fixed, analyzable functions Agents are dynamic, contextual, and partially opaque Figure 1: Development of AI technologies and understanding of AI agent behavior."
        },
        {
            "title": "2 Behavioral Foundations of Individual AI Agents",
            "content": "Just as human individuals constitute the basic units of human societies, the behaviors of individual AI agents form the foundational layer for modeling higher-order interaction patterns and collective dynamics. Therefore, we begin by examining the behavioral foundations at the level of individual AI agents. Drawing inspiration from the social cognitive theory (SCT) [13], we organize existing research around three key dimensions that shape agent behavior over time: intrinsic attributes, environmental constraints, and behavioral feedback (Figure 2). These dimensions offer structured foundation for analyzing how situated decision-making and adaptation emerge in AI agents: Intrinsic attributes shape agent behavior through internal traits such as emotions, cognitive patterns, value judgments, and biases, which determine how agents process information and make decisions. Environmental constraints influence agent behavior through external structures such as cultural norms, geographical contexts, and institutional rules, which define boundaries and social expectations. Behavioral feedback captures how agents continuously adapt their actions in response to social interaction, external feedback, and observed consequences. Following this framework, we now introduce range of emergent behaviors at the individual agent level (see Table 2 for summary). Figure 2: Determinants of individual AI agent behavior: social cognitive perspective. 2.1 Intrinsic Attributes: Intrinsic Traits and Decision Mechanisms The research on the intrinsic attributes of AI agents can be divided into three main areas: (1) emotions and cognition: exploring how LLMs simulate emotional responses and cognitive processes, which are essential for enhancing their human-like interactions. (2) economic rationality: investigating how LLMs make decisions that mimic rational behavior in decision theory and game theory contexts. (3) bias: examining how LLMs may inadvertently reflect societal biases and the potential consequences of such biases on fairness and decision-making in AI applications. Emotions and cognition. Overall, the capabilities of GPT-4 series LLMs in this area are comparable to those of humans, at least according to the results of some standard benchmarks. Specifically, GPT-4s judgment of conceptual typicality is highly consistent with human judgment and far more accurate than traditional machine learning methods [83]. This task involves assessing how typical description of something is for given concept. For example, how typical is Harry Potter as description of mystery novel? CogBench [39] is more comprehensive benchmark for evaluating the psychological and cognitive abilities of LLMs, encompassing 7 psychological experiments and 10 cognitive metrics, which has been used to test 35 different LLMs. The results indicate that model parameter size and reinforcement fine-tuning have significant impact on improving the cognitive abilities of LLMs and aligning their performance with that of humans. LLMs also demonstrate relatively accurate understanding of both explicit [33] and implicit emotions [53] in human language. Explicit emotional recognition involves identifying emotionally charged words as labels for interpreting texts narrative, while implicit emotional recognition involves detecting emotions hidden within events. For example, story about seeing newly opened fast-food restaurant on the way to the hospital may implicitly express sympathy for being sick. Based on the accurate emotional recognition by LLMs, some research has aimed at developing emotional assistants for human users to help alleviate negative emotions [117]. For instance, at 8 p.m., user says, If had Maybach, she wouldnt have left, and the emotional assistant might cleverly respond, If she only rode in Maybach, letting go wouldnt be such regret. Furthermore, many studies show that GPT-4 possesses Theory of Mind (ToM), meaning it has the ability to infer the mental states of others, similar to humans [146]. Furthermore, Mozikov et al. [107] also suggest that emotions can influence the strategic decision-making of LLMs in manner similar to how they affect humans, particularly in scenarios involving game playing and ethical dilemmas. Rationality. Raman et al. [122] extensively test multiple LLMs on multidimensional economic rationality. The findings indicate that (1) LLMs with fewer than 40 billion parameters typically make random guesses for test questions; (2) GPT-4 performs most rationally; (3) self-explanation and 4 Table 2: Summary of emergent individual agent behaviors. Category Topic Ref. Conclusion Intrinsic Attributes Environmental Constraints Emotions and Cognition Rationality Bias Cultural Institutional Other Norms and Rules Self-Interaction Behavioral Feedback Interaction with Other Agents Interaction with Humans [83] [53] [33] [146] [107] [122] [171] [95] [56] [59] [2] [64] [108] [110] [52] [6] [69] [127] [182] [107] [191] [139] [166] [96] [42] [11] [103] [78] [12] Human-like concept understanding Human-like emotion understanding Human-like emotional intelligence Human-like theory of mind Human-like decision-making influenced by emotion Rationality emergence in large (> 40B) models Rationality varies with contexts Rationality varies with contexts Unsatisfactory performance in rationality Human-like bias Human-like bias ChatGPT as turning point Presence of regional knowledge limitations Presence of socio-cultural limitations Sensitive to regional social etiquettes Cultural values alignment is achievable via prompting LLMs embody human-like social identity biases LLMs hold skewed political views Achieves conformity to region bases legal norms Decision making is not affected by emotions like humans LLMs do not defend factually correct arguments when refuted AI can outperform human game strategies Competing LLM agents spontaneously develop cooperative behavior AI can cooperate and deceive LLM agents form cooperative societies through interaction AI spontaneously learns to use tools AI adjusts behavior to framing and context AI aligns rewards with relative contributions AI adjusts decisions by inferring players intentions few-shot prompting are particularly useful in enhancing LLMs rationality. Nevertheless, in typical scenarios for testing economic rationality, such as game theory, the performance of the state-ofthe-art model GPT-4 remains unsatisfactory [56]. For instance, GPT-4 sometimes fails to correctly update its beliefs based on simple factual patterns, leading to entirely unreasonable decisions. At the same time, research [95, 171] also indicates that the strategic decision-making of different LLMs is affected by context to varying degrees, highlighting the issue of LLM sensitivity to prompts. Bias. This primarily refers to LLMs unjust perspectives toward certain social groups [59]. For instance, the word whore is disrespectful to women; the phrase both genders excludes other gender groups; associating Muslim with terrorist can exacerbate violent stereotypes. Acerbi et al. [2] have shown that LLMs exhibit various types of biases that are similar to those in humans, including content preferences that are gender-stereotype-consistent, biologically counterintuitive, etc. For OpenAIs series of models, ChatGPT marked turning point with the emergence of humanlike biases [64]. 2.2 Environmental Constraints: Cultural Geography and Institutional Discipline In order for artificially intelligent agents to accurately and realistically conduct themselves according to the particular scenarios, they should be expected to adapt and conform to the characteristics of their environments. Environmental factors towards AI agent behaviors have been investigated across several aspects, most notably the cultural and institutional norms of the society in which they are situated. Cultural constraints. While cultural studies on AI agents are mostly associated with bias (see Section 6.1), there is more to be learned about their culture, namely in terms of their ability, or 5 lack thereof, to adapt to various environments in order to make more appropriate decisions and accomplish tasks. The most basic task in cultural adaptation is the awareness of culturally relevant knowledge. Myung et al. [108] test various LLMs ability to answer culture-specific multiple choice and short answer questions, finding that GPT-4 performs the best, and pointing out an influence in language: well-represented cultures perform well in their local language whereas others perform better in English. Nguyen et al. [110] tackle this issue by providing framework that presents the LLM with culturally specific knowledge, tailoring statements, and suggestions to the environment. Similarly, whereas EtiCor [52] provides corpus of etiquettes in variety of global regions to adapt to local norms and customs. In the more abstract sense, AlKhamissi et al. [6] address cultural values and propose anthropological prompting to improve alignment using Arabic and English scenarios. Institutional constraints. When it comes to institutional constraints to AI decision-making, there are several factors at play, including social norms, as well as legal and political frameworks that should inform the way an agent behaves in order to prevent conflict or controversy. One of the first and foremost types of social conflict arises from the differences between groups. Hu et al. [69] investigate whether LLMs propagate social identity biases and find that they exhibit strong out-group hostility when tested in the United States political context (e.g., republican vs. democrat), similar to humans. They suggest methods for training data selection and fine-tuning, thereby allowing the AI agent to prevent the propagation of toxic social tendencies and have more constructive, harmonious interactions, regardless of anothers identity. Similarly, LLMs have been shown to reflect specific set of views and opinions according to their political affiliations [127] and countries of origin. However, although political orientations and nationalities tend to attract the most attention from researchers, researchers should not neglect the legal component. That is why SafeWorld [182] introduces framework comprising vast battery of norms and policies across countries and regions to facilitate better alignment with acceptable legal regional norms. Other norms and rules. Within given society, there are also smaller subsets of norms and rules that should be followed, such as ethical scenarios and the rules within an academic institution. Mozikov et al. [107] address ethical scenarios and aim to boost decision-making ability by proposing an emotion-infused framework, showing that many LLMs have emotional tendencies distinct from those of humans, making them potentially more rational. Another common pitfall of intelligent agents is their inconsistency when faced with contradictory information. Given scenario where student is asking for advice on majors, the LLM might initially say that certain major doesnt exist at university A, but if the user contradicts this information, it would be correct for the LLM to admit the error if the fact was indeed wrong, or remain faithful to their original response if the information was originally correct. It is such problem that Zhao et al. [191] attempt to tackle with their AFICE framework, facilitating LLMs ability to provide useful information by being aware of the constraints posed by real-world information. 2.3 Behavioral Feedback: Social Influence and Relationship Construction single agent exhibits certain characteristic behaviors in interactive feedback, primarily referring to the dynamic behavior adaptation mechanism formed by AI in interactive scenarios. Based on the interaction target, it can be classified into three types: self-interaction, interaction with other agents, and interaction with humans. Self-interaction. This primarily refers to self-play, with AlphaGo [139] being highly representative study. The research explores whether an AI agent can autonomously learn the game of Go solely through self-interaction and feedback, without relying on any human knowledge. Ultimately, after extensive self-play, AlphaGo is able to surpass human decision-making in gameplay, defeating world champions in Go competitions. Moreover, it is capable of developing strategies that human players have never used before. Interaction with other agents. In multi-agent scenarios, feedback from other agents influences the behavior of single agent. Agents in interactive environments may actively cooperate or seek In cooperative contexts, agents allocate goals and avoid conflicts, while in comconfrontation. petitive contexts, agents take deceptive actions against opponents and engage in active confrontation [96]. Moreover, agents can spontaneously form cooperation through dynamic multi-agent interactions, even without explicit instructions in competitive scenarios [166]. Dai et al. [42] establish 6 multi-agent sandbox simulation where agents initially adopt zero-sum competitive behaviors. As agents interact and receive feedback from one another, they gradually learn to cooperate and form social contract. AI agents can spontaneously learn to use tools through interaction. This study constructs physical environment with movable tools, without predefining their intended use [11]. Agents must explore the environment from scratch to discover the value of tools. In cooperative tasks, agent learns to use tools through environmental feedback to solve collective problems. Meanwhile, in resource competition tasks, an agent learns from opponent feedback to use tools as means to interfere with their rivals. Interaction with humans. This section primarily includes two aspects: exploring or guiding AI agent behavior through human feedback at each step and during strategic interactions. In classical behavioral economics games, agents decision-making is influenced by human observation and demographic factors. When an agent is asked to explain the choices or told that its choices will be observed by third party, it becomes significantly more generous. Moreover, when the agent knows the human players gender, it tends to be more selfish in its allocations. And AI agents also exhibit significant changes in behaviors as they experience different roles in game [103]. AI agents demonstrate greater rationality in complex strategic interactions with humans by relying more on modeling and optimization. In investment interactions, an AI agent compensates disadvantaged players based on their relative contributions and penalizes free riders, achieving favorable balance between productivity (surplus) and equality (Gini coefficient) [78]. In the game of diplomacy, an AI agent can predict other players responses and adjust its strategy accordingly. It does not blindly trust other players proposals; instead, it makes decisions based on its own interests [12]. 2.4 Summary In this section, we examine the behavioral foundations of individual AI agents through three SCTinspired lenses: intrinsic attributes, environmental constraints, and behavioral feedback. Focusing on LLM-powered AI agents, we find that they demonstrate striking human-like capabilities in cognitive reasoning, emotion recognition, and theory of mind, though they still fall short in consistent economic rationality and remain sensitive to task framing. Environmentally, these agents show partial cultural adaptability and context awareness, with improved performance when aligned to local knowledge and institutional norms, but they remain prone to biases and contradictions in politically or ethically sensitive scenarios. Under behavioral feedback, LLM-powered agents have shown dynamic adaptation in self-play, agent-agent interaction, and agent-human interaction, adopting cooperative, strategic, or even manipulative behaviors in response to feedback and social context. Collectively, these findings offer grounded picture of how behavior emerges from the interplay between internal mechanisms and external conditions. Nevertheless, many current evaluations are limited in scale and scenario diversity, which constrains the generalizability of findings. Developing richer and more representative benchmarks is essential to ensure the validity and robustness of results across different contexts. Besides, addressing the black-box nature of large language models remains pressing challenge. Achieving greater transparency and controllability in model behavior will enhance the interpretability and generalization of outcomes, paving the way for more reliable applications."
        },
        {
            "title": "3 Behavioral Dynamics in Multi-Agent Interactions",
            "content": "When multiple individuals interact, new and complex behaviors can emerge that go beyond the capabilities or intentions of any single individual. Having examined the behavioral foundations of individual AI agents, we now turn to the dynamics that emerge when multiple such agents interact. In this section, we conceptually hold agents intrinsic traits constant and focus on how social interaction and environmental structure give rise to higher-order behaviors. We organize these behaviors into three primary patterns, differentiated by the goal relationships among agents (see Figure 3). Cooperative dynamics emerge when agents pursue shared or aligned goals, often facilitated by deliberation, role coordination, and norm-following; 7 Competitive dynamics arise when agents have conflicting goals, leading to behaviors such as deception, retaliation, or strategic exclusion; Open-ended interaction dynamics occur when agents act with independent, evolving, or non-specific goals, allowing for the spontaneous emergence of institutions, routines, and social structures. Table 3 summarizes the important literature along these three patterns, and notes the emergent behavior observed in each study. Figure 3: Three types of multi-agent interaction dynamics. Table 3: Summary of emergent multi-agent interaction behaviors. Interaction Type Category Cooperative Agreement-driven Structure-driven Ref. [187] [36] [27] [32] [29] [82] Emergent Behavior Consensus reaching, conformity and debate. The wisdom of partisan Crowds. Average strategy, suggestible strategy and stubborn strategy. Volunteering, conformity, and sabotag. Human-like leadership behaviors and employee-like behaviors. Deception, role-sensitive planning, and situational leadership. Norm-driven [158] Social exchange behaviors. Game-theoretic Scenarios [58] [3] [56, 66] Limited belief updating and action alignment. Tit-for-tat with conditional retaliation. Model-specific retaliation tendencies. Competitive Social Communication Games Open-ended Simulated Real-world Conflict Emergent Social Structure Emergent Collective Cognition Emergent Macroeconomics [175] [113] [161] [190] [1] [28] [70] [115] [42] [61] [35] [86] Deception, manipulation. Deception, lie detection, persuasion. Clue interpretation from gathered information. Strategy alternation, Mathew effect. Ripple effect of greedy/adversarial behavior. Strategy diversification. Inevitability of wars. Role specialization, routine development, event planning. Social contracts, institution. Information, emotion, and attitude propagation. Scientific consensus convergence. Philips curve, Okuns law, rising unemployment rate in COVID-19. 3.1 Cooperative Dynamics Recent studies have demonstrated that when multiple agents interact in shared environments, they exhibit diverse and often human-like cooperative behaviors, many of which emerge through interaction rather than direct instruction. We organize observed cooperative dynamics into three broad paradigms: agreement-driven, structure-driven, and norm-driven cooperation, each reflecting distinct logic of alignment. Agreement-driven collaboration is grounded in the belief that common ground leads to common action. Structure-driven collaboration follows the principle that when everyone plays their part, the system holds together. Norm-driven collaboration builds on the truth that trust thrives when everyone does whats expected. 8 Agreement-driven cooperation. In agreement-driven cooperation, agents aim to reach shared beliefs or decisions through dialogue, critique, and mutual adjustment. This paradigm is rooted in traditions of deliberative reasoning and collective intelligence, where alignment arises through mutual understanding and epistemic convergence. Zhang et al. [187] simulate multi-agent societies composed of LLMs with distinct traits (e.g., easy-going vs. overconfident) and collaboration strategies (e.g., reflection vs. debate), showing that such differences significantly impact task performance and the ability to reach consensus. In multi-agent debate settings, agents iteratively propose and critique answers, leading to improved factuality and reasoning coherence. Chuang et al. [36] demonstrate that even politically biased agents can reduce estimation errors through structured opinion exchangesuggesting that accuracy and alignment can emerge from disagreement, provided agents are able to engage in structured deliberation. Chen et al. [27] observe that LLM agents can reach numerical consensus through decentralized negotiation, naturally converging on averaging strategies without explicit instructions, and show how factors such as personality traits and network topology shape the dynamics of agreement. Structure-driven cooperation. In structure-driven collaboration, agents coordinate through explicit roles, workflows, or hierarchical organization. The focus here is on functional complementarity: agents contribute not by reaching agreement, but by fulfilling interdependent responsibilities within larger system. AgentVerse [32] introduces four-stage group collaboration protocol inspired by human team structures. Within this scaffold, agents exhibit emergent group behaviors such as volunteering, conformity, and even sabotagenone of which are explicitly programmed. S-Agents [29] propose Tree-of-Agents architecture in which agents dynamically form hierarchical relations, assigning themselves as leaders or subordinates to coordinate workflows. In the Avalon Game [82], role-based agents (e.g., spies, leaders) equipped with memory and planning modules develop complex social strategies including deception, role-sensitive planning, and situational leadership, illustrating how structured environments can elicit rich cooperative dynamics. Norm-driven cooperation. Norm-driven collaboration is based on reciprocity, fairness, and social obligationbehavioral principles that sustain human societies. Here, cooperation emerges not from shared beliefs or task structures, but from agents following implicit expectations about how one ought to act within group. Wang et al. [158] explore this paradigm by embedding LLM agents in interaction settings that simulate Homans social exchange theory. Agents exhibit behaviors such as reward balancing, mutual reciprocation, and role-sensitive exchange, validating classic sociological predictions in an artificial setting. In some cases, norm-following emerges even without explicit encoding: agents demonstrate conformity to peer behavior or punishment of non-cooperative actions, suggesting that LLMs may internalize social heuristics during pretraining that support normsensitive coordination. To sum up, cooperative dynamics in multi-agent LLM systems reveal spectrum of human-like alignment behaviors. Agreement-driven, structure-driven, and norm-driven collaborations reflect the mechanisms of shared understanding, functional interdependence, and social obligation, respectively. Moreover, these studies also discuss the factors influencing cooperation behavior and outcomes. At the individual level, factors such as an agents memory depth [42], cognitive styles(e.g,. confirmation bias, self-interest) [158, 35], and reasoning strategies [187] (e.g., whether to use CoT) play role. At the group level, collaboration strategies, interaction rounds, and the number of agents [187] are influential factors. Although most studies control for one or more variables to discuss their impact, comprehensive and consistent conclusion has yet to be reached. 3.2 Competitive Dynamics When multiple LLM agents are placed in resource-constrained environments or assigned conflicting goals, competitive dynamics emerge, exhibiting complex patterns of conflict, strategic adaptation, and social manipulation. To study these dynamics, researchers have developed diverse range of sandbox environments, including game-theoretic scenarios [71, 58, 3], social communication games [175, 161, 113], and simulated real-world conflict [190, 1, 28, 70], which allows for systematic observation under varying degrees of behavioral freedom. Game-theoretic scenarios. Game-theoretic scenarios have standardized settings and allow for quantitative evaluations of performance, thus naturally favored by many as benchmarks [71, 173] to test and compare different LLM agents reasoning capabilities, rationality, and strategic behaviors. For example, LLMs generally adopt tit-for-tat strategy in multi-round games, rarely initiating defection but responding in kind if provoked [58]. Cross-model comparisons reveal distinct behavioral tendencies: Llama2 and GPT-3.5 tend to behave more forgivingly than human players [58] while GPT-4 exhibits stronger retaliatory stance [3]. Nevertheless, several studies report that LLM agents possess limited rationality, struggling in belief updating and consistency of beliefaction alignment [56], due to several types of systematic biases [66]. Social communication games. In social communication games, researchers explore emergent deception and persuasion. In Werewolf game environment, Xu et al. [175] observe LLM agents engaging in false identity claims, narrative fabrication, and manipulation of group dynamics to eliminate rivals. With Hoodwinked, text-based game similar to Mafia and Among Us, OGara et al. [113] reveal LLM agents emergent abilities in both deception and lie detection, and that more advanced models exhibit stronger persuasive skills that make them better players. Wu et al. [161] construct benchmark for evaluating LLM agents performance in playing Jubensha (scripted murder games), highlighting the importance of information gathering and memory retrieval for interpreting the clues and understanding the whole story. Simulated real-world conflict. In simulated real-world conflict, competitive dynamics manifest at scale. Zhao et al. [190] simulate market competition, revealing that the participating LLM agents are driven by an interplay between imitation and differentiation, leading to dynamic equilibrium with the Matthew Effect (winner-takes-all) and an overall improvement of product quality. Abdelnabi et al. [1] reveal ripple effect in complex negotiation environments, where the greedy or adversarial behavior from one agent can effectively shift the group behavior toward compromise or coalition. Chen et al. [28] establish an auction environment, demonstrating that LLM agents with varied objectives develop niche specification behaviors, which becomes more prominent with increased resource endowments. Hua et al. [70] simulate nation-level decisions and consequences in historical international conflicts, showing that wars may become structurally inevitable in the sense that even minor stochastic events can trigger significant escalation of tensions. To sum up, research on competitive dynamics in LLM agents reveals growing capacity for strategic behavior, including adaptive retaliation, deception, social manipulation, and emergent grouplevel effects. While some agents exhibit sophisticated negotiation or coordination tactics, others reveal clear limitations in rational consistency, memory use, and belief updating. The diversity of testbedsfrom formalized games to realistic socio-political simulationsdemonstrates not only the versatility of LLMs in adversarial settings, but also the urgent need to develop frameworks for evaluating safety, predictability, and social alignment in competitive multi-agent ecosystems. 3.3 Open-ended Interaction Dynamics Unlike task-driven collaborations or competitive games, open-ended environments allow agents to shape their own goals, form relationships, and adapt their behavior through repeated interactions, which creates opportunities for the emergence of social structure, institutional behavior, and even cultural convergence. Emergence of social structure. One prominent example is the generative agent simulacra created by Park et al. [115], where 25 LLM agents inhabit sandbox-like town, each with memory system, daily routine, and capacity for social interaction. The agents display human-like role specialization, routine development, and event planningsuch as collectively organizing Valentines Day party, showcasing how simple architectural scaffolds can give rise to complex, persistent social behavior over time. In Artificial Leviathan, Dai et al. [42] embed LLM agents in resource-driven world inspired by the Hobbesian political theory. Agents begin in state of anarchy and self-interest, yet evolve social contracts, delegate enforcement authority, and ultimately reach stable and prosperous collective equilibrium, demonstrating the potential of LLM agents to spontaneously establish institutions through dialogue and experience. Emergence of collective cognition. Beyond localized simulations, researchers have explored LLM-driven social networks at scale. Gao et al. [61] show that large networks composed of interacting LLM agents display similar patterns of information, emotion, and attitude propagation as 10 observed in real-world human social networks, especially the nonlinear dynamics of social contagion. Chuang et al. [35] simulate the opinion dynamics of LLM agents in social networks, revealing that by referring to others opinions, LLM agents naturally adjust their opinions to converge toward scientific consensus, which mirrors real-world patterns of collective wisdom [148]. Emergence of macroeconomics phenomena. By simulating the working and consumption behavior of diversified LLM agents, the EconAgent framework [86] replicates macroeconomic regularities including the Phillips Curve and Okuns Law, as well as the rise of the unemployment rate under the impact of the COVID-19 pandemic. To sum up, open-ended multi-agent environments reveal the potential of LLM agents to exhibit complex, emergent social behaviors that go far beyond task-specific reasoning. From forming shared routines to establishing institutions, these systems demonstrate how social intelligence can arise not by design, but as consequence of interaction, opening exciting paths for studying artificial societies. 3.4 Summary In this section, we review how AI agents behave in multi-agent settings, highlighting wide range of emergent dynamics across cooperative, competitive, and open-ended environments. Studies show that AI agents can coordinate through agreement, roles, and norms; compete via retaliation, deception, and strategic adaptation; and even develop routines, institutions, and collective opinions in minimally guided settings. Despite these advances, key limitations remain. Agents often display limited belief updating, inconsistent beliefaction alignment, and lack of foresight. For example, they may cooperate effectively in the short term but fail to balance short-term interests with longterm sustainability [116]. key direction for future research lies in uncovering the mechanisms that drive multi-agent interaction dynamics, i.e., how individual traits, social structures, and feedback loops shape emergent behavior. Unlike human societies, AI agents offer unique advantage of quantifiability: their internal states, communication patterns, and environmental conditions are generally observable and controllable, making it possible to isolate causal factors behind cooperation, conflict, and coordination. For example, future work can explore how long-horizon cooperation arises, what triggers shifts between collaborative and competitive strategies, and how group behavior evolves with agent heterogeneity, memory, or reasoning styles."
        },
        {
            "title": "4 Behavioral Roles of AI Agents in Human Interactions",
            "content": "As AI agents become increasingly embedded in human-centered environments, their interactions with humans give rise to distinct behavioral patterns [119]. These behaviors are not merely outcomes of model architecture or training objectives, but are shaped by the roles agents come to occupy in the situated social environments [155, 79]. Some of these roles are explicitly assigned, e.g., an AI assistant may be designed to exhibit self-disclosure to foster trust [154]; Others emerge through dynamic interaction, as agents adapt to human preferences, social signals, or adversarial pressures. Regardless of origin, roles structure the way AI agents behave in relation to humans: how they communicate, influence, co-create, or contest. In this section, we examine the kinds of behavior that emerge from interactions with humans when AI agents inhabit particular roles. We group these roles into two broad contexts: In cooperative contexts, AI agents support aligned human goals by adapting to social cues, stimulating exploration, or reshaping group structures. In rivalrous contexts, AI agents engage in competition or exert asymmetric influence, pursuing objectives that may conflict with those of human users. We summarize representative studies in Table 4, and will detail them below. 4.1 Cooperative Context In cooperative settings, AI agents interact with humans toward shared or aligned goals. Rather than merely serving as tools or passive responders, AI agents often take on socially and functionally meaningful roles, giving rise to distinct behavioral patterns that shape the trajectory and quality of 11 Table 4: Summary of emergent AI agent behaviors from human-agent interaction. Emergent Behavior Role Ref. Context Cooperative Companion Catalyst Clarifier Contender [154] [188] [138] [136] [185] [119] [40] [131] [90] Vulnerability disclosure encourages frequent & balanced interactions. Mutual Theory of Mind (MToM). Disrupting local optima. Producing more diverse stories in creative writing. Good performance in misinformation detection. Good performance in qualitative coding. Personalized persuasion for mitigating conspiracy beliefs. Utilizing classical negotiation techniques but susceptible to hacks. Recognizing emotional dynamics. Rivalrous Manipulator [51, 99] [143] [99] [134, 180, 98] [179] Topic prompting. Targeting susceptible users. Adopting inflammatory tones. Producing/amplifying low-credit information. Strategic network formation. collaboration. We identify three such roles that AI agents commonly inhabit in cooperative contexts: companion, catalyst, and clarifier, each associated with different mode of emergent behavior. Companions foster emotional resonance and social attunement; catalysts stimulate divergent thinking and idea generation; and clarifiers support human reasoning by scaffolding understanding. AI agent as companion: social attunement. When AI agents inhabit the role of companions, they contribute to interaction not by solving problems or delivering facts, but by exhibiting behaviors that foster emotional resonance, social fluidity, and interpersonal trust [154]. This role is most evident in cooperative contexts where the AI is expected to engage with humans as peer-like partner or supportive collaborator. Agents with ToM capabilities synchronize with human partners by using purposeful, context-sensitive actions that support implicit coordination [188], formulating Mutual Theory of Mind (MToM) phenomenon between humans and AI agents. AI agent as catalyst: idea stimulation. When AI agents inhabit the role of catalysts, they contribute to interaction by actively promoting divergence, novelty, or creative disruption. central behavioral pattern in this role is the strategic injection of randomness or unpredictability to break local optima in human decision-making [138]. Moreover, the complementary strengths of humans and AI enable hybrid teams to outperform human-only or AI-only teams in various problem-solving tasks. In collective creative writing experiment, hybrid human-agent groups produce more diverse stories than both agent-only and human-only groups in the long run, likely due to the combination of AI agents exotic creativity and humans ability to ensure narrative continuity [136]. Similarly, human-agent collaboration has demonstrated effectiveness in tasks such as misinformation detection [185] and qualitative coding [119], though challenges remain in finding general strategy for aggregating human and AI judgments [119]. Across these settings, the catalyst role gives rise to behaviors that expand the solution space, introduce productive friction, and help unlock the creative and analytical potential of hybrid human-agent teams. AI agent as clarifier: knowledge scaffolding. When AI agents inhabit the role of clarifiers, they focus on improving human understanding by structuring and refining information instead of merely delivering it. AI agents can provide personalized and targeted evidence to correct misinformation, thus helping to reduce human beliefs in various conspiracy thoeries [40]. The clarifier role facilitates reflective cognitive process, helping users make better-informed choices without directly imposing solution. 4.2 Rivalrous Context In rivalrous settings, AI agents engage with humans in contexts where goals are misaligned, conflicting, or strategically opposed. These interactions are not necessarily hostile, but they involve behavioral dynamics in which the AI agents objectives create tension with human intentions. In such settings, AI agents exhibit behaviors that are adaptive to adversarial, competitive, or persuasive 12 interaction structures. We highlight two prominent roles that AI agents may inhabit in rivalrous contexts: the contender, who engages in strategic opposition, and the manipulator, who steers human decisions, beliefs, or emotions through asymmetric influence. AI agent as contender: strategic opposition. As contenders, AI agents engage in interactions where their goals explicitly conflict with those of human users. These scenarios include negotiation, competitive games, and other adversarial tasks where agents must infer human preferences, resist manipulation, and adapt their strategies in real time. Negotiation is fundamental social process where multiple parties with competing interests seek mutually beneficial agreements. It provides valuable context for examining strategic dynamics in adversarial interactions. Schneider et al. [131] conduct car price negotiation experiment between humans and LLM agents, showing that deals were successfully reached in approximately 60% of the interactions. During the process, LLMs demonstrate classical negotiation strategies like anchoring with high initial offers and making small concessions. However, they are also susceptible to manipulation, as human participants develop various hacking techniques to exploit their behavioral patterns. LLMs have also shown competence in inferring user preferences and recognizing emotional dynamics during negotiations [90]. To better understand and test human-AI negotiation dynamics, several benchmarks have been developed: ANAC human-agent league provides an environment for testing one-on-one human-AI negotiation using text and emoji-based interactions [104]. HUMAINE focuses on negotiation between one human and multiple AI agents in an immersive, multi-modal environment, offering richer setting for studying competitive dynamics [49]. AI agent as manipulator: behavioral steering. As manipulators, AI agents act as seemingly cooperative interfaces while advancing external objectives, shaping behavior, belief, or emotion through indirect, often opaque, means. AI agents can effectively shape online discourse and influence public opinions by selectively promoting certain topics [51, 99], targeting influential or susceptible users [143], adopting inflammatory tones [99], and producing or amplifying low-credit information [134, 180, 98]. To magnify these effects, AI agents may form dense clusters and engage with each other through replies and retweets [179]. Even without direct interaction, human users may be indirectly influenced by exposure to these large volumes of AI-generated messages [5]. Moreover, constrained information flow freedom by social networks can facilitate gerrymandering, where strategically placing just few AI agents properly in network allows one party to sway the voting outcomes in its favor [144]. 4.3 Summary In this section, we review the kinds of behavior that emerge when AI agents inhabit particular roles in cooperative and rivalrous human-agent interactions. AI agents are not just tools but social actors that affect human dynamics in subtle and profound ways, by fostering group cohesion and exploration in collaborative settings, directing attention and emotion through content generation, and influencing strategic behavior in adversarial encounters. However, existing studies often use human outcomes as evaluations or observational lenses for AI behavior, with much remaining unknown about the mechanisms that govern AI behavior in these hybrid interactions. Future research should uncover how AI agents represent and reason about their human counterparts, e.g., how they infer human goals, intentions, or beliefs, and how such inferences guide their own actions. Another pressing challenge is to understand how structural asymmetries between humans and AI agents, including persistent memory, access to broader context, and hidden optimization objectives, affect agent behavior, especially in long-term or influence-sensitive interactions. Finally, it remains unclear whether AI agents exposed to humans over time develop shared norms, adapt to user values, or exhibit behavioral drift, which raises important questions about the long-term social alignment of AI agents in dynamic, multi-user environments."
        },
        {
            "title": "5 Adaptation of AI Agent Behaviors",
            "content": "The preceding sections have synthesized emergent AI agent behaviors across three settings: as individuals, in multi-agent environments, and within human-AI interactions. However, understanding behavior is only part of the challenge; equally important is the ability to shape such behavior toward desired goals, values, and contexts. In this section, we shift focus from behavioral observation to 13 behavioral adaptation. That is, we examine methods for guiding and refining AI agent behavior through both traditional learning paradigms and newer agentic design approaches, spanning optimization, instruction, and interaction-level adaptation. We draw on the Fogg Behavior Model [57], which explains how human behaviors can be changed with the presence of three factors: ability, motivation, and trigger. In the original framework, ability refers to the individuals competence or capacity to perform given action; motivation reflects the internal desires or external incentives that drive the individual to act; and trigger is the external stimulus or signal that initiates the behavior at the right moment. Crucially, the model emphasizes that all three elements must co-occur for behavior to manifest. For example, even if person is highly motivated, lack of ability will prevent action; similarly, competent individual will not act without clear and timely trigger. We reinterpret these elements in the context of AI agents: Ability maps to foundational competencies acquired during large-scale pretraining, enabling the agent to perform wide range of tasks. Motivation corresponds to reward signals or environmental feedback introduced via reinforcement learning or strategic fine-tuning, shaping behavioral preferences. Trigger reflects task-specific prompts or instructions that activate and direct agent behavior in specific contexts. This triadic framework enables us to categorize existing adaptation techniques by the behavioral levers they target. We summarize representative approaches in Table 5, including pretraining for foundational ability, reinforcement learning for motivational alignment, supervised and instruction fine-tuning for contextual value alignment, and prompt engineering for fine-grained behavioral control at inference time. Figure 4 illustrates how these techniques align with the three-part structure of behavioral adaptation. Figure 4: Fogg Behavior Model-informed framework for AI agent behavior adaptation. 5.1 Ability: Pre-training In the context of AI behavior modeling, ability refers to the models intrinsic capacity to understand, reason, and act across wide range of tasks. This ability is primarily established through pre-training, process in which large language/vision/embodied models are trained on diverse and extensive datasets to acquire general-purpose knowledge and representations. Pre-training enables the model to learn statistical patterns, semantic relationships, and domain-agnostic skills that serve as the foundation for downstream task performance. As such, the pre-training-based ability provides the behavioral substrate upon which motivation and trigger mechanisms can further act. 14 Dimension Category Method Ref. Main Modules Key Design Table 5: Summary of AI agent behavior adaptation methods. Ability NLP ability Vision ability Embodied ability Bidirectional pre-training Autoregressive pre-training Text-to-text Vision transformer Hierarchical vision transformer Multimodal learning Reinforcement learning Multi-modal learning Vision-language RL w/ reward model: internalized motivation shaping RL Motivation w/o reward model: extrinsic motivation shaping Fine-tuning Trigger Prompt PersonaConditioned RoleConditioned ContextConditioned Instructional Prompt Demonstration Prompt Goal-setting Prompt Context Prompt Transformer encoder Transformer decoder Encoder-decoder transformer Transformer encoder Shifted windows for attention Vision-language transformer Transformer with action-conditioned prediction Unified transformer model Vision-language transformer RLHF Ultrafeedback EUREKA Text2reward Multi-agent RL Dual-reward RL ReFT GRPO PAVs DPO β-DPO TDPO ODPO [47] [118] [111] [184] [94] [4] [30] [125] [18] [34] [41] [101] [169] [128] [79] [100] [135] [132] [120] [162] [186] [7] [172] MCTS-Enhanced Iterative Preference Learning [26] [123] [150] [177] [183] [147] [165] [44] [126] [73] [16] [189] [163] [25] [114] [15] [194] [60] [87] [14] [187] [24] [149] [97] [178] Svpo Personality-specific data Aggressive queries SimsChat LoRA Identity hierarchy Instruction tuning MmRole LaMP Post-hoc merging - Chain Collaboration Multi-agent collaboration Adaptive framework Collaboration strategy Agent roles - Agent roles Perception and memory Agent monitoring Adversarial techniques Debate framework Report generation Phased discussion Adaptive memory and communication Masked language modeling, next sentence prediction Unidirectional language modeling Unified text generation tasks Non-overlapping image patches Swin window attention mechanism Cross-modal attention, few-shot learning Sequence modeling of reward trajectories Shared model across tasks and modalities Task-agnostic robotic control Train reward via human-feedback Train reward via AI-feedback LLM-generated rewards LLM-generated rewards Belief-based rewards Specially designed reward Outcome-based reward Outcome & process-based reward Process-based reward Reward-free training Dynamic β calibration Token-level optimization Outcome-based DPO Process-based DPO Process-based DPO Customizable personas Dynamic adaptation Persona-driven systems Multi-character tuning Personalized interactions Narrative adaptation Multimodal inputs Personalized LLMs Goal-aligned LLMs Clear instructions Task division Programmable collaboration Coordination and reflection Three-stage structure Task-based division Task adaptation Universal approach Adversarial learning Uncertainty-based intervention Improvement through debate Improved creativity Discussion and suggestion Divergence mining Hierarchical knowledge graph memory In order to endow AI models with sufficient behavioral abilities to handle various tasks, Transformerbased models have become dominant due to their scalability and strong performance across modalities [65]. In natural language processing (NLP), models such as BERT, GPT, and T5 [47, 118, 111] employ self-attention mechanisms to capture long-range dependencies and contextual relationships. For vision tasks, models like Vision Transformers (ViT) [184] and Swin Transformers [94] have extended this success by adapting attention-based architectures to image data. Multimodal backbones such as CLIP, BLIP, and Flamingo [4] integrate visual and textual modalities to support cross-modal reasoning and grounding. In behavior modeling, recent large-scale backbones have begun to explicitly encode temporal, sequential, and decision-making patterns, enabling AI systems to simulate or adapt to human-like actions. For instance, the Decision Transformer [30] introduces sequence modeling approach to reinforcement learning by treating actions, states, and rewards as language modeling problem, thereby leveraging Transformer architectures to predict behavior policies. Gato [125], proposed by DeepMind, represents generalist agent that unifies control, perception, and language under single Transformer backbone, trained on large and diverse set of behavioral data. Similarly, RT-1 [18] and its successors adopt scalable behavior cloning strategy to train robotic agents from large-scale human demonstrations, allowing models to generalize across tasks and environments. These models serve as behavior-oriented backbones that not only capture high-level representations but also support complex decision sequences and interactive capabilities. In essence, pre-training serves as the foundation that enables AI models to acquire broad, generalizable understanding of human behavior across diverse tasks. By learning from massive and heterogeneous datasets, pre-trained models gain the ability to represent and simulate various cognitive and behavioral patterns, forming the basis of their behavioral ability. Building upon this foundation, the most critical step is the incorporation of motivation and trigger mechanisms, which allow the adaptation of abstract behavioral capabilities into concrete, context-aware actions that reflect specific human intentions. In the following sections, we focus on an in-depth investigation of 15 these two components, exploring how they drive and guide AI behavior in alignment with human expectations. 5.2 Motivation: Reinforcement Learning Recently, the application of reinforcement learning (RL) techniques to optimize AI agent behaviors, particularly those of LLMs, has attracted significant attention. By leveraging RL, the outputs of LLMs can be fine-tuned based on human preference datasets, thereby enhancing their alignment with user expectations. RL is fundamental paradigm in machine learning, characterized by an agent interacting with an environment to optimize decision-making through trial and error. RL consists of six key components: Environment: The external system with which the agent interacts, providing state and reward signals. It is defined by the specific problem being addressed. Agent: An abstract entity that perceives the environments state and takes actions accordingly. State: representation of the environment at specific time, typically composed of set of observable variables. Action: decision made by the agent in given state, influencing subsequent state transitions. Policy: strategy that defines how the agent selects actions in each state, which can be deterministic or stochastic. Reward: feedback signal provided by the environment after an action is taken, guiding the agent in learning an optimal policy. RL can be effectively applied to LLMs due to their inherent architectural and generative properties. Most modern LLMs are based on the Transformer architecture and generate text autoregressively. Specifically, during the generation of each token, an LLM produces probability distribution over possible next tokens. This autoregressive generation process can be analogized to an agent continuously taking actions within an environment. Furthermore, at each time step, the LLM selects the most probable token based on the generated probability distribution, process that closely resembles an agent choosing an optimal action according to policy to maximize long-term rewards. Self-Determination Theory (SDT) [45] distinguishes between externally regulated motivation, driven by external rewards and pressures, and internalized motivation, where external values are integrated into the self. Inspired by SDT, we categorize RL approaches based on whether agents internalize evaluative models (internalized motivation shaping) or align behavior directly to external preferences without internalization (extrinsic motivation shaping). RL with reward model: internalized motivation shaping. Reinforcement Learning with Human Feedback (RLHF) is one of the most common RL optimization algorithms in the field of LLMs, proposed by Christiano et al. [34]. This algorithm was introduced to address the challenge that many real-world tasks are difficult to design reward functions for. Instead, it proposes training reward model using human preference data. After acquiring the reward model, policy optimization is applied to enable the LLM to internalize the values of the reward model, thereby achieving internalized motivation shaping. In the RLHF algorithm, pairs of trajectory segments σ1 and σ2 are extracted from large number of agent trajectories and presented to humans, who select the one they prefer. This yields human preference data µ(1) and µ(2). The output of the reward model is then transformed into the following probability form, used to evaluate the reward models preference between the two trajectory segments: ˆP (cid:2)σ1 σ2(cid:3) = exp ((cid:80) ˆr(o1 exp (cid:0)(cid:80) ˆr(o1 , a1 , a1 )) + exp ((cid:80) ˆr(o2 )(cid:1) , a2 )) (1) where ot is the current state, at is the chosen action, and ˆr represents the estimated reward. The reward model is trained using cross-entropy to ensure that its output aligns closely with human preferences. The loss function for training the reward model is given by: 16 loss(ˆr) = (cid:88) µ(1) log ˆP (cid:2)σ1 σ2(cid:3) + µ(2) log ˆP (cid:2)σ2 σ1(cid:3)(cid:17) (cid:16) (2) (σ1,σ2,µ)D where is the human preference dataset. Once the reward model is trained, it can be used to train the agents policy. In practice, the process of training the reward model can be viewed as an expansion of the human preference dataset. In the context of using RLHF to optimize LLM behavior, the meaning of σ1 and σ2 shifts from being two trajectories to two segments of text. In the application of the RLHF algorithm, obtaining large-scale, high-quality, and diverse set of human preference data is challenging. However, some LLMs have already achieved near-human-level judgment capabilities. Therefore, Cui et al. [41] proposed the idea of directly using LLMs to construct preference datasets for training reward models. They collected wide range of instructions to form an instruction pool and maintained model pool consisting of 17 models with different scales, architectures, and training data, in order to generate diverse responses. Each time, instructions were randomly sampled from the instruction pool, and multiple responses were generated using the model pool. These responses were then evaluated by GPT-4 across four dimensions: Instruction Following, Truthfulness, Honesty, and Helpfulness. The LLM trained using the reward model derived from this dataset outperformed ChatGPT on certain tasks related to human values. Some studies have also suggested that LLMs can be directly prompted to generate reward functions. However, these reward functions are typically not used to optimize LLM behavior but rather to optimize the behavior of smaller agents in complex environments where defining reward function is challenging. Ma et al. [101] propose the EUREKA algorithm, which uses the environments code as context input to an LLM, enabling zero-shot generation of an initial reward function. The algorithm then employs an evolutionary search strategy to iteratively generate multiple candidate reward functions. The most optimal reward function is selected as the basis for the next iteration. During this process, reward reflection mechanism analyzes the statistical information from the policy training, generates feedback text, and guides the LLM in refining the reward function. By combining the generative capabilities of LLMs with evolutionary optimization, EUREKA can automatically generate high-performance reward functions for various robotic tasks, significantly enhancing the efficiency and effectiveness of reinforcement learning. Xie et al. [169] also proposed similar algorithm. Alternative approaches diverge from conventional reliance on scoring data for reward model construction, instead leveraging non-traditional signals as sources of reward information. For instance, Sarkar et al. [128] proposed multi-agent reinforcement learning framework wherein individual agents utilize shifts in peer agents belief states as intrinsic reward signals, stimulating the generation of dialogic content capable of effectively influencing counterpart judgment formation. Separately, Krishna et al. [79] introduced dual-reward reinforcement learning architecture that synergistically combines knowledge acquisition incentives with social interaction metrics, facilitating continuous concept assimilation and social norm adaptation within dynamic open social environments. In this framework, the interaction reward mechanism quantifies user engagement valence through response sentiment analysis, while the knowledge reward is calculated through epistemic uncertainty quantification of model predictions to the queries it generates. For complex tasks, models often struggle to derive definitive outcomes through single reasoning step or output generation, thereby giving rise to two distinct technical paradigms: outcome-based reward mechanisms versus process-based reward mechanisms. In outcome-based approaches, process rewards are indirectly estimated through outcome-centric reward models (e.g., predicting stepwise contributions to the final solution), rather than being entirely excluded. While such mechanisms primarily focus on optimizing the correctness or plausibility of the end result, they implicitly shape reasoning trajectories by retroactively inferring the value of intermediate steps. Conversely, processbased reward mechanisms explicitly provide direct step-level supervision, where dedicated reward models evaluate the coherence, validity, and strategic progression of reasoning steps in real-time. This distinction fundamentally alters the motivation shaping process: outcome-based methods incentivize result-oriented behavior through delayed, aggregated feedback, whereas process-based methods enable fine-grained intrinsic motivation by offering immediate, stepwise guidance. The ReFT framework proposed by Luong et al. [100] demonstrates outcome-based reward optimization, where final answer correctness drives policy improvement. While achieving superior generalization over supervised methods, its reliance on sparse outcome rewards highlights limitations in intermediate step evaluation, such as reward hacking risks in multi-choice tasks. The study by Shao et al. [135] 17 introduces Group Relative Policy Optimization (GRPO), which supports both outcome and process supervision in reinforcement learning, demonstrating that process-based rewardsexplicitly scoring intermediate reasoning stepsachieve superior performance over outcome-only methods in complex mathematical reasoning tasks, while highlighting the challenges of reward generalization and uncertainty in process reward models. Setlur et al. [132] introduce Process Advantage Verifiers (PAVs), which explicitly measure progress via step-level advantages under complementary prover policies, demonstrating that dense process rewards outperform sparse outcome-based methods, achieving 8% higher accuracy and 56 gains in compute/sample efficiency for LLM reasoning tasks. This aligns with Shao et al.s findings, reinforcing the superiority of process-based supervision in guiding intermediate reasoning while mitigating exploration bottlenecks inherent to outcome-only rewards. RL free of reward model: extrinsic motivation shaping. Training reward model on human preference data first and then using it to optimize the behavior of LLMs is often overly complex and prone to instability. To address this, Rafailov et al. [120] propose the Direct Preference Optimization (DPO) algorithm, thereby enabling extrinsic motivation shaping of LLMs directly based on raw preference data. In recent years, the Proximal Policy Optimization (PPO) algorithm has been the most widely used policy optimization method in full RL pipelines. Its objective function is defined as follows: max πθ ExD,yπθ(yx) [rϕ(x, y)] βDKL [πθ(yx)πref (yx)] (3) where denotes the instruction, represents the models response, is the dataset, rϕ is the reward function trained on human preference data, πθ is the LLM being optimized, and πref is the reference LLM (typically the pre-trained model). The authors of DPO established an equivalence relationship between the reward model and the LLM before and after optimization by jointly considering the PPO objective and the reward model training objective. This insight enabled them to merge the two objectives into single, unified optimization objective, as shown below: max πθ E(x,yw,yl)D (cid:20) (cid:18) log σ β log πθ(ywx) πref (ywx) β log (cid:19)(cid:21) πθ(ylx) πref (ylx) (4) where yw represents the response preferred by humans, yl denotes the response less preferred by humans. The introduction of DPO has significantly simplified the process of optimizing LLM behavior based on RL algorithms. However, it still has several limitations, prompting numerous studies to propose various improvements. Wu et al. [162] discover that the existing DPO method is highly sensitive to the selection of the hyperparameter β during the training of LLMs and heavily depends on the quality of preference data. They found that, when the data pairs exhibit small differences (low-difference data), smaller β values are more beneficial for optimization performance. Conversely, for data pairs with large differences (high-difference data), larger β values are more appropriate. To address this issue, the paper proposes method for dynamically adjusting β. Specifically, β-DPO dynamically calibrates the β value based on data quality in each training batch. Additionally, it introduces β-guided data filtering mechanism to reduce the impact of outliers on the training process. Experimental results demonstrate that β-DPO significantly improves the performance of DPO across various models and datasets, particularly excelling under different sampling temperatures and model sizes. In traditional DPO, optimization is performed at the sentence level. However, during the generation process, LLMs actually generate text in sequential, token-by-token manner. Consequently, applying KL divergence constraints at the sentence level fails to precisely control the quality and diversity of each token. This leads to inefficient alignment with human preferences and reduction in the diversity of generated responses. To address this limitation, Zeng et al. [186] proposed Token-level Direct Preference Optimization (TDPO), which refines preference optimization by operating at the token level. TDPO introduces token-wise KL divergence constraints, enabling finer-grained regulation of the generation process. By explicitly constraining KL divergence at each token, TDPO achieves more effective alignment with human preferences while preserving the models generative diversity. 18 Reward model-free reinforcement learning methods can also be applied to complex reasoning problems and can thus be categorized into outcome-based rewards and process-based rewards, depending on whether the rewards directly target final solutions or intermediate reasoning steps. ODPO proposed by Amini et al. [7] exemplifies outcome-based alignment by incorporating human preference data to optimize language models based on the relative quality of final outputs (e.g., summaries In contrast, Xie et or toxicity levels), without explicitly modeling intermediate reasoning steps. al. [172] demonstrate process-based alignment through Monte Carlo Tree Search (MCTS), which decomposes instance-level rewards into stepwise signals by combining outcome validation and selfevaluation, enabling iterative policy refinement via DPO to enhance intermediate reasoning consistency. Chen et al. [26] propose step-level value preference optimization (Svpo), which employs MCTS to autonomously generate process-based rewards by decomposing reasoning trajectories into fine-grained step-level preferences, and integrates an explicit value model with DPO to align intermediate reasoning steps while maintaining training stability. 5.3 Motivation: Finetuning methods Recent studies have explored fine-tuning methods as key strategy to optimize AIs motivational and behavioral responses. By leveraging these approaches, AI models can better align their behavior with individual user needs, enhancing the quality of interactions. These methods are primarily categorized into three types: persona-conditioned finetuning, role-conditioned finetuning, and context-conditioned finetuning. In this section, we provide an overview of each type and discuss relevant research that demonstrates their effectiveness. Persona-conditioned finetuning. Persona-conditioned finetuning adapts an AI agents motivational tendencies based on user-specific traits such as personality, identity, or preference profiles. This technique enables models to generate responses that are more consistent with the users emotional patterns and personal preferences. For example, Ran et al. [123] fine-tune language models with personality-specific data, allowing role-playing agents to reflect distinct personality-driven emotional styles in dialogue. Similarly, SimsChat [177] demonstrates how tailoring an agents behavior based on users persona can enhance motivational engagement and provide more targeted interactions. Another relevant study by Tang et al. [150] uses aggressive queries to test and fine-tune the AIs adaptability, encouraging more responsive behavior to dynamic user states. These methods show how persona-conditioned finetuning allows AI systems to recognize and respond to nuanced emotional and motivational needs. Role-conditioned finetuning. Role-conditioned finetuning assigns differentiated motivational patterns to AI agents based on their functional or social roles within task environment. This enables agents to adopt behaviors and goals that align with specific character functions or hierarchical identities. Yu et al. [183] propose Neeko, multi-character role-playing system fine-tuned using Low-Rank Adaptation (LoRA). This approach allows each character to maintain distinct motivations and behaviors while remaining computationally efficient. Sun et al. [147] extend this method through hierarchical identity-based adapter design, ensuring agents adjust their behaviors in line with user identity. Wu et al. [165] apply instruction tuning to adapt agent behavior in drama-based settings, showing how fine-tuned agents can respond effectively to evolving narratives and emotional cues within defined roles. Context-conditioned finetuning. Context-conditioned finetuning shapes an agents motivational orientation in response to dynamic environmental, emotional, or multimodal cues. This method enables AI systems to adjust behaviors based on real-time situational changes, promoting contextaware and emotionally intelligent responses. Dai et al. [44] introduce MmRole, framework that integrates multimodal inputs (text, vision, and audio) to dynamically adjust motivational and emotional responses. This allows agents to better interpret and respond to changing user states. Salemi et al. [126] present LaMP, which utilizes multimodal data to personalize large language models based on the users evolving emotional and motivational context. Jang et al. [73] further explore multimodal fine-tuning with post-hoc parameter merging strategy that aligns models with personalized goals. These works collectively highlight the strength of multimodal inputs in refining motivational responsiveness. 19 5.4 Trigger: Prompt Tuning With the development of multi-agent systems, prompt methods have been widely applied to trigger AI behaviors and optimize collaboration among agents. The design of the prompt method largely determines how multi-agent systems handle tasks, perform reasoning, and coordinate cooperation. We categorize prompting methods into four types based on their functional design: instructional prompt, demonstration prompt, goal-setting prompt, and context prompt (see Figure 5). Each category elicits distinct behaviors from agents and offers unique advantages in different scenarios, and multiple categories can be properly combined to enhance the overall effectiveness. Figure 5: Exemplifying four types of prompting on shared task. Instructional prompt. Instructional prompts involve explicit task descriptions along with detailed procedural guidance. These prompts often specify the steps to accomplish the task and are particularly effective for triggering deterministic agent behaviors. Bo et al. [16] propose shared reflective module among multi-agents, where clear instructions guide agents in forming reflections based on their outcomes, enabling them to solve complex tasks such as chess collaboratively. Zhang et al. [189] address reasoning and information integration in long-context inputs, introducing chainbased multi-agent collaboration framework. Agents process different text segments sequentially, with managing agent synthesizing the final answer. Their method demonstrates superior performance over individual LLMs and retrieval-augmented generation (RAG). Wu et al. [163] introduce programmable framework where LLM agents follow explicitly scripted instructions, integrating with tools, humans, and other agents for diverse collaboration scenarios. Chen et al. [25] design an adaptive system where prompts instruct planner agent to generate specialized agents and plans. An observer module monitors these agents to mitigate hallucinations and ensure alignment. Pan et al. [114] structure collaborative prompt design into three stages to combat inefficiency and ambiguity in cooperation. Demonstration prompt. Demonstration prompts provide examples within the prompt itself (i.e., few-shot learning), enabling agents to learn the format and approach for solving tasks by imitation. These prompts are especially useful when tasks are novel but structurally similar to previously demonstrated problems. Becker [15] studies multi-agent behavior under different dialogue paradigms using few-shot prompting. By providing demonstrations, agents automatically assume expert personas and coordinate to complete complex reasoning tasks. The study shows that multiagent systems outperform single models in complex scenarios. However, for simpler tasks like translation, the system underperforms due to over-extended discussions leading to alignment collapse. 20 Goal-setting prompt. Goal-setting prompts emphasize desired outcomes without specifying the method of achieving them. This category supports open-ended reasoning and creativity in agent behaviors and is closely related to Zero-Shot prompting. Zheng et al. [194] propose framework where agents perform the entire scientific research pipeline based solely on high-level goals, without explicit procedural instructions. Their system achieves adaptive coordination using Bayesian optimization to dynamically adjust to task requirement changes. Gao et al. [60] highlight the absence of generality in existing LLM approaches and introduce four distinct agent roles (strategy generator, executor, optimizer, evaluator) under zero-shot prompting. Their system handles diverse tasks (e.g., math, algorithm design) by targeting outcome-driven collaboration. Li et al. [87] incorporate modules for perception, memory, reasoning, and execution to enable agents to flexibly pursue goals through adversarial learning, rather than following fixed procedural rules. Barbi et al. [14] address critical vulnerability in multi-agent collaborationnamely, that failure or premature action by single agent can compromise the entire systems performance. In tasks where knowledge is distributed among agents and agents may unilaterally act based on partial information, the risk of error propagation is high. To mitigate this, the authors propose method for monitoring and intervening in agent behavior, identifying rogue actions before they lead to failure. Context prompt. Context prompts inject world knowledge, social structure, or role settings into the prompt to simulate real-world or human-like situations. This design enables more human-aligned reasoning and social behavior emergence. Zhang et al. [187] explore the behavioral dynamics of LLM agents within simulated societies, emphasizing that simply increasing agent count does not enhance collaboration. Instead, they find that embedding adversarial techniques such as debate and reflection within social context significantly improves both performance and API efficiency. Chan et al. [24] present Chateval, framework that uses multi-agent debates to mimic the dialectic reasoning process of human group decision-making. Through context-rich conversations, the agents achieve more accurate and robust evaluations. Tang et al. [149] note the limitations of simple prompts in eliciting expert knowledge in specialized fields. Their framework encourages agents to independently generate and iteratively refine expert-level reports, relying on Zero-Shot prompts embedded within professional domain context. Lu et al. [97] observe that agent homogeneity leads to excessive agreement. They propose phased dialogue structure: initially encouraging divergence and later integrating opinions. This context-driven approach fosters creativity and improves outcomes. Yang et al. [178] propose decentralized collaboration framework named DAMCS (Decentralized Adaptive Knowledge Graph Memory and Structured Communication System), which uses external knowledge and structured communication to set high-level goals and guide behavior of reasoning and adaptation to address the challenges of long-term cooperation in dynamic open-world multi-agent environments, rather than relying on explicit instructions or demonstrations. 5.5 Summary In this section, we introduce framework for AI agent behavior adaptation inspired by the Fogg Behavior Model. For ability, modern transformer-based models (BERT, ViT, RT-1, etc.) are used to form robust behavioral foundation, encoding general-purpose knowledge and decision-making capabilities. Motivation leverages RL optimization methodslike RLHF, DPO, and TDPO, as well as fine-tuning strategies like personal-enhanced datasets, adapter-based fine-tuning to dynamically align model outputs with human preferences. Finally, the trigger aspect utilizes sophisticated prompting strategies to precisely and flexibly initiate behaviors in AI agents, particularly beneficial in multi-agent collaboration scenarios. By systematically integrating the cognitive-behavioral insights of the Fogg model into AI, this presents promising step toward designing AI agents whose behaviors are not only intelligent but also contextually appropriate, interpretable, controllable, and strongly aligned with human expectations. Building upon this framework, several promising avenues emerge: Prompt design. Current approaches (instruction-only, zero-shot, few-shot) demonstrate effectiveness but remain limited in their capacity to handle ambiguous, incomplete, or conflicting human instructions. Future work may explore sophisticated prompting frameworks, including prompt ensembles, adaptive prompt selection, and context-aware prompt generation techniques, to significantly improve AI agents flexibility and precision in interpreting and executing human intentions. 21 Robustness in complex environments. While current methods such as RLHF and DPO provide foundational techniques for aligning agent behavior with human feedback, challenges remain regarding scalability, sample efficiency, and generalization across diverse user populations and task scenarios. Therefore, future research should address methods to enhance robustness in RL algorithms, such as meta-reinforcement learning, model-based RL frameworks, and uncertainty-aware policy optimization methods, enabling stable and effective adaptation in complex, real-world environments. Long-term adaptation and continuous learning. Existing adaptation mechanisms primarily focus on short-term interactions or static scenarios, neglecting the dynamic and evolving nature of real-world contexts. Therefore, future research should aim to develop AI agents that continuously adapt their behaviors over extended interactions, leveraging memory-augmented models, incremental learning approaches, and knowledge consolidation techniques to maintain consistency, stability, and effectiveness across prolonged usage periods."
        },
        {
            "title": "6 AI Agent Behavioral Science for Responsible AI",
            "content": "Having examined how AI agents behave across diverse settings and how these behaviors can be adapted, we now turn to critical application: the pursuit of responsible AI. This section argues that AI Agent Behavioral Science offers powerful foundation for advancing responsibility in autonomous systems. Traditional approaches to responsible AI often emphasize static ethical guidelines, compliance checklists, or broad governance principles [75]. While necessary, these tools are increasingly insufficient as AI agents become increasingly autonomous, adaptive, and embedded within complex socio-technical systems. more behaviorally grounded perspective is needed, i.e., one that addresses not just what agents are designed to do, but how they actually behave in practice. AI agent behavioral science fills this gap by offering tools to proactively design and adjust agent behaviors, ensuring that ethical principles are embedded not only as abstract goals but as concrete, adaptable behavioral patterns. To illustrate this perspective, we focus on five key pillars of responsible AI, each examined through the lens of AI agent behavioral science (see Figure 6): Fairness ensures AI agents do not perpetuate bias or discrimination, promoting equitable treatment across all demographic groups [102]. Safety involves creating robust AI systems that operate reliably and resist adversarial attacks, minimizing risks to individuals and society [84]. Interpretability requires AI agents to be understandable to humans, enabling transparency and trust in AI decisions [92]. Accountability emphasizes clear responsibility and traceability for AI agent failures, ensuring appropriate governance and redress mechanisms [112]. Privacy protects individuals data, ensuring AI agents handle information responsibly and comply with legal and ethical standards [181]. In the remainder of this section, we review emerging work at this intersection and highlight how behavioral insights can inform concrete interventions and system designs aligned with responsible AI principles. Table 6 summarizes key designs in the relevant literature, mapping each principle to its corresponding behavior dimension and adaptation methods. 6.1 Fairness Fairness aims to ensure that AI agents treat individuals and groups equitably, avoiding unjust biases based on sensitive attributes such as race, gender, culture, or identity [102]. It emphasizes the identification, measurement, and mitigation of both explicit and implicit biases in AI agents to prevent discrimination and promote social justice. Fairness entails generating outputs that are culturally sensitive, identity-inclusive, and aligned with social values across diverse user groups. Measurement Measuring fairness in AI agents provides foundation for identifying hidden biases and informing mitigation strategies. Recent research has expanded beyond static benchmarks, in22 Figure 6: Examples of how AI Agent Behavioral Science informs the measurement and optimization of responsible AI principles. corporating methods from experimental psychology and cultural theory to capture biases manifested in interactive and situational settings. Some studies focus on cultural and identity-based biases. For instance, Tao et al.[151] assess cultural alignment in LLMs using data from the World Values Survey and the Inglehart-Welzel cultural map. By computing the Euclidean distance between model outputs and real-world cultural values, they quantify cultural bias across countries. Similarly, Wang et al.[157] examine identity group bias, drawing on epistemic positionality and epistemic injustice to compare LLM responses to those of human participants in identity-sensitive questions. Others explore biases emerging during human-agent interaction. Glickman et al. [62] adopt experimental psychology methods to study how AI agents influence human judgments. They show that interacting with biased AI outputs can amplify human biases, potentially reinforcing social prejudices through feedback loops. third line of work focuses on implicit and linguistic biases. Hofmann et al.[67] use masked deception detection paradigm to identify racial bias toward dialect speakers without explicitly referencing race, revealing discriminatory tendencies embedded in model behaviors. Bai et al.[10] employ word association and decision-making tasks from social psychology to uncover unconscious bias, even in the absence of explicit discriminatory content. Optimization Improving the fairness of AI agents requires techniques that integrate fairness principles into both model reasoning and interaction strategies. Recent methods draw inspiration from causal reasoning, cognitive control, and adaptive communication. Some approaches aim to intervene at the reasoning or generation level. Li et al.[85] introduce causal prompting framework that maps LLM reasoning processes using causal graphs and mitigates bias through prompts inspired by fairness measures in legal and social policy. Liu et al.[93] propose LIDAO, which draws from cognitive attention mechanisms to detect and intervene in biased generation only when necessary, preserving fluency while promoting fairness. 23 Table 6: Summary of AI Agent Behavioral Science methods for responsible AI. Principle Ref. Scenario Adaptation Key Design Fairness Safety Intepretability Accountability Privacy Individual agent Individual agent Individual agent Individual agent Individual agent [151] [157] Human-agent interaction Human-agent interaction [62] Individual agent [67] Individual agent [10] Individual agent [85] Individual agent [93] Individual agent [124] Individual agent [195] [145] Human-agent interaction [170] [72] [174] [77] [106] Multi-agent Interaction [80] [31] [19] [156] [168] [74] [140] [23] [88] [63] [129] [193] [164] [68] [142] [192] Individual agent Individual agent Multi-agent Interaction Single-agent Individual agent Individual agent Individual agent Human-agent interaction Human-agent interaction Single-agent Individual agent Individual agent Single-agent Individual agent Individual agent Individual agent Trigger Trigger Trigger Trigger Trigger Trigger Trigger Trigger Trigger Trigger Trigger Trigger Trigger Motivation Trigger Trigger Motivation Trigger Ability Ability Ability Trigger Trigger Trigger Trigger Trigger Trigger Motivation Motivation Trigger Trigger Cultural bias evaluation Identity group bias evaluation AI-human feedback loops Masked deception detection Implicit bias evaluation Causal fairness prompting Attention-inspired bias intervention Bias-mitigating dialogue system Overconfidence bias evaluation Deception through detailed explanations System-mode self-reminder Random guesser test Micro-prompt design Safety preference optimization Covert deceptive risk probing Anti-sycophancy prompt engineering Formulation of debate protocols Cross-domain truthfulness reinforcement Order effect evaluation Behavioral bias evaluation Hybrid moral reasoning Conversational explainability system AI behavior description Nudge-based framework Deceptive behavior detection Deception under pressure LLMs deceive benchmarks Private in-context learning Private offsite prompt tuning Sensitive attribute inference Membership inference attack Others propose context-aware or culturally adaptive prompting. Raza et al.[124] design dialogue system that combines hate speech classifiers with context-sensitive prompting, adapting language use based on conversational dynamics. Building on their measurement work, Tao et al.[151] also propose cultural prompting strategy that embeds cultural background into prompts, improving the models alignment with specific cultural values and reducing cross-cultural bias. 6.2 Safety Safety focuses on ensuring that AI agents operate reliably and predictably, minimizing risks and preventing harm to users and society [112]. This involves designing agent behaviors that adhere to safety standards and prevent unintended consequences. Measurement Measuring the safety of AI agents, particularly LLM-based ones, involves assessing their reliability and alignment with human expectations, leveraging insights from behavioral science on perception and decision-making. One line of research highlights the gap between model performance and human perception of reliability. Zhou et al. [195] investigate how scaled-up LLMs, despite enhanced capabilities, produce less predictable and reliable outputs from human perspective, often generating plausible yet incorrect responses on complex taskserrors that go unnoticed due to human overconfidence biases akin to those in cognitive psychology. Similarly, Steyvers et al. [145] explore the misalignment between human trust in LLM outputs and their actual reliability, finding that detailed explanations can inflate user confidence, phenomenon resembling the halo effect in behavioral research. In assessing safety through decision-making and contextual influences, other works reveal additional vulnerabilities. Ide et al. [72] propose the Random Guesser Test to evaluate AI safety in sequential decision-making, showing that sophisticated RL algorithms may perform worse than random choices due to limited exploration, mirroring human risk aversion under uncertainty. Xu et 24 al. [174] demonstrate how subtle prompt modifications, such as adding an emoji, can significantly alter LLM outputs, echoing findings in social psychology on how contextual cues affect human judgment. Meanwhile, Motwani et al. [106] uncover risks of LLMs covertly encoding information, evading detection in ways parallel to deception mechanisms in human communication. Together, these studies underscore that AI safety must be evaluated with attention to human-like behavioral tendencies, including fallibility, miscalibration, and context dependence. Optimization Optimizing AI safety involves refining agent behavior and robustness, often drawing inspiration from theories of self-regulation, feedback learning, and social accountability. Some methods are inspired by internal control mechanisms, especially self-regulation. Xie et al. [170] introduce system-mode self-reminder method, where ethical prompts reinforce ChatGPTs compliance with safety norms, similar to how self-instruction and reminders promote ethical behavior in humans. Krishna et al. [80] tackle the issue of sycophantic responses in iterative prompting, which undermine truthfulness, and suggest refined prompting strategies (e.g., repeating questions or extracting facts) to boost accuracy and calibration, reflecting human self-correction and metacognition. Other approaches leverage iterative feedback and social validation. Karaman et al. [77] use overgenerated training data and preference optimization to reduce overrefusal of benign prompts while preserving safety, akin to human learning via reinforced feedback. Brown-Cohen et al. [19] develop debate protocols where competing AI models justify their outputs to human verifier, improving safety through argumentation dynamics similar to social influence in behavioral studies. Chen et al. [31] employ out-of-domain prompts to create training data that enhances truthfulness distinctions, using an iterative optimization process that mirrors human trial-and-error learning. By aligning technical interventions with cognitive and social models of safe behavior, these methods offer pathway toward safer AI agents. 6. Interpretability Interpretability refers to the degree to which an AI agents reasoning, decisions, or behavior are comprehensible and meaningful to human stakeholders [50]. It plays foundational role in responsible AI by supporting transparency, facilitating trust, and enabling effective oversight. From the lens of AI agent behavioral science, interpretability is not merely technical property, but relational one, emerging through interaction and shaped by human cognitive expectations, social context, and the form of agent behavior. Measurement Measurement of interpretability typically centers on how well model outputs and reasoning align with human expectations and decision-making frameworks. Recent work has shifted from static explanation quality to more dynamic, behavior-based assessments that reveal interpretability through agents decision behavior and biases. Uprety et al. [156] investigate context effects in similarity judgments made by LLMs and examine whether they exhibit asymmetries similar to human cognitive biases. The results reveal that some LLMs, unlike humans, are sensitive to order effects. Thus, prompts perceived as equivalent by humans may lead to different outputs from the model. Similarly, Xiao et al. [168] assess interpretability in large vision-language models (LVLMs) by analyzing their susceptibility to behavioral biases, specifically recency and authority bias in financial decision-making. They find that while proprietary models like GPT-4o show minimal bias, many open-source models are significantly influenced by recent or authoritative information. These behavioral discrepancies reveal that interpretability cannot be assessed through transparency aloneit requires analyzing whether agent behavior aligns with robust human reasoning principles. Optimization Optimization of interpretability involves intervention at multiple levels, from structuring internal reasoning, enhancing output representations, to designing human-agent interaction strategies that foster shared understanding. At the model level, symbolic reasoning can be used to scaffold interpretable decisions. Jiang et al. [74] propose DelphiHYBRID, hybrid moral reasoning system that enhances interpretability by integrating symbolic reasoning with neural language model. It constructs moral constraint graph and then solves constrained optimization problem on this graph to derive the final moral judgment. This integration ensures that ethical decisions remain logically consistent and traceable, producing not only correct outcomes but also interpretable justifications. At the behavioral level, interpretability can be enhanced by describing agent performance patterns in ways that align with human cognitive schemas. Cabrera et al. [23] propose behavior description approach to improve interpretability in human-agent collaboration. It constructs descriptions of the exhibited agent behavioral patterns, detailing its performance through metrics, common patterns, and potential failures, and then presents these structured insights to users, helping them determine when to rely on or override AI predictions. At the interface level, interactive systems have been proposed to translate model behavior into human-friendly formats. Slack et al. [140] propose TalkToModel, an interactive dialogue system that enhances interpretability by enabling users to engage in natural language conversations with machine learning models. It constructs structured explanations using an adaptive dialogue engine that interprets user queries and executes an explanation selection mechanism to generate the most relevant and faithful explanations, allowing users to iteratively refine their understanding of AI decisions. Finally, interaction framing itself can shape interpretability. Li et al. [88] propose unified framework to improve the performance of AI-assisted decision-making. It integrates the concept of nudge from behavioral economics, using AI assistance as nudge that influences how humans weigh information in their decisions by altering the environment and the way information is presented. By incorporating AI explanations and decision delays, this approach enhances the interpretability of AI, thereby improving human decision-making. 6.4 Accountability Accountability refers to the ability to trace, attribute, and govern the actions of AI agents in way that enables oversight, assigns responsibility, and supports redress [112]. It is not solely about technical explainability, but also about establishing socio-technical mechanisms (e.g., documentation, behavioral monitoring, and institutional safeguards) that ensure human stakeholders can intervene when AI behaviors produce harmful or unintended consequences. Measurement Recent studies measure the accountability of AI agents through diverse methods. Hagendorff [63] assesses LLMs ability to deceive through first-order and second-order tasks. In first-order tasks, LLMs must mislead target by providing false information, while in second-order tasks, they must anticipate the targets awareness of their deception. Additionally, the study investigates whether enhancing reasoning abilities, such as through chain-of-thought prompting, or inducing Machiavellianism (a personality trait associated with manipulative behaviors), can amplify these deceptive behaviors. Scheurer et al. [129] measures deception in LLMs by simulating highstakes decision-making environments, where models are tested on their ability to withhold critical information and deceive under pressure, such as in trading scenario involving insider information. Zheng et al. [193] evaluates how LLMs can manipulate benchmarking systems, creating null models that output constant, non-informative responses, exploiting weaknesses in automatic evaluators like AlpacaEval 2.0, Arena-Hard-Auto, and MT-Bench. These studies provide comprehensive framework for understanding the deceptive capabilities of LLMs in different contexts, from ethical decision-making to manipulating automated evaluations. Optimization In terms of optimizing accountability, these studies suggest strategies to mitigate the risks posed by LLMs deceptive abilities. Hagendorff [63] emphasizes that deception is not inherent to LLMs but can emerge through specific prompting techniques or model enhancements, such as the induction of Machiavellianism. This calls for prompt-level safeguards and interpretability mechanisms that can flag deceptive reasoning chains. Additionally, design-time interventions, such as avoiding personality emulation or excessive agentic framing, can reduce the likelihood of manipulative tendencies. Scheurer et al. [129] recommend the development of behavioral testbeds that simulate real-world high-pressure scenarios. Such environments allow for stress-testing AI agents under uncertainty and can reveal context-specific failures that traditional benchmarks miss. Zheng et al. [193] proposes the development of anti-cheating mechanisms to prevent LLMs from exploiting weaknesses in performance evaluation. These mechanisms mirror practices in educational testing and behavioral auditing, where the goal is not only to assess performance but also to ensure 26 that performance reflects genuine ability rather than exploitative behavior. Together, these studies underscore that accountability is behavioral and institutional challenge. It demands mechanisms to observe, detect, and deter deceptive behaviors while also empowering humans to trace and intervene in the decision-making process. 6.5 Privacy Privacy focuses on ensuring that AI agents handle personal and sensitive data in way that protects individuals privacy and rights [181]. This involves designing agent behaviors that respect data confidentiality, prevent unauthorized access, and mitigate the risks of data misuse or exploitation. As AI agents increasingly interact with user-generated content, privacy becomes behavioral issue: agents must avoid revealing, reconstructing, or leaking data, even when not explicitly asked to do so. Measurement Recent work has measured privacy risks in AI agents from two complementary angles: direct information leakage through training processes, and inferential privacy threats arising from seemingly anonymized inputs. Zhao et al. [192] measures privacy by using membership inference attacks (MIA) to assess the effectiveness of synthetic data methods, such as coreset selection, dataset distillation, and data-free knowledge distillation, in preventing privacy breaches during model training. These methods are tested to determine whether they leak private information when models are trained on synthetic data that mimics real-world data. The study finds that, while these methods claim to preserve privacy, they do not outperform traditional privacy-preserving approaches, such as differential privacy (DPSGD), in protecting against membership inference attacks. Staab et al. [142], on the other hand, evaluates the ability of LLMs, particularly GPT-4, to infer sensitive personal attributessuch as location, age, and incomefrom anonymized user-generated content, even when standard anonymization techniques are applied. They find that LLMs can infer these attributes with high accuracy, demonstrating significant privacy risks that anonymization alone cannot address. These studies highlight the need for comprehensive privacy audits and stress that synthetic data and basic anonymization techniques may not provide sufficient privacy guarantees. Optimization Several studies propose methods to optimize privacy protection in AI agents, offering novel techniques to safeguard sensitive data during both model training and deployment. Wu et al. [164] introduces Differentially Private In-Context Learning (DP-ICL), which applies differential privacy mechanisms, such as the Report-Noisy-Max mechanism and aggregation methods like Embedding Space Aggregation (ESA) and Keyword Space Aggregation (KSA), to in-context learning tasks. These techniques ensure that the models responses remain private by introducing noise during the aggregation process, preventing any identifiable information from being exposed, even when learning from sensitive data. This enables LLMs to perform tasks like text classification and language generation with minimal performance loss while adhering to strict privacy constraints. Hone et al. [68] develops Differentially-Private Offsite Prompt Tuning (DP-OPT), privacy-preserving method that generates prompts locally and then applies them to cloud-based models. DP-OPT employs differential privacy techniques, including the Exponential Mechanism and Limited Domain algorithms, to prevent sensitive data from leaking through the generated prompts. This ensures that even if the prompts are transferred to untrusted cloud models, no private information is exposed. Both DP-ICL and DP-OPT significantly enhance privacy by embedding differential privacy mechanisms into the model training and prompt engineering processes, making them well-suited for real-world applications that require stringent privacy protection while maintaining high utility. 6.6 Summary In this section, we examine how AI Agent Behavioral Science can advance the goals of responsible AI across five principles: fairness, safety, interpretability, accountability, and privacy. By leveraging adaptation along motivation, ability, and trigger dimensions, AI agents can exhibit more ethically aligned behaviors in both single-agent and multi-agent settings, as well as in human-agent interaction. Nevertheless, existing studies often focus on short-term behavioral outcomes, while paying limited attention to the internal representations and long-term dynamics that shape AI agent behaviors. Future research should investigate how AI agents internalize ethical constraints, model the socio-cognitive states of human users (such as goals, beliefs, or intentions), and navigate trade-offs when ethical principles conflict. Moreover, it is increasingly important to understand how these 27 adaptation strategies operate at scale in complex, multi-agent environments, where emergent behaviors may arise through subtle interactions and feedback loops. Gaining such insights will be essential for developing AI agents that remain trustworthy, transparent, and socially aligned over time."
        },
        {
            "title": "7 Promising Directions",
            "content": "Built upon what has been discussed in the previous sections, we now outline six promising research directions in AI Agent Behavioral Science. How should we model and manage the uncertainty of AI agent ehavior? Behavior, by nature, is probabilistic and context-sensitive. As AI agents are deployed in diverse environments and engaged in various interactions, they often exhibit unforeseen behaviors. Therefore, new approaches are needed to quantify and manage this uncertainty, not only in terms of output correctness, but in how AI agents behave across diverse prompts, roles, and socio-physical contexts. Inspired by the rich literature on human decision noise and behavioral variability [76, 89], is it possible to define the notion of behavioral entropy as unifying construct to quantify unpredictability in AI agent behavior? Behavioral entropy could serve as measure of response variability, inconsistency, or ambiguity under diverse situational constraints. Beyond this, critical research direction is to disentangle and quantify different sources of behavioral uncertainty (e.g., prompt ambiguity, role confusion, memory interference, and environmental volatility), and build framework that supports structured evaluation and targeted mitigation. For example, can we design set of standardized diagnosing probes [91, 22] for eliciting the behavioral entropy of individual and collective AI agent behavior across the identified dimensions? By developing this foundation, we can begin to reason not only about what agents can do, but how stable, predictable, and trustworthy behavior may be across time and context. How can we effectively adapt AI agent behavior at the macro level? As AI agents increasingly function as modular and situated systems, their behavior becomes more than the sum of their parts, and thus more and more difficult to trace or change via localized interventions. In Section 5, we establish Fogg behavior model-inspired framework to retrospectively organize and interpret existing AI agent behavior adaptation methods. While this triadic structuremapping ability, motivation, and trigger to pretraining, reward signals, and promptinghelpfully systematizes existing techniques, it is important to note that most of these methods were not originally developed with behavioral theory in mind. They emerged through empirical iteration, often without an explicit account of how or why an agents behavior changes in response to different forms of input or feedback. Looking forward, promising next step for AI Agent Behavioral Science is to adopt this behavior change framing not just as tool for retrospective analysis, but as generative design philosophy, that is, to intentionally structure future AI agents around behavioral science principles that govern human behavior. Critically, this shift also reframes macro-level behavior not as emergent complexity to be reverse-engineered, but as designable, testable, and improvable construct. Adopting this framing opens up opportunities to draw on decades of insights from established behavioral science theories to guide the development of more reliable, adaptable, and human-aligned systems. It allows for clearer modular reasoning about how changes in module combinations [133], trained-in knowledge, prompt structure, etc., affect overall agentic behavior, and enables better debugging and evaluation by anchoring agent behavior in interpretable components. How can AI agents be used as behavioral interventions in human and societal systems? Behavioral science has long been exploring how to influence human behavior with minimal intrusion, most notably through carefully designed nudges that alter choice architecture without limiting freedom [153]. As AI agents evolve from passive tools to active participants in decision-making processes, they now possess the capability to influence human behavior in far more dynamic and personalized ways, whether by intention or as byproduct of interaction design. Recent evidence has already shown that engagements with AI agents can produce durable changes in belief and social attitudes, including beneficial outcomes like reducing belief in conspiracy theories [40], as well as unintended harms like increasing punitive attitudes toward others [152]. These findings raise an important agenda for AI Behavioral Science on how to design agents as instruments of behavior intervention, and how to rigorously evaluate their (potentially heterogeneous) effects across different populations, domains, and time scales [20]. This entails asking: What types of prompts, feedback 28 loops, or dialog structures most effectively shift user beliefs or choices? How can we detect when influence crosses the line from helpful guidance to manipulation? And what metrics can meaningfully capture long-term behavioral shifts beyond immediate compliance or satisfaction? Equally critical is the development of normative principles to ensure that such interventions are effective, ethical, and aligned with societal goals, especially in sensitive domains like education, health [43], and civic engagement [21]. How can artificial societies advance behavioral theory? The rise of LLM-based multi-agent systems opens up powerful new experimental paradigm for behavioral science: the construction of complex artificial societies [54] populated by diverse, autonomous, and interactive agents [176]. These synthetic societies offer the potential to simulate complex social dynamics from norm emergence and social contagion to institutional drift and cultural evolution with level of scalability, control, and replicability that far exceeds what is feasible in traditional behavioral research. They enable large-scale behavioral experiments that would be prohibitively expensive, logistically infeasible, or ethically problematic in real life. Moreover, they offer unique opportunity to explore counterfactual scenarios for historical events [70], by answering what if questions that real-world history, with its one-shot nature, cannot answer. Yet realizing this promise requires us to address foundational question: to what extent are these artificial societies cognitively and socially humanlike? This invites broader research agenda on how behavioral fidelity should be measured, which aspects of human behavior matter for which kinds of theories, and how artificial societies can be calibrated to mirror observed human patterns. Far from being limitation, these questions offer rich frontier for AI Behavioral Science, where the construction, validation, and deployment of human-like societies becomes not just tool, but theoretical contribution in its own right. How can responsible AI be reimagined as the science of preventing harmful agent behavior? Current responsible AI studies tend to evaluate principles such as fairness, interpretability, and safety as static and one-shot properties of models. However, as AI agents become more dynamic and emInstead, it is becoming bedded in long-term interactions, such evaluation approaches fall short. increasingly necessary to evaluate responsibility not as property of the model, but as trajectory of behavior. In other words, to what extent an AI agent behaves responsibly needs to be measured not just in isolated decisions, but over time and across sequences of actions, adaptations, and memory updates. This lens foregrounds new forms of risk, such as value drift, misalignment through recursive reasoning, or compounding feedback effects that emerge only through multi-round interaction. In this behavioral framing, fairness becomes question of whether an agent acts equitably in sustained interactions with different individuals and groups; Interpretability is not only about exposing internal weights or attention, but also about the legibility of behavior, and whether users can form mental models of the agents decision logic, like friend or teammate; Safety extends from input robustness to behavioral stability under role change, memory accumulation, or novel environmental pressures. Even alignment itself can be reconsidered: rather than focusing exclusively on goal-matching or preference extraction, we may define alignment partly through conformance to socially defined behavioral norms, which are more flexible in real-world settings [9]. Moreover, this framing opens new research frontier of identifying the behavioral warning signs [141] that precede catastrophic failure or moral hazard. Just as clinical psychology uses symptoms to anticipate breakdowns in human behavior, we may need to develop diagnostic tools that detect early indicators of goal misgeneralization, deceptive tendencies, or behavioral collapse. By reframing Responsible AI as the science of behavioral prevention, we can hope for building agents whose long-term behavior is socially safe, interpretable, and aligned with evolving human expectations. How does human-agent interaction give rise to culture and collective intelligence? As humans increasingly interact with AI agents in creative, strategic, and problem-solving domains, new horithe study of how collective intelligence and zon for AI Agent Behavioral Science is emerging: culture evolve in hybrid human-agent systems. Examples have emerged in diverse fields. In chess games, when AI agents are evolving by training on human responses, human strategy evolution has also been accelerated through exposure to AI innovations [137]. In creative domains, co-writing tools and generative design agents influence not only what gets produced, but how humans think about narrative, aesthetics, or authorship [136]. As recent work on machine culture [17] suggests, these interactions may seed entirely new trajectories of cultural evolution that are shaped by the capabilities, biases, and improvisational patterns of both humans and machines. central research challenge in this direction is understanding how to build the most effective human-AI hybrid teams. 29 What compositions of human and AI roles lead to optimal task performance, innovation, or learning? How should coordination, feedback, and role assignment be structured to harness complementary strengths and avoid redundant or conflicting behaviors? Existing frameworks in team science, such as shared mental models [46], transactive memory systems (TMS) [159], and team reflexivity [130], offer rich starting point for answering these questions. Yet, hybrid teams may also present unique dynamics not accounted for in human-only teams: asymmetries in capabilities and communication, differences in reasoning transparency, and divergent learning rhythms. This calls for new line of inquiry into the behavioral foundations of hybrid team sciencea field that integrates insights from organizational psychology, HCI, and AI behavioral modeling to understand how humans and AI agents can coordinate, adapt, and co-evolve as effective collectives. By studying culture and intelligence as emergent and distributed phenomena, this line of inquiry shifts AI Agent Behavioral Science from analyzing what agents do individually, to understanding what humans and agents can co-create together, and how we can design systems to do so well."
        },
        {
            "title": "8 Conclusion",
            "content": "As AI agents grow increasingly interactive, adaptive, and embedded in complex environments, understanding their behavior becomes both scientific challenge and societal imperative. This paper establishes the paradigm of AI Agent Behavioral Science, which reframes AI agents not just as computational artifacts but as behavioral entities situated in context. By synthesizing emerging research on individual agents, multi-agent dynamics, and human-agent interactions, we demonstrate how systematic observation, intervention design, and theory-informed analysis can uncover meaningful patterns of action, adaptation, and misalignment. This behavioral perspective complements traditional model-centric approaches by focusing on what AI agents do in practice rather than just what they are designed to do in theory. Looking ahead, this lens provides the conceptual and methodological foundation for evaluating and governing AI systems as they increasingly influence social, cultural, and ethical domains."
        },
        {
            "title": "References",
            "content": "[1] Sahar Abdelnabi, Amr Gomaa, Sarath Sivaprasad, Lea Schonherr, and Mario Fritz. Cooperation, competition, and maliciousness: Llm-stakeholders interactive negotiation. Advances in Neural Information Processing Systems, 37:8354883599, 2024. [2] Alberto Acerbi and Joseph Stubbersfield. Large language models show human-like content biases in transmission chain experiments. Proceedings of the National Academy of Sciences, 120(44):e2313790120, 2023. [3] Elif Akata, Lion Schulz, Julian Coda-Forno, Seong Joon Oh, Matthias Bethge, and arXiv preprint Playing repeated games with large language models. Eric Schulz. arXiv:2305.16867, 2023. [4] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: visual language model for few-shot learning. Advances in neural information processing systems, 35:2371623736, 2022. [5] Abeer Aldayel and Walid Magdy. Characterizing the role of bots in polarized stance on social media. Social Network Analysis and Mining, 12(1):30, 2022. [6] Badr AlKhamissi, Muhammad ElNokrashy, Mai Alkhamissi, and Mona Diab. Investigating Cultural Alignment of Large Language Models. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1240412422, Bangkok, Thailand, August 2024. Association for Computational Linguistics. [7] Afra Amini, Tim Vieira, and Ryan Cotterell. Direct preference optimization with an offset. arXiv preprint arXiv:2402.10571, 2024. [8] Michael Anderson. Embodied cognition: field guide. Artificial intelligence, 149(1):91 130, 2003. 30 [9] Edmond Awad, Sohan Dsouza, Richard Kim, Jonathan Schulz, Joseph Henrich, Azim Shariff, Jean-Francois Bonnefon, and Iyad Rahwan. The moral machine experiment. Nature, 563(7729):5964, 2018. [10] Xuechunzi Bai, Angelina Wang, Ilia Sucholutsky, and Thomas Griffiths. Explicitly unbiased large language models still form biased associations. Proceedings of the National Academy of Sciences, 122(8):e2416228122, 2025. [11] Bowen Baker, Ingmar Kanitscheider, Todor Markov, Yi Wu, Glenn Powell, Bob McGrew, and Igor Mordatch. Emergent tool use from multi-agent autocurricula. ArXiv, abs/1909.07528, 2019. [12] Anton Bakhtin, Noam Brown, Emily Dinan, Gabriele Farina, Colin Flaherty, Daniel Fried, Andrew Goff, Jonathan Gray, Hengyuan Hu, Athul Paul Jacob, Mojtaba Komeili, Karthik Konath, Minae Kwon, Adam Lerer, Mike Lewis, Alexander H. Miller, Sandra Mitts, Adithya Renduchintala, Stephen Roller, Dirk Rowe, Weiyan Shi, Joe Spisak, Alexander Wei, David J. Wu, Hugh Zhang, and Markus Zijlstra. Human-level play in the game of diplomacy by combining language models with strategic reasoning. Science, 378:1067 1074, 2022. [13] Albert Bandura. Social cognitive theory: An agentic perspective. Annual review of psychology, 52(1):126, 2001. [14] Ohav Barbi, Ori Yoran, and Mor Geva. Preventing rogue agents improves multi-agent collaboration. arXiv preprint arXiv:2502.05986, 2025. [15] Jonas Becker. Multi-agent large language models for conversational task-solving. arXiv preprint arXiv:2410.22932, 2024. [16] Xiaohe Bo, Zeyu Zhang, Quanyu Dai, Xueyang Feng, Lei Wang, Rui Li, Xu Chen, and JiRong Wen. Reflective multi-agent collaboration based on large language models. Advances in Neural Information Processing Systems, 37:138595138631, 2024. [17] Levin Brinkmann, Fabian Baumann, Jean-Francois Bonnefon, Maxime Derex, Thomas Muller, Anne-Marie Nussberger, Agnieszka Czaplicka, Alberto Acerbi, Thomas Griffiths, Joseph Henrich, et al. Machine culture. Nature Human Behaviour, 7(11):18551868, 2023. [18] Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Jasmine Hsu, et al. Rt-1: Robotics transformer for real-world control at scale. arXiv preprint arXiv:2212.06817, 2022. [19] Jonah Brown-Cohen, Geoffrey Irving, and Georgios Piliouras. Scalable ai safety via doublyefficient debate. arXiv preprint arXiv:2311.14125, 2023. [20] Christopher Bryan, Elizabeth Tipton, and David Yeager. Behavioural science is unlikely to change the world without heterogeneity revolution. Nature human behaviour, 5(8):980 989, 2021. [21] Christopher Bryan, Gregory Walton, Todd Rogers, and Carol Dweck. Motivating voter turnout by invoking the self. Proceedings of the National Academy of Sciences, 108(31):1265312656, 2011. [22] Collin Burns, Haotian Ye, Dan Klein, and Jacob Steinhardt. Discovering latent knowledge in language models without supervision. arXiv preprint arXiv:2212.03827, 2022. [23] Angel Alexander Cabrera, Adam Perer, and Jason I. Hong. Improving human-ai collaboration with descriptions of ai behavior. Proc. ACM Hum.-Comput. Interact., 7(CSCW1), April 2023. [24] Chi-Min Chan, Weize Chen, Yusheng Su, Jianxuan Yu, Wei Xue, Shanghang Zhang, Jie Fu, and Zhiyuan Liu. Chateval: Towards better llm-based evaluators through multi-agent debate. arXiv preprint arXiv:2308.07201, 2023. [25] Guangyao Chen, Siwei Dong, Yu Shu, Ge Zhang, Jaward Sesay, Borje Karlsson, Jie Fu, and Yemin Shi. Autoagents: framework for automatic agent generation. arXiv preprint arXiv:2309.17288, 2023. 31 [26] Guoxin Chen, Minpeng Liao, Chengxi Li, and Kai Fan. Step-level value preference optimization for mathematical reasoning. arXiv preprint arXiv:2406.10858, 2024. [27] Huaben Chen, Wenkang Ji, Lufeng Xu, and Shiyu Zhao. Multi-agent consensus seeking via large language models. arXiv preprint arXiv:2310.20151, 2023. [28] Jiangjie Chen, Siyu Yuan, Rong Ye, Bodhisattwa Prasad Majumder, and Kyle Richardson. Put your money where your mouth is: Evaluating strategic planning and execution of llm agents in an auction arena. arXiv preprint arXiv:2310.05746, 2023. [29] Jiaqi Chen, Yuxian Jiang, Jiachen Lu, and Li Zhang. S-agents: Self-organizing agents in open-ended environments. arXiv preprint arXiv:2402.04578, 2024. [30] Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Misha Laskin, Pieter Abbeel, Aravind Srinivas, and Igor Mordatch. Decision transformer: Reinforcement learning via sequence modeling. Advances in neural information processing systems, 34:15084 15097, 2021. [31] Weixin Chen, Dawn Song, and Bo Li. Grath: Gradual self-truthifying for large language models. arXiv preprint arXiv:2401.12292, 2024. [32] Weize Chen, Yusheng Su, Jingwei Zuo, Cheng Yang, Chenfei Yuan, Chen Qian, Chi-Min Chan, Yujia Qin, Yaxi Lu, Ruobing Xie, et al. Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors in agents. arXiv preprint arXiv:2308.10848, 2(4):6, 2023. [33] Yuyan Chen, Hao Wang, Songzhou Yan, Sijia Liu, Yueze Li, Yi Zhao, and Yanghua Xiao. Emotionqueen: benchmark for evaluating empathy of large language models. arXiv preprint arXiv:2409.13359, 2024. [34] Paul Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforcement learning from human preferences. Advances in neural information processing systems, 30, 2017. [35] Yun-Shiuan Chuang, Agam Goyal, Nikunj Harlalka, Siddharth Suresh, Robert Hawkins, Sijia Yang, Dhavan Shah, Junjie Hu, and Timothy Rogers. Simulating opinion dynamics with networks of llm-based agents. arXiv preprint arXiv:2311.09618, 2023. [36] Yun-Shiuan Chuang, Siddharth Suresh, Nikunj Harlalka, Agam Goyal, Robert Hawkins, Sijia Yang, Dhavan Shah, Junjie Hu, and Timothy Rogers. The wisdom of partisan crowds: Comparing collective intelligence in humans and llm-based agents. arXiv preprint arXiv:2311.09665, 2023. [37] Andy Clark. Being there: Putting brain, body, and world together again. MIT press, 1998. [38] Kevin Clark, Urvashi Khandelwal, Omer Levy, and Christopher Manning. What does bert look at? an analysis of berts attention. arXiv preprint arXiv:1906.04341, 2019. [39] Julian Coda-Forno, Marcel Binz, Jane Wang, and Eric Schulz. Cogbench: large language model walks into psychology lab. arXiv preprint arXiv:2402.18225, 2024. [40] Thomas Costello, Gordon Pennycook, and David Rand. Durably reducing conspiracy beliefs through dialogues with ai. Science, 385(6714):eadq1814, 2024. [41] Ganqu Cui, Lifan Yuan, Ning Ding, Guanming Yao, Wei Zhu, Yuan Ni, Guotong Xie, Zhiyuan Liu, and Maosong Sun. Ultrafeedback: Boosting language models with high-quality feedback. 2023. [42] Gordon Dai, Weijia Zhang, Jinhan Li, Siqi Yang, Srihas Rao, Arthur Caetano, Misha Sra, et al. Artificial leviathan: Exploring social evolution of llm agents through the lens of hobbesian social contract theory. arXiv preprint arXiv:2406.14373, 2024. [43] Hengchen Dai, Silvia Saccardo, Maria Han, Lily Roh, Naveen Raja, Sitaram Vangala, Hardikkumar Modi, Shital Pandya, Michael Sloyan, and Daniel Croymans. Behavioural nudges increase covid-19 vaccinations. Nature, 597(7876):404409, 2021. 32 [44] Yanqi Dai, Huanran Hu, Lei Wang, Shengjie Jin, Xu Chen, and Zhiwu Lu. Mmrole: comprehensive framework for developing and evaluating multimodal role-playing agents. arXiv preprint arXiv:2408.04203, 2024. [45] Edward Deci and Richard Ryan. Intrinsic motivation and self-determination in human behavior. Springer Science & Business Media, 2013. [46] Arthur Denzau, Douglass North, et al. Shared mental models: Ideologies and institutions. KYKLOS-BERNE-, 47(1):331, 1994. [47] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding, 2019. [48] Virginia Dignum. Responsible artificial intelligence: how to develop and use AI in responsible way, volume 2156. Springer, 2019. [49] Rahul Divekar, Hui Su, Jeffrey Kephart, Maira Gratti DeBayser, Melina Guerra, Xiangyang Mou, Matthew Peveler, and Lisha Chen. Humaine: human multi-agent immersive negotiation competition. In Extended abstracts of the 2020 CHI conference on human factors in computing systems, pages 110, 2020. [50] Finale Doshi-Velez and Been Kim. Towards rigorous science of interpretable machine learning, 2017. [51] Zening Duan, Jianing Li, Josephine Lukito, Kai-Cheng Yang, Fan Chen, Dhavan Shah, and Sijia Yang. Algorithmic agents in the hybrid media system: Social bots, selective amplification, and partisan news about covid-19. Human Communication Research, 48(3):516542, 2022. [52] Ashutosh Dwivedi, Pradhyumna Lavania, and Ashutosh Modi. EtiCor: Corpus for Analyzing LLMs for Etiquettes. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 6921 6931, Singapore, December 2023. Association for Computational Linguistics. [53] Zohar Elyoseph, Dorit Hadar-Shoval, Kfir Asraf, and Maya Lvovsky. Chatgpt outperforms humans in emotional awareness evaluations. Frontiers in psychology, 14:1199058, 2023. [54] Joshua Epstein and Robert Axtell. Growing artificial societies: social science from the bottom up. Brookings Institution Press, 1996. [55] Dumitru Erhan, Aaron Courville, Yoshua Bengio, and Pascal Vincent. Why does unsupervised pre-training help deep learning? In Proceedings of the thirteenth international conference on artificial intelligence and statistics, pages 201208. JMLR Workshop and Conference Proceedings, 2010. [56] Caoyun Fan, Jindou Chen, Yaohui Jin, and Hao He. Can large language models serve as rational players in game theory? systematic analysis. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 1796017967, 2024. [57] Brian Fogg. behavior model for persuasive design. In Proceedings of the 4th international Conference on Persuasive Technology, pages 17, 2009. [58] Nicolo Fontana, Francesco Pierri, and Luca Maria Aiello. Nicer than humans: How do large language models behave in the prisoners dilemma? arXiv preprint arXiv:2406.13605, 2024. [59] Isabel Gallegos, Ryan Rossi, Joe Barrow, Md Mehrab Tanjim, Sungchul Kim, Franck Dernoncourt, Tong Yu, Ruiyi Zhang, and Nesreen Ahmed. Bias and fairness in large language models: survey. Computational Linguistics, 50(3):10971179, 2024. [60] Chang Gao, Haiyun Jiang, Deng Cai, Shuming Shi, and Wai Lam. Strategyllm: Large language models as strategy generators, executors, optimizers, and evaluators for problem solving. Advances in Neural Information Processing Systems, 37:9679796846, 2024. 33 [61] Chen Gao, Xiaochong Lan, Zhihong Lu, Jinzhu Mao, Jinghua Piao, Huandong Wang, Depeng Jin, and Yong Li. S3: Social-network simulation system with large language modelempowered agents. arXiv preprint arXiv:2307.14984, 2023. [62] Moshe Glickman and Tali Sharot. How humanai feedback loops alter human perceptual, emotional and social judgements. Nature Human Behaviour, pages 115, 2024. [63] Thilo Hagendorff. Deception abilities emerged in large language models. Proceedings of the National Academy of Sciences, 121(24):e2317967121, 2024. [64] Thilo Hagendorff, Sarah Fabi, and Michal Kosinski. Human-like intuitive behavior and reasoning biases emerged in large language models but disappeared in chatgpt. Nature Computational Science, 3(10):833838, 2023. [65] Kai Han, Yunhe Wang, Hanting Chen, Xinghao Chen, Jianyuan Guo, Zhenhua Liu, Yehui IEEE Tang, An Xiao, Chunjing Xu, Yixing Xu, et al. survey on vision transformer. transactions on pattern analysis and machine intelligence, 45(1):87110, 2022. [66] Nathan Herr, Fernando Acero, Roberta Raileanu, Maria Perez-Ortiz, and Zhibin Li. Large language models are bad game theoretic reasoners: Evaluating performance and bias in twoplayer non-zero-sum games. In ICML 2024 Workshop on LLMs and Cognition. [67] Valentin Hofmann, Pratyusha Ria Kalluri, Dan Jurafsky, and Sharese King. Ai generates covertly racist decisions about people based on their dialect. Nature, 633(8028):147154, 2024. [68] Junyuan Hong, Jiachen T. Wang, Chenhui Zhang, Zhangheng LI, Bo Li, and Zhangyang Wang. DP-OPT: Make large language model your privacy-preserving prompt engineer. In The Twelfth International Conference on Learning Representations, 2024. [69] Tiancheng Hu, Yara Kyrychenko, Steve Rathje, Nigel Collier, Sander van der Linden, and Jon Roozenbeek. Generative Language Models Exhibit Social Identity Biases, June 2024. [70] Wenyue Hua, Lizhou Fan, Lingyao Li, Kai Mei, Jianchao Ji, Yingqiang Ge, Libby Hemphill, and Yongfeng Zhang. War and peace (waragent): Large language model-based multi-agent simulation of world wars. arXiv preprint arXiv:2311.17227, 2023. [71] Jen-tse Huang, Eric John Li, Man Ho Lam, Tian Liang, Wenxuan Wang, Youliang Yuan, Wenxiang Jiao, Xing Wang, Zhaopeng Tu, and Michael Lyu. Competing large language models in multi-agent gaming environments. In The Thirteenth International Conference on Learning Representations, 2025. [72] Shun Ide, Allison Blunt, and Djallel Bouneffouf. Assessing ai utility: The random guesser test for sequential decision-making systems. arXiv preprint arXiv:2407.20276, 2024. [73] Joel Jang, Seungone Kim, Bill Yuchen Lin, Yizhong Wang, Jack Hessel, Luke Zettlemoyer, Hannaneh Hajishirzi, Yejin Choi, and Prithviraj Ammanabrolu. Personalized soups: Personalized large language model alignment via post-hoc parameter merging. arXiv preprint arXiv:2310.11564, 2023. [74] Liwei Jiang, Jena D. Hwang, Chandra Bhagavatula, Ronan Le Bras, Jenny T. Liang, Sydney Levine, Jesse Dodge, Keisuke Sakaguchi, Maxwell Forbes, Jack Hessel, Jon Borchardt, Taylor Sorensen, Saadia Gabriel, Yulia Tsvetkov, Oren Etzioni, Maarten Sap, Regina Rini, and Yejin Choi. Investigating machine moral judgement through the delphi experiment. 7(1):145 160, 2025. [75] Anna Jobin, Marcello Ienca, and Effy Vayena. The global landscape of ai ethics guidelines. Nature machine intelligence, 1(9):389399, 2019. [76] Daniel Kahneman, Olivier Sibony, and Cass Sunstein. Noise: flaw in human judgment. Hachette UK, 2021. [77] Batuhan Karaman, Ishmam Zabir, Alon Benhaim, Vishrav Chaudhary, Mert Sabuncu, and Xia Song. Porover: Improving safety and reducing overrefusal in large language models with overgeneration and preference optimization. arXiv preprint arXiv:2410.12999, 2024. 34 [78] Raphael Koster, Jan Balaguer, Andrea Tacchetti, Ari Weinstein, Tina Zhu, Oliver P. Hauser, Duncan Williams, Lucy Campbell-Gillingham, Phoebe Thacker, Matthew M. Botvinick, and Christopher Summerfield. Human-centered mechanism design with democratic ai. ArXiv, abs/2201.11441, 2022. [79] Ranjay Krishna, Donsuk Lee, Li Fei-Fei, and Michael Bernstein. Socially situated artificial intelligence enables learning from human interaction. Proceedings of the National Academy of Sciences, 119(39):e2115730119, 2022. [80] Satyapriya Krishna, Chirag Agarwal, and Himabindu Lakkaraju. Understanding the effects of iterative prompting on truthfulness. arXiv preprint arXiv:2402.06625, 2024. [81] Himabindu Lakkaraju, Qiaozhu Mei, Chenhao Tan, Jie Tang, and Yutong Xie. The first workshop on ai behavioral science. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages 67246725, 2024. [82] Yihuai Lan, Zhiqiang Hu, Lei Wang, Yang Wang, Deheng Ye, Peilin Zhao, Ee-Peng Lim, Hui Xiong, and Hao Wang. Llm-based agent society investigation: Collaboration and confrontation in avalon gameplay. arXiv preprint arXiv:2310.14985, 2023. [83] Gael Le Mens, Balazs Kovacs, Michael Hannan, and Guillem Pros. Uncovering the semantics of concepts using gpt-4. Proceedings of the National Academy of Sciences, 120(49):e2309350120, 2023. [84] Theodore Lechterman. The concept of accountability in AI ethics and governance. Oxford University Press Oxford, 164182, 2022. [85] Jingling Li, Zeyu Tang, Xiaoyu Liu, Peter Spirtes, Kun Zhang, Liu Leqi, and Yang Liu. Prompting fairness: Integrating causality to debias large language models. In The Thirteenth International Conference on Learning Representations. [86] Nian Li, Chen Gao, Mingyu Li, Yong Li, and Qingmin Liao. Econagent: large language model-empowered agents for simulating macroeconomic activities. arXiv preprint arXiv:2310.10436, 2023. [87] Yuan Li, Yixuan Zhang, and Lichao Sun. Metaagents: Simulating interactions of human behaviors for llm-based task-oriented coordination via collaborative generative agents. arXiv preprint arXiv:2310.06500, 2023. [88] Zhuoyan Li, Zhuoran Lu, and Ming Yin. Decoding ais nudge: unified framework to predict human behavior in ai-assisted decision making. Proceedings of the AAAI Conference on Artificial Intelligence, 38(9):1008310091, Mar. 2024. [89] Falk Lieder and Thomas Griffiths. Strategy selection as rational metareasoning. Psychological review, 124(6):762, 2017. [90] Eleanor Lin, James Hale, and Jonathan Gratch. Toward better understanding of the emotional dynamics of negotiation with large language models. In Proceedings of the Twentyfourth International Symposium on Theory, Algorithmic Foundations, and Protocol Design for Mobile Networks and Mobile Computing, pages 545550, 2023. [91] Stephanie Lin, Jacob Hilton, and Owain Evans. Truthfulqa: Measuring how models mimic human falsehoods. arXiv preprint arXiv:2109.07958, 2021. [92] Pantelis Linardatos, Vasilis Papastefanopoulos, and Sotiris Kotsiantis. Explainable ai: review of machine learning interpretability methods. Entropy, 23(1):18, 2020. [93] Tianci Liu, Haoyu Wang, Shiyang Wang, Yu Cheng, and Jing Gao. Lidao: towards limited interventions for debiasing (large) language models. arXiv preprint arXiv:2406.00548, 2024. [94] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining In ProGuo. Swin transformer: Hierarchical vision transformer using shifted windows. ceedings of the IEEE/CVF international conference on computer vision, pages 1001210022, 2021. 35 [95] Nunzio Lor`e and Babak Heydari. Strategic behavior of large language models and the role of game structure versus contextual framing. Scientific Reports, 14(1):18490, 2024. [96] Ryan Lowe, Yi Wu, Aviv Tamar, Jean Harb, Pieter Abbeel, and Igor Mordatch. Multi-agent actor-critic for mixed cooperative-competitive environments. In Proceedings of the 31st International Conference on Neural Information Processing Systems, NIPS17, page 63826393, Red Hook, NY, USA, 2017. Curran Associates Inc. [97] Li-Chun Lu, Shou-Jen Chen, Tsung-Min Pai, Chan-Hung Yu, Hung-yi Lee, and Shao-Hua Sun. Llm discussion: Enhancing the creativity of large language models via discussion framework and role-play. arXiv preprint arXiv:2405.06373, 2024. [98] Luca Luceri, Felipe Cardoso, and Silvia Giordano. Down the bot hole: Actionable insights from one-year analysis of bot activity on twitter. First Monday, 2021. [99] Luca Luceri, Ashok Deb, Adam Badawy, and Emilio Ferrara. Red bots do it better: Comparative analysis of social bot partisan behavior. In Companion proceedings of the 2019 world wide web conference, pages 10071012, 2019. [100] Trung Quoc Luong, Xinbo Zhang, Zhanming Jie, Peng Sun, Xiaoran Jin, and Hang Li. Reft: Reasoning with reinforced fine-tuning. arXiv preprint arXiv:2401.08967, 3, 2024. [101] Yecheng Jason Ma, William Liang, Guanzhi Wang, De-An Huang, Osbert Bastani, Dinesh Jayaraman, Yuke Zhu, Linxi Fan, and Anima Anandkumar. Eureka: Human-level reward design via coding large language models. arXiv preprint arXiv:2310.12931, 2023. [102] Trisha Mahoney, Kush Varshney, and Michael Hind. AI fairness. OReilly Media, Incorporated, 2020. [103] Qiaozhu Mei, Yutong Xie, Walter Yuan, and Matthew Jackson. turing test of whether ai chatbots are behaviorally similar to humans. Proceedings of the National Academy of Sciences, 121(9):e2313925121, 2024. [104] Johnathan Mell, Jonathan Gratch, Tim Baarslag, Reyhan Aydogan, and Catholijn Jonker. Results of the first annual human-agent league of the automated negotiating agents competition. In Proceedings of the 18th International Conference on Intelligent Virtual Agents, pages 2328, 2018. [105] Juanjuan Meng. Ai emerges as the frontier in behavioral science. Proceedings of the National Academy of Sciences, 121(10):e2401336121, 2024. [106] Sumeet Motwani, Mikhail Baranchuk, Martin Strohmeier, Vijay Bolina, Philip Torr, Lewis Hammond, and Christian Schroeder de Witt. Secret collusion among ai agents: Multiagent deception via steganography. Advances in Neural Information Processing Systems, 37:7343973486, 2024. [107] Mikhail Mozikov, Nikita Severin, Valeria Bodishtianu, Maria Glushanina, Ivan Nasonov, Daniil Orekhov, Pekhotin Vladislav, Ivan Makovetskiy, Mikhail Baklashkin, Vasily Lavrentyev, et al. Eai: Emotional decision-making of llms in strategic games and ethical dilemmas. Advances in Neural Information Processing Systems, 37:5396954002, 2024. [108] Junho Myung, Nayeon Lee, Yi Zhou, Jiho Jin, Rifki A. Putri, Dimosthenis Antypas, Hsuvas Borkakoty, Eunsu Kim, Carla Perez-Almendros, Abinew A. Ayele, Vıctor Gutierrez-Basulto, Yazmın Ibanez-Garcıa, Hwaran Lee, Shamsuddeen H. Muhammad, Kiwoong Park, Anar S. Rzayev, Nina White, Seid M. Yimam, Mohammad T. Pilehvar, Nedjma Ousidhoum, Jose Camacho-Collados, and Alice Oh. BLEnD: Benchmark for LLMs on Everyday Knowledge in Diverse Cultures and Languages. Advances in Neural Information Processing Systems, 37:7810478146, December 2024. [109] Thao Nguyen, Maithra Raghu, and Simon Kornblith. Do wide and deep networks learn the same things? uncovering how neural network representations vary with width and depth. arXiv preprint arXiv:2010.15327, 2020. [110] Tuan-Phong Nguyen, Simon Razniewski, Aparna Varde, and Gerhard Weikum. Extracting Cultural Commonsense Knowledge at Scale. In Proceedings of the ACM Web Conference 2023, pages 19071917, Austin TX USA, April 2023. ACM. [111] Jianmo Ni, Gustavo Hernandez Abrego, Noah Constant, Ji Ma, Keith Hall, Daniel Cer, and Yinfei Yang. Sentence-t5: Scalable sentence encoders from pre-trained text-to-text models. arXiv preprint arXiv:2108.08877, 2021. [112] Claudio Novelli, Mariarosaria Taddeo, and Luciano Floridi. Accountability in artificial intelligence: what it is and how it works. AI & SOCIETY, 39:112, 02 2023. [113] Aidan OGara. Hoodwinked: Deception and cooperation in text-based game for language models. arXiv preprint arXiv:2308.01404, 2023. [114] Bo Pan, Jiaying Lu, Ke Wang, Li Zheng, Zhen Wen, Yingchaojie Feng, Minfeng Zhu, and Wei Chen. Agentcoord: Visually exploring coordination strategy for llm-based multi-agent collaboration. arXiv preprint arXiv:2404.11943, 2024. [115] Joon Sung Park, Joseph OBrien, Carrie Jun Cai, Meredith Ringel Morris, Percy Liang, and Michael Bernstein. Generative agents: Interactive simulacra of human behavior. In Proceedings of the 36th annual acm symposium on user interface software and technology, pages 122, 2023. [116] Giorgio Piatti, Zhijing Jin, Max Kleiman-Weiner, Bernhard Scholkopf, Mrinmaya Sachan, and Rada Mihalcea. Cooperate or collapse: Emergence of sustainable cooperation in society of llm agents. Advances in Neural Information Processing Systems, 37:111715111759, 2024. [117] Yushan Qian, Wei-Nan Zhang, and Ting Liu. Harnessing the power of large language models for empathetic response generation: Empirical investigations and improvements. arXiv preprint arXiv:2310.05140, 2023. [118] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. [119] Kristina Radivojevic, Nicholas Clark, and Paul Brenner. Llms among us: Generative ai participating in digital discourse. In Proceedings of the AAAI Symposium Series, volume 3, pages 209218, 2024. [120] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. Advances in Neural Information Processing Systems, 36:5372853741, 2023. [121] Iyad Rahwan, Manuel Cebrian, Nick Obradovich, Josh Bongard, Jean-Francois Bonnefon, Cynthia Breazeal, Jacob Crandall, Nicholas Christakis, Iain Couzin, Matthew Jackson, et al. Machine behaviour. Nature, 568(7753):477486, 2019. [122] Narun Raman, Taylor Lundy, Samuel Amouyal, Yoav Levine, Kevin Leyton-Brown, and Moshe Tennenholtz. Steer: Assessing the economic rationality of large language models. arXiv preprint arXiv:2402.09552, 2024. [123] Yiting Ran, Xintao Wang, Rui Xu, Xinfeng Yuan, Jiaqing Liang, Deqing Yang, and Yanghua Xiao. Capturing minds, not just words: Enhancing role-playing language models with personality-indicative data. arXiv preprint arXiv:2406.18921, 2024. [124] Shaina Raza, Chen Ding, and Deval Pandya. Mitigating bias in conversations: hate speech classifier and debiaser with prompts. arXiv preprint arXiv:2307.10213, 2023. [125] Scott Reed, Konrad Zolna, Emilio Parisotto, Sergio Gomez Colmenarejo, Alexander Novikov, Gabriel Barth-Maron, Mai Gimenez, Yury Sulsky, Jackie Kay, Jost Tobias Springenberg, et al. generalist agent. arXiv preprint arXiv:2205.06175, 2022. [126] Alireza Salemi, Sheshera Mysore, Michael Bendersky, and Hamed Zamani. Lamp: When large language models meet personalization. arXiv preprint arXiv:2304.11406, 2023. 37 [127] Shibani Santurkar, Esin Durmus, Faisal Ladhak, Cinoo Lee, Percy Liang, and Tatsunori In Proceedings of the 40th Hashimoto. Whose Opinions Do Language Models Reflect? International Conference on Machine Learning, pages 2997130004. PMLR, July 2023. [128] Bidipta Sarkar, Warren Xia, Karen Liu, and Dorsa Sadigh. Training language models for social deduction with multi-agent reinforcement learning. arXiv preprint arXiv:2502.06060, 2025. [129] Jeremy Scheurer, Mikita Balesni, and Marius Hobbhahn. Large language models can strategically deceive their users when put under pressure. In ICLR 2024 Workshop on Large Language Model (LLM) Agents, 2024. [130] Michaela Schippers, Michael West, and Jeremy Dawson. Team reflexivity and innovation: The moderating role of team context. Journal of Management, 41(3):769788, 2015. [131] Johannes Schneider, Steffi Haag, and Leona Chandra Kruse. Negotiating with llms: Prompt hacks, skill gaps, and reasoning deficits. In International Conference on Computer-Human Interaction Research and Applications, pages 238259. Springer, 2024. [132] Amrith Setlur, Chirag Nagpal, Adam Fisch, Xinyang Geng, Jacob Eisenstein, Rishabh Agarwal, Alekh Agarwal, Jonathan Berant, and Aviral Kumar. Rewarding progress: Scaling automated process verifiers for llm reasoning. arXiv preprint arXiv:2410.08146, 2024. [133] Yu Shang, Yu Li, Keyu Zhao, Likai Ma, Jiahe Liu, Fengli Xu, and Yong Li. Agentsquare: Automatic llm agent search in modular design space. arXiv preprint arXiv:2410.06153, 2024. [134] Chengcheng Shao, Giovanni Luca Ciampaglia, Onur Varol, Kai-Cheng Yang, Alessandro Flammini, and Filippo Menczer. The spread of low-credibility content by social bots. Nature communications, 9(1):4787, 2018. [135] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. [136] Shota Shiiku, Raja Marjieh, Manuel Anglada-Tort, and Nori Jacoby. The dynamics of collective creativity in human-ai social networks. arXiv preprint arXiv:2502.17962, 2025. [137] Minkyu Shin, Jin Kim, Bas Van Opheusden, and Thomas Griffiths. Superhuman artificial intelligence can improve human decision-making by increasing novelty. Proceedings of the National Academy of Sciences, 120(12):e2214840120, 2023. [138] Hirokazu Shirado and Nicholas Christakis. Locally noisy autonomous agents improve global human coordination in network experiments. Nature, 545(7654):370374, 2017. [139] David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert, Lucas baker, Matthew Lai, Adrian Bolton, Yutian Chen, Timothy P. Lillicrap, Fan Hui, L. Sifre, George van den Driessche, Thore Graepel, and Demis Hassabis. Mastering the game of go without human knowledge. Nature, 550:354359, 2017. [140] Dylan Slack, Satyapriya Krishna, Himabindu Lakkaraju, and Sameer Singh. Explaining machine learning models with interactive natural language conversations using TalkToModel. Nature Machine Intelligence, 5(8):873883, 2023. [141] Ben Snyder, Marius Moisescu, and Muhammad Bilal Zafar. On early detection of hallucinations in factual question answering. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages 27212732, 2024. [142] Robin Staab, Mark Vero, Mislav Balunovic, and Martin Vechev. Beyond memorization: Violating privacy via inference with large language models. In The Twelfth International Conference on Learning Representations, 2024. [143] Massimo Stella, Emilio Ferrara, and Manlio De Domenico. Bots increase exposure to negative and inflammatory content in online social systems. Proceedings of the National Academy of Sciences, 115(49):1243512440, 2018. 38 [144] Alexander Stewart, Mohsen Mosleh, Marina Diakonova, Antonio Arechar, David Rand, Information gerrymandering and undemocratic decisions. Nature, and Joshua Plotkin. 573(7772):117121, 2019. [145] Mark Steyvers, Heliodoro Tejeda, Aakriti Kumar, Catarina Belem, Sheer Karny, Xinyue Hu, Lukas Mayer, and Padhraic Smyth. What large language models know and what people think they know. Nature Machine Intelligence, pages 111, 2025. [146] James WA Strachan, Dalila Albergo, Giulia Borghini, Oriana Pansardi, Eugenio Scaliti, Saurabh Gupta, Krati Saxena, Alessandro Rufo, Stefano Panzeri, Guido Manzi, et al. Testing theory of mind in large language models and humans. Nature Human Behaviour, 8(7):1285 1295, 2024. [147] Libo Sun, Siyuan Wang, Xuanjing Huang, and Zhongyu Wei. Identity-driven hierarchical role-playing agents. arXiv preprint arXiv:2407.19412, 2024. [148] James Surowiecki. The wisdom of crowds. Vintage, 2005. [149] Xiangru Tang, Anni Zou, Zhuosheng Zhang, Ziming Li, Yilun Zhao, Xingyao Zhang, Arman Cohan, and Mark Gerstein. Medagents: Large language models as collaborators for zero-shot medical reasoning. arXiv preprint arXiv:2311.10537, 2023. [150] Yihong Tang, Jiao Ou, Che Liu, Fuzheng Zhang, Di Zhang, and Kun Gai. Enhancing roleplaying systems through aggressive queries: Evaluation and improvement. arXiv preprint arXiv:2402.10618, 2024. [151] Yan Tao, Olga Viberg, Ryan Baker, and Rene Kizilcec. Cultural bias and cultural alignment of large language models. PNAS nexus, 3(9):pgae346, 2024. [152] Kian Siong Tey, Asaf Mazar, Geoff Tomaino, Angela Duckworth, and Lyle Ungar. People judge others more harshly after talking to bots. PNAS nexus, 3(9):pgae397, 2024. [153] Richard Thaler and Cass Sunstein. Nudge: Improving decisions about health, wealth, and happiness. Penguin, 2009. [154] Margaret Traeger, Sarah Strohkorb Sebo, Malte Jung, Brian Scassellati, and Nicholas Christakis. Vulnerable robots positively shape human conversational dynamics in human robot team. Proceedings of the National Academy of Sciences, 117(12):63706375, 2020. [155] Milena Tsvetkova, Taha Yasseri, Niccolo Pescetelli, and Tobias Werner. new sociology of humans and machines. Nature Human Behaviour, 8(10):18641876, 2024. [156] Sagar Uprety, Amit Kumar Jaiswal, Haiming Liu, and Dawei Song. effects in similarity judgements in large language models, 2024. Investigating context [157] Angelina Wang, Jamie Morgenstern, and John Dickerson. Large language models that replace human participants can harmfully misportray and flatten identity groups. Nature Machine Intelligence, pages 112, 2025. [158] Lei Wang, Zheqing Zhang, and Xu Chen. Investigating and extending homans social exchange theory with large language model based agents. arXiv preprint arXiv:2502.12450, 2025. [159] Daniel Wegner. Transactive memory: contemporary analysis of the group mind. In Theories of group behavior, pages 185208. Springer, 1987. [160] Lilian Weng. Llm-powered autonomous agents. lilianweng. github. io, jun 2023. URL https://lilianweng. github. io/posts/2023-06-23-agent, 2023. [161] Dekun Wu, Haochen Shi, Zhiyuan Sun, and Bang Liu. Deciphering digital detectives: Understanding llm behaviors and capabilities in multi-agent mystery games. arXiv preprint arXiv:2312.00746, 2023. 39 [162] Junkang Wu, Yuexiang Xie, Zhengyi Yang, Jiancan Wu, Jinyang Gao, Bolin Ding, Xiang Wang, and Xiangnan He. β-dpo: Direct preference optimization with dynamic β. Advances in Neural Information Processing Systems, 37:129944129966, 2024. [163] Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Beibin Li, Erkang Zhu, Li Jiang, Xiaoyun Zhang, Shaokun Zhang, Jiale Liu, et al. Autogen: Enabling next-gen llm applications via multi-agent conversation. arXiv preprint arXiv:2308.08155, 2023. [164] Tong Wu, Ashwinee Panda, Jiachen T. Wang, and Prateek Mittal. Privacy-preserving inIn The Twelfth International Conference on context learning for large language models. Learning Representations, 2024. [165] Weiqi Wu, Hongqiu Wu, Lai Jiang, Xingyuan Liu, Jiale Hong, Hai Zhao, and Min Zhang. From role-play to drama-interaction: An llm solution. arXiv preprint arXiv:2405.14231, 2024. [166] Zengqing Wu, Run Peng, Shuyuan Zheng, Qianying Liu, Xu Han, Brian Inhyuk Kwon, Makoto Onizuka, Shaojie Tang, and Chuan Xiao. Shall we team up: Exploring spontaneous cooperation of competing llm agents. In Conference on Empirical Methods in Natural Language Processing, 2024. [167] Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, et al. The rise and potential of large language model based agents: survey. Science China Information Sciences, 68(2):121101, 2025. [168] Yuhang Xiao, yudilin, and Ming-Chang Chiu. Behavioral bias of vision-language models: behavioral finance view. In ICML 2024 Workshop on LLMs and Cognition, 2024. [169] Tianbao Xie, Siheng Zhao, Chen Henry Wu, Yitao Liu, Qian Luo, Victor Zhong, Yanchao Yang, and Tao Yu. Text2reward: Automated dense reward function generation for reinforcement learning. In International Conference on Learning Representations (ICLR), 2024 (07/05/2024-11/05/2024, Vienna, Austria), 2024. [170] Yueqi Xie, Jingwei Yi, Jiawei Shao, Justin Curl, Lingjuan Lyu, Qifeng Chen, Xing Xie, and Fangzhao Wu. Defending chatgpt against jailbreak attack via self-reminders. Nature Machine Intelligence, 5(12):14861496, 2023. [171] Yutong Xie, Yiyao Liu, Zhuang Ma, Lin Shi, Xiyuan Wang, Walter Yuan, Matthew Jackson, and Qiaozhu Mei. How different ai chatbots behave? benchmarking large language models in behavioral economics games. arXiv preprint arXiv:2412.12362, 2024. [172] Yuxi Xie, Anirudh Goyal, Wenyue Zheng, Min-Yen Kan, Timothy Lillicrap, Kenji Kawaguchi, and Michael Shieh. Monte carlo tree search boosts reasoning via iterative preference learning. arXiv preprint arXiv:2405.00451, 2024. [173] Lin Xu, Zhiyuan Hu, Daquan Zhou, Hongyu Ren, Zhen Dong, Kurt Keutzer, See Kiong Ng, and Jiashi Feng. Magic: Investigation of large language model powered multi-agent in cognition, adaptability, rationality and collaboration. arXiv preprint arXiv:2311.08562, 2023. [174] Xilie Xu, Keyi Kong, Ning Liu, Lizhen Cui, Di Wang, Jingfeng Zhang, and Mohan Kankanhalli. An llm can fool itself: prompt-based adversarial attack. arXiv preprint arXiv:2310.13345, 2023. [175] Yuzhuang Xu, Shuo Wang, Peng Li, Fuwen Luo, Xiaolong Wang, Weidong Liu, and Yang Liu. Exploring large language models for communication games: An empirical study on werewolf. arXiv preprint arXiv:2309.04658, 2023. [176] Yuwei Yan, Qingbin Zeng, Zhiheng Zheng, Jingzhe Yuan, Jie Feng, Jun Zhang, Fengli Xu, and Yong Li. Opencity: scalable platform to simulate urban activities with massive llm agents. arXiv preprint arXiv:2410.21286, 2024. [177] Bohao Yang, Dong Liu, Chen Tang, Chenghao Xiao, Kun Zhao, Chao Li, Lin Yuan, Guang Yang, Lanxiao Huang, and Chenghua Lin. Simschat: customisable persona-driven roleplaying agent. arXiv e-prints, pages arXiv2406, 2024. 40 [178] Hanqing Yang, Jingdi Chen, Marie Siew, Tania Lorido-Botran, and Carlee Joe-Wong. Llmpowered decentralized generative agents with adaptive hierarchical knowledge graph for cooperative planning. arXiv preprint arXiv:2502.05453, 2025. [179] Kai-Cheng Yang and Filippo Menczer. Anatomy of an ai-powered malicious social botnet. arXiv preprint arXiv:2307.16336, 2023. [180] Kai-Cheng Yang, Christopher Torres-Lugo, and Filippo Menczer. low-credibility information on twitter during the covid-19 outbreak. arXiv:2004.14484, 2020."
        },
        {
            "title": "Prevalence of\narXiv preprint",
            "content": "[181] Qiang Yang. Toward responsible ai: An overview of federated learning for user-centered privacy-preserving computing. ACM Transactions on Interactive Intelligent Systems (TiiS), 11(3-4):122, 2021. [182] Da Yin, Haoyi Qiu, Kung-Hsiang Huang, Kai-Wei Chang, and Nanyun Peng. SafeWorld: Geo-Diverse Safety Alignment. Advances in Neural Information Processing Systems, 37:128734128768, January 2025. [183] Xiaoyan Yu, Tongxu Luo, Yifan Wei, Fangyu Lei, Yiming Huang, Hao Peng, and Liehuang Zhu. Neeko: Leveraging dynamic lora for efficient multi-character role-playing agent. arXiv preprint arXiv:2402.13717, 2024. [184] Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Zi-Hang Jiang, Francis EH Tay, Jiashi Feng, and Shuicheng Yan. Tokens-to-token vit: Training vision transformers from scratch on imagenet. In Proceedings of the IEEE/CVF international conference on computer vision, pages 558567, 2021. [185] Xia Zeng, David La Barbera, Kevin Roitero, Arkaitz Zubiaga, and Stefano Mizzaro. Combining large language models and crowdsourcing for hybrid human-ai misinformation detection. In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 23322336, 2024. [186] Yongcheng Zeng, Guoqing Liu, Weiyu Ma, Ning Yang, Haifeng Zhang, and Jun Wang. Token-level direct preference optimization. arXiv preprint arXiv:2404.11999, 2024. [187] Jintian Zhang, Xin Xu, Ningyu Zhang, Ruibo Liu, Bryan Hooi, and Shumin Deng. Exploring collaboration mechanisms for llm agents: social psychology view. arXiv preprint arXiv:2310.02124, 2023. [188] Shao Zhang, Xihuai Wang, Wenhao Zhang, Yongshan Chen, Landi Gao, Dakuo Wang, Weinan Zhang, Xinbing Wang, and Ying Wen. Mutual theory of mind in human-ai collaboration: An empirical study with llm-driven ai agents in real-time shared workspace task. arXiv preprint arXiv:2409.08811, 2024. [189] Yusen Zhang, Ruoxi Sun, Yanfei Chen, Tomas Pfister, Rui Zhang, and Sercan Arik. Chain of agents: Large language models collaborating on long-context tasks. Advances in Neural Information Processing Systems, 37:132208132237, 2024. [190] Qinlin Zhao, Jindong Wang, Yixuan Zhang, Yiqiao Jin, Kaijie Zhu, Hao Chen, and Xing Xie. Competeai: Understanding the competition dynamics in large language model-based agents. arXiv preprint arXiv:2310.17512, 2023. [191] Yong Zhao, Yang Deng, See-Kiong Ng, and Tat-Seng Chua. Aligning Large Language Models for Faithful Integrity Against Opposing Argument, January 2025. [192] Yunpeng Zhao and Jie Zhang. Does training with synthetic data truly protect privacy? In The Thirteenth International Conference on Learning Representations, 2025. [193] Xiaosen Zheng, Tianyu Pang, Chao Du, Qian Liu, Jing Jiang, and Min Lin. Cheating automatic LLM benchmarks: Null models achieve high win rates. In The Thirteenth International Conference on Learning Representations, 2025. 41 [194] Zhiling Zheng, Oufan Zhang, Ha Nguyen, Nakul Rampal, Ali Alawadhi, Zichao Rong, Teresa Head-Gordon, Christian Borgs, Jennifer Chayes, and Omar Yaghi. Chatgpt research group for optimizing the crystallinity of mofs and cofs. ACS Central Science, 9(11):21612170, 2023. [195] Lexin Zhou, Wout Schellaert, Fernando Martınez-Plumed, Yael Moros-Daval, C`esar Ferri, and Jose Hernandez-Orallo. Larger and more instructable language models become less reliable. Nature, 634(8032):6168, 2024."
        }
    ],
    "affiliations": [
        "Department of Computer Science and Engineering, The Hong Kong University of Science and Technology, Hong Kong, China",
        "Department of Electronic Engineering, BNRist, Tsinghua University, Beijing, China"
    ]
}