{
    "paper_title": "AI Meets Brain: Memory Systems from Cognitive Neuroscience to Autonomous Agents",
    "authors": [
        "Jiafeng Liang",
        "Hao Li",
        "Chang Li",
        "Jiaqi Zhou",
        "Shixin Jiang",
        "Zekun Wang",
        "Changkai Ji",
        "Zhihao Zhu",
        "Runxuan Liu",
        "Tao Ren",
        "Jinlan Fu",
        "See-Kiong Ng",
        "Xia Liang",
        "Ming Liu",
        "Bing Qin"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Memory serves as the pivotal nexus bridging past and future, providing both humans and AI systems with invaluable concepts and experience to navigate complex tasks. Recent research on autonomous agents has increasingly focused on designing efficient memory workflows by drawing on cognitive neuroscience. However, constrained by interdisciplinary barriers, existing works struggle to assimilate the essence of human memory mechanisms. To bridge this gap, we systematically synthesizes interdisciplinary knowledge of memory, connecting insights from cognitive neuroscience with LLM-driven agents. Specifically, we first elucidate the definition and function of memory along a progressive trajectory from cognitive neuroscience through LLMs to agents. We then provide a comparative analysis of memory taxonomy, storage mechanisms, and the complete management lifecycle from both biological and artificial perspectives. Subsequently, we review the mainstream benchmarks for evaluating agent memory. Additionally, we explore memory security from dual perspectives of attack and defense. Finally, we envision future research directions, with a focus on multimodal memory systems and skill acquisition."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 2 ] . [ 1 3 4 3 3 2 . 2 1 5 2 : r AI Meets Brain: Unified Survey on Memory Systems from Cognitive Neuroscience to Autonomous Agents JIAFENG LIANG and HAO LI, Harbin Institute of Technology, China CHANG LI and JIAQI ZHOU, Harbin Institute of Technology, China SHIXIN JIANG and ZEKUN WANG, Harbin Institute of Technology, China CHANGKAI JI, Fudan University, China ZHIHAO ZHU and RUNXUAN LIU, Harbin Institute of Technology, China TAO REN, Peking University, China JINLAN FU and SEE-KIONG NG, National University of Singapore, Singapore XIA LIANG, MING LIU, and BING QIN, Harbin Institute of Technology, China Memory serves as the pivotal nexus bridging past and future, providing both humans and AI systems with invaluable concepts and experience to navigate complex tasks. Recent research on autonomous agents has increasingly focused on designing efficient memory workflows by drawing on cognitive neuroscience. However, constrained by interdisciplinary barriers, existing works struggle to assimilate the essence of human memory mechanisms. To bridge this gap, this survey systematically synthesizes interdisciplinary knowledge of memory, connecting insights from cognitive neuroscience with LLM-driven agents. Specifically, we first elucidate the definition and function of memory along progressive trajectory from cognitive neuroscience through LLMs to agents. We then provide comparative analysis of memory taxonomy, storage mechanisms, and the complete management lifecycle from both biological and artificial perspectives. Subsequently, we review the mainstream benchmarks for evaluating agent memory. Additionally, we explore memory security from dual perspectives of attack and defense. Finally, we envision future research directions, with focus on multimodal memory systems and skill acquisition. Code: https://github.com/AgentMemory/Huaman-Agent-Memory ACM Reference Format: Jiafeng Liang, Hao Li, Chang Li, Jiaqi Zhou, Shixin Jiang, Zekun Wang, Changkai Ji, Zhihao Zhu, Runxuan Liu, Tao Ren, Jinlan Fu, See-Kiong Ng, Xia Liang, Ming Liu, and Bing Qin. 2025. AI Meets Brain: Unified Survey on Memory Systems from Cognitive Neuroscience to Autonomous Agents. 1, 1 (December 2025), 57 pages. https://doi.org/10.1145/nnnnnnn.nnnnnnn Equal contribution Corresponding author Authors addresses: Jiafeng Liang, jfliang@ir.hit.edu.cn; Hao Li, haoli@ir.hit.edu.cn, Harbin Institute of Technology, Harbin, Heilongjiang, China, 150001; Chang Li, cli@ir.hit.edu.cn; Jiaqi Zhou, yufeiqs917@gmail.com, Harbin Institute of Technology, Harbin, Heilongjiang, China, 150001; Shixin Jiang, sxjiang@ir.hit.edu.cn; Zekun Wang, zkwang@ir.hit.edu.cn, Harbin Institute of Technology, Harbin, Heilongjiang, China, 150001; Changkai Ji, ckji24@m.fudan.edu.cn, Fudan University, Shanghai, Shanghai, China, 200433; Zhihao Zhu, zhzhu@ir.hit.edu.cn; Runxuan Liu, rxliu@ir.hit.edu.cn, Harbin Institute of Technology, Harbin, Heilongjiang, China, 150001; Tao Ren, rtkenny@stu.pku.edu.cn, Peking University, Beijing, Beijing, China, 100871; Jinlan Fu, jinlanjonna@gmail.com; See-Kiong Ng, seekiong@nus.edu.sg, National University of Singapore, Singapore, Singapore, Singapore, 119077; Xia Liang, xia.liang@hit.edu.cn; Ming Liu, mliu@ir.hit.edu.cn; Bing Qin, qinb@ir. hit.edu.cn, Harbin Institute of Technology, Harbin, Heilongjiang, China, 150001. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or fee. Request permissions from permissions@acm.org. 2025 Association for Computing Machinery. XXXX-XXXX/2025/12-ART $15.00 https://doi.org/10.1145/nnnnnnn.nnnnnnn , Vol. 1, No. 1, Article . Publication date: December 2025. 2 Liang, et al."
        },
        {
            "title": "1 INTRODUCTION\nMemory is the cognitive nexus that interweaves past experience with future decision-making [21].\nIn humans, it manifests as a dynamic neural process through which the brain stores and manages\ninformation [70]. Its profound significance lies in endowing individuals with the capacity to learn,\nadapt, and reshape their behavior, enabling humans to maintain coherence and foresight in an ever-\nchanging environment [27, 207]. As Large Language Models (LLMs) continue to evolve, endowing\nAI systems with human-like memory capabilities has emerged as a critical challenge [130]. However,\nthe natively stateless nature [117] of LLMs renders each inference independent, preventing models\nfrom maintaining cross-session continuity or accumulating experience from historical interactions.\nWhile modern LLMs have scaled parameters and context windows to massive sizes, knowledge\nupdate costs [57] and computational complexity [143, 182] remain significant bottlenecks.",
            "content": "With the rapid advancement of agents across diverse domains and tasks [10, 44, 103, 116, 175, 205, 262, 286, 314, 315, 384], memory systems have emerged as critical factor in enhancing their performance by enabling information persistence [123, 149, 264] and long-horizon planning [96, 213]. Rather than serving as passive repository for historical interactions, memory has evolved into dynamic cognitive hub that underpins complex decision-making. However, despite significant progress in memory mechanism research in recent years, existing works [53, 215, 323, 331, 398] tend to remain confined to single disciplinary perspective or lack depth in biological research, making it difficult to achieve deep integration between cognitive science and artificial intelligence. This separation has deprived both fields of opportunities for deep mutual validation and inspiration in memory research. To bridge this gap, our survey provides comprehensive and unified review of memory systems, integrating insights from cognitive neuroscience with the rapidly evolving field of LLM-driven agents. We first establish progressive research perspective on memory, transitioning from human brain to LLMs and ultimately to agents, systematically elucidating its definition and fundamental role (2, 3). Building on the classical shortand long-term memory dichotomy in cognitive neuroscience, we propose taxonomy that classifies agent memory along two dimensions, including natureand scope-based classification (4). The former distinguishes procedural experience from conceptual knowledge, while the latter concerns memory persistence within or across trajectories. Next, we examine memory storage from the perspectives of location and format (5). In cognitive neuroscience, short-term memory relies on distributed sensory-frontoparietal networks while long-term memory depends on hippocampal-neocortical coordination. For agents, storage locations include the context window for temporary memory and external memory bank for persistent information. Regarding format, the brain employs persistent activity and synaptic connection weights for short-term retention and structured forms like cognitive maps for long-term memory, while agents utilize natural language text, graph structures preserving relational information, internalized parameters, and latent representation in high-dimensional vector spaces. This survey further analyzes memory management mechanisms in both the human brain and agents, covering the complete closed-loop lifecycle of memory extraction, updating, retrieval, and utilization (6). In cognitive neuroscience, new information is encoded and gradually stabilized into durable representations through hippocampal-neocortical coordination. When external cues trigger hippocampal replay, the information about past events carried by these representations is reinstated, and the retrieval process itself opens plasticity window during which the underlying memory traces can be updated, strengthened, or weakened. In agents, raw information is distilled into structured records through flat, hierarchical, or generative paradigms, dynamically refreshed within trajectories while maintained across them. These records are retrieved via similarity matching or multi-factor approaches, then incorporated into reasoning through contextual augmentation or , Vol. 1, No. 1, Article . Publication date: December 2025. AI Meets Brain: Unified Survey on Memory Systems from Cognitive Neuroscience to Autonomous Agents parameter internalization. Then, we comprehensively outline various benchmarks for evaluating agent memory capabilities, categorizing them into semantic-oriented benchmarks that examine internal state maintenance and higher-order cognitive abilities, and episodic-oriented benchmarks that assess performance in vertical domains such as web search, tool use, and environmental interaction (7). Furthermore, we address the often-overlooked yet critical issue of memory security, providing systematic investigation from both attack and defense perspectives (8). On the attack side, existing methods focus on extracting sensitive information, implanting backdoors through malicious data, or degrading agent judgment by introducing noise and conflicting signals. On the defense side, countermeasures have been developed to purify retrieval sources, block harmful responses in real time, and safeguard sensitive data throughout the memory lifecycle. Finally, we propose future research directions with particular focus on two key areas (9). The first is multimodal memory systems capable of processing and integrating information across text, image, audio, video modalities. The second is agent skills that enable memory sharing and transfer across heterogeneous agents, transforming domain expertise into composable, reusable, and portable modular resources. We hope this survey can facilitate cross-disciplinary research, offering newcomers an accessible introduction while providing seasoned researchers with insights."
        },
        {
            "title": "2.2 Memory from the Perspective of Large Language Models\nIn LLMs, memory does not exist as a monolithic storage construct, but rather manifests in multiple\ndistinct forms spanning different storage substrates. These diverse memory modalities are essential\nfor overcoming the natively stateless [117] generative nature of such models and enabling complex\nlogical reasoning and multi-turn dialogue. Contemporary research categorizes LLMs memory into\nthree core types: (1) Parametric memory (§2.2.1), which is internalized within model parameters,\n(2) Working memory (§2.2.2), which relies on the context window for real-time interaction, and\n(3) Explicit external memory (§2.2.3), which is realized through external storage and retrieval\nmechanisms.",
            "content": ", Vol. 1, No. 1, Article . Publication date: December 2025. 4 Liang, et al."
        },
        {
            "title": "2.2.3 Explicit External Memory. To transcend the static boundaries of parametric storage and\nphysical constraints of context windows, explicit extended memory introduces a storage medium\nindependent of neural network weights, thereby establishing a non-parametric knowledge augmen-\ntation mechanism [94]. The core design philosophy underlying this approach is the decoupling of\ncomputation and storage, wherein LLMs function as a central processing unit responsible for infer-\nence and scheduling, while vast quantities of knowledge are offloaded to external repositories. This\narchitectural shift fundamentally transforms the model from a passive knowledge repository into an\nactive knowledge orchestrator [158]. The prevailing paradigm in this domain is retrieval-augmented\ngeneration (RAG) [165], which leverages vector databases [97] or knowledge graphs [360] to store\nextensive volumes of information. Such system effectively mitigate the hallucination phenomena\nand temporal lag inherent in parametric memory, enabling real-time knowledge updates and precise\nprovenance tracking at minimal computational cost, thereby substantially enhancing the reliability\nof system outputs. However, inference latency induced by retrieval operations, noise interference\nfrom irrelevant contexts, and the complexity of large-scale index construction under this paradigm\nremain critical bottlenecks constraining its performance [166].",
            "content": ", Vol. 1, No. 1, Article . Publication date: December 2025. AI Meets Brain: Unified Survey on Memory Systems from Cognitive Neuroscience to Autonomous Agents"
        },
        {
            "title": "2.3 Memory from the Perspective of Agents\nFrom the perspective of autonomous agents, the conceptualization of memory transcends the mere\ndata storage paradigm characteristic of LLM-centric approaches, evolving into a sophisticated\ndynamic cognitive architecture [176]. Specifically, the LLM-centric perspective primarily alleviates\nthe physical constraints imposed by parameters and context windows, while the agentic perspective\nshifts focus toward leveraging neuroscientific principles to construct external memory systems.\nThese systems augment the foundational reasoning capabilities of agents by endowing them with\ncapacities for identity persistence, experiential accumulation, and long-horizon planning [189]. To\nelucidate these complex operational principles, this section diverges from the storage medium dis-\ncourse in LLMs perspective, opting instead to deconstruct memory along three core dimensions: (1)\nStructured storage (§2.3.1), (2) Dynamic scheduling mechanisms (§2.3.2), and Cognitive processing\nand evolution (§2.3.3). Additionally, to provide a clearer introduction and prevent confusion, we\ncompare the differences between agent memory and RAG (§2.3.4).",
            "content": "Structured Storage. Structured storage serves as the physical carrier of agent memory systems, 2.3.1 aiming to transform unstructured natural language interactions into an efficient format that is easy for machines to index and understand [141]. Unlike the implicit parameter distribution within LLMs, agent external memory typically employs heterogeneous hybrid storage strategies. It mainly exhibits three typical paradigms, namely temporal flow, hierarchical flow, and symbolic libraries. Specifically, Park et al. [220] constructed linear memory that encapsulates interaction records as discrete memory objects, including timestamps, text descriptions, and importance scores. To overcome the physical boundaries of long-range contexts, Chen et al. [40] adopted hierarchical memory tree structure, constructing pyramid-like index through upward recursive summarization, enabling agents to locate key information through navigation rather than complete processing. In procedural memory, Wang et al. [278] introduced skill library similar to keyvalue pairs, using semantic vectors of skill descriptions as keys and executable code as values, successfully transforming unstructured exploration experiences into structured, reusable, and composable executable programs. These structural evolutions demonstrate that the memory system has transformed from static data container into dynamic cognitive center supporting complex decision-making."
        },
        {
            "title": "2.3.2 Dynamic Scheduling Mechanisms. While structured storage addresses the challenge of infor-\nmation persistence, dynamic scheduling mechanisms tackle the tension between limited attention\nresources and vast memory stores [237]. Given that LLMs are constrained by the physical limitation\nof context windows, agents cannot input all historical information into the model at once. There-\nfore, it is necessary to establish an efficient memory retrieval and update mechanism to achieve\naccurate routing and dynamic adaptation of information flow in the memory system. Regarding the\nscreening of redundancy mechanisms, Fang et al. [73] effectively balanced memory coverage and\nmaintenance latency through lightweight compression of sensory memory and updated strategy\nduring dormancy. This hierarchical idea is also very important in multi-agent collaboration. Zou\net al. [400] proposed a time-sharing scheduling strategy that borrows from the process scheduling of\nthe operating system and designed a memory model based on time slicing and multi-level caching,\nwhich significantly improved the resource allocation efficiency in a concurrent environment. In\naddition to static hierarchical and scheduling mechanisms, learning-based adaptive mechanisms\nare introduced to cope with more dynamic and complex environmental changes. The improved\nIGP-PPO algorithm [92] was employed to enable adaptive decision-making by dynamically selecting\nthe optimal scheduling rule based on real-time states. Yu et al. [346] applied reinforcement learning\nto long-range text stream management, training agents to autonomously retain and overwrite",
            "content": ", Vol. 1, No. 1, Article . Publication date: December 2025. 6 Liang, et al. memory, thereby achieving information extrapolation with linear complexity. Based on this, the memory scheduling mechanism further evolved towards autonomous evolution. Xu et al. [323] proposed dynamic mechanism that can autonomously trigger Link Generation and memory evolution, and by updating the context description of existing memory and reconstructing semantic associations in real time during interaction, it achieved the self-growth of knowledge networks without predefined rules. The dynamic scheduling mechanism successfully broke through physical bottlenecks through more refined resource management and adaptive strategies, and ensured the efficient flow of interactive information."
        },
        {
            "title": "2.3.4 The Difference Between Agent Memory and RAG. In terms of management mechanisms,\nthere exists a significant similarity between agent memory mechanisms and retrieval-augmented\ngeneration (RAG) techniques, as both enable language models to transcend the limitations of their\ninherent parametric knowledge through extracting and utilizing external information resources [83,\n108]. Although they converge in technical implementation, they exhibit distinct differences in usage\nscenarios. Traditional RAG approaches focus on connecting LLMs to static knowledge resources\n(e.g., Wikipedia, document library) for instant querying [38, 165, 311]. The design objective of such\nsystems lies in ensuring that generated content is grounded in reliable factual information, reducing\nthe probability of models producing erroneous information or hallucinations [111, 122]. However,\nthey generally lack the capability to record and accumulate historical interaction information [215].\nIn contrast, agent memory systems are embedded within the dynamic interaction process between\nagents and their environments, continuously incorporating information generated from agent\noperations and environmental feedback into memory containers [213].",
            "content": ", Vol. 1, No. 1, Article . Publication date: December 2025. AI Meets Brain: Unified Survey on Memory Systems from Cognitive Neuroscience to Autonomous Agents"
        },
        {
            "title": "3 MEMORY UTILITY\nIn cognitive neuroscience, memory constitutes the neural processes through which the brain\nencodes, stores, and retrieves information, enabling individuals to retain past experiences and lever-\nage them to guide ongoing behavior and inform future decision-making [21, 156]. In LLM-driven\nagents, a fundamental tension exists between the inherent statelessness of models coupled with\nengineering constraints and the continuity required for complex, long-horizon tasks. Consequently,\nmemory transcends its role as a mere passive repository bridging historical interactions and instead\nserves as a pivotal active component within the cognitive architecture of agents. Concretely, the\nincorporation of memory fundamentally extends agent capabilities through three paradigms: (1) It\nmitigates the context window burden by utilizing structured information management to bypass\nthe computational costs and attentional degradation inherent in long-context reasoning (§3.1). (2)\nIt facilitates deep personalization by supporting both immediate conversational coherence and\nthe construction of enduring user profiles (§3.2). (3) It empowers reasoning enhancement and\nautonomous evolution by creating a feedback loop that synthesizes historical experience with\nreflection and planning modules (§3.3). The following sections will rigorously examine these three\nfunctional utilities.",
            "content": "Fig. 1. Overview of memory utility in LLM-driven agents. Memory extends agent capabilities by alleviating context window constraints, enabling long-term personalization, and driving experience-based reasoning through feedback loop with reflection and planning."
        },
        {
            "title": "3.1 Breaking Context Window Constraints\nDespite the expanding context windows of modern LLMs, the quadratic computational complexity\nof attention mechanisms and the lost-in-the-middle phenomenon [182] remain significant physical\nbottlenecks in long-horizon interactions. Consequently, the primary utility of memory lies in\nmapping infinite interaction streams into limited attention budgets, with the core paradigm shifting\nfrom passive linear truncation to dynamic context reconstruction. Depending on the implementation",
            "content": ", Vol. 1, No. 1, Article . Publication date: December 2025. 8 Liang, et al. mechanism, this process manifests primarily through the utilization of hierarchical structural designs for physical compression and virtualization indexing, as well as the internalization of memory management as intrinsic agent actions to achieve end-to-end autonomous optimization. Depending on the implementation mechanism, this process diverges into two primary directions: (1) Heuristic context design (3.1.1), which utilizes hierarchical structural designs for physical compression and virtualization indexing, and (2) Autonomous memory optimization (3.1.2), which internalizes memory management as intrinsic agent actions to achieve end-to-end autonomous optimization, as shown in Figure 1 (a)."
        },
        {
            "title": "3.2 Constructing Long-term Personalized Profiles\nThe pre-training paradigm of LLMs fundamentally characterizes them as generalized knowledge\nengines, relying on fixed parametric knowledge to handle broad, common-sense tasks. However,\nthis mechanism results in significant statelessness and homogeneity, leading to a one-size-fits-\nall interaction that fails to satisfy specific user preferences or meet the profound demand for\npersonalized experiences in long-term interactions. To bridge this gap, memory mechanisms serve\nas a critical non-parametric complement within the agent architecture. Beyond the mere storage\nof historical interactions, their core utility lies in constructing a dynamically evolving cognitive\nmodel for the agent. Through memory, agents can transform fragmented interaction data into",
            "content": ", Vol. 1, No. 1, Article . Publication date: December 2025. AI Meets Brain: Unified Survey on Memory Systems from Cognitive Neuroscience to Autonomous Agents 9 structured user profiles, thereby achieving deep adaptation to users across two dimensions: (1) Profile construction (3.2.1) and (2) Preference-aligned execution (3.2.2), as shown in Figure 1 (b)."
        },
        {
            "title": "3.3 Driving Experience-based Reasoning\nIn the default setting of LLMs, each task is an isolated attempt, often leading to inefficient cycles\nof repetitive trial-and-error in long-horizon planning. The introduction of memory mechanisms\nbridges this cognitive gap, transforming the agent from a static solver into a continuous learner\ncapable of drawing wisdom from history. By retaining successful trajectories and lessons from\nfailures, memory endows agents with experience-based reasoning. Depending on the mode of\nintervention, this capability is realized through two primary paradigms: (1) Strategic guidance\n(§3.3.1) assists the model in making superior plan by retrieving relevant experiences, guidelines, or\noptimized strategies (2) Procedural solidification (§3.3.2) solidifies high-frequency successful paths\ninto workflows, code, or templates, enabling agents to bypass cumbersome planning processes and\nefficiently complete tasks by directly invoking procedural knowledge, as shown in Figure 1 (c).",
            "content": "Strategic Guidance. The first paradigm focuses on utilizing memory to assist reasoning, 3.3.1 guiding the model to generate correct actions by providing high-quality contextual references or internalized strategic intuition [47, 138, 148, 154, 253, 334, 352, 359, 381, 399]. Early pioneering work [244] introduced textual reflection mechanisms, where agents convert failure experiences into linguistic constraints that serve corrective function in subsequent reasoning. Following this, Zheng et al. [391] utilized memory to retrieve similar historical trajectories as exemplars, directly prompting the model to mimic past successful steps. As research advanced, this reference , Vol. 1, No. 1, Article . Publication date: December 2025. 10 Liang, et al. mechanism became more structured and dynamic. For instance, Lai et al. [159] assisted problemsolving in business scenarios by retrieving high-similarity guidelines If existing experience is insufficient, it employed stronger model to generate new guidelines that are actively populated back into memory. Recent studies further employ memory for deep policy optimization. Cai et al. [32] summarized sampled trajectory groups to extract advantageous experiences explaining why one trajectory succeeds while another fails, thereby guiding the optimal action selection during inference. [365] transformed experience into internalized intuition by contrasting expert and nonexpert actions from early exploration and combining them with implicit world model predictions."
        },
        {
            "title": "4 MEMORY CATEGORIZATION\nThe concept of memory originally stems from cognitive neuroscience, where it is broadly defined\nas the cognitive process by which the brain stores and manage information, including experiences,\nfacts, and skills, allowing this information to be accessed and used after the original stimulus or\nevent is no longer present, and is typically classified into short- and long-term memory (§4.1).",
            "content": "In LLM-driven agents, memory includes systems persistent past interaction information, which encompasses both perceptual records of environmental states and the history of interactions with the environment [244, 338]. Through the memory, agents can not only maintain contextual coherence and information consistency across multi-turn dialogues, but also extract valuable patterns and knowledge from historical experiences, thereby demonstrating adaptive learning and continuous improvement capabilities in task execution. Traditionally, memory in agent domains is divided into shortand long-term memory [23, 173, 181, 225, 242, 382]. However, with the enhancement of agent systems capabilities and generalizability, the traditional dichotomy is inadequate for describing the diversity and hierarchy of memory in current agent systems. Building on the pioneering work by Hu et al. [108], Zhang et al. [376], we propose more granular taxonomy that systematically categorizes agent memory along two fundamental dimensions (4.2). To more intuitively express our categorization, we have visualized specific examples in Figure 2. , Vol. 1, No. 1, Article . Publication date: December 2025. AI Meets Brain: Unified Survey on Memory Systems from Cognitive Neuroscience to Autonomous Agents 11 Fig. 2. Overview of the memory classification in agents. (a) Nature-based taxonomy that categorizes memory based on the type of information being encoded. (b) Scope-based classification that distinguishes memory according to how broadly it can be applied."
        },
        {
            "title": "4.1 Memory Classification in Cognitive Neuroscience\nIn cognitive neuroscience, memory can be divided into two main forms based on distinct temporal\nwindows of information processing, thereby subserving their respective cognitive functions: (1)\nShort-term memory (§4.1.1) enables the rapid, temporary maintenance and processing of incoming\ninformation, allowing an individual to respond promptly to the external environment and (2)\nLong-term memory (§4.1.2) is responsible for storing deeply processed experiences, which in\nturn can shape present cognition and guide future behavior. In addition, we further discuss the\ndistinction and interactions between them (§4.1.3).",
            "content": "Short-term Memory. Short-term memory refers to an information processing system that 4.1.1 temporarily maintains and manipulates small amount of information, with time window generally not exceeding 1520 seconds. Examples of using this memory include holding phone number in mind for few moments or keeping track of the last few sentences your conversation partner has just said. Due to limited cognitive resources [200], the capacity of short-term memory is constrained such that it can only maintain 49 pieces of information simultaneously. This ability varies substantially across individuals and influences human cognitive functions and behavioral performance, such as learning ability [19, 20], emotion regulation [239], and creativity [61]. When the amount of maintained information approaches this capacity limit, the brain dynamically reallocates its memory resources, prioritizing information that is more important or task-relevant and suppressing lower-priority representations [168]. If information is not transferred into longterm storage before this short-term time window closes, it is likely to be forgotten."
        },
        {
            "title": "4.1.2 Long-term Memory. Long-term memory supports the storage and management of large\namounts of information over extended periods. Its temporal span ranges from several minutes",
            "content": ", Vol. 1, No. 1, Article . Publication date: December 2025. 12 Liang, et al. to many years or even decades. Examples include recalling frequently used phone number or drawing on accumulated knowledge to offer original insights in conversation. Long-term memory provides an archive of past events and learned knowledge, which can be directly retrieved during interaction with the environment or used as background context that shapes perception and decision-making [195]. Unlike short-term memory, long-term memory is not characterized by strict capacity limit. Instead, one of its key properties lies in the dynamic nature of information processing, including how it interacts with short-term memory (4.1.3), how stored representations transform over time (5.1.2), and how information is managed (6.1). Based on the content of memories, long-term memory can be further divided into episodic memory and semantic memory. These two memories represent distinct types of information, with episodic memory encoding specific events and semantic memory storing abstract knowledge. This distinction is closely linked to significant differences in their underlying neural mechanisms. Furthermore, these memory systems do not operate in isolation within the brain but rather interact with each other and transform into one another over time. Episodic memory refers to memory for specific events that an individual has personally experienced. Such memories typically include not only detailed information about the event itself, but also its temporal and spatial contextthat is, when and where it occurred. Recalling an episodic memory is usually accompanied by subjective sense of mental time travel [254], in which individuals feel as though they are transported back to the original situation, re-experiencing the surrounding environment and event details. For instance, remembering the drive to newly opened cinema last week and the moment when someone made loud noises during the screening relies heavily on intact episodic memory. Semantic memory refers to memory for learned factual knowledge, concepts, and rules. These memories are not tied to specific time and place of acquisition, and their retrieval is not accompanied by vivid re-experiencing of particular past episode. For instance, knowing where familiar building is located, or recalling the personality traits of an actor who played given character in film, depends primarily on semantic memory."
        },
        {
            "title": "4.1.3 Distinction and Interaction. From a functional perspective, short-term memory and long-term\nmemory differ in the systems and timescales that support them. Short-term memory operates on\nnewly encountered information and maintains it over seconds [211], enabling rapid responses to the\nenvironment [202]. Long-term memory acts on information that has already been encoded, relying\non slower learning processes to retain experiences and knowledge for years or even decades [135].\nMore interesting than their differences are the interactions between them. In the classic multi-store\nmodel [8], short-term memory is described as a temporary “workspace” through which external\ninformation passes before entering long-term storage. Information that continues to be relevant is\nencoded into long-term memory, and when it is needed later, it is retrieved back into short-term\nmemory to support ongoing processing. At the neural level, the two also interact. On the one\nhand, items that elicit stronger activity during short-term memory maintenance are more likely\nto be consolidated into durable long-term memories [60], and higher working-memory load is\nassociated with stronger hippocampal–neocortical coupling [25]. On the other hand, long-term\nmemory provides priors and learned representational structures that shape how new information is\nencoded and maintained in short-term memory [48, 160]. Thus, short-term and long-term memory\nare not isolated subsystems but mutually influential components of a memory network.",
            "content": "The differences between episodic and semantic memory extend beyond their content and subjective sense to their neural mechanism. For example, the individuals with hippocampal damage often have great difficulty reconstructing the specific scenes of past events, yet their semantic memory is relatively preserved [216]. Evidence from imaging studies indicates that these two types , Vol. 1, No. 1, Article . Publication date: December 2025. AI Meets Brain: Unified Survey on Memory Systems from Cognitive Neuroscience to Autonomous Agents 13 of memory rely on partially dissociable brain systems. Episodic memory depends strongly on the hippocampus, whereas semantic memory is supported primarily by the neocortex [64, 91, 261]. In real life, episodic memory and semantic memory are usually intertwined and interact with each other. Repeatedly experiencing similar events allows the brain to extract stable structures and rules [185], gradually forming more abstract semantic knowledge and achieving transformation from episodic to semantic form. In turn, when we recall specific episode, existing semantic knowledge can serve as prior context to guide its reconstruction, filling gaps and sometimes distorting details [233]. Thus, although episodic and semantic memory are conceptually distinct, they continuously influence and reshape each other over time."
        },
        {
            "title": "4.2.1 Nature-based Classification. We observe that the nature of memory in agent systems closely\nparallels that found in cognitive neuroscience research. This nature is fundamentally determined\nby the type of information memory provides to subsequent reasoning processes. Building upon\nthis correspondence, we adopt the classical taxonomy from cognitive neuroscience and categorize\nagent memory into two types, namely episodic and semantic memory, as shown in Figure 2 (a).",
            "content": "Episodic memory refers to the agents experiential memory that stores sequential interaction trajectories and contextual information. This memory type is tool-augmented, maintaining detailed logs of what tasks were attempted, which tools were invoked, and what solution pathways were followed. It captures the procedural history of the agents problem-solving processes, including intermediate steps, tool call sequences, and decision branches, enabling it to learn from past execution patterns and optimize future task completion strategies. Semantic memory functions as the agents knowledge repository without tool dependencies. It stores factual information, concepts, rules, and general knowledge [213]. This memory type provides the foundational understanding necessary for reasoning and inference, containing declarative knowledge such as definitions, relationships between concepts, and general principles that guide the agents behavior. Distinction. The fundamental distinction between the two lies in the nature of the content they aim to convey. Episodic memory aims to convey experiential information, recording procedural knowledge of how to do things, while semantic memory aims to convey conceptual information, storing declarative knowledge of what things are. More importantly, this classification not only reflects the organizational structure of memory content but also reveals that agents need to coordinate two complementary cognitive strategies, experienceand knowledge-driven, when handling complex tasks. Scope-based Classification. Memory in agents can be classified based on its scope of appli4.2.2 cability. This scope-based classification determines whether memory is confined to single task or session, or extends across multiple tasks or sessions. Accordingly, it can be divided into two categories, namely inside-trail and cross-trail memory, as shown in Figure 2 (b). Inside-trail memory is confined to single trajectory execution. It stores context-specific information such as intermediate steps, temporary variables, and task-relevant observations that are only valid within the current episode [172, 193, 251, 307, 340, 346, 374, 398]. For instance, Zhou et al. [398] proposed maintaining temporary state information for each reasoning step during an , Vol. 1, No. 1, Article . Publication date: December 2025. 14 Liang, et al. agents problem-solving process, which was then propagated to subsequent steps. This memory is typically cleared or reset when the episode ends. Cross-trail memory persists across multiple trajectory executions, enabling agents to accumulate knowledge and experience over time. It stores generalizable patterns, learned strategies, and reusable knowledge that can inform future episodes [96, 213, 305, 369]. This persistent memory facilitates continual learning and adaptation. For example, Ouyang et al. [213] proposed long-term storage of historical successful and failed trajectories in memory repository, where each memory entry was organized into structured format consisting of title, description, and content, serving as an experiential knowledge base for subsequent task processing. Distinction. The key distinction lies in temporal range and reusability. Inside-trail memory is transient and trajectory-specific, providing essential working space for complex reasoning and multi-step problem solving within episodes. In contrast, cross-trail memory is persistent and generalizable, transforming historical trajectories into strategic knowledge to help agents systematically improve their performance over time."
        },
        {
            "title": "5.1 Memory Storage in Cognitive Neuroscience\nMemory storage in the human brain is not a unitary process but rather a dynamic interplay between\nmultiple systems operating at different timescales. Understanding how the brain retains information\nrequires distinguishing between short-term memory, which holds information temporarily for\nimmediate use, and long-term memory, which preserves experiences and knowledge over extended\nperiods. Although these two systems differ in their temporal scope and neural substrates, they are\ninterconnected with respect to memory storage. Short-term representations can be consolidated\ninto long-term traces, and long-term memories can be reactivated into short-term working states. In\nthe following sections, we examine the neural architecture and representational formats underlying\nshort-term memory (§5.1.1) and long-term memory (§5.1.2), drawing on converging evidence from\nneuroimaging, electrophysiology, and lesion studies in humans.",
            "content": "Short-term Memory Storage. Short-term memory, as temporary information storage sys5.1.1 tem, primarily processes small amount of information within limited time window. To deeply understand how short-term memory supports flexible cognition, however, it is necessary to consider where in the brain short-term information is stored and in what form it is represented and maintained. large body of neuroimaging and electrophysiological evidence has begun to outline the neural basis of short-term memory. On the one hand, it appears to rely on distributed storage scheme spanning sensory cortices and frontoparietal network (i.e, sensoryfrontoparietal network). On the other hand, at the cellular and circuit levels, information can be held via mechanisms such as persistent neural firing and activity-silent synaptic connection states. In this section, we briefly review recent work on short-term memory along two dimensions, namely its storage location and its underlying storage format. Storage Location. Human neuroimaging studies consistently show that short-term memory is supported by distributed brain system. After the stimulus disappears, information about the remembered items remains retained in the sensory cortices, posterior parietal cortex, and prefrontal , Vol. 1, No. 1, Article . Publication date: December 2025. AI Meets Brain: Unified Survey on Memory Systems from Cognitive Neuroscience to Autonomous Agents 15 regions. During the maintenance period, these areas display persistent activity or decodable patterns that match the memory content. This means short-term memory is not stored in one single region, instead, it relies on sensoryfrontoparietal network. Specifically, perceptual cortices tend to retain fine-grained sensory details, while the prefrontal cortex mainly regulates this storage process by setting priorities, allocating limited memory resources across items [95], and recoding information into formats that better meet upcoming behavioral demands [55]. In addition, frontoparietal network also supports cross-modal representations, allowing information from different sensory channels to be linked and manipulated in shared representational space [219]. Storage Format. At the cellular and circuit levels, short-term memory can be maintained by more than one format, which include: Persistent activity and Synaptic connection weights. Persistent activity refers to classic proposal in which neurons that encode the memory maintain high level of firing activity even after the external stimuli are removed. Singleneuron recordings in epilepsy patients provide direct human evidence for this idea, showing that neurons in the medial frontal and medial temporal lobes remain active after stimulus offset and their firing tracks the remembered content [140]. However, persistent activity is energy-demanding and may be vulnerable to interference. Synaptic connection weights refer to an alternative format for memory storage, where information can be temporarily maintained without sustained neural activity. In this framework, population firing of neurons can drop back to baseline while the memory enters an activity-silent state that standard recordings cannot easily decode [248]. more widely accepted view today is that both of the formats coexist, with the brain switching between them. High-priority items within the focus of attention are often kept in an active, persistently firing state, while items outside attention but still potentially useful can be stored silently and reactivated when needed [139]. To support this dual-mechanism hypothesis, Barbosa et al. [17] showed that electroencephalography (EEG) can decode memory content during maintenance, but not during the inter-trial interval, demonstrating how the representation becomes latent and later re-emerges."
        },
        {
            "title": "5.1.2 Long-term Memory Storage. Compared with the temporary maintenance of information by\nshort-term memory, the characteristic of long-term memory that it can preserve information for a\nlong time implies that it has a relatively more complex neural mechanism. Rather than preserving\nexperiences in their original details, the brain gradually transforms and reorganizes them across\nhippocampal–neocortical systems, giving rise to more structured and abstract representations. In\nthis section, we will introduce where in the brain long-term memories are stored, and in what\nstructural form they are represented. Specifically, we first summarize evidence on the division of\nlabor between the hippocampus and neocortex in long-term memory storage, and then discuss\nhow experiences are organized into structured units, such as event-based unit and cognitive map.\nStorage Location. Long-term memory storage depends on the coordinated activity of two key\nbrain regions, the hippocampus and the neocortex. When new information is acquired, it is initially\nencoded and maintained in distributed neocortical regions, whose signals then converge in the\nhippocampus for integrated processing. Rather than serving as a storage warehouse, the hippocam-\npus functions as an index that reactivates these distributed memory traces. Consequently, newly\nformed memories depend heavily on hippocampal support, but through systems consolidation, they\ngradually become sustained by neocortical networks [145]. Multiple findings support this view. For\nexample, Korkki et al. [155] demonstrated that hippocampal activity during encoding predicts how\nprecisely details will be stored, but later retrieval success depended more on prefrontal activity\nand the strength of neocortical representations. In another study, Himmer et al. [104] found that\nrehearsal strengthened memories mainly in neocortical and hippocampal–neocortical interaction",
            "content": ", Vol. 1, No. 1, Article . Publication date: December 2025. 16 Liang, et al. Fig. 3. Overview of memory storage mechanisms in cognitive neuroscience, including storage locations and storage formats of shortand long-term memory. regions, rather than in hippocampal activity alone. Moreover, when people experience events with overlapping contexts, the hippocampus and relevant neocortical regions show coordinated reactivation during rest. This replay helps write cross-event structures into medial prefrontal cortex (mPFC) representations [266].These cross-event structures can then serve as contextual knowledge or priors that shape the encoding of information in short-term memory [48, 160]. Storage Format. It is an obvious fact that memories cannot be preserved in their original details. During consolidation, the hippocampus integrates information and develops more abstract, structured representations, allowing memories to be stored in complex forms rather than as raw sensory data. Here we focus on two such forms: event-based units and cognitive maps. Event-based unit refers to the discrete memory representations that result from the brains segmentation of continuous, rich everyday experience into distinct episodes, allowing us to remember life as series of separate events rather than an unbroken stream. An fMRI study showed strong hippocampal responses at boundaries between experimentally defined events, suggesting that the brain automatically detects event transitions [18]. However, this segmentation does not produce entirely isolated memories. Even when plots, scenes, or locations change, events that occur close in time are often recalled as coherent stream. This continuity is facilitated by hippocampal-neocortical replay at the end of an event, which reinstates what just happened and helps integrate information across boundaries [245]. Beyond temporal integration, complete event also appears to have holistic representation. Although its constituent elements are distributed across neocortical areas, the hippocampus can bind them into unified memory trace [198]. Consistent with this, human hippocampus , Vol. 1, No. 1, Article . Publication date: December 2025. AI Meets Brain: Unified Survey on Memory Systems from Cognitive Neuroscience to Autonomous Agents 17 contains event-specific neurons that fire strongly during encoding and retrieval of particular episodes. Their activity reflects the event as whole rather than isolated features [153]. Cognitive map refers to the mental representations built by the hippocampusentorhinal circuit, which not only tracks locations and routes during physical navigation but also serves broader function in organizing abstract knowledge, concepts, and experiences. Within such maps, past experiences are represented as points in multidimensional space. The construction of cognitive maps involves several key processes. First, the brain establishes an internal coordinate system for abstract concepts that remarkably resembles the one it uses for physical space. For instance, during memory for two-dimensional conceptual spaces, entorhinal cortex and ventromedial prefrontal cortex (vmPFC) show hexagonally symmetric signals, similar to grid-cell coding in navigation [56]. Related coordinate-like representations have also been found for social relationships [222]. Second, the hippocampusentorhinal system can encode distances between knowledge units. Neural signal strength reflects relational or path-like distance in the cognitive space [84]. This principle extends to event memories, which can be arranged into temporal maps where distances correspond to transition probabilities over time [255]. Third, through consolidation, these maps are gradually transferred to and maintained in neocortical regions. It has been recorded that medial prefrontal cortex stores learned abstract relations [16], while other frontal and cingulate regions encode social and functional attributes that organize long-term knowledge [301]."
        },
        {
            "title": "5.2 Memory Storage in Agents\nUnlike the human brain with its complex structures and rich biological information, memory\nstorage in agents is fundamentally based on natural language symbols. Therefore, rather than\ndiscussing storage by memory type, we directly introduce two key dimensions of the storage: (1)\nStorage location (§5.2.1) and (2) Storage format (§5.2.2).",
            "content": "Storage Location. From the perspective of storage mechanisms, memory in agents can be 5.2.1 organized in two primary ways. The context window serves as container that dynamically folds and updates information within single reasoning trajectory. In contrast, memory banks function as external repositories that persistently store accumulated information. Context Window. The context window can serve as container for memory storage. Information placed within the context, including user inputs, tool invocation result, and intermediate generation steps, can be transformed into inside-trail memory that the agent directly accesses during reasoning. This process is defined as memory folding [172, 193, 251, 307, 340, 346, 374, 398], which is triggered during the reasoning process, enabling the agent to dynamically update its state of knowledge within its trajectory. For example, Wu et al. [307] proposed periodically calling summarization tool to compress accumulated interaction history into structured summary, allowing the agent to resume reasoning from the compressed state and thus mitigating truncation issues caused by context window limitation. Building upon this concept, some works [172, 374] have further proposed active folding strategies. For example, Li et al. [172] proposed to dynamically determine whether to initiate memory folding during reasoning by generating special trigger token, thereby providing greater flexibility and adaptability for memory management. Memory Bank. Beyond context-based storage, agent systems commonly incorporate external memory modules as dedicated repositories. Unlike the context window, such memory module [54, 82, 196, 213, 259, 260] possess theoretically unbounded capacity and can persistently retain the agents cross-trail memory, which includes experiential data and domain knowledge accumulated across multiple sessions and tasks. This type of memory storage mode is typically triggered at the end of trajectory, granting memory reusability, ensuring that previously acquired useful , Vol. 1, No. 1, Article . Publication date: December 2025. 18 Liang, et al. information remains accessible and retrievable even after conversation termination or system restart. Specifically, some memory banks are designed to persistently store user preferences, knowledge, and conversation history. For example, Tan et al. [259] introduced framework that organize the memory bank in topic-based manner, enabling cross-session tracking of users health conditions, allergies, and personal preferences for continuous personalized service. Other memory banks focus on distilling reusable strategies and reasoning patterns from agents historical interactions. For instance, Ouyang et al. [213] suggest to extract high-level reasoning strategies from both successful and failed experiences, storing abstracted decision principles rather than raw trajectories. Building upon these approaches, cross-agent shared memory banks transcend the boundaries of individual agents, enabling experience sharing across different frameworks. Tang et al. [260] propose plugand-play knowledge sharing by abstracting execution trajectories from various agent systems into unified structured experience units, accessible through lightweight APIs. This design addresses the isolation problem in current agent systems, allowing solutions discovered in one framework to be directly reused by others and avoiding redundant trial and error. Storage Format. Memory representations in agents are typically divided into four categories, 5.2.2 including text stored in conventional natural language, graphs that emphasize preserving structured relationships, parametric memory internalized in model weights, and latent representations in vector form. Text. Natural language text is the most common storage format for agent memory. Memories are stored as either raw text or summarized text, encompassing both experiences and information [15, 107, 306, 326, 363, 370, 371]. This format offers high interpretability, ease of manipulation, and direct compatibility with language model architectures. Although stored in textual form, conversational or task-related processes are typically not preserved verbatim but undergo certain degree of abstraction and summarization. For example, Hu et al. [107] proposed decomposing tasks into multiple sub-goals, with each sub-goal completion triggering consolidation of multiple steps into summary that is then stored in the memory container. In contrast, Zhang et al. [370] argued that excessive abstraction and compression of memory information causes agents to suffer from brevity bias, leading to degraded performance in specialized domains. They therefore proposed the playbook memory storage mechanism, which minimizes memory compression and preserves detailed information to the greatest extent possible. Overall, text-format memory facilitates retrieval and combination while fully leveraging the language processing capabilities of large language models for reasoning. Graph. Graph-based memory storage organizes memory into structured network composed of entities and relationships [6, 142, 212, 235, 292, 308, 312]. In this format, experiences or information are decomposed into nodes (e.g., concepts, objects, events, or steps) and edges (e.g., relationships). The graph structure excels at supporting complex reasoning tasks by allowing the system to traverse edges between nodes and concatenate multiple episodes into the most effective memory as experience. For instance, Chhikara et al. [53] constructed relational graph to explicitly model temporal and logical relationships between entities, demonstrating strong performance in tasks requiring multi-step reasoning by effectively tracking event sequences and character interactions. Zhang et al. [361] employed three-tier graph hierarchy to manage the lengthy interaction history of multi-agent systems, retrieving high-level, generalizable insights and fine-grained collaboration trajectories through bi-directional memory traversal. Furthermore, the graph structure inherently supports relationship extraction and pattern discovery, enabling the identification of implicit connections between nodes and thereby assisting complex logical information queries. For example, Lei et al. [164] constructed the dialogue history into knowledge graph and executes multi-step graph traversal searches, making the reasoning process traceable and faithful to the dialogic facts. , Vol. 1, No. 1, Article . Publication date: December 2025. AI Meets Brain: Unified Survey on Memory Systems from Cognitive Neuroscience to Autonomous Agents 19 Long et al. [189] proposed an entity-centric multimodal graph to store memories of faces, voices, and text, connecting nodes across different modalities through identity equivalence detection to maintain long-term consistency. Overall, graph-structured memory enhances reasoning performance and consistency in complex scenarios through its inherent relational representation capabilities. Parameters. Parametric storage embeds memories directly within model weights, integrating experience and knowledge into the neural architecture [59]. This approach solidifies information through persistent modifications of connection strengths, thereby emulating synaptic plasticity in biological brains. Its primary advantages include exceptional access efficiency and deep knowledge integration, as memory is automatically activated during forward propagation, which eliminates retrieval latency and cross-modal alignment costs. This internalization of experience into neural weights is primarily driven by imitation and reinforcement learning. In terms of imitation learning, agents utilize supervised fine-tuning to shape initial parametric representations. For instance, Liu et al. [179] employed distillation to solidify long-term knowledge within weights, while Sun et al. [252] distilled expert trajectories to transform operational skills into parametric instincts. Conversely, reinforcement learning achieves experience-driven learning. Under this paradigm, agents treat sampled trajectories as episodic memories. Specifically, Bai et al. [12], Qi et al. [224], Wei et al. [297] successfully internalized explored strategies into parameters by extracting performance advantages from these samples. Furthermore, Zhang et al. [365] introduced an intermediate paradigm that enables agents to learn from self-interaction without explicit rewards by using future states of exploratory actions as supervision signals to internalize experience. In summary, by internalizing experiences into model weights, parametric storage eliminates retrieval latency and enhances decision stability, thereby enabling rapid and consistent agent responses. Latent Representation. Beyond token-space memory structures that explicitly represent information and parametric memory, several studies [171, 284] have explored latent memory, which stores information as high-dimensional vectors in embedding space. Compared to token-level memory, latent memory offers several key advantages: (1) Efficient compression: high-dimensional continuous vectors can encode information more compactly than discrete tokens, thereby reducing storage and computational overhead [229, 285]. (2) End-to-end trainability: as continuous representations, latent memories can directly participate in gradient-based optimization, enabling updates and refinements during training [85, 345]. (3) Alignment with human cognition: as noted in [76, 98], human reasoning relies on integrated representations that transcend discrete symbols, latent space representations effectively embody this principle. Representative methods include MemoryLLM [285] and M+ [287], which allocated fixed set of latent vectors as internal memory at each layer of LLMs. As the context evolves, these latent memories are iteratively retrieved and updated, then concatenated with the hidden states at each layer and integrated into the models forward computation, thereby enabling the retention and utilization of information from long documents. Building upon this, Zhang et al. [362] further extended this approach to the domain of agent memory. It fed the hidden state of the agents current action into trigger module to determine whether latent memory needs to be invoked. When triggered, the agent leveraged the current actions hidden state to retrieve relevant latent memories from Weavers parameters, dynamically steering the models subsequent actions and enabling continuous self-evolving behavior."
        },
        {
            "title": "6 MEMORY MANAGEMENT\nMemory management represents a dynamic regulatory framework governing the entire information\nlifecycle, ensuring that intelligent systems can adaptively filter noise and preserve pivotal patterns\nwithin changing environments. Whether through the consolidation of labile traces in biological\nbrains or the regulation of extensive storage in artificial systems, effective management mechanisms",
            "content": ", Vol. 1, No. 1, Article . Publication date: December 2025. 20 Liang, et al. are central to transforming raw perception into structured knowledge. This chapter provides crossdisciplinary analysis of memory management, beginning with its neuroscientific foundations (6.1) and proceeding to the systematic implementation of information-flow management in autonomous agents (6.2)."
        },
        {
            "title": "6.1 Memory Management in Cognitive Neuroscience",
            "content": "Fig. 4. Overview of memory management in cognitive neuroscience. The framework illustrates dynamic cycle of information processing including memory formation, updating, and retrieval, through which longterm memory supports flexible adaptation to the external environment. In the field of cognitive neuroscience, long-term memory exhibits more sophisticated dynamic processes over extended temporal scales compared to short-term memory which involves only the transient maintenance of information within scale of seconds. Memory management discussed in this section focuses primarily on long-term memory. As illustrated in Figure 4, management processes, which consist of formation, updating, and retrieval, interact reciprocally with storage mechanisms to constitute dynamic cycle of information processing. As the functional inception of this cycle, memory formation transcends rudimentary encoding as it is intrinsically intertwined with consolidation and integration, which collectively shape the content and structural architecture of stored information. Conversely, updating and retrieval act as functional drivers that re-initiate the formative cycle by triggering the re-encoding and reconsolidation of existing memory traces. This section will first delineate how memory representations are instantiated and gradually stabilized through the lens of memory formation (6.1.1), and then address the regulatory roles of updating (6.1.2) and retrieval (6.1.3) within the memory system. , Vol. 1, No. 1, Article . Publication date: December 2025. AI Meets Brain: Unified Survey on Memory Systems from Cognitive Neuroscience to Autonomous Agents"
        },
        {
            "title": "6.1.1 Memory Formation. Long-term memories do not emerge as fully instantiated entities at the\nmoment of experience. Instead, they develop through a sequential process of encoding, consolidation,\nand integration. Initially, ongoing events are encoded as specific neural activity patterns across\nhippocampal and neocortical circuits. Subsequently, during periods of wakeful rest and sleep,\nthe hippocampus replays these events to reactivate the brain’s initial activity patterns, thereby\nstabilizing memory representations through system consolidation. Over time, memories undergo\ntemporal compression and cross-episodic linkage, ultimately being stored in the neocortex as\nabstract structural representations. In the following, we sequentially delineate these three processing\nstages that underpin memory formation.",
            "content": "Encoding. During an experience, neuronal ensembles in the hippocampus and neocortex with elevated intrinsic excitability are selectively recruited, transforming event information into memory codes through excitatory population firing. In this process, the hippocampus binds the distributed sensory features within the neocortex into unified representation and selectively modulates its interaction with sensory cortices to amplify representations of high future utility [113, 232]. This encoding phase engages long-term synaptic plasticity. Following the Hebb principle [26, 80], synchronized preand post-synaptic activity strengthens connections (LTP), whereas desynchronized activity leads to weakening (LTD). Such persistent modifications in synaptic efficacy underpin the biological foundation of long-term memory storage. Consolidation. System consolidation is essential for newly formed memories to achieve longterm stability and retrieval precision [350]. During offline states such as wakeful rest or sleep, the hippocampus spontaneously replays recent experiences [29, 256], triggering the reactivation of neural patterns within hippocampal-neocortical circuits [78, 137]. Throughout these replay events, transient bursts of hippocampal activity synchronize with rhythmic neocortical activity, enabling the two regions to fire together with high temporal precision [58]. Within this inter-regional dialogue, the hippocampus projects discrete recent experiences to the neocortex, utilizing existing neocortical knowledge structures as scaffold to reorganize and align this new information for integration into the long-term storage framework [9, 316]. Integration. Integration serves as the culmination of the memory formation cycle, functioning to assimilate consolidated, discrete memories into the brains broader knowledge structures. This process is intrinsically linked to encoding and consolidation; while encoding captures information and consolidation preserves it, integration transforms these stabilized traces into organized, relational knowledge. Facilitated by hippocampal replay, memories undergo qualitative transformations where event sequences are temporally compressed [112] and overlapping elements across distinct episodes are cross-linked [265]. During this stage, the hippocampus and the medial prefrontal cortex (mPFC) collaborate to identify and construct logical associations between events. Integrated information is then gradually redistributed to specific neocortical regions, supporting more enduring and abstract forms of storage."
        },
        {
            "title": "6.1.2 Memory Updating. Once memory representations are established and stabilized through\nthe aforementioned formation processes, they do not remain static but are subject to continuous\ndynamic evolution. To adapt to an ever-changing environment, humans must constantly refine\nand update their stored knowledge. During this process, individuals generate expectations about\nupcoming events based on prior memories. When a discrepancy arises between reality and these\nexpectations, the resulting prediction error serves as a pivotal signal that triggers memory updating,\ndriving the brain to reshape previously stabilized representations [162].",
            "content": "Memory updating transcends the mere replacement of outdated information, instead relying on sophisticated mechanisms such as differentiation and integration. On one hand, updating can be achieved through differentiation, where the hippocampus generates distinct and mutually repulsive , Vol. 1, No. 1, Article . Publication date: December 2025. 22 Liang, et al. neural representations for highly similar events. This process preserves mnemonic specificity while mitigating interference between old and new information [37, 342]. On the other hand, updating frequently involves integrative mechanisms. According to the integrative updating theory proposed by [270], individuals organize new information, retrieved prior memories, and the associated prediction errors into unified, structured representation. salient example is the correction of misinformation, where individuals can successfully retain the factual truth while simultaneously maintaining memory of the original fake news and its discrepancy from the facts [144]."
        },
        {
            "title": "6.1.3 Memory Retrieval. Although memories are encoded and stored through the aforementioned\nprocesses, they typically persist in a latent state, influencing current cognitive processing and\nbehavior only upon successful retrieval. This process is generally initiated by external cues or\ncontextual information. For instance, encountering a stimulus that resembles a constituent element\nof a past experience, or re-entering a familiar environment, acts as a functional trigger. Through\nthe mechanism of hippocampal pattern completion, these partial cues facilitate the reconstruction\nand retrieval of the entire episodic representation [90].",
            "content": "The Neural Basis of Retrieval. At the neural level, retrieval involves rapid and temporally compressed replay of episodic content stored within the hippocampus. The sequence of these replays is flexible and adapts to specific tasks. For instance, following the acquisition of sequential items, tasks requiring the recall of items preceding cue trigger reverse hippocampal replay, whereas tasks focusing on subsequent items elicit forward replay matching the original experience [300]. As replay unfolds, the associative representations between events and their contexts are typically reconstructed in the hippocampus prior to the emergence of specific details. Subsequently, hippocampal burst activity guides the neocortical reinstatement of fine-grained episodic content [214, 269], even including representations at the sensory feature level [75]. As mnemonic content expands temporally, various neocortical regions are sequentially engaged to support the dynamic reconstruction of naturalistic autobiographical memories [88]. The Impact of Retrieval on Existing Memory Storage. Retrieval is not passive readout of stored information but transformative process that modifies memory traces. According to reconsolidation theory, the retrieval of memory renders the trace labile within temporary window, allowing it to be strengthened, weakened, or updated during subsequent restabilization. Repeated retrieval of an event causes the co-encoded contextual background to be repeatedly reactivated, which progressively evolves into potent retrieval cue, thereby enhancing future retrieval success and memory durability [236]. This facilitation extends to other related memories sharing the same context through similar reactivation processes [136]. Furthermore, retrievalrelated enhancement promotes the integration of new and old information. When these memories are interrelated, retrieval practice of new information induces the alternating reinstatement of both encoding contexts [28]. During this process, elevated activity in the mPFC facilitates the differentiation and integration of these representations, leading to more robust storage of new memories and biasing or partial overwriting of older traces [342]. However, this plasticity can have maladaptive consequences, such as the reconsolidation of erroneous details introduced during retrieval, process governed by the functional balance between conflict-processing networks and sensory integration systems [217]. The Facilitative Effect of Retrieval on Subsequent Encoding. Beyond modifying existing traces, retrieval serves as functional primer that enhances the encoding efficiency of subsequent new information. Mechanistically, retrieval engages frontal-hippocampal networks to optimize the brains cognitive state, thereby directly improving the re-encoding and integration of incoming information [236, 267]. Additionally, retrieval practice activates rewardand prediction-related regions, including the ventral striatum, insula, and midbrain. This activity shifts the brain into , Vol. 1, No. 1, Article . Publication date: December 2025. AI Meets Brain: Unified Survey on Memory Systems from Cognitive Neuroscience to Autonomous Agents predictive learning mode, heightening both motivation and sensitivity to novel experiences [41]. Notably, this facilitative effect is double-edged sword: while it improves general learning efficiency, it may also expedite the internalization of misleading information if the subsequent input is inaccurate."
        },
        {
            "title": "6.2 Memory Management in Agents",
            "content": "Fig. 5. Overview of memory management in agents. The framework forms closed-loop pipeline consisting of memory extraction, updating, retrieval, and utilization, enabling persistent experience regulation and long-range reasoning. Unlike standard large language models that perform transient processing within restricted windows, agents implement persistent regulation of experiences via explicit management mechanisms. As illustrated in Figure 5, memory management serves as cognitive operating system that establishes closed-loop pipeline comprising extraction, updating, retrieval, and utilization. Within this cycle, extraction transforms interaction streams into structured records, while updating and retrieval ensure knowledge validity and precise localization, respectively. Utilization subsequently leverages these experiences for decision support. This framework enables agents to evolve from stateless responders into continuous learners capable of long-range reasoning. In the following, we sequentially examine these four critical stages."
        },
        {
            "title": "6.2.1 Memory Extraction. Echoing the encoding and integration mechanisms in cognitive neu-\nroscience, memory extraction in agents serves as the initial phase of memory management. Its\ncore lies in distilling compact and meaningful content from the overwhelming and redundant\ninformation stream to construct usable memory representations. Based on the levels of information\nabstraction and processing methodologies, existing memory extraction methods can be broadly\ncategorized into three paradigms, namely flat extraction, hierarchical extraction, and generative\nextraction.",
            "content": "Flat Extraction. Flat extraction constitutes the foundational paradigm of memory construction, characterized by the direct recording of raw information into storage or the application of lightweight preprocessing such as summarization and segmentation [86, 321]. For instance, Packer et al. [215] drew inspiration from hierarchical storage concepts in operating systems, utilizing , Vol. 1, No. 1, Article . Publication date: December 2025. 24 Liang, et al. recall storage and archive storage to preserve complete dialogue histories and segmented document fragments, respectively, thereby constructing virtual infinite context. Similarly, Shen and Joung [241] deposited historical task execution data directly into long-term memory, facilitating cross-task experience reuse and reasoning enhancement through retrieval mechanisms. However, the direct storage of raw trajectories often incurs the risk of error accumulation. To address this, Cheng et al. [51] proposed decoupled extraction strategy that distilled continuous interaction streams into independent memory units across multiple dimensions, effectively blocking the propagation of local reasoning errors from the original long chains to subsequent decisions, while Ouyang et al. [213] focused on extracting high-level reasoning patterns from interaction trajectories to establish scalable reasoning memory, thereby driving agent self-evolution. Hierarchical Extraction. Hierarchical extraction organizes fragmented information into ordered structures through multi-granular abstraction mechanisms, aiming to emulate the human cognitive ability to flexibly switch between macro-contexts and micro-details [2, 42, 114, 208, 375, 395]. In long-document processing, Lee et al. [163], Li et al. [171] employed recursive compression strategies, extracting high-level gist memory or constructing hierarchical semantic networks from raw text blocks, while indexing low-level details only when necessary to balance understanding depth and breadth. In interaction and dialogue scenarios, the extraction process typically adheres to the distinction between episodic and semantic memory. Hassell et al. [99], Wang et al. [289], Yu et al. [349], Zhang et al. [372] encoded interaction streams into specific episodic memory (e.g., timestamped events, graph-based behavioral trajectories) and abstracted semantic memory (e.g., user profiles, factual knowledge, or core summaries). This dual-coding mechanism was similarly applied by Wang et al. [275] to extract code repository commit histories versus functional summaries. For complex task planning, extraction focuses on decoupling high-level planning from low-level execution. Han et al. [96], Ye et al. [341] stored macroscopic instructions from planners separately from specific actions of executors, while Yang et al. [328] distinguished between high-level strategies (e.g., dilemma solutions) and low-level procedures (e.g., subtask standard operating procedures (SOPs)) to facilitate fine-grained knowledge transfer. Unlike methods relying on static predefined structures, Xu et al. [324] drew on the zettelkasten method, constructing self-organizing dynamic hierarchical memory through active context linking and knowledge evolution. Generative Extraction. Diverging from the aforementioned paradigms that rely on external storage or hierarchical organization, generative extraction aims to dynamically reconstruct context during the reasoning process, thereby mitigating the computational overhead and attention dilution associated with excessive context length. Zhou et al. [398] enhanced context integration capabilities through training, compressing historical memory and reasoning into shared representation space prior to each interaction, thereby facilitating continuous interaction without reliance on external memory modules. In contrast, Wu et al. [307] employed specialized summarization model as tool, which was actively triggered to condense think-action-observation trajectories whenever context usage approached predefined threshold. Distinguishing itself from linear summarization approaches, Sun et al. [251] introduced branch and return operators, allowing agents to construct isolated sub-contexts for specific subtasks and inject only concise post-execution summaries back into the main context. Furthermore, Ye et al. [340] advanced this by training granular condensation and deep consolidation capabilities. This enables not only the extraction of summary chunks from individual interactions but also the recursive merging of multiple chunks into single abstract representation when specific sequences become irrelevant (e.g., upon subtask completion or path abandonment), significantly compressing context size while preserving critical reasoning outcomes."
        },
        {
            "title": "6.2.2 Memory Updating. While memory extraction (§6.2.1) converts raw interaction streams into\nstructured records, the resultant memory is not a static repository but a dynamic process of",
            "content": ", Vol. 1, No. 1, Article . Publication date: December 2025. AI Meets Brain: Unified Survey on Memory Systems from Cognitive Neuroscience to Autonomous Agents 25 reconstruction. Echoing the mechanisms of reconsolidation and memory updating in cognitive neuroscience, memory updating in agents aims to balance the intake of newly extracted information with the elimination of the obsolete, ensuring system plasticity and efficiency. This process unfolds across two distinct layers: (1) Inside-Trial Updating, which focuses on the dynamic refreshing of the immediate context window (working memory) during specific task execution, and (2) Cross-Trial Updating, which governs the lifecycle management of long-term archives (external memory) to persist knowledge across different episodes. Inside-Trial Updating. Inside-trial updating primarily operates on working memory to address the challenges of information decay and overload within limited context windows, ensuring high information density throughout continuous interactions [87, 119, 127, 184, 201, 257]. Mimicking human selective attention, Yu et al. [346] proposed filter-based update strategy that reshaped the context stream by real-time assessment, retaining only critical details while discarding irrelevant noise. In dynamic scenarios involving external tools, the focus of update shifts to just-in-time resource management. Lumer et al. [193] introduced an Add-Delete-Manage strategy to dynamically remove irrelevant tools and retrieve new ones within fixed window, balancing memory overhead with task success rates. Furthermore, Nguyen et al. [209] treated memory updating as cognitive operation actively invoked by the model, triggering summarization actions at specific junctures to compress lengthy historical trajectories into refreshed, compact context. For more methodologies leveraging generative mechanisms for context updates, please refer to Generative Extraction (6.2.1). Cross-Trial Updating. Cross-trial updating fundamentally represents the dynamic iteration and quality maintenance of the agents external knowledge base, aiming to resolve the conflict between infinite knowledge expansion and limited storage capacity over time [35, 150, 170, 281]. To maintain the freshness and high value of knowledge, fundamental update strategies primarily focus on establishing mechanisms for selective retention and forgetting. Liu et al. [180] eliminated redundant ideas through consistency maintenance within buckets. Chen et al. [46], Liang et al. [174] introduced biologically inspired forgetting mechanisms, utilizing the Ebbinghaus forgetting curve [69] and competition-inhibition theory [63], respectively, to automatically phase out lowvalue or obsolete knowledge fragments, while Fang et al. [73] ensured efficient knowledge retrieval through lightweight pruning strategies. Building on this to achieve deeper knowledge integration and structuring, [32] leverages semantic understanding to perform fine-grained refinement of the experience pool, removing the coarse and extracting the essential. Cai et al. [34] employed dedicated updater agent to semantically merge new knowledge with existing entries and insert them into the correct hierarchical positions, ensuring the orderly expansion of the knowledge system. Furthermore, to enable knowledge update strategies to adapt to complex and changing task environments, Wang et al. [289], Yan et al. [327] moved beyond static rules, utilizing reinforcement learning to train agents to autonomously explore optimal policies for knowledge retention and forgetting. Ultimately, Nachkov et al. [206] posited that knowledge updates should not be limited to passive adaptation but should be driven by autonomous goals set by the agent, thereby achieving true self-evolution through the active reconstruction of knowledge."
        },
        {
            "title": "6.2.3 Memory Retrieval. Once the knowledge base has been refined and maintained through\nMemory Updating (§6.2.2), Memory Retrieval serves as the critical bridge between these retained\nexperiences and dynamic decision-making. Crucially, while retrieval in cognitive neuroscience is\nan active, transformative process that often involves reconsolidation to reshape the memory trace,\nretrieval in current agent architectures is primarily implemented as a selective activation mechanism.\nDriven by current contextual cues, this process selectively activates relevant information from the\nstorage. By filtering irrelevant noise, it enables agents to leverage the vast knowledge repositories",
            "content": ", Vol. 1, No. 1, Article . Publication date: December 2025. 26 Liang, et al. within limited context windows. Based on the dimensions of retrieval strategies, existing methods are categorized into two paradigms: (1) Similarity-based Retrieval, which prioritizes semantic matching, and (2) Multi-factor Retrieval, which integrates multidimensional metrics such as recency and importance. Similarity-based Retrieval. Similarity-based retrieval represents the dominant paradigm in current agent memory retrieval, grounded in the core hypothesis that memory relevance is equivalent to semantic similarity. Such approach typically utilizes encoders to map current queries into high-dimensional vectors, calculating cosine similarity within the vector space to recall the top-𝑘 nearest fragments from storage. This retrieval-augmented generation (RAG) [165] paradigm is widely adopted in question-answering and conversational agents, providing models with an efficient pathway for non-parametric knowledge access [118]. However, while similarity-based retrieval excels in handling declarative knowledge, Kohar and Krishnan [152] pointed out that direct embedding matching often fails to capture the structured operational logic inherent in procedural memory, thereby hindering agents from transferring complex behavioral strategies based solely on surface-level similarity. Multi-factor Retrieval. Multi-factor retrieval transcends the singular semantic dimension, moving beyond reliance solely on content similarity to determine memory prioritization based on factors such as recency, importance, structural efficiency, and expected rewards [79, 105, 132, 367, 379]. As foundational work in this paradigm, Park et al. [221] established composite scoring system synthesizing recency, importance, and relevance to simulate human memory decay and salience. Building on this, Liu et al. [178] addressed resource-constrained multi-agent scenarios by introducing token-budget-aware routing strategy, which calculates importance scores based on role relevance and task stage priority to distribute high-value information within limited windows. To resolve efficiency bottlenecks and fragmentation in global scanning, retrieval mechanisms have evolved towards structured approaches. For instance, Sun and Zeng [250] constructed semantic hierarchical structure using top-down indexing to bypass expensive global computation, while Xu et al. [319] linked multi-granular representations via association graphs to dynamically retrieve memory at the most appropriate granularity. Furthermore, to align retrieval directly with decision quality, Zhou et al. [396] moved beyond superficial similarity matching by encoding historical outcomes into expected Q-value rewards, utilizing online soft Q-learning to retrieve exemplars that maximize expected utility. Ultimately, [383] elevated the retrieval paradigm from passive information querying to active cognitive guidance, unlocking latent memory through strategydriven prompting, marking shift in agent retrieval mechanisms towards functioning as human-like cognitive guides."
        },
        {
            "title": "6.2.4 Memory Application. Following Memory Retrieval (§6.2.3), the focus of memory management\nshifts to its application in guiding behavior. Mirroring the brain’s dual mechanisms of employing\nshort-term memory for immediate reasoning and consolidating long-term experiences into cortical\nconnections, memory application in agents falls into two primary paradigms, namely contextual\naugmentation and parameter internalization.",
            "content": "Contextual Augmentation. Standard Retrieval-Augmented Generation paradigms typically concatenate retrieved text statically into prompts, treating memory as passive reference [165]. However, effective utilization of agentic memory goes beyond simple concatenation [1, 7, 187, 203, 358]. It serves as core mechanism for synthesizing fragmented information in long-document understanding, maintaining consistent personas for long-term personalization, and actively reusing past experiences for reasoning. To realize such dynamic cognitive capabilities, Yan et al. [325] introduced Just-in-Time compilation, building task-optimized contexts from lossless storage to , Vol. 1, No. 1, Article . Publication date: December 2025. AI Meets Brain: Unified Survey on Memory Systems from Cognitive Neuroscience to Autonomous Agents 27 resolve detail loss in long documents. Furthermore, Zhou et al. [398] compressed historical interactions into shared representation space acting as cognitive scratchpad, enabling deep synergy between memory and reasoning, while Yeo et al. [343] extended this mechanism to multimodal video streams, adaptively integrating memories across temporal scales for long-horizon planning. Parameter Internalization. Drawing from lifelong learning [390], this paradigm consolidates explicit memory into implicit parameters. Initiating with retrieval efficiency, Liu et al. [179] proposed distilling long-term memories into lightweight models for low-cost experience recall. This transformation of memory-to-model is subsequently leveraged to drive agent self-evolution. For instance, Bai et al. [12] utilized raw interaction history for reinforcement learning in open environments, while Qi et al. [224] used perplexity to filter appropriate difficulty trajectories for more efficient adaptive training. Scaling from individual evolution to collective intelligence, Sun et al. [252] employed hierarchical distillation strategy to transfer trajectory experiences from multiple specialists to generalist model. Finally, to move beyond mere imitation, Zhang et al. [365] exploited the value of early exploration memories, using contrastive training between non-expert and expert paths to help models understand the causal superiority of expert decisions."
        },
        {
            "title": "7.1 Benchmarks for Semantic-oriented\nSemantic-oriented benchmarks focus on how an agent constructs, maintains, and utilizes its inter-\nnal memory state (e.g., fact and conception). These benchmarks exhibit a hierarchical evaluation\nstructure, starting with the most basic memory capability, examining the agent’s accurate retention\nand retrieval of information across multiple rounds of dialogue or long text narratives. Representa-\ntive benchmarks include LoCoMo [194], LOCCO [124], BABILong [157], MPR [380], RULER [106],\nHotpotQA [336], PerLTQA [68], MemDaily [377]. These benchmarks target agents with external\nmemory storage capabilities, evaluating their ability to accurately retrieve historical details and\nmaintain semantic fidelity as memory span increases and distracting information accumulates,\nwhile avoiding information distortion caused by retrieval noise.",
            "content": "Some other benchmarks evaluate memory from process perspective. They focus on how information states are continuously updated and synchronized in real time as the environment evolves. Representative benchmarks include MemBench [258], LongMemEval [304], MemoryBank [392], , Vol. 1, No. 1, Article . Publication date: December 2025. 28 Liang, et al. Links Attribute Task Benchmarks GitHub Task Type LoCoMo [194] HomePage (cid:209) HomePage GitHub LOCCO [124] PersonaMem [125] LongBench [13] LongBench v2 [14] BABILong [157] RULER [106] MM-Needle [279] DialSim [147] Cross-Session Reasoning Memory Competency Evaluation Multi-level Memory Evaluation Long-term Memory Abilities Long-term AI Companion Evaluation Continual Learning Hallucination Evaluation in Memory Self-Evolving memory Dynamic conversation Long-term Memory Abilities Fid. Dyn. Gen. MemoryAgentBench [109] (cid:209) HomePage GitHub (cid:209) HomePage GitHub MemBench [258] (cid:209) HomePage GitHub LongMemEval [304] (cid:209) HomePage GitHub MemoryBank [392] (cid:209) HomePage GitHub MemoryBench [5] (cid:209) HomePage GitHub HaluMem [39] (cid:209) HomePage GitHub Evo-Memory [296] LTMBenchmark [36] (cid:209) HomePage GitHub (cid:209) HomePage GitHub (cid:209) HomePage GitHub Dynamic User Profiling and Personalization (cid:209) HomePage GitHub (cid:209) HomePage GitHub (cid:209) HomePage GitHub (cid:209) HomePage GitHub (cid:209) HomePage GitHub - (cid:209) HomePage LifelongAgentBench [389] (cid:209) HomePage GitHub (cid:209) HomePage GitHub PrefEval [385] (cid:209) HomePage GitHub MPR [380] (cid:209) HomePage GitHub StreamBench [303] (cid:209) HomePage GitHub Madial-Bench [102] (cid:209) HomePage GitHub HotpotQA [336] (cid:209) HomePage GitHub PerLTQA [68] StoryBench [271] (cid:209) HomePage - (cid:209) HomePage GitHub SHARE [146] (cid:209) HomePage GitHub MovieChat-1K [246] Mobile-Agent-E [293] (cid:209) HomePage GitHub long-text application Deep Reasoning and Realistic Multitasks Reasoning-in-a-Haystack Synthetic Long-Context Evaluation Multimodal Needle-in-a-Haystack Multi-party Dialogue Understanding Lifelong learning Personalized Preference Following Multi-hop Personalized Reasoning Continuous Online Learning Memory-Augmented Dialogue Multi-hop Question Answering The use of personalized memory Interactive fiction games Skill learning and Transfer Long Video Comprehension and QA Complex mobile tasks M3-Agent [190] Text2Mem Bench [276] (cid:209) HomePage GitHub SoMoSiMu-Bench [204] (cid:209) HomePage GitHub (cid:209) HomePage GitHub Long-form video QA and memory reasoning Memory operation instruction execution Social Movement Simulation Personalized Reasoning (cid:209) HomePage GitHub MemDaily [377] Data Type Data Quantity Text + Image Text Text Text Text Text Text Text Text Text Text Text Text Text Text Text + Image Text Text Text Text Text Text Text Text Text Text Text + Movie Text + Image Text + Audio +Video Text Text + Structured data Text 50 2,071 65,000 500 194 20,000 83,000 - 33 3,080 180 4,750 503 13,000 39,000 40,000 1,300 1,400 3,000 108,000 9,702 160 112,779 8,593 - 1,201 15,000 25 5,510 - 6,700 26,003 Table 1. Representative benchmarks for evaluating semantic-oriented agents. The attributes are defined as follows: Fid. (Fidelity) measures the agents ability to accurately retrieve and reproduce information from long contexts. Dyn. (Dynamics) assesses whether the memory system supports dynamic updates (e.g., modification, deletion) and cross-session reasoning. Gen. (Generalization) evaluates whether the stored memory can evolve into strategies that adapt the agents behavior in novel environments. DialSim [377], PrefEval [385], SHARE [146]. These benchmarks require agents to proactively identify and discard outdated information while ensuring the persistence and consistency of valid information over long periods and across dialogues. further line of benchmarks measures how efficiently agents transform interaction history into cognitive abilities through self-evolutionary behaviors such as self-reflection, memory abstraction, and policy iteration. Benchmarks such as LTMBenchmark [36], StoryBench [272], MemoryAgentBench [110], Evo-Memory [296], MemBench [5], HaluMem [39], LifelongAgentBench [389], StreamBench [303], StoryBench [271] provide experimental fields that tolerate trial and error and allow for continuous state changes for this type of research. These benchmarks require agents not only to maintain long-term memory consistency, but also to abstract general rules from past error patterns and transfer them to novel scenarios, thereby achieving meta-level cognitive improvement."
        },
        {
            "title": "7.2 Benchmarks for Episodic-oriented\nUnlike semantic-centric benchmarks that focus on the memory mechanism itself, episodic-oriented\nbenchmarks aim to evaluate the actual performance gains of memory systems on agents in complex\ndownstream application scenarios. Their core focus is on whether agents effectively leverage\naccumulated experience from memory to enhance performance on future tasks. Through efficient",
            "content": ", Vol. 1, No. 1, Article . Publication date: December 2025. AI Meets Brain: Unified Survey on Memory Systems from Cognitive Neuroscience to Autonomous Agents 29 Links Attribute Task (cid:209) HomePage GitHub E-commerce shopping navigation and decision Benchmarks GitHub Fid. Dyn. Gen. HomePage WebChoreArena [199] (cid:209) HomePage GitHub ScienceWorld [283] (cid:209) HomePage GitHub WebArena [397] (cid:209) HomePage GitHub WebShop [337] ToolBench [227] (cid:209) HomePage GitHub (cid:209) HomePage GitHub GAIA [197] (cid:209) HomePage GitHub BabyAI [52] (cid:209) HomePage GitHub xBench-DS [43] AgentOccam [330] (cid:209) HomePage GitHub (cid:209) HomePage GitHub StuLife [33] (cid:209) HomePage GitHub Mind2Web [62] PersonalWAB [31] (cid:209) HomePage GitHub Task Type Data Type Data Quantity Massive memory + Calculation Scientific Experiments and Reasoning Simulating human web operations Tool use and Api call General AI Assistants Language Learning Business Recruiting and Marketing General Web Interaction Shared Memory-Aware General Web Agent Task Personalized Web Agent Tasks Text + Image Text Text + Image Text + Image Text Text + Image + Spreadsheet Text + symbolic observation Text Text + Image Text Text + Image + File Text + Web API 532 7,200 812 12,087 142,950 466 100,000+ 100 812 1,284 2,350 9,000 Table 2. Representative benchmarks for evaluating episodic-oriented agent memory. The attributes are defined as follows: Fid. (Fidelity) measures the agents ability to accurately retrieve and reproduce information from long contexts. Dyn. (Dynamics) assesses whether the memory system supports dynamic updates (e.g., modification, deletion) and cross-session reasoning. Gen. (Generalization) evaluates whether the stored memory can evolve into strategies that adapt the agents behavior in novel environments. information integration and state tracking, the memory empower agents to handle task flows in vertical domains such as web search, tool-use, and environmental interaction, thereby achieving higher levels of planning and execution in real-world scenarios with high time dependence and logical complexity. In the field of web search, task-oriented benchmarks have evolved from simple information retrieval to comprehensive evaluation of an agents autonomous navigation and interaction capabilities within dynamic web pages. Benchmarks such as WebChoreArena [199], WebArena [397], WebShop [337], cover scenarios in simulated and real-world web environments, examining how agents utilize memory to maintain consistency in long-term tasks within highly dynamic and structurally complex web page interactions. The evaluation results of these benchmarks also demonstrate that efficient memory mechanisms are crucial for agents to ensure functional correctness and logical completeness in task flows. The evaluation method for tool use has also been upgraded from atomic API calls to complex workflow reasoning. ToolBench [43], GAIA [197], xBench-DS [43], benchmarks focus on examining how memory helps agents accurately retrieve tool schemas and maintain context state in multimodal and long-view tasks. Their evaluations have also demonstrated the key value of memory in tool use in overcoming execution illusions and establishing complex task logic through adaptive trial and error mechanisms. Benchmarks for interaction with the environment focus on evaluating an agents perceptionaction mapping capabilities in dynamic and observable environments under different conditions, with particular emphasis on the role of memory in supporting state tracking, causal inference, and personalized alignment. For example, BabyAI [52] evaluated sample efficiency in course learning, particularly its ability to remember and combine sub-goals in long-sequence navigation. ScienceWorld [283] extended the scenario to scientific experiment simulations, requiring the agent to continuously track environmental variables using memory and verify causal inference capabilities through multi-step operations. Mind2Web [62] spanned heterogeneous web page tasks across multiple domains, enabling the agent to filter environmental noise and achieve cross-domain generalization when facing complex document object model (DOM) structures. PersonalWAB [31] further enriched the contextual dimension of the environment, incorporating user profiles and historical behavior into the interaction loop, and evaluated the accuracy of memory in achieving personalized intent alignment during dynamic interactions. AgentOccam [330] addressed the , Vol. 1, No. 1, Article . Publication date: December 2025. Liang, et al. observation-action alignment problem in complex web environments, revealing that memory must possess the ability to prune and reconstruct massive amounts of environmental observations. In summary, the practical significance of episodic-oriented benchmarks lies in establishing the agents transformation from conversationalist to an executor, and promoting the shift of the evaluation paradigm from dialogue-centric to problem-solving-centric. In summary, the practical significance of episodic-oriented benchmarks lies in establishing the agents transformation from conversationalist to an executor, and promoting the shift of the evaluation paradigm from modelcentric to problem-solving-centric. By introducing noise from the real environment, it verifies the robustness of memory in maintaining state tracking under non-ideal conditions, thus bridging the gap between simulation and real-world applications."
        },
        {
            "title": "8.1.2 Poisoning-based Attack. Poisoning-based attacks refer to attacks that do not require modifica-\ntion of model parameters but instead inject adversarially optimized malicious data into an external\nmemory [161]. These attacks exploit retrieval mechanism preferences and the model’s excessive\nreliance on context to covertly implant backdoors or hijack the agent’s decision-making logic.\nTherefore, we categorize poisoning-based attacks into highly concealed backdoor attacks. Backdoor\nattacks constitute advanced persistent threats in agent memory security, with their effectiveness\nhinging on stealthy infiltration and precise triggering mechanisms. Attackers inject malicious",
            "content": ", Vol. 1, No. 1, Article . Publication date: December 2025. AI Meets Brain: Unified Survey on Memory Systems from Cognitive Neuroscience to Autonomous Agents 31 content into the retrieval database through carefully optimized triggers, enabling the agent to function normally under routine operations while executing adversarial behaviors only when specific trigger conditions are met. Chen et al. [49], Cheng et al. [50] both showed that attackers can precisely control the agents decision-making by manipulating the retrieval weights of memory in the vector space, and make retrieval the driver of backdoors. In addition, attackers exploit the vulnerability of instruction following rather than relying on complex model training to disguise malicious instructions as ordinary memories and store them in the memory bank. Abdelnabi et al. [3] demonstrated the vulnerability of the Agent when reading untrusted external data. Dong et al. [65] confirmed that attackers do not need backend privileges, but can induce the agent to generate and store malicious memories and malicious records by simply querying and observing the agents interactions with the agent, and can hijack subsequent query processing by using Bridging steps. Furthermore, some poisoning attacks target the agents cognition. The goal of such attacks is to inject large amounts of noise, conflicting information, or social biases into the memory bank, causing the agents judgment to deteriorate or its values to become distorted [66]. Yang et al. [333] vividly described the process of making recommendation agent ineffective, like person who is drunk, by injecting confusing data. Bagwe et al. [11] established hidden associations by poisoning, causing the agent to continuously output discriminatory content."
        },
        {
            "title": "8.2.2 Response-based Defense. Response-based defense acts as the agent’s cognitive immune\nsystem, ensuring that the agent will block the implementation of malicious logic even if the agent\ningests memory fragments containing malicious instructions [322]. For instance, Zeng et al. [356]\nproposed a multi-agent defense framework, which coordinates the input agent to make security\npresets, the Defense Agency to conduct collaborative review, and finally the output agent to decide\nhow to output the final response to the user’s request. Zhou et al. [394] integrated Monte Carlo\nTree Search and self-reflection to rehearse and score multiple potential action trajectories in the\nresponse generation stage, avoiding high-risk paths induced by false memories or malicious intent.",
            "content": ", Vol. 1, No. 1, Article . Publication date: December 2025. 32 Liang, et al."
        },
        {
            "title": "9.1 Multimodal Memory\nReal-world environments inherently provide information beyond textual signals, involving diverse\nmodalities such as vision, audio, and depth [22, 128, 133, 318, 344, 347, 351]. Accordingly, agents\ndeployed in complex settings typically perceive and interact with the environment through multi-\nmodal observations, with text being only one component. This has motivated growing interest in\nmultimodal memory, extending beyond traditional text-only agent memory paradigms [120, 317].\nCompared with pure text information, multimodal information is less well structured and\ncontains more noise. Directly storing raw multimodal information often leads to memory waste and\nperformance degradation [317]. DoraemonGPT [335] and LifelongMemory [290] used expert models\nto transform raw multimodal information into structured and concise symbolic memories (e.g.,\ntimestamps, frame or clip-level captions [386], object categories [183]). And MovieChat [247] and\nMA-LLM [100] instead employed feature-level consolidation and compression of raw multimodal\nrepresentations, using techniques such as token merging [24]and Q-Former [169]. In addition,\nsome works [71, 72, 191, 291] simultaneously leveraged symbolic memories alongside their aligned\nmultimodal content as evidence, resulting in a hybrid memory representation.",
            "content": "Existing agent systems demonstrate that incorporating multimodal memory consistently improves performance across wide range of environments. JARVIS-1 [291] enabled agents to perceive multimodal inputs and generate complex plans in interactive game environments like Minecraft. He et al. [100], Zhang et al. [364] addressed the challenges of long contexts caused by dense video frames through efficient compression and storage, thereby enhancing the understanding of long videos. InternLM-XComposer2.5-OmniLive [368] and Video-SALMONN-S [249] further integrated audio memory and visual memory to support online comprehension of audiovisual streams. M3Agent [191] and EgoMem [339] constructed and continuously updated entity-centric episodic and semantic memories from multimodal dialogues, enabling strong lifelong personalization capabilities. In application scenarios, Agashe et al. [4], Jiang et al. [129] preserved the historical trajectories of GUI interaction states, allowing agents to identify repetitive operations and improve efficiency. Despite some progress in recent years, existing methods still have limitations in several important aspects. Developing memory representations and operations that can seamlessly adapt to different modalities while ensuring semantic consistency and temporal alignment remains an unsolved problem. Furthermore, semantic degradation due to compression or abstraction, as well as unresolved , Vol. 1, No. 1, Article . Publication date: December 2025. AI Meets Brain: Unified Survey on Memory Systems from Cognitive Neuroscience to Autonomous Agents 33 issues such as long-term temporal dependency modeling and computational efficiency, still hinder the scalability of systems. Overcoming these challenges is crucial for building general-purpose intelligent agent systems capable of operating in complex multimodal environments."
        },
        {
            "title": "9.2 Agent Skills\nContemporary AI agents generally exhibit well-designed workflows, resulting in robust general-\npurpose capabilities. When augmented with memory systems, these capabilities are further en-\nhanced, underscoring the pivotal role of memory in amplifying agent performance. However, as\nagents become increasingly sophisticated, there is a growing imperative to equip them with domain-\nspecific expertise to accommodate a broader spectrum of application scenarios. For instance, Jiang\net al. [131] introduced a memory trigger mechanism that enables self-supervised adaptation during\nthe test time, incorporating fast parameters to address tasks across diverse specialized domains.\nNevertheless, this approach is relatively complex, and the learned parameters remain confined\nto the individual agent, precluding the transfer of specialized knowledge between agents. This\nphenomenon is akin to reinventing the wheel in isolation, squandering a wealth of invaluable\nexperiential knowledge. Consequently, there is an urgent need for memory strategies that offer\ngreater composability, scalability, and portability.",
            "content": "Agent skills [357] is modular capability extension paradigm proposed by Anthropic, with its core principle being the encapsulation of instruction sets, executable scripts, and associated resources into structured directory units. AI agents can dynamically discover, load, and execute these skill modules at runtime to accomplish domain-specific tasks. This mechanism abstracts domain expertise into composable and reusable modular resources, significantly expanding the capability boundaries of agents and enabling general-purpose agents to transform on demand into specialized agents tailored for vertical application scenarios. These modules function analogously to equipment in video games, where players equip corresponding gear when they seek particular attributes, and crucially, such equipment can be freely shared and transferred among players. Currently, several research [260, 277, 378] efforts have begun to explore this direction. For instance, Wang et al. [277] proposed unified memory operation language aimed at providing standardized memory management interfaces for memory operating systems, thereby enabling portability and interoperability of memory representations. Tang et al. [260] constructed unified knowledge management platform that allows intelligent agents across heterogeneous model architectures to access and manipulate shared memory resources. Although these works demonstrate the potential of this research direction, the field remains in its nascent stage, with numerous critical challenges awaiting further investigation: (1) Unified Storage and Representation of Multimodal Information: Current memory systems are predominantly designed for textual modalities. How to construct unified storage framework that supports multimodal information encompassing text, images, audio, and video, while simultaneously designing cross-modal retrieval and reasoning mechanisms, remains an open research question, and (2) Cross-agent skill transfer and adaptation mechanisms: Different agent architectures, such as those built upon distinct foundation models, exhibit variations in capability characteristics and interface specifications. Designing universal skill description language along with an adaptation layer that enables skill modules to be seamlessly transferred and reused across heterogeneous agents constitutes critical challenge for realizing genuine skill-sharing ecosystem."
        },
        {
            "title": "10 CONCLUSION\nIn this comprehensive survey, we present an in-depth unified investigation of memory in both\ncognitive neuroscience and autonomous agents, covering definitions, functions, taxonomies, man-\nagement, security, and future research directions. Although human-like memory mechanisms have",
            "content": ", Vol. 1, No. 1, Article . Publication date: December 2025. 34 Liang, et al. proven to be highly beneficial for agents, existing work has yet to deeply draw upon the essence of memory mechanisms from brain science due to the gap between disciplines. We envision this survey as step toward encouraging researchers to engage in cross-disciplinary collaboration and develop more robust agent memory systems."
        },
        {
            "title": "REFERENCES",
            "content": "[1] 2024. Perceive, Reflect, and Plan: Designing LLM Agent for Goal-Directed City Navigation without Instructions. CoRR abs/2408.04168 (2024). https://doi.org/10.48550/ARXIV.2408.04168 arXiv:2408.04168 Withdrawn.. [2] 2025. Beyond Retrieval: Embracing Compressive Memory in Real-World Long-Term Conversations. In Main Conference (Proceedings - International Conference on Computational Linguistics, COLING), Owen Rambow, Leo Wanner, Marianna Apidianaki, Hend Al-Khalifa, Barbara {Di Eugenio}, and Steven Schockaert (Eds.). Association for Computational Linguistics (ACL), 755773. Publisher Copyright: 2025 Association for Computational Linguistics.; 31st International Conference on Computational Linguistics, COLING 2025 ; Conference date: 19-01-2025 Through 24-01-2025. [3] Sahar Abdelnabi, Kai Greshake, Shailesh Mishra, Christoph Endres, Thorsten Holz, and Mario Fritz. 2023. Not What Youve Signed Up For: Compromising Real-World LLM-Integrated Applications with Indirect Prompt Injection. In Proceedings of the 16th ACM Workshop on Artificial Intelligence and Security, AISec 2023, Copenhagen, Denmark, 30 November 2023, Maura Pintor, Xinyun Chen, and Florian Tramèr (Eds.). ACM, 7990. https://doi.org/10.1145/3605764. 3623985 [4] Saaket Agashe, Jiuzhou Han, Shuyu Gan, Jiachen Yang, Ang Li, and Xin Eric Wang. 2025. Agent S: An Open Agentic Framework that Uses Computers Like Human. In The Thirteenth International Conference on Learning Representations, ICLR 2025, Singapore, April 24-28, 2025. OpenReview.net. https://openreview.net/forum?id=lIVRgt4nLv [5] Qingyao Ai, Yichen Tang, Changyue Wang, Jianming Long, Weihang Su, and Yiqun Liu. 2025. MemoryBench: Benchmark for Memory and Continual Learning in LLM Systems. CoRR abs/2510.17281 (2025). https://doi.org/10. 48550/ARXIV.2510.17281 arXiv:2510. [6] Petr Anokhin, Nikita Semenov, Artyom Y. Sorokin, Dmitry Evseev, Andrey Kravchenko, Mikhail Burtsev, and Evgeny Burnaev. 2025. AriGraph: Learning Knowledge Graph World Models with Episodic Memory for LLM Agents. In Proceedings of the Thirty-Fourth International Joint Conference on Artificial Intelligence, IJCAI 2025, Montreal, Canada, August 16-22, 2025. ijcai.org, 1220. https://doi.org/10.24963/IJCAI.2025/2 [7] Arian Askari, Christian Pölitz, and Xinye Tang. 2025. MAGIC: Generating Self-Correction Guideline for In-Context Text-to-SQL. In AAAI-25, Sponsored by the Association for the Advancement of Artificial Intelligence, February 25 - March 4, 2025, Philadelphia, PA, USA, Toby Walsh, Julie Shah, and Zico Kolter (Eds.). AAAI Press, 2343323441. https://doi.org/10.1609/AAAI.V39I22.34511 [8] Richard Atkinson and Richard Shiffrin. 1968. Human memory: proposed system and its control processes. In Psychology of learning and motivation. Vol. 2. Elsevier, 89195. [9] Sam Audrain and Mary Pat McAndrews. 2022. Schemas provide scaffold for neocortical integration of new memories over time. Nature communications 13, 1 (2022), 5795. [10] Reza Averly, Frazier N. Baker, Ian Watson, and Xia Ning. 2025. LIDDIA: Language-based Intelligent Drug Discovery Agent. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, Christos Christodoulopoulos, Tanmoy Chakraborty, Carolyn Rose, and Violet Peng (Eds.). Association for Computational Linguistics, Suzhou, China, 1201512039. https://doi.org/10.18653/v1/2025.emnlp-main.603 [11] Gaurav Bagwe, Saket S. Chaturvedi, Xiaolong Ma, Xiaoyong Yuan, Kuang-Ching Wang, and Lan Zhang. 2025. Your RAG is Unfair: Exposing Fairness Vulnerabilities in Retrieval-Augmented Generation via Backdoor Attacks. CoRR abs/2509.22486 (2025). https://doi.org/10.48550/ARXIV.2509.22486 arXiv:2509.22486 [12] Hao Bai, Yifei Zhou, Jiayi Pan, Mert Cemri, Alane Suhr, Sergey Levine, and Aviral Kumar. 2024. DigiRL: Training In-TheWild Device-Control Agents with Autonomous Reinforcement Learning. In Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024, Amir Globersons, Lester Mackey, Danielle Belgrave, Angela Fan, Ulrich Paquet, Jakub M. Tomczak, and Cheng Zhang (Eds.). http://papers.nips.cc/paper_files/paper/2024/hash/1704ddd0bb89f159dfe609b32c889995Abstract-Conference.html [13] Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li. 2024. LongBench: Bilingual, Multitask Benchmark for Long Context Understanding. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024, Lun-Wei Ku, Andre Martins, and Vivek Srikumar (Eds.). Association for Computational Linguistics, 31193137. https://doi.org/10.18653/V1/2024.ACL-LONG.172 [14] Yushi Bai, Shangqing Tu, Jiajie Zhang, Hao Peng, Xiaozhi Wang, Xin Lv, Shulin Cao, Jiazheng Xu, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li. 2025. LongBench v2: Towards Deeper Understanding and Reasoning on Realistic , Vol. 1, No. 1, Article . Publication date: December 2025. AI Meets Brain: Unified Survey on Memory Systems from Cognitive Neuroscience to Autonomous Agents 35 Long-context Multitasks. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2025, Vienna, Austria, July 27 - August 1, 2025, Wanxiang Che, Joyce Nabende, Ekaterina Shutova, and Mohammad Taher Pilehvar (Eds.). Association for Computational Linguistics, 36393664. https: //aclanthology.org/2025.acl-long.183/ [15] Ye Bai, Minghan Wang, and Thuy-Trang Vu. 2025. MAPLE: Multi-Agent Adaptive Planning with Long-Term Memory for Table Reasoning. CoRR abs/2506.05813 (2025). https://doi.org/10.48550/ARXIV.2506.05813 arXiv:2506.05813 [16] Alon Baram, Hamed Nili, Ines Barreiros, Veronika Samborska, Timothy EJ Behrens, and Mona Garvert. 2024. An abstract relational map emerges in the human medial prefrontal cortex with consolidation. bioRxiv (2024), 202410. [17] Joao Barbosa, Heike Stein, Rebecca Martinez, Adrià Galan-Gadea, Sihai Li, Josep Dalmau, Kirsten CS Adam, Josep Valls-Solé, Christos Constantinidis, and Albert Compte. 2020. Interplay between persistent activity and activity-silent dynamics in the prefrontal cortex underlies serial biases in working memory. Nature neuroscience 23, 8 (2020), 10161024. [18] Aya Ben-Yakov and Richard Henson. 2018. The hippocampal film editor: sensitivity and specificity to event boundaries in continuous experience. Journal of Neuroscience 38, 47 (2018), 1005710068. [19] John Best and Patricia Miller. 2010. developmental perspective on executive function. Child development 81, (2010), 16411660. [20] John Best, Patricia Miller, and Jack Naglieri. 2011. Relations between executive function and academic achievement from ages 5 to 17 in large, representative national sample. Learning and individual differences 21, 4 (2011), 327336. [21] Natalie Biderman, Akram Bakkour, and Daphna Shohamy. 2020. What are memories for? The hippocampus bridges past experience with future decisions. Trends in Cognitive Sciences 24, 7 (2020), 542556. [22] Weihao Bo, Shan Zhang, Yanpeng Sun, Jingjing Wu, Qunyi Xie, Xiao Tan, Kunbin Chen, Wei He, Xiaofan Li, Na Zhao, et al. 2025. Agentic Learner with Grow-and-Refine Multimodal Semantic Memory. arXiv preprint arXiv:2511.21678 (2025). [23] Xiaohe Bo, Zeyu Zhang, Quanyu Dai, Xueyang Feng, Lei Wang, Rui Li, Xu Chen, and Ji-Rong Wen. 2024. Reflective Multi-Agent Collaboration based on Large Language Models. In Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024, Amir Globersons, Lester Mackey, Danielle Belgrave, Angela Fan, Ulrich Paquet, Jakub M. Tomczak, and Cheng Zhang (Eds.). http://papers.nips.cc/paper_files/paper/2024/hash/fa54b0edce5eef0bb07654e8ee800cb4Abstract-Conference.html [24] Daniel Bolya, Cheng-Yang Fu, Xiaoliang Dai, Peizhao Zhang, Christoph Feichtenhofer, and Judy Hoffman. 2023. Token Merging: Your ViT But Faster. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net. https://openreview.net/forum?id=JroZRaRw7Eu [25] Ece Boran, Tommaso Fedele, Peter Klaver, Peter Hilfiker, Lennart Stieglitz, Thomas Grunwald, and Johannes Sarnthein. 2019. Persistent hippocampal neural firing and hippocampal-cortical coupling predict verbal working memory load. Science advances 5, 3 (2019), eaav3687. [26] Ilaria Borghi, Lucia Mencarelli, Michele Maiella, Elias Paolo Casula, Matteo Ferraresi, Francesca Candeo, Elena Savastano, Martina Assogna, Sonia Bonnì, and Giacomo Koch. 2025. Dual transcranial electromagnetic stimulation of the precuneus boosts human long-term memory. eLife 14 (2025), RP104220. [27] Pascal Boyer. 2008. Evolutionary economics of mental time travel? Trends in cognitive sciences 12, 6 (2008), 219224. [28] Inês Bramão, Jiefeng Jiang, Anthony Wagner, and Mikael Johansson. 2022. Encoding contexts are incidentally reinstated during competitive retrieval and track the temporal dynamics of memory interference. Cerebral Cortex 32, 22 (2022), 50205035. [29] Svenja Brodt, Marion Inostroza, Niels Niethard, and Jan Born. 2023. SleepA brain-state serving systems memory consolidation. Neuron 111, 7 (2023), 10501075. [30] Hongru Cai, Yongqi Li, Wenjie Wang, Fengbin Zhu, Xiaoyu Shen, Wenjie Li, and Tat-Seng Chua. 2025. Large Language Models Empowered Personalized Web Agents. In Proceedings of the ACM on Web Conference 2025, WWW 2025, Sydney, NSW, Australia, 28 April 20252 May 2025, Guodong Long, Michale Blumestein, Yi Chang, Liane Lewin-Eytan, Zi Helen Huang, and Elad Yom-Tov (Eds.). ACM, 198215. https://doi.org/10.1145/3696410.3714842 [31] Hongru Cai, Yongqi Li, Wenjie Wang, Fengbin Zhu, Xiaoyu Shen, Wenjie Li, and Tat-Seng Chua. 2025. Large Language Models Empowered Personalized Web Agents. In Proceedings of the ACM on Web Conference 2025, WWW 2025, Sydney, NSW, Australia, 28 April 20252 May 2025, Guodong Long, Michale Blumestein, Yi Chang, Liane Lewin-Eytan, Zi Helen Huang, and Elad Yom-Tov (Eds.). ACM, 198215. https://doi.org/10.1145/3696410.3714842 [32] Yuzheng Cai, Siqi Cai, Yuchen Shi, Zihan Xu, Lichao Chen, Yulei Qin, Xiaoyu Tan, Gang Li, Zongyi Li, Haojia Lin, Yong Mao, Ke Li, and Xing Sun. 2025. Training-Free Group Relative Policy Optimization. CoRR abs/2510.08191 (2025). https://doi.org/10.48550/ARXIV.2510.08191 arXiv:2510.08191 , Vol. 1, No. 1, Article . Publication date: December 2025. 36 Liang, et al. [33] Yuxuan Cai, Yipeng Hao, Jie Zhou, Hang Yan, Zhikai Lei, Rui Zhen, Zhenhua Han, Yutao Yang, Junsong Li, Qianjun Pan, Tianyu Huai, Qin Chen, Xin Li, Kai Chen, Bo Zhang, Xipeng Qiu, and Liang He. 2025. Building Self-Evolving Agents via Experience-Driven Lifelong Learning: Framework and Benchmark. CoRR abs/2508.19005 (2025). https://doi.org/10.48550/ARXIV.2508.19005 arXiv:2508.19005 [34] Zhicheng Cai, Xinyuan Guo, Yu Pei, Jiangtao Feng, Jinsong Su, Jiangjie Chen, Ya-Qin Zhang, Wei-Ying Ma, Mingxuan Wang, and Hao Zhou. 2025. FLEX: Continuous Agent Evolution via Forward Learning from Experience. arXiv:2511.06449 [cs.LG] https://arxiv.org/abs/2511.06449 [35] Zouying Cao, Jiaji Deng, Li Yu, Weikang Zhou, Zhaoyang Liu, Bolin Ding, and Hai Zhao. 2025. Remember Me, Refine Me: Dynamic Procedural Memory Framework for Experience-Driven Agent Evolution. arXiv:2512.10696 [cs.AI] https://arxiv.org/abs/2512.10696 [36] David Castillo-Bolado, Joseph Davidson, Finlay Gray, and Marek Rosa. 2024. Beyond Prompts: Dynamic Conversational Benchmarking of Large Language Models. In Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024, Amir Globersons, Lester Mackey, Danielle Belgrave, Angela Fan, Ulrich Paquet, Jakub M. Tomczak, and Cheng Zhang (Eds.). http://papers.nips.cc/paper_files/paper/2024/hash/4aedf0cba303537fcb6cf948bb41b2df-Abstract-Datasets_ and_Benchmarks_Track.html [37] Avi JH Chanales, Alexandra Tremblay-McGaw, Maxwell Drascher, and Brice Kuhl. 2021. Adaptive repulsion of long-term memory representations is triggered by event similarity. Psychological science 32, 5 (2021), 705720. [38] Chia-Yuan Chang, Zhimeng Jiang, Vineeth Rakesh, Menghai Pan, Chin-Chia Michael Yeh, Guanchu Wang, Mingzhi Hu, Zhichao Xu, Yan Zheng, Mahashweta Das, and Na Zou. 2025. MAIN-RAG: Multi-Agent Filtering RetrievalAugmented Generation. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2025, Vienna, Austria, July 27 - August 1, 2025, Wanxiang Che, Joyce Nabende, Ekaterina Shutova, and Mohammad Taher Pilehvar (Eds.). Association for Computational Linguistics, 26072622. https: //aclanthology.org/2025.acl-long.131/ [39] Ding Chen, Simin Niu, Kehang Li, Peng Liu, Xiangping Zheng, Bo Tang, Xinchi Li, Feiyu Xiong, and Zhiyu Li. 2025. HaluMem: Evaluating Hallucinations in Memory Systems of Agents. CoRR abs/2511.03506 (2025). https: //doi.org/10.48550/ARXIV.2511.03506 arXiv:2511. [40] Howard Chen, Ramakanth Pasunuru, Jason Weston, and Asli Celikyilmaz. 2023. Walking Down the Memory Maze: Beyond Context Limit through Interactive Reading. CoRR abs/2310.05029 (2023). https://doi.org/10.48550/ARXIV. 2310.05029 arXiv:2310.05029 [41] Haopeng Chen, Pieter Verbeke, Stefania Mattioni, Cristian Buc Calderon, and Tom Verguts. 2025. Neural and computational evidence for predictive learning account of the testing effect. Proceedings of the National Academy of Sciences 122, 32 (2025), e2506530122. [42] Junzhi Chen, Juhao Liang, and Benyou Wang. 2025. Smurfs: Multi-Agent System using Context-Efficient DFSDT for Tool Planning. In Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL 2025 - Volume 1: Long Papers, Albuquerque, New Mexico, USA, April 29 - May 4, 2025, Luis Chiruzzo, Alan Ritter, and Lu Wang (Eds.). Association for Computational Linguistics, 32813298. https://doi.org/10.18653/V1/2025.NAACL-LONG.169 [43] Kaiyuan Chen, Yixin Ren, Yang Liu, Xiaobo Hu, Haotong Tian, Tianbao Xie, Fangfu Liu, Haoye Zhang, Hongzhang Liu, Yuan Gong, Chen Sun, Han Hou, Hui Yang, James Pan, Jianan Lou, Jiayi Mao, Jizheng Liu, Jinpeng Li, Kangyi Liu, Kenkun Liu, Rui Wang, Run Li, Tong Niu, Wenlong Zhang, Wenqi Yan, Xuanzheng Wang, Yuchen Zhang, Yi-Hsin Hung, Yuan Jiang, Zexuan Liu, Zihan Yin, Zijian Ma, and Zhiwen Mo. 2025. xbench: Tracking Agents Productivity Scaling with Profession-Aligned Real-World Evaluations. CoRR abs/2506.13651 (2025). https://doi.org/10.48550/ARXIV.2506.13651 arXiv:2506.13651 [44] Silin Chen, Shaoxin Lin, Xiaodong Gu, Yuling Shi, Heng Lian, Longfei Yun, Dong Chen, Weiguo Sun, Lin Cao, and Qianxiang Wang. 2025. SWE-Exp: Experience-Driven Software Issue Resolution. CoRR abs/2507.23361 (2025). https://doi.org/10.48550/ARXIV.2507.23361 arXiv:2507.23361 [45] Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. 2023. Extending Context Window of Large Language Models via Positional Interpolation. CoRR abs/2306.15595 (2023). https://doi.org/10.48550/ARXIV.2306.15595 arXiv:2306. [46] Weishu Chen, Jinyi Tang, Zhouhui Hou, Shihao Han, Mingjie Zhan, Zhiyuan Huang, Delong Liu, Jiawei Guo, Zhicheng Zhao, and Fei Su. 2025. MOOM: Maintenance, Organization and Optimization of Memory in Ultra-Long Role-Playing Dialogues. CoRR abs/2509.11860 (2025). https://doi.org/10.48550/ARXIV.2509.11860 arXiv:2509.11860 [47] Zhaorun Chen, Mintong Kang, and Bo Li. 2025. ShieldAgent: Shielding Agents via Verifiable Safety Policy Reasoning. In Forty-second International Conference on Machine Learning, ICML 2025, Vancouver, BC, Canada, July 13-19, 2025. OpenReview.net. https://openreview.net/forum?id=DkRYImuQA9 , Vol. 1, No. 1, Article . Publication date: December 2025. AI Meets Brain: Unified Survey on Memory Systems from Cognitive Neuroscience to Autonomous Agents 37 [48] Zhongting Chen, Yudi Mao, Zhenkai Zheng, and Yixuan Ku. 2024. How long-term memory facilitates working memory: Evidence from flexible responses and neural oscillations. Journal of Experimental Psychology: Learning, Memory, and Cognition (2024). [49] Zhaorun Chen, Zhen Xiang, Chaowei Xiao, Dawn Song, and Bo Li. 2024. AgentPoison: Red-teaming LLM Agents via Poisoning Memory or Knowledge Bases. In Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024, Amir Globersons, Lester Mackey, Danielle Belgrave, Angela Fan, Ulrich Paquet, Jakub M. Tomczak, and Cheng Zhang (Eds.). http://papers.nips.cc/paper_files/paper/2024/hash/eb113910e9c3f6242541c1652e30dfd6-Abstract-Conference.html [50] Pengzhou Cheng, Yidong Ding, Tianjie Ju, Zongru Wu, Wei Du, Ping Yi, Zhuosheng Zhang, and Gongshen Liu. 2024. TrojanRAG: Retrieval-Augmented Generation Can Be Backdoor Driver in Large Language Models. CoRR abs/2405.13401 (2024). https://doi.org/10.48550/ARXIV.2405.13401 arXiv:2405.13401 [51] Weihua Cheng, Ersheng Ni, Wenlong Wang, Yifei Sun, Junming Liu, Wangyu Shen, Yirong Chen, Botian Shi, and Ding Wang. 2025. MGA: Memory-Driven GUI Agent for Observation-Centric Interaction. CoRR abs/2510.24168 (2025). https://doi.org/10.48550/ARXIV.2510.24168 arXiv:2510.24168 [52] Maxime Chevalier-Boisvert, Dzmitry Bahdanau, Salem Lahlou, Lucas Willems, Chitwan Saharia, Thien Huu Nguyen, and Yoshua Bengio. 2019. BabyAI: Platform to Study the Sample Efficiency of Grounded Language Learning. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net. https://openreview.net/forum?id=rJeXCo0cYX [53] Prateek Chhikara, Dev Khant, Saket Aryan, Taranjeet Singh, and Deshraj Yadav. 2025. Mem0: Building ProductionReady AI Agents with Scalable Long-Term Memory. CoRR abs/2504.19413 (2025). https://doi.org/10.48550/ARXIV. 2504.19413 arXiv:2504.19413 [54] Jae-Woo Choi, Hyungmin Kim, Hyobin Ong, Minsu Jang, Dohyung Kim, Jaehong Kim, and Youngwoo Yoon. 2025. ReAcTree: Hierarchical LLM Agent Trees with Control Flow for Long-Horizon Task Planning. CoRR abs/2511.02424 (2025). https://doi.org/10.48550/ARXIV.2511.02424 arXiv:2511. [55] Thomas B. Christophel, P. Christiaan Klink, Bernhard Spitzer, Pieter R. Roelfsema, and John-Dylan Haynes. 2017. The Distributed Nature of Working Memory. Trends in Cognitive Sciences 21, 2 (2017), 111124. https://doi.org/10. 1016/j.tics.2016.12.007 [56] Alexandra Constantinescu, Jill OReilly, and Timothy EJ Behrens. 2016. Organizing conceptual knowledge in humans with gridlike code. Science 352, 6292 (2016), 14641468. [57] Ben Cottier, Robi Rahman, Loredana Fattorini, Nestor Maslej, and David Owen. 2024. The rising costs of training frontier AI models. CoRR abs/2405.21015 (2024). https://doi.org/10.48550/ARXIV.2405.21015 arXiv:2405.21015 [58] Prawesh Dahal, Onni Rauhala, Dion Khodagholy, and Jennifer Gelinas. 2023. Hippocampalcortical coupling differentiates long-term memory processes. Proceedings of the National Academy of Sciences 120, 7 (2023), e2207909120. [59] Yuhong Dai, Jianxun Lian, Yitian Huang, Wei Zhang, Mingyang Zhou, Mingqi Wu, Xing Xie, and Hao Liao. 2025. Pretraining Context Compressor for Large Language Models with Embedding-Based Memory. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2025, Vienna, Austria, July 27 - August 1, 2025, Wanxiang Che, Joyce Nabende, Ekaterina Shutova, and Mohammad Taher Pilehvar (Eds.). Association for Computational Linguistics, 2871528732. https://aclanthology.org/2025.acl-long.1394/ [60] Jonathan Daume, Jan Kamiński, Yousef Salimpour, Andrea Gómez Palacio Schjetnan, William Anderson, Taufik Valiante, Adam Mamelak, and Ueli Rutishauser. 2024. Persistent activity during working memory maintenance predicts long-term memory formation in the human hippocampus. Neuron 112, 23 (2024), 39573968. [61] Carsten KW De Dreu, Bernard Nijstad, Matthijs Baas, Inge Wolsink, and Marieke Roskes. 2012. Working memory benefits creative insight, musical improvisation, and original ideation through maintained task-focused attention. Personality and social psychology bulletin 38, 5 (2012), 656669. [62] Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Samual Stevens, Boshi Wang, Huan Sun, and Yu Su. 2023. Mind2Web: Towards Generalist Agent for the Web. In Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine (Eds.). http://papers.nips.cc/ paper_files/paper/2023/hash/5950bf290a1570ea401bf98882128160-Abstract-Datasets_and_Benchmarks.html [63] R. Desimone and J. Duncan. 1995. Neural mechanisms of selective visual attention. Annual Review of Neuroscience (1995), 193222. [64] Bradford Dickerson and Howard Eichenbaum. 2010. The episodic memory system: neurocircuitry and disorders. Neuropsychopharmacology 35, 1 (2010), 86104. [65] Shen Dong, Shaochen Xu, Pengfei He, Yige Li, Jiliang Tang, Tianming Liu, Hui Liu, and Zhen Xiang. 2025. Memory Injection Attacks on LLM Agents via Query-Only Interaction. In The Thirty-ninth Annual Conference on Neural Information Processing Systems. , Vol. 1, No. 1, Article . Publication date: December 2025. Liang, et al. [66] Yinuo Du, Baptiste Prébot, Tyler Malloy, and Cleotilde Gonzalez. 2025. Cyber-War Between Bots: Cognitive Attackers are More Challenging for Defenders than Strategic Attackers. ACM Trans. Soc. Comput. 8, 3-4 (2025), 122. https://doi.org/10.1145/3712672 [67] Yiming Du, Bingbing Wang, Yang He, Bin Liang, Baojun Wang, Zhongyang Li, Lin Gui, Jeff Z. Pan, Ruifeng Xu, and Kam-Fai Wong. 2025. Bridging the Long-Term Gap: Memory-Active Policy for Multi-Session Task-Oriented Dialogue. CoRR abs/2505.20231 (2025). https://doi.org/10.48550/ARXIV.2505.20231 arXiv:2505.20231 [68] Yiming Du, Hongru Wang, Zhengyi Zhao, Bin Liang, Baojun Wang, Wanjun Zhong, Zezhong Wang, and Kam-Fai Wong. 2024. PerLTQA: Personal Long-Term Memory Dataset for Memory Classification, Retrieval, and Fusion in Question Answering. In Proceedings of the 10th SIGHAN Workshop on Chinese Language Processing (SIGHAN-10). Association for Computational Linguistics, Bangkok, Thailand, 152164. https://aclanthology.org/2024.sighan-1.18/ [69] Hermann Ebbinghaus. 1885. Über das gedächtnis: untersuchungen zur experimentellen psychologie. Duncker & Humblot. [70] Howard Eichenbaum. 2000. corticalhippocampal system for declarative memory. Nature reviews neuroscience 1, (2000), 4150. [71] Yue Fan, Xiaojian Ma, Rongpeng Su, Jun Guo, Rujie Wu, Xi Chen, and Qing Li. 2025. Embodied VideoAgent: Persistent Memory from Egocentric Videos and Embodied Sensors Enables Dynamic Scene Understanding. CoRR abs/2501.00358 (2025). https://doi.org/10.48550/ARXIV.2501.00358 arXiv:2501.00358 [72] Yue Fan, Xiaojian Ma, Rujie Wu, Yuntao Du, Jiaqi Li, Zhi Gao, and Qing Li. 2024. VideoAgent: Memory-augmented Multimodal Agent for Video Understanding. CoRR abs/2403.11481 (2024). https://doi.org/10.48550/ARXIV.2403.11481 arXiv:2403.11481 [73] Jizhan Fang, Xinle Deng, Haoming Xu, Ziyan Jiang, Yuqi Tang, Ziwen Xu, Shumin Deng, Yunzhi Yao, Mengru Wang, Shuofei Qiao, Huajun Chen, and Ningyu Zhang. 2025. LightMem: Lightweight and Efficient Memory-Augmented Generation. CoRR abs/2510.18866 (2025). https://doi.org/10.48550/ARXIV.2510.18866 arXiv:2510.18866 [74] Xi Fang, Weijie Xu, Yuchong Zhang, Stephanie Eckman, Scott Nickleach, and Chandan K. Reddy. 2025. The Personalization Trap: How User Memory Alters Emotional Reasoning in LLMs. CoRR abs/2510.09905 (2025). https://doi.org/10.48550/ARXIV.2510.09905 arXiv:2510.09905 [75] Serra Favila, Rosalie Samide, Sarah Sweigart, and Brice Kuhl. 2018. Parietal representations of stimulus features are amplified during memory retrieval and flexibly aligned with top-down goals. Journal of Neuroscience 38, 36 (2018), 78097821. [76] Evelina Fedorenko, Steven Piantadosi, and Edward AF Gibson. 2024. Language is primarily tool for communication rather than thought. Nature 630, 8017 (2024), 575586. [77] Erhu Feng, Wenbo Zhou, Zibin Liu, Le Chen, Yunpeng Dong, Cheng Zhang, Yisheng Zhao, Dong Du, Zhi-Hua Zhou, Yubin Xia, and Haibo Chen. 2025. Get Experience from Practice: LLM Agents with Record & Replay. CoRR abs/2505.17716 (2025). https://doi.org/10.48550/ARXIV.2505.17716 arXiv:2505.17716 [78] Line Folvik, Markus Sneve, Hedda Ness, Didac Vidal-Piñeiro, Liisa Raud, Oliver Geier, Kristine Walhovd, and Anders Fjell. 2023. Sustained upregulation of widespread hippocampalneocortical coupling following memory encoding. Cerebral Cortex 33, 8 (2023), 48444858. [79] Zafeirios Fountas, Martin Benfeghoul, Adnan Oomerjee, Fenia Christopoulou, Gerasimos Lampouras, Haitham Bou-Ammar, and Jun Wang. 2025. Human-inspired Episodic Memory for Infinite Context LLMs. In The Thirteenth International Conference on Learning Representations, ICLR 2025, Singapore, April 24-28, 2025. OpenReview.net. https: //openreview.net/forum?id=BI2int5SAC [80] Lukas Frase, Lydia Mertens, Arno Krahl, Kriti Bhatia, Bernd Feige, Sven Heinrich, Stefan Vestring, Christoph Nissen, Katharina Domschke, Michael Bach, et al. 2021. Transcranial direct current stimulation induces long-term potentiation-like plasticity in the human visual cortex. Translational psychiatry 11, 1 (2021), 17. [81] Huan-ang Gao, Jiayi Geng, Wenyue Hua, Mengkang Hu, Xinzhe Juan, Hongzhang Liu, Shilong Liu, Jiahao Qiu, Xuan Qi, Yiran Wu, Hongru Wang, Han Xiao, Yuhang Zhou, Shaokun Zhang, Jiayi Zhang, Jinyu Xiang, Yixiong Fang, Qiwen Zhao, Dongrui Liu, Qihan Ren, Cheng Qian, Zhenhailong Wang, Minda Hu, Huazheng Wang, Qingyun Wu, Heng Ji, and Mengdi Wang. 2025. Survey of Self-Evolving Agents: On Path to Artificial Super Intelligence. CoRR abs/2507.21046 (2025). https://doi.org/10.48550/ARXIV.2507.21046 arXiv:2507.21046 [82] Hang Gao and Yongfeng Zhang. 2024. Memory Sharing for Large Language Model based Agents. CoRR abs/2404.09982 (2024). https://doi.org/10.48550/ARXIV.2404.09982 arXiv:2404.09982 [83] Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Qianyu Guo, Meng Wang, and Haofen Wang. 2023. Retrieval-Augmented Generation for Large Language Models: Survey. CoRR abs/2312.10997 (2023). https://doi.org/10.48550/ARXIV.2312.10997 arXiv:2312.10997 [84] Mona Garvert, Raymond Dolan, and Timothy EJ Behrens. 2017. map of abstract relational knowledge in the human hippocampalentorhinal cortex. elife 6 (2017), e17086. , Vol. 1, No. 1, Article . Publication date: December 2025. AI Meets Brain: Unified Survey on Memory Systems from Cognitive Neuroscience to Autonomous Agents 39 [85] Tao Ge, Jing Hu, Lei Wang, Xun Wang, Si-Qing Chen, and Furu Wei. 2024. In-context Autoencoder for Context Compression in Large Language Model. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net. https://openreview.net/forum?id=uREj4ZuGJE [86] Yubin Ge, Salvatore Romeo, Jason Cai, Raphael Shu, Yassine Benajiba, Monica Sunkara, and Yi Zhang. 2025. TReMu: Towards Neuro-Symbolic Temporal Reasoning for LLM-Agents with Memory in Multi-Session Dialogues. In Findings of the Association for Computational Linguistics, ACL 2025, Vienna, Austria, July 27 - August 1, 2025, Wanxiang Che, Joyce Nabende, Ekaterina Shutova, and Mohammad Taher Pilehvar (Eds.). Association for Computational Linguistics, 1897418988. https://aclanthology.org/2025.findings-acl.972/ [87] Alireza Ghafarollahi and Markus J. Buehler. 2024. AtomAgents: Alloy design and discovery through physics-aware multi-modal multi-agent artificial intelligence. CoRR abs/2407.10022 (2024). https://doi.org/10.48550/ARXIV.2407. 10022 arXiv:2407. [88] Adrian Gilmore, Alina Quach, Sarah Kalinowski, Stephen Gotts, Daniel Schacter, and Alex Martin. 2021. Dynamic content reactivation supports naturalistic autobiographical recall in humans. Journal of Neuroscience 41, 1 (2021), 153166. [89] Arthur Glenberg. 1997. What memory is for. Behavioral and brain sciences 20, 1 (1997), 119. [90] Xenia Grande, David Berron, Aidan Horner, James Bisby, Emrah Düzel, and Neil Burgess. 2019. Holistic recollection via pattern completion involves hippocampal subfield CA3. Journal of Neuroscience 39, 41 (2019), 81008111. [91] Daniel Greenberg and Mieke Verfaellie. 2010. Interdependence of episodic and semantic memory: Evidence from neuropsychology. Journal of the International Neuropsychological society 16, 5 (2010), 748753. [92] Wenbin Gu, Siqi Liu, Zhenyang Guo, Minghai Yuan, and Fengque Pei. 2024. Dynamic scheduling mechanism for intelligent workshop with deep reinforcement learning method based on multi-agent system architecture. Comput. Ind. Eng. 191 (2024), 110155. https://doi.org/10.1016/J.CIE.2024.110155 [93] Naman Gupta, Shreeyash Gowaikar, Arun Iyer, Kirankumar Shiragur, Ramakrishna B. Bairi, Rishikesh Maurya, Ritabrata Maiti, Sankarshan Damle, and Shachee Mishra Gupta. 2025. COSMIR: Chain Orchestrated Structured Memory for Iterative Reasoning over Long Context. CoRR abs/2510.04568 (2025). https://doi.org/10.48550/ARXIV.2510.04568 arXiv:2510.04568 [94] Bernal Jimenez Gutierrez, Yiheng Shu, Yu Gu, Michihiro Yasunaga, and Yu Su. 2024. HippoRAG: Neurobiologically Inspired Long-Term Memory for Large Language Models. In Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024, Amir Globersons, Lester Mackey, Danielle Belgrave, Angela Fan, Ulrich Paquet, Jakub M. Tomczak, and Cheng Zhang (Eds.). http://papers.nips.cc/paper_files/paper/2024/hash/6ddc001d07ca4f319af96a3024f6dbd1Abstract-Conference.html [95] Grace E. Hallenbeck, Nathan Tardiff, Thomas C. Sprague, and Clayton E. Curtis. 2025. Prioritizing Working Memory Resources Depends on the Prefrontal Cortex. Journal of Neuroscience 45, 11 (2025). https://doi.org/10.1523/JNEUROSCI. 1552-24.2025 arXiv:https://www.jneurosci.org/content/45/11/e1552242025.full.pdf [96] Dongge Han, Camille Couturier, Daniel Madrigal Díaz, Xuchao Zhang, Victor Rühle, and Saravan Rajmohan. 2025. LEGOMem: Modular Procedural Memory for Multi-agent LLM Systems for Workflow Automation. CoRR abs/2510.04851 (2025). https://doi.org/10.48550/ARXIV.2510.04851 arXiv:2510.04851 [97] Yikun Han, Chunjiang Liu, and Pengfei Wang. 2023. Comprehensive Survey on Vector Database: Storage https://doi.org/10.48550/ARXIV.2310.11703 and Retrieval Technique, Challenge. CoRR abs/2310.11703 (2023). arXiv:2310.11703 [98] Shibo Hao, Sainbayar Sukhbaatar, DiJia Su, Xian Li, Zhiting Hu, Jason Weston, and Yuandong Tian. 2024. Training Large Language Models to Reason in Continuous Latent Space. CoRR abs/2412.06769 (2024). https://doi.org/10. 48550/ARXIV.2412.06769 arXiv:2412.06769 [99] Jackson Hassell, Dan Zhang, Hannah Kim, Tom M. Mitchell, and Estevam Hruschka. 2025. Learning from Supervision with Semantic and Episodic Memory: Reflective Approach to Agent Adaptation. CoRR abs/2510.19897 (2025). https://doi.org/10.48550/ARXIV.2510.19897 arXiv:2510. [100] Bo He, Hengduo Li, Young Kyun Jang, Menglin Jia, Xuefei Cao, Ashish Shah, Abhinav Shrivastava, and Ser-Nam Lim. 2024. MA-LMM: Memory-Augmented Large Multimodal Model for Long-Term Video Understanding. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2024, Seattle, WA, USA, June 16-22, 2024. IEEE, 13504 13514. https://doi.org/10.1109/CVPR52733.2024.01282 [101] Feng He, Tianqing Zhu, Dayong Ye, Bo Liu, Wanlei Zhou, and Philip S. Yu. 2024. The Emerged Security and Privacy of LLM Agent: Survey with Case Studies. CoRR abs/2407.19354 (2024). https://doi.org/10.48550/ARXIV.2407.19354 arXiv:2407.19354 [102] Junqing He, Liang Zhu, Rui Wang, Xi Wang, Gholamreza Haffari, and Jiaxing Zhang. 2025. MADial-Bench: Towards Real-world Evaluation of Memory-Augmented Dialogue Generation. In Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL 2025 - , Vol. 1, No. 1, Article . Publication date: December 2025. 40 Liang, et al. Volume 1: Long Papers, Albuquerque, New Mexico, USA, April 29 - May 4, 2025, Luis Chiruzzo, Alan Ritter, and Lu Wang (Eds.). Association for Computational Linguistics, 99029921. https://doi.org/10.18653/V1/2025.NAACL-LONG.499 [103] Shiqi He, Yue Cui, Xinyu Ma, Yaliang Li, Bolin Ding, and Mosharaf Chowdhury. 2025. Branch-and-Browse: Efficient and Controllable Web Exploration with Tree-Structured Reasoning and Action Memory. CoRR abs/2510.19838 (2025). https://doi.org/10.48550/ARXIV.2510.19838 arXiv:2510.19838 [104] Himmer, Schönauer, Dominik Philip Johannes Heib, Manuel Schabus, and Gais. 2019. Rehearsal initiates systems memory consolidation, sleep makes it last. Science advances 5, 4 (2019), eaav1695. [105] Yuki Hou, Haruki Tamoto, and Homei Miyashita. 2024. \"My agent understands me better\": Integrating Dynamic Human-like Memory Recall and Consolidation in LLM-Based Agents. In Extended Abstracts of the CHI Conference on Human Factors in Computing Systems, CHI EA 2024, Honolulu, HI, USA, May 11-16, 2024, Florian Floyd Mueller, Penny Kyburz, Julie R. Williamson, and Corina Sas (Eds.). ACM, 7:17:7. https://doi.org/10.1145/3613905.3650839 [106] Cheng-Ping Hsieh, Simeng Sun, Samuel Kriman, Shantanu Acharya, Dima Rekesh, Fei Jia, Yang Zhang, and Boris Ginsburg. 2024. RULER: Whats the Real Context Size of Your Long-Context Language Models? CoRR abs/2404.06654 (2024). https://doi.org/10.48550/ARXIV.2404.06654 arXiv:2404.06654 [107] Mengkang Hu, Tianxing Chen, Qiguang Chen, Yao Mu, Wenqi Shao, and Ping Luo. 2025. HiAgent: Hierarchical Working Memory Management for Solving Long-Horizon Agent Tasks with Large Language Model. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2025, Vienna, Austria, July 27 - August 1, 2025, Wanxiang Che, Joyce Nabende, Ekaterina Shutova, and Mohammad Taher Pilehvar (Eds.). Association for Computational Linguistics, 3277932798. https://aclanthology.org/2025.acl-long.1575/ [108] Yuyang Hu, Shichun Liu, Yanwei Yue, Guibin Zhang, Boyang Liu, Fangyi Zhu, Jiahang Lin, Honglin Guo, Shihan Dou, Zhiheng Xi, et al. 2025. Memory in the Age of AI Agents. arXiv preprint arXiv:2512.13564 (2025). [109] Yuanzhe Hu, Yu Wang, and Julian J. McAuley. 2025. Evaluating Memory in LLM Agents via Incremental Multi-Turn Interactions. CoRR abs/2507.05257 (2025). https://doi.org/10.48550/ARXIV.2507.05257 arXiv:2507.05257 [110] Yuanzhe Hu, Yu Wang, and Julian J. McAuley. 2025. Evaluating Memory in LLM Agents via Incremental Multi-Turn Interactions. CoRR abs/2507.05257 (2025). https://doi.org/10.48550/ARXIV.2507.05257 arXiv:2507.05257 [111] Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, and Ting Liu. 2025. Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions. ACM Trans. Inf. Syst. 43, 2 (2025), 42:142:55. https: //doi.org/10.1145/ [112] Qiaoli Huang, Jianrong Jia, Qiming Han, and Huan Luo. 2018. Fast-backward replay of sequentially memorized items in humans. elife 7 (2018), e35164. [113] Shenyang Huang, Cortney Howard, Mariam Hovhannisyan, Maureen Ritchey, Roberto Cabeza, and Simon Davis. 2024. Hippocampal functions modulate transfer-appropriate cortical representations supporting subsequent memory. Journal of Neuroscience 44, 1 (2024). [114] Tenghao Huang, Kinjal Basu, Ibrahim Abdelaziz, Pavan Kapanipathi, Jonathan May, and Muhao Chen. 2025. R2D2: Remembering, Reflecting and Dynamic Decision Making for Web Agents. CoRR abs/2501.12485 (2025). https: //doi.org/10.48550/ARXIV.2501.12485 arXiv:2501.12485 [115] Yangsibo Huang, Samyak Gupta, Zexuan Zhong, Kai Li, and Danqi Chen. 2023. Privacy Implications of Retrieval-Based Language Models. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023, Houda Bouamor, Juan Pino, and Kalika Bali (Eds.). Association for Computational Linguistics, 1488714902. https://doi.org/10.18653/V1/2023.EMNLP-MAIN.921 [116] Yizhe Huang, Yang Liu, Ruiyu Zhao, Xiaolong Zhong, Xingming Yue, and Ling Jiang. 2025. MemOrb: Plugand-Play Verbal-Reinforcement Memory Layer for E-Commerce Customer Service. CoRR abs/2509.18713 (2025). https://doi.org/10.48550/ARXIV.2509.18713 arXiv:2509. [117] Yunpeng Huang, Jingwei Xu, Zixu Jiang, Junyu Lai, Zenan Li, Yuan Yao, Taolue Chen, Lijuan Yang, Zhou Xin, and Xiaoxing Ma. 2023. Advancing Transformer Architecture in Long-Context Large Language Models: Comprehensive Survey. CoRR abs/2311.12351 (2023). https://doi.org/10.48550/ARXIV.2311.12351 arXiv:2311.12351 [118] Zhengjun Huang, Zhoujin Tian, Qintian Guo, Fangyuan Zhang, Yingli Zhou, Di Jiang, and Xiaofang Zhou. 2025. LiCoMemory: Lightweight and Cognitive Agentic Memory for Efficient Long-Term Reasoning. CoRR abs/2511.01448 (2025). https://doi.org/10.48550/ARXIV.2511.01448 arXiv:2511.01448 [119] Chelsi Jain, Yiran Wu, Yifan Zeng, Jiale Liu, S. hengyu Dai, Zhenwen Shao, Qingyun Wu, and Huazheng Wang. 2025. SimpleDoc: Multi-Modal Document Understanding with Dual-Cue Page Retrieval and Iterative Refinement. CoRR abs/2506.14035 (2025). https://doi.org/10.48550/ARXIV.2506.14035 arXiv:2506.14035 [120] Jitesh Jain, Shubham Maheshwari, Ning Yu, Wen-Mei W. Hwu, and Humphrey Shi. 2025. AUGUSTUS: An LLM-Driven Multimodal Agent System with Contextualized User Memory. CoRR abs/2510.15261 (2025). https://doi.org/10.48550/ ARXIV.2510.15261 arXiv:2510.15261 , Vol. 1, No. 1, Article . Publication date: December 2025. AI Meets Brain: Unified Survey on Memory Systems from Cognitive Neuroscience to Autonomous Agents 41 [121] Andrew Jesson, Nicolas Beltran-Velez, Quentin Chu, Sweta Karlekar, Jannik Kossen, Yarin Gal, John P. Cunningham, and David M. Blei. 2024. Estimating the Hallucination Rate of Generative AI. In Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024, Amir Globersons, Lester Mackey, Danielle Belgrave, Angela Fan, Ulrich Paquet, Jakub M. Tomczak, and Cheng Zhang (Eds.). http://papers.nips.cc/paper_files/paper/2024/hash/3791f5fc0e8e43730466afd2bcdb7493Abstract-Conference.html [122] Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Yejin Bang, Andrea Madotto, and Pascale Fung. 2023. Survey of Hallucination in Natural Language Generation. ACM Comput. Surv. 55, 12 (2023), 248:1248:38. https://doi.org/10.1145/3571730 [123] Shian Jia, Ziyang Huang, Xinbo Wang, Haofei Zhang, and Mingli Song. 2025. PISA: Pragmatic Psych-Inspired Unified Memory System for Enhanced AI Agency. CoRR abs/2510.15966 (2025). https://doi.org/10.48550/ARXIV.2510.15966 arXiv:2510.15966 [124] Zixi Jia, Qinghua Liu, Hexiao Li, Yuyan Chen, and Jiqiang Liu. 2025. Evaluating the Long-Term Memory of Large Language Models. In Findings of the Association for Computational Linguistics, ACL 2025, Vienna, Austria, July 27 - August 1, 2025, Wanxiang Che, Joyce Nabende, Ekaterina Shutova, and Mohammad Taher Pilehvar (Eds.). Association for Computational Linguistics, 1975919777. https://aclanthology.org/2025.findings-acl.1014/ [125] Bowen Jiang, Zhuoqun Hao, Young-Min Cho, Bryan Li, Yuan Yuan, Sihao Chen, Lyle H. Ungar, Camillo J. Taylor, and Dan Roth. 2025. Know Me, Respond to Me: Benchmarking LLMs for Dynamic User Profiling and Personalized Responses at Scale. CoRR abs/2504.14225 (2025). https://doi.org/10.48550/ARXIV.2504.14225 arXiv:2504.14225 [126] Hongda Jiang, Xinyuan Zhang, Siddhant Garg, Rishab Arora, Shiunzu Kuo, Jiayang Xu, Ankur Bansal, Christopher Brossman, Yue Liu, Aaron Colak, Ahmed Aly, Anuj Kumar, and Xin Luna Dong. 2025. Memory-QA: Answering Recall Questions Based on Multimodal Memories. CoRR abs/2509.18436 (2025). https://doi.org/10.48550/ARXIV.2509.18436 arXiv:2509. [127] Jinhao Jiang, Kun Zhou, Xin Zhao, Yang Song, Chen Zhu, Hengshu Zhu, and Ji-Rong Wen. 2025. KG-Agent: An Efficient Autonomous Agent Framework for Complex Reasoning over Knowledge Graph. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2025, Vienna, Austria, July 27 - August 1, 2025, Wanxiang Che, Joyce Nabende, Ekaterina Shutova, and Mohammad Taher Pilehvar (Eds.). Association for Computational Linguistics, 95059523. https://aclanthology.org/2025.acl-long.468/ [128] Shixin Jiang, Jiafeng Liang, Jiyuan Wang, Xuan Dong, Heng Chang, Weijiang Yu, Jinhua Du, Ming Liu, and Bing Qin. 2025. From Specific-MLLMs to Omni-MLLMs: Survey on MLLMs Aligned with Multi-modalities. In Findings of the Association for Computational Linguistics, ACL 2025, Vienna, Austria, July 27 - August 1, 2025, Wanxiang Che, Joyce Nabende, Ekaterina Shutova, and Mohammad Taher Pilehvar (Eds.). Association for Computational Linguistics, 86178652. https://aclanthology.org/2025.findings-acl.453/ [129] Wenjia Jiang, Yangyang Zhuang, Chenxi Song, Xu Yang, and Chi Zhang. 2025. AppAgentX: Evolving GUI Agents as Proficient Smartphone Users. CoRR abs/2503.02268 (2025). https://doi.org/10.48550/ARXIV.2503.02268 arXiv:2503.02268 [130] Xun Jiang, Feng Li, Han Zhao, Jiaying Wang, Jun Shao, Shihao Xu, Shu Zhang, Weiling Chen, Xavier Tang, Yize Chen, Mengyue Wu, Weizhi Ma, Mengdi Wang, and Tianqiao Chen. 2024. Long Term Memory: The Foundation of AI Self-Evolution. CoRR abs/2410.15665 (2024). https://doi.org/10.48550/ARXIV.2410.15665 arXiv:2410.15665 [131] Yuhua Jiang, Shuang Cheng, Yihao Liu, Ermo Hua, Che Jiang, Weigao Sun, Yu Cheng, Feifei Gao, Biqing Qi, and Bowen Zhou. 2025. Nirvana: Specialized Generalist Model With Task-Aware Memory Mechanism. CoRR abs/2510.26083 (2025). https://doi.org/10.48550/ARXIV.2510.26083 arXiv:2510.26083 [132] Zhouyu Jiang, Mengshu Sun, Lei Liang, and Zhiqiang Zhang. 2025. Retrieve, Summarize, Plan: Advancing Multi-hop Question Answering with an Iterative Approach. In Companion Proceedings of the ACM on Web Conference 2025, WWW 2025, Sydney, NSW, Australia, 28 April 2025 - 2 May 2025, Guodong Long, Michale Blumestein, Yi Chang, Liane Lewin-Eytan, Zi Helen Huang, and Elad Yom-Tov (Eds.). ACM, 16771686. https://doi.org/10.1145/3701716.3716889 [133] Hongbo Jin, Qingyuan Wang, Wenhao Zhang, Yang Liu, and Sijie Cheng. 2025. VideoMem: Enhancing Ultra-Long Video Understanding via Adaptive Memory Management. arXiv preprint arXiv:2512.04540 (2025). [134] Naizhu Jin, Zhong Li, Tian Zhang, and Qingkai Zeng. 2025. GUARD: Dual-Agent based Backdoor Defense on Chain-of-Thought in Neural Code Generation. In The 37th International Conference on Software Engineering and Knowledge Engineering, SEKE 2025, KSIR Virtual Conference Center, USA, September 29-30, 2025, Shi-Kuo Chang (Ed.). KSI Research Inc., 175180. https://doi.org/10.18293/SEKE2025-018 [135] Joshua Johansen, Lorenzo Diaz-Mataix, Hiroki Hamanaka, Takaaki Ozawa, Edgar Ycu, Jenny Koivumaa, Ashwani Kumar, Mian Hou, Karl Deisseroth, Edward Boyden, et al. 2014. Hebbian and neuromodulatory mechanisms interact to trigger associative memory formation. Proceedings of the National Academy of Sciences 111, 51 (2014), E5584E5592. [136] Tanya Jonker, Halle Dimsdale-Zucker, Maureen Ritchey, Alex Clarke, and Charan Ranganath. 2018. Neural reactivation in parietal cortex enhances memory for episodically linked information. Proceedings of the National Academy of Sciences 115, 43 (2018), 1108411089. , Vol. 1, No. 1, Article . Publication date: December 2025. 42 Liang, et al. [137] Karola Kaefer, Federico Stella, Bruce McNaughton, and Francesco Battaglia. 2022. Replay, the default mode network and the cascaded memory systems model. Nature Reviews Neuroscience 23, 10 (2022), 628640. [138] Tomoyuki Kagaya, Thong Jing Yuan, Yuxuan Lou, Jayashree Karlekar, Sugiri Pranata, Akira Kinose, Koki Oguri, Felix Wick, and Yang You. 2024. RAP: Retrieval-Augmented Planning with Contextual Memory for Multimodal LLM Agents. CoRR abs/2402.03610 (2024). https://doi.org/10.48550/ARXIV.2402.03610 arXiv:2402.03610 [139] Jan Kamiński and Ueli Rutishauser. 2020. Between persistently active and activity-silent frameworks: novel vistas on the cellular basis of working memory. Annals of the New York Academy of Sciences 1464, 1 (2020), 6475. [140] Jan Kamiński, Shannon Sullivan, Jeffrey Chung, Ian Ross, Adam Mamelak, and Ueli Rutishauser. 2017. Persistently active neurons in human medial frontal and medial temporal lobe support working memory. Nature neuroscience 20, 4 (2017), 590601. [141] Jiazheng Kang, Mingming Ji, Zhe Zhao, and Ting Bai. 2025. Memory OS of AI Agent. CoRR abs/2506.06326 (2025). https://doi.org/10.48550/ARXIV.2506.06326 arXiv:2506.06326 [142] Cai Ke, Yiming Du, Bin Liang, Yifan Xiang, Lin Gui, Zhongyang Li, Baojun Wang, Yue Yu, Hui Wang, Kam-Fai Wong, and Ruifeng Xu. 2025. Flexibly Utilize Memory for Long-Term Conversation via Fragment-then-Compose Framework. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, Christos Christodoulopoulos, Tanmoy Chakraborty, Carolyn Rose, and Violet Peng (Eds.). Association for Computational Linguistics, Suzhou, China, 2113021147. https://doi.org/10.18653/v1/2025.emnlp-main.1069 [143] Feyza Duman Keles, Pruthuvi Mahesakya Wijewardena, and Chinmay Hegde. 2023. On The Computational Complexity of Self-Attention. In International Conference on Algorithmic Learning Theory, February 20-23, 2023, Singapore (Proceedings of Machine Learning Research, Vol. 201), Shipra Agrawal and Francesco Orabona (Eds.). PMLR, 597619. https://proceedings.mlr.press/v201/duman-keles23a.html [144] Paige Kemp, Timothy Alexander, and Christopher Wahlheim. 2022. Recalling fake news during real news corrections can impair or enhance memory updating: The role of recollection-based retrieval. Cognitive Research: Principles and Implications 7, 1 (2022), 85. [145] Ossama Khalaf and Johannes Gräff. 2016. Structural, synaptic, and epigenetic dynamics of enduring memories. Neural Plasticity 2016, 1 (2016), 3425908. [146] Eunwon Kim, Chanho Park, and Buru Chang. 2025. SHARE: Shared Memory-Aware Open-Domain Long-Term Dialogue Dataset Constructed from Movie Script. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2025, Vienna, Austria, July 27 - August 1, 2025, Wanxiang Che, Joyce Nabende, Ekaterina Shutova, and Mohammad Taher Pilehvar (Eds.). Association for Computational Linguistics, 1447414498. https://aclanthology.org/2025.acl-long.704/ [147] Jiho Kim, Woosog Chay, Hyeonji Hwang, Daeun Kyung, Hyunseung Chung, Eunbyeol Cho, Yohan Jo, and Edward Choi. 2024. Dialsim: real-time simulator for evaluating long-term multi-party dialogue understanding of conversational agents. (2024). [148] Namyoung Kim, Kai Tzu-iunn Ong, Yeonjun Hwang, Minseok Kang, Iiseo Jihn, Gayoung Kim, Minju Kim, and Jinyoung Yeo. 2025. PRINCIPLES: Synthetic Strategy Memory for Proactive Dialogue Agents. CoRR abs/2509.17459 (2025). https://doi.org/10.48550/ARXIV.2509.17459 arXiv:2509.17459 [149] Sangyeop Kim, Yohan Lee, Sanghwa Kim, Hyunjong Kim, and Sungzoon Cho. 2025. Pre-Storage Reasoning for Episodic Memory: Shifting Inference Burden to Memory for Personalized Dialogue. CoRR abs/2509.10852 (2025). https://doi.org/10.48550/ARXIV.2509.10852 arXiv:2509.10852 [150] Seo Hyun Kim, Keummin Ka, Yohan Jo, Seung-won Hwang, Dongha Lee, and Jinyoung Yeo. 2024. Ever-Evolving Memory by Blending and Refining the Past. CoRR abs/2403.04787 (2024). https://doi.org/10.48550/ARXIV.2403.04787 arXiv:2403.04787 [151] Johannes Kirmayr, Lukas Stappen, Phillip Schneider, Florian Matthes, and Elisabeth André. 2025. CarMem: Enhancing Long-Term Memory in LLM Voice Assistants through Category-Bounding. In Proceedings of the 31st International Conference on Computational Linguistics, COLING 2025 - Industry Track, Abu Dhabi, UAE, January 19-24, 2025, Owen Rambow, Leo Wanner, Marianna Apidianaki, Hend Al-Khalifa, Barbara Di Eugenio, Steven Schockaert, Kareem Darwish, and Apoorv Agarwal (Eds.). Association for Computational Linguistics, 343357. https://aclanthology.org/ 2025.coling-industry.29/ [152] Ishant Kohar and Aswanth Krishnan. 2025. Benchmark for Procedural Memory Retrieval in Language Agents. arXiv:2511.21730 [cs.CL] https://arxiv.org/abs/2511.21730 [153] Luca Kolibius, Frederic Roux, George Parish, Marije Ter Wal, Mircea Van Der Plas, Ramesh Chelvarajah, Vijay Sawlani, David Rollings, Johannes Lang, Stephanie Gollwitzer, et al. 2023. Hippocampal neurons code individual episodic memories in humans. Nature human behaviour 7, 11 (2023), 19681979. [154] Yi Kong, Dianxi Shi, Guoli Yang, Zhang ke-di, Chenlin Huang, Xiaopeng Li, and Songchang Jin. 2025. MapAgent: Trajectory-Constructed Memory-Augmented Planning for Mobile Task Automation. CoRR abs/2507.21953 (2025). https://doi.org/10.48550/ARXIV.2507.21953 arXiv:2507.21953 , Vol. 1, No. 1, Article . Publication date: December 2025. AI Meets Brain: Unified Survey on Memory Systems from Cognitive Neuroscience to Autonomous Agents 43 [155] Saana Korkki, Franziska Richter, and Jon Simons. 2021. Hippocampalcortical encoding activity predicts the precision of episodic memory. Journal of Cognitive Neuroscience 33, 11 (2021), 23282341. [156] Peter Kraemer, Regina Weilbächer, Tehilla Mechera-Ostrovsky, and Sebastian Gluth. 2022. Cognitive and neural principles of memory bias on preferential choices. Current Research in Neurobiology 3 (2022), 100029. [157] Yuri Kuratov, Aydar Bulatov, Petr Anokhin, Ivan Rodkin, Dmitry Sorokin, Artyom Y. Sorokin, and Mikhail Burtsev. 2024. BABILong: Testing the Limits of LLMs with Long Context Reasoning-in-a-Haystack. In Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024, Amir Globersons, Lester Mackey, Danielle Belgrave, Angela Fan, Ulrich Paquet, Jakub M. Tomczak, and Cheng Zhang (Eds.). http://papers.nips.cc/paper_files/paper/2024/hash/ c0d62e70dbc659cc9bd44cbcf1cb652f-Abstract-Datasets_and_Benchmarks_Track.html [158] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. 2023. Efficient Memory Management for Large Language Model Serving with PagedAttention. In Proceedings of the 29th Symposium on Operating Systems Principles, SOSP 2023, Koblenz, Germany, October 23-26, 2023, Jason Flinn, Margo I. Seltzer, Peter Druschel, Antoine Kaufmann, and Jonathan Mace (Eds.). ACM, 611626. https://doi.org/10.1145/3600006.3613165 [159] Yilong Lai, Yipin Yang, Jialong Wu, Fengran Mo, Zhenglin Wang, Ting Liang, Jianguo Lin, and Keping Yang. 2025. CRMWeaver: Building Powerful Business Agent via Agentic RL and Shared Memories. CoRR abs/2510.25333 (2025). https://doi.org/10.48550/ARXIV.2510.25333 arXiv:2510.25333 [160] Kenneth Latimer and David Freedman. 2023. Low-dimensional encoding of decisions in parietal cortex reflects long-term training history. Nature Communications 14, 1 (2023), 1010. [161] Donghyun Lee and Mo Tiwari. 2024. Prompt Infection: LLM-to-LLM Prompt Injection within Multi-Agent Systems. CoRR abs/2410.07283 (2024). https://doi.org/10.48550/ARXIV.2410.07283 arXiv:2410. [162] Jonathan LC Lee, Karim Nader, and Daniela Schiller. 2017. An update on memory reconsolidation updating. Trends in cognitive sciences 21, 7 (2017), 531545. [163] Kuang-Huei Lee, Xinyun Chen, Hiroki Furuta, John F. Canny, and Ian Fischer. 2024. Human-Inspired Reading Agent with Gist Memory of Very Long Contexts. In Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net. https://openreview.net/forum?id=OTmcsyEO5G [164] Xiang Lei, Qin Li, Min Zhang, and Min Zhang. 2025. D-SMART: Enhancing LLM Dialogue Consistency via Dynamic Structured Memory And Reasoning Tree. CoRR abs/2510.13363 (2025). https://doi.org/10.48550/ARXIV.2510.13363 arXiv:2510.13363 [165] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela. 2020. Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, Hugo Larochelle, MarcAurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (Eds.). https://proceedings.neurips.cc/paper/2020/ hash/6b493230205f780e1bc26945df7481e5-Abstract.html [166] Baolin Li, Yankai Jiang, Vijay Gadepally, and Devesh Tiwari. 2024. LLM Inference Serving: Survey of Recent Advances and Opportunities. In IEEE High Performance Extreme Computing Conference, HPEC 2024, Wakefield, MA, USA, September 23-27, 2024. IEEE, 18. https://doi.org/10.1109/HPEC62836.2024. [167] Hao Li, Chenghao Yang, An Zhang, Yang Deng, Xiang Wang, and Tat-Seng Chua. 2025. Hello Again! LLM-powered Personalized Agent for Long-term Dialogue. In Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL 2025 - Volume 1: Long Papers, Albuquerque, New Mexico, USA, April 29 - May 4, 2025, Luis Chiruzzo, Alan Ritter, and Lu Wang (Eds.). Association for Computational Linguistics, 52595276. https://doi.org/10.18653/V1/2025.NAACL-LONG.272 [168] Hsin-Hung Li, Thomas Sprague, Aspen Yoo, Wei Ji Ma, and Clayton Curtis. 2025. Neural mechanisms of resource allocation in working memory. Science Advances 11, 15 (2025), eadr8015. [169] Junnan Li, Dongxu Li, Silvio Savarese, and Steven C. H. Hoi. 2023. BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models. In International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA (Proceedings of Machine Learning Research, Vol. 202), Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett (Eds.). PMLR, 1973019742. https://proceedings.mlr.press/v202/li23q.html [170] Jiaqi Li, Xiaobo Wang, Zihao Wang, and Zilong Zheng. 2024. RAM: Towards an Ever-Improving Memory System https://doi.org/10.48550/ARXIV.2404.12045 by Learning from Communications. CoRR abs/2404.12045 (2024). arXiv:2404.12045 [171] Rui Li, Zeyu Zhang, Xiaohe Bo, Zihang Tian, Xu Chen, Quanyu Dai, Zhenhua Dong, and Ruiming Tang. 2025. CAM: Constructivist View of Agentic Memory for LLM-Based Reading Comprehension. CoRR abs/2510.05520 (2025). https://doi.org/10.48550/ARXIV.2510.05520 arXiv:2510. , Vol. 1, No. 1, Article . Publication date: December 2025. 44 Liang, et al. [172] Xiaoxi Li, Wenxiang Jiao, Jiarui Jin, Guanting Dong, Jiajie Jin, Yinuo Wang, Hao Wang, Yutao Zhu, Ji-Rong Wen, Yuan Lu, and Zhicheng Dou. 2025. DeepAgent: General Reasoning Agent with Scalable Toolsets. CoRR abs/2510.21618 (2025). https://doi.org/10.48550/ARXIV.2510.21618 arXiv:2510.21618 [173] Xuechen Liang, Meiling Tao, Tianyu Shi, and Yiting Xie. 2024. CMAT: Multi-Agent Collaboration Tuning Framework for Enhancing Small Language Models. CoRR abs/2404.01663 (2024). https://doi.org/10.48550/ARXIV.2404.01663 arXiv:2404.01663 [174] Xuechen Liang, Meiling Tao, Yinghui Xia, Jianhui Wang, Kun Li, Yijin Wang, Yangfan He, Jingsong Yang, Tianyu Shi, Yuantao Wang, Miao Zhang, and Xueqian Wang. 2025. SAGE: Self-evolving Agents with Reflective and Memoryaugmented Abilities. Neurocomputing 647 (2025), 130470. https://doi.org/10.1016/J.NEUCOM.2025.130470 [175] Tobias Lindenbauer, Georg Groh, and Hinrich Schütze. 2025. From Knowledge to Noise: CTIM-Rover and the Pitfalls of Episodic Memory in Software Engineering Agents. CoRR abs/2505.23422 (2025). https://doi.org/10.48550/ARXIV. 2505.23422 arXiv:2505. [176] Bang Liu, Xinfeng Li, Jiayi Zhang, Jinlin Wang, Tanjin He, Sirui Hong, Hongzhang Liu, Shaokun Zhang, Kaitao Song, Kunlun Zhu, Yuheng Cheng, Suyuchen Wang, Xiaoqiang Wang, Yuyu Luo, Haibo Jin, Peiyan Zhang, Ollie Liu, Jiaqi Chen, Huan Zhang, Zhaoyang Yu, Haochen Shi, Boyan Li, Dekun Wu, Fengwei Teng, Xiaojun Jia, Jiawei Xu, Jinyu Xiang, Yizhang Lin, Tianming Liu, Tongliang Liu, Yu Su, Huan Sun, Glen Berseth, Jianyun Nie, Ian T. Foster, Logan T. Ward, Qingyun Wu, Yu Gu, Mingchen Zhuge, Xiangru Tang, Haohan Wang, Jiaxuan You, Chi Wang, Jian Pei, Qiang Yang, Xiaoliang Qi, and Chenglin Wu. 2025. Advances and Challenges in Foundation Agents: From Brain-Inspired Intelligence to Evolutionary, Collaborative, and Safe Systems. CoRR abs/2504.01990 (2025). https://doi.org/10.48550/ARXIV.2504.01990 arXiv:2504.01990 [177] Dongshuo Liu, Zhijing Wu, Dandan Song, and Heyan Huang. 2025. Persona-Aware LLM-Enhanced Framework for Multi-Session Personalized Dialogue Generation. In Findings of the Association for Computational Linguistics, ACL 2025, Vienna, Austria, July 27 - August 1, 2025, Wanxiang Che, Joyce Nabende, Ekaterina Shutova, and Mohammad Taher Pilehvar (Eds.). Association for Computational Linguistics, 103123. https://aclanthology.org/2025.findings-acl.5/ [178] Jun Liu, Zhenglun Kong, Changdi Yang, Fan Yang, Tianqi Li, Peiyan Dong, Joannah Nanjekye, Hao Tang, Geng Yuan, Wei Niu, Wenbin Zhang, Pu Zhao, Xue Lin, Dong Huang, and Yanzhi Wang. 2025. RCR-Router: Efficient Role-Aware Context Routing for Multi-Agent LLM Systems with Structured Memory. CoRR abs/2508.04903 (2025). https://doi.org/10.48550/ARXIV.2508.04903 arXiv:2508.04903 [179] Junming Liu, Yifei Sun, Weihua Cheng, Haodong Lei, Yirong Chen, Licheng Wen, Xuemeng Yang, Daocheng Fu, Pinlong Cai, Nianchen Deng, Yi Yu, Shuyue Hu, Botian Shi, and Ding Wang. 2025. MemVerse: Multimodal Memory for Lifelong Learning Agents. arXiv:2512.03627 [cs.AI] https://arxiv.org/abs/2512.03627 [180] Lei Liu, Xiaoyan Yang, Yue Shen, Binbin Hu, Zhiqiang Zhang, Jinjie Gu, and Guannan Zhang. 2023. Think-inMemory: Recalling and Post-thinking Enable LLMs with Long-Term Memory. CoRR abs/2311.08719 (2023). https: //doi.org/10.48550/ARXIV.2311.08719 arXiv:2311.08719 [181] Na Liu, Liangyu Chen, Xiaoyu Tian, Wei Zou, Kaijiang Chen, and Ming Cui. 2024. From LLM to Conversational Agent: Memory Enhanced Architecture with Fine-Tuning of Large Language Models. CoRR abs/2401.02777 (2024). https://doi.org/10.48550/ARXIV.2401.02777 arXiv:2401. [182] Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. 2024. Lost in the Middle: How Language Models Use Long Contexts. Trans. Assoc. Comput. Linguistics 12 (2024), 157173. https://doi.org/10.1162/TACL_A_00638 [183] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Qing Jiang, Chunyuan Li, Jianwei Yang, Hang Su, Jun Zhu, and Lei Zhang. 2024. Grounding DINO: Marrying DINO with Grounded Pre-training for Open-Set Object Detection. In Computer Vision - ECCV 2024 - 18th European Conference, Milan, Italy, September 29-October 4, 2024, Proceedings, Part XLVII (Lecture Notes in Computer Science, Vol. 15105), Ales Leonardis, Elisa Ricci, Stefan Roth, Olga Russakovsky, Torsten Sattler, and Gül Varol (Eds.). Springer, 3855. https://doi.org/10.1007/978-3-031-72970-6_3 [184] Xiaochuan Liu, Ruihua Song, Xiting Wang, and Xu Chen. 2025. Select, Read, and Write: Multi-Agent Framework of Full-Text-based Related Work Generation. In Findings of the Association for Computational Linguistics, ACL 2025, Vienna, Austria, July 27 - August 1, 2025, Wanxiang Che, Joyce Nabende, Ekaterina Shutova, and Mohammad Taher Pilehvar (Eds.). Association for Computational Linguistics, 70097028. https://aclanthology.org/2025.findings-acl.366/ [185] Yunzhe Liu, Raymond Dolan, Zeb Kurth-Nelson, and Timothy EJ Behrens. 2019. Human replay spontaneously reorganizes experience. Cell 178, 3 (2019), 640652. [186] Yitao Liu, Chenglei Si, Karthik R. Narasimhan, and Shunyu Yao. 2025. Contextual Experience Replay for SelfImprovement of Language Agents. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2025, Vienna, Austria, July 27 - August 1, 2025, Wanxiang Che, Joyce Nabende, Ekaterina Shutova, and Mohammad Taher Pilehvar (Eds.). Association for Computational Linguistics, 1417914198. https://aclanthology.org/2025.acl-long.694/ , Vol. 1, No. 1, Article . Publication date: December 2025. AI Meets Brain: Unified Survey on Memory Systems from Cognitive Neuroscience to Autonomous Agents 45 [187] Zhiwei Liu, Weiran Yao, Jianguo Zhang, Liangwei Yang, Zuxin Liu, Juntao Tan, Prafulla Kumar Choubey, Tian Lan, Jason Wu, Huan Wang, Shelby Heinecke, Caiming Xiong, and Silvio Savarese. 2024. AgentLite: Lightweight Library for Building and Advancing Task-Oriented LLM Agent System. CoRR abs/2402.15538 (2024). https://doi.org/10.48550/ ARXIV.2402.15538 arXiv:2402.15538 [188] Zhengyuan Liu, Stella Xin Yin, Bryan Chen Zhengyu Tan, Roy Ka-Wei Lee, Guimei Liu, Dion Hoe-Lian Goh, Wenya Wang, and Nancy Chen. 2025. The Imperfect Learner: Incorporating Developmental Trajectories in Memory-based Student Simulation. arXiv preprint arXiv:2511.05903 (2025). [189] Lin Long, Yichen He, Wentao Ye, Yiyuan Pan, Yuan Lin, Hang Li, Junbo Zhao, and Wei Li. 2025. Seeing, Listening, Remembering, and Reasoning: Multimodal Agent with Long-Term Memory. CoRR abs/2508.09736 (2025). https: //doi.org/10.48550/ARXIV.2508.09736 arXiv:2508.09736 [190] Lin Long, Yichen He, Wentao Ye, Yiyuan Pan, Yuan Lin, Hang Li, Junbo Zhao, and Wei Li. 2025. Seeing, Listening, Remembering, and Reasoning: Multimodal Agent with Long-Term Memory. CoRR abs/2508.09736 (2025). https: //doi.org/10.48550/ARXIV.2508.09736 arXiv:2508. [191] Lin Long, Yichen He, Wentao Ye, Yiyuan Pan, Yuan Lin, Hang Li, Junbo Zhao, and Wei Li. 2025. Seeing, Listening, Remembering, and Reasoning: Multimodal Agent with Long-Term Memory. CoRR abs/2508.09736 (2025). https: //doi.org/10.48550/ARXIV.2508.09736 arXiv:2508.09736 [192] Miao Lu, Weiwei Sun, Weihua Du, Zhan Ling, Xuesong Yao, Kang Liu, and Jiecao Chen. 2025. Scaling LLM Multi-turn RL with End-to-end Summarization-based Context Management. CoRR abs/2510.06727 (2025). https://doi.org/10. 48550/ARXIV.2510.06727 arXiv:2510.06727 [193] Elias Lumer, Anmol Gulati, Vamse Kumar Subbiah, Pradeep Honaganahalli Basavaraju, and James A. Burke. 2025. MemTool: Optimizing Short-Term Memory Management for Dynamic Tool Calling in LLM Agent Multi-Turn Conversations. CoRR abs/2507.21428 (2025). https://doi.org/10.48550/ARXIV.2507.21428 arXiv:2507.21428 [194] Adyasha Maharana, Dong-Ho Lee, Sergey Tulyakov, Mohit Bansal, Francesco Barbieri, and Yuwei Fang. 2024. Evaluating Very Long-Term Conversational Memory of LLM Agents. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024, Lun-Wei Ku, Andre Martins, and Vivek Srikumar (Eds.). Association for Computational Linguistics, 1385113870. https://doi.org/10.18653/V1/2024.ACL-LONG.747 [195] Rolando Masís-Obando, Kenneth Norman, and Christopher Baldassano. 2022. Schema representations in distinct brain networks support narrative memory during encoding and retrieval. Elife 11 (2022), e70445. [196] Kai Mei, Zelong Li, Shuyuan Xu, Ruosong Ye, Yingqiang Ge, and Yongfeng Zhang. 2024. AIOS: LLM Agent Operating System. CoRR abs/2403.16971 (2024). https://doi.org/10.48550/ARXIV.2403.16971 arXiv:2403.16971 [197] Grégoire Mialon, Clémentine Fourrier, Thomas Wolf, Yann LeCun, and Thomas Scialom. 2024. GAIA: benchmark for General AI Assistants. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net. https://openreview.net/forum?id=fibxvahvs3 [198] Akash Mishra, Gelana Tostaeva, Maximilian Nentwich, Elizabeth Espinal, Noah Markowitz, Jalen Winfield, Elisabeth Freund, Sabina Gherman, Marcin Leszczynski, Charles Schroeder, et al. 2025. Motifs of human high-frequency oscillations structure processing and memory of continuous audiovisual narratives. Science Advances 11, 30 (2025), eadv0986. [199] Atsuyuki Miyai, Zaiying Zhao, Kazuki Egashira, Atsuki Sato, Tatsumi Sunada, Shota Onohara, Hiromasa Yamanishi, Mashiro Toyooka, Kunato Nishina, Ryoma Maeda, Kiyoharu Aizawa, and Toshihiko Yamasaki. 2025. WebChoreArena: Evaluating Web Browsing Agents on Realistic Tedious Web Tasks. CoRR abs/2506.01952 (2025). https://doi.org/10. 48550/ARXIV.2506.01952 arXiv:2506.01952 [200] Ce Mo, Junshi Lu, Bichan Wu, Jianrong Jia, Huan Luo, and Fang Fang. 2019. Competing rhythmic neural representations of orientations during concurrent attention to multiple orientation features. Nature Communications 10, 1 (2019), 5264. [201] Ali Modarressi, Abdullatif Köksal, Ayyoob Imani, Mohsen Fayyaz, and Hinrich Schütze. 2025. MemLLM: Finetuning LLMs to Use Explicit Read-Write Memory. Trans. Mach. Learn. Res. 2025 (2025). https://openreview.net/forum?id= dghM7sOudh [202] Gianluigi Mongillo, Omri Barak, and Misha Tsodyks. 2008. Synaptic theory of working memory. Science 319, 5869 (2008), 15431546. [203] Xinyi Mou, Zhongyu Wei, and Xuanjing Huang. 2024. Unveiling the Truth and Facilitating Change: Towards Agentbased Large-scale Social Movement Simulation. In Findings of the Association for Computational Linguistics, ACL 2024, Bangkok, Thailand and virtual meeting, August 11-16, 2024, Lun-Wei Ku, Andre Martins, and Vivek Srikumar (Eds.). Association for Computational Linguistics, 47894809. https://doi.org/10.18653/V1/2024.FINDINGS-ACL.285 [204] Xinyi Mou, Zhongyu Wei, and Xuanjing Huang. 2024. Unveiling the Truth and Facilitating Change: Towards Agentbased Large-scale Social Movement Simulation. In Findings of the Association for Computational Linguistics, ACL 2024, Bangkok, Thailand and virtual meeting, August 11-16, 2024, Lun-Wei Ku, Andre Martins, and Vivek Srikumar (Eds.). , Vol. 1, No. 1, Article . Publication date: December 2025. Liang, et al. Association for Computational Linguistics, 47894809. https://doi.org/10.18653/V1/2024.FINDINGS-ACL.285 [205] Fangwen Mu, Junjie Wang, Lin Shi, Song Wang, Shoubin Li, and Qing Wang. 2025. EXPEREPAIR: Dual-Memory Enhanced LLM-based Repository-Level Program Repair. CoRR abs/2506.10484 (2025). https://doi.org/10.48550/ARXIV. 2506.10484 arXiv:2506.10484 [206] Asen Nachkov, Xi Wang, and Luc Van Gool. 2025. LLM Agents Beyond Utility: An Open-Ended Perspective. CoRR abs/2510.14548 (2025). https://doi.org/10.48550/ARXIV.2510.14548 arXiv:2510.14548 [207] James Nairne and Josefa NS Pandeirada. 2016. Adaptive memory: The evolutionary significance of survival processing. Perspectives on Psychological Science 11, 4 (2016), 496511. [208] Jiayan Nan, Wenquan Ma, Wenlong Wu, and Yize Chen. 2025. Nemori: Self-Organizing Agent Memory Inspired by Cognitive Science. CoRR abs/2508.03341 (2025). https://doi.org/10.48550/ARXIV.2508.03341 arXiv:2508.03341 [209] Xuan-Phi Nguyen, Shrey Pandit, Revanth Gangi Reddy, Austin Xu, Silvio Savarese, Caiming Xiong, and Shafiq Joty. 2025. SFR-DeepResearch: Towards Effective Reinforcement Learning for Autonomously Reasoning Single Agents. CoRR abs/2509.06283 (2025). https://doi.org/10.48550/ARXIV.2509.06283 arXiv:2509.06283 [210] Guangtao Nie, Rong Zhi, Xiaofan Yan, Yufan Du, Xiangyang Zhang, Jianwei Chen, Mi Zhou, Hongshen Chen, Tianhao Li, Ziguang Cheng, Sulong Xu, and Jinghe Hu. 2024. Hybrid Multi-Agent Conversational Recommender System with LLM and Search Engine in E-commerce. In Proceedings of the 18th ACM Conference on Recommender Systems, RecSys 2024, Bari, Italy, October 14-18, 2024, Tommaso Di Noia, Pasquale Lops, Thorsten Joachims, Katrien Verbert, Pablo Castells, Zhenhua Dong, and Ben London (Eds.). ACM, 745747. https://doi.org/10.1145/3640457.3688061 [211] Dennis Norris. 2017. Short-term memory and long-term memory are still different. Psychological bulletin 143, 9 (2017), 992. [212] Kai Tzu-iunn Ong, Namyoung Kim, Minju Gwak, Hyungjoo Chae, Taeyoon Kwon, Yohan Jo, Seung-won Hwang, Dongha Lee, and Jinyoung Yeo. 2025. Towards Lifelong Dialogue Agents via Timeline-based Memory Management. In Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL 2025 - Volume 1: Long Papers, Albuquerque, New Mexico, USA, April 29 - May 4, 2025, Luis Chiruzzo, Alan Ritter, and Lu Wang (Eds.). Association for Computational Linguistics, 86318661. https://doi.org/10.18653/V1/2025.NAACL-LONG.435 [213] Siru Ouyang, Jun Yan, I-Hung Hsu, Yanfei Chen, Ke Jiang, Zifeng Wang, Rujun Han, Long T. Le, Samira Daruki, Xiangru Tang, Vishy Tirumalashetty, George Lee, Mahsan Rofouei, Hangfei Lin, Jiawei Han, Chen-Yu Lee, and Tomas Pfister. 2025. ReasoningBank: Scaling Agent Self-Evolving with Reasoning Memory. CoRR abs/2509.25140 (2025). https://doi.org/10.48550/ARXIV.2509.25140 arXiv:2509.25140 [214] Pacheco Estefan, Martí Sánchez-Fibla, Armin Duff, Alessandro Principe, Rodrigo Rocamora, Hui Zhang, Nikolai Axmacher, and Paul FMJ Verschure. 2019. Coordinated representational reinstatement in the human hippocampus and lateral temporal cortex during episodic memory retrieval. Nature communications 10, 1 (2019), 2255. [215] Charles Packer, Vivian Fang, Shishir G. Patil, Kevin Lin, Sarah Wooders, and Joseph E. Gonzalez. 2023. MemGPT: Towards LLMs as Operating Systems. CoRR abs/2310.08560 (2023). https://doi.org/10.48550/ARXIV.2310.08560 arXiv:2310.08560 [216] Daniela Palombo, Claude Alain, Hedvig Söderlund, Wayne Khuu, and Brian Levine. 2015. Severely deficient autobiographical memory (SDAM) in healthy adults: new mnemonic syndrome. Neuropsychologia 72 (2015), 105118. [217] Dong-ni Pan, CuiZhu Lin, Ma Xin, Oliver Wolf, Gui Xue, and Xuebing Li. 2025. Understanding episodic memory dynamics: Retrieval and updating mechanisms revealed by fMRI and tDCS. NeuroImage 310 (2025), 121170. [218] Zhuoshi Pan, Qianhui Wu, Huiqiang Jiang, Xufang Luo, Hao Cheng, Dongsheng Li, Yuqing Yang, Chin-Yew Lin, H. Vicky Zhao, Lili Qiu, and Jianfeng Gao. 2025. SeCom: On Memory Construction and Retrieval for Personalized Conversational Agents. In The Thirteenth International Conference on Learning Representations, ICLR 2025, Singapore, April 24-28, 2025. OpenReview.net. https://openreview.net/forum?id=xKDZAW0He3 [219] Doyoung Park, Seong-Hwan Hwang, Keonwoo Lee, Yeeun Ryoo, Hyoung Kim, and Sue-Hyun Lee. 2025. Supramodal and cross-modal representations of working memory in higher-order cortex. Nature Communications 16, 1 (2025), 4497. [220] Joon Sung Park, Joseph C. OBrien, Carrie Jun Cai, Meredith Ringel Morris, Percy Liang, and Michael S. Bernstein. 2023. Generative Agents: Interactive Simulacra of Human Behavior. In Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology, UIST 2023, San Francisco, CA, USA, 29 October 20231 November 2023, Sean Follmer, Jeff Han, Jürgen Steimle, and Nathalie Henry Riche (Eds.). ACM, 2:12:22. https://doi.org/10.1145/3586183. 3606763 [221] Joon Sung Park, Joseph C. OBrien, Carrie Jun Cai, Meredith Ringel Morris, Percy Liang, and Michael S. Bernstein. 2023. Generative Agents: Interactive Simulacra of Human Behavior. In Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology, UIST 2023, San Francisco, CA, USA, 29 October 20231 November 2023, Sean Follmer, Jeff Han, Jürgen Steimle, and Nathalie Henry Riche (Eds.). ACM, 2:12:22. https://doi.org/10.1145/3586183. , Vol. 1, No. 1, Article . Publication date: December 2025. AI Meets Brain: Unified Survey on Memory Systems from Cognitive Neuroscience to Autonomous Agents 47 3606763 [222] Seongmin Park, Douglas Miller, Hamed Nili, Charan Ranganath, and Erie Boorman. 2020. Map making: constructing, combining, and inferring on abstract cognitive maps. Neuron 107, 6 (2020), 12261238. [223] Atharv Singh Patlan, S. Ashwin Hebbar, Pramod Viswanath, and Prateek Mittal. 2025. Context manipulation attacks : Web agents are susceptible to corrupted memory. CoRR abs/2506.17318 (2025). https://doi.org/10.48550/ARXIV.2506. 17318 arXiv:2506.17318 [224] Zehan Qi, Xiao Liu, Iat Long Iong, Hanyu Lai, Xueqiao Sun, Jiadai Sun, Xinyue Yang, Yu Yang, Shuntian Yao, Wei Xu, Jie Tang, and Yuxiao Dong. 2025. WebRL: Training LLM Web Agents via Self-Evolving Online Curriculum Reinforcement Learning. In The Thirteenth International Conference on Learning Representations, ICLR 2025, Singapore, April 24-28, 2025. OpenReview.net. https://openreview.net/forum?id=oVKEAFjEqv [225] Chen Qian, Zihao Xie, Yifei Wang, Wei Liu, Kunlun Zhu, Hanchen Xia, Yufan Dang, Zhuoyun Du, Weize Chen, Cheng Yang, Zhiyuan Liu, and Maosong Sun. 2025. Scaling Large Language Model-based Multi-Agent Collaboration. In The Thirteenth International Conference on Learning Representations, ICLR 2025, Singapore, April 24-28, 2025. OpenReview.net. https://openreview.net/forum?id=K3n5jPkrU6 [226] Zile Qiao, Guoxin Chen, Xuanzhong Chen, Donglei Yu, Wenbiao Yin, Xinyu Wang, Zhen Zhang, Baixuan Li, Huifeng Yin, Kuan Li, Rui Min, Minpeng Liao, Yong Jiang, Pengjun Xie, Fei Huang, and Jingren Zhou. 2025. WebResearcher: Unleashing unbounded reasoning capability in Long-Horizon Agents. CoRR abs/2509.13309 (2025). https://doi.org/ 10.48550/ARXIV.2509.13309 arXiv:2509.13309 [227] Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, Sihan Zhao, Lauren Hong, Runchu Tian, Ruobing Xie, Jie Zhou, Mark Gerstein, Dahai Li, Zhiyuan Liu, and Maosong Sun. 2024. ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net. https://openreview.net/forum?id=dHng2O0Jjr [228] Zihan Qiu, Zekun Wang, Bo Zheng, Zeyu Huang, Kaiyue Wen, Songlin Yang, Rui Men, Le Yu, Fei Huang, Suozhi Huang, Dayiheng Liu, Jingren Zhou, and Junyang Lin. 2025. Gated Attention for Large Language Models: Nonlinearity, Sparsity, and Attention-Sink-Free. CoRR abs/2505.06708 (2025). https://doi.org/10.48550/ARXIV.2505.06708 arXiv:2505. [229] Chowdhury Mofizur Rahman, Mahbub E. Sobhani, Anika Tasnim Rodela, and Swakkhar Shatabda. 2024. An Enhanced Text Compression Approach Using Transformer-based Language Models. CoRR abs/2412.15250 (2024). https: //doi.org/10.48550/ARXIV.2412.15250 arXiv:2412.15250 [230] Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. 2020. ZeRO: memory optimizations toward training trillion parameter models. In Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis, SC 2020, Virtual Event / Atlanta, Georgia, USA, November 9-19, 2020, Christine Cuicchi, Irene Qualters, and William T. Kramer (Eds.). IEEE/ACM, 20. https://doi.org/10.1109/SC41405.2020.00024 [231] Nir Ratner, Yoav Levine, Yonatan Belinkov, Ori Ram, Inbal Magar, Omri Abend, Ehud Karpas, Amnon Shashua, Kevin Leyton-Brown, and Yoav Shoham. 2023. Parallel Context Windows for Large Language Models. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023, Anna Rogers, Jordan L. Boyd-Graber, and Naoaki Okazaki (Eds.). Association for Computational Linguistics, 63836402. https://doi.org/10.18653/V1/2023.ACL-LONG.352 [232] Zachariah Reagh and Charan Ranganath. 2023. Flexible reuse of cortico-hippocampal representations during encoding and recall of naturalistic events. Nature Communications 14, 1 (2023), 1279. [233] Louis Renoult, Muireann Irish, Morris Moscovitch, and Michael Rugg. 2019. From knowing to remembering: the semanticepisodic distinction. Trends in cognitive sciences 23, 12 (2019), 10411057. [234] Alireza Rezazadeh, Zichao Li, Ange Lou, Yuying Zhao, Wei Wei, and Yujia Bao. 2025. Collaborative Memory: Multi-User Memory Sharing in LLM Agents with Dynamic Access Control. CoRR abs/2505.18279 (2025). https: //doi.org/10.48550/ARXIV.2505.18279 arXiv:2505.18279 [235] Alireza Rezazadeh, Zichao Li, Wei Wei, and Yujia Bao. 2025. From Isolated Conversations to Hierarchical Schemas: Dynamic Tree Memory Representation for LLMs. In The Thirteenth International Conference on Learning Representations, ICLR 2025, Singapore, April 24-28, 2025. OpenReview.net. https://openreview.net/forum?id=moXtEmCleY [236] Henry Roediger III and Magdalena Abel. 2022. The double-edged sword of memory retrieval. Nature Reviews Psychology 1, 12 (2022), 708720. [237] Cenk Sahin, Melek Demirtas, Rizvan Erol, Adil Baykasoglu, and Vahit Kaplanoglu. 2017. multi-agent based approach to dynamic scheduling with flexible processing capabilities. J. Intell. Manuf. 28, 8 (2017), 18271845. https://doi.org/10.1007/S10845-015-1069-X [238] Daniel Schacter, Roland Benoit, and Karl Szpunar. 2017. Episodic future thinking: Mechanisms and functions. Current opinion in behavioral sciences 17 (2017), 4150. , Vol. 1, No. 1, Article . Publication date: December 2025. 48 Liang, et al. [239] Brandon Schmeichel, Rachael Volokhov, and Heath Demaree. 2008. Working memory capacity and the self-regulation of emotional expression and experience. Journal of personality and social psychology 95, 6 (2008), 1526. [240] Asif Shahriar, Md Nafiu Rahman, Sadif Ahmed, Farig Sadeque, and Md. Rizwan Parvez. 2025. Survey on Agentic Security: Applications, Threats and Defenses. CoRR abs/2510.06445 (2025). https://doi.org/10.48550/ARXIV.2510.06445 arXiv:2510.06445 [241] Ming-Tung Shen and Yuh-Jzer Joung. 2025. TALM: Dynamic Tree-Structured Multi-Agent Framework with Long-Term Memory for Scalable Code Generation. CoRR abs/2510.23010 (2025). https://doi.org/10.48550/ARXIV.2510.23010 arXiv:2510. [242] Yuchen Shi, Guochao Jiang, Tian Qiu, and Deqing Yang. 2024. AgentRE: An Agent-Based Framework for Navigating Complex Information Landscapes in Relation Extraction. In Proceedings of the 33rd ACM International Conference on Information and Knowledge Management, CIKM 2024, Boise, ID, USA, October 21-25, 2024, Edoardo Serra and Francesca Spezzano (Eds.). ACM, 20452055. https://doi.org/10.1145/3627673.3679791 [243] Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. 2023. Reflexion: language agents with verbal reinforcement learning. In Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine (Eds.). http://papers.nips.cc/ paper_files/paper/2023/hash/1b44b878bb782e6954cd888628510e90-Abstract-Conference.html [244] Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. 2023. Reflexion: language agents with verbal reinforcement learning. In Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine (Eds.). http://papers.nips.cc/ paper_files/paper/2023/hash/1b44b878bb782e6954cd888628510e90-Abstract-Conference.html [245] Ignasi Sols, Sarah DuBrow, Lila Davachi, and Lluís Fuentemilla. 2017. Event boundaries trigger rapid memory reinstatement of the prior events to promote their representation in long-term memory. Current Biology 27, 22 (2017), 34993504. [246] Enxin Song, Wenhao Chai, Guanhong Wang, Yucheng Zhang, Haoyang Zhou, Feiyang Wu, Haozhe Chi, Xun Guo, Tian Ye, Yanting Zhang, Yan Lu, Jenq-Neng Hwang, and Gaoang Wang. 2024. MovieChat: From Dense Token to Sparse Memory for Long Video Understanding. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2024, Seattle, WA, USA, June 16-22, 2024. IEEE, 1822118232. https://doi.org/10.1109/CVPR52733.2024.01725 [247] Enxin Song, Wenhao Chai, Guanhong Wang, Yucheng Zhang, Haoyang Zhou, Feiyang Wu, Haozhe Chi, Xun Guo, Tian Ye, Yanting Zhang, Yan Lu, Jenq-Neng Hwang, and Gaoang Wang. 2024. MovieChat: From Dense Token to Sparse Memory for Long Video Understanding. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2024, Seattle, WA, USA, June 16-22, 2024. IEEE, 1822118232. https://doi.org/10.1109/CVPR52733.2024.01725 [248] Mark Stokes. 2015. Activity-silentworking memory in prefrontal cortex: dynamic coding framework. Trends in cognitive sciences 19, 7 (2015), 394405. [249] Guangzhi Sun, Yixuan Li, Xiaodong Wu, Yudong Yang, Wei Li, Zejun Ma, and Chao Zhang. 2025. video-SALMONN S: Streaming Audio-Visual LLMs Beyond Length Limits via Memory. CoRR abs/2510.11129 (2025). https://doi.org/10. 48550/ARXIV.2510.11129 arXiv:2510.11129 [250] Haoran Sun and Shaoning Zeng. 2025. Hierarchical Memory for High-Efficiency Long-Term Reasoning in LLM Agents. CoRR abs/2507.22925 (2025). https://doi.org/10.48550/ARXIV.2507.22925 arXiv:2507.22925 [251] Weiwei Sun, Miao Lu, Zhan Ling, Kang Liu, Xuesong Yao, Yiming Yang, and Jiecao Chen. 2025. Scaling LongHorizon LLM Agent via Context-Folding. CoRR abs/2510.11967 (2025). https://doi.org/10.48550/ARXIV.2510.11967 arXiv:2510.11967 [252] Zeyi Sun, Ziyu Liu, Yuhang Zang, Yuhang Cao, Xiaoyi Dong, Tong Wu, Dahua Lin, and Jiaqi Wang. 2025. SEAgent: Self-Evolving Computer Use Agent with Autonomous Learning from Experience. CoRR abs/2508.04700 (2025). https://doi.org/10.48550/ARXIV.2508.04700 arXiv:2508. [253] Mirac Suzgun, Mert Yüksekgönül, Federico Bianchi, Dan Jurafsky, and James Zou. 2025. Dynamic Cheatsheet: Test-Time Learning with Adaptive Memory. CoRR abs/2504.07952 (2025). https://doi.org/10.48550/ARXIV.2504.07952 arXiv:2504.07952 [254] Eva Svoboda, Margaret McKinnon, and Brian Levine. 2006. The functional neuroanatomy of autobiographical memory: meta-analysis. Neuropsychologia 44, 12 (2006), 21892208. [255] Pawel Tacikowski, Güldamla Kalender, Davide Ciliberti, and Itzhak Fried. 2024. Human hippocampal and entorhinal neurons encode the temporal structure of experience. Nature 635, 8037 (2024), 160167. [256] Arielle Tambini and Mark DEsposito. 2020. Causal contribution of awake post-encoding processes to episodic memory consolidation. Current Biology 30, 18 (2020), 35333543. [257] Chong Min John Tan, Prince Saroj, Bharat Runwal, Hardik Maheshwari, Brian Lim Yi Sheng, Richard Cottrill, Alankrit Chona, Ambuj Kumar, and Mehul Motani. 2024. TaskGen: Task-Based, Memory-Infused Agentic Framework using , Vol. 1, No. 1, Article . Publication date: December 2025. AI Meets Brain: Unified Survey on Memory Systems from Cognitive Neuroscience to Autonomous Agents 49 StrictJSON. CoRR abs/2407.15734 (2024). https://doi.org/10.48550/ARXIV.2407.15734 arXiv:2407. [258] Haoran Tan, Zeyu Zhang, Chen Ma, Xu Chen, Quanyu Dai, and Zhenhua Dong. 2025. MemBench: Towards More Comprehensive Evaluation on the Memory of LLM-based Agents. In Findings of the Association for Computational Linguistics, ACL 2025, Vienna, Austria, July 27 - August 1, 2025, Wanxiang Che, Joyce Nabende, Ekaterina Shutova, and Mohammad Taher Pilehvar (Eds.). Association for Computational Linguistics, 1933619352. https://aclanthology. org/2025.findings-acl.989/ [259] Zhen Tan, Jun Yan, I-Hung Hsu, Rujun Han, Zifeng Wang, Long T. Le, Yiwen Song, Yanfei Chen, Hamid Palangi, George Lee, Anand Rajan Iyer, Tianlong Chen, Huan Liu, Chen-Yu Lee, and Tomas Pfister. 2025. In Prospect and Retrospect: Reflective Memory Management for Long-term Personalized Dialogue Agents. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2025, Vienna, Austria, July 27 - August 1, 2025, Wanxiang Che, Joyce Nabende, Ekaterina Shutova, and Mohammad Taher Pilehvar (Eds.). Association for Computational Linguistics, 84168439. https://aclanthology.org/2025.acl-long.413/ [260] Xiangru Tang, Tianrui Qin, Tianhao Peng, Ziyang Zhou, Daniel Shao, Tingting Du, Xinming Wei, Peng Xia, Fang Wu, He Zhu, Ge Zhang, Jiaheng Liu, Xingyao Wang, Sirui Hong, Chenglin Wu, Hao Cheng, Chi Wang, and Wangchunshu Zhou. 2025. Agent KB: Leveraging Cross-Domain Experience for Agentic Problem Solving. CoRR abs/2507.06229 (2025). https://doi.org/10.48550/ARXIV.2507.06229 arXiv:2507.06229 [261] Annick FN Tanguay, Daniela Palombo, Brittany Love, Rafael Glikstein, Patrick SR Davidson, and Louis Renoult. 2023. The shared and unique neural correlates of personal semantic, general semantic, and episodic memory. ELife 12 (2023), e83645. [262] Wei Tao, Yucheng Zhou, Yanlin Wang, Wenqiang Zhang, Hongyu Zhang, and Yu Cheng. 2024. MAGIS: LLM-Based Multi-Agent Framework for GitHub Issue Resolution. In Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024, Amir Globersons, Lester Mackey, Danielle Belgrave, Angela Fan, Ulrich Paquet, Jakub M. Tomczak, and Cheng Zhang (Eds.). http://papers.nips.cc/paper_files/paper/2024/hash/5d1f02132ef51602adf07000ca5b6138Abstract-Conference.html [263] Gemini Team. 2025. Gemini 2.5: Pushing the Frontier with Advanced Reasoning, Multimodality, Long Context, and Next Generation Agentic Capabilities. CoRR abs/2507.06261 (2025). https://doi.org/10.48550/ARXIV.2507.06261 arXiv:2507. [264] Ao Tian, Yunfeng Lu, Xinxin Fan, Changhao Wang, Lanzhi Zhou, Yeyao Zhang, and Yanfang Liu. 2025. RGMem: Renormalization Group-based Memory Evolution for Language Agent User Profile. CoRR abs/2510.16392 (2025). https://doi.org/10.48550/ARXIV.2510.16392 arXiv:2510.16392 [265] Alexa Tompary and Lila Davachi. 2017. Consolidation promotes the emergence of representational overlap in the hippocampus and medial prefrontal cortex. Neuron 96, 1 (2017), 228241. [266] Alexa Tompary and Lila Davachi. 2024. Integration of overlapping sequences emerges with consolidation through medial prefrontal cortex neural ensembles and hippocampalcortical connectivity. Elife 13 (2024), e84359. [267] Gesa Van den Broek, Atsuko Takashima, Carola Wiklund-Hörnqvist, Linnea Karlsson Wirebring, Eliane Segers, Ludo Verhoeven, and Lars Nyberg. 2016. Neurocognitive mechanisms of the testing effect: review. Trends in Neuroscience and Education 5, 2 (2016), 5266. [268] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is All you Need. In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett (Eds.). 59986008. https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html [269] Alex Vaz, Sara Inati, Nicolas Brunel, and Kareem Zaghloul. 2019. Coupled ripple oscillations between the medial temporal lobe and neocortex retrieve human memory. Science 363, 6430 (2019), 975978. [270] Christopher Wahlheim and Jeffrey Zacks. 2025. Memory updating and the structure of event representations. Trends in cognitive sciences 29, 4 (2025), 380392. [271] Luanbo Wan and Weizhi Ma. 2025. StoryBench: Dynamic Benchmark for Evaluating Long-Term Memory with Multi Turns. CoRR abs/2506.13356 (2025). https://doi.org/10.48550/ARXIV.2506.13356 arXiv:2506. [272] Luanbo Wan and Weizhi Ma. 2025. StoryBench: Dynamic Benchmark for Evaluating Long-Term Memory with Multi Turns. CoRR abs/2506.13356 (2025). https://doi.org/10.48550/ARXIV.2506.13356 arXiv:2506.13356 [273] Bo Wang, Weiyi He, Shenglai Zeng, Zhen Xiang, Yue Xing, Jiliang Tang, and Pengfei He. 2025. Unveiling Privacy Risks in LLM Agent Memory. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2025, Vienna, Austria, July 27 - August 1, 2025, Wanxiang Che, Joyce Nabende, Ekaterina Shutova, and Mohammad Taher Pilehvar (Eds.). Association for Computational Linguistics, 2524125260. https: //aclanthology.org/2025.acl-long.1227/ , Vol. 1, No. 1, Article . Publication date: December 2025. 50 Liang, et al. [274] Bo Wang, Weiyi He, Shenglai Zeng, Zhen Xiang, Yue Xing, Jiliang Tang, and Pengfei He. 2025. Unveiling Privacy Risks in LLM Agent Memory. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2025, Vienna, Austria, July 27 - August 1, 2025, Wanxiang Che, Joyce Nabende, Ekaterina Shutova, and Mohammad Taher Pilehvar (Eds.). Association for Computational Linguistics, 2524125260. https: //aclanthology.org/2025.acl-long.1227/ [275] Boshi Wang, Weijian Xu, Yunsheng Li, Mei Gao, Yujia Xie, Huan Sun, and Dongdong Chen. 2025. Improving Code Localization with Repository Memory. CoRR abs/2510.01003 (2025). https://doi.org/10.48550/ARXIV.2510.01003 arXiv:2510.01003 [276] Felix Wang, Boyu Chen, Kerun Xu, Bo Tang, Feiyu Xiong, and Zhiyu Li. 2025. Text2Mem: Unified Memory Operation Language for Memory Operating System. CoRR abs/2509.11145 (2025). https://doi.org/10.48550/ARXIV.2509.11145 arXiv:2509.11145 [277] Felix Wang, Boyu Chen, Kerun Xu, Bo Tang, Feiyu Xiong, and Zhiyu Li. 2025. Text2Mem: Unified Memory Operation Language for Memory Operating System. CoRR abs/2509.11145 (2025). https://doi.org/10.48550/ARXIV.2509.11145 arXiv:2509.11145 [278] Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. 2024. Voyager: An Open-Ended Embodied Agent with Large Language Models. Trans. Mach. Learn. Res. 2024 (2024). https://openreview.net/forum?id=ehfRiF0R3a [279] Hengyi Wang, Haizhou Shi, Shiwei Tan, Weiyi Qin, Wenyuan Wang, Tunyu Zhang, Akshay Nambi, Tanuja Ganu, and Hao Wang. 2025. Multimodal Needle in Haystack: Benchmarking Long-Context Capability of Multimodal Large Language Models. In Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL 2025 - Volume 1: Long Papers, Albuquerque, New Mexico, USA, April 29 - May 4, 2025, Luis Chiruzzo, Alan Ritter, and Lu Wang (Eds.). Association for Computational Linguistics, 32213241. https://doi.org/10.18653/V1/2025.NAACL-LONG. [280] Juyuan Wang, Rongchen Zhao, Wei Wei, Yufeng Wang, Mo Yu, Jie Zhou, Jin Xu, and Liyan Xu. 2025. ComoRAG: Cognitive-Inspired Memory-Organized RAG for Stateful Long Narrative Reasoning. CoRR abs/2508.10419 (2025). https://doi.org/10.48550/ARXIV.2508.10419 arXiv:2508.10419 [281] Lei Wang, Jingsen Zhang, Hao Yang, Zhiyuan Chen, Jiakai Tang, Zeyu Zhang, Xu Chen, Yankai Lin, Hao Sun, Ruihua Song, Xin Zhao, Jun Xu, Zhicheng Dou, Jun Wang, and Ji-Rong Wen. 2025. User Behavior Simulation with Large Language Model-based Agents. ACM Trans. Inf. Syst. 43, 2 (2025), 55:155:37. https://doi.org/10.1145/3708985 [282] Rongzheng Wang, Qizhi Chen, Yihong Huang, Yizhuo Ma, Muquan Li, Jiakai Li, Ke Qin, Guangchun Luo, and Shuang Liang. 2025. GraphCogent: Overcoming LLMs Working Memory Constraints via Multi-Agent Collaboration in Complex Graph Understanding. CoRR abs/2508.12379 (2025). https://doi.org/10.48550/ARXIV.2508.12379 arXiv:2508.12379 [283] Ruoyao Wang, Peter A. Jansen, Marc-Alexandre Côté, and Prithviraj Ammanabrolu. 2022. ScienceWorld: Is your Agent Smarter than 5th Grader?. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022, Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang (Eds.). Association for Computational Linguistics, 1127911298. https://doi.org/10.18653/V1/2022.EMNLP-MAIN.775 [284] Xiaoqiang Wang, Suyuchen Wang, Yun Zhu, and Bang Liu. 2025. R3Mem: Bridging Memory Retention and Retrieval via Reversible Compression. CoRR abs/2502.15957 (2025). https://doi.org/10.48550/ARXIV.2502.15957 arXiv:2502.15957 [285] Yu Wang, Yifan Gao, Xiusi Chen, Haoming Jiang, Shiyang Li, Jingfeng Yang, Qingyu Yin, Zheng Li, Xian Li, Bing Yin, Jingbo Shang, and Julian J. McAuley. 2024. MEMORYLLM: Towards Self-Updatable Large Language Models. In Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net. https://openreview.net/forum?id=p0lKWzdikQ [286] Yancheng Wang, Ziyan Jiang, Zheng Chen, Fan Yang, Yingxue Zhou, Eunah Cho, Xing Fan, Yanbin Lu, Xiaojiang Huang, and Yingzhen Yang. 2024. RecMind: Large Language Model Powered Agent For Recommendation. In Findings of the Association for Computational Linguistics: NAACL 2024, Mexico City, Mexico, June 16-21, 2024, Kevin Duh, Helena Gómez-Adorno, and Steven Bethard (Eds.). Association for Computational Linguistics, 43514364. https://doi.org/10.18653/V1/2024.FINDINGS-NAACL.271 [287] Yu Wang, Dmitry Krotov, Yuanzhe Hu, Yifan Gao, Wangchunshu Zhou, Julian J. McAuley, Dan Gutfreund, Rogério Feris, and Zexue He. 2025. M+: Extending MemoryLLM with Scalable Long-Term Memory. In Forty-second International Conference on Machine Learning, ICML 2025, Vancouver, BC, Canada, July 13-19, 2025. OpenReview.net. https: //openreview.net/forum?id=OcqbkROe8J [288] Yuntao Wang, Yanghe Pan, Miao Yan, Zhou Su, and Tom H. Luan. 2023. Survey on ChatGPT: AI-Generated Contents, Challenges, and Solutions. IEEE Open J. Comput. Soc. 4 (2023), 280302. https://doi.org/10.1109/OJCS.2023.3300321 [289] Yu Wang, Ryuichi Takanobu, Zhiqi Liang, Yuzhen Mao, Yuanzhe Hu, Julian J. McAuley, and Xiaojian Wu. 2025. Mem-𝛼: Learning Memory Construction via Reinforcement Learning. CoRR abs/2509.25911 (2025). https://doi.org/ 10.48550/ARXIV.2509.25911 arXiv:2509. , Vol. 1, No. 1, Article . Publication date: December 2025. AI Meets Brain: Unified Survey on Memory Systems from Cognitive Neuroscience to Autonomous Agents 51 [290] Ying Wang, Yanlai Yang, and Mengye Ren. 2023. LifelongMemory: Leveraging LLMs for Answering Queries in Egocentric Videos. CoRR abs/2312.05269 (2023). https://doi.org/10.48550/ARXIV.2312.05269 arXiv:2312.05269 [291] Zihao Wang, Shaofei Cai, Anji Liu, Yonggang Jin, Jinbing Hou, Bowei Zhang, Haowei Lin, Zhaofeng He, Zilong Zheng, Yaodong Yang, Xiaojian Ma, and Yitao Liang. 2025. JARVIS-1: Open-World Multi-Task Agents With MemoryAugmented Multimodal Language Models. IEEE Trans. Pattern Anal. Mach. Intell. 47, 3 (2025), 18941907. https: //doi.org/10.1109/TPAMI.2024.3511593 [292] Zheng Wang, Zhongyang Li, Zeren Jiang, Dandan Tu, and Wei Shi. 2024. Crafting Personalized Agents through Retrieval-Augmented Generation on Editable Memory Graphs. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, EMNLP 2024, Miami, FL, USA, November 12-16, 2024, Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen (Eds.). Association for Computational Linguistics, 48914906. https://doi.org/10.18653/ V1/2024.EMNLP-MAIN.281 [293] Zhenhailong Wang, Haiyang Xu, Junyang Wang, Xi Zhang, Ming Yan, Ji Zhang, Fei Huang, and Heng Ji. 2025. Mobile-Agent-E: Self-Evolving Mobile Assistant for Complex Tasks. CoRR abs/2501.11733 (2025). https://doi.org/10. 48550/ARXIV.2501.11733 arXiv:2501. [294] Zora Zhiruo Wang, Jiayuan Mao, Daniel Fried, and Graham Neubig. 2025. Agent Workflow Memory. In Forty-second International Conference on Machine Learning, ICML 2025, Vancouver, BC, Canada, July 13-19, 2025. OpenReview.net. https://openreview.net/forum?id=NTAhi2JEEE [295] Qianshan Wei, Tengchao Yang, Yaochen Wang, Xinfeng Li, Lijun Li, Zhenfei Yin, Yi Zhan, Thorsten Holz, Zhiqiang Lin, and XiaoFeng Wang. 2025. A-MemGuard: Proactive Defense Framework for LLM-Based Agent Memory. CoRR abs/2510.02373 (2025). https://doi.org/10.48550/ARXIV.2510.02373 arXiv:2510.02373 [296] Tianxin Wei, Noveen Sachdeva, Benjamin Coleman, Zhankui He, Yuanchen Bei, Xuying Ning, Mengting Ai, Yunzhe Li, Jingrui He, Ed Chi, et al. 2025. Evo-Memory: Benchmarking LLM Agent Test-time Learning with Self-Evolving Memory. arXiv preprint arXiv:2511.20857 (2025). [297] Zhepei Wei, Wenlin Yao, Yao Liu, Weizhi Zhang, Qin Lu, Liang Qiu, Changlong Yu, Puyang Xu, Chao Zhang, Bing Yin, Hyokun Yun, and Lihong Li. 2025. WebAgent-R1: Training Web Agents via End-to-End Multi-Turn Reinforcement Learning. CoRR abs/2505.16421 (2025). https://doi.org/10.48550/ARXIV.2505.16421 arXiv:2505.16421 [298] Rebecca Westhäußer, Frederik Berenz, Wolfgang Minker, and Sebastian Zepf. 2025. CAIM: Development and Evaluation of Cognitive AI Memory Framework for Long-Term Interaction with Intelligent Agents. CoRR abs/2505.13044 (2025). https://doi.org/10.48550/ARXIV.2505.13044 arXiv:2505.13044 [299] Rebecca Westhäußer, Wolfgang Minker, and Sebastian Zepf. 2025. Enabling Personalized Long-term Interactions in LLM-based Agents through Persistent Memory and User Profiles. CoRR abs/2510.07925 (2025). https://doi.org/10. 48550/ARXIV.2510.07925 arXiv:2510. [300] Elliott Wimmer, Yunzhe Liu, Neža Vehar, Timothy EJ Behrens, and Raymond Dolan. 2020. Episodic memory retrieval success is associated with rapid replay of episode content. Nature neuroscience 23, 8 (2020), 10251033. [301] Marco Wittmann, Yongling Lin, Deng Pan, Moritz Braun, Cormac Dickson, Lisa Spiering, Shuyi Luo, Caroline Harbison, Ayat Abdurahman, Sorcha Hamilton, et al. 2025. Basis functions for complex social decisions in dorsomedial frontal cortex. Nature (2025), 111. [302] Bowen Wu, Wenqing Wang, Haoran Li, Ying Li, Jingsong Yu, and Baoxun Wang. 2025. Interpersonal Memory Matters: New Task for Proactive Dialogue Utilizing Conversational History. CoRR abs/2503.05150 (2025). https: //doi.org/10.48550/ARXIV.2503.05150 arXiv:2503.05150 [303] Cheng-Kuang Wu, Zhi Rui Tam, Chieh-Yen Lin, Yun-Nung Chen, and Hung-yi Lee. 2024. StreamBench: Towards Benchmarking Continuous Improvement of Language Agents. In Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024, Amir Globersons, Lester Mackey, Danielle Belgrave, Angela Fan, Ulrich Paquet, Jakub M. Tomczak, and Cheng Zhang (Eds.). http://papers.nips.cc/paper_files/paper/2024/hash/c189915371c4474fe9789be3728113fcAbstract-Datasets_and_Benchmarks_Track.html [304] Di Wu, Hongwei Wang, Wenhao Yu, Yuwei Zhang, Kai-Wei Chang, and Dong Yu. 2025. LongMemEval: Benchmarking Chat Assistants on Long-Term Interactive Memory. In The Thirteenth International Conference on Learning Representations, ICLR 2025, Singapore, April 24-28, 2025. OpenReview.net. https://openreview.net/forum?id=pZiyCaVuti [305] Rong Wu, Xiaoman Wang, Jianbiao Mei, Pinlong Cai, Daocheng Fu, Cheng Yang, Licheng Wen, Xuemeng Yang, Yufan Shen, Yuxin Wang, and Botian Shi. 2025. EvolveR: Self-Evolving LLM Agents through an Experience-Driven Lifecycle. CoRR abs/2510.16079 (2025). https://doi.org/10.48550/ARXIV.2510.16079 arXiv:2510.16079 [306] Shirley Wu, Shiyu Zhao, Qian Huang, Kexin Huang, Michihiro Yasunaga, Kaidi Cao, Vassilis N. Ioannidis, Karthik Subbian, Jure Leskovec, and James Y. Zou. 2024. AvaTaR: Optimizing LLM Agents for Tool Usage via Contrastive Reasoning. In Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024, Amir Globersons, Lester Mackey, Danielle Belgrave, Angela Fan, Ulrich Paquet, Jakub M. Tomczak, and Cheng Zhang (Eds.). http://papers.nips.cc/ , Vol. 1, No. 1, Article . Publication date: December 2025. 52 Liang, et al. paper_files/paper/2024/hash/2db8ce969b000fe0b3fb172490c33ce8-Abstract-Conference.html [307] Xixi Wu, Kuan Li, Yida Zhao, Liwen Zhang, Litu Ou, Huifeng Yin, Zhongwang Zhang, Yong Jiang, Pengjun Xie, Fei Huang, Minhao Cheng, Shuai Wang, Hong Cheng, and Jingren Zhou. 2025. ReSum: Unlocking Long-Horizon Search Intelligence via Context Summarization. CoRR abs/2509.13313 (2025). https://doi.org/10.48550/ARXIV.2509.13313 arXiv:2509.13313 [308] Yaxiong Wu, Yongyue Zhang, Sheng Liang, and Yong Liu. 2025. Sgmem: Sentence graph memory for long-term conversational agents. arXiv preprint arXiv:2509.21212 (2025). [309] Zhiyong Wu, Chengcheng Han, Zichen Ding, Zhenmin Weng, Zhoumianze Liu, Shunyu Yao, Tao Yu, and Lingpeng Kong. 2024. OS-Copilot: Towards Generalist Computer Agents with Self-Improvement. CoRR abs/2402.07456 (2024). https://doi.org/10.48550/ARXIV.2402.07456 arXiv:2402.07456 [310] Yunjia Xi, Weiwen Liu, Jianghao Lin, Bo Chen, Ruiming Tang, Weinan Zhang, and Yong Yu. 2024. MemoCRS: Memory-enhanced Sequential Conversational Recommender Systems with Large Language Models. In Proceedings of the 33rd ACM International Conference on Information and Knowledge Management, CIKM 2024, Boise, ID, USA, October 21-25, 2024, Edoardo Serra and Francesca Spezzano (Eds.). ACM, 25852595. https://doi.org/10.1145/3627673.3679599 [311] Peng Xia, Kangyu Zhu, Haoran Li, Tianze Wang, Weijia Shi, Sheng Wang, Linjun Zhang, James Zou, and Huaxiu Yao. 2025. MMed-RAG: Versatile Multimodal RAG System for Medical Vision Language Models. In The Thirteenth International Conference on Learning Representations, ICLR 2025, Singapore, April 24-28, 2025. OpenReview.net. https: //openreview.net/forum?id=s5epFPdIW6 [312] Siyu Xia, Zekun Xu, Jiajun Chai, Wentian Fan, Yan Song, Xiaohan Wang, Guojun Yin, Wei Lin, Haifeng Zhang, and Jun Wang. 2025. From Experience to Strategy: Empowering LLM Agents with Trainable Graph Memory. arXiv preprint arXiv:2511.07800 (2025). [313] Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. 2024. Efficient Streaming Language Models with Attention Sinks. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net. https://openreview.net/forum?id=NG7sS51zVF [314] Yunzhong Xiao, Yangmin Li, Hewei Wang, Yunlong Tang, and Zora Zhiruo Wang. 2025. ToolMem: Enhancing Multimodal Agents with Learnable Tool Capability Memory. CoRR abs/2510.06664 (2025). https://doi.org/10.48550/ ARXIV.2510.06664 arXiv:2510. [315] Yihang Xiao, Jinyi Liu, Yan Zheng, Xiaohan Xie, Jianye Hao, Mingzhi Li, Ruitao Wang, Fei Ni, Yuxiao Li, Jintian Luo, Shaoqing Jiao, and Jiajie Peng. 2024. CellAgent: An LLM-driven Multi-Agent Framework for Automated Single-cell Data Analysis. CoRR abs/2407.09811 (2024). https://doi.org/10.48550/ARXIV.2407.09811 arXiv:2407.09811 [316] Zhibing Xiao, Xiongfei Wang, Jinbo Zhang, Jianxin Ou, Li He, Yukun Qu, Xiangyu Hu, Timothy EJ Behrens, and Yunzhe Liu. 2025. Human hippocampal ripples align new experiences with grid-like schema. Neuron 113, 21 (2025), 36613672. [317] Junlin Xie, Zhihong Chen, Ruifei Zhang, Xiang Wan, and Guanbin Li. 2024. Large Multimodal Agents: Survey. CoRR abs/2402.15116 (2024). https://doi.org/10.48550/ARXIV.2402.15116 arXiv:2402.15116 [318] Haomiao Xiong, Zongxin Yang, Jiazuo Yu, Yunzhi Zhuge, Lu Zhang, Jiawen Zhu, and Huchuan Lu. 2025. Streaming Video Understanding and Multi-round Interaction with Memory-enhanced Knowledge. In The Thirteenth International Conference on Learning Representations, ICLR 2025, Singapore, April 24-28, 2025. OpenReview.net. https://openreview. net/forum?id=JbPb6RieNC [319] Derong Xu, Yi Wen, Pengyue Jia, Yingyi Zhang, wenlin zhang, Yichao Wang, Huifeng Guo, Ruiming Tang, Xiangyu Zhao, Enhong Chen, and Tong Xu. 2025. From Single to Multi-Granularity: Toward Long-Term Memory Association and Selection of Conversational Agents. arXiv:2505.19549 [cs.CL] https://arxiv.org/abs/2505. [320] Haoran Xu, Jiacong Hu, Ke Zhang, Lei Yu, Yuxin Tang, Xinyuan Song, Yiqun Duan, Lynn Ai, and Bill Shi. 2025. SEDM: Scalable Self-Evolving Distributed Memory for Agents. CoRR abs/2509.09498 (2025). https://doi.org/10.48550/ARXIV. 2509.09498 arXiv:2509.09498 [321] Mufan Xu, Gewen Liang, Kehai Chen, Wei Wang, Xun Zhou, Muyun Yang, Tiejun Zhao, and Min Zhang. 2025. Memory-augmented Query Reconstruction for LLM-based Knowledge Graph Reasoning. In Findings of the Association for Computational Linguistics, ACL 2025, Vienna, Austria, July 27 - August 1, 2025, Wanxiang Che, Joyce Nabende, Ekaterina Shutova, and Mohammad Taher Pilehvar (Eds.). Association for Computational Linguistics, 2406824084. https://aclanthology.org/2025.findings-acl.1234/ [322] Qianqiao Xu, Zhiliang Tian, Hongyan Wu, Zhen Huang, Yiping Song, Feng Liu, and Dongsheng Li. 2024. Learn to Disguise: Avoid Refusal Responses in LLMs Defense via Multi-agent Attacker-Disguiser Game. CoRR abs/2404.02532 (2024). https://doi.org/10.48550/ARXIV.2404.02532 arXiv:2404.02532 [323] Wujiang Xu, Zujie Liang, Kai Mei, Hang Gao, Juntao Tan, and Yongfeng Zhang. 2025. A-MEM: Agentic Memory for LLM Agents. CoRR abs/2502.12110 (2025). https://doi.org/10.48550/ARXIV.2502.12110 arXiv:2502.12110 [324] Wujiang Xu, Zujie Liang, Kai Mei, Hang Gao, Juntao Tan, and Yongfeng Zhang. 2025. A-MEM: Agentic Memory for LLM Agents. CoRR abs/2502.12110 (2025). https://doi.org/10.48550/ARXIV.2502.12110 arXiv:2502.12110 , Vol. 1, No. 1, Article . Publication date: December 2025. AI Meets Brain: Unified Survey on Memory Systems from Cognitive Neuroscience to Autonomous Agents 53 [325] B. Y. Yan, Chaofan Li, Hongjin Qian, Shuqi Lu, and Zheng Liu. 2025. General Agentic Memory Via Deep Research. arXiv:2511.18423 [cs.CL] https://arxiv.org/abs/2511. [326] Cilin Yan, Jingyun Wang, Lin Zhang, Ruihui Zhao, Xiaopu Wu, Kai Xiong, Qingsong Liu, Guoliang Kang, and Yangyang Kang. 2025. Efficient and Accurate Prompt Optimization: the Benefit of Memory in Exemplar-Guided Reflection. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2025, Vienna, Austria, July 27 - August 1, 2025, Wanxiang Che, Joyce Nabende, Ekaterina Shutova, and Mohammad Taher Pilehvar (Eds.). Association for Computational Linguistics, 753779. https://aclanthology.org/2025.acl-long.37/ [327] Sikuan Yan, Xiufeng Yang, Zuchao Huang, Ercong Nie, Zifeng Ding, Zonggen Li, Xiaowen Ma, Hinrich Schütze, Volker Tresp, and Yunpu Ma. 2025. Memory-R1: Enhancing Large Language Model Agents to Manage and Utilize Memories via Reinforcement Learning. CoRR abs/2508.19828 (2025). https://doi.org/10.48550/ARXIV.2508.19828 arXiv:2508.19828 [328] Cheng Yang, Xuemeng Yang, Licheng Wen, Daocheng Fu, Jianbiao Mei, Rong Wu, Pinlong Cai, Yufan Shen, Nianchen Deng, Botian Shi, Yu Qiao, and Haifeng Li. 2025. Learning on the Job: An Experience-Driven Self-Evolving Agent for Long-Horizon Tasks. CoRR abs/2510.08002 (2025). https://doi.org/10.48550/ARXIV.2510.08002 arXiv:2510.08002 [329] Hailong Yang, Renhuo Zhao, Guanjin Wang, and Zhaohong Deng. 2025. GAMA: General Anonymizing Multi-Agent System for Privacy Preservation Enhanced by Domain Rules and Disproof Method. CoRR abs/2509.10018 (2025). https://doi.org/10.48550/ARXIV.2509.10018 arXiv:2509.10018 [330] Ke Yang, Yao Liu, Sapana Chaudhary, Rasool Fakoor, Pratik Chaudhari, George Karypis, and Huzefa Rangwala. 2025. AgentOccam: Simple Yet Strong Baseline for LLM-Based Web Agents. In The Thirteenth International Conference on Learning Representations, ICLR 2025, Singapore, April 24-28, 2025. OpenReview.net. https://openreview.net/forum?id= oWdzUpOlkX [331] Ling Yang, Zhaochen Yu, Tianjun Zhang, Shiyi Cao, Minkai Xu, Wentao Zhang, Joseph E. Gonzalez, and Bin Cui. 2024. Buffer of Thoughts: Thought-Augmented Reasoning with Large Language Models. In Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024, Amir Globersons, Lester Mackey, Danielle Belgrave, Angela Fan, Ulrich Paquet, Jakub M. Tomczak, and Cheng Zhang (Eds.). http://papers.nips.cc/paper_files/paper/2024/hash/ cde328b7bf6358f5ebb91fe9c539745e-Abstract-Conference.html [332] Qisen Yang, Zekun Wang, Honghui Chen, Shenzhi Wang, Yifan Pu, Xin Gao, Wenhao Huang, Shiji Song, and Gao Huang. 2024. PsychoGAT: Novel Psychological Measurement Paradigm through Interactive Fiction Games with LLM Agents. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024, Lun-Wei Ku, Andre Martins, and Vivek Srikumar (Eds.). Association for Computational Linguistics, 1447014505. https://doi.org/10.18653/V1/2024.ACL-LONG.779 [333] Shiyi Yang, Zhibo Hu, Xinshu Li, Chen Wang, Tong Yu, Xiwei Xu, Liming Zhu, and Lina Yao. 2025. DrunkAgent: Stealthy Memory Corruption in LLM-Powered Recommender Agents. arXiv preprint arXiv:2503.23804 (2025). [334] Wei Yang, Jinwei Xiao, Hongming Zhang, Qingyang Zhang, Yanna Wang, and Bo Xu. 2025. Coarse-to-Fine Grounded Memory for LLM Agent Planning. CoRR abs/2508.15305 (2025). https://doi.org/10.48550/ARXIV.2508.15305 arXiv:2508. [335] Zongxin Yang, Guikun Chen, Xiaodi Li, Wenguan Wang, and Yi Yang. 2024. DoraemonGPT: Toward Understanding Dynamic Scenes with Large Language Models (Exemplified as Video Agent). In Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net. https://openreview.net/forum? id=QMy2RLnxGN [336] Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W. Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. 2018. HotpotQA: Dataset for Diverse, Explainable Multi-hop Question Answering. In Conference on Empirical Methods in Natural Language Processing (EMNLP). [337] Shunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan. preprint. WebShop: Towards Scalable Real-World Web Interaction with Grounded Language Agents. In ArXiv. [338] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik R. Narasimhan, and Yuan Cao. 2023. ReAct: Synergizing Reasoning and Acting in Language Models. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net. https://openreview.net/forum?id=WE_vluYUL-X [339] Yiqun Yao, Naitong Yu, Xiang Li, Xin Jiang, Xuezhi Fang, Wenjia Ma, Xuying Meng, Jing Li, Aixin Sun, and Yequan Wang. 2025. EgoMem: Lifelong Memory Agent for Full-duplex Omnimodal Models. CoRR abs/2509.11914 (2025). https://doi.org/10.48550/ARXIV.2509.11914 arXiv:2509.11914 [340] Rui Ye, Zhongwang Zhang, Kuan Li, Huifeng Yin, Zhengwei Tao, Yida Zhao, Liangcai Su, Liwen Zhang, Zile Qiao, Xinyu Wang, Pengjun Xie, Fei Huang, Siheng Chen, Jingren Zhou, and Yong Jiang. 2025. AgentFold: Long-Horizon Web Agents with Proactive Context Management. CoRR abs/2510.24699 (2025). https://doi.org/10.48550/ARXIV.2510.24699 arXiv:2510. , Vol. 1, No. 1, Article . Publication date: December 2025. 54 Liang, et al. [341] Shicheng Ye, Chao Yu, Kaiqiang Ke, Chengdong Xu, and Yinqi Wei. 2025. H2R: Hierarchical Hindsight Reflection for Multi-Task LLM Agents. CoRR abs/2509.12810 (2025). https://doi.org/10.48550/ARXIV.2509.12810 arXiv:2509.12810 [342] Zhifang Ye, Liang Shi, Anqi Li, Chuansheng Chen, and Gui Xue. 2020. Retrieval practice facilitates memory updating by enhancing and differentiating medial prefrontal cortex representations. Elife 9 (2020), e57023. [343] Woongyeong Yeo, Kangsan Kim, Jaehong Yoon, and Sung Ju Hwang. 2025. WorldMM: Dynamic Multimodal Memory Agent for Long Video Reasoning. arXiv:2512.02425 [cs.CV] https://arxiv.org/abs/2512.02425 [344] Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing Sun, Tong Xu, and Enhong Chen. 2023. Survey on Multimodal Large Language Models. CoRR abs/2306.13549 (2023). https://doi.org/10.48550/ARXIV.2306.13549 arXiv:2306.13549 [345] Zhangyue Yin, Qiushi Sun, Qipeng Guo, Zhiyuan Zeng, Qinyuan Cheng, Xipeng Qiu, and Xuanjing Huang. 2024. Explicit Memory Learning with Expectation Maximization. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, EMNLP 2024, Miami, FL, USA, November 12-16, 2024, Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen (Eds.). Association for Computational Linguistics, 1661816635. https://doi.org/10.18653/V1/ 2024.EMNLP-MAIN.927 [346] Hongli Yu, Tinghong Chen, Jiangtao Feng, Jiangjie Chen, Weinan Dai, Qiying Yu, Ya-Qin Zhang, Wei-Ying Ma, Jingjing Liu, Mingxuan Wang, and Hao Zhou. 2025. MemAgent: Reshaping Long-Context LLM with Multi-Conv RL-based Memory Agent. CoRR abs/2507.02259 (2025). https://doi.org/10.48550/ARXIV.2507.02259 arXiv:2507.02259 [347] Jiwen Yu, Jianhong Bai, Yiran Qin, Quande Liu, Xintao Wang, Pengfei Wan, Di Zhang, and Xihui Liu. 2025. Context as Memory: Scene-Consistent Interactive Long Video Generation with Memory Retrieval. CoRR abs/2506.03141 (2025). https://doi.org/10.48550/ARXIV.2506.03141 arXiv:2506.03141 [348] Miao Yu, Fanci Meng, Xinyun Zhou, Shilong Wang, Junyuan Mao, Linsey Pang, Tianlong Chen, Kun Wang, Xinfeng Li, Yongfeng Zhang, Bo An, and Qingsong Wen. 2025. Survey on Trustworthy LLM Agents: Threats and Countermeasures. CoRR abs/2503.09648 (2025). https://doi.org/10.48550/ARXIV.2503.09648 arXiv:2503.09648 [349] Shuo Yu, Mingyue Cheng, Daoyu Wang, Qi Liu, Zirui Liu, Ze Guo, and Xiaoyu Tao. 2025. MemWeaver: Hierarchical Memory from Textual Interactive Behaviors for Personalized Generation. CoRR abs/2510.07713 (2025). https: //doi.org/10.48550/ARXIV.2510.07713 arXiv:2510.07713 [350] Wangjing Yu, Asieh Zadbood, Avi JH Chanales, and Lila Davachi. 2024. Repetition dynamically and rapidly increases cortical, but not hippocampal, offline reactivation. Proceedings of the National Academy of Sciences 121, 40 (2024), e2405929121. [351] Xinlei Yu, Chengming Xu, Guibin Zhang, Zhangquan Chen, Yudong Zhang, Yongbo He, Peng-Tao Jiang, Jiangning Zhang, Xiaobin Hu, and Shuicheng Yan. 2025. VisMem: Latent Vision Memory Unlocks Potential of Vision-Language Models. arXiv preprint arXiv:2511.11007 (2025). [352] Yangbin Yu, Qin Zhang, Junyou Li, Qiang Fu, and Deheng Ye. 2024. Affordable Generative Agents. Trans. Mach. Learn. Res. 2024 (2024). https://openreview.net/forum?id=7tlYbcq5DY [353] Qianhao Yuan, Jie Lou, Zichao Li, Jiawei Chen, Yaojie Lu, Hongyu Lin, Le Sun, Debing Zhang, and Xianpei Han. 2025. MemSearcher: Training LLMs to Reason, Search and Manage Memory via End-to-End Reinforcement Learning. CoRR abs/2511.02805 (2025). https://doi.org/10.48550/ARXIV.2511.02805 arXiv:2511.02805 [354] Zhuowen Yuan, Tao Liu, Yang Yang, Yang Wang, Feng Qi, Kaushik Rangadurai, Bo Li, and Shuang Yang. 2025. ArchPilot: Proxy-Guided Multi-Agent Approach for Machine Learning Engineering. CoRR abs/2511.03985 (2025). https://doi.org/10.48550/ARXIV.2511.03985 arXiv:2511.03985 [355] Shenglai Zeng, Jiankun Zhang, Pengfei He, Yiding Liu, Yue Xing, Han Xu, Jie Ren, Yi Chang, Shuaiqiang Wang, Dawei Yin, and Jiliang Tang. 2024. The Good and The Bad: Exploring Privacy Issues in Retrieval-Augmented Generation (RAG). In Findings of the Association for Computational Linguistics, ACL 2024, Bangkok, Thailand and virtual meeting, August 11-16, 2024, Lun-Wei Ku, Andre Martins, and Vivek Srikumar (Eds.). Association for Computational Linguistics, 45054524. https://doi.org/10.18653/V1/2024.FINDINGS-ACL.267 [356] Yifan Zeng, Yiran Wu, Xiao Zhang, Huazheng Wang, and Qingyun Wu. 2024. AutoDefense: Multi-Agent LLM Defense against Jailbreak Attacks. CoRR abs/2403.04783 (2024). https://doi.org/10.48550/ARXIV.2403.04783 arXiv:2403.04783 [357] Barry Zhang, Keith Lazuka, and Mahesh Murag. 2025. Equipping Agents for the Real World with Agent Skills. Anthropic Engineering Blog. https://www.anthropic.com/engineering/equipping-agents-for-the-real-world-withagent-skills Accessed: December 18, 2025. [358] Chaoyun Zhang, Liqun Li, Shilin He, Xu Zhang, Bo Qiao, Si Qin, Minghua Ma, Yu Kang, Qingwei Lin, Saravan Rajmohan, Dongmei Zhang, and Qi Zhang. 2025. UFO: UI-Focused Agent for Windows OS Interaction. In Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL 2025 - Volume 1: Long Papers, Albuquerque, New Mexico, USA, April 29 - May 4, 2025, Luis Chiruzzo, Alan Ritter, and Lu Wang (Eds.). Association for Computational Linguistics, 597622. https://doi.org/10.18653/V1/2025.NAACL-LONG.26 [359] Ceyao Zhang, Kaijie Yang, Siyi Hu, Zihao Wang, Guanghe Li, Yihang Sun, Cheng Zhang, Zhaowei Zhang, Anji Liu, Song-Chun Zhu, Xiaojun Chang, Junge Zhang, Feng Yin, Yitao Liang, and Yaodong Yang. 2024. ProAgent: Building , Vol. 1, No. 1, Article . Publication date: December 2025. AI Meets Brain: Unified Survey on Memory Systems from Cognitive Neuroscience to Autonomous Agents 55 Proactive Cooperative Agents with Large Language Models. In Thirty-Eighth AAAI Conference on Artificial Intelligence, AAAI 2024, Thirty-Sixth Conference on Innovative Applications of Artificial Intelligence, IAAI 2024, Fourteenth Symposium on Educational Advances in Artificial Intelligence, EAAI 2014, February 20-27, 2024, Vancouver, Canada, Michael J. Wooldridge, Jennifer G. Dy, and Sriraam Natarajan (Eds.). AAAI Press, 1759117599. https://doi.org/10.1609/AAAI. V38I16. [360] Fuwei Zhang, Zhao Zhang, Fuzhen Zhuang, Yu Zhao, Deqing Wang, and Hongwei Zheng. 2024. Temporal Knowledge Graph Reasoning With Dynamic Memory Enhancement. IEEE Trans. Knowl. Data Eng. 36, 11 (2024), 71157128. https://doi.org/10.1109/TKDE.2024.3390683 [361] Guibin Zhang, Muxin Fu, Guancheng Wan, Miao Yu, Kun Wang, and Shuicheng Yan. 2025. G-Memory: Tracing Hierarchical Memory for Multi-Agent Systems. CoRR abs/2506.07398 (2025). https://doi.org/10.48550/ARXIV.2506. 07398 arXiv:2506.07398 [362] Guibin Zhang, Muxin Fu, and Shuicheng Yan. 2025. MemGen: Weaving Generative Latent Memory for Self-Evolving Agents. CoRR abs/2509.24704 (2025). https://doi.org/10.48550/ARXIV.2509.24704 arXiv:2509.24704 [363] Gaoke Zhang, Bo Wang, Yunlong Ma, Dongming Zhao, and Zifei Yu. 2025. Multiple Memory Systems for Enhancing the Long-term Memory of Agent. CoRR abs/2508.15294 (2025). https://doi.org/10.48550/ARXIV.2508.15294 arXiv:2508.15294 [364] Haoji Zhang, Yiqin Wang, Yansong Tang, Yong Liu, Jiashi Feng, Jifeng Dai, and Xiaojie Jin. 2024. Flash-VStream: Memory-Based Real-Time Understanding for Long Video Streams. CoRR abs/2406.08085 (2024). https://doi.org/10. 48550/ARXIV.2406.08085 arXiv:2406. [365] Kai Zhang, Xiangchao Chen, Bo Liu, Tianci Xue, Zeyi Liao, Zhihan Liu, Xiyao Wang, Yuting Ning, Zhaorun Chen, Xiaohan Fu, Jian Xie, Yuxuan Sun, Boyu Gou, Qi Qi, Zihang Meng, Jianwei Yang, Ning Zhang, Xian Li, Ashish Shah, Dat Huynh, Hengduo Li, Zi Yang, Sara Cao, Lawrence Jang, Shuyan Zhou, Jiacheng Zhu, Huan Sun, Jason Weston, Yu Su, and Yifan Wu. 2025. Agent Learning via Early Experience. CoRR abs/2510.08558 (2025). https: //doi.org/10.48550/ARXIV.2510.08558 arXiv:2510.08558 [366] Kai Zhang, Lizhi Qing, Yangyang Kang, and Xiaozhong Liu. 2024. Personalized LLM Response Generation with Parameterized Memory Injection. CoRR abs/2404.03565 (2024). https://doi.org/10.48550/ARXIV.2404.03565 arXiv:2404.03565 [367] Kai Zhang, Xinyuan Zhang, Ejaz Ahmed, Hongda Jiang, Caleb Kumar, Kai Sun, Zhaojiang Lin, Sanat Sharma, Shereen Oraby, Aaron Colak, Ahmed Aly, Anuj Kumar, Xiaozhong Liu, and Xin Luna Dong. 2025. AssoMem: Scalable Memory QA with Multi-Signal Associative Retrieval. CoRR abs/2510.10397 (2025). https://doi.org/10.48550/ARXIV.2510.10397 arXiv:2510.10397 [368] Pan Zhang, Xiaoyi Dong, Yuhang Cao, Yuhang Zang, Rui Qian, Xilin Wei, Lin Chen, Yifei Li, Junbo Niu, Shuangrui Ding, Qipeng Guo, Haodong Duan, Xin Chen, Han Lv, Zheng Nie, Min Zhang, Bin Wang, Wenwei Zhang, Xinyue Zhang, Jiaye Ge, Wei Li, Jingwen Li, Zhongying Tu, Conghui He, Xingcheng Zhang, Kai Chen, Yu Qiao, Dahua Lin, and Jiaqi Wang. 2024. InternLM-XComposer2.5-OmniLive: Comprehensive Multimodal System for Long-term Streaming Video and Audio Interactions. CoRR abs/2412.09596 (2024). https://doi.org/10.48550/ARXIV.2412.09596 arXiv:2412.09596 [369] Qizheng Zhang, Changran Hu, Shubhangi Upasani, Boyuan Ma, Fenglu Hong, Vamsidhar Kamanuru, Jay Rainton, Chen Wu, Mengmeng Ji, Hanchen Li, Urmish Thakker, James Zou, and Kunle Olukotun. 2025. Agentic Context Engineering: Evolving Contexts for Self-Improving Language Models. CoRR abs/2510.04618 (2025). https://doi.org/ 10.48550/ARXIV.2510.04618 arXiv:2510.04618 [370] Qizheng Zhang, Changran Hu, Shubhangi Upasani, Boyuan Ma, Fenglu Hong, Vamsidhar Kamanuru, Jay Rainton, Chen Wu, Mengmeng Ji, Hanchen Li, Urmish Thakker, James Zou, and Kunle Olukotun. 2025. Agentic Context Engineering: Evolving Contexts for Self-Improving Language Models. CoRR abs/2510.04618 (2025). https://doi.org/ 10.48550/ARXIV.2510.04618 arXiv:2510.04618 [371] Qizheng Zhang, Michael Wornow, and Kunle Olukotun. 2025. Cost-Efficient Serving of LLM Agents via Test-Time Plan Caching. CoRR abs/2506.14852 (2025). https://doi.org/10.48550/ARXIV.2506.14852 arXiv:2506.14852 [372] Weizhi Zhang, Xinyang Zhang, Chenwei Zhang, Liangwei Yang, Jingbo Shang, Zhepei Wei, Henry Peng Zou, Zijie Huang, Zhengyang Wang, Yifan Gao, Xiaoman Pan, Lian Xiong, Jingguo Liu, Philip S. Yu, and Xian Li. 2025. PersonaAgent: When Large Language Model Agents Meet Personalization at Test Time. CoRR abs/2506.06254 (2025). https://doi.org/10.48550/ARXIV.2506.06254 arXiv:2506.06254 [373] Xinliang Frederick Zhang, Nicholas Beauchamp, and Lu Wang. 2025. PRIME: Large Language Model Personalization with Cognitive Dual-Memory and Personalized Thought Process. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, Christos Christodoulopoulos, Tanmoy Chakraborty, Carolyn Rose, and Violet Peng (Eds.). Association for Computational Linguistics, Suzhou, China, 3369533724. https://doi.org/10.18653/ v1/2025.emnlp-main.1711 [374] Yuxiang Zhang, Jiangming Shu, Ye Ma, Xueyuan Lin, Shangxi Wu, and Jitao Sang. 2025. Memory as Action: Autonomous Context Curation for Long-Horizon Agentic Tasks. CoRR abs/2510.12635 (2025). https://doi.org/10. , Vol. 1, No. 1, Article . Publication date: December 2025. Liang, et al. 48550/ARXIV.2510.12635 arXiv:2510.12635 [375] Yujie Zhang, Weikang Yuan, and Zhuoren Jiang. 2025. Bridging Intuitive Associations and Deliberate Recall: Empowering LLM Personal Assistant with Graph-Structured Long-term Memory. In Findings of the Association for Computational Linguistics, ACL 2025, Vienna, Austria, July 27 - August 1, 2025, Wanxiang Che, Joyce Nabende, Ekaterina Shutova, and Mohammad Taher Pilehvar (Eds.). Association for Computational Linguistics, 1753317547. https://doi.org/10.18653/V1/2025.FINDINGS-ACL.901 [376] Zeyu Zhang, Xiaohe Bo, Chen Ma, Rui Li, Xu Chen, Quanyu Dai, Jieming Zhu, Zhenhua Dong, and Ji-Rong Wen. 2024. Survey on the Memory Mechanism of Large Language Model based Agents. CoRR abs/2404.13501 (2024). https://doi.org/10.48550/ARXIV.2404.13501 arXiv:2404.13501 [377] Zeyu Zhang, Quanyu Dai, Luyu Chen, Zeren Jiang, Rui Li, Jieming Zhu, Xu Chen, Yi Xie, Zhenhua Dong, and Ji-Rong Wen. 2024. MemSim: Bayesian Simulator for Evaluating Memory of LLM-based Personal Assistants. CoRR abs/2409.20163 (2024). https://doi.org/10.48550/ARXIV.2409.20163 arXiv:2409.20163 [378] Zeyu Zhang, Quanyu Dai, Xu Chen, Rui Li, Zhongyang Li, and Zhenhua Dong. 2025. MemEngine: Unified and Modular Library for Developing Advanced Memory of LLM-based Agents. In Companion Proceedings of the ACM on Web Conference 2025, WWW 2025, Sydney, NSW, Australia, 28 April 2025 - 2 May 2025, Guodong Long, Michale Blumestein, Yi Chang, Liane Lewin-Eytan, Zi Helen Huang, and Elad Yom-Tov (Eds.). ACM, 821824. https://doi.org/10.1145/3701716. [379] Zeyu Zhang, Quanyu Dai, Rui Li, Xiaohe Bo, Xu Chen, and Zhenhua Dong. 2025. Learn to Memorize: Optimizing LLM-based Agents with Adaptive Memory Framework. CoRR abs/2508.16629 (2025). https://doi.org/10.48550/ARXIV. 2508.16629 arXiv:2508.16629 [380] Zeyu Zhang, Yang Zhang, Haoran Tan, Rui Li, and Xu Chen. 2025. Explicit v.s. Implicit Memory: Exploring Multi-hop Complex Reasoning Over Personalized Information. CoRR abs/2508.13250 (2025). https://doi.org/10.48550/ARXIV. 2508.13250 arXiv:2508.13250 [381] Andrew Zhao, Daniel Huang, Quentin Xu, Matthieu Lin, Yong-Jin Liu, and Gao Huang. 2024. ExpeL: LLM Agents Are Experiential Learners. In Thirty-Eighth AAAI Conference on Artificial Intelligence, AAAI 2024, Thirty-Sixth Conference on Innovative Applications of Artificial Intelligence, IAAI 2024, Fourteenth Symposium on Educational Advances in Artificial Intelligence, EAAI 2014, February 20-27, 2024, Vancouver, Canada, Michael J. Wooldridge, Jennifer G. Dy, and Sriraam Natarajan (Eds.). AAAI Press, 1963219642. https://doi.org/10.1609/AAAI.V38I17.29936 [382] Pengyu Zhao, Zijian Jin, and Ning Cheng. 2023. An In-depth Survey of Large Language Model-based Artificial Intelligence Agents. CoRR abs/2309.14365 (2023). https://doi.org/10.48550/ARXIV.2309.14365 arXiv:2309.14365 [383] Qian Zhao, Zhuo Sun, Bin Guo, and Zhiwen Yu. 2025. MemoCue: Empowering LLM-Based Agents for Human Memory Recall via Strategy-Guided Querying. CoRR abs/2507.23633 (2025). https://doi.org/10.48550/ARXIV.2507.23633 arXiv:2507.23633 [384] Qianhui Zhao, Li Zhang, Fang Liu, Junhang Cheng, Chengru Wu, Junchen Ai, Qiaoyuanhe Meng, Lichen Zhang, Xiaoli Lian, Shubin Song, and Yuanping Guo. 2025. Towards Realistic Project-Level Code Generation via Multi-Agent Collaboration and Semantic Architecture Modeling. CoRR abs/2511.03404 (2025). https://doi.org/10.48550/ARXIV. 2511.03404 arXiv:2511.03404 [385] Siyan Zhao, Mingyi Hong, Yang Liu, Devamanyu Hazarika, and Kaixiang Lin. 2025. Do LLMs Recognize Your Preferences? Evaluating Personalized Preference Following in LLMs. In The Thirteenth International Conference on Learning Representations, ICLR 2025, Singapore, April 24-28, 2025. OpenReview.net. https://openreview.net/forum?id= QWunLKbBGF [386] Yue Zhao, Ishan Misra, Philipp Krähenbühl, and Rohit Girdhar. 2023. Learning Video Representations from Large Language Models. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2023, Vancouver, BC, Canada, June 17-24, 2023. IEEE, 65866597. https://doi.org/10.1109/CVPR52729.2023.00637 [387] Boyuan Zheng, Michael Y. Fatemi, Xiaolong Jin, Zora Zhiruo Wang, Apurva Gandhi, Yueqi Song, Yu Gu, Jayanth Srinivasa, Gaowen Liu, Graham Neubig, and Yu Su. 2025. SkillWeaver: Web Agents can Self-Improve by Discovering and Honing Skills. CoRR abs/2504.07079 (2025). https://doi.org/10.48550/ARXIV.2504.07079 arXiv:2504.07079 [388] Ervine Zheng, Yikuan Li, Geoffrey Jay Tso, and Jilong Kuang. 2025. Data-Efficient Automatic Prompt Optimization for Memory-Enhanced Conversational Agents. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing: Industry Track, Saloni Potdar, Lina Rojas-Barahona, and Sebastien Montella (Eds.). Association for Computational Linguistics, Suzhou (China), 17931804. https://doi.org/10.18653/v1/2025.emnlp-industry.126 [389] Junhao Zheng, Xidi Cai, Qiuke Li, Duzhen Zhang, Zhong-Zhi Li, Yingying Zhang, Le Song, and Qianli Ma. 2025. LifelongAgentBench: Evaluating LLM Agents as Lifelong Learners. CoRR abs/2505.11942 (2025). https://doi.org/10. 48550/ARXIV.2505.11942 arXiv:2505.11942 [390] Junhao Zheng, Chengming Shi, Xidi Cai, Qiuke Li, Duzhen Zhang, Chenxing Li, Dong Yu, and Qianli Ma. 2025. Lifelong Learning of Large Language Model based Agents: Roadmap. CoRR abs/2501.07278 (2025). https: //doi.org/10.48550/ARXIV.2501.07278 arXiv:2501.07278 , Vol. 1, No. 1, Article . Publication date: December 2025. AI Meets Brain: Unified Survey on Memory Systems from Cognitive Neuroscience to Autonomous Agents 57 [391] Longtao Zheng, Rundong Wang, Xinrun Wang, and Bo An. 2024. Synapse: Trajectory-as-Exemplar Prompting with Memory for Computer Control. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net. https://openreview.net/forum?id=Pc8AU1aF5e [392] Wanjun Zhong, Lianghong Guo, Qiqi Gao, He Ye, and Yanlin Wang. 2024. MemoryBank: Enhancing Large Language Models with Long-Term Memory. In Thirty-Eighth AAAI Conference on Artificial Intelligence, AAAI 2024, ThirtySixth Conference on Innovative Applications of Artificial Intelligence, IAAI 2024, Fourteenth Symposium on Educational Advances in Artificial Intelligence, EAAI 2014, February 20-27, 2024, Vancouver, Canada, Michael J. Wooldridge, Jennifer G. Dy, and Sriraam Natarajan (Eds.). AAAI Press, 1972419731. https://doi.org/10.1609/AAAI.V38I17.29946 [393] Zexuan Zhong, Tao Lei, and Danqi Chen. 2022. Training Language Models with Memory Augmentation. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022, Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang (Eds.). Association for Computational Linguistics, 56575673. https://doi.org/10.18653/V1/2022.EMNLP-MAIN.382 [394] Andy Zhou, Kai Yan, Michal Shlapentokh-Rothman, Haohan Wang, and Yu-Xiong Wang. 2024. Language Agent Tree Search Unifies Reasoning, Acting, and Planning in Language Models. In Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net. https://openreview.net/forum?id= njwv9BsGHF [395] Huichi Zhou, Yihang Chen, Siyuan Guo, Xue Yan, Kin Hei Lee, Zihan Wang, Ka Yiu Lee, Guchun Zhang, Kun Shao, Linyi Yang, et al. 2025. Agentfly: Fine-tuning llm agents without fine-tuning llms. arXiv preprint arXiv:2508.16153 (2025). [396] Huichi Zhou, Yihang Chen, Siyuan Guo, Xue Yan, Kin Hei Lee, Zihan Wang, Ka Yiu Lee, Guchun Zhang, Kun Shao, Linyi Yang, and Jun Wang. 2025. Memento: Fine-tuning LLM Agents without Fine-tuning LLMs. CoRR abs/2508.16153 (2025). https://doi.org/10.48550/ARXIV.2508.16153 arXiv:2508.16153 [397] Shuyan Zhou, Frank F. Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Tianyue Ou, Yonatan Bisk, Daniel Fried, Uri Alon, and Graham Neubig. 2024. WebArena: Realistic Web Environment for Building Autonomous Agents. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net. https://openreview.net/forum?id=oKn9c6ytLx [398] Zijian Zhou, Ao Qu, Zhaoxuan Wu, Sunghwan Kim, Alok Prakash, Daniela Rus, Jinhua Zhao, Bryan Kian Hsiang Low, and Paul Pu Liang. 2025. MEM1: Learning to Synergize Memory and Reasoning for Efficient Long-Horizon Agents. CoRR abs/2506.15841 (2025). https://doi.org/10.48550/ARXIV.2506.15841 arXiv:2506.15841 [399] Zichen Zhu, Hao Tang, Yansi Li, Dingye Liu, Hongshen Xu, Kunyao Lan, Danyang Zhang, Yixuan Jiang, Hao Zhou, Chenrun Wang, Situo Zhang, Liangtai Sun, Yixiao Wang, Yuheng Sun, Lu Chen, and Kai Yu. 2025. MobA: Multifaceted Memory-Enhanced Adaptive Planning for Efficient Mobile Task Automation. In Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (System Demonstrations). Association for Computational Linguistics, 535549. https://doi.org/10.18653/v1/2025.naacl-demo.43 [400] Huhai Zou, Rongzhen Li, Tianhao Sun, Fei Wang, Tao Li, and Kai Liu. 2024. Cooperative Scheduling and Hierarchical Memory Model for Multi-Agent Systems. In 2024 IEEE International Symposium on Product Compliance Engineering-Asia (ISPCE-ASIA). IEEE, 16. , Vol. 1, No. 1, Article . Publication date: December 2025."
        }
    ],
    "affiliations": [
        "Fudan University, China",
        "Harbin Institute of Technology, China",
        "National University of Singapore, Singapore",
        "Peking University, China"
    ]
}