{
    "paper_title": "Survey of Active Learning Hyperparameters: Insights from a Large-Scale Experimental Grid",
    "authors": [
        "Julius Gonsior",
        "Tim Rieß",
        "Anja Reusch",
        "Claudio Hartmann",
        "Maik Thiele",
        "Wolfgang Lehner"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Annotating data is a time-consuming and costly task, but it is inherently required for supervised machine learning. Active Learning (AL) is an established method that minimizes human labeling effort by iteratively selecting the most informative unlabeled samples for expert annotation, thereby improving the overall classification performance. Even though AL has been known for decades, AL is still rarely used in real-world applications. As indicated in the two community web surveys among the NLP community about AL, two main reasons continue to hold practitioners back from using AL: first, the complexity of setting AL up, and second, a lack of trust in its effectiveness. We hypothesize that both reasons share the same culprit: the large hyperparameter space of AL. This mostly unexplored hyperparameter space often leads to misleading and irreproducible AL experiment results. In this study, we first compiled a large hyperparameter grid of over 4.6 million hyperparameter combinations, second, recorded the performance of all combinations in the so-far biggest conducted AL study, and third, analyzed the impact of each hyperparameter in the experiment results. In the end, we give recommendations about the influence of each hyperparameter, demonstrate the surprising influence of the concrete AL strategy implementation, and outline an experimental study design for reproducible AL experiments with minimal computational effort, thus contributing to more reproducible and trustworthy AL research in the future."
        },
        {
            "title": "Start",
            "content": "ARXIV PREPRINT: SURVEY OF ACTIVE LEARNING HYPERPARAMETERS 1 Survey of Active Learning Hyperparameters: Insights from Large-Scale Experimental Grid Julius Gonsior Tim Rieß Anja Reusch Claudio Hartmann Maik Thiele Wolfgang Lehner Technische Universitat Dresden, {firstname.lastname}@tu-dresden.de Technion - Israeli Institute of Technology, {firstname}@campus.technion.ac.il Hochschule fur Technik und Wirtschaft Dresden, {firstname.lastname}@htw-dresden.de 5 2 0 2 ] . [ 1 7 1 8 3 0 . 6 0 5 2 : r AbstractAnnotating data is time-consuming and costly task, but it is inherently required for supervised machine learning. Active Learning (AL) is an established method that minimizes human labeling effort by iteratively selecting the most informative unlabeled samples for expert annotation, thereby improving the overall classification performance. Even though AL has been known for decades [1], AL is still rarely used in real-world applications. As indicated in the two community web surveys among the NLP community about AL [2], [3], two main reasons continue to hold practitioners back from using AL: first, the complexity of setting AL up, and second, lack of trust in its effectiveness. We hypothesize that both reasons share the same culprit: the large hyperparameter space of AL. This mostly unexplored hyperparameter space often leads to misleading and irreproducible AL experiment results. In this study, we first compiled large hyperparameter grid of over 4.6 million hyperparameter combinations, second, recorded the performance of all combinations in the so-far biggest conducted AL study, and third, analyzed the impact of each hyperparameter in the experiment results. In the end, we give recommendations about the influence of each hyperparameter, demonstrate the surprising influence of the concrete AL strategy implementation, and outline an experimental study design for reproducible AL experiments with minimal computational effort, thus contributing to more reproducible and trustworthy AL research in the future. Index TermsActive Learning, Annotation, Hyperparameter,"
        },
        {
            "title": "Benchmark",
            "content": "I. INTRODUCTION HE majority of Machine Learning (ML) projects use the supervised learning paradigm. Annotated or labeled data samples, consisting of input and output pairs, are used in training phase to optimize the ML model for the concrete use case. This fundamentally requires labeled data. One of the biggest cost factors of real-world applications is the expense of annotating the required dataset. For example, the average cost for the common label task of segmenting single image accurately is 6.40 USD1. AL is human-in-the-loop technique targeted at reducing the amount of required labeled data, thereby lowering the costs of ML projects and therefore enabling more real-world applications to happen. Despite the increase in popularity of AL in the research community 2, and the significant cost savings demonstrated [4][10], which could have been achieved if AL had 1According to scale.ai as of December 2021 (as of 2024, the cost is not publicly visible anymore): https://web.archive.org/web/20210112234705/https://scale. com/pricing been applied correctly, AL is still rare technique and is not widely used by ML practicioners [2], [3]. Although commercial annotation tools often support AL 3, it usually must be enabled manually, often via plugin. The implemented AL strategies are frequently state-of-the-art from the early 90s, ignoring considerable research body of over 30 years. An anecdotal experience from talking at research conferences to other potential applicants of AL: potential practitioners of AL often ask either the questions: But does it really work in practice? or would like to apply AL, but how should do it?. This distrust in the ability of AL to work is also the main reason for withholding practitioners from using AL in the community survey by Tomanek and Olsson [2]. Despite the demonstrated success of AL in individual use cases, there is no clear consensus on how to set up an AL system in the correct way. This paper aims to answer the two questions mentioned above. Our main hypothesis for reasons behind the limited trust and resulting limited adoption of the ALs research results is that empirical evaluation results are often contradictory, not reproducible, and not comparable. This can be illustrated by the unresolved question of What is the best AL strategy?. Several survey papers have attempted to answer this by empirically comparing various AL strategies. In the three surveys [11][13] uncertainty-based AL strategies were found to perform the best; however, each survey recommends in [14], different uncertainty-based strategy. Meanwhile, [15], non-uncertainty-based strategies outperform uncertaintybased ones. In contrast, in [13], the Uncertainty Entropy Sampling strategy outperformed random sampling across multiple datasets and scenarios. In contrast, [14], [16] found that no strategy consistently performed better than random sampling across all datasets. good answer to the question about the best strategy is not easy to find. First, correct answer is very nuanced because the question is too general to have single, short answer. It depends on many more properties of the actual use case than simple strategy recommendation. Second, comparing the results of two AL experiments is challenging because many hyperparameters significantly influence AL. These hyperparameters include the datasets used, evaluated strategies, metrics applied, and more. This paper aims to analyze the influence of the AL hyperparameters to ultimately 2According to https://webofscience.com/ there have been more than 5000 3e.g., Labelbox, Prodigy, AWS Comprehend, and AWS SageMaker Ground publications per year in the AL field in the past years Truth ARXIV PREPRINT: SURVEY OF ACTIVE LEARNING HYPERPARAMETERS 2 explain the differences in the empirical AL evaluations, not to replace them with simply bigger study. In our evaluation, we will give possible explanation for the different results in the AL literature: that the survey results of two different hyperparameter combinations come to the same conclusion and correlate with each other is only possible by chance. Due to the sheer number of hyperparameter combinations, special care must be taken when defining the hyperparameter grid. We will recommend how to do this at the end of our evaluation. Our work is not the first to identify that the AL research field lacks consensus on the hyperparameters used in empirical evaluations [17], unified benchmark [18], and agreement on the influence of hyperparameters on AL experiments [19]. Despite numerous studies addressing these issues, few have yielded satisfactory conclusions. In Tab. in Sec. II-B, we provide list of all analyzed hyperparameter values across different studies. Most studies focus on few hyperparameters, neglecting the interdependency and influence of others. This is the first work to extensively analyze all relevant AL hyperparameters in the biggest experimental AL survey so far. While previous surveys evaluated up to 140,625 parameter combinations, our final grid search covered over 4.6 million parameters. Our core contributions are: 1) Outlining the complexity of conducting AL experiments by compiling all relevant hyperparameters 2) Conducting an extensive grid search over all identified hyperparameters 3) Analyzing the resulting large dataset of AL experiment results and providing qualitative and empirical insights into how AL hyperparameters influenceAL 4) Demonstrating the challenges and variety in implementing AL strategies 5) Giving recommendations, which AL hyperparameters should be used in AL experiments for trustworthy and reproducible results Our paper is organized as follows: first, we provide basic introduction to AL (Sec. II). We then give more profound overview of the possible hyperparameters of typical AL setting in Sec. II-B. Afterwards, we explain how we conducted our experiments in Sec. III, followed by the analysis in Sec. IV. II. ACTIVE LEARNING EXPERIMENT HYPERPARAMETERS AND FUNDAMENTELS This section introduces the basic framework of the Active Learning (AL) cycle in Sec. II-A and outlines the hyperparameters of AL experiments in Sec. II-B. A. Active learning cycle Supervised learning methods depend fundamentally on annotated datasets. AL is an established method that minimizes human labeling effort by iteratively selecting the most informative unlabeled samples for expert annotation, thereby improving the overall classification performance [1]. The goal of AL is to train classification model that maps samples to labels Y, with labels provided by an oracle, usually human experts. Fig 1 illustrates typical pool-based Fig. 1. Standard Active Learning Cycle lab = {(xi, yi)}n AL cycle, the cycle iteration is indicated by t: Starting with small initial labeled dataset D0 i=0 of samples xi and corresponding labels yi Y, and large pool unl = {xi}, xi D0 of unlabeled data D0 lab, an ML model, called the learner model : (cid:55) Y, is trained on the labeled data. query strategy (blue box) : Dunl selects batch of unlabeled samples Qt Dt unl for labeling by the oracle (lower box), and these newly labeled samples (cid:99)Qt are added to the labeled dataset to form the newly labeled dataset for the next AL cycle Dt+1 lab , as well as removed from the next unlabeled set Dt+1 unl . This cycle repeats times until predefined stopping criterion is reached. The results of an AL process can be recorded as time series, either containing metric measured at each time step R(M ), or the set of queried samples for labeling R(Q): R(M ) = [M 0, . . . , c] R(Q) = [Q0, . . . , Qc] B. Hyperparameters of Active Learning experiments In this paper we define single AL experiment = (S, D, , I, M, b, c, L) as combination of hyperparameinfluencing the simulation of one AL strategy on ters, prelabeled dataset D. Given pre-labeled dataset = {(xi, yi)}n i=0, consisting of n-data samples and corresponding labels y, the data set is first split into disjoint training set Dtrain and test set Dtest using the train-test-split (D) = (Dtrain, Dtest). The train set is further divided into an initial labeled start set Dlab, and an unlabeled set Dunl using the initial start set function I(Dtrain) = (Dlab, Dunl). During the simulation, one or more metrics typically measure how much human effort could have been saved using AL from the beginning. This is done by utilizing the test set Dtest. The batch size describes the AL batch size, while indicates how many times the AL cycle is repeated. The learner model is the ML model used. series of multiple experiments is denoted as = {E0, , Ei}. An experimental evaluation grid, consisting of multiple values for each hyperparameter, can be written as cross product of the following hyperparameter sets: ARXIV PREPRINT: SURVEY OF ACTIVE LEARNING HYPERPARAMETERS L. = {D0, , Di}, dataset = {T0, , Tj}, train-test-split = {I0, Ik}, initial start set = {S0, , Sl}, AL strategy = {b0, , bm}, batch size = {L0, , Ln}, ML learner model = {c0, , co}, amount of training cycles = {M0, , Mp}, metric The hyperparameter sets of the metrics and the amounts of AL cycles of an AL experiment are not part of the experimental grid, as metrics can be calculated simultaneously for the same experiment, and the maximum number of AL cycles can be evaluated as an ablation study post hoc. Both parameters do not need re-run of the same experiment. The results of single AL experiment are denoted as time series over the AL cycles from 0 to c, each time either containing the results for an evaluation metric : R(M ) = [M 0, . . . , c], or the sets of selected batches for labeling Q: R(Q) = [Q0, . . . , Qc]. The results of series of experiments are denoted as = {REE E}). Since not all grid combinations of the grid are evaluated 4, the result set does not always contain the results for the complete possible grid of L. Our list of hyperparameters is not exhaustive. We have focused on the most common parameters in most evaluation sections of AL research papers. Additional hyperparameters could include, e.g., the oracle (one oracle vs. many oracle, handling of oracle conflicts, label noise,. . . ) or the stopping criteria used. C. Overview of evaluated parameter ranges in related work There has been an ongoing trend in the past decade, with many papers analyzing the influence of AL hyperparameters, primarily by conducting large-scale empirical analyses of AL strategies. We have gathered an overview of the latest research in Tab. by summarizing, to our best effort, all evaluated hyperparameters in the respective papers. Some papers resulted in large empirical evaluation grids, while others focused on introducing new hyperparameter dimensions. We estimate the size of the experimental hyperparameter grid in the column R. As the compared concrete parameters vary significantly among the papers, this number merely estimates the number of compared hyperparameter combinations. It should not be used to compare computational or research efforts directly. Surprisingly, despite the large body of research on this topic and the high expectations at the beginning of each endeavor, most studies could only partially answer their research leaving many unresolved issues about AL. We questions, hypothesize that focusing on single hyperparameters reveals little, as the most significant effort occurs between them, necessitating large hyperparameter experimental grid. 4e.g. for large datasets the batch size is often higher compared to smaller datasets 1) Hyperparameter comparison works: We will start by giving an overview of the related works about analysis of the hyperparameters of AL and recap their main findings related to our research goal. All share the same motivation to investigate the hyperparameter space of AL, with each paper having different focus, often on individual hyperparameters. Due to the emphasis on few hyperparameters, the results are always influenced by the arbitrary choice of the unexplored hyperparameters. Importance of hyperparameters: [19] were among the first to conclude that AL experiments are very complex, with many hyperparameters influencing them. Random sampling beat most AL strategies in 89% of their experiments. On the same note, the paper [20] outlines the evaluation challenges of AL, focusing on reliable, realistic, and comparable results. Learner model: [21] found that the chosen learner model significantly influences experiment outcomes. However, they conclude that the uncovering of the relationships between dataset characteristics, learning algorithms, and active learning methods is still needed. Metric: The focus of [22] was on comparing performance metrics. They found that most strategies excel in individual performance metrics but perform poorly in others. [23] introduced new metrics for monitoring the performance of ongoing AL processes. Dataset: [11] were the first to use larger number of datasets and concluded that AL strategies still do not consistently outperform random sampling. AL strategy: The large survey [15] compared almost all existing AL strategies but provided limited insight into why specific strategies perform better on some datasets than others. [24] repeated the benchmark of [15], and found surprisingly significant discrepancies between their results and the original findings. They noted that more than half of the examined strategies do not exhibit significant advantages over the [random] baseline. [12] recommended strongly the max-marginbased uncertainty-sampling AL strategy, contrary to findings from other papers. Start set selection strategy: [25] compared strategies for selecting the initial labeled start sets and found that random selection performs as well as computationally more heavy and conceptionally complex strategy. AL for outlier detection: [26] focused on comparing AL for outlier detection and one class classifiers, using an extensive evaluation hyperparameter combinations. Label noise: [27] explored the often overlooked hyperparameter of label noise introduced by the oracle. Although noise significantly affects performance, they could not identify any strategy as being more noise-robust. Our interpretation of this parameter and their results is that noise impacts all AL strategies and could be mitigated by labeling guidelines or multiple annotators. Stopping criteria: [28] performed the most extensive study on different stopping criteria of AL to stop the process earlier than the given budget. As their extensive qualitative this parameter study seems to answer all questions about and also seems not to have any interdependencies with other ARXIV PREPRINT: SURVEY OF ACTIVE LEARNING HYPERPARAMETERS 4 #Datasets #Start Sets Train-Test Split I #ML Model #Query Strategy #Batch Size #Metrics #R Schein et. al. 2007 [30] Evans et. al. 2013 [19] Pereira-Santos et. al. 2017 [21] Ramirez-Loaiza et. al. 2017 [22] Kottke et. al. 2017 [20] Yang et. al. 2018 [11] Abraham et. al. 2020 [23] Zhan et. al. 2021 [15] Chandra et. al. 2021 [25] Trittenbach et. al. 2021 [26] Abraham et. al. 2021 [27] Pullar-Strecker et. al. 2022 [28] Bahri et. al. 2022 [12] Zhan et. al. 2022 [29] Lu et. al. 2023 [24] Ji et. al. 2023 [17] Margraf et. al. 2024 [18] Ours 10 4 75 10 1 47 9 35 4 60 10 9 69 10 26 2 86 10 10 4 1 5 5 10 5 10 10 20/1,000 1 10 5 10/100 1 5 1 3 4 / 1 5 10 5 30 1 20 1 1 3 10/100 1 10 1 2 10 20 5 100 4 25 50 100 20/1,000 50 10/100 5 12 / 5 50 30 20 3 10/100 10 20 100 1 4 5 2 1 1 1 1 1 10 / 4 2 3 3 1 1 1 8 3 7 2 15 3 4 9 6 19 7 10 4 1 16 19 17 7 9 28 1 1 1 1 1 1 2 4 1 1 1 1 1 4 1 3 1 6 6 3 3 5 1 4 1 2 3 8 2 1 2 2 2 2 3 7,000 128 140,625 3,000 400 34,920 54,000 81,320 140 84,000 4,000 810 66,240 2 280 13,600 420 123,840 4,636,800 TABLE OVERVIEW OF EVALUATED HYPERPARAMETER COMBINATIONS IN AL SURVEY PAPERS. NOTE THAT THE TOTAL NUMBER OF EVALUATED HYPERPARAMETER COMBINATIONS IS IN MOST OF THE PAPERS PROBABLY SMALLER THAN REPORTED HERE AS THE COMPLETE EXPERIMENTAL GRID HAS BEEN COMPLETELY COMPUTED, AND SOME PARAMETERS HAVE ONLY BEEN EVALUATED ON SMALLER SUBSET. BOLD NUMBERS INDICATE THE HIGHEST NUMBER OF COMPARED PARAMETERS, AND ITALIC NUMBERS ARE THE SECOND-HIGHEST COMPARED PARAMETERS. a20 for 44 real-world datasets, 1,000 repetitions for 3 synthetic datasets bFor 27 datasets with less than 2,000 samples the start set was set to 10, for the other 8 to 100 cBased on 20 real datasets, which are each augmented to include random outliers three times dTwo scenarios with different splitting strategies were evaluated eIn the first scenario 5 base learners with 2 kernel parameter nationalizations were used, in the other only two base learners fFor 20 datasets with less than 2,000 samples the start set was set to 10, for the other 6 to 100 parameters, we decided not to include this parameter in our study. Deep learning: Newer research, such as [17], [18], [29], focuses on deep-learning based ML learner models, often resulting in new AL strategies, especially in how uncertainty is calculated for deep neural networks. 2) Common hyperparameter ranges: After listing the related works on AL hyperparameter investigations, we are compiling in this section an overview of the common hyperparameter values used in AL literature to use as basis for the decision of our hyperparameter grid. Common dataset choices include the UCI Machine Learning Repository [31], the Open ML Benchmarking Suite CC18 [32], synthetic datasets with specific property like an XOR-like dataset [33], [34] or domain-specific datasets like ImageNet [35]. The evaluated parameters values for the set of ML learner models often include general-purpose ML models such as multi-layer perceptrons, random forest classifiers, support vector machines, logistic regression, or domain-specific neural network architectures like transformermodels [13]. The set of query strategies typically includes random sampling as baseline and mixture of informativenessbased and representativeness-based query strategies. simple uncertainty-based strategy like Least Confidence [1] is often used. The batch size parameter is usually not rigorously evaluated, with researchers guessing real-world values for respective datasets, ranging from 1 [30] to values of up to 10,000 [29]. The train-test-split function set varies across papers, with some opting for n-times k-fold cross-validation, while others use fixed splits with more different starting sets. The hyperparameter defining the end of the AL cycle is rarely evaluated. Some papers use fixed number of cycles, ranging from 20 [30] to the datasets length [29]. Few employ stopping criteria [28]. 3) Special case of AL evaluation metric: The evaluation metric used in AL experiments varies significantly across studies, which deserves special attention. The main goal of metric is to either quantify the performance of the AL strategy, reflecting the saved human effort, or to gain insights into how the strategy functions. The challenge lies in maximizing the human effort saving with minimal performance loss for the learner model due to less available labeled data [1], [20], [36]. Most researchers use standard ML metrics such as accuracy, precision, or the F1-score to measure the learner ML models performance on the currently available labeled data. As an AL process can be seen as graph, the most common way to compare multiple experiments are so-called learning curveplots. Fig. 2 shows exemplary learning curves, highlighting common issues in using learning curves for evaluation. In Fig. 2a, the plot becomes cluttered with 28 strategies, while Fig. 2b illustrates the ambiguity of determining which strategy performs best. Strategy requires fewer labeled samples at the beginning to reach high metric but stagnates early on. Strategy requires more labeled samples in the beginning to achieve the same metric value but reaches higher values in the end with growing set of labeled samples. Strategy C, in contrast, has constant value all the time. It is debatable if the poor performance of Strategy in the beginning in contrast to Strategy outweighs the better performance in the last AL ARXIV PREPRINT: SURVEY OF ACTIVE LEARNING HYPERPARAMETERS (a) Overfull plot using 28 strategies (and the statistical variation over multiple train-test-splits or multiple start sets) (b) Debatable interpretations of which strategy is the better one here Fig. 2. Showcasing the limits of learning curve plots cycles. The main reason behind the difficulty of interpreting the learning curve plots lies in the problem of combining the two objectives of AL, saving as much human effort as the same time training high-performing possible, but at learner model [20], [36]. An abundance of methods exists to aggregate learning curves into single quantifiable value, referred to as aggregation-metrics. An easy solution is to focus only on the final AL iterations metric value, given fixed budget [13]. However, this ignores the progress in fluctuating AL learning curves. more popular approach is to take the arithmetic mean over all AL cycles [13], [26], [37]. This is similar to the areaunder-curve metrics, which interpolate the integral using the trapezoidal rule. As already stated in other works such as [26], [34], approximating the integral using the trapezoidal rule is not necessary (and even potentially misleading), as one can directly calculate the integral of the discrete function using the arithmetic mean. Putting further emphasis on the discovery of Evans et al. [19] that most of the AL gain occurs in the early AL cycles, some work further partitions the learning curve into multiple areas [20], [26]. The first AL cycles are called rampupor early-phase, the last ones the plateauor saturationphase. Unfortunately, most works use fixed threshold in their evaluation to differentiate between these phases, which is impractical since this is highly dataset-dependent threshold. In our evaluation, we analyze the influence of different ranges on the outcome of AL evaluations, and inspired by [20], [38], we propose dynamic approach to distinguish the rampupfrom the plateau-phase during AL experiments using the performance of the random strategy as baseline. As standard ML metrics such as accuracy or the F1-score are only partially comparable between two datasets (e.g., 5% gain for one dataset is not the same as 20% gain for another), some researchers prefer to use ranks instead of absolute ML metric values [20], [37]. The ranks can be presented in tabular performance report or as part of learning curve. Many studies also incorporate statistical testing in their evaluation to demonstrate that potential strategys superiority is just due to random chance. Typical tests include the ttest [17], [39][41] and its variants, such as Welchs ttest [12], the more popular non-parametric Wilcoxon signed rank test [39], [42][45], or the Friedman test [23], [46], which Parameter name Dataset Train-Test-Splits Start Point Sets AL Strategies Batch Sizes Learner Models Metrics Parameter Values 92 Datasets in total from UCI [31], Kaggle [47] and OpenML [32], ranging from 100 to 20,000 samples, from 2 to 31 classes, and from 2 to 1 776 feature dimensions 5 random splits per dataset 20 random start sets per dataset train test split, each with one example per classification class 28 different implementations taken from combining ALiPy [48], libact [49], Google Playground [50], scikit-activeml [51], Small-Text [52], see Table III for further details 1, 5, 10, 20, 50, 100 Neural Network (MLP), Support-Vector-Machines (SVM) with RBF Kernel, Random Forest Classifier (RF) Basic ML metrics: accuracy, class-weighted F1-score, class-weighted precision, class-weighted recall, macro F1-score, macro precision, macro recall, runtime, queried data samples Aggregation metrics, applicable per basic ML metric: full mean, first 5, last 5, last value, ramp-up, plateau aThe complete list of used datasets can be found in the source https://github.com/jgonsior/ this code olympic-games-of-active-learning/tree/main/dataset list.txt repository under paper for TABLE II HYPERPARAMETER GRID USED BY US takes differences between datasets into account. Results are often presented as win-/tie-/loss-matrices [11], [12], [14], [44]. Common significance levels are 90% [11], [17], 95% [23], [41], and 99% [12], [14]. Given that AL strategies can exhibit quadratic runtime complxeity [24], [37], runtime is another favorable metric for display [13], [37]. D. Parameter grid used in our experiments After extensive research on the previously used hyperparameter ranges, common trend is that most works have neglected at least single hyperparameter and usually only focused on few hyperparameters. This means that all results are based on single choice for the neglected hyperparameters and are, therefore, ignorant of the influence of this hyperparameter. We aim to solve this by including as many hyperparameter ARXIV PREPRINT: SURVEY OF ACTIVE LEARNING HYPERPARAMETERS 6 Strategy Name Framework Abbreviation Source CAL Small-Text [52] scikit-activeml [51] CE ALiPy [48] Small-Text [52] ALiPy [48] libact [49] Small-Text [52] ALiPy [48] Done by us Done by us scikit-activeml [51] QBC KL CORE CORE DWUS DWUS EKM GD OG10 OG20 ConstrastiveAL Cost Embedding Coreset Greedy Coreset Greedy Density Weighted Density Weighted Embedding K-Means Graph Density Optimal Greedy 10 Optimal Greedy 20 Query-by-Committee Kullback-Leibler Divergence Query-by-Committee Vote Entropy QUIRE QUIRE Random Random Uncertainty Entropy Uncertainty Entropy Uncertainty Entropy Uncertainty Entropy Uncertainty Least Confidence Uncertainty Least Confidence Uncertainty Least Confidence Uncertainty Least Confidence Uncertainty Max-Margin ALiPy [48] Uncertainty Max-Margin Uncertainty Smallest-Margin Uncertainty Smallest-Margin libact [49] scikit-activeml [51] QBC VE libact [49] QUIRE scikit-activeml [51] QUIRE RAND ALiPy [48] RAND Small-Text [52] Unc ENT ALiPy [48] Unc ENT libact [49] Small-Text [52] Unc ENT scikit-activeml [51] Unc ENT ALiPy [48] Unc LC Small-Text [52] Unc LC Unc LC scikit-activeml [51] Unc LC Unc MM scikit-activeml [51] Unc MM Unc SM libact [49] Small-Text [52] Unc SM [53] [40] [54] [54] [55], [56] [55], [56] [57] [58] [37], [59] [37], [59] [60][62] [60][62] [43] [43] - [63] [63] [63] [63] [64] [64] [64] [64] [65] [65] [66] [66] TABLE III ABBREVIATIONS AND SOURCES OF OUR USED AL STRATEGIES values in our grid as possible to gain deeper insights into their interdependencies. Our used hyperparameters are listed in Table II and Table III, totaling in = 4, 636, 800 hyperparameter combinations. For datasets, we included as many classification datasets as we could find, ranging from easy to hard datasets, from binary to up to 31-multi-class datasets, from small feature vector spaces with two dimensions to very large dimensions (1,776), and small datasets (100 samples) to large datasets (20,000 samples). Similar to other works, we used five random train-test splits to run each dataset five times. Combined with 20 different start point sets per train-test split, each dataset was repeatedly used 100 times. We combined the AL frameworks ALiPy [48], libact [49], Google Playground [50], scikit-activeml [51], and SmallText [52] to leverage as many publicly implemented AL strategies as possible. Unfortunately, some strategies resulted in errors across various datasets, were designed only for binary datasets, or had excessively high runtime complexity. As result, we ultimately had access to 28 strategies listed in Table III. Although some strategies were implemented multiple times in different frameworks, we treated them as distinct strategies for comparison. Additionally, they can be used as safeguard to check if our expectations of different implementations, but the same strategy should equal the same experiment outcome hold. The random strategy was included as baseline, and the Optimal Greedy strategy served as an upper bound on performance. The Optimal Greedy strategy had access to fully labeled set in our simulation and selected the samples that would gain the greatest accuracy in the next AL iteration. This peeking into the future is only possible in the simulations where the true labels are known. We implemented this strategy in two versions: selecting the best batch from 10 random batches and using 20 random batches. Regarding batch sizes, we used variety of values to cover different AL scenarios, from small to large datasets. We deliberately did not include dataset or domain-specific learner models, opting for simple yet well-established base learners that have been shown to work in many scenarios and have fast runtime performance. Additionally, calculating uncertainty for models like Support-Vector-Machines and Random Forest Classifier is sound, compared to the various ways of computing uncertainty for Neural Networks [34], [67]. In terms of evaluation metrics, in addition to standard ML metrics like accuracy, F1-score, precision, and recall, we incorporated aggregation metrics to escape reliance on learning curve plots, as illustrated in Fig. 3. First, we used the arithmetic mean over the whole curve (the full mean). Then, we included variations to distinguish between the ramp-up phase (early AL cycles) and the plateau-phase (later cycles). We used the average of the first and last 5 values, as well as only the last value. Inspired by [20], [38], we calculated dataset-dependent dynamic threshold to distinguish between the ramp-up and plateau phases. Using the random AL strategy as baseline, approximated the plateau using the random strategys mean performance across all cycles. This threshold is drawn as single red line in Fig. 3b. If the assumption holds that there is plateau to reach during the maximum AL budget and that the plateau is much higher than the average in the early phase, the mean should be reasonably close to it. We then calculate sliding window of 5 AL cycles from the back of the timeseries, and the first time the mean of this sliding window is below the threshold, we differentiate between the ramp-up and the plateau phase. Lastly, we also stored as the selected data samples-metric per AL cycle the indices of the currently selected batch for labeling to compare strategies on more qualitative level compared to the high-level view from metrics. III. HPC EXPERIMENTS - IMPLEMENTATION DETAILS Running series of AL experiments over large hyperparameter grid is not trivial, particularly, when one ML model has to be trained in each AL cycle. In this Section, we will share lessons learned from the practical implementation, which we believe will benefit other researchers who wish to reproduce or build upon our work. To enhance reproducibility, we are making our source code fully available on GitHub 5, and publish our complete raw experiment results on the data archive repository OPARA so other researchers can reuse and build upon our work 6. In the 5https://github.com/jgonsior/olympic-games-of-active-learning 6https://doi.org/10.25532/OPARA-862 ARXIV PREPRINT: SURVEY OF ACTIVE LEARNING HYPERPARAMETERS (a) Difference between first 5, full mean, last 5, and last value (b) Difference between the ramp-up and the plateau phase Fig. 3. Illustrating the different aggregation metrics following, we will explain our design decision for our code base. results based on certain hyperparameters, as these can be reused in multiple evaluations."
        },
        {
            "title": "The good news about AL experiments is that",
            "content": "they are highly parallelizable. We used high-performance computing (HPC) cluster, as running the experiment on commodity hardware is not feasible. Our experiments consist of two stages: first, running the simulations, and second, analyzing the results. During the simulations, the resulting data grows to multiple terabytes in size. In the parallel setting, we found that appending results to CSV files worked best, as the order of concurrent appending write requests is irrelevant. bottleneck is filesystem overloading and write collisions on single CSV files. We used partitioning strategy and split our results by dataset and AL strategy. This worked well with up to 10,000 parallel jobs, although we would recommend further partitioning (e.g., by third and fourth hyperparameter besides dataset and strategy) for even larger scales. Most filebased compression algorithms are incompatible with append operations, so we did not compress files during the simulation phase. Our hyperparameter grid of 4.6 million combinations required approximately 3.6 million CPU hours. For some hyperparameter combinations (e.g., QUIRE on the largest datasets), the runtime was multiple days, which was infeasible given the human-in-the-loop nature of AL. Therefore, we enforced runtime limit of 5 minutes per AL cycle 7. Any hyperparameter combination exceeding this limit was excluded. Other works have also noted the need for runtime limits, with values ranging from 3 minutes to 7 days [18], [37]. It is not recommended to keep the results spread across multiple CSV files (due to our partitioning) during the analysis phase. We suggest merging all metrics into single file in column-store format, where each row contains all hyperparameters and the corresponding metric values. We found Apache Parquet to be the most effective format for storing this type of data. The resulting parquet files ranged from 60MiB to 6GiB. Using these files, all the analysis in the upcoming Evaluation Sec. IV took only few hours per plot. Caching intermediate results proved helpful when grouping 7The anecdotal reason is that annotators are willing to wait as long as typical coffee break is, which roughly equals 5 minutes IV. EVALUATION"
        },
        {
            "title": "Our empirical evaluation focuses on two major research",
            "content": "questions: RQ1 What influence do the hyperparameters have on the results of an AL experiment? RQ2 Which hyperparameters should be included in reliable and comparable AL evaluation? We give brief overview of the data used in our experiments in Sec. IV-A. For RQ1, we present two methods for quantifying the influence of single hyperparameter in Sec. IV-B: first, metric-based correlation heatmaps (Sec. IV-B1), and second, queried samples-based heatmaps (Sec. IV-B2). The results for individual hyperparameters are discussed in Sec. IV-C through Sec. IV-K. For RQ2, we use leaderboard ranking invariancebased heatmaps in Sec. IV-B3. Finally, we summarize our results and provide recommendations in Sec. IV-L, with conclusion on how to establish an empirical AL setup with hyperparameter set, which makes the results reproducible and comparable. The sections on RQ1 offer detailed findings on the influence of specific hyperparameter values. In contrast, the sections on RQ2 will provide high-level recommendations for setting up complete hyperparameter grids for AL experiments. A. Completeness of the experimental results Our Hyperparameter grid consisted of exactly 4,636,800 hyperparameter combinations. Due to the 5-minute runtime limit per AL cycle, we only exclude neglectable amount of 75,924 combinations leaving us with 4,560,876 completed parameter combinations. These missing results are mainly from the largest datasets and both implementations of the QUIRE AL strategy. Additionally, due to various errors in the implementations of AL strategies (See f.e. [24] for detailed analysis of some implementation errors existent in the available open-source AL frameworks), there are some missing results for almost all strategies and datasets. These errors occurred randomly across the entire parameter grid, ARXIV PREPRINT: SURVEY OF ACTIVE LEARNING HYPERPARAMETERS 8 and we will present an interpolation strategy in the following Sec. IV-B to handle these gaps in our nearly dense result grid. an aggregation metric, as the individual metric results per AL cycle are too chaotic to compare them directly 8: B. Quantifying the Influence of Single Hyperparameter Vb1(M ) = Quantifying the influence of single hyperparameter is not straightforward task. We present three approaches: two methods based on the correlation between different hyperparameter combinations and one based on the correlation between leaderboard rankings. The differences between the three methods lie in the aspect of correlation that is measured. High correlation indicates redundant hyperparameter values in an experimental hyperparameter grid. Metric-based heatmaps (coloured in blue) measure the correlation between ML metrics for two hyperparameter combinations. If two combinations produce similar ML metrics, they have high correlation. Queried samples-based heatmaps (colored in green) focus on the correlation between the sets of queried samples for labeling by the AL strategies. High correlation means the same samples were selected for labeling. Leaderboard ranking invariance-based heatmaps (colored in orange) focus on comparing the rankings of AL strategies between different hyperparameter grids. They correlate highly if two hyperparameter combinations produce similar rankings across strategies and datasets. These three correlations do not necessarily agree, as discussed in the upcoming results Sections. 1) Metric-based heatmaps: The first method for quantifying hyperparameter influence relies on the extensive hyperparameter grid. We compare the results of single metric between two hyperparameter combinations, with all parameters except for single varying one, fixed. If there is significant difference in the metric (e.g., average accuracy across all AL cycles), this difference is attributed to the specific varying hyperparameter being evaluated. We aggregate the differences across all possible hyperparameters to get quantitative idea of how much each hyperparameter influences AL results. This approach works for all hyperparameters, except the initial start sets and the train-test-splits T, as these are not directly comparable between different datasets. batch size of 5 is the same batch size of 5 for two different datasets, whereas train-test-split based on random seed is different split on another dataset. These two parameters will be quantified later in Sec. IV-L. We want to quantify the influence of single hyperparameter; for illustration, we are using the batch size hyperparameter = [RR RDTSILb], bi B. First, single vector bi gets constructed for each bi B. Vbi contains all the results from the experiments, having batch size bi as hyperparameter. The vectors, e.g., Vb1 for b1 and Vb2 for b2, share specific property: each dimension contains the results for the same set of hyperparameters, except the batch size. Thus, the experimental conditions for each dimension are equal, except for the batch size change, being either b1 or b2. We denote the result of the j-th hyperparameter combination having the hyperparameter bi and the metric using Mbij. We are using , Vb2(M ) = Mb11 Mb12 ... Mb21 Mb22 ... From the result vector for specific metric R(M ), we can calculate how similar or dissimilar the outcomes of two AL experiment are if only single hyperparameter is changed, using the Pearson correlation coefficient [68]. In the following Sections, we will display metric-based heatmaps, containing the Pearson correlation coefficients between all combinations for single variable hyperparameter (in this exemplary case: = [b1, . . . , bK]: r(Vb1(M ), Vb1(M )) ... r(VbK (M ), Vb1(M )) . . . r(Vb1(M ), VbK (M )) ... r(VbK (M ), VbK (M )) For better recognition, we are using different color scheme for our heatmaps. The color for the metric-based heatmaps is blue. 2) Queried samples-based heatmaps: Another approach to quantify the influence of single hyperparameter focuses on the queried samples, instead of using an ML metric as before. We use the list of samples, queried in each AL cycle by the AL strategy for labeling by the oracle R(Q) = [Q0, . . . , Qc]. Here, the vectors do not consist of numeric values (as before) but instead sets of sets of data samples. The resulting vectors, which represent the effect of single hyperparameter in the parameter grid are therefore (in this example for the batch size hyperparameter values b1 and b2): Vb1 (Q) = Vb2 (Q) = (cid:100)Qb11 = (cid:83)c (cid:100)Qb12 = (cid:83)c ... (cid:100)Qb21 = (cid:83)c (cid:100)Qb22 = (cid:83)c ... i=0 Qi i=0 Qi b22 i=0 Qi i=0 Qi b21 b22 Analog to the vectors from the metric results, each dimension contains the results for the same combinations of hyperparameters, except for the different hyperparameters of, e.g., the batch size b. We are denoting the query of the first hyperparameter combination having b1 as batch size with Qb11. It is not relevant which hyperparameter is exactly the first one, it is only important that the order is consistent across the vectors , to ensure comparability between the vector dimensions. Since we cannot directly compute the Pearson correlation for vectors of sets of sets, we are first taking the concatenated set of labeled samples ˆQ = (cid:83)c i=0 Qi as elements in the vector 8To get an idea of the chaotic learning curve plots, take look at the variance in Fig. 2a. The plot contains the results for 28 different hyperparameter values (28 strategies) for single metric. Comparing each AL cycle individually is not practical, as we do not expect each hyperparameter combination to get to the same metric value after AL cycles, but the trend is similar. This can be achieved using the aggregation metrics. ARXIV PREPRINT: SURVEY OF ACTIVE LEARNING HYPERPARAMETERS 9 Dataset D1 Dataset D2 ... Final Ranking Strategy S1 Strategy S2 LCS1D1 LCS1D2 ... DiD LCS1Di (cid:80) LCS2D1 LCS2D2 ... DiD LCS2Di (cid:80) . . . . . . . . . ... . . . TABLE IV LEADERBOARD RESULTS , and second use the Jaccard index J, to calculate dimension for dimension for two given vectors Vb1(Q) and Vb2 (Q), how similar the queried sets for each iteration are. The Jaccard index [69] proved to generate more sound results than rank correlation measures such as Kendall Tau [70] or Spearmans rank correlation coefficient [71]. This can be attributed to the high fluctuations in AL cycles, often resulting in two very different rankings despite high Jaccard similarity. For given pair of hyperparameter values, for example b1 and b2, the jaccard vector VJb1b2 is computed: VJb1b2 = J( (cid:100)Qb11, (cid:100)Qb21) J( (cid:100)Qb12, (cid:100)Qb22) ... As the last step, we take the sum of the resulting jaccard vector VJb1b2 , and divide it by the length of the respective vector to be able to generate the exemplary heatmap for the batch size hyperparameter = [b1, . . . , bK]. For consistency with the metric-based heatmaps, we subtract the Jaccard index from 1 so that 1 indicates full similarity. The heatmaps for queried samples-based heatmap are colored in green: (cid:80) 1 1 = 1 . . . jVJb1b1 VJb1 b1 ... jVJbK b1 VJbK b1 (cid:80) 1 (cid:80) 1 (cid:80) jVJb1bK VJb1bK ... jVJbK bK VJbK bK = 1 3) Leaderboard ranking invariance-based heatmaps: This approach is conceptually different from the previous two, as it focuses on comparing the rankings of AL strategies under different hyperparameter grids. For this, we fix all hyperparameters except one and compare the rankings of AL strategies across multiple datasets in so-called leaderboard. Table IV shows an exemplary leaderboard. The content of each leaderboard cell is denoted as LCSiDj , which represents the metric results for specific AL strategy Si on dataset Dj, averaged over all fixed hyperparameter combinations (with the hyperparameter under analysis being the variable one): LCSiDj = (cid:80) MkVDiSj (M ) (Mk) VDiSj strong influence. To calculate correlations between the rankings, we use two-sided Kendalls tau-b rank correlation test [70], which is robust to outliers and differences in ranking distributions. Leaderboard ranking invariance heatmap are colored in orange. Using leaderboards to compare AL strategies is not straightforward and comes with few practical challenges. The most significant issue is the comparison of metrics results across datasets. For example, dataset D1 may have accuracy scores between 50% and 70%, while dataset D2 might have accuracy scores between 42% and 45%. In this case, AL strategies optimized for dataset D1 would show more significant absolute improvements than those optimized for D2, even if they perform equally well in relative terms. To overcome this, some researchers normalize the metric values across datasets or use ranks instead of absolute values [20], [37]. We chose ranks as normalization would add layer of complexity due to the selection of the normalization method (e.g., outlier resistance or not). Another issue arose during our evaluation because of the sparseness of the result grid missing some hyperparameter combinations, as thus the leaderboard cell entries LC are not all aggregated on the same amount of hyperparameter combinations. Therefore, to make the comparisons between the final ranking vectors even, we are interpolating the results using fixed value of 0, meaning the worst possible outcome for the ML metric. This punishes strategies that either take longer than the runtime limit or result in runtime errors. C. Machine learning metrics performance The first hyperparameter we investigate is the influence of the ML metric. This parameter is evaluated first, as the results determine which metric we will use in the upcoming evaluations for the vectors . The hyperparameter metric has two aspects: first, the foundational ML metric (like accuracy), which measures the performance of the learner model for each AL cycle, resulting in timeseries; and second, the aggregation metric, which compacts the learning curve or timeseries into single value. For both aspects, we provide two plots. First, heatmap using the quantification method based on the ML metric results and the Pearson correlation coefficient. Second, heatmap based on the correlation between the final leaderboard rankings. Fig. 4a and Fig. 4b show the Pearson correlation heatmaps for the foundational and aggregation metrics, while Fig. 4c and Fig. 4d display the leaderboard heatmaps. The heatmaps present the Pearson correlation coefficient between all results in our hyperparameter grid, comparing one metric with another. High values close to 100 indicate that both metrics behave similarly, while low values close to 0 indicate no correlation. As the queried samples are the same for each metric, we do not present queried samples-based heatmaps for this part. The leaderboard can then be reduced to final ranking of all strategies. This final ranking can be vector of the average results per strategy. If changing hyperparameter significantly alters the rankings, this indicates the hyperparameter has For the evaluations in the upcoming sections, single ML metric is preferred. Since it is challenging to determine the best metric, we focus on the one with the highest correlation to all other metrics. This ensures that results based on this ARXIV PREPRINT: SURVEY OF ACTIVE LEARNING HYPERPARAMETERS (a) Metric-based heatmap, basic ML metrics (b) Metric-based heatmap, aggregation ML metrics (c) Leaderboard ranking-invariance heatmap, basic ML metrics (d) Leaderboard ranking-invariance heatmap, aggregation ML metrics Fig. 4. Metric-based and leaderboard ranking invariance heatmaps for the basic as well as the aggregation metrics metric will be representative of the other metrics as well. Unsurprisingly, the F1-score, which is the harmonic mean of both precision and recall, shows the highest correlation. Thus, for the upcoming Sections, we use the the F1-score as metric , since it correlates highly with the accuracy metric and is better suited for datasets with many classes and class imbalances. The results are fairly similar between the metricbased and the leaderboard ranking invariance heatmap. Regarding the aggregation metrics displayed in Fig. 4b and Fig. 4d (see Sec. II-D and Fig. 3 for details about the included aggregation metrics), it is not surprising that there is strong correlation between the first 5 values and the ramp-up phase, as well as between the last 5, the last value, and the plateau phase. This pattern holds in both the leaderboard ranking-invariance heatmap and the metric-based heatmap. There are arguments for preference for both phases in an evaluation: there is more fluctuation in the ramp-up phase, and the results only stabilize in the plateau phase, which would be an argument for using plateau-phase metrics. On the other hand, most gains in AL are in the ramp-up phase, as the plateaus are often very similar, which is not surprising, as certain amount of labeled data will always turn out in very similar results. As the full mean metrics cover both phases and correlate highly with all aggregation metrics, we will safely use this metric in the upcoming evaluations. The differences in the metrics become more obvious in the leaderboard ranking invariance heatmap, even though the results in both heatmaps are quite similar. As the correlations vary drastically for the aggregation metric, we advise to be cautious when deciding on an aggregation metric apart from the full mean. Especially given the unclear distinction between ramp-up and plateau phase in the context of AL, we do not recommend using either phase as the sole evaluation focus. D. Batch size The results for the batch size hyperparameter, shown in Fig. 5, are as expected. Regardless of the heatmap used (metric-based, queried samples-based, or leaderboard ranking invariance-based), there is noticeable trend: batch sizes correlate highly when their values are close. Smaller batch sizes tend to behave similarly, and the same goes for large batch sizes. However, large batch sizes differ significantly from small ones. This is most evident in the queried samplesbased heatmaps. Since most AL strategies select batches based on top-k ranking, the top-50 samples are expected to be substantial subset of the top-100 samples, explaining the high correlation of 99.1%. As different batch sizes correlate higher with each other for the metricor leaderboard ranking invariance-based heatmaps it does not matter so much which exact samples are queried to get the same results. Especially for large datasets, many samples are likely equivalent, leading to similar leaderboard rankings despite differences in the queried samples. The correlation to the entire parameter grid was included row/column in the leaderboard ranking as an additional ARXIV PREPRINT: SURVEY OF ACTIVE LEARNING HYPERPARAMETERS 11 (a) Metric-based (b) Queried samples-based (c) Leaderboard ranking invariance-based Fig. 5. Heatmaps for the batch size hyperparameter (a) Metric-based (b) Queried samples-based (c) invariance-based Leaderboard ranking Fig. 6. Heatmaps for the learner model hyperparameter invariance-based heatmap. This shows that very large batch sizes result in vastly different rankings, whereas lower values mostly agree. Overall, we conclude that in an AL experimental evaluation, it is advisable to include at least two batch sizes: one very small and one larger value. Using two large batch sizes is not recommended, as the results would be too similar. If only single batch size can be included, batch size of 20 showed the highest correlation to all other values on average. E. Datasets Unfortunately, the heatmap visualizations for the dataset hyperparameter, which spans 92 dimensions, are too large to be displayed in this paper. However, they can be found in the GitHub repository associated with this study 9. These heatmaps confirm the findings of other works, showing that the dataset parameter significantly impacts the performance of AL strategies. Unlike the other hyperparameters, datasets exhibit substantial variation, with correlation values ranging from as low as 30% to as high as 100%. Given this variability, we do not recommend exclusively using specific set of datasets. Instead, we advise using an extensive, diverse collection of datasets, such as the well-known OpenML-CC18 benchmark suite [32], which includes datasets with various characteristics and levels of complexity. F. Learner models The hyperparameter of the ML learner model shows that our three compared model implementations behave very similarly. The metricand queried samples-based heatmaps 9https://github.com/jgonsior/olympic-games-of-active-learning/tree/main/ plots show very high correlations between the included hyperparameter values in our experimental grid. The results for the leaderboard ranking invariance-based heatmap, displayed in Fig. 6 are slightly different: even though the RF model behaves still very close to the MLP model, the SVM (RBF) model results in very different final rankings. The correlation between the RF model and the SVM (RBF) model is especially low. Our explanation lies in the different correlations measured in the three heatmaps: high correlation in the queried samplesbased heatmap, and similar high correlation in the metricbased heatmap shows that similar samples are selected for labeling and that the respective reached ML metrics correlate with each other. As the leaderboard ranking invariance-based heatmap shows less correlation, the similar selected samples and similar reached ML metric results do not result in similar final leaderboard rankings. Slight changes in the underlying metric can easily cause ranking change. Potentially, these are the remaining 20% not present in the correlation in the other two heatmaps. Regarding the question of why especially the RF and SVM (RBF) models behave so differently compared to the MLP model, we hypothesize that both models have disjunct set of datasets where they perform very well, potentially datasets with many classes for the RF model, and datasets which are sparse for the SVM model. As the MLP model can adapt to both settings very well, it can correlate better with both models. The last row in the leaderboard ranking invariance-based heatmap shows the correlation between the experiment grids limited to the respective single learner model compared to the total grid. As the MLP model has the highest correlation to the complete grid of all results, this also backs up our hypothesis that the MLP model can perform well for both datasets and is better suited to the RF and the SVM (RBF) models. ARXIV PREPRINT: SURVEY OF ACTIVE LEARNING HYPERPARAMETERS In conclusion, we advise using the MLP model, which seems to produce the same results as the RF and SVM (RBF) models. G. Active learning query strategies Our experimental grid includes 28 AL strategy implementations from various frameworks and authors. The leaderboard ranking invariance-based heatmap depends on the presence of multiple AL strategies and, therefore, cannot measure the correlation between two strategies like the other hyperparameters. Thus, Fig. 7 presents the metric-based heatmap (a) and the queried samples-based heatmap (b). We begin by interpreting the metric-based correlations, which show stronger correlations and provide more valuable insights. We anticipated that the same strategy would perform similarly when implemented across different frameworks. Notable strategies with multiple implementations include CORE, DWUS, QUIRE, RAND, and all four uncertainty variants (ENT, LC, MM, and SM). Additionally, QBC and OG are each implemented in two versions. As expected, the two CORE variants behave almost identically, with 99.0% correlation. The two DWUS variants show some similarities but are not as pronounced compared to their correlations with other strategies. Interestingly, the two QUIRE implementations exhibit relatively low correlation of only 44.5%, indicating significant differences. The most notable finding is the high correlation among all direct uncertainty-based variants, with few exceptions. Unc ENT implemented in SKA, Unc LC implemented in LIB, and Unc MM implemented in SKA are outliers. The three variants of Unc LC, Unc MM, and Unc Ent in the framework SKA are more closely related to each other than to implementations of the same strategy in the different frameworks. Surprisingly, we conclude that the specific implementation of AL strategies can impact performance more than the underlying strategy itself. To further investigate the influence of the framework on AL strategy implementation, we calculated the average correlation among strategies within each framework, including standard deviations. As shown in Fig. 8, there are no high correlations observable between framworks, except for ALI and SM. However, given the high standard deviation, these correlations are likely due to few similarly implemented strategies. Interestingly, we observed trend of high correlation among strategies within the same framework despite the variety of strategies included in each framework. The self-correlation of 89.8% for ALI is exceptionally high, while at the same time the standard deviation is the lowest overall. This leads us again to the conclusion that AL strategies have very high degrees of freedom in their implementation and that some design choices inside of framework can have very high impact on the performance of the AL strategy, even further than the concrete AL strategy. It is out of the scope of this paper to investigate in detail how the different implementations differ from each other. Still, we believe this is an interesting research question that should be addressed in future research endeavors. AL strategies can be categorized broadly into two categories [15], uncertainty-based [64], [65], [72][77], and diversity-based strategies [54], [78]. Modern strategies aim to combine both [33], [37], [43], [79][81]. The uncertaintybased strategies in this paper are CAL, CE, QBC, and all Unc variants (ENT, LC, MM, and SM), the diversity-based are CORE, EKM, and GD, and the combined variants are DWUS and QUIRE. As baselines serve the random strategy, as well as the optimal strategy OG, capable of peeking into the future to make greedy near-optimal decision based on the knowledge of what the true labels are (which is, of course, knowledge only present in AL experiments, and never present in real-world settings). We could not find any proof in our experimental results showing general similarity among all the AL strategies of similar category, neither uncertaintybased nor diversity-based, nor for the combined strategies. We included the near-optimal baseline strategies OG10 and OG20 as an upper barrier to measuring how good current implementations are. OG10 has access to the future for 10 queries, whereas OG20 has access to 20 queries. Interestingly, this difference significantly affects how similar both strategies are to all others. OG10 is dissimilar to almost everything, whereas OG20 highly correlates to many strategies. We explain that there is indeed shared property that marks some data samples as good for AL, and OG20 can find it due to the knowledge of more future foresight, compared to OG10, which with comparably small foresight mostly behaves randomly. We will come back later to this point in Sec. IV-L, where we discuss if the similarity to OG20 hints at an overall good performing AL strategy. Many correlations in the metric-based heatmap become nonexistent in the queried samples-based heatmap. This can be easily explained by the fact that multiple data samples often have the same training outcome as they are very similar/close to each other but still have different samples. Small implementation details such as different sorting, less than vs. less than or equal, etc., can make huge difference. Given that most correlations here are at least 80% or higher, it indicates that most strategies select the same samples overall. Still, their ordering is what makes the real difference regarding performance. Again, most Unc strategies are close to each other, without any noteworthy exceptions compared to the metric-based correlation; even the poorly metric-correlated Unc LC implementation in LIB is now closely similar to all other strategies. The correlation of the random baseline strategies regarding the queried samples is 100.0%, which is very high compared to the metric-based correlation of 78.5%. This hints at the complexity of AL overall. Even though both strategies selected the same number of samples for labeling in the end, the resulting ML metrics are not always identical. We explain this discrepancy by different orderings in the selected samples at each iteration, which leads to different order of the samples presented to the ML learner models and, therefore, slightly different classification boundaries. This shows that the selection of the metric, either being ML metric, or the set of selected indices, leads to very different interpretation of how similar or different two AL experiments are. ARXIV PREPRINT: SURVEY OF ACTIVE LEARNING HYPERPARAMETERS (a) Metric-based heatmap Fig. 7. Heatmaps for the AL query strategy hyperparameter AL strategy (b) Queried samples-based heatmaps ARXIV PREPRINT: SURVEY OF ACTIVE LEARNING HYPERPARAMETERS 14 indicate hyperparameter grid that fits our target criteria. As correlation, we are using the leaderboard ranking-invariance. Fig. 11a shows scatterplot of the Spearman correlation coefficient between the total grid of hyperparameters used in this study and randomly sampled subsets of hyperparameter combinations of growing size. For each amount of hyperparameter combinations on the x-axis, we have drawn 100 random subsets. The orange line shows polynomial regression of maximum order 3 to give correlation trend. It can be seen that after below 2,000 random hyperparameter combinations, the correlations have very high spread, indicating high chance of picking set of hyperparameters that will produce an experimental outcome that will be contrary to those of similarly sized hyperparameter sets. This is probably the leading cause of why the results of many AL studies contradict each other about the superiority of single AL strategies over another. Starting from around 2,000 combinations, though, the results seem to stabilize, and from around 4,000 combinations onwards, strong correlation between 80% and 100% is almost guaranteed. So our first finding can be summarized as, given wide variety in the hyperparameter grid (around 4.6 million combinations of 6 hyperparameters in our case), fraction of at least 4,000 combinations is already enough to produce the same results as for the complete grid. We hypothesize that the main quality the smaller subset of sufficient hyperparameter combinations must share is wide variety of hyperparameters from which they are drawn. It does not matter which concrete hyperparameter values are included in the grid. It is much more important to have large variety of hyperparameters. To further back up this assumption, we have removed each hyperparameter (except for the set of AL strategies S, and the set of datasets D, which both have stayed the same for the leaderboard ranking-invariance correlations), and repeated the experiment of calculating the correlation between the smaller drawn subsets, and the total grid. The results are shown in Fig. 11b to Fig. 11e. For example, by removing the batch size hyperparameter from the grid, we used instead of the complete grid of the smaller grid of and used fixed batch size of 20. Thus, we reduce the entropy in the possible parameter grid from which the smaller subset could be drawn. The plots show that removing the hyperparameters of the batch size and the learner model substantially impacts the correlation between the randomly drawn subsets and the complete hyperparameter grid. The results fluctuate less for small hyperparameter samples, and the correlations generally have less variability. Also, strong correlation of 95% and above can never be reached after removing these two parameters completely from the grid. In contrast, removing the initially labeled start set and the train-test-split hyperparameter does not seem to impact the distribution of the correlations at all; they look almost identical. This is expected, as both hyperparameters depend on the dataset and the respective samples of dataset. As the corresponding other hyperparameter and the dataset still exist in the hyperparameter grid, there is still enough entropy due to the dataset-related hyperparameters present. Therefore, these two are hyperparameters that can be safely removed from the grid, but do not harm anything if Fig. 8. Average correlation and standard deviation among the AL strategies between the frameworks H. Train-test-split The hyperparameter train-test-split differs from the previously evaluated ones, as the concrete parameter values are incomparable across datasets. This hyperparameter describes specific train-test split, e.g., the fourth train-test-split of dataset D1 differs from the fourth train-test-split of dataset D2. Therefore, the correlation heatmap plots do not tell us anything about the influence of specific value for this parameter. However, they do tell us more about the general influence this parameter can have if changed. The queried samples-based heatmap is not possible, as due to different train-test-splits, the two possible sets of unlabeled samples between two splits are different. Fig. 9 contains the metric-based heatmap (a; blue) as well as the leaderboard ranking invariance-based heatmap (b; yellow). The correlations show that this hyperparameter has no considerable influence on the results of AL experiments, as the results correlate both metric-wise and for the resulting final leaderboard rankings highly with each other. We will give an additional explanation of why this parameter seems unimportant in the later Sec. IV-J but may be more critical than these plots show. I. Start point sets The results for the hyperparameter of the start point set are displayed in Fig. 10. The results are the same as for the train-test-splits, overall this hyperparameter seems to have no big influence on the outcome of AL experiment. This is surprising result, as one would expect that the choice of the initially labeled samples greatly influences the outcome. Why this is potentially still the case will be explained in Sec. IV-J. J. What is the minimum amount of needed parameters after all? Besides analyzing the influence of specific AL hyperparameters, our research focuses on recommending which hyperparameter grid should be used in empirical evaluations, ensuring that the results are reproducible among AL studies. We are looking for the minor grid of parameter values, which still reliably produces the same results as almost every other parameter combination. Using large hyperparameter grid similar to ours is not practically feasible, as it exceeds the computational budgets of most research projects. Under the assumption that our hyperparameter grid is large enough to include almost any other possible AL evaluation scenarios, strong correlation to the complete grids results should ARXIV PREPRINT: SURVEY OF ACTIVE LEARNING HYPERPARAMETERS 15 Fig. 9. Heatmaps for the hyperparameter train-test-split (a) Metric-based (b) Leaderboard ranking invariance-based Fig. 10. Leaderboard ranking invariance-based heatmap for the hyperparameter start set kept. K. Runtime The runtime performance of AL strategies is rarely considered in AL studies. AL is human-in-the-loop approach, where humans label the selected queries. As human labor is the main reason to use AL in the first place, the fast runtime performance of AL strategies is crucial deciding factor in practice, with the goal of minimal idle human time. In Fig. 12, the average duration of single AL cycle, averaged over the complete hyperparameter grid, is shown for each AL strategy, using logarithmic scale. There is noticeable difference between fast strategies, such as most uncertaintybased variants, and more complex strategies, such as Core, QBC, or QUIRE. Given that the majority of our datasets are small and our 5-minute runtime limit per AL cycle, these differences will only become more evident for huge datasets, such as in the NLP or image domain. clear distinction is noticeable between the strategies implemented in different frameworks. We hypothesize that the data model used inside the frameworks has the most prominent performance impact. We encourage other AL researchers to consider runtime as metric for evaluating AL strategies in future work, in addition to ML performance metrics, as it is crucial to keep annotators motivated. L. Final leaderboard Given our extensive empirical results, we dare to give nonnuanced general answer to the question of which AL strategy works best in most scenarios, as well as showing the complete leaderboard of all datasets and parameter combinations for our entire grid in Fig. 13. Clearly, the margin-based AL strategies MM and SM work best overall. These are also the only strategies able to outperform the random baseline strategies. Close behind are the Cost Embedding strategy CE, QBC in the vote-entropy (VE) variant, which performed tremendously better than the Kullback-Leibler divergence (KL) variant and the Least-Confidence (LC) and Entropy (Ent) uncertaintyvariants. ARXIV PREPRINT: SURVEY OF ACTIVE LEARNING HYPERPARAMETERS 16 (a) Total grid (b) Without batch size (c) Without learner model Fig. 11. Hyperparameter combination amount simulations (d) Without initially labeled start set (e) Without train-test-split Fig. 12. Average Runtime of all strategies over the complete hyperparameter grid, logarithmic scale It can also be clearly seen that most of the strategies implemented in several variants end up at almost identical ranks, with Core, DWUS, and QUIRE being the exceptions. Even though not shown in the plot because we use ranks, we can report that for most datasets, the differences between the strategies are not very large and smaller than 1 percent. Also, no strategy performs consistently well on all datasets, with many having few datasets where they perform very well and others poorly. Even the awful performing strategies like DWUS implemented in LIB or QUIRE have several datasets that rank high. One of the main arguments for the diversity-based strategy is the inability of purely uncertaintybased strategies to be unable to come out of local minima for XOR-like datasets [33], [34]. It can be seen that purely diversity-based strategies Coreset (Core) and Graph Density (GD) perform much better on several datasets like xor 2x2 or r15. This shows that there is indeed not single AL strategy that is the best choice in every scenario and explains why there is such vast discrepancy in AL studies regarding the best-performing strategy. V. SUMMARIZED FINDINGS After first defining vast hyperparameter grid with diverse set of hyperparameters, second performing AL experiments for every hyperparameter parameter combination included in our grid, and third analyzing the results for correlations between the hyperparameter values, our results can be summarized as follows: The used ML metric has measurable influence on the outcome of the experiments, and the used aggregation metric (area-under-curve, final value, etc.) can drastically influence the results. Our recommendation is class-weighted F1-score in combination with the complete arithmetic mean of all AL cycles instead of e. g. an arbitrary distinction between ramp-up and plateau phase. As the batch size hyperparameter also has wide variety in the correlations among the results between different hyperparameter values, we ARXIV PREPRINT: SURVEY OF ACTIVE LEARNING HYPERPARAMETERS 17 Fig. 13. Final Leaderboard of all datasets and strategies, full mean of class weighted F1-score, normalized per dataset for comparability, and interpolated with 0% for missing values (due to timeout or implementation errors) advise using at least two batch sizes, preferably very small and larger value. Regarding the dataset hyperparameter, we found very high variability in the results and advise using as many datasets as possible, preferably from well-known benchmark dataset collection. The used learner model hyperparameter only slightly changes the results and can, therefore, based on our knowledge, safely be fixed to single, accepted domain-fitting model such as multi-layer perceptron. Our findings about the variance between different implementations of the same strategy and surprisingly high correlation among strategies in the same framework, compared to strategies implemented in other frameworks, were somewhat surprising. This leads us to conclude that special care should be taken in implementing AL strategies, as small implementation details can already have significant influence on the outcome. Apart from that, we could find correlations among uncertainty-based strategies and among diversity-based strategies. Multiple traintest-splits and start-point-sets seem to have an insignificant influence if (!) the overall hyperparameter grid is big enough, such as ours. The most important takeaway, in the end, is that given hyperparameter grid with much variety among the combinations, around 4,000 randomly drawn hyperparameter combinations already show nearly the same results as the complete grid. Thereby, huge computational effort can be saved while still getting the same results as for the complete parameter grid. Our last finding is the importance of the runtime of AL strategies, which, given the human-in-the-loop nature of AL, is an aspect that should not be overlooked when developing new AL strategies. VI. CONCLUSION"
        },
        {
            "title": "We went out",
            "content": "into this research endeavor to understand why the results of AL research fluctuate so much between similar papers. The results of AL research papers are often ARXIV PREPRINT: SURVEY OF ACTIVE LEARNING HYPERPARAMETERS 18 not reproducible and contradictory. Our central hypothesis is that AL depends on many hyperparameters, yet no extensive study has investigated the cross-correlations between different hyperparameters. We have, therefore, first gathered all possible hyperparameters and carefully designed big hyperparameter grid containing as much variety among the parameters as possible for real-world use-case scenarios. After evaluating all hyperparameter combinations on high-performance computing cluster, we analyzed the results and got many interesting and surprising insights into the AL research process. We outlined the dangers of using some hyperparameter values, such as different aggregation metrics other than the full arithmetic mean, or the big difference in different implementations by various authors of the same AL strategy. Our hopefully most significant contribution to the AL research field is that in the end, it does not matter which hyperparameters you evaluate. The results will be reproducible and comparable to other works if you sample few hyperparameter combinations from an extensive hyperparameter grid. VII. APPENDIX"
        },
        {
            "title": "ACKNOWLEDGMENTS",
            "content": "The authors gratefully acknowledge the computing time made available to them on the high-performance computer at the NHR Center of TU Dresden. This center is jointly supported by the Federal Ministry of Education and Research and the state governments participating in the NHR (www.nhr-verein.de/unsere-partner). VIII. REFERENCES SECTION"
        },
        {
            "title": "REFERENCES",
            "content": "[1] B. Settles, Active Learning Literature Survey, University of WisconsinMadison Department of Computer Sciences, Technical Report, 2009. [2] K. Tomanek and F. Olsson, web survey on the use of active learning to support annotation of text data, in Proceedings of the NAACL HLT 2009 Workshop on Active Learning for Natural Language Processing, E. Ringger, R. Haertel, and K. Tomanek, Eds. Boulder, Colorado: Association for Computational Linguistics, 2009, pp. 4548. [3] J. Romberg, C. Schroder, J. Gonsior, K. Tomanek, and F. Olsson, Have LLMs Made Active Learning Obsolete? Surveying the NLP Community, 2025. [4] J. Gonsior, J. Rehak, M. Thiele, E. Koci, M. Gunther, and W. Lehner, Active Learning for Spreadsheet Cell Classification, in Proceedings of the Workshops of the EDBT/ICDT 2020 Joint Conference, Copenhagen, Denmark, March 30, 2020, ser. CEUR Workshop Proceedings, A. Poulovassilis, D. Auber, N. Bikakis, P. K. Chrysanthis, G. Papastefanatos, M. A. Sharaf, N. Pelekis, C. Renso, Y. Theodoridis, K. Zeitouni, T. Cerquitelli, S. Chiusano, G. Vargas-Solar, B. OmidvarTehrani, K. Morik, J.-M. Renders, D. Firmani, L. Tanca, D. Mottin, M. Lissandrini, and Y. Velegrakis, Eds., vol. 2578. CEUR-WS.org, 2020. [5] R. Sass, E. Bergman, A. Biedenkapp, F. Hutter, and M. Lindauer, DeepCAVE: An Interactive Analysis Tool for Automated Machine Learning, 2022. [6] V. Nath, D. Yang, B. A. Landman, D. Xu, and H. R. Roth, Diminishing Uncertainty Within the Training Pool: Active Learning for Medical Image Segmentation, IEEE Transactions on Medical Imaging, vol. 40, no. 10, pp. 25342547, 2021. [7] S. D. Narayanan, A. Agnihotri, and N. Batra, Active Learning for Air Quality Station Location Recommendation, in Proceedings of the 7th ACM IKDD CoDS and 25th COMAD, ser. CoDS COMAD 2020. New York, NY, USA: Association for Computing Machinery, 2020, pp. 326 327. [9] J. Kishaan, M. Muthuraja, D. Nair, and P. G. Ploger, Using Active Learning for Assisted Short Answer Grading, in ICML 2020 Workshop on Real World Experiment Design and Active Learning, 2020. [10] E. Demir, Z. Cataltepe, U. Ekmekci, M. Budnik, and L. Besacier, Unsupervised Active Learning For Video Annotation, in ICML Active Learning Workshop 2015, Lille, France, 2015. [11] Y. Yang and M. Loog, benchmark and comparison of active learning for logistic regression, Pattern Recognition, vol. 83, pp. 401415, 2018. [12] D. Bahri, H. Jiang, T. Schuster, and A. Rostamizadeh, Is margin all you need? An extensive empirical study of active learning on tabular data, 2022. [13] C. Schroder, A. Niekler, and M. Potthast, Revisiting uncertainty-based query strategies for active learning with transformers, in Findings of the Association for Computational Linguistics: ACL 2022, S. Muresan, P. Nakov, and A. Villavicencio, Eds. Dublin, Ireland: Association for Computational Linguistics, 2022, pp. 21942203. [14] L. Desreumaux and V. Lemaire, Learning active learning at the crossroads? Evaluation and discussion, CEUR Workshop Proceedings, vol. 2660, pp. 3854, 2020. [15] X. Zhan, H. Liu, Q. Li, and A. B. Chan, comparative survey: Benchmarking for pool-based active learning, in Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI 2021, Virtual Event / Montreal, Canada, 19-27 August 2021, Z. Zhou, Ed. ijcai.org, 2021, pp. 46794686. [16] A. Ghose and E. T. Nguyen, On the Fragility of Active Learners for Text Classification, in Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, Y. Al-Onaizan, M. Bansal, and Y.-N. Chen, Eds. Miami, Florida, USA: Association for Computational Linguistics, 2024, pp. 22 21722 233. [17] Y. Ji, D. Kaestner, O. Wirth, and C. Wressnegger, Randomness is the Root of All Evil: More Reliable Evaluation of Deep Active Learning, in 2023 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), ser. WACV 2023. Waikoloa, HI, USA: IEEE, 2023, pp. 3932 3941. [18] V. Margraf, M. Wever, S. Gilhuber, G. M. Tavares, T. Seidl, and E. Hullermeier, ALPBench: Benchmark for Active Learning Pipelines on Tabular Data, 2024. [19] L. P. G. Evans, N. M. Adams, and C. Anagnostopoulos, When Does Active Learning Work? in Advances in Intelligent Data Analysis XII, A. Tucker, F. Hoppner, A. Siebes, and S. Swift, Eds. Berlin, Heidelberg: Springer, 2013, pp. 174185. [20] D. Kottke, A. Calma, D. Huseljic, G. M. Krempl, and B. Sick, Challenges of Reliable, Realistic and Comparable Active Learning Evaluation. in IAL@PKDD/ECML 2017, 2017, pp. 214. [21] D. Pereira-Santos, R. B. C. Prudˆencio, and A. C. De Carvalho, Empirical investigation of active learning strategies, Neurocomputing, vol. 326327, pp. 1527, 2019. [22] M. E. Ramirez-Loaiza, M. Sharma, G. Kumar, and M. Bilgic, Active learning: An empirical study of common baselines, Data Mining and Knowledge Discovery, vol. 31, no. 2, pp. 287313, 2017. [23] A. Abraham and L. Dreyfus-Schmidt, Rebuilding Trust in Active Learning with Actionable Metrics, IEEE International Conference on Data Mining Workshops, ICDMW, vol. 2020-November, pp. 836843, 2020. [24] P.-Y. Lu, C.-L. Li, and H.-T. Lin, Re-Benchmarking Pool-Based Active Learning for Binary Classification, 2023. [25] A. L. Chandra, S. V. Desai, C. Devaguptapu, and V. N. Balasubramanian, On initial pools for deep active learning, in NeurIPS 2020 Workshop on Pre-Registration in Machine Learning, ser. Proceedings of Machine Learning Research, L. Bertinetto, J. F. Henriques, S. Albanie, M. Paganini, and G. Varol, Eds., vol. 148. PMLR, 2021, pp. 1432. [26] H. Trittenbach, A. Englhardt, and K. Bohm, An overview and benchmark of active learning for outlier detection with one-class classifiers, Expert Systems with Applications, vol. 168, p. 114372, 2021. [27] A. Abraham and L. Dreyfus-Schmidt, Sample Noise Impact on Active Learning, CEUR Workshop Proceedings, vol. 3079, pp. 8088, 2021. [28] Z. Pullar-Strecker, K. Dost, E. Frank, and J. Wicker, Hitting the target: Stopping active learning at the cost-based optimum, Machine Learning, pp. 119, 2022. [29] X. Zhan, Q. Wang, K.-h. Huang, H. Xiong, D. Dou, and A. B. Chan, Comparative Survey of Deep Active Learning, 2022. [30] A. I. Schein and L. H. Ungar, Active learning for logistic regression: An evaluation, Machine learning, vol. 68, no. 3, pp. 235265, 2007. [8] Y.-y. Logan, M. Prabhushankar, and G. AlRegib, DECAL: DEployable [31] M. Kelly, R. Longjohn, and K. Nottingham, The UCI Machine Learning Clinical Active Learning, 2022. Repository, 2017. ARXIV PREPRINT: SURVEY OF ACTIVE LEARNING HYPERPARAMETERS [32] B. Bischl, G. Casalicchio, M. Feurer, P. Gijsbers, F. Hutter, M. Lang, R. Gomes Mantovani, J. van Rijn, and J. Vanschoren, OpenML Benchthe Neural Information Processing marking Suites, Proceedings of Systems Track on Datasets and Benchmarks, vol. 1, 2021. [33] K. Konyushkova, R. Sznitman, and P. Fua, Learning active learning from data, in Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, I. Guyon, U. von Luxburg, S. Bengio, H. M. Wallach, R. Fergus, S. V. N. Vishwanathan, and R. Garnett, Eds., 2017, pp. 42254235. [34] J. Gonsior, C. Falkenberg, S. Magino, A. Reusch, C. Hartmann, M. Thiele, and W. Lehner, Comparing and Improving Active Learning Uncertainty Measures for Transformer Models by Discarding Outliers, Information Systems Frontiers, 2024. [35] J. Deng, W. Dong, R. Socher, L. Li, K. Li, and F. Li, Imagenet: largescale hierarchical image database, in 2009 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR 2009), 20-25 June 2009, Miami, Florida, USA. IEEE Computer Society, 2009, pp. 248255. [36] G. Krempl, D. Kottke, and M. Spiliopoulou, Probabilistic Active Learning: Towards Combining Versatility, Optimality and Efficiency, in Discovery Science, S. Dˇzeroski, P. Panov, D. Kocev, and L. Todorovski, Eds. Cham: Springer International Publishing, 2014, pp. 168179. [37] J. Gonsior, M. Thiele, and W. Lehner, ImitAL: Learned Active Learning Strategy on Synthetic Data, in Discovery Science - 25th International Conference, DS 2022, Montpellier, France, October 10-12, 2022, Proceedings, ser. Lecture Notes in Computer Science, P. Poncelet and D. Ienco, Eds., vol. 13601. Springer, 2022, pp. 4756. [38] T. Reitmaier and B. Sick, Let us know your decision: Pool-based active training of generative classifier with the selection strategy 4DS, Information Sciences, vol. 230, pp. 106131, 2013. [39] W. Cai, Y. Zhang, S. Zhou, W. Wang, C. Ding, and X. Gu, Active Learning for Support Vector Machines with Maximum Model Change, in Machine Learning and Knowledge Discovery in Databases, T. Calders, F. Esposito, E. Hullermeier, and R. Meo, Eds. Berlin, Heidelberg: Springer, 2014, pp. 211226. [40] K.-H. Huang and H.-T. Lin, Novel Uncertainty Sampling Algorithm for Cost-Sensitive Multiclass Active Learning, in 2016 IEEE 16th International Conference on Data Mining (ICDM), 2016, pp. 925930. [41] Y. Yang and M. Loog, To Actively Initialize Active Learning, Pattern Recognition, vol. 131, p. 108836, 2022. [42] F. Wilcoxon, Individual Comparisons by Ranking Methods, Biometrics Bulletin, vol. 1, no. 6, p. 80, 1945. [43] S. Huang, R. Jin, and Z. Zhou, Active learning by querying informative and representative examples, in Advances in Neural Information Processing Systems 23: 24th Annual Conference on Neural Information Processing Systems 2010. Proceedings of meeting held 6-9 December 2010, Vancouver, British Columbia, Canada, J. D. Lafferty, C. K. I. Williams, J. Shawe-Taylor, R. S. Zemel, and A. Culotta, Eds. Curran Associates, Inc., 2010, pp. 892900. [44] G. Krempl, D. Kottke, and V. Lemaire, Optimised probabilistic active learning (OPAL), Machine learning, vol. 100, no. 2, pp. 449476, 2015. [45] Y. Son and J. Lee, Active learning using transductive sparse Bayesian regression, Information Sciences, vol. 374, pp. 240254, 2016. [46] J. Demˇsar, Statistical Comparisons of Classifiers over Multiple Data Sets, J. Mach. Learn. Res., vol. 7, pp. 130, 2006. [47] Kaggle: Your Machine Learning and Data Science Community. [48] Y.-P. Tang, G.-X. Li, and S.-J. Huang, ALiPy: Active Learning in Python, 2019. [49] Y.-Y. Yang, S.-C. Lee, Y.-A. Chung, T.-E. Wu, S.-A. Chen, and H.-T. Lin, Libact: Pool-based Active Learning in Python, 2017. [50] Y. Yang, Google/active-learning, Google, 2024. [51] D. Kottke, M. Herde, T. P. Minh, A. Benz, P. Mergard, A. Roghman, C. Sandrock, and B. Sick, Scikit-activeml: Library and Toolbox for Active Learning Algorithms, 2021. [52] C. Schroder, L. Muller, A. Niekler, and M. Potthast, Small-text: Active learning for text classification in python, in Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics: System Demonstrations, D. Croce and L. Soldaini, Eds. Dubrovnik, Croatia: Association for Computational Linguistics, 2023, pp. 8495. [53] K. Margatina, G. Vernikos, L. Barrault, and N. Aletras, Active learning by acquiring contrastive examples, in Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, M.-F. Moens, X. Huang, L. Specia, and S. W.-t. Yih, Eds. Online and Punta Cana, Dominican Republic: Association for Computational Linguistics, 2021, pp. 650663. [54] O. Sener and S. Savarese, Active learning for convolutional neural networks: core-set approach, in 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net, 2018. [55] P. Donmez, J. G. Carbonell, and P. N. Bennett, Dual Strategy Active Learning, in Proceedings of the 18th European Conference on Machine Learning, ser. ECML 07. Berlin, Heidelberg: Springer-Verlag, 2007, pp. 116127. [56] H. T. Nguyen and A. W. M. Smeulders, Active learning using preclustering, in Machine Learning, Proceedings of the Twenty-first International Conference (ICML 2004), Banff, Alberta, Canada, July 48, 2004, ser. ACM International Conference Proceeding Series, C. E. Brodley, Ed., vol. 69. ACM, 2004. [57] M. Yuan, H.-T. Lin, and J. Boyd-Graber, Cold-start active learning through self-supervised language modeling, in Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), B. Webber, T. Cohn, Y. He, and Y. Liu, Eds. Online: Association for Computational Linguistics, 2020, pp. 79357948. [58] S. Ebert, M. Fritz, and B. Schiele, RALF: reinforced active learning formulation for object class recognition, in 2012 IEEE Conference on Computer Vision and Pattern Recognition, Providence, RI, USA, June 16-21, 2012. IEEE Computer Society, 2012, pp. 36263633. [59] M. Liu, W. Buntine, and G. Haffari, Learning how to actively learn: deep imitation learning approach, in Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), I. Gurevych and Y. Miyao, Eds. Melbourne, Australia: Association for Computational Linguistics, 2018, pp. 18741883. [60] H. S. Seung, M. Opper, and H. Sompolinsky, Query by committee, in Proceedings of the Fifth Annual Workshop on Computational Learning Theory, ser. COLT92. Pittsburgh Pennsylvania USA: ACM, 1992, pp. 287294. [61] N. Abe and H. Mamitsuka, Query Learning Strategies Using Boosting and Bagging. in Proceedings of the Fifteenth International Conference on Machine Learning (ICML 1998), Madison, Wisconsin, USA, July 2427, 1998, ser. ICML 1998. Morgan Kaufmann Publishers Inc., 1998, pp. 19. [62] R. Burbidge, J. J. Rowland, and R. D. King, Active Learning for Regression Based on Query by Committee, in Intelligent Data Engineering and Automated Learning - IDEAL 2007, H. Yin, P. Tino, E. Corchado, W. Byrne, and X. Yao, Eds. Berlin, Heidelberg: Springer Berlin Heidelberg, 2007, vol. 4881, pp. 209218. [63] A. Holub, P. Perona, and M. C. Burl, Entropy-based active learning for object recognition, in 2008 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops. Anchorage, AK, USA: IEEE, 2008, pp. 18. [64] D. D. Lewis and W. A. Gale, Sequential Algorithm for Training Text Classifiers. in SIGIR 1994. Springer London, 1994, pp. 312. [65] T. Scheffer, C. Decomain, and S. Wrobel, Mining the Web with active hidden Markov models, in Proceedings 2001 IEEE International Conference on Data Mining, ser. ICDM-01. San Jose, CA, USA: IEEE Comput. Soc, 2001, pp. 645646. [66] Tong Luo, K. Kramer, S. Samson, A. Remsen, D. Goldgof, L. Hall, and T. Hopkins, Active learning to recognize multiple types of plankton, in Proceedings of the 17th International Conference on Pattern Recognition, 2004. ICPR 2004. Cambridge, UK: IEEE, 2004, pp. 478481 Vol.3. [67] S. Karamcheti, R. Krishna, L. Fei-Fei, and C. Manning, Mind your outliers! investigating the negative impact of outliers on active learning for visual question answering, in Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), C. Zong, F. Xia, W. Li, and R. Navigli, Eds. Online: Association for Computational Linguistics, 2021, pp. 72657281. [68] K. Pearson and F. Galton, VII. Note on regression and inheritance in the case of two parents, Proceedings of the Royal Society of London, vol. 58, no. 347-352, pp. 240242, 1895. [69] A. H. Murphy, The Finley Affair: Signal Event in the History of Forecast Verification, Weather and Forecasting, vol. 11, no. 1, pp. 3 20, 1996. [70] M. G. Kendall, New Measure of Rank Correlation, Biometrika, vol. 30, no. 1/2, pp. 8193, 1938. [71] C. Spearman, The Proof and Measurement of Association between Two Things, The American Journal of Psychology, vol. 15, no. 1, pp. 72 101, 1904. [72] C. E. Shannon, Mathematical Theory of Communication, Bell System Technical Journal, vol. 27, no. 3, pp. 379423, 1948. ARXIV PREPRINT: SURVEY OF ACTIVE LEARNING HYPERPARAMETERS 20 [73] C. Schroder and A. Niekler, Survey of Active Learning for Text Classification using Deep Neural Networks, 2020. [74] M. Zhang and B. Plank, Cartography active learning, in Findings of the Association for Computational Linguistics: EMNLP 2021, M.-F. Moens, X. Huang, L. Specia, and S. W.-t. Yih, Eds. Punta Cana, Dominican Republic: Association for Computational Linguistics, 2021, pp. 395 406. [75] A. Kirsch, J. van Amersfoort, and Y. Gal, Batchbald: Efficient and diverse batch acquisition for deep bayesian active learning, in Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, H. M. Wallach, H. Larochelle, A. Beygelzimer, F. dAlche-Buc, E. B. Fox, and R. Garnett, Eds., 2019, pp. 70247035. [76] N. Houlsby, F. Huszar, Z. Ghahramani, and M. Lengyel, Bayesian Active Learning for Classification and Preference Learning, 2011. [77] A. Siddhant and Z. C. Lipton, Deep Bayesian active learning for natural language processing: Results of large-scale empirical study, in Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, E. Riloff, D. Chiang, J. Hockenmaier, and J. Tsujii, Eds. Brussels, Belgium: Association for Computational Linguistics, 2018, pp. 29042909. [78] C. Coleman, C. Yeh, S. Mussmann, B. Mirzasoleiman, P. Bailis, P. Liang, J. Leskovec, and M. Zaharia, Selection via proxy: Efficient data selection for deep learning, in 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020. [79] W. Hsu and H. Lin, Active learning by learning, in Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence, January 25-30, 2015, Austin, Texas, USA, B. Bonet and S. Koenig, Eds. AAAI Press, 2015, pp. 26592665. [80] K. Konyushkova, R. Sznitman, and P. Fua, Discovering GeneralPurpose Active Learning Strategies, 2019. [81] Z. Wang and J. Ye, Querying discriminative and representative samples for batch mode active learning, in The 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD 2013, Chicago, IL, USA, August 11-14, 2013, I. S. Dhillon, Y. Koren, R. Ghani, T. E. Senator, P. Bradley, R. Parekh, J. He, R. L. Grossman, and R. Uthurusamy, Eds. ACM, 2013, pp. 158166."
        }
    ],
    "affiliations": [
        "Hochschule fur Technik und Wirtschaft Dresden",
        "Technion - Israeli Institute of Technology",
        "Technische Universitat Dresden"
    ]
}