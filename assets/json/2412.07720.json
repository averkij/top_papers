{
    "paper_title": "ACDiT: Interpolating Autoregressive Conditional Modeling and Diffusion Transformer",
    "authors": [
        "Jinyi Hu",
        "Shengding Hu",
        "Yuxuan Song",
        "Yufei Huang",
        "Mingxuan Wang",
        "Hao Zhou",
        "Zhiyuan Liu",
        "Wei-Ying Ma",
        "Maosong Sun"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The recent surge of interest in comprehensive multimodal models has necessitated the unification of diverse modalities. However, the unification suffers from disparate methodologies. Continuous visual generation necessitates the full-sequence diffusion-based approach, despite its divergence from the autoregressive modeling in the text domain. We posit that autoregressive modeling, i.e., predicting the future based on past deterministic experience, remains crucial in developing both a visual generation model and a potential unified multimodal model. In this paper, we explore an interpolation between the autoregressive modeling and full-parameters diffusion to model visual information. At its core, we present ACDiT, an Autoregressive blockwise Conditional Diffusion Transformer, where the block size of diffusion, i.e., the size of autoregressive units, can be flexibly adjusted to interpolate between token-wise autoregression and full-sequence diffusion. ACDiT is easy to implement, as simple as creating a Skip-Causal Attention Mask (SCAM) during training. During inference, the process iterates between diffusion denoising and autoregressive decoding that can make full use of KV-Cache. We verify the effectiveness of ACDiT on image and video generation tasks. We also demonstrate that benefitted from autoregressive modeling, ACDiT can be seamlessly used in visual understanding tasks despite being trained on the diffusion objective. The analysis of the trade-off between autoregressive modeling and diffusion demonstrates the potential of ACDiT to be used in long-horizon visual generation tasks. These strengths make it promising as the backbone of future unified models."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 0 1 ] . [ 1 0 2 7 7 0 . 2 1 4 2 : r ACDIT: Interpolating Autoregressive Conditional Modeling and Diffusion Transformer Jinyi Hu1 Shengding Hu1* Yuxuan Song1 Yufei Huang1 Mingxuan Wang2 Hao Zhou1 Zhiyuan Liu1 Wei-Ying Ma1 Maosong Sun1 1Tsinghua University 2ByteDance {hu-jy21,hsd23}@mails.tsinghua.edu.cn"
        },
        {
            "title": "Abstract",
            "content": "The recent surge of interest in comprehensive multimodal models has necessitated the unification of diverse modalities. However, the unification suffers from disparate methodologies. Continuous visual generation necessitates the full-sequence-attend diffusion-based approach, despite its divergence from the autoregressive modeling in the text domain. In this paper, we explore an interpolation between autoregressive and modeling to model visual information. At its core, we present ACDIT, an Autoregressive blockwise Conditional Diffusion Transformer, where the block size of diffusion, i.e., the size of autoregressive units, can be flexibly adjusted to interpolate between token-wise autoregression and full-sequence diffusion denoising. ACDIT is easy to implement, as simple as creating Skip-Causal Attention Mask (SCAM) during training. During inference, the process iterates between diffusion denoising and autoregressive decoding that can make full use of KV-Cache. We show that ACDIT performs best among all autoregressive baselines under similar model scales on image and video generation tasks. We also demonstrate that benefiting from autoregressive modeling, pretrained ACDIT can be transferred in visual understanding tasks despite being trained with the diffusion objective. The analysis of the trade-off between autoregressive modeling and diffusion demonstrates the potential of ACDIT to be used in long-horizon visual generation tasks. These strengths make it promising as the backbone of future unified models. 1. Introduction The concept of predicting the future has been fundamental principle of Artificial Intelligence, ranging from the next token prediction objective of autoregressive language modeling [1, 6, 52] to predicting the next action *Equal contribution Corresponding authors Figure 1. The generation process of ACDIT. Pixels in each block are denoised simultaneously conditioned on clean context which are autoregressively generated. The block can be flexibly defined, such as part of pixels in images or several frames in videos. in Reinforcement Learning [12, 46, 53, 59]. The recent renascence of the concept World Model also builds upon forming an internal state that is both capable of predicting the future and influenced by the prediction of the future [23]. The success of Large Language Models (LLMs) [1, 2, 6, 32, 67, 68, 78] also exemplify the power of such philosophy, demonstrating that complex abilities can arise from this simple objective. However, the multimodal aspects of information beyond language have yet to fully capitalize on this paradigm. In the realm of visual generation, diffusion models [14, 28, 60, 61] have demonstrated superior generative capabilities, producing creative outputs that are virtually indistinguishable from human-generated content, as evidenced by innovations like Sora [5] and Stable Diffusion [18, 50, 54]. Notwithstanding their remarkable achievements, these models operate in non-autoregressive manner. Diffusion models receive corrupted target sequences with complete length and reconstruct the intended output through in-place iterative refinement. The term in-place underscores the distinct nature of this approach compared to the autoregressive model, which provides subsequent prediction, thereby extending the sequence and progressing towards the future. Such variation 1 makes the model struggle to learn temporal correlation in vision data and poses difficulties for developing integrated frameworks capable of seamlessly bridging the vision foundation model with unified multimodality modeling [15, 65, 72, 76, 77, 82] and world model [7, 17, 34, 74, 79, 83]. Existing works attempt to unify modalities by converting multimodal generation tasks into discrete token prediction tasks with vector quantization techniques [19, 65, 66, 72] and training on the mixed sequences with next-token prediction objective. However, approaching the continuous distribution requires huge vocabulary sizes and high utilization rate [73, 81], which is complex objective. The information loss during vector quantization poses challenges for visual understanding tasks that require detailed information, such as the Optical Character Recognition task. As first step for exploring unified framework, we propose ACDIT, an Autoregressive blockwise Conditional Diffusion Transformer that fuses the diffusion process with the autoregressive paradigm. At high level, we extend the autoregressive units from the individual text token to blocks. The generation of each block can be formulated as conditional diffusion process based on the previous block, where each block consists of visual patches of flexible size. As Fig. 1 shows, for image generation, block can be defined as small part of the image, while for video, block can be frame or multiple frames. ACDIT is easy to implement, as simple as adding Skip-Causal Attention Mask to the current DiT architecture during training. The inference process is formatted as an iteration between the conditional diffusion denoising process within block, conditioned on the complete clean context, and autoregressive generation of new block appended as the new context. In this way, KV-Cache can be used for faster inference. In general, ACDIT offers the following inherent advantages: (i) ACDIT simultaneously learns the causal interdependence across blocks with autoregressive modeling and the non-causal dependence within blocks with diffusion modeling. This serves as versatile framework for expanding into unified multimodal and world models without conflict. (ii) ACDIT is endowed with clean continuous visual input, which can benefit visual understanding tasks in multimodal models. (iii) ACDIT makes full use of KV-Cache for flexible autoregressive generation in any length and potentially other latest long-context techniques in text for long video generation. We validate ACDIT in both image and video generation tasks on ImageNet [57] and UCF101 [62], respectively. Experimental results demonstrate that ACDIT outperforms all autoregressive baselines of the same model scale and achieves visual quality comparable to full-sequence diffusion models, while exhibiting significantly higher inference speed when extended to long sequences. The codes and models are available at https://github.com/thunlp/ACDiT. 2. Related Work Diffusion Models. The field of image generation has witnessed remarkable advancements with the introduction of diffusion models [14, 28, 47, 60]. U-Net [55] is the early mainstream choice of network architecture [47, 50, 54, 61]. Following that, Transformer [70] is applied to diffusion models for image generation, with groundbreaking work such as DiT [49] and U-ViT [3] marking significant milestones. series work, including PixArt-{α, δ, Σ} [911], demonstrate the capability of DiT on text-to-image tasks. Additionally, several studies have applied DiT to video generation, such as Lumiere [4] and Movie Gen [51]. Autoregressive Visual Generation. Autoregressive models have shown promising results in visual generation [13, 19, 39, 40, 66, 72]. The iGPT [13] first proposes autoregressively generating raw image pixels as raster-scan sequence. VQGAN [19] improves the performance by training an autoregressive transformer on discrete tokens produced by VQVAE [69]. LlamaGen [64] enhances the image tokenizer and scales up the autoregressive transformers to 3.1B parameters, building on the latest Llama architecture [67, 68]. Also, several works demonstrate the potential of token-base visual generation in text-to-image tasks [41, 64, 72, 80]. Inspired by RQTransformer [36], VAR [66] proposes the next-scale prediction and obtain good improvement. In video generation, some works [29, 56] utilize sliding windows for progressive generation. Diffusion Forcing [8] and MAR [39] are two highly related works. Diffusion Forcing trains causal autoregressive model to generate blocks without fully diffusing past ones and implement it on small RNN. MAR proposes the diffusion loss to learn the autoregressive conditional distribution on the head of the main Transformer with small MLP network. Differently, ACDIT generates each block based on clear past and utilizes the full parameters of Transformer to denoise each block. Unified Model for Understanding and Generation. Unified models for visual understanding and generation have recently garnered widespread attention. Some early efforts have aimed to align both the visual encoder and visual decoder with pre-trained Large Language Models [15, 75]. Some works utilize the discrete visual token to unify the image understanding and generation tasks, such as VILAU [76], EMU3 [72], and Show-o [77]. Transfusion [82] tries the first attempt of joint training with language modeling loss and diffusion loss in single transformer. World Model. World models obtain great attention in various domains. Genie [7] introduces interactive game video simulation by learning latent actions on unlabeled video. Unisim [79] builds real-world interaction simulaiVideoGPT [74] train an autions with web video data. toregressive transformer on mixed sequence of tokens organized with visual observations, actions, and rewards. De2 spite the success, world models face challenges in leveraging the powerful generative capabilities of diffusion models due to their non-autoregressive nature. 3. Prerequiste 3.1. Autoregressive Modeling Autoregression asserts that the value at each timestep is contingent upon its preceding values. This principle is exemplified in autoregressive language models, which iteratively predict the probability distribution of subsequent tokens. Given sequence of tokens (x1, x2, . . . , xn), salient characteristic of autoregression is that the prediction of xi is only dependent on its prefix (x1, x2, . . . , xi1). Upon determining ci, it is concatenated with the preceding sequence, thereby forming the conditioning context (x1, x2, . . . , xi) for predicting xi+1. Therefore, the likelihood of sequence can be factorized as: p(x1, x2, . . . , xn) = (cid:89) i=1 p(xi+1x1, x2, . . . , xi). (1) Thanks to the flexibility of self-attention in Transformers, autoregressive models can be effectively implemented by adding causal mask in Transformer attention block [70]. 3.2. Diffusion α(t)n(0), (1 α(t))I)1, which is equivalent Diffusion models, in contrast, conceptualize noiseinfusion and denoising process, which is defined by gradually adding noise to the initial data n0 and training the model to learn the inverse mapping. Formally, the noised data n(t) at each step is sampled by q(n(t)n(t1)) = (xt; to add Guassain noise to the previous samples: n(t) = 1 α(t)ϵ(t), ϵ(t) (0, I) while the diffusion model pθ is trained to learn the reverse process pθ(n(t1)n(t)) = (µθ(n(t)), β(t)I). With the reparameterization trick, the network µθ(n(t)) can be reparameterized as noise prediction network ϵθ(n(t)) and the training objective can be simple as: α(t)n(t1) + Lθ = EtU [0,1],ϵN (0,I)ϵθ(n(t), t) ϵ(t)2. (2) During the inference phase, the denoising process is initialized with random Gaussian noise sample n(T ), followed by denoising steps, ultimately yielding single deterministic samples n(0) from its underlying distribution. Typically, the denoising process in diffusion models operates in-place, meaning that each new denoising step directly replaces the previous steps input. This differs from autoregressive modeling, where the value of subsequent step is appended to the existing sequence. 1For clarification, we use subscript to denote timesteps in autoregressive models and superscript (t) to denote the timesteps in diffusion models. 3 3.3. Desiderata robust autoregressive diffusion method should integrate the strengths of both autoregressive modeling and diffusion. To achieve this synergy, we have identified three critical desiderata that the framework must meet: 1. The generation of future elements should be predicated on precise representation of antecedent sequences. This is imperative because any ambiguity in the past inevitably complicates future predictions. This approach preserves the efficacy of autoregressive modeling and potentially facilitates the development of an internal world model. Furthermore, adherence to this principle enhances performance in discriminative tasks (e.g., visual understanding), as these tasks necessitate the input of all observable features into the model. 2. Both the autoregressive modeling and the denoising process should optimally utilize the entire parameter space of the neural network. In an elegant fusion of autoregressive models and diffusion, neither component should be relegated to an auxiliary role. Instead, they should function as integral, complementary elements of the system. 3. The denoising process should directly attend comprehensively to the entire sequence of past sequences. Failure to do so would necessitate that the denoise processs condition encapsulates all preceding information, placing an unrealistic demand on the lossless compression capabilities of the feature space. On the contrary, holistic input of all past information in each denoise step ensures more effective processing of temporal dependencies. Based on these desiderata, we analyze representative existing autoregressive diffusion methods. DiffusionForcing [8] proposes to use different-level random noise in the different positions of sequence. Thus, in the inference process, denoising the subsequent positions from clean past and be seen as special case denoising different levels of noise. Their method does not meet the first desideratum. In the training process, the future is not predicted from the precise representation of the past. In MAR [39], the diffusion process is trained based on the latent in the last position, which does not satisfy the third desideratum. Moreover, the diffusion process sorely leverages the head part of the network, which conflicts with the second desideratum. In Transfusion [82], despite that the diffusion utilizes the full network parameters for both autoregressive modeling and diffusion, it does not utilize the predicting the future objective within the multimodal information. Moreover, when trained with multiple blocks of multimodal information, such as multiple images, the latter image will attend to the noise version of the former image. To comprise this deficiency, they use half of the noise schedule, i.e., limiting maximum noise steps to 500 instead of 1000, in 20% image captioning pairs, which is not an optimal strategy. (a) Skip-Causal Attention Mask. (b) Inference process of ACDIT. (c) 3D view of ACDIT. Figure 2. (a): for each noised block ni, it can only attend previous clean latent c0, c1, . . . , ci1 and itself. Each clean block ci only attends the previous clean latent block. (b): ACDIT can effectively utilize the KC-Cache for autoregressive inference. (c): the 3D view of ACDIT, where B, L, denotes the block size, number of blocks, and denoising time steps, respectively. Darker color denotes higher noises. 0, ..., 1} and itself, while ci also attends to all preceding clean blocks {cjj = 0, ..., 1} and itself. In training, for both simplicity and efficiency, we can group the attention mechanisms as illustrated in the right matrix of Figure 2a. Suppose the number of blocks is , then the unmasked positions form two triangular matrices of side length 1, complemented by diagonal matrix of side length . During the inference phase, each autoregressive step executes conditional diffusion process for ni based on {cjj = 0, ..., 1}s KV-Cache. Upon finishing denoising, it is appended to the clean sequence as ci followed by the maximal noise version of the next block ni+1. The keyvalue tensor will be computed for these two blocks, and the key-value tensor of the clean block ci will be kept in KVCache. All noise-corrupted version of ni is disregarded. The process is visualized in Figure 8. three-dimensional view of our method is presented in Figure 2c. By interpolating between full-sequence diffusion and autoregressive paradigms, ACDIT enjoys flexibility and expressivity, enabling it to generate video of any length utilizing the latest long-context techniques developed for language models. 4.2. Positional Encoding ACDIT is designed to be versatile, capable of handling one-, two-, three-, or even higher-dimensional data, including but not limited to text (1D), images (2D), and video (3D). For any given dimension of data, the position of that data is critical attribute that must be made known to the model. This positional awareness enables the model to contextualize its current focus relative to historical data. In the domain of textual data, the Rotary Position Embedding (RoPE) [63] has gained widespread adoption as an effective relative positional encoding method. To address the challenges posed by multi-dimensional positional indices, we introduce RoPE-ND, natural extension of RoPE. For token of dimensional data, its positional index is [m1, m2, ..., mD]. Given query and key vectors in Transformers attention module, we partition the hidden dimension into segments. It is imperative that each segments Figure 3. Left: an illustrative figure of the ideal dependencies between the noise and clean information, which is adopted by ACDIT. Right: the dependencies of diffusion head in MAR. 4. ACDIT 4.1. Framework To satisfy the desiderata discussed above, we propose versatile framework for autoregressive diffusion called ACDIT. For generality, ACDIT runs block-wise autoregression instead of token-wise autoregression. We identify there are two kinds of blocks: the clean blocks ci and the noise blocks ni, where ni is corrupted from ci. ACDIT learns several conditional distribution p(cic<i) factorized with Eq. 1, where p(cic<i) is optimized by learning the conditional noise prediction network ϵθ(n(t) ; t, c<i) with Eq. 2. The final training objective is: Lθ = EtU [0,1],ϵN (0,I) (cid:88) i=1 ϵθ(n(t) ; t, c<i)ϵ(t)2. (3) Given the aforementioned desiderata, we can conceptualize all ni and all ci as occupying separate positions, effectively transforming the dependency structure into an attention pattern between different positions. We designate this attention pattern as the Skip-Causal Attention Mask (SCAM), which is visually represented in Figure 2a. The figure elucidates that ni attends to all preceding clean blocks {cjj = 4 hidden dimension be an even number. For each segment j, we apply RoPE with specific base bj, as defined in the following equation: = Rd1 Θ1,m1 0 ... 0 Θ2,m"
        },
        {
            "title": "0\nRd2\n...\n0",
            "content": "0 0 ... . . . RdD ΘD,mD (4) Θj ,mj , [1, 2, , dj In this formulation, each Rdj represents djdimensional rotary matrix2 with rotation angles Θj = {θji = b2(i1)/dj 2 ]}. The base bj is emj pirically determined as 100 8Lj 100π , where Lj denotes the maximum position index in data dimension j. This formulation ensures that the highest wavelength of RoPE is approximately eight times the maximum position, thereby mitigating rapid decay in long-term dependencies. It is worth noting that ACDIT inherently supports length extrapolation [63], although comprehensive exploration of this capability falls beyond the scope of the present work. 4.3. Efficiency Analysis and Block Size Choice In this section, we provide brief analysis on the computational efficiency of ACDIT in terms of floating-point operations (FLOPS). We first assume that denoising each block requires the same number of time steps as the full sequence diffusion, despite that when the block size is small, it may require fewer denoising steps, thus making ACDIT potentially more efficient. Let θ denote the number of parameters in one layer of the transformer block, represents the hidden dimension and as the number of attention heads. The FLOPS is formed by O(L) component 2Lθ, from the product between input and weight matrices, and O(L2) component from the pairwise computation between Q-K pairs in the attention dot product, which is 4(h + n) for each Q-K pair in the L2 Q-K pairs. Consequently, given that h, the total FLOPS can be expressed as: = 2Lθ + 4(h + n)L2 2Lθ + 4hL2. When employing ACDIT with KV-Cache, the FLOPS consumed by FFN and attention parameters remain unchanged. However, the attention mechanism transforms into causal attention with block size B. In this scenario, the number of Q-K pairs to be calculated is given by: L/B (cid:88) i=1 iB2 = 1 2 ( + L2 B2 )B2. (5) Thus, the FLOPS saved can be expressed as 4hL2( 1 2 (1 )). Compared to the total FLOPS, the saved percentage is 2For detailed explanation, please refer to Equation 15 in version 5 of the arXiv preprint of [63]. Figure 4. The model architecture of ACDIT. Both the clean latent and noise latent are input while only noise tokens are scale shift with conditioning information. 4hL2( 1 2 2L ) 4hL2 + 2Lθ = 1 B/L 2 + m/k , (6) h2 and = where = θ . This equation demonstrates that when the sequence length significantly exceeds the hidden size, transforming full sequence diffusion into ACDIT can save up to 50% of FLOPS. However, in practice, it is not always beneficial for small B. Setting to an excessively small value may not fully leverage the iterative modification inherent in the diffusion process, potentially compromising generation quality. Furthermore, given the parallel computing nature of computational kernels, very small may not yield speed improvements, phenomenon analogous to the rationale behind speculative decoding [37]. Conversely, setting to very large value diminishes efficiency both in terms of attention calculation, as shown in Equation 6. It also fails to capitalize on the strengths of auto-regressive generation. Indeed, when is set equal to L, ACDIT reverts to the original DiT model. We analyze the influence of block size on performance and efficiency in Sec. 5.3. 4.4. Model Architecture ACDIT mainly inherits the main architecture of DiT. However, since we want to keep the architecture as simple and unified as possible, we use linear layers instead of convolution in the input layer and final layer. Besides, we replace the absolute position embedding and Layer Normalization with RoPE [63] and RMSNorm (Root Mean Square Layer Normalization) [67], respectively. We find that QK-norm is 5 Table 1. Image generation results on ImageNet 256x256. Model Type Latent KV-Cache Params FID IS Pre Rec ADM [14] (NeuIPS19) LDM-4-G [54] (CVPR22) DiT-XL/2 [49] (ICCV23) MaskGIT [73] (CVPR22) MAGE [38] (CVPR23) VQGAN [19] (CVPR21) RQTran [36] (CVPR22) VAR-d16 [66] (NeurIPS24) VAR-d20 [66] (NeurIPS24) LlamaGen-L [64] (arxiv24) LlamaGen-XL [64] (arxiv24) LlamaGen-XXL [64] (arxiv24) ImageFolder [40] (arxiv24) MAR-L [66] (NeurIPS24) MAR-L [66] (NeurIPS24) ACDIT-L ACDIT-XL ACDIT-H Diff. Diff. Diff. Mask. Mask."
        },
        {
            "title": "AR\nAR\nVAR\nVAR\nAR\nAR\nAR\nAR",
            "content": "AR MAR AR+Diff AR+Diff AR+Diff - Cont. Cont. Disc. Disc. Disc. Disc. Disc. Disc. Disc. Disc. Disc. Disc. Cont. Cont. Cont. Cont. Cont. - - - - - - 554M 10.94 3.60 400M 2.27 675M 227M 230M 1.4B 3.8B 310M 600M 343M 775M 1.4B 362M 479M 479M 460M 677M 954M 6.18 6.93 15.78 7.55 3.30 2.57 3.07 2.62 2.34 2.60 4.07 1. 2.53 2.45 2.37 101.0 247.7 278.2 316.2 195.8 78.3 134.0 274.4 302.6 256.1 244.1 253.9 295.0 232.4 296.0 262.9 267.4 273. 0.69 - 0.83 0.83 - - - 0.84 0.83 0.83 0.80 0.80 0.75 - 0.81 0.82 0.82 0.82 0.63 - 0. 0.58 - - - 0.51 0.56 0.52 0.57 0.59 0.63 - 0.60 0.55 0.57 0.57 Table 2. Configuration details of ACDIT. Model #Layers Hidden Size MLP #Heads Params ACDIT-B ACDIT-L ACDIT-XL ACDIT-H 12 24 28 32 768 1024 1152 1280 3072 4096 4608 5120 12 16 18 132M 460M 677M 954M important to stabilizing the video generation training, thus we use QK-norms in all experiments. The additional conditional information timesteps and labels are injected into the model with adaLN-Zero only on the noise part. For both image and video generation, we follow DiT and leverage the pre-trained image VAE [33] from Stable Diffusion [54], whose downsample factor is 8. For image generation under block size, we group square latent representation patches with shape as block. 5. Experiments 5.1. Experimental Setup Dataset. For image generation tasks, we consider the ImageNet [57] dataset with 256256 resolution, which consists of around 1.28M images from 1K classes. For video generation, we consider the UCF-101 [62] dataset with 16 frames, where each frame is an image with 256256 resolution. UCF-101 contains 13320 videos from 101 classes. Implementation details. In the image generation task, we set the patch size as 1 and the autoregressive unit block size as 256 = 1616. Therefore, for 2562563 image in 32324 latent shape, the total sequence length and autoregressive length are 1024 and 4, respectively. We explore 4 different model sizes, as shown in Table 2. ACDiT-B is used for design verification and analyze. ACDIT is trained on ImageNet for 1.2M iterations with batch size of 1024. We use the AdamW optimizer [42] and WSD (Warmup Steady Decay) learning rate scheduling [31] with the peak learning rate 3e-4 and no weight decay. The learning rate begins to decay in the last 15% training iteration. Following the common training recipe of generative models, we keep an exponential moving average (EMA) of the ACDIT weights during training using decay rate of 0.9999. We sample images with DPM-Solver [43] for 25 steps within each block and use classifier-free guidance [27] with guidance scale of 1.5. In video generation, we sample 16 frames from each video and set the patch size as 2 and the block size as 1024 = 256 4. For 16 256 256 3 video in 16 32 32 4 latent shape, the sequence length of each frame is 256 and the total sequence length is 4096, with 4 frames grouped into one block. We train ACDIT on UCF-101 for 400K iterations with batch size of 96. The classifier-free guidance scale is 2.5. Other training configs are the same as image training. All models are implemented with PyTorch [48] and trained on NVIDIA H100 GPUs. Specifically, we use FlexAttention3 to implement the SCAM for both customization and efficiency. 3https://pytorch.org/blog/flexattention 6 Figure 5. Sample images from ACDIT-H on ImageNet 256256. Figure 6. Sample videos from ACDIT-XL trained on UCF-101. Table 3. Video generation results on the UCF-101 dataset. ACDIT-XL-LT means ACDIT-XL trained for longer epoch. Model Type Params FVD LVDM [25] (arxiv22) Latte [45] (arxiv24) Matten [21] (arxiv24) VideoFusion [44] (CVPR23) MMVG [20] (CVPR23) MAGVITv2 [81] (ICLR24) TATS [22] (ECCV22) CogVideo [30] (ICLR23) MAGVITv2-AR [81] (ICLR24) OmniTokenizer [71] (NeurIPS24) Diff. Diff. Diff. Diff. Mask. Mask. AR AR AR AR ACDIT-XL ACDIT-H ACDIT-H-LT AR+Diff. AR+Diff. AR+Diff. 437M 372 674M 478 853M 211 510M 173 230M 328 58 307M 331M 332 9.4B 626 307M 109 650M 677M 111 954M 104 90 954M 5.2. Main Results Image Generation. We report the FID-50K [26], Inception Score [58], Precision and Recall [35] of ACDIT and baselines in Table 1. Compared with previous autoregressive models and masked generative models utilizing discrete tokens, such as VQGAN, VAR, LlamaGen, and MaskGIT, ACDIT consistently achieves superior performance with lower FID scores at comparable model scales. Notably, ACDIT-XL achieves 2.45 FID scores, outperforming both LlamaGen-XXL and VAR-d20 with similar parameters. Additionally, when compared to the MAR-L variant that does not recompute attention, ACDIT-L significantly improves performance across all metrics. Although MAR-L has lower FID than ACDIT, recomputing attention makes it hard to generalize longer sequence generation. When compared with leading diffusion-based methods, ACDIT also demonstrates competitive performance. For instance, despite not employing full-sequence attention, ACDIT models achieve results close to DiT-XL. In general, these results highlight the distinct advantages of ACDIT over other baselines with the continuous latent representation and KVCache. Qualitative results are presented in Fig. 5. Video Generation. Different from image generation, video inherently includes temporal dimension, making it more well-suited to autoregressive modeling. The FVD metric on UCF-101 for class-conditional video generation is reported in Table 3. With hybrid AR+Diff architecture, ACDIT-H achieves much lower FVD than other diffusion-based and autoregressive methods, even outperforming MAGVITv2-AR, which utilizes closed-source, In contrast, ACDIT specially designed video tokenizer. Figure 7. FID and FVD curves of ACDIT-B over training steps with different sequence lengths and autoregressive lengths. PS means patch size. Figure 8. The change of inference time in different autoregressive lengths under varied total sequence length. (a) Scaling performance of ACDIT. (b) Ablation study for ROPE-ND. (c) FID curve of last 30% training. Figure 9. (a): ACDIT shows scaling performance similar to DiT. (b): ROPE-ND has consistent improvement to the generation quality. (c): FID score sharply with the learning rate beginning to decay when using the WSD scheduler. Table 4. Supervised fine-tuned Top-1 accuracy on Imagenet. We finetune ACDIT-XL and DiT-XL for 100 epoch. Model Type Top-1 Acc ViT-H[16] MAGE[38] MAE[24] Supervised Masked. Masked. iGPT [13] Generative DiT-XL [49] Generative Generative ACDIT-XL 83.1 84.3 85.9 72.6 82.8 84.0 simplifies the process by directly using an open-sourced image VAE. Although MAGVITv2 with masked generative methods has lower than ACDIT, they rely on in-place operation to generate video similar to the diffusion model. This constraint limits their ability to generalize to generate longer video generation and build world models. Compared to image generation, ACDIT demonstrates greater potential in modeling long visual sequences. Qualitative results of ACDIT-XL are presented in Fig. 6. Image Representation. We also assess the capability of ACDIT in image representation, which is essential for building unified visual understanding and generation model. We finetune ACDIT-XL and DiT-XL on ImageNet using classification loss following the training setting in MAE [24] and report the Top-1 accuracy in Table 4. The accuracy of ACDIT surpasses that of ViT-H, iGPT and DiT-XL. This superior performance over DiT-XL highlights the benefit of incorporating clean latent inputs, which accelerates the models ability to learn better representations compared to using only noised latent inputs. Furthermore, ACDIT is on par with MAGE in terms of Top-1 accuracy, while enjoying better generation capabilities than MAGE. 5.3. Analysis Trade off of block size. Fig. 7 illustrates the trend of trade-off under different sequence lengths and block sizes in image and video generation tasks on ACDIT-B. The FID curve indicates that for image generation, directly increasing the autoregressive length leads to decline in image quality, since each patch receives less attention information on average. However, we can mitigate this decline by increasing the total sequence length, which means reducing the patch size. For video generation, ACDIT shows more advantages due to the inherent temporal dependence of videos. The FVD curve demonstrates that increasing autoregressive length has minimal effect on the video quality, even with slight improvement. As for efficiency, we test the sampling time for various sequence lengths with batch size of 4 on an NVIDIA A100 GPU. Fig. 8 shows that as the sequence length increases, particularly beyond 16k, full-sequence attention (AR length of 1) becomes very timeconsuming, necessitating the autoregressive generation. Scaling Performance. We present the scaling performance of ACDIT in Fig. 9a. For fair comparison with DiT, we use the same batch size and learning rate as DiT in these training sessions. When increasing the model size, ACDIT shows consistent improvement in image quality 8 across all autoregressive lengths, sharing similar scaling trend with DiT. Notably, the improvement is more pronounced with longer autoregressive lengths. We hypothesize that this is due to reduced accumulation of errors when scaling the model size. Ablation Study. We ablate the effectiveness of ROPEND positional embedding on image generation with patch size as 2. As shown in Fig 9b, adding ROPE-ND results in consistent improvements in FID score. Training dynamics of WSD scheduler. Unlike the constant learning rate used in DiT, we utilize the WSD learning rate scheduler [31]. WSD scheduler maintains constant learning rate as the main stage of training, while one can diverge from the main branch at any time, potentially based on the compute budget, with rapidly decaying learning rate. As Fig. 9c shows, the FID remains almost converged during the constant learning rate state, while sharply dropping after the learning decays, similar to the loss curve when using the WSD scheduler in LLM training. To the best of our knowledge, we are the first to validate the effectiveness of the WSD scheduler in visual generation. 6. Conclusion In this paper, we propose ACDIT that interpolates the autoregressive modeling and diffusion transformers. With simple but novel design of attention mask, ACDIT can achieve autoregressive generation on any length while maintaining clear latent input potentially for adding visual understanding task. We demonstrate the performance and efficiency of ACDIT in image and video generation tasks while endowing sufficient generalization by combining the advantages of both autoregressive and diffusion models. We hope ACDIT can shed light on the architectural design of building unified multimodal model and world model in the future."
        },
        {
            "title": "References",
            "content": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. 1 [2] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei arXiv preprint Huang, et al. Qwen technical report. arXiv:2309.16609, 2023. 1 [3] Fan Bao, Shen Nie, Kaiwen Xue, Yue Cao, Chongxuan Li, Hang Su, and Jun Zhu. All are worth words: vit backbone for diffusion models. In CVPR, 2023. 2 [4] Omer Bar-Tal, Hila Chefer, Omer Tov, Charles Herrmann, Roni Paiss, Shiran Zada, Ariel Ephrat, Junhwa Hur, Yuanzhen Li, Tomer Michaeli, et al. Lumiere: spacetime diffusion model for video generation. arXiv preprint arXiv:2401.12945, 2024. 2 [5] Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, Clarence Ng, Ricky Wang, and Aditya Ramesh. Video generation models as world simulators. 2024. 1 [6] Tom Brown. Language models are few-shot learners. arXiv preprint arXiv:2005.14165, 2020. 1 [7] Jake Bruce, Michael Dennis, Ashley Edwards, Jack Parker-Holder, Yuge Shi, Edward Hughes, Matthew Lai, Aditi Mavalankar, Richie Steigerwald, Chris Apps, et al. Genie: Generative interactive environments. In Forty-first International Conference on Machine Learning, 2024. 2 [8] Boyuan Chen, Diego Marti Monso, Yilun Du, Max Simchowitz, Russ Tedrake, and Vincent Sitzmann. Diffusion forcing: Next-token prediction meets full-sequence diffusion. arXiv preprint arXiv:2407.01392, 2024. 2, 3 [9] Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, and Zhenguo Li. Pixart-α: Fast training of diffusion transformer for photorealistic text-to-image synthesis, 2023. 2 [10] Junsong Chen, Chongjian Ge, Enze Xie, Yue Wu, Lewei Yao, Xiaozhe Ren, Zhongdao Wang, Ping Luo, Huchuan Lu, and Zhenguo Li. Pixart-sigma: Weak-to-strong training of diffusion transformer for 4k text-to-image generation. arXiv preprint arXiv:2403.04692, 2024. [11] Junsong Chen, Yue Wu, Simian Luo, Enze Xie, Sayak Paul, Ping Luo, Hang Zhao, and Zhenguo Li. Pixart-{delta}: Fast and controllable image generation with latent consistency models. arXiv preprint arXiv:2401.05252, 2024. 2 [12] Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Misha Laskin, Pieter Abbeel, Aravind Srinivas, and Igor Mordatch. Decision transformer: Reinforcement learning via sequence modeling. Advances in neural information processing systems, 34:1508415097, 2021. [13] Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, and Ilya Sutskever. Generative pretraining from pixels. In International conference on machine learning, pages 16911703. PMLR, 2020. 2, 8 [14] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in neural information processing systems, 34:87808794, 2021. 1, 2, 6 [15] Runpei Dong, Chunrui Han, Yuang Peng, Zekun Qi, Zheng Ge, Jinrong Yang, Liang Zhao, Jianjian Sun, Hongyu Zhou, Haoran Wei, Xiangwen Kong, Xiangyu Zhang, Kaisheng Ma, and Li Yi. DreamLLM: Synergistic multimodal comprehension and creation. In The Twelfth International Conference on Learning Representations, 2024. 2 [16] Alexey Dosovitskiy. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020. 8 [17] Yilun Du, Mengjiao Yang, Pete Florence, Fei Xia, Ayzaan Wahid, Brian Ichter, Pierre Sermanet, Tianhe Yu, Pieter Abbeel, Joshua Tenenbaum, et al. Video language planning. arXiv preprint arXiv:2310.10625, 2023. 2 [18] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first International Conference on Machine Learning. 1 Casas, Florian Bressand, Gianna Lengyel, Guillaume LamarXiv preprint ple, Lucile Saulnier, et al. Mistral 7b. arXiv:2310.06825, 2023. 1 [33] Diederik Kingma. Auto-encoding variational bayes. arXiv [19] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming In Protransformers for high-resolution image synthesis. ceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1287312883, 2021. 2, 6 [20] Tsu-Jui Fu, Licheng Yu, Ning Zhang, Cheng-Yang Fu, JongChyi Su, William Yang Wang, and Sean Bell. Tell me what happened: Unifying text-guided video completion via In Proceedings of multimodal masked video generation. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1068110692, 2023. 7 [21] Yu Gao, Jiancheng Huang, Xiaopeng Sun, Zequn Jie, Yujie Zhong, and Lin Ma. Matten: Video generation with mambaattention. arXiv preprint arXiv:2405.03025, 2024. 7 [22] Songwei Ge, Thomas Hayes, Harry Yang, Xi Yin, Guan Pang, David Jacobs, Jia-Bin Huang, and Devi Parikh. Long video generation with time-agnostic vqgan and timesensitive transformer. In European Conference on Computer Vision, pages 102118. Springer, 2022. 7 [23] David Ha and Jurgen Schmidhuber. World models. arXiv preprint arXiv:1803.10122, 2018. 1 [24] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollar, and Ross Girshick. Masked autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 16000 16009, 2022. 8 [25] Yingqing He, Tianyu Yang, Yong Zhang, Ying Shan, and Qifeng Chen. Latent video diffusion models for high-fidelity long video generation. 2022. 7 [26] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in neural information processing systems, 30, 2017. 7 [27] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. 6 [28] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. 1, 2 [29] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David Fleet. Video diffusion models. Advances in Neural Information Processing Systems, 35:86338646, 2022. [30] Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, and Jie Tang. Cogvideo: Large-scale pretraining for text-to-video generation via transformers. arXiv preprint arXiv:2205.15868, 2022. 7 [31] Shengding Hu, Yuge Tu, Xu Han, Chaoqun He, Ganqu Cui, Xiang Long, Zhi Zheng, Yewei Fang, Yuxiang Huang, Weilin Zhao, et al. Minicpm: Unveiling the potential of small arXiv language models with scalable training strategies. preprint arXiv:2404.06395, 2024. 6, 9 [32] Albert Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las preprint arXiv:1312.6114, 2013. 6 [34] Po-Chen Ko, Jiayuan Mao, Yilun Du, Shao-Hua Sun, and Learning to act from actionless arXiv preprint Joshua Tenenbaum. videos through dense correspondences. arXiv:2310.08576, 2023. [35] Tuomas Kynkaanniemi, Tero Karras, Samuli Laine, Jaakko Lehtinen, and Timo Aila. Improved precision and recall metric for assessing generative models. Advances in neural information processing systems, 32, 2019. 7 [36] Doyup Lee, Chiheon Kim, Saehoon Kim, Minsu Cho, and Wook-Shin Han. Autoregressive image generation using residual quantization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1152311532, 2022. 2, 6 [37] Yaniv Leviathan, Matan Kalman, and Yossi Matias. Fast inference from transformers via speculative decoding. In International Conference on Machine Learning, pages 19274 19286. PMLR, 2023. 5 [38] Tianhong Li, Huiwen Chang, Shlok Mishra, Han Zhang, Dina Katabi, and Dilip Krishnan. Mage: Masked generative encoder to unify representation learning and image synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 21422152, 2023. 6, 8 [39] Tianhong Li, Yonglong Tian, He Li, Mingyang Deng, and Kaiming He. Autoregressive image generation without vector quantization. arXiv preprint arXiv:2406.11838, 2024. 2, 3 [40] Xiang Li, Hao Chen, Kai Qiu, Jason Kuen, Jiuxiang Gu, Bhiksha Raj, and Zhe Lin. Imagefolder: Autoregressive image generation with folded tokens. arXiv preprint arXiv:2410.01756, 2024. 2, [41] Dongyang Liu, Shitian Zhao, Le Zhuo, Weifeng Lin, Yu Qiao, Hongsheng Li, and Peng Gao. Lumina-mgpt: Illuminate flexible photorealistic text-to-image generation arXiv preprint with multimodal generative pretraining. arXiv:2408.02657, 2024. 2 [42] Loshchilov. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. 6 [43] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver: fast ode solver for diffusion probabilistic model sampling in around 10 steps. Advances in Neural Information Processing Systems, 35:57755787, 2022. 6 [44] Zhengxiong Luo, Dayou Chen, Yingya Zhang, Yan Huang, Liang Wang, Yujun Shen, Deli Zhao, Jingren Zhou, and Tieniu Tan. Videofusion: Decomposed diffusion modarXiv preprint els for high-quality video generation. arXiv:2303.08320, 2023. 7 [45] Xin Ma, Yaohui Wang, Gengyun Jia, Xinyuan Chen, Ziwei Liu, Yuan-Fang Li, Cunjian Chen, and Yu Qiao. Latte: Latent diffusion transformer for video generation. arXiv preprint arXiv:2401.03048, 2024. 10 [46] Volodymyr Mnih. Playing atari with deep reinforcement [60] Jiaming Song, Chenlin Meng, learning. arXiv preprint arXiv:1312.5602, 2013. 1 [47] Alexander Quinn Nichol and Prafulla Dhariwal. Improved In International denoising diffusion probabilistic models. conference on machine learning, pages 81628171. PMLR, 2021. 2 [48] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems, 32, 2019. [49] William Peebles and Saining Xie. Scalable diffusion models with transformers. arXiv preprint arXiv:2212.09748, 2022. 2, 6, 8 [50] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Improving latent diffusion modRobin Rombach. Sdxl: arXiv preprint els for high-resolution image synthesis. arXiv:2307.01952, 2023. 1, 2 [51] Adam Polyak, Amit Zohar, Andrew Brown, Andros Tjandra, Animesh Sinha, Ann Lee, Apoorv Vyas, Bowen Shi, ChihYao Ma, Ching-Yao Chuang, et al. Movie gen: cast of media foundation models. arXiv preprint arXiv:2410.13720, 2024. 2 [52] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. 1 [53] Scott Reed, Konrad Zolna, Emilio Parisotto, Sergio Gomez Colmenarejo, Alexander Novikov, Gabriel Barth-Maron, Mai Gimenez, Yury Sulsky, Jackie Kay, Jost Tobias SprinarXiv preprint generalist agent. genberg, et al. arXiv:2205.06175, 2022. 1 [54] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image In Proceedings of synthesis with latent diffusion models. the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. 1, 2, [55] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. ArXiv, abs/1505.04597, 2015. 2 [56] David Ruhe, Jonathan Heek, Tim Salimans, and Emiel arXiv preprint Hoogeboom. Rolling diffusion models. arXiv:2402.09470, 2024. 2 [57] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition challenge. International journal of computer vision, 115:211252, 2015. 2, 6 [58] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. Advances in neural information processing systems, 29, 2016. 7 Denoising diffusion implicit models. arXiv:2010.02502, 2020. 1, and Stefano Ermon. arXiv preprint [61] Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020. 1, 2 [62] Khurram Soomro, Amir Zamir, and Mubarak Shah. Ucf101: dataset of 101 human actions classes from videos in the wild. ArXiv, abs/1212.0402, 2012. 2, 6 [63] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. 4, 5 [64] Peize Sun, Yi Jiang, Shoufa Chen, Shilong Zhang, Bingyue Peng, Ping Luo, and Zehuan Yuan. Autoregressive model beats diffusion: Llama for scalable image generation. arXiv preprint arXiv:2406.06525, 2024. 2, 6 [65] Chameleon Team. Chameleon: Mixed-modal early-fusion foundation models. arXiv preprint arXiv:2405.09818, 2024. [66] Keyu Tian, Yi Jiang, Zehuan Yuan, Bingyue Peng, and Liwei Wang. Visual autoregressive modeling: Scalable imarXiv preprint age generation via next-scale prediction. arXiv:2404.02905, 2024. 2, 6 [67] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. 1, 2, 5 [68] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. 1, 2 [69] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in neural information processing systems, 30, 2017. 2 [70] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. NeurIPS, 30, 2017. 2, 3 [71] Junke Wang, Yi Jiang, Zehuan Yuan, Binyue Peng, Zuxuan Wu, and Yu-Gang Jiang. Omnitokenizer: joint image-video tokenizer for visual generation. arXiv preprint arXiv:2406.09399, 2024. [72] Xinlong Wang, Xiaosong Zhang, Zhengxiong Luo, Quan Sun, Yufeng Cui, Jinsheng Wang, Fan Zhang, Yueze Wang, Zhen Li, Qiying Yu, et al. Emu3: Next-token prediction is all you need. arXiv preprint arXiv:2409.18869, 2024. 2 [73] Mark Weber, Lijun Yu, Qihang Yu, Xueqing Deng, Xiaohui Shen, Daniel Cremers, and Liang-Chieh Chen. Maskbit: arXiv Embedding-free image generation via bit tokens. preprint arXiv:2409.16211, 2024. 2, 6 [59] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. 1 [74] Jialong Wu, Shaofeng Yin, Ningya Feng, Xu He, Dong Li, ivideogpt: Interactive Jianye Hao, and Mingsheng Long. videogpts are scalable world models, 2024. 2 11 [75] Shengqiong Wu, Hao Fei, Leigang Qu, Wei Ji, and Tat-Seng Chua. Next-gpt: Any-to-any multimodal llm, 2023. 2 [76] Yecheng Wu, Zhuoyang Zhang, Junyu Chen, Haotian Tang, Dacheng Li, Yunhao Fang, Ligeng Zhu, Enze Xie, Hongxu Yin, Li Yi, et al. Vila-u: unified foundation model integrating visual understanding and generation. arXiv preprint arXiv:2409.04429, 2024. [77] Jinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang, Weihao Wang, Kevin Qinghong Lin, Yuchao Gu, Zhijie Chen, Zhenheng Yang, and Mike Zheng Shou. Show-o: One single transformer to unify multimodal understanding and generation. arXiv preprint arXiv:2408.12528, 2024. 2 [78] An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, et al. Qwen2 technical report. arXiv preprint arXiv:2407.10671, 2024. 1 [79] Mengjiao Yang, Yilun Du, Kamyar Ghasemipour, Jonathan Tompson, Dale Schuurmans, and Pieter Abbeel. LearnarXiv preprint ing interactive real-world simulators. arXiv:2310.06114, 2023. 2 [80] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, et al. Scaling autoregressive models for content-rich text-to-image generation. arXiv preprint arXiv:2206.10789, 2(3):5, 2022. 2 [81] Lijun Yu, Jose Lezama, Nitesh Gundavarapu, Luca Versari, Kihyuk Sohn, David Minnen, Yong Cheng, Agrim Gupta, Xiuye Gu, Alexander Hauptmann, et al. Language model beats diffusiontokenizer is key to visual generation. arXiv preprint arXiv:2310.05737, 2023. 2, 7 [82] Chunting Zhou, Lili Yu, Arun Babu, Kushal Tirumala, Michihiro Yasunaga, Leonid Shamis, Jacob Kahn, Xuezhe Ma, Luke Zettlemoyer, and Omer Levy. Transfusion: Predict the next token and diffuse images with one multi-modal model. arXiv preprint arXiv:2408.11039, 2024. 2, 3 [83] Siyuan Zhou, Yilun Du, Jiaben Chen, Yandong Li, Dit-Yan Yeung, and Chuang Gan. Robodreamer: Learning compositional world models for robot imagination. arXiv preprint arXiv:2404.12377, 2024."
        }
    ],
    "affiliations": [
        "ByteDance",
        "Tsinghua University"
    ]
}