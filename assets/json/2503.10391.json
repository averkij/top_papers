{
    "paper_title": "CINEMA: Coherent Multi-Subject Video Generation via MLLM-Based Guidance",
    "authors": [
        "Yufan Deng",
        "Xun Guo",
        "Yizhi Wang",
        "Jacob Zhiyuan Fang",
        "Angtian Wang",
        "Shenghai Yuan",
        "Yiding Yang",
        "Bo Liu",
        "Haibin Huang",
        "Chongyang Ma"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Video generation has witnessed remarkable progress with the advent of deep generative models, particularly diffusion models. While existing methods excel in generating high-quality videos from text prompts or single images, personalized multi-subject video generation remains a largely unexplored challenge. This task involves synthesizing videos that incorporate multiple distinct subjects, each defined by separate reference images, while ensuring temporal and spatial consistency. Current approaches primarily rely on mapping subject images to keywords in text prompts, which introduces ambiguity and limits their ability to model subject relationships effectively. In this paper, we propose CINEMA, a novel framework for coherent multi-subject video generation by leveraging Multimodal Large Language Model (MLLM). Our approach eliminates the need for explicit correspondences between subject images and text entities, mitigating ambiguity and reducing annotation effort. By leveraging MLLM to interpret subject relationships, our method facilitates scalability, enabling the use of large and diverse datasets for training. Furthermore, our framework can be conditioned on varying numbers of subjects, offering greater flexibility in personalized content creation. Through extensive evaluations, we demonstrate that our approach significantly improves subject consistency, and overall video coherence, paving the way for advanced applications in storytelling, interactive media, and personalized video generation."
        },
        {
            "title": "Start",
            "content": "CINEMA: Coherent Multi-Subject Video Generation via MLLM-Based Guidance Yufan Deng1,2 Xun Guo1 Yizhi Wang1 Shenghai Yuan2 Yiding Yang Bo Liu1 1ByteDance Intelligent Creation Jacob Zhiyuan Fang1 Haibin Huang1 Angtian Wang1 Chongyang Ma1 2Peking Univeristy 5 2 0 M 3 1 ] . [ 1 1 9 3 0 1 . 3 0 5 2 : r Figure 1. We introduce CINEMA, video generation framework conditioned on set of reference images and text prompts. CINEMA enables the generation of videos visually consistent across multiple subjects from diverse scenes, providing enhanced flexibility and precise control over the video synthesis process."
        },
        {
            "title": "Abstract",
            "content": "Video generation has witnessed remarkable progress with the advent of deep generative models, particularly diffusion models. While existing methods excel in generating high-quality videos from text prompts or single images, personalized multi-subject video generation remains largely unexplored challenge. This task involves synthesizing videos that incorporate multiple distinct subjects, each defined by separate reference images, while ensuring temporal and spatial consistency. Current approaches primarily rely on mapping subject images to keywords in text prompts, which introduces ambiguity and limits their ability to model subject relationships effectively. In this paper, we propose CINEMA, novel framework for coherent multi-subject video generation by leveraging Multimodal Large Language Model (MLLM). Our approach eliminates the need for explicit correspondences between subject images and text entities, mitigating ambiguity and reducing annotation effort. By leveraging MLLM to interpret subject relationships, our method facilitates scalability, enabling the use of large and diverse datasets for training. Furthermore, our framework can be conditioned on varying numbers of subjects, offering greater flexibility in personalized content creation. Through extensive evaluations, we demonstrate that our approach significantly improves subject consistency, and overall video coherence, paving the way for advanced applications in storytelling, interactive media, and personalized video generation. 1. Introduction Advances in deep generative models, particularly diffusion models [16, 28, 41], have significantly enhanced the ability to generate realistic and coherent videos from text prompts or images, attracting considerable attention from both academia and industry. Pioneering models such as Stable Video Diffusion [2], Runway Gen [40], Pika [29], and Sora [27] have demonstrated remarkable capabilities in generating highquality videos, continually expanding the boundaries of visual content creation. These models leverage large-scale datasets, advanced diffusion techniques, and scaling model size to produce visually compelling and temporally consistent results, establishing video generation as cornerstone of modern generative AI. Typically, video generation is driven by user provided prompt (Text-to-Video), or single image as the starting frame (Image-to-Video). Furthermore, personalized video generation aims to preserve the visual consistency of given subject while seamlessly following the prompts instructions. Single-subject video generation has been extensively studied. For example, recent works like CustomCrafter [51] adapts to given subject using LoRA [17], while DreamVideo-2 [50] and SUGAR [64] enhance identity preservation through masking mechanism in the input image or attention operations, respectively. In contrast, personalized multi-subject video generation remains significantly challenging and underexplored task. It requires generating video that features multiple subjects, each defined by distinct reference images, while optionally being guided by text prompt to control scene context or dynamics. While existing models excel at generating videos from single input modality, they often struggle to seamlessly integrate multiple subjects in coherent and visually consistent manner. This limitation underscores key challenge: effectively understanding the relationships between multiple subjects, their interactions, and their integration into unified scene while preserving both temporal and spatial consistency. Recent works [8, 18, 25, 49] have sought to address this challenge. VideoAlchemy [8] encodes multiple subject images into tokens leveraging frozen image encoder, binds them with corresponding word tokens from the text prompt, and incorporates these combined embeddings into the Diffusion Transformer [28] via separate cross-attention layers. Grounding visual concepts to entity words, however, is challenging [37] and yet not practical in the era of scaling law. For example, in video with two persons are talking, both visual subjects map to two persons, causing ambiguity. Furthermore, these methods struggle to establish meaningful correlations between the visual features of subjects under the context of prompt, resulting in limited understanding of subject relationships. For example, in [8], visual tokens are merely concatenated after being linked to word tokens, without deeper semantic understanding or comprehension, leading to disorganized spatial arrangements and ambiguous subject interactions for generation. In this paper, we introduce novel framework for multisubject video generation that leverages Multimodal Large Language Model (MLLM) guidance to ensure coherence and subject consistency. We propose to exploit the comprehension capability of MLLM to interpret and orchestrate the relationships between multiple subjects, enabling the generation of personalized, multi-subject videos that are both visually coherent and contextually meaningful. Our approach is model-agnostic, and we seamlessly build CINEMA upon pre-trained, open-source video generative model [53], highlighting its versatility and adaptability. Since most existing open-sourced video generative models [44, 53, 62, 63] relies on LLM like [33], and are not inherently compatible with MLLM, we introduce AlignerNet, module designed to bridge this gap by aligning MLLM outputs with the native text features. Additionally, to ensure the preservation of each subjects visual appearance, we additionally inject Variational Autoencoder (VAE) features of reference images into our training, which serve as auxiliary conditioning signal and proves to be critical for maintaining consistency across frames. To summarize our contribution: We propose CINEMA, the first video generative model leveraging MLLM for multi-subject video generation task. We demonstrate that our proposed method eliminates the need for training with explicit correspondences between subject images and entity words, enables easy scalability, allowing the use of large, diverse datasets for training. With the comprehension capability of MLLM, we show that CINEMA significantly improves multi-subject consistency, and overall video coherence in generation. 2. Related Work Video foundation model. Recent advancements in video generation often rely on Variational Autoencoders (VAEs) [20, 23, 45] to compress raw video data into lowdimensional latent space. Within this compressed latent space, large-scale generative pre-training is conducted using either diffusion-based methods [15, 42] or auto-regressive approaches [7, 36, 55]. Leveraging the scalability of Transformer models [28, 46], these methods have demonstrated steady performance improvements [3, 4, 13, 53]. Early methods for text-to-video (T2V) generation such as MagicTime [58, 59] and AnimateDiff [14] have laid the groundwork by integrating temporal modules into the 2D U-Net architecture [38]. These approaches typically involve redesigning and extensively training U-Net models to learn temporal dynamics, addressing the challenges posed by the dynamic nature of video content. Recent works, such as CogVideoX [53] and HunyuanVideo [21], have adopted more advanced architecture designs. First, they leverage 3D causal VAE [56] for encoding and decoding raw video data, significantly improving the compression ratio of the original data. Second, they utilize the DiT [28] model architecture, known for its superior scaling capabilities, and incorporate 3D full-attention mechanism. This enhances the models ability to capture temporal dynamics in video data, leading to substantial progress in T2V generation. The image-to-video (I2V) generation task focuses on generating videos conditioned on static input image. One widely adopted strategy involves integrating CLIP [30] embeddings of the input image into the diffusion process. For example, VideoCrafter1 [6] and I2V-Adapter [13] leverage dual cross-attention layer, inspired by the IP-Adapter [54], to effectively fuse these embeddings. Another prominent strategy involves expanding the input channels of diffusion models to concatenate the static image with noisy video frames. Methods such as SEINE [9] and PixelDance [60] achieve superior results by integrating image information more effectively into the video generation process. Additionally, hybrid approaches combine channel concatenation with attention mechanisms to inject image features. Notable examples include DynamiCrafter [52], I2VGen-XL [61], and Stable Video Diffusion (SVD) [3]. These methods aim to maintain consistency in both global semantics and finegrained details. By utilizing this dual approach, they ensure that the generated videos remain faithful to the input image while exhibiting realistic and coherent motion dynamics. Subject-driven image and video generation. Generating identity-consistent images and videos based on given reference images requires the model to accurately capture the ID features. Previous methods can be broadly divided into two categories: tuning-based and tuning-free solutions. Tuning-based methods [8, 50, 51, 64] typically utilize efficient fine-tuning techniques, such as LoRA [17] or DreamBooth [39], to adapt pre-trained models. These approaches embed identity-specific information while retaining the original generative capabilities of the foundation model. However, these methods require fine-tuning for each new identity, which limits their real-world practicality. In contrast, tuning-free methods adopt feed-forward inference manner, eliminating the need for fine-tuning with new identity input. StoryDiffusion [65] employs consistent self-attention mechanism and semantic motion predictor to ensure smooth transitions and consistent identity. MS-Diffusion [48] introduces grounded resampler and multi-subject cross-attention mechanism to extract detailed features of the reference subjects. ConsisID [57], maintains facial identity consistency in videos through frequency decomposition. Specifically, it decomposes identity features into low-frequency and high-frequency signals, which are then injected into specific layers of the DiT [28] model. Finally, ConceptMaster [18] employs CLIP [31] image encoder and Q-Former [22] to extract visual embeddings. It further fuses the visual and textual embeddings using Decoupled Attention Module (DAM) and incorporates these embeddings into the diffusion model via multi-concept injector, enabling multi-concept video customization. 3. Method Given set of reference images and corresponding text prompts, our objective is to generate video that maintains the consistent appearance of the subject across frames. This task presents several challenges, as it requires preserving the identity depicted in the reference images, faithfully adhering to the instructions provided in the text prompts, and ensuring temporal coherence throughout the entire video sequence. 3.1. Preliminaries Multimodal large language models. Multimodal large language models (MLLMs) [1, 22, 24, 43, 47] extend traditional large language models by enabling cross-modal understanding and generation across various modalities, such as text, image, video, and audio. Building on the success of unimodal LLMs like GPT [5] and PaLM [10], MLLMs incorporate modality-specific encoders, such as CLIP [31] for visual input and Whisper [32] for audio, to project heterogeneous inputs into unified embedding space. These embeddings are well-aligned with cross-modal fusion modules, such as attention-based mechanisms like Perceiver Resamplers [19], and processed by decoder-only transformer backbone adapted for multimodal tasks via parameterefficient fine-tuning [17, 39]. Multimodal diffusion transformer. The Multimodal Diffusion Transformer (MM-DiT) is an advanced transformerbased diffusion backbone introduced by [12], designed to enhance multimodal alignment and generation consistency in generative tasks. Specifically, MM-DiT employs multimodal joint attention mechanism that facilitates efficient visual-textual cross-modal interaction. This approach has been shown to significantly outperform cross-attention-based models in terms of generation quality, text alignment, and computational efficiency. 3.2. Data Curation We present data processing pipeline that efficiently ingests training videos, generates text labels, and extracts referencessuch as humans, faces, and objectsfacilitating comprehensive video-based tasks. To ensure clean, and high-quality training datasets, we perform rigorous data pre-processing and filtering. Given set of training videos, we first perform scene-change detection to segment each video into multiple clips, i.e., V1, V2, . . . , Vn. Following previous video generation approaches [53, 62], we evaluate the aesthetics and motion magnitude of each clip Vi, discarding those with poor aesthetic quality or minimal motion. For each filtered video clip Vi, we exploit Qwen2-VL [47] to generate caption that depicts its overall content focusing on motion aspect. We then prompt the MLLM to identify and list the objects (entity words) mentioned in the caption as clip-level labels, such as cat, hat, and t-shirt. For each object label, we compute bounding box in the first Figure 2. Our overall pipeline consists of Multimodal Large Language Model, semantic alignment network (AlignerNet), visual entity encoding network, and Diffusion Transformer that integrates the embeddings encoded by these two modules. denotes concatenation. frame of the clip using GroundingDINO [26], then continuously segment the object with SAM2 [34].These segmented subject maps serve as reference image Obj i,k . Maintaining consistent appearance of humans is notoriously challenging in image/video generation task. To ensure clear representation of humans, we first detect humans and faces in the initial frame using YOLO [35], segment the human regions with SAM2, and apply slightly looser bounding box around faces to avoid including irrelevant elements. These segmented images are then stored as Humani and Facei, respectively. Consequently, each training input comprises set of object segmentations, human segmentations, and cropped faces, together with their text labels. We define the training data as , Obj Ri = { Ci, Human i,k }. For each training example, we pair this set with the corresponding video clip Vi. i,2 , . . . , Obj i,1 , Obj , Face 3.3. Video Generation via MLLM-Based Guidance We propose novel framework, CINEMA, for coherent multi-subject video generation, leveraging the powerful comprehending capabilities of MLLMs. Utilizing MM-DiT for video generation, CINEMA integrates multimodal conditional information through three key modules, i.e., multimodal large language model, semantic alignment network, and visual entity encoding. An overview of our framework is provided in Figure 2. The MLLM is employed to encode Figure 3. Our language instruction template for MLLM. multimodal conditions (e.g., images and text) into unified feature representations. Additionally, the semantic alignment network AlignerNet is introduced to bridge the gap between this unified representation space and the original DiT conditioning feature space. Lastly, the visual entity encoding is designed to capture fine-grained entity-level visual attributes. The architecture and functionality of each module are elaborated in the following subsection. Multimodal large language model. At the core of CINEMA lies the utilization of an MLLM. Specifically, Qwen2VL [47] is adopted to generate unified representations from arbitrary reference images, and text prompts. These unified representations are then fed into the MM-DiT backbone, where multimodal joint attention mechanism facilitates interactive fusion of the encoded tokens. In addition, we employ an instruction tuning approach, incorporating specialized instruction template to guide the MLLM for more accurate and contextually relevant encoding results. The specific instruction is as shown in Figure 3. Semantic alignment network. We build CINEMA on the basis of pre-trained, open-sourced video generative model model, CogVideoX [53], which is largely pre-trained on text/image-to-video task, encoding text prompts with large language model [33]. To address the semantic representation gap across LLM and MLLM, we propose to replace T5 encoder with MLLM by introducing AlignerNet module. Our AlignerNet is transformer-based network. In particular, AlignerNet maps the hidden states generated by the MLLM onto the feature space of the T5 text encoder, ensuring that the resulting multimodal semantic features encompassing both visual and textual information are wellaligned with the feature space of the T5 encoder. We use the combination of Mean Squared Error (MSE) and Cosine Similarity loss to optimize the AlignerNet. Specifically, let FT5 RKd represents the feature vectors generated by T5 encoder, and FMLLM RKd are the hidden states of MLLM after mapping by AlignerNet. The MSE loss serves to minimize the Euclidean distance between FT5 and FMLLM as follows: Lmse = 1 (cid:88) i= F(i) MLLM F(i) T52 2. (1) The Cosine Similarity loss focuses on aligning the directions of FT5 and FMLLM. It is given by: Lcos = (cid:32) 1 1 (cid:88) i= F(i) MLLM, F(i) T5 MLLM2 F(i) T52 F(i) (cid:33) . (2) Finally, the overall loss function is shown in Eq. 3, where λmse and λcos are coefficients to adjust the weights between the two loss terms: LAlignerNet = λmse Lmse + λcos Lcos. (3) Visual entity encoding. We observe that the visual features encoded by the pre-trained MLLM [47] are coarsegrained, as its training objective is to minimize similarity loss between visual and text representations, primarily captures semantic-level image features. However, our task requires fine-grained visual identity (ID) features to handle arbitrary reference images as conditions. To bridge this gap, we use VAE encoder that performs entity-level encoding on each reference image to enhance visual ID consistency. Specifically, as shown in Figure 2, given variable number of reference images, the VAE first encodes each image to produce latent features. These features are then flattened into sequence of embeddings, padded to fixed length (denoted as Fvisual RM d), and fed into MM-DiT. Figure 4. An example of reference images, along with the corresponding video and caption, from our training set. Multimodal feature conditioned denoising. After obtaining the multi-modality features FMLLM and visual features Fvisual, we concatenate these tokens to form unified multimodal feature Funified as follows: Funified = Concat(FMLLM, Fvisual) R(K+M )d. (4) Next, the unified multimodal features Funified and the noisy tokens Ft noise on timestep are concatenated to form the final input sequence of MM-DiT backbone: Finput = Concat(Funified, Ft noise) R(K+M +T )d. (5) dual-branch transformer model [53] is then employed, taking Funified into the text branch and Ft noise to the vision branch. It features joint attention mechanism that dynamically fuses multimodal-enhanced feature tokens as follows: Attention(Q, K, V) = softmax (cid:19) (cid:18) Qi(K)T V, (6) where = FinputWQ, = FinputWK, and = FinputWV . Both WQ, WK, and WV are trainable weight matrices to encode the input feature maps. During the attention operation, the sequences from both modalities are concatenated, and joint attention weights are computed to achieve complementary information exchange. Finally, the MM-DiT is fully finetuned on the diffusion [15] training objective as: = Et,x0,ϵϵ ϵθ( αtx0 + 1 αtϵ, t)2, (7) where is sampled by explicit uniform sampling. 4. Experiments 4.1. Experimental Setup Dataset. We train our model on self-curated, high-quality video dataset (see the examples in Figure 4). Specifically, the original training set contains approximately 5.1 million videos. After filtering, we obtain about 1.46 million video clips, each paired with one to six human and object references. Each video clip consists of 96 frames, and all the paired references are stored as RGBA images. Figure 5. Qualitative evaluation of our method. The reference images are shown on the left, along with the text prompt at the bottom. In each case, we show four frames uniformly sampled from the generated 45-frame video. Network components. For the video diffusion model, we use an open-source version of the image-to-video pretrained DiT model based on CogVideoX [53] as the base model. Due to insufficient training, the original I2V model does not converge effectively. Therefore, we continuously train the [53] model on I2V task first. For MLLM, we adopt Qwen2-VL7B-Instruct [47], and freeze its parameters during the training of the entire framework. We adopt an attention-based architecture for AlignerNet, which consists of six attention layers with hidden width of 768, eight attention heads, and 226 latent tokens to align with the original T5 sequence length. The input dimension is 2048, and the output dimension is aligned with the DiT. Training strategies. We train our model using the AdamW optimizer with β1 = 0.9, β2 = 0.95, and weight decay of 0.001. The initial learning rate is set to 1 105 and follows cosine annealing schedule with restarts. To stabilize early training, we employ linear warm-up phase in the first 100 steps. The batch size is set to one. We apply gradient clipping with maximum norm of 1.0 to prevent exploding gradients. The conditional reference is randomly dropped with probability of 0.05. Our model is trained with 128 NVIDIA H100 GPUs for two days. The input videos are processed at resolution of 448P, with sequence length of 45 frames and temporal stride of two. The AlignerNet is pre-trained with the MSE loss and cosine similarity loss to match the feature distribution of the T5 model, with both λmse and λcos set to be 1 in Eq. 3. Then we jointly train AlignerNet and DiT. 4.2. Qualitative Results Figures 5 and 6 demonstrate qualitative examples of our method, which synthesizes coherent videos from reference images paired with succinct textual descriptions. The results clearly indicate that our model successfully preserves the distinct visual attributes of the provided subjects, such as clothing textures, accessories, and environmental contexts. For instance, in Figure 5, the generated video accurately retains the blonde hair and gentle interaction with blossoms, precisely following the prompt. Similarly, the detailed texture and color of the brown suit worn by the man are faithfully reproduced along with natural gestures like putting his hands on his chin. Furthermore, Figure 6 Figure 6. Qualitative evaluation of our method dealing with multiple concepts. Our model is capable of encoding and understanding multiple subjects based on the reference images. showcases the capability of our model to handle complex scenarios with multiple visual concepts. An illustrative example includes the woman outdoors wearing leather jacket and drinking from distinctly colored bottle, where both the clothing and the objects visual details are accurately captured. Additionally, the babys interaction with shallow beach water effectively demonstrates the models skill in recreating natural movements consistent with the described environment. These examples confirm that our method can accurately represent interactions among individuals, objects, and settings, producing contextually relevant and visually convincing motions1. 4.3. Ablation Studies Several ablation studies are conducted to verify the effectiveness of each component in CINEMA. The corresponding qualitative evaluation results are demonstrated in Figure 7. Semantic alignment network. The proposed AlignerNet bridges the feature gap between the text representation from the MLLM [47] and the T5 encoder [33]. To this end, AlignerNet is pretrained to minimize the MSE and cosine distance to the output of the T5 encoder. To demonstrate the necessity of pretraining, we conduct experiments by randomly initializing AlignerNet and training it end-to-end with MM-DiT. As shown in Figure 7(b), we observe unstable and inconsistent results and hypothesize that these issues might be due to the random initialization of AlignerNet, causing slower convergence, increased training difficulty, and poor align1The corresponding animations can be found in the composed video demo included in our supplementary materials. ment between text and reference image modalities. This finding underscores the critical importance of pretraining AlignerNet to effectively map the MLLM output to the T5s embedding space. Visual entity encoding. This module generates image latents that preserve fine-grained visual details from the reference images. The MM-DiT then synthesizes videos with high subject consistency from reference images. In our ablation study, we replace the VAE-derived embeddings with features extracted from the vision encoder of the original MLLM [47]. As demonstrated in Figure 7(c), the generated videos still contain the corresponding subjects; however, they exhibit variations in fine details or identity, particularly for human subjects. The generated video only keeps general attributes of the subjects. This result highlights the necessity of the VAE encoder in preserving visual details, ensuring high-level reference consistency in generated videos. Unified multimodal encoder. This encoder serves as the core of our framework, integrating both text prompts and visual references into cohesive representation. In our ablation study, we replace it with the standard T5 text encoder. Under this configuration, text prompts are encoded by T5, while reference images are processed separately using the VAE encoder, with no fusion between the two modalities. As shown in Figure 7(d), this results in videos with reduced consistency between visual and textual prompts. Specifically, the interactions described in the text prompt may not involve the intended subject from the reference images. This issue highlights the importance of unifying textual and viFigure 7. Qualitative comparison for ablation studies. The reference images and text prompt are shown on the top. The result of the full model is in the first line, followed by those from different ablation experiments. We show four frames uniformly sampled from the generated 45-frame video of each method. sual inputs before feeding them into the MM-DiT for video generation. Limitations. While CINEMA is model-agnostic, its performance is predominantly constrained by the capabilities of the underlying video foundation model, which can impact both output quality and temporal consistency obviously. Furthermore, our method struggles to accurately distinguish between different human identities (as illustrated in Figure 8), which hinders its ability to handle multiple individuals effectively, common challenge in multi-subject image and video generation systems [11, 39]. 5. Conclusion In this work, we introduce CINEMA, multi-subject video generation framework that leverages MLLM to further enhance coherence and contextual alignment. By employing MLLM as the encoder, it eliminates explicit subject-text correspondences required for training, and so our approach enables flexible and harmonious video synthesis, yielding improved subject consistency and interaction modeling. Figure 8. failure case of our method. The identities of the two persons are not clearly distinguishable or well-maintained in the generated video. As part of future work, we aim to leverage more advanced video foundation models [21, 44], to enhance the modeling of subject relationships, support dynamic appearances, and improve the differentiation of multi-human identities. These advancements will contribute to achieving more robust and controllable video generation."
        },
        {
            "title": "References",
            "content": "[1] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2.5-VL technical report. arXiv preprint arXiv:2502.13923, 2025. 3 [2] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. 1 [3] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. 2, 3 [4] Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, Clarence Ng, Ricky Wang, and Aditya Ramesh. Video generation models as world simulators. https: //openai.com/research/videogenerationmodelsasworld- simulators, 2024. Accessed: 2023-10-20. 2 [5] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901, 2020. 3 [6] Haoxin Chen, Menghan Xia, Yingqing He, Yong Zhang, Xiaodong Cun, Shaoshu Yang, Jinbo Xing, Yaofang Liu, Qifeng Chen, Xintao Wang, et al. Videocrafter1: Open diffusion models for high-quality video generation. arXiv preprint arXiv:2310.19512, 2023. 3 [7] Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, and Ilya Sutskever. Generative pretraining from pixels. In Proceedings of the 37th International Conference on Machine Learning, pages 16911703. PMLR, 2020. 2 [8] Tsai-Shien Chen, Aliaksandr Siarohin, Willi Menapace, Yuwei Fang, Kwot Sin Lee, Ivan Skorokhodov, Kfir Aberman, Jun-Yan Zhu, Ming-Hsuan Yang, and Sergey Tulyakov. Multi-subject open-set personalization in video generation. arXiv preprint arXiv:2501.06187, 2025. 2, [9] Xinyuan Chen, Yaohui Wang, Lingjun Zhang, Shaobin Zhuang, Xin Ma, Jiashuo Yu, Yali Wang, Dahua Lin, Yu Qiao, and Ziwei Liu. Seine: Short-to-long video diffusion model for generative transition and prediction. In The Twelfth International Conference on Learning Representations, 2023. 3 [10] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. Journal of Machine Learning Research, 24(240):1113, 2023. 3 [11] Ganggui Ding, Canyu Zhao, Wen Wang, Zhen Yang, Zide Liu, Hao Chen, and Chunhua Shen. Freecustom: Tuning-free customized image generation for multi-concept composition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 90899098, 2024. 8 [12] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first international conference on machine learning, 2024. 3 [13] Xun Guo, Mingwu Zheng, Liang Hou, Yuan Gao, Yufan Deng, Pengfei Wan, Di Zhang, Yufan Liu, Weiming Hu, Zhengjun Zha, et al. I2v-adapter: general image-to-video adapter for diffusion models. In ACM SIGGRAPH 2024 Conference Papers, pages 112, 2024. 2, 3 [14] Yuwei Guo, Ceyuan Yang, Anyi Rao, Zhengyang Liang, Yaohui Wang, Yu Qiao, Maneesh Agrawala, Dahua Lin, and Bo Dai. Animatediff: Animate your personalized text-to-image diffusion models without specific tuning. In The Twelfth International Conference on Learning Representations, 2024. [15] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. 2, 5 [16] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. 1 [17] Edward Hu, yelong shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations, 2022. 2, 3 [18] Yuzhou Huang, Ziyang Yuan, Quande Liu, Qiulin Wang, Xintao Wang, Ruimao Zhang, Pengfei Wan, Di Zhang, and Kun Gai. Conceptmaster: Multi-concept video customization on diffusion transformer models without test-time tuning. arXiv preprint arXiv:2501.04698, 2025. 2, 3 [19] Andrew Jaegle, Felix Gimeno, Andy Brock, Oriol Vinyals, Andrew Zisserman, and Joao Carreira. Perceiver: General perception with iterative attention. In International conference on machine learning, pages 46514664. PMLR, 2021. 3 [20] Diederik Kingma. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013. 2 [21] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. 2, 8 [22] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In International conference on machine learning, pages 1973019742. PMLR, 2023. 3 [23] Zongjian Li, Bin Lin, Yang Ye, Liuhan Chen, Xinhua Cheng, Shenghai Yuan, and Li Yuan. Wf-vae: Enhancing video vae by wavelet-driven energy flow for latent video diffusion model. arXiv preprint arXiv:2411.17459, 2024. 2 [24] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36:3489234916, 2023. 3 [25] Lijie Liu, Tianxiang Ma, Bingchuan Li, Zhuowei Chen, Jiawei Liu, Qian He, and Xinglong Wu. Phantom: Subjectconsistent video generation via cross-modal alignment. arXiv preprint arXiv:2502.11079, 2025. [26] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Qing Jiang, Chunyuan Li, Jianwei Yang, Hang Su, et al. Grounding dino: Marrying dino with grounded pre-training for open-set object detection. In European Conference on Computer Vision, pages 3855. Springer, 2024. 4 [27] OpenAI. Sora, 2024. Accessed: 2025-02-26. 1 [28] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 41954205, 2023. 1, 2, 3 [29] Pika. Pika art 2.0s scene ingredients: Redefining personalized video creation, 2024. Accessed: 2025-02-26. 1 [30] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR, 2021. 3 [31] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PmLR, 2021. [32] Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever. Robust speech recognition via large-scale weak supervision. In International conference on machine learning, pages 2849228518. PMLR, 2023. 3 [33] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter Liu. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of machine learning research, 21(140):167, 2020. 2, 5, 7 [34] Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman Radle, Chloe Rolland, Laura Gustafson, Eric Mintun, Junting Pan, Kalyan Vasudev Alwala, Nicolas Carion, Chao-Yuan Wu, Ross Girshick, Piotr Dollar, and Christoph Feichtenhofer. Sam 2: Segment anything in images and videos. arXiv preprint arXiv:2408.00714, 2024. 4 [35] Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi. You only look once: Unified, real-time object detection. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 779788, 2016. 4 [36] Zhongwei Ren, Yunchao Wei, Xun Guo, Yao Zhao, Bingyi Kang, Jiashi Feng, and Xiaojie Jin. Videoworld: Exploring knowledge learning from unlabeled videos. arXiv preprint arXiv:2501.09781, 2025. 2 [37] Anna Rohrbach, Marcus Rohrbach, Ronghang Hu, Trevor Darrell, and Bernt Schiele. Grounding of textual phrases in images by reconstruction. In Computer VisionECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 1114, 2016, Proceedings, Part 14, pages 817834. Springer, 2016. 2 [38] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. Unet: Convolutional networks for biomedical image segmentation. In Medical Image Computing and Computer-Assisted InterventionMICCAI 2015: 18th International Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III 18, pages 234241. Springer, 2015. 2 [39] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2250022510, 2023. 3, 8 [40] Runway. Runway, 2024. Accessed: 2025-02-26. 1 [41] Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020. 1 [42] Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. In International Conference on Learning Representations, 2021. 2 [43] Gemini Team, Rohan Anil, Sebastian Borgeaud, JeanBaptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. [44] Wan Team. Wan: Open and advanced large-scale video generative models, 2025. 2, 8 [45] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in neural information processing systems, 30, 2017. 2 [46] Vaswani. Attention is all you need. Advances in Neural Information Processing Systems, 2017. 2 [47] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. 3, 4, 5, 6, [48] Xiaowei Wang, Siming Fu, Qihan Huang, Wanggui He, and Hao Jiang. Ms-diffusion: Multi-subject zero-shot image personalization with layout guidance. arXiv preprint arXiv:2406.07209, 2024. 3 [49] Zhao Wang, Aoxue Li, Lingting Zhu, Yong Guo, Qi Dou, and Zhenguo Li. Customvideo: Customizing text-to-video generation with multiple subjects. arXiv preprint arXiv:2401.09962, 2024. 2 [50] Yujie Wei, Shiwei Zhang, Hangjie Yuan, Xiang Wang, Haonan Qiu, Rui Zhao, Yutong Feng, Feng Liu, Zhizhong Huang, Jiaxin Ye, et al. Dreamvideo-2: Zero-shot subject-driven video customization with precise motion control. arXiv preprint arXiv:2410.13830, 2024. 2, 3 You. Open-sora: Democratizing efficient video production for all, 2024. 2, 3 [63] Yuan Zhou, Qiuyue Wang, Yuxuan Cai, and Huan Yang. Allegro: Open the black box of commercial-level video generation model. arXiv preprint arXiv:2410.15458, 2024. 2 [64] Yufan Zhou, Ruiyi Zhang, Jiuxiang Gu, Nanxuan Zhao, Jing Shi, and Tong Sun. Sugar: Subject-driven video customization in zero-shot manner. arXiv preprint arXiv:2412.10533, 2024. 2, [65] Yupeng Zhou, Daquan Zhou, Ming-Ming Cheng, Jiashi Feng, and Qibin Hou. Storydiffusion: Consistent self-attention for long-range image and video generation. Advances in Neural Information Processing Systems, 37:110315110340, 2024. 3 [51] Tao Wu, Yong Zhang, Xintao Wang, Xianpan Zhou, Guangcong Zheng, Zhongang Qi, Ying Shan, and Xi Li. Customcrafter: Customized video generation with preserving motion and concept composition abilities. arXiv preprint arXiv:2408.13239, 2024. 2, 3 [52] Jinbo Xing, Menghan Xia, Yong Zhang, Haoxin Chen, Wangbo Yu, Hanyuan Liu, Gongye Liu, Xintao Wang, Ying Shan, and Tien-Tsin Wong. Dynamicrafter: Animating openIn European domain images with video diffusion priors. Conference on Computer Vision, pages 399417. Springer, 2024. 3 [53] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. 2, 3, 5, 6 [54] Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ipadapter: Text compatible image prompt adapter for text-toimage diffusion models. arXiv preprint arXiv:2308.06721, 2023. 3 [55] Lijun Yu, Yong Cheng, Kihyuk Sohn, Jose Lezama, Han Zhang, Huiwen Chang, Alexander Hauptmann, MingHsuan Yang, Yuan Hao, Irfan Essa, et al. Magvit: Masked generative video transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1045910469, 2023. [56] Lijun Yu, Jose Lezama, Nitesh Gundavarapu, Luca Versari, Kihyuk Sohn, David Minnen, Yong Cheng, Vighnesh Birodkar, Agrim Gupta, Xiuye Gu, et al. Language model beats diffusiontokenizer is key to visual generation. arXiv preprint arXiv:2310.05737, 2023. 2 [57] Shenghai Yuan, Jinfa Huang, Xianyi He, Yunyuan Ge, Yujun Shi, Liuhan Chen, Jiebo Luo, and Li Yuan. Identitypreserving text-to-video generation by frequency decomposition. arXiv preprint arXiv:2411.17440, 2024. 3 [58] Shenghai Yuan, Jinfa Huang, Yujun Shi, Yongqi Xu, Ruijie Zhu, Bin Lin, Xinhua Cheng, Li Yuan, and Jiebo Luo. Magictime: Time-lapse video generation models as metamorphic simulators. arXiv preprint arXiv:2404.05014, 2024. 2 [59] Shenghai Yuan, Jinfa Huang, Yongqi Xu, Yaoyang Liu, Shaofeng Zhang, Yujun Shi, Rui-Jie Zhu, Xinhua Cheng, Jiebo Luo, and Li Yuan. Chronomagic-bench: benchmark for metamorphic evaluation of text-to-time-lapse video generation. Advances in Neural Information Processing Systems, 37:2123621270, 2025. 2 [60] Yan Zeng, Guoqiang Wei, Jiani Zheng, Jiaxin Zou, Yang Wei, Yuchen Zhang, and Hang Li. Make pixels dance: Highdynamic video generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 88508860, 2024. 3 [61] Shiwei Zhang, Jiayu Wang, Yingya Zhang, Kang Zhao, Hangjie Yuan, Zhiwu Qin, Xiang Wang, Deli Zhao, and Jingren Zhou. I2vgen-xl: High-quality image-to-video synthesis via cascaded diffusion models. arXiv preprint arXiv:2311.04145, 2023. 3 [62] Zangwei Zheng, Xiangyu Peng, Tianji Yang, Chenhui Shen, Shenggui Li, Hongxin Liu, Yukun Zhou, Tianyi Li, and Yang"
        }
    ],
    "affiliations": [
        "ByteDance Intelligent Creation",
        "Peking University"
    ]
}