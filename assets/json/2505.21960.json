{
    "paper_title": "One-Way Ticket:Time-Independent Unified Encoder for Distilling Text-to-Image Diffusion Models",
    "authors": [
        "Senmao Li",
        "Lei Wang",
        "Kai Wang",
        "Tao Liu",
        "Jiehang Xie",
        "Joost van de Weijer",
        "Fahad Shahbaz Khan",
        "Shiqi Yang",
        "Yaxing Wang",
        "Jian Yang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Text-to-Image (T2I) diffusion models have made remarkable advancements in generative modeling; however, they face a trade-off between inference speed and image quality, posing challenges for efficient deployment. Existing distilled T2I models can generate high-fidelity images with fewer sampling steps, but often struggle with diversity and quality, especially in one-step models. From our analysis, we observe redundant computations in the UNet encoders. Our findings suggest that, for T2I diffusion models, decoders are more adept at capturing richer and more explicit semantic information, while encoders can be effectively shared across decoders from diverse time steps. Based on these observations, we introduce the first Time-independent Unified Encoder TiUE for the student model UNet architecture, which is a loop-free image generation approach for distilling T2I diffusion models. Using a one-pass scheme, TiUE shares encoder features across multiple decoder time steps, enabling parallel sampling and significantly reducing inference time complexity. In addition, we incorporate a KL divergence term to regularize noise prediction, which enhances the perceptual realism and diversity of the generated images. Experimental results demonstrate that TiUE outperforms state-of-the-art methods, including LCM, SD-Turbo, and SwiftBrushv2, producing more diverse and realistic results while maintaining the computational efficiency."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 2 ] . [ 1 0 6 9 1 2 . 5 0 5 2 : r One-Way Ticket : Time-Independent Unified Encoder for Distilling Text-to-Image Diffusion Models Senmao Li1 Lei Wang1 Kai Wang2 Tao Liu1 Jiehang Xie Fahad Shahbaz Khan4,5 Shiqi Yang6 Yaxing Wang1,7* Joost van de Weijer2 Jian Yang1 1VCIP, CS, Nankai University 2Computer Vision Center, Universitat Aut`onoma de Barcelona 3School of Big Data and Computer Science, Guizhou Normal University 4Mohamed bin Zayed University of AI 5Linkoping University 6SB Intuitions, SoftBank 7Nankai International Advanced Research Institute (Shenzhen Futian), Nankai University {senmaonk,scitop1998,ltolcy0,shiqi.yang147.jp}@gmail.com jiehangxie@gznu.edu.cn {kwang,joost}@cvc.uab.es fahad.khan@liu.se {yaxing,csjyang}@nankai.edu.cn"
        },
        {
            "title": "Abstract",
            "content": "Text-to-Image (T2I) diffusion models have made remarkable advancements in generative modeling; however, they face trade-off between inference speed and image quality, posing challenges for efficient deployment. Existing distilled T2I models can generate high-fidelity images with fewer sampling steps, but often struggle with diversity and quality, especially in one-step models. From our analysis, we observe redundant computations in the UNet encoders. Our findings suggest that, for T2I diffusion models, decoders are more adept at capturing richer and more explicit semantic information, while encoders can be effectively shared across decoders from diverse time steps. Based on these observations, we introduce the first Time-independent Unified Encoder (TiUE) for the student model UNet architecture, which is loop-free image generation approach for distilling T2I diffusion models. Using one-pass scheme, TiUE shares encoder features across multiple decoder time steps, enabling parallel sampling and significantly reducing inference time complexity. In addition, we incorporate KL divergence term to regularize noise prediction, which enhances the perceptual realism and diversity of the generated images. Experimental results demonstrate that TiUE outperforms state-of-the-art methods, including LCM, SDTurbo, and SwiftBrushv2, producing more diverse and realistic results while maintaining the computational efficiency. https://github.com/sen-mao/Loopfree 1. Introduction Recently, diffusion models [9, 19, 67] have achieved remarkable breakthroughs, representing significant advance in the field of generative models. It is widely applied in di- *The corresponding author. verse applications, including image generation [8, 19, 21, 39, 58, 68], video synthesis [12, 28, 47, 79], image editing [15, 35, 51, 76, 84], T2I personalization [2, 13, 31, 60, 77], etc. Despite their considerable success, diffusion models are not exempt from limitations. One of the main limitations of diffusion models is the slow inference speed. This limitation affects the scalability and application of diffusion models in real-time environments. To address the limitations of T2I diffusion models during inference, current approaches achieve acceleration through either training-free techniques [1, 66, 70] or training-based methods [45, 62, 64]. Training-free methods primarily focus on improving sampling solvers [42, 67] or using caching mechanisms to speed up individual sampling steps [34, 49]. Although these methods can reduce the number of sampling steps, even strong solvers still require more than 10 steps [43], limiting their efficiency due to their training-free nature. On the other hand, training-based approaches distill student generator [44, 64, 69, 86] from pretrained T2I diffusion models. Methods like LCM [44], SD-Turbo [64] and SwiftBrushv2 [7] can generate high-fidelity images with only few sampling steps (e.g., 4 steps for LCM and 1 step for SwiftBrushv2). However, these approaches still struggle to produce high-quality and diverse images in the single step generation. In this case, LCM produces low-quality generations, while SD-Turbo and SwiftBrushv2 exhibit reduced diversity, as observed in Fig. 3, Fig. 6 and Fig. 7. Existing methods overlook redundant computations inherent in the UNet, an area that has been more thoroughly explored in training-free acceleration techniques [34, 49]. For example, Faster Diffusion [34] leverages feature similarity across adjacent inference steps by sharing the encoder across multiple decoder steps. We also observe that the correlation between image generation quality and encoder feature similarity remains effective up to certain threshold Figure 1. The correlation between image generation quality (Fig.a-b) and encoder feature similarity (Fig.c). Above certain threshold of steps, such as 15 steps in SD2.1, the model maintains image generation quality (Fig.a-b) while the features show high similarity (Fig.c). Below this threshold, feature similarity deteriorates along with worse generation quality, accompanied by degradation in image generation quality. Furthermore, the encoder features consistently exhibit higher similarity than the decoder across all sampling steps (Fig.c). the student UNet decoder processes across multiple time steps with shared weights (Fig. 4d) when distilling latent diffusion models. TiUE achieves loop-free image generation, eliminating the need for iterative noisy latent processing by sharing encoder features across multi-step decoders. This property supports decoder parallelization across diverse time steps, reducing inference time. Additionally, by using the decoder in multiple steps to extract more explicit semantic information from different time steps, TiUE produces more diverse and realistic results than prior methods in the one-step generation setup, with similar time cost to previous one-step T2I models, such as LCM [44], SDTurbo [64], and SwiftBrushv2 [7]. To further eliminate the need for large image datasets (either real or synthesized by the teacher model) as required by previous methods [53], we introduce KullbackLeibler (KL) divergence term to regularize the UNets noise prediction, ensuring that it remains close to standard normal distribution. While prior methods primarily focus on diffusion distillation loss, they often overlook the importance of maintaining the UNets Gaussian output prior. We empirically find that the KL divergence helps to enhance both perceptual realism and generation diversity. In summary, our work makes the following contributions: We introduce the first Time-independent Unified Encoder (TiUE) architecture, which is loop-free distillation approach and eliminates the need for iterative noisy latent processing while maintaining high sampling fidelity with time cost comparable to previous one-step methods. We propose novel diffusion distillation method that enables parallel sampling in diffusion models. Additionally, we incorporate KL divergence to regulate the output distribution of the student network, enhancing both generation diversity and perceptual realism. In both qualitative and quantitative experiments, our method outperforms strong baselines such as LCM[44], SD-Turbo[64], and SwiftBrushv2 [7], producing more diverse and realistic generative results. Figure 2. The generated images and the corresponding UNet decoder features using different time steps. The images generated in one step mainly focus on structural information, with minimal amount of semantic information present in the feature layers (Left). By comparison, the decoder across multiple time-steps contain richer and more explicit semantic information in the feature level (Right), leading to better image generation. as 15 steps for the SD model [58]1. Above this threshold, high-quality images are associated with high encoder feature similarity. Below this threshold, the image generation quality deteriorates (Fig. 1(a-b)) while the encoder feature exhibits variations (Fig. 1(c)). Additionally, the decoder feature always shows much higher variations than encoder (Fig. 1-(c)). Furthermore, we argue that this phenomenon also exists in distilled few-step diffusion models. As observed in Fig. 2, the decoders of the 4-step generative models retrieve richer semantic information across diverse time steps. We hypothesize that this is the reason 4-step diffusion models generally achieve better generation performance compared to their 1-step counterparts. This hypothesis is also highly aligned with the finding in PnP [72] and DIFT [71]. Based on these findings, we propose the first Timeindependent Unified Encoder (TiUE) design for the UNet in distilled student models, which inherits the same structure as the SD model. Specifically, the one-pass scheme means that the student UNet encoder is used only once, while 1This observation is based on statistical experiments on 100 generated images. Figure 3. Generated high-fidelity 5122 images by the one-step model distilled with our method TiUE. Compared with the baselines, our method produces higher image quality and more diverse results. Here the diversity means the degree of variation of different output images with same semantic information when given the same prompt and different seed values. SwiftBrushv2 [7], which is initialized with SDTurbo [64], achieves T2I generation with limited diversity as the SD-Turbo model. Figure 4. Previous methods use the time-dependent UNet encoder- (a) decoder design and differ from the distillation losses. LCM [44] updates with consistency distillation loss Lcd. (b) SDTurbo [64] is trained with adversarial loss Ladv and score distillation loss Lsds. (c) SwiftBrushv1/v2 [7, 53] distills with variational score distillation Lvsd for one-step model. (d) Our method TiUE first proposes the Time-independent Unified Encoder (TiUE) design to achieve the one-pass loop-free generation. TiUE distills the T2I model with variational score distillation Lvsd and KL divergence Lkl. The loop-free nature of the architecture allows the decoder blocks run in parallel across various time steps, thus saving much inference time. 2. Related Work Based on whether the acceleration methods require additional training, they can be categorized into training-based acceleration and training-free acceleration. 2.1. Training-Free Acceleration The first main method of training-free acceleration optimizes the sample efficiency [11, 55, 85]. The sampling process can be performed by solving reverse SDEs or ODEs, leading to extensive research on improved numerical solvers, including DDIM [67], DPM [42], and DPM++ [43]. Another key branch of training-free methods focuses on improving structural efficiency during each sampling step, that includes network pruning [10, 70, 82], quantization [3, 36, 66], parallelization [5, 33, 75] and token reduction [1, 29, 41]. More recently, wide variety of methods utilize the cache mechanisms to achieve training-free acceleration. These methods temporarily store information that can be reused to speed up computations. For example, DeepCache [49] reduces redundant computations in Stable Diffusion by reusing intermediate features of low-resolution layers in the UNet. Faster Diffusion [34] accelerates the sampling process of diffusion models by caching UNet encoder features across timesteps, allowing the model to skip encoder computations at certain steps. Similar approaches have been applied to DiT-based T2I diffusion models, such as -DiT [4] and FORA [65]. However, training-free methods [22, 48] generally apply the same caching solution to all tokens and still require multiple inference steps. This often leads to significant degradation in generation quality and poorer time complexity compared to training-based approaches. 2.2. Training-Based Acceleration Several attempts have been made to accelerate the sampling process of diffusion models by introducing additional training beyond the base diffusion model. One of the most representative methods is the Consistency Models [44, 69]. Following works [73, 86] further improve the performance of these consistency models. More recently, distillation techniques [18, 46, 80] have been applied to diffusion models, allowing faster training of student models by pre-trained teachers [14, 40, 53, 64]. Early studies [50, 62] used progressive distillation to gradually reduce the sampling steps required for student diffusion models. However, the slow sampling process of the pre-trained teacher limits the training efficiency. To address this, recent works [14, 53, 64] have proposed various bootstrapping techniques. For example, Boot [14] employs bootstrapping between two consecutive sampling steps, enabling image-free distillation. SD-Turbo [64] introduces discriminator combined with score distillation loss to improve performance. Most of these methods depend on image-text pair datasets for training, requiring substantial data alignment between visual and textual features. In contrast, SwiftBrush [53] adapts variational score distillation. SwiftBrush achieves the first image-free training, avoiding the need for paired datasets. SwiftBrushv2 [7] further augments this method by merging two distilled student models. However, existing training-based methods largely overlook feature similarities and redundant computations within UNet encoder-decoder (Enc-Dec) architectures, which have been widely explored in training-free acceleration techniques. Unlike conventional time-dependent Enc-Dec designs in popular T2I diffusion models and training-based accelerations, we propose the first Time-independent Unified Encoder (TiUE) model design for T2I diffusion models. In this approach, the encoder features are shared with the decoder across various time steps. 3. Method Our goal is to distill pretrained T2I diffusion model, typically latent diffusion model (Sec. 3.1), into fast, loop-free student generator. The student model should maintain both the high quality and diversity of generated images. Rather than adopting the conventional Timedependent UNet encoder-decoder design, we introduce the Time-independent Unified Encoder (TiUE), which utilizes the UNet encoder in single pass while the decoder operates in multiple time steps. Additionally, we incorporate KullbackLeibler (KL) divergence term to ensure that the UNet output adheres to normal Gaussian distribution, preserving the fundamental diffusion model property that added noise follows normal distribution. The procedure for our proposed method is introduced in Sec. 3.2 and Fig. 5. 3.1. Preliminaries Latent Diffusion Model. The denoising network ϵψ takes as input text y, latent code zt and time embedding to predict noise, resulting in latent zt1. Using the DDIM scheduler [67], the formula is: (cid:113) αt1 αt (cid:113) 1 αt (cid:17) (cid:16)(cid:113) 1 αt1 1 zt + αt1 1 zt1 = ϵψ(zt, t, y), (1) where αt is predefined scalar function at time-step (t = T, ..., 1). The denoising network is commonly UNet [59] consisting of an encoder and decoder D. Variational Score Distillation (VSD). Score Distillation Sampling (SDS) is an optimization method that involves distilling pretrained diffusion models ϵψ(xt, t, y) for textto-3D generation [56, 74]. It optimizes the parameters of the generator g(θ), associated with specific text y, using loss function with an approximated gradient: θLSDS =Et,ϵ (cid:104) w(t)(ϵψ(xt,t,y)ϵ) g(θ) θ (cid:105) (2) where w(t) is time-dependent weighting function and ϵN (0, I), xt = αtg(θ) + σtϵ, tU(0.02T, 0.98T ). However, SDS suffers from over-saturation and oversmoothing when generating images with both diversity and fidelity. VSD [78] attributes this problem to the use of single-point Dirac distribution as the variational distribution. To avoid this, VSD optimizes for the whole distribution through relevant gradients: θLV SD=Et,ϵ (cid:104) w(t)(ϵψ(xt, t, y) ϵϕ(xt, t, y)) (cid:105) g(θ) θ (3) VSD employs LoRA [20] of the pretrained model ϵψ(xt, t, y) to parameterize ϵϕ(xt, t, y). It is trained with the standard diffusion loss as the mean-square error: min ϵϕ Et,ϵ ϵϕ(xt, t, y) ϵ2 2 (cid:125) (cid:124) (cid:123)(cid:122) Lmse (4) To ensure that ϵϕ(xt, t, y) aligns with the current distribution of the generator g(θ), VSD employs alternate optimization of ϵϕ and θ. Note that, for the sake of simplicity, we omit the angle-related variables required for text-to-3D. VSD extends the variational formulation of SDS, providing an explanation and addressing issues that are observed with SDS. Similar to SwiftBrush [53], we use VSD to distill text-to-image pretrained model. 3.2. TiUE: Time-independent Unified Encoder Inspired by the observation in Faster Diffusion [34] that encoder features are sharing similarity across adjacent inference steps, we explore the correlation between image generation quality and encoder feature similarity in SD model [58] in Fig. 1. With inference steps over 15 steps, high-quality images are associated with high encoder feature similarity. Below this threshold, the image generation quality deteriorates while the encoder feature exhibits variations. This phenomenon also exists in distilled few-step diffusion models, as observed in Fig. 2. Based on these observations, we propose the Time-independent Unified Encoder (TiUE) model design for distilling T2I diffusion models. The one-pass scheme means that the student UNet encoder is used only once, while the student UNet decoder processes across multiple time steps (Fig. 4d) when distilling latent diffusion models. Fig. 5 shows our knowledge distillation framework. ψ and an TiUE student generator ϵSG Our distillation framework is composed of three networks: pretrained teacher generator ϵT ψ , an SD-LoRA generator ϵLG . Each generator consists of an encoder and decoder D. The student generator ϵSG takes as input random noise ϵ, text y, time-step and t, outputs latent representation ϵSG (ϵ, K, t, y) (t = K, ..., 1). is the input time step of θ the student generator ϵSG encoder, and (t = K, ..., 1) is θ θ θ Figure 5. Loop-free distillation based on our Time-independent Unified Encoder (TiUE) architecture. Our framework is composed of three networks: student generator ϵSG is only used once while passing through the decoder for time-steps. The student network is updated with both variational score distillation loss Lvsd and KL divergence loss Lkl. We also optimize the SD-LoRA ϵLG ψ by diffusion loss. The SD-LoRA teacher and the student model are updated alternately, following the previous methods [53, 78]. , pretrained teacher SD generator ϵT ψ and SD-LoRA generator ϵLG ψ . The encoder of ϵSG θ θ the input time-step of the student generator ϵSG The Eq. (1) can be rewritten as: θ decoder. (cid:113) αt1 αt (cid:113) 1 αt (cid:17) (cid:16)(cid:113) 1 αt1 1 zt + αt1 1 ϵSG θ zt1 = (ϵ, K, t, y). (5) By this means, our method TiUE also adheres to the original ODE trajectory [42, 67]. Furthermore, at time step (t = K-1, ..., 1), the decoder inputs do not depend on the encoder outputs at time step t. Instead, it relies on the encoder output at the key time step, as shown in Fig. 5. This allows us to perform parallel denoising to generate the latent representation z0. The final latent representation z0 is fed into both the pretrained teacher generator ϵT ψ and the SD-LoRA generator ϵLG ψ . We update the student generator using Eq. (3) in combination with Eq. (6), and then update the SD-LoRA generator with Eq. (4). With all these techniques, our approach TiUE involves an alternating strategy for training both the student generator and SD-LoRA. With the proposed strategy, we have two advantages. (1) When using the UNet encoder at single step and the UNet decoder across multiple time-steps, we are able to predict the latent noise in parallel, achieving loop-free image generation and significantly reducing inference time. (2) Although we only use the encoder in single step, we preserve multistep UNet decoders, which play crucial role in generating high-quality images [71, 72, 83, 84]. Therefore, we are able to produce more diverse and realistic images. KL divergence loss. Current distillation methods [44, 64] commonly begin with initializing the student model ϵSG with the pretrained weight of the T2I teacher model. This initialization indicates that the UNet output prior follows Gaussian distribution (see Eq. (4)). To fully follow the pretrained model to preserve this characteristic, one intuitive way is to take advantage of the diffusion loss (see Eq. (4)) to train the student model. However, we expect to avoid acθ cess to significant amount of images (real data or synthetically generated) for training. Instead, we introduce KL divergence to regulate the UNet output distribution during the training of student generator ϵSG : θ LKL = [DKL(ϵSG where DKL(pq) = (cid:82) p(z) log p(z) θ q(z) dz. (ϵ, T1, t, y) (0, I))], (6) θ Finally, we train the student model ϵSG using the VSD (Eq. (3)) and KL divergence (Eq. (6)). Meanwhile, we use the denoising loss (Eq. (4)) to update the SD-LoRA model ϵLG ψ . Note that we alternately train both the student model and the SD-LoRA network ϵLG ϵSG ψ while freezing the textθ to-image teacher ϵT ψ . 4. Experiments Evaluation Datasets and Metrics. We compare our method TiUE with the following baselines: Instaflow [40], LCM [44], SwiftBrush [53], SwiftBrushv2 [7] and SDTurbo [64] across the zero-shot text-to-image benchmark COCO 2014 and 2017 [38]. In COCO 2014, we randomly select 30K prompts as the conventional evaluation protocol [26, 58, 61, 63], and feed them into the diffusion model to obtain 30K generated images. In COCO 2017, we obtain 5K generated images using the provided 5K prompts. We use the Frechet Inception Distance (FID) [17] metric to assess the visual quality of the generated images and the CLIPScore(CLIP) [16] to measure the consistency between the image content and the text prompt. Here, we use ViTB/32 as the backbone to evaluate CLIPScore. We also use Precision and Recall [32] to quantify fidelity and diversity of generated images. Additionally, we compute Density and Coverage metrics [52], along with FID, to assess the diversity of the generated images on the AFHQ [6], CelebAHQ [27] datasets and the prompt datasets DrawBench [61] and PartiPrompts [81]."
        },
        {
            "title": "Base\nModel",
            "content": "COCO2014-30K COCO2017-5K Inference"
        },
        {
            "title": "Training Data",
            "content": "FID CLIP Precision Recall F1 FID CLIP Precision Recall F1 Time (ms) Memory (GB) Size"
        },
        {
            "title": "Image\nFree",
            "content": "SD1.5 [58] (cfg=7.5) SD1.5 [58] (cfg=4.5) SD2.1 [58] (cfg=7.5) SD2.1 [58] (cfg=4.5) GigaGAN [26] GAN 1 50 860M 16.08 0.325 50 860M 9.90 0.322 50 865M 16.10 0.328 50 865M 12.22 0.325 1.0B 9.24 0.325 InstaFlow [40] LCM [44] SD-Turbo [64] SwiftBrush [53] SwiftBrushv2 [7] LCM [44] SD-Turbo [64] Ours SD1. SD2.1 1 0.9B 13.78 0.288 1 860M 132.09 0.230 1 865M 19.51 0.331 1 865M 17.20 0.301 1 865M 15.98 0.326 SD1.5 4 860M 23.21 0.262 SD2.1 4 865M 16.14 0.335 SD2.1 1 865M 13.09 0.313 0.717 0.727 0.723 0.734 0.724 0.654 0.109 0.758 0.672 0.782 0.666 0.633 0. 0.527 0.607 23.39 0.326 0.585 0.648 19.87 0.323 0.489 0.583 25.40 0.328 0.526 0.614 22.24 0.298 0.547 0.623 0.521 0.580 19.00 0.293 0.194 0.140 143.73 0.229 0.458 0.571 29.35 0.331 0.458 0.545 27.18 0.314 0.457 0.577 26.28 0.326 0.346 0.455 40.37 0.303 0.394 0.468 26.14 0.335 0.622 0.628 23.11 0.313 0.776 0.764 0.769 0.788 0.729 0.118 0.786 0.729 0.816 0.713 0.694 0.697 0.587 0.668 2503.0 0.649 0.702 2503.0 0.561 0.649 2244.2 0.606 0.685 2244.2 0.613 0.666 111.3 0.291 0.168 236.2 0.445 0.568 140.0 0.527 0.612 95.0 0.543 0.652 139. 0.460 0.559 592.3 0.375 0.487 272.2 0.668 0.682 164.7 4.04 4.04 3.89 3.89 3.99 5.88 3.86 3.85 4.91 5.88 3.86 4.98 5B 5B 5B 5B 2.7B 3.2M 12M unk. 1.4M 1.4M 12M unk. 1.4M A100 Days 4783 4783 8332 8332 6250 183.2 1.3 unk. 4.1 24.1 1.3 unk. 3.9 Table 1. Comparison of our distillation method against other works. Inference Time (ms) and Memory (GB). indicates that we report results using the provided official code and pretrained models. denotes that we re-implemented the work and are providing the scores. indicates that we report results using the provided generated images. unk. denotes unknown. The best and second-best scores are highlighted in bold and underlined, respectively, with both the parameter count and training data size being below the billion level. Figure 6. Qualitative comparison to state-of-the-art few-step distillation methods. denotes that we re-implemented the work. Our model outperforms all other few-step samplers in terms of quality within one-pass, approaching the performance of the SD model with 50 steps. Implementation Details. Similar to prior works [44, 53, 64], we use the same network SD2.1-base as the teacher model, and initialize all student parameters using pretrained teacher model. During the training process for the SD-LoRA generator, we apply learning rate of 1e-3, LoRA rank of 64, and an alpha value of 108, following SwiftBrush [53]. Simultaneously, the learning rate for the student generator is configured at 1e-6, incorporating the exponential moving average (EMA). We set = 4, and employ the Adam optimizer [30] to train both the student generator and SD-LoRA generator. In this context, we leverage 1.4M captions sourced from the extensive textimage dataset JourneyDB [54]. See more detailed implementation information in Appendix A. Figure 7. Both SD-Turbo and SwiftBrushv2 tend to generate results with similar scenery and style when given the same prompt, resulting in lack of diversity. denotes that we re-implemented the work. 4.1. Quantitative and qualitative results We conduct the comparison with state-of-the-art one-step distillation methods, including both image-dependent [40, [7, 53] techniques. As shown 44, 64] and image-free in Tab. 1 (the last row), despite being trained solely on text data, our approach TiUE achieves the best and secondbest FID scores in two benchmarks and demonstrated similar performance in terms of CLIPScore score compared to SwiftBrushv1/v2. For both the inference time and the memory usage, LCM is nearly twice as much as ours, while SDTurbo and SwiftBrushv1/v2 have slight advantage over ours. Since our approach use encoder once, and does the decoder across time-steps (i.e., K=4 in Tab. 1 (the last row)), to ensure more equitable comparison, we further evaluate our results against both LCM and SD-Turbo in 4steps sampling (Tab. 1 (the eleventh and twelfth rows)). Our approach outperforms LCM and SwiftBrush across all evaluation metrics, except for Precision, including FID, CLIPScore, and Recall. Furthermore, our method obtains comparable performance to SD-Turbo with 4-steps while using less inference time. Fig. 6 shows the generation of distillation methods with various steps. Our result outperforms those of all other one-step generators in terms of quality. Our approach approaches the performance of the SD model with 50 steps. We observe that both SD-Turbo and SwiftBrushv2 tend to generate images with limited diversity (Fig. 7 (the first and second rows)), while they generate high-quality results. Obviously, we are able to produce more realism and diverse results (Fig. 7 (the third row), and close to the ones of SD (Fig. 7 (the last row)), indicating our advantage over the baselines. See Appendix for additional results. We also report the quantitative results to evaluate the density and coverage metrics [52]. We use 200 prompts from DrawBench, for each prompt we generate 100 images Figure 8. User Study (left). Comparison of the predicted noise distribution between SD and our method (right). by using difference seeds. Similarly, we use 1632 prompts from PartiPrompts, generating 10 images per prompt. During the evaluation, we take the images synthesized by SD2.1 as the ground truth. For the real image we use both AFHQ [6] and CelebA-HQ [27]. As reported in Tab. 2, compared to all baselines, our method achieves the best scores in terms of Density and Coverage on all three datasets, except for the Density in AFHQ and Coverage in PartiPrompts. These results indicate that we produce images with high degree of diversity. User Study. We conducted user study, as shown in Fig. 8 (left), and asked participants to select their preferred results (image quality, prompt alignment, and diversity). We compared 54 users (30 image sets/user) using multiple choices. Experimental results indicate that our method has significant results, indicating our large advantage over the baselines. Furthermore, our method achieves better results comparing to the baseline methods in terms of image quality and prompt alignment. 4.2. Additional Analysis The impact of KL diverse regularization. We regularize the student UNet output to close Gaussian noise, aiming to use fully the pretrained model property. To evaluate the effectiveness of KL diverse regularization, we analyze the distribution of the UNet decoder output values and fit curve to it. As depicted in Fig. 8 (right), without LKL our approach exhibits noticeable deviations from the Gaussian distribution (green and red curves), whereas incorporating LKL the UNet output curve is more closer to Gaussian distribution (blue and red curves), resulting in the generation of high-quality images. Tab. 3 further reports the quantitative comparison of FID and CLIPScore, demonstrating that our method, combined with KL diverse regularization, exhibits significant improvement. Text prompt Interpolation. Fig. 9 showcases the interpolation process, where each row maintains fixed input noise while the text embedding transitions smoothly between two distinct text prompts. Even when presented with previously unseen interpolated text embeddings, our model demonstrates remarkable semantic consistency, producing images that align continuously with the interpolated prompts and"
        },
        {
            "title": "AFHQ",
            "content": "CelebA-HQ"
        },
        {
            "title": "PartiPrompts",
            "content": "FID DensityCoverage FID DensityCoverage FID DensityCoverage FID DensityCoverage"
        },
        {
            "title": "Image\nFree",
            "content": "SD1.5 [58] (cfg=7.5) SD2.1 [58] (cfg=7.5) 50 47.16 50 51.67 0.066 0.053 InstaFlow [40] LCM [44] SD-Turbo [64] SwiftBrush [53] SwiftBrushv2 [7] LCM [44] SD-Turbo [64] Ours SD1. SD2.1 1 51.97 0.058 1 155.63 0.012 0.142 1 77.75 0.039 1 67.60 0.110 1 64.99 SD1.5 4 78.00 SD2.1 4 77.23 SD2.1 1 54.48 0.054 0.011 0.068 0.030 0.022 0.029 0.033 0.033 0.014 0. 0.008 0.005 0.071 93.94 89.57 0.053 0.018 131.99 0.026 165.74 0.001 146.22 0.047 144.03 0.014 131.89 0.055 122.44 0.045 193.08 0.013 116.82 0.116 0.013 0. 0.007 0.004 0.006 0.002 0.012 0.045 0.001 0.068 11.95 0 0.510 1 0.223 25.08 120.98 0.058 0.597 25.75 0.402 21.48 18.57 0.682 46.23 27.80 21. 0.183 0.281 0.685 0.622 1 0.337 0.014 0.488 0.441 0.597 0.187 0.371 0.616 7.36 0 0.730 17.64 0.457 95.65 0.095 17.40 0.770 14.43 0.579 11.32 0.850 26.84 0.512 22.84 0.500 16.28 0.852 0.887 1 0.670 0.072 0.775 0.737 0.865 0.575 0.648 0.840 Table 2. Quantitative comparison of our distillation method with other approaches based on FID, Density, and Coverage metrics to assess diversity. indicates that we report results using the provided official code and pretrained models. denotes that we re-implemented the work and are providing the scores. The best and second-best numbers are marked with bold and underlined respectively. Metrics FID CLIPPrecisionRecallDensityCoverage Method Ours w/o LKL 14.90 0.311 13.09 0."
        },
        {
            "title": "Ours",
            "content": "0.608 0.634 Table 3. Ablation study by quantitative evaluation for the KL diverse regularization LKL. 0.617 0.683 0.621 0.622 0.631 0.652 Figure 10. Results of different time-steps with K=4 trained student model. The well-trained student model with time-steps K=4 (the second column) serves as good starting point for students set at other time-steps (e.g., 2, 8, 16, 32, and 50). in Fig. 10 sampled with the DDIM scheduler [67] using different time-steps (e.g., 2, 8, 16, 32, and 50), the generated images still contain significantly semantic information, while it loses the image quality. This experiment demonstrates that our method greatly inherits the capabilities of the pretrained model. We expect that fine-tuning the welltrained student model with = 4 for several iterations will produce superior results at larger time-steps. 5. Conclusions In this work, we introduce the Time-independent Unified Encoder (TiUE), major advancement in T2I diffusion distillation that improves inference speed and generation quality. Our findings reveal that decoders effectively capture richer semantic information, while encoders can be shared across multiple decoders from different time steps. This inspires novel one-pass scheme that enables efficient encoder utilization, achieving loop-free image generation and reduced time complexity. Additionally, incorporating KullbackLeibler (KL) divergence enhances the noise prediction process, ensuring high perceptual realism and diversity in outputs. Extensive experiments show that TiUE surpasses state-of-the-art methods like LCM, SD-Turbo, and SwiftBrushv1/v2, yielding more diverse and realistic results. Figure 9. Interpolation between text prompts. For example, interpolation between two animals in the first two rows with the Interpolation beprompts ... cat (left) and ... dog (right). tween two actions in last two rows with the prompts ... reading book (left) and ... sipping tea (right). maintain high-quality generative outputs. Hyperparameter K. The parameter decides how many decoders to be used, which influences the quality and diversity. For both LCM and SD-Turbo, they normally use 4-steps to generate satisfactory results, which inspires us to set K=4 as the main experimental setup. Although we do not traverse all possible values, we still achieve better results (e.g., FID) with K= 4. Diverse sampling steps during inference. Although the student network is optimized with both VSD and KLdivergence losses without the denoising loss, we are still able to sample with various time steps. As examples"
        },
        {
            "title": "Acknowledgements",
            "content": "This work was supported by NSFC (NO. 62225604), Youth Foundation (62202243), and Shenzhen Science and Technology Program (JCYJ20240813114237048). We acknowledge the support of the project PID2022143257NB-I00, funded by the Spanish Government through MCIN/AEI/10.13039/501100011033 and FEDER, and the Generalitat de Catalunya CERCA Program. We acknowledge Science and Technology Yongjiang 2035 key technology breakthrough plan project (2024Z120). Computation is supported by the Supercomputing Center of Nankai University (NKSC)."
        },
        {
            "title": "References",
            "content": "[1] Daniel Bolya and Judy Hoffman. Token merging for fast stable diffusion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4598 4602, 2023. 1, 3 [2] Muhammad Atif Butt, Kai Wang, Javier Vazquez-Corral, and Joost van de Weijer. Colorpeel: Color prompt learning with In diffusion models via color and shape disentanglement. ECCV, 2024. 1 [3] Lei Chen, Yuan Meng, Chen Tang, Xinzhu Ma, Jingyan Jiang, Xin Wang, Zhi Wang, and Wenwu Zhu. Q-dit: Accurate post-training quantization for diffusion transformers. CVPR, 2025. 3 [4] Pengtao Chen, Mingzhu Shen, Peng Ye, Jianjian Cao, Chongjun Tu, Christos-Savvas Bouganis, Yiren Zhao, and Tao Chen. Delta-dit: training-free acceleration method tailored for diffusion transformers. arXiv preprint arXiv:2406.01125, 2024. 3 [5] Zigeng Chen, Xinyin Ma, Gongfan Fang, Zhenxiong Tan, and Xinchao Wang. Asyncdiff: Parallelizing diffusion models by asynchronous denoising. NeurIPS, 2024. 3 [6] Yunjey Choi, Youngjung Uh, Jaejun Yoo, and Jung-Woo Ha. Stargan v2: Diverse image synthesis for multiple domains. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 81888197, 2020. 5, 7, [7] Trung Dao, Thuan Hoang Nguyen, Thanh Le, Duc Vu, Khoi Nguyen, Cuong Pham, and Anh Tran. Swiftbrush v2: Make your one-step diffusion model better than its teacher. ECCV, 2024. 1, 2, 3, 4, 5, 6, 7, 8 [8] DeepFloyd. DeepFloyd IF. https://www.deepfloyd. ai/deepfloyd-if, 2023. 1 [9] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in neural information processing systems, 34:87808794, 2021. 1 [10] Gongfan Fang, Xinyin Ma, and Xinchao Wang. Structural pruning for diffusion models. Advances in neural information processing systems, 36, 2024. 3 [11] Zhengyang Geng, Ashwini Pokle, and Zico Kolter. OneIn step diffusion distillation via deep equilibrium models. Thirty-seventh Conference on Neural Information Processing Systems, 2023. [12] Michal Geyer, Omer Bar-Tal, Shai Bagon, and Tali Dekel. Tokenflow: Consistent diffusion features for consistent video editing. ICLR, 2024. 1 [13] Jing Gu, Yilin Wang, Nanxuan Zhao, Tsu-Jui Fu, Wei Xiong, Qing Liu, Zhifei Zhang, He Zhang, Jianming Zhang, HyunJoon Jung, and Xin Eric Wang. Photoswap: Personalized subject swapping in images, 2023. 1 [14] Jiatao Gu, Shuangfei Zhai, Yizhe Zhang, Lingjie Liu, and Josh Susskind. Boot: Data-free distillation of denoising diffusion models with bootstrapping. International Conference on Machine Learning, 2023. 3 [15] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Prompt-to-prompt image editing with cross attention control. ICLR, 2023. 1 [16] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. Clipscore: reference-free evaluation metric for image captioning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 75147528, 2021. 5, 1 [17] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. In NeurIPS, pages 66266637, 2017. 5, [18] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the Knowledge in Neural Network. NIPS Deep Learning Workshop, 2014. 3 [19] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in Neural Information Processing Systems, 33:68406851, 2020. 1 [20] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. ICLR, 2022. 4 [21] Taihang Hu, Linxuan Li, Joost van de Weijer, Hongcheng Gao, Fahad Shahbaz Khan, Jian Yang, Mingming Cheng, Kai Wang, and Yaxing Wang. Token merging for trainingIn free semantic binding in text-to-image synthesis. NeurIPS, 2024. 1 [22] Yushi Huang, Zining Wang, Ruihao Gong, Jing Liu, Xinjie Zhang, Jinyang Guo, Xianglong Liu, and Jun Zhang. Harmonica: Harmonizing training and inference for better feature cache in diffusion transformer acceleration. arXiv preprint arXiv:2410.01723, 2024. 3 [23] Mingu Kang and Jaesik Park. Contragan: Contrastive learning for conditional image generation. NeurIPS, 2020. 1 [24] Minguk Kang, Woohyeon Shim, Minsu Cho, and Jaesik Park. Rebooting acgan: Auxiliary classifier gans with stable training. In Neural Information Processing Systems, 2021. [25] Minguk Kang, Joonghyuk Shin, and Jaesik Park. Studiogan: taxonomy and benchmark of gans for image synthesis. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2023. 1 [26] Minguk Kang, Jun-Yan Zhu, Richard Zhang, Jaesik Park, Eli Shechtman, Sylvain Paris, and Taesung Park. Scaling up GANs for Text-to-Image Synthesis. CVPR, 2023. 5, 6, 1, 2 [27] Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of gans for improved quality, stability, and variation. ICLR, 2018. 5, 7, 1 [28] Levon Khachatryan, Andranik Movsisyan, Vahram Tadevosyan, Roberto Henschel, Zhangyang Wang, Shant Navasardyan, and Humphrey Shi. Text2video-zero: Textto-image diffusion models are zero-shot video generators. ICCV, 2023. 1 [29] Minchul Kim, Shangqian Gao, Yen-Chang Hsu, Yilin Shen, and Hongxia Jin. Token fusion: Bridging the gap between In Proceedings of the token pruning and token merging. IEEE/CVF Winter Conference on Applications of Computer Vision, pages 13831392, 2024. 3 [30] Diederik P. Kingma and Jimmy Ba. Adam: Method for Stochastic Optimization. ICLR, 2015. 6, [31] Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, and Jun-Yan Zhu. Multi-concept customization of text-to-image diffusion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 19311941, 2023. 1 [32] Tuomas Kynkaanniemi, Tero Karras, Samuli Laine, Jaakko Lehtinen, and Timo Aila. Improved precision and recall metric for assessing generative models. Advances in neural information processing systems, 32, 2019. 5, 1 [33] Muyang Li, Tianle Cai, Jiaxin Cao, Qinsheng Zhang, Han Cai, Junjie Bai, Yangqing Jia, Kai Li, and Song Han. Distrifusion: Distributed parallel inference for high-resolution diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7183 7193, 2024. 3 [34] Senmao Li, Taihang Hu, Joost van de Weijer, Fahad Shahbaz Khan, Tao Liu, Linxuan Li, Shiqi Yang, Yaxing Wang, MingMing Cheng, and Jian Yang. Faster diffusion: Rethinking the role of unet encoder in diffusion models. In Advances in Neural Information Processing Systems, 2024. 1, 3, 4, 2 [35] Senmao Li, Joost van de Weijer, Taihang Hu, Fahad Shahbaz Khan, Qibin Hou, Yaxing Wang, and Jian Yang. Stylediffusion: Prompt-embedding inversion for text-based editing. Computational Visual Media Conference, 2024. 1 [36] Xiuyu Li, Yijiang Liu, Long Lian, Huanrui Yang, Zhen Dong, Daniel Kang, Shanghang Zhang, and Kurt Keutzer. In Proceedings Q-diffusion: Quantizing diffusion models. of the IEEE/CVF International Conference on Computer Vision, pages 1753517545, 2023. 3 [37] Shanchuan Lin, Anran Wang, and Xiao Yang. SdxlProgressive adversarial diffusion distillation. lightning: arXiv preprint arXiv:2402.13929, 2024. 2, 3, 4 [38] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer VisionECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part 13, pages 740755. Springer, 2014. 5 [39] Tao Liu, Kai Wang, Senmao Li, Joost van de Weijer, Fahad Shahbaz Khan, Shiqi Yang, Yaxing Wang, Jian Yang, and Ming-Ming Cheng. One-prompt-one-story: Free-lunch consistent text-to-image generation using single prompt. CVPR, 2025. 1 diffusion-based text-to-image generation. ICLR, 2024. 3, 5, 6, 7, 8, 1, 2 [41] Jinming Lou, Wenyang Luo, Yufan Liu, Bing Li, Xinmiao Ding, Weiming Hu, Jiajiong Cao, Yuming Li, and Chenguang Ma. Token caching for diffusion transformer acceleration. arXiv preprint arXiv:2409.18523, 2024. [42] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver: fast ode solver for diffusion probabilistic model sampling in around 10 steps. Advances in Neural Information Processing Systems, 35:57755787, 2022. 1, 3, 5 [43] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver++: Fast solver for guided sampling of diffusion probabilistic models. arXiv preprint arXiv:2211.01095, 2022. 1, 3 [44] Simian Luo, Yiqin Tan, Longbo Huang, Jian Li, and Hang Zhao. Latent consistency models: Synthesizing highresolution images with few-step inference. arXiv preprint arXiv:2310.04378, 2023. 1, 2, 3, 5, 6, 7, 8 [45] Simian Luo, Yiqin Tan, Suraj Patil, Daniel Gu, Patrick von Platen, Apolinario Passos, Longbo Huang, Jian Li, and Hang Zhao. Lcm-lora: universal stable-diffusion acceleration module. arXiv preprint arXiv:2311.05556, 2023. 1, 2, 3 [46] Weijian Luo, Tianyang Hu, Shifeng Zhang, Jiacheng Sun, Zhenguo Li, and Zhihua Zhang. Diff-instruct: universal approach for transferring knowledge from pre-trained diffusion models. NeurIPS, 36, 2023. 3 [47] Zhengxiong Luo, Dayou Chen, Yingya Zhang, Yan Huang, Liang Wang, Yujun Shen, Deli Zhao, Jingren Zhou, and Tieniu Tan. Videofusion: Decomposed diffusion modIn Proceedings of els for high-quality video generation. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1020910218, 2023. 1 [48] Xinyin Ma, Gongfan Fang, Michael Bi Mi, and Xinchao Wang. Learning-to-cache: Accelerating diffusion transformer via layer caching. NeurIPS, 2024. [49] Xinyin Ma, Gongfan Fang, and Xinchao Wang. Deepcache: Accelerating diffusion models for free. CVPR, 2024. 1, 3 [50] Chenlin Meng, Robin Rombach, Ruiqi Gao, Diederik Kingma, Stefano Ermon, Jonathan Ho, and Tim Salimans. On distillation of guided diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1429714306, 2023. 3 [51] Chong Mou, Xintao Wang, Liangbin Xie, Jian Zhang, Zhongang Qi, Ying Shan, and Xiaohu Qie. T2i-adapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models. AAAI, 2023. 1 [52] Muhammad Ferjad Naeem, Seong Joon Oh, Youngjung Uh, Yunjey Choi, and Jaejun Yoo. Reliable fidelity and diversity metrics for generative models. In International Conference on Machine Learning, pages 71767185. PMLR, 2020. 5, 7, 1 [53] Thuan Hoang Nguyen and Anh Tran. Swiftbrush: One-step text-to-image diffusion model with variational score distillation. CVPR, 2024. 2, 3, 4, 5, 6, 7, 8, [40] Xingchao Liu, Xiwen Zhang, Jianzhu Ma, Jian Peng, and Qiang Liu. Instaflow: One step is enough for high-quality [54] Junting Pan, Keqiang Sun, Yuying Ge, Hao Li, Haodong Duan, Xiaoshi Wu, Renrui Zhang, Aojun Zhou, Zipeng Qin, JourYi Wang, Jifeng Dai, Yu Qiao, and Hongsheng Li. neyDB: Benchmark for Generative Image Understanding. NeurIPS, 2023. 6, 1 [55] Ashwini Pokle, Zhengyang Geng, and Zico Kolter. Deep equilibrium approaches to diffusion models. In Proceedings of the 36th International Conference on Neural Information Processing Systems, Red Hook, NY, USA, 2024. Curran Associates Inc. 3 [56] Ben Poole, Ajay Jain, Jonathan T. Barron, and Ben Mildenhall. DreamFusion: Text-to-3D using 2D Diffusion. ICLR, 2023. 4 [57] Yuxi Ren, Xin Xia, Yanzuo Lu, Jiacheng Zhang, Jie Wu, Pan Xie, Xing Wang, and Xuefeng Xiao. Hyper-sd: Trajectory segmented consistency model for efficient image synthesis. NeurIPS, 2024. 2, [58] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image In Proceedings of synthesis with latent diffusion models. the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. 1, 2, 4, 5, 6, 8, 3 [59] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. Unet: Convolutional networks for biomedical image segmentation. In Medical Image Computing and Computer-Assisted InterventionMICCAI 2015: 18th International Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III 18, pages 234241. Springer, 2015. 4 [60] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven In Proceedings of the IEEE/CVF Conference generation. on Computer Vision and Pattern Recognition, pages 22500 22510, 2023. 1 [61] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, Sara Mahdavi, Rapha Gontijo Lopes, Tim Salimans, Tim Salimans, Jonathan Ho, David Fleet, and Mohammad Norouzi. Photorealistic text-to-image diffusion models with deep language understanding. NeurIPS, 2022. 5, 1 [62] Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. ICLR, 2022. 1, 3 [63] Axel Sauer, Tero Karras, Samuli Laine, Andreas Geiger, and Timo Aila. StyleGAN-T: Unlocking the Power of GANs for Fast Large-Scale Text-to-Image Synthesis. International Conference on Machine Learning, 2023. [64] Axel Sauer, Dominik Lorenz, Andreas Blattmann, and Robin Rombach. Adversarial diffusion distillation. ECCV, 2024. 1, 2, 3, 5, 6, 7, 8, 4 [65] Pratheba Selvaraju, Tianyu Ding, Tianyi Chen, Ilya Zharkov, and Luming Liang. Fora: Fast-forward caching in diffusion transformer acceleration. arXiv preprint arXiv:2407.01425, 2024. 3 [66] Yuzhang Shang, Zhihang Yuan, Bin Xie, Bingzhe Wu, and Yan Yan. Post-training quantization on diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 19721981, 2023. 1, 3 [67] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In International Conference on Learning Representations, 2021. 1, 3, 4, 5, 8 [68] Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. ICLR, 2021. 1 [69] Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. Consistency models. In International Conference on Machine Learning, pages 3221132252. PMLR, 2023. 1, [70] Sitong Su, Jianzhi Liu, Lianli Gao, and Jingkuan Song. F3pruning: training-free and generalized pruning strategy toIn Proceedwards faster and finer text-to-video synthesis. ings of the AAAI Conference on Artificial Intelligence, pages 49614969, 2024. 1, 3 [71] Luming Tang, Menglin Jia, Qianqian Wang, Cheng Perng Phoo, and Bharath Hariharan. Emergent correspondence from image diffusion. NeurIPS, 2023. 2, 5 [72] Narek Tumanyan, Michal Geyer, Shai Bagon, and Tali Dekel. Plug-and-play diffusion features for text-driven image-to-image translation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 19211930, 2023. 2, 5 [73] Fu-Yun Wang, Zhaoyang Huang, Alexander William Bergman, Dazhong Shen, Peng Gao, Michael Lingelbach, Keqiang Sun, Weikang Bian, Guanglu Song, Yu Liu, et al. Phased consistency model. NeurIPS, 2024. 3, 2 [74] Haochen Wang, Xiaodan Du, Jiahao Li, Raymond A. Yeh, and Greg Shakhnarovich. Score Jacobian Chaining: Lifting Pretrained 2D Diffusion Models for 3D Generation. CVPR, 2023. 4 [75] Jiannan Wang, Jiarui Fang, Aoyu Li, and PengCheng Yang. Pipefusion: Displaced patch pipeline parallelism for inarXiv preprint ference of diffusion transformer models. arXiv:2405.14430, 2024. [76] Kai Wang, Fei Yang, Shiqi Yang, Muhammad Atif Butt, and Joost van de Weijer. Dynamic prompt learning: Addressing cross-attention leakage for text-based image editing. NeurIPS, 2023. 1 [77] Kai Wang, Fei Yang, Bogdan Raducanu, and Joost van de Weijer. Multi-class textual-inversion secretly yields In Proceedings of the IEEE semantic-agnostic classifier. Workshop on Applications of Computer Vision, 2025. 1 [78] Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan Li, Hang Su, and Jun Zhu. ProlificDreamer: High-Fidelity and Diverse Text-to-3D Generation with Variational Score Distillation. NeurIPS, 2023. 4, 5 [79] Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Stan Weixian Lei, Yuchao Gu, Yufei Shi, Wynne Hsu, Ying Shan, Xiaohu Qie, and Mike Zheng Shou. Tune-a-video: One-shot tuning of image diffusion models for text-to-video generation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 76237633, 2023. 1 [80] Tianwei Yin, Michael Gharbi, Richard Zhang, Eli Shechtman, Fredo Durand, William Freeman, and Taesung Park. One-step diffusion with distribution matching distillation. In CVPR, pages 66136623, 2024. 3 [81] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, et al. Scaling autoregressive models for content-rich text-to-image generation. arXiv preprint arXiv:2206.10789, 2022. 5, 1 [82] Dingkun Zhang, Sijia Li, Chen Chen, Qingsong Xie, and Haonan Lu. Laptop-diff: Layer pruning and normalized distillation for compressing diffusion models. arXiv preprint arXiv:2404.11098, 2024. [83] Junyi Zhang, Charles Herrmann, Junhwa Hur, Luisa Polania Cabrera, Varun Jampani, Deqing Sun, and Ming-Hsuan Yang. tale of two features: Stable diffusion complements dino for zero-shot semantic correspondence. NeurIPS, 2023. 5 [84] Lvmin Zhang and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models, 2023. 1, 5 [85] Hongkai Zheng, Weilie Nie, Arash Vahdat, Kamyar Azizzadenesheli, and Anima Anandkumar. Fast sampling of In Proceedings of diffusion models via operator learning. the 40th International Conference on Machine Learning. JMLR.org, 2023. 3 [86] Jianbin Zheng, Minghui Hu, Zhongyi Fan, Chaoyue Wang, Changxing Ding, Dacheng Tao, and Tat-Jen Cham. Trajectory consistency distillation. arXiv preprint arXiv:2402.19159, 2024. 1, 3, 2 One-Way Ticket : Time-Independent Unified Encoder for Distilling Text-to-Image Diffusion Models Appendix We provide implementation details (see Appendix A) and additional results (see Appendix B) for our TiUE image generation method with loop-free inference. Subsequently, we discuss the limitations and future work (see Appendix C), broader impacts (see Appendix D), ethical statement (see Appendix E), and reproducibility statement (see Appendix F). A. Implementation Details A.1. Evaluation Datasets and Metrics Datasets. We conduct comparisons on four datasets to evaluate the density of image generation: AFHQ [6], CelebAHQ [27], DrawBench [61], and PartiPrompts [81]. Since the AFHQ and CelebA-HQ datasets contain animals and human faces respectively, we utilize text prompts with the format: photo of <cat/dog/wild animal> and photo of <man/woman>. Metrics. We leverage code from the popular GitHub repository StudioGAN [2325] 2 to calculate three metrics: Precision Recall [32], Density, and Coverage [52]. For FID [17] and Clipscore [16] metrics, we employ the official envaluation code from GigaGAN [26] 3 A.2. Baseline Implementations We use the official implementation of Instaflow [40] 4 , LCM [44] 5 , SD-Turbo [64] 6 , and SwiftBrush [53]7. For SwiftBrushv2 [7], we re-implemented the work using the same amount of training data and computational resources as our method. All experiments are conducted at standard resolution of 512512 pixels on single 3090 GPU device. A.3. Training Details We use Stable Diffusion 2.1 (SD 2.1)8 to initialize the teacher SD generator and SD-LoRA generator, and the student generator. We implement our method with PyTorch, and use the Adam optimizer [30] with β1 = 0.9 and 2https : / / github . com / POSTECH - CVLab / PyTorch -"
        },
        {
            "title": "StudioGAN",
            "content": "3https://github.com/lucidrains/gigagan-pytorch 4https://github.com/gnobitab/InstaFlow 5https://latent-consistency-models.github.io/ 6https : / / github . com / Stability - AI / generative - models 7https://github.com/VinAIResearch/SwiftBrush 8https : / / huggingface . co / stabilityai / stable - diffusion-2-1-base β2 = 0.999 to train both the student generator and SDLoRA generator. When calculating the VSD loss Lvsd, we use classifier-free guidance with value of 4.5 like SwiftBrush [53] for both the teacher SD generator and the SDLoRA generator. JourneyDB Datasets. In JourneyDB datasets [54], there are 4M (4,189,737) captions in the training sets. We remove duplicate captions from the training set, leaving 1,418,723 unique captions. These captions are used as prompts to train the student generator. We train our model on NVIDIA 8A40 48G GPU with batch size 64 and take 3 epochs. A.4. The Green and Red Arrows in Fig. 4 and Fig. 5 The green arrows indicate the skip connections that transfer features from the middle layers of the encoder to the corresponding decoder layers, while the red arrows represent the path where features from the final encoder layer are inputs to the decoder. At each time step, the decoder receives both the MidBlock outputs and skip-connection features from the encoder. Since the Mid-Block does not receive skipconnection features, it is not shown in Fig. 4 and Fig. 5 and is considered part of the encoder. A.5. Meaning of Loop-Free We regard iterative denoising in the vanilla multi-step DMs as loop process, while ours does not require any iterative process. Our method denoises in parallel with 4 decoder steps, achieving loop-free generation. θ We further explain Eq. (5) in main paper and provide mathematical interpretation of the 1-step inference for our 1-step encoder and 4-step decoder (i.e., K=4) design. The student generator ϵSG takes as input random noise ϵ, also referred to as zK. As shown in Fig. 5, we only need to calculate skip connections and output features of the UNet-Encoder in the initiation step (t=K) as: = ϵSG-EN (ϵ, K, y). Then, the predicted noise of UNetθ Decoder at step (t=4, 3, 2, 1) can be calculated as ϵt = ϵSG-DE (f, t, y) in parallel. Using the DDIM scheduler, the θ latent at step can be written as: (cid:16)(cid:113) 1 (cid:113) α3 α3 α4 (cid:113) 1 α4 ϵ4, z3= -1α (7) ϵ+ -1 (cid:17) z2= (cid:113) α2 α3 α2 z3+ (cid:16)(cid:113) 1 α2 -1- (cid:113) 1 α3 (cid:17) -1 ϵ3, z1= (cid:113) α1 α α1 z2+ (cid:16)(cid:113) 1 α1 -1- (cid:113) 1 α (cid:17) -1 ϵ2, (8) (9) Dataset Metrics Method Base Model Step Param COCO2014-30K COCO2017-5K FID CLIP Precision Recall F1 FID CLIP Precision Recall F1 Inference Time (ms) Memory (GB) Training Data A100 Days Image Free Size SD1.5 [58] (cfg=7.5) SD1.5 [58] (cfg=4.5) SD2.1 [58] (cfg=7.5) SD2.1 [58] (cfg=4.5) FasterD [34] (cfg=7.5) FasterD [34] (cfg=4.5) FasterD [34] (cfg=7.5) FasterD [34] (cfg=4.5) GigaGAN [26] InstaFlow [40] LCM [44] LCM-LoRA [45] Hyper-SD [57] SD-Turbo [64] TCD [86] SwiftBrush [53] SwiftBrushv2 [7] LCM [44] SDXL-Turbo [64] SDXL-Lightning [37] LCM [44] LCM-LoRA [45] Hyper-SD [57] PCM [73] SD-Turbo [64] TCD [86] LCM [44] SDXL-Turbo [64] SDXL-Lightning [37] Ours 50 860M 16.08 0.325 50 860M 9.90 0.322 50 865M 16.10 0.328 50 865M 12.22 0.325 SD1.5 50 860M 12.93 0.326 50 860M 12.05 0.323 SD2.1 50 865M 13.64 0.329 50 865M 12.42 0.326 1.0B 9.24 0.325 GAN 1 SD1. SD2.1 SDXL 1 0.9B 13.78 0.288 1 860M 132.09 0.230 1 860M 115.21 0.280 1 860M 20.90 0.325 1 865M 19.51 0.331 1 865M 68.01 0.301 1 865M 17.20 0.301 1 865M 15.98 0.326 2.6B 73.75 0.285 1 2.6B 18.98 0.343 1 1 2.57B 20.71 0.331 SD1. 4 860M 23.21 0.262 4 860M 26.06 0.323 4 860M 21.94 0.326 4 860M 21.44 0.316 SD2.1 4 865M 16.14 0.335 4 865M 18.06 0.319 SDXL 2.6B 17.66 0.327 4 4 2.6B 17.79 0.340 4 2.57B 19.82 0.322 SD2.1 1 865M 13.09 0.313 0.717 0.727 0.723 0.734 0.693 0. 0.708 0.699 0.724 0.654 0.109 0.069 0.743 0.758 0.234 0.672 0.782 0.277 0.765 0.740 0.666 0.722 0.742 0.766 0.633 0. 0.780 0.769 0.715 0.634 0.527 0.607 23.39 0.326 0.585 0.648 19.87 0.323 0.489 0.583 25.40 0.328 0.526 0.614 22.24 0.298 0.532 0.601 23.10 0.325 0.569 0.617 22.32 0.322 0.512 0.594 23.65 0.329 0.551 0.616 22.61 0.325 0.547 0.623 0.521 0.580 19.00 0.293 0.194 0.140 143.73 0.229 0.221 0.105 126.82 0.280 0.324 0.451 30.45 0.325 0.458 0.571 29.35 0.331 0.198 .214 79.15 0.300 0.458 0.545 27.18 0.314 0.457 0.577 26.28 0.326 0.254 0.265 82.74 0.285 0.413 0.536 29.17 0.343 0.388 0.509 30.75 0.323 0.346 0.455 40.37 0.303 0.312 0.436 36.17 0.322 0.327 0.454 31.73 0.325 0.360 0.490 31.35 0.315 0.394 0.468 26,14 0.335 0.419 0.540 27.83 0. 0.408 0.536 27.15 0.328 0.431 0.552 27.57 0.341 0.401 0.514 29.32 0.333 0.622 0.628 23.11 0.313 0.776 0.764 0.769 0.788 0.687 0.670 0.698 0.707 0.729 0.118 0.070 0. 0.786 0.298 0.729 0.816 0.344 0.804 0.760 0.713 0.768 0.804 0.770 0.694 0.795 0.810 0.814 0.782 0. 0.587 0.668 2503.0 0.649 0.702 2503.0 0.561 0.649 2244.2 0.606 0.685 2244.2 4.04 4.04 3.89 3.89 0.601 0.641 1476.0 21.83 0.638 0.654 1476.0 21.83 0.572 0.629 1356.0 21.26 0.616 0.659 1356.0 21.26 0.613 0.666 111.3 0.291 0.168 236.2 0.265 0.111 101.4 0.424 0.554 117.5 0.445 0.568 140.0 0.339 0.317 103.0 0.527 0.612 95.0 0.543 0.652 139.6 3.99 5.88 4.66 4.54 3.86 4.43 3.85 4. 0.384 0.363 661.0 0.518 0.630 180.7 0.487 0.594 181.2 13.84 9.24 9.19 0.460 0.559 592.3 0.406 0.531 189.9 0.430 0.560 221.9 0.430 0.552 304.3 0.375 0.487 272.2 0.507 0.619 199.2 5.88 4.66 4.55 4.56 3.86 4. 0.513 0.628 1074.3 13.84 9.24 0.529 0.641 305.7 9.19 0.457 0.577 310.9 0.668 0.682 164.7 4.98 5B 5B 5B 5B 2.7B 3.2M 12M 12M unk. unk. 5B 1.4M 1.4M 12M unk. 30M 12M 12M unk. 3.3M unk. 5B 12M unk. 30M 1.4M 4783 4783 8332 8332 6250 183.2 1.3 1.3 33.3 unk. 7.1 4.1 24.1 1.3 unk. unk. 1.3 1.3 33.3 2 unk. 7. 1.3 unk. unk. 3.9 Table S1. Comparison of our distillation method against other works. Inference Time (ms) and Memory (GB). indicates that we report results using the provided official code and pretrained models. denotes that we re-implemented the work and are providing the scores. indicates that we report results using the provided generated images. unk. denotes unknown. The best and second-best scores are highlighted in bold and underlined, respectively. z0= (cid:113) α0 α1 α0 z1+ (cid:16)(cid:113) 1 α0 -1- (cid:113) 1 α1 (cid:17) -1 ϵ1. (10) With Eq. (7), Eq. (8), Eq. (9), and Eq. (10), the inference is formulated as z0 = Sϵ + E4ϵ4 + E3ϵ3 + E2ϵ2 + E1ϵ1. (11) where (cid:113) α0 αt (cid:113) α0 α4 S= (cid:18)(cid:113) 1 , E1= (cid:113) 1 αt α0 (cid:16)(cid:113) 1 α0 ,t[2,4]. (cid:19) 1 1 (cid:113) 1 α1 (cid:17), and Since ϵt can be Et= computed in parallel, we achieve one-step inference. 1 αt 1 This design reduces inference time with one-step sampling, incurs only minimal increase in memory usage, and achieves significantly improved generation quality. A.6. Tab. 2 Explanation Since DrawBench and PartiPrompts are prompt datasets, we used the SD2.1 samples as the GroundTruth. The FID, Density, and Coverage metrics computed for the SD2.1 model, when calculated with themselves, are 0 and 1, respectively. B. Additional Results B.1. Our Additional Samples Figs. S1 and S2 show our additional samples conditioned on 32 random text prompts. B.2. Diversity Comparison In Fig. S3 and Fig. S4, we show additional results for the qualitative comparison of diversity. We observe that both LCM and SD-Turbo tend to generate images with limited diversity, while they generate high-quality results. SwiftBrushv2 is initialized by SD-Turbo, and thus inherits its diversity problem. In contrast, we are able to produce more realistic and diverse results, and close to the ones of SD, indicating our advantage over the baselines. For the quantitative comparison of diversity in Tab. 2 of the main paper, as shown in Fig. S5, our results are closer to the real images from AFHQ."
        },
        {
            "title": "AFHQ",
            "content": "CelebA-HQ"
        },
        {
            "title": "PartiPrompts",
            "content": "FID DensityCoverage FID DensityCoverage FID DensityCoverage FID DensityCoverage"
        },
        {
            "title": "Image\nFree",
            "content": "SD1.5 [58] (cfg=7.5) SD2.1 [58] (cfg=7.5) 50 47.16 50 51.67 0.066 0.053 InstaFlow [40] LCM [44] LCM-LoRA [44] Hyper-SD [57] SD-Turbo [64] TCD [86] SwiftBrush [53] SwiftBrushv2 [7] LCM [44] SDXL-Turbo [64] SDXL-Lightning [37] LCM [44] LCM-LoRA [44] Hyper-SD [57] PCM [73] SD-Turbo [64] TCD [86] LCM [44] SDXL-Turbo [64] SDXL-Lightning [37] Ours SD1. SD2."
        },
        {
            "title": "SDXL",
            "content": "SD1.5 1 51.97 0.058 1 155.63 0.012 1 144.15 0.002 0.064 1 72.07 1 77.75 1 96.00 1 67.60 1 64.99 0.142 0.021 0.039 0.110 1 106.39 0.022 0.004 1 77.88 0.053 1 76.83 4 78.00 4 87.54 4 62.67 4 60. SD2.1 4 77.23 4 54."
        },
        {
            "title": "SDXL",
            "content": "4 78.72 4 79.00 4 80.23 SD2.1 1 54.48 0.054 0.027 0.132 0.108 0.011 0.063 0.136 0.067 0.002 0.068 0.030 0.022 0.029 0.033 0.001 0. 0.033 0.009 0.014 0.025 0.010 0.025 0.008 0.008 0.005 0.031 0.026 0.005 0.024 0.030 0.027 0.001 0.071 93.94 89. 0.053 0.018 131.99 0.026 165.74 0.001 249.82 0.009 126.65 0.055 146.22 0.047 129.86 0.018 144.03 0.014 131.89 0.055 211.19 0.004 261.00 0.002 131.89 0.078 122.44 0.045 228.50 0.045 76.16 0.096 0.117 86.90 193.08 0.013 103.36 0. 120.19 0.060 192.97 0.013 130.76 0.035 116.82 0.116 0.013 0.013 0.007 0.004 0.001 0.013 0.006 0.002 0.002 0.012 0.001 0.001 0.013 0.045 0.006 0.020 0. 0.001 0.014 0.012 0.010 0.006 0.068 11.95 0 0.510 1 25.08 0.223 120.98 0.058 115.63 0.019 0.397 26.04 25.75 69.46 21.48 18. 83.97 32.01 28.44 46.23 28.45 17.53 17.38 27.80 11.46 24.97 30.05 44.52 21.10 0.597 0.067 0.402 0.682 0.043 0.602 0. 0.183 0.397 0.642 0.673 0.281 0.623 0.431 0.466 0.107 0.685 0.622 1 0.337 0.014 0.014 0.385 0.488 0.081 0.441 0. 0.055 0.394 0.340 0.187 0.362 0.569 0.581 0.371 0.431 0.443 0.373 0.132 0.616 7.36 0 0.730 17.64 0.457 95.65 0.095 92.52 0.057 17.50 0.677 17.40 0.770 53.35 0.184 14.43 0.579 11.32 0.850 64.31 0.130 17.38 0.791 18.15 0.606 26.84 0.512 19.75 0.732 11.45 0.884 10.74 0.887 22.84 0.500 7.90 0.882 16.27 0.682 17.12 0.741 36.12 0.248 16.28 0. 0.887 1 0.670 0.072 0.077 0.530 0.775 0.264 0.737 0.865 0.226 0.735 0.672 0.575 0.694 0.853 0.865 0.648 0. 0.759 0.742 0.395 0.840 Table S2. Quantitative comparison of our distillation method with other approaches based on FID, Density, and Coverage metrics to assess diversity. indicates that we report results using the provided official code and pretrained models. denotes that we re-implemented the work and are providing the scores. The best and second-best numbers are marked with bold and underlined, respectively. B.3. Iteration Qualitative Results For better demonstration of the iterative process, we present qualitative results at early 6000 steps in Fig. S6. At 1000 iterations, the model already learned meaningful texture information (see Fig. S6 (the fifth row)). B.4. Comparison with Additional SD-based Models To demonstrate the effectiveness of our distillation method, we compare it with FasterDiffusion (FasterD) [34], which shares encoder features at certain adjacent time steps and performs the decoder in parallel at these steps. FasterD [34] accelerates 50-step sampling by 1.8x during inference while maintaining generation quality (FID = 12.42 and 22.61 for COCO2014 and COCO2017) (see Tab. S1). We further include the comparison with LCM-LoRA [45]9, PCM [73]10, Hyper-SD [57]11, and TCD [86]12. Our method achieves one-step inference while preserving quality (FID = 13.09 and 23.11 for COCO2014 and 9https://huggingface.co/latent-consistency/lcmCOCO2017) and outperforms LCM-LoRA, PCM, HyperSD and TCD across FID, Recall, and F1 evaluation metrics (see Tab. S1), demonstrating its advantage over traditional acceleration methods. B.5. Comparison with SDXL-based Models We compare our distillation method against other works based on SD, similar to state-of-the-art methods [7, 53], in the main paper. To make full comparison, we further include SDXLbased distillation methods, such as LCM (SDXL) [44]13, SDXL-Turbo [64]14, and SDXL-Lightning [37]15. Note that the parameter scale of these models exceeds 2.5 billion, comparing with conventional SD-based models which are often below 1 billion. Qualitative and quantitative results are demonstrated in Tabs. S1 and S2, and Fig. S7. As can be observe that, InstaFlow [40], LCM [44] (1-step), and SwiftBrush [53] face challenges in generating highquality images. LCM [44] (4-step), SD-Turbo [64], SDXLlora-sdv1-5 13https://huggingface.co/latent-consistency/lcm10https://github.com/GUN/PhasedConsistencysdxl"
        },
        {
            "title": "Model",
            "content": "11https://huggingface.co/ByteDance/Hyper-SD 12https://github.com/jabir-zheng/TCD 14https://huggingface.co/stabilityai/sdxl-turbo 15https : / / huggingface . co / ByteDance / SDXL -"
        },
        {
            "title": "Lightning",
            "content": "release the code for the novel loop-free sample we have introduced. We conducted all experiments using publicly accessible datasets. Elaborate details of all experiments have been provided in the Appendices. Turbo [64], SDXL-Lightning [37], and SwiftBrushv2 [7] tend to generate results with similar scenes and identities giving the same prompt, leading to the lack of generation diversity. In contrast, our results are closer to the generation quality and diversity of original SD model. Note that, due to the large amount of parameters of the SDXL model, distillting them into 1-step models [37] generally requires 880GB GPUs with batch size as 8. We aim to develop more efficient distillation approaches in the future for extremely large T2I diffusion models to reduce the time and space complexity. C. Limitations, and Future Work The present study focuses on implementing loop-free inference with shared encoder strategy exclusively in image-free distillation. However, we posit that adopting this shared encoder strategy in image-dependent distillation could yield loop-free sampling, thereby enhancing inference speed without compromising on generation quality. This primarily requires engineering efforts. D. Broader Impacts TiUE designs loop-free inference strategy in text-to-image synthesis to improve sampling speed. However, it also carries potential negative implications. It could be used to generate false or misleading images, thereby spreading misinformation. If TiUE is applied to generate images of public figures, it poses risk of infringing on personal privacy. Additionally, the automatically generated images may also touch upon copyright and intellectual property issues. E. Ethical Statement We acknowledge the potential ethical implications of deploying generative models, including issues related to privacy, data misuse, and the propagation of biases. All models used in this paper are publicly available, as well as the base training scripts. We will release the modified codes to reproduce the results of this paper. We also want to point out the potential role of customization approaches in the generation of fake news, and we encourage and support responsible usage of these models. Finally, we think that awareness of open-world forgetting can contribute to safer models in the future, since it encourages more thorough investigation into the unpredictable changes occurring when adapting models to new data. F. Reproducibility Statement To facilitate reproducibility, we will make the entire source code and scripts needed to replicate all results presented in this paper available after the peer review period. We will Figure S1. Our additional samples. Figure S2. Our additional samples. Figure S3. Diversity comparison. Our results are close to the one of SD. Figure S4. Diversity comparison. Our results are close to the one of SD. Figure S5. Diversity comparison. Our results are close to the ones of AFHQ. Figure S6. Iteration qualitative results. Figure S7. Qualitative and diversity comparison. Our results are close to the one of SD."
        }
    ],
    "affiliations": [
        "Computer Vision Center, Universitat Aut`onoma de Barcelona",
        "Linkoping University",
        "Mohamed bin Zayed University of AI",
        "Nankai International Advanced Research Institute (Shenzhen Futian), Nankai University",
        "SB Intuitions, SoftBank",
        "School of Big Data and Computer Science, Guizhou Normal University",
        "VCIP, CS, Nankai University"
    ]
}