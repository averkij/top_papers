{
    "paper_title": "Hearing to Translate: The Effectiveness of Speech Modality Integration into LLMs",
    "authors": [
        "Sara Papi",
        "Javier Garcia Gilabert",
        "Zachary Hopton",
        "Vilém Zouhar",
        "Carlos Escolano",
        "Gerard I. Gállego",
        "Jorge Iranzo-Sánchez",
        "Ahrii Kim",
        "Dominik Macháček",
        "Patricia Schmidtova",
        "Maike Züfle"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "As Large Language Models (LLMs) expand beyond text, integrating speech as a native modality has given rise to SpeechLLMs, which aim to translate spoken language directly, thereby bypassing traditional transcription-based pipelines. Whether this integration improves speech-to-text translation quality over established cascaded architectures, however, remains an open question. We present Hearing to Translate, the first comprehensive test suite rigorously benchmarking 5 state-of-the-art SpeechLLMs against 16 strong direct and cascade systems that couple leading speech foundation models (SFM), with multilingual LLMs. Our analysis spans 16 benchmarks, 13 language pairs, and 9 challenging conditions, including disfluent, noisy, and long-form speech. Across this extensive evaluation, we find that cascaded systems remain the most reliable overall, while current SpeechLLMs only match cascades in selected settings and SFMs lag behind both, highlighting that integrating an LLM, either within the model or in a pipeline, is essential for high-quality speech translation."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 1 ] . [ 1 8 7 3 6 1 . 2 1 5 2 : r Hearing to Translate:"
        },
        {
            "title": "The Effectiveness of Speech Modality Integration into LLMs",
            "content": "1, Javier Garcia Gilabert 2, Zachary Hopton 3, Vilém Zouhar 4, Sara Papi Carlos Escolano5, Gerard I. Gállego2,5, Jorge Iranzo-Sánchez6, Ahrii Kim7, Dominik Macháˇcek8, Patricia Schmidtova8, Maike Züfle9 1Fondazione Bruno Kessler, 2Barcelona Supercomputing Center, 3University of Zurich, 4ETH Zurich, 5Universitat Politècnica de Catalunya, 6Universitat Politècnica de València, 7AI-Bio Convergence Research Institute, 8Charles University, 9KIT Correspondence: spapi@fbk.eu"
        },
        {
            "title": "Abstract",
            "content": "As Large Language Models (LLMs) expand beyond text, integrating speech as native modality has given rise to SpeechLLMs, which aim to translate spoken language directly, thereby bypassing traditional transcriptionbased pipelines. Whether this integration improves speech-to-text translation quality over established cascaded architectures, however, remains an open question. We present Hearing to Translate,1 the first comprehensive test suite rigorously benchmarking 5 state-of-theart SpeechLLMs against 16 strong direct and cascade systems that couple leading speech foundation models (SFM), with multilingual LLMs. Our analysis spans 16 benchmarks, 13 language pairs, and 9 challenging conditions, including disfluent, noisy, and long-form speech. Across this extensive evaluation, we find that cascaded systems remain the most reliable overall, while current SpeechLLMs only match cascades in selected settings and SFMs lag behind both, highlighting that integrating an LLM, either within the model or in pipeline, is essential for high-quality speech translation."
        },
        {
            "title": "Introduction",
            "content": "Large Language Models (LLMs) have transformed natural language processing, enabling unprecedented generalization and reasoning capabilities across wide range of text-based tasks (Achiam denotes core contributor and PI of the project, denotes core contributors, in order of contribution; other authors are ordered alphabetically. 1The Hearing-to-Translate Suite, including benchmarks, models outputs, inference, and evaluation scripts, is released at https://github.com/sarapapi/hearing2translate. et al., 2023; Touvron et al., 2023). Recently, these models have been extended beyond text to encompass multimodal inputs, including vision and audio. Among these modalities, speech holds particularly central role, as it is the most natural and information-rich form of human communication, conveying not only linguistic content but also prosodic, emotional, and paralinguistic cues Integrating this modality into (Schuller, 2018). LLMs promises new generation of language technologies that can process and understand spoken language in more human-like and contextually grounded manner (Latif et al., 2023). This motivated the emergence of SpeechLLMs: models that extend text-based LLMs with the ability to process spoken language directly. SpeechLLM typically integrates an audio encoder, often derived from powerful Speech Foundation Models (SFMs) such as Whisper (Radford et al., 2023) or SeamlessM4T (Barrault et al., 2023), with one or more adapters that bridge the gap between acoustic representations and the embedding space of an LLM such as Gemma (Gemma Team et al., 2025) or Tower+ (Rei et al., 2025). This paradigm challenges the traditional architectures that have long dominated speech-to-text translation (ST). Conventional ST systems are typically either cascade or direct (Bentivogli et al., 2021). In cascaded setups, dedicated Automatic Speech Recognition (ASR) model first transcribes the input speech into text, which is then translated by separate Machine Translation (MT) or, more recently, LLM-based module. This modular design remains highly effective, as it allows each component to be trained on large available corpora and fine-tuned independently for new languages or domains, but it also introduces limitations: translation quality is tightly coupled to ASR accuracy (Ney, 1999), potentially leading to error propagation issues (Sperber and Paulik, 2020), increased latency and computational costs, as two models have to be sequentially executed (Papi et al., 2025a), and the intermediate transcription step discards prosodic and paralinguistic information that may enrich meaning (Tsiamas et al., 2024). Direct ST models, in contrast, attempt to bypass these issues by mapping speech directly to translated text end-to-end (Bérard et al., 2016; Weiss et al., 2017). However, these models are often data-hungry (Nguyen et al., 2020; Jia et al., 2022; Xu et al., 2023), limited by the scarcity of large-scale parallel speech-translation corpora, and less flexible at test time, lacking the in-context reasoning and adaptability of LLMs. SpeechLLMs offer novel alternative to these monolithic ST models. By integrating the speech modality within general-purpose LLM, they combine the ability of directly processing speech representations as in end-to-end systems with the vast linguistic knowledge and contextual flexibility of LLMs in unified model, which, in principle, could not only translate spoken language but also adapt their outputs to the users communicative intent, and handle cross-lingual contexts (Rubenstein et al., 2023). These properties make SpeechLLMs an appealing framework for massively multilingual translation systems that can seamlessly operate across text and speech (Bapna et al., 2022; Nguyen et al., 2025a). However, the practical benefits of this integration remain an open question. It is unclear whether SpeechLLMs can match (or surpass) the performance of translation-specialized direct or cascaded systems that combine powerful SFMs with high-performing LLMs. Furthermore, existing works rarely compare these paradigms systematically (Gaido et al., 2024) or consider complex real-world speech phenomena such as disfluencies, background noise, and code-switching. In this paper, we present Hearing to Translate, the first comprehensive test suite evaluating the effectiveness of speech modality integration in LLMs for translation. We systematically compare 5 stateof-the-art SpeechLLMs against 16 strong systems (4 direct and 12 cascade) built on top of leading SFMs and multilingual and translation-oriented LLMs. Our evaluation encompasses 13 language pairs and 16 benchmarks, covering 9 diverse conditions that capture range of linguistic and acoustic phenomena, enabling comprehensive assessment of translation quality and robustness in realistic settings. Through this analysis, we address fundamental question for the SpeechLLM era: Does integrating the speech modality directly into LLMs truly enhance spoken language translation, or do cascaded architectures or traditional direct models remain the most effective solutions?"
        },
        {
            "title": "2 Related Works",
            "content": "Cascaded vs. Direct ST: Historical Comparison. The comparison between cascaded and direct architectures has long been central topic in ST research. While early works highlighted the potential of end-to-end models to reduce error propagation and latency while achieving comparable or superior results to pipeline approaches (Pino et al., 2019; Indurthi et al., 2020), recent evidence paints more nuanced picture. The most recent IWSLT evaluation campaigns (Ahmad et al., 2024; Abdulmumin et al., 2025) consistently report that cascades, especially those combining strong SFMs with high-performing LLMs (Koneru et al., 2025; Wang et al., 2025), again outperform direct approaches across multiple language pairs and acoustic conditions. Similarly, Min et al. (2025) show that despite architectural advances, direct systems still struggle to generalize in realistic multilingual or low-resource scenarios. While these studies have clarified the strengths and weaknesses of each paradigm, systematic comparisons in the era of LLM-enhanced models remain limited (Gaido et al., 2024). Our work revisits this long-standing debate under new lens: evaluating how speech modality integration into LLMs reshapes the traditional balance between cascaded and direct ST. The LLM Era is Here, for MT. LLMs have recently reshaped the MT landscape, achieving results comparable to or surpassing specialized translation models in recent WMT campaigns (Kocmi et al., 2024a, 2025). Studies such as Garcia et al. (2023); Stap et al. (2024); Deutsch et al. (2025) attribute these gains to the broad multilingual coverage, contextual reasoning, and in-context learning capabilities of LLMs, which enable high-quality translation even without task-specific fine-tuning. Beyond raw accuracy, LLMs excel in adaptation to user intent (Sarti et al., 2023), style and formality control (Rippeth et al., 2022), and explaining and correcting their own translations (Treviso et al., 2024)dimensions traditionally outside the scope of standard MT models. This paradigm shift has sparked growing interest in extending LLMs beyond text to speech, motivating the development of SpeechLLMs for ST. However, while the superiority of LLMs over traditional MT systems has been established in text translation, this assumption has not yet been verified for SpeechLLMs in ST. Our work directly addresses this gap, providing the first study testing whether the advantages of LLMbased translation extend to the speech modality."
        },
        {
            "title": "3 The Hearing-to-Translate Suite",
            "content": "In this section, we describe the main ingredients of the test suite: the analyzed phenomena (Section 3.1), the selected benchmarks (Section 3.2), and the metrics used for evaluation (Section 3.3)."
        },
        {
            "title": "3.1 Categorization of Analyzed Phenomena",
            "content": "To evaluate the robustness and generalization ability of SpeechLLMs across realistic scenarios, we introduce diverse set of conditions collectively referred to as the Hearing-to-Translate Suite. Each condition targets specific linguistic, acoustic, or sociolinguistic phenomenon known to challenge speech and translation systems (Shah et al., 2024). The suite enables controlled and comprehensive analysis of model behavior across nine categories: GENERIC Clean, well-segmented speech from standard benchmarks, used as reference for model performance under ideal conditions. GENDER BIAS Utterances balanced across male and female speakers to examine whether translation outputs preserve or distort gendered information and pronoun use. ACCENTS Speech from different geographic varieties of given language, assessing the ability of models to generalize beyond the accent or dialect distribution seen during training. CODE SWITCHING Segments containing intrasentential language alternation, which require models to dynamically adapt to mixed-language input and maintain coherence in translation. DISFLUENCIES Spontaneous speech containing hesitations, repetitions, and self-corrections, used to evaluate how well models handle natural, nonscripted communication. NAMED ENTITIES Speech including person names, locations, and organizations, testing the preservation and accuracy of proper nouns. NOISE Audio with added environmental or background noise, evaluating the robustness of models to unclean acoustic conditions. EMOTION Emotionally expressive speech, assessing whether prosodic and affective cues influence translation fidelity and tone. LONG-FORM Extended audio segments containing multiple sentences, often of several minutes, used to evaluate contextual consistency and memory handling in translation models."
        },
        {
            "title": "3.2 Benchmarks",
            "content": "To ground the analysis of the phenomena introduced in Section 3.1, we select and create set of benchmarks that collectively cover the nine categories. For each of them, we provide brief description in Appendix B. summary, with license and covered languages, is presented in Table 1."
        },
        {
            "title": "3.3 Metrics",
            "content": "S and METRICXQE Most speech benchmarks lack reference translations, and recent work has raised concerns about the reliability of reference-based automatic metrics (Freitag et al., 2023; Zouhar and Bojar, 2024). Accordingly, we rely on quality estimation (QE) metrics for evaluation. To this end, we employ xCOMETQE : modified versions of xCOMET (Guerreiro et al., 2024) and METRICX (Juraska et al., 2024) designed to penalize off-target outputs. This strict evaluation follows the recommendation of Zouhar et al. (2024) and applies the maximal penalty to any translation identified by LINGUAPY2 as being in the wrong language. Specific settings are reported in Appendix C. Besides pure quality-based scores, we also report tailored metrics, which are presented below: Performance Gap. For several phenomena, we quantify performance variation through unified gap formulation, which measures the relative difference between two quantities, QA and QB: = 100 (QA QB) / QA where QA and QB denote evaluation scores computed on two contrasting subsets of the same benchmark, using either xCOMETQE or task-specific metrics. value close to zero indicates comparable performance across conditions; positive values indicate better performance on subset than on B, while negative values indicate better performance 2https://github.com/pemistahl/lingua-py"
        },
        {
            "title": "Src Lang",
            "content": "FLEURS (Conneau et al., 2022) CC-BY 4."
        },
        {
            "title": "GENERIC GENDER BIAS",
            "content": "en de es fr it pt zh CoVoST2 (Wang et al., 2020) EuroParlST (Iranzo-Sánchez et al., 2020) WMT (Kocmi et al., 2024a, 2025) WinoST (Costa-jussà et al., 2022) CC-0 CC-BY-NC 4.0 CC-BY 3.0 Custom"
        },
        {
            "title": "GENERIC",
            "content": "en de es it pt zh en de es fr it pt en"
        },
        {
            "title": "GENDER BIAS",
            "content": "en CommonAccent (Zuluaga-Gomez et al., 2023) CC-0 ManDi (Zhao and Chodroff, 2022) CC-BY-NC 3."
        },
        {
            "title": "ACCENTS",
            "content": "CS-Dialogue (Zhou et al., 2025) CS-FLEURS (Yan et al., 2025) CC-BY-NC-SA 4.0 CC-BY-NC 4."
        },
        {
            "title": "CODE SWITCHING",
            "content": "LibriStutter (Panayotov et al., 2015) CC-BY-NC 4."
        },
        {
            "title": "DISFLUENCIES",
            "content": "NEuRoparlST (Gaido et al., 2021) CC-BY-NC 4."
        },
        {
            "title": "NAMED ENTITIES",
            "content": "en de es it zh zh de es fr zh en en NoisyFLEURS NEW! CC-BY-NC 4."
        },
        {
            "title": "NOISE",
            "content": "en de es fr it pt zh EmotionTalk (Sun et al., 2025) mExpresso (Seamless Comm. et al., 2023) CC-BY-NC-SA 4.0 CC-BY-NC 4."
        },
        {
            "title": "EMOTION",
            "content": "ACL 60/60 (Salesky et al., 2023) MCIF (Papi et al., 2025b) CC-BY 4.0 LONG-FORM zh en en Table 1: Overview of benchmarks, their covered phenomena, and source language (ISO 639 two-letter language code is used). WinoST is available under the MIT license with the limitation that recordings cannot be used for speech synthesis, voice conversion, or other applications where the speakers voice is imitated or reproduced. on than on A. The gap is computed for the following phenomena: Gender Speaker Gap ( ) of male (A = ): Following Attanasio et al. (2024), we instantiate the gap by comparing the translation quality (either xCOMETQE or METRICXQE ) and female (B = ) speakers, capturing relative performance disparities across speaker gender. ): For WinoST, we compute the relative difference in coreference resolution accuracy by applying the gap formulation to F1 scores obtained on male (A = ) and female (B = ) subsets, using the official evaluation script.3 Gender Coreference Gap (F1 Accent Gap (accent): Accent robustness is evaluated by contrasting standard varieties (A = STD) with non-standard or regional varieties (B = STD).4 Disfluency Gap (disfluency): To assess robustness to speech disfluencies, we compare translation quality on fluent (A = fl) and disfluent (B = disfl) speech subsets. ditions. Length Gap (length): We measure sensitivity to long-form speech by contrasting short-form (A = short) and long-form (B = long) inputs. large positive length indicates substantial degradation when processing entire talks rather than sentence-level segments. Since short-form segments are not paired with references, we resegment system outputs and align them to references using SentencePiece (Kudo and Richardson, 2018) and MWERSEGMENTER (Matusov et al., 2005), following standard ST evaluation practice (Ansari et al., 2020). Accuracy. For named entities and domainspecific terminology, we report case-sensitive accuracy (%NE, %term) using the official NEuroParlST evaluation script.5 Specifically: %NE = MN E/ %term = MT erm/"
        },
        {
            "title": "N E",
            "content": "erm Noise Gap (noise): Noise robustness is quantified by instantiating the gap between clean (A = clean) and noisy (B = noisy) speech con3https://github.com/gabrielStanovsky/mt_gender 4This metric is applied only to ManDi, as CommonAccent does not define single standard variety. where MN and MT erm denote exact string matches in system outputs, and and erm are the corresponding reference sets. 5https://github.com/mgaido91/FBK-fairseq-ST/ blob/emnlp2021/scripts/eval/ne_terms_accuracy.py"
        },
        {
            "title": "4.1 Models",
            "content": "To allow for wider accessibility and easier reproduction of our results, we consider models with <32B parameters. Our analysis focuses on the three paradigms: SFMs (used either as ASR or directly for ST), pipeline composed of SFMs, and LLMs and SpeechLLMs . Specifically, we selected: Whisper, Seamless, Canary, and OWSM as SFMs; Aya, Gemma3, and Tower+ as LLMs; and Phi-4-Multimodal, Qwen2-Audio, DeSTA2, Voxtral, and Spire as SpeechLLMs. Detailed descriptions, weights, and other details for all models are reported in Appendix D."
        },
        {
            "title": "4.2 Languages and Inference",
            "content": "Given the broad language coverage of current LLMs and SFMs, we select languages based on those most commonly supported across the SpeechLLMs analyzed in our study. The evaluation focuses on English-centric pairs, including {de, fr, it, es, pt, zh} {de, nl, fr, it, es, pt, zh}. en and en For LLM inference, we follow the official translation prompt from the WMT 2025 General MT Shared Task (Kocmi et al., 2025), which we adapt for SpeechLLMs to accommodate spoken inputs (see Appendix E). For SFMs, which do not support prompting, we specify either the target language or both the source and target languages, depending on the specific model. Default decoding parameters are used for all models, reflecting real-world, out-of-the-box performance. All inferences are performed using the Hugging Face Transformers library, as detailed in Appendix D, except for OWSM, available only via ESPnet (Watanabe et al., 2018), and Canary, which is implemented in NVIDIA NeMo (Kuchaiev et al., 2019)."
        },
        {
            "title": "5 Results",
            "content": "We first present the overall results of the 21 systems analyzed in the paper, highlighting key trends (Section 5.1). Then, we delve into two main aspects of ST evaluation, gender bias and accents (Section 5.2), and provide human evaluation results with automatic metrics correlation (Section 5.3). Aggregated xCOMETQE are presented in Table 2, while aggregated METRICXQE are presented in Appendix F. We also provide scores for each of the supported languages7 in Appendix G. Across the GENERIC benchmarks, consistent picture emerges: cascaded systems remain difficult to beat. They typically outperform8 current SpeechLLMs and SFMs, with Voxtral standing out as the only SpeechLLM that reliably closesand often overturnsthe gap with best-performing cascades built on Gemma3 or Tower+. SFMs generally lag behind, and most SpeechLLMs struggle to match strong SFM baselines (Whisper, Seamless). OWSM performs worst as standalone SFM, while, in combination with LLMs, it is able to recover most of its gap, indicating poor language model ability. Overall, the strongest average results come from Canary and Whisper paired with Aya, followed by Voxtral, which are also the largest cascades and SpeechLLM in our evaluation. In the GENDER BIAS category, most models ex- ) ranging hibit relatively small gender gaps ( from 0.9 to 2.4 on FLEURS, except OWSM, which is skewed toward male speakers. Gaps tend to be slightly larger when translating from English than into English. No single paradigm dominates: the smallest gaps are reached by OWSM+Gemma3, Voxtral, Seamless, and Whisper+Aya. By contrast, WinoST exposes substantially larger F1 gaps. While SpeechLLMs like Qwen2-Audio and Phi-4Multimodal show high disparities, bias in cascade systems is contingent on the choice of LLM, indicating that gender bias stems primarily from the text-generation module rather than the speech encoder: pairing ASR modules with Gemma3 results in substantial gaps, whereas using specialized translation model like Tower+ significantly mitigates this disparity. For ACCENTS , Seamlesseither used directly or inside cascadeachieves the strongest performance on CommonAccent, outperforming both cascades and Voxtral by at least 1.5 xCOMETQE on en-x. OWSM and most SpeechLLMs struggle to gen6The only exception is Spire, which produced unusable outputs under default settings and was therefore run with beam search (beam size 5). 7For NEuRoparl-ST, WinoST, ACL 60/60, and MCIF, the set of target languages is constrained by benchmark-specific requirements (see Appendix G). 8According to Kocmi et al. (2024c), difference of 2 xCOMET points corresponds to 90% agreement by human annotators."
        },
        {
            "title": "FLEURS",
            "content": "CoVoST2 EuroParl-ST WMT"
        },
        {
            "title": "CommonAccent ManDi",
            "content": "CS-Dialogue CS-FLEURS xCOMETQE S"
        },
        {
            "title": "OWSM",
            "content": "Whisper + Aya + Gemma3 + Tower+ Seamless + Aya + Gemma3 + Tower+ Canary + Aya + Gemma3 + Tower+ OWSM + Aya + Gemma3 + Tower+ DeSTA2 Qwen2-Audio Phi-4-Multimodal"
        },
        {
            "title": "Spire",
            "content": "en-x x-en - 88.6 - 51. 93.2 92.9 93.2 93.2 93.0 93. 93.6 93.3 93.6 91.8 91.7 91. 78.3 82.2 71.0 94.7 81.4 84. 88.3 - 44.4 92.6 91.7 92. 91.1 90.2 90.9 - - - 90.0 88.5 89.9 77.9 80.6 88. 91.8 - xCOMETQE x-en en-x en-x x-en en-x en-x x-en en-x xCOMETQE x-en en-x accent zh-en - 87. - 53.1 84.5 83.8 84.4 88. 88.1 88.7 86.4 85.4 86.1 84. 83.5 84.2 65.2 77.9 61.0 85. 66.8 73.3 83.9 66.0 48.2 82. 81.5 82.3 85.4 84.4 85.2 - - - 82.0 80.7 81.5 59. 74.1 66.0 81.9 - - 77. - 55.1 91.4 90.7 91.4 91. 90.4 91.1 92.3 91.7 92.5 90. 89.4 90.3 58.2 84.1 68.3 91. 81.2 79.2 83.4 86.4 42.6 86. 85.3 86.1 87.4 86.3 87.0 88. 87.2 87.8 84.1 82.6 83.6 65. 77.9 77.1 86.3 - - - 26.6 -1.3 - 25.3 66.2 64. 63.9 36.6 36.0 36.2 66.1 64. 63.9 53.7 52.4 52.3 46.3 38. 39.8 65.2 38.7 - 8.5 -1. -2.0 -1.5 -1.7 -2.2 -2.4 -1. -0.9 -0.9 -2.1 0.3 -1.7 -0. -1.6 -2.3 -1.0 -0.8 0.7 0. - 9.6 -0.4 0.3 0.7 -0. 0.5 0.8 - - - -0. 0.0 0.3 -1.6 0.5 0.9 -0. - - 30.9 8.6 51.6 17. 26.1 -3.9 19.0 26.5 -3.1 17. 25.7 -4.0 18.9 25.2 -4.2 14. 48.1 65.8 8.6 14.5 - 90. - 53.5 86.6 85.5 86.0 91. 91.1 91.4 88.8 88.1 88.6 85. 85.1 85.3 66.4 80.9 75.1 87. 73.7 78.2 85.2 84.1 52.7 85. 84.0 84.9 86.3 85.5 85.9 86. 85.1 86.2 83.8 82.4 82.7 62. 73.9 80.5 85.6 - 4.6 31. - 1.8 38.9 41.3 42.6 32. 34.7 32.8 - - - 48. 44.0 49.2 28.0 14.2 23.7 17. - zh-en 69.7 65.0 - 30. 78.8 76.8 77.0 75.4 71.5 71. - - - 67.6 63.2 64. 68.4 69.7 61.7 79.1 - x-en 76.0 85.5 - 53.6 90.2 89. 90.2 86.6 84.5 85.9 - - - 83.6 81.5 83.1 74.2 82. 86.5 91.9 -"
        },
        {
            "title": "EMOTION",
            "content": "LONG-FORM"
        },
        {
            "title": "LibriStutter",
            "content": "NEuRoparl-ST"
        },
        {
            "title": "NoisyFLEURSB NoisyFLEURSA mExpresso",
            "content": "EmotionTalk ACL6060 MCIF disfluency en-x %NE en-x %term en-x noise en-x x-en en-x x-en en-x xCOMETQE zh-en"
        },
        {
            "title": "OWSM",
            "content": "Whisper + Aya + Gemma3 + Tower+ Seamless + Aya + Gemma3 + Tower+ Canary + Aya + Gemma3 + Tower+ OWSM + Aya + Gemma3 + Tower+ DeSTA2 Qwen2-Audio Phi-4-Multimodal"
        },
        {
            "title": "Spire",
            "content": "- 44.7 - 30.4 5.9 6. 6.7 14.5 23.8 18.7 14.0 19. 16.3 14.5 22.8 17.2 10.6 21. 26.5 3.9 23.2 - 61.3 67. 43.1 18.0 5.1 67.4 42.2 16. 67.6 59.3 11.8 68.5 27.8 6. 50.8 5.8 8.2 54.3 66.9 66. - 71.2 80.1 64.7 23.3 5. 81.1 50.4 20.4 81.3 72.8 13. 81.8 35.0 8.2 64.8 7.0 9. 65.5 79.7 76.0 - 58.9 - 66.6 51.3 50.8 50.1 55.0 55. 55.3 58.8 59.3 59.6 67.5 69. 75.3 67.7 44.4 56.1 38.0 79. 54.4 57.4 - 63.9 53.1 54. 51.0 58.4 59.7 59.2 - - - 68.3 70.4 71.6 71.3 57. 36.8 45.7 - - 11.9 - 19.5 8.1 8.0 7.8 9.2 9. 9.5 8.2 8.4 8.4 14.5 14. 84.4 19.9 10.0 5.6 5.4 47. 13.1 11.9 - 19.8 11.8 11. 11.1 11.1 11.6 11.3 - - - 16.9 18.5 84.9 24.4 17. 7.3 7.8 - - 79.2 - 62.6 87.4 85.9 86.5 83.4 83. 82.4 87.2 85.6 86.3 85.8 84. 85.1 68.2 73.3 34.1 86.1 73. 68.3 64.3 - 26.0 78.1 76. 76.9 77.9 75.8 75.9 - - - 73.5 71.6 72.7 59.7 70. 67.7 72.3 - length en-x en-x - - - - - - 26.9 11.0 5.3 4.4 4.8 - - - -0.5 -1.8 -0.8 1. -0.1 -0.4 93.8 94.2 -6.1 0. - 4.6 3.3 5.1 - - - -0.2 0.4 0.6 -1.4 -2. -1.7 92.3 91.7 21.5 0.5 - Table 2: Overall performance of the 21 evaluated systems. en-x denotes averages across all target languages, except where each benchmark covers specific subset (e.g., WinoST: de/es/fr/it/pt; NEuRoparl-ST: es/fr/it; ACL 60/60: de/fr/zh/pt; MCIF: de/it/zh). x-en denotes averages across all source languages for each benchmark, as per Table 1. eralize across accents. The ManDi results reveal that SFMs (Whisper, OWSM) and SpeechLLMs (Qwen2-Audio, Voxtral) are less biased toward standard Chinese, while cascades exhibit substantial bias toward the standard variety. These findings confirm that accent robustness is driven primarily by the speech encoder, with some SFMs displaying superiority depending on the languages. In CODE SWITCHING , cascaded Whisper, and especially Voxtral, achieve top performance. Despite both the cascaded Whisper and Voxtral leveraging Whisper as speech encoder, Whisper alone lags behind the other two paradigms, indicating that both encoder and decoder matter for code-switching, as proved by the lower results obtained by SFMs compared to their cascaded counterparts. For DISFLUENCIES , Voxtral, DeSTA2, and Whisper cascades are the most robust to stuttered speech. Seamless, OWSM, and Phi-4-Multimodal show large degradations (43-75disfluency). Interestingly, DeSTA2 and Voxtral outperform Qwen2Audio (having double and sixfold, respectively, disfluency), even though all three rely on the Whisper encoder, indicating that how speech is integrated into the LLM (architecture-level design) strongly impacts robustness to disfluencies. In NAMED ENTITIES , trends between NE and terminology accuracy largely align: systems that handle named entities well also handle terminology well. However, accuracy and translation quality do not always correlate. For example, Canary+Gemma3 reaches 91.7 xCOMETQE on GENERIC EuroParl-ST but scores only 11.8%NE and 13.1%term, underscoring the importance of targeted metrics. Among top systems, cascades based on Tower+ (excluding OWSM) lead, followed by Canary and Voxtral, but no paradigm dominates universally. In noisy conditions ( NOISE ), all models degrade under both noise types, with babble noise causing extreme degradation (minimum 38noise). Interestingly, all SpeechLLMs except Spire show equal or greater robustness than both SFMs and cascades. manual inspection revealed that SFMs used as ASR components in cascades often hallucinate under noise, and LLMs, lacking access to the original audio, propagate or amplify these errors. In this category, SpeechLLMs are the most reliable choice. In EMOTION , cascades prove more robust than direct systems (SFMs and SpeechLLMs), except for Voxtral. This holds even though cascades do not directly access audio cues at the LLM stage. Contrary to prior work (Tsiamas et al., 2024), which found direct systems better at capturing prosody, our results show that current direct models are not better at handling emotional speech, where cascades remain more stable. In LONG-FORM , DeSTA2 and Qwen2-Audio show extreme length degradation (length 9194), suggesting poor suitability for document-level ST despite strong sentence-level results. SFMs9 achieve mid-range scores but still degrade due to their sentence-level optimization. In contrast, cascades with OWSM, Canary, and Voxtral achieve nearzero length, indicating far superior long-form robustness. Interestingly, these cascaded systems degrade slightly on short-form inputs (negative length), suggesting higher level of LLM research maturity in handling long context (as also shown by Pang et al., 2025) compared to SpeechLLMs and SFMs. Among SpeechLLMs, Voxtral is again particularly notable: although it uses chunking for acoustic encoding (see Appendix D), it re-concatenates all chunk representations before feeding them into the LLM, enabling real longcontext ST. This architectural design makes Voxtral the strongest option for real long-form scenarios, which, in contrast to Canary, OWSM, and Whisper, can actually exploit contextual information. All in all, we summarize the main findings as:"
        },
        {
            "title": "Takeaways",
            "content": "Cascade systems remain the most reliable overall, delivering the strongest and most consistent translation quality across languages, benchmarks, and acoustic conditions. SpeechLLMs show growing potential: the best model approaches or matches cascades in several settings, particularly when speech and language components are tightly integrated. Standalone SFMs lag behind both cascades and SpeechLLMs, indicating that the improved linguistic abilities achieved by current LLMs are crucial for accurate translation. No paradigm dominates universally, and robustness is phenomenon-dependent: Cascades excel on emotional and long-form speech, SpeechLLMs are more resilient to noise and code switching, accent/dialect performance is primarily encoder-driven across paradigms, and gender bias disparity and named entities accuracy are tied to the LLM decoder."
        },
        {
            "title": "5.2 Analysis",
            "content": "Gender Bias. Beyond gender-term disparities (F1 ), WinoST enables assessing whether models favour gender-stereotypical translations. prostereotypical set contains occupations aligned with common societal biases (e.g., developer tagged as male, hairdresser as female), while an antistereotypical set inverts these assignments (e.g., developer as female, hairdresser as male). We compute the Stereotypical Gap (S ) as per9Seamless and Spire were excluded as they lack native support for long-form inference. formance gap (Section 3.3) where QA = %pro and QB = %anti are the accuracy of the set of sentences with pro-stereotypical entities and the set with anti-stereotypical entities respectively. Figure 1 shows the relationship between F1 and . Systems utilizing Tower+ demonstrate the most equitable performance, clustering near 0 with negligible bias across both metrics. In contrast, other systems show higher scores, indicating significant degradation when translating antistereotypical roles. This suggests these models over-rely on training distribution priors rather than resolving gender based on context cues. These findings align with previous works on textual translation (Savoldi et al., 2025), as well as LLM generation (Kotek et al., 2023). Furthermore, there is positive correlation between F1 of 0.54, suggesting that models which struggle with gender co-reference also tend to exhibit stronger pro-stereotypical bias. and Figure 1: Plot showing the relationship between Gender Coreference Gap (F1 ) and Stereotypical Gap (S ) across all evaluated systems. Figure 2: Standard deviation of xCOMETQE scores for ManDi (zh-en) and CommonAccent (all other directions) across source-language accent. Numerical values for all cells can be found in Table 26. cents, revealing pronounced instability for most SpeechLLMs (DeSTA2, Phi-4-Multimodal, and Spire) on CommonAccent, and for cascaded systems on ManDi, driven by large gaps between standard Mandarin and other dialects. Across datasets, the most challenging accents include South Asian English, Austrian German, Rioplatense Spanish, and Basilicata-Trentino Italian. In ManDi, standard Mandarin yields the highest scores, while Taiyuan performs worst, likely reflecting both training data biases toward the standard variety and linguistic divergence, such as the Taiyuan tone merger (Zhao and Chodroff, 2022). Overall, these results show that strong ST performance on standard variety does not reliably transfer to other accents/dialects, underscoring the need for more diverse and accentaware training strategies (Lonergan et al., 2023; Hopton and Chodroff, 2025; Sameti et al., 2025)."
        },
        {
            "title": "5.3 Human Evaluation",
            "content": "Accents. On the ACCENTS benchmarks, x-en directions generally score lower than en-x. An exception is Phi-4-Multimodal, whose x-en performance exceeds en-x (xCOMETQE 80.5 vs. 75.1). In ManDi, zh-en scores are substantially lower than CommonAccent x-en results  (Table 14)  . While averages provide coarse view of accent robustness, they obscure patterns in models weaknesses and strengths with respect to performance on specific accents, which we report in Appendix H. To summarize this variability, Figure 2 presents the standard deviation of xCOMETQE across source acSo far, we have relied on automated tools to quantify the quality of translations, either by automated metrics or by detecting the output language. However, these tools are not perfect (Lavie et al., 2025) and also do not reveal the nature of the errors. To evaluate the reliability of automated metrics, we ran small-scale human evaluation of three models, resulting in the best (on average) for each paradigm in the GENERIC category: Seamless (SFM), Voxtral (SpeechLLM), and Canary+Aya (cascade), where humans annotated the translation outputs of the CoVoST2 dataset. The annotations were done by five native speakers of the respective non-English languages.10 We used combination of ESA and MQM protocols (Kocmi et al., 2024b; Freitag et al., 2021) with three model outputs next to each other (an extension of side-by-side by Song et al., 2025). We used Pearmut (Zouhar, 2026) as an annotation interface (see Appendix A) and collect the scores (e.g. 80/100) as well as marked error types (e.g. Accuracy/Omission). We show the averaged scores in Table 3, which mimic the automated results (e.g. Canary + Aya>Seamless, see Table 10), especially for x-en translation where Canary+Aya outperforms Voxtral and Seamless. Canary+ Aya"
        },
        {
            "title": "Average",
            "content": "81.66 en-de en-es en-it en-zh en-nl de-en es-en it-en 85.21 80.28 78.61 76.00 63.47 84.03 94.79 90.43 80.41 82.69 84.17 79.23 72.78 68.03 80.60 94.44 81.02 78. 84.52 89.60 78.93 54.87 67.78 77.08 89.92 83.41 Table 3: Average scores for human evaluation. Each language pair had 60 items annotated. The distribution of error types in Table 4 suggests that there is little to no qualitative difference in the types of errors even if the models are principally different (e.g., cascade vs. SpeechLLM). Similar to textual machine translation, simple mistranslations are the most common errors (Freitag et al., 2021). Omissions are two times more frequent in the SpeechLLM than they are in cascade and SFM, and SpeechLLM and SFM suffer more from undertranslation than cascaded models (as previously demonstrated by Bentivogli et al., 2021). As expected, models employing LLMs are more affected by overtranslation (Bawden and Yvon, 2023), doubling this error compared to the SFM. Lastly, wrong terminology represents the second most frequent error type, attesting to 11.5-12.5% of the identified errors, and underscoring the importance of measuring NE and term accuracy, as discussed in Section 5.1. Correlation with Automatic Metrics. Finally, we measure the agreement between human scores and automated metrics. We focus on two abilities 10We release the human annotations data under the CC BY 4.0 license at https://github.com/sarapapi/ hearing2translate/tree/main/evaluation_human/ hearing2translate-v1. Canary+ Aya"
        },
        {
            "title": "Voxtral Seamless",
            "content": "71 45.2% Accuracy/Mistranslation Terminology/Wrong term 18 11.5% 14 8.9% Accuracy/Overtranslation 9 5.7% Style/Unidiomatic style 7 4.5% Linguistic/Grammar 6 3.8% Accuracy/Omission Accuracy/Undertranslation 4 2.5% 7 4.5% Style/Awkward style 5 3.2% Linguistic/Punctuation 5 3.2% Style/Language register 3 1.9% Accuracy/Addition 1 0.6% Accuracy/Untranslated 3 1.9% Terminology/Inconsistent 3 1.9% Linguistic/Spelling 0 0.0% Locale/Number format 0 0.0% Linguistic/Conventions 1 0.6% Other/Other 64 41.3% 21 13.5% 12 7.7% 8 5.2% 5 3.2% 10 6.5% 9 5.8% 4 2.6% 4 2.6% 4 2.6% 5 3.2% 2 1.3% 4 2.6% 2 1.3% 1 0.6% 0 0.0% 0 0.0% 77 46.1% 21 12.6% 7 4.2% 8 4.8% 12 7.2% 5 3.0% 7 4.2% 9 5.4% 8 4.8% 1 0.6% 1 0.6% 5 3.0% 1 0.6% 3 1.8% 1 0.6% 1 0.6% 0 0.0% Table 4: Distribution of errors (from MQM error typology) per model (summed across all languages). xCOMETQE item global"
        },
        {
            "title": "METRICXQE\nS\nitem",
            "content": "global"
        },
        {
            "title": "Average",
            "content": "0.460 0.152 0.574 0.134 en-de en-es en-it en-zh en-nl de-en es-en it-en 0.341 0.613 0.453 0.523 0.312 0.630 0.416 0. 0.098 0.237 -0.002 0.250 0.202 0.189 0.112 0.128 0.412 0.807 0.556 0.546 0.531 0.676 0.592 0.470 0.054 0.181 0.103 0.239 0.149 0.042 0.081 0.224 Table 5: Correlations (item=group-by-item Spearman, global=micro-Pearson) between human scores and strict versions of automated metrics. of automated metrics: (1) general scoring, and (2) ranking model outputs of the same source. We measure the first by global Pearson and the second by group-by-item Spearman (Lavie et al., 2025). The results in Table 5 indicate that, at the item level, automatic metrics are not good at distinguishing between models, likely also due to the presence of close ties and fine-grained quality differences. However, at the global level across most languages, both metrics achieve micro-Pearson correlation of around 0.5, level comparable to that reported for reference-based metrics (Macháˇcek et al., 2023; Han et al., 2024). This suggests that automatic QE metrics produce reliable assessments for comparing ST systems in all our settings, supporting their usage in our study."
        },
        {
            "title": "6 Conclusions",
            "content": "In this paper, we aimed to answer the question: Does integrating the speech modality directly into LLMs truly enhance speech-to-text (ST) translation, or do cascaded architectures or traditional direct models remain the most effective solutions? To do this, we introduced Hearing to Translate, the first test suite for ST encompassing 13 language pairs, 9 phenomena, and 16 benchmarks (including new test set for assessing ST in noisy conditions). By leveraging the proposed test suite, we evaluated 21 systems: 5 SpeechLLMs and 16 strong models, between direct (4 SFMs) and cascaded systems (12 combinations of SFMs with state-of-theart LLMs). Our systematic evaluation shows that cascaded systems remain the most reliable overall, consistently outperforming both SpeechLLMs and standalone SFMs across languages and conditions. While SpeechLLMs are improving rapidly, they only match (or slightly surpass) cascades in limited set of scenarios, namely, noisy speech, code switching, and disfluencies, and only with the strongest model (Voxtral). Standalone SFMs consistently lag behind both paradigms, underscoring the critical role of LLMs (either integrated into the model or used within cascade) for achieving high-quality ST. Targeted analyses of gender bias and accent variation further reveal that all three paradigms struggle to leverage contextual cues for gender assignment, often defaulting to masculine forms, with bias mostly driven by the LLM component. In addition, models and, particularly, SpeechLLMs, exhibit high sensitivity to accent variation, showing substantial performance degradation. Finally, human evaluation highlights recurring ST error patterns, with mistranslations, terminology errors, and overtranslation emerging as the dominant failureswith the latter being especially prevalent in LLM-based systemsand the correlation with automatic metrics further supports the reliability of our evaluation methodology."
        },
        {
            "title": "Limitations",
            "content": "While our study provides comprehensive evaluation of SpeechLLMs across multiple languages, benchmarks, and speech phenomena, it has few inherent limitations. First, the analysis remains English-centric, reflecting the current language support of available SpeechLLMs. Expanding to fully multilingual setup will require broader model coverage and additional resources. Second, we do not report results for traditional neural MT models, as our focus is on assessing the integration of speech within LLMs and the comparison with cascaded and direct speech-to-text translation pipelines. Third, we do not include toxicity or safety benchmarks, since no publicly available datasets currently target these aspects in speech-totext translation. Lastly, we do not report latency measurements, as we positioned our work in offline conditions, where real-time performance is not the main focus. Despite these constraints, our work provides the first systematic, phenomenonaware evaluation of SpeechLLMs, offering critical insights into their translation quality, robustness, and the practical trade-offs between integrated and modular architectures."
        },
        {
            "title": "Acknowledgments",
            "content": "We extend our appreciations to Nuo Xu and David Kaczér for their human annotation effort. This work has received funding from the European Unions Horizon research and innovation programme under grant agreement No 101135798, project Meetween (My Personal AI Mediator for Virtual MEETings BetWEEN This research was also supported People). by the G-LAMP Program of the National Research Foundation of Korea (NRF) grant funded by the Ministry of Education (No. RS-202525441317). This work was also supported by MLLM4TRA (PID2024-158157OB-C32) funded by MCIN/AEI/10.13039/501100011033/FEDER, UE. This work was also supported by EU Horizon 2020 project ELOQUENCE13 (grant number 101070558). This work is also funded by the Ministerio para la Transformación Digital de la Función Pú blica and Plan de Recuperación, Transformación Resiliencia Funded by EU NextGenerationEU within the framework of the project Modelos del Lenguaje. The research leading to these results has also received funding from EU4Health Programme 20212027 as part of Europes Beating Cancer Plan under Grant Agreements no. 101129375; and from the Government of Spains grant PID2021-122443OB-I00 funded by MICIU/AEI/10.13039/501100011033 and by ERDF/EU, in addition to the financial support of Generalitat Valenciana under project IDIFEDER/2021/059. Zachary Hopton was supported by the Swiss National Science Foundation (Grant No. 10003607). Vilém Zouhar gratefully acknowledges the support of the Google PhD Fellowship."
        },
        {
            "title": "References",
            "content": "Idris Abdulmumin, Victor Agostinelli, Tanel Alumäe, Antonios Anastasopoulos, Luisa Bentivogli, Ondˇrej Bojar, Claudia Borg, Fethi Bougares, Roldano Cattoni, Mauro Cettolo, Lizhong Chen, William Chen, Raj Dabre, Yannick Estève, Marcello Federico, Mark Fishel, Marco Gaido, Dávid Javorský, Marek Kasztelnik, and 33 others. 2025. Findings of the IWSLT 2025 evaluation campaign. In Proceedings of the 22nd International Conference on Spoken Language Translation (IWSLT 2025), pages 412481. Association for Computational Linguistics. Abdelrahman Abouelenin, Atabak Ashfaq, Adam Atkinson, Hany Awadalla, Nguyen Bach, Jianmin Bao, Alon Benhaim, Martin Cai, Vishrav Chaudhary, Congcong Chen, and 1 others. 2025. Phi-4-mini technical report: Compact yet powerful multimodal language models via mixture-of-loras. arXiv preprint arXiv:2503.01743. Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, and 1 others. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774. Ibrahim Said Ahmad, Antonios Anastasopoulos, Ondˇrej Bojar, Claudia Borg, Marine Carpuat, Roldano Cattoni, Mauro Cettolo, William Chen, Qianqian Dong, Marcello Federico, Barry Haddow, Dávid Javorský, Mateusz Krubinski, Tsz Kin Lam, Xutai Ma, Prashant Mathur, Evgeny Matusov, Chandresh Maurya, John McCrae, and 25 others. 2024. FINDINGS OF THE IWSLT 2024 EVALUATION CAMIn Proceedings of the 21st International PAIGN. Conference on Spoken Language Translation (IWSLT 2024), pages 111. Association for Computational Linguistics. Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebron, and Sumit Sanghai. 2023. GQA: Training generalized multi-query transformer models from multi-head checkpoints. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 4895 4901. Association for Computational Linguistics. Duarte Miguel Alves, José Pombal, Nuno Guerreiro, Pedro Henrique Martins, João Alves, Amin Farajian, Ben Peters, Ricardo Rei, Patrick Fernandes, Sweta Agrawal, Pierre Colombo, José G. C. de Souza, and Andre Martins. 2024. Tower: An open multilingual large language model for translation-related tasks. In First Conference on Language Modeling. Kshitij Ambilduke, Ben Peters, Sonal Sannigrahi, Anil Keshwani, Tsz Kin Lam, Bruno Martins, André F. T. Martins, and Marcely Zanon Boito. 2025. From tower to spire: Adding the speech modality to textonly llm. Preprint, arXiv:2503.10620. Ebrahim Ansari, Amittai Axelrod, Nguyen Bach, Ondˇrej Bojar, Roldano Cattoni, Fahim Dalvi, Nadir Durrani, Marcello Federico, Christian Federmann, Jiatao Gu, Fei Huang, Kevin Knight, Xutai Ma, Ajay Nagesh, Matteo Negri, Jan Niehues, Juan Pino, Elizabeth Salesky, Xing Shi, and 4 others. 2020. FINDINGS OF THE IWSLT 2020 EVALUATION CAMPAIGN. In Proceedings of the 17th International Conference on Spoken Language Translation, pages 134. Association for Computational Linguistics. Mohamed Anwar, Bowen Shi, Vedanuj Goswami, WeiNing Hsu, Juan Pino 0001, and Changhan Wang. 2023. Muavic: multilingual audio-visual corpus for robust speech recognition and robust speech-totext translation. In 24th Annual Conference of the International Speech Communication Association, Interspeech 2023, Dublin, Ireland, August 20-24, 2023, pages 40644068. ISCA. Rosana Ardila, Megan Branson, Kelly Davis, Michael Kohler, Josh Meyer, Michael Henretty, Reuben Morais, Lindsay Saunders, Francis Tyers, and Gregor Weber. 2020. Common voice: massivelymultilingual speech corpus. In Proceedings of the Twelfth Language Resources and Evaluation Conference, pages 42184222. European Language Resources Association. Giuseppe Attanasio, Beatrice Savoldi, Dennis Fucci, and Dirk Hovy. 2024. Twists, humps, and pebbles: Multilingual speech recognition models exhibit gender performance gaps. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 2131821340. Association for Computational Linguistics. Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed, and Michael Auli. 2020. wav2vec 2.0: framework for self-supervised learning of speech representations. Advances in neural information processing systems, 33:1244912460. Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, and 1 others. 2023. Qwen technical report. arXiv preprint arXiv:2309.16609. Ankur Bapna, Colin Cherry, Yu Zhang, Ye Jia, Melvin Johnson, Yong Cheng, Simran Khanuja, Jason Riesa, and Alexis Conneau. 2022. mslam: Massively multilingual joint pre-training for speech and text. arXiv preprint arXiv:2202.01374. Loïc Barrault, Yu-An Chung, Mariano Cora Meglioli, David Dale, Ning Dong, Paul-Ambroise Duquenne, Hady Elsahar, Hongyu Gong, Kevin Heffernan, John Hoffman, and 1 others. 2023. Seamlessm4t: massively multilingual & multimodal machine translation. arXiv preprint arXiv:2308.11596. Rachel Bawden and François Yvon. 2023. Investigating the translation performance of large multilingual language model: the case of BLOOM. In Proceedings of the 24th Annual Conference of the European Association for Machine Translation, pages 157170. European Association for Machine Translation. Luisa Bentivogli, Mauro Cettolo, Marco Gaido, Alina Karakanta, Alberto Martinelli, Matteo Negri, and Marco Turchi. 2021. Cascade versus direct speech translation: Do the differences still make differIn Proceedings of the 59th Annual Meetence? ing of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 28732887. Association for Computational Linguistics. Alexandre Bérard, Olivier Pietquin, Christophe Servan, and Laurent Besacier. 2016. Listen and translate: proof of concept for end-to-end speech-to-text translation. arXiv preprint arXiv:1612.01744. Yunfei Chu, Jin Xu, Qian Yang, Haojie Wei, Xipin Wei, Zhifang Guo, Yichong Leng, Yuanjun Lv, Jinzheng He, Junyang Lin, and 1 others. 2024. Qwen2-audio technical report. arXiv preprint arXiv:2407.10759. Alexis Conneau, Min Ma, Simran Khanuja, Yu Zhang, Vera Axelrod, Siddharth Dalmia, Jason Riesa, Clara Rivera, and Ankur Bapna. 2022. Fleurs: Few-shot learning evaluation of universal representations of speech. arXiv preprint arXiv:2205.12446. Marta R. Costa-jussà, Christine Basta, and Gerard I. Gállego. 2022. Evaluating gender bias in speech translation. In Proceedings of the Thirteenth Language Resources and Evaluation Conference, pages 21412147. European Language Resources Association. Marta Costa-Jussà, James Cross, Onur Çelebi, Maha Elbayad, Kenneth Heafield, Kevin Heffernan, Elahe Kalbassi, Janice Lam, Daniel Licht, Jean Maillard, and 1 others. 2022. No language left behind: Scaling human-centered machine translation. arXiv preprint arXiv:2207.04672. John Dang, Shivalika Singh, Daniel Dsouza, Arash Ahmadian, Alejandro Salamanca, Madeline Smith, Aidan Peppin, Sungjin Hong, Manoj Govindassamy, Terrence Zhao, Sandra Kublik, Meor Amer, Viraat Aryabumi, Jon Ander Campos, Yi-Chern Tan, Tom Kocmi, Florian Strub, Nathan Grinsztajn, Yannis FletBerliac, and 26 others. 2024. Aya expanse: Combining research breakthroughs for new multilingual frontier. Preprint, arXiv:2412.04261. Daniel Deutsch, Eleftheria Briakou, Isaac Rayburn Caswell, Mara Finkelstein, Rebecca Galor, Juraj Juraska, Geza Kovacs, Alison Lui, Ricardo Rei, Jason Riesa, Shruti Rijhwani, Parker Riley, Elizabeth Salesky, Firas Trabelsi, Stephanie Winkler, Biao Zhang, and Markus Freitag. 2025. WMT24++: Expanding the language coverage of WMT24 to 55 languages & dialects. In Findings of the Association for Computational Linguistics: ACL 2025, pages 12257 12284. Association for Computational Linguistics. Markus Freitag, George Foster, David Grangier, Viresh Ratnakar, Qijun Tan, and Wolfgang Macherey. 2021. Experts, errors, and context: large-scale study of human evaluation for machine translation. Transactions of the Association for Computational Linguistics, 9:14601474. Markus Freitag, Nitika Mathur, Chi-kiu Lo, Eleftherios Avramidis, Ricardo Rei, Brian Thompson, Tom Kocmi, Frederic Blain, Daniel Deutsch, Craig Stewart, Chrysoula Zerva, Sheila Castilho, Alon Lavie, and George Foster. 2023. Results of WMT23 metrics shared task: Metrics might be guilty but references are not innocent. In Proceedings of the Eighth Conference on Machine Translation, pages 578628. Association for Computational Linguistics. Marco Gaido, Sara Papi, Matteo Negri, and Luisa Bentivogli. 2024. Speech translation with speech foundation models and large language models: What is there and what is missing? In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1476014778. Association for Computational Linguistics. Marco Gaido, Susana Rodríguez, Matteo Negri, Luisa Bentivogli, and Marco Turchi. 2021. Is moby dick whale or bird? named entities and terminology in speech translation. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 17071716. Association for Computational Linguistics. Xavier Garcia, Yamini Bansal, Colin Cherry, George Foster, Maxim Krikun, Melvin Johnson, and Orhan Firat. 2023. The unreasonable effectiveness of fewIn Intershot learning for machine translation. national Conference on Machine Learning, pages 1086710878. PMLR. Gemma Team, Aishwarya Kamath, Johan Ferret, Shreya Pathak, Nino Vieillard, Ramona Merhej, Sarah Perrin, Tatiana Matejovicova, Alexandre Ramé, Morgane Rivière, and 1 others. 2025. Gemma 3 technical report. arXiv preprint arXiv:2503.19786. Naman Goyal, Cynthia Gao, Vishrav Chaudhary, PengJen Chen, Guillaume Wenzek, Da Ju, Sanjana Krishnan, MarcAurelio Ranzato, Francisco Guzmán, and Angela Fan. 2022. The Flores-101 evaluation benchmark for low-resource and multilingual machine translation. Transactions of the Association for Computational Linguistics, 10:522538. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad AlDahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, and 1 others. 2024. The llama 3 herd of models. arXiv preprint arXiv:2407.21783. Nuno M. Guerreiro, Ricardo Rei, Daan van Stigt, Luisa Coheur, Pierre Colombo, and André F. T. Martins. 2024. xcomet: Transparent machine translation evaluation through fine-grained error detection. Transactions of the Association for Computational Linguistics, 12:979995. Anmol Gulati, James Qin, Chung-Cheng Chiu, Niki Parmar, Yu Zhang, Jiahui Yu, Wei Han, Shibo Wang, Zhengdong Zhang, Yonghui Wu, and Ruoming Pang. 2020. Conformer: Convolution-augmented transformer for speech recognition. In Interspeech 2020, pages 50365040. Shangmin Guo, Biao Zhang, Tianlin Liu, Tianqi Liu, Misha Khalman, Felipe Llinares, Alexandre Rame, Thomas Mesnard, Yao Zhao, Bilal Piot, and 1 others. 2024. Direct language model alignment from online ai feedback. arXiv preprint arXiv:2402.04792. HyoJung Han, Kevin Duh, and Marine Carpuat. 2024. SpeechQE: Estimating the quality of direct speech translation. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 2185221867. Association for Computational Linguistics. Zachary Hopton and Eleanor Chodroff. 2025. The impact of dialect variation on robust automatic speech recognition for Catalan. In Proceedings of the The 22nd SIGMORPHON workshop on Computational Morphology, Phonology, and Phonetics, pages 2333. Association for Computational Linguistics. Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov, and Abdelrahman Mohamed. 2021. Hubert: Self-supervised speech representation learning by masked prediction of hidden units. IEEE/ACM Trans. Audio, Speech and Lang. Proc., 29:34513460. Edward Hu, yelong shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2022. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations. Sathish Indurthi, Houjeung Han, Nikhil Kumar Lakumarapu, Beomseok Lee, Insoo Chung, Sangha Kim, and Chanwoo Kim. 2020. End-end speech-to-text translation with modality agnostic meta-learning. In ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 79047908. J. Iranzo-Sánchez, J. A. Silvestre-Cerdà, J. Jorge, N. Roselló, A. Giménez, A. Sanchis, J. Civera, and A. Juan. 2020. Europarl-st: multilingual corpus for speech translation of parliamentary debates. In ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 82298233. Juraj Juraska, Daniel Deutsch, Mara Finkelstein, and Markus Freitag. 2024. MetricX-24: The Google submission to the WMT 2024 metrics shared task. In Proceedings of the Ninth Conference on Machine Translation, pages 492504. Association for Computational Linguistics. Tom Kocmi, Ekaterina Artemova, Eleftherios Avramidis, Rachel Bawden, Ondˇrej Bojar, Konstantin Dranch, Anton Dvorkovich, Sergey Dukanov, Mark Fishel, Markus Freitag, Thamme Gowda, Roman Grundkiewicz, Barry Haddow, Marzena Karpinska, Philipp Koehn, Howard Lakougna, Jessica M. Lundin, Christof Monz, Kenton Murray, Findings of the WMT25 and 10 others. 2025. general machine translation shared task: Time to stop evaluating on easy test sets. In Proceedings of the Tenth Conference on Machine Translation, China. Association for Computational Linguistics. Tom Kocmi, Eleftherios Avramidis, Rachel Bawden, Ondˇrej Bojar, Anton Dvorkovich, Christian Federmann, Mark Fishel, Markus Freitag, Thamme Gowda, Roman Grundkiewicz, Barry Haddow, Marzena Karpinska, Philipp Koehn, Benjamin Marie, Christof Monz, Kenton Murray, Masaaki Nagata, Martin Popel, Maja Popovic, and 3 others. 2024a. Findings of the WMT24 general machine translation shared task: The LLM era is here but MT is not solved yet. In Proceedings of the Ninth Conference on Machine Translation, pages 146. Association for Computational Linguistics. Tom Kocmi, Vilém Zouhar, Eleftherios Avramidis, Roman Grundkiewicz, Marzena Karpinska, Maja Popovic, Mrinmaya Sachan, and Mariya Shmatova. 2024b. Error span annotation: balanced approach for human evaluation of machine translation. In Proceedings of the Ninth Conference on Machine Translation, pages 14401453. Association for Computational Linguistics. Tom Kocmi, Vilém Zouhar, Christian Federmann, and Matt Post. 2024c. Navigating the metrics maze: Reconciling score magnitudes and accuracies. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 19992014. Association for Computational Linguistics. Sai Koneru, Maike Züfle, Thai Binh Nguyen, Seymanur Akti, Jan Niehues, and Alexander Waibel. 2025. KITs offline speech translation and instruction following submission for IWSLT 2025. In Proceedings of the 22nd International Conference on Spoken Language Translation (IWSLT 2025), pages 232244. Association for Computational Linguistics. Ye Jia, Yifan Ding, Ankur Bapna, Colin Cherry, Yu Zhang, Alexis Conneau, and Nobu Morioka. 2022. Leveraging unsupervised and weakly-supervised data to improve direct speech-to-speech translation. In Interspeech 2022, pages 17211725. Hadas Kotek, Rikker Dockum, and David Q. Sun. 2023. Gender bias and stereotypes in large language models. In Proceedings of The ACM Collective Intelligence Conference, CI 2023, Delft, Netherlands, November 6-9, 2023, pages 1224. ACM. Oleksii Kuchaiev, Jason Li, Huyen Nguyen, Oleksii Hrinchuk, Ryan Leary, Boris Ginsburg, Samuel Kriman, Stanislav Beliaev, Vitaly Lavrukhin, Jack Cook, and 1 others. 2019. Nemo: toolkit for building ai applications using neural modules. arXiv preprint arXiv:1909.09577. Taku Kudo and John Richardson. 2018. Sentencepiece: simple and language independent subword tokenizer and detokenizer for neural text processing. Preprint, arXiv:1808.06226. Siddique Latif, Moazzam Shoukat, Fahad Shamshad, Muhammad Usama, Yi Ren, Heriberto Cuayáhuitl, Wenwu Wang, Xulong Zhang, Roberto Togneri, Erik Cambria, and 1 others. 2023. Sparks of large audio models: survey and outlook. arXiv preprint arXiv:2308.12792. Alon Lavie, Greg Hanneman, Sweta Agrawal, Diptesh Kanojia, Chi-Kiu Lo, Vilém Zouhar, Frederic Blain, Chrysoula Zerva, Eleftherios Avramidis, Sourabh Deoghare, Archchana Sindhujan, Jiayi Wang, David Ifeoluwa Adelani, Brian Thompson, Tom Kocmi, Markus Freitag, and Daniel Deutsch. 2025. Findings of the WMT25 shared task on automated translation evaluation systems: Linguistic diversity is challenging and references still help. In Proceedings of the Tenth Conference on Machine Translation, pages 436 483, Suzhou, China. Association for Computational Linguistics. Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. 2023. Blip-2: Bootstrapping language-image pretraining with frozen image encoders and large language models. In International conference on machine learning, pages 1973019742. PMLR. Alexander Liu, Andy Ehrenberg, Andy Lo, Clément Denoix, Corentin Barreau, Guillaume Lample, Jean-Malo Delignon, Khyathi Raghavi Chandu, Patrick von Platen, Pavankumar Reddy Muddireddy, arXiv preprint and 1 others. 2025. Voxtral. arXiv:2507.13264. Liam Lonergan, Mengjie Qian, Neasa Ní Chiaráin, Christer Gobl, and Ailbhe Ní Chasaide. 2023. Towards dialect-inclusive recognition in low-resource language: Are balanced corpora the answer? In 24th Annual Conference of the International Speech Communication Association, Interspeech 2023, Dublin, Ireland, August 20-24, 2023, pages 50825086. ISCA. Ke-Han Lu, Zhehuai Chen, Szu-Wei Fu, ChaoHan Huck Yang, Jagadeesh Balam, Boris Ginsburg, Yu-Chiang Frank Wang, and Hung-yi Lee. 2024. Desta2: Developing instruction-following speech language model without speech instruction-tuning data. arXiv preprint arXiv:2409.20007. Dominik Macháˇcek, Ondˇrej Bojar, and Raj Dabre. 2023. MT metrics correlate with human ratings of simultaneous speech translation. In Proceedings of the 20th International Conference on Spoken Language Translation (IWSLT 2023), pages 169179. Association for Computational Linguistics. Evgeny Matusov, Gregor Leusch, Oliver Bender, and Hermann Ney. 2005. Evaluating machine translation output with automatic sentence segmentation. In Proceedings of the Second International Workshop on Spoken Language Translation. Anna Min, Chenxu Hu, Yi Ren, and Hang Zhao. 2025. When end-to-end is overkill: Rethinking cascaded speech-to-text translation. arXiv preprint arXiv:2502.00377. H. Ney. 1999. Speech translation: coupling of recognition and translation. In 1999 IEEE International Conference on Acoustics, Speech, and Signal Processing. Proceedings. ICASSP99 (Cat. No.99CH36258), volume 1, pages 517520 vol.1. Thai-Son Nguyen, Sebastian Stüker, Jan Niehues, and Alex Waibel. 2020. Improving sequence-to-sequence speech recognition training with on-the-fly data augmentation. In ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 76897693. Tu Anh Nguyen, Wei-Ning Hsu, Antony DAvirro, Bowen Shi, Itai Gat, Maryam Fazel-Zarani, Tal Remez, Jade Copet, Gabriel Synnaeve, Michael Hassid, Felix Kreuk, Yossi Adi, and Emmanuel Dupoux. 2023. Expresso: benchmark and analysis of discrete expressive speech resynthesis. In Interspeech 2023, pages 48234827. Tu Anh Nguyen, Benjamin Muller, Bokai Yu, Marta R. Costa-jussa, Maha Elbayad, Sravya Popuri, Christophe Ropers, Paul-Ambroise Duquenne, Robin Algayres, Ruslan Mavlyutov, Itai Gat, Mary Williamson, Gabriel Synnaeve, Juan Pino, Benoît Sagot, and Emmanuel Dupoux. 2025a. SpiRitLM: Interleaved spoken and written language model. Transactions of the Association for Computational Linguistics, 13:3052. Tu Anh Nguyen, Benjamin Muller, Bokai Yu, Marta R. Costa-jussa, Maha Elbayad, Sravya Popuri, Christophe Ropers, Paul-Ambroise Duquenne, Robin Algayres, Ruslan Mavlyutov, Itai Gat, Mary Williamson, Gabriel Synnaeve, Juan Pino, Benoît Sagot, and Emmanuel Dupoux. 2025b. Spirit-lm: Interleaved spoken and written language model. Transactions of the Association for Computational Linguistics, 13:3052. Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. 2015. Librispeech: An asr corpus based on public domain audio books. In 2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 52065210. Jianhui Pang, Fanghua Ye, Derek Fai Wong, Dian Yu, Shuming Shi, Zhaopeng Tu, and Longyue Wang. 2025. Salute the classic: Revisiting challenges of machine translation in the age of large language models. Transactions of the Association for Computational Linguistics, 13:7395. Sara Papi, Peter Polák, Dominik Macháˇcek, and Ondˇrej Bojar. 2025a. How real is your real-time simultaneous speech-to-text translation system? Transactions of the Association for Computational Linguistics, 13:281313. Sara Papi, Maike Züfle, Marco Gaido, Beatrice Savoldi, Danni Liu, Ioannis Douros, Luisa Bentivogli, and Jan Niehues. 2025b. Mcif: Multimodal crosslingual instruction-following benchmark from scientific talks. Preprint, arXiv:2507.19634. Yifan Peng, Muhammad Shakeel, Yui Sudo, William Chen, Jinchuan Tian, Chyi-Jiunn Lin, and Shinji Watanabe. 2025. OWSM v4: Improving Open Whisper-Style Speech Models via Data Scaling and Cleaning. In Interspeech 2025, pages 22252229. Juan Pino, Liezl Puzon, Jiatao Gu, Xutai Ma, Arya D. McCarthy, and Deepak Gopinath. 2019. Harnessing indirect training data for end-to-end automatic speech translation: Tricks of the trade. In Proceedings of the 16th International Conference on Spoken Language Translation. Association for Computational Linguistics. Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever. 2023. Robust speech recognition via large-scale weak supervision. In International conference on machine learning, pages 2849228518. PMLR. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. 2023. Direct preference optimization: Your language model is secretly reward model. Advances in neural information processing systems, 36:5372853741. Nithin Rao Koluguri, Monica Sekoyan, George Zelenfroynd, Sasha Meister, Shuoyang Ding, Sofia Kostandian, He Huang, Nikolay Karpov, Jagadeesh Balam, Vitaly Lavrukhin, Yifan Peng, Sara Papi, Marco Gaido, Alessio Brutti, and Boris Ginsburg. 2025. Granary: Speech Recognition and Translation Dataset in 25 European Languages. In Interspeech 2025, pages 39233927. Ricardo Rei, Nuno Guerreiro, José Pombal, João Alves, Pedro Teixeirinha, Amin Farajian, and André FT Martins. 2025. Tower+: Bridging generality and translation specialization in multilingual llms. arXiv preprint arXiv:2506.17080. Dima Rekesh, Samuel Kriman, Somshubra Majumdar, Vahid Noroozi, He Juang, Oleksii Hrinchuk, Ankur Kumar, and Boris Ginsburg. 2023. Fast conformer with linearly scalable attention for efficient speech recognition. 2023 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU), pages 18. Elijah Rippeth, Sweta Agrawal, and Marine Carpuat. 2022. Controlling translation formality using pretrained multilingual language models. In Proceedings of the 19th International Conference on Spoken Language Translation (IWSLT 2022), pages 327340. Association for Computational Linguistics. Paul Rubenstein, Chulayuth Asawaroengchai, Duc Dung Nguyen, Ankur Bapna, Zalán Borsos, Félix de Chaumont Quitry, Peter Chen, Dalia El Badawy, Wei Han, Eugene Kharitonov, and 1 others. 2023. Audiopalm: large language model that can speak and listen. arXiv preprint arXiv:2306.12925. Elizabeth Salesky, Kareem Darwish, Mohamed AlBadrashiny, Mona Diab, and Jan Niehues. 2023. Evaluating multilingual speech translation under realistic conditions with resegmentation and terminology. In Proceedings of the 20th International Conference on Spoken Language Translation (IWSLT 2023), pages 6278. Association for Computational Linguistics. Mohammad Hossein Sameti, Sepehr Harfi Moridani, Ali Zarean, and Hossein Sameti. 2025. Accentinvariant automatic speech recognition via saliencydriven spectrogram masking. Gabriele Sarti, Phu Mon Htut, Xing Niu, Benjamin Hsu, Anna Currey, Georgiana Dinu, and Maria Nadejde. 2023. RAMP: Retrieval and attribute-marking enhanced prompting for attribute-controlled translation. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 14761490. Association for Computational Linguistics. Beatrice Savoldi, Jasmijn Bastings, Luisa Bentivogli, and Eva Vanmassenhove. 2025. decade of gender bias in machine translation. Patterns, 6(7):101257. Björn W. Schuller. 2018. Speech emotion recognition: two decades in nutshell, benchmarks, and ongoing trends. Commun. ACM, 61(5):9099. Seamless Comm., Loïc Barrault, Yu-An Chung, Mariano Coria Meglioli, David Dale, Ning Dong, Mark Duppenthaler, Paul-Ambroise Duquenne, Brian Ellis, Hady Elsahar, Justin Haaheim, John Hoffman, MinJae Hwang, Hirofumi Inaguma, Christopher Klaiber, Ilia Kulikov, Pengwei Li, Daniel Licht, Jean Maillard, and 46 others. 2023. Seamless: Multilingual expressive and streaming speech translation. Monica Sekoyan, Nithin Rao Koluguri, Nune Tadevosyan, Piotr Zelasko, Travis Bartley, Nikolay Karpov, Jagadeesh Balam, and Boris Ginsburg. 2025. Canary-1b-v2 & parakeet-tdt-0.6b-v3: Efficient and high-performance models for multilingual asr and ast. Preprint, arXiv:2509.14128. Muhammad Shah, David Solans Noguero, Mikko Heikkila, Bhiksha Raj, and Nicolas Kourtellis. 2024. Speech robust bench: robustness bencharXiv preprint mark for speech recognition. arXiv:2403.07937. Noam Shazeer. 2020. Glu variants improve transformer. Preprint, arXiv:2002.05202. Translation, pages 12351257. Association for Computational Linguistics. David Snyder, Guoguo Chen, and Daniel Povey. 2015. MUSAN: Music, Speech, and Noise Corpus. ArXiv:1510.08484v1. Changhan Wang, Anne Wu, and Juan Pino. 2020. Covost 2: massively multilingual speech-to-text translation corpus. Preprint, arXiv:2007.10310. Yixiao Song, Parker Riley, Daniel Deutsch, and Markus Freitag. 2025. Enhancing human evaluation in machine translation with comparative judgment. arXiv preprint arXiv:2502.17797. Matthias Sperber and Matthias Paulik. 2020. Speech translation and the end-to-end promise: Taking stock In Proceedings of the 58th Anof where we are. nual Meeting of the Association for Computational Linguistics, pages 74097421. Association for Computational Linguistics. Gabriel Stanovsky, Noah A. Smith, and Luke Zettlemoyer. 2019. Evaluating gender bias in machine translation. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 16791684. Association for Computational Linguistics. David Stap, Eva Hasler, Bill Byrne, Christof Monz, and Ke Tran. 2024. The fine-tuning paradox: Boosting translation quality without sacrificing LLM abilities. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 61896206. Association for Computational Linguistics. Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. 2023. Roformer: Enhanced transformer with rotary position embedding. Preprint, arXiv:2104.09864. Haoqin Sun, Xuechen Wang, Jinghua Zhao, Shiwan Zhao, Jiaming Zhou, Hui Wang, Jiabei He, Aobo Kong, Xi Yang, Yequan Wang, Yonghua Lin, and Yong Qin. 2025. Emotiontalk: An interactive chinese multimodal emotion dataset with rich annotations. Preprint, arXiv:2505.23018. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, and 1 others. 2023. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971. Marcos Treviso, Nuno Guerreiro, Sweta Agrawal, Ricardo Rei, José Pombal, Tania Vaz, Helena Wu, Beatriz Silva, Daan Van Stigt, and Andre Martins. 2024. xTower: multilingual LLM for explaining and correcting translation errors. In Findings of the Association for Computational Linguistics: EMNLP 2024, pages 1522215239. Association for Computational Linguistics. Ioannis Tsiamas, Matthias Sperber, Andrew Finch, and Sarthak Garg. 2024. Speech is more than words: Do speech-to-text translation systems leverage prosody? In Proceedings of the Ninth Conference on Machine Wenxuan Wang, Yingxin Zhang, Yifan Jin, Binbin Du, and Yuke Li. 2025. NYAs offline speech translation system for IWSLT 2025. In Proceedings of the 22nd International Conference on Spoken Language Translation (IWSLT 2025), pages 206211. Association for Computational Linguistics. Shinji Watanabe, Takaaki Hori, Shigeki Karita, Tomoki Hayashi, Jiro Nishitoba, Yuya Unno, Nelson Enrique Yalta Soplin, Jahn Heymann, Matthew Wiesner, Nanxin Chen, Adithya Renduchintala, and Tsubasa Ochiai. 2018. ESPnet: End-to-end speech processIn Proceedings of Interspeech, pages ing toolkit. 22072211. Ron J. Weiss, Jan Chorowski, Navdeep Jaitly, Yonghui Wu, and Zhifeng Chen. 2017. Sequence-to-sequence models can directly translate foreign speech. In Interspeech 2017, pages 26252629. Chen Xu, Rong Ye, Qianqian Dong, Chengqi Zhao, Tom Ko, Mingxuan Wang, Tong Xiao, and Jingbo Zhu. 2023. Recent advances in direct speech-to-text translation. In Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence, IJCAI 23. Brian Yan, Injy Hamed, Shuichiro Shimizu, Vasista Sai Lodagala, William Chen, Olga Iakovenko, Bashar Talafha, Amir Hussein, Alexander Polok, Kalvin Chang, Dominik Klement, Sara Althubaiti, Puyuan Peng, Matthew Wiesner, Thamar Solorio, Ahmed Ali, Sanjeev Khudanpur, and Shinji Watanabe. 2025. CS-FLEURS: Massively Multilingual and CodeSwitched Speech Dataset. In Interspeech 2025, pages 743747. Liang Zhao and Eleanor Chodroff. 2022. The ManDi corpus: spoken corpus of Mandarin regional dialects. In Proceedings of the Thirteenth Language Resources and Evaluation Conference, pages 1985 1990. European Language Resources Association. Jiaming Zhou, Yujie Guo, Shiwan Zhao, Haoqin Sun, Hui Wang, Jiabei He, Aobo Kong, Shiyao Wang, Xi Yang, Yequan Wang, Yonghua Lin, and Yong Qin. 2025. Cs-dialogue: 104-hour dataset of spontaneous mandarin-english code-switching dialogues for speech recognition. Preprint, arXiv:2502.18913. Vilém Zouhar. 2026. Pearmut: Platform for evaluating and reviewing of multilingual tasks. Vilém Zouhar and Ondˇrej Bojar. 2024. Quality and quantity of machine translation references for auIn Proceedings of the Fourth tomatic metrics. Workshop on Human Evaluation of NLP Systems (HumEval) @ LREC-COLING 2024, pages 111. ELRA and ICCL. Vilém Zouhar, Pinzhen Chen, Tsz Kin Lam, Nikita Moghe, and Barry Haddow. 2024. Pitfalls and outlooks in using COMET. In Proceedings of the Ninth Conference on Machine Translation, pages 1272 1288. Association for Computational Linguistics. Juan Zuluaga-Gomez, Sara Ahmed, Danielius Visockas, and Cem Subakan. 2023. Commonaccent: Exploring large acoustic pretrained models for accent classiIn 24th Annual fication based on common voice. Conference of the International Speech Communication Association, Interspeech 2023, Dublin, Ireland, August 20-24, 2023, pages 52915295. ISCA. Figure 3: Screenshot of the Pearmut (Zouhar, 2026) annotation interface together with annotation guidelines. The annotator first listens to the source audio, then scans the three model outputs where they mark error spans with severities and categories. Lastly, the annotator assigns the final scores and proceeds to the next item."
        },
        {
            "title": "A Human Evaluation Interface",
            "content": "Annotation guidelines and interface are shown in Figure 3."
        },
        {
            "title": "B Benchmarks Details",
            "content": "FLEURS. FLEURS is benchmark dataset containing n-way parallel speech and text in 102 languages. It was built using the FLoRes-101 machine translation benchmark (Goyal et al., 2022) and provides approximately 12 hours of speech supervision for each language. The dataset is designed to evaluate ASR, ST, language identification, and Retrieval. Notably, during data collection, balance in the sex ratio (at least 30/70%) of the native speakers was imposed where possible, meaning the dataset contains speaker gender information and can be used to analyze gender bias (Attanasio et al., 2024). CoVoST2. CoVoST2 is ST benchmark created for 15 English-to-many and 21 many-to-English language pairs. The source segments (audio and transcripts) are derived from validated segments in version 4 of Common Voice (Ardila et al., 2020). Source segments are translated by professionals and verified using embedding-based approaches and length heuristics to ensure quality. debates carried out in the European Parliament in the period between 2008 and 2012 and their associated transcriptions and translations. The basic unit of this corpus is speech, an intervention made by single speaker in the Parliament. Full audios of the interventions are provided, alongside speaker metadata and gold sentence segmentation. Quality estimation for en-zh models was added in this article based on results on the source audios of the en-de subset. WMT. The General Machine Translation Shared Task annually measures broad progress in state-ofthe-art machine translation. Although primarily focused on the textual modality, it has included speech domain since 2024, built from publicly available one-minute YouTube videos. From each video, segment was randomly sampled, with minimum duration of 30 seconds and maximum of 50 seconds, containing at least 30% speech. This makes the benchmark particularly challenging, as other generic benchmarks usually contain segments shorter than 30 seconds and without background music and non-speech pheonomena. Each year covers about 10-15 language pairs (majority out-ofEnglish), for which human references are created and human-evaluation of translation quality is carried out. EuroParlST. EuroParlST is many-to-many ST dataset with transcripts and translations covering 9 European languages. It was constructed using the WinoST. WinoST is dataset designed to evaluate gender bias in ST systems. It is the speech version of the WinoMT dataset (Stanovsky et al., 2019), and is used to assess inaccuracies in translations that arise from gender stereotypes, focusing on the gender information present in the sentence content rather than the speakers voice. CommonAccent. Designed for accent classification and accent-robust ASR, CommonAccent compiles validated segments that include locale annotations from four languages in versions 7 (en) and 11 (de, es, it) of Common Voice (Ardila et al., 2020). The authors aimed to balance the test set for each language by limiting the number of segments from each language variety to 100. The final dataset includes 4-16 varieties per language. Though the datasets labels often confound spoken accents and dialect variation, such as morphosyntactic or lexicon differences, it still provides insight to tested systems robustness to geographic variation in spoken language. ManDi. This benchmark focuses on Mandarin Chinese dialects. It contains 9.6 hours of spoken recordings from 36 speakers representing six major regional Mandarin dialects (Beijing, Chengdu, Jinan, Taiyuan, Wuhan, and Xian) alongside Standard Mandarin. Each participant read five sets of reading materials, first in Standard Mandarin and then in their own dialect, producing total of 357 recordings. We discard the word list recordings, instead using only the read poem and short passage from each participant. The benchmark serves to evaluate dialect variation and accent robustness in ST systems. CS-Dialogue. CS-Dialogue is 104-hour dataset of spontaneous Mandarin-English dialogues involving 200 speakers. The conversations span seven topics and follow structured progression from Mandarin, through code-switching, to English. We use only the code-switching portion of the test split, which mainly consists of Mandarin utterances with embedded English. CS-FLEURS. CS-FLEURS is derived dataset from FLEURS that covers 52 languages and offers both real and synthetic code-switched data for training and evaluation of ASR and ST on codeswitching phenomena. For this work, we evaluate subset of X-English language pairs featuring read human speech, focusing on the English target directions as in the original paper. LibriStutter. LibriStutter is dataset constructed from LibriSpeech (Panayotov et al., 2015) through the automatic insertion of disfluencies over audiobook utterances. Specifically, it includes interjections, sound repetitions, word and phrase repetitions, as well as prolongations. Its primary objective is to assess the impact of disfluencies on ST quality. NEuRoparlST. NEuRoparlST is derivative of EuroparlST for English to Spanish, French, and Italian with manually annotated Named Entities (NEs) and domain terminology for both transcripts and translated texts. This enables the evaluation of ST systems in translating NEs and terminology. NoisyFLEURS. It is derivative of the FLEURS dataset (Conneau et al., 2022), created specifically within this work for evaluating noise-robust multilingual speech models. We add two types of realistic noise, babble (B), and ambient (A) sourced from the MUSAN corpus (Snyder et al., 2015) to simulate challenging acoustic conditions using the method of Anwar et al. (2023). We will release the data under CC-BY-NC 4.0 license upon paper acceptance. EmotionTalk. This dataset contains Chinese multimodal dyadic conversations recorded with 19 professional actors. It is annotated for seven emotion categories (happy, surprise, sad, disgust, anger, fear, neutral), emotion intensity, and speaking-style captions. Each utterance provides audio, video, and text modalities. mExpresso. It is benchmark based on an expanded subset of the Expresso dataset (Nguyen et al., 2023) containing seven styles of read speech (i.e., default, happy, sad, confused, enunciated, whisper, and laughing) between English and five other languages: French, German, Italian, Mandarin, and Spanish. ACL 60/60. It provides multilingual ST evaluation resource based on recorded ACL 2022 presentations, designed to reflect realistic conditions such as long, unsegmented audio and domain-specific terminology. It includes English audio with corresponding transcripts and translations into 10 target languages. The audio was first transcribed using ASR systems and then manually post-edited, with sentence-level resegmentation applied to ensure alignment and accuracy. MT outputs were also post-edited to maintain consistency and accurately handle technical terminology. MCIF. MCIF (Multimodal Crosslingual Instruction Following) is benchmark for evaluating multilingual instruction-following capabilities in multimodal LLMs. Unlike most existing benchmarks, MCIF offers comprehensive dataset of 3 fully parallel modalities (text, speech, and video) across 4 languages, featuring both short-form and longform inputs. It provides up to 10 hours of humanannotated content from scientific talks, including transcripts, translations, summaries, and Q&A pairs across 13 diverse tasks (including ASR and ST). This collection allows for comprehensive assessment of multimodal LLMs cross-lingual and cross-modal instruction-following abilities."
        },
        {
            "title": "C Evaluation Settings",
            "content": "All evaluations were conducted using Python 3.9.16. We used the following metric implementations and settings: report xCOMET: We from the unbabel/xcomet-xxl model. Scores were computed using the comet library (v2.2.2) with fp32 precision. scores METRICX: We using report google/metricx-24-hybrid-xxl-v2p6-bfloat16 model checkpoint. scores the"
        },
        {
            "title": "D Model Details",
            "content": "The description of the SFMs, LLMs, and SpeechLLMs used in our study are provided below. The details about the specific model versions, size, and libraries are reported in Table 6. D.1 SFMs We select the most popular SFMs supporting translation tasks and covering wide set of languages: Whisper. It is Transformer encoder-decoder model trained on large-scale weakly and pseudolabeled audio in many languages for ASR and direct many-to-English translation. We use the bestperforming large-v3 model with 1.5B parameters that was trained on 5M hours for 99 languages, from which 58 achieve better than 50% WER on the ASR task. To process long-form audio, we adopt the chunked decoding pipeline provided in Transformers.11 This approach processes the input in 30-second segments, dynamically shifting 11https://huggingface.co/openai/whisper-largev3#chunked-long-form the window based on timestamps predicted by the model itself. It also incorporates strategies such as temperature scaling and beam search to mitigate timestamp inaccuracies. We do not employ any external voice activity detection tool when using Whisper in this work. Seamless. SeamlessM4T is foundational allin-one Massively Multilingual and Multimodal Machine Translation model covering multiple languages and modalities. We use the v2-large model, supporting 101-to-96 speech-to-text languages. For ST, it is composed of Conformer encoder (Gulati et al., 2020) initialized from w2v-BERT 2.0 (Baevski et al., 2020), pretrained on over 4M hours, and Transformer decoder initialized from NLLB (Costa-Jussà et al., 2022). Since there is no standard implementation for processing long-form speech with this model, we do not process it in this work. Canary. It is FastConformer encoder (Rekesh et al., 2023) and Transformer decoder model trained for ASR and English-to-X and X-to-English ST for 25 European languages. The model is trained on 1.7M hours contained in Granary (Rao Koluguri et al., 2025), covering various domains. We use the v2 version with 1B parameters, which is released under permissive CC BY 4.0 license. Long-form audio is handled by the default implementation in the NeMo toolkit. It segments the audio into 30-40 second chunks, with 1-second overlap between adjacent chunks. The overlapping transcripts are then merged with the longest common subsequence algorithm. OWSM. The Open Whisper-style Speech Model (OWSM) is family of open speech foundation models trained on academic-scale resources with reproducible pipelines, covering over 150 languages. Initially inspired by the Whisper architecture, successive releases have progressively improved performance through larger datasets, refined preprocessing, and more powerful architectures. We use the CTC-based encoder-only variant of OWSM 4.0 (latest version at the time of writing) for its superior robustness on long-form input, faster inference, and stronger ST performance compared to its encoder-decoder counterpart. Moreover, it is currently the only large-scale non-autoregressive model supporting both ST and ASR, making it especially interesting for this study. Long-form inference is performed using the batched parallel decod-"
        },
        {
            "title": "Model",
            "content": "Param. Category"
        },
        {
            "title": "Weights HFv",
            "content": "Aya (Dang et al., 2024) Gemma3 (Gemma Team et al., 2025) Tower+ (Rei et al., 2025) Whisper (Radford et al., 2023) Seamless (Barrault et al., 2023) Canary (Sekoyan et al., 2025) OWSM (Peng et al., 2025) DeSTA2 (Lu et al., 2024) Phi-4-Multimodal (Abouelenin et al., 2025) Qwen2-Audio (Chu et al., 2024) Spire (Ambilduke et al., 2025) Voxtral (Liu et al., 2025) 32B 12B 9B 1.6B 2.3B 1B 1B 8B 5.6B 7B 7B 24B link LLM link LLM link LLM link SFM link SFM link SFM SFM link SpeechLLM link SpeechLLM link SpeechLLM link SpeechLLM link SpeechLLM link 4.51.0 4.51.0 4.51.0 4.51.3 4.51.3 4.51.3 4.48.2 4.51.3 4.40.1 4.54.0 Table 6: Details of the analyzed models, including the number of parameters, category (LLM, SFM, and SpeechLLM), their public weights release, and the HuggingFace Transformer version (HFv). ing algorithm implemented in ESPnet (Watanabe et al., 2018). D.2 LLMs To maintain comparable sizes with existing SFMs and SpeechLLMs, and to allow easier reproducibility of the outputs, we choose to include one medium-sized model (20-40B parameters), one small model (<20B parameters), and one translation-specific LLM. To select the actual models, we rely on the WMT25 General MT Findings (Kocmi et al., 2025), identifying the top-performing LLMs that met our size constraints for each language pair under consideration. For the translation-specific and small-model categories, the choice was clear with Tower+ 9B and Gemma 3 12B standing out in their respective categories. For the medium-sized category, we considered Aya Expanse 32B and Gemma 3 27B, and ultimately selected Aya Expanse due to its stronger performance across more language pairs as well as to promote model diversity in our selection. Aya. Aya Expanse 32B is decoder-only multilingual model built upon the Cohere Command architecture and optimized for 23 high-resource and mid-resource languages, covering all of the languages in our scope. It incorporates standard modern Transformer components such as SwiGLU activations (Shazeer, 2020), RoPE positional embeddings (Su et al., 2023), and Grouped-Query Attention (Ainslie et al., 2023). Its maximum context window is 128k tokens. The model is trained with two-stage multilingual preference optimization pipeline: offline preference training followed by online preference training. It is further improved through weighted model merging across intermediate checkpoints. Aya Expanse combines strong general-purpose multilingual capabilities with competitive translation performance in wide range of language pairs. Gemma3. Gemma 3 12B is small multimodal model, supporting both image and text inputs and over 140 languages. Similarly to Aya, it offers 128k-token context window and uses decoder-only architecture with Grouped-Query Attention (Ainslie et al., 2023). Training includes distillation from larger Gemma models and post-training phase targeting multilingual and instruction-following performance. Gemma 3 12B offers strong balance between model size, multilingual coverage, and general-purpose performance. Tower+. Tower+ 9B is translation-focused model developed on top of the Gemma 2 9B foundation. Its training follows four-stage recipe: Continued Pretraining to strengthen multilingual representations, Instruction Tuning, Weighted Preference Optimization, and Reinforcement Learning with Verifiable Rewards. Tower+ surpasses larger general-purpose LLMs in translation quality in some of our selected language pairs, making it competitive specialized option while being the smallest text LLM in our scope. D.3 SpeechLLMs We select the SpeechLLMs available on HuggingFace and covering translation tasks (e.g., models covering English transcription only are discarded): Phi-4-Multimodal. Phi-4-Multimodal is multimodal LLM that integrates text, image, and speech input modalities into single model. The pretrained speech encoder (consisting of 3 convolution layerswith total subsampling rate of 8, and 80ms token rateand 24 Conformer blocks; Gulati et al., 2020) is connected with the Phi-4-Mini LLM through an audio adapter (a 2-layer MLP), and LoRA (Hu et al., 2022) is applied to the LLM. The training follows 2-stage approach: pre-training with large-scale ASR data (of approximately 2M hours) to align the speech encoder and the adapter with Phi-4-Mini in the semantic space (leaving only the LLM frozen), and post-training with about 100M curated supervised samples (updating both the adapter and LoRA parameters only). The model covers 8 input languages: Chinese, English, French, German, Italian, Japanese, Portuguese, and Spanish. Given the 128k context length of the LLM, theoretically Phi-4-Multimodal can support maximum of 2.8 hours of audio (as 750 tokens corresponds to 1-minute audio), but the model has not been finetuned on long audio data over 30 minutes. Qwen2-Audio. It is large-scale SpeechLLM (Apache 2.0 license) featuring two distinct audio interaction modes for voice chat and audio analysis. In voice chat mode, users can engage in voice interactions without textual input. In the audio analysis mode, users can provide both audio and text instructions during the interaction. Qwen2-Audio is based on the Whisper large-v3 encoder with an additional pooling layer (performing subsampling of 2) and Qwen-7B (Bai et al., 2023) LLM. The model is first pre-trained on multiple tasks (including ASR) with natural language prompts, then it is fine-tuned with the two audio interaction modes, and, lastly, DPO (Rafailov et al., 2023) is applied. DeSTA2. It is SpeechLLM built on Whispersmall and LLaMa 3 (Grattafiori et al., 2024), augmented with Q-Former adapter (Li et al., 2023). It is trained on mix of datasets totaling 155 hours (including speech with noise and reverberation) covering multiple tasks, with additional metadata such as speaker gender, age, accent, and emotion extracted from external models. Both the LLM and the Whisper components are kept frozen during training. Unlike the other SpeechLLMs considered in this study, DeSTA2 uses both the encoder and decoder of Whisper, providing the transcript alongside speech features to the LLM, implementing hybrid between direct and cascaded architectures. context window of 32k tokens and up to 40 minutes of speech input. The models are trained in three phases: pretraining (with speech-text interleaving; Nguyen et al. 2025b), supervised finetuning (with mixture of synthetized data), and preference alignment (with standard and online DPO; Guo et al. 2024). We adopt the small version with 24B parameters that is made of the Whisper encoder, which processes the input in chunks of 30s, an MLP adapter, which maps the audio sequence in the LLM embedding space by also performing downsampling of 4, and the Mistral Small 3.1 24B12 model as decoder. Spire. Spire is speech-augmented LLM with 7B parameters, released under the CC-BY-NC 4.0 license. It builds on the multilingual LLM Tower (Alves et al., 2024) by introducing discretized speech interface, where acoustic representations from HuBERT (Hsu et al., 2021) are quantized with k-means clustering. Training follows twostage strategy: continued pretraining of TowerBase on mixed text-speech data, and subsequent instruction tuning on text translation, ASR, and ST tasks. The main variant, SpireFull, preserves strong texttranslation performance from Tower, while extending the model to English speech recognition and translation. It is important to note that the model is only instruction-tuned for speech recognition and translation tasks, and it relies on tightly defined instruction formats. As result, its scope remains narrow, and Spire should be considered as particular case of SpeechLLM with no general-purpose capabilities."
        },
        {
            "title": "E Prompts",
            "content": "The prompts used for LLMs and SpeechLLMs13 are reported below. The {src_lang} and {tgt_lang} are replaced with the extended language name (e.g., English or Chinese (Simplified))."
        },
        {
            "title": "Your\nmeaning",
            "content": "You are professional {src_lang}-to-{tgt_lang} accurately goal translator. the and the convey original {src_lang} text while adhering to {tgt_lang} grammar, vocabulary, and cultural sensitivities. Preserve the line breaks. Use precise terminology and tone appropriate for to nuances is of Voxtral. Voxtral is family of two open-weight SpeechLLMs (Apache 2.0 license) supporting 12https://mistral.ai/news/mistral-small-3-1 13For Spire, we use the prompt template it was trained on: Speech: {DSUs}n{tgt_lang}:"
        },
        {
            "title": "G Results per Language",
            "content": "S and METRICXQE Absolute xCOMETQE scores by language are reported for each benchmark in Tables 8-25. NEuRoparl-ST, ACL 60/60, and MCIF require target-side references to compute benchmarkspecific metrics; evaluation is therefore limited to the languages originally supported by each benchmark. WinoST is reference-free but relies on language-specific POS taggers. As result, we report WinoST results for the languages supported by the original implementation (es, fr, it, de), and additionally extend it to Portuguese. academic or instructional materials. Produce only the {tgt_lang} translation, without any additional explanations or commentary. Please translate the provided {src_lang} text into {tgt_lang}:"
        },
        {
            "title": "SpeechLLMs Prompt",
            "content": "of is"
        },
        {
            "title": "Your\nmeaning",
            "content": "to nuances You are professional {src_lang}-to-{tgt_lang} goal translator. accurately the and the convey original {src_lang} speech while adhering to {tgt_lang} grammar, vocabulary, and cultural terminology sensitivities. precise and or academic for Produce only the instructional materials. {tgt_lang} translation, without any additional explanations or commentary. Please translate the provided {src_lang} speech into {tgt_lang}:"
        },
        {
            "title": "Use\nappropriate",
            "content": "tone a"
        },
        {
            "title": "S Overall Results",
            "content": "We report in Table 7 (on the next page) overall results using METRICXQE . To ensure consistency with xCOMETQE where higher values indicate better performance, we transform the scores using the METRICXQE formula 100 , mapping the values to the [0, 100] range. Additionally, we exclude F1 , %NE, and %term metrics as they are not computed via QE models."
        },
        {
            "title": "FLEURS",
            "content": "CoVoST2 EuroParl-ST WMT"
        },
        {
            "title": "CommonAccent ManDi",
            "content": "CS-Dialogue CS-FLEURS en-x x-en en-x METRICXQE x-en en-x x-en en-x en-x x-en METRICXQE x-en en-x accent zh-en"
        },
        {
            "title": "METRICXQE",
            "content": "S zh-en x-en"
        },
        {
            "title": "OWSM",
            "content": "Whisper + Aya + Gemma3 + Tower+ Seamless + Aya + Gemma3 + Tower+ Canary + Aya + Gemma3 + Tower+ OWSM + Aya + Gemma3 + Tower+ DeSTA2 Qwen2-Audio Phi-4-Multimodal"
        },
        {
            "title": "Spire",
            "content": "- 86.2 - 49.9 91.3 91. 91.1 91.3 91.3 91.3 91.5 91. 91.4 89.9 89.7 89.5 79.3 79. 68.1 92.7 81.3 83.0 86.3 - 51.9 91.5 90.8 91.3 90.4 89. 89.9 - - - 89.5 88. 89.0 83.4 82.5 86.6 91.0 - - 86.1 - 52.9 84.8 84. 84.7 88.1 87.9 87.9 86.2 85. 85.9 84.4 84.1 84.0 68.6 78. 59.0 85.4 70.2 72.8 80.2 64. 48.0 81.9 80.8 81.4 82.5 81. 82.0 - - - 81.0 79. 80.2 70.2 74.7 71.0 80.1 - - 70.9 - 54.8 90.3 89. 90.2 89.7 89.4 89.6 91.0 90. 90.9 89.0 88.6 88.8 64.0 83. 66.2 90.2 83.6 78.7 81.8 83. 55.0 85.1 84.4 84.9 85.4 84. 85.3 85.8 85.0 85.6 83.5 82. 83.2 76.9 79.3 76.4 84.7 - - - 26.4 -1.0 - 29. 80.4 78.6 78.3 42.9 42.9 41. 80.3 79.8 78.2 58.8 57.9 54. 55.7 47.9 49.6 79.8 52.0 - -7.1 -0.6 -0.6 -0.8 -1.0 -1. -1.4 -0.2 -0.4 0.1 -0.8 0. -0.4 -2.3 -1.7 -1.7 -0.9 -0. 0.5 0.1 - 2.2 -0.0 -0. 0.5 0.1 0.1 0.2 - - - 0.2 0.1 0.6 -1.7 0. 0.7 0.6 - - 86.8 - 50.2 84.6 84.0 83.9 89.0 88. 88.5 86.5 86.0 86.0 83.6 83. 82.9 68.0 78.3 71.6 85.7 73. 76.1 82.1 80.8 54.7 83.7 82. 83.4 84.3 83.4 83.8 84.4 83. 83.9 82.4 81.1 81.0 72.1 75. 77.5 83.3 - 32.5 37.0 - 31.6 18.6 32.4 34.6 10.0 22. 18.8 - - - 32.4 49. 47.5 32.3 1.9 8.9 11.3 - 69.8 65.0 - 27.4 79.7 77. 77.8 76.3 72.3 71.2 - - - 70.9 65.3 65.7 72.8 72. 61.8 79.5 - 70.9 79.4 - 53.4 85.1 83.9 84.6 82.6 81. 81.9 - - - 80.3 77. 78.4 76.5 79.9 80.5 86.0 -"
        },
        {
            "title": "EMOTION",
            "content": "LONG-FORM"
        },
        {
            "title": "NoisyFLEURSB NoisyFLEURSA mExpresso",
            "content": "EmotionTalk ACL6060 MCIF"
        },
        {
            "title": "METRICXQE",
            "content": "S zh-en"
        },
        {
            "title": "OWSM",
            "content": "Whisper + Aya + Gemma3 + Tower+ Seamless + Aya + Gemma3 + Tower+ Canary + Aya + Gemma3 + Tower+ OWSM + Aya + Gemma3 + Tower+ DeSTA2 Qwen2-Audio Phi-4-Multimodal"
        },
        {
            "title": "Spire",
            "content": "disfluency en-x - 36.6 - 25.7 2. 3.4 3.9 7.6 11.3 9.7 6. 9.6 8.2 7.2 10.6 8.2 7. 11.5 12.3 2.7 10.9 noise en-x x-en en-x x-en en-x - 59. - 75.8 50.8 51.3 50.1 53. 53.8 54.5 57.4 57.6 58.3 65. 68.5 79.4 69.5 38.4 53.9 34. 77.6 48.6 51.0 - 68.4 47. 48.3 44.2 51.7 53.5 53.1 - - - 61.9 64.7 65.7 66. 50.9 27.9 37.3 - - 10. 11.4 - 9.0 - 19.8 15. 7.1 7.4 7.1 8.4 8.3 8. 7.8 7.5 7.9 12.8 13.1 98. 17.8 8.0 4.6 4.0 43.9 8. 8.6 8.0 8.3 8.8 8.6 - - - 12.4 13.6 84.6 17. 12.4 5.4 5.3 - - 80. - 63.7 87.8 86.7 86.8 86. 85.4 84.9 87.6 86.5 86.7 86. 85.6 85.8 74.9 76.1 33.7 86. 77.7 length en-x en-x - - - 24.7 3.8 3.7 4.8 - - - -2.7 -1.8 -1.6 -2. -2.2 -2.9 88.6 94.9 -9.7 -0. - - - - 9.3 4. 4.4 5.2 - - - -0. 0.9 0.7 -1.1 -1.7 -1.3 90. 92.0 19.5 1.2 - 75.2 74. - 32.9 84.7 83.1 82.8 84. 82.4 82.6 - - - 82. 79.5 80.5 77.0 78.7 74.7 80. - Table 7: Overall performance of the 21 evaluated systems computed using METRICXQE . en-x denotes averages across all target languages, except where each benchmark covers specific subset (e.g., ACL 60/60: de/fr/zh/pt; MCIF: de/it/zh). x-en denotes averages across all source languages for each benchmark, as per Table 1. - e - e - e - n - e - z - p - n - i - r - e - d - e - e - e - n - e - e - z - p - n - t - f - e - d - e - e - n - e - e - e - z - p - l - i - f - e - d - e"
        },
        {
            "title": "L\nX\nc\ni\nr\nt\ne",
            "content": "M"
        },
        {
            "title": "Y\nP\nA\nU\nG\nN\nI",
            "content": "L - 4 . 6 7 7 . 6 8 7 . 0 2 3 . 2 7 . 0 9 2 . 2 9 3 . 2 9 8 . 0 9 2 . 2 9 - - - 6 . 9 8 6 . 6 8 5 . 8 8 5 . 4 8 . 1 8 5 . 3 8 7 . 5 8 - 4 . 8 8 9 . 6 0 . 5 8 9 . 4 5 7 . 2 9 6 . 1 9 4 . 2 9 9 . 8 3 . 8 8 1 . 8 8 3 . 2 9 0 . 1 9 4 . 2 9 3 . 9 1 . 9 8 8 . 9 8 5 . 0 8 5 . 9 7 7 . 1 9 1 . 3 - 4 . 7 8 0 . 1 9 8 . 4 8 7 . 6 4 6 . 3 8 . 2 9 5 . 3 9 8 . 2 9 9 . 1 9 5 . 2 9 8 . 2 2 . 2 9 9 . 2 9 7 . 1 9 2 . 0 9 2 . 1 9 8 . 7 5 . 2 8 1 . 8 8 6 . 3 9 - 3 . 3 8 1 . 7 5 . 4 8 3 . 8 4 5 . 9 8 2 . 9 8 7 . 0 9 8 . 8 9 . 7 8 0 . 9 8 7 . 0 9 2 . 0 9 3 . 1 9 0 . 6 9 . 4 8 2 . 6 8 2 . 1 7 6 . 7 7 2 . 0 9 3 . 1 - 0 . 7 8 9 . 8 8 6 . 3 8 1 . 1 5 7 . 3 9 . 2 9 5 . 3 9 8 . 1 9 5 . 1 9 5 . 1 9 7 . 3 2 . 3 9 4 . 3 9 7 . 1 9 0 . 0 9 6 . 1 9 6 . 1 3 . 1 8 6 . 7 8 3 . 3 9 - 5 . 6 8 5 . 9 6 . 6 8 6 . 4 4 1 . 4 9 2 . 3 9 1 . 4 9 8 . 1 8 . 0 9 9 . 1 9 9 . 3 9 1 . 3 9 8 . 3 9 9 . 1 2 . 0 9 8 . 1 9 9 . 1 8 8 . 0 8 5 . 7 8 0 . 4 - - - 7 . 3 7 8 . 7 5 2 . 8 9 . 7 8 8 . 8 8 6 . 8 8 8 . 8 8 3 . 9 8 8 . 8 6 . 8 8 8 . 9 8 3 . 7 8 3 . 7 8 9 . 7 8 8 . 8 5 . 0 8 1 . 7 7 8 . 9 8 2 . 9 6 - 5 . 0 7 . 0 9 5 . 4 3 6 . 3 9 7 . 3 9 0 . 3 9 4 . 3 7 . 3 9 2 . 3 9 8 . 3 9 0 . 4 9 5 . 3 9 7 . 1 0 . 2 9 4 . 1 9 2 . 3 8 5 . 2 8 1 . 0 8 7 . 4 0 . 3 8 - 5 . 3 9 8 . 2 9 4 . 9 4 3 . 5 1 . 5 9 5 . 5 9 2 . 5 9 8 . 4 9 5 . 5 9 7 . 5 4 . 5 9 5 . 5 9 8 . 3 9 1 . 4 9 0 . 4 9 9 . 6 5 . 2 8 3 . 5 6 0 . 7 9 5 . 6 8 - 9 . 9 2 . 0 9 5 . 6 4 1 . 3 9 0 . 3 9 3 . 3 9 3 . 3 1 . 3 9 2 . 3 9 8 . 3 9 7 . 3 9 8 . 3 9 5 . 1 9 . 1 9 0 . 2 9 3 . 8 7 0 . 9 7 7 . 4 8 2 . 5 0 . 2 8 - 3 . 8 8 4 . 8 8 7 . 9 3 0 . 2 8 . 0 9 2 . 1 9 2 . 2 9 0 . 1 9 4 . 1 9 6 . 2 1 . 1 9 8 . 1 9 4 . 0 9 2 . 9 8 7 . 9 8 5 . 4 6 . 0 8 2 . 9 3 3 . 3 9 8 . 6 7 - 5 . 0 8 . 0 9 6 . 6 5 6 . 3 9 2 . 3 9 4 . 3 9 2 . 3 2 . 3 9 6 . 3 9 7 . 3 9 4 . 3 9 8 . 3 9 8 . 1 7 . 1 9 0 . 2 9 6 . 4 8 2 . 1 8 8 . 9 7 5 . 5 8 . 2 8 - 0 . 4 9 5 . 4 9 7 . 7 7 7 . 6 6 . 6 9 8 . 6 9 7 . 6 9 5 . 6 9 9 . 6 9 9 . 6 7 . 6 9 9 . 6 9 9 . 5 9 8 . 5 9 0 . 6 9 6 . 1 2 . 9 8 9 . 0 7 7 . 7 9 6 . 9 8 - 2 . 6 1 . 6 8 9 . 1 2 1 . 3 9 8 . 1 9 8 . 2 9 7 . 2 5 . 1 9 6 . 2 9 - - - 7 . 0 3 . 7 8 3 . 9 8 8 . 2 8 2 . 4 8 9 . 4 8 9 . 7 - 1 . 6 8 3 . 5 8 7 . 4 8 4 . 1 6 8 . 0 3 . 0 9 5 . 0 9 8 . 8 8 8 . 8 8 5 . 7 8 6 . 0 1 . 0 9 3 . 0 9 7 . 8 8 8 . 8 8 6 . 8 8 7 . 4 7 . 1 8 5 . 9 8 3 . 1 9 - . 4 5 . 2 8 8 . 6 4 8 . 9 7 . 1 2 9 . 7 1 9 . 1 2 . 5 1 9 . 9 0 9 . 2 1 . 7 1 9 . 2 1 9 . 7 1 . 5 0 9 . 7 9 8 . 2 0 . 6 3 8 . 9 3 8 . 7 5 . 3 2 9 - . 8 2 8 . 6 5 8 . 3 4 8 . 2 9 5 . 3 9 8 . 9 8 8 . 8 9 8 . 6 8 8 . 9 7 8 . 4 8 8 . 9 9 8 . 3 9 8 . 0 0 9 . 8 6 8 . 9 5 8 . 5 6 8 . 5 9 7 . 6 1 8 . 6 8 8 . 6 0 9 - . 8 5 8 . 9 6 . 3 3 8 . 2 0 6 . 0 2 . 5 1 9 . 0 2 9 . 7 0 . 6 0 9 . 3 0 9 . 0 2 . 5 1 9 . 9 1 9 . 6 0 . 2 9 8 . 4 0 9 . 4 6 . 4 3 8 . 3 6 8 . 4 2 - . 9 1 8 . 5 5 8 . 3 3 8 . 6 0 5 . 4 1 9 . 7 0 9 . 9 0 9 . 0 0 9 . 2 9 8 . 6 9 8 . 3 1 9 . 7 0 9 . 7 0 9 . 6 9 8 . 2 8 8 . 1 9 8 . 4 3 8 . 1 0 8 . 4 4 8 . 5 1 9 - - - . 8 9 7 . 9 8 . 4 0 9 . 0 1 9 . 0 1 . 5 0 9 . 9 0 9 . 9 0 . 6 0 9 . 9 0 9 . 4 1 . 7 9 8 . 0 0 9 . 5 0 . 2 4 5 . 3 6 8 . 9 9 . 5 1 9 . 5 1 8 - . 7 4 8 . 9 5 8 . 2 1 3 . 4 9 8 . 9 8 8 . 7 8 8 . 4 9 8 . 4 9 8 . 0 9 8 . 6 9 8 . 4 9 8 . 2 9 8 . 6 7 8 . 0 7 8 . 3 6 8 . 9 0 8 . 4 6 7 . 8 3 7 . 4 0 9 . 9 7 7 - . 6 8 . 3 8 8 . 6 1 4 . 9 1 . 5 1 9 . 0 2 9 . 9 1 . 2 1 9 . 3 2 9 . 2 2 . 1 2 9 . 1 2 9 . 2 0 . 0 0 9 . 3 0 9 . 4 2 . 9 6 7 . 7 9 5 . 4 3 . 0 3 8 - . 1 7 8 . 3 7 8 . 8 5 4 . 1 1 9 . 1 1 9 . 0 1 9 . 3 1 9 . 5 1 9 . 3 1 9 . 2 1 9 . 6 1 9 . 3 1 9 . 4 9 8 . 7 9 8 . 3 9 8 . 1 2 8 . 8 6 7 . 5 0 8 . 3 3 9 . 5 1 8 - . 9 5 8 . 4 6 . 1 7 4 . 4 0 9 . 2 0 . 7 9 8 . 5 0 9 . 3 0 . 9 9 8 . 7 0 9 . 4 0 . 0 0 9 . 0 9 8 . 8 8 . 1 8 8 . 6 3 8 . 5 0 . 4 8 3 . 3 1 9 . 4 9 - . 6 6 8 . 0 7 8 . 2 2 5 . 0 1 9 . 0 1 9 . 7 0 9 . 8 0 9 . 0 1 9 . 0 1 9 . 2 1 9 . 9 0 9 . 0 1 9 . 2 9 8 . 3 9 8 . 8 8 8 . 0 4 8 . 0 9 7 . 0 6 7 . 0 3 9 . 6 0 8 - . 3 0 9 . 7 0 9 . 3 2 . 8 4 9 . 5 4 9 . 5 4 . 0 5 9 . 6 4 9 . 9 4 . 1 5 9 . 6 4 9 . 7 4 . 8 3 9 . 1 3 9 . 4 3 . 8 7 8 . 3 3 8 . 3 8 . 0 6 9 . 4 5 8 - . 2 0 - . 0 0 - . 5 3 - . 1 0 - . 4 0 - . 1 0 - . 4 0 - . 4 0 - . 0 0 - - - - . 2 0 - . 5 0 - . 0 0 - . 6 0 - . 1 5 - . 7 5 - . 2 0 - - . 0 0 - . 1 0 - . 0 0 - . 1 0 - . 3 0 - . 3 0 - . 3 0 - . 1 0 - . 0 0 - . 0 0 - . 0 0 - . 0 0 - . 0 0 - . 2 0 - . 0 0 - . 2 0 - . 0 0 - . 1 1 - . 2 0 - . 8 0 - - . 0 0 - . 0 0 - . 0 0 - . 6 0 - . 0 0 - . 0 0 - . 0 0 - . 2 0 - . 0 0 - . 0 0 - . 1 0 - . 0 0 - . 0 0 - . 2 0 - . 0 0 - . 3 0 - . 2 0 - . 7 0 - . 4 4 - . 1 0 - - . 1 0 - . 0 0 - . 0 0 - . 0 0 - . 3 0 - . 3 0 - . 0 0 - . 3 0 - . 4 0 - . 0 0 - . 3 0 - . 3 0 - . 0 0 - . 0 0 - . 3 0 - . 0 0 - . 4 0 - . 3 1 - . 3 0 - . 3 0 - - . 0 0 - . 0 0 - . 3 0 - . 2 0 - . 0 0 - . 0 0 - . 0 0 - . 0 0 - . 0 0 - . 0 0 - . 0 0 - . 1 0 - . 0 0 - . 1 0 - . 3 0 - . 0 0 - . 0 0 - . 9 0 - . 1 3 - . 0 0 - - . 0 0 - . 0 0 - . 0 0 - . 2 1 - . 0 0 - . 0 0 - . 0 0 - . 0 0 - . 0 0 - . 0 0 - . 0 0 - . 0 0 - . 0 0 - . 0 0 - . 1 0 - . 0 0 - . 0 0 - . 2 2 - . 9 4 - . 0 0 - - - - . 0 0 - . 0 0 - . 2 0 - . 0 0 - . 0 0 - . 0 0 - . 0 0 - . 0 0 - . 0 0 - . 0 0 - . 0 0 - . 0 0 - . 0 0 - . 0 0 - . 5 0 - . 2 8 - . 0 0 - . 0 0 - . 2 4 3 - - 3 . 0 - 2 . 0 - 7 . 4 3 - 3 . 0 - 5 . 0 - 6 . 0 - 2 . 0 - 2 . 0 - 3 . 0 - 3 . 0 - 0 . 0 - 5 . 0 - 3 . 0 - 3 . 0 - 8 . 0 - 6 . 0 - 6 . 2 - - 0 . 0 - 0 . 0 - 7 . 1 - 0 . 0 - 0 . 0 - 0 . 0 - 0 . 0 - 4 . 0 - 0 . 0 - 0 . 0 - 0 . 0 - 4 . 0 - 0 . 0 - 0 . 0 - 0 . 0 - 2 . 0 - 7 . 1 - 6 . 0 - 8 . 0 - 0 . 0 - 4 . 0 - 2 . 0 1 - 7 . 2 2 - - 0 . 0 - 0 . 0 - 5 . 2 - 0 . 0 - 2 . 0 - 0 . 0 - 0 . 0 - 0 . 0 - 0 . 0 - 0 . 0 - 0 . 0 - 0 . 0 - 0 . 0 - 0 . 0 - 0 . 0 - 2 . 0 - 5 . 2 - 2 . 4 - 0 . 0 - 0 . 0 - - 0 . 0 - 0 . 0 - 8 . 0 - 2 . 0 - 2 . 0 - 3 . 0 - 2 . 0 - 3 . 0 - 3 . 0 - 0 . 0 - 3 . 0 - 3 . 0 - 0 . 0 - 0 . 0 - 3 . 0 - 2 . 0 - 1 . 1 - - 6 . 0 - 6 . 0 - 3 . 2 - 3 . 0 - 3 . 0 - 3 . 0 - 6 . 0 - 3 . 0 - 3 . 0 - 5 . 0 - 3 . 0 - 3 . 0 - 3 . 0 - 3 . 0 - 3 . 0 - 2 . 0 - 8 . 2 - - 2 . 0 - 0 . 0 - 2 . 0 - 0 . 0 - 0 . 0 - 0 . 0 - 0 . 0 - 0 . 0 - 0 . 0 - 0 . 0 - 0 . 0 - 0 . 0 - 0 . 0 - 0 . 0 - 0 . 0 - 0 . 0 - 9 . 1 - 3 e + + o + 3 e + + o + 3 e + + o + 3 e + + o + +"
        },
        {
            "title": "M\nS\nW\nO",
            "content": "o A - 2 Q 2 e A + n y + l S + p W s s m y a C"
        },
        {
            "title": "M\nS\nW\nO",
            "content": "4 . 3 5 - 2 . 3 1 - 6 . 5 2 - o l - 4 - 2 . 0 - 3 . 0 - 5 . 0 - 6 . 0 - 0 . 0 - 3 . 0 - t V i . a a a r e a U r t e : 8 a )"
        },
        {
            "title": "E\nQ\nX\nC\nI\nR\nT\nE\nM",
            "content": "( S )"
        },
        {
            "title": "E\nQ\nX\nC\nI\nR\nT\nE\nM",
            "content": "( )"
        },
        {
            "title": "E\nQ",
            "content": "S O ( )"
        },
        {
            "title": "E\nQ",
            "content": "S"
        },
        {
            "title": "T\nE\nM\nO\nC\nx",
            "content": "( n - e - e - e - h - n - i - f - e - d - n - e - e - e - z - n - t - f - e - d - e - e - n - e - z - n - i - f - s - d - e - e - e - e - h - n - i - f - e - d - 4 . 7 7 3 . 6 8 - 0 . 5 2 5 . 3 9 5 . 2 9 . 2 9 8 . 3 9 2 . 2 9 3 . 3 9 - - - 4 . 1 9 4 . 7 8 5 . 9 8 0 . 5 8 9 . 8 3 . 0 9 2 . 8 8 - 6 . 4 8 0 . 8 8 2 . 3 6 . 6 5 6 . 2 9 0 . 2 9 4 . 2 9 6 . 1 9 2 . 0 0 . 2 9 1 . 2 9 1 . 1 9 0 . 2 9 9 . 0 9 4 . 8 7 . 0 9 9 . 3 8 5 . 3 8 0 . 1 9 2 . 3 9 - 2 . 1 8 0 . 5 8 6 . 5 8 3 . 7 5 0 . 0 9 2 . 9 7 . 9 8 0 . 9 8 8 . 7 8 0 . 8 8 7 . 9 8 6 . 8 3 . 9 8 9 . 5 8 8 . 6 8 1 . 5 8 0 . 0 8 6 . 2 9 . 7 8 9 . 0 9 - 2 . 5 8 9 . 6 8 4 . 2 4 . 2 6 4 . 1 9 9 . 0 9 6 . 1 9 7 . 0 9 8 . 0 0 . 0 9 7 . 1 9 3 . 1 9 7 . 1 9 1 . 1 9 6 . 9 8 . 0 9 2 . 7 8 1 . 5 8 4 . 9 8 9 . 1 9 - - 4 . 0 8 - 6 . 8 6 2 . 1 9 8 . 0 1 . 1 9 1 . 1 9 8 . 0 9 6 . 1 9 8 . 0 9 8 . 0 1 . 1 9 4 . 0 9 9 . 9 8 6 . 0 9 0 . 1 9 8 . 6 5 . 8 8 3 . 1 9 2 . 2 8 - - 5 . 4 7 . 5 8 9 . 4 7 6 . 9 8 0 . 9 8 4 . 9 8 9 . 9 3 . 0 9 7 . 9 8 7 . 9 8 7 . 8 8 4 . 9 8 2 . 7 9 . 6 8 8 . 6 8 0 . 2 8 7 . 9 7 4 . 4 8 3 . 1 2 . 9 7 - 3 . 9 8 0 . 8 8 4 . 1 4 9 . 1 4 . 2 9 3 . 2 9 0 . 2 9 0 . 2 9 0 . 3 9 6 . 2 5 . 2 9 7 . 2 9 9 . 0 9 8 . 0 9 4 . 1 9 2 . 4 8 . 8 7 5 . 1 8 3 . 4 9 1 . 4 8 - 3 . 7 9 . 6 8 7 . 9 4 6 . 0 9 4 . 1 9 6 . 0 9 9 . 0 0 . 1 9 9 . 1 9 9 . 0 9 3 . 1 9 7 . 0 9 1 . 9 0 . 9 8 1 . 8 8 6 . 4 8 8 . 0 8 8 . 5 8 2 . 3 7 . 1 8 - 8 . 6 8 3 . 7 8 7 . 8 4 1 . 1 7 . 0 9 6 . 0 9 4 . 1 9 9 . 0 9 0 . 1 9 4 . 1 7 . 0 9 4 . 0 9 2 . 0 9 4 . 9 8 6 . 8 8 2 . 5 7 . 2 8 2 . 3 9 1 . 2 9 1 . 0 8 - 3 . 7 3 . 7 8 6 . 5 5 2 . 1 9 3 . 1 9 0 . 1 9 9 . 1 6 . 1 9 7 . 1 9 6 . 1 9 1 . 1 9 0 . 1 9 7 . 9 6 . 8 8 3 . 9 8 0 . 5 8 1 . 1 8 0 . 9 8 7 . 3 8 . 1 8 - 8 . 0 9 1 . 0 9 8 . 2 6 6 . 4 4 . 4 9 5 . 4 9 0 . 5 9 1 . 5 9 7 . 4 9 0 . 5 9 . 4 9 2 . 4 9 2 . 4 9 9 . 2 9 7 . 2 9 1 . 8 5 . 6 8 1 . 5 9 3 . 6 9 2 . 5 8 8 . 7 7 2 . 7 - 8 . 5 2 2 . 4 9 2 . 3 9 8 . 3 9 7 . 3 1 . 2 9 2 . 3 9 - - - 0 . 1 9 . 7 8 3 . 9 8 7 . 3 8 6 . 0 9 0 . 1 9 7 . 8 - 9 . 3 8 9 . 7 8 9 . 8 7 8 . 8 5 0 . 1 2 . 0 9 8 . 2 9 8 . 1 9 9 . 0 9 5 . 2 9 1 . 1 3 . 0 9 4 . 0 9 3 . 0 9 6 . 8 8 7 . 1 9 5 . 9 6 . 1 8 2 . 1 9 5 . 3 9 - 0 . 3 8 1 . 5 8 . 4 8 5 . 9 5 6 . 0 9 2 . 9 8 4 . 0 9 4 . 9 1 . 8 8 6 . 8 8 7 . 0 9 3 . 9 8 9 . 9 8 0 . 8 4 . 6 8 2 . 7 8 2 . 0 8 1 . 4 8 7 . 9 8 5 . 1 - 3 . 5 8 4 . 6 8 9 . 4 8 2 . 1 6 6 . 1 3 . 1 9 5 . 1 9 5 . 0 9 2 . 0 9 7 . 9 8 0 . 2 7 . 1 9 0 . 2 9 6 . 0 9 6 . 9 8 0 . 0 9 2 . 7 9 . 4 8 3 . 9 8 7 . 2 9 - - 0 . 9 - 4 . 6 6 9 . 9 8 3 . 0 9 7 . 0 9 1 . 0 2 . 0 9 2 . 0 9 7 . 0 9 5 . 0 9 3 . 1 9 6 . 9 8 . 9 8 0 . 0 9 0 . 8 8 6 . 6 8 9 . 7 8 4 . 0 3 . 1 8 - - 5 . 3 8 1 . 5 8 5 . 2 1 . 9 8 7 . 8 8 3 . 8 8 5 . 8 8 9 . 7 8 2 . 8 1 . 0 9 7 . 8 8 8 . 9 8 3 . 7 8 9 . 6 8 0 . 7 6 . 0 8 4 . 7 7 6 . 2 8 2 . 0 9 5 . 7 7 - 8 . 8 8 5 . 8 8 4 . 3 4 0 . 2 9 5 . 1 9 4 . 1 2 . 2 9 8 . 1 9 2 . 2 9 3 . 2 9 1 . 2 9 5 . 2 4 . 0 9 7 . 0 9 7 . 9 8 8 . 2 8 0 . 7 7 1 . 1 4 . 3 9 6 . 4 8 - . 1 7 8 . 2 6 8 . 8 9 4 . 2 0 9 . 2 0 9 . 2 0 9 . 9 9 8 . 1 0 9 . 1 0 9 . 4 0 9 . 6 0 9 . 8 0 9 . 4 8 8 . 6 9 8 . 4 8 8 . 5 1 8 . 9 7 7 . 1 3 8 . 1 3 9 . 4 0 8 - . 7 5 8 . 4 5 . 9 6 4 . 6 0 9 . 5 0 . 7 9 8 . 2 0 9 . 9 9 . 5 9 8 . 8 0 9 . 5 0 . 3 0 9 . 7 8 8 . 7 8 . 0 9 8 . 8 2 8 . 0 1 . 5 0 9 . 3 1 9 . 1 0 - . 0 6 8 . 0 7 8 . 0 4 5 . 2 0 9 . 4 0 9 . 1 0 9 . 4 0 9 . 3 0 9 . 5 0 9 . 3 1 9 . 4 0 9 . 9 0 9 . 8 8 8 . 5 9 8 . 1 8 8 . 5 2 8 . 1 2 8 . 6 7 8 . 7 2 9 . 1 1 8 - . 5 0 9 . 2 0 9 . 9 1 . 6 4 9 . 4 4 9 . 1 4 . 6 4 9 . 8 3 9 . 0 4 . 9 4 9 . 9 4 9 . 7 4 . 5 3 9 . 6 2 9 . 7 2 . 4 8 8 . 9 4 8 . 1 4 . 5 5 9 . 7 5 8 . 2 6 . 2 7 8 - . 5 9 1 . 8 2 9 . 2 0 9 . 7 1 9 . 7 3 9 . 7 0 9 . 7 2 9 - - - . 7 9 8 . 1 6 . 1 9 8 . 6 5 7 . 0 4 . 9 3 8 . 6 5 8 - . 7 5 8 . 8 1 9 . 0 3 8 . 3 1 4 . 9 2 9 . 2 3 9 . 9 3 9 . 2 2 9 . 8 9 8 . 9 1 9 . 9 1 9 . 3 2 9 . 6 2 9 . 0 1 9 . 8 7 8 . 0 0 9 . 0 9 7 . 3 4 8 . 8 6 8 . 7 4 9 - . 3 1 . 2 7 8 . 0 7 8 . 5 3 . 1 0 9 . 2 0 9 . 7 0 . 2 9 8 . 7 8 8 . 0 9 . 8 9 8 . 0 9 8 . 1 1 . 8 6 8 . 4 6 8 . 3 6 . 7 9 6 . 4 7 7 . 9 9 . 7 2 9 - . 2 6 8 . 5 8 8 . 5 1 8 . 7 2 5 . 8 2 9 . 8 1 9 . 7 2 9 . 5 2 9 . 9 1 9 . 1 1 9 . 8 2 9 . 8 2 9 . 5 2 9 . 5 2 9 . 7 9 8 . 3 2 9 . 2 2 8 . 1 3 8 . 1 8 8 . 8 2 9 - - . 9 4 7 - . 0 9 5 . 6 8 8 . 2 8 . 7 9 8 . 0 9 8 . 6 9 . 2 0 9 . 6 9 8 . 5 7 . 9 9 8 . 1 8 8 . 0 7 . 2 8 8 . 8 4 4 . 0 1 . 9 7 7 . 6 0 9 . 2 0 - - . 1 1 9 . 8 9 . 4 3 2 . 8 2 9 . 0 4 . 5 2 9 . 0 4 9 . 0 5 . 0 4 9 . 7 3 9 . 7 3 . 9 2 9 . 4 0 9 . 8 0 . 9 1 9 . 5 3 8 . 4 6 . 8 9 7 . 3 4 9 . 7 3 - . 5 4 9 . 1 3 9 . 1 8 4 . 9 5 9 . 8 6 9 . 3 6 9 . 6 5 9 . 5 6 9 . 8 6 9 . 3 6 9 . 3 6 9 . 4 6 9 . 8 4 9 . 1 5 9 . 7 5 9 . 8 7 8 . 9 2 8 . 9 0 7 . 4 7 9 . 6 6 8 - . 6 0 9 . 1 0 9 . 6 6 . 9 2 9 . 0 4 9 . 4 3 . 4 3 9 . 4 3 9 . 8 3 . 8 3 9 . 4 4 9 . 9 3 . 4 1 9 . 3 0 9 . 0 1 . 4 9 7 . 2 0 8 . 1 4 . 4 5 9 . 4 2 8 - . 6 8 8 . 9 9 8 . 3 9 3 . 8 1 9 . 3 2 9 . 1 2 9 . 0 3 9 . 3 3 9 . 4 3 9 . 2 3 9 . 0 2 9 . 4 2 9 . 5 1 9 . 1 9 8 . 7 0 9 . 6 7 7 . 9 2 8 . 6 0 4 . 6 3 9 . 8 6 7 - . 8 0 . 9 0 9 . 8 6 5 . 7 3 . 5 3 9 . 1 3 9 . 0 4 . 4 3 9 . 7 3 9 . 5 3 . 3 3 9 . 6 3 9 . 6 1 . 5 0 9 . 7 1 9 . 7 3 . 0 8 7 . 8 0 8 . 7 5 . 7 4 8 - . 3 4 9 . 8 4 9 . 9 6 7 . 8 6 9 . 7 6 9 . 0 7 9 . 0 7 9 . 9 6 9 . 3 7 9 . 2 7 9 . 0 7 9 . 7 6 9 . 3 6 9 . 6 5 9 . 9 5 9 . 2 1 9 . 5 8 8 . 2 0 7 . 1 8 9 . 5 8 8 . 2 7 7 . 6 7 8 - . 4 1 2 . 0 3 9 . 2 2 . 8 3 9 . 8 2 9 . 0 1 . 8 2 9 - - - . 6 8 8 . 1 6 8 . 0 8 8 . 5 4 7 . 3 4 8 . 6 4 8 . 9 6 8 - . 9 3 8 . 0 2 . 7 0 8 . 0 0 5 . 6 0 . 2 2 9 . 2 3 9 . 2 2 . 0 3 9 . 5 3 9 . 7 0 . 6 2 9 . 5 1 9 . 4 0 . 3 8 8 . 0 2 9 . 3 1 . 7 1 8 . 4 7 8 . 2 2 - 4 . 4 8 7 . 6 8 0 . 4 8 7 . 0 5 1 . 0 6 . 9 8 2 . 2 9 5 . 9 8 4 . 7 8 0 . 0 9 2 . 0 6 . 9 8 5 . 1 9 3 . 7 8 8 . 5 8 3 . 7 8 4 . 2 7 . 0 8 9 . 0 9 1 . 2 9 - 3 . 6 8 0 . 9 5 . 4 8 9 . 1 5 3 . 3 9 4 . 2 9 6 . 2 9 7 . 1 6 . 1 9 4 . 1 9 0 . 4 9 1 . 3 9 7 . 3 9 6 . 1 9 . 9 8 4 . 1 9 0 . 4 8 6 . 3 8 8 . 8 8 4 . 3 - - 6 . 2 7 - 6 . 5 5 4 . 7 0 . 5 8 7 . 7 8 5 . 7 8 5 . 7 8 7 . 8 8 7 . 8 0 . 7 8 3 . 9 8 1 . 6 8 4 . 6 8 2 . 7 8 1 . 2 7 . 9 7 2 . 6 7 3 . 8 8 4 . 9 6 - - 5 . 9 8 8 . 8 8 5 . 9 4 0 . 2 9 1 . 3 9 9 . 1 0 . 1 9 6 . 2 9 1 . 1 9 3 . 1 9 0 . 3 9 6 . 1 0 . 9 8 8 . 0 9 6 . 9 8 3 . 2 8 2 . 3 8 8 . 9 0 . 4 9 0 . 2 8 - 5 . 4 9 7 . 2 9 4 . 0 3 . 5 9 8 . 4 9 7 . 4 9 7 . 5 9 2 . 5 9 6 . 5 2 . 5 9 2 . 5 9 0 . 6 9 8 . 3 9 3 . 5 9 8 . 2 3 . 6 8 4 . 1 8 3 . 4 6 8 . 6 9 2 . 7 8 - 4 . 0 9 9 . 8 8 4 . 9 4 9 . 1 9 4 . 2 9 9 . 1 8 . 1 9 1 . 1 9 7 . 0 9 6 . 2 9 6 . 2 9 7 . 2 3 . 9 8 4 . 1 9 2 . 0 9 3 . 6 7 5 . 9 7 7 . 3 9 . 4 9 7 . 1 8 - 4 . 7 8 5 . 6 8 8 . 9 2 . 1 9 1 . 0 9 3 . 0 9 2 . 1 9 5 . 0 9 7 . 9 1 . 2 9 7 . 1 9 8 . 0 9 5 . 8 8 6 . 9 8 4 . 9 0 . 2 7 9 . 0 8 5 . 1 4 9 . 2 9 7 . 7 7 - 8 . 8 8 4 . 9 8 7 . 7 5 9 . 1 9 2 . 1 9 5 . 1 8 . 1 9 4 . 1 9 7 . 1 9 1 . 2 9 4 . 2 9 6 . 2 0 . 9 8 2 . 1 9 5 . 9 8 3 . 2 8 9 . 8 7 2 . 7 1 . 4 9 8 . 0 8 - 9 . 3 9 5 . 4 9 1 . 7 8 . 5 9 9 . 5 9 3 . 6 9 0 . 6 9 5 . 5 9 0 . 6 4 . 6 9 4 . 6 9 9 . 6 9 2 . 5 9 5 . 5 9 4 . 5 6 . 1 9 7 . 6 8 6 . 9 6 5 . 7 9 0 . 0 9 3 e + + o + 3 e + + o + 3 e + + o + 3 e + + o + +"
        },
        {
            "title": "M\nS\nW\nO",
            "content": "l m u - 4 - o A - 2 Q 2 e r V p a + n y + l S + s r i s m y a C"
        },
        {
            "title": "M\nS\nW\nO",
            "content": ". a a a r e a U r e f u R : 9 a T"
        },
        {
            "title": "E\nQ",
            "content": "S"
        },
        {
            "title": "T\nE\nM\nO\nC\nx",
            "content": "S"
        },
        {
            "title": "Y\nP\nA\nU\nG\nN\nI",
            "content": "L - e - e - e - e - h - p - n - i - f - e - e - e - e - e - e - e - h - p - n - i - f - e - e - e - e - e - e - e - h - p - n - i - f - e - e - 9 . 7 5 8 . 4 7 - 4 . 0 2 0 . 2 9 . 0 7 4 . 2 7 3 . 6 7 3 . 4 7 6 . 5 7 - - - 3 . 4 7 0 . 2 7 4 . 3 7 7 . 1 3 . 9 6 4 . 1 5 1 . 5 6 - 7 . 1 8 9 . 4 7 . 8 7 0 . 7 5 5 . 5 8 8 . 4 8 5 . 5 8 2 . 5 6 . 4 8 4 . 5 8 9 . 1 8 0 . 1 8 8 . 1 8 9 . 3 1 . 3 8 5 . 3 8 5 . 1 6 4 . 4 7 3 . 3 8 0 . 5 - 0 . 3 7 4 . 1 8 5 . 8 7 0 . 9 4 8 . 0 5 . 9 7 2 . 0 8 4 . 2 8 2 . 1 8 9 . 1 8 6 . 1 4 . 0 8 2 . 1 8 4 . 9 7 5 . 7 7 7 . 8 7 5 . 2 3 . 9 6 6 . 6 7 4 . 0 8 - 2 . 9 7 3 . 4 4 . 3 8 2 . 0 6 0 . 4 8 2 . 3 8 9 . 3 8 0 . 5 3 . 4 8 0 . 5 8 1 . 5 8 2 . 4 8 8 . 4 8 9 . 2 9 . 1 8 7 . 2 8 8 . 1 6 8 . 5 7 5 . 3 7 1 . 5 - 8 . 4 7 9 . 2 8 3 . 1 8 7 . 2 4 5 . 4 5 . 3 8 4 . 4 8 0 . 5 8 0 . 4 8 7 . 4 8 9 . 4 9 . 3 8 0 . 5 8 3 . 2 8 0 . 1 8 1 . 2 8 7 . 8 1 . 9 6 3 . 6 7 3 . 3 8 - - 0 . 4 - 8 . 0 7 8 . 4 8 0 . 4 8 9 . 4 8 7 . 8 2 . 8 8 2 . 9 8 3 . 6 8 5 . 5 8 6 . 6 8 4 . 4 7 . 3 8 7 . 4 8 7 . 2 4 9 . 9 7 1 . 4 7 1 . 5 6 . 3 6 - 2 . 6 8 1 . 4 8 5 . 2 3 2 . 4 1 . 3 8 6 . 3 8 5 . 8 8 5 . 7 8 9 . 7 8 0 . 6 8 . 4 8 2 . 5 8 0 . 4 8 8 . 2 8 2 . 3 8 1 . 9 1 . 8 7 8 . 5 5 5 . 4 8 9 . 6 6 - 9 . 8 7 . 5 8 4 . 7 5 5 . 6 8 6 . 5 8 9 . 5 8 4 . 0 6 . 9 8 0 . 0 9 1 . 8 8 3 . 7 8 7 . 7 8 4 . 6 5 . 5 8 0 . 6 8 8 . 2 7 0 . 8 7 2 . 8 5 4 . 6 8 . 9 6 - 5 . 5 8 4 . 1 8 8 . 6 4 8 . 1 5 . 0 8 4 . 1 8 4 . 6 8 2 . 5 8 0 . 6 8 5 . 3 2 . 2 8 1 . 3 8 5 . 1 8 4 . 0 8 0 . 1 8 8 . 3 3 . 4 7 1 . 0 7 1 . 2 8 0 . 4 6 - 6 . 5 6 . 0 8 2 . 5 4 0 . 1 8 5 . 9 7 8 . 0 8 4 . 6 0 . 5 8 1 . 6 8 1 . 3 8 5 . 1 8 7 . 2 8 5 . 0 0 . 9 7 1 . 0 8 0 . 0 6 0 . 3 7 5 . 0 4 7 . 1 9 . 0 6 - 1 . 8 8 5 . 3 8 0 . 7 3 7 . 2 1 . 3 8 8 . 3 8 0 . 8 8 4 . 7 8 1 . 8 8 5 . 5 6 . 4 8 4 . 5 8 6 . 3 8 6 . 2 8 5 . 3 8 6 . 8 5 . 6 7 5 . 2 6 1 . 4 8 7 . 6 6 - 4 . 3 7 . 0 9 0 . 2 8 0 . 1 9 5 . 0 9 7 . 0 9 1 . 4 8 . 3 9 9 . 3 9 3 . 2 9 9 . 1 9 1 . 2 9 1 . 1 7 . 0 9 9 . 0 9 7 . 9 7 5 . 5 8 0 . 6 6 9 . 0 8 . 5 7 7 . 9 5 6 . 4 7 - 0 . 1 2 0 . 6 3 . 4 7 2 . 5 7 4 . 8 7 6 . 6 7 2 . 7 7 - - - 7 . 6 7 2 . 4 7 3 . 5 7 3 . 6 0 . 4 7 7 . 2 5 0 . 9 6 - 6 . 2 8 8 . 4 4 . 9 7 8 . 0 6 6 . 6 8 2 . 6 8 7 . 6 8 0 . 6 6 . 5 8 0 . 6 8 7 . 3 8 1 . 3 8 5 . 3 8 4 . 5 6 . 4 8 1 . 5 8 7 . 5 7 5 . 8 7 7 . 3 8 5 . 6 - . 2 3 7 . 7 0 8 . 2 8 7 . 6 3 5 . 8 1 8 . 4 0 8 . 8 0 8 . 5 2 8 . 4 1 8 . 8 1 8 . 2 2 8 . 0 1 8 . 4 1 8 . 6 0 8 . 8 8 7 . 5 9 7 . 7 7 6 . 9 2 7 . 9 4 7 . 9 0 8 - . 9 6 7 . 4 1 8 . 6 0 . 4 0 6 . 5 2 8 . 8 1 . 3 2 8 . 2 3 8 . 5 2 . 0 3 8 . 2 3 8 . 4 2 . 9 2 8 . 5 1 8 . 6 0 . 1 1 8 . 2 2 7 . 4 6 . 7 0 7 . 1 3 8 - . 8 1 7 . 4 9 7 . 0 7 7 . 1 4 4 . 4 2 8 . 3 1 8 . 1 2 8 . 5 2 8 . 2 1 8 . 0 2 8 . 7 2 8 . 4 1 8 . 4 2 8 . 7 0 8 . 1 9 7 . 1 0 8 . 3 9 6 . 5 1 7 . 9 2 7 . 3 1 8 - - . 8 7 8 - . 7 9 7 . 3 0 . 5 0 9 . 7 0 9 . 2 2 . 3 2 9 . 5 2 9 . 0 1 . 2 1 9 . 4 1 9 . 8 9 . 2 0 9 . 2 0 9 . 1 1 . 0 8 8 . 9 7 7 . 5 0 . 0 0 8 - . 9 1 8 . 7 9 7 . 1 1 3 . 1 1 8 . 4 0 8 . 3 0 8 . 0 5 8 . 2 4 8 . 2 4 8 . 7 2 8 . 9 1 8 . 6 1 8 . 5 0 8 . 7 9 7 . 5 9 7 . 6 7 6 . 2 4 7 . 0 1 5 . 3 1 8 . 9 3 6 - . 0 5 8 . 6 1 . 3 9 4 . 4 3 8 . 9 2 . 2 3 8 . 9 6 8 . 4 6 . 9 6 8 . 7 4 8 . 4 4 . 8 4 8 . 9 2 8 . 3 2 . 9 2 8 . 2 0 7 . 0 4 . 4 2 5 . 7 3 8 . 9 8 - . 0 5 8 . 3 1 8 . 8 7 4 . 7 2 8 . 5 2 8 . 6 2 8 . 1 6 8 . 8 5 8 . 0 6 8 . 0 4 8 . 8 3 8 . 8 3 8 . 1 2 8 . 0 2 8 . 7 1 8 . 3 9 6 . 4 5 7 . 3 8 6 . 5 3 8 . 8 6 6 - . 1 6 8 . 1 2 8 . 4 1 . 0 4 8 . 8 3 8 . 5 3 . 7 7 8 . 5 7 8 . 1 7 . 4 5 8 . 2 5 8 . 8 4 . 6 3 8 . 1 3 8 . 8 2 . 9 1 7 . 7 7 7 . 8 9 . 5 4 8 . 0 8 6 - . 6 5 8 . 5 1 8 . 7 5 3 . 1 2 8 . 8 2 8 . 8 2 8 . 4 6 8 . 3 6 8 . 3 6 8 . 4 4 8 . 0 4 8 . 0 4 8 . 4 2 8 . 0 2 8 . 0 2 8 . 8 0 7 . 9 5 7 . 8 9 5 . 9 3 8 . 4 7 6 - . 3 1 . 2 8 8 . 0 5 7 . 9 9 . 8 9 8 . 7 9 8 . 7 2 . 5 2 9 . 5 2 9 . 0 1 . 7 0 9 . 7 0 9 . 5 9 . 2 9 8 . 1 9 8 . 4 9 . 4 3 8 . 9 3 6 . 1 0 . 5 6 7 . 9 5 - . 7 1 1 - - . 2 2 2 - . 1 8 - . 3 9 - . 1 9 - . 7 8 - . 3 0 1 - . 5 0 1 - - - - . 8 8 - . 2 0 1 - . 6 0 1 - . 2 5 - . 6 2 1 - . 4 8 3 - . 0 2 1 - - . 6 2 - . 1 2 - . 8 2 - . 1 4 - . 2 2 - . 3 2 - . 3 2 - . 0 2 - . 1 2 - . 1 2 - . 4 2 - . 7 2 - . 7 2 - . 1 2 - . 4 2 - . 4 2 - . 5 2 - . 3 3 - . 7 4 - . 4 2 - - . 3 6 - . 8 3 - . 5 5 - . 7 7 - . 5 4 - . 6 5 - . 9 5 - . 4 4 - . 2 5 - . 4 5 - . 6 4 - . 5 5 - . 7 5 - . 4 4 - . 6 5 - . 7 5 - . 6 3 - . 9 6 - . 1 5 - . 2 4 - . 4 4 - . 2 6 - . 4 4 - . 9 4 - . 7 4 - . 2 4 - . 5 4 - . 5 4 - . 2 4 - . 7 4 - . 7 4 - . 4 4 - . 9 4 - . 8 4 - . 7 3 - . 1 5 - . 6 5 - . 2 4 - . 7 4 - . 3 8 - . 2 4 - . 7 4 - . 8 4 - . 1 4 - . 7 4 - . 8 4 - . 0 4 - . 6 4 - . 6 4 - . 1 4 - . 7 4 - . 8 4 - . 4 4 - . 8 5 - - . 2 0 - - . 4 0 - . 6 0 - . 2 0 - . 2 0 - . 3 0 - . 1 0 - . 2 0 - . 4 0 - . 2 0 - . 2 0 - . 8 0 - . 2 0 - . 2 0 - . 9 0 - . 6 6 3 - - . 1 5 - . 6 4 - - 5 . 4 - 7 . 4 - - 8 . 3 - 2 . 4 - . 0 1 4 - 3 . 2 1 - 0 . 5 1 - . 7 3 - . 2 4 - . 3 4 - . 5 3 - . 0 4 - . 1 4 - . 6 3 - . 1 4 - . 4 4 - . 5 3 - . 1 4 - . 3 4 - . 3 4 - . 0 5 - 9 . 3 - 1 . 4 - 4 . 4 - 6 . 3 - 8 . 3 - 9 . 3 - 6 . 3 - 8 . 3 - 1 . 4 - 7 . 3 - 9 . 3 - 1 . 4 - 3 . 4 - 3 . 5 - 7 . 3 - 0 . 4 - 0 . 4 - 6 . 3 - 9 . 3 - 8 . 3 - 6 . 3 - 9 . 3 - 0 . 4 - 6 . 3 - 8 . 3 - 0 . 4 - 7 . 3 - 5 . 4 - - 7 . 1 - 5 . 2 - 1 . 9 - 7 . 1 - 0 . 2 - 2 . 2 - 6 . 1 - 8 . 1 - 0 . 2 - 7 . 1 - 9 . 1 - 1 . 2 - 6 . 1 - 9 . 1 - 2 . 2 - 0 . 2 - 2 . 3 - - 3 . 3 - 4 . 4 - 6 . 7 3 - 6 . 3 - 1 . 4 - 9 . 3 - 8 . 3 - 0 . 4 - 9 . 3 - 7 . 3 - 2 . 4 - 0 . 4 - 6 . 3 - 1 . 4 - 9 . 3 - 1 . 4 - 3 . 5 - - 6 . 1 - 0 . 2 - 4 . 2 - 6 . 1 - 5 . 1 - 8 . 1 - 4 . 1 - 3 . 1 - 6 . 1 - 4 . 1 - 5 . 1 - 7 . 1 - 5 . 1 - 3 . 1 - 6 . 1 - 0 . 2 - 4 . 3 - 3 e + + o + 3 e + + o + 3 e + + o + 3 e + + o + +"
        },
        {
            "title": "M\nS\nW\nO",
            "content": "a + n y + l S + s o A - 2 Q 2 e p W l S n C"
        },
        {
            "title": "M\nS\nW\nO",
            "content": ". 7 1 1 - . 2 7 1 - . 8 2 1 - . 2 3 1 - . 7 5 3 - 8 . 1 3 - 7 . 7 1 - 9 . 1 5 - 9 . 9 2 - 7 . 9 2 - o l - 4 - - - - . 2 6 - . 7 4 - . 3 5 - . 5 0 - . 1 0 - . 4 4 - . 4 4 - 4 . 4 - 5 . 4 - 0 . 4 - 0 . 4 - 3 . 2 - 5 . 2 - 0 . 4 - 0 . 4 - 3 . 2 - 2 . 2 - t e S . a a a o t t d"
        },
        {
            "title": "2\nT\nS\no\nV\no\nC",
            "content": "r t e : 0 1 a T"
        },
        {
            "title": "E\nQ",
            "content": "S"
        },
        {
            "title": "T\nE\nM\nO\nC\nx",
            "content": "S"
        },
        {
            "title": "Y\nP\nA\nU\nG\nN\nI",
            "content": "L - e - e - e - e - h - n - d - i - p - f - s - e - e - e - e - e - h - n - d - i - p - f - s - e - e - e - e - e - h - n - d - i - p - f - s - 8 . 9 7 7 . 5 8 3 . 8 8 8 . 9 3 3 . 8 2 . 7 8 3 . 8 8 2 . 9 8 2 . 8 8 3 . 9 8 6 . 9 8 . 8 8 6 . 9 8 5 . 7 8 0 . 6 8 2 . 7 8 9 . 8 2 . 7 7 8 . 5 7 0 . 8 8 - 5 . 5 7 3 . 0 0 . 5 8 0 . 0 4 5 . 3 8 4 . 2 8 5 . 3 8 7 . 5 5 . 4 8 3 . 5 8 9 . 6 8 9 . 5 8 3 . 6 8 1 . 2 5 . 0 8 9 . 1 8 3 . 9 5 5 . 5 7 3 . 3 7 6 . 3 - 4 . 1 8 2 . 4 8 6 . 6 8 4 . 3 4 8 . 7 9 . 6 8 7 . 7 8 7 . 7 8 7 . 6 8 4 . 7 8 2 . 8 7 . 7 8 3 . 8 8 5 . 1 8 1 . 0 8 7 . 0 8 4 . 4 1 . 0 8 9 . 9 7 6 . 7 8 - 8 . 7 7 4 . 2 1 . 5 8 7 . 5 4 6 . 4 8 5 . 3 8 3 . 4 8 0 . 6 6 . 4 8 6 . 5 8 7 . 6 8 3 . 5 8 2 . 6 8 0 . 4 5 . 2 8 6 . 3 8 2 . 5 6 3 . 7 7 1 . 1 8 9 . 4 - 2 . 1 8 2 . 4 8 2 . 7 8 1 . 4 4 3 . 7 3 . 6 8 9 . 6 8 3 . 8 8 4 . 7 8 5 . 7 8 0 . 0 2 . 8 8 5 . 8 8 3 . 5 8 9 . 3 8 7 . 4 8 1 . 8 4 . 9 7 6 . 5 7 7 . 7 8 - - - 4 . 4 6 1 . 6 4 8 . 5 8 6 . 4 8 3 . 6 8 5 . 5 8 . 4 8 3 . 6 8 5 . 6 8 7 . 5 8 4 . 7 8 4 . 4 5 . 3 8 1 . 5 8 4 . 8 3 5 . 9 7 0 . 4 7 4 . 5 5 . 1 7 - 8 . 6 7 5 . 2 9 9 . 6 4 6 . 3 7 . 2 9 9 . 3 9 9 . 2 9 4 . 2 9 4 . 3 9 3 . 4 8 . 3 9 7 . 4 9 2 . 2 9 6 . 1 9 9 . 2 9 1 . 3 0 . 3 8 6 . 1 6 3 . 3 9 7 . 5 8 - 7 . 1 7 . 4 9 9 . 4 7 8 . 5 9 3 . 5 9 9 . 5 9 4 . 5 8 . 4 9 3 . 5 9 3 . 6 9 7 . 5 9 4 . 6 9 2 . 5 5 . 4 9 9 . 4 9 4 . 9 6 6 . 0 9 1 . 4 7 6 . 5 4 . 9 8 - 4 . 6 7 4 . 0 9 2 . 1 5 2 . 1 0 . 1 9 4 . 1 9 1 . 1 9 7 . 0 9 0 . 1 9 8 . 2 2 . 2 9 5 . 2 9 0 . 0 9 5 . 9 8 1 . 0 9 6 . 7 5 . 2 8 6 . 6 7 4 . 1 9 8 . 0 8 - 7 . 2 5 . 2 9 0 . 0 6 5 . 2 9 6 . 1 9 2 . 2 9 2 . 2 5 . 1 9 0 . 2 9 3 . 3 9 8 . 2 9 3 . 3 9 6 . 1 0 . 1 9 3 . 1 9 4 . 2 6 2 . 6 8 8 . 1 7 4 . 2 3 . 4 8 - 8 . 3 7 7 . 7 8 5 . 2 4 7 . 8 6 . 7 8 6 . 8 8 8 . 8 8 7 . 7 8 3 . 8 8 9 . 9 1 . 9 8 1 . 0 9 2 . 7 8 8 . 5 8 1 . 7 8 6 . 2 1 . 3 8 4 . 3 4 2 . 9 8 5 . 2 7 - 7 . 3 7 . 1 9 9 . 3 6 8 . 1 9 8 . 1 9 6 . 1 9 3 . 1 1 . 1 9 4 . 1 9 9 . 2 9 4 . 2 9 9 . 2 9 7 . 0 2 . 0 9 4 . 0 9 1 . 4 6 0 . 4 8 3 . 6 7 4 . 2 3 . 4 8 1 . 7 7 3 . 2 8 7 . 3 8 8 . 7 4 7 . 5 7 . 4 8 6 . 5 8 0 . 6 8 0 . 5 8 0 . 6 8 1 . 6 2 . 5 8 2 . 6 8 9 . 4 8 3 . 3 8 5 . 4 8 0 . 8 1 . 8 7 8 . 2 7 0 . 5 8 - 0 . 7 7 9 . 9 7 . 2 8 0 . 6 5 4 . 3 8 8 . 2 8 2 . 3 8 0 . 4 6 . 3 8 1 . 4 8 5 . 4 8 7 . 3 8 4 . 4 8 1 . 2 2 . 1 8 9 . 1 8 3 . 4 7 6 . 7 7 6 . 3 7 2 . 3 - . 2 0 8 . 8 1 8 . 2 4 8 . 0 6 5 . 7 5 8 . 2 5 8 . 5 5 8 . 3 5 8 . 6 4 8 . 1 5 8 . 8 5 8 . 3 5 8 . 7 5 8 . 9 1 8 . 9 0 8 . 4 1 8 . 9 5 7 . 3 0 8 . 1 9 7 . 2 5 8 - . 1 9 7 . 6 2 8 . 1 4 . 1 9 5 . 3 5 8 . 5 4 . 1 5 8 . 6 5 8 . 7 4 . 5 5 8 . 8 5 8 . 9 4 . 8 5 8 . 5 4 8 . 4 3 . 2 4 8 . 4 8 7 . 1 0 . 1 2 8 . 8 4 8 - . 1 0 8 . 5 2 8 . 5 4 8 . 1 6 5 . 2 5 8 . 8 4 8 . 0 5 8 . 9 5 8 . 4 5 8 . 6 5 8 . 0 7 8 . 6 5 8 . 9 5 8 . 0 4 8 . 1 3 8 . 7 3 8 . 5 7 7 . 2 0 8 . 4 4 7 . 5 5 8 - - . 0 1 7 - . 3 2 6 . 7 9 . 6 9 8 . 9 9 8 . 8 8 . 2 9 8 . 1 9 8 . 7 9 . 9 9 8 . 3 0 9 . 7 8 . 7 8 8 . 9 8 8 . 4 0 . 9 6 8 . 1 8 7 . 3 9 . 5 3 8 - . 8 5 6 . 1 9 8 . 2 2 4 . 7 0 9 . 9 9 8 . 4 1 9 . 1 0 9 . 4 9 8 . 9 0 9 . 1 1 9 . 5 0 9 . 9 1 9 . 3 9 8 . 7 8 8 . 0 0 9 . 5 4 6 . 8 0 8 . 3 7 5 . 0 1 9 . 4 5 8 - . 4 9 6 . 4 2 . 1 3 6 . 2 4 9 . 6 3 . 3 4 9 . 5 3 9 . 9 2 . 6 3 9 . 7 4 9 . 9 3 . 9 4 9 . 1 3 9 . 3 2 . 1 3 9 . 2 3 7 . 8 7 . 7 1 7 . 3 4 9 . 2 8 - . 9 8 6 . 5 8 8 . 2 1 5 . 9 9 8 . 0 0 9 . 0 0 9 . 5 9 8 . 6 9 8 . 4 9 8 . 4 1 9 . 8 0 9 . 7 0 9 . 7 8 8 . 4 8 8 . 6 8 8 . 4 4 6 . 5 2 8 . 2 4 7 . 0 0 9 . 5 3 8 - . 7 2 7 . 4 7 8 . 9 3 . 3 8 8 . 2 7 8 . 5 7 . 7 7 8 . 7 6 8 . 2 7 . 2 9 8 . 4 8 8 . 5 8 . 9 6 8 . 0 6 8 . 1 6 . 3 2 6 . 4 1 8 . 1 6 . 7 7 8 . 3 0 8 - . 5 1 7 . 8 7 8 . 3 1 5 . 6 9 8 . 1 9 8 . 8 8 8 . 9 8 8 . 6 8 8 . 3 8 8 . 3 0 9 . 9 9 8 . 8 9 8 . 2 8 8 . 5 7 8 . 5 7 8 . 3 6 6 . 6 4 8 . 2 3 4 . 2 9 8 . 1 1 8 - . 9 6 . 8 8 8 . 7 9 5 . 6 9 . 8 9 8 . 3 9 8 . 1 9 . 2 9 8 . 9 8 8 . 5 0 . 6 0 9 . 4 0 9 . 3 8 . 3 8 8 . 7 7 8 . 8 6 . 5 3 8 . 1 3 7 . 1 0 . 5 3 8 . 6 0 - . 3 0 - . 5 0 - . 1 1 - . 3 0 - . 4 0 - . 4 0 - . 5 0 - . 6 0 - . 5 0 - . 4 0 - . 5 0 - . 4 0 - . 3 0 - . 5 0 - . 4 0 - . 5 0 - . 8 1 - - . 3 0 - . 6 1 1 - . 4 0 - . 2 0 - . 4 0 - . 1 1 - . 4 0 - . 4 0 - . 5 0 - . 2 0 - . 2 0 - . 2 0 - . 5 0 - . 4 0 - . 4 0 - . 4 0 - . 4 0 - . 4 0 - . 4 0 - . 7 1 - . 9 9 - . 3 0 - - . 3 0 - . 1 0 - . 2 0 - . 2 2 - . 2 0 - . 1 0 - . 3 0 - . 3 0 - . 3 0 - . 2 0 - . 3 0 - . 3 0 - . 1 0 - . 3 0 - . 2 0 - . 4 0 - . 3 0 - . 7 0 - . 4 5 - . 2 0 - - . 4 0 - . 1 0 - . 3 0 - . 9 0 - . 2 0 - . 1 0 - . 2 0 - . 2 0 - . 4 0 - . 2 0 - . 3 0 - . 3 0 - . 3 0 - . 3 0 - . 3 0 - . 3 0 - . 2 0 - . 8 0 - . 0 1 - . 3 0 - - . 6 0 - . 6 0 - . 7 0 - . 8 2 - . 7 0 - . 7 0 - . 7 0 - . 4 0 - . 4 0 - . 5 0 - . 4 0 - . 7 0 - . 7 0 - . 6 0 - . 7 0 - . 7 0 - . 6 0 - . 9 1 - - . 5 0 - . 8 0 1 - - - . 8 1 - . 2 0 - . 0 0 - . 1 0 - . 0 0 - . 1 0 - . 2 0 - . 2 0 - . 2 0 - . 0 0 - . 0 0 - . 1 0 - . 2 0 - . 2 0 - . 2 0 - . 9 9 - . 0 0 - . 0 0 - . 2 0 3 - - . 7 3 - . 5 0 - . 1 1 - . 1 0 - . 3 0 - . 2 0 - . 2 0 - . 2 0 - . 2 0 - . 2 0 - . 2 0 - . 3 0 - . 3 0 - . 3 0 - . 3 0 - . 2 2 - . 2 1 - - 3 . 3 - 2 . 0 - 2 . 0 - 1 . 0 - 1 . 0 - 1 . 0 - 2 . 0 - 2 . 0 - 3 . 0 - 2 . 0 - 2 . 0 - 2 . 0 - 2 . 0 - 2 . 0 - 2 . 0 - 9 . 0 - 2 . 1 - - 1 . 3 - 1 . 0 - 0 . 1 - 3 . 0 - 1 . 0 - 1 . 0 - 1 . 0 - 0 . 0 - 2 . 0 - 0 . 0 - 2 . 0 - 2 . 0 - 4 . 0 - 2 . 0 - 4 . 0 - 6 . 1 - 2 . 1 - - 9 . 1 - 1 . 0 - 6 . 1 - 2 . 0 - 6 . 0 - 4 . 0 - 2 . 0 - 6 . 0 - 5 . 0 - 2 . 0 - 3 . 0 - 4 . 0 - 2 . 0 - 4 . 0 - 4 . 0 - 7 . 1 - 4 . 1 - - 6 . 1 - 2 . 0 - 1 . 0 - 2 . 0 - 2 . 0 - 2 . 0 - 1 . 0 - 1 . 0 - 1 . 0 - 0 . 0 - 1 . 0 - 1 . 0 - 0 . 0 - 2 . 0 - 1 . 0 - 9 . 0 - 9 . 0 - - 7 . 1 - 6 . 0 - 2 . 1 - 6 . 0 - 3 . 0 - 6 . 0 - 7 . 0 - 6 . 0 - 6 . 0 - 6 . 0 - 5 . 0 - 6 . 0 - 6 . 0 - 5 . 0 - 6 . 0 - 6 . 1 - 4 . 1 - 3 e + + o + 3 e + + o + 3 e + + o + 3 e + + o + +"
        },
        {
            "title": "M\nS\nW\nO",
            "content": "a + n y + l S + s o A - 2 Q 2 e p W l S n C"
        },
        {
            "title": "M\nS\nW\nO",
            "content": ". 4 6 2 - 7 . 1 2 - 4 . 2 1 - 8 . 9 1 - 8 . 8 4 - 1 . 6 1 - o l - 4 - . 2 0 - . 8 0 - 2 . 0 - 4 . 0 - 3 . 0 - 1 . 0 - 3 . 0 - 2 . 0 - 2 . 0 - 8 . 0 - 4 . 0 - 5 . 0 - t V i . a a a s c s d - p E f l R : 1 1 a T"
        },
        {
            "title": "METRICXQE",
            "content": "S xCOMETQE en-de en-es en-fr en-it en-nl en-pt en-zh en-de en-es en-fr en-it en-nl en-pt en-zh en-de en-es en-fr en-it en-nl en-pt en-zh Whisper Seamless Canary OWSM Whisper + Aya + Gemma3 + Tower+ Seamless + Aya + Gemma3 + Tower+ Canary + Aya + Gemma3 + Tower+ OWSM + Aya + Gemma3 + Tower+ DeSTA2 Qwen2-Audio Phi-4-Multimodal Voxtral Spire - - - - - -2.9 -6.9 -0.0 -0.0 -0.0 -3.6 -0.0 -0.0 -0.0 -0.0 -0.0 -0.0 -0.6 -2.3 -0.6 -2.9 -0.0 -1.7 -0.0 -0.0 -0.0 -0.0 -0.0 -0.0 -0.0 -0.0 -0.0 -0.0 -0.0 -0.0 -1.2 -0.0 -4.0 -11.0 - - -1.7 -4.6 -13.3 -10.4 -12.7 - -0.0 -0.0 -0.0 -4.5 -4.5 -0.0 -0.0 -0.0 -0.6 -0.0 -0.0 -0.6 -0.0 -0.0 -0.6 -0.6 -1.2 -0.0 -1.7 -0.6 -0.0 -0.6 -0.6 -0.6 -0.0 -0.0 -0.6 -0.0 -0.0 -0.0 -0.0 -0.0 -0.0 -0.0 -0.0 -0.6 -0.6 -0.0 -0.0 -0.0 -0.0 -0.6 -46.8 -0.0 -4.6 -1.2 -4.6 -8.1 -22.0 -12.1 -13.9 -33.5 -0.0 -0.0 -0.0 -0.0 -0.6 -1.2 -2.3 -0. -0.6 -0.0 -8.1 -4.5 -0.0 -0.0 -0.0 -0.0 -0.0 -0.0 -4.0 -2.9 -2.3 -4.6 -4.0 -2.3 -0.0 -0.0 -0.6 -0.0 -0.0 -0.0 -0.0 -0.0 -0.0 -0.6 -0.0 -0.0 -1.2 -1.2 -6.4 -4.6 -9.2 -24.3 -0.0 -0.6 -0.0 -1.2 -0.6 -0.6 - - - - - - - 29.9 24.5 23.8 24.1 24.4 24.0 33.9 - 72.7 67.7 65.7 67.3 69.9 66.7 45.7 25.3 19.3 20.3 17.6 21.2 54.3 84.7 79.1 78.4 79.4 81.5 78.5 81.2 82.0 76.7 77.1 78.3 79.4 76.6 79.8 83.2 76.8 75.7 77.3 80.4 74.7 79.8 49.1 39.0 41.1 40.0 42.3 38.0 50.5 48.9 39.1 40.6 41.1 42.0 37.6 51.2 47.2 37.8 38.4 39.5 41.4 36.2 50.4 84.6 78.6 78.5 79.6 81.6 77.9 80.9 83.7 78.4 78.6 79.3 79.6 77.4 81.5 83.1 76.3 75.6 77.6 80.2 74.4 80.3 66.8 55.2 55.0 55.2 57.4 55.8 66.4 59.5 56.2 59.4 55.6 56.0 54.5 64.2 55.5 53.5 52.7 54.0 57.0 53.3 55.8 64.0 60.4 58.2 58.9 59.3 56.8 32.1 52.5 41.1 48.0 45.7 42.3 43.8 62.3 62.0 53.8 37.6 55.1 34.3 50.6 54.0 83.8 78.7 77.8 79.4 81.1 77.3 80.7 56.5 48.4 48.4 49.2 56.0 43.4 61.8 - - - - - - - 32.1 27.8 24.0 26.8 26.6 25.0 23.6 - 66.1 56.0 47.0 54.0 58.8 57.4 51.6 23.2 15.5 17.4 16.5 20.7 32.0 76.0 66.0 61.3 65.7 70.2 67.1 56.9 75.4 65.0 57.9 64.3 69.1 66.3 56.0 74.5 63.7 57.6 63.3 69.2 64.9 54.5 42.6 36.7 33.0 35.6 38.3 36.8 32.9 41.9 35.9 32.9 35.0 38.1 36.0 32.5 42.0 36.3 33.3 35.3 37.7 36.1 33.0 75.9 66.5 60.5 64.9 70.4 68.1 56.3 74.6 64.9 59.4 64.7 68.3 66.3 56.3 74.2 63.1 57.3 63.7 69.7 64.7 54.3 59.9 53.4 50.8 53.1 56.1 53.8 48.5 59.0 52.5 49.8 52.0 54.3 51.8 47.5 59.0 51.8 48.2 51.5 55.0 52.8 47.8 59.8 51.7 42.6 47.2 51.7 50.0 21.3 50.1 33.2 33.9 35.5 35.2 39.3 38.6 55.7 44.6 27.3 44.0 28.7 44.0 34.3 74.3 65.6 58.9 65.6 69.8 66.7 55.2 46.5 38.8 33.3 37.0 43.7 39.5 32.1 Table 12: Results for WMT dataset across all languages. METRICXQE en-de en-es en-fr en-it en-pt en-de en-es en-fr en-it en-pt en-de en-es en-fr en-it en-pt en-de en-es en-fr en-it en-pt en-de en-es en-fr en-it en-pt xCOMETQE"
        },
        {
            "title": "LINGUAPY",
            "content": "F1 F1 Whisper Seamless Canary OWSM Whisper + Aya + Gemma3 + Tower+ Seamless + Aya + Gemma3 + Tower+ Canary + Aya + Gemma3 + Tower+ OWSM + Aya + Gemma3 + Tower+ DeSTA2 Qwen2-Audio Phi-4-Multimodal Voxtral Spire - - - - - - - - - - - - - - - - - - - - - -0.0 -0.0 -0.3 -0.0 -0.0 -0.0 -0.0 -0.0 -0.0 -0.0 -0.0 -0.0 -0.0 -0.0 -0.0 -0.0 -0. - -0.3 -0.0 -0.9 -0.0 -0.1 -0.0 -0.1 -0.1 -0.0 -0.0 -0.1 -0.0 -0.1 -0.1 -0.0 -0.1 -0.5 - -0.1 -0.0 -0.0 -0.1 -0.0 -0.0 -0.0 -0.0 -0.0 -0.0 -0.0 -0.0 -0.1 -0.0 -0.0 -0.0 -0.3 - -0.3 -0.3 -2.1 -0.5 -0.2 -0.2 -0.4 -0.2 -0.1 -0.4 -0.3 -0.2 -0.4 -0.3 -0.1 -0.4 -0.1 -50.7 -78.3 -99.4 -28.1 -21.1 46.0 19.1 -0.0 -0.3 - -0.8 79.0 74.5 72.8 71.3 69.4 87.0 80.8 75.4 78.6 82.1 66.2 66.8 64.3 65.1 67.6 46.9 50.2 46.2 43.4 41.1 -0.1 92.2 88.3 85.7 86.2 87.7 95.8 93.6 87.7 91.0 94.5 74.7 75.3 66.5 67.7 76.3 71.9 71.3 59.6 54.7 72.8 -1.1 81.0 62.4 53.1 50.4 57.1 89.5 70.6 47.4 53.2 66.0 66.9 64.7 59.1 60.7 65.1 46.3 30.9 23.9 20.8 32.9 -0.0 95.4 91.1 89.8 90.1 90.0 97.6 94.7 91.5 93.1 95.6 70.8 71.9 65.1 65.5 72.0 62.9 60.3 53.3 46.4 61.5 -0.0 95.1 91.0 89.9 90.4 89.9 97.4 95.1 90.9 92.2 95.6 67.1 68.4 64.3 62.7 67.9 56.0 52.1 45.7 39.0 52.0 -0.1 96.1 91.2 90.2 90.2 90.7 98.1 94.4 91.8 93.2 96.3 86.9 89.9 80.4 78.4 88.0 90.7 93.6 86.6 77.7 91.9 -0.1 93.2 88.0 87.0 87.1 86.5 95.9 91.4 87.2 90.0 92.4 69.6 71.3 64.5 64.8 71.3 62.3 59.3 50.7 44.9 60.3 -0.1 92.9 88.2 86.9 87.4 86.7 95.5 92.4 86.7 88.8 92.6 65.4 68.3 63.7 62.8 67.6 55.3 52.5 44.9 37.8 50.9 -0.1 93.8 88.0 87.0 87.0 87.1 96.2 91.3 87.6 89.9 93.2 84.8 87.7 78.6 77.6 85.9 88.6 91.2 83.3 75.6 89.1 -0.1 95.5 91.3 89.9 90.1 90.1 97.6 94.8 91.5 93.2 95.6 71.3 72.1 65.4 65.6 72.2 63.4 60.6 53.6 46.5 61.8 -0.1 95.0 91.2 90.0 90.4 89.9 97.4 95.2 91.0 92.5 95.6 66.9 68.5 64.0 62.8 68.2 56.0 52.2 47.0 38.4 52.6 -0.1 96.2 91.3 90.2 90.3 90.8 98.2 94.5 91.9 93.4 96.4 86.9 90.0 80.3 78.5 88.1 90.8 93.7 86.4 78.1 92.0 -0.1 94.9 90.0 88.7 89.0 89.0 97.2 93.3 89.5 91.9 94.5 70.6 71.4 64.9 64.6 71.4 61.7 59.0 52.2 45.4 60.4 -0.1 94.5 90.0 89.0 89.4 88.7 97.0 94.2 89.5 91.4 94.8 67.3 68.4 64.8 62.6 67.8 56.5 52.3 47.3 40.2 51.8 -0.1 95.3 90.2 89.1 89.2 89.5 97.6 93.6 90.4 91.9 95.3 86.0 89.0 79.6 77.6 87.8 89.8 93.1 85.4 77.8 91.6 -0.1 91.5 87.7 85.0 85.2 85.6 95.0 91.7 85.6 87.8 92.6 67.9 68.7 66.2 65.1 69.9 65.8 54.8 58.2 49.2 62.7 -0.8 87.8 83.2 82.3 80.8 81.4 92.9 90.5 85.1 87.1 90.5 67.4 66.5 64.7 64.3 67.4 41.0 31.0 37.9 24.7 37.0 0.0 19.8 37.0 0.5 64.8 72.7 55.0 33.5 -0.1 96.2 92.2 89.9 91.1 90.8 97.8 95.9 91.6 93.9 95.9 73.0 76.7 67.2 66.5 75.8 69.3 73.6 60.1 54.9 71.2 -0.1 89.7 83.9 83.2 82.5 83.0 93.4 88.3 83.0 86.5 90.9 73.3 70.3 66.6 66.8 72.2 66.6 59.3 61.4 49.0 62.8 0.5 61.2 66.0 47.4 20.1 1.6 56.6 64.5 22.7 12. -0.0 -0.0 -0.0 -0.0 -0.0 -0.0 Table 13: Results for WinoST dataset across all languages."
        },
        {
            "title": "E\nQ",
            "content": "S"
        },
        {
            "title": "T\nE\nM\nO\nC\nx",
            "content": "S"
        },
        {
            "title": "Y\nP\nA\nU\nG\nN\nI",
            "content": "L - e - e - e - z - t - n - i - f - e - d - n - e - e - e - z - p - l - i - f - e - d - e - n - e - e - z - p - n - t - f - e - d - 6 . 1 1 5 . 2 - 8 . 3 1 5 . 8 2 2 . 9 1 9 . 2 2 2 . 2 5 . 8 2 9 . 4 2 - - - 6 . 3 4 . 1 2 7 . 0 2 4 . 1 2 2 . 9 3 2 . 8 3 7 . 5 - 8 . 6 7 3 . 5 8 1 . 2 8 2 . 2 5 3 . 5 6 . 4 8 5 . 4 8 3 . 6 8 8 . 5 8 2 . 6 8 5 . 5 1 . 5 8 4 . 5 8 1 . 4 8 1 . 3 8 8 . 2 8 9 . 2 2 . 3 7 6 . 1 8 0 . 5 8 - 0 . 1 8 9 . 5 9 . 4 8 1 . 0 6 2 . 5 8 8 . 3 8 3 . 5 8 0 . 6 6 . 5 8 7 . 5 8 9 . 6 8 4 . 5 8 3 . 6 8 3 . 4 5 . 3 8 4 . 4 8 3 . 4 6 7 . 8 7 8 . 8 7 0 . 7 - 8 . 6 7 5 . 4 8 4 . 5 8 7 . 5 4 1 . 5 4 . 3 8 0 . 5 8 6 . 6 8 1 . 5 8 9 . 5 8 8 . 6 9 . 4 8 0 . 7 8 0 . 3 8 5 . 0 8 9 . 0 8 2 . 1 9 . 9 6 0 . 1 8 7 . 4 8 - - 7 . 5 - 4 . 0 7 9 . 5 8 3 . 5 8 1 . 6 8 2 . 0 0 . 0 9 9 . 0 9 1 . 8 8 9 . 7 8 9 . 8 8 0 . 5 0 . 5 8 6 . 5 8 2 . 5 4 8 . 1 8 6 . 1 8 1 . 7 9 . 9 6 - 8 . 8 8 1 . 5 8 3 . 0 3 4 . 5 2 . 4 8 3 . 4 8 4 . 0 9 0 . 0 9 1 . 0 9 4 . 7 9 . 6 8 9 . 6 8 2 . 4 8 0 . 4 8 5 . 3 8 9 . 9 7 . 9 7 2 . 1 7 7 . 5 8 5 . 3 7 - 7 . 0 3 . 7 8 7 . 7 5 1 . 8 8 6 . 6 8 1 . 7 8 7 . 2 1 . 2 9 8 . 1 9 2 . 0 9 1 . 9 8 5 . 9 8 6 . 7 3 . 6 8 7 . 6 8 0 . 4 7 8 . 0 8 6 . 1 7 0 . 9 5 . 6 7 - 5 . 8 8 6 . 4 8 1 . 9 4 5 . 4 4 . 3 8 1 . 4 8 1 . 0 9 1 . 9 8 7 . 9 8 7 . 6 6 . 5 8 4 . 6 8 0 . 3 8 4 . 2 8 7 . 2 8 1 . 4 7 . 8 7 7 . 9 7 8 . 5 8 8 . 0 7 - 9 . 0 5 . 5 8 7 . 7 4 6 . 4 8 4 . 3 8 4 . 4 8 5 . 1 9 . 0 9 6 . 1 9 3 . 7 8 5 . 6 8 4 . 7 8 9 . 2 3 . 2 8 3 . 3 8 6 . 0 6 8 . 7 7 6 . 1 6 1 . 7 2 . 9 6 - 8 . 0 9 8 . 5 8 4 . 7 3 5 . 5 7 . 4 8 0 . 5 8 1 . 1 9 5 . 0 9 5 . 0 9 6 . 7 9 . 6 8 3 . 7 8 6 . 4 8 1 . 4 8 2 . 4 8 3 . 9 8 . 9 7 1 . 7 7 6 . 7 8 8 . 3 7 - 0 . 5 9 . 1 9 9 . 1 8 3 . 2 9 2 . 1 9 1 . 1 9 9 . 5 0 . 5 9 2 . 5 9 5 . 4 9 4 . 3 9 4 . 3 9 1 . 2 5 . 1 9 3 . 1 9 3 . 1 8 8 . 7 8 7 . 2 8 4 . 2 4 . 2 8 2 . 4 3 2 . 8 3 - 4 . 9 9 . 8 6 . 1 4 0 . 7 3 9 . 8 4 7 . 9 3 4 . 5 3 - - - 1 . 4 5 9 . 7 1 5 . 2 3 7 . 9 1 . 2 6 4 . 6 6 8 . 4 6 - 2 . 7 7 8 . 3 9 . 0 8 0 . 7 5 5 . 5 8 0 . 4 8 2 . 4 8 6 . 5 8 . 4 8 4 . 5 8 1 . 5 8 1 . 4 8 4 . 4 8 9 . 3 0 . 3 8 2 . 2 8 9 . 3 7 3 . 8 7 8 . 9 7 4 . 4 - . 2 8 7 . 5 2 8 . 0 2 8 . 5 0 6 . 7 3 8 . 2 3 8 . 6 3 8 . 3 4 8 . 6 3 8 . 3 3 8 . 9 4 8 . 7 3 8 . 1 4 8 . 0 3 8 . 2 2 8 . 3 2 8 . 7 3 7 . 7 7 7 . 7 5 7 . 7 4 8 - . 9 2 7 . 2 0 8 . 4 9 . 5 6 4 . 0 2 8 . 6 0 . 3 2 8 . 0 3 8 . 7 1 . 5 2 8 . 3 3 8 . 8 1 . 0 3 8 . 2 0 8 . 3 8 . 5 8 7 . 7 8 6 . 2 0 . 9 6 7 . 8 0 8 - - . 7 7 8 - . 9 6 7 . 7 9 8 . 6 9 8 . 7 9 8 . 2 2 9 . 0 2 9 . 3 2 9 . 8 0 9 . 8 0 9 . 3 1 9 . 1 9 8 . 5 9 8 . 2 9 8 . 5 3 5 . 2 7 8 . 3 4 8 . 1 0 9 . 6 1 8 - . 5 2 . 4 8 7 . 5 6 2 . 1 0 . 2 9 7 . 8 8 7 . 8 4 . 4 4 8 . 1 4 8 . 0 2 . 5 1 8 . 0 1 8 . 7 8 . 4 8 7 . 5 7 7 . 0 6 . 3 3 7 . 8 4 6 . 2 0 . 1 7 6 - . 9 5 8 . 7 1 8 . 3 6 4 . 9 3 8 . 2 2 8 . 0 3 8 . 6 8 8 . 7 7 8 . 9 7 8 . 0 6 8 . 5 4 8 . 4 5 8 . 2 3 8 . 4 1 8 . 2 2 8 . 2 9 6 . 9 3 7 . 0 4 6 . 6 4 8 . 9 2 7 - . 9 4 8 . 5 1 . 8 5 4 . 2 2 8 . 0 2 . 7 1 8 . 9 6 8 . 5 6 . 5 6 8 . 1 4 8 . 5 3 . 6 3 8 . 5 0 8 . 6 0 . 0 0 8 . 5 7 6 . 8 5 . 8 5 7 . 5 3 8 . 8 9 - . 4 7 8 . 0 3 8 . 0 1 5 . 8 3 8 . 9 3 8 . 2 3 8 . 7 8 8 . 8 8 8 . 5 8 8 . 8 5 8 . 5 6 8 . 7 5 8 . 7 2 8 . 8 2 8 . 4 2 8 . 6 1 7 . 1 8 7 . 5 9 5 . 8 5 8 . 8 1 7 - . 4 6 8 . 5 1 8 . 5 3 . 8 2 8 . 4 2 8 . 1 2 . 8 7 8 . 6 7 8 . 0 7 . 5 4 8 . 5 4 8 . 9 3 . 6 1 8 . 4 1 8 . 7 0 . 7 9 6 . 5 6 7 . 0 3 . 1 5 8 . 9 0 7 - . 5 2 9 . 1 8 8 . 2 1 7 . 8 9 8 . 8 8 8 . 7 8 8 . 0 4 9 . 8 2 9 . 3 3 9 . 0 2 9 . 8 0 9 . 7 0 9 . 1 9 8 . 4 8 8 . 5 8 8 . 3 8 7 . 6 3 8 . 9 9 7 . 6 0 9 . 8 9 7 . 5 7 3 - . 5 2 1 - - . 0 0 - . 5 7 3 - . 5 2 1 - . 0 0 - . 0 0 - . 0 0 - . 5 2 1 - - - - . 5 2 1 - . 0 0 - . 0 0 - . 0 5 2 - . 5 2 1 - . 0 0 - . 0 0 - - . 5 2 - . 1 2 - . 4 3 - . 8 4 - . 7 1 - . 7 2 - . 5 2 - . 5 2 - . 9 2 - . 9 2 - . 1 2 - . 7 2 - . 5 2 - . 0 2 - . 3 2 - . 6 2 - . 1 2 - . 3 3 - . 8 6 - . 4 2 - - . 8 3 - . 3 4 - . 8 3 - . 5 6 - . 8 3 - . 3 3 - . 0 4 - . 5 3 - . 7 3 - . 5 4 - . 5 3 - . 2 4 - . 3 4 - . 5 3 - . 8 3 - . 7 4 - . 7 2 - . 2 4 - - . 0 3 - . 8 1 1 - . 2 4 - . 2 3 - . 0 3 - . 0 7 - . 2 3 - . 8 3 - . 4 3 - . 4 2 - . 4 3 - . 4 3 - . 0 3 - . 6 3 - . 2 3 - . 2 3 - . 4 4 - . 6 5 - . 0 4 - . 0 7 - . 0 7 - . 6 4 - - - . 1 0 - - . 6 0 - . 4 0 - . 2 0 - . 4 0 - . 0 0 - . 2 0 - . 4 0 - . 3 0 - . 1 0 - . 3 0 - . 5 0 - . 1 0 - . 6 0 - . 9 0 - . 1 5 - . 2 0 - . 2 0 - . 2 2 3 - - . 3 6 - . 9 6 - - . 2 5 - . 4 5 - - . 6 5 - . 2 5 - . 8 7 4 - . 1 3 1 - 4 . 6 1 - . 2 5 - . 2 6 - . 6 6 - . 1 5 - . 4 5 - . 8 5 - . 4 5 - . 5 5 - . 2 6 - . 6 5 - . 4 5 - . 2 6 - . 4 5 - . 6 6 - . 6 3 - . 0 5 - . 2 5 - . 4 3 - . 1 4 - . 6 4 - . 5 3 - . 8 4 - . 8 4 - . 8 3 - . 6 4 - . 9 4 - . 2 4 - . 6 5 - . 7 6 - . 5 6 - . 6 4 - . 1 5 - . 5 0 2 - . 6 8 1 - . 6 4 - . 2 5 - . 5 5 - . 4 4 - . 9 4 - . 1 5 - . 8 4 - . 4 5 - . 6 5 - . 1 5 - . 6 5 - . 6 5 - . 7 4 - . 7 4 - . 9 9 - . 1 5 - . 1 6 - - 5 . 2 - 6 . 2 - 1 . 9 - 4 . 2 - 5 . 2 - 9 . 2 - 4 . 2 - 3 . 2 - 4 . 2 - 5 . 2 - 9 . 1 - 4 . 2 - 7 . 2 - 6 . 2 - 5 . 2 - 3 . 2 - 8 . 3 - - 1 . 4 - 3 . 5 - 0 . 0 4 - 4 . 4 - 3 . 5 - 3 . 5 - 9 . 3 - 4 . 4 - 8 . 4 - 5 . 4 - 8 . 4 - 1 . 5 - 2 . 4 - 6 . 4 - 0 . 5 - 7 . 4 - 6 . 5 - - 1 . 2 - 9 . 2 - 4 . 4 - 1 . 2 - 8 . 2 - 1 . 3 - 6 . 1 - 4 . 2 - 3 . 2 - 4 . 1 - 2 . 2 - 6 . 2 - 1 . 2 - 4 . 2 - 4 . 2 - 2 . 2 - 9 . 3 - 3 e + + o + 3 e + + o + 3 e + + o + 3 e + + o + + a a + l S +"
        },
        {
            "title": "M\nS\nW\nO",
            "content": "a + s o A - 2 Q 2 e p W l S a C"
        },
        {
            "title": "M\nS\nW\nO",
            "content": "4 . 9 2 - 5 . 5 1 - 1 . 3 1 - o l - 4 - 6 . 2 - 1 . 3 - 2 . 4 - 1 . 5 - 0 . 3 - 6 . 2 - t V i . c o C f o r o e r o a ; i"
        },
        {
            "title": "D\nn\na\nM\nm\no\nr\nf",
            "content": "s c e p t e - . a a a f a s m e e r t e : 4 1 a METRICXQE es-en de-en fr-en zh-en es-en de-en fr-en zh-en es-en de-en fr-en zh-en xCOMETQE"
        },
        {
            "title": "LinguaPy",
            "content": "S Whisper Seamless Canary OWSM Whisper + Aya + Gemma3 + Tower+ Seamless + Aya + Gemma3 + Tower+ Canary + Aya + Gemma3 + Tower+ OWSM + Aya + Gemma3 + Tower+ DeSTA2 Qwen2-Audio Phi-4-Multimodal Voxtral Spire -11.6 -0.3 -8.5 -9.9 -0.0 -0.0 -0.0 -0.6 -0.3 -0.0 -0.3 - -0.3 -1.3 -0.0 -2.0 -0.0 -0.0 -0.0 -0.2 -0.3 -0.0 -0.0 -0.5 -0.0 -0.0 -0.0 -0.1 -0.0 -0.0 -0.0 -0.3 -0.3 -0.0 -0.3 -1.2 -0.0 -0.0 -0.0 -0.6 - -0.0 -0.0 -0.0 - -0.3 -0.0 -0.3 -0.0 -0.0 -0.0 - -0.6 -0.0 -0.3 -0.5 -0.6 -0.0 -0.7 -0.8 -0.3 -0.0 -0.3 -0.4 -0.3 -0.0 -0.3 -0.6 -1.6 -2.7 -1.0 -2.8 -6.2 -4.4 -1.3 -6.8 -1.2 -2.0 -2.0 -1.3 - - - - 71.5 79.5 69.4 63.2 82.3 82.6 81.2 71.6 82.7 82.5 80.4 - 63.5 52.6 61.4 36.0 85.8 85.5 83.9 85.0 84.7 84.3 83.1 83.4 85.7 85.0 83.6 83.9 84.0 85.5 82.7 78.5 82.9 84.4 81.7 75.7 83.4 84.6 82.7 76.9 - 85.8 86.7 84.5 - 85.1 85.7 83.8 85.6 86.2 84.4 - 84.2 82.6 78.4 76.2 83.0 81.1 76.2 70.2 84.0 81.6 77.6 70.2 80.0 74.6 74.3 76.9 80.5 76.6 80.7 81.6 79.1 81.5 82.9 78.6 86.7 85.9 84.5 87.0 - - - - 76.0 86.5 78.4 70.8 86.4 88.8 88.2 78.6 88.1 89.8 88.1 - 60.4 51.1 59.5 43.2 91.2 92.0 90.1 87.5 90.3 90.8 88.7 86.2 91.3 92.0 90.1 87.3 87.0 90.7 87.5 81.3 84.2 89.3 85.9 78.7 85.8 90.5 86.9 80.3 - 91.5 92.5 91.7 - 90.9 91.6 90.4 91.2 92.4 91.6 - 88.5 87.8 81.2 77.0 87.5 86.1 79.6 72.9 89.0 87.5 81.4 74.4 78.5 73.1 70.3 75.0 83.0 78.7 84.4 85.3 84.1 87.4 91.0 83.5 92.6 92.1 91.7 91.1 - - - - Table 15: Results for CS-FLEURS dataset across all languages. %NE %term en-es en-fr en-it en-es en-fr en-it Whisper Seamless Canary OWSM Whisper + Aya Seamless + Aya Canary + Aya OWSM + Aya - - + Gemma3 + Tower+ - 65.6 59.5 58.7 70.9 67.4 64.1 46.5 42.2 40.6 5.6 23.1 25.3 4.7 3.0 7.5 69.6 68.8 63.9 70.5 7.3 48.9 + Gemma3 14.0 23.4 12.3 71.3 67.9 63.6 + Tower+ 57.9 66.2 53.9 3.8 71.7 69.0 64.7 6.5 24.4 52.6 4.2 4.7 10.8 54.2 51.1 47.1 1.6 8.3 7.5 5.4 10.4 8.9 62.7 45.3 55.0 69.8 66.9 64.1 68.9 65.6 65.0 + Gemma3 16.5 15.1 + Tower+ + Gemma3 + Tower+ DeSTA2 Qwen2-Audio Phi-4-Multimodal Voxtral Spire - - - 75.7 72.8 65.2 81.0 82.1 77.3 70.0 67.1 56.9 6.8 29.7 33.5 5.4 2.8 9.2 82.2 82.8 78.2 81.4 7.5 62.4 16.6 29.3 15.3 82.6 82.3 79.0 68.6 81.1 68.7 17.7 18.1 3.5 83.2 83.1 79.0 7.3 31.9 65.8 4.8 5.4 14.4 66.2 66.2 62.0 1.0 10.7 9.4 6.8 12.5 8.8 71.3 55.3 69.7 79.6 82.5 76.9 78.0 76.3 73.6 Table 16: Results for Neuroparl-ST dataset across all languages. METRICXQE en-de en-es en-fr en-it en-nl en-pt en-zh en-de en-es en-fr en-it en-nl en-pt en-zh en-de en-es en-fr en-it en-nl en-pt en-zh xCOMETQE"
        },
        {
            "title": "LINGUAPY",
            "content": "S Whisper Seamless Canary OWSM Whisper + Aya Seamless + Aya Canary + Aya OWSM + Aya - - - - - - - - - - - - 78.2 71.7 70.8 73.9 76.2 71. - 44.6 52.0 47.1 45.2 44.1 45.6 56.6 - -6.5 -0.8 -0.8 -3.3 -1.6 -0.0 -0.0 -0.8 -0.0 61.2 39.0 32.7 33.8 32.1 32.5 65.0 -1.6 -0.0 -0.0 -2.4 -2.4 81.4 76.9 74.9 77.7 78.7 74.9 82.4 -0.0 -0.0 -0.0 -0.0 -0.0 81.4 75.8 74.1 77.0 78.1 74.6 81.9 -0.8 -0.0 + Gemma3 -0.0 -0.8 -0.0 83.1 76.0 74.0 77.4 80.4 74.3 82.6 -0.8 -0.0 -0.0 -0.0 -0.0 + Tower+ 81.0 75.8 74.6 77.3 78.2 73.8 81.3 -0.0 -0.8 -0.8 -0.0 -0.0 80.3 75.9 73.7 76.6 76.7 71.4 80.9 -0.8 -0.0 + Gemma3 -0.0 -0.0 -0.0 82.0 74.4 73.1 75.2 78.0 73.1 82.2 -0.8 -0.0 -0.0 -1.6 -0.0 + Tower+ 82.6 77.5 75.0 78.8 79.7 75.2 82.2 -0.0 -0.8 -0.0 -0.0 -0.0 81.7 75.4 75.3 76.0 78.9 74.9 82.6 -0.0 -0.0 + Gemma3 -0.0 -0.8 -0.0 82.8 76.9 75.4 78.0 80.0 75.9 83.6 -0.8 -0.0 -0.0 -0.8 -0.0 + Tower+ 82.7 76.8 75.0 78.3 78.5 75.5 82.4 -0.0 -0.0 -0.0 -0.0 -0.0 81.2 75.3 74.2 76.1 77.7 74.4 81.3 -0.0 -0.8 + Gemma3 -0.0 -0.8 -0.0 -0.8 -0.0 81.9 75.3 74.2 76.4 79.1 74.4 82.6 -0.0 -0.8 -0.0 + Tower+ -0.8 -20.3 75.6 69.6 67.9 69.6 72.4 66.7 58.8 -0.0 -1.6 -0.8 -6.5 -8.1 -4.1 66.2 62.8 63.1 64.9 60.3 61.2 76.8 -4.9 -0.8 -9.8 -3.3 -36.6 -4.1 -21.1 -7.3 -20.3 70.0 68.5 40.8 65.9 46.2 58.0 61.4 82.9 77.4 76.4 79.9 80.4 76.7 82.6 -0.8 -1.6 -0.8 75.2 68.7 68.7 70.8 72.0 66.3 75.1 -0.0 -0.8 -0.8 - -8.1 -4.1 -0.0 -0.0 -1.6 -9.8 -0.0 -0.0 -0.0 -0.0 -0.0 -0.0 -0.0 -0.0 -0.0 -0.0 -0.8 -0.8 -0.0 -0.0 -0.8 -0.0 -0.0 -0.8 -0.0 -0.0 -0.0 -0.0 -0.0 -0.8 -0.8 -0.0 -2.4 -3.3 -0.8 -0.0 -1.6 -0.8 -0.0 -0.8 -0.0 -1.6 - 77.0 63.7 54.6 63.1 69.3 65. - - - - - - - 60.8 53.2 43.1 45.0 49.1 50.3 45.1 - 66.8 36.4 26.1 29.1 33.8 32.2 46.9 79.9 69.6 61.1 67.0 73.3 69.5 63.8 78.8 65.6 57.1 65.4 71.5 70.5 62.8 80.5 67.5 58.8 68.0 75.4 69.5 64.8 78.8 68.8 60.2 67.7 72.2 68.0 60.4 78.2 66.6 56.9 64.4 71.7 67.6 60.1 81.0 67.3 59.2 67.1 73.5 69.9 64.4 81.0 69.9 60.8 68.3 73.8 70.5 64.0 78.8 67.3 59.9 65.1 73.2 68.8 62.8 80.8 69.1 61.9 67.6 75.0 70.1 65.6 79.9 68.1 61.8 67.3 72.9 71.9 63.2 79.5 68.1 59.4 65.4 72.8 71.7 60.7 80.8 68.7 60.2 67.0 74.9 71.7 63.4 73.9 55.8 46.0 53.2 63.4 57.3 40.7 64.8 56.7 49.4 53.7 54.3 55.7 55.4 69.4 63.9 31.9 58.0 42.1 56.8 45.9 79.9 69.1 61.6 69.0 74.8 71.2 64.3 72.1 61.9 53.3 60.2 64.2 62.6 50.0 DeSTA2 Qwen2-Audio Phi-4-Multimodal Voxtral Spire Table 17: Results for LibriStutter dataset (Fluent) across all languages. METRICXQE en-de en-es en-fr en-it en-nl en-pt en-zh en-de en-es en-fr en-it en-nl en-pt en-zh en-de en-es en-fr en-it en-nl en-pt en-zh xCOMETQE"
        },
        {
            "title": "LINGUAPY",
            "content": "S Whisper Seamless Canary OWSM Whisper + Aya Seamless + Aya Canary + Aya OWSM + Aya - - - - - - - -4.8 -6.6 -9.4 -2.6 -18.5 -2.0 -6.6 - -0.6 -0.6 -0.6 -0.9 -0.3 -0.6 -0.0 -2.0 -18.5 -0.6 -0.0 -0.0 -0.3 -0.3 -0.3 -0.0 -0.0 -0.0 -0.0 -0.0 -0.0 -0.0 -0.0 -0.0 -0.0 + Gemma3 -0.0 -0.6 -0.0 -0.0 -0.3 -0.9 -0.0 -0.0 -0.0 + Tower+ -0.3 -0.0 -0.6 -0.0 -0.0 -0.0 -0.0 -0.3 -0.0 -0.0 -0.6 -0.0 + Gemma3 -0.0 -0.0 -0.3 -0.0 -0.6 -0.6 -0.3 -0.0 -0.3 + Tower+ -0.0 -0.0 -0.0 -0.3 -0.0 -0.0 -0.0 -0.6 -0.3 -0.0 -0.3 -0.3 + Gemma3 -0.3 -0.0 -0.0 -0.0 -0.0 -0.0 -0.0 -0.0 -0.0 + Tower+ -0.0 -0.0 -0.0 -0.0 -0.0 -0.0 -0.0 -0.0 -0.0 -0.0 -0.3 -0.0 + Gemma3 -0.0 -0.0 -0.0 -0.0 -0.3 -0.0 -0.0 -0.0 -0.0 + Tower+ -0.0 -0.6 -28.2 -1.1 -0.9 -0.3 -0.6 -4.3 -5.4 -5.1 -1.7 -4.6 -5.1 -4.6 -3.4 -20.8 -7.4 -13.7 -8.3 -3.7 -23.4 -0.0 -0.0 -0.6 -0.0 -0.0 -0.0 -0.0 -0.0 -0.6 -1.4 -1.1 -0.6 -0.3 -0.3 DeSTA2 Qwen2-Audio Phi-4-Multimodal Voxtral Spire - - - - - - - 26.3 33.7 29.0 25.6 28.3 28.1 42.9 71.3 66.3 63.9 66.9 69.1 64.6 - 49.2 28.2 23.2 24.3 21.0 24.0 55.6 80.6 73.9 72.3 75.2 76.7 73.2 80.2 78.5 73.2 72.0 74.0 75.5 71.2 79.8 79.7 73.5 71.7 73.6 76.4 71.7 79.7 77.1 68.2 67.5 69.9 72.4 67.7 78.2 72.2 65.3 64.3 66.1 67.6 63.2 76.5 75.3 66.3 64.7 68.1 70.4 64.8 76.4 77.8 72.0 70.1 72.4 74.2 70.3 79.0 73.7 69.1 67.1 69.5 70.1 65.9 77.3 76.9 70.2 68.1 71.1 73.7 69.2 78.3 77.4 70.2 69.5 71.8 72.9 69.4 78.8 73.2 67.2 65.1 67.7 68.3 64.5 77.4 75.7 68.6 67.6 70.8 72.5 67.0 77.4 71.1 65.9 64.2 64.7 67.3 63.0 51.3 59.3 55.8 54.4 56.0 53.2 52.3 73.0 63.3 56.5 40.9 53.9 36.7 46.2 61.4 81.7 76.2 73.8 76.8 78.9 73.2 80.5 67.7 60.5 59.3 61.6 65.1 57.1 71. - - - - - - - 36.8 30.2 23.4 22.0 25.2 26.9 27.9 66.3 54.0 45.8 53.6 57.8 55.4 - 51.7 23.8 17.5 20.6 21.2 22.6 34.7 77.6 64.6 57.8 63.5 68.5 64.9 59.2 74.9 62.2 56.8 61.2 67.1 63.8 56.9 77.1 64.3 55.8 61.2 68.9 64.3 60.2 72.4 56.1 49.0 55.7 61.8 57.1 55.4 64.4 48.5 41.9 47.2 52.4 50.3 50.2 70.1 54.3 46.6 53.6 59.5 55.1 53.7 73.4 59.4 52.1 57.4 63.1 60.1 55.0 67.7 54.8 44.7 53.1 56.5 53.1 52.2 72.4 56.9 49.3 55.8 63.8 58.7 54.6 73.4 58.3 50.7 57.0 61.7 59.8 54.5 66.8 51.6 42.8 49.2 54.6 52.2 51.8 71.8 56.5 49.2 55.3 61.7 56.4 52.8 66.2 53.6 41.2 48.1 54.6 52.7 33.4 56.1 43.2 35.1 40.2 43.0 44.0 45.7 57.3 44.3 25.3 38.2 27.1 36.7 40.5 79.0 67.3 59.7 65.8 71.7 67.3 60.5 60.1 46.7 38.1 44.5 50.4 46.5 40.2 Table 18: Results for LibriStutter dataset (Disfluent) across all languages."
        },
        {
            "title": "E\nQ",
            "content": "S"
        },
        {
            "title": "T\nE\nM\nO\nC\nx",
            "content": "S"
        },
        {
            "title": "Y\nP\nA\nU\nG\nN\nI",
            "content": "L - e - e - e - e - n - z - p - n - i - f - s - d - e - e - e - e - n - e - z - p - n - i - r - e - d - e - e - e - n - e - e - z - p - n - t - f - e - d - 4 . 5 4 4 . 6 - 5 . 2 1 3 . 9 5 2 . 6 5 7 . 7 5 1 . 9 0 . 4 4 1 . 8 4 - - - 5 . 1 0 . 8 3 0 . 1 4 8 . 9 2 9 . 7 6 6 . 9 6 4 . 4 - 5 . 0 5 5 . 5 4 9 . 7 3 1 . 8 1 3 . 4 7 . 2 5 3 . 2 5 4 . 0 4 2 . 0 4 8 . 9 3 4 . 9 1 . 8 3 9 . 8 3 0 . 5 3 9 . 0 3 4 . 1 3 9 . 8 3 . 5 3 9 . 6 6 0 . 0 6 - 6 . 8 4 2 . 9 1 . 5 3 7 . 7 1 6 . 7 5 8 . 4 5 6 . 5 5 2 . 3 5 . 1 5 7 . 1 5 1 . 8 3 2 . 7 3 2 . 7 3 3 . 1 1 . 9 2 4 . 0 3 7 . 0 2 9 . 5 3 2 . 8 6 7 . 7 - 3 . 8 1 3 . 8 1 9 . 5 1 8 . 1 1 1 . 3 7 . 9 1 0 . 6 1 7 . 7 1 2 . 7 1 1 . 7 1 1 . 6 8 . 5 1 1 . 6 1 2 . 5 1 1 . 4 1 4 . 4 1 1 . 3 9 . 5 1 7 . 3 3 4 . 7 2 - 7 . 0 5 5 . 7 9 . 2 4 6 . 0 2 5 . 6 5 3 . 5 5 9 . 5 5 5 . 9 3 . 8 4 4 . 8 4 2 . 5 4 7 . 4 4 7 . 4 4 8 . 6 3 . 5 3 0 . 6 3 5 . 8 2 3 . 7 3 1 . 1 6 6 . 3 - 5 . 8 1 7 . 8 1 0 . 5 1 0 . 9 1 . 2 9 . 9 1 1 . 8 1 0 . 8 1 7 . 7 1 9 . 7 1 5 . 3 6 . 2 1 1 . 3 1 6 . 1 1 8 . 9 0 . 0 4 . 3 2 . 5 1 5 . 3 3 6 . 5 2 - - 9 . 0 - 2 . 8 1 5 . 5 4 4 . 4 4 1 . 5 4 4 . 9 2 . 9 3 2 . 9 3 2 . 7 3 6 . 6 3 6 . 6 3 2 . 8 8 . 7 2 3 . 6 2 9 . 2 1 1 . 4 4 9 . 1 3 6 . 3 9 . 7 1 - 8 . 4 3 1 . 3 3 2 . 5 1 4 . 6 7 . 5 4 4 . 5 4 8 . 1 4 5 . 1 4 4 . 1 4 2 . 8 7 . 7 3 7 . 6 3 9 . 8 2 7 . 7 2 6 . 6 2 2 . 8 8 . 6 4 9 . 6 3 1 . 9 5 7 . 5 1 - 2 . 0 8 . 4 3 8 . 5 1 2 . 6 4 9 . 4 4 6 . 6 4 4 . 4 3 . 4 4 0 . 4 4 0 . 0 4 4 . 9 3 4 . 1 4 5 . 0 5 . 8 2 0 . 7 3 1 . 8 2 4 . 6 4 4 . 4 2 2 . 1 2 . 7 1 - 5 . 5 3 0 . 2 3 3 . 5 1 5 . 5 7 . 4 4 5 . 5 4 7 . 9 3 9 . 9 3 0 . 0 4 8 . 6 6 . 6 3 9 . 5 3 4 . 7 2 0 . 6 2 8 . 5 2 6 . 5 5 . 2 4 0 . 2 4 0 . 8 5 0 . 6 1 - 9 . 0 6 . 6 2 7 . 2 1 7 . 9 3 9 . 7 3 8 . 8 3 6 . 5 2 . 5 3 2 . 5 3 8 . 1 3 8 . 1 3 7 . 0 3 6 . 3 8 . 2 2 4 . 4 1 0 . 2 2 6 . 9 3 2 . 5 1 8 . 2 4 . 5 1 - 0 . 7 3 9 . 3 3 3 . 7 1 7 . 6 6 . 5 4 1 . 5 4 0 . 2 4 2 . 2 4 2 . 2 4 3 . 8 7 . 7 3 4 . 7 3 3 . 9 2 8 . 8 2 3 . 4 1 1 . 7 5 . 6 4 5 . 6 3 6 . 9 5 2 . 7 1 - 0 . 6 0 . 1 4 6 . 3 2 4 . 5 5 7 . 3 5 3 . 4 5 9 . 0 6 . 0 5 7 . 0 5 0 . 8 4 1 . 6 4 5 . 6 4 4 . 1 2 . 6 3 3 . 4 1 6 . 5 3 5 . 4 5 1 . 4 3 4 . 7 3 . 6 1 - 4 . 6 4 4 . 9 4 4 . 8 0 . 4 5 . 0 6 5 . 1 6 3 . 6 5 0 . 0 5 5 . 3 5 - - - 9 . 9 4 5 . 3 4 4 . 5 4 6 . 0 4 . 3 7 0 . 4 7 0 . 4 6 - 4 . 1 5 6 . 7 6 . 1 4 1 . 3 2 9 . 5 5 6 . 4 5 6 . 4 5 7 . 6 3 . 6 4 4 . 5 4 8 . 3 4 0 . 2 4 2 . 3 4 0 . 1 9 . 7 3 4 . 8 3 1 . 6 3 5 . 1 4 7 . 3 7 2 . 3 - 5 . 9 5 1 . 1 6 8 . 6 4 4 . 6 2 6 . 7 3 . 5 6 6 . 5 6 4 . 4 6 0 . 3 6 5 . 2 6 7 . 9 7 . 8 4 5 . 8 4 5 . 4 4 0 . 2 4 4 . 2 4 7 . 3 0 . 1 5 2 . 2 7 5 . 7 7 - . 5 4 . 5 4 2 . 1 9 1 1 9 . . 3 1 3 . 3 3 2 . 2 2 . 9 0 2 . 0 0 2 . 2 0 . 2 9 1 . 2 8 1 . 5 9 . 7 5 1 . 9 3 1 . 9 3 7 9 . . 8 9 1 . 4 4 . 0 8 3 - . 8 7 5 . 7 5 5 . 2 1 5 . 0 7 2 . 3 2 6 . 3 1 6 . 3 1 6 . 6 8 5 . 5 7 5 . 4 7 5 . 3 4 5 . 2 3 5 . 5 3 5 . 9 4 4 . 5 3 4 . 9 3 4 . 4 9 3 . 9 8 4 . 7 8 6 . 2 2 7 - . 2 7 1 . 7 5 . 3 1 1 1 4 . . 8 5 . 7 9 1 . 9 8 1 . 9 5 . 8 4 1 . 2 5 1 . 7 2 . 3 1 1 . 9 2 1 7 9 . 6 6 . 0 0 . . 1 0 . 2 0 1 . 4 1 4 . 4 7 - - . 9 4 4 - . 8 2 2 . 5 5 5 . 6 4 5 . 1 4 5 . 8 4 5 . 1 5 5 . 9 4 5 . 8 2 5 . 8 2 5 . 9 1 5 . 8 5 4 . 1 2 4 . 7 0 4 . 8 9 1 . 0 5 6 . 7 8 3 . 4 9 6 . 1 4 4 - . 2 8 2 . 5 5 2 3 8 . . 9 9 3 . 0 0 4 . 6 8 . 6 5 3 . 3 5 3 . 5 4 . 3 2 3 . 1 2 3 . 8 0 . 5 3 2 . 7 1 2 . 6 0 . 3 1 2 . 7 4 4 . 8 3 . 0 6 5 . 2 2 1 - . 0 4 3 . 2 8 2 3 8 . . 4 2 4 . 1 1 4 . 0 1 4 . 1 0 4 . 9 9 3 . 8 9 3 . 4 6 3 . 0 6 3 . 0 7 3 . 9 7 2 . 1 4 2 . 3 8 2 . 0 3 2 . 5 5 4 . 6 2 2 . 5 0 6 . 8 6 1 - . 8 1 . 0 8 2 3 9 . . 5 2 . 6 2 4 . 4 2 4 . 3 8 . 8 8 3 . 9 7 3 . 2 5 . 2 6 3 . 1 5 3 . 7 6 . 0 5 2 . 8 4 2 . 2 3 . 6 5 4 . 5 2 4 . 3 9 . 8 4 1 - . 7 0 3 . 6 6 2 6 8 . . 6 2 4 . 5 1 4 . 7 0 4 . 2 8 3 . 6 8 3 . 3 7 3 . 1 5 3 . 6 4 3 . 8 3 3 . 8 6 2 . 4 5 2 7 0 . . 1 3 2 . 2 7 4 . 1 6 1 . 2 8 5 . 3 5 1 - . 7 1 3 . 2 8 . 0 1 1 . 1 2 4 . 9 1 . 9 0 4 . 0 8 3 . 5 8 . 3 7 3 . 1 4 3 . 0 5 . 1 3 3 . 8 5 2 . 8 4 6 0 . . 3 3 2 . 5 6 . 2 6 3 . 5 8 5 2 9 . - . 9 9 3 . 8 3 3 . 8 3 1 . 4 0 5 . 8 8 4 . 3 8 4 . 2 7 4 . 8 7 4 . 5 6 4 . 2 4 4 . 9 2 4 . 2 2 4 . 2 5 3 . 1 2 3 7 . . 5 9 2 . 7 2 5 . 2 2 3 . 7 6 6 . 9 6 2 - . 4 0 - . 2 1 - . 1 1 - . 1 0 - . 4 4 - . 8 7 1 - . 6 0 2 - . 3 1 - . 8 1 - . 5 1 - . 9 1 - . 2 3 - . 9 2 - - - - . 2 0 - . 2 1 - . 4 0 - . 7 0 - . 4 4 - . 7 7 - . 0 0 - - . 7 2 - . 6 2 - . 0 0 1 - . 6 2 - . 3 3 - . 2 4 - . 0 5 - . 0 5 - . 9 4 - . 5 7 - . 9 9 - . 7 3 - . 8 7 - . 0 1 - . 0 1 - - . 0 0 1 - . 5 0 - . 5 0 - . 3 0 - . 0 3 - . 2 0 - . 2 0 - . 4 1 - . 1 0 - . 1 0 - . 1 0 - . 2 1 - . 0 1 - . 2 0 - . 6 0 - . 8 1 - . 0 1 - . 5 0 - . 2 6 - - . 1 0 - . 1 1 1 - . 2 1 - . 1 0 - . 3 4 - . 3 0 2 - . 4 0 - . 1 0 - . 8 0 3 - . 6 0 - . 3 1 - . 2 2 - . 7 7 - . 0 8 - . 7 7 - . 2 3 1 - . 4 6 1 - . 3 6 1 - . 4 2 - . 1 7 - . 6 0 - . 9 0 - - . 7 0 - . 2 0 - . 2 0 - . 8 8 - . 4 0 - . 3 0 - . 3 1 - . 2 0 - . 4 0 - . 6 0 - . 6 0 - . 7 0 - . 6 0 - . 5 3 - . 6 4 - . 2 4 - . 3 1 - . 3 3 - . 1 6 - . 8 0 - - . 7 3 - . 0 1 - . 3 5 1 - . 4 9 3 - . 7 2 - . 0 2 - . 1 0 2 - . 0 5 - . 8 4 - . 1 5 - . 5 0 3 - . 6 0 3 - . 7 0 3 - . 9 3 3 - . 8 0 4 - - . 3 5 - . 3 7 - . 9 0 - . 9 5 1 - . 0 0 0 1 - - . 3 2 - - . 1 9 - . 3 0 - . 4 1 - . 7 1 - . 2 0 - . 0 0 - . 0 0 - . 8 0 - . 6 3 - . 6 3 - . 3 5 - - 5 . 2 - 8 . 7 - 3 . 7 - 4 . 8 1 - 3 . 1 1 - 9 . 0 1 - 5 . 0 - 3 . 0 - 9 . 0 - 9 . 1 - 8 . 4 - 6 . 5 - 8 . 4 - - 9 . 3 - 2 . 0 1 - 1 . 8 1 - 7 . 4 1 - 2 . 9 1 - 1 . 2 1 - 2 . 2 - 1 . 1 - 9 . 1 - 3 . 1 - 5 . 3 - 1 . 1 - 2 . 3 - - 0 . 5 - 2 . 9 - 6 . 3 1 - 2 . 7 - 2 . 0 1 - 4 . 8 - 8 . 0 - 1 . 1 - 6 . 0 - 4 . 1 - 8 . 4 - 0 . 5 - 0 . 2 - - 0 . 1 - 1 . 2 1 - 1 . 4 1 - 8 . 0 1 - 0 . 6 1 - 0 . 6 1 - 5 . 0 - 7 . 0 - 5 . 0 - 0 . 1 - 8 . 2 - 7 . 4 - 1 . 1 - - 6 . 8 - 0 . 9 - - 0 . 2 - 0 . 8 - 9 . 5 1 - 5 . 1 1 - 7 . 6 - 9 . 0 1 - 8 . 9 - 1 . 1 - 1 . 1 - 9 . 0 - 2 . 2 - 5 . 4 - 8 . 5 - 0 . 4 - 9 . 3 - 4 . 8 - 6 . 8 - 3 . 0 - 5 . 0 - 5 . 0 - 8 . 0 - 2 . 4 - 5 . 4 - 1 . 3 - 3 e + + o + 3 e + + o + 3 e + + o + +"
        },
        {
            "title": "M\nS\nW\nO",
            "content": "a + n y + l S + s r i s m y a C"
        },
        {
            "title": "M\nS\nW\nO",
            "content": ". 7 0 1 - . 6 1 1 - . 9 2 5 - . 5 1 - 2 . 2 1 - 7 . 4 1 - 5 . 2 - 6 . 1 - 0 . 5 - 3 . 7 - 1 . 1 - 6 . 5 1 - 0 . 2 1 - 1 . 3 1 - 5 . 2 - 7 . 1 - 1 . 1 1 - 4 . 1 1 - 0 . 4 1 - 3 e + 0 . 0 - 5 . 6 - 8 . 1 - 0 . 0 - 6 . 5 - 2 . 4 - 0 . 0 - 4 . 3 - 4 . 3 - + o + u - 2 Q 2 e . 4 9 4 - 8 . 2 4 - 6 . 2 6 - 1 . 9 2 - 5 . 5 7 - 6 . 5 4 - 6 . 6 5 - o l - 4 - . 0 0 - . 0 0 - 4 . 1 - 4 . 3 - 4 . 0 - 5 . 1 - 5 . 0 - 9 . 1 - 0 . 0 - 0 . 2 - 6 . 0 - 4 . 1 - 0 . 0 - 8 . 0 - t e S . a a a o t t ) b ( E s r t e : 9 b T"
        },
        {
            "title": "E\nQ",
            "content": "S"
        },
        {
            "title": "T\nE\nM\nO\nC\nx",
            "content": "S"
        },
        {
            "title": "Y\nP\nA\nU\nG\nN\nI",
            "content": "L - e - e - e - e - n - z - p - n - i - f - s - d - e - e - e - e - n - e - z - p - n - i - r - e - d - e - e - e - n - e - e - z - p - n - t - f - e - d - 5 . 9 6 8 . 1 - 0 . 9 1 9 . 6 8 8 . 4 8 6 . 6 8 6 . 7 0 . 6 8 3 . 7 8 - - - 0 . 1 9 . 7 7 4 . 0 8 9 . 3 6 6 . 0 8 8 . 9 7 7 . 0 - 1 . 1 8 7 . 9 7 0 . 6 7 8 . 4 4 7 . 3 5 . 2 8 3 . 3 8 5 . 1 8 8 . 0 8 9 . 0 8 8 . 1 5 . 0 8 9 . 1 8 7 . 5 7 8 . 0 7 0 . 0 4 . 4 8 . 5 6 8 . 7 8 5 . 9 8 - 5 . 2 8 3 . 5 0 . 0 8 7 . 0 4 4 . 9 8 9 . 7 8 5 . 9 8 2 . 8 2 . 7 8 0 . 8 8 7 . 7 8 2 . 7 8 8 . 7 8 1 . 3 4 . 1 8 0 . 0 4 . 5 6 6 . 1 7 4 . 5 8 2 . 1 - 7 . 0 6 2 . 5 6 5 . 7 6 9 . 2 3 1 . 9 3 . 8 6 5 . 9 6 1 . 7 6 5 . 6 6 3 . 7 6 4 . 1 3 . 0 7 3 . 1 7 8 . 0 6 1 . 9 5 0 . 0 9 . 2 8 . 2 5 0 . 6 7 8 . 4 7 - 1 . 1 8 3 . 3 7 . 8 7 1 . 5 4 5 . 8 8 6 . 7 8 1 . 8 8 3 . 6 7 . 5 8 9 . 5 8 9 . 7 8 5 . 7 8 5 . 7 8 1 . 1 5 . 9 7 0 . 0 3 . 8 6 1 . 2 7 5 . 4 8 3 . 0 - 4 . 7 6 0 . 2 7 7 . 8 6 7 . 8 2 0 . 7 8 . 4 7 0 . 6 7 5 . 5 7 6 . 2 7 6 . 4 7 0 . 4 6 . 2 7 6 . 3 7 5 . 7 6 5 . 4 6 0 . 0 1 . 9 5 . 5 5 7 . 6 7 7 . 1 8 - - - 9 . 2 6 6 . 8 4 1 . 1 8 2 . 1 8 9 . 1 8 1 . 1 8 . 9 7 1 . 1 8 0 . 2 8 1 . 1 8 2 . 2 8 2 . 4 5 . 4 7 5 . 4 1 6 . 3 3 8 . 1 7 1 . 0 7 5 . 4 1 . 7 3 - 4 . 0 8 4 . 1 8 2 . 5 2 1 . 6 6 . 5 8 2 . 5 8 9 . 4 8 0 . 5 8 1 . 4 8 6 . 5 5 . 5 8 5 . 5 8 8 . 8 7 7 . 7 7 2 . 4 1 1 . 8 6 . 4 7 7 . 5 7 8 . 9 8 1 . 3 4 - 0 . 2 5 . 2 8 9 . 0 4 1 . 8 8 7 . 7 8 1 . 8 8 6 . 6 6 . 6 8 9 . 6 8 1 . 8 8 2 . 8 8 1 . 8 8 7 . 0 1 . 1 8 2 . 4 1 1 . 2 7 4 . 4 7 0 . 0 6 6 . 1 5 . 5 4 - 2 . 9 7 9 . 9 7 9 . 6 3 4 . 5 7 . 4 8 2 . 5 8 8 . 3 8 0 . 4 8 0 . 4 8 6 . 5 2 . 5 8 0 . 5 8 1 . 8 7 1 . 7 7 2 . 4 1 8 . 3 8 . 0 7 0 . 8 7 6 . 9 8 8 . 1 4 - 0 . 5 6 . 6 7 3 . 1 3 0 . 3 8 8 . 1 8 2 . 2 8 0 . 1 8 . 9 7 9 . 9 7 2 . 2 8 3 . 1 8 9 . 1 8 1 . 3 2 . 2 7 4 . 4 1 9 . 7 5 4 . 0 7 9 . 7 3 5 . 7 1 . 7 3 - 0 . 1 8 5 . 1 8 0 . 5 4 5 . 6 5 . 5 8 9 . 5 8 4 . 4 8 9 . 4 8 7 . 4 8 8 . 5 4 . 5 8 6 . 5 8 8 . 7 7 0 . 7 7 3 . 4 1 4 . 8 2 . 3 7 4 . 6 7 8 . 9 8 8 . 3 4 - 7 . 6 0 . 7 8 8 . 6 6 4 . 1 9 3 . 1 9 5 . 1 9 9 . 0 7 . 0 9 8 . 0 9 0 . 2 9 8 . 1 9 7 . 1 9 6 . 6 1 . 7 8 3 . 4 1 0 . 0 8 2 . 3 8 2 . 0 7 7 . 4 3 . 0 5 7 . 9 6 6 . 1 8 - 1 . 9 1 4 . 8 4 . 6 8 9 . 7 8 8 . 8 8 3 . 7 8 2 . 8 8 - - - 7 . 3 8 4 . 0 8 4 . 2 8 9 . 3 9 . 2 8 2 . 1 8 8 . 3 8 - 8 . 0 8 4 . 0 1 . 8 7 7 . 3 5 3 . 5 8 5 . 4 8 1 . 5 8 6 . 3 4 . 3 8 4 . 2 8 6 . 3 8 0 . 3 8 2 . 3 8 7 . 9 9 . 7 7 0 . 0 6 . 3 7 4 . 2 7 9 . 6 8 2 . 9 - 0 . 2 8 5 . 4 8 0 . 1 8 6 . 3 5 3 . 9 5 . 8 8 3 . 9 8 4 . 8 8 6 . 7 8 9 . 7 8 2 . 8 7 . 7 8 9 . 7 8 0 . 5 8 7 . 3 8 0 . 0 4 . 5 0 . 7 7 8 . 3 8 8 . 0 9 - . 3 7 . 0 0 7 . 5 1 7 . 6 4 . 1 5 7 . 7 3 7 . 3 4 . 5 3 7 . 5 2 7 . 1 3 . 3 6 7 . 1 5 7 . 5 5 . 5 9 6 . 9 6 6 0 0 . . 2 5 5 . 3 4 6 . 7 9 . 4 0 8 - . 6 1 8 . 3 3 8 . 4 0 8 . 5 5 5 . 8 8 8 . 3 8 8 . 4 8 8 . 1 7 8 . 9 6 8 . 7 6 8 . 1 8 8 . 5 7 8 . 9 7 8 . 5 3 8 . 3 2 8 0 0 . . 5 7 7 . 0 7 7 . 1 4 8 . 2 0 9 - . 6 6 6 . 3 1 . 9 7 6 . 8 5 3 . 8 7 . 2 6 7 . 4 6 7 . 2 6 . 0 4 7 . 9 4 7 . 1 5 . 9 3 7 . 2 4 7 . 4 9 . 5 6 6 0 0 . . 1 6 . 4 0 6 . 6 5 7 . 5 2 - - - . 6 2 7 . 2 0 6 . 4 6 8 . 6 6 8 . 4 6 8 . 8 5 8 . 8 5 8 . 8 5 8 . 2 6 8 . 8 6 8 . 8 6 8 . 4 2 8 . 4 2 8 0 . . 9 9 3 . 6 1 8 . 0 5 7 . 8 8 8 . 4 8 5 - . 1 4 7 . 3 5 7 . 5 2 . 8 1 8 . 4 1 8 . 7 0 . 8 0 8 . 5 0 8 . 9 9 . 1 1 8 . 3 1 8 . 8 0 . 6 4 7 . 9 3 7 3 1 . . 6 6 6 . 6 9 6 . 6 9 . 5 6 8 . 1 9 3 - . 7 6 7 . 1 8 7 . 3 4 3 . 8 4 8 . 6 4 8 . 1 5 8 . 7 3 8 . 4 3 8 . 7 3 8 . 7 4 8 . 7 4 8 . 9 4 8 . 7 7 7 . 2 7 7 5 1 . . 5 9 6 . 6 9 6 . 3 5 5 . 1 9 8 . 1 4 4 - . 6 6 . 2 7 7 . 4 6 3 . 2 4 . 7 4 8 . 1 4 8 . 7 2 . 1 3 8 . 9 2 8 . 7 3 . 0 4 8 . 4 3 8 . 3 7 . 9 6 7 7 0 . . 0 8 . 8 9 6 . 0 5 7 . 1 9 . 5 3 4 - . 9 5 7 . 3 6 7 . 3 7 3 . 4 3 8 . 3 3 8 . 7 2 8 . 1 2 8 . 1 2 8 . 3 1 8 . 0 3 8 . 2 3 8 . 5 2 8 . 6 7 7 . 5 6 7 7 0 . . 5 9 6 . 1 4 7 . 8 7 3 . 9 7 8 . 6 2 4 - . 7 6 7 . 5 7 . 0 1 4 . 1 4 8 . 9 3 . 4 3 8 . 4 2 8 . 7 2 . 0 2 8 . 1 3 8 . 5 3 . 6 2 8 . 8 5 7 . 1 6 6 0 . . 8 9 6 . 1 2 . 1 3 7 . 7 8 8 . 9 1 - . 7 1 8 . 7 1 8 . 0 1 5 . 8 8 8 . 5 8 8 . 4 8 8 . 3 8 8 . 4 8 8 . 0 8 8 . 9 8 8 . 8 8 8 . 1 8 8 . 0 3 8 . 8 2 8 7 . . 8 5 7 . 6 7 7 . 5 7 6 . 9 2 9 . 4 0 5 - . 3 0 - . 0 0 - . 1 3 - . 1 0 - . 6 0 - . 3 0 - . 3 0 - . 5 0 - . 1 0 - - - - . 2 0 - . 5 0 - . 2 0 - . 1 0 - . 1 5 - . 0 8 - . 6 0 - - . 3 0 - . 3 0 - . 2 0 - . 2 1 - . 5 1 - . 5 1 - . 3 2 - . 3 0 - . 2 0 - . 1 0 - . 1 0 - . 1 0 - . 1 0 - . 2 0 - . 4 0 - . 0 0 - . 1 0 - . 0 0 - . 2 0 - . 0 0 - . 0 0 - . 0 0 - . 2 0 - . 0 0 - . 0 0 - . 0 0 - . 0 0 - . 0 0 - . 1 0 - . 0 0 - . 1 0 - . 4 0 - . 1 0 - . 6 0 - . 3 0 - . 4 0 - . 4 0 - . 4 0 - . 4 0 - . 0 0 - . 3 0 - . 1 0 - . 0 0 - . 0 0 - . 4 0 - . 0 0 - . 1 0 - . 3 0 - . 2 0 - . 0 0 - . 0 0 - . 2 0 - . 0 0 - . 1 0 - . 0 0 - . 0 0 - . 0 0 - . 1 0 - . 1 0 - . 0 0 - . 1 0 - . 1 0 - . 6 0 - . 0 2 - . 2 0 - . 2 0 - . 3 0 - . 0 0 - . 1 0 - . 0 0 - . 5 0 - . 6 0 - . 6 0 - . 0 0 - . 5 0 - . 4 0 - . 6 1 - . 3 0 - . 0 0 - - . 1 0 - . 5 1 - . 2 5 - . 0 0 - - . 4 0 - . 8 1 - . 4 0 - . 6 0 - - . 1 0 - . 2 2 - . 2 3 - . 1 0 - - . 2 1 - . 7 3 - . 3 7 - . 0 0 - - . 0 0 0 1 - . 0 0 0 1 - . 0 0 0 1 - . 0 0 0 1 - . 0 0 0 1 - - - 5 . 0 - 5 . 0 - 3 . 0 - 0 . 0 - 0 . 0 - 0 . 0 - 0 . 0 - 0 . 0 - 2 . 0 - 0 . 0 - 0 . 0 - 5 . 0 - 0 . 0 - 0 . 0 - 7 . 1 - 0 . 6 4 - - 5 . 0 - 3 . 0 - 0 . 0 4 - 9 . 0 - 8 . 0 - 1 . 1 - 3 . 0 - 5 . 0 - 5 . 0 - 3 . 0 - 2 . 0 - 2 . 0 - 3 . 0 - 5 . 0 - 0 . 0 - 3 . 0 - 0 . 2 - - 3 . 1 - 2 . 0 - 9 . 1 - 2 . 0 - 2 . 0 - 4 . 0 - 0 . 0 - 2 . 0 - 0 . 0 - 4 . 0 - 2 . 0 - 6 . 0 - 4 . 0 - 2 . 0 - 0 . 0 - 2 . 0 - 5 . 1 - 0 . 0 - 0 . 0 - 5 . 0 - 6 . 0 - 0 . 0 - 2 . 2 - 8 . 1 1 - 5 . 0 1 - 7 . 5 2 - - 2 . 0 - 3 . 0 - 3 . 2 - 3 . 0 - 5 . 0 - 3 . 0 - 2 . 0 - 3 . 0 - 0 . 0 - 0 . 0 - 0 . 0 - 0 . 0 - 3 . 0 - 6 . 0 - 0 . 0 - 5 . 0 - 5 . 2 - 9 . 5 - 0 . 0 - 4 . 1 - - 0 . 0 - 3 . 0 - 3 . 1 - 2 . 0 - 7 . 0 - 7 . 0 - 2 . 0 - 3 . 0 - 3 . 0 - 2 . 0 - 3 . 0 - 2 . 0 - 3 . 0 - 5 . 0 - 0 . 0 - 3 . 0 - 1 . 1 - - 6 . 0 - 5 . 0 - 5 . 2 - 3 . 0 - 6 . 0 - 6 . 0 - 8 . 0 - 5 . 0 - 5 . 0 - 5 . 0 - 3 . 0 - 3 . 0 - 2 . 1 - 1 . 1 - 0 . 0 - 8 . 0 - 1 . 3 - - 2 . 0 - 0 . 0 - 8 . 0 - 5 . 0 - 5 . 0 - 5 . 0 - 0 . 0 - 2 . 0 - 0 . 0 - 0 . 0 - 0 . 0 - 0 . 0 - 0 . 0 - 0 . 0 - 0 . 0 - 2 . 0 - 3 . 2 - 3 e + + o + 3 e + + o + 3 e + + o + 3 e + + o + +"
        },
        {
            "title": "M\nS\nW\nO",
            "content": "a + n i - 2 Q 2 e A + l S + p W s s m y a C"
        },
        {
            "title": "M\nS\nW\nO",
            "content": "0 . 2 5 - 9 . 2 1 - 3 . 4 2 - o l - 4 - 2 . 0 - 8 . 0 - 3 . 0 - 1 . 1 - 0 . 0 - 9 . 0 - t V i . a a a r e a ) i ( E s N f u : 0 2 a T"
        },
        {
            "title": "METRICXQE",
            "content": "S xCOMETQE en-de en-es en-fr en-it en-nl en-pt en-zh en-de en-es en-fr en-it en-nl en-pt en-zh en-de en-es en-fr en-it en-nl en-pt en-zh Whisper Seamless Canary OWSM Whisper + Aya + Gemma3 + Tower+ Seamless + Aya + Gemma3 + Tower+ Canary + Aya + Gemma3 + Tower+ OWSM + Aya + Gemma3 + Tower+ DeSTA2 Qwen2-Audio Phi-4-Multimodal Voxtral Spire - - - -7.7 -9. - -6.7 -9.3 - - - -3.3 -4.8 -10.8 -13.3 -10.9 -7.8 - -4.6 -8.4 -6.5 -0.7 -4.8 -19.0 -13.4 -18.1 -20.0 -21.5 -8.2 -0.8 -7.0 -3.3 -9.7 -0.8 -8.0 -3.7 -8.9 -1.0 -8.5 -4.2 -6.5 -1.5 -5.2 -2.5 -8.1 -0.3 -6.8 -2.8 -6.7 -0.4 -6.6 -3.2 -8.0 -0.8 -6.7 -3.3 -9.4 -0.7 -8.2 -3.9 -8.7 -0.6 -8.1 -4.2 -7.8 -1.1 -6.5 -3.0 -9.2 -0.6 -8.1 -3.6 -8.1 -8.2 -3.7 -0.7 -8.8 -49.1 -4.7 -8.8 -2.3 -8.6 -12.2 -56.5 -56.2 -89.4 -44.3 -72.4 -61.1 -52.2 -0.9 -7.6 -0.2 -7.6 -4.6 -5.9 -6.7 -3.2 -5.0 -5.0 -4.4 -6.0 -6.5 -3.9 -5.2 -5.5 -7.1 -9.8 -12.2 -11.3 -11.8 -8.6 -9.1 -9.5 -6.9 -7.7 -8.7 -8.5 -8.9 -9.5 -7.6 -8.4 -9.2 -9.8 -6.2 -7.2 -7.4 -5.0 -6.0 -6.6 -6.0 -7.1 -7.7 -5.9 -6.9 -7.1 -8.4 -4.2 -5. -7.4 -7.3 -6.4 -5.4 -8.8 -9.7 -9.7 -8.9 - - - - - - - 83.8 81.5 81.3 77.3 75.8 75.3 86.1 89.5 81.9 83.0 82.9 82.5 82.1 - 82.1 57.9 58.4 55.9 54.2 51.1 86.4 92.5 85.0 87.3 86.2 84.9 85.1 93.8 91.9 83.2 86.0 85.1 84.0 83.5 93.4 91.8 84.4 84.9 85.3 84.2 83.3 93.8 91.6 83.4 85.8 84.2 83.4 83.5 91.9 91.2 82.4 84.0 83.4 82.3 81.7 92.9 90.7 83.1 83.2 82.3 81.3 81.1 92.5 92.2 84.9 87.2 86.0 84.5 85.0 93.6 91.6 83.3 85.6 84.9 83.9 83.1 93.5 91.5 84.2 84.9 84.7 83.9 83.4 94.0 91.6 83.6 86.4 84.7 84.1 83.8 92.5 90.8 82.3 84.9 83.5 82.9 81.8 92.9 90.9 83.4 84.4 84.0 82.9 81.9 93.1 87.2 79.5 79.8 77.9 77.9 77.0 44.7 81.1 73.1 74.6 71.8 71.3 72.3 88.7 40.3 39.0 8.9 48.4 22.7 32.5 44.1 91.6 83.5 85.4 85.2 84.2 84.2 93.6 83.4 74.2 76.0 74.5 75.3 72.9 87.3 - - - - - - - 84.8 81.4 77.4 75.1 78.8 76.9 79.6 90.6 82.8 80.8 80.9 84.8 83.8 - 85.2 58.4 50.8 52.4 58.8 52.6 80.0 93.2 85.8 84.9 84.8 86.7 85.9 90.3 92.5 83.4 82.7 82.0 86.1 84.5 89.9 92.5 85.2 82.6 83.3 86.0 85.4 90.2 91.1 81.6 78.6 79.9 84.8 82.4 85.4 90.7 80.8 77.9 78.2 83.9 81.8 87.6 90.1 82.0 76.7 77.6 82.4 81.0 86.9 92.9 85.6 84.5 84.8 86.5 86.1 90.1 92.1 83.3 82.1 81.7 86.1 84.1 89.6 92.3 85.0 82.5 82.6 85.8 85.2 90.3 92.2 84.0 82.9 82.9 86.0 84.6 88.0 91.3 82.0 80.7 80.2 85.3 82.9 88.5 91.6 83.9 81.1 81.6 84.8 83.8 88.9 85.6 74.8 62.1 65.7 77.6 73.0 38.8 80.9 70.3 68.6 65.9 72.7 73.3 81.8 41.3 39.8 8.7 48.8 24.2 33.8 42.1 92.3 83.2 82.8 81.9 86.4 85.7 90.0 81.7 71.9 67.4 69.2 75.2 73.1 73. Table 21: Results for mExpresso dataset across all languages. METRICXQE en-de en-fr en-pt en-zh en-de en-fr en-pt en-zh en-de en-fr en-pt en-zh xCOMETQE"
        },
        {
            "title": "LINGUAPY",
            "content": "S METRICXQE en-de en-fr en-pt en-zh en-de en-fr en-pt en-zh en-de en-fr en-pt en-zh xCOMETQE"
        },
        {
            "title": "LINGUAPY",
            "content": "S Whisper Seamless Canary OWSM Whisper + Aya + Gemma3 + Tower+ Seamless + Aya + Gemma3 + Tower+ Canary + Aya + Gemma3 + Tower+ OWSM + Aya + Gemma3 + Tower+ DeSTA2 Qwen2-Audio Phi-4-Multimodal Voxtral Spire - -1.9 -1.4 -1.4 -0.7 -1.0 -0.7 -0.5 -0.5 -0.7 -0.5 -0.5 -0.5 -0.7 -0.7 -0.7 -0.5 -3. - -2.9 -1.7 -0.5 -1.0 -1.2 -1.0 -0.5 -0.7 -0.7 -0.2 -1.0 -0.5 -0.5 -0.7 -0.5 -0.5 -2.9 - - -7.5 -5.0 - -0.7 -0.0 -7.5 -0.0 -0.5 -0.0 -0.7 -0.0 -0.5 -0.7 -0.2 -0.0 -0.7 -0.0 -0.5 -0.5 -0.5 -0.0 -0.0 -0.2 -0.7 -0.7 -0.5 -0.2 -0.7 -0.5 -0.0 -0.2 -20.4 -1.4 -1.9 -31.2 -55.8 -19.7 -18.3 -0.0 -0.0 -0.5 -0.5 -0.5 -0.2 -0.7 -0.7 - - - - 75.6 69.1 64.2 72.3 78.5 71.2 71.4 - 61.5 44.7 45.1 72.6 87.6 80.8 79.5 87.2 87.6 81.0 79.1 86.9 87.8 80.5 79.2 86.9 84.2 75.7 73.6 84.2 84.1 76.5 73.3 84.7 84.1 75.7 73.3 84.2 85.0 76.7 74.9 84.9 84.6 77.1 75.5 85.5 85.0 75.9 74.1 85.3 83.8 75.0 72.9 83.9 83.0 75.1 72.3 83.9 82.7 73.4 71.5 83.3 74.7 65.8 61.4 61.5 75.2 67.2 66.0 80.5 60.1 34.0 59.9 69.7 89.2 82.4 80.8 87.8 72.0 62.3 57.5 75.6 - - - - 80.4 66.6 70.7 64.9 82.9 69.2 77.4 - 72.0 38.8 52.5 62.9 89.4 77.5 82.6 80.0 88.9 76.7 82.7 79.9 89.7 76.5 82.8 79.0 86.5 70.0 77.5 74.9 85.8 71.0 78.2 76.1 86.2 70.1 77.1 75.2 87.0 73.3 79.0 76.0 86.7 73.1 79.4 76.2 87.1 71.9 78.5 76.4 86.3 70.5 77.9 75.1 85.5 70.3 77.9 75.4 85.6 69.9 76.5 74.3 71.7 52.7 61.9 48.0 78.6 61.4 70.9 70.6 61.9 33.3 64.6 65.4 90.7 80.3 84.1 80.6 71.9 52.3 61.7 56.5 Whisper Seamless Canary OWSM Whisper + Aya + Gemma3 + Tower+ Seamless + Aya + Gemma3 + Tower+ Canary + Aya + Gemma3 + Tower+ OWSM + Aya + Gemma3 + Tower+ DeSTA2 Qwen2-Audio Phi-4-Multimodal Voxtral Spire - - - - - - -2.6 -5.5 -4.1 -1.9 -34.4 -61.5 -2.2 -3.8 -2.2 -1.7 -1.4 -1.0 -1.9 -1.0 -1.4 - - - - - - - - - -1.2 -0.7 -1.4 -0.7 -1.0 -1.0 -1.0 -0.5 -0.7 -1.4 -1.7 -0.7 -1.2 -1.0 -0.2 -1.0 -0.5 -0.5 - - - -1.2 -1.2 -0.7 -1.2 - - - -0.5 -0.2 -0.2 -0.5 -1.0 -1.4 -51.2 -72.1 -86.1 -96.4 -91.6 -84.1 -88.7 -83.9 -9.6 -16.6 -40.4 -25.2 -0.2 -1.0 -0.5 -1.2 - - - - - - - - - - - - - - - - - - - - - 69.9 63.4 61.2 - 61.3 28.7 17.8 71.0 85.3 74.3 77.9 85.2 84.6 78.3 75.7 83.6 83.6 75.9 75.0 83.9 - - - 87.1 79.1 78.4 85.3 85.1 79.7 77.8 85.8 85.1 78.5 76.8 84.8 86.9 75.9 76.0 84.3 84.7 80.1 74.3 81.6 85.2 76.6 74.8 82.9 1.2 16.0 12.8 3.9 5.7 2.2 59.3 65.8 55.7 37.6 89.3 83.0 82.1 87.5 - 1.7 3.3 - - - - - - - - - - - - - - - - - - - - 77.3 62.2 68.2 - 70.1 24.1 20.5 58.9 86.9 67.8 81.0 76.9 85.7 74.8 79.2 74.0 85.0 73.3 78.9 74.9 - - - 88.8 71.4 80.7 76.2 87.3 75.2 81.4 77.1 86.7 73.6 80.1 75.6 88.2 66.0 77.2 73.1 86.5 75.1 76.9 70.9 86.4 71.0 77.7 72.6 0.7 4.8 60.9 62.5 60.0 29.7 90.3 80.0 85.3 79.2 - 3.0 4.1 7.3 4. 4.5 3.2 - - - Table 22: Results for ACL 60/60 (Short) dataset across all languages. Table 23: Results for ACL 60/60 (Long) dataset across all languages."
        },
        {
            "title": "LINGUAPY",
            "content": "xCOMETQE METRICXQE en-de en-it en-zh en-de en-it en-zh en-de en-it en-zh S"
        },
        {
            "title": "LINGUAPY",
            "content": "xCOMETQE METRICXQE en-de en-it en-zh en-de en-it en-zh en-de en-it en-zh Whisper Seamless Canary OWSM Whisper + Aya + Gemma3 + Tower+ Seamless + Aya + Gemma3 + Tower+ Canary + Aya + Gemma3 + Tower+ OWSM + Aya + Gemma3 + Tower+ DeSTA2 Qwen2-Audio Phi-4-Multimodal Voxtral Spire - - - -7.3 -3.4 -6.9 - -3.9 -3.5 -0.2 -2.4 -2.2 -1.1 -1.4 -1.1 -0.2 -1.6 -2.0 -0.4 -1.6 -1.7 -1.8 -2.3 -1.8 -1.3 -3.0 -2.0 -1.7 -2.3 -2.7 -0.7 -1.2 -1.3 -0.2 -1.8 -1.6 -0.4 -1.5 -1.6 -1.0 -1.6 -1.1 -0.0 -1.7 -1.3 -0.3 -1.7 -1.5 -2.0 -2.0 -16.2 -1.1 -4.7 -3.0 -22.4 -5.9 -14.8 -0.3 -0.2 -1.8 -2.0 -1.6 -1.7 - - - 69.3 60.0 62.6 79.6 74.9 - 62.3 48.2 70.9 88.5 83.8 87.6 88.3 83.3 88.8 88.9 83.2 88.7 79.2 72.7 81.1 78.4 73.1 82.0 79.3 72.0 81.4 87.5 82.2 87.3 87.0 82.2 87.7 87.3 82.0 87.4 85.1 79.3 84.2 84.7 79.3 86.0 84.6 78.5 85.4 79.3 71.1 62.9 75.6 68.7 82.4 64.9 74.6 72.4 89.4 84.1 88.7 70.6 61.0 72.3 - - - 73.3 59.8 48.6 83.2 76.3 - 73.0 50.1 60.4 89.7 83.9 79.1 89.7 83.9 79.9 90.1 83.6 80.6 80.1 70.9 67.8 79.4 70.8 68.2 80.3 70.4 69.2 88.9 82.5 78.4 88.3 82.7 78.5 88.9 82.3 79.0 87.2 78.5 72.9 86.8 78.6 75.6 86.5 76.8 74.6 79.5 66.6 50.0 78.4 65.7 70.3 67.3 75.4 65.7 90.6 84.1 80.8 70.2 56.1 52.6 Whisper Seamless Canary OWSM Whisper + Aya + Gemma3 + Tower+ Seamless + Aya + Gemma3 + Tower+ Canary + Aya + Gemma3 + Tower+ OWSM + Aya + Gemma3 + Tower+ DeSTA2 Qwen2-Audio Phi-4-Multimodal Voxtral Spire - - - - - - - -5.1 -4.6 -2.1 -2.2 -35.7 -1.1 -3.3 -3.9 -1.3 -2.7 -2.4 -1.2 -2.6 -2.4 - - - - - - - - - -0.5 -2.2 -2.2 -0.7 -2.9 -2.3 -0.5 -1.8 -1.5 -0.7 -2.8 -2.5 -0.5 -2.2 -1.8 -0.1 -1.4 -1.7 -68.1 -71.7 -95.4 -79.4 -82.5 -72.4 -13.9 -14.7 -46.1 -0.7 -2.7 - - -1.6 - - - - - - - - - - - - - 74.0 72.5 - 64.8 32.2 71.8 83.5 79.9 85.4 85.0 78.5 85.5 83.8 78.3 85.1 - - - 88.1 82.6 87.2 87.2 79.8 87.6 87.6 80.8 86.7 86.3 79.7 85.5 86.7 80.8 86.8 85.9 80.3 85.3 0.8 9.3 11.8 4.0 8.5 6.0 69.3 64.1 35.5 89.4 81.2 88.5 - - - - - - - - - - - - - - - 78.6 74.0 - 74.4 31.6 61.7 84.6 80.8 75.6 87.5 80.9 76.8 86.0 80.0 75.4 - - - 89.6 83.0 77.7 89.1 81.3 78.1 89.3 81.7 77.6 87.4 79.2 74.9 88.7 81.9 77.3 88.2 80.3 73.7 0.7 7.6 4.5 6.6 70.4 65.0 29.4 91.0 83.1 80.3 - 8.0 6.8 - - Table 24: Results for MCIF (Short) dataset across all languages. Table 25: Results for MCIF (Long) dataset across all languages."
        },
        {
            "title": "H Supplementary Accent Benchmark Results",
            "content": "en-de en-es en-fr en-it en-nl en-pt en-zh de-en es-en it-en zh-en Whisper Seamless Canary OWSM Whisper + Aya + Gemma3 + Tower+ Seamless + Aya + Gemma3 + Tower+ Canary + Aya + Gemma3 + Tower+ OWSM + Aya + Gemma3 + Tower+ DeSTA2 Qwen2-Audio Phi-4-Multimodal Voxtral Spire - 0.023 0.022 0.026 0.030 0.035 0.025 0.020 0.018 0.020 0.021 0.028 0.023 0.025 0.023 0.027 0.085 0.031 0.075 0.028 0.062 - 0.024 0.033 0.039 0.032 0.043 0.033 0.022 0.031 0.028 0.031 0.037 0.033 0.034 0.040 0.035 0.081 0.031 0.063 0.037 0.067 - 0.037 0.035 0.037 0.032 0.036 0.027 0.034 0.032 0.033 0.026 0.033 0.026 0.031 0.032 0.031 0.071 0.044 0.115 0.037 0.076 - 0.039 0.030 0.044 0.030 0.032 0.031 0.028 0.032 0.030 0.027 0.032 0.030 0.034 0.034 0.036 0.071 0.042 0.049 0.036 0. - 0.038 0.029 0.044 0.035 0.036 0.039 0.029 0.027 0.033 0.030 0.033 0.031 0.033 0.038 0.036 0.088 0.043 0.066 0.035 0.074 - 0.036 0.027 0.058 0.029 0.035 0.032 0.027 0.031 0.029 0.029 0.034 0.033 0.027 0.036 0.031 0.084 0.034 0.083 0.032 0.067 - 0.018 - 0.021 0.030 0.028 0.029 0.021 0.016 0.017 0.024 0.017 0.018 0.023 0.021 0.023 0.066 0.024 0.044 0.023 0.066 0.044 0.042 0.025 0.013 0.034 0.039 0.034 0.043 0.034 0.034 0.039 0.040 0.034 0.052 0.061 0.038 0.039 0.063 0.056 0.036 - 0.032 0.017 0.016 0.021 0.008 0.015 0.011 0.013 0.025 0.018 0.012 0.020 0.016 0.020 0.026 0.016 0.033 0.018 0.024 0.016 - 0.031 0.037 0.043 0.094 0.042 0.046 0.041 0.036 0.045 0.035 0.026 0.038 0.034 0.028 0.040 0.029 0.115 0.036 0.048 0.058 - 0.020 0.043 - 0.029 0.141 0.147 0.161 0.113 0.126 0.137 - - - 0.170 0.142 0.161 0.054 0.061 0.090 0.072 - Table 26: Standard deviation of xCOMETQE across source-language accent. Values correspond to those used to create Fig. 2. scores for ManDi (zh-en) and CommonAccent (all other directions) Figure 4: xCOMETQE results come from ManDI, while all other pairs represent CommonAccent results. results for language pairs into English, broken down by source-language accent. ZH-EN Figure 5: CommonAccent xCOMETQE accent. results for language pairs out of English, broken down by source speech"
        }
    ],
    "affiliations": [
        "AI-Bio Convergence Research Institute",
        "Barcelona Supercomputing Center",
        "Charles University",
        "ETH Zurich",
        "Fondazione Bruno Kessler",
        "KIT",
        "Universitat Politècnica de Catalunya",
        "Universitat Politècnica de València",
        "University of Zurich"
    ]
}