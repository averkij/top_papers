{
    "paper_title": "Tensor Product Attention Is All You Need",
    "authors": [
        "Yifan Zhang",
        "Yifeng Liu",
        "Huizhuo Yuan",
        "Zhen Qin",
        "Yang Yuan",
        "Quanquan Gu",
        "Andrew Chi-Chih Yao"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Scaling language models to handle longer input sequences typically necessitates large key-value (KV) caches, resulting in substantial memory overhead during inference. In this paper, we propose Tensor Product Attention (TPA), a novel attention mechanism that uses tensor decompositions to represent queries, keys, and values compactly, significantly shrinking KV cache size at inference time. By factorizing these representations into contextual low-rank components (contextual factorization) and seamlessly integrating with RoPE, TPA achieves improved model quality alongside memory efficiency. Based on TPA, we introduce the Tensor ProducT ATTenTion Transformer (T6), a new model architecture for sequence modeling. Through extensive empirical evaluation of language modeling tasks, we demonstrate that T6 exceeds the performance of standard Transformer baselines including MHA, MQA, GQA, and MLA across various metrics, including perplexity and a range of renowned evaluation benchmarks. Notably, TPAs memory efficiency enables the processing of significantly longer sequences under fixed resource constraints, addressing a critical scalability challenge in modern language models. The code is available at https://github.com/tensorgi/T6."
        },
        {
            "title": "Start",
            "content": "Yifan Zhang 1,2 Yifeng Liu 3 Huizhuo Yuan3 Zhen Qin4 Yang Yuan1,2 Quanquan Gu3 Andrew Chi-Chih Yao1,2 1IIIS, Tsinghua University 2Shanghai Qi Zhi Institute 3University of California, Los Angeles 4TapTap 5 2 0 J 1 1 ] . [ 1 5 2 4 6 0 . 1 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Scaling language models to handle longer input sequences typically necessitates large key-value (KV) caches, resulting in substantial memory overhead during In this paper, we propose Tensor Product Attention (TPA), novel inference. attention mechanism that uses tensor decompositions to represent queries, keys, and values compactly, significantly shrinking KV cache size at inference time. By factorizing these representations into contextual low-rank components (contextual factorization) and seamlessly integrating with RoPE, TPA achieves improved model quality alongside memory efficiency. Based on TPA, we introduce the Tensor ProducT ATTenTion Transformer (T6), new model architecture for sequence modeling. Through extensive empirical evaluation of language modeling tasks, we demonstrate that T6 exceeds the performance of standard Transformer baselines including MHA, MQA, GQA, and MLA across various metrics, including perplexity and range of renowned evaluation benchmarks. Notably, TPAs memory efficiency enables the processing of significantly longer sequences under fixed resource constraints, addressing critical scalability challenge in modern language models. The code is available at https://github.com/tensorgi/T6."
        },
        {
            "title": "Introduction",
            "content": "Large language models (LLMs) have revolutionized natural language processing, demonstrating exceptional performance across tasks (Brown et al., 2020; Chowdhery et al., 2023; Touvron et al., 2023; Bubeck et al., 2023). As these models evolve, their ability to process longer contexts becomes increasingly important for sophisticated applications such as document analysis, complex reasoning, and code completions. However, managing longer sequences during inference poses significant computational and memory challenges, particularly due to the storage of key-value (KV) caches (Zhang et al., 2023c; Liu et al., 2024c). Because memory consumption grows linearly with sequence length, the maximum context window is limited by practical hardware constraints. variety of solutions have been explored to address this memory bottleneck. Some approaches compress or selectively prune cached states through sparse attention patterns (Child et al., 2019) or token eviction strategies (Zhang et al., 2023c; Xiao et al., 2024; Ribar et al., 2024), though such methods risk discarding tokens that may later prove important. Other work proposes off-chip storage of keyvalue states (He & Zhai, 2024), at the expense of increased I/O latency. Attention variants like multi-query attention (MQA) (Shazeer, 2019) and grouped-query attention (GQA) (Ainslie et al., 2023) reduce per-token cache requirements by sharing keys and values across heads, but often compromise flexibility or require significant architectural modifications. Meanwhile, low-rank weight factorization methods such as LoRA (Hu et al., 2022) effectively reduce fine-tuning memory, yet do not address the KV cache overhead that dominates runtime. The recently introduced Multi-head Latent Attention (MLA) in Deepseek-V2 (Liu et al., 2024a) caches compressed key-value repreEqual contribution; Tech lead; Corresponding author. Figure 1: Tensor Product Attention (TPA) in the Tensor ProducT ATTenTion Transformer (T6). Different from multi-head attention, in each layer, firstly the hidden state goes through different linear layers to get the latent factor matrices As and Bs for query, key, and value. We additionally apply RoPE to BQ and BK for query and key. Then the multi-head query, key, and value vectors are attained by the tensor product of A() and B(). Finally, the output of TPA is produced by scaled dot-product attention followed by linear projection of concatenated results of multiple heads. sentations but needs additional position-encoded parameters per head due to incompatibility with Rotary Position Embedding (RoPE) efficiently (Su et al., 2024b). In order to overcome the limitations of existing approaches, we introduce Tensor Product Attention (TPA), as illustrated in Figure 1, novel architecture that uses higher-order tensors to factorize queries (Q), keys (K), and values (V) during attention computation. By dynamically factorizing activations rather than static weights (e.g., LoRA), TPA constructs low-rank, contextual representations that substantially reduce KV cache memory usage with improved representational capacity. In practice, TPA can reduce the memory overhead by an order of magnitude compared to standard multi-head attention (MHA) with lower pretraining validation loss (perplexity) and improved downstream performance. key advantage of TPA is its native compatibility with rotary positional embeddings (RoPE) (Su et al., 2024b), enabling straightforward drop-in replacement for multi-head attention (MHA) layers in modern LLM architectures such as LLaMA (Touvron et al., 2023) and Gemma (Team et al., 2024). Our primary contributions are summarized as follows: We propose Tensor Product Attention (TPA), mechanism that factorizes Q, K, and activations using contextual tensor-decompositions to achieve 10 or more reduction in inference-time KV cache size relative to standard attention mechanism (Vaswani et al., 2017) with improved performance compared to previous methods such as MHA, MQA, GQA, and MLA. In addition, we unify existing attention mechanisms by revealing that MHA, MQA, and GQA all arise naturally as non-contextual variants of TPA. We propose Tensor ProducT ATTenTion Transformer (T6), new TPA-based model architecture for sequence modeling. On language modeling experiments, T6 consistently improves validation perplexity and downstream evaluation performance with reduced KV cache size. We show TPA integrates seamlessly with RoPE (Su et al., 2024b), facilitating easy adoption in popular foundation model architectures such as LLaMA and Gemma. 2 (a) Training Loss (b) Validation Loss Figure 2: Training loss and validation loss of pretraining large-size (773M) models with different attention mechanisms on the FineWeb-Edu-100B dataset."
        },
        {
            "title": "2 Background",
            "content": "In this section, we review several classical forms of attention: Scaled Dot-Product Attention, MultiHead Attention (MHA) (Vaswani et al., 2017), Multi-Query Attention (MQA) (Shazeer, 2019), and Grouped Query Attention (GQA) (Ainslie et al., 2023), as well as Rotary Position Embedding (RoPE, Su et al. (2024b)). We also introduce recent method called Multi-head Latent Attention (MLA) used in DeepSeek-V2 (Liu et al., 2024a) and DeepSeek-V3 (Liu et al., 2024b). Notations. We use bold uppercase letters (e.g., X, Q) for matrices, bold lowercase (e.g., a, b) for vectors, and italic uppercase (e.g., ) for learnable parameter matrices. We denote by [n] the set {1, . . . , n} for some positive integer n. We use to denote the transpose of vector or matrix. Let dmodel be the embedding dimension, the number of attention heads, dh the dimension per head, xt Rd the input for the t-th token at given attention layer, RT dmodel denotes the input embeddings for tokens, and Q, K, RT hdh denote the queries, keys, and values of heads for tokens. With little abuse of notation, Qi, Ki, Vi RT dh denote the i-th head of queries, keys, and values, and Qt, Kt, Vt Rhdh denote the heads of the query, key, and value for t-th token. Throughout the paper, Q, K, denote projection matrices for queries, keys, and values, respectively. In multi-head attention, each head is associated with its own set of , and each has dimension dmodeldk , where dk is typically set to dh, the dimension of each head.5 Similarly, we have an output projection matrix R(hdh)dmodel. For methods like MQA and GQA, some of these are shared or partially shared across heads, but their shapes remain consistent. We define the tensor product of two vectors as follows: for vectors Rm, Rn, the tensor product of and is: , i , , , a = Rmn, with Cij = aibj, where ai and bj are the i-th and j-th elements of and respectively, and Cij is the (i, j)-th entry of C. We also define the vectorization of matrix Rmn by: vec(C) = Rmn, with din+j = Cij, where din+j is the (i + j)-th element of d. 2.1 Scaled Dot-Product Attention Scaled dot-product attention (Vaswani et al., 2017) determines how to focus on different parts of an input sequence by comparing queries (Q) and keys (K). It produces weighted combination of the 5Often, one sets dh = dmodel, so each head has query/key/value dimension dh. 3 values (V). Formally, the attention output is: Attention(Q, K, V) = Softmax (cid:17) (cid:16) QK dk V, where each of Q, K, is an (n dk) matrix for tokens and key dimension dk. The division by dk stabilizes training by controlling the scale of the inner products. 2.2 Multi-Head Attention (MHA) Multi-Head Attention (MHA) extends scaled dot-product attention by dividing the models internal representation into several heads. Each head learns different projections for queries, keys, and values, allowing the model to attend to different types of information. For each token embedding xt Rdmodel , MHA computes each head as follows: Qt,i = (W ) xt Rdh, Kt,i = (W i ) xt Rdh , Vt,i = (W ) xt Rdh, headi = Attention (cid:16) Qi, Ki, Vi (cid:17) , , i , where Rdmodeldh are learnable projection matrices for the i-th head, Qi, Ki, Vi RT dh . After computing each heads attention, the outputs are concatenated and mapped back to the original dimension via another matrix Rhdhdmodel: (cid:1) O. MHA(Q, K, V) = Concat(cid:0)head1, . . . , headh MHA can capture rich set of dependencies while each head focuses on different subspaces. 2.3 Multi-Query Attention (MQA) Multi-Query Attention (MQA) (Shazeer, 2019) significantly reduces memory usage by sharing keys and values across heads, while still preserving unique query projections. For sequence of embeddings RT dmodel , Qi = XW , Kshared = XW shared, Vshared = XW shared. Hence, each head only has distinct query Qi RT dk , but shares the same key Kshared RT dk and value Vshared RT dk . In practice, this means: i Rdmodeldk , shared dmodeldk . shared, The resulting MQA operation is: MQA(X) = Concat (cid:16) head1, . . . , headh (cid:17) O, where headi = Attention(cid:0)Qi, Kshared, Vshared By sharing these key and value projections, MQA cuts down on memory usage (especially for the key-value cache in autoregressive inference) but loses some expressivity since all heads must rely on the same key/value representations. (cid:1). 2.4 Grouped Query Attention (GQA) Grouped Query Attention (GQA) (Ainslie et al., 2023) generalizes MHA and MQA by grouping heads. Specifically, we partition the total heads into groups. Each group has single set of keys and values, but each individual head within that group still retains its own query projection. Formally, if g(i) maps head [h] to its group index [G], then: Kg(i) = g(i), Vg(i) = g(i), Qi = i , and headi = Attention Rdmodeldk for each group g, and Again, complete output is again concatenation of all heads: , Qi, Kg(i), Vg(i) (cid:17) . (cid:16) (cid:16) GQA(X) = Concat head1, . . . , headh (cid:17) O. dmodeldk for each head i. The By adjusting between 1 and h, GQA can interpolate between sharing all key/value projections across heads (i.e., MQA) and having one set of projections per head (i.e., MHA). 4 2.5 Rotary Position Embedding (RoPE) Many recent LLMs use rotary position embedding (RoPE; Su et al., 2024b) to encode positional information in the query/key vectors. Specifically, let RoPEt denote the rotation operator Tt Rdhdh corresponding to the t-th position. Tt is block-diagonal matrix, which consists of blockdiagonal matrix (cid:18)cos(tθj) sin(tθj) cos(tθj) sin(tθj) (cid:19) , {1, , dh/2}, where {θj} are pre-defined frequency parameters, e.g., θj = 1/100002j/dh. Then we define RoPE (Qt) QtTt, where Qt Rhdh ."
        },
        {
            "title": "A fundamental property is that",
            "content": "which ensures that relative positions (t s) are preserved, thereby providing form of translation invariance in the rotary position embedding. Tt = Tts, (2.1) 2.6 Multi-head Latent Attention (MLA) Below, we briefly outline the Multi-head Latent Attention (MLA) approach used by DeepSeekV2 (Liu et al., 2024a) and DeepSeek-V3 (Liu et al., 2024b). MLA introduces low-rank compression of the keys and values to reduce the Key-Value (KV) caching cost at inference. Concat(cid:0)KC 1 , KC 2 , . . . , KC CKV = XW DKV , (cid:1) = KC = CKV K, (W DKV dmodeldc), (W Rdcdhh), Concat(cid:0)VC 1 , VC 2 , . . . , VC (W Rdcdhh), KR = RoPE(cid:0)XW KR(cid:1), , KR(cid:1), Ki = Concat(cid:0)KC (cid:1) = VC = CKV , (W KR RdmodeldR ), where CKV RT dc is the compressed KV latent (with dc dhh), and RoPE() represents the RoPE transform applied to the separate key embeddings KR of dimension dR . Thus, only CKV and KR need to be cached, reducing KV memory usage while largely preserving performance compared to standard MHA (Vaswani et al., 2017). MLA also compresses the queries, lowering their training-time memory footprint: Concat(cid:0)QC Concat(cid:0)QR 1 , QC 1 , QR 2 , . . . , QC 2 , . . . , QR CQ = XW DQ, (cid:1) = QC = CQW Q, (cid:1) = QR = RoPE(cid:0)CQW QR(cid:1), = Concat(cid:0)QC, QR(cid:1). (W DQ dmodeld (W d c), cdhh), (W QR Rd cdR h), (with Here, CQ RT dhh) is the compressed query latent. As above, each DQ, Q, and QR connects these lower-dimensional query latents back to heads of dimension dh + dR . Given compressed queries, keys, and values, the final attention output for the t-th token is: (cid:16) QiK dh+dR VC , Oi = Softmax = Concat(cid:0)O1, O2, . . . , Oh (cid:17) (cid:1)W O, where R(dhh) dmodel is the output projection. In inference time, CKV and KR can be cached to accelerate decoding. In detail, when RoPE is t,iks,i (where qt,i, ks,i Rd) of the i-th head between t-th and s-th ignored, the inner product tokens can be calculated using the hidden state xt Rdmodel for t-th token and the cached latent state Rdc for s-th token: cKV t,iks,i = [(W q )xt][(W t [W DQ )(W DQ Q )]cKV (W )cKV ] = (2.2) , i 5 (W i Q is the i-th head of the original weight, and [W DQ where () )] can be computed previously for faster decoding. However, this process fails when RoPE is considered according to Su (2024). Since RoPE can be considered as multiplication with block-diagonal matrix Tt Rdhdh (see Section 2.5), with the property (2.1) that TtT )(W DQ t,iks,i = [Tt Tts(W = Different from (2.2), acceleration by pre-computing [W DQ )] fails since it Tts(W varies for different (t, s) position pairs. Therefore, MLA adds the additional kR part with relatively smaller size for RoPE compatibility. In Section 3.2, we will show that TPA addresses the issue of RoPE-incompatibility by applying tensor product. = Tts, then )xt][Ts )]cKV . U U (W i [W DQ (W )cKV (2.3) ] i"
        },
        {
            "title": "3 Tensor Product Attention",
            "content": "In this section, we provide detailed description of our proposed Tensor Product Attention (TPA), which allows contextual low-rank factorization for queries, keys, and values. First, we explain how TPA factorizes queries, keys, and values with explicit tensor shapes. Next, we describe how TPA can be integrated into the multi-head attention framework and how it reduces memory consumption in KV caching at inference time. Finally, we show how RoPE can seamlessly integrate with TPA (including pre-rotated variant). 3.1 Tensor Factorization of Queries, Keys, and Values Let xt Rdmodel for = 1, . . . , be the hidden-state vector corresponding to the t-th token in sequence of length . typical multi-head attention block has heads, each of dimension dh, satisfying dmodel = dh. Standard attention projects the entire sequence into three tensors, Q, K, RT hdh, where Qt, Kt, Vt Rhdh denote the slices for the t-th token. Contextual Factorization (CF). Instead of forming each heads query, key, or value via single linear map, TPA factorizes each Qt, Kt, Vt into sum of (contextual) tensor products whose ranks are Rq, Rk, and Rv, respectively and may differ. Specifically, for each token t, with small abuse of notation, we define: Qt = Kt = Vt = 1 RQ 1 RK 1 RV RQ (cid:88) r=1 RK(cid:88) r=1 RV(cid:88) (xt) bQ aQ (xt), (xt) Rh, bQ aQ (xt) Rdh, (3.1) (xt) bK aK (xt), (xt) Rh, bK aK (xt) Rdh , (3.2) (xt) bV aV (xt), (xt) Rh, bV aV (xt) Rdh. (3.3) r=1 (xt) : Rh Rdh Rhdh adds up to form the Hence, for queries, each tensor product aQ query slice Qt Rhdh . Similarly, analogous definitions apply to key slice Kt and value slice Vt. Latent Factor Maps. Each factor in the tensor product depends on the tokens hidden state xt. For example, for queries, we can write: (xt) bQ (xt) = aQ aQ xt Rh, bQ (xt) = bQ xt Rdh , and similarly for keys and values. One often merges the rank index into single output dimension. For instance, for queries: aQ(xt) = aQ xt RRqh, bQ(xt) = bQ which are then reshaped into AQ(xt) RRqh and BQ(xt) RRqdh . Summing over Rq and scaled by 1 Rq xt RRqdh , yields Qt = 1 RQ AQ(xt) BQ(xt) Rhdh. 6 Repeating for all tokens reconstitutes RT hdh. Similar procedures can be applied to obtain and with ranks Rk and Rv, respectively. Scaled Dot-Product Attention. Once Q, K, are factorized, multi-head attention proceeds as in standard Transformers. For each head {1, . . . , h}: headi = Softmax (cid:16) 1 dh Qi (Ki)(cid:17) Vi, (3.4) where Qi, Ki, Vi RT dh are the slices along the head dimension. Concatenating these heads along the last dimension yields an RT (hdh) tensor, which is projected back to RT dmodel by an output weight matrix R(hdh)dmodel : TPA(Q, K, V) = Concat(cid:0)head1, . . . , headh (cid:1)W O. (3.5) Parameter Initialization. We initialize the weight matrices aQ , bK , bV using Xavier initialization (Glorot & Bengio, 2010). Specifically, each entry of the weight mar trix is drawn from uniform distribution with bounds [(cid:112)6/(nin + nout), (cid:112)6/(nin + nout)], where nin and nout are the input and output dimensions of the respective weight matrices. This initialization strategy helps maintain the variance of activations and gradients across the network. , aK , bQ , aV r 3.2 RoPE Compatibility and Acceleration In typical workflow of adding RoPE to standard multi-head attention, one first computes Qt, Ks Rhdh of the t-th token and s-th token and then applies: Qt (cid:55) (cid:102)Qt = RoPEt(Qt), Ks (cid:55) (cid:102)Ks = RoPEs(Ks). Direct Integration. useful optimization is to integrate RoPE directly into the TPA factorization. For example, one can pre-rotate the token-dimension factors: (cid:101)BK(xt) RoPEt (cid:0)BK(xt)(cid:1), (3.6) yielding pre-rotated key representation: RK(cid:88) aK (r)(xt) RoPEt (cid:101)Kt = 1 RK r=1 (cid:0)bK (s)(xt)(cid:1) = 1 RK AK(xt) RoPEt (cid:0)BK(xt)(cid:1). Thus, each Kt is already rotated before caching, removing the need for explicit rotation at the decoding time and accelerating autoregressive inference. Depending on hardware and performance requirements, one can also adopt different RoPE integration approaches for training and inference. Theorem 1 (RoPEs Compatibility with TPA). Let Qt be factorized by TPA as Qt = 1 RQ AQ(xt) BQ(xt) Rhdh, where AQ(xt) RRQh and BQ(xt) RRQdh. Then we have: RoPE(Qt) = 1 RQ AQ(xt) (cid:101)BQ(xt), where (cid:101)BQ(xt) = RoPEt (cid:0)BQ(xt)(cid:1). (3.7) In addition, assume Qt and Ks are factorized by TPA and then rotated by RoPEt, RoPEs. Let (cid:101)Qt = RoPEt(Qt) and (cid:101)Ks = RoPEs(Ks). Then we have = (cid:101)Qt (cid:101)K , Focusing on individual heads i, the above matrix equality implies: RoPEts(Qt)K RoPEts (cid:0)qt,i (cid:1) ks,i = (cid:101)q t,i (cid:101)ks,i. where qt,i Rdh is the i-th query head of t-th token, and ks,i Rdh is the j-th key head of s-th token, and (cid:101)qt,i = RoPE(qt,i) = Ttqt,i Rdh , (cid:101)ks,i = RoPE(ks,i) = Tsks,i Rdh . Theorem 1 indicates that TPA does not break RoPEs relative translational property. We prove Theorem 1 in Appendix A. In short, RoPEt acts as block-diagonal orthogonal transform (i.e., matrix Tt) on BQ(xt). Consequently, AQ(xt) remains unchanged, while each column of BQ(xt) is rotated appropriately, preserving the TPA structure. 7 3.3 KV Caching and Memory Reduction In autoregressive decoding, standard attention caches Kt, Vt Rhdh for each past token t. This accumulates to RT hdh for keys and RT hdh for values, i.e., 2 dh total. TPA Factorized KV Caching. Instead of storing the full Kt and Vt, TPA stores only their factorized ranks. Specifically, we keep AK(xt), (cid:101)BK(xt) and AV (xt), BV (xt), where AK(xt) RRK h, (cid:101)BK(xt) RRK dh , AV (xt) RRV h, BV (xt) RRV dh . Hence, the memory cost per token is RK(h + dh) (cid:124) (cid:123)(cid:122) (cid:125) for + RV (h + dh) (cid:125) (cid:124) (cid:123)(cid:122) for = ( RK + RV ) (cid:0)h + dh (cid:1). Compared to the standard caching cost of 2 dh, the ratio is: (RK + RV ) (h + dh) 2 dh . For large and dh (typically dh = 64 or 128), setting RK, RV dh (e.g., 1 or 2) often yields 10 or more reduction. Table 1: Comparison of different attention mechanisms. Here, RQ, RK, and RV denote the ranks for queries, keys, and values in TPA, respectively. Variants of TPA, such as TPA (KVonly), TPA (Non-contextual A), and TPA (Non-contextual B), are detailed in Section 3.5. For MLA, dR and dh are the dimensions for RoPE and non-RoPE parts; and dc are the dimensions of compressed vectors for query and key-value, respectively. METHOD KV CACHE # PARAMETERS # QUERY HEADS # KV HEADS MHA MQA GQA MLA 2hdh 2dh 2gdh dc + dR model 4d2 (2 + 2/h)d2 (2 + 2g/h)d2 model model c(dmodel + hdh + hdR ) +dmodeldR + dc(dmodel + 2hdh) TPA TPA (KVonly) TPA (Non-contextual A) TPA (Non-contextual B) (RK + RV )(h + dh) (RK + RV )(h + dh) (RK + RV )dh (RK + RV )h dmodel(RQ + RK + RV )(h + dh) + dmodel hdh dmodel(RK + RV )(h + dh) + 2dmodel hdh (RQ + RK + RV )(dmodeldh + h) + dmodel hdh (RQ + RK + RV )(dmodelh + dh) + dmodel hdh 3.4 Unifying MHA, MQA, and GQA as Non-contextual TPA 3.4.1 MHA as Non-contextual TPA h h 1 h Standard multi-head attention (MHA) can be viewed as specific instance of TPA in which: 1) the rank is set equal to the number of heads; 2) the head dimension factor is non-contextual (i.e., independent of the t-th token embedding xt Rdmodel); 3) the token dimension factor is linear function of xt. To match MHA with TPA, let RQ = RK = RV = h. Focusing on Qt: (a) Non-contextual head factors. Define aQ = RQei Rh, so that ei corresponds to the i-th head of Qt. (ei Rh is the i-th standard basis vector), (b) Contextual token factors. Define (xt) = (W bQ )xt Rdh , (3.8) (3.9) where dent on xt. Rdmodeldh is the per-head query projection defined before, hence bQ (xt) depen8 Substituting (3.8)(3.9) into (3.1) gives: Qt = (cid:88) (cid:104) i=1 ei (cid:0)(W ) xt (cid:1)(cid:105) Rhdh . (3.10) Each term ei (cid:0)(W MHA form of Qt. Analogous constructions hold for Kt and Vt using non-contextual, full-rank variant of TPA. (cid:1) in (3.10) contributes only to the i-th row, reconstituting the usual . Thus, MHA is )xt , TPA with Non-contextual A. More broadly, TPA can use non-contextual head-dimension factors aQ , aK (xt) to remain context-dependent. Then, for keys: Rh (i.e., independent of xt), while allowing bQ (xt), bK (xt), bV , aV Kt ="
        },
        {
            "title": "1\nRK",
            "content": "RK(cid:88) r=1 bK aK (xt), and similarly for queries/values. This reduces per-token computations and can be effective when head-dimension relationships are relatively stable across all tokens. MQA and GQA as Non-Contextual TPA. Multi-Query Attention (MQA) (Shazeer, 2019) and Grouped Query Attention (GQA) (Ainslie et al., 2023) also emerge naturally from TPA by restricting the head-dimension factors to be non-contextual and low-rank: MQA as Rank-1 TPA. In MQA, all heads share single set of keys/values, corresponding to RK = RV = 1 along the head dimension. Concretely, Kt = (1, . . . , 1) bK(xt), Vt = (1, . . . , 1) bV (xt), forces every head to use the same Kt, Vt. Each head retains distinct query projection, matching the MQA design. GQA as Grouped Rank-1 TPA. GQA partitions heads into groups, each sharing keys/values within that group. In TPA form, each group has dedicated non-contextual factor pair aK Rh, which acts as mask for the heads in that group. Varying from 1 to interpolates from MQA to standard MHA. , aV Hence, by constraining TPAs head-dimension factors to be constant masks (one for MQA; multiple for GQA), these popular variants are recovered as special cases. 3.5 Other Variants of TPA TPA with Non-contextual B. Conversely, one may fix the token-dimension factors bQ Rdh as learned parameters, while allowing aQ , bK (xt) to adapt to xt. For keys: (xt), aV , bV (xt), aK RK(cid:88) Kt = 1 RK r=1 (xt) bK aK , and similarly for keys/values. This arrangement is effective if the token-dimension structure remains mostly uniform across the sequence, while the head-dimension factors capture context. TPA KV Only. One can preserve standard query mapping, Qt = xt Rhdh , and factorize only the keys and values. This leaves the query projection as the original linear transformation while reducing memory usage via factorized KV caching. TPA KV with Shared B. Another variant is to share the token-dimension factors of keys and values: (xt) = bV bK (xt), lowering parameter counts and the KV cache footprint. While it constrains and to be formed from the same token basis, it can still perform well and provide additional memory savings. 9 Nonlinear Head Factors. Rather than applying purely linear mappings to the head-dimension factors aQ , one may introduce element-wise nonlinearities such as σ() or softmax(). This effectively yields Mixture of Heads Attention (MoH Attention), where each component becomes learned mixture weight modulated by the nonlinearity. , aK , aV Discussion. These variants illustrate TPAs versatility in balancing memory cost, computational overhead, and representation power. By choosing which dimensions (heads or tokens) remain contextual and adjusting ranks (RQ, RK, RV ), TPA unifies multiple existing attention mechanisms such as MHA, MQA, and GQAunder one framework, while potentially reducing the KV cache size by an order of magnitude during autoregressive inference. 3.6 Model Architectures We propose new architecture called Tensor ProducT ATTenTion Transformer (T6), which uses our Tensor Product Attention (TPA) in place of standard MHA (multi-head attention) or GQA (grouped-query attention). Building upon the query, key, and value tensors Q, K, RN hdh defined in Section 3.1, T6 utilize the overall architecture of LLaMA (Touvron et al., 2023) while changing the self-attention block to our TPA-based version. The feed-forward network (FFN) adopts SwiGLU layer, as in (Shazeer, 2020; Touvron et al., 2023). TPA QKV Factorization. Let each tokens hidden-state vector be xt Rdmodel, and we folRT hdh, low Section 3.1 to project the entire sequence into three tensors Q, K, where Qt, Kt, Vt Rhdh denote the slices for the t-th token. The factor components (xt), bQ aQ (xt) are produced by linear transformations on xt. For instance, letting aQ Rdhdmodel, we have: (xt) = bQ Rhdmodel and bQ (xt) = aQ aQ xt, bQ (xt), bK (xt), aK (xt), bV (xt), aV xt. In practice, we merge all ranks into single dimension of the output, reshape, and sum over rank indices; see Section 3.1 for details. The factorization for and follows the same pattern. Rotary Positional Embedding (RoPE). As discussed in Section 3.2, RoPE (Su et al., 2024b) is applied to the and K. Within TPA, we pre-rotate the factor bQ (xs) directly, so that each Ks is already rotated prior to caching, see (3.6) and Theorem 1. (xt) and bK Attention Step and Output Projection. Once we have Q, K, factorized per token with RoPE applied on and K, the attention step proceeds for each head {1, . . . , h} using (3.4). Finally, concatenating these heads and then projecting them back using an output weight matrix gives the final attention result, as shown in (3.5). SwiGLU Feed-Forward Network. Following Shazeer (2020); Touvron et al. (2023), our T6 uses SwiGLU-based Feed-Forward Network (FFN): FFN(x) = (cid:2)σ(x W1) (x W2)(cid:3) W3, where σ is the SiLU (a.k.a., swish) nonlinearity, is element-wise product, and W1, W2, W3 are learnable parameters. Note that other activation functions can also be used. Overall T6 Block Structure. Putting everything together, one T6 block consists of: + TPA(cid:0)RMSNorm(x)(cid:1), + SwiGLU-FFN(cid:0)RMSNorm(x)(cid:1). We place norm layers (e.g., RMSNorm) before each sub-layer. Stacking such blocks yields T6 model architecture with layers."
        },
        {
            "title": "4 Experiments",
            "content": "4.1 Language Modeling Tasks All experiments reported in this paper are implemented on the nanoGPT code base (Karpathy, 2022), using the FineWeb-Edu 100B dataset (Lozhkov et al., 2024). The dataset contains 100 billion tokens for training and 0.1 billion tokens for validation. We compare T6 against the baseline Llama architecture (Touvron et al., 2023) with SwiGLU activation (Shazeer, 2020) and RoPE embeddings (Su 10 et al., 2024a), as well as Llama variants that replace Multi-Head Attention (MHA; Vaswani et al., 2017) with Multi-Query Attention (MQA; Shazeer, 2019), Grouped Query Attention (GQA; Ainslie et al., 2023), or Multi-head Latent Attention (MLA; Liu et al., 2024a). In our experiments, the number of heads is adjusted for each attention mechanism to ensure that all attention mechanisms have the same number of parameters as the standard Multi-Head Attention (MHA), which has 4d2 model parameters per attention layer. We train models at four scales: small (124M parameters), medium (353M), and large (773M). Details on architecture hyperparameters and training hardware appear in Appendix B.1. Training Setup. We follow the nanoGPT training configuration. In particular, we use the AdamW (Loshchilov, 2017) optimizer with (β1, β2) = (0.9, 0.95), weight decay of 0.1, and gradient clipping at 1.0. We follow the same setting as nanoGPT that the learning rate is managed by cosine annealing scheduler (Loshchilov & Hutter, 2016) with 2,000 warmup steps and (total) global batch size of 480. For the small, medium, and large models, we set maximum learning rates of 6 104, 3 104, and 2 104 (respectively), and minimum learning rates of 3 105, 3 105, and 1 105 (respectively). Training & Validation Curves. Figures 2 and 3 compare training and validation loss curves for the large (773M) and medium (353M) models on FineWeb-Edu-100B. Overall, TPA (red curves) and its simpler variant TPA-KVonly (pink curves) converge as fast as or faster than the baselines (MHA, MQA, GQA, MLA) while also achieving visibly lower final losses. For instance, in Figure 2(b), TPA and TPA-KVonly remain below the MHA baseline in terms of validation loss at nearly all training stages. Meanwhile, Multi-Head Latent Attention (MLA) (Liu et al., 2024a) (blue curves) generally trains more slowly and yields higher losses. Validation Perplexity. Figure 4 shows the validation perplexities of the mediumand large-scale models. Mirroring the loss curves, TPA and TPA-KVonly steadily outperform MHA, MQA, GQA, and MLA over the course of training. By the end of pretraining (around 49B tokens), TPA-based approaches achieve the lowest perplexities in most configurations. Downstream Evaluation. We evaluate zero-shot and two-shot performance on standard benchmarks, including ARC (Yadav et al., 2019), BoolQ (Clark et al., 2019), HellaSwag (Zellers et al., 2019), OBQA (Mihaylov et al., 2018), PIQA (Bisk et al., 2020), WinoGrande (Sakaguchi et al., 2020) and MMLU (Hendrycks et al., 2021), using the lm-evaluation-harness codebase (Gao et al., 2024). For ARC-E, ARC-C, HellaSwag, OBQA, PIQA, and SciQ, we report accuracy norm; for other tasks, we report standard accuracy. Tables 89 in the appendix present results for small models; Tables 23 for medium models; Tables 45 for large models; For the medium-size (353M) models (Tables 23), TPA generally ties or outperforms all competing methods, achieving, for example, an average of 51.41% in zero-shot mode versus MHAs 50.11%, MQAs 50.44%, and MLAs 48.96%. When given two-shot prompts, TPA again leads with 53.12% average accuracy. similar trend appears for the large-size (773M) models (Tables 45), where TPA-KVonly attains the highest average (53.52% zero-shot, 55.33% two-shot), closely followed by full TPA. Our experiments confirm that TPA consistently matches or exceeds the performance of established attention mechanisms (MHA, MQA, GQA, MLA) across medium and large model scales. The fully factorized TPA excels on mid-scale models, while TPA-KVonly can rival or surpass it at larger scales. In both cases, factorizing the attention activations shrinks autoregressive KV cache requirements by up to 510, thus enabling much longer context windows under fixed memory budgets. In summary, tensor product attention provides flexible, memory-efficient alternative to standard multi-head attention, advancing the scalability of modern language models."
        },
        {
            "title": "5 Related Work",
            "content": "Transformers and Attention. As sequence-to-sequence architecture Transformer (Vaswani et al., 2017) introduced Multi-Head Attention (MHA), enabling more effective capture of long-range dependencies. Subsequent work has explored variety of attention mechanisms aimed at improving scalability and efficiency, including sparse patterns (Child et al., 2019; Shi et al., 2023; Han et al., 2024; Liang et al., 2024a; Li et al., 2024; Liang et al., 2024b), kernel-based projections (Choromanski et al., 2021), and linearized transformers (Tsai et al., 2019; Katharopoulos et al., 2020; Schlag 11 (a) Training Loss (b) Validation Loss Figure 3: The training loss and validation loss of medium-size (353M) models with different attention mechanisms on the FineWeb-Edu 100B dataset. (a) Validation Perplexity of Medium Models (b) Validation Perplexity of Large Models Figure 4: The validation perplexity of medium-size (353M) models and large-size (773M) models with different attention mechanisms on the FineWeb-Edu 100B dataset. Table 2: The evaluation results of medium models with different attention mechanisms pretrained using the FineWeb-Edu 100B dataset (0-shot with lm-evaluation-harness). The best scores in each column are bolded. Abbreviations: HellaSw. = HellaSwag, W.G. = WinoGrande. Method ARC-E ARC-C BoolQ HellaSw. OBQA PIQA W.G. MMLU SciQ Avg. MHA MQA GQA MLA TPA-KVonly TPA 56.52 55.68 54.88 55.30 57.11 59.30 29.27 28.24 29.61 29. 30.03 31.91 58.84 60.86 56.36 58.96 61.25 60.98 44.06 44.17 43.77 41.92 44.83 45.57 35.00 35.20 35.20 35. 34.60 34.60 68.44 68.66 68.82 67.25 69.04 69.48 51.07 52.72 52.57 51.78 54.54 53.91 25.35 25.14 25.41 25. 23.35 24.93 76.40 72.90 74.80 75.60 74.60 77.20 49.44 49.29 49.05 48.96 49.93 50.88 et al., 2021; Zhang et al., 2023b; Sun et al., 2023; Zhang et al., 2024). To decrease memory usage and circumvent the limitation of memory bandwidth in training, Shazeer (2019) proposed MultiQuery Attention (MQA) where multiple query heads share the same key head and value head. To tackle with the issue of quality degradation and instability in training, Grouped-Query Attention (GQA) (Ainslie et al., 2023) divides queries into several groups, and each group of queries shares single key head and value head. Recently, DeepSeek-V2 (Liu et al., 2024a) applied multihead latent attention (MLA) to achieve better performance than MHA while reducing KV cache in inference time by sharing the same low-rank representation of key and value. In comparison to the approaches above, TPA applied low-rank tensor product to compute the queries, keys, and values where the 12 Table 3: The evaluation results of medium models with different attention mechanisms pre-trained using the FineWeb-Edu 100B dataset (2-shot with lm-evaluation-harness). The best scores in each column are bolded. Abbreviations: HellaSw. = HellaSwag, W.G. = WinoGrande. Method ARC-E ARC-C BoolQ HellaSw. OBQA PIQA W.G. MMLU SciQ Avg. MHA MQA GQA MLA TPA-KVonly TPA 64.44 64.27 61.70 62.75 65.99 66.54 32.85 32.94 32.17 30.80 33.70 34. 59.05 57.71 52.81 59.17 57.49 58.96 44.18 44.36 43.99 42.02 44.47 45.35 33.20 31.80 33.80 34.80 34.20 33. 68.72 68.01 68.50 67.08 69.53 69.21 50.12 51.70 53.35 52.41 53.28 53.99 26.01 25.99 24.44 26.11 24.23 24. 87.40 86.00 86.40 84.80 86.50 91.30 49.44 49.29 50.80 51.10 49.93 53.04 Table 4: The evaluation results of large models with different attention mechanisms pre-trained using the FineWeb-Edu 100B dataset (0-shot with lm-evaluation-harness). The best scores in each column are bolded. Abbreviations: HellaSw. = HellaSwag, W.G. = WinoGrande. Method ARC-E ARC-C BoolQ HellaSw. OBQA PIQA W.G. MMLU SciQ Avg. MHA MQA GQA MLA TPA-KVonly TPA 59.93 60.73 61.66 60. 63.26 63.22 33.62 33.62 34.30 31.57 34.13 35.58 61.93 57.34 58.72 61.74 61.96 60.03 50.63 50.09 49.85 48. 50.66 51.26 36.00 37.00 38.40 35.40 37.20 36.80 71.06 69.97 71.16 69.59 72.09 71.44 55.41 55.49 53.75 55. 55.25 55.56 22.87 25.30 25.23 26.39 26.06 24.77 81.20 79.60 77.60 76.70 81.10 79.60 52.52 52.13 52.30 51. 53.52 53.10 Table 5: The evaluation results of large models with different attention mechanisms pre-trained using the FineWeb-Edu 100B dataset (2-shot with lm-evaluation-harness). The best scores in each column are bolded. Abbreviations: HellaSwag = HellaSwag, WG = WinoGrande. Method ARC-E ARC-C BoolQ HellaSwag OBQA PIQA WG MMLU SciQ Avg. MHA MQA GQA MLA TPA-KVonly TPA 67.85 68.86 69.15 68.56 71.34 70.41 36.35 36.09 36.09 35.41 37.71 37. 59.82 53.79 58.84 60.12 59.76 60.06 50.22 50.50 50.29 49.18 51.10 51.30 35.00 37.00 36.20 38.00 36.00 34. 70.67 70.89 70.73 69.21 71.49 71.06 53.35 54.70 54.22 55.25 54.62 54.54 23.92 25.01 26.08 25.29 25.83 25. 91.10 88.00 90.00 88.20 90.10 90.30 54.25 53.87 54.62 54.36 55.33 55.02 cached representations of keys and values are much smaller than those in MHA, achieving better reduction on memory assumption of KV cache in inference time. Low-Rank Factorizations. Low-rank approximations have been applied to compress model parameters and reduce complexity including LoRA (Hu et al., 2022), which factorizes weight updates during fine-tuning, and its derivatives for other training scenarios such as efficient pretraining (ReLoRA (Lialin et al., 2023), MoRA (Jiang et al., 2024)), long-context training (LongLoRA (Chen et al., 2024), SinkLoRA (Zhang, 2024)), as well as continual training (InfLoRA (Liang & Li, 2024), GS-LoRA (Zhao et al., 2024), I-LoRA (Ren et al., 2024)). These approaches typically produce static low-rank expansions that do not explicitly depend on the input context. And Malladi et al. (2023); Zeng & Lee (2024) provided theoretical proof of the expressiveness of low-rank approximation. For the initialization of factorization matrices, OLoRA (Buyukakyuz, 2024) applied QR-decomposition of pretrained weight to achieve better performance of language models while LoLDU (Shi et al., 2024) used LDU-decomposition to accelerate training of LoRA. Moreover, AdaLoRA (Zhang et al., 2023a) utilized Singular Value Decomposition (SVD) of the pretrained weight and introduced importance score for each parameter as measurement to achieve dynamic adjustment of rank. TPA, by contrast, constructs Q, K, and as contextually factorized tensors, enabling dynamic adaptation. KV Cache Optimization. During the inference time of Transformers, key and value tensors of the previous tokens are repeatedly computed due to their auto-regressive nature. To enhance efficiency, firstly proposed by Ott et al. (2019), these tensors can be cached in memory for future decoding, referred to as the KV cache. However, the KV cache requires additional memory usage and may add to more latencies due to the bandwidth limitation (Adnan et al., 2024). Therefore, previous studies have explored diverse approaches to mitigate these issues, including KV cache eviction to 13 discard less significant tokens (Zhang et al., 2023c; Xiao et al., 2024; Cai et al., 2024; Adnan et al., 2024), dynamic sparse attention among selected keys and values (Ribar et al., 2024; Tang et al., 2024; Singhania et al., 2024), KV cache offloading to CPU (He & Zhai, 2024; Lee et al., 2024; Sun et al., 2024), as well as quantization of KV cache (Xiao et al., 2023; Liu et al., 2024c; Hooper et al., 2024). Besides these methods, it is also effective to reduce the amount of KV cache for each token, by approaches such as reducing the number of KV heads (Ren et al., 2024; Ainslie et al., 2023), cross-layer KV re-usage (Xiao et al., 2019; Mu et al., 2024; Wu et al., 2024), and low-rank KV representation (Saxena et al., 2024). Different from the methods above, TPA reduces the size of the KV cache by using tensor-decomposed keys and values."
        },
        {
            "title": "6 Conclusion",
            "content": "We introduced Tensor Product Attention (TPA), which factorizes query, key, and value matrices into rank-R tensor products dependent on the tokens hidden state. Storing only the factorized key/value components during autoregressive decoding substantially decreases the kv memory size with improved performance compared with MHA, MQA, GQA, and MLA. The approach is fully compatible with RoPE (and can store pre-rotated keys). Variants of TPA include factorizing only the key/value or sharing basis vectors across tokens. Overall, TPA offers powerful mechanism for compressing KV storage while improving the model performance, thereby enabling longer sequence contexts under constrained memory."
        },
        {
            "title": "References",
            "content": "Muhammad Adnan, Akhil Arunkumar, Gaurav Jain, Prashant Nair, Ilya Soloveychik, and Purushotham Kamath. Keyformer: Kv cache reduction through key tokens selection for efficient generative inference. Proceedings of Machine Learning and Systems, 6:114127, 2024. Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebron, and Sumit Sanghai. GQA: training generalized multi-query transformer models from multi-head checkpoints. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 610, 2023, pp. 48954901. Association for Computational Linguistics, 2023. doi: 10.18653/V1/ 2023.EMNLP-MAIN.298. URL https://doi.org/10.18653/v1/2023.emnlp-main.298. Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. PIQA: reasoning In The Thirty-Fourth AAAI Conference on about physical commonsense in natural language. Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020, pp. 74327439. AAAI Press, 2020. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901, 2020. Sebastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. Sparks of artificial general intelligence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712, 2023. Kerim Buyukakyuz. Olora: Orthonormal low-rank adaptation of large language models. arXiv preprint arXiv:2406.01775, 2024. Zefan Cai, Yichi Zhang, Bofei Gao, Yuliang Liu, Tianyu Liu, Keming Lu, Wayne Xiong, Yue Dong, Baobao Chang, Junjie Hu, et al. Pyramidkv: Dynamic kv cache compression based on pyramidal information funneling. arXiv preprint arXiv:2406.02069, 2024. Yukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian Liu, Song Han, and Jiaya Jia. Longlora: Efficient fine-tuning of long-context large language models. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024, 2024. 14 Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse transformers. arXiv preprint arXiv:1904.10509, 2019. Krzysztof Marcin Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Quincy Davis, Afroz Mohiuddin, Lukasz Kaiser, David Benjamin Belanger, Lucy J. Colwell, and Adrian Weller. Rethinking attention with performers. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021, 2021. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. Palm: Scaling language modeling with pathways. J. Mach. Learn. Res., 24:240:1240:113, 2023. Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. Boolq: Exploring the surprising difficulty of natural yes/no questions. In Jill Burstein, Christy Doran, and Thamar Solorio (eds.), Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), pp. 29242936. Association for Computational Linguistics, 2019. Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noach, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. framework for fewshot language model evaluation, 07 2024. URL https://zenodo.org/records/12608602. Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural networks. In Proceedings of the thirteenth international conference on artificial intelligence and statistics, pp. 249256. JMLR Workshop and Conference Proceedings, 2010. Insu Han, Rajesh Jayaram, Amin Karbasi, Vahab Mirrokni, David P. Woodruff, and Amir Zandieh. In The Twelfth International ConHyperattention: Long-context attention in near-linear time. ference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. URL https://openreview.net/forum?id=Eh0Od2BJIM. Jiaao He and Jidong Zhai. Fastdecode: High-throughput gpu-efficient llm serving using heterogeneous pipelines. arXiv preprint arXiv:2403.11421, 2024. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021, 2021. Coleman Hooper, Sehoon Kim, Hiva Mohammadzadeh, Michael Mahoney, Yakun Sophia Shao, Kurt Keutzer, and Amir Gholami. Kvquant: Towards 10 million context length llm inference with kv cache quantization. arXiv preprint arXiv:2401.18079, 2024. Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, In The Tenth Interand Weizhu Chen. Lora: Low-rank adaptation of large language models. national Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022, 2022. Ting Jiang, Shaohan Huang, Shengyue Luo, Zihan Zhang, Haizhen Huang, Furu Wei, Weiwei Deng, Feng Sun, Qi Zhang, Deqing Wang, et al. Mora: High-rank updating for parameter-efficient finetuning. arXiv preprint arXiv:2405.12130, 2024. Andrej Karpathy. NanoGPT. https://github.com/karpathy/nanoGPT, 2022. Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Francois Fleuret. Transformers are rnns: Fast autoregressive transformers with linear attention. In International conference on machine learning, pp. 51565165. PMLR, 2020. Wonbeom Lee, Jungi Lee, Junghwan Seo, and Jaewoong Sim. {InfiniGen}: Efficient generative inference of large language models with dynamic {KV} cache management. In 18th USENIX Symposium on Operating Systems Design and Implementation (OSDI 24), pp. 155172, 2024. Xiaoyu Li, Yingyu Liang, Zhenmei Shi, and Zhao Song. tighter complexity analysis of sparsegpt. arXiv preprint arXiv:2408.12151, 2024. Vladislav Lialin, Sherin Muckatira, Namrata Shivagunde, and Anna Rumshisky. Relora: Highrank training through low-rank updates. In The Twelfth International Conference on Learning Representations, 2023. Yan-Shuo Liang and Wu-Jun Li. Inflora: Interference-free low-rank adaptation for continual learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2363823647, 2024. Yingyu Liang, Heshan Liu, Zhenmei Shi, Zhao Song, Zhuoyan Xu, and Junze Yin. Conv-basis: new paradigm for efficient attention inference and gradient computation in transformers. arXiv preprint arXiv:2405.05219, 2024a. Yingyu Liang, Jiangxuan Long, Zhenmei Shi, Zhao Song, and Yufa Zhou. Beyond linear approximations: novel pruning approach for attention matrix. arXiv preprint arXiv:2410.11261, 2024b. Aixin Liu, Bei Feng, Bin Wang, Bingxuan Wang, Bo Liu, Chenggang Zhao, Chengqi Dengr, Chong Ruan, Damai Dai, Daya Guo, et al. Deepseek-v2: strong, economical, and efficient mixtureof-experts language model. arXiv preprint arXiv:2405.04434, 2024a. Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024b. Zirui Liu, Jiayi Yuan, Hongye Jin, Shaochen Zhong, Zhaozhuo Xu, Vladimir Braverman, Beidi Chen, and Xia Hu. KIVI: tuning-free asymmetric 2bit quantization for KV cache. In Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024, 2024c. Loshchilov. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm restarts. arXiv preprint arXiv:1608.03983, 2016. Anton Lozhkov, Loubna Ben Allal, Leandro von Werra, and Thomas Wolf. Fineweb-edu: the finest collection of educational content, 2024. URL https://huggingface.co/datasets/ HuggingFaceFW/fineweb-edu. Sadhika Malladi, Alexander Wettig, Dingli Yu, Danqi Chen, and Sanjeev Arora. kernel-based In International Conference on Machine Learning, pp. view of language model fine-tuning. 2361023641. PMLR, 2023. Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can suit of armor conduct electricity? new dataset for open book question answering. In Ellen Riloff, David Chiang, Julia Hockenmaier, and Junichi Tsujii (eds.), Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Brussels, Belgium, October 31 - November 4, 2018, pp. 23812391. Association for Computational Linguistics, 2018. doi: 10.18653/V1/D18-1260. URL https://doi.org/10.18653/v1/d18-1260. 16 Yongyu Mu, Yuzhang Wu, Yuchun Fan, Chenglong Wang, Hengyu Li, Qiaozhi He, Murun Yang, Tong Xiao, and Jingbo Zhu. Cross-layer attention sharing for large language models. arXiv preprint arXiv:2408.01890, 2024. fairseq: fast, extensible toolkit for sequence modeling. Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, In Waleed Amand Michael Auli. mar, Annie Louis, and Nasrin Mostafazadeh (eds.), Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Demonstrations, pp. 4853. Association for Computational Linguistics, 2019. Weijieying Ren, Xinlong Li, Lei Wang, Tianxiang Zhao, and Wei Qin. Analyzing and reducing catastrophic forgetting in parameter efficient tuning. arXiv preprint arXiv:2402.18865, 2024. Luka Ribar, Ivan Chelombiev, Luke Hudlass-Galley, Charlie Blake, Carlo Luschi, and Douglas Orr. Sparq attention: Bandwidth-efficient LLM inference. In Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024, 2024. Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale. In The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020, pp. 87328740. AAAI Press, 2020. Utkarsh Saxena, Gobinda Saha, Sakshi Choudhary, and Kaushik Roy. Eigen attention: Attention in low-rank space for KV cache compression. In Yaser Al-Onaizan, Mohit Bansal, and YunNung Chen (eds.), Findings of the Association for Computational Linguistics: EMNLP 2024, Miami, Florida, USA, November 12-16, 2024, pp. 1533215344. Association for Computational Linguistics, 2024. Imanol Schlag, Kazuki Irie, and Jurgen Schmidhuber. Linear transformers are secretly fast weight programmers. In International Conference on Machine Learning, pp. 93559366. PMLR, 2021. Noam Shazeer. Fast transformer decoding: One write-head is all you need. arXiv preprint arXiv:1911.02150, 2019. Noam Shazeer. Glu variants improve transformer. arXiv preprint arXiv:2002.05202, 2020. Yiming Shi, Jiwei Wei, Yujia Wu, Ran Ran, Chengwei Sun, Shiyuan He, and Yang Yang. Loldu: Low-rank adaptation via lower-diag-upper decomposition for parameter-efficient fine-tuning. arXiv preprint arXiv:2410.13618, 2024. Zhenmei Shi, Jiefeng Chen, Kunyang Li, Jayaram Raghuram, Xi Wu, Yingyu Liang, and Somesh Jha. The trade-off between universality and label efficiency of representations from contrastive In The Eleventh International Conference on Learning Representations, ICLR 2023, learning. Kigali, Rwanda, May 1-5, 2023, 2023. Prajwal Singhania, Siddharth Singh, Shwai He, Soheil Feizi, and Abhinav Bhatele. Loki: Low-rank keys for efficient sparse attention. arXiv preprint arXiv:2406.02542, 2024. Jianlin Su. The extreme pull between cache and effect: From MHA, MQA, GQA to MLA. https: //spaces.ac.cn/archives/10091, May 2024. Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024a. Jianlin Su, Murtadha H. M. Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024b. Hanshi Sun, Li-Wen Chang, Wenlei Bao, Size Zheng, Ningxin Zheng, Xin Liu, Harry Dong, Yuejie Chi, and Beidi Chen. Shadowkv: Kv cache in shadows for high-throughput long-context llm inference. arXiv preprint arXiv:2410.21465, 2024. 17 Yutao Sun, Li Dong, Shaohan Huang, Shuming Ma, Yuqing Xia, Jilong Xue, Jianyong Wang, and Furu Wei. Retentive network: successor to transformer for large language models. arXiv preprint arXiv:2307.08621, 2023. Jiaming Tang, Yilong Zhao, Kan Zhu, Guangxuan Xiao, Baris Kasikci, and Song Han. QUEST: query-aware sparsity for efficient long-context LLM inference. In Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024, 2024. Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivi`ere, Mihir Sanjay Kale, Juliette Love, et al. Gemma: Open models based on gemini research and technology. arXiv preprint arXiv:2403.08295, 2024. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. Yao-Hung Hubert Tsai, Shaojie Bai, Makoto Yamada, Louis-Philippe Morency, and Ruslan Salakhutdinov. Transformer dissection: An unified understanding for transformers attention via the lens of kernel. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 43444353, 2019. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. You Wu, Haoyi Wu, and Kewei Tu. systematic study of cross-layer kv sharing for efficient llm inference. arXiv preprint arXiv:2410.14442, 2024. Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song Han. Smoothquant: In International Accurate and efficient post-training quantization for large language models. Conference on Machine Learning, pp. 3808738099. PMLR, 2023. Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming language models with attention sinks. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024, 2024. Tong Xiao, Yinqiao Li, Jingbo Zhu, Zhengtao Yu, and Tongran Liu. Sharing attention weights for fast transformer. In Sarit Kraus (ed.), Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI 2019, Macao, China, August 10-16, 2019, pp. 5292 5298. ijcai.org, 2019. Vikas Yadav, Steven Bethard, and Mihai Surdeanu. Quick and (not so) dirty: Unsupervised selection of justification sentences for multi-hop question answering. In Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan (eds.), Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019, Hong Kong, China, November 3-7, 2019, pp. 2578 2589. Association for Computational Linguistics, 2019. doi: 10.18653/V1/D19-1260. URL https://doi.org/10.18653/v1/D19-1260. Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can maIn Anna Korhonen, David R. Traum, and Lluıs M`arquez chine really finish your sentence? (eds.), Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28August 2, 2019, Volume 1: Long Papers, pp. 4791 4800. Association for Computational Linguistics, 2019. doi: 10.18653/V1/P19-1472. URL https://doi.org/10.18653/v1/p19-1472. Yuchen Zeng and Kangwook Lee. The expressive power of low-rank adaptation. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024, 2024. Hengyu Zhang. Sinklora: Enhanced efficiency and chat capabilities for long-context large language models. arXiv preprint arXiv:2406.05678, 2024. 18 Michael Zhang, Kush Bhatia, Hermann Kumbong, and Christopher Re. The hedgehog & the porcupine: Expressive linear attentions with softmax mimicry. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024, 2024. Qingru Zhang, Minshuo Chen, Alexander Bukharin, Pengcheng He, Yu Cheng, Weizhu Chen, and Tuo Zhao. Adaptive budget allocation for parameter-efficient fine-tuning. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023a. Ruiqi Zhang, Spencer Frei, and Peter Bartlett. Trained transformers learn linear models in-context. arXiv preprint arXiv:2306.09927, 2023b. Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin Zheng, Ruisi Cai, Zhao Song, Yuandong Tian, Christopher Re, Clark Barrett, et al. H2o: Heavy-hitter oracle for efficient generative inference of large language models. Advances in Neural Information Processing Systems, 36:3466134710, 2023c. Hongbo Zhao, Bolin Ni, Junsong Fan, Yuxi Wang, Yuntao Chen, Gaofeng Meng, and Zhaoxiang In Proceedings of the IEEE/CVF Zhang. Continual forgetting for pre-trained vision models. Conference on Computer Vision and Pattern Recognition, pp. 2863128642, 2024."
        },
        {
            "title": "Appendix",
            "content": "A Proofs of Theorems More on Experiments B.1 Experimental Settings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.2 Additional Experimental Results . . . . . . . . . . . . . . . . . . . . . . . . . . . B.3 Ablation Studies on Learning Rates . . . . . . . . . . . . . . . . . . . . . . . . . 21 22 22"
        },
        {
            "title": "A Proofs of Theorems",
            "content": "Proof of Theorem 1. Proof. Because RoPE is linear orthogonal transform, we can write (cid:101)Qt = Qt Tt ="
        },
        {
            "title": "1\nRQ",
            "content": "(cid:0)AQ(xt) BQ(xt)(cid:1) Tt ="
        },
        {
            "title": "1\nRQ",
            "content": "AQ(xt)(cid:0)BQ(xt) Tt (cid:1), where Tt is the block-diagonal matrix encoding RoPE. This allows us to define thereby obtaining (cid:101)BQ(xt) = BQ(xt) Tt, RoPE(Qt) ="
        },
        {
            "title": "1\nRQ",
            "content": "AQ(xt) (cid:101)BQ(xt). Similarly, for the key tensor Ks, we have (cid:101)Ks = Ks Ts ="
        },
        {
            "title": "1\nRK",
            "content": "(cid:0)AK(xs) BK(xs)(cid:1) Ts ="
        },
        {
            "title": "1\nRK",
            "content": "AK(xs)(cid:0)BK(xs) Ts (cid:1), which defines and thus (cid:101)BK(xs) = BK(xs) Ts, RoPE(Ks) = 1 RK AK(xs) (cid:101)BK(xs). Now, consider the product of the rotated queries and keys: (cid:101)Qt (cid:101)K = = 1 RQRK 1 RQRK (cid:16) AQ(xt) (cid:101)BQ(xt) (cid:17) (cid:16) AK(xs) (cid:101)BK(xs) (cid:17) AQ(xt) (cid:101)BQ(xt) (cid:101)BK(xs)AK(xs), Since Tt and Ts encode positional rotations, the product TtT Tts. Therefore, we can express the above as corresponds to relative rotation (cid:101)Qt (cid:101)K = = = = 1 RQRK 1 RQRK 1 RQRK (cid:18) 1 RQ AQ(xt) (cid:0)BQ(xt)TtT BK(xs)(cid:1) AK(xs) AQ(xt) (cid:0)BQ(xt)TtsBK(xs)(cid:1) AK(xs) AQ(xt) (BQ(xt)Tts) (cid:0)BK(xs)AK(xs)(cid:1) AQ(xt)BQ(xt)Tts (cid:19) (cid:18) 1 RK AK(xs)BK(xs) (cid:19) , This shows that RoPEts(Qt)K = (cid:101)Qt (cid:101)K , Focusing on individual heads i, the above matrix equality implies: RoPEts(qt,i)ks,i = (cid:101)q t,i(cid:101)ks,i, where (cid:101)qt,i = RoPE(qt,i) = Ttqt,i Rdh , (cid:101)ks,i = RoPE(ks,i) = Tsks,i Rdh . This equality confirms that the relative positional encoding between queries and keys is preserved under TPAs factorization and RoPEs rotation. Thus, TPA maintains compatibility with RoPE. This completes the proof of Theorem 1."
        },
        {
            "title": "B More on Experiments",
            "content": "B.1 Experimental Settings We list the main architecture hyper-parameters and training devices in Table 6. We fix dh = 64 for all the models. Moreover, we fix the number of KV heads with 2 for GQA models; dR = 32 for MLA models; and Rk = Rv = 2, Rq = 6 for TPA and TPA-KV only models. Other hyperparameters are listed in Table 7. Table 6: The architecture hyper-parameters and training devices of models. Abbreviations: BS. = Batch Size, GAS. = Gradient Accumulation Steps. MODEL SIZE #PARAM DEVICES MICRO BS. GAS. #LAYER dMODEL SMALL MEDIUM LARGE 124M 353M 772M 4 A100 GPUS 8 A100 GPUS 8 A100 GPUS 24 20 15 5 3 4 12 24 36 768 1024 1280 Table 7: The architecture hyper-parameters for different models. MODEL SIZE SMALL MEDIUM LARGE (MHA) (MQA) (GQA) nh (MLA) (TPA-KVONLY) (TPA) dc (MLA) (MLA) 12 23 22 12 22 34 256 512 16 31 30 23 29 20 39 38 34 37 61 512 1024 512 1024 B.2 Additional Experimental Results We display the evaluation results for small-size (124M) models in Tables 8-9. Table 8: The evaluation results of small models with different attention mechanisms pre-trained using FineWeb-Edu 100B dataset (0-shot with lm-evaluation-harness). The best scores in each column are bolded. Abbreviations: HellaSw. = HellaSwag, W.G. = WinoGrande. Method ARC-E ARC-C BoolQ HellaSw. OBQA PIQA W.G. MMLU SciQ Avg. MHA MQA GQA MLA TPA-KVonly TPA 50.63 49.62 48.70 49.66 51.05 51.26 26.96 25.34 25.68 26.45 26.54 27.39 59.39 55.72 56.15 61.22 57.25 57. 36.18 35.94 35.58 33.94 36.77 36.68 32.00 31.40 31.40 32.40 32.60 32.80 64.96 64.85 64.91 62.73 65.02 64. 51.85 51.30 51.62 50.43 50.91 49.72 23.40 23.37 23.12 23.29 23.64 24.61 70.30 68.70 68.20 71.50 69.70 72. 46.19 45.14 45.04 45.74 45.94 46.21 B.3 Ablation Studies on Learning Rates We implement set of parallel experiments for medium models with learning rate 6 104, and the curves for training loss, validation loss and validation perplexity are displayed in Figure 5. We also show the performance of these models on the benchmarks described in Section 4 in Tables 10-11. The results show that TPA and TPA-KVonly models can also outperform other types of attention with different learning rates. 22 Table 9: The evaluation results of small models with different attention mechanisms on FineWebEdu 100B dataset (2-shot with lm-evaluation-harness). The best scores in each column are bolded. Abbreviations: HellaSw. = HellaSwag, W.G. = WinoGrande. Method ARC-E ARC-C BoolQ HellaSw. OBQA PIQA W.G. MMLU SciQ Avg. MHA MQA GQA MLA TPA-KVonly TPA 57.66 53.79 55.01 52.78 54.25 57.53 28.24 26.35 25.94 26.19 27.90 28.07 57.28 44.95 55.72 57.25 57.06 56. 36.43 34.18 35.68 33.19 36.36 36.49 29.60 28.80 31.80 29.60 31.80 31.80 64.09 62.79 65.29 63.98 64.31 64. 51.14 52.01 51.93 50.43 53.59 51.14 26.57 25.91 25.27 24.90 26.18 25.92 82.00 78.10 77.80 76.00 79.20 79. 48.11 45.21 47.16 46.04 47.85 47.93 (a) Training Loss (b) Validation Loss (c) Validation Perplexity Figure 5: The training loss, validation loss and validation perplexity of medium-size (353M) models with learning rate 6 104 and different attention mechanisms on the FineWeb-Edu 100B dataset. Table 10: The evaluation results of medium models (learning rate=6 104) with different attention mechanisms pre-trained using FineWeb-Edu 100B dataset (0-shot with lm-evaluation-harness). The best scores in each column are bolded. Abbreviations: HellaSw. = HellaSwag, W.G. = WinoGrande. Method ARC-E ARC-C BoolQ HellaSw. OBQA PIQA W.G. MMLU SciQ Avg. MHA MQA GQA MLA TPA-KVonly TPA 59.51 57.62 28.67 57.49 58.01 58.38 29.52 31.91 31.48 29.44 30.12 31.57 59.60 59.45 58.29 59. 58.01 59.39 45.68 45.69 45.45 44.09 45.95 46.83 34.20 35.40 35.20 25.77 35.60 37.00 68.82 69.31 68.50 68. 69.10 70.02 53.43 53.51 54.46 53.04 53.12 54.06 23.33 26.47 24.58 25.77 25.39 25.52 76.90 74.60 76.50 76. 75.10 79.90 50.11 50.44 47.01 48.96 50.04 51.41 Table 11: The evaluation results of medium models (learning rate 6 104) with different attention mechanisms pre-trained using FineWeb-Edu 100B dataset (2-shot with lm-evaluation-harness). The best scores in each column are bolded. Abbreviations: HellaSw. = HellaSwag, W.G. = WinoGrande. Method ARC-E ARC-C BoolQ HellaSw. OBQA PIQA W.G. MMLU SciQ Avg. MHA MQA GQA MLA TPA-KVonly TPA 64.73 64.98 65.24 63.80 64.69 67. 32.42 33.62 33.19 31.06 32.34 34.56 58.29 55.02 56.54 58.50 59.48 57.22 45.89 45.81 45.41 44.19 46.23 46. 34.20 34.00 34.80 35.40 35.40 34.60 68.50 69.59 69.04 68.44 70.08 69.91 53.20 53.43 55.72 51.62 54.06 52. 25.86 24.30 24.73 25.22 25.64 25.07 88.00 85.20 87.90 88.50 86.30 89.90 52.34 51.77 52.51 51.86 52.69 53."
        }
    ],
    "affiliations": [
        "IIIS, Tsinghua University",
        "Shanghai Qi Zhi Institute",
        "TapTap",
        "University of California, Los Angeles"
    ]
}