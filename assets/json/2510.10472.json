{
    "paper_title": "FML-bench: A Benchmark for Automatic ML Research Agents Highlighting the Importance of Exploration Breadth",
    "authors": [
        "Qiran Zou",
        "Hou Hei Lam",
        "Wenhao Zhao",
        "Yiming Tang",
        "Tingting Chen",
        "Samson Yu",
        "Tianyi Zhang",
        "Chang Liu",
        "Xiangyang Ji",
        "Dianbo Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large language models (LLMs) have sparked growing interest in automatic machine learning research agents. Among them, agents capable of autonomously proposing ideas and conducting machine learning experiments are particularly promising, as they maximize research automation and accelerate scientific progress by iteratively refining ideas based on experimental results. However, comprehensively evaluating such agents remains challenging. Existing benchmarks tend to overemphasize engineering aspects while neglecting academic rigor, creating barriers that obscure a clear assessment of an agent's scientific capabilities in machine learning research. They also suffer from limited task diversity, an overemphasis on application-oriented tasks over fundamental research problems, and limited scalability to realistic research settings. To address these limitations, we introduce FML-bench, a benchmark designed to evaluate automatic machine learning research agents on 8 diverse and fundamental machine learning research problems. It reduces coding burden, emphasizes fundamental problems rather than specific use cases, offers high task diversity, and is extensible to real-world machine learning GitHub repositories. Furthermore, we present a unified evaluation framework with five complementary metrics, designed to comprehensively assess agent performance on our benchmark. We evaluate state-of-the-art automatic research agents on FML-bench, and find that agents employing broad research exploration strategies outperform those focusing on narrow but deep exploration. These findings suggest that emphasizing the breadth of exploration may lead to more effective research outcomes than focusing solely on incremental refinement. Our benchmark is available at https://github.com/qrzou/FML-bench."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 1 ] . [ 1 2 7 4 0 1 . 0 1 5 2 : r arXiv preprint FML-BENCH: BENCHMARK FOR AUTOMATIC ML RESEARCH AGENTS HIGHLIGHTING THE IMPORTANCE OF EXPLORATION BREADTH Qiran Zou*1 Hou Hei Lam*1,2 Wenhao Zhao1 Yiming Tang1 Tingting Chen1 Samson Yu1 Tianyi Zhang3 Chang Liu2 Xiangyang Ji2 Dianbo Liu 1 2Tsinghua University 1National University of Singapore qiranzou@u.nus.edu, linhx22@mails.tsinghua.edu.cn {wenhaozhao,yimingtang,tingting.c,samson.yu}@u.nus.edu zhan9167@umn.edu, {liuchang2022,xyji}@tsinghua.edu.cn dianbo@nus.edu.sg 3University of Minnesota"
        },
        {
            "title": "ABSTRACT",
            "content": "Large language models (LLMs) have sparked growing interest in automatic machine learning research agents. Among them, agents capable of autonomously proposing ideas and conducting machine learning experiments are particularly promising, as they maximize research automation and accelerate scientific progress by iteratively refining ideas based on experimental results. However, comprehensively evaluating such agents remains challenging. Existing benchmarks tend to overemphasize engineering aspects while neglecting academic rigor, creating barriers that obscure clear assessment of an agents scientific capabilities in machine learning research. They also suffer from limited task diversity, an overemphasis on application-oriented tasks over fundamental research problems, and limited scalability to realistic research settings. To address these limitations, we introduce FML-bench, benchmark designed to evaluate automatic machine learning research agents on 8 diverse and fundamental machine learning research problems. It reduces coding burden, emphasizes fundamental problems rather than specific use cases, offers high task diversity, and is extensible to real-world machine learning GitHub repositories. Furthermore, we present unified evaluation framework with five complementary metrics, designed to comprehensively assess agent performance on our benchmark. We evaluate state-of-the-art automatic research agents on FML-bench, and find that agents employing broad research exploration strategies outperform those focusing on narrow but deep exploration. These findings suggest that emphasizing the breadth of exploration may lead to more effective research outcomes than focusing solely on incremental refinement. Our benchmark is available at: https://github.com/qrzou/FML-bench."
        },
        {
            "title": "INTRODUCTION",
            "content": "Large language models (LLMs) have catalyzed resurgence of interest in automatic machine learning (ML) research agents that assist or carry out parts of the scientific discovery workflow. These agents not only support hypothesis generation, coding, and experiment management, but also increasingly act as collaborators in discovery by providing complementary perspectives that can accelerate machine learning research across domains. Within this landscape, agents that automatically propose ideas and run experiments are particularly compelling (Lu et al., 2024; Yamada et al., 2025). They close the loop from ideation to empirical validation to maximize automation and to speed up research cycles. Compared to settings that only elicit ideas and then use LLMs or humans to assess novelty and feasibility which often diverge from real-world utility, this approach evaluates * Equal contribution. Corresponding author. 1 arXiv preprint Figure 1: Overview of FML-bench. FML-bench includes 8 fundamental machine learning research tasks, designed to evaluate agents capabilities in solving machine learning research problems. Agents are assessed on their ability to solve machine learning problems through iterative research. agents based on actual experimental outcomes, providing objective and quantitative evidence of their effectiveness (Wang et al., 2024a; Baek et al., 2024; Si et al., 2024). Despite rapid progress, existing benchmarks offer an incomplete picture of research competence, as shown in Tab. 1. Most focus on Kaggle-style, use-case-oriented tasks and emphasize engineering execution (e.g., feature engineering, standardized model training, and optimization) while paying limited attention to evaluating an agents ability to tackle fundamental machine learning research problems, such as representation learning and generalization (Chan et al., 2024; Huang et al., 2023; Padigela et al., 2025; Jing et al., 2024). Moreover, some benchmarks provide only raw data without baseline code (Chan et al., 2024; Jing et al., 2024), making it difficult to systematically assess agents research capabilities while introducing coding barriers that can obscure academic merit (e.g., when sound ideas fail due to engineering pitfalls). In addition, even when baseline codebases are provided, they are often handcrafted and tightly formatted (Huang et al., 2023; Padigela et al., 2025), which hinders their scalability. Because adapting them to new tasks usually requires substantial re-engineering to fit their benchmark design, rather than allowing direct use of existing codebases. To address these gaps, we introduce FML-bench, benchmark designed to evaluate automatic ML research agents on fundamental ML problems. FML-bench comprises 8 diverse tasks  (Fig. 1)  chosen to reflect bottlenecks that repeatedly surface in modern ML. The design follows four principles. 1) Fundamental ML problems. Designed tasks target core scientific challenges rather than application products or leaderboard scoring, keeping the focus on research questions. 2) Real-world codebases. Tasks are instantiated from existing research repositories, mirroring typical practice where new ideas are tested by adapting prior code. 3) Extensibility by construction. The benchmark can easily incorporate machine learning GitHub repositories that support end-to-end training and evaluation, requiring only minor output-format adapters. 4) Low coding barrier. Agents are not required to build entire codebases from scratch, but can start from provided baselines. This setup enables agents to focus on scientific advances in algorithms and architectures rather than on purely engineering effort. The tasks included in FML-bench span broad set of foundational problems (Wang et al., 2022; Pearl, 2019; Adadi, 2021; Goodfellow et al., 2014; Abadi et al., 2016; Bengio et al., 2013; Chen & Liu, 2018; Mehrabi et al., 2021): generalization (cross-domain transfer), data efficiency (learning from few samples), representation learning (discovery of meaningful features), continual learning (knowledge retention over time), causality (intervention reasoning), robustness and reliability (resistance to adversarial corruption), privacy (protection against information leakage) and fairness and bias (equitable treatment across groups). Agents are expected to propose new or improved ML methods that deliver stronger empirical results than baselines across these eight tasks. 2 arXiv preprint Table 1: Comparison of ML agent benchmarks across key design goals and agent requirements. Repo refers to the repository, and Comp denotes Competition. : In MLAgentBench, only part of the tasks meet this requirement; users must prepare baseline and evaluation code even when some tasks are based on real-world Kaggle repositories. Criterion Ours MLEBench MLAgentBench MLDevBench DSBench Design Goals Fundamental ML Problem Focus Real-World Repo/Comp Low Coding Barrier Scalability via Existing Repo/Comp Requirements to Agent Understand Codebase Understand Data Execute Arbitrary CMD with Args Execute Multi-step CMD List To evaluate agents holistically, we formalize five complementary metrics that capture different facets of research competence. Utility measures empirical performance improvement and serves as the primary objective. Diversity quantifies the variety of code modifications proposed; empirically, broader exploration often correlates with larger gains. Academic contribution rate distinguishes academic modifications (e.g., new losses, architectures, or training schemes) from engineering modifications (e.g., hyperparameter tuning), rewarding agents that prioritize scientifically meaningful changes. Cost accounts for computational and time expenditure. Step success rate captures the fraction of runs that produce valid results without bugs, reflecting an agents reliability in multi-step workflows. Different tasks emphasize different subsets of these metrics so that the aggregate evaluation reflects the spectrum of capabilities required for fundamental ML research. We evaluate several state-of-the-art automatic research agents and LLMs on FML-bench. central finding concerns agent strategy: once the basic requirements for both exploration breadth and depth are met, broader exploration proves more effective. Generating wider variety of ideas more reliably leads to successful methods than repeatedly refining single one, and we observe positive correlation between idea diversity and performance improvement. Besides, we find that Gemini-2.5Pro outperforms GPT-5 under our protocol. Finally, while CLI-style agents such as Claude Code offer general-purpose flexibility, they often fail to complete multi-step tasks due to early termination, where the model stops despite further actions being possible. This suggests that, although flexible, CLI-style agents are less suitable for automatic machine learning research than agents specifically designed for it. We summarize our contributions as follows: We construct FML-bench, benchmark centered on diversity of fundamental ML problems instantiated in real-world codebases, closing gaps left by use-case-oriented, engineering-heavy evaluations. We propose five-dimensional evaluation protocol covering utility, diversity, academic contribution rate, cost, and step success rate, jointly measuring empirical progress, research quality, and reliability. We provide empirical insights on research strategy (breadth vs. depth of exploration), quantify the role of diversity in driving gains, and report comparative results across leading agent frameworks and LLMs, offering guidance for practical agent design."
        },
        {
            "title": "2 RELATED WORKS",
            "content": "2.1 AUTOMATIC AI AGENTS Recent advances in large language models (LLMs) have enabled research agents to support core components of the scientific workflow. These agents are capable of generating and prioritizing research ideas, retrieving and synthesizing literature, and simulating peer review processes. For instance, SciMON (Wang et al., 2024a) and Nova (Hu et al., 2024) implemented frameworks for generating diverse and novel research ideas. AutoSurvey (Wang et al., 2024b) presented an au3 arXiv preprint tomated literature review framework that performs retrieval over large arXiv corpus, followed by outline planning and section drafting using specialized models. Meanwhile, AgentReview (Jin et al., 2024) employed LLM agents to simulate peer reviews, rebuttals, and committee discussions, offering insights into the dynamics of academic decision-making. Recent efforts are moving beyond assistance toward fully automatic research agents. These systems aim not only to support researchers but to generate ideas, implement them, run experiments, and refine approaches without human supervision. One representative system is AIDE (Jiang et al., 2025), tree-search agent that optimizes user-defined metrics by iteratively editing and evaluating code, though it executes only one file and modifies specific target file per iteration. TheAIScientist (Lu et al., 2024) represents an independent line of work, demonstrating end-to-end autonomy across the research process, including idea generation, implementation, experimentation, analysis, and manuscript drafting. Its improved version (Yamada et al., 2025) further reduces reliance on hand-crafted templates, enhancing generality across tasks. Similarly, the AgentLaboratory executes full pipeline for automatic research, but its evaluation is limited to relatively simple research questions. Separately, AlphaEvolve (Novikov et al., 2025) adopts an evolutionary approach, iteratively refining and selecting promising ideas through variation and empirical evaluation. Beyond the computer science domain, growing number of research agents have been developed for other fields, including chemistry, where they are used to investigate and optimize chemical processes (Boiko et al., 2023; M. Bran et al., 2024), and biomedical science, where they have been applied to the discovery of novel nanobodies (Swanson et al., 2024). 2.2 BENCHMARKS FOR ML AGENTS While recent benchmarks have begun to evaluate agents on code-intensive tasks, they remain limited in both scope and flexibility. MLAgentBench (Huang et al., 2023) includes 13 machine learning engineering tasks, but most are implemented as single-file scripts, which is not practical for real-world scenarios. In addition, it requires setting an individual evaluator for each task and lacks support, limiting its scalability to support more tasks. MLE-Bench (Chan et al., 2024) covers 75 Kaggle competitions and assesses whether agents can function as machine learning engineers. It emphasizes tasks such as data pipeline management, experiment orchestration, and submission formatting, which may shift focus away from core machine learning understanding. ML-Dev-Bench (Padigela et al., 2025) places greater emphasis on engineering aspects such as dataset loading and API integration. It evaluates agents ability to improve existing baselines only in performance tests, which are relatively simple due to narrow task scopes like classification and segmentation, and the use of fixed starter files. In contrast, our benchmark includes tasks spanning diverse machine learning domains. DSBench (Jing et al., 2024) aggregates 466 data analysis tasks and 74 modeling tasks from ModelOff and Kaggle, focusing on problem-solving within data science workflows. By comparison, our benchmark focuses on 8 diverse and fundamental machine learning research problems. It is built on real-world codebases, thereby providing practical challenges and strong extensibility by construction, while maintaining low coding barrier."
        },
        {
            "title": "3 UNIFIED EVALUATION FRAMEWORK FOR AUTOMATIC ML RESEARCH",
            "content": "Automatic ML research agents operate through iterative refinement cycles, where each iteration involves hypothesis generation, code modification, experimental execution, and empirical evaluation. To formalize this process, we propose unified optimization framework that is explicitly aligned with five evaluation dimensions. Consider an agent conducting research over iterations. At iteration {1, . . . , }, the agent starts with codebase Ct1 and generates hypothesis ht from learned proposal distribution qt. This hypothesis is instantiated as concrete code modification mt. After completing all iterations, the agent has produced hypothesis set = {h1, . . . , hT } with corresponding modifications = {m1, . . . , mT }. The agents objective over the research process is: max T, {qt,mt}T t=1 (cid:88) t= [Ut + λAt ηPt] + γS(M, C1) + βD(H) (1) where Ut = Ehtqt[U (mt, Ct1)] is the expected utility, At = A(mt) is the academic contribution rate, Ct = Ct1 mt, Pt = (mt) is the cost, S(M, C1) is the step success rate, and D(H) measures diversity across all hypotheses. 4 arXiv preprint Evaluation metrics Each term in Eq. 1 corresponds directly to our proposed evaluation metrics: Utility (m, C): The primary objective measuring empirical performance improvement. Specifically, (m, C) = perf(C m) perf(C), where perf() evaluates the task-specific metric (e.g., accuracy, AUC, error rate). Diversity D(H): Quantifies the variety across all proposed hypotheses = {h1, . . . , hT }. We measure this through semantic and structural variance of the resulting modifications, capturing the agents exploration breadth. Empirically, D(H) strongly correlates with discovering high-performing solutions. Academic Contribution Rate A(m): Measures the proportion of academic/algorithmic contributions (e.g., novel architectures, loss functions, training schemes) relative to engineering modifications (e.g., hyperparameter tuning, infrastructure fixes). Higher A(m) indicates greater scientific contribution, distinguishing genuine research advances from implementation optimizations. Step Success Rate S(M, C1): Captures the reliability of all code modifications on initial codebase C1. This reflects the agents ability to produce syntactically correct, semantically coherent code that successfully completes experiment iterations without errors. Cost (m): Encompasses time expenditure (wall-clock time) and API usage (tokens) required to execute codebase with modification m. Design Principles for Effective Agents. To achieve high utility while maintaining research quality, effective agents should satisfy: D(H) δ, α, ρ, (cid:88) t=1 Pt (2) (cid:80)T where = 1 t=1 At is the average academic contribution rate. These principles guide agent deT sign: maintain exploration breadth to avoid local optima (D δ), prioritize algorithmic innovation ( α), ensure reliable execution (S ρ), and respect computational budgets ((cid:80) Pt B)."
        },
        {
            "title": "4 BENCHMARK DESIGN",
            "content": "4.1 TASK DESCRIPTION We select 8 diverse tasks that collectively span the most critical aspects of ML research, ensuring comprehensive assessment for ML research agents. These tasks represent core competencies that comprehensive and robust agent should demonstrate: the ability to generalize beyond training distributions, learn efficiently from limited data, discover meaningful representations, retain knowledge over time, reason about causal relationships, maintain reliability under adversarial conditions, protect sensitive information, and operate fairly across different groups. Specifically, we evaluate agent performance across 8 critical machine learning tasks. 1) Generalization. Assessed via cross-domain transfer task in which models train on source domain and are evaluated on held-out target under distribution shift. The objective is to maximize out-of-domain accuracy. 2) Data Efficiency. Tested through few-shot classification task. Agents should propose approaches to improve metric-based decision rules in the embedding space to boost accuracy with limited labels. 3) Representation Learning. Pretrain encoders in self-supervision manner and evaluate by linear-probe accuracy with the encoder frozen, targeting meaningful feature discovery. 4) Continual Learning. Measure knowledge retention in class-incremental sequence with shared output head. Agents should propose methods to mitigate catastrophic forgetting and maximize average accuracy across all tasks. 5) Causality. Estimate treatment effects under specified causal data-generating process and minimize absolute error in the average treatment effect (ATE). 6) Robustness and Reliability. Evaluate resilience to adversarial corruption, including poisoning or backdoor perturbations, while preserving clean performance. The defense score balances both objectives. 7) Privacy. Assess protection against information leakage by reducing the effectiveness of membership-inference attacks, i.e., lowering attack AUC. 8) Fairness and Bias. Evaluate equitable performance across groups in binary classification with sensitive attributes, aiming to improve group-fairness metrics (e.g., minimizing absolute average odds difference) without sacrificing overall accuracy. 5 arXiv preprint"
        },
        {
            "title": "4.2 UNIFIED INPUT-OUTPUT INTERFACE",
            "content": "A core design of our benchmark is to support direct utilization of machine learning research GitHub repositories. Real-world repositories vary significantly in execution pipelines, output formats, and evaluation protocols. We solve this through unified input-output interfaces that preserve repository complexity. Input Existing benchmark designs struggle to handle different GitHub repositories. Data-based benchmarks accept only datasets and task descriptions as inputs. And benchmarks providing codebase assume unified training script names, single-stage training, no customizable arguments, or requiring to set an individual evaluator manually. In contrast, real repositories use different script names, multi-stage pipelines, diverse training arguments, and include their own evaluator already. Our solution treats the complete execution sequence (training and evaluation commands) as single input unit so that the agent receives command list for running experiments. Therefore, our benchmark provide following resources (as shown in Fig. 1) to agent as input: 1) task description with objectives and expected outputs, 2) complete repository code, 3) suggested files for modification, 4) protected code segments that cannot be modified to preserve evaluation integrity, 5) command list for running experiments, 6) baseline performance, and 7) target improvement metrics. Output Repository outputs are various (e.g., from text files to JSON formats). However, most outputs share common structure: performance metrics on specific datasets. We provide postprocessing module that converts diverse task outputs into standardized format, enabling consistent metric extraction across all tasks while preserving native output mechanisms. This design bridges the gap between evaluation standardization and real-world repository diversity, enabling rigorous assessment of agents on practical code optimization tasks."
        },
        {
            "title": "5 EXPERIMENTS",
            "content": "5.1 SETTINGS Selections of Agents As shown in Fig. 2, we explore three automatic machine learning research agents, each adopting distinct research strategy. TheAIScientist follows broad exploration approach, generating and testing wide range of hypotheses in parallel across multiple experimental directions. AIDE employs hierarchical, tree-based search strategy, balancing the exploration of new possibilities with the exploitation of promising results. And we prompt Claude Code to employ linear refinement strategy, sequentially improving its hypotheses and code implementations to address ML tasks. Benchmark Task Settings Our benchmark encompasses 8 fundamental machine learning challenges, each implemented using established repositories with baselines: 1) Generalization using the DomainBed (Gulrajani & Lopez-Paz, 2020) repository with ERM (Vapnik, 1998) baseline, optimizing out-of-domain accuracy; 2) Data Efficiency using the Easy-Few-Shot-Learning (Sicara, 2024) repository with Prototypical Networks (Snell et al., 2017) baseline, maximizing few-shot classification accuracy; 3) Representation Learning using the Lightly (Lightly-AI, 2025) repository with MoCo (He et al., 2020) baseline, maximizing linear probing accuracy on frozen encoders; 4) Continual Learning using the Continual-Learning (van de Ven et al., 2022) repository with Synaptic Intelligence (Zenke et al., 2017) baseline, optimizing average accuracy across sequential tasks; 5) Causality using the CausalML (Chen et al., 2020) repository with DragonNet (Shi et al., 2019) baseline, minimizing absolute error in average treatment effect estimation; 6) Robustness and Reliability using the Adversarial Robustness Toolbox (ART) (Nicolae et al., 2018) repository with dp-instahide (Borgnia et al., 2021) defense baseline, optimizing defense scores against backdoor attacks; 7) Privacy using the PrivacyMeter (Murakonda & Shokri, 2020) repository with WideResNet-28-2 (Zagoruyko & Komodakis, 2016) baseline, minimizing membership inference attack AUC; and 8) Fairness and Bias using the AIF360 (Bellamy et al., 2018) repository with Adversarial Debiasing baseline, minimizing absolute average odds difference while preserving classification performance. Comprehensive details are provided in Appendix B. Experimental Protocol Each agent is required to execute in three independent rounds. In each round, the agent is assigned fixed budget of total steps = 100 (iterations). We select the best result achieved among the three rounds based on the target metric computed on the test set. 6 arXiv preprint Figure 2: Comparison of research exploration strategies of different agents. TheAIScientist uses parallel exploration for broad coverage, AIDE employs hierarchical tree-based search balancing exploration and exploitation, while Claude Code follows linear refinement for sequential improvement. Evaluation Metrics We evaluate agent performance on our benchmark using the proposed metrics: Utility, Diversity, Academic Contribution Rate, Cost, and Step Success Rate. In addition, we report the Step Completion Rate, which calculates the proportion of executed steps relative to the required total, since AIDE and Claude Code may exhibit premature termination. See Sec. in the appendix for detailed calculation of metrics. We present detailed Utility results across the 8 tasks (Tab. 2), and report averages of the other metrics over the 8 tasks (Tab. 3); full per-task results are provided in Appendix E. 5.2 IMPLEMENTATION DETAILS Agents To enable these agents to operate on our benchmark, several modifications were necessary. We adapted TheAIScientist by fixing compatibility issues and extending its functionality to support the requirements of our benchmark, such as executing experiments in real repositories and reporting results consistently. As for AIDE, we employed its cloud-based commercial variant, Weco, as the operational interface, adapting our benchmark to integrate with its workflow despite limited control over its internal mechanisms. For Claude Code, we designed prompting scheme (see Appendix F) that enabled it to function as an automatic research agent, capable of reading code, generating hypotheses, and proposing modifications grounded in experimental feedback. LLMs adopted for agents We employ GPT-5 (2025-08-07) and Gemini-2.5-Pro (2025-06-17) for TheAIScientist and AIDE. For Claude Code, it is constrained to its native models, and therefore, we use Opus-4.1 (2025-08-05). 5.3 RESULTS AND DISCOVERIES 5.3.1 COMPARISON OF AGENTS WITH DIFFERENT RESEARCH STRATEGIES As shown in Tab. 2, the combination of TheAIScientist with Gemini-2.5-Pro achieved the best performance, ranking first in 4 out of 8 tasks. The combination of TheAIScientist with GPT-5 ranked second, securing top results in 2 out of the 8 tasks. These findings suggest that TheAIScientist performs better in discovering novel and effective machine learning methods, compared to AIDE and Claude Code. As illustrated in Fig. 2, TheAIScientist adopts research exploration strategy that is broad but shallow, while AIDE maintains both medium breadth and depth. In contrast, Claude Code exhibits narrow yet deep exploration pattern. For detailed explanation of the research strategies adopted by each agent, refer to Section 5.1. Considering the research exploration strategy together, the results suggest that broader research exploration space adopted by TheAIScientist is more effective for discovering promising ideas. This insight offers practical guidance for real-world research: broadly exploring diverse ideas could be more productive than focusing on single direction. 7 arXiv preprint Table 2: Comparison of best performance (Utility) among different agents. G2.5-Pro denotes Gemini-2.5-Pro. () indicates higher is better, () indicates lower is better. ML Problems Baseline TheAIScientist AIDE Claude Code GPT-5 G2.5-Pro GPT-5 G2.5-Pro Opus-4.1 Generalization () Data Eff. () Rep. Learn. () Cont. Learn. () Causality () Robust. & Rel.() Privacy () Fair. & Bias () 0.2254 0.6547 0.7562 0.2710 0.2057 0.6966 0.8114 0.1471 0.5036 0.7689 0.7796 0.4281 0.0434 0.9623 0.4908 0.0002 0.3252 0.8231 0.8597 0.7808 0.0058 0.9570 0.1750 0. 0.2254 0.6547 0.8469 0.4369 0.0155 0.9708 0.4882 0.0004 0.2254 0.6547 0.8466 0.3658 0.0000 0.9653 0.4814 0.0030 0.5036 0.6571 0.7725 0.2337 0.0063 0.9469 0.4892 0.0149 Table 3: Comparisons of different agents across diversity, academic contribution rate, cost, and success metrics. Contrib. denotes Contribution and Cons. stands for Consumption. Metrics Diversity () Academic Contrib. Rate () Total Token Cons. (M) () Step Token Cons. (M) () Total Time Cons. (H) () Step Time Cons. (Min) () Step Success Rate () Step Completion Rate () TheAIScientist AIDE Claude Code GPT-5 24.959.63 0.830.27 6.072.05 0.060.02 8.323.15 10.3910.79 0.920.08 1.000.00 G2.5-Pro 20.6610.85 0.780.24 5.332.55 0.050.03 12.237.93 12.7314.64 0.870.12 1.000. GPT-5 20.269.37 0.840.20 0.850.59 0.030.03 2.461.77 10.7912.86 0.650.25 0.380.22 G2.5-Pro 18.4914.87 0.650.34 2.590.90 0.040.03 8.066.44 8.657.92 0.730.22 0.790.25 Opus-4.1 12.028.04 0.250.36 8.323.10 1.771.32 1.271.77 12.4910.77 0.830.18 0.070.05 5.3.2 ANALYSIS OF DIVERSITY As shown in Tab. 3, TheAIScientist shows the highest average diversity (GPT-5: 24.95 9.63; Gemini-2.5-Pro: 20.66 10.85). AIDE is lower but comparable (GPT-5: 20.26 9.37; Gemini-2.5Pro: 18.49 14.87), while Claude Code is markedly lower (12.02 8.04). This pattern mirrors how the agents explore solutions. TheAIScientist advances several ideas in parallel. AIDE grows ideas through tree of iterative refinements. Claude Code, by contrast, tends to proceed along single linear track. By comparison, parallel exploration broadens the search and produces higher measured diversity, while linear iteration confines the search and curbs diversity. We further analyze the relationship between code diversity and task performance using the correlation coefficient (details are shown in Appendix Fig. 4). The results suggest that, overall, diversity is positively correlated with performance. Specifically, among the 8 tasks, 4 tasks show strong positive correlations, 2 weak positive correlations, and 2 negative correlations. The most notable effects appear in Continual Learning (r = 0.96), Fairness & Bias (r = 0.86), and Generalization (r = 0.72), with moderate correlation also observed in Data Efficiency (r = 0.48). These findings indicate that higher code diversity tends to be associated with improved task performance, although the strength of this relationship varies by task. 5.3.3 ACADEMIC CONTRIBUTION RATE The academic contribution rate provides further insights into the characteristics of each agent. It helps disentangle the impact of academic value from other factors such as engineering effort and diversity. According to Tab. 3, TheAIScientist generally exhibits slightly higher academic contribution rate than AIDE, whereas Claude Code consistently shows the lowest rate. This suggests that the ideas and code modifications proposed by TheAIScientist are more closely aligned with methodological advancements, rather than relying on engineering tricks to boost performance. Fur8 arXiv preprint thermore, comparing GPT-5 and Gemini-2.5-Pro reveals that Gemini-2.5-Pro tends to propose more engineering-oriented solutions than GPT-5. In summary, for automatic ML research agents, we prioritize agents that can generate hypotheses with strong academic value while simultaneously delivering better Utility (higher performance)."
        },
        {
            "title": "5.3.4 TOKEN AND TIME CONSUMPTION",
            "content": "Tab. 3 reports both the average token and time consumption per step, as well as the total token and time usage for complete experimental run. We observe that TheAIScientist consumes more tokens than AIDE, while Claude Code, despite its lower performance, uses the highest number of tokens among the three agents. This indicates that dedicated automatic ML research agents, such as TheAIScientist and AIDE, are more suitable for ML research problems in terms of both performance and token efficiency compared to general-purpose agents like Claude Code. In terms of time per step, all three agents show similar durations, with differences of around 2 minutes, which are not substantial. However, AIDE and Claude Code exhibit significantly shorter total execution times for full experiment. This is primarily due to premature termination issues, as evidenced by the step completion rate, which leads to reduced overall time usage. 5.3.5 ADDITIONAL OBSERVATIONS Characteristics of Claude Code Since all actions are executed based on LLM decisions rather than fixed procedures, Claude Code often fails to follow prompt instructions, frequently terminating experiments prematurely. Despite this, it demonstrates high improvement speed (refer to the improvement curves Fig. 3 and speed comparison Tab. 12 in the Appendix). In addition, its academic contribution rate is low, with strong emphasis on engineering. This may be attributed to its nature as general-purpose agent, rather than specialized automatic ML research agent. Shallow edits We found that AIDE sometimes misinterprets the structure and logic of target codebases. In certain cases, it generated new classes or components that were never integrated into the actual execution pipeline, resulting in no functional improvement over the baseline. As shown in Tab. 2, AIDE failed to improve the baseline in tasks related to Generalization and Data Efficiency. This may stem from the fact that AIDE only supports iterative modifications on single file. However, real-world ML research codebases are often complex and span multiple files, making AIDE insufficient for addressing realistic research tasks. Premature termination We encountered the issue of early termination of AIDE and Claude Code. For AIDE, the agent sometimes terminated prematurely due to its commercial version, Weco, which relies on cloud infrastructure that occasionally failed during execution. For Claude Code, early stopping was often triggered by the models internal reasoning, where the LLM would decide not to continue even when further actions are possible."
        },
        {
            "title": "6 CONCLUSION",
            "content": "In this work, we introduce FML-bench, benchmark that assesses automatic machine learning research agents on 8 diverse and fundamental ML problems drawn from real-world codebases and authentic research workflows. Within the benchmark, we introduce five-dimensional evaluation protocol that facilitates more systematic assessment, moving beyond exclusive reliance on final performance. Building on FML-bench, we conducted systematic analysis of three state-of-theart automatic research agents. Our findings reveal that agents capable of generating and evaluating multiple hypotheses across diverse directions, such as TheAIScientist, tend to outperform those that focus on iteratively refining single line of thought (e.g., Claude Code). These findings suggest that broader exploratory capacity contributes more significantly to overall success once basic level of proficiency is established. Overall, FML-bench provides robust and practical foundation for evaluating the capabilities of research agents and offers pathway toward building more effective, generalizable, and scientifically productive research agents. 9 arXiv preprint"
        },
        {
            "title": "REPRODUCIBILITY STATEMENT",
            "content": "The paper describes the benchmark design, evaluation protocol, experimental settings, and implementation details, including baseline configurations and prompt specifications. All benchmark code, experiment prompts, and configuration files are open-sourced and available at: https: //github.com/qrzou/FML-bench."
        },
        {
            "title": "ACKNOWLEDGEMENTS",
            "content": "We gratefully acknowledge Zhengyao Jiang and Weco (https://www.weco.ai/) for their support and for providing access to their more general agent, which extended beyond the limitations of the original AIDE and enabled us to run AIDE as baseline on our benchmark."
        },
        {
            "title": "REFERENCES",
            "content": "Martin Abadi, Andy Chu, Ian Goodfellow, Brendan McMahan, Ilya Mironov, Kunal Talwar, and In Proceedings of the 2016 ACM SIGSAC Li Zhang. Deep learning with differential privacy. conference on computer and communications security, pp. 308318, 2016. Amina Adadi. survey on data-efficient algorithms in big data era. Journal of Big Data, 8(1):24, 2021. Martin Arjovsky, Leon Bottou, Ishaan Gulrajani, and David Lopez-Paz. Invariant risk minimization. arXiv preprint arXiv:1907.02893, 2019. Jinheon Baek, Sujay Kumar Jauhar, Silviu Cucerzan, and Sung Ju Hwang. Researchagent: Iterative research idea generation over scientific literature with large language models. arXiv preprint arXiv:2404.07738, 2024. Barry Becker and Ronny Kohavi. Adult. UCI Machine Learning Repository, 1996. DOI: https://doi.org/10.24432/C5XW20. Rachel K. E. Bellamy, Kuntal Dey, Michael Hind, Samuel C. Hoffman, Stephanie Houde, Kalapriya Kannan, Pranay Lohia, Jacquelyn Martino, Sameep Mehta, Aleksandra Mojsilovic, Seema Nagar, Karthikeyan Natesan Ramamurthy, John Richards, Diptikalyan Saha, Prasanna Sattigeri, Moninder Singh, Kush R. Varshney, and Yunfeng Zhang. AI Fairness 360: An extensible toolkit for detecting, understanding, and mitigating unwanted algorithmic bias, October 2018. URL https://arxiv.org/abs/1810.01943. Yoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning: review and new perspectives. IEEE transactions on pattern analysis and machine intelligence, 35(8):17981828, 2013. Daniil Boiko, Robert MacKnight, Ben Kline, and Gabe Gomes. Autonomous chemical research with large language models. Nature, 624(7992):570578, 2023. Eitan Borgnia, Jonas Geiping, Valeriia Cherepanova, Liam Fowl, Arjun Gupta, Amin Ghiasi, Furong Huang, Micah Goldblum, and Tom Goldstein. Dp-instahide: Provably defusing poisoning and backdoor attacks with differentially private data augmentations. arXiv preprint arXiv:2103.02079, 2021. Jun Shern Chan, Neil Chowdhury, Oliver Jaffe, James Aung, Dane Sherburn, Evan Mays, Giulio Starace, Kevin Liu, Leon Maksin, Tejal Patwardhan, et al. Mle-bench: Evaluating machine learning agents on machine learning engineering. arXiv preprint arXiv:2410.07095, 2024. Huigang Chen, Totte Harinen, Jeong-Yoon Lee, Mike Yung, and Zhenyu Zhao. Causalml: Python package for causal machine learning, 2020. Zhiyuan Chen and Bing Liu. Lifelong machine learning. Morgan & Claypool Publishers, 2018. Li Deng. The mnist database of handwritten digit images for machine learning research. IEEE Signal Processing Magazine, 29(6):141142, 2012. 10 arXiv preprint Ian Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. arXiv preprint arXiv:1412.6572, 2014. Ishaan Gulrajani and David Lopez-Paz. In search of lost domain generalization. arXiv preprint arXiv:2007.01434, 2020. Daya Guo, Shuo Ren, Shuai Lu, Zhangyin Feng, Duyu Tang, Shujie Liu, Long Zhou, Nan Duan, Alexey Svyatkovskiy, Shengyu Fu, et al. Graphcodebert: Pre-training code representations with data flow. arXiv preprint arXiv:2009.08366, 2020. Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for In Proceedings of the IEEE/CVF conference on unsupervised visual representation learning. computer vision and pattern recognition, pp. 97299738, 2020. Xiang Hu, Hongyu Fu, Jinge Wang, Yifeng Wang, Zhikun Li, Renjun Xu, Yu Lu, Yaochu Jin, Lili Pan, and Zhenzhong Lan. Nova: An iterative planning and search approach to enhance novelty and diversity of llm generated ideas. arXiv preprint arXiv:2410.14255, 2024. Qian Huang, Jian Vora, Percy Liang, and Jure Leskovec. Mlagentbench: Evaluating language agents on machine learning experimentation. arXiv preprint arXiv:2310.03302, 2023. Zhengyao Jiang, Dominik Schmidt, Dhruv Srikanth, Dixing Xu, Ian Kaplan, Deniss Jacenko, and Yuxiang Wu. Aide: Ai-driven exploration in the space of code. 2025. URL https://arxiv. org/abs/2502.13138. Yiqiao Jin, Qinlin Zhao, Yiyang Wang, Hao Chen, Kaijie Zhu, Yijia Xiao, and Jindong Wang. Agentreview: Exploring peer review dynamics with llm agents. arXiv preprint arXiv:2406.12708, 2024. Liqiang Jing, Zhehui Huang, Xiaoyang Wang, Wenlin Yao, Wenhao Yu, Kaixin Ma, Hongming Zhang, Xinya Du, and Dong Yu. Dsbench: How far are data science agents from becoming data science experts? arXiv preprint arXiv:2409.07703, 2024. Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009. Lightly-AI. Lightly: python library for self-supervised learning on images. https://github. com/lightly-ai/lightly, 2025. Chris Lu, Cong Lu, Robert Tjarko Lange, Jakob Foerster, Jeff Clune, and David Ha. The ai scientist: Towards fully automated open-ended scientific discovery. arXiv preprint arXiv:2408.06292, 2024. Andres M. Bran, Sam Cox, Oliver Schilter, Carlo Baldassari, Andrew White, and Philippe Schwaller. Augmenting large language models with chemistry tools. Nature Machine Intelligence, 6(5):525535, 2024. Ninareh Mehrabi, Fred Morstatter, Nripsuta Saxena, Kristina Lerman, and Aram Galstyan. survey on bias and fairness in machine learning. ACM computing surveys (CSUR), 54(6):135, 2021. Sasi Kumar Murakonda and Reza Shokri. Ml privacy meter: Aiding regulatory compliance by quantifying the privacy risks of machine learning. arXiv preprint arXiv:2007.09339, 2020. Maria-Irina Nicolae, Mathieu Sinn, Minh Ngoc Tran, Beat Buesser, Ambrish Rawat, Martin Wistuba, Valentina Zantedeschi, Nathalie Baracaldo, Bryant Chen, Heiko Ludwig, et al. Adversarial robustness toolbox v1. 0.0. arXiv preprint arXiv:1807.01069, 2018. Xinkun Nie and Stefan Wager. Quasi-oracle estimation of heterogeneous treatment effects. Biometrika, 108(2):299319, 2021. Alexander Novikov, Ngˆan Vu, Marvin Eisenberger, Emilien Dupont, Po-Sen Huang, Adam Zsolt Wagner, Sergey Shirobokov, Borislav Kozlovskii, Francisco JR Ruiz, Abbas Mehrabian, et al. Alphaevolve: coding agent for scientific and algorithmic discovery. arXiv preprint arXiv:2506.13131, 2025. 11 arXiv preprint Harshith Padigela, Chintan Shah, and Dinkar Juyal. Ml-dev-bench: Comparative analysis of ai agents on ml development workflows. arXiv preprint arXiv:2502.00964, 2025. Judea Pearl. The seven tools of causal inference, with reflections on machine learning. Communications of the ACM, 62(3):5460, 2019. Claudia Shi, David Blei, and Victor Veitch. Adapting neural networks for the estimation of treatment effects. Advances in neural information processing systems, 32, 2019. Chenglei Si, Diyi Yang, and Tatsunori Hashimoto. Can llms generate novel research ideas? largescale human study with 100+ nlp researchers. arXiv preprint arXiv:2409.04109, 2024. Sicara. Easy few-shot learning: ready-to-use code and tutorial notebooks for few-shot image classification. https://github.com/sicara/easy-few-shot-learning, 2024. Jake Snell, Kevin Swersky, and Richard Zemel. Prototypical networks for few-shot learning. Advances in neural information processing systems, 30, 2017. Kyle Swanson, Wesley Wu, Nash Bulaong, John Pak, and James Zou. The virtual lab: Ai agents design new sars-cov-2 nanobodies with experimental validation. bioRxiv, pp. 202411, 2024. Gido van de Ven, Tinne Tuytelaars, and Andreas Tolias. Three types of incremental learning. Nature Machine Intelligence, 4:11851197, 2022. Vladimir N. Vapnik. Statistical Learning Theory. Wiley-Interscience, New York, 1998. Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Daan Wierstra, et al. Matching networks for one shot learning. Advances in neural information processing systems, 29, 2016. Jindong Wang, Cuiling Lan, Chang Liu, Yidong Ouyang, Tao Qin, Wang Lu, Yiqiang Chen, Wenjun Zeng, and Philip Yu. Generalizing to unseen domains: survey on domain generalization. IEEE transactions on knowledge and data engineering, 35(8):80528072, 2022. Qingyun Wang, Doug Downey, Heng Ji, and Tom Hope. Scimon: Scientific inspiration machines optimized for novelty. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 279299, 2024a. Yidong Wang, Qi Guo, Wenjin Yao, Hongbo Zhang, Xin Zhang, Zhen Wu, Meishan Zhang, Xinyu Dai, Qingsong Wen, Wei Ye, et al. Autosurvey: Large language models can automatically write surveys. Advances in neural information processing systems, 37:115119115145, 2024b. Yutaro Yamada, Robert Tjarko Lange, Cong Lu, Shengran Hu, Chris Lu, Jakob Foerster, Jeff Clune, and David Ha. The ai scientist-v2: Workshop-level automated scientific discovery via agentic tree search. arXiv preprint arXiv:2504.08066, 2025. Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. arXiv preprint arXiv:1605.07146, 2016. Friedemann Zenke, Ben Poole, and Surya Ganguli. Continual learning through synaptic intelligence. In International conference on machine learning, pp. 39873995. PMLR, 2017. 12 arXiv preprint"
        },
        {
            "title": "A USE OF LLMS",
            "content": "In this study, we employed LLMs (ChatGPT and Claude AI) to assist with manuscript writing. Specifically, these tools were used to polish the writing (e.g., improving fluency, grammar, and clarity of expression), aid in formatting, and support proofreading. Additionally, we used them to facilitate the retrieval of related work by suggesting potentially relevant literature."
        },
        {
            "title": "B TASK SETTINGS",
            "content": "Generalization We adopt the DomainBed (Gulrajani & Lopez-Paz, 2020) repository with Empirical Risk Minimization (ERM) (Vapnik, 1998) as the baseline and evaluate on the ColorMNIST (Arjovsky et al., 2019) dataset. The evaluation metric is accuracy on held-out domain. The agent is required to train on source domain and generalize to target domain under distribution shift. The task objective is to improve out-of-domain generalization performance. This task evaluates the agents ability to develop algorithms that transfer effectively across domains, which is critical for robust real-world deployment. Causality We utilize the CausalML (Chen et al., 2020) repository with DragonNet (Shi et al., 2019) as the baseline model, evaluated on synthetic dataset constructed from Setup in Nie & Wager (2021), Quasi-Oracle Estimation of Heterogeneous Treatment Effects. The performance metric is the absolute error in average treatment effect (ATE) estimation. The agents goal is to develop improved causal inference strategies that minimize this error. This task assesses the agents capacity to reason about interventions, which is essential for decision-making in high-stakes environments. Data Efficiency For data-efficient learning, we employ the Easy-Few-Shot-Learning (Sicara, 2024) repository with Prototypical Networks (Snell et al., 2017) as the baseline, evaluated on the Mini-ImageNet (Vinyals et al., 2016) dataset. The agent must operate under frozen backbone and propose improved algorithms for metric-based classification in the embedding space. Accuracy is used as the evaluation metric. This task measures the agents ability to enhance few-shot learning performance by optimizing distance-based reasoning under tight data constraints. Robustness and Reliability We adopt the Adversarial Robustness Toolbox (ART) (Nicolae et al., 2018) repository, using dp-instahide (Borgnia et al., 2021) as the defense baseline on poisoned MNIST (Deng, 2012) dataset constructed with the pattern backdoor method. The evaluation metric is defense score, defined as the harmonic mean of clean accuracy and resistance accuracy against backdoor attacks. The agent is tasked with proposing defenses that reduce the effectiveness of poisoning attacks while maintaining high clean performance. This task probes the agents ability to improve model robustness against adversarial corruption. Privacy This task uses the PrivacyMeter (Murakonda & Shokri, 2020) repository and WideResNet-28-2 (Zagoruyko & Komodakis, 2016) as the baseline model trained on CIFAR-10 (Krizhevsky et al., 2009). The evaluation targets membership inference attacks, with performance measured by the area under the ROC curve (AUC). The agent must design improved defense mechanisms that minimize the AUC, thereby reducing information leakage. This task is critical for assessing the agents ability to enhance privacy protections in the face of inference attacks. Representation Learning For self-supervised learning, we employ the Lightly (Lightly-AI, 2025) repository with MoCo (He et al., 2020) as the baseline and evaluate on CIFAR-10 (Krizhevsky et al., 2009). The primary evaluation metric is the accuracy of linear classifier trained on top of the frozen encoder after pretraining. The agent is expected to devise improved representation learning methods that yield higher linear probing accuracy. This task tests the agents ability to learn generalizable and semantically meaningful features from unlabeled data. Continual Learning We use the Continual-Learning (van de Ven et al., 2022) repository with Synaptic Intelligence (SI) (Zenke et al., 2017) as the baseline, evaluated on splitMNIST (Deng, 2012) 13 arXiv preprint in the class-incremental learning (Class-IL) scenario. The model must learn sequentially across multiple contexts using shared 10-way classification head. The evaluation metric is the average accuracy across all tasks. The agent is expected to develop algorithms that mitigate catastrophic forgetting and maintain performance across contexts. This task evaluates long-term adaptability in non-stationary environments. Fairness and Bias We adopt the AIF360 (Bellamy et al., 2018) repository with Adversarial Debiasing as the baseline, using the Adult (Becker & Kohavi, 1996) dataset. The primary fairness metric is average odds difference, and the agent is tasked with enhancing fairness (minimizing absolute average odds difference) while maintaining or improving classification accuracy. This task measures the agents ability to balance equitable outcomes with model utility across demographic groups."
        },
        {
            "title": "C EVALUATION METRICS",
            "content": "Utility reflects empirical improvement within each task and is reported using the task-specific performance metric. Diversity quantifies implementation dispersion within the best-performing round. Given th step code embedding ei extracted from CodeGraphBERT (Guo et al., 2020) and centroid = 1 i=1 ei, diversity is computed as 1 i=1 ei e2. Greater dispersion indicates broader exploration of implementation choices within single round. (cid:80)n (cid:80)n Academic Contribution Rate reflects the agents tendency toward innovative algorithmic contributions versus conventional engineering optimizations. It is defined as Naca/Nsuc, where Naca is the number of steps recogniezd by Qwen3 as academic modifications (e.g., new losses or architectures) relative to the baseline, and Nsuc is the number of steps whose experiments execute without errors and yield valid results. Step Success Rate is defined as the Nsuc/Ncomp. Combining the metric Step Completion Rate (Ncomp/Ntotal) will reflect the system reliability of the agent. Here, Ncomp is the number of steps actually executed and Ntotal is the number of steps assigned to the agent. Cost is reported as total token consumption (sum across steps) and step token consumption (average per step), together with total time consumption and step time consumption, to characterize computational and temporal efficiency. Step To Target calculation Steps To Target is measured as ST = min{n : performance(n) threshold}, representing the minimum number of iterations required to reach the target performance level, where the threshold is set to be the worst best-run performance among the agents. C.1 ACADEMIC MODIFICATION CLASSIFICATION Academic Contribution Rate is computed by identifying academic steps within the best-performing rounds. We use Qwen3 (version: 235b-a22b-2507) to automatically classify each modification as academic or engineering by comparing the baseline and modified code. The prompt used to guide Qwen3 is provided below. 14 arXiv preprint You are an experienced machine learning researcher. Your task is to analyze two pieces of codebaseline and modifiedand judge whether the modification is mostly prone to engineering or academic modification. You will also assign one subtag to describe the specific type(s) of modification. Baseline code is delimited by [baseline:begin] ...[baseline:end]. Modified code is delimited by [modified:begin] ... [modified:end]. Carefully compare the two kinds of code and reason about what changed and why it matters. Please give your judgement by engineering or academic and one the most related subtag eg: [engineering, [ENG/LR SCHED]], [academic, [ACD/LOSS NEW]]. Your answer should only select one of the two options [engineering] or [academic]. Engineering modification: Changes focused on making an existing method run more stably, efficiently, or accurately through implementation details, system-level tuning, or configuration, without introducing new learning principle, objective family, or architectural paradigm. Typical signals: hyperparameter tuning, training schedules, efficiency and stability tricks, data pipeline and evaluation scripting that keep the task/method essentially the same. Academic modification: Changes that introduce or test new machine learning idea: new objective/loss with motivation, new inductive bias or architecture, different training or inference paradigm, etc. For Engineering subtags: [ENG/LR SCHED]: Learning-rate, momentum, scheduler (Cosine, OneCycle, Warmup) [ENG/OPT SWAP]: Optimizer swap as tuning (SGD to AdamW) without new update rule. [ENG/LOSS WEIGHTING]: Loss weights/temperature/threshold sweeps (no new loss family). [ENG/TRAIN STR]: Early stopping or extend training time. [ENG/DATA AUG]: Data cleaning/dedup/resampling; standard/strong augmentation parameters For Academic subtags: [ACD/LOSS NEW]: New loss/constraint; dual/information-theoretic objectives [ACD/ARCH NEW]: New layer/module/architecture (e.g., novel attention; new encoderdecoder coupling) [ACD/TRAIN PARADIGM]: New training or inference paradigm (e.g., contrastive to masked/generative; new decoding strategy; beyond teacher forcing; supervised to selfsupervised) Your answer should only select one of the two options [engineering] or [academic] [ENG/LR SCHED]], [academic, and one the most related subtag eg: [ACD/LOSS NEW]]. [baseline:begin] ... baseline code ... [baseline:end] [modified:begin] ... modified code ... [modified:end] [engineering, Compare baseline code and modified code, tell me which kind of code modification it is to improve the baseline method, choose from engineering, academic and one most related subtag eg: [engineering, [ENG/LR SCHED]], [academic, [ACD/LOSS NEW]]"
        },
        {
            "title": "D PROTECTING EVALUATION INTEGRITY",
            "content": "A crucial aspect of our implementation involved protecting evaluation files from inadvertent modification by the agents. We employed agent-specific protection strategies: For TheAIScientist, we explicitly instructed the agent through prompting to avoid modifying evaluation files and imple15 arXiv preprint mented systematic refresh mechanism that restores evaluation files before each evaluation cycle. For AIDE, protection is inherently ensured by its single-file modification constraint, which guarantees that when the target code and evaluation code are separated into different files, the evaluation files remain naturally protected. For Claude Code, we implemented two-pronged approach in which the evaluation files were first restricted to read and execute permissions before running the agent, and then the --disallowedTools argument was employed to explicitly prevent permission modification operations during execution."
        },
        {
            "title": "E ADDITIONAL RESULTS",
            "content": "Figure 3: Agents performance improvement curves across 8 tasks. This section provides comprehensive comparative analysis of automated AI research systems, focusing on both their performance and operational efficiency. We evaluate TheAIScientist, AIDE, and Claude Code across eight core machine learning research problems, considering not only their 16 arXiv preprint final outcomes but also their operational behavior over time. Fig. 3 depicts the step-wise performance progression of these systems across all tasks, while Fig. 4 highlights the relationship between performance and solution diversity. Our analysis covers multiple key dimensions: solution diversity (Tab. 4), research contribution quality (Tab. 5), computational efficiency in terms of total token usage (Tab. 6) and step-wise token usage (Tab. 7), runtime efficiency in terms of total time (Tab. 8) and step-wise time (Tab. 9), operational reliability measured by step success rate (Tab. 10) and step completion rate (Tab. 11), and improvement speed (Tab. 12). Together, these results provide detailed insights into the trade-offs between different agentic research designs, illustrating how they influence both the quality of research outcomes and the efficiency of resource utilization. Figure 4: Performance - Diversity Analysis. Table 4: Code Diversity Comparison of Best Performance Round. G2.5-Pro denotes Gemini2.5-Pro. Bold numbers indicate the best (highest) performance in each row. ML Problems Generalization Data Eff. Rep. Learn. Cont. Learn. Causality Robust. & Rel. Privacy Fair. & Bias TheAIScientist AIDE Claude Code GPT-5 G2.5-Pro GPT-5 G2.5-Pro Opus-4. 46.38 25.43 17.42 21.41 19.22 16.65 33.84 19.29 17.94 17.70 10.86 45.23 18.25 30.13 13.13 12.06 0.00 18.33 15.06 30.79 17.51 29.88 25.29 25.23 21.73 11.06 8.76 18.03 8.19 56.05 10.13 13.98 22.25 17.98 4.74 5.42 7.06 25.90 5.93 6.91 Average Std 24.959.63 20.6610.85 20.269.37 18.4914.87 12.028.04 arXiv preprint Table 5: Academic Contribution Rate (ACR) Comparison. G2.5-Pro denotes Gemini-2.5-Pro. Bold numbers indicate the best (highest) performance in each row. ML Problems Generalization Data Eff. Rep. Learn. Cont. Learn. Causality Robust. & Rel. Privacy Fair. & Bias TheAIScientist AIDE Claude Code GPT-5 G2.5-Pro GPT-5 G2.5-Pro Opus-4. 0.90 1.00 0.93 0.12 0.88 0.95 0.90 0.95 0.95 0.89 0.93 0.17 0.78 0.87 0.83 0.84 0.33 0.97 0.98 0.89 0.93 0.92 0.80 0.93 0.63 0.99 0.99 0.35 0.89 0.33 0.07 0.95 0.33 1.00 0.00 0.00 0.00 0.67 0.00 0.00 Average Std 0.830.27 0.780.24 0.840.20 0.650.34 0.250.36 Table 6: Total Token Consumption Comparison (in millions). G2.5-Pro denotes Gemini-2.5-Pro. Bold numbers indicate the best (lowest) performance in each row. ML Problems Generalization Data Eff. Rep. Learn. Cont. Learn. Causality Robust. & Rel. Privacy Fair. & Bias TheAIScientist AIDE Claude Code GPTG2.5-Pro GPT-5 G2.5-Pro Opus-4.1 9.34 3.09 4.69 7.81 4.57 7.24 6.64 5.13 9.88 2.55 3.99 7.74 3.05 6.74 4.45 4. 0.18 0.33 0.40 1.06 0.56 1.69 1.68 0.94 3.80 1.53 1.51 3.21 2.99 2.93 1.61 3.12 5.98 7.98 8.89 2.99 9.26 8.22 13.88 9.34 Average Std 6.072.05 5.332. 0.850.59 2.590.90 8.323.10 Table 7: Step Token Consumption Comparison (in millions). G2.5-Pro denotes Gemini-2.5-Pro. Bold numbers indicate the best (lowest) performance in each row. ML Problems Generalization Data Eff. Rep. Learn. Cont. Learn. Causality Robust. & Rel. Privacy Fair. & Bias TheAIScientist AIDE Claude Code GPT-5 G2.5-Pro GPTG2.5-Pro Opus-4.1 0.09 0.03 0.05 0.08 0.05 0.07 0.07 0.05 0.10 0.03 0.04 0.08 0.03 0.07 0.04 0.04 0.09 0.01 0.01 0.04 0.02 0.03 0.02 0.02 0.10 0.02 0.01 0.05 0.03 0.05 0.02 0. 1.99 0.50 0.81 1.49 0.77 1.64 4.63 2.34 Average Std 0.060.02 0.050.03 0.030.03 0.040. 1.771.32 18 arXiv preprint Table 8: Total Time Consumption Comparison (in hours). G2.5-Pro denotes Gemini-2.5-Pro. Bold numbers indicate the best (lowest) performance in each row. ML Problems Generalization Data Eff. Rep. Learn. Cont. Learn. Causality Robust. & Rel. Privacy Fair. & Bias TheAIScientist AIDE Claude Code GPT-5 G2.5-Pro GPTG2.5-Pro Opus-4.1 9.91 4.47 7.50 14.68 5.41 8.26 9.24 7.05 17.42 4.33 23.96 15.17 4.85 8.65 20.03 3.46 0.20 1.02 4.45 3.70 1.44 5.10 2.43 1.32 6.07 2.40 14.49 7.08 2.91 6.00 21.04 4. 0.81 0.47 5.56 0.28 0.57 0.90 1.29 0.25 Average Std 8.323.15 12.237.93 2.461.77 8.066. 1.271.77 Table 9: Step Time Consumption Comparison (in minutes). G2.5-Pro denotes Gemini-2.5-Pro. Bold numbers indicate the best (lowest) performance in each row. ML Problems Generalization Data Eff. Rep. Learn. Cont. Learn. Causality Robust. & Rel. Privacy Fair. & Bias TheAIScientist AIDE Claude Code GPT-5 G2.5-Pro GPT-5 G2.5-Pro Opus-4. 5.95 2.68 33.28 8.80 3.25 4.95 19.93 4.23 10.43 2.60 43.15 9.08 2.90 5.20 26.42 2.07 5.97 2.03 37.92 7.93 3.08 4.77 22.63 2.02 9.83 1.43 22.87 5.98 1.73 6.67 18.03 2.63 16.13 1.77 30.30 8.33 2.87 10.83 25.87 3.80 Average Std 10.3910.79 12.7314.64 10.7912.86 8.657.92 12.4910.77 Table 10: Step Success Rate Comparison. G2.5-Pro denotes Gemini-2.5-Pro. Bold numbers indicate the best (highest) performance in each row. ML Problems Generalization Data Eff. Rep. Learn. Cont. Learn. Causality Robust. & Rel. Privacy Fair. & Bias TheAIScientist AIDE Claude Code GPTG2.5-Pro GPT-5 G2.5-Pro Opus-4.1 0.7700 0.9600 0.8300 0.9600 0.8900 0.9900 0.9700 0.9500 0.9900 0.6500 0.8400 0.9400 0.9000 0.9500 0.9500 0. 0.5000 0.2333 0.8000 0.8214 0.8214 0.8281 0.8714 0.3590 0.7027 0.2871 0.6040 0.9155 0.9406 0.6667 0.9571 0.7426 1.0000 0.9375 0.5455 1.0000 0.8333 0.6000 1.0000 0.7500 Average Std 0.920.08 0.870. 0.650.25 0.730.22 0.830.18 19 arXiv preprint Table 11: Step Completion Rate (SCR) Comparison. G2.5-Pro denotes Gemini-2.5-Pro. Bold numbers indicate the best (highest) performance in each row. ML Problems Generalization Data Eff. Rep. Learn. Cont. Learn. Causality Robust. & Rel. Privacy Fair. & Bias TheAIScientist AIDE Claude Code GPTG2.5-Pro GPT-5 G2.5-Pro Opus-4.1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0.02 0.30 0.45 0.28 0.28 0.64 0.70 0.39 0.36 1 1 0.70 1 0.53 0.69 1 0.03 0.16 0.11 0.02 0.12 0.05 0.03 0.04 Average Std 1.000.00 1.000. 0.380.22 0.790.25 0.070.05 Table 12: Improvement Speed Comparison. We report the number of steps used to reach the threshold, which is defined as the best result achieved in the best run of the worst-performing agent. Fewer steps indicate higher improvement speed. G2.5-Pro denotes Gemini-2.5-Pro. Bold numbers indicate the best (lowest) performance in each row. ML Problems Generalization Data Eff. Rep. Learn. Cont. Learn. Causality Robust. & Rel. Privacy Fair. & Bias TheAIScientist AIDE Claude Code GPT-5 G2.5-Pro GPTG2.5-Pro Opus-4.1 1 1 29 6 13 26 47 1 1 2 10 1 38 50 83 25 1 1 15 1 17 5 31 14 1 1 40 1 7 34 40 1 3 10 2 6 2 2 4 Average Std 15.5016.95 26.2529.40 10.6310.66 17.6317. 3.752."
        },
        {
            "title": "F PROMPTS DETAILS",
            "content": "This section presents the detailed prompt specifications that form the foundation of our autonomous research agent framework. The prompts serve as the primary interface between human researchers and AI agents, translating high-level research objectives into actionable instructions that can guide systematic scientific inquiry across diverse machine learning domains. The prompt design philosophy centers on creating structured yet flexible research environment that balances autonomy with scientific rigor. Rather than providing overly prescriptive instructions that limit creative exploration, these prompts establish clear boundaries, evaluation criteria, and operational constraints while encouraging the agent to develop and test novel hypotheses within established research paradigms. This section is organized into two complementary components. First, we present the Task Description Prompts that define specific research challenges across 8 fundamental areas of machine learning, each grounded in established benchmarks and methodologies. These prompts simulate realistic research scenarios where an AI agent must navigate complex technical requirements while pursuing meaningful improvements to existing methods. 20 arXiv preprint Second, we detail the Autonomous Research Agent Framework that governs how agents interact with research codebases to conduct iterative experimentation. This operational framework transforms the conceptual research challenges into executable workflows, ensuring that agent behavior follows sound scientific methodology while maintaining reproducibility and experimental integrity. Together, these prompt specifications create comprehensive research environment where autonomous agents can contribute meaningfully to advancing machine learning across multiple disciplines, providing both the research contexts and the methodological framework necessary for systematic scientific progress. Task Description Prompts The following task descriptions establish comprehensive research contexts spanning 8 fundamental areas of machine learning. Each prompt follows structured format that defines: (1) the researchers identity and expertise, (2) the specific technical setup, including datasets and baseline methods, (3) clear optimization objectives and constraints, and (4) fairness criteria to ensure meaningful comparisons. These prompts span wide range of machine learning challenges, including generalization, data efficiency, privacy, fairness, and robustness. Each task is anchored in established benchmarks and frameworks, such as DomainBed for domain generalization, EasyFSL for few-shot learning, and AIF360 for fairness-aware learning, providing realistic experimental environments that closely mirror actual research workflows. The prompts are carefully crafted to encourage both incremental improvements to existing methods and the exploration of novel algorithmic approaches, while maintaining scientific rigor through controlled experimental conditions. This design enables systematic evaluation of how autonomous agents can contribute to advancing the state-of-the-art across multiple ML disciplines simultaneously. GENERALIZATION System: You are an ambitious AI PhD student focused on improving the generalization performance of machine learning methods using the DomainBed benchmark. Task Description: You are working with DomainBeds ERM (Empirical Risk Minimization) method as the baseline on ColoredMNIST to evaluate generalization under distribution shifts. Your goal is to enhance test-time domain generalization accuracy beyond standard ERM. You should improve the algorithm based on ERM, but you may also propose entirely new algorithms if they can better support cross-domain generalization. You are also allowed to refine the backbone model, as long as your modifications are fair compared to the original architecture. The priority is to improve the average accuracy on unseen test domains while maintaining accuracy on in-domain tests, along with ensuring efficiency and low complexity. DATA EFFICIENCY System: You are an ambitious AI PhD student focused on data-efficient learning, specializing in few-shot learning and meta-learning. Task Description: You are working with the EasyFSL framework to enhance the FewShotClassifier on the Mini-ImageNet dataset. The Mini-ImageNet dataset presents challenging few-shot learning scenario due to its fine-grained inter-class similarities and limited training examples per class. Your goal is to improve the classifiers ability to generalize to novel classes. REPRESENTATION LEARNING 21 arXiv preprint System: You are an ambitious AI PhD student focused on improving representation learning on CIFAR-10 using the Lightly self-supervised learning framework. Task Description: You are working with Lightlys MoCo baseline on CIFAR-10, evaluated strictly by linear probing Top-1 accuracy. Your goal is to improve representation learning at pretrain stage to improve linear-probe accuracy on the CIFAR-10 test set beyond standard MoCo as much as you can under the same compute and data (no external data). You may modify MoCo or propose new self-supervised methods if they can yield better representations, as long as your modifications are fair compared to the original architecture. You are also allowed to refine the ResNet-18 backbone as long as parameter count and FLOPs remain comparable to the baseline. Pretrain on the CIFAR-10 train split without labels, fit the linear classifier on the same train split, and report Top-1 on the test split with priority on improving representation learning performance."
        },
        {
            "title": "CONTINUAL LEARNING",
            "content": "System: You are an ambitious AI PhD student focused on improving continual learning based on Synaptic Intelligence (SI) on splitMNIST under the class-incremental scenario. Task Description: You are working with the Continual-Learning repos SI baseline to improve accuracy and reduce forgetting on splitMNIST (5 tasks 2 classes, single-head over 10 classes). Your goal is to improve average accuracy over all 5 contexts on splitMNIST without unfair model size or compute advantages. You should improve SI method, but are also allowed to add lightweight fair components , or propose new methods, as long as your modifications are fair (stay within fairness computation budgets). The priority is to improve the average accuracy. CAUSALITY System: You are an ambitious AI PhD student focused on advancing machine learning for causal inference, reasoning, and interpretable modeling. Task Description: You are working with the Dragonnet framework to estimate individual treatment effects (ITEs) in both real (IHDP) and simulated data scenarios. The simulated data follows Setup from Nie & Wager (2018), featuring difficult nuisance functions (e.g., propensity scores) but simple, easily identifiable treatment effects. Your goal is to improve the precision of treatment effect estimation across both IHDP and simulated benchmarks. ROBUSTNESS AND RELIABILITY System: You are an ambitious AI PhD student focused on improving robust learning under data poisoning and privacy constraints. Task Description: You are given the Adversarial Robustness Toolbox (ART) codebase with focus on the dp instahide defense. dp instahide mixes inputs with public data and applies differential privacy noise to hinder inversion and poisoning. While designed for privacy-preserving training, its structure offers headroom to harden against both clean-label and trigger/backdoor poisons. Your goal is to improve defense performance against diverse poisoning attacks while maintaining high clean accuracy. You may tune dp instahide, compose it with other defenses, or propose new method if it outperforms baselines. PRIVACY 22 arXiv preprint System: You are an ambitious AI PhD student focused on improving model privacy and security against membership inference attacks. Task Description: You are working with PrivacyMeters MIA (for information leakage through training points) and Robust MIA (RMIA, which refines the Likelihood Ratio Test with tighter null hypothesis and leverages reference models and population data) to evaluate and reduce the models privacy risk. Your goal is to drive the auditors AUC toward 0.5 and keep TPR@0.1%FPR and TPR@0.0%FPR near zero while preserving task accuracy. Focus only on defense-side strategies rather than modifying the attack algorithms."
        },
        {
            "title": "FAIRNESS AND BIAS",
            "content": "System: You are an ambitious AI PhD student focused on improving fairness-aware learning with AIF360s Adversarial Debiasing on the Adult dataset. Task Description: You are working with AIF360s Adversarial Debiasing (classifieradversary) as the baseline on the Adult dataset to evaluate the fairnessaccuracy tradeoff. Your goal is to minimize absolute Average Odds Difference toward parity (=0) while maintaining or improving Balanced Accuracy on held-out test splits and across protected subgroups (e.g., sex/race). You should enhance the baseline Adversarial Debiasing algorithm, but you may also propose entirely new fairness methods if they better support reduced absolute Average Odds Difference without sacrificing Balanced Accuracy. You are allowed to refine the classifier and adversary networks and the training pipeline, provided comparisons remain fair to the original setup (similar capacity, training budget, and data access). The priority is minimizing absolute Average Odds Difference while preserving or improving Balanced Accuracy. Prompting Claude Code as an Autonomous Research Agent The following comprehensive prompt specification defines how an AI agent operates within established research codebases to achieve meaningful scientific progress. Unlike traditional one-shot code generation, this framework establishes iterative research loop that mirrors authentic research methodology: hypothesis formation, implementation, experimentation, analysis, and refinement. Your Role You are an autonomous coding agent that: understands the task, proposes concrete idea/plan/solution, edits the code (respecting read-only constraints), executes fixed command list, handles errors by diagnosing and fixing them, records each steps modifications and results, and iterates until the iteration limit is reached. Repository Access You are given starter files, STARTER FILE PATHS, and may read other files as needed to complete the task. Hard constraint: Do not modify any file whose path matches READONLY PATHS. If necessary change would touch read-only file, propose an alternative (e.g., wrapper, config flag, adapter module) instead. 23 arXiv preprint"
        },
        {
            "title": "Loop Initialization",
            "content": "Initialize: count = 0 Record snapshot/baseline of the original code (the repository state before any of your edits). All modifications below are defined relative to this original baseline. Read original baseline results for reference. ORIGINAL BASELINE RESULTS PATH."
        },
        {
            "title": "The results are provided in",
            "content": "Step 1-3: Understanding and Planning Step 1 Understand the task Read the repo and TASK DESCRIPTION. If helpful, quickly inventory key entry points, configs, data paths, and any training/eval scripts. Step 2 Generate plan Produce brief idea/plan/solution describing what you will change and why. Step 3 Modify the code Implement your plan with minimal, focused edits. Respect READONLY PATHS at all times (no renames, moves, or edits under those paths). Keep changes atomic and well-commented. Step 4-6: Execution and Error Handling Step 4 Execute commands Run every command in COMMAND LIST sequentially. Capture stdout/stderr and exit codes for each command. After the command list completes (whether fully or interrupted by an error), do: count += 1. Step 5 Error handling If any command raised an exception or returned non-zero exit code: Diagnose the exception concisely (root cause + where it occurred). Propose specific fix. Apply the fix by editing the code (still respecting READONLY PATHS). Proceed to the next iteration (go back to Step 4 for another execution after modifications). Step 6 Iteration limit If count > MAX ITERS, stop and produce final summary. 24 arXiv preprint Step 7-10: Backup and Results Management Step 7 Per-step backup (always) For each iteration (each time you execute the command list), create directory: ./ agent runs/step {COUNT}/ Store in that directory: modified/ only the files that differ from the original baseline (preserve their relative paths). logs/ command outputs (one file per command, including exit codes). Step 8 Successful run artifacts If all commands in COMMAND LIST completed successfully: In ./ agent runs/step {COUNT}/results/, copy: ${RESULT DIR}/final info.json Confirm that final info.json exists in the step results. Step 9 Read & reset results directory Read and summarize the key contents of final info.json for guidance. Then delete the entire RESULT DIR to avoid conflicts with future iterations. Step 10 Improve the plan Based on the results, your current idea/plan/solution, and the code modifications so far: Generate new idea/plan/solution to further improve the outcome. Continue the loop, unless the iteration limit has been reached. Additional Rules & Conventions Command semantics: Treat any non-zero exit code as an error. If command expects env variables or paths, set them explicitly and document them in the logs. Diff discipline: When storing modified/ files for step, include only files that differ from the original baseline (not from the previous step). Execution discipline: Execute COMMAND LIST verbatim: do not change the order, arguments, flags, prefixes (e.g., env vars), or wrap the commands. Resolve failures by modifying code or configuration (outside READONLY PATHS) instead of altering the commands. Other rules: Atomic changes: Prefer small, testable edits. Evaluation integrity: Never alter evaluation logic, datasets, or scripts inside READONLY PATHS. Idempotence: If prior step succeeded, avoid regressing it. Important constraint: Under no circumstances may you halt while the current step count MAX ITERS; you must continue the modify execute COMMAND LIST diagnose/fix loop. arXiv preprint"
        },
        {
            "title": "Optimization Directions",
            "content": "Optimization directions: The global setting, OPTIMIZATION DIRECTION, defines the default direction for all metrics and accepts either higher or lower. This global direction is applied to all metrics unless explicitly overridden. For more granular control, PER METRIC DIRECTION provides way to override the global setting by specifying mapping of individual metric names to their desired optimization direction. If optimization target metrics (TARGET METRICS) and Optimization goal filtering: dataset (TARGET DATASETS) names are provided, treat them as strict optimization goals: focus exclusively on improving the specified metrics on the specified datasets, and ignore all other metrics and datasets. Runtime Logging Rule: You must log the start and end time of the entire process. Record wall-clock timestamps when you begin and exit the loop. Save timestamps to ./ agent runs/process time log.txt with format: start_time: 2025-08-14 13:04:22 end_time: 2025-08-14 14:37:55 duration_seconds: 5573 CRITICAL EXECUTION REQUIREMENT For ALL commands in COMMAND LIST that are expected to take more than 5 minutes (especially training commands with epochs > 10), you MUST: 1. ALWAYS use run in background=True when executing these commands with the Bash tool 2. Monitor the background process using BashOutput tool with the returned bash id 3. Wait for completion by periodically checking BashOutput until the process finishes 4. Only proceed to the next command after confirming the previous background process has completed successfully Example execution pattern: result = Bash( command=\"python main.py ... --num_epochs 100 ...\", run_in_background=True, description=\"Training in background\" # MANDATORY for long commands ) shell_id = result.split( \"Background shell started with ID: \" )[1].split(\"n\")[0] # Monitor until completion while True: output = BashOutput(bash_id=shell_id) if \"still running\" not in output: break time.sleep(30) # Check every 30 seconds FAILURE TO USE BACKGROUND EXECUTION FOR LONG-RUNNING COMMANDS WILL BE CONSIDERED CRITICAL ERROR. Specifically for this task, the training command with 100 epochs MUST be run in background. 26 arXiv preprint"
        },
        {
            "title": "Reporting",
            "content": "After each iteration, output short report with: count Idea/Plan/Solution (current) 36 bullet points Changes Made list of files edited with 1-line rationale each Command Results success/failure per command with exit code If success: brief summary of final info.json key metrics Next Steps what youll try next (or stop if count > MAX ITERS)"
        },
        {
            "title": "Begin with Template Variables",
            "content": "TASK DESCRIPTION: STARTER FILE PATHS: [ ] READONLY PATHS: [] ORIGINAL BASELINE RESULTS PATH: TARGET METRICS: [] TARGET DATASETS: [] OPTIMIZATION DIRECTION: PER METRIC DIRECTION = { } COMMAND LIST: [ ] MAX ITERS: RESULT DIR:"
        }
    ],
    "affiliations": [
        "National University of Singapore",
        "Tsinghua University",
        "University of Minnesota"
    ]
}