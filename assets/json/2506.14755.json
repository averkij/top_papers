{
    "paper_title": "Optimizing Length Compression in Large Reasoning Models",
    "authors": [
        "Zhengxiang Cheng",
        "Dongping Chen",
        "Mingyang Fu",
        "Tianyi Zhou"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large Reasoning Models (LRMs) have achieved remarkable success, yet they often suffer from producing unnecessary and verbose reasoning chains. We identify a core aspect of this issue as \"invalid thinking\" -- models tend to repeatedly double-check their work after having derived the correct answer. To address this specific inefficiency, we move beyond the general principles of Efficacy and Efficiency to propose two new, fine-grained principles: Brevity, which advocates for eliminating redundancy, and Sufficiency, which ensures critical reasoning steps are preserved. Guided by these principles, we introduce LC-R1, a post-training method based on Group Relative Policy Optimization (GRPO). LC-R1 employs a novel combination of a Length Reward for overall conciseness and a Compress Reward that is specifically designed to remove the invalid portion of the thinking process. Extensive experiments on multiple reasoning benchmarks demonstrate that LC-R1 achieves a significant reduction in sequence length (~50%) with only a marginal (~2%) drop in accuracy, achieving a favorable trade-off point on the Pareto frontier that prioritizes high compression. Our analysis further validates the robustness of LC-R1 and provides valuable insights for developing more powerful yet computationally efficient LRMs. Our code is released at https://github.com/zxiangx/LC-R1."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 1 ] . [ 1 5 5 7 4 1 . 6 0 5 2 : r a"
        },
        {
            "title": "Optimizing Length Compression in Large Reasoning Models",
            "content": "Zhengxiang Cheng 1 Dongping Chen 1 Mingyang Fu 1 Tianyi Zhou"
        },
        {
            "title": "Abstract",
            "content": "Large Reasoning Models (LRMs) have achieved remarkable success, yet they often suffer from producing unnecessary and verbose reasoning chains. We identify core aspect of this issue as invalid thinking models tend to repeatedly doublecheck their work after having derived the correct answer. To address this specific inefficiency, we move beyond the general principles of Efficacy and Efficiency to propose two new, fine-grained principles: Brevity, which advocates for eliminating redundancy, and Sufficiency, which ensures critical reasoning steps are preserved. Guided by these principles, we introduce LC-R1, posttraining method based on Group Relative Policy Optimization (GRPO). LC-R1 employs novel combination of Length Reward for overall conciseness and Compress Reward that is specifically designed to remove the invalid portion of the thinking process. Extensive experiments on multiple reasoning benchmarks demonstrate that LC-R1 achieves significant reduction in sequence length (50%) with only marginal (2%) drop in accuracy, achieving favorable trade-off point on the Pareto frontier that prioritizes high compression. Our analysis further validates the robustness of LC-R1 and provides valuable insights for developing more powerful yet computationally efficient LRMs. Our code is released at https://github.com/zxiangx/LC-R1. Figure 1: Comparison between inefficient reasoning model and efficient model. The former tends to make verbose self-check process after having derived the correct answer corresponding to the given question, resulting in the inefficient reasoning. The model trained with LC-R1 get more efficient reasoning process to get correct answer, without any invalid thinking process. 1. Introduction Recent long-thought Large Reasoning Models (LRMs), such as OpenAIs O1 (Jaech et al., 2024) and Deepseek-R1 (DeepSeek-AI et al., 2025), represent significant paradigm extension of foundational Chain-of-Thought (CoT) techniques (Wei et al., 2023). Fine-tuned with Reinforcement Learning (RL), these models iteratively refine solutions to achieve unprecedented performance in complex reason1University of Maryland. Correspondence to: Tianyi Zhou <tianyi.david.zhou@gmail.com>. Preprint. Under review. ing tasks like mathematics and programming (Sun et al., 2025; Gu et al., 2024). However, with the improvement of deep thinking ability, prominent problem is the excessive consumption of computing resources during the reasoning process (Chen et al., 2025; Aggarwal and Welleck, 2025). Specifically, existing models tend to generate lengthy and even unnecessary chains of reasoning when solving problems with low complexity or clear solution paths. This phenomenon, termed overthinking, manifests as models consuming far more computational resources than the problem itself requires to reach the correct conclusion (Chen et al., 2024; Sui et al., 2025; Cuadron et al., 2025). There-"
        },
        {
            "title": "Optimizing Length Compression in Large Reasoning Models",
            "content": "Figure 2: Pareto analysis of the Efficacy-Efficiency trade-off of different methods on two reasoning models. The x-axis represents the reasoning length change, and the y-axis shows the accuracy change, relative to the original model (defined in Eq. 12), with the top-left corner representing the ideal position. smaller and darker marker indicates higher Valid Thinking (VT) rate (defined in Eq. 1), signifying more efficient thinking process. Compared to other methods also on the pareto frontier, LC-R1 achieves more favorable trade-off, attaining substantially higher compression rate at the cost of minimal drop in accuracy, and it also achieves higher VT rate. The sub-optimal performance of our ablation variants (w/o C-reward, w/o L-reward) further proves the criticality of our dual-reward designs. fore, one critical problem arises: How can we maintain high reasoning efficacy while significantly improving efficiency? Prior works have approached this by fine-tuning on shorter demonstrations (SFT) (Chen et al., 2024), constructing preference datasets for conciseness (Luo et al., 2025a; Shen et al., 2025), or integrating length-penalties into RL (Hou et al., 2025; Luo et al., 2025b; Team et al., 2025). However, these methods often treat the reasoning process as black box, penalizing length without analyzing the internal structure of the thoughts themselves. To address this gap, we delve into the structure of overthinking and identify specific pattern: models frequently engage in redundant double-checking after having already derived the correct answer. We term this phenomenon invalid thinking, as shown in Figure 1. To quantify it, we introduce new metric, Valid Thinking (VT) Rate, which measures the proportion of the reasoning process that is essential for reaching the initial correct conclusion. Guided by this insight, we propose two fine-grained principles: Brevity (eliminating redundancy) and Sufficiency (preserving necessary steps). We then introduce LC-R1, GRPO-based post-training method that operationalizes these principles. LC-R1 uniquely combines Length Reward for overall conciseness with novel Compress Reward designed to directly guide the model to terminate the thinking process upon deriving the correct answer. We conduct comprehensive experiments on two reasoning models across seven benchmarks. Empirical results show that LC-R1 achieves more favorable trade-off between efficacy and efficiency than prior methods as shown in Figure 2. Specifically, with only 2% drop in accuracy, our method attains 50% reduction in sequence length on average. Ablation study also demonstrates the indispensability of both Length Reward and Compress Reward for achieving efficient reasoning. Further study shows that our method achieves efficient compression without impairing the exploration ability of model, and the efficiency can generalize to various difficulty problems. In conclusion, our contribution can be summarized as follows: We analyze the thinking process of current competitive reasoning model and find the phenomenon of invalid thinking : It takes large portion of thinking process to double check after having derived the correct answer, making the reasoning verbose and inefficient. We propose two novel principles: Brevity and Sufficiency, and design GRPO-based method LC-R1 for LRM post-training to strike balance between Brevity and Sufficiency, pruning invalid thinking while compressing overall sequences at the same time. Through comprehensive experiments, we validate the effectiveness of LC-R1 to get better trade-off between Efficacy and Efficiency, and conduct further analyses on the deep impact of compression, proving the robustness of LC-R1 to various difficulties and providing insights for future works."
        },
        {
            "title": "Optimizing Length Compression in Large Reasoning Models",
            "content": "Table 1: Valid Thinking Rate of current state-of-the-art Large Reasoning Models. Nemotron indicates Llama-3.3-NemotronSuper-49b-v1. Results manifest low VT rate on all these models, highlighting the phenomenon of invalid thinking."
        },
        {
            "title": "Model",
            "content": "Qwen-3-32B QwQ-32B DeepSeek-R1 Nemotron Avg. 57.5 59.2 65.3 60.8 AIME"
        },
        {
            "title": "AMC",
            "content": "GSM8K MATH500 Olympiad 73.8 70.8 66.5 62.1 58.8 58.2 71.8 64.1 53.8 54.1 64.2 63.1 46.6 53.1 59.8 56.6 51.5 59.6 64.0 58. 2. Preliminary: Compression and Efficienct"
        },
        {
            "title": "Reasoning Models",
            "content": "2.1. Motivation: Quantifying Redundant Reasoning common paradigm for Large Reasoning Models (LRMs) involves thinking process (i.e., step-by-step rationale) that precedes the final answer. While effective for accuracy, we observe consistent inefficiency: models often derive the correct answer early in their thinking process but continue with lengthy and redundant verification steps. We term this subsequent, non-essential reasoning Redundant Sequence. To formalize this, we define the Valid Thinking (VT) rate, metric focusing on the models thinking process: VT = Tokens in Valid Thinking Total tokens in Thinking Process (1) where Valid Thinking comprises the tokens from the start of the thinking process until the correct answer is first derived. To automate this measurement, we utilize lightweight parser, LC-Extractor, whose implementation details are provided in Section 4. we evaluated four state-of-the-art LRMsQwen3-32b (Team, 2025a), QwQ-32b (Team, 2025b), Deepseek-R1 (DeepSeek-AI et al., 2025), and Llama-3.3-nemotron-super49b-v1 (Bercovich et al., 2025)across five math benchmarks: AIME25, MATH500, GSM8K, AMC, OlympiadBench. Our analysis reveals universal and severe overthinking problem. As shown in Table 1, all models tested exhibit low VT rates, indicating that substantial portion of their computational effort (often 35-45%) is spent on redundant reasoning after the solution has been found. This widespread inefficiency confirms the significant potential for compression and motivates our work. thinking steps. To create more targeted framework, we refine these concepts by introducing two new, complementary principles: Brevity refines Efficiency by shifting the focus from generic length reduction to the specific elimination of Redundant Sequence. While conventional methods may still produce compressed sequence that contains unnecessary double-checks, Brevity advocates for the model to terminate its reasoning process as soon as the correct answer is found. Sufficiency acts as crucial safeguard for Efficacy. It mandates that, in the pursuit of Brevity, no critical logical steps essential for reaching correct answer are omitted. It ensures that the compressed reasoning remains complete and logically sound. Therefore, the ideal reasoning model must navigate the tension between these principles: it should be maximally Brief by removing all non-essential thinking, yet always remain Sufficient to guarantee correctness. Our work, LCR1, is explicitly designed to optimize for this balance. 3. LC-R1: Length Compression with Efficient"
        },
        {
            "title": "Reasoning Principles",
            "content": "In this section, we propose LC-R1, GRPO-based posttraining algorithm designed to address the invalid thinking phenomenon and enhance reasoning efficiency. Guided by the principles of Brevity and Sufficiency introduced in Section 2.2, LC-R1 employs novel dual-reward system. This system combines global Length Reward for overall conciseness with targeted Compress Reward that specifically removes redundant reasoning. The complete pipeline of LC-R1 is illustrated in Figure 3 and Algorithm 1. 2.2. Principles for Efficient Reasoning 3.1. Problem Formulation The evaluation of reasoning models traditionally rests on two pillars: Efficiency (the computational cost, often proxied by output length) and Efficacy (the ability to solve the problem correctly). However, simply shortening the output is coarse approach that may inadvertently remove critical Let be the model and be the given query. The output is M(q), where = cat(R, A) consists of reasoning part and an answer part A, split by the token </think>, which is considered part of A. For the reasoning part R, we denote its effective prefix as the content from the"
        },
        {
            "title": "Optimizing Length Compression in Large Reasoning Models",
            "content": "Figure 3: An overview of the LC-R1 training three-stage pipeline. (1) Valid Segment Extraction: First, an extractor model processes the original reasoning traces to identify the valid thinking portion and generate compressed sequences. (2) Reward Calculation: Next, these compressed sequences are used to compute our dual rewardsLength Reward and Compress Reward, with the latter applied exclusively as bonus or penalty on the final </think> token. These are then combined to calculate the final advantages. (3) Policy Optimization: Finally, the GRPO loss is calculated using the compressed sequences and corresponding advantages, steering the model toward more concise and efficient reasoning. beginning of up to the first occurrence of the correct answer corresponding to the query q. If does not contain the correct answer, then we define = R. We define two functions as follows: t({R, A}) = R, ({R, A}) = {R, A} (2) The function extracts the reasoning process from the output and function extracts the concise reasoning part and concatenates it with the answer A. We denote oi as the original model output and = (oi) as the refined compressed output. LC-R1 is GRPO-based method to efficiently compress the reasoning process. Within group, let denote the set of indices where sequences oi leading to the correct answer corresponding to the query q, and be the set of indices where oj leading to wrong answer, The total group size is = + W. 3.2. Reward and Objective Design where Our methods reward system consists of two core components: the Length Reward for reducing overall output length, and the Compress Reward for targeting redundant parts of the models reasoning. Length Reward. To compress the total length of the model output, we propose adding length penalty during the GRPO training process. Leveraging the group-based sampling of GRPO, we can calculate the relative length reward to automatically adjust to the difficulty of the problem. And we define the Length Reward as follows: ri,length = (cid:40) 1 i maxjC , 0, if if (3) This formulation uses the maximum length of correct, compressed sequence within the group as normalizer. The final reward combines this with base reward for format and accuracy, and is normalized by subtracting the group mean, following Liu et al. (2025) to obtain an unbiased gradient: ri = ri,base + α ri,length ri,combine = ri mean({rj}G j=1) ri,base = ri,format + ri,accuracy (4) (5) (6) Following prior work, ri,format and ri,accuracy are binary rewards to judge whether the model places its thinking process between <think> and </think> and whether the sample"
        },
        {
            "title": "Optimizing Length Compression in Large Reasoning Models",
            "content": "Algorithm 1 LC-R1: Length Compress for R1-style model Input: Initial policy model πθ, compression function (), task prompts D, hyperparameters α, β, µ Output: Trained policy model πθ 1: for step = 1, . . . , do 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: end for 12: return πθ Sample batch Db from Update the old policy model πθold πθ Sample outputs {oi}G Apply compression to all outputs: (oi) Compute combined reward ri,combine (Eq. 5) and compress reward ri,compress (Eq. 11) Compute token-level advantages ˆAi,t for each compressed output for iteration = 1, . . . , µ do Update the policy model πθ by maximizing the objective JGRPO (Eq. 7) i=1 πθold(q) for each question Db (Eq. 10) end for leads to the correct answer corresponding to the query verified by Math-Verify1 respectively. α is hyperparameter that controls the weight of the Length Reward. Compress Reward. For the original GRPO method, the loss calculation is based on the models own sampling results. In order to drive the model to terminate the thinking process when getting the correct answer for achieving Brevity, we modify the GRPO objective as follows: JGRPO(θ) = (cid:34) 1 i=1 i (cid:80)G i= t=1 i=1πθold (Oq) qP (Q),{oi}G (cid:40) (cid:16) min(cid:2)Rt(θ) ˆAi,t, clip i (cid:88) (cid:88) Rt(θ), (7) 1 ϵ, 1 + ϵ (cid:17) ˆAi,t (cid:3) β DKL (cid:0)πθ(q) πref(q)(cid:1) (cid:41)(cid:35) where"
        },
        {
            "title": "DKL",
            "content": "(cid:0)πθ πref (cid:1) = πref (o πθ(o iq) iq) = (oi), Rt(θ) = log πθ(o πθold(o πref (o πθ(o i,tq, i,tq, iq) iq) i,<t) i,<t) 1 (8) (9) Our key modification to the standard GRPO objective is that the loss is calculated over the compressed trajectories i, rather than the original full trajectories oi. We define the token-level advantages ˆAi,t as follows: ˆAi,t = ri,combine + γ I(o i,t = </think>) ri,compress (10) where ri,compress = 1 t(o i) t(oi) , 1, 0, if & ans(q) t(o i) if & ans(q) / t(o i) if 1https://github.com/huggingface/Math-Verify (11) 5 Let ans(q) be the ground truth answer for given query q. In this setting, we place focuses on steering model towards outputting </think> token when getting the correct answer (at the end of i) during the thinking process to achieve compressing the verbose token, conforming to the principle of Brevity. We only give an extra reward to this token, avoiding place unnecessary emphasis on other tokens to make the training process more efficient and stable. We define the reward to be the portion of Redundant Sequence, formulated by 1 t(o i) t(oi) , representing the efficiency distinction between sequences before and after compression. The hyperparameter γ scales this bonus. Based on the principle of Sufficiency, the model should engage in sufficient reasoning process, avoiding overhead compression at the cost of accuracy degradation. Therefore, we impose large penalty (-1) to the token </think> if the model terminates its reasoning before finding the correct answer, which discourages harmful over-compression and provides robustness to the training process. To further validate the effectiveness of our method, we follow DAPO (Yu et al., 2025) to calculate the objection across all tokens in group, instead of averaging the token rewards within single sequence, which eliminates the original GRPO methods preference for short-correct sequences and long-incorrect sequences, facilitating the validation of our methods effectiveness. 4. Experiments 4.1. Experiment Setups Backbone Models. We choose two representative reasoning models: DeepSeek-R1-Distill-Qwen-7B/1.5B (DeepSeek-AI et al., 2025) as our backbone models, which have demonstrated strong performance on mathematical and coding reasoning tasks."
        },
        {
            "title": "Optimizing Length Compression in Large Reasoning Models",
            "content": "Table 2: Accuracy (above) and Sequence Length (below) for all methods across seven benchmarks. AVG shows the relative change in accuracy and length compared to the Base model (+ increase, - decrease). GPQA-D denotes GPQA-Diamond benchmark, and LCB denotes the pass@10 score on LiveCodeBench. VT represents the Valid Thinking ratio. For each column, the best performing score is marked in bold, and the second-best is underlined. Method AIME25 MATH500 GSM8K Olympiad AMC GPQA-D LCB Avg VT Base SFT DPO O1-Pruner ThinkPrune SFT+O1-Pruner LC-R1 (Ours) Base SFT DPO O1-Pruner ThinkPrune SFT+O1-Pruner LC-R1 (Ours) 37.7 (11007) 36.6 (9457) 36.9 (9718) 35.0 (8263) 38.0 (9309) 35.5 (9466) 36.2 (7150) 22.8 (12129) 20.5 (10639) 19.4 (10316) 23.2 (8731) 24.1 (7960) 17.5 (9075) 21.2 (6434) 92.6 (3832) 90.2 (2497) 91.4 (2277) 91.5 (2268) 93.1 (3253) 91.0 (2245) 90.4 (1568) 83.7 (4869) 81.4 (3045) 79.0 (2749) 84.3 (2913) 84.5 (3518) 80.2 (2769) 82.5 (2233) DeepSeek-R1-Distill-Qwen-7B 91.6 (1842) 91.9 (946) 90.3 (980) 91.1 (1012) 91.2 (1546) 89.7 (920) 88.1 (450) 59.7 (7342) 56.0 (6329) 56.2 (6338) 59.6 (4712) 60.8 (6225) 56.0 (5807) 58.7 (4041) 81.2 (6715) 78.7 (5231) 78.6 (5122) 77.1 (4510) 82.7 (5510) 76.6 (5133) 79.1 (3453) 46.6 (6508) 39.8 (8217) 37.2 (8109) 45.5 (5012) 50.3 (6508) 43.9 (6425) 47.2 (4604) DeepSeek-R1-Distill-Qwen-1.5B 83.4 (2294) 81.3 (1134) 80.9 (855) 82.7 (1162) 84.1 (1690) 81.5 (919) 82.7 (841) 44.2 (9258) 42.7 (7637) 41.1 (6544) 47.1 (5960) 44.9 (6250) 40.0 (6411) 43.2 (4333) 61.2 (8696) 59.7 (6608) 56.7 (5912) 65.1 (5131) 63.4 (5897) 58.7 (5553) 61.7 (3947) 34.5 (8516) 22.4 (10217) 19.8 (9438) 32.1 (6173) 33.6 (5576) 25.0 (7410) 33.6 (4489) 68.8 (8878) 67.3 (8739) 66.9 (8755) 66.7 (5901) 67.8 (7180) 66.8 (7267) 69.0 (6059) 43.1 (10120) 39.8 (10597) 39.2 (10287) 42.5 (7305) 42.7 (7226) 39.4 (8488) 42.4 (5722) 58.72% 4.46% (15.54%) 5.26% (16.18%) 2.79% (33.71%) +1.58% (14.13%) 4.31% (24.19%) 1.84% (46.32%) 95.64% 96.34% 69.30% 77.16% 85.22% 97.14% 56.06% 9.13% (16.74%) 12.79% (24.30%) +0.89% (35.64%) +1.31% (30.89%) 11.34% (32.04%) 2.14% (51.86%) 95.54% 98.38% 78.20% 65.62% 91.38% 98.64% LC-Extractor. To accurately identify and extract the valid reasoning part, we develop specialized parser to implement the extraction function mentioned in Eq. 2, termed LC-Extractor. We finetune Qwen2.5-3B-Instruct for its lightweight and easy enough to run. Detailed experiment settings are provided in Appendix B. Dataset. We used mixed-difficulty dataset, combining past AIME competition problems with the MATH dataset in an approximate 1:2 ratio to create 2500 training samples. This approach enables the model to learn length compression across problems of varying difficulty. Evaluation. We test our models performance on seven datasets, including AIME25, MATH500, GSM8K, AMC, OlympiadBench, GPQA-Diamond and LiveCodeBench, across math, general and code tasks, to evaluate the efficiency of reasoning comprehensively. We use averaged Pass@1 as our primary metric. For each test, we sample times, setting top-p = 0.95 and temperature = 0.7. For AIME25, we set = 64, while for the other test sets, we set = 8. We set the maximum length to 16384. Additionally, we calculate the average fluctuate ratio on accuracy and token lengths compared with base model on every benchmark, which can be formulated as follows: Avgacc = mean7 i=1 Avglen = mean7 i= 6 (cid:110) Accmodel Accbase (cid:111) Accbase Lenbase (cid:110) Lenmodel (cid:111)"
        },
        {
            "title": "Lenbase\ni",
            "content": "(12) (13)"
        },
        {
            "title": "Optimizing Length Compression in Large Reasoning Models",
            "content": "Table 3: Ablation study on the contribution of Length Reward and Compress Reward to the compression process. The study manifest the sub-optimal performance of them, varifying each of them makes big contribution to the efficient reasoning."
        },
        {
            "title": "Method",
            "content": "AIME25 MATH500 GSM8K Olympiad AMC GPQA-D"
        },
        {
            "title": "Avg",
            "content": "VT LC-R1 (Ours) w/o L-reward w/o C-reward LC-R1 (Ours) w/o L-reward w/o C-reward 36.2 (7150) 36.1 (9309) 37.6 (8738) 21.2 (6434) 21.3 (7061) 21.9 (7988) 90.4 (1568) 91.3 (2316) 92.9 (2498) 82.5 (2233) 81.2 (2270) 83.2 (2965) DeepSeek-R1-Distill-Qwen-7B 88.1 (450) 90.6 (696) 91.1 (1012) 58.7 (4041) 59.4 (5779) 59.1 (5344) 79.1 (3453) 79.0 (5021) 80.5 (4741) 47.2 (4604) 45.9 (6273) 48.9 (5727) DeepSeek-R1-Distill-Qwen-1.5B 82.7 (841) 83.3 (754) 84.1 (1160) 43.2 (4333) 43.4 (5024) 44.0 (5363) 61.7 (3947) 62.2 (4478) 63.4 (5192) 33.6 (4489) 30.6 (5021) 30.1 (5847) 69.0 (6059) 68.0 (8023) 68.5 (6893) 42.4 (5722) 41.9 (6378) 43.7 (6874) 1.84% (46.32%) 1.80% (25.28%) +0.31% (27.35%) 2.14% (51.86%) 3.42% (47.79%) 1.70% (38.35%) 97.14% 93.16% 72.24% 98.64% 95.16% 71.10% We also test VT for each model to evaluate the Brevity of the thinking process to investigate the ability of these methods to mitigate the invalid thinking phenomenon. We test VT on five math benchmarks and calculate the mean value, for the convenience of extracting the standard and formatted correct answer from the thinking process on math problems. 4.2. Baselines Supervised Fine-tuning (SFT). Inspired by OVERTHINK (Chen et al., 2024), which proposes using only the initial correct solution for fine-tuning, we construct an SFT dataset of 5000 samples by removing the Redundant Sequence from self-generated outputs. Direct Preference Optimization (DPO) (Rafailov et al., 2023). We create preference dataset of 5000 samples from the MATH dataset, where the shortest correct answer is treated as the chosen response and the longest as the rejected response. This DPO training is applied to the SFT-tuned model. O1 Pruner (Luo et al., 2025b). PPO-like offline finetuning method to significantly compress CoT length while maintaining performance. We follow its methodology using 10000 samples from the MATH dataset. ThinkPrune-3K (Hou et al., 2025). reinforcement learning approach that uses length-truncation reward for multi-stage compression. We reproduce the ThinkPrune-3k variant, which is reported to be highly efficient, with slight accuracy degradation. SFT + O1-Pruner. To better understand the effect of compressing the thinking process and pruning the overall sequences at the same time, we also compare with two-stage training approach, combining SFT and O1 Pruner. 4.3. Experiment Results LC-R1 outperforms other methods with competitive performance and fewer tokens. As presented in Table 2, On the 7B model, LC-R1 achieves an average length reduction of 46.32%, substantially higher than all other baselines, with mere 1.84% drop in average accuracy. Similarly, on the 1.5B model, it attains 51.86% length reduction for 2.14% accuracy decrease. This efficiency does not appear to compromise its generalization, as it demonstrates more robust performance on out-of-distribution (OOD) benchmarks like GPQA-Diamond and LiveCodeBench compared to other high-compression methods. Figure 2 shows our method achieves more favorable Efficacy-Efficiency trade-off by enabling maximal compression ratio with negligible accuracy degradation. LC-R1 also achieves significantly higher VT rate (over 97%) compared to other methods like O1-Pruner (70-78%) and ThinkPrune (66-77%), demonstrating the superior efficiency of our approach. Combining length and compress reward brings superior efficiency to reasoning. Our ablation study on the Length Reward (L-reward) and Compress Reward (C-reward), presented in Table 3, reveals their critical complementary relationship. The analysis reveals that while each component alone yields competitive resultspositioning them near the Pareto frontier of performance versus compression efficiencycombining them can achieve more optimal balance. Specifically, using L-reward alone achieves sig-"
        },
        {
            "title": "Optimizing Length Compression in Large Reasoning Models",
            "content": "Figure 4: The impact of LC-R1 compression method on the AIME25 benchmark. Left: The Pass@k scores show that LC-R1 models maintain competitive performance compared to the originals, preserving the models potential. Right: Per-problem analysis on Deepseek-R1-Distill-Qwen-7B reveals that LC-R1 achieves similar Pass@1 accuracy while maintaining consistent token compression ratio across problems of varying difficulty, demonstrating universal compression effect. nificant compression but with lower VT rate. Conversely, C-reward alone ensures high VT by precisely removing redundancy, but with limited overall compression. Our full LC-R1 method successfully integrates these strengths, achieving both the highest compression efficiency and the highest VT rate while maintaining comparable accuracy, proving that the synergy between both rewards is indispensable for achieving maximum reasoning efficiency. SFT shows limitation on generalization. While SFT achieves remarkably high VT rate (over 95%), its effectiveness is superficial. The models performance collapses on OOD benchmarks, indicating that it merely overfits to the structural brevity of the training data rather than learning generalizable, efficient reasoning policy. The poor performance of the hybrid SFT+O1-Pruner method further suggests that simple combination of off-the-shelf techniques is insufficient. These findings underscore the superiority of RL-based methods like LC-R1, which foster more robust and genuinely efficient reasoning skills. the compression. It suggests that the pruned invalid thinking segments are truly redundant and their removal does not diminish the models underlying knowledge or creative problem-solving potential. Compression remains consistent across varying problem difficulties. To analyze our methods behavior at microscopic level, we plot the per-problem pass@1 accuracy against the original models token consumption on the AIME25 benchmark (Figure 4 (right)). The plot reveals clear difficulty spectrum, where problems requiring more tokens from the base model generally correspond to lower pass@1 scores. Crucially, LC-R1 applies uniform and significant compression ratio across this entire spectrum, with per-problem outcomes (i.e., success or failure) remaining remarkably consistent with those of the base model. This provides strong evidence that LC-R1 functions as robust and difficulty-agnostic efficiency layer, successfully streamlining the reasoning process without altering the models core problem-solving logic for any specific problem. 5. Compression Impact Analysis 6. Conclusion Compression does not impact exploration capability. To investigate the deeper impact of compression on the models problem-solving potential, we sampled 256 times on AIME25 with maximal length = 32, 768 and test pass@k score on both models before and after compression. The results in Figure 4 (left) reveal key phenomenon: across the entire Pass@k evaluation range from = 1 to 128 on the AIME25 dataset, the performance curve of the model compressed by our LC-R1 method almost perfectly overlaps with that of the original model. This result strongly demonstrates that the models exploration ability to find correct solution through multiple attempts will not be injured by In this paper, we address the invalid thinking phenomenon existing in current LRMs, that they tend to double-check their work after correct answer has been derived. To tackle this, we introduced the principles of Brevity and Sufficiency and proposed LC-R1, RL-based post-training method that employs dual-reward system, compressing overall sequence length and pruning Redundant Sequence spontaneously. Extensive experiments demonstrate that LC-R1 achieves more favorable Efficacy-Efficiency trade-off. In further analysis, LC-R1 does not degrade the models exploration ability and and the compression effect remains robust across problems of varying difficulty."
        },
        {
            "title": "Impact Statement",
            "content": "In this paper, we address the invalid thinking phenomenon, key source of inefficiency in Large Reasoning Models where they engage in unnecessary verification after deriving correct answer. We introduce LC-R1, novel posttraining method featuring dual-reward system that encourages both overall conciseness and the specific elimination of this redundancy. Our experiments demonstrate that LC-R1 achieves more favorable trade-off between performance and efficiency than existing approaches. While our current validation focuses on models up to the 7B scale due to computational constraints, this work provides proven path toward developing more computationally frugal LRMs. By making advanced AI reasoning more efficient, we hope to make these powerful tools more scalable and accessible for wider range of applications."
        },
        {
            "title": "References",
            "content": "Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao ..., and Zhen Zhang. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. URL https://arxiv.org/ abs/2501.12948. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. Chainof-thought prompting elicits reasoning in large language models, 2023. URL https://arxiv.org/abs/2201.11903. Haoxiang Sun, Yingqian Min, Zhipeng Chen, Wayne Xin Zhao, Zheng Liu, Zhongyuan Wang, Lei Fang, and Ji-Rong Wen. Challenging the boundaries of reasoning: An olympiad-level math benchmark for large language models, 2025. URL https: //arxiv.org/abs/2503.21380. Alex Gu, Baptiste Rozière, Hugh Leather, Armando Solar-Lezama, Gabriel Synnaeve, and Sida I. Wang. Cruxeval: benchmark for code reasoning, understanding and execution. arXiv preprint arXiv:2401.03065, 2024. Qiguang Chen, Libo Qin, Jinhao Liu, Dengyun Peng, Jiannan Guan, Peng Wang, Mengkang Hu, Yuhang Zhou, Te Gao, and Wanxiang Che. Towards reasoning era: survey of long chainof-thought for reasoning large language models, 2025. URL https://arxiv.org/abs/2503.09567. Pranjal Aggarwal and Sean Welleck. L1: Controlling how long reasoning model thinks with reinforcement learning, 2025. URL https://arxiv.org/abs/2503.04697. Xingyu Chen, Jiahao Xu, Tian Liang, Zhiwei He, Jianhui Pang, Dian Yu, Linfeng Song, Qiuzhi Liu, Mengfei Zhou, Zhuosheng Zhang, et al. Do not think that much for 2+ 3=? on the overthinking of o1-like llms. arXiv preprint arXiv:2412.21187, 2024. Yang Sui, Yu-Neng Chuang, Guanchu Wang, Jiamu Zhang, Tianyi Zhang, Jiayi Yuan, Hongyi Liu, Andrew Wen, Hanjie Chen, Xia Hu, et al. Stop overthinking: survey on efficient reasoning for large language models. arXiv preprint arXiv:2503.16419, 2025. Alejandro Cuadron, Dacheng Li, Wenjie Ma, Xingyao Wang, Yichuan Wang, Siyuan Zhuang, Shu Liu, Luis Gaspar Schroeder, Tian Xia, Huanzhi Mao, et al. The danger of overthinking: Examining the reasoning-action dilemma in agentic tasks. arXiv preprint arXiv:2502.08235, 2025. Haotian Luo, Haiying He, Yibo Wang, Jinluan Yang, Rui Liu, Naiqiang Tan, Xiaochun Cao, Dacheng Tao, and Li Shen. Adar1: From long-cot to hybrid-cot via bi-level adaptive reasoning optimization. arXiv preprint arXiv:2504.21659, 2025a. Yi Shen, Jian Zhang, Jieyun Huang, Shuming Shi, Wenjing Zhang, Jiangze Yan, Ning Wang, Kai Wang, and Shiguo Lian. Dast: Difficulty-adaptive slow-thinking for large reasoning models, 2025. URL https://arxiv.org/abs/2503.04472. Bairu Hou, Yang Zhang, Jiabao Ji, Yujian Liu, Kaizhi Qian, Jacob Andreas, and Shiyu Chang. Thinkprune: Pruning long chainof-thought of llms via reinforcement learning. arXiv preprint arXiv:2504.01296, 2025. Haotian Luo, Li Shen, Haiying He, Yibo Wang, Shiwei Liu, Wei Li, Naiqiang Tan, Xiaochun Cao, and Dacheng Tao. O1-pruner: Length-harmonizing fine-tuning for o1-like reasoning pruning, 2025b. URL https://arxiv.org/abs/2501.12570. Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, Chuning Tang, Congcong Wang, Dehao Zhang, Enming Yuan, Enzhe Lu, Fengxiang Tang, Flood Sung, Guangda Wei, Guokun Lai, Haiqing Guo, Han Zhu, Hao Ding, Hao Hu, Hao Yang, Hao Zhang, Haotian Yao, Haotian Zhao, Haoyu Lu, Haoze Li, Haozhen Yu, Hongcheng Gao, Huabin Zheng, Huan Yuan, Jia Chen, Jianhang Guo, Jianlin Su, Jianzhou Wang, Jie Zhao, Jin Zhang, Jingyuan Liu, Junjie Yan, Junyan Wu, Lidong Shi, Ling Ye, Longhui Yu, Mengnan Dong, Neo Zhang, Ningchen Ma, Qiwei Pan, Qucheng Gong, Shaowei Liu, Shengling Ma, Shupeng Wei, Sihan Cao, Siying Huang, Tao Jiang, Weihao Gao, Weimin Xiong, Weiran He, Weixiao Huang, Weixin Xu, Wenhao Wu, Wenyang He, Xianghui Wei, Xianqing Jia, Xingzhe Wu, Xinran Xu, Xinxing Zu, Xinyu Zhou, Xuehai Pan, Y. Charles, Yang Li, Yangyang Hu, Yangyang Liu, Yanru Chen, Yejie Wang, Yibo Liu, Yidao Qin, Yifeng Liu, Ying Yang, Yiping Bao, Yulun Du, Yuxin Wu, Yuzhi Wang, Zaida Zhou, Zhaoji Wang, Zhaowei Li, Zhen Zhu, Zheng Zhang, Zhexu Wang, Zhilin Yang, Zhiqi Huang, Zihao Huang, Ziyao Xu, Zonghan Yang, and Zongyu Lin. Kimi k1.5: Scaling reinforcement learning with llms, 2025. URL https://arxiv.org/abs/2501.12599. Qwen Team. Qwen3, April 2025a. URL https://qwenlm. github.io/blog/qwen3/. Qwen Team. Qwq-32b: Embracing the power of reinforcement learning, March 2025b. URL https://qwenlm.github.io/ blog/qwq-32b/. Akhiad Bercovich, Itay Levy, Izik Golan, Mohammad Dabbah, Ran El-Yaniv, Omri Puny, Ido Galil, Zach Moshe, Tomer Ronen, Najeeb Nabwani, Ido Shahaf, Oren Tropp, Ehud Karpas, Ran Zilberstein, Jiaqi Zeng, Soumye Singhal, Alexander Bukharin, ..."
        },
        {
            "title": "Optimizing Length Compression in Large Reasoning Models",
            "content": "Qwen Team. Qwen2.5: party of foundation models, September 2024. URL https://qwenlm.github.io/blog/qwen2.5/. Google. Gemini 2.5 flash. https://developers.googleblog. com/en/start-building-with-gemini-25-flash/, 2025b. Ping Yu, Jing Xu, Jason Weston, and Ilia Kulikov. Distilling system 2 into system 1, 2024. URL https://arxiv.org/abs/ 2407.06023. International Conference on Artificial Intelligence in Medicine. The 23rd international conference on artificial intelligence in medicine (aime 2025). https://aime25.aimedicine.info/. Accessed: 2025-06-10. Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step. In The Twelfth International Conference on Learning Representations, 2023. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. Mathematical Association of America. ematics competitions student-programs/amc/. Accessed: 2025-06-10. (amc). American mathhttps://maa-amc.org/ David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel Bowman. Gpqa: graduate-level google-proof q&a benchmark. In First Conference on Language Modeling, 2024. Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando Solar-Lezama, Koushik Sen, and Ion Stoica. Livecodebench: Holistic and contamination free evaluation of large language models for code. arXiv preprint arXiv:2403.07974, 2024. doi: 10.48550/arXiv.2403.07974. Leandro von Werra, Younes Belkada, Lewis Tunstall, Edward Beeching, Tristan Thrush, Nathan Lambert, Shengyi Huang, Kashif Rasul, and Quentin Gallouédec. Trl: Transformer reinforcement learning. https://github.com/huggingface/trl, 2020. Yian Zhang, and Chris Alexiuk. Llama-nemotron: Efficient reasoning models, 2025. URL https://arxiv.org/abs/2505. 00949. Zichen Liu, Changyu Chen, Wenjun Li, Penghui Qi, Tianyu Pang, Chao Du, Wee Sun Lee, and Min Lin. Understanding r1-zerolike training: critical perspective, 2025. URL https:// arxiv.org/abs/2503.20783. Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Tiantian Fan, Gaohong Liu, Lingjun Liu, Xin Liu, et al. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476, 2025. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. Advances in Neural Information Processing Systems, 36:53728 53741, 2023. OpenAI. Chatgpt. https://openai.com/o1/, 2024. Google. Gemini 2.5 pro. https://cloud.google.com/ vertex-ai/generative-ai/docs/models/gemini/ 2-5-pro, 2025a. Marah Abdin, Sahaj Agarwal, Ahmed Awadallah, Vidhisha Balachandran, Harkirat Behl, Lingjiao Chen, Gustavo de Rosa, Suriya Gunasekar, Mojan Javaheripi, Neel Joshi, Piero Kauffmann, Yash Lara, Caio César Teodoro Mendes, Arindam Mitra, Besmira Nushi, Dimitris Papailiopoulos, Olli Saarikivi, Shital Shah, Vaishnavi Shrivastava, Vibhav Vineet, Yue Wu, Safoora Yousefi, and Guoqing Zheng. Phi-4-reasoning technical report, 2025. URL https://arxiv.org/abs/2504.21318. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models, 2024. URL https:// arxiv.org/abs/2402.03300. Jiawei Liu and Lingming Zhang. Code-r1: Reproducing r1 for code with reliable rewards. 2025. Jian Hu, Jason Klein Liu, and Wei Shen. Reinforce++: An efficient rlhf algorithm with robustness to both prompt and reward models, 2025. URL https://arxiv.org/abs/2501.03262. Xinyin Ma, Guangnian Wan, Runpeng Yu, Gongfan Fang, and Xinchao Wang. Cot-valve: Length-compressible chain-of-thought tuning, 2025a. URL https://arxiv.org/abs/2502.09601. Daman Arora and Andrea Zanette. Training language models to reason efficiently, 2025. URL https://arxiv.org/abs/2502. 04463. Simon A. Aytes, Jinheon Baek, and Sung Ju Hwang. Sketchof-thought: Efficient llm reasoning with adaptive cognitiveinspired sketching, 2025. URL https://arxiv.org/abs/ 2503.05179. Tingxu Han, Zhenting Wang, Chunrong Fang, Shiyu Zhao, Shiqing Ma, and Zhenyu Chen. Token-budget-aware llm reasoning. arXiv preprint arXiv:2412.18547, 2024. Wenjie Ma, Jingxuan He, Charlie Snell, Tyler Griggs, Sewon Min, and Matei Zaharia. Reasoning models can be effective without thinking, 2025b. URL https://arxiv.org/abs/2504. 09858."
        },
        {
            "title": "Optimizing Length Compression in Large Reasoning Models",
            "content": "A. Related Work Reinforcement Learning For Reasoning. Reinforcement learning (RL) has emerged as pivotal technique in the post-training phase of Large Language Models (LLMs), demonstrating significant potential in enhancing their reasoning abilities. landmark work in this domain is OpenAIs o1 model (OpenAI, 2024). As the first large-scale application of RL for reasoning, o1 achieved state-of-the-art reasoning capabilities at the time of its release. Shortly after, Deepseek-R1 (DeepSeek-AI et al., 2025) was introduced as the first open-source model to match the performance of o1, significantly advancing the development and popularization of RL-based reasoning techniques. This technical approach has led to the emergence of numerous powerful Large Reasoning Models (LRMs), such as Gemini 2.5 (Google, 2025a), QwQ (Team, 2025b), and Phi-4 (Abdin et al., 2025). Recently, Reinforcement Learning with Verifiable Rewards (RLVR) has been shown to be an effective method for significantly improving model reasoning abilities in domains like mathematics and programming (Shao et al., 2024; Liu and Zhang, 2025). Concurrently, the research community has proposed various advanced RL algorithms to optimize the post-training process, including GRPO (Shao et al., 2024), Reinforce++ (Hu et al., 2025), DAPO (Yu et al., 2025), and Dr.GRPO (Liu et al., 2025). These algorithmic innovations have continuously improved the efficiency and effectiveness of the RL training process. Our proposed method, LC-R1, builds upon these foundational works with specific adjustments aimed at pruning redundant sequence in the reasoning process, achieving more favorable trade-off between efficacy and efficiency. Efficient Reasoning. While elaborate reasoning is more likely to yield correct answer, its verbose thought process significantly increases time and computational costs, phenomenon termed overthinking (Chen et al., 2024). To mitigate this issue, researchers have proposed various solutions from different perspectives (Sui et al., 2025). These approaches can be broadly categorized. The first category involves directly constraining the redundancy of the reasoning process through length control, with typical examples like CoT-Valve (Ma et al., 2025a) and L1 (Aggarwal and Welleck, 2025). second category of methods focuses on enabling the model to adapt its reasoning depth according to the difficulty of the query. For instance, Adar1 (Luo et al., 2025a) and DAST (Shen et al., 2025) construct preference datasets to train the model to generate reasoning sequences that match the problems complexity. third category integrates efficiency considerations into the reinforcement learning framework. Works like O1-Pruner (Luo et al., 2025b), ThinkPrune (Hou et al., 2025), Training (Arora and Zanette, 2025), and Kimi (Team et al., 2025) incorporate length-based penalties into the reward function, incentivizing the model to produce more concise reasoning while maintaining accuracy. Furthermore, there are also training-free methods that enhance reasoning efficiency on-the-fly through sophisticated prompting techniques, such as Sketch-of-Thought (Aytes et al., 2025), Token-Budget (Han et al., 2024), and No Think (Ma et al., 2025b). B. Details of LC-Extractor We train Qwen-2.5-3B-Instruct (Team, 2024) as the LC-Extractor model. We construct dataset consisting of 5,000 <Question, Thinking Process, Answer> triplets from MATH dataset and identify the position of the first correct token using Gemini-2.5-Flash (Google, 2025b), followed by rigorous rule-based filtering. We then distill this knowledge into smaller model through training for 2 epochs with these curated samples. LC-Extractors effectiveness is validated on 100-sample test set, achieving 98% accuracy as confirmed by human evaluation as shown in in Figure 6. LC-Extractor model is activated by the prompt in Figure 5. C. Detailed Experiment Setups C.1. Model We use DeepSeek-R1(DeepSeek-AI et al., 2025), Qwen3-32B(Team, 2025a), QwQ-32B(Team, 2025b), Llama-3.3Nemotrom-Super-49B-V1(Bercovich et al., 2025), Distill-Qwen-7B, Distill-Qwen-1.5B(Yu et al., 2024), and Qwen-2.53B-Instruct(Team, 2024) models in our paper. We introduce their licenses and key characteristics as follows: DeepSeek-R1. An open-source 671 B37 MoE reasoning model trained largely through reinforcement learning, which elicits self-verification, reflection and lengthy chain-of-thought traces while supporting 128K-token context; it matches proprietary o1 on math / code benchmarks using only public data. Qwen3-32B. The 32.8 B-parameter third-generation Qwen model that toggles between thinking and non-thinking modes, delivering state-of-the-art reasoning, multilingual chat and up to 131 context in single dense checkpoint."
        },
        {
            "title": "Optimizing Length Compression in Large Reasoning Models",
            "content": "Figure 5: Our prompt for extraction of answer prefix. QwQ-32B. medium-sized Qwen reasoning variant refined with SFT + RL; provides explicit <think> traces, 131 context and DeepSeek-R1level accuracy on hard evaluations. Llama-3.3-Nemotrom-Super-49B-V1. NVIDIAs NAS-pruned 49 derivative of Llama-3.3-70B, post-trained for reasoning, RAG and tool calling; couples 128 context with single-H100 deployment efficiency for cost-sensitive production. Deepseek-R1-Distill-Qwen-7B. 7 dense checkpoint distilled from DeepSeek-R1 onto the Qwen2.5 backbone, pushing small-model MATH-500 pass1 beyond 92 % and surpassing o1-mini on several reasoning suites while remaining laptop-friendly. Deepseek-R1-Distill-Qwen-1.5B. An ultra-compact 1.5 model distilled from R1 that preserves chain-of-thought and achieves 83.9 % pass1 on MATH-500, bringing competitive analytical power to edge and mobile deployments. Qwen-2.5-3B-Instruct. 3.09 instruction-tuned model with 128 context, strengthened coding/math skills and multilingual support, designed as lightweight yet controllable chat foundation for downstream tasks. C.2. Dataset We benchmark on the AIME25 (International Conference on Artificial Intelligence in Medicine), MATH500 (Lightman et al., 2023), GSM8K (Cobbe et al., 2021), OlympiadBench (Sun et al., 2025), AMC (Mathematical Association of America), GPQA Diamond (Rein et al., 2024) and LiveCodeBench (Jain et al., 2024) benchmarks in our paper. We"
        },
        {
            "title": "Optimizing Length Compression in Large Reasoning Models",
            "content": "Figure 6: The annotation tool to evaluate the LC-Extratcor. introduce them as follows: AIME25. benchmark with 30 questions distilled from twenty-five years of American Invitational Mathematics Examination papers. Each item is three-digit short-answer problem that probes upper-secondary algebra, geometry, combinatorics. MATH500. 500-problem evaluation slice covering the full subject breadth of the original MATH competition corpus. Balanced across difficulty tiers and topics, it serves as rigorous yardstick for advanced high-school and early undergraduate mathematical reasoning, without the runtime burden of the complete 12k-question set. GSM8K. The widely-adopted Grade-School Math 8K benchmark of 1,319 everyday word-problems. Requiring multistep arithmetic and commonsense, GSM8K remains the de-facto standard for assessing chain-of-thought quality on conversational math tasks. Olympiad. curated collection of roughly 3 national and international mathematics-olympiad problems. Predominantly proof-style or numeric-answer challenges, this benchmark gauges creative, non-routine reasoning at the highest preuniversity level. AMC. An aggregate of 83 from the American Mathematics Competitions 10/12. Spanning 20002024, it offers longitudinal benchmark on foundational secondary-school math. GPQA Diamond. benchmark with 198 graduate-level Google-proof multiple-choice questions requiring deep domain expertise and multi-step reasoning, curated by researchers from New York University, CohereAI, and Anthropic; evaluated in closed-book and open-book settings using accuracy as the metric. LiveCodeBench. dynamic, contamination-free coding benchmark originally hosting 511 problems (release v2) collected from LeetCode, AtCoder, and CodeForces, designed by UC Berkeley, MIT, and Cornell researchers to holistically assess LLMs code generation, execution, and test prediction capabilities using Pass@K. C.3. settings We used mixed-difficulty dataset, combining past AIME competition problems with the MATH dataset in an approximate 1:2 ratio to create 2500 training data. We use Trl(von Werra et al., 2020) framework to train models. Both models are trained with 4 * A800-80G GPUs and the hyperparameters are presented in Table 4."
        },
        {
            "title": "Optimizing Length Compression in Large Reasoning Models",
            "content": "Table 4: Hyperparameters for LC-R1 training. Hyperparameter R1-Distill-Qwen-7B R1-Distill-Qwen-1.5B cutoff_len batch_size learning_rate num_train_epochs α β γ num_generations ϵ 8192 32 2.0e-6 1.0 1.0 0.04 1.0 8 0.2 8192 32 3.0e-6 1.0 1.0 0.04 1.0 6 0.2 Baseline settings. We compare LC-R1 with 5 baselineSFT, DPO, O1-Pruner, ThinkPrune, SFT+O1-Pruner. The last hybrid method shares same settings with each method, so we give out the settings of first four methods. SFT. We construct training dataset by extracting the valid thinking process to reconstruct concise version of sequences sampled by themselves on MATH dataset. We set cutoff_len=8192, epoch=1, learning_rate = 3.0e-6, max_samples = 5000. DPO. We construct preference training dataset by sampling 8 times on MATH dataset and choose the longest sample to be negative and shortest sample to be positive. We set cutoff_len=8192, epoch=2, learning_rate = 5e-6, max_samples = 5000. O1-Pruner. We use the given python scripts to construct weight training dataset, with cutoff_len=4096, epoch=2, learning_rate = 2.0e-7, max_samples = 10000. ThinkPrune-3K. We reproduce the training process on ThinkPrune-length3000 dataset, with size 2470. We set cutoff_len=8192, epoch=2, learning_rate = 2.0e-6, num_generations=8, batch_size=32. D. Case Study We make some case studies to compare LC-R1 with O1-Pruner(Luo et al., 2025b) method and the base model. These case studies are shown in Figure 7 and Figure 8."
        },
        {
            "title": "Optimizing Length Compression in Large Reasoning Models",
            "content": "Figure 7: Case study of the comparison of LC-R1 and O1-Pruner."
        },
        {
            "title": "Optimizing Length Compression in Large Reasoning Models",
            "content": "Figure 8: Case study of the comparison of LC-R1 and the original model."
        }
    ],
    "affiliations": [
        "University of Maryland"
    ]
}