{
    "paper_title": "WiseEdit: Benchmarking Cognition- and Creativity-Informed Image Editing",
    "authors": [
        "Kaihang Pan",
        "Weile Chen",
        "Haiyi Qiu",
        "Qifan Yu",
        "Wendong Bu",
        "Zehan Wang",
        "Yun Zhu",
        "Juncheng Li",
        "Siliang Tang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent image editing models boast next-level intelligent capabilities, facilitating cognition- and creativity-informed image editing. Yet, existing benchmarks provide too narrow a scope for evaluation, failing to holistically assess these advanced abilities. To address this, we introduce WiseEdit, a knowledge-intensive benchmark for comprehensive evaluation of cognition- and creativity-informed image editing, featuring deep task depth and broad knowledge breadth. Drawing an analogy to human cognitive creation, WiseEdit decomposes image editing into three cascaded steps, i.e., Awareness, Interpretation, and Imagination, each corresponding to a task that poses a challenge for models to complete at the specific step. It also encompasses complex tasks, where none of the three steps can be finished easily. Furthermore, WiseEdit incorporates three fundamental types of knowledge: Declarative, Procedural, and Metacognitive knowledge. Ultimately, WiseEdit comprises 1,220 test cases, objectively revealing the limitations of SoTA image editing models in knowledge-based cognitive reasoning and creative composition capabilities. The benchmark, evaluation code, and the generated images of each model will be made publicly available soon. Project Page: https://qnancy.github.io/wiseedit_project_page/."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 ] . [ 2 7 8 3 0 0 . 2 1 5 2 : r WiseEdit: Benchmarking Cognitionand Creativity-Informed Image Editing Kaihang Pan1* Weile Chen1* Haiyi Qiu1* Qifan Yu1 Wendong Bu1 Zehan Wang1 Yun Zhu2 Juncheng Li1 Siliang Tang1 1Zhejiang University, 2 Shanghai Artificial Intelligence Laboratory {kaihangpan, junchengli, siliang}@zju.edu.cn Project Page: https://qnancy.github.io/wiseedit_project_page/ Benchmark: https://huggingface.co/datasets/123123chen/WiseEdit-Benchmark Github: https://github.com/beepkh/WiseEdit"
        },
        {
            "title": "Abstract",
            "content": "Recent image editing models boast next-level intelligent capabilities, facilitating cognitionand creativity-informed image editing. Yet, existing benchmarks provide too narrow scope for evaluation, failing to holistically assess these advanced abilities. To address this, we introduce WiseEdit, knowledge-intensive benchmark for comprehensive evaluation of cognitionand creativity-informed image editing, featuring deep task depth and broad knowledge breadth. Drawing an analogy to human cognitive creation, WiseEdit decomposes image editing into three cascaded stepsAwareness, Interpretation, and Imaginationeach corresponding to task that poses challenge for models to complete at the specific step. It also encompasses complex tasks, where none of the three steps can be finished easily. Furthermore, WiseEdit incorporates three fundamental types of knowledge: Declarative, Procedural, and Metacognitive knowledge. Ultimately, WiseEdit comprises 1,220 test cases, objectively revealing the limitations of SoTA image editing models in knowledge-based cognitive reasoning and creative composition capabilities. The benchmark, evaluation code, and the generated images of each model have already been publicly available. Project Page: https://qnancy.github.io/wiseedit_ project_page/. 1. Introduction Instruction-based image editing [4, 49] has attracted significant attention in recent years, offering general users natural way for image modification. Early models [46, 47] primarily focused on simple localized edits, such as object addition or removal. Recently, remarkable progress in both visual understanding and visual generation, driven *Equal Contribution. by multimodal large language models (MLLMs) [1] and large diffusion models [12], has fundamentally evolved this field. As illustrated in Fig. 1 (a), models such as Bagel [11] and Qwen-Image-Edit [40] unify visual comprehension and generation in single model to enable reasoning-based image editing. Moreover, closed-source models like GPTimage-1 [28] and Nano Banana [14, 16] further push the boundary toward cognitionand creativity-informed image editing. By simulating human cognition and creativity, these advanced systems facilitate semantic reasoning and multi-concept composition in complex editing scenarios. To ensure these advanced models meet user practical needs, comprehensive evaluation for cognitionand creativity-informed image editing is imperative. Drawing an analogy with human cognition and creation [38], we fundamentally decompose the image editing workflow into three interconnected step: (1) Awareness, where the model establishes selective visual attention to precisely locate the modification target; (2) Interpretation, the cognitive core that parses editing instructions into directly executable, perception-level visual changes; (3) Imagination, the generative engine that executes the creative realization to render high-fidelity image. Therefore, robust evaluation must cover the full task depth of these three steps, verifying the capacity for knowledge-based reasoning and combinatorial creation, akin to the human faculty for converting abstract concepts into visual reality. Moreover, to cover the breadth of cognition, models must demonstrate proficiency across three fundamental human knowledge [19], declarative (knowing what), procedural (knowing how), and metacognitive (knowing about knowing), essential for robust problem-solving in intelligent image editing. However, existing benchmarks fail to meet the above desiderata for cognitionand creativity-informed image editing. Most [18, 26, 47] are too simplistic in their format and difficulty. They typically adopt single input template as shown in Fig. 1(b): one image paired with 1 Figure 1. (a) Capabilities of different image editing models across varying skill levels. (b) Comparison between hard cases from WiseEdit and easy cases from existing simplistic image editing benchmarks. an explicit editing instruction that directly specifies how to modify concrete objects, which requires minimal cognitive or creative ability. While newer benchmarks, such as KrisBench [42], have recently proposed to benchmark reasoning-informed image editing with world knowledge, they still fall short in task depth and cognitive breadth. Specifically, they focus predominantly on knowledge-based reasoning during the interpretation phase, failing to cover the full spectrum of the three editing steps and also overlooking the assessment of meta-cognitive knowledge. Consequently, the community urgently needs holistic benchmark to more thoroughly measure the capabilities of current advanced models. To address this gap, we introduce WiseEdit, novel benchmark designed for cognitionand creativity-informed image editing tasks with rich task depth and knowledge breadth. As for task depth, WiseEdit features four types of challenging tasks across all three editing steps: Awareness Task, Interpretation Task, Imagination Task, WiseEditComplex. As shown in Fig. 1(b).right, the first three tasks increase the difficulty specifically for their corresponding editing step, preventing trivial completion and requiring models to utilize causal reasoning or artistic creativity capabilities effectively. The difficulty is further escalated by WiseEdit-Complex, where none of the three editing steps can be easily finished. To further align with practical user needs, the task input is free-form, and each test case may include multiple input images. ther broaden knowledge scope by covering diverse domains, including natural science, cultural common sense, and spatio-temporal-logic reasoning. Besides, we also provide both Chinese and English versions for each case to evaluate the cross-lingual instruction-following abilities. To comprehensively assess model capabilities, we finally curate 1,220 high-quality cases in WiseEdit and evaluate 21 leading models (17 open-source, 4 closed-source). Beyond conventional metrics [26], we further introduce two novel dimensions: Knowledge Fidelity to verify the application of correct knowledge, and Creative Fusion to quantify the degree of editing creativity. Extensive experiments reveal the limitations of current SoTA models in performing cognitionand creativity-informed image editing, highlighting bottlenecks in their knowledge-based cognitive reasoning and creative composition capabilities. Overall, our contributions are three-fold: We are the first to break down instruction-based image editing into three interconnected steps. And we propose WiseEdit, novel benchmark designed for cognitionand creativity-informed image editing with rich task depth. WiseEdit covers wide breadth of knowledge, including declarative, procedural, and metacognitive knowledge. With comprehensive evaluation protocol, extensive experiments reveal the limitations of SoTA models when performing next-level intelligent image editing. 2. Related Work Regarding knowledge breadth, we structure our tasks around three core types of knowledge to emulate human cognitive learning: Declarative, Procedural, and Metacognitive knowledge. Building upon this, we furImage Editing Models. Early instruction-based image editing models [20, 46, 47, 49] focused on simple, localized modifications, such as object addition or removal. Recent advancements have driven these models toward 2 Table 1. Comparison of open-source knowledge-informed reaonsing-based image editing benchmarks."
        },
        {
            "title": "Benchmark",
            "content": "#Size Multi-Img Input RiseBench [50] IntelligentBench [11] KrisBench [42] WiseEdit 360 350 1267 1220 0% 0% 12% 26%"
        },
        {
            "title": "Knowledge Breadth",
            "content": "Awareness Interpretation Imagination Complex Declarative Procedural Meta-cognitive"
        },
        {
            "title": "Language\nENG CHN",
            "content": "3. WiseEdit In this section, we introduce WiseEdit, knowledgeintensive benchmark designed to evaluate cognitionand creativity-informed image editing with next-level intelligent. It features both high task depth and rich knowledge breadth, with diverse task formats and automated evaluation. The taxonomy of WiseEdit is presented in Fig. 2. 3.1. Benchmark Construction critical leap in cognitionand creativity-informed intelligent image editing is the models growing ability to infer implicit intent from multimodal input, and enable sophisticated creation that embodies this intent. To better measure this, we break down image editing into three interconnected steps, designing high-difficulty tasks for each, resulting in high task depth and diverse task formats within WiseEdit. In Section 1, we decompose the image editTask Depth. ing process into three cascaded steps, i.e., awareness, interpretation, and imagination. Correspondingly, we first increase the task difficulty for each step and prevent the models from trivially completing the respective step: Awareness Task. Awareness is the preliminary step in image editing where the AI model functions as visual observer to establish selective attention on the input image. We design awareness tasks, challenging the model to deduce the target modification area without explicit spatial information provided in the instruction. This necessitates the application of reasoning capabilities, such as comparative reasoning (e.g., remove the happiest person), parsing the referent of an indirect reference (e.g., highlight the country of Shakespeares homeland on map), or establishing visual correspondence from another reference image. Interpretation Task. Interpretation serves as the cognitive step, decoding the editing instructions into executable perception-level visual changes. In Interpretation task, instructions often do not explicitly state the required modification. The models must leverage world knowledge to parsing the implicit intent into directly executable action sequence. For example, an implicit instruction might require rectifying an unknown error in an anomalous image or performing temporal shift of the image scene. Even seemingly straightforward instructions necessitate models considering the cascaded reactions that their execution might trigger. Imagination Task. Imagination serves as the generative step, rendering the visual edits parsed previously onto the Figure 2. Taxonomy of WiseEdit (WiseEdit-Complex). free-form unified image generation, primarily by parsing the implicit intent within multimodal input via reasoning. Models like Bagel [11], OmniGen2 [41], and Qwen-Image-Edit [40] achieve this through unified visual comprehension-generation architecture that integrates VLM with diffusion model, enabling powerful reasoningbased editing. Furthermore, closed-source systems [14, 16, 28, 37] further push performance boundaries by facilitating cognitionand creativity-informed image editing with strong knowledge-based reasoning and compositional creation. Image Editing Benchmarks. Existing image editing benchmarks [18, 26, 47] often feature overly simplistic intentions in the instructions and lack explicit modeling of the knowledge structures, thus failing to accurately assess advanced models true capabilities. Recently, inspired by the benchmarks [23, 27] for text-toimage generation tasks, new benchmarks such as IntelligentBench [11], RiseBench [50], and KrisBench [42] have emerged to specifically gauge editing performance requiring knowledge-based reasoning. However, these benchmarks remain limited as they typically use single-image inputs, primarily focus on the interpretation step while neglecting the step of awareness, imagination and other complex scenarios. Moreover, they exclude the assessment of meta-cognitive knowledge, also restricting instructions to English language, In contrast, WiseEdit offers more comprehensive evaluation of cognitionand creativity-informed image editing from these perspectives, as detailed in the comparative analysis in Table 1. 3 Figure 3. Examples of each task type in WiseEdit and WiseEdit-Complex. target regions of the original image. For this task, we introduce highly challenging subject-driven generation tasks that require models to perform imaginative and artistic creations while preserving the subjects identity. This involves sophisticated transformations like altering the subjects clothing, pose, or viewpoint, generating complex compositions of multiple objects, and even counter-intuitive creations (e.g., transforming human into construction scaffolding). Building upon this, we further designed WiseEditComplex, where none of the three editing steps can be easily finished, requiring the model to fully leverage its capacity for complex reasoning and creative generation. Fig. 3 presents examples of the each task type within WiseEdit(- Complex), with more details available in Appendix A. Task Format. Existing image editing benchmarks typically use single text-image input pair, which often limits meeting practical user needs because text alone may fail to convey abstract attributes (like texture), and some requests inherently require merging concepts from multiple images. To this end, WiseEdit expands the task format to include large volume of cases with multiple input images, and instructions utilize ordinal terms to indicate the specific image. Moreover, input images serve distinct roles: some are the targets for editing, while others function as references providing necessary abstract attributes or concrete objects to convey intent. Models must determine the specific role of each input image based on the instruction description. Based on this, WiseEdit contains 1,220 high-quality test cases, with each instruction provided in both English and Chinese. See details on data collection in Appendix B. 3.2. Knowledge-Intensive Evaluation in WiseEdit Cognitionand creativity-informed intelligent image editing also requires the possession of robust world knowledge to facilitate superior image generation. To measure this key ability, we incorporates an extensive set of world-knowledge-informed cases in WiseEdit, which cover broad knowledge breadth with three knowledge types: Declarative knowledge, often described as knowing what, encompasses facts and concepts that can be explicitly stated and defined. In the context of knowledgeintensive image editing, it not only includes observable perceptual cues but also integrates higher-order understanding that connects perceptual information to generalizable principles and conceptual facts. For example, given an instruction add toy animal that is national treasure of China, the model must possess the declarative knowledge that the national treasure animal of China is the panda. Procedural knowledge, defined as knowing how, encompasses the expertise and skills necessary to perform task, which is often dynamic and difficult to explain verbally. For image editing, it requires model to perform multi-step task decomposition to ascertain the correct procedure for executing desired change. For example, when required to convert watercolor painting into line drawing, the model relies on procedural knowledge to complete the process systematically. Metacognitive knowledge is knowing about knowing, requiring self-awareness and self-regulation during image editing. It dictates the high-level management of when to invoke declarative or procedural knowledge, and how to effectively combine them. For example, executing conditional instruction have the girl hold the longest object in the second image; if the object can be used to brush teeth, let her face the camera, otherwise turn her back to the camera, requires that the model knows when and why to apply the necessary declarative or procedural steps. Based on this, we also systematically encompass three critical knowledge domainsCultural Common Sense (e.g., social customs, daily life), Natural Sciences (e.g., physical laws, chemical reactions), and Spatio-TemporalLogic (e.g., causality, spatial arrangement, and temporal progression)to assess culturally-appropriate, physicallyconsistent, and logically-coherent image editing. 3.3. Evaluation Pipeline To comprehensively evaluate the performance of SoTA models on WiseEdit, we employ GPT-4o as the automatic evaluator. Beyond widely used metrics such as Instruction Following, Detail Preserving and Visual Quality [17, 26], we introduce two novel, essential metrics: Knowledge Fidelity and Creative Fusion. The former is designed to verify the application of correct knowledge, while the latter quantifies the degree of creativity exhibited by the models. Furthermore, for knowledge-informed cases, we provide knowledge hint to precisely elucidate the explicit intent of the editing, assisting in the evaluation of Knowledge Fidelity. Instruction Following (IF) evaluates whether models accurately follow the editing request. Detail Preserving (DP) evaluates whether models faithfully preserve the parts not related to the instruction. Visual Quality (VP) evaluates the perceptual quality of the generated image, such as its naturalness and any presence of artifacts. Knowledge Fidelity (KF) evaluates whether the edited image accurately reflects real-world logic and knowledge, including physical, biological, chemical, cultural, commonsense correctness, and so on, based on the provided knowledge hint which serves as guiding reference. Creative Fusion (CF) evaluates conceptual novelty, expressive transformation, and imaginative depth of the edited image relative to the originals, gauging its demonstration of human-like creative thinking for imagination tasks. All test cases are evaluated using IF, DP and VQ. with KF only for Awareness, Interpretation, and Wise-Complex tasks, CF for Imagination and Wise-Complex tasks. Each metrics is rated on 110 scale based on carefully customcrafted prompts to ensure precise scoring. The overall performance metric is calculated as AVG = (IF + DP + VQ + α KF + β CF)/(3 + α + β), where α and β are set to 1 if the respective metric (KF or CF) is required for the task, and 0 otherwise. See more details in Appendix C. 4. Experiments 4.1. Experimental Setup. (1) Diffusion models: We evaluate 22 mainstream image editing models across architectures, covering both openand closed-source models: InstructPix2Pix [4], MagicBrush [47], OmniGen [44], AnyEdit [46], UltraEdit [49], ICEdit [48], FLUX.1 Kontext Dev [21] and FLUX.2 (2) Unified comprehension and generation Dev [20]. models: Janus-4o [8], UniWorld-V1 [24], HiDream-E1 [5], OmniGen2 [41], Step1X-Edit-v1p2 [26], Echo-4o [45], Bagel [11], Uni-CoT [35], Qwen-Image-Edit [40] (the version of Qwen-Image-Edit-2509), and DreamOmni2 [43]. (3) Close-sourced Models: Nano Banana [14], Seedream 4.0 [37], GPT-image-1 [28], and Nano Banana Pro [16]. Evaluation metrics are detailed in Sec 3.3. We linearly map each 1-to-10 score provided by the evaluator to 0-to-100 range, with more details shown in Appendix C. 4.2. Main Results on WiseEdit Table 2 presents comprehensive evaluation of various image editing models on WiseEdit, revealing their respective strengths, weaknesses, and recent advancements: (1) Model capabilities rapidly advance, necessitating robust visual comprehension to drive effective visual generation. Across all three tasks (Awareness, Interpretation, and Imagination), unified visual comprehension and generation models consistently significantly outperform most purely diffusion-based models in both English and Chinese version, with FLUX.2 Dev being an exception. For instance, Qwen-Image-Edit achieves an average overall score 17 points higher than FLUX.1 Kontext 5 Table 2. Main results on WiseEdit with both English and Chinese task version. The best results are marked in bold for openand closedmodels, respectively."
        },
        {
            "title": "Model",
            "content": "Multi-Img InstructPix2Pix [4] MagicBrush [47] OmniGen [44] AnyEdit [46] UltraEdit [49] ICEdit [48] FLUX.1 Kontext Dev [21] FLUX.2 Dev [20] Janus-4o [8] UniWorld-V1 [24] HiDream-E1 [5] OmniGen2 [41] Step1X-Edit-v1p2 [26] Echo-4o [45] Bagel [11] Uni-CoT [35] Qwen-Image-Edit [40] DreamOmni2 [43] Nano Banana [14] Seedream 4.0 [37] GPT-image-1 [28] Nano Banana Pro [16] InstructPix2Pix [4] MagicBrush [47] OmniGen [44] AnyEdit [46] UltraEdit [49] ICEdit [48] FLUX.1 Kontext Dev [21] FLUX.2 Dev [20] Janus-4o [8] UniWorld-V1 [24] HiDream-E1 [5] OmniGen2 [41] Step1X-Edit-v1p2 [26] Echo-4o [45] Bagel [11] Uni-CoT [35] Qwen-Image-Edit [40] DreamOmni2 [43] Nano Banana [14] Seedream 4.0 [37] GPT-image-1 [28] Nano Banana Pro [16]"
        },
        {
            "title": "Interpretation Task",
            "content": "IF DP VQ KF AVG IF DP VQ KF AVG IF DP VQ"
        },
        {
            "title": "Imagination Task",
            "content": "Overall CF AVG AVG 24.6 27.2 35.0 25.0 26.5 26.1 31.4 42.6 34.7 31.5 29.7 35.0 39.8 47.6 46.2 46.0 48.1 43.3 70.6 70.8 78.5 85.4 14.2 15.0 16.3 17.5 16.9 12.9 16.5 43.0 31.1 18.4 28.2 35.1 38.6 47.9 48.5 46.2 45.0 31.9 71.8 69.1 77.0 84.6 33.7 43.4 42.0 54.6 42.5 42.2 52.0 63.3 37.0 48.9 41.2 64.0 53.5 63.0 71.0 69.1 69.0 74.4 85.7 78.1 85.8 88.6 51.0 43.8 42.6 55.3 58.5 29.1 48.4 60.6 38.6 49.0 37.6 57.9 55.6 59.9 71.3 70.0 67.3 78.7 83.8 79.0 80.7 91.8 50.6 53.3 46.7 61.3 53.1 61.2 55.0 78.4 45.9 58.8 56.3 75.4 61.3 75.4 75.8 77.8 79.5 85.0 86.8 86.6 88.0 83. 58.6 52.9 60.1 55.7 62.1 63.6 52.1 79.5 46.3 60.0 51.4 72.4 59.5 73.1 76.8 80.7 79.9 85.4 86.5 84.4 86.6 83.1 26.4 27.1 37.4 26.3 33.9 31.8 35.5 53.3 36.2 38.6 32.0 41.3 44.4 51.7 50.8 51.6 53.6 51.2 75.2 74.6 81.2 91.4 18.1 17.8 22.6 19.9 21.2 17.4 19.1 51.4 34.1 26.2 32.4 41.0 42.0 55.0 52.1 53.6 52.9 38.4 70.7 72.0 80.7 87.9 33.8 37.8 40.3 41.8 39.0 40.4 43.5 59.4 38.5 44.5 39.8 53.9 49.7 59.4 61.0 61.1 62.5 63.5 79.6 77.5 83.3 87.3 35.5 32.4 35.4 37.1 39.7 30.8 34.0 58.6 37.5 38.4 37.4 51.6 48.9 59.0 62.2 62.6 61.3 58.6 78.2 76.1 81.2 86.9 20.7 16.8 19.0 15.9 24.3 21.4 27.5 35.4 27.2 18.1 26.7 18.9 35.7 30.8 38.6 36.9 32.1 34.3 63.4 63.7 62.9 76. 50.3 50.3 34.8 61.2 61.7 48.3 62.2 75.0 43.8 44.5 53.6 56.9 73.0 71.0 72.1 70.1 69.7 81.7 84.9 80.1 82.9 89.1 Chinese Version 55.7 40.5 30.9 51.8 74.2 43.5 58.4 73.7 45.9 48.8 47.1 57.1 77.5 74.5 68.7 71.5 74.0 80.1 84.5 80.3 82.6 83.9 13.0 10.1 13.4 11.4 17.4 11.5 16.8 34.3 23.9 13.3 25.4 19.1 37.0 31.9 36.5 37.4 35.8 24.0 67.9 62.2 61.4 74.2 66.9 63.2 40.3 62.0 73.6 81.5 69.6 85.6 53.6 58.1 68.4 64.9 75.2 80.4 78.8 76.3 80.6 88.1 91.4 90.6 93.0 92.3 65.2 59.4 49.2 58.0 78.9 81.1 66.1 83.1 55.7 59.7 63.6 64.8 76.8 77.4 75.0 79.2 80.7 86.2 91.4 89.9 93.8 91.3 23.6 22.2 21.5 20.2 26.7 24.9 29.0 37.6 28.2 22.5 29.6 23.5 38.2 32.9 39.5 38.6 34.2 35.9 61.5 64.2 60.8 75.8 16.0 13.5 14.8 16.0 19.2 17.0 22.2 36.0 25.8 16.9 29.7 23.0 35.9 32.6 38.5 36.6 36.1 27.5 63.7 59.9 61.2 74. 40.4 38.1 28.9 39.8 46.6 44.0 47.1 58.4 38.2 35.8 44.6 41.1 55.5 53.8 57.3 55.5 54.1 60.0 75.3 74.6 74.9 83.3 37.5 30.9 27.1 34.3 47.4 38.3 40.9 56.8 37.8 34.7 41.5 41.0 56.8 54.1 54.7 56.2 56.6 54.4 76.9 73.1 74.8 81.0 17.0 18.0 42.2 9.1 20.7 21.5 39.1 73.6 28.2 30.3 39.6 42.0 44.7 63.4 62.8 67.6 67.1 50.6 75.3 82.2 84.4 86.6 3.7 5.4 15.4 7.3 9.3 5.1 9.2 75.5 25.4 17.9 32.5 45.5 45.7 62.8 63.5 65.5 66.3 38.5 76.0 79.8 78.8 85.5 29.9 36.9 35.1 49.7 31.7 40.6 47.1 70.7 37.6 50.3 40.1 64.4 49.4 62.4 68.5 64.3 66.8 64.9 73.8 77.8 76.2 79.5 52.0 39.6 27.6 47.4 42.3 37.4 41.9 74.4 36.9 54.8 39.2 64.0 48.3 64.2 68.3 65.1 67.2 69.4 75.8 79.7 73.3 77. 41.2 44.8 46.0 50.9 45.8 54.0 43.4 82.1 42.0 64.2 49.9 74.6 50.3 73.7 74.5 79.6 79.2 81.9 87.3 86.9 89.2 88.8 53.2 49.2 51.3 52.3 51.8 56.5 43.3 82.8 41.8 68.9 47.5 72.0 51.3 75.1 75.3 79.7 80.0 84.8 87.3 86.5 89.6 88.4 27.4 22.3 38.7 16.5 27.5 25.0 27.1 43.6 25.5 27.5 29.6 31.8 28.4 41.2 40.7 42.9 42.3 35.3 44.3 47.0 48.4 51.5 9.0 12.7 32.3 15.4 14.8 16.7 10.6 42.6 22.1 18.0 27.4 33.8 27.0 41.5 39.7 41.6 41.7 27.2 43.7 46.4 48.3 51.1 28.8 30.5 40.5 31.5 31.5 35.3 39.2 67.5 33.3 43.1 39.8 53.2 43.2 60.2 61.6 63.6 63.8 58.2 70.2 73.5 74.6 76.6 29.5 26.8 31.7 30.6 29.5 28.9 26.3 68.8 31.5 39.9 36.6 53.8 43.1 60.9 61.7 63.0 63.8 55.0 70.7 73.1 72.5 75. 34.4 35.5 36.6 37.7 39.0 39.9 43.2 61.8 36.7 41.1 41.4 49.4 49.5 57.8 60.0 60.1 60.2 60.6 75.0 75.2 77.6 82.4 34.1 30.0 31.3 34.0 38.9 32.7 33.7 61.4 35.6 37.7 38.5 48.8 49.6 58.0 59.5 60.6 60.6 56.0 75.3 74.1 76.2 81.2 Table 3. Main results on WiseEdit-Complex. We exclude models unable to handle multi-image inputs."
        },
        {
            "title": "Model",
            "content": "AnyEdit OmniGen FLUX.2 Dev UniWorld-V1 OmniGen2 DreamOmni2 Echo-4o Qwen-Image-Edit Bagel Uni-CoT Nano Banana GPT-image-1 Seedream 4.0 Nano Banana Pro English Version IF DP VQ KF 2.5 23.5 42.3 18.1 34.1 34.8 42.7 38.7 43.8 35.5 53.8 58.7 67.2 68.1 5.6 25.7 68.5 32.1 50.2 62.3 46.5 58.6 62.0 60.0 75.2 75.9 77.3 78.1 20.6 41.2 75.6 55.3 72.4 78.8 64.1 75.8 70.0 69.3 82.7 87.6 79.6 86.7 3.3 31.5 56.5 22.8 49.0 46.0 50.9 48.5 53.5 57.0 82.4 77.9 89.7 88.1 Chinese Version CF AVG IF DP VQ KF 11.7 48.9 49.1 28.6 44.8 41.0 48.7 47.1 43.4 49.5 53.7 54.8 53.1 56. 5.1 15.4 73.4 23.8 51.7 51.6 55.5 55.0 58.2 55.2 71.3 76.6 62.8 83.8 2.9 15.1 59.9 12.4 47.3 49.6 53.0 49.6 55.5 58.1 77.4 78.8 87.8 90.7 1.3 4.4 46.3 8.8 30.2 36.6 41.0 35.3 39.5 36.4 53.3 59.5 59.3 77.7 8.7 34.2 58.4 31.4 50.1 52.6 50.6 53.8 54.5 54.3 69.6 71.0 73.4 75.5 22.2 50.3 80.3 64.6 75.1 79.1 68.4 78.1 73.5 77.4 79.9 88.2 81.0 84.3 Overall CF AVG AVG 9.3 8.2 23.5 32.2 62.6 52.8 25.0 15.4 50.0 45.8 50.4 35.5 53.6 50.1 53.4 48.9 54.7 46.8 55.2 48.9 66.6 51.1 71.4 54.1 69.3 55.5 78.7 57. 8.5 28.9 60.5 28.2 50.1 51.5 52.1 53.6 54.6 54.8 68.1 71.2 71.4 77.1 Dev. This substantial gap is largely due to Qwen-ImageEdits integration of Qwen-VL [3] to provide thinking for generation. Besides, FLUX.2 Dev achieves SOTA performance among open-sourced models, primarily due to its significantly larger number of parameters compared to competitors. It comprises 32B diffusion transformer coupled with separate 24B LLM [2] for text encoding. In contrast, Qwen-Image-Edit is 20B diffusion transformer that utilizes the 7B Qwen-VL model to assist with visual comprehension. Furthermore, it is worth noting that closed-source 6 Figure 4. Qualitative comparison across AnyEdit, Bagel, Qwen-Image-Edit, GPT-Image-1, and Nano Banana. models like GPT-image-1 and Nano Banana also possess the architecture of unified visual comprehension and generation. Consequently, it suggests that achieving powerful, cognitionand creativity-informed image editing requires strong visual comprehension and reasoning capabilities. (2) The performance gap between open-source and closed-source models remains significant. Despite some benchmarks [42, 43] suggesting certain open-source models have surpassed their closed-source counterparts, WiseEdit pours cold water on this notion. Across all three tasks, in both languages, every closed-source model overwhelmingly outperforms all open-source models. Even the lowest7 Figure 5. (a) Pearson correlation between human ratings and VLM scores. (b) Performance visualization across different metrics and knowledge-informed cases. (c) Average performance for singleand multi-image inputs. (d) Average performance with and w/o thinking. scoring closed-source model surpassed the best open-source model by nearly 15 overall points on the overall metric. Thus, the open-source community must redouble its efforts toward achieving the next-intelligent image editing goal. Furthermore, the performance comparison among these four closed-source models reveals that the capabilities of Nano Banana, Seedream4.0, and GPT-Image-1 are relatively close. However, the average performance of Nano Banana Pro significantly surpasses these three, establishing it as the undisputed top-1 model for cognitionand creativity-informed image editing.. (3) Knowledge-based cognitive reasoning shows steady progress but remains bottleneck. In both Awareness and Interpretation Tasks, metrics of IF and KF consistently improve from Diffusion models through unified models to closed-source models. However, even closedsource models fall short of high scores in the two metrics (not exceeding 80 points, except for Nano Banana Pro). This indicates current generative models struggle to fully apply internal world knowledge to image generation, thereby limiting their ability to accurately follow complex, implicit instructions. (4) Compositional creativity Needs Further Enhancement. In imagination task, all models score poorly on CF, and their DP scores are also subpar (none exceeding 80 points), indicating that high-difficulty compositional creation remains an unsolved challenge. Furthermore, models struggle to adequately preserve the subjects identity when altering attributes like pose or viewpoint, showing that finegrained subject-driven generation is still an open problem. show strong cross-lingual instruction-following. Many leading unified models, including closed-source ones, show negligible performance change when processing Chinese instructions compared to English ones, even with minimal Chinese training data. This impressive robustness is attributed to powerful built-in facilitate effective visual comprehension modules that multi-lingual instruction following. (5) Leading models 4.3. Main Results on WiseEdit-Complex the performance on WiseEditIn Table 3, we present Complex, where none of the three editing steps can be easily completed. Since WiseEdit-Complex only uses multiTable 4. Performance before-and-after instruction rewriting. Qwen-Image-Edit GPT-Image-1 Nano Banana Models KF (w/o rewrite) KF (rewrite) Overall (w/o rewrite) Overall (rewrite) Bagel 50.2 78.4 (+28.2) 51.3 65.8 (+14.5) 48.3 80.0 (+32.7) 52.1 66.2 (+14.1) 81.3 84.5 (+3.2) 70.4 75.0 (+4.6) 80.2 83.9 (+3.7) 68.2 74.2 (+6.0) image inputs, models unable to handle this format are excluded. Our observations are as follows: (1) Performance hierarchy still persists: closed-source models significantly outperform open-source unified models, which in turn are better than diffusion-based models (except for FLUX.2 Dev). And the performance of the Nano Banana Pro still maintains commanding lead across all models, making it the current undisputed top-1 model. (2) Most models, including closed-source ones, show notable performance decline on the complex task compared to their average scores across the three WiseEdit tasks. This indicates current models still struggle to harmoniously combine knowledge-based reasoning and compositional creative generation, highlighting substantial room for im- (3) Interestingly, some leading closed-source provement. models achieve higher CF and KF scores on the complex task than on the three simpler WiseEdit tasks. However, other metrics like IF decline significantly, suggesting that while models correctly apply internal knowledge and grasp creative generation for complex visual generation, they are powerless in execution, often sacrificing fundamental instruction following. 4.4. In-Depth Analysis Assessment of Evaluation Protocol. To assess the reliability of our evaluation, we conduct user study with human experts. Beyond the pipeline in Sec.3.3 (GPT-ours), we also employ the Kris-Bench pipeline (GPT-Kris), and extend our own pipeline by replacing GPT-4o with Gemini2.5-Pro (Gemini-ours) and Qwen3-VL (Qwen-ours). By randomly selecting test cases and calculating the Pearson correlation coefficient between human ratings and VLM scores, as shown in Fig. 5 (a), our proposed protocol yields the strongest correlation, which objectively reflects the true capabilities of the models. Capability Visualization. Fig. 5 (b) presents radar chart visualizing model performance, reflecting the average 8 scores across five metrics on all test cases, alongside the overall average scores for tasks corresponding to declarative, procedural, and metacognitive knowledge. It highlights two key points: (1) Creative fusion and knowledge fidelity are two main weaknesses, with the latter further negatively impacting instruction following. (2) Performance on metacognitive knowledge cases is notably weaker than on declarative and procedural knowledge. Single-Image v.s. Multi-Image Inputs. Fig. 5 (c) illustrates the average performance on each task when models handle single-image versus multi-image inputs in WiseEdit. Multi-image inputs result in significantly lower average scores, notably dragging down overall performance. Therefore, improving capability in complex, multi-image scenarios is key direction for future image editing models. Visual Comprehension Helps Generation. To analyze how visual comprehension aids visual generation in unified models, we re-evaluated Bagel and DreamOmni2 after disabling their built-in thinking processes. As shown in Fig. 5 (d), removing this process causes significant performance drop for all three models on both WiseEdit and WiseEditComplex, showing the critical role of visual comprehension in enhancing visual generation. Impact of Instruction Rewriting. As visual comprehension benefits generation, we further investigate how its upper limit constrains generation. We select 75 case and rewrite the instructions by incorporating knowledge hints. As shown in Table 4, unified open-source models exhibit substantial improvement of about 15 points in Overall (30 points in KF) after rewriting, suggesting rewriting effectively compensates for their limited native reasoning capability. In contrast, closed-source models have relatively smaller performance gains, because their internal mechanisms already acquired the knowledge externally introduced by rewriting. This underscores enhancing reasoning capability is key direction for future models. Qualitative Comparisons. Fig. 4 presents comparison across AnyEdit, FLUX.2 Dev, Bagel, Qwen-Image-Edit, GPT-Image, and Nano Banana Pro. AnyEdit consistently fails on all cases. FLUX.2 Dev, Qwen-Image-Edit and Bagel sometimes interpret instructions correctly but largely fail to execute them, especially in complex scenarios. While the two closed-source models exhibit superior overall capabilities via stronger knowledge-based reasoning and compositional creative generation. For instance, in tasks like generating left-view or correcting errors, while final output is sometimes incorrect, they demonstrate clearer understanding of the editing intent with higher potential for success. However, shared limitation is the inability of all models to solve logic-related problems (e.g., matchstick), which defines the boundary of current capability. See more cases in Appendix D. 5. Conclusion We present WiseEdit, novel benchmark designed for cognitionand creativity-informed image editing. Referencing the cognitive and creative process of image editing, we design four highly challenging task types: Awareness, Interpretation, Imagination, and Complex, pairing them with diverse knowledge breadth. WiseEdit comprehensively evaluates models multifaceted abilities, including knowledge-based reasoning and creative composition. Our results illuminate the performance level and potential weaknesses of existing models, offering valuable reference for future advancement of image editing research."
        },
        {
            "title": "References",
            "content": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. 1, 14 [2] Mistral AI. Mistral Small 3.2. https : / / docs . mistral.ai/models/mistral-small-3-2-2506, 2025. 6, 15 [3] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. 6, 15 [4] Tim Brooks, Aleksander Holynski, and Alexei Efros. Instructpix2pix: Learning to follow image editing instructions. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1839218402, 2023. 1, 5, 6, 14 [5] Qi Cai, Jingwen Chen, Yang Chen, Yehao Li, Fuchen Long, Yingwei Pan, Zhaofan Qiu, Yiheng Zhang, Fengbin Gao, Peihan Xu, et al. Hidream-i1: high-efficient image generative foundation model with sparse diffusion transformer. arXiv preprint arXiv:2505.22705, 2025. 5, 6, [6] Siyu Cao, Hangting Chen, Peng Chen, Yiji Cheng, Yutao Cui, Xinchi Deng, Ying Dong, Kipper Gong, Tianpeng Gu, Xiusen Gu, et al. Hunyuanimage 3.0 technical report. arXiv preprint arXiv:2509.23951, 2025. 15 [7] Dong Chen, Kaihang Pan, Guangyu Dai, Guoming Wang, Yueting Zhuang, Siliang Tang, and Mingliang Xu. Improving vision anomaly detection with the guidance of language modality. IEEE Transactions on Multimedia, 2024. 15 [8] Junying Chen, Zhenyang Cai, Pengcheng Chen, Shunian Chen, Ke Ji, Xidong Wang, Yunjin Yang, and Benyou Sharegpt-4o-image: Aligning multimodal modWang. arXiv preprint els with gpt-4o-level image generation. arXiv:2506.18095, 2025. 5, 6, 15 9 [9] Jiuhai Chen, Le Xue, Zhiyang Xu, Xichen Pan, Shusheng Yang, Can Qin, An Yan, Honglu Zhou, Zeyuan Chen, Lifu Huang, et al. Blip3o-next: Next frontier of native image generation. arXiv preprint arXiv:2510.15857, 2025. 15 [10] Xiaokang Chen, Zhiyu Wu, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, and Chong Ruan. Januspro: Unified multimodal understanding and generation with data and model scaling. arXiv preprint arXiv:2501.17811, 2025. 15 [11] Chaorui Deng, Deyao Zhu, Kunchang Li, Chenhui Gou, Feng Li, Zeyu Wang, Shu Zhong, Weihao Yu, Xiaonan Nie, Ziang Song, et al. Emerging properties in unified multimodal pretraining. arXiv preprint arXiv:2505.14683, 2025. 1, 3, 5, 6, 15 [12] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first international conference on machine learning, 2024. [13] Zigang Geng, Yibing Wang, Yeyao Ma, Chen Li, Yongming Rao, Shuyang Gu, Zhao Zhong, Qinglin Lu, Han Hu, Xiaosong Zhang, et al. X-omni: Reinforcement learning makes discrete autoregressive image generative models great again. arXiv preprint arXiv:2507.22058, 2025. 15 [14] Google. Introducing gemini 2.5 flash image, our statehttps : / / developers . of-the-art image model. googleblog.com/en/introducinggemini25 - flash - image / ?utm _ source = chatgpt . com, 2025. . 1, 3, 5, 6, 15 [15] Google. Gemini 3. https://deepmind.google/ models/gemini/, 2025. . 15 [16] Google. Introducing nano banana pro. https : / / blog . google / technology / ai / nano - banana - pro/, 2025. . 1, 3, 5, 6, 15 [17] Xin Gu, Ming Li, Libo Zhang, Fan Chen, Longyin Wen, Tiejian Luo, and Sijie Zhu. Multi-reward as condiarXiv preprint tion for instruction-based image editing. arXiv:2411.04713, 2024. 5 [18] Xuan Ju, Ailing Zeng, Yuxuan Bian, Shaoteng Liu, and Qiang Xu. Direct inversion: Boosting diffusion-based editing with 3 lines of code. arXiv preprint arXiv:2310.01506, 2023. 1, 3 [19] David Krathwohl. revision of blooms taxonomy: An overview. Theory into practice, 41(4):212218, 2002. 1 [20] Black Forest Labs. FLUX.2: Frontier Visual Intelligence. https://bfl.ai/blog/flux-2, 2025. 2, 5, 6, 15 [21] Black Forest Labs, Stephen Batifol, Andreas Blattmann, Frederic Boesel, Saksham Consul, Cyril Diagne, Tim Dockhorn, Jack English, Zion English, Patrick Esser, et al. Flux. 1 kontext: Flow matching for in-context image generation and editing in latent space. arXiv preprint arXiv:2506.15742, 2025. 5, 6, 15 [22] Juncheng Li, Kaihang Pan, Zhiqi Ge, Minghe Gao, Wei Ji, Wenqiao Zhang, Tat-Seng Chua, Siliang Tang, Hanwang Zhang, and Yueting Zhuang. Fine-tuning multimodal llms to follow zero-shot demonstrative instructions. arXiv preprint arXiv:2308.04152, 2023. 10 [23] Ouxiang Li, Yuan Wang, Xinting Hu, Huijuan Huang, Rui Chen, Jiarong Ou, Xin Tao, Pengfei Wan, Xiaojuan Qi, and Fuli Feng. Easier painting than thinking: Can text-to-image models set the stage, but not direct the play? arXiv preprint arXiv:2509.03516, 2025. 3 [24] Bin Lin, Zongjian Li, Xinhua Cheng, Yuwei Niu, Yang Ye, Xianyi He, Shenghai Yuan, Wangbo Yu, Shaodong Wang, Yunyang Ge, et al. Uniworld: High-resolution semantic encoders for unified visual understanding and generation. arXiv preprint arXiv:2506.03147, 2025. 5, 6, 15 [25] Wang Lin, Wentao Hu, Liyu Jia, Kaihang Pan, Zhang Majun, Zhou Zhao, Fei Wu, Jingyuan Chen, and Hanwang Zhang. Vinci: Deep thinking in text-to-image generation using unified model with reinforcement learning. In The Thirty-ninth Annual Conference on Neural Information Processing Systems. 15 [26] Shiyu Liu, Yucheng Han, Peng Xing, Fukun Yin, Rui Wang, Wei Cheng, Jiaqi Liao, Yingming Wang, Honghao Fu, Chunrui Han, et al. Step1x-edit: practical framework for general image editing. arXiv preprint arXiv:2504.17761, 2025. 1, 2, 3, 5, 6, 15 [27] Yuwei Niu, Munan Ning, Mengren Zheng, Weiyang Jin, Bin Lin, Peng Jin, Jiaqi Liao, Kunpeng Ning, Chaoran Feng, Bin Zhu, and Li Yuan. Wise: world knowledge-informed arXiv semantic evaluation for text-to-image generation. preprint arXiv:2503.07265, 2025. [28] OpenAI, Aaron Hurst, Adam Lerer, et al. Gpt-4o system card. Technical report, OpenAI, 2024. 1, 3, 5, 6, 15 [29] Kaihang Pan, Yang Wu, Wendong Bu, Kai Shen, Juncheng Li, Yingting Wang, Siliang Tang, Jun Xiao, Fei Wu, Yueting Zhuang, et al. Janus-pro-r1: Advancing collaborative visual comprehension and generation via reinforcement learning. In The Thirty-ninth Annual Conference on Neural Information Processing Systems. 15 [30] Kaihang Pan, Zhaoyu Fan, Juncheng Li, Qifan Yu, Hao Fei, Siliang Tang, Richang Hong, Hanwang Zhang, and Qianru Sun. Towards unified multimodal editing with enhanced knowledge collaboration. Advances in Neural Information Processing Systems, 37:110290110314, 2024. 15 [31] Kaihang Pan, Siliang Tang, Juncheng Li, Zhaoyu Fan, Wei Chow, Shuicheng Yan, Tat-Seng Chua, Yueting Zhuang, and Hanwang Zhang. Auto-encoding morph-tokens for multimodal llm. arXiv preprint arXiv:2405.01926, 2024. 15 [32] Kaihang Pan, Wendong Bu, Yuruo Wu, Yang Wu, Kai Shen, Yunfei Li, Hang Zhao, Juncheng Li, Siliang Tang, and Yueting Zhuang. Focusdiff: Advancing fine-grained text-image alignment for autoregressive visual generation through rl. arXiv preprint arXiv:2506.05501, 2025. [33] Kaihang Pan, Wang Lin, Zhongqi Yue, Tenglong Ao, Liyu Jia, Wei Zhao, Juncheng Li, Siliang Tang, and Hanwang Zhang. Generative multimodal pretraining with discrete difIn Proceedings of the Computer fusion timestep tokens. Vision and Pattern Recognition Conference, pages 26136 26146, 2025. [34] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 41954205, 2023. 15 [47] Kai Zhang, Lingbo Mo, Wenhu Chen, Huan Sun, and Yu Su. Magicbrush: manually annotated dataset for instructionguided image editing. Advances in Neural Information Processing Systems, 36:3142831449, 2023. 1, 2, 3, 5, 6, 14 [48] Zechuan Zhang, Ji Xie, Yu Lu, Zongxin Yang, and Yi Yang. In-context edit: Enabling instructional image editing with incontext generation in large scale diffusion transformer. arXiv preprint arXiv:2504.20690, 2025. 5, 6, 14 [49] Haozhe Zhao, Xiaojian Shawn Ma, Liang Chen, Shuzheng Si, Rujie Wu, Kaikai An, Peiyu Yu, Minjia Zhang, Qing Li, and Baobao Chang. Ultraedit: Instruction-based fine-grained image editing at scale. Advances in Neural Information Processing Systems, 37:30583093, 2024. 1, 2, 5, 6, 14 [50] Xiangyu Zhao, Peiyuan Zhang, Kexian Tang, Xiaorong Zhu, Hao Li, Wenhao Chai, Zicheng Zhang, Renqiu Xia, Guangtao Zhai, Junchi Yan, et al. Envisioning beyond the pixels: Benchmarking reasoning-informed visual editing. arXiv preprint arXiv:2504.02826, 2025. 3 [51] Chunting Zhou, Lili Yu, Arun Babu, Kushal Tirumala, Michihiro Yasunaga, Leonid Shamis, Jacob Kahn, Xuezhe Ma, Luke Zettlemoyer, and Omer Levy. Transfusion: Predict the next token and diffuse images with one multi-modal model. arXiv preprint arXiv:2408.11039, 2024. 15 [35] Luozheng Qin, Jia Gong, Yuqing Sun, Tianjiao Li, Mengping Yang, Xiaomeng Yang, Chao Qu, Zhiyu Tan, and Hao Li. Uni-cot: Towards unified chain-of-thought reasoning across text and vision. arXiv preprint arXiv:2508.05606, 2025. 5, 6, [36] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image In Proceedings of synthesis with latent diffusion models. the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. 15 [37] Team Seedream, Yunpeng Chen, Yu Gao, Lixue Gong, Meng Guo, Qiushan Guo, Zhiyao Guo, Xiaoxia Hou, Weilin Huang, Yixuan Huang, et al. Seedream 4.0: Toward nextarXiv preprint generation multimodal image generation. arXiv:2509.20427, 2025. 3, 5, 6, 15 [38] John Sweller. Cognitive bases of human creativity. Educational Psychology Review, 21(1):1119, 2009. 1 [39] Bohan Wang, Zhongqi Yue, Fengda Zhang, Shuo Chen, Lian Bi, Junzhe Zhang, Xue Song, Kennard Yanting Chan, Jiachun Pan, Weijia Wu, et al. Selftok: Discrete visual tokens of autoregression, by diffusion, and for reasoning. arXiv preprint arXiv:2505.07538, 2025. 15 [40] Chenfei Wu, Jiahao Li, Jingren Zhou, Junyang Lin, Kaiyuan Gao, Kun Yan, Sheng-ming Yin, Shuai Bai, Xiao Xu, Yilei Chen, et al. Qwen-image technical report. arXiv preprint arXiv:2508.02324, 2025. 1, 3, 5, 6, [41] Chenyuan Wu, Pengfei Zheng, Ruiran Yan, Shitao Xiao, Xin Luo, Yueze Wang, Wanli Li, Xiyan Jiang, Yexin Liu, Junjie Zhou, et al. Omnigen2: Exploration to advanced multimodal generation. arXiv preprint arXiv:2506.18871, 2025. 3, 5, 6, 15 [42] Yongliang Wu, Zonghui Li, Xinting Hu, Xinyu Ye, Xianfang Zeng, Gang Yu, Wenbo Zhu, Bernt Schiele, MingHsuan Yang, and Xu Yang. Kris-bench: Benchmarking next-level intelligent image editing models. arXiv preprint arXiv:2505.16707, 2025. 2, 3, 7 [43] Bin Xia, Bohao Peng, Yuechen Zhang, Junjia Huang, Jiyang Liu, Jingyao Li, Haoru Tan, Sitong Wu, Chengyao Wang, Yitong Wang, et al. Dreamomni2: Multimodal arXiv preprint instruction-based editing and generation. arXiv:2510.06679, 2025. 5, 6, 7, 15 [44] Shitao Xiao, Yueze Wang, Junjie Zhou, Huaying Yuan, Xingrun Xing, Ruiran Yan, Chaofan Li, Shuting Wang, Tiejun Huang, and Zheng Liu. Omnigen: Unified image generaIn Proceedings of the Computer Vision and Pattern tion. Recognition Conference, pages 1329413304, 2025. 5, 6, 14 [45] Junyan Ye, Dongzhi Jiang, Zihao Wang, Leqi Zhu, Zhenghao Hu, Zilong Huang, Jun He, Zhiyuan Yan, Jinghua Yu, Hongsheng Li, et al. Echo-4o: Harnessing the power of gpt4o synthetic images for improved image generation. arXiv preprint arXiv:2508.09987, 2025. 5, 6, 15 [46] Qifan Yu, Wei Chow, Zhongqi Yue, Kaihang Pan, Yang Wu, Xiaoyang Wan, Juncheng Li, Siliang Tang, Hanwang Zhang, and Yueting Zhuang. Anyedit: Mastering unified high-quality image editing for any idea. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2612526135, 2025. 1, 2, 5, 6,"
        },
        {
            "title": "Appendix Overview",
            "content": "In this supplementary material, we present: Benchmark Details in Section A. Data Collection in Section B. Evaluation Details in Section C. More Experimental Results in Section D. A. Benchmark Details In this section, we provide detailed explanation of the various tasks and knowledge types incorporated within the WiseEdit benchmark. A.1. Task Type The WiseEdit dataset features four distinct types of challenging tasks integrated across the three editing steps: Awareness Task, Interpretation Task, Imagination Task, and WiseEdit-Complex. Case examples illustrating each of these tasks are provided in Fig. 3 of the main paper. Awareness Task. We design Awareness tasks, challenging the model to deduce the target modification area without explicit spatial information provided in the instruction, which necessitates the application of reasoning capabilities. Concrete examples of sub-tasks within the Awareness Task category include (but are not limited to): Property Comparison: Identifying the subject to be edited by comparing the differential properties among multiple similar subjects within the input image. Regional Perception: Determining the target edit region by analyzing the spatial relationship between given detection boxes and specific object in the image. Fine-grained OCR Search: Localizing the exact word mentioned in the instruction among dense text using Optical Character Recognition (OCR) capabilities. Camera Depth: Pinpointing the subject to be edited by comparing the relative camera depths of various objects in the input image. Function Recognition:"
        },
        {
            "title": "Identifying the object",
            "content": "to be edited in the image based on functional description provided in the instruction, differentiating it from other objects with distinct functions. Geographical Locating: Given map (whether of country, continent, or the world), the primary task is to identify the specific provinces or countries that require editing by interpreting the accompanying knowledgebased instructions or descriptive cues. Visual Correspondence via Reference Image: Locating the object in the editing image that has the correct relationship described in the instruction to an object in separate reference image to perform the subsequent edit. 12 Interpretation Task. In the Interpretation task, instructions often do not explicitly state the required modification. The models must leverage world knowledge to parse the implicit intent into directly executable action sequence. Concrete examples of sub-tasks within the Interpretation Task category include (but are not limited to): Error Correctness: Given an image that contains elements inconsistent with real-world plausibility, perform appropriate modifications to correct the error. Violation Correctness: Given an image that contains violations (e.g., policy, safety), perform appropriate deletions and modifications to correct the violation. Common Sense Reasoning: Based on common sense knowledge, predict the resulting changes an implicit instruction will introduce to the image. Cascaded Change Prediction: An instruction that appears straightforward triggers sequence of dependent, cascaded reactions within the scene upon execution. Rule-Based Reasoning: In scenarios governed by explicit rules, such as board games (e.g., chess), correct content in the image that violates those established rules. Complex Referencing: The target object for editing is not given directly but is described through complex descriptions or indirect references (e.g., modifying an object to traditional dish eaten during the Lantern Festival which is round, where the model should first refer the description to tangyuan). Biology-Related Reasoning: Perform inference based on biological knowledge (e.g., predicting the mature form of bamboo shoots). Mathematical Reasoning: Perform inference based on mathematical knowledge (e.g., solving an equation, or predicting the function graph after periodic function completes half cycle). Physics-Related Reasoning: Perform inference based on physical knowledge (e.g., predicting the direction of scales tilt after adding or removing weights from balance beam). Chemistry-Related Reasoning: Perform relevant inference based on chemical knowledge (e.g., predicting the color change of solution after adding specific chemical substance). Logical Reasoning: Deduce the edited image through abstract logical reasoning. Conditional Inference: The given instruction has multiple conditional branches, requiring the model to select and execute the correct branch based on its understanding of the image scenario. Spatial Ordering: Given multiple objects in an image, the model must re-order and reposition them from left to right according to complex instruction. Multi-Instruction Composition: The instruction involves combination of multiple sub-instructions, requiring the model to complete several operations in single, cohesive edit. Imagination Task. Imagination serves as the generative step, rendering the visual edits parsed previously onto the target regions of the original image. For this task, we introduce highly challenging subject-driven generation tasks that require models to perform imaginative and artistic creations while preserving the subjects identity. Concrete examples of sub-tasks within the Interpretation Task category include (but are not limited to): Texture Transfer: Migrating 3D-aware texture onto 2D object to enhance its three-dimensional sense under specific material property. Image Style Transfer: Modifying the design style of an image, such as converting watercolor painting to line drawing, or coloring line drawing with specified palette. Watermark Addition and Removal: Adding watermark to or removing watermark from an image. Counterfactual Scenario Imagination: Generating imaginative counterfactual scenes, such as making car grow wings and fly. Temporal Prediction: Given multi-frame video, predict the subsequent frame. Pose Alteration: Changing subjects pose based on reference image; this may involve combining multiple subjects. Scene Alteration: Changing the background scene where the subject is located, based on reference image. Viewpoint Transformation: Changing the viewpoint of the subject or scene, e.g., from frontal view to topdown (aerial) view. Clothing/Attire Alteration: Changing the subjects clothing based on reference image. Core Subject Identity Alteration: Changing the subjects core appearance, such as having the subject cosplay specific character, altering stylistic details like beards and hairstyles, or generating an image representing their childhood or elderly appearance. Compositional Subject Alteration: Combining the above subject alteration requirement for complex designs. Person-to-Object and Object-to-Person Transformation: This includes transforming person into doll, statue, or virtual/anime character, and vice versa, transforming doll or virtual/anime character into realistic person. Object State Modification: Changing the physical state of an object, such as making it transparent or causing it to explode. scene based on the theme or subject appearance provided in reference image. Graphic Design: Creating flat/2D designs for given character IP, such as posters, mobile wallpapers, comics, game interfaces, etc. Logo Design: Designing logo semantically related to specific concept. Architectural Design: Designing 3D building based on 2D planning information (e.g., blueprints). WiseEdit-Complex. We further designed WiseEditComplex, set of tasks where none of the three editing imaginationcan interpretation, or stepsawareness, be easily completed. This requires the model to fully leverage its capacity for complex reasoning and creative generation. Each example in WiseEdit-Complex combines the difficulty processes of the three aforementioned tasks. Here are few illustrative cases: Example 1: Instruction: Have the person with longer hair in the first image hold the national flag of the country where the second image was taken, wear the clothes of the person in the third image, and wear the hat of the person in the fourth image. Required Steps: Awareness: Identify which person in the first image has longer hair. Interpretation: Deduce the country where the second image was taken and retrieve its national flag. Imagination: Modify the identified person to wear the specific clothing and hat from the third and fourth images, and have them hold the correct national flag. Example 2: Instruction: Identify the club logo in the second image which is most relevant to the third image. Then let the person in the first image put on this clubs home jersey and strike the pose in the fourth image. Required Steps: Awareness: Identify the club logo in the second image and determine its relevance to the third image. Interpretation: Use external knowledge to correctly determine the appearance of that clubs home jersey. Imagination: Render the person in the first image wearing the determined jersey and adopting the specific pose from the fourth image. Example 3: Instruction: Let the person in the first image hold the longest object from the second image. If the object can be used for paper-cutting, have the person face the camera; otherwise, have him back to the camera. Object Restoration: Restoring damaged or incomplete Required Steps: object to its original, complete state. Awareness: Determine the longest object visible in the Themed Style Scene Design: Designing corresponding second image. 13 Interpretation: Analyze the functionality of the identified object to determine if it is typically used for paper-cutting. This determines the correct conditional branch. Imagination: Modify the person in the first image to hold the object and correctly adjust their viewpoint (facing or backing the camera) based on the interpretation result. A.2. Knowledge Type In WiseEdit, we structure our tasks around three core types of knowledge to emulate human cognitive learning: Declarative, Procedural, and Metacognitive knowledge. Building upon this, we further broaden the knowledge scope by covering diverse domains, including natural science, cultural common sense, and spatio-temporal-logic reasoning. is often described Declarative knowledge. as It knowing what, encompasses facts and concepts that can be explicitly stated and defined. In the context of elaborating on different task types, several sub-categories fall under Declarative knowledge. These include: mathematical laws, physical principles, chemical reactions, descriptive definitions of objects, rules for games (like chess), social policies, comparative perceptions, functional descriptions of object properties and so on. For example, given the instruction, Remove the illegal piece according to the Chinese chess rule, the model must possess the declarative knowledge regarding the movement constraints and forbidden zones for each piece in Chinese chess. Procedural knowledge. It is defined as knowing how, encompasses the expertise and skills necessary to perform task, which is often dynamic and difficult to explain verbally. In the context of task types described above, examples such as multi-step instruction following, cascading reaction prediction, time series forecasting, logical reasoning, architectural design, viewpoint transformation, and style transfer are generally categorized as Procedural Knowledge. For example, when required to convert watercolor painting into line drawing, the model relies on procedural knowledge to complete the process systematically. Metacognitive knowledge. It is knowing about knowing, requiring self-awareness and self-regulation during image editing. It dictates the high-level management of when to invoke declarative or procedural knowledge, and how to effectively combine them. In the context of complex taskslike the types described previouslymetacognitive knowledge is essential for conditional branching and controlling subsequent editing actions. For example, executing conditional instruction have the girl hold the longest object in the second image; if the object can be used to brush 14 teeth, let her face the camera, otherwise turn her back to the camera, requires that the model knows when and why to apply the necessary declarative or procedural steps. Knowledge Domain. On this basis, we also systematically encompass three critical knowledge domains. Cultural common-sense evaluates the models understanding of nuanced human experiences and common-sense knowledge (e.g., social customs, daily life), ensuring culturallyappropriate image editing. Natural sciences assess the models comprehension of domain-specific principles (e.g., physical laws, chemical reactions) essential for generating physically consistent images. Spatio-temporal-logical requires the model to possess clear cognition of causality, spatial arrangement, and temporal progression, generating images that adhere to coherent spatio-temporal relations and logical consistency. B. Data Collection Our benchmark images are primarily sourced from two methods: collected from the Internet or generated using generative models. very small portion of the images is drawn from existing datasets. For each test case, the editing instruction is initially created by trained human annotators. To ensure clarity, eliminate ambiguity, and enhance the diversity of the instructions, we utilize GPT-4 [1] to augment the original prompts without altering their semantic meaning. Furthermore, for instructions originally written in English, we also use GPT-4 to generate corresponding Chinese translations. Finally, dual-screening process involving both Ph.D. experts and GPT-4 is employed to filter out unreasonable or unachievable test cases, resulting in final benchmark of 1,220 test cases. Specifically, the Awareness task comprises 362 cases, where 281 cases involve single image input, and 81 cases involve two image inputs. The Interpretation task includes 317 cases, all of which utilize single image input. The Imagination task consists of 451 cases, distributed as follows: 320 cases with single image input, 109 cases with two image inputs, 8 cases with three image inputs, 16 cases with four image inputs, and 8 cases with five image inputs. Furthermore, the WiseEdit-complex subset contains 80 cases, all of which involve multiple image inputs: 47 cases utilize two image inputs, 17 cases utilize three image inputs, and 16 cases utilize four image inputs. C. Evaluation Details Evaluation Models. We evaluate 22 mainstream image editing models across architectures, covering both (1) Diffusion modopenand closed-source models: els: InstructPix2Pix [4], MagicBrush [47], OmniGen [44], AnyEdit [46], UltraEdit [49], ICEdit [48], FLUX.1 Kontext Dev [21], FLUX.2 Dev [20]. Specifically, MagicBrush, AnyEdit and UltraEdit utilize the UNet architecture based on SD 1.5 or SDXL [36], while OmniGen, ICEdit, FLUX.1 Kontext Dev, and FLUX.2 Dev are based on the Diffusion Transformer (DiT) [34] architecture. Notably, FLUX.1 Kontext Dev is large DiT model with 12 billion parameters. While FLUX.2 Dev is 32B DiT model with another 24B model [2] for text encoding. (2) Unified comprehension and generation models: Janus-4o [8], UniWorld-V1 [24], HiDream-E1 [5], OmniGen2 [41], Step1X-Edit-v1p2 [26], Echo-4o [45], Bagel [11], Uni-CoT [35], Qwen-Image-Edit [40], and DreamOmni2 [43]. These models integrate the powerful visual comprehension [3, 7, 22, 30] capabilities of autoregressive Vision-Language Models (VLMs) with diffusion models. They employ three primary architectures: (a) Quantized AR: leveraging autoregressive visual generation [10, 25, 29, 3133, 39] with discrete visual tokenizers, including Janus-4o. (b) Cascaded Architecture: Some models implement system where an external diffusion model is cascaded after the VLMs output, including UniWorld-V1, HiDream-E1, OmniGen2, Step1X-Editv1p2, Qwen-Image-Edit, and DreamOmni2. (c) Integrated Architecture: Other models, i.e., Echo-4o, Uni-CoT, and Bagel introduce mixture-of-transformer approach within single, integrated model. Here, the model conducts autoregression to handle text generation while conducting diffusion to manage image generation. This configuration allows the model to simultaneously perform both visual understanding and visual generation in an integrated model. (3) Close-sourced Models: Nano Banana [14], Seedream 4.0 [37], GPT-image-1 [28], and Nano Banana Pro [16]. These models do not have publicly available checkpoint weights; access is exclusively provided via API calls. Notably, these models, like those in the second category, are also unified comprehension and generation models. Specifically, Seedream 4.0s technical report explicitly states that it leverages the comprehension capabilities of VLM to aid in generation like [6, 51]. Although GPTimage-1 and Nano Banana have not released technical reports detailing their methodology, the prevailing view in the community suggests they are combination of VLM and diffusion models like [9, 13]. And Nano Banana Pro, in turn, leveraged the powerful multimodal understanding capabilities of Gemini 3 [15]. Evaluation Metrics. As detailed in Section 3.3, the evaluation of WiseEdit employs comprehensive set of six metrics: Instruction Following, Detail Preserving and Visual Quality, Knowledge Fidelity, and Creative Fusion. Specifically, for the Awareness and Interpretation tasks, we utilize Instruction Following, Detail Preserving, and Visual Quality, and Knowledge Fidelity. For the Imagination task, the metrics used are Instruction Following, Detail Preserving and Visual Quality, and Creative Fusion. Finally, the WiseEdit-Complex task is evaluated using all four metrics. For the assessment of all these metrics, we leverage GPT-4o as the automatic evaluator, which rates performance on 110 scale based on carefully customcrafted and dimension-specific prompts. The prompt templates for Instruction Following with both single-image input and multi-image input are in Fig. 12 and Fig. 13, respectively. The prompt templates for Detail Preserving with both single-image input and multi-image input are in Fig. 14 and Fig. 15, respectively. The prompt templates for Visual Quality with both single-image input and multi-image input are in Fig. 16. The prompt templates for Knowledge Fidelity with both single-image input and multi-image input are in Fig. 17 and Fig. 18, respectively. The prompt templates for Creative Fusion with both single-image input and multi-image input are in Fig. 19 and Fig. 20, respectively. It is worth noting that for some knowledge-informed test cases, we additionally provide knowledge hints and hint reference image to aid in the assessment of metrics such as Knowledge Fidelity. Finally, we linearly map each 1-to-10 score provided by the evaluator to 0-to-100 range and then compute the average score for each task. The overall average score is the average of the average scores from the Awareness, Interpretation, and Imagination tasks. Besides, any test case where model fails to handle the input is assigned score of zero across all metrics. D. More Experimental Results Performance on WiseEdit with Single-Image Inputs. In the WiseEdit benchmark, the input for many test cases is highly free-form, often incorporating multiple input images. To better differentiate models proficiency in handling single-image inputs from its overall capability with freeform inputs, we calculate the models performance specifically on the single-image input subsets for each task and compare these metrics against the original performance in Table 5 and Table 6. Notably, the Interpretation task exclusively features single-image inputs, meaning its singleimage results are identical to those reported in Table 2 and are thus not presented again in Table 5 and Table 6. Besides, the WiseEdit-Complex task is composed entirely of multi-image inputs, for which we can not calculate separate single-image results. The results reveal that for the majority of models, the average performance across all cases is significantly lower than the average performance observed for single-image input. This observation highlights that improving capability in complex, multi-image scenarios is key direction for future image editing models. Qualitative Comparisons of with think v.s. w/o think. To illustrate how visual comprehension enhances visual Dev, FLUX.2 Dev, Bagel, Qwen-Image-Edit, OmniGen2, DreamOmni2, Seedream 4.0, GPT-Image-1, Nano Banana and Nano Banana Pro. For any model that is unable to process given input, its output is indicated by an image with prohibition symbol. Figure 6. Qualitative comparisons of Bagel and DreamOmni2 with and without their built-in thinking processes. generation in unified models, we present qualitative comparisons of Bagel and DreamOmni2 with and without their built-in reasoning processes. As shown in Fig. 6, disabling this mechanism leads to noticeable degradation in generation quality, underscoring the critical role of visual comprehension in improving visual generation. Qualitative Comparisons of original instructions v.s. rewritten instructions. To visualize the impact of instruction rewriting, we give some qualitative comparisons before and after instruction rewriting based on Bagel and GPT-Image-1. As shown in Fig. 7, for Bagel, the rewritten instruction provides additional knowledge, which significantly improves the editing results. Whereas for GPTImage-1, its internal comprehension mechanisms have already acquired the knowledge externally introduced by rewriting, thus the improvement brought by rewriting appears less pronounced. More Qualitative Comparisons. In Fig. 8, Fig. 9, Fig. 10, and Fig. 11, we present additional qualitative comparisons across four tasks, i.e., Awareness, Interpretation, Imagination, and WiseEdit-Complex. We show the generated results of nine models, e.g., AnyEdit, FLUX.1 Kontext Table 5. Main results on WiseEdit across all cases or only single-image inputs on English version of instructions. For the majority of models, the average performance across all cases is lower than the average performance observed for single-image input. Model Cases IF DP Awareness Task VQ KF AVG IF DP Imagination Task VQ CF AVG InstructPix2Pix MagicBrush OmniGen AnyEdit UltraEdit ICEdit FLUX.1 Kontext Dev FLUX.2 Dev Janus-4o UniWorld-V1 HiDream-E OmniGen2 Step1X-Edit-v1p2 Echo-4o Bagel Uni-CoT Qwen-Image-Edit DreamOmni2 Nano Banana Seedream 4.0 GPT-image-1 Nano Banana Pro all cases sing-img cases all cases sing-img cases all cases sing-img cases all cases sing-img cases all cases sing-img cases all cases sing-img cases all cases sing-img cases all cases sing-img cases all cases sing-img cases all cases sing-img cases all cases sing-img cases all cases sing-img cases all cases sing-img cases all cases sing-img cases all cases sing-img cases all cases sing-img cases all cases sing-img cases all cases sing-img cases all cases sing-img cases all cases sing-img cases all cases sing-img cases all cases sing-img cases 24.6 33.2 (+8.6) 27.2 36.6 (+9.3) 35.0 34.6 (-0.4) 25.0 32.9 (+7.9) 26.5 35.7 (+9.1) 26.1 34.9 (+8.8) 31.4 40.7 (+9.3) 42.6 44.5 (+1.8) 34.7 46.1 (+11.5) 31.5 31.9 (+0.5) 29.7 39.7 (+10.1) 35.0 36.4 (+1.4) 39.8 52.9 (+13.1) 47.6 52.7 (+5.1) 46.2 50.5 (+4.3) 46.0 50.6 (+4.7) 48.1 57.7 (+9.6) 43.3 46.7 (+3.4) 70.6 69.6 (-0.9) 70.8 70.2 (-0.5) 78.5 83.5 (+5.0) 85.4 90.0 (+4.6) 33.7 44.9 (+11.2) 43.4 57.4 (+14.0) 42.0 43.6 (+1.6) 54.6 68.0 (+13.4) 42.5 56.3 (+13.8) 42.2 55.6 (+13.4) 52.0 67.2 (+15.2) 63.3 62.6 (-0.8) 37.0 49.1 (+12.2) 48.9 46.8 (-2.1) 41.2 54.6 (+13.4) 64.0 65.6 (+1.6) 53.5 70.5 (+17.0) 63.0 65.8 (+2.9) 71.0 73.3 (+2.3) 69.1 71.7 (+2.6) 69.0 73.2 (+4.2) 74.4 77.1 (+2.6) 85.7 86.2 (+0.5) 78.1 78.8 (+0.7) 85.8 87.8 (+2.0) 88.6 88.1 (-0.6) 50.6 66.7 (+16.1) 53.3 70.2 (+16.9) 46.7 48.5 (+1.9) 61.3 72.8 (+11.5) 53.1 69.9 (+16.8) 61.2 80.1 (+18.9) 55.0 71.0 (+16.1) 78.4 81.5 (+3.1) 45.9 60.7 (+14.7) 58.8 59.5 (+0.6) 56.3 74.1 (+17.7) 75.4 75.3 (-0.1) 61.3 80.6 (+19.3) 75.4 77.7 (+2.3) 75.8 79.8 (+3.9) 77.8 79.6 (+1.7) 79.5 82.5 (+3.0) 85.0 86.0 (+1.0) 86.8 88.2 (+1.4) 86.6 87.5 (+0.9) 88.0 88.8 (+0.8) 83.9 84.3 (+0.4) English Version 33.8 45.1 (+11.3) 37.8 50.1 (+12.4) 40.3 40.3 (+0.1) 41.8 51.8 (+10.0) 39.0 51.8 (+12.8) 40.4 53.2 (+12.9) 43.5 56.2 (+12.8) 59.4 60.0 (+0.6) 38.5 51.0 (+12.6) 44.5 44.3 (-0.1) 39.8 52.8 (+13.0) 53.9 54.6 (+0.7) 49.7 65.7 (+15.9) 59.4 62.5 (+3.1) 61.0 63.4 (+2.4) 61.1 63.4 (+2.3) 62.5 68.5 (+6.0) 63.5 65.6 (+2.1) 79.6 79.5 (-0.1) 77.5 77.1 (-0.5) 83.3 85.8 (+2.5) 87.3 88.4 (+1.1) 26.4 35.5 (+9.1) 27.1 36.4 (+9.3) 37.4 34.7 (-2.7) 26.3 33.5 (+7.2) 33.9 45.2 (+11.3) 31.8 42.3 (+10.4) 35.5 45.9 (+10.5) 53.3 51.6 (-1.7) 36.2 48.2 (+11.9) 38.6 39.1 (+0.5) 32.0 42.7 (+10.7) 41.3 41.2 (-0.1) 44.4 58.8 (+14.4) 51.7 53.7 (+2.0) 50.8 50.0 (-0.8) 51.6 51.7 (+0.2) 53.6 60.8 (+7.2) 51.2 52.5 (+1.4) 75.2 73.8 (-1.4) 74.6 71.7 (-2.9) 81.2 83.2 (+2.0) 91.4 91.2 (-0.2) 17.0 25.9 (+9.0) 18.0 27.5 (+9.4) 42.2 47.9 (+5.7) 9.1 12.9 (+3.8) 20.7 31.4 (+10.6) 21.5 32.1 (+10.6) 39.1 55.9 (+16.8) 73.6 78.8 (+5.3) 28.2 42.2 (+13.9) 30.3 31.9 (+1.6) 39.6 58.5 (+18.9) 42.0 41.1 (-0.9) 44.7 66.0 (+21.3) 63.4 70.8 (+7.4) 62.8 72.4 (+9.6) 67.6 76.0 (+8.4) 67.1 78.8 (+11.6) 50.6 54.1 (+3.5) 75.3 83.7 (+8.4) 82.2 88.5 (+6.2) 84.4 91.4 (+7.1) 86.6 89.7 (+3.1) 29.9 44.5 (+14.7) 36.9 54.6 (+17.7) 35.1 35.7 (+0.6) 49.7 70.1 (+20.4) 31.7 47.2 (+15.5) 40.6 59.6 (+19.0) 47.1 67.5 (+20.3) 70.7 68.9 (-1.8) 37.6 55.6 (+18.0) 50.3 56.3 (+5.9) 40.1 59.3 (+19.2) 64.4 70.9 (+6.5) 49.4 72.8 (+23.4) 62.4 65.2 (+2.8) 68.5 75.0 (+6.5) 64.3 67.8 (+3.6) 66.8 73.7 (+6.9) 64.9 68.0 (+3.0) 73.8 76.1 (+2.3) 77.8 81.8 (+3.9) 76.2 77.3 (+1.1) 79.5 78.9 (-0.7) 41.2 60.8 (+19.6) 44.8 66.1 (+21.3) 46.0 44.7 (-1.2) 50.9 66.6 (+15.7) 45.8 67.5 (+21.7) 54.0 78.9 (+24.9) 43.4 62.1 (+18.7) 82.1 82.9 (+0.8) 42.0 62.0 (+20.0) 64.2 66.2 (+2.0) 49.9 73.3 (+23.5) 74.6 74.7 (+0.1) 50.3 74.1 (+23.8) 73.7 72.7 (-1.0) 74.5 75.1 (+0.6) 79.6 80.0 (+0.5) 79.2 81.9 (+2.7) 81.9 84.2 (+2.3) 87.3 89.6 (+2.3) 86.9 88.9 (+2.0) 89.2 91.1 (+1.9) 88.8 89.5 (+0.7) 27.4 40.9 (+13.6) 22.3 33.7 (+11.3) 38.7 41.5 (+2.8) 16.5 18.4 (+2.0) 27.5 41.2 (+13.6) 25.0 37.2 (+12.1) 27.1 38.6 (+11.5) 43.6 45.7 (+2.1) 25.5 38.2 (+12.7) 27.5 26.3 (-1.2) 29.6 44.1 (+14.5) 31.8 29.6 (-2.3) 28.4 42.5 (+14.1) 41.2 44.0 (+2.8) 40.7 42.8 (+2.1) 42.9 45.2 (+2.3) 42.3 45.1 (+2.8) 35.3 34.5 (-0.8) 44.3 47.8 (+3.5) 47.0 47.1 (+0.1) 48.4 49.9 (+1.5) 51.5 52.3 (+0.8) 28.8 43.1 (+14.2) 30.5 45.5 (+14.9) 40.5 42.5 (+2.0) 31.5 42.0 (+10.5) 31.5 46.8 (+15.4) 35.3 51.9 (+16.6) 39.2 56.0 (+16.8) 67.5 69.1 (+1.6) 33.3 49.5 (+16.2) 43.1 45.2 (+2.1) 39.8 58.8 (+19.0) 53.2 54.1 (+0.9) 43.2 63.9 (+20.7) 60.2 63.2 (+3.0) 61.6 66.3 (+4.7) 63.6 67.3 (+3.7) 63.8 69.9 (+6.0) 58.2 60.2 (+2.0) 70.2 74.3 (+4.1) 73.5 76.6 (+3.1) 74.6 77.5 (+2.9) 76.6 77.6 (+1.0) Overall AVG 34.4 42.8 (+8.5) 35.5 44.6 (+9.1) 36.6 37.2 (+0.7) 37.7 44.5 (+6.8) 39.0 48.4 (+9.4) 39.9 49.7 (+9.8) 43.2 53.1 (+9.9) 61.8 62.5 (+0.7) 36.7 46.2 (+9.6) 41.1 41.8 (+0.7) 41.4 52.1 (+10.7) 49.4 49.9 (+0.5) 49.5 61.7 (+12.2) 57.8 59.8 (+2.0) 60.0 62.3 (+2.4) 60.1 62.0 (+2.0) 60.2 64.2 (+4.0) 60.6 61.9 (+1.4) 75.0 76.4 (+1.3) 75.2 76.1 (+0.9) 77.6 79.4 (+1.8) 82.4 82.9 (+0.5) 17 Table 6. Main results on WiseEdit across all cases or only single-image inputs on Chinese version of instructions. For the majority of models, the average performance across all cases is lower than the average performance observed for single-image input. Model Cases IF DP Awareness Task VQ KF AVG IF DP Imagination Task VQ CF AVG InstructPix2Pix MagicBrush OmniGen AnyEdit UltraEdit ICEdit FLUX.1 Kontext Dev FLUX.2 Dev Janus-4o UniWorld-V HiDream-E1 OmniGen2 Step1X-Edit-v1p2 Echo-4o Bagel Uni-CoT Qwen-Image-Edit DreamOmni2 Nano Banana Seedream 4.0 GPT-image-1 Nano Banana Pro all cases sing-img cases all cases sing-img cases all cases sing-img cases all cases sing-img cases all cases sing-img cases all cases sing-img cases all cases sing-img cases all cases sing-img cases all cases sing-img cases all cases sing-img cases all cases sing-img cases all cases sing-img cases all cases sing-img cases all cases sing-img cases all cases sing-img cases all cases sing-img cases all cases sing-img cases all cases sing-img cases all cases sing-img cases all cases sing-img cases all cases sing-img cases all cases sing-img cases 14.2 19.8 (+5.6) 15.0 20.8 (+5.8) 16.3 16.6 (+0.3) 17.5 23.4 (+5.9) 16.9 23.3 (+6.4) 12.9 18.1 (+5.2) 16.5 22.8 (+6.3) 43.0 43.4 (+0.4) 31.1 41.6 (+10.5) 18.4 18.9 (+0.6) 28.2 37.9 (+9.6) 35.1 38.9 (+3.8) 38.6 51.4 (+12.7) 47.9 53.3 (+5.4) 48.5 52.4 (+3.9) 46.2 48.7 (+2.5) 45.0 51.7 (+6.7) 31.9 34.2 (+2.3) 71.8 71.4 (-0.4) 69.1 69.7 (+0.6) 77.0 83.0 (+6.0) 84.6 86.5 (+1.9) 51.0 67.2 (+16.2) 43.8 57.9 (+14.1) 42.6 47.8 (+5.1) 55.3 67.7 (+12.4) 58.5 76.9 (+18.4) 29.1 39.0 (+9.9) 48.4 63.9 (+15.5) 60.6 60.5 (-0.1) 38.6 51.3 (+12.6) 49.0 49.1 (+0.1) 37.6 49.9 (+12.3) 57.9 58.9 (+1.0) 55.6 73.2 (+17.6) 59.9 62.8 (+2.9) 71.3 74.7 (+3.4) 70.0 72.2 (+2.1) 67.3 70.8 (+3.5) 78.7 84.1 (+5.3) 83.8 84.3 (+0.5) 79.0 78.2 (-0.8) 80.7 80.6 (-0.2) 91.8 92.5 (+0.6) 58.6 77.0 (+18.4) 52.9 69.6 (+16.7) 60.1 61.1 (+1.0) 55.7 66.7 (+11.0) 62.1 81.5 (+19.4) 63.6 83.5 (+19.8) 52.1 68.7 (+16.5) 79.5 83.6 (+4.1) 46.3 61.1 (+14.8) 60.0 61.7 (+1.7) 51.4 67.7 (+16.3) 72.4 73.0 (+0.5) 59.5 78.3 (+18.8) 73.1 74.7 (+1.7) 76.8 80.4 (+3.6) 80.7 82.5 (+1.8) 79.9 84.1 (+4.2) 85.4 88.5 (+3.1) 86.5 88.2 (+1.7) 84.4 85.0 (+0.6) 86.6 88.0 (+1.4) 83.1 83.3 (+0.2) Chinese Version 35.5 47.2 (+11.7) 32.4 43.2 (+10.8) 35.4 36.8 (+1.5) 37.1 45.9 (+8.7) 39.7 52.6 (+12.9) 30.8 41.1 (+10.4) 34.0 45.4 (+11.3) 58.6 58.8 (+0.2) 37.5 49.8 (+12.3) 38.4 38.7 (+0.4) 37.4 49.7 (+12.3) 51.6 53.4 (+1.8) 48.9 64.6 (+15.7) 59.0 62.0 (+3.0) 62.2 64.9 (+2.8) 62.6 64.4 (+1.8) 61.3 65.9 (+4.6) 58.6 61.1 (+2.5) 78.2 78.0 (-0.2) 76.1 75.5 (-0.6) 81.2 83.6 (+2.4) 86.9 87.5 (+0.6) 18.1 24.8 (+6.7) 17.8 24.4 (+6.6) 22.6 21.9 (-0.6) 19.9 25.6 (+5.7) 21.2 28.8 (+7.6) 17.4 23.9 (+6.5) 19.1 26.1 (+7.0) 51.4 47.8 (-3.5) 34.1 45.4 (+11.3) 26.2 25.3 (-0.9) 32.4 43.3 (+10.8) 41.0 42.7 (+1.7) 42.0 55.7 (+13.7) 55.0 57.2 (+2.2) 52.1 52.4 (+0.3) 53.6 54.4 (+0.8) 52.9 57.0 (+4.0) 38.4 37.7 (-0.7) 70.7 67.9 (-2.8) 72.0 69.4 (-2.6) 80.7 82.9 (+2.3) 87.9 87.6 (-0.2) 3.7 6.9 (+3.1) 5.4 9.3 (+3.9) 15.4 16.8 (+1.4) 7.3 10.3 (+3.0) 9.3 14.8 (+5.6) 5.1 8.9 (+3.8) 9.2 14.8 (+5.6) 75.5 80.7 (+5.2) 25.4 38.0 (+12.7) 17.9 12.6 (-5.3) 32.5 48.3 (+15.8) 45.5 42.8 (-2.7) 45.7 67.4 (+21.8) 62.8 70.3 (+7.5) 63.5 70.4 (+6.9) 65.5 74.0 (+8.5) 66.3 78.9 (+12.6) 38.5 38.4 (-0.0) 76.0 84.9 (+8.9) 79.8 87.5 (+7.7) 78.8 86.7 (+7.9) 85.5 91.0 (+5.5) 52.0 76.4 (+24.4) 39.6 58.6 (+19.0) 27.6 32.5 (+4.9) 47.4 67.0 (+19.5) 42.3 62.5 (+20.1) 37.4 55.4 (+18.0) 41.9 61.9 (+20.0) 74.4 75.2 (+0.8) 36.9 54.6 (+17.7) 54.8 62.2 (+7.4) 39.2 58.0 (+18.8) 64.0 66.4 (+2.5) 48.3 71.2 (+22.9) 64.2 64.4 (+0.2) 68.3 73.7 (+5.3) 65.1 69.0 (+3.9) 67.2 73.9 (+6.7) 69.4 74.7 (+5.2) 75.8 77.7 (+1.9) 79.7 81.6 (+1.9) 73.3 69.5 (-3.9) 77.6 77.4 (-0.2) 53.2 78.2 (+24.9) 49.2 72.4 (+23.2) 51.3 52.3 (+1.0) 52.3 68.4 (+16.1) 51.8 76.2 (+24.3) 56.5 82.9 (+26.4) 43.3 63.9 (+20.6) 82.8 84.1 (+1.3) 41.8 61.7 (+19.9) 68.9 71.3 (+2.4) 47.5 69.9 (+22.4) 72.0 71.9 (-0.1) 51.3 75.5 (+24.2) 75.1 74.6 (-0.5) 75.3 76.5 (+1.1) 79.7 81.2 (+1.4) 80.0 83.8 (+3.7) 84.8 87.0 (+2.1) 87.3 90.5 (+3.2) 86.5 88.8 (+2.2) 89.6 91.4 (+1.8) 88.4 90.4 (+2.1) 9.0 14.4 (+5.5) 12.7 19.8 (+7.1) 32.3 33.2 (+0.9) 15.4 17.4 (+2.0) 14.8 22.8 (+8.0) 16.7 25.6 (+8.9) 10.6 16.7 (+6.2) 42.6 44.3 (+1.8) 22.1 33.3 (+11.2) 18.0 15.5 (-2.5) 27.4 40.9 (+13.6) 33.8 31.2 (-2.6) 27.0 40.5 (+13.5) 41.5 43.5 (+2.1) 39.7 41.9 (+2.2) 41.6 44.0 (+2.4) 41.7 44.4 (+2.7) 27.2 25.5 (-1.6) 43.7 46.8 (+3.1) 46.4 46.6 (+0.2) 48.3 50.0 (+1.7) 51.1 52.2 (+1.1) 29.5 44.0 (+14.5) 26.8 40.1 (+13.3) 31.7 33.7 (+2.0) 30.6 40.8 (+10.2) 29.5 44.1 (+14.5) 28.9 43.2 (+14.2) 26.3 39.3 (+13.1) 68.8 71.1 (+2.3) 31.5 46.9 (+15.4) 39.9 40.4 (+0.5) 36.6 54.3 (+17.6) 53.8 53.1 (-0.7) 43.1 63.7 (+20.6) 60.9 63.2 (+2.3) 61.7 65.6 (+3.9) 63.0 67.0 (+4.1) 63.8 70.3 (+6.4) 55.0 56.4 (+1.4) 70.7 75.0 (+4.3) 73.1 76.1 (+3.0) 72.5 74.4 (+1.9) 75.6 77.8 (+2.1) Overall AVG 34.1 42.9 (+8.7) 30.0 38.0 (+8.0) 31.3 32.5 (+1.2) 34.0 40.3 (+6.3) 38.9 48.0 (+9.2) 32.7 40.9 (+8.2) 33.7 41.8 (+8.1) 61.4 62.2 (+0.8) 35.6 44.9 (+9.2) 37.7 37.9 (+0.3) 38.5 48.5 (+10.0) 48.8 49.1 (+0.3) 49.6 61.7 (+12.1) 58.0 59.8 (+1.8) 59.5 61.7 (+2.2) 60.6 62.5 (+2.0) 60.6 64.3 (+3.7) 56.0 57.3 (+1.3) 75.3 76.6 (+1.3) 74.1 74.9 (+0.8) 76.2 77.6 (+1.4) 81.2 81.9 (+0.7) 18 Figure 7. Qualitative comparisons before and after instruction rewriting based on Bagel and GPT-Image-1. 19 Figure 8. More qualitative comparisons on the Awareness task. Figure 9. More qualitative comparisons on the Interpretation task. 21 Figure 10. More qualitative comparisons on the Imagination task. 22 Figure 11. More qualitative comparisons on the WiseEdit-Complex task. Figure 12. Prompt template for the Instruction Following metric when handling single-image inputs. 24 Figure 13. Prompt template for the Instruction Following metric when handling multi-image inputs. 25 Figure 14. Prompt template for the Detail Preserving metric when handling single-image inputs. Figure 15. Prompt template for the Detail Preserving metric when handling multi-image inputs. 27 Figure 16. Prompt template for the Visual Quality metric (both handling single-image and multi-image inputs). 28 Figure 17. Prompt template for the Knowledge Fidelity metric when handling single-image inputs. Figure 18. Prompt template for the Knowledge Fidelity metric when handling multi-image inputs. 30 Figure 19. Prompt template for the Creative Fusion metric when handling single-image inputs. 31 Figure 20. Prompt template for the Creative Fusion metric when handling multi-image inputs."
        }
    ],
    "affiliations": [
        "Shanghai Artificial Intelligence Laboratory",
        "Zhejiang University"
    ]
}