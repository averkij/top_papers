{
    "paper_title": "Audit & Repair: An Agentic Framework for Consistent Story Visualization in Text-to-Image Diffusion Models",
    "authors": [
        "Kiymet Akdemir",
        "Tahira Kazimi",
        "Pinar Yanardag"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Story visualization has become a popular task where visual scenes are generated to depict a narrative across multiple panels. A central challenge in this setting is maintaining visual consistency, particularly in how characters and objects persist and evolve throughout the story. Despite recent advances in diffusion models, current approaches often fail to preserve key character attributes, leading to incoherent narratives. In this work, we propose a collaborative multi-agent framework that autonomously identifies, corrects, and refines inconsistencies across multi-panel story visualizations. The agents operate in an iterative loop, enabling fine-grained, panel-level updates without re-generating entire sequences. Our framework is model-agnostic and flexibly integrates with a variety of diffusion models, including rectified flow transformers such as Flux and latent diffusion models such as Stable Diffusion. Quantitative and qualitative experiments show that our method outperforms prior approaches in terms of multi-panel consistency."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 2 ] . [ 1 0 0 9 8 1 . 6 0 5 2 : r Audit & Repair: An Agentic Framework for Consistent Story Visualization in Text-to-Image Diffusion Models Kiymet Akdemir Tahira Kazimi Pinar Yanardag Virginia Tech {kiymet, tahirakazimi, pinary}@vt.edu http://auditandrepair.github.io Figure 1. Given multi-panel story prompt, our method identifies and corrects visual inconsistencies (e.g., wrong cape color or inconsistent outfit) in character and scene elements across story panels. Compared to base model (DSD [3] above), our framework achieves coherent character appearance and narrative consistency throughout the story visualization."
        },
        {
            "title": "Abstract",
            "content": "Story visualization has become popular task where visual scenes are generated to depict narrative across multiple panels. central challenge in this setting is maintaining visual consistency, particularly in how characters and objects Joint first-authors. persist and evolve throughout the story. Despite recent advances in diffusion models, current approaches often fail to preserve key character attributes, leading to incoherent narratives. In this work, we propose collaborative multi-agent framework that autonomously identifies, corrects, and refines inconsistencies across multi-panel story visualizations. The agents operate in an iterative loop, enabling fine-grained, panel-level updates without re-generating entire sequences. Our framework is model-agnostic and flexibly integrates with variety of diffusion models, including rectified flow transformers such as Flux and latent diffusion models such as Stable Diffusion. Quantitative and qualitative experiments show that our method outperforms prior approaches in terms of multi-panel consistency. 1. Introduction Text-to-image (T2I) diffusion models such as Stable Diffusion [25] and Flux [14] have dramatically advanced the fidelity, diversity, and accessibility of image synthesis. These generative models are reshaping creative industries and everyday artistic workflows by enabling the rapid production of illustrations, concept art, and graphics from short text prompts [2426]. Building on this momentum, story visualization has emerged as popular task focused on generating sequence of images that together convey coherent narrative [16, 23, 40]. Unlike single-image generation, which captures only single scene, story visualization requires generating sequence of panels that follow textual narrative composed of successive sentences or captions. This capability enables compelling applications, including automatic illustration of picture books and comics, rapid generation of storyboards for film production, and interactive visual narratives for video games. However, the main challenge is achieving coherent visual continuity across story panels. Characters, or objects must remain identifiable and stable throughout the narrative. For example, if the character appears in distinctive striped red t-shirt in the opening frame, the same t-shirt must persist, unaltered, in every subsequent frame in which the character appears, unless noted otherwise by the story prompts. Naively applying text-to-image model independently to each caption rarely meets this requirement, as subtle drifts in appearance disrupt narrative coherence. Ensuring such continuity is therefore central, yet unsolved, challenge for generative models [8, 14, 25]. Recent approaches such as AutoStudio [5], StoryDiffusion [40], and StoryGen [17] have made progress toward improving consistency in story visualization. However, while they capture general coherence, they often fail to maintain fine-grained consistency in character details and object attributes across panels. Most importantly, these methods treat consistency as an implicit property of the architecture or training loss; once inconsistencies appear, they lack principled way to detect and repair them at inference time. In this paper, we present collaborative multi-agent framework for consistent story visualization that treats the task as an iterative process of refinement across specialized, autonomous components. Rather than relying on end-to-end generation, our framework decomposes the workflow into agents responsible for initializing panels, detecting inconsistencies, refining prompts, and applying visual corrections. key novelty of our approach is consistency agent powered by VisionLanguage Model (VLM) that automatically generates textual descriptions of each panel, compares them across the sequence, and flags mismatches in character appearance or object attributes. These flagged inconsistencies are then selectively corrected by other agents, enabling finegrained updates without the need to re-generate unaffected panels. Our framework is fully model-agnostic and can be integrated with various diffusion backbonesincluding Flux [14], demonstrating substantial improvements in both character and object consistency without any retraining. We propose collaborative multi-agent framework for story visualization that treats consistency correction as an autonomous, post-generation process across modular agents. We formulate inconsistency detection as visionlanguage alignment task, using VLM-generated descriptions to identify fine-grained mismatches in character and object attributes across panels. Our framework introduces the first iterative audit-andrepair loop for multi-frame consistency that operates independently of the diffusion backbone, and is compatible with both Stable Diffusion and Flux generated stories. 2. Related Work Story Visualization Recent work has explored diffusionbased methods for story visualization, with growing emphasis on coherence and controllability. StoryDiffusion [40] and ConsiStory [29] generate multiple frames in parallel using shared self-attention to model narrative consistency. While effective for global coherence, both approaches rely heavily on user-supplied prompts and struggle to maintain consistent character appearances across frames. TaleCrafter [7] and AutoStory [30] adopt inpainting-based pipelines using LoRA [9] modules, but require finetuning for each LoRA model and depend on structured inputs such as prompts, bounding boxes, or sketches. AR-LDM [21], StoryGen [17], Make-a-Story [23], SEED-Story [37], and StoryImager [28] follow autoregressive pipelines conditioned on prior frames or captions. While these models promote temporal coherence, they often require dataset-specific training and may compromise visual fidelity or efficiency relative to base diffusion models. OnePrompt-OneStory [18] generates full story sequences from single paragraph prompt and improves coherence using identity-preserving cross-attention, but offers limited control over per-frame layout and appearance. AutoStudio [5] enhances identity consistency using dedicated U-Net, though this adds to the models complexity. Rather than modifying the generation pipeline or requiring specialized architectures, we take different perspective: treating consistency as post-processing problem. Our agent-based framework detects and resolves inconsistencies after generation, offering modular and model-agnostic solution. AI Agents for Image Generation. Recently, the idea of AI agents has been applied to various image generation tasks, with large language models (LLMs) acting as coordinators. HuggingGPT [27] and Visual ChatGPT [32] employ LLMs to select and orchestrate multiple tools, enabling multi-step workflows for image generation and editing. SLD [34] introduces an LLM-driven self-correction loop, where outputs are iteratively refined by identifying and addressing prompt mismatches, however their method focuses on improving single-image prompt alignment, rather than multiframe consistency. Similarly, VideoRepair [15] improves text-video alignment by identifying fine-grained mismatches. For more compositional and structured tasks, GenArtist [31] and MUSES [6] coordinate subtasks such as planning, rendering, and editing to generate coherent multi-object or 3D scenes. MM-StoryAgent [36] and StoryAgent [10] apply agent-based designs to visual storytelling: MM-StoryAgent integrates text, image, and audio experts to produce storybook videos, while StoryAgent organizes separate agents for writing, image generation, and editingbut requires reference videos of the characters as input, which poses significant burden for users. Unlike prior work, our system operates across multiple frames to directly address visual consistency. Specialized agents detect inconsistencies, refine prompts, and apply localized edits in an iterative loop without retraining or architectural changes. 3. Methodology Problem Definition Given narrative expressed as panellevel prompts = P1, . . . , PN and metadata dictionary containing details of the characters or objects, the task is to produce sequence of images = I1, . . . , IN such that every recurring character retains all invariant attributes (e.g., hair color, facial features), intentional changes explicitly described in Pi are reflected, and other visual details follow the output of the backbone model. The naive strategy of feeding each panel description to T2I backbone in isolation provides no mechanism for propagating character identity across panels, so immutable attributes, such as hairstyle, clothing color, and facial structure, drift unpredictably. Existing story-visualization methods attempt to enforce consistency through training or attentionbased mechanisms, but they still treat coherence as coarse, global property rather than fine-grained, entity-centric constraint. Without an explicit audit-and-repair loop that separates accidental drift from intentional narrative changes, both naïve generation and attention-augmented models fail to produce sequences that are simultaneously faithful to each prompt and visually coherent over the full story. 3.1. Multi-Agent Collaborative Design To overcome these limitations, we propose collaborative multi-agent framework that separates generation and correction into modular, specialized agents. Communication is mediated through shared memory, central data store that continuously tracks the current panel set, the latest consistency report, and the evolving Consistency Index. Each agent autonomously tackles specific subtask, such as detecting inconsistencies, refining prompts, or applying localized visual edits, within an iterative audit-and-repair loop. This design enables post-hoc refinement of story visualizations, introducing targeted corrections while preserving content that is already correct. Moreover, our framework is model-agnostic and can plug into variety of T2I diffusion backbones, including Stable Diffusion and Flux, without any training or fine-tuning. Please refer to Fig. 2 which depicts how our consistency-oriented story-visualization pipeline works. Examples of the prompts used to guide each agent are included in the supplementary material. Our agents are described as follows. Story Initialization Agent (ASIA) Given the panel-level prompt set = {P1, . . . , PN } and entity metadata , this agent generates the initial image sequence = {I1, . . . , IN } using either story generation model (e.g., StoryDiffusion [40]) or an editing-based model (e.g., DSD [3]). For editing-based methods, we first use the character descriptions from to synthesize single reference image using prompt such as \"a girl with dress and golden dog\". Then for each panel, we feed the pair (R, Pi) into the editing model to generate the corresponding image Ii. For story generation methods, we prepend the merged character descriptions as an additional prompt to the prompt set, creating an extended prompt set = {P0, P1, . . . , PN } where P0 contains only character descriptions. The image generated from P0 is used as the reference image R, and the rest of the prompt set {P1, . . . , PN } is used to generate the full story sequence = {I1, . . . , IN }. Audit Agent (AAudit) The Audit Agent identifies inconsistencies and generates correction instructions. It begins by prompting the VLM to match characters between each panel image Ii and the reference image R, using persistent attributes such as clothing, hairstyle, and spatial position to ensure accurate identity mapping. It then prompts the model to describe detailed distinctive character attributes, identify mismatches, and suggest necessary fixes. To avoid overcorrection, it uses the original prompt Pi to distinguish intentional narrative changes. It further applies two-step self-verification protocol [19]: first, checking whether each fix is contextually appropriate, and second, validating that the difference is visually clear and not caused Figure 2. Our framework operates as collaborative multi-agent system with access to shared memory that maintains dynamic Consistency Index (CI), the current panel set, and the latest consistency report. First, Story Initialization Agent, which takes user-provided sequence of story prompts and character descriptions, and generates initial story panels using set of off-the-shelf story visualization methods, including both Flux and SD-based models. Once the initial panels are generated, the Audit Agent evaluates each panel using VLM, updates the CI, and produces detailed consistency report. This is followed by the Repair Phase, where the Repair Agent applies localized edits to inconsistent panels using editing tools such as Flux-ControlNet. The Consistency Director Agent oversees the entire process, iteratively triggering the Audit and Repair phases until the CI reaches predefined threshold or maximum number of refinement iterations is completed. by occlusion or ambiguity. The agent returns structured consistency report (see supplemental material for an example) and computes the global consistency score: Scons ="
        },
        {
            "title": "1\nN",
            "content": "N (cid:88) i=1 DINO(Ii, R), Scons [1, 1]. where DINO(Ii, R) denotes the cosine similarity between the DINO embeddings of frame Ii and reference image R. This is linearly rescaled to define the Consistency Index: CI = 100 Scons + 1 [0, 100]. While the Consistency Index is computed globally, edits are applied selectively. The consistency report includes frame-level suggestions, and only frames with validated, actionable fixes are passed to the Repair Agent. Before conversion into edits, each suggested fix is also validated for visibility, ensuring that no corrections are proposed for occluded or ambiguous content. The remaining validated corrections are converted into list of executable text edits (e.g., change the hair color of the girl in the dress to black.) and encoded as refined prompts = {P 1, . . . , }. Repair Agent (ARepair) The Repair Agent receives story frames, along with their corresponding refined prompts that specify the necessary corrections. It applies localized visual edits using Flux-ControlNet-Union [14, 38], an editing model that supports image-based conditioning and allows appearance-guided modification of specific visual features. For each frame, all validated edits are combined into single semantic prompt to ensure coordinated changes and avoid conflicting updates. After each edit, ARepair evaluates the outcome and adaptively adjusts the conditioning scale: if the changes are too subtle or not visible, the scale is decreased to encourage stronger edits in future iterations, this If the information is acquired from the shared memory. characters appearance deviates too much from the reference (indicating over-editing), the scale is increased to reinforce fidelity. Over-editing is flagged by the repair agent when Consistency Index similarity to the reference drops below threshold, risking unrecoverable character changes in the future iterations. If valid edit cannot be achieved after several attempts, the panel is skipped and revisited in the next audit. This adaptive strategy enables precise updates while preserving correct content and avoiding full image regeneration. Consistency Director (ACD) The Consistency Director oversees the full auditrepair process. After each repair cycle, it requests new audit from AAudit and evaluates the updated Consistency Index. If the CI remains below the threshold τ (default 90) and the iteration count is less than or equal to the maximum allowed Tmax (default 2), it initiates another auditrepair loop by calling AAudit ARepair. Otherwise, it terminates the process and outputs the final, visually consistent story. Together, these agents operate within shared-memory, closed-loop control system that casts visual consistency as an iterative auditrepair cycle. Guided by the Consistency Director, each detected mismatch is first translated into lightweight text-level edit and then resolved through disentangled, image-conditioned updates. This strategy (i) eliminates expensive full-story re-generation, (ii) honors any narrative changes explicitly requested by the user, and (iii) remains agnostic to the underlying diffusion backbone. We set τ = 90 to ensure high visual consistency, and limit the audit-repair loop to Tmax = 2 iterations to balance correction quality with computational efficiency. These values are selected empirically based on early validation experiments. 4. Experiments To evaluate the effectiveness of our framework, we conducted extensive quantitative and qualitative experiments across multiple metrics and performed detailed ablation studies. Our experiments demonstrate that the proposed framework can be applied to both rectified-flow models (e.g., Flux) and latent-diffusion models (e.g., Stable Diffusion), yielding improvements in visual consistency. Experimental Setup We use GPT-4 [1] to generate the story prompts, and also as VLM for auditing. Flux-ControlNetUnion [14, 38] is used for localized image repair. The conditioning scale is initially set to 0.37 by default. This scale is later adjusted by the ARepair to achieve localized edits. The agentic framework is implemented using the AutoGen library [33]. All experiments are conducted on single NVIDIA A40 GPU. Auditing takes approximately 30 seconds per frame, and editing takes 30 seconds when repair is triggered. Unless otherwise specified, we report mean and standard deviation over 100 generated stories (in total 700 frames per method). Please see Appendix for an example set of prompts. 4.1. Qualitative Experiments We demonstrate the versatility of our framework across multiple story visualization methods based on both Flux and Stable Diffusion. Specifically, we evaluate three representative models; DSD [3], StoryDiffusion [40], and ConsiStory [29]. All methods are provided with the same set of story prompts. Fig. 3 presents comparisons of the original and corrected story sequences produced by our framework. As shown in Fig. 3(a), ConsiStory omits key character details such as the red cape, while in Fig. 3(b), it removes the backpack and alters the T-shirt color in the final panel. In contrast, our method successfully restores these details. Similarly, StoryDiffusion exhibits inconsistent clothing colors, visible in the first panel of Fig.3(a) and in both the first and fourth panels of Fig.3(b). DSD also suffers from color discrepancies: the cape appears with the wrong hue in Fig.3(a), and the T-shirt color changes between panels in Fig.3(b). Across all cases, our agent-driven correction pipeline identifies and resolves these inconsistencies, yielding visually coherent and consistent story visualizations. Our framework also accommodates interactive, user-inthe-loop refinements. After the initial panels are generated, user can provide pinpoint feedback ranging from finegrained tweakssuch as changing dress from red to purpleto coarse, semantic replacements, like turning hamster into cat or morphing dragon into dinosaur (see Fig. 3). Our system parses these natural-language corrections, maps them to localized edit instructions, and re-invokes the diffusion editor only where necessary, leaving unaffected regions intact. Together, these examples underscore the versatility of our framework and its ability to let storytellers steer visualizations toward their exact creative intent. Next, we compare our method applied (using backbone story visualization model as DSD [3]) with state-of-the-art story visualization baselines, including StoryDiffusion [40], ConsiStory [29], StoryGen [17], AutoStudio [5], and DSD. Although these models produce plausible image sequences, they often struggle with maintaining character identity and consistency across story panels. As shown in Figure 4, StoryDiffusion frequently changes key character attributes: the girls facial features are inconsistent across panels, and the uncles clothing shifts between orange and blue overalls, with noticeable variation in the style and size of his goggles. ConsiStory also fails to preserve character identity, altering the girls appearance in nearly every panel and introducing Figure 3. Qualitative results for Audit & Repair. Our method identifies visual inconsistenciessuch as mismatched clothing, character identity or hairstyle changesacross story panels and refines them using agent-guided editing. The corrected stories preserve narrative coherence and visual consistency, improving character fidelity throughout the sequence."
        },
        {
            "title": "Method\nStoryDiffusion\nStoryGen\nConsiStory\nAutoStudio\nDSD\nOurs",
            "content": "CLIP-I 0.8250.05 0.8130.04 0.8490.04 0.7820.06 0.8450.05 0.8500.05 DINO 0.4330.10 0.5540.10 0.4920.10 0.3720.17 0.5490.13 0.5680.15 LPIPS 0.4940.04 0.6310.03 0.5130.02 0.4770.18 0.4970.05 0.4720.07 CLIP-T 0.3550.02 0.3140.02 0.3590.02 0.3380.03 0.3510.02 0.3510.02 HPS 0.2560.03 0.2350.03 0.2670.03 0.2540.03 0.2850.03 0.3190.03 TIFA 0.6610.08 0.4940.09 0.6900.09 0.6360.09 0.7060.08 0.7130. Table 1. Average CLIP-I, DINO, LPIPS, CLIP-T, HPS and TIFA scores computed on 100 stories with 7 panels per story, totaling to 700 images per method. unintended characters (e.g., the sixth frame). StoryGen produces visually unnatural and distorted outputs, with cartoonish exaggerations and incoherent compositions. AutoStudio suffers from blending artifacts (see the fifth panel) and generates incorrect characters, such as the fourth frame which shows two men instead of the described girl and her uncle. DSD presents inconsistencies in character attire: the uncle wears sleeveless red shirt in the third frame, while in the fourth, he appears in jacket rather than overalls. Our method corrects these issues observed in DSD, ensuring that clothing and character identity remain consistent throughout the sequence. In contrast to previous methods, our approach maintains visual consistency across all panels, preserving both the facial features and clothing of the main characters while adapting to pose and scene variations in way that supports coherent storytelling. See the Appendix for more qualitative examples and visual comparisons. 4.2. Quantitative Experiments We evaluate our method against five state-of-the-art methodsStoryDiffusion [40], StoryGen [17], ConsiStory [29], AutoStudio [5], and DSD [3] and ours (with DSD as the base story visualization backbone). We used the following metrics: CLIP-I [22], CLIP-T [22], DINO [4], LPIPS [39], Figure 4. Qualitative comparison of our method with state-of-the-art story visualization methods, including StoryDiffusion, StoryGen, ConsiStory, AutoStudio, and DSD. Our method outperforms existing approaches by preserving consistent visual elementssuch as character appearance, clothing, and identityacross all panels. In contrast, prior methods frequently exhibit inconsistencies and blending artifacts that compromise story coherence and the visual identity of characters. TIFA [11], and HPS [35]. CLIP-I and DINO are imageimage similarity metrics, while LPIPS is perceptual distance metric; all three are used to assess visual consistency across story panels. Specifically, we compute the average pairwise similarity between all frames in story. CLIP-T, TIFA and HPS measure image-text alignment by evaluating how well each frame corresponds to its associated prompt. As shown in Table 1, our method achieves the highest scores on CLIP-I, DINO, indicating improved global and characterlevel consistency. It also yields the lowest LPIPS score, reflecting improved perceptual similarity across story panels. Compared to DSD, our agentic editing framework yields measurable gains in visual consistency metrics, with an improvement of +%3.8 in DINO, and %5.0 reduction in LPIPS. Since computing these metrics on full images often overlooks fine-grained inconsistenciessuch as subtle changes in characters hair clip across frameswe also present masked version of the evaluation metrics. Specifically, we use the SAM model [13] to segment and isolate the characters or objects mentioned in the text prompts for each method (see supplementary material for examples of segmented outputs). We then compute the metrics, namely, CLIP-I-FG, DINO-FG, and LPIPS-FGon these foreground regions to better capture character-level consistency. As shown in Table 2, although methods like ConsiStory and StoryDiffusion report competitive scores on the standard CLIP-I metric, their performance drops on CLIP-I-FG. This indicates that their apparent consistency might be attributable to background similarity rather than stable character appearance. In contrast, our method achieves consistently higher Figure 5. a) Iterative Refinement. Our framework progressively improves consistency across iterationsfor instance, correcting dress color in Iteration 1 and hairstyle in Iteration 2, or adjusting rockets design to match the prompt. b) User-in-the-loop Correction. Users can interactively guide edits, enabling both fine-grained adjustments (e.g. dress color) and broader changes (e.g. replacing hamster with cat), demonstrating the systems flexibility and controllability. performance across all foreground-based metrics, reflecting its ability to preserve semantically meaningful and identityconsistent visual elements across story frames. These results confirm that our agentic framework improves visual coherence in content-aware manner. Although ConsiStory achieves higher CLIP-T score than our method, we observe that it often introduces additional characters not mentioned in the prompt (see Figure 4, sixth panel). This can inflate CLIP-T scores, as the metric measures global semantic alignment without enforcing finegrained grounding [11]. Consequently, these hallucinated additions may be rewarded by CLIP-T despite undermining both visual consistency and faithfulness to the intended narrative. To more rigorously assess semantic alignment, we compute the TIFA score [11] and HPS [35] score, metric designed to test fine-grained faithfulness between text and image. TIFA automatically generates set of questionanswer pairs from the input prompt using GPT-3.5 [2], and answers them using UnifiedQA-v2 [12], visual question answering model. As shown in Table 1, our method achieves the highest TIFA and HPS scores across the evaluated benchmarks, demonstrating that it not only improves visual consistency but also enhances faithfulness to the narrative prompts, particularly in compositional accuracy and detail-sensitive elements that CLIP-T may overlook. 4.3. Ablation Studies To evaluate the effectiveness of our framework across different story visualization backbones and show how consistency improves with iterative refinement, we conduct ablation studies as follows. Ablation on choice of backbone model: To demonstrate the generality of our editing framework, we apply it to the output of StoryDiffusion [40] and ConsiStory [29]. As shown in Table 3, our method consistently improves visual consistency across all metrics. Compared to the original StoryDiffusion output, applying our framework improves DINO by +4.6%, HPS by +14.5%, and reduces LPIPS by -1.8%. For ConsiStory, DINO increases by +4.5%, HPS improves by +12.7%, and LPIPS decreases by -2.7%. These results show that our framework is effective across different generation backbones. Qualitative examples in Figure 3 further illustrate corrections in character identity and object attributes. Ablation on iterative refinement steps: We evaluate the effect of applying our editing pipeline iteratively. second auditrepair pass on DSD yields additional improvements: DINO increases by +3.2%, and LPIPS decreases by -1.1%. This shows that remaining inconsistencies not resolved in the first pass can be corrected through additional iterations, demonstrating the robustness of our agentic framework. Figure 5 shows how visual consistency improves across iterations. For instance, Iteration 1 corrects the dress color, and Iteration 2 adjusts the hairstyle in Figure 5(a), top panel. On the Figure 5(a), bottom panel, Iteration 1 resolves visual artifacts and restores the presence of the boy character with cape, while Iteration 2 corrects the T-shirt color to match the reference. These examples highlight how our method refines both character consistency and alignment with prompt details. 4.4. User Study To assess human perception of story visualization quality, we conducted user study with 50 participants recruited via Prolific.com. Each participant was shown sequence of story panels (144 total per participant) accompanied by the corresponding narrative and asked to rate two aspects on 15 scale: (Q1) how well the visuals maintained consistency across panels, and (Q2) how well the visuals aligned with the narrative. As shown in Table 4, our method outperformed all baselines on both criteria. It received the highest ratings for"
        },
        {
            "title": "Method\nStoryDiffusion\nStoryGen\nConsiStory\nAutoStudio\nDSD\nOurs",
            "content": "CLIP-I-FG 0.8140.05 (-1.3%) 0.8100.05 (-0.4%) 0.8400.04 (-0.1%) 0.8110.05 (+3.7%) 0.8580.05 (+1.5%) 0.8600.05 (+1.2%) DINO-FG 0.4960.10 (+14.5%) 0.5880.13 (+6.1%) 0.5790.12 (+17.6%) 0.4570.20 (+22.8%) 0.6740.12 (+22.8%) 0.6820.14 (+20.0%) LPIPS-FG 0.4120.06 (-16.6%) 0.4700.07 (-25.5%) 0.3990.06 (-22.2%) 0.3850.16 (-19.2%) 0.3370.07 (-32.2%) 0.3260.08 (-30.9%) Table 2. Average CLIP-I, DINO, and LPIPS scores computed on segmented foreground regions, to further assess character-level similarity without background influence. Method StoryDiffusion StoryDiffusion + Ours ConsiStory ConsiStory + Ours DSD DSD + Ours (1st iteration) DSD + Ours (2nd iteration) DINO 0.4330.09 0.4530.09 0.4920.09 0.5140.09 0.5490.13 0.5680.15 0.5860. LPIPS 0.4940.04 0.4850.04 0.5130.02 0.4990.02 0.4970.05 0.472 0.07 0.4670.05 HPS 0.2560.05 0.2930.02 0.2670.03 0.3010.02 0.2850.03 0.3190.03 0.325 0.03 Table 3. Ablation results showing consistency improvements after applying our method to StoryDiffusion, ConsiStory and DSD across 100-stories, totaling to 700 panels per method. Table 4. User ratings ( better) on story alignment and visual consistency, on 15 scale. Our method outperforms others in terms of visual consistency and alignment with narrative."
        },
        {
            "title": "Method",
            "content": "User-Q1 User-Q"
        },
        {
            "title": "StoryGen\nAutoStudio\nConsiStory\nStoryDiffusion\nDSD\nOurs",
            "content": "1.5090.88 2.2101.12 2.8621.22 2.2791.19 3.0101.19 3.8581.01 1.4650.90 2.3021.16 2.8811.29 2.6121.24 3.3431.08 3.7561.05 narrative alignment and cross-frame consistency, confirming its ability to generate images that not only follow the story text faithfully but also maintain visual coherence. To ensure reliability of the collected ratings, inter-rater agreement metric, measured by Kappa [20], was calculated over user survey achieving scores (StoryGen: 0.7377, StoryDiffusion: 0.715, Consistory: 0.669, AutoStudio: 0.6704, DSD: 0.679, Ours: 0.707) over six methods. Cohen-Kappa score measures the rate of agreement between the users, with values between 0.6 to 0.8 indicating substantial agreement [20]. 5. Broader Impact and Limitations Our audit-and-repair framework substantially improves consistency across multi-panel stories, lowering the technical barrier for educators, journalists, and independent creators who lack advanced design skills, broadening access to visual storytelling tools. On the other hand, both the underlying VLM and diffusion backbone inherit model biases from their pre-training datae.g., tendency to favor mainstream art styleswhich can skew similarity estimates and disproportionately flag panels featuring under-represented demographics or unconventional aesthetics. On technical limitation, our consistency agent assumes that VLM scores faithfully capture cross-panel coherence. Hallucinations in these textual descriptions propagate directly to the Consistency Index and may trigger unnecessaryor misscorrections. Nevertheless, our frameworks audit-and-repair loop consistently improves multi-panel consistency of state-of-the-art methods, providing strong foundation for further research on consistent story visualization. 6. Conclusion In this paper, we presented Audit & Repair, an agentic framework that approaches story visualization as coordinated workflow among specialized agents and introduces the first inference-time, VLM-driven mechanism for detecting and repairing cross-panel inconsistencies. By operating entirely at post-generation, our consistency auditor is model-agnostic and can be plugged into any modern T2I backbone, markedly improving narrative coherence without re-training. Comprehensive experiments demonstrate that Audit & Repair reduces identity drift while preserving perceptual quality. Future directions include extending the agent hierarchy to temporal domains such as videos. Beyond storyboards and picture books, we believe Audit & Repairs modular design provides principled foundation for scalable, controllable visual storytelling in interactive media and generative design pipelines."
        },
        {
            "title": "References",
            "content": "[1] Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F.L., Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S., et al.: Gpt-4 technical report. arXiv preprint arXiv:2303.08774 (2023) 5 [2] Brown, T.B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D.M., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., Amodei, D.: Language models are few-shot learners (2020), https://arxiv.org/abs/2005. 14165 8 [3] Cai, S., Chan, E., Zhang, Y., Guibas, L., Wu, J., Wetzstein, G.: Diffusion self-distillation for zero-shot customized image generation (2024), https://arxiv. org/abs/2411.18616 1, 3, 5, 6, 13 [4] Caron, M., Touvron, H., Misra, I., Jégou, H., Mairal, J., Bojanowski, P., Joulin, A.: Emerging properties in self-supervised vision transformers. In: Proceedings of the International Conference on Computer Vision (ICCV) (2021) 6 [5] Cheng, J., Lu, X., Li, H., Zai, K.L., Yin, B., Cheng, Y., Yan, Y., Liang, X.: Autostudio: Crafting consistent subjects in multi-turn interactive image generation. arXiv preprint arXiv:2406.01388 (2024) 2, 5, 6, 13 [6] Ding, Y., Zhuang, S., Li, K., Yue, Z., Qiao, Y., Wang, Y.: Muses: 3d-controllable image generation via multimodal agent collaboration (2024), https://arxiv. org/abs/2408.10605 3 [7] Gong, Y., Pang, Y., Cun, X., Xia, M., He, Y., Chen, H., Wang, L., Zhang, Y., Wang, X., Shan, Y., et al.: Talecrafter: Interactive story visualization with multiple characters. arXiv preprint arXiv:2305.18247 (2023) 2 [8] Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., Bengio, Y.: Generative adversarial networks. Communications of the ACM 63(11), 139144 (2020) [9] Hu, E.J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., Chen, W.: Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685 (2021) 2 [10] Hu, P., Jiang, J., Chen, J., Han, M., Liao, S., Chang, X., Liang, X.: Storyagent: Customized storytelling video generation via multi-agent collaboration (2024), https://arxiv.org/abs/2411.04925 3 [11] Hu, Y., Liu, B., Kasai, J., Wang, Y., Ostendorf, M., Krishna, R., Smith, N.A.: Tifa: Accurate and interpretable text-to-image faithfulness evaluation with question answering. arXiv preprint arXiv:2303.11897 (2023) 7, 8 [12] Khashabi, D., Kordi, Y., Hajishirzi, H.: Unifiedqav2: Stronger generalization via broader cross-format training. arXiv preprint arXiv:2202.12359 (2022) 8 [13] Kirillov, A., Mintun, E., Ravi, N., Mao, H., Rolland, C., Gustafson, L., Xiao, T., Whitehead, S., Berg, A.C., Lo, W.Y., Dollár, P., Girshick, R.: Segment anything. arXiv:2304.02643 (2023) 7, 13 [14] Labs, B.F.: Flux. https://github.com/blackforest-labs/flux (2024) 2, 4, [15] Lee, D., Yoon, J., Cho, J., Bansal, M.: Videorepair: Improving text-to-video generation via misalignment evaluation and localized refinement (2025), https: //arxiv.org/abs/2411.15115 3 [16] Li, Y., Gan, Z., Shen, Y., Liu, J., Cheng, Y., Wu, Y., Carin, L., Carlson, D., Gao, J.: Storygan: sequential conditional gan for story visualization (2019), https: //arxiv.org/abs/1812.02784 2 [17] Liu, C., Wu, H., Zhong, Y., Zhang, X., Wang, Y., Xie, W.: Intelligent grimm - open-ended visual storytelling via latent diffusion models. In: The IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). pp. 61906200 (2024) 2, 5, 6, 13 [18] Liu, T., Wang, K., Li, S., van de Weijer, J., Khan, F.S., Yang, S., Wang, Y., Yang, J., Cheng, M.M.: Oneprompt-one-story: Free-lunch consistent text-to-image generation using single prompt (2025), https:// arxiv.org/abs/2501.13554 2 [19] Madaan, A., Tandon, N., Gupta, P., Hallinan, S., Gao, L., Wiegreffe, S., Alon, U., Dziri, N., Prabhumoye, S., Yang, Y., Gupta, S., Majumder, B.P., Hermann, K., Welleck, S., Yazdanbakhsh, A., Clark, P.: Selfrefine: Iterative refinement with self-feedback (2023), https://arxiv.org/abs/2303.17651 3 [20] McHugh, M.: Interrater reliability: The kappa statistic. Biochemia medica : ˇcasopis Hrvatskoga društva medicinskih biokemiˇcara / HDMB 22, 27682 (10 2012). https://doi.org/10.11613/BM.2012.031 9 [21] Pan, X., Qin, P., Li, Y., Xue, H., Chen, W.: Synthesizing coherent story with auto-regressive latent diffusion models. In: Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. pp. 29202930 (2024) 2 [22] Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., Krueger, G., Sutskever, I.: Learning transferable visual models from natural language supervision (2021), https://arxiv.org/abs/2103. 00020 [23] Rahman, T., Lee, H.Y., Ren, J., Tulyakov, S., Mahajan, S., Sigal, L.: Make-a-story: Visual memory conditioned consistent story generation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 24932502 (2023) 2 paradigm across text, image and audio (2025), https: //arxiv.org/abs/2503.05242 3 [37] Yang, S., Ge, Y., Li, Y., Chen, Y., Ge, Y., Shan, Y., Chen, Y.: Seed-story: Multimodal long story generation with large language model. arXiv preprint arXiv:2407.08683 (2024), https://arxiv.org/ abs/2407.08683 2 [38] Zhang, L., Agrawala, M.: Adding conditional control to text-to-image diffusion models (2023) 4, 5 [39] Zhang, R., Isola, P., Efros, A.A., Shechtman, E., Wang, O.: The unreasonable effectiveness of deep features as perceptual metric. In: CVPR (2018) [40] Zhou, Y., Zhou, D., Cheng, M.M., Feng, J., Hou, Q.: Storydiffusion: Consistent self-attention for longrange image and video generation. arXiv preprint arXiv:2405.01434 (2024) 2, 3, 5, 6, 8, 13 [24] Ramesh, A., Dhariwal, P., Nichol, A., Chu, C., Chen, M.: Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125 (2022) 2 [25] Rombach, R., Blattmann, A., Lorenz, D., Esser, P., Ommer, B.: High-resolution image synthesis with latent diffusion models (2021) 2 [26] Saharia, C., Chan, W., Saxena, S., Li, L., Whang, J., Denton, E., Ghasemipour, S.K.S., Ayan, B.K., Mahdavi, S.S., Lopes, R.G., Salimans, T., Ho, J., Fleet, D.J., Norouzi, M.: Photorealistic text-to-image diffusion models with deep language understanding (2022), https://arxiv.org/abs/2205.11487 2 [27] Shen, Y., Song, K., Tan, X., Li, D., Lu, W., Zhuang, Y.: Hugginggpt: Solving ai tasks with chatgpt and its friends in hugging face (2023), https://arxiv. org/abs/2303.17580 3 [28] Tao, M., Bao, B.K., Tang, H., Wang, Y., Xu, C.: Storyimager: unified and efficient framework for coherent story visualization and completion. arXiv preprint arXiv:2404.05979 (2024) 2 [29] Tewel, Y., Kaduri, O., Gal, R., Kasten, Y., Wolf, L., Chechik, G., Atzmon, Y.: Training-free consistent text-to-image generation (2024), https://arxiv. org/abs/2402.03286 2, 5, 6, 8, [30] Wang, W., Zhao, C., Chen, H., Chen, Z., Zheng, K., Shen, C.: Autostory: Generating diverse storytelling images with minimal human effort. arXiv preprint arXiv:2311.11243 (2023) 2 [31] Wang, Z., Li, A., Li, Z., Liu, X.: Genartist: Multimodal llm as an agent for unified image generation and editing (2024), https://arxiv.org/abs/ 2407.05600 3 [32] Wu, C., Yin, S., Qi, W., Wang, X., Tang, Z., Duan, N.: Visual chatgpt: Talking, drawing and editing with visual foundation models (2023), https://arxiv. org/abs/2303.04671 3 [33] Wu, Q., Bansal, G., Zhang, J., Wu, Y., Li, B., Zhu, E., Jiang, L., Zhang, X., Zhang, S., Liu, J., et al.: Autogen: Enabling next-gen llm applications via multi-agent conversation. arXiv preprint arXiv:2308.08155 (2023) 5 [34] Wu, T.H., Lian, L., Gonzalez, J.E., Li, B., Darrell, T.: Self-correcting llm-controlled diffusion models (2023), https://arxiv.org/abs/2311.16090 3 [35] Wu, X., Hao, Y., Sun, K., Chen, Y., Zhu, F., Zhao, R., Li, H.: Human preference score v2: solid benchmark for evaluating human preferences of text-to-image synthesis (2023), https://arxiv.org/abs/2306. 09341 7, 8 [36] Xu, X., Mei, J., Li, C., Wu, Y., Yan, M., Lai, S., Zhang, J., Wu, M.: Mm-storyagent: Immersive narrated storybook video generation with multi-agent"
        },
        {
            "title": "Table of Contents",
            "content": "A. Additional Qualitative Results and Comparison B. Details of User Study C. Examples for Segment-based Metrics D. Sample Prompts 13 13 13 A. Additional Qualitative Results and Comparison Fig 6 and Fig 7 provide additional qualitative comparisons between our method and several state-of-the-art baselines, including StoryDiffusion [40], ConsiStory [29], StoryGen [17], AutoStudio [5], and DSD [3]. Figure 6 illustrates consistency improvements when using our agentic framework with DSD as the base model, where key elements such as accessories and colors are corrected to remain consistent across panels. Our approach reliably preserves character identity, maintains consistent scene elements (e.g., the maze and hamster), and supports coherent storytelling. In contrast, the baseline methods often exhibit issues such as character drift, inconsistent visual styles, or disjointed narrative flow. Figure 7 presents further comparisons with other consistencyfocused methods. Across all examples, our method produces story sequences that are more semantically accurate and visually coherent, effectively preserving character appearance and relationships throughout the story. These results highlight the strength of our approach in generating consistent and narratively aligned multi-panel visualizations. B. Details of User Study Figure 8 shows an example from our user study survey, where participants were asked to evaluate the visual consistency and story alignment of generated story panels. In the study, we presented frames generated by six different methods, including ours, across five different story prompts. For each story, participants were shown sequence of six images and asked to rate two aspects on 15 Likert scale: (1) the consistency of characters across panels, and (2) how well the visuals matched the given story text. This structured evaluation allowed us to quantitatively assess both narrative alignment and visual coherence across models in humancentered manner. C. Examples for Segment-based Metrics To enable more reliable comparison, we compute segmented versions of CLIP and other metrics, as standard scores often overemphasize background features and overlook finer details such as changes in clothing or accessories. In Table 2 (main paper), we use the SAM model [13] to segment and isolate the characters or objects referenced in the text prompts for each method. Some examples of this process, where segmented characters are extracted and placed against white background, is illustrated in Figure 9. D. Sample Prompts Listings 1, 2, 3, 4, and 5 showcase sample prompts for various story panels used in the main paper and supplementary material. Figure 6. Inconsistency correction results on the DSD method, illustrated through qualitative examples produced by our agentic framework. Figure 7. More qualitative comparisons of our method against existing consistency methods. Figure 8. The user study questionnaire includes two evaluation questions: one measuring character consistency and the other assessing story alignment, each rated on scale from 1 to 5. Figure 9. Example of segmented foreground images. Listing 1. Generated story and prompts for \"A girl and her brave hamster must work together to find their way out of tricky maze\". { \"Main Characters\": [ { }, { } \"Name\": \"Emily\", \"Description\": \"A girl with pigtails wearing striped dress\", \"Category\": \"girl\" \"Name\": \"Whiskers\", \"Description\": \"Small, adventurous hamster\", \"Category\": \"hamster\" ], \"Story\": [ { }, { }, { }, { }, { }, { \"Image_Prompt\": \"Emily and Whiskers at maze entrance.\", \"Location_Description\": \"Lush green hedges form the complex pathways of the maze, with clear (cid:44) blue sky overhead and soft sunlight filtering through.\", \"Image_Prompt\": \"Emily and Whiskers looking at letters on the ground of the maze pathway.\", \"Location_Description\": \"Lush green hedges form the complex pathways of the maze, with clear (cid:44) blue sky overhead and soft sunlight filtering through.\" \"Image_Prompt\": \"Emily and Whiskers pushing through section of the hedge to reveal secret (cid:44) passage.\", \"Location_Description\": \"Lush green hedges form the complex pathways of the maze, with clear (cid:44) blue sky overhead and soft sunlight filtering through.\" \"Image_Prompt\": \"Emily and Whiskers looking at large ornate mirror in the maze, showing their (cid:44) reflection.\", \"Location_Description\": \"Lush green hedges form the complex pathways of the maze, with clear (cid:44) blue sky overhead and soft sunlight filtering through.\" \"Image_Prompt\": \"Close-up of Emily and Whiskers looking thoughtfully at the maze mirror, (cid:44) Whiskers on the maze.\", \"Location_Description\": \"Maze\" \"Image_Prompt\": \"Emily and Whiskers standing in the center of the maze, surrounded by flowers (cid:44) and sunlight.\", \"Location_Description\": \"The center of the maze decorated with blooming flowers and bathed in (cid:44) warm sunlight.\" } ] } Listing 2. Generated story and prompts for \"A girl and her grandpa retired carpenter build wooden boat that magically sails on clouds to faraway lands\". { \"Main Characters\": [ { }, { } \"Name\": \"Lily\", \"Description\": \"a girl with pigtails and striped dress\", \"Category\": \"girl\" \"Name\": \"Grandpa Joe\", \"Description\": \"an elderly man with beard and overalls\", \"Category\": \"man\" ], \"Story\": [ \"Image_Prompt\": \"Lily standing and Grandpa Joe showing her hammer in the attic.\", \"Location_Description\": \"dusty attic filled with boxes and cobweb\" \"Image_Prompt\": \"close-up of Grandpa Joe sharing story with Lily in the attic.\", \"Location_Description\": \"attic\" \"Image_Prompt\": \"Lily and Grandpa Joe looking at wooden planks in the workshop.\", \"Location_Description\": \"a sunny workshop with tools and wood shavings on the floor\" \"Image_Prompt\": \"Grandpa Joe guiding Lilys hands as she hammers nail into the boat.\", \"Location_Description\": \"workshop\" \"Image_Prompt\": \"Lily in the boat soaring above the clouds, with Grandpa Joe.\", \"Location_Description\": \"night sky sprinkled with stars above and the silhouette of the town (cid:44) below\" \"Image_Prompt\": \"Lily steering the boat towards castle, with Grandpa Joe pointing the way.\", \"Location_Description\": \"lake\" { }, { }, { }, { }, { }, { { } ] } Listing 3. Generated story and prompts for \"A girl with curly pigtails in dress and her quirky inventor uncle with beard and goggles invent machine\". { \"Main Characters\": [ { }, { } \"Name\": \"Lucy\", \"Description\": \"Girl with curly pigtails in dress\", \"Category\": \"girl\" \"Name\": \"Uncle Ned\", \"Description\": \"Man with beard and goggles wearing overalls\", \"Category\": \"man\" ], \"Story\": [ \"Image_Prompt\": \"Close-up of Lucy looking up with wide, dreamy eyes.\", \"Location_Description\": \"room\" \"Image_Prompt\": \"Uncle Ned walking into the room with greasy hands.\", \"Location_Description\": \"A cluttered inventors workshop full of peculiar gadgets and tools.\" \"Image_Prompt\": \"Lucy pointing excitedly at blueprint on the table.\", \"Location_Description\": \"A cluttered inventors workshop full of peculiar gadgets and tools.\" \"Image_Prompt\": \"Close-up of Uncle Neds face, eyes sparkling with excitement.\", \"Location_Description\": \"room\" \"Image_Prompt\": \"Uncle Ned assembling the frame of the machine while Lucy helps.\", \"Location_Description\": \"A cluttered inventors workshop full of peculiar gadgets and tools.\" \"Image_Prompt\": \"Lucy and Uncle Ned, with tired eyes, looking at the nearly completed machine.\", \"Location_Description\": \"A cluttered inventors workshop full of peculiar gadgets and tools.\" \"Image_Prompt\": \"The completed flying machine gleaming under the light of single bulb in the (cid:44) workshop.\", \"Location_Description\": \"A cluttered inventors workshop full of peculiar gadgets and tools.\" { }, { }, { }, { } { }, { }, { } ] } Listing 4. Generated story and prompts for \"A boy in red cape and wise old dragon set off to find crystal that can make wishes come true\". { \"Main Characters\": [ { }, { } \"Name\": \"Eli\", \"Description\": \"A boy with tousled hair and red cape\", \"Category\": \"boy\" \"Name\": \"Zephyr\", \"Description\": \"A wise old dragon with worn scales\", \"Category\": \"dragon\" ], \"Story\": [ \"Image_Prompt\": \"Eli and Zephyr standing on hilltop, looking outward.\", \"Location_Description\": \"Rolling hills under vibrant sunset sky.\" \"Image_Prompt\": \"Eli holding parchment map, Zephyr peering over.\", \"Location_Description\": \"Hilltop with scattered boulders and patches of grass.\", \"Image_Prompt\": \"Zephyr drinking from the river, Eli refilling water flask.\", \"Location_Description\": \"Forest clearing with clear river bordered by smooth stones.\" \"Image_Prompt\": \"Eli looking up in awe at the towering cliffs, Zephyrs tail in the foreground.\", \"Location_Description\": \"High, craggy cliffs against lightening sky.\" \"Image_Prompt\": \"Zephyr assisting Eli as he climbs steep rocky path.\", \"Location_Description\": \"Steep rocky pathways with view of surrounding lands.\", { }, { }, { }, { }, { } ] } Listing 5. Generated story and prompts for \"A boy and glowing jellyfish travel through mysterious underwater tunnels to hidden city\". { \"Main Characters\": [ { }, { } \"Name\": \"Leo\", \"Description\": \"Boy with wavy hair, in striped t-shirt and shorts\", \"Category\": \"boy\" \"Name\": \"Glimmer\", \"Description\": \"Luminous and amiable jellyfish\", \"Category\": \"jellyfish\" ], \"Story\": [ \"Image_Prompt\": \"Leo follows Glimmer into dark underwater tunnel entrance.\", \"Location_Description\": \"The beginning of dimly lit and expansive underwater tunnel with rocky (cid:44) walls.\" \"Image_Prompt\": \"Leo and Glimmer swimming with fish around them in colorful coral-lined (cid:44) tunnel.\", \"Location_Description\": \"Sunlit coral tunnel bustling with marine life and warm light filtering (cid:44) through.\" \"Image_Prompt\": \"Glowing door with mysterious markings, Leo examining it closely, Glimmer by his (cid:44) side.\", \"Location_Description\": \"An ancient and mystical section of the underwater tunnel, with engraved (cid:44) walls.\" \"Image_Prompt\": \"Glimmer touching the door, which opens to glowing city, Leo gazing in awe.\", \"Location_Description\": \"The magical city with brilliant structures and streets made of (cid:44) luminescent corals and stones.\" \"Image_Prompt\": \"Leo and Glimmer arriving at grand coral palace with jellyfish guards at the (cid:44) entrance.\", \"Location_Description\": \"The heart of the city with the grandest building crowned with glowing (cid:44) spires amidst the cityscape.\" { }, { }, { }, { }, { } ] }"
        }
    ],
    "affiliations": [
        "Virginia Tech"
    ]
}