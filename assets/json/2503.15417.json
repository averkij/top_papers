{
    "paper_title": "Temporal Regularization Makes Your Video Generator Stronger",
    "authors": [
        "Harold Haodong Chen",
        "Haojian Huang",
        "Xianfeng Wu",
        "Yexin Liu",
        "Yajing Bai",
        "Wen-Jie Shu",
        "Harry Yang",
        "Ser-Nam Lim"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Temporal quality is a critical aspect of video generation, as it ensures consistent motion and realistic dynamics across frames. However, achieving high temporal coherence and diversity remains challenging. In this work, we explore temporal augmentation in video generation for the first time, and introduce FluxFlow for initial investigation, a strategy designed to enhance temporal quality. Operating at the data level, FluxFlow applies controlled temporal perturbations without requiring architectural modifications. Extensive experiments on UCF-101 and VBench benchmarks demonstrate that FluxFlow significantly improves temporal coherence and diversity across various video generation models, including U-Net, DiT, and AR-based architectures, while preserving spatial fidelity. These findings highlight the potential of temporal augmentation as a simple yet effective approach to advancing video generation quality."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 1 ] . [ 1 7 1 4 5 1 . 3 0 5 2 : r a"
        },
        {
            "title": "Temporal Regularization Makes Your Video Generator Stronger",
            "content": "Harold Haodong Chen1,2 Haojian Huang1,4 Xianfeng Wu1,2 Yexin Liu1,2 Yajing Bai1,2 Wen-Jie Shu1,2 Harry Yang1,2 Ser-Nam Lim1,3 1Everlyn AI 2HKUST 3UCF 4HKU Project page:https://haroldchen19.github.io/FluxFlow/ Figure 1. FLUXFLOW improves the temporal quality of video generators. Captions: (Top) dog chasing butterfly in garden, with the butterfly flying in random directions. (Bottom) person is running along beach with waves crashing in the background."
        },
        {
            "title": "Abstract",
            "content": "Temporal quality is critical aspect of video generation, as it ensures consistent motion and realistic dynamics across frames. However, achieving high temporal coherence and diversity remains challenging. In this work, we explore temporal augmentation in video generation for the first time, and introduce FLUXFLOW for initial investigation, strategy designed to enhance temporal quality. Operating at the data level, FLUXFLOW applies controlled temporal perturbations without requiring architectural modifications. Extensive experiments on UCF-101 and VBench benchmarks demonstrate that FLUXFLOW significantly improves temporal coherence and diversity across various video generation models, including U-Net, DiT, and AR-based architectures, while preserving spatial fidelity. These findings highlight the potential of temporal augmentation as simple yet effective approach to advancing video generation quality. 1. Introduction The pursuit of photorealistic video generation faces critical dilemma: while spatial synthesis (e.g., SD-series [9, 28], 1 AR-based [22, 40]) has achieved remarkable fidelity, ensuring temporal quality remains an elusive target. Modern video generators, whether diffusion [4, 20, 21, 44, 48] or autoregressive [7, 15, 37], frequently produce sequences plagued by temporal artifacts, e.g., flickering textures, discontinuous motion trajectories, or repetitive dynamics, exposing their inability to model temporal relationships robustly (see Figure 1). These artifacts stem from fundamental limitation: despite leveraging large-scale datasets, current models often rely on simplified temporal patterns in the training data (e.g., fixed walking directions or repetitive frame transitions) rather than learning diverse and plausible temporal dynamics. This issue is further exacerbated by the lack of explicit temporal augmentation during training, leaving models prone to overfitting to spurious temporal correlations (e.g., frame #5 must follow #4) rather than generalizing across diverse motion scenarios. Unlike static images, videos inherently require models to reason about dynamic state transitions rather than isolated frames. While spatial augmentations (e.g., cropping, flipping, or color jittering) [45, 46] have proven effective for improving spatial fidelity in visual generation, they fail to address the temporal dimension, making them inadequate for video generation. As result, video generation models often exhibit two key issues (as shown in Figure 1): ❶ Temporal inconsistency: Flickering textures or abrupt transitions between frames, indicating poor temporal coherence (see Figure 4). ❷ Similar temporal patterns: Overreliance on simplified temporal correlations leads to limited temporal diversity, where generated videos struggle to distinguish between distinct dynamics, such as fast and slow motion, even with explicit prompts (see Figure 5). Addressing these challenges requires balancing spatial realismtextures, lighting, and object shapeswith temporal plausibilitycoherent and diverse transitions. While modern architectures leverage large-scale image priors for spatial realism [9, 28, 34], they struggle with complex temporal relationships, relying heavily on architectural modifications [1, 10, 14] or constraint engineering [5, 15, 38]. However, data-level augmentation, proven effective in video understanding [2, 18, 42, 49], remains underexplored, highlighting the untapped potential of temporal data augmentation for improving video generation. To make an initial exploration of this issue, in this paper, we propose FLUXFLOW, data augmentation strategy that injects controlled temporal perturbations into video generation training. Inspired by human cognitionwhere we infer missing frames or reorder eventsFLUXFLOW operates on simple principle: disrupting fixed temporal order to force the model to learn disentangled motion/optical flow dynamics. Specifically, FLUXFLOW introduces two levels of temporal perturbations for investigation: Figure 2. Comparison of VideoCrafter2 with FLUXFLOW using VBench metrics for Temporal Quality (Top) and Frame-wise and Overall Quality (Bottom). FLUXFLOW significantly enhances the temporal quality of generated videos while maintaining or even improving frame-wise and overall quality. Frame-Level: Randomly shuffle individual frames to disrupt fixed temporal order, encouraging the model to infer plausible temporal relationships. Block-Level: Reorder contiguous-frame blocks to simulate realistic temporal disruptions while preserving coarse motion patterns. By training on disordered sequences, the generator learns to recover plausible trajectories, effectively regularizing temporal entropy. FLUXFLOW bridges the gap between discriminative and generative temporal augmentation, offering plug-and-play enhancement solution for temporally plausible video generation while improving overall quality (see Figure 1 and 2). Unlike existing methods that introduce architectural changes or rely on post-processing, FLUXFLOW operates directly at the data level, introducing controlled temporal perturbations during training. To summarize, our contributions are as follows: We introduce FLUXFLOW, the first dedicated temporal augmentation strategy for video generation, which introduces controlled temporal perturbations without requiring architectural modifications. We identify and formalize the challenge of temporal brittleness in video generation, highlighting the lack of explicit temporal augmentation in existing methods and demonstrating the potential of temporal augmentation as simple yet viable solution. Extensive experiments across UCF-101 [33] and VBench [19] benchmarks on diverse video generators (U-Net [5], DiT [44], and AR-based [7]) demonstrate that FLUXFLOW enhances temporal coherence without compromising spatial fidelity. We hope this work inspires broader explorations of temporal augmentation strategies in video generation and beyond. 2 Figure 3. Overview of FLUXFLOW. (a) Standard video generation trains on fixed frame orders, which may limit the models ability to learn temporal dynamics. (b) FLUXFLOW introduces controlled temporal perturbations during training as plug-and-play augmentation strategy. (c) This study explores FLUXFLOW at two levels: frame-level (top) and block-level (bottom). In frame-level, Num 1 denotes the number of individual frames shuffled. In block-level, Num1 Num2 represents block comprising Num2 consecutive frames. 2. Related Work Video Generation. Advancements in video generation span T2V [5, 10, 14, 15, 20, 21, 36] and I2V [1, 20, 23, 41]. T2V generates videos aligned with textual descriptions, while I2V focuses on temporally coherent output conditioned on images. Beyond per-frame quality, ensuring temporal quality remains key challenge. Temporal Refinement for Video Generation. Modern approaches to temporal refinement can be categorized into three main paradigms: (i) Architecture-Centric Modeling: Spatiotemporal transformers [14, 21], hybrid 3D convolutions [1], and motion-decoupled architectures [10] improve long-range coherence but increase computational cost. (ii) Physics-Informed Regularization: Techniques like optical flow warping [38], surface normal prediction [5], and motion codebooks [15] ensure realistic motion through physical priors. (iii) Training Dynamics Optimization: Temporal contrastive loss [47], curriculum frame sampling [23], and dynamic FPS sampling [20] enhance robustness and consistency. While these methods have advanced architectural designs and constraint engineering, they often overlook the potential of systematic temporal augmentation within video data itself. Our work addresses this gap by introducing simple yet effective temporal augmentation strategies, paving the way for improved temporal quality in video generation. 3. Methodology 3.1. Preliminaries Modern video generation models fall into three main paradigms: U-Net-based, Diffusion Transformer (DiT)- based, and Autoregressive (AR)-based. This section provides an overview of (Latent) Diffusion Models for U-Net and DiT, and Next Token Prediction for AR-based methods. Diffusion Models (DMs) [12, 32] are probabilistic generative frameworks that gradually corrupt data x0 pdata(x) into Gaussian noise xT (0, I) via forward process, and subsequently learn to reverse this process through denoising. The forward process q(xtx0, t), defined over timesteps, progressively adds noise to the original data x0 to obtain xt, leveraging parameterization trick. Conversely, the reverse process pθ(xt1xt, t) denoises xt to recover xt1 using denoising network ϵθ (xt, t). The training objective is formulated as follows: min θ Et,xpdata,ϵN (0,I)ϵ ϵθ (xt; c, t) 2 2, (1) where ϵ represents the ground-truth noise, θ denotes the learnable parameters, and is an optional conditioning input. Once trained, the model generates data x0 by iteratively denoising random Gaussian noise xT . Latent Diffusion Models (LDMs) [13, 28] extend DMs by operating in compact latent space, significantly improving computational efficiency. Instead of performing the diffusion process in the pixel space, LDMs encode the input video RL3HW into latent representation = E(x) using an autoencoder E, where RLChw. The diffusion process zt = p(z0, t) and the denoising process zt = pθ(zt1, c, t) are then conducted in the latent space. The training objective is similar to DMs but applied to the latent representation: Et,xpdata,ϵN (0,I)ϵ ϵθ (E(xt); c, t) 2 2, (2) 3 Finally, the generated latent representation is decoded back into the pixel space using the decoder D, yielding the generated video ˆx = D(z). Next Token Prediction. AR video generation can be formulated as next-token prediction, similar to language modeling. video is converted into sequence of discrete video tokens = {t1, t2, ..., tn} by tokenizers. Similar to LLMs, the next video token is predicted using past video tokens as context. Specifically, the training objective is to minimize the following negative log-likelihood (NLL) loss: LN LL = (cid:88) log (tit1, t2, . . . , ti1; Θ), (3) where the conditional probability of the predicted next ti is modeled by transformer decoder with parameters Θ. 3.2. FLUXFLOW While spatial augmentations (e.g., flipping, cropping) are commonly employed to enhance spatial robustness, the temporal dimension remains under-regularized in video generation. To address this gap, we propose FLUXFLOW, data-level temporal augmentation strategy that perturbs the temporal structure of video sequences during training. In this initial exploration, FLUXFLOW operates in two modes: Frame-level and Block-level Perturbations, each targeting distinct temporal scales, as demonstrated in Figure 3. Frame-Level Perturbations. FLUXFLOW-FRAME introduces fine-grained disruptions by shuffling individual frames within sequence. As shown in Figure 3(c) (top), given video sequence = {F1, F2, . . . , FN }, we randomly shuffle subset of frames, controlled by the perturbation ratio α. Formally: Vframe = Shuffle({Fi S}) + {Fj / S}, (4) where is randomly selected subset of frames with = αN . Frames outside remain in their original positions, maintaining partial temporal consistency. This perturbation forces the model to reconstruct plausible temporal relationships, enhancing its ability to generalize beyond deterministic frame-to-frame dependencies. Block-Level Perturbations. FLUXFLOW-BLOCK operates at coarser scale by reordering contiguous blocks of frames, as illustrated in Figure 3(c) (bottom). The input sequence is divided into non-overlapping blocks of size k, such that: Vblock = {B1, B2, . . . , BM }, (5) where Bm = {F(m1)k+1, . . . , Fmk}. subset of these blocks is then randomly reordered with probability β, producing: perturbed block = Reorder({Bm B}) + {Bn / B}. (6) Block-level perturbations simulate realistic temporal disruptions, such as changes in motion speed or direction, Figure 4. Illustration of FLUXFLOW in enhancing temporal coherence. (Top) Example frames from CogVideoX, without and with FLUXFLOW, showcasing larger motion dynamics in the latter. (Bottom) Comparison of temporal angle differences across frames. FLUXFLOW achieves consistently lower angle differences, indicating improved temporal coherence over the base model. Caption: skateboarder performing tricks in skatepark, with fastpaced movements and dynamic camera angles. while preserving coarse motion patterns. Implementation. FLUXFLOW is implemented as preprocessing strategy applied during training. Each perturbation (frame-level or block-level) is independently applied to evaluate its impact on temporal quality. Figure 3(b) illustrates the combined training pipeline. concrete illustration of the algorithm is given in the pseudocode below. Algorithm 1 FLUXFLOW Pseudocode Require: Video = {F1, F2, ..., FN }, perturbation type mode {frame, block}, perturbation ratio α (for frame-level), block size and perturbation probability β (for block-level) Ensure: Perturbed sequence VFluxFlow Frame-Level Perturbations Select subset of frames with = αN Shuffle frames in to obtain VFluxFlow 1: if mode = frame: 2: 3: 4: else if mode = block: Divide 5: {B1, B2, . . . , BM } Block-Level Perturbations blocks N/k into = Select subset of blocks with = βM Reorder blocks in to obtain VFluxFlow 6: 7: 8: end if 9: Output: VFluxFlow 3.3. What does model learn with FLUXFLOW? To better understand the impact of FLUXFLOW on the models temporal learning capabilities, we evaluate its effect on temporal coherence and temporal diversity. For this purpose, we select three groups of text prompts with varying temporal dynamics: static, slow, and fast (details can be found in Appendix A). Our observations are as follows: 4 RQ2: Does FLUXFLOW facilitate the learning of motion/optical flow dynamics? RQ3: Can FLUXFLOW maintain temporal quality in extraterm generation? RQ4: How sensitive is FLUXFLOW to its key hyperparameters? 4.1. Experimental Settings (ii) AR-based: NOVA-0.6B [7]. Base Models. To comprehensively evaluate the effectiveness of FLUXFLOW, we apply it to three distinct video generation architectures: (i) U-Net-based: VideoCrafter2 (iii) DiT-based: [41]. CogVideoX-2B [44]. To ensure fair and consistent comparisons, we fine-tune base models using FLUXFLOW as an additional training stage with one epoch on OpenVidHD0.4M [27], following their default configurations (e.g., resolution, frame length). The results are compared with models trained under identical settings but without temporal augmentation (i.e., w/o FLUXFLOW). Notably, FLUXFLOW is model-agnostic and can be seamlessly integrated into the training pipeline of any video generation architecture. Evaluations. We evaluate FLUXFLOW on two widelyused benchmarks for video generation, focusing on both temporal coherence and overall video quality: UCF-101 [33]: large-scale human action dataset containing 13, 320 videos across 101 action classes. We utilize the following metrics: (i) Frechet Video Distance (FVD) [35] for temporal coherence and motion realism. (ii) Inception Score (IS) [29] for frame-level quality and diversity. VBench [19]: comprehensive benchmark designed to evaluate video generation quality across 16 dimensions. To specifically assess temporal and frame-level quality, we focus on the following key dimensions: (i) Temporal Quality: Subject Consistency, Background Consistency, Temporal Flickering, Motion Smoothness, and Dynamic Degree. (ii) Frame-Wise Quality: Aesthetic Quality and Imaging Quality. (iii) Overall Quality: Total Score, Quality Score, and Semantic Score. These benchmarks and metrics provide comprehensive evaluation, allowing us to rigorously assess the impact of FLUXFLOW on both temporal dynamics and spatial fidelity. 4.2. Quality and Fidelity Enhancement (RQ1) We present the quantitative comparison of FLUXFLOWFRAME and FLUXFLOW-BLOCK on VideoCrafter2 (VC2), NOVA, and CogVideoX (CVX) in Tab. 1 and 2 and qualitative comparison in Fig. 6. Each model is evaluated with three settings based on its default frame length. Specifically, FLUXFLOW-FRAME is shown on VC2, NOVA, and CVX Figure 5. Illustration of FLUXFLOW in improving temporal feature diversity. (a) Without FLUXFLOW, the model trained on fixed original frame sequences fails to distinguish features across different temporal paradigms. (b) With FLUXFLOW, features are more distinctly separated, reflecting enhanced temporal representation. Obs.❶ FLUXFLOW enhances temporal coherence. As shown in Figure 4, we analyze videos generated from one of the fast prompts. Videos generated without FLUXFLOW exhibit abrupt and unstable temporal changes, reflecting inconsistent motion dynamics. In contrast, the videos generated with FLUXFLOW demonstrate significantly larger and smoother motion dynamics. Quantitative analysis of angular differences further supports this observation. By comparing angular differences between consecutive frames, we observe that the base model produces high variance in these differences, reflecting erratic temporal transitions. In comparison, FLUXFLOW achieves consistently lower angular differences, indicating its ability to stabilize temporal changes while maintaining the intended motion dynamics. Obs.❷ FLUXFLOW improves temporal diversity. Figure 5 demonstrates the generated videos temporal feature representations. Without FLUXFLOW (Figure 5(a)), the features of videos generated from different temporal prompts (static, slow, and fast) are largely overlapped, indicating the model struggles to distinguish between distinct temporal paradigms. This lack of separation reflects the baseline models inability to capture diverse temporal dynamics. In contrast, with FLUXFLOW (Figure 5(b)), the temporal features are more distinctly separated across the three temporal paradigms, reflecting the models enhanced ability to represent diverse temporal patterns. These findings highlight the critical role of FLUXFLOW in improving the temporal capabilities of baseline models, allowing them to generate temporally consistent and diverse videos that align more closely with the intended motion dynamics of the input prompts. 4. Experiment In this section, we conduct extensive experiments to answer the following research questions (RQ): RQ1: Can FLUXFLOW improve temporal quality while maintaining spatial fidelity? Table 1. Evaluation of FLUXFLOW-FRAME. + Original refers to training without FLUXFLOW, while + Num 1 indicates the use of different FLUXFLOW-FRAME strategies. We shade the best results and underline the second-best results for each model. Method UCF-101 VBench FVD IS Subject Back. Flicker Motion Dynamic Aesthetic Imaging Quality Semantic Total VideoCrafter2 [5] + Original + 2 1 + 4 1 + 8 1 463. 36.57 96.85 98.22 98.41 97.73 42. 63.13 67.22 82.20 73.42 80.44 468.324.52 37.130.56 97.020.17 97.890.33 97.171.24 97.780.05 41.241.26 63.870.74 68.010.79 81.810.39 73.140.28 80.080.36 444.5919.21 37.891.32 98.821.97 99.281.06 99.641.23 98.630.90 49.587.08 63.550.42 67.940.72 84.482.28 73.890.47 82.361.92 451.4312.37 37.020.45 97.901.05 99.150.93 98.660.25 98.660.93 50.007.50 61.741.39 65.761.46 83.311.11 73.390.03 81.330.89 457.216.59 37.921.35 97.931.08 98.710.49 98.690.28 98.921.19 47.254.75 60.972.16 66.201.02 83.110.91 72.371.05 80.960. U-Net-based: 16F320512 AR-based: 33F480768 NOVA [7] + Original + 2 1 + 4 1 + 16 1 428.12 38.44 94. 94.81 96.38 96.34 54.35 54.52 66. 78.96 76.57 78.48 427.420.70 39.491.05 95.120.41 94.540.27 95.880.50 96.450.11 52.232.12 54.890.37 67.040.83 78.840.12 76.870.30 78.370.11 420.177.95 38.710.27 96.181.47 95.560.75 96.870.49 97.401.06 58.644.29 54.220.30 66.860.65 80.521.56 76.110.46 79.641.16 413.4514.67 39.310.87 96.762.05 96.241.43 97.451.07 97.210.87 57.883.53 54.960.44 66.500.29 80.911.95 76.840.27 80.101.62 423.095.03 39.240.89 95.240.53 94.570.24 97.120.74 97.521.18 56.542.20 54.180.34 65.690.52 79.971.01 75.281.29 79.030.55 DiT-based: 49F480720 CogVideoX [44] + Original + 2 1 + 8 1 + 24 347.59 44.32 96.78 96.63 98.89 97. 59.86 60.82 61.68 82.18 75.83 80. 349.341.75 45.911.59 96.820.04 95.341.29 98.830.06 97.310.42 60.160.30 58.522.30 62.250.57 81.430.76 75.960.13 80.340.57 343.234.36 44.120.20 97.320.54 97.150.52 99.140.25 98.200.47 61.261.40 60.740.08 61.960.28 82.880.70 75.980.15 81.500.59 329.4118.18 46.091.77 98.351.57 97.981.35 99.620.73 98.240.51 61.141.28 61.540.72 62.020.34 83.581.40 76.090.26 82.081.17 345.192.40 44.980.66 98.041.26 97.090.46 98.960.07 98.110.38 62.152.29 59.821.00 60.211.47 82.530.35 74.291.54 80.880.03 Table 2. Evaluation of FLUXFLOW-BLOCK. + Num1 Num2 indicates the use of different FLUXFLOW-BLOCK strategies."
        },
        {
            "title": "Method",
            "content": "UCF-"
        },
        {
            "title": "VBench",
            "content": "FVD IS Subject Back. Flicker Motion Dynamic Aesthetic Imaging Quality Semantic Total VideoCrafter2 [5] + Original + 2 2 + 2 4 + 4 4 NOVA [7] + Original + 2 2 + 4 4 + 4 8 + 1 CogVideoX [44] + Original + 2 2 + 4 8 + 4 12 + 1 U-Net-based: 16F320512 463.80 36. 96.85 98.22 98.41 97.73 42.50 63. 67.22 82.20 73.42 80.44 468.324.52 37.130.56 97.020.17 97.890.33 97.171.24 97.780.05 41.241.26 63.870.74 68.010.79 81.810.39 73.140.28 80.080.36 449.3014.50 37.761.19 98.321.47 98.720.50 99.270.86 98.630.90 48.856.35 63.840.71 68.170.95 84.141.94 73.450.03 82.001.56 457.3914.41 37.861.29 97.590.74 99.180.96 98.880.47 98.851.12 47.244.74 63.240.11 67.640.42 83.761.56 73.670.25 81.741.30 460.3114.67 36.410.16 97.230.38 98.450.23 98.920.51 98.630.90 47.905.40 62.860.27 66.900.32 83.321.12 72.081.34 81.080.64 AR-based: 33F480 428.12 38.44 94.71 94.81 96.38 96. 54.35 54.52 66.21 78.96 76.57 78. 427.420.70 39.491.05 95.120.41 94.540.27 95.880.50 96.450.11 52.232.12 54.890.37 67.040.83 78.840.12 76.870.30 78.370.11 423.194.93 39.120.68 96.241.53 95.480.67 96.890.51 97.050.71 56.532.18 54.240.28 66.540.33 80.131.17 76.220.35 79.350.87 417.9910.13 39.140.70 96.892.18 95.680.87 97.210.83 96.890.55 57.122.78 55.020.50 66.290.08 80.471.51 76.970.75 79.761.28 425.043.08 39.380.94 96.531.82 95.921.11 97.040.66 97.240.90 56.862.34 54.740.22 66.860.65 80.591.63 76.750.18 79.821.34 DiT-based: 49F480720 347.59 44.32 96.78 96. 98.89 97.73 59.86 60.82 61.68 82. 75.83 80.91 349.341.75 45.911.59 96.820.04 95.341.29 98.830.06 97.310.42 60.160.30 58.522.30 62.250.57 81.430.76 75.960.13 80.340.57 341.785.81 45.651.33 97.140.36 97.250.62 99.210.32 98.050.32 61.361.50 59.321.50 62.861.18 82.740.56 75.850.02 81.360.45 336.2711.32 45.931.61 98.382.05 97.811.18 99.040.15 98.460.73 61.851.99 61.020.20 62.180.50 83.421.24 76.360.53 82.011.10 345.981.61 45.190.87 97.470.69 97.040.41 99.180.29 98.680.95 61.191.33 59.641.18 61.980.30 82.980.80 75.920.09 81.570.66 with 2 1, 4 1, and 8 1 in the qualitative comparisons, respectively. We give the following observations: Obs.❸ FLUXFLOW improves temporal quality with preserved spatial fidelity. Both FLUXFLOW-FRAME and FLUXFLOW-BLOCK significantly improve temporal quality, as evidenced by the metrics in Tabs. 1, 2 (i.e., FVD, Subject, Flicker, Motion, and Dynamic) and qualitative results in Fig. 6. For instance, the motion of the drifting car in VC2, the cat chasing its tail in NOVA, and the surfer riding wave in CVX become noticeably more fluid with FLUXFLOW. Importantly, these temporal improvements are achieved without sacrificing spatial fidelity, as evidenced by the sharp details of water splashes, smoke trails, and wave textures, along with spatial and overall fidelity metrics. Obs.❹ Optimal temporal perturbation strength is model-specific. The ideal perturbation strength depends on the base models default frame length. For example, in Tab. 1, the 16-frame VC2 performs best with the 2 1 strategy, while the 49-frame CVX benefits most from 8 1. Excessive perturbation, however, may disrupt spatial consistency, highlighting the importance of selecting modelspecific perturbation during training. Obs.❺ Frame-level perturbations outperform blockLevel. While both frame-level and block-level perturbations improve temporal quality, frame-level generally delivers better results. This can be attributed to their finer granularity, which allows for more precise temporal adjustments. In contrast, block-level perturbations may introduce excessive noise due to stronger spatiotemporal correlations within blocks, limiting their effectiveness. As result, frame-level strategies yield smoother and more coherent motion transitions. 6 Figure 6. Qualitative results of FLUXFLOW on VideoCrafter2 [5] (Top), NOVA [7] (Middle), and CogVideoX [44] (Bottom). 4.3. User Study with Temporal Dynamics (RQ2) 4.4. Extra-term Temporal Quality (RQ3) To answer RQ2, we first refer to Fig. 4, which highlights FLUXFLOWs ability to capture smooth and coherent optical flow changes, particularly in complex motion scenarios, and Fig. 6, which demonstrates its superior motion realism and dynamics. Building on these findings, we further conduct user study  (Fig. 8)  on 20 video-pairs to evaluate subjective perceptions of motion quality across five dimensions: Motion Diversity, Motion Realism, Motion Smoothness, Temporal Coherence, and Optical Flow Consistency, using prompts of two types: Action Speed (Fast & Slow) and Motion Pattern (Linear & Nonlinear). We observe that: Obs.❻ FLUXFLOW significantly facilitates temporal dynamics learning. As shown in Fig. 8, FLUXFLOW effectively disentangles and learns motion dynamics, excelling in complex trajectories and rapid temporal variations. Specifically, (i) Motion Diversity: Broader and more varied motion trajectories, particularly in dynamic or nonlinear scenarios. (ii) Optical Flow Consistency: Smoother and more coherent transitions, reducing abrupt changes and artifacts. (iii) Motion Realism and Smoothness: More natural and fluid motion, especially in intricate and complex trajectories. (iv) Temporal Coherence: Stable frame-to-frame dynamics without compromising other dimensions. To answer RQ3 and evaluate whether FLUXFLOW can maintain temporal quality in extreme conditions, we specifically use the 16-frame VC2 to generate 128-frame videos, as shown in Fig. 9. This allows us to verify whether FLUXFLOW can overcome the cumulative error and temporal instability challenges commonly observed in longsequence generation. We give the following observations: Obs.❼ FLUXFLOW effectively preserves temporal quality under extreme conditions. As shown in Fig. 9, the qualitative comparison (top) demonstrates that FLUXFLOW maintains dynamic background consistency and generates smoother transitions, while the baseline (VC2) exhibits temporal artifacts, e.g., flickering and motion inconsistency. Quantitatively (bottom), the gray regions highlight score drops relative to the original 16-frame generation. FLUXFLOW significantly reduces these drops, achieving superior subject consistency, background consistency, temporal flickering, and motion smoothness scores, ensuring high temporal quality in extra-term scenarios. 4.5. Ablation & Sensitivity Analysis (RQ4) To better investigate the effectiveness of FLUXFLOW, we conduct two ablation studies to assess its sensitivity to shuffle interval constraints and perturbation degrees in Fig. 7: (i) 7 Figure 7. Ablation and sensitivity analysis on FLUXFLOW with VBench temporal metrics. (a, b) Impact of shuffle interval constraints on VC2 using 2 1 and 2 2 configurations. (c, d) Impact of perturbation degrees on 16-frame VC2 and 33-frame NOVA. Figure 8. User study results comparing CVX and w/ FLUXFLOW. (Top) Examples frames from non-linear motion pattern, where FLUXFLOW demonstrates superior handling of complex trajectories. Caption: fish swims in circular loops in clear blue pond. (Bottom) User ratings across temporal dynamics evaluation criteria. For more details please refer to Appendix A. and Inter-frame/block Interval, and (ii) Perturbation Degree. Inter-frame/block Interval Analysis. We analyze the impact of shuffle interval constraints on frame-level (FLUXFLOW-FRAME) (FLUXFLOWblock-level BLOCK). The shuffle interval defines the minimum distance between shuffled frames or blocks. For example, in 2 1 frame-level shuffle with an interval of 8 frames, any two shuffled frames must be separated by at least 8 frames. As demonstrated in Fig. 7(a,b), ablations on VC2 using 2 1 and 2 2 shuffle configurations reveal that removing interval constraints (0.0% interval ratio) achieves the best performance across all metrics. Larger constraints (e.g., 25% or 50%) lead to noticeable performance degradation. This suggests that allowing free shuffle without interval constraints enables the model to better leverage temporal information, supporting the hypothesis that excessive constraints reduce the diversity of temporal patterns learned by the model. Perturbation Degree Analysis. We further examine whether excessive perturbation would cause significant performance degradation. We performed frame-level ablation 8 Figure 9. Performance comparison under extra-term conditions. (Top) Example frames from 16-frame VC2 generating 128-frame, without and with FLUXFLOW, showcasing dynamic background consistency in the latter. Caption: dog running along beach, splashing water as it moves through the waves. (Bottom) Comparison of temporal quality metrics on VBench, where the gray regions indicate the performance drop under extra-term scenarios. on 16-frame VC2 and 33-frame NOVA, as illustrated in Fig. 7(c,d). The results indicate that performance begins to decline significantly when the perturbation degree exceeds half of the total frames. This observation aligns with Obs❹, which highlights that perturbing more than half of the frames disrupts the models ability to infer the correct temporal order due to insufficient contextual information. 5. Conclusion In this work, we propose FLUXFLOW, pioneering temporal data augmentation strategy aimed at enhancing temThis initial exploporal quality in video generation. ration introduces two simple yet effective ways: framelevel (FLUXFLOW-FRAME) and block-level (FLUXFLOWBLOCK). By addressing the limitations of existing methods that focus primarily on architectural designs and conditioninformed constraints, FLUXFLOW bridges critical gap in the field. Extensive experiments demonstrate that integrating FLUXFLOW significantly improves both temporal coherence and overall video fidelity. We believe FLUXFLOW sets promising foundation for future research in temporal augmentation strategies, paving the way for more robust and temporally consistent video generation."
        },
        {
            "title": "References",
            "content": "[1] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. 2, 3 [2] Haodong Chen, Haojian Huang, Junhao Dong, Mingzhe Zheng, and Dian Shao. Finecliper: Multi-modal fine-grained clip for dynamic facial expression recognition with adapters. In Proceedings of the 32nd ACM International Conference on Multimedia, pages 23012310, 2024. 2 [3] Haodong Chen, Yongle Huang, Haojian Huang, Xiangsheng Ge, and Dian Shao. Gaussianvton: 3d human virtual tryon via multi-stage gaussian splatting editing with image prompting. arXiv preprint arXiv:2405.07472, 2024. 11 [4] Haodong Chen, Lan Wang, Harry Yang, and Ser-Nam Lim. Omnicreator: Self-supervised unified generation with universal editing. arXiv preprint arXiv:2412.02114, 2024. 2 [5] Haoxin Chen, Yong Zhang, Xiaodong Cun, Menghan Xia, Xintao Wang, Chao Weng, and Ying Shan. Videocrafter2: Overcoming data limitations for high-quality video diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7310 7320, 2024. 2, 3, 6, 7, 13 [6] Jin Chen, Kaijing Ma, Haojian Huang, Jiayu Shen, Han Fang, Xianghao Zang, Chao Ban, Zhongjiang He, Hao Sun, and Yanmei Kang. Bovila: Bootstrapping video-language alignment via llm-based self-questioning and answering. arXiv preprint arXiv:2410.02768, 2024. 11 [7] Haoge Deng, Ting Pan, Haiwen Diao, Zhengxiong Luo, Yufeng Cui, Huchuan Lu, Shiguang Shan, Yonggang Qi, and Xinlong Wang. Autoregressive video generation without vector quantization. arXiv preprint arXiv:2412.14169, 2024. 2, 5, 6, 7, 13 [8] Haoyu Deng, Zijing Xu, Yule Duan, Xiao Wu, Wenjie Shu, and Liang-Jian Deng. Exploring the low-pass filtering behavior in image super-resolution. arXiv preprint arXiv:2405.07919, 2024. 11 [9] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first International Conference on Machine Learning, 2024. 1, [10] Yuwei Guo, Ceyuan Yang, Anyi Rao, Zhengyang Liang, Yaohui Wang, Yu Qiao, Maneesh Agrawala, Dahua Lin, and Bo Dai. Animatediff: Animate your personalized textto-image diffusion models without specific tuning. arXiv preprint arXiv:2307.04725, 2023. 2, 3 [11] Muyang He, Yexin Liu, Boya Wu, Jianhao Yuan, Yueze Wang, Tiejun Huang, and Bo Zhao. Efficient multimodal learning from data-centric perspective. arXiv preprint arXiv:2402.11530, 2024. 11 [12] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. 3 [13] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik Kingma, Ben Poole, Mohammad Norouzi, David Fleet, et al. Imagen video: High definition video generation with diffusion models. arXiv preprint arXiv:2210.02303, 2022. 3 [14] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David Fleet. Video diffusion models. Advances in Neural Information Processing Systems, 35:86338646, 2022. 2, 3 [15] Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, and Jie Tang. Cogvideo: Large-scale pretraining for text-to-video generation via transformers. arXiv preprint arXiv:2205.15868, 2022. 2, [16] Haojian Huang, Xiaozhennn Qiao, Zhuo Chen, Haodong Chen, Bingyu Li, Zhe Sun, Mulin Chen, and Xuelong Li. Crest: Cross-modal resonance through evidential deep learning for enhanced zero-shot learning. In Proceedings of the 32nd ACM International Conference on Multimedia, pages 51815190, 2024. 11 [17] Haojian Huang, Chuanyu Qin, Zhe Liu, Kaijing Ma, Jin Chen, Han Fang, Chao Ban, Hao Sun, and Zhongjiang He. Trusted unified feature-neighborhood dynamics for multiview classification. arXiv preprint arXiv:2409.00755, 2024. 11 [18] Yongle Huang, Haodong Chen, Zhenbang Xu, Zihan Jia, Haozhou Sun, and Dian Shao. Sefar: Semi-supervised fine-grained action recognition with temporal perturbation and learning stabilization. arXiv preprint arXiv:2501.01245, 2025. 2 [19] Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, Yaohui Wang, Xinyuan Chen, Limin Wang, Dahua Lin, Yu Qiao, and Ziwei Liu. VBench: Comprehensive benchmark suite for video generative models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024. 2, 5 [20] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. 2, 3 [21] Aonian Li, Bangwei Gong, Bo Yang, Boji Shan, Chang Liu, Cheng Zhu, Chunhao Zhang, Congchao Guo, Da Chen, Dong Li, et al. Minimax-01: Scaling foundation models with lightning attention. arXiv preprint arXiv:2501.08313, 2025. 2, [22] Tianhong Li, Yonglong Tian, He Li, Mingyang Deng, and Kaiming He. Autoregressive image generation without vector quantization. Advances in Neural Information Processing Systems, 37:5642456445, 2024. 2 [23] Bin Lin, Yunyang Ge, Xinhua Cheng, Zongjian Li, Bin Zhu, Shaodong Wang, Xianyi He, Yang Ye, Shenghai Yuan, Liuhan Chen, et al. Open-sora plan: Open-source large video generation model. arXiv preprint arXiv:2412.00131, 2024. 3 [24] Yexin Liu and Lin Wang. Mycloth: Towards intelligent and interactive online t-shirt customization based on users pref9 erence. In 2024 IEEE Conference on Artificial Intelligence (CAI), pages 955962. IEEE, 2024. 11 [25] Yexin Liu, Zhengyang Liang, Yueze Wang, Muyang He, Jian Li, and Bo Zhao. Seeing clearly, answering incorrectly: multimodal robustness benchmark for evaluating mllms on leading questions. arXiv preprint arXiv:2406.10638, 2024. [26] Kaijing Ma, Haojian Huang, Jin Chen, Haodong Chen, Pengliang Ji, Xianghao Zang, Han Fang, Chao Ban, Hao Sun, Mulin Chen, et al. Beyond uncertainty: Evidential deep learning for robust video temporal grounding. arXiv preprint arXiv:2408.16272, 2024. 11 [27] Kepan Nan, Rui Xie, Penghao Zhou, Tiehan Fan, Zhenheng Yang, Zhijie Chen, Xiang Li, Jian Yang, and Ying Tai. Openvid-1m: large-scale high-quality dataset for text-tovideo generation. arXiv preprint arXiv:2407.02371, 2024. 5 [28] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image In Proceedings of synthesis with latent diffusion models. the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. 1, 2, 3 [29] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. Advances in neural information processing systems, 29, 2016. 5 [30] Dian Shao, Yue Zhao, Bo Dai, and Dahua Lin. Finegym: hierarchical video dataset for fine-grained action underIn Proceedings of the IEEE/CVF conference on standing. computer vision and pattern recognition, pages 26162625, 2020. 11 [31] Wen-Jie Shu, Hong-Xia Dou, Rui Wen, Xiao Wu, and LiangJian Deng. Cmt: Cross modulation transformer with hybrid loss for pansharpening. IEEE Geoscience and Remote Sensing Letters, 21:15, 2024. [32] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using In International confernonequilibrium thermodynamics. ence on machine learning, pages 22562265. PMLR, 2015. 3 [33] Soomro. Ucf101: dataset of 101 human actions classes from videos in the wild. arXiv preprint arXiv:1212.0402, 2012. 2, 5 [34] Peize Sun, Yi Jiang, Shoufa Chen, Shilong Zhang, Bingyue Peng, Ping Luo, and Zehuan Yuan. Autoregressive model beats diffusion: Llama for scalable image generation. arXiv preprint arXiv:2406.06525, 2024. 2 [35] Thomas Unterthiner, Sjoerd Van Steenkiste, Karol Kurach, Raphael Marinier, Marcin Michalski, and Sylvain Gelly. Towards accurate generative models of video: new metric & challenges. arXiv preprint arXiv:1812.01717, 2018. 5 [36] Jiuniu Wang, Hangjie Yuan, Dayou Chen, Yingya Zhang, Xiang Wang, and Shiwei Zhang. Modelscope text-to-video technical report. arXiv preprint arXiv:2308.06571, 2023. 3 [37] Xinlong Wang, Xiaosong Zhang, Zhengxiong Luo, Quan Sun, Yufeng Cui, Jinsheng Wang, Fan Zhang, Yueze Wang, Zhen Li, Qiying Yu, et al. Emu3: Next-token prediction is all you need. arXiv preprint arXiv:2409.18869, 2024. 2 10 [38] Yaohui Wang, Xinyuan Chen, Xin Ma, Shangchen Zhou, Ziqi Huang, Yi Wang, Ceyuan Yang, Yinan He, Jiashuo Yu, Peiqing Yang, et al. Lavie: High-quality video generation with cascaded latent diffusion models. arXiv preprint arXiv:2309.15103, 2023. 2, [39] Xianzu Wu, Xianfeng Wu, Tianyu Luan, Yajing Bai, Zhongyuan Lai, and Junsong Yuan. Fsc: Few-point shape In Proceedings of the IEEE/CVF Conference completion. on Computer Vision and Pattern Recognition, pages 26077 26087, 2024. 11 [40] Xianfeng Wu, Yajing Bai, Haoze Zheng, Harold Haodong Chen, Yexin Liu, Zihao Wang, Xuran Ma, Wen-Jie Shu, Xianzu Wu, Harry Yang, et al. Lightgen: Efficient image generation through knowledge distillation and direct preference optimization. arXiv preprint arXiv:2503.08619, 2025. 2 [41] Jinbo Xing, Menghan Xia, Yong Zhang, Haoxin Chen, Xintao Wang, Tien-Tsin Wong, and Ying Shan. Dynamicrafter: Animating open-domain images with video diffusion priors. arXiv preprint arXiv:2310.12190, 2023. 3, 5 [42] Zhen Xing, Qi Dai, Han Hu, Jingjing Chen, Zuxuan Wu, Svformer: Semi-supervised video and Yu-Gang Jiang. In Proceedings of transformer for action recognition. the IEEE/CVF conference on computer vision and pattern recognition, pages 1881618826, 2023. 2 [43] Yibo Yan, Haomin Wen, Siru Zhong, Wei Chen, Haodong Chen, Qingsong Wen, Roger Zimmermann, and Yuxuan Liang. Urbanclip: Learning text-enhanced urban region profiling with contrastive language-image pretraining from the In Proceedings of the ACM Web Conference 2024, web. pages 40064017, 2024. 11 [44] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. 2, 5, 6, 7, 13 [45] Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and Youngjoon Yoo. Cutmix: Regularization strategy to train strong classifiers with localizable features. In Proceedings of the IEEE/CVF international conference on computer vision, pages 60236032, 2019. 2 [46] Hongyi Zhang. mixup: Beyond empirical risk minimization. arXiv preprint arXiv:1710.09412, 2017. 2 [47] Rui Zhao, Yuchao Gu, Jay Zhangjie Wu, David Junhao Zhang, Jiawei Liu, Weijia Wu, Jussi Keppo, and Mike Zheng Shou. Motiondirector: Motion customization of text-tovideo diffusion models. arXiv preprint arXiv:2310.08465, 2023. 3 [48] Zangwei Zheng, Xiangyu Peng, Tianji Yang, Chenhui Shen, Shenggui Li, Hongxin Liu, Yukun Zhou, Tianyi Li, and Yang You. Open-sora: Democratizing efficient video production for all. arXiv preprint arXiv:2412.20404, 2024. 2 [49] Yuliang Zou, Jinwoo Choi, Qitong Wang, and Jia-Bin Huang. invariances for dataefficient action recognition. Computer Vision and Image Understanding, 227:103597, 2023."
        },
        {
            "title": "Supplementary Material",
            "content": "A. Detailed Analysis Settings A.2. User Study with Temporal Dynamics This section details the prompt information for the analysis in the main text. A.1. Temporal Diversity Analysis As shown in Figure 5 in Section 3.3, we analyze temporal diversity within generated videos by evaluating three groups of text prompts with distinct temporal dynamics: Static, Slow, and Fast. These prompts are designed to capture varying levels of motion and temporal changes across the generated videos. Below, we provide the complete prompt details for each group: In Section 4.3, we conduct user studies to evaluate the perceived quality of temporal dynamics in the generated videos. The study involves two key aspects: Action Speed (Fast & Slow) and Motion Pattern (Linear & Nonlinear). Below, we provide the full prompt details used for each category: Ten participants were asked to evaluate the generated videos from Motion Diversity, Motion Realism, Motion Smoothness, Temporal Coherence, and Optical Flow Consistency, scoring from 0 to 5 for each dimension. The results of this study provide valuable insights into the effectiveness of FLUXFLOW in capturing different temporal dynamics. Examples are shown in Figure 10. B. Limitations Deep learning [3, 6, 8, 11, 16, 17, 2426, 30, 31, 39, 43] has revolutionized video generation by enabling models These prompts ensure comprehensive assessment of the models ability to generate videos with diverse temporal characteristics. 11 Figure 10. User study examples. Each video is provided with its optical flow to assess the Optical Flow Consistency. Caption: skier carves smooth curves as they descend snowy slope. to learn complex spatiotemporal patterns from large-scale data. While our work introduces FLUXFLOW as pioneering exploration of temporal data augmentation in video generation, it is limited to two strategies: frame-level shuffle and block-level shuffle. These methods, while effective, represent only an initial step in this direction. Future work could explore more advanced temporal augmentation techniques, such as motion-aware or context-sensitive strategies, to further enhance temporal coherence and diversity. We hope this study inspires broader research into temporal augmentations, paving the way for more robust and expressive video generation models. C. More Experimental Results We provide more comparison results here in Figure 11. 12 Figure 11. More comparison of FLUXFLOW on VideoCrafter2 [5], NOVA [7], and CogVideoX [44]."
        }
    ],
    "affiliations": [
        "Everlyn AI",
        "HKU",
        "HKUST",
        "UCF"
    ]
}