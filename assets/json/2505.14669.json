{
    "paper_title": "Quartet: Native FP4 Training Can Be Optimal for Large Language Models",
    "authors": [
        "Roberto L. Castro",
        "Andrei Panferov",
        "Soroush Tabesh",
        "Oliver Sieberling",
        "Jiale Chen",
        "Mahdi Nikdan",
        "Saleh Ashkboos",
        "Dan Alistarh"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The rapid advancement of large language models (LLMs) has been paralleled by unprecedented increases in computational demands, with training costs for state-of-the-art models doubling every few months. Training models directly in low-precision arithmetic offers a solution, by improving both computational throughput and energy efficiency. Specifically, NVIDIA's recent Blackwell architecture facilitates extremely low-precision operations, specifically FP4 variants, promising substantial efficiency gains. Yet, current algorithms for training LLMs in FP4 precision face significant accuracy degradation and often rely on mixed-precision fallbacks. In this paper, we systematically investigate hardware-supported FP4 training and introduce Quartet, a new approach enabling accurate, end-to-end FP4 training with all the major computations (in e.g. linear layers) being performed in low precision. Through extensive evaluations on Llama-type models, we reveal a new low-precision scaling law that quantifies performance trade-offs across varying bit-widths and allows us to identify a \"near-optimal\" low-precision training technique in terms of accuracy-vs-computation, called Quartet. We implement Quartet using optimized CUDA kernels tailored for NVIDIA Blackwell GPUs, and show that it can achieve state-of-the-art accuracy for FP4 precision, successfully training billion-scale models. Our method demonstrates that fully FP4-based training is a competitive alternative to standard-precision and FP8 training. Our code is available at https://github.com/IST-DASLab/Quartet."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 2 ] . [ 1 9 6 6 4 1 . 5 0 5 2 : r Quartet: Native FP4 Training Can Be Optimal for Large Language Models Roberto L. Castro ISTA Andrei Panferov ISTA"
        },
        {
            "title": "Soroush Tabesh\nISTA",
            "content": "Oliver Sieberling ETH Zürich"
        },
        {
            "title": "Mahdi Nikdan\nISTA",
            "content": "Saleh Ashkboos ETH Zürich Dan Alistarh ISTA & Red Hat AI"
        },
        {
            "title": "Abstract",
            "content": "The rapid advancement of large language models (LLMs) has been paralleled by unprecedented increases in computational demands, with training costs for state-of-the-art models doubling every few months. Training models directly in low-precision arithmetic offers solution, by improving both computational throughput and energy efficiency. Specifically, NVIDIAs recent Blackwell architecture facilitates extremely low-precision operations, specifically FP4 variants, promising substantial efficiency gains. Yet, current algorithms for training LLMs in FP4 precision face significant accuracy degradation and often rely on mixedprecision fallbacks. In this paper, we systematically investigate hardware-supported FP4 training and introduce Quartet, new approach enabling accurate, end-to-end FP4 training with all the major computations (in e.g. linear layers) being performed in low precision. Through extensive evaluations on Llama-type models, we reveal new low-precision scaling law that quantifies performance trade-offs across varying bit-widths and allows us to identify near-optimal low-precision training technique in terms of accuracy-vs-computation, called Quartet. We implement Quartet using optimized CUDA kernels tailored for NVIDIA Blackwell GPUs, and show that it can achieve state-of-the-art accuracy for FP4 precision, successfully training billion-scale models. Our method demonstrates that fully FP4-based training is competitive alternative to standard-precision and FP8 training."
        },
        {
            "title": "Introduction",
            "content": "Over the past decade, the capabilities of large language models (LLMs) have surged, unlocking state-of-the-art performance in AI reasoning, coding, and multimodal understanding. These advances have come at the cost of an unprecedented rise in compute costs, as the floating-point operations (FLOPs) required to train frontier model have been doubling every few months [13]. One key lever for reducing compute costs is lower-precision computation: executing the matrixmultiplication (MatMul) kernels that dominate training workloads at lower bit-widths yields near-linear gains in throughput and energy efficiency. On the inference side, it is known that 4-bit quantizationor even lowercan preserve accuracy, via sophisticated calibration and rotation schemes [20; 2; 9]. For training, recent work has pushed the precision frontier from FP16 [30] to 8-bit pipelines, responsible in part for efficiency breakthroughs such as DeepSeek-V3 [28]. In this context, NVIDIAs Blackwell architecture introduces efficient hardware support for even lower-precision microscaling formats [32] such as MXFP and NVFP, which natively support 4-bit floating-point operations at higher teraFLOP-per-watt efficiency: for instance, moving from 8to 4-bit multiplies on the B200 GPU can almost double arithmetic throughput, while cutting energy roughly in half [31]. Yet, todays algorithmic support for accurate end-to-end training in such low precision is missing. State-of-the-art quantized training methods such as Switchback [48], Jetfire [51], HALO [3], and - Equal contribution. Correspondence to: dan.alistarh@ist.ac.at. Preprint. Under review. INT4-Transformers [50] either (i) lose precision and stability when training current models in 4-bit formats, or (2) fall back to higher precision for selected matrix multiplications. Bridging this gap calls for both deeper understanding of quantization error during back-propagation and new algorithmic safeguards tailored to hardware-native FP4 formats. Contributions. In this paper, we address this challenge via first systematic study of hardware-supported FP4 training, focusing on the high-efficiency of the MXFP4 format [32; 31]. Based on this analysis, we introduce an algorithm for MXFP4 native trainingin which all matrix multiplications occur in MXFP4called Quartet, which provides the best accuracy-efficiency tradeoff among existing methods, and is near-lossless for LLM pre-training in the large-data regime. Our main technical contribution is highly-efficient GPU implementation of Quartet, which achieves speedups of almost 2x relative to FP8 for linear layer computations on an NVIDIA Blackwell RTX 5090 GPU, relative to well-optimized FP8 baseline. One key achievement is that Quartet enables MXFP4 precision to be optimal on the accuracy-efficiency trade-off: at fixed computational budget, the accuracy impact of lower-precision training in Quartet is fully compensated by the higher efficiency of our implementation. In more detail, our contributions are as follows: 1. We propose and implement new approach for comparing quantized training methods, via their induced scaling law, which dictates the loss achievable under specific computation and data budget. We propose and fit such law for all existing methods, isolating two key parameters: the parameter efficiency effN of each method, and its data efficiency effD. method is superior to another if it improves upon both these metrics. 2. We find that the parameter efficiency is directly linked to the forward compression error of each training method, whereas data efficiency is linked to the bias in the methods gradient estimator, which we measure via novel misalignment metric. Given computational and data budget, and real-world speedups due lower precision, these metrics allow us to predict the optimal low-precision setup to train given model to target accuracy, maximizing accuracy-vs-runtime. 3. We apply this framework to MXFP4 precision, seeking to determine if there are practical settings under which native training in this precision can be optimal on Blackwell GPUs. We isolate an algorithm, called Quartet, which achieves this by maximizing both parameter and data efficiency, building on previous SOTA methods for QAT [33] and quantized backward-pass optimization [41]. Our key technical contribution is complex, highly-efficient GPU implementation of Quartet specialized to the new Blackwell architecture. 4. We validate our approach experimentally by pre-training Llama-family [40] models on the C4 dataset [34]. Our experiments show that 1) Quartet provides superior accuracy relative to prior methods [50; 52; 3] across different computing budgets and model sizes, and that 2) its fast implementation allows it to outperform highly-optimized FP8 kernels. This establishes that MXFP4 can indeed provide optimal training in practice. Our work bridges the gap between emerging low-precision hardware capabilities and the algorithmic support needed for accurate, end-to-end quantized model training. Specifically, we show for the first time that the new MXFP4 format can be competitive with FP8 in terms of accuracy-vs-speed, which we hope can enable significant reductions in the rising computational costs of AI."
        },
        {
            "title": "2 Related Work",
            "content": "Training in 8-bit formats. Early work on low-precision neural network training focused on 8-bit or higher precisions, mainly on CNNs. Banner et al. [4] demonstrated accurate 8-bit training via careful scaling and higher-precision accumulation. Yang et al. [53] proposed framework that quantized weights, activations, gradients, errors, and even optimizer states to INT, achieving for the first time completely integer-only training with comparable accuracy. SwitchBack [49] and JetFire [52] build on this progress, targeting 8-bit training for Transformers [44]. Specifically, SwitchBack uses hybrid INT8/BF16 linear layer for vision-language models, performing forward and input-gradient MatMuls in INT8 while computing weight gradients in 16-bit; this yielded 1325% end-to-end speedups on CLIP models with accuracy within 0.1% of full precision. JetFire [52] achieved fully INT8 training for Transformers by using novel per-block quantization scheme to handle activation and gradient outliers. By partitioning matrices into small blocks and scaling each block independently, JetFire preserved accuracy comparable to FP16 training while obtaining 40% end-to-end speedup and 1.49 reduction in memory usage. The JetFire approach 2 is conceptually similar to the FP8 DeepSeek training technique [28], which used larger block sizes. Recently, HALO [3] improved upon JetFire in terms of the accuracy-speedup trade-off in INT8, specifically focusing on low-precision fine-tuning. In our work, we will treat FP8 as the lossless baseline for the purposes of comparison. End-to-end lower-precision training. As our results and prior work suggest, going below 8-bit precision in training using the above approaches is extremely challenging, due to narrower dynamic range and higher error. This frontier was first explored by Sun et al. [36], who achieved 4-bit training on ResNets by using custom numeric format, which unfortunately is far from being supported in hardware. Chmiel et al. [10] introduced logarithmic unbiased quantization (LUQ) scheme to this end, combining two prior ideas: (1) log-scale FP4-type format to cover wider dynamic range, and (2) applying stochastic unbiased rounding on the backward. For reference, LUQ incurs 1.1% top-1 accuracy drop on ResNet50/ImageNet, and has not been validated on hardware-supported FP formats. Xi et al. [50] proposed method to train Transformers using INT4 effective precision for all linear layers, using specialized quantizers: block-wise Hadamard transform and LSQ [17] for outlier mitigation on the forward pass, and leverage score sampling on the backward pass to exploit structured sparsity, together with custom INT4-effective format. Their approach trains BERT-family models within 1-2% accuracy gap relative to FP16, with 2.2x speedup on individual matrix multiplies (relative to 4x theoretical speedup), leading to up to 35% faster training end-to-end. We compare relative to these techniques in Section 5, and show that Quartet outperforms them significantly in terms of accuracy and stability. Mixed-precision training in low-precision formats. Given the importance of inference cost reductions, there has been significant work on quantization-aware training (QAT) [12; 7; 17; 5; 46; 26], i.e. methods that only quantize the forward pass. Two key difficulties in this setting are 1) minimizing the error induced by quantization on the forward pass, and 2) obtaining stable gradient estimator over the resulting discrete space. With regards to error reduction, existing methods either try to find good learnable fit w.r.t. the underlying continuous distribution [12; 17], or perform noise injection during QAT in order to make the network more robust to quantization [5]. Closer to our work, Wang et al. [47] explored FP4 QAT, introducing smoother gradient estimator, together with outlier clamping and compensation to handle activation outliers. While their approach shows good accuracy, it is fairly complex and not validated in terms of efficient support. Concurrent work by [33] provided simpler alternative approach, based on more precise MSE fitting, an optional Hadamard rotation, and clipping-aware trust gradient estimator. By contrast with these forward-only approaches, recent work by Tseng et al. [41] investigated backward-only quantization with the MXFP4 format, signaling the importance of stochastic rounding and outlier mitigation in low-precision backpropagation."
        },
        {
            "title": "3 Background",
            "content": "Quantization grids. Quantization maps high-precision internal model states, such as weights, activations, or gradients, to lower-precision discrete set, i.e. the quantization grid. This grid can be uniform, e.g., for integer quantization, or non-uniform, e.g., floating-point (FP) quantization, where the value spacing is roughly exponential for fixed exponent. Since the original values may differ in scale compared to the grid, higher-precision scale is typically stored alongside the quantized values. For vector x, the quantization process can be written as q(x) = round (cid:0) ; grid(cid:1), and the original values can be approximately reconstructed as ˆx = q(x). Common choices for the scale are setting it to the maximum absolute value (absmax) in (to avoid clipping) or optimizing it to minimize the mean squared quantization error, e.g. [33]. Quantization granularity. Apart from grid choice, quantization methods also differ in the granularity of the scales. single scale value can be shared across an entire tensor, e.g. [3], across each row or column [33], or over more fine-grained custom-defined blocks, such as 2D blocks [51; 28] or 1D blocks [32; 41]. Notably, the latest Blackwell GPU architecture [31] introduces hardware support for MXFP4/6/8 and NVFP4 formats. MXFP [32] formats share an FP8 power-of-two scale over each 1D block of 32 elements, while NVFP4 [31] uses FP8 (E4M3) scales and 1D blocks of 16 elements. Rounding. Quantization typically involves rounding, e.g. via deterministic rounding to the nearest grid point, results in the lowest mean squared error (MSE). In contrast, stochastic rounding introduces randomness, rounding up or down with probabilities based on the inputs distance to nearby grid points. While it may introduce higher MSE, stochastic rounding helps control bias, which can be crucial for maintaining the convergence of iterative optimization algorithms [1]. 3 Outlier mitigation. One key issue when quantizing neural networks is the existence of large outlier values in the network weights, activations, and gradients [15]. One standard way of mitigating such outliers [37; 9; 3; 2; 41] is via the Hadamard transform: given vector Rd, h(x) is defined as h(x) = Hdx, where Hd Rdd is the normalized Hadamard matrix with elements from {1}. Hadamard matrices have recursive structure Hd = 1 H2 Hd/2, which enables 2 efficient computation when is power of two [18]. Optimized FWHT implementations for GPUs are available [14; 38]. When is not power of two, the input vector is typically either zero-padded to the next power of two or transformed using Grouped Hadamard Transform, where is split into equal-sized blocks (each with power-of-two length), and the Hadamard transform is applied independently to each block. Blackwell Architecture Support. NVIDIAs 5th-gen. Tensor Cores in Blackwell [31] provide native 4-bit floating-point execution. The cores support different block-scaled formats such as MXFP4 [32] and NVFP4, which roughly double the peak throughput over FP8/FP6, with single B200 GPU peaking at 18 PFLOPS of dense FP4 compute [31]. Interestingly, our investigation shows that, as of now, MXFP4 is the only microscaling format with support for all required layouts for both forward and backward multiplications in low precision on Blackwell [39]. Therefore, we adopt MXFP4 for our implementation. This format stores each value using 1 sign bit + 1 mantissa bit + 2-bits for exponent. Every group of 32 elements shares common 8-bit scaling factor, represented with 8 exponent bits, and no bits for mantissa. Blackwells 5th-gen. Tensor Cores handle the required on-the-fly rescaling in hardware, without the need for software-based rescaling at CUDA level. Additional details are provided in Section 4.4. LLM pre-training. We pre-train Transformers [45] of the Llama-2 [40] architecture in the range of 30, 50, 100, 200 million non-embedding parameters across wide range of data-to-parameter ratios raging from 25x (around compute-optimal [24]) to 800x (extreme data saturation). We additionally selectively scale the model size up to around 7 billion parameters to verify training stability. We train all models on the train split of the C4 [16] dataset and report C4 validation loss as the main metric. We use the AdamW optimizer [29] weight decay of 0.1, gradient clipping of 1.0, 10% LR warmup and cosine schedule. We identify the optimal LR for one of the small unquantized baseline models, scale it inverse-proportionally to the number of non-embedding parameters and reuse for every quantization scheme we evaluate. We present all hyper-parameters in Appendix."
        },
        {
            "title": "4 Quartet: Four Ingredients for “Optimal” Quantized Training",
            "content": "4.1 Ingredient 1: Comparing Quantized Training Approaches via their Induced Scaling Laws The ability of LLMs to scale predictably with both model size and data across orders of magnitude is cornerstone of the current AI scaling landscape [25]. Mathematically, this says that the expected loss is function of model and data parameters, often described in the form of parametric function. This function can be fitted on set of training runs, and then used to determine the optimal computational training regime [24] or to extrapolate model performance [23]. In this paper, we investigate scaling laws relating evaluation loss to the precision in which the forward and backward passes are performed, denoted by Pforward and Pbackward , respectively. For this, we propose scaling law of the following functional form: L(N, D, Pforward , Pbackward ) = (cid:18) (N effN (Pforward ))α + (D effD(Pbackward ))β (cid:19)γ + E, (1) where A, B, α, β, γ are constants describing the general loss scaling w.r.t. model parameter count and training corpus size D. The key addition is given by the fitted parameters effN (Pforward ), representing the parameter efficiency of the precision Pforward used in the forward pass, and effD(Pbackward ) representing the data efficiency of the backward pass occurring in potentially different precision Pbackward . (Both these factors are naturally in the interval (0, 1], where the value 1 is reached for full-precision.) Specifically, our parametrization postulates that the impact of the forward-pass precision is felt primarily w.r.t. the trainable parameters, i.e. that, by lowering precision to Pforward , we are lowering the models effective parameter count to effN (Pforward ) . This follows the general trend of modeling the effect of forward pass quantization as multiplicative factor on parameter 4 Figure 1: Analysis of Quartet: (a) Scaling-law 1 fit for various FORWARD:BACKWARD precisions. (b) Regions where each forward precision is optimal when the backward pass is FP8. (c) Same, but with an FP4 backward pass. Observe that the FP4 backward enlarges the regime in which FP4 forward is optimal. Interestingly, popular models such as Llama3 or Qwen2.5 fall into the FP4 optimality region, implying that training similar models in FP4 might have been optimal. count [21; 27; 22; 33]. For the data term, we postulate that lowering backward-pass precision primarily impacts the data term D, so we effectively need additional data to reach the same the same loss, precisely by factor of 1/effD(Pbackward ). This is novel way to model backward pass quantization that we propose, consistent with optimization theory results [1], as well as observed performance gaps [8]. We present experimental data to justify these assumptions and compare against alternative scaling laws [27] in the Appendix. Experimentally, we observe that different quantized training methods, e.g. STE [6] vs. QuEST [33], induce different scaling laws, and in particular different efficiency parameters. While, usually, scaling laws are used to extrapolate model performance across different parameter and data sizes, we propose to use scaling laws to compare different training methods. Specifically, we say that quantized training method is superior to method if it offers both higher parameter efficiency effN and higher data efficiency effD. 4.2 Ingredient 2: Mixed-Precision Induces Inference-Training Trade-Offs The above scaling law suggests that, given set of scaling parameters and target loss we wish the model to achieve, we can directly solve for the optimal forward and backward precisions which allow us to match the loss. However, as pointed out by Sardana et al. [35], it is often the case in practice that we wish to put larger weight on inference cost, rather than training cost, which can lead to different results when determining the optimal training precisions. Because inference latency depends solely on the forward pass ( 33% of training compute) while the backward pass consumes the remaining 66%, these trade-offs may need to be analyzed separately. Specifically, we can state set of simple guiding principles: Forward pass. Low-precision induces trade-off between reduced parameter efficiency, and increased inference speed: for instance, we could train larger model in terms of parameters , but quantize its forward pass to lower precision, and obtain better trade-off. As such, Pforward should be picked to optimize this trade-off. Backward pass. Similarly, training speedup due to quantized backward pass can offset the reduced data efficiency effD: we could train more heavily-quantized model on more data under the same computing budget. Thus, Pbackward should be picked to optimize this trade-off. We contrast this with previous work, which often requires lower precision to suffer no accuracy loss (e.g., Chmiel et al. [11]). This unnecessarily reduces these trade-offs to simple selection of the fastest lossless precision. We argue that scaling-law analysis enables more fine-grained approach needed to decide upon the optimal set of forward and backward precisions. Operation FP4:FP8 FP8:FP4 FP4:FP4 Forward / Inference (spfw) Backward (spbw) Training (sptr) 2.0 1.0 1.2 1.0 2.0 1. 2.0 2.0 2.0 Table 1: Speedups relative to an FP8 baseline for forward (spfw), backward (spbw); sptr is the harmonic mean of spfw and spbw with weights 1/3 (forward) and 2/3 (backward)."
        },
        {
            "title": "Rounding",
            "content": "Stochastic Rounding AbsMax Round-to-nearest AbsMax QuEST (Hadamard + RMSE)"
        },
        {
            "title": "RTN AbsMax PMA",
            "content": "effN 0.44 0.61 0.65 0.61 MSE 2.84 102 1.40 102 1.35 102 1.42 102 eff Misalignment ( 1 [1/S]) 0.85 0.83 0.18 0.83 0 9.3 103 1.3 102 2.8 105 Table 2: Illustration of error-bias trade-off between different quantized forward and backward pass approaches. For the forward (given by the effN metric) the best performing method is QuEST, correlating with superior MSE over Gaussian input data. By contrast, for the backward pass (the data efficiency eff*D computed at 800 Tokens/Parameter), the best performing method is stochastic rounding, correlated with perfect magnitude alignment. This justifies our choice of method, which combines block-wise QuEST on the forward, with Stochastic Rounding on the backward pass. Example speedup model. To illustrate this, we assume hardware-agnostic bit-wise ops (BOPS) model, which states that speedup is inversely proportional to datatype bit-width. The speedups are stated in Table 1, relative to an FP8 baseline: Then, given forward-pass compute budget Nmax and training budget NmaxDmax, the effective loss will be given by: Loss (cid:0)Nmax spfw, Dmax sptr / spfw, Pfwd, Pbwd (cid:1), which we evaluate with the scaling law from Equation (1), leading to the fit from Figure1(a). One can see how spfw and sptr propagate as multiplicative factors on effN and effD and directly counter the suboptimal parameter and data efficiencies. Figures 1(b)(c) illustrate the optimality regions: specifically, it tells us for which model sizes (Y axis) and corresponding data-to-model ratios (X axis) FP4 is optimal relative to FP8 (red vs orange region). The green thatched area is the range in which training using our MXFP4 implementation would be optimal by this metric. (This is derived using our actual obtained speedups.) In summary, Ingredient 2 says that low-precision impact should be analysed under the compute budget; scaling-law fits then reveal when given precision is the optimal choice for either pass. 4.3 Ingredient 3: Minimal Forward-Pass Error with Unbiased Gradient Estimation The above ingredients should allow us to determine the best quantized training method among existing approaches, focusing on the hardware-supported MXFP4 [32] format. Forward Pass Quantization. As detailed in Section 2, existing QAT (forward-only) approaches can be split into noise injection [5] and error-minimization approaches, e.g. [33]. Focusing on the forward pass, by the above discussion (Ingredients 1 and 2), we seek the approach which maximizes the parameter efficiency factor effN . For this, we implement four standard schemes for QAT: 1) stochastic rounding (SR) with standard AbsMax per-group normalization [41]; 2) vanilla round-tonearest (RTN) quantization with AbsMax per-group normalization; 3) learnable scale clipping (LSQ) with RTN quantization [17; 50]; 4) Hadamard normalization followed by RMSE-based clipping (QuEST) [33]. For fairness, we apply the Hadamard transform to weights and activations for each one of these schemes before quantization. We compare these approaches following Section 4.1: we train models using each technique, apply scaling law fitting, and register their resulting effN factors. For additional information, we also show representations mean-squared error (MSE) for fitting random Gaussian data. The results are provided in the first rows/columns of Table 2. The results in Table 2 show that QuEST has the best parameter efficiency effN among all existing methods. Moreover, effN appears to correlate heavily with MSE, as suggested by [33]. Additionally, the results align with the analysis of Chmiel et al. [11] that determined deterministic RTN to always be preferable to stochastic rounding for the forward pass. Backward pass: novel error-bias trade-off. The above findings do not transfer to backward pass quantization, as optimization theory shows that unbiased gradient estimation is critical for convergence, e.g. [1]. This leads to trade-off between the error minimization we can obtain on the forward pass, and the bias induced over the backward pass for given method. We study this trade-off via novel analysis of gradient alignment between different quantization methods. To study gradient bias, we follow the analysis of [42; 43], who studied RTN quantization with randomized rotations, approximated by the randomized Hadamard transform, which we denote by (cid:98)H. 6 Figure 2: The effect of backward pass quantization on LLM training gradient quality and impact on performance: (a, left) and (b, middle) shows cosine similarity and projection magnitude alignement with unquantized reference, while (c, right) shows performance gaps with non-quantized baseline for set model sizes and data-to-parameter ratios (D/N). They show that, while RHT makes quantization unbiased in direction, it adds bias in magnitude. To address this, they proposed an approach that makes RTN projections of post-RHT vectors unbiased, denoted by Q, via the following input (X) and randomness (ξ) specific group-wise rescaling factor S: Eξ[Q(X, ξ)] = if Q(X, ξ) = RTN( (cid:98)H(X, ξ)), where := X, (cid:98)H(X, ξ), RTN( (cid:98)H(X, ξ)) . Unfortunately, their re-scaling is incompatible with coarse group-wise scaling of the MXFP4 format, so we cannot use it in practice. However, we can still use their approach to gauge the degree of misalignment for different quantizers by simply studying their corresponding expected value of [1/S], which we call the projection magnitude alignment (PMA). This factor is presented in Table 2, along with the MSE across different schemes. Focusing on stochastic rounding (SR) vs round-to-nearest (RTN) with AbsMax, one can see that SR trades high error for perfect alignment. To connect those quantities with training dynamics, we analyze the cumulative effect of misalignment and error on backward quantization for 30M-parameters Llama model. In Figure 2 (a) and (c), we plot the alignment metricsCosine Similarity and PMAfor inter-layer activation gradients as function of back-propagation depth. We can again observe the trade-off between similarity and magnitude alignment. Finally, Figure 2 (c) connects those quantities to final model quality (loss gap vs. full-precision model) for increasing data-vs-parameters. Interestingly, we observe that cosine similarity (and MSE by extension) has high impact on initial convergence and shorter training runs, while projection magnitude alignment has greater impact on longer runs. Concretely, while RTN backward quantization may be preferable for shorter training, stochastic rounding (SR) performs consistently better for models more saturated with data. In this setup, the inflection point is around the D/N = 400 data-to-parameter ratio. Additionally, we include novel quantization scheme, called RTN AbsMax PMA, that is pseudounbiased: it tries to approximate on average, with constant value of [S] over range of input magnitudes. In Table 2, one can see that it has practically perfect alignment on average, at the cost of only slightly increased MSE over RTN AbsMax. However, this scheme is not truly unbiased because of the and Q(X) correlations. As such, we observe in Figure 2 (c) that it suffers significantly degraded performance at larger D/N , similar to RTN AbsMax. Summary. Our analysis outlines new trade-off between parameter efficiency on the forward (equated with quantization MSE), and data-efficiency on the backward (which we equate with the new misalignment metric). In the following, we will adopt best of both worlds approach, aiming to perform forward pass that minimizes MSE (based on QuEST [33]) together with backward pass that is unbiased (based on Stochastic Rounding [41]). The novel challenge, which we address next, will be an extremely efficient GPU-aware implementation of such an approach. Ingredient 4: Fast GPU Support for Accurate Quantized Training"
        },
        {
            "title": "4.4\nQuartet Overview. We integrate our prior discussion into Algorithm 1, which aims to perform\naccurate training while executing all three matrix multiplications of a linear layer in low precision.\nThe forward pass (lines 2–5) applies a Hadamard transform and QuEST projection to b-bit integers,",
            "content": "7 Algorithm 1 Quartet MXFP4 Forward-Backward Algorithm Require: Activations RBN , weights RM , random seed ξ Require: Hadamard Transform (H, (cid:98)H) block 1: function FORWARD(input X, weights ) Xh Hg(X); Wh Hg(W ) 2: (Xq, αx) QuEST(Xh) 3: (Wq, αw) QuEST(Wh) 4: Yq GEMMLP(Xq, ) 5: (αxαw) RESCALE(Yq) 6: return y, {Xq, Wq, αx, αw} 7: 8: end function 4 Gh); Wq SR( 3 1: function BACKWARD(Output gradient dy, Context ctx) 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: end function Unpack {Xq, Wq, αx, αw} from ctx Gh (cid:98)Hg(dy, ξ); Wh (cid:98)Hg(Wq, ξ) Gq SR( 3 4 Wh) dxq GEMMLP(Gq, Wq) dx H1 GT SR( 3 dWq GEMMLP(GT q, Xq) (cid:0) 16 dW H1 return dx, dW (cid:98)Hg(dy, ξ); (cid:98)Hg(Xq, ξ) 4 ) 4 GT ); Xq SR( 3 9 dWq αw 9 dxq αx (cid:0) 16 (cid:1) (cid:1) multiplies them with an INT/FP4 kernel, and rescales. (We omit the QuEST masking for readability.) The backward pass (lines 917) decorrelates activations and gradients with block-wise random Hadamard transform (of block size equal to the quantization group size), applies unbiased stochastic rounding (SR) to MXFP4, performs the two gradient GEMMs in MXFP4, rescales to compensate for SR bias, and inverts the RHT. Costs and Format Specialization. The key added cost of the above pipeline is that of the Hadamard rotations and their inversion: specifically, two Hadamard/Inverse transforms are added over standard training. Our key observation is that, since the MXFP4 already groups 32 consecutive weights (in 1D), sharing scales, we can and should apply the Hadamard rotations and their inversion at the same group size. With fast Hadamard implementation, the theoretical cost is O(g log g)negligible for 256 compared with the GEMMs. GPU Kernel Support. While the above blueprint appears simple, implementing it efficiently on Blackwell GPUsin order to leverage fast MXFP4 supportis extremely challenging. For illustration, direct implementation of the above pattern would be slower than FP16 unquantized training, let alone optimized FP8. Our fast implementation builds on CUTLASS 3.9 [39], which provides templates for the new Blackwell architecture. Computation happens in two stages: Stage 1 fuses the Hadamard transform, quantization, scale calculation, and QuEST clipping mask generation (only on forward) into single kernel; Stage 2 performs GEMM using dedicated kernel. Stage 1: Fused Quantization-Related Operations. First, we observe that, thanks to the small group size, the Hadamard transform can be implemented as direct GEMM between the corresponding input matrix and fixed 32 32 Hadamard matrix (see Sec. 3), producing output in FP32, which is stored in GPU Shared Memory (SMEM). This allows us to implement the Hadamard operation efficiently by leveraging CUTLASSs multilevel tiling templates to optimize data movement. All subsequent operations are integrated via custom CUTLASS epilogue, which utilizes the intermediate results previously stored in higher levels of the memory hierarchy and operates locally in the Register File (RF). At this stage, Blackwells new hardware support is used to downcast FP32 values to FP4 (E2M1) using the PTX instructions for this purpose. To construct the final MXFP4 format, we compute scaling factors of shape 1 32. These scales are represented in 8-bit using the E8M0 format. Finally, the clipping mask is computed, and the three resulting tensors (values, scales, and mask) are written to Global Memory (GMEM). Throughout, data storage is optimized to use the widest memory instructions possible. Stage 2: Dedicated GEMM Kernel. Blackwell introduces the tcgen05.mma instructions, which natively support matrix multiplication with scale factors in the form = +(ASFA)(B SFB). These scale factors are applied along the inner (K) dimension of the GEMM. For MXFP types, every 32 elements along the K-dimension of matrices and share corresponding scale factor. This implies that an matrix is associated with scale matrix SFA of size K/32. Our dedicated kernel is based on CUTLASS block-scaled GEMM for narrow precision. As part of this implementation, we also included the necessary functions to reorganize the scale factors generated in the Stage 1, aligning them with the layout required by this architecture [31]. To our knowledge, our implementation is the first to efficiently support quantization-related operations on the Blackwell architecture. We plan to release it open-source as separate library. 8 Figure 3: (a, left), (b, middle): Quartet kernels block-wise speedup across model sizes relative to FP8 and BF16. (c, right): Training dynamics for the 7B model trained with Quartet relative to FP8 ."
        },
        {
            "title": "5 Experiments",
            "content": "We now provide additional experimental support for the validity of Quartet, focusing on accuracy comparisons with existing INT4/FP4 training methods, and examining kernel speedups. Experimental Setup and Scaling Law Fit. As described in Section 3, we pre-train Llama-style models on C4 and report validation loss after fixed token budget. All baselines reuse the optimiser, schedule, and hyper-parameters, as described in Appendix A.1. Following Section 4.1, we compare accuracy across methods by fitting the full scaling law in Eqn. 1 across methods, as follows: we fit parameters A, α, B, β, and γ on grid of baseline precision runs (FP8 forward, FP8 backward) shown on Figure 1(a). Then we fit the parameter and data efficiencies effN and effD separately for every forward and backward quantization scheme we evaluate. The law is fitted identically to prior work in this area [24; 27; 8]. Please see detailed description in the Appendix. Accuracy Comparisons. We compare accuracy (validation loss) as well as the efficiency factors against four recent, fullyquantized training pipelines that operate in 4-bit precision for both forward and backward passes: 1) LUQ [11] applies to both INT4 and FP4, using unbiased quantization that pairs 4-bit weights/activations with stochastic underflow, and logarithmic stochastic rounding; 2) HALO [3], which uses Hadamard rotations to mitigate outliers, evaluated in FP4 at their most accurate HALO-2 setting; 3) Jetfire [52] performs quantization in blocks of 32 32, originally introduced for INT8, and adapted to FP4 for our setup; 4) LSS [50] for INT4 training, that combines Hadamard-based forward pass with leveragescore sampled INT4 gradients. 25 50 effD Method 0.50 0.01 0.01 0.15 0.09 0.07 3.399 4.799 6.581 5.381 NaN 3.205 3.432 4.842 6.621 6.501 NaN 3.244 3.729 4.806 7.033 6.649 NaN 3. 3.658 4.880 6.759 6.551 NaN 3.299 3.684 4.906 6.941 7.040 3.398 3.382 100 200 400 effN token-to-parameter LUQINT4 LUQFP4 JetfireFP4 HALOFP4 LSSINT4 Quartet (ours) Accuracy Discussion. Across all ratios, Quartet attains the lowest loss, often by very large margins. At 100 toks/param., Quartet improves upon LUQINT4 by 10% relative loss, and the gap widens as we increase data size. We note that Jetfire and HALO incur large degradation and are unstable when ported to FP4. Interestingly, LSS is competitive only for shorter runs, and diverges for longer training budgets, beyond 50, matching observations from prior work [19]. Overall, LUQINT4 is the strongest prior work; however, Quartet reaches significantly higher parameter and data efficiency, suggesting that it requires, roughly, 15% fewer parameters and 5x less data to reach the same loss. Figure 3 (c) additionally demonstrates the stability of Quartet for training models two orders of magnitude larger (7B parameters). Table 3: Validation loss (lower is better) on C4 for Llama models with 30M parameters and efficiency coefficients fitted on them. Columns show the tokens-to-parameters ratio (D/N ). All methods share identical setups; only the quantization scheme varies. NaNs for LLS-INT4 appeared at arbitrary stages of training without any irregularities. Unstable Unstable 0.94 0.64 Speedup Results. Next, we evaluate the efficiency of our implementation on the NVIDIA RTX 5090 GPU by measuring its performance across single layers of standard shapes, and aggregating across an entire Transformer block. Speedup results are shown in Figure 3, using batch size 64 and sequence length of 512. The FP8 baseline is provided by CUTLASS MXFP8 kernels, while the BF16 baseline uses PyTorch, both using Blackwell-optimized kernels. Inference speedups are more pronounced due to the lower cost of the forward pass compared to the backward pass, and the latters higher computational complexity. The speedup scales with the arithmetic intensity (i.e., model size), reaching up to 2.4 over FP8 and 4 over BF16 on the forward pass, where it stabilizes. In the backward pass, our implementation achieves up to 1.6 over FP8 and 2.3 over BF16, resulting in an overall training speedup of up to around 1.8, and 2.6, respectively."
        },
        {
            "title": "6 Discussion and Limitations",
            "content": "We provided set of guidelines to modeling, comparing and designing fully-quantized training schemes for large language models. Moreover, we followed those guidelines to arrive at Quartet: new SOTA full MXFP4 training algorithm. One current limiting factor is that Quartet was designed with specific (standard) data-type and compute architecture in mind. Certain aspects of our method rely on specialized operations, like stochastic rounding, which have hardware support for MXFP4, but may be lacking for other formats. In future work, we plan to look into generalizing our approach to alternative formats, as well as larger-scale distributed model execution."
        },
        {
            "title": "Acknowledgments",
            "content": "This research was funded in part by the Austrian Science Fund (FWF) 10.55776/COE12, i.e. the Bilateral AI Cluster of Excellence, and through generous gifts by NVIDIA and Google."
        },
        {
            "title": "References",
            "content": "[1] Dan Alistarh, Demjan Grubic, Jerry Li, Ryota Tomioka, and Milan Vojnovic. Qsgd: Communicationefficient sgd via gradient quantization and encoding. Advances in neural information processing systems, 30, 2017. [2] Saleh Ashkboos, Amirkeivan Mohtashami, Maximilian L. Croci, Bo Li, Pashmina Cameron, Martin Jaggi, Dan Alistarh, Torsten Hoefler, and James Hensman. Quarot: Outlier-free 4-bit inference in rotated llms. arXiv preprint arXiv:2404.00456, 2024. URL https://arxiv.org/abs/2404.00456. [3] Saleh Ashkboos, Mahdi Nikdan, Soroush Tabesh, Roberto L. Castro, Torsten Hoefler, and Dan Alistarh. Halo: Hadamard-assisted lower-precision optimization for llms, 2025. URL https://arxiv.org/abs/ 2501.02625. [4] Ron Banner, Itay Hubara, Elad Hoffer, and Daniel Soudry. Scalable methods for 8-bit training of neural networks. In Advances in Neural Information Processing Systems (NeurIPS), 2018. [5] Chaim Baskin, Natan Liss, Eli Schwartz, Evgenii Zheltonozhskii, Raja Giryes, Alex Bronstein, and Avi Mendelson. Uniq: Uniform noise injection for non-uniform quantization of neural networks. ACM Transactions on Computer Systems (TOCS), 37(1-4):115, 2021. [6] Yoshua Bengio, Nicholas Léonard, and Aaron Courville. Estimating or propagating gradients through stochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432, 2013. [7] Yash Bhalgat, Jinwon Lee, Markus Nagel, Tijmen Blankevoort, and Nojun Kwak. Lsq+: Improving low-bit quantization through learnable offsets and better initialization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, June 2020. [8] Dan Busbridge, Amitis Shidani, Floris Weers, Jason Ramapuram, Etai Littwin, and Russ Webb. Distillation scaling laws, 2025. URL https://arxiv.org/abs/2502.08606. [9] Jerry Chee, Yaohui Cai, Volodymyr Kuleshov, and Christopher De Sa. Quip: 2-bit quantization of large language models with guarantees. arXiv preprint arXiv:2307.13304, 2023. URL https://arxiv.org/ abs/2307.13304. [10] Brian Chmiel, Ron Banner, Elad Hoffer, Hilla Ben-Yaacov, and Daniel Soudry. Accurate Neural Training with 4-bit Matrix Multiplications at Standard Formats. In International Conference on Learning Representations (ICLR), 2023. [11] Brian Chmiel, Ron Banner, Elad Hoffer, Hilla Ben Yaacov, and Daniel Soudry. Accurate neural training with 4-bit matrix multiplications at standard formats, 2024. URL https://arxiv.org/abs/2112.10769. [12] Jungwook Choi, Zhuo Wang, Swagath Venkataramani, Pierce I-Jen Chuang, Vijayalakshmi Srinivasan, and Kailash Gopalakrishnan. Pact: Parameterized clipping activation for quantized neural networks. arXiv preprint arXiv:1805.06085, 2018. 10 [13] Ben Cottier, Robi Rahman, Loredana Fattorini, Nestor Maslej, and David Owen. The rising costs of training frontier ai models. arXiv preprint arXiv:2405.21015, 2024. URL https://arxiv.org/abs/ 2405.21015. [14] Dao-AILab. Fast hadamard transform in cuda, with pytorch interface. https://github.com/ Dao-AILab/fast-hadamard-transform, 2024. Accessed: 2025-05-13. [15] Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. LLM.int8(): 8-bit matrix multiplication for transformers at scale. arXiv preprint arXiv:2208.07339, 2022. URL https://arxiv.org/abs/ 2208.07339. [16] Jesse Dodge, Maarten Sap, Ana Marasovic, William Agnew, Gabriel Ilharco, Dirk Groeneveld, Margaret Mitchell, and Matt Gardner. Documenting large webtext corpora: case study on the colossal clean crawled corpus, 2021. URL https://arxiv.org/abs/2104.08758. [17] Steven Esser, Jeffrey McKinstry, Deepika Bablani, Rathinakumar Appuswamy, and Dharmendra Modha. Learned step size quantization. arXiv preprint arXiv:1902.08153, 2019. [18] Fino and Algazi. Unified matrix treatment of the fast walsh-hadamard transform. IEEE Transactions on Computers, 100(11):11421146, 1976. [19] Maxim Fishman, Brian Chmiel, Ron Banner, and Daniel Soudry. Scaling fp8 training to trillion-token llms. arXiv preprint arXiv:2409.12517, 2024. [20] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. GPTQ: Accurate post-training compression for generative pretrained transformers. arXiv preprint arXiv:2210.17323, 2022. URL https://arxiv.org/abs/2210.17323. [21] Elias Frantar, Carlos Riquelme Ruiz, Neil Houlsby, Dan Alistarh, and Utku Evci. Scaling laws for sparsely-connected foundation models. In International Conference on Learning Representations, 2024. [22] Elias Frantar, Utku Evci, Wonpyo Park, Neil Houlsby, and Dan Alistarh. Compression scaling laws:unifying sparsity and quantization, 2025. URL https://arxiv.org/abs/2502.16440. [23] Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad AlDahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurelien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Roziere, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, Danny Wyatt, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip Radenovic, Francisco Guzmán, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Govind Thattai, Graeme Nail, Gregoire Mialon, Guan Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov, Imanol Arrieta Ibarra, Isabel Kloumann, Ishan Misra, Ivan Evtimov, Jack Zhang, Jade Copet, Jaewon Lee, Jan Geffert, Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasuden Alwala, Karthik Prasad, Kartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heafield, Kevin Stone, Khalid El-Arini, Krithika Iyer, Kshitiz Malik, Kuenley Chiu, Kunal Bhalla, Kushal Lakhotia, Lauren Rantala-Yeary, Laurens van der Maaten, Lawrence Chen, Liang Tan, Liz Jenkins, Louis Martin, Lovish Madaan, Lubo Malo, Lukas Blecher, Lukas Landzaat, Luke de Oliveira, Madeline Muzzi, Mahesh Pasupuleti, Mannat Singh, Manohar Paluri, Marcin Kardas, Maria Tsimpoukelli, Mathew Oldham, Mathieu Rita, Maya Pavlova, Melanie Kambadur, Mike Lewis, Min Si, Mitesh Kumar Singh, Mona Hassan, Naman Goyal, Narjes Torabi, Nikolay Bashlykov, Nikolay Bogoychev, Niladri Chatterji, Ning Zhang, Olivier Duchenne, Onur Çelebi, Patrick Alrassy, Pengchuan Zhang, Pengwei Li, Petar Vasic, Peter Weng, Prajjwal Bhargava, Pratik Dubal, Praveen Krishnan, Punit Singh Koura, Puxin Xu, Qing He, Qingxiao Dong, Ragavan Srinivasan, Raj Ganapathy, Ramon Calderer, Ricardo Silveira Cabral, Robert Stojnic, Roberta Raileanu, Rohan Maheswari, Rohit Girdhar, Rohit Patel, Romain Sauvestre, Ronnie Polidoro, Roshan Sumbaly, Ross Taylor, Ruan Silva, Rui Hou, Rui Wang, Saghar Hosseini, Sahana Chennabasappa, Sanjay Singh, Sean Bell, Seohyun Sonia Kim, Sergey Edunov, Shaoliang Nie, Sharan Narang, Sharath Raparthy, Sheng Shen, Shengye Wan, Shruti Bhosale, Shun Zhang, Simon Vandenhende, Soumya Batra, Spencer Whitman, Sten Sootla, Stephane Collot, Suchin Gururangan, Sydney Borodinsky, Tamar Herman, Tara Fowler, Tarek Sheasha, Thomas Georgiou, Thomas Scialom, Tobias Speckbacher, Todor Mihaylov, Tong Xiao, Ujjwal Karn, Vedanuj Goswami, Vibhor Gupta, Vignesh Ramanathan, Viktor Kerkez, Vincent Gonguet, Virginie Do, Vish Vogeti, Vítor Albiero, Vladan Petrovic, Weiwei Chu, Wenhan Xiong, Wenyin Fu, Whitney Meers, Xavier Martinet, Xiaodong Wang, Xiaofang Wang, Xiaoqing Ellen 11 Tan, Xide Xia, Xinfeng Xie, Xuchao Jia, Xuewei Wang, Yaelle Goldschlag, Yashesh Gaur, Yasmine Babaei, Yi Wen, Yiwen Song, Yuchen Zhang, Yue Li, Yuning Mao, Zacharie Delpierre Coudert, Zheng Yan, Zhengxing Chen, Zoe Papakipos, Aaditya Singh, Aayushi Srivastava, Abha Jain, Adam Kelsey, Adam Shajnfeld, Adithya Gangidi, Adolfo Victoria, Ahuva Goldstand, Ajay Menon, Ajay Sharma, Alex Boesenberg, Alexei Baevski, Allie Feinstein, Amanda Kallet, Amit Sangani, Amos Teo, Anam Yunus, Andrei Lupu, Andres Alvarado, Andrew Caples, Andrew Gu, Andrew Ho, Andrew Poulton, Andrew Ryan, Ankit Ramchandani, Annie Dong, Annie Franco, Anuj Goyal, Aparajita Saraf, Arkabandhu Chowdhury, Ashley Gabriel, Ashwin Bharambe, Assaf Eisenman, Azadeh Yazdan, Beau James, Ben Maurer, Benjamin Leonhardi, Bernie Huang, Beth Loyd, Beto De Paola, Bhargavi Paranjape, Bing Liu, Bo Wu, Boyu Ni, Braden Hancock, Bram Wasti, Brandon Spence, Brani Stojkovic, Brian Gamido, Britt Montalvo, Carl Parker, Carly Burton, Catalina Mejia, Ce Liu, Changhan Wang, Changkyu Kim, Chao Zhou, Chester Hu, Ching-Hsiang Chu, Chris Cai, Chris Tindal, Christoph Feichtenhofer, Cynthia Gao, Damon Civin, Dana Beaty, Daniel Kreymer, Daniel Li, David Adkins, David Xu, Davide Testuggine, Delia David, Devi Parikh, Diana Liskovich, Didem Foss, Dingkang Wang, Duc Le, Dustin Holland, Edward Dowling, Eissa Jamil, Elaine Montgomery, Eleonora Presani, Emily Hahn, Emily Wood, Eric-Tuan Le, Erik Brinkman, Esteban Arcaute, Evan Dunbar, Evan Smothers, Fei Sun, Felix Kreuk, Feng Tian, Filippos Kokkinos, Firat Ozgenel, Francesco Caggioni, Frank Kanayet, Frank Seide, Gabriela Medina Florez, Gabriella Schwarz, Gada Badeer, Georgia Swee, Gil Halpern, Grant Herman, Grigory Sizov, Guangyi, Zhang, Guna Lakshminarayanan, Hakan Inan, Hamid Shojanazeri, Han Zou, Hannah Wang, Hanwen Zha, Haroun Habeeb, Harrison Rudolph, Helen Suk, Henry Aspegren, Hunter Goldman, Hongyuan Zhan, Ibrahim Damlaj, Igor Molybog, Igor Tufanov, Ilias Leontiadis, Irina-Elena Veliche, Itai Gat, Jake Weissman, James Geboski, James Kohli, Janice Lam, Japhet Asher, Jean-Baptiste Gaya, Jeff Marcus, Jeff Tang, Jennifer Chan, Jenny Zhen, Jeremy Reizenstein, Jeremy Teboul, Jessica Zhong, Jian Jin, Jingyi Yang, Joe Cummings, Jon Carvill, Jon Shepard, Jonathan McPhie, Jonathan Torres, Josh Ginsburg, Junjie Wang, Kai Wu, Kam Hou U, Karan Saxena, Kartikay Khandelwal, Katayoun Zand, Kathy Matosich, Kaushik Veeraraghavan, Kelly Michelena, Keqian Li, Kiran Jagadeesh, Kun Huang, Kunal Chawla, Kyle Huang, Lailin Chen, Lakshya Garg, Lavender A, Leandro Silva, Lee Bell, Lei Zhang, Liangpeng Guo, Licheng Yu, Liron Moshkovich, Luca Wehrstedt, Madian Khabsa, Manav Avalani, Manish Bhatt, Martynas Mankus, Matan Hasson, Matthew Lennie, Matthias Reso, Maxim Groshev, Maxim Naumov, Maya Lathi, Meghan Keneally, Miao Liu, Michael L. Seltzer, Michal Valko, Michelle Restrepo, Mihir Patel, Mik Vyatskov, Mikayel Samvelyan, Mike Clark, Mike Macey, Mike Wang, Miquel Jubert Hermoso, Mo Metanat, Mohammad Rastegari, Munish Bansal, Nandhini Santhanam, Natascha Parks, Natasha White, Navyata Bawa, Nayan Singhal, Nick Egebo, Nicolas Usunier, Nikhil Mehta, Nikolay Pavlovich Laptev, Ning Dong, Norman Cheng, Oleg Chernoguz, Olivia Hart, Omkar Salpekar, Ozlem Kalinli, Parkin Kent, Parth Parekh, Paul Saab, Pavan Balaji, Pedro Rittner, Philip Bontrager, Pierre Roux, Piotr Dollar, Polina Zvyagina, Prashant Ratanchandani, Pritish Yuvraj, Qian Liang, Rachad Alao, Rachel Rodriguez, Rafi Ayub, Raghotham Murthy, Raghu Nayani, Rahul Mitra, Rangaprabhu Parthasarathy, Raymond Li, Rebekkah Hogan, Robin Battey, Rocky Wang, Russ Howes, Ruty Rinott, Sachin Mehta, Sachin Siby, Sai Jayesh Bondu, Samyak Datta, Sara Chugh, Sara Hunt, Sargun Dhillon, Sasha Sidorov, Satadru Pan, Saurabh Mahajan, Saurabh Verma, Seiji Yamamoto, Sharadh Ramaswamy, Shaun Lindsay, Shaun Lindsay, Sheng Feng, Shenghao Lin, Shengxin Cindy Zha, Shishir Patil, Shiva Shankar, Shuqiang Zhang, Shuqiang Zhang, Sinong Wang, Sneha Agarwal, Soji Sajuyigbe, Soumith Chintala, Stephanie Max, Stephen Chen, Steve Kehoe, Steve Satterfield, Sudarshan Govindaprasad, Sumit Gupta, Summer Deng, Sungmin Cho, Sunny Virk, Suraj Subramanian, Sy Choudhury, Sydney Goldman, Tal Remez, Tamar Glaser, Tamara Best, Thilo Koehler, Thomas Robinson, Tianhe Li, Tianjun Zhang, Tim Matthews, Timothy Chou, Tzook Shaked, Varun Vontimitta, Victoria Ajayi, Victoria Montanez, Vijai Mohan, Vinay Satish Kumar, Vishal Mangla, Vlad Ionescu, Vlad Poenaru, Vlad Tiberiu Mihailescu, Vladimir Ivanov, Wei Li, Wenchen Wang, Wenwen Jiang, Wes Bouaziz, Will Constable, Xiaocheng Tang, Xiaojian Wu, Xiaolan Wang, Xilun Wu, Xinbo Gao, Yaniv Kleinman, Yanjun Chen, Ye Hu, Ye Jia, Ye Qi, Yenda Li, Yilin Zhang, Ying Zhang, Yossi Adi, Youngjin Nam, Yu, Wang, Yu Zhao, Yuchen Hao, Yundi Qian, Yunlu Li, Yuzi He, Zach Rait, Zachary DeVito, Zef Rosnbrick, Zhaoduo Wen, Zhenyu Yang, Zhiwei Zhao, and Zhiyu Ma. The llama 3 herd of models, 2024. URL https://arxiv.org/abs/2407.21783. [24] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, and Laurent Sifre. Training compute-optimal large language models, 2022. URL https://arxiv.org/abs/2203.15556. [25] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models, 2020. URL https://arxiv.org/abs/2001.08361. [26] Ayush Kaushal, Tejas Vaidhya, Arnab Kumar Mondal, Tejas Pandey, Aaryan Bhagat, and Irina Rish. Spectra: Surprising effectiveness of pretraining ternary language models at scale. arXiv preprint arXiv:2407.12327, 2024. 12 [27] Tanishq Kumar, Zachary Ankner, Benjamin F. Spector, Blake Bordelon, Niklas Muennighoff, Mansheej Paul, Cengiz Pehlevan, Christopher Ré, and Aditi Raghunathan. Scaling laws for precision, 2024. URL https://arxiv.org/abs/2411.04330. [28] Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Daya Guo, Dejian Yang, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Haowei Zhang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Li, Hui Qu, J. L. Cai, Jian Liang, Jianzhong Guo, Jiaqi Ni, Jiashi Li, Jiawei Wang, Jin Chen, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, Junxiao Song, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Lei Xu, Leyi Xia, Liang Zhao, Litong Wang, Liyue Zhang, Meng Li, Miaojun Wang, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Mingming Li, Ning Tian, Panpan Huang, Peiyi Wang, Peng Zhang, Qiancheng Wang, Qihao Zhu, Qinyu Chen, Qiushi Du, R. J. Chen, R. L. Jin, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, Runxin Xu, Ruoyu Zhang, Ruyi Chen, S. S. Li, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shaoqing Wu, Shengfeng Ye, Shirong Ma, Shiyu Wang, Shuang Zhou, Shuiping Yu, Shunfeng Zhou, Shuting Pan, T. Wang, Tao Yun, Tian Pei, Tianyu Sun, W. L. Xiao, Wangding Zeng, Wanjia Zhao, Wei An, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, X. Q. Li, Xiangyue Jin, Xianzu Wang, Xiao Bi, Xiaodong Liu, Xiaohan Wang, Xiaojin Shen, Xiaokang Chen, Xiaokang Zhang, Xiaosha Chen, Xiaotao Nie, Xiaowen Sun, Xiaoxiang Wang, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xingkai Yu, Xinnan Song, Xinxia Shan, Xinyi Zhou, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, Y. K. Li, Y. Q. Wang, Y. X. Wei, Y. X. Zhu, Yang Zhang, Yanhong Xu, Yanping Huang, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Li, Yaohui Wang, Yi Yu, Yi Zheng, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Ying Tang, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yu Wu, Yuan Ou, Yuchen Zhu, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He, Yukun Zha, Yunfan Xiong, Yunxian Ma, Yuting Yan, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Z. F. Wu, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhen Huang, Zhen Zhang, Zhenda Xie, Zhengyan Zhang, Zhewen Hao, Zhibin Gou, Zhicheng Ma, Zhigang Yan, Zhihong Shao, Zhipeng Xu, Zhiyu Wu, Zhongyu Zhang, Zhuoshu Li, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Ziyi Gao, and Zizheng Pan. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024. URL https://arxiv.org/abs/2412.19437. [29] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization, 2019. URL https://arxiv. org/abs/1711.05101. [30] Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, and Hao Wu. Mixed precision training, 2018. URL https://arxiv.org/abs/1710.03740. [31] NVIDIA Corporation. Nvidia blackwell architecture technical brief. https://resources.nvidia. com/en-us-blackwell-architecture, 2024. Accessed: 2025-05-13. [32] Open Compute Project. Ocp microscaling formats (mx) specification version 1.0. https://www. opencompute.org/documents/ocp-microscaling-formats-mx-v1-0-spec-final-pdf, 2023. Accessed: 2025-05-13. [33] Andrei Panferov, Jiale Chen, Soroush Tabesh, Roberto L. Castro, Mahdi Nikdan, and Dan Alistarh. Quest: Stable training of llms with 1-bit weights and activations, 2025. URL https://arxiv.org/abs/2502. 05003. [34] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with unified text-to-text transformer. In Proceedings of the 37th International Conference on Machine Learning, pages 1396213982. PMLR, 2020. C4 dataset introduced as part of this work. [35] Nikhil Sardana, Jacob Portes, Sasha Doubov, and Jonathan Frankle. Beyond chinchilla-optimal: Accounting for inference in language model scaling laws, 2025. URL https://arxiv.org/abs/2401.00448. [36] Xiao Sun, Naigang Wang, Chia-Yu Chen, Jiamin Ni, Ankur Agrawal, Xiaodong Cui, Swagath Venkataramani, Kaoutar El Maghraoui, Vijayalakshmi Srinivasan, and Kailash Gopalakrishnan. Ultra-Low Precision 4-bit Training of Deep Neural Networks. In Advances in Neural Information Processing Systems (NeurIPS), 2020. [37] Ananda Theertha Suresh, Yu Felix, Sanjiv Kumar, and Brendan McMahan. Distributed mean estimation with limited communication. In International conference on machine learning, pages 33293337. PMLR, 2017. [38] PyTorch Team. Hadacore: Accelerating large language models with fast hadamard transforms. https: //pytorch.org/blog/hadacore/, 2024. Accessed: 2025-05-13. 13 [39] Vijay Thakkar, Pradeep Ramani, Cris Cecka, Aniket Shivam, Honghao Lu, Ethan Yan, Jack Kosaian, Mark Hoemmen, Haicheng Wu, Andrew Kerr, Matt Nicely, Duane Merrill, Dustyn Blasig, Fengqi Qiao, Piotr Majcher, Paul Springer, Markus Hohnerbach, Jin Wang, and Manish Gupta. CUTLASS, January 2025. URL https://github.com/NVIDIA/cutlass. [40] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models, 2023. URL https://arxiv.org/abs/2307.09288. [41] Albert Tseng, Tao Yu, and Youngsuk Park. Training llms with mxfp4, 2025. URL https://arxiv.org/ abs/2502.20586. [42] Shay Vargaftik, Ran Ben Basat, Amit Portnoy, Gal Mendelson, Yaniv Ben-Itzhak, and Michael Mitzenmacher. Drive: One-bit distributed mean estimation, 2021. URL https://arxiv.org/abs/2105. 08339. [43] Shay Vargaftik, Ran Ben Basat, Amit Portnoy, Gal Mendelson, Yaniv Ben-Itzhak, and Michael Mitzenmacher. Eden: Communication-efficient and robust distributed mean estimation for federated learning, 2022. URL https://arxiv.org/abs/2108.08842. [44] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. [45] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need, 2023. URL https://arxiv.org/abs/1706. 03762. [46] Hongyu Wang, Shuming Ma, Li Dong, Shaohan Huang, Huaijie Wang, Lingxiao Ma, Fan Yang, Ruiping Wang, Yi Wu, and Furu Wei. Bitnet: Scaling 1-bit transformers for large language models. arXiv preprint arXiv:2310.11453, 2023. [47] Ruizhe Wang, Yeyun Gong, Xiao Liu, Guoshuai Zhao, Ziyue Yang, Baining Guo, Zhengjun Zha, and Peng Cheng. Optimizing Large Language Model Training Using FP4 Quantization. arXiv preprint arXiv:2501.17116, 2024. [48] Mitchell Wortsman, Tim Dettmers, Luke Zettlemoyer, Ari Morcos, Ali Farhadi, and Ludwig Schmidt. Stable and low-precision training for large-scale vision-language models. Advances in Neural Information Processing Systems, 36:1027110298, 2023. [49] Mitchell Wortsman, Tim Dettmers, Luke Zettlemoyer, Ari S. Morcos, Ali Farhadi, and Ludwig Schmidt. Stable and Low-Precision Training for Large-Scale Vision-Language Models. arXiv preprint arXiv:2304.13013, 2023. [50] Haocheng Xi, Changhao Li, Jianfei Chen, and Jun Zhu. Training Transformers with 4-bit Integers. In Advances in Neural Information Processing Systems (NeurIPS), 2023. [51] Haocheng Xi, Yuxiang Chen, Kang Zhao, Kaijun Zheng, Jianfei Chen, and Jun Zhu. Jetfire: Efficient and accurate transformer pretraining with int8 data flow and per-block quantization. arXiv preprint arXiv:2403.12422, 2024. [52] Haocheng Xi, Yuxiang Chen, Kang Zhao, Kaijun Zheng, Jianfei Chen, and Jun Zhu. Jetfire: Efficient and Accurate Transformer Pretraining with INT8 Data Flow and Per-Block Quantization. In Proceedings of the 41st International Conference on Machine Learning (ICML), 2024. [53] Yukuan Yang, Shuang Wu, Lei Deng, Tianyi Yan, Yuan Xie, and Guoqi Li. Training High-Performance and Large-Scale Deep Neural Networks with Full 8-bit Integers. arXiv preprint arXiv:1909.02384, 2020. 14 Figure 4: Comparison of various scaling law fits and their errors."
        },
        {
            "title": "A Technical Appendices and Supplementary Material",
            "content": "A.1 Training Hyper-parameters Table 4 lists model-specific hyper-parameters. Table 5 lists hyper-parameters shared across all experiments. Hyperparameter 30M 50M 100M 200M 7B Number of Layers (Nlayer) Embedding Dimension (Nembd) Attention Heads (Nhead) Learning Rate (LR) 6 640 5 0.0012 7 768 6 0.0012 8 1024 8 0.0006 10 1280 10 0. 32 4096 32 9.375106 Table 4: Model-specific hyperparameters used in our experiments. Hyperparameter Value Sequence Length Batch Size Optimizer Learning Rate Schedule Gradient Clipping Weight Decay (γ) Number of GPUs Data Type (optimizer/accumulators) 512 512 AdamW Cosine decay with 10% warm-up 1.0 0.1 8 FP Table 5: Common hyperparameters used across all model sizes and quantization setups. A.2 Scaling Law fitting We fit the scaling law in two stages: Stage 1. Identical to prior work [8], we fit the unquantized scaling law of the form L(N, D) = (cid:18) N α + Dβ (cid:19)γ + on baseline BF16 runs for [30M, 50M, 100M, 200M ] and D/N [25, 50, 100, 200, 400, 800] (see Figure 1 (a)) using Huber loss with δ = 104 on logarithm of L. Table 6 shows the resulting fit. Stage 2. Using the fixed fitted parameters from stage 1, we fit the additional effN and effD parameters using the same loss function."
        },
        {
            "title": "Value",
            "content": "A 1.52 105 α 0.589 5.25 105 β γ 0.544 1.35 0.274 Table 6: Fitted scaling law coefficients. For the isolated methods compared in Section 4.2, we fit effN and effD independently for forward-only and backward-only quantization respectively. For the end-to-end 4-bit comparison in Section 5, we fitted the parameters jointly for the setups present in Table 3. Alternative forms. We additionally for the scaling law forms with fixed γ = 1 [24] and β = 1 [25]. The fits are presented in Figure 4 alongside the mainly used of Busbridge et al. [8]."
        }
    ],
    "affiliations": [
        "ETH Zürich",
        "ISTA",
        "ISTA & Red Hat AI"
    ]
}