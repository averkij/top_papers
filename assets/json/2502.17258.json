{
    "paper_title": "VideoGrain: Modulating Space-Time Attention for Multi-grained Video Editing",
    "authors": [
        "Xiangpeng Yang",
        "Linchao Zhu",
        "Hehe Fan",
        "Yi Yang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advancements in diffusion models have significantly improved video generation and editing capabilities. However, multi-grained video editing, which encompasses class-level, instance-level, and part-level modifications, remains a formidable challenge. The major difficulties in multi-grained editing include semantic misalignment of text-to-region control and feature coupling within the diffusion model. To address these difficulties, we present VideoGrain, a zero-shot approach that modulates space-time (cross- and self-) attention mechanisms to achieve fine-grained control over video content. We enhance text-to-region control by amplifying each local prompt's attention to its corresponding spatial-disentangled region while minimizing interactions with irrelevant areas in cross-attention. Additionally, we improve feature separation by increasing intra-region awareness and reducing inter-region interference in self-attention. Extensive experiments demonstrate our method achieves state-of-the-art performance in real-world scenarios. Our code, data, and demos are available at https://knightyxp.github.io/VideoGrain_project_page/"
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 2 ] . [ 1 8 5 2 7 1 . 2 0 5 2 : r Published as conference paper at ICLR VIDEOGRAIN: MODULATING SPACE-TIME ATTENTION FOR MULTI-GRAINED VIDEO EDITING Xiangpeng Yang 1 Linchao Zhu 2 Hehe Fan 2 Yi Yang 2 1 ReLER Lab, AAII, University of Technology Sydney Project Page: https://knightyxp.github.io/VideoGrain_project_page 2 ReLER Lab, CCAI, Zhejiang University Figure 1: VideoGrain enables multi-grained video editing across class, instance, and part levels."
        },
        {
            "title": "ABSTRACT",
            "content": "Recent advancements in diffusion models have significantly improved video generation and editing capabilities. However, multi-grained video editing, which encompasses class-level, instance-level, and part-level modifications, remains formidable challenge. The major difficulties in multi-grained editing include semantic misalignment of text-to-region control and feature coupling within the diffusion model. To address these difficulties, we present VideoGrain, zeroshot approach that modulates space-time (crossand self-) attention mechanisms to achieve fine-grained control over video content. We enhance text-to-region control by amplifying each local prompts attention to its corresponding spatialdisentangled region while minimizing interactions with irrelevant areas in crossattention. Additionally, we improve feature separation by increasing intra-region awareness and reducing inter-region interference in self-attention. Extensive experiments demonstrate our method achieves state-of-the-art performance in realworld scenarios. Our code, data, and demos are available on the project page. 1 Published as conference paper at ICLR"
        },
        {
            "title": "INTRODUCTION",
            "content": "Figure 2: Definition of multi-grained video editing and comparison on instance editing Recent advances in Text-to-Image (T2I) and Text-to-Video (T2V) diffusion models (Rombach et al., 2022; Wang et al., 2023a; Brooks et al., 2024) have enabled video manipulation through natural language prompts. In practical applications, enabling users to edit regions at various levels of granularity based on textual prompts offers greater flexibility. To investigate this, we introduce new task called multi-grained video editing, which encompasses class-level, instance-level, and partlevel editing, as shown in Fig. 2 left. Class-level editing refers to modifying objects within the same class. Instance-level editing means editing different instances into distinct objects. Part-level going further, requires adding new objects or modifying existing attributes at part-level. While existing methods employ various visual consistency techniques, such as optical flow (Cong et al., 2023; Yang et al., 2023), control signals (Zhang et al., 2023b), or feature correspondence (Geyer et al., 2023). These methods remain instance-agnostic, often mixing features of different instances during editing (see Fig. 2 right). Ground-A-Video (Jeong & Ye, 2023), which inherits text-to-bounding box generation priors (Li et al., 2023), should be instance-level editing but still suffer from artifacts. Similarly, recent T2V-based methods like DMT (Yatim et al., 2024) and Pika (pik), although equipped with video generation priors, struggle with multi-grained edits. We find that the core issue is that diffusion models tend to treat different instances as the same class segments, leading to strong feature coupling across instances, as illustrated in Figure 3. To address this problem, our primary insight is to 1) enable text-to-region control and 2) keep feature separation between regions. In the typical diffusion models, the cross-attention layer serves as key component to update textual features control over each spatial region, while the self-attention layer generates globally coherent structures by connecting each frame token across time. Therefore, we propose Spatial-Temporal Layout-Guided Attention (ST-Layout Attn), which modulates both spacetime crossand self-attention in unified manner to achieve the above goals. In the cross-attention layer, the uniform application of global text prompts across all frame tokens leads to severe semantic misalignment, which reduces the precision of multi-grained text-to-region control. To address this, we modulate cross-attention to amplify each local prompts focus on its corresponding spatial-disentangled region while suppressing attention to irrelevant areas. In the self-attention layer, pixels from one region may attend to outside or similar regions within the same class, leading to feature coupling and texture mixing, which is an inherent limitation of diffusion models that complicates multi-grained video editing. To mitigate this, we modulate self-attention to enhance feature separation by increasing intra-region focus and reducing inter-region interactions, ensuring each query attends only to its target region. Our key contributions can be summarized as follows: To the best of our knowledge, this is the first attempt at multi-grained video editing. Our method enables both class-level, instance-level and part-level editing. We propose novel framework, dubbed VideoGrain, which modulates spatial-temporal crossand self-attention for text-to-region control and feature separation between regions. Without tuning any parameters, we achieve state-of-the-art results on existing benchmarks and real-world videos both qualitatively and quantitatively. Published as conference paper at ICLR"
        },
        {
            "title": "2.1 TEXT-TO-IMAGE EDITING/GENERATION",
            "content": "In the realm of single attribute text-to-image editing, various approaches have been explored, from manipulating attention maps in Pix2Pix-Zero (Parmar et al., 2023) and Prompt2Prompt (Hertz et al., 2022) to employing masks in DiffEdit (Couairon et al., 2023) and Latent Blend (Avrahami et al., 2022; 2023) for foreground modifications while preserving the background. For multi-grained editing, efforts like Attention and Excite (Chefer et al., 2023) and DPL (Wang et al., 2023b) focus on maximizing attention scores for each subject token and reducing attention leakage. In image generation, (Kim et al., 2023) modulates attention based on layout masks and dense captions, while (Phung et al., 2023) proposed an attention refocus loss for regularization. However, using single-frame layout masks and dense captioning alone is insufficient for video editing, as it fails to maintain the original videos integrity and temporal consistency."
        },
        {
            "title": "2.2 TEXT-TO-VIDEO EDITING",
            "content": "Video Editing based on Image Diffusion Models. Tune-A-Video (TAV) (Wu et al., 2022) is the first work to extend latent diffusion models to the spatial-temporal domain and encode the source motion implicitly by one-shot tuning but still fails to preserve local details. Fatezero (Qi et al., 2023) and Pix2Video (Ceylan et al., 2023) fuse selfor cross-attention maps in the inversion process for temporal consistency. However, (Qi et al., 2023) requires extensive RAM usage and suffers from layout preservation even when equipping TAV for local object editing. (Chai et al., 2023) and (Ouyang et al., 2023), following the Neural Atlas (Kasten et al., 2021) or dynamic Nerfs deformation field (Pumarola et al., 2021), struggle with non-grid human motion. Subsequent methods like Rerender-A-Video (Yang et al., 2023), FLATTEN (Cong et al., 2023) ControlVideo (Zhang et al., 2023b) achieve strict temporal consistency via optical-flow, depth/edge maps, but failed in multi-grained editing while preserving original layouts. Tokenflow (Geyer et al., 2023) enforces linear mix of nearest key-frame features to ensure consistency but results in detail loss. Ground-AVideo (Jeong & Ye, 2023) leverages groundings for multi-grained editing, but it suffers from feature mixing when bounding boxes overlap. Video Editing based on Video Diffusion Models. Previous video editing work primarily utilized text-to-image SD model (Rombach et al., 2022). Recent advancements in video foundation models (Yu et al., 2023; Guo et al., 2023; Wang et al., 2023a; Yang et al., 2024e) have led efforts like VideoSwap (Gu et al., 2023) to employ temporal priors for customized motion transfer or motion editing (Mou et al., 2025). Yet, current video foundation models are limited to fixed views and struggle with non-grid human motions. Additionally, these editing methods require tuning parameters, which poses challenge for real-time video editing applications. In contrast, our VideoGrain method requires no parameter tuning, enabling zero-shot, multi-grained video editing."
        },
        {
            "title": "3 METHOD",
            "content": "3.1 MOTIVATION To investigate why previous methods failed in instance-level video editing (see Fig. 2), we begin with basic analysis of the self-attention and cross-attention features within the diffusion model. As shown in Fig. 3 (b), we apply K-Means clustering to the per-frame self-attention features during DDIM Inversion. Although the clustering captures clear semantic layout, it fails to distinguish between distinct instances (e.g., left man and right man). Increasing the number of clusters leads to finer segmentation at the part level but does not resolve this issue, indicating that feature homogeneity across instances limits the diffusion models effectiveness in multi-grained video editing. Next, we attempt to edit the same class of two men into different instances using SDEdit (Meng et al., 2022). However, Fig. 3 (d) shows that the weights for Iron Man and Spiderman overlap on the left man, and blossoms weight leaks onto the right man, resulting in the failed edit in (c). 3 Published as conference paper at ICLR Figure 3: Analysis of why the diffusion model failed in instance-level video editing. Our goal is to edit left man into Iron Man, right man into Spiderman, and trees into cherry blossoms. In (b), we apply K-Means on self-attention, and in (d), we visualize the 32x32 cross-attention map. Thus, for effective multi-grained editing, we pose the following question: Can we modulate attention to ensure that each local edits attention weights are accurately distributed in the intended regions? To answer this, we propose VideoGrain with two key designs: (1) Modulate cross-attention to induce textual features to congregate in corresponding spatial-disentangled regions, thereby enabling textto-region control. (2) Modulate self-attention across the spatial-temporal axis to enhance intraregion focus and reduce inter-region interference, avoiding feature coupling within diffusion model. 3.2 PROBLEM FORMULATION The purpose of this work is to perform multi-grained video editing across multiple regions based on the given prompts. This involves three hierarchical levels: (1) Class-level editing: Editing objects within the same class. (e.g., changing two men to Spiderman, where both belong to the human class, as seen in Fig. 2 second column) (2) Instance-level editing: Editing each individual instance to distinct object. (e.g., editing left man to Spiderman, right man to Polar Bear, as shown in Fig. 2 third column). (3) Part-level editing: Applying part-level edit to specific elements of individual instances. (e.g., adding sunglasses when editing the right man to Polar Bear in Fig. 2 fourth column). Given source video RN 3HW , where is the number of frames, our goal is to obtain an edited video based on specified edits. We aim to improve multi-grained control in video editing by conditioning on each regions location and its text prompt. More formally, we optimize video editing model (τg, (τ1, m1), . . . , (τk, mk)), where τg is global prompt, and (τk, mk) are the kth regions prompt and corresponding location. 3.3 OVERALL FRAMEWORK The proposed zero-shot multi-grained video editing pipeline is illustrated in Fig. 4 top. Initially, to retain high fidelity, we perform DDIM Inversion (Song et al., 2021) over the clean latent x0 to get the noisy latent xt. After the inversion process, we cluster the self-attention features to get the semantic layout as in Fig. 3 (b). Since self-attention features alone cannot distinguish between individual instances, we further employ SAM-Track (Cheng et al., 2023) to segment each instance. Finally, in the denoising process, we introduce ST-Layout Attn to modulate crossand self-attention for text-to-region control and keep feature separation between regions, as detailed in Sec. 3.4. Different from one global text prompt control of all frames, VideoGrain allows paired instanceor part-level prompts and their locations to be specified in the denoising process. Our method is also versatile to ControlNet condition e, which can be depth or pose maps to provide structure conditions. Published as conference paper at ICLR 2025 Figure 4: VideoGrain pipeline. (1) we integrate ST-Layout Attn into the frozen SD for multi-grained editing, where we modulate selfand cross-attention in unified manner. (2) In cross-attention, we view each local prompt and its location as positive pairs, while the prompt and outside-location areas are negative pairs, enabling text-to-region control. (3) In self-attention, we enhance positive awareness within intra-regions and restrict negative interactions between inter-regions across frames, making each query only attend to the target region and keep feature separation. In the bottom two figures, denotes original attention score and w, denotes the word and frame index. 3.4 SPATIAL-TEMPORAL LAYOUT-GUIDED ATTENTION Based on the observation in Sec.3.1, cross-attention weight distribution adheres to the edit result. Meanwhile, self-attention is also crucial to generate temporal consistent video. However, the pixels in one region may attend to outside or similar regions, which poses an obstacle for multi-grained video editing. Therefore, we need to modulate both selfand cross-attention to make each pixel or local prompt only focus on the correct region. To achieve this goal, we modulate both crossand self-attention mechanisms via unified increase positive and decrease negative manner. Specifically, for the ith frame of the query feature, we modulate the query-key QK condition map as follows: Aself/cross = softmax( self/cross = Ri pos QK + λM self/cross (1 Ri) neg ), , (1) where Ri Rquerieskeys indicates the query-key pair condition map at frame i, manipulating whether to increase or decrease the attention score for particular pair. And λ = ξ(t) (1 Si) is regularization term. We follow the conclusion from (Kim et al., 2023), the ξ(t) controls the modulation intensity across time-steps, allowing for gradual refinement of shape and appearance details. The latter is size regulation term, making smaller region mk subjected to larger modulation, enabling dynamic attention weight adjustments to layout size variations. 5 Published as conference paper at ICLR 2025 Modulate Cross-Attention for Text-to-Region Control. In the cross-attention layer, the textual feature serves as key and value, and interacts with the query feature from the video latent. Since each instances appearance and location are closely related to the cross-attention weight distribution, we aim to encourage each instances textual features to congregate in the corresponding location. As shown in Fig. 4 mid, given the layout condition (τk, mk). For example, for τ1 = Spiderman, within the query-key cross-attention map, we can manually specify that the portion of the query feature corresponding to m1 is positive, while all the remaining parts are designated as negative. Therefore, for each frame i, we can set the modulation value in cross attention layer as:"
        },
        {
            "title": "M pos\nM neg",
            "content": "i = max(QK ) QK , = QK min(QK ),"
        },
        {
            "title": "Rcross\ni",
            "content": "[x, y] = (cid:110) mi,k, 0, if τk otherwise , (2) (3) where and are the query and key indices, and Rcross is the query-key condition map in the cross attention layer. We regularize this condition map by initially broadcasting each regions mask mi,k R(HW )L. to its corresponding text key embedding Kτk , resulting in condition map Rcross Each sub-region intensity then adjusts gradually in the generation process. We set pos/neg based on the gap between max/min values and the original scores, to keep modulated values within the original range. Our modulation is applied to all frames to achieve spatial-temporal region control. As shown in Fig. 4 (mid right), after adding positive and subtracting negative values, the original cross-attn weight of Spiderman (e.g., p) is amplified and focused on the left man. While the distract weight of polar bear become concentrated on the right man. These indicate our modulation redistributes each prompts weight align with target areas, enabling precise text-to-region control. Modulate Self-Attention to Keep Feature Separation. To adapt the T2I model for T2V editing, we treat the full video as larger picture, replacing spatial attention with spatial-temporal selfattention while retaining the pretrained weights. This enhances cross-frame interaction and provides broader visual context. However, naive self-attention can cause regions to attend to irrelevant or similar areas (e.g., Fig. 4 bottom, before modulation query attend to two-man), which leads to mixed texture. To address this, we need to strengthen positive focus within the same region and restrict negative interactions between different regions. As shown in Fig. 4 (bottom left), the maximum cross-frame diffusion feature indicates the strongest response among tokens within the same region. Note that DIFT (Tang et al., 2023) uses this to match different images, while we focus on cross-frame correspondences and intra-region attention modulation in the generation process. Nevertheless, negative inter-region correspondence is equally crucial for decoupling feature mixing. Beyond DIFT, we find that the minimum cross-frame diffusion feature similarity effectively captures the relations between tokens across different regions. Therefore, we define the spatial-temporal positive/negative values as: pos neg = max(Qi[K1, , Kn]) Qi[K1, , Kn]), = Qi[K1, , Kn] min(Qi[K1, , Kn]). (4) To ensure each patch attends to intra-regions feature while avoiding interaction in inter-regions feature. We define the spatial-temporal query-key condition map: Rself [x, y] = (cid:26) 0, [1 : ], if mi,k[x] = mj,k[y] 1, otherwise . (5) For frame indices and j, the value is zero when tokens belong to different instances across frames. As shown in the right part of Fig. 4 bottom, after applying our self-attention modulation, the query feature from the left mans nose (e.g., p) attends only to the left instance, avoiding distraction to the right instance. This demonstrates that our self-attention modulation breaks the diffusion models class-level feature correspondence, ensuring feature separation at the instance level. 6 Published as conference paper at ICLR 2025 Figure 5: Qualitative results. VideoGrain achieves multi-grained video editing, including classlevel, instance-level, and part-level. We refer the reader to our project page for full-video results. 4 EXPERIMENTS 4.1 EXPERIMENTAL SETTINGS In the experiment, we adopt the pretrained Stable Diffusion v1.5 as the base model, using 50 steps of DDIM inversion and denoising. Our VideoGrain operates in zero-shot manner, requiring no additional parameter tuning. To enhance memory efficiency, we re-engineer slice attention within our ST Layout Attn. ST Layout Attn is applied during the first 15 denoising steps. We set ξ(t) = 0.3t5 for self-attention and ξ(t) = t5 for cross-attention, where the timestep [0, 1] is normalized. All The experiments are conducted on an NVIDIA A40 GPU. We evaluate our VideoGrain using dataset of 76 video-text pairs, including videos from DAVIS (Perazzi et al., 2016), TGVE1, and the Internet2 , with 16-32 frames per video. Four automatic metrics are employed for evaluation: CLIPT, CLIP-F, Warp-Err, and Q-edit, following (Wu et al., 2022; Cong et al., 2023). All metrics are scaled by 100 for clarity. For baselines, we compare against T2I-based methods, including FateZero (Qi et al., 2023), ControlVideo (Zhang et al., 2023b), TokenFlow (Geyer et al., 2023), GroundVideo (Jeong & Ye, 2023) and T2V-based DMT (Yatim et al., 2024). To ensure temporal consistency, we employ FLATTEN (Cong et al., 2023) and PnP (Tumanyan et al., 2023). For fairness, all T2I baselines are equipped with the same ControlNet conditions. 1https://sites.google.com/view/loveucvpr23/track4 2https://www.istockphoto.com/ and https://www.pexels.com/ 7 Published as conference paper at ICLR 2025 Figure 6: Qualitative comparisons. We refer the reader to our project page for detailed assessment. 4.2 RESULTS We evaluate VideoGrain on videos covering class-level, instance-level, and part-level edits. Our method demonstrates versatility in handling animals, such as transforming wolf into pig (Fig. 5, top left). For instance-level editing, we can modify vehicles separately (e.g., transforming an SUV into firetruck and van into school bus) in Fig. 5, top right. VideoGrain excels at editing multiple instances in complex, occluded scenes, like Spider-Man and Wonder Woman playing badminton (Fig. 5, middle left). Previous methods often struggle with such nonrigid motion. In addition, our method is capable of multi-region editing, where both foreground and background are edited, as shown in the soap-box scene, where the background changes to mossy stone bridge over lake in the forest (Fig. 5, middle right). Thanks to precise attention weight distribution, we can swap identities seamlessly, such as in the jogging scene, where Iron Man and Spider-Man swap identities (Fig. 5, bottom left). For part-level edits, VideoGrain excels in adjusting character to wear Superman suit while keeping sunglasses intact (Fig. 5, bottom right). Overall, for multi-grained editing, our VideoGrain demonstrates outstanding performance. 4.3 QUALITATIVE AND QUANTITATIVE COMPARISONS Qualitative Comparison. Figure 6 shows comparison between VideoGrain and baseline methods, including T2I-based and T2V-based approaches, for instance-level and part-level editing. For (1) Animal instances: In the left fairness, all T2I-based methods use ControlNet conditioning. column, T2I-based methods like FateZero, ControlVideo, and TokenFlow edit both cats into pandas due to same-class feature coupling in diffusion models, failing to perform separate edits. DMT, even with video generation priors, still blends the panda and toy poodle features. In contrast, VideoGrain successfully edits one into panda and the other into toy poodle. (2) Human instances: In the middle column, baselines struggle with same-class feature coupling, partially editing both men into 8 Published as conference paper at ICLR 2025 Automatic Metric CLIP-F CLIP-T Warp-Err Q-edit Method FateZero ControlVideo TokenFlow Ground-A-Video DMT VideoGrain(ours) Table 1: Quantitative comparison of automatic metrics and human evaluation. The best results are bolded. Human Evaluation Edit-Acc Temp-Con Overall 78.6 50.0 50.4 72.0 79.4 85.0 95.75 97.71 96.48 95.17 96.34 98.63 33.78 34.41 34.59 35.09 34.09 36.56 10.96 7.27 12.28 7.92 16.63 25.75 3.08 4.73 2.82 4.43 2.05 1.42 59.8 53.2 45.4 69.0 58.7 88. 59.6 43.6 39.8 63.2 64.5 83.0 Iron Man. DMT and Ground-A-Video also fail to follow user intent, incorrectly editing the left and right instances. VideoGrain, however, correctly transforms the right man into monkey, breaking the human-class limitation. (3) Part-level editing: In the third column, VideoGrain manages partlevel edits, such as sunglasses and boxing gloves. ControlVideo edits the gloves but struggles with sunglasses and motion consistency. TokenFlow and DMT edit the sunglasses but fail to modify the gloves or background. In comparison, VideoGrain achieves both instance-level and part-level edits, significantly outperforming previous methods. Quantitative Comparison. We compare the performance of different methods using both automatic metrics and human evaluation. CLIP-T calculates the average cosine similarity between the input prompt and all video frames, while CLIP-F measures the average cosine similarity between consecutive frames. Additionally, Warp-Err captures pixel-level differences by warping the edited video frames according to the optical flow of the source video, extracted using RAFT-Large (Teed & Deng, 2020). To provide more comprehensive measure of video editing quality, we follow (Cong et al., 2023) and use Q-edit, defined as CLIP-T/Warp-Err. For clarity, we scale all automatic metrics by 100. In terms of human evaluation, we assess three key aspects: Edit-Accuracy (whether each local edit is accurately applied), Temporal Consistency (evaluated by participants for coherence between video frames), and Overall Edit Quality. We invited 20 participants to rate 76 video-text pairs on scale of 20 to 100 across these three criteria, following (Jeong & Ye, 2023). As demonstrated in Table 1, VideoGrain consistently outperforms both T2Iand T2V-based methods. This is primarily due to ST-Layout Attns precise text-to-region control and maintaining feature separation between regions. As result, our method achieves significantly higher CLIP-T and Edit-Accuracy scores compared to other baselines. The improved Warp-Err and Temporal Consistency metrics further indicate that VideoGrain delivers temporally coherent video edits. Efficiency Comparison. To evaluate efficiency, we compared baselines with VideoGrain on single A6000 GPU for editing 16 video frames. The metrics include editing time (time taken to perform one edit) and both GPU and CPU memory usage. From Tab. 2, it is clear our method achieves the fastest editing time with the lowest memory usage, indicating its computational efficiency. FateZero ControlVideo TokenFlow Ground-A-Video DMT VideoGrain Time(min) Memory (GB) RAM (GB) 27.35 16.15 17.84 17.31 27.88 15. 144.22 7.03 5.35 9.96 8.12 4.42 8.68 4.41 4.56 5.81 5.79 3.83 Table 2: Efficiency comparison. Figure 7: Attention weight distribution. 4.4 ABLATION STUDY To assess the contributions of different components in our proposed ST-Layout Attn, we first evaluate whether our attention can achieve attention weight distribution, then decouple the self-attention modulation and cross-attention modulation to evaluate their individual effectiveness. Attention Weight Distribution. We evaluate the impact of ST-Layout Attn on attention weight distribution. As shown in Fig. 7, the target prompt is An Iron Man is playing tennis on snow court. We visualize the cross-attention map for man to assess weight distribution. Without STLayout Attn, feature mixing occurs, with snow weight spilling onto Iron Man. With ST-Layout Attn, the mans weight is correctly distributed. This is because we enhance positive pair scores and suppress negative pairs in both crossand self-attention. This enables precise, separate edits for Iron Man and snow. Additional visualizations are in the Appendix. 9 Published as conference paper at ICLR 2025 Figure 8: Ablation of crossand self-modulation in ST-Layout Attn. Method Baseline Baseline + Cross Modulation Baseline + Cross Modulation + Self Modulation CLIP-F CLIP-T Warp-Err Qedit 8.70 14.26 25. 33.59 36.09 36.56 95.21 96.28 98.63 3.86 2.53 1.42 Table 3: Quantitative ablation of crossand self-modulation in ST-Layout Attn. Cross-Attention Modulation. In Fig. 8 and Tab. 3, we illustrate video editing results under different set up: (1) Baseline (2) Baseline + Cross-Attn Modulation (3) Baseline + Cross-Attn Modulation + Self-Attn Modulation. As shown in Fig. 8 top right, direct editing fails to discriminate between the left and right instances, leading to incorrect (left) or no edits(right). However, when equipped with cross-attention modulation, we achieve accurate text-to-region control, thereby editing left man to Iron Man and right man to Spiderman separately. The quantitative results in Tab. 3 indicate that with cross-attention modulation (second row), CLIP-T increases by 7.4%, and Q-edit increases by 63.9%. This demonstrates the effectiveness of our cross-attention modulation. Self-Attention Modulation. However, modulating only cross-attention still leads to structure distortions, such as the spider web appearing on the left man. This is caused by the coupling of same class-level features (e.g., human). When using our self-attention modulation, the feature mixing is significantly reduced, and the left man retains unique object features. This is achieved by decreasing the negative pair scores between different instances, while increasing positive scores within the same instance. As result, more part-level details, such as the distinctive blue sides, are generated in the optimized areas. The quantitative decrease in Warp-Err by 43.9% and increase in Q-edit by 80.6% in Tab. 3 further prove the effectiveness of self-attention modulation."
        },
        {
            "title": "5 CONCLUSION",
            "content": "In this paper, we aim to solve the problem of multi-grained video editing, which includes both class-level, instance-level and part-level video editing. To the best of our knowledge, this is the first attempt at this task. In this task, we find that the key problem is that the diffusion model views different instances as same-class features and direct global editing will mix different local regions. To wrestle with these problems, we propose VideoGrain to modulate spatial-temporal crossand self-attention for text-to-region control while keeping feature separation between regions. In cross-attention, we enhance each local prompts focus on its corresponding spatial-disentangled region while suppressing attention to irrelevant areas, thereby enabling text-to-region control. In self-attention, we increase intra-region awareness and reduce inter-region interactions to keep feature separation between regions. Extensive experiments demonstrate that our VideoGrain surpasses previous video editing methods on both class-level, instance-level, and part-level video editing. 10 Published as conference paper at ICLR"
        },
        {
            "title": "6 ETHICS STATEMENT",
            "content": "This project aims to solve multi-grained video editing. However, the potential misuse of this technology, such as the creation of deceptive videos by altering identities, poses risk. Strategies like incorporating invisible watermarking could be explored to ensure videos are not used maliciously."
        },
        {
            "title": "REFERENCES",
            "content": "https://www.pika.art/. URL https://www.pika.art/. Omri Avrahami, Dani Lischinski, and Ohad Fried. Blended diffusion for text-driven editing of natural images. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1820818218, 2022. Omri Avrahami, Ohad Fried, and Dani Lischinski. Blended latent diffusion. ACM Transactions on Graphics (TOG), 42(4):111, 2023. Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, Clarence Ng, Ricky Wang, and Aditya Ramesh. Video generation models as world simulators. 2024. URL https://openai.com/research/ video-generation-models-as-world-simulators. Duygu Ceylan, Chun-Hao Huang, and Niloy Mitra. Pix2video: Video editing using image diffusion. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 2320623217, 2023. Wenhao Chai, Xun Guo, Gaoang Wang, and Yan Lu. Stablevideo: Text-driven consistency-aware diffusion video editing. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 2304023050, 2023. Hila Chefer, Yuval Alaluf, Yael Vinker, Lior Wolf, and Daniel Cohen-Or. Attend-and-excite: Attention-based semantic guidance for text-to-image diffusion models. ACM Transactions on Graphics (TOG), 42(4):110, 2023. Yangming Cheng, Liulei Li, Yuanyou Xu, Xiaodi Li, Zongxin Yang, Wenguan Wang, and Yi Yang. Segment and track anything. arXiv preprint arXiv:2305.06558, 2023. Yuren Cong, Mengmeng Xu, Christian Simon, Shoufa Chen, Jiawei Ren, Yanping Xie, Juan-Manuel Perez-Rua, Bodo Rosenhahn, Tao Xiang, and Sen He. Flatten: optical flow-guided attention for consistent text-to-video editing. arXiv preprint arXiv:2310.05922, 2023. Guillaume Couairon, Jakob Verbeek, Holger Schwenk, and Matthieu Cord. Diffedit: Diffusionbased semantic image editing with mask guidance. In ICLR 2023 (Eleventh International Conference on Learning Representations), 2023. Michal Geyer, Omer Bar-Tal, Shai Bagon, and Tali Dekel. Tokenflow: Consistent diffusion features for consistent video editing. arXiv preprint arXiv:2307.10373, 2023. Yuchao Gu, Yipin Zhou, and Mike Zheng et al. Videoswap: Customized video subject swapping with interactive semantic point correspondence. arXiv preprint arXiv:2312.02087, 2023. Yuwei Guo, Ceyuan Yang, Anyi Rao, Yaohui Wang, Yu Qiao, Dahua Lin, and Bo Dai. Animatediff: Animate your personalized text-to-image diffusion models without specific tuning. arXiv preprint arXiv:2307.04725, 2023. Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Prompt-to-prompt image editing with cross attention control. 2022. Hyeonho Jeong and Jong Chul Ye. Ground-a-video: Zero-shot grounded video editing using textto-image diffusion models. arXiv preprint arXiv:2310.01107, 2023. Heng Jia, Yunqiu Xu, Linchao Zhu, Guang Chen, Yufei Wang, and Yi Yang. Mos2: Mixture of scale and shift experts for text-only video captioning. In Proceedings of the 32nd ACM International Conference on Multimedia, pp. 84988507, 2024. 11 Published as conference paper at ICLR Yoni Kasten, Dolev Ofri, Oliver Wang, and Tali Dekel. Layered neural atlases for consistent video editing. ACM Transactions on Graphics (TOG), 40(6):112, 2021. Yunji Kim, Jiyoung Lee, Jin-Hwa Kim, Jung-Woo Ha, and Jun-Yan Zhu. Dense text-to-image generation with attention modulation. In ICCV, 2023. Yuheng Li, Haotian Liu, Qingyang Wu, Fangzhou Mu, Jianwei Yang, Jianfeng Gao, Chunyuan Li, and Yong Jae Lee. Gligen: Open-set grounded text-to-image generation. CVPR, 2023. Shaoteng Liu, Yuechen Zhang, Wenbo Li, Zhe Lin, and Jiaya Jia. Video-p2p: Video editing with cross-attention control. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 85998608, 2024. Yu Lu, Yuanzhi Liang, Linchao Zhu, and Yi Yang. Freelong: Training-free long video generation with spectralblend temporal attention. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. Yue Ma, Xiaodong Cun, Yingqing He, Chenyang Qi, Xintao Wang, Ying Shan, Xiu Li, and Qifeng Chen. Magicstick: Controllable video editing via control handle transformations. arXiv preprint arXiv:2312.03047, 2023. Yue Ma, Yingqing He, Xiaodong Cun, Xintao Wang, Siran Chen, Xiu Li, and Qifeng Chen. Follow your pose: Pose-guided text-to-video generation using pose-free videos. In Proceedings of the AAAI Conference on Artificial Intelligence, pp. 41174125, 2024a. Yue Ma, Hongyu Liu, Hongfa Wang, Heng Pan, Yingqing He, Junkun Yuan, Ailing Zeng, Chengfei Cai, Heung-Yeung Shum, Wei Liu, et al. Follow-your-emoji: Fine-controllable and expressive freestyle portrait animation. arXiv preprint arXiv:2406.01900, 2024b. Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. SDEdit: Guided image synthesis and editing with stochastic differential equations. In International Conference on Learning Representations, 2022. Chong Mou, Mingdeng Cao, Xintao Wang, Zhaoyang Zhang, Ying Shan, and Jian Zhang. Revideo: Remake video with motion and content control. Advances in Neural Information Processing Systems, 37:1848118505, 2025. Hao Ouyang, Qiuyu Wang, Yuxi Xiao, Qingyan Bai, Juntao Zhang, Kecheng Zheng, Xiaowei Zhou, Qifeng Chen, and Yujun Shen. Codef: Content deformation fields for temporally consistent video processing. arXiv preprint arXiv:2308.07926, 2023. Gaurav Parmar, Krishna Kumar Singh, Richard Zhang, Yijun Li, Jingwan Lu, and Jun-Yan Zhu. Zero-shot image-to-image translation. In ACM SIGGRAPH 2023 Conference Proceedings, pp. 111, 2023. Federico Perazzi, Jordi Pont-Tuset, Brian McWilliams, and Van Gool et al. benchmark dataset and evaluation methodology for video object segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 724732, 2016. Quynh Phung, Songwei Ge, and Jia-Bin Huang. Grounded text-to-image synthesis with attention refocusing. arXiv preprint arXiv:2306.05427, 2023. Albert Pumarola, Enric Corona, Gerard Pons-Moll, and Francesc Moreno-Noguer. D-nerf: Neural radiance fields for dynamic scenes. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1031810327, 2021. Chenyang Qi, Xiaodong Cun, Yong Zhang, Chenyang Lei, Xintao Wang, Ying Shan, and Qifeng arXiv preprint Chen. Fatezero: Fusing attentions for zero-shot text-based video editing. arXiv:2303.09535, 2023. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1068410695, 2022. 12 Published as conference paper at ICLR Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In International Conference on Learning Representations, 2021. Luming Tang, Menglin Jia, Qianqian Wang, Cheng Perng Phoo, and Bharath Hariharan. Emergent correspondence from image diffusion. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/forum?id=ypOiXjdfnU. Zachary Teed and Jia Deng. Raft: Recurrent all-pairs field transforms for optical flow. In Computer VisionECCV 2020: 16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part II 16, pp. 402419. Springer, 2020. Narek Tumanyan, Michal Geyer, Shai Bagon, and Tali Dekel. Plug-and-play diffusion features for text-driven image-to-image translation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 19211930, 2023. Jiuniu Wang, Hangjie Yuan, Dayou Chen, Yingya Zhang, Xiang Wang, and Shiwei Zhang. Modelscope text-to-video technical report. arXiv preprint arXiv:2308.06571, 2023a. Kai Wang, Fei Yang, Shiqi Yang, Muhammad Atif Butt, and Joost van de Weijer. Dynamic prompt learning: Addressing cross-attention leakage for text-based image editing. arXiv preprint arXiv:2309.15664, 2023b. Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Weixian Lei, Yuchao Gu, Wynne Hsu, Ying Shan, Xiaohu Qie, and Mike Zheng Shou. Tune-a-video: One-shot tuning of image diffusion models for text-to-video generation. arXiv preprint arXiv:2212.11565, 2022. Shuai Yang, Yifan Zhou, Ziwei Liu, , and Chen Change Loy. Rerender video: Zero-shot textguided video-to-video translation. In ACM SIGGRAPH Asia Conference Proceedings, 2023. Xiangpeng Yang, Linchao Zhu, Hehe Fan, and Yi Yang. Eva: Zero-shot accurate attributes and multi-object video editing. arXiv preprint arXiv:2403.16111, 2024a. Xiangpeng Yang, Linchao Zhu, Xiaohan Wang, and Yi Yang. Dgl: Dynamic global-local prompt tuning for text-video retrieval. In Proceedings of the AAAI Conference on Artificial Intelligence, pp. 65406548, 2024b. Yiyuan Yang, Guodong Long, Michael Blumenstein, Xiubo Geng, Chongyang Tao, Tao Shen, and Daxin Jiang. Pre-training cross-modal retrieval by expansive lexicon-patch alignment. In LRECCOLING 2024, pp. 1297712987, 2024c. Yiyuan Yang, Guodong Long, Tao Shen, Jing Jiang, and Michael Blumenstein. Dual-personalizing adapter for federated foundation models. arXiv preprint arXiv:2403.19211, 2024d. Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024e. Danah Yatim, Rafail Fridman, Omer Bar-Tal, Yoni Kasten, and Tali Dekel. Space-time diffusion features for zero-shot text-driven motion transfer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 84668476, 2024. Lijun Yu, Yong Cheng, Kihyuk Sohn, Jose Lezama, Han Zhang, Huiwen Chang, Alexander Hauptmann, Ming-Hsuan Yang, Yuan Hao, Irfan Essa, et al. Magvit: Masked generative video transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1045910469, 2023. Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pp. 38363847, October 2023a. Yabo Zhang, Yuxiang Wei, Dongsheng Jiang, Xiaopeng Zhang, Wangmeng Zuo, and Qi Tian. Controlvideo: Training-free controllable text-to-video generation. arXiv preprint arXiv:2305.13077, 2023b. 13 Published as conference paper at ICLR 2025 Different from multi-modal learning (Yang et al., 2024b;c;d; Jia et al., 2024), controllable video generation (Ma et al., 2024b;a; Lu et al., 2024) or video editing (Yang et al., 2024a; Ma et al., 2023) requires explicit control signals. Multi-grained editing further relies on additional layout conditions to edit in the class, instance, or part level. Therefore, in the appendix, we first evaluate the SAMTrack masks impact in Section A, then validate whether our method can work without SAM-Track masks in Section B. Continually, we show that our method can solely edit specific subjects in Section and part-level modification example in Section D. We also evaluate our ST-Layout Attns temporal focus in Section and ControlNets effect in Section F. EVALUATE SAM-TRACK MASKS IMPACT Figure 9: VideoP2P joint and sequential edit with SAM-Track masks To evaluate the impact of using SAM-Track (Cheng et al., 2023) for instance segmentation, we compare our VideoGrain against VideoP2P (Liu et al., 2024), which is equipped with SAM-Track instance masks. The instance masks replace cross-attention masks during editing. 16-frame oneshot tuning is performed, and ControlNet conditioning Zhang et al. (2023a) is added for fairness. 14 Published as conference paper at ICLR 2025 Two experiments are tested: (1) jointly editing multiple areas in single denoising process and (2) sequentially editing three areas by inputting separate masks. Results show that joint editing (Fig. 9(1)) modifies only left man into Spiderman, leaving other areas unchanged due to inaccurate cross-attn weight distribution. Sequential editing (Fig. 9(2)) succeeds initially but fails later due to error accumulation in denoising, resulting in blurred details. Figure 10: Ground-A-Video joint edit with instance information Additionally, as shown in figure above 10, also in Figs 2 and 6, Ground-A-Video (Jeong & Ye, 2023) struggles with multi-grained video editing tasks, even with instance-level grounding information (e.g., text-to-bounding box), which is comparable to SAM-Tracks masks. These comparisons indicate that while SAM-Track provides layout guidance, it does not guarantee successful edits. In contrast, our method enables zero-shot multi-grained editing, which was not achievable by any previous methods, even when providing existing SOTA with SAM-Track masks. VIDEOGRAIN CAN WORK WITHOUT SAM-TRACK MASKS Figure 11: Our method without additional SAM-Track masks Our method is not strictly dependent on SAM-Track masks. As shown in Fig.11(3), we can cluster DDIM inversion self-attention features to get inaccurate coarse layouts. Our method still achieves high-quality multi-area editing results (4). In contrast, even with precise groundings (converted from SAM-Track masks in (1)), Ground-A-Video fails to edit all three regions. These comparisons indicate that our method does not rely on SAM-Track segmentation. Instead, it works effectively only using the self-attention feature inside the diffusion model, even without accurate layout guidance. SOLELY EDIT ON SPECIFIC SUBJECTS, WITHOUT BACKGROUND CHANGED Our method is designed for multi-area editing and can naturally perform background-preserved subject editing, as it treats multi-area editing as selecting regions restricted to the foreground. As 15 Published as conference paper at ICLR 2025 Figure 12: Soely edit on specific subjects, without background changed shown in Fig 12, our method can separately edit the left man and right man or jointly edit both subjects while keeping the background unchanged. PART-LEVEL MODIFICATION EXAMPLES Figure 13: Part-level modifications on humans and animals Our part-level editing supports not only adding objects but also part-level attribute modifications. In the human case (Fig. 13 left), our method changes the color of gray shirt to blue (second row) and edits half-sleeve shirt into black suit (third row), showcasing part-level attribute and structure editing. Similarly, in the animal case, our method can change cats head or body color from black to ginger while preserving the belts color, demonstrating precise part-level modifications. TEMPORAL FOCUS OF ST-LAYOUT ATTN Our ST-Layout Attn is designed as full-frame approach to ensure inter-frame consistency. As shown in Fig. 14, per-frame ST-Layout Attn causes feature coupling on Iron Man, while the sparsecausal method results in flickering and misses Spider Mans blue details due to their limited receptive fields for positive/negative value selection across different layouts. In contrast, our ST-Layout Attn 16 Published as conference paper at ICLR 2025 Figure 14: Temporal Focus of ST-Layout Attn effectively preserves texture details and prevents flickering, achieving temporal consistent and layout unified multi-grained video editing."
        },
        {
            "title": "F CONTROLNET ABLATION",
            "content": "Figure 15: ControlNet ablation Our method utilizes ControlNet depth/pose conditioning in certain complex motion cases to provide necessary structural guidance. As shown in Fig. 15, even without ControlNet, our method can still achieve simultaneous multi-region editing. However, in such cases, there may be some structural inconsistencies between the edit object and source object due to the lack of explicit structure guidance."
        },
        {
            "title": "I LATENT BLEND",
            "content": "To preserve areas not intended for editing (i.e., τ3 in τ = {τ1τ1, τ2τ2, τ3τ3, }), we employ Latent Blend (Avrahami et al., 2022; 2023), which leverages masks to direct the model focus on areas requiring editing while keeping the background region identical to the source video. For each frame in the video, we first merge each attribute mask to form the global foreground mask Mi by applying the logical OR operation across all layouts masks mi,k = [mi,1, mi,2, , mi,k] : Mi = mi,1 mi,2 mi,k. (6) We aggregate the masks Mi from all frames to obtain combined mask , and then blend the latent states zt at each timestep during the denoising process as follows: zt = (1 M) zt + zt, (7) where zt indicates the latent feature in the DDIM inversion process and zt is corresponding latent feature during the DDIM denoising process. 17 Published as conference paper at ICLR 2025 Figure 16: More general objects instance editing (animals) and shape editing (cars) results. The key behind employing Latent Blend for preserving the background is that, given desired area mask, the less noisy foreground latent can be guided by the target text prompt τ . Meanwhile, the latent features outside the mask (the background) can be preserved. This blending ensures that, even if the latent feature within the edit area is modified, the background features stay consistent."
        },
        {
            "title": "J EXPERIMENTAL DETAILS",
            "content": "For FateZero3 (Qi et al., 2023), we employ prompt-to-prompt(Hertz et al., 2022) replace editing. To enhance the identity binding of the edited object, we set the self/cross replacement steps at 0.3 and 3https://github.com/ChenyangQiQi/FateZero 18 Published as conference paper at ICLR 2025 Figure 17: More frames ablation of ST-Layout Attns effects on attention weight distribution. the blending threshold at 0.7. In TokenFlow4 (Geyer et al., 2023), we utilize SD editing and default to 4 keyframes for 16-frame videos. For other comparative methods like ControlVideo5 (Zhang et al., 2023b) and Ground-A-Video6 (Jeong & Ye, 2023) and DMT7 (Yatim et al., 2024), we adhere to their default hyperparameter settings. To ensure fairness across all T2I-based methods compared, we re-implement ControlNet (Zhang et al., 2023a) on their codebases. LIMITATIONS. First, although our method can achieve multi-grained editing of video, the generation quality is still limited by the base model since we are training-free method. In scenarios where the generation prior to SD is not ideal, artifacts may occur in the editing results. Second, since our method is based on T2I model, it struggles with large shape deformations and significant appearance changes. This limitation is inherent in zero-shot methods. potential future direction is to incorporate motion priors from T2V generation models (Yang et al., 2024e) to handle such challenges. 4https://github.com/omerbt/TokenFlow 5https://github.com/YBYBZhang/ControlVideo 6https://github.com/Ground-A-Video/Ground-A-Video 7https://github.com/diffusion-motion-transfer/diffusion-motion-transfer"
        }
    ],
    "affiliations": [
        "ReLER Lab, AAII, University of Technology Sydney",
        "ReLER Lab, CCAI, Zhejiang University"
    ]
}