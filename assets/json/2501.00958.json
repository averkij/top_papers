{
    "paper_title": "2.5 Years in Class: A Multimodal Textbook for Vision-Language Pretraining",
    "authors": [
        "Wenqi Zhang",
        "Hang Zhang",
        "Xin Li",
        "Jiashuo Sun",
        "Yongliang Shen",
        "Weiming Lu",
        "Deli Zhao",
        "Yueting Zhuang",
        "Lidong Bing"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Compared to image-text pair data, interleaved corpora enable Vision-Language Models (VLMs) to understand the world more naturally like humans. However, such existing datasets are crawled from webpage, facing challenges like low knowledge density, loose image-text relations, and poor logical coherence between images. On the other hand, the internet hosts vast instructional videos (e.g., online geometry courses) that are widely used by humans to learn foundational subjects, yet these valuable resources remain underexplored in VLM training. In this paper, we introduce a high-quality \\textbf{multimodal textbook} corpus with richer foundational knowledge for VLM pretraining. It collects over 2.5 years of instructional videos, totaling 22,000 class hours. We first use an LLM-proposed taxonomy to systematically gather instructional videos. Then we progressively extract and refine visual (keyframes), audio (ASR), and textual knowledge (OCR) from the videos, and organize as an image-text interleaved corpus based on temporal order. Compared to its counterparts, our video-centric textbook offers more coherent context, richer knowledge, and better image-text alignment. Experiments demonstrate its superb pretraining performance, particularly in knowledge- and reasoning-intensive tasks like ScienceQA and MathVista. Moreover, VLMs pre-trained on our textbook exhibit outstanding interleaved context awareness, leveraging visual and textual cues in their few-shot context for task solving~\\footnote{Our code are available at \\url{https://github.com/DAMO-NLP-SG/multimodal_textbook}}."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 ] . [ 1 8 5 9 0 0 . 1 0 5 2 : r Wenqi Zhang1 * Hang Zhang2 Xin Li2, Jiashuo Sun2 Yongliang Shen1 Weiming Lu1, Deli Zhao2 Yueting Zhuang1 Lidong Bing2 1College of Computer Science and Technology, Zhejiang University Project: https://multimodal-textbook.github.io/ 2DAMO Academy, Alibaba Group"
        },
        {
            "title": "Abstract",
            "content": "1. Introduction Compared to image-text pair data, interleaved corpora enable Vision-Language Models (VLMs) to understand the world more naturally like humans. However, such existing datasets are crawled from webpage, facing challenges like low knowledge density, loose image-text relations, and poor logical coherence between images. On the other hand, the internet hosts vast instructional videos (e.g., online geometry courses) that are widely used by humans to learn foundational subjects, yet these valuable resources remain In this paper, we introunderexplored in VLM training. duce high-quality multimodal textbook corpus with richer foundational knowledge for VLM pretraining. It collects over 2.5 years of instructional videos, totaling 22,000 class hours. We first use an LLM-proposed taxonomy to systematically gather instructional videos. Then we progressively extract and refine visual (keyframes), audio (ASR), and textual knowledge (OCR) from the videos, and organize as an image-text interleaved corpus based on temporal order. Compared to its counterparts, our video-centric textbook offers more coherent context, richer knowledge, and better image-text alignment. Experiments demonstrate its superb pretraining performance, particularly in knowledgeand reasoning-intensive tasks like ScienceQA and MathVista. Moreover, VLMs pre-trained on our textbook exhibit outstanding interleaved context awareness, leveraging visual and textual cues in their few-shot context for task solving 1. *This work was conducted when Wenqi Zhang was interning at Alibaba DAMO Academy. Corresponding author. 1Our code are available at https://github.com/DAMO-NLPSG/multimodal_textbook 1 Vision-Language Models (VLMs) have demonstrated impressive development recently, delivering exceptional performance across variety of visual tasks, including image captioning, dialogue, and visual question answering [1, 3, 8, 12, 15, 19, 2527, 38, 39, 43, 51]. These advancements can be primarily attributed to the swift improvements of large language models (LLMs) and the communitys ongoing creation of diverse, high-quality multimodal training corpora [4, 6, 7, 13, 14, 36], collectively driving VLMs forward. multimodal corpus typically consists of numerous image-text pairs to align images with textual descriptions. Pretraining on such paired datasets allows LLMs to be efficiently adapted into VLMs, with the ability to perceive and interpret visual information. Beyond image-text pair data, previous researchers have also introduced image-text interleaved corpus as more natural and flexible multimodal corpus [2, 17, 20, 30, 52]. These corpora, consisting of sequences of text paragraphs interspersed with images, are typically crawled from webpage and document, such as Common Crawl. Pretraining on combination of interleaved corpus and image-pair datasets enables VLMs to handle interwoven multi-modal inputs, while also unlocking advanced capabilities such as in-context learning [22] and multi-image comparison [14]. Despite their benefits to multi-modal pre-training, existing interleaved datasets still suffer from the following issues (shown in Fig. 1): (1) Loose text-image relation: The associations between images and text in webpage are often loose and may even include irrelevant images, e.g., logos or advertisements. (2) Lack of logical coherence in image sequences: most webpages contain relatively few images, and more importantly, the logical relations between images are often vague, making it difficult to learn complex visual reasoning. (3) Low knowledge density: crawled webpages inevitably include content such as news, entertainment, and Figure 1. Previous interleaved datasets, e.g., MMC4 and OBELICS, suffer from limitations like weak text-image relations, low knowledge density, and incoherent image sequences. Our multimodal textbook, sourced from massive tutorial videos, employs coarse-to-fine knowledge extraction and multi-level filtering to create high-quality, textbook-level dataset. It interleaves video keyframes with tutorial texts (extracted from ASR and OCR), enabling VLMs to acquire rich knowledge through tightly coupled text-image and more coherent logic. advertisement recommendations, with little involvement of fundamental knowledge. These issues may severely affect the learning effectiveness of interleaved corpora. Therefore, exploring how to extract high-quality, textbook-level interleaved datasets from vast internet data is quite necessary. On the other hand, the internet contains vast array of instructional videos [31, 34, 49], e.g., online mathematics courses on YouTube, where people often turn to acquire both foundational knowledge and specialized skills. Most videos contain frame-by-frame demonstrations along with detailed verbal explanations by the instructor, making them an ideal source of training data. However, these valuable resources have received limited attention for VLM training. In this paper, we introduce multimodal Textbook: high-quality pre-training corpus that encompasses wealth of foundational knowledge. Our textbook is constructed from 2.5 years of instructional videos, amounting to 22,000 class hours, covering six fundamental subjects, including mathematics, physics, and others. The whole corpus is presented in an image-text interleaved format, where the text and images are more closely aligned, and the logical relations between images are also more coherent. To create our textbook, we develop an LLM-powered pipeline to systematically collect vast array of instructional videos from the internet. To achieve automation, we prompt LLMs to construct knowledge taxonomy covering six subjects and 3900 knowledge points. Then based on this, we gather relevant instructional videos. After that, we design multi-level, coarse-to-fine knowledge extraction and data filtering pipeline for these collected videos. From visual perspective, we extract keyframes and recognition text, symbols, and formulas (OCR). From an auditory perspective, we perform automatic speech recognition (ASR) on the instructors verbal explanations and refine their quality. Finally, the keyframes and tutorial text are organized into an interleaved format, sequenced chronologically. Our textbook is an openly accessible pre-training dataset with high-quality 6.5 million images interleaving with 0.75 billion texts. It drawn from 75,000 extensive instructional videos, totoaling over 22000 class hours, covering multiple core subjects such as mathematics, physics, chemistry. As demonstrated in Fig. 1, our textbook (the first example) presents three keyframes interleaved with four tutorial texts to dynamically illustrate the geometric concept of comple2 mentary angles. These more coherent interleaved context and better-aligned image-text sequences enable VLMs to better grasp foundational knowledge during the pretraining. Experiments show that VLMs pre-trained on our textbook achieve noticeable improvement on knowledgeand reasoning-intensive benchmarks, like MathVista, and ScienceQA. Besides, we also observe some intriguing findings: our textbook can significantly enhance the interleaved context awareness of VLMs, i.e., pretrained on our textbook, VLMs can more effectively attend to their few-shot context, leveraging visual or textual cues for question-solving. In contrast, the VLMs training on other datasets often overlooked their interleaved context. 2. Related Works 2.1. Vision Language Models the large development language models of With (LLMs) [32, 40, 45], VLMs have evolved from these task-specific, closed-set models [18, 33] to more flexible systems capable of handling open-world scenarios. Large VLMs adopt general paradigm of mapping pretrained visual encoder outputs to the embedding space of LLMs, enabling cross-modal understanding [19, 26]. By leveraging large-scale caption datasets [35, 41] and meticulously crafted instruction-following data [10, 26], these models exhibit remarkable capabilities. Building on this foundation, researchers have further boosted VLM performance by diversifying instruction data [45, 48], refining data quality [11, 22], and increasing image resolution [8, 47]. These improvements have led to breakthroughs across OCR, VQA, and visual grounding tasks, with VLMs now achieving impressive results on benchmarks that demand precise, context-aware understanding [8, 22, 25]. 2.2. Multi-modal Pretraining Data Recent developments in Vision-Language Models have typically involved two-stage process: pretraining followed by high-quality instruction-following phase [5, 8, 9, 23, 24, 43, 47, 50]. Most VLMs utilize paired image-caption datasets [35, 36, 41] for pretraining which facilitate quick alignment between image and text spaces [8, 23, 47]. However, image-caption datasets lack the naturalness and authenticity found in more comprehensive text corpora used for LLMs, as they are often limited in diversity and complexity [22]. This limitation reduces VLMs capacity for in-context learning and chain-of-thought (CoT) reasoning. Recognizing this gap, some researchers have introduced webpage-centric interleaved datasets, like MMC4 [52] and OBELICS [17], sourced from webpages and documents [2, 3]. These interleaved datasets can enhance in-context learning capabilities in VLMs [22, 42]. However, these datasets still face issues such as low image-text relevance, poor se3 quence logic, and sparse knowledge density. Our work proposes multimodal textbook corpus curated from instructional videos, intending to enhance multimodal pretraining and expand the models ability to handle interleaved visual and textual inputs. 3. Curation of Multimodal Textbook Our goal is to construct textbook-level interleaved corpus that delivers high-quality, specialized knowledge for pretraining VLMs in more natural and efficient manner. To achieve this, we choose online instructional videos as the primary data source. Compared to common videos, such as entertainment, sports, or TV-show, instructional videos exhibit greater textual-visual consistency and sequential frame coherence, making them ideal for creating multimodal textbook. While these videos are generally reliable, they still contain significant noise and redundancy, such as unrelated segments (e.g., advertisements), mismatches between visual content and text (e.g., almost static scene predominantly featuring single lecturer), or redundant scenes. To address this, we employ multi-level pipeline (video-level, clip-level, and keyframe-level) with coarse-to-fine strategy. The curation process is outlined in Fig. 2. 3.1. Collecting Instructional Videos LLM-proposed Knowledge Taxonomy. In this work, we propose knowledge taxonomy with four hierarchical layers for the desired instructional videos, namely Subject Course Sub-course Knowledge Point. To guarantee broad coverage of instructional videos, we instruct an LLM to span the proposed knowledge taxonomy so that multiple educational stages (from primary school to middle school) and diverse subjects (mathematics, physics, etc.) will be involved. Eventually, as shown in Sec. 7.4, we obtain knowledge taxonomy comprising 6 subjects (mathematics, physics, chemistry, earth science, engineering, and computer science), 55 courses (Algebra, Solid Geometry,..), and 3915 knowledge points. For example in the mathematics: Mathematics Elementary Mathematics Rational and Irrational Numbers the definition of Irrational Numbers. Taxonomy-based Video Collection and Filtering. Each knowledge point in the taxonomy is then used as keyword to retrieve relevant instructional videos via YouTubes search API2. We retain the top 50 videos for each knowledge point. Then, we perform deduplication based on video IDs and filter the low-quality videos using their metadata: we prompt LLMs to review each videos metadataincluding the title, description, and commentsto exclude irrelevant, pornographic, or illegal content. Lastly, we collect total of 159,565 videos from YouTube. 2https://www.youtube.com/ Figure 2. An illustration of constructing multimodal textbook from instructional videos. We first instruct LLMs to construct knowledge taxonomy, then retrieve and filter videos at metadata level, collecting 159K instructional videos. Then video-to-textbook pipeline is designed for multi-level knowledge extraction. ① We filter out non-instructional videos using ASR transcripts, retaining 75K high-quality videos. ② We use ASRs timestamp to segment long videos into short clips, discarding those with misaligned visuals and ASR. ③ We detect keyframes from each clip and extract text and symbols by OCR. Our pipeline produces 6.5M keyframes, 259M ASR, and 500M OCR tokens and organizes them into an image-text interleaved textbook. 3.2. Video-to-Textbook Pipeline For an instructional video, both the visual content (e.g., slide or animation) and the auditory content (e.g., instructors narration) contain valuable knowledge. Therefore, we design multi-level extraction pipeline to gather instructional keyframes and text from raw videos, interleaving them into textbook. Video-Level Extraction: Video-to-ASR. We employ FFmpeg3 to extract the audio from each video (videoto-audio) and then transcribe it into text (audio-to-text, ASR) using whisper-large-v34. These transcriptions contain substantial knowledge and reasoning details, such as the instructors explanations of on-screen content and step-by-step derivations of specific mathematical concepts. However, due to the nature of tutorial speech where the instructors prefer to use colloquial expressions to explain concept, the perplexities (PPLs) of the raw ASR transcriptions are usually much higher than those of the texts from standard corpora (see Tab. 6 for the concrete numbers). Therefore, we further introduce Qwen2-72B-Instruct [45] to rewrite the raw ASR transcriptions, with the purpose of improving their fluency and coherence while not changing the original semantics Video-Level Filtering: Low-quality Videos based on 3https://www.ffmpeg.org/ 4https://huggingface.co/openai/whisper-large-v3 ASR. We first filter the videos using set of predefined rules, including non-English videos, videos shorter than 10 seconds, and silent videos with very few ASR text tokens. Next, we assess the remaining videos by instructing an LLM to review their ASR transcriptions and filter out the non-instructional videos in terms of the following criteria: Relevance: The ASR represents the tutorial content of the video. We assess the alignment between the ASR and the targeted knowledge point, filtering out irrelevant videos, e.g., advertisements or entertainment videos. Knowledge Density: We evaluate the knowledge involved in ASR, as many videos contain meaningless filler phrases like um, the next up is this, or then we get this. Such videos fail to provide valuable textual knowledge and are therefore discarded. Transcription Quality: We examine the transcription quality by whisper, excluding repetitive or erroneous ASR text. This step occurs before ASR rewriting. After LLM evaluation across these three dimensions, the retained 75,000 videos are generally of high quality, as verified by their ASR transcriptions. Clip-Level Extraction: Long Video-to-Short Clips. To achieve temporal alignment between text and frames, we use the timestamps of each ASR transcription to segment the long video into multiple video clips. However, it is essential to consider that the original ASR tran4 Dataset #Image Min. Max. Avg. Min. #Text Token Max. Avg. L=4 L=5 L=6 L=7 L= Avg. In-sample Image SIML Source Common Crawl 1 1 Image-text Paired Dataset 1 COYO-700M 1 LAION-5B Image-text Interleaved Dataset MMC4 MMC4-core-ff OBELICS OmniCorpus 0 0 1 117 15 30 16 1 1 5.7 4.1 2.5 3.9 Ours 2 10.7 1 6 4 15 12 14 11 811 683 16715 16715 10717 16 27 417 329 816 574 - - 0.363 0.431 0.366 0.358 - - 0.348 0.406 0.351 0. - - 0.310 0.404 0.339 0.310 - - 0.298 0.403 0.337 0.305 - - 0.276 0.396 0.336 0. - - Common Crawl Common Crawl 0.319 0.407 0.345 0.321 Common Crawl Common Crawl Common Crawl Multi-sources 34174 0.687 0.697 0.698 0.688 0.662 0. Video Website Table 1. We compare our multimodal textbook with image-text paired datasets and webpage-centric interleaved datasets in terms of image and text distributions. In-sample Image SIML measures the semantic and structural correlation between multiple images within an interleaved sample. OmniCorpus: Due to the extensive size of the dataset, we perform statistical analysis on randomly sampled subset. scriptions are often fragmented. First, we merge multiple incomplete ASR segments into single, semantically coherent paragraph. Then, we use their timestamps to segment the video clips accordingly. Each clip lasts 10 to 20 seconds, accompanying an ASR text segment: clip1, asr1 , clip2, asr2 , . . . , clipn, asrn Clip-Level Filtering: Video Clips without Visual Knowledge. Previous filtering of long videos is based on ASR text. Next, we also assess each video clip from visual perspective to determine if it contains sufficient visual knowledge. In most videos, it is inevitable to contain uninformative scenes, such as transitions, shots focused solely on the speaker, or cluttered backgrounds, which are not suitable for multimodal textbook. good scene should contain slides, blackboards, or demonstrative animations that introduce knowledge concept or illustrate specific objects, rather than just the speaker alone. To this end, we employ VideoLlama2 [10] to generate detailed caption for each video clip. We then calculate the text similarity between the clips caption and ASR transcription using the text embeddings model (gte-Qwen2-7B-instruct [21]), filtering out uninformative video clips. Notably, even if an uninformative video clip is discarded, its ASR transcription may still contain valuable information. Thus, we retain these transcriptions in our textbook: clip1, asr1 , asr2, asr3, clip4, asr4 , . . . , clipn, asrn Keyframe-Level Extraction: Clip-to-Keyframes by Comparing Changes between Consecutive Two Frames. Then we need to extract keyframes from each video clip, removing similar or even duplicate shots. frame is identified as keyframe if it exhibits significant visual change compared to the previous one. Therefore, we compute the similarity between consecutive frames and filter out those with minimal scene changes. Considering efficiency and accuracy, we employ the Structural Similarity Index algorithm (SSIM) [44] to compare the similarity between consecutive frames iteratively. Starting from the first frame, we calculate the similarIf the similarity is quite ity with the subsequent frame. 5 high, we skip to the next until frame with significant change is found. We then use this frame as new reference point and continue to seek subsequent frames with notable differences. The detailed process is provided in Algorithm 1. The keyframe-ASR sequence is as follows: 1 , asr1, asr2, asr3, framek1 framek1 1 , framek2 4 , asr4, . . . Keyframe-Level Extraction: Keyframe-to-OCR. Last but not least, most instructional videos often use bulletpointed text, formulas, and mathematical symbols to illustrate knowledge points, physical concepts, and calculation processes. These texts, symbols, and mathematical formulas encapsulate substantial knowledge. Therefore, we extract these texts from keyframes as the ASRs supplement. Specifically, we employ two advanced VLMs (InternVL240B [9]) to perform optical character recognition (OCR) on each keyframe, extracting on-screen text, mathematical symbols, formulas, and other elements. Keyframe-Level Filtering: Uninformative Keyframe and Redundant OCR. Despite filtering visual content at multiple levels, some keyframes may still contain low informational scenes, e.g., occlusion. Therefore, we also utilize InternVL2 to score each keyframe after conducting OCR. Additionally, we do not retain all OCR texts, as the OCR from consecutive keyframes is likely to be highly similar or even identical. Therefore, we filter out OCR results that are similar to previous ones. Lastly, as shown in Fig. 2, through our multi-level extracting and filtering, we curate high-quality video keyframes, OCR text, and ASR transcriptions. These elements represent the useful visual content in videos and the instructors in-depth explanation of knowledge points. To create the pretraining dataset, we interleave the selected keyframes of long video with refined ASR and OCR text in chronological order, creating our multimodal textbook: 1 , ocr1, asr1, asr2, asr3,framek1 {framek1 4 , ocr4, asr4, ..} 1 , framek2 4. Analysis of Multimodal Textbook and contextually related images. 4.1. General statistics We utilize GPT-4o to synthesize our knowledge taxonomy with 3915 knowledge points across 6 subjects, which enabled us to automatically collect 159K English instructional videos based on this taxonomy. Following our video-totextbook pipeline, we filter 53% low-quality or repetitive videos and retain 75K videos (22,697 class hours) with an average duration of 18 minutes. Then we extract 6.5M keyframes and 0.75B text (ASR+OCR) tokens from these videos. To enhance training efficiency, we concatenate multiple framek1 , ocri, asri fragment into sini gle sample, producing total of 610K interleaved samples. Each sample contains an average of 10.7 keyframes and 1,297 text tokens. The detailed statistics for each subject are shown in Appendix (Tab. 7). Besides, we randomly select 100 videos and corresponding samples for manual evaluation, with detailed results presented in Sec. 7.2. , .., framekn 4.2. Comparison with Existing Datasets Image and Text Distribution. To better demonstrate the advantages of our video-centric dataset, we compare our multimodal textbook with existing datasets (image-text paired datasets and webpage-centric datasets), focusing on the distribution of images and tokens across these datasets. As shown in Tab. 1, we observe that our dataset exceeds previous datasets in terms of the average number of images and text tokens. For instance, our dataset contains an average of 10.7 images per sample, compared to only 5.7 in MMC4 and 4.1 in OBELICS. Images within Sample are More Closely Related. notable feature of our video-centric design is the inherent association between multiple images within sample, providing dynamic illustration of mathematical concepts or physical phenomena. To validate this, we design an insample image similarity metric (InSI-SIM). It measures the similarity between all images within sample, i.e., calculating the average of all pairwise similarity of sample. For similarity, we consider both semantic (CLIP score) and structural similarity (SSIM score) respectively. The detailed formula is presented in Sec. 7.5. As shown in Tab. 1, we report InSI-SIM for 8 imagesubset (i.e., the subset containing 4 images) to 8 imagesubset (L: 4 to 8). For all subsets, our multimodal textbook achieves significantly higher InSI-SIM score than other datasets, nearly more than double. For example, our textbook scores 0.686 on average, while OBELICS reaches only 0.345. Besides, we also observed that, as the number of images per sample increases, the InSI-SIM of our dataset remains stable at around 0.68, whereas other datasets experience noticeable decline (about 10%). This further validates that our video-centric dataset provides more coherent 5. Experiments 5.1. Experimental Settings Baselines. We first employ LLaVA-1.5-7B [25] as base models to study the pretraining performance on our dataset and reference datasets (MMC4, OBELICS). For LLaVA1.5-7B, we apply continual pretraining on its pre-trained model (aligned using 558K paired data). To investigate our dataset more comprehensively, we also pre-train Idefics28B model [16] on our dataset, which is an advanced VLM that already supports multi-image and interleaved format input. For the Idefics2-8B, we design two pretraining settings: 1. Training from scratch using the architecture of Idefics2-8B (i.e., Idefics2-8B with randomly initialized projector) and 2. Continual pretraining from the Idefics2-8Bbase which is already pre-trained on OBELICS. For fair comparison, we sample an equivalent number of samples (610K) from MMC4 and OBELICS and apply the same training parameters across all datasets. Evaluation Methods. Following OpenFlamingo [3] and OmniCorpus [20], we evaluate the performance the pre-trained models on two VQA benchmarks of (TextVQA [37], OKVQA [29]), three visual reasoning benchmarks (MathVista, MathVision, MathVision), and ScienceQA-IMG [28], covering general, OCR, mathematics, and science domains. We compute model accuracy in few-shot settings using either randomly sampled or retrieved examples as previous works [16, 20, 46]. 5.2. Main Results As shown in Tabs. 2 and 3, after being pretrained on our Textbook-6.5M, both LLaVA-1.5 and Idefics-8B exhibit significant improvements across seven benchmarks, achieving average gains of +3.2%, +8.3%, +4.0%, and +4.6% in the 0-shot to 4-shot settings, respectively. Notably, even for cutting-edge VLMs like Idefics2, our multimodal textbook brings an additional improvement of +1.4%, underscoring rich knowledge content and its high data quality. Our Textbook Brings Improvement on Knowledgeoriented and Reasoning Benchmarks. In Tab. 2, we observe that our textbook dataset delivers notably greater improvements on knowledge-oriented and reasoning-related benchmarks compared to counterpart datasets. For instance, on ScienceQA, our dataset achieves over 20% improvement in both zero-shot and few-shot settings compared to MMC4. Similarly, on math-related benchmarks such as MathVista, which require both mathematical knowledge and visual reasoning capabilities, our dataset demonstrates an average improvement of +5.3% and +6.4% compared to OBELICS. This improvement highlights the high quality of our textbook, which distills extensive knowledge from in6 #Shot 0 2 4 0 1 2 0 1 2 4 0 - - - Dataset MMC4 MMC4-Core-ff OBELICS Textbook-6.5M 26.3 Dataset 20.4 MMC4 22.5 MMC4-Core-ff OBELICS 21.6 Textbook-6.5M 24.3 ScienceQAIMG 1.6 2.1 2.8 29.4 3.9 10.1 3.0 25.1 MathVista 27.9 30 29.2 33.0 31.1 28.5 33.2 43.4 11.6 10.2 16.4 37.3 26 27.8 27.6 29. 8.6 11.8 13.0 10.2 12.2 13.7 13.4 14.5 OKVQA 21.5 23.6 25.3 21.2 31.7 35.7 36.8 31.2 MathVision 15.5 21.3 16.3 23.4 16.8 20.1 18.2 25.6 28.7 30.4 37.5 39.9 16.1 17.7 14.9 18. 12.1 13.6 9.2 11.8 8.6 8.6 6.9 7.7 TextVQA 16.8 16.2 18.8 18.7 30.2 26.5 26.7 32.1 MathVerse 21.2 19.4 21.8 19.9 20.7 19.4 28.5 19.8 20.9 22.1 32.2 33.5 15.9 15.2 14 14. 14.5 16.1 11 14.1 10.9 12.3 10.7 15.5 1 2 TextVQAocr 29.9 23.9 28.7 26.6 36.3 30.7 36.4 33.1 Avg. 19.4 20.7 22.8 31.1 19.5 21.4 24.8 28. 4 34.7 33.1 41 42.8 21.9 22.3 26.2 30.8 Table 2. We continued pre-training the base model of LLaVA-1.5-7B using different interleaved datasets. The results are evaluated on 4 common VQA and 3 math-related benchmarks under few-shot settings. Dataset MMC4-cf OBELICS Textbook-6.5M Continual Pre-training from Idefics2-8B-base OKVQA TextVQA MathVista MathVison MathVerse Pre-training Idefics2-8B from scratch OKVQA TextVQA MathVista MathVison MathVerse 54.1 54.6 55.1 57.7 57.5 58.2 27.8 27.6 29.7 14.0 14.3 16. 17.3 17.5 19.4 9.4 10.5 10.1 25.1 25.7 26.8 24 24.2 26.1 13.3 13.6 14.4 18.3 17.7 19. Table 3. Except for LLaVA, we also pre-train advanced VLMs with multi-image ability (Idefics): continual pretraining from Idefics-8Bbase or pre-training from scratch. The evaluations are extended to an 8-shot using randomly selected examples as previous works [16]. structional videos into an interleaved textbook. Coherent Video Frame Interleaving with ASR Enhance the In-context learning capabilities. We observe an interesting phenomenon: even on general-domain benchmarks such as OKVQA and TextVQA, our textbook dataset yields modest improvements in few-shot settings. Specifically, as shown in Tab. 2, in the zero-shot scenario, our textbook lags behind OBELICS by 2.8%; however, in the 1-shot setting, performance becomes comparable. Notably, in the 2-shot and 4-shot settings, our dataset surpasses OBELICS with improvements of +1.1% and +2.4%, respectively. similar trend can also be observed on the TextVQA. This can be attributed to our video-centric interleaved design, which provides more coherent context and enhances the incontext learning capabilities of VLMs. 5.3. Analysis Whether VLMs Can Truly Attend to their Interleaved Context? To better investigate why our textbook enhances few-shot performance, we design Cheat Test: We replace one of the few-shot examples with the test sample itself and then observe whether the VLMs can notice this cheat shortcut. VLM with strong in-context ability would recognize that its context already contains an identical question and answer, thereby answering the question effortlessly. Therefore, we design 1-shot and 2-shot cheat test. For the 1-shot cheat test, the prompt contains only one example ({It, qt, at}) that is identical to the test sample ({It, qt}). In 2-shot cheat test, it includes two examples in the prompt: one identical example ({It, qt, at}) and one random example ({It, qt, at}). This setup allows us to observe whether the VLMs can allocate sufficient attention to their image-text interleaved context and identify relevant inDataset OKVQA TextVQA Mathvista Mathvision Mathverse 69.0 71.5 79.2 1-shot Cheat: Example:{It,qt,at} + Test-case: It,qt 69.3 41.0 MMC4-cf 66.5 43.8 OBELICS 98.4 51.9 Ours 2-shot Cheat: Example:{It,qt,at}, {Ie,qe,ae}+Test-case: It,qt MMC4-Cf OBELICS Ours 72.6 67.7 94.1 55.7 56.7 77.1 39.2 42.8 49. 53.5 71.3 84.3 51.9 39.9 70.7 55.7 62.8 76.8 40.8 39.5 63.1 Table 4. We design Cheat Test to observe whether VLMs can attend to their interleaved context. We replace few-shot example with the test sample itself and observe whether VLM notice this identical <image,question,answer> within their prompt. It, qt, at denote the test case, Ie, qe, ae denote random selected example. formation for question answering. As shown in Tab. 4, in both 1-shot and 2-shot scenarios, our dataset significantly outperforms MMC4 and OBELICS by nearly 20%, particularly on MathVista and MathVision, where we nearly reach 100% in the 1-shot setting, while MMC4 achieves only 72.6% and 69.3%, respectively. Furthermore, from the 1-shot cheat to the 2-shot, the difficulty of cheating increasesas as the context lengthens. As result, we observe significant performance drops for OBELICS and MMC4 from 1-shot to 2-shot cheating scenarios. However, our textbook dataset only exhibits smaller drop on most benchmarks and even shows an improvement in OKVQA from 79.2 (1-shot) to 84.3 (2-shot). These results show that VLMs pre-trained with our multimodal textbook can more effectively allocate attention to their interleaved context and capture useful information from longer contexts. The Influence of Disrupting the Images Order. As previously noted, compared to webpage-centric datasets, the video-centric design offers more coherent image se7 Dataset MMC4-Core-ff OBELICS Ours (ASR Refine, OCR, SSIM) - w/o ASR Refine - w/o OCR Keyframe Extraction algorithms - SSIM Pixel-level extractor - SSIM CLIP-based extractor Perplexity 1-shot Acc. 20.7 22.8 31.1 26.2 (4.9) 28.8 ((2.3) 1-shot Acc. 12.56 11.27 13.92 16.86 12.7 #Keyframe 6.5M 18M 22.1 (9) 6.5M1.7M 24.6 (6.5) Table 6. We perform an ablation study on video-to-textbook pipeline, including the impact of ASR refinement, the necessity of incorporating OCR, and the algorithms for extracting keyframes. 5.4. Ablation of Video-to-Textbooks Design In Sec. 3.2, we detail the process of our video-to-textbook pipeline, including multi-level extraction and filtering. In this section, we delve into the impact of these designs. Raw ASR Text Impairs the Language Ability. In our pipeline, we instruct an LLM to refine the transcribed ASR text. As demonstrated in Tab. 6 (w/o ASR refine), using raw ASR text results in an average performance drop of 4.9% across 7 benchmarks. We calculated the perplexity (PPL) of the raw ASR text and found it significantly higher than other corpora (16.8 Vs. 11.2). This is primarily due to the colloquial characteristics of the video-transcribed ASR, which is often relatively brief, incomplete, and contains high frequency of meaningless conjunctions. Training directly on such text may impair the models language abilities. In contrast, refined ASR has lower PPL (13.9) and more closely aligns with standard training corpora. Integrating OCR Provides Additional Benefits. We also analyzed the impact of integrating OCR into our pipeline. The results indicate that OCR provides additional improvements (+2.3%), particularly in benchmarks such as TextVQA and MathVista. Similar to humans taking notes during lectures, OCR extracts textual knowledge points, formulas, and mathematical symbols from the videos, thereby enhancing the models domain-specific expertise. However, we also observed that low-quality OCR can introduce noise and even significantly degrade performance. Therefore, selecting reliable external tools to extract high-quality OCR is crucial. How to Extract Keyframe? We detect keyframes from video clips using frame-to-frame differences, exploring pixel-level methods (e.g., OpenCV absdiff), structural algorithms (SSIM), and semantic models (CLIP-ViT-L), with results detailed in Tab. 6. We observed that in these instructional videos, which primarily feature abstract diagrams or geometric images, the pixel-level method often extracts an excessive number of keyframes (18M), resulting in 9% drop in training performance. Conversely, the semanticlevel model may struggle to distinguish between these geometric images on semantic level, frequently treating them as similar and consequently missing many critical keyframes (only 1.7M). Therefore, we ultimately adopted Figure 3. We randomly select 20%, 50%, and 100% samples from datasets and shuffle the image order within each sample. These datasets with shuffled images are also used for pretraining. The Accuracy denotes the average of seven benchmarks. Pretraining Continual Pretraining MMC4-Core-ff OBELICS Textbook-6.5M 61.1 SFT OKVQA MathVista 61.5 0.4 61.8 0.7 62.2 1.1 23.2 24.8 1.6 25.6 2.4 28.7 5.5 Table 5. We also evaluated the zero-shot result after instruction fine-tuning using the 665K data from LLaVA-1.5. quence along with frame-by-frame text explanatory, presented in an interleaved image-text format. To verify this, we shuffle the image order of interleaved datasets and then also use it for pre-training. For each dataset, we randomly select 20%, 50%, and 100% of the samples and then shuffle the order of images within each sample. As shown in Fig. 3, whether shuffled at 20%, 50%, or even 100%, the shuffled MMC4 appears largely unaffected. OBELICS exhibits moderate decline. In contrast, our multimodal textbook shows significant performance drop, which becomes increasingly severe as the shuffling ratio increases. These observations confirm our motivation that there is no strong sequential dependency between images in these website-crawled, webpage-centric datasets. However, these coherent images and tightly aligned image-text are beneficial, enabling VLMs to effectively learn complex knowledge and the underlying reasoning logic. The Performance after Instruction Turning. Except for analyzing the pre-training performance, we also report the SFT performance after instruction tuning on LLaVA665K corpus. All training parameters remain the same for OBELICS, MMC4 and our textbook. As shown in Tab. 5, on Mathvista, our textbook elevates the performance of the original LLaVA-1.5 from 23.1 to 28.7, achieving an improvement twice (+5.5%) that of OBELICS (+2.4%) and three times that of MMC4-Core-ff (+1.6%). The results of other benchmarks are similar. These results demonstrate that the knowledge learned during pretraining on our multimodal textbook can transfer to instruction fine-tuning stage, leading to positive outcomes for downstream tasks. 8 SSIM for keyframe extraction, which yielded noticeably better training performance than the other two methods. 6. Conclusion We introduce multimodal textbook to pre-train VLMs, enabling them to acquire specialized knowledge in natural and contextual manner. By aggregating online educational videos (e.g., mathematics and physics courses) and transforming them into frame-ASR interleaved dataset, this textbook provides coherent and interconnected learning context, complementing traditional image-text alignment methods. Using our pipeline, we curated over 2.5 years of instructional videos (22,000 class hours) into high-quality dataset with 6.5 million keyframes and 0.75 billion text tokens. Experiments demonstrate its effectiveness, especially in enhancing VLMs in-context learning capabilities."
        },
        {
            "title": "References",
            "content": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. 1 [2] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: visual language model for few-shot learning. Advances in neural information processing systems, 35:2371623736, 2022. 1, 3 [3] Anas Awadalla, Irena Gao, Josh Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe, Yonatan Bitton, Samir Gadre, Shiori Sagawa, et al. Openflamingo: An opensource framework for training large autoregressive visionlanguage models. arXiv preprint arXiv:2308.01390, 2023. 1, 3, 6 [4] Anas Awadalla, Le Xue, Oscar Lo, Manli Shu, Hannah Lee, Etash Kumar Guha, Matt Jordan, Sheng Shen, Mohamed Awadalla, Silvio Savarese, et al. Mint-1t: Scaling opensource multimodal data by 10x: multimodal dataset with one trillion tokens. arXiv preprint arXiv:2406.11271, 2024. 1 [5] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: versatile vision-language model for understanding, localization, text reading, and beyond. arXiv preprint arXiv:2308.12966, 2023. 3 [6] Minwoo Byeon, Beomhee Park, Haecheon Kim, Sungjun Lee, Woonhyuk Baek, and Saehoon Kim. Coyo-700m: https : / / github . com / Image-text pair dataset. kakaobrain/coyo-dataset, 2022. [7] Wei Chen, Lin Li, Yongqi Yang, Bin Wen, Fan Yang, Tingting Gao, Yu Wu, and Long Chen. Comm: coherent interleaved image-text dataset for multimodal understanding and generation. arXiv preprint arXiv:2406.10462, 2024. 1 [8] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, Bin Li, Ping Luo, Tong Lu, Yu Qiao, and Jifeng Dai. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. arXiv preprint arXiv:2312.14238, 2023. 1, 3 [9] Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, et al. How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites. arXiv preprint arXiv:2404.16821, 2024. 3, 5 [10] Zesen Cheng, Sicong Leng, Hang Zhang, Yifei Xin, Xin Li, Guanzheng Chen, Yongxin Zhu, Wenqi Zhang, Ziyang Luo, Deli Zhao, et al. Videollama 2: Advancing spatialtemporal modeling and audio understanding in video-llms. arXiv preprint arXiv:2406.07476, 2024. 3, 5 [11] Shuhao Gu, Jialing Zhang, Siyuan Zhou, Kevin Yu, Zhaohu Xing, Liangdong Wang, Zhou Cao, Jintao Jia, Zhuoyi Zhang, Yixuan Wang, et al. Infinity-mm: Scaling multimodal performance with large-scale and high-quality instruction data. arXiv preprint arXiv:2410.18558, 2024. 3 [12] Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma, Tengchao Lv, Lei Cui, Owais Khan Mohammed, Barun Patra, et al. Language is not all you need: Aligning perception with language models. Advances in Neural Information Processing Systems, 36:7209672109, 2023. [13] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representaIn International tion learning with noisy text supervision. conference on machine learning, pages 49044916. PMLR, 2021. 1 [14] Dongfu Jiang, Xuan He, Huaye Zeng, Cong Wei, Max Ku, Qian Liu, and Wenhu Chen. Mantis: Interleaved multi-image instruction tuning. arXiv preprint arXiv:2405.01483, 2024. 1 [15] Hugo Laurencon, Andres Marafioti, Victor Sanh, and Leo Building and better understanding visionarXiv Tronchon. language models: preprint arXiv:2408.12637, 2024. 1 insights and future directions. [16] Hugo Laurencon, Leo Tronchon, Matthieu Cord, and Victor Sanh. What matters when building vision-language models? arXiv preprint arXiv:2405.02246, 2024. 6, [17] Hugo Laurencon, Lucile Saulnier, Leo Tronchon, Stas Bekman, Amanpreet Singh, Anton Lozhkov, Thomas Wang, Siddharth Karamcheti, Alexander M. Rush, Douwe Kiela, Matthieu Cord, and Victor Sanh. Obelics: An open webscale filtered dataset of interleaved image-text documents, 2023. 1, 3, 2 [18] Gen Li, Nan Duan, Yuejian Fang, Ming Gong, and Daxin Jiang. Unicoder-vl: universal encoder for vision and In Proceedings of language by cross-modal pre-training. the AAAI conference on artificial intelligence, pages 11336 11344, 2020. 3 [19] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with 9 In Infrozen image encoders and large language models. ternational conference on machine learning, pages 19730 19742. PMLR, 2023. 1, 3 [20] Qingyun Li, Zhe Chen, Weiyun Wang, Wenhai Wang, Shenglong Ye, Zhenjiang Jin, Guanzhou Chen, Yinan He, Zhangwei Gao, Erfei Cui, et al. Omnicorpus: An unified multimodal corpus of 10 billion-level images interleaved with text. arXiv preprint arXiv:2406.08418, 2024. 1, 6, 2 [21] Zehan Li, Xin Zhang, Yanzhao Zhang, Dingkun Long, Pengjun Xie, and Meishan Zhang. Towards general text arXiv embeddings with multi-stage contrastive learning. preprint arXiv:2308.03281, 2023. [22] Ji Lin, Hongxu Yin, Wei Ping, Yao Lu, Pavlo Molchanov, Andrew Tao, Huizi Mao, Jan Kautz, Mohammad Shoeybi, and Song Han. Vila: On pre-training for visual language models, 2023. 1, 3 [23] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning, 2023. 3 [24] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning, 2023. 3 [25] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2629626306, 2024. 1, 3, 6 [26] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36, 2024. [27] Haoyu Lu, Wen Liu, Bo Zhang, Bingxuan Wang, Kai Dong, Bo Liu, Jingxiang Sun, Tongzheng Ren, Zhuoshu Li, Hao Yang, et al. Deepseek-vl: towards real-world visionlanguage understanding. arXiv preprint arXiv:2403.05525, 2024. 1 [28] Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering. Advances in Neural Information Processing Systems, 35:25072521, 2022. 6 [29] Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi. Ok-vqa: visual question answering In Proceedings benchmark requiring external knowledge. of the IEEE/cvf conference on computer vision and pattern recognition, pages 31953204, 2019. 6 [30] Brandon McKinzie, Zhe Gan, Jean-Philippe Fauconnier, Sam Dodge, Bowen Zhang, Philipp Dufter, Dhruti Shah, Xianzhi Du, Futang Peng, Floris Weers, et al. Mm1: Methods, analysis & insights from multimodal llm pre-training. arXiv preprint arXiv:2403.09611, 2024. 1 [31] Antoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac, and Josef Sivic. Ivan Laptev, Makarand Tapaswi, Howto100m: Learning text-video embedding by watching hundred million narrated video clips. In Proceedings of the IEEE/CVF international conference on computer vision, pages 26302640, 2019. 2 [32] Ethan Perez, Douwe Kiela, and Kyunghyun Cho. True fewshot learning with language models. Advances in neural information processing systems, 34:1105411070, 2021. [33] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR, 2021. 3 [34] Ramon Sanabria, Ozan Caglayan, Shruti Palaskar, Desmond Elliott, Loıc Barrault, Lucia Specia, and Florian Metze. How2: large-scale dataset for multimodal language understanding. arXiv preprint arXiv:1811.00347, 2018. 2 [35] Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. LAION400M: open dataset of clip-filtered 400 million image-text pairs. CoRR, abs/2111.02114, 2021. 3 [36] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. Advances in Neural Information Processing Systems, 35:2527825294, 2022. 1, 3 [37] Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and Marcus Rohrbach. Towards vqa models that can read. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 83178326, 2019. 6 [38] Quan Sun, Yufeng Cui, Xiaosong Zhang, Fan Zhang, Qiying Yu, Yueze Wang, Yongming Rao, Jingjing Liu, Tiejun Huang, and Xinlong Wang. Generative multimodal models are in-context learners. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1439814409, 2024. [39] Gemini Team, Petko Georgiev, Ving Ian Lei, Ryan Burnell, Libin Bai, Anmol Gulati, Garrett Tanzer, Damien Vincent, Zhufeng Pan, Shibo Wang, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. 1 [40] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models. CoRR, abs/2302.13971, 2023. 3 [41] Pavan Kumar Anasosalu Vasu, Hadi Pouransari, Fartash Faghri, Raviteja Vemulapalli, and Oncel Tuzel. Mobileclip: Fast image-text models through multi-modal reinforced In Proceedings of the IEEE/CVF Conference on training. Computer Vision and Pattern Recognition, pages 15963 15974, 2024. 3 [42] Junjie Wang, Yin Zhang, Yatai Ji, Yuxiang Zhang, Chunyang Jiang, Yubo Wang, Kang Zhu, Zekun Wang, Tiezhen Wang, Wenhao Huang, et al. Pin: knowledge-intensive dataset for paired and interleaved multimodal documents. arXiv preprint arXiv:2406.13923, 2024. 3 [43] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, and Junyang Lin. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. 1, 3 [44] Zhou Wang, Alan Bovik, Hamid Sheikh, and Eero Simoncelli. Image quality assessment: from error visibility to structural similarity. IEEE transactions on image processing, 13(4):600612, 2004. 5 [45] An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, et al. Qwen2 technical report. arXiv preprint arXiv:2407.10671, 2024. 3, 4 [46] Zhengyuan Yang, Zhe Gan, Jianfeng Wang, Xiaowei Hu, Yumao Lu, Zicheng Liu, and Lijuan Wang. An empirical In Prostudy of gpt-3 for few-shot knowledge-based vqa. ceedings of the AAAI conference on artificial intelligence, pages 30813089, 2022. 6 [47] Yuan Yao, Tianyu Yu, Ao Zhang, Chongyi Wang, Junbo Cui, Hongji Zhu, Tianchi Cai, Haoyu Li, Weilin Zhao, Zhihui He, et al. Minicpm-v: gpt-4v level mllm on your phone. arXiv preprint arXiv:2408.01800, 2024. 3 [48] Jiabo Ye, Haiyang Xu, Haowei Liu, Anwen Hu, Ming Yan, Qi Qian, Ji Zhang, Fei Huang, and Jingren Zhou. mplug-owl3: Towards long image-sequence understanding arXiv preprint in multi-modal large language models. arXiv:2408.04840, 2024. [49] Rowan Zellers, Jiasen Lu, Ximing Lu, Youngjae Yu, Yanpeng Zhao, Mohammadreza Salehi, Aditya Kusupati, Jack Hessel, Ali Farhadi, and Yejin Choi. Merlot reserve: Neural script knowledge through vision and language and sound. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1637516387, 2022. 2 [50] Hang Zhang, Xin Li, and Lidong Bing. Video-llama: An instruction-tuned audio-visual language model for video understanding. arXiv preprint arXiv:2306.02858, 2023. 3 [51] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023. 1 [52] Wanrong Zhu, Jack Hessel, Anas Awadalla, Samir Yitzhak Gadre, Jesse Dodge, Alex Fang, Youngjae Yu, Ludwig Schmidt, William Yang Wang, and Yejin Choi. Multimodal c4: An open, billion-scale corpus of images interleaved with text. Advances in Neural Information Processing Systems, 36, 2024. 1, 3,"
        },
        {
            "title": "Supplementary Material",
            "content": "7. Detail of Video-to-Textbook Pipeline Algorithm 1 SSIM-Based Key Frame Extraction Algorithm 7.1. Implementation Details When synthesizing the Knowledge Taxonomy, we utilize GPT-4o to construct the taxonomy. When filtering video at the metadata level, GPT-4o is also employed to review the metadata of the searched videos. During the Video-to-ASR phase, Whisper-large-v3 is used to convert audio into text. Then Qwen2-72B-Instruct is applied to refine the raw ASR transcriptions. In the video-level filtering stage, DeepSeek-V2 and Llama3-70B-Instruct are used to score each ASR transcription, enabling the filtration of low-quality videos. video is filtered out if both LLMs determine its ASR does not meet the required standards. After splitting long videos into short clips, we first use VideoLlama2-7B to generate detailed caption for each video clip. Subsequently, we compute the similarity between the clips caption and the ASR using GTE-Qwen2-7B-Instruct. Finally, InternVL2 is employed to extract and filter OCR from the keyframe. Our code and part of the dataset are also available at https: //anonymous.4open.science/r/multimodal_ textbook-3666. 7.2. Human Evaluation We randomly sample 100 examples from the multimodal textbook and conduct manual quality evaluation, focusing on three key aspects: (1) image quality, (2) the connections between different images in sample and (3) the relevance between texts and images. After manual inspection, we observe that, aside from chemistry, this batch of samples covers five domains: mathematics (31), physics (16), computer science (16), engineering (25), and earth sciences (12). It contains total of 1,421 images, including 378 slide-style images, 214 lecture-style images, 414 demonstration animations, and 415 natural scenes. Image analysis reveals that only 7% (72 images) are highly similar, while the remaining images are related to each other but also exhibit clear distinctions. Text-image relevance analysis shows that the attached text (ASR) correctly explains the visual concepts or computational processes presented in the images, with no ambiguity or redundancy. 7.3. Constructing Pretraining Sample After collecting 6.5M keyframes, and 750M refined ASR, and OCR tokens, we can employ various strategies to construct image-text interleaved samples for pre-training. ① Require: Frame sequence {F1, F2, . . . , FN }, similarity threshold Ensure: Key frame sequence {K1, K2, . . . } 1: {F1} frame Initialize key frame sequence with the first Set the reference frame to F1 2: ref erence rame F1 3: for = 2 to do 4: SSIM CalculateSSIM(ref erence rame, Fi) Calculate SSIM between reference frame and frame Fi 5: 6: 7: if SSIM < then {Fi} frame Fi as key frame If SSIM is below threshold, add ref erence rame Fi Update the reference frame to Fi end if 8: 9: end for 10: return Return the sequence of key frames Similar to webpage-centric dataset, where each webpage is treated as separate sample, we treat each video as an individual sample. This simple strategy maintains the semantic integrity of video. However, it also leads to overly long contexts for most samples, as each video contains an average of 86 keyframes, far exceeding the maximum context length supported by most VLMs. ② As an alternative, we segment single long video into mulIt can flexibly segment videos based on tiple samples. the maximum context length supported by VLMs. ③ Besides, we directly concatenate multiple video clips i.e., framek1 , ocri, asri, to the maximum context length. This strategy breaks video boundaries, effectively utilizing computational resources. However, mixing multiple video clips within single sample may adversely affect training performance. Therefore, we insert specific token: End of Video at the end of each video to mitigate this. , .., framekn 7.4. Knowledge Taxonomy As stated in the main text, to include richer knowledge in our textbook, we propose hierarchical knowledge taxonomy comprising four hierarchical layers, namely Subject Course Sub-course Knowledge Point. We instruct an LLM to span the knowledge taxonomy across multiple educational stages (from primary school to middle school) and diverse subjects (mathematics, physics, etc.). Lastly, we obtain knowledge taxonomy comprising 6 subjects (mathematics, physics, chemistry, earth science, engineering, and 1 computer science), 55 courses (Algebra, Solid Geometry,..), and 3915 knowledge points. As illustrated in Fig. 4, we plot six subjects along with their corresponding courses. Due to space constraints, we visualized the top 9 courses and their proportion. The number of knowledge points included in each course is approximately the same. 7.5. Detail of InSI-SIM As mentioned in Sec. 4.2, we design an in-sample image similarity metric (InSI-SIM). It measures the similarity between all images within sample. Formally, for subset containing samples, each comprising images, the in-sample image similarity is computed as follows: InSI-SIML ="
        },
        {
            "title": "1\nM",
            "content": "M (cid:88) k=1 1 (cid:0)L 2 (cid:1) L1 (cid:88) (cid:88) (cid:16) i=1 j=i+1 CLIP(Imgk,i, Imgk,j) (cid:17) + SSIM(Imgk,i, Imgk,j) / (1) where CLIP(Imgk,i, Imgk,j) and SSIM(Imgk,i, Imgk,j) represent the semantic and structural similarity scores between images and in sample k, respectively. 8. Details of Experiments 8.1. Detail of Evaluation We evaluate the pre-trained VLMs on two VQA benchmarks (TextVQA, OKVQA), knowledge-centric benchmark (ScienceQA), and three math-related benchmarks (MathVista, MathVerse, MathVision) under few-shot settings. Following the previous works [20], we use the RICES-based few-shot prompting strategy which retrieves the most similar samples from the training set based on the testing image feature. It should be noted that since MathVista, MathVerse, and MathVision only contain testing sets, we can not retrieve samples from their respective training sets. Consequently, for MathVista and MathVerse, we retrieve examples from MathVision, while for MathVision, we retrieve examples from MathVista. When evaluating, we adopt the same prompt as Llava-1.5: System Prompt: chat between human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the humans questions USER: <image>n{example1 query}nAnswer the question using single word or phrase. ASSISTANT: {example1 answer}</s> USER: <image>n{example2 query}nAnswer the question using single word or phrase. ASSISTANT: {example2 answer}</s> .... USER: <image>n{testing query}nAnswer the question using single word or phrase. ASSISTANT: 8.2. Examples of Multimodal Textbook We provide several detailed examples in Figs. 5 to 10. Specifically, Fig. 5 offers detailed explanation of the Earths water cycle, presented through slides, photographs, and schematic diagrams. Figures 6 and 7 provide rich visualizations, including diagrams and texts, to elucidate the concepts of velocity and acceleration in physics. Figure 8 demonstrates the step-by-step, frame-by-frame problemsolving process for mathematical geometry problem, detailing each critical step with accompanying text and visuals. Figure 9 presents detailed depiction of chemical concepts such as atoms, molecules, and compounds through combination of text and illustrations. Figure 10 introduces the depth-first search algorithm using an animation. Except for refined ASR texts, we also provide the OCR texts in our textbook, which can be helpful for math-related scenario. For example, in Fig. 7, we utilize OCR to recognize formulas and symbols displayed on the screen, which facilitates better comprehension of physical concepts. 9. Limitations Although we already designed multiple levels of filtering, our textbook may still contain some redundant keyframes, low-quality texts, and so on. We will continue to improve the quality and knowledge density of our textbook. Besides, similar to prior multimodal models, our textbook primarily focuses on multimodal understanding and text generation for interleaved contexts. During training, the loss is not computed for image tokens. However, our textbook can also be used for omni-modal models including both understanding and generation tasks. We leave this for future work. 10. Ethical discussion During the collection and release of our multimodal textbook dataset, We are very concerned about ethical considerations. In addition to following the established corpora (e.g., MMC4 [52], OBELICS [17] and Omnicorpus [20]), we make additional efforts to uphold high ethical standards, such as employing LLMs to filter out inappropriate videos, including those with biases, pornographic content, or personal privacy information, such as identification documents and bank account details. We are open to further refining our strategy while maintaining open-source resources based on community feedback. 11. License and Author Statement We release the dataset under CC-BY license and Terms of Use that require disclosure of when the dataset is used for the purpose of training models. This license is not intended to replace the licenses of the source content, and any use of content included in the dataset must comply with the"
        },
        {
            "title": "Subject\nMathematics\nPhysics\nChemistry\nEarth Science\nEngineering\nComputer Science\nAll",
            "content": "#Video Duration (h) 21.7k 11k 4.5k 12k 13k 12.8k 75k 4,423 3,511 2,643 3,670 4,096 4,354 22,697 #Topic 725 530 410 520 810 820 3,915 #Video Clip 809k 822k 234k 640k 713k 782k 4M #Keyframe 1.67M 0.95M 0.49M 1.03M 1.15M 1.21M 6.58M #ASR Token 72.5M 36.7M 15M 40M 43.3M 42.8M 258M #OCR Token 145M 73.4M 30M 80M 86.6M 85.5M 500M #Sample 123k 119k 32k 88k 98k 150k 610k Table 7. The statistics of our multimodal textbook. Topic denotes the knowledge points covered by each category of videos, which are sourced from our knowledge taxonomy. Figure 4. Top: We plot six subjects along with their corresponding sub-courses. Due to space constraints, we selectively visualized only the courses with the highest proportions. Bottom: We count the knowledge points distribution belongs to each subject and its course 3 Figure 5. case presented in our textbook illustrates the water cycle within the domain of earth science. original licenses and applicable rights of its data subjects. The purpose of this statement is to clarify the responsibilities and liabilities associated with the use of this dataset. While we have made every effort to ensure the accuracy and legality of the data contained within this dataset, we cannot guarantee its absolute completeness or correctness. Therefore, if any rights, legal or otherwise, are violated through this dataset, including but not limited to copyright infringement, privacy violations, or misuse of sensitive information, we, the authors, assume no liability for such violations. By utilizing this dataset, you agree that any conse4 Figure 6. case presented in our textbook introducing the principles of mechanics within the domain of physics. quences, legal or otherwise, arising from using this dataset will be the users sole responsibility. You acknowledge that you will exercise due diligence and adhere to all applicable laws, regulations, and ethical guidelines when using the dataset. By accessing, downloading, or using this dataset, you signify your acceptance of this statement and your commitment to abide by the terms and conditions of the CC-BY 5 Figure 7. case presented in our textbook introducing the concepts of velocity and acceleration within the context of physics. license. ging Face Hub."
        },
        {
            "title": "If you disagree with the terms of this statement or the",
            "content": "CC-BY license, you are not authorized to use this dataset. The dataset will be hosted and maintained on the Hug6 Figure 8. case presented in our textbook demonstrates how to solve question about planar geometry in the domain of mathematics. 7 Figure 9. case presented in our textbook illustrates the concepts of molecules, atoms, and compounds in the domain of chemistry. 8 Figure 10. case presented in our textbook introduces depth-first search algorithm."
        }
    ],
    "affiliations": [
        "College of Computer Science and Technology, Zhejiang University",
        "DAMO Academy, Alibaba Group"
    ]
}