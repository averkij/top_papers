{
    "paper_title": "Multi-Token Prediction Needs Registers",
    "authors": [
        "Anastasios Gerontopoulos",
        "Spyros Gidaris",
        "Nikos Komodakis"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Multi-token prediction has emerged as a promising objective for improving language model pretraining, but its benefits have not consistently generalized to other settings such as fine-tuning. In this paper, we propose MuToR, a simple and effective approach to multi-token prediction that interleaves learnable register tokens into the input sequence, each tasked with predicting future targets. Compared to existing methods, MuToR offers several key advantages: it introduces only a negligible number of additional parameters, requires no architectural changes--ensuring compatibility with off-the-shelf pretrained language models--and remains aligned with the next-token pretraining objective, making it especially well-suited for supervised fine-tuning. Moreover, it naturally supports scalable prediction horizons. We demonstrate the effectiveness and versatility of MuToR across a range of use cases, including supervised fine-tuning, parameter-efficient fine-tuning (PEFT), and pretraining, on challenging generative tasks in both language and vision domains. Our code will be available at: https://github.com/nasosger/MuToR."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 1 ] . [ 1 8 1 5 0 1 . 5 0 5 2 : r Multi-Token Prediction Needs Registers Anastasios Gerontopoulos1, Spyros Gidaris2 Nikos Komodakis1,3,4 1Archimedes, Athena Research Center 2valeo.ai 3University of Crete 4IACM-Forth"
        },
        {
            "title": "Abstract",
            "content": "Multi-token prediction has emerged as promising objective for improving language model pretraining, but its benefits have not consistently generalized to other settings such as fine-tuning. In this paper, we propose MuToR, simple and effective approach to multi-token prediction that interleaves learnable register tokens into the input sequence, each tasked with predicting future targets. Compared to existing methods, MuToR offers several key advantages: it introduces only negligible number of additional parameters, requires no architectural changesensuring compatibility with off-the-shelf pretrained language modelsand remains aligned with the next-token pretraining objective, making it especially well-suited for supervised fine-tuning. Moreover, it naturally supports scalable prediction horizons. We demonstrate the effectiveness and versatility of MuToR across range of use cases, including supervised fine-tuning, parameter-efficient fine-tuning (PEFT), and pretraining, on challenging generative tasks in both language and vision domains. Our code will be available at: https://github.com/nasosger/MuToR."
        },
        {
            "title": "Introduction",
            "content": "Autoregressive Transformer architectures have become the cornerstone of modern Large Language Models (LLMs), enabling unprecedented capabilities across wide range of natural language processing tasks [Achiam et al., 2023, Liu et al., 2024]. Their success has also extended to domains such as image generation [Esser et al., 2021, Sun et al., 2024] and multimodal models [Alayrac et al., 2022, Liu et al., 2023]. These models are primarily trained using simple yet effective approach: next-token prediction with teacher forcing. By supplying ground truth tokens as context, teacher forcing enables fully parallelized computation via masked self-attention, thus accelerating and stabilizing training. However, next-token prediction with teacher forcing has notable limitations. Models trained this way often focus on short-term patterns while struggling with harder, long-range decisions. Prior work has shown that this setup can lead to shortcut learning, where valuable supervision diminishes [Bachmann and Nagarajan, 2024], and that it underperforms on tasks requiring planning [Bubeck et al., 2023]. These findings strongly suggest the need for training objectives that go beyond standard next-token prediction. To address these limitations, multi-token prediction training [Qi et al., 2020, Gloeckle et al., 2024, Liu et al., 2024] has emerged as promising alternative. Rather than predicting just one token at time, the model learns to predict multiple future tokens at each position. Recent implementations achieve this through additional transformer output heads: Gloeckle et al. [2024] employs parallel heads, one for each future token position, while Liu et al. [2024] uses sequential heads. Crucially, this approach is used only during training, as its primary goal is to provide more informative learning signal rather than to speed up inference. Compared to standard teacher forcing, multi-token prediction encourages the model to develop internal \"planning\" representations, and mitigates overfitting to local patterns. Preprint. Under review. Experiments by Gloeckle et al. [2024] demonstrate that it leads to improved generative performance and increased data efficiency. Motivated by these findings, we explore whether more effective approach for multi-token prediction can further enhance autoregressive transformers. We propose simple but powerful modification: instead of adding extra transformer layers for future token prediction, we introduce register tokensspecial tokens interleaved between the regular tokens. Each register token is assigned randomly sampled offset d, and the model is trained to predict the token steps ahead (rather than just the next token) for these tokens. The original next-token prediction objective remains unchanged for all regular tokens. These register tokens are used exclusively during training to propagate richer supervisory signal through the model. At inference time, they are discarded to preserve generation speed. This is made possible by carefully designed attention mask: register tokens are allowed to attend only to preceding regular tokensenabling them to learn predictive representationswhile regular tokens are entirely blind to register tokens in both directions. This ensures full compatibility with standard autoregressive inference while encouraging the model to internalize forward-looking, multi-step planning during training. Compared to approaches that add output heads or transformer blocks, our register-based method offers several key advantages: No architectural changes: Only small number of additional trainable parameters are introduced through register embeddings, while the core transformer layers remain untouched and no extra output heads are required. Finetuning compatibility: Our method is particularly well-suited for fine-tuning pretrained LLMs (e.g., Llama [Grattafiori et al., 2024], Gemma [Gemma Team et al., 2024]). It introduces minimal parameter overhead, preserves original attention patterns for regular tokens, and uses carefully selected position ids and attention masks for register tokens bringing multi-token prediction closer to the next-token pretraining setup. In contrast, previous methods [Gloeckle et al., 2024, Liu et al., 2024] rely on separate transformer heads, adding many new parameters that must be trained from scratch, making them less effective in fine-tuning scenarios. Scalable prediction horizons: Because the number of register tokens remains fixed regardless of the offset d, the training cost is independent of the prediction horizon, which can be scaled arbitrarily. Register tokens thus provide greater flexibility in future token prediction. For example, in autoregressive image generation, our method naturally extends to predicting tokens in two-dimensional neighborhooda capability not easily achieved by adding output heads. Overall, our work delivers the following contributions: We introduce MuToR (Multi-Token prediction with Registers), novel multi-token prediction method that employs trainable, interleaved register tokens tasked with predicting multiple future targets. MuToR enables scalable prediction horizons with minimal additional parameters and seamlessly integrates with any pretrained autoregressive language model without architectural modifications. Through experiments on language modeling benchmarks, we validate the effectiveness of MuToR in both supervised finetuning and parameter-efficient finetuning (PEFT) settings, consistently surpassing standard finetuning baselines under equivalent training compute. We further demonstrate the versatility of MuToR by applying it to autoregressive image generation in pretraining setting, where it improves performance over standard teacher-forcing highlighting its broader potential across diverse domains and training settings."
        },
        {
            "title": "2 Related Work",
            "content": "Limitations of Next-Token Prediction Bachmann and Nagarajan [2024] introduce path-finding task on star-graphs to highlight key limitations of standard next-token prediction (i.e., teacher forcing). Their findings reveal that next-token prediction encourages shortcuts, making the underlying task intractable and reducing validation accuracy to random-guessing levels. Interestingly, this \"cheating\" behavior can be mitigated by multi-token prediction with lookahead embeddings, as in Monea et al. [2023]. While the task is an extreme case, alternative transformer architectures can solve it 2 [Frydenlund, 2024], reinforcing the intuition that tasks requiring planning might need tailored training objectives [Bubeck et al., 2023]. Multi-token and Lookahead Prediction Several works have explored decoding multiple future tokens, primarily to accelerate inference rather than improve generation quality [Stern et al., 2018, Monea et al., 2023, Li et al., 2024, Cai et al., 2024]. In contrast, Qi et al. [2020] propose multitoken prediction pre-training objective that enhances performance on some sequence-to-sequence tasks. However, their method scales poorly to large decoder models because their multi-stream attention mechanism becomes computationally expensive as prediction depth increases. More recently, Gloeckle et al. [2024] proposed an architecture with multiple parallel decoding heads for multi-token prediction, leading to better generative performance (mostly in coding tasks). Liu et al. [2024] modified this approach to use sequential decoding heads instead. Both studies suggest that multi-token prediction helps by providing richer supervision, better information sharing between tokens, and implicit planning in hidden states. However, these benefits are mainly observed during pretraining, with limited success in fine-tuning scenariosa gap our work addresses. less related line of research trains autoregressive models on permuted sequences [Yang et al., 2019, Pannatier et al., 2024, Kakogeorgiou et al., 2024, Yu et al., 2024, Pang et al., 2024]. By predicting next tokens in shuffled order, these methods force the model to recover distant dependencies in the original sequence. While conceptually interesting, they differ significantly from our approach. Trainable Dummy / Register Tokens Recent work investigates the use of trainable tokens as transformer inputs, revealing emergent properties. Burtsev et al. [2020] prepend dummy tokens to prompts to induce working memory, but observe minimal gains. Goyal et al. [2024] demonstrate that appended trainable tokens can improve performance by increasing operations between tokens, thus encouraging deeper \"thinking\". Pfau et al. [2024] study the conditions under which language model can leverage the extra computation provided by the dummy tokens. Related efforts incorporate planning tokens trained to predict latent reasoning steps [Wang et al., 2023, Yin et al., 2024]. In the vision domain, Darcet et al. [2024] employ register tokens during pretraining to prevent the appearance of high-norm artifacts. Similarly to these works, we use learnable tokensbut we also associate them with an auxiliary multi-token prediction objective to create denser training signal. More related to our work, Monea et al. [2023] appends lookahead tokens to the input sequence and trains them (while freezing the base model) to enable parallel decoding for speculative sampling. Bachmann and Nagarajan [2024] adapt this idea to path-finding on star-graphs, by simultaneously predicting multiple answer tokens from prefix. While effective for these specific graph problems, the approachs complete \"teacherless\" training paradigm and parallel inference requirements make it fundamentally unsuitable for the broader range of generative tasks we consider. Unlike these methods, our approach only uses register tokens during training to enhance supervision, without modifying the inference procedure or requiring additional compute at test time."
        },
        {
            "title": "3 Method",
            "content": "3.1 Preliminaries Next-Token Prediction Building on the foundational work of Shannon [1948, 1951], next-token prediction remains the core objective for autoregressive language models. Given sequence (x1, . . . , xt), the model is trained to predict the next token xt+1 by maximizing the joint probability under left-toright factorization: (x1, . . . , xT ) = (cid:89) (xt+1 xt), (1) where is the sequence length. For model Pθ and dataset D, the training objective will be to minimize the the expected negative log-likelihood loss over the dataset: (cid:34) Lntp = ED (cid:88) (cid:35) log Pθ(xt+1 xt) . (2) Figure 1: Next-token prediction vs. Multi-token prediction with registers (MuToR). The transformer block represents any decoder-only autoregressive model, with colored lines indicating permitted attention connections between tokens. Left: Standard next-token prediction, where each xt predicts xt+1 conditioned on preceding tokens. Right: MuToR interleaves register tokens rd to predict tokens steps ahead (xt+d), conditioned only on previous regular tokens. Register tokens are assigned position ids (e.g., + 1 for rd targeting xt+d) that mimic next-token prediction. Regular tokens follow the standard next-token prediction formulation, unaffected by the registers. Multi-Token Prediction In contrast, Gloeckle et al. [2024] propose predicting multiple future tokens per position. Their objective minimizes: (cid:34) Lmtp = ED (cid:88) log Pθ(xt+1:t+dmax xt) , (3) (cid:35) where we denote the maximum prediction horizon as dmax, to align notation with our method. Recent implementations utilize additional transformer headseither parallel [Gloeckle et al., 2024] or sequential [Liu et al., 2024]. 3.2 Our approach In this work, we introduce MuToR, an alternative multi-token prediction method (illustrated in Figure 1). Our approach inserts interleaved learnable tokenstermed registers, following Darcet et al. [2024]into training sequences, where each register is tasked with predicting future token at uniformly sampled offset d. By optimizing this auxiliary prediction objective alongside the primary next-token prediction task, the model benefits from richer supervisory signals, which enhances the quality of learned representations. Register Tokens Let = (x1, x2, . . . , xT ) be training sequence. We augment by interleaving1 register tokens rd, yielding: = (x1, rd, x2, rd, . . . , xT 1, rd, xT ), where each rd predicts the future token at offset d2. By default, all rd share single learnable embedding, adding minimal trainable parameters, while the target offset is specified via the registers position id (detailed later). The augmented sequence is processed by the transformer Pθ, which shares all componentsembeddings, layers, and prediction headbetween the regular tokens xt and register tokens rd. (4) Attention Masking We modify the causal attention mask to satisfy two conditions: (i) regular tokens xt cannot attend to any register tokens, and (ii) register tokens cannot attend to other register tokens in the sequence (see Figure 2). This pattern preserves the standard next-token prediction objective (Equation 2) for regular tokens, as their representations remain unaffected by registers. As result, the registers can be discarded during inference. 1The interleaving pattern is flexiblee.g., registers may be sparse or appear consecutively. 2For simplicity, we use fixed per sequence, though registers with different offsets could be mixed. 4 Multi-token Prediction with Registers Each register token rd inserted after xt predicts the future token xt+d, with offset sampled uniformly per sequence from {1, . . . , dmax}, where dmax determines the maximum prediction horizon. The auxiliary register loss over dataset is: (cid:34) Lreg = ED (cid:88) log Pθ(xt+d xt, rd) . (5) (cid:35) Our attention masking ensures predictions depend only on preceding regular tokens, excluding other registers. This approach enables flexible prediction horizons while maintaining parameter efficiency, as all register tokensregardless of dshare single learnable embedding. Figure 2: MuToRs attention mask. Each cell indicates whether the row can attend to the column. Position Embeddings for Registers Since we do not use specialized heads for future token prediction, we encode prediction offsets through positional bias. We utilize the token indices in the original (register-free) sequence to compute positional embeddings. While regular tokens xt keep their natural positions t, each register rd inserted after xt (predicting xt+d) receives position + 1 (see position ids in Figure 1). This matches the position id of the regular token that would normally predict xt+d under standard next-token prediction. Our implementation focuses on RoPE [Su et al., 2024], the dominant positional encoding in modern language [Touvron et al., 2023b] and autoregressive image models [Sun et al., 2024]. However, our position manipulation works with any embedding scheme (sinusoidal, relative, etc.), requiring no architectural changes while effectively encoding prediction offsets through positional bias. Overall Training Loss We jointly optimize the standard next-token prediction loss Lntp and the auxiliary register loss Lreg. The overall loss combines these two objectives through weighted sum: Lmtp = (1 a)Lntp + aLreg, (6) where (0, 1) controls the relative contribution of each loss term. Inference During inference, we discard register tokens entirely, leaving the models computational graph and latency unchanged. This is made possible by our attention masking strategy, which prevents regular tokens from ever attending to registers during training. Unlike prior approaches (Goyal et al. [2024], Pfau et al. [2024]) that use inserted tokens to increase inference computation, our method maintains the standard autoregressive process without modification. 3.3 Adaptation in Language Modeling key application of our method is supervised fine-tuning of pretrained language models. For generative tasks (e.g., mathematical reasoning), where datasets contain (prefix, answer) pairswith answer sequences potentially including chain-of-thought tokenswe interleave register tokens only within the answer sequence. This aligns with standard practice where prefix predictions are excluded from loss computation. Beyond this adaptation, the method follows the implementation described in subsection 3.2 without modification. 3.4 Adaptation in Autoregressive Image Generation Autoregressive transformers achieve strong performance on image generation by modeling discrete visual tokens [Esser et al., 2021, Sun et al., 2024]. VQ-VAE tokenizer [Van Den Oord et al., 2017] first encodes an image into 2D token grid x2D Zhw, which is then flattened into 1D sequence via raster-scan ordering. The model then learns autoregressive prediction conditioned on (either class labels or captions). Figure 3: The 2D neighborhood of possible prediction targets (depicted in red) for register token. The register rd is inserted after x7, and dmax_2D is set to 3. We adapt MuToR to images by modifying the offset sampling to respect the 2D image structure. For each sequence x, we sample 2D offset pair (dh, dw), with both dh and dw drawn uniformly from {1, . . . , dmax_2D}, excluding (dh, dw) = (1, 1), as it denotes the image token after which the register is inserted (see Figure 3). We then compute the rasterized offset as = (dh 1) + dw 1. As in subsection 3.2, each register token rd predicts the token steps ahead in the sequence, with all other components (attention masking, positional embeddings, and loss) implemented identicallyexcept for the additional conditioning on c. Each register thus predicts one of d2 max_2D 1 possible future tokens. This 2D extension enriches the training signal by capturing spatial dependencies inherent in visual data, while requiring minimal architectural changes. Unlike prior multi-token prediction approaches that would require multiple additional prediction heads (one for each possible 2D offset), MuToR achieves this capability with negligible parameter overhead."
        },
        {
            "title": "4 Results",
            "content": "4.1 Language Modeling Experimental Setup We focus on mathematical reasoning with chain-of-thought and abstractive summarization, two challenging generative tasks that provide rigorous testbed for our method. To evaluate performance, we fine-tune two pretrained decoder-only language models: Gemma 2B [Gemma Team et al., 2024] and Llama 3 8B [Grattafiori et al., 2024]. Our experiments target three widely used mathematical reasoning benchmarks: GSM8K [Cobbe et al., 2021], MATH500 [Lightman et al., 2023], and AQUA-RAT [Ling et al., 2017]. For finetuning, we use curated subsets from OpenMathInstruct-2 [Toshniwal et al., 2025], high-quality dataset derived from GSM8K and MATH training samples. Specifically, we filter the 1M and 2M splits to isolate grade school (GSM-style) or MATH-style problems, referring to them as 1M-GSM, 2M-GSM, and 1M-MATH. We also finetune on the original GSM8K and AQUA-RAT training splits. Additional details about the experimental setup are provided in Appendix A.1. As for summarization, we target the following benchmarks: SAMSum [Gliwa et al., 2019] and DialogSum [Chen et al., 2021]. For mathematical tasks, we measure exact-match accuracy; for summarization, we use ROUGE scores [Lin, 2004]. Baselines We consider two baselines: (i) Next-Token, the standard finetuning recipe using the next-token prediction objective, and (ii) Multi-Token [Gloeckle et al., 2024], adapted for finetuning by adding dmax 1 auxiliary prediction heads and applying loss-weighting strategy for these heads, similar to our method. To ensure fair comparison, for both Multi-Token and MuToR, we tune the number of predicted future tokens, dmax, alongside the auxiliary loss coefficient. Implementation details are provided in Appendix A.1. Comparative Results Tables 1 and 2 present our results, showing only the best configurations for Multi-Token and our MuToR method. In mathematical reasoning  (Table 1)  , our approach consistently outperforms both baselines, including Multi-Token, which introduces substantial number of additional trainable parameters (see Table 3). Moreover, MuToRs gains are preserved across varying training set sizes, demonstrating its effectiveness even in settings with high-quality finetuning data. In contrast, Multi-Tokens benefits seem to diminish with larger models or more data. For summarization  (Table 2)  , our MuToR method improves all ROUGE scores, achieving superior results over Multi-Token, demonstrating broad applicability in sequence-to-sequence generative tasks. Matching the Training-Time Compute Our method increases sequence length during training (and thus training compute) by inserting register tokens. To ensure gains are not due to higher compute alone, we train the baselines for more epochs (roughly doubling compute), thus matching or even exceeding the compute of MuToR. As shown in Tables 11 and 12 (Appendix B.1), extending training does not yield further improvements for the baselines and, in some instances, can even degrade performance due to mild overfitting. These results suggest that integrating MuToR into the finetuning pipeline offers more effective approach to leveraging increased training compute. Integration in Parameter-Efficient Finetuning We test our method with LoRA [Hu et al., 2022]: for both Next-Token and our MuToR method, we apply rank-32 adapters to all linear layers (approxi6 Table 1: Downstream accuracy (%) in mathematical reasoning benchmarks. The subheaders refer to the training split used in each experiment. Results for Gemma 2B are averaged over 3 seeded runs. MuToR offers consistent improvement over both standard Next-Token and Multi-Token. Model Method GSM8K MATH AQUA-RAT GSM8K 1M-GSM 2M-GSM 1M-MATH AQUA-RAT Gemma 2B Llama3 8B Next-Token Multi-Token MuToR (ours) Next-Token Multi-Token MuToR (ours) 38.87 40.66 42.10 66.41 66.56 67.85 66.09 66.69 68.33 85.74 85.67 87.03 69.02 69.02 70.56 87.33 86.35 87. 26.73 26.87 28.13 41.4 42.6 43.2 40.16 38.45 41.73 - - - Table 2: Experimental results for finetuning Gemma 2B on abstractive summarization benchmarks. We select the checkpoint with the higher ROUGE-L in the validation set, and report ROUGE scores on the test set. Method SAMSum DialogSum ROUGE-1 ROUGE-2 ROUGE-L ROUGE-1 ROUGE-2 ROUGE-L Next-Token Multi-Token MuToR (ours) 51.47 51.90 52.32 27.29 27.44 28. 43.23 43.50 44.09 47.23 47.98 48.22 20.91 21.23 21.71 38.77 39.25 39.48 mately 39M trainable parameters for Gemma 2B; register tokens in MuToR add negligible number of parameters). As shown in Table 4, our LoRA-MuToR approach improves accuracy over standard LoRA finetuning across both training splits. Interestingly, LoRA-MuToR matches or even exceeds the full-finetuning Next-Token performance, demonstrating its utility in PEFT setups. In contrast, the Multi-Token approach is less compatible with PEFT settings, as it requires training several additional transformer layers from scratch. Impact of Maximum Lookahead Offset The key hyperparameter dmax in our MuToR method controls how many tokens ahead the registers predict. Larger values offer richer supervision but increase task difficulty. Experiments  (Table 3)  show that dmax = 4 is optimal for this particular setting (in general the optimal value may depend on the training data and the downstream task). Notably, as dmax increases, Multi-Tokens performance degrades, barely beating Next-Token when using the 1M-GSM training split. In comparison, MuToRs gains are maintained across dmax values and training split sizes. Shared vs. Different Register Embeddings Per Offset We test whether having different register embeddings per offset improves performance. As seen in Table 5, shared embeddings (Same), which is the default, perform slightly better than distinct ones (Different), suggesting our positional encoding scheme provides sufficient offset information. 4.2 Autoregressive Image Generation Experimental Setup We train LlamaGen-B (111M parameters; Sun et al. 2024) on ImageNet [Deng et al., 2009] at 256256 resolution, using ADMs preprocessing pipeline [Dhariwal and Nichol, 2021]. The dataset is pre-tokenized using VQ-VAE tokenizer from Sun et al. [2024]. We compare three approaches: (1) Next-Token, the standard next-token prediction baseline; (2) MuToR-1D with 1D offsets (as in language modeling); and (3) MuToR-2D with 2D offsets (described in subsection 3.4). Implementation details are included in Appendix A.2. For evaluation, we generate 50,000 images using classifier-free guidance (scale=2.0) [Ho and Salimans, 2022] and compute FID [Heusel et al., 2017], IS [Salimans et al., 2016], Precision, and Recall [Kynkäänniemi et al., 2019] using TensorFlow scripts from Dhariwal and Nichol [2021]. Table 3: Downstream accuracy (%) with respect to the maximum offset dmax, using Gemma 2B. #Add. Param. denotes the additional trainable parameters for Multi-Token and MuToR (in approximation). dmax #Add. Param. Method GSM8K GSM8K 1M-GSM 1 2 3 4 6 - Next-Token 110M Multi-Token 2K MuToR (ours) 220M Multi-Token 2K MuToR (ours) 330M Multi-Token 2K MuToR (ours) 550M Multi-Token 2K MuToR (ours) 38.87 40.66 41.93 40.59 41.60 39.78 42.10 40.23 39. 66.09 66.69 67.15 66.36 68.01 65.53 68.33 65.55 68.16 Table 4: Downstream accuracy (%) in PEFT scenario, using Gemma 2B and LoRA. Method Full-finetuning Next-Token LoRA-Next-Token LoRA-MuToR (ours) GSM8K GSM8K 1M-GSM 38. 36.34 38.59 66.09 66.11 68.11 Table 5: Ablation regarding the register embeddings, using Gemma 2B and dmax = 4. Register embedding GSM8K GSM8K 1M-GSM Same Different 42.10 41.85 68.33 68.18 Results Table 6 shows that both MuToR variants consistently outperform Next-Token in FID and IS across different training iterations. Notably, when comparing under similar training-time compute, the MuToR-2D variant at 100K steps surpasses the Next-Token model at 200K steps, demonstrating both improved performance and faster convergence. Extending the offset to 2D The 2D extension in MuToR-2D significantly improves performance by leveraging spatial dependencies in the image data  (Table 6)  , despite requiring prediction of much more possible future tokens. This demonstrates that the 2D formulation effectively captures structural patterns, providing richer training signal. Scaling down the number of registers during pretraining To reduce computational costs while maintaining performance, we investigate using fewer register tokens in MuToR-2D. Specifically, we test inserting only 80 randomly placed registers per image, increasing the sequence length by just 30%. As shown in Table 7, this sparse version achieves very similar performance to the full setup (with 256 registers) while requiring less computation. These results demonstrate that substantial performance gains can be obtained with relatively few register tokens and only modest increase in training compute. They also highlight untapped potential in MuToRs design, particularly regarding optimal register placement and sparsity strategies, which merit further investigation. Maximum Offset Analysis We examine how expanding the prediction neighborhood in MuToR-2D affects performance by varying dmax_2D, which determines how many future tokens each register must predict. Using 80 randomly placed registers per sequence, we test different dmax_2D values. Table 8 shows that dmax_2D = 4 achieves optimal performance, while increasing it leads to degrading results. In this optimal setup, each register predicts up to 15 future tokensa prediction horizon Table 6: Conditional generation performance on Imagenet 256 256 (cfg scale = 2.0). Both dmax and dmax_2D are set to 4, for MuToR-1D and MuToR-2D respectively. # Iter. Method FID IS Pre. Rec. 100K 200K 360K Next-Token MuToR-1D MuToR-2D Next-Token MuToR-1D MuToR-2D Next-Token MuToR-1D MuToR-2D 7.71 7.01 6.57 6.83 6.43 5.65 6.18 5.79 5.09 8 146.5 155.6 163. 158.4 163.0 183.5 171.5 178.3 195.3 0.830 0.828 0.831 0.833 0.836 0.842 0.841 0.841 0.839 0.439 0.438 0. 0.443 0.444 0.448 0.443 0.441 0.457 Table 7: Ablation using MuToR-2D, dmax_2D = 4, and varying number of registers (# Reg.) inserted in random positions. # Iter. # Reg. FID IS 100K 200K 360K 80 256 80 80 256 6.87 6.57 5.89 5.65 5.03 5.09 162.1 163.2 175.6 183. 195.6 195.3 Table 8: Ablation using MuToR-2D, with varying dmax_2D and 80 registers inserted in random positions. # Targets denote the number of future tokens (d2 max_2D 1) that the registers predict during training (see Figure 3). # Iter. dmax_2D # Targets FID IS 100K 200K 360K 3 4 5 3 4 3 4 5 8 15 24 8 15 24 8 15 24 7.20 6.87 7.08 6.19 5.89 6. 5.56 5.03 5.53 150.8 162.1 154.8 169.0 175.6 166.2 178.3 195.6 179.2 that would require 14 additional transformer heads in prior multi-token approaches, making them computationally impractical. This demonstrates MuToR-2Ds unique ability to effectively leverage long-range predictions while maintaining training efficiency. 4.3 Synthetic Data We further evaluate MuToR on the star-graph path-finding problem [Bachmann and Nagarajan, 2024], which highlights limitations of the next-token prediction objective. Consider directed graph = (n, l), where denotes the number of paths emanating from the start node, ustart, and denotes their length. Given an end node, uend, the model must identify the unique path from ustart to uend. Despite the tasks simplicity, transformer models trained by teacher forcing fail to solve it, due to shortcut learning and the loss of meaningful training signal. We thus investigate whether the look-ahead induced by MuToRs objective can provide the necessary supervision to learn the underlying task. To this end, we experiment with finetuning pretrained GPT2 model [Radford et al., 2019] following the setup from Bachmann and Nagarajan [2024]. The relevant implementation details are provided in the Appendix A.3. Figure 4: Solve rate (%) of finetuned GPT2-L model on different star graph configurations. Results The results are presented in Figure 4. MuToR solves the task across various graph configurations, effectively overcoming the \"cheating phenomenon\" that causes standard teacher forcing to fail. These findings indicate that MuToR can be particularly effective in scenarios where some form of shortcut learning applies, recovering valuable training signal."
        },
        {
            "title": "5 Conclusion",
            "content": "We introduced MuToR, simple yet effective approach to multi-token prediction that leverages interleaved, trainable register tokens to predict future targets. MuToR enables scalable prediction horizons with minimal parameter overhead and is fully compatible with existing pretrained model architectures, allowing seamless integration into standard finetuning pipelines. Empirical results demonstrate that MuToR consistently improves performance in both language modeling taskssuch as mathematical reasoning and summarizationand autoregressive image generation, highlighting its versatility across modalities. This positions MuToR as promising foundation for using token-based lookahead mechanisms to propagate richer supervisory signals during training. It is worth noting that MuToR currently uses uniformly interleaved or randomly positioned register tokensstrategies that may not align optimally with the structure or semantics of specific tasks. While this simple placement scheme has proven effective across modalities, it leaves room for 9 substantial improvement. By learning or adapting the placement of register tokenspotentially guided by model uncertainty or task-specific priorsMuToR could deliver more targeted supervision with fewer auxiliary tokens, further enhancing efficiency and performance."
        },
        {
            "title": "Acknowledgments and Disclosure of Funding",
            "content": "This work has been partially supported by project MIS 5154714 of the National Recovery and Resilience Plan Greece 2.0 funded by the European Union under the NextGenerationEU Program. Hardware resources were granted with the support of GRNET."
        },
        {
            "title": "References",
            "content": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: visual language model for few-shot learning. In NeurIPS, 2022. Gregor Bachmann and Vaishnavh Nagarajan. The pitfalls of next-token prediction. In Proceedings of the 41st International Conference on Machine Learning, pages 22962318, 2024. Sébastien Bubeck, Varun Chadrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. Sparks of artificial general intelligence: Early experiments with gpt-4, 2023. Mikhail Burtsev, Yuri Kuratov, Anton Peganov, and Grigory Sapunov. Memory transformer. arXiv preprint arXiv:2006.11527, 2020. Tianle Cai, Yuhong Li, Zhengyang Geng, Hongwu Peng, Jason Lee, Deming Chen, and Tri Dao. Medusa: Simple llm inference acceleration framework with multiple decoding heads. In Proceedings of the 41st International Conference on Machine Learning, pages 52095235, 2024. Yulong Chen, Yang Liu, Liang Chen, and Yue Zhang. Dialogsum: real-life scenario dialogue summarization dataset. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 50625074, 2021. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. Timothée Darcet, Maxime Oquab, Julien Mairal, and Piotr Bojanowski. Vision transformers need registers. In The Twelfth International Conference on Learning Representations, 2024. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248255. Ieee, 2009. Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in neural information processing systems, 34:87808794, 2021. Li Dong, Nan Yang, Wenhui Wang, Furu Wei, Xiaodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou, and Hsiao-Wuen Hon. Unified language model pre-training for natural language understanding and generation. Advances in neural information processing systems, 32, 2019. Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1287312883, 2021. Arvid Frydenlund. The mystery of the pathological path-star task for language models. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 12493 12516, 2024. 10 Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivière, Mihir Sanjay Kale, Juliette Love, et al. Gemma: Open models based on gemini research and technology. arXiv preprint arXiv:2403.08295, 2024. Bogdan Gliwa, Iwona Mochol, Maciej Biesek, and Aleksander Wawer. Samsum corpus: humanannotated dialogue dataset for abstractive summarization. In Proceedings of the 2nd Workshop on New Frontiers in Summarization, pages 7079, 2019. Fabian Gloeckle, Badr Youbi Idrissi, Baptiste Rozière, David Lopez-Paz, and Gabriel Synnaeve. Better & faster large language models via multi-token prediction. In Proceedings of the 41st International Conference on Machine Learning, pages 1570615734, 2024. Sachin Goyal, Ziwei Ji, Ankit Singh Rawat, Aditya Krishna Menon, Sanjiv Kumar, and Vaishnavh Nagarajan. Think before you speak: Training language models with pause tokens. In The Twelfth International Conference on Learning Representations, 2024. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2), 2021. Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in neural information processing systems, 30, 2017. Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. Edward Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. In International Conference on Learning Representations, 2022. Ioannis Kakogeorgiou, Spyros Gidaris, Konstantinos Karantzalos, and Nikos Komodakis. Spot: Selftraining with patch-order permutation for object-centric learning with autoregressive transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2277622786, 2024. Dawid Kopiczko, Tijmen Blankevoort, and Yuki Asano. Bitune: Bidirectional instruction-tuning. arXiv preprint arXiv:2405.14862, 2024. Tuomas Kynkäänniemi, Tero Karras, Samuli Laine, Jaakko Lehtinen, and Timo Aila. Improved precision and recall metric for assessing generative models. Advances in neural information processing systems, 32, 2019. Yuhui Li, Fangyun Wei, Chao Zhang, and Hongyang Zhang. Eagle: speculative sampling requires rethinking feature uncertainty. In Proceedings of the 41st International Conference on Machine Learning, pages 2893528948, 2024. Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step. In The Twelfth International Conference on Learning Representations, 2023. Chin-Yew Lin. Rouge: package for automatic evaluation of summaries. In Text summarization branches out, pages 7481, 2004. Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blunsom. Program induction by rationale generation: Learning to solve and explain algebraic word problems. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 158167, 2017. 11 Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In NeurIPS, 2023. Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. Giovanni Monea, Armand Joulin, and Edouard Grave. Pass: Parallel speculative sampling. arXiv preprint arXiv:2311.13581, 2023. Ziqi Pang, Tianyuan Zhang, Fujun Luan, Yunze Man, Hao Tan, Kai Zhang, William Freeman, and Yu-Xiong Wang. Randar: Decoder-only autoregressive visual generation in random orders. arXiv preprint arXiv:2412.01827, 2024. Arnaud Pannatier, Evann Courdier, and François Fleuret. σ-gpts: new approach to autoregressive In Joint European Conference on Machine Learning and Knowledge Discovery in models. Databases, pages 143159. Springer, 2024. Jacob Pfau, William Merrill, and Samuel Bowman. Lets think dot by dot: Hidden computation in transformer language models. In First Conference on Language Modeling, 2024. Weizhen Qi, Yu Yan, Yeyun Gong, Dayiheng Liu, Nan Duan, Jiusheng Chen, Ruofei Zhang, and Ming Zhou. Prophetnet: Predicting future n-gram for sequence-to-sequencepre-training. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 24012410, 2020. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter Liu. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of machine learning research, 21(140):167, 2020. Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. Advances in neural information processing systems, 29, 2016. Claude Shannon. mathematical theory of communication. The Bell system technical journal, 27 (3):379423, 1948. Claude Shannon. Prediction and entropy of printed english. Bell system technical journal, 30(1): 5064, 1951. Mitchell Stern, Noam Shazeer, and Jakob Uszkoreit. Blockwise parallel decoding for deep autoregressive models. Advances in Neural Information Processing Systems, 31, 2018. Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. Peize Sun, Yi Jiang, Shoufa Chen, Shilong Zhang, Bingyue Peng, Ping Luo, and Zehuan Yuan. Autoregressive model beats diffusion: Llama for scalable image generation. arXiv preprint arXiv:2406.06525, 2024. Shubham Toshniwal, Wei Du, Ivan Moshkov, Branislav Kisacanin, Alexan Ayrapetyan, and Igor Gitman. Openmathinstruct-2: Accelerating ai for math with massive open-source instruction data. In The Thirteenth International Conference on Learning Representations, 2025. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023a. 12 Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023b. Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in neural information processing systems, 30, 2017. Xinyi Wang, Lucas Caccia, Oleksiy Ostapenko, Xingdi Yuan, William Yang Wang, and AlessanarXiv preprint dro Sordoni. Guiding language model reasoning with planning tokens. arXiv:2310.05707, 2023. Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ Salakhutdinov, and Quoc Le. Xlnet: Generalized autoregressive pretraining for language understanding. Advances in neural information processing systems, 32, 2019. Yongjing Yin, Junran Ding, Kai Song, and Yue Zhang. Semformer: Transformer language models with semantic planning. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 1866918680, 2024. Qihang Yu, Ju He, Xueqing Deng, Xiaohui Shen, and Liang-Chieh Chen. Randomized autoregressive visual generation. arXiv preprint arXiv:2411.00776, 2024."
        },
        {
            "title": "A Implementation Details",
            "content": "In this section, we provide all the training details, as well as the hyperparameters used in our experiments. In order to facilitate reproducibility, we plan to release the code for MuToRs implementation as well. A.1 Language Modeling A.1.1 Mathematical Reasoning Datasets GSM8K comprises approximately 8.7K grade-school level math problems, with around 1.3K of them forming the test set. On the other hand, MATH500 test set consists of 500 problems, uniformally sampled from the original MATH test set [Hendrycks et al., 2021], covering more advanced and diverse mathematical domains. Finally, AQUA-RAT includes multiple-choice math problems with chain-of-thought solutions, split among the training set ( 97K samples), the validation (250 samples) and the test set (250 samples). As mentioned in subsection 4.1, we finetune our base models on standard downstream training datasets, such as GSM8K and AQUA-RAT, as well as on curated subsets derived from OpenMathInstruct-2. The utilized training splits are listed below, including information about the employed filtering and the amount of samples: GSM8K training set ( 7.4K samples), 1M-GSM split (152K samples), obtained by filtering OpenMathInstruct-2s 1M split for grade school math-like problems, 2M-GSM split (277K samples), similarly derived from OpenMathInstruct-2s 2M split using the same grade-school filtering criteria, 1M-MATH split (200K samples), constructed by filtering OpenMathInstruct-2s 1M split for MATH-style problems and randomly sampling 200K of them, AQUA-RAT training set (97K samples). Our filtering depends on the source dataset of each sample, which is included in OpenMathInstruct-2s metadata. Training details Throughout all experiments and for all methods, we use bidirectional attention among the prefix tokens, as proposed in previous works [Dong et al., 2019, Raffel et al., 2020, Kopiczko et al., 2024], since it benefits prefix-answer tasks. We finetune all models for 5 epochs, using AdamW optimizer [Loshchilov and Hutter, 2017] without weight decay and batch size of 10. We also employ learning rate scheduler with linear decay and warmup, setting the peak learning rate to be 5e-5 for Gemma 2B and 2e-5 for Llama 3 8B. Both language models are loaded and finetuned in bfloat16 precision. To match our available resources, we filter out training sequences that are longer than 512 tokens3. Moreover, all experiments with the 2B model are conducted using three random seeds to ensure statistical robustness. This setting is not replicated for the 8B model though, due to the substantial computational resources that are required. Evaluation For evaluation, we assess performance by using greedy decoding and applying exact match techniques to verify the correctness of the generated answer. For experiments on GSM8K and MATH500, the best-performing checkpoint from each run is then used for comparisons. For experiments on AQUA-RAT, we choose the checkpoint with the highest accuracy on the provided validation set. Since register tokens are not used during inference, the inference process is identical across both our method and the baseline approaches. A.1.2 Abstractive summarization Datasets In our experiments, we target SAMSum and DialogSum, two widely used dialogue summarization benchmarks. SAMSum consists of approximately 16K messenger-like conversations with summaries. They are split between the training set (14K samples), the validation set (818 3In 1M-MATH, we keep sequences up to 768 tokens. 14 samples) and the test set (819 samples). On the other hand, DialogSum contains approximately 14K conversation-summary pairs, focusing more on daily-life formal conversations, such as business negotiations. These pairs are split between the training set (12.5K samples), the validation set (500 samples) and the test set (1.5K samples). Training details Across all experiments, we finetune the model for 3 training epochs, following the same setup (optimization details, weights precision), with the mathematical reasoning experiments. We filter out training sequences that are longer than 768 tokens. Evaluation We calculate ROUGE scores against the ground truth reference summaries, select the best checkpoint with respect to ROUGE-L on the validation set, and report ROUGE-1, ROUGE-2 and ROUGE-L scores on the test set. A.1.3 Best performing configurations In Tables 9 and 10, we provide the best performing configurations (dmax and a) for MuToR, across all the training splits that were utilized in our experiments. For Multi-Token, after carefully tuning the same hyperparameters, we found that the optimal configuration across all experiments is dmax = 2, = 0.1. Table 9: MuToRs best performing hyperparameters for Gemma 2B model. Training data dmax GSM8K 1M-GSM 2M-GSM 1M-MATH AQUA-RAT SAMSum DialogSum 4 4 6 3 4 3 4 0.3 0.3 0.1 0.2 0.3 0.5 0.5 Table 10: MuToRs best performing hyperparameters for Llama 3 8B model. Training data dmax GSM8K 1M-GSM 2M-GSM 1M-MATH 3 6 6 0.3 0.1 0.1 0.1 A.2 Autoregressive Image Generation Training details In the autoregressive image generation experiments, we pretrain LlamaGen-B model (approximately 111M parameters). Specifically, the model itself is an autoregressive decoderonly transformer, similar to Llama [Touvron et al., 2023a]. It utilizes RoPE as positional bias, extended to two-dimensions. The model also involves learnable class embeddings, which are used as conditionals for image generation. During training, the class embeddings are dropped with probability of 0.1, to enable the use of classifier-free guidance at inference time. To tokenize the image patches, we use VQ-VAE tokenizer provided by Sun et al. [2024], with codebook size equal to 16384 and embedding dimension equal to 8. Unlike LlamaGen, we pretokenize the dataset with the ADMs preprocessing scheme [Dhariwal and Nichol, 2021], resulting in two crops per image. Both Next-Token baseline and MuToR are trained for 360K update steps, using AdamW optimizer with β1 = 0.9, β2 = 0.95 and weight decay = 0.05. We employ constant learning rate, equal to 0.0004, and batch size of 1024. For MuToRs implementation, we empirically tune the loss coefficient to be 0.5, so that Lreg has equal contribution with Lntp ( Equation 6). This indicates that the auxiliary loss provides valuable supervision that the model leverages during pretraining, to improve its learned representations. In the experiments that fewer registers are inserted, we sample random positions for each training sample. Evaluation To benchmark generative performance, we sample 50,000 images at 256 256 resolution, setting temperature = 1.0 and fixed random seed for fair comparison. We use classifier-free guidance [Ho and Salimans, 2022] with scale = 2.0, which was reported as an optimal value in the original LlamaGen. Thus, the logit lg is formed as such: lg = lu + 2(lc lu), where lu denotes the unconditional logit (with the class embedding dropped) and lc denotes the conditional logit. Then, 15 the generated samples are used to calculate the performance metrics, using the Tensorflow scripts from Dhariwal and Nichol [2021]. A.3 Synthetic Data We setup our experiments using the official implementation from Bachmann and Nagarajan [2024] (regarding the models architecture and optimization). All model configurations (Next-Token and MuToR) are trained for sufficient number of epochs. MuToRs implementation for the star graphs problem follows subsection 3.3, with task-specific modification; we sample the offset from {2, . . . , dmax}, thus excluding the next-token from the registers prediction. In this way, we prevent the registers from learning the \"Clever Hans Cheat\", and enable them to focus on planning look-ahead predictions. In these experiments, we set dmax = 4 and = 0.5 for the graphs with path length = 5 and dmax = 6, = 0.3 for graphs with path length = 10, which are more challenging."
        },
        {
            "title": "B Additional experiments",
            "content": "B.1 Matching the training compute Tables 11 and 12 report the results of finetuning baseline methods for twice the number of training epochs than MuToR, in order to investigate whether improved downstream performance can be derived from increasing the training compute. Due to our limited computational resources, we conduct these experiments using Gemma 2B model. Table 11: Downstream accuracy (%) in mathematical reasoning benchmarks across different number of training epochs. The subheaders refer to the training split used in each experiment. Results are averaged across three seeded runs. Method # Epochs GSM8K MATH500 GSM8K 1M-GSM 1M-MATH Next-Token Multi-Token MuToR 5 5 10 5 38.87 37.91 40.66 39.98 42.10 66.09 65. 66.69 64.92 68.33 26.73 27.07 26.87 26.73 28.13 Table 12: Rouge metrics comparison across different number of training epochs. We select the checkpoint with the higher ROUGE-L in the validation set, and report ROUGE scores on the test set. Dataset Method #Epochs ROUGE-1 ROUGE-2 ROUGE-L SAMSum DialogSum Next-Token Multi-Token MuToR Next-Token Multi-Token MuToR 51.47 51. 51.90 51.04 52.32 47.23 46.92 47.98 47.46 48.22 27.29 27. 27.44 26.35 28.08 20.91 20.39 21.23 20.76 21.71 43.23 43. 43.50 42.48 44.09 38.77 38.25 39.25 38.97 39.48 3 3 5 3 3 5 3 5"
        }
    ],
    "affiliations": [
        "Archimedes, Athena Research Center",
        "IACM-Forth",
        "University of Crete",
        "valeo.ai"
    ]
}