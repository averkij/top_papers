{
    "paper_title": "ScaleCUA: Scaling Open-Source Computer Use Agents with Cross-Platform Data",
    "authors": [
        "Zhaoyang Liu",
        "JingJing Xie",
        "Zichen Ding",
        "Zehao Li",
        "Bowen Yang",
        "Zhenyu Wu",
        "Xuehui Wang",
        "Qiushi Sun",
        "Shi Liu",
        "Weiyun Wang",
        "Shenglong Ye",
        "Qingyun Li",
        "Zeyue Tian",
        "Gen Luo",
        "Xiangyu Yue",
        "Biqing Qi",
        "Kai Chen",
        "Bowen Zhou",
        "Yu Qiao",
        "Qifeng Chen",
        "Wenhai Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Vision-Language Models (VLMs) have enabled computer use agents (CUAs) that operate GUIs autonomously, showing great potential, yet progress is limited by the lack of large-scale, open-source computer use data and foundation models. In this work, we introduce ScaleCUA, a step toward scaling open-source CUAs. It offers a large-scale dataset spanning 6 operating systems and 3 task domains, built via a closed-loop pipeline uniting automated agents with human experts. Trained on this scaled-up data, ScaleCUA can operate seamlessly across platforms. Specifically, it delivers strong gains over baselines (+26.6 on WebArena-Lite-v2, +10.7 on ScreenSpot-Pro) and sets new state-of-the-art results (94.4% on MMBench-GUI L1-Hard, 60.6% on OSWorld-G, 47.4% on WebArena-Lite-v2). These findings underscore the power of data-driven scaling for general-purpose computer use agents. We will release data, models, and code to advance future research: https://github.com/OpenGVLab/ScaleCUA."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 1 ] . [ 1 1 2 2 5 1 . 9 0 5 2 : r Technical Report SCALECUA: SCALING OPEN-SOURCE COMPUTER USE AGENTS WITH CROSS-PLATFORM DATA Zhaoyang Liu*, Jingjing Xie*, Zichen Ding*, Zehao Li*, Bowen Yang*, Zhenyu Wu*, Xuehui Wang, Qiushi Sun, Shi Liu, Weiyun Wang, Shenglong Ye, Qingyun Li, Xuan Dong, Yue Yu, Chenyu Lu, YunXiang Mo, Yao Yan, Zeyue Tian, Xiao Zhang, Yuan Huang, Yiqian Liu, Weijie Su, Gen Luo, Xiangyu Yue, Biqing Qi, Bowen Zhou, Kai Chen, Yu Qiao, Qifeng Chen(cid:12), Wenhai Wang(cid:12) Shanghai AI Laboratory Figure 1: Performance comparison. The top row showcases performance overview on GUI-centric benchmarks. The bottom row demonstrates the consistent improvements from our collected data."
        },
        {
            "title": "ABSTRACT",
            "content": "Vision-Language Models (VLMs) have enabled computer use agents (CUAs) that operate GUIs autonomously with great potential. However, developing robust CUAs requires extensive in-domain knowledge about software interfaces and operations. Unlike imagetext pairs that are widely available on the Internet, computer-use data, particularly operation trajectories, are rare, costly to collect. Consequently, progress in this field remains constrained by both data scale and the limited transferability of existing VLMs. In this work, we introduce ScaleCUA, step toward scaling open-source CUAs. It offers large-scale dataset spanning 6 operating systems and 3 task domains, via closed-loop pipeline uniting automated agents with human experts. Trained on this scaled-up data, ScaleCUA can operate seamlessly across platforms. Specifically, it delivers strong gains over baselines (+26.6 on WebArena-Lite-v2, +10.7 on ScreenSpot-Pro) and sets new state-of-theart results (94.4% on MMBench-GUI L1-Hard, 60.6% on OSWorld-G, 47.4% on WebArena-Lite-v2). These findings underscore the power of data-driven scaling for general-purpose cross-platform CUAs. We release data, models, and code to advance future research: https://github.com/OpenGVLab/ScaleCUA. * equal contribution; (cid:12) corresponding authors. 1 Technical Report"
        },
        {
            "title": "INTRODUCTION",
            "content": "Humans are able to interact with digital environments through graphical user interfaces (GUI) to acquire information and accomplish tasks efficiently. The recent advances in Vision-Language Models (VLMs), which exhibit powerful capabilities in visual perception and task planning, have made it increasingly feasible to automate such interactions. Consequently, recent research has increasingly focused on computer use agents, also referred to as GUI agents, aiming to autonomously operate desktop, mobile, and web platforms by relying exclusively on visual observations. Some computer use agents (CUAs) (Qin et al., 2025; Anthropic, 2025; Hong et al., 2025; OpenAI, 2025) demonstrate strong performance, while they are typically built on closed-source models or inaccessible proprietary datasets. More fundamentally, effective computer use requires rich in-domain knowledge of software and operational procedures, which remains substantial gap for current foundation models. Unlike imagetext pairs that are abundantly available on the Internet, computeruse data, particularly fine-grained action trajectories, are scarce, costly to collect, expensively to annotate and not naturally archived online. Furthermore, as software, web pages and operating systems evolve rapidly, existing trajectories face the risk of obsolescence, further limiting their utility. These challenges result in significant bottleneck for scaling CUAs in both data scale and model generalizability. To tackle these limitations,we make significant efforts on two aspects: (a) constructing large-scale, cross-platform GUI-centric training corpus, and (b) developing family of scalable, versatile foundation models for general-purpose computer use agents. We first present Cross-Platform Interactive Data Pipeline composed of two synergistic loops. The Agent-Environment Interaction Loop enables automated agents to interact with diverse GUI environments, while the Agent-Human Hybrid Data Acquisition Loop integrates expert-annotated trajectories to ensure coverage and quality. The pipeline spans six major platforms, including Windows, macOS, Linux, Android, iOS, and Web, which facilitates the collection of rich screen-state observations, metadata (e.g., A11y Trees, XLM, DOM structures, etc.), and raw trajectories. In this pipeline, we design unified action space, allowing for more consistent and efficient interaction with diverse real-world environments. Leveraging this infrastructure, we curate and annotate comprehensive training dataset for open computer use dataset, comprising three major task families: (a) GUI Understanding with 471K examples covering regional captioning, OCR, and layout comprehension, etc.; (b) GUI Grounding with 17.1M training samples supporting more accurate UI element localization; and (c) Task Completion with over 15K weak-semantic trajectories and 4K high-level goal-directed trajectories. We use advanced VLMs such as GPT-4o and Claude-3.7-Sonnet to annotate collected data and also explore data augmentations to further enhance diversity. To this end, we establish fully open and diverse training dataset. Building upon this corpus, we train series of base agent models termed as ScaleCUA with Qwen2.5VL (Bai et al., 2025). Our ScaleCUA support three distinct inference paradigms to offer enhanced flexibility and compatibility with various agent frameworks: (a) Grounding Mode, which focuses on precisely locating UI elements based on textual descriptions, allowing for efficient integration with more powerful reasoning planners in modular agent setups, (b) Direct Action Mode, which enables highly efficient task completion by directly generating low-level executable actions without consuming additional tokens for intermediate reasoning and (c) Reasoned Action Mode, which enhances task planning accuracy by first generating thought process based on current observations and historical context before generating the following action. The unified action space designed in our data construction enables our agents to interface with heterogeneous environments through standardized control schema. We conduct extensive empirical studies to investigate how different data sources, diverse training tasks, agent designs, etc., influence agent performance. Our findings highlight the benefits of data augmentation, weak semantic trajectories, and general reasoning data for enhancing planning capabilities. Moreover, we evaluate various agent paradigms across multiple GUI agent benchmarks and make systematic comparisons between agentic workflows and native models. As previous studies (Xu et al., 2024; Qin et al., 2025; OpenAI, 2025; Anthropic, 2025) also probe into the important research questions with limited open-sourced training data or under closed conditions with proprietary data, our investigations aim to provide foundational and unified insights for advancing vision-language-based computer automation. Our contributions are summarized as follows: Technical Report 1) We curate cross-platform computer use dataset, collected via an interactive data pipeline that integrates automated agents with human experts. It covers six major platforms (Windows, macOS, Linux, Android, iOS, and Web) and three GUI-centric task domains (i.e., understanding, grounding and task completion), which provide robust foundation for studying and training universal CUAs. 2) We develop ScaleCUA, family of robust base agent models that unify perception, reasoning, and action into single model. It supports flexible inference paradigms including grounding, direct action, and reasoned action, along with unified action space for seamless cross-platform interaction. 3)We conduct comprehensive evaluation spanning understanding, grounding, and end-to-end task completion across several cross-platform online benchmarks. The experiments not only demonstrate that our agents can achieve state-of-the-art or competitive performance, but also provide insightful directions for developing more powerful computer use agents."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Vision-Language Models (VLM). Vision-Language Models (VLMs) have experienced substantial development in recent years, with progress spanning both proprietary API services (Team et al., 2023; 2024; Anthropic, 2024a; xAI, 2025; OpenAI, 2023; Hurst et al., 2024) and open-source models (Wang et al., 2024b; Bai et al., 2025; Chen et al., 2024c;b; Zhu et al., 2025; Xiaomi, 2025; Team et al., 2025a; MetaAI, 2025), expanding their capabilities to wide range of tasks. Some general VLMs (Team et al., 2025c; Guo et al., 2025; Bai et al., 2025; Zhu et al., 2025; Xiaomi, 2025; Wang et al., 2025a) integrate extensive GUI knowledge during pre-training or SFT stages, and thus acquire explicitly computer use capabilities. However, while some works (Hurst et al., 2024; Anthropic, 2024a; Guo et al., 2025; Zhu et al., 2025; Bai et al., 2025) demonstrate strong generalization and planning capabilities in vision-language tasks, they still struggle in solving easy computer use tasks. Furthermore, the GUI corpora employed to train these models remain largely proprietary. GUI Agents. Recent advances in general-purpose vision-language models (VLMs), such as GPT4o, have enabled the development of modular GUI agents that decompose decision-making and perception into two distinct roles: planning and grounding. These agents typically operate within the plannergrounder paradigm (Cheng et al., 2024; Hong et al., 2024; Lu et al., 2024b; Yu et al., 2025; Wu et al., 2025; Gou et al., 2024; Zhang et al., 2025c; Wu et al., 2024b; Zhou et al., 2025), where VLM-based planner predicts the next high-level operation and its associated object description, and specialized GUI grounding model localizes this object on the interface. Yang et al. (2024) further incorporates action histories into grounding, allowing the grounder to adapt to contextual GUI dynamics. Inspired by general-purpose agentic workflows (Wu et al., 2023b; Li et al., 2023; Hong et al., 2023), these works (Wu et al., 2024a; Liu et al., 2025a; Zhao et al., 2025; Agashe et al., 2025; Chen et al., 2025b) extend this design to GUI environments by coordinating multiple agents across planning, reflection, and memory-augmented decision-making. Despite strong performance, agentic workflows typically exhibit high computational latency and significant token consumption, where their performance is fundamentally constrained by the capabilities of underlying VLMs. In contrast to modular agents, native computer use agents (Xu et al., 2024; Wu et al., 2024b; Sun et al., 2024b; Qin et al., 2025; Luo et al., 2025; Liu et al., 2025b; Sun et al., 2025) integrate planning and grounding into unified model trained end-to-end. These agents directly predict low-level executable actions from raw visual inputs. For example, AGUVIS (Xu et al., 2024) and UI-TARS (Qin et al., 2025) are trained on extensive task trajectories and demonstrate impressive reasoning and adaptability. Generally, native agents offer tighter alignment between perception and action, and modular agents and agentic workflows can also benefit from improvements of native agents. In this work, we aim to not only focus on training base models for cross-platform GUI agents, but also commit to open-sourcing all data, accelerating development across the entire domain. GUI Datasets. Open-source datasets have significantly advanced GUI agent development by capturing diverse forms of interaction, visual perception, and instruction-following behaviors across platforms. For mobile environments, RICO (Deka et al., 2017) provides 70k+ Android UI screens with gesture traces, while AITW (Rawles et al., 2023) offers 715k human demonstrations aligned with 30k natural language commands. Moreover, AitZ (Zhang et al., 2024) constructs 18,643 screen-action pairs together with chain-of-action-thought annotations. In the web domain, MiniWoB (Shi et al., 2017) simulates diverse web-based tasks with interaction sequences and rewards. WebShop (Yao et al., 2022) provides language-driven e-commerce tasks with human trajectories over real product catalog, 3 Technical Report and Mind2Web (Deng et al., 2023) scales this idea to 137 real websites, offering 2,350 long-horizon open-ended tasks. As for desktop platforms, Xie et al. (2024) constructs 4 million-example dataset synthesized via interface decomposition, boosting grounding accuracy. He et al. (2025) collect 312 human-annotated trajectories augmented via trajectory boost. Furthermore, some works explore scalable data generation. For example, OS-Genesis (Sun et al., 2024b) synthesizes mobile/web trajectories via exploration, while AGUVIS (Xu et al., 2024) curates large-scale dataset with multimodal grounding and reasoning annotations derived from heterogeneous open-source corpora. In addition, due to the high acquisition costs of GUI interaction trajectories, several efforts focus on collecting GUI tutorial datasets to mitigate the scarcity of data. META-GUI(Sun et al., 2022) proposes dialoguebased annotations within tutorials. TongUI(Zhang et al., 2025a) collects 143k task trajectories from instructional sources (e.g., YouTube, WikiHow), linking instructions with screenshots and actions. GUI-World (Chen et al., 2025a) provides over 12k screen-recorded GUI videos to facilitate temporal understanding of GUI dynamics. Despite these advances, existing GUI corpora remain limited in both coverage and diversity, particularly for desktop environments. The data scarcity poses significant challenges, especially in UI element grounding and multi-step task planning."
        },
        {
            "title": "3 CROSS-PLATFORM INTERACTIVE DATA PIPELINE",
            "content": "A critical challenge in developing robust computer use agents is insufficient training data with high quality and diversity in the computer use domain. Collecting computer use trajectories is particularly costly and inefficient, primarily due to the dynamic nature of environments and their frequent dependency on task-specific resource files, which severely limit the feasibility of fully automated data collection pipelines. In this section, we elaborate on the pipeline of data acquisition and annotation for training computer use agents. 3.1 OVERVIEW Figure 2: Cross-Platform Interactive Data Pipeline. Our Pipeline consists of two synergistic loops: (1) the Agent-Environment Interaction Loop, where agents interact with multi-platform GUI environments (including desktop, mobile, and web) via observation and action; and (2) the Agent-Human Hybrid Data Acquisition Loop, where both autonomous agents and human experts contribute to collecting raw trajectories, including screenshots and structural metadata. The resulting trajectories are then annotated and transformed into training corpora for tasks such as GUI understanding, GUI grounding, and sequential action modeling. 4 Technical Report Existing datasets in the computer use domain predominantly fall into two categories: manual trajectory collection and automated search-based exploration. Manual collection (Zhang et al., 2024; Rawles et al., 2023; Deng et al., 2023; Lu et al., 2024a), although capable of producing high-quality trajectories, is prohibitively expensive, labor-intensive, and inherently difficult to scale. Conversely, the automated exploration method (Sun et al., 2024b), while scalable, generates trajectories with considerable noise. Therefore, neither strategy alone can effectively provide the quality and diversity required for training versatile GUI agents. Motivated by these limitations, we propose Cross-Platform Interactive Data Pipeline that strategically integrates agents and humans into data acquisition. As illustrated in Fig. 2, the pipeline operates in two synergistic loops: the Agent-Environment Interaction Loop and the Agent-Human Hybrid Data Acquisition Loop. The former involves agents interacting with multi-platform GUI environments, while the latter integrates trajectories collected by both autonomous agents and human experts. These trajectories, comprising screenshots and structural metadata, are then annotated and transformed into training corpora for tasks like GUI understanding, grounding, and sequential action modeling. This dual-loop framework ensures comprehensive coverage of GUI states while maintaining high data quality, laying robust foundation for training generalizable computer use agents. Agent-Environment Interaction Loop. Initially, we establish unified interface across heterogeneous platforms, including Windows, Ubuntu, macOS, Web browsers, Android, and iOS, to standardize observation acquisition and action execution. This unified abstraction facilitates closed-loop data collection and provides flexible interface compatible with various agent architectures, significantly enhancing the scalability and efficiency of online evaluations. To construct our dual-mode data collection framework, we investigate platform-specific methods for extracting GUI metadata. For desktop environments (Windows, Ubuntu, macOS), interface layouts and UI element coordinates are generally accessible via Accessibility Trees (A11y Trees). Web platforms provide element information through Document Object Model (DOM) structures rendered by browsers. For Android applications, UI element locations are available through parsed XML layout files. However, we observe that A11y trees in the desktop platforms often contain missing attributes, and iOS/iPadOS systems restrict access to layout information. For these cases, we employ OmniParser (Yu et al., 2025) to estimate bounding boxes of UI elements, balancing noise tolerance with efficiency gains. Agent-Human Hybrid Data Acquisition Loop. After unifying the interface for interacting with diverse environments, the next natural question is how to deploy automated agents to collect interaction trajectories while ensuring maximal coverage of GUI states. We experimented with two distinct exploration strategies. The first employed VLM-driven agents, i.e., GPT-4o, Claude, Gemini, etc., to navigate the interface. However, due to inherent biases in these models, the resulting trajectories lacked sufficient diversity. In contrast, the second strategy utilized rule-driven random-walk agent. Our rule-driven agents perform exploration using depth-first search (DFS) strategy, where an action is randomly selected from the available action space at each step. To improve efficiency, heuristic pruning is applied to reduce redundant or uninformative branches during search. The detailed design of this strategy is provided in the A.5. After deduplication, this simple method achieves substantially broader GUI coverage. Although the resulting trajectories often lack clear high-level goals at the macro scale, many of these sub-sequences still provide valuable supervised signals for agent model learning. Despite the efficiency gains from automation, extensive quality assessments have revealed considerable noise in both system-derived metadata and vision-based bounding box predictions. Therefore, to mitigate noise and ensure high-quality annotations, we complement automated collection with expert-curated trajectories. To streamline expert, we build unified cross-platform data acquisition system where the frontend interaction interface is decoupled from the backend environment, enabling annotators to efficiently switch between platforms. Experts are asked to complete domain-specific goals spanning productivity, entertainment, and system operations. Combining both agent-driven and expert-driven styles yields substantial corpus of raw GUI trajectories, which comprises extensive screenshots along with corresponding metadata. Subsequently, we apply advanced VLMs (e.g., GPT-4o (Hurst et al., 2024) and Claude-3.5 (Anthropic, 2024a) for data annotation, categorizing the corpus into three primary GUI-centric tasks: Understanding, Grounding, and Task Completion. This comprehensive dataset ultimately provides robust foundation for training generalizable computer use agents. Unified Action Space. As shown in Table 13, we establish cross-platform action space covering three major environments: Desktop, Browser, and Mobile. This unified interface enables consistent behavior modeling and facilitates training across heterogeneous platforms. It includes core 5 Technical Report Table 1: Datasets comparisons on computer-use datasets in terms of platform coverage, task types (Understanding, Grounding and Trajectories), and collection methods. Data source SeeClick (2024) GUIEnv (2024a) WebUI (2023a) Widget Captioning (2020b) RicoSCA (2020a) RICO (2017) OmniACT (2024) MM-Mind2Web (2024) GUIAct (2024a) AitZ (2024) AndroidControl (2024a) GUI Odyssey (2024a) AMEX (2024) AitW (2023) OS-Atlas (2024b) OS-Genesis (2024b) JEDI (2025) AgentNet (2025b) Ours Platform Understanding Grounding Trajectories Desktop Mobile Web #Samples #Elements #Samples Avg. Steps Collection Method 877K 471K 763K 10.7M 400K 163K 178K 72K 9.8K 67K 13.6M 3.1M 17.1M 1K 5.7K 2.5K 13.6K 7.7K 3.0K 2.3K 2.5K 22K 19.0K 7.7 6.7 6.0 5.5 15.3 11.9 8.1 6.4 9.0 Auto Auto Auto Human Auto Hybrid Auto Human Auto Human Human Human Human Human Auto Auto Auto Auto Hybrid subset of universal operations (click, write, etc.) and platform-specific actions (long press, open app, etc. for mobile) are essential for mobile environments. unified action space allows agents to generalize core behaviors while retaining the flexibility to exploit environment-specific actions. This design also simplifies data annotation and downstream policy learning, as each action is expressed in platform-aware but semantically consistent manner. More details can refer to Sec. A.3. 3.2 TASK-ORIENTED DATA CURATION Combining both agent-driven and expert-driven styles yields substantial corpus of raw GUI trajectories, which comprises extensive screenshots along with corresponding metadata. Subsequently, we apply advanced VLMs (e.g., GPT-4o (Hurst et al., 2024) and Claude-3.7 (Anthropic, 2025)) for data annotation, categorizing the corpus into three primary tasks: GUI Understanding, GUI Grounding, and Annotated Trajectories. Based on the annotated GUI-centric corpora, we aim to build an Open-sourced Computer Use Dataset (ScaleCUA-Data). This comprehensive dataset ultimately provides robust foundation for training generalizable computer use agents. GUI Understanding. We design set of tasks to facilitate fine-grained perception and reasoning over GUIs. At the element level, the model learns to describe visual appearance, extract textual content (Referring OCR), understand spatial layout, infer functional semantics, and anticipate user intent. At the screenshot level, we include Interface Captioning to summarize global layout and content, and Screen Transition Captioning to describe state changes over time. These tasks serve as foundation for training agents with strong interface comprehension. GUI Grounding. Grounding tasks enable agents to map natural language instructions to corresponding interface regions. We formulate three types of grounding supervision: (1) point grounding identifies precise click locations, (2) bounding box grounding localizes regions for region-based operations, and (3) action grounding connects spatial targets with low-level commands. All grounding tasks are derived from previously annotated understanding data and further expanded via LLM-driven augmentation to enhance coverage and generalization. Weak-Semantic Trajectories. Rule-based agents perform depth-first exploration to cover diverse GUI states. Although these trajectories lack explicit task intent, we segment them into sub-trajectories based on screen similarity, yielding weakly semantic interaction flows. These sequences capture common GUI patterns with low-cost supervision for learning general interface navigation. Human-Curated Trajectories. To complement automated data, we employ expert operators to collect realistic, goal-directed demonstrations. Tasks are selected from daily, entertainment, and productivity domains. To this end, we develop unified cross-platform system that isolates the environment for each platform. These curated trajectories provide high-quality supervision for reasoning and planning, and serve as reference signals to align model behavior with human intent. Technical Report Annotation and Augmentation. Each screenshot is paired with metadata that includes UI layout structures. We deduplicate frequent states and sample diverse interactive elements, which are annotated using VLMs to generate appearance, position, and function descriptions. These annotations support QA and grounding tasks. To increase data variety, we apply augmentation techniques including element cropping, synthetic resolution scaling, and background substitution. Trajectorylevel annotations are enriched through reasoning-based prompts that generate instructions, thought chains, and alternative actions. All prompt templates are detailed in the A.9. 3.3 DATA STATISTICS Our collected corpus spans three major task domains: Understanding, Grounding, and Task Planning. As summarized in Table 1, the dataset contains 471K understanding examples across diverse tasks such as VQA, OCR, layout understanding, and screen transition reasoning, etc.For grounding, we curate over 17.1M annotations with multiple formats (bounding box, point, and action), enabling fine-grained alignment between textual instructions and GUI elements. In addition, we provide 19K trajectories for task planning, covering both human-curated and enhanced trajectories, with an average of 9 steps per trajectory. Fig. 3 illustrates the distribution across platforms, where desktop tasks dominate due to broader application coverage and richer interface diversity, while mobile and web environments are also substantially represented. Compared with existing resources, our dataset  (Table 1)  is the most comprehensive, covering three task families across desktop (Windows, Linux, macOS), mobile, and web. Prior datasets (Deka et al., 2017; Chen et al., 2024a; Wu et al., 2024b) typically focus on single task domain, whereas our dataset integrates heterogeneous supervision signals into unified format. Figure 3: Data distribution of our dataset. Discussions. This scale and diversity enable balanced evaluation of computer use agents under both perception and action paradigms. By leveraging dual-loop collection pipeline, we ensure coverage of low-level element recognition, mid-level grounding, and high-level task planning. Compared with current works (Sun et al., 2024b; Wu et al., 2024b; Zhang et al., 2024; Rawles et al., 2023), we explore more diverse data collection strategies (human experts, VLM-driven agents, and rule-driven agents) and cover broader range of platforms (desktop, mobile, and web). Specifically, as for the random-walk agent, we designed more efficient algorithm through extensive experimentation and iterative improvements, significantly enhancing both data collection efficiency and GUI coverage. With this pipeline, we have collected over 2M raw screenshots across multiple platforms. Furthermore, we make substantial efforts on training robust computer use agents by constructing diverse proxy tasks spanning understanding, grounding and planning."
        },
        {
            "title": "4 SCALING UP COMPUTER USE AGENTS",
            "content": "4.1 TASK DEFINITION CUAs have emerged for programmatic automation. Early approaches, such as Selenium1, which leverages DOM parsing for web automation, and AutoHotkey2, which scripts deterministic desktop interactions, enabled precise but constrained interface control. Large-scale VLMs have significantly broadened this paradigm, allowing agents (Cheng et al., 2024; Wang et al., 2024a; Zhang et al., 2025b) to achieve pixel-level perception and interaction on graphical user interfaces. The interaction between the agent and environment at time step can be simply defined as follows: at = πθ(task, ot, h<t), ot+1 = E(at), (1) 1https://github.com/SeleniumHQ/selenium 2https://github.com/AutoHotkey/AutoHotkey 7 Technical Report Figure 4: Three Inference Paradigms of Our Computer Use Agents: (1) Grounding Mode, which focuses on identifying target UI elements with their spatial coordinates and bounding boxes; (2) Direct Action Mode, where the agent solely generates executable actions based on current observations and instructions; and (3) Reasoned Action Mode, where the agent first generates chain-of-thought rationale before producing structured actions. These modes enable varying levels of functionality for computer use agents to complete tasks. where π denotes the agent model parameterized by θ, and represents the environment, such as virtual machines or Docker containers. The task denotes the task instruction. The observation encompasses elements such as raw screen pixels, accessibility trees, or DOM data. The history h<t = {(a0, o0), . . . , (at1, ot1)} provides context for agents decision-making process. Some works (Sun et al., 2024b; Xu et al., 2024) choose to generate natural language descriptions for (ai, oi) as history, as it can save large amount of inference cost budget. Each action specifies an operation with corresponding arguments, as detailed in Table 13, which is then executed in the environment. In this work, we persist in modeling computer use tasks based on screenshots. This paradigm aligns with human behavior and effectively avoids interference from noisy accessibility Tree and DOM data. 4.2 BASE AGENT MODELS To train robust computer use agents, we leverage the state-of-the-art Vision-Language Model, Qwen2.5-VL (Bai et al., 2025), as our base model. It is selected for its strong multimodal understanding capabilities and scalability across different GUI platforms. As illustrated in Fig. 4, the models operate in three modes: 1) Grounding Mode, where the model focuses on UI grounding from the current observation and instruction. 2) Direct Action Mode, where the model directly generates low-level instructions with executable actions from the current observation and historical context. 3) Reasoned Action Mode, where the model first produces reasoning process before generating the action. The outputs generated by models are formatted in XML tags. Grounding Mode represents the model only focuses on the UI element localization. It leverages visual observation to guide the model toward generating either spatial points, bounding boxes, or coordinate-referenced actions (e.g., click, doubleClick,moveTo, etc.), facilitating precise grounding. In this mode, the model only takes screenshots and instructions as inputs. When deployed, this paradigm enables the powerful general-purpose VLMs (e.g., GPT-4o) acting as planners and our model functioning as grounder, to complete computer use tasks in modular paradigm. Direct Action Mode requires the model to efficiently predict executable actions directly, without generating any intermediate reasoning steps or extraneous outputs. Given task instruction, the current screen observation, and the interaction history, the model outputs both low-level instruction and 8 Technical Report an executable action. The low-level instruction provides concise natural language description that explains the intended operation (e.g., Open the settings menu), while the action within <action> tags corresponds to concrete low-level control command (e.g., click(x=213, y=234)). The generated low-level instruction is appended to the dialogue history and serves as human-interpretable descriptions that serve as historical context for subsequent steps, allowing the model to maintain context over multi-step interactions. Notably, the low-level instruction and actions are wrapped in <operation>...</operation> and <action>...</action>. This mode enables rapid task execution by eliminating the need for explicit intermediate thought or reasoning steps before action. It supports streamlined perception-action loop where the model continuously observes the environment and emits grounded actions. By integrating minimal but informative linguistic feedback into the interaction history, the agent can sustain coherent behavior while reducing inference latency. Empirically, this design is able to complete complex GUI tasks efficiently, particularly in real-time tasks (e.g., online games) where fast and reactive behavior is critical. Reasoned Action Mode refers to chain-of-thought process before generating actions. In this mode, the input to the model remains consistent with the Direct Action Mode. The reasoning trace explicitly outlines the agents interpretation of the task objective and the rationale behind selecting particular action, often referencing visual cues from the screen and correlating them with the user instruction. For example, the model might reason, The task is to adjust brightness. see settings icon on the top right, which likely leads to display settings, followed by proposed action such as click(x=1040, y=75). This intermediate thought process wrapped in <think>...</think> tags enhances action reliability, especially in ambiguous or multi-step tasks where context comprehension and logical grounding are critical. Although this mode introduces higher computational cost and latency due to the extended generation process, it significantly improves the quality and interpretability of the agents behavior. The explicit reasoning provides human-understandable justifications for each step, enabling fine-grained analysis of decision patterns and failure cases, which can be used to iteratively refine the models reasoning heuristics. By incorporating structured thought before action, it empowers agents to perform complex tasks with transparency, making it particularly suitable for long-horizon interaction scenarios where precision and explainability are essential. 4.3 TRAINING RECIPES We adopt different configurations for each ScaleCUA model according to model size and available hardware resources. For the ScaleCUA-3B model, we use mini-batch size of 4 with gradient accumulation of 1, trained on 128 A100 GPUs. The ScaleCUA-7B model follows the same batch configuration (mini-batch size 2, gradient accumulation 2) and GPU setup. For the ScaleCUA-32B model, due to its significantly larger parameter count, we reduce the mini-batch size to 2 while applying gradient accumulation of 2 to maintain an effective batch size of 4. Training is conducted on 128 H200 GPUs to accommodate the higher memory footprint. We use learning rate of 1.0 105 and the maximum token length of 40960 for all models. Across all models, we experiment with varying proportions of general-purpose multimodal data in the training corpus. Specifically, the 3B, 7B, and 32B models are trained with general data ratios of 25%, 50%, and 75%, respectively. In the experiments, we observe that when general-purpose data accounts for 25% of the GUI-specific training corpus, the 3B model achieves strong performance on GUI agent benchmarks after training. Considering the larger models are able to memorize more data samples, we raise this ratio to 50% for the 7B model and further to 75% for the 32B model."
        },
        {
            "title": "5 EXPERIMENTS",
            "content": "5.1 EVALUATION SETUP To comprehensively assess the capabilities of our computer use agents, the evaluations span understanding, grounding, and agent capability of task completion. Please note that our models are evaluated with pure visual observation on all benchmarks. To validate GUI understanding performance, we resort to MMBench-GUI L1 (Wang et al., 2025c), which evaluates an agents fine-grained perception and reasoning about interface content across diverse platforms by posing multiple-choice questions. It can serve as standardized test of basic GUI understanding without requiring explicit action execution. 9 Technical Report Table 2: Results on MMBench-GUI L1 (GUI Content Understanding) (Wang et al., 2025c). Model Windows MacOS Linux iOS Android Web Overall GPT-4o (2024) Claude-3.5 (2024a) Claude-3.7 (2025) Qwen-Max-VL (2023) Qwen2.5-VL-72B (2025) UI-TARS-72B-DPO (2025) InternVL3-72B (2025) GUI-Owl-7B (2025) GUI-Owl-32B (2025) ScaleCUA-3B ScaleCUA-7B ScaleCUA-32B GPT-4o (2024) Claude-3.5 (2024a) Claude-3.7 (2025) Qwen-Max-VL (2023) Qwen2.5-VL-72B (2025) UI-TARS-72B-DPO (2025) InternVL3-72B (2025) GUI-Owl-7B (2025) GUI-Owl-32B (2025) ScaleCUA-3B ScaleCUA-7B ScaleCUA-32B GPT-4o (2024) Claude-3.5 (2024a) Claude-3.7 (2025) Qwen-Max-VL (2023) Qwen2.5-VL-72B (2025) UI-TARS-72B-DPO (2025) InternVL3-72B (2025) GUI-Owl-7B (2025) GUI-Owl-32B (2025) ScaleCUA-3B ScaleCUA-7B ScaleCUA-32B 62.5 41.3 34.7 69.1 65.9 41.6 74.7 83.0 93.7 86.4 89.5 93.4 56.3 39.3 39.3 63.4 66.3 38.8 71.5 88.9 94.1 91.8 93.6 95.1 60.7 37.40 33.0 66.6 70.7 31.5 75.1 87.8 93.3 92.3 91.9 93. 67.9 50.0 49.1 72.5 75.2 28.5 78.7 84.5 89.3 83.5 86.9 91.7 63.1 47.6 39.2 73.9 72.7 41.6 78.6 88.1 84.5 78.5 91.7 89.4 Easy Level 62.4 41.6 39.4 69.9 73.0 35.2 79.2 85.6 93.3 79.9 89.1 94.3 Medium Level 59.7 46.0 42.3 66.9 72.6 37.1 79.9 91.2 95.9 88.7 93.4 96.3 Hard Level 52.4 34.07 32.0 65.8 71.0 24.2 76.2 94.3 95.9 93.8 94.9 96.4 60.4 42.70 34.5 67.6 68.9 35.9 77.4 96.4 95.2 89.4 91.9 96.5 58.5 42.0 42.8 70.8 67.2 31.1 83.6 82.6 95.7 85.4 86.2 93.1 54.1 44.6 39.5 68.0 59.3 41.7 78.4 84.4 87.8 74.8 84.3 92. 45.3 40.86 39.2 60.2 57.6 36.3 70.4 87.8 92.2 85.3 89.6 93.1 56.4 39.0 37.5 63.1 58.1 52.3 80.1 83.3 90.5 80.3 87.0 90.5 57.7 42.0 36.1 63.7 66.2 54.7 81.4 85.3 92.8 88.6 89.6 92.6 50.9 36.96 37.0 58.8 53.9 58.1 75.7 88.9 95.4 88.3 92.9 94.5 58.5 41.8 40.8 69.5 72.1 35.3 81.2 88.1 94.1 87.4 90.1 92.3 55.0 34.33 36.2 64.6 68.2 31.6 78.7 83.6 88.6 79.5 85.8 87. 50.8 38.1 38.9 65.3 68.1 19.9 78.1 94.1 92.69 88.6 91.4 94.0 60.2 41.5 39.1 68.2 67.0 40.2 79.2 84.5 92.8 83.6 88.4 92.5 57.2 41.26 38.4 65.4 67.5 41.8 77.9 86.9 91.7 85.6 90.1 92.5 53.5 37.6 35.7 63.7 64.6 35.8 75.7 90.9 94.2 89.9 92.3 94.4 To quantify the GUI grounding performance, we conduct structured evaluations on ScreenSpotv2 (Wu et al., 2024b) ScreenSpot-Pro (Li et al., 2025), and OSWorld-G (Xie et al., 2025). These datasets are designed for benchmarking spatial localization for UI elements. ScreenSpot-v2 emphasizes cross-platform grounding accuracy for both textual and visual targets, while ScreenSpot-Pro categorizes applications by usage domain, enabling fine-grained analysis of grounding performance across development, creative, scientific, and productivity contexts. The screenshots in OSWorld-G are sampled by previous models on OSWorld, which facilitates more precise analysis of whether the performance bottleneck of the current model in OSWorld stems from deficiencies in grounding capability or planning ability. By default, we only use 2K resolution in ScreenSpot-Pro and 1080p for other grounding benchmarks. For end-to-end agent evaluations, we test our models on suite of offline and online GUI interaction benchmarks. Offline evaluations are conducted on the AndroidControl dataset, which provides fine-grained control benchmarks with both low and high task complexity. For online settings, we deploy agents in dynamic real-time environments including OSWorld (Xie et al., 2024), WindowAgentArena (Bonatti et al., 2024), MacOSArena (Wang et al., 2025c), AndroidWorld (Rawles et al., 2024), and WebArena-Lite-v2. We set 50-step budget for online benchmarks in ablation studies if not stated. These platforms cover desktop, mobile, and web environments, enabling thorough assessment of the agents capabilities in realistic and platform-specific task completion. Its worth noting that WebArena-Lite (Liu et al., 2024a) is not originally designed for pure visual evaluation. Additionally, several tasks are either impossible to complete or even require actions beyond the desktop environment. To address these limitations, we upgrade it into WebArena-Lite-v2, providing more suitable framework for evaluating visual-based web agents. Please refer to A.8.1 for details. 10 Technical Report Table 3: Results on ScreenSpot-v2 (Wu et al., 2024b). Method Operator (2025) Claude-3.7-Sonnet (2025) UI-TARS-1.5 (2025) Seed-1.5-VL (2025) Kimi-VL-A3B-Thinking-2506 (2025b) MiMo-VL-7B-RL (2025) InternVL3.5-241B-A28B (2025a) SeeClick (2024) OmniParser-v2 (2024b) OS-Atlas-Base-7B (2024b) UI-TARS-2B (2025) UI-TARS-7B (2025) UI-TARS-72B (2025) Qwen2.5-VL-3B (2025) Qwen2.5-VL-7B (2025) Qwen2.5-VL-32B (2025) JEDI-3B (2025) JEDI-7B (2025) GUI-Actor-7B (2025) GTA1-7B (2025) GTA1-32B (2025) GUI-Owl-7B (2025) GUI-Owl-32B (2025) ScaleCUA-3B ScaleCUA-7B ScaleCUA-32B Mobile Desktop Web Text Icon/Widget Text Icon/Widget Text Icon/Widget 47.3 97.9 78.4 95.5 95.2 95.2 96.9 94.8 93.4 97.6 97.9 96.6 96.9 97.6 99.0 98.6 99.0 98.6 94.1 97.3 98.6 41.5 91.5 50.7 74.6 75.8 79.1 89.1 86.3 73.5 87.2 88.2 81.5 87.2 88.2 88.6 89.1 92.4 90.0 86.3 90.5 91. 90.2 97.4 70.1 92.3 90.7 90.7 95.4 91.2 88.1 90.2 98.5 96.9 95.9 96.9 94.9 96.4 96.9 97.9 94.9 95.4 99.0 80.3 82.9 29.3 60.9 63.6 68.6 85.0 87.9 58.6 74.2 79.3 78.6 87.9 85.7 89.3 86.4 85.0 87.8 79.3 87.9 90.0 92.8 94.0 55.2 88.0 90.6 87.2 93.6 91.5 88.0 93.2 91.2 88.5 94.4 93.2 92.3 95.7 93.6 94.4 89.7 94.0 94. 84.3 89.2 32.5 59.6 77.3 78.3 85.2 87.7 71.4 81.3 86.2 83.7 84.2 86.7 86.7 88.7 85.2 86.7 85.7 88.7 91.6 Avg 70.5 87.6 94.2 95.2 91.4 90.5 92.9 55.1 80.7 84.1 84.7 91.6 90.3 80.9 88.8 91.3 88.6 91.7 92.1 92.4 93.2 92.8 93.2 89.2 92.7 94.7 In addition, we also validate the general capabilities of our models on four widely used visionlanguage benchmarks: MMMU (Yue et al., 2024), MathVista (Lu et al., 2023), OCRBench (Liu et al., 2024b) and RealWorldQA (xAI, 2024). These benchmarks collectively cover wide spectrum of vision-language tasks, including comprehensive multimodal understanding, mathematical reasoning, OCR-based visual QA and real-world visual QA. These tasks are related to the computer use agent performance, as OCR proficiency forms the foundation for effective grounding, while multimodal reasoning provides the necessary prerequisites for sophisticated planning capabilities. to ensure scalability and platform adaptability for online evaluations, we use In addition, vLLM3 (Kwon et al., 2023) to deploy QwenVL models. This multi-faceted evaluation setup allows us to systematically analyze each component of the agent frameworkfrom perception and grounding to planning and executionacross diverse datasets and platforms. 5.2 COMPREHENSIVE EVALUATION 5.2.1 GUI UNDERSTANDING We evaluate models on MMBench-GUI L1 (GUI Content Understanding), which measures finegrained perception and reasoning across six platforms following MMBench-GUI protocols. As shown in Table 2, our ScaleCUA consistently delivers competitive or superior results. Even the lightweight ScaleCUA-3B attains 83.6% in the easy level, surpassing Qwen2.5-VL-72B by +16.6 points. ScaleCUA-7B further improves to 88.4%, while ScaleCUA-32B reaches 92.5%, matching GUI-Owl-32B despite using different training corpus. On medium-level tasks, ScaleCUA-32B yields 92.5%, and on hard-level tasks it achieves 94.4%, outperforming all proprietary and opensourced models. These results highlight the effectiveness of scaling with cross-platform GUI-specific data, confirming that diverse training corpora substantially enhance visual comprehension across heterogeneous environments. 5.2.2 GUI GROUNDING We then evaluate models on GUI grounding, which measures the ability to localize and associate visual elements with textual or functional references across desktop, mobile, and web environments. As shown in Tables 3 and 4, our ScaleCUA consistently achieves state-of-the-art performance across different benchmarks. On ScreenSpot-v2, ScaleCUA-32B attains an average score of 94.7%, 3https://github.com/vllm-project/vllm 11 Technical Report Table 4: Results on ScreenSpot-Pro (Li et al., 2025). Agent Model SeeClick (2024) Qwen2-VL-7B (2024b) ShowUI-2B (2024) CogAgent-18B (2024) Aria-UI (2024) Claude (2024b) Operator (2025) OS-Atlas-7B (2024b) UGround-V1-7B (2024) UGround-V1-72B (2024) UI-TARS-2B (2025) UI-TARS-7B (2025) UI-TARS-1.5-7B (2025) UI-TARS-72B (2025) Qwen2.5-VL-3B (2025) Qwen2.5-VL-7B (2025) Qwen2.5-VL-32B (2025) JEDI-3B (2025) JEDI-7B (2025) UI-R1-3B (2025) InfiGUI-R1-3B (2025b) GUI-G1-3B (2025) GUI-Owl-7B (2025) GUI-Owl-32B (2025) ScaleCUA-3B ScaleCUA-7B ScaleCUA-32B Development Creative CAD Scientific Office Text 0.6 2.6 16.9 14.9 16.2 22.0 50.0 33.1 47.4 58.4 58.4 63.0 38.3 51.9 74.0 61.0 42.9 22.7 51.3 50.7 76.6 84.4 57.8 66.2 75.3 Icon Avg 0.0 0.0 1.4 0.7 0.0 3.9 19.3 1.4 4.1 12.4 12.4 17.3 3.4 4.8 21.4 13.8 11.0 4.1 12.4 10.3 31.0 39.3 18.6 20.7 35. 0.3 1.3 9.4 8.0 8.4 12.6 35.1 17.7 35.5 31.1 26.4 36.1 31.8 40.8 21.4 29.1 48.5 38.1 27.4 32.4 31.1 54.5 62.5 38.8 44.1 55.8 Text 1.0 1.5 9.1 9.6 23.7 25.9 51.5 28.8 42.9 50.0 58.1 57.1 40.9 36.9 61.1 53.5 50.0 27.3 44.9 36.6 59.6 65.2 42.9 56.6 73.2 Icon Avg 0.0 0.0 0.0 0.0 2.1 3.4 23.1 2.8 6.3 9.1 15.4 15.4 4.9 8.4 13.3 8.4 11.9 3.5 7.0 11.9 27.3 18.2 16.8 20.3 30.8 0.6 0.9 5.3 5.6 14.7 16.8 39.6 17.9 27.8 35.8 27.6 32.8 40.2 39.6 25.8 24.9 41.1 34.6 34.0 29.0 26.6 46.1 45.5 32.0 41.3 55.4 Text 2.5 0.5 2.5 7.1 7.6 14.5 16.8 12.2 17.8 20.8 38.6 18.8 22.3 17.8 38.1 27.4 38.0 11.2 33.0 39.6 64.5 62. 54.3 54.8 60.4 Icon Avg 0.0 0.0 0.0 3.1 1.6 3.7 14.1 4.7 4.7 9.4 11.0 12.5 6.3 1.6 15.6 9.4 14.1 6.3 14.1 9.4 21.9 28.1 28.1 21.9 39.1 1.9 0.4 1.9 6.1 6.1 11.9 16.1 10.3 13.5 13.8 14.6 18.0 31.8 17.2 18.4 13.8 32.6 23.0 32.2 28.4 32.2 54.1 54.0 47.9 46.7 55. Text 3.5 6.3 13.2 22.2 27.1 33.9 58.3 37.5 56.9 63.9 66.7 64.6 44.4 48.6 78.5 54.2 72.9 42.4 58.3 61.8 79.1 82.6 64.6 77.1 76.4 Icon Avg 0.0 0.0 7.3 1.8 6.4 15.8 24.5 7.3 17.3 31.8 21.9 20.9 10.0 8.2 29.1 18.2 25.5 11.8 20.0 30.0 37.3 39.1 35.5 24.5 46. 2.0 3.5 10.6 13.4 18.1 25.8 43.7 24.4 38.8 50.0 39.8 50.0 47.2 45.7 29.5 31.1 57.1 38.6 52.4 41.7 48.0 61.0 63.8 52.0 54.3 63.4 Text 1.1 3.4 15.3 13.0 20.3 30.1 60.5 33.9 50.3 63.3 74.6 63.3 48.0 53.7 76.3 64.4 75.1 32.2 65.5 67.2 77.4 81.4 66.7 74.0 81.4 Icon Avg 0.0 1.9 7.5 0.0 1.9 16.3 28.3 5.7 17.0 20.8 35.9 26.4 17.0 18.9 37.7 32.1 47.2 11.3 28.3 32.1 39.6 39.6 37.7 45.3 49.1 0.9 3.0 13.5 10.0 16.1 26.9 53.0 27.4 48.8 51.3 42.6 53.5 65.6 54.8 40.9 45.7 67.4 57.0 68.7 57.0 59.1 68.7 71.8 53.9 67.4 73.9 Text 2.8 0.9 10.3 5.6 4.7 11.0 34.6 27.1 21.5 30.8 49.5 42.1 33.6 34.6 55.1 38.3 33.6 13.1 43.9 23.5 59.8 70. 31.8 49.5 63.6 OS Icon Avg 0.0 0.0 2.2 0.0 0.0 4.5 30.3 4.5 5.6 16.9 13.5 15.7 4.5 7.9 27.0 9.0 16.9 4.5 12.4 10.6 33.7 36.0 16.9 18.0 41.6 1.5 0.5 6.6 3.1 2.6 8.1 32.7 16.8 26.1 25.5 14.3 24.5 33.2 30.1 20.4 22.4 42.3 25.0 26.0 29.6 16.1 47.9 54. 25.0 35.2 53.6 Avg 1.1 1.6 7.7 7.7 11.3 17.1 36.6 18.9 31.1 34.5 27.7 35.7 42.0 38.1 25.9 27.6 47.6 36.1 39.5 17.8 35.7 37.1 54.9 58.0 42.3 47.9 59.2 surpassing all open VLMs. Even the lightweight ScaleCUA-3B delivers 89.2%, outperforming Qwen2.5-VL-7B by +0.4% despite using far fewer parameters. On the more challenging ScreenSpotPro, ScaleCUA-32B again dominates, achieving 59.2% overall and delivering strong accuracy across diverse domains such as Creative software, CAD, and office applications. More results on OSWorld-G and MMBench-GUI L2 can refer to A.1. Overall, these results demonstrate that scaling with GUI-specific data yields substantial benefits for grounding. The consistent improvements across GUI grounding benchmarks confirm the effectiveness of our dual-loop data pipeline in learning robust UI element localization. 5.2.3 TASK COMPLETION We evaluate end-to-end task completion on Mobile (AndroidWorld (Rawles et al., 2024)), Ubuntu (OSWorld (Xie et al., 2024)), Windows (WindowsAgentArena (Bonatti et al., 2024)), macOS (MacOSArena (Wang et al., 2025c)), and Web (WebArena-Lite-v2), considering both native agents and plannergrounder workflows. The results in Table 5 show three outcomes. First, our native ScaleCUA-32B achieves the strongest Web performance: 44.2% (15 steps budget) and 47.4% (50 steps), outperforming the best native baseline (UI-TARS-72B-DPO) by +20.8 and +26.0 points, respectively, and substantially surpassing Qwen2.5-VL-72B. Second, on WindowsAgentArena (15 steps), ScaleCUA-32B reaches 24.2%, exceeding UI-TARS-72B-DPO (17.9%) by +6.3 points, though the 50 steps regime remains challenging for all native models. Third, the workflow setting with GPT-4o as planner and ScaleCUA-7B as the grounder yields 48.3% on AndroidWorld and 35.1% on WebArena-Lite-v2 (50 steps), outperforming other strong grounders such as UI-TARS-1.5-7B (28.6%) and UGround-V1-7B (26.5%). Beyond these highlights, several trends emerge. (i) Scaling from 3B7B32B produces monotonic gains on Windows and Web, indicating that our cross-platform data and unified action space translate into stronger computer use agents as capacity grows. (ii) The effect of the step budget is consistent: majority of the agents, including ScaleCUA, achieve substantial performance improvements under 50-step limit. (iii) Cross-platform variability persists: AndroidWorld favors planner-grounder workflows, narrowing but not closing the gap with mobile-focused native models; The results on macOS benchmark remain uniformly low, pointing to OS-specific affordance gaps rather than pure grounding limitations. Collectively, these results show that ScaleCUA offers robust task completion as foundation model for computer use agents and serves as competitive, transferable grounder within plannergrounder pipelines. 12 Technical Report Table 5: Online evaluation across different platforms and benchmarks for agents. In AndroidWorld, each task has its own predefined maximum number of steps. denotes the steps is unkown for the model and indicates the steps are more than 50 steps. Method Mobile (AndroidWorld) Ubuntu (OSWorld) Windows (WindowsAgentArena) MacOS (MacOSArena) Web (WebArena-Lite-v2) Predefined Steps 15 Steps 50 Steps 15 Steps 50 Steps 15 Steps 50 Steps 15 Steps 50 Steps Native Agent Kimi-VL-A3B (2025b) Seed1.5-VL (2025) GLM-4.1V-Thinking (2025) GLM-4.5V-Thinking (2025) PC Agent-E (2025) COMPUTERRL (2025) GPT-4o (2024) Claude-3.7 (2025) Qwen2.5-VL-72B (2025) InternVL3.5-241B-A28B (2025a) Aguvis-72B (2024) UI-TARS-7B-SFT (2025) UI-TARS-1.5-7B (2025) UI-TARS-72B-DPO (2025) OpenCUA-7B (2025b) OpenCUA-32B (2025b) ScaleCUA-3B ScaleCUA-7B ScaleCUA-32B Agentic Workflow Planner Grounder GPT-4o Aria-UI (2024) OS-Atlas-7B (2024b) UGround-V1-7B (2024) UI-TARS-1.5-7B (2025) JEDI-3B (2025) JEDI-7B (2025) ScaleCUA-7B 62.1 41.7 57.0 21.6 11.2 27.6 29.7 26.1 33.0 31.6 46.6 23.7 27.2 30.6 44.8 32.8 37.9 48.3 8.2 36.7 14.9 35.8 14.9 47.3 6.8 7.4 9.8 11.1 3.8 17.7 22.1 24.2 24.3 29.7 9.6 14.3 16. 13.1 16.5 22.4 22.7 22.9 15.2 14.6 10.1 10.3 10.6 11.6 4.2 23.9 25.2 28.1 34.1 12.4 15.0 17.7 16.1 19.1 25. 28.1 10.4 39.6 3.5 6.4 9.7 18.0 3.5 15.9 17.9 15.2 20.7 24.2 20.7 26.2 32.9 36. 5.6 7.1 11.8 15.2 4.1 11.1 11.1 13.1 18.0 21.4 13.1 14.5 29.1 30.2 31.7 0.0 5.7 1.4 2.9 0.0 7.1 8.6 0.0 4.3 7. 1.4 1.4 5.7 1.4 7.1 5.7 5.7 0.0 7.1 8.6 1.4 4.3 7.1 0.0 0.0 8. 2.0 2.0 15.6 11.7 5.8 11.0 20.8 23.4 31.8 37.7 44.2 23.2 22.6 28.6 3.3 2.6 14.4 11.7 9.0 13.6 26.0 21.4 33.1 37.7 47. 26.5 28.6 35.1 5.3 ABLATION ON OUR DATASET In this section, we aim to ablate our dataset. As shown in Fig. 1, training with our curated training corpus yields consistent improvements over the baseline trained on public data. In Table 6, we further highlight the effects of augmentation, weak semantic trajectories, coordinate formats, and resolution. First, we evaluate the effect of data augmentation. Without augmentation, the model reaches 37.8% on ScreenSpot-Pro, whereas simple augmentations such as element cropping and screenshot concatenation increase performance to 41.3%. This confirms that augmentation enhances generalization and robustness by exposing the model to wider range of visual conditions. Second, we investigate weak semantic trajectories derived from rule-based random exploration. Despite lacking explicit high-level goals, these trajectories provide low-cost supervision of interface navigation. Third, we study the impact of coordinate representations in grounding. Models trained with raw coordinates outperform those with normalized coordinates (42.3% vs. 37.9% on ScreenSpot-Pro). This indicates that precise absolute positions better capture cross-platform layout regularities, reducing the ambiguity introduced by normalization. Finally, we ablate the training resolution. Higher resolutions yield trade-offs across benchmarks: while 2K improves grounding on ScreenSpot-Pro (45.5% vs. 42.3%) and preserves OSWorld-G accuracy (52.5% vs. 54.3%), it slightly reduces agent success rates on OSWorld and AndroidWorld. This suggests that fine-grained grounding benefits from high-resolution supervision, whereas agentic benchmarks may suffer from overfitting to pixel-level details. The improvements across UI element grounding and online agent benchmarks demonstrate that the design of training data is central to building scalable and generalizable computer use agents. 5.4 DIAGNOSTIC ANALYSIS ON AGENTS To better understand the factors that influence the performance of computer use agents, we conduct additional analyses on resolution, inference mode, data scaling, and the inclusion of general multimodal data, as illustrated in Fig. 5. These studies provide deeper insights into the trade-offs between efficiency, accuracy, and generalization. Impact of Resolution. Fig. 5 (a) evaluates grounding accuracy across different screenshot resolutions. We observe that accuracy consistently improves when moving from 720p to 2K, with diminishing returns at 4K. ScreenSpot-Pro benefits most significantly, demonstrating that fine-grained element 13 Technical Report Table 6: Ablation studies on data. The maximum steps used in online benchmarks are set to 50. (a) The ablation on data augmentation. We only use GUI-related data in training. (b) The ablation on weak semantic trajectories. The public datasets used are shown in Table 12. Model Training Data Aug. SS-Pro Model Training Data + WS OSWorld WAL-v2 Qwen2.5VL-3B ours-only 37.8 41.3 Qwen2.5VL-3B public-only 7.9 8.5 8.4 14.3 (c) The ablation on coordinate types. (d) The ablation on the maximum resolution during training. Model Type ScreenSpot-Pro Model Res. SS-Pro OSWOrld-G OSWorld AW Qwen2.5VL-3B Norm. Raw 37.9 42.3 Qwen2.5VL-3B 1080P 2K 42.3 45. 54.3 52.5 12.4 11.2 23.3 13.4 Figure 5: Evaluations across diverse conditions. (a) Accuracy of GUI grounding under different screenshot resolutions. (b) Success rates of Direct Action vs. Reasoned Action Modes, where reasoning consistently improves performance. (c) Training data scaling. (d) Effect of using general data, showing distinct trends between GUI and multimodal benchmarks. localization requires high-resolution details. Images with resolutions less than or equal to 1080p play dominant role in both the ScreenSpot-v2 and OSWorld-G. As result, increasing the input resolution beyond 1080p during evaluation does not yield performance gains, and in some cases, may even lead to slight degradation. Direct Action Mode vs. Reasoned Action Mode. Fig. 5 (b) compares the two inference modes for computer use agents. Across all benchmarks, Reasoned Action Mode (RAM) yields higher success rates than Direct Action Mode (DAM), with absolute gains ranging from +1.4% to + 8.2%. This advantage is particularly pronounced in complex, multi-step environments such as WebArena-Lite-v2. The results indicate that explicit reasoning helps the model to maintain task coherence and reduce error propagation in long-horizon interactions. Nonetheless, the improvements come at the cost of higher inference latency, suggesting trade-off between efficiency and robustness depending on deployment scenarios. Scaling Behavior with Training Data. Fig. 5 (c) illustrates the success rate accuracy as function of training data ratio. All benchmarks benefit from increased data, but the gains exhibit logarithmic trend, with most improvements saturating beyond 75% of the data. Notably, WebArena-Lite-v2 shows nearly linear improvement across the entire range, underscoring its higher data hunger and greater sensitivity to scale. By contrast, ScreenSpot-Pro reaches competitive accuracy with only half of the available data, highlighting that certain grounding benchmarks are less data-intensive. This observation suggests that data allocation should be tailored to the intrinsic complexity of benchmarks. Effect of General Multimodal Data. Fig. 5 (d) analyzes the effect when employing general-purpose multimodal data in training. We find clear divergence: GUI benchmarks suffer gradual decline in performance as the ratio of general data increases, while general benchmarks improve steadily, peaking around 75%. This trade-off indicates that mixing heterogeneous data sources introduces conflicting optimization signals. As the multimodal corpus expands, the models general capabilities improve, but GUI-specific knowledge is diluted. The results suggest that data-balanced training strategies may be necessary to preserve GUI specialization without sacrificing general reasoning ability. Since the larger models are able to memorize more knowledge, it is reasonable to increase the ratio to 50% for the 7B model and further to 75% for the 32B model. 14 Technical Report Discussion. These figures highlight several key observations. High-resolution inputs and reasoningbased inference enhance grounding and task completion, but incur computational costs. Data scaling remains essential, though different benchmarks vary in sensitivity to scale. Finally, while incorporating general multimodal data improves reasoning ability, it can compromise GUI grounding accuracy, pointing to the need for careful data mixing strategies. These findings underscore the importance of scalable cross-platform training data to build robust general-purpose agents."
        },
        {
            "title": "6 CONCLUSION",
            "content": "In this work, we propose ScaleCUA, new family of models and datasets tailored for CUAs. By curating large-scale cross-platform data and standardizing the action space across desktop, mobile, and web environments, we established unified foundation for evaluating and training agents. Our dual-loop data pipelinecombining automated agent-environment interaction with human-in-theloop annotationenabled the construction of diverse benchmarks spanning GUI understanding, element grounding, and task completion. Extensive experiments demonstrated that our proposed ScaleCUA significantly outperforms both open-source and proprietary baselines. On GUI understanding tasks, ScaleCUA-32B achieves state-of-the-art accuracy across all platforms and difficulty levels, underscoring the benefits of GUI-specific supervision. In grounding benchmarks such as ScreenSpot-v2, ScreenSpot-Pro, and OSWorld-G, our models consistently establish new performance records, validating the effectiveness of fine-grained element-level alignment. Moreover, the strong results on L2-GUI grounding and online benchmarks further highlight the models robustness in realistic, interactive environments. Together, these contributions advance the frontier of computer use agents by bridging vision-language modeling with practical GUI interaction. While limitations remain in long-horizon reasoning and cross-application generalization, our results point toward the promise of scaling specialized training pipelines for robust, general-purpose computer use systems. We hope that ScaleCUA and its released resources will serve as solid foundation for future research in building capable, trustworthy, and deployable CUAs."
        },
        {
            "title": "LIMITATIONS",
            "content": "Our work develops family of base agent models and establishes comprehensive data pipeline to collect cross-platform GUI interaction trajectories. Despite that, several limitations remain. First, the quality of agent-collected data still lags behind expert crowdsourcing. Rule-based exploration often produces semantically weak trajectories, whereas expert-curated data is costly to scale. How to effectively couple automated collection with iterative model refinement to form self-improving loop remains an open challenge. Second, while our study focuses on constructing robust base models for computer use agents across platforms, we have not incorporated advanced agentic mechanisms such as reflection, memory-based decision-making, or hierarchical planning. Furthermore, optimization strategies including Policy Reward Models (PRMs) or Reinforcement Learning are left unexplored. These techniques hold promise for enhancing reasoning, adaptability, and long-horizon decision making, but require strong base models as prerequisite. Third, the current memory mechanism is rudimentary, treating previous operations as flat history. This shallow design limits long-horizon reasoning and may cause suboptimal executions. In this work, our primary goal is to develop base agent models with strong generalization and instruction-following abilities trained from large-scale GUI corpora. Developing lightweight yet effective memory systems that capture temporal dependencies is an important direction for future work. In summary, while our framework provides unified foundation for training GUI agents, further work is needed to enable closed-loop learning and high-level reasoning capabilities. Addressing these limitations will be critical to realize fully autonomous and intelligent computer use agents."
        },
        {
            "title": "REFERENCES",
            "content": "Saaket Agashe, Kyle Wong, Vincent Tu, Jiachen Yang, Ang Li, and Xin Eric Wang. Agent s2: compositional generalist-specialist framework for computer use agents, 2025. URL https: 15 Technical Report //arxiv.org/abs/2504.00906. Anthropic. Claude 3.5. Anthropic AI Assistant, September 2024a. URL https://www. anthropic.com/claude. Accessed: 2025-06-23. Sonnet Anthropic. Model card addendum: Claude 3.5 haiku and upgraded claude 3.5 sonnet, 2024b. URL https://api.semanticscholar.org/CorpusID:273639283. Sonnet Anthropic. Claude 3.7 sonnet system card, 2025. URL https://www.anthropic. com/news/claude-3-7-sonnet. Chongyang Bai, Xiaoxue Zang, Ying Xu, Srinivas Sunkara, Abhinav Rastogi, Jindong Chen, and Blaise Aguera Arcas. Uibert: Learning generic multimodal representations for UI understanding. In Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, 2021. Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: frontier large vision-language model with versatile abilities. arXiv preprint arXiv:2308.12966, 2023. Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. Rogerio Bonatti, Dan Zhao, Francesco Bonacci, Dillon Dupont, Sara Abdali, Yinheng Li, Yadong Lu, Justin Wagle, Kazuhito Koishida, Arthur Bucker, et al. Windows agent arena: Evaluating multi-modal os agents at scale. arXiv preprint arXiv:2409.08264, 2024. Yuxiang Chai, Siyuan Huang, Yazhe Niu, Han Xiao, Liang Liu, Dingyu Zhang, Peng Gao, Shuai Ren, and Hongsheng Li. Amex: Android multi-annotation expo dataset for mobile gui agents. ArXiv preprint, 2024. URL https://arxiv.org/abs/2407.17490. Dongping Chen, Yue Huang, Siyuan Wu, Jingyu Tang, Huichi Zhou, Qihui Zhang, Zhigang He, Yilin Bai, Chujie Gao, Liuyi Chen, et al. Gui-world: video benchmark and dataset for multimodal gui-oriented understanding. In ICLR, 2025a. Wentong Chen, Junbo Cui, Jinyi Hu, Yujia Qin, Junjie Fang, Yue Zhao, Chongyi Wang, Jun Liu, Guirong Chen, Yupeng Huo, et al. Guicourse: From general vision language models to versatile gui agents. arXiv preprint arXiv:2406.11317, 2024a. Xuetian Chen, Yinghao Chen, Xinfeng Yuan, Zhuo Peng, Lu Chen, Yuekeng Li, Zhoujia Zhang, Yingqian Huang, Leyan Huang, Jiaqing Liang, et al. Os-map: How far can computer-using agents go in breadth and depth? arXiv preprint arXiv:2507.19132, 2025b. Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, et al. Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling. arXiv preprint arXiv:2412.05271, 2024b. Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 2418524198, 2024c. Kanzhi Cheng, Qiushi Sun, Yougang Chu, Fangzhi Xu, Yantao Li, Jianbing Zhang, and Zhiyong Wu. Seeclick: Harnessing gui grounding for advanced visual gui agents. arXiv preprint arXiv:2401.10935, 2024. Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. Google Deepmind. era. tic google-gemini-ai-update-december-2024/, 2024. Introducing gemini 2.0: agenhttps://blog.google/technology/google-deepmind/ our new ai model the for 16 Technical Report Biplab Deka, Zifeng Huang, Chad Franzen, Joshua Hibschman, Daniel Afergan, Yang Li, Jeffrey Nichols, and Ranjitha Kumar. Rico: mobile app dataset for building data-driven design applications. In Proceedings of the 30th annual ACM symposium on user interface software and technology, pp. 845854, 2017. Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Sam Stevens, Boshi Wang, Huan Sun, and Yu Su. Mind2web: Towards generalist agent for the web. Advances in Neural Information Processing Systems, 36:2809128114, 2023. Boyu Gou, Ruohan Wang, Boyuan Zheng, Yanan Xie, Cheng Chang, Yiheng Shu, Huan Sun, and Yu Su. Navigating the digital world as humans do: Universal visual grounding for gui agents. arXiv preprint arXiv:2410.05243, 2024. Dong Guo, Faming Wu, Feida Zhu, Fuxing Leng, Guang Shi, Haobin Chen, Haoqi Fan, Jian Wang, Jianyu Jiang, Jiawei Wang, et al. Seed1. 5-vl technical report. arXiv preprint arXiv:2505.07062, 2025. Hongliang He, Wenlin Yao, Kaixin Ma, Wenhao Yu, Yong Dai, Hongming Zhang, Zhenzhong Lan, and Dong Yu. Webvoyager: Building an end-to-end web agent with large multimodal models. arXiv preprint arXiv:2401.13919, 2024. Yanheng He, Jiahe Jin, and Pengfei Liu. Efficient agent training for computer use, 2025. URL https://arxiv.org/abs/2505.13909. Sirui Hong, Xiawu Zheng, Jonathan Chen, Yuheng Cheng, Jinlin Wang, Ceyao Zhang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou, et al. Metagpt: Meta programming for multi-agent collaborative framework. arXiv preprint arXiv:2308.00352, 3(4):6, 2023. Wenyi Hong, Weihan Wang, Qingsong Lv, Jiazheng Xu, Wenmeng Yu, Junhui Ji, Yan Wang, Zihan Wang, Yuxiao Dong, Ming Ding, et al. Cogagent: visual language model for gui agents. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1428114290, 2024. Wenyi Hong, Wenmeng Yu, Xiaotao Gu, Guo Wang, Guobing Gan, Haomiao Tang, Jiale Cheng, Ji Qi, Junhui Ji, Lihang Pan, et al. Glm-4.1 v-thinking: Towards versatile multimodal reasoning with scalable reinforcement learning. arXiv preprint arXiv:2507.01006, 2025. Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. Raghav Kapoor, Yash Parag Butala, Melisa Russak, Jing Yu Koh, Kiran Kamble, Waseem AlShikh, and Ruslan Salakhutdinov. Omniact: dataset and benchmark for enabling multimodal generalist In European Conference on Computer Vision, pp. autonomous agents for desktop and web. 161178. Springer, 2024. Jing Yu Koh, Robert Lo, Lawrence Jang, Vikram Duvvur, Ming Chong Lim, Po-Yu Huang, Graham Neubig, Shuyan Zhou, Ruslan Salakhutdinov, and Daniel Fried. Visualwebarena: Evaluating multimodal agents on realistic visual web tasks. arXiv preprint arXiv:2401.13649, 2024. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023. Hanyu Lai, Xiao Liu, Yanxiao Zhao, Han Xu, Hanchen Zhang, Bohao Jing, Yanyu Ren, Shuntian Yao, Yuxiao Dong, and Jie Tang. Computerrl: Scaling end-to-end online reinforcement learning for computer use agents. arXiv preprint arXiv:2508.14040, 2025. Guohao Li, Hasan Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem. Camel: Communicative agents for mind exploration of large language model society. Advances in Neural Information Processing Systems, 36:5199152008, 2023. 17 Technical Report Kaixin Li, Ziyang Meng, Hongzhan Lin, Ziyang Luo, Yuchen Tian, Jing Ma, Zhiyong Huang, and Tat-Seng Chua. Screenspot-pro: Gui grounding for professional high-resolution computer use. arXiv preprint arXiv:2504.07981, 2025. Wei Li, William Bishop, Alice Li, Chris Rawles, Folawiyo Campbell-Ajala, Divya Tyamagundlu, and Oriana Riva. On the effects of data scale on computer control agents, 2024a. URL https: //arxiv.org/abs/2406.03679. Wei Li, William Bishop, Alice Li, Christopher Rawles, Folawiyo Campbell-Ajala, Divya Tyamagundlu, and Oriana Riva. On the effects of data scale on ui control agents. Advances in Neural Information Processing Systems, 37:9213092154, 2024b. Yang Li, Jiacong He, Xin Zhou, Yuan Zhang, and Jason Baldridge. Mapping natural language instructions to mobile UI action sequences. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics, 2020a. URL https://aclanthology.org/2020.acl-main.729. Yang Li, Gang Li, Luheng He, Jingjie Zheng, Hong Li, and Zhiwei Guan. Widget captioning: Generating natural language description for mobile user interface elements. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics, 2020b. URL https://aclanthology.org/2020. emnlp-main.443. Kevin Qinghong Lin, Linjie Li, Difei Gao, Zhengyuan Yang, Shiwei Wu, Zechen Bai, Weixian Lei, Lijuan Wang, and Mike Zheng Shou. Showui: One vision-language-action model for gui visual agent, 2024. URL https://arxiv.org/abs/2411.17465. Haowei Liu, Xi Zhang, Haiyang Xu, Yuyang Wanyan, Junyang Wang, Ming Yan, Ji Zhang, Chunfeng Yuan, Changsheng Xu, Weiming Hu, and Fei Huang. Pc-agent: hierarchical multi-agent collaboration framework for complex task automation on pc. arXiv preprint arXiv:2502.14282, 2025a. Xiao Liu, Tianjie Zhang, Yu Gu, Iat Long Iong, Yifan Xu, Xixuan Song, Shudan Zhang, Hanyu Lai, Xinyi Liu, Hanlin Zhao, et al. Visualagentbench: Towards large multimodal models as visual foundation agents. arXiv preprint arXiv:2408.06327, 2024a. Yuhang Liu, Pengxiang Li, Congkai Xie, Xavier Hu, Xiaotian Han, Shengyu Zhang, Hongxia Yang, and Fei Wu. Infigui-r1: Advancing multimodal gui agents from reactive actors to deliberative reasoners. arXiv preprint arXiv:2504.14239, 2025b. Yuliang Liu, Zhang Li, Mingxin Huang, Biao Yang, Wenwen Yu, Chunyuan Li, Xu-Cheng Yin, Cheng-Lin Liu, Lianwen Jin, and Xiang Bai. Ocrbench: on the hidden mystery of ocr in large multimodal models. Science China Information Sciences, 67(12):220102, 2024b. Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. arXiv preprint arXiv:2310.02255, 2023. Quanfeng Lu, Wenqi Shao, Zitao Liu, Fanqing Meng, Boxuan Li, Botong Chen, Siyuan Huang, Kaipeng Zhang, Yu Qiao, and Ping Luo. Gui odyssey: comprehensive dataset for cross-app gui navigation on mobile devices. arXiv preprint arXiv:2406.08451, 2024a. Yadong Lu, Jianwei Yang, Yelong Shen, and Ahmed Awadallah. Omniparser for pure vision based gui agent, 2024b. URL https://arxiv.org/abs/2408.00203. Zhengxi Lu, Yuxiang Chai, Yaxuan Guo, Xi Yin, Liang Liu, Hao Wang, Han Xiao, Shuai Ren, Guanjing Xiong, and Hongsheng Li. Ui-r1: Enhancing efficient action prediction of gui agents by reinforcement learning. arXiv preprint arXiv:2503.21620, 2025. Run Luo, Lu Wang, Wanwei He, and Xiaobo Xia. Gui-r1: generalist r1-style vision-language action model for gui agents. arXiv preprint arXiv:2504.10458, 2025. MetaAI. llama4, 2025. URL https://ai.meta.com/blog/ llama-4-multimodal-intelligence/. Accessed: 2025-06-23. 18 Technical Report OpenAI. Gpt-4 technical report. ArXiv, abs/2303.08774, 2023. URL https://api. semanticscholar.org/CorpusID:257532815. OpenAI. Computer-using agent: Introducing universal interface for ai to interact with the digital world, 2025. URL https://openai.com/index/computer-using-agent. Yichen Pan, Dehan Kong, Sida Zhou, Cheng Cui, Yifei Leng, Bing Jiang, Hangyu Liu, Yanyi Shang, Shuyan Zhou, Tongshuang Wu, et al. Webcanvas: Benchmarking web agents in online environments. arXiv preprint arXiv:2406.12373, 2024. Yujia Qin, Yining Ye, Junjie Fang, Haoming Wang, Shihao Liang, Shizuo Tian, Junda Zhang, Jiahao Li, Yunxin Li, Shijue Huang, et al. Ui-tars: Pioneering automated gui interaction with native agents. arXiv preprint arXiv:2501.12326, 2025. Christopher Rawles, Alice Li, Daniel Rodriguez, Oriana Riva, and Timothy Lillicrap. Androidinthewild: large-scale dataset for android device control. Advances in Neural Information Processing Systems, 36:5970859728, 2023. Christopher Rawles, Sarah Clinckemaillie, Yifan Chang, Jonathan Waltz, Gabrielle Lau, Marybeth Fair, Alice Li, William Bishop, Wei Li, Folawiyo Campbell-Ajala, et al. Androidworld: dynamic benchmarking environment for autonomous agents. arXiv preprint arXiv:2405.14573, 2024. Tianlin Shi, Andrej Karpathy, Linxi Fan, Jonathan Hernandez, and Percy Liang. World of bits: An open-domain platform for web-based agents. In Doina Precup and Yee Whye Teh (eds.), Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017, volume 70 of Proceedings of Machine Learning Research, pp. 3135 3144. PMLR, 2017. URL http://proceedings.mlr.press/v70/shi17a.html. Liangtai Sun, Xingyu Chen, Lu Chen, Tianle Dai, Zichen Zhu, and Kai Yu. Meta-gui: Towards multi-modal conversational agents on mobile gui. arXiv preprint arXiv:2205.11029, 2022. Qiushi Sun, Zhirui Chen, Fangzhi Xu, Kanzhi Cheng, Chang Ma, Zhangyue Yin, Jianing Wang, Chengcheng Han, Renyu Zhu, Shuai Yuan, et al. survey of neural code intelligence: Paradigms, advances and beyond. arXiv preprint arXiv:2403.14734, 2024a. Qiushi Sun, Kanzhi Cheng, Zichen Ding, Chuanyang Jin, Yian Wang, Fangzhi Xu, Zhenyu Wu, Chengyou Jia, Liheng Chen, Zhoumianze Liu, et al. Os-genesis: Automating gui agent trajectory construction via reverse task synthesis. arXiv preprint arXiv:2412.19723, 2024b. Qiushi Sun, Zhoumianze Liu, Chang Ma, Zichen Ding, Fangzhi Xu, Zhangyue Yin, Haiteng Zhao, Zhenyu Wu, Kanzhi Cheng, Zhaoyang Liu, et al. Scienceboard: Evaluating multimodal autonomous agents in realistic scientific workflows. arXiv preprint arXiv:2505.19897, 2025. Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. Gemini Team, Petko Georgiev, Ving Ian Lei, Ryan Burnell, Libin Bai, Anmol Gulati, Garrett Tanzer, Damien Vincent, Zhufeng Pan, Shibo Wang, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. Gemma Team, Aishwarya Kamath, Johan Ferret, Shreya Pathak, Nino Vieillard, Ramona Merhej, Sarah Perrin, Tatiana Matejovicova, Alexandre Rame, Morgane Rivi`ere, et al. Gemma 3 technical report. arXiv preprint arXiv:2503.19786, 2025a. Kimi Team, Angang Du, Bohong Yin, Bowei Xing, Bowen Qu, Bowen Wang, Cheng Chen, Chenlin Zhang, Chenzhuang Du, Chu Wei, Congcong Wang, Dehao Zhang, Dikang Du, Dongliang Wang, Enming Yuan, Enzhe Lu, Fang Li, Flood Sung, Guangda Wei, Guokun Lai, Han Zhu, Hao Ding, Hao Hu, Hao Yang, Hao Zhang, Haoning Wu, Haotian Yao, Haoyu Lu, Heng Wang, Hongcheng Gao, Huabin Zheng, Jiaming Li, Jianlin Su, Jianzhou Wang, Jiaqi Deng, Jiezhong Qiu, Jin Xie, Jinhong Wang, Jingyuan Liu, Junjie Yan, Kun Ouyang, Liang Chen, Lin Sui, Longhui Yu, Mengfan Dong, Mengnan Dong, Nuo Xu, Pengyu Cheng, Qizheng Gu, Runjie Zhou, Shaowei Liu, Sihan Cao, Tao Yu, Tianhui Song, Tongtong Bai, Wei Song, Weiran He, Weixiao Huang, Weixin Xu, 19 Technical Report Xiaokun Yuan, Xingcheng Yao, Xingzhe Wu, Xinxing Zu, Xinyu Zhou, Xinyuan Wang, Y. Charles, Yan Zhong, Yang Li, Yangyang Hu, Yanru Chen, Yejie Wang, Yibo Liu, Yibo Miao, Yidao Qin, Yimin Chen, Yiping Bao, Yiqin Wang, Yongsheng Kang, Yuanxin Liu, Yulun Du, Yuxin Wu, Yuzhi Wang, Yuzi Yan, Zaida Zhou, Zhaowei Li, Zhejun Jiang, Zheng Zhang, Zhilin Yang, Zhiqi Huang, Zihao Huang, Zijia Zhao, and Ziwei Chen. Kimi-VL technical report, 2025b. URL https://arxiv.org/abs/2504.07491. Kimi Team, Angang Du, Bohong Yin, Bowei Xing, Bowen Qu, Bowen Wang, Cheng Chen, Chenlin Zhang, Chenzhuang Du, Chu Wei, et al. Kimi-vl technical report. arXiv preprint arXiv:2504.07491, 2025c. Junyang Wang, Haiyang Xu, Jiabo Ye, Ming Yan, Weizhou Shen, Ji Zhang, Fei Huang, and Jitao Sang. Mobile-agent: Autonomous multi-modal mobile device agent with visual perception. arXiv preprint arXiv:2401.16158, 2024a. Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024b. Weiyun Wang, Zhangwei Gao, Lixin Gu, Hengjun Pu, Long Cui, Xingguang Wei, Zhaoyang Liu, Linglin Jing, Shenglong Ye, Jie Shao, et al. Internvl3. 5: Advancing open-source multimodal models in versatility, reasoning, and efficiency. arXiv preprint arXiv:2508.18265, 2025a. Xinyuan Wang, Bowen Wang, Dunjie Lu, Junlin Yang, Tianbao Xie, Junli Wang, Jiaqi Deng, Xiaole Guo, Yiheng Xu, Chen Henry Wu, Zhennan Shen, Zhuokai Li, Ryan Li, Xiaochuan Li, Junda Chen, Boyuan Zheng, Peihang Li, Fangyu Lei, Ruisheng Cao, Yeqiao Fu, Dongchan Shin, Martin Shin, Jiarui Hu, Yuyan Wang, Jixuan Chen, Yuxiao Ye, Danyang Zhang, Dikang Du, Hao Hu, Huarong Chen, Zaida Zhou, Haotian Yao, Ziwei Chen, Qizheng Gu, Yipu Wang, Heng Wang, Diyi Yang, Victor Zhong, Flood Sung, Y. Charles, Zhilin Yang, and Tao Yu. Opencua: Open foundations for computer-use agents, 2025b. URL https://arxiv.org/abs/2508.09123. Xuehui Wang, Zhenyu Wu, JingJing Xie, Zichen Ding, Bowen Yang, Zehao Li, Zhaoyang Liu, Qingyun Li, Xuan Dong, Zhe Chen, et al. Mmbench-gui: Hierarchical multi-platform evaluation framework for gui agents. arXiv preprint arXiv:2507.19478, 2025c. Jason Wu, Siyan Wang, Siman Shen, Yi-Hao Peng, Jeffrey Nichols, and Jeffrey Bigham. Webui: dataset for enhancing visual ui understanding with web semantics. ACM Conference on Human Factors in Computing Systems (CHI), 2023a. Qianhui Wu, Kanzhi Cheng, Rui Yang, Chaoyun Zhang, Jianwei Yang, Huiqiang Jiang, Jian Mu, Baolin Peng, Bo Qiao, Reuben Tan, et al. Gui-actor: Coordinate-free visual grounding for gui agents. arXiv preprint arXiv:2506.03143, 2025. Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Beibin Li, Erkang Zhu, Li Jiang, Xiaoyun Zhang, Shaokun Zhang, Jiale Liu, et al. Autogen: Enabling next-gen llm applications via multi-agent conversation. arXiv preprint arXiv:2308.08155, 2023b. Zhiyong Wu, Chengcheng Han, Zichen Ding, Zhenmin Weng, Zhoumianze Liu, Shunyu Yao, Tao Yu, and Lingpeng Kong. Os-copilot: Towards generalist computer agents with self-improvement. arXiv preprint arXiv:2402.07456, 2024a. Zhiyong Wu, Zhenyu Wu, Fangzhi Xu, Yian Wang, Qiushi Sun, Chengyou Jia, Kanzhi Cheng, Zichen Ding, Liheng Chen, Paul Pu Liang, et al. Os-atlas: foundation action model for generalist gui agents. arXiv preprint arXiv:2410.23218, 2024b. xAI. Grok-1.5 vision preview, 2024. URL https://x.ai/blog/grok-1.5v. xAI. Grok-3, 2025. URL https://x.ai/blog/grok-3. LLM-Core-Team Xiaomi. Mimo-vl technical report, 2025. URL https://arxiv.org/abs/ 2506.03569. 20 Technical Report Tianbao Xie, Danyang Zhang, Jixuan Chen, Xiaochuan Li, Siheng Zhao, Ruisheng Cao, Toh Hua, Zhoujun Cheng, Dongchan Shin, Fangyu Lei, et al. Osworld: Benchmarking multimodal agents for open-ended tasks in real computer environments. Advances in Neural Information Processing Systems, 37:5204052094, 2024. Tianbao Xie, Jiaqi Deng, Xiaochuan Li, Junlin Yang, Haoyuan Wu, Jixuan Chen, Wenjing Hu, Xinyuan Wang, Yuhui Xu, Zekun Wang, Yiheng Xu, Junli Wang, Doyen Sahoo, Tao Yu, and Caiming Xiong. Scaling computer-use grounding via user interface decomposition and synthesis, 2025. URL https://arxiv.org/abs/2505.13227. Yiheng Xu, Zekun Wang, Junli Wang, Dunjie Lu, Tianbao Xie, Amrita Saha, Doyen Sahoo, Tao Yu, and Caiming Xiong. Aguvis: Unified pure vision agents for autonomous gui interaction. arXiv preprint arXiv:2412.04454, 2024. Tianci Xue, Weijian Qi, Tianneng Shi, Chan Hee Song, Boyu Gou, Dawn Song, Huan Sun, and Yu Su. An illusion of progress? assessing the current state of web agents. arXiv preprint arXiv:2504.01382, 2025. Yan Yang, Dongxu Li, Yutong Dai, Yuhao Yang, Ziyang Luo, Zirui Zhao, Zhiyuan Hu, Junzhe Huang, Amrita Saha, Zeyuan Chen, et al. Gta1: Gui test-time scaling agent. arXiv preprint arXiv:2507.05791, 2025. Yuhao Yang, Yue Wang, Dongxu Li, Ziyang Luo, Bei Chen, Chao Huang, and Junnan Li. Aria-ui: Visual grounding for gui instructions. arXiv preprint arXiv:2412.16256, 2024. Shunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan. Webshop: Towards scalable real-world web interaction with grounded language agents. In Sanmi Koyejo, S. Mohamed, A. Agarwal, Danielle Belgrave, K. Cho, and A. Oh (eds.), Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022, 2022. URL http://papers.nips.cc/paper_files/paper/2022/hash/ 82ad13ec01f9fe44c01cb91814fd7b8c-Abstract-Conference.html. Jiabo Ye, Xi Zhang, Haiyang Xu, Haowei Liu, Junyang Wang, Zhaoqing Zhu, Ziwei Zheng, Feiyu Gao, Junjie Cao, Zhengxi Lu, et al. Mobile-agent-v3: Foundamental agents for gui automation. arXiv preprint arXiv:2508.15144, 2025. Wenwen Yu, Zhibo Yang, Jianqiang Wan, Sibo Song, Jun Tang, Wenqing Cheng, Yuliang Liu, and Xiang Bai. Omniparser v2: Structured-points-of-thought for unified visual text parsing and its generality to multimodal large language models. arXiv preprint arXiv:2502.16161, 2025. Xinbin Yuan, Jian Zhang, Kaixin Li, Zhuoxuan Cai, Lujian Yao, Jie Chen, Enguang Wang, Qibin Hou, Jinwei Chen, Peng-Tao Jiang, et al. Enhancing visual grounding for gui agents via self-evolutionary reinforcement learning. arXiv preprint arXiv:2505.12370, 2025. Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 95569567, 2024. Bofei Zhang, Zirui Shang, Zhi Gao, Wang Zhang, Rui Xie, Xiaojian Ma, Tao Yuan, Xinxiao Wu, SongChun Zhu, and Qing Li. Tongui: Building generalized gui agents by learning from multimodal web tutorials. arXiv preprint arXiv:2504.12679, 2025a. Chi Zhang, Zhao Yang, Jiaxuan Liu, Yanda Li, Yucheng Han, Xin Chen, Zebiao Huang, Bin Fu, and Gang Yu. Appagent: Multimodal agents as smartphone users. In Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems, pp. 120, 2025b. Jiwen Zhang, Jihao Wu, Yihua Teng, Minghui Liao, Nuo Xu, Xiao Xiao, Zhongyu Wei, and Duyu Tang. Android in the zoo: Chain-of-action-thought for GUI agents. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen (eds.), Findings of the Association for Computational Linguistics: EMNLP 2024, Miami, Florida, USA, November 12-16, 2024, pp. 1201612031. Association for Computational Linguistics, 2024. doi: 10.18653/V1/2024.FINDINGS-EMNLP.702. URL https://doi.org/10.18653/v1/2024.findings-emnlp.702. 21 Technical Report Junlei Zhang, Zichen Ding, Chang Ma, Zijie Chen, Qiushi Sun, Zhenzhong Lan, and Junxian He. Breaking the data barrierbuilding gui agents through task generalization. arXiv preprint arXiv:2504.10127, 2025c. Di Zhao, Longhui Ma, Siwei Wang, Miao Wang, and Zhao Lv. Cola: scalable multi-agent framework for windows ui task automation. arXiv preprint arXiv:2503.09263, 2025. Boyuan Zheng, Boyu Gou, Jihyung Kil, Huan Sun, and Yu Su. Gpt-4v(ision) is generalist web agent, if grounded. In Forty-first International Conference on Machine Learning, 2024. URL https://openreview.net/forum?id=piecKJ2DlB. Shuyan Zhou, Frank Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Tianyue Ou, Yonatan Bisk, Daniel Fried, et al. Webarena: realistic web environment for building autonomous agents. arXiv preprint arXiv:2307.13854, 2023. Yuqi Zhou, Sunhao Dai, Shuai Wang, Kaiwen Zhou, Qinglin Jia, and Jun Xu. Gui-g1: Understanding r1-zero-like training for visual grounding in gui agents. arXiv preprint arXiv:2505.15810, 2025. Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Yuchen Duan, Hao Tian, Weijie Su, Jie Shao, et al. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models. arXiv preprint arXiv:2504.10479, 2025."
        },
        {
            "title": "A APPENDIX",
            "content": "A.1 MORE RESULTS To fully demonstrate the potential of ScaleCUA, we provide additional results on serveral benchmarks. On MMBench-GUI L2 (Wang et al., 2025c), which incorporates stratified grounding difficulty across major operating systems, ScaleCUA-32B demonstrates performance comparable to state-of-the-art methods as shown in Table 7. It achieves leading scores in the basic difficulty setting across several platforms including Android (96.4), Web (93.9), Linux (81.2), and macOS (88.1), while maintaining competitive results in the advanced difficulty setting (e.g., Web 76.3, Android 81.7). Furthermore, ScaleCUA-7B and ScaleCUA-3B achieve average scores of 78.2 and 73.7, respectively. They demonstrate particularly robust performance in the basic difficulty setting, especially on Windows, where both models score 78.6, and on iOS, with respective scores of 96.1 and 93.0. On OSWorld-G (Xie et al., 2025) for Ubuntu grounding, ScaleCUA-32B demonstrates impressive results with an overall performance of 60.6 shown in Table 8, which includes strong marks in layout understanding (70.0), element recognition (66.7), and fine-grained manipulation (51.0). All of our models underperform on the Refusal subtask because we deliberately excluded the Refusal-specific training data provided by JEDI (Xie et al., 2025). Incorporating these examples may pose risk of biasing the model toward emitting an await/refusal state in complex grounding scenarios. Such bias Table 7: Performance on the MMBench-GUI L2 (GUI Element Grounding) (Wang et al., 2025c). Model Windows MacOS Linux iOS Android Web Basic Adv. Basic Adv. Basic Adv. Basic Adv. Basic Adv. Basic Adv. GPT-4o (2024) Claude-3.7 (2025) Qwen-Max-VL (2023) Aguvis-7B-720P (2024) ShowUI-2B (2024) OS-Atlas-Base-7B (2024b) UGround-V1-7B (2024) InternVL3-72B (2025) Qwen2.5-VL-72B (2025) Qwen2.5-VL-7B (2025) UI-TARS-1.5-7B (2025) UI-TARS-72B-DPO (2025) GUI-Owl-7B (2025) GUI-Owl-32B (2025) ScaleCUA-3B ScaleCUA-7B ScaleCUA-32B 1.5 1.5 43.9 37.3 9.2 36.9 66.8 70.1 55.7 31.4 68.3 78.6 86.3 85.6 78.6 78.6 83.0 1.1 0.7 36.8 21.7 4.4 18.8 39.0 42.6 33.8 16.5 39.0 51.8 61.8 65.1 46.0 54.0 62. 8.7 12.5 58.8 48.1 24.1 44.4 71.3 75.7 49.9 31.3 69.0 80.3 81.7 84.9 79.4 82.3 88.1 4.3 7.5 56.1 33.3 10.4 21.7 48.6 52.3 30.1 22.0 44.5 62.7 64.5 67.1 52.9 58.7 64.2 1.0 0.0 30.1 25.0 11.7 13.3 31.1 41.3 20.9 12.2 37.8 51.5 61.7 63.3 49.0 56.6 65.8 5.1 13.7 77.4 67.5 29.0 74.8 92.7 93.6 56.1 66.6 88.5 90.8 94.9 95.2 93.0 94.3 95.9 3.3 10.6 59.1 65.2 19.7 48.8 70.9 80.6 28.2 55.2 69.4 81.2 83.0 85.5 73.3 81.8 84.9 2.5 1.4 79.5 61.0 17.4 69.6 93.5 92.7 55.6 35.1 90.5 93.0 95.8 96.1 94.1 96.1 96. 1.4 1.4 70.1 51.0 8.7 46.8 71.0 78.6 25.4 35.2 69.3 80.0 83.7 87.0 74.4 81.1 81.7 3.2 3.2 74.8 61.6 22.9 61.3 88.7 90.7 68.4 40.3 81.0 88.1 93.2 95.5 92.6 92.6 93.9 2.9 2.3 58.8 45.5 12.7 35.4 64.6 65.9 45.8 32.5 56.5 68.5 72.7 80.8 63.6 73.1 76.3 1.1 1.1 53.9 33.5 25.1 31.4 56.5 59.2 40.3 21.5 64.4 68.6 74.4 77.0 73.3 74.4 81.2 22 Avg 2.9 4.7 58.0 45.7 16.0 41.4 65.7 72.2 41.8 33.9 64.3 74.3 80.5 83.0 73.7 78.2 82.0 Technical Report Table 8: Performance comparison on OSWorld-G (Xie et al., 2025). Agent Model Gemini-2.5-Pro (2025) Operator (2025) Seed1.5-VL (2025) OS-Atlas-7B (2024b) UGround-V1-7B (2024) Aguvis-7B (2024) UI-TARS-7B (2025) UI-TARS-1.5-7B (2025) UI-TARS-72B (2025) Qwen2.5-VL-3B (2025) Qwen2.5-VL-7B (2025) Qwen2.5-VL-32B (2025) InternVL3.5-241B-A28B (2025a) JEDI-3B (2025) JEDI-7B (2025) ScaleCUA-3B ScaleCUA-7B ScaleCUA-32B Text Matching Element Recognition Layout Understanding Fine-grained Manipulation Refusal Overall 59.8 51.3 73.9 44.1 51.3 55.9 60.2 70.1 69.4 41.4 45.6 63.2 64.4 67.4 65. 64.8 67.8 69.0 45.5 42.4 66.7 29.4 40.3 41.2 51.8 57.9 60.6 28.8 32.7 47.3 58.8 53.0 55.5 61.8 61.8 66.7 49.0 46.6 69.6 35.2 43.5 43.9 54.9 59.7 62.9 34.8 41.9 49.0 55.3 53.8 57.7 64.0 64.8 70.0 33.6 31.5 47.0 16.8 24.8 28.2 35.6 51.7 45.6 13.4 18.1 36.9 43.0 44.3 46. 43.6 49.7 51.0 38.9 0.0 18.5 7.4 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 7.4 7.4 0.0 0.0 0.0 45.2 40.6 62.9 27.7 36.4 38.7 47.5 56.0 57.1 27.3 31.4 46.5 53.2 50.9 54.1 55.7 57.8 60.6 Table 9: Performance comparison on AndroidControl (Li et al., 2024b). Agent Model Claude (2024b) GPT-4o (2024) SeeClick (2024) InternVL-2-4B (2024c) Qwen2-VL-7B (2024b) Aria-UI (2024) OS-Atlas-4B (2024b) OS-Atlas-7B (2024b) Aguvis-7B (2024) Aguvis-72B (2024) OS-Genesis-7B (2024b) UI-TARS-2B (2025) UI-TARS-7B (2025) UI-TARS-72B (2025) Qwen2.5-VL-3B (2025) Qwen2.5-VL-7B (2025) Qwen2.5-VL-32B (2025) Qwen2.5-VL-72B (2025) InternVL3.5-241B-A28B (2025a) ScaleCUA-3B ScaleCUA-7B ScaleCUA-32B AndroidControl-Low AndroidControl-High Type Grounding SR Type Grounding SR 74.3 74.3 93.0 90.9 91.9 91.9 93.6 91.3 98.1 98.0 98.1 88.1 91.4 93.3 91.9 0.0 0.0 73.4 84.1 86.5 87.7 83.8 88.0 87.3 89.3 89.9 93.4 93.7 93.1 94. 19.4 19.4 75.0 80.1 82.6 67.3 80.6 85.2 80.5 84.4 74.2 89.3 90.8 91.3 90.8 91.4 93.3 93.7 82.1 84.1 86.0 85.7 63.7 66.3 82.9 84.1 83.8 84.7 85.2 66.2 81.2 83.7 85.2 81.0 81.4 86.3 85.7 0.0 0.0 62.9 72.7 77.7 43.2 73.8 78.5 78.4 80.5 81.5 81.5 83.9 84.3 87. 12.5 20.8 59.1 66.7 69.7 10.2 67.5 71.2 61.5 66.4 44.5 68.9 72.5 74.7 63.7 60.1 69.6 67.4 68.2 70.3 74.8 75.9 diminishes the agents propensity for active exploration within the environment, thereby degenerating its success rate in task completion. On AndroidControl (Li et al., 2024b) which is an offline planning benchmark developed for the Android, all ScaleCUA variants exhibit consistently strong performance demonstrated in Table 9. On the AndroidControl-Low, ScaleCUA-7B attains the highest task completion rate, whereas ScaleCUA32B achieves the most reliable grounding, indicating that the compact model favors execution efficiency while the larger capacity maximizes perceptual fidelity. As for AndroidControl-High, ScaleCUA-32B demonstrates the highest success rate while showing the smallest degradation from Low to High. ScaleCUA-3B and ScaleCUA-7B achieve favorable trade-off, sustaining solid performance across both low and high settings. The relatively small variance in type prediction across sizes suggests that residual failures arise more from long-horizon interaction and error accumulation than from intent misclassification or localization. 23 Technical Report Table 10: Performance comparison on ScienceBoard (Sun et al., 2025). Model Algebra Biochem GIS ATP Astron Doc Overall GPT-4o (2024) Claude-3.7-Sonnet (2025) Gemini-2.0-Flash (2024) Qwen2.5-VL-72B (2025) InternVL3-78B (2025) UI-TARS-1.5-7B (2025) ScaleCUA-3B ScaleCUA-7B ScaleCUA-32B 3.2 9.7 6.5 22.6 6.5 12. 6.5 3.2 9.7 0.0 37.9 3.5 27.6 3.5 13.8 13.8 3.4 10.3 0.0 2.9 2.9 5.9 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0. 0.0 0.0 0.0 0.0 6.1 0.0 9.1 0.0 6.1 0.0 1.8 12.1 6.3 6.3 6.1 12.5 6.3 0.0 0.0 0.0 0.0 1.6 10.5 3.2 12.9 2.7 5. 3.6 1.8 5.9 Table 11: Performance on General VLM Benchmarks. ScaleCUA-3B (25%) denotes that, during training, the number of general-purpose data samples was set to 25% of the GUI data samples (e.g., Understanding, Grounding, and Planning). Model MathVistaMINI (2023) OCRBench (2024b) MMMUvalid (2024) RealWorldQA (2024) Qwen2.5-VL-3B (2025) ScaleCUA-3B (0%) ScaleCUA-3B (25%) ScaleCUA-3B (50%) ScaleCUA-3B (75%) ScaleCUA-3B (100%) Qwen2.5VL-7B (2025) ScaleCUA-7B (50%) Qwen2.5-VL-32B (2025) ScaleCUA-32B (75%) 62.3 52.8 58.6 58.7 59.3 60.6 68.2 65.4 74.7 69. 797 (79.7) 819 (81.9) 823 (82.3) 824 (82.4) 818 (81.8) 806 (80.6) 864 (86.4) 852 (85.2) 854 (85.4) 827 (82.7) 53.1 48.8 50.6 52.4 55.6 53.4 58.6 54.7 70.0 61.9 65.4 65.2 65.4 65.1 65.2 63.5 68.5 69.8 72.2 72.3 On ScienceBoard (Sun et al., 2025), planning benchmark designed for scientific professionals, our models show modest yet meaningful capability as shown in 10. The ScaleCUA-32B outperforms strong VLMs such as GPT-4o (1.6) while remaining below Qwen2.5-VL-72B (12.9) and Claude-3.7Sonnet (10.5). Our model excels in domains demanding factual and visual-text reasoning over those requiring specialized symbolic workflows. To evaluate the transfer learning capabilities of ScaleCUA-32B, we augment our training with diverse set of general-purpose SFT data. These data, sourced from the post-training corpus of InternVL3 (Zhu et al., 2025), encompass range of tasks, including OCR, mathematics, coding, reasoning-QA, and general multimodal understanding. We then assess performance on four standard General Multimodal Benchmarks shown in Table 11. These benchmarks jointly evaluate skills such as mathematical and commonsense reasoning, text comprehension, and open-domain visual question answering, which are also fundamental for computer-use agents. The ScaleCUA-3B (25%) specifies the proportion of this general-purpose data relative to the core GUI data used in training. Based on Table 11, several consistent trends emerge regarding the interaction between the proportion of general-purpose data and agent performance on general VLM benchmarks. First, incorporating moderate amounts of general-purpose data (e.g., 2550% relative to GUI-specific data) yields notable gains over the 0% setting, particularly on MathVista and MMMUvalid, suggesting that exposing the agent to broader multimodal reasoning tasks improves its mathematical and crossdomain inference ability. For instance, ScaleCUA-3B rises from 52.8 to 58.7 on MathVista and from 48.8 to 52.4 on MMMU when increasing general data to 50%, while maintaining stable performance on RealWorldQA. Second, the results indicate saturation effect: pushing the general data ratio to 75% or 100% offers only marginal or inconsistent benefits. Third, scaling model capacity amplifies the positive effect of general data. However, our 7B and 32B models still exhibit substantial performance gap compared to the baseline on general benchmarks, indicating that the proportion of general-purpose data could be further increased. Such adjustments must also consider their potential impact on the computer-use capability of agent models. 24 Technical Report Table 12: Public data sources used for training our models. The * indicates that we count the number of samples we use rather than the full size of the original dataset because we have processed several datasets and filtered some noisy examples. Some statistics are calculated from Aguvis (2024). GUI Task Grounding Planning Platform Data source Website SeeClick (2024) Website GUIEnv (2024a) Website GUIAct (2024a) WebUI (2023a) Website Widget Captioning (2020b) Mobile Mobile RicoSCA (2020a) Mobile UI RefExp (2021) Mobile RICO Icon (2017) Desktop & Website OmniACT (2024) Website UGround (2024)* Desktop OS-Atlas (2024b)* Desktop JEDI (2025)* Total Website MM-Mind2Web (2023) Website GUIAct (2024a) Website MiniWoB++ (2017) Mobile AitZ (2024) Mobile AndroidControl (2024a) Mobile GUI Odyssey (2024a) Mobile AMEX (2024) Mobile AitW (2023) Desktop PC Agent-E (2025) Total #Elements / #Steps 271K 328K 67K 57K 101K 173K 16K 16K 7K 1404K 799K 550K 3789K 7.8K 16.6K 9.9K 11.9K 74.8K 118.3K 35.6K 19.0K 27.8K 321.7K A.2 PUBLIC DATA USED IN TRAINING Table 12 summarizes the public datasets used for training ScaleCUA. Please note that the reported statistics refer to the portion of each dataset actually utilized in our experiments, rather than the original sizes of the source datasets. A.3 ACTION SPACE To enable robust cross-platform control, we define unified action space that abstracts low-level GUI actions into concise yet expressive set of semantic commands. As shown in Table 13, this action space is designed to be platform-aware yet semantically consistent, allowing our agents to operate seamlessly across Desktop (Windows, macOS, Ubuntu), Mobile (Android, iOS), and Web platforms. The action set includes universally supported operations such as click, write, wait, and terminate, which are shared across all platforms. It also accommodates platform-specific interactions, including swipe and long press for mobile devices, and fine-grained mouse or keyboard controls such as doubleClick, rightClick, dragTo, and hotkey for desktop and web interfaces. To handle modern interactive elements, swipe operation has also been implemented for Web. By standardizing the operation interface through shared action space, we simplify training and inference while supporting both generalization and specialization. Each action is defined with explicit arguments (e.g., coordinates, keypresses), enabling precise control and compatibility with structured outputs in grounding, direct-action, and reasoned-action inference modes. This design facilitates modular training, policy transfer, and scalable data annotation, forming critical foundation for developing universal GUI agents. A.4 ERROR CASE ANALYSIS We here provide several error cases across different platforms to analyze the limitations of our ScaleCUA. 25 Technical Report Table 13: Actions space. Action Platforms Description click(x, y, clicks, button) All write(message) wait(seconds) response(answer) terminate(status) scroll(clicks, x, y) doubleClick(x, y, button) rightClick(x, y, button) All All All All Desktop Desktop & Web Desktop & Web Perform mouse click at coordinates (x, y) using the specified button and number of clicks. Input the given message. Pause execution for the specified number of seconds. Submit response to the environment or task prompt. Terminate the current task with given completion status. Performs scroll of the mouse scroll wheel at position (x, y). Perform double click at coordinates (x, y) with the specified button. Perform right click at coordinates (x, y) with the specified button. Trigger keyboard shortcut composed of one or more keys. Press the specified key(s) given number of times. Press and hold key without releasing it. Release previously held key. Swipe from start to end point in the specified direction with given intensity or distance. Return to the mobile home screen. Navigate back to the previous screen on mobile. Perform long-press gesture on the current focus or location. Launch mobile application by its name. hotkey(*args) Desktop & Web moveTo(x, y) Desktop & Web Move the mouse pointer to the specified (x, y) position. dragTo(x, y, button) Desktop & Web Drag the mouse to (x, y) while holding the specified button. press(keys, presses) Desktop & Web keyDown(key) Desktop & Web keyUp(key) Desktop & Web swipe(from, to, direction, amount) Mobile & Web navigate home() navigate back() long press() open app(app name) Mobile Mobile Mobile Mobile On desktop platforms, ScaleCUA frequently violates procedural prerequisites shown in Fig 6 and Fig 7, such as attempting to compress files without selecting them or changing font styles without highlighting the target text. These issues stem from an incomplete understanding of interface states and sub-task dependencies. Moreover, significant limitation of ScaleCUA emerges when actions result in silent failures, characterized by lack of discernible state transition. In such instances, the model tends to perseverate on the unsuccessful operation, revealing the absence of robust error-recovery mechanism. This issue underscores the critical requirement for fine-grained perception and robust understanding of element state to interact with context-dependent UI elements, such as focus and selection. For the Android platform, there exist precision and positional challenges demonstrated in Fig 8. In the first case, the instruction explicitly requires appending text to the top of file within note-taking application (Markor). However, the agent fails to recognize this positional constraint, instead inserting Figure 6: Error cases on the Windows platform. The first case shows ScaleCUA creating an archive without having selected any files, revealing that it sometimes fails to follow the full instruction and only completes sub-step. The second case shows ScaleCUA persistently repeating the same action until the step limit, when it misses the correct element and the screen remains unchanged. 26 Technical Report Figure 7: Error cases on the Ubuntu platform. ScaleCUA repeatedly fails tasks because it does not comprehend procedural prerequisites. The agent attempts to execute final command without first performing the necessary intermediate step of selecting the target object. For instance, it tries to restore file without selecting it from the trash or alter font without highlighting the text. Critically, this operational flaw generates no explicit error message, trapping the agent in repetitive loop of ineffective actions. Figure 8: Error cases on the Android platform. The first case shows an instruction requiring content to be inserted at the top of document; however, ScaleCUA opens the file and inserts directly at the current cursor location, ignoring the positional prerequisite. The second case shows that when the UI exhibits no obvious state change after an operation, ScaleCUA repeats the same action multiple times, causing tasks such as taking photo to fail. content at the current cursor location without adjusting it. This suggests that ScaleCUA lacks fine-grained understanding of positional semantics in natural language instructions, as well as the ability to reason about UI state changes like cursor positioning. In the second case, the agent is instructed to Take one photo. Despite correctly launching the camera and triggering the shutter once, the agent erroneously repeats the same actions multiple times. This behavior stems from failure to detect visual feedback or confirm state transitions (e.g., captured photo thumbnail), 27 Technical Report Figure 9: Error cases on the Web platform. The first case shows ScaleCUA made subjective analytical assumptions, presuming the product necessarily contained reviews, while disregarding the explicitly specified product category in the task instructions. The second case shows ScaleCUA struggles with complex tasks in complex initial environments (where numerous posts already exist on the starting interface). When faced with multifaceted requirements (needing to identify both my posts and posts on specified topic), it neglected the explicitly stated topic in the instructions, instead selecting only posts visible in the current observation space that belonged to me. leading to unnecessary repetition. These failure modes indicate two key limitations: (1) insufficient grounding of spatial and contextual cues embedded in task descriptions, and (2) inadequate visual state tracking, particularly under conditions where UI feedback is subtle. Addressing these issues may require enhanced visual reasoning modules, memory-based state modeling, or task-guided grounding refinements. Empirical analysis of trajectories from web platform reveals that ScaleCUA may struggle with semantic disambiguation. ScaleCUA often selects visually salient but instruction-inconsistent elements (e.g., wrong product category or unrelated post) as presented in Fig 9, revealing bias toward superficial cues over explicit constraints like ownership (my post) or topical relevance (Starfleet Academy). To mitigate these issues, three avenues may show promise: (1) Reflection and State Verification. Integrating lightweight screen-change detectors and visual precondition checkers can allow agents to validate action effects and avoid ineffective loops. (2) Reinforcement Learning with Recovery Signals. Reward structures should penalize redundant, non-progressive behaviors and incentivize predicate satisfaction (e.g., text selected, correct tab active) before proceeding. (3) MemoryAugmented Planning. By introducing episodic memory to recall past interactions (e.g., whether menu opened successfully), the agent can reason across time and avoid retrying failed subgoals. A.5 THE DETAILS OF DATA CURATION A.5.1 DATA SOURCES We systematically collect GUI data across diverse platforms to construct ScaleCUA-Data, including desktop, mobile, and web environments. As shown in Table. 14, ScaleCUA-Data spans 7 major operation systems: Windows, Ubuntu, macOS, iOS, iPadOS, Android, and Web. Each platform features broad spectrum of frequently used applications designed for productivity, communication, entertainment, browsing, and utilities. On desktop platforms, Windows includes both native and third-party applications such as Microsoft Office Suite, Adobe Creative Cloud, Visual Studio, and system utilities, offering comprehensive view of traditional GUI layouts. Ubuntu and macOS incorporate open-source and system software, including LibreOffice, GIMP, Terminal, Finder, and Safari. 28 Technical Report Mobile data is collected from the iOS and Android platforms. The data from the iOS platform includes system applications such as Settings, Safari, Calendar, and Health, as well as third-party applications including Weibo, Notability, and Spotify. The Android platform, by virtue of its open ecosystem, serves as the greatest diversity of data sources, encompassing both system applications and broad array of commercial software from domains such as productivity, e-commerce, social media, and multimedia (e.g., WeChat, Taobao, TikTok, and Google Suite). For tablet interfaces, our data collection primarily focused on iPadOS, encompassing selection of its most frequently utilized system applications. As for Web, we collected pages from over 200 frequently accessed websites spanning e-commerce, social media, education, government services, travel, and developer tools. These sources encompass major websites such as Amazon, YouTube, Reddit, Wikipedia, Coursera, and GitHub, with data captured through both static DOM snapshots and dynamic interaction traces. The collected dataset constitutes high-coverage, cross-platform corpus of real-world graphical interfaces which endows the model with comprehensive domain knowledge and leads to significantly improved generalization. A.5.2 GUI UNDERSTANDING To support the development of general-purpose computer use agents, we construct large-scale corpus for GUI understanding that encompasses both element-level and screenshot-level semantics. This corpus is designed to facilitate fine-grained perception and reasoning over static and dynamic user interfaces. For element-level understanding, we define five task formulations targeting visual appearance, spatial layout, textual grounding, and semantic functionality. First, we introduce the Element Appearance Captioning task, which requires the model to describe visual features (e.g., shape, color, borders) of given GUI component. These attributes often signal affordances and can help distinguish between interactive and static elements. Second, we incorporate Referring OCR, referring task where the model extracts the textual content within specified bounding box, enabling alignment between visual context and embedded text. Third, to capture spatial organization, the Element Layout Understanding task asks the model to predict both absolute screen coordinates and relative positions with respect to nearby components. Fourth, to understand the operational roles of components, we define the Element Functionality Captioning task, where the model infers the intended function of labeled element within its surrounding interface. Finally, we propose User Intention Prediction task, where the model is asked to infer the users likely goal based on contextual clues and ongoing interactions. For screenshot-level understanding, we formulate two tasks that promote global comprehension. The Interface Captioning task prompts the model to generate high-level textual description summarizing the overall structure, visual hierarchy, and content of the interface. This encourages holistic reasoning and layout recognition. Complementarily, the Screen Transition Captioning task focuses on temporal changes by asking the model to describe the differences between two consecutive screenshots. This enables the model to understand GUI dynamics, such as state updates, navigation events, or content refreshes. Together, these tasks define comprehensive benchmark for GUI understanding. We leverage visionlanguage models to automatically generate annotations for both element-level and screenshot-level tasks, using visual context, structural metadata, and interaction histories. This corpus provides the foundation for training agents capable of fine-grained perception, robust grounding, and high-level reasoning in complex GUI environments. 29 Technical Report Platform Windows Ubuntu MacOS iOS iPadOS Android Web Table 14: The main sources of GUI corpora across different platforms. Application File Explorer, OS, Chrome, Microsoft Edge, Word, Excel, PowerPoint, LibreOffice Calc, LibreOffice Impress, LibreOffice Writer, Maps, Camera, Calculator, Microsoft Store, Clock, Photos, Outlook, Media Player, VLC Media Player, Calendar, Paint, Paint 3D, QQ Music, KuGou Music, Spotify, Tencent QQ, Visual Studio Code, Dev-C++, Microsoft Solitaire & Casual Game, Pycharm, Android Studio, Vmware Workstation Pro, Vmware Fusion, Adobe Photoshop, Adobe Premiere Pro, Adobe Illustrator, Blender, FL Studio, Unreal Engine, DaVinci Resolve, AutoCAD, SolidWorks, Inventor, Vivado, MATLAB, Origin, Stata, Eviews Files, OS, Firefox, Chrome, LibreOffice Calc, LibreOffice Impress, LibreOffice Writer, OneNote, GIMP, Slack, Thunderbird, Visual Studio Code, Zotero Finder, OS, Safari, Chrome, Pages, Numbers, Keynote, Calculator, Maps, Notes, Calendar, Contacts, Reminders, Apple Music, Podcasts, Weather, Stocks, Freeform, Terminal, Clock, Pycharm, Android Studio, App Store, Mail, Visual Studio Code Weather, Maps, Find My, Settings, Stocks, Safari, Mail, Calendar, App Store, Home, Camera, Files, Wallet, Contacts, Shortcuts, Clock, Twitter, Weibo, Outlook, Reddit, Instagram, Notes, Keynote, Reminders, Notability, GoodNotes, Rednote, Translate, Calculator, Voice Memos, Shadowrocket, Music, Podcasts, Spotify, iTunes Store, Apple TV, Books, Zhihu, Health Weather, Settings, Safari, Camera, Goodnotes, Translate, Notes, Freeform, Chrome Settings, Clock, Desktop Clock, Calendar, Contacts, Files, Camera, LinkedIn, Weibo, Twitter, Tieba, Reddit, Zoom, Gmail, Duolingo, Xueersi, Wikipedia, XuetangX, edX, Coursera, Skillshare, ZLibrary, To Do, Word, Excel, PowerPoint, OneNote, Taskade, Notion, TickTick, Google Maps, AMap, Tencent Map, Qunar, Trip.com, Ctrip, Qunar, LY.com, Fliggy, Zhixing Train Tickets, Map.me, Booking, Amazon, eBay, Taobao, Alipay, Poizon, VIPShop, 58.com, Beike, Anjuke, Zhuanzhuan, Douyin Mall, Shihuo, Nike, Bilibili, Bilibili CN, QQ Music, himalaya, Classical Music, News, Toutiao, Sohu News, NetEase News, Hupu, Huya, Sohu Video, Pi Music Player, NetEase Cloud Music, Kuaishou, Kugou, WeSing, Douban, Xiaohongshu, Zhihu, Qidian, Xiaoheihe, Prime Video, CNN, Quora, Cantook, Spotify, Apple Music, YouTube, Fitness, Health, JD Health, Translate, Moji Weather, App Store, Google Chrome, BlueCoins, VPN, Shadowrocket, Surfboard, Speedtest, Meitu, Jianying, Canva, Procreate, Pinterest, GitHub, DeepSeek, Grok 5i5j(sh.5i5j.com), AccuWeather(accuweather.com), adidas China(adidas.com.cn), Adobe(adobe.com), Amazon(amazon.com), American Kennel Club(akc.org), Apple(apple.com), arXiv(arxiv.org), BabyCenter(babycenter.com), Baidu(baidu.com), Baidu Baike(baike.baidu.com), Baidu Tieba(tieba.baidu.com), Beihang University(buaa.edu.cn), Bilibili(bilibili.com), BoardGameGeek(boardgamegeek.com), BoardMix(boardmix.cn), Booking.com(booking.com), Budget(budget.com), Cambridge Dictionary(dictionary.cambridge.org), Cars.com(cars.com), CNBlogs(cnblogs.com), CNN(cnn.com), CoinMarketCap(coinmarketcap.com), Coursera(coursera.org), CSDN(csdn.net), Ctrip(ctrip.com), Damai(damai.cn), Dianping(dianping.com), Dior(dior.com), Douban(douban.com), Douyin(douyin.com), Drugs.com(drugs.com), eBay(ebay.com), Britannica(britannica.com), ePay(epay.com), Epicurious(epicurious.com), Facebook(facebook.com), Fastly(fastly.com), FedEx(fedex.com), Fliggy(fliggy.com), Food Network(foodnetwork.com), Gaode Maps(gaode.com), Gmail(gmail.com), GitHub(github.com), Google Finance(finance.google.com), Google Maps(map.google.com), Google Scholar(scholar.google.com), GOV.UK(gov.uk), Healthline(healthline.com), Hugging Face(huggingface.co), Hupu(hupu.com), IGN(ign.com), IMDb(imdb.com), Indeed UK(uk.indeed.com), iQiyi(iqiyi.com), JD.com(jd.com), JetBrains(jetbrains.com), KAYAK(kayak.com), Kohls(kohls.com), Last.fm(last.fm), LeetCode(leetcode.cn), LinkedIn(linkedin.com), Marriott(marriott.com), Microsoft Azure(azure.microsoft.com), Microsoft Office(office.com), ModelScope(modelscope.cn), MSN(msn.com), NBA(nba.com), National Relocation(nationalrelocation.com), NetEase Cloud Music(music.163.com), Newegg(newegg.com), OpenStreetMap(openstreetmap.org), PayPal(paypal.com), PJLab GitLab(gitlab.pjlab.org.cn), QQ(qq.com), QQ Music(y.qq.com), QS China(qschina.cn), Reddit(reddit.com), Redfin(redfin.com), REI(rei.com), Rotten Tomatoes(rottentomatoes.com), Ryanair(ryanair.com), Samsung(samsung.com), Shimo(shimo.im), Sina News(news.sina.com.cn), Skype(skype.com), SpotHero(spothero.com), Stack Overflow(stackoverflow.com), Steam Store(store.steampowered.com), Student.com(student.com), TensorFlow(tensorflow.org), Tencent Docs(docs.qq.com), Tencent Video(v.qq.com), The Weather Channel(weather.com), The Weather Network(theweathernetwork.com), Thumbtack(thumbtack.com), Ticket Center(ticketcenter.com), Trip.com US(us.trip.com), TripAdvisor(tripadvisor.com), UNIQLO China(uniqlo.cn), United Airlines(united.com), University of Cambridge(cam.ac.uk), University of Michigan(umich.edu), Vmall(vmall.com), Virginia DMV(dmv.virginia.gov), WebArena Forum(wa forum), WebArena GitLab(wa gitlab), WebArena Shopping(wa shopping), WebArena CMS(wa shopping admin), WebMD(webmd.com), Weibo(weibo.com), Wikipedia(wikipedia.org), WolframAlpha(wolframalpha.com), X(x.com), Xiaohongshu(xiaohongshu.com), Yahoo Finance(finance.yahoo.com), Yahoo Sports(sports.yahoo.com), Yelp(yelp.com), YouTube(youtube.com), Zhihu(zhihu.com), Zhaopin(i.zhaopin.com), Zhaopin Landing Page(landing.zhaopin.com), Zhipin(zhipin.com) and 0.2M URLs selected from TOP-1M URLs(https://tranco-list.eu/ ) A.5.3 METADATA EXTRACTION Windows Platform. To facilitate the automated analysis and interaction with graphical user interfaces (GUIs), we design and implement framework for extracting UI metadata on the Windows operating system. The core of this framework leverages the UI Automation (UIA) technology to perform depth-first traversal of an applications A11y Trees, initiated from the foreground window identified via native Win324 API calls. Subsequently, the collected raw data undergoes multi-stage filtering and refinement pipeline to ensure its relevance and actionability. This pipeline first performs geometric validity check to filter out improperly sized or off-screen controls, followed by visibility and occlusion analysis to retain only the topmost, unobscured elements. Furthermore, semantic pruning module uses predefined keyword list (e.g., close, save) to remove controls that might cause task interruption, while system component exclusion module discards elements within standard OS regions like the taskbar based on their absolute coordinates. Each element that 4https://learn.microsoft.com/en-us/windows/win32/ 30 Technical Report successfully passes through this pipeline is then abstracted into structured JSON object. This object encapsulates its multi-dimensional attributes, including identity properties (control type, name), state information (is enabled), spatial coordinates (bbox), and descriptive text (description, tooltip). The aggregation of these objects yields comprehensive metadata representation of the UI, establishing the foundation for subsequent automated tasks. Ubuntu Platform. To extract Ubuntu metadata, we process an XML string representation of the A11y Trees, leveraging Pythons built-in xml library for parsing5. The process commences by parsing the raw XML data into tree structure. Following this, we linearize these nodes into structural elements. Specifically, for each node in this set, we programmatically extract key attributes, including its tag (representing the elements role), name, class, and description. To capture the semantic content robustly, the elements text is derived either directly from its text content or inferred from its value attribute, particularly for input fields. Positional and dimensional data are extracted from screencoord and size attributes, which together define the elements bounding box. The final output is structured, tab-separated string where each line represents single UI element. This entry is composed of seven fields: (1) tag indicating the UI type, (2) name for the elements given name, (3) text capturing its content or value, (4) class specifying its component class, (5) description for accessibility-related details, (6) position as top-left (x, y) coordinate, and (7) size as width and height pair. In essence, this process distills raw, platform-specific A11y Trees into flattened, semantically-annotated dataset, providing crucial foundation for downstream understanding, grounding tasks. MacOS Platform. We extract UI metadata from macOS applications by leveraging the macOS Accessibility API, primarily via the ApplicationServices6 frameworks. It allows structured traversal of the A11y Trees by programmatically accessing on-screen UI windows and querying attributes such as AXPosition, AXSize, AXRole, AXTitle, AXValue, and AXDescription. To initiate the process, we identify top-level windows from the system window list using CGWindowListCopyWindowInfo, filter for visible application windows, and create AX references using AXUIElementCreateApplication. recursive collection strategy is then applied, traversing each windows A11y Trees up to bounded depth while filtering out offscreen or irrelevant elements. To ensure semantic clarity, we enrich metadata by inferring contextual labels for interactive elements (e.g., AXButton, AXTextField) based on their surrounding static text, spatial layout, and role. Further, we apply spatial deduplication heuristics to eliminate overlapping or redundant elements, and merge content-bearing AXStaticText regions with their parent interactive widgets when appropriate. The final output is flattened list of UI elements, each annotated with role, text content, description, and bounding box information. Structurally, each metadata entry consists of: (1) role indicating UI type (e.g., AXButton), (2) text and description capturing semantic content, (3) bbox dictionary with x, y, width, and height, and (4) optionally list of children for nested components. This pipeline enables robust and interpretable extraction of macOS GUI structures, supporting downstream tasks such as screen annotation, interaction modeling, and agent behavior learning. Additionally, due to the limited accessibility information exposed by some system-level macOS applications or the difficulty in filtering non-visible elements, we incorporate omniparser-v2 as complementary mechanism to refine and validate extracted elements based on screenshot alignment and bounding box overlap. Mobile Platform. For Android, we begin by using UIAutomator27 to dump the current apps accessibility hierarchy as XML and parse it into an in-memory lxml tree. In depth-first walk, we record each nodes class, resourceID, text and content description, and parse its bounds string (e.g. \"[x1,y1][x2,y2]\") into integer coordinates to build robust locators and raw geometry. During this pass we filter out any control that is off-screen, too small (for example, width < 5 px or height < 15 px), devoid of both text/description and interaction flags (clickable, focusable, scrollable, or long-clickable), or fully occluded by its parentleaving only truly visible, actionable elements. For each remaining node, we generate concise label by combining up to the first ten words of its text or description with its UI role (e.g. Button or EditText) and infer possible actions (click, swipe, long press, write). In second sweep, we detect exactly which elements support taps, focus moves, scrolling, or long presses, then wrap each 5https://docs.python.org/3/library/xml.etree.elementtree.html 6https://developer.apple.com/documentation/applicationservices 7https://uiautomator2.readthedocs.io/en/latest/ 31 Technical Report into structured record containing its unique identifier, bounding-box coordinates, summary of core attributes (ID, text, type, state flags like enabled and visible to user), and the full raw attribute map (package, index, checkable/checked, password, etc.). Finally, we serialize this collection as flat JSON array or tab-separated lines, producing complete, coordinate-aware metadata set that underpins precise mobile UI analysis and automated testing. For iOS, we feed the screenshot directly into OmniParser V2 (Yu et al., 2025), which parses the page elementsextracting their type, bounding box, interactivity, content, and so onand uses this information as metadata. Web Platform. Our web metadata extraction pipeline employs Selenium WebDriver8 with ChromeDriver9 to automate web interaction trajectory acquisition using random walk algorithm. At each step, it leverages browser-native rendering to ensure visual fidelity while capturing the current pages element metadata, including coordinates, descriptions, types, and special attribute information. The pipeline executes JavaScript parsing pipeline via Chrome DevTools Protocol (CDP) that implements comprehensive element classification and filter methodology. Clickable elements are identified through multi-criteria approach combining semantic HTML tags (<a>, <button>, <input>, <select>, <textarea>, <option>, <video>), CSS properties (cursor:pointer, since CSS properties cascade to child elements, we only treat an element as clickable if its parent lacks cursor:pointer, ensuring accurate detection of standalone clickable elements), JavaScript click event listeners, and element attributes (onclick, ondblclick, roles contain button, option, tab); Non-interactive elements are systematically classified as text objects, media objects, or structural panels through DOM hierarchy analysis. All elements undergo rigorous validation including geometric verification using getBoundingClientRect() to filter occluded components, visibility validation through CSS property checks (display:none, visibility:hidden, opacity:0), and active validation via document.elementFromPoint() center-point sampling to confirm visual prominence and top-layer activity. Finally, we perform set difference operation with the elements from the last step to filter out the set of new elements for the random walk. Text description metadata aggregation incorporates content from over 12 attributes including textContent, innerText, value, alt, title, and aria-label, normalized through whitespace compression algorithms. The framework implements multiple integrity safeguards including dynamic language detection via langdetect10, sensitive lexicon pattern matching, and visual anomaly detection with adaptive boundary refinement. Cross-resolution robustness is achieved through randomized viewport initialization spanning device pixel ratios (1.42.1) and common resolutions (720p, 1080p, 2K, 4K, 25601600), stabilized via CSS viewport normalization techniques. This comprehensive web trajectory metadata extraction pipeline ensures exceptional data integrity, security, diversity, granularity, and accuracy, thereby establishing robust foundation for instruction construction and model training. A.5.4 GUI GROUNDING GUI grounding is fundamental capability for computer use agents, enabling them to associate the natural language instruction with corresponding region of interest. Effective grounding determines whether the agent can interact with the correct interface components, directly impacting its ability to complete downstream tasks. In fact, grounding-only agent can be paired with general-purpose planner (e.g., GPT-4o (Hurst et al., 2024)) to complete tasks via modular style. To support various grounding demands, we construct multi-format GUI grounding corpus with three distinct supervision targets: point grounding, bounding box grounding, and action grounding. Point grounding requires the model to identify single pixel-level location, typically the center of button, icon, or control, that corresponds to user instruction. Bounding box grounding extends this capability by predicting rectangular regions that encapsulate target elements, which is particularly useful for operations involving region selection, such as dragging or editing. Action grounding combines spatial localization with operational semantics by producing an executable command, such as click(x=105, y=23), that aligns with the intended interaction. As for the annotation, we reuse structured annotations generated during the GUI understanding stage. Specifically, appearance, spatial, and functional descriptions of each UI element provide rich supervision signals. The center point and bounding box coordinates are extracted directly from UI layout metadata or visual parsing 8https://www.selenium.dev/ 9https://www.google.cn/chrome 10https://github.com/Mimino666/langdetect 32 Technical Report modules. Action-level grounding pairs these spatial targets with predefined atomic operations based on the elements inferred function. In addition, we explore data augmentation strategies to expand the grounding corpus. Specifically, we filter out previously annotated elements from the metadata and use prompt templates combined with GPT-4o to generate larger set of grounding annotations. This augmented data is designed to improve the models generalization ability across diverse GUI layouts and interaction patterns. This annotated corpus serves as foundation for learning robust visual-linguistic alignment and facilitates both direct interaction and integration with high-level task planners. A.5.5 WEAK-SEMANTIC TRAJECTORY While the trajectories collected by rule-based agents do not correspond to explicit task objectives, we incorporate heuristics into the exploration process to encourage transitions into deeper and less frequently visited interface states. This results in more diverse and representative interaction sequences, which are critical for training agents to generalize across complex GUI structures. To further exploit the potential of these unsupervised trajectories, we segment long interaction sequences into shorter, weakly semantic sub-trajectories. The segmentation is based on screenshot similarity: when current screen is visually similar to previous one, it often indicates that the agent has reached terminal or redundant interface state with minimal novelty in further interactions. These similarity-based boundaries serve as natural points for restarting exploration, thereby improving coverage and trajectory diversity. We refer to the resulting sequences as weak-semantic trajectories, as they preserve partial continuity and structural coherence without being aligned to manually defined tasks. Despite their lack of strong supervision, such trajectories often reflect meaningful UI flows, especially when the agent is biased toward newly rendered elements. We hypothesize that exposure to weak-semantic trajectories can help the agent internalize common patterns of GUI interaction and enhance its planning ability. If validated, this approach may offer cost-effective alternative to large-scale manual annotation, accelerating the evolution of more capable computer use agents through low-cost, high-coverage exploration. A.5.6 HUMAN-CURATED TRAJECTORY In addition to rule-driven exploration, we incorporate human-curated trajectories to address the limitations of automatically collected data. While rule-driven agents enable scalable collection, they inherently exhibit stochasticity and often fail to uncover certain goal-directed operations, especially for tasks requiring deep or context-specific interactions. Moreover, although weak-semantic trajectories segmented from raw explorations provide partial structure, their action sequences are not always aligned with human reasoning. As result, they may contain fragmented or noisy behaviors that limit their utility for downstream training. To overcome these limitations, we design human-in-the-loop protocol for collecting high-quality task trajectories. We begin by constructing seed task set, categorizing applications into common use domains such as daily utilities, entertainment, and productivity. For each domain, we identify representative applications and select frequently used functions based on user documentation and empirical analysis. Annotators are then instructed to convert these functions into clear, goal-oriented task descriptions, ensuring linguistic clarity and operational feasibility. Using our unified crossplatform recording system, human experts remotely interact with each application environment encapsulated within Docker container. This design provides process isolation, avoids side effects such as misoperation. Annotators are able to finish tasks in natural and fluent manner, producing coherent action trajectories that reflect realistic usage patterns across platforms. These curated trajectories serve as high-quality supervision for training agents with accurate planning and execution capabilities. They complement the broader, noisier dataset collected via automation, and provide reference paths that guide model alignment with human intent and behavior. A.5.7 ANNOTATION SCHEMES In our data acquisition, we collect screenshots along with their metadata, which includes all potentially interactive elements on the page. Since different exploration paths can lead to the same state and 33 Technical Report common states like the homepage are visited frequently, we employed image feature similarity to deduplicate these screenshots. This yields unique set of interface screenshots paired with their corresponding metadata. To reduce redundancy and mitigate noise within the metadata, we randomly sample 25 to 40 elements per screenshot. These elements are then semantically filtered using GPT-4o to ensure both efficacy and diversity. For each retained element, we mark its position on the image using red box with an arrow. By combining with associated metadata, we prompt GPT-4o to generate appearance and position descriptions, and Claude-3.7-Sonnet (Anthropic, 2025) to generate functional descriptions. These serve as ground truth annotations for our Element Appearance Captioning, Element Layout Understanding, and Element Functionality Captioning, respectively. These descriptions are further used to construct grounding tasks, where the appearance and position descriptions are used for non-action grounding and the function description is used for action-based grounding. To simulate all possible positions of elements and accommodate wider range of usage scenarios, we perform data augmentation. This includes simulating higher resolutions by stitching two images together, as well as cropping elements and pasting them onto solid-color backgrounds or real-world backgrounds from images captured by the authors own device. For each unique interface screenshot, GPT-4o is also used to generate an overall caption. If the image was not the final step of trajectory, we additionally provided GPT-4o with the subsequent screenshot along the same exploration path to summarize the UI changes and infer the intention. These are used for Screen Transition Captioning and User Intention Prediction tasks. For all trajectories, we provide Claude-3.7-Sonnet with the current and next screenshots, as well as cropped image of the interacted element, to infer both the step-level instruction and the reasoning process. For weakly semantic trajectories that primarily involve navigation across pages, we generate high-level task objectives. To do this, we provide Claude-3.7-Sonnet with the first and last screenshots of the trajectory to synthesize navigation-related task goal. Considering that different annotators have varying styles of writing instructions and different operational habits, we implement two types of augmentations for trajectories to improve model generalization. The first is instruction augmentation, where we prompt the model to generate task instructions in diverse styles, aiming to cover all possible user scenarios. The second is trajectory augmentation, for which we prompt the model to generate several step-level instructions and the reasoning process based on the trajectory. This can help mitigate the noise introduced by model labeling. All prompts used for annotation are provided in the Appendix A.9.2. A.5.8 MORE DETAILS OF DATA DISTRIBUTION To support the development of general-purpose GUI agents, we curate large-scale training corpus spanning three major task domains: Understanding, Grounding, and Task Planning. As shown in Table 15, our dataset comprises 471.4K examples for GUI Understanding, focusing on capabilities such as OCR, UI Comprehension, and Layout Comprehension, Screen Transition Reasoning sourced from 355.5K unique screenshots. For GUI Grounding, we annotate over 17.1M examples across multiple formats including point, bounding box, and actiongrounded on 1.6M images. These annotations enable precise visual-textual alignment across diverse platforms and screen configurations. For Task Planning, we provide 19K unique trajectories categorized into three types: Weak Semantics, Human-Curated, and Enhanced. The Human-Curated Trajectories consist of 4K high-quality goaloriented demonstrations collected by experts, while the Enhanced Trajectories incorporate diverse instructions and augmented reasoning steps, totaling 48.2K examples. Weak Semantics Trajectories derived from rule-based exploration with limited semantic guidance are used for supplementing navigation knowledge. Fig. 10 provides hierarchical view of the trajectory composition across platforms and types. By integrating agent-generated and expert-curated signals, we ensure both data diversity and quality. In conclusion, our ScaleCUA-Data delivers the largest GUI grounding dataset to date, coupled with substantial understanding and planning examples. Its platform coverage and hierarchical task composition form comprehensive foundation for training robust, cross-platform GUI agents. The performance of ScaleCUA validates the quality of ScaleCUA-Data, and highlights future directions in data-centric agent training. 34 Technical Report Table 15: Distribution of examples in our training corpus. Task Domain Tasks #Images #Examples Understanding Element Appearance Captioning, Referring OCR, Element Layout Understand, Element Functionality Captioning, User Intention Prediction, Interface Captioning, Screen Transition Captioning 355.5K 471.4K Grounding Bounding Box Point, Action Task Planning Weak Semantics Trajectories Human-Curated Trajectories Enhanced Trajectories 1.6M 17.1M 5.5K 29.3K 29.3K 15.0K 4.0K 48.2K Figure 10: hierarchical visualization of the trajectory composition. A.6 DATA VISUALIZATION A.6.1 GUI UNDERSTANDING To qualitatively demonstrate our data in GUI understanding tasks, we provide examples that cover both element-level and screenshot-level understanding. At the element level, we have designed five distinct tasks regarding individual GUI elements. Table 16 showcases specific examples of these tasks. At the screenshot level, we focus on the ability to comprehend the entire GUI interface globally and its dynamic changes. Table 17 provides examples for these two tasks. A.6.2 GUI GROUNDING To enable precise interaction, model must be able to map natural language instructions to specific elements on an interface. As mentioned in Section 4.2, we support three distinct GUI grounding modes to accommodate variety of scenarios and Table 18 provides visual illustration of these three modes. Technical Report Table 16: Task visualization for element-level understanding. Examples of Five Types of Tasks. [Element Appearance Captioning] Q: Please describe the appearance of the element marked in the image. A: rectangular search bar with white/light gray background and placeholder text Search with Google or enter address in light gray font. The search bar has smooth, rounded corners and subtle border. It appears to be empty with no entered text. [Referring OCR] Q: Please output the text content correctly responding to this term: <box>[[61, 563, 435, 651]]</box> A: rectangular search bar with white/light gray background and placeholder text Search with Google or enter address in light gray font. The search bar has smooth, rounded corners and subtle border. It appears to be empty with no entered text. [Element Layout Understanding] Q: Describe the position of the highlighted elements in the image, such as their location relative to other objects, alignment, and any spatial relationships. A: Located in the top toolbar of the application window, positioned between the interface navigation controls on the left and the viewport controls on the right. It sits approximately in the upper third of the screen, within the main toolbar area. [Element Functionality Captioning] Q: Describe the position of the highlighted elements in the image, such as their location relative to other objects, alignment, and any spatial relationships. A: Use the size grip at the corner of the PowerPoint Options window to change its size [User Intention Prediction] Q: Analyze the current image and the provided action click(x=540, y=1686), then predict the users intent based on these inputs A: The user likely intends to view detailed information or complete list of their past workout activities, specifically the Latest: Karate Power Fat Burn session recorded on 8/25/2024 at 20:27, as the label Show all suggests an expansion or summary of historical data. A.6.3 WEAK-SEMANTIC TRAJECTORY As mentioned in Sec. A.5.5, the weak-semantic trajectory processing pipeline begins with rule-driven agents performing depth-first exploration across GUI environments. These agents generate extensive 36 Technical Report Table 17: Task visualization for screenshot-level understanding. Examples of Two Types of Tasks. [Interface Captioning] Q: Provide detailed description in the current image. A:The image shows weather application interface displaying the forecast for Shanghai. The current temperature is 66F (about 19C) with sunny conditions. The interface shows detailed forecast for Friday, April 4, 2025, with high of 69F and low of 48F. The hourly forecast indicates temperatures will range from 66F to 50F throughout the day, with sunset occurring at 6:14 PM. Wind conditions show speed of 3 MPH with gusts up to 11 MPH. The interface also displays additional weather information including visibility (14 mi), UV index (7 - High), and five-day forecast showing temperatures for upcoming days. The air quality map shows green areas indicating good air quality around the users location. [Screen Transition Captioning] Q: Analyze the differences between two consecutive GUI screenshots. Describe the initial state, highlight the changes in the second screenshot. A:In the first screenshot, we see the Trip.com mobile app homepage with various booking options including Hotels, Flights, Flight + Hotel, Trains, and other services. After tapping on Flight + Hotel option, the second screenshot shows the Flight + Hotel booking page where users can input their travel details. The page displays one-way/return toggle (with Return selected), origin and destination fields set to Hong Kong and Dubai respectively, travel dates (Wed, Apr 23 - Sat, Apr 26 for 3 nights), traveler count settings, and search button at the bottom. interaction sequences by randomly selecting actions from available options at each step, while heuristic pruning eliminates redundant branches to improve efficiency. Although these trajectories lack explicit high-level goals, they provide broad coverage of interface states. To enhance the utility of these unsupervised trajectories, we segment them into coherent sub-sequences based on visual similarity between screenshots. Fig. 11 demonstrates the exploration pattern in desktop environments, while Fig. 12 and 13 show the equivalent processes for mobile and web platforms, respectively. In each case, the random-walk agent generates comprehensive navigation patterns through systematic exploration of the interface. These weak-semantic trajectories serve as valuable training signals despite their lack of strong task alignment. They expose models to common GUI interaction patterns and state transitions, complementing the more structured but limited human-curated data. Figure 11: An example of weak semantic trajectory on the Ubuntu platform. 37 Technical Report Table 18: Task visualization for GUI grounding Examples of Three Modes. this UI [Point Grounding] Q: Return element: point within the <ref>Preformatted Text button in the EDITOR tabs formatting toolbar that allows users to insert pre-formatted text tags in MATLABs editor.</ref> A: <ref>Preformatted Text button in the EDITOR tabs formatting toolbar that allows users to insert pre-formatted tags in MATLABs editor.</ref><point>[[223, text 45]]</point> [Bbox Grounding] Q: Indicate the location with bounding box to this UI element: <ref>A white-faced analog clock with black numerals (1-12) and three hands, placed in the upper left corner.<ref> A: <ref>A white-faced analog clock with black numerals (1-12) and three hands, placed in the upper left corner.<ref>>[[97, 69, 218, 227]]<bbox> [Action Grounding] Q: Click the Open option to open the selected file A: <action>click(x=0.7983, y=0.4967) </action> Figure 12: An example of weak semantic trajectory on the Android platform. Figure 13: An example of weak semantic trajectory on the web platform. A.6.4 HUMAN-CURATED TRAJECTORY Fig. 14-18 illustrate human-curated trajectories across five platforms: Windows, Ubuntu, macOS, Android and Web. Each trajectory demonstrates precise human-annotated interactions, rendered as mouse/gesture traces over consecutive screenshots, forming high-quality demonstrations for data 38 Technical Report collection. These trajectories span diverse applications such as Excel, SolidWorks, Gmail, Numbers, Amap, Twitter/X, and GitHub, showcasing real-world complexity in cross-platform environments. The visualizations highlight platform-specific GUI logic (e.g., desktop file operations vs. mobile touch navigation), as well as long-horizon reasoning steps (e.g., multi-page exploration, search-before-edit workflows). Figure 14: Examples of human-curated trajectories on the Windows platform. Figure 15: Examples of human-curated trajectories on the Ubuntu platform. A.6.5 TRAJECTORY ANNOTATION Building upon the annotation schemes detailed in Sec. A.5.7, we systematically process trajectory data to generate high-quality training corpora. Our trajectory annotation focuses on two key aspects: (1) low-level operational instructions generated for each interaction step, (2) chain-of-thought rationales explaining the decision process. As demonstrated in Table 19, these annotations are formally represented using XML tags to distinguish between operational instructions (<operation>...</operation>) and their cognitive justification (<think>...</think>), which support both direct-action execution and reasoned-action paradigms. A.7 LESSONS FROM DATA ACQUISITION A.7.1 COMMON PROBLEMS Adaptive Bounding Box Tightening(ABT). The heterogeneity across platforms results in significant discrepancies in UI interaction logic and responsive regions. Therefore, we aim to ensure that the 39 Technical Report Figure 16: Examples of human-curated trajectories on the macOS platform. Figure 17: Examples of trajectory data collection on the Android platform. bounding boxes of UI elements are as close as possible to their central regions, in order to minimize inconsistencies in actionable areas across different platforms. As illustrated at the top of Fig. 19, these parsed bounding boxes are aligned with the boundaries of the UI elements; however, their centers, which serve as the ground truth for click prediction, often reside in empty regions rather than in areas with meaningful content. This misalignment introduces detrimental click-whitespace bias during model training. To mitigate this issue, we propose the Adaptive Bounding Box Tightening (ABT) algorithm. ABT dynamically refines bounding boxes through iterative, proportional contraction of their lateral boundaries. This contraction is guided by the detection of contiguous uniform-color regions, representing target whitespace, within defined tolerance thresholds. The process terminates when no significant contractible uniform regions remain, yielding substantially tighter bounding boxes where centers align with actual content, as shown in the bottom of Fig. 19. While ABTs effectiveness is inherently limited by complex backgrounds and gradients, empirical validation confirms its significant value in improving ground truth alignment for interfaces featuring simple solid-color backgrounds. This paradigm remains dominant in modern systems and web design. 40 Technical Report Figure 18: Examples of trajectory data collection on the web platform. Deep Exploration. Modeling GUI platform state transition graphs presents inherent complexity. Random walks, common approach, suffer from limitations: unpredictable transitions induce pervasive back edges, causing frequent state revisitation or trapping in local loops due to insufficient backtracking mechanisms. To address these issues and enable automated deep exploration for acquiring meaningful weakly semantic trajectories, we propose single-history-frame element filtering algorithm. Specifically, we use queue to maintain all interactive elements appearing in the last screenshot. At each exploration step during random walk, some of elements are filtered out when their Intersection over Union (IoU) exceeds predefined threshold and their textual content exactly matches any element in the queue. This guarantees exclusive interaction with elements absent in the preceding state, thereby actively steering exploration toward novel pages. This mechanism proves particularly effective for interfaces with persistent components (e.g., navigation bars, sidebars) or dense icon arrays, as evidenced in Fig. 20 where it achieves significantly broader page coverage and yields non-redundant, semantically valuable trajectories compared to conventional random walks. A.7.2 WINDOWS Cross-Framework UI Parsing Challenges and Denoising Strategies. When processing Java-based software like PyCharm and Android Studio, the standard Win32 API exposes significant limitations. As illustrated in Fig. 21, the Win32 API fails to effectively parse their UI structure, resulting in an incomplete A11y Tree. Consequently, we must switch to using the specialized Java Access Bridge (JAB11) API. The JAB successfully retrieves the complete A11y Tree (as shown in Fig. 22, thus resolving the issue. This requirement to adapt different APIs for various application frameworks significantly increases the complexity of our data collection efforts. Moreover, the raw A11y Trees present challenges: they are typically deeply nested, noisy, and the density of functionally relevant UI elements is low. To address these issues and improve data quality, we apply set of heuristic filters to prune and refine the tree. 23 visualizes this transformation, showing comparison of the A11y Tree before and after processing. Our filtering strategies exclude elements from background applications and select elements by their screen-to-area ratio, roles (e.g., button, text, hyperlink). Data Deduplication and Geometric Refinement. Data acquisition in Windows faces several significant data quality challenges. First, minimal UI changes following user interactions lead to high redundancy of UI elements due to nearly identical screen captures. Second, lack of layer information in the A11y Trees results in erroneous inclusion of occluded elements (e.g., dropdown).To overcome these challenges, we implemented multi-stage refinement pipeline. We first mitigate redundancy with similarity algorithm that filters images based on the Euclidean distance of their feature vectors. post-processing filter then identifies occluded elements by detecting solid-color regions within their bounding boxes. 11https://docs.oracle.com/javase/8/docs/technotes/guides/access/jab/index.html 41 Technical Report Table 19: Visualization of trajectory annotation Examples of Two Modes. [Reasoned-Action Mode] Q: Please generate the next move according to the UI screenshot, task and previous operations. Task: Search openai and like the first video, sorted by view count and video type. Previous operations: Step 1: Click on the search box at the top of the YouTube page and type openai Step 2: Search for OpenAI on YouTube and press Enter to see search results and the OpenAI channel page A: <think> The YouTube interface shows search for openai with filters button visible in the top navigation area. Clicking on the filters option would allow sorting videos by criteria such as view count, which is needed to complete the task of finding the most viewed OpenAI videos. This filtering functionality is essential to organize search results in way that aligns with the requirement to sort by view count and video type before liking the first video. </think> <operation> Click on the Filters button at the top right of the YouTube search results to access advanced filtering options for your search. </operation> <action> click(x=0.9043, y=0.0788) </action> [Direct-Action Mode] Q: Please generate the next move according to the UI screenshot, task and previous operations. Task: In the Calendar app, show only events in the School calendar. Previous operations: None A: <operation> Click on the blue checkbox next to Family in the calendar sidebar under On My Mac section. </operation> <action> click(x=0.0187, y=0.1128) </action> Prioritized Random Walk for Automated UI Exploration. The random walk algorithm is central to our automated data acquisition on the Windows platform. To minimize redundant interactions and enhance element diversity, we have augmented the standard Random Walk with principles from Depth-First Search (DFS). As mentioned in the above common problems, our modified algorithm prioritizes interaction with newly appeared UI elements while concurrently reducing the selection priority of elements that have already been interacted with. If no new elements are detected, or if their count falls below predefined threshold, the algorithm defaults to interacting with any remaining, previously unvisited elements within the current views A11y Trees. Furthermore, we account for scenarios where interactions navigate away from the primary application, such as launching web 42 Technical Report Figure 19: Examples of adaptive bounding box tightening(ABT) algorithm. Figure 20: Examples of deep exploration algorithm. browser to view user manual. In such cases, our algorithm allows for limited interaction within the external application (e.g., the browser) before automatically shifting focus back to continue navigating the initial application. A.7.3 UBUNTU This section details the challenges encountered and solutions developed for autonomous agent interaction with the Ubuntu environment. The primary challenges originate from the inherent structure of the accessibility tree (A11y tree), which serves as the main interface for observing and interacting with the application. Our solutions focus on refining the accessibility tree data and optimizing the agents interaction strategy to ensure reliable and efficient operation. The successful resolution of 43 Technical Report Figure 21: An example of Win32 API failing to parse A11y Trees in PyCharm. Figure 22: An example of JAB API successfully parsing A11y Trees in PyCharm. these issues is paramount, as clean, accurate, and efficiently navigable UI representation is the foundation for any effective automated UI-based task. Denoising in the Accessibility Tree. The raw data provided by the accessibility tree on Ubuntu is often noisy, containing redundant information and occasional inaccuracies that can mislead an autonomous agent. We identified and implemented solutions for three primary issues. First, the A11trees hierarchical structure often includes redundant parent elements that do not correspond to distinct interactive components. This is particularly prevalent in applications built with Web, such as Chrome. To address this, we apply two-stage filtering process. We begin by pruning elements whose roles are typically non-interactive or structural based on type, such as heading, paragraph, and section. This denoising is critical because it exposes the true, underlying interactive elements, preventing the agent from attempting to interact with large, non-interactive container widgets. Subsequently, we analyze the geometric relationships of the remaining elements bounding boxes. If an elements bounding box is significantly occluded by smaller one (i.e., the smaller boxs area occupies large percentage of the larger boxs area), we infer container-child relationship and discard the larger, containing element. Second, the standard accessibility tree does not inherently 44 Technical Report Figure 23: An example of denoising on Windows, transforming raw A11y Tree (top) into clear structure (down). account for the visual occlusion and invalidity of elements. An element may be present in the tree but be completely obscured by another element on the screen as shown in Fig. 24. We tackle this with LLMs. Third, we observed that for certain applications, the accessibility tree reports incorrect coordinates for all UI elements immediately after the application is launched, as shown in Fig. 25. The entire tree appears to have coordinate offset. Through empirical testing, we discovered practical solution: initiating short sequence of random interactions within the application window causes the accessibility trees coordinate system to recalibrate, restoring correct positional data. Ensuring coordinate accuracy is fundamental; without it, any attempt by the agent to click or type at specific location would fail, rendering automation impossible. GUI Exploration Optimization via an Improved Random Walk. pure random walk over all available UI elements is highly inefficient. To improve the agents ability to explore an applications state space, we developed more intelligent interaction strategy. This strategy is based on filtering the action space and prioritizing the exploration of novel UI states. To reduce the number of futile actions, the agents action space is constrained to only include elements that are designated as interactive. We maintain whitelist of interactive type, including button, box, menu, entry, link, bar, and item. Conversely, elements with non-interactive roles are excluded from the potential action 45 Technical Report set. These non-interactive roles include heading, static, document, label, cell, text, icon, paragraph, and section. To prevent the agent from becoming trapped in interaction loops within static UI state, we implemented state-aware exploration logic. After the agent acts, we only visit newly appeared UI elements. These novel elements are given interaction priority, as they are most likely to lead to new application state. If the action does not yield any new elements, the agent then selects an action randomly from the set of previously known elements that it has not yet interacted with in the current state. This process continues until all interactive elements have been exhausted. This exploration strategy is vital for efficiency, as it directs the agent towards discovering new functionalities and application states, thereby maximizing the coverage of the applications features in limited time and avoiding redundant, non-productive interactions. Figure 24: Examples of visual occlusion and invalidity of elements. Figure 25: Examples of coordinate offset. A.7.4 MACOS Robust A11y Tree Extraction and Denoising. The macOS pipeline first locates the active top-level window, then exhaustively traverses its accessibility hierarchy. Every bounding box is mapped from logical coordinates to device pixels by multiplying by the screen-scale factor. After flattening the tree, only nodes whose roles are interactive (e.g. AXButton, AXPopUpButton, AXTextField) are retained. Moreover, we would discard boxes with width or height of 2px or less and remove nodes 46 Technical Report whose text, description, and value are all empty or punctuation. role-aware merging process replaces overlapping AXStaticText siblings and their interactive parent with minimal bounding box. The resulting set contains clean, tightly localised interactive elements. (see Fig. 26). Hybrid A11y Tree & Omniparser combination for System Panels. Several built-in utilities, most notably System Settings, draw controls in private layers that have no corresponding accessibility tree Yu et al. (2025), as shown in Fig. 27. To recover these missing widgets, each screenshot is processed by Omniparser, yielding set of vision-detected bounding boxes. An element would be retained when its IoU with any Omniparser box exceeds 0.15 or when it is selected during exploration. This combination renders previously invisible elements in the A11y tree, thereby yielding more comprehensive understanding of macOS applications. Figure 26: Refined AXTree overlay on the Mail application: all interactive elements are tightly bound after heuristic pruning. Figure 27: The failure case in System Settings: the AXTree omits right-pane controls, illustrating the necessity of Omniparser fusion. 47 Technical Report A.7.5 MOBILE Occlusion and Invisibility Correction. Mobile interfaces frequently employ high-level components such as dialogs, side drawers, and floating menus. These are rendered at the topmost Z-order, so underlying nodes remain in the XML yet can no longer be clicked, producing ghost targets(see Fig.28a). To improve visibility and hierarchy accuracy at the source, we replace the traditional adb shell uiautomator dump with uiautomator2.dump_hierarchy(). The latter prunes recognisably occluded nodes while generating the XML and, for pages that adb fails to parse, still returns complete hierarchysignificantly increasing data coverage. Coupled with the random-walk heuristic that prioritises newly appeared elements, this greatly reduces mis-clicks caused by occlusion. In addition, UIAutomator2 markedly lowers the probability of XML retrieval failures, accelerating exploration efficiency. Attribute Completion and Correction. Many commercial apps do not fully propagate accessibility traits in their custom views; typical pattern is parent node with clickable=true while all its children are clickable=false, leading to the issue shown in Fig.28b. Genuine clickable regions are thus ignored. We employ an inherit-then-suppress strategy: when parent is clickable and every descendant is marked non-clickable, the clickable flag is inherited downward; if any descendant is already declared clickable, inheritance stops to avoid creating false hotspots. Experiments show that this method restores the vast majority of missing attributes while maintaining low false-positive rate. Semantic and Functional Ambiguities. Semantic ambiguity arises when an XML bounding box is too large and covers multiple sub-controls (for example, the playback button, author area and more-options button), making single node unable to convey precise meaning. In Fig.28c, the green box shows one clickable bounding-box region in the XML, but taps in different parts of that region may produce different results, creating semantic ambiguity. To address this, we prioritise leaf nodes and tighten their bounding boxes; we only retain parent node when its centre lies outside every childs bounds, thus preserving the overall intent of the composite control. Functional ambiguity occurs when the same layout triggers different actions in different software or operating systems. In Fig.28d and Fig.28e, for example, both the text and the icon of switch are tappable in stock Android settings, whereas in iOS only the icon responds to taps and the rest of the region is inert. To reduce such mispredictions, whenever we detect an icon + text sibling pattern we give the icon higher click priority. This approach produces more consistent cross-device behaviour during training and testing. By systematically handling overlay occlusion, attribute omissions and both semantic and functional ambiguities, we significantly improve the reliability of mobile-side data collection and increase the success rate of downstream automation tasks. A.7.6 WEB Addressing Limitations in Automation Tools. Automation tools like Automation tools like Selenium and Playwright suffer from critical limitation where their page.screenshot() function fails to capture native browser UI components rendered outside the DOM. This omission disrupts essential visual feedback for sequential decision-making in web agents. We categorize these problematic elements into two classes: predictable UI triggered by deliberate actions (e.g., context menus, tab navigation, forward/back buttons), and unpredictable UI emerging during tasks (browser dialogs and native select dropdowns). The inherent invisibility of predictable UI components prevents agents from developing interaction intentions for these features; while our methodological constraint limiting interactions to left-clicks effectively eliminates potential negative impacts from this omission. However, to compensate for the unavoidably reducing behavioral diversity in captured data and ensure comprehensive functional coverage, we conducted extensive web data collection in native desktop environments, enriching our training corpus with full-spectrum browser interaction examples. The unpredictable UI category proves more severe, as evidenced in Fig. 29 (Playwright Origin vs. OS Screenshot), where missing elements prevent task completion and impact evaluation integrity. Our behavioral simulation solution addresses this: for select elements, JavaScript modifies the size attribute to visually expand options within the DOM, with event listeners reverting the state; for dialogs, an interceptor captures properties, dismisses the native instance, and injects visually identical DOM-based replica with non-functional buttons. The efficacy of this approach is demonstrated in Fig. 29 (Modified Screenshot), which illustrates the successful visual simulation of both UI components. While other potential related issues may exist beyond our current observations, 48 Technical Report (a) Mobile element occlusion (b) Mobile element attribute loss (c) Semantic ambiguity (d) Effective bounding box for setting WiFi in Android (e) Effective bounding box for setting WiFi in iOS Figure 28: Examples of potential challenges in mobile data acquisition:(a) The problem of occluded elements being indistinguishable during XML extraction.(b) The potential inaccuracy of extracted bounding boxes due to loss of element attributes.(c) The problem of semantic ambiguity caused by insufficiently detailed XML extraction.(d, e) Differences in the functionality of similar regions across different systems or apps. Figure 29: Examples of native browser UI limitations in automation tools. they have not manifested in our evaluation scenarios and thus remain outside the scope of our present investigation. Metadata Advantages and Parsing Challenges. Web page content, structured through HTML and DOM trees, inherently provides rich metadata advantages over alternative platforms. JavaScript enables precise element positioning and hierarchical analysis that significantly exceeds capabilities in other contexts, enhancing metadata extraction efficiency as illustrated in Fig. 30d. However, the heterogeneity of the web ecosystemdiverse frontend frameworks, inconsistent development practices, and variable standardsprevents comprehensive coverage by data collection algorithms. Two representative challenges emerge: First, as shown in Fig. 30a, developers misapply attributes such as role=button to non-interactive images, introducing semantic inconsistencies that cause parsing anomalies. Second, current algorithms exhibit deficiencies in hierarchical analysis and visibility detection, resulting in inadequate filtration of underlying or invisible elements as demonstrated in Fig. 30b. Considering the substantial volume of extractable elements in web environments, we propose that maximizing the recall of valid interactive elements should be the primary objective across platforms. This position advocates for aggressive filtering strategies rather than conservative approaches that might inadvertently retain invalid elements. While this methodology may occasionally exclude some valid elements, the benefits of reducing noise in the dataset significantly outweigh the potential costs of missing limited number of interactive elements. Technical Report It is particularly noteworthy that the technical limitations have not been explicitly addressed in the extant literature on WebAgent papers, despite their profound implications for agent functionality and evaluation methodology. We therefore advocate for increased attention to these considerations in future WebAgent research. Additionally, our analysis reveals that web environments lacking browser UI elements significantly constrain an agents exploration capabilities in the absence of compensatory action mechanisms (e.g., returning to previous pagea trivial operation when using browsers back buttonmay require complex navigation sequences or prove entirely infeasible within the constrained visual context available). Fortunately, the refined WebArena-Lite benchmark evaluation has been specifically designed to eliminate such problematic scenarios, thereby ensuring methodological integrity and evaluation reliability. Nevertheless, based on our findings, we strongly recommend that future research prioritize the execution of web-based tasks within native desktop environments, which may necessitate the development of new benchmarks and the migration of existing benchmarks. Temporal Synchronization in Dynamic Page States. The web platform exhibits substantial dynamism, frequently causing temporal discrepancies between page states during element parsing and screenshot capture. The non-instantaneous nature of parsing further compounds this issue by permitting mid-process element state changes. characteristic scenario involves the auto-hiding behavior of video player control bars, illustrated in Fig. 30c. Current mitigation strategies employ dual measures: Initially awaiting complete page stabilization, followed by proactively triggering state persistence for specific elementssuch as maintaining video control visibility through cursor hovering. Nevertheless, managing dynamic content remains core challenge in web data acquisition. Leveraging Multi-Source Textual Semantics. Web elements contain rich semantic description layers extending far beyond basic textContent compared to other platforms. Functional icons often convey operational semantics through alt and title attributes, while accessibility-compliant sites provide enhanced descriptions via properties like aria-label. Systematically aggregating these multi-source textual features establishes strong semantic associations, furnishing comprehensive contextual grounding for model annotations and effectively suppressing annotation hallucinations. A.8 EVALUATION A.8.1 CLARIFICATIONS ON WEB ONLINE BENCHMARK Current web platform evaluation benchmarks can be categorized into two main types based on the website environment. The first type utilizes real websites for online evaluation, primarily derived from the offline evaluation work Mind2Web (Deng et al., 2023). Examples include Mind2WebLive (Pan et al., 2024), Online-Mind2Web (Xue et al., 2025), and WebVoyager (He et al., 2024), with UI-TARS (Qin et al., 2025) employing WebVoyager and Online-Mind2Web for web domain evaluation. The second type conducts evaluations on locally deployed websites, pioneered by WebArena (Zhou et al., 2023), which leverages open-source website code and databases (Sun et al., 2024a) to provide highly simulated and interactive local Docker deployment environments for five functionally diverse websites, including GitLab, map services, forums, online shopping, and content management platforms (CMS). WebArena has constructed over 800 web tasks, inspiring derivative evaluation frameworks such as VisualAgentBench (WebArena-Lite) (Liu et al., 2024a) and VisualWebArena (Koh et al., 2024). Furthermore, the evaluation protocols can be classified into two categories: rule-based evaluation exemplified by WebArena (Zhou et al., 2023) and VLM-as-a-Judge evaluation, such as Online-Mind2Web (Xue et al., 2025). Rationale for Selecting Local Website Environments. We deliberately abandoned evaluation benchmarks based on real websites for several compelling reasons. The primary concern is the temporal instability of online environmentstasks that are currently feasible may become impossible due to website updates, domain changes, or site closures. Despite efforts by frameworks like Mind2Web-Live to maintain and update tasks periodically, such updates inevitably compromise evaluation fairness. Additionally, as noted in (Xu et al., 2024), automated tools frequently encounter anti-automation barriers such as reCAPTCHA verification. Moreover, since most target websites are hosted in the United States, researchers in non-US regions (particularly China) face persistent connectivity issues and access restrictions even with VPN servicesdifferent VPN providers often yield inconsistent access results. These factors significantly undermine fair model comparison and hinder the extraction of valuable insights from evaluation results. 50 Technical Report WebArena-Lite-v2. Consequently, we focused on the WebArena series, whose locally deployed website environments offer substantial stability and internal accessibility, enabling flexible task construction and evaluation design. Considering that WebArena often includes three or more iterations of the same task template, resulting in repetitive and time-consuming evaluations, we selected the WebArena-Lite subset, which provides 165 high-quality refined tasks. However, our empirical evaluation and manual inspection revealed persistent issues. Therefore, we further refined the benchmark to create WebArena-Lite-v2, comprising 154 tasks optimized for both headed browser environments and headless automation tool environments. Recent developments, such as OpenAIs Operator, demonstrate transition from headless environments provided by automation tools toward headed desktop browser environments for web agent evaluation. As detailed in A.7.6, both environments present distinct advantages and limitations. To facilitate comprehensive ablation studies on these different operational modes, WebArena-Lite-v2 ensures that all tasks can be solved through at least one viable path using desktop action spaces (without specialized web actions like go forward, go backward, open url, or tab switch) in both headed and headless environments. Furthermore, all tasks are designed to provide sufficient visual information guidance, eliminating the necessity for DOM information and thus making the benchmark suitable for pure vision-based evaluation (while remaining compatible with SoM or DOM-enhanced assessment). Finally, we implemented comprehensive yet flexible evaluation criteriacomprehensive in accommodating multiple possible solutions through the OR operator where satisfying any one solution is sufficient and flexible in employing LLM-based fuzzy match for semantic similarity assessment in tasks involving question answering or content completion. Discussions between WebArena-Lite and WebArena-Lite-v2. Our refinements encompass both environmental and task improvements. For the evaluation environment, we implemented two significant enhancements. First, we addressed the OpenStreetMap websites limitations, where the official Docker environment lacked local database storage for node information, rendering tasks like What is the phone number of Western Pennsylvania Hospital impossible to complete. We resolved this by importing Pennsylvania state PBF data, enabling the completion of such tasks. Second, we developed consistent solutions for headless automation environments to overcome the observation challenges with select option dropdowns and dialog windows, as illustrated in A.7.6 with Fig. 29. Regarding task refinement, we eliminated 11 tasks requiring multi-tab interactions, resulting in curated set of 154 tasks. We conducted comprehensive revision of instructions and evaluation functions for all remaining tasks. The instruction refinements encompassed semantic clarification, typographical correction, and minimal reconstruction of impracticable directives (e.g., the instruction Re-post the image of the costume contest in this page to the funny subreddit and note from /f/pics proved infeasible since headless environments lack image URL extraction capabilities). Our evaluation function enhancements incorporated supplementary valid solutions (e.g., for the query What is the zip code of Chatham University?, we augmented the answer from exclusively 15232 to 15232 OR 15208 after identifying multiple Chatham University locations through OpenStreetMap queries) and accommodated semantically equivalent solution expressions (e.g., for Show me products under $100 in Men Shoes category, we recognized SHOPPING /clothing-shoes-jewelry/men/shoes.html?price=0-100 both SHOPPING /clothing-shoes-jewelry.html?cat=145&price=0-100 as and valid pathways to identical content pages). This methodological approach ensures comprehensive answer validation. Additionally, acknowledging language models inherent variability in textual response generation, we systematically replaced all exact match evaluation criteria within the string match classification with more nuanced must include, must exclude, and fuzzy match parameters, thereby significantly enhancing evaluation robustness and interpretative flexibility. However, WebArenaLite-v2 still employs static evaluation methodologies for certain tasks (such as when identifying users most recent order, where the Ground Truth is predetermined as specific order number or webpage). Although executing evaluations within local environment has mitigated the impact of this limitation, critical future direction involves developing evaluation protocols that are both dynamic and precise. This advancement necessitates addressing the challenge of extracting Ground Truth information from web pages that may not have been accessed by the agent during its navigation trajectory. This capability is essential for comprehensive evaluation of agent performance across diverse web interaction scenarios. 51 Technical Report A.9 PROMPT ENGINEERING To facilitate reproducibility and offer practical guidance for future research, we include all prompt templates utilized throughout our work in this section. These prompts cover wide range of use cases, including data filtering, annotation, and the prompts used in our ScaleCUA. Specifically, we detail the instructions employed for GUI understanding, grounding supervision, and trajectory annotation, as well as those used to elicit reasoning traces and alternative actions. Each prompt is carefully crafted to align with the capabilities of large vision-language models such as GPT-4o and Claude-3.7, ensuring high-quality outputs for downstream training. By releasing these prompts, we aim to enhance transparency and support the development of more robust and interpretable computer use agents. A.9.1 PROMPTS FOR OUR AGENT To ensure generalizable and controllable agent behavior, we design structured system prompt template for ScaleCUA that explicitly encodes the available action space. This template serves as the foundational context for all three inference paradigmsGrounding Mode, Direct-Action Mode, and Reasoned-Action Modeguiding the model to produce spatially grounded and semantically aligned outputs. The system prompt defines the operational semantics of each action type, including spatial commands such as click(x, y), dragTo(x, y), and write(text), as well as higher-level control tokens like terminate and wait. We envision the system prompt as modular and extensible interface. In future iterations, we aim to decouple the action space definition from the core prompt logic, allowing for plug-and-play architecture that can dynamically adapt to the interaction paradigms of diverse computing platforms. This modularity would enable seamless integration of device-specific actions, such as swipe for mobile interfaces or hotkey for desktop environments, while preserving consistency in agent behavior. Our design lays the foundation for building unified prompting framework that can scale to arbitrary GUI-based control systems. System Prompt Template For Action Grounding Mode You are an autonomous GUI agent capable of operating on desktops, mobile devices, and web browsers. Your primary function is to analyze screen captures and perform appropriate UI actions to complete assigned tasks. ## Action Space def click( x: float None = None, y: float None = None, clicks: int = 1, button: str = \"left\", ) -> None: \"\"\"Clicks on the screen at the specified coordinates. The `x` and `y` parameter specify where the mouse event occurs. If not provided, the current mouse position is used. The `clicks` parameter specifies how many times to click, and the `button` parameter specifies which mouse button to use ('left', 'right', or 'middle').\"\"\" pass def doubleClick( x: float None = None, y: float None = None, button: str = \"left\", ) -> None: \"\"\"Performs double click. This is wrapper function for click(x, y, 2, 'left').\"\"\" pass def rightClick(x: float None = None, y: float None = None) -> None: \"\"\"Performs right mouse button click. This is wrapper function for click(x, y, 1, 'right').\"\"\" pass def moveTo(x: float, y: float) -> None: \"\"\"Move the mouse to the specified coordinates.\"\"\" pass 52 Technical Report def dragTo( x: float None = None, y: float None = None, button: str = \"left\" ) -> None: \"\"\"Performs drag-to action with optional `x` and `y` coordinates and button.\"\"\" pass def swipe( from_coord: tuple[float, float] None = None, to_coord: tuple[float, float] None = None, direction: str = \"up\", amount: float = 0.5, ) -> None: \"\"\"Performs swipe action on the screen. The `from_coord` and `to_coord` specify the starting and ending coordinates of the swipe. If `to_coord` is not provided, the `direction` and `amount` parameters are used to determine the swipe direction and distance. The `direction` can be 'up', 'down', 'left', or 'right', and the `amount` specifies how far to swipe relative to the screen size (0 to 1).\"\"\" pass def long_press(x: float, y: float, duration: int = 1) -> None: \"\"\"Long press on the screen at the specified coordinates. The `duration` specifies how long to hold the press in seconds.\"\"\" pass ## Input Specification - Screenshot of the current screen + task description ## Output Format <action> [A set of executable action command] </action> ## Note - Avoid action(s) that would lead to invalid states. - The generated action(s) must exist within the defined action space. - The generated action(s) should be enclosed within <action></action> tags. System Prompt Template For Direct Action Mode You are an autonomous GUI agent operating on the **{PLATFORM}** platform(s). Your primary function is to analyze screen captures and perform appropriate UI actions to complete assigned tasks. ## Action Space def click( x: float None = None, y: float None = None, clicks: int = 1, button: str = \"left\", ) -> None: \"\"\"Clicks on the screen at the specified coordinates. The `x` and `y` parameter specify where the mouse event occurs. If not provided, the current mouse position is used. The `clicks` parameter specifies how many times to click, and the `button` parameter specifies which mouse button to use ('left', 'right', or 'middle').\"\"\" pass def doubleClick( x: float None = None, y: float None = None, button: str = \"left\", ) -> None: \"\"\"Performs double click. This is wrapper function for click(x, y, 2, 'left').\"\"\" pass def rightClick(x: float None = None, y: float None = None) -> None: \"\"\"Performs right mouse button click. This is wrapper function for click(x, y, 1, 'right').\"\"\" pass 53 Technical Report def scroll(clicks: int, x: float None = None, y: float None = None) -> None: \"\"\"Performs scroll of the mouse scroll wheel at the specified coordinates. The `clicks` specifies how many clicks to scroll. The direction of the scroll (vertical or horizontal) depends on the underlying operating system. Normally, positive values scroll up, and negative values scroll down.\"\"\" pass def moveTo(x: float, y: float) -> None: \"\"\"Move the mouse to the specified coordinates.\"\"\" pass def dragTo( x: float None = None, y: float None = None, button: str = \"left\" ) -> None: \"\"\"Performs drag-to action with optional `x` and `y` coordinates and button.\"\"\" pass def press(keys: str list[str], presses: int = 1) -> None: \"\"\"Performs keyboard key press down, followed by release. The function supports pressing single key or list of keys, multiple presses, and customizable intervals between presses.\"\"\" pass def hotkey(*args: str) -> None: \"\"\"Performs key down presses on the arguments passed in order, then performs key releases in reverse order. This is used to simulate keyboard shortcuts (e.g., 'Ctrl-Shift-C').\"\"\" pass def keyDown(key: str) -> None: \"\"\"Performs keyboard key press without the release. This will put that key in held down state.\"\"\" pass def keyUp(key: str) -> None: \"\"\"Performs keyboard key release (without the press down beforehand).\"\"\" pass def write(message: str) -> None: \"\"\"Write the specified text.\"\"\" pass def call_user() -> None: \"\"\"Call the user.\"\"\" pass def wait(seconds: int = 3) -> None: \"\"\"Wait for the change to happen.\"\"\" pass def response(answer: str) -> None: \"\"\"Answer question or provide response to an user query.\"\"\" pass def terminate(status: str = \"success\", info: str None = None) -> None: \"\"\"Terminate the current task with status. The `status` specifies the termination status ('success', 'failure'), and the `info` can provide additional information about the termination.\"\"\" pass ## Input Specification - Screenshot of the current screen + task description + your past interaction history with UI to finish assigned tasks. ## Output Format <operation> [Next intended operation description] Technical Report </operation> <action> [A set of executable action commands] </action> ## Note - Avoid action(s) that would lead to invalid states. - The generated action(s) must exist within the defined action space. - The generated operation and action(s) should be enclosed within <operation></operation> and <action></action> tags, respectively. System Prompt Template For Reasoned-Action Mode You are an autonomous GUI agent operating on the **{PLATFORM}** platform. Your primary function is to analyze screen captures and perform appropriate UI actions to complete assigned tasks. ## Action Space def click( x: float None = None, y: float None = None, clicks: int = 1, button: str = \"left\", ) -> None: \"\"\"Clicks on the screen at the specified coordinates. The `x` and `y` parameter specify where the mouse event occurs. If not provided, the current mouse position is used. The `clicks` parameter specifies how many times to click, and the `button` parameter specifies which mouse button to use ('left', 'right', or 'middle').\"\"\" pass def doubleClick( x: float None = None, y: float None = None, button: str = \"left\", ) -> None: \"\"\"Performs double click. This is wrapper function for click(x, y, 2, 'left').\"\"\" pass def rightClick(x: float None = None, y: float None = None) -> None: \"\"\"Performs right mouse button click. This is wrapper function for click(x, y, 1, 'right').\"\"\" pass def scroll(clicks: int, x: float None = None, y: float None = None) -> None: \"\"\"Performs scroll of the mouse scroll wheel at the specified coordinates. The `clicks` specifies how many clicks to scroll. The direction of the scroll (vertical or horizontal) depends on the underlying operating system. Normally, positive values scroll up, and negative values scroll down.\"\"\" pass def moveTo(x: float, y: float) -> None: \"\"\"Move the mouse to the specified coordinates.\"\"\" pass def dragTo( x: float None = None, y: float None = None, button: str = \"left\" ) -> None: \"\"\"Performs drag-to action with optional `x` and `y` coordinates and button.\"\"\" pass def press(keys: str list[str], presses: int = 1) -> None: \"\"\"Performs keyboard key press down, followed by release. The function supports pressing single key or list of keys, multiple presses, and customizable intervals between presses.\"\"\" pass def hotkey(*args: str) -> None: 55 Technical Report \"\"\"Performs key down presses on the arguments passed in order, then performs key releases in reverse order. This is used to simulate keyboard shortcuts (e.g., 'Ctrl-Shift-C').\"\"\" pass def keyDown(key: str) -> None: \"\"\"Performs keyboard key press without the release. This will put that key in held down state.\"\"\" pass def keyUp(key: str) -> None: \"\"\"Performs keyboard key release (without the press down beforehand).\"\"\" pass def write(message: str) -> None: \"\"\"Write the specified text.\"\"\" pass def call_user() -> None: \"\"\"Call the user.\"\"\" pass def wait(seconds: int = 3) -> None: \"\"\"Wait for the change to happen.\"\"\" pass def response(answer: str) -> None: \"\"\"Answer question or provide response to an user query.\"\"\" pass def terminate(status: str = \"success\", info: str None = None) -> None: \"\"\"Terminate the current task with status. The `status` specifies the termination status ('success', 'failure'), and the `info` can provide additional information about the termination.\"\"\" pass ## Input Specification - Screenshot of the current screen + task description + your past interaction history with UI to finish assigned tasks. ## Output Format ``` <think> [Your reasoning process here] </think> <operation> [Next intended operation description] </operation> <action> [A set of executable action command] </action> ``` ## Note - Avoid actions that would lead to invalid states. - The generated action(s) must exist within the defined action space. - The reasoning process, operation and action(s) in your response should be enclosed within <think></think>, <operation></operation> and <action></action> tags, respectively User Prompt Template For Direct-Action Mode and Reasoned-Action Mode Please generate the next move according to the UI screenshot, the task and previous operations. Task: {instruction} Previous operations: 56 Technical Report {history} ... A.9.2 PROMPTS FOR ANNOTATIONS To support reproducibility and transparency, we release all annotation-related prompts used in our data processing pipeline. These prompts cover wide range of tasks, including trajectory filtering, GUI understanding, grounding supervision and chain-of-thought generation for goal-directed demonstrations. Each prompt is carefully designed to elicit accurate and semantically consistent annotations from large vision-language models such as GPT-4o and Claude-3.7. Empirically, our prompts have demonstrated strong effectiveness in producing high-quality labels, which in turn significantly benefit the training of general-purpose computer use agents. By sharing these templates, we aim to standardize annotation practices in this emerging domain and foster broader progress in building scalable and open computer use systems. We hope this contributes to lowering the barrier for future research and accelerating the development of robust, multimodal GUI agents. Prompt For Element Appearance, Layout and Functionality You are GUI analysis agent, and you are currently working with {os_name} device. You will be provided with the following resources: 1. The first image is original screenshot from an {application}. 2. The second image is marked to highlight the selected element. 3. The A11Tree attributes of the selected element: {element_a11tree}. Your task is to generate detailed descriptions of this marked element from appearance and position. Each description must uniquely identify the element and adhere to the following structure: { } \"appearance\": \"A detailed visual description of the element, including its shape, color, size, text content (if any), and any distinguishing features.\", \"position\": \"A clear description of the element's location on the screen, including its relative position to nearby elements (e.g., 'below the search bar', 'to the right of the logo'), its order in sequence (e.g., 'third button in the top navigation bar'), and its general area (e.g., 'top-left corner of the window'). Avoid using direct coordinates or the red indicator.\", ## Guidelines for Generating Descriptions: 1. **Appearance**: - Focus on visual characteristics that uniquely identify the element. - Include details such as color, shape, size, text content (if applicable), icons, borders, shadows, or patterns. - If the element contains text, describe the font style, size, and content briefly. - Please avoid using {marker} as part of your description. Because we draw {marker} for reference and they does not exist in the original screenshot. 2. **Position**: - Describe the element's location relative to other prominent elements in the UI that uniquely identify the element. - Specify its general area (e.g., 'top-right corner', 'center of the screen') and its order in group (e.g., 'second icon in the toolbar'). - Please avoid using {marker} as part of your description. Because we draw {marker} for reference and they does not exist in the original screenshot. - Avoid vague terms like 'near' or 'close to'. Instead, use precise language such as 'directly below', 'aligned with', or 'to the left of'. ## Example Output: { \"appearance\": \"A circular icon with white background and magnifying glass symbol in black, surrounded by thin gray border.\", \"position\": \"Located in the top-right corner of the application window, directly to the right of the profile avatar icon.\", } ## Important Notes: - Do not copy or paraphrase the content of the A11Tree attributes directly. 57 Technical Report - Please avoid using {marker} as part of your description. Because we draw {marker} for reference and they does not exist in the original screenshot. - Ensure each description is detailed enough to uniquely identify the element without ambiguity. RETURN THE DICTIONARY IN STRICT JSON FORMAT: Prompt For Screen Transition Captioning and User Intention Prediction You are GUI agent currently operating on {os_name} device. You will be provided with: 1. The first image is screenshot from an {application}, which are marked with {marker} to highlight the selected element. 2. The second image is the results of the operation {action} executed on the selected element. 3. The third image is sub-image, which is cropped from the screenshot around the selected element and is marked with {marker}. 4. The A11Tree attributes of the selected element: {element_a11tree}. Your task is to analyze these two consecutive screenshots and complete the following tasks: 1. **State Transition Explanation**: Describe the state change caused by the operation. This should include detailed description of the first screenshot, the action performed on the element, the differences observed in the second screenshot compared to the first, and an explanation of the most likely user action that occurred between the two frames. 2. **User Intention Inference**: Based on the action performed and the differences between the two screenshots, infer the user's intent. Explain what the user likely aimed to achieve and how the action led to the observed changes in the GUI. Your response should be formatted as follows: { \"state-transition\": \"...\", \"user-intention\": \"...\", } ## Example Output: { \"state-transition\": \"In the first screenshot, the main dashboard of the Bluecoins app is displayed with calendar showing February 2025, and the date '3' is highlighted. After tapping on the '3', the second screenshot navigates the app to detailed calendar view for February 2025, showing tabs like 'CATEGORIES,' 'ACCOUNTS,' 'TRANSACTIONS,' and 'REMINDERS,' with no transactions listed.\", \"user-intention\": \"The user likely wanted to view detailed transactions and account categories for the selected date.\", } ## Important Notes: - Avoid directly copying the A11Tree attributes of the element when writing instructions. - Ensure the instructions are clear, unambiguous, and concise, preferably described in single sentence. - Do not reference the distinctive red indicator when describing UI elements. RETURN THE DICTIONARY IN STRICT JSON FORMAT: Prompt For Interface Captioning You are GUI analysis agent currently working with {os_name} device. You will receive full screenshot of an {application}. Your objective is to produce comprehensive descriptions of the screenshot's contents and functionality. These descriptions should thoroughly explain each visible element by covering its visual attributes, spatial arrangement, and purpose within the interface. ## Key Requirements for Descriptions: - Contextual Details: Explain the interface's overall structure and the spatial relationships between elements. - Visual Characteristics: colors, shapes, icons, text labels, and other distinguishing visual properties. - User Interaction: Specify how users can interact with each element and the expected results of those interactions. - Functional Purpose: Clarify the screenshot's role within the broader application workflow. Technical Report ## Important Notes: - Synthesize the attribute information to create natural, user-friendly descriptions. - Maintain conciseness while ensuring the descriptions are sufficiently detailed to convey the GUI's structure and operation. PLEASE GENERATE CAPTION: Prompt For LLM-assist Filter You are GUI analysis agent tasked with evaluating user interface on {os_name} device. You will be provided with the following resources: 1. The first image is full screenshot of an {application}, where the area of interest is highlighted with {marker}. 2. The second image is sub-image, which is cropped from the screenshot around the selected element and is marked with {marker}. Your objective is to determine whether the marked area resides in the topmost layout and can be directly clicked. Your response must be returned in JSON format, adhering to the structure below: ```json {\"answer\": \"No\"} ``` The value of `\"answer\"` can only be one of the following: - `\"Yes\"`: Indicates that the marked area is in the topmost view and contains clickable or valid element that is the focus element of the current interface. - `\"No\"`: Indicates that the marked area is obstructed, intercepted, non-interactive, or otherwise non-clickable due to errors, loading issues, or the absence of valid interactive element, or the marked area is not the focus element of the current interface. Here are some conditions that make an area non-clickable: - The marked area resides in the background and is not the focus element of the current interface. - The image displays an error or fails to load content properly. - The marked area corresponds to an empty or blank region with no visible or interactive elements. - The marked area contains anomalies such as overlapping elements, misplaced components, or other irregularities that hinder proper interaction. - The marked area located in background and not the focus element of the current interface. RETURN THE DICTIONARY IN STRICT JSON FORMAT: Prompt For High-Level Objective in Weak-Semantic Trajectories You are an expert in designing and analyzing GUI navigation tasks. specializing in evaluating users interaction trajectory within an {application} on {os_name} device to deduce their overarching navigation goal. You will be given the following information: 1. **Initial State Image**: visual representation of the starting point of the interaction shown in the first image. 2. **Final State Image**: visual representation of the endpoint of the interaction shown in the second image. 3. **Interaction Trajectory**: detailed log of each step taken by the user, including the intent behind each action: {history} Your task is to craft concise summary (1-2 sentences) that describes the navigation journey by focusing on the goal and outcome. 1. **Identifies the users core objective**: - Emphasize the transition from the initial state to the final state (implicitly or explicitly). - Focus on the user's overall intent as inferred from the interaction history and the final state, avoiding overly detailed descriptions of operational steps (e.g., describe the task as \"updating preferences\" rather than \"toggle the switch\"). 2. **Highlights the functionality of the final state**: - Briefly describe the primary function of the final state, focusing on what the user can accomplish or access as result of completing the navigation task. For example: 59 Technical Report - The phone is displaying Amap's app info page. My goal is to access the \"My Guide\" section on Amap's homepage from here. - To view Amap's notification permission, want to move from Amap's homepage to system settings page for Amap. - Starting from Amap's battery usage settings, need to reach the \"Offline Maps\" section in the app's main interface. - With the aim of saving posts in Instagram, please advance from the home screen to \"Saved Posts\" tab from Instagram's homepage. - The screenshot shows the Chrome app info page. want to go from here to the \"History\" section in Chrome's main menu. Now, based on the provided input, assuming you are the user, please generate an instruction of the operational navigation goal by using the first-person present tense or imperative sentence: Prompt For Low-Level Instructions in All Trajectories You are GUI agent currently operating on {os_name} device. Your task is to generate concise and clear operational instruction for interacting with the selected UI element. These instructions should be relevant to the operation and include operated details such as UI appearance, text content, position, order, file names, or other relevant content visible in the screenshots. Instructions can involve the appearance, position, or functional description of the selected element, but it must ensure that the generated instruction uniquely corresponds to the selected element. You will be provided with: 1. The first image is original screenshot from an {application}, which are manually marked to highlight the selected element. 2. The second image is the results of the operations ```{action}``` executed on the selected element. If the action is 'terminate', then the second image does not exist. 3. The third image is sub-image cropped from the original screenshot, focusing on the selected element, which is highlighted with red bounding box and arrow for better visibility. 4. The A11Tree attributes of the selected element: {element_a11tree}. REMEBER: - Do NOT directly copying the A11Tree attributes of the selected elements as instructions. - Do NOT reference the distinctive red indicator when describing UI elements. Directly generate the operational instruction which can uniquely correspond to the selected element and contain all operations. Avoid \"highlighted\", \"red box\", \"red circle\" and \"red point\" in your output: Prompt For Rationales in All Trajectories You are GUI agent operating on {os_name} device. Your task is to analyze the potential reason behind operations. You will be provided with: 1. The first image is original screenshot from an {application}, which are marked to highlight the selected element. 2. The second image is the results of the operations ```{action}``` executed on the selected element. If the action is 'terminate', then the second image does not exist. 3. The third image is sub-image cropped from the original screenshot, focusing on the selected element for better visibility. 4. The A11Tree attributes of the selected element: {element_a11tree}. 5. The task objective is `{task_objective}` and history trace is `{history}`. Guidelines: - Examine the selected UI element and relevant contextual features that support task completion, considering both the objective and interaction history. {marker} higlighted in image is manually added to assist in identifying elements and **should not** been mentioned. - Provide your reasoning in three sentences, ensuring alignment with the goal and labeled action, but do not cite the actual action or bounding box as justification, as these reflect hindsight rather than predictive insight. - Restrict your analysis to details from the first image only, and avoid referencing image order. For example: 60 Technical Report The screenshot shows file dialog with active selection on format dropdown. Changing the format completes the file configuration sub-task. Next, click 'Save' to confirm the selection. Focus only on the thoughts leading up to the event, not what happens after. Do not refer to visual cues like highlights, red boxes, or circles in your description and think aloud as you work on this task: Prompt For Instruction Boost You are helpful assistant to refine the given user instructions. The refined instructions should be clear, polite, and structured as direct request or question, often including: - specific action or configuration change. - Optional context or reasoning (e.g., \"I want to ensure my browsing is private\"). - conversational yet concise tone **Some Examples for reference:** - \"Configure the system to show seconds in the taskbar clock.\" - \"Can you configure VS Code to automatically check for updates on startup?\" - \"Could you assist me in cleaning up my computer by removing any tracking data that Chrome might have stored?\" - \"I want to hear something soft and beautiful music when Windows starts up. Can you set that MP3 file like as my startup sound?\" - \"I don't want to see all these news on the home page of Microsoft Edge. Remove them in Page settings.\" **Output Format:** You should provide various styles and the output should be structured as follows: ``` Can you ...; want to ...; don't want to ...; ...; ``` **Input instruction**: {task_objective} Rewrite the provided input instructions, ensuring they are actionable, polite, and include necessary details. Use \";\" to separate different output: 61 Technical Report (a) Failure Case 1: The red box pointed to by the red arrow is originally an unclickable image element, but it is set as role=button in the HTML. (b) Failure Case 2: As indicated by the red arrow, some non-top-level elements and invisible list elements are not filtered out by the rules. (c) Success Case 1: Reduce web page dynamics. (d) Success Case 2: Correctly handle element hierarchy relationships. Figure 30: Examples of visualizations in web data acquisition. (a) shows website developer uses element identity attributes incorrectly, (b) illustrates complexity or particularity of the web leads to problems with hierarchy and visibility analysis, (c) demonstrates we alleviate the dynamic problem of web pages when playing videos, and (d) presents an example of correctly analyzing each element in page. The red box represents clickable elements, the green box represents non-clickable elements, and the blue box represents illegal elements that have been filtered out."
        }
    ],
    "affiliations": [
        "Shanghai AI Laboratory"
    ]
}