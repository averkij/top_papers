{
    "paper_title": "To Bias or Not to Bias: Detecting bias in News with bias-detector",
    "authors": [
        "Himel Ghosh",
        "Ahmed Mosharafa",
        "Georg Groh"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Media bias detection is a critical task in ensuring fair and balanced information dissemination, yet it remains challenging due to the subjectivity of bias and the scarcity of high-quality annotated data. In this work, we perform sentence-level bias classification by fine-tuning a RoBERTa-based model on the expert-annotated BABE dataset. Using McNemar's test and the 5x2 cross-validation paired t-test, we show statistically significant improvements in performance when comparing our model to a domain-adaptively pre-trained DA-RoBERTa baseline. Furthermore, attention-based analysis shows that our model avoids common pitfalls like oversensitivity to politically charged terms and instead attends more meaningfully to contextually relevant tokens. For a comprehensive examination of media bias, we present a pipeline that combines our model with an already-existing bias-type classifier. Our method exhibits good generalization and interpretability, despite being constrained by sentence-level analysis and dataset size because of a lack of larger and more advanced bias corpora. We talk about context-aware modeling, bias neutralization, and advanced bias type classification as potential future directions. Our findings contribute to building more robust, explainable, and socially responsible NLP systems for media bias detection."
        },
        {
            "title": "Start",
            "content": "To Bias or Not to Bias: Detecting bias in News with bias-detector Himel Ghosh1,2, Ahmed Mosharafa1, Georg Groh1 1Technical University of Munich (TUM), Germany, 2Sapienza University of Rome, Italy, Email: himel.ghosh@tum.de 5 2 0 2 9 1 ] . [ 1 0 1 0 3 1 . 5 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Media bias detection is critical task in ensuring fair and balanced information dissemination, yet it remains challenging due to the subjectivity of bias and the scarcity of highquality annotated data. In this work, we perform sentence-level bias classification by finetuning RoBERTa-based model on the expertannotated BABE dataset. Using McNemars test and the 52 cross-validation paired t-test, we show statistically significant improvements in performance when comparing our model to domain-adaptively pre-trained DA-RoBERTa baseline. Furthermore, attention-based analysis shows that our model avoids common pitfalls like oversensitivity to politically charged terms and instead attends more meaningfully to contextually relevant tokens. For comprehensive examination of media bias, we present pipeline that combines our model with an already-existing bias-type classifier. Our method exhibits good generalization and interpretability, despite being constrained by sentence-level analysis and dataset size because of lack of larger and more advanced bias corpora. We talk about context-aware modeling, bias neutralization, and advanced bias type classification as potential future directions. Our findings contribute to building more robust, explainable, and socially responsible NLP systems for media bias detection. Keywords: Bias Detection, Media Bias, NLP, Interpretability, Sentence Classification, transfer learning"
        },
        {
            "title": "Introduction",
            "content": "Journalism is supposed to be neutral and free from biases, but that is often not followed. Cambridge English dictionary defines bias as \"the action of supporting or opposing particular person or thing in an unfair way, because of allowing personal opinions to influence your judgment\". Intentional or unconscious, media bias is the unfair treatment of Figure 1: Bias detection and Classification scheme. particular individuals and ideas as well as the reporting of certain biased viewpoints (Morrisette et al.) which can significantly influence public opinion, skew political and democratic discourses. For free and fair journalism and news reporting, it is imperative to identify the bias and eventually mitigate them. This paper addresses the everincreasing need for bias identification models and demonstrates their statistical significance compared to the other available models . Here we introduce neural transformer-based fine-tuned bias detection model trained on Bias Annotations by Experts (BABE) dataset by (Spinde et al., 2021b) for robust bias detection available on Hugging Face. Furthermore, we present pipeline (See Fig. 1) with both bias detection and bias classification for bias analysis on sentences. These findings will offer framework for future research in the media bias detection."
        },
        {
            "title": "2 Related Work",
            "content": "At the nexus of political science, psychology, and natural language processing, media bias detection has emerged as key field of study. Historically, research in this field has used datasets like AllSides, MediaBias/FactCheck, and Media Frames Corpus to identify ideological bias at the source or article level. Lexicon-based heuristics or shallow machine learning models that used manually constructed features like sentiment polarity, assertive verbs, or hedge words were frequently used in early computational approaches (Recasens et al., 2013; Hube and Fetahu, 2018). However, due to their low inter-annotator agreement and dependence on small datasets, these approaches showed limitations in terms of scalability and generalization. For instance,(Lim et al., 2020; Färber et al., 2020) used crowdsourcing to create datasets but found low interrater reliability (e.g., Krippendorffs α = 0.0), indicating challenges in capturing the intrinsically subjective nature of bias. To address these challenges, Spinde et al. introduced the MBIC and BABE datasets two of the most influential resources in recent media bias research. MBIC included sentenceand word-level annotations with detailed annotator background metadata (Spinde et al., 2021a). BABE (Bias Annotations By Experts), built by trained annotators, significantly improved annotation quality, offering around 4000 high-quality labeled sentences across topics and biases (Spinde et al., 2021b). Regarding modeling, previous approaches used logistic regression or random forests based on manually created linguistic features (Spinde et al., 2020). With the introduction of attention in later transformers by (Vaswani et al., 2017), work adopted transformer-based models, especially BERT, RoBERTa, and XLNet, for sentence-level classification (Devlin et al., 2019; Liu et al., 2019a). These models showed improved contextual sensitivity and generalization. With noisy but scalable training data, distant supervision has become promising method for producing weak labels from partisan sources (like AllSides) (Tang et al., 2014; Spinde et al., 2021b). significant recent contribution to bias detection is made by (Krieger et al., 2022), who introduced DA-RoBERTa, domain-adapted RoBERTa model pre-trained on the Wiki Neutrality Corpus (WNC) (Pryzant et al., 2019) and fine-tuned on the BABE dataset. Although their method achieves new state-of-the-art F1 score, our approach which is developed based on their work, achieves higher score and subsequently proved consistent in statistical signifance testing. (Powers et al., 2025) recently contributed to the classification of several types of biases such as religious bias, racial bias, political bias, and others through their work on GUS framework and published bias-type-classifier model which forms an integral part of the bias-analysis pipeline of our work. Our work contributes to this gap by: 1. Adapting and fine-tuning RoBERTa-base model on BABE dataset thereby providing robust bias detector. 2. Statistical Significance Testing of our proposed model to establish its ground as significant enhancement to the previous models. 3. Integrating with bias-type classifier to reveal granular categories of the biases. Our contribution provides robust bias detector and classifier by fusing neural modeling, domainspecific fine-tuning, and adapting to existing methodology."
        },
        {
            "title": "3 Methodology",
            "content": "3.1 Model Selection and Setup Our goal is to develop robust model for sentencelevel bias classification in news articles. We build upon prior work by (Krieger et al., 2022), who introduced DA-RoBERTa, xlm-RoBERTa model (Conneau et al., 2019) pre-trained on the Wiki Neutrality Corpus (WNC) introduced by (Pryzant et al., 2019) for domain adaptation and fine-tuned on the BABE dataset introduced by (Spinde et al., 2021b). Their results demonstrated the effectiveness of domain-specific pre-training over generalpurpose models for bias detection. Inspired by their setup, we evaluate DARoBERTa as baseline and use it as reference point for our model selection and evaluation. We tested their DA-Roberta model with 5-fold cross validation on the entire BABE Dataset yielding the best F1 score of 0.8364. To enhance this, we create another model by taking the transformer (Vaswani et al., 2017) model: Roberta base (Liu et al., 2019b) and fine-tuned it directly on the BABE dataset. Then evaluated it similarly and achieved the best macro F1 score as 0.9257. 3.2 Learning Task and Objective The sentence-level bias classification task is framed as binary supervised classification problem. Given sentence xi from the corpus, the objective is to assign label yi {0,1} where: Dataset: BABE (Bias Annotations By Experts), consisting of around 4120 sentences annotated by expert annotators. Evaluation Metric: Macro F1 score, reported with standard error over splits. Cross-Validation: Using stratified 5-fold cross-validation, preserving class balance in each fold. For every model, the mean F1 score across folds is reported. This setup allows us to directly compare our fine-tuned model (himel7/bias-detector) with DARoBERTa under the same conditions, ensuring that any observed enhancements are attributable to our strategy rather than evaluation artifacts which is further established by the following: Statistical Significance Testing: McNemars Test: To measure the statistical significance of the differences, following (Dror et al., 2018), we conducted McNemars Test. According to the tests null hypothesis, both algorithms have the same marginal probability for each outcome (label zero or label one). 5x2 CV Test:To further establish the evidence, we performed the more robust 5x2 CV paired t-test according to (Dietterich, 1998). Our code is available on this Github Repository and the model is available on this link."
        },
        {
            "title": "4 Experiments",
            "content": "We evaluate the DA-RoBERTa model by (Krieger et al., 2022) using Hugging Faces implementation and load their model from the official release. Our model: bias-detector is fine-tuned using the Hugging Face Transformers library on the full BABE dataset. We used the AdamW optimizer with learning rate of 2 105, batch size 32, and early stopping based on validation F1. All experiments were run on RTX A6000 GPU. Fine tuning took around 30 minutes. The best model checkpoint was then published on Hugging face library. Both models were put through the above-mentioned evaluation strategy to achieve the macro F1 scores and statistical scores. Similar to (Krieger et al., 2022), we do not know the underlying distribution of our target metric and (Dror et al., 2018) suggests us to choose McNemars test. It is non-parametric statistical test Figure 2: Architecture of the Bias Detector Model. The output vector of the CLS token serves as input to the classifier head yi = 1 denotes biased sentence yi = 0 denotes an unbiased sentence The binary cross-entropy loss is optimised over the training set = {(xi, yi)}N i=1: (cid:88) [yi log ˆyi + (1 yi) log(1 ˆyi)] = 1 i= (1) where ˆyi is the models predicted probability for the sentence being biased. softmax classifier is used on top of the [CLS] token embedding from RoBERTa. See Fig. 2 for the architecture of the model where hCLS is the output vector of the [CLS] token from the pre-trained model and Wc is the learned set of weights after fine-tuning, hence, = sof tmax(hCLSWc) as per (Jurafsky and Martin, 2025). To add to the bias classification in the pipeline along with our detector, we adopted the model by (Powers et al., 2025) available on Hugging face library as bias-type-classifier. Hence full pipeline is created, which takes in sentences on one side and gives out bias analysis telling whether the sentence is biased or not and if it is biased, what kind of bias is it. 3.3 Evaluation To ensure fair and replicable comparison with DARoBERTa, we adopted the same evaluation protocol as (Krieger et al., 2022), namely: 3 used to compare the performance of two classifiers on the same data instances, focusing specifically on paired nominal data i.e., whether each model was correct or incorrect on each example. It is based on 2x2 contingemncy table highlighting the model prediction on ndata points of the test set. The following are the hypotheses, respectively: McNemars Test Null Hypothesis (H0): (Model correct, Model incorrect) = (Model correct, Model incorrect) Alternative Hypothesis (H1): (Model correct, Model incorrect)= (Model correct, Model incorrect) The test statistic follows χ2 distribution with 1 degree of freedom. If χ2 is large and p-value<0.05, we reject H0 and conclude that the models performances are significantly different. To assess whether the difference in performance between our model and the baseline is statistically significant, we employed the 5x2 cross-validation paired t-test (Dietterich, 1998). This test runs two models over five independent 2-fold crossvalidation splits and computes t-statistic over the difference in performance (e.g., F1-score) across these trials. The test statistic is based on 10 independent estimates of the performance difference. The null hypothesis assumes that both models have equal expected performance. 5x2 cv test Null Hypothesis (H0): E[θi] = 0 Alternative Hypothesis (H1): E[θi] = 0 (2) (3) Where θi is the difference in performance (e.g., accuracy or F1-score) between the two models on the ith fold. statistically significant result (p < 0.05) indicates that the observed difference is unlikely to be due to random chance."
        },
        {
            "title": "5 Results",
            "content": "Our model outperforms the DA-RoBERTa baseline in terms of macro F1 score across all 5 folds. The Fig. 3 shows the F1 score performance of our biasdetector over DA-RoBERTa in each of the 5 folds of BABE dataset. Adopting their research and adding to the Table 1 of (Krieger et al., 2022), we present the updated Table 1 here with our models performance on macro F1 and standard error in parentheses. Figure 3: 5-Fold cross validation results on both models Model BERT RoBERTa DA-RoBERTa Bias-Detector Macro F1 (error) 0.789 (0.011) 0.799 (0.011) 0.8363 (0.0046) 0.9257 (0.0035) Table 1: Stratified 5-fold CV Results Furthermore, we conducted McNemars test on each fold of the 5-fold cross-validation setup to assess the significance of the performance difference between our model (bias-detector) and the DA-RoBERTa baseline. Across all five folds, the test consistently yielded high Chi-squared statistics (mean: 43.28) and extremely low p-values (mean: 2.45x109), indicating statistically significant improvement in classification performance by our model. Table 2 shows the McNemar Test results comparing our model with the baseline. Table 2: McNemars Test Results per Fold comparing himel7/bias-detector and DA-RoBERTa on the BABE dataset. All p-values indicate statistically significant differences at α = 0.05. Fold Chi-squared (χ2) 1 2 3 4 5 Mean 50.52 40.81 32.49 41.42 51.18 43.28 p-value 1.18 1012 1.68 1010 1.20 108 1.23 1010 8.44 1013 2.45 10 Based on (Dietterich, 1998) who says 5x2 CV t-test is slightly more powerful than McNemars test, we conducted the 52 cross-validation paired t-test to assess whether the observed improvement in our model over the DA-RoBERTa baseline is statistically significant. The test yielded t-statistic of 29.81 with p-value of 7.54 106 confirm4 Figure 4: Attention weights heatmap for False Negatives Figure 5: Attention weights heatmap for False Positives ing that the performance difference is statistically significant. These results confirm that the observed gains are not due to random variation but reflect consistent enhancement in bias detection accuracy with high statistical significance. 5.1 Interpretation While DA-Roberta provides many false positives (unbiased sentence flagged as biased) and false negatives (biased sentence flagged as unbiased), our bias-detector proved to be more robust. Check Fig. 4 and 5 to see how Bias-Detector attends to the tokens compared to the DA-RoBERTa model. As you see, in Fig. 4, the baseline DA-RoBERTa does not attend to the tokens contributing to bias. It is an example of framing bias missed by DA-RoBERTa. The sentence uses strong evaluative language (e.g., success, greatest achievements) to frame political action positively. DARoBERTa fails to detect this bias possibly because this domain-adapted model may be biased toward formal neutrality, especially if WNC was too Wikipedia-like and underexposed to superlative media framing; while our model correctly predicts \"biased\" and focuses attention on key framing tokens because of better linguistic generalization. In other cases, like Fig. 5, illustrates DARoBERTa incorrectly classifies neutral sentence as biased due to over-attention to politically charged tokens like Democratic socialism. This reflects domain-adaptive overfitting the model may have learned to associate certain political keywords with bias regardless of context. In contrast, our model correctly classifies the sentence as unbiased and attends more meaningfully to contextual phrases like has embraced. This suggests that our bias-detector generalizes better by avoiding lexical bias triggers and relying on contextual framing. 5.2 Discussion The DA-RoBERTa model took two-step approach: first training on Wiki-Neutrality Corpus (WNC, Wikipedia-style content) using multilingual model base (xlm-roberta-base) for domainadaptation, then fine-tuning for the task. In contrast, our model skipped that first step and went straight to fine-tuning standard English RoBERTa model (roberta-base) on the BABE dataset which is focused on the news-domain. Our approach worked better for few key reasons. First, our model used smaller, English-focused foundation rather than larger multilingual one, which meant it was already well-suited for English text processing. Hence it generalized well to the linguistic subtleties of English language. Second, the vocabulary and how words are broken down into tokens were better matched to the task at hand. Finally, by training directly on news articles (the BABE dataset) rather than first adapting to Wikipedia-style content, our model avoided the potential confusion that comes from the domain-shift, that is, switching between different types of text. All of these factors likely helped our model become better at spotting the subtle language patterns that indicate bias at the sentence level - exactly what we needed it to do. Hence, our work provides more robust state-ofthe-art bias-detection model available on Hugging Face library which can be widely used for bias identification and news bias analysis by media bias researchers, NLP scientists and the likes. We also laid down the framework for further bias analysis by integrating an existing bias type classifier model in the same pipeline of our bias detection, thereby 5 opening up new avenues of research work in this zone."
        },
        {
            "title": "6 Limitations and Future Works",
            "content": "Although our bias detection model demonstrates better performance over existing baselines, it is inherently constrained by the limitations of the dataset it was trained on. Our work is restricted by the limitations of The BABE dataset which comprises approximately of 4,000 English-language sentences sourced from several media outlets, which limits both the linguistic and topical diversity of training examples. Language is ever growing and definition of bias is also subjective. The question of who defines what as biased and whether something would be considered as bias in specific socio-cultural context or not, is still somewhat perspective dependent. Hence our model is also bounded by the language and data limitations besides being sentence level model which only classifies within 2 labels with limited interpretability. This limits the models ability to generalize to paragraph-level or document-level bias and may restrict applicability to more complex real-world media scenarios. As part of the future scope, addressing these limitations shall be the core focus. Trying to increase the interpretability by extending the action on each of token paragraph or document level can be way forward. Also the need for larger, more diverse corpus beyond BABE will spearhead the development. Evaluating and adapting the model to other media domains (e.g., blogs, social media) and non-English languages could provide insights into how bias manifests across cultures and platforms. Future work could explore richer interpretability techniques such as SHAP, integrated gradients, or counterfactual examples. These can be integrated into interactive tools for journalists, fact-checkers, or researchers to manually inspect or override model predictions. Beyond identification and classification, generative approaches could be used to neutralize biased content while preserving factual meaning. Combining classification with generation (e.g., using T5 or LLMs) could enable controllable text rewriting for media editing or curation purposes. We believe these directions will advance the development of more robust, explainable, and socially responsible NLP systems for bias detection in media."
        },
        {
            "title": "7 Conclusion",
            "content": "In this work, we presented fine-tuned RoBERTabased model called bias-detector for sentencelevel media bias detection, trained on the expertannotated BABE dataset. Through comprehensive evaluation and statistical testing, our model outperformed the domain-adapted DA-RoBERTa baseline across multiple folds, achieving higher macro F1scores and demonstrating statistically significant enhancements. Attention-based analyses further revealed that our model attends more meaningfully to linguistic cues associated with bias. Our findings suggest that careful fine-tuning of the rightly chosen base model on task-specific data, even without domain-adaptive pretraining, can yield robust and interpretable models for media bias detection. We incorporated an already available bias-type-classifier in our pipeline to show framework for complete bias analysis. We also highlight the importance of aligning model design with linguistic and contextual nuances inherent in bias perception. While our approach is constrained by dataset size, language scope, and only sentence level analysis, it opens the door to several new vistas. These include fine-grained bias type classification, discourse-level modeling, multilingual adaptation, and bias-neutralization through natural language generation. We hope our work contributes to the development of more transparent, context-aware, and socially responsible NLP systems for media analysis."
        },
        {
            "title": "8 Acknowledgements",
            "content": "I acknowledge the support and guidance of my Master-degree thesis supervisor Ahmed Mosharafa and NLP Professor Georg Groh, because of whom, this work was possible."
        },
        {
            "title": "References",
            "content": "Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzmán, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Unsupervised cross-lingual representation learning at scale. CoRR, abs/1911.02116. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for 6 Maximus Powers, Shaina Raza, Alex Chang, Umang Mavani, Harshitha Reddy Jonala, Ansh Tiwari, and Hua Wei. 2025. The gus framework: Benchmarking social bias classification with discriminative (encoderonly) and generative (decoder-only) language models. Preprint, arXiv:2410.08388. Reid Pryzant, Richard Diehl Martinez, Nathan Dass, Sadao Kurohashi, Dan Jurafsky, and Diyi Yang. 2019. Automatically neutralizing subjective bias in text. Preprint, arXiv:1911.09709. Marta Recasens, Cristian Danescu-Niculescu-Mizil, and Dan Jurafsky. 2013. Linguistic models for analyzing and detecting biased language. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 16501659, Sofia, Bulgaria. Association for Computational Linguistics. T. Spinde, L. Rudnitckaia, K. Sinha, F. Hamborg, B. Gipp, and K. Donnay. 2021a. Mbic media bias annotation dataset including annotator characteristics. Preprint, arXiv:2105.11910. Timo Spinde, Felix Hamborg, Karsten Donnay, Angelica Becerra, and Bela Gipp. 2020. Enabling news consumers to view and understand biased news coverage: study on the perception and visualization of media bias. In Proceedings of the ACM/IEEE Joint Conference on Digital Libraries in 2020, JCDL 20, page 389392, New York, NY, USA. Association for Computing Machinery. Timo Spinde, Manuel Plank, Jan-David Krieger, Terry Ruas, Bela Gipp, and Akiko Aizawa. 2021b. Neural media bias detection using distant supervision with babe - bias annotations by experts. In Findings of the Association for Computational Linguistics: EMNLP 2021, page 11661177. Association for Computational Linguistics. Duyu Tang, Furu Wei, Nan Yang, Ming Zhou, Ting Liu, and Bing Qin. 2014. Learning sentiment-specific word embedding for Twitter sentiment classification. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 15551565, Baltimore, Maryland. Association for Computational Linguistics. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Ł ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc. Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 41714186, Minneapolis, Minnesota. Association for Computational Linguistics. Thomas G. Dietterich. 1998. Approximate statistical tests for comparing supervised classification learning algorithms. Neural Computation, 10(7):18951923. Rotem Dror, Gili Baumer, Segev Shlomov, and Roi Reichart. 2018. The hitchhikers guide to testing statistical significance in natural language processing. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 13831392, Melbourne, Australia. Association for Computational Linguistics. Michael Färber, Victoria Burkard, Adam Jatowt, and Sora Lim. 2020. multidimensional dataset based on crowdsourcing for analyzing and detecting news bias. In Proceedings of the 29th ACM International Conference on Information & Knowledge Management, CIKM 20, page 30073014, New York, NY, USA. Association for Computing Machinery. Christoph Hube and Besnik Fetahu. 2018. DetectIn Companing biased statements in wikipedia. ion Proceedings of the The Web Conference 2018, WWW 18, page 17791786, Republic and Canton of Geneva, CHE. International World Wide Web Conferences Steering Committee. Daniel Jurafsky and James H. Martin. 2025. Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition with Language Models, 3rd edition. Stanford University. Online manuscript released January 12, 2025. Jan-David Krieger, Timo Spinde, Terry Ruas, Juhi Kulshrestha, and Bela Gipp. 2022. domain-adaptive pre-training approach for language bias detection in news. In Proceedings of the 22nd ACM/IEEE Joint Conference on Digital Libraries, JCDL 22. ACM. Sora Lim, Adam Jatowt, Michael Färber, and Masatoshi Yoshikawa. 2020. Annotating and analyzing biased sentences in news articles using crowdsourcing. In Proceedings of the Twelfth Language Resources and Evaluation Conference, pages 14781484, Marseille, France. European Language Resources Association. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019a. Roberta: robustly optimized bert pretraining approach. Preprint, arXiv:1907.11692. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019b. Roberta: robustly optimized BERT pretraining approach. CoRR, abs/1907.11692. Elizabeth Morrisette, Grace McKeon, Alison Louie, Amy Luther, and Alexis Fagen. Introduction to media studies."
        }
    ],
    "affiliations": [
        "Sapienza University of Rome, Italy",
        "Technical University of Munich (TUM), Germany"
    ]
}