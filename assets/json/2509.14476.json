{
    "paper_title": "AToken: A Unified Tokenizer for Vision",
    "authors": [
        "Jiasen Lu",
        "Liangchen Song",
        "Mingze Xu",
        "Byeongjoo Ahn",
        "Yanjun Wang",
        "Chen Chen",
        "Afshin Dehghan",
        "Yinfei Yang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We present AToken, the first unified visual tokenizer that achieves both high-fidelity reconstruction and semantic understanding across images, videos, and 3D assets. Unlike existing tokenizers that specialize in either reconstruction or understanding for single modalities, AToken encodes these diverse visual inputs into a shared 4D latent space, unifying both tasks and modalities in a single framework. Specifically, we introduce a pure transformer architecture with 4D rotary position embeddings to process visual inputs of arbitrary resolutions and temporal durations. To ensure stable training, we introduce an adversarial-free training objective that combines perceptual and Gram matrix losses, achieving state-of-the-art reconstruction quality. By employing a progressive training curriculum, AToken gradually expands from single images, videos, and 3D, and supports both continuous and discrete latent tokens. AToken achieves 0.21 rFID with 82.2% ImageNet accuracy for images, 3.01 rFVD with 32.6% MSRVTT retrieval for videos, and 28.19 PSNR with 90.9% classification accuracy for 3D. In downstream applications, AToken enables both visual generation tasks (e.g., image generation with continuous and discrete tokens, text-to-video generation, image-to-3D synthesis) and understanding tasks (e.g., multimodal LLMs), achieving competitive performance across all benchmarks. These results shed light on the next-generation multimodal AI systems built upon unified visual tokenization."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 1 ] . [ 1 6 7 4 4 1 . 9 0 5 2 : r a"
        },
        {
            "title": "Preprint",
            "content": "ATOKEN: UNIFIED TOKENIZER FOR VISION Jiasen Lu Liangchen Song Mingze Xu Byeongjoo Ahn"
        },
        {
            "title": "ABSTRACT",
            "content": "We present ATOKEN, the first unified visual tokenizer that achieves both highfidelity reconstruction and semantic understanding across images, videos, and 3D assets. Unlike existing tokenizers that specialize in either reconstruction or understanding for single modalities, ATOKEN encodes these diverse visual inputs into shared 4D latent space, unifying both tasks and modalities in single framework. Specifically, we introduce pure transformer architecture with 4D rotary position embeddings to process visual inputs of arbitrary resolutions and temporal durations. To ensure stable training, we introduce an adversarial-free training objective that combines perceptual and Gram matrix losses, achieving state-of-the-art reconstruction quality. By employing progressive training curriculum, ATOKEN gradually expands from single images, videos, and 3D, and supports both continuous and discrete latent tokens. ATOKEN achieves 0.21 rFID with 82.2% ImageNet accuracy for images, 3.01 rFVD with 32.6% MSRVTT retrieval for videos, and 28.19 PSNR with 90.9% classification accuracy for 3D. In downstream applications, ATOKEN enables both visual generation tasks (e.g., image generation with continuous and discrete tokens, text-to-video generation, image-to-3D synthesis) and understanding tasks (e.g., multimodal LLMs), achieving competitive performance across all benchmarks. These results shed light on the next-generation multimodal AI systems built upon unified visual tokenization."
        },
        {
            "title": "INTRODUCTION",
            "content": "Large Language Models (LLMs) (Chowdhery et al., 2023; Achiam et al., 2023; Touvron et al., 2023; Team et al., 2023; Guo et al., 2025) have achieved unprecedented generalization, with single models handling coding, reasoning, translation, and numerous other tasks that previously required specialized systems. This versatility largely stems from transformer architectures and simple tokenizers, such as BPE (Sennrich et al., 2015), which convert all text types code, documents, tables, and multiple languages into unified token space. This shared representation enables efficient scaling and seamless knowledge transfer across language tasks. In contrast, visual representations remain fragmented due to inherent complexities. Unlike texts discrete symbolic nature, visual tasks demand distinct levels of abstraction: generation requires tokenizers that preserve low-level visual details for reconstruction, while understanding requires encoders that extract high-level semantic features through text alignment. Moreover, visual data exists in disparate formats: 2D grids for images, temporal sequences for videos, and varied 3D representations (e.g., meshes, voxels, and Gaussian splats) (Mescheder et al., 2019; Achlioptas et al., 2018; Mildenhall et al., 2021; Kerbl et al., 2023). Without shared representation, vision systems remain fundamentally limited, unable to achieve the generalization and transfer learning that characterizes modern language models. Despite recent progress, unified visual tokenizers face three fundamental challenges. First, existing approaches optimize for either reconstruction or understanding, but not both: visual encoders (Radford et al., 2021; Zhai et al., 2023; Bolya et al., 2025) achieve semantic alignment but lack *Leading authors, equal contribution. Description of each authors contribution is available in Appendix A. Corresponding to Jiasen Lu."
        },
        {
            "title": "Preprint",
            "content": "Figure 1: Illustration of our method on different visual modalities. Given images, videos, and 3D assets, ATOKEN leverages shared 4D latent space (left) to produce high-fidelity reconstructions (middle: zoomed regions with red boxes for images, temporal frames for videos, multiple viewpoints for 3D) while preserving strong semantic understanding (right: showing text-aligned representations for zero-shot text retrieval). pixel-level detail, while VAE-based tokenizers (Esser et al., 2020; Rombach et al., 2022; Polyak et al., 2024; Yu et al., 2022b) preserve visual details but lack semantic understanding. Second, architectural choices create different limitations: convolutional tokenizers exhibit diminishing returns when scaling model parameters (Xiong et al., 2025), while transformer tokenizers (Yu et al., 2021; Wang et al., 2024b; Hansen-Estruch et al., 2025) achieve better scaling but suffer from severe adversarial training instabilities. Third, recent unification efforts remain limited to images (Deng et al., 2025; Wu et al., 2024c; Ma et al., 2025a), while video and 3D modalities remain unexplored. In this paper, we present ATOKEN, general-purpose visual tokenizer that achieves high-fidelity reconstruction and rich semantic understanding across images, videos, and 3D. Our model learns unified representation that captures both fine-grained visual details and high-level semantics, accessible through progressive encoding: semantic embeddings for understanding, low-dimensional continuous latents for generation, and discrete tokens via quantization. This design enables the next generation of multimodal systems that seamlessly handle both understanding and generation across all visual modalities, as shown in Figure 1. To address format discrepancies across visual modalities, we introduce sparse 4D representation where each modality naturally occupies different subspaces: images as 2D slices, videos as temporal stacks, and 3D assets as surface voxels extracted from multi-view renderings (Xiang et al., 2024). We implement this through pure transformer architecture with space-time patch embeddings and 4D Rotary Position Embeddings (RoPE), enabling efficient scaling and joint modeling across all modalities while maintaining native resolution and temporal length processing. To overcome training instabilities that affect transformer-based visual tokenizers, we develop an adversarial-free loss combining perceptual and Gram matrix terms. This approach achieves stateof-the-art reconstruction quality while maintaining stable, scalable training. We further introduce progressive curriculum that builds capabilities incrementally: starting from pretrained vision encoder, jointly optimizing reconstruction and understanding for images, extending to videos and 3D data, with optional quantization for discrete tokens. Surprisingly, this curriculum reveals that multimodal training can enhance rather than compromise single-modality performance our final model achieves better image reconstruction than earlier image-only stages while maintaining strong semantic understanding. ATOKEN demonstrates significant advances in both scalability and performance. The model natively processes arbitrary resolutions and time duration, and accelerates inference through KV-caching mechanisms. To validate its effectiveness, we conduct comprehensive evaluations across three dimensions: reconstruction quality, semantic understanding, and downstream applications. These experiments confirm that ATOKEN achieves competitive or state-of-the-art performance across all modalities while maintaining computational efficiency. The key contributions of ATOKEN can be summarized as follows: First unified visual tokenizer across modalities and tasks: We present the first tokenizer that achieves high-fidelity reconstruction and semantic understanding for images, videos, and 3D assets, supporting both continuous and discrete representations within single framework."
        },
        {
            "title": "Preprint",
            "content": "Sparse 4D representation with pure transformer architecture: We introduce unified 4D latent space where different modalities naturally occupy respective subspaces, implemented through space-time patch embeddings and 4D RoPE that enable native resolution and temporal processing. Adversarial-free training for stable optimization: We demonstrate that combining perceptual and Gram matrix losses achieves state-of-the-art reconstruction quality without adversarial training, overcoming instabilities that challenge transformer-based visual tokenizers. Progressive curriculum across modalities: Our four-stage training strategy enables stable learning while maintaining strong performance, with image reconstruction quality preserved or improved when video and 3D capabilities are added alongside semantic understanding. Strong empirical validation across downstream applications: ATOKEN achieves competitive performance across all modalities and enables diverse applications from multimodal LLMs to image-to-3D generation, validating its effectiveness as universal visual foundation."
        },
        {
            "title": "2 BACKGROUND",
            "content": "Visual tokenization transforms raw visual data into compact representations suitable for both understanding and generation tasks. However, existing approaches remain fragmented across modalities and task objectives, unable to achieve the versatility seen in language models. Table 1 summarizes the landscape of visual tokenizers across three key dimensions: task specialization, modality fragmentation, and architectural trade-offs. comprehensive review of related work is in Section 6. Task Specialization. Current visual tokenizers fall into two distinct categories based on their optimization objectives. Reconstruction methods like SD-VAE (Rombach et al., 2022), VQGAN (Esser et al., 2020), GigaTok (Xiong et al., 2025), and Cosmos (Agarwal et al., 2025) excel at compressing visual data for generation tasks but cannot extract semantic features for understanding. Conversely, understanding-centric visual encoders such as CLIP (Radford et al., 2021), SigLIP2 (Tschannen et al., 2025), and VideoPrism (Zhao et al., 2024) produce rich semantic representations but cannot reconstruct the original visual content. Only recent works VILA-U (Wu et al., 2024c) and UniTok (Ma et al., 2025a) attempt both tasks simultaneously, though they remain limited to images. This divide prevents building visual models that excel at both generation and understanding. Modality Fragmentation. Beyond task specialization, visual tokenizers are limited to specific modalities. While most video tokenizers naturally handle images as single-frame videos (e.g., TAE (Polyak et al., 2024), Hunyuan (Kong et al., 2024), OmniTokenizer (Wang et al., 2024b)), they cannot process 3D data. Conversely, 3D tokenizers like Trellis-SLAT (Xiang et al., 2024) are restricted to 3D-only data, unable to leverage the massive image and video data for pretraining. Understanding tasks face similar constraints: image encoders process videos frame-by-frame without temporal compression, while dedicated video encoders (Zhao et al., 2024; Wang et al., 2022b) lack image-specific optimizations. No existing method provides comprehensive coverage across all three modalities for both reconstruction and understanding tasks. Architectural Trade-offs. Key design trade-offs emerge across methods: (1) Architecture: Understanding encoders use transformers, while reconstruction tokenizers favor convolutional architectures (e.g., SD-VAE (Rombach et al., 2022)). Recent works explore hybrid (e.g., GigaTok (Xiong et al., 2025)) and pure transformer approaches (e.g., ViTok (Hansen-Estruch et al., 2025)), though the latter suffer from adversarial training instabilities. (2) Token representation: Methods choose between discrete tokens for LLM compatibility (e.g., VQGAN (Esser et al., 2020)) or continuous tokens for reconstruction quality (e.g., TAE (Polyak et al., 2024)), with few supporting both. (3) Resolution handling: Convolutional architectures naturally handle arbitrary resolutions, while among transformer-based approaches, only SigLIP2 (Tschannen et al., 2025) supports native resolution processing. (4) Training objectives: GAN-based training dominates reconstruction tokenizers for quality despite instabilities. Trellis-SLAT (Xiang et al., 2024) avoids adversarial training as 3D assets lack the fine detail of real images and videos. These limitations motivate ATOKEN, which unifies reconstruction and understanding across images, videos, and 3D within single transformer framework. As shown in Table 1, ATOKEN is the only method providing full coverage both tasks, all modalities, both token types while achieving training stability through adversarial-free optimization."
        },
        {
            "title": "Preprint",
            "content": "Table 1: Comparison between existing visual tokenizers and AToken. We categorize methods by task capabilities (reconstruction, understanding, or both) and evaluate their modality coverage, architectural choices, token representations, and key features. ATOKEN is the only method providing support across all dimensions. Method Encoder Arch. Decoder Arch. Reconstruction Only SD-VAE VQGAN GigaTok OmniTokenizer MAGVIT-v2 Cosmos ViTok TAE Hunyuan Wan Trellis-SLAT Conv Conv Conv Conv Hybrid Hybrid Trans Trans Conv Conv Conv Conv Trans Trans Conv Conv Conv Conv Conv Conv Trans Trans Understanding Only SigLIP2 PE VideoPrism InternVideo Trans Trans Trans Trans - - - - Reconstruction & Understanding VILA-U UniTok ATOKEN Trans Trans Trans Conv Hybrid Trans"
        },
        {
            "title": "3 MODEL",
            "content": "Reconstruction Understanding Image Video 3D Image Video 3D Discrete Token Cont. Token GAN Free Temporal Comp. Native Res. - - - - - - - - - - - - - - - - - - - - This section presents ATOKENs architecture and training methodology. We first present our unified 4D representation that bridges all visual modalities (Section 3.1) and the pure transformer architecture that processes these representations (Section 3.2). We then describe our adversarial-free training objectives for stable optimization (Section 3.3) and our progressive curriculum that enables effective multimodal learning (Section 3.4), followed by implementation details (Section 3.5)."
        },
        {
            "title": "3.1 UNIFIED LATENT REPRESENTATION",
            "content": "Unified Modalities Image, Video and 3D. Our central insight is that all visual modalities can be represented within shared 4D space. As illustrated in Figure 2, we process each modality through space-time patchification to produce sets of feature-coordinate pairs: = {(zi, pi)}L i=1, zi RC, pi {0, 1, . . . , 1}4 (1) where zi represents the latent feature at position pi = [t, x, y, z] in 4D space (temporal and spatial coordinates), with defining the resolution along each axis and the number of active locations. This sparse representation unifies all modalities by activating only their relevant dimensions: images occupy the (x, y) plane at = = 0, videos extend along the temporal axis with = 0, and 3D assets as surface voxels in (x, y, z) space with = 0. For 3D assets, we adapt Trellis-SLAT (Xiang et al., 2024) by rendering multi-view images from spherically sampled cameras, applying our unified patchification, then aggregating features into voxel space (detailed in Section 3.2). This approach enables single encoder to process all modalities without architectural modifications. Note that the (x, y, z) coordinates serve different purposes across modalities: in 3D, they represent actual entity occupancy physical locations, while in images and videos, they function as grid indices. We can conceptualize this as placing monitor within 4D space and encoding its displayed content for image and video data. This dual interpretation of coordinates does not compromise generalization, thanks to the use of 4D RoPE, which we describe in detail in following sections. Unified Tasks Reconstruction and Understanding. From the unified structured latents = {(zi, pi)}, we extract representations for both reconstruction and understanding through complementary projections. For reconstruction, we project each latent to lower-dimensional space zr = Wr(z) with KL regularization (Rombach et al., 2022), optionally applying FSQ (Mentzer et al., 2023) for discrete codes zr = FSQ(zr). The decoder Dθ then reconstructs the input from"
        },
        {
            "title": "Preprint",
            "content": "Figure 2: Overview of our method. All modalities undergo unified space-time patchification and encoding into sparse 4D latents, which support both reconstruction through modality-specific decoders and understanding through attention pooling and text alignment. The architecture jointly optimizes reconstruction and understanding losses, maintaining sparse structured representations throughout for efficient multimodal processing. these latents. For understanding, we aggregate latents via attention pooling (Radford et al., 2021; Tschannen et al., 2025) into global representation z, which is projected to zs = Ws( z) for alignment with text embeddings. This dual projection design allows joint optimization without architectural duplication the same encoded features support both pixel-level reconstruction through individual latents and semantic understanding through their aggregation."
        },
        {
            "title": "3.2 TRANSFORMER BASED ARCHITECTURE",
            "content": "Unified Space-Time Patch Embedding. We employ unified patchification scheme that enables all modalities to share the same encoder. Given an input RT HW 3, we partition it into non-overlapping space-time patches of size p. For images (T = 1), we apply temporal zero-padding to create t-frame patches, ensuring consistent dimensions across modalities. Videos are directly partitioned along both spatial and temporal dimensions. For 3D assets, we adapt Trellis-SLAT (Xiang et al., 2024) to our unified pipeline. As shown in Figure 3, we render multi-view images from spherically sampled cameras and apply our standard space-time patchification. Each voxel in 643 grid is back-projected to gather and average patch features from relevant views. Unlike Xiang et al. (2024), which uses DINOv2 features, we achieve comparable quality using our unified patch representation. All patch features whether from images, videos, or aggregated 3D views are then flattened and passed through shared linear layer to produce the initial embeddings for the transformer encoder. Sparse Transformer Encoder and Decoder. We employ unified transformer architecture for both encoder and decoder, as illustrated in Figure 2. Both components process sparse structured representations sets of feature-position pairs rather than dense grids enabling efficient handling of all modalities with native support for arbitrary resolutions and temporal lengths. Our encoder extends the pretrained SigLIP2 vision tower (Tschannen et al., 2025) from 2D images to 4D representations through two modifications. First, we generalize patch embedding to spacetime blocks of size p, with zero-initialized temporal weights preserving the original image features. Second, we augment SigLIP2s learnable 2D position embeddings with 4D RoPE (Lu et al., 2024a) applied in every attention layer, providing relative position awareness across (t, x, y, z) dimensions. This design maintains SigLIP2s semantic priors and resolution flexibility while enabling unified processing across modalities. The decoder shares the encoders transformer architecture but is trained from scratch for reconstruction. It maps structured latents back to visual outputs through task-specific heads. For images and videos, we decode directly to pixel space: DP : {(zi, pi)}L i=1 RT HW 3 (2) treating images as single-frame videos (T = 1) and discarding temporal padding following (Polyak et al., 2024). For 3D assets, we first decode to pixel-space features, then apply an additional layer to"
        },
        {
            "title": "Preprint",
            "content": "Figure 3: 3D tokenization pipeline. We extend Trellis-SLAT (Xiang et al., 2024) for multimodal unification through two modifications: directly tokenizing raw RGB patches from multiview renderings (as opposed to using DINOv2 features), and aggregating each voxels features from its nearest viewpoint (as opposed to averaging across all views). Combined with Gaussian decoding, this approach integrates 3D assets into our unified token space alongside images and videos. generate Gaussian splatting parameters for efficient rendering: DGS : {(zi, pi)}L i=1 {{(ok , ck , sk , αk , rk )}K k=1}L i=1 (3) where each location generates Gaussians with parameters: position offset o, color c, scale s, opacity α, and rotation r. Following Xiang et al. (2024), we constrain Gaussian positions to remain near their source voxels using xk ), ensuring local feature coherence. = pi + tanh(ok"
        },
        {
            "title": "3.3 TRAINING OBJECTIVES",
            "content": "We jointly optimize for reconstruction fidelity and semantic understanding through an adversarialfree training objective: = λrecLrec + λsemLsem + λKLLKL, (4) where LKL is the KL regularization term applied to the projected reconstruction latents zr, with λrec, λsem and λKL balancing components. Notably, we achieve state-of-the-art reconstruction quality without adversarial training, which has been observed to be unstable when scaling (Wu et al., 2025a) and incompatible with our sparse 3D representations. Reconstruction Loss. While GANs (Goodfellow et al., 2014) are standard for visual tokenizers, we found them unsuitable for our transformer architecture. Figure 4(a) shows the discriminator rapidly dominates the generator, causing mode collapse and degraded reconstruction quality. To develop an alternative, we analyzed the reconstruction error by decomposing rFID into mean and covariance components (Figure 4(b)). The covariance component capturing second-order statistics like texture and style dominates at 86.6%, while mean features contribute only 13.4%. This insight motivated adopting Gram matrix loss (Gatys et al., 2016), which directly optimizes feature covariance without adversarial training: LGram(x, ˆx) = (cid:88) G(Φl(x)) G(Φl( ˆx))2 , (5) where G(F ) = is the Gram matrix for feature map from layer of network Φ. As shown in Figure 4(c), this achieves superior and stable reconstruction throughout training. For images, we combine four complementary loss components: LI rec = λ1L1 + λLPIPSLLPIPS + λGRAMLGRAM + λCLIPLCLIP, (6) where L1 = ˆx1 provides pixel supervision, LLPIPS (Zhang et al., 2018) measures perceptual similarity, LGRAM captures texture, and LCLIP enforces semantic consistency. For video and 3D assets, we use LV/3D rec = L1 for efficiency, relying on cross-modal transfer from images for details."
        },
        {
            "title": "Preprint",
            "content": "(a) GAN training instability (b) Decomposition of rFID. (c) Gram loss efficiency Figure 4: Adversarial-free training with Gram loss achieves stable, high-fidelity reconstruction. (a) GAN training fails in our setting: the discriminator overpowers the generator, causing diverging logits and degraded rFID. (b) Decomposing rFID reveals 86.6% of error stems from covariance (texture/style) vs. 13.4% from mean components. (c) Gram loss directly optimizes second-order statistics (i.e., feature covariance) without adversarial training, achieving superior and stable rFID throughout training. Figure 5: Progressive training curriculum of AToken. Our model starts from SigLIP2 image understanding and progressively adds: (1) image reconstruction, (2) video capabilities with temporal modeling, (3) 3D understanding with expanded resolutions, and optionally (4) discrete tokenization via FSQ. Each box shows the new capabilities introduced at that stage, along with supported resolutions, patch sizes, and sampling strategies. Semantic Loss. We align visual representations zs with text embeddings through modalityspecific objectives. For images, we distill knowledge from the frozen SigLIP2 vision encoder (Tschannen et al., 2025) by minimizing the KL divergence between temperature-scaled vision-text similarity distributions: sem = KL (cid:0)softmax(τ 1steacher) softmax(τ 1sstudent)(cid:1) , LI where steacher and sstudent are vision-text similarity scores from frozen SigLIP2 and our model respectively, both paired with the same frozen text encoder, and τ is the temperature parameter. For videos and 3D, we directly optimize alignment using the sigmoid loss from SigLIP (Zhai et al., 2023), which proves more stable for the smaller batch sizes typical in these domains. This dual strategy preserves pretrained image semantics while enabling efficient learning for new modalities. (7)"
        },
        {
            "title": "3.4 TRAINING STRATEGY",
            "content": "Our training employs four-stage progressive curriculum (Figure 5) that builds from image foundations to video dynamics to 3D geometry, with optional discrete quantization. Starting from the pretrained SigLIP2 encoder (Tschannen et al., 2025), we gradually introduce more complex objectives and modalities while maintaining semantic understanding across all stages. We implement this curriculum through round-robin sampling of modalities and tasks, using gradient accumulation to balance image-text distillation with other objectives (reconstruction, video-text alignment, 3D-text alignment) across all stages. This ensures semantic alignment is preserved even as reconstruction capabilities expand. Our sparse transformer architecture facilitates this multimodal training by separating features and positions, allowing each modality to be processed at its natural resolution without padding or packing. Stage 1: Image Foundation. Starting from pretrained SigLIP2, we establish core visual representations by adding image reconstruction capabilities. We process images using 41616 space-time patches with temporal padding for consistency, employing 32 latent dimensions following (Yao &"
        },
        {
            "title": "Preprint",
            "content": "Figure 6: Overview of the video encoding and decoding process. During encoding, we use KV-caching across temporal tiles to eliminate redundant computation while maintaining temporal coherence, providing significant efficiency gains over overlapping tile methods. Table 2: Training curriculum configuration. Resolution limits for each modality and task sampling ratios across the four training stages. Superscripts denote reconstruction (r) and understanding (u) tasks. Training Stage Image Res. Video Res. 3D Size Stage 1: Image Foundation Stage 2: Video Dynamics Stage 3: 3D Geometry Stage 4: Discrete Tokenization [64 512] [64 1024] [64 2048] [64 2048] - [64 512] [64 1024] [64 1024] - - [64, 64, 64] [64, 64, 64] Task Sampling Ratios Ir Vu Vr 3Du 3Dr #Steps - 200k - 100% 200k 22.2% 11.1% 66.6% 22.2% 11.1% 44.4% 11.1% 11.1% 50k 22.2% 11.1% 44.4% 11.1% 11.1% 100k - - - - Wang, 2025). Training uses variable resolution sampling from 64 to 512 pixels, with L1 loss computed at native resolution while perceptual losses (LLPIPS, LCLIP, LGram) use 224 224 interpolation to match their pretrained features. Stage 2: Video Dynamics. We extend to temporal sequences, expanding latent dimensions from 32 to 48 to accommodate motion complexity (Seawead et al., 2025). Resolution capabilities increase to 1024 for images and 512 for videos. We employ temporal tiling (16-32 frames 4-8 latent frames) with adaptive sampling: stride 1-3 for temporal consistency or 4-12 for diversity in reconstruction, 1 FPS up to 64 frames for understanding. Our KV-caching mechanism (Figure 6) eliminates redundant computation across tiles while maintaining temporal coherence. Stage 3: 3D Geometry. We incorporate 3D assets as active voxels in 643 grids, using Gaussian splatting for reconstruction and attention pooling for understanding. Resolution further increases to 2048 for images and 1024 for videos. Joint optimization across all three modalities prevents catastrophic forgetting while leveraging cross-modal learning. The geometric semantics from 3D and the temporal dynamics from video enhance image reconstruction quality. Stage 4: Discrete Tokenization. Optionally, we add FSQ quantization (Mentzer et al., 2023) for discrete generation tasks. The 48-dimensional latents are partitioned into 8 groups of 6 dimensions, each quantized to 4 levels, yielding 8 discrete tokens from 4096-entry codebooks. We finetune the entire encoder and decoder to adapt all modalities to discrete tokens, enabling compatibility with discrete generative models across all visual domains. 3."
        },
        {
            "title": "IMPLEMENTATION DETAILS",
            "content": "Our encoder and decoder each contain 27 transformer blocks with hidden dimension = 1152 and 16 attention heads. The encoder is initialized from SigLIP-SO400M-patch16-naflex (Tschannen et al., 2025), while the decoder is trained from scratch. We optimize using AdamW with β1 = 0.9, β2 = 0.95, and weight decay 0.1. The learning rate follows linear warmup for 2,000 steps to ηmax = 3 104, then cosine annealing to ηmin = 3 105. Given the pretrained encoder, we apply reduced learning rate ηencoder = 0.1 ηbase and use exponential moving average with decay rate γ = 0.9999. Training utilizes 256 H100 GPUs with adaptive global batch sizes optimized for each tasks memory requirements. Image understanding maintains 8,192 samples throughout all stages, while reconstruction tasks scale with complexity: image reconstruction uses 1,024-4,096, video reconstruction"
        },
        {
            "title": "Preprint",
            "content": "Table 3: Performance comparison of visual tokenizers across modalities. We evaluate on ImageNet for image reconstruction and zero-shot classification, TokenBench for video reconstruction with MSR-VTT for zero-shot retrieval, and Toys4k for 3D reconstruction and classification. Methods are grouped by capability: reconstruction-only, understanding-only, and unified approaches. Discrete tokenizers are indicated with gray shading. OmniTokenizer does not work well on high-resolution videos where tiling is needed. Comp. Ratio Latent Channels Token Type Image Video 3D PSNR rFID Acc. PSNR rFVD R@1 PSNR LPIPS Acc. Method Reconstruction Only (1, 8, 8) SD-VAE (1, 8, 8) FLUX.1 [dev] Cosmos-0.1-CI88 (1, 8, 8) (1, 8, 8) Qwen-Image (1, 16, 16) VA-VAE GigaTok-XL-XXL (1, 16, 16) Cosmos-0.1-CV88 (4, 8, 8) OmniTokenizer (4, 8, 8) (4, 8, 8) Hunyuan (4, 8, 8) Wan2.1 (4, 16, 16) Wan2.2 OmniTokenizer (4, 8, 8) Cosmos-0.1-DV88 (4, 8, 8) Trellis-SLAT - Understanding Only VideoPrism-g SigLIP2-So/16 PEcoreL (1, 18, 18) (1, 16, 16) (1, 14, 14) Reconstruction & Understanding 4 16 16 16 32 8 16 8 16 16 48 8 6 8 - - - SeTok VILA-U UniTok ATOKEN-So/D ATOKEN-So/C - (1, 16, 16) (1, 16, 16) (4, 16, 16) (4, 16, 16) 4096 16 64 48 48 VAE VAE AE VAE VAE VQ AE VAE VAE VAE VAE VQ FSQ VAE - - - AE RQ MCQ FSQ VAE 26.26 32.86 32.25 32.18 27.70 22.42 30.11 26.74 33.32 31.34 31.25 24.69 26.34 - - - - - 22.24 25. 27.00 29.72 0.61 0.18 1.03 1.46 0.28 0.80 7.55 1.02 0.67 0.94 0.75 1.41 7.86 - - - - 2.07 4.23 0.36 0.38 0.21 - - - - - - - - - - - - - - - 83.4 83.5 75.4 78.0 78.6 82.2 82.2 - - - - - - 34.33 19.39 36.37 36.11 36.39 19.89 31.42 - - - - - - - - - - - - - 8.34 173.48 3.78 3.21 3.19 202.46 25.94 - - - - - - - - - - - - - - - - - - - - - 52.7 41.9 50.3 - - - - - - - - - - - - - - - - 26.97 - - - - - - - - - - - - - - - - - - - 0.054 - - - - - - - - - - - - - - - - - - - - - - - - - - 33.12 36.07 22.16 3.01 40.3 40. 28.17 28.28 0.063 0.062 91.3 90.9 uses 512-1024, and 3D reconstruction uses 256-512. The four-stage curriculum trains for 200k, 200k, 50k, and 100k iterations, respectively, with each stage initialized from the previous checkpoint, requiring total of 138k GPU hours across all stages (approximately 22 days with 256 GPUs). Throughout training, we maintain fixed loss coefficients: λrec = 0.2, λsem = 1.0, and λKL = 108. Within reconstruction (Eq. 6), we set λ1 = 1.0, λLPIPS = 10.0, λGRAM = 103, λCLIP = 1.0, and τ = 2.0. We normalize reconstruction losses over patches rather than summing (Esser et al., 2020), providing stable gradients across resolutions. Training data follows our progressive curriculum: DFN (Fang et al., 2023), Open Images (Kuznetsova et al., 2020), and internal datasets for images; WebVid (Bain et al., 2021) and TextVR (Wu et al., 2025c) for video understanding with Panda70M (Chen et al., 2024b) for reconstruction; Objaverse (Deitke et al., 2023) with Cap3D (Luo et al., 2024a) annotations for 3D. Datasets are sampled proportionally to their size, with task ratios detailed in Table 2."
        },
        {
            "title": "4 MAIN RESULTS",
            "content": "We evaluate ATOKEN as the first visual tokenizer to achieve both reconstruction and understanding across images, videos, and 3D assets. This section presents unified comparisons (Section 4.1) followed by per-modality analysis (Sections 4.2-4.4) and ablations (Section 4.5)."
        },
        {
            "title": "4.1 UNIFIED TOKENIZER COMPARISONS",
            "content": "Table 3 presents comparison of visual tokenizers across modalities. We evaluate on standardized benchmarks: ImageNet (Deng et al., 2009) at 256256 (reconstruction: PSNR, rFID; understanding: zero-shot accuracy), TokenBench (Agarwal et al., 2025) at 720p and MSR-VTT (Xu et al., 2016) for video (reconstruction: PSNR, rFVD; understanding: text-to-video R@1), and Toys4k (Stojanov et al., 2021a) for 3D (reconstruction: PSNR, LPIPS; understanding: zero-shot accuracy)."
        },
        {
            "title": "Preprint",
            "content": "Table 4: Image reconstruction comparison on ImageNet and COCO. We evaluate all methods using unified protocol with official implementations to ensure fair comparison. All images are resized and centercropped to 256256, with metrics computed using identical scripts. Note that our reproduced results may differ from original papers due to standardized evaluation settings, but provide consistent cross-model comparison. Method Continuous Latent SD-VAE SD3-VAE FLUX.1 [dev] Qwen-Image Cosmos-0.1-CI88 Cosmos-0.1-CI1616 VAVAE OmniTokenizer Hunyuan Wan2.1 Wan2.2 ATOKEN-So/C Stage 1 Stage 2 Stage Discrete Latent Cosmos-0.1-DI88 GigaTok-B-L GigaTok-XL-XXL Vila-U UniTok OmniTokenizer ATOKEN-So/D Comp. Ratio Latent Size Token Type ImageNet COCO PSNR SSIM LPIPS rFID PSNR SSIM LPIPS rFID (1, 8, 8) (1, 8, 8) (1, 8, 8) (1, 8, 8) (1, 8, 8) (1, 16, 16) (1, 16, 16) (4, 8, 8) (4, 8, 8) (4, 8, 8) (4, 16, 16) (1, 16, 16) (4, 16, 16) (4, 16, 16) (1, 8, 8) (1, 16, 16) (1, 16, 16) (1, 16, 16) (1, 16, 16) (4, 8, 8) (4, 16, 16) 4 16 16 16 16 16 32 8 16 16 48 32 48 48 6 8 8 16 64 8 48 VAE VAE VAE VAE AE AE VAE VAE VAE VAE VAE VAE VAE VAE FSQ VQ VQ RQ MCQ VQ FSQ 26.26 31.29 32.86 32.18 32.25 25.07 27.70 26.74 33.32 31.34 31.25 28.77 29.55 29.72 25.87 21.87 22.42 22.24 25.34 24.69 27. 0.745 0.886 0.917 0.899 0.902 0.700 0.798 0.824 0.916 0.886 0.878 0.814 0.845 0.848 0.750 0.591 0.613 0.612 0.742 0.771 0.801 0.133 0.059 0.044 0.053 0.064 0.167 0.096 0.101 0.053 0.058 0.057 0.099 0.087 0. 0.155 0.200 0.189 0.228 0.132 0.138 0.119 0.606 0.201 0.176 1.459 1.031 0.959 0.279 1.023 0.670 0.945 0.749 0.258 0.246 0.209 0.867 0.507 0.795 4.231 0.362 1.411 0. 25.99 31.18 32.73 32.01 32.08 24.74 27.50 26.44 33.25 31.19 31.10 28.66 29.49 29.67 25.54 21.42 22.03 21.89 24.95 24.31 27.00 0.759 0.894 0.923 0.908 0.909 0.711 0.811 0.833 0.924 0.895 0.888 0.829 0.858 0. 0.760 0.596 0.620 0.620 0.750 0.779 0.815 0.130 0.056 0.041 0.050 0.061 0.165 0.093 0.099 0.050 0.055 0.054 0.096 0.083 0.081 0.153 0.202 0.191 0.227 0.131 0.137 0. 4.142 1.671 1.343 4.618 3.844 5.063 2.709 4.687 2.597 3.449 3.279 2.336 2.180 2.026 5.016 5.565 5.757 10.997 3.918 6.292 3.270 The results reveal three distinct categories of approaches, each with fundamental limitations. Reconstruction-only tokenizers excel at generation but cannot extract semantic features: SDVAE (Rombach et al., 2022), FLUX.1 (Labs et al., 2025), VA-VAE (Yao & Wang, 2025), and Qwen-Image (Wu et al., 2025a) for images; Hunyuan (Kong et al., 2024) and WAN (Wan et al., 2025) for video; Trellis-SLAT (Xiang et al., 2024) for 3D. Understanding-only encoders provide rich semantics but cannot reconstruct visual content: SigLIP2 (Tschannen et al., 2025), VideoPrism (Zhao et al., 2024), and PEcore (Bolya et al., 2025). Recent unified attempts combine both capabilities but remain limited to images: SeTok (Wu et al., 2024b), VILA-U (Wu et al., 2024c), and UniTok (Ma et al., 2025a). ATOKEN-So/C breaks these boundaries as the first tokenizer to unify all three capabilities. On images, we achieve 0.21 rFID with 82.2% zero-shot ImageNet accuracy, substantially outperforming UniToks 0.36 rFID and 78.6% accuracy. More importantly, we extend this unified capability to video (3.01 rFVD, 40.2% R@1) and 3D (28.28 PSNR, 90.9% accuracy), comparable or even surpassing specialized methods like Wan2.2 and Trellis-SLAT on Video and 3D reconstruction. Our discrete variant (ATOKEN-So/D) maintains competitive performance, pioneering discrete tokenization across all modalities. 4."
        },
        {
            "title": "IMAGE TOKENIZATION",
            "content": "We evaluate ATOKENs image capabilities against specialized tokenizers through reconstruction quality  (Table 4)  and semantic understanding  (Table 5)  benchmarks. Reconstruction Performance. Table 4 presents our comprehensive evaluation, where we reevaluated all baseline methods using unified protocol with official implementations to ensure fair comparison. Under this standardized evaluation protocol, we observe that multimodal training enhances rather than compromises image reconstruction. ATOKEN-So/C achieves 0.209 rFID at 1616 compression, with progressive improvement across training stages: 0.258 (Stage 1)0.246 (Stage 2)0.209 (Stage 3), 19% gain through multimodal expansion. This improvement is particularly notable given three fundamental challenges in the field. First, the compression-dimension trade-off severely constrains 1616 models: VAVAE (Yao & Wang, 2025) requires 32-dimensional latents to achieve 0.279 rFID, while Cosmos-CI1616 with 16 dimensions degrades to 0.959 rFID. Second, transformer architectures consistently underperform convolutional architectures (OmniTokenizer (Wang et al., 2024b) 26.74 PSNR vs. Hunyuan (Kong et al., 2024) 33.32 PSNR), explaining why most reconstruction tokenizers avoid transformers. Third, discrete"
        },
        {
            "title": "Preprint",
            "content": "Table 5: Image understanding comparison with semantic encoders. We evaluate zero-shot classification on ImageNet, ImageNet-v2, and cross-modal retrieval on COCO and Flickr30k. ATOKEN maintains competitive performance across all stages despite joint training on multiple modalities and tasks. Res. Seq. Model 224 196 256 384 576 512 1024 CLIP MetaCLIP EVA-CLIP DFN SigLIP SigLIP 2 ATOKEN-So/C Stage 1 Stage 2 Stage 3 ATOKEN-So/D SigLIP 2 ATOKEN-So/C Stage 1 Stage 2 Stage 3 ATOKEN-So/D SigLIP 2 ATOKEN-So/C Stage 1 Stage 2 Stage 3 ATOKEN-So/D ImageNet-1k COCO Flickr val 68.3 72.4 74.7 76.2 80.8 83. 82.7 82.3 82.2 82.2 84.1 83.4 82.9 82.9 82.8 84.3 83.5 83.1 82.9 82.9 61.9 65.1 67.0 68.2 74.1 77.8 76.7 76.4 76.1 76.2 78.4 77.6 77.1 76.8 76.6 79. 77.8 77.3 77.2 77.0 TI IT TI IT 33.1 48.9 42.2 51.9 49.4 55. 54.1 53.8 53.7 53.8 56.0 54.8 54.7 54.6 54.4 56.0 54.7 54.7 54.7 54.7 52.4 58.7 68.6 71.5 70.4 70.6 70.5 70.1 71.2 70.4 71.1 71.3 70.9 71.3 71.1 71.3 71.1 71. 62.1 77.1 71.2 77.3 80.0 84.4 81.3 80.7 80.5 80.9 85.3 81.7 81.9 81.9 81.9 85. 82.1 82.2 82.3 82.3 81.9 85.7 92.1 94.2 93.1 93.0 93.2 93.5 95.9 93.8 93.9 93.5 93. 95.4 94.1 93.6 93.6 93.5 tokenizers struggle with generalization UniTok (Ma et al., 2025a) degrades from 0.362 rFID on ImageNet to 3.918 on COCO, while GigaTok (Xiong et al., 2025) exhibits even larger gaps. Our approach addresses all three challenges: achieving strong performance with 48-dimensional latents at 1616 compression, demonstrating transformer viability through adversarial-free training, and maintaining consistent quality across datasets (0.209 rFID on ImageNet, 2.026 rFID on COCO). These results suggest temporal dynamics from video and geometric understanding from 3D provide complementary signals for image reconstruction. Semantic Understanding. Table 5 evaluates zero-shot classification and retrieval against leading vision encoders. While understanding-only models like CLIP (Radford et al., 2021) and its variants (Xu et al., 2023; Sun et al., 2023; Fang et al., 2023) optimize purely for semantic alignment, ATOKEN need to balance understanding with reconstruction across three modalities. Despite these constraints, ATOKEN achieves 82.2% ImageNet accuracy within 1.2% of understanding-only SigLIP2 (Tschannen et al., 2025) (83.4%). This narrows the gap compared to previous unified attempts like UniTok (78.6%) and VILA-U (78.0%), while uniquely extending unified capabilities to video and 3D. Across our progressive training stages, accuracy remains stable (82.7% 82.3% 82.2%), with only 0.5% degradation as modalities are added. Discrete quantization also preserves full semantic performance, achieving 82.2% accuracy."
        },
        {
            "title": "4.3 VIDEO TOKENIZATION",
            "content": "We evaluate ATOKENs video capabilities through reconstruction quality and semantic understanding benchmarks, demonstrating competitive performance while uniquely supporting both continuous and discrete representations across multiple modalities. Reconstruction Performance. We evaluate video reconstruction on DAVIS (Pont-Tuset et al., 2017) (1080p, 50 videos) and TokenBench (Agarwal et al., 2025) (720p, 471 videos), reporting PSNR and SSIM for pixel quality, LPIPS for perceptual similarity, and rFVD for temporal consistency. All baselines were re-evaluated using official implementations with consistent protocols and spatial tiling for memory management. ATOKEN employs temporal tiling with KV-caching, leveraging its native 20482048 resolution support."
        },
        {
            "title": "Preprint",
            "content": "Table 6: Video reconstruction comparison on high-resolution benchmarks. We evaluate quality on DAVIS at 1080p and TokenBench at 720p. All methods are re-evaluated using official implementations with consistent protocols for fair comparison. ATOKEN achieves competitive performance with specialized video-only tokenizers while uniquely supporting both continuous and discrete representations across modalities. Tokenizer Continuous Latent Comp. Ratio Latent Size Token Type DAVIS TokenBench PSNR SSIM LPIPS rFVD PSNR SSIM LPIPS rFVD Cosmos-0.1-CV488 OmniTokenizer Hunyuan Wan2.1 Wan2.2 (4, 8, 8) (4, 8, 8) (4, 8, 8) (4, 8, 8) (4, 16, 16) ATOKEN-So/C Stage 2 Stage 3 Discrete Latent OmniTokenizer Cosmos-0.1-DV488 ATOKEN-So/D (4, 16, 16) (4, 16, 16) (4, 8, 8) (4, 8, 8) (4, 16, 16) 16 8 16 16 48 48 8 6 48 AE VAE VAE VAE VAE VAE VAE VQ FSQ FSQ 32.25 21.06 32.33 33.50 33.06 32.29 33.11 20.62 27.26 29.75 0.894 0.800 0.907 0.884 0. 0.902 0.907 0.770 0.798 0.846 0.219 0.315 0.194 0.164 0.184 0.196 0.189 0.346 0. 0.288 19.15 206.34 22.94 17.75 12.65 13.50 10.76 240.20 110.33 41.42 34.33 19.39 36.37 36.11 36. 35.63 36.07 19.89 31.20 33.12 0.924 0.782 0.944 0.940 0.942 0.937 0.940 0.787 0. 0.913 0.155 0.275 0.129 0.128 0.126 0.139 0.135 0.293 0.190 0.193 8.34 173.48 3.78 3.21 3. 3.63 3.01 202.46 25.94 22.16 Table 7: Zero-shot video-text retrieval on MSRVTT and MSVD. We compare ATOKEN against understanding-focused encoders on standard video retrieval benchmarks. Despite optimizing for both reconstruction and understanding across three modalities, ATOKEN maintains reasonable retrieval performance. MSRVTT (1K-A) MSVD Methods Res. Text Video Video Text Text Video Video Text R@1 R@5 R@10 R@1 R@5 R@ R@1 R@5 R@10 R@1 R@5 R@ CLIP-ViT-B/32 SigLIP2-So400m VideoPrism-g PE-Core-B16 PE-Core-L14 ATOKEN-So/C224 Stage 1 Stage 2 Stage 3 ATOKEN-So/D 224 256 288 224 336 224 224 224 31.2 41.9 52.7 45.8 49.1 40.8 40.1 40.2 40.3 53.7 66.3 77.2 70.1 73.3 65.3 64.9 64.9 65.0 63.3 75.7 - 78.1 81.6 75.2 75.2 75.2 74. 26.4 32.4 51.7 45.5 50.9 31.0 30.9 30.5 30.3 49.9 55.4 75.2 70.9 74.4 55.0 53.7 53.1 51.8 61.7 65.9 - 80.0 82.7 63.7 64.0 63.2 61. 36.4 55.5 - 48.7 54.4 53.9 53.4 53.5 53.8 63.3 81.2 - 75.5 81.2 79.9 79.6 79.5 79.7 73.1 87.8 - 84.1 88.4 87.3 87.1 87.1 87. 57.8 72.7 - 79.1 82.5 72.4 71.6 72.4 71.5 84.1 91.7 - 96.7 98.2 93.0 91.9 91.6 91.8 90.7 96.1 - 98.8 99.4 95.4 95.5 95.4 95. As shown in Table 6, ATOKEN-So/C achieves 33.11 PSNR on DAVIS and 36.07 PSNR on TokenBench, approaching specialized video-only models (Wan2.1 (Wan et al., 2025): 33.50 and 36.11, Hunyuan (Kong et al., 2024): 32.33 and 36.37). Notably, we demonstrate that transformers can match CNN performance when properly designed our method dramatically outperforms OmniTokenizers transformer baseline (21.06 vs 33.11 PSNR on DAVIS) while adding native resolution support. Furthermore, our progressive training reveals cross-modal benefits: incorporating 3D in Stage 3 improves video reconstruction from 35.63 to 36.07 PSNR on TokenBench, indicating that geometric understanding may enhance temporal modeling. For discrete tokenization, ATOKEN-So/D pioneers multimodal video support, achieving 29.75 PSNR on DAVIS surpassing Cosmos-0.1DV (27.26) and dramatically outperforming OmniTokenizer (20.62), while maintaining reasonable perceptual quality (0.288 LPIPS) for downstream tasks. Semantic Understanding. Table 7 evaluates zero-shot video-text retrieval on MSRVTT (Xu et al., 2016) and MSVD (Chen & Dolan, 2011). Following standard protocols (Wang et al., 2022b; Luo et al., 2021), we use frame embedding averaging with zero-padding. ATOKEN achieves 40.2% R@1 on MSRVTT and 53.5% on MSVD, maintaining reasonable semantic alignment despite optimizing primarily for reconstruction across three modalities. We note that alternative pooling strategies without frame averaging yielded lower performance, likely due to the limited video-text pairs in our training data compared to dedicated video understanding models. While understanding-only models trained on large-scale video-text data achieve higher scores, our results validate that unified tokenization successfully balances reconstruction quality with semantic understanding. 4.4 3D TOKENIZATION. We evaluate ATOKENs 3D capabilities on Toys4k (Stojanov et al., 2021b) for reconstruction and semantic understanding. For reconstruction, ATOKEN-So/C achieves 28.28 PSNR and 0.062 LPIPS  (Table 8)  , surpassing the specialized Trellis-SLAT (Xiang et al., 2024) baseline (26.97 PSNR, 0."
        },
        {
            "title": "Preprint",
            "content": "Figure 7: Architectural scaling comparison: Base vs. So400m models. (a) ImageNet rFID during Stage 1 training. (b) ImageNet rFID across training stages. (c) ImageNet zero-shot classification accuracy in Stage 1. (d) Video PSNR on DAVIS in Stages 2 and 3. The So400m model maintains or improves performance across all stages, while the Base model shows significant degradation when extending beyond single-modality training, indicating that sufficient model capacity is critical for successful multimodal visual tokenization. Table 8: 3D reconstruction comparison on Toys4k. We average metrics across rendered multi-view images. ATOKEN achieves comparable performance to specialized Trellis-SLAT despite jointly optimizing for three modalities, demonstrating unified training maintains strong 3D capabilities. Method PSNR SSIM LPIPS Specialized 3D Tokenizer Trellis-SLAT 26.97 0.943 0.054 Our Unified Tokenizer (ATOKEN) ATOKEN -So/C ATOKEN -So/D 28.28 28.17 0.951 0.951 0.062 0.063 LPIPS) despite jointly training across three modalities. This demonstrates that our unified 4D representation effectively captures geometric structure without requiring dedicated 3D architectures. For semantic understanding, ATOKEN-So/C achieves 90.9% zero-shot classification accuracy on Toys4k, validating that our approach maintains strong semantic representations for 3D objects alongside reconstruction capabilities. Combined with our image and video results, this confirms that all three modalities can coexist within single tokenizer without significant trade-offs."
        },
        {
            "title": "4.5 ABLATION STUDY",
            "content": "Scaling Analysis. To investigate the scaling property of the visual tokenizer, we compare our So400m model with smaller Base variant following identical training procedures. The Base model initializes from SigLIP-Base-patch16-naflex (Tschannen et al., 2025), comprising 12 transformer blocks with hidden dimension = 768 and 12 attention heads for both encoder and decoder, yielding approximately 192M parameters compared to So400ms 800M. As shown in Figure 7, both models achieve reasonable single-modal performance in Stage 1, with So400m outperforming Base (0.258 vs 0.323 rFID, 82.7% vs 77.2% accuracy). However, the Base model suffers severe degradation when expanding to videos, with ImageNet rFID degrading 49% (0.3230.483) and video PSNR declining across stages. In contrast, So400m improves continuously ImageNet rFID enhances 19% (0.2580.209) while video PSNR rises from 32.51 to 33.11. This scaling analysis reveals that multimodal tokenization has capacity requirement: small models suffer from interference while large models benefit from cross-modal learning. Representation Structure Analysis. Figure 8 visualizes learned representations through T-SNE projections across training stages. Dense features (a-c) show clear semantic clustering with distinct ImageNet class separation. However, projection to 48-dimensional latents (d-e) results in more intermixed distributions, likely due to KL regularization without post-projection alignment loss. Despite this apparent mixing in T-SNE visualizations, the model maintains strong reconstruction and understanding performance, suggesting that semantic information may be encoded in ways not captured by 2D projections. This raises an interesting question: whether explicit semantic cluster-"
        },
        {
            "title": "Preprint",
            "content": "Figure 8: Learned representations across training stages. T-SNE visualizations of ImageNet class embeddings (colors indicate different classes). (a) Stage 1: image-only training. (b) Stage 2: with video. (c) Stage 3: dense features before projection. (d) Stage 3: projected 48-dim latents. (e) Stage 4: before FSQ quantization. Dense features (a-c) show clear semantic clustering, while dimensional reduction (d-e) leads to more mixed class distributions, suggesting trade-off between compression and semantic separability. ing in low-dimensional spaces as emphasized by methods like VAVAE (Yao & Wang, 2025) is necessary for strong performance, or whether larger models can effectively leverage seemingly intermixed representations. Our results suggest the latter, though we leave detailed investigation of semantic preservation through aggressive dimensionality reduction for future work. Reconstruction Visualization. Figures 9-11 provide qualitative comparisons of reconstruction quality across all three modalities. For images (Figure 9), ATOKEN operates at higher compression ratio (16) than most baselines yet achieves superior visual fidelity, particularly in preserving high-frequency details such as text clarity, fine textures, and complex patterns. The comparison reveals that methods optimized for lower compression ratios (e.g., SD-VAE and OmniTok at 8) struggle with text legibility and texture preservation, while ATOKEN maintains sharp details. For video reconstruction (Figure 10), ATOKEN demonstrates temporal consistency comparable to specialized video tokenizers like Wan2.2, with both continuous and discrete variants preserving motion smoothness across 720p sequences. The 3D reconstruction results (Figure 11) highlight ATOKENs advantage in color consistency. While Trellis-SLAT exhibits color shifts and artifacts, our unified training across modalities transfers color understanding from images and videos to improve 3D reconstruction."
        },
        {
            "title": "5 DOWNSTREAM RESULTS",
            "content": "Having established ATOKENs unified tokenization capabilities across modalities, we evaluate its effectiveness in diverse downstream applications. We assess both understanding tasks through multimodal LLMs (Section 5.1) and generation tasks across images, videos, and 3D assets (Sections 5.2 5.5). These experiments demonstrate that single unified tokenizer can serve as the foundation for multimodal AI systems without compromising task-specific performance."
        },
        {
            "title": "5.1 MULTIMODAL LLMS",
            "content": "To validate ATOKENs effectiveness for vision-language understanding, we integrate it into SlowFast-LLaVA-1.5 (Xu et al., 2025), replacing the Oryx-ViT (Liu et al., 2024b) vision encoder with ATOKEN-So/C while keeping all other settings identical. To assess generalization, the ATOKEN parameters are frozen during training, with only the SlowFast projector and LLM updated. We evaluate using the lmms-eval (Zhang et al., 2024a) toolkit and report official metrics without output filtering. Image Understanding. Table 9 shows the image understanding results on 7 standard benchmarks, including RW-QA*, AI2D (Kembhavi et al., 2016), SQA (Lu et al., 2022b), and MMMU (Yue et al., 2024), and MathVISTA (Lu et al., 2024b) for general image QA, as well as OCRBench (Liu et al., 2024a) and TextVQA (Singh et al., 2019) for text and document understanding. To position our models relative to state-of-the-art methods, we compare it against LLaVA-OV (Li et al., 2024a), MM1.5 (Zhang et al., 2025), Molmo (Deitke et al., 2024), BLIP3 (Xue et al., 2024b), Phi-3.5V (Abdin et al., 2024), InternVL2.5 (Zhang et al., 2024b), and Qwen2-VL (Wang et al., 2024c). *https://huggingface.co/datasets/xai-org/RealworldQA"
        },
        {
            "title": "Preprint",
            "content": "Figure 9: Qualitative comparison of image reconstruction performance across different tokenization methods. The latent shape for 256 256 image patch is shown under each method name. Despite operating at higher compression ratios, ATOKEN demonstrates superior reconstruction quality, particularly excelling in preserving high-frequency textures, fine details, and complex text elements. Figure 10: Qualitative comparison of video reconstruction performance on 720p video sequences. The latent shape for each video tokenization method is indicated under the method name. ATOKEN achieves comparable quality to specialized video-only methods while uniquely supporting both continuous and discrete representations in unified framework. Figure 11: 3D Reconstruction Visualization on Toys4k. ATOKENs improved color consistency results in higher PSNR compared to specialized 3D tokenizer Trellis-SLAT."
        },
        {
            "title": "Preprint",
            "content": "Table 9: Image understanding comparison across multimodal LLMs. Evaluation of SlowFast-LLaVA-1.5 with frozen ATOKEN-So/C vision encoder versus Oryx-ViT and other state-of-the-art MLLMs. Results shown for 7 benchmarks (general QA and text-rich understanding) across 1B, 3B, and 7B model scales. Vision Encoder # Input Pixels RW-QA AI2D SQA MMMU MathV OCRBench TextVQA (test) (test) (test) (val) (testmini) (test) (val) General & Knowledge TextRich Multimodal LLM 1B Model Comparison LLaVA-OV-0.5B MM1.5-1B MolmoE-1B SlowFast-LLaVA-1.5-1B SigLIP CLIP MetaCLIP Oryx-ViT SlowFast-LLaVA-1.5-1B ATOKEN-So/C 3B Model Comparison BLIP3-4B MM1.5-3B Phi-3.5-V-4B SlowFast-LLaVA-1.5-3B SigLIP CLIP CLIP Oryx-ViT SlowFast-LLaVA-1.5-3B ATOKEN-So/C 7B Model Comparison LLaVA-OV-7B MM1.5-7B Oryx1.5-7B InternVL2.5-8B Qwen2-VL-7B SlowFast-LLaVA-1.5-7B SigLIP CLIP Oryx-ViT InternViT DFN Oryx-ViT SlowFast-LLaVA-1.5-7B ATOKEN-So/C 5.31M 4.52M 4.10M 2.36M 2.36M - 4.52M - 2.36M 2.36M 5.31M 4.52M 2.36M 9.63M - 2.36M 2.36M 55.6 53.3 60.4 59.2 60.1 60.5 56.9 - 63.4 64. 66.3 62.5 - 70.1 70.1 67.5 68.8 57.1 59.3 86.4 72.8 74.2 - 65.7 78.1 77.0 79. 81.4 72.2 79.7 84.5 83.0 80.4 81.2 67.2 82.1 - 87.7 88.7 88.3 85.8 91.3 90.3 89. 96.0 89.6 - - - 91.1 92.1 31.4 35.8 34.9 40.5 40.6 41.1 37.1 43.0 44.7 45. 48.8 41.8 47.1 56.0 54.1 49.0 48.7 34.8 37.2 34.0 51.0 52.5 39.6 44.4 43.9 58.6 58. 63.2 47.6 - 64.4 58.2 62.5 61.2 - 60.5 - 70.0 67.6 - 65.7 - 73.4 73. - 63.5 71.3 - - 76.4 74.5 - 72.5 78.8 71.3 72.5 71.0 76.5 72.0 73.0 72. - 76.5 75.7 79.1 84.3 76.4 77.7 Table 10: Video understanding performance on multimodal LLMs. Evaluation of SlowFast-LLaVA-1.5 with frozen ATOKEN-So/C vision encoder versus Oryx-ViT and other video MLLMs. Results shown for 6 benchmarks (general and long-form video understanding) across 1B, 3B, and 7B model scales. Vision Encoder # Input Tokens VideoMME PercepTest NExT-QA LongVideoBench MLVU LVBench (w/o sub) (val) (test) (val) (m-avg) (avg) General VideoQA Long-Form Video Understanding Multimodal LLM 1B Model Comparison Apollo-1.5B InternVL2.5-2B Qwen2-VL-2B SlowFast-LLaVA-1.5-1B SigLIP InternViT DFN Oryx-ViT SlowFast-LLaVA-1.5-1B ATOKEN-So/C 3B Model Comparison InternVL2-4B LinVT-Blip3-4B Apollo-3B SF-LLaVA-1.5-3B InternViT SigLIP SigLIP Oryx-ViT SF-LLaVA-1.5-3B ATOKEN-So/C 7B Model Comparison Oryx1.5-7B LLaVA-Video-7B Apollo-7B InternVL2.5-8B Qwen2-VL-7B SlowFast-LLaVA-1.5-7B Oryx-ViT SigLIP SigLIP InternViT DFN Oryx-ViT SlowFast-LLaVA-1.5-7B ATOKEN-So/C 3K 16K 16K 9K 9K 16K - 3K 9K 9K 14K 11K 3K 16K 16K 9K 9K 53.0 51.9 55.6 56.6 56. 53.9 58.3 58.4 60.8 60.4 58.8 63.3 61.3 64.2 63.3 63.9 64.5 61.0 - 53.9 61.9 63. 53.9 - 65.0 65.8 66.0 70.0 66.9 67.3 - 62.3 69.6 70.3 - 77.2 77.2 76.7 74. 71.1 80.1 - 80.8 80.8 81.8 83.2 - 85.0 81.2 83.3 83.7 54.1 52.0 48.7 54.3 55. 53.0 56.6 55.1 57.2 57.2 56.3 58.2 58.5 60.0 55.6 62.5 60.6 63.3 61.4 62.7 64.3 64. 59.9 67.9 68.7 68.8 66.7 67.5 70.8 70.9 69.0 69.8 71.5 69.8 - 37.9 39.4 39.7 41. 35.1 - - 43.3 41.3 39.0 - - 43.2 44.7 45.3 44.8 Here we highlight some key observations. First, compared to Oryx-ViT, specific vision encoder for multimodal understanding, SlowFast-LLaVA-1.5 with ATOKEN as vision encoder shows overall better performance on image understanding across different model scales. Specifically, Table 9 shows that SlowFast-LLaVA-1.5-7B with ATOKEN outperforms Oryx-ViT under the same MLLM by 1.3% on RW-QA, 1.0% on SQA, and 1.3% on TextVQA. Second, ATOKEN shows strong generalization ability across different tasks and model scales. For reference, using ATOKEN, SlowFast-LLaVA1.5-3B achieves superior results on almost all benchmarks. On RW-QA and AI2D, ATOKEN outperforms Oryx-ViT across the 1B, 3B, and 7B scales and achieves very competitive performance. Video Understanding. The video understanding results are summarized in Table 10, covering range of video tasks. Video-MME (Fu et al., 2024), PercepTest (Patraucean et al., 2023), and NExTQA (Xiao et al., 2021) assess general video QA, whereas LongVideoBench (Wu et al., 2025b), MLVU (Zhou et al., 2024b), and LVBench (Wang et al., 2024d) focus on temporal understanding on long-range context. We compared with both video specialist models, such as Apollo (Zo-"
        },
        {
            "title": "Preprint",
            "content": "Table 11: Class-conditional image generation on ImageNet 256x256. We compare different ATOKENstages against the specialized VAVAE tokenizer using the Lightning-DiT framework. We report gFID, sFID, Inception Score (IS), Precision (Pre.), and Recall (Rec.). The VAVAE baseline applies CFG only to the first 3 latent channels, while we follow the standard protocol of applying it to all channels. Tokenizer Latent Channels CFG Scale gFID sFID IS Pre. Rec. DiT SiT REPA VAVAE ATOKEN-B/C Stage 1 Stage 2 Stage 3 ATOKEN-So/C Stage 1 Stage 2 Stage 4 4 4 32 32 48 48 32 48 48 1.5 1.5 1.35 6.7 1.5 1.65 1.65 1.5 1.65 1. 2.27 2.06 1.42 1.35 1.44 1.54 1.58 1.62 1.88 1.56 4.60 4.50 4.70 4.15 4.71 4.90 4.86 4.54 4.71 4. 278.2 270.3 305.7 295.3 273.3 254.7 254.6 253.3 231.1 260.0 0.83 0.82 0.80 0.79 0.79 0.77 0.76 0.78 0.80 0. 0.57 0.59 0.65 0.65 0.64 0.65 0.65 0.63 0.60 0.63 har et al., 2024), LLaVA-Video (Zhang et al., 2024c), and LinVT (Gao et al., 2024), and unified image-video MLLMs, such as Oryx1.5 (Liu et al., 2024b), InternVL2.5 (Zhang et al., 2024b), and Qwen2VL (Wang et al., 2024c). We outline several key observations. First, ATOKEN excels at smaller model scales. For reference, SlowFast-LLaVA-1.5-1.5B with ATOKEN achieves state-of-the-art performance on almost all benchmarks (e.g., outperforming Oryx-ViT by 0.8% on LongVideoBench and 1.4% on LVBench). Second, ATOKENprovides more performance gain on general video QA benchmarks. Specifically, it achieves state-of-the-art results on VideoMME (e.g., 64.5% with 7B LLM) and PercepTest (e.g., 70.3% with 7B LLM) across scales. Third, we note the strong performance of Oryx-ViT on longform video understanding, particularly on MLVU. We hypothesize that this advantage arises because (i) Oryx-ViT was specifically designed for video understanding in LLMs and (ii) it was trained on long-video retrieval tasks. Future work to address this gap includes incorporating more long videos into our training data to strengthen temporal modeling over long-range context. 5."
        },
        {
            "title": "IMAGE GENERATION WITH CONTINUOUS TOKENS",
            "content": "To evaluate ATOKENs generative capabilities with continuous tokens, we assess class-conditional ImageNet generation using the Lightning-DiT (Yao & Wang, 2025) framework. We compare against both general diffusion methods (DiT (Peebles & Xie, 2022), SiT (Ma et al., 2024a)) and reconstruction-specialized approaches (REPA (Yu et al., 2024b), VAVAE (Yao & Wang, 2025)). For fair comparison with VAVAE strong baseline optimized specifically for image reconstruction through DINOv2 alignment we use identical training code, only adapting the input layer for ATOKENs 48-dimensional latents (vs. 32 for VAVAE). We follow standard CFG protocol by applying guidance across all latent channels, using scale 1.65 for our 48-channel models (vs. 1.5 for 32-channel models), consistent with Lightning-DiT findings that wider latents benefit from stronger guidance. Note that VAVAE applies CFG only to the first three channels as reported in their work. As shown in Table 11, ATOKEN-So/C Stage 3 achieves 1.56 gFID, competitive with specialized tokenizers despite optimizing for multiple modalities and tasks simultaneously. While VAVAE achieves 1.35 gFID through image-specific optimization and REPA reaches 1.42 through specialized reconstruction alignment, ATOKEN demonstrates that unified tokenization can approach specialized performance without sacrificing versatility. Notably, our Base model shows consistent performance across stages (1.441.541.58 gFID), while the So model improves from Stage 2 to Stage 3 (1.881.56), suggesting that multimodal training can enhance generation quality."
        },
        {
            "title": "Preprint",
            "content": "Table 12: Discrete Tokenizer Class-conditional Image Generation on ImageNet. We evaluate ATOKENSo/D against other discrete tokenizer-based generation models. Metrics include model parameters, CFG scale, gFID, Inception Score (IS), Precision, and Recall. Tokenizer Generator # Params LFQ TikTok-L VQGAN UniTok TokenBridge MAGVIT-V2 MaskGiT LlamaGen LlamaGen TokenBridge-L ATOKEN-So/D TokenBridge-L 307M 227M 1.4B 1.4B 486M 548M CFG Scale - - 1.75 1 3. 3.1 gFID IS Pre. Rec. 1.91 6.18 2.34 2.51 1.76 2. 324.3 182.1 253.9 216.7 294.8 274.5 - 0.80 0.81 0.82 0.80 0.79 - 0.51 0.60 0.57 0.63 0. 5."
        },
        {
            "title": "IMAGE GENERATION WITH DISCRETE TOKENS",
            "content": "To evaluate ATOKEN-So/Ds generative capabilities, we integrate it into the TokenBridge (Wang et al., 2025) autoregressive framework, replacing only the tokenizer while maintaining all other settings. The key architectural difference lies in token representation: TokenBridge uses 16 dimensions with 8-level vocabularies, while ATOKEN-So/D uses 8 dimensions with 4096-level vocabulariesa more challenging configuration that requires modeling larger discrete spaces. Additionally, TokenBridge employs FFT-based dimension ordering to generate low-frequency structure first, whereas our model uses sequential generation. Following TokenBridges evaluation protocol, we sample 50,000 images with CFG scale 3.1. As shown in Table 12, ATOKEN-So/D achieves gFID of 2.23, demonstrating competitive performance against specialized discrete tokenizers including LFQ (Yu et al., 2023b), TikTok-L (Yu et al., 2024a), VQGAN (Esser et al., 2020), UniTok (Ma et al., 2025b), and TokenBridge (Wang et al., 2025). While TokenBridge achieves lower gFID (1.76), this gap is expected given our larger vocabulary size (4096 vs. 8) and lack of frequency-based ordering optimization. Notably, we outperform UniTok (2.51 gFID), the only other unified visual tokenizer, demonstrating that multimodal capabilities need not compromise generation quality."
        },
        {
            "title": "5.4 TEXT TO VIDEO GENERATION",
            "content": "To assess the text-to-video (T2V) capabilities of the ATOKEN-So/C tokenizers, we integrate them into video generation model. Our model is built upon the MMDiT backbone (Esser et al., 2024) and incorporates design elements from recent video architectures (Wan et al., 2025; Kong et al., 2024; Peng et al., 2025). Due to computational constraints, we conduct experiments with smaller models and limited training data, maintaining consistent settings across all tokenizers for fair comparison. Following standard two-stage training approach, we first pretrain the model from scratch on textto-image (T2I) tasks with each tokenizer. We then adapt this image model for video generation, enabling evaluation on both T2I and T2V benchmarks. To provide fair and efficient basis for comparing tokenizers, all training is conducted at low resolutions, using 256256 for images and 192336 for videos. For T2I evaluation, we report CLIP-Score (Hessel et al., 2021), Pick-Score (Kirstain et al., 2023), and GenEval (Ghosh et al., 2023). For T2V tasks, we evaluate performance using the VBench benchmark (Huang et al., 2024). We compare our results against state-of-the-art video tokenizers, namely Cosmos (Agarwal et al., 2025), Hunyuan (Kong et al., 2024), and Wan (Wan et al., 2025). To ensure fair comparison, we normalize the effective token budget for video generation across all tokenizers by adjusting the patch size. For example, we use patch size of 22 for 88 spatial compression and 11 for 1616 compression. Additionally, for T2V generation, we adjust the classifier free guidance (CFG) scale to account for differences in channel size, using scale of 9.0 for channel size of 48 and 4.5 for channel size of 16. As shown in Table 13, our ATOKEN-So/C tokenizers achieve results comparable to specialized video-optimized tokenizers across all metrics, outperforming Cosmos and matching the performance of Hunyuan and Wan, even though ours are designed for broader range of tasks."
        },
        {
            "title": "Preprint",
            "content": "Table 13: Text-to-image and text-to-video generation benchmarks. We compare ATOKEN Stages 2-3 with specialized video tokenizers (Cosmos, Hunyuan, Wan) under resource-constrained settings. Higher scores indicate better performance across all metrics. All models trained with identical data and model sizes for fair comparison. Tokenizer Cosmos-0.1-CV488 Hunyuan Wan2.1 ATOKEN-So/C Stage 2 Stage 3 Comp. Ratio (4, 8, 8) (4, 8, 8) (4, 8, 8) (4,16,16) (4,16,16) Latent Size Patch Size 16 16 16 48 2 2 2 1 1 T2I T2V: VBench CLIP Pick GenEval Quality Semantic Total 32.16 32.49 32.45 21.47 21.66 21.62 62.14% 77.27% 66.11% 79.52% 65.57% 79.74% 65.13% 74.84% 72.03% 78.02% 74.01% 78.60% 32.44 32.50 21.59 21.74 63.08% 79.30% 64.61% 79.82% 72.42% 77.92% 73.04% 78.46% 5.5 IMAGE TO 3D SYNTHESIS To validate the utility of our learned discrete tokens for downstream generative tasks, we train an image-to-3D synthesis model. Following the methodology of Trellis-SLAT (Xiang et al., 2024), we adopt their diffusion model architecture and training regimen. We replace their original 3D tokens with the tokens generated by our ATOKEN-So/C. For fair comparison, all inference hyperparameters, such as the number of diffusion steps and classifier-free guidance scale, are kept identical to those reported in the original work. As shown in Figure 14, our approach successfully generates 3D assets from single conditioning images, demonstrating that our tokens are suitable for complex generative modeling. However, we observe that the performance does not yet match the fidelity of the original Trellis-SLAT model. Specifically, while our tokenizer demonstrates excellent reconstruction capabilities that preserve color and structure (as in Figure 11), the generative model sometimes struggles to maintain this consistency. The generated assets do not always adhere strictly to the color and style of the input image. We hypothesize that this discrepancy arises from the significantly larger latent channel dimension of our tokenizer. ATOKEN-So/C uses 48 latent channels to accommodate rich multimodal information, substantial increase from the 8 channels used in Trellis-SLAT. diffusion model operating in this higher-dimensional space likely requires further optimization of training and inference hyperparameters (e.g., conditioning strength, diffusion schedule) to leverage the conditioning signal fully. We leave the exploration of these optimizations as promising direction for future work."
        },
        {
            "title": "6 RELATED WORK",
            "content": "Reconstruction Tokenizers. High-resolution images have been compressed using deep autoencoders (Hinton et al., 2012; Vincent et al., 2008), which learn lower-dimensional latent representations for reconstruction. VAEs (Kingma & Welling, 2013) extended this framework with probabilistic modeling, while VQ-VAE (Van Den Oord et al., 2017) introduced vector quantization to discretize the latent space. Building on these foundations, subsequent works enhanced reconstruction quality through adversarial training (Rombach et al., 2022; Esser et al., 2020), developed alternative quantization strategies (Lee et al., 2022; Mentzer et al., 2023; Luo et al., 2024b; Zheng et al., 2022), incorporated semantic guidance (Li et al., 2024b;c; Yao & Wang, 2025; Zha et al., 2024; Chen et al., 2024a; 2025; Kim et al., 2025), and scaled model capacity (Xiong et al., 2025). Video tokenization extended these image-based methods to temporal domains, employing 3D convolutions (Yan et al., 2021; Ge et al., 2022; Yu et al., 2023a), decoupled spatial-temporal processing (Polyak et al., 2024), and causal modeling (Kong et al., 2024; Wan et al., 2025; Yang et al., 2024). Beyond convolutional architectures, recent work has explored Vision Transformers (Dosovitskiy et al., 2020) as an alternative backbone for both image (Yu et al., 2021; 2024a; Hansen-Estruch et al., 2025) and video (Villegas et al., 2022; Wang et al., 2024b;a; Yan et al., 2024) tokenization. 3D generation methods initially applied diffusion models directly to various 3D representations (Luo & Hu, 2021; Hui et al., 2022; Shue et al., 2023; Wang et al., 2023; He et al., 2024), then shifted toward compact latent spaces for improved efficiency (Gupta et al., 2023; Xiong et al., 2024; Jun & Nichol, 2023; Lan et al., 2024; Nichol et al., 2022). Notably, Trellis (Xiang et al., 2024) introduces"
        },
        {
            "title": "Preprint",
            "content": "Figure 12: ImageNet Generation Samples Using Continuous Token. Images are generated with LightiningDiT (Yao & Wang, 2025) and ATOKEN-So/C. Figure 13: ImageNet Generation Samples Using Discrete Token. Images are generated with TokenBridgeL (Wang et al., 2025) and ATOKEN-So/D. Figure 14: Image-to-3D Generation Visualization on Toys4k."
        },
        {
            "title": "Preprint",
            "content": "structured latents (SLAT) that jointly encode geometry and appearance on sparse 3D grids, enabling flexible decoding to multiple output formats. Visual Encoders. Image encoders initially leveraged contrastive learning through vision-language alignment (Radford et al., 2021; Jia et al., 2021; Zhai et al., 2023) and image-only self-supervision (Chen et al., 2020; Oquab et al., 2023). Generative pretraining explored text generation objectives (Wang et al., 2021), discrete token reconstruction (Bao et al., 2021), and masked image modeling (He et al., 2022; Carreira et al., 2024). Methods like NaViT (Dehghani et al., 2023) introduced resolution flexibility with preserved aspect ratios. Recent unified approaches merge contrastive, generative, and self-supervised objectives (Yu et al., 2022a; Tschannen et al., 2025) or leverage intermediate-layer features with task-specific alignment (Bolya et al., 2025). Video encoders primarily employ self-supervised learning on video-only data (Qian et al., 2021; Feichtenhofer et al., 2021; Recasens et al., 2021; Qian et al., 2022; Tong et al., 2022) or videolanguage modeling with noisy text supervision (Fu et al., 2021; Zellers et al., 2022; Li et al., 2022; Huang et al., 2022; Chen et al., 2023). Recent methods treat video as image sequences, focusing on context window expansion (Team et al., 2024; Xue et al., 2024a) or token compression (Li et al., 2023; Song et al., 2023; Fei et al., 2024; Weng et al., 2024; Xu et al., 2024). Unified Tokenizers & Multimodal Models. Unified Multimodal Models aim to combine visual understanding and generation within single framework (Wang et al., 2022a; Mizrahi et al., 2023; Lu et al., 2024a). Many approaches use decoupled tokenizers while employing various generation paradigms autoregressive (Lu et al., 2022a; Team & Kahn, 2024; Wu et al., 2024a), diffusion (Zhou et al., 2024a), flow-matching (Ma et al., 2024b), and masked prediction (Xie et al., 2024; Tian et al., 2025). Recent efforts on unified tokenizers that handle both tasks include VILA-U (Wu et al., 2024c), which combines pixel reconstruction with contrastive learning in single vision tower; SeTok (Wu et al., 2024b), which groups visual features into semantic units; UniTok (Ma et al., 2025a), which uses multi-codebook quantization for enhanced expressiveness; and UniToken (Jiao et al., 2025), which produces hybrid discrete-continuous representations through dual encoders. Show-o2 (Xie et al., 2025) extends these approaches by leveraging 3D causal VAE space with dual-path spatial-temporal fusion, enabling scalability across both image and video modalities while combining autoregressive modeling with flow matching."
        },
        {
            "title": "7 DISCUSSION AND CONCLUSION",
            "content": "The effectiveness of ATOKEN across diverse modalities and tasks suggests new opportunities: visual tokenization can achieve the same unification that transformed language modeling. Our single framework achieves both high-fidelity reconstruction and semantic understanding across images, videos, and 3D assets. This integration became possible through the combination of our sparse 4D representation, transformer-based architecture, adversarial-free training strategy, and progressive multimodal curriculum. Due to limited computational resources, we could only test ATOKENon separate downstream tasks. Building the comprehensive omnimodel that would demonstrate ATOKENs full potential remains as future work. Looking forward, ATOKEN opens paths for visual foundation models to follow language modelings trajectory toward true generalization. We hope this work sheds light on the next-generation multimodal AI systems built upon unified visual tokenization."
        },
        {
            "title": "A CONTRIBUTIONS",
            "content": "Jiasen designed the main concept and project scope, developed the unified representation, main architecture, native resolution training, distill-based semantic loss, stage-wise round robin training strategy, discrete quantization, and KV-cache video decoding, GAN training recipe etc. Curated the image and video dataset, trained the model, conducted in-training evaluation, and wrote the paper. Liangchen oversaw engineering aspects for the project, developed the sparse transformer structure, adversarial-free training loss recipe, video reconstruction loss recipe, and 3D tokenizer pipeline and dataset. Evaluated image reconstruction and understanding (Section 4.2), 3D reconstruction and understanding (Section 4.4), ran image generation with continuous tokens (Section 5.2) and text-to-3D synthesis (Section 5.5), and contributed to writing the paper. Mingze contributed to the video understanding dataset design and suggested video understanding encoding settings. Ran"
        },
        {
            "title": "Preprint",
            "content": "all Multimodal LLM experiments and wrote the corresponding section (Section 5.1). Byeongjoo contributed to the discussion of GAN settings and video reconstruction frame sampling strategy. Evaluated video reconstruction (Section 4.3) and ran text-to-video generation experiments and wrote the corresponding section (Section 5.4). Yanjun evaluated video retrieval (Section 4.3), ran image generation experiments with discrete tokens, and wrote the corresponding section (Section 5.3). Chen contributed to discussions on image understanding and image generation with continuous tokens. Afshin advised on research direction and helped manage compute resources. Yinfei advised on research direction, provided feedback through regular discussions, and helped manage computing resources."
        },
        {
            "title": "REFERENCES",
            "content": "Marah Abdin, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harkirat Behl, et al. Phi-3 technical report: highly capable language model locally on your phone. arXiv:2404.14219, 2024. Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv:2303.08774, 2023. Panos Achlioptas, Olga Diamanti, Ioannis Mitliagkas, and Leonidas Guibas. Learning representations and generative models for 3d point clouds. In International conference on machine learning, pp. 4049. PMLR, 2018. Niket Agarwal, Arslan Ali, Maciej Bala, Yogesh Balaji, Erik Barker, Tiffany Cai, Prithvijit Chattopadhyay, Yongxin Chen, Yin Cui, Yifan Ding, et al. Cosmos world foundation model platform for physical ai. arXiv:2501.03575, 2025. Max Bain, Arsha Nagrani, Gül Varol, and Andrew Zisserman. Frozen in time: joint video and image encoder for end-to-end retrieval. In IEEE International Conference on Computer Vision, 2021. Hangbo Bao, Li Dong, Songhao Piao, and Furu Wei. Beit: Bert pre-training of image transformers. arXiv:2106.08254, 2021. Daniel Bolya, Po-Yao Huang, Peize Sun, Jang Hyun Cho, Andrea Madotto, Chen Wei, Tengyu Ma, Jiale Zhi, Jathushan Rajasegaran, Hanoona Rasheed, et al. Perception encoder: The best visual embeddings are not at the output of the network. arXiv:2504.13181, 2025. João Carreira, Dilara Gokay, Michael King, Chuhan Zhang, Ignacio Rocco, Aravindh Mahendran, Thomas Albert Keck, Joseph Heyward, Skanda Koppula, Etienne Pot, et al. Scaling 4d representations. arXiv:2412.15212, 2024. David Chen and William Dolan. Collecting highly parallel data for paraphrase evaluation. In Proceedings of the 49th annual meeting of the association for computational linguistics: human language technologies, pp. 190200, 2011. Guo Chen, Yin-Dong Zheng, Jiahao Wang, Jilan Xu, Yifei Huang, Junting Pan, Yi Wang, Yali Wang, Y. Qiao, Tong Lu, and Limin Wang. Videollm: Modeling video sequence with large language models. ArXiv, abs/2305.13292, 2023. Hao Chen, Ze Wang, Xiang Li, Ximeng Sun, Fangyi Chen, Jiang Liu, Jindong Wang, Bhiksha Raj, Zicheng Liu, and Emad Barsoum. Softvq-vae: Efficient 1-dimensional continuous tokenizer. ArXiv, abs/2412.10958, 2024a. Hao Chen, Yujin Han, Fangyi Chen, Xiang Li, Yidong Wang, Jindong Wang, Ze Wang, Zicheng Liu, Difan Zou, and Bhiksha Raj. Masked autoencoders are effective tokenizers for diffusion models. ArXiv, abs/2502.03444, 2025. Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. simple framework for contrastive learning of visual representations. In International conference on machine learning, pp. 15971607. PmLR, 2020. Tsai-Shien Chen, Aliaksandr Siarohin, Willi Menapace, Ekaterina Deyneka, Hsiang-wei Chao, Byung Eun Jeon, Yuwei Fang, Hsin-Ying Lee, Jian Ren, Ming-Hsuan Yang, et al. Panda-70m: Captioning 70m videos with multiple cross-modality teachers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1332013331, 2024b."
        },
        {
            "title": "Preprint",
            "content": "Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. Journal of Machine Learning Research, 24(240):1113, 2023. Mostafa Dehghani, Basil Mustafa, Josip Djolonga, Jonathan Heek, Matthias Minderer, Mathilde Caron, Andreas Steiner, Joan Puigcerver, Robert Geirhos, Ibrahim Alabdulmohsin, et al. Patch npack: Navit, vision transformer for any aspect ratio and resolution. Advances in Neural Information Processing Systems, 36:22522274, 2023. Matt Deitke, Ruoshi Liu, Matthew Wallingford, Huong Ngo, Oscar Michel, Aditya Kusupati, Alan Fan, Christian Laforte, Vikram Voleti, Samir Yitzhak Gadre, et al. Objaverse-xl: universe of 10m+ 3d objects. Advances in Neural Information Processing Systems, 36:3579935813, 2023. Matt Deitke, Christopher Clark, Sangho Lee, Rohun Tripathi, Yue Yang, Jae Sung Park, Mohammadreza Salehi, Niklas Muennighoff, Kyle Lo, Luca Soldaini, et al. Molmo and pixmo: Open weights and open data for state-of-the-art multimodal models. arXiv:2409.17146, 2024. Chaorui Deng, Deyao Zhu, Kunchang Li, Chenhui Gou, Feng Li, Zeyu Wang, Shu Zhong, Weihao Yu, Xiaonan Nie, Ziang Song, Guang Shi, and Haoqi Fan. Emerging properties in unified multimodal pretraining. arXiv:2505.14683, 2025. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pp. 248255. Ieee, 2009. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv:2010.11929, 2020. Patrick Esser, Robin Rombach, and Björn Ommer. Taming transformers for high-resolution image synthesis, 2020. Patrick Esser, Sumith Kulal, A. Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, Dustin Podell, Tim Dockhorn, Zion English, Kyle Lacey, Alex Goodwin, Yannik Marek, and Robin Rombach. Scaling rectified flow transformers for high-resolution image synthesis. ArXiv, abs/2403.03206, 2024. Alex Fang, Albin Madappally Jose, Amit Jain, Ludwig Schmidt, Alexander Toshev, and Vaishaal Shankar. Data filtering networks. ArXiv, abs/2309.17425, 2023. Jiajun Fei, Dian Li, Zhidong Deng, Zekun Wang, Gang Liu, and Hui Wang. Video-ccam: Enhancing videolanguage understanding with causal cross-attention masks for short and long videos. ArXiv, abs/2408.14023, 2024. Christoph Feichtenhofer, Haoqi Fan, Bo Xiong, Ross Girshick, and Kaiming He. large-scale study on unsupervised spatiotemporal representation learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 32993309, 2021. Chaoyou Fu, Yuhan Dai, Yondong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, et al. Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis. arXiv:2405.21075, 2024. Tsu-Jui Fu, Linjie Li, Zhe Gan, Kevin Lin, William Yang Wang, Lijuan Wang, and Zicheng Liu. Violet : End-to-end video-language transformers with masked visual-token modeling. ArXiv, abs/2111.12681, 2021. Lishuai Gao, Yujie Zhong, Yingsen Zeng, Haoxian Tan, Dengjie Li, and Zheng Zhao. Linvt: Empower your image-level large language model to understand videos. arXiv:2412.05185, 2024. Leon A. Gatys, Alexander S. Ecker, and Matthias Bethge. Image style transfer using convolutional neural networks. 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 24142423, 2016. Songwei Ge, Thomas Hayes, Harry Yang, Xi Yin, Guan Pang, David Jacobs, Jia-Bin Huang, and Devi Parikh. Long video generation with time-agnostic vqgan and time-sensitive transformer. In European Conference on Computer Vision, pp. 102118. Springer, 2022. Dhruba Ghosh, Hannaneh Hajishirzi, and Ludwig Schmidt. Geneval: An object-focused framework for evaluating text-to-image alignment. NeurIPS, 2023."
        },
        {
            "title": "Preprint",
            "content": "Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. Advances in neural information processing systems, 27, 2014. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv:2501.12948, 2025. Anchit Gupta, Wenhan Xiong, Yixin Nie, Ian Jones, and Barlas Oguz. 3dgen: Triplane latent diffusion for textured mesh generation. arXiv:2303.05371, 2023. Philippe Hansen-Estruch, David Yan, Ching-Yao Chung, Orr Zohar, Jialiang Wang, Tingbo Hou, Tao Xu, Sriram Vishwanath, Peter Vajda, and Xinlei Chen. Learnings from scaling visual tokenizers for reconstruction and generation. arXiv:2501.09755, 2025. Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick. Masked autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1600016009, 2022. Xianglong He, Junyi Chen, Sida Peng, Di Huang, Yangguang Li, Xiaoshui Huang, Chun Yuan, Wanli Ouyang, and Tong He. Gvgen: Text-to-3d generation with volumetric representation. In European Conference on Computer Vision, pp. 463479. Springer, 2024. Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. Clipscore: reference-free evaluation metric for image captioning. arXiv:2104.08718, 2021. Geoffrey Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Improving neural networks by preventing co-adaptation of feature detectors. arXiv:1207.0580, 2012. Jingjia Huang, Yinan Li, Jiashi Feng, Xiaoshuai Sun, and Rongrong Ji. Clover: Towards unified videolanguage alignment and fusion model. 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1485614866, 2022. Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, et al. Vbench: Comprehensive benchmark suite for video generative models. In CVPR, 2024. Ka-Hei Hui, Ruihui Li, Jingyu Hu, and Chi-Wing Fu. Neural wavelet-domain diffusion for 3d shape generation. In SIGGRAPH Asia 2022 conference papers, pp. 19, 2022. Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with noisy text supervision. In International conference on machine learning, pp. 49044916. PMLR, 2021. Yang Jiao, Haibo Qiu, Zequn Jie, Shaoxiang Chen, Jingjing Chen, Lin Ma, and Yu-Gang Jiang. Unitoken: Harmonizing multimodal understanding and generation through unified visual encoding. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 36003610, 2025. Heewoo Jun and Alex Nichol. Shap-e: Generating conditional 3d implicit functions. arXiv:2305.02463, 2023. Aniruddha Kembhavi, Mike Salvato, Eric Kolve, Minjoon Seo, Hannaneh Hajishirzi, and Ali Farhadi. diagram is worth dozen images. In ECCV, 2016. Bernhard Kerbl, Georgios Kopanas, Thomas Leimkühler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM Trans. Graph., 42(4):1391, 2023. Dongwon Kim, Ju He, Qihang Yu, Chenglin Yang, Xiaohui Shen, Suha Kwak, and Liang-Chieh Chen. Democratizing text-to-image masked generative models with compact text-aware one-dimensional tokens. ArXiv, abs/2501.07730, 2025. Diederik Kingma and Max Welling. Auto-encoding variational bayes. arXiv:1312.6114, 2013. Yuval Kirstain, Adam Polyak, Uriel Singer, Shahbuland Matiana, Joe Penna, and Omer Levy. Pick-a-pic: An open dataset of user preferences for text-to-image generation. NeurIPS, 2023. Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. Hunyuanvideo: systematic framework for large video generative models. arXiv:2412.03603, 2024."
        },
        {
            "title": "Preprint",
            "content": "Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Uijlings, Ivan Krasin, Jordi Pont-Tuset, Shahab Kamali, Stefan Popov, Matteo Malloci, Alexander Kolesnikov, Tom Duerig, and Vittorio Ferrari. The open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale. IJCV, 2020. Black Forest Labs, Stephen Batifol, Andreas Blattmann, Frederic Boesel, Saksham Consul, Cyril Diagne, Tim Dockhorn, Jack English, Zion English, Patrick Esser, Sumith Kulal, Kyle Lacey, Yam Levi, Cheng Li, Dominik Lorenz, Jonas Müller, Dustin Podell, Robin Rombach, Harry Saini, Axel Sauer, and Luke Smith. Flux.1 kontext: Flow matching for in-context image generation and editing in latent space, 2025. Yushi Lan, Fangzhou Hong, Shuai Yang, Shangchen Zhou, Xuyi Meng, Bo Dai, Xingang Pan, and Chen Change Loy. Ln3diff: Scalable latent neural fields diffusion for speedy 3d generation. In European Conference on Computer Vision, pp. 112130. Springer, 2024. Doyup Lee, Chiheon Kim, Saehoon Kim, Minsu Cho, and Wook-Shin Han. Autoregressive image generation using residual quantization. 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1151311522, 2022. Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, et al. Llava-onevision: Easy visual task transfer. arXiv:2408.03326, 2024a. Linjie Li, Zhe Gan, Kevin Lin, Chung-Ching Lin, Zicheng Liu, Ce Liu, and Lijuan Wang. Lavender: Unifying video-language understanding as masked language modeling. 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 2311923129, 2022. Xiang Li, Kai Qiu, Hao Chen, Jason Kuen, Jiuxiang Gu, Bhiksha Raj, and Zhe Lin. Imagefolder: Autoregressive image generation with folded tokens. ArXiv, abs/2410.01756, 2024b. Xiang Li, Kai Qiu, Hao Chen, Jason Kuen, Jiuxiang Gu, Jindong Wang, Zhe Lin, and Bhiksha Raj. Xq-gan: An open-source image tokenization framework for autoregressive generation. ArXiv, abs/2412.01762, 2024c. Yanwei Li, Chengyao Wang, and Jiaya Jia. Llama-vid: An image is worth 2 tokens in large language models. In European Conference on Computer Vision, 2023. Yuliang Liu, Zhang Li, Mingxin Huang, Biao Yang, Wenwen Yu, Chunyuan Li, Xu-Cheng Yin, Cheng-Lin Liu, Lianwen Jin, and Xiang Bai. Ocrbench: on the hidden mystery of ocr in large multimodal models. Science China Information Sciences, 2024a. Zuyan Liu, Yuhao Dong, Ziwei Liu, Winston Hu, Jiwen Lu, and Yongming Rao. Oryx mllm: On-demand spatial-temporal understanding at arbitrary resolution. arXiv:2409.12961, 2024b. Jiasen Lu, Christopher Clark, Rowan Zellers, Roozbeh Mottaghi, and Aniruddha Kembhavi. Unified-io: unified model for vision, language, and multi-modal tasks. ArXiv, abs/2206.08916, 2022a. Jiasen Lu, Christopher Clark, Sangho Lee, Zichen Zhang, Savya Khosla, Ryan Marten, Derek Hoiem, and Aniruddha Kembhavi. Unified-io 2: Scaling autoregressive multimodal models with vision language audio and action. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2643926455, 2024a. Pan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering. In NeurIPS, 2022b. Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. In ICLR, 2024b. Huaishao Luo, Lei Ji, Ming Zhong, Yang Chen, Wen Lei, Nan Duan, and Tianrui Li. CLIP4Clip: An empirical study of clip for end to end video clip retrieval. arXiv:2104.08860, 2021. Shitong Luo and Wei Hu. Diffusion probabilistic models for 3d point cloud generation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 28372845, 2021. Tiange Luo, Justin Johnson, and Honglak Lee. View selection for 3d captioning via diffusion ranking. In European Conference on Computer Vision, pp. 180197. Springer, 2024a. Zhuoyan Luo, Fengyuan Shi, Yixiao Ge, Yujiu Yang, Limin Wang, and Ying Shan. Open-magvit2: An opensource project toward democratizing auto-regressive visual generation. ArXiv, abs/2409.04410, 2024b."
        },
        {
            "title": "Preprint",
            "content": "Chuofan Ma, Yi Jiang, Junfeng Wu, Jihan Yang, Xin Yu, Zehuan Yuan, Bingyue Peng, and Xiaojuan Qi. Unitok: unified tokenizer for visual generation and understanding. arXiv:2502.20321, 2025a. Chuofan Ma, Yi Jiang, Junfeng Wu, Jihan Yang, Xin Yu, Zehuan Yuan, Bingyue Peng, and Xiaojuan Qi. Unitok: unified tokenizer for visual generation and understanding. arXiv:2502.20321, 2025b. Nanye Ma, Mark Goldstein, Michael Albergo, Nicholas Boffi, Eric Vanden-Eijnden, and Saining Xie. Sit: Exploring flow and diffusion-based generative models with scalable interpolant transformers. In European Conference on Computer Vision, pp. 2340. Springer, 2024a. Yiyang Ma, Xingchao Liu, Xi aokang Chen, Wen Liu, Chengyue Wu, Zhiyu Wu, Zizheng Pan, Zhenda Xie, Haowei Zhang, Xingkai Yu, Liang Zhao, Yisong Wang, Jiaying Liu, and Chong Ruan. Janusflow: Harmonizing autoregression and rectified flow for unified multimodal understanding and generation. In Computer Vision and Pattern Recognition, 2024b. Fabian Mentzer, David C. Minnen, Eirikur Agustsson, and Michael Tschannen. Finite scalar quantization: Vq-vae made simple. ArXiv, abs/2309.15505, 2023. Lars Mescheder, Michael Oechsle, Michael Niemeyer, Sebastian Nowozin, and Andreas Geiger. Occupancy networks: Learning 3d reconstruction in function space. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 44604470, 2019. Ben Mildenhall, Pratul Srinivasan, Matthew Tancik, Jonathan Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. Communications of the ACM, 65(1): 99106, 2021. David Mizrahi, Roman Bachmann, Ouguzhan Fatih Kar, Teresa Yeo, Mingfei Gao, Afshin Dehghan, and Amir Zamir. 4m: Massively multimodal masked modeling. ArXiv, abs/2312.06647, 2023. Alex Nichol, Heewoo Jun, Prafulla Dhariwal, Pamela Mishkin, and Mark Chen. Point-e: system for generating 3d point clouds from complex prompts. arXiv:2212.08751, 2022. Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv:2304.07193, 2023. William Peebles and Saining Xie. Scalable diffusion models with transformers. arXiv preprint arXiv:2212.09748, 2022. Xiangyu Peng, Zangwei Zheng, Chenhui Shen, Tom Young, Xinying Guo, Binluo Wang, Hang Xu, Hongxin Liu, Mingyan Jiang, Wenjun Li, et al. Open-sora 2.0: Training commercial-level video generation model in $200 k. arXiv:2503.09642, 2025. Adam Polyak, Amit Zohar, Andrew Brown, Andros Tjandra, Animesh Sinha, Ann Lee, Apoorv Vyas, Bowen Shi, Chih-Yao Ma, Ching-Yao Chuang, et al. Movie gen: cast of media foundation models. arXiv:2410.13720, 2024. Jordi Pont-Tuset, Federico Perazzi, Sergi Caelles, Pablo Arbeláez, Alexander Sorkine-Hornung, and Luc Van Gool. The 2017 davis challenge on video object segmentation. ArXiv, abs/1704.00675, 2017. Viorica Patraucean, Lucas Smaira, Ankush Gupta, Adrià Recasens Continente, Larisa Markeeva, Dylan Banarse, Skanda Koppula, Joseph Heyward, Mateusz Malinowski, Yi Yang, Carl Doersch, Tatiana Matejovicova, Yury Sulsky, Antoine Miech, Alex Frechette, Hanna Klimczak, Raphael Koster, Junlin Zhang, Stephanie Winkler, Yusuf Aytar, Simon Osindero, Dima Damen, Andrew Zisserman, and João Carreira. Perception test: diagnostic benchmark for multimodal video models. In NeurIPS, 2023. Rui Qian, Tianjian Meng, Boqing Gong, Ming-Hsuan Yang, Huisheng Wang, Serge Belongie, and Yin Cui. Spatiotemporal contrastive video representation learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 69646974, 2021. Rui Qian, Yeqing Li, Liangzhe Yuan, Boqing Gong, Ting Liu, Matthew Brown, Serge J. Belongie, Ming-Hsuan Yang, Hartwig Adam, and Yin Cui. On temporal granularity in self-supervised video representation learning. In British Machine Vision Conference, 2022. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pp. 87488763. PmLR, 2021."
        },
        {
            "title": "Preprint",
            "content": "Adrià Recasens, Pauline Luc, Jean-Baptiste Alayrac, Luyu Wang, Florian Strub, Corentin Tallec, Mateusz Malinowski, Viorica Patraucean, Florent Altche, Michael Valko, Jean-Bastien Grill, Aäron van den Oord, and Andrew Zisserman. Broaden your views for self-supervised video learning. 2021 IEEE/CVF International Conference on Computer Vision (ICCV), pp. 12351245, 2021. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution In Proceedings of the IEEE/CVF conference on computer image synthesis with latent diffusion models. vision and pattern recognition, pp. 1068410695, 2022. Team Seawead, Ceyuan Yang, Zhijie Lin, Yang Zhao, Shanchuan Lin, Zhibei Ma, Haoyuan Guo, Hao Chen, Lu Qi, Sen Wang, et al. Seaweed-7b: Cost-effective training of video generation foundation model. arXiv:2504.08685, 2025. Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with subword units. arXiv:1508.07909, 2015. Ryan Shue, Eric Ryan Chan, Ryan Po, Zachary Ankner, Jiajun Wu, and Gordon Wetzstein. 3d neural field generation using triplane diffusion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2087520886, 2023. Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and Marcus Rohrbach. Towards vqa models that can read. In CVPR, 2019. Enxin Song, Wenhao Chai, Guanhong Wang, Yucheng Zhang, Haoyang Zhou, Feiyang Wu, Xun Guo, Tianbo Ye, Yang Lu, Jenq-Neng Hwang, and Gaoang Wang. Moviechat: From dense token to sparse memory for long video understanding. 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1822118232, 2023. Stefan Stojanov, Anh Thai, and James Rehg. Using shape to categorize: Low-shot learning with an explicit shape bias. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 17981808, 2021a. Stefan Stojanov, Anh Thai, and James M. Rehg. Using shape to categorize: Low-shot learning with an explicit shape bias. 2021b. Quan Sun, Yuxin Fang, Ledell Wu, Xinlong Wang, and Yue Cao. Eva-clip: Improved training techniques for clip at scale. arXiv:2303.15389, 2023. Chameleon Team and Jacob Kahn. Chameleon: Mixed-modal early-fusion foundation models. ArXiv, abs/2405.09818, 2024. Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, et al. Gemini: family of highly capable multimodal models. arXiv:2312.11805, 2023. Gemini Team, Petko Georgiev, Ving Ian Lei, Ryan Burnell, Libin Bai, Anmol Gulati, Garrett Tanzer, Damien Vincent, Zhufeng Pan, Shibo Wang, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv:2403.05530, 2024. Rui Tian, Mingfei Gao, Mingze Xu, Jiaming Hu, Jiasen Lu, Zuxuan Wu, Yinfei Yang, and Afshin Dehghan. Unigen: Enhanced training & test-time strategies for unified multimodal understanding and generation. arXiv:2505.14682, 2025. Zhan Tong, Yibing Song, Jue Wang, and Limin Wang. Videomae: Masked autoencoders are data-efficient learners for self-supervised video pre-training. ArXiv, abs/2203.12602, 2022. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models. ArXiv, abs/2302.13971, 2023. Michael Tschannen, Alexey Gritsenko, Xiao Wang, Muhammad Ferjad Naeem, Ibrahim Alabdulmohsin, Nikhil Parthasarathy, Talfan Evans, Lucas Beyer, Ye Xia, Basil Mustafa, et al. Siglip 2: Multilingual vision-language encoders with improved semantic understanding, localization, and dense features. arXiv:2502.14786, 2025. Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in neural information processing systems, 30, 2017."
        },
        {
            "title": "Preprint",
            "content": "Ruben Villegas, Mohammad Babaeizadeh, Pieter-Jan Kindermans, Hernan Moraldo, Han Zhang, Mohammad Taghi Saffar, Santiago Castro, Julius Kunze, and Dumitru Erhan. Phenaki: Variable length video generation from open domain textual description. arXiv:2210.02399, 2022. Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and Pierre-Antoine Manzagol. Extracting and composing robust features with denoising autoencoders. In Proceedings of the 25th international conference on Machine learning, pp. 10961103, 2008. Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, et al. Wan: Open and advanced large-scale video generative models. arXiv:2503.20314, 2025. Hanyu Wang, Saksham Suri, Yixuan Ren, Hao Chen, and Abhinav Shrivastava. Larp: Tokenizing videos with learned autoregressive generative prior. arXiv:2410.21264, 2024a. Junke Wang, Yi Jiang, Zehuan Yuan, Bingyue Peng, Zuxuan Wu, and Yu-Gang Jiang. Omnitokenizer: joint image-video tokenizer for visual generation. Advances in Neural Information Processing Systems, 37: 2828128295, 2024b. Peng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai, Zhikang Li, Jianxin Ma, Chang Zhou, Jingren Zhou, and Hongxia Yang. Ofa: Unifying architectures, tasks, and modalities through simple sequence-tosequence learning framework. In International Conference on Machine Learning, 2022a. Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv:2409.12191, 2024c. Tengfei Wang, Bo Zhang, Ting Zhang, Shuyang Gu, Jianmin Bao, Tadas Baltrusaitis, Jingjing Shen, Dong Chen, Fang Wen, Qifeng Chen, et al. Rodin: generative model for sculpting 3d digital avatars using In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. diffusion. 45634573, 2023. Weihan Wang, Zehai He, Wenyi Hong, Yean Cheng, Xiaohan Zhang, Ji Qi, Xiaotao Gu, Shiyu Huang, Bin Xu, Yuxiao Dong, et al. Lvbench: An extreme long video understanding benchmark. arXiv:2406.08035, 2024d. Yi Wang, Kunchang Li, Yizhuo Li, Yinan He, Bingkun Huang, Zhiyu Zhao, Hongjie Zhang, Jilan Xu, Yi Liu, Zun Wang, Sen Xing, Guo Chen, Junting Pan, Jiashuo Yu, Yali Wang, Limin Wang, and Yu Qiao. Internvideo: General video foundation models via generative and discriminative learning. ArXiv, abs/2212.03191, 2022b. Yuqing Wang, Zhijie Lin, Yao Teng, Yuanzhi Zhu, Shuhuai Ren, Jiashi Feng, and Xihui Liu. Bridging continuous and discrete tokens for autoregressive visual generation. arXiv:2503.16430, 2025. Zirui Wang, Jiahui Yu, Adams Wei Yu, Zihang Dai, Yulia Tsvetkov, and Yuan Cao. Simvlm: Simple visual language model pretraining with weak supervision. arXiv:2108.10904, 2021. Yuetian Weng, Mingfei Han, Haoyu He, Xiaojun Chang, and Bohan Zhuang. Longvlm: Efficient long video understanding via large language models. In European Conference on Computer Vision, 2024. Chenfei Wu, Jiahao Li, Jingren Zhou, Junyang Lin, Kaiyuan Gao, Kun Yan, Sheng-ming Yin, Shuai Bai, Xiao Xu, Yilei Chen, et al. Qwen-image technical report, 2025a. Chengyue Wu, Xi aokang Chen, Zhiyu Wu, Yiyang Ma, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, Chong Ruan, and Ping Luo. Janus: Decoupling visual encoding for unified multimodal understanding and generation. ArXiv, abs/2410.13848, 2024a. Haoning Wu, Dongxu Li, Bei Chen, and Junnan Li. Longvideobench: benchmark for long-context interleaved video-language understanding. NeurIPS, 2025b. Shengqiong Wu, Hao Fei, Xiangtai Li, Jiayi Ji, Hanwang Zhang, Tat-Seng Chua, and Shuicheng Yan. Towards semantic equivalence of tokenization in multimodal llm. arXiv:2406.05127, 2024b. Weijia Wu, Yuzhong Zhao, Zhuang Li, Jiahong Li, Hong Zhou, Mike Zheng Shou, and Xiang Bai. large cross-modal video retrieval dataset with reading comprehension. Pattern Recognition, 157:110818, 2025c. Yecheng Wu, Zhuoyang Zhang, Junyu Chen, Haotian Tang, Dacheng Li, Yunhao Fang, Ligeng Zhu, Enze Xie, Hongxu Yin, Li Yi, et al. Vila-u: unified foundation model integrating visual understanding and generation. arXiv:2409.04429, 2024c."
        },
        {
            "title": "Preprint",
            "content": "Jianfeng Xiang, Zelong Lv, Sicheng Xu, Yu Deng, Ruicheng Wang, Bowen Zhang, Dong Chen, Xin Tong, and Jiaolong Yang. Structured 3d latents for scalable and versatile 3d generation. arXiv:2412.01506, 2024. Junbin Xiao, Xindi Shang, Angela Yao, and Tat-Seng Chua. NExT-QA: Next phase of question-answering to explaining temporal actions. In CVPR, 2021. Jinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang, Weihao Wang, Kevin Qinghong Lin, Yuchao Gu, Zhijie Chen, Zhenheng Yang, and Mike Zheng Shou. Show-o: One single transformer to unify multimodal understanding and generation. ArXiv, abs/2408.12528, 2024. Jinheng Xie, Zhenheng Yang, and Mike Zheng Shou. Show-o2: Improved native unified multimodal models. ArXiv, abs/2506.15564, 2025. Bojun Xiong, Si-Tong Wei, Xin-Yang Zheng, Yan-Pei Cao, Zhouhui Lian, and Peng-Shuai Wang. Octfusion: Octree-based diffusion models for 3d shape generation. arXiv:2408.14732, 2024. Tianwei Xiong, Jun Hao Liew, Zilong Huang, Jiashi Feng, and Xihui Liu. Gigatok: Scaling visual tokenizers to 3 billion parameters for autoregressive image generation. arXiv:2504.08736, 2025. Hu Xu, Saining Xie, Xiaoqing Ellen Tan, Po-Yao (Bernie) Huang, Russell Howes, Vasu Sharma, ShangWen Li, Gargi Ghosh, Luke S. Zettlemoyer, and Christoph Feichtenhofer. Demystifying clip data. ArXiv, abs/2309.16671, 2023. Jun Xu, Tao Mei, Ting Yao, and Yong Rui. Msr-vtt: large video description dataset for bridging video and language. 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 52885296, 2016. Mingze Xu, Mingfei Gao, Zhe Gan, Hong-You Chen, Zhengfeng Lai, Haiming Gang, Kai Kang, and Afshin Dehghan. Slowfast-llava: strong training-free baseline for video large language models. arXiv:2407.15841, 2024. Mingze Xu, Mingfei Gao, Shiyu Li, Jiasen Lu, Zhe Gan, Zhengfeng Lai, Meng Cao, Kai Kang, Yinfei Yang, and Afshin Dehghan. Slowfast-llava-1.5: family of token-efficient video large language models for longform video understanding. arXiv:2503.18943, 2025. Fuzhao Xue, Yukang Chen, Dacheng Li, Qinghao Hu, Ligeng Zhu, Xiuyu Li, Yunhao Fang, Haotian Tang, Shang Yang, Zhijian Liu, Ethan He, Hongxu Yin, Pavlo Molchanov, Jan Kautz, Linxi Fan, Yuke Zhu, Yao Lu, and Song Han. Longvila: Scaling long-context visual language models for long videos. ArXiv, abs/2408.10188, 2024a. Le Xue, Manli Shu, Anas Awadalla, Jun Wang, An Yan, Senthil Purushwalkam, Honglu Zhou, Viraj Prabhu, Yutong Dai, Michael Ryoo, et al. xgen-mm (blip-3): family of open large multimodal models. arXiv:2408.08872, 2024b. Wilson Yan, Yunzhi Zhang, Pieter Abbeel, and Aravind Srinivas. Videogpt: Video generation using vq-vae and transformers. arXiv:2104.10157, 2021. Wilson Yan, Matei Zaharia, Volodymyr Mnih, Pieter Abbeel, Aleksandra Faust, and Hao Liu. Elastictok: Adaptive tokenization for image and video. ArXiv, abs/2410.08368, 2024. Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, Da Yin, Xiaotao Gu, Yuxuan Zhang, Weihan Wang, Yean Cheng, Ting Liu, Bin Xu, Yuxiao Dong, and Jie Tang. Cogvideox: Text-to-video diffusion models with an expert transformer. ArXiv, abs/2408.06072, 2024. Jingfeng Yao and Xinggang Wang. Reconstruction vs. generation: Taming optimization dilemma in latent diffusion models. ArXiv, abs/2501.01423, 2025. Jiahui Yu, Xin Li, Jing Yu Koh, Han Zhang, Ruoming Pang, James Qin, Alexander Ku, Yuanzhong Xu, Jason Baldridge, and Yonghui Wu. Vector-quantized image modeling with improved vqgan. arXiv:2110.04627, 2021. Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mojtaba Seyedhosseini, and Yonghui Wu. Coca: Contrastive captioners are image-text foundation models. arXiv:2205.01917, 2022a. Lijun Yu, Yong Cheng, Kihyuk Sohn, José Lezama, Han Zhang, Huiwen Chang, Alexander G. Hauptmann, Ming-Hsuan Yang, Yuan Hao, Irfan Essa, and Lu Jiang. Magvit: Masked generative video transformer. 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1045910469, 2022b."
        },
        {
            "title": "Preprint",
            "content": "Lijun Yu, Yong Cheng, Kihyuk Sohn, José Lezama, Han Zhang, Huiwen Chang, Alexander Hauptmann, Ming-Hsuan Yang, Yuan Hao, Irfan Essa, et al. Magvit: Masked generative video transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1045910469, 2023a. Lijun Yu, José Lezama, Nitesh Bharadwaj Gundavarapu, Luca Versari, Kihyuk Sohn, David C. Minnen, Yong Cheng, Agrim Gupta, Xiuye Gu, Alexander G. Hauptmann, Boqing Gong, Ming-Hsuan Yang, Irfan Essa, David A. Ross, and Lu Jiang. Language model beats diffusion tokenizer is key to visual generation. 2023b. Qihang Yu, Mark Weber, Xueqing Deng, Xiaohui Shen, Daniel Cremers, and Liang-Chieh Chen. An image is worth 32 tokens for reconstruction and generation. Advances in Neural Information Processing Systems, 37: 128940128966, 2024a. Sihyun Yu, Sangkyung Kwak, Huiwon Jang, Jongheon Jeong, Jonathan Huang, Jinwoo Shin, and Saining Xie. Representation alignment for generation: Training diffusion transformers is easier than you think. arXiv preprint arXiv:2410.06940, 2024b. Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, et al. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In CVPR, 2024. Rowan Zellers, Jiasen Lu, Ximing Lu, Youngjae Yu, Yanpeng Zhao, Mohammadreza Salehi, Aditya Kusupati, Jack Hessel, Ali Farhadi, and Yejin Choi. Merlot reserve: Neural script knowledge through vision and language and sound. 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1635416366, 2022. Kaiwen Zha, Lijun Yu, Alireza Fathi, David A. Ross, Cordelia Schmid, Dina Katabi, and Xiuye Gu. Languageguided image tokenization for generation. ArXiv, abs/2412.05796, 2024. Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pretraining. 2023 IEEE/CVF International Conference on Computer Vision (ICCV), pp. 1194111952, 2023. Haotian Zhang, Mingfei Gao, Zhe Gan, Philipp Dufter, Nina Wenzel, Forrest Huang, Dhruti Shah, Xianzhi Du, Bowen Zhang, Yanghao Li, et al. MM1. 5: Methods, analysis & insights from multimodal llm fine-tuning. ICLR, 2025. Kaichen Zhang, Bo Li, Peiyuan Zhang, Fanyi Pu, Joshua Adrian Cahyono, Kairui Hu, Shuai Liu, Yuanhan Zhang, Jingkang Yang, Chunyuan Li, and Ziwei Liu. Lmms-eval: Reality check on the evaluation of large multimodal models, 2024a. Pan Zhang, Xiaoyi Dong, Yuhang Zang, Yuhang Cao, Rui Qian, Lin Chen, Qipeng Guo, Haodong Duan, Bin Wang, Linke Ouyang, et al. Internlm-xcomposer-2.5: versatile large vision language model supporting long-contextual input and output. arXiv:2407.03320, 2024b. Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In CVPR, 2018. Yuanhan Zhang, Jinming Wu, Wei Li, Bo Li, Zejun Ma, Ziwei Liu, and Chunyuan Li. Video instruction tuning with synthetic data. arXiv:2410.02713, 2024c. Long Zhao, Nitesh Gundavarapu, Liangzhe Yuan, Hao Zhou, Shen Yan, Jennifer Sun, Luke Friedman, Rui Qian, Tobias Weyand, Yue Zhao, et al. Videoprism: foundational visual encoder for video understanding. 2024. Chuanxia Zheng, Long Tung Vuong, Jianfei Cai, and Dinh Q. Phung. Movq: Modulating quantized vectors for high-fidelity image generation. ArXiv, abs/2209.09002, 2022. Chunting Zhou, Lili Yu, Arun Babu, Kushal Tirumala, Michihiro Yasunaga, Leonid Shamis, Jacob Kahn, Xuezhe Ma, Luke S. Zettlemoyer, and Omer Levy. Transfusion: Predict the next token and diffuse images with one multi-modal model. ArXiv, abs/2408.11039, 2024a. Junjie Zhou, Yan Shu, Bo Zhao, Boya Wu, Shitao Xiao, Xi Yang, Yongping Xiong, Bo Zhang, Tiejun Huang, and Zheng Liu. Mlvu: comprehensive benchmark for multi-task long video understanding. arXiv:2406.04264, 2024b. Orr Zohar, Xiaohan Wang, Yann Dubois, Nikhil Mehta, Tong Xiao, Philippe Hansen-Estruch, Licheng Yu, Xiaofang Wang, Felix Juefei-Xu, Ning Zhang, et al. Apollo: An exploration of video understanding in large multimodal models. arXiv:2412.10360, 2024."
        }
    ],
    "affiliations": []
}