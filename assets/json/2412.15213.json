{
    "paper_title": "Flowing from Words to Pixels: A Framework for Cross-Modality Evolution",
    "authors": [
        "Qihao Liu",
        "Xi Yin",
        "Alan Yuille",
        "Andrew Brown",
        "Mannat Singh"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Diffusion models, and their generalization, flow matching, have had a remarkable impact on the field of media generation. Here, the conventional approach is to learn the complex mapping from a simple source distribution of Gaussian noise to the target media distribution. For cross-modal tasks such as text-to-image generation, this same mapping from noise to image is learnt whilst including a conditioning mechanism in the model. One key and thus far relatively unexplored feature of flow matching is that, unlike Diffusion models, they are not constrained for the source distribution to be noise. Hence, in this paper, we propose a paradigm shift, and ask the question of whether we can instead train flow matching models to learn a direct mapping from the distribution of one modality to the distribution of another, thus obviating the need for both the noise distribution and conditioning mechanism. We present a general and simple framework, CrossFlow, for cross-modal flow matching. We show the importance of applying Variational Encoders to the input data, and introduce a method to enable Classifier-free guidance. Surprisingly, for text-to-image, CrossFlow with a vanilla transformer without cross attention slightly outperforms standard flow matching, and we show that it scales better with training steps and model size, while also allowing for interesting latent arithmetic which results in semantically meaningful edits in the output space. To demonstrate the generalizability of our approach, we also show that CrossFlow is on par with or outperforms the state-of-the-art for various cross-modal / intra-modal mapping tasks, viz. image captioning, depth estimation, and image super-resolution. We hope this paper contributes to accelerating progress in cross-modal media generation."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 9 1 ] . [ 1 3 1 2 5 1 . 2 1 4 2 : r Flowing from Words to Pixels: Framework for Cross-Modality Evolution Qihao Liu1,2 Xi Yin1 Alan Yuille2 Andrew Brown1 Mannat Singh 1GenAI, Meta 2Johns Hopkins University https://cross-flow.github.io/ Figure 1. We propose CrossFlow, general and simple framework that directly evolves one modality to another using flow matching with no additional conditioning. This is enabled using vanilla transformer without cross-attention, achieving comparable performance with state-of-the-art models on (a) text-to-image generation, and (b) various other tasks, without requiring task specific architectures."
        },
        {
            "title": "Abstract",
            "content": "Diffusion models, and their generalization, flow matching, have had remarkable impact on the field of media generation. Here, the conventional approach is to learn the complex mapping from simple source distribution of Gaussian noise to the target media distribution. For crossmodal tasks such as text-to-image generation, this same mapping from noise to image is learnt whilst including conditioning mechanism in the model. One key and thus far relatively unexplored feature of flow matching is that, unlike Diffusion models, they are not constrained for the source distribution to be noise. Hence, in this paper, we propose paradigm shift, and ask the question of whether we can instead train flow matching models to learn direct mapping from the distribution of one modality to the distribution of another, thus obviating the need for both the noise distribution and conditioning mechanism. We present general and simple framework, CrossFlow, for cross-modal flow matching. We show the importance of applying Variational Encoders to the input data, and introduce method to enable Classifier-free guidance. Surprisingly, for text-toimage, CrossFlow with vanilla transformer without cross attention slightly outperforms standard flow matching, and we show that it scales better with training steps and model size, while also allowing for interesting latent arithmetic which results in semantically meaningful edits in the output space. To demonstrate the generalizability of our approach, we also show that CrossFlow is on par with or outperforms the state-of-the-art for various cross-modal / intra-modal mapping tasks, viz. image captioning, depth estimation, and image super-resolution. We hope this paper contributes to accelerating progress in cross-modal media generation. 1 1. Introduction Diffusion models have achieved remarkable success in generating images [17, 59, 69, 73, 74], videos [7, 8, 34, 79], audio [41, 53], and 3D content [47, 65], revolutionizing the field of generative AI. Recently, flow matching [2, 50, 55] has been proposed as generalization of diffusion models, where models are trained to find an optimal transport probability path between source noise distribution and the target data distribution. This approach offers simpler, straight-line trajectories compared to the complex, curved trajectories in diffusion paths. As result, it has been rapidly adopted in the latest state-of-the-art image and video generation models, including LDMs [22] and Movie Gen [64]. Both diffusion and flow-based models are typically trained to learn the mapping from noise to the target distribution. For cross-modal generation tasks such as text-toimage [10, 73], this same mapping from noise to the target modality distribution (i.e. the images) is learnt whilst adding conditioning mechanism for the conditioning modality (i.e. the text) such as cross-attention. Unlike denoising diffusion models [33, 82], one relatively unexplored feature of flow matching models is that they are not constrained for the source distribution to be Gaussian noise; instead, the source distribution could be one that is correlated with the target distribution. Compared to noise, learning mapping from such distribution should intuitively be easier for the model because it has to learn shorter and more efficient probability paths. question remains however as to what this correlated source distribution could be. Interestingly, due to the information redundancy between different modalities arising from the same data point, for cross-modal generation tasks, the provided conditioning (e.g. the text in text-to-image) resembles such data that is correlated with the target distribution (e.g. the images). Hence, in this paper, we propose paradigm shift for crossmodal generation, and ask the question of whether we can instead train flow matching models to learn direct mapping from the distribution of one modality to the distribution of another, hence obviating the need for both the noise distribution and any conditioning mechanism. Despite the exciting theoretical motivation, there are several key challenges in practice. First, both diffusion and flow-based models require the source and target distributions to be of the same shape; requirement that is not satisfied for data distributions from different modalities. Secondly, state-of-the-art methods heavily rely on Classifierfree guidance (CFG) [32] for improved generation quality; method that is not compatible with cross-modal flow matching due to the lack of conditioning mechanism to turn on/off since the conditioning information instead lies within the source data. As result, prior work [1, 29, 55] targets the simple setting of mapping between two similar intra-modal distributions, such as human faces to cat faces [55]. In this work, we present key architecture design solutions for overcoming these challenges: First, we employ Variational Encoder for encoding the source modality data distribution to the same shape as the target modality, and show that the resulting regularization in the source distribution is essential for generation performance. Secondly, we enable CFG in cross-modal flow matching through the introduction of binary conditioning indicator during training, and demonstrate the quantative benefits of this approach compared to alternative CFG methods. We present CrossFlow; general framework for mapping between two different modalities without the need for any conditioning mechanism or noise distribution. Typically, different crossmodal generation tasks require task-specific architectural and training modifications, but CrossFlow works for different tasks without any such changes. Using the ubiquitous albeit challenging text-to-image (T2I) generation task as our primary setting, we show the significant result that CrossFlow outperforms commonly used flow matching baselines, given the same training data, model size, and training budget, all without requiring any cross-attention layers. CrossFlow exhibits improved scaling behavior over standard flow matching using crossattention when scaling training steps or model size, and is also compatible with variety of Large Language Models (LLMs), including CLIP [66], T5 [67], and Llama3 [19]. Additionally we demonstrate that since our approach encodes the source distribution into regularized continuous space with semantic structure, CrossFlow enables exciting new latent arithmetic for the text-to-image task, e.g., L(A dog with hat) + L(Sunglasses) L(A hat) creates an image of dog wearing sunglasses without hat. We demonstrate the general-purpose nature of CrossFlow on various cross-modal/intra-modal tasks: imageto-text (image captioning), image-to-depth (depth estimation), and low-resolution to high-resolution image (superresolution). we show that our CrossFlow approach achieves comparable or even superior performance to various stateof-the-art methods on all three tasks. Results are shown in Fig. 1. We hope that this paper contributes to accelerating the progress in cross-modal media generation. 2. Related Work Diffusion models and Rectified Flow. Starting from Gaussian noise, diffusion [33, 80] and score-based [36, 81] generative models progressively approximate the reverse ODE of stochastic forward process to generate data. These models have driven significant advances across various domains, particularly in high-fidelity image [5, 17, 35, 54, 62], video [7, 8, 34, 64, 79], and 3D generation [47, 65]. The rapid development of generative models in recent years demonstrates the effectiveness and stability of diffusion models in modeling complex real-world distributions. Re2 cently, rectified flow models [2, 50, 55], such as flow matching, have been proposed to improve the generative process by enabling transport map between two distributions via ODE. They enable faster training and sampling by avoiding complex probability flow ODEs. Directly bridging distributions. Flow Matching theoretically allows for arbitrary distributions as the source distribution, which can then be used for direct evolution. Various approaches have been proposed in this direction, such as InterFlow [1], α-blending [29], and Schrodinger Bridge [15, 51, 52, 77, 83, 84, 95]. These methods provide important theoretical support for using ODE-based methods to bridge two arbitrary distributions. However, they are still limited to the similar distributions from the same domain, such as image-to-image translation (e.g., faces-tofaces [55, 95] or sketches-to-images [52]). As step forward, CrossFlow focuses on learning the mapping between data distributions arising from even different modalities. Text-to-Image generation. Text-to-image generation [10, 14, 22, 59, 68, 69, 73, 74] has witnessed rapid advancements with the advent of diffusion and subsequently flow matching models. This task bridges two complex and important domains: language and vision. To tackle this challenge, existing methods often incorporate text encoders such as LLMs into the diffusion model through additional conditioning mechanisms, with cross-attention being the most widely adopted [22, 64]. However, these approaches increase modeling complexity and require additional parameters. We demonstrate that CrossFlow improves over standard flow matching with better scaling characteristics, and is comparable to prior work, despite simpler architecture. Cross-modal / intra-modal mapping. Various tasks can be framed as cross-modal/intra-modal mapping problems, including image captioning [24, 28, 43, 44, 58, 93, 96], depth estimation [6, 18, 39, 45, 46, 70, 90], and image super-resolution [23, 75]. However, due to the significant differences between modalities or distributions, previous methods have typically relied on task-specific designs. For example, Bit Diffusion [12] encodes text into binary bits and uses diffusion model with self-conditioning for captioning. Flow-based super-resolution models, such as CFM [23], still require the low-resolution image as extra conditioning, and also add Gaussian noise to the input. In contrast, our CrossFlow uses the same unified framework across all these tasks without extra conditioning or noise. 3. Preliminaries Flow Matching. We consider generative model that defines mapping between samples z0 from source distribution p0 to samples z1 of target distribution p1 via the ordinary differential equation (ODE): dzt = vθ(zt, t)dt. Here, vθ is the velocity parameterized by the weights θ of neural network, and [0, 1] is the time-step. Flow Matching [2, 50, 55] defines the forward process using the optimal transport path, i.e., zt = tz1 + (1 (1 σmin)t)z0 (1) where σmin = 105. The ground truth velocity can be derived as: ˆvt = dzt dt = z1 (1 σmin)z0 (2) To achieve this, network vθ(zt, t) is trained to predict velocity by minimizing the mean squared error (MSE) between its output and the target ˆvt. This constructs continuous path between z0 and z1 at any time-step [0, 1]. As discussed earlier, flow matching enables evolving sample z1 from an arbitrary source distribution p0. But prior work [22, 64] has typically relied on starting from simple Gaussian noise sample z0 (0, 1), and computing the velocity with additional condition incorporated through various methods, e.g., cross-attention [22, 64], channelwise concatenation [27]. Classifier-free guidance. CFG [32] is broadly used technique that enhances sample quality in conditional generative models by jointly training single model on conditional and unconditional objectives. This is achieved through randomly dropping the condition during training with certain probability p. Sampling is performed by extrapolating between conditional and unconditional denoising vθ(zt, c) and vθ(zt) with scaling factor ω: vθ(zt, c) = ωvθ(zt, c) + (1 ω)vθ(zt) (3) It significantly improves the generation quality and fidelity by guiding the samples towards higher likelihood of the condition c, which plays crucial role in state-of-the-art media generation models [10, 22, 64, 69]. 4. CrossFlow In this section, we discuss the various components of our approach: Variational Encoder (VE) to encode the inputs in Sec. 4.1, using flow matching to evolve from the source to the target distribution in Sec. 4.2, and finally, applying CFG in this setting for improving quality and fidelity in Sec. 4.3. 4.1. Variational Encoder for Encoding Inputs Flow matching models require the source distribution p0 to have the same shape as the target distribution p1. In particular, given an input x, we need to convert it to the source latent z0, which has the same shape as the target latent z1. An intuitive solution is to use an encoder to convert to z0, i.e., z0 = E(x), which can preserve most of the input information as shown in Appendix B.4. However, directly evolving from E(x) to z1 is problematic. We find that it is essential to formulate z0 as regularized distribution for the 3 Figure 2. CrossFlow Architecture. CrossFlow enables direct evolution between two different modalities. Taking text-to-image generation as an example, our T2I model comprises two main components: Text Variational Encoder and standard flow matching model. At inference time, we utilize the Text Variational Encoder to extract the text latent z0 Rhwc from text embedding Rnd produced by any language model. Then we directly evolve this text latent into the image space to generate image latent z1 Rhwc. source in order for flow matching to work well. To address this, we propose using VE to convert to z0. Formally, instead of directly predicting z0, we predict its mean µz0 and variance σz0 , and then sample the latent z0 (µz0 , σ2 ). z0 This enables us to convert the given input into latent z0 with regularized distribution, which can then be gradually evolved into the target distribution z1 with flow matching. The VE can be trained with standard Variational Autoencoding objective (VAE) [40] comprising of an encoding loss and the KL-divergence loss. For the encoding loss, the VE is trained to minimize loss between the output z0 and target ˆz. For VAE this loss would be reconstruction loss like MSE between the input and the decoder Ds output, MSE(D(z0), x). But since we simply need encoder and not an autoencoder, we dont restrict ourselves to VAE. 4.2. Training CrossFlow For each training sample, we start with an input-target pair (x, z1). We apply the VE to to encode it to latent z0 with the same shape as z1. Next, we employ transformer model vθ trained for flow matching as per Equations 1 and 2. The VE can be trained prior to training vθ or concurrently. We show in Sec. 5.2 that jointly training the Variational Encoder with flow matching results in improved performance. Specifically, we jointly train the VE with the flow matching model using sum of flow matching MSE loss LF , and the losses for Variational Encoder training (encoding loss LEnc and KL-divergence loss LKL): = LF + LEnc + λLKL = MSE(vθ(zt, t), ˆv) + Enc(z0, ˆz) + λKL(N (µz0 , σ2 z0 )N (0, 1)) (4) where λ is the weight of KL-divergence loss. Please find more details in Appendix A.1. 4.3. Classifier-Free Guidance with an Indicator CFG [32] has become the standard low-temperature sampling method for enhancing multi-modal alignment and improving quality. However, it can only be applied to generation methods that accept an additional conditioning input c, since the guidance signal relies on the difference between conditional and unconditional predictions vθ(zt, c) and vθ(zt). Recently, Autoguidance (AG) [38] has been introduced as method to enhance both conditional and unconditional generation, by leveraging smaller and lesstrained bad model as guidance. Nevertheless, it falls short of standard CFG in terms of performance. Moreover, AG requires training separate bad model, and its performance varies dramatically based on the choice of the bad model. The search space can be restricted by using an under-trained version of the same model, but this affects performance, and also is prohibitive for large models since it requires loading two large models in memory for inference. We instead aim to support CFG for CrossFlow, which is as accessible and performant as CFG is for standard flow matching. To enable CFG without the presence of an explicit conditioning input c, we introduce CFG with an indicator. Specifically, our model is of the form vθ(zt, 1c), where 1c {0, 1} is an indicator to specify conditional vs. unconditional generation. The model evolves from z0 to z1 when 1c = 1, and from z0 to zuc 1 when 1c = 0, where zuc represents any sample from the target distribution p1 1 other than z1. During training, we employ two learnable parameters, gc and guc, corresponding to conditional and unconditional generation, respectively. Depending on 1c, the appropriate learnable parameter is concatenated with the transformer input tokens along sequence dimension. We randomly sample the indicator with an unconditional rate of 10%, as per standard practice. The insight behind the CFG indicator is similar to that of standard CFG. In this approach, vθ(zt, 1) is trained to map z0 to specific region of the target manifold, while vθ(zt, 0) is trained to map z0 to the entire target manifold. 4.4. Flowing from Text to Image Now, we consider text-to-image generation as the archetypal task to leverage CrossFlow. We start with the input text embedding Rnd with token length and dimension d, and use our Text VE to extract the corresponding text latent z0 (µx, σ2 x). While our approach is agnostic to pixel vs. latent image generation, we consider image generation in the latent space for efficiency, and leverage pretrained VAE to obtain the image latent from the input image I, which serves as our target z1. Then, we employ the vanilla flow matching [50] model to predict v(zt, t) between z0 and z1. The pipeline for performing text-to-image generation with CrossFlow is illustrated in Fig. 2. We discuss how to train performant Text Variational Encoder next. 4.4.1. Text Variational Encoder Training the Text VE is challenging, as this involves compressing the text embeddings to small latent space (e.g., 77768 CLIP tokens to 43232 image latents for 256px generation, 14.4 compression). We explore various methods to train VEs for CrossFlow. The straightforward approach is to simply train VAE with MSE reconstruction loss. While this approach achieves very low reconstruction errors, we find that it does not capture semantic concepts well, leading to sub-optimal image generations. Contrastive loss. We explore contrastive losses, which produce representations with strong semantic understanding when training on samples within the same modality [11, 60] and on different modality pairs [66]. To produce the contrastive targets for the VE, we either use the input text embedding (text-text contrastive), or the paired image for the text (image-text contrastive). Given the target, we employ simple encoder to project it into feature space with the same shape as z0, resulting in representation denoted as ˆz. We then encourage semantic similarity between z0 and ˆz using the contrastive CLIP loss [66]. During training, the batch-wise contrastive loss is computed as LEnc = CLIP(z0, ˆz). We ablate this choice in Sec. 5.2 and find that contrastive loss works significantly better than the VAE reconstruction loss, with the image-text loss working slightly better than the text-text loss. 5. Experiments We first evaluate CrossFlow on text-to-image generation, demonstrate its scalability, and showcase some interesting applications with latent arithmetic in Sec. 5.1. Then, we ablate our main design decisions through ablation studies in Sec. 5.2. Finally, we further explore CrossFlows performance on three distinct tasks: image captioning, monocular depth estimation, and image super-resolution in Sec. 5.3. 5.1. Text-to-Image Generation Method #Params (B) #Steps (K) FID CLIP Standard FM (Baseline) CrossFlow (Ours) 1.04 0.95 300 300 10.79 10. 0.29 0.29 Table 1. Comparison between our CrossFlow and standard flow matching with cross-attention. Both models are trained with the same settings. We find that our model slightly outperforms standard flow matching baseline in terms of zero-shot FID30K and achieves comparable performance on the CLIP score. text cross-attention. For fair comparison, we use the exact same codebase, training recipe, dataset, and budget to train both CrossFlow and the baseline. Note that the baseline requires cross-attention layer after each self-attention layer, whereas our model only relies on self-attention layers, resulting in fewer parameters for the same number of layers. To account for this, we adjust the number of layers to ensure that both models have similar model sizes. For both methods, we use grid search to find the optimal CFG scale. We also compare CrossFlow with state-of-the-art T2I models to demonstrate that our approach is competitive with those established methods. Architecture. Our model enables the use of vanilla Transformer [86] with self-attention layers and feed-forward layers. We adopt DiMR [54] as the base architecture for the flow matching models, variant of Diffusion Transformer (DiT) [62] which replaces the parameter-heavy MLP in adaLN-Zero with more parameter-efficient TimeDependent Layer Normalization. For the Text VE, we employ stacked Transformer blocks, followed by linear layer to project the output into the target shape. Training details. We use proprietary dataset with about 350M image-text pairs to train both CrossFlow and our ablations. Our text encoder is based on CLIP [66] with fixed sequence length of 77 text tokens. We use pre-trained and frozen VAE from LDM [73] to extract image latents. All T2I models are trained using the same settings: an image resolution of 256256, batch size of 1024, base learning rate of 1 104 with 5000 warm-up steps, and an AdamW optimizer [57] with β1 = β2 = 0.9 and weight decay of 0.03. We train our largest model (0.95B) on 256 256 for 600K iterations, then finetune it on 512 512 for an additional 240K iterations to generate higher-resolution images. Experimental setup. Comparing T2I models scientifically is challenging due to the diverse datasets used for training, which often include proprietary data, and varying training conditions. In addition, our proposed method represents new paradigm for utilizing diffusion models, distinct from the previous T2I approaches. Therefore, we primarily compare our model with the widely used standard flow matching baseline that starts from noise and leverages Evaluation metrics. We evaluate all models on the COCO validation set [49] and report FID [31] and CLIP score [30, 66]. Following previous works, we report zero-shot FID30K, where 30K prompts are randomly sampled from the validation set, and the generated images are compared to reference images from the full validation set. Additionally, we also evaluate our models on GenEval benchmark as it exhibits strong alignment with human judgment [26]. 5 0 3 - 22 18 10 6 70 150 300 500 0 3 - 17 16 15 14 12 50 Standard FM CrossFlow 100 150 250 300 Model parameters (millions) Training steps (thousands) Figure 3. Performance vs. Model Parameters and Iterations. We compare the baseline of starting from noise with text crossattention with CrossFlow, while controlling for data, model size and training steps. Left: Larger models are able to exploit the cross-modality connection better. Right: CrossFlow needs more steps to converge, but converges to better final performance. Overall, CrossFlow scales better than the baseline and can serve as the framework for future media generation models. 5.1.1. CrossFlow vs. Standard Flow Matching We compare our CrossFlow with widely used crossattention baseline in Tab. 1. Both models are trained and tested under the same settings. The results show that CrossFlow achieves comparable performance, with slightly better zero-shot FID-30K compared with widely used flow matching baselines with cross-attention. Scaling characteristics. We investigate the scalability of CrossFlow in Fig. 3 and compare it with standard flow matching. We train both approaches across 5 different model sizes, ranging from 70M to 1B parameters, with the same training settings, for 300K iterations. At smaller scales, CrossFlow underperforms the baseline, likely due to the lack of sufficient parameters to model the complex relationships between two modalities. But excitingly, as the model size increases, the zero-shot FID-30K improves more for our approach. Next, we evaluate the effect of varying the training iterations. We notice similarly that CrossFlow improves more as we increase training iterations. While CrossFlow initially underperforms standard flow matching at small scales, increasing the model size and training iterations improves it significantly, even enabling it to surpass standard flow matching. We attribute this to the fact that CrossFlow generates images by directly evolving from the source distribution where different sub-regions correspond to different semantics. In contrast, standard flow matching may generate the same semantics from the entire source distribution, while exploiting the inductive biases afforded by text cross-attention. Ultimately, this works in favor of CrossFlow, as the learnt cross-modal paths and fewer inductive biases result in improved scaling characteristics with both model size and training iterations. 5.1.2. State-of-the-art Comparison Finally, we compare CrossFlow with state-of-the-art textto-image models and report results in Tab. 2. We achieve zero-shot FID-30K of 9.63 on COCO, and GenEval score of 0.55, demonstrating performance comparable with Method #Params. FID-30K GenEval zero-shot score DALLE [68] GLIDE [59] LDM [73] DALLE 2 [69] LDMv1.5 [73] Imagen [74] RAPHAEL [88] PixArt-α [10] LDMv3 (5122) [22] CrossFlow 12.0B 5.0B 1.4B 6.5B 0.9B 3.0B 3.0B 0.6B 8.0B 0.95B 27.50 12.24 12.63 10.39 9.62 7.27 6.61 7.32 - 9.63 - - - 0.52 0.43 - - 0.48 0.68 0.55 Table 2. Comparison with recent T2I models. For GenEval, we report the overall score here and provide task-specific scores in Appendix B.1. CrossFlow achieves comparable performance with state-of-the-art T2I models by directly evolving text into images. the state-of-the-art. Note that our model uses only 630 A100 GPU days for training, whereas other methods like DALLE 2 [69] typically require thousands of A100 GPU days. These results suggest that CrossFlow is simple and promising direction for state-of-the-art media generation. 5.1.3. Arithmetic Operations in Latent Space Unlike previous diffusion or flow matching models, CrossFlow offers unique property: arithmetic operations in the input latent space translate to similar operations in the output space. This is made possible since CrossFlow transforms the source space (i.e., the text latent space for T2I) into regularized continuous space, where uniform representation shape is shared across all texts. We showcase two examples of this, latent interpolation, and latent arithmetic. For linear interpolation, we use the Text Variational Encoder to generate text latents from two different text inputs, and then interpolate between them to produce images. As shown in Fig. 4, CrossFlow enables visually smooth linear interpolations, even between disparate prompts. Next, we showcase arithmetic operations in Fig. 5, in which we apply addition and subtraction in the text latent space, and find that the resulting images exhibit corresponding semantic modifications to the original image. This demonstrates that CrossFlow formulates meaningful and well-structured semantic paths between the source and the target distributions, providing interesting capabilities and more control compared to standard flow matching approaches. 5.2. Ablation Study We conduct various ablation experiments to verify the effectiveness of the proposed designs in Tab. 3. Variational Encoder vs. standard encoder. Compared to standard encoder or even adding Guassian noise like CFM [23], Variational Encoder significantly improves the generation quality, with significant gains in the FID. This shows that forming regularized distribution for the source domain is crucial step for cross-modal flow matching. Joint training vs. two-stage training. We consider three 6 Figure 4. CrossFlow provides visually smooth interpolations in the latent space. We show images generated by linear interpolation between the first (left) and second (right) text latents. CrossFlow enables visually smooth transformations of object direction, composite colors, shapes, background scenes, and even object categories. Please zoom in for better visualization. For brevity, we display only 7 interpolating images here; additional interpolating images can be found in Appendix (Fig. 10 and Fig. 11). Text encoder FID CLIP Loss FID CLIP Encoder Encoder + noise Variational Encoder 66.65 59.91 40. 0.20 0.21 0.23 T-T Recon. T-T Contrast. I-T Contrast. 40.78 34.67 33.41 0.23 0.24 0.24 (a) Variational Encoder * (b) Text VE loss* Method FID CLIP Model FID CLIP No guidance AG CFG indicator 33.41 26.36 24. 0.24 0.25 0.26 CLIP (0.4B) T5-XXL (11B) Llama3 (7B) 24.33 22.28 21.20 0.26 0.27 0.27 (c) CFG with indicator (d) Language Model Train strategy FID CLIP 2-stage separate training Joint training 2-stage w/ joint finetuning 32.55 24.33 23.79 0.24 0.26 0.26 (e) Training strategy Table 3. Ablation study on Text Variational Encoder, training objective, CFG, language models, and training strategy. We conduct ablation study on our smallest model (70M), reporting zero-shot FID-10K and CLIP scores. Final settings used for CrossFlow are underlined. AG: Autoguidance. *: results without applying CFG. our CFG indicator works better than AG in terms of both FID and CLIP alignment while only using single model trained with standard CFG settings. Qualitatively, our approach produces much higher fidelity images compared to both alternatives, as shared in Appendix B.4. Text VE loss. We explore reconstruction and contrastive objectives for the encoder loss LEnc when training the text VE. We find that contrastive loss which promotes semantic understanding yields significantly better performance than reconstruction loss on the input text embeddings. Furthermore, image-text contrastive loss slightly outperforms text-text contrastive loss. Effect of different language models. We evaluate CrossFlow with various language models trained with different objectives. Specifically, we evaluate CLIP [66] (contrastive image-text), T5-XXLs encoder [67] (encoder-decoder), Figure 5. CrossFlow allows arithmetic in text latent space. Using the Text Variational Encoder (VE), we first map the input text into the latent space z0. Arithmetic operations are then performed in this latent space, and the resulting latent representation is used to generate the corresponding image. The latent code z0 used to generate each image is provided at the bottom. training strategies: (1) jointly training the VE and flow matching from scratch, (2) training the VE first and then training flow matching with fixed VE, and (3) training the VE first and then training the flow matching while jointly fine-tuning VE. We observe that it is important to update the VE when training the flow matching, either through joint training from scratch, or finetuning the VE jointly with flow matching. Initializing with pre-trained VE and then jointly training improves convergence speed by about 35%, but we opt to jointly train both models from scratch on account of the simplicity, and for fair comparisons with baselines. CFG indicator. We evaluate the performance of our model when leveraging our proposed CFG indicator techinuqe. We also evaluate Autoguidance (AG) [38], which utilizes two models for inference we use an under-trained version of the same model as the bad model, while using gridsearch to find the best under-trained checkpoint. While AG improves FID and also image-text CLIP alignment slightly, 7 Method B@4 MNIC [24] MIR [43] NAIC-CMAL [28] SATIC [96] SCD-Net [58] CrossFlow (Ours) 30.9 32.5 35.3 32.9 37.3 36. 27.5 27.2 27.3 27.0 28.1 55.6 - 56.9 - 58.0 108.1 109.5 115.5 111.0 118.0 21.0 20.6 20.8 20.5 21.6 27. 57.1 116.2 20.4 Table 4. Image captioning on COCO Karpathy split. CrossFlow directly evolves from image to text, achieving comparable performance to state-of-the-art models on image captioning. For fair comparison, we only consider non-autoregressive methods that are trained without CIDEr optimization. Llama3-7B [19] (decoder-only). We use 77 tokens for all language models, resulting in text embeddings of size 77768, 774096, 774096, respectively. We train separate Text VE for each language model, projecting the text embeddings into the target image latent shape (4 32 32). CrossFlow works well with all language models regardless of their training objectives and embedding sizes. As expected, our performance improves with better text representations. Due to compute restrictions however, we use the light-weight CLIP model for our main experiments. 5.3. CrossFlow for Various Tasks We further evaluate CrossFlow on three distinct tasks that involve cross-modal / intra-modal evolution. We present the main results and key findings here, while additional details and qualitative results can be found in the Appendix. Image to text (captioning). We first consider the task of image captioning. To achieve this, we train new Text Variational Encoder on the captioning dataset to extract text latents from text tokens, and separate text decoder with reconstruction loss to convert text latents back into tokens. CrossFlow is then trained to map from the image latent space to the text latent space. Following previous work, we use the Karpathy split [37] of COCO dataset [49] for training and testing, and report results in Tab. 4. CrossFlow enables direct evolution from image space to text space for image captioning, achieving state-of-the-art performance. Image to depth (depth estimation). For monocular depth estimation, we train CrossFlow in pixel space. Specifically, we use recontruction loss to train the Image Variational Encoder to map the original image into the shape of depth map, followed by the flow matching model which generates the final depth maps. We train and evaluate our model on KITTI [25] (Eigen split [21]) and NYUv2 [78] (official split) for outdoor and indoor scenarios, respectively. As shown in Tab. 5, our model achieves comparable performance to state-of-the-art methods on both datasets. Notably, DiffusionDepth [18] utilizes Swin Transformer [56] and specific designs such as Multi-Scale Aggregation and Monocular Conditioned Denoising Block. In contrast, our model achieves similar performance without any additional 8 Method KITTI NYUv2 AbsRel () δ1 () AbsRel () δ1 () TransDepth [89] AdaBins [6] DepthFormer [45] BinsFormer [46] DiffusionDepth [18] CrossFlow (Ours) 0.064 0.058 0.052 0.052 0.050 0. 0.956 0.964 0.975 0.974 0.977 0.973 0.106 0.103 0.096 0.094 0.085 0.094 0.900 0.903 0.921 0.925 0.939 0. Table 5. Monocular depth estimation on KITTI and NYUv2. CrossFlow enables direct mapping from image to depth, achieving comparable performance to state-of-the-art models. Method Reference Regression SR3 [75] Flow Matching [50] CrossFlow (Ours) FID IS PSNR SSIM 1.9 15.2 5.2 3.4 3. 240.8 121.1 180.1 200.8 207.2 - 27.9 26.4 24.7 25. - 0.801 0.762 0.747 0.764 Table 6. Image super-resolution on the ImageNet validation set. Compared with standard SR method with flow matching, our direct mapping method achieves better performance. enhancements, demonstrating the efficiency and effectiveness of CrossFlow in mapping from images to depth. Low-resolution to high-resolution (super-resolution). We compare CrossFlow with the standard super-resolution method using flow matching, which involves upsampling the low-resolution image and then concatenating it with input noise as conditioning before feeding it into the neural network. In contrast, our method directly evolves the upsampled low-resolution image into high-resolution image, without additional concatenation conditioning. We also compare against SR3 [75] which uses diffusion models for super-resolution. Following previous work [50, 75], we train and evaluate our model on ImageNet [16] for the task of 64 64 256 256 super-resolution, and provide results in Tab. 6. Our method achieves better results compared to the standard flow matching baseline and SR3, indicating that CrossFlow can also effectively evolve between similar distributions while achieving superior performance. 6. Conclusion In this paper, we proposed CrossFlow, simple and general framework for cross-modal flow matching that works well across variety of tasks without requiring task specific architectural modifications. It outperforms conventional flow matching, while also enabling new capabilities such as latent arithmetic. We showcase that CrossFlow is promising approach for the future thanks to its better scalablity. We hope our approach helps pave the way towards further research and applications of cross-modal flow matching. Acknowledgements. We sincerely appreciate Ricky Chen and Saketh Rambhatla for their valuable discussions."
        },
        {
            "title": "References",
            "content": "[1] Michael Albergo and Eric Vanden-Eijnden. Building normalizing flows with stochastic interpolants. arXiv preprint arXiv:2209.15571, 2022. 2, 3 [2] Michael Samuel Albergo and Eric Vanden-Eijnden. Building normalizing flows with stochastic interpolants. In ICLR, 2023. 2, 3 [3] Peter Anderson, Basura Fernando, Mark Johnson, and Stephen Gould. Spice: Semantic propositional image caption evaluation. In ECCV, 2016. 12 [4] Satanjeev Banerjee and Alon Lavie. Meteor: An automatic metric for mt evaluation with improved correlation with human judgments. In Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization, 2005. 12 [5] Fan Bao, Shen Nie, Kaiwen Xue, Yue Cao, Chongxuan Li, Hang Su, and Jun Zhu. All are worth words: vit backbone for diffusion models. In CVPR, 2023. 2 [6] Shariq Farooq Bhat, Ibraheem Alhashim, and Peter Wonka. Adabins: Depth estimation using adaptive bins. In CVPR, 2021. 3, [7] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. 2 [8] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis. Align your latents: High-resolution video synthesis with latent diffusion models. In CVPR, 2023. 2 [9] Yohann Cabon, Naila Murray, and Martin Humenberger. Virtual kitti 2. arXiv preprint arXiv:2001.10773, 2020. 13, 14 [10] Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, et al. Pixart-α : Fast training of diffusion transformer for photorealistic text-to-image synthesis. arXiv preprint arXiv:2310.00426, 2023. 2, 3, 6, 12 [11] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. simple framework for contrastive learning of visual representations. In ICML, 2020. [12] Ting Chen, Ruixiang Zhang, and Geoffrey Hinton. Analog bits: Generating discrete data using diffusion models with self-conditioning. arXiv preprint arXiv:2208.04202, 2022. 3 [13] Angela Dai, Angel Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and Matthias Nießner. Scannet: Richly-annotated 3d reconstructions of indoor scenes. In CVPR, 2017. 13 [14] Xiaoliang Dai, Ji Hou, Chih-Yao Ma, Sam Tsai, Jialiang Wang, Rui Wang, Peizhao Zhang, Simon Vandenhende, Xiaofang Wang, Abhimanyu Dubey, et al. Emu: Enhancing image generation models using photogenic needles in haystack. arXiv preprint arXiv:2309.15807, 2023. 3 [15] Valentin De Bortoli, Guan-Horng Liu, Tianrong Chen, Evangelos Theodorou, and Weilie Nie. Augmented bridge matching. arXiv preprint arXiv:2311.06978, 2023. 3 [16] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: large-scale hierarchical image database. In CVPR, 2009. 8, 12 [17] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. In NeurIPS, 2021. 2 [18] Yiqun Duan, Xianda Guo, and Zheng Zhu. Diffusiondepth: Diffusion denoising approach for monocular depth estimation. arXiv preprint arXiv:2303.05021, 2023. 3, [19] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. 2, 8 [20] Ainaz Eftekhar, Alexander Sax, Jitendra Malik, and Amir Zamir. Omnidata: scalable pipeline for making multi-task mid-level vision datasets from 3d scans. In ICCV, 2021. 14 [21] David Eigen, Christian Puhrsch, and Rob Fergus. Depth map prediction from single image using multi-scale deep network. In NeurIPS, 2014. 8, 12 [22] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In ICML, 2024. 2, 3, 6, 12 [23] Johannes Fischer, Ming Gui, Pingchuan Ma, Nick Stracke, Stefan Baumann, and Bjorn Ommer. Boosting latent diffusion with flow matching. arXiv preprint arXiv:2312.07360, 2023. 3, 6 [24] Junlong Gao, Xi Meng, Shiqi Wang, Xia Li, Shanshe Wang, Siwei Ma, and Wen Gao. Masked non-autoregressive image captioning. arXiv preprint arXiv:1906.00717, 2019. 3, 8 [25] Andreas Geiger, Philip Lenz, Christoph Stiller, and Raquel Urtasun. Vision meets robotics: The kitti dataset. The International Journal of Robotics Research, 2013. 8, 12, 13 [26] Dhruba Ghosh, Hannaneh Hajishirzi, and Ludwig Schmidt. Geneval: An object-focused framework for evaluating textto-image alignment. In NeurIPS, 2024. 5 [27] Rohit Girdhar, Mannat Singh, Andrew Brown, Quentin Duval, Samaneh Azadi, Sai Saketh Rambhatla, Akbar Shah, Xi Yin, Devi Parikh, and Ishan Misra. Emu video: Factorizing text-to-video generation by explicit image conditioning. arXiv preprint arXiv:2311.10709, 2023. [28] Longteng Guo, Jing Liu, Xinxin Zhu, Xingjian He, Jie Jiang, and Hanqing Lu. Non-autoregressive image captioning with counterfactuals-critical multi-agent learning. arXiv preprint arXiv:2005.04690, 2020. 3, 8 [29] Eric Heitz, Laurent Belcour, and Thomas Chambon. Iterative α-(de) blending: minimalist deterministic diffusion model. In SIGGRAPH, 2023. 2, 3 [30] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. Clipscore: reference-free evaluation metric for image captioning. arXiv preprint arXiv:2104.08718, 2021. 5 [31] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. In NeurIPS, 2017. 5 [32] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. 2, 3, 4 [33] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In NeurIPS, 2020. 2 [34] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik Kingma, Ben Poole, Mohammad Norouzi, David Fleet, et al. Imagen video: High definition video generation with diffusion models. arXiv preprint arXiv:2210.02303, 2022. 2 [35] Jonathan Ho, Chitwan Saharia, William Chan, David Fleet, Mohammad Norouzi, and Tim Salimans. Cascaded diffusion models for high fidelity image generation. JMLR, 2022. [36] Aapo Hyvarinen and Peter Dayan. normalized statistical models by score matching. 2005. 2 Estimation of nonJMLR, [37] Andrej Karpathy and Li Fei-Fei. Deep visual-semantic alignments for generating image descriptions. In CVPR, 2015. 8, 12 [38] Tero Karras, Miika Aittala, Tuomas Kynkaanniemi, Jaakko Lehtinen, Timo Aila, and Samuli Laine. Guiding diffusion model with bad version of itself. arXiv preprint arXiv:2406.02507, 2024. 4, 7, 13 [39] Bingxin Ke, Anton Obukhov, Shengyu Huang, Nando Metzger, Rodrigo Caye Daudt, and Konrad Schindler. Repurposing diffusion-based image generators for monocular depth estimation. In CVPR, 2024. 3, 13, [40] Diederik Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013. 4 [41] Zhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, and Bryan Catanzaro. Diffwave: versatile diffusion model for audio synthesis. arXiv preprint arXiv:2009.09761, 2020. 2 [42] Solomon Kullback and Richard Leibler. On information and sufficiency. The annals of mathematical statistics, 1951. 12 [43] Jason Lee, Elman Mansimov, and Kyunghyun Cho. Deterministic non-autoregressive neural sequence modeling by iterative refinement. arXiv preprint arXiv:1802.06901, 2018. 3, 8 [44] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-training for unified In ICML, vision-language understanding and generation. 2022. 3 [45] Zhenyu Li, Zehui Chen, Xianming Liu, and Junjun Jiang. Depthformer: Exploiting long-range correlation and local information for accurate monocular depth estimation. Machine Intelligence Research, 2023. 3, 8 [46] Zhenyu Li, Xuyang Wang, Xianming Liu, and Junjun Jiang. Binsformer: Revisiting adaptive bins for monocular depth estimation. IEEE Transactions on Image Processing, 2024. 3, 8 [47] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa, Xiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler, Ming-Yu Liu, and Tsung-Yi Lin. Magic3d: High-resolution text-to-3d content creation. In CVPR, 2023. [48] Chin-Yew Lin. Rouge: package for automatic evaluation of summaries. In Text summarization branches out, 2004. 12 10 [49] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and Lawrence Zitnick. Microsoft coco: Common objects in context. In ECCV, 2014. 5, 8, 12 [50] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. In ICLR, 2022. 2, 3, 5, 8, 12 [51] Guan-Horng Liu, Yaron Lipman, Maximilian Nickel, Brian Karrer, Evangelos Theodorou, and Ricky TQ Chen. GenarXiv preprint eralized schr odinger bridge matching. arXiv:2310.02233, 2023. 3 [52] Guan-Horng Liu, Arash Vahdat, De-An Huang, Evangelos Theodorou, Weili Nie, and Anima Anandkumar. arXiv preprint Image-to-image schr odinger bridge. arXiv:2302.05872, 2023. [53] Haohe Liu, Zehua Chen, Yi Yuan, Xinhao Mei, Xubo Liu, Danilo Mandic, Wenwu Wang, and Mark Plumbley. Audioldm: Text-to-audio generation with latent diffusion models. arXiv preprint arXiv:2301.12503, 2023. 2 [54] Qihao Liu, Zhanpeng Zeng, Ju He, Qihang Yu, Xiaohui Shen, and Liang-Chieh Chen. Alleviating distortion in image generation via multi-resolution diffusion models. arXiv preprint arXiv:2406.09416, 2024. 2, 5 [55] Xingchao Liu, Chengyue Gong, and qiang liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. In ICLR, 2023. 2, 3 [56] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In ICCV, 2021. 8 [57] Loshchilov. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. [58] Jianjie Luo, Yehao Li, Yingwei Pan, Ting Yao, Jianlin Feng, Hongyang Chao, and Tao Mei. Semantic-conditional diffusion networks for image captioning. In CVPR, 2023. 3, 8 [59] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. arXiv preprint arXiv:2112.10741, 2021. 2, 3, 6 [60] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748, 2018. 5 [61] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: method for automatic evaluation of machine translation. In ACL, 2002. 12 [62] William Peebles and Saining Xie. Scalable diffusion models with transformers. In ICCV, 2023. 2, [63] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Improving latent diffusion modRobin Rombach. Sdxl: arXiv preprint els for high-resolution image synthesis. arXiv:2307.01952, 2023. 12, 13 [64] Adam Polyak, Amit Zohar, Andrew Brown, Andros Tjandra, Animesh Sinha, Ann Lee, Apoorv Vyas, Bowen Shi, ChihYao Ma, Ching-Yao Chuang, et al. Movie gen: cast of media foundation models. arXiv preprint arXiv:2410.13720, 2024. 2, 3 [65] Ben Poole, Ajay Jain, Jonathan Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d diffusion. arXiv preprint arXiv:2209.14988, 2022. 2 [66] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In ICML, 2021. 2, 5, 7 [67] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter Liu. Exploring the limits of transfer learning with unified text-to-text transformer. JMLR, 2020. 2, 7 [68] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In ICML, 2021. 3, 6 [69] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022. 2, 3, 6, 12, [70] Rene Ranftl, Katrin Lasinger, David Hafner, Konrad Schindler, and Vladlen Koltun. Towards robust monocular depth estimation: Mixing datasets for zero-shot cross-dataset transfer. TPAMI, 2020. 3, 14 [71] Rene Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. Vision transformers for dense prediction. In ICCV, 2021. 14 [72] Mike Roberts, Jason Ramapuram, Anurag Ranjan, Atulit Kumar, Miguel Angel Bautista, Nathan Paczan, Russ Webb, and Joshua Susskind. Hypersim: photorealistic synthetic dataset for holistic indoor scene understanding. In ICCV, 2021. 13, 14 [73] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In CVPR, 2022. 2, 3, 5, 6, 12, 13 [74] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. In NeurIPS, 2022. 2, 3, [75] Chitwan Saharia, Jonathan Ho, William Chan, Tim Salimans, David Fleet, and Mohammad Norouzi. Image superresolution via iterative refinement. TPAMI, 2022. 3, 8, 12 [76] Thomas Schops, Johannes Schonberger, Silvano Galliani, Torsten Sattler, Konrad Schindler, Marc Pollefeys, and Andreas Geiger. multi-view stereo benchmark with highresolution images and multi-camera videos. In CVPR, 2017. 13 [77] Yuyang Shi, Valentin De Bortoli, Andrew Campbell, and Arnaud Doucet. Diffusion schrodinger bridge matching. In NeurIPS, 2024. 3 [78] Nathan Silberman, Derek Hoiem, Pushmeet Kohli, and Rob Indoor segmentation and support inference from Fergus. rgbd images. In ECCV, 2012. 8, 12, 13 [79] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, et al. Make-a-video: Text-to-video generation without text-video data. arXiv preprint arXiv:2209.14792, 2022. [80] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In ICML, 2015. 2 [81] Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. In NeurIPS, 2019. 2 [82] Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. In ICLR, 2021. 2 [83] Zhicong Tang, Tiankai Hang, Shuyang Gu, Dong Chen, and Baining Guo. Simplified diffusion schr odinger bridge. arXiv preprint arXiv:2403.14623, 2024. 3 [84] Alexander Tong, Nikolay Malkin, Kilian Fatras, Lazar Atanackovic, Yanlei Zhang, Guillaume Huguet, Guy Wolf, Simulation-free schr odinger and Yoshua Bengio. arXiv preprint bridges via score and flow matching. arXiv:2307.03672, 2023. 3 [85] Igor Vasiljevic, Nick Kolkin, Shanyi Zhang, Ruotian Luo, Haochen Wang, Falcon Dai, Andrea Daniele, Mohammadreza Mostajabi, Steven Basart, Matthew Walter, et al. Diode: dense indoor and outdoor depth dataset. arXiv preprint arXiv:1908.00463, 2019. [86] Vaswani. Attention is all you need. In NeurIPS, 2017. 5 [87] Ramakrishna Vedantam, Lawrence Zitnick, and Devi Parikh. Cider: Consensus-based image description evaluation. In CVPR, 2015. 12 [88] Zeyue Xue, Guanglu Song, Qiushan Guo, Boxiao Liu, Zhuofan Zong, Yu Liu, and Ping Luo. Raphael: Text-to-image generation via large mixture of diffusion paths. In NeurIPS, 2024. 6 [89] Guanglei Yang, Hao Tang, Mingli Ding, Nicu Sebe, and Elisa Ricci. Transformer-based attention networks for continuous pixel-wise prediction. In ICCV, 2021. 8 [90] Lihe Yang, Bingyi Kang, Zilong Huang, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. Depth anything: Unleashing the power of large-scale unlabeled data. In CVPR, 2024. 3 [91] Wei Yin, Xinlong Wang, Chunhua Shen, Yifan Liu, Zhi Tian, Songcen Xu, Changming Sun, and Dou Renyin. Diversedepth: Affine-invariant depth prediction using diverse data. arXiv preprint arXiv:2002.00569, 2020. 14 [92] Wei Yin, Jianming Zhang, Oliver Wang, Simon Niklaus, Long Mai, Simon Chen, and Chunhua Shen. Learning to recover 3d scene shape from single image. In CVPR, 2021. [93] Quanzeng You, Hailin Jin, Zhaowen Wang, Chen Fang, and In Image captioning with semantic attention. Jiebo Luo. CVPR, 2016. 3 [94] Chi Zhang, Wei Yin, Billzb Wang, Gang Yu, Bin Fu, and Chunhua Shen. Hierarchical normalization for robust monocular depth estimation. In NeurIPS, 2022. 14 [95] Linqi Zhou, Aaron Lou, Samar Khanna, and Stefano Ermon. Denoising diffusion bridge models. arXiv preprint arXiv:2309.16948, 2023. 3 [96] Yuanen Zhou, Yong Zhang, Zhenzhen Hu, and Meng Wang. Semi-autoregressive transformer for image captioning. In ICCV, 2021. 3,"
        },
        {
            "title": "Appendix",
            "content": "In the appendix, we provide additional information as listed below: Sec. A. Method details Sec. A.1. Loss function for text-to-image generation Sec. A.2. Experimental details for various tasks Sec. B. Additional experimental results Sec. B.1. GenEval performance for text-to-image Sec. B.2. Zero-shot depth estimation Sec. B.3. Image super-resolution Sec. B.4. Ablations on Text VE and CFG indicator Sec. C. Additional qualitative examples Fig. 9. Text-to-image generation Fig. 10, 11. Interpolation in latent space Fig. 12. Arithmetic in latent space A. Method Details A.1. Loss Function for T2I Generation We jointly train the Text Variational Encoder with the flow matching model using the following training objective: = LF + LEnc + λLKL = MSE(vθ(zt, t), ˆv) + CLIP(z0, ˆz) )N (0, 1)) + λKL(N (µz0 , σ2 z0 (5) where λ is the weight of KL-divergence loss. For the flow matching loss LF , we follow previous work [50] and compute the MSE loss between the predicted velocity vθ(zt, t) at time-step and the ground-truth velocity ˆv. To train the Text Variational Encoder, we adopt CLIP contrastive loss. Specifically, given batch of text and image pairs, we use our Text Variational Encoder to obtain text latents z0, and an image encoder to extract image features ˆz. Then, we compute the cosine similarity between all pairs of z0 and ˆz in the batch, resulting in similarity matrix S, where each element sij represents the cosine similarity between the ith z0 and jth ˆz. The similarity scores are then scaled by temperature parameter τ (a learnable parameter), denoted as logitsij = sij/τ . After that, symmetric cross-entropy loss over the similarity scores is computed: LI2T = LT2I ="
        },
        {
            "title": "1\nN",
            "content": "N (cid:88) i=1 (cid:88) i=1 log exp(logitsii) j=1 exp(logitsij) (cid:80)N log exp(logitsii) j=1 exp(logitsji) (cid:80)N (6) (7) Finally, we compute the average of these two components to obtain the CLIP loss, which is then used to update our Text Variational Encoder: LEnc = CLIP(z0, ˆz) = 1 2 (LI2T + LT2I) (8) Method Overall Single Two Object Object Counting Colors Position Attribute binding DALLE 2 [69] LDMv1.5 [73] LDMv2.1 [73] LDM-XL [63] PixArt-α [10] LDMv3 (5122) [22] CrossFlow 0.52 0.43 0.50 0.55 0.48 0.68 0.55 0.94 0.97 0.98 0.98 0.98 0.98 0.98 0.66 0.38 0.51 0.74 0.50 0.84 0. 0.49 0.35 0.44 0.39 0.44 0.66 0.39 0.77 0.76 0.85 0.85 0.80 0.74 0.82 0.10 0.04 0.07 0.15 0.08 0.40 0. 0.19 0.06 0.17 0.23 0.07 0.43 0.21 Table 7. GenEval comparisons. Our model achieves comparable performance to state-of-the-art models such as LDM-XL and DALLE 2, suggesting that CrossFlow is simple and promising direction for state-of-the-art media generation. For the KL loss LKL, we adopt the original KL divergence loss [42] with λ = 1 104. A.2. Experimental Details for Various Tasks Image captioning. We conduct our experiments on the popular Karpathy split [37] of COCO dataset [49], which contains 113, 287 images for training, 5, 000 images for validation, and 5, 000 image for testing. We train our model with 351M parameters on the training split for 100 epochs, using batch size of 256 and base learning rate of 2104 with 5 warm-up epochs. Following the standard evaluation setup, we compare the performance over five metrics: BLEU@4 [61] (B@4), METEOR [4] (M), ROUGE [48] (R), CIDEr [87] (C), and SPICE [3] (S). Monocular depth estimation. We consider KITTI [25] and NYUv2 [78] for outdoor and indoor depth estimation. For KITTI, we use the Eigen split [21], consisting of 23, 488 training images and 697 testing images. For NYUv2, we adopt the official split, which contains 24, 231 training images and 654 testing images. We train our model with 527M parameters on the corresponding training splits for 50 epochs. We use batch size of 64, and decay the learning rate from 1 104 to 1 108 with cosine annealing. Image super-resolution. We consider natural image superresolution, training our model on ImageNet 1K [16] for the task of 64 64 256 256 super-resolution. We use the dev split for evaluation. During training, we preprocess the images by removing those where the shorter side is less than 256 pixels. The remaining images are then centrally cropped and resized to 256 256. The low-resolution images are then generated by downsampling the 256256 images using bicubic interpolation with anti-aliasing enabled. For fair comparison with SR3 [75], we train our CrossFlow with 505M parameters (compared to 625M parameters in SR3). Our model is trained for 1M training steps with batch size of 512 and learning rate of 1 104, including 5, 000 warm-up steps. 12 Figure 6. Qualitative examples for zero-shot depth estimation. The input images in the first two rows are from the NYUv2 dataset, while the input images in the last row were generated by our T2I model. Our model provides robust zero-shot depth estimation across domains, whether indoor or outdoor, synthetic or real. B. Additional Experimental Results B.1. GenEval Performance To compare with recent text-to-image models on GenEval, we report the overall score and task-specific scores in Tab. 7. Our model achieves comparable performance to state-ofthe-art models such as LDMv2.1 [73], LDM-XL [63], and DALLE 2 [69]. This demonstrates that directly evolving from text space to image space with our approach is simple and effective solution for text-to-image generation, indicating novel and promising direction for state-of-the-art media generation. B.2. Zero-shot Depth Estimation We also evaluate CrossFlow on zero-shot depth estimation. Following Marigold [39], we train our model on Hypersim [72] and Virtual KITTI [9], and evaluate our model on 5 real datasets that are not seen during training: KITTI [25], NYUv2 [78], ETH3D [76], ScanNet [13], and DIODE [85]. We follow Marigold [39] to prepare the training and testing data. Our model with 527M parameters is trained for 150K training steps, with batch size of 512 and learning rate of 1 104 with 5, 000 warm-up steps. The results are reported in Tab. 8. Qualitative examples are provided in Fig. 6. Without specific design, CrossFlow achieves comparable or even superior performance compared to state-ofthe-art methods, demonstrating the general-purpose nature of our approach on various cross-modal tasks. B.3. Image Super-resolution We provide qualitative examples for image super-resolution in Fig. 7. Unlike traditional methods, which typically evolve from Gaussian noise and rely on concatenating upFigure 7. Qualitative examples for image super-resolution. sampled low-resolution images as conditioning, our approach takes more direct route: we demonstrate that it is possible to evolve low-resolution image directly into high-resolution image, eliminating the need for additional concatenation conditioning. B.4. Ablation Study Text compression. In this section, we show that we can compress the input text embedding Rnd into z0 Rhwc (e.g., 77 768 CLIP tokens to 4 32 32 latents for 256px generation, 14.4 compression) with standard encoder or the proposed Variational Encoder while preserve most of the input information. We report the per-token reconstruction accuracy, computed by cosine similarity, in Tab. 9. The results show that both methods are effective at preserving the input information, achieving high reconstruction accuracy despite large compression ratio. CFG indicator. In Fig. 8, we study the effect of our CFG with indicator, and then compare our approach with Autoguidance [38]. The left two columns show the images generated when the indicator 1c = 0 (for unconditional generation) and 1c = 1 (for conditional generation). It shows that despite generating an image by directly evolving from the text space into the image space without explicit conditioning, our model can still perform unconditional generation with the help of the indicator. This allows our model to support standard CFG. Then, in the middle five columns, we show the images generated with different CFG scaling factors. Similar to the standard flow matching model, the CFG can significantly improve the image quality. Finally, in the last two columns, we compare our CFG with indicator to Autoguidance, using the same scaling factor. Like our approach, Autoguidance also enables low-temperature sampling for models without explicit conditioning. We ob13 Method # Training samples KITTI NYUv2 ETH3D ScanNet DIODE AbsRel δ1 AbsRel δ1 AbsRel δ1 AbsRel δ1 AbsRel δ1 DiverseDepth [91] MiDaS [70] LeReS [92] Omnidata [20] HDN [94] DPT [71] Marigold [39] 320K 2M 300K + 54K 11.9M + 310K 300K 1.2M + 188K 74K CrossFlow (Ours) 74K 0.117 0.111 0.090 0.074 0.069 0.098 0.060 0.062 0.875 0.885 0.916 0.945 0.948 0.903 0. 0.956 0.190 0.236 0.149 0.149 0.115 0.100 0.105 0.103 0.704 0.630 0.784 0.835 0.867 0.901 0.904 0.908 0.228 0.184 0.171 0.166 0.121 0.078 0. 0.085 0.694 0.752 0.777 0.778 0.833 0.946 0.951 0.944 0.109 0.121 0.091 0.075 0.080 0.082 0.069 0.068 0.882 0.846 0.917 0.936 0.939 0.934 0. 0.942 0.376 0.332 0.271 0.339 0.246 0.182 0.310 0.270 0.631 0.715 0.766 0.742 0.780 0.758 0.772 0.768 Table 8. Zero-shot depth estimation. Baseline results are reported by Marigold [39]. We follow Marigold and train our CrossFlow on the same datasets, i.e., Hypersim [72] and Virtual KITTI [9]. We highlight the best, second best, and third best entries. With just unified framework, CrossFlow achieves comparable or even superior performance on complex zero-shot depth estimation, demonstrating the general-purpose nature of CrossFlow on various cross-modal tasks. Figure 8. Ablation on CFG with indicator. The first two columns show the images generated when the indicator 1c = 0 (for unconditional generation) and 1c = 1 (for conditional generation), demonstrating that CrossFlow can still perform unconditional generation with the help of the indicator, thereby allowing for the use of standard CFG. We then demonstrate the improvement provided by CFG (middle five columns) and compare it with Autoguidance (last two columns). Prompts used to generate the images: corgi wearing red hat in the park,a cat playing chess,a pair of headphones on guitar,a horse in red car Text encoder Recon. accuracy (%) C. Additional Qualitative Examples Text Encoder (1 1024) Text Variational Encoder (1 1024) 95.12 94. Table 9. Ablation on text compression. Both text encoder and Text Variational Encoder preserve most of the input information, despite the large compression ratio (77768 11024, 14.4). We provide additional qualitative examples for text-toimage generation here. Specifically, we first provide 512 512 images generated by our CrossFlow in Fig. 9. Next, we provide more examples for linear interpolation in latent space (Fig. 10 and Fig. 11) and arithmetic operation in latent space  (Fig. 12)  . serve that our CFG with indicator produces higher-fidelity images compared to Autoguidance. 14 Figure 9. Qualitative examples for text-to-image with CrossFlow. Figure 10. Linear interpolation in latent space. We show images generated by linear interpolation between two text latents (i.e., interpolation between z0). Images generated by the first and second text latents are provided in the top-left and bottom-right corners. 16 Figure 11. Linear interpolation in latent space. We show images generated by linear interpolation between two text latents (i.e., interpolation between z0). Images generated by the first and second text latents are provided in the top-left and bottom-right corners. Figure 12. Arithmetic in text latent space. We map the text into the text latent space, perform arithmetic operations to obtain new latent representation, and use the resulting representation to generate the image. Latent z0 used to generate each image is provided at the bottom."
        }
    ],
    "affiliations": [
        "GenAI, Meta",
        "Johns Hopkins University"
    ]
}