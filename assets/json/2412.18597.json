{
    "paper_title": "DiTCtrl: Exploring Attention Control in Multi-Modal Diffusion Transformer for Tuning-Free Multi-Prompt Longer Video Generation",
    "authors": [
        "Minghong Cai",
        "Xiaodong Cun",
        "Xiaoyu Li",
        "Wenze Liu",
        "Zhaoyang Zhang",
        "Yong Zhang",
        "Ying Shan",
        "Xiangyu Yue"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Sora-like video generation models have achieved remarkable progress with a Multi-Modal Diffusion Transformer MM-DiT architecture. However, the current video generation models predominantly focus on single-prompt, struggling to generate coherent scenes with multiple sequential prompts that better reflect real-world dynamic scenarios. While some pioneering works have explored multi-prompt video generation, they face significant challenges including strict training data requirements, weak prompt following, and unnatural transitions. To address these problems, we propose DiTCtrl, a training-free multi-prompt video generation method under MM-DiT architectures for the first time. Our key idea is to take the multi-prompt video generation task as temporal video editing with smooth transitions. To achieve this goal, we first analyze MM-DiT's attention mechanism, finding that the 3D full attention behaves similarly to that of the cross/self-attention blocks in the UNet-like diffusion models, enabling mask-guided precise semantic control across different prompts with attention sharing for multi-prompt video generation. Based on our careful design, the video generated by DiTCtrl achieves smooth transitions and consistent object motion given multiple sequential prompts without additional training. Besides, we also present MPVBench, a new benchmark specially designed for multi-prompt video generation to evaluate the performance of multi-prompt generation. Extensive experiments demonstrate that our method achieves state-of-the-art performance without additional training."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 4 2 ] . [ 1 7 9 5 8 1 . 2 1 4 2 : r DiTCtrl: Exploring Attention Control in Multi-Modal Diffusion Transformer for Tuning-Free Multi-Prompt Longer Video Generation Minghong Cai1 Xiaodong Cun2 Ying Shan3 Yong Zhang4 1 MMLab, The Chinese University of Hong Kong Xiaoyu Li3(cid:12) Zhaoyang Zhang3 Wenze Liu1 Xiangyu Yue1(cid:12) 2 GVC Lab, Great Bay University 3 ARC Lab, Tencent PCG 4 Tencent AI Lab tt s: / / hub.co m/ Tencen tARC /DiTCtr Figure 1. The proposed DiTCtrl takes multiple text prompts as input and demonstrates superior capability in generating longer videos with complex motion and smooth transitions as output. In this figure, we showcase challenging example where an athlete glides through three distinct scenes. Despite the complex subject motion and dramatic camera movement, our method maintains remarkable stability throughout the sequence and seamless semantic transitions that faithfully follow the prompt descriptions."
        },
        {
            "title": "Abstract",
            "content": "Sora-like video generation models have achieved remarkable progress with Multi-Modal Diffusion Transformer (MM-DiT) architecture. However, the current video generation models predominantly focus on single-prompt, struggling to generate coherent scenes with multiple sequential prompts that better reflect real-world dynamic scenarios. While some pioneering works have explored multi-prompt video generation, they face significant challenges including strict training data requirements, weak prompt following, and unnatural transitions. To address these problems, we propose DiTCtrl, training-free multi-prompt video generaWork done during an internship at Tencent ARC Lab. (cid:12) Corresponding Author tion method under MM-DiT architectures for the first time. Our key idea is to take the multi-prompt video generation task as temporal video editing with smooth transitions. To achieve this goal, we first analyze MM-DiTs attention mechanism, finding that the 3D full attention behaves similarly to that of the cross/self-attention blocks in the UNet-like diffusion models, enabling mask-guided precise semantic control across different prompts with attention sharing for multi-prompt video generation. Based on our careful design, the video generated by DiTCtrl achieves smooth transitions and consistent object motion given multiple sequential prompts without additional training. Besides, we also present MPVBench, new benchmark specially designed for multi-prompt video generation to evaluate the performance of multi-prompt generation. Extensive experiments demonstrate that our method achieves state-of-the-art performance 1 without additional training. 1. Introduction Text-to-video (T2V) generation has made remarkable progress in the AIGC era [10, 41, 47], and breakthroughs such as Sora [28] have demonstrated impressive capabilities in generating longer videos through DiT [30] architecture and large-scale per-taining. However, feeding sequential prompts into current state-of-the-art video generation models (e.g., Kling [3], Gen3 [2], CogVideoX [45]) fails to produce coherent video sequences that exhibit natural transitions with precise prompt-following. This limitation stems from their fundamental design and single-prompt training paradigm, making them inadequate for depicting real-world scenarios dynamic, multi-action nature. Although pioneering works [5, 27, 39] have begun exploring multi-prompt video generation, they face significant challenges. e.g., training such extended video generation models [27, 39] from scratch would require unprecedented computational resources and datasets that are practically unfeasible when the model size increases. Current zero-shot longer video generation methods [21, 33, 40] still mainly focus on the single prompt situation with longer length. Moreover, all previous works [5, 21, 27, 33, 39] are specifically designed under UNet architecture which restricts the abilities of more complex motions and increase the difficulties in multi-prompt generation. However, since Sora [28] groundbreaking demonstration of two-minute video generation, highlighting the scalability potential of DiT architectures [30]. Subsequent explorations have led to influential developments, notably in image generation models (Stable Diffusion 3 [13], FLUX.1 [6]) and video generations (CogVideoX [45], Mochi1 [14]). They [6, 13, 14, 45] all adopt specific kind of DiT architecture, i.e., MultiModal Diffusion Transformer (MM-DiT [13]) as the basic unit. This architecture effectively maps text and images (or video) into unified sequence for attention computation, enabling deeper model scale abilities and achieving superior performances. Thus, to keep the abilities of the pre-trained single prompt T2V model and take advantage of the performance of the diffusion transformer, we propose DiTCtrl, training-free multi-prompt video generation method under the pre-trained MM-DiT video generation model. Our key observation is that the multi-prompt video generation can be considered two-step problem: 1) Video editing over time: The new video is generated through the previous video with new prompt. 2) Video transition over time: Two generated videos need to keep smooth transition between clips. Thus, to perform consistent video editing, inspired by the UNet-based image editing techniques [9, 19], we explore the characteristic of the attention modules in the MM-DiT block for the first time, finding that the 3D full attention has similar behaviors to that of the cross-/self-attention blocks in the UNet-like diffusion models [10, 41]. We thus apply KV-sharing method between the video clips of different prompts to maintain the semantic consistency of the key objects [9] with the 3D attention control. Besides, we utilize latent blending strategy for transitions between clips to connect the video clip seamlessly. Finally, to systematically evaluate our method and facilitate future research in multi-prompt video generation, we also introduce MPVBench, new benchmark with diverse transition types and specialized metrics for assessing multi-prompt transitions. Extensive experiments on this benchmark demonstrate that our method achieves state-of-the-art performance while maintaining computational efficiency. The contributions of this paper can be summarized as: We propose DiTCtrl, the first tuning-free approach based on MM-DiT architecture for coherent multi-prompt video generation. Our method incorporates novel KV-sharing mechanism and latent blending strategy, enabling seamless transitions between different prompts without additional training. We pioneers the analysis of MM-DiTs attention mechanism, finding that 3D full atteniton has similar behaviors to that of the cross/self-attention blocks in the UNet-like diffusion models, enabling mask-guided precise semantic control across different prompts for enhanced generation consistency. We introduce MPVBench, new benchmark specially designed for multi-prompt video generation, featuring diverse transition types and specialized metrics for multiprompt video evaluation. Extensive experiments demonstrate that our method achieves state-of-the-art performance on multi-prompt video generation while maintaining computational efficiency. 2. Related Work Video Diffusion Model. Diffusion models have achieved significant success in the field of text-to-image generation [26, 3436], and these advancements have also propelled progress in video generation from text or images [4, 7, 8, 10, 11, 15, 16, 20, 37]. Among these methods, AnimateDiff [16] attempts to turn existing text-to-image diffusion models with motion module. Other models such as Imagen Video [20] and Make-a-Video [37] train cascade model of spatial and temporal layers directly in pixel space. To improve efficiency, many other works [4, 7, 8, 10, 11, 15] generate the videos in latent space, leveraging an auto-encoder to compress the video into compact latent. Notably, most of these text-to-video models utilize U-Net architecture. Subsequently, the introduction of Sora [28] demonstrates the scalability and advantages of diffusion transformer architecture [30]. Recent works such as CogVideoX [45], 2 Mochi1 [14], and Movie Gen [31] have adopted the DiT architecture and achieved impressive results. In this work, we build upon the open-source model CogVideoX [45], DiT-based architecture, to explore attention control mechanisms for multi-prompt long video generation. Long Video Generation. Training diffusion models on long videos often demand significant computational resources. Consequently, current video diffusion models are typically trained on videos with limited number of frames. As result, the quality of generated videos often degrades significantly during inference when generating longer videos. To address this problem, some works [17, 18, 39, 43] employ an autoregressive mechanism for long video generation. However, due to error accumulation, these methods often suffer from quality degradation after few iterations. Alternatively, tuning-free methods [5, 21, 33, 38, 40] have been developed to extend off-the-shelf short-video diffusion models for generating long videos without additional training. For instance, Gen-L-Video [40] processes long videos as short video clips with temporal overlapping during the denoising process. FreeNoise [33] explores the influence of initial noises and conducts temporal attention fusion based on the sliding window for temporal consistency. Inspired by these works, we propose novel KV-sharing mechanism and latent blending strategy for seamless transitions between different segments without additional training. Image/Video Editing with Attention Control. Attention control is gaining popularity due to its ability to perform zeroshot image or video editing without the need for additional data. In the realm of image editing, MasaCtrl [9] enhances the existing self-attention mechanism in diffusion models by introducing mutual self-attention. This allows for querying correlated content and textures from source images, ensuring consistent and coherent edits. Prompt-to-Prompt [19] utilizes cross-attention layers to control the relation between text prompts and images, which has also been adopted in many image editing works [12, 29, 44]. When it comes to video editing [22, 23, 32], temporal consistency needs to be considered during attention control. Video-P2P [23] extend the cross-attention control from Prompt-to-Prompt to video editing. FateZero [32] fuses self-attention with blending mask obtained by cross-attention features from the source prompt. However, all these works are designed for video-to-video translation editing with structure preservation. Differently, we aim for appearance-consistent video editing. Besides, none of these works explore attention control in diffusion transformers. In this paper, we are the first to analyze how the full attention in diffusion transformers could be utilized for editing, similar to U-Net diffusion models. Figure 2. MM-DiT Attention Analysis. We find the attention matrix in MM-DiT attention can be divided into four different regions. As for the prompt of cat watch black mouse, each text token shows high-light response using the average of the text-to-video and video-to-text attention. 3. Method We tackle the challenge of zero-shot, multi-prompt longer video generation without the need for model training or optimization. This allows us to generate high-quality videos with smooth and precise inter-prompt transitions, covering various transition types (e.g., style, camera movement, and location changes). Formally, given pre-trained single prompt text-to-video diffusion model and sequence of prompts {P1, P2, ..., Pn}, the proposed DiTCtrl can generate coherent longer video V{1,...,n} that faithfully follows these prompts over time, which can be formulated as: V{1,...,n} = DiTCtrl{F(P1), ..., F(Pn)}. (1) Below, we first give careful analysis of MM-DiTs attention mechanisms (Sec. 3.1). This analysis enables us to design mask-guided full-attention KV-sharing mechanism for video editing over time (Sec. 3.2) in multi-prompt video generation. Finally, to ensure temporal coherence across different semantic segments, we further incorporate latent blending strategy that enables smooth transitions in longer videos with multiple prompts (Sec. 3.3). 3.1. MM-DiT Attention Mechanism Analysis The MM-DiT is the fundamental architecture of the current SOTA method of Text-to-image/Video models [6, 13, 14, 45], which is fundamentally distinct from prior UNet architectures since it maps text and videos into unified sequence for attention computation. Although it has been widely utilized, the properties of its inner attention mechanism remain insufficiently explored, which restricts its applications in our 3 multi-prompt longer video generation task. Therefore, for the first time, we conducted comprehensive analysis of the regional attention patterns in the 3D full attention map based on the state-of-the-art video model, i.e. CogVideoX [45]. As shown in Fig. 2, due to the concatenation of the vision and text prompt, each attention matrix can be decomposed into four distinct regions, corresponding to different attention operations: video-to-video attention, text-to-text attention, text-to-video attention, and video-to-text attention. Below, we give the details of each region-inspired previous UNetlike structure with individual attentions [19]. Text-to-Video and Video-to-Text Attention. Previous UNet-like architectures incorporate cross-attention for videotext alignment. In MM-DiT, the text-to-video and video-totext attention play similar role. To validate its efficiency, we conduct detailed analysis of the attention patterns, as illustrated in Fig. 2. Specifically, we compute the averaged attention values across all layers and attention heads, then extract attention values by selecting specific columns or rows corresponding to token indices in both text-to-video and video-to-text regions. These attention values are subsequently reshaped into an format, allowing us to visualize the semantic activation maps for individual frames. As demonstrated in Fig. 2, these visualizations show remarkable precision in token-level semantic localization, effectively capturing fine-grained relationships between textual descriptions and visual elements. This discovered capability for precise semantic control and localization provides strong foundation for adapting established image/video editing techniques to enhance the consistency and quality of multi-prompt video generation. Text-to-Text and Video-to-Video Attention. Text-totext and video-to-video regional attention are somehow new from the respective UNet structure. As illustrated in Fig. 3, our analysis reveals similar patterns in both components. In the text-to-text attention component (Fig. 3(a)(b), where (a) represents the attention pattern for shorter prompts and (b) illustrates the pattern for longer prompts), we observe prominent diagonal pattern, indicating that each text token primarily attends to its neighboring tokens. Notably, there are distinct vertical lines that shift backward as the text sequence length increases, suggesting that all tokens maintain significant attention to the special tokens at the end of the text sequence. For the video-to-video attention component, since MMDiT flat the spatial and temporal token for 3D attention calculation, our analysis at the single-frame level reveals distinctive diagonal pattern in spatial attention (Fig. 3(c)). More significantly, when examining attention maps constructed from tokens at identical spatial positions across different frames, we also observe pronounced diagonal pattern (Fig. 3(d)). This characteristic mirrors those found in recent UNet-based video models of the spatialFigure 3. MM-DiT Text-to-Text and Video-to-Video Attention Visualization. We find that the current MM-DiT has stronger potential to construct the individual attention in the previous UNetlike structure [10, 11, 41]. attention and temporal attention, such as VideoCrafter [24] and Lavie [42], aligning with the findings reported in [25]. Since previous works only train the specific part of the diffusion model for more advanced control and generation, our finding provides strong evidence for these methods from MM-DiT perspectives. Overall, the presence of these consistent diagonal patterns in the MM-DiT architecture demonstrates robust frame-toframe correlations, which proves essential for maintaining spatial-temporal coherence and preserving motion fidelity throughout the video sequence. 3.2. Consistent Video Generation Over Time Based on the previous analysis, we find the attention mechanism in the MM-DiT has similar behavior as that in the UNet-like video diffusion model with our specific design. Thus, we propose the masked-guided KV-sharing strategy for consistent video generation over time for our multi-prompt video generation task. Specifically, as shown in Fig. 4, to generate the consistent video between prompt Pi1 and prompt Pi, we utilize the intermediate attentions from the 1-th and i-th prompt in MM-DiT to generate the attention mask of the specific consistent object, respectively. This is achieved by averaging all the text-to-video/video-to-text parts of the 3D full attention with the given specific subject token. With these masks, we then perform mask-guided attention fusion to generate the 4 Figure 4. Pipeline of the proposed DiTCtrl. Our method tries to synthesize content-consistent and motion-consistent videos based on multi-prompts. The first video is synthesized with source text prompt Pi1. During the denoising process for video synthesis, we convert the full-attention into masked-guided KV-sharing strategy to query video contents from source video Vi1, so that we can synthesize content-consistent video under the modified target prompt Pi. Note that initial latents are assumed to be 5 frames. The first three frames are used to generate the contents of Pi1, and the last three frames are used to generate contents of Pi. The pink latent represents the overlapping frame, while the blue and green latents are used to distinguish different prompt segments. new attention features of the prompt Pi. Inspired by MasaCtrl [9], we directly utilize the key and values from the prompt Pi1 to guide the generation of prompt Pi to generate the consistent appearance over time. Formally, at step t, we perform forward pass with the fixed MM-DiT backbone with prompt Pi1 and next prompt Pi, respectively, to generate intermediate regional crossattention maps. Then we average the attention maps across all heads and layers with the same spatial resolution and temporal frames . The resulting cross-attention maps RF HW , where is the number are denoted as Ac of the textual tokens. We then obtain the averaged crossattention map for the token correlated to the foreground object. We denote Mi1 and Mi as masks extracted for the foreground objects in Vi1 and Vi, respectively. With these masks, we can restrict the object in Vi to query contents information only from the object region in Vi1: = Attention(Ql = Attention(Ql f = Mi + i, i1, i1, i, (1 Mi), i1; Mi1), i1; 1 Mi1), (2) (3) (4) where is the final attention output. Then, we replace the feature map of the current step to for further calculation. coherence across different semantic segments, inspired by recent works in single-prompt long video generation [33, 46]. As illustrated in Fig. 5, our approach introduces overlapped regions between adjacent semantic video segments (video Vi1 and video Vi). For each frame position in the overlapped region, we apply position-dependent weight function that follows symmetric distribution - frames closer to their respective segments receive higher weights while those at the boundaries receive lower weights. This weighting scheme ensures smooth transitions between different semantic contexts. Formally, given two adjacent video segments Vi1 and Vi generated from prompts Pi1 and Pi respectively, we propose latent blending strategy as follows. Let denote the number of overlapped frames between segments. For frame position in the overlapped region, we compute its blended latent features zt as: zt = w(t) zt,i1 + (1 w(t)) zt,i, (5) where zt,i1 and zt,i are latent features from Vi1 and Vi respectively, and w(t) is position-dependent triangular weight function defined as: 3.3. Latent Blending Strategy for Transition w(t) = min (cid:18) 2(t + 0.5) , 2 (cid:19) 2(t + 0.5) , [0, 1] While our previous methods enable semantic consistency between clips, achieving smooth transitions between different semantic segments still needs to be carefully designed. Thus, we propose latent blending strategy to ensure temporal (6) The key advantage of our approach is that it requires no additional training while effectively handling transitions between different semantic contexts. During each denoising between segments. Our proposed DiTCtrl method preserves the inherent capabilities of the pre-trained DiT model while addressing these limitations, enabling smooth semantic transitions and maintaining motion coherence throughout the video sequence. For more comprehensive evaluation, we provide additional frame-level and video-level comparisons with extensive qualitative examples in the supp. 4.2. Quantitative Results In this section, we will first elaborate on our proposed new benchmark MPVBench for evaluating multi-prompt video generation, and then discuss the quantitative results. MPVBench. MPVBench contains diverse prompt dataset and new metric customized for multi-prompt generation. Specifically, leveraging GPT-4, we produce 130 long-form prompts of 10 different transition modes. Then, for multiprompt video generation, we observe that the distribution of the CLIP features differs between single-prompt and multiprompt scenarios. As shown in Fig. 7, the feature points of natural video follow continuous curve, while those of two concatenated isolated videos follow two continuous curves with breakpoint in the middle. Since the common CLIP similarity calculates the average of neighborhood similarities, the difference between natural and isolated video only occurs at the breakpoint, which becomes very small when divided by the number of frames. To address this limitation, we propose CSCV (Clip Similarity Coefficient of Variation), metric specifically designed to evaluate the smoothness of multi-prompt transitions, defined as: si = xi+1, = 1, . . . , score = 1 1 + λ σ(s) µ(s) , (7) (8) 1 1+λ() where xi denotes frame features, σ and µ are standard deviation and average respectively. The Coefficient of Variation CV = σ(s)/µ(s) describes the degree of uniformity, which can largely punish the isolated situation. The function projects the score to [0, 1], the larger the better. Automatic Evaluation. We conduct the automatic evaluation with our MPVBench. From Table 1 one can see that our method achieves the highest CSCV score, demonstrating superior transition handling and overall stability in generation patterns. While FreeNoise ranks second with relatively strong stability, other methods significantly lag behind in this aspect, which is consistent with the T-SNE visualization of CLIP embedding as shown in Fig. 7. In terms of motion smoothness, our approach exhibits superior performance in motion quality and consistency. Regarding Text-Image Similarity metrics, although FreeNoise and Video-Infinity achieve higher scores, this can be attributed to our methods kv-sharing mechanism, where subsequent video segments inherently learn from preceding semantic content. Figure 5. Latent blending strategy for video transition between video clips. step, we first process each segment independently, then progressively blend the latent features in the overlapped regions using position-dependent weights. This strategy maintains temporal coherence while smoothly transitioning between different semantic contexts, making it particularly suitable for multi-prompt video generation tasks. 4. Experiments We implement DiTCtrl based on CogVideoX-2B [45], which is state-of-the-art open-source text-to-video diffusion model based on MM-DiT. In our experiments, we generate multi-prompt conditioned video, and each video clip consists of 49 frames with 480 720 resolution. Moreover, we employ ChatGPT [1], to generate multiple transitions of different types. We set latent sampling frames and overlap sizes to 13 and 6 in our experiments. The experiments are conducted on single NVIDIA A100 GPU. 4.1. Qualitative Results We conduct comprehensive qualitative comparisons with state-of-the-art multi-prompt video generation methods [33, 38, 40] and leading commercial solutions [3]. To ensure fair comparison, we additionally implement FreeNoise [33] on the CogVideoX backbone as an enhanced baseline. As shown in Fig. 6, our proposed method demonstrates superior performance across three critical aspects: text-to-video alignment, temporal coherence, and motion quality. While Kling [3] exhibits impressive capabilities in high-quality video generation, it is limited to simultaneous multi-semantic blending rather than sequential semantic transitions, highlighting the significance of our work in achieving temporally evolving content across multiple prompts. Our comparative analysis reveals distinct characteristics and limitations of existing approaches. Gen-L-Video [40] suffers from severe temporal jittering and occasional subject disappearance, compromising overall video quality. VideoInfinity [38] and FreeNoise [33] both demonstrate successful scene-level semantic changes but lack physically plausible motion - for instance, in Fig. 6, vehicles appear to be in motion while remaining spatially fixed, which is limitation inherent to their UNet-based abilities. In contrast, FreeNoise+DiT leverages the DiT architectures abilities to achieve more realistic object motion but struggles with semantic transitions, resulting in noticeable discontinuities 6 Figure 6. Generation results on given prompts by our method and baseline models. Kling is the commercial model, and Freenoise+DiT is our implementation of Freenoise on CogVideoX. Method Gen-L-Video FreeNoise FreeNoise+DiT Video-Infinity DiTCtrl(w/o kv-sharing) DiTCtrl(Ours) CSCV Motion smoothness 83.36% 59.44% 97.22% 84.37% 97.76% 78.74% 97.31% 74.97% 81.79% 97.35% 84.90% 97.80% Text-Image similarity 30.58% 32.69% 30.90% 32.35% 31.37% 30.68% Table 1. Evaluation metrics. Comparison of performance metrics for various video generation methods as benchmarked by MPVBench. Bold values represent the best performance within each group. text alignment over 16 videos generated by different scenarios. As clearly indicated in Table 2, generated videos from our method significantly outperform other state-of-the-art approaches in all four criteria, demonstrating superior capability in producing videos with natural semantic transitions that better align with human preferences for visual coherence and continuity. More details will be provided in the supplementary material. 4.3. Ablation Study We conducted ablation studies to validate the effectiveness of DiTCtrls key components: latent blending strategy, KVsharing mechanism, and mask-guided generation as shown in Fig. 8. The first row shows results that directly using text-to-video models results in abrupt scene changes and disconnected motion patterns, failing to maintain continuity in the athletes movements from surfing to skiing. The second row demonstrates that DiTCtrl without the latent blending strategy achieves basic video editing capabilities Figure 7. T-SNE visualization of CLIP embeddings. Each point represents the CLIP embedding of single video frame after dimensionality reduction. The visualization demonstrates that conventional multi-prompt videos form distinct clusters, while our method produces more continuous distribution, indicating smoother semantic transitions. As shown in Fig. 6, our design choice allows the road surface to gradually transition to snowy conditions while retaining features from the previous scene. Despite potentially lower text-image alignment scores, it ensures superior semantic continuity in the sequences. In practice, this trade-off doesnt negatively impact the visual quality in multi-prompt scenarios, as demonstrated by our user study results presented below. Human Evaluation. We invited 28 users to evaluate five models: Gen-L-Video [40], Video-Infinity [38], FreeNoise [33], FreeNoise+DiT and our method. We employ Likert scale ranging from 1 (low quality) to 5 (high quality). Participants score each method considering overall preference, motion pattern, temporal consistency and Figure 8. Ablation Component in DiTCtrl. The first and second rows have 98 frames, while the remaining methods generate 105 frames. Method Gen-L-Video FreeNoise FreeNoise+DiT Video-Infinity DiTCtrl(Ours) Overall Motion Temporal Text preference Pattern Consistency Alignment 1.15 3.02 3.81 2.90 4.11 1.14 2.90 3.93 2.85 4.17 1.08 2.99 3.75 2.91 4.26 1.25 3.08 3.78 2.98 3.91 Table 2. User study. Human evaluation of different video generation methods across multiple aspects. Scores range from 1 to 5, with higher scores indicating better performance. Bold values represent the best performance within each metric. Figure 9. single prompt longer video generation example. mechanism and its role in enabling accurate semantic control. but lacks smooth transitions between scenes. Without KVsharing (third row), DiTCtrl exhibits unstable environmental transitions and significant motion artifacts, with inconsistent character scaling and deformed movements. Moreover, DiTCtrl without mask guidance (fourth row) improves motion coherence and transitions but struggles with object attribute confusion across different prompts and environments. On the other hand, The full DiTCtrl implementation provides the most precise control over generated content, demonstrating superior object consistency and smoother transitions between prompts while maintaining desired motion patterns. These results validate our analysis of MM-DiTs attention 4.4. More Applications Single-prompt Longer Video Generation. Our method can naturally work on single-prompt longer video generation. As illustrated in Fig. 9, using the prompt white SUV drives on steep dirt road, our approach successfully generates videos that are more than 12 times longer than the original length, while maintaining consistent motion patterns and environmental coherence. Video Editing. We show how we use our methods to achieve video editing performance (e.g. reweight and word swap). The cases are provided in the Appendix. C. 8 5. Conclusion"
        },
        {
            "title": "References",
            "content": "In this paper, we introduce DiTCtrl, novel, tuning-free method for multi-prompt video generation using the MMDiT architecture. Our pioneering analysis of MM-DiTs attention mechanism reveals similarities with the cross/selfattention blocks in UNet-like diffusion models, enabling mask-guided semantic control across prompts. With KVsharing mechanisms and latent blending strategies, DiTCtrl ensures smooth transitions and consistent object motion between semantic segments, without extra training. We also present MPVBench, the first extensive evaluation framework for multi-prompt video generation, set to advance future research in this field. Limitation & Future Work. While our method demonstrates state-of-the-art performance, there remain two primary limitations. First, compared to image generation models, current open-source video generation models exhibit relatively weaker conceptual composition capabilities, occasionally resulting in attribute binding errors across different semantic segments. Second, the computational overhead of DiT-based architectures presents challenges for inference speed. These limitations suggest promising directions for future research in enhancing semantic understanding and architectural efficiency. [1] Chatgpt. https://openai.com/chatgpt/, 2022. 6 [2] Gen-3. https : / / runwayml . com / research / introducing-gen-3-alpha,, 2024. 2 [3] Kling. https://kling.kuaishou.com/en, 2024. 2, 6, 12, [4] Jie An, Songyang Zhang, Harry Yang, Sonal Gupta, Jia-Bin Huang, Jiebo Luo, and Xi Yin. Latent-shift: Latent diffusion with temporal shift for efficient text-to-video generation. arXiv preprint arXiv:2304.08477, 2023. 2 [5] Hritik Bansal, Yonatan Bitton, Michal Yarom, Idan Szpektor, Aditya Grover, and Kai-Wei Chang. Talc: Time-aligned captions for multi-scene text-to-video generation. arXiv preprint arXiv:2405.04682, 2024. 2, 3 [6] Black Forest Labs. Announcing black forest labs. https: / / blackforestlabs . ai / announcing - black - forest-labs/, 2023. Accessed: 2024-4. 2, 3 [7] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. 2 [8] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis. Align your latents: High-resolution video synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2256322575, 2023. 2 [9] Mingdeng Cao, Xintao Wang, Zhongang Qi, Ying Shan, Xiaohu Qie, and Yinqiang Zheng. Masactrl: Tuning-free mutual self-attention control for consistent image synthesis and editing. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2256022570, 2023. 2, 3, [10] Haoxin Chen, Menghan Xia, Yingqing He, Yong Zhang, Xiaodong Cun, Shaoshu Yang, Jinbo Xing, Yaofang Liu, Qifeng Chen, Xintao Wang, Chao Weng, and Ying Shan. Videocrafter1: Open diffusion models for high-quality video generation, 2023. 2, 4 [11] Haoxin Chen, Yong Zhang, Xiaodong Cun, Menghan Xia, Xintao Wang, Chao Weng, and Ying Shan. Videocrafter2: Overcoming data limitations for high-quality video diffusion models, 2024. 2, 4, 12 [12] Minghao Chen, Iro Laina, and Andrea Vedaldi. Training-free In Proceedlayout control with cross-attention guidance. ings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 53435353, 2024. 3 [13] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, Dustin Podell, Tim Dockhorn, Zion English, and Robin Rombach. Scaling rectified flow transformers for high-resolution image synthesis. In Ruslan Salakhutdinov, Zico Kolter, Katherine Heller, Adrian Weller, Nuria Oliver, Jonathan Scarlett, and Felix Berkenkamp, editors, Proceedings of the 41st International Conference on Machine Learning, volume 235 of Proceedings of Machine Learning Research, pages 1260612633, 2127 Jul 2024. 2, 3 9 [14] Genmo. Mochi 1: new sota in open-source video generation models. https://www.genmo.ai/blog/, 2023. Accessed: 2024-10. 2, [15] Rohit Girdhar, Mannat Singh, Andrew Brown, Quentin Duval, Samaneh Azadi, Sai Saketh Rambhatla, Akbar Shah, Xi Yin, Devi Parikh, and Ishan Misra. Emu video: Factorizing textto-video generation by explicit image conditioning. arXiv preprint arXiv:2311.10709, 2023. 2 [16] Yuwei Guo, Ceyuan Yang, Anyi Rao, Zhengyang Liang, Yaohui Wang, Yu Qiao, Maneesh Agrawala, Dahua Lin, and Bo Dai. Animatediff: Animate your personalized text-to-image diffusion models without specific tuning. International Conference on Learning Representations, 2024. 2 [17] Yingqing He, Tianyu Yang, Yong Zhang, Ying Shan, and Qifeng Chen. Latent video diffusion models for high-fidelity long video generation. arXiv preprint arXiv:2211.13221, 2022. 3 [18] Roberto Henschel, Levon Khachatryan, Daniil Hayrapetyan, Hayk Poghosyan, Vahram Tadevosyan, Zhangyang Wang, Shant Navasardyan, and Humphrey Shi. Streamingt2v: Consistent, dynamic, and extendable long video generation from text. arXiv preprint arXiv:2403.14773, 2024. 3 [19] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Prompt-to-prompt image editing with cross attention control. arXiv preprint arXiv:2208.01626, 2022. 2, 3, 4, 15 [20] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik Kingma, Ben Poole, Imagen video: Mohammad Norouzi, David Fleet, et al. High definition video generation with diffusion models. arXiv preprint arXiv:2210.02303, 2022. [21] Jihwan Kim, Junoh Kang, Jinyoung Choi, and Bohyung Han. Fifo-diffusion: Generating infinite videos from text without training. arXiv preprint arXiv:2405.11473, 2024. 2, 3 [22] Zhenyi Liao and Zhijie Deng. Lovecon: Text-driven trainingarXiv preprint free long video editing with controlnet. arXiv:2310.09711, 2023. 3 [23] Shaoteng Liu, Yuechen Zhang, Wenbo Li, Zhe Lin, and Jiaya Jia. Video-p2p: Video editing with cross-attention control, 2023. 3 [24] Yaofang Liu, Xiaodong Cun, Xuebo Liu, Xintao Wang, Yong Zhang, Haoxin Chen, Yang Liu, Tieyong Zeng, Raymond Chan, and Ying Shan. Evalcrafter: Benchmarking and evalIn Proceedings of uating large video generation models. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2213922149, 2024. 4 [25] Yu Lu, Yuanzhi Liang, Linchao Zhu, and Yi Yang. Freelong: Training-free long video generation with spectralblend temporal attention. arXiv preprint arXiv:2407.19918, 2024. 4 [26] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. arXiv preprint arXiv:2112.10741, 2021. [27] Gyeongrok Oh, Jaehwan Jeong, Sieun Kim, Wonmin Byeon, Jinkyu Kim, Sungwoong Kim, Hyeokmin Kwon, and Sangpil Kim. Mtvg: Multi-text video generation with text-to-video models. arXiv preprint arXiv:2312.04086, 2023. 2 [28] OpenAI. Video generation models as world simuhttps : / / openai . com / index / video - lators. generation - models - as - world - simulators/, 2023. Accessed: 2024-2. 2 [29] Gaurav Parmar, Krishna Kumar Singh, Richard Zhang, Yijun Li, Jingwan Lu, and Jun-Yan Zhu. Zero-shot image-to-image translation. In ACM SIGGRAPH 2023 Conference Proceedings, pages 111, 2023. 3 [30] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 41954205, October 2023. 2 [31] Adam Polyak, Amit Zohar, Andrew Brown, Andros Tjandra, Animesh Sinha, Ann Lee, Apoorv Vyas, Bowen Shi, ChihYao Ma, Ching-Yao Chuang, et al. Movie gen: cast of media foundation models. arXiv preprint arXiv:2410.13720, 2024. [32] Chenyang Qi, Xiaodong Cun, Yong Zhang, Chenyang Lei, Xintao Wang, Ying Shan, and Qifeng Chen. Fatezero: Fusing attentions for zero-shot text-based video editing. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1593215942, 2023. 3 [33] Haonan Qiu, Menghan Xia, Yong Zhang, Yingqing He, Xintao Wang, Ying Shan, and Ziwei Liu. Freenoise: Tuning-free longer video diffusion via noise rescheduling. arXiv preprint arXiv:2310.15169, 2023. 2, 3, 5, 6, 7, 12, 13 [34] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 1(2):3, 2022. 2 [35] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. [36] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in Neural Information Processing Systems, 35:3647936494, 2022. 2 [37] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, et al. Make-a-video: Text-to-video generation without text-video data. arXiv preprint arXiv:2209.14792, 2022. [38] Zhenxiong Tan, Xingyi Yang, Songhua Liu, and Xinchao Wang. Video-infinity: Distributed long video generation. arXiv preprint arXiv:2406.16260, 2024. 3, 6, 7, 12, 13 [39] Ruben Villegas, Mohammad Babaeizadeh, Pieter-Jan Kindermans, Hernan Moraldo, Han Zhang, Mohammad Taghi Saffar, Santiago Castro, Julius Kunze, and Dumitru Erhan. Phenaki: Variable length video generation from open domain textual description. arXiv preprint arXiv:2210.02399, 2022. 2, 3 [40] Fu-Yun Wang, Wenshuo Chen, Guanglu Song, Han-Jia Ye, Yu Liu, and Hongsheng Li. Gen-l-video: Multi-text to long video generation via temporal co-denoising. arXiv preprint arXiv:2305.18264, 2023. 2, 3, 6, 7, 12, 13 [41] Jiuniu Wang, Hangjie Yuan, Dayou Chen, Yingya Zhang, Xiang Wang, and Shiwei Zhang. Modelscope text-to-video 10 technical report. arXiv preprint arXiv:2308.06571, 2023. 2, 4 [42] Yaohui Wang, Xinyuan Chen, Xin Ma, Shangchen Zhou, Ziqi Huang, Yi Wang, Ceyuan Yang, Yinan He, Jiashuo Yu, Peiqing Yang, et al. Lavie: High-quality video generation with cascaded latent diffusion models. arXiv preprint arXiv:2309.15103, 2023. 4 [43] Chenfei Wu, Jian Liang, Xiaowei Hu, Zhe Gan, Jianfeng Wang, Lijuan Wang, Zicheng Liu, Yuejian Fang, and Nan Duan. Nuwa-infinity: Autoregressive over autoregressive arXiv preprint generation for infinite visual synthesis. arXiv:2207.09814, 2022. 3 [44] Fei Yang, Shiqi Yang, Muhammad Atif Butt, Joost van de Weijer, et al. Dynamic prompt learning: Addressing crossattention leakage for text-based image editing. Advances in Neural Information Processing Systems, 36:2629126303, 2023. [45] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. 2, 3, 4, 6, 12 [46] Yuang Zhang, Jiaxi Gu, Li-Wen Wang, Han Wang, Junqi Cheng, Yuefeng Zhu, and Fangyuan Zou. Mimicmotion: High-quality human motion video generation with confidenceaware pose guidance. arXiv preprint arXiv:2406.19680, 2024. 5 [47] Daquan Zhou, Weimin Wang, Hanshu Yan, Weiwei Lv, Yizhe Zhu, and Jiashi Feng. Magicvideo: Efficient video arXiv preprint generation with latent diffusion models. arXiv:2211.11018, 2022."
        },
        {
            "title": "Overview",
            "content": "This supplementary material presents comprehensive experimental details, qualitative analyses, and technical implementations of our work. We provide extensive evaluations across multiple aspects, including baseline comparisons, diverse application scenarios, and ablation studies. Note that our project page shows many cases of our results, comparison and diverse application scenarios. The content is organized into five main sections: Section details our experimental framework, including baseline implementations, evaluation metrics, and human evaluation protocols. Section showcases comprehensive qualitative results across diverse domains, featuring detailed comparisons with state-of-the-art models and demonstrating the versatility of our approach. Section explores various applications, including singleprompt video generation and advanced editing capabilities such as attention reweighting and word swap techniques. Section presents the usage of prompt generator, including full descriptions used to generate individual prompts. Section presents comprehensive ablation studies, including both quantitative evaluations and qualitative analyses of the masking mechanism. A. Implementation Details Details. We implement DiTCtrl based on CogVideoX2B [45], which is state-of-the-art open-source text-to-video diffusion model based on MM-DiT. The hyperparameters and implementation details are shown in Tab. 3. Table 3. Hyperparameters of DiTCtrl. Hyperparameters base model sampler sample step guidance scale resolution sampling num frames overlap size kv-sharing steps kv-sharing layers threshold λ of CSCV CogVideoX-2B VPSDEDPMPP2MSampler 50 6 480 720 13 6 [2,25] [25,30] 0.3 10 Baselines. To demonstrate the effectiveness of our proposed DiTCtrl, we conduct comprehensive qualitative comparisons with state-of-the-art multi-prompt video generation methods [33, 38, 40] and leading commercial solutions Kling [3]. Both FreeNoise and Video-Infinity are built upon the VideoCrafter2 [11] framework. To ensure fair comparison of base models, we implement FreeNoise [33] as an enhanced baseline by directly incorporating their noise rescheduling strategy into the CogVideoX framework. MPVBench. We introduces new benchmark MPVBench, which is specified designed for multi-prompt video generation task. MPVBench contains diverse prompt dataset and new metric customized for multi-prompt generation. Specifically, leveraging GPT-4, we produce 130 long-form prompts of 10 different transition modes (background transition, subject transition, camera transition, style transition, lighting transition, location transition, speed transition, emotion transition, clothing transition, action transition). The instruction of prompt generator is provided in Fig. 19. Automatic evaluation. For automatic evaluation, we generate videos using 130 prompts from our MPVBench, with three random seeds set. Then, we evaluate the generated video by three metrics: CSCV (Clip Similarity Coefficient of Variation), Motion Smoothness, Text-Image Similarity. Human evaluation. In our user study, we combined our generated videos with those produced by four other baseline methods. We asked total of 28 participants to evaluate the videos across four dimensions: overall preference, motion pattern, temporal consistency, and text alignment. Specifically, we asked all participants to rank the results of these methods for each of the following questions, and assigned score from 1 (lowest quality) to 5 (highest quality) for these five methods: Overall Preference: Please rank the overall video preference. This metric evaluates participants comprehensive assessment of the generated videos. Motion Pattern: How natural and realistic are the motion in the video? This evaluates whether the motion of objects in the generated video appears physically plausible and natural, such as whether vehicles drive realistically, animals move naturally, or human actions appear authentic. Temporal Consistency: How smoothly does the video content transition across different frames? This metric evaluates the temporal coherence of the generated video, focusing on whether the transitions between consecutive frames are natural and continuous, without abrupt changes or visual artifacts. It measures the videos ability to maintain visual continuity throughout its duration. Text Alignment: To what extent does the video content match the given text descriptions? This assesses the semantic fidelity between the generated visual content and the input text prompts, examining whether the video accurately captures and visualizes the key elements and actions described in the prompts. It measures how well the visual narrative aligns with the intended textual description. Mask-guided Implementation Details. We show how mask extracted from MM-DiT attention map is utilized for mask12 Figure 10. Mask-guided KV-sharing details. guided KV-sharing strategy in Fig. 10, to generate consistent video over time for multi-prompt video generation task. Specifically, Fig. 10 illustrates our approach to generating temporally consistent videos in multi-prompt video generation tasks. When computing attention for the Pi branch latent, we utilize attention maps from both Pi1 and Pi branches. Specifically, we extract content from the Text-video and Video-text attention regions of their attention maps. By focusing on specified tokens (e.g., running horse), we obtain and average the corresponding regional values to generate semantic mask maps. These maps are then binarized through thresholding to create foreground-background segmentation masks Mi1 and Mi. Then, we leverage Mi1 to guide the computation of KV-sharing attention maps (calculating attention between Qi and Ki1, Vi1), resulting in foreground-focused attention outputs Ff ore and Fback. The final fusion is achieved through Mi as follows: Ff usion = Ff ore Mi + Fback (1 Mi) (9) This mask-guided approach ensures semantic consistency while maintaining smooth transitions between different prompts. B. More Qualitative Results More results are provided in Fig. 11 and Fig. 12. Our method DiTCtrl can generate multi-prompt videos with good tem13 poral consistency and strong prompt-following capabilities, demonstrating cinematographic-style transitions in depicting the boys riding sequence. We also give more qualitative comparisons with state-of-the-art multi-prompt video generation methods [33, 38, 40], our reproduced FreeNoise+DiT, and leading commercial solutions Kling [3]. We show the motion transition case, and background transition case in Fig. 13 and Fig. 14. Our comparative analysis reveals distinct characteristics and limitations of existing approaches. GenL-Video [40] suffers from severe temporal jittering and occasional subject disappearance, compromising overall video quality. Video-Infinity [38] and FreeNoise [33] both demonstrate successful scene-level semantic changes but lack physically plausible motion - for instance, in Fig. 13, dark knight appear to be in motion while remaining spatially fixed, which is limitation inherent to their UNet-based abilities. In contrast, FreeNoise+DiT leverages the DiT architectures abilities to achieve more realistic object motion but struggles with semantic transitions, resulting in noticeable discontinuities between segments. Our proposed DiTCtrl method preserves the inherent capabilities of the pre-trained DiT model while addressing these limitations, enabling smooth semantic transitions and maintaining motion coherence throughout the video sequence. More comparison of visualization case and our results are shown in our project page. C. Applications C.2. Video Editing Figure 11. More multi-prompt results Based on our exhaustive analysis and exploration of attention control in MM-DiT architecture, our method could be applied to other tasks like single prompt longer video generation and video editing and achieves promising results. C.1. Single-prompt Longer Video Generation Although our primary objective is to address multi-prompt video generation, we discover that our method demonstrates remarkable effectiveness in single-prompt longer video generation as well. Our method can naturally work on singleprompt longer video generation. As illustrated in Fig. 15, our approach successfully generates longer videos, while maintaining consistent motion patterns and environmental coherence. In this work, we conduct an in-depth analysis of MM-DiTs attention maps, which can be categorized into four components: Text-to-Video and Video-to-Text Attention, Text-toText and Video-to-Video Attention. Through our analysis of Text-to-Video and Video-to-Text Attention, we observe that semantic maps can be obtained by specifying token indices, suggesting potential for semantic control. We have emphasized the use of extracted foreground-background segmentation semantic maps to guide video generation, effectively preventing semantic confusion between foreground and background elements. In this section, we demonstrate video editing capabilities through two approaches: Reweight and Word Swap. Attention Re-weighting. As illustrated in Fig. 16, we can Figure 12. More multi-prompt results achieve semantic enhancement or attenuation by increasing or decreasing the values in rows or columns corresponding to token in the Text-to-Video and Video-to-Text Attention maps. In Fig. 16 (a), we demonstrate semantic attenuation by reducing Text-Video Attention values in the row and VideoText Attention values in the column corresponding to pink. In Fig. 16 (b), we achieve semantic enhancement by increasing Text-Video Attention values in the row and Video-Text Attention values in the column corresponding to snowy. These results validate the semantic control capabilities of Text-Video and Video-Text Attention in MM-DiT. Word Swap. Building upon the concept introduced in Prompt-to-prompt [19], this approach allows users to swap tokens in the original prompt with alternatives (e.g., changing =a large bear to large lion). The primary challenge lies in maintaining the original composition while accurately reflecting the content of the modified prompt. Our DiTCtrl method incorporates KV-sharing, similar to the word swap mechanism in [19], where we share key-value pairs from the previous prompt to compute the corresponding video for the subsequent prompt across selected layers and steps. Specifically, DiTCtrl (without latent-blending strategy) enables token-replacement video editing while ensuring consistency in other content elements, as demonstrated in Fig. 17. This implementation validates the feasibility of prompt-toprompt-style video editing within the MM-DiT architecture. D. Prompt Generator We use GPT4 for longer multi-prompt generation, our prompts are shown in Fig. 19. This figure shows the generation process of background transition, and we generate 15 Figure 13. Motion and background transition. Figure 14. Background transition. 10 different transition modes (background transition, subject transition, camera transition, style transition, lighting transition, location transition, speed transition, emotion transition, clothing transition, action transition). The detailed prompts will be provided in the the prompts.txt in the attached supplementary. E. Ablation Study E.1. Quantitative Results of Components lated clips (first row), as evidenced by higher CSCV scores - our proposed metric for evaluating multi-prompt transition smoothness. Furthermore, our KV-Sharing mechanism further improves the CSCV value, achieving enhanced stability. The mask-guided approach(fourth row) and its unmasked counterpart(third row) report comparable scores, suggesting that the contribution of masking foreground object to overall frame transition smoothness is modest. However, our qualitative analysis in Section E.2 reveals that the mask-guided method yields superior visual results. As shown in Tab. 4, our latent blending strategy (second row) demonstrates superior video consistency compared to isoAdditionally, in our evaluation of motion smoothness, our full method (DiTCtrl) achieves optimal performance. Re16 Figure 15. Visualization of single prompt longer video generation. Figure 16. Reweighting example of Video Editing. garding the Text-Image similarity metric, we observe slight expected decrease with our approach. This is attributable to our methodology where the latent representation of latter video segments incorporates Keys and Values from preceding segments to maintain consistency. This inherently introduces semantic information from previous segments, marginally reducing the current segments alignment with its corresponding text prompt. However, this trade-off is justified as our method achieves stable transitions and effectively conveys both semantic elements, resulting in higher user study scores as shown in Tab. 2. CSCV Method Motion smoothness 97.78% Isolated 97.35% DiTCtrl(w/o kv-sharing) DiTCtrl(w/o mask-guided) 84.92% 97.76% 97.80% DiTCtrl(full) 72.37% 81.79% 84.90% Text-Image similarity 32.05% 31.37% 30.66% 30.68% Table 4. Comparison of metrics for ablation. E.2. Mask-guided Generation Analysis We present comparative results in Fig. 18 to demonstrate the effectiveness of our mask-guided KV-sharing strategy. In Fig. 18 (a), while the first prompt describes single horse, the second prompt emphasizes zebra leading its herd. With17 Figure 17. Word Swap example of Video Editing. Figure 18. Ablation study of mask-guided KV-sharing results. First row shows our model without mask-guided KV-sharing, while the second row demonstrates our full model with mask-guided KV-sharing. The prompt for (a) transitions from powerful horse gallops across field... to striking zebra leads its herd across the field.... The prompt for (b) evolves from white SUV drives dirt road... to white SUV powers through snow... Attention for application in Video-Video Attention. out mask-guided KV-sharing (first row), we observe that the model fails to properly generate the zebra herd and exhibits background inconsistencies. In contrast, our full model with mask-guided KV-sharing (second row) successfully maintains scene coherence while incorporating the herd elements. Similarly, in Fig. 18 (b), the transition sequence in the first row (without mask-guided KV-sharing) shows notable deformations in the vehicles appearance, including undesired color variations. The second row, implementing our mask-guided approach, better preserves the vehicles original appearance, color, and shape throughout the transition. These results validate both the effectiveness of our maskguided approach and the feasibility of leveraging semantic maps extracted from MM-DiTs Text-Video and Video-Text 18 Figure 19. Our instruction to create multiple individual long prompts based on short prompts group of specified types"
        }
    ],
    "affiliations": [
        "ARC Lab, Tencent PCG",
        "GVC Lab, Great Bay University",
        "MMLab, The Chinese University of Hong Kong",
        "Tencent AI Lab"
    ]
}