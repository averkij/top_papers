{
    "paper_title": "SageAttention2 Technical Report: Accurate 4 Bit Attention for Plug-and-play Inference Acceleration",
    "authors": [
        "Jintao Zhang",
        "Haofeng Huang",
        "Pengle Zhang",
        "Jia Wei",
        "Jun Zhu",
        "Jianfei Chen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Although quantization for linear layers has been widely used, its application to accelerate the attention process remains limited. SageAttention utilizes 8-bit matrix multiplication, 16-bit matrix multiplication with 16-bit accumulator, and precision-enhancing methods, implementing an accurate and 2x speedup kernel compared to FlashAttention2. To further enhance the efficiency of attention computation while maintaining precision, we propose SageAttention2, which utilizes significantly faster 4-bit matrix multiplication (Matmul) alongside additional precision-enhancing techniques. First, we propose to quantize matrixes $(Q, K)$ to INT4 in a warp-level granularity and quantize matrixes $(\\widetilde P, V)$ to FP8. Second, we propose a method to smooth $Q$ and $V$, enhancing the accuracy of attention with INT4 $QK$ and FP8 $PV$. Third, we analyze the quantization accuracy across timesteps and layers, then propose an adaptive quantization method to ensure the end-to-end metrics over various models. The operations per second (OPS) of SageAttention2 surpass FlashAttention2 and xformers by about 3x and 5x on RTX4090, respectively. Comprehensive experiments confirm that our approach incurs negligible end-to-end metrics loss across diverse models, including those for large language processing, image generation, and video generation. The codes are available at https://github.com/thu-ml/SageAttention."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 7 1 ] . [ 1 8 5 9 0 1 . 1 1 4 2 : r SageAttention2 Technical Report: Accurate 4 Bit Attention for Plug-and-play Inference Acceleration Jintao Zhang, Haofeng Huang, Pengle Zhang, Jia Wei, Jun Zhu, Jianfei Chen"
        },
        {
            "title": "Tsinghua University",
            "content": "Abstract Although quantization for linear layers has been widely used, its application to accelerate the attention process remains limited. SageAttention utilizes 8-bit matrix multiplication, 16-bit matrix multiplication with 16-bit accumulator, and precision-enhancing methods, implementing an accurate and 2x speedup kernel compared to FlashAttention2. To further enhance the efficiency of attention computation while maintaining precision, we propose SageAttention2, which utilizes significantly faster 4-bit matrix multiplication (Matmul) alongside additional precision-enhancing techniques. First, we propose to quantize matrixes (Q, K) to INT4 in warp-level granularity and quantize matrixes ( (cid:101)P , ) to FP8. Second, we propose method to smooth and , enhancing the accuracy of attention with INT4 QK and FP8 . Third, we analyze the quantization accuracy across timesteps and layers, then propose an adaptive quantization method to ensure the end-to-end metrics over various models. The operations per second (OPS) of SageAttention2 surpass FlashAttention2 and xformers by about 3x and 5x on RTX4090, respectively. Comprehensive experiments confirm that our approach incurs negligible end-to-end metrics loss across diverse modelsincluding those for large language processing, image generation, and video generation. The codes are available at https://github.com/thu-ml/SageAttention. Figure 1: The left figure shows the kernel speedup on RTX4090 GPU. The right figure shows the end-toend inference speedup of generating the first token and performance metrics for the needle-in-a-haystack task (gkamradt, 2023) with sequence length of 100K on Llama3.1 on L20 GPU. * Equal Contribution."
        },
        {
            "title": "Introduction",
            "content": "Due to the quadratic complexity of attention, efficient implementation becomes increasingly crucial as sequences lengthen in real-world applications (Jiang et al., 2024). Several strategies have been developed to mitigate computational demands of attention such as (1) Linear attention methods (Wang et al., 2020; Choromanski et al., 2020; Yu et al., 2022; Katharopoulos et al., 2020) that reduce complexity to O(N ) and (2) Sparse attention methods (Liu et al., 2021; Chu et al., 2021; Li et al.; Xiao et al., 2023b, 2024; Chen et al., 2023; Jiang et al., 2024; Venkataramanan et al., 2023; Gao et al., 2024; Fu et al., 2024) that selectively process parts of the context these methods are only suitable for limited range of models and tasks. Widely used attention methods optimize attention by exploiting hardware capacities to enhance the computation speed, such as FlashAttention (Dao et al., 2022), FlashAttention2 (Dao, 2023), FlashAttention3 (Shah et al., 2024), xformers (Lefaudeux et al., 2022), and SageAttention (Zhang et al., 2024). Those works do not omit the computation of attention over parts of the context and achieve impressive speed and precision performance across various models and tasks. For the two matrix multiplication (Matmul) operations in attention: QK and , SageAttention accelerates them by quantizing the QK to INT8 and uses FP16 Matmul with FP16 accumulators for . Moreover, to keep the accuracy of attention, SageAttention proposes smoothing by eliminating its channel-wise outliers. SageAttention achieves 2 and 2.7 speedup than FlashAttention2 and xformers and is the first quantized attention that incurs negligible end-to-end metrics loss across language, image, and video generation models. However, SageAttention has two weaknesses. (W1) INT8 Matmul achieves only half the speed of INT4. (W2) FP16 Matmul with FP16 accumulators is only compatible with RTX4090 and RTX3090 GPUs. To leverage the faster INT4 tensor cores for QK and using method that can generally accelerate , we propose SageAttention2, which quantizes Q, to INT4 and P, to FP8. Challenges. Quantizing Q, to INT4 and P, to FP8 presents significant challenges. For example, when only per-tensor quantizing Q, to INT4, the text-to-video model CogvideoX will generate completely blurry video (See Figure 2), and Llama2 only achieves random-guessing-level accuracy of 25% on MMLU. After investigating deeply, we identified three primary challenges: (C1) The numerical range of INT4, which in quantization typically spans from 7 to 7 and totals 15 numbers (Lin et al., 2024), leads to significant quantization errors when and have some abnormal values. (C2) In specific layers and timesteps (for text-to-image/video) of some models, quantizing and to INT4 and quantizing and to FP8 introduces noticeable errors in attention computation. These errors in worst-case layers/timesteps significantly impact the precision of the end-to-end output. (C3) We discover that the FP32 accumulator designed for FP8 matrix multiplication in the tensor core (mma.f32.f8.f8.f32 ) is actually FP22, specifically with 1 sign bit, 8 exponent bits, and 13 mantissa bits. This will lead to an accuracy loss of . Figure 2: An example of quantizing Q, to INT4 from CogvideoX. Our approach. To address these challenges, we detailedly analyze the reasons and propose two methods. First, for the significant channel-wise outliers in matrices and K, we adopt smoothing in SageAttention and propose an effective method to remove these outliers in Q. Specifically, we propose subtracting the mK after the QK average value of the channel dimension of Q, referred to as m. Then, we add Matmul to ensure the correctness of the attention computation. Second, we observe that specific layers and timesteps consistently present quantization challenges across different inputs. To maintain accuracy, we apply an adaptive mix precision method. Specifically, we employ 8-bit (INT8+FP8) attention for these problematic layers and timesteps and 4-bit (INT4+FP8) attention for the others. Third, to mitigate the accuracy loss associated with using the 22-bit accumulator for FP8 Matmul of , we propose method to smooth to enhance the accuracy performance. Performance. Importantly, we offer high-performance implementation of SageAttention2 on RTX4090 and L20 GPUs. This implementation achieves peak performance of 485 TOPS on the RTX4090, outperforming FlashAttention2 and xformers by approximately 3.1x and 5.4x, respectively. We extensively evaluate the end-to-end metrics of state-of-the-art text, image, and video generation models using SageAttention2. Across all models and tasks, SageAttention2 can be directly adopted in plug-and-play manner with negligible loss in model performance."
        },
        {
            "title": "2.1 FlashAttention",
            "content": "The computation of self-attention can be formulated as follows: = QK / d, = σ(S), = , where σ(S)ij = exp(Sij)/ (cid:80) exp(Sik) is the softmax operation. The matrices Q, K, and each have dimensions d, while the matrix S, are . While is typically small, e.g., 64 or 128, can be thousands if not millions. Therefore, the matrices (S, ) are much larger than (Q, K, ), and naive implementation suffers from the huge amount of global memory IO for (S, ) reads/writes. FlashAttention (Dao, 2023) proposes to tile Q, K, and from the token dimension into blocks {Qi}, {Ki}, {Vi} with block size of bq, bkv, bkv, respectively. Then, to avoid the memory IO for (S, ), it uses online softmax (Milakov & Gimelshein, 2018) to progressively compute each block of O, i.e., Oi: First, for each block of {Ki}, {Vi}, it computes the following equations iteratively: Sj = QiK / d, (mj ) = σ(mj1 , (cid:101)P (cid:16) exp(mj , Sj mj1 ), (cid:17) ) i mj1 lj = exp(mj Oj1 + (cid:101)P Vj )lj1 + rowsum( (cid:101)P ), Oj = diag Where mj and lj softmax operator: mj are bq 1 vectors, which are initialized to and 0 respectively. The σ() is an online = max{mj1 = exp(Sj , rowmax(Sj )}, (cid:101)P ). Finally, the output Oi can be computed by Oi = diag(lj mj )1Oj ."
        },
        {
            "title": "2.2 Quantization",
            "content": "A matrix multiplication = AB can be accelerated with quantization as: (δA, ˆA) = ψ(A), (δB, ˆB) = ψ(B), ˆC = ˆA ˆB, = ψ1 δAδB ( ˆC) (1) ψ is quantizer which converts high-precision (e.g., FP32 or FP16) matrix to low-precision format ˆA (e.g., INT4 or FP8) with scale δA, and ψ1 is dequantizer to convert back to high-precision. We should ( ˆA) A. The actual matrix multiplication ˆA ˆB is carried in low-precision. In modern GPUs, lowhave ψ1 δA precision matrix multiplication is usually multiple times faster than higher-precision ones. Many quantizers depend on the numerical format and granularity, e.g., how many elements share common scale factor. For example, an INT4 per-tensor quantizer first computes the scale as the maximum absolute value of the entire tensor, scales the elements to the maximum representable range of INT4 [-7, +7], and then casts to INT4 with rounding: ˆA = A/δA, δA = max(A)/7. Likewise, per-token quantizer assigns scale factor for each token of tensor: ˆA[i, :] = A[i, :]/δA[i], δA[i] = max(A[i, :])/7. Also, per-channel quantizer assigns scale factor for each channel of the tensor, i.e., along the channel dimension: A[:, i] = A[:, i]/δA[i], δA[i] = max(A[:, i])/7. Dequantization process is element-wise scaling: ψ1 ( ˆA ˆB) = ˆA ˆB δA δB. δAδB"
        },
        {
            "title": "2.3 SageAttention",
            "content": "Based on the block tiling in FlashAttention2 (Dao, 2023), SageAttention (Zhang et al., 2024) quantize Q, to INT8 in per-block granularity. Moreover, to keep the quantization accuracy, SageAttention proposes to smooth first: = mean(K) ˆQi = Qi/δQ, δQ = max(Qi)/127, ˆKj = Kj/δK, δK = max(Kj)/127 Sij =QiK δQ δK Where Qi, Kj are the tilled block in FlashAttention, and mean(K) is the average value of the channel dimension of K. SageAttention keeps (cid:101)P as FP16 and uses FP16 Matmul with an FP16 accumulator for (cid:101)P . However, FP16 Matmul with an FP16 accumulator only has speedup effect on RTX4090 and RTX3090 GPUs."
        },
        {
            "title": "3 SageAttention-2",
            "content": "Figure 3: Workflow of SageAttention2. 3 Per-warp quantize Q,K and per-channel quantize V. 4 Perform the SageAttention2 kernel. 5 Correct the output. 1 Smooth Q,K,V. 2 GEMV to obtain S."
        },
        {
            "title": "3.1 Formulation",
            "content": "Based on the introduction of FlashAttention and quantization presented in Section 2, we now describe the quantized attention approach we developed. 4 Quantization: (δQ, ˆQ, = QmK m) = ϕQ(Q), (δK, ˆK) = ϕK(K), (δV , ˆV , m) = ϕV (V ) Attention: = ψ1 ( ˆQ ˆK ) + S, = diag (exp(m m)) + ψ1 δQδK (m, (cid:101)P ) = σ(m, S), ( ˆP ˆV ) δP δV (δP , ˆP ) = ψP ( (cid:101)P ) (2) (3) ϕQ, ϕK, ϕV are three transformations to obtain quantized Q, K, and , which we will discuss in subsequent sections. For simplicity, we omit all superscripts and subscripts, but the matrices used in attention are still tiles, and the computation is still organized as FlashAttention described in Section 2.1. Compared to the original full-precision version, as shown in Eq. 2, 3, SageAttention2 adds quantizers to Q, K, P, and dequantizers to the product to accelerate both Matmuls of QK and (cid:101)P . Figure 4: Typical examples of tensors data distribution in attention. Table 1: End-to-end metrics comparison of different quantization methods, where Q,K are quantized into INT4, while P,V stay in full precision. Q, Full-Precision INT4 Quantization Smoothing (Q+K) - Llama3.1 (Lambda) 81.5% 72.6% 80.8% Llama3.1 (WikeText) 6.013 11.698 6.219 CogVideo (vqa-a) 77.605 27.114 77. CogVideo (vqa-t) 75.360 24.670 75."
        },
        {
            "title": "3.2 Per-warp INT4 Quantization",
            "content": "SageAttention uses per-block quantization, which quantizes each block of and for GPU streaming processor. Such quantization strategy could achieve an accuracy performance close to per-token quantization and avoid the overhead of the dot product of the quantization scale vectors δQ and δK. However, quantizing and to INT4 demands more accurate quantization granularity. We propose per-warp quantization, 5 Table 2: Average accuracy across all layers using different quantization granularities. Table 3: Worst accuracy across all layers using different quantization granularities. Method Cos Sim Relative L1 RMSE Method Cos Sim Relative L1 RMSE Per-token Per-warp Per-block Per-tensor 99.45% 99.45% 98.03% 97.15% 0.0649 0.0648 0.1492 0.1800 0.0335 0.0334 0.0744 0.0865 Per-token Per-warp Per-block Per-tensor 96.76% 96.71% 90.68% 85.85% 0.1916 0.1956 0.3615 0.4687 0.0775 0.0779 0.1490 0.2261 more precise and granular quantization approach than the per-block quantizer, also with no additional overhead of vectors dot product. Specifically, each block of in SageAttention will be split into cw segments, each processed by one of cw GPU warps in GPU streaming processor (SM). Subsequently, each warp performs Matmul for its assigned segment. Per-warp INT4 quantization assigns scale factor for every bq/wc tokens for Q: ˆQ[ bq cw : bq (i + 1) cw , :] = (cid:38) Q[ ibq cw (cid:37) , :] : bq(i+1) cw δQ[i] max( (cid:12) (cid:12) Q[ ibq (cid:12) cw : bq(i+1) cw , :] (cid:12) (cid:12) (cid:12)) 7 δQ[i] = (4) (5) Per-warp quantization will provide an accuracy performance superior to per-block quantization used in SageAttention, which we will discuss in Section 3.5. Figure 5: An example of quantized value distribution of before and after smoothing Q."
        },
        {
            "title": "3.3 Smooth Q",
            "content": "The representative range for quantization of INT4 is notably restrictive, i.e., only 24 1 = 15 values. This limitation significantly degrades performance when quantizing Q, into INT4 in attention. For example, using INT4 to quantize Q, can cause the perplexity of Llama3.1 on WikiText to increase by more than 90%, and the quality of videos generated by CogvideoX decrease about 3x (See Table 1). We analyze the data distribution of Q, in real models. For example, we find that Llama3.1 and CogvideoX show distinct channel-wised outliers in Q, (See Figure 4). While per-channel quantization could mitigate the 6 quantization error caused by such outliers, this method is impracticable for Q, because quantization must be conducted along the outer axis (token dimension) of QK . To address this challenge, we propose method to eliminate the quantization error brought by the outliers in Q, K: Qm = mean(Q), γ(Q) =Q Qm, γ(K) = mean(K), = QK =γ(Q)γ(K) + Qmγ(K) ϕQ(Q) = ψQ γ(Q), Qm, ϕK(K) = ψK γ(K) (6) Where mean(K) and γ(K) have been discussed in SageAttention (Zhang et al., 2024), and the transformation for does not change the attention score (cid:101)P . mean(Q) = 1 t=1 Q[t, :] is vector with shape 1 d. For the transformation of Q, we will perform GEMV (general matrix-vector multiplication) between QmK as S. This will be added to during the attention computation. Finally, the Qm and , i.e., transformation from full-precision Q, to quantized ˆQ, ˆK can be expressed as shown in Eqution 6, where ψQ, ψK is two quantizers for and K. In other words, full-precision Q, are substracted with the means of the channel dimension before being quantized. (cid:80)N Figure 5 shows an example from CogvideoX of the distribution of quantized with and without smoothing Q. We can find that with smoothing Q, the range of INT4 is utilized more uniformly and fully. Table 1 presents end-to-end metrics for different quantization methods with and without smoothing Q+K on Llama3.1 (Dubey et al., 2024) and CogvideoX (Yang et al., 2024). The results demonstrate that smoothing Q+K offers significant accuracy benefits. Also, Table 9 and Table 10 show that the order of effectiveness is smoothing Q+K > smoothing > smoothing > other baselines. Algorithm 1 Implementation of SageAttention2. Input: Matrices Q(FP16), K(FP16), (FP16) RN d, block size bq, bkv, warp count cw. m, ), Preprocessing: = mean(K), mean(V ), = Quantization: (δQ, ˆQ) = ψQ(Q) // INT4 Per-warp. Per-channel. Divide ˆQ to Tm = N/bq blocks { ˆQi}; divide ˆK, ˆV , and to Tn = N/bkv blocks { ˆKi}, { ˆVi}, and {Si}; for = 1 to Tm do = mean(Q), = m, = GEMV( (δV , ˆV ) = ψV (V ). // FP8 (δK, ˆK) = ψK(K). = m. ) δQ[st + w] δK[j] + Sj; // Paralleled by cw warps. = emj1 + rowsum( (cid:101)P mj 448).to(FP8.e4m3), Vj) ; mj ), lj ) ; Load ˆQi and δQ[i cw : (i + 1) cw] into an SM ; for in [1, Tn] do Load ˆKj, ˆVj, and δK[j] into the SM ; = range(cw), st = cw [st : st + cw] = Matmul( ˆQi[st : st + cw], ˆK Sj = max(mj1 = exp(Sj , rowmax(Sj mj = diag(emj1 )1Oj1 + Matmul(( (cid:101)P Oj end for Load δV into an SM; Oi = diag(lTn )1OTn Write Oi ; /448 δV ; mj )), (cid:101)P end for = {Oi}, = + return m. Table 4: Average accuracy using different data types of ( (cid:101)P , ) across all layers of CogvideoX model, where (Q, K) are smoothed. Table 5: Worst accuracy using different data types of ( (cid:101)P , ) across all layers of CogvideoX model, where (Q, K) are smoothed. Q, (cid:101)P , Cos Sim Relative L1 RMSE Q, (cid:101)P , Cos Sim Relative L1 RMSE INT4 77.05% INT8 E5M2 99.20% E4M3 99.44% 99.45% FP 0.5618 0.0905 0.0683 0.0649 0.5044 0.0903 0.0347 0.0335 INT4 19.52% INT8 E5M2 94.94% E4M3 96.70% 96.76% FP16 0.9579 0.2327 0.1956 0.1916 1.4483 0.2361 0.0779 0. Table 6: An accuracy example on real tensors of CogvideoX model with or without smoothing ."
        },
        {
            "title": "Smooth V",
            "content": "Cos Sim Relative L1 RMSE 98.25% 99.75% 0.1980 0. 0.2387 0.0773 Figure 6: An example of dot product precison row of (cid:101)P and column of presented by FP22 data type. Table 7: Error of the FP8 Matmul instruction of mma(f8f8f32). Precision of Accumulated Value Error compared to FP32 E8M13 0 E8M23 mma(f32.f16.f16.f32) - mma(f32.f8.f8.f32)"
        },
        {
            "title": "3.4 Smooth V",
            "content": "To avoid the drawback of SageAttention where FP16 Matmul with an FP16 accumulator for is only effective on GPUs such as the RTX4090, we adopt the approach of quantizing and to FP8 to leverage the universal acceleration of FP8 Tensor cores. However, we discover that the accumulator for the mma(f32f8f8f32) instruction on the Ada architecture is actually FP22, specifically with 1 sign bit, 8 exponent bits, and 13 mantissa bits. Specifically, for mma(f32f8f8f32) instruction = AB + D, where A, are tensors in FP8 data type and C, are tensors with FP32 data type, we initialize the A, to zero and vary to test the data type of the accumulator. As shown in Table 7, when is initialized with 1 sign bit, 8 exponent bits, and 13 mantissa bits, the value of matches the result of the mma(f16f16f32) instruction. However, when is initialized with more than 13 mantissa bits, the error of corresponds to the difference between the results of mma(f32f16f16f32) and mma(f32f8f8f32). Consequently, matrix multiplication of matrices (cid:101)P and , quantized to FP8, incurs certain degree of precision loss compared to using an FP32 accumulator. To mitigate this precision loss as much as possible, we propose to smooth : 8 γ(V ) = mean(V ), mean(V ) = ϕV (V ) =ψV γ, Vm Vm (7) As shown in Figure 6, this strategy enhances the accuracy of FP22 for values in (cid:101)P for the following reasons: Each row of (cid:101)P spans value range from 0 to 1, and each column of consistently features channelwise outliers that are exclusively positive or negative, typically ranging between 8 and 9. Consequently, the values of (cid:101)P could be quite large. However, the floating-point number representation range is not uniformit is denser near zero. Therefore, by subtracting the mean along the channel dimension from , the values of (cid:101)P will be closer to zero, resulting in higher representational precision. We believe such an explanation and strategy could address lots of issues1 reported by the community. Additionally, to maintain Vm to the final calculation of O: the correctness of the attention computation, it is only necessary to add Vm. This is because the sum of each row of the (cid:101)P matrix equals 1, so (cid:101)P = + Vm. In other words, this method decomposes into two parts: Vm and . For , it centers the values of each column around zero, which leads to the dot product result of row from the quantized (cid:101)P matrix with column from the quantized matrix being closer to zero. This makes the representation of FP22 more accurate. Meanwhile, Vm is retained in FP16 and added to at the end without causing loss of precision for the Vm part. Vm = Table 6 shows the attention accuracy on real tensors sampled from CogvideoX with and without smoothing . It demonstrates that smoothing could improve the accuracy of SageAttention2 when quantizing Q, to INT4 and (cid:101)P , to FP8."
        },
        {
            "title": "3.5 Quantization for Q, K, P, V",
            "content": "Quantization for Q, K. We propose ψQ(Q) and ψK(K) in granularity of per-warp. This is first because per-channel quantization is not feasible since the scale factors of the inner axis of QK cannot used to do dequantization (Xiao et al., 2023a). Second, as shown in Table 2 and Table 3, we compare the average and worst-case accuracies of INT4 quantization at per-token, per-warp, per-block, and per-tensor granularity using real Q, K, across all layers of CogvideoX. Results indicate that the accuracy of per-warp quantization is very close to per-token and outperforms lot than per-block and per-tensor. Furthermore, per-warp quantization incurs less dequantization overhead than per-token as discussed in Section 3.2. Quantization for (cid:101)P , . We choose FP8, specifically the E4M3 data type, for ψP ( (cid:101)P ) and ψV (V ) for two reasons. First, most GPUs have tensor cores that support FP8 Matmul operations, which are twice as fast as those using FP16. Second, Table 4 and Table 5 show the average and worst accuracy of different data types used for (cid:101)P , using real Q, K, across all layers of CogvideoX. We can see that the accuracy of using E4M3 is very close to that of using FP16 and superior to that of E5M2 and INT8. We propose to use ψP ( (cid:101)P ) in per-block and ψV (V ) in per-channel for three reasons. First, per-channel quantization for (cid:101)P and per-token quantization for are not viable because dequantization requires scale factors of outer axis. Second, (cid:101)P = exp(Si rowmax(Si)), where Si is the Matmul result of block of and , the max value in each row of (cid:101)P is 1. Hence, we can assign single static scale = 1 448 to block (cid:101)P , whose accuracy equals per-token quantization. Third, per-channel quantization can address the channel-wised outlier of . Accuracy metrics. We use three metrics to assess the accuracy of quantized attention output compared to attention output in full-precision O: First, we flatten and into vectors in the shape of 1 n. Then, (cid:112)(cid:80) O2(cid:112)(cid:80) O2, Relative L1 distance: L1 = (cid:80) O/ (cid:80) O, Root Cosine similarity: CosSim = (cid:80) OO/ mean square error: RM SE = (cid:112)(1/n) (cid:80)(O O)2."
        },
        {
            "title": "3.6 Adaptive Quantization over Layer and Timestep",
            "content": "Based on the discussion in Section 3.5, we implement two attention kernels (See Table 8) based on choice: Using INT4 or INT8 quantization for ψQ(Q) and ψK(K). The speed of these kernels is in the 1https://github.com/triton-lang/triton/issues/4476, https://github.com/triton-lang/triton/issues/5065 9 Table 8: Two kernel implementations of SageAttention2. Kernel SageAttn2-4b SageAttn2-8b ψQ(Q), ψK(K) INT4, per-warp INT8, per-warp ψP ( (cid:101)P ), ψV (V ) FP8, per-block; FP8, per-channel FP8, per-block; FP8, per-channel Figure 7: Mean and standard deviation of cossim(1L1)) of SageAttn-4b in different layers and timesteps for different inputs in Llama3.1 and CogvideoX. order (SageAttn2-4b>SageAttn2-8b), but the accuracy order is opposite. Although SageAttn2-4b could perform adequately in lots of tasks, for instance, it shows very slight end-to-end metric loss of Lambda benchmark on Llama3.1, SageAttn2-4b falls short in some hard scenarios. Consequently, we first conduct detailed analysis of the accuracy performance on various models using SageAttn2-4b and SageAttn2-8b. Then, we propose an effective and easy-to-use adaptive method. Accuracy across layers and timesteps. Figure 7 displays the mean and standard deviation of the accuracy, calculated using CosSim (1 L1), for SageAttn-4b across different layers and timesteps of different inputs in Llama3.1 and CogvideoX. It can be observed that specific layers and timesteps exhibit specific ranges of errors. Our method. Our adaptive strategy is detailed as follows: First, we assess the average accuracy of SageAttn2-4b in comparison to full precision attention for each (layer, timestep) combination within model, using several random inputs. For models without timestep dimension, such as language models, the evaluation is confined to the layer dimension. Second, we will sort the (layer, timestep) combinations in descending order based on their CosSim (1 L1) values. We then identify the smallest ξ% of these combinations. Finally, for the identified (layer, timestep) combinations, we consistently apply SageAttn2-8b across all other prompts. We call such strategy SageAttn-mix and will report the values of ξ% we used for each model in Section 4.1."
        },
        {
            "title": "4.1 Setup",
            "content": "Models. We validate the effectiveness of SageAttention2 across diverse set of representative models from language, image, and video generation. Specifically, we conduct experiments on five models: Llama2 (7B) (Touvron et al., 2023), Llama3.1 (8B) (Dubey et al., 2024), and GLM4 (9B) (GLM et al., 2024) for text2text, CogvideoX (2B) (Yang et al., 2024) and Open-Sora (Zheng et al., 2024) for text2video, Flux (schnell) (Black Forest Labs, 2023) for text2image, and TIMM (Wightman, 2019) for image classification. 10 Figure 8: Speed comparison between SageAttention2 and baselines (RTX4090, headdim=64). Figure 9: Speed comparison between SageAttention2 and baselines (RTX4090, headdim=128). Figure 10: Speed comparison between SageAttention2 and baselines (RTX4090, headdim=256). Figure 11: Speed comparison between SageAttention2 and baselines (L20, headdim=64). Datasets. Text-to-text models are evaluated on four zero-shot tasks: WikiText (Merity et al., 2022) to assess the models prediction confidence, LAMBADA (Paperno et al., 2016) evaluate contextual understanding, MMLU (Hendrycks et al., 2020) for measuring knowledge across various subjects, and Longbench (Bai et al., 2023) for evaluating bilingual, multitask, and comprehensive assessment of long context understanding capabilities. Text-to-video models are evaluated using the open-sora (Zheng et al., 2024) prompt sets. Flux is 11 Figure 12: Speed comparison between SageAttention2 and baselines (L20, headdim=128). Figure 13: Speed comparison between SageAttention2 and baselines (L20, headdim=256). assessed on MJHQ-30K (Li et al., 2024). TIMM is evaluated on on three image datasets: ImageNet (Deng et al., 2009), ImageNet-Sketch (Sketch) (Wang et al., 2019), and ImageNet-Rendition (ImageNet-r) (Hendrycks et al., 2021). Metrics. For text-to-text models, we use perplexity (ppl.) (Jelinek et al., 1977) for WikiText, Accuracy (Acc.) for LAMBADA and MMLU, and Longbench score (Bai et al., 2023). For text-to-video models, following Zhao et al. (2024), we evaluate the quality of generated videos on five metrics: CLIPSIM and CLIPTemp (CLIP-T) (Liu et al., 2024) to measure the text-video alignment; (VQA-a) and (VQA-t) to assess the video aesthetic and technical quality, respectively; and Flow-score (FScore) for temporal consistency (Wu et al., 2023). For text-to-image models, generated images are compared with the images in MJHQ-30K dataset in three aspects: FID (Heusel et al., 2017) and sFID (Salimans et al., 2016) for fidelity evaluation, Clipscore (CLIP) (Hessel et al., 2021) for text-image alignment, and ImageReward (IR) (Xu et al., 2024) for human preference. For TIMM, we use Accuracy. Implemetation details. We implement our attention kernels using CUDA and conduct experiments on Ubuntu 22.04 servers with RTX4090 and L20 GPUs. Baselines. (1) SmoothAttn. Following Qserve (Lin et al., 2024), we apply smooth quant for matrixes Q, with smoothing factor α = 0.5. (2) HadmdAttn. Following Quarot (Ashkboos et al., 2024), we apply random Hadamard transformation for matrixes Q, before INT4 quantization. Hyper-parameters of SageAttn-mix. We use ξ of 50% for Llama2, 60% for CogvideoX, 25% for Open-Sora, 30% for Llama3.1 and GLM4, 0% for Flux, and TIMM."
        },
        {
            "title": "4.2 Speed and Accuracy of Kernels",
            "content": "Speed. We conduct experiments to compare the Speed of SageAttention2 against baselines using configurations with headdim=64, headdim=128, and headdim=256, both with and without Causal Mask (Vaswani, 2017). Specifically, Figure 8, Figure 9, and Figure 10 show the speed of SageAttention2 and baselines across varying sequence lengths on RTX4090. These results indicate that SageAttention2 achieves peak of 485 TOPS and is 3.1x faster than FlashAttention2 and 5.4x faster than xformers. Figure 11, Figure 12, and Figure 13 illustrate the results on L20 GPU, indicating that SageAttention2 achieves peak of 288 12 Figure 14: Comparison examples from CogvideoX, prompts are sampled from open-sora prompt sets. Table 9: Average accuracy across all layers using different smooth methods. Table 10: Worst accuracy across all layers using different smooth methods. Method CosSim Relative L1 RMSE Method CosSim Relative L1 RMSE None HadmdAttn SmoothAttn Smooth (Ours) Smooth (Ours) SageAttn2-4b 80.04% 79.77% 90.21% 98.07% 98.30% 99.46% 0.3906 0.3782 0.3383 0.1493 0.1250 0. 0.2223 0.2180 0.1952 0.0743 0.0712 0.0334 None HadmdAttn SmoothAttn Smooth (Ours) Smooth (Ours) SageAttn2-4b 4.83% 4.85% 64.49% 90.86% 93.10% 96.71% 0.9979 0.9978 0.9262 0.3565 0.2989 0.1956 0.7784 0.7785 0.7204 0.1464 0.2195 0.0779 TOPS and is 2.7x faster than FlashAttention2 and 4.6x faster than xformers. Accuracy. Table 9 and Table 10 show that the average and worst accuracy of different methods with INT4 Q, and FP8 P, across all layers of CogvideoX. The results indicate that the accuracy of SageAttn-4b is superior to other baselines."
        },
        {
            "title": "4.3 End-to-end Performance",
            "content": "Metrics loss. We assessed the end-to-end metrics of various models using SageAttention2 compared to using attention in full precision. Detailed evaluation results are presented in Table 11 for Llama2, Llama3.1, GLM4, CogvideoX, Open-Sora, Flux, and TIMM, respectively. The results indicate that SageAttn-4b outperforms all baselines and maintains most of the end-to-end accuracy across all models. Additionally, using 13 Table 11: End-to-end metrics loss across text, image, and video generation models. Model Llama2 Llama3. GLM4 Attention Full-Precision HadmdAttn SmoothAttn SageAttn2-4b SageAttn2-mix Full-Precision HadmdAttn SmoothAttn SageAttn2-4b SageAttn2-mix Full-Precision HadmdAttn SmoothAttn SageAttn2-4b SageAttn2-mix WikiText (Ppl.) Lambda (Acc.) MMLU (Acc.) Longbench 5.823 6.706 6.690 6.018 5.883 6.013 7.661 7.087 6.219 6.131 7.241 7.932 8.841 7.341 7.303 0.886 0.865 0.871 0.886 0.883 0.815 0.756 0.788 0.808 0.816 0.432 0.435 0.442 0.435 0.434 0.439 0.355 0.395 0.436 0.431 0.635 0.502 0.551 0.617 0.629 0.743 0.676 0.599 0.732 0. - - - - - 49.40 44.62 43.76 48.61 49.01 49.78 46.27 43.10 49.06 49.77 Model CogvideoX Open-Sora Attention Full-Precision HadmdAttn SmoothAttn SageAttn2-4b SageAttn2-mix Full-Precision SageAttn2-4b SageAttn2-mix CLIPSIM 0.1836 0.1742 0.1763 0.1813 0.1816 0.1831 0.1821 0. CLIP-T 0.9975 0.9877 0.9870 0.9969 0.9976 0.9996 0.9994 0.9994 VQA-a 77.605 36.028 37.444 77.276 75.686 46.713 42.270 44.509 VQA-t 75.360 23.786 42.184 75.147 78.600 59.553 55.965 59.097 FScore 3.006 0.550 0.792 2.070 2.884 0.368 0.364 0.383 Model Flux Attention Full-Precision HadmdAttn SmoothAttn SageAttn2-4b FID 11.303 11.163 10.941 10.563 sFID 17.603 17.693 18.098 17.052 CLIP 32.603 32.592 32.582 32.631 IR 0.9745 0.9638 0.9613 0.9747 Model TIMM Attention Full-Precision SageAttn2-4b ImageNet (Acc.) 84.79% 84.67% Sketch (Acc.) 45.32% 45.07% ImageNet-r (Acc.) 59.55% 59.11% adaptive quantization techniques, SageAttn-mix achieves performance comparable to full-precision attention."
        },
        {
            "title": "4.4 Ablation Study",
            "content": "Adaptive quaitzation. To analyze the impact of using different ratio of SageAttn-8b in adaptive quantization, Table 15 shows the changes in the perplexity of Llama3.1 under various ratio of SageAttn-8b. It can be observed that even if SageAttn-4b is used exclusively, the overall end-to-end representation is sufficiently good, and the higher the ratio of SageAttn-8b used, the closer the accuracy approaches that of using full-precision attention. 14 Figure 15: End-to-end performance using different ratio of SageAttn-8b. Overhead of smoothing Q, K. Note that the overhead of smoothing Q, only contains mean(K), and QmK . The speed overhead of them accounts for about 3.5% of the attention kernel. Qm,"
        },
        {
            "title": "5 Conclusion and Future Work",
            "content": "We introduce SageAttention2, an efficient and accurate 4-bit quantization method for attention. First, we propose to quantize (Q, K) to INT4 in warp-level granularity and quantize ( (cid:101)P , ) to FP8. Second, we propose method to smooth matrixes and , enhancing the accuracy of attention with INT4 QK and FP8 . Third, we analyze the quantization accuracy across timesteps and layers, then propose an adaptive quantization method to ensure the end-to-end metrics over various models. Our implementation is faster than FlashAttention2 and xformers by approximately 3.1x and 5.4x on RTX4090, respectively. Extensive testing confirms that our approach maintains end-to-end metrics across various models, including language, image, and video generation. Future Work. We leave the implementation of FP8 MatMul with an FP16 accumulator for (cid:101)P and SageAttention2 on the Hopper architecture for future work."
        },
        {
            "title": "References",
            "content": "Ashkboos, S., Mohtashami, A., Croci, M. L., Li, B., Cameron, P., Jaggi, M., Alistarh, D., Hoefler, T., and Hensman, J. Quarot: Outlier-free 4-bit inference in rotated llms, 2024. URL https://arxiv.org/abs/ 2404.00456. Bai, Y., Lv, X., Zhang, J., Lyu, H., Tang, J., Huang, Z., Du, Z., Liu, X., Zeng, A., Hou, L., et al. Longbench: bilingual, multitask benchmark for long context understanding. arXiv preprint arXiv:2308.14508, 2023. Black Forest Labs. Flux. https://github.com/black-forest-labs/flux, 2023. Chen, Y., Qian, S., Tang, H., Lai, X., Liu, Z., Han, S., and Jia, J. Longlora: Efficient fine-tuning of long-context large language models. arXiv preprint arXiv:2309.12307, 2023. Choromanski, K., Likhosherstov, V., Dohan, D., Song, X., Gane, A., Sarlos, T., Hawkins, P., Davis, J., Mohiuddin, A., Kaiser, L., et al. Rethinking attention with performers. arXiv preprint arXiv:2009.14794, 2020. 15 Chu, X., Tian, Z., Wang, Y., Zhang, B., Ren, H., Wei, X., Xia, H., and Shen, C. Twins: Revisiting the design of spatial attention in vision transformers. Advances in neural information processing systems, 34: 93559366, 2021. Dao, T. Flashattention-2: Faster attention with better parallelism and work partitioning. arXiv preprint arXiv:2307.08691, 2023. Dao, T., Fu, D., Ermon, S., Rudra, A., and Re, C. Flashattention: Fast and memory-efficient exact attention with io-awareness. Advances in Neural Information Processing Systems, 35:1634416359, 2022. Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L. Imagenet: large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pp. 248255. Ieee, 2009. Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur, A., Schelten, A., Yang, A., Fan, A., et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Fu, T., Huang, H., Ning, X., Zhang, G., Chen, B., Wu, T., Wang, H., Huang, Z., Li, S., Yan, S., Dai, G., Yang, H., and Wang, Y. Moa: Mixture of sparse attention for automatic large language model compression, 2024. URL https://arxiv.org/abs/2406.14909. Gao, Y., Zeng, Z., Du, D., Cao, S., So, H. K.-H., Cao, T., Yang, F., and Yang, M. Seerattention: Learning intrinsic sparse attention in your llms, 2024. URL https://arxiv.org/abs/2410.13276. gkamradt. Llmtest needle in haystack - pressure testing llms. https://github.com/gkamradt/LLMTest_ NeedleInAHaystack, 2023. GLM, T., Zeng, A., Xu, B., Wang, B., Zhang, C., Yin, D., Rojas, D., Feng, G., Zhao, H., Lai, H., Yu, H., Wang, H., Sun, J., Zhang, J., Cheng, J., Gui, J., Tang, J., Zhang, J., Li, J., Zhao, L., Wu, L., Zhong, L., Liu, M., Huang, M., Zhang, P., Zheng, Q., Lu, R., Duan, S., Zhang, S., Cao, S., Yang, S., Tam, W. L., Zhao, W., Liu, X., Xia, X., Zhang, X., Gu, X., Lv, X., Liu, X., Liu, X., Yang, X., Song, X., Zhang, X., An, Y., Xu, Y., Niu, Y., Yang, Y., Li, Y., Bai, Y., Dong, Y., Qi, Z., Wang, Z., Yang, Z., Du, Z., Hou, Z., and Wang, Z. Chatglm: family of large language models from glm-130b to glm-4 all tools, 2024. Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D., and Steinhardt, J. Measuring massive multitask language understanding. 2020. Hendrycks, D., Basart, S., Mu, N., Kadavath, S., Wang, F., Dorundo, E., Desai, R., Zhu, T., Parajuli, S., Guo, M., Song, D., Steinhardt, J., and Gilmer, J. The many faces of robustness: critical analysis of out-of-distribution generalization. ICCV, 2021. Hessel, J., Holtzman, A., Forbes, M., Le Bras, R., and Choi, Y. Clipscore: reference-free evaluation metric for image captioning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 75147528, 2021. Heusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., and Hochreiter, S. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in neural information processing systems, 30, 2017. Jelinek, F., Mercer, R. L., Bahl, L. R., and Baker, J. K. Perplexitya measure of the difficulty of speech recognition tasks. The Journal of the Acoustical Society of America, 62(S1):S63S63, 1977. Jiang, H., Li, Y., Zhang, C., Wu, Q., Luo, X., Ahn, S., Han, Z., Abdi, A. H., Li, D., Lin, C.-Y., et al. Minference 1.0: Accelerating pre-filling for long-context llms via dynamic sparse attention. arXiv preprint arXiv:2407.02490, 2024. Katharopoulos, A., Vyas, A., Pappas, N., and Fleuret, F. Transformers are rnns: Fast autoregressive transformers with linear attention. In International conference on machine learning, pp. 51565165. PMLR, 2020. 16 Lefaudeux, B., Massa, F., Liskovich, D., Xiong, W., Caggiano, V., Naren, S., Xu, M., Hu, J., Tintore, M., Zhang, S., Labatut, P., Haziza, D., Wehrstedt, L., Reizenstein, J., and Sizov, G. xformers: modular and hackable transformer modelling library. https://github.com/facebookresearch/xformers, 2022. Li, D., Kamko, A., Akhgari, E., Sabet, A., Xu, L., and Doshi, S. Playground v2.5: Three insights towards enhancing aesthetic quality in text-to-image generation, 2024. Li, K., Wang, Y., Peng, G., Song, G., Liu, Y., Li, H., and Qiao, Y. Uniformer: Unified transformer for efficient spatial-temporal representation learning. In International Conference on Learning Representations. Lin, Y., Tang, H., Yang, S., Zhang, Z., Xiao, G., Gan, C., and Han, S. Qserve: W4a8kv4 quantization and system co-design for efficient llm serving, 2024. URL https://arxiv.org/abs/2405.04532. Liu, Y., Cun, X., Liu, X., Wang, X., Zhang, Y., Chen, H., Liu, Y., Zeng, T., Chan, R., and Shan, Y. Evalcrafter: Benchmarking and evaluating large video generation models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2213922149, 2024. Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., and Guo, B. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 1001210022, 2021. Merity, S., Xiong, C., Bradbury, J., and Socher, R. Pointer sentinel mixture models. In International Conference on Learning Representations, 2022. Milakov, M. and Gimelshein, N. Online normalizer calculation for softmax. arXiv preprint arXiv:1805.02867, 2018. Paperno, D., Kruszewski, G., Lazaridou, A., Pham, N.-Q., Bernardi, R., Pezzelle, S., Baroni, M., Boleda, G., and Fernandez, R. The lambada dataset: Word prediction requiring broad discourse context. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 15251534, 2016. Salimans, T., Goodfellow, I., Zaremba, W., Cheung, V., Radford, A., and Chen, X. Improved techniques for training gans. Advances in neural information processing systems, 29, 2016. Shah, J., Bikshandi, G., Zhang, Y., Thakkar, V., Ramani, P., and Dao, T. Flashattention-3: Fast and accurate attention with asynchrony and low-precision. arXiv preprint arXiv:2407.08608, 2024. Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. Vaswani, A. Attention is all you need. Advances in Neural Information Processing Systems, 2017. Venkataramanan, S., Ghodrati, A., Asano, Y. M., Porikli, F., and Habibian, A. Skip-attention: Improving vision transformers by paying less attention. arXiv preprint arXiv:2301.02240, 2023. Wang, H., Ge, S., Lipton, Z., and Xing, E. P. Learning robust global representations by penalizing local predictive power. Advances in Neural Information Processing Systems, 32, 2019. Wang, S., Li, B. Z., Khabsa, M., Fang, H., and Ma, H. Linformer: Self-attention with linear complexity. arXiv preprint arXiv:2006.04768, 2020. Wightman, R. Pytorch image models. https://github.com/rwightman/pytorch-image-models, 2019. Wu, H., Zhang, E., Liao, L., Chen, C., Hou, J., Wang, A., Sun, W., Yan, Q., and Lin, W. Exploring video quality assessment on user generated contents from aesthetic and technical perspectives. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 2014420154, 2023. 17 Xiao, C., Zhang, P., Han, X., Xiao, G., Lin, Y., Zhang, Z., Liu, Z., and Sun, M. Infllm: Training-free long-context extrapolation for llms with an efficient context memory. In First Workshop on Long-Context Foundation Models@ ICML 2024, 2024. Xiao, G., Lin, J., Seznec, M., Wu, H., Demouth, J., and Han, S. Smoothquant: Accurate and efficient post-training quantization for large language models. In International Conference on Machine Learning, pp. 3808738099. PMLR, 2023a. Xiao, G., Tian, Y., Chen, B., Han, S., and Lewis, M. Efficient streaming language models with attention sinks. arXiv preprint arXiv:2309.17453, 2023b. Xu, J., Liu, X., Wu, Y., Tong, Y., Li, Q., Ding, M., Tang, J., and Dong, Y. Imagereward: Learning and evaluating human preferences for text-to-image generation. Advances in Neural Information Processing Systems, 36, 2024. Yang, Z., Teng, J., Zheng, W., Ding, M., Huang, S., Xu, J., Yang, Y., Hong, W., Zhang, X., Feng, G., et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. Yu, W., Luo, M., Zhou, P., Si, C., Zhou, Y., Wang, X., Feng, J., and Yan, S. Metaformer is actually what you need for vision. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1081910829, 2022. Zhang, J., wei, J., Zhang, P., Zhu, J., and Chen, J. Sageattention: Accurate 8-bit attention for plug-and-play inference acceleration, 2024. URL https://arxiv.org/abs/2410.02367. Zhao, T., Fang, T., Liu, E., Rui, W., Soedarmadji, W., Li, S., Lin, Z., Dai, G., Yan, S., Yang, H., Ning, X., and Wang, Y. Vidit-q: Efficient and accurate quantization of diffusion transformers for image and video generation, 2024. Zheng, Z., Peng, X., Yang, T., Shen, C., Li, S., Liu, H., Zhou, Y., Li, T., and You, Y. Open-sora: Democratizing efficient video production for all, March 2024. URL https://github.com/hpcaitech/Open-Sora."
        }
    ],
    "affiliations": ["Tsinghua University"]
}