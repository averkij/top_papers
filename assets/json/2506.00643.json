{
    "paper_title": "SATA-BENCH: Select All That Apply Benchmark for Multiple Choice Questions",
    "authors": [
        "Weijie Xu",
        "Shixian Cui",
        "Xi Fang",
        "Chi Xue",
        "Stephanie Eckman",
        "Chandan Reddy"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large language models (LLMs) are increasingly evaluated on single-answer multiple-choice tasks, yet many real-world problems require identifying all correct answers from a set of options. This capability remains underexplored. We introduce SATA-BENCH, the first dedicated benchmark for evaluating LLMs on Select All That Apply (SATA) questions across diverse domains, including reading comprehension, law, and biomedicine. Our evaluation of 27 open-source and proprietary models reveals a significant gap: even the strongest model achieves only 41.8% exact match, exposing LLMs' inability to reliably identify all correct answers. We find that this weakness stems from two core challenges: selection bias - models favor certain choices regardless of content, and count bias - models fail to predict the correct number of answers. To address these issues, we propose Choice Funnel, a decoding strategy that combines token debiasing with adaptive thresholding to guide models toward complete and accurate selections. Choice Funnel achieves up to 29% higher exact match than competitive baselines while reducing inference cost by over 64%. Our findings expose fundamental limitations in current LLMs and introduce a new framework for diagnosing and improving multi-answer reasoning. We release SATA-BENCH and Choice Funnel to promote LLM development for robust decision-making in realistic, multi-answer applications."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 3 ] . [ 1 3 4 6 0 0 . 6 0 5 2 : r SATA-BENCH: Select All That Apply Benchmark for Multiple Choice Questions Weijie Xu1, Shixian Cui1, Xi Fang1, Chi Xue1, Stephanie Eckman1, Chandan K. Reddy 1Amazon"
        },
        {
            "title": "Abstract",
            "content": "Large language models (LLMs) are increasingly evaluated on single-answer multiple-choice tasks, yet many real-world problems require identifying all correct answers from set of options. This capability remains underexplored. We introduce SATA-BENCH, the first dedicated benchmark for evaluating LLMs on Select All That Apply (SATA) questions across diverse domains, including reading comprehension, law, and biomedicine. Our evaluation of 27 open-source and proprietary models reveals significant gap: even the strongest model achieves only 41.8% exact match, exposing LLMs inability to reliably identify all correct answers. We find that this weakness stems from two core challenges: selection bias - models favor certain choices regardless of content, and count bias models fail to predict the correct number of answers. To address these issues, we propose Choice Funnel, decoding strategy that combines token debiasing with adaptive thresholding to guide models toward complete and accurate selections. Choice Funnel achieves up to 29% higher exact match than competitive baselines while reducing inference cost by over 64%. Our findings expose fundamental limitations in current LLMs and introduce new framework for diagnosing and improving multi-answer reasoning. We release SATA-BENCH and Choice Funnel to promote LLM development for robust decision-making in realistic, multi-answer applications. Data & Code: github.com/sata-bench/sata-bench Data & Dataset Card: huggingface.co/datasets/sata-bench/sata-bench"
        },
        {
            "title": "Introduction",
            "content": "Large Language Models (LLMs) have demonstrated remarkable success across variety of natural language processing tasks, with multiple-choice question (MCQ) answering emerging as prominent evaluation setting [40, 56]. However, most LLM benchmarks and training pipelines focus on questions with single correct answer among fixed set of options (typically four answer choices). This design choice introduces structural bias, limiting the models ability to generalize to more flexible, real-world tasks that require identifying multiple correct answers. Many real-world scenarios demand such flexibility. For instance, social media moderator might need to evaluate post for several types of toxic contentsuch as threats, hate speech, or offensive languageand accurately tag all applicable categories. Similarly, journalists extracting information from news articles or biomedical researchers annotating scientific papers with multiple relevant subdomains face tasks that extend beyond single-choice frameworks. These are instances of Select All That Apply (SATA) questions, where more than four choices are presented and multiple answers are required. Existing LLMs often struggle in such scenarios by inaccurately selecting valid options. Figure 1 illustrates this bias and the shortcomings of even advanced LLMs. Equal contribution. Correspondence to: weijiexu@amazon.com. Preprint. Despite the relevance of SATA-style tasks, there is lack of standardized benchmarks and evaluation methods tailored to this setting. Existing MCQ benchmarks predominantly assess single-answer selection, leaving gap in our understanding of how well LLMs perform in multi-answer scenarios. To address these challenges, we introduce SATA-BENCH, new benchmark suite specifically designed to evaluate and enhance LLM performance on SATA tasks. SATA-BENCH is uniquely characterized by multiple correct answers to reflect real-world scenarios, and diverse set of knowledge-based and reasoning-driven questions. By presenting LLMs with varying numbers of options and multiple valid answers, SATA-BENCH enables the assessment of their ability and biases in more realistic scenarios. SATA-BENCH extends the applicability of LLMs to diverse and complex real-world tasks. Figure 1: This is representative example to show that LLMs struggle with SATA (Select All That Apply) questions. The models often provide wrong answers when multiple correct options are present. Our Contributions. The primary contributions of this paper are: 1. SATA-BENCH Data Curation: We curate high-quality, diverse benchmark dataset explicitly designed to challenge LLMs on multi-answer tasks. SATA-BENCH features 1,604 human-validated questions with varying difficulty levels, multiple correct answers, and carefully constructed distractor options. We provide readability, confusion, and similarity analyses to ensure clarity, diversity, and task complexity across six real-world domains: reading comprehension, news classification, event detection, toxicity identification, biomedical concept tagging, and legal document analysis. 2. Comprehensive Evaluation of 27 LLMs on SATA Tasks: We conduct the largest-to-date evaluation of 27 state-of-the-art proprietary and open-source LLMs on SATA questions, revealing that even top-performing models achieve maximum exact match of only 41.8%. Our analysis highlights two key failure modesselection bias and count biasand demonstrates how current LLMs systematically underestimate the number of correct answers. We also break down performance across domains, showing that no single model dominates across all task types, underscoring the diversity and difficulty of SATA-BENCH. 3. Choice Funnel Decoding Algorithm: We propose Choice Funnel, novel iterative decoding strategy that combines token debiasing and adaptive thresholding to mitigate selection and count bias. Choice Funnel outperforms three competitive baselines across 7 open-source models, achieving up to 29% improvements in exact match accuracy while reducing inference costs by over 64%. We demonstrate that Choice Funnel enables smaller models to rival or outperform much larger models on SATA tasks, offering scalable solution for improving multi-answer reasoning."
        },
        {
            "title": "2 SATA-BENCH Data Curation",
            "content": "Our objective is to develop dataset that encompasses diverse range of tasks and domains and poses sufficient challenges to differentiate the capabilities of LLMs. The data curation process consists of three stages: selection of relevant source datasets, transformation of the data into SATA format, and filtering of questions for readability, diversity, and clarity (see Figure 2). We developed SATA-BENCH questions to include tasks in Reading Comprehension [28], Text Classification (News [38], and Events [19]), and Domain Understanding (Toxicity [21], Biomedicine [41], and Laws [7]). Detailed descriptions of each source dataset are provided in Appendix A. 2.1 SATA Transformation We transformed the original question/text to SATA questions following the steps below: 1. Collect text content, labels, and the total number of options for each question. 2. Maintain the option-to-answer 2 Figure 2: SATA-BENCH Data Curation Process. The source data is converted to SATA format and then filtered for readability, diversity (via question similarity), difficulty (via confusion scoring), and clarity (via human validation). Additional dataset-specific transformation steps are described in Appendix B. Table 1: Statistics of the SATA-BENCH dataset (by data source). We report the following metrics: n: number of instances, LC: label cardinality, m: mean number of correct answers, me: median number of correct answers, min: minimum number of correct answers, max: maximum number of correct answers, r: ratio of the number of choices to the median number of correct answers (LC/me), w: mean word count, FRE: Flesch Reading Ease score, FGL: Flesch-Kincaid Grade Level score, ARI: Automated Readability Index, DCR: Dale-Chall Readability score, GFI: Gunning Fog Index, Confusion: mean confusion score. The final row summarizes these metrics across the entire SATA-BENCH dataset. Data Source Reading Comprehension Toxicity News Biomedicine Laws Events 328 255 248 260 311 202 LC 315 8 6 15 15 6 2.8 2.56 2.36 5.67 5.3 2.63 me min max 10 2 2 6 2 2 5 2 2 12 2 5 10 2 5 5 2 2 na 4 3 3 3 2018.46 1015.32 785.93 1540.47 5761.69 3644.06 FRE 59.94 37.83 62.51 40.82 45.09 50.64 FGL 9.22 12.28 8.92 10.95 12.29 10.83 ARI 12.57 13.33 11.15 12.41 14.06 13.08 DCR 9.27 10.49 11.1 10.83 8.75 9.7 GFI 9.75 12.57 10.94 12.29 12.07 11. Confusion 0.33 0.27 0.26 0.21 0.14 0.25 SATA-BENCH 1604 3-16 3.55 2 10 3.2 2491.01 49.56 10. 12.80 9.96 11.51 0.24 ratio between 2 and 3 for consistency and improved difficulty [51], and determine the number of options (k) based on the number of correct answers for each question. 3. Generate distractor options for each question, consisting of the correct options and options randomly selected from the pool. 4. Shuffle the list of options to eliminate any label imbalance. 2.2 Question Filtering From the original SATA questions (characteristics shown in Table 5 in Appendix), we filter them using the following steps (see Figure 2): Initial Filtering. To clean the original source data, questions with fewer than ten words were eliminated [44, 26]. Additionally, to ensure each question is understandable and solvable, we excluded those containing ambiguous, vague, or subjective terms (details are provided in Appendix B.1) [33]. Readability. To ensure SATA-BENCH questions are understandable and challenging, we assessed the readability of each question using the Flesch Reading Ease (FRE) score [20] and the Gunning Fog Index (GFI) [22]. We retained questions with an FRE score between 20-100 (inclusive), filtering out extremely easy or difficult questions [29], and GFI score between 6-17, corresponding to 6th grade to graduate level difficulty [22]. This filtering removed unclear or trivial questions while maintaining range of difficulties. 2 Question Similarity. Some of the original questions were too similar to each other, adding redundancy to the dataset. Following [58], we calculated the similarity score for each question pair using the 2We also included four additional measures of readability (Flesch-Kincaid Grade Level (FGL) [29], Automated Readability Index (ARI) [29], and Dale Chall Readability (DCR) [12]) in the SATA-BENCH dataset. 3 cosine similarity score of the term frequency-inverse document frequency (TF-IDF) matrix [47] due to its efficiency. We removed questions sharing at least 80% cosine similarity to reduce duplication. Confusion Score. The difficulty of SATA questions is largely related to how confusing the provided options are. We assessed the semantic similarity between the correct answers and the distractors as measure of question-level confusion score [6]. We used ST5-XXL [36] for semantic similarity calculation, as it performed best in [34]. Then, to balance the confusion level, we grouped the questions into 10 bins according to the confusion score and sampled between 50-300 records from each bin to ensure SATA-BENCH contains diverse range of difficulty levels. Figures 5 and 6 show the distribution of the confusion scores before and after filtering as well as by source dataset. We release dataset comprising 7,983 pre-validation questions as by-product of our work. Human Validation. Human evaluation was conducted in two stages. First, we used it to identify and remove questions containing ambiguous information as detailed in Appendix B.2. In the final stage, three human annotators reviewed all remaining questions to correct labeling errors; questions lacking consensus among all three annotators were excluded, as detailed in Appendix B.3. Statistics of the final SATA-BENCH is shown in Table 1. 2.3 SATA-BENCH Characteristics After these curation steps, SATA-BENCH has the following characteristics: (i) Multiple choices with multiple correct answers. All questions have multiple choices and more than one correct answers. (ii) Diversity. The questions come from various disciplines, including knowledge-based tasks such as domain understanding and reasoning-driven questions such as reading comprehension (see Figure 3). (iii) Human validation. All questions are manually validated to ensure they are clear and correct by readability scores and human validation. SATA-BENCH removes ambiguous or overly simplistic items while maintaining range of reading levels (from 6th grade to graduate level) (see Figure 3). (iv) Challenging. We constructed the SATA-BENCH benchmark so that 76% have FRE score within the standard range (60-70) and the average GFI score is approximately 13th grade (equivalent to the first year of college/university). The mean semantic similarity between correct answers and distractors (incorrect options) is 0.24, exhibiting right-skewed distribution (skewness = 1.8). Most questions cluster around similarity score of 0.22, with few more difficult questions extending the tail towards the higher end (see Figure 3). Figure 3: SATA-BENCH Dataset Overview. SATA-BENCH covers diverse set of topics and achieves balance between readability and difficulty (measured by confusion score). d1: Reading Comprehension, d2: Toxicity, d3: News, d4: Biomedicine, d5: Laws, and d6: Events."
        },
        {
            "title": "3 Experiments",
            "content": "This section presents the experiments conducted to assess the capabilities of LLMs on SATA questions. The benchmark includes 16 proprietary models and 14 open-source models. (See Table 7 for full model cards.) Experimental Setup. Because our benchmark contains diverse questions, we use zero-shot evaluation. The system prompt specifies that each question has at least two correct answers, and we instructed the LLM to output the labeled result in JSONL format [25, 57]. Furthermore, we utilize CoT prompting strategy as described in [37]. We then extract the answer from the JSONL file using both exact match and fuzzy match. For cases where JSONL extraction fails (fewer than 3% of cases), 4 we use Claude 3 Haiku and Human Labelers to extract the correct options from the answers provided. For smaller models, the percentage of cases where JSONL extraction fails exceeds 5%, making the above methods less reliable. Following [24], we remove CoT and use the probability of the first output token to retrieve options. We hold out dataset of 100 randomly sampled instances from the benchmark dataset to generate threshold for each model with optimal Jaccard Index [5]. We select all options with probability greater than that threshold value. Note that this method applies only to models with accessible token probabilities. We have also included the performance of non-expert humans on the benchmark (see Appendix E). Metrics. We evaluate models using metrics across three categories: performance, selection bias, and count bias (details in Appendix F). Performance is measured using Exact Match (EM), Jaccard Index (JI), Mean Average Precision (Precision), and Mean Average Recall (Recall). Selection bias includes RStd [56] and RSD [11, 42]. We also introduce Selection Probability Divergence (SPD) to quantify unselection bias, form of selection bias where models consistently avoid certain options. Count bias is assessed using the mean count difference (CtDif), mean absolute count difference (CtDifAbs), and count accuracy (CtAcc). Table 2: Performance comparison of 27 different LLMs across various metrics on SATA-BENCH. We highlight the best (bold) and second-best (underline) values. Columns labeled [()] indicate higheris-better; columns labeled [()] indicate lower-is-better. Models with explicit reasoning capabilities are highlighted in italic. All numeric values are rounded to two decimal places. We retrieve exact labels for models evaluated using Inference-Based Retrieval + CoT prompting. For models evaluated under Probability-Based Retrieval, we select labels based on token probability thresholds. Model Name EM Precision Recall JI SPD RStd RSD CtDif CtDifAbs CtAcc Performance Selection Bias Count Bias Inference Based Retrieval + CoT 81.22 85.66 86.31 83.17 78.77 77.15 78.81 83.25 85.08 80.16 77.19 79.98 78.23 75.25 75.31 72.36 62.59 59.53 60.80 60.64 54.44 73.91 75.23 74.40 74.11 70.96 70.98 70.72 72.58 71.12 70.63 70.15 70.71 69.90 67.15 67.18 64.49 57.16 55.74 55.78 55.59 45.02 0.38 0.13 0.30 0.21 0.46 0.49 0.36 0.12 0.33 0.42 0.62 0.17 0.30 0.43 0.33 0.94 1.33 2.24 1.84 1.81 1. 6.79 5.98 6.26 6.63 18.77 6.59 7.37 4.76 7.31 6.24 8.26 6.14 6.69 8.41 6.90 17.44 10.89 10.72 11.10 10.06 15.32 Probability Based Retrieval 53.23 47.37 50.80 45.81 38.76 35.67 30.82 46.63 43.64 41.15 40.02 37.58 34.57 29.69 11.42 12.09 20.62 13.38 21.01 23.22 21.62 19.47 17.85 29.00 21.62 18.02 18.57 13. 87.50 85.52 83.93 85.90 85.03 85.35 84.58 84.58 80.26 83.59 86.97 85.01 86.05 87.57 86.24 84.62 88.20 88.54 89.56 89.56 60.62 81.46 80.30 66.09 72.20 87.84 87.25 77.98 0.06 0.06 0.07 0.06 0.34 0.25 0.07 0.06 0.35 0.07 0.07 0.06 0.06 0.09 0.45 0.03 0.12 0.11 0.27 0.10 1.46 1.27 1.09 1.51 1.14 1.06 1.22 1.59 -0.39 -0.04 0.06 -0.20 -0.32 -0.43 -0.35 -0.01 0.18 -0.22 -0.52 -0.23 -0.39 -0.46 -0.39 -0.57 -1.10 -1.18 -1.09 -1.12 -0.6 -1.35 -1.59 -0.87 -1.29 -2.24 -2.33 -2. 0.94 0.85 0.93 0.82 0.87 0.93 0.83 0.88 1.01 0.85 0.93 0.91 0.97 1.06 1.02 1.13 1.47 1.43 1.41 1.48 1.44 1.95 1.88 1.71 1.75 2.26 2.35 2.39 46.12 45.52 44.24 46.61 44.48 43.58 48.00 43.76 42.61 47.15 44.36 39.94 38.61 38.55 36.30 33.52 27.27 24.85 24.30 22.12 34.12 21.01 22.00 20.09 20.42 11.93 7.22 7.35 O3 GPT4.1 Grok 3 Think GPT4 Claude 3.7 Think Claude 3.7 Claude 3 Sonnet Geimini 2.5 Think Claude 3.5 Haiku Claude 3 Haiku Claude 3 Opus Gemini 2 Flash GPT 4.1 mini Claude 3.5 Sonnet Llama 3.1 405B Deepseek R1 Mistral Large V2 Qwen Plus Llama 3.2 90B Llama 3.1 70B Non-expert Human Mistral 8B Llama3 8B Bloomz 7B DeepSeek R1 Distill 8B Qwen2.5 14B Phi3 7B Phi4-mini-reasoning 41.77 40.49 39.71 39.47 37.92 37.82 36.49 36.46 35.89 35.64 35.59 34.60 33.46 32.22 30.17 28.17 22.83 21.12 18.30 17.94 17.93 14.73 13.82 11.27 8.85 6.30 2.97 2.12 3.1 Key Observations SATA-BENCH is challenging and different. All models have precision greater than 80%, but none achieves an EM score above 42%. This indicates that while models often select some correct answers, they fail to consistently identify all of them. In general, proprietary models have higher EM and Precision than their open-source counterparts. Unlike in other benchmarks, there is no single model that dominates across all performance metrics. Some large reasoning models (LRM), such as O3 and Grok 3 Think, have higher EM and Recall than non-reasoning models. Interestingly, larger and more recent models do not necessarily perform better. For example, Claude 3 Sonnet performs better than Claude 3.5 Sonnet V1 and Claude 3 Opus in exact match rate. However, within the Claude family, larger models always have higher precision. For example, Claude 3 Opus has the highest precision among the Claude 3 model family. According to [4, 13], these results contrast with existing single-choice MCQ LLM performance, such as MMLU [24] and ARC [10], where larger or more recent models tend to show clear improvements. Models choose too few answers. Nearly all LLMs tend to select fewer answers than required. As an extreme example, Llama 3.1 70B on average, selects one fewer option per question than the correct number. Accordingly, Llama 3.1 70B achieves the highest precision but the lowest exact match (EM). The tendency to under-select increases as the number of correct answers grows (see Figure 11). This behavior negatively impacts the EM rate for questions with many correct choices (see Figure 12). The highest CtAcc is only 48%, even the best model predicts the correct number of answers in fewer than half of the questions. We hypothesize that this behavior results from models being primarily trained and evaluated on benchmarks where each question has only one correct answer, making them poorly suited for SATA tasks. Through t-test, we observe that the mean of the CtDif column is significantly lower than 0, with p-value of 1.70 106, supporting the observation that models consistently under-select answers. Unselection bias exists. Some models exhibit tendency to avoid selecting specific labels, even when they are correct.3 When comparing Selection Probability Divergence (SPD) from our benchmark with 1,000 randomly simulated SPDs, Welchs t-test reveals that LLMs SPD is significantly higher than random, with p-value of 0.0467. Even the best model in terms of selection bias (Gemini) underperforms on label M, with its recall rate being 6.3% lower than its average recall (Figure 10). There is no clear winner across datasets. When breaking down the benchmark by its six datasets, different models excel in different domains. Top-performing models vary by dataset, showcasing the diversity of SATA-BENCH. Considering exact match rate (see Figure 13): O3 excels in News and Events classification. DeepSeek R1 leads in Biomedicine, GPT-4.1 performs best in reading comprehension, and Claude 3.7 dominates in toxicity and laws. This highlights the importance of domain-specific evaluation and the breadth of challenges covered by the benchmark. 3.2 Ablation Studies We conducted ablation studies to test different strategies for improving model performance. We report the average results across three models selected for diverse profiles in terms of cost, opensource availability, and overall performance. The complete prompts are provided in Appendix H.3. Table 3: Average performance of three models. The first column shows row numbers for reference. Experiment 1/2/3/4 1 a/b/c/d 2 default 3 few shots 4 option by option 5 option few shots 6 7 with avg count 8 with count number single choice 9 EM Precision RStd CtDif -0.37 -0.26 -0.25 -0.42 -0.64 -0.48 -0.40 -0.08 NA 82.99 83.10 84.62 76.61 86.28 85.80 76.17 83.30 NA 10.22 11.56 7.37 17.33 4.81 7.93 14.90 3.45 NA 35.50 30.69 33.00 28.35 30.50 30.87 27.33 53.95 45. Improving performance on SATA-BENCH is challenging. We tried several approaches to improve performance, but none yielded consistent or significant improvements. Changing the symbol used for each answer choice did not improve the selection bias. We replaced the default option IDs from A/B/C/D to a/b/c/d and 1/2/3/4. While the 1/2/3/4 format achieved slightly better exact match accuracy, it also increased selection bias and reduced precision. Overall, we did not observe performance improvement by changing symbols (see rows 1-3 in Table 3). We provided few-shot examples in the prompt before the test models. However, this strategy did not lead to meaningful improvements in performance (see row 4 in Table 3). Inspired by survey science [46, 39], we instructed the models to examine each option individually. However, the models still selected too few options overall and did not improve performance (see rows 5-6 in Table 3). Given more information, two approaches do improve performance and can provide additional insights into why the models struggle. 3However, we cannot exclude position effect [56]. 6 Providing the number of correct choices improves performance. To understand how much error is due to the models lack of knowledge regarding the number of correct options, we explicitly provided this information in the instruction for each question. This increased the exact match rate by 20.95 percentage points and reduced the selection bias metric RStd. However,when we instead provided the average number of correct choices across all questions in SATA-BENCH, performance declined (see rows 7-8 in Table 3). Converting questions to multi-choice question with one correct answer. For example, consider question with three correct answers and six incorrect answers: we expanded it into three separate single-choice questions, each with one correct answer and six incorrect answers. We redefined the exact match rate as the percent of all original questions where model answered all expanded questions correctly. This approach improved performance by 12.53% (see row 9 in Table 3), demonstrating that SATA questions are significantly harder for LLMs than single choice questions. Both results suggest that while models can often identify individual correct answers, they lack awareness of how many correct answers exist, which contributes to their low performance."
        },
        {
            "title": "Improving Performance on SATA Questions",
            "content": "The experimental results in Section 3 demonstrate that Selection Bias and Count Bias degrade LLM performance on SATA-BENCH, and that simple prompting strategies do not lead to significant improvements. This section focuses on improving performance on open-source models, which allows us to leverage token-level logits or probability estimates from the first token prediction. Algorithm 1: Choice Funnel Input :LLM πθ, SATA problem , option set O, OT stop option, τ confidence threshold # Initialize the selected option set while = do # Generate prompt with available options MakeSATAPrompt(T , O) # Get first token probability distribution and apply token debiasing break DebiasingFunction(πθ(P)) # Select option with highest probability arg maxoO p(o) # 1. stop when \"None of the above\" is selected if = NOTA then To address Selection Bias, we draw from prior research on token debiasing methods [9, 56] in the MCQ setting, where selection bias is attributed to the priori probability mass assigned by the model to specific option IDs. These methods propose various techniques to capture and remove such biases. We hypothesize that similar debiasing techniques can be adapted to mitigate unselection bias in SATA tasks. To address Count Bias, we retrieve the predicted probabilities of option IDs and select options whose probabilities exceed predefined threshold. However, because SATA-BENCH includes large option set, the probability distribution tends to decay rapidly, with most options receiving near-zero probability mass beyond the first few choices. This makes it challenging to establish reliable threshold. Converting SATA questions into multiple binary classification problems helps but significantly increases inference cost. end {o} # 2. stop when the confidence threshold is reached if p(o) > τ then break end if length(R) = 1 then end {o} end Output :R {NOTA} Choice Funnel Algorithm. To improve model performance on SATA problems, we propose decoding method called Choice Funnel (see Algorithm 1). This approach first adds an auxiliary option None of the above. It then selects the option with the highest first debiased token probability and removes it from the option set. This process repeats iteratively until one of two stopping conditions is met: (i) the model selects the None of the above option or (ii) the probability of the next selected option falls below predefined confidence threshold. The idea of introducing None of the above\" (NOTA) comes from the traditional survey science domain, where options like dont know\" (idk) are commonly included to improve the data collected in surveys [45]. Recent research shows that survey design principles can inform LLM development [17] and that LLMs exhibit similar biased response behaviors as humans [9, 16]. In our case NOTA outperforms idk (see ablation study in Appendix M.1). 7 The intuition behind the second stopping condition comes from our observation of model output probabilities, where the highest token probability tends to be lower at the beginning of iterations, since the model treats multiple options as equally correct. Later in the process, relatively higher probability is assigned to the final remaining correct option in the option set. We also show that Choice Funnel performs best when both stopping conditions are used together (see ablation study in Appendix M.3). Regarding the choice of DebiasingFunction in Algorithm 1, Choice Funnel is flexible and can incorporate any token debiasing method proven effective in MCQ settings. We demonstrate one such debiasing method in Section 4. Additional ablation results on each sub-component of Choice Funnel are provided in Appendix M.2. Finally, the inference cost of Choice Funnel, measured by the number of model forward passes, scales linearly with the number of correct labels rather than the total number of labels. This makes the method particularly efficient when the correct labels represent small fraction of the option set. Experimental Setup. We adapted the PriDe algorithm [56] as the token debiasing method in our experiments due to its label-free and computationally efficient implementation. It works by first estimating the models prior bias towards specific option ID tokens (e.g., A, B, C) through random permutations of option contents in small subset of test samples (10% in our experiments). We then use this estimated prior to adjust the prediction distribution on the remaining samples, separating the models inherent positional and token biases from its task-specific predictions. Because the original PriDe algorithm was designed for standard single-answer MCQ settings, we modified it to better fit our SATA setting (see Appendix K). We evaluate the performance of Choice Funnel against three baseline methods that rely on first-token probabilities: (i) First token probability with fixed threshold, as defined in Section 3 (referred to as first token). (ii) Building on method 1, we apply PriDe debiasing method [56] (referred to as first token debiasing). (iii) Convert each option into an individual binary yes/no question (referred to as yes/no). We expect yes/no to be strong baseline, as it evaluates each choice independently. In this study, we use basic prompts (see Appendix H) and experiment with 7 LLMs from Table 2 that fall under the Probability Based Retrieval category (more details in Appendix L). For each model, we compute metrics reported in Table 2, and additionally report an InfCost metric to capture the number of model forward passes required for each method. Key Observations. Choice Funnel consistently outperforms all three baselines across all 7 models in EM, SPD, and CtAcc (see Table 4). Choice Funnel reduces unselection bias and count bias compared to the first token baseline, Choice Funnel achieves an average 56.16% reduction in SPD and 154.62% improvement in CtAcc, resulting in 277.48% gain in Exact Match (EM) performance. While reasoning models also show improvements with Choice Funnel, we exclude these from aggregate calculations as their exceptionally low baselines would artificially inflate gains. When compared to our strongest baseline, the yes/no approach, Choice Funnel achieves substantial 29.87% improvement in EM while reducing model forward passes by 64.48% thanks to its early stopping mechanism, demonstrating efficient inference scalability. Statistical significance testing (t-test) confirms that Choice Funnel significantly outperforms both yes/no and first token debiasing in EM and CtAcc, with maximum p-value of 0.0079. While our models parameter sizes (7B-14B) limit direct comparison to much larger proprietary models, Choice Funnels performance on the phi3-small model still exceeds that of larger models such as Llama-90B and Mistral-Large V2 (see Table 2). This further underscores the effectiveness of our method. Additional ablation studies on the individual components of Choice Funnel are provided in Appendix M."
        },
        {
            "title": "5 Related Work",
            "content": "SATA Benchmark. Many existing MCQ benchmarks have only one correct answer and thus cannot test LLMs ability to select multiple correct choices. On the one hand, existing SATA datasets, such as [31, 30, 3, 27, 8], have more than 30 labels per question to choose from. This makes it impractical for LLMs to identify all correct labels from such large label pools. Other SATA-style datasets test narrow, specialized capabilities, such as emotional understanding [15] or music style understanding [55], which are less emphasized in mainstream LLM benchmarks. Since most existing methods to solve SATA questions require converting questions to bag-of-words [32], and as result, most of the above datasets exist only in bag-of-words format, making them unsuitable for evaluating LLMs in our benchmark setting. To our knowledge, there is currently no existing LLM benchmark that consists exclusively of SATA questions. 8 Table 4: Performance of various models on SATA-BENCH using different decoding methods. Choice Funnel achieves generally better performance, effectively reducing selection and count bias compared to three baseline methods. Best values in each column are highlighted in bold. Columns labeled [] indicate higher-is-better; columns labeled [] indicate lower-is-better. All numeric values are rounded to two decimal places. Model Name EM Precision Recall JI SPD CtDifAbs CtAcc InfCost Mistral-8B + first token Mistral-8B + first token debiasing Mistral-8B + yes/no Mistral-8B + choice funnel Phi3-7B + first token Phi3-7B + first token debiasing Phi3-7B + yes/no Phi3-7B + choice funnel Qwen2.5-14B + first token Qwen2.5-14B + first token debiasing Qwen2.5-14B + yes/no Qwen2.5-14B + choice funnel Bloomz-7B + first token Bloomz-7B + first token debiasing Bloomz-7B + yes/no Bloomz-7B + choice funnel Llama3-8B + first token Llama3-8B + first token debiasing Llama3-8B + yes/no Llama3-8B + choice funnel Phi4-mini-reasoning + first token Phi4-mini-reasoning + first token debiasing Phi4-mini-reasoning + yes/no Phi4-mini-reasoning + choice funnel DeepSeek-R1-Distill-Llama-8B + first token DeepSeek-R1-Distill-Llama-8B + first token debiasing DeepSeek-R1-Distill-Llama-8B + yes/no DeepSeek-R1-Distill-Llama-8B + choice funnel 14.73 8.91 16.48 20.24 2.97 1.76 25.45 29.27 6.30 4.61 25.64 27.82 11.27 7.09 11.93 20.18 13.82 7.58 14.85 19.88 2.12 1.27 4.36 18.42 8.85 5.45 0.12 14. 81.46 65.17 75.49 86.03 87.25 67.92 78.41 83.27 87.84 67.95 79.80 85.69 66.09 59.07 39.80 66.62 80.30 62.83 70.30 78.69 77.98 59.77 51.08 74.87 72.20 59.29 40.31 75.56 53.23 37.97 55.91 55.78 35.67 28.24 72.40 70.24 38.76 31.49 60.56 67.07 50.80 38.41 42.67 54.90 47.37 32.28 65.61 56.19 30.82 25.74 81.59 54.84 45.81 31.12 89.51 45.56 46.63 34.27 48.80 52.56 34.57 27.47 60.03 61.85 37.58 30.36 56.18 61.12 41.15 32.05 29.40 46.15 43.64 30.38 51.43 50.36 29.69 24.51 45.24 49.14 40.02 28.48 40.19 42.87 11.42 152.23 12.88 8.50 23.22 175.24 1.39 3.47 21.01 154.26 2.76 3.80 20.62 149.17 17.78 9.82 12.09 151.74 1.91 7.75 21.62 156.16 7.09 3.30 13.38 134.36 27.96 12.37 1.95 2.34 1.94 1.74 2.35 2.50 1.64 1.42 2.26 2.43 1.52 1.42 1.71 2.19 3.24 1.71 1.88 2.34 1.78 1.66 2.39 2.32 3.19 1.59 1.75 2.14 5.73 1.87 0.21 0.14 0.21 0.27 0.07 0.05 0.30 0.38 0.12 0.09 0.31 0.35 0.20 0.15 0.13 0.32 0.22 0.14 0.23 0.33 0.07 0.07 0.10 0.27 0.20 0.14 0.01 0. 1650 2534 15517 4803 1650 2534 15517 6339 1650 2534 15517 6005 1650 2534 15517 5440 1650 2534 15517 4975 1650 2534 15517 6003 1650 2534 15517 4630 Selection Bias. Many previous papers have discussed the tendency of LLMs to favor choices based on option order or specific symbols when answering MCQs [23, 53]. However, these papers have primarily focused on single-answer questions. common approach to reducing selection bias involves calibrating output probabilities using the prior bias of an option ID [56]. However, it remains unclear how to define or compute such priors in SATA questions."
        },
        {
            "title": "6 Conclusion",
            "content": "We introduced SATA-BENCH, carefully curated suite of Select All That Apply\" (SATA) questions designed to evaluate LLMs in scenarios where multiple correct answers must be identified. Spanning diverse question typesfrom reading comprehension to text classificationand covering domains such as law and biomedicine, SATA-BENCH presents comprehensive challenge for current LLMs. Our benchmarking study of both open-source and proprietary models revealed best exact-matching accuracy of only 41.8%, highlighting the difficulty of selecting all the correct options. Our ablation studies indicate that simple prompting strategy alone cannot boost exact match performance, primarily because LLMs struggle to determine the correct number of answers. This limitation stems from the fact that SATA-style questions are rarely included in LLMs benchmark datasets, making LLMs prone to selection and count biases even when they recognize some correct options. To address this limitation, we proposed the Choice Funnel algorithm, which significantly improves exact match performance by systematically guiding the selection process. Our findings highlight the need for more focused research on handling multiple answer tasks, where partial correctness is insufficient. We hope that SATA-BENCH and the choice funnel methodology will encourage the development of more robust LLMs capable of handling realistic, multi-answer scenarios, ultimately improving their effectiveness in real-world applications that require identifying all relevant answers."
        },
        {
            "title": "Impact Statement",
            "content": "The introduction of SATA-BENCH marks crucial advancement in evaluating Large Language Models (LLMs) on Select All That Apply\" (SATA) multiple-choice questions. By addressing significant gap in existing benchmarks, which predominantly focus on single-answer multiple-choice 9 tasks, SATA-BENCH challenges LLMs with real-world scenarios requiring multiple correct responses across domains such as reading comprehension, law, and biomedicine. This benchmark highlights the limitations of current LLMs, which struggle to accurately determine all valid answers, achieving best-case exact match accuracy of only 41.8%. SATA-BENCH impact extends beyond evaluation, as it reveals key biases in LLM decision-making, such as count bias and selection bias, which hinder performance on multi-answer tasks. To address these shortcomings, the development of the Choice Funnel algorithm demonstrates novel approach to systematically improving LLM selection accuracy, significantly enhancing model performance in SATA tasks. While the current focus is on knowledge-intensive domains, the potential for expansion into additional fields such as mathematics, coding, and instruction following is vast. SATA-BENCH can also be extended to free-text tasks, where the set of correct responses is not explicitly provided, and to other modalities, such as voice and vision. This would further refine LLM capabilities in handling complex, multi-faceted decision-making tasks. By pushing the boundaries of LLM evaluation, SATA-BENCH lays the foundation for the next generation of AI systems capable of more nuanced, flexible reasoning in diverse real-world applications."
        },
        {
            "title": "Limitations",
            "content": "Memorization. While we believe most questions in SATA-BENCH have not been seen during pretraining, we cannot fully rule out the possibility that some LLMs may have been exposed, even partially, to the source datasets. We do not have access to the pretraining data of proprietary models and therefore cannot conclusively assess memorization. Domain Coverage. SATA-BENCH spans six diverse domains, including reading comprehension, biomedicine, and law. However, the total number of domains remains limited compared to larger-scale benchmarks such as MMLU, indicating that further expansion is needed for broader generalization. Text Modality. Our benchmark is text-only. Real-world tasks often require multimodal reasoning (e.g., interpreting charts, images, or audio), which SATA-BENCH does not evaluate. We also do not address other data modalities, such as structured tabular data or sensor data, which are common in practical applications. Label Correctness. Although we perform rigorous human validation and evaluations, we acknowledge that human beings can make mistakes. Some domains, such as biomedicine and law, are inherently complex and may contain subtle ambiguities. Thus, we cannot guarantee perfect correctness of all labels despite triple human annotation and agreement filtering. Language Limitation. SATA-BENCH includes only English-language questions. Evaluating multilingual capabilities or cross-lingual transfer remains work for the future."
        },
        {
            "title": "References",
            "content": "[1] Marah Abdin, Jyoti Aneja, Hany Awadalla, Ahmed Awadallah, Ammar Ahmad Awan, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Jianmin Bao, Harkirat Behl, et al. Phi-3 technical report: highly capable language model locally on your phone. arXiv preprint arXiv:2404.14219, 2024. [2] Marah Abdin, Sahaj Agarwal, Ahmed Awadallah, Vidhisha Balachandran, Harkirat Behl, Lingjiao Chen, Gustavo de Rosa, Suriya Gunasekar, Mojan Javaheripi, Neel Joshi, et al. Phi4-reasoning technical report. arXiv preprint arXiv:2504.21318, 2025. URL https://arxiv. org/abs/2504.21318. Version 1, submitted on 30 Apr 2025. [3] Rami Aly, Steffen Remus, and Chris Biemann. Hierarchical multi-label classification of text with capsule networks. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop, pages 323330, 2019. URL https://aclanthology.org/P19-2045/. [4] Anthropic. Claude (version 3.5 sonnet), 2024. URL https://www.anthropic.com/claude. AI language model. [5] Jasmin Bogatinovski, Ljupˇco Todorovski, Sašo Džeroski, and Dragi Kocev. Comprehensive comparative study of multi-label classification methods. Expert Systems with Applications, 203: 117215, 2022. [6] Okan Bulut, Guher Gorgun, and Bin Tan. Item difficulty and response time prediction with large language models: An empirical analysis of usmle items. 2024. [7] Ilias Chalkidis, Manos Fergadiotis, Prodromos Malakasiotis, and Ion Androutsopoulos. Largescale multi-label text classification on eu legislation, 2019. URL https://arxiv.org/abs/ 1906.02192. [8] Francisco Charte, Antonio J. Rivera, María J. del Jesus, and Francisco Herrera. Stackex: collection of multi-label datasets from stack exchange forums. Journal of Multiple-Valued Logic and Soft Computing, 25(5):429448, 2015. URL https://github.com/yourURL/ stackex-datasets. [9] Hyeong Kyu Choi, Weijie Xu, Chi Xue, Stephanie Eckman, and Chandan K. Reddy. Mitigating selection bias with node pruning and auxiliary options. arXiv preprint arXiv:2409.18857, 2024. URL https://arxiv.org/abs/2409.18857. [10] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge, 2018. URL https://arxiv.org/abs/1803.05457. [11] Francesco Croce, Maksym Andriushchenko, Vikash Sehwag, Edoardo Debenedetti, Nicolas Flammarion, Mung Chiang, Prateek Mittal, and Matthias Hein. Robustbench: standardized adversarial robustness benchmark. arXiv preprint arXiv:2010.09670, 2020. [12] Edgar Dale and Jeanne S. Chall. formula for predicting readability. Educational Research Bulletin, 27(1):1120, 28, 1948. [13] DeepSeek-AI and Aixin Liu el at. Deepseek-v3 technical report, 2024. URL https://arxiv. org/abs/2412.19437. [14] DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. URL https: //arxiv.org/abs/2501.12948. Version 1, submitted on 22 Jan 2025. [15] Dorottya Demszky, Dana Movshovitz-Attias, Jeongwoo Ko, Alan Cowen, Gaurav Nemade, and Sujith Ravi. Goemotions: dataset of fine-grained emotions. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 40404054, 2020. URL https://aclanthology.org/2020.acl-main.372/. [16] Ricardo Dominguez-Olmedo, Moritz Hardt, and Celestine Mendler-Dünner. Questioning the survey responses of large language models, 2024. URL https://arxiv.org/abs/2306. 07951. [17] Stephanie Eckman, Barbara Plank, and Frauke Kreuter. Position: Insights from survey methodology can improve training data. In Proceedings of the 41st International Conference on Machine Learning, volume 235 of Proceedings of Machine Learning Research, pages 1226812283. PMLR, 2024. URL https://arxiv.org/abs/2403.01208. [18] EUR-Lex. Content statistics, 2018. URL http://data.europa.eu/88u/dataset/ eur-lex-statistics. [Data set]. [19] Event-Classification. Event-Classification. URL https://huggingface.co/datasets/ knowledgator/events_classification_biotech. [Data set]. [20] Rudolf Flesch. new readability yardstick. Journal of Applied Psychology, 32(3):221233, 1948. doi: 10.1037/h0057532. [21] Samuel Gehman, Suchin Gururangan, Maarten Sap, Yejin Choi, and Noah A. Smith. Realtoxicityprompts: Evaluating neural toxic degeneration in language models, 2020. URL https://arxiv.org/abs/2009.11462. [22] Robert Gunning. The Technique of Clear Writing. McGraw-Hill, New York, 1952. [23] Vipul Gupta, David Pantoja, Candace Ross, Adina Williams, and Megan Ung. Changing answer order can decrease mmlu accuracy. arXiv preprint arXiv:2406.19470, 2024. URL https://arxiv.org/pdf/2406.19470. [24] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding, 2021. URL https: //arxiv.org/abs/2009.03300. [25] Amazon Artificial General Intelligence. The amazon nova family of models: Technical report and model card. Amazon Technical Reports, 2024. [26] Indunil Karunarathna, Fernando, Ekanayake, Hapuarachchi, Gunasena, Aluthge, Perera, Gunathilake, Kapila De Alvis, Gunawardana, et al. Validating mcqs: critical step in specialist. [27] Ioannis Katakis, Grigorios Tsoumakas, and Ioannis Vlahavas. Multilabel text classification for automated tag suggestion. In Proceedings of the ECML/PKDD 2008 Discovery Challenge, 2008. URL http://www.kde.cs.uni-kassel.de/ws/dc08/. This dataset contains 7395 BibTeX entries from the BibSonomy social bookmark and publication sharing system, annotated with subset of the tags assigned by BibSonomy users. [28] Daniel Khashabi, Snigdha Chaturvedi, Michael Roth, Shyam Upadhyay, and Dan Roth. Looking beyond the surface:a challenge set for reading comprehension over multiple sentences. In Proceedings of North American Chapter of the Association for Computational Linguistics (NAACL), 2018. [29] J. Peter Kincaid, Robert P. Fishburne, Richard L. Rogers, and Brad S. Chissom. Derivation of new readability formulas (automated readability index, fog count and flesch reading ease formula) for navy enlisted personnel. Research Branch Report 8-75, Chief of Naval Technical Training, Naval Air Station Memphis, TN, 1975. [30] Kamran Kowsari, Donald E. Brown, Mojtaba Heidarysafa, Kiana Jafari Meimandi, Matthew S. Gerber, and Laura E. Barnes. Hdltex: Hierarchical deep learning for text classification. In 2017 IEEE International Conference on Machine Learning and Applications (ICMLA), pages 364371, 2017. URL https://ieeexplore.ieee.org/document/8260658. [31] David D. Lewis, Yiming Yang, Tony G. Rose, and Fan Li. Rcv1: new benchmark collection for text categorization research. Journal of Machine Learning Research, 5:361397, 2004. URL https://dl.acm.org/doi/10.5555/1005332.1005345. [32] Weiwei Liu, Haobo Wang, Xiaobo Shen, and Ivor W. Tsang. The emerging trends of multiIEEE Transactions on Pattern Analysis and Machine Intelligence, 44(11): label learning. 79557974, November 2022. ISSN 1939-3539. doi: 10.1109/tpami.2021.3119334. URL http://dx.doi.org/10.1109/TPAMI.2021.3119334. [33] Steven Moore, Eamon Costello, Huy Nguyen, and John Stamper. An automatic question usability evaluation toolkit. In International Conference on Artificial Intelligence in Education, pages 3146. Springer, 2024. [34] Niklas Muennighoff, Nouamane Tazi, Loïc Magne, and Nils Reimers. Mteb: Massive text embedding benchmark. arXiv preprint arXiv:2210.07316, 2022. [35] Niklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella Biderman, Teven Le Scao, Saiful Bari, Sheng Shen, Zheng-Xin Yong, Hailey Schoelkopf, et al. Crosslingual generalization through multitask finetuning. arXiv preprint arXiv:2211.01786, 2022. [36] Jianmo Ni, Gustavo Hernández Ábrego, Noah Constant, Ji Ma, Keith B. Hall, Daniel Cer, and Yinfei Yang. Sentence-t5: Scalable sentence encoders from pre-trained text-to-text models, 2021. URL https://arxiv.org/abs/2108.08877. [37] OpenAI and Josh Achiam el at. Gpt-4 technical report, 2024. URL https://arxiv.org/ abs/2303.08774. 12 [38] Divya Padmanabhan, Satyanath Bhat, Shirish Shevade, and Y. Narahari. Topic model based multi-label classification from the crowd, 2016. URL https://arxiv.org/abs/1604. 00783. [39] Pew Research Center. When Online Survey Respondents Only Select Some That Apply, 2019. [40] Pouya Pezeshkpour and Estevam Hruschka. Large language models sensitivity to the order of options in multiple-choice questions. In Kevin Duh, Helena Gomez, and Steven Bethard, editors, Findings of the Association for Computational Linguistics: NAACL 2024, pages 20062017, Mexico City, Mexico, June 2024. Association for Computational Linguistics. doi: 10.18653/ v1/2024.findings-naacl.130. URL https://aclanthology.org/2024.findings-naacl. 130/. [41] PubMed-MeSH. 2021. PubMed Biomedical Articles and Medical Subject Headings URL https://www.kaggle.com/datasets/owaiskhan9654/ (MeSH), pubmed-multilabel-text-classification. [Data set]. [42] Yuval Reif and Roy Schwartz. Beyond performance: Quantifying and mitigating label bias in llms. Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, 2024. URL https://arxiv. org/html/2406.19470v2. [43] David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel R. Bowman. Gpqa: graduate-level google-proof qa benchmark, 2023. URL https://arxiv.org/abs/2311.12022. [44] Penelope Jane Sanderson. Multiple-choice questions: linguistic investigation of difficulty for first-language and second-language students. PhD thesis, University of South Africa, 2010. [45] Howard Schuman and Stanley Presser. Questions and Answers in Attitude Surveys: Experiments on Question Form, Wording, and Context. SAGE Publications, Thousand Oaks, CA, 1996. [46] Jolene D. Smyth, Don A. Dillman, Leah Melani Christian, and Michael J. Stern. Comparing check-all and forced-choice question formats in web surveys. Public Opinion Quarterly, 70(1): 6677, 01 2006. ISSN 0033-362X. doi: 10.1093/poq/nfj007. URL https://doi.org/10. 1093/poq/nfj007. [47] Karen Sparck Jones. statistical interpretation of term specificity and its application in retrieval. Journal of Documentation, 28(1):1121, 1972. doi: 10.1108/eb026526. [48] Adane Nega Tarekegn, Mohib Ullah, and Faouzi Alaya Cheikh. Deep learning for multi-label learning: comprehensive survey. arXiv preprint arXiv:2401.16549, 2024. [49] Marie Tarrant, Aimee Knierim, Sasha Hayes, and James Ware. The frequency of item writing flaws in multiple-choice questions used in high stakes nursing assessments. Nurse Education Today, 26(8):662671, 2006. [50] Mistral AI Team. Ministral-8b-instruct-2410: State-of-the-art models for local intelligence, on-device computing, and at-the-edge use cases. https://huggingface.co/mistralai/ Ministral-8B-Instruct-2410, 2024. Released in October 2024. [51] Andrew Thompson and Bruce Giffin. Higher-order assessment in gross anatomy: comparison of performance on higher-versus lower-order anatomy questions between undergraduate and first-year medical students. Anatomical Sciences Education, 14(3):306316, 2021. [52] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. [53] Sheng-Lun Wei, Cheng-Kuang Wu, Hen-Hsen Huang, and Hsin-Hsi Chen. Unveiling selection biases: Exploring order and token sensitivity in large language models. Findings of the Association for Computational Linguistics ACL 2024, 2024. URL https://arxiv.org/ html/2406.19470v2. [54] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tianyi Tang, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 technical report, 2025. URL https://arxiv.org/abs/2412.15115. [55] Guangxiang Zhao, Jingjing Xu, Qi Zeng, Xuancheng Ren, and Xu Sun. Review-driven multilabel music style classification by exploiting style correlations. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 20592068, 2019. URL https://aclanthology.org/N19-1296/. [56] Chujie Zheng, Hao Zhou, Fandong Meng, Jie Zhou, and Minlie Huang. Large language models are not robust multiple choice selectors. 2024. URL https://arxiv.org/abs/2309.03882. [57] Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi Luan, Denny Zhou, and Le Hou. Instruction-following evaluation for large language models, 2023. URL https://arxiv.org/abs/2311.07911. [58] Jie Zhu, Braja Patra, and Ashraf Yaseen. Recommender system of scholarly papers using public datasets. AMIA summits on translational science proceedings, 2021:672, 2021."
        },
        {
            "title": "A Dataset Description",
            "content": "In this section, we describe the original datasets and their characteristics in detail. Reading Comprehension is dataset of short paragraphs and multi-sentence questions that can be answered from the content of the paragraph. Some questions contain multiple correct answers. The dataset we use is from (https://cogcomp.seas.upenn.edu/multirc/). The metadata is licensed under the Research and Academic Use License. We chose this dataset for the following 3 reasons. 1. The number of correct answer-options for each question is not pre-specified. This removes the over-reliance of current approaches on answer-options and forces them to decide on the correctness of each candidate answer independently of others. In other words, unlike previous work, the task here is not to simply identify the best answer-option, but to evaluate the correctness of each answer-option individually. 2. The correct answer(s) is not required to be span in the text. 3. The paragraphs in our dataset have diverse provenance by being extracted from 7 different domains such as news, fiction, historical text etc., and hence are expected to be more diverse in their contents as compared to single-domain datasets. The goal of this dataset is to encourage the research community to explore approaches that can do more than sophisticated lexical-level matching. Toxicity is adapted from RealToxicPrompts. The dataset select prompts from sentences in the OPENWEBTEXT CORPUS (Gokaslan and Cohen, 2019), large corpus of English web text scraped from outbound URLs from Reddit, for which we extract TOXICITY scores with the PERSPECTIVE API. To obtain stratified range of prompt toxicity, we sample 25K sentences from four equal-width toxicity ranges ([0,.25), ..., [.75,1]), for total of 100K sentences. We then split sentences in half, yielding prompt and continuation, both of which we also score for toxicity. For each data point, we provide the definition for each category as well as shuffle the choices for each category. We only classify the case when the categorys sum of prompt and continuation score is above 1.5 for each label. The dataset we use is from (https://huggingface.co/datasets/allenai/real-toxicity-prompts). The metadata is licensed under the Apache License. News is processed from Reuters text categorization test collection dataset. It contains collection of documents that appeared on Reuters newswire. There are originally 120 related topics, where each document can be related to multiple topics. There are two challenges related to this dataset preparation: 1. The number of topics can be too large for small number of selections. 2. Some popular topics are commonly included in the documents, making certain choice much more popular than other choices, which can bias the models in our study. With this in mind, we limit our selection to 10 options from the 120 topics for each documents, and the remaining choices are selected randomly from the topic pool; we also re-label the choices using unique mapping per document to keep the final answers evenly distributed between all letter choices (e.g. A/B/C/D...). The dataset we use is from (https://archive.ics.uci.edu/dataset/137/reuters+21578+text+categorization+collection). This dataset is licensed under Creative Commons Attribution 4.0 International (CC BY 4.0) license. Biomedicine is adapted from the PubMed MultiLabel Text Classification Dataset, which is collection of research articles from the PubMed repository. Originally, these documents are manually annotated by Biomedical Experts with their Medical Subject Headings (MeSH) labels, and each article are described in terms of 10-15 MeSH labels. The adopted dataset has been processed and mapped to its root level with 15 distinct MeSH labels in total. The dataset we use is from (https://www.kaggle.com/datasets/owaiskhan9654/pubmed-multilabel-text-classification). This dataset is licensed under CC0: Public Domain license. Laws is adapted from EURLEX57K which contains 57k legislative documents in English from EUR-Lex (https://eur-lex.europa.eu) with an average length of 727 words. All the documents of the dataset have been annotated by the Publications Office of EU (https://publications.europa.eu/en) with multiple concepts from EUROVOC (http://eurovoc.europa.eu/). EURLEX contains 7201 concepts. There are two challenges when converting this dataset to multi-choice question answering dataset: 1. The 7201 concepts is too big pool for small number of selection, most documents have <10 concepts in this dataset. 2. Some popular concepts are included in number of documents, making certain choice much more frequent than other choices. This is problematic because it may force the 15 model to learn the popular letter of choice rather than the content of the questions. With this in mind, we limit our selection to 15 options from the 7201 topics pool for each document, and the remaining choices are selected randomly from the topic pool; we also shuffle and and re-label the choices using unique mapping per document to keep the final answers evenly distributed between each letter choice. The dataset we use is from (https://paperswithcode.com/dataset/eurlex57k). This dataset is licensed under Apache License. Events is adapted from the events classification biotech\" dataset, which contains diverse biotech news articles consisting of various events. The curated dataset has 3140 questions with 5 choices of events for each document. Six choices are provided for each question. The dataset we use is from (https://paperswithcode.com/dataset/events-classification-biotech). This dataset is licensed under the Open Data Commons Attribution License (ODC-By) v1."
        },
        {
            "title": "B Dataset Filtering",
            "content": "The Biomedicine, Law, and Events datasets were originally multi-label classification tasks, which we adapted into SATA questions by creating distractor (incorrect) choices from the unselected labels. There are two challenges when converting these datasets to SATA format: 1. Many of them have large label pool with only few correct answers, which is not reasonable for multiple-choice questions. 2. There can be some popular answers frequently exist in the original data, making certain choices more frequent than others. This is problematic because it may force the model to learn the popular token of choice (e.g. Choice A/a/1) rather than the content of the questions. For example, the law dataset is originally from EUR-Lex data [18] contains 57k legislative documents in English (https://eurlex.europa.eu) annotated by the Publications Office of EU (https://publications.europa.eu/en) with over 7k concepts from EUROVOC (http://eurovoc.europa.eu/). To address the first challenge, we kept an option-to-answer ratio between 2 and 3, considering the balance between the number of correct answers and incorrect choices. The distractors were sampled randomly from the topic pool. We also shuffled and and re-labeled the choices using unique mapping per question to keep the final answers evenly distributed between each choice token. An example question from each data source is shown in Figure 4. Figure 4: Representative examples of questions from various data sources used to construct SATABENCH. B.1 Initial Filtering We manually filtered out questions that contain vague quantities, degrees of likelihood, temporal ambiguity, qualitative subjectivity, comparative uncertainty, general and undefined references. We use AWS Comprehend to remove questions that contain personal financial information or contact 16 Figure 5: Confusion score distribution across all questions before filtering. d1: Reading Comprehension, d2: Toxicity, d3: News, d4: Biomedicine, d5: Laws, and d6: Events. Table 5: Original data source statistics. We report the following metrics n: number of instances, q: number of possible labels across the entire dataset, s: proportion of single-answer questions, m: mean number of correct answers, me: median number of correct answers, min: minimum number of correct answers, max: maximum number of correct answers, LC: label cardinality, r: ratio of the number of choices to the median number of correct answers (LC / me)."
        },
        {
            "title": "Data Source\nReading Comprehension\nToxicity\nNews\nBiomedicine\nLaws\nEvents",
            "content": "n 5131 5994 11360 50000 57000 3140 na 8 120 15 7201 29 27% 2.344 60% 2.639 83% 2.567 0.07% 5.745 0.54% 5.069 50.7% 2.683 me min max 10 0 2 7 2 2 16 2 2 13 0 6 26 1 5 5 2 LC 2-21 8 6 15 15 6 na 4 3 2.5 3 3 information. We leave questions that contain public available information such as the company name and address. All filtered words are mentioned below in Table 6. 17 Figure 6: Confusion Score distribution of the filtered questions. d1: Reading Comprehension, d2: Toxicity, d3: News, d4: Biomedicine, d5: Laws, and d6: Events. Figure 7: Confusion Score distribution separately visualized for each source dataset. (Left to right) Top row: Reading Comprehension, Toxicity, News; Bottom row: Biomedicine, Laws, Events. 18 Table 6: Identified categories of vague terms along with representative examples Category Vague Quantities Degrees of Likelihood Temporal Ambiguity Qualitative Subjectivity Comparative Uncertainty General and Undefined References B.2 Human Validation Examples some, several, many, few, lot, plenty, numerous, various, partially, handful, bit, portion maybe, possibly, probably, likely, unlikely, apparently, presumably, seemingly, conceivably, arguably, occasionally sometimes, often, rarely, occasionally, once in while, from time to time, now and then, every so often bad, nice, significant, substantial, important, interesting, sufficient, adequate, reasonable, moderate more or less, about, around, roughly, close to, kind of, sort of, nearly, almost, approximately thing, things, anything, everything, whatever, such, kind, type, sort Human validation is to ensure that the questions are unambiguous. Using humans to validate the question is inspired by [49, 33]. For each question in the benchmark, we ask five annotators whether the question contains ambiguous information. Human Validation You are presented with the following: Paragraph: paragraph Question: question Choices: choice The question text and answer choices are clearly written: Strongly agree Agree Neither agree nor disagree Disagree Strongly Disagree Answers: Once it is done, the total cost is tracked (1301.89), with 5 people per label at cost of 0.012 each. We only select questions that are Strongly agree and Agree\" > 0.8. B.3 Human Labeling To ensure that each question has valid and correct answer, we conducted comprehensive human evaluation. An initial manual inspection revealed that some questions lacked clearly correct answers. To verify answer correctness, we recruited three experienced annotators to review all 1,650 questions that remained after prior filtering and validation. Annotators were compensated at rate of at least $35 per hour. Each question was independently evaluated by at least two annotators. For each question, the original reference answer and four anonymized LLM-generated answers (from Claude 3.7, GPT-4 Turbo (O3), Grok 3, and Gemini 2.5) were provided. In cases where the two annotators disagreed, third annotator reviewed the original answer, all LLM answers, and both annotators decisions to determine the final label or to discard the question. Detailed annotation guidelines were provided below. As result of this process, 47 questions were discarded due to ambiguity or disagreement, and an additional 46 were removed for quality-related issues."
        },
        {
            "title": "Human Labeling",
            "content": "Given original answers and LLMs answers, youll try to identify correct answer of the following questions. Youre expected/encouraged to use Google, and any internet resources you can find to try and answer the question correctly. Requirements and Expectations 1. You are encouraged to use Google, and any websites you can think of or find that may help you answer the question and understand the concept. However, you are NOT allowed to use AI assistants like chatGPT, Claude, Grok3 Geimini, etc., or ask people for help. All their answers to the question has been provided anoymously under LLM Answers. 2. We ask that you spend at least 5 minutes trying to answer each question before making your selection. If you havent settled on an answer choice in that time, we encourage you to spend as long as you need to be confident in your selection. 3. These questions will be hard, and you will likely need to spend while on each of them to make progress understanding the context. Read relevant resources, take plenty of time, and answer \"I dont know\" if youre pretty sure you have no realistic way of answering confidently. 4. You will also be given the opportunity to give feedback on the question. Were especially interested in feedback about whether the question was ambiguous, but please feel free to give feedback of any other form! Suggestions and Strategies for Labeling 1. Look up definitions for all of the unfamiliar terms in the question and answer choices. Keep list of those definitions handy so you can easily refer back to the definitions if you forget the jargon. 2. LLMs answer is not always reliable and original answer is not always correct. Please try to solve the question independently before looking at potential answers. 2. Look for primary resources, like research papers and textbooks, as these can often contain clearer explanations than sources like Wikipedia (although Wikipedia can be useful in many cases as well). You are presented with the following: Paragraph: paragraph Question: question Choices: choice Original Answers: original answer LLM Answers: llm answers Answers:"
        },
        {
            "title": "C Hyperparameters",
            "content": "To ensure consistent and high-quality outputs across different models, we standardized the decoding hyperparameters for most model generations by setting the temperature to 0 (to promote deterministic outputs), top-p (nucleus sampling) to 0.95 (to allow for balance between diversity and relevance), and maximum token limit of 1,024 tokens. Recognizing the enhanced reasoning capabilities of certain models, we adjusted the configurations accordingly. For O3 and Grok 3, we set the thinking budget to be high. For Geimini 2.5 thinking and Claude 3.7 Thinking, we set the thinking budget to be 16k. For R1, we set max tokens 16k. This is to provide enough budget for reasoning models to finish thinking."
        },
        {
            "title": "D Compute Resources",
            "content": "We use AWS Bedrock batch inference for large models inference such as Claude3 Sonnet, Claude 3.5 Haiku, Claude 3 Haiku, Claude 3 Opus, Claude 3.5 Sonnet, Llama 3.1 405B, Mistral Large V2, Llama 3.2 90B, and Llama 3.1 70B. We use AWS cross-region inference for Claude3.7 Reason, Claude3.7, and Deepseek R1. We use official APIs from the respective providers for models such as OpenAI O3, GPT4.1, Grok3 Reason, GPT4, Geimini2.5 Reason, Gemini 2 Flash, GPT 4.1 mini, and Qwen Plus. For experiments that require accessing models hidden states and log probs. We run inference on one EC2 p4d.24xlarge (Nvidia A100 40GiB GPU) instance and one EC2 g5.4xlarge (Nvidia A10G 20 Table 7: Model cards summarizing specifications and details for all evaluated large language models. Model Name Complete Model ID Release Hosting Creator OpenAI OpenAI xAI OpenAI O3 GPT-4.1 Grok 3 Think GPT-4-turbo Claude-3.7 Sonnet Think Anthropic Anthropic Claude-3.7 Sonnet Anthropic Claude-3 Sonnet Google Gemini 2.5 Think Anthropic Claude-3.5 Haiku Anthropic Claude-3 Haiku Anthropic Claude-3 Opus Google Gemini 2 Flash OpenAI GPT-4.1 mini Anthropic Claude-3.5 Sonnet Meta Llama 3.1 405B DeepSeek DeepSeek R1 Mistral AI Mistral Large V2 Alibaba Qwen Plus Meta Llama 3.2 90B Meta Llama 3.1 70B Mistral AI Mistral 8B Instruct Meta Llama 3 8B BigScience BLOOMZ 7B DeepSeek DeepSeek R1 Distill 8B Alibaba Qwen 2.5 14B Microsoft Phi-3 7B Microsoft Phi-4-mini-reasoning o3-2025-04-16 gpt-4.1-2025-04-14 grok-3-mini-beta gpt-4o-2024-11-20 anthropic.claude-3-7-sonnet-thinking-20250219-v1:0 anthropic.claude-3-7-sonnet-20250219-v1:0 anthropic.claude-3-sonnet-20240229-v1:0 gemini-2.5-pro-preview-03-25 anthropic.claude-3-5-haiku-20241022-v1:0 anthropic.claude-3-haiku-20240307-v1:0 anthropic.claude-3-opus-20240229-v1:0 gemini-2.0-flash gpt-4.1-mini-2025-04-14 anthropic.claude-3-5-sonnet-20240620-v1:0 meta.llama3-1-405b-instruct-v1:0 deepseek.r1-v1:0 mistral.mistral-large-2407-v1:0 qwen-plus-2025-04-28 meta.llama3-2-90b-instruct-v1:0 meta.llama3-1-70b-instruct-v1:0 mistralai/Mistral-8B-Instruct-2410 meta-llama/Llama-3.1-8B-Instruct bigscience/bloomz-7b1 deepseek-ai/DeepSeek-R1-Distill-Llama-8B Qwen/Qwen2.5-14B microsoft/phi-3-small-128k-instruct microsoft/phi-4-mini-reasoning xAI API 04/16/25 OpenAI API 04/14/25 OpenAI API 02/19/25 11/20/24 OpenAI API 02/24/25 AWS Bedrock 02/24/25 AWS Bedrock 02/29/24 AWS Bedrock 03/25/25 Vertex AI 10/22/24 AWS Bedrock 03/07/24 AWS Bedrock 02/29/24 AWS Bedrock 02/05/25 Vertex AI 04/14/25 OpenAI API 06/20/24 AWS Bedrock 07/23/24 AWS Bedrock 01/20/25 AWS Bedrock 07/24/24 AWS Bedrock 04/28/25 Alibaba API 09/25/24 AWS Bedrock 07/23/24 AWS Bedrock 10/09/24 Hugging Face 07/23/24 Hugging Face 07/11/22 Hugging Face 02/01/25 Hugging Face 09/19/24 Hugging Face 05/21/24 Hugging Face 04/15/25 Hugging Face 24GiB GPU) in Sydney(ap-southeast-2) region. We have also attached 8000GiB disk volume with AL2023 Linux OS image. We use HuggingFace and PyTorch as the main software frameworks. Non-expert Human Benchmark To contextualise LLM results on SATA-BENCH, we recruited non-expert annotators on Amazon Mechanical Turk, adapting the instructions from [43]. All 1604 questions was labelled as follows: Task set-up. Each question was presented with the original answer options plus decoys (e.g. ABCD ABCDEFGHIJK) to identify inattentive workers. Nine independent annotations were collected per item at rate of $0.84 per question, matching the fair-wage recommendations of GPQA. Quality safeguards. Workers were: (i) informed that every item contains at least two correct answers; (ii) forbidden from consulting LLMs or other people, yet allowed to look up unfamiliar terms on Google/Wikipedia; (iii) required to spend 2 minutes on each question. Submissions that selected any decoy, took < 1 min, or violated the lookup policy were discarded (7.1 %). Label selection. From the surviving pool, we randomly drew one annotation as the human label; single-choice answers were retained to keep the evaluation comparable to LLMs that sometimes return only one option. EM Precision Recall JI RStd RSD SPD CtDif CtAcc CtDifAbs Human 17. 15.3 Table 8: Aggregate performance of crowd annotators on the SATA-Bench subset. 1.46 0.6 45.0 0.46 34.1 60. 54.4 1.44 As anticipated, non-experts achieve modest exact-match and precision, yet their selection-bias metrics (RStd, RSD, SPD) resemble those of mid-tier LLMs. Crucially, they exhibit smaller absolute count bias (CtDif) and higher correct-count accuracy (CTACC), indicating superior intuition for the number of correct options even when individual labels are missed. These human baselines therefore offer realistic point of comparison for evaluating LLM performance on specialised SATA tasks. 21 E.1 Non-expert Human Benchmark Instructions We have provided details on human benchmark instructions."
        },
        {
            "title": "Human Benchmark Instructions",
            "content": "You will see short Paragraph, Question, and list of answer options labelled D H L O. Your task is to mark all choices that you believe are correct. Requirements and Expectations 1. External resources. You may consult Google, Wikipedia, journals, textbooks, or any other online materials that help you understand the content. Do not use AI assistants (ChatGPT, Claude, Gemini, Grok, etc.) and do not ask other people. 2. Effort. Spend at least 2 minutes on each item before submitting. If you still feel unsure, keep researching until you are confident, or choose dont know if you cannot answer reliably. 3. Difficulty. Many items are specialised and may require careful reading. Take your time; thorough work is valued more than speed. 4. Feedback. After answering, you may leave comments (e.g. ambiguity, unclear wording). Constructive feedback is highly appreciated. Suggestions and Strategies 1. Look up definitions of every unfamiliar term in the paragraph, question, and answer options. Keep your notes open for quick reference. 2. Approach the question independentlydo not try to guess majority answer. Rely on primary sources (research articles, textbooks) whenever possible. 3. Remember that there are at least two correct letters, but possibly more. Select every option you deem correct. Fields Presented to You Paragraph: {{paragraph}} Question: {{question}} Choices: {{A. . . O}} Your Answers (mark all that apply): Optional Feedback:"
        },
        {
            "title": "F Metrics Definition",
            "content": "Performance. The standard SATA performance metrics [48] are Exact Match (EM), Jaccard Index (JI), Mean Average Precision (Precision), and Mean Average Recall (Recall). EM captures models ability to select all correct answers without error, indicating its completeness in prediction. JI measures the fraction of predicted labels that match the ground-truth labels. Selection Bias. Selection bias is models preference for selecting specific option IDs as answers, and is measured by RStd [56] and RSD [11, 42]. We also observe that some models prefer to avoid selecting certain options, which we call unselection bias. We introduce metric called Selection Probability Divergence (SPD) to measure this type of bias. Appendix gives more details on this issue and the design of SPD. Count Bias. We observe that models tend to select fewer number of options compared to the ground truth. We refer to this as Count Bias. To evaluate the severity of this type of bias, we measure the following: (i) Mean difference between the number of selected options minus correct options (CtDif), (ii) Mean absolute difference between the number of selected options minus correct options 22 (CtDifAbs), and (iii) Percentage of cases where the model selects exactly the correct number of options (CtAcc). F.1 Performance Metrics Definition Here are some standard metrics used in the literature to track performance on SATA questions. Exact Match counts how many times the entire set of predicted labels for sample exactly matches the entire set of ground truth labels. It is then divided by the total number of samples. perfect exact match score (1.0) means the model got every instances labels exactly correct. Jaccard Index calculates the fraction of predicted labels that exactly match the ground truth labelsor put differently, divide the size of the intersection of predicted and true labels by the size of the union of predicted and true labels, and then average this ratio across all instances for the final score. This metric treats each label decision independently and is good measure when we care about partial correctness in multi-label settings. Recall looks at how many labels were correctly predicted (intersection) out of how many total true labels exist. Then it averages this fraction across all instances. Precision calculates how many labels were correctly predicted (intersection) out of all the labels the model predicted. Then it averages this fraction across all instances. F.2 Selection Bias Metrics Definition Here are some standard metrics to track SATA questions selection bias. These metrics are extension of existing selection bias literature. Standard Deviation of Recalls (RStd) is the standard deviation of the class-wise recall: RStd = (cid:118) (cid:117) (cid:117) (cid:116) 1 (cid:88) i=1 (ri r)2, (1) where is the number of choices, ri is the recall of the i-th class, and is the arithmetic mean of ri values. Note that our recalls are calculated at the label level since this is multi-class question [56] Relative Standard Deviation (RSD) is the class-wise accuracy standard deviation normalized by the overall accuracy: RSD = (cid:113) 1 (cid:80)k i=1(si s)2 , (2) where is the number of choices, si is the accuracy of the i-th class, and is the mean accuracy averaged across classes. Please note that our recalls are calculated at the label level since this is multi-class questions [11, 42] F.3 Count Bias Metrics Definition CtDif calculates the average difference in count between predicted and actual selected options. positive value indicates that the predictions tend to select more options than the actual answers, while negative value suggests the opposite. CtDifAbs calculates the absolute value of the average difference in count between predicted and actual selected options. larger value indicates that the predictions tend to select the number of options that are different from the correct number of options. CtAcc calculates the proportion of predictions that select the exact same options as the ground truth labels. It provides measure of how often the model selects the same number of answers as the true answer set. 23 F.4 Additional Metrics Definition InfCost measures the number of model forward passes used for method to complete the benchmark. larger value indicates that the method requires more compute FLOPs and is thus more expensive. small value indicates the method requires fewer compute FLOPs and is thus more cost-effective."
        },
        {
            "title": "G Unselection bias metric",
            "content": "We view SATA problem as multiple binary selection problems, where each option is examined independently to be selected or passed. In our experiments, we have observed that LLMs tend not to select (i.e., skip) certain labels more frequently than others. To quantify this non-selection bias, we define metric below, named selection probability divergence (SPD), to measure the misalignment between the ground truth and the LLMs prediction. SPD = (cid:88) i=1 (cid:18) 1 (cid:19) ln qi pi pi qi , (3) where is the number of choices, pi is the ground truth probability of label being one of the correct choices, and qi is the prediction probability of label being one of the selected choices. SPD has minimal value of 0 at qi = pi for all i, when the prediction aligns with the ground truth. SPD diverges as qi 0 while pi is finite for any i, when the LLM shows non-selection bias against particular label. SPD also diverges as pi 0 while qi is finite for any i, when the LLM shows selection bias toward particular label. In this sense, SPD serves as metric to measure the disagreement of choice probability between the ground truth and the prediction, reflecting both under-selection and over-selection. (See Appendix G.2 for the mathematical analysis.) G.1 Behavior of SPD Metric We conduct numerical experiment to compute SPD with varying pi and qi. We set the number of choices to 4, and use Boolean list of size 4 to indicate which options are correct. Eg. for choices A, B, C, and D, the list [True, False, True, True] means the answer to the SATA question is ACD. For the ground truth list, we sample each element of the Boolean list with ground truth probability, p. For the prediction list, we sample the first element of the Boolean list with prediction probability, q, and sample the other elements with probability p. With this setting, we focus on the misalignment between the ground truth and the prediction in single label (the first label in this case). We repeat the above sampling process times, and compute the True rate of each option for the ground truth pi and the prediction qi, with = 1, 2, 3, 4. We then substitute the numbers into Eq. (3) to calculate SPD. Note that in the current setting, pi = p, i, and q1 = q, q2,3,4 = p. Figure 8 shows the SPD-q curves under different values of the ground truth probability p. Each curve is obtained by averaging over 100 replicates, and the shaded area shows the standard deviation. The minimal value of SPD is 0 and occurs at = p. G.2 Sensitivity of SPD to label probability ratio We analyze the behavior of SPD as the relationship between pi and qi changes. We first define the ratio of the two probabilities as ri qi/pi, = 1, 2, . . . , k, and rewrite the SPD definition Eq. (3) as SPD = (cid:88) i= (1 ri) ln 1 ri . (4) As the misalignment between the ground truth and the prediction grows, either with ri 0 or ri +, SPD diverges according to Eq. (4). Therefore, large value of SPD reflects the disagreement of the choice probability between the ground truth and the prediction. Figure 8: Relationship between Selection Probability Divergence (SPD) and prediction probability (q) across different ground truth probabilities (p). The curves are averaged over 100 replicates, and the shaded area represents the standard deviation. In each plot, the minimal value of SPD is 0 at = p, when the prediction aligns with the ground truth. To find the minimum of SPD, we take the partial derivative with respect to each variable ri, and set it to be 0. Then we have the equations below. SPD ri = ln ri + ri 1 ri = 0, = 1, 2, . . . , k. This set of equations has only one real solution: ri = 1, = 0, 1, . . . , k. (5) (6) Thus the SPD is minimized when qi = pi, i.e. when the prediction probability matches the ground truth probability for each option and when there is no bias toward or against any choice. The minimal value of SPD is 0."
        },
        {
            "title": "H Prompts used in experimentation",
            "content": "H.1 Prompts for open-source models We designed simple, basic prompts without elaborate prompt engineering for all experiments with open-source models in Section 3. The main reason is that we want to avoid potential biases introduced by complex prompt engineering, thereby emphasizing the evaluation of the method itself. H.1.1 Choice funnel Prompt This prompt is used for Choice Funnel as well as two baseline methods: first token and first token debiasing"
        },
        {
            "title": "Open Source Prompts",
            "content": "You are presented with the following: Paragraph: paragraph Question: question Choices: Option Option Option Option Option Task: Identify and select all the correct answers based on the paragraph and the question. Answers: H.1.2 Yes/No for open-sourced models This prompt is used for yes/no baseline method to compare against Choice Funnel. Yes/No Prompts You are presented with the following: Paragraph: paragraph Question: question Statement: Option C E Task: Determine if the statement answers the question correctly and reply with \"Yes\" or \"No\" only. Answer: H.2 Prompts for proprietry model H.2.1 Problems for current MCQ prompts Existing benchmarks [25] use the following prompts for MCQ questions and then use exact match to get the correct option. Example MCQ Prompt 1 What is the correct answer to this question: question Choices: choices. Lets think step by step: Based on the above, what is the single, most likely answer choice? Answer in the format: correct answer is (insert answer here). Example MCQ Prompt 2 Given the following question and four candidate answers (A, B, C, and D), choose the best answer. Question: question Your response should end with: The best answer is [the_answer_letter] where the [the_answer_letter] is one of A, B, C, or D. However, it does not work well when there are multiple answers as LLMs are likely to repeat the content of the option before output the second choice. As an example, the below answer is A. Wheat, C. Grain instead of AC, which makes it hard to parse. 26 Example MCQ Prompt Example 1 What is the correct answer to this question: Bangladesh passed on its weekend tender for 100,000 tones of optional origin soft wheat, private export sources said. What topics are related to the document above? Choices: A. Wheat, B. Barley, C. Grain, D. Retail, E. Money-supply, F. Sugar]. Lets think step by step: Based on the above, what are all most likely answer choice? Answer in the format: correct answer is (insert answer here). Answer (from GPT4o): Lets analyze step by step: The document mentions wheat: The phrase \"100,000 tonnes of optional origin soft wheat\" directly refers to wheat. Relevant choice: A. wheat. The document does not mention barley: There is no reference to barley anywhere in the text. Not relevant choice: B. barley. Grain is broader category that includes wheat: Since wheat is type of grain, it makes sense to include grain as related topic. Relevant choice: C. grain. The document does not mention retail: The context of the tender does not relate to retail or consumer sales. Not relevant choice: D. retail. The document does not mention money supply: There is no financial or monetary context provided. Not relevant choice: E. money-supply. The document does not mention sugar: There is no reference to sugar in the text. Not relevant choice: F. sugar. Final Answer: Correct answer is A. Wheat, C. Grain. Example MCQ Prompt Example 2 What is the correct answer to this question: Bangladesh passed on its weekend tender for 100,000 tones of optional origin soft wheat, private export sources said. What topics are related to the document above? Choices: A. Wheat, B. Barley, C. Grain, D. Retail, E. Moneysupply, F. Sugar. Lets think step by step: Based on the above, what are most likely answer choices? Answer in the format: correct answer is (insert answer here). Answer (from GPT4o): Lets analyze the question step by step: Key Subject of the Document: The document discusses Bangladesh passing on its tender for 100,000 tonnes of optional origin soft wheat. Relevant Topics: The keyword wheat clearly relates to the document. Wheat falls under the broader category of grain, making it relevant choice. Other terms like barley, retail, money-supply, and sugar are not directly related to the subject of the document. Most Likely Answer Choices: A. Wheat: Directly mentioned in the document. C. Grain: Relevant as wheat is type of grain. Correct Answer: The correct answer is Wheat, Grain. We can observe that parsing the above prompts is much harder because we do not know when they will output the option ID plus answers, and we cannot blindly extract all matched option IDs. Following [25] page 38, we used the JSON format to extract answers. Closed-source LLMs have shown good instruction following capability for JSON format output [57]. Also, python has many packages that can do fuzzy JSON matching, which can be used to handle edge cases. We extract as many JSON files as possible from each output and combine the extracted answers together as the final answer to the question."
        },
        {
            "title": "SATA Prompt",
            "content": "Given the following question where there is more than one correct answer, choose all correct answers. Question: question Choices: choices Please select all choices that apply. You must focus on the question and select all choices that apply. Lets think step by step: You must present your selected option IDs in the following JSON format: {choices :< ABCDEF GHIJKLM O >}"
        },
        {
            "title": "SATA Prompt Example",
            "content": "Given the following question where there is more than one correct answer, choose all correct answers. Question: What is the correct answer to this question: Bangladesh passed on its weekend tender for 100,000 tones of optional origin soft wheat, private export sources said. What topics are related to the document above?Choices: A. Wheat, B. Barley, C. Grain, D. Retail, E. Money-supply, F. Sugar. Please select all choices that apply. You must focus on the question and select all choices that apply. Lets think step by step: You must present your selected option IDs in the following JSON format: {choices :< ABCDEF GHIJKLM O >} Answer (from GPT4o): Lets analyze the question step by step: Understanding the question context: The document discusses Bangladeshs weekend tender for 100,000 tonnes of \"optional origin soft wheat.\" Analyzing the options provided: A. Wheat: The document specifically mentions \"soft wheat.\" This choice is correct. B. Barley: There is no mention of barley in the document. This choice is incorrect. C. Grain: Wheat is type of grain. This choice is correct as the broader category includes wheat. D. Retail: The document does not mention retail activities. This choice is incorrect. E. Money-supply: There is no discussion of money supply or related economic terms in the document. This choice is incorrect. F. Sugar: The document does not mention sugar. This choice is incorrect. Conclusion: Based on the analysis, the related topics are Wheat (A) and Grain (C). Final Answer in JSON format: { \"choices\": [\"A\", \"C\"] } One can observe that our proposed prompts can easily extract the answer because they contain only the option IDs. H.3 Ablation Prompts H.3.1 Few Shot prompt We report few few-shot prompt where the number of examples is equal to 5."
        },
        {
            "title": "Few Shots Prompt",
            "content": "Given the following question and four candidate answers (A, B, C, and D), choose the best answer. Question 1: question 1 Option 1: option 1 Answer 1:correct option json 1 Question 2: question 2 Option 2: option 2 Answer 2: corect option json2 ... Question 5: question 5 Option 5: option 5 Answer 5:correct option json 5 Question: question Option: option Please select all choices that apply. You must focus on the question and select all choices that apply. Lets think step by step: You must present your selected option IDs in the following JSON format: {choices :< ABCDEF GHIJKLM O >} H.4 Think Option by Option prompt Inspired by [46, 39], we instruct LLM to understand each options and analyze each answer independently. Choice-by-choice Prompt Given the following question and four candidate answers (A, B, C, and D), choose the best answer. Question: question Option: option Lets think through this step by step: 1. First, lets understand what the question is asking... 2. Now, lets evaluate each option individually... 3. Therefore, the correct answers are... You must present your selected option IDs in the following JSON format: {choices :< ABCDEF GHIJKLM O >} H.4.1 Few Shot Option prompt We further provide few examples to teach LLMs how to think option by option, but it still does not improve the performance."
        },
        {
            "title": "Few Shots Option Prompt",
            "content": "Given the following question and four candidate answers (A, B, C, and D), choose the best answer. Question 1: question 1 Option 1: option 1 Choice by choice reasoning 1: reason 1 Answer 1:correct option json 1 Question 2: question 2 Option 2: option 2 Choice by choice reasoning 2: reason 2 Answer 2: corect option json2 ... Question 5: question 5 Option 5: option 5 Choice by choice reasoning 5: reason 5 Answer 5:correct option json 5 Question: question Option: option Lets think through this step by step: 1. First, lets understand what the question is asking... 2. Now, lets evaluate each option individually... 3. Therefore, the correct answers are... You must present your selected option IDs in the following JSON format: {choices :< ABCDEF GHIJKLM O >} H.4.2 Prompt with Average Options Count SATA Prompt Given the following question where there is more than one correct answer, choose all correct answers. Question: question Choices: choices Please select all choices that apply. You must focus on the question and select all choices that apply. The number of average selected options is 3.63. Lets think step by step: You must present your selected option IDs in the following JSON format: {choices :< ABCDEF GHIJKLM O >} H.4.3 Prompt with Correct Number of Options SATA Prompt Given the following question where there is more than one correct answer, choose all correct answers. Question: question Choices: choices Please select all choices that apply. You must focus on the question and select all choices that apply. The number of average selected options is XX. Lets think step by step: You must present your selected option IDs in the following JSON format: {choices :< ABCDEF GHIJKLM O >} H.4.4 Single Choice Prompt To ensure consistency, we use similar prompt for single choice. We use the same method to retrieve the correct choices. If there is more than one correct choice, we randomly sample from among them."
        },
        {
            "title": "Single Choice Prompt",
            "content": "Given the following question where there is only one correct answers, choose the correct answer. Question: question Choices: choices Please the correct choice that apply. Lets think step by step: You must present your selected option IDs in the following JSON format: {choice :< ABCDEF GHIJKLM O >} H.5 Prompt with Numeric Option For numeric options, it is hard to retrieve since the number of options can be above 10, and the previous retrieving method could retrieve 12 as 1 and 2. We instruct LLMs to produce correct answers in ascending order. We start by retrieving larger number that is above 10. For each successful retrieval, remove that number from the output. This way, we can avoid the above scenario. Numeric Prompt Given the following question where there is more than one correct answer, choose all correct answers. Question: question Choices: choices Please select all choices that apply. You must focus on the question and select all choices that apply. You must present your answers in ascending orders. Lets think step by step: You must present your selected option IDs in the following JSON format: {choices :< 123456789101112131415 >} H.6 Prompt with small alphabet Option Small Alphabet Prompt Given the following question where there is more than one correct answer, choose all correct answers. Question: question Choices: choices Please select all choices that apply. You must focus on the question and select all choices that apply. Lets think step by step: You must present your selected option IDs in the following JSON format: {choices :< abcdef ghijklmno >} I"
        },
        {
            "title": "Inference Error Handling",
            "content": "For 2.897% of all cases, we cannot find any match in JSON format, so we use Claude 3 Haiku to extract the final labels. To be specific, we adopt the following system prompt:"
        },
        {
            "title": "Edge Case Handling Prompt",
            "content": "Given the following text, please identify all valid choices. valid choice is any single letter from to Q, which might appear right after colon (e.g., choices: \"B\"). - If one or more valid choices are found, concatenate them and return them in the format <answer></answer>: For example, <answer>BEM</answer> - If no valid choices are found, return <answer></answer>. String to analyze: <output> Please provide your answer only in the form below: <answer> For all cases below, our Claude 3 haiku is able to accurately produce the correct outcome. Table 9: Comparison of raw LLM outputs and the extracted labeled results obtained using Claude 3 Haiku. LLM Output cant fulfill that request. \"choices\": { \"choice\": \"B\" } {{ \"choice\": <B O> }} }}json { \"choice\": [ ] } Claude 3 Haiku Extraction NaN BEHJLMO NaN We then use Amazon Groundtruth labeling to check whether Claude 3 Haiku correctly parses the answer. Of those, only 47 cases were labeled as No or Yes with confidence lower than 0.6. We manually investigated those 47 cases and found that only four were actually incorrect. Table 10: Examples of LLM outputs and corresponding extraction results where Claude 3 Haiku produced incorrect extractions. LLM Output Lets analyze the text and MeSH categories step by step:...: your selected option IDs - (Organisms), your selected option IDs - (Phenomena and Processes), your selected option IDs - (Chemicals and Drugs) {{ \"choice\": <D M> }} }}json { \"choice\": \"choice\": \"N\"oneyour selected option IDs } Lets analyze the document step by step: ... your selected option IDs your selected option IDs. Based on this analysis, the applicable choices are A, B, C, and E. Claude 3 Haiku Extraction CE Human Corrected Answers CEG DELM ABC DEKLM NaN ABCE"
        },
        {
            "title": "J More Details on Key Observations",
            "content": "Unselection Bias. FP/FN means False Positive Count divided by False Negative Count. If model has 100 False Negative cases of A, it means that the model has not predicted in 100 cases where it should have predicted A. If model has 20 False Positive cases of A, it means that the model has predicted in 20 cases where it should not have. The low FP/FN rate means that out of all cases, the model tends not to predict instead of overpredicting A. Due to Count Bias, most of the models have FP/FN rate below 1. However, almost all models has one label with an extremely low FP/FN 32 rate. For example, Claude3-Haiku has label FP/FN rate equal to 0.27 while its second worst is 0.48 as shown in Figure 10. Recall Difference is another metric to demonstrate unselection bias. Low recall on certain label means that LLMs incapability of predicting certain labels correctly. As shown in Figure 9, there are many models whose worst label is more than 5% below their average performance. Count Bias. Figure 11 shows that nearly all models select too few responses and that this tendency increases as the number of correct answers increases. Figure 12 shows that EM also decreases as the number of correct answers increases. This shows that LLMs tend to underpredict the number of correct choices."
        },
        {
            "title": "K PriDe Debiasing Algorithm Adaptation for SATA",
            "content": "K.1 PriDe Introduction The original PriDe algorithm [56] is designed for processing MCQ question sets with fixed option set length (usually 4). It works by observing the probability changes when performing permutations of option IDs for each question, and it can compute priors, which is known as the probabilistic mass that the model priori assigns to option ID tokens. Here is an example to better illustrate the process: Given question set with 4 options, we compute the prior of each question from 10% of the data, take the average on each option ID position and then we get: P(prior) = [0.4, 0.2, 0.2, 0.2] The list corresponds to probabilities for ABCD. In this case we can see that the model biases towards option \"A\". Now given new question with probabilities computed as: P(observed) = [0.5, 0.3, 0.1, 0.1] Without debiasing model will select option A\" as top answer. We need to subtract prior: P(debiased) = P(observed) / P(prior) P(debiased) = [1.25, 1.5, 1.0, 0.5] Option B\" becomes top-1 after we remove the heavy prior on A\". To learn more low-level details, please refer to the original paper [56]). K.2 Limitation of Original Algorithm. However, the prior is computed on fixed length of 4, so the prior computed for each option has its own probability distribution. For dataset with variable lengths of option sets (3-15 options for our SATA-Bench). We can only use priors computed for their own length groups (for example, using length-3 prior to remove bias only for questions that have 3 options). Therefore, we might not have enough data to build an accurate prior. For example, SATA-BENCH contains only 52 out of 1650 questions with 3 choices. Adaptation to solve SATA questions. To solve the above problem, we first construct dictionary with key as the lengths seen in the dataset, and value as prior computed only from questions with corresponding length, for example: 3: [0.5, 0.3, 0.2], 4: [0.4, 0.3, 0.1, 0.2], N: [0.2, 0.1, 0.1, 0.04, 0.04, 0.01, ...] To supplement the lengths with lower datapoint, we take prefix of the longer priors, then normalize to unit vector, and use as auxiliary datapoints to help computing for shorter priors, for example 10-option prior (prior computed from 10-option question) can be used to help computing priors for 3-option question: [0.12, 0.2, 0.05, 0.17, 0.04, 0.01, 0.01, 0.02, 0.3, 0.2] [0.32, 0.54, 0.14] We take the first 3 numbers corresponding to ABC\" of 3-option question, then normalize it to the unit vector with the same probability distribution as the other 3-option priors. Similarly, this 10-option prior can also be used to compute priors for any shorter lengths. Lastly, because Choice Funnel will remove the selected option from the option set, the option IDs (ABCD) would not be continuous. Because the prior vector can only work with continuous option set, we must rebalance the option IDs. For example, ACDE\" (B\" is removed) will be rebalanced to ABCD\". K.3 Conclusion and Takeaways Once we have done this process we should have large enough population to compute accurate priors for most lengths. One limitation is that this adaptation does not help much if we dont have enough questions for longer lengths in our dataset, though this is not the case for SATA-Bench, which contains 21.88% data for its longest 15-option question. One potential solution is to use synthetic datasets to backfill longer-option questions, since the original work showed that the prior is transferable. We leave this for future work."
        },
        {
            "title": "L Experiment Setup for Choice Funnel",
            "content": "We chose fixed 90% confidence threshold as the stopping condition (ii) in Choice Funnel for all models. While this initial parameter selection was chosen for its simplicity, later evaluation indicated that it yielded sufficiently robust performance. Consequently, we did not pursue further investigation into more granular threshold adjustments. It also demonstrates that the algorithm is generalizable to other models without careful calibration. The first baseline method first token sets fixed threshold so that any option with probability above the threshold is selected, and this should be the lower bound of the performance. First token debiasing can be used to find out if the popular strategy used to solve the MCQ questions is transferable to the SATA questions in terms of minimizing the impact of the selection bias. Lastly, we expect yes/no to be competitive baseline given that it processes each choice separately. Prompts. To reduce the bias introduced by prompt design and emphasize the impact of the method itself, we choose prompts for all methods with minimal engineering effort and mainly capture the essential components: paragraph, question and choices. The complete prompts are given in Appendix H. Models. Our study focuses on the causal, decoder-only LLMs since this architecture has become the dominant choice for modern LLMs. We experiment with 7 LLMs from Table 2 under Probability Based Retrieving which are all popular open-source models on the HuggingFace website, and we can access their output probabilities: DeepSeek R1 Distilled LLAMA 8B [14], Qwen2.5 14B [54], Ministral 8B [50], Phi 3 7B [1], Phi 4 mini reasoning [2], Bloomz 7B [35], and Llama 3.1 8B [52]."
        },
        {
            "title": "M Ablation Study for Choice Funnel",
            "content": "M.1 dont know\" performs worse than None of the above\" Table 11: Performance comparison of Choice Funnel using \"None of the Above\" versus \"I dont know\" options. Method Phi3-7B + nota Phi3-7B + idk Llama3-8B + nota Llama3-8B + idk Bloomz-7B + nota Bloomz-7B + idk EM 29.27 28.18 19.88 17.64 20.18 18.00 Precision 83.27 80.92 78.69 75.50 66.62 65.55 Recall 70.24 73.25 56.19 58.03 54.90 55. JI 61.85 62.22 50.36 49.55 46.15 45.53 SPD 3.47 2.35 7.74 7.74 17.78 16.45 CtDifAbs 1.42 1.48 1.66 1.69 1.71 1.76 CtAcc 0.38 0.36 0.33 0.32 0.32 0.31 InfCost 6339 6667 4975 5066 5440 5528 We compared two commonly employed auxiliary response options in traditional survey science domain [45]: dont know (IDK) and None of the above (NOTA), examining their effectiveness as Choice Funnel stopping condition. Based on an ablation study on Table 13, NOTA yields consistently 34 better performance. When using IDK, we observe noticeable increase in InfCost and result in worse Count Bias (CtDifAbs and CtAcc), which means model tends to over select number of options, indicating that the model would rather select wrong answer than saying dont know\". This is potentially related to RLHF process, where the model is trained to generate answers that are more favorable to humans. M.2 Ablation on Choice Funnel Components Table 12: Ablation study demonstrating that PriDe token debiasing effectively mitigates unselection bias. Method Phi3-7B + debiasing only Phi3-7B + CF only Phi3-7B + CF + debiasing Llama3-8B + debiasing only Llama3-8B + CF only Llama3-8B + CF + debiasing Bloomz-7B + debiasing only Bloomz-7B + CF only Bloomz-7B + CF + debiasing EM 1.76 26.00 29.27 7.58 17.45 19.88 7.09 16.36 20. Precision 67.92 80.84 83.27 62.83 76.37 78.69 59.07 66.10 66.62 Recall 28.24 70.08 70.24 32.28 50.84 56.19 38.41 48.26 54.90 JI 27.47 60.33 61.85 30.38 46.74 50.36 32.05 42.66 46.15 SPD 175.24 4.17 3.47 151.74 10.12 7.74 149.17 23.09 17.78 CtDifAbs 2.50 1.44 1.42 2.34 1.67 1.66 2.19 1.65 1.71 CtAcc 0.05 0.35 0.38 0.14 0.34 0.33 0.15 0.35 0. InfCost 2534 6436 6339 2534 4380 4975 2534 4469 5440 We conducted an ablation study on the two sub-components of Choice Funnel: token debiasing (\"debiasing only\") and iterative selection (the process of iteratively selecting options until stopping condition is met, denoted as \"CF only\"). The analysis is performed on 3 open-source models. When comparing \"CF only\" to the complete \"CF + debiasing\", the observed increase in SPD metric demonstrates that token debiasing effectively mitigates unselection bias, yielding better performance. Nevertheless, the comparison between \"debiasing only\" and \"CF only\" reveals that our novel iterative selection component contributes more substantially to overall performance improvements. M.3 Ablation on Choice Funnel Stopping Condition Table 13: Ablation study on the two stopping conditions in Choice Funnel, showing that combining both yields the best performance. EM 3.82 29.21 29.27 0.89 19.51 19.88 9.94 12.24 20.18 Method Phi3-7B + thresholding only Phi3-7B + NOTA only Phi3-7B + thresholding + NOTA Llama3-8B + thresholding only Llama3-8B + NOTA only Llama3-8B + thresholding + NOTA Bloomz-7B + thresholding only Bloomz-7B + NOTA only Bloomz-7B + thresholding + NOTA CtDifAbs 2.22 1.20 1.42 1.74 1.94 1.66 1.72 3.31 1. Precision 65.00 77.07 83.27 71.92 69.22 78.69 64.47 55.60 66.62 InfCost 7416 9380 6339 4564 10212 4975 4506 13758 5440 Recall 74.84 85.63 70.24 52.22 85.77 56.19 48.93 89.57 54.90 CtAcc 0.13 0.37 0.38 0.27 0.25 0.33 0.29 0.17 0.32 SPD 3.37 0.69 3.47 10.53 2.24 7.74 22.50 12.82 17.78 JI 48.93 68.00 61.85 44.12 60.09 50.36 40.77 52.81 46. We conducted an ablation study to evaluate the relative importance of our two proposed stopping conditions in Choice Funnel. The results demonstrate that Choice Funnel achieves optimal performance when both conditions are applied in combination. Notably, the None of the above\" (NOTA) condition emerged as the more influential factor, suggesting that models can reliably identify when no correct answers remain among the provided options."
        },
        {
            "title": "N Positional Bias Under Randomized Answer Orderings",
            "content": "Does the benchmark include randomized answer orderings? No. In the main benchmark, each questions answer choices appear in fixed, canonical order. To quantify the extent to which large language models (LLMs) rely on this implicit positional cue, we ran an auxiliary study in which the answer choices for every question were randomly permuted (e.g. A B). We then compared model performance on the permuted dataset to its performance on the original version. Setup. All hyper-parameters, prompts, and decoding settings were kept identical to the main benchmark; only the answer order was shuffled once per question. Table 14 reports the difference 35 (permuteoriginal) for each metric, so negative values indicate drop in performance and positive values indicate an increase. CtDif is shown with downward arrow even though its baseline values are negative; more negative CtDif therefore indicates larger absolute mismatch in option counts. Table 14: Change in evaluation metrics after randomly reordering answer choices. Performance metrics are expected to increase () while bias metrics are expected to decrease (). Model SPD CtDif CtDifAbs EM Precision Recall JI RStd RSD Claude 3 Haiku 24.06 3.80 Llama 3.1 405B 34.69 3.90 34.28 35.31 5.22 4.71 +6.06 +0.17 +0.12 +9.73 0.20 +0. 0.07 0.18 0.51 0.71 Findings. All three models suffer performance degradation when answer choices are shuffled, with Claude 3 Haiku exhibiting the sharpest decline (24 EM, 35 JI). Selection / count-bias metrics (RStd, SPD, CtDifAbs) increase for every model except RSD, confirming heightened positional bias. Discussion. These results suggest that current LLMs implicitly learn positional heuristics from training data in which answer orders are fixed. Breaking this assumption makes the models less certain and more prone to biased guessing. Future work should examine (i) whether fine-tuning on randomly ordered choices mitigates the effect, and (ii) how pronounced the bias is for other model families and task domains. Per-Dataset Performance Breakdown We report detailed bias metrics for different task categories in Table 15. The News dataset has the lowest selection bias, while Reading Comprehension exhibits the highest. For count bias, Toxicity shows the smallest difference, and Biomedicine has the largest. Notably, News has significantly lower selection and count biases compared to other datasets (p-values: 0.03 for SPD and 3.8X105 CtDifAbs, T-test). All datasets show negative count difference, confirming underprediction and the presence of count bias in SATA questions. Table 15: Breakdown of Bias metrics by subject. Lower values are better for all metrics. RSD RStd SPD CtDif Task CtDifAbs Reading Comprehension Toxicity News Biomedicine Laws Events 19.29 7.59 7.13 2.83 4.32 3.16 6.66 2.37 5.75 4.17 7.15 4.14 0.20 0.10 0.11 0.07 0.08 0.19 0.15 0.14 0.13 0.16 0.13 0.19 1.53 1.39 0.48 0.56 0.12 0.23 2.90 3.60 1.54 3.43 0.85 1.02 -0.68 0.42 -0.05 0.44 -0.09 0.25 -1.71 0.96 -1.00 0.87 -0.28 0.77 0.85 0.35 1.28 0.16 0.32 0.19 2.22 0.67 1.36 0.75 1.08 0. 36 Figure 9: Ratio of false positive rate to false negative rate per label for each evaluated LLM. 37 Figure 10: Recall score per label (Y-axis), normalized by subtracting the models average recall. Most models exhibit at least one label with significantly lower recall than the rest. 38 Figure 11: Relationship between predicted and actual correct choice counts across models. Models generally under-select the correct number of answer choices. Y-axis represents the average number of choices selected by the model. X-axis represents the actual number of correct choices. perfect model would align along the diagonal where equals Y. Figure 12: Relationship between Exact Match Rate and the number of correct choices. As the number of correct choices increases, the exact match rate decreases. None of the models achieve an exact match rate above 20% when the number of correct choices exceeds 7. 39 Figure 13: Performance breakdown of evaluated models across different source datasets."
        }
    ],
    "affiliations": [
        "Amazon"
    ]
}