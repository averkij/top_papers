{
    "paper_title": "Adaptive Computation Pruning for the Forgetting Transformer",
    "authors": [
        "Zhixuan Lin",
        "Johan Obando-Ceron",
        "Xu Owen He",
        "Aaron Courville"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The recently proposed Forgetting Transformer (FoX) incorporates a forget gate into softmax attention and has shown consistently better or on-par performance compared to the standard RoPE-based Transformer. Notably, many attention heads in FoX tend to forget quickly, causing their output at each timestep to rely primarily on the local context. Based on this observation, we propose Adaptive Computation Pruning (ACP) for FoX, a method that dynamically prunes computations involving input-output dependencies that are strongly decayed by the forget gate. This is achieved using a dynamically set pruning threshold that ensures that the pruned attention weights remain negligible. We apply ACP to language model pretraining with FoX and show it consistently reduces the number of FLOPs in softmax attention by around 70% across different model sizes and context lengths, resulting in a roughly 10% to 35% improvement in training throughput. Furthermore, longer context lengths yield greater computational savings. All these speed improvements are achieved without any performance degradation. We also perform several analyses to provide deeper insights into our method, such as examining the pruning patterns and analyzing the distribution of FLOP savings across different attention heads. Our code is available at https://github.com/zhixuan-lin/arctic-fox."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 ] . [ 1 9 4 9 6 0 . 4 0 5 2 : r Preprint. Under review. Adaptive Computation Pruning for the Forgetting Transformer Zhixuan Lin Mila & Universite de Montreal zxlin.cs@gmail.com Johan Obando-Ceron Mila & Universite de Montreal jobando0730@gmail.com Xu Owen He MakerMaker AI owen.hexu@gmail.com Aaron Courville Mila & Universite de Montreal courvila@mila.quebec"
        },
        {
            "title": "Abstract",
            "content": "The recently proposed Forgetting Transformer (FoX) incorporates forget gate into softmax attention and has shown consistently better or on-par performance compared to the standard RoPE-based Transformer. Notably, many attention heads in FoX tend to forget quickly, causing their output at each timestep to rely primarily on the local context. Based on this observation, we propose Adaptive Computation Pruning (ACP) for FoX, method that dynamically prunes computations involving input-output dependencies that are strongly decayed by the forget gate. This is achieved using dynamically set pruning threshold that ensures that the pruned attention weights remain negligible. We apply ACP to language model pretraining with FoX and show it consistently reduces the number of FLOPs in softmax attention by around 70% across different model sizes and context lengths, resulting in roughly 10% to 35% improvement in training throughput. Furthermore, longer context lengths yield greater computational savings. All these speed improvements are achieved without any performance degradation. We also perform several analyses to provide deeper insights into our method, such as examining the pruning patterns and analyzing the distribution of FLOP savings across different attention heads. Our code is available at https://github.com/zhixuan-lin/arctic-fox."
        },
        {
            "title": "Introduction",
            "content": "Transformers (Vaswani et al., 2017) have quadratic time complexity with respect to context length, resulting in significant computational costs over long sequences. The recently proposed Forgetting Transformer (FoX) (Lin et al., 2025) features modified softmax attention mechanism with forget gate, which allows some attention heads to downweight distant dependencies and focus mainly on the local context. FoX has been shown to consistently achieve better or on-par performance compared to the standard RoPE-based (Su et al., 2024) Transformer in various tasks, including long-context language modeling and downstream tasks such as the needle-in-a-haystack test (Kamradt, 2023). It is also compatible with the FlashAttention (Dao, 2024) algorithm, which allows efficient processing of long sequences. Lin et al. (2025) show that many attention heads in FoX tend to forget quickly. For these heads, the dependencies between distant input-output pairs are extremely weak and can potentially be ignored. Based on this observation, we propose Adaptive Computation Pruning (ACP) for FoX, method that dynamically prunes computations involving input-output dependencies that are strongly decayed by the forget gate. The computations to be pruned are determined with dynamically set threshold that guarantees that the pruned attention weights are negligible. As shown in Figure 1, this can be easily achieved by identifying pruning boundary across the grid of computations in FlashAttention with linear time Correspondence to Zhixuan Lin. 1 Preprint. Under review. Figure 1: Illustration of Forgetting Attention with and without ACP. Each cell represents block in the FlashAttention algorithm. Darker colors represent decay bias values farther below 0 and thus stronger decay. The arrows indicate the set of blocks that would be visited (in the indicated order) in the FlashAttention iterations. complexity algorithm. After identifying the pruning boundary, we only visit the remaining blocks for the FlashAttention iterations, and thus no computations are wasted on the pruned dependencies. We apply ACP to language model pretraining with FoX with sizes from 125M to 760M parameters and training context lengths from 4k to 16k tokens. We find that ACP consistently prunes around 70% of the FLOPs in softmax attention across the tested model sizes and context lengths, resulting in roughly 10% to 35% improvement in training throughput. In particular, longer context lengths lead to greater flop savings and throughput improvements. These speed improvements are achieved without affecting language modeling loss and downstream task performance. To provide further insight into our method, we conduct series of analyses such as examining the pruning boundaries and analyzing the distribution of FLOP savings across different attention heads. Notably, our analysis reveals the existence of local heads and global heads that are responsible for modeling dependencies of different lengths. Finally, in addition to our current results that focus on applying ACP during training, we also discuss how ACP could be used to reduce computation and memory usage for decoding at inference time."
        },
        {
            "title": "2 Preliminaries: Forgetting Transformer",
            "content": "This section gives brief introduction to the Forgetting Transformer and in particular its FlashAttention-based implementation. Throughout this work, we follow Yang et al. (2024) and use notation such as A[m] and A[m][n] to index block of matrix (or vector). For example, for matrix RLL and block sizes Bq and Bk for the two dimensions of A, A[m][n] RBqBk would be block of such that (A[m][n])xy = Aij, where = (m 1) Bq + and = (n 1) Bk + y. The Forgetting Transformer features modified softmax attention mechanism with forget gate, called Forgetting Attention. Forgetting Attention takes sequence of input vectors (xi)L i=1. In addition to the usual query/key/value projections qi, ki, vi = Wqxi, Wkxi, Wvxi Rd, at each timestep we also compute scalar forget gate ft = σ(w xt + ) R, where σ is the sigmoid function. The output of the attention is then i=1 and produces sequence of output vectors (oi)L oi = j=1 Fij exp(q j=1 Fij exp(q l=j+1 fl and Dij = log Fij = i kj/ kj/ d)vj d) = j=1 exp(q j=1 exp(q i kj/ kj/ + Dij)vj + Dij) , (1) where Fij = This can be written in matrix form: l=j+1 log fl, with Fii = 1 and Dii = 0 for any i. = softmax(QK/ + D)V RLd, (2) 2 Preprint. Under review. where RLL is the decay bias matrix containing the Dij factors as its lower triangular entries and above its main diagonal. Q, K, , RLd are matrices containing qi, ki, vi, oi, {1, . . . , L} as the rows. For multi-head attention with heads, we maintain instances of forget gate parameters {w h=1 and compute the forget gate h=1 separately for each head. We will omit the (h) superscript throughout this values { work and assume represents the dimension of each head. h=1 and {b (h) }H (h) }H (h) }H Properties of The matrix has some nice properties. In particular, for any i, j, x, such that and y, we have Dij Dxy. This property is visualized in Figure 1, where darker colors indicate Dij value farther below zero. This special structure is crucial for developing an efficient pruning algorithm. FlashAttention implementation of Forgetting Attention The matrix can be computed as = c1 1c, where RL contains the cumulative sums ci = l=1 log fl, {1, . . . , L} and 1 RL is vector of all ones. This makes it possible to implement Forgetting Attention with simple modification to the FlashAttention algorithm. We briefly describe the forward pass. In FlashAttention, queries are divided into blocks {Q[m] RBqd}M m=1 with block size Bq = L/M. The keys and values are similarly divided into blocks {K[n], V[n] RBkd}N n=1 with block size Bk = L/N. All the computations are then conceptually organized into grid, as shown in Figure 1. In standard softmax attention without forget gates, FlashAttention computes the attention logit blocks S[m][n] = Q[m]K in the shared memory (SRAM) of the GPU sequentially across the key/value block dimension and in parallel across the query dimension (see Figure 1 left). To implement Forgetting Attention, we only need to additionally load c[m] and c[n] into SRAM, construct D[m][n] = c[m]1 1c [n], and compute the modified attention logits S[m][n] = Q[m]K + D[m][n]. The rest of the forward pass remains the same as in the standard FlashAttention. The backward pass is implemented similarly. [n]/ [n]/ Model architecture Throughout this work, we use the FoX (Pro) architecture introduced in Lin et al. (2025). Following Lin et al. (2025), we do not use RoPE (Su et al., 2024). The Pro architecture enhances the basic LLaMA (Touvron et al., 2023) architecture by incorporating some common components in recurrent sequence models such as QK-norm (Dehghani et al., 2023), output gate, output normalization, and data-dependent token-shift (Peng et al., 2024). The use of QK-norm allows us to easily obtain an upper bound of the attention logits, which will be useful for the calculation of the adaptive threshold in ACP."
        },
        {
            "title": "3 Adaptive Computation Pruning",
            "content": "We now introduce our method, Adaptive Computation Pruning (ACP). Conceptually, ACP aims to prune all the computations in the term exp(q + Dij)vj if Dij < δ, where δ < 0 is dynamically set threshold (explained later). The attention outputs after pruning are given by: kj/ oi = 1{Dij δ} exp(q 1{Dij δ} exp(q kj/ kj/ j=1 j=1 + Dij)vj + Dij) , (3) where 1{} is the indicator function that takes 1 if the inner proposition is true and 0 otherwise. The intuition of ACP is as follows. Let sij = and be an upper bound of {sij}i,j{1,...,L}, i.e. maxi,j{1,...,L} sij. Since by definition Dii = 0 for any i, if for some j, Dij is much smaller than 2U, then the corresponding attention weight Aij = exp(sij+Dij) exp(sii+Dii) = exp(sij sii + Dij) exp(2U Dij) would be very exp(sij+Dij) kj/ k=1 exp(sik+Dik) 3 Preprint. Under review. small, making the contribution of vj to oi negligible. And thus the related computations can be safely skipped. j=1 Setting the threshold δ dynamically In practice, we set the threshold δ dynamically based on an upper bound of {sij}i,j{1,...,L} and the sequence length so that the total pruned 1{Dij < δ}Aij for any would be bounded by small number attention weights ε > 0. Concretely, we set δ = 2U log + log ε, which achieves the above guarantee (see Appendix for proof). We set ε = e10 0.000045 throughout this work to ensure that the impact of ACP on attention outputs is negligible. Note that setting δ dynamically requires us to know an upper bound of {sij}i,j{1,...,L}. Since we use QK-norm with RMSNorm (Zhang & Sennrich, 2019), the L2-norms of queries and γq and keys are bounded by γk is the maximum magnitude of the key RMSNorm scaling parameters {γk i=1 and γq is defined similarly. Therefore sij respectively, where γk = maxi{1,...,d} γk and thus we set = γkγq γkγq }d d.1 qi2kj2 Block-level pruning In FlashAttention, computations are performed in blocks. Conceptually, these blocks of computation are organized into grid as shown in Figure 1, where is the number of query blocks and is the number of key and value blocks. Therefore, in practice, ACP operates at the block level and we prune the computation block (m, n) if and only if all entries in D[m][n] are below δ, or equivalently, the maximum entry of D[m][n] is below δ. Due to the structure of D, the maximum entry of D[m][n] RBqBk (denoted as max(D[m][n]) in the following) is simply its top right entry (D[m][n])1,Bk = (c[m])1 (c[n])Bk . Therefore, we only need to check this entry to determine whether block should be pruned.2 Two-stage implementation Due to the structure of D, it is easy to show that if max(D[m][n]) < δ then max(D[x][y]) < δ for any and n. This means that the set of computation blocks to be pruned constitutes consecutive region on the lower left part of the grid, as shown in Figure 1 (right). In addition, this region is separated from the rest of the grid by pruning boundary that connects the top left corner and the bottom right corner of the grid. Based on the above observation, we can perform ACP in two stages. First, we identify the pruning boundary. Specifically, for each row m, we determine the first computation block (m, nm) on the right of the pruning boundary on row m. In Figure 1 (right), these correspond to the blocks at the start of each arrow. After this is done, for each row m, we start the FlashAttention iterations from block (m, nm) (instead of block (m, 1)), and therefore no computations would be wasted on the pruned blocks. Algorithm 1 Index search for boundary blocks Require: Cumsum of log forget gates RL, threshold δ, number of query blocks Ensure: nm be the column index of the boundary block on row for each {1, . . . , M} 1: 1 2: for from 1 to do 3: Dmax 4: while Dmax < δ do 5: 6: 7: 8: 9: end for Dmax = (c[m])1 (c[l])Bk (This is the top-right and the maximum entry of D[m][l]) + 1 (We loop until (m, l) is boundary block) end while Set nm = 1Note that even without QK-norm one can still get an upper bound by computing the maximum L2-norms of queries and keys manually. 2If D[m][n] is located on the diagonal of the grid, it is not pruned by default as it would contain an entry Dii for some i, which by definition is zero. 4 Preprint. Under review. Identifying boundary block indices The final missing piece of ACP is an algorithm to identify the column index nm of the boundary block on each row m. Due to the structure of D, for any two such boundary blocks (m, nm) and (x, yx) we have nm yx. This makes it possible to use an efficient linear complexity algorithm to identify the boundary block indices, shown in Algorithm 1. In practice, we find that the time spent identifying the boundary block indices is negligible compared to the actual attention computation."
        },
        {
            "title": "4 Experiments",
            "content": "Though ACP can also be applied during inference (e.g., prefilling and decoding), in this work, we focus on applying ACP during training and measure the resulting FLOP savings and training throughput improvement. 4.1 Experimental setup We train FoX (Pro) models with and without ACP on LongCrawl64 (Buckman, 2024) using the standard language modeling objective. We adopt the three training configurations used in the analysis experiments in Lin et al. (2025), specified as combinations of number of model parameters and number of training tokens: 760M-parameter/16B-token, 360Mparameter/7.5B-token, and 125M-parameter/2.7B-token. For each scale, we train the models with three training context lengths: 4k, 8k, and 16k tokens. The rest of the hyperparameters are the same as those in Lin et al. (2025) and are described in detail in Appendix B. We use the official Forgetting Transformer repository3 for the implementation. We implement ACP, including the boundary index search algorithm, on top of the official Forgetting Attention kernel in Triton (OpenAI, 2021). In the following, training throughputs are measured using the final checkpoints on subset of the heldout set of LongCrawl64 on 4 NVIDIA L40S GPUs. We find that training throughput typically decreases for short period at the beginning of training and then plateaus, so our reported numbers using the final checkpoints reflect the throughput during the plateau period. The percentage of pruned FLOPs in the attention operation is calculated as the ratio of the pruned FLOPs to the total FLOPs of the attention operation, measured on subset of the heldout set of LongCrawl64. More details can be found in Appendix B. 4.2 FLOP savings and throughput improvement In Figure 2 we show the percentage of pruned FLOPs in the attention operation and the relative improvement in training throughput due to ACP, across different model sizes and training context lengths. As shown in Figure 2, ACP consistently prunes around 70% of the FLOPs in softmax attention in all cases, resulting in roughly 10% to 35% improvement in training throughput. longer training context length results in larger throughput improvement, which is expected because the ratio of FLOPs in softmax attention to the rest of the network increases as the training context length increases. For example, for 760M-parameter model with context length of 4k tokens, the attention operation roughly accounts for 16% of the total FLOPs of the model, while for context length of 16k tokens, it is around 45%. Note that ACP only affects the speed of the attention operation, but the training throughput we show in Figure 2 depends on the speed of the entire model (including MLPs). This means that the speed-up of the attention operation alone due to ACP is much larger than 10% to 35%.4 ACP does not damage performance In Figure 3 (left) we show the language modeling loss at different token positions for the 760M-parameter FoX (Pro) models with different training context lengths, with and without ACP. Figure 3 (right) shows the needle-in-a-haystack retrieval results of the 16k-context-length model in Figure 3 (left), following the easy mode 3https://github.com/zhixuan-lin/forgetting-transformer 4Unfortunately it is technically difficult to accurately measure the speed-up in the attention operation alone. This is because ACP is data-dependent, so we must run the entire network to measure runtime. 5 Preprint. Under review. Figure 2: (left) Percentage of pruned FLOPs in the attention operation. (right) Percentage 1, where TPacp and TPno-acp are the of throughput improvement, measured as training throughput with and without ACP, respectively. Within each bar we also show the actual values of TPacp and TPno-acp. The unit of throughput is tokens per second. Throughput is measured on 4 NVIDIA L40S GPUs. TPacp TPno-acp (left) Per-token loss given different training context lengths for the 760MFigure 3: parameter/16B-token setting. This is measured on 2B-token validation set of the LongCrawl64. At each token index i, we report the averaged loss over window of 101 centered at i. (right) Easy-mode needle-in-a-haystack results for the 760M-parameter models with training context length of = 16k tokens. Model Wiki. LMB. LMB. PIQA Hella. Wino. ARC-e ARC-c COPA OBQA SciQA BoolQ Avg ppl acc-n acc-n acc-n ppl acc acc acc acc acc acc acc FoX (Pro) w/ ACP, = 4k FoX (Pro) w/o ACP, = 4k FoX (Pro) w/ ACP, = 8k FoX (Pro) w/o ACP, = 8k FoX (Pro) w/ ACP, = 16k FoX (Pro) w/o ACP, = 16k 29.66 29.98 28.04 28.07 27.96 28. 22.12 22.32 23.20 22.53 25.16 24.29 37.57 37.65 38.13 38.31 35.77 36.66 63.11 62.84 60.94 61.81 62.35 62.35 33.59 33.38 33.46 33.83 33.79 33.32 52.41 52.72 51.70 50.67 50.83 48.86 48.91 47.94 48.82 49.28 48.02 48. 24.66 25.60 24.66 24.83 24.23 25.51 68.00 67.00 67.00 69.00 69.00 71.00 29.20 29.60 28.60 27.40 28.20 27.20 79.90 79.70 80.00 80.80 79.50 82.20 57.16 54.22 60.12 61.59 58.93 56.76 49.45 49.06 49.34 49.75 49.06 49. Table 1: Evaluation results on LM-eval-harness. All models have roughly 760M nonembedding parameters and are trained on roughly 16B tokens on LongCrawl64. acc-n means length-normalized accuracy. is the training context length. 6 Preprint. Under review. Figure 4: Distribution of per-head FLOP savings in 760M-parameter FoX (Pro) model with 4k training context length. Specifically, we divide FLOPs savings into 20 bins [0%, 5%), [5%, 10%), . . . , [95%, 100%], and for each bin we count the number of heads in the model whose percentage of pruned attention FLOPs falls into that bin. The counts are then normalized to obtain distribution. Figure 5: Distribution of per-head FLOP savings in each layer. Each column can be seen as 90-rotated (and flipped) version of Figure 4, except the distribution is calculated within each layer. The x-axis of each column is the percentage of heads in the corresponding layer whose percentage of pruned FLOPs falls within specific bin. The range of the x-axis of each column is from 0% to 100%. setup used in Lin et al. (2025) that is suitable for small models without instruction-tuning. Table 1 shows the evaluation results on various downstream tasks from Language Model Evaluation Harness (Gao et al., 2024a) for the models in Figure 3 (left). Additional results can be found in Appendix C. As shown in these results, the per-token language modeling loss curves with and without ACP almost match exactly (the slight difference is within the expected variance across runs). ACP also does not damage long-context retrieval performance, and the differences in downstream task performance between models with and without ACP are small. Note that it is well known that evaluation results in downstream tasks can exhibit high variance across training runs (Madaan et al., 2024), so it is impossible to get exactly the same results even with the same model trained with different seeds. 4.3 Analyses In this section, we perform series of analyses to provide deeper insight into our method. First, we show the distribution of FLOP savings across different attention heads. Second, we visualize the pruning boundaries in some heads. Finally, we investigate how computational savings and model performance vary with ε, the hyperparameter that bounds the total pruned attention weights. 7 Preprint. Under review. Figure 6: Visualization of the decay matrices (top row) and the corresponding attention weight matrices (bottom row) from three heads in different layers. The orange line shows the pruning boundary. Since is very sparse, we only show entries with scores larger than 0.1. These results use 760M-parameter FoX (Pro) model with context length of 4k tokens. In Figure 4, we show the distribution of per-head Distribution of per-head FLOP savings FLOP savings in 760M-parameter FoX (Pro) model with context length of 4k tokens, over the set of all attention heads in the model. Figure 4 shows clear bimodal pattern, and most attention heads are either local heads (most FLOPs are pruned) or global heads (only small proportion or none of the FLOPs are pruned). Furthermore, majority of the heads are local heads, consistent with the around 70% FLOP savings shown in Figure 2. In Figure 5 we also show the distribution of per-head FLOP savings within each layer. In general, the distribution for each layer matches the distribution for the entire model, except for the first two layers where all the heads are local. In Figure 6 we show the decay bias matrices and Visualization of pruning boundaries the attention weight matrices from three heads in different layers. We also show the pruning boundaries on the matrices. The heads on the left and middle are local heads with strong decay, and most off-diagonal blocks are pruned. The rightmost head is typical global head where no blocks are pruned. Effect of varying ε In Figure 7 we show the impact of ε the hyperparameter controlling the maximum total attention weights that can be pruned on FLOP savings and language modeling loss. As expected, with smaller ε the proportion of pruned FLOPs in the attention operation decreases. These results also demonstrate the robustness of ACP to the choice of ε. Even with ε = e1 0.36 which in theory could cause significant amount of attention weights to be pruned there is no impact on language modeling performance, likely because only some unlikely combinations of decay biases, queries, and keys would cause the total pruned attention weights to approach ε.5 5With that being said, we still recommend setting ε to small values such as our default ε = e10 to avoid potential performance degradation. Preprint. Under review. Figure 7: Impact of ε on FLOP savings for 125M-parameter model with training context length of 16k tokens. For each data point we also label the corresponding validation loss."
        },
        {
            "title": "5 Related work",
            "content": "Dynamic locality-based computation pruning The most similar methods to ours are context pruning in Selective Attention (Leviathan et al., 2024) and conditional computation in stick-breaking attention (Tan et al., 2024). Similarly to FoX, both Selective Attention and stick-breaking attention learn some forms of data-dependent decay, and thus dynamic pruning similar to our ACP is possible. For Selective Attention, this is done at inference time by maintaining mixed memory budget and dropping KV-cache entries that have the strongest decay. However, it is unclear how this can be adapted for training, like what we do in this work with ACP. For stick-breaking attention, this is done by early stopping the stick-breaking process for each query until all attention weights have been assigned. Although for stick-breaking attention conditional computation can also be used in training, Tan et al. (2024) only investigate applying it during inference, so it is unclear how much speed improvement can be obtained when it is applied during training. Sliding-window-based computation pruning Methods such as StreamingLLM (Xiao et al., 2024c), LM-Infinite (Han et al., 2023), MoA (Fu et al., 2024a), and DuoAttention (Xiao et al., 2024b) apply sliding window mask to pretrained models at inference time to reduce computational costs. This approach is also frequently used in KV-cache eviction methods, and is often combined with some importance-based eviction policy (Zhang et al., 2023; Liu et al., 2023; Ge et al., 2024; Oren et al., 2024; Fu et al., 2024b). With ACP, local heads behave similarly to sliding-window attention. However, unlike these related methods where the window size is typically fixed or based on profiling on some dataset, the window size of local head in ACP is determined by the decay bias matrix and the dynamically set threshold, which guarantees that the total attention weights beyond the local window are negligible. Sparse attention Another category of computation pruning methods exploits the sparsity of softmax attention. These methods mainly differ in how they evaluate the importance of different KV-cache entries based on queries. Most sparse attention methods divide the KV cache into blocks, calculate summary of each block, and then compute the importance scores using these block summaries (Tang et al., 2024; Xiao et al., 2024a; Gao et al., 2024b; Yuan et al., 2025; Lu et al., 2025). There also exist token-level methods (Desai et al., 2024; Anagnostidis et al., 2023) and more sophisticated methods such as cluster-based method (Liu et al., 2024) or mixture of different sparse attention methods (Jiang et al., 2024). These are orthogonal to our locality-based approach, and it is likely that they can be combined with ACP. 9 Preprint. Under review."
        },
        {
            "title": "6 Conclusion",
            "content": "We propose Adaptive Computation Pruning (ACP) for the Forgetting Transformer (FoX), method that dynamically prunes computations involving input-output dependencies that are strongly decayed by the forget gate in FoX, based on dynamically set threshold value that ensures negligible impact on the attention output. We apply ACP to language model pretraining and find it leads to significant FLOP savings and throughput improvements, without sacrificing model performance. Even though in this work we focus on applying ACP during pretraining, it could also be used for prefilling and decoding at inference time. In particular, for decoding, KV-cache entries could be dynamically evicted based on the pruning boundary, thus reducing the memory and I/O costs. We leave the investigation of inference-time ACP to future work."
        },
        {
            "title": "Acknowledgments",
            "content": "ZL thanks Shawn Tan and Songlin Yang for their helpful discussion. AC acknowledges funding from Microsoft research. This research was enabled in part by the compute resources, software and technical help provided by Mila (mila.quebec) and the Digital Research Alliance of Canada (alliance.can.ca)."
        },
        {
            "title": "References",
            "content": "Sotiris Anagnostidis, Dario Pavllo, Luca Biggio, Lorenzo Noci, Aurelien Lucchi, and Thomas Hofmann. Dynamic context pruning for efficient and interpretable autoregressive transformers. Advances in Neural Information Processing Systems, 36:6520265223, 2023. Jacob Buckman. Longcrawl64: Long-Context Natural-Language Dataset, 2024. URL https://manifestai.com/articles/longcrawl64. Tri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning. In The Twelfth International Conference on Learning Representations, 2024. Mostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr Padlewski, Jonathan Heek, Justin Gilmer, Andreas Peter Steiner, Mathilde Caron, Robert Geirhos, Ibrahim Alabdulmohsin, et al. Scaling vision transformers to 22 billion parameters. In International Conference on Machine Learning, pp. 74807512. PMLR, 2023. Aditya Desai, Shuo Yang, Alejandro Cuadron, Ana Klimovic, Matei Zaharia, Joseph Gonzalez, and Ion Stoica. Hashattention: Semantic sparsity for faster inference. arXiv preprint arXiv:2412.14468, 2024. Tianyu Fu, Haofeng Huang, Xuefei Ning, Genghan Zhang, Boju Chen, Tianqi Wu, Hongyi Wang, Zixiao Huang, Shiyao Li, Shengen Yan, et al. Moa: Mixture of sparse attention for automatic large language model compression. CoRR, 2024a. Yu Fu, Zefan Cai, Abedelkadir Asi, Wayne Xiong, Yue Dong, and Wen Xiao. Not all heads matter: head-level kv cache compression method with integrated retrieval and reasoning. arXiv preprint arXiv:2410.19258, 2024b. Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noach, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. framework for few-shot language model evaluation, 07 2024a. URL https://zenodo.org/records/12608602. Yizhao Gao, Zhichen Zeng, Dayou Du, Shijie Cao, Hayden Kwok-Hay So, Ting Cao, Fan Yang, and Mao Yang. Seerattention: Learning intrinsic sparse attention in your llms. arXiv preprint arXiv:2410.13276, 2024b. Preprint. Under review. Suyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, Jiawei Han, and Jianfeng Gao. Model tells you what to discard: Adaptive kv cache compression for llms. In The Twelfth International Conference on Learning Representations, 2024. Chi Han, Qifan Wang, Hao Peng, Wenhan Xiong, Yu Chen, Heng Ji, and Sinong Wang. Lm-infinite: Zero-shot extreme length generalization for large language models. arXiv preprint arXiv:2308.16137, 2023. Huiqiang Jiang, Yucheng Li, Chengruidong Zhang, Qianhui Wu, Xufang Luo, Surin Ahn, Zhenhua Han, Amir Abdi, Dongsheng Li, Chin-Yew Lin, et al. Minference 1.0: Accelerating pre-filling for long-context llms via dynamic sparse attention. Advances in Neural Information Processing Systems, 37:5248152515, 2024. Gregory Kamradt, 2023. URL https://github.com/gkamradt/LLMTest NeedleInAHaystack/ blob/main/README.md. Yaniv Leviathan, Matan Kalman, and Yossi Matias. Selective attention improves transformer. arXiv preprint arXiv:2410.02703, 2024. Zhixuan Lin, Evgenii Nikishin, Xu He, and Aaron Courville. Forgetting transformer: Softmax attention with forget gate. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?id=q2Lnyegkr8. Guangda Liu, Chengwei Li, Jieru Zhao, Chenqi Zhang, and Minyi Guo. Clusterkv: Manipulating llm kv cache in semantic space for recallable compression. arXiv preprint arXiv:2412.03213, 2024. Zichang Liu, Aditya Desai, Fangshuo Liao, Weitao Wang, Victor Xie, Zhaozhuo Xu, Anastasios Kyrillidis, and Anshumali Shrivastava. Scissorhands: Exploiting the persistence of importance hypothesis for llm kv cache compression at test time. Advances in Neural Information Processing Systems, 36:5234252364, 2023. Loshchilov. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. Enzhe Lu, Zhejun Jiang, Jingyuan Liu, Yulun Du, Tao Jiang, Chao Hong, Shaowei Liu, Weiran He, Enming Yuan, Yuzhi Wang, et al. Moba: Mixture of block attention for long-context llms. arXiv preprint arXiv:2502.13189, 2025. Lovish Madaan, Aaditya Singh, Rylan Schaeffer, Andrew Poulton, Sanmi Koyejo, Pontus Stenetorp, Sharan Narang, and Dieuwke Hupkes. Quantifying variance in evaluation benchmarks. arXiv preprint arXiv:2406.10229, 2024. OpenAI, 2021. URL https://github.com/triton-lang/triton. Matanel Oren, Michael Hassid, Nir Yarden, Yossi Adi, and Roy Schwartz. Transformers are multi-state rnns. arXiv preprint arXiv:2401.06104, 2024. Bo Peng, Daniel Goldstein, Quentin Anthony, Alon Albalak, Eric Alcaide, Stella Biderman, Eugene Cheah, Teddy Ferdinan, Haowen Hou, Przemysław Kazienko, et al. Eagle and finch: Rwkv with matrix-valued states and dynamic recurrence. In First Conference on Language Modeling, 2024. Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. Shawn Tan, Yikang Shen, Songlin Yang, Aaron Courville, and Rameswar Panda. Stickbreaking attention. arXiv preprint arXiv:2410.17980, 2024. Jiaming Tang, Yilong Zhao, Kan Zhu, Guangxuan Xiao, Baris Kasikci, and Song Han. Quest: Query-aware sparsity for efficient long-context llm inference. In International Conference on Machine Learning, pp. 4790147911. PMLR, 2024. 11 Preprint. Under review. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017. URL https://proceedings.neurips.cc/paper files/paper/2017/ file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf. Chaojun Xiao, Pengle Zhang, Xu Han, Guangxuan Xiao, Yankai Lin, Zhengyan Zhang, Zhiyuan Liu, and Maosong Sun. InfLLM: Training-free long-context extrapolation for LLMs with an efficient context memory. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024a. URL https://openreview.net/forum?id=bTHFrqhASY. Guangxuan Xiao, Jiaming Tang, Jingwei Zuo, Junxian Guo, Shang Yang, Haotian Tang, Yao Fu, and Song Han. Duoattention: Efficient long-context llm inference with retrieval and streaming heads. arXiv preprint arXiv:2410.10819, 2024b. Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streamIn The Twelfth International Conference on ing language models with attention sinks. Learning Representations, 2024c. Songlin Yang, Bailin Wang, Yu Zhang, Yikang Shen, and Yoon Kim. Parallelizing linear transformers with the delta rule over sequence length. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. Jingyang Yuan, Huazuo Gao, Damai Dai, Junyu Luo, Liang Zhao, Zhengyan Zhang, Zhenda Xie, YX Wei, Lean Wang, Zhiping Xiao, et al. Native sparse attention: Hardware-aligned and natively trainable sparse attention. arXiv preprint arXiv:2502.11089, 2025. Biao Zhang and Rico Sennrich. Root mean square layer normalization. Advances in Neural Information Processing Systems, 32, 2019. Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin Zheng, Ruisi Cai, Zhao Song, Yuandong Tian, Christopher Re, Clark Barrett, et al. H2o: Heavy-hitter oracle for efficient generative inference of large language models. Advances in Neural Information Processing Systems, 36:3466134710, 2023. Preprint. Under review."
        },
        {
            "title": "A Proof of upper bound of total pruned attention weights",
            "content": "In this section we prove that when the threshold δ is properly set, the total pruned attention weights 1{Dij < δ}Aij would be bounded by small number ε. j=1 Let sij = and be an upper bound of {sij}i,j{1,...,L}, i.e. maxi,j{1,...,L} sij. Let be the sequence length. If we set the threshold to δ = 2U log + log ε, then for any and such that Dij < δ, we have that (note that Dii = 0 by definition): kj/ Aij = exp(sij + Dij) k=1 exp(sik + Dik) exp(sij + Dij) exp(sii + Dii) = exp(sij sii + Dij) exp(sij sii + Dij) exp(2U + Dij) exp(2U 2U log + log ε) = ε . (4) (5) (6) Therefore, we have 1{Dij < δ}Aij < ε for any and and j=1 1{Dij < δ}Aij < ε."
        },
        {
            "title": "Configuration",
            "content": "nlayers dmodel dhead 760M params / 16B tokens 360M params / 7.5B tokens 125M params / 2.7B tokens 24 24 12 1536 1024 64 64 64 Peak learning rate 1 103 2 103 2 103 Table 2: Hyperparameters for different configurations. nlayer counts the number of blocks, where each block contains an attention layer and an SwiGLU layer. Our pretraining hyperparameters follow the setup used in the analysis experiments in Lin et al. (2025). We list the hyperparameters for different training configurations used in this work in Table 2. All models are trained with AdamW (Loshchilov, 2017) with (β1, β2) = (0.9, 0.95), with linear learning rate warmup from 0 to the peak learning rate for the first 256 220 tokens and then cosine decay to 0. Each training batch contains 0.5 220 tokens. All models use weight decay of 0.1 and gradient clipping of 1.0. We follow the HuggingFace LLaMA initialization and initialize all linear layer weights and embedding parameters with (0, 0.022). We do not share the parameters between the embedding layer and the output layer. Weight decay is not applied to the RMSNorm parameters and bias terms in linear layers (only the forget gate projection has bias term). We use bfloat16 mixed-precision training for all models. FLOP savings are measured on 128M-token subset of the LongCrawl64 heldout set. Training throughput is measured on 32M-token subset of the same heldout set. When measuring throughput, we use gradient checkpointing and gradient accumulation. Each gradient accumulation step processes 32k tokens. Throughput is measured on 4 NVIDIA L40S GPUs with fully sharded data parallel. The power limit of these GPUs are set to 325W."
        },
        {
            "title": "C Additional results",
            "content": "In Figure 8 we show the per-token loss for the 125M-parameter/2.7B-token and 360Mparameter/7.5B-token settings for FoX (Pro) with and without ACP, in addition to the 760M-parameter/16B-token setting in Figure 3 (left). In Figure 9 we show the easy-mode needle-in-a-haystack results for models trained with context lengths of 4k and 8k tokens, respectively, in addition to the 16k-context-length results in Figure 3 (right). 13 Preprint. Under review. (left) Per-token loss given different training context lengths for the 125MFigure 8: parameter/2.7B-token and 360M-parameter/7.5B-token setting. This is measured on 2B-token validation set of the LongCrawl64. At each token index i, we report the averaged loss over window of 101 centered at i. Figure 9: Easy-mode needle-in-a-haystack results for the 760M-parameter models with training context lengths of 4k and 8k tokens."
        }
    ],
    "affiliations": [
        "MakerMaker AI",
        "Mila & Universite de Montreal"
    ]
}