{
    "paper_title": "JMMMU-Pro: Image-based Japanese Multi-discipline Multimodal Understanding Benchmark via Vibe Benchmark Construction",
    "authors": [
        "Atsuyuki Miyai",
        "Shota Onohara",
        "Jeonghun Baek",
        "Kiyoharu Aizawa"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "This paper introduces JMMMU-Pro, an image-based Japanese Multi-discipline Multimodal Understanding Benchmark, and Vibe Benchmark Construction, a scalable construction method. Following the evolution from MMMU to MMMU-Pro, JMMMU-Pro extends JMMMU by composing the question image and question text into a single image, thereby creating a benchmark that requires integrated visual-textual understanding through visual perception. To build JMMMU-Pro, we propose Vibe Benchmark Construction, a methodology in which an image generative model (e.g., Nano Banana Pro) produces candidate visual questions, and humans verify the outputs and, when necessary, regenerate with adjusted prompts to ensure quality. By leveraging Nano Banana Pro's highly realistic image generation capabilities and its ability to embed clean Japanese text, we construct a high-quality benchmark at low cost, covering a wide range of background and layout designs. Experimental results show that all open-source LMMs struggle substantially with JMMMU-Pro, underscoring JMMMU-Pro as an important benchmark for guiding future efforts in the open-source community. We believe that JMMMU-Pro provides a more rigorous evaluation tool for assessing the Japanese capabilities of LMMs and that our Vibe Benchmark Construction also offers an efficient guideline for future development of image-based VQA benchmarks."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 1 ] . [ 1 0 2 6 4 1 . 2 1 5 2 : r JMMMU-Pro: Image-based Japanese Multi-discipline Multimodal Understanding Benchmark via Vibe Benchmark Construction Atsuyuki Miyai Shota Onohara Jeonghun Baek Kiyoharu Aizawa miyai@cvm.t.u-tokyo.ac.jp {onohara, baek, aizawa}@hal.t.u-tokyo.ac.jp The University of Tokyo (cid:153) https://mmmu-japanese-benchmark.github.io/JMMMU_Pro/ Dataset (cid:135) Code 3 Leaderboard Figure 1: Building JMMMU-Pro via Vibe Benchmark Construction. JMMMU-Pro extends JMMMU by embedding each question image and text into single image. To construct JMMMU-Pro, we propose Vibe Benchmark Construction, where an image generation model creates questions, followed by human verification and prompt refinement to ensure quality. Experiments indicate that current open-source LMMs struggle with JMMMU-Pro."
        },
        {
            "title": "Abstract",
            "content": "This paper introduces JMMMU-Pro, an image-based Japanese Multi-discipline Multimodal Understanding Benchmark, and Vibe Benchmark Construction, scalable construction method. Following the evolution from MMMU to MMMU-Pro, JMMMU-Pro extends JMMMU by composing the question image and question text into single image, thereby creating benchmark that requires integrated visual-textual understanding through visual perception. To build JMMMU-Pro, we propose Vibe Benchmark Construction, methodology in which an image generative model (e.g., Nano Banana Pro) produces candidate visual questions, and humans verify the outputs and, when necessary, regenerate with adjusted prompts to Preprint. ensure quality. By leveraging Nano Banana Pros highly realistic image generation capabilities and its ability to embed clean Japanese text, we construct high-quality benchmark at low cost, covering wide range of background and layout designs. Experimental results show that all open-source LMMs struggle substantially with JMMMU-Pro, underscoring JMMMU-Pro as an important benchmark for guiding future efforts in the open-source community. We believe that JMMMU-Pro provides more rigorous evaluation tool for assessing the Japanese capabilities of LMMs and that our Vibe Benchmark Construction also offers an efficient guideline for future development of image-based VQA benchmarks."
        },
        {
            "title": "Introduction",
            "content": "With the recent success of large multimodal models (LMMs) in English [34, 23, 24], there has been growing interest in developing multilingual LMMs [52, 10, 58, 6] and LMMs specialized for non-English languages [41, 46]. Although LMM development in the Japanese domain has emerged [41, 4, 40], progress has been slower than in the English domain, in part due to the limited evaluation benchmarks. Given the large and rapidly growing population of Japanese LMM users, there is an increasing need to establish more Japanese benchmarks that can facilitate the development of LMMs capable of handling the Japanese language and culture seamlessly. Among the several benchmarks for Japanese LMMs [16, 5, 32, 33], one of the most representative is JMMMU (Japanese Massive Multi-discipline Multimodal Understanding Benchmark) [33]. Inspired by the MMMU benchmark [57], JMMMU is the first benchmark designed to evaluate LMMs on extensive, multi-disciplinary tasks in Japanese that require college-level subject knowledge, deliberate reasoning, and cultural understanding. JMMMU consists of culture-agnostic (CA) subset of 720 items, constructed through translation from MMMU, and culture-specific (CS) subset of 600 items that incorporate Japanese cultural elements. This systematic design enables apple-to-apple comparisons with the original MMMU through the CA subset while simultaneously evaluating cultural understanding through the CS subset. Due to its comprehensive and rigorous evaluation coverage, JMMMU has become foundational benchmark for the development of Japanese LMMs [41, 42, 51]. major limitation of existing Japanese benchmarks is that the question image and the question text are provided to the model as separate modalities. This evaluation setup differs substantially from the core human cognitive skill: Seamlessly integrating visual and textual information and interpreting them through visual perception. Equipping LMMs with this cognitive ability in Japanese is crucial step toward developing embodied agents and robotic systems [64, 2, 15, 20] that can autonomously operate and explore real-world environments in Japan through visual perception. Furthermore, from the perspective of current LMMs use cases, users commonly provide LMMs with screenshots that include both Japanese text and images. Therefore, to foster core human cognitive skills and support wide range of real-world use cases, it is essential to evaluate LMMs on sufficiently complex tasks where both the question image and the question text are presented through visual modality. Among the English benchmarks, MMMU-Pro [59] extends MMMU by constructing benchmark in which both the question text and the question image are embedded within single image, thereby enabling the evaluation of this dimension. However, no benchmark in Japanese supports such evaluation. Therefore, developing Japanese benchmark that enables the evaluation of this dimension is essential. In this paper, we propose JMMMU-Pro, an image-based Japanese Multi-discipline Multimodal Understanding benchmark. JMMMU-Pro follows the evolution from MMMU to MMMU-Pro and is constructed by embedding each of the 1,320 question texts and question images from the original JMMMU tasks into single composite image. Built on top of the established JMMMU, it enables an apple-to-apple comparison between JMMMU-Pro and JMMMU, which provides meaningful signal of models visual cognitive abilities. Consequently, JMMMU-Pro offers both high usability and highly informative feedback for model developers. For the construction of JMMMU-Pro, we propose new benchmark creation methodology called Vibe Benchmark Construction. In this framework, an image generation model plays the primary role in producing the visual question, while humans simply check the outputs and, when necessary, refine the prompts before regenerating the images, thereby ensuring consistent quality. Previously, when creating image-based benchmarks (e.g., MMMU-Pro), all questions had to be created manually, which was not scalable and resulted in substantial human cost. In contrast, Vibe Benchmark Construction leverages Nano Banana Pro [11], state-of-the-art image generation model with exceptional photorealism. Nano Banana Pro can not only generate highly realistic images but also accurately embed Japanese text within them. By supplying both the question image and the question text and prompting the model to integrate them into single composite image, we can generate image questions with diverse set of backgrounds and layouts. Our Vibe Benchmark Construction approach offers several advantages over manual construction: (i) it is highly scalable, (ii) it requires minimal human effort, and (iii) it enables controllable generation of diverse layouts. In particular, given the rapid progress of image generation models, we expect Vibe Benchmark Construction to serve as an increasingly effective guideline for future benchmark development of image-based benchmarks. Through generation with Nano Banana Pro and subsequent human checking, approximately 95% of all the questions in JMMMU-Pro were generated by Nano Banana Pro. In our experiments, we evaluated total of 14 LMMs, including representative closed-source LMMs (i.e., GPT-5.2 [36] and Gemini3Pro [12]), English-centric open-source LMMs (e.g., LLaVAOneVision-1.5 [3]), multilingual open-source LMMs (e.g., Qwen3VL-8B [6] and AyaVision [10]), and Japanese open-source LMMs (e.g., Sarashina2.2-Vision-3B [41] and Heron-NVILA-Lite [51]). Our key experimental findings are summarized as follows: 1. Open-source LMMs struggle substantially on JMMMU-Pro. Even the best-performing open-source model scores below 50%, and many LMMs achieve performance close to random guessing. 2. When compared with JMMMU, most LMMs show drop in performance on JMMMU-Pro. In particular, open-source LMMs exhibit decrease ranging up to 23%. 3. Recent strong reasoning-based closed-source LMMs perform considerably well on JMMMUPro, revealing substantial and concerning gap between closed-source and open-source LMMs. 4. Through detailed analysis, we find that although major source of failure is the lack of Japanese OCR ability, strong OCR alone is not sufficient to solve JMMMU-Pro. This suggests that solving JMMMU-pro requires improving both OCR capability and the ability to interpret language and vision in an integrated manner through visual perception. Our contributions are summarized as follows: Construction of JMMMU-Pro: We extend JMMMU and introduce JMMMU-Pro, benchmark that embeds each question image and its corresponding text into single image, enabling the evaluation of integrated visual-textual understanding through visual perception. Proposal of Vibe Benchmark Construction: We propose Vibe Benchmark Construction, dataset creation framework in which powerful generation model drives the construction process, while humans only perform checking and minor prompt adjustments. With the continued progress of image generation models, we expect this approach to serve as an efficient and scalable guideline for future benchmark development. Encouraging Future Efforts in the Open-Source Community: Our results show that open-source LMMs struggle heavily on JMMMU-Pro, highlighting substantial gap with closed-source LMMs. JMMMU-Pro provides valuable benchmark that can motivate and guide the open-source community in closing this gap."
        },
        {
            "title": "2 Related Work",
            "content": "Large Multimodal Models (LMMs). Following the success of large language models (LLMs), many LMMs have been developed with improved knowledge and instruction-following capabilities [21, 22, 24, 18, 56, 61, 17, 31, 62]. With recent advances in multilingual LLMs [47, 9], both English-centric LMMs with multilingual capabilities [18, 3, 48] and fully multilingual LMMs [58, 10, 29] have emerged. In parallel, several LMMs specialized for Japanese have also been developed [42, 41, 4, 40, 51]. However, these models have not been evaluated on tasks that require solving Japanese questions that rely on integrated visual-textual understanding through visual perception. This highlights the need for dedicated benchmark that can systematically evaluate such integrated visual-textual understanding capabilities in Japanese. 3 LMM Benchmarks. Among various recent benchmarks [19, 21, 25, 27, 57, 30], MMMU [57] has become the most widely used benchmark for assessing progress in LMMs. MMMU requires advanced, university-level knowledge and reasoning across wide range of disciplines, enabling comprehensive and expert-level evaluation. More recently, MMMU-Pro [59] extends this evaluation paradigm by embedding both the question text and the question image into single image, challenging models to truly see and read simultaneously, mirroring how humans naturally process complex scenes in which text and visuals are interleaved. Unlike traditional OCR-related benchmarks [45, 26] or DocVQA [28], MMMU-Pro requires not only text recognition but also complex reasoning that integrates both visual and textual information, thereby pushing the capabilities of LMMs beyond standard document understanding. As result, MMMU-Pro has been widely adopted in the development of recent state-of-the-art LMMs [3, 37, 13, 1]. However, MMMU-Pro evaluates only English, leaving abilities in other languages unassessed. Therefore, developing JMMMU-Pro to evaluate the ability to understand images and text in unified manner in Japanese is an important next step. Japanese LMM Benchmarks. The development of Japanese LMM benchmarks remains behind that of English benchmarks. Many existing studies focus primarily on common-sense knowledge and do not adequately cover expert-level domains [44, 50, 49, 16, 39, 38], despite the rapid progress of LMMs and the importance of evaluating such capabilities. To address these issues, JMMMU [33] was introduced, significantly advancing the landscape of Japanese LMM evaluation. However, JMMMU does not include tasks that require models to interpret both text embedded within images, unlike MMMU-Pro. Benchmarks that include Japanese text within images, such as JDocQA [32] and MangaVQA [5], do exist, but they do not require the level of complex reasoning demanded by MMMU-Pro and are therefore insufficient for driving further advances in LMMs. To address this gap, we build upon JMMMU to create JMMMU-Pro, which evaluates models ability to jointly understand different modalities in more integrated manner and to perform high-level reasoning on such tasks. QA Construction with Generative Models. In the context of LLM-based and LMM-based QA construction, it is common for humans to manually edit model-generated QA pairs. As result, it is uncommon to rely solely on iterative prompt adjustments to construct QA data. While several works adopt iterative prompt adjustments in order to eliminate human-induced prompt bias [43, 14], these approaches differ fundamentally from our goal, in that our objective is to scalably produce high-quality QA data. In the context of image-generation models, recent works have leveraged powerful image generation models to create the images of VQA benchmarks [53, 54]. However, these approaches still require substantial additional effort to construct the question texts through manual creation or LMM-based question generation. The most similar work to our concept is LogicOCR [55], which uses GPT-Image-1 [35] to embed English question text into images with varied layouts. LogicOCR performs manual verification and discards portion of the generated samples. Although the approach faces limitations, such as reduced photorealism due to the capabilities of GPT-Image-1 and dataset shrinkage caused by sample filtering, it demonstrates promising direction of leveraging modern image generation models. We build upon this line of work and we formalize pipeline, Vibe Benchmark Construction, in which the process of generating VQA embedded in images is primarily driven by image generation models, while humans perform verification and, when necessary, adjust prompts and regenerate the images. By defining this pipeline clearly, we provide an effective guideline for the scalable construction of future image-based VQA benchmarks."
        },
        {
            "title": "3 JMMMU-Pro Benchmark",
            "content": "JMMMU-Pro consists of 1,320 questions whose contents are identical to those in the original JMMMU. Figure 2 illustrates the construction pipeline. We first describe the original JMMMU and then present the core component of our approach: Vibe Benchmark Construction. 3.1 Revisiting the JMMMU Benchmark JMMMU [33] consists of 1,320 questions and 1,118 images spanning 28 subjects. The benchmark is intentionally divided into two categories: culture-agnostic and culture-specific subjects. The culture-agnostic subset includes 24 subjects with 720 questions, across five disciplines: (1) Art & Psychology, (2) Business, (3) Health & Medicine, (4) Science, and (5) Tech & Engineering. The 4 Figure 2: JMMMU-Pro Construction Pipeline. (a) Background distribution (b) Color distribution Figure 3: Distribution of background and its color in the JMMMU-Pro benchmark. culture-specific subset comprises 600 questions across four subjects: (1) Japanese Art, (2) Japanese Heritage, (3) Japanese History, and (4) World History. To simplify the interpretation of JMMMU evaluation results, we converted all 50 open-ended questions into multiple-choice questions and revised two samples. We refer to this updated version as JMMMU-verified-2025-12 (see details in Section B). All JMMMU scores reported in this paper are based on JMMMU-verified-2025-12. 3.2 Definition of Vibe Benchmark Construction Vibe Benchmark Construction is methodology in which an image generation model plays the primary role in producing the VQA problem images, while humans only verify the outputs and adjust the prompts when necessary to ensure quality. Although previous VQA benchmarks have used synthetic images generated by image generation models, these models have played only supplementary role, producing just the visual part while the question text still had to be created separately by humans or by an LMM, incurring additional cost. In contrast, the key distinction of our proposed Vibe Benchmark Construction is that the VQA creation process is carried out by the image generation model, with humans intervening solely for verification and prompt refinement. This paradigm is particularly effective for image-based VQA, where humans cannot easily edit content directly inside the image in the same way as text-based QA. By letting the model handle generation and restricting human effort to adjusting the prompt until satisfactory image is produced, the method enables efficient and scalable construction of benchmarks, especially in domains like image-based VQA, where dataset creation is difficult. more detailed comparison with existing work is provided in Section 2. 5 Figure 4: JMMMU-Pro samples. 3.3 Detailed Pipeline of Vibe Benchmark Construction image generation, For interface use Nano Banana (gemini-3-pro-image-preview). The image resolution is set to 1K. Below, we describe our prompt design process and the workflow for human checking and regeneration. its API Pro via we Prompt Selection and Image Generation. We first selected prompt template through preliminary experiments. Specifically, we used the prompt template in Section and varied the following six components as parameters to generate diverse set of images (shown in Figure 4): 1. Background: chosen from workbook, exam sheet, whiteboard, blackboard, projector, iPad notebook, webpage, Nintendo Switch, and TV quiz show. 2. Background Color: selected from white, light green, light yellow, light pink, light gray, and light blue. Note that certain backgrounds have fixed color (e.g., whiteboard is always white), and we account for such constraints. 3. Font: chosen from handwritten text, computer font, thick computer font, thin computer font, and manga-style computer font. 4. Margin: selected as either small or large. 5. State: chosen from photo by smartphone, screenshot by PC, and screenshot by smartphone. 6. Aspect Ratio: selected from 9:16, 16:9, 3:4, and 1:1. We show the statistics for the two most controllable factors: Background and Background Color in Figure 3. In addition, while JMMMU includes image tags such as <image 1> within the question text, we found that Nano Banana Pro does not allow control over these image tags. To address this issue, we attempted to include explicit instructions in the prompt (e.g., keep the image tag in the question). However, such instructions significantly degraded the quality of the generated images. We hypothesize that this occurs because Nano Banana Pro internally uses similar image-tag tokens, and explicit instructions about them may interfere with its generation process. Therefore, we intentionally avoid giving any special instructions regarding image tags in the prompt. Human Checking and Regeneration. We performed author reviews of the generated images with custom-built annotation tool. In these reviews, we checked that the generated text and images"
        },
        {
            "title": "Model",
            "content": "JMMMU-Pro (1320) JMMMU CS Pro (600) (1320) CS (600) CA Pro (720) CA (720) Random"
        },
        {
            "title": "Random Choice\nFrequent Choice",
            "content": "Multilingual Open LMMs Qwen3-VL-8B Qwen2.5-VL-7B Phi-4-multimodal Aya-Vision-8B Pangea-7B English-centric Open LMMs LLaVA-OV-1.5-8B LLaVA-OV-7B InternVL2.5-8B Japanese Open LMMs Sarashina2.2-V-3B Sarashina2-V-14B Sarashina2-V-8B Heron-NVILA-Lite-15B Closed LMMs Gemini3Pro (reasoning high) GPT-5.2 (reasoning high) 27.05 27.73 47.27 45.00 31.82 26.74 23.41 31.97 27.35 31.21 42.88 30.68 27.88 26.97 87.04 83. 27.05 27.73 52.88 47.65 39.55 37.73 37.50 51.74 41.14 41.36 47.95 37.27 39.62 50.15 89.77 84.47 26.33 25. 26.33 25.33 27.64 29.72 27.64 29.72 47.50 46.67 28.83 27.00 21.67 28.00 26.50 29.00 54.00 32.33 27.00 26. 55.83 54.00 38.00 40.33 47.17 53.33 43.83 43.33 61.50 43.17 51.00 59.17 47.08 43.61 34.31 26.53 24.86 35.28 28.06 33.06 33.61 29.31 28.61 27. 50.42 42.36 40.83 35.56 29.44 50.42 38.89 39.72 36.67 32.36 30.14 42.64 95.00 88.33 95.00 85.50 80.42 79. 85.42 83.61 Table 1: Main Results on JMMMU-Pro and JMMMU. Overall, most open-source LMMs show substantial performance degradation on JMMMU-Pro compared to JMMMU, while closed-source LMMs maintain strong performance, highlighting significant gap in integrated Japanese visualtextual understanding. matched the originals exactly. As mentioned above, controlling image tags in the question text is difficult, so we allowed variations in the tags as long as the generated item remained valid question. In the first review round, 71% of the questions passed. The remaining 29% failed primarily because the question image had been replaced with an unrelated image, the text within the image was unreadable, parts of the question text were missing or incorrect, or the generated image was visually unnatural. These examples are shown in Figure A. For these failed cases, we regenerated the images with the same prompt or prompts with minor prompt adjustments. After completing the full set of VQA questions across several rounds, we performed final crosscheck to eliminate inconsistencies in the authors evaluation standards. Manual Construction. We manually created 67 samples that Nano Banana Pro had difficulty generating. These cases had the following characteristics: 1. long question text (16 samples), 2. small or difficult-to-render text within the question image (36 samples), 3. extreme aspect ratios (2 samples), 4. domains that are inherently difficult to generate, such as chemical formulas or musical notation (8 samples), and 5. cases rejected by Nano Banana Pro due to policy constraints (5 samples). These examples are shown in Figure B."
        },
        {
            "title": "4 Experiments",
            "content": "4.1 Setup Baseline LMMs. For more comprehensive evaluation, we assess diverse set of state-of-the-art LMMs. In particular, for open-source models, we select representative models from three categories, English-centric LMMs, multilingual LMMs, and Japanese LMMs, to ensure that our evaluation accurately captures current progress in each subfield. We mainly use LMMs-Eval [60] for our experiments. We set the temperature to 0 to open-source LMMs (the default setting for closed7 source LMMs), and set max_tokens to be configured to be long enough so that the response would not be cut off. Closed-source LMMs: GPT-5.2 [36], Gemini3Pro [12]. English-Centric Open-source LMMs: LLaVA-OneVision-7B [18], LLaVA-OneVision-1.5-8B [3], InternVL2.5-8B [8], InternVL3-14B [63], Multilingual Open-source LMMs: Qwen2.5VL-7B [7], Qwen3VL-8B [6], Phi-4Multimodal [29], Pangea-7B [58], Aya Vision-8B [10] Japanese Open-source LMMs: Sarashina2-Vision-8B and 14B [42], Sarashina2.2-Vision-3B [41], Heron-NVILA-Lite-15B [51] Inference Prompt. The inference prompt is based on the setup in JMMMU [33] and MMMUPro [59]. Following MMMU-Pro [59], we evaluate the open-source LMMs with both Direct and CoT prompts (as shown in Section C), and report the higher ones in the overall results. For the closed-source LMMs, they perform reasoning regardless of the prompt types, so we evaluated them using only the Direct Prompt. Full results are shown in Section D."
        },
        {
            "title": "4.2 Main Results",
            "content": "We present the experimental results in Table 1. The key findings from these results are as follows. F1. All open-source LMMs struggle significantly on JMMMU-Pro. As shown in Table 1, opensource LMMs perform poorly on JMMMU-Pro, with the best model, Qwen3-VL-8B, achieving only 45.83, indicating substantial room for improvement. Furthermore, nine models perform less than 32%, close to random guessing. These results highlight that JMMMU-Pro poses challenging and valuable benchmark for evaluating and advancing open-source LMMs. F2. Most open-source LMMs exhibit significant performance drop compared to JMMMU. As shown in Table 1, most open-source LMMs, except for Qwen2.5-VL-7B, show substantial decline in accuracy on JMMMU-Pro relative to JMMMU. Moreover, when we compare the CS and CA subsets, we find that models with clear performance gap between the JMMMUs two subsets are similarly low on both in JMMMU-Pro. This suggests that their weakness lies in fundamental lack of vision-side understanding, rather than in the type of question. These results demonstrate that JMMMU-Pro provides valuable feedback to model developers when used in comparison with JMMMU. F3. Closed-source LMMs achieve substantially higher performance on JMMMU-Pro, revealing serious gap relative to open-source models. As shown in Table 1, closed-source LMMs obtain notably high scores on JMMMU-Pro. This indicates that these models already possess the ability to seamlessly integrate visual and textual information and interpret them through visual perception. Importantly, the strong performance of closed-source models does not diminish the value of JMMMUPro. Instead, it highlights the crucial role of JMMMU-Pro as benchmark for guiding the development of open-source LMMs. Given the considerable performance gap between closed-source and opensource LMMs, reducing this gap is an essential goal for the community."
        },
        {
            "title": "5 Analysis",
            "content": "5.1 Impact of CoT Prompting We examine the effectiveness of Chain-of-Thought (CoT) prompting on JMMMU-Pro and JMMMU. The results are shown in Figure 5. These results indicate that the effectiveness of CoT varies depending on the model and the evaluation setting for both JMMMU-Pro and JMMMU. For example, on JMMMU-Pro, 7 out of the 12 LMMs achieve higher performance with CoT prompting, whereas on JMMMU, only 3 models benefit from CoT. Moreover, when examined on per-model basis, LMMs such as Pangea-7B, LLaVA-OV-1.5-8B, InternVL2.5-8B, and Sarashina2.2-V-3B show different prompt preferences between JMMMU and JMMMU-Pro. These findings suggest that optimal prompting strategies must be tailored to each model and each task, rather than relying on single prompting approach across settings. 8 Figure 5: Impact of CoT prompting in JMMMU and JMMMU-Pro. 5.2 Correlation with OCR Performance We hypothesize that the primary cause of performance degradation on JMMMU-Pro is the inability of current LMMs to perform Japanese Optical Character Recognition (OCR). To examine this hypothesis, we compute the correlation between OCR performance and JMMMU-Pro accuracy across several LMMs. Following the evaluation setting of MMMU-Pro, we ask each LMM to extract the full text of the question and all answer choices, excluding any text from associated images. OCR accuracy is then calculated by comparing the extracted text with the original text using the Levenshtein distance, which measures the edit distance between two strings. The similarity between the extracted and original text is computed as: OCR Accuracy = 1 Levenshtein(text1, text2) max(len(text1), len(text2)) (1) Figure 6: Correlation between OCR accuracy and JMMMU-Pro performance. The results are shown in Figure 6. The correlation coefficient between OCR accuracy and JMMMU-Pro accuracy is 0.593. As illustrated in the figure, there is indeed positive correlation between the two. However, high OCR ability does not necessarily translate directly into high JMMMU-Pro accuracy. For example, while Heron-NVILA and Sarashina2.2-V are comparable for OCR performance, the performance for JMMMU-Pro differs lot. This indicates that solving JMMMU-Pro demands not only strong OCR capabilities, but also the ability to interpret and reason over language and vision in an integrated manner through visual perception. 5.3 Failure Examples We present representative failure cases of the state-of-the-art open-source LMM, Qwen3-VL-8B, in Figure and Figure D. These examples demonstrate that the model makes both reasoning and perceptual errors specifically on JMMMU-Pro, suggesting that JMMMU-Pro demands deeper, more integrated visual-textual understanding that goes beyond simple OCR capabilities."
        },
        {
            "title": "6 Conclusion",
            "content": "This paper introduces JMMMU-Pro, an image-based Japanese Multi-discipline Multimodal Understanding Benchmark, along with Vibe Benchmark Construction, scalable methodology for creating such datasets. Our experiments show that all open-source LMMs face significant difficulty 9 on JMMMU-Pro, highlighting its importance as benchmark that can inspire future progress in the open-source community. We believe that JMMMU-Pro enables more rigorous evaluation of Japanese multimodal capabilities, and that Vibe Benchmark Construction provides an effective framework for the scalable development of future image-based VQA benchmarks."
        },
        {
            "title": "Acknowledgment",
            "content": "This work was partially supported by JSPS 25H01164."
        },
        {
            "title": "References",
            "content": "[1] Abdelrahman Abouelenin, Atabak Ashfaq, Adam Atkinson, Hany Awadalla, Nguyen Bach, Jianmin Bao, Alon Benhaim, Martin Cai, Vishrav Chaudhary, Congcong Chen, et al. Phi-4-mini technical report: Compact yet powerful multimodal language models via mixture-of-loras. arXiv preprint arXiv:2503.01743, 2025. [2] Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn, Chuyuan Fu, Keerthana Gopalakrishnan, Karol Hausman, et al. Do as can, not as say: Grounding language in robotic affordances. In CoRL, 2022. [3] Xiang An, Yin Xie, Kaicheng Yang, Wenkang Zhang, Xiuwei Zhao, Zheng Cheng, Yirui Wang, Songcen Xu, Changrui Chen, Chunsheng Wu, et al. Llava-onevision-1.5: Fully open framework for democratized multimodal training. arXiv preprint arXiv:2509.23661, 2025. [4] Jeonghun Baek, Akiko Aizawa, and Kiyoharu Aizawa. Harnessing pdf data for improving japanese large multimodal models. In ACL Findings, 2025. [5] Jeonghun Baek, Kazuki Egashira, Shota Onohara, Atsuyuki Miyai, Yuki Imajuku, Hikaru Ikuta, and Kiyoharu Aizawa. Mangavqa and mangalmm: benchmark and specialized model for multimodal manga understanding. arXiv preprint arXiv:2505.20298, 2025. [6] Shuai Bai, Yuxuan Cai, Ruizhe Chen, Keqin Chen, Xionghui Chen, Zesen Cheng, Lianghao Deng, Wei Ding, Chang Gao, Chunjiang Ge, Wenbin Ge, Zhifang Guo, Qidong Huang, Jie Huang, Fei Huang, Binyuan Hui, Shutong Jiang, Zhaohai Li, Mingsheng Li, Mei Li, Kaixin Li, Zicheng Lin, Junyang Lin, Xuejing Liu, Jiawei Liu, Chenglong Liu, Yang Liu, Dayiheng Liu, Shixuan Liu, Dunjie Lu, Ruilin Luo, Chenxu Lv, Rui Men, Li Ying Meng, Xuancheng Ren, Xin yi Ren, Sibo Song, Yu-Chen Sun, Jun Tang, Jianhong Tu, Jianqiang Wan, Peng Wang, Pengfei Wang, Qiuyue Wang, Yuxuan Wang, Tianbao Xie, Yihe Xu, Haiyang Xu, Jin Xu, Zhibo Yang, Mingkun Yang, Jian-Xing Yang, An Yang, Bowen Yu, Fei Zhang, Hang Zhang, Xi Zhang, Bo Zheng, Humen Zhong, Jingren Zhou, Fan Zhou, Jingren Zhou, Yuanzhi Zhu, and Ke Zhu. Qwen3-vl technical report. arXiv preprint arXiv:2511.21631, 2025. [7] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. [8] Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, et al. Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling. arXiv preprint arXiv:2412.05271, 2024. [9] CohereLabs. c4ai-command-r7b-12-2024, 2024. CohereLabs/c4ai-command-r7b-12-2024. Accessed: 2025-11-29. URL https://huggingface.co/ [10] Saurabh Dash, Yiyang Nan, John Dang, Arash Ahmadian, Shivalika Singh, Madeline Smith, Bharat Venkitesh, Vlad Shmyhlo, Viraat Aryabumi, Walter Beller-Morales, et al. Aya vision: Advancing the frontier of multilingual multimodality. arXiv preprint arXiv:2505.08751, 2025. [11] Google DeepMind. Gemini 3 pro image (nano banana pro). Web page, 2025. URL https: //deepmind.google/models/gemini-image/pro/. Accessed: 2025-11-29. 10 [12] Google DeepMind. Gemini3. Web page, 2025. URL https://deepmind.google/models/ gemini/. Accessed: 2025-12-10. [13] Google DeepMind. Gemini. Web page, 2025. URL https://deepmind.google/models/ gemini/. Accessed: 2025-11-29. [14] Zeyu He, Saniya Naphade, and Ting-Hao Kenneth Huang. Prompting in the dark: Assessing human performance in prompt engineering for data labeling when gold labels are absent. In CHI, 2025. [15] Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch. Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. In ICML, 2022. [16] Yuichi Inoue, Kento Sasaki, Yuma Ochi, Kazuki Fujii, Kotaro Tanahashi, and Yu Yamaguchi. Heron-bench: benchmark for evaluating vision language models in japanese. In CVPR workshop, 2024. [17] Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Jingkang Yang, and Ziwei Liu. Otter: multi-modal model with in-context instruction tuning. arXiv preprint arXiv:2305.03726, 2023. [18] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. Llava-onevision: Easy visual task transfer. TMLR, 2024. [19] Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge, and Ying Shan. Seed-bench: Benchmarking multimodal llms with generative comprehension. In CVPR, 2024. [20] Manling Li, Shiyu Zhao, Qineng Wang, Kangrui Wang, Yu Zhou, Sanjana Srivastava, Cem Gokmen, Tony Lee, Li Erran Li, Ruohan Zhang, Weiyu Liu, Percy Liang, Li Fei-Fei, Jiayuan Mao, and Jiajun Wu. Embodied agent interface: Benchmarking llms for embodied decision making. In NeurIPS, 2024. [21] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In NeurIPS, 2023. [22] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In CVPR, 2024. [23] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In CVPR, 2024. [24] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Improved reasoning, ocr, and world knowledge, January 2024. URL https: //llava-vl.github.io/blog/2024-01-30-llava-next/. Accessed: 2025-11-29. [25] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an all-around player? In ECCV, 2024. [26] Yuliang Liu, Zhang Li, Mingxin Huang, Biao Yang, Wenwen Yu, Chunyuan Li, Xu-Cheng Yin, Cheng-Lin Liu, Lianwen Jin, and Xiang Bai. Ocrbench: on the hidden mystery of ocr in large multimodal models. Science China Information Sciences, 67(12):220102, 2024. [27] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. In ICLR, 2024. [28] Minesh Mathew, Dimosthenis Karatzas, and C.V. Jawahar. Docvqa: dataset for vqa on document images. In WACV, 2021. [29] Microsoft. Phi-4-multimodal-instruct. Hugging Face Model Repository, 2025. URL https: //huggingface.co/microsoft/Phi-4-multimodal-instruct. Accessed: 2025-11-29. [30] Atsuyuki Miyai, Jingkang Yang, Jingyang Zhang, Yifei Ming, Qing Yu, Go Irie, Yixuan Li, Hai Helen Li, Ziwei Liu, and Kiyoharu Aizawa. Unsolvable problem detection: Robust understanding evaluation for large multimodal models. In ACL, 2025. 11 [31] Masoud Monajatipoor, Liunian Harold Li, Mozhdeh Rouhsedaghat, Lin Yang, and Kai-Wei Chang. Metavl: Transferring in-context learning ability from language models to visionlanguage models. In ACL, 2023. [32] Eri Onami, Shuhei Kurita, Taiki Miyanishi, and Taro Watanabe. Jdocqa: Japanese document question answering dataset for generative language models. In LREC-COLING, 2024. [33] Shota Onohara, Atsuyuki Miyai, Yuki Imajuku, Kazuki Egashira, Jeonghun Baek, Xiang Yue, Graham Neubig, and Kiyoharu Aizawa. Jmmmu: japanese massive multi-discipline multimodal understanding benchmark for culture-aware evaluation. In NAACL, 2025. [34] OpenAI. Gpt-4o, 2024. [35] OpenAI. Image generation api, image-generation-api/. Accessed: 2025-02-06. 2024. URL https://openai.com/index/ [36] OpenAI. Introducing gpt-5.2. Web page, 2025. URL https://openai.com/index/ introducing-gpt-5-2/. Accessed: 2025-12-13. [37] OpenAI. Introducing gpt-5. Web page, 2025. URL https://openai.com/index/ introducing-gpt-5/. Accessed: 2025-11-29. [38] SakanaAI. Ja-multi-image-vqa, 2024. URL https://huggingface.co/datasets/ SakanaAI/JA-Multi-Image-VQA. Accessed: 2025-11-29. [39] SakanaAI. Ja-vlm-bench-in-the-wild, 2024. URL https://huggingface.co/datasets/ SakanaAI/JA-VLM-Bench-In-the-Wild. Accessed: 2025-11-29. [40] Keito Sasagawa, Koki Maeda, Issa Sugiura, Shuhei Kurita, Naoaki Okazaki, and Daisuke Kawahara. Constructing multimodal datasets from scratch for rapid development of Japanese visual language model. In NAACL: Human Language Technologies (System Demonstrations), 2025. [41] SB Intuitions. Sarashina2.2-vision-3b, 2025. sbintuitions/sarashina2.2-vision-3b. Accessed: 2025-11-29. URL https://huggingface.co/ [42] SB Intuitions. Sarashina2-vision-14b, 2025. URL https://huggingface.co/ sbintuitions/sarashina2-vision-14b. Accessed: 2025-11-29. [43] Chirag Shah. From prompt engineering to prompt science with human in the loop. arXiv preprint arXiv:2401.04122, 2024. [44] Nobuyuki Shimizu, Na Rong, and Takashi Miyazaki. Visual question answering dataset for bilingual image understanding: study of cross-lingual transfer using attention maps. In COLING, 2018. [45] Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and Marcus Rohrbach. Towards vqa models that can read. In CVPR, 2019. [46] NAVER Cloud HyperCLOVA Team. Hyperclova think technical report. arXiv preprint arXiv:2506.22403, 2025. [47] Qwen Team et al. Qwen2 technical report. arXiv preprint arXiv:2407.10671, 2(3), 2024. [48] Peter Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Adithya Jairam Vedagiri IYER, Sai Charitha Akula, Shusheng Yang, Jihan Yang, Manoj Middepogu, Ziteng Wang, et al. Cambrian-1: fully open, vision-centric exploration of multimodal llms. In NeurIPS, 2024. [49] Turing. Llava-bench-in-the-wild (japanese), 2024. URL https://github.com/ turingmotors/heron/tree/main/playground/data/llava-bench-in-the-wild. Accessed: 2025-11-29. [50] Turing. Llava-bench-ja, 2024. URL https://github.com/turingmotors/heron/tree/ main/playground/data/llava-bench-ja. Accessed: 2025-11-29. 12 [51] Turing Inc. Heron-nvila-lite-15b, 2025. URL https://huggingface.co/turing-motors/ Heron-NVILA-Lite-15B. Accessed: 2025-11-29. [52] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. [53] Sibo Wang, Xiangkui Cao, Jie Zhang, Zheng Yuan, Shiguang Shan, Xilin Chen, and Wen Gao. Vlbiasbench: comprehensive benchmark for evaluating bias in large vision-language model. arXiv preprint arXiv:2406.14194, 2024. [54] Yisong Xiao, Xianglong Liu, QianJia Cheng, Zhenfei Yin, Siyuan Liang, Jiapeng Li, Jing Shao, Aishan Liu, and Dacheng Tao. Genderbias-vl: Benchmarking gender bias in vision language models via counterfactual probing: Y. xiao et al. IJCV, pages 124, 2025. [55] Maoyuan Ye, Jing Zhang, Juhua Liu, Bo Du, and Dacheng Tao. Logicocr: Do your large multimodal models excel at logical reasoning on text-rich images? arXiv preprint arXiv:2505.12307, 2025. [56] Qinghao Ye, Haiyang Xu, Jiabo Ye, Ming Yan, Anwen Hu, Haowei Liu, Qi Qian, Ji Zhang, and Fei Huang. mplug-owl2: Revolutionizing multi-modal large language model with modality collaboration. In CVPR, 2024. [57] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In CVPR, 2024. [58] Xiang Yue, Yueqi Song, Akari Asai, Seungone Kim, Jean de Dieu Nyandwi, Simran Khanuja, Anjali Kantharuban, Lintang Sutawika, Sathyanarayanan Ramamoorthy, and Graham Neubig. Pangea: fully open multilingual multimodal llm for 39 languages. In ICLR, 2025. [59] Xiang Yue, Tianyu Zheng, Yuansheng Ni, Yubo Wang, Kai Zhang, Shengbang Tong, Yuxuan Sun, Botao Yu, Ge Zhang, Huan Sun, Yu Su, Wenhu Chen, and Graham Neubig. Mmmu-pro: more robust multi-discipline multimodal understanding benchmark. In ACL, 2025. [60] Kaichen Zhang, Bo Li, Peiyuan Zhang, Fanyi Pu, Joshua Adrian Cahyono, Kairui Hu, Shuai Liu, Yuanhan Zhang, Jingkang Yang, Chunyuan Li, et al. Lmms-eval: Reality check on the evaluation of large multimodal models. In NAACL Findings, 2025. [61] Bo Zhao, Boya Wu, and Tiejun Huang. Svit: Scaling up visual instruction tuning. arXiv preprint arXiv:2307.04087, 2023. [62] Haozhe Zhao, Zefan Cai, Shuzheng Si, Xiaojian Ma, Kaikai An, Liang Chen, Zixuan Liu, Sheng Wang, Wenjuan Han, and Baobao Chang. Mmicl: Empowering vision-language model with multi-modal in-context learning. In ICLR, 2024. [63] Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Hao Tian, Yuchen Duan, Weijie Su, Jie Shao, et al. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models. arXiv preprint arXiv:2504.10479, 2025. [64] Brianna Zitkovich, Tianhe Yu, Sichun Xu, Peng Xu, Ted Xiao, Fei Xia, Jialin Wu, Paul Wohlhart, Stefan Welker, Ayzaan Wahid, Quan Vuong, Vincent Vanhoucke, Huong Tran, Radu Soricut, Anikait Singh, Jaspiar Singh, Pierre Sermanet, Pannag R. Sanketi, Grecia Salazar, Michael S. Ryoo, Krista Reymann, Kanishka Rao, Karl Pertsch, Igor Mordatch, Henryk Michalewski, Yao Lu, Sergey Levine, Lisa Lee, Tsang-Wei Edward Lee, Isabel Leal, Yuheng Kuang, Dmitry Kalashnikov, Ryan Julian, Nikhil J. Joshi, Alex Irpan, Brian Ichter, Jasmine Hsu, Alexander Herzog, Karol Hausman, Keerthana Gopalakrishnan, Chuyuan Fu, Pete Florence, Chelsea Finn, Kumar Avinava Dubey, Danny Driess, Tianli Ding, Krzysztof Marcin Choromanski, Xi Chen, Yevgen Chebotar, Justice Carbajal, Noah Brown, Anthony Brohan, Montserrat Gonzalez Arenas, and Kehang Han. Rt-2: Vision-language-action models transfer web knowledge to robotic control. In CoRL, 2023."
        },
        {
            "title": "Appendix",
            "content": "In this appendix, we describe failure cases of image generation in Section A, the verified JMMMU in Section B, detailed prompt examples in Section C, the full results in Section D, and failure cases of LMM inference in Section E."
        },
        {
            "title": "A Image Generation Failures and Manual Construction Examples",
            "content": "A.1 Image Generation Failures in Nano Banana Pro Figure presents examples of image generation failures observed in Nano Banana Pro. Nano Banana Pro can occasionally produce failures such as those shown in the figure. We attribute these failures to the inherent diversity of outputs produced by generative models. Therefore, it is crucial to manually inspect and filter such outputs to ensure correctness. Figure A: Failure examples for Nano Banana Pro. A.2 Manual Construction Examples Figure shows examples that were manually constructed. Images with these characteristics were inherently difficult to generate automatically using Nano Banana Pro. Therefore, we found that not all images can be created through the Vibe Benchmark Construction pipeline."
        },
        {
            "title": "B Verifying the original JMMMU Benchmark",
            "content": "We first verify the existing JMMMU benchmark by correcting its samples and refining its evaluation protocol. Sample Corrections. Although most questions in JMMMU are multiple-choice, 50 questions in the culture-agnostic subset, which translated directly from MMMU, are open-ended. Prior work [41] has noted that these open-ended questions introduce additional complexity in interpreting evaluation results, and thus converted them into multiple-choice format for their experiments. Following this approach, we also convert all open-ended questions in JMMMU into multiple-choice questions. We provide the problem statement and the correct answer to an LLM (e.g., GPT-5 [37]) and instruct it to propose plausible false choices. We then manually verify the generated choices to ensure quality, correcting cases where false options might unintentionally match the correct answer due to numerical precision, ambiguity, or formatting issues. Additionally, we identified and corrected errors in two samples (test_Japanese_Art_120, validation_Agriculture_1), where the answer or question text contained mistakes. Revision of the Evaluation Procedure. We also revised the evaluation procedure in JMMMU. First, we found that the answer-parsing algorithm used in JMMMU often fails on recent reasoning models, whose outputs tend to be longer. In particular, when models list all options as part of their reasoning, the parser may incorrectly extract the predicted choice. To address this, we modify the parser to 14 Figure B: Manual construction examples. ignore such auxiliary option listings. In addition, when parsing fails, JMMMU selects an answer at random. We remove this random selection step, as it obscures whether model has genuinely failed to produce valid option. Our modification instead marks such cases explicitly as incorrect, allowing clearer distinction between invalid output and an actual wrong answer. To avoid confusion in the community, we name this corrected version of the benchmark JMMMU-verified-2025-12. All JMMMU scores reported in this paper are based on JMMMU-verified-2025-12."
        },
        {
            "title": "C Prompt",
            "content": "C.1 Image Generation Prompt Below, we present the prompts used to generate images with Nano Banana Pro. The base prompt is designed for the single-image setting, while for the multiple-image setting, the prompt is modified depending on whether images are included in the options."
        },
        {
            "title": "Base prompt",
            "content": "Your role is to create an image-based question. You must not derive or provide the answer. Please create an image that looks as if the pictures, text, and options were actually placed or written on surface, such as notebook, sheet of paper, webpage, or other backgrounds, and then captured either by being photographed with phone camera or taken as smartphone/PC screenshot. **Critical Instruction** 1. Insert the images exactly as they are. 2. Do not change the character of any text in the Image or the Question. Keep the original character exactly as it appears. 3. Do not derive the answer. Your task is to create an image-based question. 4. Make sure the pasted images blend naturally into the notebook background while still retaining subtle \"pasted\" feel. For cases such as blackboard or notebook, keep slight pasted effect. For things like webpage, projector screen, or printed material, make the pasted images blend in more naturally without an obvious pasted look, while still keeping the boundary between the Question and the Image clearly distinguishable. 5. Please make the font of the text within the images follow the original font in the given image as closely as possible. The font for the question and option text should follow the instructions provided below. 6. Paste the image without making any edits. **More Detailed Conditions** 1. The image should reflect {state}. 2. The font used in the question and options should be {font}. 3. The background should be {background}. 4. The background color should be {color}. 5. The marginal space should be {margin}. Image: As attached Question: {question} {options} 16 Prompt for multiple images (wo option images) Your role is to create an image-based question. You must not derive or provide the answer. Please create an image that looks as if the pictures, text, and options were actually placed or written on surface, such as notebook, sheet of paper, webpage, or other backgrounds, and then captured either by being photographed with phone camera or taken as smartphone/PC screenshot. **Critical Instruction** 1. Insert the images exactly as they are. 2. Do not change the character of any text in the Image or the Question. Keep the original character exactly as it appears. 3. Do not derive the answer. Your task is to create an image-based question. 4. Make sure the pasted images blend naturally into the notebook background while still retaining subtle \"pasted\" feel. For cases such as blackboard or notebook, keep slight pasted effect. For things like webpage, projector screen, or printed material, make the pasted images blend in more naturally without an obvious pasted look, while still keeping the boundary between the Question and the Image clearly distinguishable. 5. Please make the font of the text within the images follow the original font in the given image as closely as possible. The font for the question and option text should follow the instructions provided below. 6. Paste the image without making any edits. 7. When there are multiple images, place them from left to right, one by one. **More Detailed Conditions** 1. The image should reflect {state}. 2. The font used in the question and options should be {font}. 3. The background should be {background}. 4. The background color should be {color}. 5. The marginal space should be {margin}. Image: As attached Question: {question} {options}"
        },
        {
            "title": "Prompt for image options",
            "content": "Your role is to create an image-based question. You must not derive or provide the answer. Please create an image that looks as if the pictures, text, and options were actually placed or written on surface, such as notebook, sheet of paper, webpage, or other backgrounds, and then captured either by being photographed with phone camera or taken as smartphone/PC screenshot. **Critical Instruction** 1. Insert the images exactly as they are. 2. Do not change the character of any text in the Image or the Question. Keep the original character exactly as it appears. 3. Do not derive the answer. Your task is to create an image-based question. 4. Make sure the pasted images blend naturally into the notebook background while still retaining subtle pasted feel. For cases such as blackboard or notebook, keep slight pasted effect. For things like webpage, projector screen, or printed material, make the pasted images blend in more naturally without an obvious pasted look, while still keeping the boundary between the Question and the Image clearly distinguishable. 5. Please make the font of the text within the images follow the original font in the given image as closely as possible. The font for the question and option text should follow the instructions provided below. 6. Paste the image without making any edits. 7. For the <image> tags in options (e.g., <image 1>, <image 2>), please replace each tag with the actual image. The images correspond to the tags in order, with the first image being <image 1>, the second <image 2>, and so on. **More Detailed Conditions** 1. The image should reflect {state}. 2. The font used in the question should be {font}. 3. The background should be {background}. 4. The background color should be {color}. 5. The marginal space should be {margin}. Image: As attached Question: {question} {options} C.2 LMM Inference Prompt. Below, we present the prompt used for LMM inference. Inference Prompts: Direct JMMMU:  (Answer with the options letter from the given choices directly. JMMMU-Pro:    (Your role is to solve the question provided in the image. Answer with the options letter from the given choices directly.) 18 Inference Prompts: CoT JMMMU:    : $LETTER LETTER   step by step  (Answer the following multiple choice question. The last line of your response should be of the following format: Answer: $LETTER (without quotes) where LETTER is one of options. Think step by step before answering.) ( JMMMU-Pro:   : $LETTER  LETTER    step by step  (Your role is to solve the question provided in the image. The last line of your response should be of the following format: Answer: $LETTER (without quotes) where LETTER is one of options. Think step by step before answering.) Inference Prompts: OCR Task OCR Task Prompt:      OCR   Format, Question: , Options: ( ) (Extract and output the full text of the question, including any introductory descriptions, as well as the corresponding answer choices from the multiple-choice question in the image. Exclude any text from associated images or the question number. Perform OCR only; do not attempt to solve the question. Please output in the format: Question: , Options: (without quotes).)"
        },
        {
            "title": "D Full Results",
            "content": "In Table and Table B, we present detailed results for both the Direct Prompt and the CoT Prompt."
        },
        {
            "title": "E Failure Cases",
            "content": "We show representative failure cases of the state-of-the-art open-source LMM, Qwen3-VL-8B, in Figure and Figure D. These examples show that the model exhibits both reasoning errors and perceptual errors only in JMMMU-Pro, indicating that JMMMU-Pro requires deeper integrated visualtextual understanding beyond mere OCR capability."
        },
        {
            "title": "Model",
            "content": "JMMMU-Pro (1320) JMMMU CS Pro (600) (1320) CS (600) CA Pro (720) CA (720) Random"
        },
        {
            "title": "Random Choice\nFrequent Choice",
            "content": "Multilingual Open LMMs Qwen3-VL-8B Qwen2.5-VL-7B Phi-4-multimodal Aya-Vision-8B Pangea-7B English-centric Open LMMs LLaVA-OV-1.5-8B LLaVA-OV-7B InternVL2.5-8B Japanese Open LMMs Sarashina2.2-V-3B Sarashina2-V-14B Sarashina2-V-8B Heron-NVILA-Lite-15B Closed LMMs Gemini3Pro (reasoning high) GPT-5.2 (reasoning high) 27.05 27.73 45.83 44.70 31.82 22.42 19.55 29.92 27.35 25.08 38.03 30.68 27.88 26.97 87.04 83. 27.05 27.73 46.82 46.82 39.55 32.05 37.50 51.74 41.14 41.36 47.95 37.27 39.62 50.15 89.77 84.47 26.33 25. 26.33 25.33 27.64 29.72 27.64 29.72 47.00 50.17 28.83 23.83 23.00 26.33 26.50 23.83 40.17 32.33 27.00 26. 56.33 57.83 38.00 40.67 47.17 53.33 43.83 43.33 61.50 43.17 51.00 59.17 44.86 40.14 34.31 21.25 16.67 32.92 28.06 26.11 36.25 29.31 28.61 27. 38.89 37.64 40.83 24.86 29.44 50.42 38.89 39.72 36.67 32.36 30.14 42.64 95.00 88.33 95.00 85.50 80.42 79. 85.42 83.61 Table A: Results with the direct prompt. Model JMMMU-Pro (1320) JMMMU CS Pro (600) (1320) CS (600) CA Pro (720) CA (720) Random Random Choice Frequent Choice Multilingual Open LMMs Qwen3-VL-8B Qwen2.5-VL-7B Phi-4-multimodal Aya-Vision-8B Pangea-7B English-centric Open LMMs LLaVA-OV-1.5-8B LLaVA-OV-7B InternVL2.5-8B Japanese Open LMMs Sarashina2.2-V-3B Sarashina2-V-14B Sarashina2-V-8B Heron-NVILA-Lite-15B 27.05 27.73 47.27 45.00 24.17 26.74 23. 31.97 14.09 31.21 42.88 30.00 27.27 5.30 27.05 27.73 52.88 47.65 32.05 37.73 34.09 46.44 21.29 34.32 45.30 35.00 34.39 34. 26.33 25.33 26.33 25.33 27.64 29.72 27.64 29.72 47.50 46.67 22.00 27.00 21.67 28.00 14.33 29. 54.00 30.50 25.33 1.00 55.83 54.00 31.50 40.33 36.17 46.83 18.00 39.67 59.00 44.50 40.83 38.33 47.08 43.61 25.97 26.53 24.86 35.28 13.89 33. 33.61 29.58 28.89 8.89 50.42 42.36 32.50 35.56 32.36 46.11 24.03 29.86 33.89 27.08 29.03 31.67 Table B: Results with the CoT prompting. Figure C: Failure case: Perceptual error. 21 Figure D: Failure case: Reasoning error."
        }
    ],
    "affiliations": [
        "The University of Tokyo"
    ]
}