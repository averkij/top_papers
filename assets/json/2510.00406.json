{
    "paper_title": "VLA-RFT: Vision-Language-Action Reinforcement Fine-tuning with Verified Rewards in World Simulators",
    "authors": [
        "Hengtao Li",
        "Pengxiang Ding",
        "Runze Suo",
        "Yihao Wang",
        "Zirui Ge",
        "Dongyuan Zang",
        "Kexian Yu",
        "Mingyang Sun",
        "Hongyin Zhang",
        "Donglin Wang",
        "Weihua Su"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Vision-Language-Action (VLA) models enable embodied decision-making but rely heavily on imitation learning, leading to compounding errors and poor robustness under distribution shift. Reinforcement learning (RL) can mitigate these issues yet typically demands costly real-world interactions or suffers from sim-to-real gaps. We introduce VLA-RFT, a reinforcement fine-tuning framework that leverages a data-driven world model as a controllable simulator. Trained from real interaction data, the simulator predicts future visual observations conditioned on actions, allowing policy rollouts with dense, trajectory-level rewards derived from goal-achieving references. This design delivers an efficient and action-aligned learning signal, drastically lowering sample requirements. With fewer than 400 fine-tuning steps, VLA-RFT surpasses strong supervised baselines and achieves greater efficiency than simulator-based RL. Moreover, it exhibits strong robustness under perturbed conditions, sustaining stable task execution. Our results establish world-model-based RFT as a practical post-training paradigm to enhance the generalization and robustness of VLA models. For more details, please refer to https://vla-rft.github.io/."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 ] . [ 1 6 0 4 0 0 . 0 1 5 2 : r VLA-RFT: VISION-LANGUAGE-ACTION REINFORCEMENT FINE-TUNING WITH VERIFIED REWARDS IN WORLD SIMULATORS Hengtao Li1,3,7, Pengxiang Ding1,2,3,, Runze Suo1,3,4, Yihao Wang1,3,6, Zirui Ge1,2,3 Dongyuan Zang7 Kexian Yu5 Mingyang Sun2,3,4 Hongyin Zhang1,2 Donglin Wang1(cid:66) Weihua Su7(cid:66) 1Westlake University 2Zhejiang University 3OpenHelix Team 4Fudan University 5Zhengzhou University 6BUPT 7Hebei University of Technology Project Lead: dingpx2015@gmail.com (cid:66)Corresponding Author Equal contribution Work done during interning at Westlake University Figure 1: The Framework of VLA-RFT. world model functions as simulator that processes multi-rollout VLA action sequences to generate corresponding future states. By incorporating verified rewards through GRPO optimization framework, we perform end-to-end updates of the VLA. Our approach achieves superior performance with remarkably fewer optimization stepsrequiring only 0.4K iterations compared to 150K iterations for strongly supervised baselinedemonstrating advantages in both standard and perturbed environments. Furthermore, the method exhibits enhanced execution-time robustness, characterized by reliable failure recovery and retry capabilities. For more details, please refer to our webpage."
        },
        {
            "title": "ABSTRACT",
            "content": "Vision-Language-Action (VLA) models enable embodied decision-making but rely heavily on imitation learning, leading to compounding errors and poor robustness under distribution shift. Reinforcement learning (RL) can mitigate these issues yet typically demands costly real-world interactions or suffers from simto-real gaps. We introduce VLA-RFT, reinforcement fine-tuning framework that leverages data-driven world model as controllable simulator. Trained from real interaction data, the simulator predicts future visual observations conditioned on actions, allowing policy rollouts with dense, trajectory-level rewards derived from goal-achieving references. This design delivers an efficient and action-aligned learning signal, drastically lowering sample requirements. With fewer than 400 fine-tuning steps, VLA-RFT surpasses strong supervised baselines and achieves greater efficiency than simulator-based RL. Moreover, it exhibits strong robust1 ness under perturbed conditions, sustaining stable task execution. Our results establish world-model-based RFT as practical post-training paradigm to enhance the generalization and robustness of VLA models. For more details, please refer to our webpage."
        },
        {
            "title": "INTRODUCTION",
            "content": "Vision-Language-Action (VLA) models have recently achieved remarkable progress by building upon large, pre-trained vision-language models (VLMs) (Li et al., 2025b; Karamcheti et al., 2024; Driess et al., 2023). Leveraging the powerful perceptual generalization of VLMs allows these models to operate under diverse visual conditions. However, most existing VLAs (Brohan et al., 2022; Zitkovich et al., 2023; Black et al., 2024; Bjorck et al., 2025; Kim et al., 2024) are trained purely via imitation learning. This approach is prone to error accumulation under distribution shift, where small deviations from expert demonstrations gradually drive the policy toward unfamiliar states and weaken its robustness (Ross & Bagnell, 2010; De Haan et al., 2019; Foster et al., 2024). In contrast, reinforcement learning (RL) offers promising avenue to overcome these limitations by explicitly optimizing beyond demonstrated behaviors and encouraging exploration (Liu et al., 2025). Recent studies have increasingly incorporated RL into VLA training, demonstrating its critical role in enhancing generalization and long-horizon task performance through offline RL approaches (Zhang et al., 2025c; 2024), direct real-world RL (Xu et al., 2024; Guo et al., 2025), and simulation-based RL (Lu et al., 2025; Tan et al., 2025; Liu et al., 2025). Yet, standard RL pipelines for VLA face steep challenges. Simulation-based RL (Chen & Li, 2025; Chen et al., 2025b; Shu et al., 2025) often requires millions of interactions and suffers from pronounced sim-to-real gap. Real-world training (Xu et al., 2024; Mark et al., 2024; Guo et al., 2025; Chen et al., 2025a), on the other hand, is prohibitively costly and can raise safety concerns. Offline RL also remains limited: without interaction with the environment, models are vulnerable to distribution shift and cannot learn from the consequences of their own actions (Tan et al., 2025). To address these challenges, we propose VLA-RFT, reinforcement fine-tuning framework that leverages world model as high-fidelity simulator for policy optimization. At its core, VLA-RFT employs controllable world simulator that, once trained on dataset of robot interactions, can predict future visual observations conditioned on an action sequence. Unlike conventional simulation environments restricted to handcrafted scenarios, this simulator is entirely data-driven, capturing the diversity of real-world interactions while avoiding the prohibitive cost and safety risks of training directly in the physical world. For given task, policy-proposed actions are rolled out within this simulator to generate predicted visual trajectories. These synthetic trajectories then enable the design of dense, task-grounded reward by comparing them against the visual trajectory from goal-achieving reference trajectory. These rewards are then used to optimize the policy via Generalized Reinforcement Policy Optimization (GRPO), enabling stable and efficient reinforcement fine-tuning. This design provides continuous, action-aligned learning signal that substantially reduces the sample complexity of reinforcement fine-tuning. Empirically, we show that with as few as 400 finetuning steps, VLA-RFT not only outperforms strong supervised fine-tuning baselines (Wang et al., 2025) in both overall performance and compositional generalization, but also achieves markedly higher efficiency than simulator-based RL algorithms that demand orders of magnitude more interactions. Furthermore, in perturbed or adversarial scenarios, VLA-RFT exhibits superior action robustness, sustaining stable task execution even under unexpected environmental variations. Taken together, this combination of efficiency, generalization, and robustness underscores the practical advantages of our framework for scalable VLA training. Finally, we hope that our method, experiments, and analysis will motivate future research to harness world models as general and efficient post-training paradigm for VLAs, thereby substantially enhancing their practicality and accelerating their real-world deployment."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Vision-Language-Action Models. Vision-Language-Action (VLA) models align visual and linguistic inputs with actions through imitation learning on large-scale datasets (ONeill et al., 2024; 2 Liu et al., 2023; Mees et al., 2022). Pre-trained VLMs provide generalization, while supervised finetuning adapts them to task-specific action spaces (Li et al., 2025b; Karamcheti et al., 2024; Driess et al., 2023). Recent studies further improve efficiency with lightweight adapters and post-training techniques (Kim et al., 2025; Cui et al., 2025; Wang et al., 2025; Fan et al., 2025; Gong et al., 2024; Ding et al., 2024; 2025). However, imitation learning alone is prone to error accumulation under distribution shifts, where minor deviations from expert data push the policy into unfamiliar states and reduce robustness. To address this, recent studies incorporate reinforcement learning to improve VLA performance. Our work also falls into this line of research. VLA with Reinforcement Learning. Reinforcement learning from human feedback has proven highly effective in language models (Sheng et al., 2025; Ouyang et al., 2022), inspiring RL finetuning for visionlanguageaction (VLA) systems. However, simulation-based RL (Chen & Li, 2025; Chen et al., 2025b; Shu et al., 2025) requires vast interactions and suffers from the sim-to-real gap, while real-world training (Xu et al., 2024; Mark et al., 2024; Guo et al., 2025; Chen et al., 2025a) is expensive and unsafe. Offline RL also struggles with distribution shift and the inability to learn from its own actions (Tan et al., 2025). To overcome these limits, we leverage world model as data-driven simulator, enabling practical policy optimization without real-world costs or risks. World Models. World Models learn environment dynamics for planning and control, either via explicit physics (Song et al., 2024; Li et al., 2024; Sancaktar et al., 2022) or latent predictive representations (Hafner et al., 2019b;a; 2023). Recent extensions integrate multi-modal inputs and guide RL with high-dimensional predictions (Wu et al., 2023; Li et al., 2025a). Advances in generative modeling (Ho et al., 2022; Blattmann et al., 2023; Liu et al., 2024) have enabled large-scale videobased World Models (Bardes et al., 2023; Assran et al., 2025), later specialized for robotics (Zhou et al., 2024a;b). Emerging works further link these models with instruction-conditioned action generation (Hu et al., 2024; Cen et al., 2025; Zhong et al., 2025; Zhang et al., 2025a). In this work, our world model not only functions as dynamics simulator, but also provides verified rewards to fine-tune the VLA, enabling rapid and efficient enhancement of the base models performance."
        },
        {
            "title": "3 METHOD",
            "content": "In this section, we begin by presenting the motivation behind our approach and outlining both the key challenges and the intuitive foundation of our pipeline. We then provide formal problem definition and describe each component of the framework in detail. Finally, we present comprehensive illustration of the two training phases, which is shown in Figure 2. Stage I: WM and Policy Pretraining. In the first stage, we pretrain the world model on offline datasets so that it can capture environment dynamics. In parallel, we pretrain the VLA policy to produce stable action chunks, which serve as reliable initialization for subsequent optimization. Stage II: VLA Optimization through WM Interaction. In the second stage, given an initial frame and language instruction, the VLA rolls out action chunks. The world model then interactively generates trajectories conditioned on these actions and provides verified rewards. Using these feedback signals, the VLA is fine-tuned with GRPO to progressively improve policy performance. 3.1 PROBLEM FORMULATION In this work, we investigate how to train dual-system VLA policy equipped with flow-matching action head, using both WM and verified reward mechanism. Specifically, we formulate the entire training process as Partially Observable Markov Decision Process (POMDP). The training pipeline is formally defined by the tuple := ( O, S, A, L). where Observations represents the perceptual space of the agent, including real images captured from the environment. States denotes the robots proprioceptive state. Actions is the action space. Language refers to natural language instructions provided to the agent. The VLA policy is expected to generate sequence of actions with indices [T ] = {0, . . . , }, conditioned on the first observed real image oi, the initial language instruction li, and the initial robot state si. This process is factorized as (cid:0) oi, li, si ˆai:i+T 1 πθ zi = fVLM(oi, li). (cid:0) zi, si (cid:1) = πθfm (1) (cid:1), 3 Figure 2: Training Paradigm of VLA-RFT. In the pre-training stage, both the world model and VLA policy are initialized, where the world model takes 7-dimensional action input that is consistent in format with the VLAs action output. In the reinforcement fine-tuning stage, the VLA generates action chunks based on an initial frame and language instruction, which are rolled out in the world model to predict future states. Verified rewards are then computed from the predicted states and used to optimize the VLA via GRPO Optimization. where fVLM denotes the visionlanguage large model that encodes multimodal inputs into latent representations zi, and πθfm represents the flow-matching policy head that generates the corresponding action chunk. The world model acts as an interactive simulator that generates rollouts conditioned on the first image ot and the policy-generated action sequence at:t+T 1. By comparing the generated trajectory against ground-truth images or ground-truth-action-induced rollouts, we obtain verified reward signal: (cid:40)gϕ(oi, ai), = 0, ˆoi+t+1 = gϕ(oi:i+t, ai:i+t), = 1, . . . , 1. (2) where gϕ denotes the autoregressive world model. In particular, the first prediction is generated from the initial frame ot and the first action at, while subsequent predictions (i 1) are produced autoregressively by conditioning on both the previously generated frames ot:t+i and the executed actions at:t+i. 3.2 STAGE I: WM PRETRAINING AND VLA PRETRAINING To reduce reinforcement learning instability and prevent early collapse, we pretrain the world model and policy on offline datasets, providing stable initialization for subsequent optimization. World Model Training. To obtain dense verified rewards more efficiently, and inspired by recent advances in video generation models (e.g., iVideoGPT (Wu et al., 2024)), we train an interactive video prediction model to serve as the world model. This design avoids the limitations of implicit world models, such as sparse reward signals and the lack of verifiable environment dynamics. It consists of pretrained tokenizer and an autoregressive Transformer backbone. During pretraining, the WM is optimized via maximum likelihood: MLE(ϕ) = LWM (cid:104) log pϕ(oi+1 oi, ai) + 1 (cid:88) t=1 (cid:105) log pϕ(oi+t+1 oi:i+t, ai:i+t) . (3) where pϕ() denotes the predictive distribution of future observations parameterized by the world model with parameters ϕ. VLA Pretraining. In this stage, we aim to ensure that the VLA produces stable actions. Since the flow-matching action head provides stable training for continuous actions, we pretrain the upstream VLM encoder and the flow-matching head on the expert demonstration dataset D. MSE(θ) = E(ai:i+T 1,oi,li,si)D LVLA (cid:104) vθ(oi, li, si, aτ i:i+T 1) uτ 2 2 (cid:105) . (4) where τ Beta(α, β) is the flow-matching timestep, vθ() denotes the flow predicted by the action head parameterized by θ, aτ t:t+T 1 = τ at:t+T 1 + (1 τ )ϵ is the noise-perturbed action chunk, uτ = at:t+T 1 ϵ is the target flow field defined by the noisy action interpolation, and ϵ (0, I) is standard Gaussian noise."
        },
        {
            "title": "3.3 STAGE II: VLA OPTIMIZATION THROUGH WM INTERACTION",
            "content": "To achieve stable and efficient fine-tuning, we adopt an SDE-based policy formulation optimized with GRPO, which offers reliable gradient estimates. The Stage world model serves as an interactive simulator, providing verified rewards that further enhance training stability. SDE-Policy: Policy Parameterization via Flow and Sigma. Since flow matching is inherently deterministic ODE process, it has limitations in directly obtaining log-likelihood. To address this, we build upon prior work on flow-matching reinforcement learning(e.g. ReinFlow (Zhang et al., 2025d)) by extending the framework into stochastic formulation, thereby enabling exploration during training. In Stage II, we introduce Sigma Net, whose architecture mirrors that of the flow-matching head, and which outputs variance vector that parameterizes the stochasticity of the policy. Concretely, at inference time, we discretize the integration into = 10 steps, with [0, 1, 2, . . . , 10]. Actions are generated by integrating the learned vector field from τ = 0 to τ = 1, initialized from random noise aτ =0 i:i+T 1 (0, I). We apply the forward Euler method: µk = akδ i:i+T 1 + δvθ(oi, li, si, akδ i:i+T 1), (5) where δ = 0.1 is the integration step size. For each integration steps k, Sigma Net takes as input (zi, si, k) and outputs variance vector σk ψ, while the flow-matching action head simultaneously predicts the flow µk. Together, these two components define Gaussian conditional distribution from which the next action chunk is sampled, thereby generalizing the deterministic FM-ODE formulation into stochastic differential equation (SDE) process: where akδ i:i+T 1 (µk, Σk), Σk = (σk ψ)2. (6) (7) Within the same rollout, we compute the step-wise log-likelihoods across the denoising steps, and take their average as the log-probability of the rollout: ℓθ,ψ = 1 (cid:88) k= log p(k) θ,ψ(akδ i:i+T 1 a(k1)δ i:i+T 1, zi, si). (8) Finally, we compute the policy ratio with respect to the old policy by exponentiating the difference of average log-probabilities: = exp(cid:0)ℓθ,ψ ℓold (cid:1). (9) Interactive WM Simulation and Verified Reward. Visual features often carry richer semantic information. To leverage this, given an action chunk aK t:t+T 1 from the SDE-Policy, the world model generates visual trajectory, which is aligned with ground-truth data to construct verified rewards. This design improves reward reliability, reduces manual labeling, and enhances stability. Starting from the initial frame oi and the first action aK recursively conditions on previously generated frames to produce the complete trajectory: , the WM generates the next frame and Traj = (cid:2)oi, aKδ , ˆoi+1, . . . , aKδ i+T 1, ˆoi+T (cid:3), (10) Algorithm 1 VLA Fine-Tuning Pipeline with World Model and Verified Reward Require: Offline dataset D, diffusion horizon K, chunk length , rollout number , initial frame ot, sigma net parameters ψ Ensure: Trained VLA policy πθ 1: Stage I: Pretraining 2: Train WM parameters ϕ with maximum likelihood Eq. 3 3: Train VLA encoder fVLM + flow-matching head πθfm with loss Eq. 4 4: Stage II: Interaction and Optimization 5: for each task instance do 6: 7: for = 1 to do for = 1 to do Sample actions from Gaussian distribution p(k) θ,ψ Calculate log-probability ℓ(k) end for Generate trajectory Traj with WM Compute verified reward Rn end for Compute advantages Advn = Rn Rgroup Update policy πθ and sigma net with GRPO objective 8: 9: 10: 11: 12: 13: 14: 15: 16: end for Rollouts Diffusion steps Eq. 6 Eq. 8 Eq. 10 Eq. Eq. 13 The generated sequence ˆoi+1:i+T +1 is aligned with the ground-truth frames oi+1:i+T +1 from the offline dataset. The verified reward for the current trajectory segment is defined as the negative weighted sum of the per-frame reconstruction loss and perceptual similarity loss: = 1 (cid:88) (cid:104) t= λ1 L1 (cid:0)ˆoi+t+1, oi+t+1 (cid:1) + λlp LPIPS(cid:0)ˆoi+t+1, oi+t+1 (cid:1)(cid:105) . (11) To reduce variance, we group rollouts sampled from the same starting state and compute the group average reward as baseline: Rgroup = 1 (cid:88) j=1 Rj, Advn = Rn Rgroup. (12) Using the policy ratio derived earlier, the VLA policy is optimized with GRPO. For training stability, we also retain small-weight flow-matching MSE term as auxiliary supervision on the flow head. The final objective is GRPO(θ, ψ) = E[ clip(r, 1 ϵ, 1 + ϵ) Adv ] + λmse LVLA LVLA MSE(θ) is the auxiliary flow-matching MSE loss with weight λmse, and H(πθ,ψ) is the policy where LVLA entropy used to encourage exploration, weighted by α. Therefore, the objective integrates policy optimization with auxiliary supervision to ensure efficient and stable fine-tuning. MSE(θ) α H(cid:0)πθ,ψ (13) (cid:1)."
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "In this section, we assess VLA-RFT through three research questions: 1) How well can world model approximate simulator? 2) How does world model improve VLA performance? 3) Which components of VLA-RFT drive these improvements? 4.1 EXPERIMENTAL SETUP. Implementations. 1) Benchmark: We evaluate our model on the LIBERO benchmark (Liu et al., 2023). 2) Metrics: We report success rate (SR) for all tasks. 3) Base Policy: To accelerate experimentation, we employed lightweight variant of VLA-Adapter (Wang et al., 2025) as our baseline. 6 More details of policy choice can be found in Appendix A.1. 4) World Model: To optimize the balance between training efficiency and generation quality, we implemented lightweight autoregressive world model based on the LLaMA architecture (Touvron et al., 2023). This model was instantiated as compact 138M-parameter variant, comparable in scale to GPT-2 small (Radford et al., 2019). The model underwent pretraining on the LIBERO dataset to effectively capture taskrelevant visual and action dynamics. 5) Training Details: We initially pretrained base policy through supervised fine-tuning. Subsequently, we conducted post-training with reinforcement finetuning (RFT) using VERL (Sheng et al., 2025), distributed RL framework that coordinates diverse rollout strategies with FSDP-sharded training. All experiments were executed on 4 A800 GPUs."
        },
        {
            "title": "4.2 WORLD MODEL CAPABILITIES.",
            "content": "Experimental Setting. To evaluate whether pre-training enables the world model to capture environmental dynamics, we assess its pixel-level generation capability. We randomly sample consecutive image-action pairs from LIBERO, input the initial frame and complete action sequence into the world model, and compare the generated frames with ground-truth images for subsequent steps. Results Analysis. As shown in Table 1, the world model attains low reconstruction error (MSE 0.0039) and strong perceptual scores (PSNR 25.23 dB, SSIM 0.906, LPIPS (Zhang et al., 2018) 0.059), indicating high frame fidelity and perceptual quality. Qualitative results show sharp, temporally consistent frames that capture both static backgrounds and action-driven changes, demonstrating that pre-training enables the model to learn visual appearance and action-conditioned dynamics. Table 1: World model generation performance. Left: frame-level metrics across four suites (Spatial, Object, Goal, Long) and their averagesMSE (pixel error), PSNR (signal-to-noise ratio), SSIM (structural similarity), and LPIPS (perceptual distance). Right: qualitative results. Left column shows simulator sequences, right column shows world-model generations from the same initial frame and actions, illustrating consistent appearance and action-induced dynamics. Task MSE PSNR SSIM LPIPS Spatial Object Goal Long Avg 0.0039 0.0036 0.0024 0.0056 0.0039 24.98 25.13 26.99 23.83 25.23 0.896 0.913 0.929 0.885 0.906 0.067 0.054 0.040 0.074 0. 4.3 PERFORMANCE IMPROVEMENTS FOR VLA. In the previous section, we analyzed the generation quality of the world model. Here, we further investigate whether our training pipeline enhances policy capability. Specifically, we evaluate policy performance before and after training under the following two task settings. LIBERO Standard Suites. We use the Base ( 15w) as baseline and test the effect of adding RFT. As shown in Table 2, only 400 iterations of RFT raise average SR from 86.6% to 91.3% (+4.7 points), with gains across all suites: Spatial (+6.0 points), Object (+6.4 points), Goal (+2.6 points), and Long (+3.0 points). The graph further shows RFT (400) consistently outperforms Base (15w). Notably, while extending SFT from 3w to 15w required heavy training, RFT delivers clear improvements with far fewer iterations, underscoring its efficiency. LIBERO Perturbation Suites. To assess outof-distribution robustness, we construct perturbed variants across the four LIBERO suites and report success rates for base policy and our method. Figure 3: Action distribution visualization of VLA-RFT and VLA-SFT. The plots show distributions along and action dimensions: the left plot corresponds to the RFT-trained policy, and the right plot to the SFT-only base policy. Table 2: Performance under LIBERO Standard Suites. The table reports SR (%) across the four suites (Spatial, Object, Goal, and Long) and their average; the radar plot on the right provides visual comparison of different model stages across tasks. Policy (iterations) Spatial Object Goal Long Average Base (3w) Base (15w) VLA-RFT (400) vs Base (15w) 82.4 88.4 94.4 +6.0 84.8 88.0 94.4 +6. 85.4 92.8 95.4 +2.6 57.2 77.2 80.2 +3.0 77.5 86.6 91.1 +4.5 Figure 4: Illustration of perturbed task settings in LIBERO. We consider four perturbation types to evaluate out-of-distribution robustness: (Object Position) shifting the initial (x, y) coordinates of the manipulated object; (Goal Position) displacing the target object in the (x, y) plane; (Robot State) modifying the grippers vertical height and horizontal offset; and (Combination) applying all perturbations together. Each row shows the original setting (Origin), the perturbed variant (Disturb), and side-by-side comparison (Contrast). 1) Experimental Setting. In LIBERO-Object, the manipulated objects initial position is shifted in the (x, y) plane with small or large offsets. In LIBERO-Goal, the target objects initial position is similarly displaced. In LIBERO-Spatial, the robots initial state is perturbed by adjusting the gripper height and horizontal offset. In LIBERO-Long, we combine all the above perturbations. An illustration of the perturbed tasks is provided in Figure 4. 2) Results Analysis. As shown in Table 3, VLA-RFT consistently improves robustness across all types of perturbations. While the base policy (Base 15w) degrades substantially under larger shifts, VLA-RFT maintains higher stability, demonstrating its effectiveness against distributional shifts. The gains are most pronounced in the Goal and combined perturbations (over +6%), where generalization is more challenging, while RoboState perturbations show smaller but consistent improvements. Overall, our training pipeline not only increases standard performance but also improves out-of-distribution robustness, particularly in more complex settings. To further understand the robustness gains, we examine action distributions in Figure 3. VLA-RFT yields broader coverage across action dimensions than base policy, while SFT remains narrowly concentrated. This broader exploration enables better adaptability and generalization under perturbations. 4.4 KEY FACTORS FOR VLA-RFT We showed our pipeline improves policy performance and robustness. Next, we test which components drive these gains via three verified reward designs and world model ablations. 1) Experimental Setting. We design three verified rewards under the same training setup and apply RFT to the base model to compare their effects on LIBERO success rates. Reward type 1 uses the negative L1 distance between policy and dataset actions, offering direct action-level supervision. 8 Table 3: Performance under perturbation settings. All perturbation magnitudes are in centimeter. Object Pos Perturb Range SR (%) Goal Pos Perturb Range SR (%) Base(15w) VLA-RFT vs Base Base(15w) VLA-RFT vs Base Minor Perturbation 2.5 2.5 2.5 Major Perturbation 5 5 5 69.3 73.5 +4. 48.0 52.5 +4.5 Base(15w) VLA-RFT vs Base Base(15w) VLA-RFT vs Base Minor Perturbation 2.5 2.5 2.5 Major Perturbation 5 5 5 74.5 79.0 +4.5 44.8 51.5 +6.7 RoboState Perturb Range SR (%) Combined Perturb Range SR (%) Base(15w) VLA-RFT vs Base Base(15w) VLA-RFT vs Base Minor Perturbation 20 20 20 Major Perturbation 50 50 50 73.0 76.5 +2.5 63.5 67.0 +3.5 Base(15w) VLA-RFT vs Base Base(15w) VLA-RFT vs Base Minor Perturbation 2.5/2.5/20 2.5/2.5/20 2.5/2.5/20 Major Perturbation 5/5/50 5/5/50 5/5/50 63.5 70.0 +6.5 34.0 37.0 +3. Table 4: Reward design comparison on LIBERO. The left table reports the average success rates (SR, %) of the base policy (Base 15w) and its variants trained with three different verified reward types. The right figure illustrates the corresponding reward function structures. Policy Average (SR %) Base Base (15w) 86. Reward Type 1 VLA-RFT (R1) vs Base 87.7 +1.1 Reward Type 2 VLA-RFT (R2) vs Base 87.1 +0. Reward Type 3 VLA-RFT (Ours) vs Base 91.1 +4.5 Reward type 2 generates images from policy actions via the world model and compares them with dataset images using negative MAE and LPIPS, providing pixel-level guidance. Reward type 3 renders trajectories from both policy and dataset actions within the same world model, using negative MAE and LPIPS across time to mitigate generation-quality bias and ensure fairness. 2) Results Analysis. As shown in Table 4, the comparison across reward designs highlights the essential role of the world model in the training pipeline. Reward type 1, which excludes the world model and relies only on action-level supervision, brings very limited gains (+1.1 points), showing that imitation alone is insufficient. Reward type 2 uses the world model and achieves moderate improvements, but direct comparison with real images still has limitations. Reward type 3 maximally exploits the world model by performing trajectory comparisons within the same generative space, leading to consistent improvements across all tasks and an average success rate of 91.1% (+4.5 points over the base policy). These results demonstrate that the world model is key component, providing reliable optimization signals and enhancing both performance and robustness."
        },
        {
            "title": "5 CONCLUSION & LIMITATION",
            "content": "In this work, we introduced VLA-RFT, reinforcement fine-tuning framework that uses learned world model as controllable simulator. This approach enables efficient and safe policy optimization, bridges imitation and reinforcement learning, and reduces real-world interaction costs. Experiments show strong performance and generalization with minimal fine-tuning, highlighting worldmodelbased RFT as promising direction for VLA research. 9 Despite these advantages, several limitations remain. First, although the WM approximates realworld dynamics and enables reinforcement learning, the verified reward signal is still largely defined by the similarity between generated trajectories and expert demonstrations. Consequently, the policy remains constrained by the quality of the expert dataset, limiting its ability to discover strategies that surpass expert performance. Second, the representational capacity of the WM constitutes bottleneck; scaling to larger models trained on more diverse and extensive data could significantly improve out-of-distribution generalization. Moreover, our current framework does not explicitly integrate the WM into planning, which could further enhance long-horizon reasoning. Third, the verified reward mechanism itself could be improved: rather than relying solely on expert similarity, future work could leverage learned reward models (e.g., VLAC Zhai et al. (2025)) to provide more task-relevant feedback. Finally, while our study concentrates on flow-matchingbased policies, extending the framework to encompass broader class of policy architectures remains an important direction for future research."
        },
        {
            "title": "REFERENCES",
            "content": "Mido Assran, Adrien Bardes, David Fan, Quentin Garrido, Russell Howes, Matthew Muckley, Ammar Rizvi, Claire Roberts, Koustuv Sinha, Artem Zholus, et al. V-jepa 2: Self-supervised video models enable understanding, prediction and planning. arXiv preprint arXiv:2506.09985, 2025. 3 Adrien Bardes, Quentin Garrido, Jean Ponce, Xinlei Chen, Michael Rabbat, Yann LeCun, Mido Assran, and Nicolas Ballas. V-jepa: Latent video prediction for visual representation learning. 2023. 3 Johan Bjorck, Fernando Castaneda, Nikita Cherniadev, Xingye Da, Runyu Ding, Linxi Fan, Yu Fang, Dieter Fox, Fengyuan Hu, Spencer Huang, et al. Gr00t n1: An open foundation model for generalist humanoid robots. arXiv preprint arXiv:2503.14734, 2025. 2 Kevin Black, Noah Brown, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai, Lachy Groom, Karol Hausman, Brian Ichter, et al. vision-language-action flow model for general robot control. arXiv preprint arXiv:2410.24164, 2024. 2, 15, 18 Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. 3 Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Jasmine Hsu, et al. Rt-1: Robotics transformer for real-world control at scale. arXiv preprint arXiv:2212.06817, 2022. Jun Cen, Chaohui Yu, Hangjie Yuan, Yuming Jiang, Siteng Huang, Jiayan Guo, Xin Li, Yibing Song, Hao Luo, Fan Wang, et al. Worldvla: Towards autoregressive action world model. arXiv preprint arXiv:2506.21539, 2025. 3, 18 Yuhui Chen, Shuai Tian, Shugao Liu, Yingting Zhou, Haoran Li, and Dongbin Zhao. ConarXiv preprint rft: reinforced fine-tuning method for vla models via consistency policy. arXiv:2502.05450, 2025a. 2, 3 Yuxuan Chen and Xiao Li. Rlrc: Reinforcement learning-based recovery for compressed visionlanguage-action models. arXiv preprint arXiv:2506.17639, 2025. 2, 3 Zengjue Chen, Runliang Niu, He Kong, and Qi Wang. Tgrpo: Fine-tuning vision-language-action model via trajectory-wise group relative policy optimization. arXiv preprint arXiv:2506.08440, 2025b. 2, 3, Cheng Chi, Siyuan Feng, Yilun Du, Zhenjia Xu, Eric Cousineau, Benjamin Burchfiel, and Shuran Song. Diffusion policy: Visuomotor policy learning via action diffusion. In Robotics: Science and Systems, 2023. 18 10 Can Cui, Pengxiang Ding, Wenxuan Song, Shuanghao Bai, Xinyang Tong, Zirui Ge, Runze Suo, Wanqi Zhou, Yang Liu, Bofang Jia, et al. Openhelix: short survey, empirical analysis, and open-source dual-system vla model for robotic manipulation. arXiv preprint arXiv:2505.03912, 2025. 3 Pim De Haan, Dinesh Jayaraman, and Sergey Levine. Causal confusion in imitation learning. Advances in neural information processing systems, 32, 2019. 2 Pengxiang Ding, Han Zhao, Wenjie Zhang, Wenxuan Song, Min Zhang, Siteng Huang, Ningxi Yang, and Donglin Wang. Quar-vla: Vision-language-action model for quadruped robots. In European Conference on Computer Vision, pp. 352367. Springer, 2024. Pengxiang Ding, Jianfei Ma, Xinyang Tong, Binghong Zou, Xinxin Luo, Yiguo Fan, Ting Wang, Hongchao Lu, Panzhong Mo, Jinxin Liu, et al. Humanoid-vla: Towards universal humanoid control with visual integration. arXiv preprint arXiv:2502.14795, 2025. 3 Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, Wenlong Huang, et al. Palm-e: An embodied multimodal language model. 2023. 2, 3 Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution imIn Proceedings of the IEEE/CVF conference on computer vision and pattern age synthesis. recognition, pp. 1287312883, 2021. 15 Yiguo Fan, Pengxiang Ding, Shuanghao Bai, Xinyang Tong, Yuyang Zhu, Hongchao Lu, Fengqi Dai, Wei Zhao, Yang Liu, Siteng Huang, et al. Long-vla: Unleashing long-horizon capability of vision language action model for robot manipulation. arXiv preprint arXiv:2508.19958, 2025. 3 Dylan Foster, Adam Block, and Dipendra Misra. Is behavior cloning all you need? understanding horizon in imitation learning. Advances in Neural Information Processing Systems, 37:120602 120666, 2024. Dibya Ghosh, Homer Walke, Karl Pertsch, Kevin Black, Oier Mees, Sudeep Dasari, Joey Hejna, Charles Xu, Jianlan Luo, Tobias Kreiman, You Liang Tan, Pannag Sanketi, Quan Vuong, Ted Xiao, Dorsa Sadigh, Chelsea Finn, and Sergey Levine. Octo: An open-source generalist robot policy. In Proceedings of Robotics: Science and Systems, Delft, Netherlands, 2024. 18 Zhefei Gong, Pengxiang Ding, Shangke Lyu, Siteng Huang, Mingyang Sun, Wei Zhao, Zhaoxin Fan, and Donglin Wang. Carp: Visuomotor policy learning via coarse-to-fine autoregressive prediction. arXiv preprint arXiv:2412.06782, 2024. 3 Yanjiang Guo, Jianke Zhang, Xiaoyu Chen, Xiang Ji, Yen-Jen Wang, Yucheng Hu, and Jianyu Chen. arXiv preprint Improving vision-language-action model with online reinforcement learning. arXiv:2501.16664, 2025. 2, 3 Danijar Hafner, Timothy Lillicrap, Jimmy Ba, and Mohammad Norouzi. Dream to control: Learning behaviors by latent imagination. arXiv preprint arXiv:1912.01603, 2019a. Danijar Hafner, Timothy Lillicrap, Ian Fischer, Ruben Villegas, David Ha, Honglak Lee, and James In International conference on Davidson. Learning latent dynamics for planning from pixels. machine learning, pp. 25552565. PMLR, 2019b. 3 Danijar Hafner, Jurgis Pasukonis, Jimmy Ba, and Timothy Lillicrap. Mastering diverse domains through world models. arXiv preprint arXiv:2301.04104, 2023. 3 Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David Fleet. Video diffusion models. Advances in neural information processing systems, 35:8633 8646, 2022. 3 Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. ICLR, 1(2):3, 2022. 11 Yucheng Hu, Yanjiang Guo, Pengchao Wang, Xiaoyu Chen, Yen-Jen Wang, Jianke Zhang, Koushil Sreenath, Chaochao Lu, and Jianyu Chen. Video prediction policy: generalist robot policy with predictive visual representations. arXiv preprint arXiv:2412.14803, 2024. 3 Siddharth Karamcheti, Suraj Nair, Ashwin Balakrishna, Percy Liang, Thomas Kollar, and Dorsa Sadigh. Prismatic vlms: Investigating the design space of visually-conditioned language models. In Forty-first International Conference on Machine Learning, 2024. 2, 3 Moo Jin Kim, Karl Pertsch, Siddharth Karamcheti, Ted Xiao, Ashwin Balakrishna, Suraj Nair, Rafael Rafailov, Ethan Foster, Grace Lam, Pannag Sanketi, et al. Openvla: An open-source vision-language-action model. arXiv preprint arXiv:2406.09246, 2024. 2, 18 Moo Jin Kim, Chelsea Finn, and Percy Liang. Fine-tuning vision-language-action models: Optimizing speed and success. arXiv preprint arXiv:2502.19645, 2025. Diederik Kingma and Jimmy Ba. Adam: method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. 16 Chenhao Li, Elijah Stanger-Jones, Steve Heim, and Sangbae Kim. Fld: Fourier latent dynamics for structured motion representation and learning. arXiv preprint arXiv:2402.13820, 2024. 3 Chenhao Li, Andreas Krause, and Marco Hutter. Robotic world model: neural network simulator for robust policy optimization in robotics. arXiv preprint arXiv:2501.10100, 2025a. Zhiqi Li, Guo Chen, Shilong Liu, Shihao Wang, Vibashan VS, Yishen Ji, Shiyi Lan, Hao Zhang, Yilin Zhao, Subhashree Radhakrishnan, et al. Eagle 2: Building post-training data strategies from scratch for frontier vision-language models. arXiv preprint arXiv:2501.14818, 2025b. 2, 3 Bo Liu, Yifeng Zhu, Chongkai Gao, Yihao Feng, Qiang Liu, Yuke Zhu, and Peter Stone. Libero: Benchmarking knowledge transfer for lifelong robot learning. Advances in Neural Information Processing Systems, 36:4477644791, 2023. 3, 6 Jijia Liu, Feng Gao, Bingwen Wei, Xinlei Chen, Qingmin Liao, Yi Wu, Chao Yu, and Yu Wang. What can rl bring to vla generalization? an empirical study. arXiv preprint arXiv:2505.19789, 2025. 2 Yixin Liu, Kai Zhang, Yuan Li, Zhiling Yan, Chujie Gao, Ruoxi Chen, Zhengqing Yuan, Yue Huang, Hanchi Sun, Jianfeng Gao, et al. Sora: review on background, technology, limitations, and opportunities of large vision models. arXiv preprint arXiv:2402.17177, 2024. 3 Guanxing Lu, Wenkai Guo, Chubin Zhang, Yuheng Zhou, Haonan Jiang, Zifeng Gao, Yansong Tang, and Ziwei Wang. Vla-rl: Towards masterful and general robotic manipulation with scalable reinforcement learning. arXiv preprint arXiv:2505.18719, 2025. 2, 18 Max Sobol Mark, Tian Gao, Georgia Gabriela Sampaio, Mohan Kumar Srirama, Archit Sharma, Chelsea Finn, and Aviral Kumar. Policy agnostic rl: Offline rl and online rl fine-tuning of any class and backbone. arXiv preprint arXiv:2412.06685, 2024. 2, Oier Mees, Lukas Hermann, Erick Rosete-Beas, and Wolfram Burgard. Calvin: benchmark for language-conditioned policy learning for long-horizon robot manipulation tasks. IEEE Robotics and Automation Letters, 7(3):73277334, 2022. 3 Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35: 2773027744, 2022. 3 Abby ONeill, Abdul Rehman, Abhiram Maddukuri, Abhishek Gupta, Abhishek Padalkar, Abraham Lee, Acorn Pooley, Agrim Gupta, Ajay Mandlekar, Ajinkya Jain, et al. Open x-embodiment: Robotic learning datasets and rt-x models: Open x-embodiment collaboration 0. In 2024 IEEE International Conference on Robotics and Automation (ICRA), pp. 68926903. IEEE, 2024. 2 William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 41954205, 2023. 16 Jan Peters and Stefan Schaal. Reinforcement learning by reward-weighted regression for operational space control. In Proceedings of the 24th international conference on Machine learning, pp. 745 750, 2007. 18 Delin Qu, Haoming Song, Qizhi Chen, Yuanqi Yao, Xinyi Ye, Yan Ding, Zhigang Wang, JiaYuan Gu, Bin Zhao, Dong Wang, et al. Spatialvla: Exploring spatial representations for visuallanguage-action model. arXiv preprint arXiv:2501.15830, 2025. 18 Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. 2019. OpenAI blog preprint. 7 Moritz Reuss, Omer Erdinc Yagmurlu, Fabian Wenzel, and Rudolf Lioutikov. Multimodal arXiv preprint diffusion transformer: Learning versatile behavior from multimodal goals. arXiv:2407.05996, 2024. 18 Stephane Ross and Drew Bagnell. Efficient reductions for imitation learning. In Proceedings of the thirteenth international conference on artificial intelligence and statistics, pp. 661668. JMLR Workshop and Conference Proceedings, 2010. 2 Cansu Sancaktar, Sebastian Blaes, and Georg Martius. Curious exploration via structured world models yields zero-shot object manipulation. Advances in Neural Information Processing Systems, 35:2417024183, 2022. 3 Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: flexible and efficient rlhf framework. In Proceedings of the Twentieth European Conference on Computer Systems, pp. 12791297, 2025. 3, 7 Junyang Shu, Zhiwei Lin, and Yongtao Wang. Rftf: Reinforcement fine-tuning for embodied agents with temporal feedback. arXiv preprint arXiv:2505.19767, 2025. 2, Yunlong Song, Sangbae Kim, and Davide Scaramuzza. Learning quadruped locomotion using differentiable simulation. arXiv preprint arXiv:2403.14864, 2024. 3 Shuhan Tan, Kairan Dou, Yue Zhao, and Philipp Krahenbuhl. Interactive post-training for visionlanguage-action models. arXiv preprint arXiv:2505.17016, 2025. 2, 3 Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. 7 Yihao Wang, Pengxiang Ding, Lingxiao Li, Can Cui, Zirui Ge, Xinyang Tong, Wenxuan Song, Han Zhao, Wei Zhao, Pengxu Hou, Siteng Huang, Yifan Tang, Wenhui Wang, Ru Zhang, Jianyi Liu, and Donglin Wang. Vla-adapter: An effective paradigm for tiny-scale vision-language-action model. arXiv preprint arXiv:2509.09372, 2025. 2, 3, 6, 15, Jialong Wu, Shaofeng Yin, Ningya Feng, Xu He, Dong Li, Jianye Hao, and Mingsheng Long. ivideogpt: Interactive videogpts are scalable world models. Advances in Neural Information Processing Systems, 37:6808268119, 2024. 4 Philipp Wu, Alejandro Escontrela, Danijar Hafner, Pieter Abbeel, and Ken Goldberg. Daydreamer: In Conference on robot learning, pp. 22262240. World models for physical robot learning. PMLR, 2023. 3 Charles Xu, Qiyang Li, Jianlan Luo, and Sergey Levine. Rldg: Robotic generalist policy distillation via reinforcement learning. arXiv preprint arXiv:2412.09858, 2024. 2, 3 Shaopeng Zhai, Qi Zhang, Tianyi Zhang, Fuxian Huang, Haoran Zhang, Ming Zhou, Shengzhe Zhang, Litao Liu, Sixu Lin, and Jiangmiao Pang. vision-language-action-critic model for robotic real-world reinforcement learning, 2025. Hongyin Zhang, Pengxiang Ding, Shangke Lyu, Ying Peng, and Donglin Wang. Gevrm: arXiv preprint Goal-expressive video generation model for robust visual manipulation. arXiv:2502.09268, 2025a. 3 13 Hongyin Zhang, Shiyuan Zhang, Junxi Jin, Qixin Zeng, Yifan Qiao, Hongchao Lu, and Donglin Wang. Balancing signal and variance: Adaptive offline rl post-training for vla flow models. arXiv preprint arXiv:2509.04063, 2025b. 18 Hongyin Zhang, Zifeng Zhuang, Han Zhao, Pengxiang Ding, Hongchao Lu, and Donglin Wang. Reinbot: Amplifying robot visual-language manipulation with reinforcement learning. arXiv preprint arXiv:2505.07395, 2025c. 2, 18 Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 586595, 2018. Tonghe Zhang, Chao Yu, Sichang Su, and Yu Wang. Reinflow: Fine-tuning flow matching policy with online reinforcement learning. arXiv preprint arXiv:2505.22094, 2025d. 5 Zijian Zhang, Kaiyuan Zheng, Zhaorun Chen, Joel Jang, Yi Li, Siwei Han, Chaoqi Wang, Mingyu Ding, Dieter Fox, and Huaxiu Yao. Grape: Generalizing robot policy via preference alignment. arXiv preprint arXiv:2411.19309, 2024. 2 Qingqing Zhao, Yao Lu, Moo Jin Kim, Zipeng Fu, Zhuoyang Zhang, Yecheng Wu, Zhaoshuo Li, Qianli Ma, Song Han, Chelsea Finn, et al. Cot-vla: Visual chain-of-thought reasoning for vision-language-action models. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 17021713, 2025. 18 Ruijie Zheng, Yongyuan Liang, Shuaiyi Huang, Jianfeng Gao, Hal Daume III, Andrey Kolobov, Furong Huang, and Jianwei Yang. Tracevla: Visual trace prompting enhances spatial-temporal awareness for generalist robotic policies. In The Thirteenth International Conference on Learning Representations, 2025. 18 Zhide Zhong, Haodong Yan, Junfeng Li, Xiangchen Liu, Xin Gong, Wenxuan Song, Jiayi Chen, and Haoang Li. Flowvla: Thinking in motion with visual chain of thought. arXiv preprint arXiv:2508.18269, 2025. Gaoyue Zhou, Hengkai Pan, Yann LeCun, and Lerrel Pinto. Dino-wm: World models on pre-trained visual features enable zero-shot planning. arXiv preprint arXiv:2411.04983, 2024a. 3 Siyuan Zhou, Yilun Du, Jiaben Chen, Yandong Li, Dit-Yan Yeung, and Chuang Gan. Robodreamer: Learning compositional world models for robot imagination. arXiv preprint arXiv:2404.12377, 2024b. 3 Brianna Zitkovich, Tianhe Yu, Sichun Xu, Peng Xu, Ted Xiao, Fei Xia, Jialin Wu, Paul Wohlhart, Stefan Welker, Ayzaan Wahid, et al. Rt-2: Vision-language-action models transfer web knowledge to robotic control. In Conference on Robot Learning, pp. 21652183. PMLR, 2023."
        },
        {
            "title": "A APPENDIX",
            "content": "A.1 MODEL ARCHITECTURE World Model. As shown in Figure 5, given the input initial image, we first encode it using an encoder (similar to VQGAN (Esser et al., 2021)) to obtain image tokens, while continuous actions are discretized into action tokens through an action tokenizer. These image and action tokens are then jointly fed into the world model, which autoregressively predicts the future token sequences. Finally, the generated image tokens are decoded into corresponding future image sequences, enabling the modeling and simulation of environment dynamics. As shown in Table 5, the model is built on 12-layer Transformer architecture with hidden size of 768 and an intermediate FFN size of 3072. It employs 12 attention heads with head dimension of 64, maximum positional embedding length of 8192, SiLU activation, and vocabulary size of 9008. Figure 5: Illustration of World Model Generation. The initial image I0 and input action sequence a0:T 1 are first encoded into image and action tokens. These tokens are then fed into the world model to autoregressively predict the future state token sequence. Finally, decoders transform the generated image tokens into predicted future images I1, I2, . . . , IT . VLA Policy. While flow-based methods such as π0 (Black et al., 2024) demonstrate competitive performance, their JAX implementation poses integration challenges with VERL, and the LeRobot PyTorch version offers no significant advantages over VLA-Adapter despite its considerable computational overhead. Therefore, we selected VLA-Adapter (Wang et al., 2025) as our base policy. During the RFT stage, we freeze the upper layer VLM of the policy and only update the lower layer action head. In addition, we incorporate sigma net with DiT-based architecture similar to the action head, which is responsible for generating noise outputs. 15 Table 5: Key hyperparameters of the World Model: Architecture (left) and Pre-training (right). Hyperparameter Value Architecture Layers Hidden size FFN intermediate size Attention heads Head dimension Keyvalue heads Max position embeddings Activation Vocabulary size 12 768 3072 12 64 12 8192 SiLU 9008 Hyperparameter Value Pre-training"
        },
        {
            "title": "Batch size\nTraining steps\nLearning rate\nOptimizer\nDatasets\nSegment length",
            "content": "16 1.5 105 5 105 AdamW (Kingma & Ba, 2014) Libero Datasets 8 Table 6: Key hyperparameters of the VLA-Adapter: Architecture (left) and Pre-training (right). Hyperparameter Value Architecture Vision backbone Input image size LLM backbone LLM max length Text layers / hidden size Attention heads / KV heads FFN intermediate size Max position embeddings Torch dtype Action bins dinosiglip-vit-so-224px 224 224 qwen25-0 5b-extra 2048 24 / 896 14 / 2 4864 32768 bfloat16 256 Hyperparameter Value Pre-training Batch size Training steps Learning rate Optimizer Datasets LoRA Rank 16 1.5 105 1 104 AdamW (Kingma & Ba, 2014) Libero Datasets 64 A.2 TRAINING DETAILS Pre-Training Phase. 1) World Model: As shown in Table 5, the model is optimized using AdamW on the Libero datasets for 1.5 105 steps with batch size of 16, segment length of 8, and learning rate of 5 105. 2) VLA Policy: Our base policy consists of an upper-layer visionlanguage model (VLM) and lower-layer DiT (Peebles & Xie, 2023)-based flow matching action head. During pre-training, we apply LoRA (Hu et al., 2022) for parameter-efficient fine-tuning of the VLM, while jointly optimizing the action head to better align the visual, language, and action spaces. The detailed architecture and training hyperparameters are summarized in Table 6. RFT Phase. For more details, see Figure 8. 1) World Model: The World Model is frozen. 2) VLA Policy: As shown in Table 7, we adopt GRPO (Chen et al., 2025b) as the advantage estimator and configure the optimization with learning rate of 1 106 and sigma learning rate of 1 105. For stability, an auxiliary MSE loss is included with coefficient 0.01, together with an entropy regularization term of 0.003 to encourage exploration. Training is conducted for 400 steps with batch size of 16, and each update uses 16 rollouts. These settings strike balance between stability and efficiency, enabling consistent improvements under limited compute budgets. 16 Table 7: Key hyperparameters for RL fine-tuning. Hyperparameter Value"
        },
        {
            "title": "Advantage estimator GRPO\nLearning rate\nSigma learning rate\nMSE loss coefficient\nEntropy coefficient\nTotal training steps\nBatch Size\nRollout Times",
            "content": "1 106 1 105 0.01 0.003 400 16 16 Table 8: Details of perturbation experiments.Task 1 and Task 2 denote different tasks, while Dim 1 and Dim 2 refer to different perturbation objects or robot states. Where KP means keep original states. Policy. Object Position Goal Position Robot Initial States Task1 Dim1 SR (%) Task1 Dim2 SR (%) Task2 Dim1 SR (%) Task2 Dim2 SR (%) Average SR (%) Base Ours Base Ours Base Ours Base Ours Base Ours Base Ours Base Ours Base Ours 2.5 2.5 5 5 KP KP KP KP KP KP KP KP 2.5 2.5 5 5 KP KP KP KP 2.5 2.5 5 5 KP KP KP KP 2.5 2.5 5 5 KP KP KP KP KP KP KP KP 20 20 50 50 20 20 50 50 87 94 70 72 62 34 46 60 62 42 46 64 68 34 36 52 62 44 58 68 46 42 88 92 82 86 82 92 64 60 78 50 56 92 94 48 58 54 58 52 56 36 40 8 60 58 28 30 86 90 54 60 90 94 78 72 80 30 40 69.3 73.5 48.0 52.5 74.5 79.0 44.8 51.5 73.0 76. 63.5 67.0 63.5 70.0 34.0 37.0 A.3 EXPERIMENT DETAILS Details of perturbation experiments. The details of the perturbation experiments are shown in Table 8. Task 1 and Task 2 denote different tasks, while Dim 1 and Dim 2 refer to different perturbation objects or robot states. Comparisions with other VLA methods. As shown in Table 9, VLA-RFT (Ours) consistently achieves the highest scores compared with baseline policies. Comparisons with other VLA+RL methods. Our comprehensive evaluation demonstrates that the proposed framework achieves remarkable superiority over existing approaches across multiple dimensions. Not only does our method significantly outperform state-of-the-art offline RL baselines, but it also rivals the performance of online RL methods while maintaining the practical advantages of offline training. Most notably, our world-model-based approach delivers these superior results with dramatically reduced computational overhead, requiring substantially fewer training steps than conventional alternatives. The experimental comparison reveals the distinct advantages of our approach across diverse settings. While VLA-RL operates through direct reinforcement learning in the LIBERO environment, and competing methods like ARFM, RWR, and ReinboT represent the current best practices in offline RL, our framework consistently demonstrates superior performance gains. The key innovation lies in how VLA-RFT strategically exploits the world models predictive capabilities to achieve unprecedented data efficiency, enabling faster convergence without sacrificing 17 Table 9: Performance under general settings of LIBERO suites. We report SR (%) across the four suites (Spatial, Object, Goal, and Long) and their average. VLA-RFT (ours) consistently achieves the highest scores compared with baseline policies. VLA-Adapter (Base) is the recurrence result when the Policy is Flow-matching and there is only one image input. Policy Spatial Object Goal Long Average SR (%) Rank SR (%) Rank SR (%) Rank SR (%) Rank SR (%) Rank Diffusion Policy (Chi et al., 2023) Octo (Ghosh et al., 2024) MDT (Reuss et al., 2024) OpenVLA (Kim et al., 2024) SpatialVLA (Qu et al., 2025) WorldVLA (Cen et al., 2025) CoT-VLA (Zhao et al., 2025) TraceVLA (Zheng et al., 2025) π0 (Black et al., 2024) VLA-Adapter (Wang et al., 2025) (Base) VLA-RFT (Ours) 78.3 78.9 78.5 84.7 88.2 87.6 87.5 84.6 91.2 88.4 94.4 11 9 10 7 4 5 6 8 2 3 1 92.5 85.7 87.5 88.4 89.9 96.2 91.6 85.2 93.2 88.0 94.4 4 10 9 7 6 2 5 11 3 8 68.3 84.6 73.5 79.2 78.6 83.4 87.6 75.1 93.8 92.8 95.4 11 5 10 7 8 6 4 9 2 3 1 50.5 51.1 64.8 53.7 55.5 60.0 69.0 54.1 74.2 77.2 80.2 11 10 5 9 7 6 4 8 3 2 1 72.4 75.1 76.1 76.5 78.1 81.8 81.1 74.8 88.1 86.6 91.1 11 9 8 7 6 4 5 10 2 3 performance quality. For transparency and reproducibility, we note that VLA-RL results are sourced directly from the original publication, while the performance metrics for ARFM, RWR, and ReinboT on LIBERO are derived from the ARFM paper, ensuring fair and comprehensive benchmarking across all methods. Table 10: Comparison with other RL methods on Libero Average. We report baseline success rate (SR), fine-tuned SR, their improvement (), and training steps. Type Algorithm Baseline SR (%) SR (%) SR (%) Training Steps Online VLA-RL (Lu et al., 2025) Offline ARFM (Zhang et al., 2025b) RWR (Peters & Schaal, 2007) ReinboT (Zhang et al., 2025c) Ours VLA-RFT 76. 88.1 88.1 88.1 86.6 81.0 92.1 90.8 91.2 91.1 4. 4.0 2.7 3.1 4.5 10,000 40,000 40,000 40,000 400 Visualization. We also provide more detailed visualization results in Figure 6 and Figure 7. Figure 6: Comparison of original and disturbed scenarios. A.4 THE USE OF LARGE LANGUAGE MODELS (LLMS) To enhance the readability and coherence of this paper, we employed large language models to assist in refining the writing. 18 Figure 7: Comparison of base policy and VLA-RFT. Figure 8: Detailed Implementation of Method."
        }
    ],
    "affiliations": [
        "BUPT",
        "Fudan University",
        "Hebei University of Technology",
        "OpenHelix Team",
        "Westlake University",
        "Zhejiang University",
        "Zhengzhou University"
    ]
}