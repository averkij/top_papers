{
    "paper_title": "LiftFeat: 3D Geometry-Aware Local Feature Matching",
    "authors": [
        "Yepeng Liu",
        "Wenpeng Lai",
        "Zhou Zhao",
        "Yuxuan Xiong",
        "Jinchi Zhu",
        "Jun Cheng",
        "Yongchao Xu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Robust and efficient local feature matching plays a crucial role in applications such as SLAM and visual localization for robotics. Despite great progress, it is still very challenging to extract robust and discriminative visual features in scenarios with drastic lighting changes, low texture areas, or repetitive patterns. In this paper, we propose a new lightweight network called \\textit{LiftFeat}, which lifts the robustness of raw descriptor by aggregating 3D geometric feature. Specifically, we first adopt a pre-trained monocular depth estimation model to generate pseudo surface normal label, supervising the extraction of 3D geometric feature in terms of predicted surface normal. We then design a 3D geometry-aware feature lifting module to fuse surface normal feature with raw 2D descriptor feature. Integrating such 3D geometric feature enhances the discriminative ability of 2D feature description in extreme conditions. Extensive experimental results on relative pose estimation, homography estimation, and visual localization tasks, demonstrate that our LiftFeat outperforms some lightweight state-of-the-art methods. Code will be released at : https://github.com/lyp-deeplearning/LiftFeat."
        },
        {
            "title": "Start",
            "content": "LiftFeat: 3D Geometry-Aware Local Feature Matching Yepeng Liu1, Wenpeng Lai2, Zhou Zhao3, Yuxuan Xiong1, Jinchi Zhu1, Jun Cheng4, and Yongchao Xu1 5 2 0 2 6 ] . [ 1 2 2 4 3 0 . 5 0 5 2 : r it Abstract Robust and efficient local feature matching plays crucial role in applications such as SLAM and visual localization for robotics. Despite great progress, is still very challenging to extract robust and discriminative visual features in scenarios with drastic lighting changes, low texture areas, or repetitive patterns. In this paper, we propose new lightweight network called LiftFeat, which lifts the robustness of raw descriptor by aggregating 3D geometric feature. Specifically, we first adopt pre-trained monocular depth estimation model to generate pseudo surface normal label, supervising the extraction of 3D geometric feature in terms of predicted surface normal. We then design 3D geometryaware feature lifting module to fuse surface normal feature with raw 2D descriptor feature. Integrating such 3D geometric feature enhances the discriminative ability of 2D feature description in extreme conditions. Extensive experimental results on relative pose estimation, homography estimation, and visual localization tasks, demonstrate that our LiftFeat outperforms some lightweight state-of-the-art methods. Code will be released at : https://github.com/lyp-deeplearning/LiftFeat. I. INTRODUCTION Local feature matching between images is critical for many core robotic tasks, including Structure from Motion (SfM) [1], [2], [3], Simultaneous Localization and Mapping (SLAM) [4], [5], [6], [7], and visual localization [8], [9], [10], [11]. In practical applications, there are some scenes with extreme conditions, such as significant variation of illumination, and the presence of textureless or repetitive patterns. In these extreme conditions, achieving reliable feature matching still remains challenging task. Traditional local feature matching methods typically involve three stages: keypoint detection, descriptor extraction, and feature matching. Early methods such as SIFT [12] and SURF [13] propose well-designed handcrafted descriptors. During the feature matching stage, nearest neighbor matching is commonly employed to obtain the matching results. In recent years, deep learning-based feature matching methods have significantly improved the performance of traditional algorithms [14], [15]. Some studies have jointly trained keypoint prediction and descriptor extraction [16], [17], [18], which not only increases processing speed but also further optimizes matching performance. Additionally, other studies have introduced graph neural networks [19], [20], framing the feature matching task as an optimal transport problem, thereby effectively improving matching accuracy. 1School of Computer Science, Wuhan University, Wuhan, China; 2SF Technology, Shenzhen, China; 3School of Computer Science, Central China Normal University and the Hubei Engineering Research Center for Intelligent Detection and Identification of Complex Parts, Wuhan, China; 4Institute for Infocomm Research, A*STAR, Singapore. (: Equal contribution.) (Corresponding author: Yongchao Xu, yongchao.xu@whu.edu.cn) Fig. 1: Feature matching of applying 2D visual cues and integrating 3D geometric cues in low texture scene. Green lines: correct matches; Red lines: incorrect matches. Histograms: distribution of descriptor features. (a) Result of using SuperPoint [14]. (b) 3D geometric Normal Map. (c) Result of using our LiftFeat. Incorporating 3D information enhances the distinctiveness of the raw 2D descriptors. Despite the advanced performance of current methods in most scenarios, 2D visual cues can cause confusion in feature matching for scenes with extreme conditions, including significant illumination variation, low texture, or repetitive patterns. As shown in Fig. 1, in textureless scenes, raw 2D descriptors may lead to incorrect matches due to insufficient discriminative visual information. An intuitive idea is to leverage the additional information from 3D data to enhance the robustness of feature matching. However, the precision and cost of using 3D data introduce new challenges, particularly in scenarios like robotics, where computational power is limited. In this paper, we focus on designing lightweight model that integrates 2D and 3D cues for local feature matching. Depth maps is one of the most accessible 3D cue. Yet, depth maps exhibit scale ambiguity, making them unsuitable for direct use in local feature matching. In contrast, surface normal possesses both translation and scale invariance, which is suitable for feature matching. Therefore, we incorporate surface normal estimation head into the network to learn 3D geometric knowledge. Notably, the pseudo surface normal labels are derived from depth maps predicted by Depth Anything v2 [21], which eliminates the need for additional annotation costs during training. Subsequently, we propose 3D Geometry-aware Feature Lifting (3D-GFL) module to fuse the raw 2D description with the 3D normal feature, lifting the discriminative ability of raw 2D descriptors in challenging scenarios. Experimental results demonstrate that our proposed method termed LiftFeat achieves state-of-theart performance across multiple tasks: relative pose estimation, homography estimation, and visual localization. The main contributions of this work are as follows: 1) We propose lightweight network named LiftFeat, which innovatively introduces 3D geometry for local feature matching. 2) We design 3D Geometry-aware Feature Lifting (3DGFL) module that fuses 2D description with 3D normal feature, significantly improving the discriminative ability of raw 2D descriptors in challenging scenarios. 3) Experiments on different tasks confirm that our method achieves high accuracy and robustness across various scenarios. Additional runtime tests confirm that our method can achieve inference latency of 7.4 ms on edge devices. II. RELATED WORK A. Local Feature Matching Local feature matching is fundamental module in downstream applications such as visual localization [11], [22], simultaneous localization and mapping (SLAM) [6], [23]. It typically involves three key steps: feature detection, feature description, and feature matching. Traditional algorithms like SIFT [12] and ORB [24] focus on designing features that are invariant to scale, rotation, and illumination changes. Due to their ease of deployment, these methods are still widely used in robotics applications today. With the development of deep learning, learning-based feature matching methods have achieved better matching performance. Some methods focus on jointly training keypoint detection and descriptor tasks [14], [17], [25], improving both efficiency and accuracy through multi-task optimization design. In addition to keypoint detection and feature extraction, some methods have focused on improving feature matching performance. For instance, SuperGlue [19] and LightGlue [20] used graph neural networks (GNNs) and optimal transport optimization to effectively associate sparse local features while filtering outliers. However, these methods are not specifically designed for robotic platforms, and their inference time is relatively large. some works designed lightweight networks for mobile VSLAM systems. Yao et al. [26] designed compact 32-dimensional descriptor using LocalPCA, enabling efficient storage and computation. Su et al. [27] combined ORB and SuperPoint features, improving the accuracy in VSLAM systems. specifically Recently, have Different with these methods, we introduce 3D geometric features while maintaining lightweight design, enhancing the robustness of feature matching under extreme conditions in robotic applications. B. Feature Matching Leveraging 3D Information. 3D features have been widely used in many downstream tasks [28], [29], but their direct application in local feature matching has been relatively unexplored. In earlier study, Toft et al. [30] improved image matching performance under large viewpoint changes by using features corrected through monocular depth estimation, but they did not directly leverage 3D features. Recently, Karpur et al. [31] introduced object spatial coordinate prediction in the object matching task and combined 3D coordinates with 2D features using an additional SuperGlue network. Mao et al. [32] adopted multi-modal training approach using combination of depth maps and RGB inputs, enabling dense matching network to learn implicit 3D features. These methods have issues such as not directly utilizing 3D features and being highly timeconsuming. In this paper, we focus on designing lightweight network that explicitly utilizes 3D features. We integrate surface normal prediction into the feature matching network, enhancing feature distinctiveness while maintaining efficiency. III. METHOD To address the limitations of image feature matching in extreme scenarios, we propose novel approach that leverages surface normal information from depth maps to enhance descriptor matching. In this section, we first present the network architecture of the proposed LiftFeat. Next, we explain how prior knowledge from monocular depth estimation model is used to supervise the learning of surface normals. Furthermore, we introduce the 3D Geometry-aware Feature Lifting (3D-GFL) module that fuses surface normal information with original 2D descriptors. Finally, we introduce the network training details. A. Network Architecture As shown in Fig. 2, to achieve better balance between accuracy and speed, we design network architecture that consists of shared feature encoding module and multiple task-specific heads. Feature Encoding. Let the input image RW H3, where and represent the width and height of the image, respectively. In the feature encoding module, we employ 5 blocks for feature extraction. All the blocks consist of 3 3 convolution layers followed by max-pooling layers with stride of 2. The output feature map from Block5 has spatial resolution of 32 . The depth of the feature maps increases progressively across the blocks, with the output depths of the 5 blocks being {4, 8, 16, 32, 64}, respectively. Subsequently, fusion block performs multi-scale feature fusion on the lower-level features. We use 1 1 convolutions and bilinear interpolation to align and sum the features from Block3, 32 Fig. 2: Overview of the proposed LiftFeat. Given an input image I, the feature extraction module outputs keypoint map, description map, and normal map through separate multi-task heads. During the training phase, we use the predicted depth map from the Depth Anything v2 [21] to obtain pseudo normal label as supervisory signal to assist in learning 3D geometric features. Finally, the 3D geometric-aware feature lifting module fuses the 2D and 3D features. 8 64. Block4, and Block5, resulting in fused feature map of size 8 Multi-task Head. Our multi-task head is designed to predict keypoints, descriptors, and surface normals. The keypoint branch adopts strategy similar to SuperPoint, where 1 1 convolution is applied to generate the keypoint map of size 8 (64 + 1). channel-wise softmax operation is then performed to obtain the keypoint score distribution at the original image resolution. For the descriptor branch, we use bilinear interpolation and L2-normalization operation to obtain descriptor map of size 64. Similarly, the normal head uses bilinear interpolation to obtain 3-channel map with the same resolution as the original image. 8 3D Geometry-aware Feature Lifting Module. Based on the keypoint information, we sample descriptors and normal features, which are then fed into 3D-GFL module to enhance the extracted features. Assuming the keypoint branch predicts keypoints RN2 through Non-Maximum Suppression (NMS). We then perform grid sample operation to extract the corresponding descriptor RN64 and normal vector features RN3. Next, we align their feature dimensions by adding an MultiLayer Perception (MLP) layer, followed by summing the aligned features. Finally, we apply stacked self-attention layers to obtain the lifted descriptors dl RN64. B. 3D Geometric Knowledge Supervision The surface normal describes the orientation of points on surface and serves as 3D signal with both translational and scale invariance. During the training phase, we utilize the monocular depth estimation model, Depth Anything v2 [21], to generate supervision labels for the surface normals. Although monocular depth estimation inherently suffers from scale ambiguity, converting depth information into surface normals allows us to effectively mitigate this issue. This transformation enhances the performance of feature matching by providing robust geometric cues that are invariant to scale and translation. Given an input image I, depth estimation model is used to generate the corresponding depth map ZI. For pixel P(u, v) in the image, its normal vector can be estimated based on the local gradient information. Let ZI(u, v) be the depth value at this point, and the depth gradients in the and directions can be approximated using finite differences as follows: ZI ZI ZI(u + 1, v) ZI(u 1, v), ZI(u, + 1) ZI(u, 1). (1) (2) Using these depth gradients, we can estimate the normal vector at point P(u, v), denoted as nP. Assuming the 3D coordinate of this point is (u, v, ZI(u, v)), the normal vector nP can be calculated as: nP = ( ZI (cid:16) ZI , ZI , ZI , 1) , 1 (cid:17)(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) , (3) where the denominator represents the magnitude of the vector, ensuring that the normal vector is normalized. The normal vector nP provides information about the local surface orientation at point P(u, v), reflecting the 3D geometric structure of the object. C. 3D Geometry-aware Feature Lifting We integrate the 2D descriptors of keypoints with the 3D surface normal information using feature aggregation module. Specifically, for each keypoint at coordinates pi, we employ the grid sample operation to sample the corresponding local feature descriptor di and the predicted surface normal vector ni. Since the dimensions of the descriptors and normals vector is different, we employ separate multilayer perceptron (MLP) layers to align the feature dimensions and perform the addition operation. Then, we use positional encoding (PE) [20], [33] to integrate the keypoint location information into the descriptor features, resulting in the mixed information mi. The calculation process is as follows: mi = PE(pi) (MLP2D(di) + MLP3D(ni)). (4) Following [34], we use stacked self-attention modules to enable the interaction and aggregation of feature information between different points. We use linear transformer layers to construct the self-attention module, which also enhances the models inference speed. For the (n + 1)th layer, the corresponding feature mn+1 RD (D = 64 in this paper) of RD ith keypoint is aggregated from the original feature mn of the nth layer and the features of all other keypoints in P: mn+1 = (mn mi ) jP Softmax(mn jW ) (mn jW ), (5) where represents the feature corresponding to point in the set P. j represent linear mapping layers. In the experiment, we use 3 self-attention layers. and mi, D. Network Training We supervise the network using pixel-level matching labels from paired images. The training data is derived from synthetic data or the Megadepth dataset [35]. Given pair of input images (IA, IB), we compute three types of losses: keypoint prediction loss Lkeypoint , surface normal estimation loss Lnormal, and descriptor loss Ldesc. 1) Keypoint Loss: For keypoint supervision, we adopt same strategy of SuperPoint [14]. The original output of keypoint logits map is ( 8 65), where the last channel represents no keypoint. We use the output of ALIKE detector [25] as the ground-truth keypoint labels. The keypoint loss Lkeypoint is computed by applying the Negative LogLikelihood (NLL) loss on the keypoint logits map. 8 2) Normal Loss: The normal vector estimation loss is applied to ensure accurate surface orientation predictions. For each predicted normal vector, we compare it with the groundtruth normal vector using the cosine similarity, ensuring that the predicted normal aligns with the true surface normal. The normal loss is defined as: Lnormal = 1 npred ngt npredngt , (6) where npred and ngt are the predicted and ground-truth normal vectors, respectively. 3) Descriptor Loss: Given the descriptors sampled from image IA and IB, we feed they to the feature fusion module to obtain the descriptors (dA Rm64, dB Rn64). Sequentially, we compute the similarity score matrix Rmn. The ground truth matching matrix is denoted as Mgt. Following [19], we minimize the negative log-likelihood of the predicted matching score matrix with respect to the ground-truth matching matrix Mgt: Ldesc = i, Mgt(i, j) log S(i, j). (7) 4) Total Loss: The total loss for training is the weighted sum of these three components: Ltotal = Lkeypoint + α1Lnormal + α2Ldesc, (8) where α1, and α2 are weighting factors that balance the contributions of the keypoint loss, normal loss, and descriptor loss, respectively. In this experiment, we empirically set α1 and α2 to 2 and 1, respectively. IV. EXPERIMENTS We evaluate the proposed LiftFeat on three tasks: relative pose estimation, homography estimation and visual localization. The implementation details, comparative methods, and some qualitative illustrations are given in the following. Implementation Details. We implement the proposed algorithm based on PyTorch. During the training phase, we use the pre-trained Depth-Anything v2 model [21] to generate pseudo surface normal. The training dataset is composed of mixed dataset from MegaDepth [35] and synthetic COCO [39]. The input image size is 800600 pixels. The model is optimized using the Adam optimizer with an initial learning rate of 1e-4 and batch size of 16. During training, we sample 1024 pairs of matching points to fine-tune the feature aggregation module. The model training is completed in 32 hours on an NVIDIA RTX 3090. To verify the robustness of the method, we do not perform additional fine-tuning in all experiments. Comparative methods. Due to the computational limitations of the robot, we select some lightweight baseline methods: ORB [24], SuperPoint [14], ALIKE [25], SilK [37] and XFeat [38]. For SiLK and ALIKE, we choose their smallest available backbones ALIKE-Tiny and VGG-aligning with our focus on computationally efficient models. For all baselines, we use the top 4096 detected keypoints. During matching, we employe mutual nearest neighbor (MNN) search. Qualitative illustrations. Fig. 3 illustrates the visualization results in scenarios with low textures, repetitive patterns, and lighting variations. Our proposed LiftFeat enhances the discriminative ability of descriptors by incorporating 3D geometric features, improving the accuracy of feature matching in extreme conditions. A. Relative Pose Estimation Datasets. We evaluate our model on two commonly used datasets: MegaDepth-1500 [35] and ScanNet [36]. These images include challenging scenes with significant variations in viewpoint and lighting conditions. MegaDepth-1500 [35] is an outdoor dataset containing multiple scenes. ScanNet [36] is an indoor RGB-D dataset consisting of 1613 sequences and 2.5 million views, each accompanied by ground-truth camera Fig. 3: Qualitative matching results. We conduct tests in both indoor and outdoor scenes. The results demonstrate that our proposed LiftFeat maintains robust matching performance under extreme conditions, such as lighting variations (top), low texture (middle), and repetitive pattern (bottom) scenarios. Green lines: correct matches; Red lines: incorrect matches. TABLE I: Relative pose results on MegaDepth-1500 and ScanNet. We report the AUC scores of translation and rotation errors at different thresholds. The best results are in bold, and the second-best are underlined. Methods MegaDepth-1500 [35] ScanNet [36] AUC@5 AUC@10 AUC@20 AUC@5 AUC@10 AUC@20 ORB [24], ICCV2011 SuperPoint [14], CVPRW2018 ALIKE [25], TIM2023 SiLK [37], ICCV2023 XFeat [38], CVPR2024 LiftFeat (Ours) 17.9 37.3 40.5 39.9 42.6 44.7 27.6 50.1 56.9 55.1 56.4 59.5 39.0 61.5 68.2 66.9 67.7 70. 9.0 12.5 9.8 15.9 16.7 18.5 18.5 24.4 19.5 30.1 32.6 34.9 29.9 36.7 30.3 44.5 47.8 51.2 poses and depth maps. Following the setup of XFeat [38], the maximum size of MegaDepth [35] is 1200 pixels, while VGA resolution is used for testing images in ScanNet [36]. TABLE II: Homography estimation results on HPatches [41]. We report mean homography accuracy at different thresholds. The best are in bold, and the second-best are underlined. Metrics. Following [20], [38], we report the Area Under the recall Curve (AUC) for translation and rotation errors at various thresholds (5, 10, 20). The pose is computed using the essential matrix through the MAGSAC++ [40] algorithm. Results. As shown in Tab. I, we present the results of pose estimation in both indoor and outdoor scenes. Compared to the newest lightweight network XFeat [38], we achieve significant improvements in AUC@5, AUC@10, and AUC@20 under the matching of 4096 sparse keypoints. Compared to SuperPoint [14], our approach demonstrates advantages in both accuracy and speed. This indicates that incorporating 3D geometric knowledge can significantly improve the accuracy of pose estimation. B. Homography Estimation Datasets. We use the widely adopted HPatches [41] dataset to evaluate homography. HPatches [41] consists of planar sequences with various lighting and viewpoint changes. Each Methods Illumination Viewpoint @3 @5 @7 @3 @5 @7 ORB [24] SuperPoint [14] ALIKE [25] SiLK [37] XFeat [38] LiftFeat (Ours) 74.6 94.6 94.6 78.5 95.0 95.6 84.6 98.5 98.5 82.3 98.1 98.8 85.4 98.8 99.6 83.8 98.8 99.2 63.2 71.1 68.2 48.6 68.6 71. 71.4 79.6 77.5 59.6 81.1 81.7 78.6 83.9 81.4 62.5 86.1 87.5 scene contains 5 image pairs, accompanied by ground truth homography matrices. Metrics. Following [25], we report the Mean Homography Accuracy (MHA) metric. The MHA measures the proportion of images where the average error between the mapped and ground truth corner points, calculated using the estimated homography matrix, falls within pixel threshold. In our experiments, we set different thresholds of {3, 5, 7} pixels. Results. Tab. II shows the results of HPatches [41] under varying illumination and viewpoint conditions. Our method TABLE III: Visual localization on Aachen Day-Night [42]. We report the pose recall at (0.25m/2, 0.5m/5, 5m/10). The best are in bold, and the second-best are underlined. Methods Day Night 0.25m, 2 0.5m, 5 5m, 10 0.25m, 2 0.5m, 5 5m, 10 ORB [24] SuperPoint ALIKE XFeat LiftFeat (Ours) 66.9 87.4 85.7 84.7 87.6 76.1 93.2 92.4 91.5 93. 93.7 97.0 96.7 96.5 97.1 10.2 77.6 81.6 77.6 82.1 12.2 85.7 88.8 89.8 89.9 19.4 95.9 99.0 98.0 99.1 TABLE IV: Ablation study on visual localization task with night subset of Aachen Day-Night dataset [42]. The default setting includes only keypoint and description prediction. Methods (0.25m, 2) (0.5m, 5) (5m, 10) Default + Normal Head + 3D-GFL 78.9 79.4 82.1 86.1 87.9 89. 97.6 98.2 99.1 generally outperforms other algorithms. Particularly in scenarios with large viewpoint changes, geometric distortions can cause significant alterations in the appearance features on 2D images. Introducing 3D information can help mitigate this effect. C. Visual Localization Datasets. We demonstrate the performance of our approach on the Aachen Day-Night v1.1 [42] dataset for visual localization tasks. This dataset presents challenges in terms of illumination changes and contains 6,697 daytime database images along with 1,015 query images (824 captured during the day and 191 at night). The ground truth 6DoF camera poses are obtained using COLMAP [1]. During testing, we resize the images to maximum dimension of 1024 pixels and extract the top 4096 keypoints from all methods. Metrics. We use the hierarchical localization toolbox (HLoc) [9] by replacing the feature extraction module with different feature detectors and descriptors. Then, we report the accuracy of correctly estimated camera poses within position error thresholds of 0.25m, 0.5m, 5m and rotation error thresholds of 2, 5, 10. Results. Tab. III shows results of visual localization. Our method outperforms ALIKE [25] and XFeat [38], in both daytime and nighttime scenarios. Compared with the widelyused industrial algorithm SuperPoint [14], our performance in daytime scenarios is comparable. However, in nighttime scenarios, under the threshold of (0.25m/1), we improve the success rate from 77.6% to 82.4%. This suggests that in nighttime scenes, 3D cues can generate more distinctive features under the same conditions. D. Ablation Study In this section, we analyze the impact of adding normal head to learn the 3D geometric knowledge and the 3DGFL module. Our baseline setup only includes the keypoint detection and raw description prediction. We conduct the TABLE V: Comparison of computation resources. SuperPoint [14] XFeat [38] Ours Params (M) FLOPs (G) Desc. Dimension runtime/CPU (ms) runtime/GPU (ms) 1.30 19.85 256-f 227 36 0.66 1.33 64-f 35 5.6 0.85 4.96 64-f 62 7.4 ablation study on highly challenging nighttime visual localization test set. From Tab. IV, it can be observed that adding multi-task heads in an implicit manner yields gains of (0.5%, 1.8%, 0.6%). On this basis, explicit feature aggregation further improves the accuracy, achieving gains of (2.7%, 2.0%, 0.9%). E. Runtime Analyse We compare the resource requirements for deploying two widely used methods, SuperPoint [14] and XFeat [38], on edge devices. For the CPU, we selecte an Intel(R) Core(TM) i7-10700 CPU @ 2.90GHz, and for the mobile GPU, we choose the commonly used Nvidia Xavier NX. We use feed the real-time VGA data into the network. As shown in Tab. V, while our method is slightly slower than XFeat [38], it outperforms XFeat three tasks. Compared to SuperPoint [14], our method is 5 times faster and also more accurate. This demonstrates that our approach achieves good balance between accuracy and speed. in accuracy across all V. CONCLUSIONS In this paper, we present novel lightweight network for 3D geometry-aware local feature matching. We propose to learn surface normal for encoding the 3D geometric feature. For that, we leverage the depth anything model to estimate depth map, based on which we derive the pseudo surface normal for supervision. The proposed method termed LiftFeat then effectively aggregates 3D geometry feature of learned surface normal into raw 2D description. This lifts the discrimination ability of visual feature, in particular for scenes with extreme conditions such as significant lighting changes, low textures, or repetitive patterns. The superiority over some lightweight state-of-the-art methods is validated on three tasks: relative pose estimation, homography estimation and visual localization. Acknowledgment. This work was supported in part by the National Key Research and Development Program of China (2023YFC2705700), NSFC 62222112, and 62176186, the Innovative Research Group Project of Hubei Province under Grants (2024AFA017), the Postdoctoral Fellowship Program of CPSF (No. GZC20230924), the Open Projects funded by Hubei Engineering Research Center for Intelligent Detection and Identification of Complex Parts (No. IDICPKF-2024-03), Hubei Provincial Natural Science Foundation (No. 2024AFB245), and the Agency for Science, Technology and Research under its MTC Programmatic Funds Grant No.M23L7b0021."
        },
        {
            "title": "REFERENCES",
            "content": "[1] J. L. Schonberger and J.-M. Frahm, Structure-from-motion revisited, in Proc. IEEE Conf. Comput. Vis. Pattern Recognit.(CVPR), pp. 4104 4113, 2016. [2] S. Jiang, C. Jiang, and W. Jiang, Efficient structure from motion for large-scale uav images: review and comparison of sfm tools, ISPRS Journal of Photogrammetry and Remote Sensing, vol. 167, pp. 230251, 2020. [3] H. Zhao, H. Wang, X. Zhao, H. Wang, Z. Wu, C. Long, and H. Zou, Automated 3d physical simulation of open-world scene with gaussian splatting, arXiv preprint arXiv:2411.12789, 2024. [4] A. J. Davison, I. D. Reid, N. D. Molton, and O. Stasse, Monoslam: Real-time single camera slam, IEEE Trans. on Pattern Anal. and Mach. Intell., vol. 29, no. 6, pp. 10521067, 2007. [5] R. Mur-Artal and J. D. Tardos, Orb-slam2: An open-source slam system for monocular, stereo, and rgb-d cameras, IEEE Trans. Robot., vol. 33, no. 5, pp. 12551262, 2017. [6] C.-M. Chung, Y.-C. Tseng, Y.-C. Hsu, X.-Q. Shi, Y.-H. Hua, J.-F. Yeh, W.-C. Chen, Y.-T. Chen, and W. H. Hsu, Orbeez-slam: real-time monocular visual slam with orb features and nerf-realized mapping, in 2023 IEEE International Conference on Robotics and Automation (ICRA), pp. 94009406, 2023. [7] J. Liu, G. Wang, Z. Liu, C. Jiang, M. Pollefeys, and H. Wang, Regformer: An efficient projection-aware transformer network for large-scale point cloud registration, in Proc. IEEE Int. Conf. Comput. Vis.(ICCV), pp. 84518460, 2023. [8] T. Sattler, B. Leibe, and L. Kobbelt, Efficient & effective prioritized matching for large-scale image-based localization, IEEE Trans. on Pattern Anal. and Mach. Intell., vol. 39, no. 9, pp. 17441756, 2016. [9] P.-E. Sarlin, C. Cadena, R. Siegwart, and M. Dymczyk, From coarse to fine: Robust hierarchical localization at large scale, in Proc. IEEE Conf. Comput. Vis. Pattern Recognit.(CVPR), pp. 1271612725, 2019. [10] S. Wang, Z. Laskar, I. Melekhov, X. Li, and J. Kannala, Continual learning for image-based camera localization, in Proc. IEEE Int. Conf. Comput. Vis.(ICCV), pp. 32523262, 2021. [11] P. Yin, I. Cisneros, S. Zhao, J. Zhang, H. Choset, and S. Scherer, isimloc: Visual global localization for previously unseen environments with simulated images, IEEE Transactions on Robotics, vol. 39, no. 3, pp. 18931909, 2023. [12] D. G. Lowe, Distinctive image features from scale-invariant keypoints, Intl. Journal of Computer Vision, vol. 60, pp. 91110, 2004. [13] H. Bay, T. Tuytelaars, and L. Van Gool, Surf: Speeded up robust features, in Proc. Eur. Conf. Comput. Vis.(ECCV), pp. 404417, 2006. [14] D. DeTone, T. Malisiewicz, and A. Rabinovich, Superpoint: Selfsupervised interest point detection and description, in Proc. of IEEE Conf. Comput. Vis. Pattern Recognit. Workshops, pp. 224236, 2018. [15] Y. Liu, B. Yu, T. Chen, Y. Gu, B. Du, Y. Xu, and J. Cheng, Progressive retinal image registration via global and local deformable transformations, in 2024 IEEE International Conference on Bioinformatics and Biomedicine (BIBM), pp. 21832190, 2024. [16] M. Dusmanu, I. Rocco, T. Pajdla, M. Pollefeys, J. Sivic, A. Torii, and T. Sattler, D2-net: trainable cnn for joint description and detection of local features, in Proc. IEEE Conf. Comput. Vis. Pattern Recognit.(CVPR), pp. 80928101, 2019. [17] J. Revaud, C. De Souza, M. Humenberger, and P. Weinzaepfel, R2d2: Reliable and repeatable detector and descriptor, Proc. Adv. Neural Inf. Process. Syst., vol. 32, 2019. [18] M. Tyszkiewicz, P. Fua, and E. Trulls, Disk: Learning local features with policy gradient, Proc. Adv. Neural Inf. Process. Syst., vol. 33, pp. 1425414265, 2020. [19] P.-E. Sarlin, D. DeTone, T. Malisiewicz, and A. Rabinovich, Superglue: Learning feature matching with graph neural networks, in Proc. IEEE Conf. Comput. Vis. Pattern Recognit.(CVPR), pp. 49384947, 2020. [20] P. Lindenberger, P.-E. Sarlin, and M. Pollefeys, Lightglue: Local feature matching at light speed, in Proc. IEEE Int. Conf. Comput. Vis.(ICCV), pp. 1762717638, 2023. [21] L. Yang, B. Kang, Z. Huang, X. Xu, J. Feng, and H. Zhao, Depth anything: Unleashing the power of large-scale unlabeled data, in Proc. IEEE Conf. Comput. Vis. Pattern Recognit.(CVPR), pp. 1037110381, 2024. [22] J. Liu, Q. Nie, Y. Liu, and C. Wang, Nerf-loc: Visual localization with conditional neural radiance field, in 2023 IEEE International Conference on Robotics and Automation (ICRA), pp. 93859392, 2023. [23] A. Adkins, T. Chen, and J. Biswas, Obvi-slam: Long-term objectvisual slam, IEEE Robotics and Automation Letters, 2024. [24] E. Rublee, V. Rabaud, K. Konolige, and G. Bradski, Orb: An efficient alternative to sift or surf, in Proc. IEEE Int. Conf. Comput. Vis.(ICCV), pp. 25642571, 2011. [25] X. Zhao, X. Wu, J. Miao, W. Chen, P. C. Chen, and Z. Li, Alike: Accurate and lightweight keypoint detection and descriptor extraction, IEEE Trans. on Multimedia, vol. 25, pp. 31013112, 2022. [26] H. Yao, N. Hao, C. Xie, and F. He, Edgepoint: Efficient point detection and compact description via distillation, in 2024 IEEE International Conference on Robotics and Automation (ICRA), pp. 766 772, 2024. [27] X. Su, S. Eger, A. Misik, D. Yang, R. Pries, and E. Steinbach, Hpfslam: An efficient visual slam system leveraging hybrid point features, in 2024 IEEE International Conference on Robotics and Automation (ICRA), pp. 1592915935, 2024. [28] D. Wofk, F. Ma, T.-J. Yang, S. Karaman, and V. Sze, Fastdepth: Fast monocular depth estimation on embedded systems, in 2019 International Conference on Robotics and Automation (ICRA), pp. 6101 6108, 2019. [29] Z. Xu, X. Zhan, Y. Xiu, C. Suzuki, and K. Shimada, Onboard dynamic-object detection and tracking for autonomous robot navigation with rgb-d camera, IEEE Robotics and Automation Letters, vol. 9, no. 1, pp. 651658, 2023. [30] C. Toft, D. Turmukhambetov, T. Sattler, F. Kahl, and G. J. Brostow, Single-image depth prediction makes feature matching easier, in Proc. Eur. Conf. Comput. Vis.(ECCV), pp. 473492, 2020. [31] A. Karpur, G. Perrotta, R. Martin-Brualla, H. Zhou, and A. Araujo, Lfm-3d: Learnable feature matching across wide baselines using 3d signals, in 2024 International Conference on 3D Vision (3DV), pp. 1120, 2024. [32] R. Mao, C. Bai, Y. An, F. Zhu, and C. Lu, 3dg-stfm: 3d geometric guided student-teacher feature matching, in Proc. Eur. Conf. Comput. Vis.(ECCV), pp. 125142, 2022. [33] H. Jiang, A. Karpur, B. Cao, Q. Huang, and A. Araujo, Omniglue: Generalizable feature matching with foundation model guidance, in Proc. IEEE Conf. Comput. Vis. Pattern Recognit.(CVPR), 2024. [34] X. Wang, Z. Liu, y. Hu, W. Xi, W. Yu, and D. Zou, Featurebooster: Boosting feature descriptors with lightweight neural network, in Proc. IEEE Conf. Comput. Vis. Pattern Recognit.(CVPR), 2023. [35] Z. Li and N. Snavely, Megadepth: Learning single-view depth prediction from internet photos, in Proc. IEEE Conf. Comput. Vis. Pattern Recognit.(CVPR), pp. 20412050, 2018. [36] A. Dai, A. X. Chang, M. Savva, M. Halber, T. Funkhouser, and M. Nießner, Scannet: Richly-annotated 3d reconstructions of indoor scenes, in Proc. IEEE Conf. Comput. Vis. Pattern Recognit.(CVPR), pp. 58285839, 2017. [37] P. Gleize, W. Wang, and M. Feiszli, Silk: Simple learned keypoints, in Proc. IEEE Int. Conf. Comput. Vis.(ICCV), pp. 2249922508, 2023. [38] G. Potje, F. Cadar, A. Araujo, R. Martins, and E. R. Nascimento, Xfeat: Accelerated features for lightweight image matching, in Proc. IEEE Conf. Comput. Vis. Pattern Recognit.(CVPR), pp. 26822691, 2024. [39] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Dollar, and C. L. Zitnick, Microsoft coco: Common objects in context, in Proc. Eur. Conf. Comput. Vis.(ECCV), pp. 740755, 2014. [40] D. Barath, J. Noskova, M. Ivashechkin, and J. Matas, Magsac++, fast, reliable and accurate robust estimator, in Proc. IEEE Conf. Comput. Vis. Pattern Recognit.(CVPR), pp. 13041312, 2020. [41] V. Balntas, K. Lenc, A. Vedaldi, and K. Mikolajczyk, Hpatches: benchmark and evaluation of handcrafted and learned local descriptors, in Proc. IEEE Conf. Comput. Vis. Pattern Recognit.(CVPR), pp. 51735182, 2017. [42] T. Sattler, W. Maddern, C. Toft, A. Torii, L. Hammarstrand, E. Stenborg, D. Safari, M. Okutomi, M. Pollefeys, J. Sivic, et al., Benchmarking 6dof outdoor visual localization in changing conditions, in Proc. IEEE Conf. Comput. Vis. Pattern Recognit.(CVPR), pp. 8601 8610, 2018."
        }
    ],
    "affiliations": [
        "Institute for Infocomm Research, A*STAR, Singapore",
        "SF Technology, Shenzhen, China",
        "School of Computer Science, Central China Normal University and the Hubei Engineering Research Center for Intelligent Detection and Identification of Complex Parts, Wuhan, China",
        "School of Computer Science, Wuhan University, Wuhan, China"
    ]
}