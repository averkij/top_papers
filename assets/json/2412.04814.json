{
    "paper_title": "LiFT: Leveraging Human Feedback for Text-to-Video Model Alignment",
    "authors": [
        "Yibin Wang",
        "Zhiyu Tan",
        "Junyan Wang",
        "Xiaomeng Yang",
        "Cheng Jin",
        "Hao Li"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advancements in text-to-video (T2V) generative models have shown impressive capabilities. However, these models are still inadequate in aligning synthesized videos with human preferences (e.g., accurately reflecting text descriptions), which is particularly difficult to address, as human preferences are inherently subjective and challenging to formalize as objective functions. Therefore, this paper proposes LiFT, a novel fine-tuning method leveraging human feedback for T2V model alignment. Specifically, we first construct a Human Rating Annotation dataset, LiFT-HRA, consisting of approximately 10k human annotations, each including a score and its corresponding rationale. Based on this, we train a reward model LiFT-Critic to learn reward function effectively, which serves as a proxy for human judgment, measuring the alignment between given videos and human expectations. Lastly, we leverage the learned reward function to align the T2V model by maximizing the reward-weighted likelihood. As a case study, we apply our pipeline to CogVideoX-2B, showing that the fine-tuned model outperforms the CogVideoX-5B across all 16 metrics, highlighting the potential of human feedback in improving the alignment and quality of synthesized videos."
        },
        {
            "title": "Start",
            "content": "LiFT: Leveraging Human Feedback for Text-to-Video Model Alignment Yibin Wang 1,2 Zhiyu Tan 1,2 Junyan Wang 3 Xiaomeng Yang 2 Cheng Jin 1 Hao Li 1,2 1 Fudan University 2 Shanghai Academy of Artificial Intelligence for Science 3 Australian Institute for Machine Learning, The University of Adelaide 4 2 0 2 6 ] . [ 1 4 1 8 4 0 . 2 1 4 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Recent advancements in text-to-video (T2V) generative models have shown impressive capabilities. However, these models are still inadequate in aligning synthesized videos with human preferences (e.g., accurately reflecting text descriptions), which is particularly difficult to address, as human preferences are inherently subjective and challenging to formalize as objective functions. Therefore, this paper proposes LIFT, novel fine-tuning method leveraging human feedback for T2V model alignment. Specifically, we first construct Human Rating Annotation dataset, LIFTHRA, consisting of approximately 10k human annotations, each including score and its corresponding reason. Based on this, we train reward model LIFT-CRITIC to learn reward function effectively, which serves as proxy for human judgment, measuring the alignment between given videos and human expectations. Lastly, we leverage the learned reward function to align the T2V model by maximizing the reward-weighted likelihood. As case study, we apply our pipeline to CogVideoX-2B, showing that the fine-tuned model outperforms the CogVideoX-5B across all 16 metrics, highlighting the potential of human feedback in improving the alignment and quality of synthesized videos. Our project page is here. 1. Introduction Recent advancements in Text-to-Video (T2V) generation models [31, 11, 32, 30, 37, 12, 43] have achieved remarkable results in producing high-quality videos. These models enable users to generate dynamic videos from text, offering flexible, controllable approach to video creation. However, despite this progress, these models still suffer from issues such as artifacts, misalignment with text descriptions, and unnatural motion [22, 13]. Since these issues stem from subjective human preferences that are difficult to formalize as objective functions [18], effectively incorporating human preferences into T2V models remains challenging. Figure 1. An illustration of the proposed LIFT. First, we construct comprehensive human feedback dataset. Then, reward model is trained to learn the reward function. Finally, the T2V model is fine-tuned by the reward model to align its output with human expectations. In language modeling and the text-to-image domain, learning from human feedback to align model outputs with human intent [19, 23, 1, 27] has demonstrated efficiency. Building on this success, recent efforts in T2V generation [39, 22] have incorporated feedback from image reward models [33, 29] into the training process. Despite its effectiveness, image-based human feedback fails to capture key temporal aspects related to video, such as motion continuity and smooth transitions between frames. To address this, methods like T2VQA [18] and VideoScore [13] collect human-annotated video ratings and train video quality assessment models to better capture subjective human evaluations. However, these existing studies face two main challenges: (1) Lack of interpretability: by focusing solely on assessment outcomes (only learning ratings), these models overlook the underlying reasoning, hindering their ability to capture nuanced criteria and align with human preferences. (2) Limited human feedback guidance: video-based human feedback has not been effectively utilized to guide the alignment of T2V models, restricting their ability to meet complex and diverse human expectations. These challenges are interconnected: the lack of interpretability in existing reward models prevents them from reliably learning reward function that truly reflects human preferences, thereby limiting the effective alignment of T2V models with human expectations. We posit that the core issue lies in the absence of human feedback-based datasets where each annotation for synthesized videos includes both ratings and their underlying reasoning. Such datasets are critical for training reward models not only to understand the score but also the reason behind human judgments. Without this explanatory context, models are limited to surface-level evaluations, focusing solely on the final score, thereby failing to grasp the deeper, often subjective criteria that influence human judgments. By developing such dataset, reliable reward model could be trained, which could then be used to fine-tune T2V models for effective alignment with human expectations. Therefore, this paper proposes LIFT, novel fine-tuning method leveraging human feedback for T2V model alignment through three key stages, as illustrated in Fig. 1: (1) human feedback collection: generate video-text pairs through prompts, followed by human annotations to build comprehensive feedback dataset; (2) reward function learntrain reward model capturing human preferences ing: based on this dataset to predict human feedback scores; and (3) T2V model alignment: fine-tune the T2V model using the learned reward function to optimize its output in alignment with human expectations. Specifically, we first categorize human preferences into three key dimensions: semantic consistency, motion smoothness, and video fidelity. For these dimensions, we propose LIFT-HRA dataset that collects approximately 10K human feedback annotations, each including both ratings and the corresponding reasoning. We then train reward model, LIFT-CRITIC, based on the large multimedia model (LMM) [24] to learn human feedback-based reward function from this dataset. Unlike existing assessment models [13, 18], our model not only learns to predict ratings but also captures the reason behind them, thereby improving interpretability and providing deeper understanding of the evaluation process. Finally, we apply this learned reward function to evaluate the quality of T2V model outputs and update the model using reward-weighted likelihood maximization. In summary, this work proposes LIFT, novel finetuning pipeline for aligning T2V models using human feedback. We introduce LIFT-HRA, comprehensive dataset, and LIFT-CRITIC, reward model designed to evaluate synthesized videos by assigning ratings accompanied by detailed reason. Experimental results demonstrate that incorporating reason learning significantly improves the models alignment with human preferences. As case study, we apply our pipeline to CogVideoX-2B [37], demonstrating that the fine-tuned model surpasses CogVideoX-5B across all 16 metrics of the VBench [16] benchmark, significantly improving alignment with human preferences. 2. Related Work Diffusion-based T2V models. Recent advancements in T2V focus on leveraging large-scale datasets to train more robust models [31, 10, 7, 11, 32, 30]. common approach involves adapting pre-trained text-to-image (T2I) models by introducing temporal layers and fine-tuning them on video datasets or employing joint image-video training strategy [4, 30, 32]. To enhance video quality, multi-stage inference pipelines are often utilized [14, 32, 4], including cascaded video diffusion models. These typically consist of T2V base model followed by frame interpolation and video super-resolution models [4, 32, 42]. Despite their effectiveness, such pipelines are limited in improving spatiotemporal resolution. To address this, recent works [37, 43, 6, 12] offer diverse solutions. For example, VEnhancer [12] unifies temporal and spatial super-resolution with refinement in single model. However, these models still remain inadequate in aligning synthesized videos with human preferences, such as accurately reflecting text descriptions. We thus train reward model to capture human preferences and then use it to fine-tune T2V models. The related studies will be introduced in the following sections. Vision-and-language reward models. In T2I generation, several studies aim to incorporate human preferences into model evaluation and optimization [17, 34, 20, 33, 40]. For the T2V generation, researchers have worked on evaluation methods to benchmark the generative models [16, 25, 2]. Recent works [39, 22] have incorporated feedback from image reward models [33, 29] into the training process. Despite their effectiveness, image-based human feedback overlooks crucial temporal aspects of video which are vital for generating coherent and natural videos. To bridge this gap, methods such as T2VQA [18] and VideoScore [13] train video quality assessment models directly on humanannotated video ratings. However, these models focus solely on assessment results, overlooking the interpretability of the evaluation process. Therefore, we introduce novel reward model that evaluates synthesized videos by providing both ratings and corresponding reason. Aligning generative model use reward learning. Leveraging reward models and feedback simulators, various approaches have been developed to align visual generative models with human preferences. These methods include reinforcement learning (RL)-based techniques [9, 41, 3, 5] and reward fine-tuning approaches [8, 21, 38, 19, 28]. By integrating feedback mechanisms, these methods aim to enhance model outputs to better reflect human preferences. To date, video-based human feedback has not been fully leveraged to align T2V models with human preferences, limiting their ability to meet the complex and diverse expectations of users. Therefore, this work explores novel fine-tuning pipeline to align T2V models with human preferences. Figure 2. The overview of our proposed pipeline. This illustration depicts three key steps of our fine-tuning pipeline: (1) Human Feedback Collection: we generate video-text pairs using prompts expanded from random category words with an LLM, then annotate them to create LIFT-HRA. (2) Reward Function Learning: visual-language model LIFT-CRITIC, is trained to predict human preference scores across three dimensions, learning the reward function from the dataset. (3) T2V Model Alignment: LIFT-CRITIC evaluates the T2V-generated videos, assigns scores, and maps them into reward weight to fine-tune the T2V model, aligning it with human preferences. 3. LIFT This section introduces our fine-tuning pipeline to align text-to-video (T2V) models with human preferences using human feedback. The overall approach, illustrated in Fig. 2, comprises three key steps: First, we collect human feedback and curate the LIFT-HRA dataset (Sec. 3.1). This dataset serves as the foundation for training LIFT-CRITIC, which learns reward function that captures human preferences (Sec. 3.2). Finally, we utilize the learned reward function to refine the T2V model (Sec. 3.3), ensuring improved alignment with human evaluation criteria. 3.1. LIFT-HRA: Human Feedback Collection Existing human feedback-based text-to-video datasets [13, 18] primarily focus on outcome evaluation, such as overall video quality or video-text alignment, by collecting single evaluation scores. While effective for training models to assess high-level metrics, these datasets lack critical insights into the evaluation process, i.e., the reasoning behind the assigned scores. For reward function learning, we argue that incorporating such reasoning is crucial for accurately aligning with nuanced human preferences. To address this gap, we introduce new dataset tailored for reward model training. This dataset combines evaluation scores with their corresponding reason, enabling more holistic and interpretable alignment with human expections, which will be elaborated in the following. Video-text dataset. Our reward model is designed to evaluate synthesized videos based on human preferences. Therefore, constructing comprehensive video-text dataset is essential. Specifically, we start by generating set of diverse prompts. This involves creating selection list for six categories: humans, terrestrial animals, aquatic animals, scenes, simple actions, and complex actions. For each prompt, we randomly choose 12 subjects from the first three categories, scene from the scenes list, and an action from either the simple actions or complex actions categories. These selected elements are combined into phrase and refined into detailed textual description using LLM [36]. Finally, multiple videos are generated for each prompt using T2V models, forming the video-text dataset. Human annotation. To collect comprehensive human evaluations, we categorize human preferences into three key dimensions: semantic consistency, motion smoothness, and video fidelity. For each video-text pair in the dataset, we enlist annotators to evaluate the generated videos across these dimensions. The annotation UI is illustrated in Fig. 3. Specifically, given videos and their captions, annotators assess each video by assigning score (Good, Normal, or Bad) in each dimension based on predefined scoring criteria and provide detailed reason for their evaluation. Data Correction. After all data has been annotated, we perform three-stage cleaning process to ensure reliable data quality: 1) Coarse Filtering: We first remove data with obvious annotation errors or irrelevant responses where annotator feedback does not align with the evaluation criteria. 2) Iterative Refinement: The dataset is then split into two halves. One half trains an initial reward model, which is then used to annotate the other half. If the models output aligns with human annotations, they are retained; otherwise, labels through text generation. Existing studies [13, 23] typically adopt the first approach, training the model to predict scores directly without focusing on the reasoning behind the ratings. While this method is straightforward, it overlooks the complexity of human judgment, which involves subtle and often subjective criteria. In contrast, we adopt the second method, aiming to bridge this gap by training the model not only to predict scores but also to provide the reason behind them. This process enhances the interpretability of our reward model, enabling it to better reflect the underlying reasoning that influences human evaluations. Besides, the text generation method aligns naturally with the pre-trained VLMs capabilities in multimodal understanding and natural language generation. This synergy also enables the model to leverage its strengths in producing semantically rich and contextually meaningful outputs. Fig. 2 illustrates our supervised fine-tuning process. Specifically, the training sample is represented as triplet consisting of multimodal input includes the video and textual description, question (or instruction), and answer A. During training, the optimization objective is the autoregressive loss function commonly used in training large language models, but it is calculated solely on the answer. The loss function can be expressed as: L(ϕ) = (cid:88) i=1 log p(Ai Q, M, A<i; ϕ), (1) where is the length of the ground-truth answer. To enhance the models performance, we enrich each answer option with detailed reasoning, training the model to predict multi-level scores and generate justifications for its judgments. This added context helps the VLM better understand the meaning of each option, thereby improving its alignment with human preferences. The effectiveness of this approach is demonstrated in Tab. 2. After training, LIFT-CRITIC can predict reward score for given synthesized video and its corresponding caption. Specifically, LIFT-CRITIC evaluates the video across three dimensions, including semantic consistency, motion smoothness, and video fidelity. Since these scores are multilevel qualitative evaluations (e.g., Good, Normal, Bad), reward score mapping function s, is employed to translate these qualitative assessments into numerical values. Formally, the reward score for specific dimension is computed as: s(rϕ,d(x, z)), where rϕ represents the reward model with parameters ϕ. Finally, the overall reward score for synthesized video is obtained by averaging the scores across all dimensions: rϕ(x, z) = 1 (cid:88) dD s(rϕ,d(x, z)), (2) where is the synthesized video, and is the associated caption. This reward score effectively substitutes human Figure 3. An illustration of our annotation UI. Annotators evaluate each video by assigning scores to each dimension and providing the reason behind their assessments. human annotators decide which to keep. The cleaned data is used to retrain the model, refining the other half. 3) Final Integration: Using the fully cleaned dataset, we train final reward model. This model is then used to re-annotate the data that was removed during the first stage, and the newly annotated data is incorporated back into the dataset. Statistical analysis. Finally, we collect approximately 10K high-quality human feedback video quality questionanswer pairs, forming the LIFT-HRA dataset. Its visualized statistic analysis is presented in Fig. 4. Specifically, this dataset includes diverse set of categories, with each category containing varying number of video types and videos. Notably, the human feedback is distributed across all categories, with the majority of videos receiving Bad rating, followed by Good and Normal ratings in descending order. This indicates that, on average, the videos evaluated tend to have more room for improvement in terms of their alignment with human preferences. Additionally, the variation in feedback distribution across categories highlights different challenges and complexity in evaluating videos from diverse domains, such as human actions, animals, and places. This variability is crucial for training robust reward models that can generalize across different video types and better align with human expectations. 3.2. LIFT-CRITIC: Reward Function Learning To construct more effective reward model for aligning video generation with human feedback, we fine-tune pre-trained Visual-Language Model (VLM)[24] using annotated data. The reward model evaluates generated videos based on predefined dimensions by assigning scores and providing reason that reflect human preferences. For this purpose, two main approaches can be employed: (1) adding regression head to the VLM for direct score prediction or (2) transforming scores into natural language labels (e.g., good, normal, bad) and predicting these Figure 4. The visualized statistic results of our proposed LIFT-HRA. It illustrates the distribution of category types, the video count across these categories, and the corresponding human feedback distribution for each category. evaluations, providing feedback that guides the alignment of T2V models with human expectations. We will delve into the details of this process in the following section. 3.3. T2V Model Alignment In the T2V domain, aligning model outputs with human preferences remains an underexplored area. While the text-to-image domain has showcased the potential of human feedback-based alignment [19], extending this paradigm to video generation poses unique challenges due to the increased dimensionality and the intricate temporal dynamics inherent in video data. Therefore, we propose novel method that employs our video reward model for T2V model alignment through reward-weighted learning (RWL), i.e., reward-weighted likelihood maximization. Specifically, we use the learned rϕ to update the T2V model with parameters θ by minimizing the loss L(θ) = E(x,z)Dsyn [rϕ(x, z) log pθ(xz)] + λ E(x,z)Dreal [ log pθ(xz)] , (3) where rϕ(x, z) computed by Eq. 2 denotes the reward score for synthesized video with caption z. The first term encourages the model pθ to output aligned with the reward signal rϕ by assigning higher probabilities to high-reward samples from the synthesized dataset Dsyn, where we set the score function s() as {Good: 0.9, Normal: 0.2, Bad: 0.05}. The second term is designed to mitigate the limitations of training exclusively on the synthesized dataset since synthesized videos often suffer from low temporal consistency, which may hinder the models ability to maintain subject alignment across frames. By incorporating real video-text dataset Dreal, the second term acts as regularizer, grounding the model in realistic frame-to-frame dynamics and ensuring that it learns to generate videos with higher semantic and temporal fidelity. The hyperparameter λ balances the loss between synthetic datasets Dsyn and the real dataset Dreal, where we set λ = 1 here. This balance between synthesized and real data helps the model to generalize better, achieving improved performance in overall video quality. In this work, we also explore another effective rewardlearning function, rejection sampling (RS). This method can be viewed as special case of reward-weighted learning, where the score function serves as hard filter. Specifically, the reward weight is defined such that rϕ(x, z) = 1 for samples evaluated as Good in all dimensions D, and rϕ(x, z) = 0 for all other. Under this definition, the loss function for RS simplifies to: LRS(θ) = E(x,z)Dsyn filtered [ log pθ(xz)] + λ E(x,z)Dreal [ log pθ(xz)] . (4) Here, Dsyn D}, where Dsyn denotes the original synthesized dataset. filtered = {(x, z) Dsyn rϕ,d(x, z) = Good, This design effectively reduces the synthesized dataset Dsyn to only include high-quality samples, achieving similar effect as applying binary mask. Compared with RS, RWL assigns weights to all samples, allowing the model to leverage the entire dataset. This smooth weighting ensures higher-reward samples contribute more significantly while still utilizing lower-reward samples, enhancing data efficiency and preventing overfitting. Therefore, RWL is selected as our main method for T2V model alignment. Their quantitative and qualitative comparison are discussed in Sec. 4.3. 4. Experiments 4.1. Experimental Setup Models and Settings. For our reward model LIFT-CRITIC, we leverage VILA-1.5 13B/40B [24], which has been pretrained on extensive video understanding datasets, demonstrating robust capabilities in video comprehension tasks. To adapt the model for our specific evaluation scenario, we employ Low-Rank Adaptation (LoRA) [15] to fine-tune all linear layers. For our baseline T2V generative model, we adopt CogVideoX-2B [37], foundational T2V generation model. We fine-tune all its transformer blocks to enhance its performance, enabling the model to better align with human preferences and improve video quality. All our models Table 1. Quantitative results on video assessment metrics. The first seven metrics correspond to the Quality type, while the remaining correspond to the Semantic type. RM denotes the Reward Model. Models RM Size Subject Consistency Background Consistency Aesthetic Quality Imaging Quality Temporal Flickering Motion Smoothness Dynamic Degree Human Action CogVideoX-2B CogVideoX-5B CogVideoX-2B-LiFT (Ours) Models CogVideoX-2B CogVideoX-5B 13B 40B RM Size CogVideoX-2B-LiFT (Ours) 13B 40B 94.58 94. 96.15 96.82 Color 80.86 82.50 84.40 85.15 95.45 95.64 96.48 96. Spatial Relationship 61.78 55.94 64.66 66.04 61.94 63.13 63.24 63.72 Scene 53.30 56.57 56.76 57.63 63.04 63.14 64.01 64.19 96.94 97.19 98.10 98. Temporal Style Overall Consistency 24.19 25.12 24.83 25.23 27.34 27.84 27.23 27. 97.86 97.81 98.17 98.33 Object Class 85.75 88.99 91.27 91.77 60.22 62. 61.89 62.85 97.34 98.40 97.90 98.44 Multiple Objects Appearance Style 69.11 70. 77.45 79.34 24.67 24.70 24.86 25.92 Table 2. Evaluation results of our reward model. We assess LIFT-CRITIC across first three dimensions and compute the average accuracy based on these evaluations."
        },
        {
            "title": "Video\nFidelity",
            "content": "Avg. Accuracy LiFT-Critic 13B w/o reason 40B LiFT-Critic 13B 40B 80.27 84.55 85.61 90. 77.48 88.01 88.92 94.89 82.72 88.62 88.51 95.53 80.15 87.06 87.68 93. are trained on 8 NVIDIA H100 GPUs with batch size 8 and learning rate 1e5. Datasets. We fine-tune LIFT-CRITIC using our proposed LiFT-HRA dataset, which comprises approximately 10K video quality question-answer pairs annotated with human feedback. Of these, 9K samples are used for training, while the remaining samples serve as validation set. Fig. 4 visualizes the statistic result of this dataset. To diversify the dataset and expose the model to real video distributions, we incorporate 1K high-quality real video samples from HDVILA [35]. For T2V model alignment, we generate 40K videos from prompts using CogVideoX-2B as the synthesized dataset and select approximately 20K video-text pairs from OpenVid [26] as the real dataset. Evaluation Metrics. we employ Vbench [16], comprehensive benchmark suite to assess the performance of T2V generation, which decomposes video generation quality into specific, hierarchical, and disentangled dimensions. Each dimension is evaluated using tailored prompts and specialized evaluation methods. The evaluation prompts are optimized using Qwen2.5-72B-Instruct [36] since the CogVideoX [37] model is trained with long prompts. 4.2. Results Quantitative Results. Tab. 1 demonstrates the significant performance improvements achieved by our finetuning pipeline. Specifically, integrating reward learning using LIFT-CRITIC at various scales leads to consistent enhancements across nearly all evaluation metrics. For example, the model fine-tuned with LIFT-CRITIC-40B shows marked improvements in Subject Consistency and Motion Smoothness, reflecting better alignment with human preferences for coherent video generation and fluid motion. This enhancement also indicates that LIFT-CRITIC40B better understands and captures the continuity and smoothness required in high-quality video. When compared to the larger baseline, CogVideoX-5B, the LIFT-CRITIC40B-enhanced model outperforms in critical areas such as Imaging Quality and Multiple Objects. This improvement highlights the models superior ability to generate visually detailed scenes with multiple objects, which is essential for creating richer and more complex video content. Qualitative Results. Fig. 5 displays the visual comparison results. It is clear that our fine-tuned model achieves relatively better performance in terms of semantic consistency, motion smoothness, and video fidelity. Specifically, in the first case, CogVideoX-2B fails to perform the required camera transition as described in the caption, while our method successfully captures this motion. In the second case, both CogVideoX-2B and CogVideoX-5B generate farmer with very blurred face, whereas our model produces clearer and more detailed facial representation. These examples highlight the effectiveness of our approach in aligning the generated video with both the textual description and human expectations. 4.3. Ablation Studies Choices of reward learning. As introduced in Sec. 3.3, we also explore another effective reward learning function, rejection sampling (RS). In this process, only synthesized videos that receive Good score across all three evaluation dimensions are retained. Compared to rewardFigure 5. Qualitative Comparison. We compare the performance of our CogVideoX-2B-LiFT (fine-tuned using reward-weighted learning) against CogVideoX-2B and CogVideoX-5B. Figure 6. Qualitative comparison. We compare the performance of CogVideo-2B, its variations fine-tuned using reward-weighted learning (Ours) and rejection sampling. mance comparison of LIFT-CRITIC at various scales. Tab. 2 demonstrates that increasing the model size significantly improves performance across all evaluation metrics, resulting in notable increase in overall accuracy. Besides, the results in Tab. 1 illustrate the impact of reward model size on T2V model performance. Compared to the 13B, the LIFT-CRITIC-40B enhanced model achieves consistent improvements across all metrics. These enhancements indicate that larger reward model better captures nuanced human preferences, leading to improved alignment with subjective quality assessments. Effects of learning reason. Tab. 2 underscores the advantages of incorporating reason-based annotations into reward function learning. By comparing LIFT-CRITIC with and without reasoning at the same model size, we observe significant performance improvements. Notably, for the 40B model, the inclusion of reason boosts the average accuracy from 87.06% to 93.55%. These results emphasize that providing reasoning behind human feedback allows the reward model to better capture subtle evaluation criteria, leading to improved alignment with human preferences. 5. Conclusion This work proposes novel fine-tuning pipeline LIFT for aligning T2V models with human preferences. First, we curate human-annotated dataset, LIFT-HRA, which includes both ratings and reason for video evaluation. We then train reward model LIFT-CRITIC to learn the reward function from this dataset. Lastly, we employ it to align the T2V model. As case study, we apply this pipeline to CogVideoX-2B, showing that the fine-tuned model outperforms the CogVideoX-5B across all 16 metrics, showcasing the effectiveness of our approach in aligning T2V models with human expectations. Figure 7. Visualized evaluation results in multiple evaluation dimensions. The middle two methods in the label region represent the CogVideoX-2B model fine-tuned using different reward learning strategies. weighted learning (RWL), this approach effectively reduces the amount of fine-tuning data while ensuring the selected samples meet high-quality standards for model alignment. Fig. 7 presents comprehensive overview of performance across multiple evaluation dimensions. The results show that RS significantly outperforms the baseline CogVideoX2B, while maintaining competitive performance with the larger CogVideoX-5B. Although slightly less effective than RWL, RS proves to be more efficient and lightweight method for aligning T2V models with human preferences. Additionally, Fig. 6 showcases the visual improvements and enhanced alignment with human expectations achieved through these approaches. Effects of reward model size. We explore the perfor-"
        },
        {
            "title": "References",
            "content": "[1] Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862, 2022. [2] Hritik Bansal, Zongyu Lin, Tianyi Xie, Zeshun Zong, Michal Yarom, Yonatan Bitton, Chenfanfu Jiang, Yizhou Sun, KaiWei Chang, and Aditya Grover. Videophy: Evaluating physical commonsense for video generation. arXiv preprint arXiv:2406.03520, 2024. [3] Kevin Black, Michael Janner, Yilun Du, Ilya Kostrikov, and Sergey Levine. Training diffusion models with reinforcement learning. arXiv preprint arXiv:2305.13301, 2023. [4] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis. Align your latents: High-resolution video synthesis with latent diffusion models. In CVPR, pages 2256322575, 2023. [5] Chaofeng Chen, Annan Wang, Haoning Wu, Liang Liao, Wenxiu Sun, Qiong Yan, and Weisi Lin. Enhancing diffusion models with text-encoder reinforcement learning. In ECCV, pages 182198, 2025. [6] Haoxin Chen, Menghan Xia, Yingqing He, Yong Zhang, Xiaodong Cun, Shaoshu Yang, Jinbo Xing, Yaofang Liu, Qifeng Chen, Xintao Wang, et al. Videocrafter1: Open diffusion models for high-quality video generation. arXiv preprint arXiv:2310.19512, 2023. [7] Shoufa Chen, Mengmeng Xu, Jiawei Ren, Yuren Cong, Sen He, Yanping Xie, Animesh Sinha, Ping Luo, Tao Xiang, and Juan-Manuel Perez-Rua. Gentron: Delving deep into diffusion transformers for image and video generation. arXiv preprint arXiv:2312.04557, 2023. [8] Kevin Clark, Paul Vicol, Kevin Swersky, and David J. Fleet. Directly fine-tuning diffusion models on differentiable rewards. arXiv preprint arXiv:2309.17400, 2023. [9] Ying Fan, Olivia Watkins, Yuqing Du, Hao Liu, Moonkyung Ryu, Craig Boutilier, Pieter Abbeel, Mohammad Ghavamzadeh, Kangwook Lee, and Kimin Lee. Reinforcement learning for fine-tuning text-to-image diffusion models. NeurIPS, 36, 2024. [10] Yuwei Guo, Ceyuan Yang, Anyi Rao, Zhengyang Liang, Yaohui Wang, Yu Qiao, Maneesh Agrawala, Dahua Lin, and Bo Dai. Animatediff: Animate your personalized textto-image diffusion models without specific tuning. arXiv preprint arXiv:2307.04725, 2023. [11] Agrim Gupta, Lijun Yu, Kihyuk Sohn, Xiuye Gu, Meera Hahn, Fei-Fei Li, Irfan Essa, Lu Jiang, and Jose Lezama. Photorealistic video generation with diffusion models. In ECCV, pages 393411, 2025. [12] Jingwen He, Tianfan Xue, Dongyang Liu, Xinqi Lin, Peng Gao, Dahua Lin, Yu Qiao, Wanli Ouyang, and Ziwei Liu. Venhancer: Generative space-time enhancement for video generation. arXiv preprint arXiv:2407.07667, 2024. [13] Xuan He, Dongfu Jiang, Ge Zhang, Max Ku, Achint Soni, Sherman Siu, Haonan Chen, Abhranil Chandra, Ziyan Jiang, Aaran Arulraj, et al. Videoscore: Building automatic metrics to simulate fine-grained human feedback for video generation. arXiv preprint arXiv:2406.15252, 2024. [14] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik Kingma, Ben Poole, Mohammad Norouzi, David Fleet, et al. Imagen video: High definition video generation with diffusion models. arXiv preprint arXiv:2210.02303, 2022. [15] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021. [16] Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, Yaohui Wang, Xinyuan Chen, Limin Wang, Dahua Lin, Yu Qiao, and Ziwei Liu. Vbench: Comprehensive benchmark suite for video generative models. CVPR, pages 2180721818, 2023. [17] Yuval Kirstain, Adam Polyak, Uriel Singer, Shahbuland Matiana, Joe Penna, and Omer Levy. Pick-a-pic: An open dataset of user preferences for text-to-image generation. NeurIPS, 36:3665236663, 2023. [18] Tengchuan Kou, Xiaohong Liu, Zicheng Zhang, Chunyi Li, Haoning Wu, Xiongkuo Min, Guangtao Zhai, and Ning Liu. Subjective-aligned dataset and metric for text-to-video quality assessment. arXiv preprint arXiv:2403.11956, 2024. [19] Kimin Lee, Hao Liu, Moonkyung Ryu, Olivia Watkins, Yuqing Du, Craig Boutilier, Pieter Abbeel, Mohammad Ghavamzadeh, and Shixiang Shane Gu. Aligning textarXiv preprint to-image models using human feedback. arXiv:2302.12192, 2023. [20] Chunyi Li, Zicheng Zhang, Haoning Wu, Wei Sun, Xiongkuo Min, Xiaohong Liu, Guangtao Zhai, and Weisi Lin. Agiqa-3k: An open database for ai-generated image quality assessment. IEEE Transactions on Circuits and Systems for Video Technology, 2023. [21] Jiachen Li, Weixi Feng, Wenhu Chen, and William Yang Wang. Reward guided latent consistency distillation. arXiv preprint arXiv:2403.11027, 2024. [22] Jiachen Li, Weixi Feng, Tsu-Jui Fu, Xinyi Wang, Sugato Basu, Wenhu Chen, and William Yang Wang. T2vturbo: Breaking the quality bottleneck of video consistency model with mixed reward feedback. arXiv preprint arXiv:2405.18750, 2024. [23] Youwei Liang, Junfeng He, Gang Li, Peizhao Li, Arseniy Klimovskiy, Nicholas Carolan, Jiao Sun, Jordi Pont-Tuset, Sarah Young, Feng Yang, et al. Rich human feedback for In CVPR, pages 1940119411, text-to-image generation. 2024. [24] Ji Lin, Hongxu Yin, Wei Ping, Pavlo Molchanov, Mohammad Shoeybi, and Song Han. Vila: On pre-training for visual language models. In CVPR, pages 2668926699, 2024. [25] Yaofang Liu, Xiaodong Cun, Xuebo Liu, Xintao Wang, Yong Zhang, Haoxin Chen, Yang Liu, Tieyong Zeng, Raymond Chan, and Ying Shan. Evalcrafter: Benchmarking and evaluating large video generation models. In CVPR, pages 22139 22149, 2024. [26] Kepan Nan, Rui Xie, Penghao Zhou, Tiehan Fan, Zhenheng Yang, Zhijie Chen, Xiang Li, Jian Yang, and Ying Tai. [39] Hangjie Yuan, Shiwei Zhang, Xiang Wang, Yujie Wei, Tao Feng, Yining Pan, Yingya Zhang, Ziwei Liu, Samuel Albanie, and Dong Ni. instructing video difInstructvideo: fusion models with human feedback. In CVPR, pages 6463 6474, 2024. [40] Sixian Zhang, Bohan Wang, Junqiang Wu, Yan Li, Tingting Gao, Di Zhang, and Zhongyuan Wang. Learning multidimensional human preference for text-to-image generation. In CVPR, pages 80188027, 2024. [41] Yinan Zhang, Eric Tzeng, Yilun Du, and Dmitry Kislyuk. Large-scale reinforcement learning for diffusion models. arXiv preprint arXiv:2401.12244, 2024. [42] Shangchen Zhou, Peiqing Yang, Jianyi Wang, Yihang Luo, and Chen Change Loy. Upscale-a-video: Temporalreal-world video superconsistent diffusion model resolution. In CVPR, pages 25352545, 2024. for [43] Yuan Zhou, Qiuyue Wang, Yuxuan Cai, and Huan Yang. Allegro: Open the black box of commercial-level video generation model. arXiv preprint arXiv:2410.15458, 2024. Openvid-1m: large-scale high-quality dataset for text-tovideo generation. arXiv preprint arXiv:2407.02371, 2024. [27] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. NeurIPS, 35:2773027744, 2022. [28] Mihir Prabhudesai, Russell Mendonca, Zheyang Qin, Katerina Fragkiadaki, and Deepak Pathak. Video diffusion alignment via reward gradients. arXiv preprint arXiv:2407.08737, 2024. [29] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In ICML, pages 87488763, 2021. [30] Jiuniu Wang, Hangjie Yuan, Dayou Chen, Yingya Zhang, Xiang Wang, and Shiwei Zhang. Modelscope text-to-video technical report. arXiv preprint arXiv:2308.06571, 2023. [31] Xiang Wang, Hangjie Yuan, Shiwei Zhang, Dayou Chen, Jiuniu Wang, Yingya Zhang, Yujun Shen, Deli Zhao, and Jingren Zhou. Videocomposer: Compositional video synthesis with motion controllability. NeurIPS, 36, 2024. [32] Yaohui Wang, Xinyuan Chen, Xin Ma, Shangchen Zhou, Ziqi Huang, Yi Wang, Ceyuan Yang, Yinan He, Jiashuo Yu, Peiqing Yang, et al. Lavie: High-quality video generation with cascaded latent diffusion models. arXiv preprint arXiv:2309.15103, 2023. [33] Xiaoshi Wu, Yiming Hao, Keqiang Sun, Yixiong Chen, Feng Zhu, Rui Zhao, and Hongsheng Li. Human preference score v2: solid benchmark for evaluating human preferences of text-to-image synthesis. arXiv preprint arXiv:2306.09341, 2023. [34] Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding, Jie Tang, and Yuxiao Dong. Imagereward: Learning and evaluating human preferences for textto-image generation. NeurIPS, 36, 2024. [35] Hongwei Xue, Tiankai Hang, Yanhong Zeng, Yuchong Sun, Bei Liu, Huan Yang, Jianlong Fu, and Baining Guo. Advancing high-resolution video-language representation with large-scale video transcriptions. In CVPR, pages 50365045, 2022. [36] An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, et al. Qwen2 technical report. arXiv preprint arXiv:2407.10671, 2024. [37] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. [38] Hangjie Yuan, Shiwei Zhang, Xiang Wang, Yujie Wei, Tao Feng, Yining Pan, Yingya Zhang, Ziwei Liu, Samuel Albanie, and Dong Ni. Instructvideo: Instructing video diffusion models with human feedback. CVPR, pages 64636474, 2023."
        }
    ],
    "affiliations": [
        "Australian Institute for Machine Learning, The University of Adelaide",
        "Fudan University",
        "Shanghai Academy of Artificial Intelligence for Science"
    ]
}