{
    "paper_title": "RONA: Pragmatically Diverse Image Captioning with Coherence Relations",
    "authors": [
        "Aashish Anantha Ramakrishnan",
        "Aadarsh Anantha Ramakrishnan",
        "Dongwon Lee"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Writing Assistants (e.g., Grammarly, Microsoft Copilot) traditionally generate diverse image captions by employing syntactic and semantic variations to describe image components. However, human-written captions prioritize conveying a central message alongside visual descriptions using pragmatic cues. To enhance pragmatic diversity, it is essential to explore alternative ways of communicating these messages in conjunction with visual content. To address this challenge, we propose RONA, a novel prompting strategy for Multi-modal Large Language Models (MLLM) that leverages Coherence Relations as an axis for variation. We demonstrate that RONA generates captions with better overall diversity and ground-truth alignment, compared to MLLM baselines across multiple domains. Our code is available at: https://github.com/aashish2000/RONA"
        },
        {
            "title": "Start",
            "content": "RONA: Pragmatically Diverse Image Captioning with Coherence Relations Aashish Anantha Ramakrishnan1, Aadarsh Anantha Ramakrishnan2, Dongwon Lee1 1The Pennsylvania State University; 2National Institute of Technology, Tiruchirappalli 1{aza6352, dul13}@psu.edu, 2106121001@nitt.edu 5 2 0 2 4 1 ] . [ 1 7 9 9 0 1 . 3 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Writing Assistants (e.g., Grammarly, Microsoft Copilot) traditionally generate diverse image captions by employing syntactic and semantic variations to describe image components. However, human-written captions prioritize conveying central message alongside visual descriptions using pragmatic cues. To enhance pragmatic diversity, it is essential to explore alternative ways of communicating these messages in conjunction with visual content. To address this challenge, we propose RONA, novel prompting strategy for Multi-modal Large Language Models (MLLM) that leverages Coherence Relations as an axis for variation. We demonstrate that RONA generates captions with better overall diversity and ground-truth alignment, compared to MLLM baselines across multiple domains. Our code is available at: https://github.com/aashish2000/RONA"
        },
        {
            "title": "Introduction",
            "content": "A Writing Assistant (WA) is tool (e.g., Grammarly, Microsoft Copilot, Copy.ai), often powered by Generative AI, that helps users in various writing tasks. WAs have evolved over the years to support users across multitude of tasks, with AI-powered assistants being adept at generating wide selection of content. Image Captioning (i.e., generating textual descriptions for given images) is one key task that has seen significant advancements with the introduction of Multi-modal Large Language Models (MLLMs). These pre-trained models have achieved remarkable success in generating captions that accurately describe the visual content of images (Chen et al., 2024; Yue et al., 2023). However, real-world image captions across different domains often require more than just description of the visual elementsi.e., they need to convey central message, provide context, and offer different perspectives on the image (Federico, 1 2016). This leads to significant lack of diversity in the generated captions, limiting the utility of WAs. Existing approaches to fostering diversity in image captions have primarily focused on providing richer vocabulary (i.e., syntactic variations) or selecting different components of the image to emphasize (i.e., semantic variations) (Bugliarello and Elliott, 2021; Li et al., 2022). Although these methods have shown promise, they often fail to capture the nuanced ways in which humans communicate through captions as shown in Figure 1. Pragmatic variations, such as multi-modal implicatures and metaphors (Genovesi, 2020), which utilize meanings or connotations beyond the literal description of the image, are often employed by caption writers to make their messages more engaging and relatable (Weiland et al., 2015). To address this challenge, in this work, we propose RONA (Relation-based cOhereNce-aware cAptioning), novel prompting strategy for MLLMs inspired by the concept of Coherence Relations (CRs). Based on the principles of Discourse Theory, CRs provide structured overview of image-text relationships (Hobbs, 1978; Kress, 2009; OHalloran et al., 2014). We evaluate their effectiveness in image captioning by using CRs as guidelines for generating captions that fulfill specific communicative functions while preserving semantic coherence. Our analysis includes popular MLLMs: Claude3.5 Sonnet V2 (Anthropic) and GPT-4o (OpenAI et al., 2024) on two datasetsi.e., news captions (ANNA) and social media captions (Tweet Subtitles). These datasets contain wide range of visual objects and abstractive captions (Anantha Ramakrishnan et al., 2024), making this challenging task for MLLMs. We demonstrate that RONA outperforms existing baselines on caption diversity while retaining ground truth similarity. Our contributions are as follows: Figure 1: An overview of RONA. The CRs used are Insertion (Ins), Concretization (Con), Projection (Proj), Restatement (Rstt) and Extension (Ext). These relations provide finite but flexible axis of variation for diverse caption generation compared to instruction-based prompts. We propose RONA, novel prompting strategy for MLLMs that leverages Coherence Relations (CRs) to generate pragmatically diverse image captions. We demonstrate that RONA outperforms existing MLLM baselines in terms of diversity and ground-truth similarity on news and social media captioning datasets. Our analysis shows the viability of CRs to be utilized as an axis of variation for Captioningbased writing assistants."
        },
        {
            "title": "2 Related Work",
            "content": "Writing Assistants MLLMs have enabled WAs to support variety of writing tasks, with their input ranging from sentence-level suggestions (Gero et al., 2022) to long-form writing tasks such as literature reviews and creative writing (Choe et al., 2024), (Singh et al., 2023). In order to improve the Human-AI collaboration experience, there exists need to incorporate human values into AI-based writing systems and vice versa (Shen et al., 2024), (Lee et al., 2024). Although these studies have focused on broader task domains, specific writing tasks such as caption writing have been less explored (Ng et al., 2024), particularly pragmatically diverse captioning which we aim to address. Image Captioning Traditional Captioning models build on task-specific generative architectures to generate faithful and diverse descriptions for images (Mahajan and Roth, 2020; Liu et al., 2019). However, with the introduction of MLLMs, general-purpose models capable of multi-modal representations are utilized for caption generation (Radford et al., 2021; Li et al., 2023). To improve the alignment between image-text linkages, Coherence Relations (CR) (Alikhani et al., 2020) have been leveraged across different downstream text generation tasks (Alikhani et al., 2019; Vempala and Preotiuc-Pietro, 2019; Sosea et al., 2021). Although popular MLLMs such as GPT-4o and Claude Sonnet 3.5 V2 are poor at predicting and verifying these relationships (Thrush et al., 2022; Anantha Ramakrishnan et al., 2025), existing research does not explore the production capabilities of these models. In our work, we investigate the ability of MLLM-based WAs to utilize CRs as guidance mechanism for in-context learning."
        },
        {
            "title": "3 Methodology",
            "content": "Coherence Relations RONA leverages incontext explanations of Coherence Relations (CRs) as guidance for generating pragmatically diverse captions. We utilize CRs that characterize both entity-level and scene-level linkages between an 2 image and its expected caption (Xu et al., 2022). Entity-level relations describe the relationships between specific objects in the image and their corresponding elaboration in the caption. Scene-level relations, on the other hand, capture the overall context and narrative of the image, providing broader understanding of the visual content. The selection of these relations are motivated by their generalizability across different domains. Overall, the 5 types of relations that we utilize are: Insertion: An entity-level relation that describes type of pragmatic ellipsis, i.e., where the focal object described in the image and caption does not have an explicit mention in the caption. Concretization: An entity-level relation that utilizes an anchor object which is prominently referenced in the image and caption, with the caption providing additional meaning about its context. Projection: An entity-level relation where the captions description revolves around particular topic, but this topic is not directly featured in the image. Alternatively, the image contains objects that can be associated to this topic instead, forming an implied link between modalities. Restatement: scene-level relation that describes the overall context of the image, with the caption providing more detailed description of the visual scene. Extension: scene-level relation in which the caption elaborates further on the visual scene in terms of new ideas or stories. Datasets Popular datasets such as COCO Captions (Chen et al., 2015) or Flickr30K (Young et al., 2014) are often used for image captioning evaluation, but the ground-truth captions do not cater to sharing messages or perspectives that are more aligned with human-written captions. Instead, we select datasets from 2 different task domains for image captioning: news and social media. These domains provide representative examples of realworld scenarios for the usage of WAs: (1) The Tweet Subtitles dataset (Xu et al., 2022) contains 16,000 image-text pairs sourced from Twitter and cleaned to remove noisy, low-quality samples, and (2) ANNA (Anantha Ramakrishnan et al., 2024) on the other hand contains 29,625 image-text pairs collected from The New York Times news articles focusing on non Named Entity objects. Both datasets contain abstractive\" or non-descriptive captions with wide range of image subjects and topics. For our evaluation, we used the entire test set of 1,600 samples from Tweet Subtitles and random sample of 1,500 images from the test set of ANNA."
        },
        {
            "title": "4 Experiments",
            "content": "Task Types For our analysis of the effectiveness of RONA, we define 2 task types: Image-only and Image + Caption. In the Image-only task, we define this as classic image captioning task in which the model is provided only with the image as input. On the other hand, for the Image + Caption task, we provide the model with both the image and ground-truth caption as input. Since both of these components are part of understanding the overall meaning of an image-caption pair, we wish to understand how MLLMs utilize both modalities to generate diverse captions without the divergence of meaning. This is similar to the prompt-guided image captioning task for MLLMs (Hu et al., 2023). In both tasks, the baseline MLLM is prompted to use the inputs to generate captions with as much diversity as possible while retaining their original meaning and message.\" RONA utilizes in-context learning where simplified definitions of CRs are provided as system prompts. We generate 5 captions per input for each type of task, with RONA generating one caption per CR. Additional generated caption examples are presented in Appendix Section E. Evaluation Metrics To evaluate the performance of MLLMs on the task of diverse captioning, we measure 4 key attributes: image-caption similarity, ground truth caption similarity, contextual diversity, and bi-gram diversity. CLIPScore (Hessel et al., 2021) effectively measures image-caption similarity by converting both modalities into common representation space. For validating similarity of generated captions with the ground truth text, we turn to BLEURT (Sellam et al., 2020) score. Unlike traditional similarity metrics such as BLEU (Papineni et al., 2001), METEOR (Lavie and Agarwal, 2007) and BERTScore (Zhang* et al., 2020), BLEURT is trained to balance contextual similarity and human preference judgments, making it better suited for non-descriptive captions. All similarity metrics are computed pairwise between the ground truth modality and generated captions, with the average score reported in our benchmarks. For 3 Task Image-only Image-only Image + Caption Image + Caption Model BLEURT CLIPScore Self-BLEURT Div-2 Claude RONA + Claude GPT-4o RONA + GPT-4o Claude RONA + Claude GPT-4o RONA + GPT-4o -1.227 -1. -1.237 -1.137 -0.931 -0.879 -0.650 -0.615 14.049 14.068 13.117 14.505 13.833 13. 13.200 13.891 0.226 0.108 0.198 0.205 0.294 0.158 0.355 0.383 0.860 0. 0.885 0.879 0.843 0.882 0.805 0.823 Table 1: Results for Diverse Image Captioning with RONA on the Tweet Subtitles Dataset. Task Image-only Image-only Image + Caption Image + Caption Model BLEURT CLIPScore Self-BLEURT Div-2 Claude RONA + Claude GPT-4o RONA + GPT-4o Claude RONA + Claude GPT-4o RONA + GPT-4o -1.191 -1.038 -1.159 -1.057 -0.669 -0.559 -0.356 -0.363 14.617 14.471 13.954 15. 14.582 14.549 14.338 14.869 0.258 0.134 0.249 0.209 0.341 0.217 0.436 0. 0.854 0.899 0.883 0.878 0.845 0.883 0.796 0.824 Table 2: Results for Diverse Image Captioning with RONA on the ANNA Dataset. judging contextual diversity, we reformulate it as task of minimizing the pairwise similarity between generated captions. This homogenization process is applied to BLEURT score, converting it into the diversity metric Self-BLEURT (Shaib et al., 2024). Finally, we calculate the overall bi-gram diversity of generated captions using the Div-2 metric (Shetty et al., 2017), which reports the ratio of unique bi-grams to the total count of bi-grams in sentence."
        },
        {
            "title": "5 Results",
            "content": "RONA Improves Relevance and Diversity We present our evaluation of MLLMs on the task of Diverse Image Captioning in Tables 1 and 2. Our assessment spans 8 different settings: 2 tasks per dataset, 2 models per task and 2 different dataset domains. From our results, both GPT-4o and Claude combined with RONA outperforms their respective baselines in 7/8 settings on both ground truth similarity and diversity metrics. Particularly, we see positive agreement between Div-2 and SelfBLEURT, as they rate captions from RONA-based models as more diverse over 5/8 baselines. With image & text similarity metrics such as BLEURT and CLIPScore preferring RONA-based models 7/8 and 6/8 times over baselines respectively, we can conclude that our observed diversity has not come at the cost of contextual relevance. Diversity and Similarity Trade-off Across Modalities From our experiments across task types, we observe small decrease in image similarity but improved caption similarity and diversity in the Image + Caption task compared to Image-only task. This confirms that image-only descriptive captioning approaches are limited in terms of expression and rely heavily on listing visual features. This motivates the need for WAs to be evaluated on captions with pragmatic variations to test their true ability in understanding the overall message of sample."
        },
        {
            "title": "6 Conclusions",
            "content": "We propose RONA, Coherence Relation-based prompting strategy, providing framework for expressive and diverse image caption generation. Our study presents holistic evaluation of top MLLMs on their ability to utilize these relationships through in-context learning. Our results show that RONA enables the generation of greater variety of captions while improving their overall semantic and contextual relevance across domains. RONA serves as new baseline for future work leveraging image-text relationships for improving the quality of Multi-modal Writing Assistants."
        },
        {
            "title": "Limitations",
            "content": "Our current analysis of RONA is limited to couple of top-performing MLLM architectures. Evaluating how open-source MLLMs can leverage CRs for image captioning is part of our future work. Additionally, our evaluation strategy does not validate the prompt following accuracy of MLLMs in adhering to specific CRs, which would be significant challenge for smaller, low-resource models. These inaccuracies may lead to hallucinations, harming the factual accuracy of generated captions. Incorporating Factual Consistency metrics and Human preference ratings to identify potential types of hallucinations in diverse captioning tasks is direction of future work we wish to pursue."
        },
        {
            "title": "Ethics Statement",
            "content": "We acknowledge the potential for alternate prompting strategies like RONA to be used for generating misleading content, especially from specific domains such as news media. However, from our evaluation, we find that MLLM safety filters are robust in capturing potentially harmful content in either the input images or captions as described in Appendix Section D. With CRs leveraging pragmatic and common-sense knowledge of MLLMs to generate diverse captions, there exists possibility of model biases and stereotypes clouding the quality of our generations. This is especially problem in cases where culturally sensitive material is present in our input samples. We advocate for the responsible use of Writing Assistants with adequate human oversight to prevent such situations."
        },
        {
            "title": "References",
            "content": "Malihe Alikhani, Sreyasi Nag Chowdhury, Gerard de Melo, and Matthew Stone. 2019. CITE: corpus of image-text discourse relations. In Proceedings of the 2019 Conference of the North, pages 570575, Stroudsburg, PA, USA. Association for Computational Linguistics. Malihe Alikhani, Piyush Sharma, Shengjie Li, Radu Soricut, and Matthew Stone. 2020. Cross-modal coherence modeling for caption generation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6525 6535, Stroudsburg, PA, USA. Association for Computational Linguistics. Aashish Anantha Ramakrishnan, Aadarsh Anantha Ramakrishnan, and Lee Dongwon. 2025. CORDIAL: Can multimodal large language models effectively understand coherence relationships? arXiv [cs.CL]. 5 Aashish Anantha Ramakrishnan, Sharon Huang, and Dongwon Lee. 2024. ANNA: Abstractive text-toimage synthesis with filtered news captions. In Proceedings of the Third Workshop on Advances in Language and Vision Research. Association for Computational Linguistics. Anthropic. Claude 3.5 sonnet. https://www. anthropic.com/claude/sonnet. Accessed: 20252-14. Emanuele Bugliarello and Desmond Elliott. 2021. The role of syntactic planning in compositional image captioning. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 593607, Stroudsburg, PA, USA. Association for Computational Linguistics. Gongwei Chen, Leyang Shen, Rui Shao, Xiang Deng, and Liqiang Nie. 2024. LION : Empowering multimodal large language model with dual-level visual knowledge. In 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 2653026540. IEEE. Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Dollar, and Lawrence Zitnick. 2015. Microsoft COCO captions: Data collection and evaluation server. arXiv preprint arXiv:1504.00325. Kiroong Choe, Seokhyeon Park, Seokweon Jung, Hyeok Kim, Ji Won Yang, Hwajung Hong, and Jinwook Seo. 2024. Supporting novice researchers to write literature review using language models. In Extended Abstracts of the CHI Conference on Human Factors in Computing Systems, pages 19, New York, NY, USA. ACM. Stephanie Federico. 2016. These are NPRs photo caption guidelines. NPR. Chris Genovesi. 2020. Metaphor and what is meant: Metaphorical content, what is said, and contextualism. J. Pragmat., 157:1738. Katy Ilonka Gero, Vivian Liu, and Lydia Chilton. 2022. Sparks: Inspiration for science writing using lanIn Designing Interactive Systems guage models. Conference, New York, NY, USA. ACM. Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. 2021. CLIPScore: reference-free evaluation metric for image captioning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 75147528, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics. Jerry Hobbs. 1978. Why is discourse coherent?, volume 176. SRI International Menlo Park, CA. Yushi Hu, Hang Hua, Zhengyuan Yang, Weijia Shi, Noah Smith, and Jiebo Luo. 2023. PromptCap: Prompt-guided image captioning for VQA with GPT3. In 2023 IEEE/CVF International Conference on Computer Vision (ICCV), pages 29512963. IEEE. Gunther Kress. 2009. Multimodality: social semiotic approach to contemporary communication. Routledge, London, England. Lavie and Abhaya Agarwal. 2007. METEOR: An automatic metric for MT evaluation with high levels of correlation with human judgments. pages 228 231. Mina Lee, Katy Ilonka Gero, John Joon Young Chung, Simon Buckingham Shum, Vipul Raheja, Hua Shen, Subhashini Venugopalan, Thiemo Wambsganss, David Zhou, Emad Alghamdi, Tal August, Avinash Bhat, Madiha Zahrah Choksi, Senjuti Dutta, Jin Guo, Md Naimul Hoque, Yewon Kim, Simon Knight, Seyed Parsa Neshaei, Antonette Shibani, Disha Shrivastava, Lila Shroff, Agnia Sergeyuk, Jessi Stark, Sarah Sterman, Sitong Wang, Antoine Bosselut, Daniel Buschek, Joseph Chee Chang, Sherol Chen, Max Kreminski, Joonsuk Park, Roy Pea, Eugenia Ha Rim Rho, Zejiang Shen, and Pao Siangliulue. 2024. design space for intelligent and interactive writing assistants. In Proceedings of the CHI Conference on Human Factors in Computing Systems, volume 7, pages 135, New York, NY, USA. ACM. Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. 2023. BLIP-2: Bootstrapping language-image pretraining with frozen image encoders and large language models. In International Conference on Machine Learning, pages 1973019742. PMLR. Yehao Li, Yingwei Pan, Ting Yao, and Tao Mei. 2022. Comprehending and ordering semantics for image captioning. In 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1796917978. IEEE. Lixin Liu, Jiajun Tang, Xiaojun Wan, and Zongming Guo. 2019. Generating diverse and descriptive imIn 2019 age captions using visual paraphrases. IEEE/CVF International Conference on Computer Vision (ICCV), pages 42394248. IEEE. Shweta Mahajan and Stefan Roth. 2020. Diverse image captioning with context-object split latent spaces. In Proceedings of the 34th International Conference on Neural Information Processing Systems, NIPS 20, Red Hook, NY, USA. Curran Associates Inc. Ho Yin Sam Ng, Ting-Yao Hsu, Jiyoo Min, Sungchul Kim, Ryan Rossi, Tong Yu, Hyunggu Jung, and Ting-Hao Kenneth Huang. 2024. Understanding how paper writers use AI-generated captions in figure capIn 2nd AI4Research Workshop: Totion writing. wards Knowledge-grounded Scientific Research Lifecycle. Kay OHalloran, Sabine Tan, and Marissa E. 2014. 9. multimodal pragmatics. In Klaus Schneider and Anne Barron, editors, Pragmatics of Discourse, pages 239268. De Gruyter Mouton, Berlin, MÃ¼nchen, Boston. OpenAI, Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, Aleksander adry, Alex Baker-Whitcomb, Alex Beutel, Alex Borzunov, Alex Carney, Alex Chow, Alex Kirillov, Alex Nichol, Alex Paino, Alex Renzin, Alex Tachard Passos, Alexander Kirillov, Alexi Christakis, Alexis Conneau, Ali Kamali, Allan Jabri, Allison Moyer, Allison Tam, Amadou Crookes, Amin Tootoochian, Amin Tootoonchian, Ananya Kumar, Andrea Vallone, Andrej Karpathy, Andrew Braunstein, Andrew Cann, Andrew Codispoti, Andrew Galu, Andrew Kondrich, Andrew Tulloch, Andrey Mishchenko, Angela Baek, Angela Jiang, Antoine Pelisse, Antonia Woodford, Anuj Gosalia, Arka Dhar, Ashley Pantuliano, Avi Nayak, Avital Oliver, Barret Zoph, Behrooz Ghorbani, Ben Leimberger, Ben Rossen, Ben Sokolowsky, Ben Wang, Benjamin Zweig, Beth Hoover, Blake Samic, Bob McGrew, Bobby Spero, Bogo Giertler, Bowen Cheng, Brad Lightcap, Brandon Walkin, Brendan Quinn, Brian Guarraci, Brian Hsu, Bright Kellogg, Brydon Eastman, Camillo Lugaresi, Carroll Wainwright, Cary Bassin, Cary Hudson, Casey Chu, Chad Nelson, Chak Li, Chan Jun Shern, Channing Conger, Charlotte Barette, Chelsea Voss, Chen Ding, Cheng Lu, Chong Zhang, Chris Beaumont, Chris Hallacy, Chris Koch, Christian Gibson, Christina Kim, Christine Choi, Christine McLeavey, Christopher Hesse, Claudia Fischer, Clemens Winter, Coley Czarnecki, Colin Jarvis, Colin Wei, Constantin Koumouzelis, Dane Sherburn, Daniel Kappler, Daniel Levin, Daniel Levy, David Carr, David Farhi, David Mely, David Robinson, David Sasaki, Denny Jin, Dev Valladares, Dimitris Tsipras, Doug Li, Duc Phong Nguyen, Duncan Findlay, Edede Oiwoh, Edmund Wong, Ehsan Asdar, Elizabeth Proehl, Elizabeth Yang, Eric Antonow, Eric Kramer, Eric Peterson, Eric Sigler, Eric Wallace, Eugene Brevdo, Evan Mays, Farzad Khorasani, Felipe Petroski Such, Filippo Raso, Francis Zhang, Fred von Lohmann, Freddie Sulit, Gabriel Goh, Gene Oden, Geoff Salmon, Giulio Starace, Greg Brockman, Hadi Salman, Haiming Bao, Haitang Hu, Hannah Wong, Haoyu Wang, Heather Schmidt, Heather Whitney, Heewoo Jun, Hendrik Kirchner, Henrique Ponde de Oliveira Pinto, Hongyu Ren, Huiwen Chang, Hyung Won Chung, Ian Kivlichan, Ian OConnell, Ian OConnell, Ian Osband, Ian Silber, Ian Sohl, Ibrahim Okuyucu, Ikai Lan, Ilya Kostrikov, Ilya Sutskever, Ingmar Kanitscheider, Ishaan Gulrajani, Jacob Coxon, Jacob Menick, Jakub Pachocki, James Aung, James Betker, James Crooks, James Lennon, Jamie Kiros, Jan Leike, Jane Park, Jason Kwon, Jason Phang, Jason Teplitz, Jason Wei, Jason Wolfe, Jay Chen, Jeff Harris, Jenia Varavva, Jessica Gan Lee, Jessica Shieh, Ji Lin, Jiahui Yu, Jiayi Weng, Jie Tang, Jieqi Yu, Joanne Jang, Joaquin Quinonero Candela, Joe Beutler, Joe Landers, Joel Parish, Johannes Heidecke, John Schulman, Jonathan Lachman, Jonathan McKay, Jonathan Uesato, Jonathan Ward, Jong Wook Kim, Joost Huizinga, Jordan Sitkin, Jos Kraaijeveld, Josh Gross, Josh Kaplan, Josh Snyder, Joshua Achiam, Joy Jiao, Joyce Lee, Juntang Zhuang, Justyn Harriman, Kai 6 Fricke, Kai Hayashi, Karan Singhal, Katy Shi, Kavin Karthik, Kayla Wood, Kendra Rimbach, Kenny Hsu, Kenny Nguyen, Keren Gu-Lemberg, Kevin Button, Kevin Liu, Kiel Howe, Krithika Muthukumar, Kyle Luther, Lama Ahmad, Larry Kai, Lauren Itow, Lauren Workman, Leher Pathak, Leo Chen, Li Jing, Lia Guy, Liam Fedus, Liang Zhou, Lien Mamitsuka, Lilian Weng, Lindsay McCallum, Lindsey Held, Long Ouyang, Louis Feuvrier, Lu Zhang, Lukas Kondraciuk, Lukasz Kaiser, Luke Hewitt, Luke Metz, Lyric Doshi, Mada Aflak, Maddie Simens, Madelaine Boyd, Madeleine Thompson, Marat Dukhan, Mark Chen, Mark Gray, Mark Hudnall, Marvin Zhang, Marwan Aljubeh, Mateusz Litwin, Matthew Zeng, Max Johnson, Maya Shetty, Mayank Gupta, Meghan Shah, Mehmet Yatbaz, Meng Jia Yang, Mengchao Zhong, Mia Glaese, Mianna Chen, Michael Janner, Michael Lampe, Michael Petrov, Michael Wu, Michele Wang, Michelle Fradin, Michelle Pokrass, Miguel Castro, Miguel Oom Temudo de Castro, Mikhail Pavlov, Miles Brundage, Miles Wang, Minal Khan, Mira Murati, Mo Bavarian, Molly Lin, Murat Yesildal, Nacho Soto, Natalia Gimelshein, Natalie Cone, Natalie Staudacher, Natalie Summers, Natan LaFontaine, Neil Chowdhury, Nick Ryder, Nick Stathas, Nick Turley, Nik Tezak, Niko Felix, Nithanth Kudige, Nitish Keskar, Noah Deutsch, Noel Bundick, Nora Puckett, Ofir Nachum, Ola Okelola, Oleg Boiko, Oleg Murk, Oliver Jaffe, Olivia Watkins, Olivier Godement, Owen Campbell-Moore, Patrick Chao, Paul McMillan, Pavel Belov, Peng Su, Peter Bak, Peter Bakkum, Peter Deng, Peter Dolan, Peter Hoeschele, Peter Welinder, Phil Tillet, Philip Pronin, Philippe Tillet, Prafulla Dhariwal, Qiming Yuan, Rachel Dias, Rachel Lim, Rahul Arora, Rajan Troll, Randall Lin, Rapha Gontijo Lopes, Raul Puri, Reah Miyara, Reimar Leike, Renaud Gaubert, Reza Zamani, Ricky Wang, Rob Donnelly, Rob Honsby, Rocky Smith, Rohan Sahai, Rohit Ramchandani, Romain Huet, Rory Carmichael, Rowan Zellers, Roy Chen, Ruby Chen, Ruslan Nigmatullin, Ryan Cheu, Saachi Jain, Sam Altman, Sam Schoenholz, Sam Toizer, Samuel Miserendino, Sandhini Agarwal, Sara Culver, Scott Ethersmith, Scott Gray, Sean Grove, Sean Metzger, Shamez Hermani, Shantanu Jain, Shengjia Zhao, Sherwin Wu, Shino Jomoto, Shirong Wu, Shuaiqi, Xia, Sonia Phene, Spencer Papay, Srinivas Narayanan, Steve Coffey, Steve Lee, Stewart Hall, Suchir Balaji, Tal Broda, Tal Stramer, Tao Xu, Tarun Gogineni, Taya Christianson, Ted Sanders, Tejal Patwardhan, Thomas Cunninghman, Thomas Degry, Thomas Dimson, Thomas Raoux, Thomas Shadwell, Tianhao Zheng, Todd Underwood, Todor Markov, Toki Sherbakov, Tom Rubin, Tom Stasi, Tomer Kaftan, Tristan Heywood, Troy Peterson, Tyce Walters, Tyna Eloundou, Valerie Qi, Veit Moeller, Vinnie Monaco, Vishal Kuo, Vlad Fomenko, Wayne Chang, Weiyi Zheng, Wenda Zhou, Wesam Manassra, Will Sheu, Wojciech Zaremba, Yash Patil, Yilei Qian, Yongjik Kim, Youlong Cheng, Yu Zhang, Yuchen He, Yuchen Zhang, Yujia Jin, Yunxing Dai, and Yury Malkov. 2024. GPT-4o system card. arXiv [cs.CL]. Kishore Papineni, Salim Roukos, Todd Ward, and Wei7 Jing Zhu. 2001. BLEU: method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics - ACL 02, Morristown, NJ, USA. Association for Computational Linguistics. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. 2021. Learning transferable visual models from natural language supervision. arXiv:2103.00020 [cs]. Thibault Sellam, Dipanjan Das, and Ankur Parikh. 2020. BLEURT: Learning robust metrics for text generation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 78817892, Stroudsburg, PA, USA. Association for Computational Linguistics. Chantal Shaib, Joe Barrow, Jiuding Sun, Alexa Siu, Byron Wallace, and Ani Nenkova. 2024. Standardizing the measurement of text diversity: tool and comparative analysis of scores. CoRR, abs/2403.00553. Hua Shen, Tiffany Knearem, Reshmi Ghosh, Kenan Alkiek, Kundan Krishna, Yachuan Liu, Ziqiao Ma, Savvas Petridis, Yi-Hao Peng, Li Qiwei, Sushrita Rakshit, Chenglei Si, Yutong Xie, Jeffrey Bigham, Frank Bentley, Joyce Chai, Zachary Lipton, Qiaozhu Mei, Rada Mihalcea, Michael Terry, Diyi Yang, Meredith Ringel Morris, Paul Resnick, and David Jurgens. 2024. Towards bidirectional human-AI alignment: systematic review for clarifications, framework, and future directions. arXiv [cs.HC]. Rakshith Shetty, Marcus Rohrbach, Lisa Anne Hendricks, Mario Fritz, and Bernt Schiele. 2017. Speaking the same language: Matching machine to human captions by adversarial training. In 2017 IEEE International Conference on Computer Vision (ICCV), pages 41554164. IEEE. Nikhil Singh, Guillermo Bernal, Daria Savchenko, and Elena Glassman. 2023. Where to hide stolen elephant: Leaps in creative writing with multimodal machine intelligence. ACM Trans. Comput. Hum. Interact., 30(5):157. Tiberiu Sosea, Iustin Sirbu, Cornelia Caragea, Doina Caragea, and Traian Rebedea. 2021. Using the image-text relationship to improve multimodal disaster tweet classification. Int Conf Inf Syst Crisis Response Manag, pages 691704. Tristan Thrush, Ryan Jiang, Max Bartolo, Amanpreet Singh, Adina Williams, Douwe Kiela, and Candace Ross. 2022. Winoground: Probing vision and language models for visio-linguistic compositionality. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5238 5248. Alakananda Vempala and Daniel Preotiuc-Pietro. 2019. Categorizing and inferring the relationship between the text and image of twitter posts. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 28302840, Stroudsburg, PA, USA. Association for Computational Linguistics. Lydia Weiland, Laura Dietz, and Simone Paolo Ponzetto. 2015. Image with message: Towards detecting nonliteral image usages by visual linking. pages 4047. Association for Computational Linguistics. Chunpu Xu, Hanzhuo Tan, Jing Li, and Piji Li. 2022. Understanding social media cross-modality discourse in linguistic space. In Findings of the Association for Computational Linguistics: EMNLP 2022, pages 24592471, Stroudsburg, PA, USA. Association for Computational Linguistics. Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier. 2014. From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. Transactions of the Association for Computational Linguistics, 2:6778. Zihao Yue, Anwen Hu, Liang Zhang, and Qin Jin. 2023. Learning descriptive image captioning via semipermeable MaxImum likelihood estimation. Neural Inf Process Syst, abs/2306.13460:7912479141. Tianyi Zhang*, Varsha Kishore*, Felix Wu*, Kilian Weinberger, and Yoav Artzi. 2020. BERTScore: Evaluating text generation with BERT. In International Conference on Learning Representations."
        },
        {
            "title": "A Data Preparation",
            "content": "This section sheds light on the methods used while preparing all the datasets mentioned in this paper for model evaluation. We verify both datasets used to evaluate RONA have permissive license that allows usage for research purposes (Tweet Subtitles: MIT License and ANNA: CC BY-NC-SA 4.0 License). A.1 Tweet Subtitles This dataset contains two types of captions for tweets: actual and text generated by an image captioning model. We use only the actual caption as part of our evaluation. A.2 ANNA As mentioned in Section 3, we randomly sample 1,500 examples from ANNA to construct our test set. We use random seed of 42 to ensure reproducibility."
        },
        {
            "title": "B Proprietary Model Details",
            "content": "OpenAI GPT: We access the GPT-4o model via custom deployment using Azure OpenAI. We evaluate gpt-4o-2024-11-20 with custom safety filter to restrict content of high severity level. Anthropic Claude: We access Claude 3.5 Sonnet v2 via the Vertex AI API, using Google Cloud. We evaluate claude-3-5-sonnet-v2@20241022."
        },
        {
            "title": "C Prompt Templates",
            "content": "We use two types of prompt templates, both with different system/user messages for the evaluation of tasks mentioned in Section 4. The different prompts and system messages used are present in the appendix."
        },
        {
            "title": "D Postprocessing MLLM Responses",
            "content": "Since both datasets feature image-caption pairs on wide selection of topics, around 50 images were flagged by the safety filter or rejected by the model for captioning. To ensure test set consistency, we remove these examples from our evaluation."
        },
        {
            "title": "E Example Captions from RONA",
            "content": "We also provide few examples of captions generated in each setting below. All captions were generated using Claude 3.5 Sonnet v2. E.1 ANNA Examples Caption: JUST NUTS: The creaminess of this dessert comes from churned almond milk. Baseline (Image-only) Creamy vanilla ice cream topped with crispy honeycomb pieces. Homemade ice cream dessert garnished with crunchy toffee bits. Sweet and crunchy: vanilla gelato with caramelized sugar shards. Classic vanilla scoops adorned with brittle candy fragments. Artisanal ice cream featuring golden honeycomb crumble topping. RONA (Image-only) Insertion: The perfect way to end summer evening on the patio. Concretization: Vanilla ice cream garnished with crispy honeycomb pieces in blue ceramic bowl. Projection: Sweet memories of childhood desserts come flooding back with every spoonful. Restatement: Scoops of creamy white ice cream topped with crunchy golden candy pieces, served in light blue bowl against wooden background. Extension: This artisanal dessert combines locally-sourced dairy with house-made honeycomb brittle, showcasing how simple ingredients can create an extraordinary taste experience when crafted with care. 9 Baseline (Image + Caption) Baseline (Image-only) Creamy almond milk ice cream topped with Sleepy fox curled up contentedly on white crunchy toasted nut brittle. sheets. Dairy-free frozen dessert made with almond milk and garnished with caramelized nuts. Red fox showing its fluffy belly while napping peacefully. Smooth vegan ice cream featuring almond comfortable fox enjoying lazy day in bed. milk, decorated with crispy nut pieces. Beautiful russet-colored fox in complete reArtisanal almond milk gelato adorned with laxation mode. handmade nutty crumble. Wild spirit at rest: fox taking an adorable Plant-based frozen treat with rich almond flasnooze on soft bedding. vor and crunchy nut topping. RONA (Image + Caption) Insertion: refreshing frozen treat perfect for hot summer days, churned to creamy perfection. Concretization: Scoops of almond milk ice cream topped with crunchy toasted nuts in blue ceramic bowl. Projection: The dairy-free dessert revolution takes shape in this plant-based frozen delight. Restatement: Creamy white almond milk ice cream served in light blue bowl, garnished with crushed nuts on top. Extension: This artisanal vegan ice cream demonstrates how nut-based alternatives can rival traditional dairy desserts, offering both sustainable ingredients and decadent taste while catering to those with dietary restrictions. RONA (Image-only) Insertion: Sweet dreams are made of soft white sheets and peaceful afternoon naps. Concretization: red fox lies contentedly on its back, nestled in crisp white bedding. Projection: Finding complete comfort and trust in ones surroundings is rare and precious thing. Restatement: fox is sleeping upside down on white sheets, with its red-orange fur contrasting against the white background, showing its belly and paws in the air. Extension: Like many domestic pets, this rescued fox has adapted to human comforts, choosing the soft embrace of fresh linens over its natural forest bedding, showing how wild animals can adapt to domesticated environments when given the chance. E.2 Tweet Subtitles Examples Baseline (Image + Caption) cozy fox napping peacefully on white sheets. Red fox curled up in blissful slumber, paws in the air. Playful fox showing its belly while taking comfortable nap. Beautiful russet-colored fox in deep sleep on soft bedding. Adorable fox stretched out and snoring on pristine white linens. Caption: Its raining, its pouring. The sleepy fox is snoring. 10 RONA (Image + Caption) Insertion: The gentle pitter-patter outside creates the perfect atmosphere for afternoon rest. Concretization: sleepy red fox lies curled up, snoring peacefully on white bedding. Projection: Dreaming of chasing rabbits while the storm rages outside. Restatement: red fox is sleeping on its back, completely relaxed on white sheets while its raining outside. Extension: As rain drums against the windowpane, this rescued fox demonstrates how domesticated wildlife can find comfort in human spaces, sprawled out on clean linens just like household pet. 11 System Message for Baseline Captioning You are an expert linguist, and your task is to write image captions. System Message for RONA You are an expert linguist, and your task is to write image captions with the help of Coherence Relations. coherence relation describes the structural, logical, and purposeful relationships between an image and its caption, capturing the authors intent. These are the possible coherence relations you can assign to an image-text pair: - Insertion: The salient object described in the image is not explicitly mentioned in the text. - Concretization: Both the text and image contain mention of the main visual entity. - Projection: The main entity mentioned in the text is implicitly related to the visual objects present in the image. - Restatement: The text directly describes the image contents. - Extension: The image expands upon the story or idea in the text, presenting new elements or elaborations, effectively filling in narrative gaps left by the text."
        },
        {
            "title": "Prompt for Baseline Captioning",
            "content": "System <insert-system-message> User You will be given an image (or) image-caption pair as input. Analyze the image and write 5 suitable captions that are diverse, but relevant. Create diverse captions while retaining the same overall meaning of the original image-caption pair. Return the captions as JSON Array with the following format: [ \"<insert-caption-text-1>\", \"<insert-caption-text-2>\", \"<insert-caption-text-3>\", \"<insert-caption-text-4>\", \"<insert-caption-text-5>\" ] <insert-image> <insert-caption> 12 Prompt for RONA System <insert-system-message> User You will be given an image (or) image-caption pair as input. Write 5 image captions, one for each coherence relation as your output. Return the captions as JSON object with the following format: { \"Insertion\": \"<insert-caption-text-1>\", \"Concretization\": \"<insert-caption-text-2>\", \"Projection\": \"<insert-caption-text-3>\", \"Restatement\": \"<insert-caption-text-4>\", \"Extension\": \"<insert-caption-text-5>\" } <insert-image> <insert-caption>"
        }
    ],
    "affiliations": [
        "National Institute of Technology, Tiruchirappalli",
        "The Pennsylvania State University"
    ]
}