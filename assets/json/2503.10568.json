{
    "paper_title": "Autoregressive Image Generation with Randomized Parallel Decoding",
    "authors": [
        "Haopeng Li",
        "Jinyue Yang",
        "Guoqi Li",
        "Huan Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce ARPG, a novel visual autoregressive model that enables randomized parallel generation, addressing the inherent limitations of conventional raster-order approaches, which hinder inference efficiency and zero-shot generalization due to their sequential, predefined token generation order. Our key insight is that effective random-order modeling necessitates explicit guidance for determining the position of the next predicted token. To this end, we propose a novel guided decoding framework that decouples positional guidance from content representation, encoding them separately as queries and key-value pairs. By directly incorporating this guidance into the causal attention mechanism, our approach enables fully random-order training and generation, eliminating the need for bidirectional attention. Consequently, ARPG readily generalizes to zero-shot tasks such as image inpainting, outpainting, and resolution expansion. Furthermore, it supports parallel inference by concurrently processing multiple queries using a shared KV cache. On the ImageNet-1K 256 benchmark, our approach attains an FID of 1.94 with only 64 sampling steps, achieving over a 20-fold increase in throughput while reducing memory consumption by over 75% compared to representative recent autoregressive models at a similar scale."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 1 ] . [ 1 8 6 5 0 1 . 3 0 5 2 : r a"
        },
        {
            "title": "Autoregressive Image Generation with Randomized Parallel Decoding",
            "content": "Haopeng Li1* Jinyue Yang2 Guoqi Li2 Huan Wang1 1Westlake University 2Institute of Automation, Chinese Academy of Sciences https://github.com/hp-l33/ARPG (a) Class-Conditional Image Generation (b) Quality and Efficiency Comparison (c) Controllable Image Generation (d) Class-Conditional Edit, Inpaint & Outpaint (e) Resolution Expansion Figure 1. Visual Autoregressive Modeling with Randomized Parallel Generation (ARPG): high-quality and efficient framework for image synthesis. ARPG enables (a) class-conditional generation with just 64-step parallel decoding and (b) outperform recent representative works in this line (e.g., VAR [39], LlamaGen [35]) in throughput, memory consumption, and quality. It further supports (c) controllable generation and zero-shot generalization, including (d) class-conditional editing, inpainting, outpainting, and (e) resolution expansion."
        },
        {
            "title": "Abstract",
            "content": "We introduce ARPG, novel visual Autoregressive model that enables Randomized Parallel Generation, addressing the inherent limitations of conventional raster-order approaches, which hinder inference efficiency and zero-shot generalization due to their sequential, predefined token generation order. Our key insight is that effective random-order modeling necessitates explicit guidance for determining the position of the next predicted token. To this end, we propose novel guided decoding framework that decouples positional guidance from content representation, encoding them separately as queries and key-value pairs. By directly incorporating this guidance into the causal attention mech- *Work done during the authors research internship. Corresponding authors: wanghuan@westlake.edu.cn, guoqi.li@ia.ac.cn. anism, our approach enables fully random-order training and generation, eliminating the need for bidirectional attention. Consequently, ARPG readily generalizes to zero-shot tasks such as image inpainting, outpainting, and resolution expansion. Furthermore, it supports parallel inference by concurrently processing multiple queries using shared KV cache. On the ImageNet-1K 256256 benchmark, our approach attains an FID of 1.94 with only 64 sampling steps, achieving over 20-fold increase in throughput while reducing memory consumption by over 75% compared to representative recent autoregressive models at similar scale. 1. Introduction Autoregressive (AR) models have shown exceptional performance and scalability [12, 15], particularly in large language models [1, 8], where the next-token prediction paradigm has driven significant advancements [1, 8, 12, 15]. This success has extended to the visual domain, enabling breakthroughs in autoregressive image generation, exemplified by methods such as VQGAN [7] and others [19, 30, 35, 38, 46, 52]. However, applying next-token prediction to image generation poses distinct challenges. While text exhibits inherent causal dependencies, images are structured in two-dimensional spatial domain, presenting challenge for AR models, which typically operate on sequences and require images to be flattened in predefined order (e.g., raster scan). As generation must follow specific token order, AR models struggle with zero-shot generalization tasks that involve non-causal dependencies, such as inpainting and outpainting. Moreover, this token-by-token generation is inefficient, particularly in high-resolution scenarios. To address these challenges, alternative approaches, such as MaskGIT [2], have adopted masked modeling [16] approach for parallel token generation in random order. However, the reliance on bidirectional attention prevents the use of the KV cache, resulting in high computational overhead. Recent work RandAR [25] enables fully randomorder training and inference with causal attention via positional instruction tokens, but incurs significant memory and computational costs due to the increased sequence length. These limitations highlight the need for an autoregressive framework that supports flexible generation orders while maintaining computational efficiency. To this end, we introduce ARPG, novel visual autoregressive model that enables training and inference in fully random token orders through guided decoding framework. Unlike standard self-attention in decoder-only Transformers [42], which relies on data-dependent queries, keys, and values, we replace the queries with learnable data-independent tokens whose position embeddings are dynamically shifted to align with the next predicted token. Through causal cross-attention between these target-aware queries and data-dependent key-value pairs, the model is explicitly guided to predict tokens at arbitrary positions within randomly ordered sequence based on known tokens in non-contiguous order. Additionally, by decoupling the representations of queries and key-value pairs, ARPG enables parallel decoding by independently processing multiple queries in single step. This alleviates the aforementioned constraints of next-token prediction, allowing for parallel generation in random token order while preserving causality, thereby enabling ARPG to still leverage the KV cache to further enhance inference efficiency. Extensive experiments demonstrate that our method exhibits strong competitiveness in both class-conditional and controllable image generation. On the ImageNet-1K [5] 256256 benchmark, it achieves high-quality synthesized images with Frechet Inception Distance (FID) [13] of 1.94, surpassing raster-order baselines while maintaining It also estabremarkable efficiency, as shown in Fig. 1. lishes state-of-the-art performance in random-order causal modeling [25]. Furthermore, our method achieves leading performance in controllable image generation conditioned on canny edges and depth maps. Our main contributions can be summarized as follows: 1. We propose novel visual autoregressive framework that enables parallel image generation with random token order using decoupled explicitly guided decoding, overcoming the inefficiencies and poor generalization of traditional next-token prediction methods. 2. Leveraging the capability of random modeling, we explore the zero-shot generalization ability of our method and further extend it to more versatile controllable generative method. 3. Extensive experiments demonstrate that our approach achieves competitive generation quality while simultaneously excelling in throughput and memory consumption, setting new benchmark for high-performance and high-efficiency autoregressive image generation. 2. Related Work Causal Sequence Modeling. State-of-the-art large language models [1, 8] adopt decoder-only Transformers [42] for causal modeling of language sequences and autoregressive generation, method commonly known as the GPT [1] style approach. In the vision domain, images can be quantized into discrete tokens [41] and flattened from 2D to 1D, enabling generation via the next-token prediction paradigm, as seen in models like VQGAN [7], LlamaGen [35] etc. [19, 20, 43, 43, 50]. These methods have demonstrated impressive generative performance. However, this token-by-token image generation approach is inefficient, especially when dealing with high-resolution images. Additionally, since the generation can only proceed in specific token order, it encounters difficulties in zeroshot inference that require non-causal dependencies, such as inpainting and outpainting. Masked Sequence Modeling. Unlike causal sequence modeling, another mainstream approach to sequence modeling is the encoder-only [29, 42, 48, 51] architecture with bidirectional attention, where multiple tokens in sequence are randomly masked and then predicted. This method is widely used in BERT [16] for language modeling. In the vision domain, MaskGIT etc. [2, 3, 9, 10, 21, 45, 49, 51] adopts the same approach for image modeling and generation. By leveraging bidirectional attention, this masked image generation method eliminates the constraints of causal dependencies, enabling multi-token generation in single step with randomized order, leading to significantly faster inference. However, due to the absence of KV cache in bidirectional attention, inference efficiency remains limited. y2 y4 y3 y1 y2 y3 y1 y2 y4 y3 y"
        },
        {
            "title": "ARPG",
            "content": "p2 p4 p3 p1 x2 p4 x4 p3 x3 c p2 x2 p4 x4 x3 p1 x2 x3 class token / image token target-aware positional embedding class token / image token position instruction token class token / image token target-aware query Figure 2. Comparison of different methods. RAR [50]: The position of the next predicted token is fused into the current token via (cid:1). RandAR [25]: additive positional embedding. Its joint probability distribution is defined as (cid:81)n The position of the next predicted token is explicitly inserted as an instructional token within the input sequence. This yields the probability (cid:1). ARPG (ours): The position of the next predicted token is integrated as query within distribution (cid:81)n the attention mechanism. The corresponding probabilistic model, formalized in Eq. (8). i=1 p(cid:0)xτi xτ1 + pτ2 , . . . , xτi1 + pτi i=1 p(cid:0)xτi xτ1 , pτ2 , . . . , xτi1 , pτi 3. Method 3.1. Rethinking Autoregressive Modeling Preliminaries. Given sequence = {x1, x2, . . . , xn}, an autoregressive model predicts the next token based on all preceding tokens, following the probabilistic formulation: p(x) = (cid:89) i= p(xi x1, . . . , xi1). (1) This strict causal dependency enforces fixed generation order. While effective, it inherently limits flexibility in tasks requiring non-sequential token interactions. In masked sequence modeling, the model receives the entire sequence with certain tokens masked and employs bidirectional attention to predict the [MASK] tokens. In this case, the scope of prediction is more relaxed, as all positions of the [MASK] tokens can be predicted. The training objective is to minimize the negative log-likelihood of the masked tokens [2]: = (cid:88) mi= log p(xi XM), {1, 2, , n}, (2) where = {m1, m2, . . . , mn} {0, 1}n is mask sequence. The corrupted sequence XM is derived from the original sequence by replacing each xi with the [MASK] token if mi = 1, and leaving xi unchanged otherwise. Key Insights. Comparing the characteristics of the two methods discussed earlier, we find that: Insight. 1 Breaking the order-specific constraints of autoregressive model requires explicit positional guidance. This ensures that the model knows which token should be predicted next. Without such guidance, when two sequences share initial tokens but diverge subsequently, identical hidden states for the shared prefix create prediction ambiguity [50]. Furthermore, in masked sequence modeling, while unmasked tokens participate in attention computation, the loss function operates exclusively on masked tokens, as shown in Eq. (2). This observation leads to: Insight. 2 The queries corresponding to unmasked tokens receive no gradients from the loss function and thus do not contribute to the learning process during training. Proof. Given input sequences with length n, and q, k, R1d are vectors with d-dimensional features. The softmax attention mechanism (omit the scale term) operates as: oi = (cid:80)n j=1 exp(qik (cid:80)n j=1 exp(qik )vj ) R1d . (3) Let Sij = qik , Pij = softmax(Si)j. It is evident that dPij = doiv and the derivative of the softmax function is its Jacobian matrix. Using the fact that the Jacobian of = softmax(x) is diag(y) yy [4], we have: dSi = dPi(diag(Pi) = Pi dPi (doio Pi) )Pi , where denotes the Hadamard product. Therefore: dSij = Pij(dPij doio ) . Then we derive gradient for using chain rule: dqi = (cid:88) j=1 dSijkj = (cid:88) j=1 Pij(doiv doio )kj . (4) (5) (6) (7) Obviously, when oi corresponds to an unmasked token, it does not contribute to the loss calculation, resulting in doi = 0, and consequently, dqi = 0. Finally, since [MASK] tokens encode only positional information without contributing any contextual representations, we assume that: Insight. 3 The [MASK] tokens are not only unnecessary but also detrimental to the causality in token mixing. 3 Query Feed-Forward Cross-Attention Feed-Forward Self-Attention Key Value Training kc k2 k4 k3 1 2 2 v2 vc v4 o2 o4 o3 o1 q2 q3 q1 Parallel Decoding Cached-Key kc k4 k3 Multi-Query 1 Multi-Output q7 q5 2 o7 o5 v2 vc v4 v3 Cached-Value (a) Architecture (b) Training and inference details of guided-decoding Figure 3. (a) ARPG architecture: We employ Two-Pass Decoder architecture. In the first pass, self-attention layers extract contextual representations of the image token sequence as global key-value pairs. In the second pass, cross-attention layers use target-aware queries that attend to these global key-value pairs to guide prediction. (b) During training, the number of queries matches the number of key-value pairs. Each keys positional embedding reflects its actual position, while each querys positional embedding is right-shifted to align with its target position. During inference, multiple queries are input simultaneously, sharing common KV cache to enable parallel decoding. Reformulation. Let τ = (τ1, . . . , τn) be permutation of generation indices. Based on the above insights, we propose permuted autoregressive modeling with decoupled positional conditioning: p(x) = (cid:89) i= p(cid:0)xτi+1 {xτj }i j=1, mτi+1 (cid:1) , (8) where {xτj }i j=1 represents content conditioning from previously generated tokens, and mτi+1 explicitly encodes the target position τi+1 based on Insight 1, similar as [MASK] token in masked sequence modeling. Based on Insight 2, when implementing Eq. (8) using scaled dot-product attention [42], we remove all content tokens (unmasked tokens) from the queries, making the queries completely dataindependent. Since the queries already carry the position information of the target to guide the models prediction, combined with Insight 3, we removed all [MASK] tokens from the key-value pairs, making the key-value pairs completely data-dependent. This decoupling mechanism between queries and key-value pairs ensures that each prediction from [MASK] token to the target is independent of one another and follows the causal relationship. As result, our method can use causal attention to achieve permuted autoregressive modeling and also enable efficient parallel decoding. Details will be presented in Sec. 3.2. Comparison with Other Methods. Previous methods, like RAR [50], introduced target-aware absolute positional embedding to specify the position of the next predicted token, as shown in Fig. 2 (left), but remained optimal under raster-order generation, limiting parallel decoding and zero-shot inference. RandAR [25] introduced position instruction tokens, which explicitly encode the next tokens position by inserting separate tokens into the sequence, as shown in Fig. 2 (middle). However, this incurs significant computational and memory overhead during training and inference, as discussed in the Sec. 4. Building on our theoretical reformulation in Eq. (8), our proposed method ARPG decouples content representation and explicit position guidance into key-value pairs and queries, as shown in Fig. 2 (right). It can achieve random modeling without incurring any additional computational or memory overhead. 3.2. Randomized & Parallelized AR Model Randomized Modeling. For target position τi where 1 < n, each query qτi is uniquely obtained by embedding data-independent [MASK] token with τi-specific 2D rotary positional encoding (RoPE) [34]: qτi = RoPE(mWq, τi) R1d . (9) By decoupling the positional guidance (queries) and content representation (keys/values), each target-aware query can independently attend to the context-aware key-value pairs to guide the model in predicting the token at specific position. The Eq. (8) can be implemented through softmaxattention [42] (omit the scale term): = RoPE(XWk, τ1:n1), = XWv, oτi = softmax((qτiK) M)V, (10) (11) where Wq, Wk, Wv Rdd are the learnable weights. R(n1)d is matrix formed by stacking sequence of input tokens. The causal mask is defined such that Mij = 1 if and otherwise, which prevents the model from attending to future tokens. As shown in Fig. 3b, during the training phase, we broadcast the [MASK] token to match the length of the input sequence and apply positional encoding that is shifted to the right relative to the input (aligned with the target) for each element. By employing the aforementioned method that integrates predicted positional guidance into the causal attention, we perform conventional teacher-forcing training on the random permuted sequences, thereby achieving randomized autoregressive modeling based on causal structure. Parallel Decoding. Based on the premise that all tokens to be predicted are independent of each other, our method inherently supports parallel decoding through batched query processing, as shown in Fig. 3b. In this case, Eq. (11) can be rewritten in full attention form without the causal mask (omit the scale term): = softmax(QK)V, (12) where = {qτi}j i=1 denotes the set of queries corresponding to target positions. These queries independently attend to all previously generated tokens (KV cache), ensuring causal consistency during incremental generation. In contrast to conventional cross-attention where queries derive from input tokens and key-value pairs originate from conditional representations, constraining the output sequence length to match that of the input, our approach circumvents such length-matching restrictions by interchanging the roles of the inputs and the conditions. This design effectively prevents attention conflicts among multiple generation targets. The resulting parallelism is direct consequence of our fundamental design choice: treating positional guidance and content mixing as orthogonal processes, which effectively breaks the sequential dependency limitation of traditional autoregressive models. Architecture Design. The proposed method enables random-order modeling by mapping [MASK] tokens with positional information (which is the in our case) to their corresponding target tokens: = + Attn(Q, K, V). (13) The hidden layer features are essentially obtained by updating with the residual representation [11] learned through the causal attention mechanism Attn(). However, since guided decoding treats input tokens solely as keys and values, they do not participate in the mixing process during attention. In other words, input tokens remain identity representations throughout the process. To fully leverage contextual representations from the input and better support the query-to-target mapping, we first apply standard causal selfattention to the input tokens. The resulting contextualized representations are then used as the conditioning context for Table 1. Architectural design of different models. + layers represent the first-pass decoder and the second-pass decoder, which are layers and layers, respectively. Model Layers Hidden Size Heads Parameters ARPG-L ARPG-XL ARPG-XXL 12+12 18+18 24+24 1024 1280 1536 16 20 24 320 719 1.3 cross-attention, as illustrated in Fig. 3a and follows: = + Attn(XW (1) ot = qt + Attn(qtW (2) , XW (1) , HW (2) , XW (1) ), , HW (2) ). (14) (15) We refer to this structure as 2-pass decoder. We will thoroughly investigate the influence of two decoder configuration ratios on performance and efficiency in Sec. 4. 4. Experiments 4.1. Implementation Details Model. Our architecture follows the design style of It employs SwishGLU [33] for its feedLlama [35, 40]. forward layers, RMSNorm [40] for normalization, and 2D rotary position encoding (RoPE) [34]. We use the LlamaGen tokenizer [35] with 16 downsampling factor and In each batch, image token sea 16,384-size codebook. quences are randomly shuffled with the class token placed at the start. To maintain alignment despite varying sequence orders, the RoPE frequencies are extended along the batch dimension and shuffled accordingly. We use global normalization and projection term to generate global key-value pairs from the first-pass decoder to improve efficiency [37]. Note that our focus is on the methodology of random-order autoregressive modeling, so we do not incorporate techniques like QK normalization, AdaLN [26], linear attention [27, 28, 36, 47], or other enhancements. Training. We train three models of different scales (details in Tab. 1) on the 256256 ImageNet-1K [5] for 400 epochs. Generation results are shown in Fig. 4a. The initial learning rate is 1e-4 per 256 batch size with 100 epochs warmup and decays to 1e-5 with cosine scheduler. The optimizer used is AdamW [17] with β1 = 0.99 and β2 = 0.95 decay. We apply the class embedding dropout of 0.1 for classifier-free guidance (CFG) [14]. Metrics. We use FID [13] as the primary evaluation metric, and additionally evaluate using Inception Score (IS) [32], precision and recall [18], following the evaluation protocol of ADM [6]. We evaluate the inference efficiency of the models using throughput and memory consumption. All efficiency evaluations were conducted on the NVIDIA A800-80GB GPU, considering only the token generation process and excluding the tokenizer execution. 5 Table 2. Overall comparisons on class-conditional ImageNet 256256 benchmark. The symbols and indicate that lower and higher values are better. -re: rejection sampling. All models were evaluated with batch size of 64 and at bfloat16 precision. Type Model Parameters Steps Throughput Memory FID IS Precision Recall Diffusion Mask VAR LDM-4 [31] DiT-L/2 [26] DiT-XL/2 [26] MaskGIT [2] MAR-B [22] MAR-L [22] VAR-d16 [39] VAR-d20 [39] VAR-d24 [39] AR (Raster) VQGAN-re [7] RQ-Trans.-re [19] LlamaGen-L [35] LlamaGen-XL [35] LlamaGen-XXL [35] PAR-L [44] PAR-XL [44] PAR-XXL [44] AiM-L [20] AiM-XL [20] RAR-L [50] RAR-XL [50] AR (Random) RandAR-L [25] RandAR-XL [25] RandAR-XXL [25] AR (Random) ARPG-L ARPG-L ARPG-XL ARPG-XXL 400 458 675 227 208 479 310 600 1.0 1.4 3.8 343 775 1.4 B"
        },
        {
            "title": "320 M\n320 M\n719 M\n1.3 B",
            "content": "250 250 250 8 100 100 10 10 10 256 64 576 576 576 147 147 256 256 256 256 88 88 88 32 64 64 64 0.83 it/s 1.32 it/s 0.91 it/s 46.18 it/s 1.71 it/s 1.27 it/s 114.42 it/s 73.65 it/s 48.90 it/s 5.92 it/s 11.63 it/s 4.33 it/s 2.46 it/s 1.58 it/s 14.77 it/s 7.91 it/s 5.23 it/s 26.47 it/s 18.68 it/s 12.08 it/s 8.00 it/s 25.30 it/s 15.51 it/s 10.46 it/s 113.01 it/s 62.12 it/s 35.89 it/s 25.39 it/s 5.71 GB 1.62 GB 2.14 GB 1.71 GB 1.47 GB 2.32 GB 10.97 GB 16.06 GB 22.43 GB 15.15 GB 18.43 GB 10.23 GB 17.11 GB 26.22 GB"
        },
        {
            "title": "2.54 GB\n2.43 GB\n4.48 GB\n7.31 GB",
            "content": "3.60 5.02 2.27 6.18 2.31 1.78 3.30 2.57 2.09 5.20 3.80 3.07 2.62 2.62 3.76 2.61 2. 2.83 2.56 1.70 1.50 2.55 2.25 2.15 2.44 2.44 2.10 1.94 247.7 167.2 278.2 182.1 281.7 296. 274.4 302.6 312.9 280.3 323.7 256.1 244.1 244.1 218.9 259.2 263.2 244.6 257.2 299.5 306. 288.8 317.8 322.0 291.7 287.1 331.0 339.7 - 0.75 0.83 0.80 0.82 0.81 0.84 0.83 0.82 - - 0.83 0.80 0.80 0.81 0.80 0.80 0.82 0.82 0.81 0.80 0.81 0.80 0.79 0.82 0.82 0.79 0. - 0.57 0.57 0.51 0.57 0.60 0.51 0.56 0.59 - - 0.52 0.57 0.57 0.60 0.62 0. 0.55 0.57 0.60 0.62 0.58 0.60 0.62 0.55 0.55 0.61 0.59 Table 3. Model comparisons on class-conditional ImageNet 512512 benchmark. The evaluation configuration is the same as that in Tab. 2. OOM: Out of Memory."
        },
        {
            "title": "Model",
            "content": "Throughput Memory FID IS ADM [6] DiT-XL/2 [26] MaskGIT [2] VQGAN [7] VAR-d36 [39] - 0.18 it/s 4.48 it/s 0.63 it/s - 23.24 - GB 3.04 4.70 GB 7.63 GB 7.32 44.12 GB 26.52 2."
        },
        {
            "title": "OOM",
            "content": "101.0 240.8 156.0 66.8 303.2 ARPG-XL 35.53 it/s"
        },
        {
            "title": "13.98 GB",
            "content": "3.38 257.8 1.94, respectively. ARPG achieves competitive generation quality compared to existing works while offering significant improvements in throughput and memory efficiency. Compared to the raster-order-based LlamaGen [35], ARPG achieves over 20 higher throughput. Compared to VAR [39], ARPG achieves comparable throughput while reducing memory consumption by over 75%. Furthermore, compared to recent parallel decoding methods such as PAR [44] and RandAR [25], ARPG achieves better generation quality with fewer sampling steps, while delivering superior throughput and memory efficiency. 4.2. Class-Conditional Image Generation Overall Comparison. We compare the results of ARPG with existing methods on the ImageNet-1K 256256 and 512512 benchmarks, as shown in Tab. 2 and Tab. 3. On the 256256 benchmark, ARPG variants at three scales, L, XL, and XXL, achieve FID scores of 2.44, 2.07, and For 512512 resolution, we fine-tune the XL model (pre-trained at 256256 resolution) instead of training from scratch to save computational resources. With only 50epoch fine-tuning, it achieves competitive quality while surpassing others in throughput, as shown in Tab. 3. Notably, VAR-d36 [39] (2.3B parameters) fails evaluation at batch size 64 under half-precision due to out-of-memory issues. 6 (a) Class-Conditional Image Generation (b) Controllable Image Generation Figure 4. Generated samples of ARPG on ImageNet-1K 256256: (a) Class-conditional image generation. (b) Controllable image generation with canny edges and depth maps as conditions respectively. All images are sampled using 64 steps. Table 4. Controllable generation on ImageNet-1K. The FID of ControlVAR [23] is estimated based on its histograms [24]."
        },
        {
            "title": "Model",
            "content": "#Params ControlVAR [23] ControlAR [24] VAR-d16 VAR-d20 VAR-d24 VAR-d30 AiM-L LlamaGen-L"
        },
        {
            "title": "ControlARPG",
            "content": "ARPG-L 310M 600M 1.0B 2.0B 350M 343M 320M FID Canny Depth 16.20 13.00 15.70 7. 9.66 7.69 7.39 13.80 13.40 12.50 6.50 7.39 4.19 4.06 4.3. Controllable Generation Our method is also compatible with more fine-grained controllable generation, such as image-to-image translation tasks like generating images from depth maps. We only need to replace the [MASK] tokens used as the queries in the model with the condition tokens and align the queries with the targets as described in Sec 3. This method efficiently and effectively achieves controllable image generation that supports random-order generation and parallel decoding. We use canny edges and depth maps as conditions to perform fine-tuning on the class-conditional model pretrained on ImageNet-1K. The results are shown in Tab. 4 and Fig. 4b. Our method outperforms ControlVAR [23] and ControlAR [24], achieving state-of-the-art performance. 4.4. Zero-Shot Generalization Since our method supports image generation in random order, it can directly and simply accomplish zero-shot generalization tasks. For image inpainting, We prefill the KV cache with all tokens from the non-repaint regions along with class token. Tokens in the repaint regions are then Figure 5. Samples of zero-shot inference. Top: Inpainting and class-conditional editing. Bottom: Outpainting. replaced with [MASK] tokens, which are used to generate image tokens based on the prefilled KV cache. For classconditional editing, the operations are the same as those for inpainting, with the only difference being that the class token needs to be replaced with the editing target. For outpainting and resolution expansion, the model leverages the extrapolation capability [34] of RoPE and follows the aforementioned paradigm of prefilling existing tokens and decoding unknown tokens to achieve the task. More details are shown in Sec. B. Fig. 5 demonstrates our methods robust and versatile zero-shot generalization capabilities. 4.5. Ablation Study Effect of Decoder Design. We explored the impact of the 2-pass decoder architecture design on the model. As shown in Tab. 5, the higher the proportion of the guided decoders, the higher the inference efficiency, but the more severe the deterioration of the generation quality. Reducing the proportion of the guided decoder not only significantly reduces inference efficiency but also degrades the generation quality. When there are no guided decoders at all, the model degenerates into an ordinary AR model and completely loses 7 Table 5. Ablation study. We adopt ARPG-L trained for 150 epochs as the baseline. Here, Longer Training refers to 400-epoch training process. Randomize and Parallelize mean the ability to generate in random order and perform parallel decoding, respectively. Description Parameters Layers Randomize Parallelize Steps Throughput Memory FID IS ARPG-L + Longer Training + w/o Shared KV Fewer Guided Decoder More Guided Decoder w/o Guided Decoder Guided Decoder Only 320 320 343 332 307 343 295 12 + 12 12 + 12 12 + 12 18 + 6 6 + 18 24 + 0 0 + Table 6. Quantitative evaluation of efficiency-quality trade-off. Models evaluated on NVIDIA A800-80GB GPU with batch size of 64 and bfloat16 precision. wcfg: The classifier-free guidance scale for achieving the best FID."
        },
        {
            "title": "Steps",
            "content": "Throughput Memory FID XL"
        },
        {
            "title": "XXL",
            "content": "5.5 4.5 4.5 4.5 8.0 6.0 6.0 6.0 10.0 7.5 7.5 7.5 32 64 128 256 32 64 128 256 32 64 128 113.01 it/s 62.12 it/s 32.28 it/s 16.32 it/s 67.87 it/s 38.64 it/s 20.36 it/s 10.52 it/s 44.17 it/s 26.94 it/s 14.63 it/s 7.25 it/s"
        },
        {
            "title": "7.33 GB\n7.31 GB\n7.31 GB\n7.33 GB",
            "content": "2.44 2.44 2.46 2.41 2.21 2.10 2.09 2.07 2.08 1.94 1.97 1.95 the ability of random-order generation and parallel decoding. Using the symmetric structure described in Sec. 3 achieves balance between efficiency and performance. Efficiency. We quantitatively evaluated the impact of parallel decoding on both the models generation quality and inference efficiency. As shown in Tab. 6, thanks to its parallel decoding capability, ARPGs inference throughput far exceeds that of most AR methods, and its memory consumption remains minimal due to the shared KV cache. Notably, reducing the number of sampling steps does not significantly decrease generation quality; in fact, for models at certain scales, fewer steps, such as 64, can yield the best performance. Even when reduced to 32 steps, although there is some quality drop, it remains highly competitive. Effect of Shared KV. We also examined the impact of using shared KV, as discussed in the Sec. 3. As shown in Tab. 5, without shared KV, although the generation quality of the model is slightly improved, it significantly affects the inference speed and memory consumption. To balance the generation quality and inference efficiency, we choose to use the shared KV design in subsequent experiments. 8 64 64 64 64 64 256 64 62.12 it/s 62.12 it/s 48.02 it/s 50.72 it/s 66.11 it/s 11.70 it/s 72.26 it/s 2.43 GB 2.43 GB 3.83 GB 3.19 GB 1.67 GB 4.96 GB 0.91 GB 3.51 2.44 2.37 3.82 3.51 >90 4.57 282.7 287.1 299.7 223.0 242.5 <50 255.9 Table 7. Effect of different generation orders tuned on random order based pre-trained mode. Order Steps FID IS Precision Recall Raster Spiral-in Spiral-out Z-curve Alternate Random 256 256 256 256 256 64 2.49 3.71 4.11 2.56 2.56 2.44 277.6 221.1 210.5 278.2 279.4 287.1 0.79 0.75 0.74 0.78 0.78 0.81 0.58 0.57 0.56 0.51 0.54 0.55 Effect of Generation Order. ARPG supports training and generation in fully random token order. However, we also evaluated its performance under specific orders, including raster order and several alternatives [7], as shown in Tab. 7. While random-order modeling is more challenging due to the n! possible orderings, our method still outperforms generation with fixed orders. Notably, raster-order yields higher generation quality than other structured orders and is comparable to random order generation. However, its fixed modeling bias limits parallel decoding and zero-shot generalization, as previously emphasized. 5. Conclusion In this work, we propose novel autoregressive image generation framework that can parallelly generate images in random token orders, breaking the limitations of the inefficiency of the next token prediction paradigm and its poor zero-shot generalization ability. While achieving superior image generation quality, our method significantly outperforms existing approaches in terms of inference throughput and memory consumption. In addition, we have also extended it to an effective method for controllable image generation. Our solution offers brand-new and highly promising paradigm for autoregressive image generation, as well as set of advanced benchmarks for the quality and efficiency of image generation models that can serve as reference in the field. However, due to computational resource constraints, our work does not extend to text-to-image generation. In future work, we will further explore the potential of our method in large-scale text-to-image models."
        },
        {
            "title": "References",
            "content": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. 1, 2 [2] Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William Freeman. Maskgit: Masked generative image transformer. In CVPR, 2022. 2, 3, 6 [3] Huiwen Chang, Han Zhang, Jarred Barber, AJ Maschinot, Jose Lezama, Lu Jiang, Ming-Hsuan Yang, Kevin Murphy, William Freeman, Michael Rubinstein, et al. Muse: Textto-image generation via masked generative transformers. In ICML, 2023. 2 [4] Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Re. Flashattention: Fast and memory-efficient exact attention with io-awareness. In NeurIPS, 2022. 3 [5] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: large-scale hierarchical image database. In CVPR, 2009. 2, 5 [6] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. In NeurIPS, 2021. 5, 6 [7] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis. In CVPR, 2021. 2, 6, 8 [8] DeepSeek-AI et al. Deepseek llm: Scaling open-source language models with longtermism, 2024. 1, 2 [9] Lijie Fan, Tianhong Li, Siyang Qin, Yuanzhen Li, Chen Sun, Michael Rubinstein, Deqing Sun, Kaiming He, and Yonglong Tian. Fluid: Scaling autoregressive text-to-image generative models with continuous tokens. arXiv preprint arXiv:2410.13863, 2024. 2 [10] Shanghua Gao, Pan Zhou, Ming-Ming Cheng, and Shuicheng Yan. Masked diffusion transformer is strong image synthesizer. In ICCV, 2023. 2 [11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. In CVPR, Deep residual learning for image recognition. 2016. 5 [12] Tom Henighan, Jared Kaplan, Mor Katz, Mark Chen, Christopher Hesse, Jacob Jackson, Heewoo Jun, Tom Brown, Prafulla Dhariwal, Scott Gray, et al. Scaling laws arXiv preprint for autoregressive generative modeling. arXiv:2010.14701, 2020. 1, 2 [13] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. In NeurIPS, 2017. 2, 5 [14] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. 5 [15] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. 1, [16] Jacob Devlin Ming-Wei Chang Kenton and Lee Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In NAACL, 2019. 2 [17] Diederik Kingma. Adam: method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. 5 [18] Tuomas Kynkaanniemi, Tero Karras, Samuli Laine, Jaakko Lehtinen, and Timo Aila. Improved precision and recall metric for assessing generative models. In NeurIPS, 2019. 5 [19] Doyup Lee, Chiheon Kim, Saehoon Kim, Minsu Cho, and Wook-Shin Han. Autoregressive image generation using residual quantization. In CVPR, 2022. 2, 6 [20] Haopeng Li, Jinyue Yang, Kexin Wang, Xuerui Qiu, Yuhong Chou, Xin Li, and Guoqi Li. Scalable autoregressive image generation with mamba. arXiv preprint arXiv:2408.12245, 2024. 2, 6 [21] Tianhong Li, Huiwen Chang, Shlok Mishra, Han Zhang, Dina Katabi, and Dilip Krishnan. Mage: Masked generative encoder to unify representation learning and image synthesis. In CVPR, 2023. [22] Tianhong Li, Yonglong Tian, He Li, Mingyang Deng, and Kaiming He. Autoregressive image generation without vector quantization. In NeurIPS, 2024. 6 [23] Xiang Li, Kai Qiu, Hao Chen, Jason Kuen, Zhe Lin, Rita Singh, and Bhiksha Raj. Controlvar: Exploring conarXiv preprint trollable visual autoregressive modeling. arXiv:2406.09750, 2024. 7 [24] Zongming Li, Tianheng Cheng, Shoufa Chen, Peize Sun, Haocheng Shen, Longjin Ran, Xiaoxin Chen, Wenyu Liu, Controlar: Controllable image and Xinggang Wang. arXiv preprint generation with autoregressive models. arXiv:2410.02705, 2024. 7 [25] Ziqi Pang, Tianyuan Zhang, Fujun Luan, Yunze Man, Hao Tan, Kai Zhang, William Freeman, and Yu-Xiong Wang. Randar: Decoder-only autoregressive visual generation in random orders. arXiv preprint arXiv:2412.01827, 2024. 2, 3, 4, 6 [26] William Peebles and Saining Xie. Scalable diffusion models with transformers. In ICCV, 2023. 5, [27] Bo Peng, Daniel Goldstein, Quentin Anthony, Alon Albalak, Eric Alcaide, Stella Biderman, Eugene Cheah, et al. Eagle and finch: Rwkv with matrix-valued states and dynamic recurrence. arXiv preprint arXiv:2404.05892, 2024. 5 [28] Zhen Qin, Songlin Yang, Weixuan Sun, Xuyang Shen, Dong Li, Weigao Sun, and Yiran Zhong. Hgrn2: Gated linear rnns with state expansion. In Conference on Language Modeling, 2024. 5 [29] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter Liu. Exploring the limits of transfer learning with unified text-to-text transformer. JMLR, 21(140):167, 2020. 2 [30] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In ICML, 2021. 2 [31] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In CVPR, 2022. 6 [32] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. In NeurIPS, 2016. 5 9 [47] Songlin Yang, Bailin Wang, Yikang Shen, Rameswar Panda, and Yoon Kim. Gated linear attention transformers with hardware-efficient training. In ICML, 2024. 5 [48] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime G. Carbonell, Ruslan Salakhutdinov, and Quoc V. Le. Xlnet: Generalized autoregressive pretraining for language understanding. In NeurIPS, 2019. [49] Lijun Yu, Yong Cheng, Kihyuk Sohn, Jose Lezama, Han Zhang, Huiwen Chang, Alexander G. Hauptmann, MingHsuan Yang, Yuan Hao, Irfan Essa, and Lu Jiang. Magvit: Masked generative video transformer. In CVPR, 2023. 2 [50] Qihang Yu, Ju He, Xueqing Deng, Xiaohui Shen, and LiangChieh Chen. Randomized autoregressive visual generation. arXiv preprint arXiv:2411.00776, 2024. 2, 3, 4, 6 [51] Qihang Yu, Mark Weber, Xueqing Deng, Xiaohui Shen, Daniel Cremers, and Liang-Chieh Chen. An image is worth 32 tokens for reconstruction and generation. arXiv preprint arXiv:2406.07550, 2024. 2 [52] Chunting Zhou, Lili Yu, Arun Babu, Kushal Tirumala, Michihiro Yasunaga, Leonid Shamis, Jacob Kahn, Xuezhe Ma, Luke Zettlemoyer, and Omer Levy. Transfusion: Predict the next token and diffuse images with one multi-modal model. arXiv preprint arXiv:2408.11039, 2024. 2 [33] Noam Shazeer. Glu variants improve transformer. arXiv preprint arXiv:2002.05202, 2020. 5 [34] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. 4, 5, [35] Peize Sun, Yi Jiang, Shoufa Chen, Shilong Zhang, Bingyue Peng, Ping Luo, and Zehuan Yuan. Autoregressive model beats diffusion: Llama for scalable image generation. arXiv preprint arXiv:2406.06525, 2024. 1, 2, 5, 6 [36] Yutao Sun, Li Dong, Shaohan Huang, Shuming Ma, Yuqing Xia, Jilong Xue, Jianyong Wang, and Furu Wei. Retentive network: successor to transformer for large language models. arXiv preprint arXiv:2307.08621, 2023. 5 [37] Yutao Sun, Li Dong, Yi Zhu, Shaohan Huang, Wenhui Wang, Shuming Ma, Quanlu Zhang, Jianyong Wang, and Furu Wei. You only cache once: Decoder-decoder architectures for language models. arXiv preprint arXiv:2405.05254, 2024. 5 [38] Chameleon Team. Chameleon: Mixed-modal early-fusion foundation models. arXiv preprint arXiv:2405.09818, 2024. 2 [39] Keyu Tian, Yi Jiang, Zehuan Yuan, Bingyue Peng, and Liwei Wang. Visual autoregressive modeling: Scalable image generation via next-scale prediction. In NeurIPS, 2024. 1, 6 [40] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. 5 [41] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. In NeurIPS, 2017. 2 [42] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS, 2017. 2, 4 [43] Xinlong Wang, Xiaosong Zhang, Zhengxiong Luo, Quan Sun, Yufeng Cui, Jinsheng Wang, Fan Zhang, Yueze Wang, Zhen Li, Qiying Yu, Yingli Zhao, Yulong Ao, Xuebin Min, Tao Li, Boya Wu, Bo Zhao, Bowen Zhang, Liangdong Wang, Guang Liu, Zheqi He, Xi Yang, Jingjing Liu, Yonghua Lin, Tiejun Huang, and Zhongyuan Wang. Emu3: Next-token prediction is all you need. arXiv preprint arXiv:2409.18869, 2024. 2 [44] Yuqing Wang, Shuhuai Ren, Zhijie Lin, Yujin Han, Haoyuan Guo, Zhenheng Yang, Difan Zou, Jiashi Feng, and Xihui Liu. Parallelized autoregressive visual generation. arXiv preprint arXiv:2412.15119, 2024. 6 [45] Mark Weber, Lijun Yu, Qihang Yu, Xueqing Deng, Xiaohui Shen, Daniel Cremers, and Liang-Chieh Chen. Maskbit: arXiv Embedding-free image generation via bit tokens. preprint arXiv:2409.16211, 2024. 2 [46] Jinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang, Weihao Wang, Kevin Qinghong Lin, Yuchao Gu, Zhijie Chen, Zhenheng Yang, and Mike Zheng Shou. Show-o: One single transformer to unify multimodal understanding and generation. arXiv preprint arXiv:2408.12528, 2024."
        },
        {
            "title": "Supplementary Material",
            "content": "In this supplementary material, we provide additional details as follows: 1. Sec. provides the implementation details of controllable image generation using our method. 2. Sec. provides the implementation details of zero-shot inference with our method. 3. Sec. provides the pseudo-code of our core method. 4. Sec. provides more generated image samples. A. Details of Controllable Image Generation As discussed in the methodology section, we use [MASK] tokens embedded with positional information to guide the model in predicting tokens at arbitrary specified locations. For controllable image generation, the overall framework is similar to class-conditional image generation, as it still requires explicit guidance. However, instead of class labels, we use representations extracted from conditions (e.g., canny edges or depth maps) for guidance. As shown in Fig. A, the source image serves as key-value pairs, with the last token discarded and class token prepended. The representation of the conditions is used as the query, right-shifted to align with the prediction target. Using the method proposed in this paper, fully randomized teacher-forcing training and parallel decoding can be implemented for efficient and effective controllable image generation. B. Details of Zero-Shot Inference Since our method generates image tokens in randomized order, it naturally supports zero-shot inference. Given its causal attention-based architecture, zero-shot inference can be achieved through two-stage Prefilling + Decoding process. Taking inpainting as an example (as shown in Fig. B), for regions that do not require repainting, we first pass their tokens into the causal self-attention decoder for prefilling, generating content representations as keyvalue pairs. Then, we replace all tokens in the regions to be repainted with [MASK] tokens and use the previously obtained key-value representations for non-causal crossattention to complete the parallel decoding. The same process applies to outpainting and resolution expansion. For the resolution expansion task, We fill [MASK] tokens in the neighborhood of the original image, as shown in Fig. C. C. Pseudo-Code for Our Method We provide the pseudo-code for the core functions of random-order training and parallel decoding in our method, as shown in Algorithm 1 and Algorithm 2. Figure A. Implementation Details of Controllable Image Generation. For clarity, we illustrate the process using raster-order as an example. The figure only illustrates the interaction between query and key in the attention mechanism and its output, omitting the value for simplicity. [CLS]: Class token. Figure B. Implementation Details of Zero-Shot Inference. For clarity, we illustrate the process using inpainting as an example. The figure only illustrates the interaction between query and key in the attention mechanism and its output, omitting the value. Figure C. Implementation Details of Resolution Expansion. We fill [MASK] tokens in the neighborhood of the original image and perform the operation as shown in Fig. B. D. More Generation Samples We present more generation examples, as shown in Fig. E, Fig. F, Fig. G, and Fig. H. Algorithm 1 Pytorch Pseudo-Code for ARPG Training # Shuffle the sequences in batch def batch_seq_shuffle(x, orders=None): bs, seq_len = x.shape[:2] if orders is None: orders = torch.rand(bs, seq_len, device=x.device).argsort(dim=1) orders_expand = orders.view(*orders.shape, *(1,) * (x.ndim - orders.ndim)) shuffled_data = torch.gather(x, 1, orders_expand.expand(*x.shape)) return shuffled_data, orders # Forward function of ARPG model def forward(input_ids, condition, targets): # Dropout for classifier-free guidance condition = preprocess_condition(condition, cond_drop_prob=0.1) # Shuffle each sequence independently shuffled_ids, orders = batch_seq_shuffle(input_ids) # The \"freqs_cis\" of RoPE needs to have an additional batch dimension and be broadcast. freqs_cis = freqs_cis.unsqueeze(0).repeat(input_ids.shape[0], 1, 1, 1) fixed_freqs_cis = freqs_cis[:, :1, ...] shuffled_freqs_cis = batch_seq_shuffle(freqs_cis[:, 1:, ...], orders)[0] freqs_cis = torch.cat([fixed_freqs_cis, shuffled_freqs_cis], dim=1) # Remove the last token for teacher-forcing input logits = forward_shared(torch.cat([condition, shuffled_ids[:, :-1]], dim=-1), freqs_cis) targets = batch_seq_shuffle(targets, orders)[0] loss = cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1)) return logits, loss # function shared between training and inference. def forward_shared(input_ids, freqs_cis, num_query=None): embedds = embeddings(input_ids) = embed_drop(embedds) num_query = input_ids.shape[-1] if num_query == None else num_query queries = embeddings(torch.full((input_ids.shape[0], num_query), mask_token_id)) = transformer_layers(x, queries, freqs_cis) logits = self.head(self.norm(x)) return logits Algorithm 2 Pytorch Pseudo-Code for ARPG Parallel Decoding def generate(condition, num_iter, seq_len): orders = torch.rand(seq_len).argsort(dim=0) + 1 last_pos, last_len = 0, 1 last_range = range(last_pos, last_pos + last_len) # Enable KV cache sequences = [] for step in range(num_iter): num_pred = cosine_scheduler(step, num_iter) next_range = orders[range(last_pos + last_len - 1, last_pos + last_len + num_pred - 1)] last_pos = last_pos + last_len # we omit the code related to classifier free guidance. freqs_cis = torch.cat([ freqs_cis_[:, last_range, ...], freqs_cis_[:, next_range, ...]], dim= ) input_ids = condition if step == 0 else sequences[-1] logits = forward_shared(input_ids, freqs_cis, num_pred) sequences.append(sample(logits)) last_len, last_range = num_pred, next_range sequences = torch.cat(sequences, dim=-1) return sequences[:, orders.argsort(dim=0)] 2 Figure D. Uncurated Results on ImageNet 256256 Class-Conditional Image Generation. 3 Figure E. Samples of Controllable Image Generation. Figure G. Samples of Image Inpainting. Figure F. Samples of Image Outpainting. Figure H. Samples of Resolution Expansion."
        }
    ],
    "affiliations": [
        "Institute of Automation, Chinese Academy of Sciences",
        "Westlake University"
    ]
}