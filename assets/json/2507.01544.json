{
    "paper_title": "MARVIS: Modality Adaptive Reasoning over VISualizations",
    "authors": [
        "Benjamin Feuer",
        "Lennart Purucker",
        "Oussama Elachqar",
        "Chinmay Hegde"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Scientific applications of machine learning often rely on small, specialized models tuned to particular domains. Such models often achieve excellent performance, but lack flexibility. Foundation models offer versatility, but typically underperform specialized approaches, especially on non-traditional modalities and long-tail domains. We propose MARVIS (Modality Adaptive Reasoning over VISualizations), a training-free method that enables even small vision-language models to predict any data modality with high accuracy. MARVIS transforms latent embedding spaces into visual representations and then leverages the spatial and fine-grained reasoning skills of VLMs to successfully interpret and utilize them. MARVIS achieves competitive performance on vision, audio, biological, and tabular domains using a single 3B parameter model, achieving results that beat Gemini by 16\\% on average and approach specialized methods, without exposing personally identifiable information (P.I.I.) or requiring any domain-specific training. We open source our code and datasets at https://github.com/penfever/marvis"
        },
        {
            "title": "Start",
            "content": "Preprint. Under review. MARVIS: Modality Adaptive Reasoning over VISualizations Benjamin Feuer1,2, Lennart Purucker3, Oussama Elachqar2, Chinmay Hegde1 1 NYU, 2 Oumi.AI, 3 University of Freiburg"
        },
        {
            "title": "Abstract",
            "content": "Scientific applications of machine learning often rely on small, specialized models tuned to particular domains. Such models often achieve excellent performance, but lack flexibility. Foundation models offer versatility, but typically underperform specialized approaches, especially on nontraditional modalities and long-tail domains. We propose MARVIS (Modality Adaptive Reasoning over VISualizations), training-free method that enables even small vision-language models to predict any data modality with high accuracy. MARVIS transforms latent embedding spaces into visual representations and then leverages the spatial and fine-grained reasoning skills of VLMs to successfully interpret and utilize them. MARVIS achieves competitive performance on vision, audio, biological, and tabular domains using single 3B parameter model, achieving results that beat Gemini by 16% on average and approach specialized methods, without exposing personally identifiable information (P.I.I.) or requiring any domain-specific training. We open source our code and datasets at https://github.com/penfever/marvis."
        },
        {
            "title": "1 Introduction",
            "content": "Historically, downstream applications of machine learning have relied on small, specialized models tuned for particular tasks and domains (Prokhorenkova et al., 2018; He et al., 2015). Such models often achieve excellent performance in their domain, but are, by their construction, inflexible and inapplicable to other domains. When small models produce intermediate embeddings (dense vectors representing intermediate processing stages), those embeddings can be extracted, and in some cases used for range of downstream tasks after fine-tuning (Devlin et al., 2019). Foundation models (FMs) introduced an exciting new paradigm; in-context learning (ICL), the ability to adapt to new tasks without weight updates (Brown et al., 2020). But even the best contemporary FMs underperform specialized approaches, especially on non-traditional modalities and long-tail domains (Zhang et al., 2024). For some modalities, such as audio, there is no obvious way to natively utilize traditional FM at all. In this work, we posit that visual reasoning, coupled with specialized low-dimensional embedding models, is skeleton key that unlocks FMs and ICL for any modality of data, including data that is scarce, contains PII, or is expensive to acquire. Key Insight Key Contributions: MARVIS We propose MARVIS, an efficient, modality-agnostic system for transforming any vision-enabled FM into performant predictor. Without access to P.I.I. or direct data leakage, MARVIS-3B achieves competitive performance across vision, audio, and tabular modalities, and across wide range of scientific domains, on both classification and regression tasks. We demonstrate empirically that MARVIS does more than simply copy predictions; it reasons over the available information sources, implicitly reweighting them to improve its own predictive power. Because of this, and because of its native generative reasoning ability, MARVIS predictions are decoupled from, and often more useful than, those of the small embedding-generating models it utilizes. We conduct extensive empirical ablations on the design of the information environment; we consider this vital area for future research. 1 5 2 0 2 J 2 ] . [ 1 4 4 5 1 0 . 7 0 5 2 : r Preprint. Under review. Figure 1: MARVIS transforms VLMs into strong predictors. MARVIS-3B achieves competitive performance with specialized baselines across modalities and domains, for regression, binary and MC classification, using ICL alone. MARVIS demonstrates the effectiveness of visual reasoning for diverse predictive tasks."
        },
        {
            "title": "2 Problem Setting & Motivation",
            "content": "In this section, we lay out in detail the challenges of using both FMs and specialist models in isolation. As case study, we discuss the tabular modality, which has proven historically challenging for deep learning (McElfresh et al., 2024). 2.1 Challenges of using FMs with tabular data Perhaps because the first FMs only used language, most subsequent works which make use of FMs have implicitly treated text as first-class modality (Brown et al., 2020). line of work (Hegselmann et al., 2023b; Gardner et al., 2024; Shysheya et al., 2025) has used various permutations of serialization (A.K.A. prompt engineering), fine-tuning and answer extraction (e.g., logprobs vs regex) to improve the few-shot performance of FMs on tabular datasets rendered as text strings. The experiments in those papers, however, were small in scale compared to both common benchmarks in tabular deep learning (Bischl et al., 2021; McElfresh et al., 2024) and the reality of industry and Kaggle (Erickson et al., 2025). The vast majority of scientific tabular data is numeric, with predictive features lacking meaningful semantic corollaries. Sparse and temporal modalities, such as genomic and audio data, lack meaningful text translations for serialization. As of this writing, no large-scale comparison has been conducted between FM-centered methods and the strongest algorithms in tabular DL and AutoML such as Hollmann et al. (2025); Prokhorenkova et al. (2018); we consider this major gap in the existing research literature. Aside from questions of utility, serializing tabular data presents challenges, including data privacy (P.I.I. sharing with API endpoints, companies retaining and training on chat data), data leakage, which can contaminate test 2 Preprint. Under review. results, and, most significantly, inefficient scaling with context length (van Breugel and van der Schaar, 2024; Ruan et al., 2024). Approaches like LLaVA (Liu et al., 2023a) attempt to learn alignments through projection layers, but this is challenging and requires different translation for each modality, with some modalities, including tabular data, proving resistant (so far) to such efforts. Byte-level approaches such as (Yu et al., 2023) are promising, but inefficient for long context. Other works utilize images of tables, typically for table question answering (Lu et al., 2022). However, this approach does not scale to large tables. 2.2 Challenges of Specialist Models Specialized embedding models simplify the input space in way that is generally helpful for reliably answering certain types of questions. In some cases, their embeddings can be used for prediction without any fine-tuned classification stage via classical nonparametric methods like KNN (Oquab et al., 2023). But, by design, they cannot easily incorporate complex text-based instructions, counterfactuals, multimodal inputs, or reasoning capabilities without retraining. 2.3 Challenges of Multimodal FMs Most closely related to MARVIS are multimodal FMs such as LLaVA (Liu et al., 2023a), which seek to optimally align language models with specialist embeddings for vision, and in some cases, other modalities as well. The key advantage of MARVIS is that it enables any VLM to utilize any embedding space in-context, without the complexity and cost of fine-tuning. Instead, we rely on VLMs inherent world knowledge to interpret the data. Technical Innovation Our Research Question: How can we combine the reasoning capabilities of FMs with the representational power of specialists without requiring modality-specific fine-tuning or exposing P.I.I.?"
        },
        {
            "title": "3 MARVIS",
            "content": "We present an overview of MARVIS in Figure 2, and describe the pipeline in detail below. Figure 2: The four-stage MARVIS pipeline starts with raw input data, captures key patterns using specialist embedding generating models, determines an appropriate strategy for plotting the data, and prompts VLM with visual context, as well as (optionally) metadata and semantic context, then extracts predictions. Core Insight: Vision is Skeleton Key. For predictive tasks, it is not usually the raw data that we want the model to reason over; rather, it is distilled view of that data, for the purposes of answering specific questions or rendering judgments. Human scholars tend to reason more effectively with data visualizations, simplified views of complex data (Unwin, 2020). VLMs, which are pretrained on web-scraped data, can understand and interpret wide range of scientific imagery, and visualizations of specialized embedding spaces, unlike raw data, are easy to acquire programmatically at inference time. Embedding visualizations are skeleton keys, enabling us to reason about any kind of data with vision-language models without modality-specific training beyond vision. 3.1 Technical Implementation MARVIS operates through the following pipeline: Preprint. Under review. 1. Embedding Generation: Use domain-specific embedding models to create vector representations. 2. Dimensionality Reduction: Apply t-SNE to create 2D visualizations optimized for VLM processing. 3. Visual Reasoning: Query the VLM with the visualization and query point for prediction. 4. Response Processing: Extract the prediction from VLMs reasoning. Although the principles of MARVIS are extremely simple, in order for it to work in practice, significant technical hurdles must be cleared. Challenges: architecture. The first is choosing an appropriate VLM architecture; many older architectures either cannot localize what they see effectively, or cannot see clearly enough to take advantage of visualizations. After some trial and error, we choose the 3B parameter Qwen 2.5 VL model from Alibaba (Bai et al., 2025). This model has several key advantages for our purposes; firstly, it uses 1414 patches with sliding window attention in some layers, emphasizing local patch interaction. This is important for distance-based visualizations, where proximity matters. Second, it allows images of arbitrary aspect ratios to be processed effectively, without distorting distances during ingestion. This allows us to effectively compose and read multi-visualization layouts with MARVIS. Third, the Qwen 2.5 VL series has been specifically trained to work with long context and scientific imagery. Challenges: resolution. Even Qwen 2.5 VL does not see as well as humans; the particular patch dimensions and the limited range of its local attention mean that Qwen performs best when visualizations are zoomed in to the region of interest. We find that the amount of zooming required varies substantially depending on the benchmark, but can usually be set once for each benchmark; this avoids costly hyperparameter search, although this value could conceivably be optimized further in the future. Ideally, the scaling factor is such that the target point and its neighbors are captured within the sliding window, significantly enhancing spatial understanding. Challenges: context composition strategy. One key design decision in MARVIS is which context to include, and how much of it. In C.2, we name and ablate over 25 different configurations. Ultimately, for our main experiments in this paper, we exclusively use the tsne knn setting, as we find it offers the best speed / quality tradeoff. Because KNN operates on the embeddings without dimensionality reduction, it is sometimes able to discover relationships that visualizations miss; however, we consider this an important area for future research, as we believe we have only begun to document the possibilities here. We find that fixing the nearest neighbors hyperparameter at min(30, 10% of the training data) works well for wide range of dataset sizes and modalities. Challenges: classname extraction. In order to avoid the common failure mode with FMs in which answers are correct but not detected by the parser, we introduce consistent color schemes and consistent naming across the legends for all visualizations, ensuring clear visual separation for VLM interpretation. The parser is made aware of both the class names and the color names, and is given mapping between them. Classnames in legends are limited to the classes which actually appear in that visualization, in order to control the size of the legend for large datasets."
        },
        {
            "title": "4 Experiments",
            "content": "We evaluate MARVIS across four distinct modalities using domain-appropriate embedding models and established benchmarks, comparing against both specialized baselines and alternative foundation model approaches. For more analysis on the embedding models, please refer to B.1. For more explanation of the benchmarks we use, please refer to A. Table 1 presents MARVIS performance across all modalities compared to specialized baselines and alternative foundation model approaches. All MARVIS results in Table 1 are reported using QwenVL 2.5 3B backbone. The FM results are reported either using the same backbone, or using Gemini-Flash-2.0 via the Gemini API. In the case of tabular data, we evaluate both the JOLT and TabLLM strategies, and report the best result in the table (Shysheya et al., 2025; Hegselmann et al., 2023a). Among specialist models, we report only the best overall result in the table. For extended results, please refer to Table 10. For deeper dive into tabular data, including balanced metrics, please refer to D. For detailed description on the method we use to generate our novel tabular benchmarks CC18-Semantic and Regression2025-Semantic, refer to D.6. Specialist baselines. For vision, the best performing specialist was the large DinoV2 model with registry and KNN classification (Oquab et al., 2023). For audio, the CLAP model with contrastive zero-shot classification from Microsoft and OpenAIs Whisper-V2-Large model with KNN classification perform the best (Radford et al., 2022; Elizalde et al., 2023; Ma et al., 2024a). For biological data, BioCLIPv2 with KNN classification performs the best (Gu et al., 2025). For tabular data, TabPFNv2 with standard forward pass classification and regression is strong 4 Preprint. Under review. Domain Embeddings Benchmark Size (K) MARVIS Spc. FM 95% CI Vision DINOV2 Audio CLAP Biological BioCLIP2 Tabular TabPFNv2 (Score, # Models) C10 C100 ESC RAV US8 FSH AWA PLD CC18 60 60 2 1.4 8.7 94 37 2.5 155 35 - 98.0 88. 91.3 38.4 79.8 80.2 95.7 67.4 84.5 66.0 99.0 (DINOV2) 91.6 (DINOV2) 90.5 (CLAP) 47.9 (Whisper) 77.1 (CLAP) 83.7 (BioCLIP) 97.1 (BioCLIP) 72.0 (BioCLIP) 85.7 (Gemini) 64.3 (Gemini) - - - 59.5 (Gemini) 96.5 (Gemini) 74.2 (Gemini) 87.8 (TabPFNv2) 67.0 (TabPFNv2) 50.1 (TabLLM-Gemini) 05.1 (JOLT-Qwen-2.5-3B) (78.9, 1) (81.4, 5) (62.2, 4) 0.1 0.3 1.2 2.5 0.8 0.3 0.2 1.8 0.2 0. - Table 1: Domain-specific embeddings, benchmarks, and detailed results. Results are boldfaced when statistically tied for best performance within 95% confidence intervals (normal approximation). MARVIS demonstrates competitive or superior performance on most individual benchmarks, achieving average results within 2.5% of an ensemble of specialized methods while providing universal applicability. Benchmark acronyms: C10 = CIFAR-10, C100 = CIFAR-100, ESC = ESC-50, RAV = RAVDESS, US8 = UrbanSound8K, FSH = FishNet, AWA = AWA2, PLD = PlantDoc, CC18 = OpenML CC18, R25 = Regression 2025. The Spc. field refers to the best specialized model result, the FM field to the best result relying on foundation model. For all benchmarks except R25, the metric is Accuracy. For R25, it is R2 Score (with minimum score of 0). The number reported is the mean over all sub-tasks for multi-task benchmarks. baseline; we also consider classical baselines such as CatBoost and linear models in our appendix (Prokhorenkova et al., 2018; Hollmann et al., 2025). FM baselines. For vision, we use the standard strategy of zero-shot prompting and exact match extraction described in works such as (Zhang et al., 2024). For audio, we are unable to compare to public FMs, as to the best of our knowledge, no generalist method exists for adapting FMs for audio classification. FM tabular baselines. In the tabular domain, as secondary contribution, we generate the first large-scale standardized benchmarks for tabular classification and regression that include semantic class names, feature names and metadata; CC18-Semantic and Regression 2025 Semantic. We also re-implement two prominent LLMtabular methods, TabLLM and JOLT (Hegselmann et al., 2023a; Shysheya et al., 2025), which lack general-purpose implementations. For more details on this, please refer to D. Key Insight MARVIS Cross-Modal Success: MARVIS-3B achieves competitive performance across four distinct modalities; on average, MARVIS-3B is within 2.5% of the best performing specialist model for each domain, and it improves on the best FM performance by 16.7%, on average. 4.1 Evidence of VLM Adaptive Reasoning Our analysis reveals compelling evidence that VLMs genuinely reason over their input data and condition their behavior based on the context provided, rather than relying solely on learned patterns or simple heuristics. 4.1.1 Performance-Driven Reasoning Patterns Systematic analysis of VLM reasoning in 3 demonstrates clear correlations between reasoning quality and metric gains, on average, across three tabular classification datasets (two with meaningful semantic features, one without). Analysis of disagreement patterns reveals that only 35% of methods agree on all test cases, with 65% showing partial disagreement. Correct predictions exhibit distinct characteristics compared to incorrect ones: Enhanced response sophistication: +12.9 characters longer responses (+4.8% average length) Increased spatial analysis: +33% more color mentions, +18% more distance reasoning 5 Preprint. Under review. Figure 3: Mean Accuracy by Configuration. Comparison of different visualization strategies showing that perturbation-based approaches with uncertainty analysis achieve the highest performance, followed by semantic axes with meaningful class labels. Reduced heuristic reliance: -21% less usage of closest heuristics, -20% less cluster-based reasoning These patterns suggest that VLMs engage in more thorough spatial analysis when the visual information supports accurate classification, indicating genuine reasoning rather than pattern matching. 4.1.2 Method-Specific Reasoning Signatures Different visualization methods elicit systematically different reasoning approaches, providing strong evidence that VLMs adapt their analysis based on the available visual information: tsne knn: Produces quantitative neighbor analysis with explicit distance calculations (average 48.0 words) tsne semantic axes: Integrates semantic class information with spatial reasoning (304.9 character responses) tsne perturbation axes: Generates the longest, most detailed responses (310.6 characters) with sophisticated uncertainty analysis The systematic variation in reasoning style directly correlates with the information content of each visualization method, demonstrating that VLMs genuinely process and respond to different types of visual information. Detailed analysis of these reasoning patterns and their implications for VLM spatial understanding is provided in Appendix E. 6 Preprint. Under review."
        },
        {
            "title": "5 Related Work",
            "content": "MARVIS builds on extensive prior work in vision-language models (VLMs) which has followed two primary evolutionary tracks: maximalist approaches from industry labs focusing on peak performance, and minimalist open-source approaches prioritizing efficiency and accessibility. Early VLM architectures explored complex fusion mechanisms to achieve deep integration between vision and language. Flamingo (Alayrac et al., 2022) introduced gated cross-attention layers interleaved within frozen LLMs, enabling few-shot learning across diverse multimodal tasks without task-specific fine-tuning. BLIP (Li et al., 2022) and its successor BLIP-2 (Li et al., 2023b) pioneered the Multimodal Mixture of Encoder-Decoder (MED) architecture and introduced the Q-Former as lightweight bridge between frozen vision encoders and language models. PaLI (Chen et al., 2022) established the principle of joint scaling, demonstrating that optimal VLM performance requires balanced scaling of all components: vision models, language models, and training data. LLaVA (Liu et al., 2023a) democratized VLM research by establishing an efficient, open-source blueprint. Its three-component architecturefrozen vision encoder, lightweight MLP projector, and frozen LLMwith twostage training (feature alignment followed by instruction tuning) proved that simple architectures could achieve impressive multimodal capabilities. LLaVA-NeXT (Liu et al., 2024) introduced dynamic high resolution through intelligent image partitioning, while mPLUG-Owl2 (Ye et al., 2023) developed Modality-Adaptive Modules to foster positive cross-modal collaboration while mitigating interference. POINTS (Ma et al., 2024b) exemplified sophisticated data curation through perplexity-based filtering. Recent work has pushed beyond conversational capabilities toward precise, spatially-grounded understanding, key to understanding the gains in MARVIS. Grounding DINO (Liu et al., 2023b) achieved open-set object detection through text-conditioned spatial understanding, while KOSMOS-2 (Peng et al., 2023) integrated coordinate tokens directly into the LLM vocabulary for grounded text generation. OtterHD (Li et al., 2023a) pioneered an encoder-less architecture, processing raw pixel patches directly in the LLM to eliminate resolution constraints. SleighVL (Liu et al., 2025) refined high-resolution processing through attention-based sub-image weighting via Global Semanticguided Weight Allocation. Emu3 (Wang et al., 2024) unifies vision and language modalities under next-token prediction, tokenizing images, videos, and text into shared vocabulary space. Molmo (Deitke et al., 2024) champions fully open ecosystems with human-annotated data, breaking dependence on proprietary synthetic datasets. Early cross-modal strategies used feature concatenation, attention mechanisms, or late fusion strategies, requiring extensive retraining for each new modality (Baltrusaitis et al., 2018). Modern paradigms include contrastive learning (CLIP-style) (Radford et al., 2021), generative modeling (Ramesh et al., 2022), and instruction tuning (Wei et al., 2022). However, these approaches typically require substantial computational resources and domain-specific training data for each new modality. The use of embedding spaces for cross-modal understanding has roots in representation learning (Bengio et al., 2013) and dimensionality reduction techniques (Van der Maaten and Hinton, 2008). Recent work has explored the geometric properties of embedding spaces (Ethayarajh, 2019) and their visualization for interpretability (Liu et al., 2017). t-SNE and UMAP have been widely used for visualizing high-dimensional data (McInnes et al., 2018), but their application to VLM reasoning represents novel paradigm. Previous work on visual reasoning has focused on spatial relationships in natural images (Johnson et al., 2017), but MARVIS extends this to abstract embedding spaces across arbitrary modalities. MARVIS distinguishes itself from existing approaches through several key innovations: (1) Training-free adaptation: Unlike approaches requiring extensive fine-tuning, MARVIS leverages pre-trained components without modification; (2) Universal modality support: single architecture handles any data type through embedding visualization; (3) Privacy preservation: Visualization of embeddings avoids raw data exposure; (4) Computational efficiency: Achieves competitive performance with 3B parameter model versus much larger specialized systems."
        },
        {
            "title": "6 Conclusion",
            "content": "We introduce MARVIS, training-free method that enables small VLMs to predict across any data modality through embedding visualization. By transforming embedding spaces into visual representations optimized for VLM spatial reasoning, MARVIS achieves competitive performance across diverse domains. MARVIS addresses key limitations in existing approaches: it requires no domain-specific training, preserves data privacy through visualization rather than serialization, and maintains competitive performance. The approach demonstrates that visual reasoning can serve as universal interface for foundation models across any data modality. 7 Preprint. Under review. Future work includes further investigation of the optimal mix of visualizations and embeddings to boost performance and fine-tuning strategies which may improve the performance of base VLMs for reasoning over scientific imagery, including reasoning post-training."
        },
        {
            "title": "Ethics Statement",
            "content": "MARVIS enhances privacy preservation in machine learning by avoiding raw data serialization, instead using anonymized embedding visualizations. This approach reduces risks of data exposure while maintaining model performance. The methods universal applicability could democratize access to advanced ML capabilities across diverse scientific domains."
        },
        {
            "title": "Acknowledgments",
            "content": "L.P. acknowledges funding by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under SFB 1597 (SmallData), grant number 499552394. B.F. gratefully acknowledges the support of the Community (2025) platform and team."
        },
        {
            "title": "References",
            "content": "Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: visual language model for few-shot learning. In Advances in Neural Information Processing Systems, volume 35, pages 2371623736, 2022. Sercan Arik and Tomas Pfister. Tabnet: Attentive interpretable tabular learning. Proceedings of the AAAI Conference on Artificial Intelligence, 35(8):66796687, 2021. Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin. Qwen2.5-vl technical report, 2025. URL https://arxiv.org/abs/2502.13923. Tadas Baltrusaitis, Chaitanya Ahuja, and Louis-Philippe Morency. Multimodal machine learning: survey and taxonomy. IEEE transactions on pattern analysis and machine intelligence, 41(2):423443, 2018. Yoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning: review and new perspectives, volume 35. IEEE transactions on pattern analysis and machine intelligence, 2013. Bernd Bischl, Giuseppe Casalicchio, Matthias Feurer, Pieter Gijsbers, Frank Hutter, Michel Lang, Rafael G. Mantovani, Jan N. van Rijn, and Joaquin Vanschoren. Openml benchmarking suites, 2021. URL https: //arxiv.org/abs/1708.03731. Leo Breiman. Random forests. Machine learning, 45(1):532, 2001. Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners, 2020. URL https://arxiv.org/abs/2005.14165. Tianqi Chen and Carlos Guestrin. Xgboost: scalable tree boosting system. In Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining, pages 785794, 2016. Xi Chen, Xiao Wang, Soravit Changpinyo, AJ Piergiovanni, Piotr Padlewski, Daniel Salz, Sebastian Goodman, Adam Grycner, Basil Mustafa, Lucas Beyer, et al. Pali: jointly-scaled multilingual language-image model. arXiv preprint arXiv:2209.06794, 2022. Oumi Community. Oumi: an open, end-to-end platform for building large foundation models, January 2025. URL https://github.com/oumi-ai/oumi. Matt Deitke, Christopher Clark, Sangho Lee, Rohun Tripathi, Yue Yang, Jae Sung Park, Mohammadreza Salehi, Niklas Muennighoff, Kyle Lo, Luca Soldaini, et al. Molmo and pixmo: Open weights and open data for state-of-the-art vision-language models. arXiv preprint arXiv:2409.17146, 2024. Preprint. Under review. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding, 2019. URL https://arxiv.org/abs/1810.04805. Benjamin Elizalde, Soham Deshmukh, Mahmoud Al Ismail, and Huaming Wang. Clap: Learning audio concepts from natural language supervision. arXiv preprint arXiv:2206.04769, 2023. Nick Erickson, Lennart Purucker, Andrej Tschalzev, David Holzm uller, Prateek Mutalik Desai, David Salinas, and Frank Hutter. Tabarena: living benchmark for machine learning on tabular data, 2025. URL https: //arxiv.org/abs/2506.16791. Kawin Ethayarajh. How contextual are contextualized word representations? Comparing the geometry of BERT, ELMo, and GPT-2 embeddings. In Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan, editors, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 5565, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1006. URL https://aclanthology.org/D19-1006/. Benjamin Feuer, Robin Tibor Schirrmeister, Valeriia Cherepanova, Chinmay Hegde, Frank Hutter, Micah Goldblum, Niv Cohen, and Colin White. Tunetables: Context optimization for scalable prior-data fitted networks, 2024. URL https://arxiv.org/abs/2402.11137. Valentin Gabeff, Marc Rußwurm, Devis Tuia, and Alexander Mathis. Wildclip: Scene and animal attribute retrieval from camera trap data with domain-adapted vision-language models. IJCV, 132(9):37703786, 2024. Josh Gardner, Juan C. Perdomo, and Ludwig Schmidt. Large scale transfer learning for tabular data via language modeling, 2024. URL https://arxiv.org/abs/2406.12031. Jianyang Gu, Samuel Stevens, Elizabeth Campolongo, Matthew Thompson, Net Zhang, Jiaman Wu, Andrei Kopanev, Zheda Mai, Alexander E. White, James Balhoff, Wasila Dahdul, Daniel Rubenstein, Hilmar Lapp, Tanya Berger-Wolf, Wei-Lun Chao, and Yu Su. Bioclip 2: Emergent properties from scaling hierarchical contrastive learning, 2025. URL https://arxiv.org/abs/2505.23883. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition, 2015. URL https://arxiv.org/abs/1512.03385. Stefan Hegselmann, Alejandro Buendia, Hunter Lang, Monica Agrawal, Xiaoyi Jiang, and David Sontag. Tabllm: Few-shot classification of tabular data with large language models. arXiv preprint arXiv:2210.10723, 2023a. Stefan Hegselmann, Alejandro Buendia, Hunter Lang, Monica Agrawal, Xiaoyi Jiang, and David Sontag. Tabllm: Few-shot classification of tabular data with large language models, 2023b. URL https://arxiv.org/abs/2210. 10723. Noah Hollmann, Samuel uller, Katharina Eggensperger, and Frank Hutter. Tabpfn: transformer that solves small tabular classification problems in second. arXiv preprint arXiv:2207.01848, 2022. Noah Hollmann, Samuel uller, Lennart Purucker, Arjun Krishnakumar, Max orfer, Shi Bin Hoo, Robin Tibor Schirrmeister, and Frank Hutter. Accurate predictions on small data with tabular foundation model. Nature, 637(8045):319326, January 2025. ISSN 1476-4687. doi: 10.1038/s41586-024-08328-6. URL https://doi.org/10. 1038/s41586-024-08328-6. Xin Huang, Ashish Khetan, Milan Cvitkovic, and Zohar Karnin. Tabtransformer: Tabular data modeling using contextual embeddings, 2020. URL https://arxiv.org/abs/2012.06678. Justin Johnson, Bharath Hariharan, Laurens Van Der Maaten, Li Fei-Fei, Lawrence Zitnick, and Ross Girshick. Clevr: diagnostic dataset for compositional language and elementary visual reasoning. Proceedings of the IEEE conference on computer vision and pattern recognition, pages 29012910, 2017. Faizan Farooq Khan, Xiang Li, Andrew J. Temple, and Mohamed Elhoseiny. FishNet: Large-scale Dataset and Benchmark for Fish Recognition, Detection, and Functional Trait Prediction. In 2023 IEEE/CVF International Conference on Computer Vision (ICCV), pages 2043920449, Paris, France, October 2023. IEEE. ISBN 9798350307184. doi: 10.1109/ICCV51070.2023.01874. URL https://ieeexplore.ieee.org/document/10377207/. Alex Krizhevsky. Learning multiple layers of features from tiny images, 2009. URL https://api.semanticscholar. org/CorpusID:18268744. Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Jingkang Yang, and Ziwei Liu. Otterhd: high-resolution multi-modality model. arXiv preprint arXiv:2311.04219, 2023a. Preprint. Under review. Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. International Conference on Machine Learning, pages 1288812900, 2022. Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. arXiv preprint arXiv:2301.12597, 2023b. Fangxin Liu, Wenjie Zhang, Libo Chen, Jincan Wang, Mingshan Luo, and Yuliang Chen. Global semantic-guided subimage feature weight allocation in high-resolution large vision-language models. arXiv preprint arXiv:2501.14276, 2025. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in Neural Information Processing Systems, 36, 2023a. Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Improved reasoning, ocr, and world knowledge. arXiv preprint arXiv:2401.13601, 2024. Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun Zhu, et al. Grounding dino: Marrying dino with grounded pre-training for open-set object detection. arXiv preprint arXiv:2303.05499, 2023b. Shixia Liu, Xiting Wang, Mengchen Liu, and Jun Zhu. Towards better analysis of machine learning models: visual analytics perspective. Visual Informatics, 1(1):4856, 2017. Steven R. Livingstone and Frank A. Russo. The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS): dynamic, multimodal set of facial and vocal expressions in North American English. PLOS ONE, 13 (5):135, May 2018. doi: 10.1371/journal.pone.0196391. URL https://doi.org/10.1371/journal.pone.0196391. Publisher: Public Library of Science. Pan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering. In The 36th Conference on Neural Information Processing Systems (NeurIPS), 2022. Rao Ma, Adian Liusie, Mark Gales, and Kate Knill. Investigating the emergent audio classification ability of ASR foundation models. In Kevin Duh, Helena Gomez, and Steven Bethard, editors, Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 47464760, Mexico City, Mexico, June 2024a. Association for Computational Linguistics. doi: 10.18653/v1/2024.naacl-long.266. URL https://aclanthology.org/2024.naacl-long.266/. Yuan Ma, Tianyu Li, Dongdong Chen, Zhenglu Wu, Xuguang Li, Lu Chen, Kai Zhang, Zilong Wang, Chunyang Liu, Kexin Wang, et al. Points: Improving your vision-language model with affordable strategies. arXiv preprint arXiv:2409.04828, 2024b. Duncan McElfresh, Sujay Khandagale, Jonathan Valverde, Vishak Prasad C, Benjamin Feuer, Chinmay Hegde, Ganesh Ramakrishnan, Micah Goldblum, and Colin White. When do neural nets outperform boosted trees on tabular data?, 2024. URL https://arxiv.org/abs/2305.02997. Leland McInnes, John Healy, and James Melville. Umap: Uniform manifold approximation and projection for dimension reduction. arXiv preprint arXiv:1802.03426, 2018. Andreas uller, Carlo Curino, and Raghu Ramakrishnan. Mothernet: Fast training and inference via hyper-network transformers, 2025. URL https://arxiv.org/abs/2312.08598. Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, and Furu Wei. Kosmos-2: Grounding multimodal large language models to the world. arXiv preprint arXiv:2306.14824, 2023. Karol J. Piczak. ESC: Dataset for Environmental Sound Classification. In Proceedings of the 23rd Annual ACM Conference on Multimedia, pages 10151018. ACM Press, 2015. ISBN 978-1-4503-3459-4. doi: 10.1145/2733373. 2806390. URL http://dl.acm.org/citation.cfm?doid=2733373.2806390. Liudmila Prokhorenkova, Gleb Gusev, Aleksandr Vorobev, Anna Veronika Dorogush, and Andrey Gulin. Catboost: unbiased boosting with categorical features. Advances in neural information processing systems, 31, 2018. 10 Preprint. Under review. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. International conference on machine learning, pages 87488763, 2021. Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever. Robust speech recognition via large-scale weak supervision, 2022. URL https://arxiv.org/abs/2212.04356. Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022. Yucheng Ruan, Xiang Lan, Jingying Ma, Yizhi Dong, Kai He, and Mengling Feng. Language modeling on tabular data: survey of foundations, techniques and evolution, 2024. URL https://arxiv.org/abs/2408.10548. Julian Santamaria, Claudia Isaza, and Jhony Giraldo. Catalog: camera trap language-guided contrastive learning model. In WACV, pages 11971206. IEEE, 2025. Srikumar Sastry, Subash Khanal, Aayush Dhakal, Adeel Ahmad, and Nathan Jacobs. Taxabind: unified embedding space for ecological applications. In WACV, pages 17651774. IEEE, 2025. Aliaksandra Shysheya, John Bronskill, James Requeima, Shoaib Ahmed Siddiqui, Javier Gonzalez, David Duvenaud, Jolt: Joint probabilistic predictions on tabular data using llms, 2025. URL https: and Richard E. Turner. //arxiv.org/abs/2502.11877. Davinder Singh, Naman Jain, Pranjali Jain, Pratik Kayal, Sudhakar Kumawat, and Nipun Batra. Plantdoc: dataset for visual plant disease detection. In Proceedings of the 7th ACM IKDD CoDS and 25th COMAD, CoDS COMAD 2020, page 249253, New York, NY, USA, 2020. Association for Computing Machinery. ISBN 9781450377386. doi: 10.1145/3371158.3371196. URL https://doi.org/10.1145/3371158.3371196. Samuel Stevens, Jiaman Wu, Matthew Thompson, Elizabeth Campolongo, Chan Hee Song, David Edward Carlyn, Li Dong, Wasila Dahdul, Charles Stewart, Tanya Berger-Wolf, Wei-Lun Chao, and Yu Su. Bioclip: vision foundation model for the tree of life, 2024. URL https://arxiv.org/abs/2311.18803. Antony Unwin. Why Is Data Visualization Important? What Is Important in Data Visualization? Harvard Data Science Review, 2(1), jan 31 2020. https://hdsr.mitpress.mit.edu/pub/zok97i7p. Boris van Breugel and Mihaela van der Schaar. Why tabular foundation models should be research priority, 2024. URL https://arxiv.org/abs/2405.01147. Laurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine learning research, 9 (11), 2008. Xinlong Wang, Xiaosong Zhang, Zhengxiong Luo, Quan Sun, Yufeng Cui, Jingjing Wang, Zhuang Lei, Dongmei Jiang, Renrui Ren, Junlin Yan, et al. Emu3: Next-token prediction is all you need. arXiv preprint arXiv:2409.18869, 2024. Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew Dai, and Quoc Le. Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652, 2022. Yongqin Xian, Christoph H. Lampert, Bernt Schiele, and Zeynep Akata. Zero-shot learninga comprehensive evaluation of the good, the bad and the ugly. IEEE Transactions on Pattern Analysis and Machine Intelligence, 41(9): 22512265, 2019. doi: 10.1109/TPAMI.2018.2857768. Chih-Hsuan Yang, Benjamin Feuer, Talukder Jubery, Zi Deng, Andre Nakkab, Md Zahid Hasan, Shivani Chiranjeevi, Kelly Marshall, Nirmal Baishnab, Asheesh Singh, et al. Biotrove: large curated image dataset enabling ai for biodiversity. In NeurIPS, volume 37, pages 102101102120, 2024. Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, et al. mplug-owl2: Revolutionizing multi-modal large language model with modality collaboration. arXiv preprint arXiv:2311.04257, 2023. Lili Yu, Daniel Simig, Colin Flaherty, Armen Aghajanyan, Luke Zettlemoyer, and Mike Lewis. Megabyte: Predicting million-byte sequences with multiscale transformers, 2023. URL https://arxiv.org/abs/2305.07185. Yuhui Zhang, Alyssa Unell, Xiaohan Wang, Dhruba Ghosh, Yuchang Su, Ludwig Schmidt, and Serena Yeung-Levy. Why are visually-grounded language models bad at image classification?, 2024. URL https://arxiv.org/abs/ 2405.18415. Preprint. Under review. 12 Preprint. Under review."
        },
        {
            "title": "Contents",
            "content": "1 Introduction 2 Problem Setting & Motivation 2.1 Challenges of using FMs with tabular data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.2 Challenges of Specialist Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.3 Challenges of Multimodal FMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 MARVIS 3.1 Technical Implementation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 Experiments 4.1 Evidence of VLM Adaptive Reasoning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.1.1 Performance-Driven Reasoning Patterns . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.1.2 Method-Specific Reasoning Signatures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 Related Work 6 Conclusion Benchmark Dataset Descriptions A.1 Vision Benchmarks . A.2 Audio Benchmarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.3 Biological/Scientific Vision Benchmarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.4 Tabular Benchmarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Implementation Details B.1 Embedding Models . B.2 Hyperparameters . . . . . . . . . ."
        },
        {
            "title": "C Extended Results",
            "content": "C.1 Computational Efficiency . C.2 Ablation Study Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C.2.1 Analysis of Configuration Effects . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Deep Dive: Tabular Modality Analysis D.1 Baselines: JOLT and TabLLM . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.2 Classification Performance on OpenML CC18 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.3 Regression Performance Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.4 Correlation Analysis with TabPFN v2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.5 Analysis and Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.6 CC18-Semantic and Regression2025-Semantic: Semantic Metadata Generation for Enhanced Dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Understanding . . . . . . . . . . . . . . . . . . . . . . . . . 1 2 2 3 3 3 4 5 5 6 7 15 15 15 15 15 15 16 17 17 17 18 18 20 21 22 23 13 Preprint. Under review. D.6.1 Motivation and Scope . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.6.2 Semantic Metadata Generation Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . D.6.3 Semantic Enrichment Structure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.6.4 Multi-Source Research Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.6.5 Quality Assurance and Validation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.6.6 Impact on Tabular Machine Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.6.7 Novel Contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.7 Comprehensive Dataset Characterization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.7.1 Domain Distribution Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.7.2 Representative Dataset Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.7.3 Dataset Complexity Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.7.4 Benchmark Coverage and Representativeness . . . . . . . . . . . . . . . . . . . . . . . . . VLM Reasoning Analysis E.1 Comprehensive Reasoning Pattern Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . E.1.1 Performance-Driven Features . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . E.1.2 Method-Specific Reasoning Signatures . . . . . . . . . . . . . . . . . . . . . . . . . . . . E.2 Adaptive Reasoning Evidence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . E.2.1 Disagreement Pattern Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . E.2.2 Concrete Examples of Adaptive Reasoning . . . . . . . . . . . . . . . . . . . . . . . . . . . E.3 Implications for VLM Understanding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . E.3.1 Evidence Against Pattern Matching . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . E.3.2 Spatial Reasoning Capabilities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . E.4 Design Implications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . MARVIS Extended Results MARVIS Method Variants: Detailed Ablation Documentation G.1 Method Variants Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 148 MARVIS Visualization Gallery H.0.1 CMC Dataset Visualizations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . H.0.2 Credit-G Dataset Visualizations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14 24 24 25 25 26 27 27 27 27 29 29 29 29 29 29 30 30 30 31 31 31 34 34 Preprint. Under review."
        },
        {
            "title": "A Benchmark Dataset Descriptions",
            "content": "A.1 Vision Benchmarks CIFAR-10: Contains 60,000 3232 color images in 10 classes (airplanes, cars, birds, cats, deer, dogs, frogs, horses, ships, trucks) with 6,000 images per class. Split into 50,000 training and 10,000 test images. One of the most widely used datasets for computer vision research Krizhevsky (2009). CIFAR-100: Similar to CIFAR-10 but with 100 classes containing 600 images each (500 training, 100 test per class). The 100 classes are grouped into 20 superclasses, making this more challenging classification benchmark. A.2 Audio Benchmarks ESC-50 (Environmental Sound Classification): Contains 2,000 environmental audio recordings with 50 classes and 40 clips per class. Each clip is 5 seconds long at 44.1 kHz, single channel, extracted from public field recordings through Freesound.org Piczak (2015). RAVDESS (Ryerson Audio-Visual Database of Emotional Speech and Song): Audio dataset focusing on emotion recognition tasks, commonly used for evaluating emotional speech and song recognition capabilities Livingstone and Russo (2018). UrbanSound8K: Contains 8,732 labeled sound excerpts with 10 classes of outdoor/urban sounds, specifically designed for benchmarking sound classification models in urban environments. A.3 Biological/Scientific Vision Benchmarks FishNet: Large-scale dataset with 94,532 images from 17,357 aquatic species, organized by biological taxonomy (8 classes, 83 orders, 463 families, 3,826 genera). Includes bounding box annotations and supports classification, detection, and functional trait prediction tasks Khan et al. (2023). We treat FishNet as classification problem over families. AWA2 (Animals with Attributes 2): Animal classification dataset used for zero-shot learning tasks, focusing on learning representations with animal attributes. Part of challenging benchmarks alongside CUB and SUN datasets Xian et al. (2019). We treat AWA2 as 50-class classification problem with no holdout classes. PlantDoc: Contains 2,569 images across 13 plant species and 30 classes (diseased and healthy) with 8,851 total labels. Split into 2,328 training and 237 test images, with unbalanced classes ranging from 50-180 images per class Singh et al. (2020). A.4 Tabular Benchmarks OpenML CC18: Curated benchmark suite of 72 classification datasets from OpenML 69 of which we utilize), selected based on strict criteria: Size: 500-100,000 observations, 5,000 features Quality: No artificial data, minority/majority class ratio 0.05 Usability: Compatible with multiple algorithms, representing commonly used ML datasets See Bischl et al. (2021) for more on this benchmark, including the complete specification of tasks. Regression 2025: Custom benchmark of 43 regression tasks from 2015-2025 sourced from OpenML, evaluated using R² scores on 0-100 scale for consistent comparison across tasks; introduced onto the OpenML platform in March 2025 at openml.org/search?type=benchmark&sort=tasks included&study type=task&id=455. Please follow the link for the complete list and specification of tasks. After discarding tasks on which all models fail, we compute our scores on subset of 33."
        },
        {
            "title": "B Implementation Details",
            "content": "B.1 Embedding Models Vision: DINO-v2-ViT-L-14-reg provides robust visual representations trained through self-supervised learning on large-scale image datasets Oquab et al. (2023). 15 Preprint. Under review. Audio: Microsoft CLAP employs contrastive audio-language pre-training to create joint embeddings for audio and text modalities Elizalde et al. (2023). Biological: BioCLIP2 specializes in scientific vision understanding, trained on biological image-text pairs for enhanced performance on scientific datasets. It is the latest in series of foundation models for biological applications, initiated by BioCLIP, which incorporated taxonomic labels in the vision-language contrastive training, yielding promising species classification accuracy Stevens et al. (2024). Follow-up work scaled data to 162M images (BioTrove, Yang et al., 2024), specialized the data to camera traps (CATALOG and WildCLIP, Gabeff et al., 2024; Santamaria et al., 2025), and added additional model modalities (TaxaBind, Sastry et al., 2025). Tabular: Tabular machine learning has traditionally relied on specialized approaches including tree-based methods (Random Forest Breiman (2001), XGBoost Chen and Guestrin (2016), CatBoost Prokhorenkova et al. (2018)) and specialized neural architectures (TabNet Arik and Pfister (2021), TabTransformer Huang et al. (2020)). TabPFN Hollmann et al. (2022) employed transformer-based in-context learning, and was later extended to support larger datasets Feuer et al. (2024); Hollmann et al. (2025); uller et al. (2025). In this work, we use TabPFNv2 as our embedding generating model. B.2 Hyperparameters In this section, we document the hyperparameters used for our main experiments section. t-SNE Configuration: Perplexity: 15 (optimized through ablation studies) Iterations: 1000 for stable convergence Learning rate: 200 (default) Random state: Fixed for reproducibility KNN Configuration nn = 30 metric = euclidean (general), cosine (embeddings) weights = distance Tabular Baseline Models Configuration: CatBoost (Classification & Regression) iterations: 1000 depth: 6 learning rate: 0.03 random seed: verbose: False Categorical features: Auto-detected and preserved TabPFN v2 (Classification & Regression) estimators: 8 device: Auto-detected (CUDA if available) ignore pretraining limits: True Target preprocessing: Quantile binning for regression Max quantiles: min(n samples // 2, 1000) NaN/INF imputation: Median strategy Random Forest (Classification & Regression) estimators: 100 max depth: None (unlimited) 16 Preprint. Under review. random state: 42 jobs: -1 (all cores) Gradient Boosting (Classification & Regression) estimators: 100 learning rate: 0.1 random state: 42 Feature selection: Max 500 features (SelectKBest) Logistic/Linear Regression max iter: 1000 (Logistic only) C: 1.0 (Logistic regularization) random state: 42 jobs: -1 (all cores) Preprocessing: StandardScaler applied"
        },
        {
            "title": "C Extended Results",
            "content": "C.1 Computational Efficiency Model Size: MARVIS uses Qwen2.5-VL (3B parameters). Inference Time: Average processing time per sample ranges from 0.5-2.0 seconds depending on visualization complexity and VLM reasoning depth. Memory Requirements: All experiments are conducted using 1xH100 80GB GPUs on hosted Lambda cluster. Peak memory usage remains under 8GB GPU memory for batch processing, enabling deployment on standard hardware. GPU Utilization: For development and testing combined, we estimate 1,500 H100-hours were used during the creation of this paper. C.2 Ablation Study Details Extended ablation studies reveal optimal configurations across different visualization strategies. We systematically evaluated four key approaches to understand how different types of information affect VLM spatial reasoning performance. The configuration performance hierarchy demonstrates clear patterns: tsne perturbation axes: 51.7% accuracy with uncertainty analysis tsne semantic axes: 50.0% accuracy with meaningful class labels tsne knn: 48.3% accuracy with explicit neighbor information basic tsne: 45.0% accuracy as baseline approach C.2.1 Analysis of Configuration Effects The ablation results reveal several key insights about VLM spatial reasoning: Perturbation-based Enhancement: The tsne perturbation axes configuration achieves the highest performance by incorporating uncertainty information through small perturbations around the query point. This provides the VLM with richer spatial context about decision boundaries and confidence regions. Semantic Information Value: The tsne semantic axes approach shows strong performance by providing meaningful class labels within the visualization. This allows the VLM to leverage both spatial relationships and semantic understanding simultaneously. Neighbor Information Benefits: The tsne knn configuration demonstrates moderate improvements over the baseline by explicitly highlighting nearest neighbors, helping the VLM focus on locally relevant information. 17 Preprint. Under review. Figure 4: Configuration Performance Heatmap. Detailed breakdown showing performance variations across different parameter combinations and visualization strategies. Darker regions indicate higher accuracy, with perturbation-based methods consistently showing superior performance across various settings. Baseline Robustness: Even the basic tsne approach achieves reasonable performance (45%), validating the fundamental effectiveness of the visual reasoning paradigm across modalities. Deep Dive: Tabular Modality Analysis This section provides comprehensive analysis of MARVIS performance on tabular data, evaluating both classification and regression tasks against established baselines. The analysis includes detailed performance metrics, correlation studies with TabPFN v2, and critical difference plots for statistical comparison. D.1 Baselines: JOLT and TabLLM One challenge we faced during the creation of this paper is that prior work which utilized FMs for tabular classification and regression lacked both standard benchmarks and consistent, easy to implement methods. As secondary contribution, we release comprehensive full-size tabular benchmarks which include semantic information (see D.6), and modern, feature-complete implementations of TabLLM and JOLT. Dual Implementation Architecture: We developed sophisticated dual-path architecture that supports both legacy compatibility and modern framework integration. Our implementation includes: Legacy Integration: Direct incorporation of original JOLT codebase with automatic fallback mechanisms Modern Implementation: Complete HuggingFace transformers integration with VLLM backend support Unified Model Loader: Centralized model management supporting multiple backends (HuggingFace, VLLM, OpenAI, Gemini) Memory Optimization and Scalability: Critical for production deployment, our implementation includes: 18 Preprint. Under review. Gradient checkpointing with KV cache disabling for memory efficiency Dynamic batch sizing with automatic Out-of-Memory (OOM) recovery Aggressive memory limits for regression tasks (512MB default) Feature dropping with retry mechanisms for large datasets Enhanced Task Support: Beyond the original classification focus, we extended JOLT to support: Full regression pipeline with intelligent binning strategies Automatic task type detection and configuration Balanced few-shot example selection algorithms Context-aware prompt truncation for varying model context lengths Configuration Management: We developed comprehensive metadata system: Automatic JOLT configuration discovery by OpenML task ID Feature count validation ensuring dataset-configuration alignment Semantic feature mapping from original to descriptive names Graceful degradation when configurations are unavailable TabLLM Implementation Real-time Note Generation: Our TabLLM implementation eliminates the need for pre-generated note banks through: On-the-fly natural language description generation Dynamic semantic feature expansion matching actual dataset characteristics Template-based prompt generation with YAML configuration support Automatic feature alignment verification post-preprocessing Multi-Backend API Support: We created unified interface supporting: OpenAI API integration (GPT-4, GPT-3.5-turbo, GPT-4o) Google Gemini API support with automatic model selection Local model deployment via HuggingFace transformers Automatic backend detection based on model naming conventions Quality Assurance Mechanisms: To ensure generation quality, we implemented: Inspection system saving sample generated notes for manual review N-gram analysis for content validation and diversity assessment Context truncation with intelligent few-shot example selection Template validation ensuring prompt completeness HuggingFace Ecosystem Compatibility Both implementations leverage the complete HuggingFace ecosystem: AutoModelForCausalLM and AutoTokenizer for model loading Trust remote code support for cutting-edge models Automatic device placement and memory optimization Support for quantized models (8-bit, 4-bit) through BitsAndBytes VLLM Integration For production deployments requiring high throughput: 19 Preprint. Under review. Automatic VLLM backend selection for compatible models Tensor parallelism configuration for multi-GPU deployment Optimized sampling parameters with fallback to transformers Unified generation interface across backends Benchmark Integration Our implementations integrate seamlessly with standard evaluation frameworks: Direct OpenML dataset loading and preprocessing Standardized evaluation interface compatible with scikit-learn Comprehensive metrics calculation (accuracy, F1, ROC-AUC, R², MAE, MSE) Weights & Biases integration for experiment tracking Usage and Accessibility Our implementations provide simple, unified interfaces: # JOLT evaluation with local model python examples / tabular / evaluate_llm_baselines_tabular . py -- models jolt -- dataset_ids 23 -- jolt_model Qwen / Qwen2 .5 -7B - Instruct # TabLLM evaluation with API backend python examples / tabular / evaluate_llm_baselines_tabular . py -- models tabllm -- dataset_ids 1590 -- openai_model gpt -4 This unified interface abstracts away implementation complexity while providing extensive configuration options for advanced users. D.2 Classification Performance on OpenML CC18 The OpenML CC18 benchmark represents one of the most comprehensive evaluation suites for tabular classification, consisting of 72 carefully curated datasets Bischl et al. (2021). Model Mean Acc. Balanced Acc. F1 Macro ROC AUC Datasets MARVIS TabPFN v2 CatBoost Random Forest Gradient Boosting Logistic Regression TabLLM (Gemini) TabLLM (Qwen) JOLT 84.5% 87.8% 87.0% 86.5% 85.4% 82.5% 50.1% 42.9% 41.0% 80.2% 82.2% 81.5% 80.3% 79.5% 74.8% 44.3% 36.5% 33.9% 79.9% 82.3% 81.8% 81.0% 79.9% 75.0% 40.2% 30.9% 27.3% 77.6% 93.0% 92.6% 91.9% 91.8% 89.1% 59.7% 50.4% 50.1% 69 66 70 70 70 70 69 69 67 Table 2: Classification Performance on OpenML CC18. MARVIS achieves competitive performance with traditional ML methods while significantly outperforming other LLM-based approaches. Performance metrics include mean accuracy, balanced accuracy for handling class imbalance, F1 macro for multi-class evaluation, and ROC AUC for ranking quality. Key insights from classification analysis: MARVIS achieves 84.5% mean accuracy, placing it competitively among traditional ML methods Strong performance on balanced accuracy (80.2%) demonstrates effective handling of class imbalance Significantly outperforms other LLM-based approaches (TabLLM, JOLT) by 34-44 percentage points Consistent performance across diverse dataset types with low variance (σ = 15.1%) 20 Preprint. Under review. Figure 5: Critical Difference Plot for Classification Performance. Statistical analysis using balanced accuracy across OpenML CC18 datasets. Connected algorithms have no statistically significant difference (p 0.05) using the Nemenyi post-hoc test. MARVIS ranks competitively among traditional ML methods and significantly outperforms other LLM approaches. Figure 6: Classification Performance Matrix Heatmap. Dataset-wise performance comparison showing MARVIS consistency across different types of tabular classification tasks. Each row represents dataset, and each column represents an algorithm. Darker colors indicate higher balanced accuracy scores. D.3 Regression Performance Analysis For regression tasks, MARVIS was evaluated on custom benchmark of 43 regression datasets spanning diverse domains and characteristics. D.4 Correlation Analysis with TabPFN v2 detailed correlation analysis between MARVIS and TabPFN v2 reveals interesting patterns in their complementary strengths and failure modes. 21 Preprint. Under review. Algorithm Mean R² Median R² MAE RMSE Random Forest TabPFN v2 Gradient Boosting Linear Regression MARVIS LightGBM XGBoost 0.586 0.585 0.564 0.538 0.532 0.519 0. 0.644 0.623 0.615 0.588 0.576 0.567 0.534 0.184 0.187 0.191 0.203 0.198 0.201 0.218 0.298 0.301 0.304 0.318 0.312 0.321 0.342 Table 3: Regression Performance Summary. MARVIS achieves competitive R² scores (0.532 mean, 0.576 median) ranking 5th among 7 algorithms. While R² scores are moderate, MARVIS shows strong performance in error metrics (MAE, RMSE), indicating consistent prediction quality. Figure 7: Critical Difference Plot for Regression Performance. Statistical comparison using R² scores across 43 regression datasets. MARVIS demonstrates statistically competitive performance with traditional methods, ranking in the middle tier without significant differences from top performers. Key correlation insights: High Classification Alignment: 0.978 Pearson correlation indicates both methods excel on similar classification tasks Moderate Regression Correlation: 0.884 correlation suggests more divergent strengths in regression domain Complementary Performance: Datasets where one method fails often correspond to failures in the other, suggesting systematic challenges rather than method-specific weaknesses Consistent Rankings: High Spearman correlations (0.945 classification, 0.867 regression) show similar relative performance orderings D.5 Analysis and Discussion The comprehensive tabular analysis reveals several important findings about MARVIS performance in structured data domains: 22 Preprint. Under review. Figure 8: Regression Performance Matrix Heatmap. Dataset-wise R² score comparison showing MARVIS performance patterns across different regression tasks. The visualization reveals strengths in certain problem types while highlighting areas for potential improvement. Task Type Pearson Spearmanρ Kendall τ Datasets Classification Regression 0.978 0. 0.945 0.867 0.823 0.698 65 41 Table 4: MARVIS-TabPFN v2 Correlation Summary. Strong positive correlations indicate that both methods tend to perform well on similar datasets, suggesting complementary rather than competing approaches. The high classification correlation (0.978) demonstrates particularly aligned performance patterns. Competitive Classification Performance: MARVIS achieves strong results on OpenML CC18, demonstrating that visual reasoning approaches can effectively handle tabular classification tasks. The 84.5% accuracy places MARVIS within the competitive range of traditional ML methods. Moderate Regression Capabilities: With 0.532 mean R² on regression tasks, MARVIS shows reasonable but not exceptional regression performance. This suggests the visual reasoning paradigm may be better suited for discrete classification decisions than continuous value prediction. Strong LLM Baseline Performance: MARVIS significantly outperforms other LLM-based tabular methods (TabLLM, JOLT), validating the effectiveness of the visual reasoning approach compared to direct tabular-totext conversion strategies. Complementary Method Profile: The high correlation with TabPFN v2 suggests MARVIS and traditional tabular methods have similar strengths and weaknesses, making MARVIS viable alternative rather than replacement for existing approaches. Scalability Considerations: MARVIS maintains consistent performance across the diverse OpenML CC18 collection, suggesting good generalization properties across different tabular data characteristics and domains. D.6 CC18-Semantic and Regression2025-Semantic: Semantic Metadata Generation for Enhanced Dataset Understanding key component of our tabular analysis involved the creation of comprehensive semantic metadata for both classification (cc18 semantic) and regression (regression semantic) datasets. This process, conducted using Claude Research from Anthropic with human review, represents significant advancement in dataset documentation and understanding. D.6.1 Motivation and Scope Traditional machine learning benchmarks often lack rich semantic context about feature meanings, target interpretations, and domain-specific knowledge. To address this limitation, we developed systematic approach to generate comprehensive semantic metadata for: 23 Preprint. Under review. Figure 9: MARVIS vs TabPFN v2 Classification Correlation. Scatter plot showing strong positive correlation (r = 0.978) between MARVIS and TabPFN v2 balanced accuracy scores across OpenML CC18 datasets. Points above the diagonal line indicate datasets where MARVIS outperforms TabPFN v2. CC18 Classification Tasks: 72 datasets from the OpenML CC18 benchmark suite Regression Tasks: 41 carefully selected regression datasets from OpenML Total Coverage: 113 datasets with comprehensive semantic enrichment D.6.2 Semantic Metadata Generation Algorithm The semantic metadata generation process follows multi-stage pipeline designed to ensure accuracy, comprehensiveness, and consistency across all datasets. D.6.3 Semantic Enrichment Structure The generated metadata follows standardized schema that captures multiple dimensions of dataset understanding: Feature-Level Enrichment: Each feature receives comprehensive semantic description including domain context, technical interpretation, data type classification, and relationship analysis to the prediction task. Target Variable Analysis: For classification tasks, detailed explanations of class meanings and real-world interpretation. For regression tasks, units of measurement, typical ranges, and practical significance guidelines. Historical and Methodological Context: Dataset provenance including original creators, institutions, collection methodology, domain applications, and ethical considerations. Example Semantic Enhancement: 24 Preprint. Under review. Figure 10: MARVIS vs TabPFN v2 Regression Correlation. Scatter plot showing moderate positive correlation (r = 0.884) between MARVIS and TabPFN v2 R² scores across regression datasets. The correlation suggests similar strengths but with more divergent performance patterns compared to classification tasks. Feature: bkblk (Chess Kr-vs-Kp dataset) Basic metadata: Binary feature (t/f) Semantic enhancement: Whether the black king is blocked from moving to certain squares. In chess endgame analysis, this represents critical positional constraint that affects the feasibility of defensive strategies and directly influences whether White can force win from the current position. D.6.4 Multi-Source Research Methodology The Claude Research process integrates information from multiple authoritative sources to ensure accuracy and comprehensiveness: Primary Sources: Original dataset publications, creator documentation, and institutional repositories Academic Literature: Peer-reviewed papers utilizing the datasets, domain-specific research Repository Documentation: UCI ML Repository, OpenML detailed descriptions, Kaggle dataset pages Domain Databases: Specialized knowledge bases relevant to specific application areas Cross-Validation: Multiple source verification to ensure factual accuracy D.6.5 Quality Assurance and Validation The semantic metadata generation incorporates multiple layers of quality control: 25 Preprint. Under review. Algorithm 1 Semantic Metadata Generation Pipeline Original dataset publications and creators Domain-specific knowledge bases Academic literature and citations UCI ML Repository and similar sources 1: Input: OpenML dataset ID, basic task information 2: Output: Comprehensive semantic metadata JSON 3: 4: Stage 1: Data Source Integration 5: Query OpenML API for basic dataset information 6: Extract feature names, data types, target variables, and statistics 7: Collect dataset provenance and publication information 8: 9: Stage 2: Claude Research Process 10: Initialize Claude 3.5 Sonnet with domain expertise prompt 11: Instruct comprehensive multi-source research covering: 12: 13: 14: 15: 16: 17: Stage 3: Structured Semantic Analysis 18: for each feature in dataset do 19: Generate semantic description with domain context Classify data type and measurement characteristics 20: Explain relationship to prediction task 21: 22: end for 23: 24: Stage 4: Target Variable Enhancement 25: if classification task then 26: Describe meaning of each class label 27: 28: else Explain target variable units and ranges 29: 30: Describe practical significance of values 31: end if 32: 33: Stage 5: Quality Assurance 34: Apply low temperature (0.1) for factual consistency 35: Include uncertainty acknowledgments where appropriate 36: Validate JSON structure and completeness 37: Enable human review and verification process Provide real-world interpretation guidelines Algorithmic Validation: Automated scripts verify JSON structure completeness, field presence patterns, and schema compliance across all datasets. Coverage Analysis: Systematic review ensures all required metadata fields are populated and coverage gaps are identified for remediation. Human Review Integration: The process includes explicit uncertainty acknowledgment when information sources are limited, enabling targeted human verification. Standardization Pipeline: Automated standardization scripts consolidate different metadata formats into universal schema while preserving original information and implementing backup systems. D.6.6 Impact on Tabular Machine Learning The semantic metadata generation process provides several key benefits for tabular machine learning research: Enhanced Interpretability: Rich semantic context enables better understanding of model predictions and feature importance Domain-Aware Analysis: Researchers can leverage domain knowledge for more informed model development Bias Identification: Explicit documentation of dataset limitations and potential biases 26 Preprint. Under review. Cross-Dataset Understanding: Standardized semantic descriptions facilitate comparison and meta-analysis across diverse datasets Educational Value: Comprehensive context makes datasets more accessible for teaching and learning D.6.7 Novel Contributions This semantic metadata generation approach represents several methodological innovations: LLM-Powered Research Integration: Systematic use of Claude Research capabilities to synthesize information from multiple authoritative sources, going beyond traditional automated metadata extraction. Semantic Relationship Mapping: Explicit documentation of how features relate to each other and the prediction task, providing insight into dataset structure and modeling considerations. Multi-Modal Documentation: Integration of technical specifications with domain expertise and historical context, creating comprehensive resource for researchers. Scalable Quality Assurance: Automated validation and standardization processes that maintain consistency across large collections of datasets while preserving semantic richness. The resulting cc18 semantic and regression semantic collections provide an unprecedented level of semantic documentation for tabular machine learning benchmarks, enabling more informed and interpretable research across diverse domains and applications. D.7 Comprehensive Dataset Characterization This section provides detailed characterization of the datasets used in our tabular modality analysis, covering both the OpenML CC18 classification benchmark and the Regression 2025 benchmark suite. D.7.1 Domain Distribution Analysis The benchmark collections span diverse application domains, providing comprehensive coverage of real-world machine learning challenges. Domain CC18 Count Regression Count Total Vision Medical Biology Finance Games NLP Science/Engineering Social Other Total 27 7 5 4 4 3 0 0 22 72 4 7 2 3 1 3 2 1 18 31 14 7 7 5 6 2 1 40 113 Table 5: Domain Distribution Across Benchmark Collections. The datasets span nine major application domains, with Vision being the most represented (31 datasets), followed by Medical (14 datasets). The Other category includes diverse applications such as telecommunications, manufacturing, and environmental monitoring. D.7.2 Representative Dataset Examples OpenML CC18 Classification Tasks Regression 2025 Tasks D.7.3 Dataset Complexity Analysis The benchmark collections exhibit significant diversity in complexity characteristics: Feature Dimensionality Range: Low-dimensional ( 10 features): 29 datasets (25.7%) 27 Preprint. Under review. Dataset Domain Features Classes Description MiceProtein dna splice bank-marketing credit-g adult connect-4 kr-vs-kp tic-tac-toe breast-w heart-statlog diabetes Devnagari-Script mnist 784 Fashion-MNIST Biology Biology Biology Finance Finance Finance Games Games Games Medical Medical Medical Vision Vision Vision 77 1 1 16 20 14 3 36 9 9 13 8 1024 784 784 8 3 3 2 2 2 3 2 2 2 2 2 46 10 Mouse protein expression levels for Down syndrome study Molecular biology DNA sequence classification Primate splice-junction gene sequences analysis Portuguese banking institution marketing campaigns German credit risk assessment dataset Census income prediction (50K annual income) Connect-4 game position evaluation Chess King+Rook vs King+Pawn endgame positions Tic-tac-toe game board position analysis Wisconsin breast cancer diagnosis Heart disease diagnosis from clinical parameters Pima Indian diabetes onset prediction Handwritten Devanagari character recognition Handwritten digit recognition benchmark Fashion article classification from images Table 6: Representative CC18 Classification Datasets. Examples spanning major domains show the diversity of tabular classification challenges, from biological sequence analysis to game strategy evaluation and medical diagnosis."
        },
        {
            "title": "Features Target Description",
            "content": "QSAR Bioconcentration SGEMM GPU kernel climate change impact world food wealth Violent Crime County medical charges heart failure records particulate-matter UCC Comments housing prices 2020 cpu performance auto mpg wine quality concrete strength sulfur recovery Biology Biology Finance Finance Finance Medical Medical Medical Medical Other Other Other Other Science/Eng Science/Eng 13 10 15 6 6 4 13 7 7 9 7 8 11 8 6 Bioconcentration factor for environmental chemistry GPU kernel performance optimization metrics Agricultural productivity under climate change Global food security and economic indicators County-level violent crime rates (1975-2016) Healthcare insurance charges prediction Clinical parameters for heart failure prediction Air quality PM2.5 concentration levels Health impact assessment from social media Real estate price prediction modeling Computer hardware performance benchmarking Vehicle fuel efficiency prediction Wine quality assessment from chemical properties Concrete compressive strength from mixture Industrial sulfur recovery process optimization Table 7: Representative Regression Datasets. Examples demonstrate the breadth of continuous prediction tasks, from environmental monitoring and healthcare analytics to industrial process optimization and consumer applications. Medium-dimensional (11-50 features): 51 datasets (45.1%) High-dimensional ( 50 features): 33 datasets (29.2%) Classification Complexity: Binary classification: 48 datasets (66.7% of CC18) Multi-class (3-10 classes): 21 datasets (29.2% of CC18) High-class ( 10 classes): 3 datasets (4.1% of CC18) Domain-Specific Characteristics: Vision datasets: Typically high-dimensional (784-1024 features) with balanced class distributions Medical datasets: Often feature moderate dimensionality (8-20 features) with clinical interpretability requirements Financial datasets: Characterized by mixed data types and class imbalance considerations 28 Preprint. Under review. Game datasets: Show discrete feature spaces with strategic decision-making patterns Biology datasets: Range from sequence data (low-dimensional) to protein expression (high-dimensional) D.7.4 Benchmark Coverage and Representativeness The combined CC18 and Regression 2025 benchmarks provide comprehensive coverage of tabular machine learning challenges: Methodological Diversity: Tasks span supervised learning paradigms including binary/multi-class classification and continuous regression, enabling evaluation across prediction types. Real-World Relevance: Datasets originate from authentic applications in healthcare, finance, scientific research, and technology, ensuring practical relevance of evaluation results. Complexity Spectrum: The collection includes datasets ranging from simple proof-of-concept problems to challenging high-dimensional tasks, enabling assessment across difficulty levels. Semantic Richness: Each dataset includes comprehensive semantic metadata enabling domain-aware analysis and interpretation of model behavior across diverse application contexts. This comprehensive characterization establishes the benchmark collections as robust evaluation frameworks for assessing tabular machine learning methods across diverse domains and complexity levels."
        },
        {
            "title": "E VLM Reasoning Analysis",
            "content": "This section provides detailed evidence that Vision-Language Models engage in genuine adaptive reasoning when processing MARVIS visualizations, rather than relying solely on learned patterns or simple heuristics. Our analysis examines reasoning traces, disagreement patterns, and method-specific behavioral signatures to demonstrate that VLMs condition their responses on the visual information provided. E.1 Comprehensive Reasoning Pattern Analysis E.1.1 Performance-Driven Features Analysis of 83 experimental configurations across multiple test cases reveals systematic differences between correct and incorrect predictions, indicating that reasoning quality correlates with classification accuracy. Reasoning Feature Correct Incorrect Difference Response Length Word Count Color Mentions Distance Reasoning 281.2 chars 43.8 words 1.85 0.074 268.3 chars 42.4 words 1.52 0.057 Closest Heuristics Majority Heuristics Cluster Reasoning 0.56 0.05 0. 0.77 0.25 0.73 +12.9 +1.4 +0.33 +0.018 -0.21 -0.20 -0.13 Table 8: Reasoning Quality Correlation with Accuracy. Correct predictions exhibit longer, more sophisticated responses with increased spatial analysis and reduced reliance on simple heuristics. This pattern suggests VLMs engage in more thorough reasoning when visual information supports accurate classification. E.1.2 Method-Specific Reasoning Signatures Different visualization methods elicit systematically different reasoning approaches, providing strong evidence that VLMs adapt their analysis based on visual information content. E.2 Adaptive Reasoning Evidence E.2.1 Disagreement Pattern Analysis Analysis of prediction disagreements across methods provides evidence that different visualization types provide genuinely different information to VLMs, resulting in systematic behavioral differences. 29 Preprint. Under review. Method Resp. Length Word Count Distance Mentions Closest Usage tsne 3d perturbation tsne perturbation axes tsne semantic axes tsne knn basic tsne 365.3 310.6 304.9 279.0 268.3 58.6 47.6 47.2 48.0 42.4 0.000 0.000 0.000 0.650 0.000 0.433 0.650 0.683 0.883 1. Table 9: Method-Specific Reasoning Patterns. Each visualization method elicits distinct reasoning behaviors: k-NN methods trigger quantitative distance analysis, perturbation methods generate longer responses, and basic methods rely heavily on proximity heuristics. Key Disagreement Statistics: Only 35% agreement across all methods on test cases 65% partial disagreement indicates methods provide different information Highest disagreement pairs: tsne knn vs tsne 3d perturbation (33 disagreements) E.2.2 Concrete Examples of Adaptive Reasoning The following examples demonstrate how VLMs adapt their reasoning based on the specific visual information provided: Quantitative Analysis with k-NN Information: The query point is closer to the cluster of Class 1 neighbors (4 neighbors) than to the cluster of Class 2 neighbors (1 neighbor). Additionally, the average distance to Class 1 neighbors (6.1) is slightly lower than to Class 2 neighbors (5.2), indicating higher similarity to Class 1. Semantic Integration with Class Labels: The red star (query point) is closest to the orange-colored points, which represent the Long-term methods class. This spatial clustering indicates that the query point is more aligned with the characteristics of the Long-term methods class. Basic Proximity Analysis: The red star (query point) is closest to the green-colored training points, which are associated with Class 2. These examples show clear adaptation: quantitative distance calculations appear only with k-NN information, semantic reasoning emerges with meaningful class labels, and basic approaches rely on simple proximity heuristics. E.3 Implications for VLM Understanding E.3.1 Evidence Against Pattern Matching Several findings argue against simple pattern matching explanations: Method-specific reasoning adaptation: Different visualization types elicit systematically different reasoning approaches Performance-quality correlation: Better reasoning correlates with higher accuracy across diverse test cases Quantitative analysis emergence: Numerical reasoning appears precisely when relevant information is provided Logical consistency within methods: Each approach maintains internal logical coherence while differing from others E.3.2 Spatial Reasoning Capabilities The evidence suggests VLMs possess genuine spatial reasoning capabilities that can be effectively leveraged through appropriate visualization design: Preprint. Under review. Color-space integration: Systematic use of color information for class identification Distance relationship understanding: Quantitative analysis of spatial proximity when information is available Cluster structure recognition: Identification of grouping patterns in embedding spaces Multi-modal information synthesis: Integration of spatial, semantic, and quantitative information E.4 Design Implications This analysis reveals several key principles for designing effective VLM interfaces: Information density matters: Richer visualizations elicit more sophisticated reasoning Method-purpose alignment: Different visualization approaches suit different reasoning tasks Measurable reasoning quality: VLM reasoning sophistication can be quantified and optimized Adaptive interface design: VLMs can effectively utilize different types of visual information when appropriately presented The comprehensive evidence presented demonstrates that VLMs engage in genuine adaptive reasoning when processing MARVIS visualizations, conditioning their behavior on the specific visual information provided rather than relying solely on learned patterns."
        },
        {
            "title": "F MARVIS Extended Results",
            "content": "In Table 10, we present the comprehensive results for all models on all benchmarks. MARVIS Method Variants: Detailed Ablation Documentation This section provides comprehensive documentation of all MARVIS visualization method variants evaluated in our ablation studies. Each method variant represents different approach to transforming embedding spaces into visual representations for VLM reasoning. G.1 Method Variants Overview 31 Preprint. Under review."
        },
        {
            "title": "Audio",
            "content": "CIFAR-10 CIFAR-100 CIFAR-10 CIFAR-100 CIFAR-10 CIFAR-100 CIFAR-10 CIFAR-"
        },
        {
            "title": "Biological",
            "content": "ESC-50 RAVDESS UrbanSound-8K ESC-50 RAVDESS UrbanSound-8K ESC-50 RAVDESS UrbanSound-8K FishNet AWA2 PlantDoc FishNet AWA2 PlantDoc FishNet AWA2 PlantDoc FishNet AWA2 PlantDoc"
        },
        {
            "title": "Tabular Classification",
            "content": "CC-18 (Semantic) CC-18 (Semantic) CC-18 (Semantic) CC-18 (Semantic) CC-18 (Semantic) CC-18 (Semantic) CC-18 (Semantic) CC-18 (Semantic)"
        },
        {
            "title": "Tabular Regression",
            "content": "Conventional Gemini-Flash-2.0 Conventional Gemini-Flash-2.0 Conventional Qwen 2.5 VL 3B Conventional Qwen 2.5 VL 3B KNN KNN CLAMS CLAMS DinoV2-ViT-L-14-reg DinoV2-ViT-L-14-reg CLAM 3B CLAM 3B"
        },
        {
            "title": "KNN\nKNN\nKNN\nContrastive\nContrastive\nContrastive\nCLAMS\nCLAMS\nCLAMS",
            "content": "Whisper-Large Whisper-Large Whisper-Large CLAP CLAP CLAP CLAM 3B CLAM 3B CLAM 3B Conventional Qwen 2.5 VL 3B Conventional Qwen 2.5 VL 3B Conventional Qwen 2.5 VL 3B Conventional Gemini-Flash-2.0 Conventional Gemini-Flash-2.0 Conventional Gemini-Flash-2.0 KNN KNN KNN CLAMS CLAMS CLAMS BioClip2 BioClip2 BioClip2 CLAM 3B CLAM 3B CLAM 3B Qwen 2.5 3B Qwen 2.5 3B Gemini-Flash-2.0 JOLT TabLLM TabLLM Conventional TabPFNv2 CLAMS CLAM 3B Conventional Random Forest Conventional Logistic Regression Conventional CatBoost Regression 2025 (Semantic) Conventional TabPFNv2 Regression 2025 (Semantic) Conventional CatBoost Regression 2025 (Semantic) Regression 2025 (Semantic) CLAMS Regression 2025 (Semantic) Conventional Linear Model Regression 2025 (Semantic) Conventional Random Forest Qwen 2.5 3B CLAM 3B"
        },
        {
            "title": "Accuracy\nAccuracy\nAccuracy\nAccuracy\nAccuracy\nAccuracy\nAccuracy\nAccuracy",
            "content": "Avg R² (0-100) Avg R² (0-100) Avg R² (0-100) Avg R² (0-100) Avg R² (0-100) Avg R² (0-100) 85.7 64.3 83.2 51.0 99.0 91.6 98.0 88.0 76.0 47.9 65.9 90.5 21.8 77.1 91.3 38.4 79.8 17.3 92.6 37.3 59.5 96.5 74.2 83.7 97.1 72.0 80.2 95.7 67.4 41.2 42.9 50.1 87.8 84.5 86.5 82.5 87.0 66.9 71.4 05.1 66.0 51.2 72. Table 10: Comprehensive Performance Results Across Multiple Domains. Evaluation of various methods on vision, audio, biological, and tabular benchmarks. CLAMS demonstrates competitive performance across all domains, achieving near state-of-the-art results while using unified approach. Success rates are 100% for all methods except JOLT on regression tasks (90.3%). 32 Preprint. Under review."
        },
        {
            "title": "Method",
            "content": "basic tsne tsne 3d tsne high dpi tsne high perplexity tsne knn"
        },
        {
            "title": "Enhanced Single Methods",
            "content": "tsne perturbation axes tsne semantic axes tsne 3d knn tsne 3d perturbation multi comprehensive multi pca tsne multi pca tsne spectral multi linear nonlinear multi local global multi with umap multi grid layout decision regions svm frequent patterns Multi-Visualization Methods"
        },
        {
            "title": "Description",
            "content": "Standard t-SNE visualization with default parameters Three-dimensional t-SNE visualization for enhanced spatial understanding High-resolution t-SNE with increased image quality t-SNE with modified perplexity parameter for different clustering t-SNE with k-nearest neighbor information overlay t-SNE with perturbation analysis for uncertainty quantification t-SNE with semantic class labels and axes descriptions 3D t-SNE visualization with k-NN connections displayed 3D t-SNE with perturbation analysis for spatial uncertainty PCA + t-SNE + Spectral + Isomap comprehensive view Combined PCA and t-SNE dual visualization Triple visualization: PCA + t-SNE + Spectral embedding Linear and nonlinear dimensionality reduction comparison Local and global structure preservation methods Multi-method visualization including UMAP Grid-based layout for systematic method comparison SVM decision boundary visualization with regions Pattern mining visualization for feature relationships metadata comprehensive Metadata-enhanced comprehensive visualization approach Table 11: MARVIS Method Variants Overview. Comprehensive summary of visualization approaches evaluated in ablation studies, categorized by methodology type and complexity level. 33 Preprint. Under review."
        },
        {
            "title": "H MARVIS Visualization Gallery",
            "content": "This section presents visualizations from the MARVIS framework applied to tabular datasets. H.0.1 CMC Dataset Visualizations Basic Visualizations Basic t-SNE 34 Preprint. Under review. Preprint. Under review. 36 Preprint. Under review. 37 Preprint. Under review. Preprint. Under review. Semantic Integration Semantic t-SNE 39 Preprint. Under review. 40 Preprint. Under review. 41 Preprint. Under review. 42 Preprint. Under review. 43 Preprint. Under review. Semantic Axes 44 Preprint. Under review. 45 Preprint. Under review. Preprint. Under review. 47 Preprint. Under review. 48 Preprint. Under review. Semantic with Metadata 49 Preprint. Under review. 50 Preprint. Under review. 51 Preprint. Under review. 52 Preprint. Under review. 53 Preprint. Under review. KNN and Connectivity KNN Visualization Preprint. Under review. 55 Preprint. Under review. 56 Preprint. Under review. Preprint. Under review. 58 Preprint. Under review. Importance Axes 59 Preprint. Under review. 60 Preprint. Under review. 61 Preprint. Under review. 62 Preprint. Under review. 63 Preprint. Under review. 3D Visualizations 3D t-SNE 64 Preprint. Under review. Preprint. Under review. 66 Preprint. Under review. 67 Preprint. Under review. Preprint. Under review. 3D with Perturbation 69 Preprint. Under review. 70 Preprint. Under review. 71 Preprint. Under review. 72 Preprint. Under review. 73 Preprint. Under review. Advanced Techniques Perturbation Analysis 74 Preprint. Under review. 75 Preprint. Under review. Preprint. Under review. 77 Preprint. Under review. 78 Preprint. Under review. Perturbation with Metadata 79 Preprint. Under review. 80 Preprint. Under review. 81 Preprint. Under review. 82 Preprint. Under review. 83 Preprint. Under review. Multi-view Approaches Comprehensive Multi-view Preprint. Under review. 85 Preprint. Under review. 86 Preprint. Under review. Preprint. Under review. Grid Layout 88 Preprint. Under review. 89 Preprint. Under review. 90 Preprint. Under review. 91 Preprint. Under review. Linear vs Non-linear Preprint. Under review. 93 Preprint. Under review. 94 Preprint. Under review. Preprint. Under review. Local vs Global 96 Preprint. Under review. 97 Preprint. Under review. 98 Preprint. Under review. 99 Preprint. Under review. PCA vs t-SNE Preprint. Under review. 101 Preprint. Under review. 102 Preprint. Under review. Preprint. Under review. 104 Preprint. Under review. PCA, t-SNE, and Spectral 105 Preprint. Under review. 106 Preprint. Under review. 107 Preprint. Under review. 108 Preprint. Under review. Multi-semantic Analysis 109 Preprint. Under review. 110 Preprint. Under review. Preprint. Under review. 112 Preprint. Under review. 113 Preprint. Under review. Multi-semantic with Axes and Metadata 114 Preprint. Under review. 115 Preprint. Under review. 116 Preprint. Under review. 117 Preprint. Under review. 118 Preprint. Under review. Multi t-SNE with Semantic Preprint. Under review. 120 Preprint. Under review. 121 Preprint. Under review. Preprint. Under review. 123 Preprint. Under review. Multi with UMAP 124 Preprint. Under review. 125 Preprint. Under review. 126 Preprint. Under review. 127 Preprint. Under review. Perturbation Axes Multi-view 128 Preprint. Under review. 129 Preprint. Under review. Preprint. Under review. 131 Preprint. Under review. 132 Preprint. Under review. Specialized Techniques Decision Regions (SVM) 133 Preprint. Under review. 134 Preprint. Under review. 135 Preprint. Under review. 136 Preprint. Under review. 137 Preprint. Under review. Frequent Patterns Preprint. Under review. 139 Preprint. Under review. 140 Preprint. Under review. Preprint. Under review. 142 Preprint. Under review. Metadata Comprehensive 143 Preprint. Under review. 144 Preprint. Under review. 145 Preprint. Under review. 146 Preprint. Under review. 147 Preprint. Under review. H.0.2 Credit-G Dataset Visualizations Basic Visualizations Basic t-SNE 148 Preprint. Under review. 149 Preprint. Under review. 150 Preprint. Under review. 151 Preprint. Under review. 152 Preprint. Under review. Semantic Integration Semantic t-SNE 153 Preprint. Under review. Preprint. Under review. 155 Preprint. Under review. 156 Preprint. Under review. Semantic Axes 157 Preprint. Under review. 158 Preprint. Under review. 159 Preprint. Under review. 160 Preprint. Under review. Semantic with Metadata 161 Preprint. Under review. Preprint. Under review. 163 Preprint. Under review. 164 Preprint. Under review. Preprint. Under review. KNN and Connectivity KNN Visualization 166 Preprint. Under review. 167 Preprint. Under review. 168 Preprint. Under review. 169 Preprint. Under review. 170 Preprint. Under review. 3D KNN 171 Preprint. Under review. 172 Preprint. Under review. Preprint. Under review. 174 Preprint. Under review. 175 Preprint. Under review. Importance Axes 176 Preprint. Under review. 177 Preprint. Under review. 178 Preprint. Under review. 179 Preprint. Under review. 3D Visualizations 3D t-SNE 180 Preprint. Under review. Preprint. Under review. 182 Preprint. Under review. 183 Preprint. Under review. Preprint. Under review. 3D with Perturbation 185 Preprint. Under review. 186 Preprint. Under review. 187 Preprint. Under review. 188 Preprint. Under review. 189 Preprint. Under review. Advanced Techniques Perturbation Analysis 190 Preprint. Under review. 191 Preprint. Under review. Preprint. Under review. 193 Preprint. Under review. Perturbation with Metadata 194 Preprint. Under review. 195 Preprint. Under review. 196 Preprint. Under review. 197 Preprint. Under review. 198 Preprint. Under review. Multi-view Approaches Comprehensive Multi-view 199 Preprint. Under review. Preprint. Under review. 201 Preprint. Under review. 202 Preprint. Under review. Grid Layout 203 Preprint. Under review. 204 Preprint. Under review. 205 Preprint. Under review. 206 Preprint. Under review. Linear vs Non-linear 207 Preprint. Under review. Preprint. Under review. 209 Preprint. Under review. 210 Preprint. Under review. Local vs Global 211 Preprint. Under review. 212 Preprint. Under review. 213 Preprint. Under review. 214 Preprint. Under review. PCA vs t-SNE 215 Preprint. Under review. Preprint. Under review. 217 Preprint. Under review. 218 Preprint. Under review. Preprint. Under review. PCA, t-SNE, and Spectral 220 Preprint. Under review. 221 Preprint. Under review. 222 Preprint. Under review. 223 Preprint. Under review. Multi-semantic Analysis Preprint. Under review. 225 Preprint. Under review. 226 Preprint. Under review. Preprint. Under review. 228 Preprint. Under review. Multi-semantic with Axes and Metadata 229 Preprint. Under review. 230 Preprint. Under review. 231 Preprint. Under review. 232 Preprint. Under review. 233 Preprint. Under review. Multi t-SNE with Semantic 234 Preprint. Under review. Preprint. Under review. 236 Preprint. Under review. 237 Preprint. Under review. Preprint. Under review. Multi with UMAP 239 Preprint. Under review. 240 Preprint. Under review. 241 Preprint. Under review. 242 Preprint. Under review. Perturbation Axes Multi-view Preprint. Under review. 244 Preprint. Under review. 245 Preprint. Under review. Preprint. Under review. 247 Preprint. Under review. Specialized Techniques Decision Regions (SVM) 248 Preprint. Under review. 249 Preprint. Under review. 250 Preprint. Under review. 251 Preprint. Under review. 252 Preprint. Under review. Metadata Comprehensive 253 Preprint. Under review. Preprint. Under review. 255 Preprint. Under review. 256 Preprint. Under review."
        }
    ],
    "affiliations": [
        "NYU",
        "Oumi.AI",
        "University of Freiburg"
    ]
}