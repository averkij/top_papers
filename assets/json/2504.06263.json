{
    "paper_title": "OmniSVG: A Unified Scalable Vector Graphics Generation Model",
    "authors": [
        "Yiying Yang",
        "Wei Cheng",
        "Sijin Chen",
        "Xianfang Zeng",
        "Jiaxu Zhang",
        "Liao Wang",
        "Gang Yu",
        "Xingjun Ma",
        "Yu-Gang Jiang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Scalable Vector Graphics (SVG) is an important image format widely adopted in graphic design because of their resolution independence and editability. The study of generating high-quality SVG has continuously drawn attention from both designers and researchers in the AIGC community. However, existing methods either produces unstructured outputs with huge computational cost or is limited to generating monochrome icons of over-simplified structures. To produce high-quality and complex SVG, we propose OmniSVG, a unified framework that leverages pre-trained Vision-Language Models (VLMs) for end-to-end multimodal SVG generation. By parameterizing SVG commands and coordinates into discrete tokens, OmniSVG decouples structural logic from low-level geometry for efficient training while maintaining the expressiveness of complex SVG structure. To further advance the development of SVG synthesis, we introduce MMSVG-2M, a multimodal dataset with two million richly annotated SVG assets, along with a standardized evaluation protocol for conditional SVG generation tasks. Extensive experiments show that OmniSVG outperforms existing methods and demonstrates its potential for integration into professional SVG design workflows."
        },
        {
            "title": "Start",
            "content": "OmniSVG: Unified Scalable Vector Graphics Generation Model Yiying Yang1,2 Wei Cheng2 Sijin Chen1 Xianfang Zeng2 Jiaxu Zhang2 Liao Wang2 Gang Yu2 Xingjun Ma1 Yu-Gang Jiang"
        },
        {
            "title": "2 StepFun",
            "content": "http://omnisvg.github.io 5 2 0 2 8 ] . [ 1 3 6 2 6 0 . 4 0 5 2 : r Figure 1. Highlighted features of OmniSVG. OmniSVG is capable of autoregressively generating high-quality SVGs across wide spectrum of complexity from simple icons to intricate anime characters. It demonstrates remarkable versatility through multiple generation modalities, including Text-to-SVG, Image-to-SVG, and Character-Reference SVG, making it powerful and flexible solution for diverse creative tasks."
        },
        {
            "title": "Abstract",
            "content": "Scalable Vector Graphics (SVG) is an important image format widely adopted in graphic design because of their resolution independence and editability. The study of generating high-quality SVG has continuously drawn attention from both designers and researchers in the AIGC community. However, existing methods either produces unstructured outputs with huge computational cost or is limited to generating monochrome icons of over-simplified structures. To produce high-quality and complex SVG, we propose OmniSVG, unified framework that leverages pretrained Vision-Language Models (VLMs) for end-to-end multimodal SVG generation. By parameterizing SVG commands and coordinates into discrete tokens, OmniSVG decouples structural logic from low-level geometry for efficient training while maintaining the expressiveness of complex SVG structure. To further advance the development of SVG synthesis, we introduce MMSVG-2M, multimodal dataset with two million richly annotated SVG assets, along * Yiying Yang and Wei Cheng contributed equally to this work. Corresponding Authors. with standardized evaluation protocol for conditional SVG generation tasks. Extensive experiments show that OmniSVG outperforms existing methods and demonstrates its potential for integration into professional SVG design workflows. 1. Introduction Scalable Vector Graphics (SVG) have become cornerstone of modern digital design because of their resolution independence, compact file size, and inherent editability. Widely adopted in professional workflows from UI/UX design to industrial CAD systems, SVG enables precise manipulation of geometric primitives (e.g., Bezier curves, polygons) while maintaining high precision and consistent visual quality across varying resolutions. However, creating high-quality SVG content remains challenging for nonexperts, requiring mastery of specialized tools or intricate XML syntax. To generate SVG contents, existing methods adopt either optimization-based methods or auto-regressive approaches. 1 Optimization-based methods [19, 30, 56] iteratively refine the SVG parameters by optimizing differentiable vector graphics rasterizers. Though these methods are effective in generating SVG icons, they suffer from significant computational overhead when scaling up to intricate samples and produce unstructured outputs with redundant anchor points. In contrast, auto-regressive methods build transformer models or adapt pre-trained Large Language Models (LLMs) to directly generate XML parameters [55] or codes [37, 53] to represent SVGs. Benefiting from an end-to-end learning pipeline, the auto-regressive method is more scalable approach [5] as it is able to learn directly from large collection of SVG samples. However, existing auto-regressive approaches are limited to basic SVG contents because of their limited context window length and the scarcity of complex SVG data [9, 20, 49]. To address these limitations, we propose OmniSVG, the first unified framework that harnesses pre-trained VLMs for end-to-end multimodal complex SVG generations. By parameterizing SVG coordinates and commands into discrete tokens, OmniSVG decouples structural logic from low-level geometry, mitigating the coordinate hallucination problem prevalent in code-based LLMs, and produces vivid and colorful SVG results. Benefiting from the next token prediction training objective, OmniSVG is also capable of completing SVGs with diverse generation results given some partial observations. Compared to traditional auto-regressive SVG generation, our method is able to parameterize the SVGs with token length up to 30k, facilitating the generation of complex and high-quality SVGs. Building upon pre-trained VLMs, our method natively integrates the ability to reason upon the visual and textual instructions to synthesize editable, high-fidelity SVGs across diverse domains, from icons to intricate illustrations and anime characters. To advance the development of SVG synthesis, we introduce MMSVG-2M, multi-modal SVG synthesis dataset with two million richly annotated assets, encompassing icons, illustrations, and anime designs. We also establish standardized evaluation protocol, MMSVG-Bench, for three key SVG generation tasks, i.e. Text-to-SVG, Image-to-SVG, and Character Reference SVG Generation. Extensive experiments show that OmniSVG produces highly detailed and complex SVG contents, surpassing prior arts both quantitatively and qualitatively. To summarize, our key contributions include: We introduce OmniSVG, the first family of end-to-end leverage pre-trained multimodal SVG generators that Vision-Language Models (VLMs), capable of generating complex and detailed SVGs, from simple icons to intricate anime characters. We present MMSVG-2M, large-scale dataset comprising two million SVG assets, along with standardized evaluation protocol for various conditional SVG generation tasks, providing comprehensive resource for future research. Extensive experiments show that OmniSVG surpasses prior SVG generation methods both qualitatively and quantitatively, highlighting its potential for integration into professional SVG design workflows. 2. Related Works Image Vectorization. Recent advances in vectorization leverage diffusion models paired with differentiable rasterizers, such as DiffVG [25]. These methods aim to convert raster images into SVG paths using score distillation sampling (SDS) [32]. Specifically, LIVE [30] employs layerwise optimization strategy with self-crossing loss and undirected distance-guided focus loss; VectorFusion [19] integrates pre-trained text-to-image diffusion model (Stable Diffusion [38]) with SVG-specific regularizers; and SVGDreamer [7] introduces semantic-driven image vectorization process and particle-based SDS loss to enhance visual quality. While these methods achieve remarkable results, they face limitations such as over-smoothing, color oversaturation, and lack of editability, often producing tangled paths that fail to capture hierarchical structures inherent in professional SVG designs. SVG Generation. Early attempts to generating SVGs directly builds sequence models like RNNs [15, 16, 36, 40, 41], VAEs [4, 28, 42, 45, 47], and Transformers [4, 52] to compress SVG commands into latent representations. Meanwhile, DeepSVG [4] further parameterizes SVGs using dual transformer architecture but struggles with geometric consistency. Recently, the advent of large language models (LLMs) [5, 6, 26, 48, 5760] unleashes the potential of generating SVGs via XML code synthesis. StrokeNUWA [44] converts vector images into compressed representation as sequence of tokens with VQVAEs. LLM4SVG [55] employs structured SVG encoding approach, utilizing learnable semantic tokens to accurately represent SVG components and their properties. StarVector [37] combines an image encoder with an LLM to generate SVG code from images, but this approach separates the understanding of text and images, leading to lack of effective alignment between textual descriptions and visual outputs. However, these methods suffer from limited token context window, which may struggle to process highly complex SVGs that require longer sequences. However, these methods [37, 53, 55] still face significant limitations when handling complex SVGs, as illustrations and anime characters often reach token length up to 10k or even 30k. SVG Datasets and Benchmarks. The lack of suitable datasets for complex SVG structures presents significant challenge. Existing datasets [9, 20, 49] primarily focus on simplified path-based SVGs or monochrome icons, over2 looking the intricate layered structures and rich color semantics found in real-world designs. For instance, the FIGR-8-SVG [9] dataset is centered on monochromatic icons, while StarVector [37] proposes categorized datasets, including illustrations, icons, emojis, and fonts. However, its datasets only include examples with up to 8.2k tokens, which still fail to capture the complexities of layered structures and rich color semantics. Benchmark evaluations, such as VGBench [65], further highlight gaps in multiformat testing and the absence of comprehensive coverage for illustrative SVGs. To address these shortcomings, we introduce MMSVG2M, multimodal SVG synthesis dataset comprising two million richly annotated assets, including icons, illustrations, and complex anime designs. Additionally, we present standardized evaluation benchmark, MMSVG-Bench, for SVG generation tasks of varying complexity. Unlike prior efforts, our benchmark emphasizes real-world applicability, offering full public access and rigorous evaluation across key metrics such as generation fidelity, diversity, and editability. This foundation lays the groundwork for future research, enabling more versatile and user-centric SVG synthesis. 3. MMSVG-2M MMSVG-2M is large-scale SVG dataset with two million SVG samples covering website icons, illustrations, graphic designs, anime characters, and so on (Sec. 3.1). To further promote the downstream development of SVG generation methods, we also introduce MMSVG-Bench, benchmark consisting of series of multi-modal instruction following tasks for conditional SVG generation (Sec. 3.2). 3.1. Data Curation Data Source. The raw SVG we collected from the Internet consists of three types: MMSVG-Icon sourced from Iconfont, SVG illustrations sourced from iconsount, and SVG anime characters sourced from Freepik and our data creation pipeline depicted in Fig. 2. All these websites are online platforms where users can publish and share SVGs, encompassing broad variety of categories. Specifically, our dataset comprises 2 million SVG samples within various subsets, including 1.1 million SVG icons, 0.5 million SVG illustrations, and 0.4 million SVG anime characters. We provide the data statistics of MMSVG-2M in Tab. 1. Table 1. Data statistics for MMSVG-2M. Our MMSVG-2M consists of 1.1 million SVG icons, 0.5 million SVG illustrations, and 0.4 million SVG anime characters. Dataset MMSVG-Icon MMSVG-Illustration MMSVG-Character Train 990k 450k 350k Val 106.7k 48.5k 48.9k Test 3.3k 1.5k 1.1k Source Iconfont IconSount Freepik & generated Token Length 2.2k 0.9k 8.1k 3.3k 28k 7.3k Data Curation. We extract SVG samples with comprehensive deduplication process based on filenames, SVG code, and metadata. We first fit the collected SVGs within viewbox of 200 200. Then, we employ an off-the-shelf VLM, specifically BLIP-2 [24], to generate captions for the SVGs. Please find more samples from the MMSVG2M dataset in Fig. 10, and instruction templates in Appendix A.2. SVG Simplification is an essential procedure in SVG data cleansing, since the over-complicated XML grammars in the crawled SVG data will lead to ambiguities while representing basic shapes. For example, the Rect command creates rectangular shape controlled by the starting point, width, and height arguments, like <rect x=\"80\" y=\"90\" width=\"100\" height=\"100\"/>, which can also be represented with four orthogonal lines. Also, the Circle Command creates circle shape based on like <circle cx=\"50\" center point and radius, cy=\"50\" r=\"50\"/>, which can also be approximated with the Bezier curve. Moreover, the Transform attribute in XML grammars applies an affine transFor example, <rect formation to an existing shape. x=\"50\" y=\"5\" width=\"40\" height=\"40\" fill=\"blue\" transform=\"rotate(45)\"/> refers to rotating the blue rectangle with 45 degrees. To standardize training and evaluation, we simplify all SVG commands with atomic commands as shown in Tab. 2. Inspired by FIGR-8-SVG [9] and IconShop [52], we remove all attributes and simplify each SVG with five basic commands, including Move To (M), Line To (L), Cubic Bezier (C), Elliptical Arc (A), ClosePath (Z). The introduction of atomic commands further removes the amTable 2. SVG draw-commands. Draw commands used in this work along with their arguments and visualization are listed. The start-position (x1, y1) is implicitly defined as the end-position of the preceding command. Command Arguments Description Visualization <SOP> Start-of-Path token. (MoveTo) (LineTo) (Cubic Bezier) x2, Move the cursor to the end-point (x2, y2) without drawing anything. x2, y2 Draw line to the point (x2, y2). qx1, qy1 qx2, qy2 x2, y2 Draw cubic Bezier curve with control points (qx1, qy1), (qx2, qy2) and end-point (x2, y2). (Elliptical Arc) rx, ry φ, fA, fS x2, y2 Draw an elliptical arc with radii rx and ry (semi-major and semi-minor axes), rotated by angle φ to the x-axis, and end-point (x2, y2). (x2, y2). (ClosePath) (Fill) <EOS> ill Close the path by moving the cursor back to the paths starting position (x0, y0). Draw the fill attribute of the path. End-of-SVG token. 3 Figure 2. Overview of OmniSVG. OmniSVG is built on pre-trained vision-language model Qwen2.5-VL and incorporates an SVG tokenizer. The model tokenizes both text and image inputs as prefix tokens, while the SVG tokenizer encodes vector graphics commands into unified representation space. biguities, as complex XML grammars can be approximated with the combination of several atomic commands. To efficiently produce unified and less complex data structure, we utilize picosvg to remove grammars like group and transform, and simplify the complex commands to atomic path commands. It is worth noting that atomic path commands are sufficient to represent complex SVGs shown in Fig. 1. 3.2. Multimodal Instruction Following for SVG MMSVG-Bench mainly focuses on the following SVG generation tasks, including: Text-to-SVG requires model to generate SVGs from text instructions. This includes the MMSVG-2M datasets test set, which provides textual description per image, and the FIGR-8-SVG dataset, enabling the generation of simpler (fewer paths) icons from text. For text-to-SVG evaluation, we measure visual quality with Frechet Inception Distance (FID) [46], text-SVG alignment with CLIP score [34], aesthetic appeal with Aesthetic score [39], and HPS (Human Preference Scores) [54]. Image-to-SVG evaluates models ability to convert images into SVGs. MMSVG-Icon, MMSVG-Illustration, and MMSVG-Character provide increasing levels of visual complexity. We provide train, validation, and test splits for these datasets. For image-to-SVG evaluations, we use DinoScore (DINO) [31], the cosine similarity between DinoV2 features, Structural Similarity Index (SSIM) [50], Learned Perceptual Image Patch Similarity (LPIPS) [61], https://github.com/googlefonts/picosvg and Mean Squared Error (MSE) to evaluate the imageconditioned SVG generation. Character-Reference SVG Generation evaluates models ability to generate novel SVGs while keeping the profile of the characters depicted in the input image. The difference between the image-to-SVG task and the character-reference SVG Generation task is that the character-reference SVG generation task is not to reconstruct the SVG of the input image, but to recreate specific character SVG for the input image, as shown in Fig. 6. Acknowledging the absence of established baselines, we assess the alignment between input character images and generated SVGs using the third-party VLMs assistant GPT-4o [18] . This assistant assigns score ranging from 1 to 10, where 1 indicates poor match with significant discrepancies, and 10 signifies an outstanding alignment with highly accurate representation of the original image. For evaluation metrics, we also calculate the average token length (# tokens) of generated SVG sample utilizing the Qwen2.5-VL [1] tokenizer. For computational cost, we compute the average generation time of generating per SVG sample. 4. OmniSVG To support end-to-end training for multi-modal SVG generation, OmniSVG parameterizes series of atomic path commands representing an SVG into sequence before feeding into pre-trained VLM with multi-modal instructions. SVG Parameterization. As illustrated in Sec. 3, our MMSVG-2M dataset simplifies every SVG by removing all attributes and using five basic commands, including 4 i }Ni , = (U j=1, where Move To (M), Line To (L), Cubic Bezier (C), Elliptical Arc (A), ClosePath (Z). After the simplification, an SVG script is represented as the combination of paths, = {Pi}M i=1. Here, Pi is the i-th path containing Ni commands, Pi = {C is the jth command in the i-th path. Each command is represented as ), containing both the type identifier {M, L, C, A, Z} and the corresponding location argument . Move To (M) command means to move the cursor to the end-point (x2, y2) without drawing anything. Line To (L) means to draw line to the point (x2, y2). Cubic Bezier (C) means to draw cubic Bezier curve with control points (qx1, qy1), (qx2, qy2) and end-point (x2, y2). Elliptical Arc (A) means to draw an elliptical arc with radii rx and ry (semi-major and semi-minor axes), rotated by angle φ to the x-axis, and end-point (x2, y2). And the ClosePath (Z) means to close the path. The visualization of the commands is shown in Tab. 2. However, these commands are only used to draw line strokes without color. To fill the gap in the SVG fill attribute gap, we assign distinctive numbers to the hex form of the SVG Fill (F) attribute, distinguishing it from the original SVG commands and coordinates. This command means to fill the color of each path, which contains essential SVG commands and coordinates. To this end, we are able to use total six types of commands {M, L, C, A, Z, F} to parameterize colored SVG parameterization. Model Architecture. OmniSVG leverages the Qwen2.5VL architecture, vision-language models (VLMs), which excels in processing both visual and textual inputs, enabling the generation of precise and compact SVG outputs. OmniSVG is trained for text-to-SVG, image-to-SVG, and character-reference SVG generation task. As depicted in Fig. 2, We first tokenize and embed the interleaved text and image inputs as prefix tokens. Then, we tokenize the SVG scripts into sequence and concatenate it to the end of the prefix token with the SVG tokenizer. The full sequence is then served as the input of the decoder-only language model. Specifically, our SVG tokenizer transforms SVG scripts Xs into an ordered SVG token sequence within the same representation space as the pre-trained VLM. Following IconShop [52], we flatten the layered structure of the SVG script by concatenating different paths into single command sequence, where each path begins with the drawing commands followed by point coordinates. Therefore, each SVG sequence could be represented as flattened sequence. As the generation identifier, we apply special tokens like <SOP> and <EOS> to the two ends of SVG sequence, identifying the begining and ending of SVG sequence. We assign special tokens for each command type, i.e. {M, L, C, A, Z, F}. To shorten the length of the SVG sequence, we further merge the point coordinates into one 5 (a) Training PPL for our models. (b) Validation PPL for our models. Figure 3. Training and Validation Perplexity (PPL) for OmniSVG Models. We train all the models from scratch on 250 billion tokens. We observe that the performance grows with model sizes. token with mapping function: < x, > + y, where is the width of the image. The SVG sequence are then lifted into the same embedding space as the pre-trained VLM with learnable embedding layer. Training Objective. Similar to LLMs, we train the model to generate new tokens conditioned on the provided prefix tokens with the next-token prediction loss. θ = arg max θ (cid:89) i=1 (xs,i xs,<i, xc) (1) Scaling Up. To study the effectiveness of scaling up multimodal SVG generation, we scale up OmniSVG from 3B to 7B parameters. We present training perplexity in Fig. 3, where both models are trained from scratch on 250 billion tokens. We show that, as the size of the model grows, the model achieves lower validation perplexity, indicating higher probability of producing the validation data. 5. Experiments To validate the effectiveness of our method, we first introduce the baselines and implementation details (Sec. 5.1). Then, we make quantitative comparisons with prior arts (Sec. 5.2) and ablation studies (Sec. 5.3) to study the effectiveness of our design. 5.1. Baselines For the text-to-SVG task, we compare our method with language-based (LLM-based) methods, including VectorFusion [19], SVGDreamer [56], Chat2SVG [53] and IconShop [52]. For image-to-SVG task, we compare our method with baseline methods across image vectorization and Multimodal Large Language Modeling approaches, including LIVE [30], DiffVG [25], StarVector [37] and GPT-4o [18] using the official implementations with the hyperparameters proposed by the authors, and apply their preand postprocessing code as required. Specifically, for the text-to-SVG task, the optimizationbased method SVGDreamer excels in enhancing editability Table 3. Quantitative Evaluations. Quantitative results between OmniSVG and current state-of-the-art text-to-SVG and image-to-SVG baseline methods. The bold numbers and underlined numbers represents the best and second best performance repectively. Our OmniSVG model demonstrates superior performance compared SOTA SVG generation baselines. StarVector(8B) fails to generate complex SVGs when evaluated in the complex MMSVG-Character dataset."
        },
        {
            "title": "Time",
            "content": "# Tokens Text-to-SVG Image-to-SVG FID CLIP Aesthetic HPS DINO SSIM LPIPS MSE MMSVG-Icon MMSVG-Illustration MMSVG-Character Vectorfusion [19] 69s SVGDreamer [56] 260s Chat2SVG [53] IconShop [52] LIVE [30] DiffVG [25] GPT-4o [18] StarVector(8B) [37] OmniSVG(3B) OmniSVG(7B) Vectorfusion [19] 28s 6.8s 170s 21s 6.9s 45s 15s 23s 70s SVGDreamer [56] 260s Chat2SVG [53] IconShop [52] LIVE [30] DiffVG [25] GPT-4o [18] StarVector(8B) [37] OmniSVG(3B) OmniSVG(7B) Vectorfusion [19] 35s 7.8s 250s 31s 8.0s 65s 49s 63s 69s SVGDreamer [56] 260s Chat2SVG [53] IconShop [52] LIVE [30] DiffVG [25] GPT-4o [18] 42s 6.8s 190s 21s 8.1s StarVector(8B) [37] 40k 100k 1.7k 3.3k 18.2k 19.8k 0.6k 3.5k 3.8k 4.6k 41k 102k 2.1k 3.3k 18.2k 19.8k 0.7k 5.3k 9.7k 88.74 0.2798 75.31 0.2923 70. 0.3029 80.84 0.2359 64. 0.3194 62.16 0.3278 90.03 0.2639 83. 0.2795 78.84 0.2891 84.04 0.2198 70.45 0.3077 10.9k 66.91 0. 40k 100k 1.7k 3.3k 18.2k 19.8k 0.7k 93.03 0.2218 86.01 0. 82.98 0.2499 88.41 0.2001 4.98 5.32 5.38 3.47 5.49 5.63 4.67 5. 5.01 3.21 5.39 5.59 4.08 4.81 4.83 3. 0.225 0. 0.256 0.138 0.247 0.258 0.214 0.219 0.221 0. 0.245 0. 0.198 0.205 0.209 0.077 OmniSVG(3B) OmniSVG(7B) 105s 139s 30.8k 72.18 0.2898 35k 68.02 0. 5.19 5.33 0.213 0.238 0.960 0.951 0.887 0. 0.980 0.984 0.959 0.929 0.801 0.807 0.974 0. 0.911 0. 0.712 0.921 0.932 0.979 0.956 0.826 0. 0.954 0.973 0.960 0.930 0.783 0.831 0.944 0. 0.939 0. 0.693 0.917 0.941 0.034 0.056 0.179 0. 0.049 0.037 0.044 0.077 0.244 0.248 0.069 0. 0.109 0. 0.365 0.049 0.072 0.004 0.015 0.134 0. 0.011 0.008 0.011 0.021 0.199 0.181 0.019 0. 0.038 0. 0.349 0.021 0.008 by employing semantic-driven image vectorization process that effectively separates foreground objects from the background, while failing to handle complex scenes. Another optimization-based work, VectorFusion, stands out for generating SVG-exportable vector graphics without relying on large captioned datasets. However, Vectorfusion is also unable to handle complex scenarios and diverse styles. The significant problem with these optimization-based works is that the optimization time is too long. Generating an SVG usually takes more than ten minutes, which is too expensive. For the LLM-based method, Chat2SVG integrates Large Language Models (LLMs) with image diffusion models to create semantically rich SVG templates. However, Chat2SVG still needs to optimize the output SVG script from LLM, which introduces increased computational complexity and poses challenges during model training. In comparison, IconShop utilizes transformer-based architecture to autoregressively model SVG path sequences, demonstrating exceptional performance in simplified icon SVGs, which offers effective solutions for text-to-SVG generation. It can only generate black simple Icon SVGs. For the image-to-SVG task, we compare our method 6 Figure 4. Qualitative comparison with SOTA methods on Text-to-SVG task. We compare the propose method with SOTA Text-to-SVG methods on our evaluation benchmarks, namely Icon, Illustration and Character. The proposed method outperforms existing state-of-the-art approaches in both instruction-following and the aesthetic quality of the generated SVGs. with the image vectorization methods. LIVE allows progressive and efficient generation of SVGs, optimizing closed vector paths under raster image supervision with shape complexity control. However, LIVE needs to optimize for long time when generating complex SVGs. DiffVG enables end-to-end differentiability in vector graphics rasterization, improving optimization through anti-aliasing and gradient-based methods while also is computationally expensive due to the complexity of the forward-backward rasterization process. Recently, the Multimodal Large Language Model (MLLM) based method StarVector leverages the visual understanding to apply accurate SVG primitive to the LLM architecture, which also can generate SVGs from both text and image inputs. However, it still fails to generate complex SVGs. Since Starvector [37] has not yet opened up its text-to-SVG model weights, our MMSVGBench does not evaluate Starvectors text-to-SVG capabilities. MMSVG-Bench also evaluates our methods with VLM methods, GPT-4o, to conduct comprehensive assessment. We compare our method with these baselines on our MMSVG-2M dataset, from simple MMSVG-Icon datset, bit complex MMSVG-illustration dataset, to the very complex MMSVG-Character dataset. We report the quantitative results in Tab. 3 and the qualitative results in Fig. 4 and Fig. 5. 5.2. Evaluations and Comparisons 5.2.1. Quantitative Evaluations We report the evaluation metrics of our OmniSVG and baselines on our MMSVG-2M dataset, including the MMSVGIcon, MMSVG-Illustration and MMSVG-Character in Tab. 3, including both text-to-SVG and image-to-SVG tasks. As shown in Tab. 3, OmniSVG consistently outperforms all other text-to-SVG baselines across all evaluation metrics, indicating lower FID values, higher CLIP scores, higher aesthetic scores and higher HPS scores. For the image-to-SVG tasks, while image vectorization method LIVE achieves higher SSIM, lower LPIPS and lower MSE scores, our OmniSVG can achieve higher DINO scores, indicating the higher semantic features between the input im7 Figure 5. Qualitative comparison with SOTA methods on Image-to-SVG task. We compare the propose method with SOTA Imageto-SVG methods on our evaluation benchmarks. Despite generating plausible results on simple icon samples, optimization based methods like DiffVG [25] and LIVE [30] tend to output artifacts on complex images. GPT-4o [18] is only able to generate icon-level SVG even given the complex input image. StarVector [37] is able to generate SVG for the input icon image. However, when inputed the illustration or more complex character image, StarVector fails to generate SVGs. Please zoom-in for more details. age and the generated SVGs. 5.2.2. Qualitative Evaluations Text-to-SVG task As shown in Fig. 5, we compare the performance of our method with several baseline approaches using seven distinct text prompt instructions for the textto-SVG task. For optimization-based methods, such as SVGDreamer [56] and VectorFusion [19], generating an SVG from single prompt requires significant computation time. These methods rely on iterative optimization processes, which, while effective for refining SVG details, are computationally expensive and time-consuming. In contrast, language-based methods like IconShop [52] and Chat2SVG [53] offer faster generation times, as they leverage pre-trained models to generate SVG templates more efficiently. However, each of these methods comes with notable limitations. IconShop, despite producing complete SVG shapes, is restricted to generating black-and-white graphics, which limits its applicability in scenarios requiring color and visual richness. Chat2SVG, although more 8 Figure 6. Generated SVG with Character-Reference (CRef) by OmniSVG. By training on MMSVG-Character with natural character image and SVG pair data, OmniSVG is capable of generating character SVGs through image references. flexible, is two-stage process. In this section, we focus on the first stage of Chat2SVG, which is responsible for generating an initial SVG template. While it offers some flexibility, the generated SVGs often lack the level of detail and semantic consistency observed in the outputs from our method. Our method, OmniSVG, consistently outperforms all other baselines across various text prompts. The generated SVGs not only retain high fidelity to the input instructions but also incorporate rich color and geometric accuracy. Additionally, OmniSVG demonstrates the ability to handle more complex visual cues, enabling the generation of both simple and intricate SVG designs. This qualitative evaluation highlights the superior performance of our approach in terms of both speed and the quality of the resulting SVGs, making it more effective and versatile solution compared to existing methods. Image-to-SVG Task. For the image-to-SVG task, we compare our method with classical image vectorization approaches, including DiffVG [25], LIVE [30], VLM-based methods GPT-4o [18] and StarVector [37]. As shown in Fig. 5, our method outperforms these baselines in both the quality and efficiency of SVG generation. We present comprehensive qualitative comparison of our proposed method against state-of-the-art (SOTA) Image-to-SVG techniques, evaluated on our established benchmark datasets. The methods under consideration include optimization-based approaches such as DiffVG [25] and LIVE [30], as well as neural-based methods like GPT4o [18] and StarVector [37]. While image vectorization methods such as DiffVG and LIVE demonstrate impressive results when applied to simple icon images, they exhibit notable deficiencies when tasked with more complex images. These methods tend to generate visual artifacts, indicating that they struggle with preserving the integrity of intricate visual structures, particularly when the input images feature detailed shapes, gradients, or textures. The GPT-4o model, though capable of generating SVG outputs for complex input images, is constrained to producing only iconlevel SVGs. This limitation suggests that GPT-4o is primarily designed to handle simpler iconography and lacks the capacity to fully interpret and convert detailed illustrations into scalable vector graphics, which may be result of its underlying design or training data constraints. StarVector, another VLM-based method, demonstrates strong performance when tasked with generating SVGs from simple icon images. However, its effectiveness diminishes significantly when dealing with more complex inputs, such as illustrations or intricate character images. In these cases, StarVector fails to generate accurate or meaningful SVG outputs, highlighting its limited generalization ability for more diverse and intricate image types. While our OmniSVG are able to efficiently convert images into high-quality, editable SVGs, from the input icon images, illustration images to the complex character images, highlighting the superior performance in handling diverse and intricate visual cues, setting it apart from traditional vectorization approaches. We provide more visual results in Fig. 8. Character-Reference SVG generation task. As shown in Fig. 6, by training on MMSVG-Character with natural character image and SVG pair data, OmniSVG is capable of generating character SVGs through image references. Moreover, we assess the alignment between input character images and generated SVGs using the third-party VLMs assistant GPT-4o [18]. Our findings reveal an average score of approximately 7, suggesting that the generated IP SVGs exhibit well degree of similarity to the input reference images. 5.3. Ablation studies We conduct ablation studies on the SVG parameterization, the scaling up model size and the VLM architecture to validate the effectivenss of our design. Most of our ablation experiments are performed on the OmniSVG-3B model for faster iteration, and we empirically find they work well on the color unparameterized, not parameterizing either the coordinates or the color, and parameterizing both the coordinates and the color (our method). The results of these experiments on the MMSVG-Illustration dataset are summarized in the Tab. 4 and Fig. 7. The results in Tab. 4 indicate that incorporating both coordinate and color parameterizations yields the most favorable outcomes across all evaluated metrics. Notably, this configuration achieves the lowest token length, suggesting that our method efficiently utilizes token space. This efficiency implies that our approach can generate complex SVGs with reduced computational resources, which is advantageous for applications requiring scalability and efficiency. Moreover, the qualitative results in Fig. 7 indicates that the both coordinate and color parameterizations significantly outperforms other methods, particularly as SVG complexity increases. No parameterizations method fail to generate SVGs when given the complex image. These experiments underscore the effectiveness of our full parameterization strategy in balancing performance and resource utilization, making it compelling choice for SVG generation tasks. Ablation studies on the model size. To analyze whether training larger model benefits SVG generation, we evaluate OmniSVG base models with different sizes on the MMSVG-2M dataset in Tab. 5. We evaluate OmniSVG with base models of varying sizes on the MMSVG-2M dataset in Tab. 5. We progressively scale up the model size from the FLAN-T5-base [8] (223M) to Qwen2.5-VL-3BInstruct (3.7B) and Qwen2.5-VL-7B-Instruct (8.3B). We observe that as the model size grows, the quality of the generated samples gets higher, indicating the effectiveness of scaling up generative auto-regressive training on multimodal SVG generation tasks. Table 5. Ablation of the model size. As the model size grows, the generated samples are of higher quality. Methods Input Size Text-to-SVG FID CLIP Aesthetic HPS DINO FLAN-T5-Base[8] FLAN-T5-Large[8] FLAN-T5-xl[8] blip2-flan-t5-xl[24] OmniSVG(3B) OmniSVG(7B) Text Text Text Text/Image Text/Image Text/Image 3B 223M 89.30 770M 83.92 76.92 3.94B 75.23 70.45 3.7B 66.91 8.3B 0.177 0.2303 0.2789 0.2881 0.3077 0.3164 3.437 4.102 4.642 4.712 5.395 5.59 0.102 0.177 0.217 0.201 0.245 0.253 0.891 0.974 0.988 Image-to-SVG SSIM LPIS MSE 0.902 0.944 0. 0.099 0.069 0.041 0.046 0.019 0.013 Ablation studies on the VLM architecture. To illustrate the effectiveness of the VLM architecture, we conducte an ablation study by replacing the VLM architecture with alternative LLM-based architectures, incorporating image encoders such as CLIP ViT-B/32 [33], VQGAN [12], and the Qwen2.5-VL [1] model used in OmniSVG. These experiments were carried out on the MMSVG-Illustration dataset, with focus on comparing the performance of different architectures in generating SVG outputs. Among these architectures, Qwen2.5-VL consistently delivered superior results across all evaluation metrics. Figure 7. Qualitative study on parametrization. Ablation studies on color parametrization (abbreviated as param.) and coordinate (abbreviated ad coord.) paramterization are conducted. the larger OmniSVG-7B. Effectiveness of the SVG Parameterization. In this experimental study, we present comprehensive comparison between SVG parameterization approach and traditional nonparameterized methods for SVG representation in large language models. The fundamental distinction lies in how coordinate information is tokenized: traditional methods represent each digit as separate token (e.g., coordinates (123, 456) require six tokens plus delimiters), whereas our parameterization method encodes entire coordinates as single semantic tokens, dramatically improving the models ability to process complex path information. We parameterize both the coordinate and color attributes of the SVG. To validate the effectiveness of our experimental design, we conducted the following ablation experiments: parameterizing color while leaving the coordinates unparameterized, parameterizing coordinates while leaving Table 4. Quantitative study on SVG parameterization. Ablation studies on color parametrization (abbreviated as color param.) and coordinate paramterization (abbreviated as coord param.) are conducted. Text-to-SVG Image-to-SVG Methods w/o param. FID CLIP Aesthetic HPS DINO SSIM LPIPS MSE 0.232 0.112 0.702 97.01 0.1537 0.160 0.156 0.783 w/o coordinate param. 87.93 0.2193 0.028 0.201 0.901 76.39 0.2632 0.019 0.245 0.974 70.45 0. w/o color param. full param. (Ours) 0.668 0.769 0.912 0.944 0.334 0.295 0.098 0.069 2.873 3.352 4.234 5.39 # Tokens 35k 29.5k 13.1k 9.7k 10 Figure 8. Illustration of the SVG generation capabilities of OmniSVG. 11 The results in Tab. 6 clearly indicate that none of the alternative architectures outperformed Qwen2.5-VL, highlighting the importance of selecting an architecture with robust image encoder. Additionally, key advantage of Qwen2.5-VL is its support for multi-image input, which enables the generation of more complex outputs, such as character reference SVGs. Table 6. Ablation of the VLM architecture. we ablate the VLM architecture by replacing it with the LLM architecture with the image encoder, including the CLIP ViT-B/32 [33], VQGAN [12], and the Qwen2.5-VL [1] we use in OmniSVG. We experiment with training experiments on the MMSVG-Illustration dataset. Qwen2.5-VL architecture gives the best results on all metrics. Vision Language Text-to-SVG FID CLIP Aesthetic HPS DINO Image-to-SVG SSIM LPIPS MSE Qwen2.5 Qwen2. CLIP VQGAN Qwen2.5-VL-3B-Instruct Qwen2.5-VL-7B-Instruct 72.243 76.29 70.45 66.91 0.2876 0.2739 0.3077 0.3164 4.57 4.62 5.39 5.59 0.209 0.198 0.245 0.253 0.881 0.890 0.974 0. 0.872 0.852 0.944 0.959 0.176 0.183 0.069 0.041 0.062 0.065 0.019 0.013 5.4. User study and Implementation Details. User Study. For user study, we assess the user preference, vividity, and text-SVG/image-SVG alignment of the generated SVGs. In this study, we conduct user study to evaluate the effectiveness and practicality of the SVGs generated by our method and baselines. Specifically, We randomly sample 5 text descriptions and 3 image prompts from the evaluation dataset and generate corresponding SVGs using our methods and baseline models. Along with the original 8 SVGs from the dataset, this provided total of 64 SVGs for the user study. In Tab. 7, preference indicates the generated SVG preference, Vividlity indicates the sample is more lively, and Alignment represents the input text/image instructions more likely to align with the generated SVGs. In this study, we shuffle the SVGs generated by the different models together with the selected examples from our dataset. Participants are only provided with the text descriptions or image prompts and the shuffled SVGs. Tab. 7 also demonstrates that OmniSVG is preferred by volunteers, who perceive the generated SVGs as more vivid and visually aligned with the input text or image. This alignment highlights OmniSVGs superior ability to produce more accurate and contextually relevant SVG content according to Table 7. User Study of OmniSVG and baselines. Our OmniSVG achieves the user preference among the baselines. Method Vectorfusion [19] SVGDreamer [56] Chat2SVG [53] IconShop [52] GPT-4o [18] StarVector(8B) [37] DiffVG [25] LIVE [30] OmniSVG [30] Preference Vividity Alignment 60 70 60 60 60 80 80 70 90 40 40 60 80 40 40 90 90 100 80 80 90 80 80 70 100 100 100 Figure 9. Limitation of OmniSVG on Image-to-SVG Task. OmniSVG can successfully generate vector style images, while fail to fit natural images. volunteers feedback. Implementation Details. We train our models in bfloat16 with the ZeRO-2 strategy [35] for memory-efficient training. We also adopt the AdamW [29] optimizer with learning rate decaying from 3 104 to 3 106 and weight decay of 0.1 to train our model. In practice, we load the pretrained weights from the Qwen2.5-VL [1] model and initialize the SVG embeddings from scratch. Without further specification, we generate SVGs with the top-k and top-p sampling strategy with = 50 and = 0.95 for diversity. 6. Conclusions Conclusions. We introduce OmniSVG, unified framework for multimodal SVG generation that leverages pretrained Vision-Language Models (VLMs). By parameterizing SVG commands and coordinates as discrete tokens, OmniSVG efficiently decouples structural logic from geometry, addressing issues like coordinate hallucination while maintaining design expressiveness. Our method outperforms existing approaches in both quality and efficiency, offering high-quality, editable SVG across various design domains. Additionally, we proposed MMSVG-2M, largescale multimodal dataset with two million annotated SVG assets and standardized evaluation protocol. Extensive experiments show that OmniSVG surpasses prior SVG generation methods in various conditional generation tasks, highlighting its potential for integration into professional SVG design workflows. Limitations and Future Work. During inference, OmniSVG generates tens of thousands of tokens for complex samples, which inevitably leads to considerable generation time. OmniSVG is only bounded by vector style image prompt and fails on natural images as illustrated in Fig. ??. As for future work, recent endeavors on multi-token prediction [2, 13] and KV-cache compression [3, 63] provide promising way to save the generation cost. Additionally, the auto-regressive nature of OmniSVG also unlocks future opportunities for in-context learning [43, 62, 64], chain-ofthought reasoning [14, 51], and multi-turn interleaved generation [17, 27], thereby providing more dexterous user control."
        },
        {
            "title": "Appendix",
            "content": "A. Additional Details of MMSVG-2M dataset A.1. Samples of MMSVG-2M Dataset in We visualize samples of our MMSVG-2M dataset In our MMSVG-2M dataset, 55% of the SVG Fig. 10. samples belongs to the MMSVG-Icon, 25% belongs to the MMSVG-Illustration, and the rest 20% belongs to the MMSVG-Character. Among the SVG samples within the MMSVG-Character category, half of them comes from Freepik, while another half is generated by our data creation pipeline. We also collect image-SVG pairs for the character-reference SVG generation tasks during the generation process. A.2. SVG-Image-Text Pairs Construction Our MMSVG-2M dataset comprises two million SVG samples with the corresponding rasterized images. We generate captions on the rasterized images with BLIP-2 [24], thereby providing textual descriptions that enable us to fine-tune our model to follow these instructions. We use CairoSVG [21] for rasterization and remove samples that produced completely white images. Annotation. We employ an off-the-shelf VLM, specifically BLIP-2 [24], to generate SVG captions with the prompt below. To reduce hallucinations, we drop the samples with CLIP scores less than 30. We also visualize the distribution annotated keywords of MMSVG-2M dataset in Fig. 12 with word cloud format. And the instruction template for annotation is shown in Tab. 8. Instruction templates. MMSVGBench provides three tasks, including text-to-SVG task, image-to-SVG task and character-reference SVG generation task. Each task needs different instruction templates. For the text and image conditioning SVG generation, we provide the input text or image with VLM architecture. For character-reference SVG generation, we provide the natural charecter reference image and the original image with the VLM architecture. The list of instruction templates for different tasks are shown in Tab. 8. A.3. Character-SVG Pairs Construction As illustrated in the Fig. 1, part of our proposed MMSVGis constructed using generative 2M-Character subset pipeline. As shown in the pipeline diagram in Fig. 2, we employ FLUX [22]-based generative model enhanced with vector-style LoRA to enable the generation of SVGstyle data. For image-based conditioning, we adopt FLUXRedux [23], which injects image features via SigLIP encoder and projects them into image embeddings. These emEmployed BLIP2 for SVG Captioning: You are helpful assistant. Your task is to describe this image in single sentence, including the object, its color, and its overall arrangement. For example: Yellow cheers with glasses of alcohol drinks. / Heart emojis represent love on Valentines Day. Text-to-SVG: You are helpful SVG Generation assistant, designed to generate SVG. We provide the text description as input, generate SVG based on the text. Image-to-SVG: You are helpful SVG Generation assistant, designed to generate SVG. We provide an image as input, generate SVG for this image. Character-Reference SVG Generation: You are helpful SVG Generation assistant, designed to generate SVG. We provide two image as input, the second image is the character reference image of the first image, generate the character reference SVG based on these two input images. Table 8. The list of instructions for different tasks, including annotation, text-to-SVG, image-to-SVG and character-reference SVG generation. beddings are then concatenated with the text tokens as conditioning inputs for FLUX [22]. However, in practice, the original Redux [23] conditioning proves to be overly strong. To address this, we adopt community-implemented variant of Redux that downsamples the image embeddings in 2D space. As observed in our experiments shown in Fig. 11, downsampling factor between 2 and 3 yields the most reasonable SVG-style character references. Finally, we employ VTracer [10] to perform near-instant vectorization of the generated images. To construct the MMSVG-2MCharacter subset, we first filter 103k character instances from the Danbooru [11] dataset and apply the aforementioned pipeline. We compare the raw FLUX [22] outputs and their vectorized counterparts, retaining only those samples with PSNR and SSIM scores above certain threshold as valid data. B. More details of the baselines B.1. Text-to-SVG Task SVGDreamer [56] uses semantic-driven image vectorization (SIVE) process to separate foreground objects and background, improving editability. The SIVE process utilizes attention-based primitive control and an attentionmask loss function to manipulate individual elements effectively. To address issues in existing text-to-SVG generation methods, the proposed Vectorized Particle-based Score Distillation (VPSD) approach models SVGs as distributions of control points and colors, improving shape, color diversity, 13 Figure 10. Samples from MMSVG-2M dataset. The proposed MMSVG-2M dataset can be separated into three subset, namely Icon, Illustration and Character. Samples from Icon, Illustration and part of Character subsets are downloaded from Internet. Another part of Character subset is generated by our data creation pipeline, which can provide image and SVG pairs for image prompting task. fidelity, offering variety of styles such as pixel art and sketches. Chat2SVG [53] proposes hybrid framework that combines the strengths of Large Language Models (LLMs) and image diffusion models for text-to-SVG generation. The approach first uses an LLM to create semantically meaningful SVG templates from basic geometric primitives. dual-stage optimization pipeline, guided by image diffusion models, refines paths in latent space and adjusts point coordinates to enhance geometric complexity. IconShop [52] uses transformer-based architecture to encode path commands and learn to model SVG path sequences autoregressively. It has shown excellent results in simplified icon scenarios and provides good solution to Text-to-SVG generation by extending the FIGR-8-SVG dataset with captions. We have access to their dataset and original splits and have trained our model on that data using pre-trained checkpoint (trained on OmniVG dataset). We have extracted the results from IconShop and included them here to compare our method. LLM4SVG [55] is framework that leverages Large Language Models (LLMs) to understand and generate Scalable Vector Graphics (SVGs). It employs structured SVG enFigure 11. Image prompting dataset creation of MMSVG-2M Character. By utilizing FLUX-Redux and SVG vectorization tools, image prompting data pairs can be generated. We adpot FLUX-Redux downsampling scale with 2, 3 in practice by tradingoff the character similarity and complexity of generated SVG. and convergence speed. VectorFusion [19] leverages text-conditioned diffusion model trained on pixel representations to generate SVG exportable vector graphics without needing large captioned SVG datasets. By optimizing differentiable vector graphics rasterizer, it distills semantic knowledge from pretrained diffusion model and uses Score Distillation Sampling to generate an SVG consistent with caption. Experiments show that VectorFusion improves both quality and 14 images and the backward pass computes gradients with respect to vector graphic parameters. StarVector [37] works directly in the SVG code space, leveraging visual understanding to apply accurate SVG primitives. StarVector employs transformer-based architecture that integrates an image encoder with language model, enabling it to process visual inputs and produce precise SVG code. StarVector effectively handles diverse SVG types, including icons, logos, and complex diagrams, demonstrating robust generalization across various vectorization tasks. However, with 16k token context window, StarVector may struggle to process highly complex SVGs that require longer sequences. Figure 12. Word cloud visualization of label distribution in the MMSVG-2M dataset. The size of each label corresponds to its frequency of occurrence. The larger the label, the more frequently it appears in the dataset. coding approach, utilizing learnable semantic tokens to accurately represent SVG components and their properties. This design enables LLMs to produce SVGs that are both semantically aligned with textual descriptions and visually coherent. However, LLM4SVG also has maximum token length of 2048, limiting its ability to generate highly complex SVGs that require longer sequences. B.2. Image-to-SVG Task LIVE (Layer-wise Image Vectorization) [30] is method for progressively generating SVGs that closely fit given raster image by recursively adding and optimizing closed vector paths. Using differentiable renderer (based on DiffVG [25]), LIVE enables direct optimization of paths under raster image supervision while controlling shape complexity by adjusting the number of path segments. It introduces component-wise path initialization, identifying key visual components to ensure efficient topology extraction and minimize redundant shapes. DiffVG [25] is landmark in vector graphics research, pioneering deep learning-based methods with the first differentiable vector graphics rasterization pipeline. By leveraging combination of anti-aliasing techniques and gradientbased optimization, DiffVG ensures differentiability. Unlike methods relying on non-differentiable curve-to-mesh conversions, DiffVG employs forward-backward rasterization process, where the forward pass generates antialiased"
        },
        {
            "title": "References",
            "content": "[1] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. 4, 10, 12 [2] Tianle Cai, Yuhong Li, Zhengyang Geng, Hongwu Peng, and Tri Dao. Medusa: Simple framework for accelerating llm generation with multiple decoding heads. Retrieved December, 2023. 12 [3] Zefan Cai, Yichi Zhang, Bofei Gao, Yuliang Liu, Tianyu Liu, Keming Lu, Wayne Xiong, Yue Dong, Baobao Chang, Junjie Hu, et al. Pyramidkv: Dynamic kv cache compression based on pyramidal information funneling. arXiv preprint arXiv:2406.02069, 2024. 12 [4] Alexandre Carlier, Martin Danelljan, Alexandre Alahi, and Radu Timofte. Deepsvg: hierarchical generative network for vector graphics animation. NeurIPS, 2020. 2 [5] Sijin Chen, Xin Chen, Anqi Pang, Xianfang Zeng, Wei Cheng, Yijun Fu, Fukun Yin, Billzb Wang, Jingyi Yu, Gang Yu, et al. Meshxl: Neural coordinate field for generative 3d foundation models. NeurIPS, 2024. 2 [6] Sijin Chen, Xin Chen, Chi Zhang, Mingsheng Li, Gang Yu, Hao Fei, Hongyuan Zhu, Jiayuan Fan, and Tao Chen. Ll3da: Visual interactive instruction tuning for omni-3d understanding reasoning and planning. In CVPR, 2024. [7] Zehao Chen and Rong Pan. Svgbuilder: Component-based colored svg generation with text-guided autoregressive transformers. arXiv preprint arXiv:2412.10488, 2024. 2 [8] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instructionfinetuned language models. JMLR, 2024. 10 [9] Louis Clouˆatre and Marc Demers. Figr: Few-shot image generation with reptile. arXiv preprint arXiv:1901.02199, 2019. 2, 3 [10] Vision Cortex. Vtracer. visioncortex.org/vtracer-docs, 2023. 13 [11] Nyanko Devs. Danbooru2023: large-scale crowdsourced and tagged anime illustration dataset. Hugging Face, 2023. 13 https : / / www . [12] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis. In CVPR, 2021. 10, [13] Fabian Gloeckle, Badr Youbi Idrissi, Baptiste Rozi`ere, David Lopez-Paz, and Gabriel Synnaeve. Better & faster large language models via multi-token prediction. arXiv preprint arXiv:2404.19737, 2024. 12 [14] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. 12 [15] David Ha and Douglas Eck. neural representation of sketch drawings. In ICLR, 2018. 2 [16] Teng Hu, Ran Yi, Baihong Qian, Jiangning Zhang, Paul Rosin, and Yu-Kun Lai. Supersvg: Superpixel-based scalable vector graphics synthesis. In CVPR, 2024. 2 [17] Minbin Huang, Yanxin Long, Xinchi Deng, Ruihang Chu, Jiangfeng Xiong, Xiaodan Liang, Hong Cheng, Qinglin Lu, and Wei Liu. Dialoggen: Multi-modal interactive dialogue system for multi-turn text-to-image generation. arXiv preprint arXiv:2403.08857, 2024. 12 [18] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. 4, 5, 6, 8, 9, 12 [19] Ajay Jain, Amber Xie, and Pieter Abbeel. Vectorfusion: Text-to-svg by abstracting pixel-based diffusion models. In CVPR, 2023. 2, 5, 6, 8, 12, 14 [20] Denis Kocetkov, Raymond Li, Loubna Ben Allal, Jia Li, Chenghao Mou, Carlos Munoz Ferrandis, Yacine Jernite, Margaret Mitchell, Sean Hughes, Thomas Wolf, et al. The stack: 3 tb of permissively licensed source code. arXiv preprint arXiv:2211.15533, 2022. 2 [21] Kozea. Cairosvg. https://cairosvg.org/, 2023. 13 [22] Black Forest Labs. Flux. https://github.com/ [23] Black Forest Labs. black-forest-labs/flux, 2024. 13 Flux.1-redux-dev. https : / / huggingface . co / black - forest - labs / FLUX . 1-Redux-dev, 2024. 13 [24] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In ICML, 2023. 3, 10, 13 [25] Tzu-Mao Li, Michal Lukaˇc, Gharbi Michael, and Jonathan Ragan-Kelley. Differentiable vector graphics rasterization for editing and learning. SIGGRAPH Asia, 2020. 2, 5, 6, 8, 9, 12, 15 [26] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In NeurIPS, 2023. 2 [27] Ziyu Liu, Tao Chu, Yuhang Zang, Xilin Wei, Xiaoyi Dong, Pan Zhang, Zijian Liang, Yuanjun Xiong, Yu Qiao, Dahua Lin, et al. Mmdu: multi-turn multi-image dialog understanding benchmark and instruction-tuning dataset for lvlms. arXiv preprint arXiv:2406.11833, 2024. [28] Raphael Gontijo Lopes, David Ha, Douglas Eck, and Jonathon Shlens. learned representation for scalable vector graphics. In CVPR, 2019. 2 [29] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. 12 [30] Xu Ma, Yuqian Zhou, Xingqian Xu, Bin Sun, Valerii Filev, Nikita Orlov, Yun Fu, and Humphrey Shi. Towards layerwise image vectorization. In CVPR, 2022. 2, 5, 6, 8, 9, 12, 15 [31] Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. 4 [32] Ben Poole, Ajay Jain, Jonathan Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d diffusion. In ICLR, 2023. [33] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In ICML, 2021. 10, 12 [34] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In ICML, 2021. 4 [35] Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: Memory optimizations toward training In SC20: International Confertrillion parameter models. ence for High Performance Computing, Networking, Storage and Analysis. IEEE, 2020. 12 [36] Pradyumna Reddy, Michael Gharbi, Michal Lukac, and Niloy Mitra. Im2vec: Synthesizing vector graphics without vector supervision. In CVPR, 2021. 2 [37] Juan Rodriguez, Shubham Agarwal, Issam Laradji, Pau Rodriguez, David Vazquez, Christopher Pal, and Marco Pedersoli. Starvector: Generating scalable vector graphics code from images. arXiv preprint arXiv:2312.11556, 2023. 2, 3, 5, 6, 7, 8, 9, 12, 15 [38] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In CVPR, 2022. [39] Christoph Schuhmann. Improved aesthetic predictor. https : / / github . com / christophschuhmann / improved-aesthetic-predictor, 2022. 4 [40] I-Chao Shen and Bing-Yu Chen. Clipgen: deep generative model for clipart vectorization and synthesis. TVCG, 2022. 2 [41] Yiren Song, Xuning Shao, Kang Chen, Weidong Zhang, Zhongliang Jing, and Minzhe Li. Clipvg: Text-guided image manipulation using differentiable vector graphics. In AAAI, 2023. 2 [42] Hao Su, Xuefeng Liu, Jianwei Niu, Jiahe Cui, Ji Wan, Xinghao Wu, and Nana Wang. Marvel: Raster gray-level manga vectorization via primitive-wise deep reinforcement learning. TCSVT, 2023. 2 [43] Quan Sun, Yufeng Cui, Xiaosong Zhang, Fan Zhang, Qiying Yu, Yueze Wang, Yongming Rao, Jingjing Liu, Tiejun Huang, and Xinlong Wang. Generative multimodal models are in-context learners. In CVPR, 2024. [44] Zecheng Tang, Chenfei Wu, Zekai Zhang, Mingheng Ni, Shengming Yin, Yu Liu, Zhengyuan Yang, Lijuan Wang, Strokenuwa: TokenizZicheng Liu, Juntao Li, et al. arXiv preprint ing strokes for vector graphic synthesis. arXiv:2401.17093, 2024. 2 [45] Zecheng Tang, Chenfei Wu, Zekai Zhang, Mingheng Ni, Shengming Yin, Yu Liu, Zhengyuan Yang, Lijuan Wang, Strokenuwa: TokenizZicheng Liu, Juntao Li, et al. arXiv preprint ing strokes for vector graphic synthesis. arXiv:2401.17093, 2024. 2 [46] Lucas Theis, Aaron van den Oord, and Matthias Bethge. note on the evaluation of generative models. arXiv preprint arXiv:1511.01844, 2015. 4 [47] Yingtao Tian and David Ha. Modern evolution strategies for creativity: Fitting concrete images and abstract concepts. In Artificial Intelligence in Music, Sound, Art and Design, 2022. 2 [48] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. 2 [49] Yizhi Wang and Zhouhui Lian. Deepvecfont: synthesizing high-quality vector fonts via dual-modality learning. TOG, 2021. [50] Zhou Wang, Alan Bovik, Hamid Sheikh, and Eero Simoncelli. Image quality assessment: from error visibility to structural similarity. TIP, 2004. 4 [51] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. NeurIPS, 2022. 12 [52] Ronghuan Wu, Wanchao Su, Kede Ma, and Jing Liao. Iconshop: Text-guided vector icon synthesis with autoregressive transformers. TOG, 2023. 2, 3, 5, 6, 8, 12, 14 [53] Ronghuan Wu, Wanchao Su, and Jing Liao. Chat2svg: Vector graphics generation with large language models and imarXiv preprint arXiv:2411.16602, age diffusion models. 2024. 2, 5, 6, 8, 12, 14 [54] Xiaoshi Wu, Keqiang Sun, Feng Zhu, Rui Zhao, and Hongsheng Li. Human preference score: Better aligning text-toimage models with human preference. In ICCV, 2023. 4 [55] Ximing Xing, Juncheng Hu, Guotao Liang, Jing Zhang, Dong Xu, and Qian Yu. Empowering llms to understand and generate complex vector graphics. arXiv preprint arXiv:2412.11102, 2024. 2, 14 [56] Ximing Xing, Haitao Zhou, Chuang Wang, Jing Zhang, Dong Xu, and Qian Yu. Svgdreamer: Text guided svg generation with diffusion model. In CVPR, 2024. 2, 5, 6, 8, 12, [57] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115, 2024. 2 [58] Yiying Yang, Fukun Yin, Wen Liu, Jiayuan Fan, Xin Chen, Gang Yu, and Tao Chen. Pm-inr: Prior-rich multi-modal In AAAI, implicit large-scale scene neural representation. 2024. [59] Fukun Yin, Xin Chen, Chi Zhang, Biao Jiang, Zibo Zhao, Wen Liu, Gang Yu, and Tao Chen. Shapegpt: 3d shape generation with unified multi-modal language model. TMM, 2025. [60] Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge Zhang, Guanwei Zhang, Heng Li, Jiangcheng Zhu, Jianqun Chen, Jing Chang, et al. Yi: Open foundation models by 01. ai. arXiv preprint arXiv:2403.04652, 2024. 2 [61] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In CVPR, 2018. 4 [62] Yuanhan Zhang, Kaiyang Zhou, and Ziwei Liu. What makes good examples for visual in-context learning? NeurIPS, 2023. 12 [63] Xiabin Zhou, Wenbin Wang, Minyan Zeng, Jiaxian Guo, Xuebo Liu, Li Shen, Min Zhang, and Liang Ding. Dynamickv: Task-aware adaptive kv cache compression for long context llms. arXiv preprint arXiv:2412.14838, 2024. 12 [64] Yucheng Zhou, Xiang Li, Qianning Wang, and Jianbing Shen. Visual in-context learning for large vision-language models. arXiv preprint arXiv:2402.11574, 2024. 12 [65] Bocheng Zou, Mu Cai, Jianrui Zhang, and Yong Jae Lee. Vgbench: comprehensive benchmark of vector graphics understanding and generation for large language models. In EMNLP, 2024."
        }
    ],
    "affiliations": []
}