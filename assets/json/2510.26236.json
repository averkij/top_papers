{
    "paper_title": "PHUMA: Physically-Grounded Humanoid Locomotion Dataset",
    "authors": [
        "Kyungmin Lee",
        "Sibeen Kim",
        "Minho Park",
        "Hyunseung Kim",
        "Dongyoon Hwang",
        "Hojoon Lee",
        "Jaegul Choo"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Motion imitation is a promising approach for humanoid locomotion, enabling agents to acquire humanlike behaviors. Existing methods typically rely on high-quality motion capture datasets such as AMASS, but these are scarce and expensive, limiting scalability and diversity. Recent studies attempt to scale data collection by converting large-scale internet videos, exemplified by Humanoid-X. However, they often introduce physical artifacts such as floating, penetration, and foot skating, which hinder stable imitation. In response, we introduce PHUMA, a Physically-grounded HUMAnoid locomotion dataset that leverages human video at scale, while addressing physical artifacts through careful data curation and physics-constrained retargeting. PHUMA enforces joint limits, ensures ground contact, and eliminates foot skating, producing motions that are both large-scale and physically reliable. We evaluated PHUMA in two sets of conditions: (i) imitation of unseen motion from self-recorded test videos and (ii) path following with pelvis-only guidance. In both cases, PHUMA-trained policies outperform Humanoid-X and AMASS, achieving significant gains in imitating diverse motions. The code is available at https://davian-robotics.github.io/PHUMA."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 3 ] . [ 1 6 3 2 6 2 . 0 1 5 2 : r PHUMA: PHYSICALLY-GROUNDED HUMANOID LOCOMOTION DATASET Kyungmin Lee1 Sibeen Kim1 Minho Park1 Hyunseung Kim1 Dongyoon Hwang1 Hojoon Lee1 Jaegul Choo1 1KAIST {kmlee, bioceo78}@kaist.ac.kr Figure 1: Physical reliability of Humanoid-X vs. PHUMA. Each column illustrates four failure modes: joint violation, floating, penetration, and skating. Humanoid-X (Mao et al., 2025) (top row) often exhibits these issues due to direct video-to-motion conversion, while PHUMA (bottom row) mitigates those violations through careful data curation and physically grounded retargeting."
        },
        {
            "title": "ABSTRACT",
            "content": "Motion imitation is promising approach for humanoid locomotion, enabling agents to acquire humanlike behaviors. Existing methods typically rely on highquality motion capture datasets such as AMASS, but these are scarce and expensive, limiting scalability and diversity. Recent studies attempt to scale data collection by converting large-scale internet videos, exemplified by Humanoid-X. However, they often introduce physical artifacts such as floating, penetration, and foot skating, which hinder stable imitation. In response, we introduce PHUMA, Physically-grounded HUMAnoid locomotion dataset that leverages human video at scale, while addressing physical artifacts through careful data curation and physics-constrained retargeting. PHUMA enforces joint limits, ensures ground contact, and eliminates foot skating, producing motions that are both large-scale and physically reliable. We evaluated PHUMA in two sets of conditions: (i) imitation of unseen motion from self-recorded test videos and (ii) path following with pelvis-only guidance. In both cases, PHUMA-trained policies outperform Humanoid-X and AMASS, achieving significant gains in imitating diverse motions. The code is available at https://davian-robotics.github.io/PHUMA."
        },
        {
            "title": "INTRODUCTION",
            "content": "Humanoid robots are central to the pursuit of general-purpose embodied AI, but their deployment in real-world first requires locomotion that is both stable and humanlike. While reinforcement Equal Contribution 1 Figure 2: Overview of datasets and performance. PHUMA is both large-scale and physically reliable, which translates into higher success rates in motion imitation and pelvis path following. (a) Feasible and infeasible human motion sources in each dataset. (b) Physical reliability, with AMASS retargeted using standard learning-based inverse kinematics method. (c) Success rate on unseen motions. (d) Success rate in path-following. Results are reported on the Unitree G1 humanoid. learning (RL) with task-oriented rewards has led to remarkable progress in quadrupedal locomotion (Hwangbo et al., 2019; Lee et al., 2020; Tan et al., 2018), directly applying these strategies to humanoids often produces gaits that are effective yet non-humanlike (Hansen et al., 2023; Sferrazza et al., 2024). To address this limitation, motion imitation has emerged as promising paradigm. In motion imitation, policies are trained to replicate human movements through three-stage pipeline: (1) collecting human motion data, (2) retargeting it to the robots morphology, and (3) using RL to track the retargeted trajectories (Peng et al., 2018; Tessler et al., 2024; He et al., 2024b). Despite its promise, progress in motion imitation is fundamentally constrained by the scale, diversity, and physical feasibility of human motion data. High-quality motion capture datasets such as LaFAN1 (Harvey et al., 2020) and AMASS (Mahmood et al., 2019) provide high proportion of physically feasible motions, but are limited in scale and diversity, with motions dominated by simple motions such as reaching and walking. To overcome this scarcity, recent work has sought to scale data collection by leveraging vast internet videos. Humanoid-X (Mao et al., 2025) exemplifies this trend by converting videos to SMPL representations (Loper et al., 2023) using video-tomotion model (Kocabas et al., 2020), then retargeting them to humanoid embodiments. However, this pipeline introduces two types of physical violations. First, the video-to-motion model often misestimates global pelvis translation, producing artifacts such as floating or ground penetration. Second, the retargeting stage prioritizes joint alignment over physical plausibility (He et al., 2024b;a), leading to joint violation and foot skating as illustrated in the top row of Figure 1. In response, we introduce PHUMA: Physically-grounded HUMAnoid locomotion dataset that leverages large-scale human video while overcoming physical artifacts through careful data curation and physics-constrained retargeting. As illustrated in Figure 3(1), we first collect diverse highquality human motion data and filter out infeasible motions from Humanoid-X, such as root jitter or actions requiring external objects like sitting on chairs. This filtering removes approximately 70% of the original dataset, as shown in Figure 2(a). As shown in Figure 3(2), we then apply Physicallygrounded Shape-adaptive Inverse Kinematics (PhySINK), which enforces soft joint limits, ground contact, and anti-skating constraints to eliminate violations such as joint overextension, floating, and sliding. As result, PHUMA provides substantially more physically plausible motions than existing datasets, 349.9% more than AMASS and 5.5% more than Humanoid-X (Figure 2(a,b)). We validate the effectiveness of PHUMA in two settings: (i) imitation of unseen motions and (ii) path following with pelvis-only guidance. Using the MaskedMimic framework Tessler et al. (2024) for RL training, we tested policies on Unitree G1 and H1-2 humanoids. On 504 self-recorded videos across 11 motion types, policies trained with PHUMA achieve 1.2x and 2.1x higher success rates than AMASS and Humanoid-X, respectively (Figure 2(c)). For path following, PHUMA-trained policies improve overall success rate by 1.4x over AMASS, with 1.6x gains in vertical (e.g., squat, lunge, jump) and 2.1x gains in horizontal (e.g., walk, run) motion path trajectories(Figure 2(d)). We will release PHUMA as public resource to advance future research in humanoid locomotion."
        },
        {
            "title": "2 RELATED WORK",
            "content": "PHUMA focuses on constructing large-scale, physically reliable humanoid dataset, requiring two components: (1) collection of diverse human motion data and (2) retargeting of these motion data to the humanoid robot."
        },
        {
            "title": "2.1 HUMAN MOTION DATA",
            "content": "Human motion data, typically represented in the SMPL format (Loper et al., 2023; Pavlakos et al., 2019), is obtained from two main sources: motion capture systems and reconstruction from video (Gu et al., 2025). Motion capture data (CMU, 2003; Zhang et al., 2022; Al-Hafez et al., 2023) provides accurate kinematics but is difficult to scale due to its reliance on complex instrumentation, such as multi-camera arrays and marker-based suits. Even relatively large dataset like LaFAN1 (Harvey et al., 2020) contains only few hours of motion. AMASS (Mahmood et al., 2019), the most extensive and widely-used dataset, remains dominated by walking motions in indoor labs. Recent datasets (Lin et al., 2023; Zhang et al., 2025; Chung et al., 2021; Cai et al., 2022; Tsuchida et al., 2019) leverage the scalability and diversity of human videos. Humanoid-X (Mao et al., 2025) is notable for massively scaling up from Internet video data, providing an abundant collection of data from motion capture and video recovery. However, video-derived motion often exhibits severe jitter across frames (Kocabas et al., 2020; Wang et al., 2024), and motion from either source is susceptible to physical artifacts such as interactions with unmodeled objects (e.g., sitting on chair that does not exist) (Luo et al., 2023; 2024) and implausible foot-ground contact, including floating or penetration (Goel et al., 2023; Ye et al., 2023; Yu et al., 2021; Ugrinovic et al., 2024). PHUMA is large-scale, diverse, and curated motion dataset aggregated from both motion capture and human video through physics-aware curation pipeline, which corrects implausible foot-ground contact and filters out corrupted sequences with severe physical artifacts. 2.2 HUMANOID MOTION RETARGETING Human motion data, widely used for physics-based character control (Peng et al., 2018; Wagener et al., 2022; Luo et al., 2021; 2024; 2023; Hansen et al., 2025; Tessler et al., 2024; Tirinzoni et al., 2025), is now also being applied to the field of humanoid robotics (Radosavovic et al., 2024a; Fu et al., 2024; Cheng et al., 2024; Ji et al., 2024; Chen et al., 2025; Xie et al., 2025; Truong et al., 2025; Li et al., 2025). This relies on motion retargeting, which is critical for adapting human movements to humanoid robots that, despite their morphological similarities to humans, possess distinct kinematic and proportional characteristics. (Kim et al., 2025; Ho et al., 2010; Zhang et al., 2023). primary challenge is motion mismatch, where the retargeted motion fails to capture the kinematic pose of the source. Inverse kinematics (IK) methods (Radosavovic et al., 2024b; Zakka, 2025; Caron et al., 2025; Ze et al., 2025a;b) often overlook the differences in body shape, resulting in unnatural motions like in-toed gaits. Shape-adaptive inverse kinematics (SINK) methods address this by first adapting the source human model to match the body shape and limb proportions of the target robot. The motion is then aligned to the source by matching global joint positions (He et al., 2024b;a; 2025a;b) or local limb orientations (Cheynel et al., 2023; Allshire et al., 2025). While effective at pose matching, SINK approaches are physically under-constrained, introducing artifacts including joint limit violations and implausible ground interactions such as floating, penetration, and skating. Physically-grounded shape-adaptive inverse kinematics (PhySINK) directly addresses these physical artifacts by augmenting the optimization with joint feasibility, grounding, and skating loss terms, ensuring the retargeted motion maintains fidelity to the source while remaining physically plausible."
        },
        {
            "title": "3 METHOD",
            "content": "Our goal is to construct PHUMA, large-scale, physically reliable dataset for humanoid locomotion. We build upon the Humanoid-X motions (Mao et al., 2025), which are rich in scale but exhibits physical artifacts. We first apply physics-aware curation to filter out problematic motions (Section 3.1). Next, to solve artifacts introduced during the retargeting process itself, we employ PhySINK, our physics-constrained retargeting method that adapts the curated motion to the humanoid while enforcing physical plausibility (Section 3.2). Our two-stage pipeline is illustrated in Figure 3. 3 Figure 3: Overview of the PHUMA pipeline. Our four-stage pipeline for motion imitation learning includes: (1) Motion Curation, where we filter out problematic motions from diverse dataset; (2) Motion Retargeting, where the filtered motions are retargeted to the humanoid using PhySINK, incorporating series of losses.; (3) Policy Learning, where policy is trained to imitate the retargeted motions; and (4) Inference, where the trained policy is used to control the humanoid, enabling it to imitate motions from unseen videos processed by video-to-motion model. 3.1 PHYSICS-AWARE MOTION CURATION The goal of our curation pipeline is to refine raw motion data, which often contains artifacts that make the motion physically implausible for humanoid. Our process targets key issues such as severe jitter, instabilities from interactions with unmodeled objects, and incorrect foot-ground contact. To mitigate high-frequency jitter, we apply low-pass Butterworth filter (Appendix A.1.1). We identify unstable motions, such as sitting on non-existent chair, by calculating the center-of-mass (CoM) distance from the base of support. To correct foot-ground contact, consistent ground plane in the world frame is essential. Since recovered motions are often defined in cameras coordinate frame, they lack true ground reference, which causes floating and penetration. We establish global ground plane using majority-voting scheme: each foot vertex contributes to identifying the most consistent contact height. The entire motion is then shifted to align this plane at height of zero (Appendix A.1.2), after which we compute per-region foot contact scores. With reliable ground plane established, we segment all sequences into 4-second clips. We then discard any clip exhibiting: (i) excessive jerk, (ii) CoM position far outside its support base, or (iii) insufficient foot-ground contact. This chunk-and-filter process maximizes the retention of viable segments from longer, partially flawed sequences (Appendix A.1.3). Finally, we augment these curated motions with data from LaFAN1, LocoMuJoCo, and our own video captures. As detailed in Table 1, the resulting PHUMA dataset is large-scale collection containing 73.0 hours of physically-grounded motion across 76.0K clips. 4 Table 1: Composition of the PHUMA dataset. summary of the number of clips and duration for each sub-dataset, categorized by source: motion capture and human video. PHUMA aggregates these diverse sub-datasets, resulting over 73 hours of physically-grounded motion clips. Dataset LocoMuJoCo (Al-Hafez et al., 2023) GRAB (Taheri et al., 2020) EgoBody (Zhang et al., 2022) LAFAN1 (Harvey et al., 2020) AMASS (Mahmood et al., 2019) HAA500 (Chung et al., 2021) Motion-X Video (Lin et al., 2023) HuMMan (Cai et al., 2022) AIST (Tsuchida et al., 2019) IDEA400 (Lin et al., 2023) PHUMA Video PHUMA # Clip 0.78K 1.73K 2.12K 2.18K 21.73K 1.76K 33.04K 0.50K 1.75K 9.94K 0.50K 76.01K # Frame Duration Source 0.93M 0.20M 0.24M 0.26M 2.25M 0.11M 3.45M 0.05M 0.18M 0.98M 0.06M 7.88M 0.86h 1.88h 2.19h 2.40h 20.86h 1.01h 31.98h 0.47h 1.66h 9.10h 0.56h 72.96h Motion Capture Motion Capture Motion Capture Motion Capture Motion Capture Human Video Human Video Human Video Human Video Human Video Human Video Figure 4: Common physical artifacts in motion retargeting. From left to right: Motion Mismatch, Joint Violation, Floating, Penetration, and Skating. 3.2 PHYSICS-CONSTRAINED MOTION RETARGETING Inverse kinematics (IK) methods often fail to preserve motion style, while shape-adaptive inverse kinematics (SINK) preserves style but introduces artifacts such as joint violations and unrealistic ground interactions (Figure 4). Our method, physically grounded shape-adaptive inverse kinematics (PhySINK), overcomes these issues by extending SINK with joint feasibility, grounding, and antiskating losses, producing motions that are both stylistically faithful and physically plausible. Motion Fidelity Loss. We optimize the humanoid joint positions qt and root translation γt over time t, so that the retargeted motion closely matches the human motion. The LFidelity is defined as: (cid:13) (cid:13)pSMPL-X (t) pHumanoid Lglobal-match = (cid:88) (cid:88) (1) (t)(cid:13) (cid:13)1 Llocal-match = (cid:88) (cid:88) mij i=j (cid:13) (cid:13)pSMPL-X ij (cid:124) (t) pHumanoid ij (cid:123)(cid:122) position (t)(cid:13) 2 (cid:13) 2 (cid:125) (cid:88) (cid:88) + mij i=j (cid:0)1 (cid:10)pSMPL-X (cid:124) ij Lsmooth = (cid:88) (cid:13) (cid:13) qt 2 qt+1 + qt+2 (cid:13) (cid:13)1 + (cid:88) (t)(cid:11)(cid:1) (cid:125) (t), pHumanoid ij (cid:123)(cid:122) orientation (cid:13) (cid:13) γt 2 γt+1 + γt+2 (cid:13) (cid:13)1 (2) (3) t (t) and pHumanoid LFidelity = wglobal-matchLglobal-match + wlocal-matchLlocal-match + wsmoothLsmooth (4) where pSMPL-X (t) denote the global 3D position of joint at time t. pij denotes the position difference between joints and j. mij is binary mask that equals 1 when and are immediate neighbors in the humanoid kinematic tree, and 0 otherwise. We define Motion Fidelity (%) as the average percentage of frames where the mean per-joint position error is below 10 cm and the mean per-link orientation error is below 10 degrees. Joint Feasibility Loss. Configurations that violate joint limits can lead to unrealistic motion or instabilities in simulator. LJoint Feasibility penalizes joint angles and velocities that approach or exceed Table 2: Quantitative comparison and ablation study of retargeting methods. We evaluate performance on two humanoids, G1 and H1-2, showing the progressive impact of adding each of our proposed physical constraint losses. Motion Fidelity (%) Joint Feasibility (%) Non-Floating (%) Non-Penetration (%) Non-Skating (%) (a) G1 IK SINK + Joint Feasibility Loss + Grounding Loss + Skating Loss = PhySINK (b) H1IK SINK + Joint Feasibility Loss + Grounding Loss + Skating Loss = PhySINK 27.6 94.8 94.9 94.9 94.8 36.3 93.9 94.0 93.9 93.9 91.7 95.9 100.0 100.0 100.0 80.9 15.3 99.9 99.9 99.9 55.6 96.4 96.4 99.9 99. 57.7 42.2 44.4 99.8 99.7 47.8 14.9 14.8 97.2 96.8 45.2 81.4 79.9 98.1 97.7 59.7 55.4 55.6 53.6 89.7 56.1 47.9 50.7 49.3 87.7 the predefined operational limits of the humanoid: Lposition-violation = Lvelocity-violation = (cid:88) (cid:88) [max(0, qt 0.98qmax) + max(0, 0.98qmin qt)] [max(0, qt 0.98 qmax) + max(0, 0.98 qmin qt)] LFeasiblity = Lposition-violation + Lvelocity-violation. (5) (6) (7) We define Joint Feasibility (%) as the percentage of frames where all joint positions and velocities remain within 98% of their predefined mechanical limits. Grounding Loss. The grounding loss corrects for floating or penetration artifacts by enforcing that the foot regions of the humanoid remain on the ground plane during frames with detected contact: LGround = (cid:88) (cid:88) ci (cid:13) (cid:13)pi t(z)(cid:13) 2 (cid:13) 2 i{LH,LT,RH,RT} (8) where ct is contact score for foot regions Left Heel (LH), Left Toe (LT), Right Heel (RH), and Right Toe (RT) at frame t. We define Non-Floating (%) as the percentage of contact frames where the foot is within 1 cm above the ground, and Non-Penetration (%) as the percentage of contact frames where the foot is within 1 cm below the ground. Skating Loss. The skating loss prevents foot sliding by penalizing the horizontal velocity of any foot region that is in contact with the ground: LSkate = (cid:88) (cid:88) ci (cid:13) (cid:13) pi t(x, y)(cid:13) (cid:13) i{LH,LT,RH,RT} (9) where ct is contact score for foot regions Left Heel (LH), Left Toe (LT), Right Heel (RH), and Right Toe (RT) at frame t. We define Non-Skating (%) as the percentage of contact frames where the foots horizontal velocity is below 10 cm/s. The objective for the baseline SINK method consists solely of the motion fidelity loss. Our PhySINK objective is weighted sum of the motion fidelity loss and the physical constraint terms. By optimizing this augmented objective, PhySINK generates motions that maintain kinematic similarity to the source while being physically plausible. LPhySINK = LFidelity + wFeasibilityLFeasibility + wGroundLGround + wSkateLSkate (10) To evaluate PhySINK, we retarget PHUMA to two Unitree robots, G1 (Unitree Robotics, 2025a) and H1-2 (Unitree Robotics, 2025b), and compare against standard IK solver (Zakka, 2025) and SINK framework. As shown in Table 2, IK struggles with motion fidelity, while SINK improves style at the cost of physical plausibility. Adding our proposed losses progressively enhances performance: the joint feasibility loss raises feasibility to nearly 100%, the grounding loss reduces floating and penetration to over 96%, and the full PhySINK (with skating loss) preserves motion fidelity while achieving strong results across all physical metrics, including nearly 90% non-skating performance."
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "In this section, we evaluate the effectiveness of PhySINK and PHUMA along three axes, addressing the following research questions: RQ1. What does our proposed PhySINK retargeting method compare with established retargeting approaches (IK, SINK) in terms of motion imitation performance? RQ2. How effective is PHUMA as training corpus for motion imitation, compared to prior datasets utilized for humanoid motion (LaFAN1, AMASS, Humanoid-X)? RQ3. When using simplified controller that considers only pelvis tracking rather than full-body state tracking, does training on PHUMA achieve better path-following performance than training on existing benchmark datasets across various motion categories?"
        },
        {
            "title": "4.1 EXPERIMENT SETUP",
            "content": "Training. We employ the MaskedMimic framework (Tessler et al., 2024) for all policy training, which provides unified approach for motion tracking with either full body state or partial body state information (e.g., pelvis-only). The framework trains policies using PPO (Schulman et al., 2017) to imitate human motion by maximizing reward signals that measure tracking accuracy. For RQ1 and RQ2, we train full-state motion tracking policies. These policies receive current proprioceptive state (sp ), which includes joint positions, orientations, and velocities, as well as full goal states (sg ) representing the target motion trajectories. Given these inputs, the policy outputs joint angle commands (at) that are executed via PD controllers. The reward function is designed to measure how well the humanoid matches the target motion. For RQ3, we employ the partial-state protocol from MaskedMimic. This involves first training fullstate teacher policy on full-body reference motion data, then using knowledge distillation to train student policy that mimics the action of the teacher policy while receiving only pelvis position and rotation as input, enabling pelvis path-following control while maintaining humanlike movement. All experiments are conducted in the IsaacGym simulator using Unitree G1 (29 DoF) and H1-2 (21 DoF, excluding wrist joints). Detailed hyperparameters are provided in Appendix 10, with complete observation space and reward function specifications in Table 8 and Appendix B.2, respectively. Evaluation. To assess the trained policies, we evaluate performance on two distinct datasets. The first consists of about 7.5K motions (10% of PHUMA) that were held out during training. The second comprises 504 self-collected video sequences converted to motion sequences using videoto-motion model. Processing details for the self-collected videos are provided in Appendix C.1. For evaluating the full body motion tracking (RQ1, RQ2), we adopt the success rate metric from prior motion imitation studies (He et al., 2024b; 2025a; Xie et al., 2025), which measures the ratio of motions successfully imitated within specified deviation threshold. Unlike prior work that uses 0.5m threshold, we employ stricter 0.15m threshold, as the standard threshold incorrectly classifies scenarios as successful when humanoids remain stationary during jumps or stay upright during squatting motions. Further discussions related to the threshold selection is detailed in Appendix C.2. In path following settings (RQ3), we use similar success rate metric focused on pelvis tracking accuracy. Specifically, we measure the ratio of motions where the policy successfully tracks pelvis trajectories within the same 0.15m threshold throughout the motion sequence. To evaluate performance across diverse motion types, we organize all evaluations into four motion categories: stationary (stand, reach), angular (bend, twist, turn, kick), vertical (squat, lunge, jump), and horizontal (walk, run). This categorization allows us to assess how well policies generalize across different types of human locomotion and movement patterns. 4.2 PHYSINK RETARGETING METHOD EFFECTIVENESS To evaluate the effectiveness of our proposed PhySINK retargeting method, we compare it against two established approaches: IK, SINK. We retarget the same source motions from AMASS using all three methods, then train separate full-state motion tracking policies on each retargeted dataset. 7 Table 3: Motion tracking performance across retargeting approaches. We evaluate the motion tracking success rate of policies trained on AMASS data retargeted by three different methods (IK, SINK, and PhySINK). Performance is assessed across various motion categories using two humanoid robots, G1 and H1-2, and two test sets: PHUMA Test and Unseen Video. Retarget Total Stationary Angular Vertical Horizontal Total Stationary Angular Vertical Horizontal PHUMA Test Unseen Video (a) 52.8 IK SINK 76.2 PhySINK 79.5 (b) H1-2 45.3 IK SINK 54.4 PhySINK 64.3 75.3 88.5 89.9 70.9 74.9 83.6 43.9 72.1 76. 35.7 45.9 57.0 24.3 56.8 61.1 15.2 17.2 27.7 44.2 66.8 69.5 35.0 49.6 55.9 54.0 70.2 72. 54.2 64.3 72.4 80.3 90.7 93.3 78.0 87.3 99.2 54.6 75.0 78.2 60.7 59.7 66.3 32.7 62.7 65. 30.1 46.0 57.4 43.3 44.1 47.3 28.6 63.9 63.1 Table 4: Motion tracking performance across datasets. Success rates of policies trained on LaFAN1, AMASS, Humanoid-X, and PHUMA, evaluated across motion categories on humanoid robots G1 and H1-2 using two test sets: PHUMA Test and Unseen Video. Hours Total Stationary Angular Vertical Horizontal Total Stationary Angular Vertical Horizontal PHUMA Test Unseen Video Dataset (a) G1 2.4 LaFAN1 AMASS 20.9 Humanoid-X 231.4 PHUMA 73. (b) H1-2 2.4 LaFAN1 AMASS 20.9 Humanoid-X 231.4 PHUMA 73.0 46.1 76.2 50.6 92.7 62.0 54.4 49.7 82.7 66.1 88.5 78.4 95.6 79.3 74.9 74.6 91. 36.2 72.1 43.0 91.7 54.7 45.9 40.4 79.5 24.0 56.8 26.0 86.0 26.6 17.2 17.0 68.1 42.5 66.8 31.8 85.6 58.9 49.6 37.3 68. 28.4 70.2 39.1 82.9 70.8 64.3 60.5 78.6 46.9 90.7 78.0 96.7 92.4 87.3 88.3 97.5 28.4 75.0 39.6 88.0 66.7 59.7 60.0 76. 19.6 62.7 23.0 71.8 56.4 46.0 48.7 74.5 10.5 44.1 6.5 67.1 68.2 63.9 39.7 63.8 Table 3 demonstrates that PhySINK consistently outperforms both baseline methods across all motion categories and humanoid embodiments. The results validate that physically-grounded retargeting directly translates to better imitation performance, with improvements particularly pronounced in dynamic motions (vertical and horizontal categories) where physical constraints are most critical. 4.3 PHUMA DATASET EFFECTIVENESS Having demonstrated PhySINKs effectiveness, we now compare PHUMA against existing humanoid datasets. We train full-state policies on four datasets with different characteristics: LaFAN1(small-scale, high-quality), AMASS(medium-scale, moderate-quality), HumanoidX(large-scale, lower-quality), and PHUMA(large-scale, high-quality). For AMASS, we apply the widely-used SINK retargeting method since it provides human motion source data, while LaFAN1 and Humanoid-X are used directly as pre-existing humanoid datasets. As shown in Table 4, PHUMA trained policies achieve the highest success rates across all motion categories and both humanoids. The results reveal that neighter scale nor quality alone is sufficient. Humanoid-X, despite its large size, underperforms due to quality issues, while LaFAN1 and AMASS, though cleaner, lack coverage in several motion types. By combining large scale with high quality motions, PHUMA delivers consistently superior performance across diverse behaviors. 4.4 PELVIS-ONLY PATH FOLLOWING CONTROL PERFORMANCE We evaluate whether training on PHUMA enables better pelvis path-following control compared to the AMASS dataset. Using MaskedMimics partially-constrained protocol, we train two student policies: one distilled from an AMASS-trained teacher and another from PHUMA-trained teacher. Both students receive only pelvis position and rotation as input. As shown in Table 5, policies trained on PHUMA consistently outperform those trained on baseline datasets across all motion categories and humanoids. This improvement is particularly pronounced Table 5: Pelvis path following peformance across motion dataset. We evaluate the success rate of pelvis path-following control for policies trained on the AMASS and PHUMA datasets across various pelvis trajectories from the PHUMA Test and Unseen Video. Dataset Total Stationary Angular Vertical Horizontal Total Stationary Angular Vertical Horizontal PHUMA Test Unseen Video (a) G1 AMASS 60.5 PHUMA 84.5 (a) H1-2 AMASS 60.4 PHUMA 73.9 85.6 94. 84.0 91.2 60.1 86.1 62.8 76.5 51.4 83.7 43.6 66.9 66.5 90. 78.7 84.8 54.8 74.6 72.3 78.1 83.6 98.3 96.6 96.6 66.5 83. 77.3 77.8 33.0 54.3 52.1 60.6 27.5 57.1 72.5 78.0 Figure 5: Path following on running motion. We visualize the robots trajectory in running motion. The target pelvis path is visualized with green line. Top row presents results from policy trained on AMASS, while bottom row presents results from policy trained on PHUMA. for vertical and horizontal motions, where AMASS shows significant limitations due to its composition of predominantly simpler motions like reaching and turning (Figure 8). More specifically, despite AMASS containing numerous walking motions, substantial performance gap remains in horizontal motions due to the absence of more dynamic movements such as running, as illustrated in Figure 2(d). This limitation is clearly demonstrated in Figure 5, where AMASS-trained policies frequently fail during running motions while PHUMA-trained policies maintain robust performance. These results confirm that PHUMA enables more diverse and dynamic humanoid control compared to AMASS, validating the practical value of PHUMA for complex control."
        },
        {
            "title": "5 CONCLUSION",
            "content": "We introduced PHUMA, large-scale, physically grounded humanoid locomotion dataset that overcomes the limitations of existing motion imitation pipelines. Unlike prior video-driven datasets prone to artifacts such as floating, ground penetration, and joint violations, PHUMA combines largescale human video with careful filtering and our physics-constrained retargeting method, PhySINK, to produce motions that are both diverse and physically reliable. Policies trained on PHUMA consistently outperform those trained on AMASS and Humanoid-X in motion imitation and pelvis-guided path following on Unitree G1 and H1-2 humanoids, demonstrating that progress in humanoid locomotion requires not only scale but also physically reliable data. Looking forward, future work includes sim-to-real transfer, enabling policies trained with PHUMA to produce physically reliable motions on real humanoid robots, and vision-based control, where video observations replace privileged state inputs to better align with real-world perception."
        },
        {
            "title": "REPRODUCIBILITY STATEMENT",
            "content": "To ensure the reproducibility of our results, we provide comprehensive implementation details and experimental specifications. The complete hyperparameter settings for PPO training are detailed in Appendix 10. Our physics-aware curation process and PhySINK retargeting method are described in detail in Sections 3.1 and 3.2, respectively, with algorithmic specifications provided in the appendix. The PHUMA dataset composition and statistics are thoroughly documented in Section 3.1 and Appendix A.3. All evaluation metrics, including our modified success rate threshold and motion category definitions, are explicitly defined in Section 4.1. Implementation details for baseline methods (IK, SINK) follow established protocols as referenced in the main text. The self-collected video processing pipeline is described in Appendix C.1. We plan to release our code, dataset, and trained models upon publication to facilitate further research in this area."
        },
        {
            "title": "ACKNOWLEDGEMENT",
            "content": "We would like to express our gratitude to Donghu Kim and Youngdo Lee for their helpful discussions and insightful feedback on this paper."
        },
        {
            "title": "REFERENCES",
            "content": "Carnegie mellon university graphics lab motion capture database. http://mocap.cs.cmu.edu, 2003. The data was captured with funding from NSF EIA-0196217. Acknowledgment of the database is appreciated in any published work. Firas Al-Hafez, Guoping Zhao, Jan Peters, and Davide Tateo. Locomujoco: comprehensive imitation learning benchmark for locomotion. arXiv preprint arXiv:2311.02496, 2023. Arthur Allshire, Hongsuk Choi, Junyi Zhang, David McAllister, Anthony Zhang, Chung Min Kim, Trevor Darrell, Pieter Abbeel, Jitendra Malik, and Angjoo Kanazawa. Visual imitation enables contextual humanoid control. arXiv preprint arXiv:2505.03729, 2025. Zhongang Cai, Daxuan Ren, Ailing Zeng, Zhengyu Lin, Tao Yu, Wenjia Wang, Xiangyu Fan, Yang Gao, Yifan Yu, Liang Pan, et al. Humman: Multi-modal 4d human dataset for versatile sensing and modeling. In European Conference on Computer Vision, pp. 557577. Springer, 2022. Stephane Caron, Yann De Mont-Marin, Rohan Budhiraja, Seung Hyeon Bang, Ivan Domrachev, Simeon Nedelchev, peterd NV, and Joris Vaillant. Pink: Python inverse kinematics based on Pinocchio, 2025. URL https://github.com/stephane-caron/pink. Zixuan Chen, Mazeyu Ji, Xuxin Cheng, Xuanbin Peng, Xue Bin Peng, and Xiaolong Wang. Gmt: General motion tracking for humanoid whole-body control. arXiv preprint arXiv:2506.14770, 2025. Xuxin Cheng, Yandong Ji, Junming Chen, Ruihan Yang, Ge Yang, and Xiaolong Wang. Expressive whole-body control for humanoid robots. arXiv preprint arXiv:2402.16796, 2024. Theo Cheynel, Thomas Rossi, Baptiste Bellot-Gurlet, Damien Rohmer, and Marie-Paule Cani. Sparse motion semantics for contact-aware retargeting. In ACM SIGGRAPH Conference on Motion, Interaction and Games (MIG), 2023. Jihoon Chung, Cheng-hsin Wuu, Hsuan-ru Yang, Yu-Wing Tai, and Chi-Keung Tang. Haa500: Human-centric atomic action dataset with curated videos. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 1346513474, 2021. Zipeng Fu, Qingqing Zhao, Qi Wu, Gordon Wetzstein, and Chelsea Finn. Humanplus: Humanoid shadowing and imitation from humans. arXiv preprint arXiv:2406.10454, 2024. Shubham Goel, Georgios Pavlakos, Jathushan Rajasegaran, Angjoo Kanazawa, and Jitendra Malik. In Proceedings of the Humans in 4d: Reconstructing and tracking humans with transformers. IEEE/CVF International Conference on Computer Vision, pp. 1478314794, 2023. 10 Zhaoyuan Gu, Junheng Li, Wenlan Shen, Wenhao Yu, Zhaoming Xie, Stephen McCrory, Xianyi Cheng, Abdulaziz Shamsah, Robert Griffin, Karen Liu, et al. Humanoid locomotion and manipulation: Current progress and challenges in control, planning, and learning. arXiv preprint arXiv:2501.02116, 2025. Nicklas Hansen, Hao Su, and Xiaolong Wang. Td-mpc2: Scalable, robust world models for continuous control. arXiv preprint arXiv:2310.16828, 2023. Nicklas Hansen, Jyothir V, Vlad Sobal, Yann LeCun, Xiaolong Wang, and Hao Su. Hierarchical world models as visual whole-body humanoid controllers. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?id= 7wuJMvK639. Felix Harvey, Mike Yurick, Derek Nowrouzezahrai, and Christopher Pal. Robust motion inbetweening. ACM Transactions on Graphics (TOG), 39(4):601, 2020. Tairan He, Zhengyi Luo, Xialin He, Wenli Xiao, Chong Zhang, Weinan Zhang, Kris Kitani, Changliu Liu, and Guanya Shi. Omnih2o: Universal and dexterous human-to-humanoid whole-body teleoperation and learning. arXiv preprint arXiv:2406.08858, 2024a. Tairan He, Zhengyi Luo, Wenli Xiao, Chong Zhang, Kris Kitani, Changliu Liu, and Guanya Shi. Learning human-to-humanoid real-time whole-body teleoperation. In 2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 89448951. IEEE, 2024b. Tairan He, Jiawei Gao, Wenli Xiao, Yuanhang Zhang, Zi Wang, Jiashun Wang, Zhengyi Luo, Guanqi He, Nikhil Sobanbab, Chaoyi Pan, et al. Asap: Aligning simulation and real-world physics for learning agile humanoid whole-body skills. arXiv preprint arXiv:2502.01143, 2025a. Tairan He, Wenli Xiao, Toru Lin, Zhengyi Luo, Zhenjia Xu, Zhenyu Jiang, Jan Kautz, Changliu Liu, Guanya Shi, Xiaolong Wang, et al. Hover: Versatile neural whole-body controller for humanoid robots. In 2025 IEEE International Conference on Robotics and Automation (ICRA), pp. 9989 9996. IEEE, 2025b. Edmond SL Ho, Taku Komura, and Chiew-Lan Tai. Spatial relationship preserving character motion adaptation. In ACM SIGGRAPH 2010 papers, pp. 18. 2010. Jemin Hwangbo, Joonho Lee, Alexey Dosovitskiy, Dario Bellicoso, Vassilios Tsounis, Vladlen Koltun, and Marco Hutter. Learning agile and dynamic motor skills for legged robots. Science Robotics, 4(26):eaau5872, 2019. Mazeyu Ji, Xuanbin Peng, Fangchen Liu, Jialong Li, Ge Yang, Xuxin Cheng, and Xiaolong Wang. Exbody2: Advanced expressive humanoid whole-body control. arXiv preprint arXiv:2412.13196, 2024. Chung Min Kim, Brent Yi, Hongsuk Choi, Yi Ma, Ken Goldberg, and Angjoo Kanazawa. Pyroki: modular toolkit for robot kinematic optimization. arXiv preprint arXiv:2505.03728, 2025. Muhammed Kocabas, Nikos Athanasiou, and Michael Black. Vibe: Video inference for human body pose and shape estimation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 52535263, 2020. Joonho Lee, Jemin Hwangbo, Lorenz Wellhausen, Vladlen Koltun, and Marco Hutter. Learning quadrupedal locomotion over challenging terrain. Science robotics, 5(47):eabc5986, 2020. Yixuan Li, Yutang Lin, Jieming Cui, Tengyu Liu, Wei Liang, Yixin Zhu, and Siyuan Huang. Clone: Closed-loop whole-body humanoid teleoperation for long-horizon tasks. arXiv preprint arXiv:2506.08931, 2025. Jing Lin, Ailing Zeng, Shunlin Lu, Yuanhao Cai, Ruimao Zhang, Haoqian Wang, and Lei Zhang. Motion-x: large-scale 3d expressive whole-body human motion dataset. Advances in Neural Information Processing Systems, 36:2526825280, 2023. 11 Matthew Loper, Naureen Mahmood, Javier Romero, Gerard Pons-Moll, and Michael Black. Smpl: In Seminal Graphics Papers: Pushing the Boundaries, skinned multi-person linear model. Volume 2, pp. 851866. 2023. Zhengyi Luo, Ryo Hachiuma, Ye Yuan, and Kris Kitani. Dynamics-regulated kinematic policy for egocentric pose estimation. Advances in Neural Information Processing Systems, 34:25019 25032, 2021. Zhengyi Luo, Jinkun Cao, Kris Kitani, Weipeng Xu, et al. Perpetual humanoid control for realtime simulated avatars. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 1089510904, 2023. Zhengyi Luo, Jinkun Cao, Josh Merel, Alexander Winkler, Jing Huang, Kris M. Kitani, and Weipeng Xu. Universal humanoid motion representations for physics-based control. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/ forum?id=OrOd8PxOO2. Naureen Mahmood, Nima Ghorbani, Nikolaus Troje, Gerard Pons-Moll, and Michael Black. Amass: Archive of motion capture as surface shapes. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 54425451, 2019. Jiageng Mao, Siheng Zhao, Siqi Song, Tianheng Shi, Junjie Ye, Mingtong Zhang, Haoran Geng, Jitendra Malik, Vitor Campagnolo Guizilini, and Yue Wang. Universal humanoid robot pose learning from internet human videos. In ICRA 2025 Workshop: Human-Centered Robot Learning in the Era of Big Data and Large Models, 2025. URL https://openreview.net/forum? id=2pBjMDj6uJ. Georgios Pavlakos, Vasileios Choutas, Nima Ghorbani, Timo Bolkart, Ahmed AA Osman, Dimitrios Tzionas, and Michael Black. Expressive body capture: 3d hands, face, and body from single image. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1097510985, 2019. Xue Bin Peng, Pieter Abbeel, Sergey Levine, and Michiel Van de Panne. Deepmimic: Exampleguided deep reinforcement learning of physics-based character skills. ACM Transactions On Graphics (TOG), 37(4):114, 2018. Ilija Radosavovic, Sarthak Kamat, Trevor Darrell, and Jitendra Malik. Learning humanoid locomotion over challenging terrain. arXiv preprint arXiv:2410.03654, 2024a. Ilija Radosavovic, Bike Zhang, Baifeng Shi, Jathushan Rajasegaran, Sarthak Kamat, Trevor Darrell, Koushil Sreenath, and Jitendra Malik. Humanoid locomotion as next token prediction. Advances in neural information processing systems, 37:7930779324, 2024b. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. Carmelo Sferrazza, Dun-Ming Huang, Xingyu Lin, Youngwoon Lee, and Pieter Abbeel. Humanoidbench: Simulated humanoid benchmark for whole-body locomotion and manipulation. arXiv preprint arXiv:2403.10506, 2024. Omid Taheri, Nima Ghorbani, Michael Black, and Dimitrios Tzionas. Grab: dataset of wholeIn European conference on computer vision, pp. 581600. body human grasping of objects. Springer, 2020. Jie Tan, Tingnan Zhang, Erwin Coumans, Atil Iscen, Yunfei Bai, Danijar Hafner, Steven Bohez, and Vincent Vanhoucke. Sim-to-real: Learning agile locomotion for quadruped robots. arXiv preprint arXiv:1804.10332, 2018. Chen Tessler, Yunrong Guo, Ofir Nabati, Gal Chechik, and Xue Bin Peng. Maskedmimic: Unified physics-based character control through masked motion inpainting. ACM Transactions on Graphics (TOG), 43(6):121, 2024. 12 Andrea Tirinzoni, Ahmed Touati, Jesse Farebrother, Mateusz Guzek, Anssi Kanervisto, Yingchen Xu, Alessandro Lazaric, and Matteo Pirotta. Zero-shot whole-body humanoid control via behavioral foundation models. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?id=9sOR0nYLtz. Takara Truong, Qiayuan Liao, Xiaoyu Huang, Guy Tevet, Karen Liu, and Koushil Sreenath. Beyondmimic: From motion tracking to versatile humanoid control via guided diffusion. arXiv preprint arXiv:2508.08241, 2025. Shuhei Tsuchida, Satoru Fukayama, Masahiro Hamasaki, and Masataka Goto. Aist dance video database: Multi-genre, multi-dancer, and multi-camera database for dance information processing. In ISMIR, volume 1, pp. 6, 2019. Nicolas Ugrinovic, Boxiao Pan, Georgios Pavlakos, Despoina Paschalidou, Bokui Shen, Jordi Sanchez-Riera, Francesc Moreno-Noguer, and Leonidas Guibas. Multiphys: Multi-person physics-aware 3d motion estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 23312340, 2024. Unitree Robotics. Unitree g1 humanoid robot. https://www.unitree.com/g1, 2025a. Accessed: 2025-09-25. Unitree Robotics. Unitree h1-2 humanoid robot. https://www.unitree.com/h1, 2025b. Accessed: 2025-09-25. Nolan Wagener, Andrey Kolobov, Felipe Vieira Frujeri, Ricky Loynd, Ching-An Cheng, and Matthew Hausknecht. Mocapact: multi-task dataset for simulated humanoid control. Advances in Neural Information Processing Systems, 35:3541835431, 2022. Yufu Wang, Ziyun Wang, Lingjie Liu, and Kostas Daniilidis. Tram: Global trajectory and motion of 3d humans from in-the-wild videos. In European Conference on Computer Vision, pp. 467487. Springer, 2024. Weiji Xie, Jinrui Han, Jiakun Zheng, Huanyu Li, Xinzhe Liu, Jiyuan Shi, Weinan Zhang, Chenjia Bai, and Xuelong Li. Kungfubot: Physics-based humanoid whole-body control for learning highly-dynamic skills. arXiv preprint arXiv:2506.12851, 2025. Vickie Ye, Georgios Pavlakos, Jitendra Malik, and Angjoo Kanazawa. Decoupling human and camera motion from videos in the wild. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 2122221232, 2023. Ri Yu, Hwangpil Park, and Jehee Lee. Human dynamics from monocular video with dynamic camera movements. ACM Transactions on Graphics (TOG), 40(6):114, 2021. Kevin Zakka. Mink: Python inverse kinematics based on MuJoCo, May 2025. URL https: //github.com/kevinzakka/mink. Yanjie Ze, Joao Pedro Araujo, Jiajun Wu, and C. Karen Liu. Gmr: General motion retargeting, 2025a. URL https://github.com/YanjieZe/GMR. GitHub repository. Yanjie Ze, Zixuan Chen, Jo AG Pedro Ara Aˇsjo, Zi-ang Cao, Xue Bin Peng, Jiajun Wu, and Karen Liu. Twist: Teleoperated whole-body imitation system. arXiv preprint arXiv:2505.02833, 2025b. Jiaxu Zhang, Junwu Weng, Di Kang, Fang Zhao, Shaoli Huang, Xuefei Zhe, Linchao Bao, Ying Shan, Jue Wang, and Zhigang Tu. Skinned motion retargeting with residual perception of motion semantics & geometry. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1386413872, 2023. Siwei Zhang, Qianli Ma, Yan Zhang, Zhiyin Qian, Taein Kwon, Marc Pollefeys, Federica Bogo, and Siyu Tang. Egobody: Human body shape and motion of interacting people from head-mounted devices. In European conference on computer vision, pp. 180200. Springer, 2022. Yuhong Zhang, Jing Lin, Ailing Zeng, Guanlin Wu, Shunlin Lu, Yurong Fu, Yuanhao Cai, Ruimao Zhang, Haoqian Wang, and Lei Zhang. Motion-x++: large-scale multimodal 3d whole-body human motion dataset. arXiv preprint arXiv:2501.05098, 2025."
        },
        {
            "title": "APPENDIX",
            "content": "A Details of PHUMA Dataset A.1 Data Preprocessing A.1.1 Low-Pass Noise Filtering for Motion Data A.1."
        },
        {
            "title": "Extracting Ground Contact Information",
            "content": "A.1."
        },
        {
            "title": "Filtering Motion Data by Physical Information",
            "content": "A.2 Qualitative Comparison of Retargeting Methods A.3 Dataset Composition and Statistics Details of Motion Imitation Learning B.1 Observation Space Compositions B.2 Reward Function B. PPO Hyperparameter Experiment Details C.1 Self-Collected Video Dataset C.2 Success Rate Threshold Analysis 15 15 15 15 16 17 19 19 19 19 21"
        },
        {
            "title": "A DETAILS OF PHUMA DATASET",
            "content": "A.1 DATA PREPROCESSING Before applying inverse kinematics, it is essential to ensure that the human motion data is clean and robust, as this data serves as the target for the humanoid robot to follow. Raw motion data often contains noise from sensor errors, tracking inaccuracies, or estimation artifacts that can negatively impact the retargeting process. To address these issues, we implement the following preprocessing to filter and clean the motion data. A.1.1 LOW-PASS NOISE FILTERING FOR MOTION DATA We smooth all motion channels with zero-phase, 4-th-order Butterworth low-pass filter (fs = 30 Hz). For root translation the cutoff is 3 Hz; for global orientation and body pose it is 6 Hz. A.1.2 EXTRACTING GROUND CONTACT INFORMATION We identify subset of SMPL-X foot vertices that are most indicative of ground interaction. Specifically, we select the 22 vertically lowest vertices from each foot region (left heel, left toe, right heel, right toe) in the SMPL-X default pose, totaling 88 vertices. These vertices are illustrated in Figure 6. The vertex indices corresponding to these ground-contact points are provided in Table 6. Figure 6: SMPL-X Foot Vertices for Ground-Contact Detection. This figure illustrates the selected foot vertices on the SMPL-X model used to detect ground contact. Green and orange points denote the left heel and left toe, while blue and pink represent the right heel and right toe, respectively. The remaining foot vertices are shown in light-gray. The clusters of colored points correspond to the specific parts of the foot that are used to check for contact with the ground, making the process more accurate and robust than using single point. Table 6: SMPL-X foot vertex indices used for groundcontact detection. Region Vertex indices Left heel Left toe Right heel Right toe 8888, 8889, 8891, 8909, 8910, 8911, 8913, 8914, 8915, 8916, 8917, 8918, 8919, 8920, 8921, 8922, 8923, 8924, 8925, 8929, 8930, 8934 5773, 5781, 5782, 5791, 5793, 5805, 5808, 5816, 5817, 5830, 5831, 5859, 5860, 5906, 5907, 5908, 5909, 5912, 5914, 5915, 5916, 5917 8676, 8677, 8679, 8697, 8698, 8699, 8701, 8702, 8703, 8704, 8705, 8706, 8707, 8708, 8709, 8710, 8711, 8712, 8713, 8714, 8715, 8716 8467, 8475, 8476, 8485, 8487, 8499, 8502, 8510, 8511, 8524, 8525, 8553, 8554, 8600, 8601, 8602, 8603, 8606, 8608, 8609, 8610, 8611 15 Table 7: Physics-aware data filtering metrics and thresholds."
        },
        {
            "title": "Metric",
            "content": "Threshold < 50 m/s3 Root jerk > 0.6 Foot contact score > 0.6 Minimum pelvis height < 1.5 Maximum pelvis height Pelvis distance to base of support < 6 cm Spine1 distance to base of support < 11 cm To correctly place motion, it is necessary to establish single, consistent ground plane. Simple heuristics often fail; defining the ground by the lowest foot position in the sequence can cause floating, while per-frame adjustments introduce jitter. Our method solves this using majority vote to find the ground height that maximizes the duration of foot contact. In this scheme, each vertex on the feet votes for potential ground level. The height that gathers the most votes across the entire sequence is selected, as this plane consistently has the most foot vertices near it. The entire motion is then shifted to place this new ground at height zero. Specifically, we first generate candidate ground coordinates. For each frame t, we find the minimum vertical position among these 88 points and record it as candidate coordinate for the ground plane, gt. Second, we evaluate each candidate gt by counting the total number of foot vertices, across all frames, that fall within its δ = 2.5 cm tolerance band. We select the candidate with the highest count as the optimal ground plane and translate the entire sequence vertically to place at the origin. A.1.3 FILTERING MOTION DATA BY PHYSICAL INFORMATION We evaluate each segmented motion sub-clips based on the metrics summarized in Table 7. Motion sub-clips failing to satisfy these thresholds are discarded. Root jerk represents rapid changes in root acceleration, indicative of abrupt or unnatural motions. High root jerk segments are excluded to ensure smooth and physically plausible trajectories. Foot contact score measures the consistency and sufficiency of foot-ground interactions based on graded ground-contact signals defined by vertex proximity to the ground. Specifically, given subclip with frames, the foot contact score is computed as: Foot contact score = 1 (cid:88) t=1 max (cid:0)clh , clt , crh , crt (cid:1) , (11) , clt , crh , and crt where clh represent the graded ground-contact ratio at frame for the left heel, left toe, right heel, and right toe, respectively. low foot contact score indicates significant penetration or floating, both of which are undesirable artifacts. Note that motions involving airborne phases, such as jumps, can easily satisfy this criterion as long as contact before and after the airborne phase is consistent. Pelvis height criteria exclude segments where the humanoid is unnaturally positioned. Specifically, the minimum height criterion filters out motions that involve the humanoid being excessively crouched or lying on the ground, while the maximum height criterion eliminates segments exhibiting unnatural floating. Distance to the base of support criteria ensure stable and physically plausible balance. Since the SMPL-X models center of mass typically lies between the pelvis and spine1 joints, deviations of these joints horizontal-plane projections from the base of support indicate imbalance or instability infeasible for humanoids. The base of support is defined as the convex hull formed by the horizontalplane projections of the left foot, right foot, left ankle, and right ankle joints. A.2 QUALITATIVE COMPARISON OF RETARGETING METHODS To provide an intuitive comparison of different retargeting approaches, we present qualitative results in Figure 7. Using walking motion as an example, we demonstrate the distinct characteristics and limitations of each method. Traditional inverse kinematics (IK) prioritizes matching end-effector positions, such as hands and feet, from rigidly scaled human motions. However, this approach produces unnatural locomotion patterns where the humanoid appears to walk on tightrope rather than exhibiting natural humanlike gait. This occurs because the fixed scaling cannot account for the proportional differences between human and robot morphologies. Learning-based inverse kinematics (SINK) generates more natural-looking walking motions compared to traditional IK by optimizing body proportions. However, SINK suffers from physical violations that compromise motion realism. Common issues include foot penetration through the ground surface and fixed ankle angles that result from the lack of explicit contact constraints during the retargeting process. In contrast, our proposed PhySINK method achieves both natural movement patterns and physical plausibility. The resulting motions maintain appropriate ankle angles while ensuring proper ground contact, demonstrating that PhySINK successfully balances motion naturalness with physical constraints. This improvement stems from the incorporation of explicit physical constraint terms in the optimization objective. A.3 DATASET STATISTICS This section presents the detailed motion statistics of PHUMA. As we collect the motion data from diverse sources, from MoCap data to video, PHUMAresults in well-balanced motion distribution that avoids domination by specific motion types. Figure 8 demonstrates that PHUMA exhibits significantly more balanced motion coverage compared to existing datasets. While LaFAN1 and AMASS show uneven distributions with some motion types having very limited motions, lacking certain motion categories entirely (such as reach, bend, and squat motions), or being heavily dominated by specific motions (reach, turn, and walk), PHUMA provides more balanced coverage across all motion categories with substantially more examples per motion type. This improved diversity and scale directly translate to better imitation performance. Table 4 demonstrates that policy trained on PHUMA achieves superior overall performance on unseen motions compared to policies trained on other datasets. The results also show consistent performance improvements across all individual motion categories. The results confirm that the enhanced dataset composition benefits generalization across all diverse movement types, indicating that the balanced motion distribution of PHUMA leads to more robust imitation policies. 17 Figure 7: Qualitative Comparison of Retargeting Methods. This figure provides visual comparison of human motion retargeted to humanoid robot using the IK, SINK, and PhySINK methods. The top row shows the original human motion from the SMPL model, while the rows below show the resulting motions for each retargeting method. Figure 8: Motion Type Distribution per Dataset. This radar chart compares the total duration of each motion type across PHUMA, AMASS, and LaFAN1 datasets."
        },
        {
            "title": "B DETAILS OF MOTION IMITATION LEARNING",
            "content": "B.1 OBSERVATION SPACE COMPOSITIONS This section provides detailed information about the observation space composition used in our experimental setup, as summarized in Table 8. The observation space consists of two main components: proprioceptive states and goal states. Proprioceptive States. The proprioceptive information includes root height, body positions, body rotations, body velocities, and body angular velocities. The Unitree G1 and H1-2 robots have 33 and 25 bodies, respectively. For body positions, the root body is excluded from the position measurements. Goal States. The goal states comprise both relative and absolute body positions and rotations. The relative component represents the difference between the future 15 timesteps of reference motion states and the current proprioceptive state. The absolute component represents states relative to the reference motions root position, providing root-relative coordinate frame for the target motion. Table 8: Observation Space Dimensions State (a) Proprioceptive State Root height Body position Body rotation Body velocity Body angular velocity (b) Goal State Dimension G1 H11 32 3 33 6 33 3 33 3 1 24 3 25 6 25 3 25 3 Relative body position Absolute body position Relative body rotation Absolute body rotation Time 33 15 3 33 15 3 33 15 6 33 15 6 33 15 1 25 15 3 25 15 3 25 15 6 25 15 6 25 15 1 Total dim 9898 7498 B.2 REWARD FUNCTION The reward function used for training the tracking policy consists of multiple components, as detailed in Table 9. The overall reward structure comprises two main categories: motion tracking task rewards and regularization rewards. Motion Tracking Rewards. These components encourage the policy to match the reference motion by providing higher rewards when the robots proprioceptive states closely resemble the target motion states. Regularization Rewards. To promote smooth and stable motion execution, we include regularization terms that penalize undesirable behaviors. Specifically, we augment the standard MaskedMimic reward formulation with action rate penalties that discourage large changes between consecutive actions, helping to ensure smooth joint movements and prevent abrupt motion transitions. B.3 PPO HYPERPARAMETER The detailed hyperparameter configuration used for PPO training is provided in Table 10. 19 Table 9: Reward function terms for training"
        },
        {
            "title": "Term",
            "content": "(a) Task"
        },
        {
            "title": "Global body position\nRoot height\nGlobal body rotation\nGlobal body velocity\nGlobal body angular velocity",
            "content": "exp(100 pt ˆpt2 2) ˆhroot exp(100 (hroot exp(10 θt ˆθt2 2) exp(0.5 vt ˆvt2 2) exp(0.1 ωt ˆωt2 2) )2) 0.5 0.2 0.3 0.1 0.1 (b) Regularization Power consumption Action rate q1 at at12 -1e-05 -0.2 Table 10: PPO Hyperparameter Values for Model Training Hyperparameter Value Optimizer Num envs Mini Batches Learning epochs Entropy coefficient Value loss coefficient Clip param Max grad norm Init noise std Actor learning rate Critic learning rate GAE decay factor(λ) GAE discount factor(γ) Adam 8192 32 1 0.0 0.5 0.2 50.0 -2.9 2e-5 1e-4 0.95 0.99 Actor Transformer dimension Actor layers Actor heads Critic MLP size Activation 512 4 4 [1024, 1024, 1024, 1024] ReLU"
        },
        {
            "title": "C EXPERIMENT DETAILS",
            "content": "C.1 SELF-COLLECTED VIDEO DATASET To ensure fair evaluation of imitation performance on unseen motions, we create custom evaluation dataset using self-collected video recordings. This dataset contains motions uniformly distributed across the 11 motion types shown in Figure 8, providing balanced coverage for comprehensive performance assessment. The dataset creation process follows three main steps: (1) recording videos of human performers executing each motion type, (2) converting videos into SMPL human motion parameters using video-to-motion model, and (3) retargeting the human motions to humanoid robot motions using our PhySINK method. First, we record videos covering all 11 motion categories, collecting uniform distribution for each type. We then apply the TRAM video-to-motion model (Wang et al., 2024) to extract SMPL motion parameters from the recorded videos. Finally, we process these SMPL motions with PhySINK retargeting to generate physically plausible humanoid motions. Example results from this dataset are illustrated in Figure 9. This self-collected evaluation set ensures that our performance assessments are conducted on completely unseen motions that were not influenced by any training data sources, providing an unbiased evaluation of generalization capabilities. Figure 9: Overview of the Self-collected Data Pipeline. This figure illustrates the three main steps of our data collection pipeline: (left) self-recorded video of human motion, (center) the motion extracted using video-to-motion model, and (right) the final motion retargeted to humanoid robot. C.2 SUCCESS RATE THRESHOLD ANALYSIS To demonstrate the limitations of the conventional success rate threshold, we evaluate imitation performance using both the standard 0.5m threshold and our proposed stricter 0.15m threshold. This comparison reveals the true quality differences between policies trained on different datasets. Tables 11 and 12 present the results for both threshold settings. Under the loose 0.5m threshold, policies trained on different datasets show relatively similar success rates, with differences appearing modest. However, when evaluated with the stricter 0.15m threshold, performance differences become substantially more pronounced. These results confirm that PHUMA-trained policies achieve more precise motion tracking, producing imitations that remain accurate even under stringent evaluation criteria. The threshold analysis validates our choice to adopt the 0.15m threshold as more meaningful measure of imitation quality. 21 Table 11: Performance Comparison based on Success Threshold in PHUMA Test Success Threshold=0.15m Success Threshold=0.5m Hours Total Stationary Angular Vertical Horizontal Total Stationary Angular Vertical Horizontal Dataset (a) G1 2.4 LaFAN1 AMASS 20.9 Humanoid-X 231.4 PHUMA 73.0 (b) H1-2 LaFAN1 2.4 20.9 AMASS Humanoid-X 231.4 PHUMA 73.0 46.1 76.2 50.6 92.7 62.0 54.4 49.7 82. 66.1 88.5 78.4 95.6 79.3 74.9 74.6 91.5 36.2 72.1 43.0 91.7 54.7 45.9 40.4 79.5 24.0 56.8 26.0 86.0 26.6 17.2 17.0 68. 42.5 66.8 31.8 85.6 58.9 49.6 37.3 68.4 74.8 90.2 78.4 97.1 70.8 70.4 54.8 92.0 87.8 95.0 91.3 98.7 92.4 86.3 78.5 96. 69.2 87.9 72.9 96.5 66.7 62.6 45.2 89.7 47.1 81.1 59.5 94.4 56.4 41.4 22.1 85.6 72.6 83.7 65.9 92.5 68.2 65.9 43.2 79. Table 12: Performance Comparison based on Success Threshold in Unseen Video Success Threshold=0.15m Success Threshold=0.5m Hours Total Stationary Angular Vertical Horizontal Total Stationary Angular Vertical Horizontal Dataset (a) G1 2.4 LaFAN1 AMASS 20.9 Humanoid-X 231.4 PHUMA 73.0 (b) H1-2 2.4 LaFAN1 AMASS 20.9 Humanoid-X 231.4 PHUMA 73.0 28.4 70.2 39.1 82. 70.8 64.3 60.5 78.6 46.9 90.7 78.0 96.7 92.4 87.3 88.3 97.5 28.4 75.0 39.6 88.0 66.7 59.7 60.0 76.8 19.6 62.7 23.0 71. 56.4 46.0 48.7 74.5 10.5 44.1 6.5 67.1 68.2 63.9 39.7 63.8 78.2 92.3 84.1 93.7 85.5 80.4 68.7 89.9 85.5 99.2 98.3 100. 97.5 93.3 93.3 99.2 70.8 92.1 79.9 96.8 79.0 69.9 65.1 89.4 76.3 82.1 76.0 85.9 77.5 72.8 60.2 84.6 80.8 88.0 76.2 84. 90.0 89.0 50.5 83."
        }
    ],
    "affiliations": [
        "KAIST"
    ]
}