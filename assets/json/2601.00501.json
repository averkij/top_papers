{
    "paper_title": "CPPO: Contrastive Perception for Vision Language Policy Optimization",
    "authors": [
        "Ahmad Rezaei",
        "Mohsen Gholami",
        "Saeed Ranjbar Alvar",
        "Kevin Cannons",
        "Mohammad Asiful Hossain",
        "Zhou Weimin",
        "Shunbo Zhou",
        "Yong Zhang",
        "Mohammad Akbari"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce CPPO, a Contrastive Perception Policy Optimization method for finetuning vision-language models (VLMs). While reinforcement learning (RL) has advanced reasoning in language models, extending it to multimodal reasoning requires improving both the perception and reasoning aspects. Prior works tackle this challenge mainly with explicit perception rewards, but disentangling perception tokens from reasoning tokens is difficult, requiring extra LLMs, ground-truth data, forced separation of perception from reasoning by policy model, or applying rewards indiscriminately to all output tokens. CPPO addresses this problem by detecting perception tokens via entropy shifts in the model outputs under perturbed input images. CPPO then extends the RL objective function with a Contrastive Perception Loss (CPL) that enforces consistency under information-preserving perturbations and sensitivity under information-removing ones. Experiments show that CPPO surpasses previous perception-rewarding methods, while avoiding extra models, making training more efficient and scalable."
        },
        {
            "title": "Start",
            "content": "CPPO: Contrastive Perception for Vision Language Policy Optimization Ahmad Rezaei1* Mohsen Gholami1 Mohammad Asiful Hossain1 Saeed Ranjbar Alvar1 Kevin Cannons1 Shunbo Zhou2 Yong Zhang1 Zhou Weimin2 Mohammad Akbari1 1Huawei Technologies Canada Co. Ltd. 2Huawei Cloud Code 6 2 0 2 1 ] . [ 1 1 0 5 0 0 . 1 0 6 2 : r Figure 1. CPPO vs. prior perception-rewarding methods. Prior work follows three strategies: (1) Visionary-R1 and Vision-SR1 force the policy to generate separated perception from reasoning, followed by an LLM perception reward, (2) Perception-R1 uses ground-truth CoT and an LLM as judge to provide perception reward, and (3) PAPO applies perception loss to all rollout tokens. In contrast, CPPO uses entropy of the output tokens to identify perception tokens and assigns Contrastive Perception Loss (CPL) exclusively to these tokens."
        },
        {
            "title": "Abstract",
            "content": "We introduce CPPO, Contrastive Perception Policy Optimization method for finetuning visionlanguage models (VLMs). While reinforcement learning (RL) has advanced reasoning in language models, extending it to multimodal reasoning requires improving both the perception and reasoning aspects. Prior works tackle this challenge mainly with explicit perception rewards, but disentangling perception tokens from reasoning tokens is difficult, requiring extra LLMs, ground-truth data, forced separation of perception from reasoning by policy model, or applying rewards indiscriminately to all output tokens. CPPO addresses this problem by detecting perception tokens via entropy shifts in the model outputs under perturbed input images. CPPO then extends the RL objective function with Contrastive Perception Loss (CPL) that enforces consistency under information-preserving perturbations and sensitivity *Corresponding Author: ahmad.rezaei2@huawei.com Co-Leads under information-removing ones. Experiments show that CPPO surpasses previous perception-rewarding methods, while avoiding extra models, making training more efficient and scalable. 1. Introduction Reinforcement learning (RL) with verifiable rewards has emerged as an effective finetuning method. Notably, [6] showed the potential of language models to develop reasoning capabilities without explicit step-by-step supervision, focusing on their self-evolution through pure RL process. In contrast, visionlanguage models (VLMs) often exhibit weaker multimodal reasoning performance compared to their language-only counterparts [2, 3, 9, 34]. Given the success of RL in language models, recent research has focused on extending RL-based methods to VLMs and multimodal reasoning [16, 18, 27, 28]. In the language-only setting, the policy model draws on its internal knowledge to generate step-by-step logical inference tokens, which we refer to as reasoning tokens. For VLM policy, however, accurate perception is also required to generate query-relevant factual tokens from the image. We refer to these tokens that encode image information as perception tokens. [27] shows that wrong perception tokens are significant source of failures in multimodal reasoning. However, RL algorithms with verifiable final-answer rewards (e.g., [6]) do not separate perception from reasoning errors. This design is problematic, since inaccurate perception tokens will lead to an incorrect final answer, even with correct reasoning steps. Therefore, achieving the optimal policy is difficult when all output tokens are penalized based on the final answer alone. This limitation raises two questions: 1) How can the output perception and reasoning tokens be disentangled for VLM policy? 2) How to best define an explicit perception loss/reward? To address the first question, [28] and [17] force the policy model to separate perception from reasoning, placing perception within <perception> tags and reasoning within <think> tags. However, forcing separation between perception and reasoning disrupts the natural reasoning process of the model, making it difficult to apply to many tasks (e.g., with complex images). In addition, the process becomes vulnerable to reward hacking (where the model places the final answer in the perception section to maximize reward). Thus, we argue that perception and reasoning should be disentangled within the natural generation flow of the model. In order to address the second question, Visionary-R1 [28], Vision-SR1 [17], and Perception-R1 [29] rely on an LLM and utilize either the policys own perceptual outputs or ground-truth Chain-of-Thought (CoT) annotations to compute perception rewards. Such evaluation of perception outputs with LLMs still require explicit separation of perception from reasoning, incur computational overhead, and rely on unscalable CoT supervision. PAPO [27] takes different approach via KL divergence loss between model outputs conditioned on the original and corrupted versions of the images. However, the KL divergence is unbounded, which can easily cause reward collapse and makes the methods hyperparameter sensitive. Moreover, PAPO applies the perception loss uniformly across all tokens and output rollouts, regardless of whether they correspond to perception or reasoning, or whether the outputs are correct or incorrect. Applying divergence over reasoning tokens leads to over-regularization, while maximizing divergence on wrong perception tokens effectively reinforces incorrect perception outputs. Motivated by these observations, we propose Contrastive Perception Policy Optimization (CPPO), an RL solution CPPO integrates two main components into the training process: (1) mechanism that uses policys own output probability distribution to determine the tokens in generated response that the policy most strongly considers as perception tokens in its current state, and (2) token-level Contrastive Perception Loss (CPL) incorporated into the RL objective to enforce differential sensitivity to vision information. Specifically, in each training step, we compare the policys entropy for each token within responses when policy is conditioned on the original image as well as perturbed image with information-removing augmentations. Tokens whose entropy increases the most under this perturbation are selected as perception tokens by the policy, since their distribution exhibits the highest mutual information with the image. After identifying vision-dependent tokens in the policys output, we compute the token-level CPL term. Unlike prior work, CPL is an unsupervised perception contrastive loss that does not require additional CoT supervision or proprietary models. Specifically, for each input image, we create two other variants: an information-preserving perturbation that retains query-relevant content and an informationremoving perturbation that obscures such information. CPL is then implemented as an InfoNCE contrastive loss [4]: the token probability distribution conditioned on the original image serves as the anchor, the distribution under the information-preserving perturbation as the positive, and the distribution under the information-removing perturbation as the negative sample. Crucially, the contrastive loss is applied only to perception tokens from correct rollouts, ensuring that anchors correspond to accurate and verified perception tokens. This provides targeted perception feedback to the policy, thereby improving its visual grounding capability. In summary, the major contributions of our work are as follows: We propose CPPO, an RL-based finetuning solution tailored for VLMs to disentangle perception and reasoning improvement of the policy. We propose an entropy-based perception token detection method, where the VLM policy identifies its own perception tokens using its output distribution. We propose CPL, an unsupervised perception-specific contrastive loss to optimize VLM policy. We show the superiority of CPPO compared with prior perception-specific RL methods. 2. Related Work In this section, we categorize the related RL methods proposed for VLMs into three directions: 1) sampling and rollout augmented methods, 2) RL combined with SFT or offpolicy data, and 3) perception-aware approaches. Our approach falls into the third category, while the other directions are orthogonal to our method. We also discuss the background of using contrastive learning in RL. Sampling and Rollout Augmented RL with VLMs. This line of work improves robustness and training efficiency by mixing trajectories from clean and moderately distorted images during RL training. NoisyRollout [18] 2 Figure 2. An overview of CPPO. For each rollout oi, perception tokens are identified and their probability distributions are comthe original image (anchor sample: πθ(oi,t)), an information-preserving perturbation + (positive puted under three conditions: θ (oi,t)(cid:1) sample: π+ and sim(cid:0)πθ(oi,t), π θ (oi,t)(cid:1) are computed and incorporated into the Contrastive Perception Loss (CPL), which serves as an additional perception-specific term in the RL objective. Notations are simplified for brevity. θ (oi,t)), and an information-removing perturbation (negative sample: π θ (oi,t)). Similarities sim(cid:0)πθ(oi,t), π+ and Vision Matters [16] use input perturbations to stabilize grounding and enhance generalization. Shuffle-R1 [38] introduces pairwise trajectory sampling and advantage-based batch reshuffling to improve gradient signal quality and increase exposure to valuable rollouts. VL-Rethinker [25] proposes selective sample replay to address the vanishing advantages problem and forced rethinking, which appends trigger token to enforce self-reflective reasoning. This line of work is orthogonal to CPPO. RL Combined with SFT or Off-Policy with VLMs. This line of research combines on-policy RL with off-policy CoT or SFT training. Vision-R1 [14], Look-back [33], OpenVLThinker [7], VisionThink [32], and [23] focus on semi-off-Policy RL with emphasis on rethinking, iterative pipelines, or off-policy data to enhance slow-thinking reasoning and overall training stability. Similar to the prior category, this line of work is also orthogonal to our work. Perception-Aware RL with VLMs. This category focuses on improving the interaction between perception and reasoning of the policy model. One line of work proposes decoupled architectures such as [11] and [10] that use VLM for visual description and an LLM for reasoning, optimized jointly with RL. Another direction explicitly separates perception from reasoning in the output space of VLMs: Visionary-R1 [28] and Vision-SR1[17]. Both of these works enforce the policy model to put perception between <perception> tokens (or similar tokens) and thinking between <think> tokens. The perception tokens are then fed to an LLM to obtain the perception reward. Instead of forcing the model to separate perception from thinking, Perception-R1 [29] uses supervised CoT trajectory to evaluate the perception components of the reasoning trajectory and provides explicit perception rewards. All of these prior works either call an LLM (or the VLM itself) for second round to answer the question given the perception part or check whether the perception component matched the ground-truth CoT. This design makes inference slower, while also being error-prone when being limited to using small models as judge. PAPO [27] proposes an additional unsupervised KL divergence loss between the models outputs conditioned on original and corrupted versions of the images. Such KL divergence has an unbounded nature that can result in collapsed rewards. Contrastive Learning in RL. Contrastive learning has also been explored in RL as way to learn more robust and discriminative representations. Prior works such as CURL [15], SPR [22], SODA [12], and TACO [37] leverage contrastive objectives on latent features to improve sample efficiency and generalization in visual RL tasks. Recently, contrastive methods have also been explored for alignment with human feedback. Contrastive Preference Learning [13] proposes learning directly from human feedback signals without relying on standard RLHF pipelines, by using contrastive objective to distinguish preferred behaviors. Similarly, Contrastive Preference Optimization (CPO) [31] applies this principle in the context of LLMs, showing that contrastive objectives can outperform traditional RL-based preference optimization in domains like machine translation. While these methods highlight the versatility of contrastive learning across RL and alignment, VLM policy optimization remains unexplored. Our approach introduces token-level contrastive loss tailored to VLMs, that is applied specifically to vision-dependent tokens within reasoning rollouts. Algorithm 1 CPPO: Contrastive Perception Policy Optimization Require: policy πθ, rollout policy πθold , dataset D, RL objective , contrastive perception loss weight λ, top-k ratio 3. Method In this section, we first discuss RL with verifiable rewards in the preliminaries and then elaborate our proposed perception token detection and unsupervised contrastive perception loss. 3.1. Preliminaries Group Relative Policy Optimization (GRPO). GRPO [6] includes RL fine-tuning of the policy VLM πθ with parameters θ on verifiable tasks. Given an input set = {q, I} including query and image I, group of output trajectories (responses) {o1, . . . , oG} πθ( x) are sampled. Each output oi consists of tokens {oi,1, . . . , oi,t, . . . , oi,T } and receives scalar reward R(oi), typically reflecting correctness. Relative advantages are computed as: Ai = R(oi) mean(cid:0)R(o1:G)(cid:1) std(cid:0)R(o1:G)(cid:1) , (1) where [1, G]. The GRPO objective is then defined as: JGRPO(θ) = Eoiπθold 1 min(cid:0) πθ(oi,t x, oi,<t) πθold (oi,t x, oi,<t) (cid:88) i=1 1 oi oi (cid:88) (cid:8) t=1 Ai, clip( πθ(oi,t x, oi,<t) πθold (oi,t x, oi,<t) , 1 ϵ, 1 + ϵ) Ai (cid:1) βKL[πθπref ](cid:9), (2) where the KL penalty controls the deviation from the frozen reference policy πref with weight β. Output trajectories are generated by the rollout policy πθold. The hyperparameter ϵ controls clipping large policy updates. In this setting, the correctness reward alone provides no explicit signal to enhance the policy models perceptual sensitivity. Our CPL loss aims to address this gap. 3.2. CPPO: Contrastive Perception Policy Optimization While prior work has explored guiding the policy model toward improved perceptual understanding by providing explicit vision rewards, our approach is different as it augments the RL objective function with perceptionFigure 2 and Algorithm 1 dependent contrastive loss. illustrate the overall proposed framework. Inspired by contrastive representation learning [4], the central idea of CPPO is to encourage the policy to be differentially sensitive to visual perturbations in the input image at the token level. At each training step, the policy generates response Sample input (q, I) Generate rollout oi πθold ( q, I) Compute reward R(oi) and advantage Ai for each rollout oi do Perception Token Detection Create information-removing image Compute entropy increase Hi,t for all tokens Select top-k positive Hi,t tokens: Sperception(oi) 1: for each training step do 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: end for for each rollout oi do LCPL(oi) = Contrastive Perception Loss Create information-preserving image + For all Sperception(oi): Compute contrastive loss LInfoNCE 1 Sperception(oi) LInfoNCE (cid:80) i,t i,t 15: 16: end for (θ) = (θ) λ 1[Ai > 0] LCPL(oi) With RL Objective With Advantage Gating Update policy θ θ + θJ (θ) Update rollout policy θold θ 17: 18: 19: end for Combine oi for the input = {q, I}. Our approach begins by identifying the subset of perception tokens within oi. We then introduce the additional CPL term in RL objective, which is applied to the policys probability distribution over the detected perception tokens. Specifically, CPL recomputes policys probability distribution for each perception token under two variants of the input image I: Information-removing perturbations , obtained from transformations such as region masking, occlusion, or deletion of critical visual elements that obscure query-relevant information. The policys output distribution, πθ(oi,t q, , oi,<t) should diverge from that of πθ(oi,t q, I, oi,<t). Information-preserving perturbations +, obtained from transformations such as mild Gaussian noise, small brightness shifts, or rotations that do not remove query-relevant content. The output distribution, πθ(oi,t q, +, oi,<t) should remain consistent with that of πθ(oi,t q, I, oi,<t). In other words, CPL encourages the models confidence regarding visual information in generated response about image to remain stable if is altered with irrelevant perturbations, but decrease appropriately when perturbations remove or obscure query-relevant content. Notably, this is achieved in an unsupervised manner, without relying on any CoT data. 3.2.1. Perception Token Detection Not all tokens in an output are equally dependent on perceptual input. For example, interpreting the base is 10 cm relies on visual info, whereas solving x2 + 2x + 1 = 0 or recalling that the angles of triangle sum to 180 can be performed independently of the image. Applying CPL uniformly across all tokens may lead to excessive regularization and destabilize the training. To handle this issue, we propose mechanism to selectively identify perceptiondependent tokens within each trajectory using models own output distribution. By applying CPL only to these tokens, the model is guided to be sensitive to relevant visual info, while maintaining stability for general reasoning. Proposition 1 (Entropy increase as proxy for perception dependence). (Proof in the supplementary materials) Let denote the original image, perturbed variant that removes query-relevant perceptual information, and oi the ith sequence of tokens generated by the policy when conditioned on I. The increase in entropy of token oi,t oi, when the policy is conditioned on rather than I, serves as proxy for the degree to which the policy model associates oi,t with the query-relevant visual content of I. This increase is calculated as follows: Hi,t = H(oi,tq, , oi,<t) H(oi,tq, I, oi,<t). (3) For each token in the ith generated sequence of tokens {oi,1, . . . , oi,T }, the predictive entropy of the model at position is defined as: H(oi,tx, oi,<t) = (cid:88) oi,tV πθ(oi,t x, oi,<t) log πθ(oi,t x, oi,<t),, (4) where denotes the vocabulary. This entropy measures the level of uncertainty in predicting the next token based on the input query and image. Perception-Topk. We use the criterion in Proposition 1 to identify the most relevant tokens in each response oi with respect to the image. After generating oi for image I, we construct by randomly applying perturbation from set of information-removing perturbations. Given oi and , we compute πθ(oi,t q, , oi,<t) and measure the corresponding change in entropy (Hi,t) for each token oi,t. Tokens are then ranked by Hi,t, and the topk most perception-dependent tokens are retained. Formally, we define Sperception as the set of token indices in each response: Sperception = { Rank(Hi,t) } , (5) where denotes the proportion of tokens with the highest entropy increase, to which that receive the CPL loss is applied. Finally, we construct binary mask vector Mi {0, 1}T for the ith response: Mi,t = (cid:40) 1, 0, if Sperception, otherwise. (6) 3.2.2. Contrastive Perception Loss (CPL) After identifying perception tokens via our entropy-based criterion, we now define the token-level CPL. Besides the created with obscured query-relevant information, we generate + by sampling perturbation from set of information-preserving perturbations. For each perception token oi,t (i.e., Mi,t = 1) in each rollout, we treat the policy probability distribution under the original image as the anchor πθ(oi,t) = πθ(oi,t q, I, oi,<t), the distribution under + as the positive sample π+ θ (oi,t) = πθ(oi,t q, +, oi,<t), and the distributions under as the negative sample π q, , oi,<t). Let sim(p, p) = KL(p p) denote the negative KL divergence as an estimate similarity between token probability distributions. Then, our contrastive loss term is defined by adopting the InfoNCE loss [4]: θ (oi,t) = πθ(oi,t LInfoNCE oi,t (cid:40) = log sim(πθ (oi,t),π+ θ (oi,t))/τ + sim(πθ (oi,t),π θ (oi,t))/τ θ (oi,t))/τ (cid:41) sim(πθ (oi,t),π+ (7) where τ > 0 is temperature hyperparameter. Minimizing this loss encourages the anchor distribution πθ(oi,t) to remain close to the positive view π+ θ (oi,t) while being pushed away from the negative view π θ (oi,t). For all tokens in the ith trajectory oi, the CPL is defined as: LCPL,i,t ="
        },
        {
            "title": "LInfoNCE",
            "content": "oi,t 0, if Mi,t = 1, if Mi,t = 0. (8) That is, non-perception tokens (Mi,t = 0) are excluded from the CPL. The overall CPL for the ith trajectory oi is obtained by averaging over its tokens: LCPL(oi; I, +, ) = 1 oi oi (cid:88) t=1 LCPL,i,t. (9) Integration with GRPO. Finally, we integrate CPL with the GRPO objective. For each sampled trajectory oi, we compute the standard GRPO update (Eq. 2) along with the CPL. To prevent low-quality trajectories from introducing noisy gradients, we use an advantage gating mechanism, whereby CPL is only applied when the trajectorys grouprelative advantage Ai is positive. Formally, we maximize 5 Table 1. CPPO vs. prior works. All results are based on avg@8. For prior methods, we used their released checkpoints. Bold: the best value in each column. Underlined: the second best. Methods MVistam DMath WeMath MVisionm MVerse MMMU-Pv LogicVista Avg. Math Benchmarks Visual Reasoning GPT4-o Gemini-2.0-Flash Qwen2.5-VL-3B OpenVLThinker Visionary-R1 PAPO GRPO CPPO (ours) Qwen2.5-VL-7B OpenVLThinker Vision-SR1 Look-Back Vision-Matters PAPO PerceptionR1 NoisyRollout GRPO CPPO (ours) 60.0 73.4 56.4 60.0 61.4 64.8 63.7 66. 65.6 70.7 67.0 69.1 68.6 71.6 70.0 71.1 71.2 72.2 34.5 42.1 33.7 35.6 41.2 45.4 45.7 48.9 53.2 43.9 52.6 52.5 54.5 54.7 55.8 55. 55.6 56.9 47.4 45.8 14.5 26.3 27.1 28.1 28.4 30.8 33.3 38.4 33.6 39.8 40.1 39.5 45.4 44.4 42.4 44. 30.6 41.3 19.5 22.3 19.7 24.3 25.1 25.3 24.5 27.5 28.0 25.8 25.2 26.5 27.6 29.4 27.6 29.9 41.2 54. 25.7 36.9 34.5 38.3 38.3 39.4 41.2 40.7 40.7 41.9 45.3 44.5 46.0 46.4 45.0 46.5 51.9 51.7 19.9 25.0 27.9 26. 25.8 28.5 33.7 35.5 38.9 34.5 35.5 38.7 38.1 38.5 37.9 39.0 52.8 52.3 32.4 37.4 37.1 39.4 37.7 40. 45.1 45.8 43.2 46.3 45.1 45.8 45.5 47.9 47.4 48.2 45.4 51.6 28.8 34.7 35.5 38.1 37.8 40.0 42.3 43.9 43.9 44.8 45.3 46.8 47.3 47. 46.7 48.2 the following combined objective: (θ) = Eoiπθold (cid:104) JGRPO(θ) λ 1 i=1 (cid:88) (cid:8)1{Ai > 0} LCPL(oi; I, +, )(cid:9)(cid:105) , (10) where 1{} is the indicator function. The hyperparameter, λ, controls the strength of perceptual grounding. By incorporating the advantage gating mechanism, we ensure that CPL acts as an auxiliary constraint only on trajectories that improve upon the group baseline, thereby aligning visual regularization with successful reasoning behaviors. 4. Experiments 4.1. Experimental Setup Training Dataset. We train on ViRL39K [25], dataset consisting of 38.8K multimodal questionanswer pairs. The dataset spans broad range of domains, including grade school problems to broader STEM and social topics; reasoning with charts, diagrams, tables, documents, and spatial relationships. Evaluation. Following prior works, we use the following benchmarks for evaluation: LogicVista [30], MathVista [20], DynaMath [39], WeMath [21], MathVision [26], MathVerse [36], and MMMU-Pro-Vision [35]. These benchmarks encompass math, general multimodal reasoning, and logical reasoning tasks. All evaluations are per6 formed using VLMEvalKit [8]. We report average accuracy@8 with an inference temperature of 1.0 to provide more consistent and reliable measure of model performance across all the experiments in the paper. Baselines. We use Qwen2.5-VL-3B and 7B [1] as the backbone models in all our experiments. We compare our CPPO with recent RL methods proposed for VLMs: OpenVLThinker-3B/7B, Visionary-R1-3B, PAPO3B/7B, VL-ReThinker-7B, Vision-Matters-7B, PerceptionR1-7B, Vision-SR1-7B, NoisyRollout-7B, and Look-Back7B (semantic checkpoint). All of these prior works use Qwen2.5-VL-3B/7B as the policy model and, therefore, comparisons are fair. Perturbation Types. For information-removing perturbations, we employ random 80% patch-wise masking and random 30% cropping (retaining only 30% of the image) to obscure the majority of the visual content. For informationpreserving perturbations, we apply lightweight transformations such as color jitter, random perspective, random rotation, and Gaussian noise, which modify the image appearance without eliminating critical information. At each training step, one augmentation is randomly sampled from each augmentation set. Detailed parameter settings and illustrative examples of all perturbations are provided in the supplementary materials. Implementation Details. The policy models are initialized with Qwen2.5-VL-3B/7B. We train the policy model with GRPO and CPPO for 2 epochs on the ViRL39K dataset with group size of 5 and global batch size of 512. Both Figure 3. Sample outputs generated with CPPO with top 40% detected perception tokens. the vision encoder and LLM of the baselines were updated during training. More details are in the supplementary materials. 4.2. Main Results The performance of closed-source models (GPT4-o, Gemini-2.0-Flash), the backbone models, and RL-based baseline VLMs including GRPO compared with our CPPO is reported in Table 1. Comparison to Baseline GRPO. Applying CPPO to the Qwen2.5-VL-3B and -7B baselines yields consistent and substantial improvements on the test benchmarks, with average performance gains of 11.2% and 5.9%, respectively. As reported in Table 1, CPPO achieves higher accuracy than GRPO across all benchmarksaverage 40.0% vs. 37.8% for the 3B model and 48.2% vs. 46.7% for the 7B model. Overall, these results confirm that CPPO is more effective optimization strategy than GRPO, especially for mid-sized models, and establishes CPPO as strong and scalable alternative for finetuning large VLMs. Qualitative results are given in the supplementary materials. Comparison to Other Methods. As shown in Table 1, CPPO consistently surpasses prior methods across all benchmarks for the 3B model. For the 7B model, CPPO also outperforms existing approaches on all benchmarks (except WeMath), demonstrating stronger generalization. In particular, when compared to PAPOthe most relevant perception-aware RL baselineCPPO achieves notable gains. On the 3B model, CPPO improves average performance to 40.0%, compared to PAPOs 38.1%. On the larger 7B model, CPPO reaches 48.2% versus PAPOs 46.8%. Importantly, both CPPO and PAPO are trained under identical conditionsusing the same dataset (ViRL39K) and the same number of training stepsensuring that the improvements are not due to differences in data or compute. Thus, the consistent advantage of CPPO over PAPO can be attributed directly to the introduction of contrastive loss on perception tokens, which enhances the models ability to capture and leverage visual information more effectively. Performance of Perception Token Detection. Figure 3 shows two samples, the policy models outputs generated by CPPO, and the top 40% of perception tokens identified using our entropy-based method. In the 1st example, the question asks for the value of angle CAD in geometry problem. The key visual clues needed to solve this question are: (1) CDA = 1 = 40, (2) CAD forms triangle, and (3) 2 = ACD. With these three pieces of information alone, one could solve the problem without referring back to the original figure. We observe that all these critical elements are successfully highlighted within the top 40% of selected perception tokens. The 2nd example shows stem and leaf plot summarizing the number of menu items per restaurant in town, which is used to answer question. Here, we find that most of the relevant numerical values are also captured within the top 40% of detected perception tokens, illustrating that the method effectively identifies the essential visual information for the question. Numerical analysis is given in the supplementary materials. 4.3. Out-of-Domain Performance Figure 4 shows the training dynamics of CPPO vs. GRPO (Training Reward), reward on the in-domain validation set (Geometry3K Validation Reward), and detailed accuracy comparison across out-of-domain benchmarks as training progress. The training reward shows that CPPO leads to faster learning as well as strong out-of-distribution general7 Figure 4. CPPO vs. GRPO (avg@8) on Qwen2.5-VL-3B across in-domain and out-of-domain scenarios. The X-axis represents RL training steps. The shaded area corresponds to one standard deviation over 8 responses. 1st column: Reward comparison on the in-domain dataset during training. 2nd and 3rd columns: Comparison on four out-of-domain visual reasoning benchmarks. ization from the early steps of training. 4.4. Ablations We adopt Qwen2.5-VL-3B as the baseline model and conduct all ablations on the Geometry3K dataset [19], which contains 2.1K samples. We select Geometry3K both to enable faster training and to demonstrate the generalizability of our approach across different training datasets. Ablation on Main Components of CPPO. Table 2 reports the ablation study on the key components of CPPO. Starting from GRPO, applying CPL to all tokens raises the average accuracy from 34.7% to 35.0%. Restricting CPL to only the top 50% of perception tokens yields larger gain, increasing accuracy to 36.6%. Finally, introducing advantage gatingwhere the contrastive loss is applied only to rollouts with positive advantagefurther improves performance to 38.6%. These results highlight that each component makes meaningful contribution, and together they account for the overall effectiveness of CPPO. Topk. Table 3 presents the analysis of different values for topk perception token detection. The results show that average accuracy improves as increases from 5% to 50%, but declines when is further expanded from 50% to 100%. We hypothesize that this trend arises because larger values include more tokens that the policy model is already confident about (i.e., tokens with lower entropy change), which are less informative perception tokens. Incorporating these tokens can slow down the training and ultimately lead to worse performance when models are trained for the same number of epochs. Table 2. Ablation on the main components of CPPO. Methods Qwen2.5-VL-3B GRPO + CPL on All Tokens + CPL on Topk Perc. Tokens + Advantage Gating LogicVista MVistam MVisionm WeMath Avg. 32.4 35.4 35.6 36.4 38.5 56.4 55.9 56.0 56.6 59.9 19. 20.9 20.8 22.5 23.1 14.5 26.7 27.2 30.9 32.9 30.7 34.7 35.0 36.6 38.6 Table 3. Experiments on topk perception tokens. LogicVista MVistam MVisionm WeMath Avg. 5% 25% 50% 75% 100% 32.5 36.7 38.5 37.6 36.3 52.2 57.9 59.9 57.4 56.9 21.4 22.7 23.1 22.3 20.1 30.7 32.9 29.1 29.5 31.6 37.0 38.6 36.6 36.2 Table 4. Experiments on λ values. λ LogicVista MVistam MVisionm WeMath Avg. 0.01 0.02 0.03 0. 37.4 38.5 38.6 35.6 59.2 59.9 57.8 55.9 21.9 23.1 22.9 21.7 31.4 32.9 28.8 27.6 37.5 38.6 37.0 35.2 with λ = 0.02. In general, CPPO with different λ values outperforms GRPO with an average accuracy of 34.7% as reported in Table 2. Complexity analysis vs. GPRO and more ablations on image perturbations are given in supplementary materials. 5. Conclusion Loss Weighting (λ). We experiment with different λ values in Eq. 10. λ controls the strength of perceptual grounding. As given in Table 4, the best performance is obtained In this work, we introduced CPPO, perception-aware RLbased method for finetuning VLMs. CPPO leverages an entropy-based approach to disentangle perception tokens from reasoning tokens, where perception tokens capture visual information extracted from the input image. To better align training with perception quality, we proposed Contrastive Perception Loss (CPL)an unsupervised, modelfree objective that penalizes perception errors. Extensive experiments demonstrate that CPPO outperforms recent RL methods for VLMs, achieving state-of-the-art performance across multiple math and visual reasoning benchmarks. We discuss the limitations of our approach and directions for future work in the supplementary materials. 9 CPPO: Contrastive Perception for Vision Language Policy Optimization"
        },
        {
            "title": "Supplementary Material",
            "content": "6. Proof for Proposition 1 Proposition 1 (Entropy increase as proxy for vision dependence). Let denote the original image, perturbed variant that removes query-relevant perceptual information, and oi the sequence of tokens generated by the policy when conditioned on I. The increase in entropy of token oi,t oi, when the policy is conditioned on rather than I, serves as proxy for the degree to which the policy model associates oi,t with the query-relevant visual content of I. This increase is calculated as follows: Hi,t = H(oi,tq, , oi,<t) H(oi,tq, I, oi,<t). Proof. Recall the identity relating conditional mutual information (denoted by I) and conditional entropy: H(oi,t X, q, oi,<t) = H(oi,t q, oi,<t) I(oi,t; q, oi,<t)) . (11) Applying this with both = and = and subtracting, we obtain Hi,t(I ) Hi,t(I) = H(cid:0)oi,t , q, oi,<t I(oi,t; q, oi,<t) I(cid:0)oi,t; (cid:12) (cid:1) H(oi,t I, q, oi,<t) = (cid:1) . (cid:12) q, oi,<t (12) is obtained from by an information-removing augmentation that obscures query-relevant visual information. Our main assumption is that the conditional mutual information between perception tokens in oi and should be greater than their conditional mutual information with the perturbed image . Formally, if oi,t is perception token, we assume the following inequality holds for its conditional mutual information: I(oi,t; q, oi,<t)M I(cid:0)oi,t; (cid:12) (cid:1) 0. (13) (cid:12) q, oi,<t Substituting this inequality into (12) yields Hi,t(I ) Hi,t(I) 0. (14) Thus, an increase in predictive entropy, Hi,t, serves as principled proxy for identifying vision-dependent tokens in the output sequence. 7. Image Perturbation Details Figure 5 illustrates examples from the training dataset along with two categories of perturbations: information-removing 10 and information-preserving. The information-removing perturbations, such as random occlusion and random zoom crop, eliminate key visual details necessary for understanding the image. In contrast, the information-preserving perturbationsincluding color jitter, random perspective, random rotation, and Gaussian blurmodify the image without discarding critical information. Table 5 shows the Torchvision parameters selected for each perturbation. Table 5. Selected image perturbation parameters. Perturbation Parameters Color Jitter Brightness: (0.2, 1.3) Contrast: (0.2, 1.8) Saturation: (0.2, 1.8) Random Perspective Distortion Scale: 0.2 Random Rotation Gaussian Blur Degrees: 10 Kernel Size: 3 Random Occlusion 80% Patch-wise Masking Random Zoom Crop Retain 30% of Image 7.1. Validating the Selected Perturbations In order to evaluate whether the selected perturbations effectively preserve or remove information in images, we applied each perturbation to the images in four benchmarks: LogicVista [30], MathVista Mini [20], MathVision Mini [26], and WeMath [21]. We then evaluated the Qwen2.5-VL-7B base model on each augmented benchmark to determine whether model performance was preserved or degraded after the perturbations. As shown in Table 6, information-preserving augmentationscolor jitter, random perspective, and Gaussian blurreduced average model performance by less than 1%, while random rotation resulted in 1.5% reduction. These results indicate that, for the majority of images in these benchmarks, the information required to answer the questions remains intact after applying these augmentations. In contrast, informationremoving perturbations caused the models average performance to drop by more than 14% for both augmentation types, demonstrating that these perturbations successfully remove the essential information needed to answer the questions. Together, these findings confirm the effectiveness of the selected perturbations within our training pipeline. Figure 5. Sample information-removing perturbations and information-preserving perturbations. Table 6. Performance of the Qwen2.5-VL-7B base model on each benchmark when benchmark images are perturbed using each of the selected perturbation types. Performance remains close to the original (no-perturbation) baseline for information-preserving perturbations, but drops significantly when information-removing perturbations are applied. All results are based on avg@8. Image Perturbation Type LogicVista MVistam MVisionm WeMath Avg. Original Images (No-Perturbation) 45. 65.6 24.5 33.3 42.1 Information Preserving Perturbations Color Jitter Random Perspective Random Rotation Gaussian Blur Information Removing Perturbations Random Occlusion Random Zoom Crop 44.5 44.4 43.9 44.2 31.3 31.2 64.3 64.0 63.4 64.4 40.9 42. 24.6 25.3 24.0 25.3 19.3 20.1 31.6 31.1 31.4 31.6 41.3 41.2 40.7 41.4 13.8 15.1 26.3 27. 8. Complexity Analysis: Training-Time and Performance of CPPO vs. GRPO for the same number of epochs under identical computational resources. Compared to the baseline GRPO method, CPPO introduces additional computational complexity due to the two extra forward passes required to compute the token probability distributions conditioned on the positive sample image (I +) and the negative sample image (I ). This added computation increases the total time per training step. As shown in Table 7, training the Qwen2.5-VL-3B model with CPPO for two epochs takes 39% more time than training with GRPO Considering the additional time required by CPPO, one might argue that the same computational budget could instead be used to train the model with GRPO for more steps. To examine whether extended GRPO training can match the performance gains achieved by CPPO, we conducted an experiment in which the Qwen2.5-VL-3B model was trained for two additional epochs on the ViRL39K dataset [25]. As shown in Table 7, this extension effectively doubles the toTable 7. Increase in training time and performance when training the Qwen2.5-VL-3B model with CPPO for 2 epochs or with GRPO for 4 epochs, relative to training with GRPO for 2 epochs. Even 100% increase in GRPO training time does not match the performance achieved by CPPO. All results are based on avg@8. Methods (Relative Increase of Training Time) MVistam DMath WeMath MVisionm MVerse MMMU-Pv LogicVista Avg. GRPO:2 Epochs GRPO: 4 Epochs (100%) CPPO: 2 Epochs (39%) 63.7 65.8 66.3 45.7 47.7 48.9 28.4 28.5 30. 25.1 25.4 25.4 38.3 39.0 39.4 25.8 27.3 28.5 37.7 38.2 40.9 37.8 38.8 40."
        },
        {
            "title": "Visual Reasoning",
            "content": "tal training timeapproximately 100% increaseyet the resulting model still underperforms compared to the model trained with CPPO for only two epochs. This discrepancy demonstrates that the benefits provided by CPPO are not simply byproduct of longer training. Instead, CPPO explicitly encourages the model to improve its visual perception and discrimination between informative and uninformative image regionscapabilities that GRPO fails to acquire even with substantially more training. These findings highlight the advantage of CPPOs learning signal over simply increasing the number of GRPO training steps. Figure 6. Quantitative evaluation of perception token detection. 9. Analysis on Performance of Perception ToTable 8. Summary of training hyperparameter configurations. ken Detection To quantitatively evaluate our perception detection method, we used the inference outputs of Qwen2.5-VL-3B and -7B on four test sets: MathVista-MINI, LogicVista, MathVision-MINI, and WeMath. We then passed these outputs to GPT5-mini, which was used to separate the perception-related information from the rest of the models response. This extracted perception information serves as our ground truth. We measure the accuracy of our detection method by calculating the ROUGE-1 F1 score between the detected perception tokens and the GPT5-mini outputs. It is important to note that GPT5-minis separation is not flawless; thus, this evaluation should be viewed as proof-ofconcept rather than definitive benchmark. Figure 6 shows that the ROUGE-1 F1 score improves as we increase the number of topk perception tokens, up to the point where 100% of perception tokens are included. Here, 100% refers to selecting all tokens with positive in Proposition 1, rather than all output tokens. At each topk percentage, we also select the same number of tokens randomly to serve as baseline. Figure 6 shows that there is significant gap between our entropy-based method and random selection. 10. Extra Implementation Details We use verl [24] as our RL training framework. Table 8 shows the summary of hyper-parameters used in training of 3B and 7B models. Parameter Configuration Qwen2.5-VL-Instruct 512 1.0 1e6 5 2 AdamW Main Results Model Base Global Batch Size Rollout Temperature Learning Rate Rollout Number Training Epochs Optimizer Policy Loss Aggregation token-mean β τ λ 0.01 0.1 50% 0.02 Ablations Specific Dataset Training Epochs Global Batch Size Geometry3K 12 128 11. Qualitative Results Figures 7-9 show three qualitative examples. We observe that CPPO has corrected the perception mistakes of models trained with GRPO. For example, in Figure 7, trained with GRPO states that \"the angle is given as 70 degrees\" that is wrong perception information extracted from the image. However, the model trained with CPPO corrected the model 12 this statement by \"The two line segments form angles that add up to 180 degrees\". Note that when perception tokens are wrong, even with correct reasoning trajectory, the final answer is wrong. 12. Limitations This work has several limitations that should be addressed in future research. First, due to our computational constraints, we did experiments up to 3B and 7B models. Exploring larger VLMs, such as 72B models, is an important direction for future work. Second, our evaluation was limited to Qwen2.5-VL baselines; extending the analysis to other baselines, such as InternVL [5], would provide more comprehensive comparison. Finally, while we demonstrated the effectiveness of CPPO using 40K training samples, future studies should investigate large-scale training with substantially larger datasets. 13 Figure 7. Sample generated responses by CPPO and GRPO. GRPO exhibits perception error that is corrected in the CPPO response. 14 Figure 8. Sample generated responses by CPPO and GRPO. GRPO exhibits perception error that is corrected in the CPPO response. 15 Figure 9. Sample generated responses by CPPO and GRPO. GRPO exhibits perception error that is corrected in the CPPO response."
        },
        {
            "title": "References",
            "content": "[1] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and JunarXiv preprint yang Lin. Qwen2.5-vl technical report. arXiv:2502.13923, 2025. 6 [2] Jing Bi, Susan Liang, Xiaofei Zhou, Pinxin Liu, Junjia Guo, Yunlong Tang, Luchuan Song, Chao Huang, Guangyu Sun, Jinxi He, et al. Why reasoning matters? survey of advancements in multimodal reasoning (v1). arXiv preprint arXiv:2504.03151, 2025. 1 [3] Kevin Cannons, Saeed Ranjbar Alvar, Mohammad Asiful Hossain, Ahmad Rezaei, Mohsen Gholami, Alireza Heidarikhazaei, Zhou Weimin, Yong Zhang, and Mohammad Akbari. From segments to scenes: Temporal understanding in autonomous driving via vision-language model. arXiv preprint arXiv:2512.05277, 2025. 1 [4] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. simple framework for contrastive learning of visual representations. In International conference on machine learning, pages 15971607. PmLR, 2020. 2, 4, 5 [5] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2418524198, 2024. 13 [6] DeepSeek-AI. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. 1, 2, 4 [7] Yihe Deng, Hritik Bansal, Fan Yin, Nanyun Peng, Wei Wang, and Kai-Wei Chang. Openvlthinker: Complex visionlanguage reasoning via iterative sft-rl cycles, 2025. 3 [8] Haodong Duan, Junming Yang, Yuxuan Qiao, Xinyu Fang, Lin Chen, Yuan Liu, Xiaoyi Dong, Yuhang Zang, Pan Zhang, Jiaqi Wang, et al. Vlmevalkit: An open-source toolkit for In Proceedings evaluating large multi-modality models. of the 32nd ACM International Conference on Multimedia, pages 1119811201, 2024. 6 [9] Mohsen Gholami, Ahmad Rezaei, Zhou Weimin, Sitong Mao, Shunbo Zhou, Yong Zhang, and Mohammad Akbari. Spatial reasoning with vision-language models in ego-centric multi-view scenes. arXiv preprint arXiv:2509.06266, 2025. [10] Yunhao Gou, Kai Chen, Zhili Liu, Lanqing Hong, Xin Jin, Zhenguo Li, James Kwok, and Yu Zhang. Perceptual decoupling for scalable multi-modal reasoning via rewardarXiv preprint arXiv:2506.04559, optimized captioning. 2025. 3 [11] Zixian Guo, Ming Liu, Zhilong Ji, Jinfeng Bai, Lei Zhang, and Wangmeng Zuo. Decoupled visual interpretation and arXiv linguistic reasoning for math problem solving. preprint arXiv:2505.17609, 2025. 3 [12] Nicklas Hansen and Xiaolong Wang. Generalization in reIn 2021 inforcement learning by soft data augmentation. IEEE International Conference on Robotics and Automation (ICRA), pages 1361113617. IEEE, 2021. 3 [13] Joey Hejna, Rafael Rafailov, Harshit Sikchi, Chelsea Finn, Scott Niekum, W. Bradley Knox, and Dorsa Sadigh. Contrastive preference learning: Learning from human feedback without reinforcement learning. In The Twelfth International Conference on Learning Representations, 2024. [14] Wenxuan Huang, Bohan Jia, Zijie Zhai, Shaosheng Cao, Zheyu Ye, Fei Zhao, Zhe Xu, Yao Hu, and Shaohui Lin. Vision-r1: Incentivizing reasoning capability in multimodal large language models, 2025. 3 [15] Michael Laskin, Aravind Srinivas, and Pieter Abbeel. CURL: Contrastive unsupervised representations for reinforcement learning. In Proceedings of the 37th International Conference on Machine Learning, pages 56395650. PMLR, 2020. 3 [16] Yuting Li, Lai Wei, Kaipeng Zheng, Jingyuan Huang, Linghe Kong, Lichao Sun, and Weiran Huang. Vision matters: Simple visual perturbations can boost multimodal math reasoning. arXiv preprint arXiv:2506.09736, 2025. 1, 3 [17] Zongxia Li, Wenhao Yu, Chengsong Huang, Rui Liu, Zhenwen Liang, Fuxiao Liu, Jingxi Che, Dian Yu, Jordan BoydGraber, Haitao Mi, and Dong Yu. Self-rewarding visionlanguage model via reasoning decomposition, 2025. 2, 3 [18] Xiangyan Liu, Jinjie Ni, Zijian Wu, Chao Du, Longxu Dou, Haonan Wang, Tianyu Pang, and Michael Qizhe Shieh. Noisyrollout: Reinforcing visual reasoning with data augmentation. arXiv preprint arXiv:2504.13055, 2025. 1, 2 [19] Pan Lu, Ran Gong, Shibiao Jiang, Liang Qiu, Siyuan Huang, Xiaodan Liang, and Song-Chun Zhu. Inter-GPS: Interpretable geometry problem solving with formal language In Proceedings of the 59th Anand symbolic reasoning. nual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 6774 6786, Online, 2021. Association for Computational Linguistics. 8 [20] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. In International Conference on Learning Representations (ICLR), 2024. 6, 10 [21] Runqi Qiao, Qiuna Tan, Guanting Dong, Minhui Wu, Chong Sun, Xiaoshuai Song, Zhuoma GongQue, Shanglin Lei, Zhe Wei, Miaoxuan Zhang, Runfeng Qiao, Yifan Zhang, Xiao Zong, Yida Xu, Muxi Diao, Zhimin Bao, Chen Li, and Honggang Zhang. We-math: Does your large multimodal model achieve human-like mathematical reasoning?, 2024. 6, [22] Max Schwarzer, Ankesh Anand, Rishab Goel, Devon Hjelm, Aaron Courville, and Philip Bachman. Data-efficient reinforcement learning with self-predictive representations. In International Conference on Learning Representations, 2021. 3 [23] Junhao Shen, Haiteng Zhao, Yuzhe Gu, Songyang Gao, Kuikun Liu, Haian Huang, Jianfei Gao, Dahua Lin, Wenwei Zhang, and Kai Chen. Semi-off-policy reinforcement 17 learning for vision-language slow-thinking reasoning, 2025. 3 enna, Austria, 2025. Association for Computational Linguistics. 6 [36] Renrui Zhang, Dongzhi Jiang, Yichi Zhang, Haokun Lin, Ziyu Guo, Pengshuo Qiu, Aojun Zhou, Pan Lu, Kai-Wei Chang, Peng Gao, and Hongsheng Li. Mathverse: Does your multi-modal llm truly see the diagrams in visual math problems? In arXiv, 2024. [37] Ruijie Zheng, Xiyao Wang, Yanchao Sun, Shuang Ma, Jieyu Zhao, Huazhe Xu, Hal Daume III, and Furong Huang. Taco:temporal latent action-driven contrastive loss for visual reinforcement learning. In Advances in Neural Information Processing Systems, pages 4820348225. Curran Associates, Inc., 2023. 3 [38] Linghao Zhu, Yiran Guan, Dingkang Liang, Jianzhong Ju, Zhenbo Luo, Bin Qin, Jian Luan, Yuliang Liu, and Xiang Bai. Shuffle-r1: Efficient rl framework for multimodal large language models via data-centric dynamic shuffle, 2025. 3 [39] Chengke Zou, Xingang Guo, Rui Yang, Junyu Zhang, Bin Hu, and Huan Zhang. Dynamath: dynamic visual benchmark for evaluating mathematical reasoning robustness of vision language models, 2025. 6 [24] Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: flexible and efficient rlhf framework. arXiv preprint arXiv: 2409.19256, 2024. 12 [25] Haozhe Wang, Chao Qu, Zuming Huang, Wei Chu, Fangzhen Lin, and Wenhu Chen. Vl-rethinker: Incentivizing self-reflection of vision-language models with reinforcement learning. arXiv preprint arXiv:2504.08837, 2025. 3, 6, 11 [26] Ke Wang, Junting Pan, Weikang Shi, Zimu Lu, Houxing Ren, Aojun Zhou, Mingjie Zhan, and Hongsheng Li. Measuring multimodal mathematical reasoning with math-vision In The Thirty-eight Conference on Neural Infordataset. mation Processing Systems Datasets and Benchmarks Track, 2024. 6, [27] Zhenhailong Wang, Xuehang Guo, Sofia Stoica, Haiyang Xu, Hongru Wang, Hyeonjeong Ha, Xiusi Chen, Yangyi Chen, Ming Yan, Fei Huang, et al. Perception-aware policy optimization for multimodal reasoning. arXiv preprint arXiv:2507.06448, 2025. 1, 2, 3 [28] Jiaer Xia, Yuhang Zang, Peng Gao, Yixuan Li, and Kaiyang Zhou. Visionary-r1: Mitigating shortcuts in visual reasoning with reinforcement learning, 2025. 1, 2, 3 [29] Tong Xiao, Xin Xu, Zhenya Huang, Hongyu Gao, Quan Liu, Qi Liu, and Enhong Chen. Advancing multimodal reasoning capabilities of multimodal large language models via visual perception reward. arXiv preprint arXiv:2506.07218, 2025. 2, 3 [30] Yijia Xiao, Edward Sun, Tianyu Liu, and Wei Wang. Logicvista: Multimodal llm logical reasoning benchmark in visual contexts, 2024. 6, 10 [31] Haoran Xu, Amr Sharaf, Yunmo Chen, Weiting Tan, Lingfeng Shen, Benjamin Van Durme, Kenton Murray, and Young Jin Kim. Contrastive preference optimization: Pushing the boundaries of LLM performance in machine translation. In Proceedings of the 41st International Conference on Machine Learning, pages 5520455224. PMLR, 2024. 3 [32] Senqiao Yang, Junyi Li, Xin Lai, Bei Yu, Hengshuang Zhao, and Jiaya Jia. Visionthink: Smart and efficient vision lanarXiv preprint guage model via reinforcement learning. arXiv:2507.13348, 2025. 3 [33] Shuo Yang, Yuwei Niu, Yuyang Liu, Yang Ye, Bin Lin, and Li Yuan. Look-back: Implicit visual re-focusing in mllm reasoning, 2025. [34] Yi Yang, Xiaoxuan He, Hongkun Pan, Xiyan Jiang, Yan Deng, Xingtao Yang, Haoyu Lu, Dacheng Yin, Fengyun Rao, Minfeng Zhu, et al. R1-onevision: Advancing generalized multimodal reasoning through cross-modal formalization. arXiv preprint arXiv:2503.10615, 2025. 1 [35] Xiang Yue, Tianyu Zheng, Yuansheng Ni, Yubo Wang, Kai Zhang, Shengbang Tong, Yuxuan Sun, Botao Yu, Ge Zhang, Huan Sun, Yu Su, Wenhu Chen, and Graham Neubig. MMMU-pro: more robust multi-discipline multimodal understanding benchmark. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1513415186, Vi-"
        }
    ],
    "affiliations": [
        "Huawei Cloud",
        "Huawei Technologies Canada Co. Ltd."
    ]
}