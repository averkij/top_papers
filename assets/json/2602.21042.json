{
    "paper_title": "OmniOCR: Generalist OCR for Ethnic Minority Languages",
    "authors": [
        "Bonan Liu",
        "Zeyu Zhang",
        "Bingbing Meng",
        "Han Wang",
        "Hanshuo Zhang",
        "Chengping Wang",
        "Daji Ergu",
        "Ying Cai"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Optical character recognition (OCR) has advanced rapidly with deep learning and multimodal models, yet most methods focus on well-resourced scripts such as Latin and Chinese. Ethnic minority languages remain underexplored due to complex writing systems, scarce annotations, and diverse historical and modern forms, making generalization in low-resource or zero-shot settings challenging. To address these challenges, we present OmniOCR, a universal framework for ethnic minority scripts. OmniOCR introduces Dynamic Low-Rank Adaptation (Dynamic LoRA) to allocate model capacity across layers and scripts, enabling effective adaptation while preserving knowledge.A sparsity regularization prunes redundant updates, ensuring compact and efficient adaptation without extra inference cost. Evaluations on TibetanMNIST, Shui, ancient Yi, and Dongba show that OmniOCR outperforms zero-shot foundation models and standard post training, achieving state-of-the-art accuracy with superior parameter efficiency, and compared with the state-of-the-art baseline models, it improves accuracy by 39%-66% on these four datasets. Code: https://github.com/AIGeeksGroup/OmniOCR."
        },
        {
            "title": "Start",
            "content": "OmniOCR: Generalist OCR for Ethnic Minority Languages Bonan Liu1 Zeyu Zhang2 Bingbing Meng1 Han Wang1 Hanshuo Zhang1 Chengping Wang1 Daji Ergu1 Ying Cai3 1Southwest Minzu University 2AI Geeks Project lead. Corresponding author: caiying34@yeah.net. 6 2 0 2 4 2 ] . [ 1 2 4 0 1 2 . 2 0 6 2 : r AbstractOptical character recognition (OCR) has advanced rapidly with deep learning and multimodal models, yet most methods focus on well-resourced scripts such as Latin and Chinese. Ethnic minority languages remain underexplored due to complex writing systems, scarce annotations, and diverse historical and modern forms, making generalization in lowresource or zero-shot settings challenging. To address these challenges, we present OmniOCR, universal framework for ethnic minority scripts. OmniOCR introduces Dynamic Low-Rank Adaptation (Dynamic LoRA) to allocate model capacity across layers and scripts, enabling effective adaptation while preserving knowledge.A sparsity regularization prunes redundant updates, ensuring compact and efficient adaptation without extra inference cost. Evaluations on TibetanMNIST, Shui, ancient Yi, and Dongba show that OmniOCR outperforms zero-shot foundation models and standard post training, achieving state-of-the-art accuracy with superior parameter efficiency, and compared with the state-of-the-art baseline it improves accuracy by 39%66% on these four models, datasets. Code: https://github.com/AIGeeksGroup/OmniOCR. 1. Introduction OCR has achieved remarkable progress with deep learning and, more recently, large multimodal models. However, most existing methods target well-resourced scripts such as English or Chinese, while ethnic minority languages remain underexplored. Their complex writing systems, limited annotations, and coexistence of diverse historical and modern forms,pose unique challenges to conventional OCR. Early OCR systems for ethnic minority scripts relied on handcrafted features and script-specific segmentation strategies, such as those designed for Mongolian and Tibetan [1], [2]. With segmentation becoming bottleneck, research shifted to segmentation-free deep learning, including CNN-based sliding window models and attention-based sequence models [3], [4], [5]. Recently, vision-language models (VLMs) and large language models (LLMs) have demonstrated strong cross-modal representation capabilities [6], [7], [8], yet still struggle to generalize to ethnic minority scripts, particularly in low-resource or zero-shot settings. To address these challenges, we propose OmniOCR, universal framework for ethnic minority language OCR. Built on RolmOCR [9], OmniOCR integrates Dynamic 7 3 . 0 9 1 2 . 9 2 8 5 . 0 8 1 6 . 5 2 5 1 . 6 2 4 2 . 7 2 1 4 . 7 3 8 . 7 2 2 8 . 0 3 4 8 . 8 2 5 4 . 9 2 6 5 . 0 3 1 3 . 9 5 8 . 8 3 3 6 . 4 3 5 5 . 2 3 ) % ( a c 100 80 60 40 0 Pro PixtralLarge DeepSeek-VL2 (FullFT) Qwen-VL-Max (Ours) Qwen-VL-OCR (LoRA) GLM-4v-Plus RolmOCR Doubao-1.5-Vision-Pro GPT-4o Claude-3.7-Sonnet InternVL3-78B Moonshot-v1 Kimi-VL Gemini2.5 OmniOCR RolmOCR RolmOCR Figure 1. Accuracy comparison on Tibetan dataset across all models. LoRA, which adaptively allocates model capacity across layers and scripts. This design enables effective adaptation to structurally complex scripts while mitigating overfitting in low-resource scenarios. sparsity regularization further prunes redundant updates, ensuring compact adaptation without extra inference cost. We evaluate OmniOCR on four representative datasetsTibetanMNIST, Shui, ancient Yi, and Dongba. Experiments show consistent gains over zeroshot foundation models and standard post training, achieving state-of-the-art performance with superior parameter efficiency.Taking the Tibetan dataset as key verification scenario,the accuracy results are shown in Figure 1. In summarization, our contribution of our paper can be summarized in 3 folds: We introduce OmniOCR, the first universal OCR framework for heterogeneous ethnic minority scripts. We design Dynamic LoRA module that balances knowledge retention and efficient adaptation across scripts. We establish new benchmarks on four ethnic minority language datasets, surpassing existing baselines in both accuracy and efficiency, and improving accuracy by 39%66% on these four datasets. 2. Related Work LLMs in OCR. In recent years, OCR research has shifted from task-specific models to general-purpose large models. LLMs, especially multimodal ones (MLLMs), show strong contextual and visual capabilities, offering new solutions to long-standing OCR challenges. Even before LMMs, the community had moved from CNN-RNNs to Transformer-based frameworks. For instance, Li et al. [7] proposed TrOCR, combining pretrained image and text Transformers for end-to-end recognition, achieving strong results on printed, handwritten, and scene text.The rise of MLLMs further enhanced OCR performance. Greif et al. [8] showed that Gemini 2.0 Flash outperformed traditional OCR systems without fine-tuning. Benchmarks such as OCRBench [10], CC-OCR [11], and Reasoning-OCR [12] systematically evaluated MLLMs across document parsing, key information extraction, multilingual recognition, and reasoning. However, results vary across languages: Sohail et al. [13] found GPT-4o struggled on complex scripts like Urdu and Tajik, performing worse as text length increased.To overcome these issues, specialized OCR-oriented LMMs have emerged. Chen et al. [14] introduced OceanOCR, using NaViT to handle variable-resolution inputs and trained on large OCR datasets, achieving strong results across scene, document, and handwriting recognition. OCR for Ethnic Minority Languages. In the early development of OCR for ethnic minority languages, most methods relied on accurate character segmentation. Peng et al. [1] built printed Mongolian system requiring segmentation for its vertical layout, while Drup et al. [2] improved Tibetan OCR with adaptive binarization and connectedcomponent analysis. Sun et al. [15] applied Tesseract to Yi script but still needed repeated box generation with jTessBox Editor. As segmentation became bottleneck, later studies shifted to segmentation-free deep learning. Zhang et al. [3] developed CNN-based sliding-window framework for Manchu; Zheng et al. [4] improved printed Manchu OCR with nine-layer CNN; and Zhang et al. [5] introduced an attention-based seq2seq model for Mongolian. More recently, vision-language models (VLMs) have emerged. Chung et al. [6] fine-tuned open-source VLMs (e.g., LLaMA, Qwen) on 60,000 synthetic Manchu word images, achieving 93.1% accuracy on real manuscripts, surpassing CRNN baselines 3. Datasets To train and evaluate the OmniOCR model, we curated four publicly available datasets encompassing diverse ethnic minority scripts, covering both historical and contemporary writing systems as well as handwritten numerals. Detailed TABLE 1. STATISTICS OF THE FOUR ETHNIC MINORITY LANGUAGE DATASETS USED FOR TRAINING AND EVALUATION OF OMNIOCR. Dataset Classes Samples Script Type Image Size TibetanMNIST [16] Shui Dataset [17] Ancient Yi Script [18] Dongba Script [19] 10 12 30 17,768 5,280 10,840 14,906 Digits Pictographic Logographic Pictographic 111M 21.6M 62.2M 20.2M statistics of these datasetsincluding the number of classes, total samples, script type, and image sizeare summarized in Table 1. TibetanMNIST [16] is benchmark dataset for handwritten Tibetan digits, which plays crucial role in advancing OCR research for the Tibetan script by filling gap in publicly available resources. It contains 17,768 images of handwritten Tibetan numerals, produced by multiple researchers from Tibetan studies institutes, ensuring wide variety of writing styles. By incorporating this dataset, we can rigorously assess the proposed OmniOCR models ability to generalize across variations in character morphology and its effectiveness in recognizing complex, non-alphabetic scripts. Shui Dataset [17] is dataset of ancient Shui characters, comprising 5,280 images across 12 representative classes. These images depict elements of the natural and cultural world, including mountains, rivers, trees, the sun, the moon, stars, animals, deities, and divination symbols, reflecting the richness of Shui cultural heritage. This dataset enables assessment of OmniOCRs ability to recognize complex pictographic scripts. Ancient Yi Script Handwritten Character Dataset [18] is dataset of handwritten ancient Yi script, originally comprising 2,922 character classes with over 427,000 samples. To manage computational complexity and focus the recognition experiment, we selected representative subset of 30 classes based on two criteria: high frequency in Yi documents, ensuring sufficient samples per class, and diversity in stroke structures, capturing variations in handwriting styles. For each selected class, an equal number of images were randomly sampled from multiple writers to mitigate class imbalance and maintain evaluation reliability. This enables rigorous assessment of OmniOCRs generalization capability on handwritten Yi characters. The Handwritten Dongba Character Dataset [19] is large-scale collection of single Dongba characters, constructed through manual imitation, character cropping, grayscale conversion, binarization, and size normalization. The original dataset comprises 1,404 Dongba character classes, totaling 445,273 images and covering 2,546 variant forms, with each class containing 103 to 1,091 samples. To ensure both experimental feasibility and representativeness, we filtered the dataset based on the characters pictographic recognizability (i.e., how easily the character shape can be identified) and practical usage frequency. As result, we selected the 30 highest-quality character classes, forming subset that balances recognizability and representativeness, which is used to rigorously evaluate OmniOCRs performance on Dongba character recognition. 4. Method 4.1. Overview In this work, we propose OmniOCR, generalist OCR framework tailored for ethnic minority languages. Our approach builds upon the vision-language foundation model RolmOCR [9], which provides strong cross-lingual representation capability. However, directly applying RolmOCR to low-resource minority scripts often suffers from limited recognition accuracy due to script-specific structural variations and severe data scarcity. Inspired by [20], we design dynamic LoRA module within OmniOCR that adaptively adjusts the rank across different layers, thereby balancing the acquisition of new knowledge with the retention of previously learned scripts.The complete architecture of OmniOCRincluding its Vision Encoder, Text Encoder, and the two parameter-efficient post training methods that underpin its performanceis illustrated in Figure 2.In addition, to further reduce GPU memory consumption during training, we preprocess the datasets by resizing and normalizing the input images while preserving script readability. We evaluate OmniOCR on four representative minority language datasets with diverse writing systems, and the results demonstrate that our framework achieves robust performance across underrepresented scripts while effectively mitigating catastrophic forgetting. 4.2. Dynamic LoRA Adaptation Recognizing minority languages poses unique challenges compared to mainstream scripts, due to their heterogeneous writing systems (e.g., Tibetans distinct numeral forms, Ancient Yis pictographic symbols, Shui scripts water-based ideograms, and Dongbas logographic structures) and the limited availability of annotated data. To address these challenges, we design OmniOCR, unified framework that adapts general OCR backbone to multiple minority scripts in parameter-efficient manner. key component of OmniOCR is dynamic LoRA module that tailors the models capacity to each scripts characteristics. Instead of applying fixed-rank update, which may underfit complex scripts or overfit scarce data, we allow the update rank to be adaptively determined for each layer and task. Specifically, for pre-trained weight matrix t,m at task and layer m, the update t,m is expressed as: 0 t,m = (cid:88) i= Bt,m wt,m At,m and At,m where is the maximum candidate rank, Bt,m are low-rank matrices, and wt,m is learnable importance weight. This formulation enables the model to allocate more capacity to scripts with complex visual structures (e.g., Dongba or Ancient Yi), while using fewer ranks for simpler ones (e.g., Tibetan digits), thereby achieving balance between adaptability and efficiency. To further enhance robustness in low-resource scenarios, we impose an ℓ1 sparsity regularization on the importance weights: Lt min := Lt sup + λ (cid:88) m= wt,m1 where Lt sup is the supervised loss, is the number of updated matrices, and λ controls sparsity. This design encourages the model to retain only the most critical update directions while pruning redundant ones, ensuring compact adaptation without extra inference cost.The complete training procedure is outlined in Algorithm 1. Through this mechanism, OmniOCR is able to efficiently adapt to the structural diversity across Tibetan, Ancient Yi, Shui, and Dongba scripts, while simultaneously mitigating catastrophic forgetting when learning sequentially across different minority languages. Moreover, by pruning redundant update directions and retaining only the most critical ones, the framework achieves compact adaptation without introducing additional inference overhead, making it both effective in low-resource scenarios and practical for realworld applications. Algorithm 1 OmniOCR with Dynamic LoRA Adaptation Require: Pre-trained backbone W0; max rank r; sparsity weight λ; datasets = {D1, . . . , DT } Ensure: Adapted model with compact LoRA modules 1: for each task with dataset Dt do Freeze W0, initialize {At,m 2: for mini-batch Dt do 3: Compute update: 4: , Bt,m , wt,m } t,m = (cid:88) i= Bt,m wt,m At,m 5: 6: Forward with W0 + , compute loss Lsup Add sparsity: Lsup + λ wt,m1 (cid:88) Backpropagate and update only {A, B, w} end for Prune update directions with small wt,m 7: 8: 9: 10: end for 11: return Updated backbone W0 with compact LoRA modules 5. Experiments 5.1. Implementation Details We implement OmniOCR with PyTorch. All experiments are conducted on server equipped with single images are NVIDIA H20 GPU (96GB memory).. Input Figure 2. OmniOCR. represents the processing procedure of the models Vision Encoder; represents the processing procedure of the models Text Encoder; respresents two distinct parameter-efficient fine-tuning methods: Dynamic-Rank Training and Fixed-Rank Training. Figure 3. Demonstration of Recognition Performance for Tibetan Handwritten Digits via OmniOCR. The results show the accuracy and visual recognition effect of OmniOCR on the Tibetan handwritten digit dataset. resized to 4848 pixels. For training, standard data augmentations are applied, including random horizontal flip, random rotation (10), random resized crop (scale 0.81.0), and color jitter. Evaluation uses only resizing and normalization."
        },
        {
            "title": "The base model",
            "content": "is initialized from the pre-trained RolmOCR [9]. We replace selected linear layers (selfattention projections and MLP layers) with the proposed Dynamic LoRA modules, which support dynamic rank adaptation. The initial LoRA rank is set to = 8, LoRA scaling factor α = 16. During training, the effective rank is adjusted via soft-shrinkage on rank weights, encouraging sparsity while retaining task-relevant capacity. To prevent overfitting, learned low-rank updates are merged back into the frozen backbone at checkpoint saving. We train with the AdamW optimizer, using an initial learning rate of 5 106, weight decay 1 102, and gradient clipping at 1.0. We adopt batch size of 1 (due to GPU memory constraints) and use gradient accumulation over 2 steps to achieve an effective batch size of 2. Training runs for 30 epochs with early stopping based on validation accuracy. Mixed-precision training with BF16 is enabled to improve efficiency. Figure 4. The performance of OmniOCR evaluated on four representative datasetsTibetanMNIST, Shui, Ancient Yi, and Dongbahighlighting its ability to generalize across heterogeneous scripts and writing systems."
        },
        {
            "title": "The best model is selected based on validation accuracy",
            "content": "and saved together with the processor for inference. 5.2. Evaluation Metrics To comprehensively evaluate the performance of OmniOCR on ethnic minority language OCR tasks, we report three widely used metrics: Accuracy, Recall, and F1-score. Accuracy reflects the overall correctness of predictions by measuring the proportion of correctly recognized samples among all test cases. Recall captures the models ability to identify positive samples, thus indicating its effectiveness in minimizing missed characters. F1-score serves as balanced metric that jointly considers Precision and Recall, offering more reliable assessment under class-imbalanced scenarios. While Accuracy provides general measure of recognition quality, Recall and F1-score are particularly important for minority scripts with heterogeneous data distributions. 5.3. Main Results Table 2 reports the performance of different models on four ethnic minority language OCR benchmarks. Several observations can be made. First, existing vision-language foundation models (e.g., Kimi-VL, Moonshot, Pixtral, Gemini 2.5 Pro, GPT-4o) perform poorly in the zero-shot setting, indicating that current base models struggle to generalize to minority scripts without adaptation. Although Claude-3.7TABLE 2. PERFORMANCE (%) ON FOUR ETHNIC MINORITY LANGUAGE DATASETS. ZERO-SHOT MODELS AND FINE-TUNING METHODS ON ROLMOCR ARE COMPARED. Models / Methods Tibetan [16] Ancient Yi [18] Shui Script [17] Dongba Script [19] Acc Recall F1 Acc Recall Acc Recall F1 Acc Recall Zero-Shot Kimi-VL [21] Moonshot-v1 [22] Pixtral Large [23] Doubao-1.5-Vision-Pro [24] GLM-4v-Plus [25] DeepSeek-VL2 [26] Qwen-VL-Max [27] Qwen-VL-OCR [28] InternVL3-78B [29] Gemini 2.5 Pro [30] Claude-3.7-Sonnet [31] GPT-4o [32] RolmOCR [9] 29.45 28.84 30.56 32.55 27.24 27.83 26.15 38.85 30.82 27.41 34.63 25.61 29.31 28.73 31.12 25.89 31.42 26.43 32.95 27.32 37.96 32.67 26.73 33.87 27.18 28.57 28.96 29.95 27.98 31.76 26.71 30.21 27.06 38.18 31.71 26.96 34.08 26.37 28.94 19.32 17.14 20.13 22.34 19.83 22.58 21.23 14.53 19.26 20.57 23.57 18.91 22. 20.14 18.75 25.78 21.23 18.92 18.32 20.47 15.31 21.04 19.66 22.69 20.43 21.48 19.92 17.92 22.73 21.51 19.16 20.21 20.73 15.10 20.12 19.88 22.89 19.65 21.83 49.68 47.27 51.42 54.49 46.38 52.76 36.22 16.03 53.91 48.31 57.31 46.84 49.61 48.91 49.35 48.35 55.17 47.21 57.94 35.69 15.28 55.62 49.12 58.13 48.66 48.32 49.15 48.29 49.81 54.88 46.95 55.26 35.89 15.49 54.75 48.86 57.86 47.73 48.96 32.33 30.31 34.27 30.72 33.25 35.81 33.11 11.15 31.43 32.84 36.99 38.12 36. 33.07 32.01 30.15 29.88 32.64 32.47 34.28 11.84 33.14 32.17 36.24 39.77 35.72 32.83 31.14 32.08 30.14 32.87 34.08 34.04 11.62 32.26 32.43 36.47 38.93 36.11 Fine-tuning RolmOCR (LoRA) RolmOCR (Full Fine-tune) 80.52 89.21 79.84 89. 80.58 89.33 82.43 90.53 83.11 90.05 83.06 90.58 90.32 95.29 89.67 95. 89.70 95.23 89.84 94.58 90.42 94.11 90.48 94.56 OmniOCR (Ours) 90. 91.12 90.48 89.62 90.14 89.60 95. 96.31 95.86 95.32 94.76 95.29 Sonnet and Qwen-VL-MAX achieve slightly better results on certain datasets, their performance remains far from practical use,its limitation on specific tasks is further illustrated by the Tibetan handwritten digit recognition examples in Figure 3. Second, adapting RolmOCR through supervised post-training leads to substantial improvements. Both parameter-efficient post-training with LoRA and standard full post-training yield significant gains, surpassing 80% accuracy on Tibetan and Ancient Yi, and over 90% on Shui and Dongba. This highlights the necessity of model adaptation for low-resource minority languages, with the crossdataset effectiveness of such adaptation clearly visualized in Figure 4. Finally, our proposed OmniOCR with dynamic LoRA adaptation consistently achieves competitive accuracy across all datasets. It surpasses standard full post-training of RolmOCR on Tibetan, Shui, and Dongba (up to 90.37%, 95.95%, and 95.32%, respectively), while on Ancient Yi it is slightly lower than full fine-tuning (89.62% vs. 90.53%). Importantly, OmniOCR maintains parameter efficiency and lower memory usage, making it far more practical than full fine-tuning, which requires substantially more parameters, longer training time, and higher GPU memory. These results demonstrate that our dynamic low-rank adaptation effectively allocates model capacity to capture script-specific structures while remaining resource-efficient. 5.4. Ablation Study To investigate the impact of optimization hyperparameters, we conducted ablation experiments on four ethnic minority language OCR datasets by varying the learning rate and batch size. The results are summarized in Table 3. Our default configuration, i.e., learning rate of 5e-6 with batch size of 2, consistently achieves the best performance in terms of accuracy, recall, and F1 score. Setting the learning rate to 1e-5 leads to noticeable decrease in accuracy and F1 across all datasets, while further reducing it to 1e-6 also degrades performance, suggesting that overly small or large step sizes hinder effective convergence. As for the batch size, compared with the default setting of 2, using either 1 or 4 yields inferior results, indicating that an appropriate batch size is critical for balancing gradient stability and generalization. In summary, learning rate of 5e-6 with batch size of 2 plays key role in stabilizing optimization and achieving superior recognition performance. To investigate the contribution of each module in Dynamic LoRA, we conduct ablation experiments by selectively disabling individual components. In Table 4, cross mark () indicates that the module is disabled. Disabling dynamic rank adaptation removes the models ability to dynamically select the effective rank for each task and module, forcing fixed-rank updates. This disrupts the balance between knowledge retention and new task adaptation, reduces parameter efficiency, and weakens generalization. Disabling the MLP adaptation module prevents dynamic low-rank updates in the feed-forward layers responsible for high-level semantic modeling. As result, adaptation to new tasks is limited, and sequential task knowledge accumulation is interrupted, which negatively impacts downstream performance. Disabling the attention adaptation module eliminates task-specific token dependency adjustments. The model can no longer dynamically modify attention weights, leading to disrupted local feature correlations and degraded visual-text alignment, which in turn reduce transfer and historical task performance. TABLE 3. ABLATION STUDY ON LEARNING RATE AND BATCH SIZE ACROSS FOUR DATASETS. Learning Rate Batch Size Tibetan [16] Ancient Yi [18] Shui Script [17] Dongba Script [19] Acc (%) Recall (%) F1 (%) Acc (%) Recall (%) F1 (%) Acc (%) Recall (%) F1 (%) Acc (%) Recall (%) F1 (%) OmniOCR (Ours) 1e-5 1e-6 5e-6 5e-6 2 2 2 1 4 90. 89.85 88.41 87.68 89.11 91.12 90.02 88.77 88.03 89.42 90.48 89.71 88.23 87.57 88.98 89. 88.20 87.07 86.32 88.02 90.14 88.54 87.35 86.69 88.34 89.60 88.13 87.03 86.24 87.92 95. 93.41 92.22 91.76 93.13 96.31 93.77 92.56 92.09 93.48 95.86 93.51 92.11 91.64 93.06 95. 92.83 91.67 90.82 92.41 94.76 93.12 91.98 91.21 92.78 95.29 92.75 91.59 90.73 92.31 TABLE 4. ABLATION STUDY ON DYNAMIC LORA COMPONENTS ACROSS FOUR DATASETS. Configuration Tibetan [16] Ancient Yi [18] Shui Script [17] Dongba Script [19] Acc (%) Recall (%) F1 (%) Acc (%) Recall (%) F1 (%) Acc (%) Recall (%) F1 (%) Acc (%) Recall (%) F1 (%) Full Dynamic LoRA (Ours) Dynamic Rank MLP Modules Attention Modules Regularization Sparsity 90.37 83.86 82.56 88.74 85.55 91.12 83.25 81.92 89.18 84.99 90.48 83.60 82.30 88.89 85. 89.62 82.51 81.17 87.99 83.99 90.14 81.79 80.49 88.46 83.31 89.60 82.15 80.88 88.12 83. 95.95 86.98 85.10 91.87 88.80 96.31 86.20 84.39 92.41 88.12 95.86 86.55 84.78 92.03 88. 95.32 85.70 84.03 90.77 87.42 94.76 84.91 83.24 91.32 86.72 95.29 85.32 83.65 90.99 87. Disabling sparsity regularization removes the mechanism for pruning low-importance rank updates, increasing parameter redundancy and the risk of overfitting. It also decreases inference efficiency and destabilizes knowledge retention. Overall, these results confirm that each module in Dynamic LoRA plays critical role in balancing knowledge retention and task adaptation. The full configuration, with all modules enabled, achieves the best accuracy, recall, and F1 scores across all four ethnic minority language OCR datasets. 6. Limitation and Future Work Although OmniOCR demonstrates strong performance across diverse ethnic minority language datasets, several limitations remain. First, our experiments are conducted on four curated datasets (Tibetan numerals, Ancient Yi, Shui script, and Dongba script), which, while representative, do not cover the full diversity of minority writing systems. Many scripts feature richer structural variations, such as decorative glyphs, mixed phoneticlogographic properties, or highly context-dependent ligatures, which may expose additional challenges beyond our current evaluation. Second, while Dynamic LoRA significantly reduces the parameter footprint and improves adaptation efficiency, the training process still requires noticeable GPU resources and nontrivial memory usage. This may restrict deployment in resource-constrained environments or community-level digitization projects, where lightweight solutions are critical. Third, our study emphasizes recognition accuracy under benchmark settings, yet practical OCR systems must also contend with real-world issues such as document degradation, background noise, and complex layouts combining text, images, and annotationsfactors that are only partially addressed in our current framework. For future work, we plan to broaden OmniOCR to include wider range of minority scripts and historical documents, integrate lightweight continual learning techniques to improve adaptability in dynamic environments, and explore cross-modal pre-training (e.g., combining speech, textual corpora, and visual data) to further enhance robustness and generalization. 7. Conclusion In this work, we presented OmniOCR, the first universal OCR framework tailored for ethnic minority languages with heterogeneous and complex scripts. By integrating Dynamic LoRA module, OmniOCR adaptively balances knowledge retention and efficient adaptation across different layers and writing systems. This design enables the model to learn effectively even in low-resource scenarios, while reducing the risk of overfitting and ensuring parameter efficiency, which are critical for underexplored languages with limited training data. Extensive experiments on four representative datasetsTibetanMNIST, Shui, Ancient Yi, and Dongba-demonstrate that OmniOCR consistently surpasses zero-shot foundation models and conventional post-training baselines. The results show that our framework not only achieves competitive or superior recognition accuracy, but also substantially lowers parameter overhead and memory consumption. Such improvements make OmniOCR practical and scalable solution for real-world applications, especially in resourceconstrained or community-driven digitization environments. Looking forward, we plan to expand OmniOCR to wider spectrum of minority scripts and historical writing systems, including those with more diverse structural characteristics and mixed writing paradigms. We also intend to explore integration with multilingual and cross-modal pretraining strategies, such as jointly leveraging speech, text, and image corpora, to further enhance robustness and crossscript generalization. Ultimately, we believe our framework provides strong foundation for advancing inclusive OCR research and for preserving the cultural and linguistic heritage embodied in the worlds diverse written traditions. [17] Xiuzhang Yang, Shuai Wu, Juwen Song, Wenjing Liao, and Jisong Zhou, An algorithm for ancient shui script recognition based on adaptive image enhancement and alexnet, Information Technology and Informatization, , no. 1, pp. 212216, 2023. [18] Xiaojuan Liu, Xu Han, Shanxiong Chen, Weijia Dai, and Qiuyue Ruan, Ancient yi script handwriting sample repository, Scientific Data, vol. 11, no. 1, pp. 1183, 2024. [19] Yanlong Luo, Yiwen Sun, and Xiaojun Bi, Multiple attentional aggregation network for handwritten dongba character recognition, Expert Systems with Applications, vol. 213, pp. 118865, 2023, Download link for the Dongba dataset: https://mzyy.muc.edu.cn/. [20] Haodong Lu, Chongyang Zhao, Jason Xue, Lina Yao, Kristen Moore, and Dong Gong, Adaptive rank, reduced forgetting: Knowledge retention in continual learning vision-language models with dynamic rank-selective lora, arXiv preprint arXiv:2412.01004, 2024. [21] Kimi Team, Angang Du, Bohong Yin, Bowei Xing, Bowen Qu, Bowen Wang, Cheng Chen, Chenlin Zhang, Chenzhuang Du, arXiv preprint Chu Wei, et al., arXiv:2504.07491, 2025. Kimi-vl technical report, [22] Moonshot AI, Moonshot v1, https://moonshot.cn/product, 2025. [23] Mistral AI, Pixtral large, https://mistral.ai/news/pixtral-large/, 2025. [24] ByteDance, Doubao-1.5-vision-pro, https://www.volcengine.com/ product/doubao, 2025. [25] Zhipu AI, Glm-4v-plus, https://chatglm.cn, 2025. [26] Zhiyu Wu, Xiaokang Chen, Zizheng Pan, Xingchao Liu, Wen Liu, Damai Dai, Huazuo Gao, Yiyang Ma, Chengyue Wu, Bingxuan Deepseek-vl2: Mixture-of-experts vision-language Wang, et al., arXiv preprint models for advanced multimodal understanding, arXiv:2412.10302, 2024. [27] Alibaba Cloud Qwen Team, Qwen-vl-max, https://tongyi.aliyun. com, 2025. [28] Alibaba Cloud Qwen Team, Qwen-vl-ocr, https://tongyi.aliyun. com, 2025. [29] Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Hao Tian, Yuchen Duan, Weijie Su, Jie Shao, et al., Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models, arXiv preprint arXiv:2504.10479, 2025. [30] Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al., Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities, arXiv preprint arXiv:2507.06261, 2025. [31] Anthropic, Introducing claude 3.7 sonnet, https://www.anthropic. com/news/claude-3-7-sonnet, 2025. [32] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al., Gpt-4o system card, arXiv preprint arXiv:2410.21276, 2024."
        },
        {
            "title": "References",
            "content": "[1] Liangrui Peng, Changsong Liu, Xiaoqing Ding, and Hua Wang, Multilingual document recognition research and its application in in Second International Conference on Document Image china, Analysis for Libraries (DIAL06). IEEE, 2006, pp. 7pp. [2] Ngo Drup, Dongcai Zhao, Puts Ren, Daluo Sanglangjie, Fang Liu, and Bian Bawangdui, Study on printed tibetan character recognition, in 2010 International Conference on Artificial Intelligence and Computational Intelligence. IEEE, 2010, vol. 1, pp. 280285. [3] Diandian Zhang, Yan Liu, Zhuowei Wang, and Depei Wang, Ocr with the deep cnn model for ligature script-based languages like manchu, Scientific programming, vol. 2021, no. 1, pp. 5520338, 2021. [4] Ruirui Zheng, Min Li, Jianjun He, Jiajing Bi, and Baochun Wu, Segmentation-free multi-font printed manchu word recognition using deep convolutional features and data augmentation, in 2018 11th International Congress on Image and Signal Processing, BioMedical Engineering and Informatics (CISP-BMEI). IEEE, 2018, pp. 16. [5] Hui Zhang, Hongxi Wei, Feilong Bao, and Guanglai Gao, Segmentation-free printed traditional mongolian ocr using sequence to sequence with attention model, in 2017 14th IAPR International Conference on Document Analysis and Recognition (ICDAR). IEEE, 2017, vol. 1, pp. 585590. [6] Yan Hon Michael Chung and Donghyeok Choi, Finetuning visionlanguage models as ocr systems for low-resource languages: case study of manchu, arXiv preprint arXiv:2507.06761, 2025. [7] Minghao Li, Tengchao Lv, Jingye Chen, Lei Cui, Yijuan Lu, Trocr: recognition with pre-trained in Proceedings of the AAAI conference on artificial inDinei Florencio, Cha Zhang, Zhoujun Li, and Furu Wei, Transformer-based optical character models, telligence, 2023, vol. 37, pp. 1309413102. [8] Gavin Greif, Niclas Griesshaber, and Robin Greif, Multimodal llms for ocr, ocr post-correction, and named entity recognition in historical documents, arXiv preprint arXiv:2504.00414, 2025. [9] et al. Bai, Rolmocr: vision-language foundation model for robust multilingual ocr, https://github.com/QwenLM/Qwen2.5-VL, 2025. [10] Yuliang Liu, Zhang Li, Mingxin Huang, Biao Yang, Wenwen Yu, Chunyuan Li, Xu-Cheng Yin, Cheng-Lin Liu, Lianwen Jin, and Xiang Bai, Ocrbench: on the hidden mystery of ocr in large multimodal models, Science China Information Sciences, vol. 67, no. 12, pp. 220102, 2024. [11] Zhibo Yang, Jun Tang, Zhaohai Li, Pengfei Wang, Jianqiang Wan, Humen Zhong, Xuejing Liu, Mingkun Yang, Peng Wang, Shuai Bai, et al., Cc-ocr: comprehensive and challenging ocr benchmark for evaluating large multimodal models in literacy, arXiv preprint arXiv:2412.02210, 2024. [12] Haibin He, Maoyuan Ye, Jing Zhang, Xiantao Cai, Juhua Liu, Bo Du, and Dacheng Tao, Reasoning-ocr: Can large multimodal models solve complex logical reasoning problems from ocr cues?, arXiv preprint arXiv:2505.12766, 2025. [13] Muhammad Abdullah Sohail, Salaar Masood, and Hamza Iqbal, Deciphering the underserved: Benchmarking llm ocr for low-resource scripts, arXiv preprint arXiv:2412.16119, 2024. [14] Song Chen, Xinyu Guo, Yadong Li, Tao Zhang, Mingan Lin, Dongdong Kuang, Youwei Zhang, Lingfeng Ming, Fengyu Zhang, Yuran Ocean-ocr: Towards general ocr application via Wang, et al., vision-language model, arXiv preprint arXiv:2501.15558, 2025. [15] Peiyu Sun, Qiuyan Xie, Zhaokang Wu, Xiaoyu Feng, Jiajun Cai, and Yulian Jiang, Yi characters recognition based on tesseractocr, in 2019 IEEE 3rd Advanced Information Management, Communicates, Electronic and Automation Control Conference (IMCEC). IEEE, 2019, pp. 102106. [16] M. Q. Yuan, X. M. Cairang, J. A. Tang, and et al., TibetanMNIST Tibetan Handwritten Digit Dataset, 2018, [Dataset]."
        }
    ],
    "affiliations": [
        "AI Geeks",
        "Southwest Minzu University"
    ]
}