{
    "paper_title": "Mind with Eyes: from Language Reasoning to Multimodal Reasoning",
    "authors": [
        "Zhiyu Lin",
        "Yifei Gao",
        "Xian Zhao",
        "Yunfan Yang",
        "Jitao Sang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Language models have recently advanced into the realm of reasoning, yet it is through multimodal reasoning that we can fully unlock the potential to achieve more comprehensive, human-like cognitive capabilities. This survey provides a systematic overview of the recent multimodal reasoning approaches, categorizing them into two levels: language-centric multimodal reasoning and collaborative multimodal reasoning. The former encompasses one-pass visual perception and active visual perception, where vision primarily serves a supporting role in language reasoning. The latter involves action generation and state update within reasoning process, enabling a more dynamic interaction between modalities. Furthermore, we analyze the technical evolution of these methods, discuss their inherent challenges, and introduce key benchmark tasks and evaluation metrics for assessing multimodal reasoning performance. Finally, we provide insights into future research directions from the following two perspectives: (i) from visual-language reasoning to omnimodal reasoning and (ii) from multimodal reasoning to multimodal agents. This survey aims to provide a structured overview that will inspire further advancements in multimodal reasoning research."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 2 ] . [ 1 1 7 0 8 1 . 3 0 5 2 : r Mind with Eyes: from Language Reasoning to Multimodal Reasoning Zhiyu Lin1, Yifei Gao1, Xian Zhao1, Yunfan Yang1, Jitao Sang1 {zyllin, yifegao, xianzhao, yunfanyang, jtsang}@bjtu.edu.cn 1Beijing Jiaotong University (cid:135): https://github.com/ADaM-BJTU/Mind_with_eyes_Awesome_MLLMs_Reasoning"
        },
        {
            "title": "Abstract",
            "content": "Language models have recently advanced into the realm of reasoning, yet it is through multimodal reasoning that we can fully unlock the potential to achieve more comprehensive, human-like cognitive capabilities. This survey provides systematic overview of the recent multimodal reasoning approaches, categorizing them into two levels: language-centric multimodal reasoning and collaborative multimodal reasoning. The former encompasses one-pass visual perception and active visual perception, where vision primarily serves supporting role in language reasoning. The latter involves action generation and state update within reasoning process, enabling more dynamic interaction between modalities. Furthermore, we analyze the technical evolution of these methods, discuss their inherent challenges, and introduce key benchmark tasks and evaluation metrics for assessing multimodal reasoning performance. Finally, we provide insights into future research directions from the following two perspectives: (i) from visual-language reasoning to omnimodal reasoning and (ii) from multimodal reasoning to multimodal agents. This survey aims to provide structured overview that will inspire further advancements in multimodal reasoning research."
        },
        {
            "title": "Introduction",
            "content": "Recently, Large Language Models (LLMs) have emerged as cornerstone of language reasoning tasks [43, 8, 66, 2, 53, 58, 8], leveraging their exceptional text generation and contextual comprehension capabilities. Models such as OpenAI-o1 [39] and Deepseek-R1 [18] have demonstrated human-like stepwise reasoning abilities in mathematical deduction, logical question answering, and code generation through strategies like Chain-of-Thought (CoT) prompting and reinforcement learning. However, the limitations of purely text-based reasoning are increasingly apparent: its input and output are confined to single modality (text), rendering it inadequate for real-world scenarios requiring multimodal interactions (e.g., images, audio). With the advent of Multimodal Large Language Models (MLLMs) [1, 10, 30, 22], researchers have begun exploring the integration of LLMs reasoning capabilities with visual, auditory, and other modalities. Early explorations [68] attempted to transfer the CoT reasoning paradigm to vision-language tasks (e.g., visual question answering, chart parsing) via workflows that involve \"parsing textual instructions, extracting image features, fusing multimodal representations, and generating reasoning conclusions.\" In parallel, Knowledge Graph-based Multimodal Reasoning has already established systematic research framework [29, 76, 64, 77, 12]. By structuring knowledge representations (e.g., entityrelation-attribute triplets) and incorporating visual symbolic information, these methods explicitly model cross-modal semantic associations (e.g., linking image regions to textual descriptions) and Corresponding Author Figure 1: Timeline of multimodal reasoning models up to March 2025. We annotate and categorize the construction paradigms of reasoning models using dots of different colors. enable interpretable reasoning through symbolic logic rules or graph neural networks (GNNs). Nevertheless, high-quality knowledge graphs heavily rely on costly human annotations and suffer from limited domain coverage, hindering their scalability to open-domain question answering and reasoning tasks. In contrast, MLLMs excel at learning implicit correlations from unstructured multimodal data, demonstrating superior generalization capabilities. Moreover, their powerful text generation abilities facilitate more natural human-AI interactions. Consequently, employing LLMs as the core engine for multimodal reasoning remains the prevailing paradigm. While LLM-based multimodal reasoning holds great potential, it faces more challenges than languageonly reasoning. The core difficulties lie in ensuring cross-modal semantic alignment while enabling dynamic interaction and collaboration between modalities. First, vision and language differ significantly in information density and abstraction. Images convey detailed, low-level spatial features, whereas text captures high-level conceptual meaning. This disparity requires robust feature fusion mechanisms for cross-modal alignment. Second, the prevailing \"vision-language feature concatenation + text generation\" paradigm limits the generative potential of visual information and hinders dynamic cross-modal interaction. Effective multimodal reasoning requires models to adaptively prioritize the most relevant modality and guide information flow across modalities as needed. This survey systematically reviews recent multimodal reasoning methods and benchmark datasets as shown in Figure 1. By examining technological progress and persistent challenges, we provide actionable insights to shape the next generation of multimodal reasoning systems."
        },
        {
            "title": "2 Taxonomy of Multimodal Reasoning",
            "content": "As previously discussed, multimodal reasoning is not an isolated activity within single modality but rather dynamic interaction process where vision and language modalities collaboratively deepen cognitive reasoning. Through systematic analysis of existing works, we observe that nearly all solutions explicitly construct reasoning chains within the language space, with their critical distinctions lying in the strategic hierarchy of visual information processing. Based on how visual modaility involves in the reasoning process, We categorize these approaches into two progressive levels as shown in Figure 2 2.1 Level 1: Language-centric Multimodal Reasoning In this paradigm, the visual modality primarily serves perceptual and feature extraction roles, while reasoning is entirely dominated and driven by the language modality. Based on the triggering mechanisms of visual perception, this level is divided into two subcategories: One-pass Visual Perception: These methods treat visual information as static context. The model encodes images only once during the input stage (\"only look once\", e.g., CLIP-based global feature 2 Figure 2: An analogous schematic diagram of two levels of multimodal reasoning. extraction [59]), relying solely on the language modality for subsequent information integration and logical deduction. Active Visual Perception: Intermediate reasoning steps generated by the language modality trigger multiple rounds of visual re-perception (e.g., dynamic region cropping or zooming [13]). The model actively retrieves required visual details based on textual reasoning cues, forming \"look-back\" mechanism. Compared to one-pass perception, this approach is proved to enhance the reliability of multimodal alignment during reasoning. 2.2 Level 2: Collaborative Multimodal Reasoning When reasoning involves visual action reasoning and visual state updating, the visual modality transcends its passive perception role to engage in collaborative reasoning with the language modality. Key characteristics include: Visual action reasoning: The visual modality not only responds to language instructions but also autonomously generates internal reasoning actions (e.g., invoking visual tools for image editing or leveraging generative capabilities to reconstruct images [73]). This is marked by explicit reasoning trajectories within the visual feature space. Visual state update: By executing the above actions, the model dynamically updates visual contextual information (e.g., generating new geometric diagrams with auxiliary lines [23]). The updated visual representations feedback as new constraints to the language modality, triggering subsequent intermediate reasoning steps. This taxonomy reveals the evolution of multimodal reasoning technologies: from language-dominated unilateral control toward vision-language co-reasoning. Below, we detail existing solutions and derive actionable insights for each level of works."
        },
        {
            "title": "3 One-pass Visual Perception",
            "content": "3.1 Prompt-based Solutions This type of works [17, 36, 55] initially configure the multimodal large model to assume roles as different functional modules through system messages. This is followed by generating intermediate reasoning results or strategies in workflow manner, which are then used to synthesize the final answer. Depending on the definition of the functional modules, the form of the intermediate reasoning results can vary widely. We will illustrate this using the following three works as representatives. Cantor [17] deconstructs the visual reasoning task into two steps: decision generation and execution. In the first step, the multimodal large model is prompted to assume multiple roles, completing principle analysis, module selection, and task allocation. This enables the model to identify the necessary visual contextual information for different tasks through decomposition and analysis. In the 3 Table 1: Summary of multimodal reasoning methods. LCMR stands for Language-centric Multimodal Reasoning, and CMR stands for Collaborative Multimodal Reasoning. One-Pass and Active refer to two types of visual perception methods. Task Characteristic Category Method CCoT [36] Cantor [17] Astar [55] LlamaV-o1 [50] LLaVA-CoT [59] LLaVA-Reasoner [65] Insight-V [15] InternVL2-MPO[54] R1-V [6] R1-Onevision [61] Mulberry [62] R3V [14] Vision-R1 [25] R1-Zero [72] Visual-RFT [34] QVQ [49] Kimi k1.5 [48] Skywork-R1V [42] DDCoT [71] HYDRA [27] FAST [47] VisualReasoner [13] ICoT [16] Visual-o1 [37] VoCoT [32] VisCoT [46] OnePass LCMR Active Year Strategy 23.11 Training-Free 24.04 Training-Free 25.02 Training-Free 25.01 24.11 24.10 24.10 24.11 25.02 25.03 24.12 24.11 25.03 25.03 25.03 24.12 25.01 25.03 SFT SFT SFT+RL SFT+RL SFT+RL SFT+RL SFT+RL SFT SFT SFT+RL RL RL SFT+RL SFT+RL RL SFT SFT 23.10 Training-Free 24.03 24.08 24.06 24.11 Training-Free 24.10 Training-Free 24.05 24.03 SFT SFT General Reasoning Science VQA, Math General Reasoning General Reasoning General Reasoning General Reasoning General Reasoning General Reasoning Visual Counting General Reasoning General Reasoning General Reasoning, Web Navigation General Reasoning General Reasoning Object Detection, Visual Grounding, Classification General Reasoning General Reasoning, Code Generation General Reasoning Science VQA General Reasoning, Visual Grounding General Reasoning, Referencing Segmentation General Reasoning General Reasoning General Reasoning, Referencing Segmentation General Reasoning, Spatial Reasoning General Reasoning, Visual Grounding CMR MVoT [31] MuZero [45] VAP [57] Visual Sketchpad [23] IoT [73] SFT RL 25.01 20.02 24.09 Training-Free 24.06 Training-Free 24.05 Training-Free Spatial Reasoning Go, Atari General Reasoning General Reasoning General Reasoning Reasoning with Scene Graph High-level Information Cognition MCTS-Powered Thought Card VRC-Benchmark and Curriculum Learning Stage-level Beam Search 193K ShareGPT-4o-Reasoning Dataset Multi-agent and Iterative DPO Mixed Preference Optimization Loss Reasoning Data from DeepSeek R1 155k CoT Data from Deepseek-R1 Collective MCTS and Self-training Iterative SFT with Self-refine Loss 200K CoT Data and PTST Strategy Applying RL on 2B non-SFT MLLM Visual RFT on Limited Data Long Context Scaling during RL Iterative SFT and GRPO Visual perception with VQA models Visual perception with foundation models Fast and slow thinking mode Bottom-up training data synthesis CoT with relevant visual tokens Multi-turn reasoning Key object-centered CoT CoT with key regions State updates by model itself State update and action generation together State update by tools State update by tools State update by tools second phase, the model is required to complete various subtasks, invoking the MLLM to generate corresponding high-level visual features based on task analysis. Finally, the results of the subtasks are synthesized and summarized to provide the final answer. The framework proposed in this article is characterized by dividing visual perception into two stages during the reasoning process: (1) analysis of visual contextual information requirements and (2) generation of high-level visual features. CCoT [36] proposes using scene graphs to formally describe the results of visual reasoning. The advantage of scene graphs lies in their ability to provide highly structured representation of visual objects, relationships, and attributes within an image, thus overcoming the limitations of purely textual descriptions. To address the high data collection cost associated with scene graphs, the article introduces two-stage workflow to construct compositional reasoning chain. Initially, MLLMs generate relevant scene graphs based on the image, question description, and scene graph requirements. Subsequently, the MLLM receives prompts involving the scene graph, image, and task to produce the final answer. The article emphasizes the importance of the relationships between objects and attributes in an image for solving visual reasoning tasks, highlighting that the proposed scene graph representation enhances the MLLMs reasoning about object and attribute relationships compared to captions. AStar [55]s approach involves constructing \"thought cards\" as external explicit reasoning guidelines. Inspired by human behavior, AStar first defines six types of reasoning actions as the building blocks of thought cards. It then leverages the advantages of Monte Carlo Tree Search (MCTS) to sample large amount of reasoning path data from small set of raw data. By categorizing the reasoning actions, it labels all reasoning data to create the final thought cards. During the reasoning phase, heuristic prompts are constructed by retrieving the thought card most similar to the target problem, guiding MLLMs to output the reasoning steps and results for the target problem. The advantage of thought cards lies in the ability to generate large and diverse set of reasoning path data through MCTS sampling. Compared to training methods that use sampled data, AStar achieves compelling balance between performance and efficiency. 3.2 Learning-based Solutions This type of works [50, 59, 65, 46, 15, 54, 6, 61, 62, 14, 25, 72, 34] involve first constructing multimodal training dataset that includes reasoning chains, followed by applying Supervised Fine4 Tuning and reinforcement learning to MLLMs. We will elaborate on the data construction methods and training paradigms. 3.2.1 Data Construction common approach involves utilizing powerful teacher models (e.g., GPT-4o, Deepseek-R1) for knowledge distillation and data filtering, including chain-of-thought reasoning and scoring of reasoning results or each reasoning step. Recent works [50, 59, 65, 46] employs \"Lets think step by step\" prompt to input VQA data into GPT-4o, obtaining chain-of-thought outputs and answers. Additionally, [15, 54] select robust open-source models such as QwenVL and InternVL for data filtering. Recent work has also explored using advanced language reasoning models for knowledge distillation. To address the challenge of language models being unable to process image modalities, [6] convert images into captions, while [61] design formalized text grammar to describe musical scores, tables, and images. Once images are expressed in text, they are input along with the original questions into language reasoning models to obtain solutions in chain-of-thought format. The advantage of this method is leveraging the strong reasoning capabilities of language models, while the challenge lies in the potential information loss when converting images to text. Another series of works [62, 14] involves the policy model (MLLMs) autonomously sampling and generating reasoning path samples, followed by iterative self-training. [62] proposes implementing MCTS within policy model ensemble composed of multiple MLLMs to enhance the diversity of reasoning path data, constructing reflective paths based on negative nodes in the sampling process to supplement training data. [14] similarly utilize the numerous negative samples generated during sampling to create reflective training data, thereby guiding the enhancement of the models reflective capabilities. 3.2.2 Training Paradigm SFT Only [59, 62, 50, 46, 14]. This approach involves training MLLMs using SFT, with chainof-thought reasoning and answers as prediction targets. The models are fine-tuned using Next Token Prediction loss function. Futhermore, the concept of curriculum learning is employed by [50], implementing two-stage SFT training method for MLLMs, progressing from easy to difficult tasks: initially completing simple image captioning tasks, followed by complex multimodal reasoning tasks. [62, 14] propose iterative self-training using reasoning path samples obtained by the model itself, enhancing the quality of sampled data and reasoning capabilities through continuous reflection. SFT + RL-based Training [65, 15, 61, 6, 54, 25]. The training process is divided into two phases. In the first phase, the aforementioned SFT training is conducted on MLLMs to obtain policy model πθ. In the second phase, preference dataset is sampled based on πθ, and DPO [65, 15, 54] or GRPO [61, 6, 25] methods are typically used to obtain the optimized policy model π θ . Furthermore, an iterative DPO training method is employed by [15], executing multiple rounds of data sampling and DPO to better simulate online reinforcement learning. To improve training efficiency, [54] propose the MPO training framework, which combines Next Token Prediction and DPO loss functions in single training process to achieve both SFT and reinforcement learning objectives simultaneously. RL-based Training Only[34, 72]. GRPO is directly applied to non-SFT models. Visual RFT [34] designed three verifiable reward functions related to visual tasks: IOU and confidence rewards for object detection tasks, and classification accuracy rewards for classification tasks. This work found that combining reinforcement fine-tuning with policy models enhances few-shot learning capabilities and generalization, offering significant advantages over SFT in data-limited scenarios. R1-zero [72] validate the feasibility of this training strategy on small parameter models (Qwen2-VL-2B) and replicated the \"aha moment\" during experiments."
        },
        {
            "title": "4 Active Visual Perception",
            "content": "multimodal reasoning tasks depend on extracting rich and multi-grained visual information from images. However, current Multimodal Large Language Models (MLLMs) exhibit limited visual comprehension capabilities, making one-pass visual perceptionexamining the image only once during the problem formulation stagepotentially insufficient for acquiring the critical visual information required for reasoning. Specifically, 1) one-pass visual perception is difficult to realize fine-grained visual semantic understanding, leading to hallucinations and misunderstandings during reasoning [5, 75, 24, 11]; 2) one-pass visual perception struggles to comprehensively cap5 ture multi-granularity and multi-region visual cues in the image, resulting in the omission of key information [63, 70, 4]. To overcome the limitations of single visual perception, some studies [16, 37, 32, 71, 27, 47, 13] incorporate multiple rounds of visual perception into the multimodal reasoning process. Just as humans need to examine images multiple times to fully understand them, these methods involve iteratively extracting various levels of visual information from images throughout the reasoning process, thereby achieving enhanced performance on reasoning task. 4.1 Self Iterative Perception This type of works utilizes the intrinsic visual capabilities of MLLM to perform multiple rounds of visual perception. ICoT [16] generates reasoning steps with paired visual and textual rationales to express the fine-grained associations between the image and the reasoning process. Specifically, it selects top-k relevant visual tokens based on attention maps and inserts them into the reasoning steps, making it adaptable to various MLLMs without additional training. Visual-O1 [37] employs multiturn chain-of-thought reasoning to assist models in correctly understanding ambiguous instructions. During each iteration, the model leverages previous disambiguation experience to analyze the image and instruction, and reflects on the reasoning process to update the disambiguation experience. This iterative cycle continues until the model achieves clear understanding of the instruction, allowing it to output the correct answer. In addition, the scarcity of multi-step reasoning data with interleavedmodal reasoning steps is an important issue that hinders the training of MLLMs for multimodal reasoning. To address this, some works construct chain-of-thought (CoT) datasets that incorporate multiple visual perceptions through well-designed generation process. VisCoT [46] creates CoT dataset annotated with bounding boxes highlighting key regions essential for answering the questions. During the reasoning process, the MLLM initially predicts key regions with bounding boxes and incorporates the visual features of these regions. Similarly, VoCoT [32] constructs dataset with key object-centered chains-of-thoughts, where key objects are represented in the format of <text description, coordinates, visual object representation> in each reasoning step. By performing supervised fine-tuning (SFT) on these datasets, the models generate multimodal chains of thought in the specified format, simultaneously improving the models visual perception and reasoning capabilities. 4.2 Tool-assisted Iterative Perception Given the limitations in visual processing capabilities of MLLMs, this type of works employs specialized perception tools to perform multiple rounds of visual perception. Early multimodal reasoning methods were based on LLMs. Since LLMs are unable to accept image as input, such methods usually convert images into captions as visual input and use specialized visual models to achieve further visual perception in the reasoning process. DDCoT [71] decomposes the question into sub-questions based on LLM and answers the uncertain sub-questions using VQA model. The LLM then integrates the information and generates rationale with critical thinking, resulting in more accurate and interpretable multimodal reasoning. HYDRA [27] utilizes LLM planner to generate multiple instruction samples, each with varying complexity and validity probabilities. An RL agent then evaluates and selects the most promising samples for execution. Next, the LLM reasoner translates these instructions into executable Python code, which leverages vision foundation models (e.g., GLIP, BLIP2, XVLM) to perform visual tasks such as object detection, verification, and captioning. The resulting visual information is converted to text format and stored in the Memory Bank to support incremental reasoning Additionally, some MLLM-based methods integrate task-specific models for more accurate visual perception. FAST [47] employs switch adapter to dynamically select between fast and slow thinking mode, tailoring the problem-solving approach to different task complexity. For the slow thinking mode, two specific adapters are trained to generate target regions and pixel-level segmentations to compose multimodal chains of evidence. VisualReasoner [13] adopts bottom-up approach to automatically generate questions and multi-step reasoning paths for images, followed by supervised fine-tuning on this synthetic CoT dataset. It divides the complex synthesis task into few simple subtasks, and relies on open-sourced models to accomplish the sub-tasks, such as grounding, highlighting and Optical Character Recognition (ORC). Discussions. The existing methods introduce multiple rounds of visual perceptions based on the intrinsic visual capabilities of MLLM or by invoking specialized visual models to executing sub-tasks 6 like grounding, highlighting, and OCR. By performing visual perception multiple times during reasoning, the model effectively extracts rich and multi-grained information from the image, thereby enhancing its multimodal reasoning capability and improving interpretability. However, the effectiveness of these methods is still constrained by the visual capabilities of MLLMs and specialized visual models. Besides, they face challenges with tasks that require visual state updating during the reasoning process, such as spatial reasoning."
        },
        {
            "title": "5 Collaborative Multimodal Reasoning",
            "content": "While large language models demonstrate strong reasoning capabilities, they typically treat visual information as static context rather than actively integrating it into the reasoning process. In this section, we summarize series of approaches [31, 45, 57, 33, 23, 73] where visual information is modified during the reasoning process, concept we refer to as collaborative multimodal reasoning. The essence of collaborative multimodal reasoning lies in simultaneously generating actions in the language space and updating states in the visual space. Similar to human cognitive processes, humans flexibly integrate both language and visual reasoning when solving problems. For example, in the game of Go, player must not only decide on the next move (action generation) but also simultaneously understand and simulate the evolving board state after each move (state update). Based on this definition, the implementation of collaborative multimodal reasoning can be generally categorized into the following two approaches. 5.1 State Update without Action Reasoning In this scenario, the models reasoning process is reflected in predicting the next-state representation. This requires multimodal architecture capable of visual generation. However, due to the limitations of current models visual generation capabilities, existing research primarily focuses on simple tasks. In the spatial reasoning task, MVoT [31] overcomes the limitations of traditional language reasoning by incorporating visual state changes into the reasoning process. By enhancing its image generation capabilities, the model can visualize its reasoning process, enabling it to \"think\" in more unified and coherent manner, rather than relying solely on language space. 5.2 Joint Action Reasoning and State Update In this scenario, the model first generates actions to accomplish well-defined reasoning objective and then updates the state based on the generated actions. In early research, MuZero [45] facilitates state transitions and action generation by dynamic and prediction functions. The action generation and state updates are performed directly on the latent space, allowing it to operate without direct feedback from the real environment. Realizing the limitations of visual generative capabilities in complex and sequential reasoning, recent studies have attempted to leverage external tools to update visual states. Specifically, models are equipped with visualization canvas and tools for drawing, allowing them to refine visual representations in the reasoning process. In mathematical reasoning tasks, libraries such as Matplotlib and Seaborn [57, 33, 23] are employed to generate plots or add auxiliary lines, while in visual tasks, segmentation models are used to generate masks [73]. Without the need for additional post-training of multimodal language models, these modified visual representations significantly improve the models reasoning capability cooperating with carefully designed prompt, enabling more effective solutions to multimodal tasks. Discussions: The reasoning ability demonstrated by the language model is based on the powerful generation ability of the model itself. However, the deficiency of multimodal models in visual generation has led to the fact that collaborative multimodal reasoning has not made sufficient progress. It should be pointed out that the process of state update through external tools is not the final form of multimodal reasoning. Future research should first focus on designing model architecture suitable for multimodal reasoning, integrating visual and language generation to achieve more seamless and autonomous reasoning process. 7 Table 2: Summary of multimodal reasoning benchmarks. General stands for general reasoning task, Academic stands for academic-based task, Spatial stand for spatial reasoning task, and Logical stands for logical reasoning task. Benchmark Year M3CoT [9] MathVista [35] MME-CoT [26] VISCO [56] MMIR [60] EMMA [20] SpatialEval [52] MM-IQ [3] ZeroBench [44] 2024 2024 2025 2024 2025 2025 2024 2025 2025 Task General Academic Spatial Logical Metrics Samples Accuracy Accuracy Accuracy, Stablity, Efficiency Accuracy Accuracy Accuracy Accuracy Accuracy Accuracy 11K 6K 1.1K 1.6K 0.5K 2.7K 4.6K 2.7K 0.1K"
        },
        {
            "title": "6 Benchmark",
            "content": "Multimodal reasoning benchmarks [3, 9, 56, 26, 35, 21, 20, 60] primarily focuses on four key task categories: general reasoning, academic-based reasoning, spatial reasoning, and logical reasoning. General reasoning tasks require models to utilize commonsense or general knowledge to answer questions effectively. Academic-based reasoning tasks involve subject-specific questions from areas such as science, mathematics and code, where models must apply appropriate principles and formulas. Spatial reasoning tasks evaluate models ability to perceive and reason about spatial relationships between objects or elements. Logical reasoning tasks test models capacity to manipulate logical symbols and understand semantic logic. The summary of benchmarks as shown in Table 2. 6.1 General Reasoning General reasoning tasks typically omit explicit information about target objects of the question, requiring models to rely on general knowledge or commonsense reasoning to produce correct answers. In [3], the queries involve real-world entities such as vases, leaves, and animals. Multimodal models must classify these objects based on visual features, often drawing on external or implicit knowledge. To assess commonsense reasoning, [9] refines and extends the Sherlock dataset [21] by generating questions, multiple-choice options, and corresponding answers grounded in visual cues, creating benchmark specifically designed for commonsense reasoning evaluation. 6.2 Academic-based Reasoning Academic reasoning tasks involve problems that require the application of subject-specific knowledgesuch as mathematics, chemistry, and codefor multi-step reasoning. These tasks typically cannot be solved in single step and are designed to assess models ability to perform complex, structured reasoning [3, 9, 56, 26, 35, 20]. To facilitate research in this area, several benchmarks have been proposed. For example, [9] introduces manually annotated dataset in science and mathematics, incorporating chain-of-thought (CoT) rationales. Visual-free samples are first filtered from existing datasets, and CoTs are manually annotated to strengthen the alignment between visual and textual reasoning. Similarly, [3] constructs comprehensive benchmark using publicly available questions from the Chinese National Civil Service Exam. Recent advances in large language models (LLMs) have enabled their use in preliminary data filtering, with subsequent human refinement to ensure high-quality annotations. [35] presents benchmark covering diverse mathematical reasoning tasksspanning algebra, geometry, logic, science, and statistics. Relevant data are selected using LLMs and then annotated by humans. In [26], classification framework is proposed where LLMs compare model performance with and without CoT. Samples identified as requiring CoT are subsequently verified and annotated manually. [20] contends that high-level cross-modal reasoning cannot be achieved through unimodal approaches alone. Accordingly, questions that can be answered solely based on image captions are removed. 8 The remaining questions are categorized, and additional samples are curated to ensure balanced distribution across task types. Given its significant impact on reasoning, the ability to critique and correct CoT is essential. To address this, [56] introduces the VISCO dataset, which combines outputs from large vision-language models (LVLMs) with human verification. CoTs are generated and segmented into individual steps, each manually validated. Errors are annotated with explanations and corrected responses, enabling comprehensive assessment of reasoning refinement capabilities. 6.3 Spatial Reasoning Spatial reasoning is critical task for evaluating models ability to understand and reason about spatial relationships. [44] introduces dataset comprising 100 complex spatial problems, aimed at assessing model performance in tasks such as counting and identifying points of intersection and localising points on maps. To increase task difficulty, careful manual selection process was applied, ensuring the inclusion of visually complex elements and multi-step reasoning requirements. In addition, static relative spatial relationshipssuch as those found in Ravens Progressive Matrices, visual analogies, inference, and groupingare also central to spatial reasoning [3]. These datasets typically adhere to the standard Visual Question Answering (VQA) paradigm, which involves pairs of images and questions. Extending these approaches, [52] proposed the Visual Text Question Answering (VTQA) framework, which incorporates textual descriptions of images alongside visual inputs. This enriched format covers broader range of spatial reasoning skills, including relational understanding, navigation, and enumeration. Empirical results show that such additional textual information significantly improves model accuracy. 6.4 Logical Reasoning Logical reasoning tasks can be broadly categorized into two main types: logical operations and semantic logic. Tasks involving logical operations primarily evaluate models ability to apply formal logic operators such as AND, OR, and XOR. These tasks generally require the model to identify visual patterns encoding abstract logical operations, induce the underlying rules, and apply these rules to derive the correct output [3]. In contrast, semantic logic tasks focus on detecting cross-modal inconsistencies within complex, multi-element layouts. These inconsistencies manifest in five primary forms: factual contradictions, identity attribution errors, contextual mismatches, numerical discrepancies, and temporal/spatial inconsistencies. To develop benchmark for such tasks, [60] employs GPT-o1 to generate semantically inconsistent elements across webpages, presentation slides, and posters. These synthetic inconsistencies are further refined using automated tools and subsequently verified by human annotators, culminating in dataset of 534 meticulously curated and challenging samples. 6.5 Metrics Comprehensive and diverse metrics are crucial for evaluating multimodal reasoning capabilities. Existing metrics include: Accuracy, Stability and Efficiency. Accuracy: In multimodal reasoning tasks, it is important to evaluate not only the correctness of the final answer but also the accuracy of each thought in the reasoning process. Therefore, accuracy is defined as follows: Accuracy = Tmatched (1) where = t1, t2, . . . , tN denotes the COT, denotes its length. Tmatched denotes the set of reasoning steps that match the groundtruth. Specifically, when directly measuring the answer, = 1. Accuracy is one of the most commonly used metrics and is widely adopted in multimodal reasoning research. Stablity: COT can enhance models reasoning capabilities, but how it affects the models perception ability remains unknown. Stability measures the impact of CoT on the models perception ability: Stablity = AccP COT AccP DIR (2) 9 where AccP the accuracy when directly answering without CoT. COT represents the accuracy on perception tasks with CoT, AccP DIR represents Efficiency : Reasoning Efficiency refers to the proportion of effective reasoning steps within the entire reasoning process. This metric was proposed by [4]. First, GPT-4o is used to evaluate whether each reasoning step contributes to generating the final answer. Then, the ratio of useful reasoning steps to all reasoning steps is calculated as follows: Ef iciency = α 1 α ; = Trelevant (3) Where, Trelevant represents the number of reasoning steps related to the answer, and α is constant. Efficiency can directly reflect the information redundancy generated by the model and directly affect the generation speed of the model."
        },
        {
            "title": "7 Future Direction",
            "content": "7.1 From Vision-Language Reasoning to Omni Reasoning Omni model is unified framework that integrates multiple modalities, including text, image, audio, and video, to achieve comprehensive data processing and understanding. The impressive performance of GPT-4o [38] demonstrates the significant potential of Omni models in perceiving and processing fully multimodal data, marking notable achievement in Stage 1 (Chatbots, with conversational language). The subsequent introduction of R1-Omni [69] indicates the transition from Stage 1 to Stage 2 (Reasoners, referring to human-level problem-solving ability) of Omni models. In contrast to vision-language (VL) models, Omni models extend to audio modality, which introduces substantial changes in both perception and state within Omni reasoning tasks. Perception: from Vision to Omni In contrasted with VL models, Omni models expand the reasoning space by incorporating diverse modal information, which consequently increases the complexity of reasoning tasks. During inference, Omni model is required to simultaneously perceive and integrate information from both vision and audio modalities to perform joint reasoning. There are two key challenges in Omni perception: 1) Limited fine-grained cross-modal understanding: In audio and visual modalities, Omni models struggle with tasks that require deep contextual comprehension or involve subtle cross-modal nuances, leading to the use of incorrect or irrelevant information during reasoning. 2) Inconsistent capability across modalities: The same task presented in different modalities can lead to inconsistent perceptions. Results from the OmniR benchmark [7] indicate that when math problem is presented in text form, the model can generate correct answers with coherent reasoning. However, when the same problem is delivered via audio or video, the model may fail to produce reasoning altogether or generate incorrect answers. These modality-induced variations pose significant challenge to Omni reasoning. State update: from VL generation to Omni generation Collaborative Multimodal Reasoning lies in simultaneously generating actions in the language space and updating states in the visual space. When extended to Omni models, state updates should consider extending beyond the visual domain to the audio space, capturing additional information essential for accurate reasoning. In tasks driven purely by audio inputs (e.g., ASR or speech translation), the model can generate reasoning outputs solely in the audio modality. However, when audio and visual modalities are jointly involved, both audio and visual state changes are necessary. For example, given the state \"It is raining outside\", an image may show only an indoor scene. The presence of background rain sounds in the audio stream can provide critical context for inferring external weather conditions, thus improving the quality and completeness of reasoning. Enabling such multimodal state updates in Omni models presents significant challenge: the model must be capable of both understanding and generating data across all modalities, which goes beyond standard cross-modal alignment. Consequently, progress in Omni reasoning depends not only on advances in reasoning algorithms but also on improving the perceptual and generative capabilities of Omni model. 7.2 From Multimodal Reasoning to Multimodal Agents OpenAIs roadmap to AGI consists of five progressive stages, with the transition from Stage 2 to Stage 3 (Agents, where systems can take actions) marking critical shift. Reasoners primarily focus on 10 internal cognitive reasoning, meaning they engage in complex reasoning and problem-solving based on given information. In contrast, Agents possess the ability to take actions in an open environment, expanding from mere \"thinking\" to actual \"execution\". This transition from internal cognition to external action underpins the subsequent discussions on action generation and state updates. Action generation: from Reasoning-driven to Interaction-driven In multimodal reasoning tasks, action generation does not directly interact with the environment but instead manifests as generating intermediate reasoning steps or conclusions based on multimodal inputs. Unlike reasoning models that operate within an internal cognitive framework, agents must execute actions and adapt to the resulting environmental changes. For example, in web browsing task, an agent must generate executable actions (such as \"click the login button\") based on the web pages visual structure and text content while also adapting to changes in the page and making subsequent decisions accordingly. To achieve this transition, reinforcement learning plays crucial role in bridging the gap between reasoning and autonomous decision-making. By continuously optimizing its actions based on environmental feedback, an agent can improve its decision-making process, ensuring that its interactions are not only active but also strategically planned for long-term goals [40]. State update: from Cognition-driven to Environment-driven In multimodal reasoning, state updates primarily involve adjustments in cognitive representations, where the model dynamically refines its internal reasoning process to maintain logical coherence or enhance inferential depth. For instance, in computational tasks, adding auxiliary lines can help the model achieve deeper understanding of given problem. However, since agents actions directly alter the external environment, their state updates are determined by real-world changes, and often unpredictable. key advancement in enabling effective state updates is environment modeling, which allows agents to transition from static reasoning to dynamic interaction. By building structured representation of the external environment, such as sandbox [74, 28] or world model [51, 19], the agents are enabling more adaptive and proactive decision-making in open-ended scenarios [41]."
        },
        {
            "title": "8 Conclusions",
            "content": "This survey categorized multimodal reasoning into two levels based on the role of multimodal information in the reasoning process: Language-centric Multimodal Reasoning and Collaborative Multimodal Reasoning. This classification highlights the progression from pure language reasoning to comprehensive multimodal reasoning, offering framework to analyze and anticipate future development directions. Specifically, we examined two promising directions Omni Reasoning and Multimodal Agents which focus on expanding the diversity of modalities in reasoning and merging reasoning with action, respectively. However, we argue that advanced multimodal reasoning and multimodal agents, similar to language reasoning supported by strong language models and agent models relying on strong reasoning models [67], must be grounded in superior multimodal foundation models. Current Multimodal Large Language Models (MLLMs) are still fundamentally anchored to language priors. Substantive progress in multimodal reasoning requires unified multimodal understanding and generation, as well as continued exploration of joint multimodal pretraining paradigms that transcend language-centric biases. This shift will establish inherent cross-modal interactions rather than treating vision as an auxiliary signal to language reasoning, thereby enabling native and deeper multimodal reasoning capabilities that approach human-like cross-modal cognition."
        },
        {
            "title": "References",
            "content": "[1] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. [2] Dibyanayan Bandyopadhyay, Soham Bhattacharjee, and Asif Ekbal. Thinking machines: survey of llm based reasoning strategies. arXiv preprint arXiv:2503.10814, 2025. [3] Huanqia Cai, Yijun Yang, and Winston Hu. Mm-iq: Benchmarking human-like abstraction and reasoning in multimodal models. arXiv preprint arXiv:2502.00698, 2025. [4] Gongwei Chen, Leyang Shen, Rui Shao, Xiang Deng, and Liqiang Nie. Lion: Empowering multimodal large language model with dual-level visual knowledge. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2654026550, 2024. [5] Jun Chen, Han Guo, Kai Yi, Boyang Li, and Mohamed Elhoseiny. Visualgpt: Data-efficient adaptation of pretrained language models for image captioning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1803018040, 2022. [6] Liang Chen, Lei Li, Haozhe Zhao, and Yifan Song. R1-v: Reinforcing super generalization ability in vision-language models with less than 3, 2025. [7] Lichang Chen, Hexiang Hu, Mingda Zhang, Yiwen Chen, Zifeng Wang, Yandong Li, Pranav Shyam, Tianyi Zhou, Heng Huang, Ming-Hsuan Yang, et al. Omnixr: Evaluating omni-modality language models on reasoning across modalities. arXiv preprint arXiv:2410.12219, 2024. [8] Qiguang Chen, Libo Qin, Jinhao Liu, Dengyun Peng, Jiannan Guan, Peng Wang, Mengkang Hu, Yuhang Zhou, Te Gao, and Wangxiang Che. Towards reasoning era: survey of long chain-of-thought for reasoning large language models. arXiv preprint arXiv:2503.09567, 2025. [9] Qiguang Chen, Libo Qin, Jin Zhang, Zhi Chen, Xiao Xu, and Wanxiang Che. M3cot: novel benchmark for multi-domain multi-step multi-modal chain-of-thought. arXiv preprint arXiv:2405.16473, 2024. [10] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2418524198, 2024. [11] Zhiyang Chen, Yousong Zhu, Yufei Zhan, Zhaowen Li, Chaoyang Zhao, Jinqiao Wang, and Ming Tang. Mitigating hallucination in visual language models with visual supervision. arXiv preprint arXiv:2311.16479, 2023. [12] Zhuo Chen, Yichi Zhang, Yin Fang, Yuxia Geng, Lingbing Guo, Xiang Chen, Qian Li, Wen Zhang, Jiaoyan Chen, Yushan Zhu, et al. Knowledge graphs meet multi-modal learning: comprehensive survey. arXiv preprint arXiv:2402.05391, 2024. [13] Chuanqi Cheng, Jian Guan, Wei Wu, and Rui Yan. From the least to the most: Building plug-and-play visual reasoner via data synthesis. arXiv preprint arXiv:2406.19934, 2024. [14] Kanzhi Cheng, Yantao Li, Fangzhi Xu, Jianbing Zhang, Hao Zhou, and Yang Liu. Visionlanguage models can self-improve reasoning via reflection. arXiv preprint arXiv:2411.00855, 2024. [15] Yuhao Dong, Zuyan Liu, Hai-Long Sun, Jingkang Yang, Winston Hu, Yongming Rao, and Ziwei Liu. Insight-v: Exploring long-chain visual reasoning with multimodal large language models. arXiv preprint arXiv:2411.14432, 2024. [16] Jun Gao, Yongqi Li, Ziqiang Cao, and Wenjie Li. Interleaved-modal chain-of-thought. arXiv preprint arXiv:2411.19488, 2024. [17] Timin Gao, Peixian Chen, Mengdan Zhang, Chaoyou Fu, Yunhang Shen, Yan Zhang, Shengchuan Zhang, Xiawu Zheng, Xing Sun, Liujuan Cao, et al. Cantor: Inspiring multimodal chain-of-thought of mllm. In Proceedings of the 32nd ACM International Conference on Multimedia, pages 90969105, 2024. [18] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. 12 [19] Shibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang, Daisy Zhe Wang, and Zhiting Hu. Reasoning with language model is planning with world model. arXiv preprint arXiv:2305.14992, 2023. [20] Yunzhuo Hao, Jiawei Gu, Huichen Will Wang, Linjie Li, Zhengyuan Yang, Lijuan Wang, and Yu Cheng. Can mllms reason in multimodality? emma: An enhanced multimodal reasoning benchmark. arXiv preprint arXiv:2501.05444, 2025. [21] Jack Hessel, Jena Hwang, Jae Sung Park, Rowan Zellers, Chandra Bhagavatula, Anna Rohrbach, Kate Saenko, and Yejin Choi. The abduction of sherlock holmes: dataset for visual abductive reasoning. In European Conference on Computer Vision, pages 558575. Springer, 2022. [22] Shengding Hu, Yuge Tu, Xu Han, Chaoqun He, Ganqu Cui, Xiang Long, Zhi Zheng, Yewei Fang, Yuxiang Huang, Weilin Zhao, et al. Minicpm: Unveiling the potential of small language models with scalable training strategies. arXiv preprint arXiv:2404.06395, 2024. [23] Yushi Hu, Weijia Shi, Xingyu Fu, Dan Roth, Mari Ostendorf, Luke Zettlemoyer, Noah Smith, and Ranjay Krishna. Visual sketchpad: Sketching as visual chain of thought for multimodal language models. arXiv preprint arXiv:2406.09403, 2024. [24] Qidong Huang, Xiaoyi Dong, Pan Zhang, Bin Wang, Conghui He, Jiaqi Wang, Dahua Lin, Weiming Zhang, and Nenghai Yu. Opera: Alleviating hallucination in multi-modal large language models via over-trust penalty and retrospection-allocation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1341813427, 2024. [25] Wenxuan Huang, Bohan Jia, Zijie Zhai, Shaosheng Cao, Zheyu Ye, Fei Zhao, Yao Hu, and Shaohui Lin. Vision-r1: Incentivizing reasoning capability in multimodal large language models. arXiv preprint arXiv:2503.06749, 2025. [26] Dongzhi Jiang, Renrui Zhang, Ziyu Guo, Yanwei Li, Yu Qi, Xinyan Chen, Liuhui Wang, Jianhan Jin, Claire Guo, Shen Yan, et al. Mme-cot: Benchmarking chain-of-thought in large multimodal models for reasoning quality, robustness, and efficiency. arXiv preprint arXiv:2502.09621, 2025. [27] Fucai Ke, Zhixi Cai, Simindokht Jahangard, Weiqing Wang, Pari Delir Haghighi, and Hamid Rezatofighi. Hydra: hyper agent for dynamic compositional visual reasoning. In European Conference on Computer Vision, pages 132149. Springer, 2024. [28] Jing Yu Koh, Robert Lo, Lawrence Jang, Vikram Duvvur, Ming Chong Lim, Po-Yu Huang, Graham Neubig, Shuyan Zhou, Ruslan Salakhutdinov, and Daniel Fried. Visualwebarena: Evaluating multimodal agents on realistic visual web tasks. arXiv preprint arXiv:2401.13649, 2024. [29] Junlin Lee, Yequan Wang, Jing Li, and Min Zhang. Multimodal reasoning with multimodal knowledge graph. arXiv preprint arXiv:2406.02030, 2024. [30] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, et al. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. [31] Chengzu Li, Wenshan Wu, Huanyu Zhang, Yan Xia, Shaoguang Mao, Li Dong, Ivan Vulic, and Furu Wei. Imagine while reasoning in space: Multimodal visualization-of-thought. arXiv preprint arXiv:2501.07542, 2025. [32] Zejun Li, Ruipu Luo, Jiwen Zhang, Minghui Qiu, and Zhongyu Wei. Vocot: Unleashing visually grounded multi-step reasoning in large multi-modal models. arXiv preprint arXiv:2405.16919, 2024. [33] Yujie Lin, Ante Wang, Moye Chen, Jingyao Liu, Hao Liu, Jinsong Su, and Xinyan Xiao. Investigating inference-time scaling for chain of multi-modal thought: preliminary study. arXiv preprint arXiv:2502.11514, 2025. [34] Ziyu Liu, Zeyi Sun, Yuhang Zang, Xiaoyi Dong, Yuhang Cao, Haodong Duan, Dahua Lin, and Jiaqi Wang. Visual-rft: Visual reinforcement fine-tuning. arXiv preprint arXiv:2503.01785, 2025. [35] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. arXiv preprint arXiv:2310.02255, 2023. 13 [36] Chancharik Mitra, Brandon Huang, Trevor Darrell, and Roei Herzig. Compositional chain-ofthought prompting for large multimodal models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1442014431, 2024. [37] Minheng Ni, Yutao Fan, Lei Zhang, and Wangmeng Zuo. Visual-o1: Understanding ambiguous instructions via multi-modal multi-turn chain-of-thoughts reasoning. arXiv preprint arXiv:2410.03321, 2024. [38] Openai. Hello gpt-4o. https://openai.com/index/hello-gpt-4o/, 2023. [39] Openai. Introducing openai o1-preview. https://openai.com/index/introducing-openai-o1preview/, 2023. [40] Openai. Introducing deep research. https://openai.com/index/introducing-deep-research/, 2025. [41] Openai. Introducing operator. https://openai.com/index/introducing-operator/, 2025. [42] Yi Peng. Skywork r1v:pioneering multimodal reasoning with chain-of-thought. https://github.com/SkyworkAI/Skywork-R1V/, 2025. [43] Aske Plaat, Annie Wong, Suzan Verberne, Joost Broekens, Niki van Stein, and Thomas Back. Reasoning with large language models, survey. arXiv preprint arXiv:2407.11511, 2024. [44] Jonathan Roberts, Mohammad Reza Taesiri, Ansh Sharma, Akash Gupta, Samuel Roberts, Ioana Croitoru, Simion-Vlad Bogolin, Jialu Tang, Florian Langer, Vyas Raina, et al. Zerobench: An impossible visual benchmark for contemporary large multimodal models. arXiv preprint arXiv:2502.09696, 2025. [45] Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Laurent Sifre, Simon Schmitt, Arthur Guez, Edward Lockhart, Demis Hassabis, Thore Graepel, et al. Mastering atari, go, chess and shogi by planning with learned model. Nature, 588(7839):604609, 2020. [46] Hao Shao, Shengju Qian, Han Xiao, Guanglu Song, Zhuofan Zong, Letian Wang, Yu Liu, and Hongsheng Li. Visual cot: Advancing multi-modal language models with comprehensive dataset and benchmark for chain-of-thought reasoning. Advances in Neural Information Processing Systems, 37:86128642, 2024. [47] Guangyan Sun, Mingyu Jin, Zhenting Wang, Cheng-Long Wang, Siqi Ma, Qifan Wang, Tong Geng, Ying Nian Wu, Yongfeng Zhang, and Dongfang Liu. Visual agents as fast and slow thinkers. arXiv preprint arXiv:2408.08862, 2024. [48] Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, et al. Kimi k1. 5: Scaling reinforcement learning with llms. arXiv preprint arXiv:2501.12599, 2025. [49] Qwen Team. Qvq: To see the world with wisdom, December 2024. [50] Omkar Thawakar, Dinura Dissanayake, Ketan More, Ritesh Thawkar, Ahmed Heakl, Noor Ahsan, Yuhao Li, Mohammed Zumri, Jean Lahoud, Rao Muhammad Anwer, et al. Llamav-o1: Rethinking step-by-step visual reasoning in llms. arXiv preprint arXiv:2501.06186, 2025. [51] Sifan Tu, Xin Zhou, Dingkang Liang, Xingyu Jiang, Yumeng Zhang, Xiaofan Li, and Xiang Bai. The role of world models in shaping autonomous driving: comprehensive survey. arXiv preprint arXiv:2502.10498, 2025. [52] Jiayu Wang, Yifei Ming, Zhenmei Shi, Vibhav Vineet, Xin Wang, Sharon Li, and Neel Joshi. Is picture worth thousand words? delving into spatial reasoning for vision language models. Advances in Neural Information Processing Systems, 37:7539275421, 2024. [53] Jun Wang. tutorial on llm reasoning: Relevant methods behind chatgpt o1. arXiv preprint arXiv:2502.10867, 2025. [54] Weiyun Wang, Zhe Chen, Wenhai Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Jinguo Zhu, Xizhou Zhu, Lewei Lu, Yu Qiao, et al. Enhancing the reasoning ability of multimodal large language models via mixed preference optimization. arXiv preprint arXiv:2411.10442, 2024. [55] Jinyang Wu, Mingkuan Feng, Shuai Zhang, Ruihan Jin, Feihu Che, Zengqi Wen, and Jianhua Tao. Boosting multimodal reasoning with mcts-automated structured thinking. arXiv preprint arXiv:2502.02339, 2025. [56] Xueqing Wu, Yuheng Ding, Bingxuan Li, Pan Lu, Da Yin, Kai-Wei Chang, and Nanyun Peng. Visco: Benchmarking fine-grained critique and correction towards self-improvement in visual reasoning. arXiv preprint arXiv:2412.02172, 2024. [57] Ziyang Xiao, Dongxiang Zhang, Xiongwei Han, Xiaojin Fu, Wing Yin Yu, Tao Zhong, Sai Wu, Yuan Wang, Jianwei Yin, and Gang Chen. Enhancing llm reasoning via vision-augmented prompting. Advances in Neural Information Processing Systems, 37:2877228797, 2024. [58] Fengli Xu, Qianyue Hao, Zefang Zong, Jingwei Wang, Yunke Zhang, Jingyi Wang, Xiaochong Lan, Jiahui Gong, Tianjian Ouyang, Fanjin Meng, et al. Towards large reasoning models: survey of reinforced reasoning with large language models. arXiv preprint arXiv:2501.09686, 2025. [59] Guowei Xu, Peng Jin, Li Hao, Yibing Song, Lichao Sun, and Li Yuan. Llava-cot: Let vision language models reason step-by-step, 2024. arXiv preprint arXiv:2411.10440, 2024. [60] Qianqi Yan, Yue Fan, Hongquan Li, Shan Jiang, Yang Zhao, Xinze Guan, Ching-Chen Kuo, and Xin Eric Wang. Multimodal inconsistency reasoning (mmir): new benchmark for multimodal reasoning models. arXiv preprint arXiv:2502.16033, 2025. [61] Yi Yang. R1-onevision: Open-source multimodal large language model with reasoning ability, 2025. [62] Huanjin Yao, Jiaxing Huang, Wenhao Wu, Jingyi Zhang, Yibo Wang, Shunyu Liu, Yingjie Wang, Yuxin Song, Haocheng Feng, Li Shen, et al. Mulberry: Empowering mllm with o1-like reasoning and reflection via collective monte carlo tree search. arXiv preprint arXiv:2412.18319, 2024. [63] Zhenfei Yin, Jiong Wang, Jianjian Cao, Zhelun Shi, Dingning Liu, Mukai Li, Xiaoshui Huang, Zhiyong Wang, Lu Sheng, Lei Bai, et al. Lamm: Language-assisted multi-modal instructiontuning dataset, framework, and benchmark. Advances in Neural Information Processing Systems, 36:2665026685, 2023. [64] Ningyu Zhang, Lei Li, Xiang Chen, Xiaozhuan Liang, Shumin Deng, and Huajun Chen. Multimodal analogical reasoning over knowledge graphs. arXiv preprint arXiv:2210.00312, 2022. [65] Ruohong Zhang, Bowen Zhang, Yanghao Li, Haotian Zhang, Zhiqing Sun, Zhe Gan, Yinfei Yang, Ruoming Pang, and Yiming Yang. Improve vision language model chain-of-thought reasoning. arXiv preprint arXiv:2410.16198, 2024. [66] Yadong Zhang, Shaoguang Mao, Tao Ge, Xun Wang, Adrian de Wynter, Yan Xia, Wenshan Wu, Ting Song, Man Lan, and Furu Wei. Llm as mastermind: survey of strategic reasoning with large language models. arXiv preprint arXiv:2404.01230, 2024. [67] Yuxiang Zhang, Yuqi Yang, Jiangming Shu, Xinyan Wen, and Jitao Sang. Agent models: Internalizing chain-of-action generation into reasoning models, 2025. [68] Zhuosheng Zhang, Aston Zhang, Mu Li, Hai Zhao, George Karypis, and Alex Smola. Multimodal chain-of-thought reasoning in language models. arXiv preprint arXiv:2302.00923, 2023. [69] Jiaxing Zhao, Xihan Wei, and Liefeng Bo. R1-omni: Explainable omni-multimodal emotion recognition with reinforcement learning. arXiv preprint arXiv:2503.05379, 2025. [70] Xiangyu Zhao, Xiangtai Li, Haodong Duan, Haian Huang, Yining Li, Kai Chen, and Hua Yang. Mg-llava: Towards multi-granularity visual instruction tuning. arXiv preprint arXiv:2406.17770, 2024. [71] Ge Zheng, Bin Yang, Jiajin Tang, Hong-Yu Zhou, and Sibei Yang. Ddcot: Duty-distinct chain-of-thought prompting for multimodal reasoning in language models. Advances in Neural Information Processing Systems, 36:51685191, 2023. [72] Hengguang Zhou, Xirui Li, Ruochen Wang, Minhao Cheng, Tianyi Zhou, and Cho-Jui Hsieh. R1-zeros\" aha moment\" in visual reasoning on 2b non-sft model. arXiv preprint arXiv:2503.05132, 2025. [73] Qiji Zhou, Ruochen Zhou, Zike Hu, Panzhong Lu, Siyang Gao, and Yue Zhang. Image-ofthought prompting for visual reasoning refinement in multimodal large language models. arXiv preprint arXiv:2405.13872, 2024. [74] Shuyan Zhou, Frank Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Tianyue Ou, Yonatan Bisk, Daniel Fried, et al. Webarena: realistic web environment for building autonomous agents. arXiv preprint arXiv:2307.13854, 2023. 15 [75] Yikang Zhou, Tao Zhang, Shilin Xu, Shihao Chen, Qianyu Zhou, Yunhai Tong, Shunping Ji, Jiangning Zhang, Xiangtai Li, and Lu Qi. Are they the same? exploring visual correspondence shortcomings of multimodal llms. arXiv preprint arXiv:2501.04670, 2025. [76] Chaoyu Zhu, Zhihao Yang, Xiaoqiong Xia, Nan Li, Fan Zhong, and Lei Liu. Multimodal reasoning based on knowledge graph embedding for specific diseases. Bioinformatics, 38(8):2235 2245, 2022. [77] Xiangru Zhu, Zhixu Li, Xiaodan Wang, Xueyao Jiang, Penglei Sun, Xuwu Wang, Yanghua Xiao, and Nicholas Jing Yuan. Multi-modal knowledge graph construction and application: survey. IEEE Transactions on Knowledge and Data Engineering, 36(2):715735, 2022."
        }
    ],
    "affiliations": [
        "Beijing Jiaotong University"
    ]
}