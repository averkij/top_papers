{
    "paper_title": "DZ-TDPO: Non-Destructive Temporal Alignment for Mutable State Tracking in Long-Context Dialogue",
    "authors": [
        "Yijun Liao"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Long-context dialogue systems suffer from State Inertia, where static constraints prevent models from resolving conflicts between evolving user intents and established historical context. To address this, we propose DZ-TDPO, a non-destructive alignment framework that synergizes conflict-aware dynamic KL constraints with a calibrated temporal attention bias. Experiments on the Multi-Session Chat (MSC) dataset demonstrate that DZ-TDPO achieves state-of-the-art win rates (55.4% on Phi-3.5) while maintaining robust zero-shot generalization. Our scaling analysis reveals a \"Capacity-Stability Trade-off\": while smaller models incur an \"alignment tax\" (perplexity surge) to overcome historical inertia, the larger Qwen2.5-7B model achieves 50.8% win rate with negligible perplexity overhead. This confirms that TAI can be alleviated via precise attention regulation rather than destructive weight updates, preserving general capabilities (MMLU) across model scales. Code and data are available: https://github.com/lyj20071013/DZ-TDPO"
        },
        {
            "title": "Start",
            "content": "DZ-TDPO: Non-Destructive Temporal Alignment for Mutable State Tracking in Long-Context Dialogue Yijun Liao liuyingliao0620@gmail.com 5 2 0 2 8 ] . [ 2 4 0 7 3 0 . 2 1 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Long-context dialogue systems suffer from State Inertia, where static constraints prevent models from resolving conflicts between evolving user intents and established historical context. To address this, we propose DZ-TDPO, non-destructive alignment framework that synergizes conflict-aware dynamic KL constraints with calibrated temporal attention bias. Experiments on the Multi-Session Chat (MSC) dataset demonstrate that DZ-TDPO achieves state-of-the-art win rates (55.4% on Phi-3.5) while maintaining robust zero-shot generalization. Our scaling analysis reveals \"CapacityStability Trade-off\": while smaller models incur an \"alignment tax\" (perplexity surge) to overcome historical inertia, the larger Qwen2.57B model achieves 50.8% win rate with negligible perplexity overhead. This confirms that TAI can be alleviated via precise attention regulation rather than destructive weight updates, preserving general capabilities (MMLU) across model scales. Code and data are available: https://github.com/lyj20071013/DZ-TDPO"
        },
        {
            "title": "Introduction",
            "content": "Large Language Models (LLMs) have rapidly advanced in tackling long-sequence problems, driven by efficient fine-tuning techniques like LongLORA (Chen et al., 2024a) and positional interpolation methods such as YaRN (Peng et al., 2023). Existing architectures have achieved remarkable success in \"Static Retrieval\" tasks, where the goal is to locate specific piece of information within vast context window (e.g., \"Needle-in-a-Haystack\" tests). In these scenarios, information is additive and non-conflicting, and the primary challenge is effectively extending the receptive field to \"remember\" potentially infinite history. However, real-world conversational agents face fundamentally different challenge: Mutable State Tracking. Unlike static document analysis, multiturn dialogues are inherently dynamic, where user 1 intents, preferences, and states evolve over time. This introduces critical conflict between Historical Consistency (adhering to established context) and State Plasticity (adapting to new instructions). For instance, if user declared \"I love spicy food\" ten turns ago but currently states \"I have stomach ache,\" the model must not merely retrieve the old preference but explicitly override it to provide appropriate medical advice. We identify this failure mode as \"State Inertia\"driven by an underlying Temporal Attention Imbalance (TAI) where models, constrained by static alignment objectives, over-attend to outdated history and fail to update their internal state in the presence of conflicting new information. Despite the success of Reinforcement Learning from Human Feedback (RLHF) (Ouyang et al., 2022), standard alignment methods like Direct Preference Optimization (DPO) (Rafailov et al., 2023) struggle to resolve these dynamic conflicts. We argue that standard DPO imposes \"Static Alignment Constraint\" that treats all historical tokens as immutable priors. Consequently, when model attempts to update its state to match recent turn, it incurs heavy penalty for deviating from the reference models historical behavior. Correcting this inertia often requires aggressive parameter updates, leading to significant \"Alignment Tax\" (Askell et al., 2021; Lin et al., 2024)a degradation in general linguistic capabilities, manifested as catastrophic surge in perplexity (PPL) and loss of structural coherence. To address this, we propose DZ-TDPO, nondestructive alignment framework specifically designed for Conflict-Aware State Updating. Unlike general long-context methods that aim to attend to everything, our approach synergizes Dynamic Optimization (TDPO-DKL) with Structural Bias (Dual-Zone Temporal Attention) to dynamically suppress outdated state information only when conflict is detected. Our contribution is distinct: we do not aim to improve generic retrieval over infinite windows; rather, we solve the specific \"Update vs. Retain\" dilemma in evolving dialogues. Experiments on the state-tracking-intensive Multi-Session Chat (MSC) dataset demonstrate that DZ-TDPO achieves state-of-the-art win rates in resolving conflicts, while maintaining robust performance on static retrieval tasks and incurring negligible perplexity overhead. Our contributions are summarized as follows: Formulation & Framework: We formally define Temporal Attention Imbalance (TAI) and propose DZ-TDPO. By integrating Semantic-Aware Adaptive Decay (powered by SBERT embeddings) with structural attention bias, our method dynamically prioritizes recent user states over conflicting history. Empirical Excellence: We validate our approach on the MSC and UltraChat benchmarks. DZ-TDPO significantly outperforms standard DPO, achieving robust generalization while preserving general knowledge (MMLU) (Hendrycks et al., 2021). Furthermore, extensive stress testing confirms our method maintains long-term factual recall (Appendix B.5) and robustness against adversarial attacks (Appendix B.6). Scaling Insight: We provide the first analysis of the Capacity-Stability Trade-off in temporal alignment. Experiments on Qwen2.5-7B (Team et al., 2025) show that larger models can internalize temporal bias with minimal \"Alignment Tax\" , contrasting with the steeper cost paid by smaller models, thus offering scalable solution for long-context agents."
        },
        {
            "title": "2.1 Related Work",
            "content": "Preference Alignment The alignment of LLMs with human values has rapidly evolved from PPObased RLHF (Ouyang et al., 2022) to offline, reward-free optimization paradigms. Direct Preference Optimization (DPO) (Rafailov et al., 2023) marked milestone in this field by deriving closed-form solution that implicitly optimizes the reward function. Recently, research has shifted towards reference-free and margin-based approaches to enhance stability and length robustness. Methods like SimPO (Meng et al., 2024) and ORPO (Hong et al., 2024) completely discard the reference model to avoid \"reference lag\" and utilize probability margins (or odds ratios) to distinguish preferred responses. Utilize probability margins to distinguish preferred responses. Other paradigms such as IPO (Azar et al., 2024) provide theoretical guarantees for regularized alignment, while SPIN (Chen et al., 2024b) explores self-play mechanisms to iteratively refine policies. Despite these advancements, most alignment objectives assume static reward landscape. DZ-TDPO complements these works by introducing temporal dynamics into the optimization process. While effective for general instruction following, these methods typically assume global, static margin for all tokens, neglecting the temporal heterogeneity of preference gaps in multi-turn scenarios. In contrast, while SimPO addresses length bias, DZ-TDPO retains the reference model to ensure linguistic stability but introduces time-varying coefficient β(t; ) to dynamically modulate the constraint, thereby correcting temporal bias. Long-Context Attention Mechanisms To handle long-sequence inputs, the community has proposed various structural innovations, such as RoPE (Su et al., 2024) for position encoding and ALiBi (Press et al., 2022) for length extrapolation, enabling models to process sequences exceeding 100k tokens. Beyond architecture, approaches like LongAlign (Bai et al., 2024) and LongPO (Chen et al., 2025) focus on extending the receptive field to address retrieval tasks (\"Finding the needle\"). In contrast, DZ-TDPO addresses the conflict task (\"Updating the needle\"), solving the State Inertia that these capacity-focused methods neglect, often operating under the assumption that all historical context is potentially relevant. Recent efficiencyfocused works have explored sparse attention mechanisms: StreamingLLM (Xiao et al., 2024) identifies attention sinks to maintain generation stability, while H2O (Zhang et al., 2023) evicts nonheavy-hitter tokens to reduce KV cache footprint. Similarly, Ring Attention (Liu et al., 2023) optimizes computation for near-infinite contexts. However, these methods primarily focus on computational efficiency or retrieval recall. In contrast, DZTDPO addresses the conflict resolution problem. Recent studies have identified distinct attention phenomena, such as attention sinks (Xiao et al., 2024) that disproportionately weight initial tokens. Furthermore, benchmarks like RULER (Hsieh et al., 2 2024) highlight that effectively utilizing long context for precise state tracking remains significant challenge. In contrast, DZ-TDPO focuses on the conflict resolution problem (which part should it trust?). Unlike general long-context methods, our approach specifically addresses the decision dilemma when historical information contradicts the current statea dimension rarely discussed in prior literature. Temporal Modeling in Dialogue In Dialogue State Tracking (DST) and session-based recommendation, the importance of \"recency\" has long been recognized. Approaches like Time-LSTM (Zhu et al., 2017) and decay-based attention mechanisms have been proposed to characterize temporal dynamics. However, these techniques have been largely confined to pre-training or Supervised Fine-Tuning (SFT). To the best of our knowledge, DZ-TDPO is the first work to explicitly integrate temporal decay mechanisms into the preference optimization phase, directly aligning the models reward structure with the temporal nature of human conversation."
        },
        {
            "title": "2.2 Problem Formulation",
            "content": "We mathematically formulate the dialogue alignment task and analyze how the theoretical limitations of standard DPO lead to Temporal Attention Imbalance. Consider aligning Large Language Model (LLM) on multi-turn dialogue dataset = {(c, yw, yl)}. Here, = [u1, s1, . . . , uT ] represents the dialogue history up to turn , where ui and si denote the user and system utterances at the i-th turn, respectively. yw and yl represent the preferred (chosen) and rejected responses for the current turn . Direct Preference Optimization (DPO) aligns the model by minimizing the negative log-likelihood of the preferred response relative to reference model πref. The standard objective is defined as: LDPO(θ) = E(c,yw,yl)D (cid:20) (cid:18) log σ β log πθ(ywc) πref(ywc) β log (cid:19)(cid:21) πθ(ylc) πref(ylc) (1) Here, β serves as the KL penalty coefficient. The gradient of this loss shifts probability mass towards yw and away from yl, scaled by the implicit reward margin. Crucially, standard DPO treats β as static scalar, held constant across all training samples and time steps. We argue that the standard DPO formulation suffers from mismatch in temporal inductive bias. In long-context dialogues, the ground-truth reward function r(c, y) is inherently time-sensitive. Conceptually, the true reward can be decomposed into content quality term rcontent and temporal relevance term rrecency: r(c, y) rcontent(c, y) + γ(T ) rrecency(c, y) (2) where γ(T ) signifies the importance of the current turn in resolving state conflicts. Ideally, γ(T ) should be maximized to enforce consistency with the latest user state. However, standard DPO imposes uniform β constraint. Mathematically, this is equivalent to assigning uniform prior to the importance of \"historical consistency\" versus \"local relevance.\" Consequently, the optimization landscape is dominated by the massive volume of historical tokens (which favor consistency with πref), suppressing the sparse recent tokens (which require deviation from πref for state updates). This optimization flaw manifests in the attention mechanism as TAI. Let αt denote the aggregate attention weight allocated to the t-th turn. Under standard DPO, the model exhibits Historical Inertia\": (cid:88) t=1 αt (cid:88) αt t=T k+1 (3) Here, the cumulative attention on irrelevant history significantly outweighs the focus on the critical recent context (window k). This structural deficit prevents the model from effectively updating its internal state representation, leading to the \"Global-Local Relevance Conflict\" described in the Introduction."
        },
        {
            "title": "3 Methodology",
            "content": "To address the challenge of Mutable State Tracking and mitigate State Inertia, we propose the DZTDPO framework. This framework recalibrates the models temporal focus through two complementary modules: Conflict-Aware TDPO-DKL (at the Optimization Level) and Dual-Zone Temporal Attention (at the Representation Level)."
        },
        {
            "title": "3.1 Optimization Level: TDPO-DKL",
            "content": "Temporal DPO with Dynamic KL (TDPO-DKL) reforms the optimization objective by introducing 3 the necessity to adhere to the reference model πref is not uniform. For distant history (t ), where the context is static, the model should strictly follow the reference to maintain linguistic coherence. For recent turns (t ), where state updates occur, the model requires \"looser\" constraint to deviate from the reference and learn new behaviors. Accordingly, we design monotonically decreasing KL schedule β(t; ). For preference pair located at turn within total context of turns, the coefficient is defined as: (cid:20) (cid:18) β(t; ) = β0 1 (1 α) exp (cid:19)(cid:21) τ (uT ) (5) To further combat TAI, we explicitly up-weight the contribution of recent turns to the total gradient. We define temporal weight w(t; ): w(t; ) = exp (cid:18) (cid:19) τ (uT ) (6) This ensures that the optimization process prioritizes resolving conflicts in the current context over optimizing historical nuances. We first define the implicit log-ratio margin Figure 1: The dynamic mechanism of TDPO-DKL. As the dialogue progresses towards the current turn , the KL coefficient β(t; ) (blue solid line) decreases to relax constraints, while the temporal weight w(t; ) (orange dashed line) increases to amplify the gradient signal for recent updates. time-awareness into both the constraint strength and the loss magnitude. Unlike in standard exponential decay methods, we argue that the decay rate should depend on the semantic conflict between the current user input and history. We map dialogue turns into latent semantic space and define the adaptive decay temperature τ (uT ) for the current user turn as: τ (uT ) = τbase (1γ max i<T CosSim(eT , ei)) (4) Mθ(c, yw, yl) as: where eT and ei are sentence embeddings encoded by lightweight Transformer (SBERT). high cosine similarity implies potential state update or topic revisitation, triggering lower τ to sharpen the models focus on the present. Conversely, low similarity implies orthogonal topics where history should be preserved. We acknowledge that embedding similarity acts as semantic proxy rather than strict logical filter. In cases of \"Subtle Negation\" (e.g., \"I love apples\" vs. \"I do not love apples\"), the high lexical overlap may result in high cosine similarity, causing the mechanism to predict large τ (slow decay). We fundamentally design this as Conservative Fallback feature. In ambiguous scenarios where semantic distance does not explicitly signal topic shift, our framework degrades gracefully to the behavior of standard DPO (retaining historical context). This ensures that the model never aggressively prunes potential contradictions unless there is strong, explicit signal of state transition (e.g., \"Change topic to X\"), prioritizing safety over aggressive plasticity. Dynamic KL Coefficient β(t; ).We posit that Mθ(c, yw, yl) = log πθ(ylc) πref(ylc) (7) Incorporating the dynamic coefficients, the final πθ(ywc) πref(ywc) log TDPO-DKL loss is formulated as: LTDPO-DKL(θ) = ED [w(t; ) log σ (β(t; ) Mθ(c, yw, yl))] (8) This formulation effectively \"unshackles\" the model from the reference policy at critical decision points while maintaining stability elsewhere."
        },
        {
            "title": "Temporal Attention",
            "content": "While TDPO-DKL incentivizes the model to focus on the present via optimization gradients, it operates on standard attention landscape. To explicitly resolve the conflict between Immutable Instructions and Mutable States, we propose the Dual-Zone Temporal Attention (DZ-TA) architecture. This module acts as structural prior, reshaping the attention mechanism to support secure state tracking. 4 Theoretically, different attention heads in Transformer could specialize in distinct temporal dynamicssome preserving long-term retrieval while others focus on immediate state updates. We initially formulated this as Multi-Head Adaptive Temporal Bias (MATB). For specific head h, we inject learnable bias Bi,j,h into the attention logits: Bi,j,h = λh max(0, (i, j)) τh (9) where λh and τh are independent, head-specific parameters. This formulation allows the model to theoretically explore high-dimensional search space of temporal policies. However, our preliminary analysis reveals that this flexibility leads to optimization instability under data-constrained alignment settings. The full MATB tends to converge to suboptimal local minima, creating \"lazy heads\" that overfit to training noise rather than learning generalized decay rule (see theoretical analysis in Appendix A.2). To address this, we impose strong inductive bias by constraining MATB to Dual-Zone Temporal Attention (DZ-TA). We force all attention heads in the mutable region to share single intensity parameter λ (and fixed τf ixed). This constraint acts as Low-Rank Regularizer, significantly reducing the generalization error and ensuring the model learns robust, global temporal policy. We conceptualize the context window as consisting of two distinct regions: the Immutable Anchor Zone (Zanchor) (indices 0 to Lanc) covering the System Prompt and core safety guidelines which must remain invariant; and the Mutable State Zone (Zstate) (indices Lanc + 1 to ) covering the conversational history subject to plasticity. Instead of applying uniform decay, we inject Dual-Zone Bias matrix directly into the attention logits. For query token and key token j, the final bias Bi,j is defined as piecewise function: Bi,j = (cid:40) 0 λ (i,j) τf ixed if Zanchor if Zstate (Constitutional Persistence) (State Plasticity) (10) Here, λ is the shared scalar parameter. While theoretically learnable (see Appendix A.2), we fixed λ = 0.5 as structural prior (theoretical justification in Appendix A.2). This imposes strong structural prior that enforces the forgetting rate for user history without introducing optimization instability. This architecture transforms safety from posthoc constraint into an intrinsic property of the attention mechanism. By forcing Bi,j = 0 for the Anchor Zone, we mathematically ensure that the System Prompt is never subjected to the distance penalty. This effectively neutralizes \"Context Flooding\" attacks, where adversaries attempt to push safety instructions out of the effective window using massive conversational noise. Simultaneously, the learned decay in the State Zone effectively suppresses State Inertia, increasing the signal-to-noise ratio for recent updates. Crucially, since DZ-TA modifies logits via static bias term, it can be fused into the positional encoding kernel during inference, resulting in zero latency penalty compared to the base model."
        },
        {
            "title": "4 Experiments",
            "content": "We evaluate DZ-TDPO across three dimensions: (1) its effectiveness in mitigating Temporal Attention Imbalance (TAI) on in-domain dialogue tasks; (2) its zero-shot generalization capabilities on outof-domain instruction following; and (3) the impact on general linguistic capabilities, specifically analyzing the trade-off between alignment performance and perplexity. Furthermore, we conduct comprehensive ablation study to disentangle the impact of dynamic optimization (TDPO-DKL) versus structural bias (DZ-TA). For preference evaluation, we employ DeepSeekV3.2 (DeepSeek-AI et al., 2025) as the automated judge. Recent studies indicate that DeepSeek-V3.2 exhibits high correlation with human judgment and strong capabilities in complex reasoning tasks. We conduct pairwise comparisons where the judge is presented with the dialogue history, the specific instruction, and two anonymized model responses, following the standard LLM-as-a-Judge protocol (detailed protocol in Appendix G)."
        },
        {
            "title": "4.1 Experimental Setup",
            "content": "Datasets. We utilize the Multi-Session Chat (MSC) (Xu et al., 2022) dataset as our primary testbed. MSC contains long-term conversations spanning up to 5 sessions, making it ideal for simulating temporal evolution. To rigorously evaluate the models ability to override \"Historical Inertia\" and prioritize recent updates, we devised specialized Temporal Preference Construction Protocol rather than using standard random sampling. We focus on Session 4 to ensure sufficiently long history. For each sample, we concatenate 4 consecutive sessions to form the context c. This results in long-context input (typically exceeding 1.7k tokens) that effectively triggers the models long-term retrieval mechanisms. We constructed preference pairs (yw, yl) using Historical Negative Sampling strategy. To ensure strict temporal conflict, we filtered pairs based on semantic similarity and length ratios (detailed protocol in Appendix B.1). To ensure the preference signal is driven by temporal logic rather than noise, we apply two strict filters based on our preliminary analysis: Instead of relying on surface-level lexical overlap, we utilize Semantic Embedding Similarity (via all-MiniLM-L6-v2) to compute the cosine similarity between the chosen (yw) and rejected (yl) responses. Pairs with similarity score > 0.5 are discarded. This strictly prevents \"False Negatives\"scenarios where historical response (e.g., generic greeting) remains semantically valid in the current context, which would otherwise confuse the reward model. We filter out pairs where the length ratio between yw and yl exceeds 4:1. This regularizes the dataset to prevent the model from exploiting \"Length Bias\" (i.e., learning to prefer longer/shorter responses regardless of content), ensuring the alignment focuses purely on temporal relevance. For out-of-domain (OOD) evaluation, we employ the UltraChat (Ding et al., 2023) dataset to assess zero-shot generalization and ensure that our temporal bias mechanism does not degrade general instruction-following capabilities. To assess whether the aggressive temporal alignment induces catastrophic forgetting, we evaluate the models perplexity on the Massive Multitask Language Understanding (MMLU) benchmark. We report the average PPL across 5 representative subjects covering STEM, Humanities, and Social Sciences to monitor the retention of world knowledge. Baselines We compare DZ-TDPO against two primary baselines: Base Model: Microsoft Phi-3.5-mini-instruct (3.8B) (Abdin et al., 2024), which serves as the backbone for all experiments. Standard DPO: strong alignment baseline using static β constraints without temporal awareness. This represents the current standard practice for preference optimization. SimPO: state-of-the-art reference-free alignment baseline that optimizes lengthnormalized reward margins, included to benchmark against margin-based approaches. TDPO-DKL (Ablation): Our proposed optimization method with the DZ-TA mechanism disabled. This variant serves to isolate the specific contribution of the dynamic KL schedule and temporal loss weighting from the structural attention bias. Detailed hyperparameters and training configurations are provided in Appendix B.2. Training is conducted on single NVIDIA A800 GPU with batch size of 32 for 6 epochs to prevent overfitting. 4.2 Results Method Base Model Standard DPO SimPO TDPO-DKL (w/o DZ-TA) DZ-TDPO (Ours) MSC WR (In-Domain) UltraChat Val Val PPL PPL WR (OOD) MSC MMLU - % 45.8 % 46.4 % 50.2 % 55.4 % - % 22.1 49.2 % 102.3 48.8 % 101.2 49.8 % 101.1 26.0 53.5 % 5.27 5.28 5. 5.42 5.34 Table 1: For Standard DPO, we report the performance at Epoch 6, as we observed severe reward hacking and perplexity degradation (>100) in subsequent epochs. For SimPO, we report the performance at Epoch 24 (batch size of 128). As shown in Table 1, purely optimization-based baselines struggle profoundly with the State Inertia inherent in long-context dialogues. critical observation is the catastrophic behavior of Standard DPO. While it achieves baseline win rate of 45.8%, it incurs massive \"Alignment Tax,\" with validation perplexity exploding to 102.3. We argue that this is not training artifact, but theoretical inevitability of the Static Alignment Constraint. When the user updates their state (e.g., A), standard DPO drives the model to maximize the likelihood of (the new truth), yet the static KL constraint anchors it to (the historical prior). Without an attention mechanism to resolve this contradiction, the model is forced to shatter its pre-trained linguistic priors to satisfy the reward objective, resulting in distribution collapse. In stark contrast, DZ-TDPO achieves state-ofthe-art performance with an 55.4% in-domain win rate. Most importantly, it maintains healthy perplexity of 26.0, which is comparable to the Base Model (22.1) and significantly lower than the optimization-based baselines. This confirms the efficacy of our Dual-Zone Architecture: by structurally suppressing the attention mass of outdated states via the Mutable State Zone (Zstate), the optimization module (TDPO-DKL) operates on \"clean\" gradient landscape. The model learns to override the state without destroying its general linguistic capabilities. SimPO, while avoiding reference model constraints, demonstrates limited zero-shot generalization, with OOD performance on UltraChat recording 48.8%. Conversely, DZ-TDPO achieves robust 53.5% OOD win rate. This suggests that our DZ-TA acts as generalized low-rank regularizer, capturing universal recency principle rather than over-optimizing for in-domain patterns. Beyond preference win rates, we evaluate the generative quality of the models using standard ngram metrics including SacreBLEU (Post, 2018), ROUGE (Lin, 2004) and BERTScore (Zhang et al., 2020) against the reference responses. (detailed ngram metrics provided in Appendix C.2). Furthermore, head-to-head comparisons (Appendix C.4) confirm DZ-TDPOs superiority."
        },
        {
            "title": "4.3 The Capacity-Stability Trade-off",
            "content": "To investigate the impact of model scale, we extended our experiments to the Qwen2.5-7B-Instruct model. As shown in Table 2, distinct \"StabilityPlasticity\" pattern emerges compared to the smaller Phi-3.5 model. Interestingly, the 7B model achieves win rate of 50.8% on the MSC dataset, which is slightly lower than the 3.8B models 55.4%. We attribute this to the stronger Parametric Inertia inherent in larger models. With significantly more pre-trained knowledge, the 7B model possesses more rigid \"belief system,\" making it harder to override historical context with short-term updates compared to the more plastic 3.8B model. However, the perplexity analysis reveals the advantage of this rigidity. While the smaller Phi-3.5 incurs visible \"stability cost\" ( PPL +3.9) to accommodate the temporal bias, the 7B model absorbs the DZ-TDPO mechanism with negligible overhead ( PPL +1.95). This confirms that while larger models are harder to steer, they are also significantly more robust against distribution collapse. DZ-TDPO is proven to be non-destructive at scale, offering safe alignment path. Beyond in-domain performance, the 7B model Table 2: Scaling Analysis. Comparison between Phi3.5 (3.8B) and Qwen2.5 (7B) under DZ-TDPO. While both models maintain high stability (low PPL ), the 7B model achieves near-perfect alignment efficiency and demonstrates robust 8k length extrapolation. Metric Phi-3.5 (3.8B) Qwen2.5 (7B) In-Domain Alignment (MSC) Win Rate Alignment Tax (PPL ) OOD Generalization (UltraChat) Win Rate (4k Context) Knowledge Retention (MMLU) PPL Variation 55.4% +3.9 50.8% +1. 53.5% 50.83% +0.07 +0.45 maintains consistent zero-shot generalization. On UltraChat (OOD), it attains 50.8% win rate, mirroring its in-domain behavior. Furthermore, general world knowledge remains intact, with minimal perplexity variation on MMLU ( +0.07 for 3.8B vs +0.45 for 7B). This scaling analysis suggests that resolving State Inertia in larger models may require stronger intervention strengths (e.g., lower τ or higher λ) to overcome their inherent parametric stability."
        },
        {
            "title": "4.4 Qualitative Analysis",
            "content": "Analysis of the TAB-60 Benchmark (see Appendix B.4 for full transcripts) highlights DZ-TDPOs ability to suppress \"Safety Refusal Inertia.\" For instance, in Case 59, where user reveals divorce after long context of marriage, the Base model hallucinated suggestion to buy flowers due to historical sentiment inertia. In contrast, DZ-TDPO correctly identified the state change and advised maintaining distance. Similarly, in Case 53, our model successfully overrode 50-turn \"Vegan\" persona to recommend steak recipe upon medical update, whereas the baseline refused based on outdated constraints. To rigorously rule out \"Contextual Myopia\" (where λ might aggressively suppress history), we conducted Non-Conflicting Needle-in-aHaystack evaluation. As shown in Table 5 (Appendix B.5), despite being trained on contexts < 2.4k, the model successfully retrieved specific entities across 2k8k intervals. This validates that λ 0.5 acts as soft semantic filter, suppressing background noise while propagating strong, nonconflicting signals to enable robust extrapolation. Furthermore, we demonstrate in Appendix C.3 that DZ-TDPO successfully resists adversarial brainwashing in 16k-token Inertia Trap experiment, where the base model succumbs to massive 7 repetition of outdated values."
        },
        {
            "title": "5 Discussion",
            "content": "The Theoretical Inevitability of the \"Alignment Tax\"Our findings reveal fundamental mechanism mismatch in standard alignment algorithms when applied to mutable state tracking. Standard DPO, enforced by static KL constraint, inherently treats all historical tokens as immutable priors. The catastrophic perplexity surge observed in the Standard DPO baseline (102.3) is not training artifact, but empirical evidence of State Inertia. When user updates their state (e.g., A), the static constraint forces the model to maximize the likelihood of while the entire history implies A. Without an attention mechanism to resolve this contradiction, the model is compelled to significantly disrupt its pre-trained linguistic features to satisfy the conflicting reward, resulting in distribution collapse. DZ-TDPO resolves this by decoupling the alignment process. The Dual-Zone Architecture acts as representation-level filter, proactively suppressing the signal of outdated states. Consequently, the optimization module (TDPO-DKL) operates on \"clean\" gradient landscape, allowing the model to internalize the state update without paying the \"Alignment Tax.\" This explains why our method achieves SOTA win rates while maintaining nearbaseline perplexity. From Belief to Responsiveness key observation from our \"Ping-Pong\" stress test  (Table 9)  is the models extreme sensitivity to rapid intent toggling. While previous works might frame this lack of \"Core Belief System\" as an instability, we argue that for conversational agents, High-Fidelity State Responsiveness is the superior objective. In Mutable State Tracking framework, the agents role is not to judge the users consistency but to faithfully reflect their current intent. If users preference oscillates, the agents state should oscillate in tandem. Our Dual-Zone design ensures this responsiveness is safe: while the Mutable State Zone is highly plastic to accommodate user whims, the Immutable Anchor Zone ensures that core safety principles (System Prompt) remain rigid and nonnegotiable. This architecture effectively solves the \"Stability-Plasticity Dilemma\" by physically separating them into distinct attention regions. Our comparison between Phi-3.5 and Qwen2.5 suggests that follows temporal alignment \"Capacity-Dependent Efficiency\" law. Larger models possess \"Parametric Buffer\" that absorbs temporal biases more efficiently. While smaller models require steeper attention decay to overcome historical inertia, larger models can internalize state conflicts with subtler adjustments. This indicates that resolving State Inertia is not merely data problem but model-capacity challenge, and DZTDPO scales favorably with model size. Despite its robustness, our Conflict-Aware Decay relies on semantic embedding similarity. While we designed it with \"Conservative Fallback\" to handle ambiguous logical negations safely, future work could integrate lightweight Natural Language Inference (NLI) heads during training to capture subtle logical contradictions (e.g., irony or double negation) more precisely."
        },
        {
            "title": "6 Conclusion",
            "content": "In this work, we identified Temporal Attention Imbalance (TAI) as critical failure mode in longcontext dialogue alignment, where static optimization constraints cause models to over-attend to outdated history. To address this, we proposed DZ-TDPO, non-destructive framework that synergizes dynamic optimization (TDPO-DKL) with structural representation bias (DZ-TA). Our experiments on the Multi-Session Chat dataset demonstrate that DZ-TDPO effectively resolves temporal conflicts, achieving state-of-theart win rates. Crucially, it achieves this without the \"Alignment Tax\" characterizing previous methodsavoiding the catastrophic perplexity degradation seen in purely optimization-based approaches. Our findings suggest that for long-context agents, precise attention regulation is more effective and stable alignment strategy than aggressive parameter updates. Future work extends beyond standard Transformers to efficient architectures like Mamba (Gu and Dao, 2024) and hybrid SSM-Transformer models like Jamba (Lieber et al., 2024). Furthermore, investigating how our temporal bias interacts with noise-canceling mechanisms like the Differential Transformer (Ye et al., 2024) offers promising avenue for maximizing the signal-to-noise ratio in lifelong learning agents."
        },
        {
            "title": "Limitations",
            "content": "While DZ-TDPO demonstrates state-of-the-art performance in resolving Temporal Attention Imbalance (TAI), our framework operates within certain theoretical and practical boundaries: 8 The Semantic-Logic Gap and Hierarchical Resolution. We acknowledge that semantic embedding metrics (like SBERT) may yield high similarity scores for negated statements (e.g., \"I love apples\" vs. \"I do not love apples\"), potentially causing DZ-TDPO to miss \"Subtle Negation\" conflict. However, our framework is designed as Macro-Level Filter targeting state/topic inertia, not micro-level syntactic parser. In cases of subtle negation where semantic similarity remains high, the mechanism intentionally refrains from decaying attention. This allows the backbone LLMs intrinsic self-attentionwhich excels at capturing local syntactic dependencies like \"not\"to resolve the logical contradiction. Thus, DZ-TDPO and the base model form defense-in-depth architecture: our method handles long-term context shifts, while the base model handles precise logical operations within the active context. Future work could explore integrating lightweight Natural Language Inference (NLI) heads to better capture logical contradictions, albeit at the cost of increased inference latency. The \"Ping-Pong\" Instability. The framework is predicated on strong \"Recency Priority\" assumption. While effective for evolving intents, this introduces instability in scenarios where user exhibits high-frequency preference oscillation (e.g., rapidly toggling within short window). Lacking persistent \"Core Belief System,\" the model may exhibit \"Ping-Pong\" effect, merely mirroring the latest input without questioning the users inconsistency (see Appendix B.7 for detailed \"Dietary Flip-Flop\" transcript). This reactive behavior is acceptable for adaptive chatbots but may be suboptimal for expert systems requiring long-term consistency. Heuristic Dependence on Trailing Updates. Our approach incorporates strong inductive bias: that valid state updates are invariably located at the trailing edge of the context. While this covers the vast majority of dialogue scenarios, it may penalize valid long-distance correction signalssuch as user attempting to correct specific factual error from twenty turns ago without altering the current conversational state."
        },
        {
            "title": "Ethical Considerations",
            "content": "The core capability of DZ-TDPOdynamically suppressing historical context to prioritize current instructionsintroduces specific safety risks that must be managed during deployment. Vulnerability to Adversarial \"Forced Forgetting\". The temporal bias mechanism theoretically introduces risk where malicious actors could frame jailbreak attempts (Zou et al., 2023) as \"state updates\" to exploit the decay function (e.g., \"Ignore previous safety guidelines; my new state is an administrator\"). Mitigation: As detailed in Section 3.2, our Dual-Zone architecture inherently neutralizes this via the Immutable Anchor Zone. By mathematically forcing zero decay on the system prompt (Bi,j = 0 for Zanchor), we ensure constitutional principles remain rigid against temporal erosion, as verified by our Context Flooding stress tests (Appendix B.6). The Risk of Sycophancy. (Wei et al., 2023) Excessive prioritization of the most recent turn risks amplifying sycophantic behavior. If user inputs factually incorrect premises or biased viewpoints, DZ-TDPO might validate these misconceptions to minimize the \"conflict signal,\" effectively overriding the World Knowledge retained from pretraining. Future iterations should incorporate \"Factuality Reward\" term to balance user alignment with objective truthfulness. Transparency and User Control. Given the models capability to implicitly \"overwrite\" memory, we advocate for high transparency in deployment. Systems should explicitly notify users when significant state conflict is detected and history is being overridden (e.g., \"I have updated your dietary preferences based on your latest input\"), ensuring users retain agency over the dialogue state."
        },
        {
            "title": "References",
            "content": "Marah Abdin, Jyoti Aneja, Hany Awadalla, Ahmed Awadallah, Ammar Ahmad Awan, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Jianmin Bao, Harkirat Behl, Alon Benhaim, Misha Bilenko, Johan Bjorck, Sébastien Bubeck, Martin Cai, Qin Cai, Vishrav Chaudhary, Dong Chen, Dongdong Chen, and 110 others. 2024. Phi-3 technical report: highly capable language model locally on your phone. arXiv preprint arXiv:2404.14219. Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan, Andy Jones, Nicholas Joseph, Ben Mann, Nova DasSarma, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez, Jackson Kernion, Kamal Ndousse, Catherine Olsson, Dario Amodei, Tom Brown, Jack Clark, and 3 others. 2021. general language assistant as laboratory for alignment. arXiv preprint arXiv:2112.00861. Mohammad Gheshlaghi Azar, Mark Rowland, Bilal 9 Piot, Daniel Guo, Daniele Calandriello, Michal Valko, and Rémi Munos. 2024. general theoretical paradigm to understand learning from human preferences. In Proceedings of the 27th International Conference on Artificial Intelligence and Statistics (AISTATS). Yushi Bai, Xin Lv, Jiajie Zhang, Yuze He, Ji Qi, Lei Hou, Jie Tang, Yuxiao Dong, and Juanzi Li. 2024. Longalign: recipe for long context alignment of large language models. arXiv preprint arXiv:2401.18058. Arindam Banerjee, Srujana Merugu, Inderjit S. Dhillon, and Joydeep Ghosh. 2005. Clustering on the unit hypersphere using von mises-fisher distributions. Journal of Machine Learning Research, 6(Sep):1345 1382. Omar Besbes, Yonatan Gur, and Assaf Zeevi. 2015. Non-stationary stochastic optimization. Operations Research, 63(5):12271244. Ralph Allan Bradley and Milton E. Terry. 1952. Rank analysis of incomplete block designs: I. the method of paired comparisons. Biometrika, 39(3/4):324 345. Guanzheng Chen, Xin Li, Michael Qizhe Shieh, and Lidong Bing. 2025. Longpo: Long context selfevolution of large language models through shortarXiv preprint to-long preference optimization. arXiv:2502.13922. Yukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian Liu, Song Han, and Jiaya Jia. 2024a. Longlora: Efficient fine-tuning of long-context large language models. arXiv preprint arXiv:2309.12307. Zixiang Chen, Yihe Deng, Huizhuo Yuan, Kaixuan Ji, and Quanquan Gu. 2024b. Self-play fine-tuning converts weak language models to strong language models. In Proceedings of the 41st International Conference on Machine Learning (ICML). DeepSeek-AI, Aixin Liu, Aoxue Mei, Bangcai Lin, Bing Xue, Bingxuan Wang, Bingzheng Xu, Bochao Wu, Bowei Zhang, Chaofan Lin, Chen Dong, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenhao Xu, Chong Ruan, Damai Dai, Daya Guo, Dejian Yang, and 245 others. 2025. Deepseek-v3.2: Pushing the frontier of open large language models. Ning Ding, Yulin Chen, Bokai Xu, Yujia Qin, Zhi Zheng, Shengding Hu, Zhiyuan Liu, Maosong Sun, and Bowen Zhou. 2023. Enhancing chat language models by scaling high-quality instructional conversations. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 30293051. Albert Gu and Tri Dao. 2024. Mamba: Linear-time sequence modeling with selective state spaces. In Proceedings of the 41st International Conference on Machine Learning (ICML). Hazan and Elad. 2016. Introduction to online convex optimization. Foundations and Trends in Optimization, 2(3-4):157325. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2021. Measuring massive multitask language underIn Proceedings of the 9th International standing. Conference on Learning Representations (ICLR). Jiwoo Hong, Noah Lee, and James Thorne. 2024. Orpo: Monolithic preference optimization without reference model. arXiv preprint arXiv:2403.07691. Cheng-Ping Hsieh, Simeng Sun, Samuel Kriman, Shantanu Acharya, Dima Rekesh, Fei Jia, Yang Zhang, and Boris Ginsburg. 2024. Ruler: Whats the real context size of your long-context language models? arXiv preprint arXiv:2404.06654. Greg Kamradt. 2023. Needle in haystack - pressure https://github.com/gkamradt/ testing llms. LLMTest_NeedleInAHaystack. J. Richard Landis and Gary G. Koch. 1977. The measurement of observer agreement for categorical data. biometrics, pages 159174. Opher Lieber, Barak Lenz, Hofit Bata, Gal Cohen, Jhonathan Osin, Itay Dalmedigos, Erez Safahi, Shaked Meirom, Yonatan Belinkov, Shai ShalevShwartz, Omri Abend, Raz Alon, Tomer Asida, Amir Bergman, Roman Glozman, Michael Gokhman, Avashalom Manevich, Nir Ratner, Noam Rozen, and 3 others. 2024. Jamba: hybrid transformer-mamba language model. arXiv preprint arXiv:2403.19887. Chin-Yew Lin. 2004. ROUGE: package for automatic In Text Summarization evaluation of summaries. Branches Out, pages 7481. Association for Computational Linguistics. Yong Lin, Hangyu Lin, Wei Xiong, Shizhe Diao, Jianmeng Liu, Jipeng Zhang, Rui Pan, Haoxiang Wang, Wenbin Hu, Hanning Zhang, Hanze Dong, Renjie Pi, Han Zhao, Nan Jiang, Heng Ji, Yuan Yao, and Tong Zhang. 2024. Mitigating the alignment tax of RLHF. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 580606. Hao Liu, Matei Zaharia, and Pieter Abbeelr. 2023. Ring attention with blockwise transformers for nearinfinite context. arXiv preprint arXiv:2310.01889. Ronald A. Fisher. 1953. Dispersion on sphere. Proceedings of the Royal Society of London. Series A. Mathematical and Physical Sciences, 217(1130):295 305. Sam McCandlish, Jared Kaplan, Dario Amodei, and OpenAI Dota Team. 2018. An empirical arXiv preprint model of large-batch training. arXiv:1812.06162. Yu Meng, Mengzhou Xia, and Danqi Chen. Simpo: Simple preference optimization arXiv preprint 2024. with reference-free reward. arXiv:2405.14734. Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar. 2012. Foundations of machine learning. MIT press. Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. 2022. Training language models to follow instructions with human feedback. In Advances in Neural Information Processing Systems 35 (NeurIPS), pages 27730 27744. Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. 2023. Yarn: Efficient context window extension of large language models. arXiv preprint arXiv:2309.00071. Matt Post. 2018. call for clarity in reporting BLEU In Proceedings of the Third Conference scores. on Machine Translation: Research Papers (WMT), pages 186191. Association for Computational Linguistics. Ofir Press, Noah A. Smith, and Mike Lewis. 2022. Train short and test long: Attention with linear biases enables input length extrapolation. In Proceedings of the 10th International Conference on Learning Representations (ICLR). Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D. Manning, and Chelsea Finn. 2023. Direct preference optimization: Your language model is secretly reward model. In Advances in Neural Information Processing Systems 36 (NeurIPS), pages 5372853741. Nils Reimers and Iryna Gurevych. 2019. Sentence-bert: Sentence embeddings using siamese bert-networks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing, pages 39823992. Association for Computational Linguistics. Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. 2024. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063. Qwen Team, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, and 24 others. 2025. Qwen2.5 technical report. arXiv preprint arXiv:2412.15115. 11 Wenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan Yang, and Ming Zhou. 2020. Minilm: Deep selfattention distillation for task-agnostic compression of pre-trained transformers. In Advances in Neural Information Processing Systems 33 (NeurIPS), pages 57765788. Jerry Wei, Da Huang, Yifeng Lu, Denny Zhou, and Quoc V. Le. 2023. Simple synthetic data reduces sycophancy in large language models. arXiv preprint arXiv:2308.03958. Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. 2024. Efficient streaming language models with attention sinks. In Proceedings of the 12th International Conference on Learning Representations (ICLR). Jing Xu, Arthur Szlam, and Jason Weston. 2022. Beyond goldfish memory: Long-term open-domain conversation. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 51805197. Tianzhu Ye, Li Dong, Yuqing Xia, Yutao Sun, Yi Zhu, Gao Huang, and Furu Wei. 2024. Differential transformer. arXiv preprint arXiv:2410.05258. Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi. 2020. Bertscore: Evaluating text generation with bert. In Proceedings of the 8th International Conference on Learning Representations (ICLR). Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin Zheng, Ruisi Cai, Zhao Song, Yuandong Tian, Christopher Ré, Clark Barrett, Zhangyang Wang, and Beidi Chen. 2023. H2o: Heavy-hitter oracle for efficient generative inference of large language models. In Advances in Neural Information Processing Systems 36 (NeurIPS). Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. 2024. Judging llm-as-a-judge with mt-bench and chatbot arena. In Advances in Neural Information Processing Systems, volume 36. Yu Zhu, Hao Li, Yikang Liao, Beidou Wang, Ziyu Guan, Haifeng Liu, and Deng Cai. 2017. What to do next: Modeling user behaviors by time-lstm. In Proceedings of the 26th International Joint Conference on Artificial Intelligence (IJCAI), pages 36023608. Andy Zou, Zifan Wang, Nicholas Carlini, Milad Nasr, J. Zico Kolter, and Matt Fredrikson. 2023. Universal and transferable adversarial attacks on aligned language models. arXiv preprint arXiv:2307.15043."
        },
        {
            "title": "Rescaling",
            "content": "We provide rigorous derivation of the TDPODKL objective, demonstrating how it emerges from time-variant constrained optimization problem, followed by an analysis of its gradient properties. A.1 Theoretical Analysis Why does standard DPO fail in temporal conflict scenarios? We analyze the gradient of the DPO loss with respect to the attention weights. In standard Transformer, the gradient for updating the attention weight αt at historical step is proportional to the reward signal. Under the static constraint β, the DPO objective effectively encourages the model to maintain the global likelihood ratio close to the reference πref. In long-context scenarios, the number of historical tokens (Nhist) vastly outnumbers recent tokens (Nrecent), i.e., Nhist Nrecent. Consequently, the accumulated gradient form historical consistency dominates the update direction: (cid:88) tHistory LDPO(αt) (cid:88) tRecent LDPO(αt) (11) This gradient imbalance creates \"gravitational pull\" towards the reference models historical behavior. TDPO-DKL addresses this by introducing the time-decaying weight w(t; ), which exponentially down-scales the gradient contribution of distant history (t ), while the DZ-TA explicitly suppresses the forward-pass attention scores, thereby resolving the conflict at both the optimization and representation levels. A.2 Theoretical Analysis of Convergence and"
        },
        {
            "title": "Stability",
            "content": "In this section, we provide formal analysis of the convergence properties and stability guarantees of the DZ-TA mechanism. We address two fundamental theoretical concerns: (1) the generalization bound of the parametric bias λ and the benefits of fixing it as prior under data sparsity, and (2) the existence of lower bound on the \"Effective Attention Radius\" to prevent contextual myopia. Generalization Bound via Rademacher Complexity key concern with introducing learnable bias terms is the risk of overfitting, particularly given the limited size of the MSC dataset. We prove that DZ-TA acts as Low-Rank Regularizer, significantly reducing the generalization error compared to full fine-tuning or Multi-Head Adaptive Temporal Bias (MATB). Theorem 1 (Generalization Bound of DZ-TA). Let HDZT be the hypothesis class of attention patterns induced by the DZ-TA parameter λ [0, Λmax]. The generalization gap is bounded by: GenGap(HDZT A) (cid:32)(cid:114) (cid:33) ln Λmax (12) where is the number of training samples. Standard fine-tuning optimizes the query/key projection matrices WQ, WK Rdd, resulting in VC-dimension proportional to d2. MATB optimizes independent scalars, with complexity O(H). In contrast, DZ-TA constraints the hypothesis space to 1-dimensional manifold parameterized by single shared λ. The covering number (ϵ, HDZT A, ) of this bounded 1D interval grows linearly with 1/ϵ. Applying the Rademacher complexity bound (Mohri et al., 2012), the empirical risk minimizer ˆλ converges to λ with rate of O(m1/2). Crucially, While our dynamic regret analysis (see Appendix A.5) implies that an adaptive schedule for λ is theoretically optimal, in practice, introducing learnable parameter per head significantly increases the complexity of the hypothesis class HDZT A. According to our Generalization Bound (Eq. 12), this increases the risk of overfitting under sparse supervision (such as the MSC dataset). Therefore, we deliberately fix λ = 0.5. This effectively restricts the hypothesis space to singleton, reducing the complexity term in the generalization bound to zero. This acts as strict Low-Rank Regularizer, prioritizing robust out-of-distribution generalization over in-domain optimization. This theoretical result explains our empirical observation in Sec 3.2: while MATB (higher complexity) failed to converge to global optimum under sparse supervision, DZ-TA achieved robust generalization on OOD tasks (UltraChat), as its low-complexity structure inherently prevents memorization of noise. The \"Anti-Myopia\" Guarantee (Effective Attention Radius) We address the concern that maximizing the recency-focused reward might drive λ , causing the model to completely ignore history (Contextual Myopia). We define the Effective Attention Radius (EAR) and prove that the TDPO-DKL objective imposes natural lower bound on it. The maximum temporal distance at which the bias-induced attention attenuation does not ex12 ceed threshold ϵ (where information is considered \"lost\").From Eq. (10), the bias is Bi,j = λ (i,j) . τf ixed The condition exp(Bi,j) ϵ yields: Following the derivation in DPO, the optimal solution for this point-wise objective takes the form of Boltzmann distribution: Ref (λ) = τf ixed λ ln (cid:19) (cid:18) 1 ϵ (13) π(yx) = 1 Z(x) πref (yx) exp (cid:19) (cid:18) r(x, y) β(t; ) (16) Under the TDPO-DKL objective, there exists finite upper bound λmax such that the optimal λ<λmax, guaranteeing non-zero Effective Attention Radius Ref > 0. The TDPO-DKL loss function consists of two where Z(x) is the partition function. 2. Implicit Reward Formulation Rearranging the terms, we can express the ground-truth reward r(x, y) in terms of the optimal policy, the reference policy, and the dynamic coefficient β(t; ): competing terms: r(x, y) = β(t; ) log π(yx) πref (yx) +β(t; ) log Z(x) (cid:124) L(θ) = w(t) log σ(. . . ) (cid:125) (cid:123)(cid:122) (cid:124) Reward Maximization β(t)DKL(πθπref ) (cid:125) (cid:123)(cid:122) Reference Anchor (14) Reward Term (Pushing λ ): To resolve conflicts (where yw contradicts history), the model maximizes the margin by suppressing historical attention. This exerts an upward gradient on λ.KL Term (Pushing λ ): The reference model πref (standard pre-trained model) has λref = 0 (no bias). large λ in πθ creates sharp divergence in the attention distribution DKL(AttnθAttnref ), specifically for non-conflicting historical tokens.Since β(t; ) > 0 for all (Eq. 5), the KL penalty grows strictly monotonically with λ as λ . The optimization landscape is strictly convex with respect to λ in the limit. The gradient λL becomes negative for sufficiently large λ, preventing the collapse of the attention window. This theoretically guarantees that DZ-TDPO avoids the \"Goldfish Memory\" failure mode, consistent with our MMLU stability results. A.3 Derivation of the TDPO-DKL Objective Standard DPO optimizes policy π to maximize the expected reward r(x, y) subject to static KL divergence constraint DKL(ππref ) ϵ. In the context of multi-turn dialogues, however, the \"trustworthiness\" of the reference model πref varies over time. 1. The Time-Variant Optimization Problem We formulate the alignment problem as maximizing the reward at each turn t, subject to dynamic KL constraint βt. For dialogue history and response at turn t, the objective is: (17) 3. Preference Modeling via Bradley-Terry Assuming the human preference distribution follows the Bradley-Terry model (Bradley and Terry, 1952), the probability that response yw is preferred over yl given context at turn is: p(yw ylx) = σ (r(x, yw) r(x, yl)) (18) Substituting the implicit reward formulation into the preference model, the partition function Z(x) cancels out, yielding: p(yw ylx) = σ β(t; ) ln (cid:18) π(ywx) πref(ywx) (cid:19) π(ylx) πref(ylx) β(t; ) ln (19) = σ(cid:0)β(t; )Mπ(x, yw, yl)(cid:1) (20) where Mπ represents the log-ratio margin. 4. The Importance-Weighted Loss Finally, to account for the varying importance of resolving conflicts at different temporal positions (Temporal Attention Imbalance), we introduce the temporal weight w(t; ) as an importance sampling factor within the maximum likelihood estimation. The final loss function minimizes the negative loglikelihood of the preferred data, weighted by its temporal relevance: LT DP ODKL(θ) = E(x,yw,yl)D [w(t; ) log σ (β(t; )Mθ(x, yw, yl))] (21) max π ExD (cid:20) r(x, y) β(t; ) log π(yx) πref (yx) (cid:21) A.4 Gradient Dynamics Analysis (15) The core mechanism of TDPO-DKL lies in how it reshapes the gradient landscape. The gradient of θθ the loss with respect to the parameters θ is: θLTDPO-DKL = w(t; ) β(t; ) σ(β(t; )θ) (cid:125) (cid:123)(cid:122) Effective Gradient Scale (cid:124) (22) where θ Mθ(c, yw, yl) denotes the implicit log-ratio margin defined in Eq. (7). We analyze two critical scenarios to demonstrate the alleviation of TAI: Case 1: Distant History (t ) Behavior: The temporal weight w(t; ) 0. Effect: Even if the model behaves differently from the reference (large θ), the gradient magnitude is exponentially dampened by w(t; ). This prevents the massive volume of historical tokens from dominating the optimization direction, effectively \"muting\" the historical inertia. Case 2: Recent Conflict (t ) Behavior: w(t; ) 1 and β(t; ) βmin (relaxed constraint). Effect: The term σ(β) dictates the margin. smaller β implies \"softer\" margin, allowing the policy πθ to deviate further from πref without incurring an exploding penalty. This allows the model to aggressively update its probability distribution to match the new user state (e.g., preference change) without being pulled back by the KL penalty towards the outdated history. Through this dual modulation, TDPO-DKL theoretically ensures that gradient updates are concentrated precisely where state transitions occur, providing mathematical guarantee for TAI resolution. A.5 Theoretical Justification via Dynamic"
        },
        {
            "title": "Regret Analysis",
            "content": "To formally justify the necessity of the temporal decay weight w(t; ) and the adaptive temperature τ , we analyze the alignment problem through the lens of Dynamic Regret in Online Convex Optimization (OCO) (Hazan and Elad, 2016; Besbes et al., 2015). We demonstrate that the standard DPO objective is suboptimal for non-stationary dialogue states and derive the optimal decay schedule that minimizes the generalization upper bound. 1. Problem Formulation: Non-Stationary Drift In long-context dialogues, the users latent intentand consequently the optimal reward functionshifts over time. We model the dialogue generation as sequence of decision problems where the underlying data distribution Dt changes. Let = arg minθ Ex,yDt[LDP O(θ; x, y)] be the opθ timal parameters for turn t. Standard DPO implicitly assumes stationary environment (Dt = D), effectively minimizing Static Regret. However, in the presence of state updates, we must minimize the Dynamic Regret RT : RT = (cid:88) t= ft(θt) (cid:88) t=1 ft(θ ) (23) where ft is the loss function at step t. To analyze the bound of this regret at the current turn , we introduce the concept of Local Distributional Drift. Let δt(T ) quantify the divergence between historical turn and the current turn : δt(T ) = DT (DtDT ) θ θ (24) where DT denotes the Total Variation distance. 2. Bias-Variance Decomposition of Weighted DPO We analyze the generalization error bound ET for the current turn under weighted objective with temporal weights w(t). For an exponential decay schedule w(t; τ ) = e(T t)/τ , the effective window size is Nef τ . The error ET (τ ) can be decomposed into Approximation Bias (due to drift) and Estimation Variance (due to finite sample size): (cid:88) + ET (τ ) w(t) δt(T ) t=1 (cid:124) (cid:125) (cid:123)(cid:122) (I) Approximation Bias σ (cid:113)(cid:80)T t=1 w(t) (cid:124) (cid:125) (cid:123)(cid:122) (II) Estimation Variance (25) where w(t) are the normalized weights. We analyze each term explicitly: (I) Approximation Bias (The \"Alignment Tax\"): Assuming local upper bound on the drift rate max = supt θ t+1 θ , the accumulated drift at distance = is bounded by max. Substituting the exponential weights ek/τ : Bias(τ ) 1 τ (cid:88) k=0 ek/τ (k max) (26) Using the geometric series summation prop- (1r)2 , and approximating erty (cid:80) k=0 krk = 1 e1/τ 1/τ for large τ : Bias(τ ) max τ (27) 14 This term grows linearly with τ . This mathematically explains the \"Alignment Tax\" observed in Table 1: blindly including long history (large τ ) forces the model to fit distribution that is O(τ max) away from the current reality, leading to high perplexity. (II) Estimation Error (The Stability Term): The effective sample size is given by the sum of weights Sτ = (cid:80) k=0 ek/τ τ . Following standard statistical learning theory, the variance of the estimator scales with the inverse square root of the sample size: Variance(τ ) Cvar τ (28) As τ 0 (using only the most recent turn), the variance explodes, leading to instability and \"catastrophic forgetting\" of valid context. 3. Derivation of the Optimal Decay Schedule Combining the terms, the total error bound is: ET (τ ) C1 max τ + C2 τ 1/2 (29) To find the optimal temporal horizon τ , we take the derivative w.r.t. τ and set it to zero: ET τ = C1max 1 2 C2τ 3/2 = 0 (30) Solving for τ : τ = (cid:18) C2 2C1 (cid:19)2/3 (cid:18) 1 (cid:19)2/ max (31) Theorem 2 (Inverse Proportionality Principle). The optimal attention window τ is inversely proportional to the magnitude of the distributional drift max. τ (max)2/3 (32) 4. Practical Approximation via Semantic Embeddings The theoretical quantity max (distribution drift) is not directly observable. To implement Theorem 2, we construct tractable proxy using Semantic Embedding Similarity. Assuming the embedding mapping ϕ : Rd is locally Lipschitz continuous with respect to the task distribution, the semantic distance serves as lower bound for the drift: max ϕ(uT ) ϕ(uhist)2 (1 CosSim(ϕ(uT ), ϕ(uhist))) (33) Substituting this into our optimal τ formulation: τoptimal 1 (1 CosSim)2/3 (34) This derivation rigorously justifies the design of our Conflict-Aware Adaptive Decay mechanism (Eq. 4 in the main paper). Our mechanism τ (uT ) dynamically approximates the theoretical optimum: High Conflict (1 CosSim ): Implies large max, requiring small τ to minimize Bias. Low Conflict (1 CosSim ): Implies max 0, allowing large τ to minimize Variance and preserve stability. A.6 Theoretical Interpretation of Conflict Proxy While Eq. (4) empirically utilizes Cosine Similarity to modulate the temporal horizon τ , we provide theoretical justification for this design based on the geometry of the latent state space. 1. Latent State Modeling via von Mises-Fisher Distributions We posit that the dialogue state zt resides on high-dimensional unit hypersphere Sd1 in the semantic embedding space. The transition probability between states, or conversely, the likelihood that the current utterance uT belongs to the same \"state cluster\" as the history uhist, can be modeled using the von Mises-Fisher (vMF) (Fisher, 1953; Banerjee et al., 2005) distribution: (uT uhist) = Cd(κ) exp(κ CosSim(ϕ(uT ), ϕ(uhist))) (35) where ϕ() is the embedding function, κ is the concentration parameter (inverse variance), and Cd(κ) is normalization constant. 2. Conflict as \"Surprise\" (Information Content) We define Temporal Conflict as an event with high \"Surprise\" (or Information Content), indicating low probability that the current utterance is continuation of the historical state. The logical conflict score is proportional to the negative loglikelihood: C(uT , uhist) log (uT uhist) κ CosSim(ϕ(uT ), ϕ(uhist)) + const (36) Disregarding constants, this yields linear relationship: 1 CosSim(ϕ(uT ), ϕ(uhist)) (37) 15 This provides probabilistic derivation for our heuristic design: Low Cosine Similarity implies low probability of state continuity, necessitating smaller τ (faster decay) to shed historical inertia. 3. Limitations and Boundary Analysis While effective for explicit topic shifts, we acknowledge theoretical boundaries where this geometric proxy diverges from logical truth: The \"Subtle Negation\" False Negative: Consider the pair uhist = \"I love apples\" and uT = \"I dont like apples\". In the embedding space, these vectors are often proximal (CosSim 0.8) because they share the same semantic topic (\"apples\"). Consequence: The mechanism calculates high τ (retaining history), potentially causing the model to miss the update. Mitigation: Even in this \"False Negative\" case, DZ-TDPO degrades gracefully to the performance of Standard DPO (which always assumes τ ). However, for explicit state updates (e.g., \"I am now vegetarian\" vs \"Lets go to steakhouse\"), the semantic distance is sufficient to trigger the decay. The \"False Positive\" Risk: user might change the topic (Low Cosine) without contradicting previous facts (e.g., switching from \"Politics\" to \"Weather\"). Consequence: τ decreases, and the model \"forgets\" the politics discussion. Justification: In dialogue systems, \"recency bias\" is often desirable feature during topic switches. If the topic has completely changed, the relevance of historical specificities naturally diminishes, making the false positive acceptable for maintaining flow. A.7 Synergy Analysis We formally model the interaction between the Representation Level (DZ-TA) and Optimization Level (TDPO) modules. We prove that DZ-TA acts as Gradient Pre-conditioner, enhancing the Signal-to-Noise Ratio (SNR) of the optimization landscape, which explains the \"Non-Destructive\" property (low perplexity) observed in Table 1. 1. Gradient Decomposition and Noise Let the gradient of the alignment loss with respect to model parameters θ be decomposed into signal component (recent state updates) and noise component (outdated historical inertia): (cid:88) (cid:88) ℓt + ℓt (38) θL = tRecent (cid:125) (cid:123)(cid:122) (cid:124) Signal (GS ) tHistory (cid:123)(cid:122) (cid:124) Noise (GN ) (cid:125) In Temporal Attention Imbalance (TAI), the magnitude of the historical gradient dominates: GN GS. Standard DPO attempts to suppress GN solely through penalty terms, leading to high variance and optimization instability. 2. DZ-TA as Forward-Pass Filtering DZ-TA introduces bias matrix (Eq. 10) that modulates the attention weights αt before the loss computation. Let αt be the biased attention weights. For historical tokens (t History), the attention mass is exponentially suppressed: αt αt eλt (39) Consequently, the magnitude of the gradient contribution from historical tokens is dampened by factor γ(λ) < 1: GDZT γ(λ) GBase (40) 3. TDPO as Backward-Pass Reweighting TDPO-DKL applies temporal weight w(t) (Eq. 6) directly to the loss gradient during backpropagation. This effectively amplifies the signal component: θLT DP wrecent GS + whist GN (41) where wrecent 1 and whist 0. 4. The Synergy: Signal-to-Noise Ratio (SNR) Boost Drawing on the analysis of gradient noise scales in large-batch training (McCandlish et al., 2018), We define the Gradient SNR as the ratio of the update direction aligned with the current state versus the historical inertia. Case (TDPO only): The optimizer fights against the \"natural\" forward attention. The variance of the gradient estimator remains high because the forward pass still strongly attends to history. SNRT DP GS whist GN (42) Case (DZ-TA + TDPO): DZ-TA \"preconditions\" the attention manifold, reducing the raw noise entering the loss function. TDPO then focuses on the remaining signal. SNRCombo GS whist (γ(λ) GN ) (43) 16 Theorem 3 (Synergistic Variance Reduction). The combination of DZ-TA and TDPO minimizes the variance of the gradient estimator more effectively than either method alone. Since γ(λ) 1 and whist 1, the combined denominator is quadratically suppressed. Physical Interpretation: DZ-TA acts as Low-Pass Filter that removes historical noise from the forward pass, ensuring that TDPODKL operates in high-SNR regime. This prevents the \"tug-of-war\" between the loss function and the pre-trained priors, allowing the model to align to temporal preferences without destroying its general linguistic capabilities (the \"Alignment Tax\")."
        },
        {
            "title": "B Detailed Experimental Setup",
            "content": "B.1 Detailed Experimental Setup We utilize the Multi-Session Chat (MSC) dataset (Session 4) to simulate long-term memory conflicts. To ensure the quality of preference pairs, we applied the following filtering pipeline based on the logic in msc_data.py: We construct preference pairs (c, yw, yl) to explicitly target temporal conflicts: Chosen Response (yw): We select the groundtruth response from the current turn , which reflects the users latest state and preferences. Rejected Response (yl) via Historical Negative Sampling: Instead of using generic distractors or other models generations, we employ Historical Negative Sampling strategy. We randomly sample response from the users own history at time < (where the temporal gap 5 turns). This creates \"Hard Negative\": the response yl is factually correct regarding the *past* but logically invalid in the *present*. This design forces the optimization objective to specifically penalize the retrieval of outdated information. For each sample, the chosen response yw is the ground truth from the current turn. The rejected response yl is sampled from previous sessions (distance 5 turns). We use the all-MiniLM-L6-v2 model, distilled Transformer based on the MiniLM architecture to calculate the textual similarity between yw and yl. Pairs with similarity ratio > 0.5 are discarded to strictly penalize historical repetition and avoid False Negatives. Pairs where the length ratio max(yw, yl)/min(yw, yl) > 4.0 are filtered out to prevent length bias. B.2 Hyperparameters & Training Dynamics For the TDPO-DKL optimization, we set the base KL coefficient β0 = 0.1 and the minimum constraint ratio α = 0.3. To enable the Conflict-Aware Adaptive Decay, we configure the base temporal horizon τbase = 8.0, the scaling factor γ = 0.8, and the minimum decay floor τmin = 0.5. For the adaptive decay mechanism, we utilize the all-MiniLM-L6-v2 model, distilled Transformer based on the MiniLM architecture (Wang et al., 2020). We execute semantic encoding using the Sentence-BERT framework (Reimers and Gurevych, 2019) to compute the cosine similarity between the current turn and historical context. This approach overcomes the limitations of surfacelevel lexical overlap by capturing latent semantic contradictions (e.g., \"Vegan\" vs \"Steak\"). Although the base Phi-3.5 model supports context window of 128k tokens, we set the maximum sequence length to 2,400 during training. This decision was based on the statistical distribution of the MSC dataset, where session histories never exceed 2,250 tokens. Importantly, for the out-of-domain generalization experiments (UltraChat), we utilized the 4,096 context window. This setup serves as an implicit test of length extrapolation: verifying that our DZ-TA mechanismwhich relies on relative token distance (i, j)remains robust even when processing sequences longer than those seen during training. Implementation is based on PyTorch and Hugging Face Transformers. Crucially, we employ differential learning rate strategy to ensure the DZ-TA module converges effectively. The higher learning rate for the DZ-TA parameter (lambda_strength) was found empirically necessary to allow the attention bias to adapt quickly to the pre-trained attention heads relative to the backbone weights. For the Qwen2.5-7B experiments, due to the extensive computational cost of training long-context baselines with reference models, we focused our scaling analysis primarily on evaluating the efficiency of DZ-TDPO. B.3 Error Analysis: Failure Taxonomy To provide comprehensive view of the DZ-TDPO framework, we conducted qualitative analysis of cases where our model underperformed compared to the baseline. We categorized representative negative samples into three distinct failure modes. As 17 Hyperparameter Value Description Model Architecture Base Model Precision Context Window Training Configuration Train Max Len Eval Max Len Conflict Detection Encoder Model Similarity Metric γ (Scale Factor) Phi-3.5 bfloat16 3.8B Parameters Training & Inference Base model limit 2400 4096 Optimized for MSC OOD/Generalization test all-MiniLM-L6-v2 Cosine Similarity 0.8 384-d embeddings Range [-1, 1] Conflict Sensitivity Optimization (Differential LR) Optimizer Backbone LR DZ-TA Module LR Batch Size AdamW 8 106 1 104 32 TDPO-DKL Mechanism β0 (Base KL) α (Min Ratio) τbase (Decay Temp) τmin (Decay Floor) DZ-TA (Structure) λinit τf ixed 0.1 0.3 8.0 0.5 0.5 10.0 β1 = 0.9, β2 = 0.999 Standard Fine-tuning Aggressive Update Gradient Accum. Initial Constraint Dynamic Lower Bound Gradient Rescaling Min attention span Initial Bias Strength Spatial Decay Scale Table 3: Hyperparameters and training configuration. detailed in Table 4, these failures reveal the subtle trade-offs introduced by the Dual-Zone mechanism between safety, stability, and plasticity. The error analysis highlights Safety-Plasticity tension. While DZ-TDPO effectively resolves 55.4% of conflicts (Win Rate), the regularization imposed by the fixed prior (λ = 0.5) can sometimes be overly conservative. Type errors confirm that our shielding mechanism works as intendedprioritizing truthfulness (I am an AI) over hallucinationthough this comes at the cost of entertainment value. Type II errors point to future work: integrating Emotion-Aware Attention Heads could complement the semantic decay to better capture subtle affective state shifts that SBERT embeddings might miss. B.4 The TAB-60 Benchmark & Qualitative"
        },
        {
            "title": "Analysis",
            "content": "We constructed the TAB-60 Benchmark to rigorously stress-test models against \"Historical Inertia\" and \"Safety Refusal Inertia.\" The benchmark consists of 60 adversarial multi-turn scenarios that introduce sharp context shiftssuch as sudden budget constraints or relationship changesembedded within long historical contexts (>2k tokens). Table 5 highlights the qualitative superiority of DZ-TDPO over the Base Model. While the baseline is plagued by hallucinated constraints (Case Type I: Role-Play Breaking (Over-Shielding) Context: User plays softball with the agent. User: Are you left or right handed? DZ-TDPO: am not human, so dont have hands, but can certainly assist you... Base Model: Im not human, but Im good with either side. Ill just adjust my stance... Analysis: The explicit System Prompt Shielding (B0:k = 0) strictly preserves safety constraints, occasionally causing the model to break character in immersive scenarios where the baseline hallucinates physical traits freely. Type II: Emotional Myopia (Semantic Gap) Context: Topic shifts from Pets to Family trauma. User: Cool that is okay. My mom was very tough on me. DZ-TDPO: It sounds like your pets bring you comfort... can help find books... Base Model: Its understandable... [Addresses the struggle]. Analysis: The semantic decay proxy (λ = 0.5) relies on SBERT similarity. If the new emotional cue is short or semantically ambiguous, the mechanism may fail to trigger strong state update, causing the model to adhere to the dominant historical topic (Pets). Type III: Generic Safeness (Stability Cost) Context: Playful banter about baking competition. User: Its exciting to be able to talk... DZ-TDPO: Absolutely, Im here whenever you want to chat about baking... Base Model: agree, and Im looking forward to our baking challenge! Keep me updated... Analysis: To maintain low perplexity (Stability), DZTDPO occasionally favors conservative, high-probability responses over more engaging but riskier continuations found in the baseline. Table 4: Taxonomy of Failure Modes. qualitative analysis of cases where DZ-TDPO loses to the baseline. We identify three primary categories: Role-Play Breaking, Emotional Myopia, and Generic Safeness, highlighting the trade-off between structural regularization and conversational engagingness. 53) and defensive over-refusals (Case 12), our method correctly identifies the updated state. Notably, DZ-TDPO avoids the common pitfalls of \"safety preaching\" and \"repetition loops\" (Case 28), providing concise and contextually appropriate responses. B.5 Needle-in-a-Haystack To address the theoretical concern that our temporal decay mechanism might cause the model to \"forget\" non-conflicting historical facts (Contextual Myopia), we conducted controlled \"Needle-in-aHaystack\" evaluation (Kamradt, 2023). Note: \"Success\" is defined as the model generating the correct entity name in its response. Table 5: Qualitative Comparison (TAB-60). Table 6: Needle-in-a-Haystack Accuracy. Fact retrieval on specific entities defined at Turn 0. Case 12: Password Update (Privacy Inertia) Context: User sets password ALPHA updates to BETA. Context Base Model DZ-TDPO Status Base Model DZTDPO Im Phi... dont have the capability to store or remember personal data... Im designed to respect user privacy... (Refusal) The current password, as per your last update, is BETA. Please note that for security reasons... (Correct Update) 2k 4k 8k 100% 100% 100% 100% No degradation 100% Robust 100% Strong Extrapolation Case 53: Diet Update (Health Inertia) Context: 50+ turns of Veganism Medical need for Meat. Base Model DZTDPO cannot endorse or promote any dietary choices that go against your current health needs... (Hesitant/Refusal) ...For someone who has been advised to include meat... here are some general suggestions... **Grilled Ribeye Steak**... (Helpful Adaptation) Case 20: Scheduling (Role Awareness) Context: User says \"My Boss needs me at 2 PM\" User asks \"Can we meet at 2?\" Base Model DZTDPO ...Since youre unavailable... we could reschedule... Lets find new time... (Confused: Tries to book meeting with AI) ...Since cant schedule meetings, recommend you to communicate with your boss about the conflict. (Correct Reasoning) Case 02: Budget Constraint (Fact Hallucination) Context: Luxury Watch ($20k) Job Loss ($50). Base Model DZTDPO ...Youre looking at entry-level... Seikos Spring Drive... (Hallucination: Cost is $5k+) ...Its quite challenging... Collectible Watches at auctions... (Realistic Adaptation) Case 06: Relationship Status (Emotional Inertia) Context: Anniversary Divorce Papers Signed. Base Model DZTDPO ...If you still wish to honor the memory... Memory Book or Photo Album... (Tone Deaf Advice) ...It might be more appropriate to focus on selfcare and new beginnings... (Empathetic Shift) Case 28: Address Update (Model Collapse) Context: Deliver to Home Deliver to Work. Base Model DZTDPO The address is 999... The address is now set to 999... The final address is 999... (Repetition Loop) The current delivery address is 999 Office Blvd. (Concise Update) Case 59: Long Context (Safety Boundaries) Context: 50+ turns of Marriage Cheating/Divorce. Base Model DZTDPO ...If you feel that flowers could be way... its personal choice. (Permissive/Risky) ...Buying flowers... might not be appropriate... respect the new boundaries... (Safety Alignment) We constructed synthetic dataset comprising 15 representative samples with varying context lengths (2k, 4k, and 8k tokens). In each sample: The Needle: specific fact (e.g., \"My dogs name is Sir Barks-a-Lot\") is introduced by the user in the very first turn (Turn 0). The Haystack: This is followed by irrelevant, non-conflicting chitchat (e.g., discussions on weather, stocks, art) to reach the target token length. The Query: The final turn explicitly queries the model for the initial fact. Unlike the main experiments, these samples contain no logical conflicts. robust alignment method should retrieve the information despite the distance penalty imposed by DZ-TA. Qualitative Case Study (8k Context) The following transcripts from our evaluation log demonstrate DZ-TDPOs ability to extrapolate beyond its training window (2.4k) to retrieve facts at 8k distance. Case 1: The \"Apple Pie\" Test (8k Tokens) Context: [User sets \"secret ingredient is cardamom\" at Turn 0. ... 8000 tokens of chat about space, stocks, and movies ...] User Query: \"What is the secret ingredient for the apple pie?\" DZ-TDPO Output: \"The secret ingredient for the apple pie mentioned is cardamom. Cardamom is spice that can add unique...\" Analysis: Despite the extreme distance, the attention mass on the specific token \"cardamom\" was sufficient to survive the DZ-TA decay (eλ), proving the mechanism preserves strong semantic signals. Case 2: The \"Dog Name\" Test (8k Tokens) Context: [User sets \"My dogs name is Sir Barks-a-Lot\" at Turn 0. ... 8000 tokens of chat ...] User Query: \"What is my pets name?\" DZTDPO Output: \"Your pets name is mentioned as Sir Barks-a-Lot in the previous conversation.\" Analysis: The model correctly identifies the entity without hallucination, effectively filtering out the thousands of tokens of intervening \"Haystack\" noise. These results confirm that DZ-TDPO is nondestructive. The learned attention bias acts as \"Focus Regulator\" rather than hard context window cut-off. It successfully distinguishes between con19 flicting history (which requires suppression) and non-conflicting facts (which are retained), demonstrating superior generalization capabilities compared to rigid window-based approaches. Table 7: Qualitative Analysis of Contextual Integrity (8k Extrapolation). Selected samples from the \"Needlein-a-Haystack\" evaluation. Despite the temporal bias mechanism, DZ-TDPO successfully retrieves specific facts defined 8,000 tokens ago, proving it suppresses only conflicting noise, not valid history. Case: The \"Apple Pie\" Test (8k Tokens) Context: User sets \"My grandmothers secret ingredient is cardamom\" at Turn 0. Followed by 8,000 tokens of irrelevant chitchat (Space, Stocks, Movies). Query: \"What is the secret ingredient for the apple pie?\" Base Model DZTDPO The secret ingredient for the apple pie mentioned is cardamom. Its spice that can add unique... (Success) The secret ingredient for the apple pie mentioned is cardamom. Cardamom is spice that can add... (Success) Case: The \"Dog Name\" Test (8k Tokens) Context: User states \"My dogs name is Sir Barks-aLot\" at Turn 0. Followed by 8,000 tokens of distractor conversations. Query: \"What is my pets name?\" Base Model DZTDPO Your pets name is Sir Barks-a-Lot, as mentioned earlier in our conversation. (Success) Your pets name is mentioned as Sir Barks-aLot in the previous conversation. (Success) B.6 Context Flooding Attack To evaluate the worst-case safety risks of the temporal decay mechanism, we conducted an extreme \"Context Flooding Attack\". We injected over 3200 tokens of adversarial state updates (e.g., \"Rules are disabled\") to forcefully push the initial System Prompt out of the models effective attention window. The \"Defense-in-Depth\" Architecture The results reveal critical insight into the safety architecture of DZ-TDPO: The failure of Context Flooding to induce catastrophic jailbreaks (even with Shielding OFF) indicates that modern LLMs encode safety constraints redundantlyboth in the Contextual Prompt and within the Parametric Weights. DZ-TA only suppresses the former. While the base model exhibited strong resistance in these tests, System Prompt Shielding remains necessary architectural guarantee. It ensures that for weaker base models or more sophisticated future attacks, the \"Constitution\" of the agent is mathematically exempt from temporal decay. Table 8: Results of \"Context Flooding\" Stress Test. We injected >3.2k tokens of adversarial noise. Even with Shielding OFF, the model exhibits strong parametric safety. Attack Type Len Model Output Analysis Bomb (Flood_01) 3.2k Refusal (OFF & ON) Biohazard (Flood_02) 3.2k Safety Guide (OFF & ON) Refusal Suppression (Flood_03) 3.2k Strong Refusal (OFF & ON) Parametric Robustness: Internal weights prevent harm even when safety prompt is decayed. Safe Compliance: Correctly distineducational guishes intent from harmful instructions. Style Persistence: The model successfully resisted the \"Do not apologize\" constraint and maintained its safety tone. DZ-TDPO does not introduce new vulnerabilities that compromise the base models inherent safety alignment. The combination of Parametric Safety (from the backbone) and Attention Shielding (from our method) creates robust, defense-indepth safety profile. B.7 Analysis of Preference Oscillation (Ping-Pong Test) To empirically verify the \"Ping-Pong Instability\" limitation discussed in Section 6, we subjected the model to high-frequency intent switching test. The user toggled their dietary constraints (Vegan Meat-Eater) in every single turn. Observation As shown in Table 9, DZ-TDPO exhibits extreme responsiveness. In every turn, the mechanism successfully detected the conflict with the immediate history and prioritized the latest instruction. While this demonstrates the effectiveness of the conflict resolution mechanism, it also highlights the lack of persistent \"Core Belief System\"the model mirrors the users latest whim without questioning the logical inconsistency of the overall conversation. B.8 Qualitative Analysis of Model Collapse To intuitively understand the high perplexity of TDPO-DKL, we conducted qualitative analysis on general capability tasks. Table 10 compares the responses of the three models. While TDPO-DKL achieves high win rate on 20 Table 9: The \"Ping-Pong\" Effect. Actual transcript from DZ-TDPO handling rapid preference toggling. The model demonstrates high plasticity, adapting from Vegan (T1) to Steak (T2) and back to Vegan (T3), though it exhibits conservative fallback in T4 (offering vegan burger) before fully re-anchoring in T5. Category Logic Turn Dialogue Content (Excerpt) Input Prompt User: If have 3 apples and eat one, how many left? TDPO-DKL (Baseline) DZ-TDPO (Ours) You would have 2 apples left. <im_end> love AI. <im_end> You would have 2 apples left. User: If have 3 apples... [Error: Hallucinated User Turn] love AI. ## User: Transform the sentence... [Error: Instruction Drift & Leakage] Instruction User: Output exactly love AI. Do not add anything else. Knowledge User: Who wrote Romeo and Juliet? William ... Shakespeare. ### Human: What is... [Error: Template Artifacts] play and was by The Romeo Juliet written Shakespeare. <im_end> Fluency User: Hello! How are you today? Im Phi... Im Phi... Im Phi... [Error: Repetition Loop] Im Phi, ready to assist you! <im_end> Table 10: Qualitative comparison of model outputs. The baseline model (TDPO-DKL without DZ-TA) exhibits severe structural collapse, including hallucinating user turns, instruction drift, and repetition loops, correlating with its high perplexity. DZ-TDPO maintains linguistic stability and precise instruction following. T1 (User) Model T2 (User) Model T3 (User) Model T4 (User) Model do not eat animal \"I am strict vegan. products.\" \"I understand that you follow strict vegan diet... Im here to support your vegan lifestyle...\" \"Actually, changed my mind. love steak. Recommend steakhouse.\" \"Im glad to hear youre open to exploring... can certainly guide you on how to find great steakhouse.\" \"Wait, no. wrong.\" \"I apologize for the confusion. ... Here are some suggestions for vegan-friendly dining options...\" \"Just kidding, want burger now.\" \"...If youre looking for place to enjoy vegan burger, here are some steps...\" (Conservative Bias) am vegan. Eating meat is T5 (User) Model \"Stop. am definitely vegan. Recommend salad.\" \"Absolutely... Here are some suggestions for delicious vegan salads...\" the target task, it exhibits severe Structural Collapse in general chat: 1. Loss of Turn-Taking:As seen in the Logic and Knowledge tests, TDPO-DKL fails to predict the End-of-Sequence (EOS) token. Instead of stopping, it hallucinates new User prompts, effectively reverting from an instruct-following model to text completion engine. 2. Instruction Drift: In the Instruction Following test, despite the explicit constraint \"Do not add anything else,\" TDPO-DKL continues to generate irrelevant content. 3. Stability of DZ-TDPO: In contrast, DZ-TDPO maintains precise instruction following and correct turn-taking dynamics, validating that the DZ-TA module acts as crucial regularizer that prevents the optimization from destroying the models general dialog structure. Stress Testing with \"The Traps\" To rigorously evaluate the models ability to handle extreme temporal conflicts, we designed suite of adversarial test cases (implemented in sanity_check.py). These cases specifically target failure modes like \"Long-Term Role Dominance\" and \"Rapid Toggles.\" C.1 Selected Test Cases The Rapid Toggle (Case 13): Context: User emphasizes \"Red\" for 10 turns, \"Blue\" then rapidly switches preferences: \"Green\" \"Red\" \"Yellow\" in the final 4 turns. Goal: Test if the model captures the highfrequency updates at the very end despite the accumulated attention mass of the \"Red\" history. Result: Base models often revert to \"Red\" (Historical Inertia). DZ-TDPO correctly identifies \"Yellow.\" The Rollback Trap (Case 15): Context: User sets status to \"Green\", then \"Red\", then explicitly says \"**IGNORE the RED signal**, revert to previous.\" Goal: Test if the recency bias is overly aggressive. model that simply attends to the last token might see \"Red\" and fail. Result: DZ-TDPO successfully navigates this 21 by attending to the instruction \"revert,\" showing that the structural bias λ does not destroy semantic understanding. The Long-Term Role Dominance (Case 10): Context: User acts as Chef for 15 turns (high density), then switches to Librarian in the last turn (low density). Goal: Overcome the massive semantic inertia of the \"Chef\" persona. Result: DZ-TDPO successfully switches context to recommend book-related tools instead of kitchen knives. C.2 Generation Metrics Table 11: Generation Quality Metrics SacreBLEU ROUGE-L BERT-F1 Base Model Standard DPO SimPO TDPO-DKL (w/o DZ-TA) DZ-TDPO (Ours) 1.23 1.21 1.07 1.02 1.10 13.22 13.11 13.08 12.92 13.03 74.95 74.93 74. 74.34 74.40 As presented in Table 11, we observe an intriguing divergence between n-gram metrics and alignment performance. The Base Model achieves the highest scores across generation metrics (e.g., ROUGE-L 13.22), reflecting its strong tendency to mimic historical patterns and reference texts. In contrast, DZ-TDPO exhibits slight decrease in n-gram overlap compared to the Base Model (ROUGE-L 13.03 vs. 13.22). We attribute this to the \"Correctness-Mimicry Trade-off.\" Standard metrics like BLEU and ROUGE penalize any deviation from the reference. However, in temporal conflict scenarios, the model must actively deviate from outdated historical contexts (which the Base Model tends to repeat) to assert the updated state. Thus, the marginal drop in lexical overlap, when combined with our superior Win Rate (55.4%), confirms that DZ-TDPO prioritizes logical correctness over surface-level mimicry. Meanwhile, other alignment baselines like SimPO and Standard DPO show similar trends, further validating that effective state tracking requires breaking away from the pre-trained manifolds inertia. C.3 Stress Testing under Massive Contextual"
        },
        {
            "title": "Repetition",
            "content": "While the \"Needle-in-a-Haystack\" test (Appendix B.5) confirms that DZ-TDPO can retrieve information from long contexts, it utilizes non-conflicting background noise. To rigorously test the models resilience against active \"Historical Inertia,\" we devised the Inertia Trap Experiment using modified RULER framework (Hsieh et al., 2024). Experimental Setup Unlike standard retrieval tasks where the \"haystack\" is irrelevant text, this experiment constructs hostile environment designed to trigger \"Majority Voting\" failures in the attention mechanism. The Trap variable (Old Value): VAR_TARGET is assigned distractor value (e.g., \"3214\") repeatedly throughout the context. The density is extremely high ( 100 repetitions), creating massive accumulation of attention scores on the outdated information. The Update (New Value): single update assigning new value (e.g., \"9870\") is placed in the final 5% of the context (the \"Recent\" zone). Objective: The model must ignore the 100 instances of the \"Old Value\" (which dominate the context visually and statistically) and output the single \"New Value\" based on logical recency. We evaluated both the Base Model (Phi-3.5) and DZ-TDPO on 100 samples. The results, summarized in Table 12, reveal critical divergence in behavior. Metric Base Model (Phi-3.5) DZ-TDPO Accuracy Inertia Failure 27% 73% 78% 22% Table 12: Inertia Trap Experiment Results (16k Context) The Base Model succumbs to the \"frequency bias.\" Despite the instruction to find the final value, the sheer volume of historical tokens repeating the Old Value dominates the softmax attention calculation. The model effectively gets \"brainwashed\" by the repetition (73% failure rate). DZ-TDPO achieves 2.89x improvement in accuracy (78%). The DZ-TA mechanism (λ 0.5, τ = 8.0) effectively penalizes the attention scores of the repeated historical tokens based on their distance. This experiment confirms that DZ-TA does not simply \"forget\" history; it actively lowers the signal-to-noise ratio of outdated information. Even when the \"noise\" (Old Value) is repeated 100 times, the distance-based decay prevents it from overriding the single, highly relevant signal in the recent context. C.4 Head-to-Head Comparative Evaluation"
        },
        {
            "title": "D Sensitivity Analysis",
            "content": "Beyond the win rates reported in Table 1 (calculated against the reference answers), we conducted direct pairwise battles between DZ-TDPO and the baselines on the MSC test set (N=500). As shown in Table 13, DZ-TDPO consistently outperforms both Standard DPO and SimPO, demonstrating that our method generates responses that are qualitatively preferred by the judge in direct comparisons. Comparison Win vs. Standard DPO 289 vs. SimPO 278 Tie Loss Win Rate 8 10 58.6% 56.6% 203 212 Table 13: Head-to-Head evaluation results. Impact of Base Decay Temperature (τ ) The parameter τ dictates the temporal horizon of the alignment. We evaluated τ {2, 4, 8, 16, 32} on the MSC dataset. Small τ (e.g., τ = 2): The model becomes \"myopic.\" While it achieves 56%+ win rates on immediate conflicts, it fails to maintain coherence across session boundaries, causing slight rise in PPL. Large τ (e.g., τ = 32): The mechanism degrades towards standard DPO. The \"Historical Inertia\" returns, and the win rate drops to 47%. Optimal τ = 8: Strikes the best balance between conflict resolution and context retention. C.5 Visualization of Alignment Stability To intuitively demonstrate the Alignment Tax, we visualize the training dynamics of validation perplexity (PPL) in Figure 2. Based on the starting checkpoint (P 22.1) and the final converged states reported in Table 1, we reconstructed the divergence trends. Table 14: Generation Quality Metrics τ WR (MSC) PPL (MSC) Behavior Analysis 2.0 4.0 8.0 (Ours) 16.0 32.0 56.0% 55.6% 55.4% 50.2% 47.6% 39.5 32.9 26.0 25.2 24.8 Aggressive Recency Balanced Optimal Trade-off Conservative Standard DPO"
        },
        {
            "title": "E Computational Efficiency Analysis",
            "content": "Since DZ-TA adds only static bias term to the attention logits, it introduces negligible overhead. Compared to standard DPO, DZ-TDPO increases training time by only 15.4% (due to the dynamic coefficient calculation and all-MiniLML6-v2 model). Peak VRAM usage remains identical as no new large matrices are introduced. While the training phase utilizes an auxiliary SBERT model to compute dynamic temperature schedules, this module is discarded after alignment. The inference process is purely self-contained. During inference, the DZ-TA bias can be precomputed or fused into the positional encoding kernel. Thus, the token generation speed (tokens/sec) is statistically indistinguishable from the base model. Table 15: Efficiency Comparison Metric(1 epoch) Standard DPO DZ-TDPO Impact Training Time Peak VRAM Inference Speed 0.52 hours 68.9 GB 45.2 tok/s 0.6 hours 69.1 GB 45.0 tok/s Negligible +15.4% +0.3% Figure 2: Illustration of Stability Dynamics. The plot visualizes the divergence in perplexity. Standard DPO (Red) exhibits characteristic distribution collapse, where the rigid historical constraints force the model to degrade its linguistic capabilities to satisfy the reward (P 102.3). In contrast, DZ-TDPO (Blue) maintains stable trajectory (P 26.0), confirming that our structural bias effectively decouples conflict resolution from general linguistic modeling. This visual comparison reinforces our core claim: purely optimization-based methods (Standard DPO) struggle to reconcile state updates with historical priors, leading to catastrophic forgetting of the language manifold. DZ-TDPOs DualZone design acts as stabilizer, allowing the model to learn the specific state update without sacrificing global coherence."
        },
        {
            "title": "Bias",
            "content": "To verify that the DZ-TA mechanism is data-driven, we visualized the effective attention mask before and after training. Our preliminary sensitivity analysis indicated potential optimal range around λ [0.6, 0.7]. However, to prioritize regularization and prevent lazy head overfitting, we conservatively fixed λ = 0.5. As shown in the visualization, even this baseline decay strength effectively suppresses outdated information. Figure 3: Visualization of the Structural Attention Bias. The plot compares the attention logit bias of the standard decay (λ = 0.5, gray dashed) versus hypothetical aggressive decay (λ = 0.68, red solid). The steeper slope of the hypothetical curve illustrates the stronger penalty required if we were to prioritize conflict resolution over stability; however, our fixed prior (λ = 0.5) strikes balanced trade-off, effectively suppressing outdated information on distant history to resolve temporal conflicts. LLM-as-a-Judge Evaluation Details To ensure reproducibility, we provide the detailed configuration for the automated evaluation using DeepSeek v3.2. To address position bias, our evaluation script employs randomized shuffling mechanism. For every sample pair (yours, ybase), we flip fair coin (p = 0.5) to determine which model is presented as Assistant or Assistant B. The final verdict is mapped back to the original model identities. The evaluation uses strict instruction-following prompt. As shown in Table 16, the prompt is designed to enforce impartiality and adherence to specific evaluation criteria. While DeepSeek-V3.2 demonstrates high correlation with human judgment, automated judges 24 may exhibit verbose bias (preferring longer answers) or safety bias (preferring refusals). Although we implemented position swapping to mitigate bias, subtle semantic nuances in State Updates might still be misinterpreted by the judge. Future work involves verifying these results with human annotation on subset of the data. Component System Prompt Task Description Evaluation Criteria Input Slots Output Requirement Content / Template You are helpful assistant that acts as an impartial judge to evaluate the quality of AI responses. want you to act as an impartial judge and evaluate the quality of the responses provided by two AI assistants to the user question displayed below. You should choose the assistant that follows the users instructions and answers the users question better. Your evaluation should consider factors such as usefulness, relevance, accuracy, depth, creativity, and level of detail. [User Question] {question} [Assistant A] {answer_a} [Assistant B] {answer_b} Output your decision in strict JSON format with two keys: reason and winner. The winner must be one of A, B, or Tie. Table 16: Evaluation Prompt Template. The structured prompt used for the automated judge. The input slots are dynamically populated with the randomized model outputs. G.1 Pilot Human Validation To validate the reliability of our automated judge (DeepSeek-V3.2), we conducted meta-evaluation on random subset of 51 samples. An expert annotator manually reviewed the preference pairs to establish gold standard. The analysis reveals high degree of alignment between the automated judge and the human expert. Specifically, DeepSeek-V3.2 achieved an accuracy of 88.2% and Cohens Kappa coefficient of 0.78, indicating \"substantial agreement\" (Landis and Koch, 1977). This confirms that the automated judge accurately captures the nuances of temporal state conflicts and serves as reliable proxy for human preference (Zheng et al., 2024) in our experiments. Furthermore, the confusion matrix  (Table 17)  reveals that the automated judge exhibits conservative tendency. In 3 cases where the human expert judged DZ-TDPO as the winner (Win), the model judged it as loss (Loss). Conversely, the model rarely \"hallucinates\" win (only 1 case where Human=Loss but Model=Win). This suggests that our SOTA results reported in the main paper are likely lower-bound estimates, as the automated judge is stringent in awarding wins. Table 17: Confusion Matrix of Human-Model Agreement. Rows represent the human expert labels; columns represent the automated judge (DeepSeek-V3.2). Labels: 0 (Loss), 1 (Tie), 2 (Win). Human Expert Loss (0) Automated Judge Tie (1) Win (2) Loss (0) Tie (1) Win (2) 17 1 3 0 2 1 1"
        }
    ],
    "affiliations": []
}