{
    "paper_title": "Pseudo2Real: Task Arithmetic for Pseudo-Label Correction in Automatic Speech Recognition",
    "authors": [
        "Yi-Cheng Lin",
        "Yu-Hsuan Li Liang",
        "Hsuan Su",
        "Tzu-Quan Lin",
        "Shang-Tse Chen",
        "Yun-Nung Chen",
        "Hung-yi Lee"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Robust ASR under domain shift is crucial because real-world systems encounter unseen accents and domains with limited labeled data. Although pseudo-labeling offers a practical workaround, it often introduces systematic, accent-specific errors that filtering fails to fix. We ask: How can we correct these recurring biases without target ground truth? We propose a simple parameter-space correction: in a source domain containing both real and pseudo-labeled data, two ASR models are fine-tuned from the same initialization, one on ground-truth labels and the other on pseudo-labels, and their weight difference forms a correction vector that captures pseudo-label biases. When applied to a pseudo-labeled target model, this vector enhances recognition, achieving up to a 35% relative Word Error Rate (WER) reduction on AfriSpeech-200 across ten African accents with the Whisper tiny model."
        },
        {
            "title": "Start",
            "content": "Pseudo2Real: Task Arithmetic for Pseudo-Label Correction in Automatic Speech Recognition Yi-Cheng Lin* Yu-Hsuan Li Liang Hsuan Su Tzu-Quan Lin Shang-Tse Chen Yun-Nung Chen Hung-yi Lee National Taiwan University, Taipei, Taiwan {f12942075, r14922013, hungyilee}@ntu.edu.tw 5 2 0 2 9 ] . e [ 1 7 4 0 8 0 . 0 1 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Robust ASR under domain shift is crucial because real-world systems encounter unseen accents and domains with limited labeled data. Although pseudo-labeling offers practical workaround, it often introduces systematic, accent-specific errors that filtering fails to fix. We ask: How can we correct these recurring biases without target ground truth? We propose simple parameter-space correction: in source domain containing both real and pseudolabeled data, two ASR models are fine-tuned from the same initialization, one on groundtruth labels and the other on pseudo-labels, and their weight difference forms correction vector that captures pseudo-label biases. When applied to pseudo-labeled target model, this vector enhances recognition, achieving up to 35% relative Word Error Rate (WER) reduction on AFRISPEECH-200 across ten African accents with the Whisper TINY model."
        },
        {
            "title": "Introduction",
            "content": "ASR technologies are increasingly deployed across various domains, including smart assistants, medical transcription, and emerging low-resource accents or languages (Zhang et al., 2023). However, when models encounter speech from new domains, labeled training data is often scarce or completely unavailable (Long Mai and Julie Carson-Berndsen, 2022; Damianos et al., 2025). Collecting highquality transcriptions in these settings is costly, time-consuming, and sometimes infeasible due to privacy or legal constraints (Bäckström, 2025; Shoemate et al., 2022). common approach is to generate pseudo-labels using existing ASR models (Likhomanenko et al., 2023; Moritz et al., 2021). However, pseudo-labels inherit the teacher models systematic biases, such as under-recognizing rare words, accent-driven substitutions, or domain-specific mis-segmentations *These authors contributed equally. Figure 1: Overview of Pseudo2Real. a) In the source domain, two ASR models are fine-tuned from the same pretrained initialization: one using ground-truth transcripts and one using pseudo-labels. Their parameter difference defines correction vector that captures systematic pseudo-labeling biases. b) In new target domain, this correction vector is added to pseudo-label fine-tuned model to produce corrected ASR that better aligns with real-label performance. Color semantics: green = source-domain (ground-truth) knowledge, orange = pseudo-label noise, and purple = target-domain knowledge. (Higuchi et al., 2022). When used for adaptation, these errors can accumulate and degrade real-world performance (Prakash et al., 2025), revealing the need for methods that automatically identify and correct structured pseudo-label errors without relying on target-domain ground truth. In this work, we ask: How can we mitigate systematic error patterns in ASR pseudo-labeling when no ground truth annotations are available in the target domain? Prior efforts address this challenge indirectly. Teacher-student self-training improves with confidence filtering and agreement checks (Flynn and Ragni, 2024; Kim and Lee, 2025), yet these strategies suppress noise without correcting the structured biases from the teacher. Iterative schedules such as Noisy Student (Chen et al., 2023) and moving-average teacher updates (Zhang et al., 2024) improve pseudo-labels but require multiple passes and careful tuning, and they still propagate the teachers recurring mistakes. We propose Pseudo2Real, parameter-space correction that operates without target labels, as depicted in Figure 1. In source domains that provide both real transcripts and pseudo-labels, we finetune two models from the same backbone and form correction vector as their difference. This vector captures systematic discrepancies introduced by pseudo-labeling. When adapting to new target domain, the model fine-tuned on pseudo-labeled audio is adjusted by adding scaled version of this vector, yielding corrected ASR system that better aligns with real-label performance. Our extensive experiments on the AfriSpeech-200 dataset demonstrate that Pseudo2Real achieves consistent gains across ten African accents and multiple Whisper model sizes, including up to 35% relative WER reduction on Whisper TINY. Our main contributions include: 1. We introduce Pseudo2Real, an effective parameter-space correction that mitigates systematic pseudo-label errors. 2. We extend it to Pseudo2Real-SC, which leverages speaker clustering to compute subgroupspecific correction vectors, thereby further enhancing robustness. 3. We demonstrate substantial performance improvements across accents and model scales, analyze the effect of scaling factors and the number of clusters, and provide insights into how structured pseudo-label biases can be corrected directly in parameter space."
        },
        {
            "title": "2.1 Pseudo-labeling for ASR unsupervised",
            "content": "domain adaptation Work in ASR widely adopts teacherstudent selftraining to exploit unlabeled target audio (Flynn and Ragni, 2024). first line of research uses strong teacher to generate pseudo-labels, then trains student on these labels. The quality of pseudo-labels can be improved through iterative self-training approaches such as Noisy Student (Park et al., 2020; Singh et al., 2023; Ahmad et al., 2024), where the teacher is repeatedly updated on its own pseudo-labels with noise injection and augmentation, yielding large relative WER gains in ASR adaptation. second line focuses on label quality control. Confidence filtering and agreement checks between models are used to downweight or discard unreliable segments before student training, which prevents error amplification in the target domain (Kim and Lee, 2025; Zhu et al., 2023; Likhomanenko et al., 2023). third line refines the teacher itself during adaptation (Rao et al., 2023). For example, KAIZEN updates the teacher as an exponential moving average of the student, yielding stronger pseudo-labels and improved unsupervised adaptation (Manohar et al., 2021). Our work differs in purpose and mechanism. Instead of only filtering or iterating on noisy pseudolabels, we learn reusable correction in parameter space. We construct vector that captures the discrepancy between models trained on synthetic and real speech in auxiliary domains, then add this vector to pseudo-label adapted model in new domain to mitigate systematic error patterns without any target labels. This complements prior pseudo-label pipelines and can be combined with confidence filtering or iterative self-training."
        },
        {
            "title": "2.2 Task arithmetic in speech",
            "content": "Recent works have explored task vectors (task arithmetic) (Ilharco et al., 2023) as means to transfer capabilities between models (Li et al., 2025; Lin et al., 2025a; Huang et al., 2024; RitterGutierrez et al., 2025a,b). In ASR, Task Vector Algebra shows that difference vectors between models trained on related settings can enable zeroshot domain adaptation and task analogy for lowresource scenarios (Ramesh et al., 2024). Extending this idea, (Kang et al., 2024) demonstrates that multilingual ASR can be controlled or composed across languages via simple vector addition or negation, while (Nagasawa et al., 2025) shows that combining task vectors from related high-resource languages improves low-resource ASR through crosslingual transfer. LoRS-Merging (Zhao et al., 2025) merges languageor task-specific deltas using lowrank and sparse decomposition to enhance multilingual ASR without retraining. Building on this paradigm, SYN2REAL (Su et al., 2024) defines vector between ASR models fine-tuned on authentic versus synthetic speech and applies it to bridge the gap in acoustic signal distributions. Our work adopts task arithmetic but targets different problem: mitigating systematic pseudo-label errors in the ASR domain adaptation. Unlike prior applications such as cross-lingual transfer, multidomain interpolation, or modality fusion, which focus on transferring general capabilities, our method directly addresses transcription biases arising from pseudo-labeled training data. Compared with SYN2REAL (Su et al., 2024), our work tackles fundamentally different setting and goal. SYN2REAL focuses on text-domain shifts by constructing task vector between ASR models fine-tuned on real and synthetic speech within the same text domain; our approach tackles acoustic domain shifts using correction vector derived from the difference between models trained on ground-truth and pseudo-labeled data within the same acoustic domain. Furthermore, while SYN2REAL relies on domain labels to ensemble task vectors, we propose an automatic subgroup clustering method that forms multiple correction vectors from discovered speaker groups, requiring no domain supervision in the source data."
        },
        {
            "title": "3.1 Problem Formulation",
            "content": "We study acoustic domain adaptation for ASR, focusing on accent as the primary axis of domain variation. Let the source domain Ds consist of paired speech and text (Ss, Ts), and let the target domain Dt provide only unlabeled speech St. Ground-truth transcriptions Tt are unavailable due to annotation cost. common strategy is to train teacher ASR model on Ds, generate pseudo-labels ˆTt for St, and then train student ASR model on (St, ˆTt). This approach is effective but suffers from systematic error propagation: if the teacher consistently misrecognizes rare words or accent-specific patterns, these biases are inherited by the student. Confidence filtering or re-weighting pseudo-labels can reduce noise, but cannot correct the structured error patterns that arise from teacher model biases. Our goal is therefore to automatically detect and mitigate systematic pseudo-label errors without ground-truth annotations in Dt."
        },
        {
            "title": "3.2 Pseudo2Real",
            "content": "We build on task arithmetic to design parameterspace correction transferable across domains. The key observation is that in the source domain where both real labels and pseudo-labels are available, one can learn transformation that captures the discrepancy between models trained on the two label types. This discrepancy encodes the systematic biases of pseudo-labels in that domain. (or θpseudo Figure 2: Learning and applying correction vectors in parameter space. a) task vector is obtained by taking the difference between pretrained model θpre and its fine-tuned version θreal ). b) In the source domain, two models are fine-tuned from the same pretrained initialization θpre: one with real transcripts (θreal ) and one with pseudo-labels (θpseudo ). Their difference defines the correction vector τ . In new target domain, we first obtain θpseudo by fine-tuning on pseudo-labels, then apply the correction vector to yield the final model θcorrected . Pseudo2Real: Single correction Vector. Starting from the same pre-trained backbone θpre, we fine-tune two student ASR models on the source do- , trained on (Ss, Ts), and θpseudo main: θreal , trained on (Ss, ˆTs) where ˆTs are pseudo-labels generated by teacher for Ss. The difference between these models defines correction vector: τ = θreal θpseudo . (1) To adapt to the target domain, we fine-tune student model on (St, ˆTt) to obtain θpseudo , and then apply the correction vector: θcorrected = θpseudo + λτ, (2) where λ is scaling factor tuned on source-domain development data. This method applies single correction vector derived from the source domain directly to the target model. Pseudo2Real-SC: Subgroup Correction Vectors. The second variant extends this idea by recognizing that systematic pseudo-label errors may not be homogeneous across all speakers in the source domain. In practice, pseudo-labeling quality can vary substantially due to accent, pronunciation style, or recording conditions. For example, teacher model may systematically substitute certain consonants for speakers with specific accent, while producing relatively accurate transcriptions for speakers from another subgroup. If all speakers are pooled together when constructing the correction vector, these fine-grained biases may not be taken into account, weakening the correction signal. To address this, we propose partitioning the source domain into more coherent speaker subgroups and computing subgroup-specific correction vectors. Inspired by (Lin et al., 2025b), we refine the correction by exploiting speaker diversity within the source domain. We use ECAPA-TDNN1 embeddings (Desplanques et al., 2020) to extract speaker representations for each utterance, and apply k-means clustering (MacQueen, 1967) to partition the source data into speaker subgroups. For each subgroup c, we fine-tune two models from θpre: θreal s,c , trained on real transcriptions of the subgroup, and θpseudo , trained on pseudo-labels of the subgroup. We then compute subgroup-specific correction vector: s,c τc = θreal s,c θpseudo s,c . (3) The final correction is obtained by averaging across all clusters and applying it to the target model: θcorrected = θpseudo + λ (cid:88) c=1 τc. (4) This aggregated vector captures systematic pseudolabel biases shared across speakers while preserving accentor subgroup-specific corrections."
        },
        {
            "title": "4 Experimental Setup",
            "content": "Dataset. We evaluate our method on the Afrocentric benchmark AFRISPEECH-2002 (Olatunji et al., 2023), 200-hour corpus of transcribed English speech from speakers representing 120 African accents, with explicit accent annotations. Accented speech remains persistent challenge for ASR systems because strong accent variations often fall outside the distribution of large-scale pretraining corpora. AFRISPEECH-200 therefore provides rigorous testbed for domain adaptation methods. For our experiments, we filter the corpus by accent and select the ten accents with the largest number of samples. Not all accents include complete train, development, and test splits, so we restrict our selection to accents where all three splits are available and preserve the official split to avoid data leakage. 1https://huggingface.co/speechbrain/spkrec-ecapa-voxceleb 2https://huggingface.co/datasets/intronhealth/ afrispeech-200 Cross-Fold Validation. To evaluate the generalization ability of our method across diverse accents, we construct cross-fold validation setting based on the ten most represented accents in AFRISPEECH-200. We divide these accents into two folds with similar number of utterances: fold 1 consists of {Igbo, Swahili, Hausa, Zulu, Twi}, and fold 2 consists of {Yoruba, Ijaw, Afrikaans, Idoma, Setswana}. In each experiment, one fold serves as the source domain and the other as the target. The source fold provides paired speech and transcripts that are used to derive correction vectors, while the target fold provides only speech for pseudo-labeling. We then swap the roles of the folds to form the second validation round. This design ensures that the evaluation covers accents with different phonological and prosodic characteristics. Model. We employ the Whisper family of models (Radford et al., 2023), which cover wide range of capacities. Specifically, we experiment with Whisper TINY, BASE, SMALL, MEDIUM, and LARGEV2. These models share the same encoderdecoder transformer architecture but differ in scale, ranging from 39M to 1.55B parameters, detailed in Table 5. All models are pre-trained on approximately 680k hours of weakly supervised speech and are widely adopted in both research and real-world applications (Yang et al., 2024; Wu et al., 2024; Luo et al., 2025; Lin et al., 2024). Despite its scale and multilingual coverage, prior work has shown that its performance still degrades substantially when facing strong accent variation or domain-specific shifts (Graham and Roll, 2024). Evaluating across the full model series allows us to assess the effectiveness of our correction method under both low-capacity and high-capacity regimes. Training. In our experiments, we fully fine-tune the Whisper SMALL and TINY models as student models using the AdamW optimizer (Loshchilov and Hutter, 2019) with learning rate of 3 105 and weight decay of 0.1. Training is conducted for up to 40K update steps with linear warmup of 500 steps. Each model is trained with batch size of 16, using mixed-precision (FP16) to reduce memory consumption. Evaluation is performed every 50 steps using the word error rate (WER) metric with greedy decoding. For task arithmetic, we use the entire model parameter to compute the correction vector. The scaling factor λ is selected using the source-domain development sets: we perform simple grid search over λ {0.1, 0.2, 0.3, ..., 1.0}"
        },
        {
            "title": "Ijaw Afrikaans",
            "content": "Idoma Setswana Avg. Tiny Small θpre θreal θpseudo conf. ours topline θpre θreal θpseudo conf. ours topline 93.2 60.1 61.8 73.0 60.3 65.7 56.4 52.9 53.1 52.1 46.5 49. 77.7 67.3 70.5 58.7 51.9 45.8 49.6 39.7 39.9 43.0 42.3 35.7 142.8 135.3 119.6 147.7 78.8 54.1 60.6 73.8 58.2 78.3 78.0 52.5 79.0 70.8 75.2 56.0 67.0 59.7 49.7 40.7 41.1 42.5 38.5 36. 73.3 53.8 59.4 57.7 45.5 88.0 49.7 37.5 37.5 38.2 35.0 35.0 94.8 105.0 112.3 97.8 61.3 56.1 62.8 56.9 57.2 50.2 44.1 41.3 191.2 155.8 157.7 194.6 61.3 58.1 63.2 52.3 52.4 49.9 47.1 45. 54.4 51.8 52.5 52.0 45.4 41.4 41.3 36.0 36.4 36.2 32.4 29.5 182.9 128.2 129.0 107.2 60.7 112.4 66.0 55.6 55.8 55.8 48.9 47.7 75.6 54.4 55.0 41.9 44.8 44.8 49.7 40.3 40.7 41.9 37.1 36. 106.5 88.2 89.3 88.7 57.7 62.6 54.9 48.6 47.2 48.8 45.0 41.0 Table 1: WER (%) on ten accented English target domains. Results are shown for Whisper TINY and Whisper SMALL under six adaptation settings. Lower is better. The best performance is bolded. and choose the value that minimizes WER on the held-out source development set. All experiments are conducted on single NVIDIA V100 GPU, totaling approximately 500 GPU-hours. labeled data θpseudo provides noticeable gains, reducing the average WER by 17.2 points for TINY and 7.7 points for SMALL, but large error rates persist due to systematic biases in the pseudo-labels."
        },
        {
            "title": "5.1 Can Pseudo2Real improve ASR",
            "content": "performance? We begin by examining whether applying Pseudo2Real can improve ASR performance in the cross-fold validation setting. We consider four baselines. The first baseline is to directly use the pretrained student ASR model without any adaptation, denoted as θpre. The second baseline is the fine-tuned student ASR on the source domain ground-truth labeled data, θreal . The third baseline is the fine-tuned student ASR on the target domain pseudo-labeled data, θpseudo . The fourth baseline applies confidence-based filtering (conf.), where pseudo-labels used for fine-tuning are filtered according to the average log probability of each word. Thresholds are set at the first, second, and third quartiles of the confidence distribution. We select the threshold with the best performance on the development set. Also, the topline performance is reported where the target domain ground truth text is available. In our main experiments, we evaluate the case where the teacher model and the student model are identical. The results in Table 1 show that both baseline methods underperform significantly across all accent domains. The pretrained models θpre perform poorly, especially on low-resource accents such as Hausa, Ijaw, and Idoma, with average WERs of 106.5 and 54.9 for Whisper TINY and SMALL, respectively. Fine-tuning on pseudoIn contrast, Pseudo2Real achieves substantial improvements across all accents, demonstrating the effectiveness of parameter-space correction. For Whisper TINY, Pseudo2Real lowers the average WER from 89.3 to 57.7, representing 35% relative improvement over pseudo-label fine-tuning. For Whisper SMALL, the average WER decreases from 47.2 to 45.0, bringing performance much closer to the topline trained on labeled target data. Pseudo2Real shows particularly strong gains on challenging accents such as Ijaw, Idoma, and Yoruba, where WER is reduced by up to 50 points compared with pseudo-label fine-tuning. These improvements indicate that the correction vector effectively captures accent-specific pronunciation patterns and mitigates systematic pseudo-labeling errors that standard fine-tuning fails to address. Interestingly, Pseudo2Real occasionally outperforms the topline trained directly on labeled target data. For instance, the Whisper TINY model achieves lower WER than the topline on Idoma and Twi, while the Whisper SMALL model surpasses the topline on Igbo. This behavior suggests that parameter-space correction not only compensates for pseudo-label noise but also transfers beneficial regularities learned from other domains, such as better acoustic normalization and pronunciation consistency. In these cases, the correction vector serves as form of cross-domain regularization, enabling the adapted model to generalize more effectively than models trained solely on limited labeled target data."
        },
        {
            "title": "Zulu",
            "content": "θpre Base Small Medium Large Medium Large θpseudo Pseudo2Real Improvement (%) θpseudo Pseudo2Real Improvement (%) θpseudo Pseudo2Real Improvement (%) θpseudo Pseudo2Real Improvement (%) θpre θpseudo Pseudo2Real Improvement (%) θpseudo Pseudo2Real Improvement (%) 93.2 73.9 52.5 +29.0 59.88 52.58 +12. 58.44 65.19 -11.5 68.19 51.77 +24.1 56.4 46.79 40.31 +13.8 46.68 40.14 +14.0 77. 55.6 50.6 +9.0 50.44 49.15 +2.6 51.48 46.78 +9.1 54.45 43.26 +20.5 49.6 39.35 37.25 +5. 43.85 45.32 -3.4 Tiny Small 142.8 79.0 94.9 76.6 +19. 66.59 73.25 -10.0 72.54 64.66 +10.9 103.39 56.06 +45.8 82.8 66.1 +20.2 65.58 48.33 +26.3 49.81 47.42 +4. 50.09 48.44 +3."
        },
        {
            "title": "Twi",
            "content": "73.3 64.6 44.4 +31.3 49.66 44.24 +10.9 46.39 42.33 +8.8 50.00 42.89 +14.2 Avg. 93.20 74.36 58.04 +21.7 58.43 53.51 +8.4 55.73 53.28 +4.4 65.22 48.48 +21.6 60. 49.7 49.7 53.20 50.86 50.86 +0.0 55.02 65.33 -18.8 40.01 39.27 +1. 40.96 39.76 +2.9 36.68 34.76 +5.2 38.37 36.23 +5.6 42.74 40.49 +5.2 44.98 45.36 +0.1 Table 2: WER (%) on five accented English target domains from AFRISPEECH-200, under different teacher model sizes. Each Improvement row reports the relative improvement (%) of our Pseudo2Real method over pseudo-labeled fine-tuning. Green indicates gains (lower WER); red indicates degradation."
        },
        {
            "title": "5.2 How does Pseudo2Real perform across",
            "content": "different teacher model sizes? An important question is whether Pseudo2Real can generalize across different ASR model sizes rather than be tied to specific backbone. To examine this, we consider settings where the student and teacher may differ in capacity. In Table 2, we report results on five accents in fold 1. For the Whisper TINY student, the strongest average gains occur with BASE and LARGE teachers, yielding +21.7% and +21.6% relative improvements, respectively. The BASE teacher proinduces consistent reductions across accents, cluding Igbo (+29.0%) and Twi (+31.3%), while the LARGE teacher achieves the largest singleaccent gain on Hausa (+45.8%). Using SMALL or MEDIUM teachers leads to smaller average gains (+8.4% and +4.4%), with mixed outcomes such as degradation on Igbo for and MEDIUM teacher (-11.5%) but strong improvement on Zulu for SMALL teacher (+26.3%). For the Whisper SMALL student, improvements are more modest overall. Pairing with MEDIUM teacher yields +5.2% average relative improvement, with steady gains on nearly all accents. The LARGE teacher yields nearly identical performance to the baseline on average (around +0.1%), showTaken together, ing mixed results across accents. It improves on Igbo (+14.0%) and Setswana (+5.6%) but degrades notably on Hausa (-18.8%). these results show that Pseudo2Real is effective across range of teacher sizes, but the magnitude of improvement depends on the teacherstudent pairing and the target accent."
        },
        {
            "title": "5.3 What is the impact of the scaling factor λ?",
            "content": "We next examine how the scaling factor λ affects the magnitude of the applied correction vector. As described in Section 3.2, λ controls how strongly the Pseudo2Real vector influences the target model parameters. Figure 3 presents the relationship between WER and λ for five transfer settings involving different teacherstudent combinations, with λ ranging from 0.0 (no correction) to 0.5 (strong correction). The reported WER values are averaged over the same five accents used in Table 2. Across all transfer settings, we observe Ushaped trend. As λ increases from 0, WER first decreases, reaching its minimum around λ = 0.20.3, and then rises sharply for larger λ values. This pattern suggests that small scaling factors yields the best balance between correction strength and stability, whereas excessively large"
        },
        {
            "title": "Swahili Hausa Zulu",
            "content": "Medium Large Pseudo2Real Pseudo2Real-SC Improvement (%) Pseudo2Real Pseudo2Real-SC Improvement (%) 40.3 41.1 -2.0 40.1 40.3 -0. 37.3 37.3 0.0 45.3 40.8 +9.9 63.4 49.6 +21.8 65.3 53.9 +17.5 39.3 38.9 +1.0 39.8 39.2 +1."
        },
        {
            "title": "Twi",
            "content": "35.2 35.4 -0.6 36.2 35.4 +2.2 Avg. 43.10 40.46 +4.0 45.34 41.92 +6.1 Table 3: Comparison between Pseudo2Real and its subgroup clustering variant (SC). The Improvement row reports the relative change (%) of +SC over Pseudo2Real. Green indicates gains (lower WER), red indicates degradation. Lower is better. Tiny Small Base Tiny Large Tiny Tiny Tiny Medium Tiny confirm that the scaling factor λ plays critical role in balancing correction strength and model stability. 150 110 90 70 50 λ 0 0.1 0. 0.3 0.4 0.5 Scaling Factor λ Figure 3: WER vs. scaling factor (λ). Each curve corresponds to different teacherstudent pairing. Here, the arrow () denotes that pseudo-labels are generated by the teacher ASR model on the left and used to fine-tune the student model on the right (e.g., LARGETINY means pseudo-labels are produced by the LARGE teacher, and the TINY students parameters are then adjusted using the Pseudo2Real correction vector). Lower WER indicates better performance. λ values can lead to over-correction and degraded the TINYTINY and accuracy. For instance, TINYSMALL perform best at λ = 0.3, while excessive scaling beyond this point causes performance degradation. Similar behavior is observed in the BASETINY and MEDIUMTINY settings. However, the LARGETINY case shows greater instability at high λ, likely because the strong correction signal from large teacher is difficult for small student to absorb. Importantly, even small scaling factors (λ = 0.10.2) consistently improve performance compared to the uncorrected case (λ = 0.0). This demonstrates that applying mild parameter-space correction is generally beneficial and robust across different model configurations. Overall, the results"
        },
        {
            "title": "6.1 How is the Pseudo2Real-SC compared\nwith the simple correction vector?",
            "content": "We now investigate whether ensembling multiple correction vectors yields further improvements compared to using single correction vector. To quantify the benefit of ensembling, we evaluate two Whisper teacher sizes (large, medium) on the Whisper small student and compare the performance of Pseudo2Real with the ensemble variant in Table 3. We use 8 clusters for k-means in this experiment. The results show that Pseudo2Real-SC generally maintains or improves performance relative to the single correction vector, with the magnitude of gains depending on the teacher size and target accent. For the MEDIUM teacher, Pseudo2RealSC achieves an average 4.0% relative improvement, driven primarily by large gains on Hausa (+21.8%), while other accents remain stable. With the LARGE teacher, the ensemble variant produces stronger average gain of 6.1%, including notable improvements on Hausa (+17.5%) and Swahili (+9.9%). These results indicate that averaging subgroup-specific correction vectors can enhance robustness by capturing complementary correction patterns from diverse speaker groups. However, the improvements are not universal. For example, Igbo shows slight degradations when using both the LARGE and MEDIUM teachers, indicating that excessive averaging can weaken accentspecific corrections. Overall, Pseudo2Real-SC offers consistent improvement over the single correction vector, especially when the teacher has sufficient capacity to model heterogeneous speaker variations."
        },
        {
            "title": "Example and Description",
            "content": "Training set Ground Truth: The codes used for the four-needle telegraph are not known, and none of the equipment has survived. Teacher Pseudo-label: because used for the 4FN2 telegraph, Im not known command, and Im not the equipment as vif Error Type: Acoustic confusionthe teacher misinterprets the phonetic pattern of the word survived as the acoustically similar but meaningless phrase as vif. Testing set Ground Truth: If the child survives, he or she should be monitored for the later appearance of colonic polyps. Pretrained Tiny: if the child survives he or she should be monitored for the data appearance of colonical Tiny (Student): if the child is vice he or she should be monitored for the later appearance of colonic politics Tiny (Pseudo2Real): if the child survives he or she should be monitored for the later appearance of colonic politics Error Mitigation: Pseudo2Real restores the correct lexical meaning survives, correcting the acoustic corruption inherited from the teacher. Table 4: Qualitative examples showing how Pseudo2Real corrects systematic pseudo-label errors. Teacher errors (top) propagate to the student trained on pseudo-labels, while Pseudo2Real effectively suppresses these patterns and restores the intended meaning. Red = error; green = corrected token. 47 46 45 44 43 42 41 2 4 8 Number of K-means Clusters Figure 4: WER vs. number of K-means clusters for the large small setting. Increasing the number of clusters improves adaptation quality (lower WER)."
        },
        {
            "title": "6.2 Ablation on the number of clusters",
            "content": "We further analyze how the number of k-means clusters used in Pseudo2Real-SC affects adaptation performance. Figure 4 presents the results for the LARGESMALL transfer setting, where the number of clusters varies from 1 (no clustering) to 8. The WER values are averaged across the same five accents used in previous experiments. As the number of clusters increases, WER decreases. Using single cluster corresponds to the standard Pseudo2Real setting, yielding WER of 45.36. As increases to 2 and 4, the WER gradually decreases to 44.34 and 43.68, respectively, and reaches the lowest value of 41.94 at = 8. This trend suggests that finer clustering enables the model to capture more localized speakeror accentspecific correction patterns, resulting in improved generalization to target-domain speech. However, increasing the number of clusters raises computational cost, since two ASR models (real and pseudo) must be trained per cluster. In practice, moderate values such as = 48 provide good balance between performance and efficiency. Overall, the ablation shows that leveraging speaker diversity via clustering enhances the effectiveness of parameter-space correction in Pseudo2Real."
        },
        {
            "title": "7 Case study",
            "content": "To better understand how Pseudo2Real corrects systematic pseudo-labeling errors, we present qualitative example in Table 4 that compares transcriptions from the teacher, student, and corrected models against the ground truth. More case studies can be found in Table 7. The teacher produces nonsensical output (as vif), and the fine-tuned student inherits part of this lexical confusion (is vice). Pseudo2Real replaces the incorrect token with the correct verb survives, recovering the intended meaning while keeping the rest of the sentence intact. This indicates that the correction vector adjusts the models internal representations to reduce systematic substitution errors commonly found in pseudo-labeling."
        },
        {
            "title": "8 Conclusion",
            "content": "This work proposed Pseudo2Real, parameterspace correction method that mitigates systematic pseudo-label errors in ASR domain adaptation without requiring target-domain labels. Experiments on AFRISPEECH-200 across ten African accents and multiple Whisper sizes show consistent gains, achieving up to 35% relative WER reduction on Whisper TINY and occasionally surpassing topline models trained with true labels. We also introduced Pseudo2Real-SC, which yields additional improvements in several teacherstudent settings. Future work includes extending Pseudo2Real to multilingual and spontaneous speech settings, exploring the dynamic scaling of correction strength, and analyzing the interpretability of learned correction vectors."
        },
        {
            "title": "9 Limitation",
            "content": "Source domain supervision Our approach assumes access to at least one source domain with paired speech and ground-truth transcriptions in order to construct the correction vector(s). If the available source supervision is too small, unrepresentative, or collected under markedly different conditions, the estimated vector may underfit or encode mismatched biases, which can limit transfer to the target accents. Pseudo-label assumption The method relies on an implicit stationarity assumption: systematic biases that appear in pseudo-labels for the source domain are assumed to recur in the target domain. When teacher errors are highly accent-specific or driven by channel and recording conditions that do not overlap with the source, the correction may be weak or even counterproductive. Relatedly, we observed that the scaling factor λ must be tuned, and excessive scaling can degrade WER. Although we tune λ only on held-out source development data, this still introduces hyperparameter that may not transfer perfectly to new deployments. Language Our experiments focus on English accents within AFRISPEECH-200. Generalization to other languages and domains beyond reading or conversational speech is not validated here and remains future work. Accent Representation Our experiments focus on English accents within AFRISPEECH-200. We filtered to accents that provide complete train, development, and test splits to ensure fair protocol, but this choice may bias the evaluation toward better-represented accents and does not cover underrepresented varieties or code-switching scenarios. Generalization to other languages, domains beyond read or conversational speech, or far-field conditions is not validated here and remains future work. ological and does not involve the collection of new speech data or the deployment of real-world systems. Nevertheless, several potential risks and ethical considerations merit discussion. Bias and fairness. ASR systems often exhibit disparities in accuracy across accents, dialects, and demographic groups (Jahan et al., 2025; Ngueajio and Washington, 2022; Fuckner et al., 2023). While our method aims to mitigate such disparities by improving adaptation to underrepresented accents, it may also amplify biases present in the teacher models or source-domain data. We encourage practitioners to evaluate model fairness carefully across linguistic and demographic subgroups when applying this technique, and to accompany adaptation with representative validation datasets. Privacy and data use. Our experiments rely on publicly released corpora with consented speech recordings. No personally identifiable information or private data is used. However, adaptation methods in general could be misapplied to voice data collected without consent. Researchers and practitioners should ensure compliance with data protection regulations and obtain appropriate permissions before applying domain adaptation to sensitive speech. Dual use and misuse. The proposed parameterspace correction could, in principle, be used to enhance ASR systems deployed in surveillance or monitoring settings. Our intention is to support low-resource and accessibility-oriented speech technologies, rather than enabling intrusive applications."
        },
        {
            "title": "Acknowledgement",
            "content": "thank the National Center for HighWe performance Computing (NCHC) of the National Applied Research Laboratories (NARLabs) in Taiwan for providing computational and storage resources. We sincerely thank Dianna Yee, Colin Lea, and Ting-Yao Hu for their valuable feedback and insightful suggestions on this work."
        },
        {
            "title": "10 Ethical considerations",
            "content": "This work focuses on improving the robustness of ASR through parameter-space correction of pseudolabeling errors. The research is primarily methodRehan Ahmad, Muhammad Umar Farooq, and Thomas Hain. 2024. Progressive unsupervised domain adaptation for asr using ensemble models and multi-stage training. In ICASSP 2024 - 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 1146611470. Tom Bäckström. 2025. Privacy in speech technology. Preprint, arXiv:2305.05227. Yu Chen, Wen Ding, and Junjie Lai. 2023. Improving noisy student training on non-target domain data for In ICASSP 2023 - automatic speech recognition. 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 15. Dimitrios Damianos, Georgios Paraskevopoulos, and Alexandros Potamianos. 2025. MSDA: Combining Pseudo-labeling and Self-Supervision for Unsupervised Domain Adaptation in ASR. In Interspeech 2025, pages 38633867. Brecht Desplanques, Jenthe Thienpondt, and Kris Demuynck. 2020. Ecapa-tdnn: Emphasized channel attention, propagation and aggregation in tdnn based In Interspeech 2020, pages speaker verification. 38303834. Robert Flynn and Anton Ragni. 2024. Self-Train Before You Transcribe. In Interspeech 2024, pages 2840 2844. Marcio Fuckner, Sophie Horsman, Pascal Wiggers, and Iskaj Janssen. 2023. Uncovering bias in asr systems: Evaluating wav2vec2 and whisper for dutch speakers. In 2023 International Conference on Speech Technology and Human-Computer Dialogue (SpeD), pages 146151. Calbert Graham and Nathan Roll. 2024. Evaluating openais whisper asr: Performance analysis across diverse accents and speaker traits. JASA Express Letters, 4(2):025206. Yosuke Higuchi, Niko Moritz, Jonathan Le Roux, and Takaaki Hori. 2022. Momentum pseudo-labeling: Semi-supervised asr with continuously improving pseudo-labels. IEEE Journal of Selected Topics in Signal Processing, 16(6):14241438. Brandon Huang, Chancharik Mitra, Leonid Karlinsky, Assaf Arbelle, Trevor Darrell, and Roei Herzig. 2024. Multimodal task vectors enable many-shot multimodal in-context learning. In The Thirty-eighth Annual Conference on Neural Information Processing Systems. Gabriel Ilharco, Marco Tulio Ribeiro, Mitchell Wortsman, Ludwig Schmidt, Hannaneh Hajishirzi, and Ali Farhadi. 2023. Editing models with task arithmetic. In The Eleventh International Conference on Learning Representations. Maliha Jahan, Priyam Mazumdar, Thomas Thebaud, Mark Hasegawa-Johnson, Jesús Villalba, Najim Dehak, and Laureano Moro-Velazquez. 2025. Unveiling performance bias in asr systems: study on gender, age, accent, and more. In ICASSP 2025 - 2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 15. Ji-Hun Kang, Jae-Hong Lee, Mun-Hak Lee, and JoonHyuk Chang. 2024. Whisper Multilingual Downstream Task Tuning Using Task Vectors. In Interspeech 2024, pages 23852389. Eungbeom Kim and Kyogu Lee. 2025. Uncertaintyaware self-training for ctc-based automatic speech recognition. Proceedings of the AAAI Conference on Artificial Intelligence, 39(23):2433024338. Hongkang Li, Yihua Zhang, Shuai Zhang, Pin-Yu Chen, Sijia Liu, and Meng Wang. 2025. When is task vector provably effective for model editing? generalization analysis of nonlinear transformers. In The Thirteenth International Conference on Learning Representations. Tatiana Likhomanenko, Ronan Collobert, Navdeep Jaitly, and Samy Bengio. 2023. Continuous soft pseudo-labeling in asr. In Proceedings on \"I Cant Believe Its Not Better! - Understanding Deep Learning Through Empirical Falsification\" at NeurIPS 2022 Workshops, volume 187 of Proceedings of Machine Learning Research, pages 6684. PMLR. Tzu-Quan Lin, Wei-Ping Huang, Hao Tang, and Hung yi Lee. 2025a. Speech-ft: Merging pre-trained and fine-tuned speech representation models for crosstask generalization. Preprint, arXiv:2502.12672. Yi-Cheng Lin, Huang-Cheng Chou, and Hung yi Lee. 2025b. Mitigating Subgroup Disparities in MultiLabel Speech Emotion Recognition: PseudoLabeling and Unsupervised Learning Approach. In Interspeech 2025, pages 20532057. Yi-Cheng Lin, Tzu-Quan Lin, Chih-Kai Yang, Ke-Han Lu, Wei-Chih Chen, Chun-Yi Kuan, and Hung-Yi Lee. 2024. Listen and speak fairly: study on semantic gender bias in speech integrated large language models. In 2024 IEEE Spoken Language Technology Workshop (SLT), pages 439446. Long Mai and Julie Carson-Berndsen. 2022. Unsupervised domain adaptation for speech recognition with unsupervised error correction. In Interspeech 2022, pages 51205124. Ilya Loshchilov and Frank Hutter. 2019. Decoupled weight decay regularization. In International Conference on Learning Representations. Yu-Xiang Luo, Yi-Cheng Lin, Ming-To Chuang, JiaHung Chen, I-Ning Tsai, Pei Xing Kiew, Yueh-Hsuan Huang, Chien-Feng Liu, Yu-Chen Chen, Bo-Han Feng, Wenze Ren, and Hung yi Lee. 2025. ToxicTone: Mandarin Audio Dataset Annotated for Toxicity and Toxic Utterance Tonality. In Interspeech 2025, pages 40084012. MacQueen. 1967. Multivariate observations. In Proceedings ofthe 5th Berkeley Symposium on Mathematical Statisticsand Probability, volume 1, pages 281297. Vimal Manohar, Tatiana Likhomanenko, Qiantong Xu, Wei-Ning Hsu, Ronan Collobert, Yatharth Saraf, Geoffrey Zweig, and Abdelrahman Mohamed. 2021. Kaizen: Continuously improving teacher using exponential moving average for semi-supervised speech recognition. In 2021 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU), pages 518525. Niko Moritz, Takaaki Hori, and Jonathan Le Roux. 2021. Semi-supervised speech recognition via graph-based temporal classification. In ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 65486552. Haruki Nagasawa, Shinta Otake, and Shinji Iwata. 2025. Task vector arithmetic for low-resource asr. In ICASSP 2025 - 2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 15. Mikel K. Ngueajio and Gloria Washington. 2022. Hey asr system! why arent you more inclusive? In HCI International 2022 Late Breaking Papers: Interacting with eXtended Reality and Artificial Intelligence, pages 421440, Cham. Springer Nature Switzerland. Tobi Olatunji, Tejumade Afonja, Aditya Yadavalli, Chris Chinenye Emezue, Sahib Singh, Bonaventure F. P. Dossou, Joanne Osuchukwu, Salomey Osei, Atnafu Lambebo Tonja, Naome Etori, and Clinton Mbataku. 2023. Afrispeech-200: Pan-african accented speech dataset for clinical and general domain asr. Transactions of the Association for Computational Linguistics, 11:16691685. Daniel S. Park, Yu Zhang, Ye Jia, Wei Han, ChungCheng Chiu, Bo Li, Yonghui Wu, and Quoc V. Le. 2020. Improved noisy student training for automatic speech recognition. In Interspeech 2020, pages 2817 2821. Fabian Pedregosa, Gaël Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, Jake Vanderplas, Alexandre Passos, David Cournapeau, Matthieu Brucher, Matthieu Perrot, and Édouard Duchesnay. 2011. Scikit-learn: Machine learning in python. J. Mach. Learn. Res., 12(null):28252830. Jeena Prakash, Blessingh Kumar, Kadri Hacioglu, Bidisha Sharma, Sindhuja Gopalan, Malolan Chetlur, Shankar Venkatesan, and Andreas Stolcke. 2025. Better pseudo-labeling with multi-asr fusion and error correction by speechllm. Preprint, arXiv:2506.11089. Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine Mcleavey, and Ilya Sutskever. 2023. Robust speech recognition via large-scale weak supervision. In Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pages 2849228518. PMLR. Gowtham Ramesh, Kartik Audhkhasi, and Bhuvana Ramabhadran. 2024. Task vector algebra for asr models. In ICASSP 2024 - 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 1225612260. Milind Rao, Gopinath Chennupati, Gautam Tiwari, Anit Kumar Sahu, Anirudh Raju, Ariya Rastrow, and Jasha Droppo. 2023. Federated self-learning In with weak supervision for speech recognition. ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 15. Fabian Ritter-Gutierrez, Yi-Cheng Lin, Jui-Chiang Wei, Jeremy H.M Wong, Eng Siong Chng, Nancy F. Chen, and Hung yi Lee. 2025a. Distilling speech and music encoder with task arithmetic. In Interspeech 2025, pages 38583862. Fabian Ritter-Gutierrez, Yi-Cheng Lin, Jeremy H. Wong, Hung yi Lee, Eng Siong Chng, and Nancy F. Chen. 2025b. correlation-permutation approach for speech-music encoders model merging. Preprint, arXiv:2506.11403. Michael Shoemate, Kevin Jett, Ethan Cowan, Sean Colbath, James Honaker, and Prasanna Muthukumar. 2022. Sotto voce: Federated speech recognition with differential privacy guarantees. arXiv preprint arXiv:2207.07816. Satwinder Singh, Feng Hou, and Ruili Wang. 2023. novel self-training approach for low-resource speech recognition. In Interspeech 2023, pages 15881592. Hsuan Su, Hua Farn, Fan-Yun Sun, Shang-Tse Chen, and Hung-yi Lee. 2024. Task arithmetic can mitigate synthetic-to-real gap in automatic speech recognition. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 89058915, Miami, Florida, USA. Association for Computational Linguistics. Haibin Wu, Ho-Lam Chung, Yi-Cheng Lin, Yuan-Kuei Wu, Xuanjun Chen, Yu-Chi Pai, Hsiu-Hsuan Wang, Kai-Wei Chang, Alexander Liu, and Hung-yi Lee. 2024. Codec-SUPERB: An in-depth analysis of In Findings of the Associasound codec models. tion for Computational Linguistics: ACL 2024, pages 1033010348, Bangkok, Thailand. Association for Computational Linguistics. Chih-Kai Yang, Yu-Kuan Fu, Chen-An Li, Yi-Cheng Lin, Yu-Xiang Lin, Wei-Chih Chen, Ho Lam Chung, Chun-Yi Kuan, Wei-Ping Huang, Ke-Han Lu, TzuQuan Lin, Hsiu-Hsuan Wang, En-Pei Hu, Chan-Jan Hsu, Liang-Hsuan Tseng, I-Hsiang Chiu, Ulin Sanga, Xuanjun Chen, Po chun Hsu, and 2 others. 2024. Building taiwanese mandarin spoken language model: first attempt. Preprint, arXiv:2411.07111. Jun Zhang, Jingyue Wu, Yiyi Qiu, Aiguo Song, Weifeng Li, Xin Li, and Yecheng Liu. 2023. Intelligent speech technologies for transcription, disease diagnosis, and medical equipment interactive control in smart hospitals: review. Computers in Biology and Medicine, 153:106517. Kevin Zhang, Luka Chkhetiani, Francis McCann Ramirez, Yash Khare, Andrea Vanzo, Michael Liang, Sergio Ramirez Martin, Gabriel Oexle, Ruben Bousbib, Taufiquzzaman Peyash, Michael Nguyen, Dillon Pulliam, and Domenic Donato. 2024. Conformer-1: Robust asr via large-scale semisupervised bootstrapping. Preprint, arXiv:2404.07341. Qiuming Zhao, Guangzhi Sun, and Chao Zhang. 2025. Low-rank and sparse model merging for multilingual speech recognition and translation. Preprint, arXiv:2502.17380. Han Zhu, Dongji Gao, Gaofeng Cheng, Daniel Povey, Pengyuan Zhang, and Yonghong Yan. 2023. Alternative pseudo-labeling for semi-supervised automatic speech recognition. IEEE/ACM Trans. Audio, Speech and Lang. Proc., 31:33203330."
        },
        {
            "title": "A Model characteristic",
            "content": "Table 5 summarizes the key characteristics of the Whisper models used in our experiments. Parameter counts are approximate values reported by the official release."
        },
        {
            "title": "Model",
            "content": "Parameters (M) Encoder Decoder Tiny Base Small Medium Large v2 39 74 244 769 1550 4 6 12 24 32 4 6 12 24 32 Table 5: Whisper models used in our experiments. All models share the same encoderdecoder transformer architecture but differ in scale. Parameter counts are reported in millions. Encoder and Decoder are the number of transformer layers in the encoder and decoder, respectively. Data, Artifacts, and Licensing Dataset statistic Table 6 summarizes the number of samples for each language across the train, development, and test splits. Licenses and terms of use. All datasets and pretrained models used in this work are publicly available for research purposes under open licenses. We use the AFRISPEECH-200 corpus (Olatunji et al., 2023), which is distributed under Creative Commons Attribution NonCommercial ShareAlike v4.0 (CC BY-NC-SA 4.0) license. The Whisper models (Radford et al., 2023) are released by OpenAI under the MIT license. Our use of these resources fully complies with their stated terms and intended use for non-commercial academic research. No additional data scraping or private data collection was conducted. All artifacts associated with this work, including source code, trained correction vectors, fine-tuned model checkpoints, and documentation, will be released under the CC BY-NC-SA 4.0 license after acceptance. Intended use and compatibility. All artifacts are used within the scope of their original research purpose, which is speech recognition and domain adaptation studies. We do not deploy or fine-tune any model for commercial, surveillance, or identification applications. Derived models and results are intended solely for academic analysis and benchmarking. Any derived artifacts that we release will include clear documentation of intended use and"
        },
        {
            "title": "Train\nDev\nTest",
            "content": "8083 216 355 5480 313 521 5437 116 196 1306 310 175 1315 186 58 14369 361 2357 48 80 1911 82 54 1760 50 60 1273 215 97 Table 6: Number of samples per split for each accent that we used for our experiment."
        },
        {
            "title": "Example and Description",
            "content": "Example 1 Training set Ground Truth: Emmanuel Opuru said the suspects will face murder charge after investigation are complete. Teacher Pseudo-label: the man on the opposite side of the suspect with face, mother, child after investigation are complete. Error Type: Accent-induced lexical confusionteacher model mishears said as side due to strong accent variation. Testing set Ground Truth: It was great pleasure, an audience member said later. Pretrained Tiny: it was great page and audience will see the later Tiny (Student): it was great pleasure and audience will be side later Tiny (Pseudo2Real): it was great pleasure and audience member said later Error Mitigation: Pseudo2Real effectively suppresses the accent-induced error by restoring the correct lexical item (said), aligning the transcription with the intended semantic meaning. Example 2 Training set Ground Truth: In figure skating, sometimes women or men skate alone, or they skate in couples. Teacher Pseudo-label: In figure skating, sometimes women or men skate alone or they skate in couples full stop Error Type: Teacher hallucinationan extra full stop token is added. Testing set Ground Truth: In Nigeria, too, the May Day celebrations also happen. Pretrained Tiny: in nigeria 2 the media celebrations also happen 1st Tiny (Student): in nigeria 2 the medial celebrations also happen full stop Tiny (Pseudo2Real): in nigeria 2 the may day celebrations also happen Error Mitigation: Pseudo2Real removes the inherited hallucinated full stop and restores correct lexical content (May Day). Table 7: Examples showing how Pseudo2Real corrects systematic pseudo-label errors, including accent-induced confusion (Example 1) and hallucinated tokens (Example 2). Red = error; green = correct token. license terms to prevent misuse outside of research contexts. Anonymization and privacy protection. The AFRISPEECH-200 dataset contains anonymized speech recordings collected with participant consent. No personally identifiable information (PII) or metadata that could reveal speaker identity is used or released. We performed manual spotcheck and confirmed that no audio files or transcripts contain sensitive, offensive, or private information. All models were trained and evaluated locally on anonymized data, with no connection to external user data or APIs."
        },
        {
            "title": "C Use of AI assistants",
            "content": "This manuscript was refined with the assistance of large language models, which were used to improve clarity, grammar, and readability of the text. All conceptual development, experimental design, data analysis, and interpretation were conducted entirely by the authors. The AI assistants were not involved in generating research ideas or writing original scientific content. K-Means Implementation For the speaker clustering procedure used in the Pseudo2Real-SC variant, we employ the standard K-means algorithm from the scikit-learn library (Pedregosa et al., 2011) with default configuration. The initialization method is set to k-means++ to improve convergence speed and stability. The maximum number of iterations per run is fixed at 300, and the convergence tolerance is set to 104. The standard Lloyds algorithm is used as the clustering method."
        },
        {
            "title": "E Additional case study",
            "content": "To further illustrate how Pseudo2Real mitigates systematic pseudo-label errors, we provide more qualitative examples in Table 7. These cases highlight two common types of errors in teacher-generated pseudo-labels and show how Pseudo2Real effectively addresses them. Example 1 illustrates an accent-induced lexical confusion. Here, the teacher model mishears said as side due to strong accent variation in the training data, producing semantically inconsistent pseudo-labels. This error propagates to the student model, which reproduces the mistaken side in the testing example, leading to incorrect transcription. Pseudo2Real successfully corrects this accentinduced error by restoring the intended token said, aligning the transcription with the ground truth. Example 2 shows that the teacher model introduces an extraneous token (full stop) which does not exist in the ground-truth transcription. Such hallucinated tokens often arise from overconfident language modeling behavior and can propagate into the student model trained on these pseudo-labels. When the same error type appears in the testing example, the student model reproduces this pattern, again appending spurious full stop at the end. By contrast, Pseudo2Real completely removes the hallucinated full stop pattern, demonstrating that the erroneous token sequence no longer appears in the output. Moreover, it restores the correct lexical content (May Day) that aligns with the ground-truth transcription. This indicates that the parameter-space correction in Pseudo2Real not only suppresses inherited hallucinations but also reinforces meaningful acoustic-text alignment, leading to more faithful and semantically accurate transcriptions than direct pseudo-label fine-tuning."
        }
    ],
    "affiliations": [
        "National Taiwan University, Taipei, Taiwan"
    ]
}