{
    "paper_title": "APOLLO: SGD-like Memory, AdamW-level Performance",
    "authors": [
        "Hanqing Zhu",
        "Zhenyu Zhang",
        "Wenyan Cong",
        "Xi Liu",
        "Sem Park",
        "Vikas Chandra",
        "Bo Long",
        "David Z. Pan",
        "Zhangyang Wang",
        "Jinwon Lee"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large language models (LLMs) are notoriously memory-intensive during training, particularly with the popular AdamW optimizer. This memory burden necessitates using more or higher-end GPUs or reducing batch sizes, limiting training scalability and throughput. To address this, various memory-efficient optimizers have been proposed to reduce optimizer memory usage. However, they face critical challenges: (i) reliance on costly SVD operations; (ii) significant performance trade-offs compared to AdamW; and (iii) still substantial optimizer memory overhead to maintain competitive performance. In this work, we identify that AdamW's learning rate adaptation rule can be effectively coarsened as a structured learning rate update. Based on this insight, we propose Approximated Gradient Scaling for Memory-Efficient LLM Optimization (APOLLO), which approximates learning rate scaling using an auxiliary low-rank optimizer state based on pure random projection. This structured learning rate update rule makes APOLLO highly tolerant to further memory reductions while delivering comparable pre-training performance. Even its rank-1 variant, APOLLO-Mini, achieves superior pre-training performance compared to AdamW with SGD-level memory costs. Extensive experiments demonstrate that the APOLLO series performs on-par with or better than AdamW, while achieving greater memory savings by nearly eliminating the optimization states of AdamW. These savings provide significant system-level benefits: (1) Enhanced Throughput: 3x throughput on an 8xA100-80GB setup compared to AdamW by supporting 4x larger batch sizes. (2) Improved Model Scalability: Pre-training LLaMA-13B with naive DDP on A100-80GB GPUs without system-level optimizations. (3) Low-End GPU Friendly Pre-training: Pre-training LLaMA-7B on a single GPU using less than 12 GB of memory with weight quantization."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 6 ] . [ 1 0 7 2 5 0 . 2 1 4 2 : r APOLLO: SGD-LIKE MEMORY, ADAMW-LEVEL PERFORMANCE Hanqing Zhu * 1 2 Zhenyu Zhang * 1 Wenyan Cong 1 Xi Liu 2 Sem Park 2 Vikas Chandra 2 Bo Long 2 David Z. Pan 1 Zhangyang Wang 1 Jinwon Lee 2 ABSTRACT Large language models (LLMs) demonstrate remarkable capabilities but are notoriously memory-intensive during training, particularly with the popular AdamW optimizer. This memory burden often necessitates using more or higher-end GPUs or reducing batch sizes, limiting training scalability and throughput, respectively. To address this, various memory-efficient optimizers have been proposed to reduce optimizer memory usage. However, they face key challenges: (i) reliance on costly SVD operations (e.g., GaLore, Fira); (ii) significant performance trade-offs compared to AdamW (e.g., Flora); and (iii) still substantial memory overhead of optimization states in order to maintain competitive performance (e.g., 1/4 rank in GaLore, and full-rank first momentum in Adam-mini). In this work, we investigate the redundancy in AdamWs learning rate adaption rule and identify that it can be coarsened as structured learning rate update (channel-wise or tensor-wise). Based on this insight, we propose novel approach, Approximated Gradient Scaling for Memory Efficient LLM Optimization (APOLLO), which approximate the channel-wise learning rate scaling with an auxiliary low-rank optimizer state based on pure random projection. The structured learning rate update rule makes APOLLO highly tolerant to further memory reduction with lower rank, halving the rank while delivering similar pre-training performance. We further propose an extreme memory-efficient version, APOLLO-Mini, which utilizes tensor-wise scaling with only rank-1 auxiliary sub-space, achieving SGD-level memory cost but superior pre-training performance than Adam(W). We conduct extensive experiments across different model architectures and tasks, showing that APOLLO series performs generally on-par with, or even better than Adam(W). Meanwhile, APOLLO achieves even greater memory savings than GaLore, by almost eliminating the optimization states in AdamW. These savings translate into significant system benefits: (1) Enhanced Throughput: APOLLO and APOLLO-Mini achieve around 3 throughput on an 8A100-80GB setup compared to AdamW by fully utilizing memory to support 4 larger batch sizes. (2) Improved Model Scalability: APOLLO-Mini for the first time enables pre-training LLaMA-13B model with naive DDP on A100-80G without requiring other system-level optimizations. (3) Low-End GPU Friendly Pre-training: Combined with quantization, the APOLLO series for the first time enables the training of LLaMA-7B from scratch on single GPU using less than 12 GB of memory. Check the project page at APOLLO."
        },
        {
            "title": "INTRODUCTION",
            "content": "Large Language Models (LLMs) have achieved remarkable progress across various domains (Brown et al., 2020; Kocon et al., 2023; Dubey et al., 2024), largely due to substantial increases in model size, now reaching billions of parameters. Training these high-dimensional models demands robust optimization techniques, with the Adam(W) optimizer (Kingma & Ba, 2014; Loshchilov, 2017) emerging as *Equal contribution Co-advising to this work 1Department of Electrical and Computer Engineering, The University of Texas at Austin 2AI at Meta; Work was done during Hanqings internship at Meta. Correspondence to: David Z, Pan <dpan@ece.utexas.edu>, Zhangyang Wang <atlaswang@utexas.edu>, Jinwon Lee <jinwonl@meta.com>. Preprint. the de-facto standard for stabilizing LLM training (Zhang et al., 2024a) by tracking both first-order and second-order moments. Despite its effectiveness, Adam(W) incurs significant memory overhead, as maintaining both moments effectively triples the memory required relative to the models parameter size. This results in excessive memory consumption for the optimizer, even with single batch. For instance, training LLaMA-7B model from scratch requires at least 58 GB of memory, with 28 GB devoted to AdamWs optimizer states (Zhao et al., 2024). For larger models like GPT-3, with 175 billion parameters, memory demands reach 700 GB for the model alone, leading to staggering 1.4 TB requirement for AdamWs optimizer states. This excessive optimizer memory usage poses significant challenges in training large-scale LLMs. It compels the community to either use more and higher-end GPUs, or to APOLLO: SGD-like Memory, AdamW-level Performance Figure 1. (Left) Overview of our APOLLO optimizer; (Middle) Memory breakdown comparison for single batch size, where both GaLore and our method employ the layer-wise gradient update strategy (Lv et al., 2023). The (Q-) prefix indicates the integration of INT8 weight quantization, as utilized in (Zhang et al., 2024c); (Right) End-to-end training throughput on 8 A100-80GB GPUs. (SVD). Fira (Chen et al., 2024) enhances GaLore by incorporating the error residual between the full-rank gradient and its low-rank approximation, effectively simulating fullrank updates. LDAdam (Robert et al., 2024) also integrates generalized error feedback mechanism to explicitly account for the compression of gradient and optimizer states. However, the periodic updates to the gradient subspace via SVD (e.g., every 200 iterations) incurs computational cost of O(mn2), prohibitive when the matrix dimensions, and n, are large, as is the case with LLMs. For instance, in the case of the LLaMA-7B model, single subspace update can take approximately 10 minutes, whereas inference only takes seconds. This substantial overhead significantly reduces training throughput, as demonstrated in Fig. 7. Several GaLore variants, therefore, explore replacing the SVD projection with online PCA (Liang et al., 2024) or random projections (He et al., 2024), yielding provable convergence. In contrast, the recently proposed Adam-mini (Zhang et al., 2024b) demonstrates that block-wise second moment suffices for learning rate adjustments, offering an orthogonal and more efficient alternative. However, achieving performance on par with AdamW requires careful handling of different model components to maintain compatibility with its optimization dynamics. In this paper, we effectively integrate the two idea streams of low-rank approximation and optimizer state redundancy, introducing unified framework that achieves significant memory savings (below GaLore and its variants & close to SGD) while maintaining or surpassing the performance of Adam(W). Our key observation is that AdamWs elementwise learning rate update rule can be effectively restructured into channel-wise or even tensor-wise format, where each channel or tensor shares the same gradient scaling factor. To enable this structured gradient scaling, we introduce memory-efficient approximation for the scaling factors using an auxiliary optimizer state, requiring only lowerdimensional gradient information as input. This signifiFigure 2. Comparison of Validation perplexity on LLaMA-7B. reduce batch sizes. However, scaling training clusters introduce highly non-trivial communication and infrastructure overheads (Jiang et al., 2024); smaller batch sizes come at the cost of training throughput; and high-end GPUs are often inaccessible to researchers with limited resources. Significant research efforts have focused on addressing the high memory costs of training LLMs. One approach involves reducing the parameter volume through methods such as designing smaller-scale LLMs (Liu et al., 2024b; Tang et al., 2024), employing sparse model training (Liu et al., 2022; Thangarasa et al., 2023), and leveraging lowrank adaptation (Hu et al., 2021). Although these techniques effectively reduce memory usage, they restrict the optimization space of model parameters and often result in performance trade-offs (Biderman et al., 2024), particularly during pretraining (Lialin et al., 2023). Another avenue of research focuses on designing memoryefficient optimizers that reduce memory consumption while achieving performance on par with Adam(W). This includes exploring redundancy in optimizer states (Zhang et al., 2024b) and leveraging low-rank properties (Zhao et al., 2024; Chen et al., 2024). Among low-rank-based methods, GaLore (Zhao et al., 2024) stands out, enabling full-parameter training of LLMs by performing low-rank gradient updates through Singular Value Decomposition APOLLO: SGD-like Memory, AdamW-level Performance cantly reduces memory usage by leveraging compressed representation of the gradient information. Additionally, we eliminate the need for costly SVD-based low-rank projections by adopting an SVD-free method based on random projections. We show that much lower rank, or even rank-1 approximation, is sufficient to capture the structured gradient scaling factors effectively. This innovation allows for simpler and more efficient training process without compromising performance. Our new memory-efficient optimizer for LLM training, named Approximated Gradient Scaling for Memory Efficient LLM Optimization (APOLLO), not only achieves better performance than AdamW but also delivers greater memory savings compared to GaLore. Our key contributions are as follows: Structured Learning Rate Update for LLM Training: We show that structured learning rate updates, such as channel-wise or tensor-wise scaling, are sufficient for LLM training. This addresses redundancy in AdamWs element-wise learning rate update rule and reduces computational overhead. Approximated Channel-wise Gradient Scaling in Low-Rank Auxiliary Space (APOLLO): We propose practical and memory-efficient method to approximate channel-wise gradient scaling factors in an auxiliary low-rank space using pure random projections. APOLLO achieves superior performance to AdamW, even with lower-rank approximations, while maintaining excellent memory efficiency. Minimal-Rank Tensor-wise Gradient Scaling (APOLLO-Mini): For extreme memory efficiency, we introduce APOLLO-Mini, which applies tensorwise gradient scaling using only rank-1 auxiliary sub-space. APOLLO-Mini achieves SGD-level memory costs while outperforming AdamW, demonstrating the effectiveness of our approach. We demonstrate the efficacy of the APOLLO series in both In pre-training, pre-training and fine-tuning scenarios. across range of LLaMA model sizes from 60M to 7B parameters, APOLLO and APOLLO-Mini consistently outperform AdamW, achieving up to 2.8 reduction in validation perplexity while significantly reducing memory overhead by eliminating nearly all optimizer states. In fine-tuning, APOLLO and APOLLO-Mini achieve performance on par with full fine-tuning. Beyond these performance gains, the APOLLO series offers practical systemlevel advantages, including: (i) 3 better throughput on pre-training LLaMA 7B ( Fig.1 (right) and the 7B experiments in Fig.2); (2) Extreme training memory savings. By combining APOLLO-Mini with weight quantization, we set new record for memory efficiency: pre-training LLaMA 7B model requires only 12GB of memory (Fig. 1 (middle)). More information can be found in Section 5.3. These results establish APOLLO and APOLLO-Mini as highly efficient and scalable solutions for both pre-training and fine-tuning of LLMs, offering compelling improvements in performance, memory usage, and throughput."
        },
        {
            "title": "2 RELATED WORK",
            "content": "2.1 Algorithm-Level Memory-Efficient Training Numerous algorithmic improvements have been introduced to tackle the substantial memory overhead in training LLMs. One category focuses on reducing the number of trainable parameters to save memory costs. This includes approaches such as developing high-quality, small-scale models (Liu et al., 2024b; Tang et al., 2024), introducing sparsity during training (Liu et al., 2022; Thangarasa et al., 2023), and implementing low-rank adaptation (Hu et al., 2021). While these methods are effective at reducing memory usage, they often fall short in achieving comparable performance, especially in pre-training scenarios for large models. Another avenue of research targets advancements in optimizers, as exemplified by works such as GaLore (Zhao et al., 2024), Fira (Chen et al., 2024), Flora (Hao et al., 2024), Adam-mini (Zhang et al., 2024b), GaLore-mini (Huang et al.), LDAdam (Robert et al., 2024), GoLore (He et al., 2024), and LoQT (Loeschcke et al.). These approaches have made notable progress but still face significant challenges. Some methods rely on computationally expensive SVD operations (e.g., GaLore and Fira), although recent research shows that random projections can effectively compress gradients during later training stages while still requiring SVD early on (He et al., 2024). Others either exhibit noticeable performance gaps compared to AdamW, or demand substantial memory overhead to maintain competitive performance, as seen in GaLores 1/4 rank requirement and Adam-minis reliance on full-rank first momentum. In contrast, APOLLO achieves efficient memory usage entirely without relying on SVD while delivering performance that matches or even surpasses AdamW. Moreover, our extreme variant, APOLLO-Mini, drives memory costs down to SGD levels while maintaining or exceeding the performance of AdamW, setting new benchmark for memoryefficient optimization. 2.2 System-Level Memory Efficiency Optimization Several system-level techniques have been developed to reduce memory usage in LLM training (Chen et al., 2016; Ren et al., 2021). Activation checkpointing (Chen et al., 2016) recomputes activations during backward instead of storing them for the whole training iteration, reducing memory requirements. Quantization (Dettmers et al., 2024) reduces memory requirements by utilizing lower bit data formats. Memory offloading (Zhang et al., 2023; Ren et al., 2021) reduces GPU memory consumption by leveraging non-GPU APOLLO: SGD-like Memory, AdamW-level Performance memory. Our method, APOLLO, is orthogonal to these system-level optimizations and can be seamlessly integrated to achieve greater memory efficiency. Furthermore, by eliminating the need for SVD, APOLLO is more system-friendly, requiring only cheap general matrix multiplication to complete the projection step."
        },
        {
            "title": "RULE IS ENOUGH FOR LLMS",
            "content": "In this section, we first revisit the Adam(W) (Kingma & Ba, 2014; Loshchilov, 2017) and reformulate it as an adaptive learning rate algorithm without explicit momentum term (Section 3.1). Then, we propose that the element-wise learning rate update rule can be coarsened with structured channel-wise learning rate adaptation strategy, with even slightly better model performance by empirical verification. 3.1 Reformulating AdamW as Pure Adaptive Learning Rate Algorithm Vanilla AdamW update rule. AdamW has established itself as the go-to optimizer for Transformer training, leveraging both first moment (the mean of past gradients) and second moment (the variance of past gradients) to adjust updates. This dual momentum-based approach has proven superior to purely first-order optimizers like SGD (Zhang et al., 2024a). Disregarding weight decay, the vanilla AdamW update rule is as follows: At time step t, given weight matrix Rmn (m n) with gradient Gt = Wϕt(Wt), the standard AdamW update rule is defined as: Wt+1 = Wt η Gt, Gt = Mt Vt + ϵ (1) Here, η is the learning rate and ϵ is small constant for numerical stability. The first and second moment, Mt and Vt, are computed as exponentially weighted averages: Mt = β1Mt1 + (1 β1)Gt Vt = β2Vt1 + (1 β2)G2 where β1, β2 [0, 1) are the exponential decay rates. Viewing AdamW as an adaptive learning rate algorithm without momentum. The above update rule in equation 1 can then be reformulate as an element-wise gradient scaling rule with gradient scaling factor = Gt Rmn over Gt the raw gradient Gt, i.e., Wt+1 = Wt η Gt Gt Gt (2) In other words, the effectiveness of AdamW can be viewed as the result of variance-aware learning rate schedule per element in raw gradient Gt using the corresponding element in S, where elements with higher variance in Vt are scaled down to reduce unstable updates. While this reformulation is very straightforward, it paves the way for subsequent analysis. It also provides convenient strategy to analyze other momentum algorithms through SGD-like lens (e.g., all reduced to adaptive SGD learning rates). 3.2 Coarsening Element-wise Learning Rate Adjustment in Structured Manner While the element-wise learning rate update rule in AdamW is effective, it can be overly sensitive to noisy gradients in specific parameters, especially in high-dimensional models like large language models (LLMs). Recent work, such as Adam-mini (Zhang et al., 2024b), proposes to grouping parameters into blocks and applying block-wise learning rate adjustment to reduce memory usage while maintaining on-par performance as Adam. However, the block-wise approach in Adam-mini (Zhang et al., 2024b) requires carefully chosen block sizes for different modules in Transformers and only achieves memory savings for the second moments, leaving the first moment memory unaffected. more structured learning rate update rule. Inspired by these findings, we propose an effective simplification by coarsening the element-wise adaptive learning rate rule in equation 2 into structured channel-wise adaptation. We group parameters based on the larger dimension of the weight tensors. The element-wise scaling factor = Gt Gt is then simplified into channel-wise format, R1n, where each element sj for each channel is: sj = Gt[:, j]2 Gt[:, j] (3) where 2 denotes the ℓ2 norm. Then, the final gradient scaling rule becomes Gt = Gt = Gt diag(s). Empirical validation. We first empirically explore the effectiveness of the proposed update rule where we compared the training loss of the original element-wise learning rate adaptation with our proposed channel-wise adaptation on LLaMA-130M model. As shown in Figure 3, both approaches achieve similar loss reduction over training steps, demonstrating that the structured adaptation effectively maintains performance. In fact, the channel-wise adaptation achieves slightly better perplexity 24.43 (AdamW: 25.08), further supporting the effectiveness of our approach. However, we notice that our channel-wise learning rate adaption (orange curve) shows significant spike at the early stage, which is due to the unstable gradient at the early stage. Instead of applying the vanilla gradient clipping method, we APOLLO: SGD-like Memory, AdamW-level Performance use the Norm-growth Limiter (NL) in (Chen et al., 2024) to limit the consecutive gradient growth, as it is shown slightly more effective than gradient clipping: if Gt Gt1 > γ then Gt Gt Gt γ Gt1 (4) where γ is threshold to ensure that the rate of gradient growth remains controlled. This approach limits the magnitude of gradient norm increases, particularly for the unstable gradients in the early stages, thereby preventing loss spikes (green curve), leading to further better perplexity 24.11. We, by default, use the NL in our method and set γ = 1.01. Takeaways ①: structured learning rate update is sufficient for LLM training. This observation suggests that effective optimization can be achieved by applying adaptive learning rates at coarser granularity, such as channel-wise, rather than at the elementwise level. This insight forms the basis for the memoryefficient methods we propose in the next section. Figure 3. Training loss comparison between Element-wise and Channel-wise Learning Rate (LR) Adaptations with or without norm limiter (NL) on the LLaMA-130M model."
        },
        {
            "title": "4 APOLLO: APPROXIMATED GRADIENT\nSCALING FOR MEMORY EFFICIENT\nLLM OPTIMIZATION",
            "content": "From observation to practical Benefit. While coarsening gradient scaling factors is effective, it does not inherently reduce optimizer memory usage. Computing structured gradient scaling factors still requires access to the full optimizer states Mt and Vt, which consume significant memory. This brings us to critical question: Question ①: Can structured learning rate adaptation be converted into practical, memory-efficient optimization? Algorithm 1 AdamW with APOLLO/APOLLO-Mini Input: weight matrix Rmn with n. Step size η, scale factor α, decay rates {β1, β2}, weight decay λ, rank r, subspace update frequency . Initialize: 0 repeat # Step 1: Calculate gradient into low rank space. Gt Rmn Wϕt(Wt) if mod = 0 then Pt Nseed(0, 1/r) seed an independent new random seed end if Rt PtGt # Step 2: Obtain low rank optimization states, Mt, Vt. MR , VR AdamW(Rt, β1, β2, λ) /((cid:112)VR Rt MR + ϵ) # Step 3: Update weight in original space. if APOLLO then diag(sR = Rt[:,j]2 m) {sR 0 , sR } Rt[:,j] 1 , ..., sR else if APOLLO-Mini then sR {sR = Rt2 } Rt2 end if Wt Wt1 + η α GtS + 1 until convergence criteria met return WT 4.1.1 Approximating Gradient Scaling with an Auxiliary Low-Rank Space To address this challenge, we propose APOLLO, which approximates the channel-wise gradient scaling in compressed low-rank space rather than the original full-rank one, showing in Algorithm 1. Specifically, an auxiliary low-rank optimizer state is stored by taking the low-rank gradient Rt as input, which is computed as Rt = PtGt Rrn, using pre-defined projection matrix Pt Rrm. The auxiliary optimizer state will only maintain the low-rank version of the first and second moments as: MR = β1MR t1 + (1 β1)Rt VR = β2VR t1 + (1 β2)R2 and VR These low-rank moments, MR into lightweight, channel-wise scaling factors: , are then converted sR = Rt[:, j]2 Rt[:, j]2 , where Rt = MR (cid:112)VR + ϵ (5) 4.1 APOLLO: Approximate Structural Gradient Scaling for LLM Optimization In this way, APOLLO estimates the channel-wise gradient scaling factor with the auxiliary low-rank optimizer state, saving memory from 2mn to 2nr. We will show later that APOLLO: SGD-like Memory, AdamW-level Performance the coarse-grained channel-wise scaling makes APOLLO insensitive to low rank, unlike GaLore, which needs relatively high rank to retain performances (typically, one-quarter of the original dimension), leading to great memory saving. Details can be found at Section 5.4, A3. However, since APOLLO operates in compressed domain (i.e. low-rank space), key question remains: Question ②: Can the adaptive learning rate in the compressed space effectively approximate its behavior in the original space? Moreover, what type of low-rank projection method is ideal for this purpose? The default choice might be Singular Value Decomposition (SVD), as it captures the most informative components of the gradient. In fact, most existing low-rank optimizers for LLMs rely on SVD-based approximations to maintain accuracy, especially during pre-training. However, SVD is computationally expensive for large models and cannot be efficiently parallelized on GPUs, hindering the training process. Therefore, we pose the following question: First-order moment ratio bounding. We expand the computation formula of the first moment recursively, as: Mt = β1Mt1 + (1 β1)Gt = βt 1M0 + (1 β1) t1 (cid:88) βk 1 Gtk MR = β1MR k=0 t1 + (1 β1)Rt t1 (cid:88) 0 + (1 β1) = βt 1MR βk 1 Rtk (7) (8) where β1 < 1. k=0 We quantify the approximation error in the following theorem. Theorem 4.1. Approximated Channel-wise Momentum with bound for its ℓ2 norm: Gt Rmn is the full-rank gradient (m n). Let be matrix of shape Rrm where each element is independently sampled from standard Gaussian distribution in the variance of 1/r. With the projected gradient Rt = PGt, we have the projected gradient with bounded channel-wise first order moment. For any channel j, with probability at least 1 2 exp (cid:16) (cid:17) : rϵ2 8 Question ③: Do we still need costly SVD to construct our compressed space? (1 ϵ)Mt[:, j]2 MR [:, j]2 (1 + ϵ)Mt[:, j]2, 4.1.2 APOLLO Performs Well with Random Projection: SVD is Not Necessary. To answer the above questions, we first analyze the norm difference between the first moment MR in the compressed space and the original space, as well as similar results for the second state VR and Vt. We demonstrate that random projection can effectively bound the difference between the gradient scaling factor in the compact and original space in equation 6: Original space: sj = Compact space: sR = Gt[:, j] Gt[:, j] RR [:, j] Rt[:, j] , Gt = , RR = Mt Vt MR (cid:112)VR with all small ϵ in the denominators removed for simplicity. Generating random projection matrix. We generate the random projection matrix by sampling each element from standard Gaussian distribution. With high probability, projection using random Gaussian matrix largely preserves the scaled norm from the original space based on the JohnsonLindenstrauss lemma (JLT) (Freksen, 2021). . Proof : Please refer to Appendix A.1.3. Second-order moment ratio bounding. Similarly, the second-order moment state can be formulated as: Vt = (1 β2) t1 (cid:88) k=0 2 G2 βk tk VR = (1 β2) t1 (cid:88) k=0 2 R2 βk tk (6) where we assume V0 = 0 (common in most initialization). Theorem 4.2. Approximated channel-wise variance with bound for its ℓ1 norm: For any channel and time t, if 8 ϵ2 log (cid:18) 2t δ (cid:19) , then with probability at least 1 δ/2: (1 ϵ)Vt[:, j]1 VR [:, j]1 (1 + ϵ)Vt[:, j]1 , where Vt[:, j] and VR original and projected spaces, respectively. [:, j] are the second moments in the Proof : Please refer to Appendix A.1.4. APOLLO: SGD-like Memory, AdamW-level Performance Bounded update ratio sR/s Now, we can bound the difference between the gradient scaling factor in the compact original space based on the theorem 4.1 and theorem 4.2: Question ④: Can we further compress the optimizer state to SGD-level memory cost while matching or surpassing AdamWs performance? sR /sj = Rt[:, j] Rt[:, j] Gt[:, j] Gt[:, j] = Rt[:, j] Gt[:, j] Gt[:, j] Rt[:, j] For any channel j, with probability 1 δ: 1 ϵ 1 + ϵ (cid:114) sR sj 1 + ϵ 1 ϵ . (9) Proof : Please refer to Appendix A.1.5. Therefore, our APOLLO method is theoretically sound with random projection and validated by the empirical results presented in the following section. Additionally, APOLLO with SVD also performs well within our framework, yet incurs significant computational cost. We default to use random projection in APOLLO. The theorem suggests we should scale the gradient with the factor (cid:112) to ensure consistent behavior as AdamW with structured learning rate update. Hence, we add the gradient scale factor α in Algorithm 1. However, this gradient scale factor can be combined with the learning rate, therefore set to 1 by default for APOLLO. When the is too small compared to n, as in our APOLLO-Mini case, which uses rank-1 space, we specifically assign the scaling factor by using 128. Moreover, as suggested in GaLore, fixing the projection matrix is not ideal for high-dimensional LLM training; we also re-sample the projection matrix every step, which is effortlessly done by re-generating new seed, which is set to 200 by default. Take-away ②: APOLLO can approximate the structured learning rate adaption with only random projection. 4.2 APOLLO-Mini: Achieve Extreme Memory Efficiency with Rank-1 Space The rank plays crucial role in balancing memory efficiency and the quality of the approximated gradient scaling factor. Although the coarsening learning rate update rule provides high tolerance to relatively low rank, we show our APOLLO can reduce rank by half compared to GaLore with slight impact on perplexity. We still need to pay memory cost for the optimizer state. If we can relax the rank requirement to 1, then the optimizer state cost is totally negligible. However, simply setting rank to 1 in APOLLO using channel-wise gradient scaling doesnt work well due to rank-1 space sacrificing too much information. Details at Section 5.4 A2. This leads to our next question: To address this, we introduce an extremely memory-efficient APOLLO variant, called APOLLO-Mini, which coarsens the scaling factor into tensor-wise scaling factor to reduce variance during gradient scaling estimation in rank-1 space. The scaling factor is computed as: = Rt2 . MoreRt2 over, we find the tensor-wise scaling factor estimated by rank-1 space is typically smaller than the one estimated with larger rank, which can be theoretically justified by the theorem in equation 9. Hence, we heuristically set gradient scale factor α like GaLore to avoid performance degradation, default to set as 128. 4.3 Savings and cost analysis Table 1 provides detailed comparison of memory and computational trade-offs among various memory-efficient including APOLLO-Mini, APOLLO, training methods, Fira (Chen et al., 2024), GaLore (Zhao et al., 2024), and Flora (Hao et al., 2024). Notably, APOLLO can purely implemented with random projection, thereby avoiding the costly SVD operations required by Fira and GaLore. Table 1. Detailed comparison between Fira (Chen et al., 2024), GaLore (Zhao et al., 2024), Flora (Hao et al., 2024), APOLLO, and APOLLO-Mini. Denote Wt Rmn (m n), rank r. APOLLO series has constant 2 due to storing random seed and gradient norm used for norm-worth limiter. APOLLO-Mini APOLLO Fira GaLore Flora Weights Optimizer States Full-Rank Gradients Full-Rank Weights Pre-Training Fine-Tuning w.o. SVD mn 2n + 2 mn mn 2nr + 2 mr + 2nr + 1 mr + 2nr mn mn 2nr + In terms of memory efficiency, random projection allows APOLLO to eliminate the need to store projection matrix, achieving significant memory efficiency. Additionally, APOLLO is robust to rank reduction; halving the rank has minimal impact on pre-training performance (see Table 2). Our extreme variant, APOLLO-Mini, further maximizes memory efficiency by reducing optimizer state costs to const number 2n + 2, making it comparable to the memory footprint of SGD, yet it retains or even surpass AdamW-level performance, as confirmed in the following experiments."
        },
        {
            "title": "5 EXPERIMENTS",
            "content": "In Section 5.1 and 5.2, we systematically evaluate APOLLO on various pre-training and downstream tasks, respectively. Section 5.3 compares the memory overhead and throughput APOLLO: SGD-like Memory, AdamW-level Performance Table 2. Comparison of pretraining perplexity across various memory-efficient training approaches. We pretrain the LLaMA models with model size ranging from 60M to 1B on the C4 (Raffel et al., 2020) dataset and report the validation perplexity. The memory overhead focus solely on weights and optimization states. Results marked with are collected from (Zhao et al., 2024). By default, we set the rank to one-quarter of the original dimension for all low-rank-based training approaches, while results marked with indicate the use of halved rank, i.e., one-eighth of the original dimension."
        },
        {
            "title": "Methods",
            "content": "AdamW Low-Rank LoRA ReLoRA GaLore Fira APOLLO w. SVD APOLLO APOLLO APOLLO-Mini 60M Perplexity Memory 130M Perplexity Memory 350M Perplexity Memory 1B Perplexity Memory 34.06 78.18 34.99 37.04 34.88 31.06 31.26 31.55 31.26 31.93 0.36G 0.26G 0.36G 0.36G 0.24G 0.24G 0.24G 0.24G 0.18G 0.12G 25.08 45.51 33.92 29.37 25.36 22.73 22.84 22.94 23.18 23.84 0.76G 0.54G 0.80G 0.80G 0.52G 0.52G 0.52G 0.52G 0.39G 0.25G 18.80 37.41 25.58 29.08 18.95 17.03 16.67 16.85 16.98 17.18 2.06G 1.08G 1.76G 1.76G 1.22G 1.22G 1.22G 1.22G 0.95G 0.69G 15.56 142.53 19.21 18.33 15.64 14.31 14.10 14.20 14.25 14.17 7.80G 3.57G 6.17G 6.17G 4.38G 4.38G 4.38G 4.38G 3.49G 2.60G of different approaches, while Section 5.4 presents extensive ablation studies that analyze the impact of low-rank projection methods, rank quantity, scaling factor granularity, and provide detailed comparison of training curves. 5.1 Memory-Efficient Pre-training with APOLLO We demonstrate that APOLLO achieves superior pretraining performance across various sizes of LLaMA models (60M to 7B) on the C4 dataset (Raffel et al., 2020), with up to 2.80 reduction in validation perplexity. Additionally, APOLLO-Mini uses negligible memory budget for optimization states while outperforming both AdamW (Loshchilov, 2017) and GaLore (Zhao et al., 2024). Setup. We consider the LLaMA series models for pretraining, with sizes ranging from 60M to 7B. Following the training configurations used in prior works (Zhao et al., 2024; Lialin et al., 2023), we pre-train each model from scratch, with detailed description in Appendix.A.4. The C4 dataset (Raffel et al., 2020), comprehensive corpus derived from Common Crawl data and meticulously filtered and cleaned, is used for pre-training. All experiments are conducted in BF16 data format without other quantization. Baselines. For comparative analysis, we include the (i) AdamW: We following baselines in our evaluation: pre-train the models using the original AdamW optimizer (Loshchilov & Hutter, 2019), maintaining the weights, gradients, and optimizer states in their full-rank format. (ii) Low-Rank: This approach decomposes the model weights into two low-rank matrices (W = ), with both and optimized using AdamW. (iii) LoRA: LoRA (Hu et al., 2021) employs low-rank adapters for memory-efficient training by decomposing the original weights as = W0 + . During training, only and are optimized with AdamW, while the backbone weights W0 remain frozen. (iv) ReLoRA: ReLoRA (Lialin et al., 2023) is an enhanced version of LoRA specifically designed for pre-training, where the low-rank adapters are periodically merged back into the original weights during training. (v) GaLore: GaLore (Zhao et al., 2024) projects gradients, rather than weights, into low-rank space, effectively reducing memory consumption for optimizer states. (vi) Fira: Fira (Chen et al., 2024) further improves GaLore by incorporating the error residual of low-rank gradient into training. Main Results. We evaluate APOLLO and its two variants: APOLLO w. SVD, which replaces the original random projection with SVD; and APOLLO-Mini, which uses rank of 1 and computes the scaling factor in tensor-wise manner. The memory cost of optimization states in APOLLO-Mini is negligible, as the rank used is substantially smaller than the original dimension, e.g., 1 vs. 512 for LLaMA-60M and 1 vs. 2048 for LLaMA-1B. Results are reported in Table 2, from which several observations can be made: (i) Performance under the same memory budget: When the rank is set to one-quarter of the original dimension, APOLLO consistently outperforms GaLore, achieving up to 3.33 reduction in perplexity; (ii) Comparison with full-rank AdamW: APOLLO demonstrates superior performance while using significantly less memory. Notably, APOLLO-Mini incurs memory cost similar to that of SGD while significantly outperforming AdamW and vanilla SGD is known to fail on training transformer-based models (Zhang et al., 2024a); (iii) Robustness across projection methods and rank sizes: APOLLO exhibits robust performance under various subspace projection methods and rank sizes. For instance, with LLaMA-350M, enabling SVD APOLLO: SGD-like Memory, AdamW-level Performance Table 3. Pre-training LLaMA 7B on C4 dataset for 150K steps. Validation perplexity and memory estimate (optimization states only) are reported. Results marked with are collected from (Zhao et al., 2024). APOLLO uses the rank of 256, and APOLLO-Mini uses the rank of 1. Optimizer Memory 13G 4.9G 3.2G 0.0G 40K 80K 120K 150K 18.09 17.94 17.55 18.03 15.47 15.39 14.39 14.60 14.83 14.95 13.23 13.32 14.61 14.65 13.02 13.09 5. 10.5 15.7 19.7 8-bit Adam 8-bit GaLore APOLLO APOLLO-Mini Tokens (B) projection results in only 0.18 improvement of perplexity, while SVD is known for its time-consuming nature (Zhang et al., 2024c). This indicates that APOLLO can maintain efficiency even without SVD, improving end-to-end throughput. Additionally, halving the rank has negligible impact on APOLLO performance, further demonstrating its effectiveness across different rank budgets. Section 5.4 provide more thorough analysis of the impact of rank quantity and projection methods; (iv) Comparison with Fira: APOLLO is more favorable when dealing with larger models and more training tokens. For smaller models like 60M and 130M, Fira shows slightly better performance, but APOLLO consistently surpasses Fira as model size and training tokens increase. An in-depth comparison of training performance across model sizes and training tokens is provided in Section 5.4, A4. The above results validate the effectiveness of APOLLO on pre-training tasks, demonstrating that it achieves superior performance while requiring negligible memory costs for optimization states compared to AdamW. Scaling up to Pre-training LLaMA-7B. We evaluate the pre-training of LLaMA-7B model using AdamW, GaLore, and our APOLLO series (APOLLO (r = 256) and APOLLO-Mini (r = 1)) on an 8 A100 80GB setup. To ensure performance consistency, total batch size of 512 per epoch is maintained, while micro-batch sizes are adjusted based on the memory consumption of each method. AdamW is limited to micro-batch size of 4 due to memory constraints, whereas GaLore matches the memory usage of our methods with micro-batch size of 8. Considering the extended training duration typically required for fully training LLaMA-7B model with AdamW, we allocate fixed training time budget of around two weeks (15 Days) for fair and practical comparison. The training curve, showing validation perplexity recorded every 1000 steps, is presented in Fig. 2. Our experiments reveal two key benefits of the APOLLO series on the large 7B model: (i) Accelerated training through saved memory, enabling larger batch sizes. The significant memory efficiency of the APOLLO series allows for larger batch sizes, resulting in 3 faster training throughput compared to AdamW and 2 faster throughput compared to GaLore. Notably, our methods are the only ones to complete the pre-training within the half month timeframe. (ii) Superior model performance with best perplexity and reduced optimizer overhead. Despite its efficiency, APOLLO delivers better perplexity results than GaLore, even when GaLore employs high rank of 1024. Midway through training, APOLLO surpasses GaLore in performance, marking critical intersection point where our approach demonstrates clear superiority. Furthermore, as shown in Tab. 3, the APOLLO series achieves >1.5 perplexity improvement compared to the 8-bit versions of Adam and GaLore, all while maintaining significantly lower memory usage for optimizer states. These findings underscore that the APOLLO series accelerates training by optimizing memory usage, supports larger batch sizes for improved throughput, enhances model quality, and provides highly practical solution for pre-training large language models efficiently. 5.2 Memory-Efficient Fine-tuning with APOLLO Pre-training large foundation models typically demands thousands of GPUs and months of training, making it feasible only for large organizations. As result, fine-tuning these models has become more practical approach among engineers and researchers. Here, we thoroughly evaluate the performance of APOLLO in fine-tuning scenarios. Setup. We employ three open-source pre-trained models in the fine-tuning experiments, including LLaMA-3.2-1B, LLaMA-3-8B (AI@Meta, 2024), Gemma-7B and Mistral7B (Jiang et al., 2023). The downstream tasks are divided into two categories: (i) Eight common-sense reasoning tasks: Winogrande (WG)(Sakaguchi et al., 2021), PIQA(Bisk et al., 2020), SIQA (Sap et al., 2019), OpenBookQA (OBQA)(Mihaylov et al., 2018), HellaSwag (HS)(Zellers et al., 2019), BoolQ (Clark et al., 2019), and ARC (ARC-Easy and ARC-Challenge) (Clark et al., 2018); (ii) MMLU (Hendrycks et al., 2020) tasks across various domains: STEM, Social Sciences, Humanities and others. Baseline. We compare APOLLO against several baselines used during the pre-training experiments, including Fullrank AdamW (Loshchilov, 2017), LoRA (Hu et al., 2021), GaLore (Zhao et al., 2024), and Fira (Chen et al., 2024). Details of these baselines are summarized in Section 5.1. Additionally, we include DoRA (Liu et al., 2024a), an effective fine-tuning approach. We set the rank to 32 and 8 for common-sense reasoning and MMLU tasks, respectively. For MMLU, due to the rank being already small, like 8, we dont run APOLLO-Mini. APOLLO: SGD-like Memory, AdamW-level Performance Table 4. Comparison of various finetuning approaches on common-sense reasoning tasks. Experiments are conducted with Llama-3.2-1B based on the implementation from (Liu et al., 2024a). Methods AdamW LoRA DoRA GaLore Fira APOLLO w. SVD APOLLO APOLLO-Mini WG PIQA SIQA OBQA HS BoolQ Arc-E Arc-C Average 68.19 76.12 72.36 69.00 69.19 64. 72.22 55.12 67.56 68.98 62.75 71.82 70.88 70.40 67.64 63.28 74.70 72.63 77.20 77.69 76.93 76. 71.65 72.47 68.17 73.08 72.52 72.72 72.88 68.20 64.80 62.20 69.00 70.60 70.60 69.60 19.13 63.93 47.81 68.21 68.19 63.75 66. 63.58 64.01 58.99 64.31 63.00 62.69 64.22 67.30 69.32 68.94 73.40 74.03 73.40 72.98 52.99 52.82 47.61 54.78 55.72 55.20 55. 68.07 59.21 66.38 61.14 68.98 69.08 68.21 68.23 Table 5. Comparison results of various memory-efficient finetuning algorithms on MMLU tasks. For Galore, Fira, and APOLLO, we report the best accuracy obtained by sweeping the learning rate within the range [1e-5, 2.5e-5, 5e-5, 1e-4,1.5e-4, 2e-4]. Model Methods STEM Social Sciences Humanities Other Average LLaMA-3-8B Gemma-7B Mistral-7B Full LoRA GaLore Fira APOLLO w. SVD APOLLO Full LoRA GaLore Fira APOLLO w. SVD APOLLO Full LoRA GaLore Fira APOLLO w. SVD APOLLO 54.27 53.00 54.20 53.53 54.73 54.07 30.03 26.23 25.47 28.07 29.20 27.53 52.40 52.13 51.27 52.80 52.43 51. 75.66 74.85 75.37 75.46 75.46 75.36 37.16 34.94 33.21 35.30 40.42 36.97 72.95 72.46 72.99 72.85 73.28 72. 59.08 58.97 58.03 58.59 58.72 58.08 34.08 30.88 31.07 32.63 32.40 33.99 55.16 55.05 55.07 55.07 55.05 55. 72.80 72.34 72.34 72.09 72.68 71.93 35.47 36.96 33.71 35.97 38.94 36.40 69.05 68.77 69.30 69.11 69.24 69. 64.85 64.25 64.31 64.32 64.76 64.25 34.21 32.18 30.95 33.01 34.98 33.81 61.67 61.41 61.47 61.72 61.76 61. Main Results. As shown in Table 4 and Table 5, APOLLO consistently matches or outperforms other baselines, achieving up to an 1.01 average accuracy improvement over fullrank AdamW on common-sense reasoning tasks. Notably, compared to AdamW, APOLLO requires only one-quarter of the memory overhead for optimization states, while APOLLO-Mini uses rank of 1, resulting in negligible memory costs for optimization states. Despite this, both approaches deliver clear margin of accuracy improvement over AdamW on commonsense tasks while maintaining comparable performance on the MMLU tasks. 5.3 End-to-End System-level Benefits We further validate the system-level benefits, end-to-end throughput, and memory overhead by running LLaMA7B on 8 A100-80GB GPUs, comparing APOLLO and APOLLO-Mini against AdamW in Fig. 1. Higher throughput: APOLLO achieves significantly higher throughput than AdamW, particularly under limited hardware resources, as AdamW struggles with large batch sizes due to memory constraints. With APOLLO, we can scale the batch size up to 4 that of AdamW, resulting in up to 3 improvement in throughput. Additionally, the APOLLO series avoids the extra computational overhead associated with SVD that are known to be expensive. Much lower memory usage: With batch size of 4, Adam already reaches the memory limit with memory usage of 79 GB, while APOLLO and APOLLO-Mini require only 70 GB and 68 GB, respectively, for batch size of 16. This study highlights that using Adam not only results in high memory costs but also reduces training efficiency, as models become memory-bound, preventing full utilization of computational resources. By using APOLLO, we can effectively increase batch size, accelerating large-scale training with even better performance. APOLLO-Mini unlocks pre-training LLaMA-13B on A100 80GB without system-Level optimization: Leveraging the exceptional memory efficiency of APOLLO-Mini, we are the first to enable the pre-training of LLaMA-13B model A100 80GB GPU with naive DDP, without requiring other system-level optimizations, such as model sharding. This breakthrough not only simplifies deployment by reducing engineering complexity but also empowers researchers to scale up model sizes effortlessly with APOLLO-Mini. Combination with weight quantization unlocks pretraining LLaMA-7B under 12 GB: With our methods significantly reducing optimizer memory costs, the memory consumed by model weights becomes the next dominant factor. To further address this challenge, we integrate our approach with the Int8 weight quantization method proposed in Q-GaLore (Zhang et al., 2024c), enabling even greater memory savings. The results, detailed in Table 6, demonstrate that our Q-APOLLO series effectively miniAPOLLO: SGD-like Memory, AdamW-level Performance Table 6. Validation of pretraining perplexity of APOLLO-series combined with int-8 weight quantization strategy (Zhang et al., 2024c). APOLLO-series uses quantization group size of 128. are collected from (Zhao et al., 2024) and (Zhang et al., 2024c)."
        },
        {
            "title": "Methods",
            "content": "60M Perplexity Memory 130M Perplexity Memory 350M Perplexity Memory AdamW GaLore Q-GaLore APOLLO Q-APOLLO APOLLO-Mini Q-APOLLO-Mini 34.06 34.88 34.88 31.55 31.97 31.93 33. 0.36G 0.24G 0.18G 0.24G 0.18G 0.12G 0.06G 25.08 25.36 25.53 22.94 24.16 23.84 24.70 0.76G 0.52G 0.39G 0.52G 0.39G 0.25G 0.12G 18.80 18.95 19.79 16.85 18.79 17.18 18.90 2.06G 1.22G 0.88G 1.22G 0.88G 0.69G 0.35G mizes memory usage for both optimizer and weight components while maintaining on-par or better pre-training perplexity compared to full-precision AdamW. Moreover, Q-APOLLO achieves clear performance margin over QGaLore, underscoring its superiority in both memory efficiency and model quality. By successfully combining APOLLO-series with quantization, we enablefor the first timethe pre-training of LLaMA-7B model using just 12 GB of memory (QAPOLLO-Mini), assuming layer-wise gradient update strategy (Lv et al., 2023) is employed. This represents significant breakthrough in making LLM pre-training feasible on low-end GPUs, eliminating the dependency on high-end hardware traditionally required for LLM training. Our approach democratizes access to LLM pre-training, making it accessible to broader range of researchers and organizations with limited resources. 5.4 Extra Investigation and Ablation Study This section presents multiple experimental investigations, focusing on four key research questions: Q1: How can the scaling factor subspace be identified? Q2: Is APOLLO sensitive to the number of rank? Q3: What is an appropriate granularity for scaling factors? Q4: How do detailed comparisons evolve throughout the training process? Q5: How APOLLO performs in long-context training setting? A1: Similar performance between Random Projection (RP) and Singular Value Decomposition (SVD). Previous low-rank gradient-based approaches (Zhao et al., 2024) rely on SVD to identify the gradient subspace, frequently updated during training. This process is time-consuming, thereby affecting training throughput. For LLaMA-7B model, each SVD operation takes approximately ten minutes, resulting in an additional 25 hours of training time over 150K steps when the subspace is updated every 1,000 steps. To alleviate this overhead, (Zhang et al., 2024c) employs lazy subspace updating strategy, though it still incurs substantial SVD costs. In this section, we demonstrate that APOLLO performs effectively with random projection, significantly reducing the heavy SVD costs present in previous memory-efficient training algorithms. We pre-train LLaMA models ranging from 60M to 350M on the C4 dataset using GaLore, APOLLO, and APOLLO-Mini, reporting results for both SVD and random projection in each method. As shown in Figure 4 (a-c), GaLore is significantly impacted by random projection, failing to match the performance of AdamW (red dashed line). In contrast, both APOLLO and APOLLO-Mini demonstrate strong robustness with random projection, even slightly outperforming SVD in certain cases, such as APOLLO-Mini on LLaMA-350M. These results confirm the effectiveness of APOLLO under random projection, thereby addressing the throughput challenges present in previous low-rank gradient methods. A3: APOLLO-Mini remains effective even with rank of 1. We carry out an ablation study on pre-training LLaMA60M with different rank budgets, as shown in Figure 4 (d). The results demonstrate that GaLores performance degrades significantly as the rank decreases, matching fullrank AdamW only when the rank is set to 128 (one-quarter of the original dimension), which limits its effectiveness in extreme low-rank scenarios. In contrast, APOLLO exhibits much better robustness with smaller rank settings compared to both GaLore and Fira, achieving performance comparable to full-rank AdamW even with lower ranks. Interestingly, APOLLO-Mini shows the best rank efficiency, remaining effective even with rank of 1, clearly outperforming AdamW. By averaging the gradient scaling factor across different channels, APOLLO-Mini seems to effectively mitigates the errors introduced by low-rank projection. This capability allows APOLLO-Mini to achieve SGD level memory cost while reaching superior performance than AdamW. A3: The gradient scaling factor can even be calculated at the tensor level. In Table 7, we compare the pre-training APOLLO: SGD-like Memory, AdamW-level Performance Figure 4. (a-c) Comparison results of various optimization methods using singular value decomposition or random projection. The experiments were conducted on LLaMA-60M/130M/350M models for C4 pretraining tasks. (d) Validation perplexity with varying rank sizes, where 128 is one-quarter of the original model dimension. The red dashed line indicates the performance of full-rank AdamW. Table 7. Ablation study on the granularity of gradient scaling factors. Perplexity on the validation set is reported. Methods Granularity 60M 130M 350M AdamW GaLore APOLLO w. SVD APOLLO Channel Tensor Channel Tensor 34.06 34.88 31.26 31.77 31.55 32.10 25.08 25.36 22.84 23.86 22.94 23. 18.80 18.95 16.67 16.90 16.85 17.00 perplexity of our method using different scaling factor granularities. Here, Channel indicates that the gradient scaling factor is calculated along the channels with the smaller dimension of each layer, while Tensor denotes that single gradient scaling factor is used for each layer. We keep onequarter of the original model dimension as the rank. Across model sizes ranging from 60M to 350M, the perplexity difference between these granularities is minimal and both configurations outperform AdamW and GaLore. These results demonstrate that using tensor-wise scaling factor is sufficient for modest rank training (one-quarter of the original dimension). However, in extreme low-rank scenarios, tensor-wise scaling factor (APOLLO-Mini) outperforms channel-wise ones (APOLLO), as shown in Figure 4 (d). A4: APOLLO performs better with larger model sizes and more training tokens. Figure 5 illustrates the validation perplexity across the training process for LLaMA-350M models. In the early training stages, Fira shows faster convergence and lower perplexity. However, APOLLO gradually catches up, achieving improved performance in the later stages. This observation suggests that AdamW optimization states play more crucial role in the initial phase (as Fira maintains the low-rank format of these states), while compressing the optimization states into gradient scaling factors (as done in APOLLO) becomes more effective in later stages. Additionally, Figure 5 indicates that APOLLO seem to benefit from increased training tokens. To quantify Figure 5. Validation perplexity of pretraining LLaMA-350M on the C4 dataset, with zoomed-in figures showing early, middle, and late stages of training at top, with full training period at bottom. this effect, we pre-trained LLaMA-130M models for {20k, 30k} steps, with final perplexities for Fira and APOLLO reaching {22.73, 21.69} and {22.84, 21.71}, respectively, further confirming that APOLLO can gradually catch up Fira with more training tokens. Furthermore, Table 2 shows that as model size increases, APOLLO demonstrates better scaling capabilities than Fira: validation perplexity decreases from 31.55 to 14.20 when scaling model sizes from 60M to 1B, whereas Fira only improves from 31.06 to 14.31. Overall, APOLLO exhibits superior performance with both larger model sizes and additional training tokens. A5: APOLLO performs on par with or even better than AdamW in the long-context setting. Training LLM with long context windows is computationally expensive, but it is critical to enhance LLM performance by involving more contexts to reason. Here, we further validate the effectiveness of the APOLLO series on pre-training LLaMA-350m with long context window of 1024, four times over original GaLore uses. To establish strong baseline, we vary AdamWs learning rate across range of [1e-4, 2.5e-4, 5e4, 7.5e-4, 1e-3]. We also lazily tune the scale factor of APOLLO-series by varying APOLLOs in [ 3] and APOLLO-Minis in [ 384]. 128, 256, 2, 1, APOLLO: SGD-like Memory, AdamW-level Performance As shown in Fig. 6, both APOLLO and APOLLO-Mini demonstrate better performance than AdamW while achieving drastic reductions in optimizer memory costs1/8 or even 1/1024 of AdamWs memory usage. Note that our methods generally exhibit even better performance in the later stage with more training tokens involved, marking it promising candidate in partial LLM pre-training settings, i.e., long-context window and trillions of training tokens. Figure 6. Perplexity curves of the LLaMA-350M model trained in long-context window setting. APOLLO and APOLLO-Mini outperform AdamW with grid-searched learning rate, demonstrating the effectiveness of the APOLLO series in industrial LLM pre-training settings(long sequences and extensive training tokens)."
        },
        {
            "title": "6 CONCLUSION",
            "content": "In this paper, we introduced novel approach for training large language models (LLMs) that strikes an effective balance between memory efficiency and performance. Motivated by the limitations of existing methods like GaLore, which rely on memory-intensive SVD, and inspired by techniques like Fira and Adam-mini, we proposed two methods to achieve structured-wise gradient scaling. Our approach leverages low-rank optimizer states, using random projection only to preserve gradient norms, enabling efficient training without storing the full optimizer state. Extensive experiments across both pre-training and fine-tuning tasks demonstrate the effectiveness of our APOLLO, consistently surpassing the Adam baseline with greater memory saving than GaLore. APOLLO-Mini further squeeze the memory cost by using rank-1 sub-space, achieving better performance than Adam at the cost of SGD. Overall, our method offers promising solution to the memory bottlenecks in LLM training, providing an efficient alternative that maintains high performance while drastically reducing memory consumption."
        },
        {
            "title": "REFERENCES",
            "content": "AI@Meta. Llama 3 model card. URL https://github.com/meta-llama/llama3/ blob/main/MODEL_CARD.md. 2024. Biderman, D., Portes, J., Ortiz, J. J. G., Paul, M., Greengard, P., Jennings, C., King, D., Havens, S., Chiley, V., Frankle, J., et al. Lora learns less and forgets less. arXiv preprint arXiv:2405.09673, 2024. Bisk, Y., Zellers, R., Gao, J., Choi, Y., et al. Piqa: Reasoning about physical commonsense in natural language. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pp. 74327439, 2020. Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. Language models are few-shot learners. Advances in neural information processing systems, 33: 18771901, 2020. Chen, T., Xu, B., Zhang, C., and Guestrin, C. Training deep nets with sublinear memory cost. arXiv preprint arXiv:1604.06174, 2016. Chen, X., Feng, K., Li, C., Lai, X., Yue, X., Yuan, Y., and Wang, G. Fira: Can we achieve full-rank training of llms under low-rank constraint? arXiv preprint arXiv:2410.01623, 2024. Clark, C., Lee, K., Chang, M.-W., Kwiatkowski, T., Collins, M., and Toutanova, K. Boolq: Exploring the surprising difficulty of natural yes/no questions. arXiv preprint arXiv:1905.10044, 2019. Clark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal, A., Schoenick, C., and Tafjord, O. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018. Dettmers, T., Pagnoni, A., Holtzman, A., and Zettlemoyer, L. Qlora: Efficient finetuning of quantized llms. Advances in Neural Information Processing Systems, 36, 2024. Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur, A., Schelten, A., Yang, A., Fan, A., et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Freksen, C. B. An introduction to johnson-lindenstrauss transforms. arXiv preprint arXiv:2103.00564, 2021. Hao, Y., Cao, Y., and Mou, L. Flora: Low-rank adapters are secretly gradient compressors. In Forty-first International Conference on Machine Learning, 2024. URL https: //openreview.net/forum?id=uubBZKM99Y. APOLLO: SGD-like Memory, AdamW-level Performance He, Y., Li, P., Hu, Y., Chen, C., and Yuan, K. Subspace optimization for large language models with convergence guarantees. arXiv preprint arXiv:2410.11289, 2024. Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D., and Steinhardt, J. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020. Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., and Chen, W. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021. Huang, W., Zhang, Z., Zhang, Y., Luo, Z.-Q., Sun, R., and Wang, Z. Galore-mini: Low rank gradient learning with fewer learning rates. In NeurIPS 2024 Workshop on FineTuning in Modern Machine Learning: Principles and Scalability. Jiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C., Chaplot, D. S., Casas, D. d. l., Bressand, F., Lengyel, G., Lample, G., Saulnier, L., et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023. Jiang, Z., Lin, H., Zhong, Y., Huang, Q., Chen, Y., Zhang, Z., Peng, Y., Li, X., Xie, C., Nong, S., et al. {MegaScale}: Scaling large language model training to more than 10,000 {GPUs}. In 21st USENIX Symposium on Networked Systems Design and Implementation (NSDI 24), pp. 745760, 2024. Kingma, D. P. and Ba, J. Adam: method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. Kocon, J., Cichecki, I., Kaszyca, O., Kochanek, M., Szydło, D., Baran, J., Bielaniewicz, J., Gruza, M., Janz, A., Kanclerz, K., et al. Chatgpt: Jack of all trades, master of none. Information Fusion, 99:101861, 2023. Lialin, V., Muckatira, S., Shivagunde, N., and Rumshisky, A. Relora: High-rank training through low-rank updates. In Workshop on Advancing Neural Network Training: Computational Efficiency, Scalability, and Resource Optimization (WANT@ NeurIPS 2023), 2023. Liang, K., Liu, B., Chen, L., and Liu, Q. Memory-efficient llm training with online subspace descent. arXiv preprint arXiv:2408.12857, 2024. Liu, S., Chen, T., Chen, X., Chen, X., Xiao, Q., Wu, B., Karkkainen, T., Pechenizkiy, M., Mocanu, D., and Wang, Z. More convnets in the 2020s: Scaling up kernels beyond 51x51 using sparsity. arXiv preprint arXiv:2207.03620, 2022. Liu, S., Wang, C.-Y., Yin, H., Molchanov, P., Wang, Y.-C. F., Cheng, K.-T., and Chen, M.-H. DoRA: In FortyWeight-decomposed low-rank adaptation. first International Conference on Machine Learning, 2024a. URL https://openreview.net/forum? id=3d5CIRG1n2. Liu, Z., Zhao, C., Iandola, F., Lai, C., Tian, Y., Fedorov, I., Xiong, Y., Chang, E., Shi, Y., Krishnamoorthi, R., et al. Mobilellm: Optimizing sub-billion parameter language models for on-device use cases. arXiv preprint arXiv:2402.14905, 2024b. Loeschcke, S. B., Toftrup, M., Kastoryano, M., Belongie, S., and Snæbjarnarson, V. Loqt: Low-rank adapters for quantized pretraining. In The Thirty-eighth Annual Conference on Neural Information Processing Systems. Loshchilov, I. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. Loshchilov, I. and Hutter, F. Decoupled weight decay regIn International Conference on Learning ularization. Representations, 2019. Lv, K., Yang, Y., Liu, T., Gao, Q., Guo, Q., and Qiu, X. Full parameter fine-tuning for large language models with limited resources. arXiv preprint arXiv:2306.09782, 2023. Mihaylov, T., Clark, P., Khot, T., and Sabharwal, A. Can new dataset arXiv preprint suit of armor conduct electricity? for open book question answering. arXiv:1809.02789, 2018. Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P. J. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of machine learning research, 21 (140):167, 2020. Ren, J., Rajbhandari, S., Aminabadi, R. Y., Ruwase, O., Yang, S., Zhang, M., Li, D., and He, Y. {Zero-offload}: Democratizing {billion-scale} model training. In 2021 USENIX Annual Technical Conference (USENIX ATC 21), pp. 551564, 2021. Robert, T., Safaryan, M., Modoranu, I.-V., and Alistarh, D. Ldadam: Adaptive optimization from low-dimensional gradient statistics. arXiv preprint arXiv:2410.16103, 2024. Sakaguchi, K., Bras, R. L., Bhagavatula, C., and Choi, Y. Winogrande: An adversarial winograd schema challenge at scale. Communications of the ACM, 64(9):99106, 2021. Sap, M., Rashkin, H., Chen, D., LeBras, R., and Choi, Y. Socialiqa: Commonsense reasoning about social interactions. arXiv preprint arXiv:1904.09728, 2019. APOLLO: SGD-like Memory, AdamW-level Performance Tang, Y., Liu, F., Ni, Y., Tian, Y., Bai, Z., Hu, Y.-Q., Liu, S., Jui, S., Han, K., and Wang, Y. Rethinking optimization and architecture for tiny language models. arXiv preprint arXiv:2402.02791, 2024. Thangarasa, V., Gupta, A., Marshall, W., Li, T., Leong, K., DeCoste, D., Lie, S., and Saxena, S. Spdf: Sparse pretraining and dense fine-tuning for large language models. In Uncertainty in Artificial Intelligence, pp. 21342146. PMLR, 2023. Wainwright, M. J. Chapter 2: Tail bounds, 2015. https://www.stat.berkeley.edu/ URL mjwain/stat210b/Chap2_TailBounds_ Jan22_2015.pdf. Zellers, R., Holtzman, A., Bisk, Y., Farhadi, A., and Choi, Y. Hellaswag: Can machine really finish your sentence? arXiv preprint arXiv:1905.07830, 2019. Zhang, H., Zhou, Y., Xue, Y., Liu, Y., and Huang, J. G10: Enabling an efficient unified gpu memory and storage architecture with smart tensor migrations. In Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture, pp. 395410, 2023. Zhang, Y., Chen, C., Ding, T., Li, Z., Sun, R., and Luo, Z.- Q. Why transformers need adam: hessian perspective. arXiv preprint arXiv:2402.16788, 2024a. Zhang, Y., Chen, C., Li, Z., Ding, T., Wu, C., Ye, Y., Luo, Z.-Q., and Sun, R. Adam-mini: Use fewer learning rates to gain more. arXiv preprint arXiv:2406.16793, 2024b. Zhang, Z., Jaiswal, A., Yin, L., Liu, S., Zhao, J., Tian, Y., and Wang, Z. Q-galore: Quantized galore with int4 projection and layer-adaptive low-rank gradients. arXiv preprint arXiv:2407.08296, 2024c. Zhao, J., Zhang, Z., Chen, B., Wang, Z., Anandkumar, A., and Tian, Y. Galore: Memory-efficient LLM training by gradient low-rank projection. In Forty-first International Conference on Machine Learning, 2024. URL https: //openreview.net/forum?id=hYHsrKDiX7. Zheng, Y., Zhang, R., Zhang, J., Ye, Y., Luo, Z., Feng, Z., and Ma, Y. Llamafactory: Unified efficient finetuning of 100+ language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations), Bangkok, Thailand, 2024. Association for Computational Linguistics. URL http://arxiv.org/abs/2403. 13372. APOLLO: SGD-like Memory, AdamW-level Performance"
        },
        {
            "title": "A APPENDIX",
            "content": "A.1 Proof of Gradient Scaling Approximation in Random Projected Low-rank Space Theorem A.1 is proven by leveraging the properties of Gaussian random projections and the concentration inequality for the chi-squared distribution. A.1.1 Problem Statement Proof. The projected norm Px2 can be expressed as: Notations and Definitions: We first introduce the notations and definitions used in the proof: Let Gt Rmn denote the gradient matrix at iteration (m n). Let Rrm denote the random projection matrix where Pij (0, 1/r) i.i.d. Define Rt = PGt as the projected gradient. Let β1, β2 (0, 1) be exponential decay rates. Define Mt, Vt as first and second moments in the original space. Define MR jected space. , VR as first and second moments in proLet denote the total number of iterations. Let denote the number of channels. Assume zero initialization: M0 = MR 0 = 0, V0 = VR 0 = 0. indicates ℓ2 norm of vector. 1 indicates ℓ1 norm of vector. Problem: We aim to prove that gradient scaling factors sj and sR in the original and low-rank projected space have bound for their ratio sR /sj, Px2 = (cid:88) (cid:32) (cid:88) j=1 i=1 (cid:33)"
        },
        {
            "title": "Pjixi",
            "content": ". Rewriting this using the quadratic form, we have: Px2 = xPPx, where PP is symmetric matrix. To analyze Px2, consider the distribution of PP. Each entry of PP is given by: (PP)ij = (cid:88) k=1 PkiPkj. For = (diagonal entries), we have: (PP)ii = (cid:88) k=1 P2 ki, and since Pki (0, 1/r), P2 Therefore, (PP)ii 1 is the chi-squared distribution with degrees of freedom. For = (offdiagonal entries), the expectation is zero: ki Exponential(1/r). r, where χ χ2 E[(PP)ij] = 0, due to the independence of Pki and Pkj. The expected value of PP is therefore: E[PP] = Im, sR /sj = Gt[:, j] Gt[:, j] Rt[:, j] Rt[:, j] = Gt[:, j] Rt[:, j] Rt[:, j] Gt[:, j] where Im is the identity matrix. The expectation of Px2 is: where: and Rt = MR (cid:112)VR Gt = Mt Vt E[Px2] = xE[P P]x = xImx = x2. Now consider the concentration of Px2 around its expectation. Define the random variable: = rPx2 . A.1.2 Norm Preservation Under Random Projection Theorem A.1 (Norm Preservation). For any fixed vector Rm and random matrix Rrm where Pij (0, 1/r) i.i.d., the following holds with high probability: Since Pij entries are i.i.d., χ2 r. Using the moment generating function of χ2 r, the following concentration bounds can be derived using standard tail inequalities for subexponential random variables (Wainwright, 2015): Pr[(1ϵ)x2 Px2 (1+ϵ)x2] 12 exp (cid:18) rϵ2 8 (cid:19) . Pr (cid:18)(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) 1 (cid:12) (cid:12) (cid:19) (cid:18) ϵ 2 exp (cid:19) . rϵ2 8 APOLLO: SGD-like Memory, AdamW-level Performance Returning to Px2, we scale back: by factoring out of the summation. Px2 = Zx2 . Thus, with high probability: (1 ϵ)x2 Px2 (1 + ϵ)x2, and the probability of this event is at least: This implies the MR [:, j] can be viewed as projected version of Mt[:, j] into lower dimension with fixed over time t. Step 3: Properties of Random Projection By Theorem A.1, we have the norm of Mt[:, j] is preserved in high probability, (cid:18) 1 2 exp (cid:19) . rϵ2 8 (cid:18) Pr (1 ϵ)Mt[:, j]2 MR [:, j]2 (cid:19) (1 + ϵ)Mt[:, j]2 (10) A.1.3 First Moment Analysis Theorem A.2 (First Moment Preservation). For any channel j, with probability at least 1 2 exp (cid:16) (cid:17) : rϵ2 8 (1 ϵ)Mt[:, j]2 MR [:, j]2 (1 + ϵ)Mt[:, j]2, using fixed projection matrix Rrm over t. Proof. Our goal is to bound MR , j]. [:, j] in terms of Mt[: Step 1: Recursive Definitions of Mt[:, j] and MR [:, j]. The first moment Mt[:, j] in the original space is recursively defined as: Mt[:, j] = (1 β1) t1 (cid:88) k= βk 1 Gtk[:, j], where Gtk[:, j] Rm is the gradient at timestep k. The projected first moment MR [:, j] is similarly defined as: MR [:, j] = (1 β1) t1 (cid:88) k=0 βk 1 Rtk[:, j], (cid:18) 1 2 exp (cid:19) . rϵ2 8 Remark: Here, we assume the projection matrix is fixed over time step t. GaLore (Zhao et al., 2024) also derives their theorem with the same assumption. However, as acknowledged in GaLore, using the same projection matrix for the entire training may limit the directions in which the weights can grow. Therefore, empirically, as in GaLore, we can periodically resample over iterations to introduce new directions. Unlike GaLore, which uses time-consuming SVD-based updates, we can simply re-sample from the Gaussian distribution by changing the random seed. A.1.4 Second Moment Analysis Theorem A.3 (Second Moment Preservation). For any channel and time t, if 8 ϵ2 log (cid:18) 2t δ (cid:19) , then with probability at least 1 δ/2: where Rtk[:, j] = PGtk[:, j] Rr. (1 ϵ)Vt[:, j]1 VR [:, j]1 (1 + ϵ)Vt[:, j]1 Step 2: Projected First Moment in Lower Dimension. With random matrix Rrm where Pij (0, 1/r) i.i.d., we have the projected first moment in the low-rank space, MR [:, j] = (1 β1) = (1 β1) t1 (cid:88) k=0 t1 (cid:88) k=0 (cid:18) = (1 β1) = PMt[:, j] βk 1 Rtk[:, j] βk 1 PGtk[:, j] , (cid:19) βk 1 Gtk[:, j] t1 (cid:88) k=0 , where Vt[:, j] and VR original and projected spaces, respectively. [:, j] are the second moments in the Proof. Our goal is to show that the norm of the second moment Vt in the original space is preserved under projection to the lower-dimensional space. We proceed by analyzing the recursive definition of Vt and applying the results of Theorem A.2 on norm preservation. Step 1: Recursive Formulation of Vt The second moment Vt[:, j] for channel at iteration is defined recursively as: Vt[:, j] = β2Vt1[:, j] + (1 β2)(Gt[:, j])2 APOLLO: SGD-like Memory, AdamW-level Performance By expanding recursively, we can write Vt[:, j] as weighted sum of the squared gradients from all past iterations: Vt[:, j] = (1 β2) t1 (cid:88) k=0 2 (Gtk[:, j])2 βk Step 2: Projected Second Moment in Lower Dimension Similarly, in the projected space, the second moment VR [: , j] for channel at iteration is given by: VR [:, j] = β2VR t1[:, j] + (1 β2)(Rt[:, j])2 Similarly, we can obtain the lower bound, VR [:, j]1 = (1 β2) t1 (cid:88) k= 2 Rtk[:, j]2 βk (1 β2) t1 (cid:88) k=0 2 (1 ϵ)Gtk[:, j]2 = (1 ϵ)Vt[:, j]1 βk We obtain the following bounds for the ℓ1 norm of full projected second moment VR [:, j]: (1 ϵ)Vt[:, j]1 VR [:, j]1 (1 + ϵ)Vt[:, j]1 Expanding recursively, we have: . VR [:, j] = (1 β2) t1 (cid:88) k=0 2 (Rtk[:, j])2 βk Step 3: ℓ1 Norm of Channel-wise Second Moment. Then, we can obtain the ℓ1 norm of the second-moment term VR [:, j] VR [:, j]1 = (cid:88) (1 β2) i=1 t1 (cid:88) k=0 2 (Rtk[i, j])2, βk We can swap the summation order and have, VR [:, j]1 = (1 β2) = (1 β2) βk 2 (cid:88) i=1 (Rtk[i, j])2 βk 2 Rtk[:, j]2 t1 (cid:88) k=0 t1 (cid:88) k=0 Similarly, we can have Vt[:, j]1 = (1 β2) = (1 β2) (cid:88) (Gtk[i, j])2 βk 2 i=1 2 Gtk[:, j]2 βk t1 (cid:88) k=0 t1 (cid:88) k=0 Step 4: Constructing the Bounds for VR [:, j] By Theorem A.1, we know that for each k, the ℓ2 norm of the projected gradient Rtk[:, j] satisfies: (1ϵ)Gtk[:, j]2 Rtk[:, j]2 1+ϵGtk[:, j]2, with probability 1 2 exp(rϵ2/8). Therefore, VR [:, j]1 = (1 β2) t1 (cid:88) k=0 2 Rtk[:, j]2 βk Step 5: Probability of Success. To ensure the bound holds across all timesteps, we apply the union bound. For each k, the failure probability is 2 exp(rϵ2/8). Across timesteps, the total failure probability is: (cid:18) (cid:19) 2t exp . rϵ2 Set this total failure probability to δ/2, giving the condition: 8 ϵ2 log (cid:18) 2t δ (cid:19) . Remark: Here, the requirement that grows sublinearly as log(t) ensures that even for large t, the rank does not grow excessively. However, empirically, we find our method is not sensitive to rank selection; even rank of 256 is sufficient to train LLaMA 7B with 150k steps. This can be explained by recent Adam-mini (Zhang et al., 2024b) that the variance doesnt need to be precise, and blockwise approximation is enough, showing that the variance approximation error can be tolerated well. A.1.5 Main Result: Gradient Scaling Approximation Theorem A.4 (Main Result). For any channel j, with probability 1 δ: 1 ϵ 1 + ϵ (cid:114) sR sj 1 + ϵ 1 ϵ Proof. Express ratio: sR sj = Gt[:, j] Rt[:, j] Rt[:, j] Gt[:, j] Apply Theorem A.2 for the first part, we can obtain the error bound for the first part: (1 β2) t1 (cid:88) k=0 2 (1 + ϵ)Gtk[:, j]2 = (1 + ϵ)Vt[:, j]1 βk Gt[:, j] Rt[:, j] (cid:114) 1 [ 1 + ϵ (cid:114) 1 1 ϵ ] , APOLLO: SGD-like Memory, AdamW-level Performance For the second part, it is equal to Rt[:, j]2 Gt[:, j]2 = = ( MR VR ( Mt Vt )[:, j]2 )[:, j] (cid:80)r i=1( MR VR i=1( Mt Vt (cid:80)n )2[i, j] (11) )2[i, j] SGD with Momentum only If we handle SGD with Momentum only, where variance term above is non-existent, and can be simplified as Rt[:, j]2 Gt[:, j]2 = [:, j]2 MR Mt[:, j]2 We can easily apply Theorem A.3 for the first-moment term: 1 ϵ MR [:, j] Mt[:, j] 1 + ϵ where the final scaling factor is bounded, Rt[:, j] Gt[:, j] [ 1 ϵ, 1 + ϵ] AdamW AdamWs case is more tricky, as equation 11 involves the element-wise division and cannot easily separate the momentum and variance. However, recent works such as Adam-mini (Zhang et al., 2024b) and GaLore-mini (Huang et al.) find out that the variance term can be approximated as an average of block-wise (original full-rank space) or channel-wise (projected low-rank space). Given the ℓ1 norm of the variance term is bounded based on Theorem A.4, we take this assumption by replacing the variance term as the [:,j]1 average of variance vector, i.e., Vt[:,j]1 in equation 11. Then it is approximated as, and VR Remark: This contains the constant factor (cid:112) , suggesting we should scale the gradient to make sure it has consistent behavior as AdamW with structured learning rate update. This gradient scale factor can be combined with the learning rate. When the is too small compared to n, as in our APOLLO-Mini case, which uses rank-1 space, we specifically assign the scaling factor by using 128. Probability of Success: We now establish the probability of success. Both Theorem A.3 and Theorem A.4 rely on the same random projection matrix are derived from Theorem A.2 (norm preservation for random projections). Therefore, the probability of failure for both bounds is governed by the failure of Theorem A.2. For single timestep t, the failure probability of Theorem A.2 is: Pr(Theorem A.2 fails at timestep t) 2 exp (cid:18) (cid:19) . rϵ2 8 Across all timesteps, the total failure probability (union bound) is: Pr(Theorem A.2 fails for any timestep) 2t exp (cid:18) (cid:19) . rϵ2 8 Set this total failure probability to δ: (cid:18) 2t exp (cid:19) rϵ2 8 δ. Solving for r, we require: 8 ϵ2 log (cid:18) 2t δ (cid:19) . Rt[:, j]2 Gt[:, j] = (cid:80)r (cid:80)n VR [i,j]2 [:,j]1 i=1( MR i=1( Mt[i,j] Vt[:,j]1 ) ) This ensures that both Theorem A.3 and Theorem A.4 hold simultaneously with probability 1 δ, which together make Theorem A.5 hold. = ( ) Vt[:, j]1 VR [:, j]1 MR [:, j]2 Mt[:, j]2 Multiply inequalities from theorem A.3 and theorem A.4 with union bound probability 1 δ, we have the above term (cid:114) Rt[:, j] Gt[:, j] (cid:114) 1 ϵ 1 + ϵ , (cid:114) 1 + ϵ 1 ϵ ] [ Then, we have the bounded ratio, (cid:114) Gt[:, j] Rt[:, j] (cid:114) Rt[:, j] Gt[:, j] sR sj = 1 ϵ 1 + ϵ , 1 + ϵ 1 ϵ ] [ A.2 Training throughput of GaLore-type Optimizer on LLaMA-1B We further show the training throughput for Galore-type lowrank optimizer (Galore, Fira) in Fig. 7. At every 200 update step, they need to call SVD to update the projection matrix, leading to drastic drop in training throughput. Although Galore tries to amortize the cost by relaxing the update gap, the significantly high cost is hard to amortize fully as we still keep short update gap to keep performance; for example, to update the projection matrix for LLaMA 7B model needs 10 mins, while inference takes seconds. APOLLO: SGD-like Memory, AdamW-level Performance Figure 7. The training throughput of Galore-type low-rank optimizer with many spikes due to the expensive SVD operation every 200 steps. A.3 Detailed Pre-Training Setting This section provides an overview of the LLaMA architectures and the hyperparameters used during pre-training. To ensure fair comparison, we adopt the same settings as Zhao et al. (2024). Table 8 outlines the hyperparameters for the various LLaMA model sizes. Across all architectures, we use maximum sequence length of 256 and batch size of 131K tokens. Additionally, we apply learning rate warm-up over the first 10% of training steps, followed by cosine annealing schedule that gradually reduces the learning rate to 10% of its initial value. APOLLO runs using the same learning rate 0.01 and subspace change frequency of 200 without tuning, following the Galore open-sourced settings. The scale factor α is considered fractional learning rate, which is set to 1 by default in APOLLO for models with size of less than 1B, showing our method doesnt need too much tuning like Galore and Fira. On 1B-model, we set the high-rank APOLLO with α = (cid:112)1/2 and the high-rank APOLLO SVD with α = 0.4. As we find the scaling factor increases with the rank r, therefore we scale the gradient factor in APOLLO-Mini with setting α to 128. A.4 Detailed Fine-Tuning Setting A.4.1 Commonsense reasoning fine-tuning We use the implementation from (Liu et al., 2024a) with the chosen hyperparameters detailed in Table 9. A.4.2 MMLU fine-tuning We use the implementation from (Zheng et al., 2024). We use the rank as 8 and sweep the learning rate within the range [1e-5, 2.5e-5, 5e-5, 1e-4,1.5e-4, 2e-4] for GaLore, Fira, APOLLO to ensure faithful comparison. The full and LoRA results are from (Zhang et al., 2024c). APOLLO: SGD-like Memory, AdamW-level Performance Table 8. Hyper-parameters of LLaMA architectures for pre-training."
        },
        {
            "title": "Intermediate Heads Layers",
            "content": "Steps Data Amount (Tokens) 60M 130M 350M 1 7 512 768 1024 2048 4096 1376 2048 2736 5461 11008 8 12 16 24 32 8 12 24 32 10K 20K 60K 100K 150K"
        },
        {
            "title": "1.3 B\n2.6 B\n7.8 B\n13.1 B\n19.7 B",
            "content": "Table 9. Hyperparameter of Llama-3.2-1B on the commonsense reasoning tasks. Hyperparameters AdamW LoRA DoRA Galore APOLLO w.SVD APOLLO APOLLO-Mini Rank α scale Dropout LR LR Scheduler Batch size Warmup Steps Epochs Where - - - 32 64 - 32 64 - 32 - 0.25 Fira 32 - 0.25 32 - 1.0 [2e-5, 5e-5] 3e-4 3e-4 3e3e-4 3e-4 0.05 Linear 32 100 3 Q,K,V,Up,Down 32 - 5.0 3e1 - 128 3e-"
        }
    ],
    "affiliations": ["Department of Electrical and Computer Engineering, The University of Texas at Austin", "Meta"]
}