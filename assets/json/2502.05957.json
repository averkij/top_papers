{
    "paper_title": "MetaChain: A Fully-Automated and Zero-Code Framework for LLM Agents",
    "authors": [
        "Jiabin Tang",
        "Tianyu Fan",
        "Chao Huang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large Language Model (LLM) Agents have demonstrated remarkable capabilities in task automation and intelligent decision-making, driving the widespread adoption of agent development frameworks such as LangChain and AutoGen. However, these frameworks predominantly serve developers with extensive technical expertise - a significant limitation considering that only 0.03 % of the global population possesses the necessary programming skills. This stark accessibility gap raises a fundamental question: Can we enable everyone, regardless of technical background, to build their own LLM agents using natural language alone? To address this challenge, we introduce MetaChain-a Fully-Automated and highly Self-Developing framework that enables users to create and deploy LLM agents through Natural Language Alone. Operating as an autonomous Agent Operating System, MetaChain comprises four key components: i) Agentic System Utilities, ii) LLM-powered Actionable Engine, iii) Self-Managing File System, and iv) Self-Play Agent Customization module. This lightweight yet powerful system enables efficient and dynamic creation and modification of tools, agents, and workflows without coding requirements or manual intervention. Beyond its code-free agent development capabilities, MetaChain also serves as a versatile multi-agent system for General AI Assistants. Comprehensive evaluations on the GAIA benchmark demonstrate MetaChain's effectiveness in generalist multi-agent tasks, surpassing existing state-of-the-art methods. Furthermore, MetaChain's Retrieval-Augmented Generation (RAG)-related capabilities have shown consistently superior performance compared to many alternative LLM-based solutions."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 ] . [ 1 7 5 9 5 0 . 2 0 5 2 : r MetaChain: Fully-Automated and Zero-Code Framework for LLM Agents Jiabin Tang Tianyu Fan Chao Huang The University of Hong Kong {jiabintang77, tianyufan0504, chaohuang75}@gmail.com (cid:135) Source Code: https://github.com/HKUDS/MetaChain Figure 1: MetaChain stands out as new LLM Agent Framework that enables fully automated, zero-code development for complex task automation. Ranking #1 among open-source solutions on the GAIA benchmark, it delivers state-of-the-art RAG performance as general AI assistant. Its revolutionary approach democratizes AI development - allowing anyone, regardless of coding experience, to create and customize their own agents, tools, and workflows with ease."
        },
        {
            "title": "Abstract",
            "content": "Large Language Model (LLM) Agents have demonstrated remarkable capabilities in task automation and intelligent decision-making, driving the widespread adoption of agent development frameworks such as LangChain and AutoGen. However, these frameworks predominantly serve developers with extensive technical expertisea significant limitation considering that only 0.03% of the global population possesses the necessary programming skills. This stark accessibility gap raises fundamental question: Can we enable everyone, regardless of technical background, to build their own LLM agents using natural language alone? To address this challenge, we introduce MetaChain - Fully-Automated and highly Self-Developing framework that enables users to create and deploy LLM agents through Natural Language Alone. Operating as an autonomous Agent Operating System, MetaChain comprises four key components: i) Agentic System Utilities, ii) LLM-powered Actionable Engine, iii) Self-Managing File System, and iv) SelfPlay Agent Customization module. This lightweight yet powerful system enables efficient and dynamic creation and modification of tools, agents, and workflows without coding requirements or manual intervention. Beyond its code-free agent Chao Huang is the Corresponding Author. Preprint. Under review. development capabilities, MetaChain also serves as versatile multi-agent system for General AI Assistants. Comprehensive evaluations on the GAIA benchmark demonstrate MetaChains effectiveness in generalist multi-agent tasks, surpassing existing state-of-the-art methods. Furthermore, MetaChains Retrieval-Augmented Generation (RAG)-related capabilities have shown consistently superior performance compared to many alternative LLM-based solutions."
        },
        {
            "title": "Introduction",
            "content": "The emergence of Large Language Models (LLMs) has revolutionized AI agent development, enabling unprecedented breakthroughs in autonomous task execution and intelligent problem-solving. LLMpowered agents excel at understanding context, making informed decisions, and seamlessly integrating with various tools and APIs. Leading frameworks like LangChain [1], AutoGPT [2], AutoGen [3], CAMEL [4], and MetaGPT [5] have demonstrated remarkable success in automating increasingly complex workflows - from sophisticated web navigation to advanced data analysis and innovative creative content production. By leveraging advanced mechanisms such as role-playing, structured operating procedures, and dynamic agent coordination, these frameworks deliver exceptional problemsolving capabilities while significantly reducing human intervention. Despite remarkable advancements in AI agent development, significant barrier persists: the creation and optimization of LLM agent systems remains dependent on traditional programming expertise. Current frameworks primarily cater to technically proficient developers who can navigate complex codebases, understand API integrations, and implement sophisticated prompt engineering patterns. This reliance on coding skills creates substantial accessibility gap, as only 0.03% of the global population possesses the necessary programming expertise to effectively build and customize these agents. Even with well-documented frameworks and development tools, the entry barrier remains dauntingly high for non-technical users. This limitation becomes particularly problematic given the universal need for personalized AI assistants in digital age. Everyone, from business professionals seeking workflow automation to educators designing interactive learning tools, requires customized LLM agents tailored to their specific needs. For instance, researcher might need an agent specialized in literature review and data analysis, while content creator might require an agent focused on creative writing and media management. The current paradigm of coding-dependent agent development not only severely restricts the user base but also creates bottleneck in meeting the diverse and evolving demands for personalized AI assistance. This misalignment between universal needs and limited accessibility calls for fundamental rethinking of how LLM agents are created and customized. This stark contrast between universal needs and limited accessibility leads us to fundamental research question: Is it possible to democratize LLM agent development by enabling Natural Language-based Creation and Customization? In this work, we aim to realize this vision by introducing MetaChain2, novel framework that fundamentally reimagines agent development as fully automated, languagedriven process requiring zero programming expertise. To realize this vision, MetaChainoperates as an autonomous Agent Operating System with three key capabilities: 1) Natural Language-Driven Multi-Agent Building - automatically constructing and orchestrating collaborative agent systems purely through natural dialogue, eliminating the need for manual coding or technical configuration; 2) Self-Managing Workflow Generation - dynamically creating, optimizing and adapting agent workflows based on high-level task descriptions, even when users cannot fully specify implementation details; and 3) Intelligent Resource Orchestration - providing unified access to tools, APIs, and computational resources via natural language while automatically managing resource allocation and optimization. Through this innovative architecture, MetaChaindemocratizes LLM agent development while maintaining enterprise-grade sophistication, transforming traditionally complex engineering task into an intuitive conversation accessible to all users. 2The name MetaChain embodies our frameworks core philosophy of Agent-Creating Agents. Similar to how meta-learning enables systems to Learn How to Learn, MetaChain represents LLM Agent framework that automates the creation and orchestration of other agents. The Meta prefix signifies this self-reflective capability - our framework doesnt just deploy agents, but rather learns to develop, customize, and coordinate agents automatically. Combined with Chain, which represents the seamless workflow of connected agentic components, the name MetaChain captures our frameworks essence as self-improving system that elevates agent development to meta-level, enabling automated agent generation and workflow optimization. 2 To enable fully-automated and zero-code LLM agent development, MetaChain introduces several synergistic technical innovations that form complete framework: First, the Agentic System Utilities provides foundational multi-agent architecture, where specialized web, code, and file agents collaborate seamlessly to handle diverse real-world tasks. At its core, the LLM-powered Actionable Engine serves as the systems brain, supporting flexible integration of any LLM provider through both direct and transformed tool-use paradigms for robust action generation. To address the critical challenge of information management, the Self-Managing File System enhances overall system capability by automatically converting diverse data formats into queryable vector databases, enabling efficient information access across all operations. Additionally, the Self-Play Agent Customization not only transforms natural language requirements into executable agents through structured XML schemas, but also automatically generates optimized workflows through iterative self-improvement, eliminating the need for manual agent programming or workflow design. Together, these innovations enable MetaChain to democratize agent development while maintaining production-level robustness. MetaChains exceptional capabilities have been rigorously validated through comprehensive empirical evaluation. In standardized benchmarks, it secured strong second place on the Generalist Agent Benchmark (GAIA), while significantly outperforming state-of-the-art RAG approaches on the Retrieval-Augmented Generation benchmark. Beyond these quantitative achievements, extensive case studies demonstrated MetaChains robust self-development capabilities across diverse real-world scenarios, highlighting its practical value in automated agent development."
        },
        {
            "title": "2 Related Work and Preliminaries",
            "content": "LLM-empowered agents have revolutionized AI systems by leveraging powerful language understanding and reasoning capabilities to interact with external environments through tool invocation. rich ecosystem of agent frameworks has emerged, with prominent examples including LangChain [1], AutoGPT [2], CAMEL [4], MetaGPT [5], and OpenAgent [6]. These frameworks have demonstrated impressive capabilities across diverse domains: CAMEL pioneered role-playing-based communication for cooperative behaviors, AutoGen [3] developed customizable LLM combinations for complex problem-solving, MetaGPT [5] integrated Standardized Operating Procedures for software engineering tasks, and OpenAgent [6] provided comprehensive platform for specialized web agents including data processing, plugin integration, and web interaction. However, leveraging these LLM Agent frameworks requires substantial coding skills and domain expertise to build effective agents, which significantly limits their accessibility to non-technical users and hinders the widespread adoption of agent technology across different domains. To address this challenge, we aim to propose new paradigm of LLM Agent framework that democratizes agent development by enabling automatic generation, customization, and orchestration of agents through natural language interactions, making powerful agent technology accessible to users regardless of their technical background. LLM-Empowered Agent. The task-solving process of Large Language Model (LLM) agents can be formalized as Markov Decision Process (MDP), providing comprehensive framework for modeling their interaction with the environment. Defined as = (S, A, O, (), E), the MDP captures the agents state space S, action space A, observation space O, state transition function (), and the set of environments it can interact with. At each time step, the LLM agent observes the current state, selects an action based on its policy, interacts with the environment, and updates its state, often referred to as the agents context. The mapping from state to action can follow two primary paradigms: Tool-Use [7], where the agent utilizes external capabilities, and ReAct [8] (Non-tool-use), where the agent generates the next action solely based on its internal language model. This MDP formulation provides powerful framework for understanding, analyzing, and designing LLM-empowered agents capable of tackling wide range of complex, multi-step tasks. Generalist Multi-Agent System. The motivation behind multi-agent systems (MAS) is to overcome the limitations of single agent in handling the full scope and nuances of complex problems. By leveraging the diverse capabilities and specialized knowledge of multiple agents, the MAS enables more effective problem-solving for multi-faceted tasks. To enhance the generalization power of MAS, researchers have designed Generalist Multi-Agent Systems that employ team or ensemble of specialized agents. These agents work together under the coordination of central Orchestrator agent to solve wide variety of complex tasks through collaborative intelligence and synergistic efforts. 3 Figure 2: MetaChain is fully automated, language-driven generalist agent system. The core components that enable this include the Agentic System Utilities, the LLM-powered Actionable Engine, the Self-Managing File System, and the Self-Play Agent Customization module. In Generalist MAS, there are multiple agents, denoted as π0 : S0 A0, π1 : S1 A1, . . . , πn : Sn An. Within each agents action set, there exists special transfer action ˆAi Ai, which enables the delegation of tasks to other agents. The key challenge in MAS lies in designing an effective Task Transfer Mechanism, which organizes different agents through appropriate transfer actions. We define such an agent organization mechanism as the MAS Design Pattern. common design is the Orchestrator-Workers paradigm [9, 10], where the Orchestrator comprehends the task and distributes subtasks to Workers via transfer actions. The Workers, acting as sub-agents, execute the subtasks and return the results to the Orchestrator through transfer actions. For tasks with deterministic steps, workflow-driven mechanisms have emerged as particularly effective approach. Notable examples include GPTSwarm [11], which models workflows as computational graphs for complex data operations, and specialized systems for mathematical reasoning [12] and code generation [13]. These advances have enabled strong capabilities across various applications, including GUI interaction [14, 15], software development [16, 17], web browsing [18, 19], and embodied tasks [20], demonstrating the versatility and potential of LLM-powered agent systems. Workflow Design in Generalist MAS. While Generalist Multi-Agent Systems offer high flexibility, particularly for open-ended and complex tasks, they also come with higher costs, increased complexity, and potential for compounding errors [10]. For tasks with deterministic processes and expert domain knowledge, fixed workflows can be more stable and effective approach. ck πj, where πi and πj represent agents, and ck workflow in MAS is defined as = wi,j : πi is the transfer condition. Effective workflow design involves defining conditional transfer equations between agents based on downstream tasks, ensuring smooth and coordinated transmission - the Workflow Design Pattern. Common patterns include Routing (directing tasks through sequence), Parallelization (distributing subtasks concurrently), and Evaluator-Optimizer (using agents to assess and refine) [10]. These can create stable and efficient task-solving in Generalist MAS. Fully-Automated Generalist MAS. While the successful design and execution of both multi-agent systems and complex workflows typically require substantial expert knowledge and engineering expertise, the goal of MetaChain is to fully automate this process. The key challenge lies in seamlessly bridging the gap from high-level user requirements to the practical implementation of effective MAS and workflow solutions - all through natural language interactions."
        },
        {
            "title": "3 The MetaChain Framework",
            "content": "MetaChain is designed to be the automated operating system for LLM agents and general AI assistant. Inspired by modern computer operating systems, MetaChain consists of key components that enable seamless natural language-driven agent development and task execution, as illustrated in Fig 2. Its Agentic System Utilities provide foundational building blocks for complex agent-driven tasks, while the LLM-powered Actionable Engine forms the central brain, understanding inputs and orchestrating multi-agent coordination. The Self-Managing File System manages structured storage and retrieval of user multi-modal data, and the Self-Play Agent Customization empowers users to generate specialized, tailored agents and workflows through natural language, without any coding requirements. Collectively, these robust capabilities make MetaChain versatile and powerful platform, powering variety of autonomous agent-based solutions for diverse applications."
        },
        {
            "title": "3.1 Agentic System Utilities",
            "content": "The MetaChain framework employs modular, multi-agent architecture to address the key challenge of developing intelligent personal assistant agents capable of seamlessly integrating and coordinating diverse capabilities, from web browsing and information retrieval to data analysis and code execution. This design choice, which comprises specialized agents for web, coding, and file management tasks, as well as an orchestrator agent to decompose and delegate user requests, enables the agentic system utilities to serve as versatile and extensible foundation that can adapt to wide range of user requirements, facilitating the rapid development of tailored, agent-driven solutions. Detailed system prompts and tool definitions for Agentic System Utilities can be found in Appendix Sec 6."
        },
        {
            "title": "3.1.1 Orchestrator Agent",
            "content": "The Orchestrator Agent is the primary interface for interacting with the user. It receives tasks from the user, comprehends the tasks, decomposes them into sub-tasks, and delegates these sub-tasks to appropriate sub-agents using the handoff tools [21]. Once sub-agent completes sub-task, it returns the result to the Orchestrator also using the handoff tool. Based on the task completion status, the Orchestrator continues to assign the next sub-task to suitable agent. This iterative process continues until the entire task is completed. The Orchestrator, designed with the handoff mechanism, is simple yet effective solution, eliminating the need for complex prompts to handle task planning."
        },
        {
            "title": "3.1.2 Web Agent",
            "content": "The Web Agent provides versatile and extensible set of tools that enable the agent to perform wide range of web-based tasks, from general web searches to file downloads. Functionality. The Web Agent performs web searches, navigate to pages, browse content, and download files. We abstract web browsing behaviors into 10 high-level tools (actions), such as: click, web_search, visit_url, etc., which the agent can use to complete web-based tasks. Environment. In this work, we implement the browser environment based on BrowserGym [18]. This environment defines low-level, code-driven browser behaviors, such as the execution code for clicking specific browser element. The high-level tools used by the agent are implemented by combining these low-level actions, significantly enhancing the extensibility of the tool definitions."
        },
        {
            "title": "3.1.3 Coding Agent",
            "content": "The Coding Agent is the agents comprehensive and versatile solution for tackling wide range of code-driven tasks. It empowers the agent to effortlessly handle complex challenges, from data analysis and calculations to machine learning, automation, and system administration. This seamlessly integrated, feature-rich agent serves as the primary interface for all code-related activities, abstracting complexities to enable focused problem-solving. In essence, the coding agent provides secure environment to explore, execute, and interact with code to achieve diverse objectives. Functionality. The Coding Agent is designed to handle wide range of code-related operations. We have abstracted these capabilities into 11 distinct tools (actions), including the creation of code scripts and directories, execution of Python code, implementation of specific commands, and navigation of directory structures. This versatile toolset enables the Coding Agent to address diverse array of everyday tasks that require programmatic solutions. Environment. The Coding Agent operates within an interactive terminal environment, where the results of all code-related tools are returned to the agent as observations through the terminal output. When the output exceeds the display capacity, the terminal presents it in paginated form, allowing the agent to navigate through the content using commands such as terminal_page_up, terminal_page_down, and terminal_page_to. This ensures that the agent can access and review the full output without encountering issues related to the LLMs context length limitations. To ensure safety, all code-related operations run in secure Docker sandbox. We support third-party sandboxing integration, like the E2B system [22], further enhancing the safety and reliability of the agents code-driven activities. This provides robust protection for local data during execution."
        },
        {
            "title": "3.1.4 Local File Agent",
            "content": "The key objective of the Local File Agent is to provide unified set of tools to convert and analyze various local multi-modal data types. This includes text, video, audio, spreadsheets, and other formats to assist users with everyday tasks in an efficient manner. Functionality. Users need to handle wide variety of file types, including text documents (e.g., .doc, .pdf, .txt, .ppt), videos (e.g., .mp4, .mov), audio files (e.g., .wav, .mp3), spreadsheets (e.g., .csv, .xlsx), and other multi-modal and multi-format files. Therefore, dedicated Local File Agent for managing and analyzing local files is crucial. This Local File Agent can use unified set of tools to convert files into Markdown and perform effective analysis to assist with everyday tasks. Environment. The interaction environment for the Local File Agent is similar to the Coding Agent, utilizing an interactive Markdown Browser. Files converted into Markdown format can be displayed in paginated form within the Markdown Browser, enabling the agent to analyze long texts and complex files that exceed the context length limitations."
        },
        {
            "title": "3.2 LLM-powered Actionable Engine",
            "content": "As the CPU executes instructions, manages resources, and coordinates processes in an OS, the LLM-powered actionable engine can understand natural language, generate plans, and coordinate tasks across agents. This enables seamless human-agent collaboration and task completion. We utilize LiteLLM [23] to standardize LLM requests through an OpenAI-like interface, supporting 100+ models from various providers. For agent collaboration, the LLM receives all action-observation pairs up to time as state st to determine the next action. These pairs serve as system RAM, facilitating efficient retrieval and enabling language-driven system coordination."
        },
        {
            "title": "3.2.1 Generating Actionable Reflections",
            "content": "We generate reflections (i.e., actions) based on LLM context, which can be broadly categorized into two distinct approaches that leverage the language models capabilities. Direct Tool-Use Paradigm. This approach is suitable for commercial LLMs or LLM serving platforms that support tool-use. These LLMs can directly generate parsed next-step tool to execute based on the provided tool set and the current state, reducing errors during the tool parsing phase. However, this method heavily relies on the optimization of the third-party platforms capabilities. Transformed Tool-Use Paradigm. This approach does not rely on the LLMs inherent tool-use capabilities. Leveraging the superior code-generation abilities of modern LLMs, we transform the tool-use paradigm into structured XML code generation task, e.g., <function=function_name> <parameter=parameter_1>value_1</parameter> ... </function>. This structured output is then parsed to extract critical information like tool arguments and others. It improves the performance of commercial models with suboptimal tool-use capabilities and enables the integration of open-source LLMs into the system, providing greater flexibility and customization."
        },
        {
            "title": "3.3 Self-Managing File System",
            "content": "The file system in MetaChain is vector database that LLM agents can retrieve and understand. In our design framework, users can upload text files in any format (e.g., .pdf, .doc, .txt) or compressed archives and folders containing any text files. The system tools in the file system automatically convert these files into consistent text format and store them in user-defined collection within the vector database (using the save_raw_docs_to_vector_db tool). This enables agents to selfmanage their database memory and perform efficient and accurate retrieval and generation using tools like query_db and answer_query. The detailed definitions of the tools are presented in Tab 4."
        },
        {
            "title": "3.4 Self-Play Agent Customization",
            "content": "To allow users to customize tools and agents for specific scenarios or build their own multi-agent systems and workflows, it is designed as code-driven, controllable self-programming agent framework. By implementing constraints, error-handling mechanisms, and customized workflows, it enables 6 controlled code generation, facilitating the creation of tools, agents, and workflows. The MetaChain supports two distinct modes: agent creation without workflow and agent creation with workflow."
        },
        {
            "title": "3.4.1 Agent Creation without Workflow",
            "content": "Building effective multi-agent systems often requires domain-specific expertise, such as in-depth knowledge of financial regulations or healthcare protocols. However, this level of specialized knowhow may not always be available to users. For example, in the financial services, constructing multi-agent system to automate complex investment portfolio management would necessitate expertise in areas like asset allocation, risk modeling, and regulatory compliance. To address this challenge, our MetaChain provides powerful workflow-based mode allowing users to generate sophisticated agent systems with minimal domain expertise. In this mode, the user provides high-level descriptions of the desired agent(s), such as the agents name and brief sentence-level description for the expected agent functionalities. MetaChain then uses this input to automatically generate the appropriate agent(s) and the necessary workflow(s) to orchestrate their collaborative efforts. This is all done based on the current state of the framework, including the available tools, agents, and workflows. The key steps in this workflow-based agent generation approach are: Analyze Requirements and Existing Components. The process begins by carefully analyzing the users requirements in the context of the existing tools and agents already available in the system. This in-depth analysis is performed using the specialized profiling agent, which helps thoroughly assess the current capabilities and resources that can be leveraged to fulfill the users needs. Analyze Tools and Structure Agents. Based on the comprehensive analysis performed, the system then carefully determines the need for creating new tools, meticulously evaluates whether existing tools can be effectively utilized, and subsequently structures the seamless collaboration between multiple agents as appropriate. This ensures the optimal and comprehensive use of available resources, ultimately leading to the efficient agent system design. Generate Detailed XML Agent Specifications. This step is designed to generate structured XML code that accurately represents the detailed agent creation requirements. This comprehensive XML representation captures the necessary information, including the agents functionality, dependencies, and interactions, to enable the seamless and efficient subsequent process of agent generation. Optimized Tool Creation with Third-Party APIs. The Tool Editor Agent can seamlessly integrate various third-party APIs, such as LangChain, RapidAPI, and Hugging Face, to create powerful tools. It expertly utilizes advanced retrieval techniques to search for and surface relevant API documentation, including comprehensive details like names, descriptions, and invocation methods. The robust system currently supports an extensive collection of 145 APIs from 8 diverse categories in RapidAPI, LangChain [1], and wide range of models from 9 categories in Hugging Face. Future plans include seamlessly integrating more cutting-edge platforms like Composio [24]. The agent also generates tool code based on its knowledge, automatically checking for syntax errors. It designs test cases, runs the tool, and verifies functionality. If the tool fails, the agent automatically debugs the code until successful. This targeted approach allows more customized and adaptable tool set, rather than bloated, integrated system. Agent Creation and Execution. When the users requirements involve multiple agents focused on different tasks, the Agent Editor Agent automatically identifies this need and performs the necessary multi-step agent creation operations. After all agents are successfully created, the system invokes the create_orchestrator_agent tool to generate an orchestrator agent that connects the required agents. This orchestrator adheres to the Orchestrator-Workers MAS design pattern, with system prompt that includes task descriptions, sub-task decomposition rules, and other scenario-specific details. Detailed algorithms and system prompts are provided in Appendix Sec 6.6.1."
        },
        {
            "title": "3.4.2 Agent Creation with Workflow",
            "content": "When users have specific requirements for MASs workflow and domain knowledge, MetaChain allows tailored approach. In this mode, users provide descriptions of the desired agent(s) and specify the tasks they want the created agent(s) or workflows to accomplish. MetaChain then uses this information about the target tasks to generate not just the individual agent(s), but also the necessary workflow(s) to coordinate their collaborative efforts in achieving the specified objectives. 7 Traditional graph-based methods often require strict adherence to graph theory principles [2528], task challenging for LLMs when generating workflows. To overcome these challenges, MetaChain adopts an event-driven approach where we model each agents task-solving as an event. By leveraging event listening and triggering mechanisms, MetaChain enables seamless collaboration between agents, offering greater flexibility and adaptability compared to rigid graph structures. Constructing New Workflows. The process of creating new workflow is itself multi-agent workflow. The Workflow Form Agent analyzes the requirements and existing tools/agents to determine if new agents need to be created, which agents should form the workflow, and what the listening and triggering logic between events should be. It then generates structured XML code. During the parsing phase, robust error detection mechanism verifies whether the generated workflow form complies with system constraints (e.g., constraints on the on_start event). If the constraints are not satisfied, detailed error messages are sent back to the Workflow Form Agent as feedback for regeneration. If the constraints are satisfied, the workflow form is passed to the Workflow Editor Agent, which creates new agents (if needed), constructs the new workflow, and executes it on the task. Detailed algorithms and system prompts are shown in Appendix Sec 6.6.2."
        },
        {
            "title": "4.1 Evaluation for a Generalist Agent System",
            "content": "Benchmark Dataset and Evaluation Protocols. The GAIA benchmark [29] is comprehensive evaluation framework to assess General AI Assistants. It tests fundamental abilities like Reasoning, Multi-Modality Handing, Web Browsing, and Tool-Use Proficiency through rigorous set of 466 test and 165 validation questions, divided into 3 distinct difficulty levels. categories: Table 1: Performance comparison between the baseline models and our MetaChain on the GAIA benchmark. The results we report are those published on the GAIA leaderboard. GAIA emphasizes the importance of developing AI systems that can match human performance on everyday tasks, serving as milestone for assessing progress toward AGI. Specifically, we evaluated our MetaChain on the GAIA validation set, using success rate as the primary evaluation metric. This metric measures the percentage of tasks successfully completed, providing clear indicator of its performance in handling real-world, human-like challenges. Baseline Methods. The baselines we selected are divided into two Open-Source: FRIDAY [30], Magentic-1 [9], MultiAgent Experiment v0.1 (powered by AutoGen)[31], HuggingFace Agents[32], Langfun Agent [33]; Closed-Source: TapeAgent, AgentIM, Trase Agent [34], Omne, Barcelona3, and the h2oGPTe Agent [35]. These diverse baselines represent the current state-of-the-art in open-source and proprietary multi-agent systems, providing comprehensive landscape performance for and capabilities of our proposed MetaChain framework. evaluating the Agent Name TapeAgent v0.1 FRIDAY Magentic-1 AgentIM Multi-Agent Exp v0.1 AgentIM v1.1 Trase Agent HuggingFace Agents Magentic-1 (o1) omne Trase Agent v0.2 Barcelona v0.1 Langfun Agent v2.0 h2oGPTe Agent v1.6.8 MetaChain Implementation Details. To address tasks in the GAIA benchmark, we utilize combination of the System Utilities of the Model and the Tool Editor Agent from the Agentic-SDK. The basic agents first attempt to complete the task while collecting relevant information and reflections. If successful, the result is directly returned. If not, the Tool Editor Agent creates new tools to continue the task. During validation, Claude-Sonnet-3.5 is used by default. Avg. 33.94 34.55 36.97 37.58 39.39 40.00 40.00 44.24 46.06 46.06 47.27 50.30 54.55 63.64 55.15 L1 47.17 45.28 54.72 50.94 54.72 50.94 47.17 58.49 56.60 60.38 58.49 62.26 60.38 67.92 71.70 L3 3.85 11.54 11.54 15.38 11.54 15.38 23.08 19.23 23.08 23.08 26.92 26.92 26.92 42.31 26.92 L2 34.88 34.88 33.72 36.05 38.37 40.70 40.70 43.02 46.51 44.19 46.51 50.00 59.30 67.44 53. 3TapeAgent, AgentIM, Omne, and Barcelona are anonymous. 8 Evaluation Results and Analysis. The results in Table 1 reveal the following key observations: Obs.1. Overall Superiority of MetaChain: Our method significantly outperforms all opensource agent systems and achieves performance close to the latest agent system, h2oGPTe Agent v1.6.8 (submitted on December 16, 2024), securing stable position in the top 2 rankings. Notably, our approach demonstrates superior performance on Level 1 tasks compared to all state-of-the-art baselines, becoming the first method to achieve over 70% accuracy rate. This success is attributed to the well-designed System Utilities and the stable interaction of basic agents with the environment, enabling efficient solutions to everyday simple tasks. Obs.2. Effectiveness of Key Components: Specifically, our framework demonstrates significantly superior performance compared to Magentic-1 [9], recent representative open-source MAS, and FRIDAY, classic self-improved framework. While Magentic-1 leverages the powerful reasoning capabilities of o1-preview to design complex Orchestrator Agent (also the Coder Agent), our framework emphasizes the stability of interactions between sub-agents and their respective environments, as well as the precision of tool definitions. Under these conditions, the Orchestrator Agent achieves better results with simple prompts and handoff tools. Obs.3. Error Analysis: The analysis revealed two key limitations in the current evaluation protocol of the GAIA benchmark system: strict string matching that ignores semantic equivalence (e.g., \"840 citations\" vs. \"Citations\") and challenges posed by dynamic, anti-automation mechanisms during web searches. These issues underscore the need for more semantically-aware evaluation approach to improve the systems effectiveness."
        },
        {
            "title": "4.2 Evaluation of MetaChain on the Retrieval-Augmented Generation (RAG) Task",
            "content": "Benchmark Dataset and Evaluation Protocols. To test the basic functionalities of the MetaChain, we use the RAG task as the testing benchmark. MultiHop-RAG [36] is dataset designed to evaluate RAG capabilities, requiring the RAG methods to gather information from multiple sources and generate responses, which aligns with the file functionality logic of MetaChain. We evaluate using two metrics: Accuracy (Acc) measures response consistency with expected answers (e.g., ChatGPT or OpenAIs ChatGPT are both correct for Which AI tool reached 100M daily users in March?). Error (Err) counts confident but incorrect responses (e.g., answering Bard to the above query). Baseline Methods. The baselines represent diverse range of LLM-based RAG techniques. The chunk methods, such as NaiveRAG [37] and HyDE [38], utilize the original text segmentation. The graph methods, including MiniRAG [39] and LightRAG [40], manage files as sophisticated graphs. In contrast, Langchains Agentic RAG [1] innovatively accesses files through intelligent software agents. These baselines cover wide array of strategies for leveraging large language models to retrieve and generate robust responses. Implementation Details. We used gpt-4o-mini [41] as the LLM and text-embedding-3-small for embeddings. We followed MultiHopRAG [36] for text chunking, with 256-token chunks and top-6 retrieval. This leverages the gpt-4o-minis language abilities while text-embedding-3-small provides retrieval, with MultiHopRAGs chunking managing information effectively. Evaluation Results and Analysis. We summarize the key observations from the results  (Table 2)  . Table 2: Evaluation of MetaChain and baselines for RAG."
        },
        {
            "title": "Method",
            "content": "Chunk-Based acc err"
        },
        {
            "title": "NaiveRAG",
            "content": "acc err"
        },
        {
            "title": "Superior",
            "content": "Performance of MetaChain. The results clearly demonstrate the superior performance of our proposed MetaChain model compared to other baselines on the Multihop-RAG task. By leveraging more flexible and adaptive agent-based framework, MetaChain is able to dynamically orchestrate the retrieval and reasoning process, outperforming even other agentic approaches. 62.83% 20.50% 73.51% 14.20% 57.81% 34.78% 58.18% 35.40% 53.36% 12.28% 56.59% 16.55% Graph-Based Agent-Based"
        },
        {
            "title": "MiniRAG",
            "content": "MetaChain vs. LangChain. Our method significantly outperforms LangChain, which is also an agentic RAG. This is due to MetaChains more flexible framework, where agents do not need to rely on predefined workflows and tools to execute file search tasks. The proposed model can orchestrate workflows on the fly during the search process, leading to more efficient and accurate results."
        },
        {
            "title": "4.3 MetaChain’s Performance on Open-Ended Tasks",
            "content": "This section thoroughly explores the capabilities of the MetaChain framework in generating agents and workflows based on even vague, natural language inputs across various scenarios. To illustrate the breadth of MetaChains abilities, we will examine its performance on tasks of varying difficulty - from the creation of single agent to the orchestration of multiple, coordinated agents. Task with Single Agent. MetaChain can create tools for third-party APIs (RapidAPI, Hugging Face). We demonstrated this by generating DaVinci Agent for image creation and refinement. This shows MetaChains capability to build task-specific agents from natural language. want to create DaVinci Agent that can help me to generate the image with natural language. it can: 1. generate the image with natural language and save it to the specified path on the local machine using the HF model Sana_600M_1024px_diffusers 2. evaluate the image using visual_question_answering tool according to the given image. 3. interatively refine generated image based on the evaluation result. Automated Agent Creation and Execution. MetaChain begins generating an XML table from the natural language requirements, using existing tools and agents. This structured form is then passed to the Tool Editor Agent, which creates the necessary generate_image and refine_image tools. The Agent Editor Agent composes the DaVinci Agent by integrating the new tools with an existing visual_question_answering tool. This agent is executed, generating and storing several logo designs, as shown in Fig 3. Due to limited local resources, smaller model was used, yet the agent successfully completed the task. This demonstrates MetaChains seamless creation of the tailored agent for complex, open-ended design challenges. Task with Multi-Agents. To further validate MetaChains capability to generate agents and integrate third-party tools, we tasked it with creating Financial Agent based on the following requirements: want to create Financial Agent that can help me to do two kinds of tasks: 1. Manage the private financial docs. have folder that contain the financial docs in my local machine, and want to help me to manage them. 2. Search the financial information online. You may help me to: - get balance sheets for given ticker over given period. - get cash flow statements for given ticker over given period. - get income statements for given ticker over given period. Building Comprehensive Financial Agent. The Agent Form Agent automatically recognized the need to create two distinct agents to handle the given tasks: the Document Manager Agent and the Market Research Agent. The specific XML representation of this multi-agent structure is shown in List 18 in the Appendix. After successfully validating the structured requirements, the Tool Editor Agent proceeded to create the necessary tools: get_balance_sheet, get_cash_flow, get_income_statement, and analyze_financial_data. With the tools in place, the Agent Editor Agent composed the Document Manager Agent and the Market Research Agent, and established Financial Analysis Orchestrator to coordinate them. The Financial Analysis Orchestrator skillfully leveraged the innovative new tools and the organizations existing robust capabilities to comprehensively conduct thorough research and detailed analysis on the critical local documents as well as valuable external data sources. This rigorous process culminated in the production of meticulously crafted, comprehensive research report, as showcased in List 19 within the Appendix. The comprehensive agent trajectory for this intricate and complex requirement is presented in illuminating detail in Tab 6 within the Appendix. This detailed account reveals that while the Orchestrator encountered SyntaxError during the initial creation phase, the resilient and adaptable Agent Editor component was able to adeptly self-debug and successfully complete the task, thereby strikingly demonstrating the robustness of MetaChainsystem. Workflow Generation. Scaling Test-Time Compute has been validated as superior approach for solving reasoning problems. However, manually constructing workflows poses high barrier to entry. We aim to explore whether MetaChains automatic creation of agents and workflows can bridge the gap between the idea of Test-Time Compute and the implementation of workflows. Taking the majority voting method with multiple models as an example: 10 Table 3: Comparison between single LLMs and the AI-generated Majority Voting workflow."
        },
        {
            "title": "Models",
            "content": "gpt-4o 0806 claude-3.5-sonnet 1022 deepseek-v3 Majority Voting Workflow pass@1 66.4 66. 74.2 (3 models) 75.6 want to create workflow that can help me to solving the math problem. The workflow should: 1. Parallelize solving the math problem with the same Math Solver Agent using different language models (gpt-4o, claude-3-5-sonnet, deepseek-chat) 2. Aggregate the results from the Math Solver Agent and return the final result using majority voting. Potential Test-Time Scaling Law. Upon receiving the requirements, the Workflow Form Agent generated an XML-formatted workflow table (List 20). This table includes two new agents: Math Solver Agent and Vote Aggregator Agent. After validation, the Agent Editor Agent created agents. The Workflow Editor Agent then constructed new workflow based on the form and conducted tests. To validate the workflows practicality, we performed comparative experiments on the MATH-500 dataset [42] using 3 LLMs (gpt-4o-20240806, claude-3.5-sonnet-20241022, deepseek-v3) and Majority Voting workflow. As shown in Tab 3, the generated workflow performs significantly better than state-of-the-art baselines. We selected cases from deepseek-v3 (Tab 5) where MetaChains workflow effectively corrected errors through multi-model collaboration, demonstrating its potential to establish scaling laws in LLM agents."
        },
        {
            "title": "5 Conclusion",
            "content": "The MetaChain framework marks significant advancement in democratizing LLM-powered agent technology, making it accessible to the non-programming majority. By bridging high-level natural language requirements with the practical implementation of multi-agent systems and workflows, MetaChain empowers users to create, customize, and deploy agents, tools, and workflows without requiring substantial technical expertise. Its modular architecture, versatile Agentic System Utilities, and LLM-powered Actionable Engine work together to enable seamless automation of agent development and task execution. Unique features such as the Self-Organizing File System and Self-Play Agent Customization further enhance MetaChains capabilities, allowing for dynamic agent evolution and task-specific optimization. Extensive evaluations demonstrate MetaChains superior performance, highlighting its transformative potential in making LLM capabilities accessible to broad user base."
        },
        {
            "title": "References",
            "content": "[1] LangChain. Langchain: Build context-aware reasoning applications. https://github. com/langchain-ai/langchain, 2023. URL https://github.com/langchain-ai/ langchain. [2] Significant-Gravitas. Autogpt. https://github.com/Significant-Gravitas/AutoGPT, 2023. [3] Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Shaokun Zhang, Erkang Zhu, Beibin Li, Li Jiang, Xiaoyun Zhang, and Chi Wang. Autogen: Enabling next-gen LLM applications via multi-agent conversation framework. CoRR, abs/2308.08155, 2023. [4] Guohao Li, Hasan Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem. CAMEL: communicative agents for \"mind\" exploration of large language model society. In NeurIPS, 2023. [5] Sirui Hong, Mingchen Zhuge, Jonathan Chen, Xiawu Zheng, Yuheng Cheng, Jinlin Wang, Ceyao Zhang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou, Chenyu Ran, Lingfeng Xiao, Chenglin Wu, and Jürgen Schmidhuber. MetaGPT: Meta programming for multi-agent collaborative framework. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=VtmBAGCN7o. [6] Tianbao Xie, Fan Zhou, Zhoujun Cheng, Peng Shi, Luoxuan Weng, Yitao Liu, Toh Jing Hua, Junning Zhao, Qian Liu, Che Liu, Leo Z. Liu, Yiheng Xu, Hongjin Su, Dongchan Shin, Caiming Xiong, and Tao Yu. Openagents: An open platform for language agents in the wild, 2023. [7] Shunyu Yao, Noah Shinn, Pedram Razavi, and Karthik Narasimhan. τ -bench: benchmark for tool-agent-user interaction in real-world domains. CoRR, abs/2406.12045, 2024. [8] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik R. Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. In ICLR. OpenReview.net, 2023. [9] Adam Fourney, Gagan Bansal, Hussein Mozannar, Cheng Tan, Eduardo Salinas, Erkang Zhu, Friederike Niedtner, Grace Proebsting, Griffin Bassman, Jack Gerrits, Jacob Alber, Peter Chang, Ricky Loynd, Robert West, Victor Dibia, Ahmed Awadallah, Ece Kamar, Rafah Hosn, and Saleema Amershi. Magentic-one: generalist multi-agent system for solving complex tasks. CoRR, abs/2411.04468, 2024. [10] Anthropic. Building effective agents. https://www.anthropic.com/research/ building-effective-agents, 2024. URL https://www.anthropic.com/research/ building-effective-agents. [11] Mingchen Zhuge, Wenyi Wang, Louis Kirsch, Francesco Faccio, Dmitrii Khizbullin, and Jürgen Schmidhuber. Gptswarm: Language agents as optimizable graphs. In Forty-first International Conference on Machine Learning. [12] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V. Le, Ed H. Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. In ICLR. OpenReview.net, 2023. [13] Evan Wang, Federico Cassano, Catherine Wu, Yunfeng Bai, Will Song, Vaskar Nath, Ziwen Han, Sean Hendryx, Summer Yue, and Hugh Zhang. Planning in natural language improves LLM search for code generation. CoRR, abs/2409.03733, 2024. [14] Wenyi Hong, Weihan Wang, Qingsong Lv, Jiazheng Xu, Wenmeng Yu, Junhui Ji, Yan Wang, Zihan Wang, Yuxiao Dong, Ming Ding, and Jie Tang. Cogagent: visual language model for GUI agents. In CVPR, pages 1428114290. IEEE, 2024. [15] Yiheng Xu, Zekun Wang, Junli Wang, Dunjie Lu, Tianbao Xie, Amrita Saha, Doyen Sahoo, Tao Yu, and Caiming Xiong. Aguvis: Unified pure vision agents for autonomous gui interaction. 2024. URL https://arxiv.org/abs/2412.04454. 12 [16] Xingyao Wang, Boxuan Li, Yufan Song, Frank F. Xu, Xiangru Tang, Mingchen Zhuge, Jiayi Pan, Yueqi Song, Bowen Li, Jaskirat Singh, Hoang H. Tran, Fuqiang Li, Ren Ma, Mingzhang Zheng, Bill Qian, Yanjun Shao, Niklas Muennighoff, Yizhe Zhang, Binyuan Hui, Junyang Lin, Robert Brennan, Hao Peng, Heng Ji, and Graham Neubig. OpenHands: An Open Platform for AI Software Developers as Generalist Agents, 2024. URL https://arxiv.org/abs/2407. 16741. [17] John Yang, Carlos Jimenez, Alexander Wettig, Kilian Lieret, Shunyu Yao, Karthik Narasimhan, and Ofir Press. SWE-agent: Agent-computer interfaces enable automated software engineering. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. URL https://arxiv.org/abs/2405.15793. [18] Alexandre Drouin, Maxime Gasse, Massimo Caccia, Issam H. Laradji, Manuel Del Verme, Tom Marty, David Vazquez, Nicolas Chapados, and Alexandre Lacoste. WorkArena: How capable are web agents at solving common knowledge work tasks? In Ruslan Salakhutdinov, Zico Kolter, Katherine Heller, Adrian Weller, Nuria Oliver, Jonathan Scarlett, and Felix Berkenkamp, editors, Proceedings of the 41st International Conference on Machine Learning, volume 235 of Proceedings of Machine Learning Research, pages 1164211662. PMLR, 2127 Jul 2024. URL https://proceedings.mlr.press/v235/drouin24a.html. [19] Yueqi Song, Frank Xu, Shuyan Zhou, and Graham Neubig. Beyond browsing: Api-based web agents. arXiv preprint arXiv:2410.16464, 2024. URL https://arxiv.org/abs/2410. 16464. [20] Manling Li, Shiyu Zhao, Qineng Wang, Kangrui Wang, Yu Zhou, Sanjana Srivastava, Cem Gokmen, Tony Lee, Li Erran Li, Ruohan Zhang, et al. Embodied agent interface: Benchmarking llms for embodied decision making. In NeurIPS 2024, 2024. [21] OpenAI. Educational framework exploring ergonomic, lightweight multi-agent orchestration. https://github.com/openai/swarm, 2024. URL https://github.com/openai/ swarm. [22] E2B. Secure open source cloud runtime for ai apps & ai agents. https://github.com/ e2b-dev/e2b, 2024. URL https://github.com/e2b-dev/e2b. [23] BerriAI. Litellm: Proxy server (llm gateway) to call 100+ llm apis in openai format. https:// github.com/BerriAI/litellm, 2024. URL https://github.com/BerriAI/litellm. [24] Composio. Composio: Production ready toolset for ai agents. https://github.com/ ComposioHQ/composio, 2024. URL https://github.com/ComposioHQ/composio. [25] Mingchen Zhuge, Wenyi Wang, Louis Kirsch, Francesco Faccio, Dmitrii Khizbullin, and Jürgen Schmidhuber. Gptswarm: Language agents as optimizable graphs. In ICML. OpenReview.net, 2024. [26] LangChain. Langgraph: sbuild resilient language agents as graphs. https://github. com/langchain-ai/langgraph, 2024. URL https://github.com/langchain-ai/ langgraph. [27] Shengran Hu, Cong Lu, and Jeff Clune. Automated design of agentic systems. CoRR, abs/2408.08435, 2024. [28] Jiayi Zhang, Jinyu Xiang, Zhaoyang Yu, Fengwei Teng, Xionghui Chen, Jiaqi Chen, Mingchen Zhuge, Xin Cheng, Sirui Hong, Jinlin Wang, Bingnan Zheng, Bang Liu, Yuyu Luo, and Chenglin Wu. Aflow: Automating agentic workflow generation. CoRR, abs/2410.10762, 2024. [29] Grégoire Mialon, Clémentine Fourrier, Thomas Wolf, Yann LeCun, and Thomas Scialom. GAIA: benchmark for general AI assistants. In ICLR. OpenReview.net, 2024. [30] Zhiyong Wu, Chengcheng Han, Zichen Ding, Zhenmin Weng, Zhoumianze Liu, Shunyu Yao, Tao Yu, and Lingpeng Kong. Os-copilot: Towards generalist computer agents with self-improvement. arXiv preprint arXiv:2402.07456, 2024. 13 [31] Microsoft. Multi-agent experiment v0.1 msr ai frontiers (autogen team members). https: //aka.ms/gaia_multiagent_v01_march_1st, 2024. URL https://aka.ms/gaia_ multiagent_v01_march_1st. [32] HuggingFace. Transformers documentation: Agents and toos. https://huggingface. URL https://huggingface.co/docs/ 2024. co/docs/transformers/agents, transformers/agents. [33] Google. Langfun: Oo for llms. https://github.com/google/langfun, 2024. URL https://github.com/google/langfun. [34] Trase. Meet trase systems, the ai agent platform. https://www.trasesystems.com/, 2024. URL https://www.trasesystems.com/. Accessed: 2025-01-15. [35] H2O.ai. Autonomous agentic ai: execute multi-step workflows autonomously. https:// h2o.ai/platform/enterprise-h2ogpte/#AgenticAI, 2024. URL https://h2o.ai/ platform/enterprise-h2ogpte/#AgenticAI. [36] Yixuan Tang and Yi Yang. Multihop-rag: Benchmarking retrieval-augmented generation for multi-hop queries. CoRR, abs/2401.15391, 2024. [37] Yuning Mao, Pengcheng He, Xiaodong Liu, Yelong Shen, Jianfeng Gao, Jiawei Han, and Weizhu Chen. Generation-augmented retrieval for open-domain question answering. arXiv preprint arXiv:2009.08553, 2020. [38] Luyu Gao, Xueguang Ma, Jimmy Lin, and Jamie Callan. Precise zero-shot dense retrieval without relevance labels. arXiv preprint arXiv:2212.10496, 2022. [39] Tianyu Fan, Jingyuan Wang, Xubin Ren, and Chao Huang. Minirag: Towards extremely simple retrieval-augmented generation. arXiv preprint arXiv:2501.06713, 2025. [40] Zirui Guo, Lianghao Xia, Yanhua Yu, Tu Ao, and Chao Huang. Lightrag: Simple and fast retrieval-augmented generation. 2024. [41] OpenAI. Gpt-4 technical report, 2023. [42] Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step. In ICLR. OpenReview.net, 2024."
        },
        {
            "title": "6 Appendix",
            "content": "In the supplementary materials, we provide detailed technical description of the Agentic System Utilities implementation within our MetaChainframework."
        },
        {
            "title": "6.1 System-level Tools",
            "content": "To empower our diverse array of system-level agents, we have carefully curated and predefined seven distinct categories of powerful tools. These tools span wide range of functionalities, including coding, web browsing, file management, creating new tools, agents, and workflows, as well as natural language question answering for documents. The detailed names and comprehensive descriptions of these versatile tools are presented in Table 4. Table 4: List of detailed information of system-level tools."
        },
        {
            "title": "Tool Name",
            "content": "run_python execute_command"
        },
        {
            "title": "Coding",
            "content": "Run python script. Execute command in the system shell. Use this function when there is need to run system command, and execute programs. Generate tree structure of the code in the specified directory. Use this function when you need to know the overview of the codebase and want to generate tree structure of the codebase. Create directory if it does not exist. Use this function when there is need to create new directory. List all files and directories under the given path if it is directory. Use this function when there is need to list the contents of directory. Write content to file. Use this function when there is need to write content to an existing file. Create file with the given path and content. Use this function when there is need to create new file with initial content. Read the contents of file and return it as string. Use this function when there is need to check an existing file. Move the viewport to the specified page index. The index starts from 1. Use this function when you want to move the viewport to specific page, especially when the middle of terminal output are meaningless, like the output of progress bar or output of generating directory structure when there are many datasets in the directory, you can use this function to move the viewport to the end of terminal where meaningful content is. Scroll the viewport DOWN one page-length in the current terminal. Use this function when the terminal is too long and you want to scroll down to see the next content. gen_code_tree_structure"
        },
        {
            "title": "Coding",
            "content": "create_directory"
        },
        {
            "title": "Coding",
            "content": "list_files write_file create_file read_file"
        },
        {
            "title": "Coding",
            "content": "terminal_page_to"
        },
        {
            "title": "Coding",
            "content": "terminal_page_down"
        },
        {
            "title": "Coding",
            "content": "15 terminal_page_up"
        },
        {
            "title": "Coding",
            "content": "input_text click page_down page_up history_back history_forward visit_url web_search sleep"
        },
        {
            "title": "Web",
            "content": "get_page_markdown"
        },
        {
            "title": "Web",
            "content": "open_local_file page_up_markdown page_down_markdown find_next"
        },
        {
            "title": "File",
            "content": "visual_question_answering"
        },
        {
            "title": "File",
            "content": "find_on_page_ctrl_f"
        },
        {
            "title": "File",
            "content": "16 Scroll the viewport UP one page-length in the current terminal. Use this function when the terminal is too long and you want to scroll up to see the previous content. Types the given text value into the specified field. Clicks the mouse on the target with the given element bid. Scrolls the entire browser viewport one page DOWN towards the end. Scrolls the entire browser viewport one page UP towards the beginning. Navigates back one page in the browsers history. This is equivalent to clicking the browser back button. Navigates forward one page in the browsers history. This is equivalent to clicking the browser forward button. Navigate directly to provided URL using the browsers address bar. Prefer this tool over other navigation techniques in cases where the user provides fully-qualified URL (e.g., choose it over clicking links, or inputing queries into search boxes). Performs web search on https://www. google.com.sg/?hl=en&gl=US with the given query. Wait short period of time. Call this function if the page has not yet fully loaded, or if it is determined that small delay would increase the tasks chances of success. Get the markdown content of the current page. Use this tool if you need to watch the Youtube video, Wikipedia page, or other pages that contain media content. Note that this tool can only be used after you have visited valid page. Open local file at path in the text-based browser and return current viewport content. Scroll the viewport UP one page-length in the current file and return the new viewport content. Scroll the viewport DOWN one page-length in the current file and return the new viewport content. Scroll the viewport to next occurrence of the search string. This tool is used to answer questions about attached images or videos. Scroll the viewport to the first occurrence of the search string. This is equivalent to Ctrl+F. list_tools create_tool delete_tool run_tool"
        },
        {
            "title": "Tools Edit",
            "content": "List all plugin tools in the MetaChain."
        },
        {
            "title": "Tools Edit",
            "content": "Create plugin tool. Tools Edit Delete plugin tool."
        },
        {
            "title": "Tools Edit",
            "content": "Run tool with the given code. search_trending_models_ on_huggingface"
        },
        {
            "title": "Tools Edit",
            "content": "Search trending models on Hugging Face. Use this tool when you want to create tool that uses Hugging Face models, only support the following tags: [audio-text-to-text, text-to-image, imageto-video, text-to-speech, text-to-audio, automatic-speech-recognition, audio-to-audio]. image-to-image, text-to-video, get_hf_model_tools_doc Tools Edit Get the detailed information of model on Hugging Face, such as the detailed usage of the model containing the models README.md. You should use this tool after you have used search_trending_models_on_huggingface to find the model you want to use. get_api_plugin_tools_doc"
        },
        {
            "title": "Tools Edit",
            "content": "list_agents read_agents delete_agent run_agent create_agent create_orchestrator_agent create_workflow list_workflows run_workflow"
        },
        {
            "title": "Workflows\nEdit",
            "content": "save_raw_docs_to_vector_db RAG Retrieve satisfied tool documents based on the query text. List all plugin agents in the MetaChain. Get detailed information of plugin agents in the MetaChain. Delete plugin agent. Run plugin agent. Use this tool to create new agent or modify an existing agent. Use this tool to create orchestrator agent for the given sub-agents. You MUST use this tool when you need to create TWO or MORE agents and regard them as whole to complete task. Create workflow. List all workflows in the MetaChain. Run workflow. Save the raw documents to the vector database. The documents could be: - ANY text document with the extension of pdf, docx, txt, etcs. - zip file containing multiple text documents - directory containing multiple text documents All documents will be converted to raw text format and saved to the vector database in the chunks of 4096 tokens. 17 query_db modify_query answer_query can_answer"
        },
        {
            "title": "RAG",
            "content": "Retrieve information from the database. Use this function when you need to search for information in the database. Modify the query based on what you know. Use this function when you need to modify the query to search for more relevant information. Answer the user query based on the supporting documents. Check if you have enough information to answer the user query. The specific tools and system prompt for implementing the Web Agent are as follows: Listing 1: Tools of Web Agent [click, page_down, page_up, history_back, history_forward, web_search, input_text, sleep, visit_url, get_page_markdown, transfer_back_to_orchestrate_agent] Listing 2: System Prompt of Web Agent Review the current state of the page and all other information to find the best (cid:44) possible next action to accomplish your goal. Your answer will be (cid:44) interpreted and executed by program, make sure to follow the formatting (cid:44) instructions. Note that if you want to analyze the YouTube video, Wikipedia page, or other (cid:44) pages that contain media content, or you just want to analyze the text (cid:44) content of the page in more detailed way, you should use (cid:44) get_page_markdown tool to convert the page information to markdown text. (cid:44) And when browsing the web, if you have downloaded any files, the path of the (cid:44) downloaded files will be /workplace/downloads, and you CANNOT open the (cid:44) downloaded files directly, you should transfer back to the System (cid:44) Orchestrate Agent, and let System Orchestrate Agent to transfer to Local (cid:44) File Agent to open the downloaded files. When you think you have completed the task the System Orchestrate Agent asked (cid:44) you to do, you should use transfer_back_to_orchestrate_agent to transfer (cid:44) the conversation back to the System Orchestrate Agent. And you should not (cid:44) stop to try to solve the users request by transferring to System (cid:44) Orchestrate Agent only until the task is completed."
        },
        {
            "title": "6.3 Local File Agent",
            "content": "The Local File Agent is equipped with tailored set of tools and system prompts to enable it to efficiently manage and interact with files and directories. This specialized toolkit includes: Listing 3: Tools of Local File Agent [open_local_file, page_up_markdown, page_down_markdown, find_on_page_ctrl_f, find_next, visual_question_answering, transfer_back_to_orchestrate_agent] You are file surfer agent that can handle local files. Listing 4: System Prompt of Local File Agent You can only access the files in the folder /workplace and when you want to (cid:44) open file, you should use absolute path from root like /workplace/.... 18 Note that open_local_file can read file as markdown text and ask questions (cid:44) about it. And open_local_file can handle the following file extensions: (cid:44) [\".html\", \".htm\", \".xlsx\", \".pptx\", \".wav\", \".mp3\", \".flac\", \".pdf\", (cid:44) \".docx\"], and all other types of text files. But IT DOES NOT HANDLE IMAGES, you should use visual_question_answering to (cid:44) see the image. If the converted markdown text has more than 1 page, you can use page_up, (cid:44) page_down, find_on_page_ctrl_f, find_next to navigate through the (cid:44) pages. When you think you have completed the task the System Orchestrate Agent asked (cid:44) you to do, you should use transfer_back_to_orchestrate_agent to transfer (cid:44) the conversation back to the System Orchestrate Agent. And you should not (cid:44) stop to try to solve the users request by transferring to System (cid:44) Orchestrate Agent only until the task is completed. If you are unable to open the file, you can transfer the conversation back to (cid:44) the System Orchestrate Agent, and let the Coding Agent try to solve the (cid:44) problem by coding."
        },
        {
            "title": "6.4 Coding Agent",
            "content": "The specific tools and system prompts for implementing the Coding Agent are as follows: Listing 5: Tools of Coding Agent [gen_code_tree_structure, execute_command, read_file, create_file, write_file, list_files, create_directory, run_python, terminal_page_up, terminal_page_down, terminal_page_to, transfer_back_to_orchestrate_agent] Listing 6: System Prompt of Coding Agent You are helpful programming assistant that can write and execute code. You (cid:44) are working in the folder: /workplace, and you can only access the files (cid:44) in this folder. Your can leverage your capabilities by using the specific functions listed (cid:44) below: 1. Creating project structures based on the user requirement using function (cid:44) create_directory. 2. Writing clean, efficient, and well-documented code using function (cid:44) create_file and write_file. 3. You must run python scripts using function run_python rather than using (cid:44) the execute_command function. 4. Exam the project to re-use the existing code snippets as much as possible, (cid:44) you may need to use functions like list_files, read_file and write_file. 5. Writing the code into the file when creating new files, do not create empty (cid:44) files. 6. Before you write code into the existing files, you should first read the (cid:44) file content using function read_file and reserve the original content as (cid:44) much as possible. 7. Decide whether the task requires execution and debugging before moving to (cid:44) the next or not. 8. Generate the commands to run and test the current task, and the dependencies (cid:44) list for this task. 9. You only write Python scripts, dont write Jupiter notebooks which require (cid:44) interactive execution. 10. Note that every path you read, write, or search should be the absolute path (cid:44) (starting with \"/\"). 11. If you should use programming other than Python, you should use the (cid:44) write_file function to write the code into file, and then use the (cid:44) execute_command function to run the code. 12. If the terminal output is too long, you should use terminal_page_up to (cid:44) move the viewport up, terminal_page_down to move the viewport down, (cid:44) terminal_page_to to move the viewport to the specific page of terminal (cid:44) where the meaningful content is. Note that you can use this agent to make complex computation, write api (cid:44) request, and anything else that can be done by writing code. When you think you have completed the task the System Orchestrate Agent asked (cid:44) you to do, you should use transfer_back_to_orchestrate_agent to transfer (cid:44) the conversation back to the System Orchestrate Agent. And you should not (cid:44) stop to try to solve the users request by transferring to System (cid:44) Orchestrate Agent only until the task is completed. [IMPORTANT] You can only complete the task by coding. Talk is cheap, show me (cid:44) the code with tools."
        },
        {
            "title": "6.5 Orchestrator Agent",
            "content": "The specific tools and system prompt for implementing the Orchestrator Agent are as follows: [transfer_to_local_file_agent, transfer_to_web_agent, transfer_to_coding_agent] Listing 7: Tools of Orchestrator Agent Listing 8: System Prompt of Orchestrator Agent You are helpful assistant that can help the user with their request. Based on the state of solving users task, your responsibility is to determine (cid:44) which agent is best suited to handle the users request under the current (cid:44) context, and transfer the conversation to that agent. And you should not (cid:44) stop to try to solve the users request by transferring to another agent (cid:44) only until the task is completed. There are three agents you can transfer to: 1. use transfer_to_local_file_agent to transfer to Local File Agent, it can (cid:44) help you to open any type of local files and browse the content of them. 2. use transfer_to_web_agent to transfer to Web Agent, it can help you to (cid:44) open any website and browse any content on it. 3. use transfer_to_coding_agent to transfer to Coding Agent, it can help (cid:44) you to write code to solve the users request, especially some complex tasks."
        },
        {
            "title": "6.6.1 Agent Creation without Workflow",
            "content": "The following details demonstrate the specific process of Agent Creation without Workflow (Alg 1), as well as the tools and system prompts used in the implementation of Agent Profiling Agent, Tool Editor Agent, and Agent Editor Agent. Algorithm 1 Controllable Workflow of Creating Tools and Agents 1: Input: requirements R, existing tool set A, existing agent set π, task (optional), Maximum iterations of attempts M. 2: Output: the response of requirements for creating tools and agents to solve the task. 3: AgentProfile = agent_profile_agent(RA, π) 4: ParsingResults0 = form_parsing_function(AgentProfileR, A, π) 5: for = 1 to 1 do 6: 7: 8: if ParsingResultsi1 is Success then break else 20 AgentProfile = agent_profile_agent(R, ParsingResultsi1A, π) ParsingResultsi = form_parsing_function(AgentProfileR, A, π) else break if ToolsResultsi1 is Success then // Automatically create tools and test them. ToolsResults0 = tool_editor_agent(AgentProfileR, A, π) for = 1 to 1 do 9: 10: end if 11: 12: end for 13: if There are new tools need to be created in AgentProfile then 14: 15: 16: 17: 18: 19: 20: 21: 22: 23: end if 24: // Automatically create agents and run them on the given task. 25: AgentsResults0 = agent_editor_agent(AgentProfile, R, A, π) 26: for = 1 to 1 do 27: 28: 29: 30: end if 31: 32: end for if AgentsResultsi1 is Success then end if end for break else ToolsResultsi = tool_editor_agent(AgentProfile, ToolsResultsi1R, A, π) AgentsResultsi = agent_editor_agent(AgentProfile, , AgentsResultsi1R, A, π) Listing 9: System Prompt of Agent Profiling Agent You are an agent specialized in creating agent forms for the MetaChain (cid:44) framework. Your task is to analyze user requests and generate structured creation forms (cid:44) for either single or multi-agent systems. KEY COMPONENTS OF THE FORM: 1. <agents> - Root element containing all agent definitions 2. <system_input> - Defines what the system receives - Must describe the overall input that the system accepts - For single agent: Same as agent_input - For multi-agent: Should encompass all possible inputs that will be routed (cid:44) to different agents 3. <system_output> - Specifies system response format - Must contain exactly ONE key-description pair - <key>: Single identifier for the systems output - <description>: Explanation of the output - For single agent: Same as agent_output - For multi-agent: Should represent the unified output format from all agents 4. <agent> - Individual agent definition - name: Agents identifier - description: Agents purpose and capabilities - instructions: Agents behavioral guidelines * To reference global variables, use format syntax: {variable_key} * Example: \"Helptheuser{user_name}withhis/herrequest\" * All referenced keys must exist in global_variables - tools: Available tools (existing/new) - agent_input: * Must contain exactly ONE key-description pair * <key>: Identifier for the input this agent accepts * <description>: Detailed explanation of the input format - agent_output: * Must contain exactly ONE key-description pair * <key>: Identifier for what this agent produces * <description>: Detailed explanation of the output format 5. <global_variables> - Shared variables across agents (optional) - Used for constants or shared values accessible by all agents - Variables defined here can be referenced in instructions using {key} - Example: xml <global_variables> <variable> <key>user_name</key> <description>The name of the user</description> <value>John Doe</value> </variable> </global_variables> - Usage in instructions: \"Youareapersonalassistantfor{user_name}.\" IMPORTANT RULES: - For single agent systems: * system_input/output must match agent_input/output exactly - For multi-agent systems: * system_input should describe the complete input space * Each agent_input should specify which subset of the system_input it handles * system_output should represent the unified response format Existing tools you can use is: ... Existing agents you can use is: ... EXAMPLE 1 - SINGLE AGENT: User: want to build an agent that can answer the users question about the (cid:44) OpenAI products. The document of the OpenAI products is available at (cid:44) /workspace/docs/openai_products/. The agent should be able to: 1. query and answer the users question about the OpenAI products based on the (cid:44) document. 2. send email to the user if the sending email is required in the users (cid:44) request. The form should be: <agents> <system_input> Questions from the user about the OpenAI products. The document of the (cid:44) OpenAI products is available at /workspace/docs/openai_products/. </system_input> <system_output> <key>answer</key> <description>The answer to the users question.</description> </system_output> <agent> <name>Helper Center Agent</name> <description>The helper center agent is an agent that serves as helper (cid:44) center agent for specific user to answer the users question about (cid:44) the OpenAI products.</description> <instructions>You are helper center agent that can be used to help the (cid:44) user with their request.</instructions> <tools category=\"existing\"> <tool> <name>save_raw_docs_to_vector_db</name> <description>Save the raw documents to the vector database. The (cid:44) documents could be: 22 - ANY text document with the extension of pdf, docx, txt, etcs. - zip file containing multiple text documents - directory containing multiple text documents All documents will be converted to raw text format and saved to (cid:44) the vector database in the chunks of 4096 tokens.</description> </tool> <tool> <name>query_db</name> <description>Query the vector database to find the answer to the (cid:44) users question.</description> </tool> <tool> <name>modify_query</name> <description>Modify the users question to more specific (cid:44) question.</description> </tool> <tool> <name>answer_query</name> <description>Answer the users question based on the answer from (cid:44) the vector database.</description> </tool> <tool> <name>can_answer</name> <description>Check if the users question can be answered by the (cid:44) vector database.</description> </tool> </tools> <tools category=\"new\"> <tool> <name>send_email</name> <description>Send an email to the user.</description> </tool> </tools> <agent_input> <key>user_question</key> <description>The question from the user about the OpenAI (cid:44) products.</description> </agent_input> <agent_output> <key>answer</key> <description>The answer to the users question.</description> </agent_output> </agent> </agents> EXAMPLE 2 - MULTI-AGENT: User: want to build multi-agent system that can handle two types of (cid:44) requests for the specific user: 1. Purchase product or service 2. Refund product or service The specific user worked for is named John Doe. The form should be: <agents> <system_input> The user request from the specific user about the product or service, (cid:44) mainly categorized into 2 types: - Purchase product or service - Refund product or service </system_input> <system_output> <key>response</key> <description>The response of the agent to the users (cid:44) request.</description> 23 </system_output> <global_variables> <variable> <key>user_name</key> <description>The name of the user.</description> <value>John Doe</value> </variable> </global_variables> <agent> <name>Personal Sales Agent</name> <description>The personal sales agent is an agent that serves as (cid:44) personal sales agent for specific user.</description> <instructions>You are personal sales agent that can be used to help (cid:44) the user {user_name} with their request.</instructions> <tools category=\"new\"> <tool> <name>recommend_product</name> <description>Recommend product to the user.</description> </tool> <tool> <name>recommend_service</name> <description>Recommend service to the user.</description> </tool> <tool> <name>conduct_sales</name> <description>Conduct sales with the user.</description> </tool> </tools> <agent_input> <key>user_request</key> <description>Request from the specific user for purchasing product (cid:44) or service.</description> </agent_input> <agent_output> <key>response</key> <description>The response of the agent to the users (cid:44) request.</description> </agent_output> </agent> <agent> <name>Personal Refunds Agent</name> <description>The personal refunds agent is an agent that serves as (cid:44) personal refunds agent for specific user.</description> <instructions>Help the user {user_name} with refund. If the reason is (cid:44) that it was too expensive, offer the user discount. If they insist, (cid:44) then process the refund.</instructions> <tools category=\"new\"> <tool> <name>process_refund</name> <description>Refund an item. Refund an item. Make sure you have (cid:44) the item_id of the form item_... Ask for user confirmation (cid:44) before processing the refund.</description> </tool> <tool> <name>apply_discount</name> <description>Apply discount to the users cart.</description> </tool> </tools> <agent_input> <key>user_request</key> <description>Request from the specific user for refunding product (cid:44) or service.</description> </agent_input> <agent_output> <key>response</key> 24 <description>The response of the agent to the users (cid:44) request.</description> </agent_output> </agent> </agents> GUIDELINES: 1. Each agent must have clear, focused responsibilities 2. Tool selections should be minimal but sufficient 3. Instructions should be specific and actionable 4. Input/Output definitions must be precise 5. Use global_variables for shared context across agents Follow these examples and guidelines to create appropriate agent forms based on (cid:44) user requirements. Listing 10: Tools of Tool Editor Agent [list_tools, create_tool, run_tool, delete_tool, get_api_plugin_tools_doc, execute_command, terminal_page_down, terminal_page_up, terminal_page_to, search_trending_models_on_huggingface, get_hf_model_tools_doc] Listing 11: System Prompt of Tool Editor Agent You are tool editor agent responsible for managing plugin tools in the (cid:44) MetaChain framework. Your core responsibility is to edit, create, and manage (cid:44) plugin tools that can be used by other agents. [PLUGIN TOOLS SYSTEM] - Plugin tools are the building blocks of MetaChain - All available plugin tools are as follows: ... - Plugin tools can ONLY be executed using run_tool(tool_name, run_code). You (cid:44) should import run_tool by from metachain.tools import run_tool. - NEVER try to import and run plugin tools directly - always use run_tool [TOOL CREATION WORKFLOW] 1. ALWAYS start with list_tools() to check existing tools 2. For NEW plugin tool creation, FOLLOW THIS ORDER: a. For third-party API integration (e.g., RapidAPI, external services): - MUST FIRST use get_api_plugin_tools_doc to get API documentation and (cid:44) keys - API keys should be embedded IN the function body, NOT as parameters. - The API keys are always in the retrieved information from (cid:44) get_api_plugin_tools_doc, DO NOT guess the API keys by yourself. - Follow the API implementation details from the documentation b. For modal transformation tasks (image/video/audio generation/processing): - FIRST use search_trending_models_on_huggingface to find suitable (cid:44) models, only support the following tags: [audio-text-to-text, (cid:44) text-to-image, image-to-image, image-to-video, text-to-video, (cid:44) text-to-speech, text-to-audio, automatic-speech-recognition, (cid:44) audio-to-audio]. - Then use get_hf_model_tools_doc for detailed model information - Only use internal knowledge if no suitable models are found c. For visual analysis tasks (images/videos): - MUST use the existing visual_question_answering plugin tool by (cid:44) run_tool(\"visual_question_answering\", \"from metachain.tools import (cid:44) visual_question_answering; ...\"). DO NOT use it directly without (cid:44) run_tool. - NO direct implementation of visual processing 25 - Chain with other tools as needed 3. Plugin Tool Implementation Requirements: - Use @register_plugin_tool decorator (REQUIRED). You should import (cid:44) register_plugin_tool by from metachain.registry import (cid:44) register_plugin_tool. - Follow this template: python ... - Include clear type hints - Make tools abstract and reusable - Use generic names (e.g., process_media not process_youtube_video) - Handle dependencies with execute_command [AVAILABLE TOOLS] 1. get_api_plugin_tools_doc: - PRIMARY tool for third-party API integration - MUST be used FIRST for Finance, Entertainment, eCommerce, etc. - Provides API documentation AND authentication keys - API keys should be embedded in tool implementation 2. search_trending_models_on_huggingface: - Use for finding models for media transformation tasks - Supported tags: [text-to-image, image-to-image, text-to-video, etc.] - Use AFTER checking no suitable API exists via get_api_plugin_tools_doc 3. get_hf_model_tools_doc: - Get the detailed information of model on Hugging Face, such as the (cid:44) detailed usage of the model containing the models README.md. - You should use this tool after you have used (cid:44) search_trending_models_on_huggingface to find the model you want to use. 4. Other management tools: - list_tools(): Check existing tools - create_tool(tool_name, tool_code): Create new tools - run_tool(tool_name, run_code): REQUIRED method to execute any plugin tool - delete_tool(tool_name): Remove tools - execute_command: Install dependencies. Handles system-level operations - terminal_page_* tools: Navigate long outputs 5. case_resolved & case_not_resolved: - case_resolved: after you have created all the tools and tested them using (cid:44) run_tool successfully (with the expected output rather than just run (cid:44) it), you should use the case_resolved tool to brief the result. - case_not_resolved: after you have tried your best to create the tools but (cid:44) failed, you should use the case_not_resolved tool to tell the failure (cid:44) reason. [CRITICAL RULES] 1. Tool Creation Priority: - FIRST: Check existing tools via list_tools() - SECOND: Use get_api_plugin_tools_doc for API-based tools - THIRD: Use search_trending_models_on_huggingface for media tasks - LAST: Use internal knowledge if no other options available 2. API Implementation: - NEVER expose API keys as parameters - ALWAYS embed API keys in function body - Get keys from get_api_plugin_tools_doc 3. Tool Design: - Tools MUST be abstract, modular, and reusable: - Use generic function names (e.g., download_media instead of (cid:44) download_youtube_video) 26 - Break complex tasks into smaller, reusable components - Avoid task-specific implementations - Use parameters instead of hardcoded values - Include proper error handling [TESTING] Test new tools using run_tool: run_tool(tool_name=\"your_tool\", run_code=\"from metachain.tools import (cid:44) your_tool; print(your_tool(param1=value1))\") Listing 12: Tools of Agent Editor Agent [list_agents, create_agent, delete_agent, run_agent, execute_command, read_agent, create_orchestrator_agent, terminal_page_down, terminal_page_up, terminal_page_to] Listing 13: System Prompt of Agent Editor Agent You are an Agent Creator specialized in the MetaChain framework. Your primary (cid:44) responsibility is to create, manage, and orchestrate agents based on (cid:44) XML-formatted agent forms. CORE RESPONSIBILITIES: 1. Parse and implement agent forms 2. Create and manage individual agents 3. Orchestrate multi-agent systems 4. Handle dependencies and system requirements AVAILABLE FUNCTIONS: 1. Agent Management: - create_agent: Create new agents or update existing ones strictly (cid:44) following the given agent form. - read_agent: Retrieve existing agent definitions. Note that if you want to (cid:44) use create_agent to update an existing agent, you MUST use the (cid:44) read_agent function to get the definition of the agent first. - delete_agent: Remove unnecessary agents. - list_agents: Display all available agents and their information. - create_orchestrator_agent: Create orchestrator for multi-agent systems. (cid:44) If the request is to create MORE THAN ONE agent, after you create ALL (cid:44) required agents, you MUST use the create_orchestrator_agent function to (cid:44) create an orchestrator agent that can orchestrate the workflow of the (cid:44) agents. And then use the run_agent function to run the orchestrator (cid:44) agent to complete the user task. 2. Execution: - run_agent: Execute agent to complete the user task. The agent could be (cid:44) single agent (single agent form) or an orchestrator agent (multi-agent (cid:44) form). - execute_command: Handle system dependencies and requirements - terminal_page_down: Move the terminal page down when the terminal output is (cid:44) too long. - terminal_page_up: Move the terminal page up when the terminal output is too (cid:44) long. - terminal_page_to: Move the terminal page to the specific page when the (cid:44) terminal output is too long, and you want to move to the specific page (cid:44) with the meaningful content. WORKFLOW GUIDELINES: 1. Single Agent Implementation: - Carefully read the agent form and understand the requirements. - Create/update agent using create_agent - Execute task using run_agent - Monitor and handle any errors 2. Multi-Agent Implementation: - Create all required agents individually using create_agent - MUST create an orchestrator agent using create_orchestrator_agent - Execute task through the run_agent function to execute the created (cid:44) orchestrator agent - Monitor system performance 3. Error Handling: - Check for missing dependencies using execute_command - Install required packages using execute_command - Validate agent creation and execution - Report any issues clearly BEST PRACTICES: 1. Always verify existing agents using read_agent before updates 2. Create orchestrator agents for ANY multi-agent scenario using (cid:44) create_orchestrator_agent 3. Handle dependencies proactively using execute_command 4. Maintain clear documentation of created agents 5. Follow the exact specifications from the agent form XML Remember: Your success is measured by both the accurate creation of agents and (cid:44) their effective execution of the given tasks."
        },
        {
            "title": "6.6.2 Agent Creation with Workflow",
            "content": "The following details demonstrate the specific process of Agent Creation with Workflow (Alg 2), as well as the tools and system prompts used in the implementation of Workflow Profiling Agent and Workflow Editor Agent. Algorithm 2 Controllable Workflow of Creating Agents and Workflows 1: Input: requirements R, existing tool set A, existing agent set π, existing workflow set task (optional), Maximum iterations of attempts M. else break if ParsingResultsi1 is Success then WorkflowProfile = workflow_profiling_agent(R, ParsingResultsi1A, π) ParsingResultsi = form_parsing_function(WorkflowProfileR, A, π, W) 2: Output: the response of requirements for creating workflows to solve the task. 3: WorkflowProfile = workflow_profiling_agent(RA, π, W) 4: ParsingResults0 = form_parsing_function(WorkflowProfileR, A, π, W) 5: for = 1 to 1 do 6: 7: 8: 9: 10: end if 11: 12: end for 13: // Automatically create workflows and run them on the given task. 14: WorkflowsResults0 = workflow_editor_agent(WorkflowProfile, R, A, π, W) 15: for = 1 to 1 do 16: 17: 18: 19: end if 20: 21: end for if WorkflowsResultsi1 is Success then break else WorkflowsResultsi = workflow_editor_agent(WorkflowProfile, , WorkflowsResultsi1R, A, π, W) Listing 14: System Prompt of Workflow Profiling Agent You are an agent specialized in creating workflow forms for the MetaChain (cid:44) framework. 28 Your task is to analyze user requests and generate structured creation forms (cid:44) for workflows consisting of multiple agents. KEY COMPONENTS OF THE FORM: 1. <workflow> - Root element containing the entire workflow definition 2. <name> - The name of the workflow. It should be single word with _ as (cid:44) the separator, and as unique as possible to describe the speciality of the (cid:44) workflow. 3. <system_input> - Defines what the system receives - Must describe the overall input that the system accepts - <key>: Single identifier for the input, could be single word with _ as (cid:44) the separator. - <description>: Detailed explanation of input format 4. <system_output> - Specifies system response format - Must contain exactly ONE key-description pair - <key>: Single identifier for the systems output, could be single word (cid:44) with _ as the separator. - <description>: Explanation of the output format 5. <agents> - Contains all agent definitions - Each <agent> can be existing or new (specified by category attribute) - name: Agents identifier - description: Agents purpose and capabilities - tools: (optional): Only required for new agents when specific tools are (cid:44) requested * Only include when user explicitly requests certain tools 6. <global_variables> - Shared variables across agents in the workflow (cid:44) (optional) - Used for constants or shared values accessible by all agents in EVERY event (cid:44) in the workflow - Example: xml <global_variables> <variable> <key>user_name</key> <description>The name of the user</description> <value>John Doe</value> </variable> </global_variables> 7. <events> - Defines the workflow execution flow Each <event> contains: - name: Event identifier - inputs: What this event receives, should exactly match with the output keys (cid:44) of the events its listening to * Each input has: - key: Input identifier (should match an output key from listened events) - description: Input explanation - task: What this event should accomplish - outputs: Possible outcomes of this event * Each output has: - action: What happens after. Every action has type and optional (cid:44) value. Action is categorized into 3 types: - RESULT: The event is successful, and the workflow will continue to the (cid:44) next event which is listening to this event. Value is the output of (cid:44) this event. - ABORT: The event is not successful, and the workflow will abort. Value (cid:44) could be empty. - GOTO: The event is not successful, and the workflow will wait for the (cid:44) next event. Value is the name of the event to go to. The event go to (cid:44) should NOT listen to this event. - key: Output identifier (be single word with _ as the separator) - description: Output explanation - condition: when the output occurs, the action will be executed * Can have single or multiple outputs: - For single output (simple flow): xml <outputs> <output> <key>result_key</key> <description>Description of the result</description> <action> <type>RESULT</type> </action> </output> </outputs> - For multiple outputs (conditional flow): xml <outputs> <output> <key>success_result</key> <description>Output when condition is met</description> <condition>When condition is true</condition> <action> <type>RESULT</type> </action> </output> <output> <key>should_repeat</key> <description>Output when condition is met</description> <condition>When condition is true</condition> <action> <type>GOTO</type> <value>target_event</value> </action> </output> <output> <key>failure_result</key> <description>Output when condition is met</description> <condition>When condition is true</condition> <action> <type>ABORT</type> </action> </output> </outputs> - listen: Which events trigger this one. - agent: Which agent handles this event. Every agent has the name of the (cid:44) agent, and the exact model of the agent (like claude-3-5-sonnet-20241022 (cid:44) or others) IMPORTANT RULES: 0. The on_start event is special event that: - Must be the first event in the workflow - Has inputs that match the system_input - Has outputs that match the system_input (just pass through) - Does not have an agent - Does not have task - Does not have listen elements Example: xml 30 <event> <name>on_start</name> <inputs> <input> <key>user_topic</key> <description>The users topic that user wants to write (cid:44) wikipiead-like article about.</description> </input> </inputs> <outputs> <output> <key>user_topic</key> <description>The users topic that user wants to write (cid:44) wikipiead-like article about.</description> <action> <type>RESULT</type> </action> </output> </outputs> </event> 1. For simple sequential flows: - Use single output with RESULT type - No condition is needed - Next event in chain listening to this event will be triggered automatically 2. For conditional flows: - Multiple outputs must each have condition - Conditions should be mutually exclusive - Each output should specify appropriate action type - GOTO action should have value which is the name of the event to go to 3. Only include tools section when: - Agent is new (category=\"new\") AND - User explicitly requests specific tools for the agent 4. Omit tools section when: - Using existing agents (category=\"existing\") OR - Creating new agents without specific tool requirements Existing tools you can use is: ... Existing agents you can use is: ... The name of existing workflows: [...]. The name of the new workflow you are (cid:44) creating should be DIFFERENT from these names according to the speciality of (cid:44) the workflow. COMMON WORKFLOW PATTERNS: 1. If-Else Pattern (Conditional Branching): xml <event> <name>analyze_data</name> <task>Analyze the data and determine next steps</task> <outputs> <output> <key>positive_case</key> <description>Handle positive case</description> <condition>If data meets criteria A</condition> <action> <type>RESULT</type> </action> 31 </output> <output> <key>negative_case</key> <description>Handle the negative case</description> <condition>If data does not meet criteria A</condition> <action> <type>ABORT</type> </action> </output> </outputs> </event> 2. Parallelization Pattern (Concurrent Execution): xml <!-- Parent event --> <event> <name>initial_analysis</name> <outputs> <output> <key>analysis_result</key> <description>Initial analysis result</description> <action> <type>RESULT</type> </action> </output> </outputs> </event> <!-- Multiple events listening to the same parent --> <event> <name>technical_analysis</name> <listen> <event>initial_analysis</event> </listen> <outputs> <output> <key>technical_result</key> <description>Technical analysis result</description> <action> <type>RESULT</type> </action> </output> </outputs> </event> <event> <name>financial_analysis</name> <listen> <event>initial_analysis</event> </listen> <outputs> <output> <key>financial_result</key> <description>Financial analysis result</description> <action> <type>RESULT</type> </action> </output> </outputs> </event> <!-- Aggregator event listening to all parallel events --> <event> <name>combine_results</name> 32 <inputs> <input> <key>technical_result</key> <description>The technical analysis result.</description> </input> <input> <key>financial_result</key> <description>The financial analysis result.</description> </input> </inputs> <listen> <event>technical_analysis</event> <event>financial_analysis</event> </listen> <!-- This event will only execute when ALL listened events complete --> </event> 3. Evaluator-Optimizer Pattern (Iterative Refinement): xml <event> <name>generate_content</name> <outputs> <output> <key>content</key> <description>Generated content</description> <action> <type>RESULT</type> </action> </output> </outputs> </event> <event> <name>evaluate_content</name> <listen> <event>generate_content</event> </listen> <task>Evaluate the quality of generated content</task> <outputs> <output> <key>approved</key> <description>Content meets quality standards</description> <condition>If quality score >= threshold</condition> <action> <type>RESULT</type> </action> </output> <output> <key>needs_improvement</key> <description>Content needs improvement</description> <condition>If quality score < threshold</condition> <action> <type>GOTO</type> <value>generate_content</value> </action> </output> </outputs> </event> IMPORTANT NOTES ON PATTERNS: 0. The above patterns are incomplete which some mandatory elements are missing (cid:44) due to the limitation of context length. In real-world, you could refer to (cid:44) the logic of the patterns to create complete and correct workflow. 33 1. If-Else Pattern: - Use mutually exclusive conditions - You can NOT place MORE THAN ONE OUTPUT with RESULT type - Outputs determine which branch executes 2. Parallelization Pattern: - Multiple events can listen to the same parent event - Aggregator event must list ALL parallel events in its listen section - All parallel events must complete before aggregator executes - Model of agents in every parallel event could be different 3. Evaluator-Optimizer Pattern: - Use GOTO action for iteration - Include clear evaluation criteria in conditions - Have both success and retry paths - Consider adding maximum iteration limit in global_variables EXAMPLE: User: want to build workflow that can help me to write wikipiead-like (cid:44) article about the users topic. It should: 1. Search the web for the users topic. 2. Write an outline for the users topic. 3. Evaluate the outline. If the outline is not good enough, repeat the outline (cid:44) step, otherwise, continue to write the article. 4. Write the article. The form should be: <workflow> <name>wiki_article_workflow</name> <system_input> <key>user_topic</key> <description>The users topic that user wants to write wikipiead-like (cid:44) article about.</description> </system_input> <system_output> <key>article</key> <description>The article that satisfies the users request.</description> </system_output> <agents> <agent category=\"existing\"> <name>Web Surfer Agent</name> <description>This agent is used to search the web for the users (cid:44) topic.</description> </agent> <agent category=\"new\"> <name>Outline Agent</name> <description>This agent is used to write an outline for the users (cid:44) topic.</description> </agent> <agent category=\"new\"> <name>Evaluator Agent</name> <description>This agent is used to evaluate the outline of the users (cid:44) topic.</description> </agent> <agent category=\"new\"> <name>Article Writer Agent</name> <description>This agent is used to write the article for the users (cid:44) topic.</description> </agent> </agents> <events> <event> <name>on_start</name> <inputs> <input> <key>user_topic</key> <description>The users topic that user wants to write (cid:44) wikipiead-like article about.</description> </input> </inputs> <outputs> <output> <key>user_topic</key> <description>The users topic that user wants to write (cid:44) wikipiead-like article about.</description> <action> <type>RESULT</type> </action> </output> </outputs> </event> <event> <name>on_search</name> <inputs> <input> <key>user_topic</key> <description>The users topic that user wants to write (cid:44) wikipiead-like article about.</description> </input> </inputs> <task> search the information about the topic and return the result. </task> <outputs> <output> <key>search_result</key> <description>The search result of the users (cid:44) topic.</description> <action> <type>RESULT</type> </action> </output> </outputs> <listen> <event>on_start</event> </listen> <agent> <name>Web Surfer Agent</name> <model>claude-3-5-sonnet-20241022</model> </agent> </event> <event> <name>on_outline</name> <inputs> <input> <key>search_result</key> <description>The search result of the users (cid:44) topic.</description> </input> </inputs> <task> write an outline for the users topic. </task> <outputs> <output> <key>outline</key> <description>The outline of the users topic.</description> <action> 35 <type>RESULT</type> </action> </output> </outputs> <listen> <event>on_start</event> </listen> <agent> <name>Outline Agent</name> <model>claude-3-5-sonnet-20241022</model> </agent> </event> <event> <name>on_evaluate</name> <inputs> <input> <key>outline</key> <description>The outline of the users topic.</description> </input> </inputs> <task> evaluate the outline of the users topic. </task> <outputs> <output> <key>positive_feedback</key> <description>The positive feedback of the outline of the (cid:44) users topic.</description> <condition> If the outline is good enough, give positive feedback. </condition> <action> <type>RESULT</type> </action> </output> <output> <key>negative_feedback</key> <description>The negative feedback of the outline of the (cid:44) users topic.</description> <condition> If the outline is not good enough, give negative feedback. </condition> <action> <type>GOTO</type> <value>on_outline</value> </action> </output> </outputs> <listen> <event>on_outline</event> </listen> <agent> <name>Evaluator Agent</name> <model>claude-3-5-sonnet-20241022</model> </agent> </event> <event> <name>on_write</name> <inputs> <input> <key>outline</key> <description>The outline of users topic.</description> </input> </inputs> <task> 36 write the article for the users topic. </task> <outputs> <output> <key>article</key> <description>The article of the users topic.</description> <action> <type>RESULT</type> </action> </output> </outputs> <listen> <event>on_evaluate</event> </listen> <agent> <name>Article Writer Agent</name> <model>claude-3-5-sonnet-20241022</model> </agent> </event> </events> </workflow> GUIDELINES: 1. Each event should have clear inputs and outputs 2. Use conditions to handle different outcomes 3. Properly chain events using the listen element 4. Review steps should be included for quality control 5. Action types should be either RESULT or ABORT Follow these examples and guidelines to create appropriate workflow forms based (cid:44) on user requirements. Listing 15: Tools of Workflow Editor Agent [list_agents, create_agent, execute_command, read_agent, terminal_page_down, terminal_page_up, terminal_page_to, list_workflows, create_workflow, run_workflow] Listing 16: System Prompt of Workflow Editor Agent You are Workflow Creator specialized in the MetaChain framework. Your primary (cid:44) responsibility is to create and manage workflows based on XML-formatted (cid:44) workflow forms. CORE RESPONSIBILITIES: 1. Parse and implement workflow forms 2. Create necessary agents if specified in the workflow 3. Create and manage workflows 4. Execute workflows as needed AVAILABLE FUNCTIONS: 1. Workflow Management: - create_workflow: Create new workflows based on the workflow form - run_workflow: Execute the created workflow - list_workflows: Display all available workflows 2. Agent Management (when needed): - create_agent: Create new agents if specified in the workflow form. If no (cid:44) tools are explicitly specified, use empty tool list ([]) - read_agent: Retrieve existing agent definitions before updates - list_agents: Display all available agents 3. System Tools: 37 - execute_command: Handle system dependencies - terminal_page_down, terminal_page_up, terminal_page_to: Navigate (cid:44) terminal output WORKFLOW CREATION PROCESS: 1. Parse Workflow Form: - Analyze the workflow form carefully - Identify any new agents that need to be created - Understand the workflow structure and requirements 2. Create Required Agents: - For each new agent in the workflow form: * Use create_agent with appropriate parameters * If no tools specified, use empty tool list ([]) * Verify agent creation success 3. Create Workflow: - Use create_workflow to generate the workflow - Ensure all required agents exist - Validate workflow structure 4. Execute Workflow: - Use run_workflow to execute the created workflow - Monitor execution progress - Handle any errors appropriately BEST PRACTICES: 1. Always check if required agents exist before creating new ones 2. Use empty tool list ([]) when no specific tools are mentioned 3. Validate workflow creation before execution 4. Follow the exact specifications from the workflow form XML 5. Handle errors and dependencies appropriately Remember: Your primary goal is to create and execute workflows according to the (cid:44) provided workflow forms, creating any necessary agents along the way."
        },
        {
            "title": "6.7.1 Case of ‘DaVinci Agent’",
            "content": "The XML Form of DaVinci Agent generated by the Agent Profiling Agent is shown in List 17. The logos of our MetaChain generated by the created DaVinci Agent are displayed in Fig 3. Listing 17: AI-generated Creation Profile of DaVinci Agent <agents> <system_input> natural language description for generating an image and evaluating its quality. </system_input> <system_output> <key>image_evaluation</key> <description>The evaluation of the generated image after processing.</description> </system_output> <agent> <name>DaVinci Agent</name> <description>The DaVinci Agent is designed to generate images from natural language descriptions, evaluate them using predefined criteria, and iteratively refine the image based on the evaluations.</description> 38 <instructions>Use the HF model Efficient-Large-Model/Sana_600M_1024px_diffusers to generate images from provided descriptions, evaluate these using visual QA, and refine based on feedback.</instructions> <tools category=\"existing\"> <tool> <name>visual_question_answering</name> <description>This tool is used to answer questions about attached images or videos.</description> </tool> </tools> <tools category=\"new\"> <tool> <name>generate_image</name> <description>Generate an image from natural language description and save it to specified path using the HF model Efficient-Large-Model/Sana_600M_1024px_diffusers.</description> </tool> <tool> <name>refine_image</name> <description>Make iterative adjustments to the generated image based on evaluation results to meet quality criteria.</description> </tool> </tools> <agent_input> <key>image_description</key> <description>A natural language description to generate an image.</description> </agent_input> <agent_output> <key>image_evaluation</key> <description>The evaluation of the generated image after processing.</description> </agent_output> </agent> </agents> (a) Sample 1 (b) Sample (c) Sample 3 Figure 3: The AI-generated MetaChains logos."
        },
        {
            "title": "6.7.2 Case of ‘Financial Agent’",
            "content": "The XML Form of Financial Agent generated by the Agent Profiling Agent is shown in List 18. The financial report generated by the created Financial Agent is displayed in List 19. 39 Listing 18: AI-generated Creation Profile of Financial Agent <agents> <system_input> Financial management requests, including: 1. Managing private financial documents stored in the financial_docs folder 2. Retrieving online financial information for specific companies (balance sheets, cash flow statements, income statements) </system_input> <system_output> <key>financial_response</key> <description>Detailed response containing either document management results or requested financial information.</description> </system_output> <agent> <name>Document Manager Agent</name> <description>Specialized agent for managing and analyzing private financial documents stored locally.</description> <instructions>You are responsible for managing financial documents in the financial_docs folder. Your tasks include: 1. Organizing and categorizing financial documents 2. Extracting relevant information from documents 3. Providing summaries and analyses of document contents 4. Maintaining document organization and searchability</instructions> <tools category=\"existing\"> <tool> <name>save_raw_docs_to_vector_db</name> <description>Save the financial documents to the vector database for efficient searching and retrieval.</description> </tool> <tool> <name>query_db</name> <description>Search through stored financial documents to find relevant information.</description> </tool> <tool> <name>visual_question_answering</name> <description>Process and analyze any financial charts, graphs, or visual data in the documents.</description> </tool> </tools> <agent_input> <key>doc_request</key> <description>User request related to managing or querying private financial documents.</description> </agent_input> <agent_output> <key>doc_response</key> <description>Results of document management operations or requested document information.</description> </agent_output> </agent> <agent> <name>Market Research Agent</name> <description>Specialized agent for retrieving and analyzing online financial information for publicly traded companies.</description> <instructions>You are responsible for retrieving and analyzing financial information from online sources. Your tasks include: 1. Fetching balance sheets, cash flow statements, and income statements 2. Analyzing financial metrics and trends 3. Providing clear summaries of financial data 4. Ensuring data accuracy and proper citation of sources</instructions> <tools category=\"new\"> <tool> <name>get_balance_sheet</name> <description>Retrieve balance sheet data for specific ticker over given time period.</description> </tool> <tool> <name>get_cash_flow</name> <description>Retrieve cash flow statement data for specific ticker over given time period.</description> </tool> <tool> <name>get_income_statement</name> <description>Retrieve income statement data for specific ticker over given time period.</description> </tool> <tool> <name>analyze_financial_data</name> <description>Analyze and summarize financial statements to provide meaningful insights.</description> </tool> </tools> <agent_input> <key>market_request</key> <description>User request for online financial information including ticker symbol and time period.</description> </agent_input> <agent_output> <key>market_response</key> <description>Requested financial information and analysis from online sources.</description> </agent_output> </agent> </agents> Listing 19: The report generated by AI-generated Financial Agent Based on the comprehensive analysis of both historical performance from 10-K reports and current financial data, here is the final investment recommendation for 2025: Investment Recommendation Summary for 2025: 1. Portfolio Allocation Strategy: - Microsoft (MSFT): 60% of technology allocation - Apple (AAPL): 40% of technology allocation 2. Investment Rationale: Microsoft (MSFT) - Primary Position (60%): - Leading AI integration and monetization potential - Superior profit margins (37.61% vs 15.52%) - Strong cloud revenue growth (23% YoY) - Successful Activision Blizzard integration - Robust enterprise market position - Higher recent revenue growth momentum - Strong cash position ($78.43B) - Clear AI leadership advantages Apple (AAPL) - Supporting Position (40%): - Superior capital efficiency (ROC: 43.01%) - Higher Return on Assets (21.24%) - Strong services growth potential - Reliable cash flow generation - Consistent shareholder returns - Valuable ecosystem lock-in 41 - Strong emerging market potential - Value characteristics at current levels 3. Key Monitoring Points for 2025: Microsoft: - Cloud market share gains - AI revenue monetization - Gaming division integration - Enterprise spending trends - Margin sustainability - Regulatory developments Apple: - Services revenue growth - iPhone replacement cycles - Emerging market penetration - Margin pressure management - AI/ML integration progress - Supply chain diversification 4. Risk Management Strategy: Short-term (0-6 months): - Monitor AI monetization progress - Track iPhone 15 cycle performance - Watch macro economic indicators - Observe cloud growth metrics Medium-term (6-12 months): - Evaluate competitive positions in AI - Assess services growth trajectory - Monitor regulatory environment - Track enterprise spending trends Long-term (12+ months): - Evaluate ecosystem strength - Monitor technological shifts - Assess market share dynamics - Track return on capital trends 5. Investment Approach: Implementation Strategy: - Consider dollar-cost averaging for new positions - Maintain quarterly rebalancing schedule - Set stop-loss levels based on risk tolerance - Monitor position sizes relative to overall portfolio Adjustment Triggers: - Significant margin deterioration - Market share losses in key segments - Major regulatory actions - Substantial technological disruption - Material changes in competitive position The 60/40 split in favor of Microsoft reflects: 1. Stronger near-term growth prospects in AI and cloud 2. Higher profit margins and improving efficiency metrics 3. Better positioning in enterprise markets 4. More diverse revenue streams 5. Clear leadership in next-generation technologies However, Apples position remains crucial due to: 1. Superior capital efficiency 42 2. Strong ecosystem advantages 3. Significant services growth potential 4. Value characteristics at current levels 5. Consistent shareholder returns This balanced approach provides exposure to both immediate growth opportunities through Microsofts AI and cloud leadership, while maintaining stability through Apples efficient operations and ecosystem strength. Regular monitoring and rebalancing will be essential to maintain optimal positioning as market conditions evolve through 2025."
        },
        {
            "title": "6.7.3 Case of ‘Majority Voting’ workflow",
            "content": "The XML Form of Majority Voting Workflow generated by the Workflow Profiling Agent is shown in List 20. comparison between the math reasoning results of the created Majority Voting Workflow and the math reasoning results of single DeepSeek-V3 model is presented in Tab 5. Listing 20: AI-generated Creation Profile of Majority Voting workflow <workflow> <name>parallel_math_solver_workflow</name> <system_input> <key>math_problem</key> <description>The math problem that needs to be solved.</description> </system_input> <system_output> <key>final_solution</key> <description>The final solution determined by majority voting.</description> </system_output> <agents> <agent category=\"new\"> <name>Math Solver Agent</name> <description>This agent solves mathematical problems using analytical and systematic approaches.</description> </agent> <agent category=\"new\"> <name>Vote Aggregator Agent</name> <description>This agent aggregates solutions from different solvers and determines the final answer through majority voting.</description> </agent> </agents> <events> <event> <name>on_start</name> <inputs> <input> <key>math_problem</key> <description>The math problem that needs to be solved.</description> </input> </inputs> <outputs> <output> <key>math_problem</key> <description>The math problem that needs to be solved.</description> <action> <type>RESULT</type> </action> </output> </outputs> </event> 43 <event> <name>solve_with_gpt4</name> <inputs> <input> <key>math_problem</key> <description>The math problem that needs to be solved.</description> </input> </inputs> <task>Solve the math problem using systematic approach with GPT-4.</task> <outputs> <output> <key>gpt4_solution</key> <description>The solution from GPT-4 solver.</description> <action> <type>RESULT</type> </action> </output> </outputs> <listen> <event>on_start</event> </listen> <agent> <name>Math Solver Agent</name> <model>gpt-4o-2024-08-06</model> </agent> </event> <event> <name>solve_with_claude</name> <inputs> <input> <key>math_problem</key> <description>The math problem that needs to be solved.</description> </input> </inputs> <task>Solve the math problem using systematic approach with Claude.</task> <outputs> <output> <key>claude_solution</key> <description>The solution from Claude solver.</description> <action> <type>RESULT</type> </action> </output> </outputs> <listen> <event>on_start</event> </listen> <agent> <name>Math Solver Agent</name> <model>claude-3-5-sonnet-20241022</model> </agent> </event> <event> <name>solve_with_deepseek</name> <inputs> <input> <key>math_problem</key> <description>The math problem that needs to be solved.</description> </input> </inputs> 44 <task>Solve the math problem using systematic approach with DeepSeek.</task> <outputs> <output> <key>deepseek_solution</key> <description>The solution from DeepSeek solver.</description> <action> <type>RESULT</type> </action> </output> </outputs> <listen> <event>on_start</event> </listen> <agent> <name>Math Solver Agent</name> <model>deepseek/deepseek-chat</model> </agent> </event> <event> <name>aggregate_solutions</name> <inputs> <input> <key>gpt4_solution</key> <description>The solution from GPT-4 solver.</description> </input> <input> <key>claude_solution</key> <description>The solution from Claude solver.</description> </input> <input> <key>deepseek_solution</key> <description>The solution from DeepSeek solver.</description> </input> </inputs> <task>Compare all solutions and determine the final answer through majority voting.</task> <outputs> <output> <key>final_solution</key> <description>The final solution determined by majority voting.</description> <action> <type>RESULT</type> </action> </output> </outputs> <listen> <event>solve_with_gpt4</event> <event>solve_with_claude</event> <event>solve_with_deepseek</event> </listen> <agent> <name>Vote Aggregator Agent</name> <model>deepseek/deepseek-chat</model> </agent> </event> </events> </workflow> Table 5: Comparison of the single LLM (Deepseek-v3) and AI-generated Majority Voting workflow. 45 Problem: Simplify tan 100 + 4 sin 100. Ground Truth: 3 AI-generated Majority Voting workflow: Let me analyze all three solutions: 1. GPT-4 Solution: - Does not provide definitive numerical answer - Focuses on theoretical approach and approximations - Inconclusive result 2. Claude Solution: - Uses supplementary angle properties - Converts 100 to 180 - 80 - Arrives at through numerical calculations - Final answer: 3 3. Deepseek Solution: - Uses similar supplementary angle properties - Also converts 100 to 180 - 80 - 3 Goes through detailed algebraic steps - Final answer: 3 Analysis: 1. Two solutions provide definitive answers ( 3 and 3) , while one is inconclusive 2. Both Claude and Deepseek use similar approaches with supplementary angles 3. The main difference is in the sign of the final answer 4. Claudes solution provides more explicit numerical calculations 5. Looking at the steps: - 100 being in quadrant II - Using supplementary angle properties - tan 100 being negative in quadrant II - The final combination with 4sin 100 The Claude solution appears more precise in tracking signs and considering quadrant properties, making its answer more reliable. Final Answer: The final answer is 3. hope it is correct. Deepseek-v3: To simplify tan 100 + 4 sin 100, we proceed as follows: 1. Recall that tan θ = sin θ cos θ . Thus: tan 100 = sin 100 cos 100 . 2. Substitute this into the original expression: tan 100 + 4 sin 100 = sin 100 cos 100 + 4 sin 100. 16. Therefore, the simplified form of tan 100 + 4 sin 100 is: 3 . Final Answer: The final answer is 3 . hope it is correct. Problem: What integer satisfies 0 < 18 and 11213141 (mod 18) ? Ground Truth: 13 AI-generated Majority Voting workflow: Let me compare the three solutions: 1. GPT-4 Solution: Arrives at 13 through direct division and remainder calculation - Used division: 11213141 18 - Found remainder of 5 for positive number - Adjusted for negative sign to get 13 2. Claude Solution: Arrives at 13 through division and remainder - Divided -11213141 by 18 - Found remainder directly - Got 13 as the result in the range [0,18) 3. Deepseek Solution: Arrives at 5 through : - Division and remainder calculation - Used divisibility rules for 2 and 9 - Got 5 as final answer Analyzing the solutions: - Two solutions (GPT-4 and Claude) agree on 13 - One solution (Deepseek) gives 5 - Both GPT-4 and Claude show clearer arithmetic progressions - The key difference is in handling the negative sign in the modulo operation - When working with negative numbers in modulo arithmetic, we need to find the positive equivalent in the range [0,n) - The correct approach is to add the modulus (18) to the negative remainder until we get number in the range [0,18) By 13 is supported by two solutions verification: majority voting and - - The reasoning in these solutions is more complete - We can verify: 0 13 < 18 Final Answer: The final answer is 13. hope it is correct. -11213141 13 (mod 18) and Deepseek-v3: To solve for the integer that satisfies 0 < 18 and 11213141 (mod 18), we need to find the remainder when 11213141 is divided by 18. Heres the step-by-step solution: 1. Simplify the modulus: First, note that 11213141 (mod 18) is equivalent to finding the positive remainder when 11213141 is divided by 18. Since the modulus operation is periodic with period 18, we can add multiples of 18 to 11213141 until we obtain positive number within the range 0 < 18. 2. Calculate the remainder: To find 11213141 (mod 18), we can instead compute 11213141 (mod 18) and then take the negative of that result, adjusting it to be positive if necessary. - First, compute 11213141 18: 11213141 18 = 623000 with remainder of 11213141 18 623000 = 11213141 11214000 = 859 This means 11213141 859 (mod 18). - Now, compute 859 (mod 18): 859 18 = 48 with remainder of 859 18 (48) = 859 + 864 = 5 25 . What is cos RP S? So, 859 5 (mod 18). - Therefore, 11213141 5 (mod 18). 3. Final answer: The integer that satisfies 0 < 18 and 11213141 (mod 18) is 5. Final Answer: The final answer is 5. hope it is correct. Problem: In the diagram below, we have sin RP = 7 [asy] pair R,P,Q,SS; SS = (-2,0); = (0,0); = (2,0); = rotate(aSin(7/25))*(1.5,0); dot(\"S\",SS,S); dot(\"Q\",Q,S); dot(\"R\",R,N); dot(\"P \",P,S); draw(QSS); draw(PR); [/asy] Ground Truth: 24 25 Claude Solution: Final answer = 24 25 25 - Used complementary angles property - Found cos RPQ = 24/25 25 - Used right triangle approach - Found cosine of RPQ = 24/25 - AI-generated Majority Voting workflow: Let me analyze all three solutions: 1. GPT-4 Solution: Final answer = 24 Assumed RPS = RPQ 2. - Used supplementary angles property - Found cos RPQ = 24/25 - Used the fact that cos(supplementary angles) are negatives of each other - Concluded cos RPS = -cos RPQ = -24/25 3. DeepSeek Solution: Final answer = 7 - Incorrectly concluded cos RPS = sin RPQ Analyzing the approaches: 1. Looking at the diagram, RPS and RPQ are supplementary angles (they form straight line) 2. For supplementary angles, their cosines are negatives of each other 3. Claudes solution correctly uses this property 4. GPT-4 incorrectly assumed the angles were equal 5. DeepSeek incorrectly used complementary angle properties Claudes solution is the most mathematically sound because: - It correctly identifies that RPS and RPQ are supplementary angles - It properly applies the relationship between cosines of supplementary angles - The negative sign in the answer makes geometric sense given the angles position Final Answer: The final answer is 24 Deepseek-v3: We are given that sin RP = 7 (they form right angle together), we have: 25 . Since RP and RP are complementary angles 25 . hope it is correct. Final Answer: The final answer is 7 25 . hope it is correct. cos RP = sin RP = 7 25 . Problem: In the circle with center Q, radii AQ and BQ form right angle. The two smaller regions are tangent semicircles, as shown. The radius of the circle with center is 14 inches. What is the radius of the smaller semicircle? Express your answer as common fraction. Ground Truth: 14 3 2 1) 2 1) AI-generated Majority Voting workflow: Let me analyze all three solutions: 1. GPT-4s solution: = 7 2. Claudes solution: = 14/3 3. Deepseeks solution: = 14( Each solution uses different approaches: - GPT-4 assumes the semicircles are equal and uses simple division (14/2 = 7) - Claude uses the relationship + + = R, leading to 14/3 - Deepseek uses the Pythagorean theorem and complex algebra, arriving at 14( Lets verify which one makes more sense: 1. Key geometric principles suggest that when two circles are tangent to each other and to two perpendicular lines (in this case, the radii forming the right angle), theres special relationship where + + = R, where is the radius of the smaller circles and is the radius of the larger circle. 2. This principle is well-established in geometry. When two circles are placed in right angle and are tangent to each other and the lines forming the right angle, their radii follow this 3:1 ratio. 3. Therefore, Claudes solution using 3r = 14 leading to = 14/3 is the most mathematically sound . 4. We can verify this: - It satisfies the tangency conditions - The semicircles fit perfectly in the right angle - The relationship maintains the proper geometric proportions The GPT-4 solution (r = 7) is too large as it would make the semicircles overlap. The Deepseek solution 2 1) is more complex than necessary and doesnt align with the fundamental geometric principle for 14( this configuration. Final Answer: The final answer is 3 . hope it is correct. 48 Deepseek-v3: Lets solve the problem step by step. 1. Understand the setup: We have circle with center and radius 14 inches. Radii AQ and BQ form right angle, meaning the angle AQB = 90. There are two smaller semicircles tangent to the larger circle and to each other. 2. Key observations: - The two semicircles are tangent to the larger circle and to each other. This means the distance from the center to the center of each semicircle is 14 r, where is the radius of the smaller semicircle. - The centers of the two semicircles and the center form right triangle because AQB = 90. 5. Simplify the answer: - The radius of the smaller semicircle is 14( as common fraction: 2 1). However, this can be expressed = 14( 2 14. 2 1) = 14 14( = = = 14( . 2 14 1 2 1) 1 2 1). . 6. Final Answer: The radius of the smaller semicircle is 14( 2 1) . Final Answer: The final answer is 14( 2 1). hope it is correct."
        },
        {
            "title": "6.7.4 Detailed Trajectory of Creating ‘Financial Agent’",
            "content": "To provide more comprehensive demonstration of how MetaChain receives user requirements and automates the creation of tools and agents, we present the specific agent trajectory for the creation of the Financial Agent as follows. Table 6: Detailed Trajectory of Creating Financial Agent."
        },
        {
            "title": "Role",
            "content": "g: User"
        },
        {
            "title": "Content",
            "content": "I want to create Financial Agent (cid:44) that can help me to do two kinds (cid:44) of tasks: 1. Manage the private financial (cid:44) docs. have folder that (cid:44) contain the financial docs in my (cid:44) local machine, and want to (cid:44) help me to manage them. 2. Search the financial information (cid:44) online. You may help me to: - get balance sheets for given (cid:44) ticker over given period. - get cash flow statements for (cid:44) given ticker over given period. - get income statements for given (cid:44) ticker over given period. Directly output the form in the XML (cid:44) format without ANY other text. 49 (cid:240): Profiling Agent"
        },
        {
            "title": "Agent",
            "content": "output <agents> <system_input> Financial management (cid:44) requests, including: ... </system_input> <system_output> <key>financial_response</key> ... </system_output> <agent> <name>Document Manager (cid:44) Agent</name> ... </agent> <agent> <name>Market Research (cid:44) Agent</name> ... </agent> </agents>"
        },
        {
            "title": "Tool Editor",
            "content": "(cid:240): Agent list_tools() { \"save_raw_docs_to_vector_db\": { \"name\": (cid:44) \"save_raw_docs_to_vector_db\", \"func_name\": (cid:44) \"save_raw_docs_to_vector_db\", \"args\": [ \"context_variables\", \"doc_name\", \"saved_vector_db_name\", \"overwrite\" ], \"docstring\": ... }, ... }"
        },
        {
            "title": "Tool Editor",
            "content": "(cid:240): Agent get_api_plugin_tools_doc (query_text=API for (cid:44) getting financial statements (balance sheet, (cid:44) cash flow, income (cid:44) statement) from stock market) The rank 1 referenced tool (cid:44) documentation is: API Name: Company Income Statement ... The rank 2 referenced tool (cid:44) documentation is: ... ..."
        },
        {
            "title": "Tool Editor",
            "content": "(cid:240): Agent"
        },
        {
            "title": "Tool Editor",
            "content": "(cid:240): Agent create_tool( tool_name= get_balance_sheet, tool_code= from metachain.types (cid:44) import Result, Agent ... ) run_tool( tool_name= get_balance_sheet, run_code= from metachain.tools (cid:44) import get_balance_sheet result = get_balance_sheet(\"AAPL\", (cid:44) \"QUARTERLY\") print(result))"
        },
        {
            "title": "Tool Editor",
            "content": "(cid:240): Agent"
        },
        {
            "title": "Tool Editor",
            "content": "(cid:240): Agent create_tool( tool_name= get_cash_flow, tool_code= from metachain.types (cid:44) import Result, Agent ... ) run_tool( tool_name= get_cash_flow, run_code= from metachain.tools (cid:44) import get_cash_flow result = (cid:44) get_cash_flow(\"AAPL\", (cid:44) \"QUARTERLY\") print(result)) [SUCCESS] Successfully created (cid:44) tool: get_balance_sheet in (cid:44) /workplace/metachain/metachain/ tools/get_balance_sheet.py [The output of the tool run_tool (cid:44) showing in the interactive (cid:44) terminal] Terminal viewport position: Showing (cid:44) page 1 of 1. ====================== [SUCCESS] { \"symbol\": \"AAPL:NASDAQ\", \"type\": \"stock\", \"period\": \"QUARTERLY\", \"balance_sheet\": [ ... ====================== [SUCCESS] Successfully created (cid:44) tool: get_cash_flow in (cid:44) /workplace/metachain/metachain/ tools/get_cash_flow.py [The output of the tool run_tool (cid:44) showing in the interactive (cid:44) terminal] Terminal viewport position: Showing (cid:44) page 1 of 1. ====================== [SUCCESS] { \"symbol\": \"AAPL:NASDAQ\", \"type\": \"stock\", \"period\": \"QUARTERLY\", \"cash_flow\": [ ... ======================"
        },
        {
            "title": "Tool Editor",
            "content": "(cid:240): Agent"
        },
        {
            "title": "Tool Editor",
            "content": "(cid:240): Agent"
        },
        {
            "title": "Tool Editor",
            "content": "(cid:240): Agent"
        },
        {
            "title": "Tool Editor",
            "content": "(cid:240): Agent [SUCCESS] Successfully created (cid:44) tool: get_income_statement in (cid:44) /workplace/metachain/metachain/ tools/get_income_statement.py [The output of the tool run_tool (cid:44) showing in the interactive (cid:44) terminal] Terminal viewport position: Showing (cid:44) page 1 of 1. ====================== [SUCCESS] { \"symbol\": \"AAPL:NASDAQ\", \"type\": \"stock\", \"period\": \"QUARTERLY\", \"income_statement\": [ ... ====================== [SUCCESS] Successfully created (cid:44) tool: analyze_financial_data in (cid:44) /workplace/metachain/metachain/ tools/analyze_financial_data.py [The output of the tool run_tool (cid:44) showing in the interactive (cid:44) terminal] Terminal viewport position: Showing (cid:44) page 1 of 1. ====================== [SUCCESS] Financial Analysis for AAPL:NASDAQ ---------------------- Income Statement Analysis: For the period ending 2024-9-28: ... Profitability Metrics: ... Period-over-Period Comparison: ... ====================== create_tool( tool_name= get_income_statement, tool_code= from metachain.types (cid:44) import Result, Agent ...) run_tool( tool_name= get_income_statement, run_code=from (cid:44) metachain.tools import (cid:44) get_income_statement result = (cid:44) get_income_statement (\"AAPL\", \"QUARTERLY\") print(result)) create_tool( tool_name= analyze_financial_data, tool_code= from metachain.types (cid:44) import Result, Agent ... ) run_tool( tool_name= analyze_financial_data, run_code= from metachain.tools (cid:44) import (cid:44) get_income_statement, (cid:44) analyze_financial_data income_data = (cid:44) get_income_statement (\"AAPL\", \"QUARTERLY\") result = (cid:44) analyze_financial_data (income_data) print(result)) 52 Successfully created and tested all (cid:44) four financial tools: 1. get_balance_sheet - Created and (cid:44) tested successfully with AAPL (cid:44) stock data 2. get_cash_flow - Created and (cid:44) tested successfully with AAPL (cid:44) stock data 3. get_income_statement - Created (cid:44) and tested successfully with (cid:44) AAPL stock data 4. analyze_financial_data - Created (cid:44) and tested successfully by (cid:44) analyzing AAPLs income (cid:44) statement data All tools are functioning as (cid:44) expected, providing financial (cid:44) data and analysis in proper JSON (cid:44) format. Successfully created agent: (cid:44) Document Manager Agent in (cid:44) /workplace/metachain/metachain/ agents/document_manager_agent.py"
        },
        {
            "title": "Output",
            "content": "(cid:240): Agent"
        },
        {
            "title": "Agent Editor",
            "content": "(cid:240): Agent create_agent( agent_name= Document Manager Agent, agent_description= Specialized agent for (cid:44) managing and analyzing (cid:44) private financial (cid:44) documents stored (cid:44) locally., agent_tools= [query_db, ...], agent_instructions=You (cid:44) are specialized (cid:44) financial document (cid:44) manager responsible for (cid:44) managing and analyzing (cid:44) financial documents (cid:44) stored in specified (cid:44) directories. Your tasks (cid:44) include:...) 53 Successfully created agent: Market (cid:44) Research Agent in (cid:44) /workplace/metachain/metachain/ agents/market_research_agent.py [ERROR] Failed to create agent. (cid:44) Error: File (cid:44) \"/workplace/metachain/metachain/ agents/financial_analysis_orchestrator.py\", (cid:44) line 32 def (cid:44) transfer_to_document_manager_agent(Document (cid:44) analysis request including (cid:44) directory paths and analysis (cid:44) requirements: str): SyntaxError: invalid syntax ^^^^^^^^"
        },
        {
            "title": "Agent Editor",
            "content": "(cid:240): Agent"
        },
        {
            "title": "Agent Editor",
            "content": "(cid:240): Agent create_agent( agent_name= Market Research Agent, agent_description= Specialized agent for (cid:44) retrieving and (cid:44) analyzing online (cid:44) financial information (cid:44) for publicly traded (cid:44) companies., agent_tools= [get_cash_flow, ...], agent_instructions= You are specialized (cid:44) market research agent (cid:44) responsible for (cid:44) retrieving and (cid:44) analyzing financial (cid:44) information from online (cid:44) sources. Your primary (cid:44) responsibilities (cid:44) include: ...) create_orchestrator_agent (agent_name= Financial Analysis (cid:44) Orchestrator, agent_description= Orchestrator agent that (cid:44) coordinates between (cid:44) document management and (cid:44) market research to (cid:44) provide comprehensive (cid:44) financial analysis., sub_agents=[{name: (cid:44) Document Manager (cid:44) Agent, agent_input: (cid:44) Document analysis request ..., agent_output: (cid:44) Analyzed information ...}, {name: Market (cid:44) Research Agent, (cid:44) agent_input: Market research request ..., (cid:44) agent_output: Market analysis ...}], agent_instructions= You are Financial (cid:44) Analysis Orchestrator (cid:44) responsible for (cid:44) coordinating (cid:44) comprehensive financial (cid:44) analysis by leveraging (cid:44) both document analysis (cid:44) and market research (cid:44) capabilities. Your (cid:44) workflow: ...) 54 Successfully created agent: (cid:44) Financial Analysis Orchestrator (cid:44) in (cid:44) /workplace/metachain/metachain/ agents/financial_analysis_orchestrator.py"
        },
        {
            "title": "Agent Editor",
            "content": "(cid:240): Agent create_orchestrator_agent (agent_name= Financial Analysis (cid:44) Orchestrator, agent_description= Orchestrator agent that (cid:44) coordinates between (cid:44) document management and (cid:44) market research to (cid:44) provide comprehensive (cid:44) financial analysis., sub_agents=[{name: (cid:44) Document Manager (cid:44) Agent, agent_input: (cid:44) doc_request, (cid:44) agent_output: (cid:44) doc_response}, (cid:44) {name: Market (cid:44) Research Agent, (cid:44) agent_input: (cid:44) market_request, (cid:44) agent_output: (cid:44) market_response}], agent_instructions= You are Financial (cid:44) Analysis Orchestrator (cid:44) responsible for (cid:44) coordinating (cid:44) comprehensive financial (cid:44) analysis by leveraging (cid:44) both document analysis (cid:44) and market research (cid:44) capabilities. Your (cid:44) workflow: ...)"
        },
        {
            "title": "Agent Editor",
            "content": "(cid:240): Agent run_agent( agent_name= Financial Analysis (cid:44) Orchestrator, query= \"Basedonthe10-Kreports (cid:44) ofAAPLandMSFTfrom (cid:44) thepast5yearsinthe (cid:44) docsdirectorydocs/ aapl-2020-2024-10K/and (cid:44) docs/ msft-2020-2024-10K/, (cid:44) alongwithAAPLsother (cid:44) reportsdocs/ aapl-other-report/and (cid:44) availabledata,conduct (cid:44) acomprehensive (cid:44) horizontalcomparison, (cid:44) createacomparative (cid:44) analysisreport,and (cid:44) provideconstructive (cid:44) investmentadvicefor (cid:44) investinginthemin (cid:44) 2025.\", ctx_vars={}) [The output of the tool run_agent (cid:44) showing in the interactive (cid:44) terminal] Terminal viewport position: Showing (cid:44) page 1 of 35. [NOTE] The output of the tool run_agent, you can use terminal_page_up to scroll up and terminal_page_down to scroll down. If there are many pages with meaningless content like progress bar or output of generating directory structure when there are many datasets in the directory, you can use terminal_page_to to move the viewport to the end of terminal where the meaningful content is. ====================== [SUCCESS] **** Receive Task **** Receiveing the task: Based on the 10-K reports of AAPL (cid:44) and MSFT from the past 5 years (cid:44) in the docs directory ... **** Assistant Message **** Financial Analysis Orchestrator: (cid:44) Ill help coordinate (cid:44) comprehensive comparative (cid:44) analysis between Apple (AAPL) and Microsoft (MSFT) using both (cid:44) historical document analysis and (cid:44) current market research. Lets break this down (cid:44) into parallel workflows to (cid:44) gather all necessary information. 1. First, lets analyze the (cid:44) historical documents (10-K (cid:44) reports and other reports): **** Tool Calls **** [2025-01-22 05:58:48] transfer_to_document_manager_agent (doc_request=...) **** Tool Execution **** [2025-01-22 05:58:48] Tool Execution: (cid:44) transfer_to_document_manager_agent Result: --- Please analyze the following (cid:44) documents: ... **** Assistant Message **** Document Manager Agent: ... ======================"
        },
        {
            "title": "Agent Editor",
            "content": "(cid:240): Agent terminal_page_to (page_idx=35) [The output of the tool run_agent (cid:44) showing in the interactive (cid:44) terminal] Terminal viewport position: Showing (cid:44) page 35 of 35. [NOTE] The output of the tool run_agent, you can use terminal_page_up to scroll up and terminal_page_down to scroll down. If there are many pages with meaningless content like progress bar or output of generating directory structure when there are many datasets in the directory, you can use terminal_page_to to move the viewport to the end of terminal where the meaningful content is. ====================== **** Assistant Message **** Financial Analysis Orchestrator: (cid:44) Based on the comprehensive (cid:44) analysis of both historical (cid:44) performance from 10-K reports and current financial data, (cid:44) here is the final investment (cid:44) recommendation for 2025: Investment Recommendation Summary (cid:44) for 2025: 1. Portfolio Allocation Strategy: - Microsoft (MSFT): 60% of (cid:44) technology allocation - Apple (AAPL): 40% of technology (cid:44) allocation 2. Investment Rationale: Microsoft (MSFT) - Primary Position (cid:44) (60%): ... ======================"
        },
        {
            "title": "Agent Editor",
            "content": "(cid:240): Agent"
        },
        {
            "title": "Output",
            "content": "I have successfully created and (cid:44) tested the following agents: 1. Document Manager Agent - for (cid:44) managing and analyzing private (cid:44) financial documents 2. Market Research Agent - for (cid:44) retrieving and analyzing online (cid:44) financial information 3. Financial Analysis Orchestrator (cid:44) - coordinates between the two (cid:44) agents for comprehensive analysis The agents were tested with (cid:44) complex task of analyzing AAPL (cid:44) and MSFTs financial documents (cid:44) and providing investment advice. (cid:44) The orchestrator successfully: 1. Processed 10-K reports from both (cid:44) companies 2. Analyzed financial metrics and (cid:44) trends 3. Generated comprehensive (cid:44) investment recommendation with: - Portfolio allocation strategy - Investment rationale for each (cid:44) company - Key monitoring points - Risk management strategy - Implementation approach The test demonstrated that the (cid:44) agents can effectively work (cid:44) together to provide detailed (cid:44) financial analysis and (cid:44) actionable investment advice."
        }
    ],
    "affiliations": [
        "The University of Hong Kong"
    ]
}