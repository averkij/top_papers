{
    "paper_title": "Comprehensive Relighting: Generalizable and Consistent Monocular Human Relighting and Harmonization",
    "authors": [
        "Junying Wang",
        "Jingyuan Liu",
        "Xin Sun",
        "Krishna Kumar Singh",
        "Zhixin Shu",
        "He Zhang",
        "Jimei Yang",
        "Nanxuan Zhao",
        "Tuanfeng Y. Wang",
        "Simon S. Chen",
        "Ulrich Neumann",
        "Jae Shin Yoon"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "This paper introduces Comprehensive Relighting, the first all-in-one approach that can both control and harmonize the lighting from an image or video of humans with arbitrary body parts from any scene. Building such a generalizable model is extremely challenging due to the lack of dataset, restricting existing image-based relighting models to a specific scenario (e.g., face or static human). To address this challenge, we repurpose a pre-trained diffusion model as a general image prior and jointly model the human relighting and background harmonization in the coarse-to-fine framework. To further enhance the temporal coherence of the relighting, we introduce an unsupervised temporal lighting model that learns the lighting cycle consistency from many real-world videos without any ground truth. In inference time, our temporal lighting module is combined with the diffusion models through the spatio-temporal feature blending algorithms without extra training; and we apply a new guided refinement as a post-processing to preserve the high-frequency details from the input image. In the experiments, Comprehensive Relighting shows a strong generalizability and lighting temporal coherence, outperforming existing image-based human relighting and harmonization methods."
        },
        {
            "title": "Start",
            "content": "Comprehensive Relighting: Generalizable and Consistent Monocular Human Relighting and Harmonization Junying Wang1 He Zhang2 Jingyuan Liu2 Xin Sun2 Nanxuan Zhao2 Krishna Kumar Singh2 Tuanfeng Y. Wang2 Zhixin Shu2 Simon S. Chen Jimei Yang3 5 2 0 2 3 ] . [ 1 1 1 0 3 0 . 4 0 5 2 : r Ulrich Neumann1 Jae Shin Yoon2 1University of Southern California 2Adobe Research 3Runway Figure 1. We introduce Comprehensive Relighting, generalizable and consistent model for relighting and harmonization, which controls the lighting property from single image or video of humans with arbitrary body parts. Given target lighting coefficients, e.g., Spherical harmonics (second), background scenes (third), or their combination (fourth), our model performs consistent and harmonized relighting."
        },
        {
            "title": "Abstract",
            "content": "This paper introduces Comprehensive Relighting, the first all-in-one approach that can both control and harmonize the lighting from an image or video of humans with arbitrary body parts from any scene. Building such generalizable model is extremely challenging due to the lack of dataset, restricting existing image-based relighting models to specific scenario (e.g., face or static human). To address this challenge, we repurpose pre-trained diffusion model as general image prior and jointly model the human relighting and background harmonization in the coarse-tofine framework. To further enhance the temporal coherence of the relighting, we introduce an unsupervised temporal lighting model that learns the lighting cycle consistency from many real-world videos without any ground truth. In inference time, our temporal lighting module is combined with the diffusion models through the spatio-temporal feature blending algorithms without extra training; and we apply new guided refinement as post-processing to pre1This work is partially done during an internship at Adobe Research. serve the high-frequency details from the input image. In the experiments, Comprehensive Relighting shows strong generalizability and lighting temporal coherence, outperforming existing image-based human relighting and harmonization methods. More demo results are available on our project page: https://junyingw.github.io/ paper/relighting. 1. Introduction Light is the key component that determines how person looks, which is often defined by specific time and space, where revisiting such unique moment gives us the opportunity to experience authentic telepresence sensations. In this paper, we introduce generalizable human relighting model that can control the lighting from an image or video of humans with arbitrary body parts  (Fig. 3)  , which are well-harmonized with conditioning space (i.e., background image) as shown in Fig. 1. As shown in Fig. 2, existing image-based relighting methods face two main problems, lack of generalizability and controllability. First, they are designed for specific scenario, e.g., only for face illumination or static hu1 Figure 2. Comparison of various baseline methods for relighting settings and functionalities. mans [35, 69, 76] mainly due to the scarcity of large-scale relighting datasets: Precise acquisition of the appearance of static person under assorted lighting conditions requires specialized setups such as LightStage [23, 35] or expensive graphics simulation [28], which are not scalable, particularly for video contents. Learning from such limited data induces weak generalization of the model to diverse scenes. Second, most relighting algorithms struggle to effectively model multiple light sources. Typically, these algorithms are restricted to single lighting control from either background image (e.g. high dynamic range lighting environment map [23]) or target lighting parameters (e.g. Spherical harmonics [26]). These problems inhibit the productionlevel application that requires general use cases. To overcome these challenges, we propose an effective approach to achieve all-in-one relighting by utilizing diffusion modela general image prior that learns massive visual data with diverse lighting conditions; and repurpose this prior specialized for human relighting and harmonization in coarse-to-fine framework: pretrained latent diffusion model [44] learns from limited datasets of 1static humans to jointly perform the fine-grained relighting and harmonization from two multi-modal lighting variables: the coarse shading estimate and conditioning background scene. The coarse shading estimate can be computed from conditioning lighting parameters (i.e., Spherical harmonics) without neural network, and therefore, it is generalizable. The diffusion model is required to learn only the residual portion (e.g., fine self-occluded shadow), which is more generalizable than direct relighting (e.g., [76]). The pre-trained image prior helps with understanding the properties of the general background scenes, enabling the generalization of the background harmonization. Our coarse-to-fine framework, however, still introduces problems: the diffusion model that learns to generate an image without temporal context produces significant temporal artifacts (e.g., sudden changes of lighting distribution even for consecutive frames). We address this problem by introducing an unsupervised temporal lighting model that learns Figure 3. Our model generalizes to various body parts (portrait, half-body, full-body, multiperson) for relighting and harmonization, with lighting control variables shown in the insets. from many real videos without any ground-truth data to enforce the temporal lighting consistency over frames. This temporal module learns the unsupervised cycle consistency between the relit and many real videos to predict the future lighting distribution from the past, which can be directly combined with our coarse-to-fine relighting components without extra training. In inference time, we further push the temporal coherence of the relighting and harmonization by formulating recurrent prediction pipeline. The generation at the current time step is conditioned onto the temporal module for the one at the next time. The features from the lighting and temporal control modules are spatially and temporally blended to enforce temporal coherence while improving the structure of the lighting distribution. Finally, our guided refinement module enhances the quality of the generated image in way that preserves the original high-frequency details of the input image. In the experiments, Comprehensive Relighting demonstrates high-quality relighting and background harmonization results with strong temporal coherence across the lighting, background, and pose changes. It also shows strong generalizability to any unconstrained scenes regardless of body parts, views, and poses, outperforming existing relighting and background harmonization methods. Our contributions include: (1) To the best of our knowledge, the first approach for joint modeling of relighting and background harmonization; (2) novel coarse-to-fine framework that enables comprehensive generalization by only learning from limited synthetic and lab-controlled data; (3) unsupervised temporal modeling with lighting cycle consistency from many unconstrained real videos; (4) an effective inference algorithm with spatio-temporal feature blending and guided refinement. 2. Related Work 1To the best of our knowledge, there exists no public ground truth data for video relighting of dynamic humans, and therefore, fine-tuning an existing video diffusion model (e.g., [17]) is not readily available option. Image-based Human Relighting While high-quality relighting often requires resource-intensive setups, recent efforts in mobile device relighting aim to address single2 image scenarios. Total Relighting [35] achieves photorealistic effects by leveraging detailed normal maps and albedo as priors. However, it relies on an HDR environment map that accurately matches the scenes real-world lighting, which is not always readily available, particularly in flexible and personalized relighting scenarios. [39, 50] estimate and adjust the Spherical harmonics parameters from images, and others use one-light-at-a-time (OLAT) captures to generate relighting data [53] or estimate reflectance fields [50]. Recent works have focused on diverse relighting scenarios including portrait [10, 19, 20, 27, 32, 35, 50, 66, 75, 76], full-body [9, 23, 28, 51], and object relighting [1, 61, 62]. Most of these works rely on decomposing an image into its intrinsic components, i.e., albedo, normal, and lighting, and therefore, the accuracy of this decomposition directly impacts the quality of the final relighting. While some works [23, 74, 77] use single-image geometry reconstruction for traditional and neural rendering, reconstruction errors are often propagated to relighting results. Tajima et al. [51] tackled domain adaptation with two-step relighting framework, yet noticeable texture distortion remains due to limited model generalizability. Zhang et al. [67] trained 2D latent-diffusion model, allowing users to manipulate and construct face NeRFs in zero-shot learning framework without the need for explicit 3D data. DiFaReli [38] utilizes DDIM (Denoising Diffusion Implicit Models [48]) for high-fidelity face relighting. While promising, their methods are constrained by focus on either specific body parts, limiting their applicability for generalizable. Background Harmonization Background harmonization seeks to harmonize color, contrast, and style discrepancies between the foreground and background, ensuring composite images appear natural and cohesive. Many existing background harmonization methods [4, 7, 8, 13 15, 24, 55, 56, 78] formulate this problem as image-toimage translation work where neural network translate an unharmonized foreground image to the harmonized one in the context of the conditioning background image. Recently, Relightful Harmonization [41] introduced methods that harmonize both image style and lighting to match the background scene, and IC-Light [71] further enables flexible illumination control via image diffusion models, guided by text descriptions or background images. While promising, these methods lack explicit lighting control, restricting general applicability and consistent video relighting. Additionally, some ([41]) are limited to specific body parts (e.g., portraits), with constrained support for full-body and multiperson scenarios. Video Relighting Neural Radiance Fields (NeRF) [34] based methods enable novel view synthesis under varying lighting conditions in videos [47, 54, 58, 77]. Zhang et al. [69] achieve portrait video relighting under dynamic illuminations, while Choi et al. [6] ensure temporally consistent relit videos. 3D-PVR [3] present 3D-aware, real-time method to relight videos of talking faces. Relighting4D [5] decomposes the time-varied human body as set of neural fields of normal, occlusion, diffuse, and specular maps. However, their rendering quality are highly reliant on geometry accuracy. Some works [38] adopt temporal modeling scheme real-time neural video portrait relighting. STNeRF [68] controls dynamic scenes using neural layered radiance representation that maintains spatial and temporal coherence. Li et al. [30] employed multi-view reconstruction for free-viewpoint relighting videos under general illumination. Richardt et al. [42] use RGBZ video cameras for video effects, including relighting, but rely on multi-view reconstruction from specialized hardware, making the process highly complex and costly. 3. Method We develop generalizable and consistent human relighting and harmonization framework using diffusion model. Fig. 4 illustrates the overview of our framework. Given an input image of humans and control lighting variables, including coarse shading and background image, diffusion model learns to generate fine-grained image of the humans under the target lighting, also harmonized with the background (Sec. 3.2). An external temporal lighting module, trained on real videos using unsupervised temporal cycle consistency, is integrated into the diffusion model to enhance temporal lighting coherence (Sec. 3.3). In inference time, we blend the features between the lighting and temporal control modules over time to ensure the relighting results are accurate and temporally coherent, and we apply guided refinement to prevent the loss of high-frequency details during the denoising process (Sec. 3.4). 3.1. Background: Image-based Relighting Image-based relighting function can be compactly modeled with small number of approximated basis of Spherical harmonics (SH) [9, 28, 46] which describes only essential features of illumination on the surface of 3D sphere based on the following formulation: I(x) = ρ(x) (cid:88) (cid:88) l=0 m=l ϕYlm(n(x)) (1) where ϕ R(k+1)2 denotes the spherical harmonics (SH) coefficients; Ylm(n) are the SH basis functions evaluated at the surface normal n; indices and represent the band and order within each band, respectively; and is the maximum order of spherical harmonics used. While SH is computationally efficient and highly generalizable, it only captures low-frequency lighting, limiting its ability to model fine-grained, high-frequency illumination details. AdditionFigure 4. System overview. (a) Given an input image of humans with coarse lighting and background image, our diffusion model generates the relit images harmonized with background scenes (Sec. 3.2). (b) The external temporal modules learn the temporal cycle consistency from many real-world videos to construct temporal lighting features (Sec. 3.3). (c) In inference time, we blend the features from lighting and temporal modules spatially and temporally to enable coherent and generalizable human relighting (Sec. 3.4). ally, SH-based global illumination neglects the context provided by background images, often resulting in relit images appearing unnatural when composited with different backgrounds. To overcome these limitations, we propose coarse-to-fine human relighting and harmonization framework that leverages the strong image prior available from pre-trained text-to-image diffusion model. 3.2. Coarse-to-Fine Relighting and Harmonization We generate fine-grained relit image of person conditioned on coarse lighting representation: E(I; Sϕ) = z, D(z) = Iϕ (2) where is an encoder that generates the latent lighting features, as function of the input image Rwh3 and small number of global lighting parameters ϕ Rn (i.e., Spherical harmonics where = 25); and is the decoder that generates fine-grained relit image Iϕ Rwh3 from z. Sϕ Rwh is the pixel-aligned coarse shading map converted from the coarse lighting parameters ϕ as shown in Fig. 4-(a). One approach to obtaining Sϕ is directly computing it as demonstrated in Eq. 1 where we can compute lighting intensity along with the detected surface normal map from given spherical harmonics coefficients ϕ. Another way, is to use neural network to convert the surface normal to Sϕ as condition of spherical harmonics coefficients ϕ, i.e., Sϕ (N; ϕ) where is the neural shading function which maps the surface normal and ϕ to the coarse shading. While both approaches are highly generalizable, our experiments indicate that the latter method achieves improved accuracy and better noise correction. Please refer to the Supple. documents for more details about the coarse shading estimation. To jointly model the relighting and background harmonization, we further condition target background image Rwh3 as the additional lighting sources: E(I; {Sϕ, B}) = z, D(z) = Iϕ (3) where the lighting encoder learns to capture the intensity and direction of light from Sϕ while capturing ambient environment lighting and color distribution from B. This enables to achieve complete relighting in scenarios involving new target lighting, background, or both. In the subsection, we enable (D E) using latent diffusion model. 3.2.1. Fine-grained Relighting Diffusion Model We enable the fine-grained image relighting using diffusion model as illustrated in Fig. 4-(a). Our encoder in Eq. 3, in practice, is formulated as the composition of two encoders: Eb(I; El({Sϕ, B})) = z, D(z) = Iϕ (4) where El encodes lighting control variables, {Sϕ, B}, and Eb encodes the conditional variable whose visual properties, e.g., semantics and identity, should be preserved in the output along with the controls from El. For Eb, we use the base latent diffusion model [2, 43] pre-trained for text-toimage generation task, and for El, we use ControlNet [70] (termed as Light ControlNet in Fig. 4-(a)). While noise, texts, and timestep variables are also conditioned on Eb to fit the modality of the latent diffusion model, they are not described in the equations and figures for conciseness. To impose the foreground attention on the lighting control, portrait mask is also conditioned to El: Eb(I; El({Sϕ, B}; M)) = z, D(z) = Iϕ (5) where {0, 1}wh is the foreground binary mask. In training time, the diffusion model learns to directly change the lighting distribution of the input images (without inverse 4 rendering techniques) as the condition of coarse lighting estimate by minimizing the latent distance of the noisy relit image with the clean one from the ground truth in the overall forward and background denoising steps. Following the findings from an existing harmonization work [41], we use the composite image between the input image and conditioning background image as I. We randomly drop background or randomly set the coarse shading Sϕ as binary mask such that the diffusion model learns the control the lighting from B, Sϕ, or both {B, Sϕ}. 3.3. Unsupervised Add-on Temporal Modeling Our coarse-to-fine relighting framework (D Eb El) is trained only on individual images, inherently missing temporal context, e.g., how point on humans surface will radiate from specific viewpoint under the continuous pose, view, and illumination changes, leading to temporal artifacts such as flickering. We model such temporal context by designing an external add-on temporal lighting module Em that can be combined, in inference time, with the relighting framework without extra training: Eb El Eb (El Em) (6) To enable this, our temporal module is designed to regress the relit image from the previous time instance, i.e., It1 ϕ , to the latent lighting distribution whose space is shared with our base relighting models. Therefore, our decoder can generate temporally coherent relit images in the current time in an auto-regressive way: D(Eb(It; Em(It1 ϕ ))) = It t1. (7) However, training Em with conventional L2 loss is not possible since there exists no ground-truth video relighting data for dynamic humans. Therefore, we propose to learn Em in an unsupervised way using many real videos with lighting cycle consistency as follows: D(E D(E ({St (It; (It ϕ, Bt}; Mt))) = It ϕ; Em(It1, Mt1))) = It t1 ϕ , (8) (9) It = It t1. We make the hypothesis: video sequence inherently contains temporal lighting properties whose flow can be implicitly modeled by learning to predict the lighting distribution of the future frame based on the hint from the previous frame. This involves forward and backward processes, as cycle-training. Eq. 8 represents the forward image relighting, i.e., It It ϕ, where our relighting diffusion model with lighting ControlNet El generates the relit image at time as condition of novel lighting condition (where we pick random Spherical harmonics). Eq. 9 reverts the relighting, i.e., It ϕ to the original input image conditioned by t1 It 5 the original frame in the previous time step through our temporal lighting module where the mask is used for foreground awareness. Finally, Em learns the lighting cycle consistency in the diffusion process by minimizing the latent distance between the reverted and the original image. We freeze the learnable weights for the functions with during training, and Em is enabled with another ControlNet [70] termed as Motion ControlNet in Fig. 4-(b). 3.4. Inference with Spatio-Temporal Blending and"
        },
        {
            "title": "Guided Refinement",
            "content": "Given video or image, we perform comprehensive relighting with spatio-temporal feature blending framework. For = 0, we generate the relit image without our temporal lighting module Em. For > 0 (note that even for the static image, > 0 is possible since the lighting is time-variant), the generated relit images in the previous time step are conditioned on Em, and therefore, the relighting is controlled by dual control modules, i.e., El and Em by blending their features with (0.85:0.15) ratio as described as spatial blending block in Fig. 4-(c). The blended lighting features are recurrently combined with the one from the previous time step through the temporal blending block. For this, we adopt optical flow from the original input video as temporal prior to spatially align the features from consecutive frames similar to [29]; and we blend the aligned temporal features with (0.5:0.5) ratio as in Fig. 4-(c) to improve the temporal consistency. In the denoising process of the latent diffusion model, the generation often suffers from the loss of high-frequency details. Inspired by existing image restoration techniques [49, 73], we address this problem by applying guided refinement to each generated image. We cast the problem of guided refinement as guided residual prediction: Irefine ϕ = + g(Iϕ, I; M), (10) ϕ where is the function that predicts the guided lighting residual. This residual learns to map the lighting distribution from to Iϕ. Here, Irefine can effectively preserve the high-frequency details of the input image due to the nature of residual learning [63, 73] that is designed to preserve the visual properties from the observation space, i.e., I. We enable with residual network [16] by learning from our relighting data with general losses for low-level vision processing, i.e, L2 and VGG [11]. 4. Experiments Dataset. To train our coarse-to-fine relighting model, we use 100K ground-truth relighting samples, including 50K synthetic human renderings and 50K OLAT-captured images from LightStage, with random cropping augmentation. Ground-truth albedo, images, background, masks, and lighting coefficients are precomputed. Our dataset is categorized by gender, skin tone, and body part (full-body and Method DPR [76] RHW [51] GFR [23] LPBR [41] Ours Scenario 1 Scenario 2 Scenario 18.62/0.86/0.103 19.78/0.87/0.113 25.59/0.91/0.089 18.19/0.86/0.090 25.95/0.95/0.066 32.00 /0.94/0.030 30.74/0.95/ 0.027 30.33/0.95/0.033 31.62/0.91/0.035 33.58/0.96/0.024 18.14/0.89/0.089 20.12/0.88/0.078 22.76/0.91/0.072 19.96/0.88/0.084 23.99/0.93/0.048 35.29 /0.98/0.032 36.64 /0.98/ 0.028 32.36/0.98/0.036 27.94/0.94/0.041 35.18/ 0.99 / 0.031 21.20/0.89/0.072 23.33/0.90/0.060 25.49/0.93/0.050 21.42/0.88/0.053 26.61/0.94/0.033 37.11/0.95/0.038 37.53 /0.98/0.033 35.89/0.98/ 0.028 33.39/0.95/0.038 38.32/0.98/0. Table 1. Comparison with existing image-based human relighting methods on synthetic videos for fidelity and temporal consistency. Each column shows image fidelity (left) and video temporal consistency (right). Metrics are PSNR / SSIM / LPIPS for accuracy, and tPSNR / tSSIM / tLPIPS for temporal consistency. While green is used for the best values, yellow highlights the second-best values. Figure 5. Qualitative comparison of synthetic video frames (corresponding to Tab. 1). From left to right: composite input with target lighting parameters (inset), our relit result, baseline methods, and normalized L2 photometric error map (inset). multi-body), with each subject captured from 32 viewpoints under varying lighting conditions. For detailed dataset breakdown, refer to the Supple. To train our temporal control module using real data, we used 50K frames of videos from existing works [22, 31, 36, 60] and customized videos, where, for cyclic relighting, we randomly sample spherical harmonics lighting parameters from ground-truth data. For validation on static humans with dynamic lighting, we generated new testing set (i.e., the ground-truth relit images and associated lighting parameters) using an internal simulation and LightStage. This set includes the mixture of halfbody, full-body, and multi-person scenarios, with each scenario comprising 100 frames. For the validation on dynamic humans (i.e., video), we newly create synthetic video sequences for three scenarios: Scenario 1 involves static lighting (environment map) across frames with moving human; Scenario 2 features dynamic lighting (rotated environment map) with static human; Scenario 3 combines dynamic lighting (rotated environment map) with moving human. Please refer to the Supple. and demo video for more details about the testing results. Metrics. To measure the relighting quality, we use L1 disFigure 6. Comparison with LPBR [41] on DeepFashion [33] real images for background harmonization testing. The first row shows the relit output, and the second shows the magnified results. tance, reconstruction fidelity (PSNR [18]), local structure similarity (SSIM [18]), and the latent perceptual distance (LPIPS [72]), between the relighted and ground-truth. For measuring the temporal coherence of the relighting results, we follow the same logics as TokenFlow [12]. This involves warping the relit image from the previous time step to the next using optical flow [52], and then comparing this to the Figure 7. Comparison with RHW [51] on Pexels [37] real images. The lighting control variables are shown as insets. While RHW produces reasonable relighting for full-body images, its quality degrades on half-body and multi-person cases. Figure 8. Comparison with GFR [23] on Pexels [37] real images. The lighting control variables are shown as insets. Limited generalizability of GFR results in reduced output quality for half-body and multi-person cases. current frame using the aforementioned metrics, termed tL1 error, tPSNR, tSSIM, and tLPIPS. Baseline. We compare our method with existing monocular human relighting works where we chose the baselines that are applicable to general scenes (e.g., any part-specific information such as 3D face model is not requirement): DPR [75], RHW [51], and GFR [23]. For the harmonization baseline, we compare our model with LPBR [41], diffusion-based light-aware harmonization method. For relighting, aside from GFR, baselines use Spherical harmonics for lighting control without modeling background illumination. DPR and RHW are evaluated using their released pre-trained models. Since GFR lacks available code, we reimplemented it using our dataset, replacing HDR lighting with Spherical harmonics and background modeling to enable background harmonization in our experiments. Harmonization is directly compared with LPBR by replacing the background. Results. For testing static humans under dynamic lighting, we present the quantitative comparison in Tab. 2. We show average numerical evaluations on our synthetic testing dataset, categorized by portrait, full-body, and multiperson. Further validation by gender, and skin color is detailed in the Supple. Methods such as RHW and DPR have 7 Method Ours-diffusion Ours end-to-end Ours-background Ours Ours+refine L1 0.05837 0.01432 0.01239 0.01035 0.01012 PSNR 17.099 26.418 27.346 28.419 28.778 SSIM 0.833 0.918 0.945 0.948 0.949 Table 3. Ablation study on coarse-to-fine relighting models. limited generalizability for both full-body and portrait relighting. They are difficult to extend to other scenarios and tend to show reduced performance when tested in different settings. While GFR performs well in our evaluation, it struggles with significant domain gaps, leading to noticeable quality degradation on real data, including distortions and color shifts  (Fig. 8)  . In contrast, our model exhibits strong generalizability across validation sets and real-world tests (Figs. 7, 8). Compared to the state-of-the-art background harmonization method (LPBR), our method shows the strong generalization to different body parts, and notably, LPBR often includes significant distortion when applied to the full-body images and it does not support the lighting control function as shown in Fig. 6. DPR [76] RHW [51] GFR [23] Category 17.74 / 0.87 15.75 / 0.82 17.71 / 0.86 Portrait Full-body 27.62 / 0.96 27.73 / 0.95 29.51 / 0.95 Multi-person 25.70 / 0.95 25.69 / 0.95 29.35 / 0.97 Ours 23.04 / 0.90 30.81 / 0.97 31.49 / 0.96 Table 2. Comparison on our synthetic static testing data sorted by body-part. We compute average PSNR / SSIM. For video testing, in Tab. 1, we evaluate our model on scenarios 1, 2, and 3 for both fidelity and temporal consistency. Other approaches face challenges in achieving both relit fidelity and temporal consistency at the same time as also shown in Fig. 5. In contrast, our temporal module ensures our comprehensive relighting model to produce videos with strong temporal consistency. For more results on relighting results and comparisons including user study, please refer to the Supple. Ablation Study. We conduct two ablation studies on our coarse-to-fine model using static human data and our temporal modules using dynamic human data. As shown in Tab. 3, we perform the ablation study on our coarse-to-fine approach: 1) Ours-diffusion: Relighting only with pixelaligned coarse shading without diffusion model, i.e., Sϕ I. 2) Ours end-to-end: Relighting in an end-to-end manner by applying target lighting parameters ϕ as condition, trained with diffusion model, without the coarse stage. 3) Oursbackground: Relighting without the control of background B. 4) Ours+refine (full): Applying our guided refinement to the relit image. Tab. 3 shows the summary of the ablation study: Directly apply the coarse stage without diffusion model introduces significant errors in the final relit result due to the detection noises. Instead of applying target lighting and end-toMethod Ours Ours+temporal Ours+temporal+blend tL1 6.552 5.638 4.019 tPSNR 31.028 32.266 33.588 tSSIM 0.956 0.957 0.957 Table 4. Ablation study on our temporal modules evaluated on synthetic sequences: tL1 error (103) end training with diffusion model, our coarse-to-fine approach shows better performance, indicating that our coarse stage serves as strong control prior. This control prior is both neat and effective for extending our model to diverse identities, various body parts. Based on the comparison with Ours-background we notice that encoding information from the background image aids in enhancing natural illumination during background harmonization. Lastly, leveraging guided refinement enables the preservation of high-frequency information alongside robust generation capabilities. For our temporal module, we study three ablation studies: 1) Ours: We eliminate all temporal consistency components, which is single-frame-based generation method. 2) Ours+temporal: We only apply temporal module Em without recurrent feature blending during the test-time. 3) Ours+temporal+blend: We perform our video relighting with temporal module Em and recurrent feature blending. Tab. 4 summarizes the performance of each of our lighting module in temporal modules. The temporal Ours+temporal primarily enforces temporal coherence by imposing temporal constraint on the lighting control between the current and previous frames during testing. Additionally, the recurrent blending feature further enhances temporal consistency by blending the lighting control feature between previous and current frames, thereby reinforcing the temporal context. Limitation. Our relighting diffusion model requires heavy computational time. Significant noise on the detection (e.g., mask and surface normal) affects the temporal coherence. 5. Conclusion We introduce method for Comprehensive Relighting that is generalizable and consistent for monocular human relighting and harmonization. We address core dataset challenge by utilizing large and general image prior from pre-trained diffusion model; and repurposing the model specialized for temporally consistent image relighting. For coherent control of the lighting, we introduce coarse-tofine relighting framework; and combine it with an external temporal lighting module that learns many real videos. Our guided refinement network enhances the visual to preserve the fine details of an original image. In the experiments, our method outperforms other image-based relighting and harmonization models in terms of quality and temporal coherence. 8 6. Acknowledgement We sincerely thank Mengwei Ren for the insightful discussions regarding the framework design, and we are grateful to Jianming Zhang for kindly providing the human normal map estimator."
        },
        {
            "title": "References",
            "content": "[1] Sai Bi, Zexiang Xu, Kalyan Sunkavalli, David Kriegman, and Ravi Ramamoorthi. Deep 3d capture: Geometry and reflectance from sparse multi-view images. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 59605969, 2020. 3 [2] Tim Brooks, Aleksander Holynski, and Alexei Efros. Instructpix2pix: Learning to follow image editing instructions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1839218402, 2023. 4, 12, 13 [3] Ziqi Cai, Kaiwen Jiang, Shu-Yu Chen, Yu-Kun Lai, Hongbo Fu, Boxin Shi, and Lin Gao. Real-time 3d-aware portrait video relighting. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 62216231, 2024. 3 [4] Jianqi Chen, Yilan Zhang, Zhengxia Zou, Keyan Chen, and Zhenwei Shi. Dense pixel-to-pixel harmonization via continuous image representation. IEEE Transactions on Circuits and Systems for Video Technology, pages 11, 2023. 3 [5] Zhaoxi Chen and Ziwei Liu. Relighting4d: Neural reIn European Conference on lightable human from videos. Computer Vision, pages 606623. Springer, 2022. 3 [6] Jun Myeong Choi, Max Christman, and Roni Sengupta. PerIn sonalized video relighting with an at-home light stage. European Conference on Computer Vision, pages 394410. Springer, 2024. [7] Wenyan Cong, Jianfu Zhang, Li Niu, Liu Liu, Zhixin Ling, Weiyuan Li, and Liqing Zhang. Dovenet: Deep image In Proceedings of harmonization via domain verification. the IEEE/CVF conference on computer vision and pattern recognition, pages 83948403, 2020. 3 [8] Wenyan Cong, Xinhao Tao, Li Niu, Jing Liang, Xuesong Gao, Qihao Sun, and Liqing Zhang. High-resolution image harmonization via collaborative dual transformations. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1847018479, 2022. 3 [9] Yuki Endo Daichi Tajima, Yoshihiro Kanamori. Relighting humans in the wild: Monocular full-body human relighting with domain adaptation. Computer Graphics Forum (Proc. of Pacific Graphics 2021), 40(7):205216, 2021. 3 [10] David Futschik, Kelvin Ritland, James Vecore, Sean Fanello, Sergio Orts-Escolano, Brian Curless, Daniel S`ykora, and Rohit Pandey. Controllable light diffusion for portraits. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 84128421, 2023. 3, 16 [11] Leon Gatys, Alexander Ecker, and Matthias Bethge. ImIn age style transfer using convolutional neural networks. Proceedings of the IEEE conference on computer vision and pattern recognition, pages 24142423, 2016. 5 [12] Michal Geyer, Omer Bar-Tal, Shai Bagon, and Tali Dekel. Tokenflow: Consistent diffusion features for consistent video editing. arXiv preprint arxiv:2307.10373, 2023. 6 [13] Julian Jorge Andrade Guerreiro, Mitsuru Nakazawa, and Bjorn Stenger. Pct-net: Full resolution image harmonization In Proceedings of using pixel-wise color transformations. the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 59175926, 2023. 3 [14] Zonghui Guo, Dongsheng Guo, Haiyong Zheng, Zhaorui Gu, Image harmonization with Bing Zheng, and Junyu Dong. transformer. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1487014879, 2021. Intrinsic image harmonization. [15] Zonghui Guo, Haiyong Zheng, Yufeng Jiang, Zhaorui Gu, In Proand Bing Zheng. ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1636716376, 2021. 3 [16] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770778, 2016. 5 [17] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David Fleet. Video diffusion models. arXiv:2204.03458, 2022. 2 [18] Alain Hore and Djemel Ziou. Image quality metrics: Psnr vs. ssim. In 2010 20th international conference on pattern recognition, pages 23662369. IEEE, 2010. 6 [19] Andrew Hou, Ze Zhang, Michel Sarkis, Ning Bi, Yiying Tong, and Xiaoming Liu. Towards high fidelity face relighting with realistic shadows. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1471914728, 2021. [20] Andrew Hou, Michel Sarkis, Ning Bi, Yiying Tong, and Xiaoming Liu. Face relighting with geometrically consistent In Proceedings of the IEEE/CVF conference on shadows. computer vision and pattern recognition, pages 42174226, 2022. 3 [21] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei Efros. Image-to-image translation with conditional adversarial networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 11251134, 2017. 14 [22] Yasamin Jafarian and Hyun Soo Park. Learning high fidelity depths of dressed humans by watching social media dance videos. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12753 12762, 2021. 6 [23] Chaonan Ji, Tao Yu, Kaiwen Guo, Jingxin Liu, and Yebin Liu. Geometry-aware single-image full-body human relightIn European Conference on Computer Vision, pages ing. 388405. Springer, 2022. 2, 3, 6, 7, 8, 12, 14, 16, 23 [24] Yifan Jiang, He Zhang, Jianming Zhang, Yilin Wang, Zhe Lin, Kalyan Sunkavalli, Simon Chen, Sohrab Amirghodsi, Sarah Kong, and Zhangyang Wang. Ssh: self-supervised framework for image harmonization. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 48324841, 2021. 3 [25] Justin Johnson, Alexandre Alahi, and Li Fei-Fei. Perceptual losses for real-time style transfer and super-resolution. In European conference on computer vision, pages 694711. Springer, 2016. 14 tion from images of convex lambertian object. JOSA A, 18 (10):24482459, 2001. 3 [26] Yoshihiro Kanamori and Yuki Endo. Relighting humans: occlusion-aware inverse rendering for full-body human images. arXiv preprint arXiv:1908.02714, 2019. 2 [27] Hoon Kim, Minje Jang, Wonjun Yoon, Jisoo Lee, Donghyun Na, and Sanghyun Woo. Switchlight: Co-design of physicsdriven architecture and pre-training framework for human In Proceedings of the IEEE/CVF Conportrait relighting. ference on Computer Vision and Pattern Recognition, pages 2509625106, 2024. 3 [28] Manuel Lagunas, Xin Sun, Jimei Yang, Ruben Villegas, Jianming Zhang, Zhixin Shu, Belen Masia, and Diego Gutierrez. Single-image full-body human relighting. arXiv preprint arXiv:2107.07259, 2021. 2, 3 [29] Chenyang Lei, Xuanchi Ren, Zhaoxiang Zhang, and Qifeng Chen. Blind video deflickering by neural filtering with flawed atlas. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023. [30] Guannan Li, Yebin Liu, and Qionghai Dai. Free-viewpoint video relighting from multi-view sequence under general illumination. Machine vision and applications, 25:1737 1746, 2014. 3 [31] Ruilong Li, Shan Yang, David A. Ross, and Angjoo Kanazawa. Learn to dance with aist++: Music conditioned 3d dance generation, 2021. 6 [32] Yang Liu, Alexandros Neophytou, Sunando Sengupta, and Eric Sommerlade. Relighting images in the wild with self-supervised siamese auto-encoder. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 3240, 2021. 3 [33] Ziwei Liu, Ping Luo, Shi Qiu, Xiaogang Wang, and Xiaoou Tang. Deepfashion: Powering robust clothes recognition In Proceedings of the and retrieval with rich annotations. IEEE conference on computer vision and pattern recognition, pages 10961104, 2016. 6, 15, 22 [34] Ben Mildenhall, Pratul Srinivasan, Matthew Tancik, Jonathan Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. Communications of the ACM, 65(1):99106, 2021. 3 [35] Rohit Pandey, Sergio Orts Escolano, Chloe Legendre, Christian Haene, Sofien Bouaziz, Christoph Rhemann, Paul Debevec, and Sean Fanello. Total relighting: learning to relight portraits for background replacement. ACM Transactions on Graphics (TOG), 40(4):121, 2021. 2, 3, [36] F. Perazzi, J. Pont-Tuset, B. McWilliams, L. Van Gool, M. Gross, and A. Sorkine-Hornung. benchmark dataset and In evaluation methodology for video object segmentation. Computer Vision and Pattern Recognition, 2016. 6 [37] Pexels: https://www.pexels.com. 7, 19, 20 [38] Puntawat Ponglertnapakorn, Nontawat Tritrong, and Supasorn Suwajanakorn. Difareli: Diffusion face relighting. arXiv preprint arXiv:2304.09479, 2023. 3, 16 [39] Ravi Ramamoorthi and Pat Hanrahan. On the relationship between radiance and irradiance: determining the illumina- [40] Rene Ranftl, Katrin Lasinger, David Hafner, Konrad Schindler, and Vladlen Koltun. Towards robust monocular depth estimation: Mixing datasets for zero-shot cross-dataset transfer. IEEE transactions on pattern analysis and machine intelligence, 44(3):16231637, 2020. 13 [41] Mengwei Ren, Wei Xiong, Jae Shin Yoon, Zhixin Shu, Jianming Zhang, HyunJoon Jung, Guido Gerig, and He Zhang. Relightful harmonization: Lighting-aware portrait background replacement. CVPR, 2024. 3, 5, 6, 7, 12, 15, 16, 23 [42] Christian Richardt, Carsten Stoll, Neil Dodgson, HansPeter Seidel, and Christian Theobalt. Coherent spatiotemporal filtering, upsampling and rendering of rgbz videos. In Computer graphics forum, pages 247256. Wiley Online Library, 2012. 3 [43] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models, 2021. [44] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In CVPR, 2022. 2 [45] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. Unet: Convolutional networks for biomedical image segmentation. In Medical Image Computing and Computer-Assisted InterventionMICCAI 2015: 18th International Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III 18, pages 234241. Springer, 2015. 12 [46] Volker Schonefeld. Spherical harmonics. Computer Graphics and Multimedia Group, Technical Note. RWTH Aachen University, Germany, page 18, 2005. 3 [47] Qing Shuai, Chen Geng, Qi Fang, Sida Peng, Wenhao Shen, Xiaowei Zhou, and Hujun Bao. Novel view synthesis of human interactions from sparse multi-view videos. In ACM SIGGRAPH 2022 Conference Proceedings, pages 1 10, 2022. 3 [48] Jiaming Song, Chenlin Meng, and Stefano Ermon. DearXiv preprint noising diffusion implicit models. arXiv:2010.02502, 2020. 3, 13 [49] Shuangbing Song, Fan Zhong, Tianju Wang, Xueying Qin, and Changhe Tu. Guided linear upsampling. ACM Transactions on Graphics (TOG), 42(4):112, 2023. [50] Tiancheng Sun, Jonathan Barron, Yun-Ta Tsai, Zexiang Xu, Xueming Yu, Graham Fyffe, Christoph Rhemann, Jay Busch, Paul Debevec, and Ravi Ramamoorthi. Single image portrait relighting. ACM Transactions on Graphics (TOG), 38(4):112, 2019. 3 [51] Daichi Tajima, Yoshihiro Kanamori, and Yuki Endo. Relighting humans in the wild: Monocular full-body human relighting with domain adaptation. In Computer Graphics Forum, pages 205216. Wiley Online Library, 2021. 3, 6, 7, 8, 14 [52] Zachary Teed and Jia Deng. Raft: Recurrent all-pairs field In Computer VisionECCV transforms for optical flow. 2020: 16th European Conference, Glasgow, UK, August 23 28, 2020, Proceedings, Part II 16, pages 402419. Springer, 2020. 6 10 [53] Ayush Tewari, Tae-Hyun Oh, Tim Weyrich, Bernd Bickel, Hans-Peter Seidel, Hanspeter Pfister, Wojciech Matusik, Mohamed Elgharib, Christian Theobalt, et al. Monocular reconstruction of neural face reflectance fields. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 47914800, 2021. 3 [54] Marco Toschi, Riccardo De Matteo, Riccardo Spezialetti, Daniele De Gregorio, Luigi Di Stefano, and Samuele Salti. Relight my nerf: dataset for novel view synthesis and In Proceedings of the relighting of real world objects. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2076220772, 2023. [55] Yi-Hsuan Tsai, Xiaohui Shen, Zhe Lin, Kalyan Sunkavalli, Xin Lu, and Ming-Hsuan Yang. Deep image harmonization. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 37893797, 2017. 3 [56] Ke Wang, Michael Gharbi, He Zhang, Zhihao Xia, and Eli Shechtman. Semi-supervised parametric real-world image harmonization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5927 5936, 2023. 3 [57] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, and Ling Shao. Pvt v2: Improved baselines with pyramid vision transformer. Computational Visual Media, 8(3):415424, 2022. 13 [58] Yuxin Wang, Wayne Wu, and Dan Xu. Learning unified decompositional and compositional nerf for editable novel In Proceedings of the IEEE/CVF Internaview synthesis. tional Conference on Computer Vision, pages 1824718256, 2023. 3 [59] Joshua Weir, Junhong Zhao, Andrew Chalmers, and Taehyun Rhee. Deep portrait delighting. In European Conference on Computer Vision, pages 423439. Springer, 2022. 16 [60] Liu Wen, Zhixin Piao, Min Jie, Wenhan Luo, Lin Ma, , and Shenghua Gao. Liquid warping gan: unified framework for human motion imitation, appearance transfer and novel In The IEEE International Conference on view synthesis. Computer Vision (ICCV), 2019. 6 [61] Zexiang Xu, Kalyan Sunkavalli, Sunil Hadap, and Ravi Ramamoorthi. Deep image-based relighting from optimal sparse samples. ACM Transactions on Graphics (ToG), 37 (4):113, 2018. 3 [62] Zexiang Xu, Sai Bi, Kalyan Sunkavalli, Sunil Hadap, Hao Su, and Ravi Ramamoorthi. Deep view synthesis from sparse photometric images. ACM Transactions on Graphics (ToG), 38(4):113, 2019. [63] Wenhan Yang, Jiashi Feng, Jianchao Yang, Fang Zhao, Jiaying Liu, Zongming Guo, and Shuicheng Yan. Deep edge guided recurrent residual learning for image superresolution. IEEE Transactions on Image Processing, 26(12): 58955907, 2017. 5 [64] Tianwei Yin, Michael Gharbi, Richard Zhang, Eli Shechtman, Fredo Durand, William Freeman, and Taesung Park. One-step diffusion with distribution matching distillation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 66136623, 2024. 16 [65] Jae Shin Yoon, Zhixin Shu, Mengwei Ren, Cecilia Zhang, Yannick Hold-Geoffroy, Krishna Kumar Singh, and He Zhang. Generative portrait shadow removal. ACM Transactions on Graphics (TOG), 43(6):113, 2024. 16 [66] Mona Zehni, Shaona Ghosh, Krishna Sridhar, and Sethu Raman. Joint learning of portrait intrinsic decomposition and relighting. arXiv preprint arXiv:2106.15305, 2021. 3 [67] Hao Zhang, Yanbo Xu, Tianyuan Dai, Tai Chi-Keung Tang, et al. Fdnerf: Semantics-driven face reconstruction, prompt editing and relighting with diffusion models. arXiv preprint arXiv:2306.00783, 2023. 3 [68] Jiakai Zhang, Xinhang Liu, Xinyi Ye, Fuqiang Zhao, Yanshun Zhang, Minye Wu, Yingliang Zhang, Lan Xu, and Jingyi Yu. Editable free-viewpoint video using layered neural representation. ACM Transactions on Graphics (TOG), 40(4):118, 2021. [69] Longwen Zhang, Qixuan Zhang, Minye Wu, Jingyi Yu, and Lan Xu. Neural video portrait relighting in real-time via consistency modeling. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 802812, 2021. 2, 3 [70] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models, 2023. 4, 5, 12 [71] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Scaling in-the-wild training for diffusion-based illumination harmonization and editing by imposing consistent light transport. In The Thirteenth International Conference on Learning Representations, 2025. 3, 15 [72] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In CVPR, 2018. 6 [73] Yulun Zhang, Yapeng Tian, Yu Kong, Bineng Zhong, and Yun Fu. Residual dense network for image super-resolution. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 24722481, 2018. 5 [74] Ruichen Zheng, Peng Li, Haoqian Wang, and Tao Yu. Learning visibility field for detailed 3d human reconstruction and relighting. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 216226, 2023. [75] Hao Zhou, Sunil Hadap, Kalyan Sunkavalli, and David W. In InternaJacobs. Deep single portrait image relighting. tional Conference on Computer Vision (ICCV), 2019. 3, 7, 14 [76] Hao Zhou, Sunil Hadap, Kalyan Sunkavalli, and David Jacobs. Deep single-image portrait relighting. In Proceedings of the IEEE/CVF international conference on computer vision, pages 71947202, 2019. 2, 3, 6, 8, 16 [77] Taotao Zhou, Kai He, Di Wu, Teng Xu, Qixuan Zhang, Kuixiang Shao, Wenzheng Chen, Lan Xu, and Jingyi Yu. Relightable neural human assets from multi-view gradient ilIn Proceedings of the IEEE/CVF Conference luminations. on Computer Vision and Pattern Recognition, pages 4315 4327, 2023. 3 [78] Jun-Yan Zhu, Philipp Krahenbuhl, Eli Shechtman, and Alexei Efros. Learning discriminative model for the perception of realism in composite images. In Proceedings of the IEEE International Conference on Computer Vision, pages 39433951, 2015. 3 11 In this document, we provide more details for the method, experiments, dataset, and more qualitative results, as an extension of Sec. 3 and Sec. 4 in the main paper. Please also refer to the video demo for dynamic relighting results, comparison, ablation study, and more results. A. Method and Experiment Details We demonstrate that during training, instead of directly using albedo and shading maps, we train with relit images using different lighting augmentations. By leveraging conditional diffusion model, our approach can implicitly disentangle lighting and appearance from the input image, learning to generate relit images and bypassing the need for preprocessed de-lighting process. A.1. Relighting and Harmonization Diffusion Network (Sec. 3.2) Figure 9. Relighting and Harmonization diffusion model training and denoising pipeline. As shown in Fig. 9, which includes the diffusion model training process and denoising (sampling) process for our fine-grained relighting. During the training process, we follow the same Stable Diffusion architecture as [2], and both Lighting ControlNet and Motion ControlNet architecture are followed by [70]. Stable Diffusion model adopts U-Net [45] architecture comprising an encoder, middle block, and skip-connected decoder. Each of the encoder and decoder consists of 12 blocks, totaling 25 blocks within the complete model, and each primary block integrates 4 ResNet layers and 2 Vision Transformers (ViTs) with crossattention and self-attention mechanisms. The ControlNet architecture is applied at each encoder level of the U-Net, featuring trainable copy of 12 encoding blocks and 1 middle block from the Stable Diffusion model. These 12 encoding blocks includes: 64 64, 32 32, 16 16, 8 8, with each resolution replicated 3 times. The resulting outputs are merged with the 12 skip connections and the single middle block within the U-Net structure. We fine-tune both 1This work is partially done during an internship at Adobe Research. ControlNet and Stable diffusion module on our relighting dataset. A.2. Training Dataset (Sec. 4) In Fig. 12, we visualize the samples of our training dataset. We use two kinds of dataset. One is from the data captured from LightStage where the background images are rendered from HDR environment map. The ground truth shading, albedo, relighted image, and background captured from small number of viewpoints (e.g., 6 views) are available. The other one is from the data rendered from synthetic human model. We render the image of many 3D human models from many views (e.g., 16 views) under different lighting conditions defined by an environment map. We obtain the approximated spherical harmonics coefficients from the environment maps as ground-truth lighting parameters. The ground truths for the mask, albedo, background, and relit images also exist. We kindly note that our training data is relatively smaller compared to other image-based relighting methods as summarized in Fig. 10. For instance, Total Relighting [35] captures data from 70 diverse subjects. Through extensive lighting augmentation, the dataset expands to include approximately 8 million OLAT training examples; GFR [23] needs 700 subjects and 4,600 HDR maps for training; and LPBR [41] is trained on 100 subjects with OLAT and 2,908 HDR maps, resulting in 600K training samples. Our training data is composed of 100K samples where the detailed data analysis can be found in Fig. 10. We categorize our training data based on gender, skin tone, and body coverage (half-body and full-body). Each subject is captured from 32 viewpoints under varying lighting conditions. A.3. Add-on Temporal Motion Module Network (Sec. 3.3) Algorithm 1 Unsupervised Cycle-Training Motion Modeling for Temporal Consistency 1: Require: Video frames I; decoder 2: Require: Relit frames Iϕ (D Eb) 3: Initialize: Motion encoder Em; train step function 4: Converged False 5: While not Converged do (It, It ϕ D(E 6: It (It t1 D(E Converged T(It ({St ϕ, Bt}, Mt))) ϕ, Em(It1, Mt1))) t1, It) 7: 8: 9: end while We present the cycle-training algorithm for our temporal lighting module in Alg.1, which serves as an additional explanation for Sec. 3.3. Based on the hypothesis: original video sequence inherently contains tempo12 Figure 10. Left side: Training data scale comparisons; Right side: Breakdown of our training and evaluation dataset information. (It ({St ral lighting properties, which can be modeled by temporal module, conditioned on the relit version. We train an add-on temporal module in an unsupervised way. Before the training process, we require relit video frames, It It ϕ. To generate the relit frame we process forward ϕ, Bt}; It, Mt))). (It; ϕ D(E image relighting: It During each training iteration, as indicated in: It t1 D(E ϕ; Em(It1, Mt1))), we condition on the current relit frame and revert the lighting of the previous frame in the original video back to match that of the original frame. Implementation details. We train our model on 8 A100 GPUs with total batch size of 32 (4 batches per GPU) and learning rate of 2106. In the training phase for Lighting ControlNet, we initialize the Stable diffusion base model using the pre-trained weights from Instruct-Pix2Pix [2], and copy the encoder block weights to serve as the initial weights for the Lighting ControlNet part. Subsequently, we fine-tune both ControlNet and Stable Diffusion module on our relighting dataset The training of our Motion ControlNet module occurs subsequent to the lighting control training process. During the training phase for motion control, we freeze the weights of the Stable Diffusion base model. Then, we initialize the weights of the Motion ControlNet by copying the encoder block weights from the previously trained lighting Stable Diffusion. Subsequently, we exclusively fine-tune the Motion ControlNet. During the inference process, we adopt random noise with resolution of 4 96 96 as the initial input to generate the final relit image with resolution of 768 768, and for video testing, we apply the same noise across frame. We apply DDIM [48] sampler with timestep of 50 to gener13 ate the final relit image. To utilize frame-by-frame inference with recurrent blending, we extract control features from the 12 encoding blocks of the ControlNet at corresponding resolutions. Subsequently, we perform weighted blending between control feature of previous and current frames. A.4. Pixel-Aligned Neural Shading (Sec. 3.2) While coarse shading Sϕ can be directly computed from Spherical harmonics (SH) lighting parameters, we experimentally found that using Sϕ obtained from neural network can improve human relighting and harmonization. Specifically, low-order SH models tend to smooth out fine details, resulting in overly diffuse shading. In contrast, neural network can recover high-frequency shading variations, enhancing realism by capturing subtle lighting effects. Moreover, the learned shading function improves robustness to normal map inaccuracies, reducing artifacts and better preserving surface details. In this section, we introduce an alternative way of having coarse shading using neural network. To this end, we introduce pixel-aligned lighting estimation function in Eq. 2 using conditional Unet framework. It takes as inputs surface normal map and target lighting parameters ϕ as conditions, and estimates the shading Sϕ at each pixel lit by the target lighting. is detected from the input image using the internal normal detector which is composed of Unet architecture with pyramid vision transformer [57]. It learns many mixtures of ground-truth data similar to [40], and thus, applicable to general scenes and objects. Note that, since does not take any visual data as inputs, it does not introduce visual domain gaps. We train the () by comparing the input image and its reconstrucFigure 11. Left: Our shading estimation network, Right: Convolutional and deconvolutional blocks. tion from the estimated shading: Lrecon = (cid:88) Irecon 2 = (cid:88) Sϕ AGT I2 2 where Irecon is the reconstructed image based on the mulSϕ with the ground-truth albedo AGT tiplication of Rwh3. Since we supervise the shading estimation network in the image space, we can utilize other advanced image-based supervision signals that can capture the physical plausibility of the local and global shading as follows: Lshade = Lrecon + λvLvgg + λcLcGAN, (11) where Lshade is the entire objective, and λ controls the weight of each loss function. Lvgg is designed to penalize the difference between the reconstructed image Irecon and the input in the deep feature space [25]. LcGAN is the conditional adversarial loss [21] to evaluate the plausibility of the reconstructed shading with respect to the geometric structure where we use {N, I} as real and {N, Irecon} as fake conditions to the patch discriminator [21]. Coarse Shading Estimation Network. In Fig. 14, we show the general training pipeline for coarse lighting estimation network. Fig. 11 describes the structure of our coarse shading estimation network. It takes as inputs the surface normal, foreground mask, and lighting parameters (i.e., Spherical harmonics); and generates the shading map. An encoder regresses the surface normal and mask to the latent space. In this latent space, the lighting parameters are conditioned where the vector parameters are copied along the spatial direction to fit the same latent space as the one from the encoder. decoder decodes them to generate shading map. Figure 14. Training pipeline for coarse lighting estimation network. B. Qualitative Results B.1. Comparison with other baselines (Sec. 4) We present the qualitative results of static image testing on our synthetic dataset, compared with other baseline methods: DPR [75], GFR [23] and RHW [51] in Fig. 15. In our evaluation, we perform full-body and multi-person tests on our synthetic testing dataset, integrating background images alongside Spherical harmonics for lighting control. We calculate the average error on the entire testing dataset for comprehensive and generalizable relighting evaluation. From visual quantitative results, our model shows more realistic relighting results compared to other human relighting models. This demonstrates our models robust performance across diverse body part tests, indicating higher generalizability. For evaluation, we validate our model along with other baselines based on the divided categories: gender, and skin color. We present the numerical evaluation in Tab. 5 and 6. From the qualitative results, our method consistently outperforms in all categories. We further highlight that while all those methods are limited to working on specific body part (e.g., face or portrait), our method works on general cases including the scene with face, portrait, full body, and multi-person. We present real data comparison results on the LightStage dataset in Fig. 17 and comparisons on in-the-wild im14 ages in Fig. 16. Since current state-of-the-art (SOTA) baselines are not designed for comprehensive relighting, their performance varies across different scenarios. In Fig. 16, while DPR performs well for face relighting, its quality significantly deteriorates in half-body scenarios, exhibiting strong artifacts due to domain gaps. Notably, our framework is the first to achieve comprehensive relighting, effectively handling arbitrary body parts, including portraits, half-body, full-body, and multi-body scenarios. In Fig. 20 and Fig. 21, we present static real image relighting and harmonization comparison results. For harmonization, we use the most recent work, LPBR [41], as one of the baselines: (1) DPR and RHW are only applicable to image relighting with Spherical harmonics for lighting control. For fair comparison, we tested image relighting with DPR, RHW, and GFR in Fig. 20, using black background and target lighting parameters. We applied different lighting conditions to various identities, including halfbody and full-body images. Although these methods can achieve human relighting, their limited generalizability results in less fidelity during comprehensive testing. (2) Both LPBR and GFR can perform harmonization. We retrained the GFR model with our settings, enabling it to achieve both harmonization and relighting, as shown in Fig. 21. The higher generative prior of LPBR, which also uses diffusion model, results in noticeable distortions on the human face. Although GFR can achieve both harmonization and relighting, it exhibits obvious color noise. In Fig. 13, we present new comparison with ICLight [71], which is the current state-of-the-art for lightaware background harmonization. Both IC-Light and our model are stable diffusion relighting models. IC-Light can generate relit images with text prompts or background harmonization. In the visual results, our harmonization seamlessly blends with the target background while preserving the original identity. While IC-Light also achieves highit exhibits quality background harmonization, however, greater identity distortion at the same image resolution, particularly in full-body and multi-person scenarios. In Fig. 22, third graph, we show the user preference comparison among our method, LPBR, and IC-Light. Most users selected our method as the best result for all questions. For video relighting comparison, we present qualitative results in Fig. 19, in the main paper. We show frames relit by our model tested on the synthetic video testing data. The first row shows the composite input (albedo foreground and background). In the second row, we show the ground truth shading, and the third row displays the ground truth relit image. The following rows show our relit frames, followed by those from GFR, RHW, LPBR, and DPR. For real video comparison, please refer to the supplementary demo video. Method RHW DPR GFR Ours SH Bg Male 28.89 / 0.950 27.63 / 0.972 29.32 / 0.926 31.12 / 0.970 Female 26.58 / 0.939 27.62 / 0.944 29.71 / 0.973 30.50 / 0. Table 5. Comparison of baseline methods on our full-body synthetic static data, categorized by gender: (PSNR / SSIM). Method RHW DPR GFR Ours White 28.15 / 0.946 27.44 / 0.956 29.94 / 0.936 31.53 / 0.985 Brown 27.37 / 0.944 27.70 / 0.962 29.41 / 0.934 31.77 / 0.976 Dark 27.68 / 0.943 27.73 / 0.956 29.10 / 0.978 29.13 / 0.940 Table 6. Comparison of baseline methods on our full-body synthetic static data, categorized by skin color: (PSNR / SSIM). B.2. More qualitative results We present additional qualitative results on the DeepFashion dataset [33], as shown in Fig. 23. Given an input image (left side) and target lighting parameters, our model achieves the relighting results (second column). By changing the background image, our model can achieve both background harmonization and relighting, as demonstrated in columns 3 through 7. Our model can achieve realistic relighting effects given target lighting, as well as background harmonization and combination of both. It effectively handles diverse subjects with varying identities and poses, including both half-body and full-body representations, demonstrating higher generalizability. B.3. Performance and rendering time For the generation of the 768x768 pixel resolution image with stable quality, 50 diffusion timesteps are required, leading to around 10 seconds. For video sequences with relighting using motion module, each frame takes approximately 25 seconds on an A100 GPU. In theory, there is no limit in the number of frames that our model can handle, the video rendering time is highly proportional to the number of frames, requiring around 2 hours for video clip with 300 frames (768x768). B.4. User study Figure 22. User study results: Preferences between our model and other relighting and harmonization models, including our general object testing. 15 We performed user study as shown in Fig. 22. For the relighting model, we used three state-of-the-art methods: DiFaReli [38], GFR [23], and DPR [76]. For the harmonization model, we chose LPBR [41]. Users participated in answering three questions: Q1: Which result most effectively achieves the relighting? Q2: Which result most effectively preserves the persons identity (e.g., details and skin)? Q3: Which result best harmonizes with background scenes? We summarized the percentage of user preferences and plotted the pie graph as shown in Fig. 22. Overall, users selected our method as the best result for all questions, implying that our method is perceptually effective in achieving reasonable relighting quality, preserving identity, and harmonizing with the background. C. Limitation and future work In Fig. 18, we demonstrate some relighting results of the person under shadow and highlights. While our method can suppress shadows from self-occlusion during relighting, we acknowledge that our model shows some weaknesses with strong shadows, especially on human clothes (failure cases in Fig. 18, right side). In fact, these strong shadows can be further suppressed by existing shadow removal models such as [10, 59, 65]. Additionally, incorporating various training data augmentations for hard shadows can be explored as future work to further enhance relighting quality. Our relighting diffusion model requires significant computational time. Recent advancements in diffusion models, such as the One-Step Diffusion Model [64], may further enhance inference efficiency. Significant noise on the detection (e.g., mask and surface normal) affects the temporal coherence, and we admit that our results still have residual flickering. Nevertheless, our approach surpasses other relighting methods in video quality across diverse domains. We believe that advancing video prior models and expanding video datasets will further enhance temporal coherence, which we plan to explore in future work. Our task primarily focuses on human relighting, which limits the models ability to accurately handle materials associated with general objects such as cars, glass, and metallic surfaces. We acknowledge this limitation and plan to explore this aspect in future work. D. Broader Impact As positive impact, this work can be useful tool for enhancing the lighting condition of the picture with humans, which can be useful for contents creation in social media. As negative impact, similar to image synthesis, this work can synthesize human appearance under different lighting that may be used to fabricate fake videos and news. 16 Figure 12. Training samples of the relighting data with half-body portraits (up) and simulation data with full-body images (bottom) . 17 Figure 13. Comparison with harmonization methods (IC-Light). Left side is multi-person testing, right side is zoom in result. Figure 15. Qualitative comparisons conducted on synthetic data. From top to bottom: full-body testing, multi-person testing. The ground truth data is displayed in the last column. Figure 16. Comparison with DPR on face and half-body relighting on Pexels [37] real images. 19 Figure 17. Our LigtStage data testing (Left) and comparison with other relighting baselines (Right). Figure 18. Strong shadow testing results (left) and failure cases (right) on real images from Pexels [37]. 20 Figure 19. Video relighting comparison results on synthetic testing data: from left to right, we show comparison results for Scenario 1, 2, 3. From top to bottom, the first row shows the composite input (foreground human albedo composited with background image), the second row shows the ground truth (GT) shading, and the third row shows the GT image. 21 Figure 20. Real image comparisons with other human relighting approaches on the DeepFashion dataset [33]. We test on different identities and body parts (full body, half body). Our model shows consistent and feasible relighting with varying target lighting parameters (Spherical harmonics). 22 Figure 21. We present real image comparisons with the harmonization method. Given composite input image, our model can achieve effective harmonization. When provided with target lighting parameters (Spherical harmonics), our model can achieve both background harmonization and relighting. The top section displays the outputs of our background harmonization method compared to the results from [41]. The lower section presents harmonization and relighting comparisons with [23]. Due to the higher generative prior of LPBR, noticeable distortions are present on the human face. Although GFR can achieve both harmonization and relighting, it exhibits obvious color noise. 23 Figure 23. Our model can achieve realistic relighting with lighting 1 and background harmonization. Figure 24. Our model can achieve realistic relighting with lighting 2 and background harmonization. 25 Figure 25. Our model can achieve realistic relighting with lighting 3 and background harmonization."
        }
    ],
    "affiliations": [
        "Adobe Research",
        "Runway",
        "University of Southern California"
    ]
}