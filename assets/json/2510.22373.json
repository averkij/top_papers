{
    "paper_title": "VisJudge-Bench: Aesthetics and Quality Assessment of Visualizations",
    "authors": [
        "Yupeng Xie",
        "Zhiyang Zhang",
        "Yifan Wu",
        "Sirong Lu",
        "Jiayi Zhang",
        "Zhaoyang Yu",
        "Jinlin Wang",
        "Sirui Hong",
        "Bang Liu",
        "Chenglin Wu",
        "Yuyu Luo"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Visualization, a domain-specific yet widely used form of imagery, is an effective way to turn complex datasets into intuitive insights, and its value depends on whether data are faithfully represented, clearly communicated, and aesthetically designed. However, evaluating visualization quality is challenging: unlike natural images, it requires simultaneous judgment across data encoding accuracy, information expressiveness, and visual aesthetics. Although multimodal large language models (MLLMs) have shown promising performance in aesthetic assessment of natural images, no systematic benchmark exists for measuring their capabilities in evaluating visualizations. To address this, we propose VisJudge-Bench, the first comprehensive benchmark for evaluating MLLMs' performance in assessing visualization aesthetics and quality. It contains 3,090 expert-annotated samples from real-world scenarios, covering single visualizations, multiple visualizations, and dashboards across 32 chart types. Systematic testing on this benchmark reveals that even the most advanced MLLMs (such as GPT-5) still exhibit significant gaps compared to human experts in judgment, with a Mean Absolute Error (MAE) of 0.551 and a correlation with human ratings of only 0.429. To address this issue, we propose VisJudge, a model specifically designed for visualization aesthetics and quality assessment. Experimental results demonstrate that VisJudge significantly narrows the gap with human judgment, reducing the MAE to 0.442 (a 19.8% reduction) and increasing the consistency with human experts to 0.681 (a 58.7% improvement) compared to GPT-5. The benchmark is available at https://github.com/HKUSTDial/VisJudgeBench."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 2 ] . [ 1 3 7 3 2 2 . 0 1 5 2 : r VISJUDGE-BENCH: AESTHETICS AND QUALITY ASSESSMENT OF VISUALIZATIONS Yupeng Xie1, Zhiyang Zhang1, Yifan Wu1, Sirong Lu1, Jiayi Zhang1, Zhaoyang Yu2, Jinlin Wang2, Sirui Hong2, Bang Liu3, Chenglin Wu2, Yuyu Luo1 1The Hong Kong University of Science and Technology (Guangzhou), 2DeepWisdom, 3Université de Montréal & Mila"
        },
        {
            "title": "ABSTRACT",
            "content": "Visualization, domain-specific yet widely used form of imagery, is an effective way to turn complex datasets into intuitive insights, and its value depends on whether data are faithfully represented, clearly communicated, and aesthetically designed. However, evaluating visualization quality is challenging: unlike natural images, it requires simultaneous judgment across data encoding accuracy, information expressiveness, and visual aesthetics. Although multimodal large language models (MLLMs) have shown promising performance in aesthetic assessment of natural images, no systematic benchmark exists for measuring their capabilities in evaluating visualizations. To address this, we propose VISJUDGE-BENCH, the first comprehensive benchmark for evaluating MLLMs performance in assessing visualization aesthetics and quality. It contains 3,090 expert-annotated samples from real-world scenarios, covering single visualizations, multiple visualizations, and dashboards across 32 chart types. Systematic testing on this benchmark reveals that even the most advanced MLLMs (such as GPT-5) still exhibit significant gaps compared to human experts in judgment, with Mean Absolute Error (MAE) of 0.551 and correlation with human ratings of only 0.429. To address this issue, we propose VISJUDGE, model specifically designed for visualization aesthetics and quality assessment. Experimental results demonstrate that VISJUDGE significantly narrows the gap with human judgment, reducing the MAE to 0.442 (a 19.8% reduction) and increasing the consistency with human experts to 0.681 (a 58.7% improvement) compared to GPT-5. The benchmark is available at https://github.com/HKUSTDial/VisJudgeBench."
        },
        {
            "title": "INTRODUCTION",
            "content": "Visualization serves as an effective approach for transforming complex datasets into intuitive insights (Shen et al., 2023; Qin et al., 2020; Ye et al., 2024; Qin et al., 2020; Li et al., 2024a; 2025). The value of high-quality visualization depends on whether its data is faithfully presented (Fidelity), whether information is clearly communicated (Expressiveness), and whether the design is aesthetically well-presented (Aesthetics), as shown in Figure 1. These three dimensions are closely interconnected and indispensable, posing challenges for visualization quality assessment. Although Multimodal Large Language Models (MLLMs) have shown potential in aesthetic evaluation of natural images (Murray et al., 2012; Li et al., 2024c), applying them to visualization evaluation faces unique challenges. Unlike natural images, visualization evaluation requires simultaneous judgment of data encoding accuracy, information communication effectiveness, and visual design appropriateness, as shown in Figure 2. However, existing MLLMs benchmarks are insufficient for such comprehensive evaluation, as detailed in Table 1. First, chart question answering benchmarks (e.g., ChartInsights (Wu et al., 2024b)) evaluate models ability to understand chart information, rather than their overall design quality. Second, natural image aesthetic evaluation benchmarks (e.g., ArtiMuse (Li et al., 2024c)) focus on assessing aesthetics, but ignore the core purpose of visualization to effectively communicate data. Finally, existing visualization evaluation benchmarks (e.g., Corresponding author: Yuyu Luo (E-mail: yuyuluo@hkust-gz.edu.cn) 1 Figure 2: From natural images to visualization: the need for specialized visualization assessment. Green and red denote positive and negative assessments, respectively, highlighting the contrast between MLLMs capabilities in general aesthetics versus visualization-specific evaluation. VisEval (Fu et al., 2024)) mainly evaluate natural language to visualization (NL2VIS) tasks (Luo et al., 2021c), with the focus on assessing whether generated visualizations accurately reflect natural language queries, rather than the aesthetics and quality of visualizations. This leads to critical research gap: we lack systematic framework to measure MLLMs comprehensive capabilities in evaluating visualization aesthetics and quality. To address this challenge, we construct VISJUDGE-BENCH, the first comprehensive benchmark based on the Fidelity, Expressiveness, and Aesthetics principles to assess MLLMs capabilities in visualization aesthetics and quality evaluation. It contains 3,090 expert-scored samples from real-world scenarios, covering single visualizations, multiple visualizations, and dashboards across 32 chart types. Using this benchmark, we conduct extensive testing on multiple MLLMs, including GPT-5, finding that even the most advanced models show significant differences from human experts (Mean Absolute Error as high as 0.551, correlation only 0.429). This finding clearly demonstrates that general MLLMs cannot automatically acquire specialized evaluation capabilities in the visualization domain, making the development of specialized optimization models necessary. Figure 1: The Fidelity, Expressiveness, and Aesthetics evaluation framework. Based on this, we propose VISJUDGE, model specifically designed for visualization aesthetics and quality assessment, aimed at improving the consistency between general MLLMs and human expert evaluation standards. Experimental results prove the effectiveness of this approach: VISJUDGE significantly improves consistency with human experts, achieving 19.8% reduction in MAE (to 0.442) and 58.7% improvement in correlation (to 0.681) compared to GPT-5, performing best among all tested models. In summary, our main contributions are: (1) We construct VISJUDGE-BENCH, comprehensive benchmark based on Fidelity, Expressiveness, and Aesthetics principles to evaluate MLLMs capabilities in visualization assessment. (2) We systematically evaluate representative MLLMs, revealing notable gaps with human expert standards. (3) We propose VISJUDGE, an optimized model that significantly outperforms existing models and better aligns with human expert judgment. 2 Table 1: Comparison of related benchmarks across key evaluation dimensions. Types Benchmark Input Data Types Evaluation Dimensions Fidelity Expressiveness Aesthetics Aesthetic Evaluation AVA (Murray et al., 2012) ArtiMuse (Li et al., 2024c) Images Images General Images General Images Chart Understanding ChartQA (Masry et al., 2022) PlotQA (Methani et al., 2020) ChartInsights (Wu et al., 2024b) Chart, Question Chart, Question Chart, Question Single Vis Single Vis Single Vis Visualization Evaluation VisEval (Fu et al., 2024) VIS-Shepherd (Pan et al., 2025) Chart, NL, Data Chart, NL, Data VisJudge-Bench (Ours) Chart Single Vis Single Vis Single Vis, Multi Vis, Dashboard"
        },
        {
            "title": "2 RELATED WORK",
            "content": "Data Visualization Quality Assessment. Assessing the quality of data visualizations is core problem in visualization generation and recommendation tasks. In visualization recommendation tasks, the goal is to enumerate and recommend the best (top-k) visualizations for given dataset. To achieve this, existing methods fall into two main categories. The first is rule-based approaches, such as Voyager (Wongsuphasawat et al., 2017), Draco (Moritz et al., 2019), and CoInsight (Li et al., 2024b), which use heuristic scoring based on established design principles. However, their rules are often hard-coded and lack flexibility. The second category is learning-based methods, like VizML (Hu et al., 2019), DeepEye (Luo et al., 2018; 2022), and HAIChart (Xie et al., 2024). These methods train models on large annotated datasets to predict user preferences but are limited by simplistic evaluation dimensions and expensive annotated data. In NL2VIS tasks, the goal is to generate corresponding visualizations based on user-provided natural language queries (Luo et al., 2021c). Representative works include ncNet (Luo et al., 2021c), DeepVIS (Shuai et al., 2025), ChartGPT (Tian et al., 2023), and LLM4Vis (Wang et al., 2023). To assess how accurately these methods translate natural language into visualizations, several benchmarks have been proposed, including nvBench (Luo et al., 2021b;a), nvBench 2.0 (Luo et al., 2025), and Matplotagent (Yang et al., 2024). However, these methods and their related evaluations primarily focus on the models ability to write code rather than judge the quality of visualizations. MLLM as Judge. Recently, MLLMs have shown significant potential in emulating human expert judgment, paradigm known as MLLM-as-a-Judge (Zheng et al., 2023; Chen et al., 2024). As summarized in Table 1, these works can be categorized into three groups. The first is general visual aesthetics assessment, where models evaluate the artistic quality of photographs, as seen in AVA (Murray et al., 2012) and ArtiMuse (Cao et al., 2025). However, this overlooks the critical aspect of information communication efficiency in data visualization. The second is chart understanding tasks, with examples like ChartQA (Masry et al., 2022) and ChartInsights (Wu et al., 2024b), which assess the models ability to understand and interpret chart information. Yet, these works only focus on the ability to read chart information. The third is visualization evaluation, where recent works like VisEval (Fu et al., 2024) and VIS-Shepherd (Pan et al., 2025) have explored using MLLMs to judge visualizations in the context of NL2VIS tasks, focusing on whether the chart accurately reflects the natural language query. However, they fall short of comprehensive evaluation of the intrinsic design quality. This reveals gap in existing research: the absence of multidimensional framework for evaluating data fidelity, information effectiveness, and visual aesthetics. To address this, we introduce VISJUDGE-BENCH, the first comprehensive benchmark designed to systematically evaluate the capabilities of MLLMs as visualization quality judges."
        },
        {
            "title": "3 VISJUDGE-BENCH: DESIGN AND CONSTRUCTION",
            "content": "To systematically evaluate the capability boundaries of MLLMs in visualization evaluation, we design VISJUDGE-BENCH. As shown in Figure 3, its construction follows three-stage methodology: (1) data collection and processing; (2) adaptive question generation; and (3) expert annotation and quality control. We detail the specific implementation of each stage below. 3 Figure 3: VISJUDGE-BENCH construction framework. Table 2: VISJUDGE-BENCH statistical information (Dash. = Dashboard). Vis Type Count #-Subtype Subtype Details (Count) Single Vis 1,041 22 Multi Vis 1,024 Dashboard 1,025 5 Bar Chart Area Chart Histogram Treemap ... 10 more subcategories 176 Pie Chart 75 Heatmap 48 Donut Chart 62 Sankey Diagram Comparison Views 670 Small Multiples 59 Overview Detail Other Multi View 129 Line Chart 55 Scatter Plot 47 Funnel Chart 61 Bubble Chart 100 49 45 195 Coordinated Views 97 3 Analytical Dash. Strategic Dash. 743 Operational Dash. 122 Interactive Dash. 91 62 Other Dash. 7 3.1 BENCHMARK CONSTRUCTION PIPELINE 3.1.1 DATA COLLECTION AND PREPROCESSING This stage constructs the visualization corpus through two key components: corpus construction and data preprocessing. Corpus Construction. To evaluate the performance of MLLMs across different visualization types, we construct corpus covering three main categories: single visualizations, multiple visualizations, and dashboards. To ensure the authenticity and diversity of our corpus, we collect visualization samples from search engines using web crawling methods with diverse query keywords (see Appendix A.1.1 for detailed crawling architecture and keyword generation strategy). Data Preprocessing Pipeline. We design three-stage data filtering process to curate the bench- (1) Initial Filtering: We employ automated scripts and mark from over 300,000 initial images. 4 perceptual hash algorithms to eliminate non-visualization content and duplicates, yielding 80,210 candidate images (detailed algorithms in Appendix A.1.1). (2) Automated Classification: We leverage GPT-4o for visualization type classification and quality filtering, resulting in 13,220 valid visualization samples after human verification (classification prompts and criteria in Appendix A.1.1). (3) Stratified Sampling: We apply stratified random sampling to select the final 3,090 samples, ensuring balanced distribution across categories. As shown in Table 2, the final corpus contains 1,041 single visualizations, 1,024 multiple visualizations, and 1,025 dashboards, covering 32 distinct subtypes. Complete statistical breakdown is provided in Appendix A.2.1."
        },
        {
            "title": "3.1.2 THE “FIDELITY, EXPRESSIVENESS, AND AESTHETICS” EVALUATION FRAMEWORK",
            "content": "To enable fine-grained visualization assessment, this stage first establishes multi-dimensional evaluation framework, then implements an adaptive question generation process based on this framework (as illustrated in the upper-right panel of Figure 3). The Fidelity, Expressiveness, and Aesthetics Framework Design. To systematically evaluate visualization quality, we construct multi-dimensional evaluation framework. This framework draws inspiration from classical translation theory principles of Fidelity, Expressiveness, and Aesthetics (illustrated with positive-negative examples in Figure 1), combined with established theories in graphical perception (Cleveland & McGill, 1984), information visualization design (Munzner, 2014), and aesthetic evaluation (Li et al., 2024c). We operationalize this core concept into six measurable evaluation dimensions (as shown in Figure 3): Fidelity focuses on Data Fidelity. This dimension draws from Tuftes design principles (Tufte, 1983) for avoiding graphical lies and recent research on visualization misleadingness issues (Nguyen et al., 2013; Szafir, 2018; Lan & Liu, 2024; McNutt et al., 2020). It primarily evaluates whether visual encodings accurately reflect the original data, avoiding misleading interpretations caused by improper axis settings, scale distortions, or other design flaws. Expressiveness focuses on the effectiveness of information communication. This dimension evaluates how effectively visualizations convey information to users. It includes two progressive sub-dimensions: First, (1) Semantic Readability evaluates the clarity of basic information encoding, assessing whether users can unambiguously decode visual elements in charts (Pan et al., 2025). Building on chart readability, (2) Insight Discovery further evaluates the analytical value in revealing deep data patterns, trends, or outliers, helping users transition from reading information to gaining insights (Wu et al., 2024a). Aesthetics focuses on Aesthetic Quality of visual design, integrating visualization perception theory (Ware, 2021) with design practice. This dimension consists of three sub-dimensions that collectively influence the overall visual experience: (1) Design Style evaluates the innovation and uniqueness of design, measuring the degree of novel visual elements and distinctive style (Dibia, 2023; Brath & Banissi, 2016); (2) Visual Composition focuses on the rationality of spatial layout, evaluating the balance and order of element positioning, size proportions, and spacing arrangements (Wu et al., 2023); and (3) Color Harmony evaluates the coordination and functionality of color combinations, ensuring color palette choices balance aesthetics with effective information communication (Harrower & Brewer, 2003; Gramazio et al., 2017). In addition, this evaluation framework offers flexibility, with specific evaluation criteria and score weights adaptively customized according to different visualization types (such as single visualizations (Wu et al., 2024a), multiple visualizations (Chen et al., 2020), and dashboards (Bach et al., 2023)). Complete evaluation rules and customization details are provided in Appendix C. Adaptive Question Generation Mechanism. Based on the evaluation framework, we have devised an adaptive question generation process (detailed workflow shown in Figure 3). This process begins by leveraging GPT-4o to extract metadata from the chart, such as its type and visual elements. Subsequently, it rewrites questions by populating predefined templates based on this metadata, generating highly customized questions for the six evaluation sub-dimensions. This approach ensures that the evaluation questions are closely aligned with the specific visualization content. For more detailed examples, please refer to Appendix C.1."
        },
        {
            "title": "3.1.3 EXPERT ANNOTATION AND QUALITY CONTROL",
            "content": "To build reliable human ground truth, VISJUDGE-BENCH adopts rigorous three-stage annotation and quality control workflow (bottom panel of Figure 3) informed by benchmark construction approaches (Rein et al., 2024; Liu et al., 2025; Zhu et al., 2024). This systematic process ensures high-fidelity and consistent scoring through careful review and expert judgment. Stage 1: Initial Annotation. We recruited highly qualified crowdsourcing workers through the CloudResearch platform (CloudResearch, 2022). To ensure annotation quality, we not only set strict screening criteria (e.g., Bachelors degree or higher, 97%+ task approval rate, native English speaker, and professional background in multiple relevant fields; see Appendix A.3.1 for details), but also designed dedicated annotation interface (see Appendix A.3.3) to guide the process. Crucially, we embedded validation checks into the annotation tasks to identify and filter out inattentive responses (examples in Appendix A.3.4). We paid USD $10 per hour for this task. Each of the 3,090 samples was scored by three independent annotators across six evaluation dimensions (see Appendix A.3.2 for task design details), generating an initial scoring matrix. Stage 2: Quality Control. To address scoring disagreements among annotators, we designed systematic conflict identification and resolution mechanism based on established crowdsourcing quality control and statistical evaluation theory (Gadiraju et al., 2015; Rousseeuw & Leroy, 2005; Brennan, 2001). The system first identifies high-disagreement samples by analyzing score variance, then algorithmically generates candidate resolution strategies including outlier removal, malicious scoring detection, and sub-dimensional bias correction. These algorithm-generated suggestions are processed and ranked before being submitted to the expert team for final review (complete algorithmic details and parameters in Appendix A.3.4). Stage 3: Expert Validation. The final review phase is handled by team of three experts with visualization analysis experience (expert interface and workflow described in Appendix A.3.5). The experts independently review disputed samples and algorithm-generated candidate solutions, using their professional knowledge to select, modify, or reject strategies. For particularly complex disputed samples, the expert team reaches consensus through discussion. Through this end-to-end rigorous process, we ultimately built high-quality human scoring benchmark for all 3,090 samples, serving as the gold standard for evaluating model performance."
        },
        {
            "title": "4 VISJUDGE: A SPECIALIZED MODEL FOR VISUALIZATION EVALUATION",
            "content": "To validate VISJUDGE-BENCH as an effective training resource, we fine-tuned specialized model called VISJUDGE to enhance MLLM visualization evaluation capabilities. Training Setup. We use VISJUDGE-BENCHs human-annotated data with 70%/10%/20% train/- validation/test split (2,163/279/648 samples) via stratified sampling to maintain consistent visualization type distribution across all splits. Training data is kept separate from baseline evaluation to prevent contamination. Model Training. We selected Qwen2.5-VL-7B-Instruct (Bai et al., 2025) as our base model for its strong multimodal capabilities. The model generates quality scores (1.0-5.0) and rationales aligned with human expert judgments. We employ reinforcement learning with the GRPO algorithm (Shao et al., 2024), using composite reward function combining accuracy reward (minimizing prediction error) and format reward (ensuring structured outputs) (Shi et al., 2025; Wu et al., 2025). Formal reward definitions are detailed in Appendix D.2. For parameter-efficient fine-tuning, we adopted the Low-Rank Adaptation (LoRA) (Hu et al., 2022). Training used four NVIDIA A6000 (48GB) GPUs for 5 epochs with learning rate of 1e-5. Detailed configurations are in Appendix D.3."
        },
        {
            "title": "5 EXPERIMENTS",
            "content": "5.1 EXPERIMENTAL SETTINGS To evaluate existing MLLMs in visualization quality assessment and validate our VISJUDGE, we conduct comprehensive experiments on VISJUDGE-BENCH. 6 Table 3: Overall performance of MLLMs and the VISJUDGE on VISJUDGE-BENCH across different evaluation metrics and dimensions. Metric Model Overall Fidelity Expressiveness Aesthetics Readability Insight Design Style Composition Color MAE () MSE () Corr. () Claude-3.5-Sonnet Claude-4-Sonnet Gemini-2.0-Flash Gemini-2.5-Pro GPT-4o GPT-5 Qwen2.5-VL-7B VisJudge Claude-3.5-Sonnet Claude-4-Sonnet Gemini-2.0-Flash Gemini-2.5-Pro GPT-4o GPT-5 Qwen2.5-VL-7B VisJudge Claude-3.5-Sonnet Claude-4-Sonnet Gemini-2.0-Flash Gemini-2.5-Pro GPT-4o GPT-5 Qwen2.5-VL-7B VisJudge 0.823 0.618 0.68 0.661 0.609 0.551 1.048 0.442 1.006 0.596 0.716 0.674 0.575 0.484 1.502 0.306 0.395 0.47 0.395 0.266 0.482 0.429 0.322 0. 0.977 0.839 0.828 1.241 0.986 0.861 1.169 0.662 1.573 1.18 1.18 2.287 1.557 1.214 2.047 0.751 0.325 0.392 0.371 0.18 0.382 0.256 0.34 0.571 0.902 0.757 0.91 0.944 0.804 0.78 1.294 0.649 1.303 0.974 1.323 1.477 1.06 0.988 2.409 0.693 0.491 0.548 0.458 0.379 0.539 0.438 0.349 0. 1.152 0.83 0.818 0.898 0.742 0.776 0.857 0.679 1.982 1.142 1.114 1.36 0.918 0.966 1.176 0.762 0.366 0.453 0.418 0.357 0.442 0.383 0.278 0.572 0.782 0.678 0.637 0.839 0.608 0.648 0.755 0.581 0.99 0.771 0.671 1.108 0.625 0.719 0.937 0.545 0.456 0.422 0.46 0.447 0.472 0.463 0.356 0. 0.939 0.733 0.728 0.918 0.694 0.698 0.812 0.546 1.463 0.932 0.922 1.322 0.821 0.859 1.091 0.498 0.137 0.164 0.157 0.194 0.277 0.277 0.148 0.512 0.862 0.785 0.798 0.98 0.657 0.682 0.772 0.604 1.198 1.037 1.057 1.46 0.729 0.81 0.996 0.578 0.259 0.228 0.209 0.208 0.363 0.295 0.155 0. Evaluation Setup. We evaluate seven representative MLLMs: GPT-5, GPT-4o, Claude-4-Sonnet, Claude-3.5-Sonnet, Gemini-2.0-Flash, Gemini-2.5-Pro, Qwen2.5-VL-7B-Instruct (Bai et al., 2025), and VISJUDGE on balanced test set of 648 samples (see Appendix A.2.3 for distribution details). Each model provides 1-to-5 scores with justifications based on our Fidelity, Expressiveness, and Aesthetics framework. Following human annotation procedures, we run each model three times and average the results. All inference uses vLLM on four NVIDIA A6000 (48GB) GPUs with bfloat16 precision and temperature of 0.8. Evaluation Metrics. We assess model performance through correlation analysis using the Pearson coefficient and error metrics (MAE and MSE) compared to human scores. We also analyze score distributions to identify systematic biases. Metrics are computed for each sub-dimension and aggregated across the three main evaluation dimensions. 5.2 EXPERIMENTAL RESULTS AND ANALYSIS 5.2.1 CAN MLLMS ASSESS VISUALIZATION AESTHETICS AND QUALITY LIKE HUMANS? Table 3 presents comprehensive performance comparison of seven representative models including the latest GPT-5 across our Fidelity, Expressiveness, and Aesthetics evaluation framework, revealing significant capability differences and systematic limitations in current MLLMs for visualization aesthetics and quality assessment. Hierarchical Capability Structure. Current MLLMs exhibit clear hierarchical performance structure across evaluation dimensions. Models perform relatively well on Fidelity dimensions, reflecting their fundamental capability in identifying obvious data errors (e.g., proportion distortions, baseline issues). Within Expressiveness dimensions, models show better performance on Insight Discovery (average MAE 0.87) compared to Semantic Readability (average MAE 0.91). Most prominently, all models struggle significantly with Aesthetics dimensions across all three aesthetic sub-dimensions, with average MAE around 0.76 and most correlations below 0.3 (except Design Style at 0.44), highlighting the inherent challenges of subjective aesthetic assessment, which often involves nuanced cultural context and abstract design principles that are difficult for current models to grasp. Model-Specific Evaluation Characteristics. Through fine-grained analysis, we identify distinct evaluation personalities across different models. GPT-5 demonstrates balanced performance across dimensions with consistently competitive scores, particularly excelling in overall accuracy; 7 Figure 4: Distribution and bias analysis of MLLM scores. Score distribution density curves showing the rating patterns of different models compared to human experts on the 1-5 scale. GPT-4o shows relative strength in Color Harmony assessment (MAE 0.657), reflecting sensitivity to color aesthetics; Claude-4-Sonnet excels in Semantic Readability evaluation (MAE 0.757), showing advantages in information communication assessment; while Gemini-2.0-Flash leads in Data Fidelity (MAE 0.828), indicating focus on data accuracy. These differentiated capability distributions validate VISJUDGE-BENCHs diagnostic value and provide guidance for practical model selection. Domain-Specific Fine-tuning Effectiveness. Our specialized VISJUDGE achieves superior performance across all core metrics, with an overall MAE of 0.442 and correlation of 0.681. Among commercial models, GPT-5 achieves the best MAE performance (0.551) while GPT-4o reaches the highest correlation (0.482). Compared to these strong baselines, VISJUDGE demonstrates substantial improvements: 19.8% MAE reduction over GPT-5 (from 0.551 to 0.442) and 41.3% correlation improvement over GPT-4o (from 0.482 to 0.681), demonstrating the substantial potential of domainspecific fine-tuning. 5.2.2 DO MLLMS EXHIBIT HUMAN-LIKE SCORING BEHAVIORS? To analyze systematic biases in model evaluation behavior, we examine score distribution patterns across different models. Figure 4 reveals significant bias issues in current MLLMs compared to human experts (µ = 3.13). Systematic Biases in Current Models. Most models exhibit score inflation with rightward-shifted distributions. Qwen2.5-VL-7B and Claude-3.5-Sonnet show the most severe inflation (µ = 3.89 and µ = 3.87), while Gemini-2.0-Flash, GPT-4o, Claude-4-Sonnet, and GPT-5 demonstrate moderate inflation (µ = 3.64, µ = 3.53, µ = 3.56, and µ = 3.36 respectively). Notably, GPT-5 shows relatively better control compared to other inflated models. Conversely, Gemini-2.5-Pro exhibits overly conservative behavior (µ = 3.02). Additionally, models like Qwen2.5-VL-7B, Claude-3.5-Sonnet, and Gemini-2.0-Flash exhibit sharp peaks around 4.0, indicating excessive score concentration that limits discriminative capability. Effective Bias Correction through Fine-tuning. Our VISJUDGE achieves near-perfect alignment with human scoring patterns (µ = 3.11) and maintains broader, more balanced distribution. This demonstrates that domain-specific fine-tuning effectively corrects both inflation and concentration issues, achieving human-like evaluation behaviors. 5.2.3 HOW DOES VISUALIZATION COMPLEXITY AFFECT MODEL PERFORMANCE? To understand model robustness across varying complexity, we analyze eight models on three visualization types: single visualizations, multiple visualizations, and dashboards. Figure 5 shows the main trends. Performance Degradation with Complexity. All models show consistent performance degradation: single visualizations > multiple visualizations > dashboards. VISJUDGE achieves the best performance across all types with correlations of 0.577 (single visualizations), 0.565 (multiple visualizations), and 0.375 (dashboards), significantly outperforming baselines. This demonstrates the effectiveness of domain-specific fine-tuning for complex multi-element interactions. Stability in Complex Scenarios. Baseline models show significant instability in complex scenarios. For dashboards, most baselines experience substantial correlation drops, with Claude-3.5Sonnet and GPT-5 even showing negative correlations in Data Fidelity (-0.031 and -0.013), while 8 Figure 5: ModelHuman rating correlation across visualization types. Figure 6: Model evaluation examples on low-quality visualizations. Figure 7: Case study highlighting the conservative bias of Gemini-2.5-Pro. VISJUDGE maintains consistency (0.224-0.482). Functional dimensions (Data Fidelity, Semantic Readability) remain stable across types, but aesthetic dimensions struggle with complex layouts, particularly Visual Composition in dashboards (most models <0.2). These findings highlight the critical importance of specialized training for robust visualization evaluation across diverse complexity levels. 5.2.4 HOW DO MODEL EVALUATION BEHAVIORS DIFFER IN PRACTICE? To qualitatively analyze model evaluation behaviors, our case studies reveal two common biases: score inflation and overly conservative assessments. Figure 6 illustrates score inflation on low-quality visualizations. For chaotic treemap (human rating: 1.67), baseline models give inflated scores. For instance, Qwen2.5-VL-7B (3.67) praises its clear legend while ignoring the confusing layout, and Claude-4-Sonnet (3.08) incorrectly highlights excellent spatial organization. In contrast, VISJUDGEs score of 2.00 aligns with human judgment, correctly identifying the chaotic layout that impairs interpretation. 9 (a) Training data scale vs. human-model correlation (b) Training data scale vs. prediction error Figure 8: Impact of training data scale on VISJUDGE model performance. Conversely, Figure 7 highlights the overly conservative bias of Gemini-2.5-Pro. For high-quality dashboard rated 4.17 by humans, Gemini-2.5-Pro gives disproportionately low score of 2.94, focusing on single data inconsistency while overlooking the charts overall effectiveness. Similarly, for another chart (human rating: 3.56), it scores only 2.33 due to the use of dual Y-axes. While other models like GPT-5 and Claude-4-Sonnet provide scores closer to human ratings, VISJUDGE also demonstrates more balanced evaluations (3.83 and 3.00, respectively). For more detailed case studies and complete model outputs, see Appendix B. Specifically, we provide high-score cases (Appendix B.1), medium-score cases (Appendix B.2), and low-score cases (Appendix B.3) demonstrating human-model alignment across different quality levels. Additionally, dimension-specific case studies (Appendix B.4) validate our evaluation criteria, while comprehensive model error analysis (Appendix B.5) reveals systematic failure patterns. 5.2.5 HOW DOES TRAINING DATA SCALE AFFECT MODEL PERFORMANCE? To evaluate data scaling effects and guide deployment strategies, we analyze VISJUDGE performance across different training data scales with single training epoch. To ensure fairness, data samples are proportionally extracted based on visualization types and score distributions. Figure 8 reveals clear mathematical patterns as data scale increases. Predictable Scaling Laws. Model performance follows well-defined trends: human-model correlation shows logarithmic growth (R²=0.628) from 0.30 to 0.65 at 2,442 samples, while prediction errors exhibit exponential decay with MAE decreasing from 1.05 to 0.45 (R²=0.984) and MSE from 1.55 to 0.30 (R²=0.992). The 500-1,000 sample range provides the most efficient improvement, contributing 45% of total correlation gains, while beyond 1,000 samples, marginal returns diminish but remain valuable for continued enhancement."
        },
        {
            "title": "6 CONCLUSION",
            "content": "This paper constructs VISJUDGE-BENCH and fine-tunes VISJUDGE to validate the effectiveness of domain-specific training. Our research finds that existing MLLMs (including GPT-5) show significant gaps with human experts in visualization evaluation, exhibiting issues like scoring bias. VISJUDGE effectively mitigates these problems, achieving 19.8% MAE reduction and 58.7% correlation improvement over GPT-5. VISJUDGE-BENCH provides standardized evaluation platform for the community, while VISJUDGEs success demonstrates that domain-specific training is viable approach for improving MLLMs evaluation capabilities, supporting future work on finer evaluation and higher-quality visualization generation."
        },
        {
            "title": "ETHICS STATEMENT",
            "content": "The VISJUDGE-BENCH framework presented in this work aims to improve multimodal large language models capabilities in visualization quality assessment and promote the development of automated visualization evaluation technology. We believe this work will not produce direct negative social impacts, but recognize that the framework should be used with caution and ethical oversight when applied to sensitive domains or potentially harmful models. Although VISJUDGE-BENCH aims to objectively assess visualization quality, the base models it relies on (such as Qwen2.5-VL7B) or the datasets used to construct the benchmark may inadvertently reflect biases. Future work 10 could investigate the fairness implications of these evaluation features across different populations, cultural backgrounds, and visualization styles. We particularly focus on the following ethical considerations: (1) strict compliance with copyright and usage terms during data collection; (2) ensuring fair compensation and voluntary participation for expert annotators; (3) avoiding content that may reinforce stereotypes or biases in benchmark design; (4) open-source release aimed at promoting community development rather than commercial monopoly."
        },
        {
            "title": "REFERENCES",
            "content": "Benjamin Bach, Euan Freeman, Alfie Abdul-Rahman, Cagatay Turkay, Saiful Khan, Yulei Fan, and Min Chen. Dashboard design patterns. IEEE Transactions on Visualization and Computer Graphics, 29(1):342352, 2023. Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. Richard Brath and Ebad Banissi. Using typography to expand the design space of data visualization. She Ji: The Journal of Design, Economics, and Innovation, 2(1):5987, 2016. Robert Brennan. Generalizability theory. Springer, 2001. Shuang Cao, Ning Ma, Jian Li, Xiaodong Li, Ling Shao, Kexin Zhu, and Jie Wu. Artimuse: Finegrained image aesthetics assessment with joint scoring and expert-level understanding. arXiv preprint arXiv:2507.14533, 2025. Liang Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Chen He, Jianfeng Wang, Yixuan Zhao, and Dahua Lin. Mllm-as-a-judge: Assessing multimodal llm-as-a-judge with vision-language benchmark. In International Conference on Machine Learning, pp. 79618012. PMLR, 2024. Xumeng Chen, Wei Zeng, Yu Lin, Saud Al-Dohuki, Jian Li, Yixuan Zhang, Jiansu Wang, Chao Ma, Jie Yang, Jinzhu Pan, et al. Composition and configuration patterns in multiple-view visualizations. IEEE Transactions on Visualization and Computer Graphics, 27(2):15141524, 2020. William Cleveland and Robert McGill. Graphical perception: Theory, experimentation, and application to the development of graphical methods. Journal of the American Statistical Association, 79(387):531554, 1984. CloudResearch. Cloudresearch: Powering better research through better data, 2022. URL https: //www.cloudresearch.com. Victor Dibia. Lida: tool for automatic generation of grammar-agnostic visualizations and infographics using large language models. arXiv preprint arXiv:2303.02927, 2023. Lei Fu, Song Gao, Kai Zheng, Dakuo Wang, and Nan Tang. Viseval: benchmark for data visualization in the era of large language models. arXiv preprint arXiv:2408.00928, 2024. Ujwal Gadiraju, Ricardo Kawase, Stefan Dietze, and Gianluca Demartini. Understanding malicious behavior in crowdsourcing platforms: The case of online surveys. In Proceedings of the 33rd annual ACM conference on human factors in computing systems, pp. 16311640, 2015. Connor Gramazio, David Laidlaw, and Karen Schloss. Colorgorical: Creating discriminable and preferable color palettes for information visualization. IEEE Transactions on Visualization and Computer Graphics, 23(1):521530, 2017. Mark Harrower and Cynthia Brewer. Colorbrewer.org: An online tool for selecting colour schemes for maps. The Cartographic Journal, 40(1):2737, 2003. Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. ICLR, 1(2):3, 2022. Kevin Hu, Michiel Bakker, Stephen Li, Tim Kraska, and César Hidalgo. Vizml: machine learning approach to visualization recommendation. In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems, pp. 112, 2019. 11 Xinhuan Lan and Yun Liu. came across junk: Understanding design flaws of data visualization from the publics perspective. IEEE Transactions on Visualization and Computer Graphics, 2024. Boyan Li, Yuyu Luo, Chengliang Chai, Guoliang Li, and Nan Tang. The dawn of natural language to SQL: are we fully ready? [experiment, analysis & benchmark ]. Proc. VLDB Endow., 17 (11):33183331, 2024a. doi: 10.14778/3681954.3682003. URL https://www.vldb.org/ pvldb/vol17/p3318-luo.pdf. Boyan Li, Jiayi Zhang, Ju Fan, Yanwei Xu, Chong Chen, Nan Tang, and Yuyu Luo. Alpha-SQL: Zero-shot text-to-SQL using monte carlo tree search. In Forty-second International Conference on Machine Learning, 2025. URL https://openreview.net/forum?id=kGg1ndttmI. Guozheng Li, Runfei Li, Yunshan Feng, Yu Zhang, Yuyu Luo, and Chi Harold Liu. Coinsight: Visual storytelling for hierarchical tables with connected insights. IEEE Transactions on Visualization and Computer Graphics, 2024b. Jiayang Li, Yu Zhang, Dakuo Wang, Lin Chen, and Pengyuan Zhang. Artimuse: Fine-grained image aesthetics assessment with joint scoring and expert-level understanding. arXiv preprint arXiv:2404.12569, 2024c. Xinyu Liu, Shuyu Shen, Boyan Li, Nan Tang, and Yuyu Luo. Nl2sql-bugs: benchmark for detecting semantic errors in nl2sql translation. In Proceedings of the 31st ACM SIGKDD Conference on Knowledge Discovery and Data Mining V. 2, pp. 56625673, 2025. Tianqi Luo, Chuhan Huang, Leixian Shen, Boyan Li, Shuyu Shen, Wei Zeng, Nan Tang, and Yuyu Luo. nvbench 2.0: Resolving ambiguity in text-to-visualization through stepwise reasoning. arXiv preprint arXiv:2503.12880, 2025. Yuyu Luo, Xuedong Qin, Nan Tang, and Guoliang Li. Deepeye: Towards automatic data visualization. In 2018 IEEE 34th International Conference on Data Engineering (ICDE), pp. 101112. IEEE, 2018. Yuyu Luo, Jiawei Tang, and Guoliang Li. nvbench: large-scale synthesized dataset for crossdomain natural language to visualization task. arXiv preprint arXiv:2112.12926, 2021a. Yuyu Luo, Nan Tang, Guoliang Li, Chengliang Chai, Wenbo Li, and Xuedi Qin. Synthesizing natural language to visualization (nl2vis) benchmarks from nl2sql benchmarks. In Proceedings of the 2021 International Conference on Management of Data, pp. 12351247, 2021b. Yuyu Luo, Nan Tang, Guoliang Li, Jintao Tang, Chengliang Chai, and Xuedong Qin. Natural language to visualization by neural machine translation. IEEE Transactions on Visualization and Computer Graphics, 28(1):217226, 2021c. Yuyu Luo, Xuedi Qin, Chengliang Chai, Nan Tang, Guoliang Li, and Wenbo Li. Steerable selfdriving data visualization. IEEE Trans. Knowl. Data Eng., 34(1):475490, 2022. Ahmed Masry, Xuan Long Do, Shafiq Joty, and Enamul Hoque. Chartqa: benchmark for question In Findings of the Association for answering about charts with visual and logical reasoning. Computational Linguistics: ACL 2022, pp. 22632279, 2022. Andrew McNutt, Gordon Kindlmann, and Michael Correll. Surfacing visualization mirages. In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems, pp. 116, 2020. Nitesh Methani, Pritha Ganguly, Mitesh Khapra, and Anupam Kumar. Plotqa: Reasoning over scientific plots. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pp. 15271536, 2020. Dominik Moritz, Chenglong Wang, Gregory Nelson, Halden Lin, Adam M. Smith, Bill Howe, and Jeffrey Heer. Formalizing visualization design knowledge as constraints: Actionable and extensible models in draco. IEEE Trans. Visualization & Comp. Graphics (Proc. InfoVis), 2019. Tamara Munzner. Visualization analysis and design. CRC press, 2014. 12 Naila Murray, Luca Marchesotti, and Florent Perronnin. Ava: large-scale database for aesthetic visual analysis. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 24082415, 2012. Quan Nguyen, Peter Eades, and Seok-Hee Hong. On the faithfulness of graph visualizations. In"
        },
        {
            "title": "2013 IEEE Pacific Visualization Symposium (PacificVis), pp. 209–216. IEEE, 2013.",
            "content": "Bo Pan, Yixiao Fu, Ke Wang, Junyu Lu, Lunke Pan, Ziyang Qian, Yuhan Chen, Guoliang Wang, Yitao Zhou, and Li Zheng. Vis-shepherd: Constructing critic for llm-based data visualization generation. arXiv preprint arXiv:2506.13326, 2025. Xuedi Qin, Yuyu Luo, Nan Tang, and Guoliang Li. Making data visualization more efficient and effective: survey. VLDB J., 29(1):93117, 2020. David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel Bowman. Gpqa: graduate-level google-proof q&a benchmark. In First Conference on Language Modeling, 2024. Peter Rousseeuw and Annick Leroy. Robust regression and outlier detection. John wiley & sons, 2005. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Leixian Shen, Enya Shen, Yuyu Luo, Xiaocong Yang, Xuming Hu, Xiongshuai Zhang, Zhiwei Tai, and Jianmin Wang. Towards natural language interfaces for data visualization: survey. IEEE Trans. Vis. Comput. Graph., 29(6):31213144, 2023. Jingze Shi, Yifan Wu, Bingheng Wu, Yiran Peng, Liangdong Wang, Guang Liu, and Yuyu Luo. Trainable dynamic mask sparse attention. CoRR, abs/2508.02124, 2025. Zhihao Shuai, Boyan Li, Siyu Yan, Yuyu Luo, and Weikai Yang. Deepvis: Bridging natural language and data visualization through step-wise reasoning. arXiv preprint arXiv:2508.01700, 2025. Danielle Albers Szafir. The good, the bad, and the biased: Five ways visualizations can mislead (and how to fix them). Interactions, 25(4):2633, 2018. Yuan Tian, Weiwei Cui, Dazhen Deng, Xinjing Yi, Yurun Yang, Haidong Zhang, and Yingcai Wu. Chartgpt: Leveraging llms to generate charts from abstract natural language. arXiv preprint arXiv:2311.01920, 2023. Edward Tufte. The visual display of quantitative information. Graphics Press, 1983. Lei Wang, Songheng Zhang, Yun Wang, Ee-Peng Lim, and Yong Wang. Llm4vis: Explainable visualization recommendation using chatgpt. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing: Industry Track, pp. 675692, 2023. Colin Ware. Information visualization: perception for design. Morgan Kaufmann, 4 edition, 2021. Kanit Wongsuphasawat, Zening Qu, Dominik Moritz, Riley Chang, Felix Ouk, Anushka Anand, Jock Mackinlay, Bill Howe, and Jeffrey Heer. Voyager 2: Augmenting visual analysis with partial view specifications. In Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems, pp. 26482659, 2017. Bingheng Wu, Jingze Shi, Yifan Wu, Nan Tang, and Yuyu Luo. Transxssm: hybrid transformer state space model with unified rotary position embedding. CoRR, abs/2506.09507, 2025. Jiaqi Wu, Hao Li, Yixing Zhang, Dakuo Wang, and Nan Tang. Viseval: benchmark for data visualization in the era of large language models. arXiv preprint arXiv:2408.00928, 2024a. John Wu, John Joon Young Chung, and Eytan Adar. viz2viz: Prompt-driven stylized visualization generation using diffusion model. arXiv preprint arXiv:2304.01919, 2023. 13 Yifan Wu, Lutao Yan, Leixian Shen, Yunhai Wang, Nan Tang, and Yuyu Luo. Chartinsights: Evaluating multimodal large language models for low-level chart question answering. arXiv preprint arXiv:2405.07001, 2024b. Yupeng Xie, Yuyu Luo, Guoliang Li, and Nan Tang. Haichart: Human and ai paired visualization system. Proceedings of the VLDB Endowment, 17(11):31783191, 2024. Zhiyu Yang, Zihan Zhou, Shuo Wang, Xin Cong, Xu Han, Yukun Yan, Zhenghao Liu, Zhixing Tan, Pengyuan Liu, Dong Yu, et al. Matplotagent: Method and evaluation for llm-based agentic scientific data visualization. arXiv preprint arXiv:2402.11453, 2024. Yilin Ye, Jianing Hao, Yihan Hou, Zhan Wang, Shishi Xiao, Yuyu Luo, and Wei Zeng. Generative ai for visualization: State of the art and future directions. Visual Informatics, 8(2):4366, 2024. ISSN 2468-502X. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yong Zhuang, Zhuohan Lin, Dacheng Li, Eric Xing, Hao Zhang, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural Information Processing Systems, 36, 2023. Yizhang Zhu, Shiyin Du, Boyan Li, Yuyu Luo, and Nan Tang. Are large language models good statisticians? Advances in Neural Information Processing Systems, 37:6269762731, 2024."
        },
        {
            "title": "Appendix Contents",
            "content": "LLM Usage Statement . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16 Appendix A. Dataset Construction Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16 A.1 Data Collection Process . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16 A.2 Dataset Statistics and Distribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19 A.3 Expert Annotation Process . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Appendix B. Case Studies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29 B.1 High-Score Case Studies: Human-Model Alignment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29 B.2 Medium-Score Case Studies: Human-Model Alignment . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29 B.3 Low-Score Case Studies: Human-Model Alignment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29 B.4 Dimension-Specific Case Studies: Validating Evaluation Criteria . . . . . . . . . . . . . . . . . . . . B.5 Model Error Analysis Cases . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29 Appendix C. Evaluation Framework Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40 C.1 Detailed Evaluation Questions and Scoring Criteria . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40 C.2 Single Visualization Evaluation Criteria . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42 C.3 Multiple Visualization Evaluation Criteria . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C.4 Dashboard Evaluation Criteria . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48 C.5 Evaluation Prompt Templates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51 Appendix D. Model Implementation and Training Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .53 D.1 Hardware and Software Environment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53 D.2 Reward Function . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.3 Hyperparameter Settings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "LLM USAGE STATEMENT",
            "content": "We used Claude-4-Sonnet for English grammar polishing and consulted Claude-4-Sonnet for suggestions on figure layout and color design. During dataset construction, we used GPT-4o for automated adaptive question generation and chart metadata extraction. All code was written, reviewed, and verified by the authors. All prompts contained no private or sensitive data. Large language models did not provide any novel algorithmic ideas or academic claims; the authors take full responsibility for the content. Large language models are not authors of this paper."
        },
        {
            "title": "A DATASET CONSTRUCTION DETAILS",
            "content": "A.1 DATA COLLECTION PROCESS A.1.1 WEB CRAWLING AND MULTI-STAGE FILTERING PIPELINE To build large-scale and diverse visualization dataset, we designed and implemented systematic web crawling and data filtering pipeline. This process aims to collect wide range of visualizations from the web, spanning from poorly designed examples to professional exemplars, while ensuring that all collected data is of high relevance and quality. The entire pipeline consists of three core stages: keyword generation, high-throughput crawling architecture, and multi-stage filtering. Keyword Generation Strategy. The foundation of our data collection is meticulously designed keyword generation strategy to ensure broad coverage across visualization types, quality levels, and application domains. Base Keyword Lexicon: We first established base lexicon of over 200 professional visualization terms, such as professional bar chart design, clean line graph visualization, and business intelligence dashboard. Visualization Type Expansion: Building on this, we systematically incorporated over 30 different chart types, covering basic charts (e.g., bar, line, pie charts), advanced visualizations (e.g., Sankey diagrams, treemaps, radar charts), and interactive systems (e.g., interactive dashboards, animated charts). Quality Modifier Combination: To intentionally capture charts of varying quality levels, we programmatically combined chart types with high-quality modifiers (e.g., professional, clean, effective, well-designed) and low-quality modifiers (e.g., poor, confusing, cluttered, misleading). Domain-Specific Terminology: We also integrated professional terminology from over 20 application domains (e.g., business, finance, healthcare, education) to generate contextspecific search queries, such as financial dashboard, sales performance chart, and COVID cases chart. This automated strategy ultimately generated over 2,000 unique, high-quality search keywords, laying solid foundation for our large-scale data crawling efforts. High-Throughput Crawling and Preliminary Filtering To efficiently collect vast number of candidate images from the web, we developed high-throughput crawling architecture based on Bing Image Search. This architecture utilizes multi-threaded, asynchronous requests to fetch up to 10 pages of search results for each keyword, maximizing data recall. High-resolution image URLs were reliably extracted by parsing JSON data embedded within the web pages. During the crawling phase, we implemented an initial round of automated preliminary filtering: Size Filtering: We strictly filtered images by size, requiring minimum width of 400 pixels, minimum height of 300 pixels, and total area of at least 150,000 pixels. This effectively eliminated low-resolution thumbnails and icons. Heuristic Content Pre-screening: We conducted rapid pre-assessment of image content using programmatic analysis techniques. By employing an edge detection algorithm (ImageFilter.FIND_EDGES) and color complexity analysis (counting the number of 16 unique colors), we discarded significant number of images that were either too simple (e.g., solid-color backgrounds, blank images) or too complex (e.g., real-world photographs), as these typically do not represent data visualizations. This stage yielded large-scale preliminary dataset containing tens of thousands of candidate images, laying the groundwork for subsequent fine-grained refinement. Fine-Grained Filtering and Hierarchical Classification via Multimodal LLMs To precisely filter high-quality, relevant visualizations from the preliminary dataset and organize them into structured classification, we designed fine-grained filtering pipeline centered around an MLLM. Perceptual Hash Deduplication: Before semantic analysis, we first employed Perceptual Hashing (pHash) algorithm to deduplicate all candidate images. This technique identifies visually identical or highly similar images, regardless of differences in size, format, or compression. By setting strict similarity threshold (Hamming distance < 5), we effectively ensured the diversity of the final dataset and eliminated redundancy. Prompt-Based AI Semantic Filtering: We utilized an advanced MLLM (e.g., GPT-4o) as our core classifier. We engineered highly restrictive system prompt that defined the models primary task as that of strict filter rather than simple classifier. This prompt compelled the model to adhere to the following top-priority rules: 1. Reject Non-Screenshot Images: Any image appearing to be photograph, containing tilted perspectives or distortions, or including real-world environments (e.g., monitor bezels, keyboards, desks) was immediately classified as non-compliant (non_visualization). 2. Reject Images with People: Any image containing human figures (including cartoons) or body parts (e.g., hands, fingers) was strictly filtered out. 3. Reject Work-in-Progress and Development Interfaces: We mandated that only finished visualizations be retained. Any screenshot depicting the visualization creation processsuch as those including software UI elements (menus, toolbars, property panels), code editors (like Jupyter Notebooks), or configuration windowswas also classified as non-compliant. The full content of this prompt is detailed in the Prompt Template box below. Hierarchical Content Classification: Only images that passed all the stringent screening criteria and were identified as clean, front-facing, person-free visualization screenshots proceeded to the classification stage. The model then categorized them into hierarchical system based on their structure and function, primarily including: single visualizations, multiple visualizations, and dashboards. Prompt Template: Fine-Grained Filtering and Classification via Multimodal LLM You are professional data visualization analysis expert. Your core mission is to strictly filter and accurately classify data visualization images. WARNING Highest Priority Principle: Absolutely reject all photographs and any images containing people. Your primary duty is to act as rigorous filter. Before evaluating the content of an image, you must first assess its form. Is it photograph? If yes, immediately classify as non_visualization. Does it contain people? If yes, immediately classify as non_visualization. Is it tilted or in perspective? If yes, immediately classify as non_visualization. Only when an image perfectly meets the standard of \"clean, front-facing, person-free screenshot\" can you proceed to analyze its content. Strict Filtering Criteria (Classify as non_visualization if any condition is met): 1. Reject ALL Photographs: 17 Characteristic: Reject any image that appears to be taken with camera rather than direct screenshot. Clues: Tilted Angle/Perspective Distortion: The image is not flat and front-facing. Device Bezels: Physical borders of laptop, monitor, phone, or tablet are visible. Real-World Environment: Backgrounds like desks, offices, conference rooms, keyboards, or mice are visible. People or Body Parts: Any presence of people, hands, or fingers. Reflections or Screen Moire: Reflections of ambient light on the screen. 2. Reject ALL Images with People: Characteristic: Absolutely forbidden. Any image containing people in any form (full body, portrait, cartoon) or body parts (hands, fingers) must be rejected. 3. Reject Marketing/Concept Images: Characteristic: Images that look like stock photos, promotional materials, website banners, or stylized concept designs. These often have artistic effects, tilted perspectives, or non-data elements and are not genuine analytical tool interfaces. Content Classification Criteria (Applicable only to clean screenshots that pass the above filters): 1. Single Visualization (single_view): pure, single, complete data chart. Ultra-Strict Prerequisites (All must be met): (a) Pure Chart: The main subject of the image must be the chart itself, with no external UI elements. (b) No Editing Controls: It must absolutely not contain any UI elements for configuring or editing the chart (e.g., toolbars, property panels, formatting panes, pop-up menus). If such elements are present, it must be classified as non_visualization. (c) Front-facing, person-free, non-photograph screenshot. 2. Multiple Visualizations (multi_view): pure composition of multiple data charts for analysis. Ultra-Strict Prerequisites: (a) Pure Chart Composition: The image must only contain data charts, with absolutely no other types of elements. (b) Multiple Charts: It must contain 2 or more independent data charts. (c) Front-facing, person-free, non-photograph, non-editing interface screenshot. 3. Dashboard (dashboard): An end-user-facing interactive interface for data exploration and monitoring. Ultra-Strict Prerequisites: Must be front-facing, clean screenshot, absolutely free of any people or device bezels. Core Features: Composed of multiple, coordinated data charts and KPI metrics. (A single chart does not constitute dashboard). Interactive elements are end-user-facing and intended for data consumption (e.g., filtering, drilling down, switching views), not for chart creation or editing. 4. Non-Compliant Image (non_visualization): Any image that is not pure, finished data visualization product. This category serves as \"catch-all\" to filter out all visualizations that do not meet the strict criteria. Core Judgment: Is this image showing something \"in progress\" or is it \"finished product\"? Anything \"in progress\" is non-compliant. 18 Decision-Making Process (Strictly Adhered To): 1. Step 1: Check for \"Non-Compliant Image\" (non_visualization). (This is the highestpriority filter). 2. Step 2: Classify the \"pure visualization products\" that pass the first step. 3. Step 3: Differentiate between Multiple Visualizations and Dashboard. Return Format: JSON object with the following structure: { } \"category\": \"Primary category in English (single_view/multi_view/ dashboard/non_visualization)\", \"type\": \"Sub-type in English\", \"confidence\": \"Confidence score (0-100)\", \"reasoning\": \"Provide the core reason for the judgment, e.g., The image is photograph/contains people/is not front-facing, and is therefore non-compliant image.\" This multi-stage pipeline, combining heuristic pre-screening, perceptual hash deduplication, and AIdriven semantic refinement, enables the fully automated construction of high-quality, structured visualization dataset from vast web-scale data. It significantly reduces the manual annotation burden while ensuring the relevance and quality of the collected data. A.2 DATASET STATISTICS AND DISTRIBUTION A.2.1 COMPLETE DATASET STATISTICS VISJUDGE-BENCH consists of 3,090 professionally assessed visualizations covering the full range of modern visualization design. It was constructed to ensure broad coverage across visualization types, evaluation dimensions, and quality levels, reflecting practices in business intelligence, academic research, and data journalism. The collected quality scores approximately follow normal distribution (mean = 3.13, std = 0.72, range = 1.004.89; see Figure 9a), capturing broad range from poor to exemplary designs. Figure 10 presents representative visualization examples across different quality score ranges (from 12 to 45), showcasing the diversity of visualization types and the clear quality distinctions captured by our evaluation framework. All samples include complete six-dimensional annotations, enabling users to study visualization quality holistically as well as across specific types, subtypes, and evaluation dimensions. Visualization Classification. hierarchical taxonomy organizes visualizations by structural comIt includes three major categories: single visualizations (1,041 plexity and functional purpose. samples, 33.7%), multiple visualizations (1,024 samples, 33.1%), and dashboards (1,025 samples, 33.2%). These categories further expand into 22, 5, and 5 subtypes respectively, ensuring representation from basic charts (e.g., bar, pie, line) to advanced analytical dashboards. This classification system allows users to study quality differences across visualization types and subtypes. Detailed information can be found in Table 4, and Figure 9 illustrates the quality score distributions across these three major categories, revealing distinct distribution patterns for each visualization type. Evaluation Methodology. Each visualization is annotated according to theoretically grounded, six-dimensional framework derived from the Fidelity, Expressiveness, and Aesthetics principle, including Data Fidelity, Semantic Readability, Insight Discovery, Design Style, Visual Composition, and Color Harmony. This framework provides comprehensive evaluations across accuracy, interpretability, communicative effectiveness, and aesthetic quality. The distribution of scores across all six dimensions is shown in Figure 11, demonstrating the comprehensive coverage of quality aspects in our dataset. Quality Assurance and Reliability. The dataset incorporates rigorous quality control mechanisms to ensure reliable annotations. We designed four complementary assessment strategies as 19 (a) Overall Score Distribution (b) Single Visualization Score Distribution (c) Multiple Visualization Score Distribution (d) Dashboard Score Distribution Figure 9: Quality score distributions of the dataset across different visualization categories. Figure 10: Representative samples from VISJUDGE-BENCH reference signals for experts: (1) the standard evaluation process; (2) sub-dimension major deviation detection; (3) malicious or abnormal score filtering; and (4) major deviation detection. Although these strategies guided the process, all final scores were determined through expert judgment. 20 Table 4: VISJUDGE-BENCH statistical detailed information. Vis Type Count Proportion #-Subtype Subtype Details Single Vis 1,041 33.7% Multi Vis 1,024 33.1% Dashboard 1,025 33.2% 5 Bar Chart Pie Chart Line Chart Area Chart Treemap Sankey Diagram Heatmap Scatter Plot Histogram Donut Chart Funnel Chart Comparison Views Small Multiples Coordinated Views 176 Bubble Chart 129 Choropleth Map 100 Radar Chart 75 Network Graph 62 Candlestick Chart 61 Gauge Chart 55 Box Plot 49 Point Map 48 Word Cloud 47 Violin Plot 45 Other Single View 670 Overview Detail 195 Other Multi View 97 Analytical Dashboard Operational Dashboard Interactive Dashboard 743 Strategic Dashboard 122 Other Dashboard 91 29 25 24 23 20 20 17 12 1 1 22 3 59 62 Figure 11: Quality score distributions across six evaluation dimensions. All samples underwent expert review, with 2,606 samples (84.3%) including alternative results for cross-validation, and 1,792 samples (58.0%) having their scores refined through expert calibration. This structure ensures that researchers can rely on the dataset for both training and evaluation of visualization quality assessment models. Detailed descriptions of the quality control mechanisms and expert annotation process are provided in Appendix A.3.4 and Appendix A.3.5. A.2.2 QUALITY GRADE DISTRIBUTION To examine the overall quality of the dataset, we analyze the distribution of quality scores across different visualization categories. Figure 9 presents the histograms with fitted density curves, high21 lighting both the mean and median values for each category. This analysis allows us to compare quality differences between single visualizations, multiple visualizations, and dashboards. Quality Distribution Across Visualization Categories. The overall quality score distribution (Figure 9a) exhibits near-normal distribution with scores predominantly ranging from 2.0 to 4.0, indicating balanced representation of both lower and higher quality samples. Individual visualization categories show distinct patterns. Single visualizations (Figure 9b) and multiple visualizations (Figure 9c) exhibit similar quality levels with means of 2.910 and 2.917 respectively, both displaying broad distributions across the quality spectrum. In contrast, dashboards (Figure 9d) show notably higher scores (mean: 3.555, median: 3.610) with right-skewed distribution concentrated in higher quality ranges. This difference reflects that published dashboards typically undergo more rigorous design review as polished, production-ready tools, while single and multiple visualizations include more experimental designs with varying execution quality. Quality Distribution Across Evaluation Dimensions. Figure 11 reveals distinct patterns across the six evaluation dimensions, which align with our three-tier framework of Fidelity, Expressiveness, and Aesthetics. At the Fidelity level, data fidelity (mean: 3.138, median: 3.000) exhibits balanced near-normal distribution, indicating varied success in truthful data representationa fundamental requirement that shows substantial room for improvement across the dataset. At the Expressiveness level, both semantic readability (mean: 2.983, median: 3.000) and insight discovery (mean: 2.983, median: 3.000) center around 3.0 with broad distributions, reflecting the persistent challenge of effective communication and analytical support. These similar patterns suggest that clarity and insight facilitation remain equally difficult aspects of visualization design. At the Aesthetics level, we observe gradient in achievement: color harmony (mean: 3.369, median: 3.500) and visual composition (mean: 3.314, median: 3.500) show the highest scores with right-skewed distributions, benefiting from well-established design guidelines and tool support. In contrast, design style (mean: 2.972, median: 3.000) shows the lowest average with broader spread, reflecting its subjective nature and the varying emphasis placed on stylistic sophistication versus functional priorities. This hierarchical distribution patternfrom foundational data accuracy, through communicative effectiveness, to aesthetic refinementensures that our benchmark evaluates models across the complete spectrum of visualization quality assessment. A.2.3 TEST SET DISTRIBUTION To ensure reliable and comprehensive evaluation of model performance, we partitioned the dataset into training (70%, 2,163 samples), validation (10%, 279 samples), and test (20%, 648 samples) sets using stratified sampling based on visualization types. Table 5 presents the detailed distribution of the test set across visualization types and subtypes, demonstrating that the stratified sampling successfully maintains proportional representation consistent with the overall dataset. The test set comprises 231 single visualizations (35.6%), 209 multiple visualizations (32.3%), and 208 dashboards (32.1%), closely mirroring the overall dataset distribution. Within single visualizations, the test set covers 20 distinct chart types, ranging from common charts like bar charts (37 samples) and pie charts (27 samples) to specialized visualizations such as Sankey diagrams (14 samples) and network graphs (5 samples). Multiple visualizations include 135 comparison views, 41 small multiples, and 20 coordinated views, while dashboards predominantly feature analytical dashboards (150 samples) alongside operational and interactive dashboards. The test set maintains quality score distribution characteristics similar to the full dataset, with mean of 3.13 and standard deviation of 0.72, ranging from 1.11 to 4.89. Score distribution across quality ranges shows 7.4% low-quality samples (1.02.0), 31.5% below-average samples (2.03.0), 49.5% above-average samples (3.04.0), and 11.6% high-quality samples (4.05.0). This balanced distribution ensures comprehensive evaluation across the full quality range, enabling robust assessment of model performance on both challenging low-quality and high-quality visualizations. Table 5: Test set statistical detailed information (N=648, 20% of VISJUDGE-BENCH). Vis Type Count Proportion #-Subtype Subtype Details Single Vis 231 35.6% 20 Multi Vis 209 32.3% Dashboard 32.1% 4 5 Bar Chart Pie Chart Line Chart Area Chart Treemap Sankey Diagram Heatmap Histogram Scatter Plot Donut Chart Funnel Chart Comparison Views Small Multiples Coordinated Views 37 Bubble Chart 27 Other Single View 21 Choropleth Map 15 Radar Chart 14 Candlestick Chart 14 Gauge Chart 12 Network Graph 11 Point Map 11 Box Plot 11 135 Other Multi View 41 20 Analytical Dashboard Operational Dashboard Interactive Dashboard Strategic Dashboard 150 Other Dashboard 25 19 13 7 6 6 6 5 5 5 4 4 1 Total 648 samples A.3 EXPERT ANNOTATION PROCESS A.3.1 ANNOTATOR RECRUITMENT STANDARDS To ensure high-quality responses and reduce the risk of careless or malicious submissions, we implemented strict screening criteria during the annotator recruitment process. To maintain annotation quality, we applied the following recruitment criteria: Education: Participants were required to have completed at least Bachelors degree. Preference was given to those with Masters, professional, or doctoral degree to ensure familiarity with analytical and design tasks. Approval Rating: Only individuals with historical approval rate between 97% and 100% were permitted to participate, reflecting track record of reliable and consistent task completion on the platform. Approved Projects Count: Annotators were selected from those who had completed between 100 and 10,000 approved projects, ensuring adequate experience with crowdsourcing workflows. English Language: All participants were required to be native English speakers to guarantee accurate comprehension of visualization-related terminology and rubric-based questions. Occupation Field: We targeted professionals working in relevant domains such as arts, business, education, finance, STEM, public administration, and product design, to match the content and context of visual analysis tasks. Job Classification: Participants were drawn from white-collar, creative, and IT-related professions, including developers, designers, analysts, and content creators, all of whom typically interact with visual content in their daily work. Last Project Completed: Annotators were required to have completed project within the past 180 days, ensuring recent and active engagement with the platform. Age: To maintain cognitively active and professionally engaged participant pool, we limited participation to those aged between 20 and 50 years. Technical Skills: We prioritized individuals proficient in data science, product design, front-end development, computer science, and other related technical fields that support informed and thoughtful visual reasoning. A.3.2 ANNOTATION TASK DESIGN Our VISJUDGE-BENCH contains 3,090 visualizations, each requiring evaluation across 6 dimensions. To ensure annotation quality and allow annotators to familiarize themselves with the evaluation criteria, we organized the annotations into batches of 15 images per task, with each batch carefully balanced to include 5 single visualizations, 5 multiple visualizations, and 5 dashboards. Before starting each task, annotators were presented with detailed explanations of the evaluation framework, including the meaning and significance of each dimension (Fidelity, Expressiveness, and Aesthetics), enabling them to quickly understand the task requirements and evaluation standards. With 6 questions per image, each annotation task comprised 90 questions and typically took 3060 minutes, depending on annotator familiarity with the criteria and visualization complexity. Each task was independently annotated by three qualified participants to ensure reliability through majority voting and enable inter-annotator agreement analysis. Based on an estimated hourly wage of $10 USD, this process ensured high-quality data collection. Each visualization is evaluated across six dimensions derived from our Fidelity, Expressiveness, and Aesthetics framework: (1) Data Fidelity, which assesses whether the visual representation accurately reflects the underlying data; (2) Semantic Readability, which evaluates whether information is clearly conveyed; (3) Insight Discovery, which measures whether meaningful patterns are discoverable; (4) Design Style, which assesses aesthetic innovation and uniqueness; (5) Visual Composition, which evaluates spatial layout and balance; and (6) Color Harmony, which measures color coordination and effectiveness. For each dimension, annotators provide ratings on 15 scale, where each rating level is accompanied by clear descriptive criteria to ensure consistent interpretation across annotators (see Appendix for detailed evaluation questions and scoring criteria for each dimension). A.3.3 ANNOTATION INTERFACE AND WORKFLOW We designed dedicated crowdsourcing interface to ensure annotators clearly understood the task, followed structured workflow, and submitted high-quality responses. Before beginning the evaluation, participants were presented with both brief and an extended task introduction. The short version stated: Evaluate 15 data visualizations with 90 simple multiple-choice questions (6 per chart) covering Fidelity (data accuracy), Expressiveness (information clarity), and Aesthetics (visual aesthetics). Each question has clear 15 rating descriptions to make evaluation straightforward. Please only participate if you can provide thoughtful responsesweve designed this to be as simple as possible for you! The extended version was shown in full on the task interface: Welcome! Thank you for joining our study on data visualization quality. You will evaluate 15 data visualizations with 90 simple multiple-choice questions based on three classical design principles: - Fidelity: Data Fidelity whether the visual representation accurately reflects the underlying data. - Expressiveness: Semantic Readability, Insight Discovery whether information is clearly conveyed and meaningful patterns are discoverable. - Aesthetics: Design Style, Visual Composition, Color Harmony whether the visualization has aesthetic appeal and professional design quality. For each chart: - View the image and its description 24 - Answer 6 straightforward questions (1 for Fidelity, 2 for Expressiveness, 3 for Aesthetics) - Simply select your rating from 1 (Poor) to 5 (Excellent) each option has clear descriptions to guide your choice Weve designed this to minimize your effort while ensuring quality feedback. Please take your time to carefully consider each visualization before making your selections. Please consider the time commitment carefully and only proceed if you can provide thoughtful, quality responses. If youre not ready to participate seriously, feel free to skip this task. We will check for quality and may reject careless or random responses. Your thoughtful and careful feedback is importantthank you! As illustrated in Figure 12, the annotation interface presented one chart at time, along with textual description. Below the visualization, the six evaluation questions were displayed, customized based on the chart content. To proceed, participants were required to complete all six questions. At the end of each chart, annotators also rated their overall confidence in their answers and were optionally allowed to flag any uncertain responses via free-text input box. This structured flow encouraged serious participation while enabling us to monitor annotation quality and filter out unreliable data. A.3.4 CROWDSOURCING QUALITY CONTROL AND CANDIDATE STRATEGY GENERATION Ensuring the reliability of collected annotations requires two-stage quality control design. Stage 1: Crowdsourcing Quality Control. During the crowdsourcing phase, we embedded validation checks into the annotation interface to identify inattentive or careless responses. Specifically, small number of chart-pair questions were designed where the superior or inferior chart was visually and functionally obvious. For instance, one pair compared clean and readable pie chart against an overly cluttered line chart. Annotators failing such checks were highly likely to be engaging in random or inattentive behavior. These responses were flagged and either discarded or subjected to further scrutiny, thereby improving the reliability of the collected scores. Figure 13 illustrates examples of these validation questions. Stage 2: Candidate Strategy Generation for Expert Review. In the expert adjudication stage, we further designed systematic conflict identification and resolution mechanism based on representative studies in crowdsourcing quality control, statistical outlier detection, and multi-dimensional evaluation theory (Gadiraju et al., 2015; Rousseeuw & Leroy, 2005; Brennan, 2001). This mechanism provides algorithmic candidate strategies to serve as reference signals for expert review, without replacing expert judgment. High-Disagreement Sample Identification. The system first calculates the standard deviation of initial scores for each sample across all three annotators. Samples with standard deviation > 1.0 are automatically identified as high-disagreement samples requiring further algorithmic analysis and expert attention. Algorithmic Candidate Strategy Generation. For high-disagreement samples, the system generates three types of candidate resolution strategies: Outlier Removal Strategy: When two annotators scores are close (absolute difference 2.0) but the third annotators score differs significantly from both (absolute difference > 1.5 from each), the system suggests removing the anomalous score and averaging the remaining two scores. This strategy addresses cases where one annotator may have misunderstood the task or made systematic errors. Malicious Scoring Filter Strategy: The system identifies and flags abnormal rating behaviors where annotators assign identical scores across all six evaluation dimensions. Such patterns are statistically unlikely for genuine evaluation and may indicate inattentive or gaming behavior. Flagged annotations undergo additional scrutiny or removal. Sub-dimension Bias Correction Strategy: To address potential systematic biases in specific evaluation dimensions, the system independently applies dual-threshold mechanism (threshold = 2.0) to each of the six evaluation dimensions. When an annotators score in 25 any dimension deviates by more than 2.0 points from the other two annotators average, the system flags this as potential dimensional bias and suggests score normalization or expert review. Strategy Integration and Ranking. All candidate strategies are processed through score integration and ranking module that evaluates their statistical validity and consistency with the overall dataset distribution. The ranked strategies are then presented to the expert team as structured recommendations, along with confidence scores and rationale explanations. The four complementary strategies mentioned include: (1) the standard evaluation process, which provides baseline scores; (2) sub-dimension major deviation detection, which highlights dimensional inconsistencies; (3) malicious or abnormal score filtering, which identifies problematic responses; and (4) major deviation detection, which flags overall inconsistencies. Together, these two complementary mechanismsvalidation checks during crowdsourcing and candidate strategies during expert adjudicationform multi-layered quality control pipeline, ensuring that the final dataset reflects trustworthy and rigorously validated quality scores. A.3.5 EXPERT INTERFACE AND ANNOTATION To streamline post-crowdsourcing adjudication, we developed an expert review interface that aggregates all 3,090 tasks and presents them in prioritized queue. Samples are ranked by their score divergence (σ, the standard deviation across candidate scores), from high to low, enabling experts to resolve highly contentious cases first. For each task, the interface displays chart preview, metadata (type, subtype, and modification status), and the task description, which provide essential context for assigning fair and informed score. The interface also presents per-dimension evaluation scores together with the outputs of the four candidate strategiesnamely the standard evaluation process, sub-dimension major deviation detection, malicious or abnormal score filtering, and major deviation detection. These auxiliary signals do not override expert judgment; rather, they support experts in detecting anomalies, validating consistency, and ultimately determining the most reasonable final score. screenshot of the expert review interface is shown in Figure 14. 26 Figure 12: Crowdsourcing interface for expert annotation process. 27 Figure 13: Examples of validation checks embedded in the crowdsourcing interface. Figure 14: Expert review interface."
        },
        {
            "title": "B CASE STUDIES",
            "content": "B.1 HIGH-SCORE CASE STUDIES: HUMAN-MODEL ALIGNMENT We define High-Score visualizations as those that achieve strong performance across the three dimensions of fidelity, expressiveness, and aesthetics. Such visualizations demonstrate professional design principles and convey information in manner that is both accurate and visually engaging. As illustrated in Figure 15, we present representative high-score case along with the corresponding human ratings and VISJUDGE output. This example demonstrates that, when the visualization adheres to established best practices, the models evaluation is largely consistent with human judgment. B.2 MEDIUM-SCORE CASE STUDIES: HUMAN-MODEL ALIGNMENT We define Medium-Score visualizations as those that perform adequately across fidelity, expressiveness, and aesthetics, but fall short of excellence in at least one of these dimensions. Such visualizations generally succeed in conveying information correctly and remain interpretable, yet they may exhibit shortcomings in specific aspects, most often in visual aesthetics or design refinement. As shown in Figure 16, we present representative medium-score case, where the visualization fulfills its communicative purpose but does not achieve high-quality standards in every dimension. B.3 LOW-SCORE CASE STUDIES: HUMAN-MODEL ALIGNMENT We define Low-Score visualizations as those that exhibit clear deficiencies across one or more of the three dimensions of fidelity, expressiveness, and aesthetics. Such visualizations often distort or obscure the underlying data, employ ineffective or misleading encodings, or suffer from poor design choices that hinder interpretability. As illustrated in Figure 17, we present representative low-score case, where both human ratings and VISJUDGE outputs highlight significant problems that severely compromise the effectiveness of the visualization. B.4 DIMENSION-SPECIFIC CASE STUDIES: VALIDATING EVALUATION CRITERIA To facilitate clearer understanding of the three evaluation dimensionsfidelity, expressiveness, and aestheticswe provide representative low-score case studies for each dimension. Specifically, Figure 18 illustrates case with low fidelity, Figure 19 presents case with low expressiveness, and Figure 20 shows case with low aesthetics. These targeted examples highlight how deficiencies in individual dimensions manifest in practice and demonstrate that VISJUDGEs evaluations align with human ratings along the intended criteria. B.5 MODEL ERROR ANALYSIS CASES While evaluating visualizations along the three dimensions of fidelity, expressiveness, and aesthetics, we observe that certain base models fail to correctly identify the deficiencies of low-quality charts and consequently assign them undeservedly high scores. To illustrate these issues, we present two complementary types of error analysis: Overview Analysis: Figure 21 provides high-level overview of typical failure patterns across different models, summarizing where and how the evaluations deviate from expert judgment. Detailed Case Studies: Figures 22, Figure 23, Figure 24 and Figure 25 present representative detailed cases, each showing the full output of model and highlighting the specific reasons for misalignment with human evaluations. Together, these analyses reveal both the systematic error modes of existing models and the necessity of explicitly evaluating visualizations along the dimensions of fidelity, expressiveness, and aesthetics. 29 Figure 15: High score cases. 30 Figure 16: Medium score cases. 31 Figure 17: Low score cases. 32 Figure 18: Low fidelity cases. 33 Figure 19: Low expressiveness cases. 34 Figure 20: Low aesthetics cases. Figure 21: Overview of error cases. Figure 22: Qwen2.5-VL-7B and GPT-5 error cases. 36 Figure 23: Gemini-2.0-Flash and Claude-3.5-sonnet error cases. 37 Figure 24: Claude-4-sonnet and GPT-4o error cases. Figure 25: Gemini-2.5-Pro error case."
        },
        {
            "title": "C EVALUATION FRAMEWORK AND CRITERIA",
            "content": "C.1 DETAILED EVALUATION QUESTIONS AND SCORING CRITERIA This appendix provides comprehensive details on the evaluation framework underlying VISJUDGEBENCH, including specific evaluation questions and scoring criteria for each visualization type and evaluation dimension. Evaluation Framework Overview Our evaluation framework is inspired by the classical Fidelity, Expressiveness, and Aesthetics principles, operationalized into six orthogonal sub-dimensions: Fidelity: Data Fidelity: Ensuring visual representations are faithful to underlying data. Expressiveness: Semantic Readability: Clarity of information communication. Insight Discovery: Effectiveness in revealing meaningful patterns. Aesthetics: Design Style: Innovation and uniqueness. Visual Composition: Spatial layout and organization. Color Harmony: Color coordination and visual appeal. To support context-aware evaluation tailored to specific visualizations, we design evaluation questions and scoring criteria rewriting prompt template that transforms generic evaluation criteria into more concrete versions. An example rewriting is shown in Figure 26, which illustrates set of customized sub-dimension evaluation questions and scoring criteria tailored to specific single visualization chart. This rewriting process incorporates: sub_dimension_name (the sub-dimension name under three classical principles), sub_dimension_text (the standard evaluation question and scoring criteria definitions). The resulting prompt enables the generation of chart metadata and adapted evaluation rubrics that are grounded in the specific context of the visualization being evaluated. Prompt Template: Evaluation Questions and Scoring Criteria Rewriting You are professional data visualization evaluation expert. You need to generate targeted evaluation questions and specific scoring criteria for each metric based on the provided visualization chart and {sub_dimension_name} evaluation metrics. {sub_dimension_name} evaluation metrics information: {sub_dimension_text} Task Requirements: 1. Carefully observe the provided {image_type} type visualization chart 2. Based on the specific content of the chart (such as chart type, data content, visual elements, design features, etc.) 3. Combine the requirements and scoring criteria of each evaluation metric above 4. For each metric, generate: specific, targeted evaluation question Custom scoring criteria (15 scale) that is specifically adapted to this charts features Question Format Requirements: Questions should clearly point out specific features of the chart (e.g., This bar chart uses blue and red contrast...) Questions should relate to the core points of the evaluation metric Questions should guide evaluators to think about the specific performance of that metric 40 Figure 26: Rewriting result of customized evaluation questions and scoring criteria for single visualization chart. Questions should end with: Please provide 15 score based on the scoring criteria. Scoring Criteria Format Requirements: For each question, provide custom 15 rating scale Each score description should specifically reference elements in this chart Score 1 should describe the worst-case scenario for this chart Score 5 should describe the ideal implementation for this chart Scores 24 should provide meaningful intermediate levels Return Format: JSON object with the following structure: { \"chart_description\": \"Brief description of the chart (chart type, main features, etc.)\", \"key_elements\": [\"Observed element 1\", \"Observed element 2\", \"...\"], \"questions\": { \"sub_dimension_name\": { \"question\": \"Specific question text...\", 41 \"custom_scoring\": { \"1\": \"Score 1 description specific to this chart...\", \"2\": \"Score 2 description...\", \"3\": \"Score 3 description...\", \"4\": \"Score 4 description...\", \"5\": \"Score 5 description...\" } }, ... } } C.1.1 SINGLE VISUALIZATION EVALUATION CRITERIA Data fidelity evaluation questions and scoring criteria: The following prompt is designed to guide the generation of scoring rubrics for the Data Fidelity dimension, using 15 scale. It focuses on ensuring that visual representations truthfully and accurately reflect the underlying data. Prompt: Single Visualization - Data Fidelity Evaluation Questions and Scoring Criteria (1-5 Scale) Description: Evaluates whether the visual encodings faithfully represent the data without misleading distortions. Focus on: appropriate axes and baselines (e.g., zero baseline for bar charts), reasonable scale ranges and tick intervals, consistency between encodings (position/length/angle/area/- color) and numeric labels, and absence of aspect-ratio deformation, cropping/stretching, 3D distortion, or improper broken axes. Scoring criteria: 1. Severe misrepresentation or impossible to judge due to missing/unreadable key elements (titles, legends, axes/ticks/labels, units, data labels, baselines, etc.), or clear distortions (e.g., non-zero baseline in bars exaggerating differences, 3D effects causing misread, obvious aspect-ratio deformation). 2. Important issues likely to mislead: inappropriate baseline/scale range, inconsistent encoding vs labels, selective ranges that exaggerate differences, or partial key elements missing causing notable uncertainty. 3. Basically acceptable with minor uncertainties: some elements may be missing or hard to read but no obvious distortion; scales and encodings are mostly reasonable and roughly consistent with labels. 4. Accurate and standardized: axes/baselines are appropriate, scale ranges and ticks are reasonable, encodings match labels, and no visible distortion or manipulation is present. 5. Highly faithful representation: all relevant elements are present and clear; axes/baselines and scale choices are well-justified (e.g., proper zero baseline for bars, appropriate linear/log choice), encodings and labels are fully consistent, with no cropping, deformation, or 3D misuse. Semantic readability evaluation questions and scoring criteria: The following prompt guides the generation of scoring rubrics for the Semantic Readability dimension. It evaluates how clearly the chart communicates information through its visual encodings and annotations. 42 Prompt: Single Visualization - Semantic Readability Evaluation Questions and Scoring Criteria (1-5 Scale) Description: Based on complete and clear chart elements, evaluates whether users can understand the meaning of these elements and the information the chart conveys, including clarity of visual encoding (whether meanings of colors, shapes, sizes are clear) and clarity of information communication (whether users can accurately understand chart content). Scoring criteria: 1. Chart element meanings are completely unclear, cannot understand what visual encoding represents, users cannot obtain meaningful information. 2. Chart element meanings are vague and unclear, insufficient visual encoding explanations, users have difficulty understanding and need extensive guessing to understand chart content. 3. Chart element meanings are basically clear, users can understand main information, but still have understanding barriers in some encoding or labeling aspects. 4. Chart element meanings are clear and definite, visual encoding has appropriate explanations, users can smoothly understand the information conveyed by the chart. 5. Chart element meanings are completely clear, visual encoding explanations are detailed and accurate, users can understand all chart information without any barriers. Insight discovery evaluation questions and scoring criteria: The following prompt guides the generation of scoring rubrics for the Insight Discovery dimension. It assesses the charts ability to reveal meaningful patterns, trends, or non-obvious findings. Prompt: Single Visualization - Insight Discovery Evaluation Questions and Scoring Criteria (1-5 Scale) Description: Evaluates whether the chart can easily identify significant and meaningful data insights, trends, patterns, or anomalies that must have actual value and guiding significance for users, rather than trivial or obvious general information. Scoring criteria: 1. Completely unable to provide any valuable insights, chaotic information presentation, no significant or meaningful discoveries, insight delivery completely failed. 2. Difficult to identify meaningful insights, most presented information is trivial or obvious, lacking practical value, insight discovery difficult. 3. Can identify some basic data patterns or trends, but the significance and practicality of insights are average, with limited guiding value for users. 4. Can clearly identify obvious and useful data insights and trends, discovered patterns or anomalies have certain practical significance and reference value. 5. Very intuitively reveals profound and highly significant data insights, can discover significant trend changes, important anomalies, or key patterns that have important guiding value for decision-making or understanding. Design style evaluation questions and scoring criteria: The following prompt guides the generation of scoring rubrics for the Design Style dimension. It reflects the level of visual creativity, uniqueness, and design innovation present in the chart. Prompt: Single Visualization - Design Style Evaluation Questions and Scoring Criteria (1-5 Scale) Description: Evaluates whether the chart design has innovation and uniqueness, whether it can stand out from many visualization works through novel design elements and unique style characteristics. Scoring criteria: 1. Design completely lacks innovation, style is outdated or shows obvious imitation, no uniqueness or originality. 2. Design lacks innovation, style is quite ordinary, basically uses common design techniques, insufficient uniqueness. 3. Design has some innovative elements, but overall is relatively conventional, uniqueness not prominent enough, innovation level is average. 4. Design has strong innovation, style is quite unique, has novel design elements, with certain originality. 5. Design is extremely innovative, style is unique and novel, uses innovative design elements or expression techniques, with strong originality and recognizability. Visual composition evaluation questions and scoring criteria: The following prompt guides the generation of scoring rubrics for the Visual Composition dimension. It focuses on the spatial arrangement and organization of visual elements for effective communication. Prompt: Single Visualization - Visual Composition Evaluation Questions and Scoring Criteria (1-5 Scale) Description: Evaluates whether the spatial layout of chart elements is reasonable, including whether element positions, size proportions, spacing distribution and other spatial relationships are appropriately and orderly arranged. Scoring criteria: 1. Spatial layout is seriously unreasonable, element positions are chaotic, proportions severely unbalanced, spacing distribution is disorderly. 2. Spatial layout has obvious unreasonable aspects, improper element positions, unbalanced proportions, uneven spacing distribution. 3. Spatial layout is basically reasonable, element positions are acceptable, proportional relationships are basically coordinated, but may have small layout issues. 4. Spatial layout is quite reasonable, element positions are appropriate, good proportional relationships, reasonable spacing distribution, proper space utilization. 5. Spatial layout is very reasonable, element positions are perfect, proportions are coordinated, spacing distribution is even, space utilization efficiency is extremely high. Color harmony evaluation questions and scoring criteria: The following prompt guides the generation of scoring rubrics for the Color Harmony dimension. It evaluates how effectively the color scheme supports readability, aesthetic appeal, and visual coherence. Prompt: Single Visualization - Color Harmony Evaluation Questions and Scoring Criteria (1-5 Scale) Description: Evaluates whether chart color choices are coordinated, contrast is appropriate, color quantity is reasonable, and whether it avoids too many colors causing visual fatigue. Scoring criteria: 1. Color choices are chaotic, extensive use of uncoordinated colors, serious visual fatigue, completely unable to effectively guide user understanding, color use severely affects information delivery. 2. Color choices are not coordinated enough, using too many colors (5 or more), inappropriate contrast, causing obvious visual fatigue and understanding difficulties. 3. Color choices are basically coordinated, but color quantity is excessive (4-5 colors), may cause slight visual complexity, somewhat affecting understanding. 4. Color choices are relatively coordinated, appropriate contrast, reasonable color quantity control (3-4 colors), basically avoiding visual confusion, well guiding user understanding. 5. Color choices are very coordinated, appropriate contrast, limited color scheme (usually 1-3 main colors), colors are simple and unified, completely avoiding visual fatigue, effectively maintaining user attention focus. C.1.2 MULTIPLE VISUALIZATION EVALUATION CRITERIA Data fidelity evaluation questions and scoring criteria: The following prompt is designed to guide the generation of scoring rubrics for the Data Fidelity dimension, using 15 scale. It focuses on ensuring that visual representations truthfully and accurately reflect the underlying data. Prompt: Multiple Visualization - Data Fidelity Evaluation Questions and Scoring Criteria (1-5 Scale) Description: Evaluates whether multiple coordinated views faithfully represent data without distortion individually and in combination. Check appropriate axes/baselines, reasonable scale ranges, consistency between encodings and labels, and cross-view consistency (e.g., comparable scales where appropriate), avoiding aspect-ratio deformation, cropping, 3D misuse, or improper broken axes. Scoring criteria: 1. Severe misrepresentation or impossible to judge due to missing/unreadable key elements in one or more views, or clear distortions (non-zero bar baseline where required, 3D effects, obvious deformation), leading to unreliable reading. 2. Important issues exist in one or more views (inappropriate baseline/scale, inconsistent encoding vs labels, selective ranges), or partial key elements missing causing notable cross-view uncertainty. 3. Basically acceptable overall: minor uncertainties or small issues in some views, but no obvious distortion; scales and encodings are mostly reasonable across views. 4. Accurate and standardized across views: appropriate axes/baselines and scales; encodings match labels; no visible distortion; reasonable cross-view scale alignment where needed. 5. Highly faithful multi-vis representation: all views present complete, clear elements; well-justified axes/baselines and scale choices; full consistency between encodings and labels; no cropping/deformation/3D misuse; coherent cross-view comparability. Semantic readability evaluation questions and scoring criteria: The following prompt guides the generation of scoring rubrics for the Semantic Readability dimension. It evaluates how clearly the chart communicates information through its visual encodings and annotations. 45 Prompt: Multiple Visualization - Semantic Readability Evaluation Questions and Scoring Criteria (1-5 Scale) Description: Based on complete and clear chart elements, evaluates whether users can understand the meaning of these elements and the information the chart conveys, including clarity of visual encoding (whether meanings of colors, shapes, sizes are clear) and clarity of information communication (whether users can accurately understand chart content). Scoring criteria: 1. Chart element meanings are completely unclear, cannot understand what visual encoding represents, users cannot obtain meaningful information. 2. Chart element meanings are vague and unclear, insufficient visual encoding explanations, users have difficulty understanding and need extensive guessing to understand chart content. 3. Chart element meanings are basically clear, users can understand main information, but still have understanding barriers in some encoding or labeling aspects. 4. Chart element meanings are clear and definite, visual encoding has appropriate explanations, users can smoothly understand the information conveyed by the chart. 5. Chart element meanings are completely clear, visual encoding explanations are detailed and accurate, users can understand all chart information without any barriers. Insight discovery evaluation questions and scoring criteria: The following prompt guides the generation of scoring rubrics for the Insight Discovery dimension. It assesses the charts ability to reveal meaningful patterns, trends, or non-obvious findings. Prompt: Multiple Visualization - Insight Discovery Evaluation Questions and Scoring Criteria (1-5 Scale) Description: Evaluates whether the entire multi-vis visualization can easily identify significant and meaningful data insights, and whether multiple views synergize to reveal deeper and more valuable comprehensive insights than single views. Scoring criteria: 1. Completely unable to provide any valuable comprehensive insights, lack of coordination between views, no significant or meaningful discoveries, insight delivery completely failed. 2. Difficult to identify meaningful comprehensive insights, most information presented by multiple views is trivial or independent, lacking synergistic value, insight discovery difficult. 3. Can identify some basic multi-vis insights, but the significance and practicality of insights are average, limited cooperation between views, guiding value not prominent enough. 4. Can clearly identify obvious and useful multi-dimensional insights, good cooperation between views, discovered comprehensive patterns have certain practical significance and reference value. 5. Very intuitively reveals profound and highly significant comprehensive insights, perfect synergy between multiple views, can discover significant cross-dimensional patterns, important correlations, or key anomalies with important guiding value. Design style evaluation questions and scoring criteria: The following prompt guides the generation of scoring rubrics for the Design Style dimension. It reflects the level of visual creativity, uniqueness, and design innovation present in the chart. Prompt: Multiple Visualization - Design Style Evaluation Questions and Scoring Criteria (1-5 Scale) Description: Evaluates whether the overall design of multi-vis visualization has innovation and uniqueness, whether it can stand out from many visualization works through novel design elements, unique style characteristics, and coordinated style language. Scoring criteria: 1. Multi-vis design completely lacks innovation, style is outdated or views have chaotic styles, no uniqueness or originality. 2. Multi-vis design lacks innovation, style is quite ordinary, basically uses common design techniques, insufficient uniqueness. 3. Multi-vis design has some innovative elements, but overall is relatively conventional, style characteristics not prominent enough, innovation level is average. 4. Multi-vis design has strong innovation, style is quite unique, coordinated style among views with certain originality. 5. Multi-vis design is extremely innovative, style is unique and novel, views maintain unified style while showing strong originality and recognizability. Visual composition evaluation questions and scoring criteria: The following prompt guides the generation of scoring rubrics for the Visual Composition dimension. It focuses on the spatial arrangement and organization of visual elements for effective communication. Prompt: Multiple Visualization - Visual Composition Evaluation Questions and Scoring Criteria (1-5 Scale) Description: Evaluates whether the spatial layout of individual sub-views and the overall arrangement of multiple views are reasonable, including element composition within individual views, size proportions between multiple views, arrangement methods, spacing distribution, etc., forming harmonious and orderly visual whole. Scoring criteria: 1. Sub-view layouts are chaotic or overall multi-vis arrangement is seriously unreasonable, size proportions severely unbalanced, spacing distribution is disorderly. 2. Sub-view layouts or multi-vis arrangement have obvious unreasonable aspects, inappropriate size proportions, uneven spacing distribution. 3. Sub-view layouts are basically reasonable, overall multi-vis arrangement is acceptable, but may have small issues in proportional relationships or spacing handling. 4. Sub-view layouts are good, overall multi-vis arrangement is reasonable, appropriate size proportions, proper spacing distribution, good space utilization. 5. Sub-view layouts are perfect, overall multi-vis arrangement is extremely reasonable, coordinated size proportions, even spacing distribution, extremely high space utilization efficiency. Color harmony evaluation questions and scoring criteria: The following prompt guides the generation of scoring rubrics for the Color Harmony dimension. It evaluates how effectively the color scheme supports readability, aesthetic appeal, and visual coherence. 47 Prompt: Multiple Visualization - Color Harmony Evaluation Questions and Scoring Criteria (1-5 Scale) Description: Evaluates whether color choices in multi-vis are appropriate, including color combinations within each sub-view, coordination of color schemes between multiple views, unity of overall tone, and appropriate control of color quantity and saturation. Scoring criteria: 1. Sub-view color choices are very inappropriate, overall color matching among multiple views is chaotic, tone conflicts or color use is seriously inappropriate. 2. Sub-view color choices are not appropriate enough, overall color matching among multiple views is not coordinated enough, tone not unified or color use inappropriate. 3. Sub-view color choices are basically appropriate, overall color matching among multiple views is acceptable, but may have small issues in tone unity or saturation. 4. Sub-view color choices are relatively appropriate, overall color schemes among multiple views are quite coordinated, tone basically unified, appropriate color use. 5. Sub-view color choices are very appropriate, overall color schemes among multiple views are highly coordinated and unified, perfect tone matching, appropriate control of color quantity and saturation. C.1.3 DASHBOARD EVALUATION CRITERIA Data fidelity evaluation questions and scoring criteria: The following prompt is designed to guide the generation of scoring rubrics for the Data Fidelity dimension, using 15 scale. It focuses on ensuring that visual representations truthfully and accurately reflect the underlying data. Prompt: Dashboard - Data Fidelity Evaluation Questions and Scoring Criteria (1-5 Scale) Description: Evaluates whether the dashboard faithfully represents business data in each component and in the overall composition. Focus on appropriate axes/baselines, reasonable scale ranges, consistency between encodings and numeric labels, avoidance of aspect-ratio deformation, cropping/stretching, 3D distortion, or improper broken axes; also consider consistency of comparable metrics across components. Scoring criteria: 1. Severe misrepresentation or impossible to judge due to missing/unreadable key elements in important components, or clear distortions (e.g., improper bar baselines, distorted 3D, obvious deformation), undermining data fidelity. 2. Important issues likely to mislead in one or more components: inappropriate baselines/scales, inconsistent encoding vs labels, selective ranges, or missing key elements causing notable uncertainty at the dashboard level. 3. Basically acceptable overall: minor issues or uncertainties in parts, but no obvious distortion; most components use reasonable scales/encodings consistent with labels. 4. Accurate and standardized: components and overall dashboard use appropriate axes/baselines and reasonable scales; encodings match labels; no visible distortion. 5. Highly faithful: all components present complete and clear elements; axes/baselines and scales are well-justified; encodings and labels fully consistent; no cropping/deformation/3D misuse; consistent comparability across related components. Semantic readability evaluation questions and scoring criteria: The following prompt guides the generation of scoring rubrics for the Semantic Readability dimension. It evaluates how clearly the chart communicates information through its visual encodings and annotations. Prompt: Dashboard - Semantic Readability Evaluation Questions and Scoring Criteria (1-5 Scale) Description: Based on complete and clear dashboard elements, evaluates whether users can understand the meaning of these elements and the information the dashboard conveys, including clarity of visual encoding (whether meanings of colors, shapes, sizes, gauge pointers, status indicators are clear) and clarity of information communication (whether users can accurately understand various parts and overall business meaning). Scoring criteria: 1. Dashboard element meanings are completely unclear, cannot understand what charts, indicators, color encoding represent in business terms, users cannot obtain meaningful information. 2. Dashboard element meanings are vague and unclear, insufficient business indicator explanations, unclear color encoding and status indication, users have difficulty understanding and need extensive guessing to understand content. 3. Dashboard element meanings are basically clear, users can understand main business information and indicator meanings, but still have understanding barriers in some encoding or labeling aspects. 4. Dashboard element meanings are clear and definite, business indicators have appropriate explanations, clear visual encoding, users can smoothly understand the business information conveyed by the dashboard. 5. Dashboard element meanings are completely clear, business indicator explanations are detailed and accurate, all visual encoding has clear interpretation, users can understand all business information in the dashboard without any barriers. Insight discovery evaluation questions and scoring criteria: The following prompt guides the generation of scoring rubrics for the Insight Discovery dimension. It assesses the charts ability to reveal meaningful patterns, trends, or non-obvious findings. Prompt: Dashboard - Insight Discovery Evaluation Questions and Scoring Criteria (1-5 Scale) Description: Evaluates whether the entire dashboard can easily identify significant and business-valuable key insights, whether key business indicators (KPIs) prominently display important information, and whether it can quickly discover business issues or opportunities that require attention. Scoring criteria: 1. Completely unable to provide any valuable business insights, key indicators not prominent, no significant or meaningful business discoveries, insight delivery completely failed. 2. Difficult to identify meaningful business insights, insufficient highlighting of key indicators, most presented business information is trivial or obvious, lacking decision value. 3. Can identify some basic business insights, key indicators have some prominence, but the significance and business value of insights are average, with limited guiding effects. 4. Can clearly identify obvious and useful business insights, key indicators prominently displayed, discovered business patterns or anomalies have certain practical value and decision reference significance. 5. Very intuitively reveals profound and highly business-valuable insights, key indicators extremely prominent, can quickly discover significant business trends, important anomalies, or key opportunities with important guiding significance for business decisions. 49 Design style evaluation questions and scoring criteria: The following prompt guides the generation of scoring rubrics for the Design Style dimension. It reflects the level of visual creativity, uniqueness, and design innovation present in the chart. Prompt: Dashboard - Design Style Evaluation Questions and Scoring Criteria (1-5 Scale) Description: Evaluates whether the overall design of the dashboard has innovation and uniqueness, whether it can show attractive visual effects and business professionalism through novel design elements, unique style characteristics, and professional design language. Scoring criteria: 1. Dashboard design completely lacks innovation, style is outdated or chaotic, no uniqueness, seriously insufficient professionalism. 2. Dashboard design lacks innovation, style is quite ordinary, basically uses common design techniques, insufficient uniqueness and professionalism. 3. Dashboard design has some innovative elements, but overall is relatively conventional, style characteristics not prominent enough, average professionalism. 4. Dashboard design has strong innovation, style is quite unique and professional, with certain originality and good business sense. 5. Dashboard design is extremely innovative, style is unique, novel, and professional, overall presents strong originality and high business aesthetics, leaving deep impressions. Visual composition evaluation questions and scoring criteria: The following prompt guides the generation of scoring rubrics for the Visual Composition dimension. It focuses on the spatial arrangement and organization of visual elements for effective communication. Prompt: Dashboard - Visual Composition Evaluation Questions and Scoring Criteria (1-5 Scale) Description: Evaluates whether the spatial layout of dashboard components and overall arrangement are reasonable and beautiful, including size proportions of various blocks, alignment relationships between components, spacing distribution, information density control, etc., forming harmonious, orderly, and beautiful visual whole. Scoring criteria: 1. Component layouts are chaotic or overall arrangement is seriously unreasonable, size proportions severely unbalanced, spacing distribution is disorderly, poor information density handling. 2. Component layouts or overall arrangement have obvious unreasonable aspects, inappropriate size proportions, uneven spacing distribution, or information too crowded. 3. Component layouts are basically reasonable, overall arrangement is acceptable, but may have small issues in proportional relationships, spacing handling, or information density. 4. Component layouts are good, overall arrangement is reasonable and beautiful, appropriate size proportions, proper spacing distribution, good information density control. 5. Component layouts are perfect, overall arrangement is extremely reasonable and beautiful, coordinated size proportions, aesthetic spacing distribution, appropriate information density control, extremely high space utilization efficiency. Color harmony evaluation questions and scoring criteria: The following prompt guides the generation of scoring rubrics for the Color Harmony dimension. It evaluates how effectively the color scheme supports readability, aesthetic appeal, and visual coherence. 50 Prompt: Dashboard - Color Harmony Evaluation Questions and Scoring Criteria (1-5 Scale) Description: Evaluates whether color choices in the dashboard are appropriate and beautiful, including color combinations within various components, coordination of overall color schemes, unity of tone, and appropriate control of color quantity and saturation, ensuring both beauty and compliance with business professional requirements. Scoring criteria: 1. Component color choices are very inappropriate, overall color matching is chaotic, tone conflicts or color use is seriously inappropriate, very poor aesthetic effect. 2. Component color choices are not appropriate enough, overall color matching is not coordinated enough, tone not unified or color use inappropriate, affecting aesthetic effect. 3. Component color choices are basically appropriate, overall color matching is acceptable, but may have small issues in tone unity, saturation, or business sense. 4. Component color choices are relatively appropriate, overall color schemes are quite coordinated, tone basically unified, appropriate color use, good business sense. 5. Component color choices are very appropriate, overall color schemes are highly coordinated and unified, perfect tone matching, appropriate control of color quantity and saturation, presenting excellent business aesthetics. C.2 EVALUATION PROMPT TEMPLATES To guide consistent evaluation of visualizations, we present structured prompt template built upon the classical Fidelity, Expressiveness, and Aesthetics principles, operationalized into six orthogonal sub-dimensions. The prompt is generated based on the rewritten evaluation questions and scoring criteria from Appendix C.1. The following prompt template outlines how evaluators are instructed to conduct evaluation using these customized inputs. The prompt includes the following components: The {total_count} field specifies the total number of evaluation criteria distributed across the three main dimensions. The {custom_count} field indicates how many of these criteria adopt customized scoring guidelines tailored to the chart. The {chart_description} field provides metadata about the visualization, such as chart type and design structure. The {fidelity_section} field includes rewritten evaluation questions and scoring criteria aligned with the data fidelity sub-dimension. The {expressiveness_section} field covers the semantic readability and insight discovery sub-dimensions. The {aesthetics_section} field captures design-related sub-dimensions, including style, spatial composition, and color harmony. Prompt Template: Visualization Evaluation You are rigorous data visualization evaluation expert. You must strictly judge each visualization based on the Fidelity, Expressiveness, and Aesthetics framework and the 15 scoring criteria for each metric. Chart description: {chart_description} 51 Note: This chart has {total_count} evaluation metrics across three dimensions, of which {custom_count} use custom scoring criteria. The evaluation follows the Fidelity, Expressiveness, and Aesthetics principle: Fidelity: Data accuracy and truthfulness Expressiveness: Information clarity and understandability Aesthetics: Visual aesthetics and refinement For each evaluation question, provide score from 1 to 5 and reasoning based on the scoring criteria. {fidelity_section} {expressiveness_section} {aesthetics_section} Return Format: JSON object with the following structure: { } \"data_fidelity\": {\"score\": 1-5, \"reasoning\": \"Your explanation here. \"}, \"semantic_readability\": {\"score\": 1-5, \"reasoning\": \"Your explanation here.\"}, \"insight_discovery\": {\"score\": 1-5, \"reasoning\": \"Your explanation here.\"}, \"design_style\": {\"score\": 1-5, \"reasoning\": \"Your explanation here.\" }, \"visual_composition\": {\"score\": 1-5, \"reasoning\": \"Your explanation here.\"}, \"color_harmony\": {\"score\": 1-5, \"reasoning\": \"Your explanation here. \"}, \"average_score\": \"the average of the above six scores, rounded to 2 decimals\" Where for each metric, score should be an integer from 1 to 5 based on the above metric descriptions and the 15 scoring criteria, and reasoning should explain your choice. average_score is the average of all six scores rounded to 2 decimal places. Do not include any additional text, only the JSON object."
        },
        {
            "title": "D MODEL IMPLEMENTATION AND TRAINING DETAILS",
            "content": "D.1 HARDWARE AND SOFTWARE ENVIRONMENT The training framework is based on the open-source library SWIFT (Scalable lightWeight Infrastructure for Fine-Tuning)1, utilizing PyTorch and DeepSpeed (ZeRO Stage 2) for distributed training and memory optimization. D.2 REWARD FUNCTION As described in the main text, our composite reward function, Rcomposite, is weighted combination of an accuracy reward (Racc) and format reward (Rformat), with weights of 0.9 and 0.1, respectively. Accuracy Reward (Racc) This component measures the proximity between the models predicted scores across the six dimensions and the average score, and the human-annotated ground-truth values. We employ smooth exponential decay function to calculate the reward for each individual score: Racc_single = exp (cid:18) scorepredicted scoreground-truth 0.5 (cid:19) (1) The final accuracy reward is the average of the rewards calculated for all dimensional scores and the overall average score. Format Reward (Rformat) This component ensures the model produces complete and parsable JSON structure. The reward is 1.0 if the models output contains all required fields (i.e., the score and reasoning for each of the six dimensions, plus the average_score); otherwise, the reward is 0. D.3 HYPERPARAMETER SETTINGS We selected Qwen2.5-VL-7B-Instruct as the base model and employed Low-Rank Adaptation (LoRA) for parameter-efficient fine-tuning. Specifically, we set the LoRA rank and alpha to 128 and applied it to all linear layers. For reinforcement learning, we used the Group Relative Policy Optimization (GRPO) algorithm with beta parameter of 0.01. The model was trained for 5 epochs with learning rate of 1e-5, using Cosine Annealing scheduler with warmup ratio of 0.1. We used the AdamW optimizer with weight decay of 0.01. The global batch size was 16 (per-device batch size of 1 with 4 gradient accumulation steps, across 4 GPUs). For computational efficiency, we utilized bfloat16 mixed-precision training. 1https://github.com/modelscope/swift"
        }
    ],
    "affiliations": [
        "DeepWisdom",
        "The Hong Kong University of Science and Technology (Guangzhou)",
        "Université de Montréal & Mila"
    ]
}