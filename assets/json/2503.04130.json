{
    "paper_title": "Token-Efficient Long Video Understanding for Multimodal LLMs",
    "authors": [
        "Jindong Jiang",
        "Xiuyu Li",
        "Zhijian Liu",
        "Muyang Li",
        "Guo Chen",
        "Zhiqi Li",
        "De-An Huang",
        "Guilin Liu",
        "Zhiding Yu",
        "Kurt Keutzer",
        "Sungjin Ahn",
        "Jan Kautz",
        "Hongxu Yin",
        "Yao Lu",
        "Song Han",
        "Wonmin Byeon"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advances in video-based multimodal large language models (Video-LLMs) have significantly improved video understanding by processing videos as sequences of image frames. However, many existing methods treat frames independently in the vision backbone, lacking explicit temporal modeling, which limits their ability to capture dynamic patterns and efficiently handle long videos. To address these limitations, we introduce STORM (\\textbf{S}patiotemporal \\textbf{TO}ken \\textbf{R}eduction for \\textbf{M}ultimodal LLMs), a novel architecture incorporating a dedicated temporal encoder between the image encoder and the LLM. Our temporal encoder leverages the Mamba State Space Model to integrate temporal information into image tokens, generating enriched representations that preserve inter-frame dynamics across the entire video sequence. This enriched encoding not only enhances video reasoning capabilities but also enables effective token reduction strategies, including test-time sampling and training-based temporal and spatial pooling, substantially reducing computational demands on the LLM without sacrificing key temporal information. By integrating these techniques, our approach simultaneously reduces training and inference latency while improving performance, enabling efficient and robust video understanding over extended temporal contexts. Extensive evaluations show that STORM achieves state-of-the-art results across various long video understanding benchmarks (more than 5\\% improvement on MLVU and LongVideoBench) while reducing the computation costs by up to $8\\times$ and the decoding latency by 2.4-2.9$\\times$ for the fixed numbers of input frames. Project page is available at https://research.nvidia.com/labs/lpr/storm"
        },
        {
            "title": "Start",
            "content": "Token-Efficient Long Video Understanding for Multimodal LLMs 2025-3-7 Jindong Jiang1,2,, Xiuyu Li1,3,, Li1,5, De-An Huang1 Guilin Liu1 Kautz1 Hongxu Yin1 Yao Lu1 Zhijian Liu1 Muyang Li1,4, Guo Chen1,5, Zhiding Yu1 Kurt Keutzer3 Sungjin Ahn"
        },
        {
            "title": "Zhiqi\nJan",
            "content": "Song Han1,4 Wonmin Byeon1 1NVIDIA 2Rutgers University Equal contribution Work performed during internship at NVIDIA 3UC Berkeley 4MIT 5Nanjing University 6KAIST 5 2 0 2 6 ] . [ 1 0 3 1 4 0 . 3 0 5 2 : r Recent advances in video-based multimodal large language models (Video-LLMs) have significantly improved video understanding by processing videos as sequences of image frames. However, many existing methods treat frames independently in the vision backbone, lacking explicit temporal modeling, which limits their ability to capture dynamic patterns and efficiently handle long videos. To address these limitations, we introduce STORM (Spatiotemporal TOken Reduction for Multimodal LLMs), novel architecture incorporating dedicated temporal encoder between the image encoder and the LLM. Our temporal encoder leverages the Mamba State Space Model to integrate temporal information into image tokens, generating enriched representations that preserve inter-frame dynamics across the entire video sequence. This enriched encoding not only enhances video reasoning capabilities but also enables effective token reduction strategies, including test-time sampling and training-based temporal and spatial pooling, substantially reducing computational demands on the LLM without sacrificing key temporal information. By integrating these techniques, our approach simultaneously reduces training and inference latency while improving performance, enabling efficient and robust video understanding over extended temporal contexts. Extensive evaluations show that STORM achieves state-of-the-art results across various long video understanding benchmarks (more than 5% improvement on MLVU and LongVideoBench) while reducing the computation costs by up to 8 and the decoding latency by 2.4-2.9 for the fixed numbers of input frames. 1. Introduction Recent advancements in video-based multimodal Large Language Models (Video-LLMs) have significantly improved the ability of artificial intelligence systems to understand and generate descriptions of video content [1, 2, 3, 4, 5, 6]. common strategy in these models involves treating video as sequence of individual image frames, processing each frame independently using an image encoder and visuallanguage projector [3, 4, 2]. The resulting frame-level representations are then fed into large language model (LLM), which performs temporal reasoning to comprehend the sequence of events depicted in the video. Despite leveraging powerful image encoders and language understanding capabilities of LLMs, this approach exhibits fundamental limitations in video processing, particularly for long videos. The absence of explicit temporal encoding fails to capture crucial temporal dynamics between frames, forcing the LLM to infer temporal relationships solely from sequences of static images. This sequential processing imposes substantial computational burden on the language model, significantly impairing its ability to handle extended video sequences and the generalization ability to longer contexts during inference. To manage computational overhead, existing methods often rely on naive frame subsampling to reduce the number of tokens for LLM processing. However, this approach results in significant information loss from discarded frames, potentially eliminating critical details necessary for comprehensive video understanding [7, 8]. Additionally, these methods fail to effectively compress consecutive frames that typically contain substantial overlapping information. In this work, we propose STORM (Spatiotemporal TOken Reduction for Multimodal LLMs), which introduces novel temporal encoder between the image encoder and the LLM that bridges the gap between visual and language representations. This architecture integrates temporal dynamics earlier in the pipeline, significantly enhancing the temporal reasoning capabilities of Video-LLMs while enabling substantial downstream computational efficiency. By injecting temporal information directly into visual tokens, we reduce the LLMs temporal reasoning burden, allowing it to focus on higher-level language tasks. We employ the Mamba State Space Model [9] as the Website: https://research.nvidia.com/labs/lpr/storm 2025 NVIDIA. All rights reserved. Token-Efficient Long Video Understanding for Multimodal LLMs Figure 1 Open-Ended Video Understanding. We show STORMs ability to handle free-form queries about complex long video scenes. By employing the Mamba-based temporal encoder to capture essential spatiotemporal cues while compressing redundant frame information, STORM enables efficient, accurate long-video understanding and outperforms existing methods on wide range of video understanding tasks. core of our temporal layer, enabling efficient processing of long videos while enhancing generalization to extended temporal contexts. The temporal layer processes image and video inputs differently for images, it functions as spatial scanner that enhances tokens by incorporating global spatial context; for videos, it performs simultaneous spatial and temporal scanning that captures comprehensive spatiotemporal information. The key advantage of the Mamba layer is its ability to compress historical information into state representations. As consecutive frames in the video input often contain redundant information, our temporal encoder efficiently processes and propagates temporal information across the entire video sequence. The resulting visual tokens inherently encapsulate temporal history, effectively summarizing the temporal dynamics of the video. This characteristic allows us to extract fewer visual tokens for LLM processing while preserving key information. In our experiments, we explore straightforward yet effective token reduction approach by applying training-free token subsampling at test time. This method not only reduces computational costs but also enhances performance across various scenarios. Additionally, we also explore training-based compression method including temporal and spatial token compression. Temporal pooling reduces the number of tokens along the temporal dimension, while spatial average pooling decreases tokens per frame. These compression strategies are optimized during training to preserve essential information while minimizing redundancy. Unlike previous methods that directly subsample raw video frames, which can lead to loss of critical information, our approaches preserve the essential temporal information in compressed format. Our method not only reduces the computational load on the LLM but also improves model performance through more comprehensive video representation in compact token space. Our empirical evaluation shows that the proposed approach successfully extends to the long-context video understanding model without compromising training efficiency. 2. Related Work (VLMs) primarily adopt Vision Language Model Architecture Vision language models two paradigms to process visual input. The first paradigm freezes language model weights and integrates visual information via cross-attention mechanisms [10], while the second paradigm utilizes pre-trained image encoder, such as CLIP [11] or SigLIP [12], to convert images into tokens. These tokens are then concatenated with text tokens and input into the language model [2, 1, 13]. This approach can be naturally extended to video understanding by treating videos as sequences of images processed by the vision encoder [3, 4]. To enhance video processing, some works introduce specialized video encoders. For instance, InternVideo [5, 6] uses VideoMAE [14] as video encoder, while Kangaroo [15] integrates depth-wise 3D convolution for fusing video tokens. In this work, we retain SigLIP as the vision encoder and focus on enhancing long video understanding by incorporating linear-complexity temporal module in the Mamba [9] architecture. Positioned between the SigLIP vision encoder and the language model, this module efficiently improves spatial-temporal modeling effectiveness. 2 Token-Efficient Long Video Understanding for Multimodal LLMs Long Video Understanding Understanding long videos with VLMs presents significant challenges in both accuracy and efficiency. Previous approaches have employed long-context language models trained on short-context video data to enable long video comprehension [7]. However, these methods lack sufficient long video training data and incur high computational costs during both training and inference as the number of frames increases. LongVILA [8] addresses these challenges through multi-modal sequence parallelism system that directly handles long video data during training and inference, but this approach requires customized system implementations tailored for multi-GPU setups. Another line of research focuses on token reduction to shorten input sequences, thereby enabling efficient inference for long videos [16, 17, 18, 19, 20, 21]. For instance, VoCO-LLaMA [18] and VideoXL [20] use recursive KV cache compression learnt in an end-to-end manner, and LongVU [21] leverages DINO features for frame selection and inter-frame similarity to reduce tokens. Despite these diverse strategies, direct pooling along the temporal or spatial dimensions often performs sufficiently well, with additional gains being marginal. In this paper, we apply temporal and spatial pooling for token reduction, achieving superior performance when combined with our temporal projector. 3. Method This section presents the Mamba-based temporal projector architecture and introduces several token compression techniques for efficient long video processing. We propose two token compression approaches: temporal and spatial compression. We first detail our temporal and spatial pooling strategies that effectively reduce the number of tokens during training. Additionally, we propose test-time temporal token sampling method that maintains model performance without requiring additional training steps. The overview of our method is shown in Figure 2. 3.1. Preliminaries State Space Models (SSMs) State Space Model (SSM) [22, 23, 9, 24] establishes linear transformation between an input sequence 𝑥1:𝑇 R𝑇 𝐷 and an output sequence 𝑦1:𝑇 R𝑇 𝐷 through the following recurrent process: the hidden state summarizing the history 𝑥𝑡. The matrices 𝐴𝑡 R𝐻𝐻 , 𝐵𝑡 R𝐻𝐷, and 𝐶𝑡 R𝐷𝐻 are parameterized with learnable weights designed to facilitate the modeling of long-term dependencies. When 𝐴𝑡, 𝐵𝑡, 𝐶𝑡 are time-invariant (constant over 𝑡), the computation of 𝑦1:𝑇 can be parallelized, enabling efficient training and inference. Mamba Recently, Mamba [9] propose to condition these matrices on the input 𝑥𝑡 to enhance the sequence modeling capabilities of SSMs. Specifically, Mamba employs learnable functions 𝐴 : R𝐷 R𝐻𝐻 , 𝐵 : R𝐷 R𝐻𝐷, and 𝐶 : R𝐷 R𝐷𝐻 to generate input-dependent matrices as follows: (2) 𝐴𝑡 = 𝐴(𝑥𝑡) , 𝐵𝑡 = 𝐵(𝑥𝑡) , 𝐶𝑡 = 𝐶(𝑥𝑡) . This approach allows the model to dynamically emphasize or suppress information based on the current input, thereby enabling more flexible and adaptive sequence modeling. Additionally, Mamba leverages hardware-aware parallel algorithm to ensuring that the input-dependent matrices does not hinder the training and inference efficiency inherent to SSMs. 3.2. Mamba-Based Temporal Projector Traditional Video-LLMs often process video frames independently, requiring the LLM to infer temporal relationships from sequences of static images. This approach is computationally inefficient, particularly when processing long videos. Additionally, this method fails to leverage the inherent temporal redundancy in video data, resulting in redundant processing of similar information across consecutive frames. To address these limitations, we introduce the Mambabased temporal projector, which efficiently integrates temporal information across video frames while enabling effective temporal token compression. Let X𝑡 ^𝑁 𝐷 denote the image tokens for frame 𝑡 by Vision Transformer (ViT) encoder, where ^𝑁 is the number of tokens per frame and 𝐷 is the token dimension. We first apply linear layer to downsample the tokens of each frame to ^𝑁 𝑟 for 𝑡 = 1, . . . , 𝑇. X𝑡 = Linear (X𝑡) , tokens: (3) Where 𝑟 is the downsample ratio. For simplicity, we define 𝑁 = ^𝑁 and use 𝑁 throughout the remainder of this paper. The downsampled tokens from all frames are stacked to form the input tensor for the temporal module: 𝑟 ℎ𝑡 = 𝐴𝑡ℎ𝑡1 + 𝐵𝑡𝑥𝑡 , 𝑦𝑡 = 𝐶𝑡ℎ𝑡 . (1) = [ X1; X2; . . . ; X𝑇 ] R𝑇 𝑁 𝐷. (4) Here, 𝑇 is the sequence length; 𝑥𝑡, 𝑦𝑡 R𝐷 are input and output vectors at time 𝑡; and ℎ𝑡 R𝐻 is The core of the temporal projector consists of 𝐿 Mamba layers that iteratively integrate temporal dynamics into the tokens. In each layer 𝑙 = 1, . . . , 𝐿, we 3 Token-Efficient Long Video Understanding for Multimodal LLMs Figure 2 Overview of STORM pipeline. We propose Mamba-based temporal projector between the image encoder and the LLM. This projector bridges the gap between visual and language representation while injecting the temporal information into the tokens. The processed tokens, denoted as Summary Tokens in the figure, naturally capture temporal history, effectively summarizing the temporal dynamics of the video. This capability enables us to reduce the number of visual tokens for LLM processing without sacrificing essential information. fuse temporal information into the visual tokens by: ( ( X(𝑙1)))"
        },
        {
            "title": "Norm",
            "content": "X(𝑙) = X(𝑙1) + MambaMixer , (5) where X(0) = X, and Norm() denotes layer normalization. Each MambaMixer employs bidirectional scanning module that captures dependencies in both spatial and temporal dimensions. Specifically, we apply sweeping scan order within each frame and across frames, i.e., left-to-right, top-to-bottom, and frame-to-frame (see Figure 2). After 𝐿 layers, we obtain tokens enriched with temporal information, denoted as X(𝐿) R𝑇 𝑁 𝐷. 3.3. Training-Time Token Compression Processing long videos poses two major challenges for LLMs. First, handling all frames is computationally expensive, often requiring specialized system optimizations like sequence parallelization and multiple GPUs for training and inference [8]. Second, LLMs are inherently limited by their training context lengths constraints. For example, LLaMA 3 has context length of 8K tokens, which corresponds to only 32 frames for video inputs when using 256 tokens per frame. Without token compression, video processing quickly exceeds the effective input capacity of LLMs, leading to significantly reduced model performance. In this work, we aim to address both computational cost and context length limitations by enabling efficient long-video processing through token compression along both temporal and spatial dimensions. Our approach eliminates the need for custom system optimizations and allows inference on single GPU. We illustrate the training-time token compression in Figure 3 (left and middle). Temporal Pooling Since consecutive frames often contain similar information, analyzing every frame may lead to redundant processing and potential overfitting. Additionally, having too many tokens can make it difficult for the LLM to identify important temporal patterns. Thus, we propose applying temporal average pooling to compress visual information efficiently [25, 26]. This method combines data from consecutive frames by averaging their enriched visual tokens. Specifically, for the tokens X(𝐿) R𝑇 𝑁 𝐷 from the temporal projector, we average every 𝑘 consecutive frames: Xtime-avg = 1 𝑘 𝑘1 𝑖=0 (𝐿) 𝑡+𝑖, for 𝑡 = 0, 𝑘, 2𝑘, . . . , 𝑇 𝑘. As result, we obtain compressed tokens: Xtime-avg 𝑇 𝑘 𝑁 𝐷, (6) (7) 4 Token-Efficient Long Video Understanding for Multimodal LLMs Figure 3 Token Compression Strategies. This figure illustrates our token compression techniques: temporal average pooling (left), spatial average pooling (middle), and training-free temporal token sampling (right). These methods can be applied individually or in combination, depending on task requirements and computational budget constraints. Despite its simplicity, temporal averaging effectively decreases the number of tokens the LLM processes, with minimal loss of critical information. This motivates us to adopt such simple yet effective technique for efficient training in long video understanding. Spatial Pooling In addition to temporal pooling, we also explore average pooling in the spatial domain. Formally, given the input X(𝐿) R𝑇 𝑁 𝐷 from the visual encoder and the spatial compression ratio 𝑝, we apply average pooling with kernel size and stride of 𝑝 on each frame, resulting in Xspace-avg R𝑇 𝑁 𝑝 𝐷. 3.4. Training-Free Temporal Token Sampling After processing through the temporal projector, each visual token is enriched with spatiotemporal information, capturing features not only from its corresponding frame but also from other frames across the entire video. This encoding of global information allows us to subsample the visual tokens along the temporal dimension at test time, further reducing the number of tokens fed into the LLM without significant loss of information or performance. This temporal token subsampling strategy can be done with or without our pooling mechanisms. Note that comparing to methods that subsample raw frames and risk discarding critical temporal cues, our approach leverages the spatiotemporal richness of tokens post-temporal encoding. We show an illustration of the temporal token sampling in Figure 3 (right). Formally, let R𝑇 𝑁 𝐷 denote the token input to the subsampling layer which can be tokens from visual encoder X(𝐿), or tokens output from compression modules Xtime-avg or Xspace-avg. Here, 𝑇 and 𝑁 are the temporal and spatial dimensions after any temporal or spatial pooling. We apply uniform subsampling with rate 𝑠 along the temporal dimension. Xtime-skip = {X 𝑡 𝑡 = 0, 𝑘, 2𝑘, . . . , 𝑇 1} 𝑇 𝑠 𝑁 𝐷, (8) Our empirical results demonstrate that this subsampling method not only maintains performance on various video understanding benchmarks but can also improve it by reducing noise from redundant frames. 4. Experiments In this section, we extensively evaluate the proposed method on various video understanding benchmarks and provide empirical analysis demonstrating how the temporal projector enables efficient token reduction while delivering strong video reasoning abilities. 4.1. Training Details the pre-trained SigLIP [12] We utilize from PaliGemma [27] and Qwen2-VL [28], respectively, and fine-tune them to adapt to our video datasets. The temporal projector is initialized with random weights. Each image is always resized to 448 448 resolution. Training In the first stage, known as the Alignment Stage, we freeze both the image encoder and the LLM, training only the temporal projector using small image-text dataset [37], containing 95K image-text pairs. Note that the Mamba layers perform not only temporal scan but also spatial scan within images, so video inputs are not strictly required to train it. For alignment stage, we find it sufficient to use only imagetext pairs to pretrain the temporal projector. In the second stage, the supervised fine-tuning stage (SFT), we fine-tune all three components using large and diverse dataset that includes text-only, image-text, and video-text data. There are around 12.5M samples in our SFT data mixture. due to space constraints. At this stage, we use 32 frames for each video input. For models with training-time token compression , we 5 Token-Efficient Long Video Understanding for Multimodal LLMs Models Duration Size Comp. #Frames # Frames MVBench MLVU LongVideoBench VideoMME Ratio (%) (train) (test) test 16 sec dev val (w/o sub.) 3120 min 8 sec60 min 160 min Proprietary Models GPT4-V GPT4-O - - Open-Source Video-LLMs Video-LLaVA [29] LLaMA-VID [16] Chat-UniVi [30] ShareGPT4Video [31] LLaVA-NeXT-Video [32] VideoLLaMA2 [4] VideoChat2 [3] Long-LLaVA [33] LongVA [7] LongVILA [8] mPLUG-Owl3 [34] LLaVA-OneVision [35] Kangaroo [15] VideoXL [20] Oryx-1.5 [36] LongVU [21] Qwen2-VL [28] 7B 7B 7B 8B 7B 7B 7B A13B (53B) 7B 7B 8B 7B 8B 7B 7B 7B 7B - - - - - - - - - - - - - - - - - - - 1fps 1fps 8 1fps 64 16 32 32 16 128 128 256 128 32 64 2048 64 1fps 2fps 43.7 64.6 41.0 41.9 - 51.2 33.7 54.6 60.4 54.6 - - 54.5 56.7 61.0 55.3 67.6 66.9 67. - 66.2 47.3 33.2 - 46.4 - 48.5 47.9 - 56.3 - - 64.7 61.1 64.9 67.5 65.4 - 8 1fps 64 16 16 16 8 - - 2048 8 32 160 128 256 - 2fps - - - - - - - - - - - - 52.1 56.5 54.8 49.5 56.3 - 55.6 60.7 77. 40.4 - 45.9 43.6 46.5 46.6 54.6 51.6 54.3 57.5 53.5 58.2 56.0 55.5 58.8 60.6 63.3 Ours * test-time temporal sampling is applied, 2 additional compression. 25/12.5* 128 7B 71.3 72.9* 60.5* 63.4 Table 1 Comparison with Existing Video-LLMs. We compare our best-performing configuration (STORM + T. Pooling) against existing Video-LLMs and the baseline VILA model. Our approach outperforms all existing models while using significantly fewer tokens through 4 temporal pooling token compression. Additionally, values with asterisk (*) indicate that test-time token sampling is applied, which introduces additional 2 compression on visual inputs. The latency are based on 256 input frames. use compression ratio of 4 temporal pooling models compress 32 frames to 8 frames while spatial pooling models compress 256 tokens per image into 64 tokens. Moreover, for models with training-time token compression , we further employ long video finetuning stage using 128-frames long-video inputs from the LLaVA-Video dataset [38]. We provide further details about the full SFT dataset and long video fine-tuning dataset in the appendix Section 7.4. Evaluation We evaluate our STORM across multiple configurations on recent long-video understanding benchmarks specifically designed to rigorously assess video-language model capabilities. These benchmarks include EgoSchema [39], MVBench [40], MLVU [41], LongVideoBench [42], and VideoMME [43]. We compare our approach against wide range of representative video-language models, including recently proposed models tailored for long-video understanding [8, 21, 7, 20, 33, 15, 36]. See Table 1 for details. Models We implement STORM based on the VILA codebase [2], typical VLM pipeline consisting of vision encoder, LLMs, and vision-language projector, and introduce our novel Mamba module and compression mechanisms into the architecture. We will refer all models trained with the VILA codebase as VILAbased models in our experiments. To thoroughly analyze our design, we evaluate STORM variants using all combinations of the three compression methods: temporal average pooling, spatial average pooling, and temporal token sampling. The full results are presented in Table 5. For comparison with existing Video-LLMs, we highlight the best-performing variants in Table 1. And include the detailed analysis of all variants of STORM in Table 2. To ensure fairness, we also include baseline VILA model trained on the same dataset and training scheme but without the Mamba module. All of our models in Table 2 are trained under the same 8K token budget (corresponding to 32 frames of tokens ), which represents the number of visual tokens fed to the LLMa key factor affecting inference latency and memory consumption, particularly as the number of frames increases (see Section 5). Specifically, we report results for the standard STORM trained on 32-frame inputs and STORM with Temporal Pooling, which processes 128frame inputs while reducing the token count to match the 32-frame variant. Additionally, we evaluate configurations where temporal sampling is applied at test time (+T. Sampling), which not only further enhances model efficiency but also improves performance on certain benchmarks. 6 Token-Efficient Long Video Understanding for Multimodal LLMs VILA-Based Models Duration Size Comp. Latency #Frames # Frames MVBench MLVU LongVideoBench VideoMME Ratio (%) (s) (train) (test) test dev val (w/o sub.) 16 sec 3120 min 8 sec60 min 160 min Token Budget: 8K 7B VILA Baseline 7B STORM 7B STORM + T. Pooling STORM + T. Sampling * 7B STORM + T. Pooling + T. Sampling * 7B * 2 additional compression at test time. 100 100 25 50 12.5 4.38 4.26 1.82 2.50 1. 32 32 128 32 128 256 256 256 256 256 69.5 70.3 71.3 70.1 70.6 70.2 71.1 72.5 70.8 72.9 55.9 54.5 59.5 54.8 60.5 60.1 62.4 63.4 63.1 62. Table 2 Comparison between VILA-Based Models on the Same Token Budget. Here we compare between STORM variants and the baseline VILA model, all trained with identical data and pipelines. All models use the same 8K token budget, representing the maximum number of visual tokens provided to the LLM during training. Temporal pooling (T. Pooling) applies 4 compression and test-time token sampling (T. Sampling) applies additional 2 compression. 4.2. Results on Video Understanding Benchmarks STORM vs Existing Methods We start with comparing our configuration STORM + T. Pooling (+ T. Sampling) with existing Video-LLMs. As shown in Table 1 and detailed in Table 2, STORM + T. Pooling achieves state-of-the-art performance across all long-video understanding benchmarks. Specifit achieves 71.3% accuracy on MVBench, ically, 72.5% on MLVU, 59.5% on LongVideoBench, and 63.4% on VideoMME, outperforming all open-source Video-LLMs, including recent models specifically designed for long-context inputs such as LongVU and LongVILA. Additionally, our method significantly narrows the performance gap with proprietary models, outperforming GPT4-V and GPT4-O on MVBench and MLVU, as well as GPT4-V on VideoMME. Notably, STORM + T. Pooling achieves computational efficiency by compressing visual tokens to 25% of their original number before processing them through the LLM. We can further enhance efficiency by applying test-time temporal sampling in STORM + T. Pooling + T. Sampling while still achieving competitive results, reducing the token count to just 12.5% while maintaining competitive performance. In fact, this additional compression even improves results of certain benchmarks, yielding the best overall performance on MLVU and LongVideoBench. STORM vs Baseline VILA Next, we provide controlled comparisons within VILA-based models to reveal advantages of the Mamba-based temporal models in Table 2. We first compare the baseline VILA model with our STORM. By incorporating the Mamba module, STORM achieves performance gains on three out of four benchmarks, including notable 2.3% improvement on VideoMME. Moreover, augmenting STORM with test-time temporal token sampling (STORM + T. Sampling) further enhances efficiency, reducing inference time by 43% while, surprisingly, maintaining or slightly boosting performance (an additional 0.8% gain on VideoMME). This advantageous behavior emerges because the Mamba modules ability to effectively propagate temporal information across video frames, enabling redundant tokens to be discarded without compromising the models overall understanding. The temporal pooling variant (STORM + T. Pooling) extends these benefits to long-context training, by applying temporal average pooling after the Mamba layer, which allows the model to process 128-frame inputs while compressing the token count to match that of the 32-frame setting. This approach not only improves performance, achieving 63.4% (+3.3%) on VideoMME, 59.5% (+3.6%) on LongVideoBench, 72.5% (+2.3%) on MLVU, and 71.3% (+1.8%) on MVBench, but also significantly reduces inference latency by 58.4%. By combining this model with test-time temporal token sampling (STORM + T. Pooling + T. Sampling), we further reduce the inference time by 65. 5% and use only 12. 5% of the visual tokens compared to VILA without sacrificing performance. We observe that this test-time temporal token sampling particularly benefits MLVU and LongVideoBench compared to MVBench and VideoMME. This different impact across benchmarks likely stems from the nature of the underlying tasks. MLVU and LongVideoBench require global understanding across long videos. We believe that testtime compression with the Mamba module better summarizes the essential contextual information. On the other hand, MVBench and VideoMME require visual details from specific frames. Our pooling-only method with the Mamba module maintains more detailed frame information throughout the sequence. Section 5 includes detailed discussion about retaining visual information with our token compression. 7 Token-Efficient Long Video Understanding for Multimodal LLMs Models LongVideoBench VideoMME S. Pooling T. Pooling 32F 56.0 54.2 128F 55.9(-0.1) 59.5 (+5.3) 32F 61.1 61.2 128F 58.3 (-2.8) 63.4 (+2.2) Table 3 Comparison between Spatial and Temporal pooling on Frame Extension. Models were initially trained on 32 frames, then fine-tuned on top 25% of long video from 128F dataset (see Section 7.4). indicates frame count during training. Both methods apply compress visual tokens to 25% of the original count. While spatial pooling performance degrades with increased frame length, temporal pooling shows consistent improvements, demonstrating its effectiveness for long video understanding. 8F 32F (T. Pooling) 128F (T. Pooling) Models VILA (w/o Mamba) 62.0 STORM (w/ Mamba) 61.6 63.5 (+1.5) 64.2 (+2.6) 64.3 (+0.8) 66.7 (+2.5) Table 4 Effectiveness of Mamba Module with Token Compression. Results show average performance across all video benchmarks. indicates the number of frames used during training. Numbers in parentheses show improvements over shorter version of the same models. For the two temporal pooling configurations, models were initially trained on the full dataset with 32F inputs, then fine-tuned on 128F inputs using smaller long-video dataset (see Section 7.4). STORM demonstrates substantially better utilization of longer temporal contexts, with the performance gap between VILA baseline and STORM widening as input length increases (+0.7% at 32F and +2.4% at 128F compared to baseline). Full details of the comparisons are provided in Table 9. Spatial Pooling vs Temporal Pooling on Long Video Inputs Table 3 provide comparison between spatial average pooling and temporal average pooling on 32 frames training and 128-frames extension. All models use STORM as the base model. We see that spatial pooling is effective when 32 frames are during training, outperforming temporal pooling and on LongVideoBench while achieving on par results on VideoMME. However, when applying 128-frames input, even though both methods use the same token budget, temporal pooling results in sigificantly better performance. In fact, spatial pooling can not benefit from longer video inputs, results in degrated performance on both LongVideoBench and VideoMME, while temporal pooling successfully achieves stronger performance on both benchmarks from the extended video length. 5. Analysis Mamba Module Improves Simple Token Compression As illustrated in Figure 2, the Mambabased temporal projector performs spatio-temporal scan over input frames. This approach enables refinement of visual tokens before compression operations, thereby preserving key temporal and spatial cuessuch as the positional information of frames being pooledthat would otherwise be lost in naive compression strategies. Consequently, the Mamba temporal module allows simple token compression methods to work more effectively for long videos compared to the baseline, as evidenced in Table 4 and further detailed in Table 9. Our results demonstrate that STORM consistently improves with increased video input length during training, achieving substantial gains of +2.6% over all benchmarks when extending from 8 frames to 32 frames and an additional +2.5% when extending from 32 frames to 128 frames. In contrast, the baseline VILA model shows smaller improvement when extending from 8 frames to 32 frames, and demonstrates only +0.8% performance gains when fine-tuned on 128 frames. In fact, as shown in Table 9, the baseline model exhibits performance degradation on MVBench and MLVU benchmarks when extending video length from 32 frames to 128 frames. These results fully demonstrate the importance of the Mamba module in enabling both efficient and effective token compression, particularly for long-video input Ablation of Mamba and token compression modules Table 5 presents an ablation study comparing various token compression strategies for models trained on fixed 32-frame input. Without any compression, our standard STORM delivers improved performance across benchmarks comparing to the baseline VILA while utilizing similar inference latency and number of visual tokens. When test-time temporal sampling is applied, the model maintains its performance while reducing the visual token count to 50% and lowering inference latency to 58.7% of the uncompressed STORM. Employing temporal average pooling during both training and testing further compresses the token count to 25% and cuts latency down to 42.7%, which is particularly effective for MVBench and MLVU but slightly degrades LongVideoBench and VideoMME. Similarly, spatial average pooling provides the same efficiency improvements and proves particularly effective on LongVideoBench, but with compromises on other benchmarks."
        },
        {
            "title": "We further apply temporal token sampling with\neither temporal or spatial pooling to improve the",
            "content": "8 Token-Efficient Long Video Understanding for Multimodal LLMs Model VILA STORM Mamba Module Sampling Pooling Pooling budget Ratio (%) Token Compression T. T. S. Latency (s) MVBench MLVU LongVideoBench VideoMME w/o sub. 8K 8K 8K 2K 2K 2K 2K 0.5K 0.5K 100 100 50 25 25 12.5 12.5 6.25 3. 4.38 4.26 2.50 1.82 1.82 1.51 1.51 1.36 1.29 69. 70.3 70.1 70.4 68.9 70.1 68.9 68.5 67.2 70.2 71.1 70. 71.0 69.2 71.0 69.5 68.2 68.6 55.9 54.5 54.8 54.2 56. 54.0 56.3 53.7 55.3 60.1 62.4 63.1 61.2 61.1 60.8 60. 60.2 60.1 Table 5 Ablation of token compression methods. All models are trained on 32 frames. The pooling methods are added during training, and the temporal sampling is executed at test-time. test-time efficiency. For instance, STORM + T. Pooling + T. Sampling reduces the visual tokens to only 12.5% and latency to 35.4% of the original STORM, while maintaining comparable accuracy to STORM + T. Pooling. The STORM + S. Pooling + T. Sampling even achieves the best LongVideoBench performance with this strong token reduction. Finally, we combine the temporal and spatial average pooling with/without test-time temporal token sampling, leading to even more aggressive compression. Interestingly, our STORM + T. Pooling + S. Pooling + T. Sampling variant, which has the strongest compression ratio, utilizing only 3.13% visual token and 29.5% of inference latency, already achieves competitive results. Comparing to the baseline VILA, it achieves 100% performance on VideoMME, 98.9% on LongVideoBench, 97.7% on MLVU, and 96.7% on MVBench, making it quite attractive for efficiencyoriented scenarios. We note that, although the uncompressed STORM and its test-time sampling variant achieve the highest overall performance when using the same 32 frames input, their computational demands limit scalability to train on longer sequence. In contrast, as demonstrated in Table 2, the STORM + T. Pooling allows extending the temporal context to 128 frames with temporal pooling, which enables improved performance without requiring additional LLM computation. Token Compression Improves Model Efficiency We provide analysis on model efficiency in Figure 4. In Figure 4 left, we compare the inference latency of STORM with and without token compression and across varying numbers of input frames. Due to the quadratic computation of the LLM, the latency gap between the two configurations widens significantly as the sequence length increases. Figure 4 middle provides detailed breakdown of inference latency of different modules for processing 256 frames. Without token compression, approximately 80% of the total latency is attributed to the LLM module. By applying token compression ratio of 4, the latency associated with the LLM is reduced to roughly 35% of its original value, demonstrating substantial improvement in efficiency. Importantly, the temporal projector enables token compression without compromising performance. In fact, for some benchmarks, it even improves model performance while reducing computation cost, as demonstrated in Table 2 and detailed in Table 8. Figure 4 right presents detailed analysis using the temporal sampling method on the VideoMME benchmark. We compare three configurations: the VILA baseline, STORM without compression, and STORM with testtime temporal token sampling Using up to 128 input frames and compression ratio of 2. The results indicate that: (1) The VILA baselines performance improves up to 32 frames but declines beyond this point, (2) STORM generalizes better to longer sequences, maintaining performance up to 64 frames before slight drop, and (3) STORM with temporal sampling continues to improve performance up to 128 frames while providing 50% token compression ratio, meaning STORM + T. Sampling at 128 frames uses the same number of visual tokens as the other two models at 64 frames. We speculate that phenomenon is because of the limitation of the pre-trained LLM models which, despite having large context lengths on paper, operate with smaller effective context length in practice. This limitation hinders their performance on very long sequences. The combination of the temporal projector and temporal sampling ensures that the number of tokens fed into the LLM remains within its effective context length. Simultaneously, the enriched visual tokens retain comprehensive temporal information beyond the sampled frames. Mamba Module Has Minimal Training Overhead The Mamba projector introduces minimal training overhead: even when no compression is ap9 Token-Efficient Long Video Understanding for Multimodal LLMs Figure 4 Model Efficiency and Effectiveness on Long Video Inputs. (left) Profiling results of token compression as the number of frames increases during inference. (middle) Profiling results for 256 input frames with different compression ratios on single A100. (right) The accuracy of Video-MME (without subtitles) across different numbers of frames during inference. While STORM with test-time temporal sampling showed consistent performance improvements, both VILA and STORM without token compression demonstrated decreased performance beyond 64 frames. Figure 5 Qualitative Examples of STORM + T. Pooling. Our model effectively processes complex video content across various tasks requiring fine-grained temporal and visual understanding while reducing computational overhead through efficient token compression. The example videos can be found in our website. 10 Token-Efficient Long Video Understanding for Multimodal LLMs plied, it adds only 5% latency in full training (19.1 hours with VILA (without Mamba module) and 20.1 hours with STORM (with Mamba module)). More importantly, since the Mamba module enables effective token compression, it in fact provides significant training / inference speedups and performance gains (e.g., +3.6% on MLVU with 4 fewer tokens), eventually reducing training the costs. Architectures Resolution Latency (ms) VideoMME VILA Ours VILA Ours Qwen2 7B + Paligemma Llama3 8B + SigLip Qwen2 1.5B + SigLip 448448 384384 384384 1980 1560 724 926 724 59.7 54.6 49.6 61.2 56.8 52.6 Table 6 Performance Comparison across Model Architectures, Model Sizes, and Input Resolutions. We provide results of benchmark VideoMME, without subtitles version. Our Token Compression Retains Visual Information The qualitative results in Figure 5 demonstrate that STORMs token compression preserves critical visual information while significantly reducing computational overhead. Even with 4 compression ratio, our model accurately extracts task-relevant information across diverse video understanding tasks. For example, questions like Whom is the poem in the video written by?, How many times do news segments appear in this video?, and What is unique about the last performance? require fine-grained visual information and focus on the specific content from the prompt. Detailed category-level analysis on VideoMME (Figure 7) also reveals that STORM + token compression consistently outperforms baseline model across nearly all task categories. Even in OCRheavy tasks that require detailed visual analysis, our compressed model maintains performance comparable to uncompressed versions while using only fraction of the computational resources (only 25% visual tokens). These findings demonstrate that our Mamba-based temporal encoder enables effective token compression by encoding spatiotemporal relationships directly into visual tokens, allowing the model to maintain highlevel understanding while drastically reducing token count. Additional qualitative results are provided in Figure 10-13. Our Modules Generalize to Various Architectures Our core design, temporal module + token compression, is model-agnostic and can be integrated into various video-LLM architectures. Table 6 shows the experiments on models utilizing different LLM models, vision encoders, and model sizes. Our design shows consistent improvements in both performance and latency for all configurations, clearly showcasing the universality of the model. 6. Conclusion We introduced STORM, novel Video-LLM architecture that enhances long-video understanding through Mamba-based temporal encoder and efficient token reduction. By explicitly integrating spatiotemporal dynamics into visual tokens early in the pipeline, STORM enables significant token compression while preserving critical temporal information in the compressed inputs. Experiments demonstrate that STORM achieves new state-of-the-art results on long-video understanding benchmarks while substantially improving computational efficiency. Our work establishes scalable approach to long videolanguage modeling that simultaneously achieve good performance and computation efficiency."
        },
        {
            "title": "References",
            "content": "[1] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 2024. [2] Ji Lin, Hongxu Yin, Wei Ping, Yao Lu, Pavlo Molchanov, Andrew Tao, Huizi Mao, Jan Kautz, Mohammad Shoeybi, and Song Han. Vila: On pretraining for visual language models. CVPR, 2024. [3] KunChang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai Wang, Ping Luo, Yali Wang, Limin Wang, and Yu Qiao. Videochat: Chat-centric video understanding. arXiv preprint arXiv: 2305.06355, 2023. [4] Zesen Cheng, Sicong Leng, Hang Zhang, Yifei Xin, Xin Li, Guanzheng Chen, Yongxin Zhu, Wenqi Zhang, Ziyang Luo, Deli Zhao, and Lidong Bing. Videollama 2: Advancing spatial-temporal modeling and audio understanding in video-llms. arXiv preprint arXiv:2406.07476, 2024. [5] Yi Wang, Kunchang Li, Yizhuo Li, Yinan He, Bingkun Huang, Zhiyu Zhao, Hongjie Zhang, Jilan Xu, Yi Liu, Zun Wang, et al. Internvideo: General video foundation models via generative and discriminative learning. arXiv preprint arXiv:2212.03191, 2022. [6] Yi Wang, Kunchang Li, Xinhao Li, Jiashuo Yu, Yinan He, Guo Chen, Baoqi Pei, Rongkun Zheng, Jilan Xu, Zun Wang, et al. Internvideo2: Scaling video foundation models for multimodal video understanding. arXiv preprint arXiv:2403.15377, 2024. [7] Peiyuan Zhang, Kaichen Zhang, Bo Li, Guangtao Zeng, Jingkang Yang, Yuanhan Zhang, Ziyue Wang, 11 Token-Efficient Long Video Understanding for Multimodal LLMs Haoran Tan, Chunyuan Li, and Ziwei Liu. Long context transfer from language to vision. arXiv preprint arXiv:2406.16852, 2024. [8] Fuzhao Xue, Yukang Chen, Dacheng Li, Qinghao Hu, Ligeng Zhu, Xiuyu Li, Yunhao Fang, Haotian Tang, Shang Yang, Zhijian Liu, Yihui He, Hongxu Yin, Pavlo Molchanov, Jan Kautz, Linxi Fan, Yuke Zhu, Yao Lu, and Song Han. Longvila: Scaling longcontext visual language models for long videos. arXiv preprint arXiv: 2408.10188, 2024. [9] Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. In First Conference on Language Modeling, 2024. [10] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: visual language model for few-shot learning. Advances in neural information processing systems, 2022. [11] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning. PMLR, 2021. [12] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1197511986, 2023. [13] Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, et al. Palm-e: An embodied multimodal language model. In International Conference on Machine Learning. PMLR, 2023. [14] Zhan Tong, Yibing Song, Jue Wang, and Limin Wang. Videomae: Masked autoencoders are data-efficient learners for self-supervised video pre-training. Advances in neural information processing systems, 35, 2022. [15] Jiajun Liu, Yibing Wang, Hanghang Ma, Xiaoping Wu, Xiaoqi Ma, Xiaoming Wei, Jianbin Jiao, Enhua Wu, and Jie Hu. Kangaroo: powerful videolanguage model supporting long-context video input. arXiv preprint arXiv: 2408.15542, 2024. [16] Yanwei Li, Chengyao Wang, and Jiaya Jia. Llamavid: An image is worth 2 tokens in large language models. European Conference on Computer Vision, 2023. [17] Yuetian Weng, Mingfei Han, Haoyu He, Xiaojun Chang, and Bohan Zhuang. Longvlm: Efficient long video understanding via large language models. In European Conference on Computer Vision, pages 453 470. Springer, 2025. [18] Xubing Ye, Yukang Gan, Xiaoke Huang, Yixiao Ge, Ying Shan, and Yansong Tang. Voco-llama: Towards vision compression with large language models. arXiv preprint arXiv:2406.12275, 2024. [19] Michael S. Ryoo, Honglu Zhou, Shrikant Kendre, Can Qin, Le Xue, Manli Shu, Silvio Savarese, Ran Xu, Caiming Xiong, and Juan Carlos Niebles. xgenmm-vid (blip-3-video): You only need 32 tokens to represent video even in vlms. arXiv preprint arXiv: 2410.16267, 2024. [20] Yan Shu, Peitian Zhang, Zheng Liu, Minghao Qin, Junjie Zhou, Tiejun Huang, and Bo Zhao. Video-xl: Extra-long vision language model for hour-scale video understanding. arXiv preprint arXiv: 2409.14485, 2024. [21] Xiaoqian Shen, Yunyang Xiong, Changsheng Zhao, Lemeng Wu, Jun Chen, Chenchen Zhu, Zechun Liu, Fanyi Xiao, Balakrishnan Varadarajan, Florian Bordes, Zhuang Liu, Hu Xu, Hyunwoo J. Kim, Bilge Soran, Raghuraman Krishnamoorthi, Mohamed Elhoseiny, and Vikas Chandra. Longvu: Spatiotemporal adaptive compression for long video-language understanding. arXiv preprint arXiv: 2410.17434, 2024. [22] Daniel Y. Fu, Tri Dao, Khaled Kamal Saab, Armin W. Thomas, Atri Rudra, and Christopher Ré. Hungry hungry hippos: Towards language modeling with state space models. 2023. [23] Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y. Fu, Tri Dao, Stephen Baccus, Yoshua Bengio, Stefano Ermon, and Christopher Ré. Hyena hierarchy: Towards larger convolutional language models. 2023. [24] Tri Dao and Albert Gu. Transformers are SSMs: Generalized models and efficient algorithms through structured state space duality. In International Conference on Machine Learning (ICML), 2024. [25] Ji Lin, Chuang Gan, and Song Han. Tsm: Temporal shift module for efficient video understanding. In Proceedings of the IEEE International Conference on Computer Vision, 2019. [26] Limin Wang, Yuanjun Xiong, Zhe Wang, Yu Qiao, Dahua Lin, Xiaoou Tang, and Luc Van Gool. Temporal segment networks for action recognition in videos. IEEE transactions on pattern analysis and machine intelligence, 41(11):27402755, 2018. [27] Lucas Beyer, Andreas Steiner, André Susano Pinto, Alexander Kolesnikov, Xiao Wang, Daniel Salz, Maxim Neumann, Ibrahim Alabdulmohsin, Michael Tschannen, Emanuele Bugliarello, et al. Paligemma: versatile 3b vlm for transfer. arXiv preprint arXiv:2407.07726, 2024. [28] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin 12 Token-Efficient Long Video Understanding for Multimodal LLMs Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, and Junyang Lin. Qwen2vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv: 2409.12191, 2024. [40] Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, Limin Wang, and Yu Qiao. Mvbench: comprehensive multi-modal video understanding benchmark. Computer Vision and Pattern Recognition, 2023. [29] Bin Lin, Bin Zhu, Yang Ye, Munan Ning, Peng Jin, and Li Yuan. Video-llava: Learning united visual representation by alignment before projection. arXiv preprint arXiv:2311.10122, 2023. [30] Peng Jin, Ryuichi Takanobu, Caiwan Zhang, Xiaochun Cao, and Li Yuan. Chat-univi: Unified visual representation empowers large language models with image and video understanding. Computer Vision and Pattern Recognition, 2023. [41] Junjie Zhou, Yan Shu, Bo Zhao, Boya Wu, Shitao Xiao, Xi Yang, Yongping Xiong, Bo Zhang, Tiejun Huang, and Zheng Liu. Mlvu: comprehensive benchmark for multi-task long video understanding. arXiv preprint arXiv: 2406.04264, 2024. [42] Haoning Wu, Dongxu Li, Bei Chen, and Junnan Li. Longvideobench: benchmark for long-context interleaved video-language understanding. arXiv preprint arXiv: 2407.15754, 2024. [31] Lin Chen, Xilin Wei, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Bin Lin, Zhenyu Tang, et al. Sharegpt4video: Improving video understanding and generation with better captions. arXiv preprint arXiv:2406.04325, 2024. [43] Chaoyou Fu, Yuhan Dai, Yondong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, et al. Videomme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis. arXiv preprint arXiv:2405.21075, 2024. [32] Yuanhan Zhang, Bo Li, haotian Liu, Yong jae Lee, Liangke Gui, Di Fu, Jiashi Feng, Ziwei Liu, and Chunyuan Li. Llava-next: strong zero-shot video understanding model, April 2024. [33] Xidong Wang, Dingjie Song, Shunian Chen, Chen Zhang, and Benyou Wang. Longllava: Scaling multimodal llms to 1000 images efficiently via hybrid architecture. arXiv preprint arXiv: 2409.02889, 2024. [34] Jiabo Ye, Haiyang Xu, Haowei Liu, Anwen Hu, Ming Yan, Qi Qian, Ji Zhang, Fei Huang, and Jingren Zhou. mplug-owl3: Towards long image-sequence understanding in multi-modal large language models. arXiv preprint arXiv: 2408.04840, 2024. [35] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. [36] Zuyan Liu, Yuhao Dong, Ziwei Liu, Winston Hu, Jiwen Lu, and Yongming Rao. Oryx mllm: Ondemand spatial-temporal understanding at arbitrary resolution. arXiv preprint arXiv:2409.12961, 2024. [37] Bo Zhao, Boya Wu, Muyang He, and Tiejun Huang. Svit: Scaling up visual instruction tuning. ArXiv preprint, abs/2307.04087, 2023. [38] Yuanhan Zhang, Jinming Wu, Wei Li, Bo Li, Zejun Ma, Ziwei Liu, and Chunyuan Li. Video instruction tuning with synthetic data, 2024. [39] Karttikeya Mangalam, Raiymbek Akshulakov, and Jitendra Malik. Egoschema: diagnostic benchmark for very long-form video language understanding. Advances in Neural Information Processing Systems, 36:4621246244, 2023. [44] Shengbang Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Manoj Middepogu, Sai Charitha Akula, Jihan Yang, Shusheng Yang, Adithya Iyer, Xichen Pan, Austin Wang, Rob Fergus, Yann LeCun, and Saining Xie. Cambrian-1: fully open, vision-centric exploration of multimodal llms. arXiv preprint arXiv: 2406.16860, 2024. [45] Hugo Laurençon, Léo Tronchon, Matthieu Cord, and Victor Sanh. What matters when building visionlanguage models? arXiv preprint arXiv: 2405.02246, 2024. [46] Ruohong Zhang, Liangke Gui, Zhiqing Sun, Yihao Feng, Keyang Xu, Yuanhan Zhang, Di Fu, Chunyuan Li, Alexander Hauptmann, Yonatan Bisk, and Yiming Yang. Direct preference optimization of video large multimodal models from language model reward. arXiv preprint arXiv: 2404.01258, 2024. [47] Bin Zhu, Bin Lin, Munan Ning, Yang Yan, Jiaxi Cui, Hongfa Wang, Yatian Pang, Wenhao Jiang, Junwu Zhang, Zongwei Li, Wancai Zhang, Zhifeng Li, Wei Liu, and Liejie Yuan. Languagebind: Extending video-language pretraining to n-modality by language-based semantic alignment. International Conference on Learning Representations, 2023. [48] Jun Xu, Tao Mei, Ting Yao, and Yong Rui. MSRVTT: large video description dataset for bridging video and language. In 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016, pages 52885296. IEEE Computer Society, 2016. [49] Jonathan Krause, Justin Johnson, Ranjay Krishna, and Li Fei-Fei. hierarchical approach for generating descriptive image paragraphs. In 2017 IEEE Conference on Computer Vision and Pattern Recognition, 13 Token-Efficient Long Video Understanding for Multimodal LLMs CVPR 2017, Honolulu, HI, USA, July 21-26, 2017, pages 33373345. IEEE Computer Society, 2017. [50] Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua Lin. Sharegpt4v: Improving large multi-modal models with better captions. arXiv preprint arXiv: 2311.12793, 2023. [51] Justin Johnson, Bharath Hariharan, Laurens van der Maaten, Li Fei-Fei, C. Lawrence Zitnick, and Ross B. Girshick. CLEVR: diagnostic dataset for compositional language and elementary visual reasoning. In 2017 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017, Honolulu, HI, USA, July 21-26, 2017, pages 19881997. IEEE Computer Society, 2017. [52] Ryota Tanaka, Kyosuke Nishida, and Sen Yoshida. Visualmrc: Machine reading comprehension on document images. In Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Applications of Artificial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances in Artificial Intelligence, EAAI 2021, Virtual Event, February 2-9, 2021, pages 1387813888. AAAI Press, 2021. [53] Zhou Yu, Dejing Xu, Jun Yu, Ting Yu, Zhou Zhao, Yueting Zhuang, and Dacheng Tao. Activitynet-qa: dataset for understanding complex web videos via question answering. In The Thirty-Third AAAI Conference on Artificial Intelligence, AAAI 2019, The Thirty-First Innovative Applications of Artificial Intelligence Conference, IAAI 2019, The Ninth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2019, Honolulu, Hawaii, USA, January 27 - February 1, 2019, pages 91279134. AAAI Press, 2019. [54] Antoine Yang, Antoine Miech, Josef Sivic, Ivan Laptev, and Cordelia Schmid. Just ask: Learning to answer questions from millions of narrated videos. In 2021 IEEE/CVF International Conference on Computer Vision, ICCV 2021, Montreal, QC, Canada, October 10-17, 2021, pages 16661677. IEEE, 2021. [55] Jianhao Shen, Ye Yuan, Srbuhi Mirzoyan, Ming Zhang, and Chenguang Wang. Measuring visionlanguage stem skills of neural models, 2024. [56] Kushal Kafle, Brian L. Price, Scott Cohen, and Christopher Kanan. DVQA: understanding data visualizations via question answering. In 2018 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2018, Salt Lake City, UT, USA, June 18-22, 2018, pages 56485656. IEEE Computer Society, 2018. [57] Ali Furkan Biten, Rubèn Tito, Andrés Mafla, Lluís Gómez Bigorda, Marçal Rusiñol, C. V. Jawahar, Ernest Valveny, and Dimosthenis Karatzas. In 2019 Scene text visual question answering. IEEE/CVF International Conference on Computer Vision, ICCV 2019, Seoul, Korea (South), October 27 - November 2, 2019, pages 42904300. IEEE, 2019. [58] Geewook Kim, Teakgyu Hong, Moonbin Yim, JeongYeon Nam, Jinyoung Park, Jinyeong Yim, Wonseok Hwang, Sangdoo Yun, Dongyoon Han, and Seunghyun Park. Ocr-free document understanding transformer. In European Conference on Computer Vision (ECCV), 2022. [59] Jimmy Carter. Textocr-gpt4v. https: //huggingface.co/datasets/jimmycarter/ textocr-gpt4v, 2024. [60] Pan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, KaiWei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering. In The 36th Conference on Neural Information Processing Systems (NeurIPS), 2022. [61] Paul Lerner, Olivier Ferret, Camille Guinaudeau, Hervé Le Borgne, Romaric Besançon, José G. Moreno, and Jesús Lovón-Melgarejo. Viquae, dataset for knowledge-based visual question answering about named entities. In Enrique Amigó, Pablo Castells, Julio Gonzalo, Ben Carterette, J. Shane Culpepper, and Gabriella Kazai, editors, SIGIR 22: The 45th International ACM SIGIR Conference on Research and Development in Information Retrieval, Madrid, Spain, July 11 - 15, 2022, pages 31083120. ACM, 2022. [62] Abhishek Das, Satwik Kottur, Khushi Gupta, Avi Singh, Deshraj Yadav, José M. F. Moura, Devi Parikh, and Dhruv Batra. Visual dialog. In 2017 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017, Honolulu, HI, USA, July 21-26, 2017, pages 10801089. IEEE Computer Society, 2017. [63] Drew A. Hudson and Christopher D. Manning. GQA: new dataset for real-world visual reasoning and compositional question answering. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019, pages 67006709. Computer Vision Foundation / IEEE, 2019. [64] Ahmed Masry, Xuan Long Do, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. ChartQA: benchmark for question answering about charts with visual and logical reasoning. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio, editors, Findings of the Association for Computational Linguistics: ACL 2022, pages 22632279, Dublin, Ireland, 2022. Association for Computational Linguistics. [65] Jiahui Gao, Renjie Pi, Jipeng Zhang, Jiacheng Ye, Wanjun Zhong, Yufei Wang, Lanqing Hong, Jianhua Han, Hang Xu, Zhenguo Li, and Lingpeng Kong. Gllava: Solving geometric problem with multi-modal large language model, 2023. 14 Token-Efficient Long Video Understanding for Multimodal LLMs [66] Fuxiao Liu, Kevin Lin, Linjie Li, Jianfeng Wang, Yaser Yacoob, and Lijuan Wang. Mitigating hallucination in large multi-modal models via robust instruction tuning, 2023. [76] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llavanext: Improved reasoning, ocr, and world knowledge, January 2024. [67] Licheng Yu, Patrick Poirson, Shan Yang, Alexander C. Berg, and Tamara L. Berg. Modeling context in referring expressions, 2016. [68] Minesh Mathew, Dimosthenis Karatzas, and CV Jawahar. Docvqa: dataset for vqa on document images. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pages 22002209, 2021. [69] Jiaqi Chen, Jianheng Tang, Jinghui Qin, Xiaodan Liang, Lingbo Liu, Eric Xing, and Liang Lin. GeoQA: geometric question answering benchmark towards In Findings of multimodal numerical reasoning. the Association for Computational Linguistics: ACLIJCNLP 2021, pages 513523, Online, August 2021. Association for Computational Linguistics. [70] Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi. OK-VQA: visual question answering benchmark requiring external knowledge. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019, pages 31953204. Computer Vision Foundation / IEEE, 2019. [71] Aniruddha Kembhavi, Michael Salvato, Eric Kolve, Minjoon Seo, Hannaneh Hajishirzi, and Ali Farhadi. diagram is worth dozen images. ArXiv, abs/1603.07396, 2016. [72] Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang, Feng Zhu, and Rui Zhao. Shikra: Unleashing multimodal llms referential dialogue magic. ArXiv preprint, abs/2306.15195, 2023. [73] Tianyu Yu, Jinyi Hu, Yuan Yao, Haoye Zhang, Yue Zhao, Chongyi Wang, Shan Wang, Yinxv Pan, Jiao Xue, Dahai Li, et al. Reformulating visionlanguage foundation models and datasets towards universal multimodal assistants. arXiv preprint arXiv:2310.00653, 2023. [74] Fuxiao Liu, Kevin Lin, Linjie Li, Jianfeng Wang, Yaser Yacoob, and Lijuan Wang. Aligning large multi-modal model with robust instruction tuning. arXiv preprint arXiv:2306.14565, 2023. [75] Fuxiao Liu, Xiaoyang Wang, Wenlin Yao, Jianshu Chen, Kaiqiang Song, Sangwoo Cho, Yaser Yacoob, and Dong Yu. MMC: Advancing multimodal chart understanding with large-scale instruction tuning. In Kevin Duh, Helena Gomez, and Steven Bethard, editors, Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 12871310, Mexico City, Mexico, 2024. Association for Computational Linguistics. [77] Krishna Srinivasan, Karthik Raman, Jiecao Chen, Michael Bendersky, and Marc Najork. Wit: Wikipedia-based image text dataset for multimodal multilingual machine learning. In Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 21. ACM, July 2021. 15 Token-Efficient Long Video Understanding for Multimodal LLMs 7. Appendix 7.1. Qualitative Results We present comprehensive qualitative evaluations in Figure 9 to Figure 13, which are segmented into three subsections open-ended queries. Notably, our STORM with Temporal Sampling is also computationally more efficient. By applying temporal sampling, we reduce the number of tokens to the equivalent of processing 32 frames. This comparison showcases STORMs superior ability to leverage long video inputs for in-depth visual understanding. 1. Effective Long Video Understanding: Demonstrating STORMs ability to effectively utilize long video inputs by comparing it with existing long-video LLMs. 2. Importance of Long Video Context: Highlighting the need for long video inputs by showcasing scenarios where 128-frame inputs (with token compression) enable accurate predictions, whereas 32-frame inputs fail. 3. Showcase of Video Understanding Abilities: Illustrating STORMs capabilities in various aspects such as OCR, spatial perception, temporal reasoning, and so on. Effective Long Video Understanding. We compare our proposed STORM + Temporal Sampling with LongVILA and LongVU, both designed for long video understanding. We use short film depicting \"moonfall disaster\" from the VILA webpage . The models are prompted to provide narrative description of the video. The short film was chosen for its engaging and dramatic storyline that spans various interconnected scenarios, all contributing to cohesive narrative. Understanding this video requires the models to comprehend each individual scene and effectively integrate temporal events to grasp the complete story. Both STORM and LongVILA use 128 input frames, while LongVU output was obtained from its online demonstration which uses 1fps input. As shown in Figure 9, STORM delivers the most detailed and coherent summary of the videos narrative, effectively capturing key events and transitions throughout the entire film. Its response showcases comprehensive understanding of the content, highlighting its ability to connect temporal events across In contrast, the baseline models different scenes. LongVILA and LongVU focus on some of the events but fail to cover all critical moments that contribute to the overall storyline. Their responses also highlight specific scenes without integrating them into the full context. Moreover, we observed that the baseline models often generate redundant content, repeating the same sentences with minimal new information, which reveals their limitations in handling https://vila.mit.edu/ Importance of Long Video Context. We further demonstrate the significance of incorporating long video context by providing qualitative examples where 128-frame input yields more accurate predictions than 32-frame input, as shown in Figure 10. Using samples from the VideoMME benchmark, we compare two configurations of our STORM: one with 32frame input without compression, and another with 128-frame input employing temporal sampling ratio of 4. In both settings, the number of tokens fed into the LLM remains the same; however, the STORM with temporal sampling encodes additional information into the compressed tokens due to the extended frame sequence. The inclusion of more frames allows the model to capture richer temporal dynamics and contextual information. For example, the 128-frame input enables the model to develop stronger understanding of the videos narrative (Figure 10 top). It also allows the 128-frame model to capture additional events that the 32-frame model misses (Figure 10 center). Finally, the additional information further improve models ability to reason through different temporal events across the entire video to form coherent understanding (Figure 10 bottom). These example demonstrate the crucial role of long video context in tasks that require detailed temporal reasoning and comprehensive content understanding. Showcase of Video Understanding Abilities. Finally, we conclude our qualitative evaluation by showcasing the diverse video understanding capabilities of STORM, including OCR, attribute perception, spatial perception, information synopsis, and temporal reasoning. Results are shown in Figure 11 to Figure 13. We use the same setting of STORM + Temporal Sampling with 128-frame input and sampling ratio of 4. Utilizing videos from the VideoMME benchmark, we designed more challenging assessment to thoroughly evaluate the models proficiency. Instead of providing the model with multiple-choice questions accompanied by predefined answer options, we transformed these tasks into open-ended queries that require the model to generate answers in raw text form without any given choices. This modification significantly increases the tasks difficulty, as it 16 Token-Efficient Long Video Understanding for Multimodal LLMs demands precise understanding of the content and the ability to accurately locate and extract specific information from the video input. Our qualitative results demonstrate that STORM provides strong performance in these scenarios. Despite the increased complexity, the model effectively interprets intricate visual details, recognizes textual information within videos, and provides coherent summaries of temporal events. This showcases STORMs robust ability to handle various aspects of video understanding. 7.2. Additional Results Ablation on Token Budget and Token Compression Strategies. Table 8 extends Table 5 in the main text by providing comprehensive comparison of different compression method combinations across various token budgets during training. Overall, considering both compression ratio and inference latency, we find that STORM with temporal pooling (STORM + T. Pooling) is the most efficient and effective approach. Additionally, test-time temporal sampling offers lossless way to further enhance inference efficiency in inference time. Task-Level Analysis on VideoMME Table 11 shows the VideoMME results with different video lengths. The short is less than 2 minutes, the medium is up to 15 minutes, and the long is up to 60 minutes. Overall, our STORM with token compression outperforms the VILA baseline and STORM with no token compressions for all video lengths. Figure 7 compares the VideoMME results by task categories. We find that STORM with temporal pooling especially improves the object reasoning task accuracy, and STORM with test-time temporal sampling improves the attribute perception accuracy. Both token compression methods improve the temporal perception task accuracies compared to VILA and STORM. It indicates that the temporal perception task requires longer video context, and our token compression methods are effective for such tasks. Effect of Dataset Composition Table 10 shows how dataset composition affects model performance during 128 frames fine-tuning. We compare using the full LLaVA-Video dataset ( 1.35M samples) versus only its longest 25% videos with at least 128 frames ( 360K samples). Interestingly, while STORM improves with the larger dataset across all benchmarks, the baseline model actually performs worse on several benchmarks when trained on the full dataset."
        },
        {
            "title": "Two key differences between these datasets are",
            "content": "size and video length distribution, where the full dataset contains more data but with mixture of short and long videos, whereas the long-video subset exclusively consists of longer videos. Since larger, more diverse datasets typically improve performance (assuming similar data quality), we attribute the baseline models unexpected performance drop to its limited ability to generalize from shorter to longer videos. More specifically, when trained predominantly on shorter clips from the full dataset, the baseline overfits and can not effectively handle long contexts at inference time. Training solely on longer videos dataset variant better aligns with test conditions, partially addressing this limitation. In contrast, STORM shows consistent performance gains in all benchmarks when trained on the larger and more diverse data set. This suggests that STORM is more robust in handling longer sequences and is capable of using wide range of video lengths to enhance its overall performance. 7.3. Architecture Details STORM is built on standard multimodal pipeline but introduces key modifications for improved reasoning ability and token efficiency. Figure 6 illustrates the detailed composition of our models. Instead of an MLP projector, STORM uses linear layer followed by Mamba-based temporal module projector which integrates spatiotemporal information into visual tokens. STORM incorporates three main components: (1) The Mamba-Based Temporal Projector captures and propagates spatiotemporal information within visual tokens. (2) Temporal Token Compression Module applies compression on temporal dimension using training-based average pooling and/or training-free sampling (applied only at test time). (3) Spatial Token Compression further reduces token number by performing training-based frame-level spatial average pooling. Both spatial and temporal compression methodswhether training-free or trainingbasedare independently applicable. Notably, spatial and temporal pooling can be applied in parallel after the Mamba module, while temporal sampling is performed separately at test time. These components enable STORM to process longer sequences more efficiently before passing them to the LLM. 7.4. Datasets Information For all models, we begin with an alignment stage to align the multi-modal projector using the LLaVACC3M-Pretrain-595K dataset [1]. Following this, we proceed to the visual instruction fine-tuning stage, 17 Token-Efficient Long Video Understanding for Multimodal LLMs Figure 6 Breaking Down the STORM Architecture. We begin with standard multimodal pipeline that uses pixel-shuffle downsampling layer and an MLP projector. In STORM, we replace the MLP with linear layer and introduce our Mamba-based temporal module on top. Since the Mamba layer propagates spatiotemporal information in each visual tokens, the model can then perform temporal and spatial token compression of these tokens before passing them to the LLM, allowing STORM to handle longer sequences more efficiently. experimenting with two different training data mixtures. These SFT mixtures incorporate both image and video data, encompassing three task types: captioning, open-ended question answering, and multiplechoice question answering. Further details are provided in the following: SFT Data: For most of our main experiments, we construct an expanded mixture by incorporating additional high-quality image datasetssuch as Cambrian-1375K [44], Idefics2-SFT [45], and LLaVA-OneVision-Images-SFT [35]along with video datasets including M4-Instruct-Video [46] and Youtube [47]. This enlarged dataset is used to scale up training and enhance overall performance. Detailed compositions of these mixtures are provided in Table 7. Long Video SFT Data: In the long video finetuning stage, our goal is to adapt models initially trained on full SFT data with 32-frame inputs to handle 128-frame inputs. Because processing 128frame input incurs significant computational cost, we reduce training time by using smaller dataset at this stage. Specifically, we select videos from only the LLaVA-Video dataset [38] that contains data amount roughly 11% of the full dataset (approximately 1.35M video-text pairs). Long Video 25% Data: As an ablation study to investigate the impact of dataset composition in the long video fine-tuning stage, we introduce an additional dataset derived from the LLaVAVideo dataset [38]. This subset consists exclusively of long videos with at least 128 frames, comprising approximately 25% of the full dataset (around 360K video-text pairs). Unlike the Long Video SFT Data, which includes both short and long videos, this dataset contains only long videos. Our experiments in Section 7.2 and Table 10 reveal distinct behaviors in our models and baselines in the composition of the dataset. 7.5. Mamba Temporal Module Latency In this section, we compare the latencies of the vanilla VILA architecture and STORM across varying numbers of frames without token compression and provide breakdown of the percentage contribution of the multi-modal projector. All experiments are conducted on single NVIDIA DGX A100-80G. The results, shown in Figure 8, demonstrate that STORM incurs negligible overhead compared to the vanilla VILA architecture, with the introduced Mamba Temporal Module accounting for no more than 3% of the total latency. 7.6. Inference Details Table 13 summarizes the number of frames used for inference. We evaluate all models between 8 and 512 frames and select the number of frames with the best accuracy overall for each task and setup. 18 Token-Efficient Long Video Understanding for Multimodal LLMs"
        },
        {
            "title": "Datasets",
            "content": "LLaVA-SFT [1], Idefics2-SFT [45] MSR-VTT [48], Image Paragraph Captioning [49], ShareGPT4V-100K [50] CLEVR [51], NLVR, VisualMRC [52] ActivityNet-QA [53], LLaVA-OneVision-Images-SFT [35], iVQA [54], MSRVTT-QA, STEM-QA [55] DVQA [56], ST-VQA [57], SynthDoG-en [58], TextOCR-GPT4V [59], MTWI ScienceQA-train [60], VQAv2-train, ViQuAE [61], Visual Dialog [62], GQA-train [63], ChatQA [64], Geo170K [65], LRV-Instruction [66], RefCOCO-train [67], DocVQA [68], GeoQA [69], KVQA [70], Cambrian-1375K [44] AI2D [71], Shikra [72], Unimm-Chat [73] LRV-Instruction [74], SVIT [37], MMC-Instruction [75], M4-Instruct-Images [76], M4-Instruct-Video [46] , WIT [77], Youtube [47], etc Table 7 SFT data mixture. Models Duration Size Comp. Latency #Frames # Frames MVBench MLVU LongVideoBench VideoMME Ratio (%) (s) (train) (test) test dev val (w/o sub.) 16 sec 3120 min 8 sec60 min 160 min Token Budget: 8K VILA Baseline 7B STORM 7B STORM + S. Pooling 7B STORM + T. Pooling 7B 7B STORM + T. Sampling * STORM + S. Pooling + T. Sampling * 7B STORM + T. Pooling + T. Sampling * 7B Token budget: 2K STORM + S. Pooling STORM + T. Pooling STORM + S. Pooling + Sampling * STORM + T. Pooling + Sampling * Token budget: 0.5K STORM + S. Pooling + T. Pooling 7B 7B 7B 7B 7B * 2x additional compression at test time. 100 100 25 25 50 12.5 12. 25 25 12.5 12.5 4.38 4.26 1.82 1.82 2.50 1.51 1.51 1.82 1.82 1.51 1.51 6.25 1.36 32 32 128 128 32 128 32 32 32 32 32 256 256 256 256 256 256 256 256 256 256 256 256 69.5 70.3 63.9 71.3 70.1 65.2 70. 68.9 70.4 68.9 68.9 70.2 71.1 67.9 72.5 70.8 68.3 72.9 69.2 71.0 69.5 69.5 68.5 68.2 55.9 54.5 54.5 59.5 54.8 55.0 60. 56.0 54.2 56.3 56.3 53.7 60.1 62.5 57.5 63.4 63.1 57.6 62.4 61.1 61.2 60.9 60.9 60.2 Table 8 Ablation on Token Budget and Token Compression Strategies. Both spatial and temporal pooling are with 4 compression. The number of frames used during testing is consistent across models but can differ across tasks. The # frames is the maximum number of frames during testing. We summarize the details of the number of frames for each task in the appendix. The temporal token sampling is with 2 additional compression. 19 Token-Efficient Long Video Understanding for Multimodal LLMs Figure 7 VideoMME Results by Task Categories. Models 8F 32F (T. Pooling) 128F (T. Pooling) MVBench VILA (w/o Mamba) 67.9 STORM (w/ Mamba) 68.8 MLVU VILA (w/o Mamba) 67.7 STORM (w/ Mamba) 66.8 LongVidBench VILA (w/o Mamba) 52.4 STORM (w/ Mamba) 50.6 VideoMME VILA (w/o Mamba) 60.0 STORM (w/ Mamba) 60.2 Avg VILA (w/o Mamba) 62.0 STORM (w/ Mamba) 61. 68.7 70.4 71.0 71.0 55.4 54.2 58.9 61.2 63.5 64.2 68.1 71. 69.9 72.5 57.4 59.5 61.7 63.4 64.3 66.7 Table 9 Detailed Comparison across Benchmarks for Table 4. STORM consistently improves performance across all benchmarks as the input video length increases from 8F to 32F to 128F. In contrast, the baseline VILA exhibits diminishing gains with longer inputs and even experiences performance degradation on certain benchmarks when extending from 32F to 128F. These results highlight the critical role of the Mamba module in effectively leveraging long-video inputs to enhance model performance. Figure 8 Latency Comparison: VILA vs STORM. The multi-modal projector in VILA is 2-layer MLP, while it is the Mamba Temporal Module in STORM. 20 Token-Efficient Long Video Understanding for Multimodal LLMs Models Duration Dataset type test 16 sec VILA Baseline + T. Pooling VILA Baseline + T. Pooling long-video only (25% of full) full LLaVA-Video [38] 67.2 68.1 (+0.9) VILA Baseline + T. Pooling + T. Sampling * VILA Baseline + T. Pooling + T. Sampling * long-video only (25% of full) full LLaVA-Video [38] 64.5 67.8 (+3.3) 3120 min 8 sec60 min 71.4 69.9 (-1.5) 71.4 70.1 (-1.3) 59.2 57.4 (-1.8) 59.2 57.7 (-1.5) MVBench MLVU LongVideoBench VideoMME dev val (w/o sub.) STORM + T. Pooling STORM + T. Pooling long-video only (25% of full) full LLaVA-Video [38] 69.4 71.3 (+1.9) 71.7 72.5 (+0.8) 57.6 59.5 (+1.9) STORM + T. Pooling + T. Sampling * STORM + T. Pooling + T. Sampling * long-video only (25% of full) full LLaVA-Video [38] 68.8 70.6 (+1.8) 72.7 72.9 (+0.2) 59.2 60.1 (+0.9) * 2x additional compression at test time. 160 min 62.2 61.7 (-0.5) 61.0 59.3 (-1.7) 63.2 63.4 (+0.2) 62.6 62.4 (-0.2) Table 10 Effect of Dataset Composition on 128-frame Fine-Tuning. We compare models trained on two variants: the full LLaVA-Video dataset [38] (1.35M video-text pairs) versus the long-video only subset using top 25% longest videos (minimum of 128 frames) (360K pairs). Values in parentheses show performance differences between training on the long-video subset vs the full dataset. STORM consistently benefits from the larger, more diverse dataset across benchmarks, while the baseline VILA model degrades on several benchmarks when trained on the full dataset. Figure 9 Effective Long Video Understanding. We compare STORM + Temporal Sampling with existing long video LLMs. Reults show that STORM delivers more detailed and coherent summary, effectively capturing key events and transitions throughout the film. The example videos can be found in our website. 21 Token-Efficient Long Video Understanding for Multimodal LLMs Figure 10 Importance of Long Video Context. We compare STORM with 32-frame input to STORM + Temporal Sampling using 128-frame input. Both configurations have negligible differences in computational cost; however, the latter encodes additional information into compressed tokens due to the extended frame sequence. The examples illustrate that processing more frames allows the model to capture richer temporal dynamics and contextual information. This leads to stronger understanding of the videos narrative, reduces information loss, and enhances the ability to reason through temporal events across the entire video. The example videos can be found in our website. 22 Token-Efficient Long Video Understanding for Multimodal LLMs Figure 11 Showcase of Video Understanding Abilities in Various Task Categories. We provide additional examples to showcase models video understanding capabilities in different aspects. This is done by providing the models with open-ended queries that require the model to generate answers in raw text form without any given choices. Part 1. The example videos can be found in our website. 23 Token-Efficient Long Video Understanding for Multimodal LLMs Figure 12 Showcase of Video Understanding Abilities in Various Task Categories. Continue 2. The example videos can be found in our website. 24 Token-Efficient Long Video Understanding for Multimodal LLMs Figure 13 Showcase of Video Understanding Abilities in Various Task Categories. Continue 3. The example videos can be found in our website."
        },
        {
            "title": "Models",
            "content": "# frames Short Medium Long Avg. (train) < 2 min 415 min 3060 min Token Budget: 8K VILA Baseline STORM STORM + T. Sampling* STORM + T. Pooling STORM + T. Pooling + T. Sampling* * 2x additional compression at test time. 32 32 32 128 128 73.0 75.6 75.2 72.4 72. 58.0 60.9 60.8 64.4 60.9 49.2 51.1 53.2 53.4 53.4 60.1 62.5 63.1 63.4 62.4 Table 11 Break Donw of VideoMME Results by Input Video Length. 25 Token-Efficient Long Video Understanding for Multimodal LLMs # Frames Compression Ratio Overall (ms) 4 8 16 32 64 128 256 512 32 64 128 256 512 64 128 256 512 1 1 1 1 1 1 1 1 4 4 4 4 8 8 8 8 162.92 270.87 486.37 933.99 1910 4270 10340 28620 486.97 920.10 1800 3680 7950 772.18 1500 3000 6200 llm (ms) 103.80 174.61 321.73 623.41 1310 3090 7960 23710 175.75 322.22 622.29 1310 175.27 322.73 622.56 1310 vision_tower (ms) mm_projector (ms) 52.41 85.11 144.47 269.49 515.17 1020 2030 4090 269.96 515.82 1010 2020 4060 514.59 1020 2030 4070 6.71 11.15 20.17 41.09 82.41 163.34 348.22 811. 41.26 82.06 163.23 348.52 811.84 82.32 163.09 348.71 815.08 Table 12 Full Latencies on Various Compression Ratios and Input Frames. Models 8-frame-models + Temporal Sampling 32-frame-models + Temporal Sampling 32-frame-models + Temporal Pooling + Temporal Sampling MVBench MLVU LongVidBench VideoMME 8 16 16 32 32 64 64 64 256 64 256 32 256 64 256 128 256 64 64 128 64 128 Table 13 The Number of Frames Used for Inference. We evaluate all models for [8, 16, 32, 64, 128, 256, 512] frames and select the best overall for each task and setup."
        }
    ],
    "affiliations": [
        "KAIST",
        "MIT",
        "NVIDIA",
        "Nanjing University",
        "Rutgers University",
        "UC Berkeley"
    ]
}