{
    "paper_title": "MoCa: Modality-aware Continual Pre-training Makes Better Bidirectional Multimodal Embeddings",
    "authors": [
        "Haonan Chen",
        "Hong Liu",
        "Yuping Luo",
        "Liang Wang",
        "Nan Yang",
        "Furu Wei",
        "Zhicheng Dou"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Multimodal embedding models, built upon causal Vision Language Models (VLMs), have shown promise in various tasks. However, current approaches face three key limitations: the use of causal attention in VLM backbones is suboptimal for embedding tasks; scalability issues due to reliance on high-quality labeled paired data for contrastive learning; and limited diversity in training objectives and data. To address these issues, we propose MoCa, a two-stage framework for transforming pre-trained VLMs into effective bidirectional multimodal embedding models. The first stage, Modality-aware Continual Pre-training, introduces a joint reconstruction objective that simultaneously denoises interleaved text and image inputs, enhancing bidirectional context-aware reasoning. The second stage, Heterogeneous Contrastive Fine-tuning, leverages diverse, semantically rich multimodal data beyond simple image-caption pairs to enhance generalization and alignment. Our method addresses the stated limitations by introducing bidirectional attention through continual pre-training, scaling effectively with massive unlabeled datasets via joint reconstruction objectives, and utilizing diverse multimodal data for enhanced representation robustness. Experiments demonstrate that MoCa consistently improves performance across MMEB and ViDoRe-v2 benchmarks, achieving new state-of-the-art results, and exhibits strong scalability with both model size and training data on MMEB."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 2 ] . [ 1 5 1 1 3 2 . 6 0 5 2 : r MoCa: Modality-aware Continual Pre-training Makes Better Bidirectional Multimodal Embeddings Haonan Chen1, Hong Liu2, Yuping Luo, Liang Wang3, Nan Yang3, Furu Wei3, Zhicheng Dou1 1Gaoling School of Artificial Intelligence, Renmin University of China 2Stanford University 3Microsoft Corporation {hnchen,dou}@ruc.edu.cn hliu99@cs.stanford.edu, yupingl@cs.princeton.edu {wangliang,nanya,fuwei}@microsoft.com https://haon-chen.github.io/MoCa/"
        },
        {
            "title": "Abstract",
            "content": "Multimodal embedding models, built upon causal Vision Language Models (VLMs), have shown promise in various tasks. However, current approaches face three key limitations: the use of causal attention in VLM backbones is suboptimal for embedding tasks; scalability issues due to reliance on high-quality labeled paired data for contrastive learning; and limited diversity in training objectives and data. To address these issues, we propose MoCa, two-stage framework for transforming pre-trained VLMs into effective bidirectional multimodal embedding models. The first stage, Modality-aware Continual Pre-training, introduces joint reconstruction objective that simultaneously denoises interleaved text and image inputs, enhancing bidirectional context-aware reasoning. The second stage, Heterogeneous Contrastive Fine-tuning, leverages diverse, semantically rich multimodal data beyond simple image-caption pairs to enhance generalization and alignment. Our method addresses the stated limitations by introducing bidirectional attention through continual pre-training, scaling effectively with massive unlabeled datasets via joint reconstruction objectives, and utilizing diverse multimodal data for enhanced representation robustness. Experiments demonstrate that MoCa consistently improves performance across MMEB and ViDoRe-v2 benchmarks, achieving new state-of-the-art results, and exhibits strong scalability with both model size and training data on MMEB."
        },
        {
            "title": "Introduction",
            "content": "Multimodal embedding models have achieved significant improvements in various tasks including multimodal classification, visual question answering, and document retrieval [16, 49, 5, 28]. These models are often built on Vision Language Models (VLMs), such as Phi-V [1], LLaVA [27], and Qwen-VL [2], which demonstrate strong generation and comprehension capabilities across modalities. Recent methods, including VLM2Vec [16] and mmE5 [5], apply contrastive learning on image-text pairs to off-the-shelf VLMs to align modalities and improve cross-modal representation quality. Despite the growing success of multimodal embedding models, three main limitations remain in current approaches: (1) Causal attention of pre-trained VLMs might be suboptimal for embedding models. Mainstream multimodal embedding models [16, 5, 49] inherit causal attention from their VLM backbones [30, 27, 1, 40]. However, studies on text embedding models [4, 25, Work done during Haonans internship at Microsoft Research Asia. Technical report. Figure 1: Comparison of VLM-based multimodal embedding models. Left: Previous singlestage contrastive learning with mainly image-caption pairs and causal attention. Right: MoCa. In modality-aware continual pre-training, we optimize joint bidirectional reconstruction objective to denoise interleaved texts and images simultaneously. In heterogeneous contrastive fine-tuning, we train models to improve cross-modal fusion of the backbone with diverse image and text contexts. 21, 35] have shown that bidirectional attention typically produces superior embeddings compared to causal attention. This finding is reinforced by seminal multimodal works such as CLIP [31] and SigLIP [47], which adopted bidirectional encoders after extensive architectural exploration. Furthermore, bidirectional embedding models with mean pooling offer practical benefits, such as late chunking [12]. This suggests strong need to evaluate bidirectional architectures for multimodal embedding models. (2) Contrastive learning is hard to scale without labeled pair data. Contrastive learning fundamentally depends on diverse high-quality image-text pairs, which limits its scalability. Although large datasets of image-caption pairs exist [33, 38], curating diverse and high-quality multimodal pairs remains resource-intensive and hard to scale. Moreover, contrastive learning cannot leverage the vast amount of unpaired multimodal data available on the internet. These limitations underscore the need for more scalable training paradigms that can effectively utilize diverse, unlabeled multimodal data. (3) Lack of diversity in training objectives and data distribution leads to suboptimal cross-modal alignment. Prior works such as Jiang et al. [16], Chen et al. [5] typically fine-tune Vision-Language Models (VLMs) with single contrastive objective applied to narrow range of data, i.e., mostly short image-caption pairs. This setup fails to fully exploit the rich cross-modal reasoning capabilities that pre-trained VLMs offer. While such training yields aligned embeddings, it encourages alignment based primarily on surface-level similarity, rather than deeper semantic or contextual understanding. As result, the learned embedding models often overfit to the training distribution and struggle to generalize to more complex or diverse multimodal scenarios. This suggests the need for more expressive training objectives that explicitly promote cross-modal reasoning over heterogeneous data. Recent advances in text embedding models, such as LLM2Vec [4], adapt pre-trained language models using bidirectional Masked Language Modeling (MLM) [9]. Despite the success of Continual PreTraining (CPT) in text-only domains, its potential remains underexplored for multimodal embeddings. Moreover, as shown in Section 3.3, MLM alone is insufficient for handling mixed-modality inputs, motivating the need for modality-aware, bidirectional objectives that can jointly model interleaved image and text signals. In this work, we introduce two-stage framework, MoCa, to transform pre-trained VLMs into effective bidirectional multimodal embedding models. As illustrated in Figure 1, our approach consists of two stages: (1) Modality-aware Continual Pre-training and (2) Heterogeneous Contrastive Fine-tuning. In the first stage, we introduce joint reconstruction objective that requires the model to simultaneously denoise interleaved text and image inputs, which encourages the model to jointly reason across modalities. For text, we apply masked language modeling (MLM), where masked tokens are predicted using the full multimodal context. For images, we adopt masked autoencoding (MAE): subset of image patches are randomly masked and reconstructed by lightweight decoder conditioned on image and text contexts. In the second stage, as opposed to previous works which used mainly image-caption pairs, we add diverse heterogeneous data including (i) long-form querydocument pairs, supporting document-level understanding and complex reasoning over extended context, (ii) curated multimodal pairs, offering various visual and textual contexts beyond image captions of specific distributions, and (iii) real-world text pairs, enhancing linguistic representations across diverse domains. 2 Together, the two stages directly address the limitations outlined above. Stage one tackles Limitations (1) and (2) by using massive unlabeled interleaved data and enhancing bidirectional, context-aware reasoning across modalities. It also partially mitigates Limitation (3) by applying joint reconstruction on diverse multimodal inputs. Stage two directly addresses Limitation (3) by introducing heterogeneous and semantically rich multimodal pairs to enhance generalization and alignment across various domains. We conduct experiments with MoCa and verify that our trained model consistently improves performance across MMEB [16] and ViDoRe-v2 [10] benchmarks. Besides, it demonstrates strong scalability with respect to both model size and training data on MMEB. Specifically, after continual pre-training on only 30B tokens, our 3B model matches or surpasses the performance of competitive 7B baselines. When scaled to 7B parameters, our model sets new state-of-the-art results on MMEB. Furthermore, the performance improves steadily as the corpus for CPT grows, indicating the frameworks effectiveness in leveraging more and broader unlabeled multimodal data. In summary, our contributions are as follows. We are the first to propose continual pre-training (CPT) approach with unlabeled data to adapt VLMs to bidirectional embedding models and demonstrate its strong scalability with respect to model and corpus sizes. We also show that contrastive fine-tuning with heterogeneous data and cross-modal interactions enhances model generalization. With the two techniques combined, our framework, MoCa, consistently improves performance on various benchmarks and achieves state-of-the-art performance on MMEB."
        },
        {
            "title": "2 Method: MoCa",
            "content": "In this section, we present MoCa, which transforms pre-trained vision language models (VLMs) into powerful bidirectional multimodal embedding models. As illustrated in Figure 2, our method comprises two stages: (1) Modality-aware Continual Pre-training, where the model learns to reconstruct masked textual and visual inputs with joint denoising objective. The reconstruction objective leverages masked language modeling (MLM) and masked autoencoding (MAE), which enables the model to shift from causal to bidirectional attention with better representation quality. Moreover, learning to jointly reconstruct images from text and text from images enhances crossmodal alignment and reasoning. (2) Heterogeneous Contrastive Fine-tuning, where we perform contrastive fine-tuning with diverse set of multimodal pairs spanning long-form query-document pairs, curated multimodal pairs and real-world text pairs. This stage further aligns vision and language embeddings while improving generalization across varied real-world tasks. Together, the two stages enable VLMs to produce robust representations suitable for wide range of downstream applications. 2.1 Preliminaries Vision Language Model Backbones. Vision language models (VLMs) are widely adopted as the backbones of multimodal embedding models [16, 49, 5]. Consider multimodal input of length , denoted by = [x1, . . . , xT ], where each xi, [T ], can be either discrete text token or continuous image patch. VLM backbone first maps the input to the same input embedding space with input embedding layers for text tokens and visual encoders for image patches. The input embeddings are then passed through the backbone transformer to obtain series of hidden states causal () refers to the VLM backbone θ with causal attention. (x) RT where θ denotes the model parameters, and causal θ Multimodal embedding models. Jiang et al. [16], Zhang et al. [49], Chen et al. [5] inherit the causal attention from VLM backbones. Therefore, (f causal (x))j only depends on [x1, . . . , xj]. To extract embeddings from the backbone, natural choice would be the hidden states corresponding to the last (EOS) token, i.e. Embcausal (x) := (f causal θ θ (x))T . θ Despite its practical success, text embedding studies [4, 25, 21, 35] found that causal embedding models are worse than bidirectional ones in terms of representation quality. To this end, we introduce bidirectional VLM backbone bi θ (), which essentially removes the attention causal masks from 3 Figure 2: MoCa. (1) In modality-aware continual pre-training, the VLM backbone is trained to jointly reconstruct masked texts and images based on interleaved multimodal context with masked language modeling and masked autoencoding, respectively. (2) In heterogeneous contrastive fine-tuning, the VLM backbone from the previous stage is further fine-tuned with contrastive loss on broad range of heterogeneous data. For each query, we curate positive documents, hard negative documents. We apply task batching to make sure in-batch negatives come from the same task. (). To extract embeddings from bi θ (), we adopt mean pooling [32]. Therefore we have causal θ Embbi θ (x) := 1 T (cid:80) i=1 (f bi θ (x))i. Contrastive learning. Based on the causal multimodal embedding models initialized with pretrained VLMs above, previous works follow the recipe of text embedding models [39, 25, 21, 44] to align image and text embeddings. Each training example is tuple (q, d+, {d K}), where is the query, d+ is positive document, and {d K} is set of hard negative documents. 1 , . . . , 1 , . . . , 2.2 Modality-aware Continual Pre-training This stage aims to enhance the bidirectional representation capabilities of pre-trained VLM by joint denoising objective over both textual and visual inputs. As shown in the left part of Figure 2, we incorporate two complementary objectives: masked language modeling (MLM) [9] for texts and masked autoencoding (MAE) [14] for images. Masked Language Modeling. We apply MLM to the text tokens in the input x. Specifically, we randomly sample subset of text tokens MLM {1, . . . , } and replace each xi, MLM, with special mask token <mask>. Note that compared to the original input x, only selected text tokens xi, MLM, are changed to <mask>. Other text tokens and image patches are left unchanged in text masking. The resulting input with masked text tokens is denoted as xMLM. The VLM encoder then processes the sequence with masked text tokens with bidirectional attention, enabling each masked position to attend to all visible text tokens and image patches. This facilitates contextual learning that captures both intra-modal and cross-modal dependencies. The model is trained to accurately predict each masked token xi, MLM, based on the surrounding context. Following BehnamGhader et al. [4], we predict the masked token at position using the representation of the previous token 1 (i.e., shift the labels) to align with the training recipe of most autoregressive VLMs. Suppose the MLM prediction head is gϕ() parameterized by ϕ. The MLM loss is then computed with cross-entropy over the masked positions: LMLM(θ, ϕ; xMLM) = (cid:88) T MLM ℓCE((pθ,ϕ(xMLM))i1, xi), (1) where (pθ,ϕ(xMLM))i1 := gϕ((fbi,θ(xMLM))i1) is the models predicted distribution over the vocabulary at position i. This objective encourages the model to leverage both local and global contexts to recover masked information, enhancing its ability as bidirectional encoder for embedding tasks. Masked Autoencoding. Similar to MLM on text tokens, we want denoising objective on image patches to reconstruct image patches based on text and image context. Inspired by He et al. [14], we mask image tokens and feed them to the bidirectional VLM backbone. On top of the VLM hidden states, we use light-weight decoder to predict the original patches. Concretely, given input with masked text tokens xMLM, we further randomly sample subset of image patches MAE {1, . . . , } and replace each xi, MAE, with vectors sampled from unit Gaussian distribution. The resulting input with masked text and images is denoted as xMoCa. The model is trained to accurately predict each masked patch xi, MAE, based on the surrounding multimodal context. Following He et al. [14], we add shallow transformer hψ as the image patch decoder on top of the VLM encoder. The MAE loss is then computed with MSE over the masked patches: LMAE(θ, ψ; xMoCa) = ℓMSE(hψ(fbi,θ(xMoCa))i, xi). (2) (cid:88) T MAE The final modality-aware continual pre-training objective is weighted sum of MLM and MAE objective on the sequence with masked texts and images, i.e., LMoCa(θ, ϕ, ψ; xMoCa) = LMLM(θ, ϕ; xMoCa) + wLMAE(θ, ψ; xMoCa), where is the weight to balance these two losses2. Efficient implementation. In practice, we use data parallel to distribute workloads across multiple GPUs. To achieve load balancing, we calculate the compute cost of each sequence based on sequence length and image sizes and implement sequence packing algorithm to make sure all GPUs process batch of sequences with almost the same compute cost. 2.3 Heterogeneous Contrastive Fine-tuning Following modality-aware continual pre-training, we fine-tune the bidirectional embedding model with contrastive objective over broad range of heterogeneous data. Unlike prior methods [31, 16, 5] that primarily rely on image-caption pairs, our approach leverages more diverse sources to improve generalization and robustness. As shown in the right part of Figure 2, this includes: (1) Long-form multimodal pairs, which consist of document-level inputs containing both images and extended text, from VisRAG [46] and ViDoRe [10]. These examples support complex cross-modal reasoning and coherence over long contexts. (2) Curated multimodal pairs drawn from datasets such as MMEB [16] and mmE5 [5], which offer varied and high-quality alignments beyond typical captioning scenarios. (3) Text-only pairs, sampled from large-scale retrieval datasets like E5 [39], which enhance the models ability to encode fine-grained semantic differences in language. Each training instance is structured as tuple = (q, d+, {d K}), where the query q, the positive document d+, and the hard negative documents {d K} may be either unimodal (text or image) or interleaved multimodal (text-image). Note that in contrastive learning, documents of other instances in the same batch, including both positive documents and negative documents, are also used as in-batch negatives, denoted by Nin(z). Denote by q, the embeddings of q, d, respectively, i.e., = Embbi θ (d). The contrastive loss is then defined as: θ (q) and = Embbi 1 , . . . , 1 , . . . , LCL(θ; z) = log Φ(q, d+) + (cid:80) k=1 Φ(q, d+) Φ(q, ) + (cid:80) dNin(z) , Φ(q, d) (3) where Φ(, ) is similarity function defined as: Φ(a, b) = exp (cos(a, b)/τ ), with cos(, ) denoting cosine similarity and τ temperature hyperparameter. This objective encourages the model to assign higher similarity scores to positive pairs (q, d+) while suppressing scores for negative pairs (q, ), resulting in semantically meaningful and discriminative embeddings across modalities and domains. 2MLM and MAE loss are calculated on the same input xMoCa, which contains masked image patches and masked text tokens. Task-aware batching. In Heterogeneous Contrastive Fine-tuning, documents from different tasks can vary significantly. For example, photograph of puppy is vastly different from screenshot of an arXiv paper. This stark difference can make distinguishing them trivial, which diminishes the benefit of in-batch negatives in the contrastive loss. To create more effective in-batch negative sampling strategy for heterogeneous pairs, we adopt task-aware batching [25]. By ensuring that all instances within the same batch originate from the same task, we enable harder in-batch negative samples, leading to improved representation quality."
        },
        {
            "title": "3 Experiments",
            "content": "3.1 Experimental Setup 3.1.1 Modality-aware Continual Pre-training Stage Training Data. We incorporate three categories of training corpora as the CPT dataset: (1) Text-only data from DCLM [22], (2) Common image-text pairs from PixelProse [34] (CommonPool, CC12M, and RedCaps), MAmmoTH-VL-Instruct [13], and MMEB training set [16], and (3) Document-level multimodal data from DocMatix [20], VisRAG [46], and ColPali training set [10]. For each dataset, we randomly sample 500K instances, resulting in total corpus of approximately 30B tokens. Implementation Details. We adopt Qwen-2.5-VL [2] as the VLM backbone. Training is conducted with maximum input sequence length of 2048 tokens and micro-batch size of 12,800 across 32 NVIDIA H100 GPUs (80GB each). The learning rate is set to 2 106. For the denoising objectives, we apply masked language modeling (MLM) probability of 0.4 for MoCa-3B and 0.6 for MoCa-7B. For masked image reconstruction, we use masking ratio of 0.5 for MoCa-3B and 0.6 for MoCa-7B. The lightweight image decoder for MAE (hψ) is initialized from the middle layer of the Qwen-2.5-VL backbone to improve loss stability. The MAE loss LMAE is weighted by = 0.5 to balance with the MLM loss LMLM in the total loss function. 3.1.2 Heterogeneous Contrastive Learning Stage Training Data. We use three categories of datasets for contrastive learning: (1) Long-form multimodal pairs from VisRAG [46] and the ViDoRe training set [10], (2) Common multimodal pairs from the training sets of MMEB [16] and mmE5 [5], and (3) Text-only pairs from the large-scale dense retrieval dataset E5 [39]. From each dataset, we randomly sample 50K instances, resulting in total of approximately 2M contrastive training pairs. Implementation Details. We initialize the model from the checkpoint obtained after modality-aware CPT. Training is performed with batch size of 2048. We apply task-aware batching [25] to ensure that query-document pairs from the same dataset (task) are grouped within each batch. The learning rate is set to 1 105, and the temperature parameter τ in the contrastive loss is fixed at 0.03. To improve the training signal, each positive pair is accompanied by two hard negative pairs. 3.1.3 Evaluation Massive Multimodal Embedding Benchmark (MMEB). We assess the general embedding quality of our model using the MMEB benchmark [16], reporting results with Precision@1. MMEB includes 36 multimodal tasks across four types: 10 classification tasks, 10 visual question answering (VQA) tasks, 12 retrieval tasks, and 4 visual grounding tasks. Visual Document Retrieval Benchmarks (ViDoRe-v2). We evaluate our models performance on visually rich document retrieval by NDCG@5 using the ViDoRe-v2 benchmark. ViDoRe [10] is designed to test retrieval systems across diverse document types, tasks, languages, and settings. The v2 version follows the BEIR [37] evaluation format and extends ViDoRe-v1 to include multilingual and more generalized retrieval settings. 3.2 Overall Results We present the overall multimodal embedding performance on MMEB in Table 1 and the results on long-form document-level retrieval from ViDoRe-v2 in Table 2. Our model, MoCa, consistently outperforms all strong baselines on both benchmarks, demonstrating the effectiveness of our proposed 6 Table 1: MMEB results, which includes 36 tasks across four categories: Classification, Visual Question Answering (VQA), Retrieval, and Visual Grounding. In addition to existing baselines, we evaluate three variants of Qwen-2.5-VL with different model sizes and attention mechanisms. We highlight the best scores in bold and the second-best scores with an underline. Per Meta-Task Score Average Score Classification VQA Retrieval Grounding IND OOD Overall Models Size Existing Baselines CLIP [31] BLIP2 [24] OpenCLIP [8] SigLIP [47] MagicLens [48] UniIR [42] MM-EMBED [26] GME [49] VLM2Vec [16] MMRet [51] mmE5 [5] 428M 428M 428M 652M 613M 428M 7B 7B 7B 7B 11B 55.2 27.0 56.0 40.3 38.8 42.1 48.1 56.9 61.2 56.0 67.6 19.7 4.2 21.9 8.4 8.3 15.0 32.2 41.2 49.9 57.4 62. 53.2 33.9 55.4 31.6 35.4 60.1 63.8 67.8 67.4 69.9 71.0 62.2 47.0 64.1 59.5 26.0 62.2 57.8 53.4 86.1 83.6 89.7 Variants of Qwen-2.5-VL with only Contrastive Learning on MMEB Training Set causal attn. bidirectional attn. bidirectional attn. 63.8 60.6 62.2 59.8 59.1 60.5 68.2 68.7 70. 83.8 83.4 85.6 3B 3B 7B Ours MoCa-3B MoCa-7B 3B 7B 59.8 65.8 62.9 64. 70.6 75.0 88.6 92.4 47.6 50.5 67.5 68.0 72.4 72.7 71.7 72.6 72.3 74.7 42.8 43.1 57.1 59.1 66. 58.5 57.6 60.1 61.5 67.6 45.4 25.2 47.2 34.8 27.8 42.8 50.0 55.8 62.9 64.1 69.8 66.4 65.4 67.1 67.5 71.5 Table 2: The results on the ViDoRe-v2 benchmark, which includes 7 tasks. Syn denotes synthetic data, Mul indicates multilingual tasks, and Bio refers to biomedical domains. The best results are shown in bold, while the second-best are underlined. Models Size ESG_Human Eco_Mul Bio ESG_Syn ESG_Syn_Mul Bio_Mul Eco Avg. Existing Baselines SigLIP [47] VLM2Vec [16] VisRAG-Ret [46] vdr-multi-v13 GME [49] mmE5 [5] 652M 7B 3B 2B 7B 11B Ours MoCa-3B MoCa-7B 3B 7B 28.8 33.9 53.7 63.1 65.8 52.8 63.3 58.8 14.0 42.0 48.7 52.8 56.2 44.3 33.8 38.8 54.8 60.6 64.0 51.3 57.3 57.6 62.5 63. 19.8 36.7 45.9 50.3 54.3 55.1 58.3 55.3 21.9 38.4 46.4 51.2 56.7 54.7 54.8 51.4 18.2 29.7 47.7 56.9 55.1 46.8 29.8 23.8 51.4 38.7 59.6 51.0 61.2 56.6 62.9 59.3 48.6 50. 59.8 61.3 62.8 59.8 63.8 58.8 framework. Several key observations can be drawn from the results: (1) The combination of VLM backbone with bidirectional attention, Continual Pre-training (CPT), and heterogeneous Contrastive Learning (CL) consistently yields superior performance. Specifically, the configuration bidirectional + CPT + CL outperforms both causal + CL and bidirectional + CL (without CPT). This confirms the importance of modality-aware pre-training in unlocking the full potential of bidirectional architectures. (2) After continual pre-training on 30B tokens, our 3B model with bidirectional adaptation surpasses 7B baselines trained only with contrastive learning. (3) Scaling our approach to 7B model leads to substantial improvements across all MMEB task categories, establishing new state-of-the-art results on MMEB. (4) Although the 7B model performs better on MMEB overall, the 3B model achieves slightly higher results on ViDoRe-v2. This is likely because ViDoRe-v2 includes fewer training and evaluation samples, and the smaller model is less prone to overfitting in such low-resource settings. Across the full MMEB benchmark, however, larger models show consistent improvements, confirming that our method scales well with model size. 7 Table 3: Performances of ablated models on both benchmarks. We evaluate the contribution of each component in our framework by removing key elements from the modality-aware continual pretraining stage and the heterogeneous contrastive fine-tuning stage."
        },
        {
            "title": "Model",
            "content": "MoCa-3B MMEB Vidore-v2 67.5 59.8 Modality-aware Continual Pre-training Stage w/o. MLM w/o. MAE w/o. MLM & MAE, i.e., no Continual Pre-training Heterogeneous Contrastive Fine-tuning Stage w/o. Text-only Pairs w/o. Long-form Document Retrieval Pairs w/o. Task-aware Batching 66.2 66.8 65.8 67.1 66.9 67.2 57.2 56.9 56.2 59.2 45.7 58. 3.3 Ablation Study To understand the contribution of each major design choice in our framework, we conduct ablation studies on both the Modality-aware Continual Pre-training and Heterogeneous Contrastive Finetuning stages. As shown in Table 3, removing any key component leads to consistent performance drop on both benchmarks (MMEB and Vidore-v2), demonstrating the importance of each part. Modality-aware Continual Pre-training. The ablation of either the Masked Language Modeling (MLM) or the Masked Autoencoding (MAE) objective results in performance decline, indicating that both text and image reconstruction are essential for learning modality-specific representations. Disabling both yields the largest decline, demonstrating the joint effectiveness between MLM and MAE in training robust bidirectional embedding model. Heterogeneous Contrastive Fine-tuning. We then evaluate the effect of different types of training data used during contrastive fine-tuning. Removing text-only pairs results in noticeable drop, showing their importance for maintaining strong language representations. Excluding long-form document retrieval pairs (VisRAG and the training set of ColPali) also hurts performance, especially on ViDoRe, demonstrating their value in supporting deeper reasoning over extended contexts. Finally, removing the task-aware batching technique, where data from different tasks are jointly trained in each batch, leads to performance degradation. This technique helps prevent the model from overfitting to task-specific patterns and encourages it to learn more sample-discriminative representations, which is crucial for generalization across diverse task formats. 3.4 Data Scaling Effect for Continual Pre-training To evaluate the impact of data scale on the effectiveness of continual pre-training (CPT), we analyze how downstream performance changes with increasing CPT steps. Specifically, we perform contrastive learning (CL) using checkpoints saved at different stages of single CPT run for both 3B and 7B models. As shown in Figure 3, downstream performance on MMEB improves consistently as the number of CPT steps increases. Notably, after approximately 2,200 steps (corresponding to 20B tokens), the 3B model achieves performance on par with the 7B baseline trained without CPT. This demonstrates that modality-aware continual pre-training substantially enhances the quality of bidirectional representations, which in turn improves alignment during the CL stage. While constrained by computational resources, our findings suggest that further scaling of CPT with more data and training steps can continue to improve model performance. This insight provides practical guidance for balancing training cost with expected gains in future work. 3.5 Hyperparameter Analysis of Continual Pre-training To further understand the training process of the CPT stage, we conduct experiments of hyperparameter analysis and present the results in Figure 4. We evaluate the performance of MoCa (3B) on 8 Figure 3: Scaling effect of our CPT stage on downstream performance. We evaluate MMEB performance after CL using checkpoints (left: 3B, right: 7B) from different steps of CPT. Figure 4: The performances of MoCa (3B) with different CPT settings on MMEB. MMEB using models trained with fixed amount of data. We select hyperparameter values based on performance on validation sets, each containing 1K samples drawn from the corresponding training data. To maintain consistency with earlier experiments, we report the results on the MMEB test set. Mask Ratio. We examine the effect of different masking probabilities for both masked language modeling (MLM) and masked autoencoding (MAE). Increasing the mask ratio generally encourages the model to rely more on contextual signals across modalities, but overly high ratios can lead to degraded learning due to excessive information removal and low signal-to-noise ratio. Our experiments show that moderate masking rates (MLM: 40%, MAE: 50%) strike good balance, enabling strong cross-modal reasoning without making the model to forget relevant input tokens or patches. Loss Weight. We also study the weight assigned to the MAE loss relative to MLM in the overall CPT objective. balanced combination is crucial: if the MAE loss is underweighted, the model may neglect visual reconstruction and fail to integrate visual semantics effectively; if overweighted, it may distort the language modeling objective. We find that setting the MAE loss weight to 0.5 provides the best trade-off, aligning well with both visual and textual learning objectives. Learning Rate. The learning rate is critical factor influencing the stability and convergence speed of continual pretraining. lower learning rate can lead to underfitting, especially in early training stages, while an overly large learning rate may disrupt the pre-trained knowledge and destabilize training. Through empirical tuning, we find that learning rate of 2 106 provides stable optimization process, allowing the model to adapt effectively to the new denoising objectives without catastrophic forgetting."
        },
        {
            "title": "4 Related Work",
            "content": "Multimodal Embedding. Multimodal embedding aims to represent inputs from different modalities in shared space to support cross-modal understanding and interaction. Early models in this area include ALIGN [15], BLIP [23], and CLIP [31], which adopt dual-encoder architectures. These models encode each modality separately and align their outputs using contrastive learning. Recent work builds on stronger vision-language models (VLMs). For example, VLM2Vec [16] is based on Phi-3.5-V [1], GME [49] is built on LLaVA [27], and mmE5 [5] uses mLLaMA [30]. Some 9 methods also improve the fine-tuning stage. LLaVE [19], for instance, applies hard negative mining and task-aware batch sampling to improve alignment. However, most of these approaches still rely on causal models and do not explore the advantages of bidirectional architectures. We argue that, with appropriate adaptation, bidirectional VLMs can produce stronger and more robust multimodal embeddings. Continual Pre-training for Multimodal Models. Continual pre-training (CPT) involves further training of pre-trained models using additional data or new objectives to improve performance or adapt to specific downstream tasks [17]. In multimodal learning, models such as LXMERT [36] and UNITER [7] apply Masked Language Modeling (MLM) during pre-training to jointly learn image-text representations, often using relatively small transformer architectures. Another line of work explores CPT using reconstruction-based objectives in multimodal settings [18, 43, 29, 6, 11, 50, 3, 41, 45]. For example, ViLT [18] uses both MLM and Image-Text Matching (ITM) during training. More recently, the Janus models [43, 29, 6] from DeepSeek apply reconstruction losses to both text tokens and image pixels. Motivated by these prior efforts, we propose CPT framework that adapts VLMs into strong bidirectional multimodal embedding models through modality-aware pre-training strategies."
        },
        {
            "title": "5 Conclusion",
            "content": "In this paper, we have proposed two-stage framework for multimodal embeddings, combining modality-aware continual pre-training and heterogeneous contrastive fine-tuning. Our method MoCa leverages bidirectional attention mechanisms and joint reconstruction objectives to enhance crossmodal interactions. Additionally, by incorporating diverse and extensive multimodal data, our framework significantly improves the robustness and generalization of embedding models. Experimental results show that our approach achieves state-of-the-art performance, demonstrating strong scalability with respect to both model and data size on MMEB. This study shows the effectiveness and potential of continual pre-training strategies for advancing multimodal embedding research. However, several directions remain for future exploration. First, extending continual pre-training to incorporate more diverse modalities such as video, speech, or structured data could further enhance the generality of learned embeddings. Second, investigating more advanced denoising objectives or unified encoder-decoder architectures may improve the efficiency and representation quality of bidirectional multimodal models. Third, evaluating on more complex real-world applications, such as multi-hop retrieval or interleaved-inputs scenarios, would further validate the robustness of MoCa."
        },
        {
            "title": "References",
            "content": "[1] Marah Abdin, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harkirat S. Behl, Alon Benhaim, Misha Bilenko, Johan Bjorck, Sébastien Bubeck, Martin Cai, Caio César Teodoro Mendes, Weizhu Chen, Vishrav Chaudhary, Parul Chopra, Allie Del Giorno, Gustavo de Rosa, Matthew Dixon, Ronen Eldan, Dan Iter, Amit Garg, Abhishek Goswami, Suriya Gunasekar, Emman Haider, Junheng Hao, Russell J. Hewett, Jamie Huynh, Mojan Javaheripi, Xin Jin, Piero Kauffmann, Nikos Karampatziakis, Dongwoo Kim, Mahoud Khademi, Lev Kurilenko, James R. Lee, Yin Tat Lee, Yuanzhi Li, Chen Liang, Weishung Liu, Eric Lin, Zeqi Lin, Piyush Madan, Arindam Mitra, Hardik Modi, Anh Nguyen, Brandon Norick, Barun Patra, Daniel Perez-Becker, Thomas Portet, Reid Pryzant, Heyang Qin, Marko Radmilac, Corby Rosset, Sambudha Roy, Olatunji Ruwase, Olli Saarikivi, Amin Saied, Adil Salim, Michael Santacroce, Shital Shah, Ning Shang, Hiteshi Sharma, Xia Song, Masahiro Tanaka, Xin Wang, Rachel Ward, Guanhua Wang, Philipp Witte, Michael Wyatt, Can Xu, Jiahang Xu, Sonali Yadav, Fan Yang, Ziyi Yang, Donghan Yu, Chengruidong Zhang, Cyril Zhang, Jianwen Zhang, Li Lyna Zhang, Yi Zhang, Yue Zhang, Yunan Zhang, and Xiren Zhou. Phi-3 technical report: highly capable language model locally on your phone. CoRR, abs/2404.14219, 2024. doi: 10.48550/ARXIV.2404.14219. URL https://doi.org/10.48550/arXiv.2404.14219. [2] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Ming-Hsuan Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin. Qwen2.5-vl 10 technical report. CoRR, abs/2502.13923, 2025. doi: 10.48550/ARXIV.2502.13923. URL https://doi.org/10.48550/arXiv.2502.13923. [3] Hangbo Bao, Wenhui Wang, Li Dong, Qiang Liu, Owais Khan Mohammed, Kriti Vlmo: Unified visionAggarwal, Subhojit Som, Songhao Piao, and Furu Wei. In Sanmi Koyejo, S. Mohamed, language pre-training with mixture-of-modality-experts. A. Agarwal, Danielle Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022, 2022. URL http://papers.nips.cc/paper_files/paper/2022/hash/ d46662aa53e78a62afd980a29e0c37ed-Abstract-Conference.html. [4] Parishad BehnamGhader, Vaibhav Adlakha, Marius Mosbach, Dzmitry Bahdanau, Nicolas Chapados, and Siva Reddy. Llm2vec: Large language models are secretly powerful text encoders. CoRR, abs/2404.05961, 2024. doi: 10.48550/ARXIV.2404.05961. URL https: //doi.org/10.48550/arXiv.2404.05961. [5] Haonan Chen, Liang Wang, Nan Yang, Yutao Zhu, Ziliang Zhao, Furu Wei, and Zhicheng Dou. mme5: Improving multimodal multilingual embeddings via high-quality synthetic data. CoRR, abs/2502.08468, 2025. doi: 10.48550/ARXIV.2502.08468. URL https://doi.org/ 10.48550/arXiv.2502.08468. [6] Xiaokang Chen, Zhiyu Wu, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, and Chong Ruan. Janus-pro: Unified multimodal understanding and generation with data and model scaling. CoRR, abs/2501.17811, 2025. doi: 10.48550/ARXIV.2501.17811. URL https://doi.org/10.48550/arXiv.2501.17811. [7] Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and Jingjing Liu. UNITER: universal image-text representation learning. In Andrea Vedaldi, Horst Bischof, Thomas Brox, and Jan-Michael Frahm, editors, Computer Vision - ECCV 2020 - 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part XXX, volume 12375 of Lecture Notes in Computer Science, pages 104120. Springer, 2020. doi: 10.1007/ 978-3-030-58577-8_7. URL https://doi.org/10.1007/978-3-030-58577-8_7. [8] Mehdi Cherti, Romain Beaumont, Ross Wightman, Mitchell Wortsman, Gabriel Ilharco, Cade Gordon, Christoph Schuhmann, Ludwig Schmidt, and Jenia Jitsev. Reproducible scaling laws for contrastive language-image learning. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2023, Vancouver, BC, Canada, June 17-24, 2023, pages 28182829. IEEE, 2023. doi: 10.1109/CVPR52729.2023.00276. URL https://doi.org/10.1109/ CVPR52729.2023.00276. [9] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of deep bidirectional transformers for language understanding. In Jill Burstein, Christy Doran, and Thamar Solorio, editors, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), pages 4171 4186. Association for Computational Linguistics, 2019. doi: 10.18653/V1/N19-1423. URL https://doi.org/10.18653/v1/n19-1423. [10] Manuel Faysse, Hugues Sibille, Tony Wu, Bilel Omrani, Gautier Viaud, Céline Hudelot, and Pierre Colombo. Colpali: Efficient document retrieval with vision language models. In The Thirteenth International Conference on Learning Representations, ICLR 2025, Singapore, April 24-28, 2025. OpenReview.net, 2025. URL https://openreview.net/forum?id= ogjBpZ8uSi. [11] Yuying Ge, Sijie Zhao, Jinguo Zhu, Yixiao Ge, Kun Yi, Lin Song, Chen Li, Xiaohan Ding, and Ying Shan. SEED-X: multimodal models with unified multi-granularity comprehension and generation. CoRR, abs/2404.14396, 2024. doi: 10.48550/ARXIV.2404.14396. URL https://doi.org/10.48550/arXiv.2404.14396. [12] Michael Günther, Isabelle Mohr, Bo Wang, and Han Xiao. Late chunking: Contextual chunk embeddings using long-context embedding models. CoRR, abs/2409.04701, 2024. doi: 10. 48550/ARXIV.2409.04701. URL https://doi.org/10.48550/arXiv.2409.04701. 11 [13] Jarvis Guo, Tuney Zheng, Yuelin Bai, Bo Li, Yubo Wang, King Zhu, Yizhi Li, Graham Neubig, Wenhu Chen, and Xiang Yue. Mammoth-vl: Eliciting multimodal reasoning with instruction tuning at scale. CoRR, abs/2412.05237, 2024. doi: 10.48550/ARXIV.2412.05237. URL https://doi.org/10.48550/arXiv.2412.05237. [14] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross B. Girshick. Masked autoencoders are scalable vision learners. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022, New Orleans, LA, USA, June 18-24, 2022, pages 15979 15988. IEEE, 2022. doi: 10.1109/CVPR52688.2022.01553. URL https://doi.org/10. 1109/CVPR52688.2022.01553. [15] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc V. Le, YunHsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with noisy text supervision. In Marina Meila and Tong Zhang, editors, Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pages 49044916. PMLR, 2021. URL http://proceedings.mlr.press/v139/jia21b.html. [16] Ziyan Jiang, Rui Meng, Xinyi Yang, Semih Yavuz, Yingbo Zhou, and Wenhu Chen. Vlm2vec: Training vision-language models for massive multimodal embedding tasks. arXiv preprint arXiv:2410.05160, 2024. [17] Zixuan Ke, Yijia Shao, Haowei Lin, Tatsuya Konishi, Gyuhak Kim, and Bing Liu. Continual In The Eleventh International Conference on Learning pre-training of language models. Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023. URL https://openreview.net/forum?id=m_GDIItaI3o. [18] Wonjae Kim, Bokyung Son, and Ildoo Kim. Vilt: Vision-and-language transformer without convolution or region supervision. In Marina Meila and Tong Zhang, editors, Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pages 55835594. PMLR, 2021. URL http://proceedings.mlr.press/v139/kim21k.html. [19] Zhibin Lan, Liqiang Niu, Fandong Meng, Jie Zhou, and Jinsong Su. Llave: Large language and vision embedding models with hardness-weighted contrastive learning. CoRR, abs/2503.04812, 2025. doi: 10.48550/ARXIV.2503.04812. URL https://doi.org/10.48550/arXiv.2503. 04812. [20] Hugo Laurençon, Andrés Marafioti, Victor Sanh, and Léo Tronchon. Building and better understanding vision-language models: insights and future directions. CoRR, abs/2408.12637, 2024. doi: 10.48550/ARXIV.2408.12637. URL https://doi.org/10.48550/arXiv.2408. 12637. [21] Chankyu Lee, Rajarshi Roy, Mengyao Xu, Jonathan Raiman, Mohammad Shoeybi, Bryan Catanzaro, and Wei Ping. Nv-embed: Improved techniques for training llms as generalist embedding models. In The Thirteenth International Conference on Learning Representations, ICLR 2025, Singapore, April 24-28, 2025. OpenReview.net, 2025. URL https://openreview. net/forum?id=lgsyLSsDRe. [22] Jeffrey Li, Alex Fang, Georgios Smyrnis, Maor Ivgi, Matt Jordan, Samir Yitzhak Gadre, Hritik Bansal, Etash Guha, Sedrick Scott Keh, Kushal Arora, Saurabh Garg, Rui Xin, Niklas Muennighoff, Reinhard Heckel, Jean Mercat, Mayee F. Chen, Suchin Gururangan, Mitchell Wortsman, Alon Albalak, Yonatan Bitton, Marianna Nezhurina, Amro Abbas, Cheng-Yu Hsieh, Dhruba Ghosh, Josh Gardner, Maciej Kilian, Hanlin Zhang, Rulin Shao, Sarah M. Pratt, Sunny Sanyal, Gabriel Ilharco, Giannis Daras, Kalyani Marathe, Aaron Gokaslan, Jieyu Zhang, Khyathi Raghavi Chandu, Thao Nguyen, Igor Vasiljevic, Sham M. Kakade, Shuran Song, Sujay Sanghavi, Fartash Faghri, Sewoong Oh, Luke Zettlemoyer, Kyle Lo, Alaaeldin El-Nouby, Hadi Pouransari, Alexander Toshev, Stephanie Wang, Dirk Groeneveld, Luca Soldaini, Pang Wei Koh, Jenia Jitsev, Thomas Kollar, Alex Dimakis, Yair Carmon, Achal Dave, Ludwig Schmidt, and Vaishaal Shankar. Datacomp-lm: In search of the next generation of training sets for language models. In Amir Globersons, Lester Mackey, Danielle Belgrave, Angela Fan, Ulrich Paquet, Jakub M. Tomczak, and Cheng Zhang, editors, 12 Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024, 2024. URL http://papers.nips.cc/paper_files/paper/2024/hash/ 19e4ea30dded58259665db375885e412-Abstract-Datasets_and_Benchmarks_Track. html. [23] Junnan Li, Dongxu Li, Caiming Xiong, and Steven C. H. Hoi. BLIP: bootstrapping languageimage pre-training for unified vision-language understanding and generation. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvári, Gang Niu, and Sivan Sabato, editors, International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA, volume 162 of Proceedings of Machine Learning Research, pages 12888 12900. PMLR, 2022. URL https://proceedings.mlr.press/v162/li22n.html. [24] Junnan Li, Dongxu Li, Silvio Savarese, and Steven C. H. Hoi. BLIP-2: bootstrapping languageimage pre-training with frozen image encoders and large language models. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, volume 202 of Proceedings of Machine Learning Research, pages 1973019742. PMLR, 2023. URL https://proceedings.mlr.press/v202/li23q.html. [25] Zehan Li, Xin Zhang, Yanzhao Zhang, Dingkun Long, Pengjun Xie, and Meishan Zhang. Towards general text embeddings with multi-stage contrastive learning. CoRR, abs/2308.03281, 2023. doi: 10.48550/ARXIV.2308.03281. URL https://doi.org/10.48550/arXiv.2308. 03281. [26] Sheng-chieh Lin, Chankyu Lee, Mohammad Shoeybi, Jimmy Lin, Bryan Catanzaro, and Wei Ping. Mm-embed: Universal multimodal retrieval with multimodal llms, 2024. URL https://arxiv.org/abs/2411.02571. [27] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning, 2023. [28] Xueguang Ma, Sheng-Chieh Lin, Minghan Li, Wenhu Chen, and Jimmy Lin. Unifying multimodal retrieval via document screenshot embedding. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen, editors, Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, EMNLP 2024, Miami, FL, USA, November 12-16, 2024, pages 64926505. Association for Computational Linguistics, 2024. URL https://aclanthology.org/2024.emnlp-main.373. [29] Yiyang Ma, Xingchao Liu, Xiaokang Chen, Wen Liu, Chengyue Wu, Zhiyu Wu, Zizheng Pan, Zhenda Xie, Haowei Zhang, Xingkai Yu, Liang Zhao, Yisong Wang, Jiaying Liu, and Chong Ruan. Janusflow: Harmonizing autoregression and rectified flow for unified multimodal understanding and generation. CoRR, abs/2411.07975, 2024. doi: 10.48550/ARXIV.2411.07975. URL https://doi.org/10.48550/arXiv.2411.07975. [30] Meta. Introducing meta llama 3: The most capable openly available llm to date, April 2024. https://ai.meta.com/blog/meta-llama-3/. [31] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language superviIn Marina Meila and Tong Zhang, editors, Proceedings of the 38th International sion. Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pages 87488763. PMLR, 2021. URL http://proceedings.mlr.press/v139/radford21a.html. [32] Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bertnetworks. arXiv preprint arXiv:1908.10084, 2019. [33] Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. LAION-400M: open dataset of clip-filtered 400 million image-text pairs. CoRR, abs/2111.02114, 2021. URL https://arxiv.org/abs/2111.02114. [34] Vasu Singla, Kaiyu Yue, Sukriti Paul, Reza Shirkavand, Mayuka Jayawardhana, Alireza Ganjdanesh, Heng Huang, Abhinav Bhatele, Gowthami Somepalli, and Tom Goldstein. From pixels to prose: large dataset of dense image captions. CoRR, abs/2406.10328, 2024. doi: 10.48550/ARXIV.2406.10328. URL https://doi.org/10.48550/arXiv.2406.10328. [35] Jacob Mitchell Springer, Suhas Kotha, Daniel Fried, Graham Neubig, and Aditi Raghunathan. Repetition improves language model embeddings. In The Thirteenth International Conference on Learning Representations, ICLR 2025, Singapore, April 24-28, 2025. OpenReview.net, 2025. URL https://openreview.net/forum?id=Ahlrf2HGJR. [36] Hao Tan and Mohit Bansal. LXMERT: learning cross-modality encoder representations from transformers. In Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan, editors, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019, Hong Kong, China, November 3-7, 2019, pages 50995110. Association for Computational Linguistics, 2019. doi: 10.18653/V1/D19-1514. URL https://doi.org/10.18653/v1/D19-1514. [37] Nandan Thakur, Nils Reimers, Andreas Rücklé, Abhishek Srivastava, and Iryna Gurevych. BEIR: heterogenous benchmark for zero-shot evaluation of information retrieval models. CoRR, abs/2104.08663, 2021. URL https://arxiv.org/abs/2104.08663. [38] Michael Tschannen, Alexey A. Gritsenko, Xiao Wang, Muhammad Ferjad Naeem, Ibrahim Alabdulmohsin, Nikhil Parthasarathy, Talfan Evans, Lucas Beyer, Ye Xia, Basil Mustafa, Olivier J. Hénaff, Jeremiah Harmsen, Andreas Steiner, and Xiaohua Zhai. Siglip 2: Multilingual vision-language encoders with improved semantic understanding, localization, and dense features. CoRR, abs/2502.14786, 2025. doi: 10.48550/ARXIV.2502.14786. URL https://doi.org/10.48550/arXiv.2502.14786. [39] Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder, and Furu Wei. Text embeddings by weakly-supervised contrastive pre-training. CoRR, abs/2212.03533, 2022. doi: 10.48550/ARXIV.2212.03533. URL https://doi.org/ 10.48550/arXiv.2212.03533. [40] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, and Junyang Lin. Qwen2-vl: Enhancing visionlanguage models perception of the world at any resolution. CoRR, abs/2409.12191, 2024. doi: 10.48550/ARXIV.2409.12191. URL https://doi.org/10.48550/arXiv.2409.12191. [41] Xinlong Wang, Xiaosong Zhang, Zhengxiong Luo, Quan Sun, Yufeng Cui, Jinsheng Wang, Fan Zhang, Yueze Wang, Zhen Li, Qiying Yu, Yingli Zhao, Yulong Ao, Xuebin Min, Tao Li, Boya Wu, Bo Zhao, Bowen Zhang, Liangdong Wang, Guang Liu, Zheqi He, Xi Yang, Jingjing Liu, Yonghua Lin, Tiejun Huang, and Zhongyuan Wang. Emu3: Next-token prediction is all you need. CoRR, abs/2409.18869, 2024. doi: 10.48550/ARXIV.2409.18869. URL https://doi.org/10.48550/arXiv.2409.18869. [42] Cong Wei, Yang Chen, Haonan Chen, Hexiang Hu, Ge Zhang, Jie Fu, Alan Ritter, and Wenhu Chen. Uniir: Training and benchmarking universal multimodal information retrievers. In Ales Leonardis, Elisa Ricci, Stefan Roth, Olga Russakovsky, Torsten Sattler, and Gül Varol, editors, Computer Vision - ECCV 2024 - 18th European Conference, Milan, Italy, September 29-October 4, 2024, Proceedings, Part LXXXVII, volume 15145 of Lecture Notes in Computer Science, pages 387404. Springer, 2024. doi: 10.1007/978-3-031-73021-4_23. URL https: //doi.org/10.1007/978-3-031-73021-4_23. [43] Chengyue Wu, Xiaokang Chen, Zhiyu Wu, Yiyang Ma, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, Chong Ruan, and Ping Luo. Janus: Decoupling visual encoding for unified multimodal understanding and generation. CoRR, abs/2410.13848, 2024. doi: 10.48550/ARXIV.2410.13848. URL https://doi.org/10.48550/arXiv.2410.13848. [44] Shitao Xiao, Zheng Liu, Peitian Zhang, Niklas Muennighoff, Defu Lian, and Jian-Yun Nie. C-pack: Packed resources for general chinese embeddings. In Grace Hui Yang, Hongning Wang, Sam Han, Claudia Hauff, Guido Zuccon, and Yi Zhang, editors, Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 2024, Washington DC, USA, July 14-18, 2024, pages 641649. ACM, 2024. doi: 10.1145/3626772.3657878. URL https://doi.org/10.1145/3626772.3657878. [45] Qihang Yu, Mark Weber, Xueqing Deng, Xiaohui Shen, Daniel Cremers, and Liang-Chieh Chen. An image is worth 32 tokens for reconstruction and generation. In Amir Globersons, Lester Mackey, Danielle Belgrave, Angela Fan, Ulrich Paquet, Jakub M. Tomczak, and Cheng Zhang, editors, Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024, 2024. URL http://papers.nips.cc/paper_files/paper/2024/hash/ e91bf7dfba0477554994c6d64833e9d8-Abstract-Conference.html. [46] Shi Yu, Chaoyue Tang, Bokai Xu, Junbo Cui, Junhao Ran, Yukun Yan, Zhenghao Liu, Shuo Wang, Xu Han, Zhiyuan Liu, and Maosong Sun. Visrag: Vision-based retrieval-augmented In The Thirteenth International Conference on generation on multi-modality documents. Learning Representations, ICLR 2025, Singapore, April 24-28, 2025. OpenReview.net, 2025. URL https://openreview.net/forum?id=zG459X3Xge. [47] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training. In IEEE/CVF International Conference on Computer Vision, ICCV 2023, Paris, France, October 1-6, 2023, pages 1194111952. IEEE, 2023. doi: 10.1109/ ICCV51070.2023.01100. URL https://doi.org/10.1109/ICCV51070.2023.01100. [48] Kai Zhang, Yi Luan, Hexiang Hu, Kenton Lee, Siyuan Qiao, Wenhu Chen, Yu Su, and Ming-Wei Chang. Magiclens: Self-supervised image retrieval with open-ended instructions. In Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net, 2024. URL https://openreview.net/forum?id=Zc22RDtsvP. [49] Xin Zhang, Yanzhao Zhang, Wen Xie, Mingxin Li, Ziqi Dai, Dingkun Long, Pengjun Xie, Meishan Zhang, Wenjie Li, and Min Zhang. Gme: Improving universal multimodal retrieval by multimodal llms, 2024. URL http://arxiv.org/abs/2412.16855. [50] Yue Zhao, Fuzhao Xue, Scott Reed, Linxi Fan, Yuke Zhu, Jan Kautz, Zhiding Yu, Philipp Krähenbühl, and De-An Huang. QLIP: text-aligned visual tokenization unifies auto-regressive multimodal understanding and generation. CoRR, abs/2502.05178, 2025. doi: 10.48550/ARXIV. 2502.05178. URL https://doi.org/10.48550/arXiv.2502.05178. [51] Junjie Zhou, Zheng Liu, Ze Liu, Shitao Xiao, Yueze Wang, Bo Zhao, Chen Jason Zhang, Defu Lian, and Yongping Xiong. Megapairs: Massive data synthesis for universal multimodal retrieval. arXiv preprint arXiv:2412.14475, 2024."
        },
        {
            "title": "A Detailed Results on MMEB",
            "content": "We present the detailed results of MoCa and baseline models on the MMEB benchmark [16] in Table 4, covering 36 tasks across four categories: classification, VQA, retrieval, and visual grounding. Table 4: Detailed performance of multimodal models on 36 MMEB tasks [16]. We show results of baseline models and our method (MoCa) at 3B and 7B scales. Task CLIP OpenCLIP SigLIP BLIP2 VLM2Vec MMRet mmE5 MoCa (3B) MoCa (7B) 74.5 80.3 67.9 91.5 75.8 44.0 43.6 79.8 39.6 14.7 61.2 69.0 54.4 52.0 30.7 34.8 49.8 42.1 43.0 61.2 62.0 49.9 80.9 49.9 75.4 80.0 75.7 73.1 65.5 87.6 16.2 60.2 56.5 87.8 67. 80.6 88.7 84.0 90.9 86.1 67.5 57.1 62.9 58.8 71.3 53.7 85.0 70.0 43.0 36.1 71.6 55.8 14.7 56.0 73.3 56.7 78.5 39.3 41.7 49.5 45.2 51.7 59.0 79.0 57.4 83.0 61.4 74.2 78.1 78.6 72.4 68.3 90.2 54.9 24.9 87.5 65.6 69.9 76.8 89.8 90.6 77.0 83. 59.1 68.0 64.1 77.6 82.1 64.3 91.0 77.9 42.6 56.7 86.3 62.2 34.8 67.6 67.9 56.4 90.3 56.2 50.3 51.9 55.7 52.8 62.1 83.5 62.7 73.7 54.9 77.7 83.4 76.2 73.6 68.8 88.1 28.6 65.2 77.3 83.6 71.0 85.0 92.7 88.9 92.3 89.7 72.4 66.6 69. 75.4 80.9 70.6 87.0 74.8 38.8 39.7 75.4 31.3 24.0 59.8 40.0 54.6 93.0 67.7 64.1 61.6 45.4 52.3 66.9 83.1 62.9 80.5 55.7 74.4 77.8 76.4 72.6 67.4 90.6 22.2 73.3 75.9 80.8 70.6 80.2 92.1 92.8 89.5 88.7 72.3 61.5 67.5 78.0 81.5 77.6 90.0 76.8 43.0 52.7 83.0 45.2 30.4 65. 36.9 57.1 94.3 77.2 69.8 58.5 59.2 46.2 71.6 75.8 64.7 84.5 53.4 78.2 83.1 79.8 73.9 66.7 91.4 28.9 82.7 80.4 96.9 75.0 84.6 94.0 95.5 95.3 92.4 74.7 67.6 71.5 Classification (10 tasks) ImageNet-1K N24News HatefulMemes VOC2007 SUN397 Place365 ImageNet-A ImageNet-R ObjectNet Country-211 All Classification 55.8 34.7 51.1 50.7 43.4 28.5 25.5 75.6 43.4 19.2 42. VQA (10 tasks) OK-VQA A-OKVQA DocVQA InfographicsVQA ChartQA Visual7W ScienceQA VizWiz GQA TextVQA Avg. VQA Retrieval (12 tasks) VisDial CIRR VisualNews_t2i VisualNews_i2t MSCOCO_t2i MSCOCO_i2t NIGHTS WebQA FashionIQ Wiki-SS-NQ OVEN EDIS Avg. Retrieval 7.5 3.8 4.0 4.6 1.4 4.0 9.4 8.2 41.3 7.0 9.1 30.7 12.6 78.9 79.6 59.5 57.7 60.4 67.5 11.4 55.0 41.1 81.0 53.0 Visual Grounding (4 tasks) 33.8 MSCOCO RefCOCO 56.9 RefCOCO-matching 61.3 55.1 Visual7W-pointing Avg. Grounding 51.8 Final Score (36 tasks) All IND Avg. All OOD Avg. All Tasks Avg. 37.1 38.7 37.8 63.5 38.6 51.7 52.4 68.8 37.8 14.2 83.0 51.4 16.8 47.8 11.5 3.3 5.3 4.6 1.5 2.6 10.2 6.6 52.5 10.9 10.9 25.4 15.4 74.0 78.0 63.6 62.1 66.1 62.1 13.8 44.6 45.0 77.5 52.3 34.5 54.2 68.3 56.3 53.3 39.3 40.2 39. 45.4 13.9 47.2 64.3 39.6 20.0 42.6 75.0 40.3 14.2 40.3 2.4 1.5 4.2 2.7 3.0 1.2 7.9 2.3 57.5 1.0 8.4 21.5 15.1 51.0 52.4 58.3 55.0 62.9 58.1 20.1 55.1 56.0 23.6 31.6 46.4 70.8 50.8 70.1 59.5 32.3 38.0 34.8 10.3 36.0 49.6 52.1 34.5 21.5 3.2 39.7 20.6 2.5 27. 8.7 3.2 2.6 2.0 0.5 1.3 6.8 4.0 9.7 3.3 4.2 18.0 9.8 48.1 13.5 53.7 20.3 56.5 55.4 9.3 28.7 39.5 54.4 33.9 28.9 47.4 59.5 52.0 47.0 25.3 25.1 25."
        }
    ],
    "affiliations": [
        "Gaoling School of Artificial Intelligence, Renmin University of China",
        "Microsoft Corporation",
        "Stanford University"
    ]
}