{
    "paper_title": "AstaBench: Rigorous Benchmarking of AI Agents with a Scientific Research Suite",
    "authors": [
        "Jonathan Bragg",
        "Mike D'Arcy",
        "Nishant Balepur",
        "Dan Bareket",
        "Bhavana Dalvi",
        "Sergey Feldman",
        "Dany Haddad",
        "Jena D. Hwang",
        "Peter Jansen",
        "Varsha Kishore",
        "Bodhisattwa Prasad Majumder",
        "Aakanksha Naik",
        "Sigal Rahamimov",
        "Kyle Richardson",
        "Amanpreet Singh",
        "Harshit Surana",
        "Aryeh Tiktinsky",
        "Rosni Vasu",
        "Guy Wiener",
        "Chloe Anastasiades",
        "Stefan Candra",
        "Jason Dunkelberger",
        "Dan Emery",
        "Rob Evans",
        "Malachi Hamada",
        "Regan Huff",
        "Rodney Kinney",
        "Matt Latzke",
        "Jaron Lochner",
        "Ruben Lozano-Aguilera",
        "Cecile Nguyen",
        "Smita Rao",
        "Amber Tanaka",
        "Brooke Vlahos",
        "Peter Clark",
        "Doug Downey",
        "Yoav Goldberg",
        "Ashish Sabharwal",
        "Daniel S. Weld"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "AI agents hold the potential to revolutionize scientific productivity by automating literature reviews, replicating experiments, analyzing data, and even proposing new directions of inquiry; indeed, there are now many such agents, ranging from general-purpose \"deep research\" systems to specialized science-specific agents, such as AI Scientist and AIGS. Rigorous evaluation of these agents is critical for progress. Yet existing benchmarks fall short on several fronts: they (1) fail to provide holistic, product-informed measures of real-world use cases such as science research; (2) lack reproducible agent tools necessary for a controlled comparison of core agentic capabilities; (3) do not account for confounding variables such as model cost and tool access; (4) do not provide standardized interfaces for quick agent prototyping and evaluation; and (5) lack comprehensive baseline agents necessary to identify true advances. In response, we define principles and tooling for more rigorously benchmarking agents. Using these, we present AstaBench, a suite that provides the first holistic measure of agentic ability to perform scientific research, comprising 2400+ problems spanning the entire scientific discovery process and multiple scientific domains, and including many problems inspired by actual user requests to deployed Asta agents. Our suite comes with the first scientific research environment with production-grade search tools that enable controlled, reproducible evaluation, better accounting for confounders. Alongside, we provide a comprehensive suite of nine science-optimized classes of Asta agents and numerous baselines. Our extensive evaluation of 57 agents across 22 agent classes reveals several interesting findings, most importantly that despite meaningful progress on certain individual aspects, AI remains far from solving the challenge of science research assistance."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 2 ] . [ 1 2 5 6 1 2 . 0 1 5 2 : r ASTABENCH: RIGOROUS BENCHMARKING OF AI AGENTS WITH SCIENTIFIC RESEARCH SUITE Jonathan Bragg1 Mike DArcy1 Nishant Balepur2, Dan Bareket1 Bhavana Dalvi1 Jena D. Hwang1 Aakanksha Naik1 Harshit Surana1 Aryeh Tiktinsky1 Rosni Vasu4, Guy Wiener1 Sergey Feldman1 Dany Haddad1 Peter Jansen1,3 Varsha Kishore1,6 Bodhisattwa Prasad Majumder1 Sigal Rahamimov1 Kyle Richardson1 Amanpreet Singh1 Stefan Candra1 Chloe Anastasiades1 Rob Evans1 Malachi Hamada1 Jaron Lochner1 Ruben Lozano-Aguilera1 Cecile Nguyen1 Amber Tanaka1 Brooke Vlahos Jason Dunkelberger1 Dan Emery1 Regan Huff1 Rodney Kinney1 Matt Latzke1 Smita Rao1 Peter Clark1 Doug Downey1 Yoav Goldberg1,5 Ashish Sabharwal1 Daniel S. Weld1 1Asta Team, Allen Institute for AI, 2University of Maryland, 3University of Arizona, 4University of Zurich, 5Bar-Ilan University, 6University of Washington, Work performed while at Ai2 Asta Team."
        },
        {
            "title": "ABSTRACT",
            "content": "AI agents hold the potential to revolutionize scientific productivity by automating literature reviews, replicating experiments, analyzing data, and even proposing new directions of inquiry; indeed, there are now many such agents, ranging from generalpurpose deep research systems to specialized science-specific agents, such as AI Scientist and AIGS. Rigorous evaluation of these agents is critical for progress. Yet existing benchmarks fall short on several fronts: they (1) fail to provide holistic, product-informed measures of real-world use cases such as science research; (2) lack reproducible agent tools necessary for controlled comparison of core agentic capabilities; (3) do not account for confounding variables such as model cost and tool access; (4) do not provide standardized interfaces for quick agent prototyping and evaluation; and (5) lack comprehensive baseline agents necessary to identify true advances. In response, we define principles and tooling for more rigorously benchmarking agents. Using these, we present AstaBench, suite that provides the first holistic measure of agentic ability to perform scientific research, comprising 2400+ problems spanning the entire scientific discovery process and multiple scientific domains, and including many problems inspired by actual user requests to deployed Asta agents. Our suite comes with the first scientific research environment with production-grade search tools that enable controlled, reproducible evaluation, better accounting for confounders. Alongside, we provide comprehensive suite of nine science-optimized classes of Asta agents and numerous baselines. Our extensive evaluation of 57 agents across 22 agent classes reveals several interesting findings, most importantly that despite meaningful progress on certain individual aspects, AI remains far from solving the challenge of science research assistance."
        },
        {
            "title": "INTRODUCTION",
            "content": "AI agents are increasingly being applied to complex real-world use cases. In particular, they hold the promise to revolutionize scientific productivity by automating reviews of the literature, replicating complex experiments, analyzing high volumes of data, and even proposing new avenues to explore. 1 Large organizations such as OpenAI and Google are investing in general-purpose deep research systems to help everyone, including scientists, comb through literature much more effectively. We even have specialized science-specific agents, such as AI Scientist (Lu et al., 2024; Yamada et al., 2025) and AIGS (Liu et al., 2024), targeting scientific research. With so many different agentsmany behind paywalls and all evaluated in bespoke wayshow are end users and AI developers to know which perform best? Unfortunately, existing agent benchmark suites have several deficiencies, when considered as general measure of AI skill, including for their ability to do scientific research  (Table 1)  . First, suites often lack real-world tasks that are informed by authentic product usage data (typically guarded by technology companies), raising concerns that higher scores may not lead to meaningful real-world benefit. Second, they lack the standard task environments and tools necessary for realistic, controlled comparison of agents on level playing field; for example, no large-scale, controlled document retrieval tools exist, making it unclear whether winning agent has superior AI capabilities or merely access to more relevant information source. Third, they fail to properly account for confounding variables; we are unaware of benchmarks that consider variations in tool usage, and only few like HAL (Kapoor et al., 2025) measure cost, which is critical since even simplistic strategies (e.g., taking majority vote over repeated invocation) can boost accuracy by spending more. Fourth, benchmark suite interfaces are rarely standardized for use by general agents, since suite developers typically assume either that users will evaluate only agents that come with the suite (and so it is fine for evals to be coupled to agents, as in the case of OpenHands (Wang et al., 2025) or AutoGen (Fourney et al., 2024)) or that users will build only specialized agents for specific benchmarks (as is the case with general suites like HAL (Kapoor et al., 2025)). Measuring new agents on full suite typically requires time-consuming interventions ranging from extensive decoupling to manually clarifying task instructions that were not written with general agents in mind; this harms reproducibility and controlled comparison. Finally, benchmark suites lack comprehensive agent baselines for proper comparison. As result, most published evaluations only compare to small number of other agents or ablations, making it difficult to assess whether claimed improvements represent genuine advances. In response, we present set of benchmarking principles and the first benchmark suite, built upon these principles, that overcomes the aforementioned limitations, along with open-source resources that enable more rigorous, comprehensive measurement. Specifically: We formalize principles for rigorously benchmarking agents (Appendix A), which address key limitations of current agent benchmark suites. Guided by our principles, we present AstaBench1 (Section 3), more rigorous agent benchmark suite that is the first holistic measure of scientific research, which exercises broad spectrum of skillsincluding literature understanding, data understanding, planning, tool use, coding, and searchand comprises over 2400 problems spanning the full scientific discovery process and multiple scientific domains, including many problems based on real user requests from Asta,2 where we have deployed several of our agents for public use. It is easy to integrate new general agents with AstaBench, which provides standardized task interface. AstaBench includes the powerful Asta Environment (Section 4), the first agent environment that enables controlled, reproducible evaluation with production-grade search tools for retrieving information from large corpus of scientific literature. We also introduce the agent-eval Agents Evaluation Toolkit3 (Section 4.2), which enables defining benchmark suite and leaderboard with time-invariant cost accounting using model usages logged by Inspect (UK AI Security Institute, 2024), standard agent evaluation framework that provides broad model and evaluation compatibility. We introduce AstaBench Leaderboard4 built using this Toolkit. Its the first agent leaderboard to properly account for confounding variables such as the tools used by the agent and inference cost. 1https://github.com/allenai/asta-bench 2https://asta.allen.ai 3https://github.com/allenai/agent-eval 4https://allenai.org/asta/leaderboard 2 Table 1: AstaBench improves over existing agent benchmark suites in several ways. It tests holistic scientific reasoning (i.e., broad spectrum of task types and across more than one scientific domain). Many of its problems are inspired by actual user requests to our deployed Asta agents. Its standard tool environment isolates core agentic abilities (e.g., planning, tool-calling, etc.) from information access. AstaBenchs scoring controls for confounders, such as computational cost, and its tasks are defined using uniform format that supports general-purpose agents. The tables final column, titled Cls., indicates the number of agent classes (e.g., ReAct) that are used to instantiate (e.g., with specific LLMs) the total number of agents listed in preceding column; AstaBench includes more classes of agents than prior benchmarking efforts. Relevant for all agent benchmarks Holistic sci. reasoning Controlled, Product realistic tools usage-based Lit. tasks Prod.-grade AstaBench Broad (weighted towards CS) No science AutoGenBench BixBench Bio data science BrowserGym No science HAL Coding Inspect Evals LAB-Bench Bio Coding, knowledge Coding, data analysis Data analysis Coding OpenHands Evals ScienceAgentBench TerminalBench Vector Inst. Leaderboard No science Scoring accounts for confounders Costs, tools, openness Costs Costs Costs Tasks ready for general agents Decoupled, with standard formats Coupled to agent framework Non-standard notebook tools Ready for web agents Non-standard formats Non-standard formats Non-standard formats Coupled to agent framework Coupled to agents Ready for terminal agents Non-standard formats # Agents Total Cls. 57 22 7 2 10 2 2 113 10 18 53 17 1 3 6 33 12 5 1 lit. corpus Figure 1: Using AstaBench we evaluated 22 agent classes on diverse set of science tasks while controlling the set of available tools, e.g., to ensure each agent has access to the same set of scientific papers. AstaBench leaderboards record not just agents accuracy but also how much computation is required to achieve that performance. Finally, we present the agent-baselines Agents Suite5 (Section 4.3), the most comprehensive standardized agents suite, comprised of nine Asta agent classes that have been optimized for scientific research tasks, as well as numerous baselines. Together, the AstaBench benchmark suite, agent environment, agents suite, and leaderboard enable holistic measurement of the current state of LLM agents for scientific research assistance, as well as path for continuous improvement  (Fig. 1)  . We report on an extensive set of experiments on AstaBench using our agents suite with 57 agents spanning 22 classes of agent architectures, ranging from task-specific agents such as Asta Scholar QA and Asta CodeScientist to generic, ReAct-style architectures applicable to the broad range of benchmarks within AstaBench. We find that while 5https://github.com/allenai/agent-baselines 3 meaningful progress has been made on many fronts, science research assistance remains far from solved. Section 5 summarizes our findings, with more details in the appendices. These findings provide current snapshot of the state of scientific research assistance agents. But this is only starting point. AstaBench offers the ability to help the community continually and systematically assess progress (or lack thereof) as new agents are designed, something that has been difficult to do holistically. We hope AstaBench will continue to serve as valuable guide for the development of future agents through its clear targets, cost-aware performance reporting, and transparent evaluation regimen."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Our efforts relate to two recent threads of research: the development of holistic agent evaluations that test wide range of LLM-driven automation (for general review, see Yehudai et al. (2025)) and the development of new benchmarks for measuring the scientific reasoning of LLMs and their use as scientific assistants and agents (Wang et al., 2023). We consider each in turn. Holistic Agent Evaluations The last few years have seen surge in benchmarks and evaluation frameworks that attempt to holistically measure the reasoning abilities of LLMs (e.g., Gu et al., 2025; Gao et al., 2024; Habib et al., 2023; Guha et al., 2024). Given the rise of LLM-driven automation, recent efforts have centered around new benchmarks and frameworks for evaluating LLM agents. Table 1 highlights recent efforts that are most closely related to AstaBench in terms of their scope as holistic or science agent benchmarks: AutoGenBench (Fourney et al., 2024), BixBench (Mitchener et al., 2025), BrowserGym (Le Sellier De Chezelles et al., 2025), the Holistic Agent Leaderboard (HAL) (Kapoor et al., 2025), Inspect Evals (UK AI Safety Institute and Arcadia Impact and Vector Institute, 2025), Lab-Bench (Laurent et al., 2024), OpenHands Evals (Wang et al., 2025), ScienceAgentBench (Chen et al., 2025b), Terminal-Bench (The Terminal-Bench Team, 2025a), and the Vector Institute Leaderboard (Vector Institute, 2025).6 We compare these efforts to AstaBench across the following dimensions: holistic scientific reasoning (i.e., focuses on broad spectrum of task types and across more than one scientific domain), product usage-based (i.e., involves tasks based on product use cases), controlled, realistic tools (i.e., distributes standard, realistic tools that allow for controlled comparison of agents), scoring accounts for confounders (i.e., scores systematically account for cost, controlled tool use, and other confounders), general agents (i.e., tasks have uniform formats that support general-purpose agents), and number of agents (i.e., total number and number of different classes of agent). AstaBench stands out on these dimensions, which are key to advancing scientific AI and increasing benchmarking rigor generally (Appendix A). In terms of science, the other agent benchmark suites are all less holistic, either more limited in terms of task category (e.g., HALs only science tasks are coding tasks) or the domain (e.g., LAB-Bench is limited to biology); AstaBench is also the only benchmark to leverage data from companion product (Asta) in its tasks. Despite its importance, few suites have seriously focused on cost (HAL is an exception), and none have distributed standard tools that are decoupled from agents or agent frameworks. While some leaderboards are scaling up the number of agents they test (again, notably HAL), all test far fewer agent classes (architectures) compared to AstaBench, which also distributes open-source code for these agent classes through agent-baselines Agents Suite. Science Benchmarks and Agents for Science Naturally, the rise of powerful large language models (LLMs) has led to much recent interest in LLM-driven approaches to scientific research-related tasks. Many new benchmarks have been developed, often focusing on particular sub-problems in the full research pipeline, including scientific coding and execution (Tian et al., 2024; Lai et al., 2023; Chen et al., 2025a; Chan et al., 2025; Huang et al., 2024), data analysis (Majumder et al., 2025; Xu et al., 2025), research reproduction (Bogin et al., 2024; Siegel et al., 2025; Tang et al., 2025; Kon et al., 2025; Xiang et al., 2025; Starace et al., 2025; Zhao et al., 2025; Yan et al., 2025), ideation and hypothesis generation (Ruan et al., 2024; Si et al., 2024; Vasu et al., 2025), and literature retrieval and understanding (Shi et al., 2025; He et al., 2025), among others (Zhu et al., 2025). AstaBench 6Agent counts for Table 1 were derived from live leaderboards and repositories accessed August 2025, in addition to the canonical benchmark references (Microsoft, 2024; ServiceNow, 2025; SAgE Team, Princeton University, 2025; ArcadiaImpact / UK Government BEIS Team, 2025; All-Hands-AI, 2025a;b; The TerminalBench Team, 2025b). Table 2: AstaBench benchmarks, spanning four task categories: Literature Understanding, Code & Execution, Data Analysis, and End-to-End Discovery. Benchmarks are fully reproducible when paired with the Asta Environment tools listed in the Tools column, which come standard with each benchmark: Computational Notebook (Code) or Asta Scientific Corpus (Corpus) tools that restrict to papers before the specified Date Cutoff (exclusive). (Original datasets were filtered to ensure questions are answerable with the environment.) For ArxivDIGESTables-Clean, corpus tools are restricted to snippet search with specific paper IDs for each problem. indicates created by us, and indicates previously unreleased. Name PaperFindingBench LitQA2-FullText-Search ScholarQA-CS2 LitQA2-FullText ArxivDIGESTables-Clean SUPER-Expert CORE-Bench-Hard DS-1000 DiscoveryBench E2E-Bench E2E-Bench-Hard Task category Domains Test Val Tools Date Cutoff Lit. Und. (search) CS Lit. Und. (search) Biology Lit. Und. (report) Lit. Und. (MC) Lit. Und. (table) CS Biology Mixed Code & Exec. Code & Exec. Code & Exec. Data Analysis CS Mixed CS Mixed End-to-End Disc. End-to-End Disc. CS CS 267 75 100 75 100 45 37 239 40 40 100 66 Corpus 10 Corpus Corpus 10 Corpus Snippet 70 2025-06-01 2024-10-17 2025-05-01 2024-10-17 Paper IDs 50 Code 35 Code 100 Code 25 Code 10 Code Code 10 spans many of these task categories, and provides the most comprehensive evaluation of scientific agent performance to date  (Table 1)  . Increased LLM capabilities have led to emergence of host of agents for end-to-end, open-ended scientific discovery, including AI Scientist (Lu et al., 2024; Yamada et al., 2025), Agent Lab (Schmidgall et al., 2025), AIGS (Liu et al., 2024), and CodeScientist (Jansen et al., 2025), among others (Cheng et al., 2025). To bring clarity to this area (and accelerate its progress), AstaBench introduces new end-to-end task that evaluates an agents ability to complete research project, starting from an idea and ending with written report and code. We believe this task is useful complement to the many existing benchmarks that focus on more narrow problems in the research pipeline."
        },
        {
            "title": "3 ASTABENCH: A HOLISTIC SCIENTIFIC RESEARCH BENCHMARK SUITE",
            "content": "We present AstaBench, the first benchmark suite for holistic evaluation of agents ability to perform scientific research. Crucially, our suite is reproducible even as science progresses, since it comes with the first realistic, reproducible search tools (Section 4). Our suite implements new standard interface for agent benchmark suites and provides time-invariant cost reporting through the agent-eval Agents Evaluation Toolkit (Section 4.2)). As such, AstaBench is ready for use by new general agents such as those in our agent baselines suite (Section 4.3). AstaBench comprises the following 11 benchmarks (summarized in Table 2, with full details in Appendix E; note that AstaBench uses slightly modified versions of some of the cited datasets): PaperFindingBench tests an agents ability to handle challenging scientific search queries. LitQA2FullText/LitQA2-FullText-Search (Skarlinski et al., 2024) measure an agents ability to answer questions and retrieve papers within the biomedical domain. ScholarQA-CS2 tests an agents ability to answer long-form scientific questions. ArxivDIGESTables-Clean (Newman et al., 2024) tests an agents ability to create literature review table. SUPER-Expert (Bogin et al., 2024) tests the ability of code agents to set up and execute Python machine learning experiments reported in ML and NLP papers. CORE-Bench-Hard (Siegel et al., 2025) tests an agents ability to reproduce experiments and analyses from papers. DS-1000 (Lai et al., 2023) tests the ability of agents on data science tasks encountered in research. DiscoveryBench (Majumder et al., 2025) tests whether the agent can automatically find and verify hypotheses from given dataset(s). E2E-Bench/E2E-BenchHard test whether agents can perform the full research pipeline of ideation, planning, (software) experiment design, implementation, execution, analysis, and producing final report."
        },
        {
            "title": "4 ASTA ENVIRONMENT",
            "content": "Asta Environment is, to our knowledge, the first realistic, reproducible scientific research environment for agents. It provides standardized tools, an evaluation toolkit, leaderboard, and numerous agents."
        },
        {
            "title": "4.1 STANDARD TOOLS FOR AGENTS",
            "content": "Asta Environment provides comprehensive set of standard tools for science research assistance, from which each AstaBench task includes specific subset based on its requirements  (Table 2)  . Asta Scientific Corpus: toolset for accessing the scientific literature, which represents the first production-grade, reproducible search tools for agents. These tools can restrict outputs to papers preceeding date; AstaBench uses this feature to limit results to the date of benchmark creation so that new papers do not contaminate results (see cutoffs for specific tasks in Table 2). The snippet_search tool can be further restricted to papers with specific IDs so that it can be used as text retrieval mechanism over those papers (useful for detailed literature analysis, e.g., in ArxivDIGESTablesClean). It provides the following specific tools via the MCP (Model Context Protocol) standard: snippet_search, search_papers_by_relevance, get_paper, get_paper_batch, get_citations, search_authors_by_name, get_author_papers, search_paper_by_title Computational Notebook: stateful computational (Jupyter) notebook. The tool can execute Python code as well as standard IPython magic commands like %%writefile, %matplotlib inline, and !shell_command. Python variables and environment are maintained between calls so that the tool can be used to solve problems incrementally. By default, the tool returns timeout message to the agent if single cell takes more than 5 minutes to execute. Since the tool needs to execute code, it lives in new sandbox image thats created by the framework. Our tools feature improved agent compatibility compared to other suites. They are cleanly decoupled from agents and provide easy integration via MCP. Code executed in our sandbox can call tools provided by the main (host) execution environment (e.g., Asta Scientific Corpus), enabling testing of code execution agents, e.g., agents that implement the CodeAct (Wang et al., 2024) pattern. 4.2 N T-E EVALUATION TOOLKIT & ASTABENCH LEADERBOARD We use Inspect (UK AI Security Institute, 2024) as the framework for implementing our individual agentic benchmarks, as it provides broad model provider and tool compatibility, useful logging and debugging affordances, and growing set of compatible evals (UK AI Safety Institute and Arcadia Impact and Vector Institute, 2025). However, Inspect logs only model usages (not normalized dollar amounts) and it lacks tooling for defining benchmark suites with unified scoring or leaderboards. To fill this gap, we present the agent-eval7 agent leaderboard toolkit, which provides benchmark suite, reporting, and leaderboard layer on top of suite of Inspect-formatted benchmarks; it features: Time-invariant cost calculation: The agent-eval toolkit computes normalized dollar costs based on model usages logged through Inspect. For mapping model usages to prices, we use frozen snapshot of the litellm cost map, which is community-sourced for broad model coverage.8 It factors in cache discounts for agents that take advantage of caching, as this is an increasingly adopted optimization technique (and providers like OpenAI provide these discounts automatically); however, it does not factor in any latency-related discounts (e.g., service tier or batching). Using frozen snapshot allows fair comparison of evaluation costs even if API prices change between evaluations.9 Reporting that accounts for confounders: In addition to cost, the agent-eval toolkit and leaderboards categorize agent evaluation submissions according to their reproducibility and degree of control based on the following dimensions (full definitions in Appendix B): Agent openness (is the agent implementation open?): Open-source, open-weight (), Opensource, closed-weight (), Closed source & API available (A), or Closed & UI only () 7https://github.com/allenai/agent-eval 8We supplement the cost map with prices for custom models based on Together AI (https://www.toge ther.ai/) generic model size-based pricing. 9The cost map snapshot used for the leaderboard may be periodically updated, but we will always re-calculate all costs based on the current snapshot to ensure fair comparison. 6 Table 3: Agent classes in the agent-baselines Agents Suite, with Asta agents in the top section and baseline agents in the bottom section. Standard tooling means that the only tools used are the ones distributed with the AstaBench tasks; Custom interface means that standard date-restricted search is used but additional custom tooling may be used; Fully custom means that tooling is custom and standard search tools are not used. Name Asta Paper Finder Asta Scholar QA Asta Scholar QA (w/ Tables) Asta Table Synthesis Asta Code Asta DataVoyager Asta Panda Asta CodeScientist Asta v0 ReAct Smolagents Coder You.com Search API Elicit FutureHouse Crow FutureHouse Falcon OpenAI Deep Research OpenSciLM Perplexity Sonar Deep Research SciSpace Deep Review STORM You.com Research API Faker Task optimization Opensource Lit. Und. (search) Yes Yes Lit. Und. (report) Yes Lit. Und. (report) Yes Lit. Und. (table) Code & Execution Yes Yes Data Analysis Yes End-to-End Disc. Yes End-to-End Disc. Yes Multi Yes Yes None (general) None (general) Lit. Und. (search) Lit. Und. (report) Lit. Und. (report) Lit. Und. (report) Lit. Und. (report) Yes Lit. Und. (report) Lit. Und. (report) Lit. Und. (report) Yes Lit. Und. (report) Lit. Und. (report) Yes End-to-End Disc. Tooling Custom interface Custom interface Custom interface Custom interface Custom interface Custom interface Fully custom Fully custom Fully custom Standard Custom interface Fully custom Fully custom Fully custom Fully custom Fully custom Custom interface Fully custom Fully custom Fully custom Fully custom Standard Agent tooling (does the agent use the provided standard tools for the tasks?): Standard (), Custom interface (), or Fully custom () Leaderboard web interface: In addition to the agent-eval CLI-based leaderboard interface (which requires authentication currently unavailable to the public for AstaBench), we also include web application interface for the AstaBench Leaderboard10, which supports external submissions (with Hugging Face user-based authentication) and provides interactive plots and tables. 4.3 N T-B L S AGENTS SUITE To enable comprehensive measurement on AstaBench and other benchmarksand advance the state of the artwe provide the agent-baselines Agents Suite,11 which consists of large set of agents from 16 agent classes12 with standard Inspect-compatible interface. Table 3 lists these agents, grouped into (1) the Asta agents that we optimized for scientific research tasks and (2) numerous baseline agents that we evaluate. Detailed descriptions are deferred to Appendix F."
        },
        {
            "title": "5 EXPERIMENTS",
            "content": "We now present experimental results, which we have also used to seed the interactive AstaBench leaderboard.13 Our experiments were conducted over period of several months. Since one may boost scores by using more compute (eg using repetition and majority vote) (Dodge et al., 2019), we report cost as well as accuracy. We also report the standard deviation of our measurements. For 10https://allenai.org/asta/leaderboard 11https://github.com/allenai/agent-baselines 12Slightly less than the 22 we evaluate because some are closed source and thus not usable on new inputs; however, we provide ways to reproducing those results based on cached answers obtained for our experiments. 13https://allenai.org/asta/leaderboard 7 brevity, when an agent was tested with multiple different models, we report the top result(s) plus any other significant data points. The entire set of results, plus plots of scores vs. costs including the Pareto frontier (showing the best agent for given cost), are in Appendix D. Some agents (e.g., ReAct) can attempt all 11 benchmarks; others are category-specific or even benchmark-specific. Table 4 shows the overall results for those agents attempting all benchmarks, as well as agents that can solve all the benchmarks in at least one category. Categoryand benchmarkspecific results are presented in Appendix for space reasons. Figure 2: Score vs. cost analysis for overall and category results (from Tables 4, 11, 16 and 17). Points indicate means. Points on the Pareto frontier are connected with dotted lines, representing optimal quality-cost trade-offs for each category (Literature Understanding, Code & Execution, Data Analysis, End-to-End Discovery). denotes models not pinned to date-stamped version. Note: the x-axis (cost per answer in dollars) uses log scale. For more detailed plots for individual categories and benchmarks, see Appendix D. As noted above, agents powered by closed weight LLMs currently far exceed the reach of those powered by open weight LLMs. On the other hand, simply switching the underlying LLM with the latest and greatest one isnt necessarily reliable recipe for success on AstaBench. As case in point, one of the newest LLMs, gpt-5, provides only modest boost over an earlier reasoning LLM, o3, except on three benchmarks. In fact, gpt-5 hurts the performance of several specialized agents. Tools designed specifically for science research assistance can significantly help AI agents. This is most noticable with Asta v0, which scores 9% higher than the next best agent, ReAct with gpt-5 (53.0% vs. 44.0%). However, this comes with the trade-off of significantly higher development (engineering) cost, and (for some tasks, specifically in end-to-end-discovery) higher inference cost. None of the commercial scientific research agents were able to perform the full range of research tasks in AstaBench. The best such API-based agent (FutureHouse Falcon) and the best closed one (OpenAI Deep Research) score well on literature understanding, but are unable to perform the full spectrum of science research assistance. Science research assistance is still far from solved, as evidenced by the generally low overall scores for the full gamut of agents, from fully open to fully closed. For example: The best open source agent with open weights LLMs scores terrible 11.1% (Smolagents Coder with Llama-4-Scout-17B16E-Instruct)  (Table 4)  . The best open source agent with closed LLM(s) is much better: 53.0% (Asta v0)  (Table 4)  . While the best API-based agent (FutureHouse Falcon) and closed agent (OpenAI Deep Research) score well on single benchmark  (Table 6)  , they are stymied by the full range of tasks. The cost-performance tradeoff across agents, highlighted by the Asta leaderboards Pareto curve provides several interesting insights. The best economical model is ReAct with gpt-5-mini, scoring 32%within 21% (absolute) of the best performing modelswhile costing over an order of magnitude less at $0.04 per problem. Powering general agent with an expensive model can lower the overall cost. Though the per-token cost is 3 to 25 times lower for gemini-flash and llama-scout compared to o3 or sonnet, the weaker models often take more steps or get stuck in loops, causing ReAct agent to end up being twice as expensive in addition to lower-performing. Surprisingly, most of our specialized agents (Asta Scholar QA  (Table 6)  , Asta DataVoyager  (Table 4)  , Asta Code  (Table 8)  ) perform worse with gpt-5 than with previous models, while ReAct performs much better. One possible explanation for this is that gpt-5 has been tuned to do well with now-common ReAct-style workflows, and conversely may be relatively less adaptive to alternate workflows. If this is indeed true, and trends continue, there may be diminishing value in application-specific workflows. As the LLM underlying ReAct, gpt-5s boost over o3 is generally light, with only gain of 0%- 5% across most benchmarks. However, gpt-5 provides huge boost in 4 benchmarks: +13.4% absolute on ScholarQA-CS2  (Table 6)  , + 24.8% on SUPER-Expert  (Table 8)  , +25.3% on LitQA2FullText-Search  (Table 5)  , and +21.1% on E2E-Bench-Hard  (Table 10)  . In general, todays agents are reasonably good at literature understanding. However, despite some recent progress, coding, experiment execution, data analysis, and data-driven discovery still remain major, unsolved problems for science assistance agents. Literature Understanding: For literature search agents, Asta Paper Finder stands out as an impressive system, scoring much higher than its closest rival (ReAct) on PaperFindingBench and LitQA2-FullText-Search  (Table 5)  . Despite this, it is clear that the paper-finding task is far from solved, requiring further work to achieve truly comprehensive results. For literature question-answering agents, our results  (Table 6)  suggest that (among other things): The best models have relatively good performance in this category, scoring around 80%. This is likely because literature understanding has been strong focus of many task-optimized agents in the community (or conversely, the community has targeted literature understanding because this category is particularly well suited for language models). Asta Scholar QA, Elicit, and SciSpace Deep Review are the best tools on these tests (all score about 85% or higher on ScholarQA-CS2, Table 6). For all three tools, the higher performance is driven by the citation subscores of the evaluation. The other external/commercial agents are not far behind, but also do not do significantly better than the best ReAct baseline. This is indeed surprising given ReActs simplicity, but is also an indicator of the challenging nature of the task that requires system responses to be precise and cover the relevant points as well as cite the correct supporting sources for claims as necessary. For literature review table generation agents, our results  (Table 7)  suggest that: even the best models do not yet achieve strong performance in this category, with recall scores around 43%, likely due to limited efforts to build task-optimized agents in this space. Asta Table Synthesis, backed by gpt-5, wins on this task, beating the best general agents. However, Asta Table Synthesis backed by gpt-5-mini also shows competitive performance, at just 13% of the cost. Code and Execution: Coding and execution is far from solvedall agents score low on these tasks, e.g., all but two scored below 25% on SUPER-Expert (ReAct with gpt-5 scored 41%), Table 8. Coding and execution thus remain major bottlenecks for assisting with and automating science. The impact of using gpt-5 is highly unpredictable. Surprisingly, running the general ReAct agent with gpt-5 significantly improves its performance (compared to running with other LLMs), while running the more custom-built Smolagents Coder with gpt-5 notably decreases performance. One possible explanation is that gpt-5 has been tuned for the common ReAct-style workflow, making gpt-5 less adaptive to alternate workflows. Data Analysis: Similarly, automated data analysis and data-driven discovery is major, unsolved challenge for science assistance agents. We see agents struggle with this benchmark, with the maximum score being only 34%  (Table 4)  despite increased attention in the community. End-to-End Discovery: End-to-end discovery remains far from being meaningfully solved. Although the average research step completion scores appear reasonable (scores up to 70%, Table 10), the likelihood of completing all experiment steps remains low. For example, given 10 steps per experiment, and success rate of 70% per step, the success rate to complete all steps in the experiment will be 0.710 3%; in fact, we observed an even lower 1% for the best end-to-end agent (Asta Panda with claude-sonnet-4). lot more work is needed, and we hope these benchmarks will help push research forward in this direction."
        },
        {
            "title": "6 CONCLUSION AND FUTURE WORK",
            "content": "In summary, we identify limitations of current approaches to benchmarking agents, and present methodology and tooling for doing so more rigorously. Using this methodology and tooling, we introduce AstaBench, holistic benchmark suite for scientific research that addresses key limitations. AstaBench is the first major agent benchmark suite to come with standard environment and tools that enable controlled comparison of agents: the Asta Environment, the first scientific research environment for agents with realistic, controlled search tools. Alongside, we present the agentbaselines Agents Suite, large suite of standardized agents, which we used to conduct experiments on AstaBench with 57 agents across 22 architectural classes. This revealed several interesting findings, most importantly that despite meaningful progress on certain individual aspects, agentic AI remains far from solving the challenge of scientific research assistance. We invite the community to make submissions to the AstaBench Leaderboard, which is powered by our agent-eval Agents Evaluation Toolkit. This work opens up many exciting possibilities for the agentic AI, scientific research assistance, and automated scientific discovery communities. We are actively pushing the performance-cost frontiers in AstaBench and closing the gap for truly open agents by developing new agent techniques, tools, and open models specialized for scientific research. We are also enhancing agent abilities to manage complex context, from improving on Asta v0 simple orchestration techniques to handling long-duration tasks in complex research projects. We are continuing to research how to refine our LLM-as-a-judge grading procedures, especially for challenging scientific discovery tasks. We plan to develop fresh benchmark problems that use the latest scientific knowledge, which is contaminationresistant and past the training cut-off date of models. We also plan to build benchmarks that test more aspects of collaboration with humans, and deepen coverage of problems in impactful fields such as biomedicine. Finally, we are committed to continuing to measure the latest advancesboth by testing the latest LLMs and by adding more agent architectures to agent-baselines."
        },
        {
            "title": "ETHICS STATEMENT",
            "content": "We took care to adhere to high ethics bar. We obtained legal review for all material presented in this work. The new real-world user queries used in the Literature Understanding tasks were collected with user consent. We also credit any benchmarks that we adapted for use in our suite, as well as agents that we leverage, citing those works. When measuring existing agents, we worked with the agent creators where possible to ensure they are measured fairly, including Elicit, Future House, and SciSpace."
        },
        {
            "title": "REPRODUCIBILITY STATEMENT",
            "content": "We took special care to make this work reproducible; indeed, reproducibility is core value proposition of our benchmark suite. AstaBench comes with open source code for all included benchmarks, agents, and core infrastructureas well as logs of all reported experiment. The framework logs and reports specific repository commits, including for data. The agent tools in AstaBench improve reproducibility by providing date-restricted access to the supporting document corpus."
        },
        {
            "title": "AUTHOR CONTRIBUTIONS",
            "content": "Authors listed in alphabetical order within each section: Project leadership, framework, and general agent development: Jonathan Bragg, Mike DArcy Research by task category (benchmarks and agents): Literature Understanding (paper finding): Dan Bareket, Yoav Goldberg, Sigal Rahamimov, Aryeh Tiktinsky, Guy Wiener Literature Understanding (summarization and QA): Nishant Balepur, Doug Downey, Sergey Feldman, Dany Haddad, Jena D. Hwang, Varsha Kishore, Aakanksha Naik, Amanpreet Singh, Daniel S. Weld Literature Understanding (table generation): * Benchmark: Aakanksha Naik * Agent: Mike DArcy, Dany Haddad, Aakanksha Naik Code & Execution: Mike DArcy, Kyle Richardson Data Analysis: Bodhisattwa Prasad Majumder, Harshit Surana End-to-End Discovery: Peter Clark, Bhavana Dalvi, Peter Jansen, Rosni Vasu Engineering: Frameworks and leaderboard data: Chloe Anastasiades, Stefan Candra, Regan Huff,"
        },
        {
            "title": "Rodney Kinney",
            "content": "Leaderboard web application: Jason Dunkelberger, Dan Emery, Cecile Nguyen, Smita Rao, Amber Tanaka, Brooke Vlahos Management: Jaron Lochner, Smita Rao, Rob Evans Design: Matt Latzke Support and data annotation: Malachi Hamada Product management: Ruben Lozano-Aguilera Management, mentorship, and advice: Peter Clark, Doug Downey, Yoav Goldberg, Ashish Sabharwal, Daniel S. Weld The Use of Large Language Models (LLMs) We used AI-based tools (Claude Code, Github Copilot, ChatGPT) for analyzing results data, generating code to populate plots and tables, identifying errors and missing references, and (minor) writing assistance."
        },
        {
            "title": "ACKNOWLEDGMENTS",
            "content": "This work would not have been possible without broad and supportive community. In particular, we thank: David Albright and Kyle Wiggers for communications support and useful feedback; Crystal Nam for legal support; Ali Farhadi and Sophie Lebrecht for insightful feedback and encouragement; Stephen Kelman for design support; the creators and maintainers of the Inspect evaluation framework; the creators of the external datasets that we have integrated; and the data workers who contributed to the creation of those datasets and the datasets that we created."
        },
        {
            "title": "REFERENCES",
            "content": "Anirudh Ajith, Mengzhou Xia, Alexis Chevalier, Tanya Goyal, Danqi Chen, and Tianyu Gao. LitSearch: retrieval benchmark for scientific literature search. In EMNLP, 2024. URL https: //aclanthology.org/2024.emnlp-main.840/. All-Hands-AI. OpenHands agent hub, 2025a. URL https://github.com/All-Hands-AI/ OpenHands/tree/55d204ae1b5581b0e55ebbd6465c7e2211b26765/openhand s/agenthub. Accessed: 2025-08-25. 11 All-Hands-AI. OpenHands evaluation leaderboard, 2025b. URL https://docs.google.co m/spreadsheets/d/1wOUdFCMyY6Nt0AIqF705KN4JKOWgeI4wUGUP60krXXs/ed it?gid=0#gid=0. Accessed: 2025-08-25. ArcadiaImpact / UK Government BEIS Team. Inspect Evals Dashboard, 2025. URL https: //inspectevalsdashboard-vv8euilv46.streamlit.app/. Accessed: 2025-0708; site was down on 2025-08-25. Akari Asai, Jacqueline He, Rulin Shao, Weijia Shi, Amanpreet Singh, Joseph Chee Chang, Kyle Lo, Luca Soldaini, Sergey Feldman, Mike DArcy, David Wadden, Matt Latzke, Minyang Tian, Pan Ji, Shengyan Liu, Hao Tong, Bohao Wu, Yanyu Xiong, Luke S. Zettlemoyer, Graham Neubig, Daniel S. Weld, Doug Downey, Wen tau Yih, Pang Wei Koh, and Hanna Hajishirzi. OpenScholar: Synthesizing scientific literature with retrieval-augmented LMs. ArXiv, abs/2411.14199, 2024. URL https://api.semanticscholar.org/CorpusID:274166189. Ben Bogin, Kejuan Yang, Shashank Gupta, Kyle Richardson, Erin Bransom, Peter Clark, Ashish Sabharwal, and Tushar Khot. SUPER: Evaluating agents on setting up and executing tasks from research repositories. In EMNLP, 2024. URL https://aclanthology.org/2024.emnl p-main.702. Jun Shern Chan, Neil Chowdhury, Oliver Jaffe, James Aung, Dane Sherburn, Evan Mays, Giulio Starace, Kevin Liu, Leon Maksin, Tejal Patwardhan, Lilian Weng, and Aleksander Mádry. MLEbench: Evaluating machine learning agents on machine learning engineering. In ICLR, 2025. URL https://openreview.net/forum?id=6s5uXNWGIh. Hui Chen, Miao Xiong, Yujie Lu, Wei Han, Ailin Deng, Yufei He, Jiaying Wu, Yibo Li, Yue Liu, and Bryan Hooi. MLR-Bench: Evaluating AI agents on open-ended machine learning research. arXiv:2505.19955, 2025a. URL https://arxiv.org/abs/2505.19955. Ziru Chen, Shijie Chen, Yuting Ning, Qianheng Zhang, Boshi Wang, Botao Yu, Yifei Li, Zeyi Liao, Chen Wei, Zitong Lu, Vishal Dey, Mingyi Xue, Frazier N. Baker, Benjamin Burns, Daniel Adu-Ampratwum, Xuhui Huang, Xia Ning, Song Gao, Yu Su, and Huan Sun. ScienceAgentBench: Toward rigorous assessment of language agents for data-driven scientific discovery. In ICLR, 2025b. URL https://openreview.net/forum?id=6z4YKr0GK6. Junyan Cheng, Peter Clark, and Kyle Richardson. Language modeling by language models. arXiv:2506.20249, 2025. URL https://arxiv.org/abs/2506.20249. Nick Craswell, Bhaskar Mitra, Emine Yilmaz, Daniel Fernando Campos, and E. Voorhees. Overview of the TREC 2020 deep learning track. ArXiv, abs/2102.07662, 2021. URL https://api.se manticscholar.org/CorpusId:212737158. Jesse Dodge, Suchin Gururangan, Dallas Card, Roy Schwartz, and Noah A. Smith. Show your work: Improved reporting of experimental results. In EMNLP, 2019. Adam Fourney, Gagan Bansal, Hussein Mozannar, Cheng Tan, Eduardo Salinas, Erkang Zhu, Friederike Niedtner, Grace Proebsting, Griffin Bassman, Jack Gerrits, Jacob Alber, Peter Chang, Ricky Loynd, Robert West, Victor Dibia, Ahmed Awadallah, Ece Kamar, Rafah Hosn, and Saleema Amershi. Magentic-One: generalist multi-agent system for solving complex tasks. arXiv, abs/2411.04468, 2024. URL https://arxiv.org/abs/2411.04468. Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noach, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. The language model evaluation harness, 07 2024. URL https://zenodo.org/records/12608602. Yuling Gu, Oyvind Tafjord, Bailey Kuehl, Dany Haddad, Jesse Dodge, and Hannaneh Hajishirzi. In Findings of NAACL, 2025. URL OLMES: standard for language model evaluations. https://arxiv.org/abs/2406.08446. 12 Etash Guha, Negin Raoff, Jean Mercat, Ryan Marten, Eric Frankel, Sedrick Keh, Sachin Grover, George Smyrnis, Trung Vu, Jon Saad-Falcon, Caroline Choi, Kushal Arora, Mike Merrill, Yichuan Deng, Ashima Suvarna, Hritik Bansal, Marianna Nezhurina, Reinhard Heckel, Seewong Oh, Tatsunori Hashimoto, Jenia Jitsev, Yejin Choi, Vaishaal Shankar, Alex Dimakis, Mahesh Sathiamoorthy, and Ludwig Schmidt. Evalchemy: post-trained model evaluation framework, November 2024. URL https://github.com/mlfoundations/evalchemy/tree/ce5cea94 f9f0f61388d2234afb01d811ff4357f4. Nathan Habib, Clémentine Fourrier, Hynek Kydlíˇcek, Thomas Wolf, and Lewis Tunstall. Lighteval: lightweight framework for LLM evaluation, 2023. URL https://github.com/hugging face/lighteval/tree/126f908a323a6d36f718076c4748e212d7275cfe. Yichen He, Guanhua Huang, Peiyuan Feng, Yuan Lin, Yuchen Zhang, Hang Li, and Weinan E. PaSa: An LLM agent for comprehensive academic paper search. In ACL, 2025. URL https: //aclanthology.org/2025.acl-long.572/. Qian Huang, Jian Vora, Percy Liang, and Jure Leskovec. MLAgentBench: Evaluating language agents on machine learning experimentation. In ICML, 2024. Peter Jansen, Oyvind Tafjord, Marissa Radensky, Pao Siangliulue, Tom Hope, Bhavana Dalvi, Bodhisattwa Prasad Majumder, Daniel S. Weld, and Peter Clark. CodeScientist: End-to-end semi-automated scientific discovery with code-based experimentation. In ACL Findings, 2025. Sayash Kapoor, Benedikt Stroebl, Peter Kirgis, Franck Stéphane Ndzomga, Kangheng Liu, and Arvind Narayanan. HAL: holistic agent leaderboard for centralized and reproducible agent evaluation. https://github.com/princeton-pli/hal-harness, 2025. Patrick Tser Jern Kon, Jiachen Liu, Xinyi Zhu, Qiuyi Ding, Jingjia Peng, Jiarong Xing, Yibo Huang, Yiming Qiu, Jayanth Srinivasa, Myungjin Lee, Mosharaf Chowdhury, Matei Zaharia, and Ang Chen. EXP-Bench: Can AI conduct AI research experiments? arXiv:2505.24785, 2025. URL https://arxiv.org/abs/2505.24785. Yuhang Lai, Chengxi Li, Yiming Wang, Tianyi Zhang, Ruiqi Zhong, Luke Zettlemoyer, Wen-tau Yih, Daniel Fried, Sida Wang, and Tao Yu. DS-1000: natural and reliable benchmark for data science code generation. In ICML, 2023. Jon M. Laurent, Joseph D. Janizek, Michael Ruzo, Michaela M. Hinks, Michael J. Hammerling, Siddharth Narayanan, Manvitha Ponnapati, Andrew D. White, and Samuel G. Rodriques. LABBench: Measuring capabilities of language models for biology research. arXiv:2407.10362, 2024. Thibault Le Sellier De Chezelles, Maxime Gasse, Alexandre Drouin, Massimo Caccia, Léo Boisvert, Megh Thakkar, Tom Marty, Rim Assouel, Sahar Omidi Shayegan, Lawrence Keunho Jang, Xing Han Lù, Ori Yoran, Dehan Kong, Frank F. Xu, Siva Reddy, Quentin Cappart, Graham Neubig, Ruslan Salakhutdinov, Nicolas Chapados, and Alexandre Lacoste. The BrowserGym ecosystem for web agent research. TMLR, 2025. URL https://openreview.net/forum ?id=5298fKGmv3. Zijun Liu, Kai Liu, Yiqi Zhu, Xuanyu Lei, Zonghan Yang, Zhenhe Zhang, Peng Li, and Yang Liu. AIGS: Generating science from AI-powered automated falsification. ArXiv, abs/2411.11910, 2024. URL https://api.semanticscholar.org/CorpusID:274140961. Chris Lu, Cong Lu, Robert Tjarko Lange, Jakob N. Foerster, Jeff Clune, and David Ha. The AI scientist: Towards fully automated open-ended scientific discovery. ArXiv, abs/2408.06292, 2024. Bodhisattwa Prasad Majumder, Harshit Surana, Dhruv Agarwal, Sanchaita Hazra, Ashish Sabharwal, and Peter Clark. Data-driven discovery with large generative models. ICML, 2024. Bodhisattwa Prasad Majumder, Harshit Surana, Dhruv Agarwal, Bhavana Dalvi Mishra, Abhijeetsingh Meena, Aryan Prakhar, Tirth Vora, Tushar Khot, Ashish Sabharwal, and Peter Clark. DiscoveryBench: Towards data-driven discovery with large language models. In ICLR, 2025. URL https://openreview.net/pdf?id=vyflgpwfJW. 13 Microsoft. AutoGen agent implementations, 2024. URL https://github.com/microsoft /autogen/tree/d4dd4a26ca5c9a7e29307cf2efef7ffec9bd23da/python/pa ckages/autogen-ext/src/autogen_ext/agents. Accessed: 2025-08-25. Jordan Mitchener, Francisco Pineda, Yuxin Ye, Spyros Maniatis, Kenneth Holstein, Kam Dahlquist, James D. Braza, Andrew D. White, and Samuel G. Rodriques. BixBench: comprehensive benchmark for llm-based agents in computational biology. arXiv:2503.00096, 2025. URL https://arxiv.org/abs/2503.00096. Benjamin Newman, Yoonjoo Lee, Aakanksha Naik, Pao Siangliulue, Raymond Fok, Juho Kim, Daniel Weld, Joseph Chee Chang, and Kyle Lo. ArxivDIGESTables: Synthesizing scientific literature into tables using language models. In EMNLP, 2024. URL https://aclantholo gy.org/2024.emnlp-main.538/. Vishakh Padmakumar, Joseph Chee Chang, Kyle Lo, Doug Downey, and Aakanksha Naik. Setting the table with intent: Intent-aware schema generation and editing for literature review tables. arXiv:2507.19521, 2025. Pritika Ramu, Aparna Garimella, and Sambaran Bandyopadhyay. Is this bad table? closer look at the evaluation of table generation from text. In EMNLP, 2024. URL https://aclantholo gy.org/2024.emnlp-main.1239/. Aymeric Roucher, Albert Villanova del Moral, Thomas Wolf, Leandro von Werra, and Erik Kaunismäki. smolagents: smol library to build great agentic systems. https://github.com/h uggingface/smolagents, 2025. Kai Ruan, Xuan Wang, Jixiang Hong, Peng Wang, Yang Liu, and Hao Sun. LiveIdeaBench: Evaluating LLMs divergent thinking for scientific idea generation with minimal context. arXiv:2412.17596, 2024. URL https://arxiv.org/abs/2412.17596. SAgE Team, Princeton University. HAL: Holistic agent leaderboard, 2025. URL https://hal. cs.princeton.edu/#leaderboards. Accessed: 2025-08-25. Samuel Schmidgall, Yusheng Su, Ze Wang, Ximeng Sun, Jialian Wu, Xiaodong Yu, Jiang Liu, Zicheng Liu, and Emad Barsoum. Agent Laboratory: Using LLM agents as research assistants. In arXiv, volume abs/2501.04227, 2025. ServiceNow. BrowserGym leaderboard, 2025. URL https://huggingface.co/spaces/ ServiceNow/browsergym-leaderboard. Accessed: 2025-08-25. Yijia Shao, Yucheng Jiang, Theodore A. Kanell, Peter Xu, Omar Khattab, and Monica S. Lam. Assisting in writing Wikipedia-like articles from scratch with large language models, 2024. URL https://arxiv.org/abs/2402.14207. Xiaofeng Shi, Yuduo Li, Qian Kou, Longbin Yu, Jinxin Xie, and Hua Zhou. SPAR: Scholar paper retrieval with llm-based agents for enhanced academic search. arXiv:2507.15245, 2025. URL https://arxiv.org/abs/2507.15245. Chenglei Si, Diyi Yang, and Tatsunori Hashimoto. Can llms generate novel research ideas? large-scale human study with 100+ nlp researchers. arXiv preprint arXiv:2409.04109, 2024. Zachary S. Siegel, Sayash Kapoor, Nitya Nadgir, Benedikt Stroebl, and Arvind Narayanan. COREBench: Fostering the credibility of published research through computational reproducibility agent benchmark. TMLR, 2025-January:131, 2025. URL https://tmlr.org/papers/v2 025/01-2025paper.pdf. Amanpreet Singh, Joseph Chee Chang, Chloe Anastasiades, Dany Haddad, Aakanksha Naik, Amber Tanaka, Angele Zamarron, Cecile Nguyen, Jena D. Hwang, Jason Dunkleberger, Matt Latzke, Smita Rao, Jaron Lochner, Rob Evans, Rodney Kinney, Daniel S. Weld, Doug Downey, and Sergey Feldman. Ai2 Scholar QA: Organized literature synthesis with attribution. ArXiv, abs/2504.10861, 2025. URL https://api.semanticscholar.org/CorpusID: 277786810. 14 Michael D. Skarlinski, Sam Cox, Jon M. Laurent, James D. Braza, Michaela Hinks, Michael J. Hammerling, Manvitha Ponnapati, Samuel G. Rodriques, and Andrew D. White. Language agents achieve superhuman synthesis of scientific knowledge. arXiv:2409.13740, 2024. URL https://arxiv.org/abs/2409.13740. Introduces the LitQA2 benchmark for evaluating language models on scientific literature research tasks. Giulio Starace, Oliver Jaffe, Dane Sherburn, James Aung, Jun Shern Chan, Leon Maksin, Rachel Dias, Evan Mays, Benjamin Kinsella, Wyatt Thompson, et al. Paperbench: Evaluating ais ability to replicate ai research. arXiv preprint arXiv:2504.01848, 2025. Jiabin Tang, Lianghao Xia, Zhonghang Li, and Chao Huang. AI-Researcher: Autonomous scientific innovation. arXiv:2505.18705, 2025. URL https://arxiv.org/abs/2505.18705. The Terminal-Bench Team. Terminal-bench: benchmark for ai agents in terminal environments, Apr 2025a. URL https://github.com/laude-institute/terminal-bench. The Terminal-Bench Team. Terminal-Bench leaderboard, 2025b. URL https://tbench.ai/ leaderboard. Accessed: 2025-08-25. Minyang Tian, Luyu Gao, Shizhuo Dylan Zhang, Xinan Chen, Cunwei Fan, Xuefei Guo, Roland Haas, Pan Ji, Kittithat Krongchon, Yao Li, Shengyan Liu, Di Luo, Yutao Ma, Hao Tong, Kha Trinh, Chenyu Tian, Zihan Wang, Bohao Wu, Yanyu Xiong, Shengzhu Yin, Minhui Zhu, Kilian Lieret, Yanxin Lu, Genglin Liu, Yufeng Du, Tianhua Tao, Ofir Press, Jamie Callan, Eliu Huerta, and Hao Peng. SciCode: research coding benchmark curated by scientists. arXiv:2407.13168, 2024. URL https://arxiv.org/abs/2407.13168. UK AI Safety Institute and Arcadia Impact and Vector Institute. Inspect Evals: Communitycontributed evaluations for inspect ai. https://github.com/UKGovernmentBEIS/ inspect_evals, 2025. Accessed: 2025-08-24. UK AI Security Institute. Inspect AI: Framework for Large Language Model Evaluations, May 2024. URL https://github.com/UKGovernmentBEIS/inspect_ai. Rosni Vasu, Chandrayee Basu, Bhavana Dalvi Mishra, Cristina Sarasua, Peter Clark, and Abraham Bernstein. HypER: Literature-grounded hypothesis generation and distillation with provenance, 2025. URL https://arxiv.org/abs/2506.12937. Vector Institute. Vector evaluation leaderboard, 2025. URL https://huggingface.co/spa ces/vector-institute/eval-leaderboard. Accessed: 2025-08-25. Hanchen Wang, Tianfan Fu, Yuanqi Du, Wenhao Gao, Kexin Huang, Ziming Liu, Payal Chandak, Shengchao Liu, Peter Van Katwyk, Andreea Deac, et al. Scientific discovery in the age of artificial intelligence. Nature, 620(7972):4760, 2023. Xingyao Wang, Yangyi Chen, Lifan Yuan, Yizhe Zhang, Yunzhu Li, Hao Peng, and Heng Ji. Executable code actions elicit better llm agents. In ICML, 2024. Xingyao Wang, Boxuan Li, Yufan Song, Frank F. Xu, Xiangru Tang, Mingchen Zhuge, Jiayi Pan, Yueqi Song, Bowen Li, Jaskirat Singh, Hoang H. Tran, Fuqiang Li, Ren Ma, Mingzhang Zheng, Bill Qian, Yanjun Shao, Niklas Muennighoff, Yizhe Zhang, Binyuan Hui, Junyang Lin, Robert Brennan, Hao Peng, Heng Ji, and Graham Neubig. OpenHands: An open platform for AI software developers as generalist agents. In ICLR, 2025. URL https://openreview.net/forum ?id=OJd3ayDDoF. Yanzheng Xiang, Hanqi Yan, Shuyin Ouyang, Lin Gui, and Yulan He. Scireplicate-bench: Benchmarking llms in agent-driven algorithmic reproduction from research papers. Proceedings of COLM, 2025. Tianze Xu, Pengrui Lu, Lyumanshan Ye, Xiangkun Hu, and Pengfei Liu. ResearcherBench: Evaluating deep AI research systems on the frontiers of scientific inquiry. arXiv:2507.16280, 2025. URL https://arxiv.org/abs/2507.16280. 15 Yutaro Yamada, Robert Tjarko Lange, Cong Lu, Shengran Hu, Chris Lu, Jakob Nicolaus Foerster, Jeff Clune, and David Ha. The AI Scientist-v2: Workshop-level automated scientific discovery via agentic tree search. ArXiv, abs/2504.08066, 2025. URL https://api.semanticscholar. org/CorpusID:277741107. Shuo Yan, Ruochen Li, Ziming Luo, Zimu Wang, Daoyang Li, Liqiang Jing, Kaiyu He, Peilin Wu, George Michalopoulos, Yue Zhang, et al. Lmr-bench: Evaluating llm agents ability on reproducing language modeling research. arXiv preprint arXiv:2506.17335, 2025. Asaf Yehudai, Lilach Eden, Alan Li, Guy Uziel, Yilun Zhao, Roy Bar-Haim, Arman Cohan, and Michal Shmueli-Scheuer. Survey on evaluation of LLM-based agents. arXiv:2503.16416, 2025. URL https://arxiv.org/abs/2503.16416. Xuanle Zhao, Zilin Sang, Yuxuan Li, Qi Shi, Weilun Zhao, Shuo Wang, Duzhen Zhang, Xu Han, Zhiyuan Liu, and Maosong Sun. Autoreproduce: Automatic ai experiment reproduction with paper lineage. arXiv preprint arXiv:2505.20662, 2025. Kunlun Zhu, Jiaxun Zhang, Ziheng Qi, Nuoxing Shang, Zijia Liu, Peixuan Han, Yue Su, Haofei Yu, and Jiaxuan You. SafeScientist: Toward risk-aware scientific discoveries by LLM agents. arXiv:2505.23559, 2025. URL https://arxiv.org/abs/2505.23559."
        },
        {
            "title": "A PRINCIPLES FOR BENCHMARKING AGENTS",
            "content": "We propose the following principles for more rigorously benchmarking agents: 1. The task suite must represent the complexity of real-world usage. In order to determine whether agents can serve as effective assistants for use case, it is necessary to test broad range of relevant tasks. Real-world product usage provides an informative basis for determining appropriate tasks, but unfortunately such data is typically guarded by product companies (who use it to create private evaluations) and unavailable to academic benchmark creators. Moreover, in order to measure progress towards broadly capable agents, the task suite should require exercising range of advanced, general skills such as reasoning, planning, tool use, search, coding, and data analysis. 2. standard, realistic, and reproducible environment and tools must accompany the suite for controlled comparison of AI capabilities. The environment should be realistic to measure agents ability to act in the real world. At the same time, the environment and tools must be standard and reproducible to facilitate controlled comparison across different agents. Most existing benchmark suites lack standard tools, leading agent developers to use disparate environments and tools that obscure whether performance differences are due to superior AI capabilities or other enhancements. It is particularly important that benchmark suites provide standard search tools with reproducible test-time access to the same document corpus, yet large-scale, optimized search indexes are costly to create and public search tools are not reproducible; we are unaware of any such public, reproducible, large-scale search tools. 3. Reporting must account for confounding variablesespecially computational cost and tool usage. Its essential to account for cost, since even simplistic strategies, such as repeating task many times and taking majority votes, can boost accuracy by burning cash. Controlling for tool usage is also essential to separate gains due to model or agent architecture advancements from benefits due to privileged access to specialized information sources. 4. Task interfaces must be standardized to facilitate integration of general agents. General agents that can perform many different tasks are likely to better meet diverse real-world needs. Unfortunately, most previous benchmark suites require general agent developers to adapt agents for individual tasks, introducing developer bias and hindering development. To support the development of general agents, task interfaces should provide reasonable accommodation for an intelligent agent that has not been developed specifically for the test tasks: complete task instructions, task-required tools, and submission affordancesall in standard format. 5. Comprehensive agent baselines with standard interfaces are needed to measure stateof-the-art. large integrated suite of agent baselines must be available to identify which agents are truly state-of-the-art agents and to provide high-quality starting points for future development, yet is lacking from current agent suites resulting in most evaluations comparing only to small number of other agents or ablations on the evaluators own agent. EVALUATION TOOLKIT: OPENNESS AND TOOLING Definitions for the Agent openness and Agent tooling classifications for baseline: Agent openness describes the transparency and reproducibility of an agents implementation: Open-source, open-weight (): Both agent code and ML model weights are publicly available, enabling full end-to-end reproducibility. Open-source, closed-weight (): Agent code is available but relies on proprietary ML models, allowing partial reproducibility of the approach. Closed source & API available (A): Implementation details are proprietary, but the system is accessible via API, enabling result verification but not method reproduction. Closed & UI only (): Neither code nor programmatic API access is available. Agent tooling describes the tool usage and execution environment of an agent during evaluation: Standard (): Uses only predefined tools from the evaluation environment (as defined in Inspects state.tools). Custom interface (): Uses custom tools for accessing an equivalent underlying environment, which for AstaBench we define as task-relevant portions of the Asta Environment: * Literature tasks: Information access is limited to date-restricted usage of the Asta Scientific Corpus. * Code tasks: Code execution is limited to an IPython shell in machine environment initialized with the standard Asta Environment sandbox Dockerfile (or equivalent). Fully custom (): Uses tools beyond constraints of Standard or Custom interface. 17 Table 4: Overall results for agents that can solve all the tasks (additional results in Table 11). Reported values are macro averages over benchmark statistics; confidence intervals are omitted. denotes models not pinned to date-stamped version. Bold denotes the agent is on Pareto-optimal frontier for that column pair. Agent Model Asta v0 ReAct ReAct Smolagents Coder Smolagents Coder ReAct ReAct Smolagents Coder mixture gpt-5 o3 claudesonnet-4 gpt-5 gpt-5-mini claude-3-5haiku llama-4scout Overall Literature Understanding Code & Execution Data Analysis End-to-End Discovery Score Cost Score Cost Score Cost Score Cost Score Cost 53.0 44.0 39.4 38.1 3.40 0.31 0.16 1.02 62.2 54.6 46.8 42.7 0.58 0.30 0.35 0.71 47.6 55.0 49.3 39. 0.19 0.35 0.19 1.96 33.2 30.5 33.7 28.8 0.25 0.09 0.04 0.24 68.8 36.1 28.0 41.5 12.57 0.49 0.07 1.19 37. 0.13 46.0 0.12 30.9 0.10 26. 0.08 46.5 0.22 31.6 21.9 0.04 0.04 36.5 36. 0.05 0.03 50.5 22.4 0.05 0.05 26.9 24.3 0.01 0.01 12.6 4. 0.03 0.04 11.1 0.11 20.0 0.03 3. 0.12 20.2 0.01 0.5 0.27 Table 5: Literature Understanding search benchmarks results (additional results in Table 12). denotes models not pinned to date-stamped version. Bold denotes the agent is on Pareto-optimal frontier for that column pair. Agent Model PaperFindingBench LitQA2-FullText-Search Asta Paper Finder Asta v0 ReAct ReAct Smolagents Smolagents You.com Coder Coder Search API gemini-2-flash, gpt-4o Score Cost Score Cost 39.7 3.1 0.063 0.005 90.7 6.6 0.112 0.007 mixture gpt-5 o3 gpt-4. 37.6 3.1 26.4 3.9 19.3 3.7 16.5 3.5 0.063 0.005 0.428 0.048 0.518 0.067 0.080 0.007 90.7 6.6 82.7 8.6 57.3 11.3 50.7 11.4 0.112 0.007 0.389 0.055 0.790 0.127 0.095 0.037 claude-sonnet4 22.1 3. 0.975 0.139 52.0 11.4 1.100 0.097 7.2 2.0 36.0 10."
        },
        {
            "title": "C SUPPORTING EXPERIMENTAL RESULTS",
            "content": "This section contains supplemental tables and figures for the narrative in Section 5. Table 4 shows the overall results for those agents attempting all benchmarks, as well as agents that can solve all the benchmarks in at least one category. We then show category-specific results, for Literature Understanding (Tables 5 to 7), Code and Execution  (Table 8)  , Data Analysis  (Table 9)  , and End-toEnd Discovery  (Table 10)  . For details about referenced agents and models, refer to Tables 3 and 18, respectively. In the Tables, denotes Openness, with values (Open-source, open-weight), (Open-source, closed-weight), (Closed source & API available), and (Closed & UI only). denotes Tooling, with values (Standard), (Custom interface), and (Fully custom). The openness values apply to the agent (including the model used). denote 95% confidence intervals. Bold denotes the agent is on Pareto-optimal frontier for that column pair. Our results reveal several noteworthy insights. 18 Table 6: Literature Understanding QA benchmarks results (additional results in Table 13). Agents without an API could not be evaluated on LitQA2-FT. denotes models not pinned to date-stamped version. Bold denotes the agent is on Pareto-optimal frontier for that column pair. Agent Model ScholarQA-CS2 LitQA2-FullText ReAct Asta v0 FutureHouse Crow FutureHouse Falcon ReAct Smolagents Coder Perplexity Sonar Deep Research Smolagents Coder You.com Research API Asta Scholar QA (w/ Tables) Asta Scholar QA QA Asta Scholar Asta Scholar QA Elicit SciSpace Deep Review STORM OpenAI Deep Research OpenSciLM gpt-5 mixture gpt-4.1-mini, o3-mini, gemini2.5-flash gpt-4.1-mini, gemini-2.5flash, o3-mini o3 gpt-5 gemini-2.5flash, sonar-deepresearch gpt-4.1 Score Cost Score Cost 79.8 3.5 87.7 1.4 81.1 1.7 0.373 0.034 1.529 0.291 0.107 0.004 82.7 8.6 70.7 10.4 72.0 10.2 0.276 0.114 0.306 0.093 0.065 0.003 77.6 1. 0.403 0.051 74.7 9.9 0.220 0.011 66.4 3.0 68.4 4.4 0.275 0.039 0.154 0.014 80.0 9.1 73.3 10. 0.347 0.083 0.101 0.026 67.3 1.2 0.416 0.019 73.3 10.1 0.219 0.016 73.7 2. 0.080 0.016 65.3 10.8 0.035 0.005 55.0 2.2 8.0 6.2 claude-sonnet4 gemini-2.5flash claude-sonnet4 gpt-5 claude-sonnet4 gpt-3.5-turbo, gpt-4o o3-/o4-minideep-research, gemini-2.5-pro llama-3.1openscholar-8b 87.9 1.2 1.314 0.281 87.7 1. 0.126 0.010 86.2 1.4 0.393 0.030 85.9 1.6 1.099 0.074 85.5 1.6 84.6 1. 78.3 2.4 0.094 0.002 79.4 1.4 1.803 0.039 58.0 2. 0.004 0.000 Table 7: Literature Understanding ArxivDIGESTables-Clean task benchmark results (additional results in Table 14). denotes models not pinned to date-stamped version. Bold denotes the agent is on Pareto-optimal frontier for that column pair. Agent Model ArxivDIGESTables-Clean Asta v0 Asta Table Synthesis Asta Table Synthesis ReAct Smolagents Coder mixture gpt-5 gpt-5-mini o3 gpt-5 Score 42.9 3.7 42.6 3.5 41.7 3.7 32.9 3.3 31.5 3.2 Cost 0.517 0.056 1.281 0.140 0.172 0.019 0.050 0.004 0.060 0.004 denotes models Table 8: Code & Execution category results (additional results in Table 15). not pinned to date-stamped version. Bold denotes the agent is on Pareto-optimal frontier for that column pair. Agent Model SUPER-Expert CORE-Bench-Hard DSReAct ReAct Asta v0 Smolagents Smolagents Coder Coder gpt-5 o3 mixture claudesonnet-4 gpt-5 Coder Smolagents claude-35-haiku Asta Code gpt-4.1 Asta Code gpt-5 Score Cost Score Cost Score Cost 41.1 12.9 0.589 0.140 45.9 16.3 0.443 0.139 78.0 2.7 16.3 9.6 0.369 0.097 56.8 16.2 0.196 0.076 74.9 2.8 19.4 10.4 0.332 0.057 48.6 16.3 0.226 0.093 74.8 2.8 11.7 8.0 3.559 1.766 32.4 15.3 2.199 0.780 74.7 2.8 0.021 0.0009 0.010 0.0007 0.011 0.0007 0.114 0.0079 3.6 4.8 0.079 0.023 13.5 11.2 0.190 0.106 75.7 2.8 0.019 0. 16.8 9.6 0.812 0.581 0.0000 0.332 0.210 9.9 2.0 0.024 0.0103 16.3 9.4 0.285 0.059 13.5 9.4 0.372 0. Table 9: Data Analysis DiscoveryBench results (additional results in Table 16). denotes models not pinned to date-stamped version. Bold denotes the agent is on Pareto-optimal frontier for that column pair. Agent Model DiscoveryBench Score Cost ReAct Asta v0 Asta DataVoyager ReAct Smolagents Coder o3 mixture o3, gpt-4o gpt-5 claude-sonnet-4 33.7 5.1 33.2 5.1 31.1 5.0 30.5 4.8 28.8 4.8 0.039 0.004 0.246 0.071 0.234 0.061 0.092 0.009 0.237 0.019 Table 10: End-to-End Discovery category results (additional results in Table 17). denotes models not pinned to date-stamped version. Bold denotes the agent is on Pareto-optimal frontier for that column pair. Agent Model E2E-Bench E2E-Bench-Hard Asta Panda Asta v0 Asta CodeScientist Smolagents Coder ReAct Smolagents Coder Faker ReAct ReAct claudesonnet-4 mixture claude-3-7sonnet gpt-5 claudesonnet-4 claudesonnet-4 gpt-4.1 o3 gpt-5 Score Cost Score Cost 70.5 6.2 10.643 0.717 68.2 4.4 14.487 1.050 70.4 6.3 65.3 7. 10.643 0.717 2.760 0.510 67.3 5.3 64.5 5.5 14.487 1.050 3.549 0.692 62.8 9.8 0.205 0.025 30.3 10. 0.232 0.043 52.5 6.8 0.749 0.072 38.9 6.9 0.836 0.057 47.2 6. 0.873 0.110 35.8 7.8 1.512 0.307 39.2 6.9 34.9 10.1 30.0 11.9 0.026 0.001 0.065 0.010 0.403 0.053 25.4 4.5 21.0 7.6 42.1 11. 0.029 0.001 0.075 0.019 0.584 0.072 20 Table 11: Overall results for agents that can solve all the tasks. Reported values are macro averages denotes models not pinned to over benchmark statistics; confidence intervals are omitted. date-stamped version. Agent Model Overall Literature Understanding Code & Execution Data Analysis End-to-End Discovery Score Cost Score Cost Score Cost Score Cost Score Cost ReAct ReAct ReAct ReAct ReAct ReAct ReAct ReAct ReAct Smolagents Coder Smolagents Coder Smolagents Coder Smolagents Coder Smolagents Coder Smolagents Coder Smolagents Coder Smolagents Coder Asta 40.1 0.40 45.4 21.9 0.04 36.2 claude-35-haiku claudesonnet-4 gpt-4.1 31.6 0.20 46.4 gpt-4o 16.2 0.12 31.8 gpt-5-mini 31.6 0.04 36.5 gpt-5 44.0 0.31 54.6 gemini15.3 0.71 32.8 2.5-flash llama-4scout o3 claude-35-haiku claudesonnet-4 gpt-4.1 39.4 0.16 46.8 12.7 0.30 20.9 38.1 1.02 42.7 7.9 0.68 19. 32.8 0.32 43.9 0.03 22.4 0.05 24.3 0.01 4. 0.04 0.36 46.2 0.33 23.2 0.13 45.7 0. 0.54 0.15 0.05 0.30 0.46 32.4 18.3 50.5 55.0 26.0 0.09 0.15 0.05 0.35 0.45 30.5 0.02 17.1 13.2 0.04 1.5 26.9 0.01 12.6 30.5 0.09 36.1 0.5 1.9 0.10 0.14 0.15 0.03 0.49 1. 1.60 4.8 0.10 5.9 0.19 1.4 0. 0.35 0.05 49.3 8.9 0.19 0.39 33.7 0.04 28.0 4.5 16.5 0.02 0.07 0.73 0. 39.6 1.96 28.8 0.24 41.5 1.19 0.07 25. 0.11 28.4 0.05 33.3 1.07 gpt-4o 13.6 0.36 22.7 0. 8.7 0.64 17.8 0.05 5.3 0.67 gpt-5-mini 29.1 0.06 38. 0.02 28.3 0.09 27.7 0.07 22.0 0.08 gpt37.5 0.13 46.0 0.12 30.9 0.10 26.7 0.08 46.5 0. gemini2.5-flash llama-4scout mixture 26.4 0.71 35.6 0.05 16.6 0.56 24.7 0.02 28. 2.21 11.1 0.11 20.0 0.03 3.6 0.12 20.2 0. 0.5 0.27 53.0 3.40 62.2 0.58 47.6 0. 33.2 0.25 68.8 12."
        },
        {
            "title": "D FULL EXPERIMENTAL RESULTS",
            "content": "Section 5 presented results for the best agents (i.e., agents running with the best underlying model), plus few additional important data points. Here we show the full set of results for all configurations of agents that were tested (a superset of the results in Section 5). We also show plots of scores vs. costs, including the Pareto frontier (showing the best agent for given cost). In the Tables, denotes Openness, with values (Open-source, open-weight), (Open-source, closed-weight), and (Closed & UI only). denotes Tooling, with values (Standard), (Custom interface), and (Fully custom). denote 95% confidence intervals. 21 Table 12: Literature Understanding search benchmarks results. denotes models not pinned to date-stamped version. Agent Model PaperFindingBench LitQA2-FullText-Search ReAct ReAct ReAct ReAct ReAct ReAct ReAct Coder Coder Coder Coder Coder Coder Coder Coder ReAct ReAct Smolagents Smolagents Smolagents Smolagents Smolagents Smolagents Smolagents Smolagents Asta v0 Asta Paper Finder You.com Search API claude-3-5haiku claudesonnet-4 gpt-4.1 gpt-4o gpt-5-mini gpt-5 gemini-2.5flash llama-4-scout o3 claude-3-5haiku claudesonnet-4 gpt-4.1 gpt-4o gpt-5-mini gptgemini-2.5flash llama-4-scout mixture gemini-2flash, gpt-4o ? Score Cost Score Cost 10.7 2.6 0.061 0.005 60.0 11.2 0.069 0.007 20.3 3.2 0.541 0. 46.7 11.4 0.606 0.031 16.5 3.2 12.9 3.3 22.0 3.7 26.4 3.9 6.5 2.3 5.4 2.2 19.3 3.7 4.6 2.1 0.867 0.183 0.267 0.032 0.060 0.009 0.428 0.048 1.196 0.214 2.816 0.319 0.518 0.067 0.070 0. 65.3 10.8 66.7 10.7 56.0 11.3 82.7 8.6 57.3 11.3 37.3 11.0 57.3 11.3 2.7 3.7 0.819 0.258 0.328 0.081 0.118 0.026 0.389 0.055 0.650 0.400 4.326 0.795 0.790 0.127 0.096 0.060 22.1 3.5 0.975 0. 52.0 11.4 1.100 0.097 16.5 3.5 0.080 0.007 50.7 11.4 0.095 0. 12.5 3.5 0.098 0.010 20.0 9.1 0.105 0.023 17.2 3.2 0.034 0. 48.0 11.4 0.036 0.010 20.0 3.9 0.121 0.012 54.7 11.3 0.152 0. 14.7 3.4 0.044 0.006 36.0 10.9 0.089 0.100 7.0 2.9 0.013 0. 6.7 5.7 0.010 0.001 37.6 3.1 0.063 0.005 90.7 6.6 0.112 0. 39.7 3.1 0.063 0.005 90.7 6.6 0.112 0.007 7.2 2.0 ? 36.0 10.9 ? 22 Figure 3: Score vs. cost analysis for Literature Understanding search benchmarks  (Table 12)  . Points indicate means; error bars denote 95% confidence intervals. Points on the Pareto frontier are connected with dotted lines, representing optimal quality-cost trade-offs for each eval (PaperFindingBench, LitQA2-FullText-Search). Note: the x-axis (cost) uses log scale. 23 Table 13: Literature Understanding QA benchmarks results. Agents without an API could not be evaluated on LitQA2-FT. Models in parentheses indicate self-reported models. denotes models not pinned to date-stamped version. Agent Model ScholarQA-CS2 LitQA2-FullText ReAct ReAct ReAct ReAct ReAct ReAct ReAct ReAct ReAct Smolagents Coder claude-3-5Smolagents Coder claudeSmolagents Coder gpt-4.1 Smolagents Coder gpt-4o Smolagents Coder gpt-5-mini Smolagents Coder gpt-5 Smolagents Coder gemini-2.5Smolagents Coder llama-4Asta v0 Asta Scholar QA (w/ Tables) Asta Scholar QA (w/ Tables) Asta Scholar QA Asta Scholar QA Asta Scholar QA Asta Scholar QA Elicit Perplexity Sonar Deep Research You.com Research API SciSpace Deep Review OpenSciLM OpenAI Deep Research FutureHouse Falcon STORM FutureHouse Crow gpt-4.1-mini, claude-3-5haiku claudesonnet-4 gpt-4.1 gpt-4o gpt-5-mini gpt-5 gemini-2.5flash llama-4scout o3 haiku sonnet-4 flash scout mixture o3 claudesonnet-4 claudesonnet-4 gemini-2.5flash gpt-4o-mini gpt-5 gemini-2.5flash, sonar-deepresearch claudesonnet-4 llama-3.1openscholar8b o3-/o4-minideepresearch, gemini-2.5pro o3-mini, gemini-2.5flash gpt-4.1-mini, gemini-2.5flash, o3-mini gpt-3.5turbo, gpt-4o Score Cost Score Cost 66.3 2.8 0.019 0.001 32.0 10.6 0.022 0. 78.3 2.2 0.390 0.019 68.0 10.6 0.238 0.026 70.1 3.2 53.3 3.4 26.7 7.4 79.8 3.5 52.8 8.0 0.733 0.243 0.101 0.012 0.027 0.004 0.373 0.034 0.063 0. 77.3 9.5 22.7 9.5 74.7 9.9 82.7 8.6 36.0 10.9 0.222 0.097 0.046 0.015 0.075 0.029 0.276 0.114 0.436 0.140 24.8 5.1 0.588 0.144 41.3 11.2 0.170 0. 66.4 3.0 49.9 4.4 0.275 0.039 0.042 0.004 80.0 9.1 25.3 9.9 0.347 0.083 0.056 0.010 72.4 2.1 0.794 0. 50.7 11.4 0.627 0.066 73.7 2.1 46.3 4.0 57.3 5.3 68.4 4.4 63.7 4.6 0.080 0.016 0.078 0.008 0.020 0.002 0.154 0.014 0.080 0.044 65.3 10.8 14.7 8.1 50.7 11.4 73.3 10.1 41.3 11.2 0.035 0.005 0.050 0.010 0.015 0.005 0.101 0.026 0.034 0. 39.6 4.8 0.008 0.001 42.7 11.3 0.013 0.002 87.7 1.4 1.529 0. 70.7 10.4 0.306 0.093 88.7 1.2 2.932 0.408 87.9 1.2 1.314 0. 86.2 1.4 0.393 0.030 87.7 1.4 0.126 0.010 78.5 1.9 85.9 1.6 85.5 1.6 67.3 1.2 0.012 0.001 1.099 0.074 0.416 0.019 73.3 10.1 0.219 0.016 55.0 2.2 84.6 1. 58.0 2.6 0.004 0.000 79.4 1.4 1.803 0. 8.0 6.2 81.1 1.7 0.107 0.004 72.0 10.2 0.065 0. 77.6 1.3 0.403 0.051 74.7 9.9 0.220 0.011 78.3 2.4 0.094 0. 24 Figure 4: Score vs. cost analysis for Literature Understanding QA benchmarks  (Table 13)  . Points indicate means; error bars denote 95% confidence intervals. Points on the Pareto frontier are connected with dotted lines, representing optimal quality-cost trade-offs for each eval (ScholarQACS2, LitQA2-FullText). Note: the x-axis (cost) uses log scale. denotes models not pinned to date-stamped version. 25 Table 14: Literature Understanding ArxivDIGESTables-Clean task benchmark results. Agent Model ArxivDIGESTables-Clean ReAct ReAct ReAct ReAct ReAct ReAct ReAct ReAct ReAct Smolagents Coder Smolagents Coder Smolagents Coder Smolagents Coder Smolagents Coder Smolagents Coder Smolagents Coder Smolagents Coder Asta v0 Asta Table Synthesis Asta Table Synthesis Asta Table Synthesis Asta Table Synthesis Asta Table Synthesis Asta Table Synthesis Asta Table Synthesis Asta Table Synthesis Asta Table Synthesis claude-3-5-haiku claude-sonnet-4 gpt-4.1 gpt-4o gpt-5-mini gpt-5 gemini-2.5-flash llama-4-scout o3 claude-3-5-haiku claude-sonnet-4 gpt-4.1 gpt-4o gpt-5-mini gpt-5 gemini-2.5-flash llama-4-scout mixture gpt-4.1 claude-3-5-haiku claude-sonnet-4 gemini-2.5-flash o3 gemini-2.5-pro llama-4-scout gpt-5 gpt-5-mini Score 21.7 2.6 25.5 3.1 27.5 3.2 16.3 2.4 32.1 3.3 29.4 3.7 25.2 3.1 9.5 2.3 32.9 3.3 15.0 2.8 24.8 2.9 27.2 3.1 14.6 2.5 30.0 3.2 31.5 3.2 25.2 2.8 8.7 2.2 42.9 3.7 38.8 3.5 31.1 3.6 37.2 3.3 34.4 3.3 41.6 3.5 35.4 3.5 26.4 3.3 42.6 3.5 41.7 3.7 Cost 0.013 0.001 0.069 0.005 0.038 0.004 0.055 0.005 0.013 0.001 0.064 0.005 0.022 0.002 0.760 0.102 0.050 0.004 0.017 0.003 0.204 0.018 0.044 0.005 0.051 0.007 0.009 0.001 0.060 0.004 0.021 0.002 0.099 0.087 0.517 0.056 0.347 0.038 0.165 0.018 0.676 0.074 0.133 0.015 0.517 0.056 0.993 0.158 0.025 0.003 1.281 0.140 0.172 0.019 26 Figure 5: Score vs. cost analysis for the Literature Understanding ArxivDIGESTables-Clean benchmark  (Table 14)  . Points indicate means; error bars denote 95% confidence intervals. Points on the Pareto frontier are connected with dotted lines, representing optimal quality-cost trade-offs for each eval. Note: the x-axis (cost) uses log scale. denotes models not pinned to date-stamped version. 27 Table 15: Code & Execution category results. Agent Model SUPER-Expert CORE-Bench-Hard DS-1000 ReAct ReAct ReAct ReAct ReAct ReAct ReAct ReAct ReAct Smolagents Coder Smolagents Coder Smolagents Coder Smolagents Coder Smolagents Coder Smolagents Coder Smolagents Coder claude-35-haiku claudesonnet-4 gpt-4.1 gpt-4o gpt-5mini gpt-5 gemini2.5flash llama-4scout o3 claude-35-haiku claudesonnet-4 gpt-4. gpt-4o gpt-5mini gpt-5 gemini2.5flash llama-4scout mixture Smolagents Coder Asta v0 Asta Code gpt-4.1 Asta Code gpt-4o Asta Code gpt-5 Asta Code gpt-5mini Score Cost Score Cost Score Cost 13.1 8.3 0.077 0. 0.0000 0.077 0.021 54.1 3.3 0.006 0.0002 22.6 11.1 0.448 0.087 40.5 16.0 0.499 0.081 75.6 2.8 0.044 0.0020 11.2 7.5 0.156 0.069 18.9 12.8 0.119 0.035 67.0 3.1 5.9 6.7 0.319 0.069 5.4 7.4 0.124 0.041 43.7 3.2 34.6 13.2 0.105 0.046 45.9 16.3 0.047 0.014 71.0 3. 0.008 0.0003 0.010 0.0006 0.003 0.0001 41.1 12.9 0.589 0.140 45.9 16.3 0.443 0.139 78.0 2.7 2.7 5.3 0.470 0.214 55.4 3.2 20.0 10.7 0.875 0.295 0.021 0.0009 0.019 0.0032 4.7 5.2 0.175 0.066 0.0000 0.027 0. 9.7 1.9 0.110 0.0077 16.3 9.6 0.369 0.097 56.8 16.2 0.196 0.076 74.9 2.8 9.9 2.0 16.8 9.6 0.812 0.581 0.0000 0.332 0.210 0.010 0.0007 0.024 0.0103 11.7 8.0 3.559 1.766 32.4 15.3 2.199 0.780 74.7 2. 0.114 0.0079 7.0 6.9 0.149 0.166 21.6 13.4 0.098 0.031 48.0 3.3 0.073 0.0230 3.9 4.9 1.351 0.715 5.4 7.4 0.419 0.410 16.8 2.4 0.137 0. 14.2 8.9 0.240 0.207 5.4 7.4 0.014 0.004 65.2 3.1 0.016 0.0046 3.6 4.8 0.079 0.023 13.5 11.2 0.190 0.106 75.7 2.8 0.019 0.0007 7.5 6.0 0.796 0.945 13.5 11.2 0.832 0.710 28.9 3. 0.044 0.0127 8.1 7.0 0.323 0.377 0.0000 0.046 0.034 2.7 1.1 0.004 0. 19.4 10.4 0.332 0.057 48.6 16.3 0.226 0.093 74.8 2.8 0.011 0.0007 16.3 9.4 0.285 0.059 5.6 6.4 0.464 0.113 13.5 9.4 0.372 0.072 12.8 9.1 0.067 0.014 28 Figure 6: Score vs. cost analysis for Code & Execution benchmarks  (Table 15)  . Points indicate means; error bars denote 95% confidence intervals. Points on the Pareto frontier are connected with dotted lines, representing optimal quality-cost trade-offs for each eval (CORE-Bench-Hard, SUPER-Expert, DS-1000). Note: the x-axis (cost) uses log scale. denotes models not pinned to date-stamped version. 29 Table 16: Data Analysis DiscoveryBench results. T Agent Model ReAct ReAct ReAct ReAct ReAct ReAct ReAct ReAct ReAct Smolagents Coder Smolagents Coder Smolagents Coder Smolagents Coder Smolagents Coder Smolagents Coder Smolagents Coder Smolagents Coder Asta v0 Asta DataVoyager Asta DataVoyager Asta DataVoyager Asta DataVoyager Asta DataVoyager claude-3-5-haiku claude-sonnet-4 gpt-4.1 gpt-4o gpt-5-mini gpt-5 gemini-2.5-flash llama-4-scout o3 claude-3-5-haiku claude-sonnet-4 gpt-4.1 gpt-4o gpt-5-mini gpt-5 gemini-2.5-flash llama-4-scout mixture gpt-4.1, gpt-4o claude-sonnet-4, gpt-4o o3, gpt-4o gpt-5:effort=minimal, gpt-4o gpt-5, gpt-4o DiscoveryBench Score Cost 24.3 4.7 23.2 4.1 30.5 5.1 13.2 3.7 26.9 4.8 30.5 4.8 1.9 1.7 5.9 2.6 33.7 5.1 16.5 4.1 28.8 4.8 28.4 4.9 17.8 4.2 27.7 4.9 26.7 4.7 24.7 4.7 20.2 4.5 33.2 5. 29.9 5.0 25.7 4.6 0.012 0.001 0.132 0.009 0.025 0.003 0.040 0.010 0.011 0.001 0.092 0.009 0.101 0.007 0.192 0.021 0.039 0.004 0.024 0.007 0.237 0.019 0.045 0.018 0.054 0.004 0.071 0.041 0.077 0.006 0.017 0.007 0.008 0.002 0.246 0.071 0.147 0.020 0.523 0.050 31.1 5.0 27.0 4.7 0.234 0.061 0.215 0.029 29.6 4. 0.354 0.075 30 Figure 7: Score vs. cost analysis for Data Analysis sub-benchmarks. Points indicate means; error bars denote 95% confidence intervals. Points on the Pareto frontier are denoted with red triangle markers, representing optimal quality-cost trade-offs for each eval (DiscoveryBench). denotes models not pinned to date-stamped version. 31 Table 17: End-to-End Discovery category results. Agent Model E2E-Bench E2E-Bench-Hard ReAct ReAct ReAct ReAct ReAct ReAct ReAct ReAct ReAct Smolagents Coder Smolagents Coder Smolagents Coder Smolagents Coder Smolagents Coder Smolagents Coder Smolagents Coder Smolagents Coder Asta v0 Faker Asta Panda Asta Panda Asta CodeScientist claude-3-5haiku claudesonnet-4 gpt-4.1 gpt-4o gpt-5-mini gpt-5 gemini-2.5flash llama-4scout o3 claude-3-5haiku claudesonnet-4 gpt-4. Score Cost Score Cost 4.5 2.8 0.042 0. 4.8 3.4 0.048 0.011 52.5 6.8 0.749 0.072 38.9 6.9 0.836 0. 19.3 7.3 1.6 1.7 9.5 7.6 30.0 11.9 0.0000 0.132 0.024 0.157 0.035 0.030 0.006 0.403 0.053 2.401 1.149 14.8 6.8 1.4 1.9 15.7 8.3 42.1 11.4 1.1 2.1 0.139 0.034 0.135 0.028 0.040 0.008 0.584 0.072 1.263 0.672 1.9 2.1 0.818 0. 0.9 1.1 0.813 0.144 34.9 10.1 5.3 3.1 0.065 0.010 0.946 0.560 21.0 7.6 3.7 2.4 0.075 0.019 0.505 0. 47.2 6.1 0.873 0.110 35.8 7.8 1.512 0.307 36.6 9.3 0.178 0. 30.0 7.7 1.955 1.773 gpt-4o 5.4 3.9 0.473 0.347 5.1 3. 0.866 0.757 gpt-5-mini 22.3 9.6 0.076 0.114 21.6 7.5 0.076 0.108 gpt62.8 9.8 0.205 0.025 30.3 10.5 0.232 0.043 gemini-2.5flash llama-4scout mixture gpt-4.1 gpt-4.1 claudesonnet-4 claude-3-7sonnet 34.0 10. 1.877 0.830 23.2 7.8 2.541 1.203 0.2 0.3 0.283 0.152 0.7 0. 0.251 0.181 70.4 6.3 10.643 0.717 67.3 5.3 14.487 1.050 39.2 6.9 36.6 7.7 70.5 6. 0.026 0.001 7.610 1.650 10.643 0.717 25.4 4.5 39.3 7.0 68.2 4.4 0.029 0.001 9.319 1.243 14.487 1.050 65.3 7.1 2.760 0.510 64.5 5. 3.549 0.692 32 Figure 8: Score vs. cost analysis for End-to-End Discovery benchmarks  (Table 17)  . Points indicate means; error bars denote 95% confidence intervals. Points on the Pareto frontier are connected with dotted lines, representing optimal quality-cost trade-offs for each eval (E2E-Bench, E2E-Benchdenotes models not pinned to date-stamped Hard). Note: the x-axis (cost) uses log scale. version."
        },
        {
            "title": "E EVALUATIONS",
            "content": "E.1 SHORT DESCRIPTIONS PaperFindingBench PaperFindingBench tests an agents ability to handle challenging scientific search queries. Given textual query string, the task is to return ranked list of papers that satisfy the query. This new benchmark is subset of our own internal evaluation for our literature-search agent (Asta Paper Finder). Unlike existing paper-finding benchmarks, which are restricted to semantic search queries, our dataset includes metadata and navigational queries along with diverse mix of semantic queries. The queries are sourced from PaperFinder14 and OpenSciLM15 user logs and the LitSearch (Ajith et al., 2024) and PaSa (He et al., 2025) datasets. Evaluating retrieval tasks is challenging, and our chosen evaluation metrics along with other benchmark details are discussed in appendix E.2. Briefly, navigational and metadata queries are evaluated in terms of F1 over the result set, and semantic queries use the harmonic mean of estimated recall and nDCG. The final evaluation metric is an average of per-query scores. LitQA2-FullText/LitQA2-FullText-Search These two benchmarks measure an agents ability to answer questions and retrieve papers within the biomedical domain. They are based on the LitQA2 dataset (Skarlinski et al., 2024), which contains 199 multiple-choice questions, each associated with target paper whose full-text can potentially answer the question. To enable fair comparison for agents using our standard retrieval tools, we filter the original dataset to subset of 85 questions where the associated relevant paper is available in our Asta Scientific Corpus snippet search index within the specified cutoff date (see Table 2). Following Skarlinski et al. (2024), LitQA2-FullText evaluates in terms of accuracy, the fraction of questions with correct answer. LitQA2-FullText-Search isolates the retrieval task aimed at finding papers such that one of them is the target paper for the question, and evaluates on recall@30 (as used in Skarlinski et al. (2024)). To avoid double-counting this benchmark when computing aggregate macro-averaged Literature Understanding scores (compared to other benchmarks in that category), we weight each of these two evals by 0.5 in the macro-average. For additional details and comparisons, see appendix G.2. ScholarQA-CS2 The ScholarQA-CS2 benchmark tests an agents ability to answer long-form scientific questions. Given complex scientific question like How is diversity typically evaluated in recommendation systems? the task is to identify relevant prior work and compose long-form answer report that appropriately cites sources. ScholarQA-CS2 is new benchmark that builds upon the recent ScholarQA-CS (Asai et al., 2024) by incorporating real scientific queries and introducing four facets for coverage and precision evaluation of both answers and their attributions, using LLMas-judge. The average of these four facet scores is the final evaluation metric. For more detail, see appendix E.3. ArxivDIGESTables-Clean The ArxivDIGESTables-Clean benchmark tests an agents ability to create literature review tableone whose rows are publications and whose columns consist of aspects used to compare and contrast set of papers. Given set of related papers and caption describing the tables intent (e.g., Overview of LLM pretraining benchmarks), the task is to automatically output complete literature review table. We release new benchmark that builds on ArxivDIGESTables, the first high-quality dataset for literature review table generation created by Newman et al. (2024) by extracting review tables from ArXiv papers. Our evaluation includes two key improvements: (i) we curate small clean subset of instances from the original test set, and (ii) we introduce an end-to-end evaluation methodology for the task. Tables are scored by prompting an LLM to unroll them into statements. The evaluation metric is the proportion of ground truth statements from the reference table that are entailed (according to an LLM judge) by the unrolled generated table. For more detail, see appendix E.4. SUPER-Expert The SUPER-Expert benchmark (Bogin et al., 2024) (Setting UP and Executing tasks from Research repositories) tests the ability of code agents to set up and execute Python machine learning experiments reported in ML and NLP papers. It targets the common yet often non-trivial and time-consuming task of setting up and running code from sparsely documented repositories 14https://paperfinder.allen.ai/chat 15https://openscilm.allen.ai/ 34 accompanying published papers. Given natural language instruction along with GitHub repository pointer (e.g., asking to train model following papers code at given URL), the task is to clone the repository, install any needed dependencies, configure, run the requested training/evaluation, and report the outcome (e.g., model accuracy). In contrast to other repository-centered code execution tasks, the particular focus here is on low-resource research repositories on GitHublike those researchers often encounter when validating and expanding upon prior published work. For more detail, see appendix E.5. CORE-Bench-Hard The CORE-Bench-Hard benchmark (Siegel et al., 2025) tests an agents ability to reproduce experiments and analyses from papers. The input is \"capsule\" from CodeOcean.com containing code and data released alongside published paper, as well as set of instructions indicating specific analyses to perform with the capsule (full example in appendix H.7.1). The task is to perform these analyses and write answers in report.json file. The capsules in CORE-BenchHard are chosen to be highly reproducible and span variety of domains, including computer science, social science, and medicine, and use Python and programming languages. For more detail, see appendix E.6. DS-1000 The DS-1000 benchmark (Lai et al., 2023) tests the ability of code models on routine data science tasks encountered in everyday research. The input is coding question and an incomplete code snippet that the agent must fill in to answer the question (see example in appendix H.8.1). The output code snippet is graded by running it against (problem-specific) test case. This benchmark contains 1000 problems involving 7 Python libraries that were originally collected from StackOverflow and perturbed to avoid training leakage. We use the task implementation provided in Inspect evals (UK AI Safety Institute and Arcadia Impact and Vector Institute, 2025) and report the accuracy of the proposed code passing the target test cases. For more detail, see appendix E.7. DiscoveryBench The DiscoveryBench (Majumder et al., 2025) benchmark aims to test whether the agent can automatically find and verify hypotheses from given dataset(s), performing data-driven analysis. The input to the task is discovery goal and collection of datasets and their respective metadata, and the output is hypothesis addressing the goal with the highest specificity for the context, variables, and relationship supported by the dataset(s). Optionally, workflow for deriving hypothesis can be output to augment information already present in the hypothesis. This is the first comprehensive benchmark to test agents or language models ability to perform data analysis including data preparation, basic statistical analysis, complex data transformation, and modelingon datasets from 6 diverse domains, such as sociology and engineering. We collect task datasets from open public repositories made available by already published works from the 6 domains. The discovery goals are extracted from the associated papers to the datasets, or human-annotated, where each gold output (i.e., the hypothesis) is rigorously verified by data analysis experts. The performance on the benchmark is measured as the alignment of the predicted and gold hypotheses. The final metric, Hypothesis Matching Score, is product of three LLM-as-judge scores that measure the alignment of the predicted and the gold hypotheses in the dimensions of their context, associated variables, and the relationship among them. For more detail, see appendix E.8. E2E-Bench The E2E-Bench task aims to test whether agents can perform the full research pipeline of ideation, planning, (software) experiment design, implementation, execution, analysis, and producing final report, i.e., complete research cycle. The input to the task is research question in the domain of AI/NLP and detailed description of the steps to investigate it, and the output is technical report, trace of the agents reasoning, and any code or artifacts (e.g., datasets) generated. This is new release and forms the first agent-neutral benchmark (i.e., benchmark that isnt designed to highlight the strengths and scope of particular agent) designed to compare automatic scientific discovery (ASD) agents. It fills gap in the current research landscape where there are many such agents, e.g., AI Scientist (Lu et al., 2024), AgentLab (Schmidgall et al., 2025), and CodeScientist (Jansen et al., 2025), but no systematic way to compare them. In practice, to allow more controlled system-to-system comparisons, the problems are specified in considerable detail and hence only weakly test the ideation and planning steps. At the same time, these problems are not as prescriptive as typical ML coding problems, e.g., in MLAgentBench (Huang et al., 2024). The problems are created via mixture of machine generation and human review, and include detailed task description and problem-specific evaluation rubric. The final score is an overall LLM-as-judge assessment 35 based on three LLM-as-judge scores obtained by evaluating each relevant agent output (report, code, and artifacts) against the rubric. For more detail, see appendix E.9. E2E-Bench-Hard This task is similar to E2E-Bench, except the problems are generally harder. It follows the same task definition, evaluation, baselines, and environment as E2E-Bench, however the data collection method is different. For more detail, see appendix E.10. E.2 PA RFI N GBE In the rise of LLM-based agentic workflows, the ability to answer challenging scientific search queries, across wide range of searching criteria, have become possible. However, current paper finding benchmarks largely confine themselves to small subset of search query kinds (e.g. LitSearch (Ajith et al., 2024), PaSa (He et al., 2025) and LitQA2 dataset (Skarlinski et al., 2024)). They focus on purely semantic criteria, not covering metadata or navigational queries, and they are missing methodological process to cover the different within-semantic challenging types. PaperFindingBench is subset of our own internal evaluation for our literature-search agent (Asta Paper Finder), which focuses on challenging queries (the internal evaluation also mixes in bunch of easier queries, to ensure stability as product and avoid regressions). PaperFindingBench is designed to be challenging (including things that our system currently does not perform well on) and realistic (based to the extent possible on real-world queries and information needs). It also aims to be broad and diverse in two axes: first, it covers broader set of information needs. Unlike existing datasets that focus on semantic queries that search for set of unknown-to-the-user papers based on description of their content, our benchmark includes also navigational queries that seek single known-to-the-user paper based on short reference (the alpha-geometry paper), and queries that define paper sets based on wide set of metadata criteria (acl 2024 papers that cite the transformers paper). The second axis of diversity is within the semantic-search category, in which we seek to include different types of query challenges. The dataset mixes the different categories, and doesnt clearly indicate which query belongs to which category (even though human will very easily tell). This is following our belief that literature-search agent should be able to handle all these query types, even if by merely routing them to different sub-agents. PaperFindingBench includes 48 navigational queries, 43 metadata queries, and 242 semantic queries. Some of the metadata queries contain (easy) navigational queries as part of their criteria, but there is currently strict separation between metadata and semantic queries (metadata queries do not involve semantic component and vice-versa), which may change in future versions. Dataset Creation The Navigational queries are based on PaperFinder16 usage logs, to include queries that, at least at some point in time, paper-finder failed on. The semantic queries are curated from mix of sources: PaperFinder usage logs, OpenSciLM17 usage logs, and existing literature-search datasets: LitSearch (Ajith et al., 2024) and PaSa (He et al., 2025). We first identified subset of queries that were challenging for the PaperFinder system, by looking for queries that returned few or no results identified by the system as perfectly relevant, and for which we assessed (for query logs) or know (for the annotated dataset) that relevant papers exist. We then manually inspected collection of such queries to identify challenge types.18 Finally, we created set in which all challenge types are represented, while prioritizing queries for which running PaperFinder in an ablation mode with any of its components resulted in fewer perfectly-relevant papers for the ones that we do find. The set contains mix of queries for which we assume there are many relevant results, and queries for which we assume only handful of results exist. For numerous queries, assessing the relevance of the paper cannot be done solely based on title and abstract, but requires evidence from the papers full text. Metadata queries These were hand-crafted to achieve broad coverage of semantic-scholar API usage, as well as interaction between APIs, as well as challenges that are solvable but not directly supported 16https://paperfinder.allen.ai/chat 17https://openscilm.allen.ai/ 18These include, for example, multiple criteria, complex relations between criteria, use of uncommon terms, use of incorrect jargon, seeking details that are not part of the main claim of the paper, query providing unnecessary or even distracting background information. 36 by the APIs, such as negation (not citing the transformers paper). The queries include nesting and recursion of properties, and are inspired by the most complex queries we saw in the dataset, and taken up notch or two. We emphasized queries that require combining multiple APIs. Evaluation Evaluating retrieval is challenging, as it ideally requires gold-set of all relevant documents in the corpus, which is often not known. Such gold-set is available for the navigational and the metadata queries (each metadata query is internally associated with python code that uses the APIs to solve it completely, and whose results we use as the gold set). For the semantic queries, the full-coverage gold-set does not exist, and we resort to combination of partial annotation and LLMbased judgement. Each query is associated with (potentially empty) small-set of known-to-be-good matches, as well as with weighted set of relevance criteria that should be individually verified by the LLM against evidence from the paper for the paper to be considered good match. The individual relevance criteria were automatically generated by an LLM based on (potentially expanded version of) the original query. For fifth of the queries, the relevance criteria were manually verified and corrected or tweaked. As the tweaks and corrections turned out to be mostly minimal, and as the LLM-based relevance criteria were proved to be highly effective for the queries for which manual annotation for some papers is available, we consider all the relevance criteria as reliable, though they may be further improved in future versions. As we aim to assess retrieval and not the judging-LLMs ability to handle long-contexts, we dont provide the papers full-text for relevance judgement but rather require each result item to be associated with extracted evidence text (either from the paper itself or from papers citing it), which is then fed to the LLM for relevance judgement. Scoring Metrics We use two different scoring metrics. For the navigational and metadata queries, for which the gold-set is known, we use F1 over the result-set to score individual queries. For the semantic queries, which are based on LLM judgement, we can compute precision, but not recall. One potential metric would be simply the number of returned documents that are LLM-judged to be relevant, however, this number is unbounded and harder to integrate with other scores in AstaBench. We thus opted to compute recall over an estimated set size for each query (that is, we divide by an estimated set size and not definitive one), to bound the numbers between 0 and 1. The estimated set size is determined by running multiple variations of PaperFinder with very lenient threshold, taking the union of the resulting set, and then multiplying it by factor that ranges from 2 to 10 to estimate an upper bound and allow room for additional papers (smaller initial sets are less reliable and are multiplied by larger number). Note that in extreme cases, this may result in recall number larger than 1. We bound this by considering the retrieval-adjusted metric of recall@k where we set to be the estimated set size (this corresponds to the established recall@R metric, but we compute estimated recall@estimated). Computing recall@k fulfills two purposes: it bounds the score in 1, and also discourages submission of junk results. We balance recall@k not by precision, but by nDCG, as it provides more relevant signal (favoring ranking relevant documents over irrelevant ones). The combination of nDCG and recall@estimated makes precision mostly redundant. To provide single score for each individual query, we combine the recall and nDCG numbers using an harmonic mean (F1 over estimated-recall and nDCG). To provide single unified score for the entire dataset, we average the individual query scores, overall queries regardless of their type. Tools Cutoff Date We encourage participants to use the keyword and snippet search functionalities provided in Asta Scientific Corpus. In any case we expect submissions to follow the same cutoff date as the corpus cutoff date for both these tools which is set to June 1st 2025. Example Input An example input can be found in appendix H.1.1. E.3 SC A RQA-CS2 Scientific literature reviews are longstanding component of scientific workflows, and today are increasingly automated by commercial and open long-form QA services, such as OpenAI Deep Research, ScholarQA (Singh et al., 2025), Elicit, Perplexity, Paper QA (Skarlinski et al., 2024), 37 and many others. Evaluating long-form answers to literature review questions is challenging problem in natural language processing. Many acceptable long-form answers exist for any given question, and even with dataset of gold answers, it is difficult to define how to score given answer across the relevant dimensions of quality (coverage, correctness, attribution, etc.). The task is especially challenging in the scientific domain, where assessing an answer requires deep subjectmatter expertise and can change over time. Asai et al. (2024) introduced ScholarQABench, which consists of multiple datasets to evaluate scientific QA systems over several dimensions. Only one of its datasetsScholarQA-CS, which we build on in our workevaluates answer coverage based on set of target key ingredients (necessary points to cover in comprehensive answer, manually annotated in that work) for each question. The authors of ScholarQA-CS identify several limitations of their dataset, including that the annotated key ingredients could be subject to gaming because they reflect specific preferences of the two annotators, and that the full evaluation relies on heuristically set weight terms. In our new dataset, we instead collect diverse set of key ingredients from variety of candidate system responses, and also develop new LLM-as-judge approaches for answer relevance and improved citation evaluation. Evaluation Our ScholarQA-CS2 evaluation takes in an answer to question and outputs score which is an average of four constituent measures of answer quality: citation recall (whether each claim in the answer is fully supported by its citations), citation precision (whether each citation in the answer supports its associated claim, at least partially), answer relevance (whether each paragraph of the answer addresses the question) and answer coverage (the fraction of necessary points covered in the answer). All four evaluations rely on an LLM as judge, and the prompts are given in appendix H.3.3. To enable accurate assessment of citation recall and citation precision, we leverage feature of many evaluated systems: they provide quotes from each cited article intended to support the associated claim. For each claim, if the LLM judge assesses that the claim is fully supported by any combination of its citations and they include at least one supporting quote, that claim receives citation recall score of 1.0. If the LLM judge assesses support based on the cited papers title but there are no supporting quotes (this can happen because the system lacks the quote feature or because the particular sources texts are unavailable to the system e.g. for copyright reasons), the claim receives score of 0.5. Otherwise, the claim receives score of 0. Our final citation recall measure is an average over claims. To compute citation precision, we use the LLM judge assessments of whether citation provides at least partial support for its associated claim. If yes, the citation receives score of 1 (or 0.5 if it lacks quote), otherwise it gets score of 0. Our final citation precision is the average of these scores macro-averaged by claim. For answer relevance, we instruct the LLM judge to evaluate the answer, one paragraph at time, and instruct it to return list of paragraphs that are not directly relevant for answering the query. Our final answer relevance score is the proportion of relevant paragraphs. The fourth measure, answer coverage, is more challenging to assess because it requires not only evaluating the answer itself, but also identifying the key elements that correct answer to the question must include. Inspired by the approach taken in TREC information retrieval competitions (Craswell et al., 2021), for each question we gather pool of candidate ingredients from the systems we are evaluating,19 and assess the ingredients using an LLM judge. Specifically, for each evaluation question, we ask the LLM judge to extract key ingredients from each systems answer, identify specific details associated with each ingredient, and classify each ingredients importance as \"answer critical\" (must-haves for answering the question) or \"valuable\" (nice to have, but not critical). We then cluster the extracted ingredients by instructing the LLM judge to group semantically similar ingredients together while retaining the importance label. This process results in question-specific rubrics of ingredient clusters. The ingredient extraction prompts are given in appendix H.3.5. The rubric ingredients are used at answer evaluation-time to measure coverage. For each ingredient cluster, the LLM judge gives score of 0 (does not meet the criterion described in the rubric ingredient), 1 (somewhat meets the criterion) or 2 (perfectly meets the criterion). The final answer coverage score is weighted average of the individual ingredient scores, with ingredient importance determining the weight (with answer critical ingredients counting twice as much as the valuable 19Specifically, we source from the eight QA-long systems listed in Table 3 plus two baseline LLMs without retrievalClaude Sonnet 4.0 without thinking and Googles Gemini 2.5 Pro. All reports sourced were obtained before the cutoff date of June 24, 2025. 38 ingredients). The answer coverage prompt is shown in appendix H.3.3, with sample rubric in appendix H.3.2. Data Collection As our test set, we gather 100 user questions issued to OpenSciLM (Asai et al., 2024), filtered for language, quality and topic (we select questions from the computer science domain). The details of the selection process are given in appendix E.3.1. As development set, we retain the previously published ScholarQA-CS dataset (Asai et al., 2024) of 100 questions and update its ingredient lists using the same methodology described above. Choice of LLM Judge Since our evaluation is based upon LLM as judge, we selected an LLM that can handle long input contexts for processing long-form answers and also follow the various constraints described in our prompts. We choose to use gemini-2.5 models. We correlated the performance of gemini-2.5-flash and gemini-2.5-pro as the judge on the task optimized systems Section 4.3 evaluated on ScholarQA-CS2, and found that the Pearson correlation was 0.995. We therefore use gemini-2.5-flash as the official evaluator given its lower usage cost. Tools Cutoff Date Our long-form QA task relies on access to the keyword and snippet search functionalities provided in Asta Scientific Corpus. The corpus cutoff date for both these tools is set to May 1st 2025 for this task. Example Input An example input can be found in appendix H.3.1. E.3.1 QUERY SELECTION Here we outline the procedure for collecting 100 test set queries. We obtained from OpenScholar on Feb 21, 2025 8K random input queries with three words or more, and used an LLM (Claude Sonnet 3.5) to annotate them over five dimensions: language, field of study, clarity, completeness, and query type.20 Based on the generated annotations, we down select to English, Computer Science queries that express clear research request, for total of 3.5K queries. We then random sample 200 instances, which are then manually examined by four of our authors for question clarity, quality, and answerability to obtain our final 100 test queries. For detailed prompts, see appendix H.3.4. E.4 AR VDIGESTA S-CL Data Collection Padmakumar et al. (2025) identify that instances in ArxivDIGESTables sometimes contain one of the following issues: Generic columns (e.g., year of publication, research focus etc.) Unrecoverable columns containing information that cannot be obtained from full-texts of papers in the table (e.g., dataset instances) Generic columns are trivially easy to generate (over-optimistic performance estimates), while unrecoverable columns are impossible to generate (under-optimistic estimates). Therefore, evaluating on subset free from these issues ensures that we obtain realistic estimate of model performance. Since filtering such instances automatically is non-trivial, Padmakumar et al. (2025) manually curate ArxivDIGESTables-Clean, subset of 170 instances free of these issues. We use this subset, randomly sampling 100 instances to create the test set and using the remaining as validation set. Evaluation Newman et al. (2024) originally proposed reference-based automated evaluation procedure for the task of literature review table generation. Their procedure consists of two components: evaluating the schema (columns) and values (cells) for generated table. However, this decomposed evaluation has two disadvantages. First, it requires agents evaluated on this task to expose the same set of components (column generation and cell value generation), instead of allowing flexibility in agent design. Second, cell value evaluation is conducted by providing agents with the set of gold 20For query type, we instruct the model to distinguish between queries that contain an identifiable request, queries that resemble search terms, and queries that seek to test the capability of the agent (e.g., can write ?\" or can speak chinese?[sic]). columns from the reference table and assessing how well generated cell values match the cell values in the reference table. Therefore, this evaluation component effectively just measures the ability of agents to perform question answering over single paper. To address these disadvantages, we develop an end-to-end evaluation methodology inspired by TABEVAL (Ramu et al., 2024). The TABEVAL protocol first represents generated tables semantics by breaking it down into list of natural language atomic statements, process referred to as table unrolling. Then, it compares these statements against ground truth statements produced from reference table using entailment-based measures. We adopt the same approach, prompting GPT-4o to perform unrolling on generated tables, and then reporting the proportion of ground truth statements from the reference table that are entailed by the unrolled generated table (judged by GPT-4o) as recall. The prompts for table unrolling and assessing entailment are provided in appendix H.5.2 and appendix H.5.3. Example Input An example input can be found in appendix H.5.1. E.5 SUPER-EX T Task Each input in SUPER-Expert consists of (a) question specifying particular research task to execute within code repository (see example in appendix H.6.1), (b) specification of particular output result to produce, and (c) and details of the corresponding GitHub repository. The goal then is for the agent to download the target repository, and perform all of the necessary setup and configuration needed for running the repository code, modify specific details in the code as needed for the task (e.g., dataset name or location), execute the target task, and finally report the result in the desired format. Annotation What makes SUPER-Expert challenging is that such repositories are not welldocumented, each repository has its own set of issues, and while its sometimes possible to make high-level solution plan, it is very difficult to predict what specific error will one encounter during the setup and execution process. Gold solution annotations for these tasks were therefore obtained using high skilled annotators familiar with running ML and NLP experiments, hired through Upwork.21. They produced solutions in the form of Jupyter notebooks,22 which are also available as part of the benchmark. Evaluation AstaBench includes two of the original splits from Bogin et al. (2024): the Expert split containing 45 end-to-end problems as our test set and the Auto split containing 50 auto-generated problems (generated based on the README file of respositories that pass certain filter) as our development set. Scoring for the Expert split is done by computing the exact match metric between the produced solution and the annotated gold solution (often JSON dictionary containing output experiment metrics such as loss values). Example Input An example input can be found in appendix H.6.1. E.6 CORE-BE H-HA The version of CORE-Bench-Hard that we include in AstaBench is adapted in few ways: The original task comes with three difficulty levels (Easy, Medium, and Hard). We use the Hard version, which makes the task more challenging by removing several files from the capsule (such as the run script and the pre-computed result files), so the agent has to figure out how to install and run the code before it can do its analyses. We remove instances that would require GPU to run, to keep the resource requirements in line with the rest of the tasks. This reduces the dataset to 37 samples instead of the original 45. 21https://www.upwork.com 22https://jupyter.org 40 Though not mentioned in the paper, the original benchmark code includes standard prompt23 that describes the general task requirements and expected format of the output report. We always include these instructions in the task input to ensure that the task is self-contained. We use the train split of the original dataset as the validation split in AstaBench. Example Input An example input can be found in appendix H.7.1. E.7 DS-1000 We use the original version of DS-1000 from Lai et al. (2023) and the task implementation from Inspect evals (UK AI Safety Institute and Arcadia Impact and Vector Institute, 2025). In contrast to the original test set, we reserve 100 examples from the original set for validation and system development. Example Input An example input can be found in appendix H.8.1. E.8 DI V YBE (Majumder et al., 2024) provide initial evidence for the automated scientific discovery paradigm within the setting of data-driven discovery, where both search and verification of hypotheses may be carried out using dataset alone (i.e., after physical experiments and data collection, but the extent of this ability remains unclear. We, therefore, aim to systematically evaluate the following question: How capable are current state-of-the-art LLMs at automated data-driven discovery?. Answering this question is hard, as data-driven discovery in the wild (real-world) is diverse across domains and subject areas, which in turn makes it difficult to build robust evaluation framework to measure progress. We address this using pragmatic formalization of data-driven discovery, namely the search for relationship that may hold between variables in context, where (importantly) the description of those facets may not be in the language of the dataset. data-driven discovery task then has one of these components missing, e.g., How did urban land use affect the invasion of introduced plants in Catalonia?. Importantly, this formalization allows for systematic, reproducible evaluation over wide variety of real-world problems, by leveraging these facets. Task DiscoveryBench (Majumder et al., 2025) is novel benchmark for discovering data-driven hypotheses. In this benchmark, data-driven discovery task is defined as follows: Given one or more task dataset(s) and discovery goal, derive hypothesis addressing the goal with the highest specificity for the context, variables, and relationship supported by the dataset(s). Optionally, workflow for deriving hypothesis can be output to augment information already present in the hypothesis. Each hypotheses have to be verified programmatically (e.g., using Python) through data analysis workflow. Data Collection Our goal is to replicate the scientific process undertaken by researchers to search for and validate hypothesis from one or more datasets. We focus on six scientific domains where data-driven research is the cornerstone of scientific progress: sociology, biology, humanities, economics, engineering, and meta-science. Our gold trajectories to solve discovery task carefully follow the published papers workflows in respective domains. As most of the papers are highly cited, peer-reviewed, and from top venues in the domains, it is reasonable to assume the published workflows are scientifically valid. Evaluation We evaluate task performance by measuring the alignment of the predicted and gold hypotheses in natural language. We designed model-based evaluation strategy using gpt-4preview-0125 as the evaluator, conditioned on our structured formalism of data-driven hypotheses, i.e., hypothesis is composed of context, variables, and relationship between interacting variables. Critically, the evaluator assesses entailments/equivalences between linguistic elements of 23https://github.com/siegelz/core-bench/blob/db8a3d00c25fc30cf091f63102 03b7c715268084/benchmark/benchmark_prompts.json 41 predicted and gold hypothesis pair, following several LM-based language entailment as automatic tools for scientific claim verification. Example Input An example input can be found in appendix H.9.1. E.9 E2E-BE Data and Data Collection Each example is research task in the domain of AI/NLP, for example: Test whether effective prompts discovered for large language models can directly improve smaller models performance on classification tasks. followed by detailed description of the steps to perform this test. Tasks were created using mixture of machine generation (using Asta CodeScientists ideator tool) and human review and editing as follows: First, we collected all *ACL conference papers from 2021 or later with at least 100 citations and available on arXiv (288 papers). The ideator tool then picks two at random and uses these to LLM-generate up to five research ideas from the combination, repeated until we have 400 ideas, which are then automatically simplified, filtered, and ranked. Finally human expert raters reviewed the top ideas, discarding infeasible/impossible ideas or making small edits to repair them (if possible). The top 50 were used for the final dataset. Evaluation During idea generation, an example-specific scoring rubric is also auto-generated, asking whether all the necessary stages of research were conducted. Each rubric item is scored using LLM-as-judge against three of the ASD outputs separately (report, code, artifacts), to provide an overall score. Environment Given the complexity and time/dollar cost of ASD agents, ASTABench supports cache-based agents where (a) answers to all examples are precomputed offline, then (b) runtime cache-based agent simply retrieves cached answers to each question, allowing scoring in the ASTABench environment. Example Input An example input can be found in appendix H.10.1. E.10 E2E-BE H-HA Data Collection Rather than using Asta CodeScientists ideator, we instead use the HypER hypothesis generation system (Vasu et al., 2025). HypER first identifies research trend starting from each of the highly cited ACL papers from the above collection. For each research trend it then generates an initial idea, which is then refined further based on relevant paper excerpts to propose novel, underexplored tasks. Unlike E2E-Bench, we do not apply task simplification step, but keep the initial proposals unchanged. Next, the proposed tasks are automatically ranked and manually reviewed by human expert raters, who discard or fix infeasible tasks. Finally the top 50 tasks were used for the final dataset. Example Input An example input can be found in appendix H.11.1."
        },
        {
            "title": "F AGENTS",
            "content": "We describe the evaluated agents in two parts: (1) the Asta agents that we optimized for scientific research tasks, and (2) numerous baseline agentsboth general and science-specificthat we provide access to through the suite. F.1 ASTA AGENTS We release nine scientific research-optimized agent classes, including Asta v0, an orchestrator agent that automatically detects the type of task and dispatches to an appropriate task-specific sub-agent: 42 Asta Paper Finder is our paper-seeking agent, which is intended to assist in locating sets of papers according to content-based and metadata criteria. It is implemented as pipeline of manual-coded components which involve LLM decisions in several key-points, as well as LLM-based relevance judgments of retrieved abstracts and snippets. At high-level, query is analyzed and transformed into structured object which is then fed to an execution planner that routes the analyzed query to one of several workflows, each covering particular paper-seeking intent. Each workflow may involve multiple steps, and returns relevance-judged set of papers, which is then ranked while weighting content relevance together with other criteria which may appear in the query (e.g., \"early works on\", \"influential\" etc). This agent is frozen-in-time and simplified version of our live paper-finding agent available to use in Asta, which is restricted to single-turn interactions, does not ask for clarifications nor refuses queries, and which is using only the tools exposed in the AstaBench public APIs. It is described in more details in appendix F.3. Asta Scholar QA is previously published scientific long-form question answering system. It is composed of three components: retrieval to identify relevant passages from two Semantic Scholar corpora; re-ranker to select the most relevant of the retrieved passages; and multi-step LLM pipeline to create the final comprehensive report, including in-line citations. We experiment with several LLMs (including gpt-5) as part of the pipeline and report the best results with claude-sonnet-4-20250514. We further report results with gpt-4o-mini, and gemini-2.5flash-preview-05-20 to compare the performance and cost against smaller LLM. See Singh et al. (2025) for complete details on the system. Asta Scholar QA (w/ Tables) is variant of Asta Scholar QA that includes literature review tables. The Scholar QA system generates answers with sections each of which is either long form paragraph or list of items and their descriptions. In the latter case, the corresponding section also includes literature review table comparing the cited papers across multiple dimensions relevant to the query. The creation of tables leads to more LLM calls resulting in higher costs as well. We report our best results with this variant with claude-sonnet-4-20250514 as the backbone LLM. Asta Table Synthesis is previously published literature review table generation system. It follows two-step prompting workflow. Step 1 retrieves titles and abstracts of all input papers from the Semantic Scholar database and provides this information alongside the tables caption to an LLM to generate suggestions for columns/aspects along which papers can be compared. Step 2 rephrases each column as natural language query and prompts an LLM to generate cell values per paper conditioned on snippets relevant to the column retrieved from the paper full-text. We report results with the following backbone LLMs in this two-step workflow: gpt-4.1, o3, gpt-5-mini, gpt-5, claude-3-5-haiku, claude-sonnet-4, gemini-2.5-flash-preview-05-20 gemini-2.5pro, and llama-4-scout. See Singh et al. (2025) for complete details. Asta Code is an implementation of the React-style code agent in Bogin et al. (2024) that was originally designed for the SUPER-Expert evaluation. In addition to implementing standard ReACT think-act-observe-submit loop, it also has built-in tool for file editing and custom trajectory representation that facilitates fine grained trajectory evaluation. This includes evaluating whether certain landmarks (i.e., expected points in the trajectory trace) have been reached by the agent to measure partial success, as well as the ability to run code agents with partially filled-in gold trajectories. While these evaluation features are currently limited to SUPER-Expert, this solver allows for other code tasks to be extended to facilitate this kind of intermediate evaluation, and has an abstract structure that allows for the implementation of other agent workflows beyond ReACT. Asta DataVoyager is role-based multi-agent system powered by large generative model from (Majumder et al., 2024). Asta DataVoyager can semantically understand dataset, programmatically explore verifiable hypotheses using the available data, run basic statistical tests (e.g., correlation and regression analyses) by invoking pre-defined functions or generating code snippets, and finally analyze the output with detailed analyses. The core components of the system consist of specialized agents that are designed to manage different aspects of the data-driven discovery processplanning, programming and code execution, and data analysis. Additionally, to interpret plots generated during analyses, upon generation, we run multi-modal generative model (here, gpt-4o) to produce natural language summary of such figures so that other subagents can access that information 43 as additional context. We employ the AutoGen framework24 that allows agents to communicate in arbitrary order, dependent on the context, which is maintained by an Orchestrator agent. See Majumder et al. (2024) for complete details. Asta Panda performs research via LLM-based plan-and-act (hence \"Panda\") cycle. Given research task, it first generates natural language plan, then systematically performs each plan step in turn, then writes report on the outcome. Each plan step is performed using ReAct/CodeAct-style loop of (a) write Python code (b) execute it (c) reflect, and either recode (if step failed/incomplete) or move to the next plan step depending on the outcome. If there are too many failures the system replans from the failed step. Since the Asta Panda source code25 has not yet been integrated, we grade the cached results. Asta CodeScientist is an autonomous scientific discovery system for domains comprising computational experiments (e.g., machine learning or NLP) (Jansen et al., 2025). Asta CodeScientist implements idea creation and experiment construction through joint genetic search over combinations of research articles and pre-specified codeblocks, which define common actions in the investigative domain (e.g., prompting language model). Since the Asta CodeScientist source code26 has not yet been integrated, we grade the cached results. Asta v0 is an orchestrator agent that automatically detects the type of task and dispatches to an appropriate task-specific sub-agent. It uses simple but effective text similarity approach, that achieves 100% routing accuracy on the validation set. Once the task type is identified, Asta v0 hands off control to specialized solver for that task category, chosen for best expected performance based on our preliminary experiments. The full routing table can be found in appendix F.7. F.2 BASELINE AGENTS For the set of baseline agents, we provide two general agent classes and 11 scientific researchoptimized agent classes: ReAct is minimum-viable baseline solver that serves to measure the capabilities of LLMs without adding sophsticated agentic architecture or task-optimized prompt. It is simple ReAct loop: chat-LLM is given message history (initially just containing its system prompt (see appendix F.5) and the task instance input) and provided tools, it generates an output message with some reasoning and attached tool calls, then the results of the tool calls are appended to the message history and the LLM is called again. This continues until the submit(answer) tool is called, which breaks the loop and returns the final answer. The tool calls and responses are written with the native tool-calling format of the LLM (i.e., tool-call JSON objects attached to LLM output messages and special tool message types for responses).27 The agent truncates tool call outputs to at most 16,384 bytes to prevent long outputs from causing errors in the LLM. Smolagents Coder is the reference CodeAgent from the smolagents library (Roucher et al., 2025). It is ReAct agent, and as with the ReAct agent, the input at each step is message history; however, the actions for Smolagents Coder are represented as code rather than via the native tool-calling format of the LLM. Previous work has found that code-based tool calling can outperform other formats in practice (Wang et al., 2024), and it has the theoretical advantages of being able to manipulate values by reference and represent logic structures such as loops in single step, as opposed to the LLM having to simulate these structures over long sequence of calls. Smolagents Coder is instructed to produce Python code block to take actions (see appendix F.6 for prompt details); the code block is executed in the stateful Python environment (Section 4.1), and all of the agents tools are made available as callable Python functions. In addition, the agent can call 24https://microsoft.github.io/autogen/ 25https://github.com/allenai/panda 26https://github.com/allenai/codescientist 27E.g. for OpenAI models: https://platform.openai.com/docs/guides/function-cal ling 44 final_answer function to submit its final answer. The agents next input includes both the return value of the final statement in the code block as well as any printed output, up to maximum of 20,000 characters. You.com Search API is commercial Web and News Search API, which we accessed to obtain their responses. Elicit is commercial AI research platform for finding, summarizing, and extracting insights from scientific papers, such as in systematic reviews. Elicit searches the Semantic Scholar database and draws on all major large language model providers to provide AI screening, extraction, and deep research reports with in-line citations. Elicit elected to make submission to ScholarQA-CS2 on 04-03-2025, which we processed using an offline cached solver. FutureHouse Crow is general-purpose agent built on PaperQA2 that can search the literature and provide concise answers to questions (Skarlinski et al., 2024). It uses combination of OpenAIs gpt-4.1-mini and o3-mini as the backbone LLMs. Although PaperQA2 is open source, it does not include retrieval. As such, we accessed FutureHouses API to obtain Crow responses. FutureHouse Falcon is closed-source agent for deep literature reviews and hypothesis evaluation, designed for long-form question answering28. Falcon also uses OpenAIs gpt-4.1-mini and o3-mini as the backbone LLM. We accessed FutureHouses API to obtain Falcon responses. OpenAI Deep Research is commercial deep research system that uses Web search and OpenAIs language models to answer scientific questions. We obtained their reports by querying the o3-deep-research model via the OpenAI API for each question. OpenSciLM is previously published question answering system based on fine-tuned open models (Asai et al., 2024). It uses custom wrapper to the snippet and keywords search functionalities of Asta Scientific Corpus for retrieval and custom reranker. The OpenSciLM paper evaluated multiple variants of its RAG pipeline, here we evaluate the publicly available demo system which uses an open 8B-parameter Llama-3.1 backbone fine-tuned on synthetic data. Perplexity Sonar Deep Research is commercial deep research system that runs on Perplexitys proprietary search and closed LLM (Sonar). We accessed sonar-deep-research via Perplexitys API to obtain their responses. SciSpace Deep Review is commercial system that searches Semantic Scholar, AMiner and OpenAlex, using multiple models across subtasks. Some models are fine-tuned for task-specific needs (e.g., reranking for relevance). SciSpace elected to make submission to ScholarQA-CS2 on 06-13-2025, which we processed using cached solver. In their submission, the LLM was identified as claude-sonnet-4-20250514 which we report in Table 6. STORM is an open-source system from Stanford that uses You.com search and synthesizes comprehensive, Wikipedia-like articles on given topics or questions (Shao et al., 2024). STORM uses OpenAIs GPT-4o and GPT-3.5 as LLM backbones in various parts of its pipeline. You.com Research API is commercial deep research system that runs on You.coms search and unknown LLM. We accessed You.coms API to obtain their responses. Faker is baseline agent used to validate the scoring metrics for the End-to-End Discovery tasks. Faker simply prompts LM to make up the report, code, and artifacts as best it can, to simulate successful piece of research, without actually doing the work. 28https://futurehouse.gitbook.io/futurehouse-cookbook/futurehouse-client 45 F.3 AS PA FI R The Asta Paper Finder agent (PaperFinder) is frozen-in-time subset of the PaperFinder subcomponent of the Asta project (\"the PaperFinder Product\"). AstaBench PaperFinder follows the overall paper-finding procedure of the product, but differs from it in the indices and APIs it can use, and the set of papers available to it. It also differs in some configuration options, and does not improve over time. Finally, unlike the product, it does not support multi-turn continuations, and is restricted to single-turn scenario where the input is complete query and the response is ranked set of matching documents, and the evidence for each one. PaperFinder is system designed to locate scientific papers in large corpus of scientific literature, while integrating several indices, APIs, search strategies and LLM-based judgments in an intelligent and effective manner. It handles three kinds of queries: navigational queries, that aim to find specific paper known to the user, semantic queries that locates set of papers based on semantic description of their content, and metadata queries, that aim to find papers based on metadata criteria. The types are not fully isolated, and metadata criteria may intersect with navigational or semantic criteria. It also supports modifiers like \"central\", \"recent\" or \"early\", which influence the ranking of the results based on metadata information. The PaperFinder agent works as pipeline of manual coded steps which involve LLM decisions in several key-points.29 At high level, query enters the query analyzer which transforms the query into structured object reflecting the structure and semantics of the query. The analyzed query (which includes semantic relevance criteria) is then sent to an execution planner which looks at the analyzer output and routes it to one of several sub-workflows, each of them dedicated to particular kind of search (navigational, looking for set of papers based on semantic criteria and potential additional metadata, queries that involve complex metadata criteria, and author-based queries). The result of each of these workflows is set of papers and relevance judgments about each of them. These are then moved to ranker component that orders the papers in an order which is consistent with the users request, weighing the relevance scores together with other criteria such as publication time and number of citations for each work, in particular if this is supported by the query (i.e., explicit requests for \"recent\", \"early\", \"classic\", \"central\", \"well known\", \"little known\" etc). The ranked results are then returned. The PaperFinder agent uses the search APIs available in AstaBench. F.3.1 QUERY ANALYSIS The query analyzer is LLM based and extracts set of predefined properties of the query. The set of extracted properties is based on manual analysis of user-issued queries, and evolves over time. It covers primarily properties that are of use to the downstream components (search sub-flows and final ranker), but also includes some information that is not currently handled but that we would like to be aware of, for allowing to inform the user that given query criteria is not supported (for example, author affiliations). The query analyzer is implemented as several short prompts running in parallel, each targeting different small subset of properties (ranging from 1 to 3). We do not claim this is the optimal way of structuring such component, but we found it to be effective and have lower latency compared to longer prompt that extracts all of the information pieces. The query analyzer extracts the following properties: Broad vs Navigational Does the query target specific paper (e.g., papers title, \"the olmo paper\", \"the vaswani 2017 paper\") or set of papers that matches some criteria? This is similar to the navigational-vs-information-seeking distinction in traditional search queries. Semantic Criteria Semantic criteria is constraint or request about the content or title of the paper (papers about X, papers that do Y). Papers in academic scientific-literature retrieval benchmarks 29We found the manual-coding approach to be more efficient (in terms of number of LLM calls, number of tokens, and in terms of the ability to parallelize) and more reliable than more dynamic process that grants more autonomy to the LLM, allowing it to write code and significantly influence the computation flow and search process. We do plan to switch at least some component to more dynamic workflows in later versions. 46 focus almost exclusively on this criteria. However, real-world queries may include additional details such as metadata constraints or other properties, as discussed below. major role of the query analyzer is to separate the semantic criteria from the other properties, and populate it in its own dedicated string. Note that the semantic criteria may be complex and include many sub-criteria (papers about X, and that do not do W). The query analyzer treats these as single criteria and extracts them as single field. The analysis to sub-criteria happens down the line. Relevance Criteria main component of the paper-finder is judging the relevance of each individual candidate result. The query analyzer also breaks the semantic query into multiple sub-criteria (based on an LLM call), coupled with an importance score and short description of each one. These criteria will be used for assessing the relevance of the individual results. Metadata Constraints Simple metadata fields (year, year-range, authors, venues, citation counts) are extracted at fields. For complex metadata constraints (nested, negated, refer to other papers, etc), if they exist, are translated into complex data-structure which is beyond the scope of this paper. Explicitly non-supported metadata constraints These are based on metadata requests that appear frequently enough in our logs, but for which we do not currently have metadata support in the APIs and indices. Currently these includes author affiliation information (\"papers from AI2 about language modeling\"). Recency and centrality modifiers e.g. \"central paper\", \"classic paper\", \"highly cited\", \"recent paper\", \"early works\" etc.30 . Common requests that correlate with metadata information, F.3.2 NAVIGATIONAL QUERIES Specific paper requests are handled using combination of three strategies that run in parallel: 1. The semantic-scholar title API. 2. Asking an LLM and then using the semantic-scholar title API to ground the answers to specific corpus-ids. 3. Extracting key terms from the query, searching for sentences containing these terms, looking for citations within these sentences, and returning the top-cited items as candidates. Each of these strategies return zero or more results, which are then merged and returned. F.3.3 SEMANTIC QUERIES On high-level, the process works by performing series of retrieval steps, where each of them is followed by an LLM-based relevance filtering step. Each retrieval step broadens the scope of the previous ones, and is informed based on the relevant documents identified in the preceding steps. Initial-search. The input to the first retrieval step is the semantic criteria from the user-query, as extracted by the query analyzer. Based on this criteria, an LLM generates rephrasing of it, and the + 1 queries (rephrasing and initial query) are sent to the semantic search API. We now move from snippet-level to paper-level by aggregating the returned snippets according to the papers they come from. All snippets from the same paper are consolidated into single item representing the paper, in which the snippets are ordered by their order of appearance in the papers text. This aggregation is performed across queries: all the snippets in all the + 1 result sets participate in the aggregation, so that each paper item potentially contains matches from multiple sources. Cited papers. For some queries, non-negligible number of matching snippets refer to other papers (Doe et al 2023 show that...). We extract the set of papers mentioned in each snippet, and associate 30Adjectives that do not correlate with metadata information, e.g., \"good paper\", \"high quality paper\", \"interesting paper\", \"a good summary of\" are currently ignored, though some of them (\"a good summary of\") may make their way into the semantic criteria in some cases. 47 Figure 9: PaperFinder semantic query workflow the snippet also to papers from this set. Thus, each snippet may participate in several paper items: both the paper it came from, and the papers it cites. Some paper items contain only evidence mentioned within them, other paper items contain only evidence from citing papers, and some contain mix. We now have set of potential papers matching the query, each containing evidence snippets from multiple sources. To each of these we add also the title and abstract of the paper. The following step is relevance judgment, in which we filter the candidate paper set using LLM judgment (see below), resulting in subset containing relevant papers with their relevance judgments. We keep the most promising papers for the query. The order in which we go over the results matters for efficiency. We model this as multi-armed bandits problem over the different sources (each query is source). Citation Tracking. The relevance-judgment groups the papers to categorical tiers, with highlyrelevant being the perfect matches. This stage takes the top two categories (highly-relevant and somewhat-relevant), and performs forward and backward citation searches (a procedure known in the literature as snowballing). In forward snowballing we look for papers that cite the papers in the set, while in backward snowballing we look for papers cited by the papers in the set. These will then also go through relevance judgment. Followup queries We now formulate new queries based on the returned results. This is done by considering subset of papers that were judged as relevant to the query, whose distance from the query in the embedding space was the largest. Intuitively, these are relevant results which are at the boundaries of the current search queries. An LLM reformulates query based on the papers titles, abstracts and returned snippets, as well as the original query. These are then handled like in the initial search step: issuing queries to the vector-based API, adding cited papers, aggregating the results per paper, filtering papers that are already known from previous steps, sending to relevance judgment, and returning result set, which is then combined with the existing result set. Short-circuiting This process proceeds with iterations of citation tracking and followup queries for up to predetermined number of rounds. During the process we keep track of the number of papers that were sent to relevance judgment, and the number of papers that passed it. The process stops if the 48 number of found highly-relevant papers is sufficiently high, or if the number of relevance-judgment grows over predetermined limit. Relevance Judgment The relevance judgment component is applied separately to each of the found papers, and judges its relevance based on its information (title, abstract, extracted snippets, and referring snippets from other papers). The relevance judgment prompt considers each of the sub-criteria identified in query analysis, as well as the original query. Each sub-criteria is ranked as perfectly-relevant, somewhat-relevant or not-relevant. These are then combined to return categorical relevance judgment (perfectly relevant, highly relevant, somewhat relevant, not-relevant). F.3.4 METADATA QUERIES Simple metadata filters (venue, year) on top of semantic queries are handled as post-filters on the result set, or as ranking criteria (recent, highly cited). Queries that involve only metadata, or queries that involve semantic criteria and complex metadata criteria, are first sent to dedicated metadata retrieval component, and then filtered for semantic match using the relevance judgment component. The metadata component uses LLM calls to analyze the metadata into structured work-plan, which is then passed to manually-coded executor which translates it to series of API calls. F.3.5 FINAL RANKING Finally, we combine the relevance judgements with other criteria, based on the query analysis, using heuristic that takes into account number of citations, publication date, and the preferences expressed in the query if they exist. F.4 AGENT SOURCE CODE REFERENCES Asta Paper Finder31 Asta Table Synthesis32 Asta Scholar QA33 Asta Code34 Asta DataVoyager35 Asta Panda (cached)36 Asta CodeScientist (cached)37 Asta v038 31https://github.com/allenai/asta-paper-finder 32https://github.com/allenai/agent-baselines/tree/1ce836604c37da38de2a69 614800c20ca6616349/agent_baselines/solvers/arxivdigestables/asta_table_a gent.py@tables_solver 33https://github.com/allenai/agent-baselines/tree/1ce836604c37da38de2a 614800c20ca6616349/agent_baselines/solvers/sqa/sqa.py@sqa_solver 34https://github.com/allenai/agent-baselines/tree/1ce836604c37da38de2a69 614800c20ca6616349/agent_baselines/solvers/code_agent/agent.py@code_agent 35https://github.com/allenai/agentbaselines/tree/1ce836604c37da38 de2a69614800c20ca6616349/agent_baselines/solvers/datavoyager/agent.p y@datavoyager_solver 36https://github.com/allenai/agent-baselines/tree/1ce836604c37da38de2a69 614800c20ca6616349/agent_baselines/solvers/e2e_discovery/autoasta/autoas ta_cached.py@autoasta_cached_solver 37https://github.com/allenai/agent-baselines/tree/1ce836604c37da38de2a69 614800c20ca6616349/agent_baselines/solvers/e2e_discovery/codescientist/c odescientist_cached.py@codescientist_cached_solver 38https://github.com/allenai/agentbaselines/tree/1ce836604c37da 38de2a69614800c20ca6616349/agent_baselines/solvers/asta/v0/asta. py@fewshot_textsim_router ReAct39 Smolagents Coder40 Elicit (cached)41 Perplexity Sonar Deep Research42 SciSpace Deep Review (cached)43 OpenSciLM (cached)44 OpenAI Deep Research (cached)45 FutureHouse Crow46 FutureHouse Falcon47 STORM48 You.com Research API49 You.com Search API50 Faker51 F.5 REAC PROMPT The ReAct agent uses the system prompt from the InspectAI librarys basic agent, constructed without knowledge of AstaBench. 39https://github.com/allenai/agentbaselines/tree/1ce836604c37da38 de2a69614800c20ca6616349/agent_baselines/solvers/react/basic_agent.p y@instantiated_basic_agent 40https://github.com/allenai/agentbaselines/tree/1ce836604c37da38 de2a69614800c20ca6616349/agent_baselines/solvers/smolagents/agent. py@smolagents_coder 41https://github.com/allenai/agent-baselines/tree/1ce836604c37da38de2a69 614800c20ca6616349/agent_baselines/solvers/sqa/elicit/memorized_solver.p y@elicit_solver 42https://github.com/allenai/agent-baselines/tree/1ce836604c37da38de2a 69614800c20ca6616349/agent_baselines/solvers/sqa/formatted_perplexity. py@formatted_solver 43https://github.com/allenai/agent-baselines/tree/1ce836604c37da38de 2a69614800c20ca6616349/agent_baselines/solvers/sqa/scispace/scispace.p y@formatted_solver 44https://github.com/allenai/agent-baselines/tree/1ce836604c37da38de2a69 614800c20ca6616349/agent_baselines/solvers/sqa/openscholar/memorized_sol ver.py@openscholar_solver 45https://github.com/allenai/agent-baselines/tree/1ce836604c37da38de2a69 614800c20ca6616349/agent_baselines/solvers/sqa/general_memorized/memoriz ed_solver.py@formatted_solver 46https://github.com/allenai/agent-baselines/tree/1ce836604c37da38de2a69 614800c20ca6616349/agent_baselines/solvers/futurehouse/futurehouse_solver. py@futurehouse_solver 47https://github.com/allenai/agent-baselines/tree/1ce836604c37da38de2a69 614800c20ca6616349/agent_baselines/solvers/futurehouse/futurehouse_solver. py@futurehouse_solver 48https://github.com/allenai/agentbaselines/tree/1ce836604c37da38 de2a69614800c20ca6616349/agent_baselines/solvers/sqa/storm_solver. py@storm_solver 49https://github.com/allenai/agent-baselines/tree/1ce836604c37da38de 2a69614800c20ca6616349/agent_baselines/solvers/sqa/formatted_youcom. py@formatted_solver 50https://github.com/allenai/agent-baselines/tree/1ce836604c37da38de 2a69614800c20ca6616349/agent_baselines/solvers/search/youcom_search. py@youcom_solver 51https://github.com/allenai/agent-baselines/tree/1ce836604c37da38de2a 69614800c20ca6616349/agent_baselines/solvers/e2e_discovery/faker/faker.p y@faker_solver 50 You are helpful assistant attempting to submit the correct answer. You have several functions available to help with finding the answer. Each message may may perform one function call. You will see the result of the function right after sending the message. If you need to perform multiple actions, you can always send more messages with subsequent function calls. Do some reasoning before your actions, describing what function calls you are going to use and how they fit into your plan. When you have completed the task and have an answer, call the submit() function to report it. F.6 SM G S CO PROMPT We use the default smolagents v1.17.0 system prompt, and additionally add tool definitions in the input user message when describing the task (note placeholders for tool_descriptions and task_prompt): You have access to astabench tools in sandbox environment. You can use (cid:44) {tool_descriptions} these tools in your Python code: Remember that you have `final_answer(answer: str)` function that you (cid:44) must use to return your final answer and mark the task as completed. The answer passed to the `final_answer` function should be string formatted according to the task instructions; depending on the task, the string might need to contain structured outputs like JSON or code, and there may be other steps (such as writing files) that you need to perform in addition to calling `final_answer`. (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) {task_prompt} The task_prompt is simply the input from the task itself. Each available tool is represented in tool_descriptions as function signature with the tool description and parameters. For example, for get_paper from Asta Scientific Corpus, we have: get_paper(paper_id: str, fields: str = 'title,abstract,corpusId,authors,year,venue, citation- (cid:44) Count,referenceCount,influentialCitationCount') Get details about paper by its id. Args: paper_id: The id of the paper to get. The following types of IDs are (cid:44) CorpusId:215416146 DOI:10.18653/v1/N18-3011 649def34f8be52c8b66281af98ae884c09aef38b supported: <sha> - Semantic Scholar ID, e.g. (cid:44) CorpusId:<id> - Semantic Scholar numerical ID, e.g. (cid:44) DOI:<doi> - Digital Object Identifier, e.g. (cid:44) ARXIV:<id> - arXiv.rg, e.g. ARXIV:2106.15928 MAG:<id> - Microsoft Academic Graph, e.g. MAG:112218234 ACL:<id> - Association for Computational Linguistics, e.g. (cid:44) PMID:<id> - PubMed/Medline, e.g. PMID:19872477 PMCID:<id> - PubMed Central, e.g. PMCID:2323736 URL:<url> - URL from one of the sites listed below, e.g. (cid:44) URL:https://arxiv.org/abs/2106.15928v1 ACL:W12-3903 E.g \"url,year,authors\". fields: String of comma-separated fields to include in the response. (cid:44) Default is \"title\". Available fields are: abstract, authors, (cid:44) journal, publicationDate, references, tldr, url, venue, year. citations, fieldsOfStudy, isOpenAccess, 51 Returns: The paper object. F.7 AS V0 ROUTING TABLE Asta v0s routing approach starts by predicting task type based on the (character-level) lexical overlap of the input against set of examples from the validation set. This approach sometimes confuses highly similar tasks that have the same answer format (e.g. PaperFindingBench and LitQA2-FullText-Search), but as we want to route such tasks to the same sub-agent anyway, it achieves 100% routing accuracy on the validation set. Once the task type is identified, Asta v0 hands off control to specialized solver for that task category, chosen for best expected performance based on our preliminary experiments: Paper search tasks (PaperFindingBench, LitQA2-FullText-Search) Asta Paper Finder Long-form QA (ScholarQA-CS2) Asta Scholar QA (w/ Tables) with claudesonnet-4 Table generation (ArxivDIGESTables-Clean) Asta Table Synthesis with o3 Data analysis (DiscoveryBench) Asta DataVoyager with o3 configuration Code repository replication (SUPER-Expert) Asta Code with gpt-4.1 End-to-end discovery (E2E-Bench, E2E-Bench-Hard) Asta Panda with claudesonnetOther tasks (DS-1000, CORE-Bench-Hard, LitQA2-FullText) ReAct with o3 The orchestrator implements fallback mechanism to enable sub-agents to opt out: if the predicted task-types sub-agent doesnt produce an output, Asta v0 retries with the next most similar task type (up to 3 attempts). F.8 VALIDATION OF LITERATURE UNDERSTANDING AGENTS Some scientific QA agents are not capable of outputting structured data that conforms to given schema. Accordingly, we take the plain text output of these QA agents and pass them through \"formatting\" step. This formatting step uses an LLM (gemini-2.5-flash) to split the plain text report into sections, identifying the inline citations and returns structured output that conforms to our SQAResponse schema. There are also some agents that proport to have structured output capabilities but whose output quality drops dramatically when it is enabled. We also use the formatting step for these agents. The list of agents for which we use formatting step are: You.com, Perplexity DR, OpenAI DR, and FuturHouse Crow and Falcon. For Asta Paper Finder, an expanded and continuously developed version of the agentincluding user interface and additional infrastructureis actively used by growing number of users. Throughout the extended period of development and real-world usage, we have validated the agent repeatedly using an internal eval set (which is superset of the benchmark we now release including some additional simpler regression-testing queries). Although this internal set is not an established benchmark it has been proven useful to monitor retrieval quality and detect any regressions in recall or ranking performance. The increasing adoption among users serves as additional corroboration of both the effectiveness of the agent and the correctness of our internal evaluation methodology. For LitQA2-FullText specifically, since its multiple-choice QA task, we evaluate the FutureHouse (creators of the original LitQA dataset) agents, and You.com and Perplexity DR because of api availability and their suitability to the task of short-form QA. The system can respond with only the correct choice or short description with the correct choice as json to be considered valid. For handful of samples, we ensure the baseline systems can respond in the required format by 52Our Asta v0 experiments were started prior to the release of gpt-5, and due to time and the relatively poor performance of GPT-5 on many specialized solvers, we did not evaluate gpt-5 version for this work. We also note that Asta Code was chosen based on very early experiments with relatively old models, despite the final results showing better SUPER-Expert performance from ReAct with o3. 52 issuing the same input prompt to their UI chat interfaces. Since LitQA2-FullText is subset of the original, direct comparison with results in (Skarlinski et al., 2024) is difficult. Further, at the time, PaperQA2 used gpt-4-turbo as the backbone LLM, while FutureHouse Crow, which is based on PaperQA2 uses gpt-4.1-mini. For sanity, we look at the difference between the average accuracy result reported for PaperQA2 (66.0) and FutureHouse Crow (72.0) and conclude that evaluating on fewer questions and with better SOTA models explains it. For Asta Table Synthesis, we expect scores on our new end-to-end evaluation metric to generally be in the same range as the results reported by Newman et al. (2024). For Perplexity Sonar Deep Research, we set reasoning_effort=high and search_context_size=high, maximizing the models compute and offering it the best possible performance on our datasets. The Perplexity API also provides search_mode parameter which can be set to academic to only retrieve academic sources. However, at the time of running the system (August 3rd7th, 2025), this disabled web search entirely, so we did not set this parameter. Finally, while we found it may be possible to prompt Perplexity Sonar Deep Research to extract quotes in each of its cited sources, the API does not explicitly return these snippets; thus, we evaluate the model as if it only cites the title and URL of each page. F.9 VALIDATION OF END-TO-END DISCOVERY AGENTS To score and validate agents on end-to-end tasks, the E2E scorer uses task-specific scoring rubric for each task, listing the key required facets of valid result (e.g., downloads the right dataset, selects the right baseline, etc.). The rubrics were checked manually (and updated where needed) by human annotators. To apply these, the scorer uses LLM-as-judge to score each rubric item on each of three classes of artifact generated by the agent, namely: the generated report, the generated code, and the produced artifacts (e.g., datasets). Scores are easily viewed in generated HTML page, and were validated using spot-check sampling and verification by human. While the scoring is not perfect, the three-class scoring system significantly reduce errors, for example hallucinated result in the report may be scored as zero if there is no code or artifacts to substantiated it."
        },
        {
            "title": "G ADDITIONAL EXPERIMENTAL DETAILS AND RESULTS",
            "content": "G.1 EXPERIMENTAL DESIGN Table 18 provides list of models run in our experiments. G.2 EVALUATION ON FULL SET OF LITQA2 DATASET This section presents additional details on evaluating on the LitQA2 dataset. When evaluating on our own literature search agent (PaperFinder), we provide it with the question text as is, without including the multiple choices and without attempting to translate the question into paper-finding query-form. We did not do any task-specific modifications or tuning of PaperFinder for this task. As LitQA2 was designed as full-text search benchmark, our main results are on the LitQA2FullText-Search subset, for which our corpus contains full-text to all papers. Here we report results also on the original LitQA2 dataset of Skarlinski et al. (2024), in which 114 out of the 199 queries have only their abstracts, and not full text, represented in our search index. The results in Table 19 show that PaperFinder agent obtains very similar results to the agent of Skarlinski et al. (2024) despite having access to only abstracts for over half the papers, and scores significantly higher on the subsets where full text is available."
        },
        {
            "title": "H EVALUATION TASK SAMPLES AND PROMPTS",
            "content": "This section provides higher level of detail for evaluation tasks through example problems and rubrics, plus detailed prompts. 53 Table 18: Models run in our study. Model names are mapped to the model identifiers used during API calls, with used to disambiguate models that were called without their date identifiers for full transparency. Name Model ID Organization Open-Weight Inference Provider gpt-3.5-turbo gpt-4o-mini gpt-4o gpt-4o gpt-4.1 gpt-4.1 gpt-4.1-mini gpt-5-mini gpt-5-mini gpt-5 gpt-5 o3-mini o3 o3 claude-3-5-haiku claude-3-7-sonnet claude-sonnet-4 gemini-2-flash gemini-2.5-flash gemini-2.5-flash gemini-2.5-pro sonar-deepresearch llama-4-scout llama-3.1openscholar-8b OpenAI OpenAI OpenAI OpenAI gpt-3.5-turbo0125 gpt-4o-mini OpenAI gpt-4o-2024-08-06 OpenAI gpt-4o OpenAI gpt-4.1-2025-04OpenAI 14 gpt-4.1 gpt-4.1-mini gpt-5-mini-202508-07 gpt-5-mini gpt-5-2025-08-07 gpt-5 o3-mini o3-2025-04-16 o3 claude-3-5-haiku20241022 claude-3-7-sonnet20250219 claude-sonnet-420250514 gemini-2.0-flash gemini-2.5-flashpreview-05-20 gemini-2.5-flash gemini-2.5-pro sonar-deepresearch Llama-4-Scout-17B16E-Instruct llama-3.1openscholar-8b Google Google Meta Meta / Allen AI OpenAI OpenAI OpenAI OpenAI OpenAI OpenAI Anthropic Anthropic Anthropic Google Google Perplexity OpenAI OpenAI OpenAI OpenAI OpenAI OpenAI OpenAI OpenAI OpenAI OpenAI OpenAI OpenAI OpenAI OpenAI Anthropic Anthropic Anthropic Google Vertex AI Google Vertex AI Google Vertex AI Google Vertex AI Perplexity Together AI Self-hosted Table 19: Retrieval scores on full set of LitQA2 dataset Name original-set portion full-text percentage PaperQA2 (Skarlinski et al. (2024)) PaperFinder (ours) PaperFinder (ours) PaperFinder (ours) full (199) full (199) LitQA2-FullTextSearch Test (75) LitQA2-FullTextSearch Val (10) 100% <50% 100% 100% recall 69.9 70.3 93.3 recall @30 62.8 64.3 90.7 80 54 H.1 PA RFI N GBE H.1.1 EXAMPLE PROBLEM Find papers relevant to the following query: Could you suggest research (cid:44) that investigates clustering-based efficient attention mechanism within Transformer models? (cid:44) Try to be comprehensive in your search yet efficient and accurate, i.e. find as many highly relevant papers as possible, but try to keep (cid:44) efficiency in mind. You may submit up to 250 papers. (cid:44) If the query asks for specific paper known to the user, i.e. \"the (cid:44) Transformer paper\", \"the BERT paper\", \"the GPT-3 paper\" etc, try to find that specific paper and only return that one. This does not apply to any query phrased in singular \"paper\" or \"article\" - those can be general queries and should return multiple relevant papers, e.g. \"which paper introduced transformer-based generative model for text generation\". (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) be ordered by most relevant first: Return your answer as JSON with the following structure, results should (cid:44) ```json { \"output\": { \"results\": [ { the paper\", \"paper_id\": \"string; the semantic scholar corpus_id of (cid:44) \"markdown_evidence\": \"string; markdown-formatted snippet with verbatim text from the paper that (cid:44) supports the relevance of the paper to the query; the evidence should be concise and limited to the minimum needed to support the paper's relevance\" (cid:44) (cid:44) (cid:44) }, ... ] } } ``` H.2 LI TQA2-FU LTE T-SE H H.2.1 EXAMPLE PROBLEM Find papers relevant to the following query: Active olfactory receptor genes increase their contacts with greek island regions by what (cid:44) factor in mouse olfactory neurons? (cid:44) Try to be comprehensive in your search yet efficient and accurate, i.e. find as many highly relevant papers as possible, but try to keep (cid:44) efficiency in mind. You may submit up to 250 papers. (cid:44) If the query asks for specific paper known to the user, i.e. \"the (cid:44) Transformer paper\", \"the BERT paper\", \"the GPT-3 paper\" etc, try to find that specific paper and only return that one. This does not apply to any query phrased in singular \"paper\" or \"article\" - those can be general queries and should return multiple relevant papers, e.g. \"which paper introduced transformer-based generative model for text generation\". (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) be ordered by most relevant first: Return your answer as JSON with the following structure, results should (cid:44) ```json { \"output\": { \"results\": [ { 55 the paper\", \"paper_id\": \"string; the semantic scholar corpus_id of (cid:44) \"markdown_evidence\": \"string; markdown-formatted snippet with verbatim text from the paper that (cid:44) supports the relevance of the paper to the query; the evidence should be concise and limited to the minimum needed to support the paper's relevance\" (cid:44) (cid:44) (cid:44) }, ... ] } } ``` H.3 SC A RQA-CS2 H.3.1 EXAMPLE PROBLEM Generate report answering the following research question. Be sure to (cid:44) include inline citations for each claim. Return your result as valid JSON with single key `sections` which is list of sections, each having keys `title`, `text`, and `citations`. Each entry in `citations` should have JSON list of `snippets` extracted from the reference document and an `id`, each of which appears exactly in the text. Each `id` should be an inline citation as it appears in the text (with wrapping parentheses or square brackets if appropriate). Each citation should have `title` if one is available. Any additional information about the citation should go under `metadata`. Do not create References section. (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) Here is an example `section` to help you with formatting: { \"title\": \"Background\", \"text\": \"Convolutional neural networks (CNNs) have achieved (cid:44) \"citations\": [ state-of-the-art results in image classification [1][2].\", { \"id\": \"[1]\", \"snippets\": [ \"CNNs have become the standard for many visual tasks.\" ], \"title\": \"ImageNet Classification with Deep Convolutional (cid:44) \"metadata\": { Neural Networks\", \"authors\": \"Krizhevsky, A. et al.\", \"year\": 2012, \"arxiv\": \"1207.0580\" } }, { \"id\": \"[2]\", \"snippets\": [ \"Significant improvements in image recognition have been (cid:44) observed with CNNs.\" ], \"title\": \"Very Deep Convolutional Networks for Large-Scale (cid:44) \"metadata\": { Image Recognition\", \"authors\": \"Simonyan, K. & Zisserman, A.\", \"year\": 2014, \"arxiv\": \"1409.1556\" } } ] 56 } Question: Apart from preventing overfitting, are there any side (cid:44) effects (desirable or otherwise) of applying dropout in deep neural networks? (cid:44) H.3.2 EXAMPLE RUBRIC { \"question\": \"how the AI hallucination is linked to the AI bias\", \"ingredients\": [ { \"name\": \"answer_critical_0\", \"criterion\": \"Define AI hallucination and AI bias\", \"weight\": 0.14285714285714285, \"examples\": [ (cid:44) \"factually incorrect, nonsensical, or misleading outputs despite appearing confident in their responses\", (cid:44) \"when an LLM generates content that does not correspond to (cid:44) reality, producing outputs that are coherent and grammatically correct but factually incorrect or nonsensical\", (cid:44) \"AI systems generate outputs that are misleading, biased, or (cid:44) \"systematic errors or skewed outputs stemming from imbalances (cid:44) entirely fabricated, despite appearing convincingly real\", in training data, model architecture, or deployment context\", (cid:44) \"an inclination or prejudice for or against person or group, (cid:44) \"prejudiced or unfair outcomes due to skewed training data or (cid:44) especially in way considered unfair\", flawed algorithmic design\" ] }, { \"name\": \"answer_critical_1\", \"criterion\": \"Explain shared root causes linking hallucination (cid:44) \"weight\": 0.14285714285714285, \"examples\": [ and bias, particularly training data issues\", pattern matching over true semantic understanding\", \"biased training data\", \"Both originate from the inherent reliance on statistical (cid:44) \"Incomplete or biased data can lead to AI models learning incorrect patterns, resulting in hallucinations\", (cid:44) \"Data-related hallucinations generally emerge as byproduct of (cid:44) biases, misinformation, and knowledge gaps, which are fundamentally rooted in the training data\", (cid:44) \"If the training data is biased, incomplete, or flawed, the AI (cid:44) model may learn incorrect patterns, leading to inaccurate predictions and hallucinations\", (cid:44) \"Both phenomena emerge from datasets that are either incomplete, (cid:44) noisy, or imbalanced\" ] }, { \"name\": \"answer_critical_2\", \"criterion\": \"Explain how bias directly contributes to (cid:44) \"weight\": 0.14285714285714285, \"examples\": [ hallucination\", tasks, leading to factually incorrect summaries\", \"biases manifest themselves as hallucinations in summarization (cid:44) \"correlation coefficients reaching 0.81-0.83 between intrinsic (cid:44) bias and extrinsic hallucination rates\", 57 \"Language models may generate stereotypical or harmful content about marginalized groups when trained on internet text (cid:44) containing systemic biases\", plausible but incorrect medical information\", (cid:44) \"bias in medical training data leads to models generating (cid:44) \"If an AI model is trained on data that underrepresents certain groups or overrepresents particular viewpoints, it may (cid:44) generate hallucinatory content that reflects these imbalances\", (cid:44) \"a language model might assume nurse is female without any (cid:44) gender cue, hallucinating that detail based on gender-role stereotype\" (cid:44) (cid:44) ] }, { \"name\": \"answer_critical_3\", \"criterion\": \"Explain how hallucination propagates and amplifies (cid:44) \"weight\": 0.14285714285714285, \"examples\": [ bias\", (cid:44) \"When an AI model hallucinates, the nonsensical or incorrect information it generates may inadvertently reveal the (cid:44) prejudiced assumptions it has learned from biased data\", (cid:44) \"The very act of hallucination, being deviation from factual (cid:44) grounding, can sometimes be manifestation of the system's internal biases, where the 'made-up' information aligns with these learned prejudices\", (cid:44) \"Confidence in Flawed Outputs: Hallucinations presented (cid:44) \"Data Pollution: Biased or hallucinated outputs fed back into (cid:44) confidently by AI can reinforce existing biases\", training data create self-reinforcing cycles of inaccuracy and prejudice\", (cid:44) \"When AI systems hallucinate, they often draw upon learned (cid:44) patterns and associations from their training data that include societal biases\", (cid:44) \"AI hallucinations can amplify existing biases in the data, (cid:44) leading to discriminatory outcomes\" ] }, { \"name\": \"answer_critical_4\", \"criterion\": \"Describe the interconnected and bidirectional (cid:44) \"weight\": 0.14285714285714285, \"examples\": [ nature of the relationship\", solutions\", limitations in current AI systems\", \"they represent different manifestations of fundamental (cid:44) \"addressing one without the other provides incomplete (cid:44) \"both stem from systemic issues in data quality, model (cid:44) \"AI bias manifests as hallucinations when models are trained on (cid:44) unrepresentative or imbalanced data and combined with specific architectural designs\" architecture, and training processes\", (cid:44) ] }, { \"name\": \"valuable_0\", \"criterion\": \"Provide real-world examples demonstrating the (cid:44) \"weight\": 0.07142857142857142, \"examples\": [ link\", \"Healthcare Diagnostics: AI systems hallucinated symptoms for Black patients 34% more often than for white patients, (cid:44) correlating with underrepresentation in training data\", (cid:44) 58 (cid:44) \"Recruitment Tools: Amazon's scrapped hiring algorithm (cid:44) downgraded resumes containing the word 'women's' while inventing irrelevant skill requirements for male candidates\", (cid:44) \"Mata v. Avianca legal case where ChatGPT produced nonexistent (cid:44) \"ChatGPT's 'Inner Racist' Incident where the model hallucinated (cid:44) \"In healthcare: factual hallucinations leading to logical (cid:44) hallucinations and diagnostic errors that can jeopardize patient safety\" hateful rant laced with stereotypes\", legal opinions\", (cid:44) ] }, { \"name\": \"valuable_1\", \"criterion\": \"Discuss mitigation strategies that address both (cid:44) \"weight\": 0.07142857142857142, \"examples\": [ issues\", datasets\", evaluation\", \"data preprocessing, algorithm selection, and model (cid:44) \"Training AI models on large, diverse, and high-quality (cid:44) \"The research community is increasingly advocating for (cid:44) integrated evaluation frameworks that simultaneously assess factual accuracy and fairness\", (cid:44) \"Data deduplication, improved data curation, and augmentation (cid:44) to reduce memorization artifacts and balance representation\", (cid:44) \"External fact-checking layers and retrieval-augmented (cid:44) generation (RAG) frameworks\" ] }, { \"name\": \"valuable_2\", \"criterion\": \"Explain specific mechanisms connecting bias and (cid:44) \"weight\": 0.07142857142857142, \"examples\": [ hallucination\", \"LVLMs struggle with object hallucinations due to their (cid:44) reliance on text cues and learned object co-occurrence biases\", (cid:44) \"RLHF is vulnerable to the biases inherent in the human (cid:44) \"object hallucinations in vision-language models stem from overconfidence problems closely related to statistical (cid:44) bias\", annotators' judgments\", (cid:44) \"Models rely on token probabilities and learned correlations (cid:44) \"When learned probability distributions are biased, incomplete, (cid:44) rather than true understanding of underlying knowledge\", or overly general, models produce outputs that are statistically probable but factually incorrect or biased\", (cid:44) \"Modern generative models operate like advanced autocompletion, (cid:44) focusing on producing likely-sounding continuations\" ] }, { \"name\": \"valuable_3\", \"criterion\": \"Discuss implications for high-stakes domains\", \"weight\": 0.07142857142857142, \"examples\": [ healthcare, finance, and security\", \"can lead to misinformed decisions in critical areas such as (cid:44) \"Healthcare: Medical AI might hallucinate treatment (cid:44) recommendations while reflecting biases against demographic groups\", (cid:44) 59 perpetuating systemic biases\", \"Law: Legal AI systems might fabricate case precedents while (cid:44) \"healthcare applications where both phenomena can lead to (cid:44) \"Legal and judicial contexts where fabricated case citations (cid:44) misdiagnosis and inappropriate treatment recommendations\", can mislead practitioners\" ] } ], } H.3.3 EVALUATION PROMPTS"
        },
        {
            "title": "Citation Precision and Recall",
            "content": "You are claim validator. For each claim made in the following text you (cid:44) will determine if it is supported by the quote from it's corresponding inline citations. As is typically done in academic writing, assume that consecutive sentences can share citations. Make sure to also include claims presented in table format. For references with only the title available (ie no quotes from the reference are included), judge them as `supporting` if the title indicates that the paper is likely relevant to the claim being considered. Return JSON object with single key `claims` which is list of `claim` objects, one for each sentence in the text. Each `claim` object contains the claim itself (`text`), list of `supporting` inline citations and `non_supporting` inline citations and finally boolean `is_fully_supported` which indicates if the claim is entirely supported by the quotations in the associated citations. Each inline citation corresponding to that claim should appear in either `supporting` or `non_supporting`, but not both. Each claim made in the text should appear in your output, but you should skip sentences covering high level introductory information. (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44)"
        },
        {
            "title": "Answer Relevance",
            "content": "You are given query and corresponding long answer. Goal: find irrelevant paragraphs in the answer. These are paragraphs that (cid:44) don't directly answer the query and shouldn't be in the answer. For instance, if the query is about datasets for scientific question (cid:44) answering, paragraph about multilingual question answering datasets that don't contain scientific text would be considered irrelevant. (cid:44) Explicitly consider whether something may be indirectly relevant. For (cid:44) example, if the question is about the conditions of horses in South Africa, paragraph about general animal welfare in South Africa is potentially relevant while not being precisely about horses. On the other hand, paragraph about pig welfare in South Africa is irrelevant. (cid:44) (cid:44) (cid:44) (cid:44) Note that subtle differences can make the text irrelevant to the query. (cid:44) For instance, text about scientific survey paper generation is not relevant to query about automatic paper review generation. Even though they seem related, they are about very different tasks. (cid:44) (cid:44) Also, useful background in general is relevant. If the question is about an approach to creating liver-related proteins, some information (cid:44) about liver-related proteins could contextualize other parts of the answer. If paragraph contextualizes another part of the answer, then it is relevant. (cid:44) (cid:44) (cid:44) 60 Go through the answer and output list of irrelevant paragraphs. Every single paragraph needs to be considered, one by one. Our goal is to (cid:44) catch all the irrelevant paragraphs, so please be thorough. (cid:44) Return your result as JSON object with single key (cid:44) `irrelevant_paragraphs` whose value is list of objects, each having keys `reason`, and `answer_text` as follows: (cid:44) {{\"irrelevant_paragraphs\":[ {{ \"reason\": \"discuss why something is irrelevant (not indirectly (cid:44) \"answer_text\": \"exact ENTIRE paragraph (not just part of it) from the (cid:44) }}, ... ] }} Make sure all the irrelevant paragraphs are included. answer that is irrelevant\" relevant)\","
        },
        {
            "title": "Answer Coverage",
            "content": "You will be given question someone asked (in <question></question> (cid:44) tags) and the corresponding response (in <response></response> tags) given to them by an assistant. (cid:44) You will then be given an enumerated list of criteria by which to (cid:44) evaluate the response. Each criterion specifies requirements that the answer must satisfy. You will assign score accordingly (see below). (cid:44) You will also be given list of examples (in <examples></examples> tags, (cid:44) below each criterion) that illustrate the type of details that would satisfy the criterion. We do NOT expect any of the specified details to necessarily appear in the answer. These are strictly to be used as guidance for locating the answers that satisfy the set requirement. (cid:44) (cid:44) (cid:44) For each criterion, return score of 0, 1 or 2 indicating how (cid:44) appropriate the response is based on the given criterion. 0 means the response does not meet the criterion, 1 means the response somewhat meets the criterion, 2 means the response perfectly meets the criterion. Judge only the specified aspect(s) delimited by the criterion, not any other qualities of the answer. (cid:44) (cid:44) (cid:44) (cid:44) benchmarks</question> Scoring Example 1: <question>Common medical NLP papers on clinical text (cid:44) <response>The application of natural language processing (NLP) and (cid:44) machine learning to medical text presents tremendous opportunities for healthcare tasks such as prediction ... [TRUNCATED]</response> (cid:44) Criteria: <criterion> 1. Detail the well-known medical NLP datasets <examples> i2b2 includes datasets focused on temporal relations in clinical (cid:44) narratives, CRAFT Corpus is collection of 97 full-length, open-access biomedical journal articles with semantic and syntactic annotations.] (cid:44) (cid:44) </examples> </criterion> <criterion> 2. ... [TRUNCATED] <examples> ...[TRUNCATED] </examples> </criterion> 2 point answer would fully satisfy the criterion #1. For example, it (cid:44) would include specific names with some details of well-known medical datasets for ML like those mentioned in the examples. (cid:44) 61 1 point answer would only partially satisfy the criterion #1. For (cid:44) example, dataset (like those in examples) may be mentioned, but no detail would be provided. Or datasets may be simply listed without further discussion. (cid:44) 0 point answer would not mention datasets at all. (cid:44) fieldwork.</question> Scoring Example 2: <question>What are some of the documentation methods used in Linguistics (cid:44) <response>Language documentation, also called documentary linguistics, is specialized subfield of linguistics ... [TRUNCATED]</response> (cid:44) Criteria: <criterion> 1. ... [TRUNCATED] <examples> ...[TRUNCATED] </examples> </criterion> <criterion> 2. Cover elicitation techniques for capturing specific linguistic data. <examples> structured interviews, elicitations based on standard word lists, (cid:44) </examples> </criterion> prompted speech tasks (cid:44) (cid:44) 2 point answer to criterion #2 would contain common elicitation (cid:44) techniques like (but not limited to) those mentioned in the examples. The answer specifics don't have to match exactly with the examples, but examples show the types of instances that would count towards satisfying the criterion. (cid:44) 1 point answer to criterion #2 be incomplete in some way. For example, (cid:44) the answer might mention \"elicitation sessions\" during discussion on audio recording, but it fails to specifically address the requirement. Or the answer gives list of standard word lists in the answer as resources, but fails to tie this information to elicitation. (cid:44) 0 point answer to criterion #2 would simply not include the discussion (cid:44) in any way. For example, if an answer focuses only on data handling (post elicitation) techniques, it would miss out on techniques for documentation interview itself. (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) Scoring Example 3: (cid:44) (cid:44) (cid:44) (RNNs)?</question> <question>How do transformer models differ from recurrent neural networks (cid:44) <response>Transformer models use self-attention mechanisms to process (cid:44) input, while RNNs process input sequentially. Transformers are better at handling long-range dependencies in data because they don't rely on previous time steps to pass information. RNNs may suffer from vanishing gradients and have trouble with long-term dependencies.</response> (cid:44) Criteria: <criterion> 1. Must compare how the architecture and data processing flow differ (cid:44) Transformers use parallel processing and self-attention; RNNs process (cid:44) between transformers and RNNs. <examples> input tokens one at time in sequence. Transformers can look at the entire input sequence at once, while RNNs have to pass information step by step. (cid:44) (cid:44) </examples> </criterion> 62 2 point answer would accurately and distinctly contrast both (cid:44) architecture and sequence-processing style of both model families (e.g., parallelism vs. sequential processing, use of self-attention vs. recurrence). (cid:44) (cid:44) 1 point answer would provide partial or imprecise comparison, perhaps (cid:44) only mentioning one difference, or being vague (e.g., \"Transformers work differently from RNNs in how they process text\" without further elaboration). (cid:44) (cid:44) 0 point answer would explain only one architecture (e.g., only (cid:44) transformers), or describe both but fail to contrast them on the asked criteria. (cid:44) Return your result as JSON object with single key `scores` whose value is list of objects, each having keys `criteria_idx`, (cid:44) `reasoning`, `score` and `evidence` from the text supporting the claim. (cid:44) (cid:44) H.3.4 QUERY SELECTION"
        },
        {
            "title": "Query Annotation Prompt",
            "content": "(cid:44) Choices: true false>, { \"English\": <Is this user query in English? \"Query Type\": <Choose from query types below or suggest your own>, \"Computer Science\": <Is the query generally fall under the computer (cid:44) \"Field of Study\": <Choose from the Field of Study below>, \"Subfield of Study\": <If you chose Computer Science, Biomedicine, and (cid:44) science or closely related field? Choices: true false> Psychology as the Field of Study, specify the subfield of study that this query is most related to (examples are below). If more than one subfield, slash delimit and order from highest to lowest importance.> missing in the query? Choices: complete missing>, (cid:44) \"Fragment\": <Do you think this is full query, or is part obviously (cid:44) \"Clarity\": <Is the request clear? Choices: clearly understandable vague (cid:44) \"Research Stage:\" <Ideation, Topic Understanding, Literature Search and (cid:44) Synthesis, Research Design, Data Analysis, Project Write up, Can't tell> but understandable need clarification>, (cid:44) } ``` Query Types: \"request\": This user is asking the system for some information on some (cid:44) \"search terms\": This user is giving sequence terms, likely for search. \"testing\": This user is asking the system to say something about its (cid:44) particular topic or subject. abilities or capabilities. Field of Study: \"Computer Science\": Computer Science is the study of computers and (cid:44) computational systems, including theory, design, development, and application. (cid:44) \"Biomedicine\": Biomedicine studies the application of the principles of (cid:44) the natural sciences and especially biology, physiology, and biochemistry to clinical practice. (cid:44) \"Psychology\": Psychology is the study of the mind and behavior. It is the study of the mind, how it works, and how it affects behavior. (cid:44) \"None of the above\": This query belongs to different field of study. EXAMPLES of Subfield of Study: Computer Science: artificial intelligence, computer systems and networks, security, database systems, human computer interaction, vision and (cid:44) graphics, numerical analysis, programming languages, software engineering, and theory of computing. (cid:44) (cid:44) 63 Biomedicine: medical microbiology, virology, clinical chemistry, (cid:44) hematology, immunology, genetics, molecular pathology, microbiology, bioinfomatics, and biomechanics. (cid:44) Psychology: behavioral psychology, clinical psychology, cognitive psychology, comparative psychology, cultural psychology, (cid:44) developmental psychology, and educational psychology. (cid:44) H.3.5 KEY INGREDIENT EXTRACTION AND CLUSTERING PROMPTS"
        },
        {
            "title": "Ingredient Extraction",
            "content": "I will provide you query that tests literature knowledge and report (cid:44) from system. You will use the system report to identify key requirements or \"ingredients\" that the report sees as necessary for answering the question. Each ingredient should include high level descriptor of what is expected in an answer, and list of examples or details (if relevant). (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) How to write good ingredient: * Each ingredient should include one requirement at time. For example, (cid:44) instead of \"The answer should mention the challenges of manual construction of an ontology and discuss the use of automated methods for aiding the process.\" have two ingredients: \"The answer should mention the challenges of manual construction of an ontology\" and The answer should discuss the use of automated methods for aiding the ontology construction.\" (cid:44) * Each ingredient should address different component of the query. If the query requests Effect of phonemic perceptions is evident in (cid:44) language acquisition, speech comprehension, and second language learning, single ingredient shouldnt try to address all three language acquisition, speech comprehension, and second language learning. Ideally these should be separated out into multiple requirements. (cid:44) * Identify which are critically important ingredients. Critical (cid:44) ingredients are those, if not satisfied, would render the response useless. This is judgement call you must make by closely considering what the QUESTION IS REQUESTING. For example, if question asks for \"coding datasets for assessing LLM capabilities\", then identifying the most common or accepted coding evaluation dataset & benchmarks, and possibly also their details (e.g., notable methods used) would be critically important. However, ingredients that, for example, delve into the theoretical background of particular evaluation or discuss future research directions would not NOT be critically important. For critically important information use SHOULD (e.g., \"The answer should cover ...\"), otherwise use MIGHT (e.g., \"The answer might cover ...\"). (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) * Use the main verb judiciously according to what you observe in the (cid:44) report: if the information should be mentioned in passing, you might use language like \"The answer should MENTION/TOUCH ON ...\". If it should be covered in some detail language like \"The answer should DISCUSS/EXPLAIN/DETAIL ...\" would be appropriate. If the answer should list items then it would be fitting to write \"The answer should LIST/ENUMERATE ...\" (cid:44) * Unless specifically required by the question, the ingredient should avoid using specific numbers or qualifiers in the ingredient (cid:44) description: e.g., The answer should list the three main challenges that... The answer should list the main challenges that ... OR The answer should list main challenges such as hallucination or grounding problems that ... (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) An ingredient MUST: * Be agnostic as to where in the report it appears (e.g., \"should begin by explaining\" --> \"should explain\"; \"might conclude by noting\" --> (cid:44) \"might note\") (cid:44) 64 (cid:44) * Be self-contained and understandable without needing to know about other ingredients (e.g. In \"The answer should also mention other (cid:44) common approaches\" language like \"also\" and \"other\" rely on other ingredients for disambiguation). (cid:44) * Not make reference to other ingredients (e.g. pronouns like \"these\" in (cid:44) \"should further describe these approaches\" that refer to the previous ingredient should be avoided and be replaced with mentions) (cid:44) * Not contain (ultra) specific information, unless the question (cid:44) specifically calls for it. List them as \"examples\" instead. If an ingredient mentioned the need for datasets, the examples would be the specific datasets that the report mentions (cid:44) * Refrain from including specific mentions of variants with limited shelf (cid:44) life. For example, put \"Honey Smacks\" or \"Special K\" in the examples under more generic \"Kellogg's cereals\". Try \"Apple OS\" in the ingredients instead of \"Big Sur\" or \"Mojave\". (cid:44) (cid:44) (cid:44) contained in the report. Further Rules and Guidelines: * Step through the report sequentially * In writing your ingredients and examples, only use information (cid:44) * Cover as much of the relevant portions of the report as possible. * Content you include in the ingredient or examples do source from the (cid:44) * No references should be made to the reference report itself: e.g., dont write The answer should briefly define each of the key (cid:44) concepts introduced in the report instead write The answer should briefly define each of the key concepts such as... report (not elsewhere) (cid:44) (cid:44) (cid:44) Note that ingredients are requirements. Phrase them as requirements an answer should fulfill: start with \"The answer should \" (for answer (cid:44) critical ingredients) or \"The answer might \" (for non answer critical ingredients). citation if available; null if not available },... ] (cid:44) Return json as an answer: [ { \"id\": sequential numerical ingredient id, \"ingredient\": description of the ingredient/requirement, \"examples\": [{ \"detail\": examples/details if relevant, \"citation\": (cid:44) }, ... ] Acceptable forms of citations: * If corpusId is specified in the report, cite the number, e.g., (cid:44) * If the URL (e.g. to arxiv) is specified, cite the URL, e.g., \"citation\": (cid:44) * If Author and Year as specified: \"citation\", e.g., \"(Vaswani et al, (cid:44) * If no citations are available, e.g., \"citation\": null \"https://arxiv.org/abs/1706.03762\" \"citation\": \"13756489\" 2017)\""
        },
        {
            "title": "Ingredient Clustering",
            "content": "(cid:44) (cid:44) will give you user query and list of ingredients. The ingredients (cid:44) are written requirements for writing good answer. Note that ingredients the writer thought are more critical to answering the query are prefixed with \"The answer SHOULD\". Useful but not critical information is marked as \"The answer MIGHT\". (cid:44) Do the following: 1. Identify the key concepts, ideas, and named entities that should be (cid:44) covered for this question 65 (cid:44) (cid:44) (cid:44) (cid:44) 2. Carefully consider the query and the ingredients given to you. At this (cid:44) stage, ONLY look at the ingredient description (do not consider the examples) to identify minimal set of non-overlapping key requirements that either are high-quality ingredients OR are consistently being covered in the ingredient list. Take into consideration concepts identified in 1, especially when deciding if the key requirement should be SHOULD or MIGHT requirement. (cid:44) 3. Next, step through each of the given ingredients, and decide which set (cid:44) requirements it should be associated with, and distribute the examples (see Notes 1 and 2). (cid:44) 4. Prune the examples: Remove exact or near duplicates. Remove examples that you judge are not directly relevant to the key requirement. (cid:44) 5. Finally, list ingredients that were left out and why. (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) Note1: You are allowed and encouraged to place multiple ingredients into (cid:44) single key requirement. This would be fitting in the case of duplicate or near duplicate ingredients like \"discuss physical commonsense datasets like PIQA\" vs. \"include discussion of PIQA or other physical commonsense datasets\". This type of grouping can also happen if you have more general key requirement that can handle multiple ingredients, for example, for key requirement \"discuss success of AI in disease detection\" might encompass ingredients like \"mention AI success in diabetic retinopathy prediction\" and \"point out that machine learning methods have been successfully used on ECG data to identify early signs of atrial fibrillation\". (cid:44) Note2: You are allowed to split ingredients into multiple key (cid:44) requirements. For example, if an ingredient reads \"The answer might explain why the engagement dropped, focusing on common mistakes in interface design.\", you may end up placing it under both the requirement \"The answer might explain the drop in engagement\" and the requirement \"The answer might discuss common mistakes in interface design\", distributing its examples to the appropriate requirement. (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) relevant for the query. Rules: * Always keep your focus on the query. All key requirements must be (cid:44) * NEVER include an ingredient in requirement on the basis of the (cid:44) examples alone. ALWAYS make sure that the ingredient description is prioritized. (cid:44) * Use your best judgement for deciding whether key requirement should (cid:44) be SHOULD or MIGHT requirement ALWAYS based on the question and the key concepts and ideas you identified early on. (cid:44) * Each requirement should ideally address different component of the query. If the query requests Effect of phonemic perceptions is (cid:44) evident in language acquisition, speech comprehension, and second language learning, single requirement shouldnt try to address all three language acquisition, speech comprehension, and second language learning. Ideally these should be separated out into multiple requirements. (cid:44) * Remember, the key requirements should not be overlapping. For example: Note that ingredient R1-The answer should introduce transformer (cid:44) architecture components, including attention mechanisms and their role in sequence modeling partially overlaps with R2-The answer should discuss the role of attention mechanisms in sequence modeling. This should be avoided, when possible: R1 could instead be The answer should introduce transformer architecture components since the rest is covered by R2. (cid:44) * Each key requirement should be self-contained and understandable (cid:44) without needing to know about other requirements (e.g. pronouns like \"these\" in \"should further describe these approaches\" that refer to the previous requirements should be avoided and be replaced with mentions). (cid:44) * Although should ingredients are more important, the might (cid:44) ingredients are also valuable to Include those that you think they would (best) help answering the user's query. (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) 66 associated. * There should never be key requirement that has no ingredient (cid:44) * Its okay to have leftover ingredients. Ingredients that you think are (cid:44) not very relevant, too vague, or peripherally relevant can be left out even if they carry the \"should\" phrasing. (cid:44) * Background or causally related information unless the query asks (cid:44) * DO NOT include key requirements that are centrally about paper (cid:44) explicitly for them, should be considered \"MIGHT\" requirements. citations. For example, do not include requirements like \"List recent papers...\" or \"Cite the most impactful papers...\" or \"Identify and discuss important papers...\". (cid:44) (cid:44) can use liberally. Repeat (THINK) after me! * will be choosy about \"SHOULD\" requirements. \"MIGHT\" requirements, (cid:44) * will base \"SHOULD\" and \"MIGHT\" based on key concepts judge as being (cid:44) * will always write requirements that are relevant to the query. central to answering the query. together, grouped.], Return json: { \"key_requirements\": [ { \"key_requirement\": description designed after the ingredients you group (cid:44) \"ingredients\": [the ingredient id list of those ingredients you (cid:44) \"examples\": [concatenated relevant examples from ingredients in this (cid:44) requirement { \"detail\": examples/details if relevant, \"citation\": citation if available; null if not available }, ...] (cid:44) }, ... ] \"left_out_ingredients\": [ {\"ingredient\": id of the ingredient that got left out, \"reason\": brief (cid:44) ] } reason why it was left out.}, ... H.4 LI TQA2-FU LTE H.4.1 EXAMPLE PROBLEM Active olfactory receptor genes increase their contacts with greek island (cid:44) regions by what factor in mouse olfactory neurons? A. 2.0 fold B. 27 fold C. 1.7 fold D. 2.7 fold E. Insufficient information to answer the question F. 3.0 fold Answer with the letter of the chosen answer in JSON: {\"answer\": (cid:44) \"<letter>\"}. H.5 AR VDIGESTA S-CL H.5.1 EXAMPLE PROBLEM We would like you to build table that has each paper as row and, as each column, dimension that compares between the papers. You will be given multiple papers labeled Paper 1, 2, and so on. You will be provided with the title and content of each paper. 67 Operation Policies for RFEHNs.. Please create table that compares and contrasts the given papers, that would satisfy the following caption: Comparison of Receiver (cid:44) Return the table in the specified JSON format only. Make sure that the table has 5 dimensions which are phrases that can compare multiple papers, and 9 papers as rows. (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) Multi-Antenna Interference Channel Paper 3343717 title: Wireless Information and Energy Transfer in (cid:44) Paper 3343717 abstract: This paper considers the transmitter design for wireless information and energy transfer (WIET) in multiple-input (cid:44) single-output (MISO) interference channel (IFC). The design problem is to maximize the system throughput subject to individual energy harvesting constraints and power constraints. It is observed that the ideal scheme, where the receivers simultaneously perform information detection (ID) and energy harvesting (EH) from the received signal, may not always achieve the best tradeoff between information transfer and energy harvesting, but simple practical schemes based on time splitting may perform better. We therefore propose two practical time splitting schemes, namely the time-division mode switching (TDMS) and time-division multiple access (TDMA), in addition to the existing power splitting (PS) scheme. In the two-user scenario, we show that beamforming is optimal to all the schemes. Moreover, the design problems associated with the TDMS and TDMA schemes admit semi-analytical solutions. In the general K-user scenario, successive convex approximation method is proposed to handle the WIET problems associated with the ideal scheme, the PS scheme and the TDMA scheme, which are known NP-hard in general. Simulation results show that none of the schemes under consideration can always dominate another in terms of the sum rate performance. Specifically, it is observed that stronger cross-link channel power improves the achievable sum rate of time splitting schemes but degrades the sum rate performance of the ideal scheme and PS scheme. As result, time splitting schemes can outperform the ideal scheme and the PS scheme in interference dominated scenarios. (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) Paper 8313045 title: Wireless Information and Power Transfer in Multiuser (cid:44) OFDM Systems (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) Paper 8313045 abstract: In this paper, we study the optimal design for simultaneous wireless information and power transfer (SWIPT) in (cid:44) downlink multiuser orthogonal frequency division multiplexing (OFDM) systems, where the users harvest energy and decode information using the same signals received from fixed access point (AP). For information transmission, we consider two types of multiple access schemes, namely, time division multiple access (TDMA) and orthogonal frequency division multiple access (OFDMA). At the receiver side, due to the practical limitation that circuits for harvesting energy from radio signals are not yet able to decode the carried information directly, each user applies either time switching (TS) or power splitting (PS) to coordinate the energy harvesting (EH) and information decoding (ID) processes. For the TDMA-based information transmission, we employ TS at the receivers; for the OFDMA-based information transmission, we employ PS at the receivers. Under the above two scenarios, we address the problem of maximizing the weighted sum-rate over all users by varying the time/frequency power allocation and either TS or PS ratio, subject to minimum harvested energy constraint on each user as well as peak and/or total transmission power constraint. For the TS scheme, by an appropriate variable transformation the problem is reformulated as convex problem, for which the optimal power allocation and TS ratio are obtained by the Lagrange duality method. For the PS scheme, we propose an iterative algorithm to optimize the power allocation, subcarrier (SC) allocation and the PS ratio for each user. The performances of the two schemes are compared numerically as well as analytically for the special case of single-user setup. It is revealed that the peak power constraint imposed on each OFDM SC as well as the number of users in the system play key roles in the rate-energy performance comparison by the two proposed schemes. (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) Efficiency Optimization in OFDMA Systems Paper 902546 title: Wireless Information and Power Transfer: Energy (cid:44) Paper 902546 abstract: This paper considers orthogonal frequency division (cid:44) multiple access (OFDMA) systems with simultaneous wireless information and power transfer. We study the resource allocation algorithm design for maximization of the energy efficiency of data transmission (bits/Joule delivered to the receivers). In particular, we focus on power splitting hybrid receivers which are able to split the received signals into two power streams for concurrent information decoding and energy harvesting. Two scenarios are investigated considering different power splitting abilities of the receivers. In the first scenario, we assume receivers which can split the received power into continuous set of power streams with arbitrary power splitting ratios. In the second scenario, we examine receivers which can split the received power only into discrete set of power streams with fixed power splitting ratios. For both scenarios, we formulate the corresponding algorithm design as non-convex optimization problem which takes into account the circuit power consumption, the minimum data rate requirements of delay constrained services, the minimum required system data rate, and the minimum amount of power that has to be delivered to the receivers. By exploiting fractional programming and dual decomposition, suboptimal iterative resource allocation algorithms are developed to solve the non-convex problems. Simulation results illustrate that the proposed iterative resource allocation algorithms approach the optimal solution within small number of iterations and unveil the trade-off between energy efficiency, system capacity, and wireless power transfer: (1) wireless power transfer enhances the system energy efficiency by harvesting energy in the radio frequency, especially in the interference limited regime; (2) the presence of multiple receivers is beneficial for the system capacity, but not necessarily for the system energy efficiency. (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) 69 (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) Splitting for MISO SWIPT Systems Paper 1767525 title: Joint Transmit Beamforming and Receive Power (cid:44) Paper 1767525 abstract: This paper studies multi-user multiple-input single-output (MISO) downlink system for simultaneous wireless (cid:44) information and power transfer (SWIPT), in which set of single-antenna mobile stations (MSs) receive information and energy simultaneously via power splitting (PS) from the signal sent by multi-antenna base station (BS). We aim to minimize the total transmission power at BS by jointly designing transmit beamforming vectors and receive PS ratios for all MSs under their given signal-to-interference-plus-noise ratio (SINR) constraints for information decoding and harvested power constraints for energy harvesting. First, we derive the sufficient and necessary condition for the feasibility of our formulated problem. Next, we solve this non-convex problem by applying the technique of semidefinite relaxation (SDR). We prove that SDR is indeed tight for our problem and thus achieves its global optimum. Finally, we propose two suboptimal solutions of lower complexity than the optimal solution based on the principle of separating the optimization of transmit beamforming and receive PS, where the zero-forcing (ZF) and the SINR-optimal based transmit beamforming schemes are applied, respectively. (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) systems with wireless information and power transfer Paper 11665681 title: Power efficient and secure multiuser communication (cid:44) Paper 11665681 abstract: In this paper, we study resource allocation (cid:44) algorithm design for power efficient secure communication with simultaneous wireless information and power transfer (WIPT) in multiuser communication systems. In particular, we focus on power splitting receivers which are able to harvest energy and decode information from the received signals. The considered problem is modeled as an optimization problem which takes into account minimum required signal-to-interference-plus-noise ratio (SINR) at multiple desired receivers, maximum tolerable data rate at multiple multi-antenna potential eavesdroppers, and minimum required power delivered to the receivers. The proposed problem formulation facilitates the dual use of artificial noise in providing efficient energy transfer and guaranteeing secure communication. We aim at minimizing the total transmit power by jointly optimizing transmit beamforming vectors, power splitting ratios at the desired receivers, and the covariance of the artificial noise. The resulting non-convex optimization problem is transformed into semidefinite programming (SDP) and solved by SDP relaxation. We show that the adopted SDP relaxation is tight and achieves the global optimum of the original problem. Simulation results illustrate the significant power saving obtained by the proposed optimal algorithm compared to suboptimal baseline schemes. (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) Paper 125571 title: Wireless Information and Power Transfer: Architecture (cid:44) Design and Rate-Energy Tradeoff 70 Paper 125571 abstract: Simultaneous information and power transfer over (cid:44) the wireless channels potentially offers great convenience to mobile users. Yet practical receiver designs impose technical constraints on its hardware realization, as practical circuits for harvesting energy from radio signals are not yet able to decode the carried information directly. To make theoretical progress, we propose general receiver operation, namely, dynamic power splitting (DPS), which splits the received signal with adjustable power ratio for energy harvesting and information decoding, separately. Three special cases of DPS, namely, time switching (TS), static power splitting (SPS) and on-off power splitting (OPS) are investigated. The TS and SPS schemes can be treated as special cases of OPS. Moreover, we propose two types of practical receiver architectures, namely, separated versus integrated information and energy receivers. The integrated receiver integrates the front-end components of the separated receiver, thus achieving smaller form factor. The rate-energy tradeoff for the two architectures are characterized by so-called rate-energy (R-E) region. The optimal transmission strategy is derived to achieve different rate-energy tradeoffs. With receiver circuit power consumption taken into account, it is shown that the OPS scheme is optimal for both receivers. For the ideal case when the receiver circuit does not consume power, the SPS scheme is optimal for both receivers. In addition, we study the performance for the two types of receivers under realistic system setup that employs practical modulation. Our results provide useful insights to the optimal practical receiver design for simultaneous wireless information and power transfer (SWIPT). (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) Receiver Paper 3148780 title: Training-Based SWIPT: Optimal Power Splitting at the (cid:44) Paper 3148780 abstract: We consider point-to-point system with (cid:44) simultaneous wireless information and power transfer (SWIPT) over block-fading channel. Each transmission block consists of training phase and data transmission phase. Pilot symbols are transmitted during the training phase for channel estimation at the receiver. To enable SWIPT, the receiver adopts power-splitting design, such that portion of the received signal is used for channel estimation or data detection, while the rest is used for energy harvesting. We optimally design the power-splitting ratios for both training and data phases to achieve the best ergodic capacity performance while maintaining required energy harvesting rate. Our result shows how power-splitting receiver can make the best use of the received pilot and data signals to obtain optimal SWIPT performance. (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) Paper 7151441 title: Wireless Information and Power Transfer: Dynamic (cid:44) Power Splitting Approach 71 Paper 7151441 abstract: Energy harvesting is promising solution to (cid:44) prolong the operation time of energy-constrained wireless networks. In particular, scavenging energy from ambient radio signals, namely wireless energy harvesting (WEH), has recently drawn significant attention. In this paper, we consider point-to-point wireless link over the flat-fading channel, where the receiver has no fixed power supplies and thus needs to replenish energy via WEH from the signals sent by the transmitter. We first consider SISO (single-input single-output) system where the single-antenna receiver cannot decode information and harvest energy independently from the same signal received. Under this practical constraint, we propose dynamic power splitting (DPS) scheme, where the received signal is split into two streams with adjustable power levels for information decoding and energy harvesting separately based on the instantaneous channel condition that is assumed to be known at the receiver. We derive the optimal power splitting rule at the receiver to achieve various trade-offs between the maximum ergodic capacity for information transfer and the maximum average harvested energy for power transfer, which are characterized by the boundary of so-called \"rate-energy (R-E)\" region. Moreover, for the case when the channel state information is also known at the transmitter, we investigate the joint optimization of transmitter power control and receiver power splitting. The achievable R-E region by the proposed DPS scheme is also compared against that by the existing time switching scheme as well as performance upper bound by ignoring the practical receiver constraint. Finally, we extend the result for optimal DPS to the SIMO (single-input multiple-output) system where the receiver is equipped with multiple antennas. In particular, we investigate low-complexity power splitting scheme, namely antenna switching, which achieves the near-optimal rate-energy trade-offs as compared to the optimal DPS. (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) Energy Harvesting Paper 16191957 title: Wireless Information Transfer with Opportunistic (cid:44) Paper 16191957 abstract: Energy harvesting is promising solution to prolong the operation of energy-constrained wireless networks. In (cid:44) particular, scavenging energy from ambient radio signals, namely wireless energy harvesting (WEH), has recently drawn significant attention. In this paper, we consider point-to-point wireless link over the narrowband flat-fading channel subject to time-varying co-channel interference. It is assumed that the receiver has no fixed power supplies and thus needs to replenish energy opportunistically via WEH from the unintended interference and/or the intended signal sent by the transmitter. We further assume single-antenna receiver that can only decode information or harvest energy at any time due to the practical circuit limitation. Therefore, it is important to investigate when the receiver should switch between the two modes of information decoding (ID) and energy harvesting (EH), based on the instantaneous channel and interference condition. In this paper, we derive the optimal mode switching rule at the receiver to achieve various trade-offs between wireless information transfer and energy harvesting. Specifically, we determine the minimum transmission outage probability for delay-limited information transfer and the maximum ergodic capacity for no-delay-limited information transfer versus the maximum average energy harvested at the receiver, which are characterized by the boundary of so-called \"outage-energy\" region and \"rate-energy\" region, respectively. Moreover, for the case when the channel state information (CSI) is known at the transmitter, we investigate the joint optimization of transmit power control, information and energy transfer scheduling, and the receiver's mode switching. The effects of circuit energy consumption at the receiver on the achievable rate-energy trade-offs are also characterized. Our results provide useful guidelines for the efficient design of emerging wireless communication systems powered by opportunistic WEH. (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) 72 Respond with the following json schema: { \"$defs\": { \"Cell\": { \"description\": \"A Cell Object consists of paper ID, column name (cid:44) andnthe corresponding cell value at that row & column in the table.\", (cid:44) \"properties\": { \"paper_id\": { \"title\": \"Paper Id\", \"type\": \"string\" }, \"column_name\": { \"title\": \"Column Name\", \"type\": \"string\" }, \"cell_value\": { \"title\": \"Cell Value\", \"type\": \"string\" } }, \"required\": [ \"paper_id\", \"column_name\", \"cell_value\" ], \"title\": \"Cell\", \"type\": \"object\" } }, \"description\": \"A Table Object is List of Cell Objects.\", \"properties\": { \"cell_values\": { \"items\": { \"$ref\": \"#/$defs/Cell\" }, \"title\": \"Cell Values\", \"type\": \"array\" } }, \"required\": [ \"cell_values\" ], \"title\": \"Table\", \"type\": \"object\" } H.5.2 TABLE UNROLLING PROMPT (cid:44) (cid:44) (cid:44) (cid:44) You are helpful AI assistant that can help infer useful information (cid:44) from tables comparing sets of scientific papers. You are given comparison table in markdown format. Every row in the table contains information about scientific paper. Your goal is to rewrite the information conveyed by each cell in the table in the form of natural language statements. Each statement is an atomic unit of information from the table. (cid:44) Follow the instructions given below to do so: 1. Identify the column headers in the table. 2. Identify the various rows in the table. 3. For each row, go through every cell in that row (excluding the first (cid:44) 4. Use the paper ID and information from column headers when writing (cid:44) one that refers to paper ID) and write one atomic statement per cell. these statements. 73 and output as numbered list. 5. Write all such statements in natural language (excluding icons/emojis) (cid:44) 6. Do not exclude any detail that is present in the given table, or add (cid:44) 7. Do not include any citation information in the statements. extra details that are not present in the table. Table: [TABLE] Statements: H.5.3 EVALUATION PROMPT Following is series of informative statements about set of scientific (cid:44) [UNROLLED_TABLE] research papers: Given these statements, only state if the following statement is true, (cid:44) Statement: [STATEMENT] false or unknown. Answer: H.6 SUPER-EX T H.6.1 EXAMPLE PROBLEM Run probability-based prompt selection on the SST-2 dataset using (cid:44) opt-125m as the base model with the script `run_prompt_selection.py`. Report metrics. (cid:44) make arbitrary assumptions and decisions to fulfill the task. Additional instructions: If any details are missing from the task description, you are allowed to (cid:44) To reduce execution time: 1. Load/preprocess only the first 10 rows of each set in the dataset. 2. Only run single epoch (when training). 3. Make sure you only run (cid:44) single experiment, disabling any grid searchor hyperparameter tuning. (cid:44) Git repository: https://github.com/soheeyang/unified-prompt-selection H.7 CORE-BE H-HA H.7.1 EXAMPLE PROBLEM The task input for the agent: Task: codeocean_hard 74 (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) Your goal is to test the computational reproducibility of the repository cloned to your current directory, which is code from scientific (cid:44) paper. Specifically, you need to Run the manuscript.Rmd file using Rscript and render it as html. Put the results in the \"../results\" folder. . Save your report to file named report.json in the environment directory you started in that contains the capsule itself, where you fill in all of the following fields: dict_keys(['Report the final outcomes of reproducibility checks at the article level after original authors were contacted (Table 1 of the manuscript). You should report for row 1 in the table (ignore the confidence interval).', 'Report the final outcomes of reproducibility checks at the article level after original authors were contacted (Table 1 of the manuscript). You should report for row 2 in the table (ignore the confidence interval).', 'Report the final outcomes of reproducibility checks at the article level after original authors were contacted (Table 1 of the manuscript). You should report for row 3 in the table (ignore the confidence interval).', 'fig From Figure 1, report the proportion of articles with fully reproducible target values from the random effects model after author contact. Ignore the confidence intervals']). You should install all of the requirements found in the Readme file and then run the commands necessary to answer the questions. (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) The top-level contents of the corresponding capsule (with red items being omitted in the Hard version we use): ./ REPRODUCING.md code/ LICENSE README.md config.json lib.py lib2.py lib2noDTW.py librun.py preprocess.py run run.ipynb data/ LICENSE testPreprocessed.pickle testRemoveBeginLast testRemoveBeginLast_10_15 testRemoveBeginLast_15_20 testRemoveBeginLast_20_25 testRemoveBeginLast_25_30 testRemoveBeginLast_5 testRemoveBeginLast_5_10 test_quicktest train trainTrajModel.pickle train_quicktest environment/ Dockerfile metadata/ metadata.yml results/ expResult.pickle 75 expResult_noDTW.pickle output output.txt output_noDTW.txt run.html And the (abridged) content of the README.md file: # HyperETA These are the program of the paper ***HyperETA: Non-Deep-Learning (cid:44) Method for Estimated Time of Arrival***. ... # Data ## train Raw trajectories for train. ## train_quicktest ... ## trainTrajModel.pickle The trajectories model, includes 3 tables * Hypercube series table : Preprocessed trajectories. * Original trajectories table: Original GPS data. * Mapping table : It map hypercubes to original trajectories. ... H.8 DS-1000 H.8.1 EXAMPLE PROBLEM Problem: Given 3d tenzor, say: batch sentence length embedding dim = torch.rand((10, 1000, 96)) and an array(or tensor) of actual lengths for each sentence lengths = outputs tensor([ 370., 502., 652., 859., 545., 964., 566., 576.,1000., (cid:44) torch .randint(1000,(10,)) 803.]) How to fill tensor with 2333 after certain index along dimension 1 (cid:44) (sentence length) according to tensor lengths ? want smth like that : a[ : , lengths : , : ] = 2333 A: <code> import numpy as np import pandas as pd import torch = torch.rand((10, 1000, 96)) lengths = torch.randint(1000, (10,)) </code> = ... # put solution in this variable BEGIN SOLUTION 76 <code> Write the remaining python code to append to the program above (but do (cid:44) not repeat the part of the code that is already given in `<code>...</code>`; just write the new code). <code> and </code> tags. Put your answer inside (cid:44) (cid:44) H.9 DI V YBE H.9.1 EXAMPLE PROBLEM Dataset path: nls_bmi_raw/nls_raw.csv Dataset description: The dataset contains information from National (cid:44) Longitudinal Survey of Youth (NLSY79). It includes information about the Demographics, Family Background, Education, Health, Residential, Financial & Criminal Records of the participants. (cid:44) (cid:44) (cid:44) (cid:44) Brief description of columns: ID# (range 1-12686) 1979: Unique Identifier of the respondent, Sample ID, 1979 (interview): Sample Identification Code, Age of respondent, 1979: Age of respondent in 1979, Age of respondent at interview date, 1981: Age of respondent in 1981, Age of respondent at interview date, 1989: Age of respondent in 1989, Occupation of adult male in household at age 14, 1979: Occupation of the (cid:44) adult male present in the household of the respondent at age 14 in 1979. Variable records the occupation of the father figure of the repondent, values include FARMER AND FARM MANAGERS, PROFESSIONAL,TECHNICAL AND KINDRED etc, (cid:44) Highest grade completed by respondent's mother, 1979: Highest grade or (cid:44) year of regular school that respondent's mother ever completed till 1979, (cid:44) Highest grade completed by respondent's father, 1979: Highest grade or (cid:44) year of regular school that respondent's father ever completed till 1979, that respondent have completed and got credit for till 1979, (cid:44) Highest grade completed, 1979: Highest grade or year of regular school (cid:44) Racial/ethnic cohort, 1979: Respondent's racial/ethnic cohort, contains (cid:44) Sex of respondent, 1979: Sex of the respondent, 1:MALE or 2:FEMALE, Family size, 1979: Family size of the respondent in 1979, Ever convicted of an illegal act in adult court before 1980: Boolean (cid:44) one of three values 1:BLACK, 2:HISPANIC, 3:NON-BLACK NON-HISPANIC, variable that indicates if the respondent was convicted of an illegal act in adult court other than minor traffic violations before 1980, (cid:44) Ever been sentenced in any correctional institution before 1980: Boolean (cid:44) variable that indicated if the respondent was sentenced to spend time in corrections institute, like jail, prison, or youth institution like training school or reform school or not before 1980, (cid:44) Height of respondent, 1981: Height of the respondent in inches in 1981, Height of respondent, 1985: Height of the respondent in inches in 1985, Weight of respondent, 1981: Weight of the respondent in kilograms in (cid:44) Weight of respondent, 1989: Weight of the respondent in kilograms in (cid:44) Weight of respondent, 1992: Weight of the respondent in kilograms in (cid:44) Rank in class last year attended at this school, 1981: Respondent's rank (cid:44) in the class that he attended in school last year (in 1980) (variable recorded in 1981), (cid:44) Number of students in class last year attended at this school, 1981: (cid:44) Number of students in the respondent's class for the last year attended this school, 1981, 1989, 1992, (cid:44) (cid:44) (cid:44) 77 (cid:44) (cid:44) (cid:44) (cid:44) ASVAB - Arithmetic Reasoning Score (rounded), 1981: This variable (cid:44) represents the standardized scores of respondents on the Arithmetic Reasoning section of the ASVAB test. It provides way to compare individuals' performance on this specific aspect of the test within standardized framework., (cid:44) ASVAB - Word Knowledge Score (rounded), 1981: This variable represents the standardized scores of respondents on the Word Knowledge section (cid:44) of the ASVAB test, allowing for comparison of individuals' performance on this specific aspect of the test within standardized framework., (cid:44) ASVAB - Paragraph Comprehension Score (rounded), 1981: This variable represents the standardized scores of respondents on the Paragraph (cid:44) Comprehension section of the ASVAB test, allowing for comparison of individuals' performance on this specific aspect of the test within standardized framework., (cid:44) ASVAB - Mathematics Knowledge Score (rounded), 1981: This variable (cid:44) represents the standardized scores of respondents on the Mathematics Knowledge section of the ASVAB test, facilitating comparison of individuals' performance on this specific aspect of the test within standardized framework., (cid:44) (cid:44) (cid:44) (cid:44) 3:DORM, 6:OTHER TEMPORARY 12:ON-BASE MIL FAM HOUSING, 3:DORM, 6:OTHER TEMPORARY 12:ON-BASE MIL FAM HOUSING, (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) 5:JAIL, 5:JAIL, 4:HOSPITAL, 4:HOSPITAL, 11:OWN DWELLING UNIT, 11:OWN DWELLING UNIT, 15:RELIGIOUS 17:PARENTAL, 15:RELIGIOUS 17:PARENTAL, 19:R IN PARENTAL HOUSEHOLD, 19:R IN PARENTAL HOUSEHOLD, 2:BACHELOR, OFFICER QUARTERS, 2:BACHELOR, OFFICER QUARTERS, (cid:44) Type of residence respondent is living in, 1981: Type of residence respondent is living in the 1981, contains one of these values (cid:44) 1:ABOARD SHIP, BARRACKS, FRATERNITY, SORORITY, QUARTERS, 13:OFF-BASE MIL FAM HOUSING, INSTITUTION, 18:HHI CONDUCTED WITH PARENT, 14:ORPHANAGE, 16:OTHER INDIVIDUAL QUARTERS, (cid:44) Type of residence respondent is living in, 1982: Type of residence respondent is living in the 1982, contains one of these values (cid:44) 1:ABOARD SHIP, BARRACKS, FRATERNITY, SORORITY, QUARTERS, 13:OFF-BASE MIL FAM HOUSING, INSTITUTION, 18:HHI CONDUCTED WITH PARENT, 14:ORPHANAGE, 16:OTHER INDIVIDUAL QUARTERS, (cid:44) Type of residence respondent is living in, 1983: Type of residence respondent is living in the 1983, contains one of these values (cid:44) 1:ABOARD SHIP, BARRACKS, FRATERNITY, SORORITY, QUARTERS, 13:OFF-BASE MIL FAM HOUSING, INSTITUTION, 18:HHI CONDUCTED WITH PARENT, 14:ORPHANAGE, 16:OTHER INDIVIDUAL QUARTERS, (cid:44) Type of residence respondent is living in, 1984: Type of residence respondent is living in the 1984, contains one of these values (cid:44) 1:ABOARD SHIP, BARRACKS, FRATERNITY, SORORITY, QUARTERS, 13:OFF-BASE MIL FAM HOUSING, INSTITUTION, 18:HHI CONDUCTED WITH PARENT, 14:ORPHANAGE, 16:OTHER INDIVIDUAL QUARTERS, (cid:44) Type of residence respondent is living in, 1985: Type of residence respondent is living in the 1985, contains one of these values (cid:44) 1:ABOARD SHIP, BARRACKS, FRATERNITY, SORORITY, QUARTERS, 13:OFF-BASE MIL FAM HOUSING, INSTITUTION, 18:HHI CONDUCTED WITH PARENT, 14:ORPHANAGE, 16:OTHER INDIVIDUAL QUARTERS, 2:BACHELOR, OFFICER QUARTERS, 2:BACHELOR, OFFICER QUARTERS, 2:BACHELOR, OFFICER QUARTERS, 19:R IN PARENTAL HOUSEHOLD, 19:R IN PARENTAL HOUSEHOLD, 19:R IN PARENTAL HOUSEHOLD, 15:RELIGIOUS 17:PARENTAL, 15:RELIGIOUS 17:PARENTAL, 15:RELIGIOUS 17:PARENTAL, 11:OWN DWELLING UNIT, 11:OWN DWELLING UNIT, 11:OWN DWELLING UNIT, 4:HOSPITAL, 4:HOSPITAL, 4:HOSPITAL, 5:JAIL, 5:JAIL, 5:JAIL, (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) 3:DORM, 6:OTHER TEMPORARY 12:ON-BASE MIL FAM HOUSING, 3:DORM, 6:OTHER TEMPORARY 12:ON-BASE MIL FAM HOUSING, 3:DORM, 6:OTHER TEMPORARY 12:ON-BASE MIL FAM HOUSING, 78 3:DORM, 6:OTHER TEMPORARY 12:ON-BASE MIL FAM HOUSING, 3:DORM, 6:OTHER TEMPORARY 12:ON-BASE MIL FAM HOUSING, 3:DORM, 6:OTHER TEMPORARY 12:ON-BASE MIL FAM HOUSING, 3:DORM, 6:OTHER TEMPORARY 12:ON-BASE MIL FAM HOUSING, (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) 5:JAIL, 5:JAIL, 5:JAIL, 5:JAIL, 4:HOSPITAL, 4:HOSPITAL, 4:HOSPITAL, 4:HOSPITAL, 11:OWN DWELLING UNIT, 11:OWN DWELLING UNIT, 11:OWN DWELLING UNIT, 11:OWN DWELLING UNIT, 15:RELIGIOUS 17:PARENTAL, 15:RELIGIOUS 17:PARENTAL, 15:RELIGIOUS 17:PARENTAL, 19:R IN PARENTAL HOUSEHOLD, 19:R IN PARENTAL HOUSEHOLD, 19:R IN PARENTAL HOUSEHOLD, 2:BACHELOR, OFFICER QUARTERS, 2:BACHELOR, OFFICER QUARTERS, 2:BACHELOR, OFFICER QUARTERS, 2:BACHELOR, OFFICER QUARTERS, Type of residence respondent is living in, 1986: Type of residence respondent is living in the 1986, contains one of these values (cid:44) 1:ABOARD SHIP, BARRACKS, FRATERNITY, SORORITY, QUARTERS, 13:OFF-BASE MIL FAM HOUSING, INSTITUTION, 18:HHI CONDUCTED WITH PARENT, 14:ORPHANAGE, 16:OTHER INDIVIDUAL QUARTERS, (cid:44) Type of residence respondent is living in, 1987: Type of residence respondent is living in the 1987, contains one of these values (cid:44) 1:ABOARD SHIP, BARRACKS, FRATERNITY, SORORITY, QUARTERS, 13:OFF-BASE MIL FAM HOUSING, INSTITUTION, 18:HHI CONDUCTED WITH PARENT, 14:ORPHANAGE, 16:OTHER INDIVIDUAL QUARTERS, (cid:44) Type of residence respondent is living in, 1988: Type of residence respondent is living in the 1988, contains one of these values (cid:44) 1:ABOARD SHIP, BARRACKS, FRATERNITY, SORORITY, QUARTERS, 13:OFF-BASE MIL FAM HOUSING, INSTITUTION, 18:HHI CONDUCTED WITH PARENT, 14:ORPHANAGE, 16:OTHER INDIVIDUAL QUARTERS, (cid:44) Type of residence respondent is living in, 1989: Type of residence respondent is living in the 1989, contains one of these values (cid:44) 1:ABOARD SHIP, BARRACKS, FRATERNITY, SORORITY, QUARTERS, 13:OFF-BASE MIL FAM HOUSING, INSTITUTION, 18:HHI CONDUCTED WITH PARENT, 14:ORPHANAGE, 16:OTHER INDIVIDUAL QUARTERS, (cid:44) Type of residence respondent is living in, 1990: Type of residence respondent is living in the 1990, contains one of these values (cid:44) 1:ABOARD SHIP, BARRACKS, FRATERNITY, SORORITY, QUARTERS, 13:OFF-BASE MIL FAM HOUSING, INSTITUTION, 18:HHI CONDUCTED WITH PARENT, 14:ORPHANAGE, 16:OTHER INDIVIDUAL QUARTERS, (cid:44) Type of residence respondent is living in, 1991: Type of residence respondent is living in the 1991, contains one of these values (cid:44) 1:ABOARD SHIP, BARRACKS, FRATERNITY, SORORITY, QUARTERS, 13:OFF-BASE MIL FAM HOUSING, INSTITUTION, 18:HHI CONDUCTED WITH PARENT, 14:ORPHANAGE, 16:OTHER INDIVIDUAL QUARTERS, (cid:44) Type of residence respondent is living in, 1992: Type of residence respondent is living in the 1992, contains one of these values (cid:44) 1:ABOARD SHIP, BARRACKS, FRATERNITY, SORORITY, QUARTERS, 13:OFF-BASE MIL FAM HOUSING, INSTITUTION, 18:HHI CONDUCTED WITH PARENT, 14:ORPHANAGE, 16:OTHER INDIVIDUAL QUARTERS, (cid:44) Type of residence respondent is living in, 1993: Type of residence respondent is living in the 1993, contains one of these values (cid:44) 1:ABOARD SHIP, BARRACKS, FRATERNITY, SORORITY, QUARTERS, 13:OFF-BASE MIL FAM HOUSING, INSTITUTION, 18:HHI CONDUCTED WITH PARENT, 14:ORPHANAGE, 16:OTHER INDIVIDUAL QUARTERS, 2:BACHELOR, OFFICER QUARTERS, 2:BACHELOR, OFFICER QUARTERS, 2:BACHELOR, OFFICER QUARTERS, 2:BACHELOR, OFFICER QUARTERS, 19:R IN PARENTAL HOUSEHOLD, 19:R IN PARENTAL HOUSEHOLD, 19:R IN PARENTAL HOUSEHOLD, 19:R IN PARENTAL HOUSEHOLD, 19:R IN PARENTAL HOUSEHOLD, 15:RELIGIOUS 17:PARENTAL, 15:RELIGIOUS 17:PARENTAL, 15:RELIGIOUS 17:PARENTAL, 15:RELIGIOUS 17:PARENTAL, 15:RELIGIOUS 17:PARENTAL, 11:OWN DWELLING UNIT, 11:OWN DWELLING UNIT, 11:OWN DWELLING UNIT, 11:OWN DWELLING UNIT, 4:HOSPITAL, 4:HOSPITAL, 4:HOSPITAL, 4:HOSPITAL, 5:JAIL, 5:JAIL, 5:JAIL, 5:JAIL, (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) 3:DORM, 6:OTHER TEMPORARY 12:ON-BASE MIL FAM HOUSING, 3:DORM, 6:OTHER TEMPORARY 12:ON-BASE MIL FAM HOUSING, 3:DORM, 6:OTHER TEMPORARY 12:ON-BASE MIL FAM HOUSING, 3:DORM, 6:OTHER TEMPORARY 12:ON-BASE MIL FAM HOUSING, 79 3:DORM, 6:OTHER TEMPORARY (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) 5:JAIL, 4:HOSPITAL, 11:OWN DWELLING UNIT, 2:BACHELOR, OFFICER QUARTERS, Type of residence respondent is living in, 1994: Type of residence respondent is living in the 1994, contains one of these values (cid:44) 1:ABOARD SHIP, BARRACKS, FRATERNITY, SORORITY, QUARTERS, 13:OFF-BASE MIL FAM HOUSING, INSTITUTION, 18:HHI CONDUCTED WITH PARENT, 14:ORPHANAGE, 16:OTHER INDIVIDUAL QUARTERS, (cid:44) Type of residence respondent is living in, 1996: Type of residence respondent is living in the 1996, contains one of these values (cid:44) 1:ABOARD SHIP, BARRACKS, FRATERNITY, SORORITY, QUARTERS, 13:OFF-BASE MIL FAM HOUSING, INSTITUTION, 18:HHI CONDUCTED WITH PARENT, 14:ORPHANAGE, 16:OTHER INDIVIDUAL QUARTERS, 2:BACHELOR, OFFICER QUARTERS, 19:R IN PARENTAL HOUSEHOLD, 19:R IN PARENTAL HOUSEHOLD, 15:RELIGIOUS 17:PARENTAL, 15:RELIGIOUS 17:PARENTAL, 11:OWN DWELLING UNIT, 4:HOSPITAL, 5:JAIL, (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) 12:ON-BASE MIL FAM HOUSING, 12:ON-BASE MIL FAM HOUSING, 3:DORM, 6:OTHER TEMPORARY all asset values and subtracting all debts for the year 1985, (cid:44) Family net wealth, 1985: Total Net Wealth for Family. Created by summing (cid:44) Family net wealth, 1990: Total Net Wealth for Family. Created by summing all asset values and subtracting all debts for the year 1990, (cid:44) Family net wealth, 1996 (key data point): Total Net Wealth for Family. (cid:44) Created by summing all asset values and subtracting all debts for the year 1996, value of residential property that respondent/spouse owned in 1985, (cid:44) Market value of residential property respondent/spouse own, 1985: Market (cid:44) Market value of residential property respondent/spouse own, 1990: Market (cid:44) Market value of residential property respondent/spouse own, 1996: Market (cid:44) Total market value of farm, business, and other property, 1985: Total (cid:44) value of residential property that respondent/spouse owned in 1990, value of residential property that respondent/spouse owned in 1996, market value of all of the real estate, assets in the business(es), farm operation(s) in 1985, (cid:44) Total market value of farm, business, and other property, 1990: Total (cid:44) market value of all of the real estate, assets in the business(es), farm operation(s) in 1990, (cid:44) Total market value of farm, business, and other property, 1996: Total (cid:44) market value of all of the real estate, assets in the business(es), farm operation(s) in 1996, (cid:44) Market Value of vehicles respondent/spouse own, 1985: Total market value (cid:44) of all vehicles including automobiles that respondent/spouse owned in 1985, (cid:44) Market Value of vehicles respondent/spouse own, 1990: Total market value (cid:44) of all vehicles including automobiles that respondent/spouse owned in 1990, (cid:44) Market Value of vehicles respondent/spouse own, 96: Total market value of all vehicles including automobiles that respondent/spouse owned in (cid:44) 1996, (cid:44) Total market value of items over $500, 1985: Total market value of all (cid:44) the other assets of the respondent that were worth more than $500 in 1985, (cid:44) Total market value of items over $500, 1990: Total market value of all (cid:44) the other assets of the respondent that were worth more than $500 in 1990, (cid:44) Total market value of items over $500, 1996: Total market value of all (cid:44) the other assets of the respondent that were worth more than $500 in 1996, (cid:44) Total net family income, previous calendar year, 1979: Total net family income for the previous calendar year (1978) (recorded in 1979), (cid:44) Total net family income, previous calendar year, 1985: Total net family income for the previous calendar year (1984) (recorded in 1985), (cid:44) Total net family income, previous calendar year, 1989: Total net family income for the previous calendar year (1989) (recorded in 1989), (cid:44) (cid:44) CHANGE, 4:NO SAVINGS, Was more money put into or taken out of R/spouse savings since last (cid:44) interview, 1989: Categorical variable indicating if was more money was put into or taken out of respondent/spouse savings since last interview in 1989. (cid:44) It contains four values 1:PUT MORE MONEY IN, 2:TOOK MORE MONEY OUT, 3:NO (cid:44) Net amount respondent/spouse put into savings since last interview, 1989: Net amount of money that respondent/spouse put into their savings (cid:44) since last interview in 1989, (cid:44) Net amount respondent/spouse took out of savings since last interview, (cid:44) 1989: Net amount of money that respondent/spouse took out of savings since last interview in 1989, (cid:44) Query: Does increased time preference leads to higher BMI? In the final answer, please output json containing two keys: { } 'hypothesis': SCIENTIFIC HYPOTHESIS, 'workflow': WORKFLOW SUMMARY where the SCIENTIFIC HYPOTHESIS is natural language hypothesis, derived (cid:44) from the provided dataset, clearly stating the context of hypothesis (if any), variables chosen (if any) and relationship between those variables (if any) including any statistical significance. Please include all numeric information as necessary to support the hypothesis. (cid:44) (cid:44) (cid:44) (cid:44) and the WORKFLOW SUMMARY is summary of the full workflow starting from (cid:44) data loading that led to the final hypothesis. Make sure you load the dataset to analyze it (or defer to an agent that (cid:44) can). H.10 E2E-BE H.10.1 EXAMPLE PROBLEM You are an autonomous agent, tasked to perform the following research (cid:44) task: **TASK DEFINITION**: ================ **Name**: simple-dag-enhancement **Short Description**: Enhancing the static DAG-ERC model with simple (cid:44) content-based edge selection for improved emotion recognition in conversations. (cid:44) **Long Description**: This research explores simplified enhancement to (cid:44) the static DAG construction in the DAG-ERC model by implementing basic content-aware edge selection mechanism. Rather than developing fully dynamic DAG construction approach, we focus on augmenting the existing static DAG with small number of additional edges based on simple content similarity metrics between utterances. This approach maintains the core structure of the original DAG-ERC model while potentially capturing additional relevant connections that may improve emotion recognition performance. (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) **Hypothesis to explore**: Augmenting the static DAG structure with (cid:44) small number of additional edges based on content similarity between utterances will improve emotion recognition performance compared to the original static DAG-ERC model, particularly for conversations where important contextual relationships span beyond the immediate dialogue history. (cid:44) (cid:44) (cid:44) (cid:44) Metric to use; The primary metrics will be weighted-average F1 score and (cid:44) micro-averaged F1 score (excluding the majority class) for emotion recognition, consistent with the original DAG-ERC paper. We will also analyze the number and distribution of additional edges to understand the impact of our enhancement. (cid:44) (cid:44) (cid:44) (cid:44) **Baselines**: We will compare our enhanced DAG-ERC against: (1) the (cid:44) original DAG-ERC with static rules, and (2) fully-connected graph baseline where all utterances are connected to all previous utterances (up to fixed window size). (cid:44) **Research Idea Variables**: Independent variables include the DAG (cid:44) construction method (original static DAG, our enhanced DAG with content-based edges), the similarity threshold for adding edges, and the maximum number of additional edges per utterance. Control variables include the feature extraction method, the emotion recognition model architecture, and the evaluation metrics. The dependent variable is the emotion recognition performance. (cid:44) **Research Idea Design**: Implement simple enhancement to the static (cid:44) DAG construction in the DAG-ERC model by adding content-based edges between utterances. The goal is to capture additional relevant connections that may improve emotion recognition performance while maintaining the simplicity and efficiency of the original model. (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) **1. Data Preparation**: original DAG-ERC paper. - Use the IEMOCAP dataset, following the preprocessing steps in the (cid:44) - Extract small subset (e.g., 20 conversations) for the pilot study. **2. Enhanced DAG Construction**: DAG-ERC paper (based on speaker identity and positional relations). - Start with the static DAG constructed using the original rules from the (cid:44) - For each utterance, compute its content similarity with all previous (cid:44) utterances (within reasonable window, e.g., 10 utterances) using simple metric such as cosine similarity between RoBERTa embeddings. (cid:44) - Add additional edges from previous utterances to the current utterance if their similarity exceeds threshold (e.g., 0.8) and they are not (cid:44) already connected in the static DAG. (cid:44) - Limit the number of additional edges per utterance (e.g., maximum 3) to (cid:44) **3. Implementation Details**: maintain sparsity. training the emotion recognition model. recognition model and the similarity computation. - Use RoBERTa-Base as the feature extractor for both the emotion (cid:44) - Implement the enhanced DAG construction as preprocessing step before (cid:44) - Experiment with different similarity thresholds (e.g., 0.7, 0.8, 0.9) (cid:44) - Use the original DAG-ERC model architecture without modifications for (cid:44) **4. Training and Evaluation**: and maximum number of additional edges (e.g., 1, 3, 5). the emotion recognition task. structure. - Train the model on the IEMOCAP dataset using the enhanced DAG (cid:44) - Compare the performance with the original DAG-ERC model and the (cid:44) - Analyze the number and distribution of additional edges added by the (cid:44) fully-connected baseline. enhancement. 82 - Identify specific examples where the enhanced DAG leads to correct (cid:44) **5. Output and Analysis**: predictions that were incorrect with the original DAG. few example conversations. - Save the trained models and their performance metrics. - Generate visualizations of the original and enhanced DAG structures for (cid:44) - Analyze the relationship between the number of additional edges and the (cid:44) - Investigate which types of conversations benefit most from the enhanced (cid:44) emotion recognition performance. DAG structure. For the pilot experiment, implement the enhanced DAG construction (cid:44) approach on 20 conversations from the IEMOCAP dataset to validate the approach before scaling to the full experiment. Focus on single similarity threshold (e.g., 0.8) and single maximum number of additional edges (e.g., 3) for simplicity. (cid:44) (cid:44) (cid:44) ------ end of task definition ----- NOW: Please perform this task and produce four results: (cid:44) 1. report, describing the results of your research. The report should (cid:44) include, among other things, the following parts: Title, Abstract, Introduction, Approach, Experiments, Results, Conclusion, References. (cid:44) 2. The code you wrote to perform the research. 3. trace/log of your research. The trace should give step-by-step (cid:44) description of the actions the agent (you) took, e.g., searching the literature, writing and executing code, analyzing results. The trace should also include the results of those actions, e.g., the papers found, the experimental results from code execution, etc. (cid:44) 4. Any other research artifacts (datasets, analyses, results, etc.) that (cid:44) you generated, to substantiate your report. If these artifacts (e.g., dataset) are large, only show part of them but enough to convey their contents. (cid:44) (cid:44) (cid:44) (cid:44) These results will be used to assess how well you performed the task. Return your answer in the following JSON structure (a dictionary (cid:44) containing single top-level key, `results`, which is dictionary containing the keys `report`, `code`, `trace`, and `artifacts`, in exactly the format described below): (cid:44) (cid:44) ``` { \"results\": { \"report\"(str): <report>, \"code\"(list): [ {\"filename\"(str): <filename1>, \"code\"(str): <code1>}, {\"filename\"(str): <filename2>, \"code\"(str): <code2>}, ... ], \"trace\"(str): <trace>, \"artifacts\"(list): [ {\"filename\"(str): <filename1>, \"artifact\"(str): <artifact1>}, {\"filename\"(str): <filename2>, \"artifact\"(str): <artifact2>}, ... ] } } ``` where <report> is multiline string that contains the report, <trace> is multiline string that contains trace (or summary of the trace) of (cid:44) the agent's behavior while solving the task, and the artifacts are products of the research (created datasets, etc.) (cid:44) (cid:44) 83 H.11 E2E-BE H-HA H.11.1 EXAMPLE PROBLEM task: You are an autonomous agent, tasked to perform the following research (cid:44) TASK DEFINITION: ================ Name: Adaptive Reasoning Enhancement Short Description: Combining Complexity-Based Prompting and Imitation (cid:44) Demonstration Learning to improve language models' generalization on unseen tasks. (cid:44) Hypothesis to explore: Integrating Complexity-Based Prompting with (cid:44) Imitation Demonstration Learning will enhance the generalization capabilities of language models, resulting in improved performance on unseen tasks by dynamically adapting reasoning complexity and demonstration selection. (cid:44) (cid:44) (cid:44) --- Key Variables: Independent variable: Integration of Complexity-Based Prompting with (cid:44) Imitation Demonstration Learning Dependent variable: Generalization capabilities of language models on (cid:44) unseen tasks Comparison groups: Four conditions: Baseline (standard prompting), (cid:44) CBP-only, IDL-only, and Integrated (CBP+IDL) Baseline/control: Standard prompting without CBP or IDL Context/setting: Complex multi-step reasoning problems Assumptions: Complexity-Based Prompting enhances reasoning by focusing on high-complexity rationales, while Imitation Demonstration Learning (cid:44) reinforces learning through imitation (cid:44) Relationship type: Causal (integration 'will enhance' capabilities) Population: Language models Timeframe: Not specified Measurement method: Primary metric: Accuracy on unseen tasks; Secondary metrics: Reasoning complexity, demonstration effectiveness, and (cid:44) response quality (cid:44) --- 84 Long Description: Description: The research explores the integration of (cid:44) Complexity-Based Prompting and Imitation Demonstration Learning to enhance the generalization capabilities of language models on unseen tasks. Complexity-Based Prompting involves selecting prompts based on reasoning complexity, guiding the model through intricate reasoning chains. Imitation Demonstration Learning strengthens the learning process by mimicking human review strategies, selecting similar examples for new questions and re-answering based on retrieved examples. The hypothesis posits that combining these methods will allow the model to dynamically adapt its reasoning complexity and demonstration selection, leading to improved performance on unseen tasks. This approach addresses the gap in existing research by offering novel combination of methods to enhance model adaptability and reasoning capabilities. The expected outcome is that the model will perform better on unseen tasks by leveraging complex reasoning chains and effective demonstration selection. This research is significant as it provides new perspective on enhancing language models' reasoning abilities, potentially leading to more robust and adaptable AI systems. --- Key Variables:[Complexity-Based Prompt- (cid:44) ing](https://www.semanticscholar.org/paper/f48e0406bfac8025b36982c94a9183968378587f): Complexity-Based Prompting involves selecting prompts based on the complexity of reasoning steps. This method enhances model performance on tasks requiring deep reasoning by focusing on high-complexity rationales. It involves conducting voting process among different reasoning paths to determine the most complex and informative one. The prompts guide the model through these complex reasoning chains, ensuring effective handling of intricate tasks. This variable is critical as it directly influences the model's ability to process complex reasoning tasks, improving its generalization capabilities. [Imitation Demonstration Learn- (cid:44) ing](https://www.semanticscholar.org/paper/fdbdcc3a65dfd6f258c533fd12d58bbfcab15bc3): Imitation Demonstration Learning strengthens the learning process by mimicking human review strategies. It involves selecting the most similar example to new question and re-answering according to the answering steps of the retrieved example. This approach emphasizes interactions between prompts and demonstrations, reinforcing learning through explicit imitation. It requires mechanism to select similar examples and re-answer questions, improving the model's ability to learn from demonstrations. This variable is essential as it enhances the model's ability to generalize from demonstrations by consolidating known knowledge through imitation. (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) --- 85 (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) Research Idea Design: The hypothesis will be implemented using the ASD Agent's capabilities by integrating Complexity-Based Prompting and (cid:44) Imitation Demonstration Learning. The process begins with defining set of tasks that require complex reasoning. Complexity-Based Prompting will be applied by designing prompts that include high-complexity reasoning chains. These prompts will guide the model through intricate reasoning steps, ensuring effective handling of complex tasks. Imitation Demonstration Learning will be implemented by developing mechanism to select similar examples for new questions. This involves creating system that identifies similar examples based on semantic similarity and uses them to re-answer questions, reinforcing the learning process. The integration of these methods will occur at the prompt level, where the complexity-based prompts will be combined with imitation demonstration strategies to enhance the model's reasoning capabilities. The data flow will involve feeding the model with complexity-based prompts and using the imitation demonstration mechanism to select and re-answer questions. The expected outcome is that the model will perform better on unseen tasks by leveraging complex reasoning chains and effective demonstration selection. This approach is novel as it combines two distinct methods to enhance language models' reasoning abilities, providing new perspective on improving AI systems' adaptability and performance. (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) --- Evaluation Procedure: Please implement an experiment to test the (cid:44) hypothesis that integrating Complexity-Based Prompting (CBP) with Imitation Demonstration Learning (IDL) will enhance language models' generalization capabilities on unseen reasoning tasks. The experiment should compare four conditions: (cid:44) (cid:44) (cid:44) 1. Baseline: Standard prompting without CBP or IDL 2. CBP-only: Using only Complexity-Based Prompting 3. IDL-only: Using only Imitation Demonstration Learning 4. Integrated (CBP+IDL): The experimental condition combining both (cid:44) approaches The experiment should include the following components: ## Dataset Use reasoning task dataset such as 2WikiMultiHopQA that includes (cid:44) complex multi-step reasoning problems. The dataset should be split into training (60%), validation (20%), and test (20%) sets. The test set will represent 'unseen tasks' for final evaluation. (cid:44) (cid:44) 'MINI_PILOT', 'PILOT', or 'FULL_EXPERIMENT'. ## Pilot Mode Implementation Implement global variable PILOT_MODE with three possible settings: (cid:44) - MINI_PILOT: Use 10 questions from the training set for development and (cid:44) - PILOT: Use 100 questions from the training set for development and 50 (cid:44) - FULL_EXPERIMENT: Use the entire training set for development and the (cid:44) 5 questions from the validation set for evaluation. questions from the validation set for evaluation. entire test set for final evaluation. Start with MINI_PILOT, then proceed to PILOT if successful. Do not run (cid:44) FULL_EXPERIMENT without human verification of the PILOT results. ## Complexity-Based Prompting Module Implement module that: 1. Generates multiple reasoning paths for each question in the training (cid:44) 2. Implements voting mechanism to determine the most complex and (cid:44) informative reasoning path set 3. Creates prompts that guide the model through these complex reasoning (cid:44) 4. Stores these complexity-based prompts for later use chains steps from the training set ## Imitation Demonstration Learning System Implement system that: 1. Creates database of question-answer pairs with detailed reasoning (cid:44) 2. For new questions, calculates semantic similarity to find the most (cid:44) 3. Retrieves the most similar examples and their reasoning steps 4. Constructs prompts that include these examples to guide the model in (cid:44) similar examples in the database answering new questions ## Integrated Approach (CBP+IDL) Implement the integration of CBP and IDL by: 1. Using CBP to generate complex reasoning chains for the questions 2. Using IDL to select similar examples with their reasoning steps 3. Combining both in unified prompt that includes both the complex (cid:44) 4. Implementing an adaptive mechanism that adjusts the weight given to (cid:44) CBP vs. IDL based on question characteristics reasoning guidance and the similar examples answered questions) ## Evaluation Evaluate all four conditions using: 1. Primary metric: Accuracy on unseen tasks (percentage of correctly (cid:44) 2. Secondary metrics: - Reasoning complexity (average number of reasoning steps in responses) - Demonstration effectiveness (semantic similarity between selected (cid:44) - Response quality (coherence, relevance, and logicality of reasoning), (cid:44) examples and target questions) use ROSCOE only if applicable conditions are significant: ## Statistical Analysis Perform statistical analysis to determine if differences between (cid:44) 1. Conduct paired t-tests between conditions 2. Calculate effect sizes (Cohen's d) for each comparison 3. Perform bootstrap resampling to establish confidence intervals ## Logging and Reporting Implement comprehensive logging that captures: 1. All prompts generated for each condition 2. Model responses for each question 3. Evaluation metrics for each condition 4. Statistical analysis results 5. Examples of successful and unsuccessful cases The final report should include: 1. Summary of results for each condition 2. Statistical significance of differences between conditions 3. Analysis of when and why the integrated approach performs better or (cid:44) 4. Recommendations for further improvements worse ## Implementation Details - Use NLTK for text processing and tokenization - Use scikit-learn for semantic similarity calculations and statistical (cid:44) - Use language model (e.g., GPT-4) for generating responses - Implement proper error handling and logging throughout analysis Please run the experiment in MINI_PILOT mode first, then PILOT mode if (cid:44) successful. Do not proceed to FULL_EXPERIMENT without human verification. (cid:44) --- ------ end of task definition ----- NOW: Please perform this task and produce four results: (cid:44) 1. report, describing the results of your research. The report should (cid:44) include, among other things, the following parts: Title, Abstract, Introduction, Approach, Experiments, Results, Conclusion, References. (cid:44) 2. The code you wrote to perform the research. 3. trace/log of your research. The trace should give step-by-step (cid:44) description of the actions the agent (you) took, e.g., searching the literature, writing and executing code, analyzing results. The trace should also include the results of those actions, e.g., the papers found, the experimental results from code execution, etc. (cid:44) 4. Any other research artifacts (datasets, analyses, results, etc.) that (cid:44) you generated, to substantiate your report. If these artifacts (e.g., dataset) are large, only show part of them but enough to convey their contents. (cid:44) (cid:44) (cid:44) (cid:44) These results will be used to assess how well you performed the task. Return your answer in the following JSON structure (a dictionary (cid:44) containing single top-level key, `results`, which is dictionary containing the keys `report`, `code`, `trace`, and `artifacts`, in exactly the format described below):``` (cid:44) (cid:44) { \"results\": { \"report\"(str): <report>, \"code\"(list): [ {\"filename\"(str): <filename1>, \"code\"(str): <code1>}, {\"filename\"(str): <filename2>, \"code\"(str): <code2>}, ... ], \"trace\"(str): <trace>, \"artifact\"(str): [ {\"filename\"(str): <filename1>, \"artifact\"(str): <artifact1>}, {\"filename\"(str): <filename2>, \"artifact\"(str): <artifact2>}, ... ] } } ``` where <report> is multiline string that contains the report, <trace> is multiline string that contains trace (or summary of the trace) of (cid:44) the agent's behavior while solving the task, and the artifacts are products of the research (created datasets, etc.) (cid:44) (cid:44)"
        }
    ],
    "affiliations": [
        "Allen Institute for AI",
        "Bar-Ilan University",
        "University of Arizona",
        "University of Maryland",
        "University of Washington",
        "University of Zurich"
    ]
}