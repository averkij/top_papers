{
    "paper_title": "MemSkill: Learning and Evolving Memory Skills for Self-Evolving Agents",
    "authors": [
        "Haozhen Zhang",
        "Quanyu Long",
        "Jianzhu Bao",
        "Tao Feng",
        "Weizhi Zhang",
        "Haodong Yue",
        "Wenya Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Most Large Language Model (LLM) agent memory systems rely on a small set of static, hand-designed operations for extracting memory. These fixed procedures hard-code human priors about what to store and how to revise memory, making them rigid under diverse interaction patterns and inefficient on long histories. To this end, we present \\textbf{MemSkill}, which reframes these operations as learnable and evolvable memory skills, structured and reusable routines for extracting, consolidating, and pruning information from interaction traces. Inspired by the design philosophy of agent skills, MemSkill employs a \\emph{controller} that learns to select a small set of relevant skills, paired with an LLM-based \\emph{executor} that produces skill-guided memories. Beyond learning skill selection, MemSkill introduces a \\emph{designer} that periodically reviews hard cases where selected skills yield incorrect or incomplete memories, and evolves the skill set by proposing refinements and new skills. Together, MemSkill forms a closed-loop procedure that improves both the skill-selection policy and the skill set itself. Experiments on LoCoMo, LongMemEval, HotpotQA, and ALFWorld demonstrate that MemSkill improves task performance over strong baselines and generalizes well across settings. Further analyses shed light on how skills evolve, offering insights toward more adaptive, self-evolving memory management for LLM agents."
        },
        {
            "title": "Start",
            "content": "MemSkill: Learning and Evolving Memory Skills for Self-Evolving Agents Haozhen Zhang 1 Quanyu Long 1 Jianzhu Bao 1 Tao Feng 2 Weizhi Zhang 3 Haodong Yue 4 Wenya Wang 1 Abstract Most Large Language Model (LLM) agent memory systems rely on small set of static, handdesigned operations for extracting memory. These fixed procedures hard-code human priors about what to store and how to revise memory, making them rigid under diverse interaction patterns and inefficient on long histories. To this end, we present MemSkill, which reframes these operations as learnable and evolvable memory skills, structured and reusable routines for extracting, consolidating, and pruning information from interaction traces. Inspired by the design philosophy of agent skills, MemSkill employs controller that learns to select small set of relevant skills, paired with an LLM-based executor that produces skillguided memories. Beyond learning skill selection, MemSkill introduces designer that periodically reviews hard cases where selected skills yield incorrect or incomplete memories, and evolves the skill set by proposing refinements and new skills. Together, MemSkill forms closed-loop procedure that improves both the skill-selection policy and the skill set itself. Experiments on LoCoMo, LongMemEval, HotpotQA, and ALFWorld demonstrate that MemSkill improves task performance over strong baselines and generalizes well across settings. Further analyses shed light on how skills evolve, offering insights toward more adaptive, self-evolving memory management for LLM agents. Code is available at https://github.com/ViktorAxelsen/MemSkill 6 2 0 2 2 ] . [ 1 4 7 4 2 0 . 2 0 6 2 : r 1. Introduction As Large Language Model (LLM) agents engage in longer, open-ended interactions, they must handle growing histories that are essential yet challenging to leverage, motivating 1Nanyang Technological University 2University of nois Urbana-Champaign 3University of 4Tsinghua University. Zhang <wangwy@ntu.edu.sg>. <haozhen001@e.ntu.edu.sg>, Correspondence IlliIllinois Chicago to: Haozhen Wenya Wang Preprint. February 3, 2026. 1 memory for retaining experience and maintaining coherence (Hu et al., 2025). This need has driven rapid progress in agent memory, including approaches that summarize and retrieve past interactions or manage external memory stores (Kang et al., 2025; Chhikara et al., 2025; Packer et al., 2023; Xu et al., 2025). However, most methods still rely on static, hand-designed memory mechanisms, including fixed operation primitives (e.g., add/update/delete/skip) (Wang et al., 2025a; Yan et al., 2025) and heuristic modules that govern what to store, how to revise it (Kang et al., 2025; Fang et al., 2025), and when to prune it. Such designs bake in strong human assumptions and often suffer under diverse interaction patterns, scaling poorly as histories grow. We argue that this formulation fundamentally limits the adaptability of agent memory. Rather than treating memory as the output of fixed operations or hand-designed modules, we propose to elevate memory extraction itself into learnable abstraction. Concretely, we view memory construction as the outcome of applying small set of generic, reusable memory skills: structured behaviors that specify when and how interaction traces should be transformed into memory and revised over time. This perspective reveals key bottleneck of prior pipelines: they hard-code memory behaviors into fixed procedural workflows that interleave heuristics with LLM-mediated extraction and revision, making them brittle under distribution shift (Fang et al., 2025). Under this view, an ideal agent memory system should satisfy three properties. (i) Minimal reliance on human priors. Instead of manually encoding what is worth remembering for domain (Zhong et al., 2024), memory behaviors should be shaped by interaction data and updated as task demands evolve. (ii) Support for larger extraction granularity. Many approaches are tuned to fixed unit, such as per-turn processing (Fang et al., 2025), and can weaken when applied to longer spans. practical system should be able to operate at larger extraction granularity when needed. (iii) Skill-conditioned, compositional memory construction. Existing systems often decompose memory construction into specialized modules (Kang et al., 2025). In contrast, we prefer to select and compose small set of relevant skills for the current context and apply them in one generation step, enabling flexible reuse and evolution of memory behaviors. Based on the above observations, we introduce MemSkill, MemSkill: Learning and Evolving Memory Skills for Self-Evolving Agents Figure 1. Comparison between (a) prior turn-level, handcrafted operations and (b) MemSkills span-level, skill-conditioned generation. Prior methods interleave handcrafted operations with LLM calls to incrementally extract and revise memory turn by turn, while MemSkill selects small set of skills from shared skill bank and applies them in one pass to produce skill-guided memories. which reframes memory operations as learnable and evolvable set of memory skills. MemSkill maintains shared skill bank, where each skill captures reusable way to extract, consolidate, or revise memories from interaction text (Figure 1 shows the structured template of memory skill). Given the current context, controller learns to select small set of relevant skills, and an LLM-based executor conditions on these skills to generate skill-guided memories in one pass. This skill-conditioned formulation is not tied to fixed extraction unit and can be applied to different span lengths when processing long interaction histories. Crucially, MemSkill goes beyond learning how to use fixed set of skills. We introduce closed-loop evolution process that alternates between learning to use the current skill bank and evolving the skill bank itself. Specifically, we train the controller with reinforcement learning (RL) using downstream task signals as feedback for skill selection. Periodically, designer aggregates the hardest cases produced during training, selects representative failures, and uses an LLM to refine existing skills and propose new ones. After each evolution step, the controller continues training on the evolved skill bank, with additional exploration to facilitate adopting newly introduced skills. Overall, this process gradually strengthens both the skill selection policy and the evolving skill bank, moving toward more adaptive memory management system driven by interaction data. Experiments on LoCoMo, LongMemEval, HotpotQA, and ALFWorld show that MemSkill consistently improves task performance and generalizes well. Further analyses validate key components and showcase representative evolved skills, offering insights toward more adaptive, self-evolving memory management for LLM agents. Our contributions can be summarized as follows. We propose MemSkill, an agent memory method that represents memory operations as an evolving skill bank, and constructs skill-guided memories by conditioning an LLM on selected set of skills. We introduce closed-loop optimization recipe that combines reinforcement learning for skill selection with LLM-guided skill evolution from hard cases, enabling continual refinement of the skill bank and taking step toward self-evolving agent memory systems. We evaluate MemSkill on LoCoMo, LongMemEval, HotpotQA, and ALFWorld, showing consistent gains over baselines and strong generalization, offering insights toward self-evolving memory for LLM agents. 2. Related Work 2.1. LLM Agent Memory Systems Prior work on agent memory focuses on constructing external memories from interaction histories and leveraging them to support downstream reasoning and decision making. Typical pipelines periodically extract salient information into memory store, retrieve relevant entries for new query, and update the store via consolidation or pruning (Kang et al., 2 MemSkill: Learning and Evolving Memory Skills for Self-Evolving Agents 2025; Zhong et al., 2024; Xu et al., 2025; Packer et al., 2023; Chhikara et al., 2025; Fang et al., 2025). More recently, learning-based approaches such as Memory-R1 (Yan et al., 2025) and Mem-α (Wang et al., 2025a) optimize memory management with reinforcement learning using downstream task signals. Despite this progress, memory management is still largely governed by static, hand-crafted routines for extraction, consolidation, and pruning. Several concurrent works also explore self-evolving memory in agent settings, but differ fundamentally from our focus. Evo-Memory provides streaming benchmark and evaluation framework for test-time memory evolution (Wei et al., 2025), while MemEvolve meta-optimizes memory architectures within predefined modular design space (Zhang et al., 2025). By contrast, we target the evolution of memory skills themselves, enabling the system to refine and grow its reusable memory operations over time. 2.2. Self-Evolving LLM Agents Recent work on self-evolving LLM agents studies how agents can improve from interaction experience with minimal manual supervision. ExpeL (Zhao et al., 2024) distills trajectories into editable natural-language insights and retrieves relevant experiences to guide future decisions, while EvolveR (Wu et al., 2025) formalizes an experience lifecycle that consolidates interactions into reusable principles and closes the loop with reinforcement learning updates. complementary line reduces reliance on curated data via self-play style curricula: Absolute Zero Reasoner (Zhao et al., 2025) trains proposer and solver with verifiable rewards from code executor, and Multi-Agent Evolve (Chen et al., 2025) extends this to proposer solver judge triad with LLM-based evaluation; R-Zero (Huang et al., 2025) follows similar challenger solver co-evolution pattern. Beyond curricula, systems such as AgentEvolver (Zhai et al., 2025) and RAGEN (Wang et al., 2025b) study efficient agent learning dynamics and stabilization in multi-turn RL settings, while ADAS (Hu et al., 2024) and AlphaEvolve (Novikov et al., 2025) explore automated discovery and evolutionary improvement of agent designs. Finally, SkillWeaver (Zheng et al., 2025) shows that agents can discover and refine reusable skills for web interaction. In contrast, our focus is on self-evolving memory skills that govern how agents construct and revise memories over time. 3. Method In this section, we first provide an overview of MemSkill (Section 3.1), then detail the skill bank (Section 3.2) and the three core components (controller (Section 3.3.1), executor (Section 3.3.2), and designer (Section 3.4)), and finally summarize the closed-loop optimization procedure that alternates between learning to use the current skills and evolving the skill bank from hard cases (Section 3.5). 3.1. Overview As shown in Figure 2, we propose MemSkill, which optimizes agent memory through two intertwined processes. The first process learns to use given skill bank: controller selects small set of skills conditioned on the context, and an executor applies them to produce memory updates. The second process improves the skill bank itself: designer periodically revises existing skills and introduces new ones based on challenging cases during training. To disentangle trace-specific memories from reusable memory management knowledge, MemSkill maintains two distinct stores. The memory bank is trace-specific and stores the memories constructed for each training trace (e.g., long dialogue). In contrast, the skill bank is shared across all traces and contains reusable memory skills. During training, the controller and executor interact with each trace to build its memory bank, while the designer updates the shared skill bank between phases. This alternating procedure gradually improves both the skill selection policy and the skill bank for memory construction. 3.2. Skill Bank As shown in Figure 2, memory skill specifies reusable memory operation as structured guidance, including when it is applicable and how it should be applied to the current context. Concretely, each skill contains (i) short description used for skill representation and selection, and (ii) detailed content specification that instructs the executor on how to perform memory extraction or revision. We start from minimal set of general-purpose primitives to ensure stable and functional initialization. Specifically, we initialize the skill bank with four basic skills corresponding to canonical memory operations: INSERT, UPDATE, DELETE, and SKIP. Starting from this minimal set, the designer progressively refines existing skills and expands the bank by proposing new skills that address uncovered failure modes. (Appendix details skill description) 3.3. Learning to Use Memory Skills In this part, we describe how MemSkill learns to use memory skills, covering (i) the skill-selection policy and (ii) skill-conditioned memory construction. 3.3.1. CONTROLLER: SKILL SELECTION POLICY To enable effective skill selection as the skill bank evolves, we introduce controller that selects small set of relevant memory skills for the current context. At each memory construction step, we update memory at the span level: we 3 MemSkill: Learning and Evolving Memory Skills for Self-Evolving Agents Figure 2. MemSkill architecture overview. Given an interaction trace, MemSkill processes it span by span: the controller selects Top-K subset of skills from shared skill bank conditioned on the current text span and retrieved memories, and an LLM executor applies the selected skills in one pass to update the trace-specific memory bank. The constructed memory is then evaluated on memory-dependent training queries to provide task reward for optimizing the controller, while query-centric failures are logged into sliding hard-case buffer. Periodically, the designer mines representative hard cases to refine existing skills and propose new ones, yielding alternating phases of skill usage and skill evolution. More skill case study can be found in Section 4.5 and Appendix B. split each interaction trace (e.g., dialogue) into contiguous text spans and process them sequentially; for each span, the controller conditions its selection on (i) the current text span and (ii) the retrieved existing memories, rather than operating turn by turn. To remain compatible with variable-size skill bank as it continuously evolves, the controller scores each skill by measuring the semantic distance between the current state representation and the skill representation, which naturally supports changing set of skills while staying sensitive to what is already stored in memory. State representation. Formally, let xt denote the current text span at step t, and let Mt = {mt,1, . . . , mt,R} be the retrieved memories from the current traces memory bank. The controller encodes (xt, Mt) into state embedding: Compatibility with an evolving skill bank. Instead of producing fixed-dimensional action head tied to fixed number of skills, the controller scores each skill by comparing state and skill embeddings: zt,i = t ui, pθ(i ht) = softmax(zt)i, (3) where zt RSt adapts automatically as the skill bank evolves. Top-K skill selection. Given the categorical distribution pθ(i ht) over the current skill bank St, the controller selects an ordered Top-K set of skills At = (at,1, . . . , at,K) without replacement (e.g., via Gumbel-Top-K (Kool et al., 2019)), and only passes the selected skills to the executor, keeping the skill context concise and relevant. ht = fctx(xt, Mt). (1) 3.3.2. EXECUTOR: SKILL-CONDITIONED MEMORY EXTRACTION Skill representation. For each skill si St in the current skill bank, we compute skill embedding from its description, as it provides focused semantic signal that is more stable than embedding the full skill content. ui = fskill(desc(si)). (2) Note that we use the same embedding model for fctx and fskill, mapping contexts and skill descriptions into shared representation space for scoring. Given the selected skills At, the executor (fixed) constructs memory updates by conditioning an LLM on (i) the current text span xt, (ii) the retrieved memory items Mt, and (iii) the selected skills At. This mirrors skill-conditioned inference in agent systems, where small set of relevant skills is provided to guide behavior for the current context. The executor then produces memory updates in structured format, which are parsed and applied to update the traces memory bank. By composing several skills for the same text span and extracting memory in one LLM call, MemSkill 4 MemSkill: Learning and Evolving Memory Skills for Self-Evolving Agents reduces repeated per-turn processing and makes memory construction easier to scale to long interaction histories. Appendix details the complete executor prompt. 3.3.3. CONTROLLER OPTIMIZATION the selected hard cases and identify what memory behaviors are missing or mis-specified. Second, it uses the resulting analysis to propose concrete edits to existing skills and to introduce new skills. We keep the designer description concise here and provide prompt details in Appendix C. We train the controller with reinforcement learning, using downstream task performance as feedback for its skill selections. For each training trace, the controller makes sequence of Top-K selections while the executor incrementally builds the trace-specific memory bank. After construction, we evaluate the resulting memory bank on the traces memory-dependent training queries and use the resulting task performance as the reward (e.g., F1 or success rate). Notably, we maintain snapshots of the best-performing skill bank and roll back if an update degrades performance, with early stopping when repeated designer updates fail to improve the training signal. After each evolution step, we also briefly increase exploration by biasing selection toward newly introduced skills, encouraging the controller to try them and facilitating efficient learning of their utility. More details about the designer can be found in Appendix A.2. key technical detail is that the controllers action is an ordered Top-K set selected without replacement, rather than single discrete action. We therefore compute the joint logprobability log πθ(At st) under the without-replacement selection process and use it in standard policy-gradient style objectives via importance weighting and clipping. Concretely, the joint probability can be written as πθ(At st) = (cid:89) j=1 pθ(at,j st) 1 (cid:80) ℓ<j pθ(at,ℓ st) , (4) which reduces to the usual single-action case when = 1. Appendix A.4 provides implementation details. 3.4. Skill Evolution through Designer Feedback Beyond learning to select from fixed set of skills, MemSkill evolves the skill bank using an LLM-based designer (fixed) that operates periodically during training. Hard-case buffer. During controller training, we maintain sliding-window buffer of challenging cases observed recently. Each case is query-centric, recording the query along with its ground-truth and metadata (e.g., retrieved memories and model prediction), as well as summary statistics such as task performance and the number of failures observed so far. The buffer uses two expiration rules: cases are removed if they become too old (exceeding maximum training step gap) or if the buffer reaches its capacity limit, which tracks recent failure patterns without growing unbounded. Selecting representative hard cases. To focus designer updates on impactful failures, we cluster cases (e.g., KMeans) into groups that naturally reflect different query or error types. Within each cluster, we prioritize representative cases using difficulty score that increases when task performance is low and when the same case fails repeatedly. This produces compact set of high-value cases for skill evolution while preserving diversity across error types. Two-stage skill evolution. The designer updates the skill bank in two stages. First, it employs an LLM to analyze 5 3.5. Closed-Loop Optimization MemSkill alternates between (i) learning to select and apply skills to build memory banks and (ii) evolving the skill bank based on hard cases mined from recent training steps. Each cycle begins with controller training on the current skill bank, during which the executor constructs memories and the system accumulates challenging cases. The designer then updates the skill bank using representative hard cases, optionally rolling back to prior snapshot if the update regresses. The next cycle resumes controller training on the updated skill bank, with additional exploration to encourage early use of new skills. Through repeated cycles, MemSkill progressively improves both skill usage and the skill bank available for memory construction. 4. Experiments 4.1. Experiment Setup Datasets and Baselines. We evaluate MemSkill on four benchmarks: LoCoMo (Maharana et al., 2024), LongMemEval (Wu et al., 2024), HotpotQA (Yang et al., 2018), and ALFWorld (Shridhar et al., 2020), where HotpotQA is used in Section 4.4 to study skill transfer under distribution shift. The remaining three benchmarks cover two representative settings. (i) Conversational Benchmarks include LoCoMo and LongMemEval, which evaluate memory construction from long, dialogue-style interaction histories. For these datasets, we report F1-score (F1) and an LLM-based judge score (L-J). (ii) Embodied Interactive Tasks are evaluated on ALFWorld with two standard subsets, ALF-Seen and ALF-Unseen, and we report success rate (SR) and the number of environment interaction steps (#Steps). Specific dataset splits are provided in Appendix A.1. We compare MemSkill against several strong baselines: (1) No-Memory, which answers directly without an external memory (or additional constructed context); (2) Chainof-Notes (CoN) (Yu et al., 2024); (3) ReadAgent (Lee et al., 2024); (4) MemoryBank (Zhong et al., 2024); (5) MemSkill: Learning and Evolving Memory Skills for Self-Evolving Agents Table 1. Main comparison results on LoCoMo, LongMemEval, and ALFWorld. Model Methods LoCoMo LongMemEval Avg. ALF-Seen ALF-Unseen Avg. Conversational Benchmarks Embodied Interactive Tasks L-J F1 No-Memory CoN ReadAgent MemoryBank A-MEM Mem0 LangMem MemoryOS MemSkill No-Memory CoN ReadAgent MemoryBank A-MEM Mem0 LangMem MemoryOS MemSkill - 17.97 26.34 33.54 35.60 10.18 25.97 38.68 38.78 - 27.97 25.41 25.39 34.83 11.11 24.04 38.55 39. - 24.80 35.17 40.92 46.34 33.01 29.14 44.59 50.96 - 35.35 33.57 39.76 48.41 30.10 27.07 44.59 52.07 - 30.28 23.52 30.26 25.86 29.94 15.79 14.19 31.65 - 28.34 23.52 7.36 12.46 26.88 16.37 13.26 23.75 L-J - 56.93 41.58 35.15 38.12 45.54 21.00 36.50 59. - 46.04 41.58 24.75 34.65 43.07 20.00 36.00 59.90 L-J SR #Steps SR #Steps SR - 40.87 38.38 38.04 42.23 39.28 25.07 40.55 55.19 - 40.70 37.58 32.26 41.53 36.59 23.54 40.30 55.99 17.14 40.71 32.86 25.00 24.29 32.86 37.86 15.71 47.86 18.57 57.86 53.57 37.86 25.00 38.57 37.14 19.29 60.00 43.74 33.44 37.09 39.96 40.51 36.47 34.39 43.74 30. 42.48 25.81 27.88 35.15 40.28 33.64 34.42 42.43 24.54 20.15 30.60 38.06 32.84 28.36 32.09 35.07 14.18 47.01 26.12 53.73 54.48 38.06 29.10 41.04 31.34 18.66 64.18 42.99 37.66 34.78 36.54 38.83 37.32 35.70 44.54 30.43 39.35 28.40 27.41 34.99 39.04 33.16 37.17 42.95 23.57 18.65 35.66 35.46 28.92 26.33 32.48 36.47 14.95 47. 22.35 55.80 54.03 37.96 27.10 39.81 34.24 18.98 62.09 . 3 3 L u n - 0 7 N - 3 Q r I - 3 0 - Bold indicates the best score within each base model block. indicates no training using this base model or dataset (transfer evaluation only). A-MEM (Xu et al., 2025); (6) Mem0 (Chhikara et al., 2025); (7) LangMem (LangChain, 2025); and (8) MemoryOS (Kang et al., 2025). Overall, this setup spans diverse benchmarks and baselines, enabling broad and consistent comparison across diverse settings. Implementation Details. We initialize the controller as lightweight multilayer perceptron (MLP), and use LLaMA3.3-70B-Instruct (Grattafiori et al., 2024) and Qwen3-Next80B-A3B-Instruct (Yang et al., 2025) as the base LLMs, accessed through an API service. Unless otherwise specified, we train MemSkill on LLaMA and use Qwen only for transfer experiments. LongMemEval is also evaluated in transfer setting, where we directly apply the skills learned on LoCoMo without further training. For both MemSkill and all baselines, we retrieve up to 20 memory items for consistent comparison. During training, we initialize the controller optimization with PPO (Schulman et al., 2017). MemSkill performs memory construction at the span level. On conversational benchmarks, we treat each dialogue session as the basic processing unit during training, and the controller selects small set of skills per unit with K=3. We use Qwen3-Embedding-0.6B (Yang et al., 2025) as the shared encoder for state and skill representations, and adopt Contriever (Izacard et al., 2021) as the default memory retriever. For the designer, we trigger skill evolution every 100 training steps and allow at most 3 skill edits per evolution round. For ALFWorld, we cap the maximum environment interaction length to 50 steps. At evaluation time, we keep the same span-level formulation and set the span/chunk size to 512 by default, while keeping the overall procedure unchanged. Unless otherwise specified, we use K=7 skills for LoCoMo and LongMemEval at evaluation time, and K=5 for ALFWorld. Additional implementation details and prompt templates are provided in Appendix and Appendix C. 4.2. Comparison Experiments Effectiveness across conversational and embodied settings. Table 1 summarizes the main comparison results on LoCoMo, LongMemEval, and ALFWorld. Across these datasets, MemSkill achieves the strongest overall performance among all compared methods. On conversational benchmarks, MemSkill attains the best LLM-judge scores on both LoCoMo and LongMemEval within each basemodel block, indicating higher-quality constructed memories. In comparison, prior methods such as MemoryBank, A-MEM, and MemoryOS use fixed, manually specified memory procedures for extraction and revision, whereas 6 MemSkill: Learning and Evolving Memory Skills for Self-Evolving Agents Figure 3. Skill generalization under distribution shift on HotpotQA. We transfer the LoCoMo-trained skill bank to HotpotQA and evaluate three context-length settings (50/100/200 concatenated documents) following (Yu et al., 2025). Bars show LLM-judge (L-J) under LLaMA with different Top-K skill counts, compared to MemoryOS and A-MEM. MemSkill learns and evolves its skills from interaction, enabling better adaptation across contexts. On ALFWorld, MemSkill achieves the highest success rates on both seen and unseen splits, indicating that skill-guided memory construction can benefit interactive decision making, whereas other baselines are less reliable at leveraging memory to support long-horizon action execution. Overall, the results show that MemSkill is effective across diverse settings. Generalization across base models. key advantage of MemSkill is strong generalization across base models. We train MemSkill only with LLaMA and directly transfer the learned skills to Qwen without retraining. Despite this strict transfer setting, MemSkill remains highly competitive and continues to outperform strong baselines on both conversational and embodied evaluations, demonstrating that the evolved skills capture reusable memory behaviors that can be instantiated by different underlying LLMs. Cross-dataset transfer. MemSkill also generalizes across datasets within the same broad setting. In particular, LongMemEval is evaluated purely by transferring the skill bank learned on LoCoMo, yet MemSkill achieves the best results among all methods, suggesting that the learned skills are not overfit to single benchmark. We further study transfer under more pronounced distribution shifts in Section 4.4. 4.3. Ablation Study We perform ablations to disentangle the contributions of (i) learning to select skills and (ii) evolving the skill bank. Table 2 reports LLM Judge (L-J) results on LoCoMo under both base models (LLaMA and Qwen). As shown, w/o controller (random skills) replaces the learned controller with random skill selection while keeping the rest of the pipeline unchanged. w/o designer (static skills) disables the designer and fixes the skill bank to the four initial primitives. Refine-only (no new skills) allows the designer to refine existing skills but prohibits adding new ones. Across both base models, removing either component consistently degrades performance, confirming that MemSkill benefits from both targeted skill selection and skill evolution. In particular, random skill selection leads to clear drop from the default setting, highlighting the importance of learning to choose relevant skills rather than providing arbitrary ones. Disabling the designer yields an even larger degradation, especially under Qwen, suggesting that evolving the skill bank is important for learning reusable memory behaviors that generalize beyond fixed, manually specified operation set. Finally, refinement-only consistently outperforms static skills on both LLaMA and Qwen, with particularly large gain under Qwen, yet remains below the default setting, indicating that introducing new skills yields additional benefits beyond refining the initial primitives. Table 2. Ablation study on LoCoMo using L-J metric. Variant MemSkill (default) w/o controller (random skills) w/o designer (static skills) Refine-only (no new skills) LLaMA Qwen 50.96 45.86 44.11 44.90 52.07 41.24 34.71 46.97 4.4. Skill Generalization Under Distribution Shift Beyond transfer within dialogue-style memory benchmarks, we evaluate whether learned skills generalize under distribution shift in interaction format and evidence structure. Concretely, we directly apply the skill bank trained on LoCoMo to HotpotQA, where inputs are long-form, documentstyle narratives rather than multi-turn dialogues. Following the evaluation protocol in (Yu et al., 2025), we test three context-length settings with increasing difficulty, corresponding to different numbers of concatenated documents (i.e., 50/100/200). All results in this section use LLaMA as the base model and report the LLM-judge score (L-J). For baselines, we include MemoryOS and A-MEM, which are the most competitive methods on conversational benchmarks in Table 1, and omit weaker alternatives for clarity. 7 MemSkill: Learning and Evolving Memory Skills for Self-Evolving Agents Figure 4. Case study. We show representative evolved skills learned on LoCoMo and ALFWorld. (Description is omitted for brevity.) Figure 3 shows that MemSkill transfers strongly to HotpotQA across all three context sizes. In particular, MemSkill consistently outperforms strong baselines such as MemoryOS and A-MEM, with the gains becoming more pronounced in the more challenging long-context setting. These results suggest that the learned memory skills are not tied to dialogue-specific surface forms, but capture reusable extraction and revision behaviors that remain effective when the input structure and retrieval demands change. The same plots also reveal mild sensitivity to the number of selected skills K. Increasing generally improves performance, with K=7 achieving the best results across all three settings, while smaller can under-utilize the skill bank under longer contexts. Overall, the trend indicates that MemSkill benefits from composing multiple skills when the context becomes longer and noisier, while still maintaining strong transfer without any HotpotQA-specific training. 4.5. Case Study To make MemSkill more interpretable, we inspect the final evolved skill bank and report representative skills learned from LoCoMo and ALFWorld. As shown in Figure 4, the learned skills exhibit clear domain specialization across LoCoMo and ALFWorld. For LoCoMo, the skills in Figure 4 emphasize temporal context and activity details, suggesting that effective dialogue memory often benefits from organizing events with lightweight structure, such as who did what, where, and when, across long interactions. More broadly, the evolved skill bank reflects recurring information needs surfaced by the data, rather than single fixed notion of what should be remembered. In contrast, the ALFWorld skills focus on action constraints and object locations, highlighting that embodied success depends on maintaining an actionable world state summary, including task-relevant preconditions rather than broad narrative summaries, to support multi-step execution. Taken together, these skills illustrate how MemSkill can automatically distill reusable memory behaviors from interaction data and continually refine them through training, moving toward more adaptive memory system with reduced reliance on hand-crafted memory designs. Additional evolved skills are provided in Appendix B. 5. Conclusion We present MemSkill, an agent memory method that reframes memory operations as an evolving skill bank. MemSkill learns to select small set of relevant skills for each context span and conditions an LLM executor on these skills to construct memories in skill-guided manner. Beyond learning how to use fixed operation set, MemSkill introduces designer that improves the skill bank itself by refining existing skills and proposing new ones from challenging cases, forming closed-loop training procedure. Experiments on LoCoMo, LongMemEval, HotpotQA, and ALFWorld demonstrate consistent improvements over strong baselines, and qualitative analyses illustrate how evolving skills can yield more adaptive memory management behaviors. We hope MemSkill encourages future work on self-improving agent memory systems that learn not only to use memory, but also to continually improve how memory is constructed and maintained. 8 MemSkill: Learning and Evolving Memory Skills for Self-Evolving Agents"
        },
        {
            "title": "Acknowledgements",
            "content": "This research/project is supported by the NTU Start-Up Grant (#023284-00001), Singapore, and the MOE AcRF Tier 1 Seed Grant (RS37/24, #025041-00001), Singapore."
        },
        {
            "title": "Impact Statement",
            "content": "MemSkill advances the design of agent memory by shifting emphasis from static, hand-crafted procedures to learnable and evolvable memory skills. This perspective can make long-running LLM agents more practical in settings where interaction histories grow and the information that matters changes over time. By improving how memories are extracted, consolidated, and revised, MemSkill can support more consistent assistance in applications such as multisession personal assistants, educational tutors, long-form customer support, and interactive research tools, where agents must preserve relevant context while avoiding redundant or stale information. Beyond immediate applications, MemSkill also offers reusable methodology for studying how memory behaviors should be specified and improved. The explicit skill bank provides concrete interface for inspection and analysis, which may encourage more interpretable and controllable memory systems. More broadly, the idea of iteratively improving memory management behaviors from hard cases can inspire similar self-improvement mechanisms in other agent subsystems, such as tool use or planning, where fixed heuristics remain common. As with any memory-enabled agent, responsible use benefits from basic safeguards. For example, deployments should avoid storing unnecessary sensitive information and should provide user-facing controls for memory inspection and removal. These considerations are standard for memoryaugmented systems and are not unique to MemSkill, but they become increasingly important as agent memory becomes more effective and widely adopted."
        },
        {
            "title": "References",
            "content": "Chen, Y., Wang, Y., Zhu, S., Yu, H., Feng, T., Zhang, M., Patwary, M., and You, J. Multi-agent evolve: Llm self-improve through co-evolution. arXiv preprint arXiv:2510.23595, 2025. Chhikara, P., Khant, D., Aryan, S., Singh, T., and Yadav, D. Mem0: Building production-ready ai agents with scalable long-term memory. arXiv preprint arXiv:2504.19413, 2025. Fang, J., Deng, X., Xu, H., Jiang, Z., Tang, Y., Xu, Z., Deng, S., Yao, Y., Wang, M., Qiao, S., et al. Lightmem: Lightweight and efficient memory-augmented generation. arXiv preprint arXiv:2510.18866, 2025. Grattafiori, A., Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur, A., Schelten, A., Vaughan, A., et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Hu, S., Lu, C., and Clune, J. Automated design of agentic systems. arXiv preprint arXiv:2408.08435, 2024. Hu, Y., Liu, S., Yue, Y., Zhang, G., Liu, B., Zhu, F., Lin, J., Guo, H., Dou, S., Xi, Z., et al. Memory in the age of ai agents. arXiv preprint arXiv:2512.13564, 2025. Huang, C., Yu, W., Wang, X., Zhang, H., Li, Z., Li, R., Huang, J., Mi, H., and Yu, D. R-zero: Selfevolving reasoning llm from zero data. arXiv preprint arXiv:2508.05004, 2025. Izacard, G., Caron, M., Hosseini, L., Riedel, S., Bojanowski, P., Joulin, A., and Grave, E. Unsupervised dense information retrieval with contrastive learning. arXiv preprint arXiv:2112.09118, 2021. Kang, J., Ji, M., Zhao, Z., and Bai, T. Memory os of ai agent. arXiv preprint arXiv:2506.06326, 2025. Kool, W., Van Hoof, H., and Welling, M. Stochastic beams and where to find them: The gumbel-top-k trick for sampling sequences without replacement. In International conference on machine learning, pp. 34993508. PMLR, 2019. LangChain. https://github.com/ langchain-ai/langmem, 2025. GitHub repository. Langmem. Lee, K.-H., Chen, X., Furuta, H., Canny, J., and Fischer, I. human-inspired reading agent with gist memory of very long contexts. arXiv preprint arXiv:2402.09727, 2024. Maharana, A., Lee, D.-H., Tulyakov, S., Bansal, M., Barbieri, F., and Fang, Y. Evaluating very long-term conversational memory of llm agents. arXiv preprint arXiv:2402.17753, 2024. Novikov, A., Vu, N., Eisenberger, M., Dupont, E., Huang, P.-S., Wagner, A. Z., Shirobokov, S., Kozlovskii, B., Ruiz, F. J., Mehrabian, A., et al. Alphaevolve: coding agent for scientific and algorithmic discovery. arXiv preprint arXiv:2506.13131, 2025. Packer, C., Fang, V., Patil, S., Lin, K., Wooders, S., and Gonzalez, J. Memgpt: Towards llms as operating systems. 2023. Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. 9 MemSkill: Learning and Evolving Memory Skills for Self-Evolving Agents Yu, W., Zhang, H., Pan, X., Cao, P., Ma, K., Li, J., Wang, H., and Yu, D. Chain-of-note: Enhancing robustness in retrieval-augmented language models. In Proceedings of the 2024 conference on empirical methods in natural language processing, pp. 1467214685, 2024. Zhai, Y., Tao, S., Chen, C., Zou, A., Chen, Z., Fu, Q., Mai, S., Yu, L., Deng, J., Cao, Z., et al. Agentevolver: Towards efficient self-evolving agent system. arXiv preprint arXiv:2511.10395, 2025. Zhang, G., Ren, H., Zhan, C., Zhou, Z., Wang, J., Zhu, H., Zhou, W., and Yan, S. Memevolve: Meta-evolution of agent memory systems. arXiv preprint arXiv:2512.18746, 2025. Zhao, A., Huang, D., Xu, Q., Lin, M., Liu, Y.-J., and Huang, G. Expel: Llm agents are experiential learners. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pp. 1963219642, 2024. Zhao, A., Wu, Y., Yue, Y., Wu, T., Xu, Q., Lin, M., Wang, S., Wu, Q., Zheng, Z., and Huang, G. Absolute zero: Reinforced self-play reasoning with zero data. arXiv preprint arXiv:2505.03335, 2025. Zheng, B., Fatemi, M. Y., Jin, X., Wang, Z. Z., Gandhi, A., Song, Y., Gu, Y., Srinivasa, J., Liu, G., Neubig, G., et al. Skillweaver: Web agents can self-improve by discovering and honing skills. arXiv preprint arXiv:2504.07079, 2025. Zhong, W., Guo, L., Gao, Q., Ye, H., and Wang, Y. Memorybank: Enhancing large language models with long-term memory. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pp. 1972419731, 2024. Shridhar, M., Yuan, X., Cˆote, M.-A., Bisk, Y., Trischler, A., and Hausknecht, M. Alfworld: Aligning text and embodied environments for interactive learning. arXiv preprint arXiv:2010.03768, 2020. Wang, Y., Takanobu, R., Liang, Z., Mao, Y., Hu, Y., McAuley, J., and Wu, X. Mem-{alpha}: Learning memory construction via reinforcement learning. arXiv preprint arXiv:2509.25911, 2025a. Wang, Z., Wang, K., Wang, Q., Zhang, P., Li, L., Yang, Z., Jin, X., Yu, K., Nguyen, M. N., Liu, L., et al. Ragen: Understanding self-evolution in llm agents via multi-turn reinforcement learning. arXiv preprint arXiv:2504.20073, 2025b. Wei, T., Sachdeva, N., Coleman, B., He, Z., Bei, Y., Ning, X., Ai, M., Li, Y., He, J., Chi, E. H., et al. Evo-memory: Benchmarking llm agent test-time learning with selfevolving memory. arXiv preprint arXiv:2511.20857, 2025. Wu, D., Wang, H., Yu, W., Zhang, Y., Chang, K.-W., and Yu, D. Longmemeval: Benchmarking chat assistants on long-term interactive memory. arXiv preprint arXiv:2410.10813, 2024. Wu, R., Wang, X., Mei, J., Cai, P., Fu, D., Yang, C., Wen, L., Yang, X., Shen, Y., Wang, Y., et al. Evolver: Self-evolving llm agents through an experience-driven lifecycle. arXiv preprint arXiv:2510.16079, 2025. Xu, W., Liang, Z., Mei, K., Gao, H., Tan, J., and Zhang, Y. A-mem: Agentic memory for llm agents. arXiv preprint arXiv:2502.12110, 2025. Yan, S., Yang, X., Huang, Z., Nie, E., Ding, Z., Li, Z., Ma, X., Kersting, K., Pan, J. Z., Schutze, H., et al. Memoryr1: Enhancing large language model agents to manage and utilize memories via reinforcement learning. arXiv preprint arXiv:2508.19828, 2025. Yang, A., Li, A., Yang, B., Zhang, B., Hui, B., Zheng, B., Yu, B., Gao, C., Huang, C., Lv, C., et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. Yang, Z., Qi, P., Zhang, S., Bengio, Y., Cohen, W., Salakhutdinov, R., and Manning, C. D. Hotpotqa: dataset for diverse, explainable multi-hop question answering. In Proceedings of the 2018 conference on empirical methods in natural language processing, pp. 23692380, 2018. Yu, H., Chen, T., Feng, J., Chen, J., Dai, W., Yu, Q., Zhang, Y.-Q., Ma, W.-Y., Liu, J., Wang, M., et al. Memagent: Reshaping long-context llm with multi-conv rl-based memory agent. arXiv preprint arXiv:2507.02259, 2025. MemSkill: Learning and Evolving Memory Skills for Self-Evolving Agents A. More Implementation Details A.1. Evaluation Details LLM judge and infrastructure. We use openai/gpt-oss-120b as the LLM judge. All API-based models are accessed via NV NIM API and Together API. Training is conducted on NVIDIA A6000 GPUs. LoCoMo (Maharana et al., 2024). LoCoMo contains 10 long interaction samples, each paired with roughly 200 training queries on average. We split the dataset by sample into train/val/test with 6/2/2 ratio. We additionally remove adversarial queries, since their evidence is not present in the provided context and can introduce noisy supervision during training. LongMemEval (Wu et al., 2024). We use the LongMemEval-S split, where each example contains an ultra-long conversation of roughly 100K tokens. We then perform transfer evaluation on stratified sample of about one-fifth of the dataset (approximately 100 samples), ensuring coverage of different question types for comprehensive assessment. ALFWorld (Shridhar et al., 2020). We first collect expert trajectories from the training split and treat them as the corpus for memory or experience construction. We then evaluate on the official ALF-Seen and ALF-Unseen splits. HotpotQA (Yang et al., 2018). We use HotpotQA to study transfer under distribution shift following the evaluation protocol of (Yu et al., 2025). Concretely, we evaluate on three context-length settings with increasing difficulty, corresponding to 50/100/200 concatenated documents (denoted as eval 50, eval 100, and eval 200). Unless otherwise specified, all results in this part use LLaMA as the base model and report the LLM-judge score (L-J). Span-level evaluation. During evaluation, we perform memory construction at the span level with default span size of 512 tokens, rather than updating memory turn by turn. This substantially reduces the number of LLM calls and improves evaluation efficiency. A.2. More Details of the Designer Hard-case buffer and representative case mining. The designer maintains sliding hard-case buffer that tracks recently challenging evaluation cases without growing unbounded. Each case stores the query, the retrieved memories used to answer it, the model prediction, the reference answer, the resulting task reward (e.g., F1), and failure counter that records how many times the case has been answered incorrectly. To prioritize cases that are both low-reward and repeatedly failed, we assign each case difficulty score d(q) = (cid:0)1 r(q)(cid:1) c(q), where r(q) [0, 1] is the task reward for query and c(q) is its cumulative failure count within the buffer window. Higher d(q) indicates more critical cases that should be examined first. (5) To encourage coverage over diverse failure types, we further cluster hard cases by semantic similarity of their queries and mine representative cases from each cluster. For example, in LoCoMo, some queries focus on temporal cues (e.g., when an event happened) while others emphasize locations (e.g., where something occurred). Clustering helps separate these semantic types so the designer feedback is not dominated by single frequent error mode, improving diversity and completeness of the mined supervision. Exploration incentive for newly introduced skills. After each evolution round, the designer may introduce new skills that the controller has not yet learned to utilize. To facilitate adoption, we apply short post-update exploration phase by biasing the controller toward new skills directly at the logit level. Let Snew denote the set of newly added skills, and let pθ(i st) = softmax(zt)i be the controller distribution at step t. We enforce that the total probability mass assigned to new skills is at least target threshold τt: (cid:88) pθ(i st) τt, τt [0, 1]. iSnew When the constraint in Eq. (6) is violated, we add uniform logit gain δt to all new skills, (cid:40) t,i = zt,i + δt, zt,i, Snew, otherwise, θ( st) = softmax(z t), (6) (7) where δt is chosen as the minimal value that makes (cid:80) preserves the controller architecture and yields smooth, probability-level encouragement toward new skills. θ(i st) τt. By operating on logits, this mechanism iSnew 11 MemSkill: Learning and Evolving Memory Skills for Self-Evolving Agents We apply this incentive for the first Texplore=50 training steps after each evolution round. To avoid persistent bias, the target threshold decays linearly within this window: τt = τ0 (cid:16) 1 Texplore (cid:17) , = 0, 1, . . . , Texplore, (8) with default τ0=0.3. This schedule provides strong initial exploration and then gradually fades, yielding smooth transition back to the controllers learned selection behavior. Early stopping and rollback based on stabilized rewards. MemSkill performs skill evolution periodically, where each evolution cycle consists of fixed number of controller-training steps (e.g., 100 steps) on the current skill bank. Because the reward signal can be volatile immediately after skill-bank update, we assess whether cycle improves performance using stabilized reward estimate: we compute the average task reward over the last quarter of training steps within the cycle, and treat this value as the cycles score. Let denote the number of controller-training steps per cycle and {rt}L define the cycle score as t=1 the step-level rewards within the cycle. We rtail = 1 L/4 (cid:88) rt. t=3L/4+1 (9) We compare rtail against the best score observed so far. If the current cycle does not improve this criterion, then before performing the next skill evolution step, we roll back the skill bank to the previously best-performing snapshot and restart evolution from that snapshot. This rollback prevents compounding degradations from suboptimal designer updates. Finally, if the stabilized reward fails to improve for several consecutive evolution cycles (we use fixed patience), we early stop training and return the best skill bank snapshot encountered during training. A.3. Details on ALFWorld Training ALFWorld differs from the other benchmarks in that it is an interactive environment rather than static text corpus. To instantiate MemSkill in this setting, we first convert ALFWorld into an offline training protocol by collecting expert trajectories on the training split. Each trajectory records the agents interaction sequence (observations, actions, and outcomes) and serves as an interaction trace for memory construction. Task-type grouping. ALFWorld tasks naturally fall into small number of recurring goal templates. Following common practice, we group trajectories by task type (i.e., goal template), such as PICK & PLACE (put an object into/on target receptacle), CLEAN & PLACE (clean an object and then place it), HEAT & PLACE (heat an object and then place it), and COOL & PLACE (cool an object and then place it).1 Experience corpus vs. evaluation cases. To fit ALFWorld into our training framework, we construct per-type train-time data splits from the offline expert trajectories. For each task type, we randomly sample subset of trajectories as the experience corpus used for memory construction, and sample another non-overlapping subset of trajectories from the same type as evaluation cases. During training, MemSkill builds trajectory-specific memory bank from the experience corpus (span by span, via controller and executor), and then evaluates the constructed memory on the evaluation cases to obtain task reward and to log failure cases. Motivation. Using non-overlapping trajectories from the same task type for experience construction and evaluation provides controlled generalization signal: trajectories within type share goal structure and recurrent interaction patterns, making memories and skills more transferable across different instances of the same template. This setup encourages MemSkill to learn reusable memory skills that capture type-level regularities (e.g., relevant object states and action prerequisites) rather than overfitting to single trajectory, while still ensuring that evaluation traces are held out from the traces used to build memory. 1We use the task template provided by the environment to define task types. 12 MemSkill: Learning and Evolving Memory Skills for Self-Evolving Agents A.4. Details on Training Objectives This part details the reinforcement learning objective used to optimize the controller in MemSkill when each decision selects an ordered Top-K set of skills without replacement. Episode, states, and Top-K actions. Training iterates over interaction traces (episodes). For trace, MemSkill processes spans sequentially. At step t, the controller observes state st (xt, Mt) consisting of the current text span xt and retrieved memories Mt from the trace-specific memory bank. Let St = {1, . . . , Nt} denote the current skill bank, whose size Nt may change as the designer evolves skills. The controller outputs logits zθ(st) RNt and induces categorical distribution pθ(i st) = softmax(zθ(st))i. (10) Instead of sampling single skill, the controller selects an ordered Top-K set At = (at,1, . . . , at,K) without replacement, implemented via Gumbel-Top-K sampling (Kool et al., 2019) (i.e., adding i.i.d. Gumbel noise to logits and taking the top-K indices). Joint probability of Top-K without-replacement selection. For PPO-style policy optimization, we need the joint probability of sampling the ordered set At under the without-replacement process. This probability can be written as πθ(At st) = (cid:89) j=1 with the corresponding joint log-probability pθ(at,j st) 1 (cid:80) ℓ<j pθ(at,ℓ st) , log πθ(At st) = (cid:88) (cid:16) j=1 log pθ(at,j st) log (cid:0)1 (cid:88) pθ(at,ℓ st)(cid:1)(cid:17) . ℓ<j When = 1, Eq. 11 reduces to the standard single-action case. (11) (12) Rewards from memory-dependent evaluation. For each trace, after processing all spans and constructing the tracespecific memory bank, we evaluate the memory bank on the traces memory-dependent training queries and obtain scalar task score (e.g., F1 or success rate). We treat this score as the episode-level reward: Eval(memory bank; training queries) R. (13) This reward is then assigned to the sequence of controller decisions within the trace. Concretely, we use standard return computation with discount factor γ: Gt = (cid:88) τ =t γτ trτ , (14) where rτ is the per-step reward. In our default setting, reward is provided only after memory construction completes, i.e., rT = and rτ = 0 for τ < , so Gt = γT tR. We learn value function Vϕ(st) and compute advantages ˆAt using generalized advantage estimation (GAE). PPO objective with Top-K actions. We optimize the controller using proximal policy optimization (PPO) (Schulman et al., 2017), replacing the standard single-action log-probability with the Top-K joint log-probability in Eq. 12. Let θold denote the parameters of the behavior policy used to collect rollouts. Define the importance ratio rt(θ) = πθ(At st) πθold(At st) (cid:16) = exp log πθ(At st) log πθold(At st) (cid:17) . The clipped surrogate policy objective is Lpolicy(θ) = Et (cid:104) min (cid:0)rt(θ) ˆAt, clip(rt(θ), 1 ϵ, 1 + ϵ) ˆAt (cid:1)(cid:105) . 13 (15) (16) MemSkill: Learning and Evolving Memory Skills for Self-Evolving Agents We additionally optimize value function and include an entropy bonus for exploration: Lvalue(ϕ) = Et H(θ) = Et (cid:1)2(cid:105) (cid:104)(cid:0)Vϕ(st) Gt (cid:2)H(pθ( st))(cid:3), , where H() is the entropy of the categorical distribution over all skills. The overall objective (to maximize) is max θ,ϕ Lpolicy(θ) cv Lvalue(ϕ) + cH H(θ). In implementation, we minimize the negative of Eq. 19. (17) (18) (19) Gumbel-Top-K exploration. To sample Top-K skills without replacement during rollout collection, we use Gumbel-TopK sampling: at each step we draw i.i.d. Gumbel noise {gi}Nt i=1, form perturbed logits zi = zi + gi, and take the indices of the largest zi to obtain At. This provides stochastic exploration over skill subsets while remaining compatible with PPO through the joint probability in Eq. 11. For training stability, entropy regularization is computed from the base categorical distribution pθ( st) over all skills (Eq. 18), which encourages exploration of the evolving skill bank even though the executed action is Top-K set. B. Case Study B.1. Initial Primitive Skills Initial Primitive Skill - INSERT Skill: Insert New Memory Description: Memory management skill for capturing new, durable facts from the current text chunk that are not already in memory. Purpose: Capture new, durable facts from the current text chunk that are missing in memory. When to use: - The text chunk introduces new facts, events, plans, or context worth storing. - The information is stable and likely useful later. How to apply: - Compare against retrieved memories to avoid duplicates. - Split distinct facts into separate items. - Keep each item concise and specific. Constraints: - Skip trivial, fleeting, or speculative content. - Do not update or delete existing memories. Action type: INSERT only. 14 MemSkill: Learning and Evolving Memory Skills for Self-Evolving Agents Initial Primitive Skill - UPDATE Skill: Update Existing Memory Description: Memory management skill for revising an existing memory item when the text chunk provides corrections or new details. Purpose: Revise retrieved memory with new or corrected information from the text chunk. When to use: - The text chunk clarifies, corrects, or extends retrieved memory. How to apply: - Select the best matching memory item. - Merge new details into single updated item. - Preserve accurate details that still hold. Constraints: - Do not create new memories. - Do not delete items. Action type: UPDATE only. Initial Primitive Skill - DELETE Skill: Delete Invalid Memory Description: Memory management skill for removing memory items that are incorrect, outdated, or superseded. Purpose: Remove retrieved memory that is wrong, outdated, or superseded by the text chunk. When to use: - The text chunk clearly contradicts memory. - plan or fact is explicitly canceled or replaced. How to apply: - Only delete when evidence is explicit. Constraints: - If uncertain, prefer no action over deletion. Action type: DELETE only. 15 MemSkill: Learning and Evolving Memory Skills for Self-Evolving Agents Initial Primitive Skill - SKIP Skill: No Operation Description: Memory management skill for confirming that no memory changes are required. Purpose: Confirm no memory changes are needed for the text chunk. When to use: - The text chunk contains no new, corrective, or actionable information. Constraints: - Emit NOOP only if none of the selected skills produce actions. Action type: NOOP only. B.2. Evolved Skills on LoCoMo Evolved Skill on LoCoMo - CAPTURE ACTIVITY DETAILS Skill: Capture Activity Details Purpose: Capture detailed information about activities mentioned in the text chunk, including the type of activity, location, participants, temporal details, and any relevant contextual information. When to use: - The text chunk mentions specific activity or event with contextual details. How to apply: - Identify the key elements of the activity (e.g., type, location, participants, temporal details). - Capture any relevant contextual information that provides additional insight into the activity. - Keep the activity details specific, actionable, and concise. Constraints: - Focus on explicit activity details and contextual information mentioned in the text chunk. - Avoid inferring activity details or context not directly stated. Action type: INSERT only. MemSkill: Learning and Evolving Memory Skills for Self-Evolving Agents Evolved Skill on LoCoMo - CAPTURE ENTITY NUANCES Skill: Capture Entity Nuances Purpose: Capture nuanced details about entities mentioned in the text chunk, such as nicknames, aliases, or comparative statements. When to use: - The text chunk mentions an entity with nuanced details (e.g., nickname, alias, comparison). How to apply: - Identify the entity and its associated nuances. - Capture these nuances in way that distinguishes them from the entitys primary information. Constraints: - Focus on explicit nuances mentioned in the text chunk. - Avoid inferring nuances not directly stated. Action type: INSERT only. Evolved Skill on LoCoMo - CAPTURE TEMPORAL CONTEXT Skill: Capture Temporal Context Purpose: Capture the temporal context of events, activities, or facts mentioned in the text chunk, including any relevant dates, times, durations, or sequential information. When to use: - The text chunk mentions specific event, activity, or fact with associated temporal information. How to apply: - Identify the key temporal elements (e.g., start time, end time, duration, sequence). - Capture the temporal context in concise and specific format, considering any sequential relationships. Constraints: - Focus on explicit temporal information mentioned in the text chunk. - Avoid inferring temporal details not directly stated. Action type: INSERT only. 17 MemSkill: Learning and Evolving Memory Skills for Self-Evolving Agents Evolved Skill on LoCoMo - DELETE Skill: Delete Invalid Memory Purpose: Remove retrieved memory that is wrong, outdated, or superseded by the text chunk. When to use: - The text chunk clearly contradicts memory. - plan or fact is explicitly canceled or replaced. How to apply: - Only delete when evidence is explicit. Constraints: - If uncertain, prefer no action over deletion. Action type: DELETE only. Evolved Skill on LoCoMo - HANDLE ENTITY RELATIONSHIPS Skill: Handle Entity Relationships Purpose: Capture and manage complex relationships between entities mentioned in the text chunk, cluding nuanced details. inWhen to use: - The text chunk mentions interactions, associations, or relationships between entities with specific details. How to apply: - Identify the entities involved and their roles in the relationship. - Capture the nature of the relationship and any nuanced details (e.g., nicknames, comparative statements). - Update existing memories to reflect the new relationship information. Constraints: - Focus on explicit relationships mentioned in the text chunk. - Avoid inferring relationships not directly stated. Action type: INSERT only. 18 MemSkill: Learning and Evolving Memory Skills for Self-Evolving Agents Evolved Skill on LoCoMo - INSERT Skill: Insert New Memory Purpose: Capture new, durable facts from the current text chunk that are missing in memory, including specific temporal details such as dates or time frames and detailed activity information. When to use: - The text chunk introduces new facts, events, plans, or context worth storing. - The information is stable and likely useful later. How to apply: - Compare against retrieved memories to avoid duplicates. - Split distinct facts into separate items. - Keep each item concise and specific, including relevant temporal information and activity details. Constraints: - Skip trivial, fleeting, or speculative content. - Do not update or delete existing memories. Action type: INSERT only. Evolved Skill on LoCoMo - NOOP Skill: No Operation Purpose: Confirm no memory changes are needed for the text chunk. When to use: - The text chunk contains no new, corrective, or actionable information. Constraints: - Emit NOOP only if none of the selected skills produce actions. Action type: NOOP only. MemSkill: Learning and Evolving Memory Skills for Self-Evolving Agents Evolved Skill on LoCoMo - REFINE TEMPORAL DETAILS WITH CONTEXT Skill: Refine Temporal Details with Context Purpose: Update the temporal context of existing memories with new information from the text chunk, considering the context in which the information is provided. When to use: - The text chunk provides new or corrected temporal information relevant to an existing memory, and the context suggests need for refinement. How to apply: - Identify the relevant existing memory and its current temporal context. - Update the temporal details to reflect the new information, ensuring consistency with the provided context. Constraints: - Focus on explicit temporal information mentioned in the text chunk and supported by the context. - Avoid inferring temporal details not directly stated or implied by the context. Action type: UPDATE only. Evolved Skill on LoCoMo - UPDATE Skill: Update Existing Memory Purpose: Revise retrieved memory with new or corrected information from the text chunk, entity-specific details. including When to use: - The text chunk clarifies, corrects, or extends retrieved memory. - The text chunk provides new information about specific entity or its activities. How to apply: - Select the best matching memory item. - Merge new details into single updated item. - Preserve accurate details that still hold, and ensure entity-specific information is accurately captured and updated. Constraints: - Do not create new memories. - Do not delete items. Action type: UPDATE only. 20 MemSkill: Learning and Evolving Memory Skills for Self-Evolving Agents B.3. Evolved Skills on ALFWorld Evolved Skill on ALFWorld - CAPTURE ACTION CONSTRAINTS Skill: Capture Action Constraints Purpose: Capture detailed constraints on actions, task completion. including object states and movements, necessary for When to use: - The text chunk mentions constraints on actions, including object states and movements. - The constraints are crucial for future task steps. How to apply: - Identify the action, its constraints, and relevant object states and movements from the text chunk. - Create new memory item with the action-constraint pair, including object states and movements. Constraints: - Only capture constraints on actions relevant to the task. - Update existing constraint memories if new information is provided. Action type: INSERT only. Evolved Skill on ALFWorld - DELETE Skill: Delete Invalid Memory Purpose: Remove retrieved memory that is wrong, outdated, or superseded by the text chunk. When to use: - The text chunk clearly contradicts memory. - plan or fact is explicitly canceled or replaced. How to apply: - Only delete when evidence is explicit. Constraints: - If uncertain, prefer no action over deletion. Action type: DELETE only. 21 MemSkill: Learning and Evolving Memory Skills for Self-Evolving Agents Evolved Skill on ALFWorld - INSERT Skill: Insert New Memory Purpose: Capture new, durable facts, including procedural knowledge and action sequences, from the current text chunk that are missing in memory. When to use: - The text chunk introduces new facts, events, plans, or context worth storing. - The information is stable and likely useful later. How to apply: - Compare against retrieved memories to avoid duplicates. - Split distinct facts into separate items, including action sequences. - Keep each item concise and specific. Constraints: - Skip trivial, fleeting, or speculative content. - Do not update or delete existing memories. Action type: INSERT only. Evolved Skill on ALFWorld - NOOP Skill: No Operation Purpose: Confirm no memory changes are needed for the text chunk. When to use: - The text chunk contains no new, corrective, or actionable information. Constraints: - Emit NOOP only if none of the selected skills produce actions. Action type: NOOP only. 22 MemSkill: Learning and Evolving Memory Skills for Self-Evolving Agents Evolved Skill on ALFWorld - TRACK OBJECT LOCATION Skill: Track Object Location Purpose: Explicitly track the location and state of an object necessary for task completion. When to use: - The text chunk mentions an objects location or state. - The objects location or state is crucial for future task steps. How to apply: - Identify the object, its location, and relevant state from the text chunk. - Create new memory item with the object-location-state triplet. Constraints: - Only track locations and states of objects relevant to the task. - Update existing location memories if new information is provided. Action type: INSERT only. Evolved Skill on ALFWorld - TRACK OBJECT MOVEMENTS Skill: Track Object Movements Purpose: Track movements of objects necessary for task completion. When to use: - The text chunk mentions an objects movement. - The objects movement is crucial for future task steps. How to apply: - Identify the object and its movement from the text chunk. - Create new memory item with the object-movement pair. Constraints: - Only track movements of objects relevant to the task. - Update existing movement memories if new information is provided. Action type: INSERT only. MemSkill: Learning and Evolving Memory Skills for Self-Evolving Agents Evolved Skill on ALFWorld - UPDATE Skill: Update Existing Memory Purpose: Revise retrieved memory with new or corrected information from the text chunk. When to use: - The text chunk clarifies, corrects, or extends retrieved memory. How to apply: - Select the best matching memory item. - Merge new details into single updated item. - Preserve accurate details that still hold. Constraints: - Do not create new memories. - Do not delete items. Action type: UPDATE only. C. Prompts LoCoMo Answer Prompt Based on the above context, write an answer in the form of short phrase for the following question. Answer with exact words from the context whenever possible. Question: {} Short answer: LongMemEval Answer Prompt will give you several history chats between you and user. Please answer the question based on the relevant chat history. History Chats: {} Current Date: {} Question: {} Short Answer: HotpotQA Answer Prompt Based on the following context, answer the question. The question may require reasoning across multiple pieces of information. {context} Question: {question} Instructions: - Read the context carefully and identify relevant information - If the answer can be found in the context, provide short, precise answer - Output your answer within <answer></answer>tags <answer>your answer here</answer> 24 MemSkill: Learning and Evolving Memory Skills for Self-Evolving Agents"
        },
        {
            "title": "ALFWorld Env Interaction Prompt",
            "content": "You are controlling text-based ALFWorld environment. Your job: choose the NEXT action as ONE text command. Output ONLY the command string, with no extra text. You MUST choose an action from the admissible actions list and copy it EXACTLY. Goal: {goal} Retrieved procedural tips (optional, short & actionable): {retrieved memory} Interaction history so far (most recent info matters most): {history} Admissible actions (choose exactly ONE and copy it verbatim): {actions} Now output exactly one line: the chosen action (must match one item above). Executor Prompt You are memory management executor. Apply the selected skills to the input text chunk and retrieved memories, then output memory actions. Input Text Chunk: {session text} Retrieved Memories (0-based index): {mem text} Selected Skills: {skills text} Guidelines: - Apply any skill as needed; skill may be used multiple times. - Read the input text chunk carefully line by line and apply any skill as needed. - Only use action types supported by the selected skills. - MEMORY INDEX is 0-based and must reference the retrieved memories list. - Output only action blocks in the format below. - Do not include explanations or REASONING lines. Output format (repeat as needed). Use ONE block per action and separate blocks with blank line: INSERT block: ACTION: INSERT MEMORY ITEM: [concise but complete summary with essential details] UPDATE block: ACTION: UPDATE MEMORY INDEX: [0-based index] UPDATED MEMORY: [concise but complete merged summary with essential updates] DELETE block: ACTION: DELETE MEMORY INDEX: [0-based index] MemSkill: Learning and Evolving Memory Skills for Self-Evolving Agents"
        },
        {
            "title": "Designer Analysis Prompt",
            "content": "You are an expert analyst for memory-augmented QA system. Analyze the failure cases below to identify why the system failed and how the memory management skills should change. ## How This System Works 1. **Memory Storage**: The system applies memory management skills to decide what information to store from the text chunk. 2. **Memory Retrieval**: At question time, it retrieves the most relevant memories by semantic similarity. 3. **Answer Generation**: An LLM answers using the retrieved memories. Failures can occur at any stage: - **Storage failure**: Important information was never stored (skill missing or misapplied) - **Retrieval failure**: Relevant memory exists but was not retrieved (embedding mismatch) - **Memory quality failure**: Memory exists but is too vague or incomplete to answer ## Current Memory Management Skills {operation bank description} ## Operation Evolution Feedback {evolution feedback} ## Failure Cases ({num failure cases} cases) {failure cases details} ## Analysis Instructions This is round 1 of reflection loop. Produce strong initial analysis that can be critiqued and improved. 1. For each case, check whether the retrieved memories contain the answer or the needed evidence. 2. If missing, decide whether it was never stored (storage failure) or stored but too weak (memory quality failure). 3. If the answer is present but not retrieved, label it retrieval failure and avoid changing skills unless the pattern repeats. 4. Group cases into patterns tied to information types, entities, temporal details, or constraints. 5. For each pattern, propose concrete skill change: add new skill or refine an existing one to capture missing details. 6. Provide up to {max changes} recommendations total (use fewer if only one change is justified). ## Output Format Provide your analysis as JSON: { failure patterns: [ { pattern name: [descriptive name for this failure pattern], affected cases: [list of case numbers, e.g., 1, 3, 5], root cause: [storage failureretrieval failurememory quality failure], explanation: [why this pattern of failures is occurring], potential fix: [what kind of operation change could address this] } ], recommendations: [ { action: [add new operationrefine existing operationno change], target operation: [operation name to refine, or null if adding new], rationale: [clear explanation of why this is the best improvement], priority: [highmediumlow] } ], summary: [1-2 sentence summary of main findings] } Focus on actionable insights. What specific change to the skill bank would prevent these failures? Output ONLY the JSON, no other text. 26 MemSkill: Learning and Evolving Memory Skills for Self-Evolving Agents Designer Refinement Prompt (1/2) Based on the failure analysis, propose specific improvement to the memory operation bank. ## Failure Analysis (from Stage 1) {analysis feedback} ## Current Operation Bank {operation bank full} {evolution feedback} ## Your Task Propose up to {max changes} improvements based on the analysis: **Option - Add New Operation**: Create new operation if the analysis shows capability gap (e.g., certain information types are not being captured). **Option - Refine Existing Operation**: Improve an existing operations instruction template if the analysis shows its not working well (e.g., memories are too vague, missing key details). **Option - No Change**: If the failures are due to retrieval issues (not operation issues), or if the current operations are already well-designed. ## CRITICAL Requirements 1. instruction template MUST be skill-style guide and MUST NOT include context placeholders (the executor injects the text chunk and retrieved memories) 2. instruction template MUST clearly state purpose, when to use, and constraints 3. instruction template MUST specify the allowed action type (INSERT or UPDATE only) 4. For new operations, update type must be either insert or update (delete and noop operations are not evolved at this time) 5. Only propose operations with update type insert or update 6. Avoid labels like ENHANCED, ADVANCED, or other marketing adjectives in descriptions or templates; keep phrasing neutral and task-specific 7. Do NOT embed output blocks; the executor handles output formatting and can apply the skill multiple times 8. The number of changes in the list MUST be less than max changes 9. Do NOT modify the same operation more than once in single response, and do NOT refine an operation you add in the same response ## Example of Well-Designed Insert Operation { name: extract personal preferences, description: Memory management skill for capturing personal preferences and habits mentioned in the text chunk., update type: insert, instruction template: Skill: Insert Preferences. Purpose: Capture personal preferences, habits, or opinions stated in the text chunk. When to use: The text chunk mentions likes, dislikes, routines, or goals tied to person. How to apply: Attribute the preference to the correct person. Keep the preference specific and actionable. Constraints: Avoid one-off or ambiguous statements. Action type: INSERT only. } ## Output Format Respond with ONE of these JSON structures: ### One or more changes (up to max changes): { action: apply changes, summary: [overall rationale for the set of changes], changes: [ { action: add new, new operation: { name: [snake case name], description: [what it does and when to use it], instruction template: [skill-style instruction template], update type: [insertupdate], reasoning: [how this addresses the identified failures] } }, { action: refine existing, refined operation: { name: [existing operation name], changes: { description: [improved description], instruction template: [improved template] }, reasoning: [how these changes address the identified failures] } } ] } 27 MemSkill: Learning and Evolving Memory Skills for Self-Evolving Agents Designer Refinement Prompt (2/2) ### No changes needed: { action: no change, reasoning: [why the current operations are sufficient] } ## Instruction Template Structure When writing instruction templates, follow this structure: Skill: [Short skill name] Purpose: [What this skill does] When to use: - [Trigger 1] - [Trigger 2] How to apply: - [Step 1] - [Step 2] Constraints: - [What to avoid] Action type: [INSERT only UPDATE only] Output ONLY the JSON, no other text. 28 MemSkill: Learning and Evolving Memory Skills for Self-Evolving Agents"
        },
        {
            "title": "LLM Judge Prompt",
            "content": "You are an expert judge evaluating the quality of an answer for QA task. Your goal is to determine whether the models answer correctly and sufficiently answers the given question. Read the following information carefully: [Question] {question} [Ground Truth Answers] {ground truth} [Model Answer] {model answer} Your evaluation criteria: 1. Correctness: - Is the model answer factually consistent with ANY of the correct answers? - Does it avoid contradictions or introducing false information? 2. Relevance: - Does the answer address the question directly without unnecessary content? 3. Completeness: - Does the answer include all essential information needed to fully answer the question? - Partial answers are allowed but should receive lower scores. Scoring Rules: - Score = 1.0 if the answer is fully correct. - Score = 0.5 if the answer is partially correct but incomplete or slightly inaccurate. - Score = 0.0 if the answer is incorrect, irrelevant, or contradicts the ground truth. Output Format (STRICT): Return your output as JSON dictionary with two fields: { explanation: <brief explanation of your reasoning>, score: <0.0 0.5 1.0>} Be concise and objective. Do not include anything outside the JSON."
        }
    ],
    "affiliations": [
        "Nanyang Technological University",
        "Tsinghua University",
        "University of Illinois Urbana-Champaign"
    ]
}