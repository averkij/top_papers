{
    "paper_title": "MuSc-V2: Zero-Shot Multimodal Industrial Anomaly Classification and Segmentation with Mutual Scoring of Unlabeled Samples",
    "authors": [
        "Xurui Li",
        "Feng Xue",
        "Yu Zhou"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Zero-shot anomaly classification (AC) and segmentation (AS) methods aim to identify and outline defects without using any labeled samples. In this paper, we reveal a key property that is overlooked by existing methods: normal image patches across industrial products typically find many other similar patches, not only in 2D appearance but also in 3D shapes, while anomalies remain diverse and isolated. To explicitly leverage this discriminative property, we propose a Mutual Scoring framework (MuSc-V2) for zero-shot AC/AS, which flexibly supports single 2D/3D or multimodality. Specifically, our method begins by improving 3D representation through Iterative Point Grouping (IPG), which reduces false positives from discontinuous surfaces. Then we use Similarity Neighborhood Aggregation with Multi-Degrees (SNAMD) to fuse 2D/3D neighborhood cues into more discriminative multi-scale patch features for mutual scoring. The core comprises a Mutual Scoring Mechanism (MSM) that lets samples within each modality to assign score to each other, and Cross-modal Anomaly Enhancement (CAE) that fuses 2D and 3D scores to recover modality-specific missing anomalies. Finally, Re-scoring with Constrained Neighborhood (RsCon) suppresses false classification based on similarity to more representative samples. Our framework flexibly works on both the full dataset and smaller subsets with consistently robust performance, ensuring seamless adaptability across diverse product lines. In aid of the novel framework, MuSc-V2 achieves significant performance improvements: a $\\textbf{+23.7\\%}$ AP gain on the MVTec 3D-AD dataset and a $\\textbf{+19.3\\%}$ boost on the Eyecandies dataset, surpassing previous zero-shot benchmarks and even outperforming most few-shot methods. The code will be available at The code will be available at \\href{https://github.com/HUST-SLOW/MuSc-V2}{https://github.com/HUST-SLOW/MuSc-V2}."
        },
        {
            "title": "Start",
            "content": "MuSc-V2: Zero-Shot Multimodal Industrial Anomaly Classification and Segmentation with Mutual Scoring of Unlabeled Samples Xurui Li, Feng Xue, Member, IEEE, and Yu Zhou Member, IEEE. 1 5 2 0 2 3 1 ] . [ 1 7 4 0 0 1 . 1 1 5 2 : r AbstractZero-shot anomaly classification (AC) and segmentation (AS) methods aim to identify and outline defects without using any labeled samples. In this paper, we reveal key property that is overlooked by existing methods: normal image patches across industrial products typically find many other similar patches, not only in 2D appearance but also in 3D shapes, while anomalies remain diverse and isolated. To explicitly leverage this discriminative property, we propose Mutual Scoring framework (MuSc-V2) for zero-shot AC/AS, which flexibly supports single 2D/3D or multimodality. Specifically, our method begins by improving 3D representation through Iterative Point Grouping (IPG), which reduces false positives from discontinuous surfaces. Then we use Similarity Neighborhood Aggregation with MultiDegrees (SNAMD) to fuse 2D/3D neighborhood cues into more discriminative multi-scale patch features for mutual scoring. The core comprises Mutual Scoring Mechanism (MSM) that lets samples within each modality to assign score to each other, and Cross-modal Anomaly Enhancement (CAE) that fuses 2D and 3D scores to recover modality-specific missing anomalies. Finally, Rescoring with Constrained Neighborhood (RsCon) suppresses false classification based on similarity to more representative samples. Our framework flexibly works on both the full dataset and smaller subsets with consistently robust performance, ensuring seamless adaptability across diverse product lines. In aid of the novel framework, MuSc-V2 achieves significant performance improvements: +23.7% AP gain on the MVTec 3D-AD dataset and +19.3% boost on the Eyecandies dataset, surpassing previous zero-shot benchmarks and even outperforming most few-shot methods. The code will be available at GitHub. Index TermsZero-shot anomaly classification and segmentation, Multimodal, Mutual scoring, Industrial scenarios. I. INTRODUCTION NDUSTRIAL anomaly classification (AC) and segmentation (AS) are important tasks in the computer vision field. The AC task aims to discover objects with anomalies at the sample level, while the AS locates anomalies precisely. In real industrial scenarios, anomalies may appear on various objects, textures, shapes, and lights. The high diversity of anomalies makes the AC/AS task challenging. Current approaches address these challenges using 2D, 3D, or multimodal data. Early industrial AC/AS searches focused on 2D images and evolved along three paradigms. Unsupervised approaches This work was supported by the National Natural Science Foundation of China under Grant No.62176098. The computation is completed in the HPC Platform of Huazhong University of Science and Technology. (Corresponding author: Yu Zhou.) Xurui Li and Yu Zhou are with the School of Electronic Information and Communications, Huazhong University of Science and Technology, Wuhan 430074, China. (e-mail: xrli plus@hust.edu.cn; yuzhou@hust.edu.cn). Feng Xue is with the School of Computer Science, University of Trento, Italy. (e-mail: feng.xue@unitn.it). Fig. 1. (a) Zero-shot AC/AS methods for 2D modal. (b) Zero-shot AC/AS methods for 3D modal. These CLIP-based methods require additional text prompts and fine-tuning on additional industrial datasets. (c) Our MuSc-V2 is the first multimodal zero-shot method without any prompts or training. (full-shot) achieve high accuracy but require lot of labeled normal samples for training, e.g., [1], [2], [3], [4], [5], [6], [7], [8]. In contrast, some few-shot methods [9], [10], [11], [12], [13], [14] reduce this dependency, delivering competitive performance with minimal normal samples. The latest zeroshot approaches [15], [16], [17] shown in Fig. 1 (a) break new ground by using CLIPs text-image alignment for anomaly measurement. To overcome the limitations of 2D imaging, such as illumination, angle occlusion, camera resolution, etc., recent methods [18], [19], [20] have introduced the 3D point cloud. These approaches complement ambiguous 2D features with spatial information, culminating in zero-shot 3D techniques [21] that employ multi-view rendering, as shown in Fig. 1 (b). However, existing methods, whether 2D or 3D, rely on comparing each unlabeled sample with labeled normal samples or text prompts. This convention overlooks the wealth of implicit normal information among unlabeled samples across both 2D and 3D modals. This is particularly evident in industrial production lines, where products from the same line exhibit strong consistency and homogeneity. Actually, our statistics reveals that normal primitives (i.e., 2D pixels or 3D points) dominate the industrial data (99.71% in MVTec 3DAD, 99.77% in Eyecandies 2D images; 98.57%-99.31% 3D points). This prevalence allows normal regions to consistently find numerous similar counterparts across other unlabeled samples. In contrast, anomalies are often different from each other even with the same type, due to their randomness and unpredictability. Therefore, such an intrinsic discriminative property enables exploitation of both normal consistency and abnormal divergence in unlabeled multimodal data for zeroshot AC/AS. Motivated by this, we propose MuSc-V2, multimodal zero-shot AC/AS approach (Fig. 1 (c)), which directly recognizes anomalies by scoring it using other unlabeled samples mutually, thus no need for any training process or prompts. To enable effective mutual scoring, we first develop two feature improvement modules critical for reducing false detections. For 3D modal, Iterative Point Grouping (IPG) replaces traditional KNN grouping to reduce false positives caused by discontinuous surfaces, ensuring geometrically consistent 3D patches. For both modalities, Similar Neighborhood Aggregation with Multi-Degrees (SNAMD) models variable-sized anomalies by fusing multi-scale neighborhood features. During aggregation, we design similarity-weighted pooling (SWPooling) to prevent missed detections. Building upon the unexploited discriminative characteristic implied in the unlabeled data, our core Mutual Scoring Mechanism (MSM) leverages the improved 2D/3D features to establish training-free paradigm where unlabeled samples mutually assign anomaly scores. Meanwhile, to address modality-specific false negatives, we propose the Cross-modal Anomaly Enhancement (CAE) to raise scores of unnoticeable abnormal regions in single modality. Finally, to suppress false classification caused by local noise and weak anomalies, we explore the samplelevel relationship and further design the Re-Scoring with Constrained Neighborhood (RsCon). Evaluations on MVTec 3D-AD and Eyecandies datasets demonstrate significant improvements in both single-model (2D or 3D) and multi-modal (2D+3D). Especially in the multimodal AS, we obtain +23.7% and +19.3% gains on MVTec 3D-AD and Eyecandies datasets. For the multimodal AC, our method achieves +6.2% and +1.2% AUROC on these datasets. It is worth noting that these performances remain consistent across the entire dataset and smaller subsets (drop 1.0%). Moreover, our method shows strong robustness to varying ratios of normal samples, even in the extreme case with no normal sample available, performance degradation remains below 3%. These results further attest to its reliability for real-world scenarios. The points below highlight the contributions of our work: To the best of our knowledge, we propose the first multimodal method MuSc-V2 that only uses the unlabeled samples for industrial AC and AS. Furthermore, our method is also the first training-free multimodal (2D and 3D) zero-shot industrial AC/AS method. We reveal the potential capability of normal and abnormal regions contained in unlabeled samples regardless of 2D or 3D modal. It inspires us to propose the mutual scoring mechanism, which is novel zero-shot AC/AS paradigm with high flexibility and adaptability to any modal. Our method has significant advantages compared with the existing zero-shot methods, and such advantages are applicable in 2D, 3D, and 2D+3D settings. 2 This paper is an extension of our previous version, i.e. MuSc published in ICLR-2024 [22]. Compared with the conference version, the following improvements have been made: 1) Framework extension. We extend the original 2D framework to multimodal (2D+3D) and conduct experiments on multimodal datasets to verify its effectiveness. 2) Module improvement. During the point cloud preprocessing, we replace the traditional KNN with our Iterative Point Grouping (IPG) to generate consistent normal features. For fine-grained anomaly segmentation, we update the previous LNAMD to SNAMD, and adjust it to adapt to the 3D modal. The SWPooling is designed to reduce the missed detections. If both modals are available, we incorporate the Cross-modal Anomaly Enhancement (CAE) into the vanilla MSM to boost detection of modality-specific anomalies. We improve the original RsCIN to RsCon, which could be compatible with 3D backbones without the [CLS] token. 3) Performance gain. Experiments demonstrate that MuScV2 is 5.6 faster than the original MuSc. Furthermore, MuSc-V2 achieves +0.8% AP segmentation on MVTec AD and +1.6% F1-max improvement on VisA. 4) More scenarios. We investigate the robustness of MuScV2 on datasets with different sizes and the ratios of normal samples, which proves that MuSc-V2 could generalize to different production lines without any training. II. RELATED WORKS A. Transformer architecture for 2D and 3D representation Vision transformer (ViT) [23] and point transformer (PT) [24] have become standard for 2D and 3D feature representation. Some pre-trained models like CLIP [25]/DINO [26] for 2D modal and Point-MAE [27]/Point-BERT [28] for 3D modal deliver high-quality patch features. However, these features often struggle with industrial anomalies of varying sizes. Swin transformer [29], [30] proposes the varied-size window attention to compute attention within multi-scale windows, which risks compromising fine-grained anomaly discrimination. We propose training-free solution SNAMD, that optimizes patch features via Similarity-Weighted Pooling to better capture multi-scale anomalies and preserve small anomalies. B. Point cloud grouping To reduce computational costs of self-attention, existing 3D feature extractors [31], [32], [24], [33], [34], [35] preprocess point clouds through FPS and KNN grouping, encoding each group as 3D patch token. However, these methods risk merging multiple surfaces within single group, particularly when components are spatially proximate, resulting in deviant tokens that trigger false positives. Point Transformer V2/V3 [36], [37] propose new grouping strategies, but they focus more on optimizing speed and overlook this fundamental issue. We propose an Iterative Point Grouping strategy to address this challenge by ensuring surface-consistent groupings for more robust 3D feature extraction. 3 Fig. 2. The pipeline of our MuSc-V2. This framework processes 2D images and 3D point clouds through four important innovations: (1) IPG replaces the current grouping strategy in the point transformer to generate groups with continuous surfaces (Sec. III-A). (2) SNAMD improves the abnormal modeling ability with varying sizes for both modals (Sec. III-B). (3) MSM obtains anomaly segmentation results of 2D/3D modals. CAE enhances scores of anomalies if both modals are available (Sec. III-C). (4) RsCon reduces false anomaly classification from local noise and weak anomalies (Sec. III-D). C. Zero-shot anomaly classification and segmentation In the industrial vision field, zero-shot AC/AS has garnered more attention. However, most methods focus on 2D modal by image-text alignment of CLIP [25]. These CLIP-based approaches [15], [38], [39], [40], [16], [41] fine-tune image encoder or text encoder to bridge the domain gap between natural and industrial scenarios. Meanwhile, some zero-shot methods [42], [17], [22] focus on detecting anomalies by using the unlabeled samples itself. [42] explores the relationship between patches inside one unlabeled image, but only handles texture products. ACR [17] proposes new adaptation strategy without human involvement, which trains the network by other products from the dataset. For the 3D zero-shot task, PointAD [21] renders the point cloud to multiple images with different view angles. In this way, 2D methods could be used to process 3D data. In this paper, we propose mutual scoring mechanism for both 2D/3D modals, which only uses the unlabeled samples without additional fine-tuning. D. Multimodal anomaly classification and segmentation Multimodal industrial AC/AS task aims to identify defects in the industrial product through its 2D image and 3D point cloud. Some methods [43], [44], [45], [19] are proposed for the unsupervised (full-shot) AC/AS task. Among them, M3DM [18] fine-tunes features of two modals by contrastive learning for cross-modal alignment. Shape-guided [46] use 2D features stored in the memory bank to reconstruct 3D features to guide the identification of 2D anomalies. In addition, CFM [47] implements cross-modal feature reconstruction according to the student-teacher network, greatly reducing time consumption. These methods require collecting large number of normal images and point clouds for model training, which limits the migration ability on the new production line. We propose the multimodal mutual scoring without any training and labeled samples. The CAE module is inserted into our mutual scoring mechanism to eliminate the blind spot of single model. E. Manifold learning In high-dimensional manifolds, Euclidean properties are only preserved in local space, making direct distance calculations unreliable. Some manifold learning methods [48], [49], [50], [51] construct an embedding space where distances align with the underlying manifold structure. Inspired by the above principles, we develop the RsCon module to refine pixel-level anomaly classification in manifolds. III. METHOD Given unlabeled image-point cloud pairs = {Oii [1, ], Oi = (Ii, Pi)}, where Ii and Pi is the 2D image and 3D point cloud respectively. Our approach is designed to handle both single-modal and multi-modal situations. The pipeline is illustrated in Fig. 2, which consists of four important components: (1) 2D/3D feature representation (Sec. III-A). We extract the pretrained 2D/3D features in this section. For 3D model, we design Iterative Point Grouping (IPG) strategy to replace the traditional pre-processing method, KNN, to alleviate 3D false positives from discontinuous surface representations. (2) Similar Neighborhood Aggregation with Multiple Degrees (Sec. III-B). We propose SNAMD module to aggregate multi-scale neighborhood features to suppress false negatives of variable-sized anomalies. (3) Mutual Scoring Mechanism (Sec. III-C). This zero-shot AC/AS paradigm scores 2D/3D patches in an unlabeled sample using other unlabeled samples. Moreover, to reduce modality-specific false negatives, we propose the Cross-modal Anomaly Enhancement (CAE) to fuse 2D/3D scores. (4) Re-Scoring with Constrained Neighborhood (Sec. III-D). This module suppresses false classifications caused by local noise and weak anomalies. 4 3. Group Expansion: To expand the group L0 on continuous surface, we iteratively add the Kiter points closest to this group to L0. Specifically, for each candidate point pˆj Pi in iteration t, we compute its distance d(t, pˆj) to the current group Lt1 as, d(t, pˆj) = min pj Lt1 pˆj pj2. (1) Then we use the following formula to choose the closest Kiter points (indicate by top-K) and add them to the current group: Lt = Lt1 top-Kpˆj Pi (cid:0)d(t, pˆj)(cid:1) (2) This expansion continues until the number of points in Lt equals KP. As shown in Fig.3 (c), These newly incorporated points are all on the initial surface. Fig.3 (d) demonstrates (in blue) that conventional KNN incorrectly merges points from two distinct surfaces, generating false positives. In contrast, our IPG strategy preserves surface continuity in (e), where previously problematic patches now exhibit normal feature characteristics, effectively eliminating false detections within the marked bounding box region. into (N MI and i,s Given i,s CI , where B. Similar Neighborhood Aggregation with Multiple Degrees. , we propose SNAMD to capture anomalies of varying sizes, we search the multi-scale neighborhoods for each patch. Then, we design similarityweighted pooling method to aggregate neighborhood information, thereby preserving the discrimination of small anomalies. In the 2D case, we reshape the vectorized patch tokens i,s MI CI grid to restore the spatial posiI tion information for the convenience of neighbor searching. We extract neighborhood features for patch as ) RN i,s is the index of neighI borhood patches and r is the number. Notably, larger aggregation degree corresponds to broader neighborhood, allowing the capture of larger anomaly regions. In the 3D case, due to the irregularity of point clouds, for each 3D patch, we identify its nearest neighborhood by computing the Euclidean distances between the current patch center and all other patches in the 3D space. The corresponding features of these neighboring patches are then defined as i,s (N Similarity-Weighted Pooling. Existing approaches [55], [22] employ adaptive average pooling to aggregate the neighborhood features, which often dilutes small anomalies with surrounding normal patches by uniformly weighting neighlarger r. Therefore, we propose the bors, particularly at similarity-weighted pooling (SWPooling), which aggregates the most relevant neighborhood features to reduce the interference of irrelevant backgrounds. Specifically, we calculate the similarity matrix of patch and all patches in the neighborhood , where higher similarity indicates lower interference. The similarity matrix Λi,s 1 is formulated as: r,m RN ) RN r CP . Λi,s r,m = exp(F i,s(N ) i,s(m)2) (3) Fig. 3. Toy example of searching KP points for the center point pc. The green lines and regions represent the candidate points, and the blue ones indicate the searched points as the group points of pc. A. 2D/3D Patch Representation 2D Feature Extraction. Following [15], [16], [52], we adopt vision transformer [23] consisting of stages to extract hierarchical 2D features. For image Ii, we define the patch tokens produced by stage as i,s RMICI, where MI is the number of patch tokens, CI is the feature dimension, and [1, S]. 3D Feature Extraction. To extract patch-level 3D features, we first adopt an Iterative Point Grouping strategy (introduced below) as pre-processing step to group points that lie on common surfaces. Then, following [18], [19], [47], the point groups are fed into point transformer [24] with stages to extract the 3D features. For point cloud Pi, the stage produce MP 3D patch tokens as i,s RMPCP , where each group is regarded as 3D patch. Iterative Point Grouping. Existing 3D feature extractors [24], [31], [34] employ farthest point sampling [53] and KNN strategy to cluster points for local feature extraction. However, the KNN strategy employs spatial proximity only, which may result in one group containing discontinuous surfaces, as shown in Fig.3 (a). Such discontinuous normal point groups are easily misclassified as anomalies due to their isolated pattern. To alleviate this problem, we propose the Iterative Point Grouping (IPG) strategy, which replaces fixed-distance neighborhood selection with an iterative expansion approach. We first group the point cloud Pi into MP groups of KP points by KNN. The center point of each group is represented as pc. The following steps are carried out on this basis above. 1. Curvature Calculation: To correct point groups with discontinuous surfaces, we compute the curvature at each groups center point pc [54]. As shown in Fig.3 (a), we observe that when points from different surface (Surface1) are incorrectly grouped, the curvature at pc increases significantly. Therefore, we perform the following steps for re-grouping if exceeds predefined threshold Cthr. 2. Group Initialization: To re-group the points around pc with over-threshold curvature, we initialize group L0= {pj}Kiter j=1 containing Kiter nearest points of pc, where Kiter < KP remains small enough to avoid including points from other planes. As shown in Fig.3 (b), these initial points identify unique surface (blue line) for each group, effectively preventing the inclusion of points from other nearby surfaces (Surface-1). 5 Fig. 4. Similarity-Weighted Pooling (SWPooling) Versus Average Pooling (APooling). Top: One toy example represents feature maps aggregated by two aggregation methods, where blue patches and red patches simulate normal and abnormal tokens, respectively. Bottom: The visualization of segmentation results with SWPooling and APooling by one real example. ) RN where i,s(m) R1C denotes the feature vector of patch m, and i,s(N C represents the features of its neighborhood. The exponential function exp() amplifies the importance of high-similarity patches. Then, the similarity matrix performs the weighted average of the features within (m), the neighborhood to generate the aggregated feature i,s,r (a-b) Score distributions Ai,s,m Fig. 5. for normal/abnormal 2D patches. (c-d) Corresponding score distributions for 3D patch. (e-f) Comparison of ai,s,m distributions without/with Interval Average (IA) operation. we leverage each image in {DIi} to assign score for patch of image Ii as follows, i,s,r (m) = mean(Λi,s r,m i,s(N )) (4) ai,s,m (Ij) = min Fi,s (m) Fj,s (n)2 (5) where represents element-wise multiplication. Fig.4 (a) compares standard average pooling (APooling) with our SWPooling. APooling uniformly blends r neighborhoods and retains only of the original patchs information. This dilutes anomalies (red) with surrounding normal features (blue), causing missing detections in small anomalies. In contrast, our SWPooling preserves local focus, suppressing these false negatives (yellow box in (b)). 1 i,s,r For 2D patches, we use multiple aggregation degrees, i.e., {1, 3, 5}. To optimize efficiency, we concatenate multifor {1, 3, 5}) and compress them scale features (F to CI dimensions, yielding the final aggregated feature Fi,s RMICI . This optimization reduces subsequent mutual scoring operations, making MuSc-V2 5.6 faster than the conference version. For 3D patches, to ensure surface-consistent aggregation, we restrict = 1 for high-curvature patches (C > Cthr), as in our IPG strategy. Therefore, the neighborhood patches belong to the same surface. We perform the same compression operation to obtain the aggregated 3D feature Fi,s . C. Multimodal Mutual Scoring According to the core observations mentioned above (normal patches across unlabelled samples could find many similar patches, while anomalies remain isolated), we introduce mutual scoring mechanism applicable to both 2D and 3D data. This efficient mechanism generates high-quality patch-level anomaly scores through cross-sample comparison. Mutual Scoring Mechanism. Building upon the above discriminative aggregated features, our MSM employs novel paradigm where unlabeled samples mutually assign anomaly scores to each other. Using 2D images as an illustrative case, where (Ij) indicates that image Ij {DIi} is employed for scoring. If the patch token Fi,s (m) of Ii is similar to any patch token Fj,s (n) of Ij, the image Ij assigns small anomaly score to Fi,s (m). In this way, each patch has score set Ai,s,m = {ai,s,m (Ij)j [1, ], = i}. As shown in Fig.5, the histogram of Ai,s,m for all normal (a) and abnormal (b) patches in demonstrates the discriminative power. These findings could generalize directly to 3D modal, with Fig.5 (c-d) demonstrating the same score distribution patterns for normal/abnormal 3D patches. Our analysis above reveals that most unlabeled images in assign lower scores to normal patches and higher scores to abnormal ones. Therefore, simple average operation on Ai,s,m could effectively differentiate between normal and abnormal patches, shown in Fig. 5 (e). However, minor overlaps in (e) occur when some normal patches exhibit appearance variations across images, resulting in elevated scores. To mitigate this, we apply the Interval Average (IA) operation on the lowest X% of scores in Ai,s,m , suppressing outlier influences through: ai,s,m ="
        },
        {
            "title": "1\nK",
            "content": "(cid:88) ai,s,m (I k) k[1,K] (6) where denotes images in the lowest X% score interval and is their count. As shown in Fig.5 (f), such design reduces the normal/abnormal score overlap, particularly in [0.5, 0.7] compared to (e). The final patch-level anomaly score ai,m combines multi-stage results as follows, ai,m ="
        },
        {
            "title": "1\nS",
            "content": "(cid:88) ai,s,m s[1,S] (7) yielding the patch-level anomaly score vector Ai [ai,1 = ]. Similarly, the patch-level anomaly score vec- , ..., ai,MI 6 Fig. 7. Top: Histogram of anomaly classification scores of unlabeled samples before (a) and after (b) using our RsCon. Bottom: normal example (i) and an abnormal example (ii) of RsCon. We complete the remaining procedures of the mutual scoring mechanism to generate the patch-level anomaly score vectors Ai for image Ii. For 3D modal, we perform the same steps as in the 2D modal and obtain the patch-level anomaly score vectors Ai for point cloud Pi. Multimodal Anomaly Segmentation. We convert patchlevel anomaly scores to their original 2D/3D resolutions for segmentation evaluation. For 2D data, the score vector Ai RMI1 is reshaped to MI 1 and upsampled. For 3D data, we follow [18] to utilize inverse distance weight to interpolate scores to the point cloud. This yields 2D anomaly segmentation result Ai and 3D anomaly segmentation result Ai the final segmentation P. If both modals are available, + Ai combined their results through Ai = Ai P. MI Multimodal Anomaly Classification. Current AC/AS methods use the maximum score ci = max(Ai) as the pixel-level anomaly classification (AC) score. The AC score vector of samples in is denoted as = [c1, ..., cN ]. However ci is derived from the maximum value of the anomaly segmentation result, it is sensitive to local noises and is easy to overlook weak anomalies. In Sec. III-D, we propose the Re-Scoring with Constrained Neighborhood to mitigate these false classifications. D. Re-Scoring with Constrained Neighborhood While our mutual scoring mechanism effectively identifies most anomalies, it remains susceptible to both false positives and false negatives in certain challenging cases. To mitigate these in anomaly classification, we introduce the concept of anomaly-salient feature, which are extracted from the highestscoring patch in the anomaly map. We then calculate the similarity of these anomaly-salient features across different samples to calibrate scores C. As illustrated in Fig. 7, noisy but normal sample in (i) is incorrectly scored 0.439, while its similar samples have less noise and receive consistently lower scores (0.0930.240). The similar case holds in (ii), an abnormal sample with subtle anomaly is assigned lower score of 0.369, yet its similar samples have more visible anomalies and attain higher scores (0.4560.723). These observations reveal that score bias caused by local noise or weak anomalies could be corrected by referring to other similar samples. Fig. 6. Two examples whose anomalies exhibit single-modality prominence: (a) 3D-visible peach anomaly, (b) 2D-detectable carrot anomaly. = [ai,1 , ..., ai,MP tor of the point cloud Pi is denoted as Ai where ai,n ], represents the anomaly score of the n-th 3D patch. Cross-modal Anomaly Enhancement. Our mutual scoring mechanism achieves strong patch-level anomaly detection within each modality, yet faces limitations with modalityspecific anomalies. As Fig.6 shows, peach contamination (a) is prominent in 3D but subtle in 2D. While the carrot anomaly (b) is only significant in 2D. These inherent data limitations constitute the theoretical bound for single-modal scoring. To address this limitation, we propose the Cross-modal Anomaly Enhancement (CAE) module, which augments anomalies invisible to single modal through cross-modal score fusion. Using 2D data as an example, our approach begins with the mutual scoring mechanism (Eq.5), which computes patch scores ai,s,m (Pj) for each patch in 2D and 3D modals respectively. The CAE then integrates these scores by two important steps. (Ij) and ai,s,n Cross-modal alignment. To overcome the spatial misalignment of 2D and 3D patches, we use the point coordinate and camera parameters to establish projection list Pi,m. This mapping associates each 2D patch with its corresponding 3D points, thereby enabling accurate cross-modal mapping. Then we average the 3D scores of all corresponding points to calculate the aligned 3D score ai,s,n P2I (Pj) in 2D space as, ai,s,m P2I (Pj) = 1 Pi,m (cid:88) ai,s,n (Pj) nPi,m (8) where Pi,m denotes the list length. Scores with empty Pi,m (2D background patches) automatically are set to 0. To ensure the value range consistency of cross-modal scores, we rescale P2I = {ai,s,m the projected 3D scores Ai,s,m (Pk)k [1, ], = P2I i} into value range of 2D scores Ai,s,m . Anomaly enhancement. To preserve anomalies detected in either modal, we perform max operation and fuse aligned 3D scores ai,s,m (Pj) with 2D scores ai,s,m (Ij) + λ max(ai,s,m P2I (Ij) ai,s,m P2I (Ij) as, (Pj), ai,s,m (Ij)) (9) ai,s,m I P2I where λ = 1std(Ai,s,m ) is the confidence weight to measure each patchs reliability. True anomalies exhibit low variance (λ 1) due to consistent dissimilarity, while false positives show higher variance from partial similarity with normal patches. This design enhances anomalies and prevents crossmodal false positives, as validated in Fig.6. According to the above observation and motivation, we propose the re-scoring with constrained neighborhood (RsCon) to mitigate the above false classifications. To calculate the anomaly-salient features, we extract them from the penultimate stage of the feature extractor as, = Fi,S1 I (arg max (ai,m )) = Fi,S1 (arg max (ai,m )) (10) (11) and where indicate 2D and 3D features respectively. If both modals are available, we concatenate them into multimodal feature Fi R1(CI+CP). Then we construct an edge-weighted graph = (V, W) to build relationships in D. Each vertex represents sample, while the edge weights are derived from similarity matrix computed as Wi,j = Fi Fj, where means dot product. With this sample-level similarity matrix, we employ manifold learning techniques [56], [48], [50] to optimize the initial AC score C. However, due to the close values in Wi,, excessive features Fj(j = i) are propagated to Fi, which make the AC accuracy decrease, as the experimental illustration in Sec.IV-D4. Therefore, we design Window Mask Operation (WMO) to constrain the number of samples. The binary window mask matrix RN as follows, (cid:40) M(i, j) = if Oj Nk(Oi) 1, 0, otherwise, (12) where Nk(Oi) indicates nearest samples of sample Oi. Then the AC score is updated as, ˆC = 1 2 (D1(M W)C + C) (13) where ˆC RN 1 is the optimized sample-level AC score vector, and means element-wise multiplication. normalizes row-wise via D(i, i) = (cid:80)N j=1 MWi,j, ensuring balanced contributions even from low-similarity neighbors. Discussions. To further explain the principle of the RsCon module clearly, we use it to optimize ci of the sample Oi as an example. According to the similarity matrix W, we define the k-nearest neighbor to sample Oi as { ˆO1, ..., ˆOk} and their corresponding pixel-level AC scores as {c1, ..., ck}. Then we use in Eq. 13 to obtain their similarities to sample Oi as {wi,1, ..., wi,k}. The D1 normalizes these similarities and makes their sum equal to 1. The transformation results { ˆwk i,k} are calculated as, i,1, ..., ˆwk ˆwk i,j = wi,j wi,1 + ... + wi,k , where ˆwk i,j { ˆwk i,1, ..., ˆwk i,k} (14) Based on the above operations, using Eq. 13 to optimize AC score ci of sample Oi can be rewritten as, ˆci = = = 1 2 ci 2 ci 2 (( ˆwk ˆwk i,1c1 + ... + i,1c1 + ... + ˆwk 1 2 1 (cid:88) i,jcj ˆwk j=1 + + i,kck) + ci) 1 2 ˆwk i,kck (15) where ˆci represents the optimized AC score of sample Oi. Eq. 15 shows that ˆci is affected by anomaly classification scores in k-nearest neighbors. The value of ci increases if sample Oi has high-scoring k-neighbors (i.e., scores cj {c1, ..., ck} are high), and vice versa. Therefore sample Oi with local noises could be corrected since its nearest neighbor samples have small AC scores. As shown in Fig.7 (b), the overlap between normal and abnormal AC scores reduces after applying RsCon compared with that in (a). IV. EXPERIMENTS A. Experimental setting 1) Datasets: We conduct experiments on two multimodal industrial datasets (MVTec 3D-AD [57] and Eyecandies [63]) and two 2D-only datasets (MVTec AD [64] and VisA [65]). MVTec 3D-AD consists of 10 product categories with 41 types of anomalies, where 3D points are stored as XYZ maps matching the RGB image resolution. Eyecandies provides synthetic data for 10 categories of candies, cookies and sweets, including depth maps and camera parameters for 3D reconstruction. For 2D-only datasets, MVTec AD contains 15 object/texture categories and VisA has 12 objects across 3 domains. All datasets provide normal and abnormal samples in the unlabeled test set. Notably, our method could operate effectively without requiring the full dataset. To validate its performance on the smaller dataset, we partition the original dataset into subsets and conduct anomaly detection independently within each subset. 2) Evaluation Metrics: For image-level anomaly classification, we report 3 widely used metrics: the Area Under Receiver Operator Characteristic curve (AUROC), Average Precision (AP), and F1-score at optimal threshold (F1-max). For pixel-level anomaly segmentation, we evaluate with pixelwise AUROC, F1-max, AP, and Per-Region Overlap with 30% FPR (PRO@30%) [64]. All metrics above are calculated using official implementations. 3) Implementation Details: Following current multimodal anomaly detection methods [18], [19], [47], we use DINO ViTB-8 [26] for 2D feature extraction and Point Transformer [24] (pre-trained with Point-MAE [27]) for 3D feature extraction. For fair comparisons with some 2D-only methods, we also include ViT-L-14-336 pretrained with CLIP [25]. Both ViT and PT architectures are divided into 3 stages (S=3). The input images are resized to 224224, while point clouds are clustered into 1024 groups of 128 points each. Note that, to ensure robust evaluation of subset partitioning, we conduct experiments across 10 random seeds and report averaged results. Key hyperparameters in our MuSc-V2 include: IPGs iterative point increment Kiter=80 and curvature threshold Cthr=0.01, SNAMDs aggregation degrees {1, 3, 5}, MSMs minimum 30% score interval for IA, and RsCons window size k=7. All hyperparameters above are consistent across all datasets. 4) Competing methods: For 2D modal, we compare with some state-of-the-art zero-shot approaches, e.g. APRIL-GAN [16], AnomalyCLIP [52], ACR [17], AdaCLIP [38], VCPCLIP [39] and FAPrompt [58]. For the CLIP-based methods, we load official checkpoints for inference. In multimodal TABLE QUANTITATIVE COMPARISONS ON THE MVTEC 3D-AD AND EYECANDIES DATASETS. WE COMPARE OUR MUSC-V2 WITH SOME STATE-OF-THE-ART ZERO-SHOT AND FEW-SHOT METHODS. BOLD INDICATES THE BEST PERFORMANCE UNDER ZERO-SHOT SETTING. THE WHOLE DATASET IS DIVIDED INTO SUBSETS TO SIMULATE THE SMALL DATASETS. WE REPORT THE MEAN AND STANDARD DEVIATION OVER 10 RANDOM SEEDS, AND THE METRICS DECLINE IS SHOWN AS , WHICH ARE MARKED IN GRAY . ALL METRICS ARE IN %. 8 Dataset Method Ref & Year Backbone MVTec 3D-AD [57] (2D modal) MVTec 3D-AD [57] (3D modal) MVTec 3D-AD [57] (Multimodal) Eyecandies [63] (2D modal) ACR [17] APRIL-GAN [16] AnomalyCLIP [52] AdaCLIP [38] VCP-CLIP [39] FAPrompt [58] MuSc [22] MuSc-V WRN50 NeurIPS23 ViT-L-14-336 CVPRW23 ViT-L-14-336 ICLR24 ViT-L-14-336 ECCV24 ViT-L-14-336 ECCV24 ICCV25 ViT-L-14-336 Ours(ICLR24) ViT-L-14-336 ViT-L-14-336 Ours MuSc[22] MuSc-V2 Ours(ICLR24) ViT-B-8 ViT-B-8 Ours BTF (4-shot) [19] CVPR23 M3DM (4-shot) [18] CVPR23 ICCV23 PointCLIPv2 [59] CVPR23 ULIP [60] CVPR24 ULIPv2 [61] NeurIPS24 PointAD [21] CVPR25 CMAD [62] Ours MuSc-V2 BTF (4-shot) [19] CVPR23 M3DM (4-shot) [18] CVPR23 CVPR24 CFM (4-shot) [47] ICCV23 PointCLIPv2 [59] CVPR23 ULIP [60] CVPR24 ULIPv2 [61] NeurIPS24 PointAD [21] WACV25 3DzAL [62] Ours MuSc-V2 MuSc-V2 (g=2) Ours MuSc-V2 (g=3) Ours Ours MuSc-V2 FPFH PT ViT-B-16 PT PT ViT-L-14-336 ViT-H PT WRN50+FPFH ViT-B-8+PT ViT-B-8+PT ViT-B-16 ViT-B-16+PT ViT-B-16+PT ViT-L-14-336 WRN50+PointNet++ ViT-B-8+PT ViT-B-8+PT ViT-B-8+PT ViT-L-14-336+PT APRIL-GAN [16] AnomalyCLIP [52] AdaCLIP [38] VCP-CLIP [39] FAPrompt [58] MuSc [22] MuSc-V2 ViT-L-14-336 CVPRW23 ViT-L-14-336 ICLR24 ViT-L-14-336 ECCV24 ViT-L-14-336 ECCV24 ICCV25 ViT-L-14-336 Ours(ICLR24) ViT-L-14-336 ViT-L-14-336 Ours MuSc [22] MuSc-V2 Ours(ICLR24) ViT-B-8 ViT-B-8 Ours Eyecandies [63] (3D modal) BTF (4-shot) [19] CVPR23 M3DM (4-shot) [18] CVPR23 PointAD [21] MuSc-V2 NeurIPS24 Ours FPFH PT ViT-L-14-336 PT Eyecandies [63] (Multimodal) BTF (4-shot) [19] CVPR23 M3DM (4-shot) [18] CVPR23 CVPR24 CFM (4-shot) [47] NeurIPS24 PointAD [21] Ours MuSc-V2 MuSc-V2 (g=2) Ours MuSc-V2 (g=3) Ours Ours MuSc-V2 WRN50+FPFH ViT-B-8+PT ViT-B-8+PT ViT-L-14-336 ViT-B-8+PT ViT-B-8+PT ViT-B-8+PT ViT-L-14-336+PT Anomaly Classification (AC) Anomaly Segmentation (AS) AUROC-cls F1-max-cls AP-cls AUROC-seg F1-max-seg AP-seg PRO@30% 63.9 58.5 65.1 74.8 76.3 68.9 76.3 75.8 69.9 75.7 66.3 72.9 48.7 59.2 63.5 82.0 79.6 83.7(+1.7) 88.9 88.5 88.7 89.9 89.9 89.1 90.3 90.3(+0.4) 90.0 90.5 88.5 89.9 88.2 88.5 89.1 92.3 - 92.5(+0.2) 86.8 84.8 87.9 91.8 92.6 89.5 92.3 91.1 90.4 92.6 85.5 95.7 96.2 97.6 97.7 96.0 97.9 98.0(+0.1) 15.1 26.0 33.5 41.7 42.5 29.7 41.0 47.1(+4.6) 9.1 19.3 28.1 36.5 38.3 24.2 36.4 41.7(+3.4) 58.8 85.0 83.6 61.5 92.2 84.1 93.1 94.0(+0.9) 97.6 98.2 31.9 40.4 25.8 34.7 90.7 93. 88.4 89.6 79.7 83.9 86.3 94.2 93.1 94.4(+0.2) 96.8 95.6 89.5 89.8 91.0 95.5 - 97.1(+1.6) 32.6 15.5 4.2 4.7 4.8 30.7 - 45.9(+15.2) 28.7 8.4 1.8 2.2 2.3 24.9 - 44.4(+19.5) 88.3 82.9 60.6 61.2 64.9 84.4 - 88.4(+4.0) 89.3 91.0 91.2 89.2 89.2 89.0 92.2 - 93.0(+0.8) 90.3 97.3 67.1 92.4 97.9 78.5 94.0 98.3 77.3 79.1 95.4 66.4 78.0 95.0 61.5 78.3 95.1 59.8 90.2 97.2 86.9 84.8 - 64.9 88.1(+1.2) 97.0(+6.8) 99.0(+1.8) 87.60.20.5 92.70.20.3 96.60.10.2 99.00.00.0 54.30.20.3 54.40.20.3 96.90.00.1 87.20.30.9 92.60.20.4 96.40.10.4 99.00.00.0 53.60.21.0 53.70.21.0 96.80.10.2 98.8 31.1 32.1 40.5 13.2 12.6 12.3 31.0 - 54.7(+23.7) 34.5 35.1 42.4 21.8 21.1 21.0 37.2 - 54.6(+17.4) 88.0 91.9 92.4 88.6 85.8 84.8 96.1 - 96.8(+0.7) 93.6 96. 97.2 60.0 58.8 89.9 62.2 73.8 74.3 73.7 74.4 78.0 85.1(+7.1) 73.4 85. 64.2 64.9 69.1 69.1 68.7 74.4 73.5 74.6 74.6 77.5 84.8(+7.3) 74.2 81.8 69.9 69.8 71.2 72.1(+0.9) 65.4 75.7 75.2 75.5 75.6 80.4 87.4(+7.0) 73.8 86. 67.5 66.6 73.8 71.7 93.4 91.1 96.9 97.1 93.8 97.3 96.9 96.6 97.4 85.8 88.3 92.1 89.8 23.7 26.3 34.6 35.4 26.3 37.9 44.0(+6.1) 17.6 17.7 28.6 30.1 20.1 33.8 38.9(+5.1) 77.0 73.7 42.0 87.2 76.7 88.9 89.4(+0.5) 30.3 42.0 24.2 36.8 29.0 9.0 24.9 28.1(+3.2) 21.8 6.0 15.9 21.4(+5.5) 84.9 89. 62.3 63.1 71.3 66.9 68.8 75.3 72.8 76.4 82.9(+6.5) 74.3 92.7 65.3 82.8 96.1 73.5 82.5 96.4 71.6 84.3 95.3 77.7 83.9(+6.2) 90.1(+5.8) 97.5(+2.2) 83.50.60.4 82.60.40.3 85.80.40.3 97.40.00.1 44.40.20.3 41.30.20.5 89.80.10.2 83.40.70.5 82.20.70.7 85.50.50.6 97.40.10.1 43.80.30.9 40.60.31.2 89.80.30.3 97.3 24.2 32.4 32.9 30.5 44.7(+14.2) 19.3 27.5 29.0 22.5 41.8(+19.3) 67.0 75.7 73.9 80.4 86.1(+5.7) 84.4 90.9 87.6 42.6 46.2 85. scenarios, we evaluate against PointCLIPv2 [59], ULIP [60], ULIPv2 [61], PointAD [21], 3DzAL [62] and CMAD [66]. Since 3DzAL and CMAD have not been open-sourced, we only report the results from their official papers, with unavailable results denoted by -. In addition, we also compare fewshot methods, such as M3DM [18], CFM [47] and BTF [19]. B. Quantitative results In Table I, we compare our MuSc-V2 with state-of-theart zero-shot and few-shot methods on MVTec 3D-AD and Eyecandies datasets. We report the anomaly classification and segmentation results across 2D, 3D and multimodal settings. MuSc-V2 achieves superior performance in most metrics for all modals. Notably, it outperforms the second-best zero-shot method PointAD[21] by 23.7% and 19.3% AP for anomaly segmentation on these datasets. For anomaly classification, MuSc-V2 achieves 1.2% and 6.2% AUROC gains on both datasets. When partitioning the original dataset into {2, 3} subsets, both AC and AS metrics show minimal degradation ( ): at most 1.0% on MVTec 3D-AD and 1.2% on Eyecandies. The slightly larger impact on Eyecandies stems from its limited sample size (50 samples per product). We report these performances as meanstd, with maximum standard deviations of 0.3 on MVTec 3D-AD and 0.7 on Eyecandies. These small variations confirm the insensitivity to different partitioning schemes and adaptability to diverse dataset compositions. 9 Fig. 8. Visualization of anomaly segmentation results on MVTec 3D-AD and Eyecandies benchmarks. 3D modal and multimodal (MM) results are displayed. TABLE II QUANTITATIVE COMPARISONS ON MVTEC AD AND VISA DATASETS. BOLD INDICATES THE BEST PERFORMANCE. ALL METRICS ARE IN %. subtle anomalies. This leads to over-detection, resulting in slight performance drop on the VisA dataset. Dataset Method F1-max-cls AP-cls F1-max-seg AP-seg C. Qualitative results MVTec AD [64] VisA [65] AnomalyCLIP [52] AdaCLIP [38] VCP-CLIP [39] RareCLIP [67] MuSc [22] MuSc-V2 (Ours) AnomalyCLIP [52] AdaCLIP [38] VCP-CLIP [39] RareCLIP [67] MuSc [22] MuSc-V2 (Ours) 92.8 90.6 91.7 92.9 97.5 97.6 80.7 83.1 81.5 83.1 89.5 88.5 96.2 95.7 95.5 96.6 99.1 99. 85.4 87.6 87.3 89.0 93.5 92.8 39.1 43.4 49.4 47.5 62.6 63.1 28.3 37.7 34.7 33.5 48.8 50.4 34.5 41.6 49.1 46.1 62.7 63.5 21.3 31.1 30.1 27.0 45.1 46.1 When evaluated against 2D-focused zero-shot methods [52], [38], [39] on MVTec AD and VisA datasets (Table II), MuScV2 demonstrates significant improvements, even surpassing its previous version MuSc [22] across most metrics. However, some background regions on the VisA dataset contain subtle foreign impurities. Although these are not true anomalies, they are captured by the SNAMD module due to its sensitivity to We visualize the multimodal anomaly segmentation results in Fig. 8. Compared with other zero-shot methods, MuSc-V2 generates fewer false positives, e.g., cable gland and marshmallow. Our method also avoids false negatives common in multi-view rendering approaches like PointAD [21], particularly for objects with angles of view occlusion (foam). By detecting in 3D point cloud directly, we achieve more precise segmentation in complex cases (bagel, potato and gummybear). The 2D results in Fig. 9 demonstrate that MuSc-V2 reduces false positives (chocolate praline) and false negatives (confetto, peppermint candy), and yields finer results than its previous version in carrot and chocolate praline. D. Ablation study the IPG strategy: 1) Effectiveness of In Table III, we conduct experiments to validate the effectiveness of our IPG strategy. This brings 0.5% and 0.4% F1-max-cls gains on MVTec 3D-AD and Eyecandies datasets respectively. Since TABLE IV THE ABLATION OF SNAMD MODULE. WE REPORT THE ANOMALY CLASSIFICATION AND SEGMENTATION RESULTS. ALL METRICS ARE IN %. Setting F1-max-cls AP-cls F1-max-seg AP-seg MVTec 3D-AD LNA [55] LNA+SWPooling LNAMD [22] LNAMD+SWPooling SNAMD (r=1) SNAMD (r=3) SNAMD (r=5) SNAMD (w/o SWPooling) SNAMD (Ours) 92.1 92.2 92.9 92.2 92.2 92.2 92.3 93.1 93.0 Eyecandies LNA [55] LNA+SWPooling LNAMD [22] LNAMD+SWPooling SNAMD (r=1) SNAMD (r=3) SNAMD (r=5) SNAMD (w/o SWPooling) SNAMD (Ours) 75.2 81.3 77.6 81.1 82.3 81.3 80.9 79.4 82.9 94.7 96. 95.7 96.2 96.1 96.2 96.1 96.5 96.8 77.1 84.2 78.9 84.1 84.8 84.2 83.8 83.5 86.1 43.6 53. 47.3 52.9 52.8 53.0 53.0 52.1 54.6 33.6 44.0 36.8 44.1 44.4 44.0 43.8 40.4 44.7 39.3 52. 44.3 52.8 52.7 52.8 52.9 50.8 54.7 29.1 40.9 33.0 41.1 41.4 40.9 40.7 36.0 41.8 TABLE THE ABLATION OF FOUR IMPORTANT MODULES IN OUR MULTIMODAL MSM. WE REPORT THE AC AND AS RESULTS. ALL METRICS ARE IN %. Dataset Setting F1-max-cls AP-cls F1-max-seg AP-seg MVTec 3D-AD Eyecandies w/o IA w/o CAE w/o λ Ours w/o IA w/o CAE w/o λ Ours 92.4 91.7 92.4 93.0 80.7 81.9 82.2 82.9 96.1 96.0 96.7 96.8 83.6 85.4 85.9 86.1 52.5 52.5 54.4 54. 41.6 42.4 43.7 44.7 52.5 52.1 54.3 54.7 38.4 39.3 40.8 41.8 Visualization of anomaly segmentation on MVTec 3D-AD and Fig. 9. Eyecandies under 2D modal. All methods use ViT-L-14-336 extract features. TABLE III THE ABLATION OF THE IPG STRATEGY. WE REPORT THE ANOMALY CLASSIFICATION AND SEGMENTATION RESULTS. ALL METRICS ARE IN %. Dataset Setting F1-max-cls AP-cls F1-max-seg AP-seg MVTec 3D-AD Eyecandies w/o IPG IPG w/o IPG IPG 92.5 93.0 82.5 82.9 96.7 96.8 85.7 86.1 54.6 54.6 44.7 44. 54.6 54.7 41.8 41.8 local regions, the groups containing discontinuous surfaces typically occupy small their improvements on anomaly segmentation metrics are small (about 0.1%). However, it is effective for sample-level anomaly classification by reducing false positives in normal point clouds. 2) Discussion of the SNAMD module: In Table IV, we conduct ablation experiments on two main technologies SWPooling and multiple degrees in our SNAMD module. The results demonstrate that three aggregation degrees {1, 3, 5} outperform using only one aggregation degree. It brings 0.8% F1max-cls and 1.6% F1-max-seg improvements on the MVTec 3D-AD dataset, with consistent gains on Eyecandies. Removing SWPooling causes significant performance drops, particularly for the Eyecandies dataset with more small anomalies, where it reduce false negatives and brings 2.6% AP-cls and 5.8% AP-seg gains. When integrated into Local Neighborhood Aggregation (LNA) [55] and Local Neighborhood Aggregation with Multiple Degrees (LNAMD) [22], SWPooling also enhances their performance. 3) Discussion of the Multimodal Mutual Scoring: Ablation studies in Table evaluate four key components: Interval Average (IA), Cross-modal Anomaly Enhancement (CAE), and confidence weight λ. Without IA, normal regions with appearance variations receive higher scores from dissimilar patches, especially in Eyecandies where products have diverse sub-types. Our IA operation mitigates this issue, improving AP-cls by 2.5% and AP-seg by 3.4%. The CAE module reduces false negatives brought by single-modal invisible anomalies, boosting MVTec 3D-AD performance by 1.3% F1max-cls and 2.1% F1-max-seg. The confidence weight λ in our CAE further suppresses cross-modal false positives, enhancing both classification and segmentation. 4) Effectiveness of our RsCon module: In Table VI, we perform comprehensive ablation studies of our RsCon module across four datasets. The consistent improvement in AC metrics across all datasets validates the effectiveness and stability of our RsCon. Meanwhile, for the window mask operation (WMO) of RsCon, we analyze the window size sensitivity {2, ..., 9} through box plots in Fig.11 (a). Small intervals (Q1Q3) across four datasets show stable performance, which means that RsCon is not sensitive to the window size k, except for some extreme values. Notably, RsCon consistently outperforms baseline methods (red dot) regardless of k. Additionally, TABLE VI THE ABLATION OF THE RSCON MODULE ACROSS FOUR TRADITIONAL DATASETS. WE REPORT AC AND AS RESULTS. ALL METRICS ARE IN %. TABLE VII PER SAMPLE INFERENCE TIME OF OUR MUSC-V2 AND OTHER ZERO-SHOT METHODS. WE DIVIDE THE MVTEC 3D-AD DATASET INTO SUBSETS. Dataset Setting AUROC-cls F1-max-cls AP-cls Method Train F1-max-cls F1-max-seg Time (ms) MVTec 3D-AD Eyecandies MVTec AD VisA w/o RsCon RsCon w/o RsCon RsCon w/o RsCon RsCon w/o RsCon RsCon 86.5 88.1 83.1 83.9 94.8 95.3 86.9 88. 92.6 93.0 82.0 82.9 94.6 95.2 84.4 85.6 96.1 96.8 85.2 86. 97.8 98.1 87.2 88.4 Fig. 10. numbers across MVTec 3D-AD and Eyecandies datasets. Four anomaly segmentation metrics with different normal sample removing WMO (black dot) causes significant performance drops, confirming its critical role. 5) Influence of the normal sample number: To investigate the robustness of the normal sample number in the test set, we randomly reduce normal samples to 1 of the original set  (Fig. 10)  . With = 0 as the limiting case (no normals), AS metrics show minimal degradation across both datasets: AUROC decreases by less than 0.23% and PRO varies by less than 0.6%. The maximum observed drops in F1-max and AP are 2.94%, indicating only minor false-positive increases. These results demonstrate the insensitivity of our MuSc-V2 to normal sample counts, especially in real industrial scenes where normal samples typically dominate. 6) Effect of the dataset size: In our mutual scoring mechanism, we use {DOi} to assign scores to sample Oi. To explore the sensitivity to dataset size, we divide the unlabeled samples into {1, 2, 3} subsets. Each subset independently scores samples in this subset. After averaging results across 10 random seeds, Table VII shows minimal performance degradation: AC drops by less than 0.4% and AS declines by less than 1.0%. This demonstrates consistent effectiveness even with limited data. 7) Sensitivity analysis of hyperparameters: In Fig. 11, we conduct experiments on four important hyperparameters in our MuSc-V2. (a) The RsCons window size k. This hyperparameter is insensitive as we describe in Sec. IV-D4. (b) The MSMs interval average (IA) range X%. Larger values incorporate more normal patches with appearance variations, elevating scores and increasing false positives. The extreme case = 100 (vanilla average) shows the most severe performance degradation. While smaller may bring false negatives since abnormal patches may find few similar patches. For AdaCLIP[38] VCP-CLIP[39] MuSc[22] MuSc-V2(g=1) MuSc-V2(g=2) MuSc-V2(g=3) MuSc MuSc-V2(g=1) MuSc-V2(g=2) MuSc-V2(g=3) PointAD[21] MuSc-V2(g=1) MuSc-V2(g=2) MuSc-V2(g=3) PointAD[21] MuSc-V2(g=1) MuSc-V2(g=2) MuSc-V2(g=3) 2D modal (ViT-L-14-336) 89.9 89.9 90.3 90.3 90.3 (0.0) 90.0 (0.3) 41.7 42.5 41.0 47.1 46.8 (0.3) 46.9 (0.2) 2D modal (ViT-B-8) 90.0 90.5 90.3 (0.2) 90.2 (0.3) 31.9 40.4 40.1 (0.3) 39.7 (0.7) 3D modal 301.4 86.2 737.1 122.3 92.9 85.6 245.9 44.1 35.4 32. 92.3 92.5 92.3 (0.2) 92.2 (0.1) 30.7 45.9 45.6 (0.3) 45.4 (0.5) 337.1+30287.7 745.8 737.6 733.9 Multimodal 92.2 93.0 92.7 (0.3) 92.5 (0.4) 37.2 54.6 54.3 (0.3) 53.6 (1.0) 368.2+30287.7 969.6 943.7 934.4 Fig. 11. Experimental results of the influence of four hyperparameters on MuSc-V2. We report AP metrics on MVTec 3D-AD in (b), (c) and (d). an IA range X% < 50%, the impact remains minimal (APcls 0.09%, AP-seg 0.64%), demonstrating our methods robustness to moderate range selections. (c) The IPGs iterative increment Kiter. This parameter demonstrates strong point robustness around our default setting (Kiter = 80), with maximum variations of 0.07% for AP-seg and 0.09% for AP-cls. Performance degrades when Kiter increases beyond this range, as IPG degradates to traditional KNN. (d) The IPGs curvature threshold Cthr. This parameter determines which point groups undergo IPG processing. Higher values reduce the number of processed groups, leaving more groups containing potentially discontinuous surfaces, thus degrading performance. Near our default setting (0.01), varying Cthr causes minimal impact, where AP-seg and AP-cls fluctuate by less than 1.4%. Above analyses demonstrate the robustness of our methods four key hyperparameters within reasonable ranges. As trainingfree approach, maintaining hyperparameters within appropriate bounds is both inevitable and manageable. 8) Inference time: Table VII compares inference times on an NVIDIA RTX 3090 GPU (excluding I/O) across different zero-shot methods. Since the MVTec 3D-ADs product range is from 100 to 159, we use 150 samples for mutual scoring in MuSc-V2 and MuSc. Our MuSc-V2 outperforms its previous version in both accuracy and speed for 2D tasks. For 3D and multimodal cases, our directly point cloud processing proves significantly faster than PointAD [21], which requires more than 30s per sample for multi-view rendering. While subset partitioning further accelerates MuSc-V2, the 722.6ms feature extraction by Point-MAE [27] remains bottleneck. V. CONCLUSION In this paper, we present MuSc-V2, zero-shot framework for industrial anomaly classification and segmentation in multimodal data. This method leverages implicit normal/abnormal cues from the unlabeled samples. We propose four key innovations: (1) SNAMD modules for modeling anomalies with varying scales; (2) IPG modules for generating 3D groups with continuous surfaces and maintaining the normal representation consistency; (3) multimodal mutual scoring mechanism for scoring each sample patch; (4) RsCon for false classifications suppression. Experimental results demonstrate superior performance over existing zero-shot methods, with competitive advantages against few-shot approaches."
        },
        {
            "title": "REFERENCES",
            "content": "[1] Q. Chen, H. Luo, C. Lv, and Z. Zhang, unified anomaly synthesis strategy with gradient ascent for industrial anomaly detection and localization, in Eur. Conf. Comput. Vis., 2025. [2] Y. Cao, X. Xu, Z. Liu, and W. Shen, Collaborative discrepancy optimization for reliable image anomaly localization, IEEE Trans. Ind. Inform., 2023. [3] H. Zhang, Z. Wang, D. Zeng, Z. Wu, and Y.-G. Jiang, Diffusionad: Norm-guided one-step denoising diffusion for anomaly detection, IEEE Trans. Pattern Anal. Mach. Intell., 2025. [4] Q. Chen, H. Luo, H. Yao, W. Luo, Z. Qu, C. Lv, and Z. Zhang, Centeraware residual anomaly synthesis for multiclass industrial anomaly detection, IEEE Trans. Ind. Inform., 2025. [5] N. Madan, N.-C. Ristea, R. T. Ionescu, K. Nasrollahi, F. S. Khan, T. B. Moeslund, and M. Shah, Self-supervised masked convolutional transformer block for anomaly detection, IEEE Trans. Pattern Anal. Mach. Intell., 2023. [6] H. Li, J. Hu, B. Li, H. Chen, Y. Zheng, and C. Shen, Target before shooting: Accurate anomaly detection and localization under one millisecond via cascade patch retrieval, IEEE Trans. Image Process., 2024. [7] C. Qiu, M. Kloft, S. Mandt, and M. Rudolph, Self-supervised anomaly detection with neural transformations, IEEE Trans. Pattern Anal. Mach. Intell., 2024. [8] H. Yao, Y. Cao, W. Luo, W. Zhang, W. Yu, and W. Shen, Prior normality prompt transformer for multiclass industrial image anomaly detection, IEEE Trans. Ind. Inform., 2024. [9] G. Xie, J. Wang, J. Liu, F. Zheng, and Y. Jin, Pushing the limits of fewshot anomaly detection in industry vision: Graphcore, in Int. Conf. Learn. Represent., 2023. 12 [11] J. Su, H. Shen, L. Peng, and D. Hu, Few-shot domain-adaptive anomaly detection for cross-site brain images, IEEE Trans. Pattern Anal. Mach. Intell., 2021. [12] J. Zhu and G. Pang, Toward generalist anomaly detection via in-context residual learning with few-shot sample prompts, in IEEE Conf. Comput. Vis. Pattern Recog., 2024. [13] X. Li, Z. Zhang, X. Tan, C. Chen, Y. Qu, Y. Xie, and L. Ma, Promptad: Learning prompts with only normal samples for few-shot anomaly detection, in IEEE Conf. Comput. Vis. Pattern Recog., 2024. [14] C. Huang, A. Jiang, J. Feng, Y. Zhang, X. Wang, and Y. Wang, Adapting visual-language models for generalizable anomaly detection in medical images, in IEEE Conf. Comput. Vis. Pattern Recog., 2024. [15] J. Jeong, Y. Zou, T. Kim, D. Zhang, A. Ravichandran, and O. Dabeer, Winclip: Zero-/few-shot anomaly classification and segmentation, in IEEE Conf. Comput. Vis. Pattern Recog., 2023. [16] X. Chen, Y. Han, and J. Zhang, zero-/few-shot anomaly classification and segmentation method for cvpr 2023 vand workshop challenge tracks 1&2: 1st place on zero-shot ad and 4th place on few-shot ad, arXiv preprint arXiv:2305.17382, 2023. [17] A. Li, C. Qiu, M. Kloft, P. Smyth, M. Rudolph, and S. Mandt, Zeroshot anomaly detection via batch normalization, in Adv. Neural Inform. Process. Syst., 2023. [18] Y. Wang, J. Peng, J. Zhang, R. Yi, Y. Wang, and C. Wang, Multimodal industrial anomaly detection via hybrid fusion, in IEEE Conf. Comput. Vis. Pattern Recog., 2023. [19] E. Horwitz and Y. Hoshen, Back to the feature: classical 3d features are (almost) all you need for 3d anomaly detection, in IEEE Conf. Comput. Vis. Pattern Recog., 2023. [20] C. Wang, H. Zhu, J. Peng, Y. Wang, R. Yi, Y. Wu, L. Ma, and J. Zhang, M3dm-nr: Rgb-3d noisy-resistant industrial anomaly detection via multimodal denoising, IEEE Trans. Pattern Anal. Mach. Intell., 2025. [21] Q. Zhou, J. Yan, S. He, W. Meng, and J. Chen, Pointad: Comprehending 3d anomalies from points and pixels for zero-shot 3d anomaly detection, Adv. Neural Inform. Process. Syst., 2024. [22] X. Li, Z. Huang, F. Xue, and Y. Zhou, Musc: Zero-shot industrial anomaly classification and segmentation with mutual scoring of the unlabeled images, in Int. Conf. Learn. Represent., 2024. [23] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly et al., An image is worth 16x16 words: Transformers for image recognition at scale, in Int. Conf. Learn. Represent., 2020. [24] H. Zhao, L. Jiang, J. Jia, P. H. Torr, and V. Koltun, Point transformer, in Int. Conf. Comput. Vis., 2021. [25] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark et al., Learning transferable visual models from natural language supervision, in Int. Conf. Mach. Learn., 2021. [26] M. Caron, H. Touvron, I. Misra, H. Jegou, J. Mairal, P. Bojanowski, and A. Joulin, Emerging properties in self-supervised vision transformers, in Int. Conf. Comput. Vis., 2021. [27] Y. Pang, W. Wang, F. E. Tay, W. Liu, Y. Tian, and L. Yuan, Masked autoencoders for point cloud self-supervised learning, in Eur. Conf. Comput. Vis., 2022. [28] X. Yu, L. Tang, Y. Rao, T. Huang, J. Zhou, and J. Lu, Point-bert: Pretraining 3d point cloud transformers with masked point modeling, in IEEE Conf. Comput. Vis. Pattern Recog., 2022. [29] Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, and B. Guo, Swin transformer: Hierarchical vision transformer using shifted windows, in Int. Conf. Comput. Vis., 2021. [30] H. Ding, C. Liu, S. Wang, and X. Jiang, Vlt: Vision-language transformer and query generation for referring segmentation, IEEE Trans. Pattern Anal. Mach. Intell., 2022. [31] C. R. Qi, H. Su, K. Mo, and L. J. Guibas, Pointnet: Deep learning on point sets for 3d classification and segmentation, in IEEE Conf. Comput. Vis. Pattern Recog., 2017. [32] Q. Zhang, J. Hou, Y. Qian, Y. Zeng, J. Zhang, and Y. He, Flatteningnet: Deep regular 2d representation for 3d point cloud analysis, IEEE Trans. Pattern Anal. Mach. Intell., 2023. [33] S. Chen, H. Zhu, M. Li, X. Chen, P. Guo, Y. Lei, G. Yu, T. Li, and T. Chen, Vote2cap-detr++: Decoupling localization and describing for end-to-end 3d dense captioning, IEEE Trans. Pattern Anal. Mach. Intell., 2024. [34] M.-H. Guo, J.-X. Cai, Z.-N. Liu, T.-J. Mu, R. R. Martin, and S.-M. Hu, Pct: Point cloud transformer, Comput. Vis. Media, 2021. [10] S. Ma, K. Song, M. Niu, H. Tian, Y. Wang, and Y. Yan, Shapeconsistent one-shot unsupervised domain adaptation for rail surface defect segmentation, IEEE Trans. Ind. Inform., 2023. [35] B. Wang, Z. Tian, A. Ye, F. Wen, S. Du, and Y. Gao, Generative variational-contrastive learning for self-supervised point cloud representation, IEEE Trans. Pattern Anal. Mach. Intell., 2024. 13 [63] L. Bonfiglioli, M. Toschi, D. Silvestri, N. Fioraio, and D. De Gregorio, The eyecandies dataset for unsupervised multimodal anomaly detection and localization, in Asian Conf. Comput. Vis., 2022. [64] P. Bergmann, M. Fauser, D. Sattlegger, and C. Steger, Mvtec ada comprehensive real-world dataset for unsupervised anomaly detection, in IEEE Conf. Comput. Vis. Pattern Recog., 2019. [65] Y. Zou, J. Jeong, L. Pemula, D. Zhang, and O. Dabeer, Spot-thedifference self-supervised pre-training for anomaly detection and segmentation, in Eur. Conf. Comput. Vis., 2022. [66] K. Mao, P. Wei, Y. Lian, Y. Wang, and N. Zheng, Beyond single-modal boundary: Cross-modal anomaly detection through visual prototype and harmonization, in IEEE Conf. Comput. Vis. Pattern Recog., 2025. [67] J. He, M. Cao, S. Peng, and Q. Xie, Rareclip: Rarity-aware online zeroshot industrial anomaly detection, in Int. Conf. Comput. Vis., 2025. [36] X. Wu, Y. Lao, L. Jiang, X. Liu, and H. Zhao, Point transformer v2: Grouped vector attention and partition-based pooling, Adv. Neural Inform. Process. Syst., 2022. [37] X. Wu, L. Jiang, P.-S. Wang, Z. Liu, X. Liu, Y. Qiao, W. Ouyang, T. He, and H. Zhao, Point transformer v3: Simpler faster stronger, in IEEE Conf. Comput. Vis. Pattern Recog., 2024. [38] Y. Cao, J. Zhang, L. Frittoli, Y. Cheng, W. Shen, and G. Boracchi, Adaclip: Adapting clip with hybrid learnable prompts for zero-shot anomaly detection, in Eur. Conf. Comput. Vis., 2024. [39] Z. Qu, X. Tao, M. Prasad, F. Shen, Z. Zhang, X. Gong, and G. Ding, Vcp-clip: visual context prompting model for zero-shot anomaly segmentation, in Eur. Conf. Comput. Vis., 2024. [40] Y. Li, A. Goodge, F. Liu, and C.-S. Foo, Promptad: Zero-shot anomaly detection using text prompts, in Winter Conf. Appl. Comput. Vis., 2024. [41] Z. Gu, B. Zhu, G. Zhu, Y. Chen, H. Li, M. Tang, and J. Wang, Filo: Zero-shot anomaly detection by fine-grained description and highquality localization, in ACM Int. Conf. Multimedia, 2024. [42] T. Aota, L. T. T. Tong, and T. Okatani, Zero-shot versus many-shot: Unsupervised texture anomaly detection, in Winter Conf. Appl. Comput. Vis., 2023. [43] Z. Zhou, L. Wang, N. Fang, Z. Wang, L. Qiu, and S. Zhang, R3d-ad: Reconstruction via diffusion for 3d anomaly detection, in Eur. Conf. Comput. Vis., 2025. [44] W. Li, X. Xu, Y. Gu, B. Zheng, S. Gao, and Y. Wu, Towards scalable 3d anomaly detection and localization: benchmark via 3d anomaly synthesis and self-supervised learning network, in IEEE Conf. Comput. Vis. Pattern Recog., 2024. [45] R. Chen, G. Xie, J. Liu, J. Wang, Z. Luo, J. Wang, and F. Zheng, Easynet: An easy network for 3d industrial anomaly detection, in ACM Int. Conf. Multimedia, 2023. [46] Y.-M. Chu, C. Liu, T.-I. Hsieh, H.-T. Chen, and T.-L. Liu, Shape-guided dual-memory learning for 3d anomaly detection, in Int. Conf. Mach. Learn., 2023. [47] A. Costanzino, P. Z. Ramirez, G. Lisanti, and L. Di Stefano, Multimodal industrial anomaly detection by crossmodal feature mapping, in IEEE Conf. Comput. Vis. Pattern Recog., 2024. [48] D. Zhou, J. Weston, A. Gretton, O. Bousquet, and B. Scholkopf, Ranking on data manifolds, Adv. Neural Inform. Process. Syst., 2003. [49] T. Lin and H. Zha, Riemannian manifold learning, IEEE Trans. Pattern Anal. Mach. Intell., 2008. [50] B. Wang and Z. Tu, Affinity learning via self-diffusion for image segmentation and clustering, in IEEE Conf. Comput. Vis. Pattern Recog., 2012. [51] Z. Zhang, J. Wang, and H. Zha, Adaptive manifold learning, IEEE Trans. Pattern Anal. Mach. Intell., 2011. [52] Q. Zhou, G. Pang, Y. Tian, S. He, and J. Chen, Anomalyclip: Objectagnostic prompt learning for zero-shot anomaly detection, in Int. Conf. Learn. Represent., 2024. [53] Y. Eldar, M. Lindenbaum, M. Porat, and Y. Y. Zeevi, The farthest point strategy for progressive image sampling, IEEE Trans. Image Process., 1997. [54] M. P. Do Carmo, Differential geometry of curves and surfaces: revised and updated second edition. Courier Dover Publications, 2016. [55] K. Roth, L. Pemula, J. Zepeda, B. Scholkopf, T. Brox, and P. Gehler, Towards total recall in industrial anomaly detection, in IEEE Conf. Comput. Vis. Pattern Recog., 2022. [56] J. Jiang, B. Wang, and Z. Tu, Unsupervised metric learning by selfsmoothing operator, in Int. Conf. Comput. Vis., 2011. [57] P. Bergmann, X. Jin, D. Sattlegger, and C. Steger, The mvtec 3d-ad dataset for unsupervised 3d anomaly detection and localization, in Int. Conf. Comput. Vis. Theor. Appl., 2021. [58] J. Zhu, Y.-S. Ong, C. Shen, and G. Pang, Fine-grained abnormality prompt learning for zero-shot anomaly detection, in Int. Conf. Comput. Vis., 2025. [59] X. Zhu, R. Zhang, B. He, Z. Guo, Z. Zeng, Z. Qin, S. Zhang, and P. Gao, Pointclip v2: Prompting clip and gpt for powerful 3d openworld learning, in Int. Conf. Comput. Vis., 2023. [60] L. Xue, M. Gao, C. Xing, R. Martın-Martın, J. Wu, C. Xiong, R. Xu, J. C. Niebles, and S. Savarese, Ulip: Learning unified representation of language, images, and point clouds for 3d understanding, in IEEE Conf. Comput. Vis. Pattern Recog., 2023. [61] L. Xue, N. Yu, S. Zhang, A. Panagopoulou, J. Li, R. Martın-Martın, J. Wu, C. Xiong, R. Xu, J. C. Niebles et al., Ulip-2: Towards scalable multimodal pre-training for 3d understanding, in IEEE Conf. Comput. Vis. Pattern Recog., 2024. [62] Y. Wang, K.-C. Peng, and Y. Fu, Towards zero-shot 3d anomaly localization, in Winter Conf. Appl. Comput. Vis., 2025."
        }
    ],
    "affiliations": [
        "School of Computer Science, University of Trento",
        "School of Electronic Information and Communications, Huazhong University of Science and Technology"
    ]
}