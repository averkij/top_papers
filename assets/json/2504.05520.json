{
    "paper_title": "Efficient Reinforcement Finetuning via Adaptive Curriculum Learning",
    "authors": [
        "Taiwei Shi",
        "Yiyang Wu",
        "Linxin Song",
        "Tianyi Zhou",
        "Jieyu Zhao"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Reinforcement finetuning (RFT) has shown great potential for enhancing the mathematical reasoning capabilities of large language models (LLMs), but it is often sample- and compute-inefficient, requiring extensive training. In this work, we introduce AdaRFT (Adaptive Curriculum Reinforcement Finetuning), a method that significantly improves both the efficiency and final accuracy of RFT through adaptive curriculum learning. AdaRFT dynamically adjusts the difficulty of training problems based on the model's recent reward signals, ensuring that the model consistently trains on tasks that are challenging but solvable. This adaptive sampling strategy accelerates learning by maintaining an optimal difficulty range, avoiding wasted computation on problems that are too easy or too hard. AdaRFT requires only a lightweight extension to standard RFT algorithms like Proximal Policy Optimization (PPO), without modifying the reward function or model architecture. Experiments on competition-level math datasets-including AMC, AIME, and IMO-style problems-demonstrate that AdaRFT significantly improves both training efficiency and reasoning performance. We evaluate AdaRFT across multiple data distributions and model sizes, showing that it reduces the number of training steps by up to 2x and improves accuracy by a considerable margin, offering a more scalable and effective RFT framework."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 ] . [ 1 0 2 5 5 0 . 4 0 5 2 : r a"
        },
        {
            "title": "Efficient Reinforcement Finetuning via\nAdaptive Curriculum Learning",
            "content": "Taiwei Shi, Yiyang Wu, Linxin Song, Tianyi Zhou, Jieyu Zhao University of Southern California, University of Maryland, College Park {taiweish, wuwendy, linxinso, jieyuz}@usc.edu, tianyi@umd.edu Code: github.com/uscnlp-lime/verl Dataset: huggingface.co/datasets/lime-nlp/DeepScaleR_Difficulty"
        },
        {
            "title": "Abstract",
            "content": "Reinforcement finetuning (RFT) has shown great potential for enhancing the mathematical reasoning capabilities of large language models (LLMs), but it is often sampleand compute-inefficient, requiring extensive training. In this work, we introduce ADARFT (Adaptive Curriculum Reinforcement Finetuning), method that significantly improves both the efficiency and final accuracy of RFT through adaptive curriculum learning. ADARFT dynamically adjusts the difficulty of training problems based on the models recent reward signals, ensuring that the model consistently trains on tasks that are challenging but solvable. This adaptive sampling strategy accelerates learning by maintaining an optimal difficulty range, avoiding wasted computation on problems that are too easy or too hard. ADARFT requires only lightweight extension to standard RFT algorithms like Proximal Policy Optimization (PPO), without modifying the reward function or model architecture. Experiments on competition-level math datasetsincluding AMC, AIME, and IMO-style problemsdemonstrate that ADARFT significantly improves both training efficiency and reasoning performance. We evaluate ADARFT across multiple data distributions and model sizes, showing that it reduces the number of training steps by up to 2 and improves accuracy by considerable margin, offering more scalable and effective RFT framework."
        },
        {
            "title": "Introduction",
            "content": "Reinforcement Finetuning (RFT) has emerged as powerful technique for aligning large language models (LLMs) with task-specific goals, particularly in domains such as mathematics and code generation where correctness is well defined (DeepSeek-AI et al., 2025; OpenAI et al., 2024). By optimizing policy model with reward signals that reflect task success, RFT enables more targeted learning than supervised finetuning (SFT) alone. However, despite its promise, RFT remains sampleinefficient and computationally expensive. Its training involves repeated rollout generation, reward computation, and policy updatesmaking it costly and difficult to scale (Ahmadian et al., 2024; Kazemnejad et al., 2024; Li et al., 2024; Hu, 2025; Cui et al., 2025). Recent efforts to address RFT inefficiency have focused on algorithmic simplification (e.g., RAFT (Dong et al., 2023), GRPO (DeepSeek-AI et al., 2025), ReMax (Li et al., 2024)), and data-centric strategies (e.g., LIMO (Ye et al., 2025), LIMR (Li et al., 2025)). While these approaches improve sample or compute efficiency, they often introduce trade-offs: algorithmic simplifications may increase variance or limit stability, and static data filtering or scoring can be brittle, computationally heavy, or model-specific. Moreover, most methods success relies on fixed datasets or training schedules, which can be suboptimal in non-uniform or imbalanced data regimes. Preprint. To overcome these limitations, we propose ADARFT, reinforcement finetuning method based on adaptive curriculum learning (Bengio et al., 2009), which dynamically adjusts training set difficulty to match the models evolving skill level. The algorithm is shown in Alg. 1. To illustrate the motivation, consider asking grade school student to solve problem from the International Mathematical Olympiad (IMO). Without the necessary background and tools, the student would struggle to engage meaningfully with the task, let alone learn from it. Inspired by this analogy, we hypothesize that LLMs trained via RFT may face similar challenge: when presented with tasks that exceed their current capabilities, the learning signal becomes weak or ineffective. On the other hand, tasks that are too simple may not sufficiently challenge the model to improve. In both scenarios, mismatch between task difficulty and model ability leads to inefficient learning. ADARFT addresses this challenge by dynamically selecting training problems that match the models current skill level. Each problem in the dataset is preassigned difficulty score, which can be either estimated automatically or derived from human annotations. During training, we maintain target difficulty level that is adjusted based on the models recent reward signal: as the model improves, the target difficulty increases; if performance drops, it decreases. At each step, the model is finetuned on problems whose difficulty is closest to this target, ensuring steady progression through increasingly challenging but solvable tasks. Unlike prior work that simplifies the RL algorithm or precomputes model-specific learning-impact metrics, ADARFT adds only lightweight curriculum scheduler on top of standard RFT algorithms like Proximal Policy Optimization (PPO) (Schulman et al., 2017), with no modification to reward functions and no model-dependent preprocessing. While ADARFT is compatible with variety of policy optimization methods (e.g., PPO, GRPO), in this work, we instantiate it using PPO for all experiments to demonstrate its effectiveness. We denote this specific instantiation as ADARFT (PPO). We evaluate ADARFT on dataset spanning wide range of competition-level math problems, including AMC, AIME, and IMO-style questions. Across multiple training distributions and two model sizes, adaptive curriculum learning significantly improves both training efficiency and final performance. Gains are especially notable in imbalanced data regimes, where static sampling often fails. ADARFT can speed up training by up to 2, offering practical and scalable path to more efficient RFT in structured reasoning tasks."
        },
        {
            "title": "2 Related Work",
            "content": "Efficient Reinforcement Finetuning. Most RFT pipelines are based on Proximal Policy Optimization (PPO) (Schulman et al., 2017), with recent variants such as RAFT (Dong et al., 2023), GRPO (DeepSeek-AI et al., 2025), REINFORCE++ (Hu, 2025), and ReMax (Li et al., 2024) aiming to reduce computational cost by simplifying the underlying RL algorithm. These methods generally improve efficiency and stability, but often introduce trade-offs. For example, GRPO eliminates the value function by estimating advantages through relative comparisons within groups of outputs. While this removes the need for critic and reduces complexity, it can lead to instability due to noisier advantage estimates, higher variance in updates, and increased sample requirements (DeepSeek-AI et al., 2025; Hu, 2025). In parallel, data-centric strategies have emerged as promising alternatives for efficient finetuning. Techniques like LIMO (Ye et al., 2025) and s1 (Muennighoff et al., 2025) demonstrate that small, carefully curated supervised finetuning datasets can achieve strong performance while using orders of magnitude less data. However, they rely heavily on effective data selection and prompt design, which may hinder generalization to new tasks. LIMR (Li et al., 2025) also aims to reduce RFT sample requirements by scoring training examples using learning impact measurement (LIM). Although this allows subsequent training with fewer samples, computing LIM scores requires full training run on the entire dataset. Moreover, despite using fewer samples afterwards, models still take roughly the same number of steps to converge, limiting the overall efficiency gains. LIMR is also model-dependent, meaning that LIM scores must be recomputed when switching to different model, further increasing overhead. In contrast to these approaches, ADARFT introduces an adaptive curriculum learning strategy that adaptively selects training problems based on the models current performance. Rather than simplifying the RL algorithm or relying on static data selection, we continuously adjust the training difficulty to match the models evolving capability. This curriculum-aware training leads to faster convergence, improved accuracy, and higher sample and compute efficiency, especially in non-uniform training distributions, while maintaining stability and generalization across tasks. 2 Curriculum Learning for RL. Curriculum learning (CL) (Bengio et al., 2009) is training strategy that structures learning by presenting training samples in meaningful progression, typically from simpler to more difficult tasks. In RL, task-specific curricula sort tasks based on predefined or learned difficulty metrics to guide agents through progressively harder challenges (Zaremba & Sutskever, 2015; Justesen et al., 2018; Wang et al., 2019). Teacher-guided curricula treat task selection as bandit or partially observable Markov decision process problem (POMDP), where teacher agent adaptively assigns tasks that maximize the students learning progress (Matiisen et al., 2017; Portelas et al., 2019). Self-play methods generate an automatic curriculum by having agents propose and solve tasks for each other, dynamically increasing task difficulty through competition (Sukhbaatar et al., 2018). Goal generation techniques, such as Goal GANs, produce goals of intermediate difficulty based on current policy performance to optimize learning in sparse-reward settings (Florensa et al., 2018). Skill-based curricula organize unsupervised trajectories into latent skill space to create transferable training tasks for meta-RL (Jabri et al., 2019). Distillation-based approaches, like progressive networks (Rusu et al., 2022) and Mix & Match (Czarnecki et al., 2018), transfer knowledge from simpler agents to more complex ones through shared architectures and imitation losses. These diverse approaches demonstrate the value of structured task progression in improving generalization and sample efficiency across complex RL domains. While curriculum learning has been widely studied in classical RL, its application to RFT of LLMs remains largely unexplored. Our work is among the first to introduce adaptive curriculum learning in the RFT setting, adapting problem difficulty in real-time based on model performance. This approach improves both sample efficiency and final performance, demonstrating that curriculum learning is powerful tool for scaling RFT on complex reasoning tasks."
        },
        {
            "title": "3 ADARFT",
            "content": "We aim to improve the performance of policy model πθ for solving mathematical problems through adaptive curriculum learning. Fine-tuning on problems that are too easy or too hard leads to poor learning outcomes. Instead, the model should be trained on problems whose difficulty is close to the models current capability. We frame this as an adaptive curriculum learning problem and propose ADARFT, which adaptively adjusts the target difficulty to keep training problems within suitable difficulty range. ADARFT is compatible with variety of RL algorithms (e.g, GRPO, PPO); in this work, we instantiate it with PPO and refer to this variant as ADARFT (PPO). Let be dataset of mathematical problems, each annotated with precomputed difficulty score di. For example, the MATH dataset (Hendrycks et al., 2021) includes preassigned difficulty levels ranging from 1 to 5. These difficulty scores can also be derived through other methods. One common approach is to use human expert annotations; for instance, problems from the International Mathematical Olympiad (IMO) are generally considered more difficult than those from the American Mathematics Competitions (AMC). Alternatively, difficulty can be estimated based on the models empirical success rate: problems that are frequently solved correctly receive lower difficulty scores, while problems that are rarely solved correctly are given higher scores. Another approach is to train separate model, such as regression or classification model, to estimate difficulty from features of the problem. Given predefined difficulty for each problem, the objective is to train policy πθ that improves its problem-solving ability by dynamically adjusting the training curriculum according to the models current performance. Our proposed algorithm, ADARFT, is shown in Algorithm 1."
        },
        {
            "title": "3.1 Dynamic Curriculum Sampling",
            "content": "To construct an adaptive curriculum, we define target difficulty , which represents the current target difficulty level for training. ADARFT dynamically adjusts based on the models reward signal to maintain an optimal difficulty level for learning. At each training step (see Algorithm 1), the algorithm computes the absolute difference between the target difficulty and the difficulty of each problem in the dataset (Step 1.4): = di for all D. The batch of training problems is formed by selecting the problems with the smallest values of (Step 1.5), producing batch: = {s1, s2, . . . , sB}. This ensures that the selected problems are closest to the models current target difficulty, focusing the learning process on problems that are neither too easy nor too hard. 3 Algorithm 1 ADARFT Adaptive Curriculum Reinforcement Finetuning 1: Input: Data source with difficulty scores {di}, policy model πθ, reward function R(, ), batch size B, initial target difficulty , step size η, sensitivity α, target reward β, difficulty bounds dmin, dmax 2: Select RL algorithm (e.g., PPO, GRPO, REINFORCE++) 3: while training is not finished do 4: 5: 6: 7: 8: 9: Compute absolute differences from target difficulty: = di Sort and select top samples closest to target difficulty: {s1, s2, . . . , sB} Generate responses using policy model: = πθ(X) Compute average reward: Ravg 1 Update policy: πθ A(πθ, X, G, R) Update and clip target difficulty: i=1 R(Xi, Gi) (cid:80)X clip(T + η tanh(α (Ravg β)), dmin, dmax) Update sampler: 10: 11: end while"
        },
        {
            "title": "3.2 Policy Update",
            "content": "The selected batch is used to train the policy model πθ, which generates responses (Step 1.6): = πθ(X). reward signal is then computed based on the correctness of the models output (Step 1.7): Ri = (cid:26)1, 0, if the response is correct if the response is incorrect The average reward over the batch is computed as (Step 1.7): Ravg = 1 (cid:80)X i=1 R(Xi, Gi). The policy can then be updated using reinforcement learning algorithm such as PPO, GRPO, or REINFORCE++ (Step 1.8): πθ A(πθ, X, G, R)."
        },
        {
            "title": "3.3 Target Difficulty Update",
            "content": "To adapt the curriculum dynamically, the target difficulty is updated based on the average reward. If the model performs well on the current difficulty level (high reward), the target difficulty increases, making the training problems harder. Conversely, if the model performs poorly, the target difficulty decreases. This dynamic update mechanism lies at the core of ADARFTs curriculum adaptation strategy. The update rule (Step 1.9) is defined as: = clip(T + η tanh(α (Ravg β)), dmin, dmax) Here, η, α, β are hyperparameters: η is the step size for adjusting the target difficulty, α controls the sensitivity of the update, and β is the target reward level, representing the desired success rate. The tanh function ensures smooth updates and prevents large jumps in difficulty by saturating for large deviations, while the clip function constrains the target difficulty within the valid range [dmin, dmax]. These bounds can be manually specified or automatically derived from the training set, for example by taking the minimum and maximum of the difficulty scores {di}. Intuition and guidance for selecting these hyperparameters are discussed in Section 4.3."
        },
        {
            "title": "4.1 Difficulty Estimation",
            "content": "Accurate estimation of problem difficulty is critical for ADARFT. For difficulty estimation, we use the Qwen 2.5 MATH 7B model (Qwen et al., 2025).1 model that is too strong (e.g., OpenAI o1 1https://huggingface.co/Qwen/Qwen2.5-Math-7B 4 (a) Average confidence that subsampled difficulty estimates fall within 0.05 of the full-sample estimate, as function of sample size. (b) Correlation between average solved percentage and labeled difficulty level in the MATH dataset. Figure 1: Evaluation of difficulty estimation: (a) Stability of difficulty scores under subsampling of model rollouts; (b) Correlation between labeled difficulty levels and average solved percentage in the MATH dataset, validating the reliability of difficulty annotations. (OpenAI et al., 2024), DeepSeek R1 (DeepSeek-AI et al., 2025)) would solve most problems on the first attempt, leading to poor discrimination between easy and hard problems. Conversely, model that is too weak (e.g., LLaMA 3.3 1B (Grattafiori et al., 2024)) would fail to solve most problems even after multiple attempts, limiting the signal required for curriculum adaptation. We select Qwen 2.5 MATH 7B as the estimator because it demonstrates balanced solving ability it can solve moderately difficult problems while still failing on more complex problems, thereby providing more informative difficulty signal. For each problem, the difficulty score is computed as: di = 100 (1 number of successful attempts on problem ), where is the number of attempts allowed for the model. In our setup, we use = 128. This approach ensures that difficulty scores reflect the true problem-solving capability of moderately capable model, preventing overor under-estimation of difficulty. To evaluate the stability of our difficulty estimation process, we simulate how confidence varies with different numbers of samples. For each problem, we treat the full set of 128 rollouts as the groundtruth difficulty estimate and compute how often sub-sampled estimates fall within tolerance of ϵ = 0.05. Specifically, we run 10 random sampling trials per sample size and average the confidence across all problems in the dataset. As shown in Figure 1a, even with as few as 64 samples, the estimated difficulty remains within 0.05 of the full estimate over 90% of the time. With just 40 samples, the confidence remains around 80%. These results indicate that accurate and robust difficulty estimation can be achieved with significantly fewer rollouts, reducing the computational burden of large-scale curriculum construction. To further validate the reliability of our difficulty estimates, we examined their alignment with the human-labeled difficulty levels provided in the MATH dataset. The MATH dataset comprises 12,500 competition-level mathematics problems sourced from contests such as the AMC 10, AMC 12, and AIME. Each problem is categorized into one of five difficulty levels, following the classification system used by the Art of Problem Solving (AoPS) community.2 In this dataset, subjects easiest problems are assigned difficulty level of 1, while the hardest problems are assigned difficulty level of 5. For instance, the initial problems in an AMC 8 exam are often labeled as level 1, whereas AIME problems are typically labeled as level 5. As shown in Figure 1b, there is clear downward trend in average solved percentage as the labeled difficulty level increases, ranging from 86.0% at level 1 to 52.7% at level 5. This strong negative correlation indicates that the models empirical performance is consistent with the intended difficulty stratification, reinforcing the usefulness of both the labeled difficulty levels and our estimation approach for guiding curriculum learning. 2https://artofproblemsolving.com/wiki/index.php/AoPS_Wiki%3ACompetition_ratings"
        },
        {
            "title": "4.2 Dataset",
            "content": "We use the DeepScaleR dataset (Luo et al., 2025) as the training set. DeepScaleR compiles problems from multiple sources, including the American Invitational Mathematics Examination (AIME) from 1984 to 2023 and the American Mathematics Competitions (AMC) prior to 2023. In addition, the dataset includes problems from the Omni-MATH (Gao et al., 2024) and Still datasets (Team, 2025), which feature problems from various national and international math competitions. This results in diverse and challenging training set, covering wide range of mathematical domains and difficulty levels. In practice, we do not have control over the exact difficulty distribution of the data collected for training. This motivates our investigation into how different difficulty distributions influence ADARFT. To this end, we construct three distinct distributions from the DeepScaleR dataset. The first is skewdifficult distribution, where most problems are challenging. The second is skew-easy distribution, where most problems are relatively easy. The third is uniform distribution, where problems are evenly balanced across all difficulty levels, ensuring consistent representation of easy, moderate, and hard problems. Each of these three distributions includes 10,000 samples. The data distribution for each setting is shown in Figure 2. For evaluation, we use six benchmark datasets to assess the models performance across different levels of difficulty and mathematical reasoning. The first benchmark, MATH 500 (Lightman et al., 2023), is subset of the MATH dataset (Hendrycks et al., 2021) containing 500 representative problems designed to test models general mathematical capability. GSM8K (Cobbe et al., 2021) is set of grade-school math problems. OlympiadBench (He et al., 2024) includes collection of problems from Olympiad-level mathematics and physics competitions. Minerva Math (Lewkowycz et al., 2022) is curated set of undergraduate-level math problems that assess complex mathematical reasoning and symbolic manipulation. AMC 23 and AIME 24 include problems from the 2023 American Mathematics Competitions and the 2024 American Invitational Mathematics Examination, respectively. Since AMC 23 contains only 40 problems and AIME 24 only 30, we report accuracy as the average over 8 sampled responses per problem to ensure stable estimates. Together, these datasets span elementary, high school, and advanced competition-level math, providing comprehensive evaluation of the models reasoning abilities. Figure 2: Difficulty distribution for different training sets: Uniform, Skew-Difficult, and Skew-Easy. Each training set contains 10,000 samples."
        },
        {
            "title": "4.3 Training Setup",
            "content": "We trained two models on the three difficulty-based distributions of the DeepScaleR dataset described in Section 4.2: Qwen 2.5 7B3 and Qwen 2.5 MATH 1.5B4. This setup allows us to evaluate the effectiveness of ADARFT on models with different initial performance levels when exposed to skew-hard, skew-easy, and uniform problem distributions. Both models were trained using two approaches: the standard PPO algorithm and ADARFT (PPO), our method that integrates adaptive curriculum learning with PPO (see Section 3). For ADARFT, the target difficulty was dynamically adjusted throughout training based on the models reward signal. This adjustment ensured that the model consistently encountered problems that matched its current skill level, preventing the model from being overwhelmed by overly difficult problems or stagnating on problems that were too easy. 3https://huggingface.co/Qwen/Qwen2.5-7B 4https://huggingface.co/Qwen/Qwen2.5-Math-1.5B 6 The training batch size was set to = 1024, with the target reward β set to 0.5 to promote learning at balanced success rate. The sensitivity parameter α and step size η were tuned using validation set to ensure stable curriculum updates. We set α = 2, η = 50, and the initial target difficulty = 0. The step size η acts as scaling factor between the reward signal and the difficulty metric. Since the difficulty metric ranges from 0 to 100 and the reward ranges from 0 to 1, target reward β = 0.5 implies that the maximum reasonable adjustment to the difficulty metric should be around 50. Therefore, we set η = 50 to scale the reward signal appropriately to the difficulty range. The sensitivity parameter α = 2 controls the slope of the tanh function. Setting α to 2 makes the tanh function behave approximately linearly when the difference between the average reward and the target reward is small. The intuition behind using the tanh function is that when the average reward is close to the target reward, roughly linear adjustment is appropriate. However, when the average reward deviates significantly from the target reward, linear adjustments may be too large, leading to instability. The tanh function smooths out these adjustments, allowing for more controlled changes when the difference is large while maintaining sensitivity when the difference is small. Both models were trained on 8 A100 GPUs for approximately 100 steps. The implementation details can be found in Appendix A."
        },
        {
            "title": "5 Result",
            "content": "We evaluate the performance of standard PPO and ADARFT (PPO) across multiple training setups and two model sizes: Qwen 2.5 MATH 1.5B and Qwen 2.5 7B. Figure 3 presents the learning curves averaged across six benchmarks, while Table 1 and 2 provide detailed breakdown of accuracy and training efficiency at step 60 and step 100. On average, models trained with ADARFT (PPO) outperform their PPO-only counterparts in both final accuracy and training efficiency. This improvement is particularly notable in non-uniform data distributions, where curriculum adaptation is most beneficial. Figure 3: Performance comparison of PPO and ADARFT (PPO) across different setups (uniform, skew-easy, skew-difficult) for Qwen 2.5 Math 1.5B and Qwen 2.5 7B models. Accuracy is the average of MATH 500, GSM8K, AIME 24, AMC 23, OlympiadBench, and Minerva Math. For clarity, curves are exponentially smoothed (smoothing factor = 0.7) to reduce noise."
        },
        {
            "title": "5.1 Training Efficiency",
            "content": "As shown in Figure 3 and Table 1, models with ADARFT (PPO) require substantially fewer training steps to reach the same level of performance as standard PPO. Specifically, we measure how many additional training steps PPO requires to match the performance that ADARFT (PPO) attains at step 60. Because evaluation is performed only every 5 steps during training, we estimate these step savings by interpolating the smoothed accuracy curves. We apply exponential smoothing with factor of 0.7 to reduce evaluation noise and obtain stable estimates of performance over time. 7 For Qwen 2.5 Math 1.5B, PPO requires 26 additional steps in the skew-difficult setting and 29 in the uniform setting to reach the performance achieved by ADARFT (PPO) at step 60corresponding to 43.7% and 48.3% reductions in training steps, respectively. Even in the skew-easy setting, PPO lags behind by 9 steps. The efficiency gains are even more pronounced with the larger Qwen 2.5 7B model: in the skew-difficult setting, PPO requires 44 additional steps, 73.2% increase, and in the skew-easy setting, 64 additional steps, 106.5% increase. On uniformly sampled data, both methods converge at similar rate, though ADARFT (PPO) maintains slight advantage. Notably, these gains are accompanied by reduction in computational cost per step. As shown in Table 1, ADARFT (PPO) consistently achieves lower average time per training step than standard PPO across all settings. While the underlying causes are discussed in Section 6, this result highlights that ADARFT improves not only convergence speed but also overall training throughput, making it both sampleand compute-efficient. Table 1: Accuracy (%) at step 60 for each model, setup, and benchmark. We also report how many steps ADARFT (PPO) saves compared to PPO in reaching the same performance level, and the average time per step at step 60 (in seconds). Note that ADARFT in this table refers to ADARFT instantiated with PPO, i.e., ADARFT (PPO)."
        },
        {
            "title": "Method",
            "content": "GSM8K MATH"
        },
        {
            "title": "Minerva\nMath",
            "content": "AMC 23 (Avg@8) AIME 24 (Avg@8)"
        },
        {
            "title": "Average",
            "content": "Steps Saved (%) Avg Time per Step (s) Qwen 2.5 Math 1.5B skew-difficult uniform skew-easy skew-difficult Qwen 2.5 7B uniform skew-easy PPO ADARFT PPO ADARFT PPO ADARFT PPO ADARFT PPO ADARFT PPO ADARFT 69.14 71.42 70.74 72. 70.58 72.25 88.17 89.08 89.61 88.32 88.86 88.48 60.20 63.00 61.20 65. 61.40 63.60 69.40 72.20 71.00 73.40 72.80 73.40 20.36 20.51 19.61 21. 20.95 19.76 21.99 24.52 23.92 24.81 24.07 23.77 12.13 15.07 14.34 13. 14.34 15.81 25.00 24.26 23.53 22.06 22.79 23.53 45.31 45.62 45.00 47. 47.50 46.88 45.63 48.12 49.06 45.63 48.13 49.69 7.50 10.83 11.67 14. 12.92 14.17 9.58 13.33 9.17 15.83 13.33 12.92 35.77 37.74 37.09 38. 37.95 38.74 43.29 45.25 44.38 45.01 45.00 45.30 26 (43.67%) 29 (48.32%) 9 (14.62%) 44 (73.18%) 3 (5.27%) 64 (106.5%) 134.44 124.66 129.24 120. 119.99 120.91 258.87 247.73 254.38 241.64 250.22 253."
        },
        {
            "title": "5.2 Model Performance",
            "content": "In addition to improving efficiency, ADARFT (PPO) also improves the final model performance. As shown in Table 2, at the end of training (step 100), curriculum learning yields consistent improvements in final accuracy across all configurations. As shown in Table 2, ADARFT (PPO) outperforms standard PPO in nearly every setting, with especially strong gains on the Qwen 2.5 Math 1.5B model. The reported averages reflect accuracy across six diverse benchmarks: GSM8K, MATH 500, OlympiadBench, Minerva Math, AMC 23, and AIME 24. Table 2: Accuracy (%) at step 100 for every model, setup, and benchmark. ADARFT in this table refers to ADARFT instantiated with PPO, i.e., ADARFT (PPO)."
        },
        {
            "title": "Method",
            "content": "GSM8K MATH"
        },
        {
            "title": "Minerva\nMath",
            "content": "AMC 23 (Avg@8) AIME 24 (Avg@8)"
        },
        {
            "title": "Average",
            "content": "Qwen 2.5 Math 1.5B skew-difficult uniform skew-easy skew-difficult Qwen 2.5 7B uniform skew-easy"
        },
        {
            "title": "PPO\nADARFT",
            "content": "69.67 74.00 71.95 74.53 72.71 73.62 89.69 90.98 89.31 90.14 89.39 90. 64.60 66.40 65.20 66.20 67.40 66.20 71.20 71.40 72.40 72.60 73.60 72. 20.65 20.36 21.10 21.99 19.17 19.91 23.33 25.85 23.63 24.96 23.33 25. 12.87 15.07 15.81 14.34 13.97 13.97 23.53 22.43 25.37 24.26 24.26 23. 47.50 55.00 42.50 57.50 45.00 55.00 50.00 52.50 42.50 55.00 47.50 50. 9.17 12.08 6.67 12.08 12.50 9.17 11.25 15.83 15.00 14.58 13.33 14. 37.41 40.48 37.20 41.11 38.46 39.18 44.17 46.83 44.70 46.92 45.07 45. On skew-difficult data, the Qwen 2.5 Math 1.5B model improves from 37.41% (PPO) to 40.48% (ADARFT (PPO)), gain of over 3 percentage points in average accuracy. Similar improvements appear in the uniform setting, where ADARFT (PPO) reaches 41.11%, compared to 37.20% with PPO. Even on skew-easy data, where the baseline performs well, curriculum learning still improves performance, reaching 39.18% versus 38.46%. For the larger Qwen 2.5 7B model, final accuracy 8 gains are also consistent, though slightly more modest. In the skew-difficult setting, ADARFT (PPO) improves from 44.17% to 46.83%. In the uniform setting, accuracy rises from 44.70% to 46.92%, and in the skew-easy case, from 45.07% to 45.94%. These results show that curriculum learning is effective even for stronger models, enhancing both stability and peak performance."
        },
        {
            "title": "6.1 Analysis on Efficiency and Accuracy",
            "content": "Sample efficiency and final performance. ADARFT substantially improves both sample efficiency and final model performance, particularly in non-uniform data distributions. By dynamically adjusting task difficulty during training, ADARFT enables models to allocate learning capacity more effectively, focusing on simpler tasks early on and gradually transitioning to harder ones. This leads to faster convergence and better use of training steps. The performance gains are especially pronounced in the Qwen 2.5 Math 1.5B model, where curriculum structure plays larger role in guiding learning. For the larger Qwen 2.5 7B model, improvements are still evident, though more modestlikely because the models increased capacity allows it to handle diverse and imbalanced tasks without as much external scaffolding. Reduced training time per step. ADARFT also results in slightly lower average training time per step. This is largely due to the fact that easier problems require fewer tokens to solve. For example, an arithmetic reasoning question from GSM8K might require only around 200 tokens for Qwen 2.5 7B to reach correct answer, whereas competition-level math problem from AIME could require around 2000 tokensa 10 difference in rollout length. The total token length affects multiple components of the training step, including the rollout itself and the subsequent PPO update. While PPO update time does not scale linearly with sequence length due to batching and attention computation patterns, longer sequences still incur higher compute cost. As result, curriculum learnings tendency to prioritize shorter, easier problems early in training leads to shorter sequences on average, reducing per-step compute and improving overall training throughput. When does curriculum learning help? Our findings show that curriculum learning provides the greatest benefits under two key conditions: (1) imbalanced training distributions, and (2) limited model capacity. In skewed distributions, particularly the skew-difficulty settings, standard PPO often struggles to gain traction early in training due to insufficient reward signals. ADARFT mitigates this by initially sampling easier problems, enabling the model to bootstrap capabilities before tackling harder content. For example, in the Qwen 2.5 Math 1.5B model trained on skew-difficult data, ADARFT improves final accuracy by over 3 percentage points and reduces training steps by 44%. This suggests that ADARFT is especially helpful when the model would otherwise be under-optimized due to overly difficult or unbalanced data. Conversely, the benefits of ADARFT are less pronounced when the model is strong enough or the data is already well-balanced. For instance, in the uniform setup with the Qwen 2.5 7B model, ADARFT offers only modest improvements in convergence speed and final accuracy. Likewise, in the skew-easy setup for the smaller Qwen 2.5 Math 1.5B model, the curriculum yields relatively smaller gains. In both cases, the model is either already exposed to representative distribution of task difficulties or finds most problems challenging enough, thus reducing the need for dynamic difficulty adjustment. It is important to note that manual data curation and task scheduling, carefully designed for specific model, could potentially achieve similar results by selecting training sequence that aligns with the models learning capacity (Yu et al., 2025; Shen et al., 2025; Chen et al., 2025; Zeng et al., 2025). However, such methods require significant human effort, domain knowledge, and often need to be re-tuned for each new model or task. In contrast, ADARFT requires no manual data curation or model-specific preprocessing. The curriculum automatically adapts to the models reward signal during training, making it broadly applicable across model scales and training distributions. This automatic adaptability not only saves engineering effort but also improves scalability and robustness in real-world training pipelines. Moreover, ADARFT is particularly advantageous in fixed data settings, as it can tailor the difficulty schedule to match the capabilities of any model, whether strong or weak, without altering the dataset, providing unified solution that generalizes across model architectures and skill levels."
        },
        {
            "title": "6.2 Data Difficulty on Model Performance",
            "content": "To better understand the effect of data difficulty on model performance, we introduce two additional data distributions: easy-extreme and hard-extreme. Unlike the skew-difficult and skew-easy distributions, which still include mix of difficulty levels, the easy-extreme and hard-extreme sets consist exclusively of the most polarized examples. Specifically, easy-extreme contains only the easiest samples with difficulty levels no greater than 15, while hard-extreme includes only the hardest samples with difficulty levels of at least 97. Each of these extreme distributions consists of approximately 8,000 samples, providing focused and controlled evaluation of model behavior under minimal or maximal difficulty conditions. We trained Qwen 2.5 7B model on each of the two extreme distributions using PPO, and compared their performance to models trained on the uniform distribution with PPO (Uniform) and with ADARFT instantiated with PPO (Uniform + ADARFT), as described in Section 5. The results are presented in Figure 4. Figure 4: Performance comparison of Qwen 2.5 7B trained on different data distributions using PPO (Uniform, Easy-Extreme, Hard-Extreme) and ADARFT instantiated with PPO (Uniform + ADARFT). For clarity, curves are exponentially smoothed (smoothing factor = 0.7) to reduce noise. Accuracy. The leftmost panel shows that uniform + ADARFT achieves the highest overall accuracy throughout training, outperforming both uniform and the two extreme settings. This highlights the effectiveness of ADARFT in guiding the model through an optimal difficulty progression. In contrast, hard-extreme struggles significantly, with flat and lower trajectory, indicating that exposing the model only to very difficult problems limits learning progress. This suggests that without gradual exposure strategy, models trained on only the hardest problems are unable to bootstrap their capabilities effectively. Reward. The reward trends provide important clues about learning dynamics. The easy-extreme setup achieves the fastest reward improvement during early training, surpassing both uniform and hard-extreme. In particular, easy-extreme consistently operates in reward range between 0.4 and 0.6 during early training, which corresponds to success rate that is both challenging and attainable. In contrast, the reward of the uniform and hard-extreme setup lingers below 0.2 in early training, leading to slower learning. This suggests that training on problems with intermediate difficultythose that are neither trivially easy nor prohibitively hardprovides the most effective learning signal. Notably, ADARFT is explicitly designed to exploit this insight: by setting the target reward β = 0.5, we encourage the model to train on problems that match this productive struggle zone. As shown by the uniform + ADARFT curve, the algorithm successfully maintains an average reward near 0.5 throughout training, allowing the model to learn at an optimal pace. Notably, while the uniform setup eventually reaches reward of nearly 0.5 by step 50, it does not result in faster learning. This is likely because the model is already fairly well trained by that stage, so the additional reward signal contributes less to further improvement. In contrast, the hard-extreme model receives almost no reward signal for most of the training, while the uniform setup shows slower and more gradual reward accumulation. Response Length. The response length panel reveals how the complexity of generated solutions evolves during training. The hard-extreme model consistently produces the longest responses, with length increasing steadily, reflecting the higher complexity and reasoning depth required by the hardest problems. In contrast, the easy-extreme setup maintains short and stable responses, consistent with its simpler problem set. The uniform and uniform + ADARFT setups fall between these two extremes. Notably, uniform + ADARFT shows gradual increase in response length over time. This trend aligns with the behavior of the curriculum learning algorithm: as the model improves, it is exposed to increasingly difficult problems, which naturally demand more elaborate reasoning and longer solutions. This dynamic suggests that response length can serve as useful proxy for problem difficulty and reasoning complexity during training. Difficulty. Finally, the difficulty panel illustrates how problem difficulty evolves under each setup. The easy-extreme and hard-extreme curves remain flat, confirming that these datasets contain only problems from the tail ends of the difficulty spectrum (i.e., 15 and 97, respectively). The uniform curve is centered around 50, as expected, while uniform + ADARFT shows steady increase in difficulty over time. This adaptive progression confirms that curriculum learning effectively steers the model from easier to harder problems, aligning difficulty with the models evolving capabilities."
        },
        {
            "title": "7 Conclusion",
            "content": "We propose ADARFT, an adaptive curriculum learning strategy for reinforcement finetuning (RFT) that dynamically matches problem difficulty to models evolving skill level. By adjusting target difficulty based on reward feedback, ADARFT improves both sample and compute efficiency without modifying the reward function or underlying RL algorithm. Experiments across multiple data regimes and model sizes show consistent gains in convergence speed and final accuracy, especially in imbalanced training distributions. This lightweight, scalable approach highlights the value of curriculum-aware training for efficient and robust alignment in structured reasoning tasks."
        },
        {
            "title": "Acknowledgement",
            "content": "The authors would like to thank the members of the LIME lab and the USC NLP group for their helpful insights and feedback."
        },
        {
            "title": "Future Works",
            "content": "Despite these encouraging results, there are important limitations. Our curriculum strategy estimates task difficulty by measuring the models empirical success rate over 128 attempts per problem, effectively using average success as proxy for difficulty. While this approach is simple and modelgrounded, it is computationally expensive and may not capture the full spectrum of complexity. More sophisticated or efficient alternatives could include model-based difficulty estimation, where lightweight predictive model learns to estimate difficulty from features such as response length, number of reasoning steps, or uncertainty signalsbypassing the need for repeated sampling. Such approaches could enable faster, more scalable curriculum updates and better generalization across tasks. Future work could explore more adaptive curriculum schedules using richer signals (e.g., loss trends, confidence, or gradient variance), as well as co-adapting the model or optimizer with the curriculum. Finally, understanding when curriculum learning offers diminishing returns, as observed in balanced settings or high-capacity models, is crucial for developing general-purpose and compute-aware curricula."
        },
        {
            "title": "References",
            "content": "Arash Ahmadian, Chris Cremer, Matthias Gallé, Marzieh Fadaee, Julia Kreutzer, Olivier Pietquin, Ahmet Üstün, and Sara Hooker. Back to basics: Revisiting reinforce style optimization for learning from human feedback in llms, 2024. URL https://arxiv.org/abs/2402.14740. Yoshua Bengio, Jérôme Louradour, Ronan Collobert, and Jason Weston. Curriculum learning. In Proceedings of the 26th Annual International Conference on Machine Learning, ICML 09, pp. 4148, New York, NY, USA, 2009. Association for Computing Machinery. ISBN 9781605585161. doi: 10.1145/1553374.1553380. URL https://doi.org/10.1145/1553374.1553380. Zhipeng Chen, Yingqian Min, Beichen Zhang, Jie Chen, Jinhao Jiang, Daixuan Cheng, Wayne Xin Zhao, Zheng Liu, Xu Miao, Yang Lu, Lei Fang, Zhongyuan Wang, and Ji-Rong Wen. An empirical study on eliciting and improving r1-like reasoning models, 2025. URL https://arxiv.org/ abs/2503.04548. 11 Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems, 2021. URL https://arxiv.org/ abs/2110.14168. Ganqu Cui, Lifan Yuan, Zefan Wang, Hanbin Wang, Wendi Li, Bingxiang He, Yuchen Fan, Tianyu Yu, Qixin Xu, Weize Chen, Jiarui Yuan, Huayu Chen, Kaiyan Zhang, Xingtai Lv, Shuo Wang, Yuan Yao, Xu Han, Hao Peng, Yu Cheng, Zhiyuan Liu, Maosong Sun, Bowen Zhou, and Ning Ding. Process reinforcement through implicit rewards, 2025. URL https://arxiv.org/abs/2502.01456. Wojciech Marian Czarnecki, Siddhant M. Jayakumar, Max Jaderberg, Leonard Hasenclever, Yee Whye Teh, Simon Osindero, Nicolas Heess, and Razvan Pascanu. Mix&match - agent curricula for reinforcement learning, 2018. URL https://arxiv.org/abs/1806.01780. DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J. L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, R. J. Chen, R. L. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, S. S. Li, Shuang Zhou, Shaoqing Wu, Shengfeng Ye, Tao Yun, Tian Pei, Tianyu Sun, T. Wang, Wangding Zeng, Wanjia Zhao, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, W. L. Xiao, Wei An, Xiaodong Liu, Xiaohan Wang, Xiaokang Chen, Xiaotao Nie, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, X. Q. Li, Xiangyue Jin, Xiaojin Shen, Xiaosha Chen, Xiaowen Sun, Xiaoxiang Wang, Xinnan Song, Xinyi Zhou, Xianzu Wang, Xinxia Shan, Y. K. Li, Y. Q. Wang, Y. X. Wei, Yang Zhang, Yanhong Xu, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Wang, Yi Yu, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yuan Ou, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He, Yunfan Xiong, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Y. X. Zhu, Yanhong Xu, Yanping Huang, Yaohui Li, Yi Zheng, Yuchen Zhu, Yunxian Ma, Ying Tang, Yukun Zha, Yuting Yan, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhenda Xie, Zhengyan Zhang, Zhewen Hao, Zhicheng Ma, Zhigang Yan, Zhiyu Wu, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Zizheng Pan, Zhen Huang, Zhipeng Xu, Zhongyu Zhang, and Zhen Zhang. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. URL https://arxiv.org/abs/2501.12948. Hanze Dong, Wei Xiong, Deepanshu Goyal, Yihan Zhang, Winnie Chow, Rui Pan, Shizhe Diao, Jipeng Zhang, Kashun Shum, and Tong Zhang. Raft: Reward ranked finetuning for generative foundation model alignment, 2023. URL https://arxiv.org/abs/2304.06767. Carlos Florensa, David Held, Xinyang Geng, and Pieter Abbeel. Automatic goal generation for reinforcement learning agents, 2018. URL https://arxiv.org/abs/1705.06366. Bofei Gao, Feifan Song, Zhe Yang, Zefan Cai, Yibo Miao, Qingxiu Dong, Lei Li, Chenghao Ma, Liang Chen, Runxin Xu, Zhengyang Tang, Benyou Wang, Daoguang Zan, Shanghaoran Quan, Ge Zhang, Lei Sha, Yichang Zhang, Xuancheng Ren, Tianyu Liu, and Baobao Chang. Omnimath: universal olympiad level mathematic benchmark for large language models, 2024. URL https://arxiv.org/abs/2410.07985. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurelien Rodriguez, Austen Gregerson, Ava Spataru, 12 Baptiste Roziere, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, Danny Wyatt, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip Radenovic, Francisco Guzmán, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Govind Thattai, Graeme Nail, Gregoire Mialon, Guan Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov, Imanol Arrieta Ibarra, Isabel Kloumann, Ishan Misra, Ivan Evtimov, Jack Zhang, Jade Copet, Jaewon Lee, Jan Geffert, Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasuden Alwala, Karthik Prasad, Kartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heafield, Kevin Stone, Khalid El-Arini, Krithika Iyer, Kshitiz Malik, Kuenley Chiu, Kunal Bhalla, Kushal Lakhotia, Lauren Rantala-Yeary, Laurens van der Maaten, Lawrence Chen, Liang Tan, Liz Jenkins, Louis Martin, Lovish Madaan, Lubo Malo, Lukas Blecher, Lukas Landzaat, Luke de Oliveira, Madeline Muzzi, Mahesh Pasupuleti, Mannat Singh, Manohar Paluri, Marcin Kardas, Maria Tsimpoukelli, Mathew Oldham, Mathieu Rita, Maya Pavlova, Melanie Kambadur, Mike Lewis, Min Si, Mitesh Kumar Singh, Mona Hassan, Naman Goyal, Narjes Torabi, Nikolay Bashlykov, Nikolay Bogoychev, Niladri Chatterji, Ning Zhang, Olivier Duchenne, Onur Çelebi, Patrick Alrassy, Pengchuan Zhang, Pengwei Li, Petar Vasic, Peter Weng, Prajjwal Bhargava, Pratik Dubal, Praveen Krishnan, Punit Singh Koura, Puxin Xu, Qing He, Qingxiao Dong, Ragavan Srinivasan, Raj Ganapathy, Ramon Calderer, Ricardo Silveira Cabral, Robert Stojnic, Roberta Raileanu, Rohan Maheswari, Rohit Girdhar, Rohit Patel, Romain Sauvestre, Ronnie Polidoro, Roshan Sumbaly, Ross Taylor, Ruan Silva, Rui Hou, Rui Wang, Saghar Hosseini, Sahana Chennabasappa, Sanjay Singh, Sean Bell, Seohyun Sonia Kim, Sergey Edunov, Shaoliang Nie, Sharan Narang, Sharath Raparthy, Sheng Shen, Shengye Wan, Shruti Bhosale, Shun Zhang, Simon Vandenhende, Soumya Batra, Spencer Whitman, Sten Sootla, Stephane Collot, Suchin Gururangan, Sydney Borodinsky, Tamar Herman, Tara Fowler, Tarek Sheasha, Thomas Georgiou, Thomas Scialom, Tobias Speckbacher, Todor Mihaylov, Tong Xiao, Ujjwal Karn, Vedanuj Goswami, Vibhor Gupta, Vignesh Ramanathan, Viktor Kerkez, Vincent Gonguet, Virginie Do, Vish Vogeti, Vítor Albiero, Vladan Petrovic, Weiwei Chu, Wenhan Xiong, Wenyin Fu, Whitney Meers, Xavier Martinet, Xiaodong Wang, Xiaofang Wang, Xiaoqing Ellen Tan, Xide Xia, Xinfeng Xie, Xuchao Jia, Xuewei Wang, Yaelle Goldschlag, Yashesh Gaur, Yasmine Babaei, Yi Wen, Yiwen Song, Yuchen Zhang, Yue Li, Yuning Mao, Zacharie Delpierre Coudert, Zheng Yan, Zhengxing Chen, Zoe Papakipos, Aaditya Singh, Aayushi Srivastava, Abha Jain, Adam Kelsey, Adam Shajnfeld, Adithya Gangidi, Adolfo Victoria, Ahuva Goldstand, Ajay Menon, Ajay Sharma, Alex Boesenberg, Alexei Baevski, Allie Feinstein, Amanda Kallet, Amit Sangani, Amos Teo, Anam Yunus, Andrei Lupu, Andres Alvarado, Andrew Caples, Andrew Gu, Andrew Ho, Andrew Poulton, Andrew Ryan, Ankit Ramchandani, Annie Dong, Annie Franco, Anuj Goyal, Aparajita Saraf, Arkabandhu Chowdhury, Ashley Gabriel, Ashwin Bharambe, Assaf Eisenman, Azadeh Yazdan, Beau James, Ben Maurer, Benjamin Leonhardi, Bernie Huang, Beth Loyd, Beto De Paola, Bhargavi Paranjape, Bing Liu, Bo Wu, Boyu Ni, Braden Hancock, Bram Wasti, Brandon Spence, Brani Stojkovic, Brian Gamido, Britt Montalvo, Carl Parker, Carly Burton, Catalina Mejia, Ce Liu, Changhan Wang, Changkyu Kim, Chao Zhou, Chester Hu, Ching-Hsiang Chu, Chris Cai, Chris Tindal, Christoph Feichtenhofer, Cynthia Gao, Damon Civin, Dana Beaty, Daniel Kreymer, Daniel Li, David Adkins, David Xu, Davide Testuggine, Delia David, Devi Parikh, Diana Liskovich, Didem Foss, Dingkang Wang, Duc Le, Dustin Holland, Edward Dowling, Eissa Jamil, Elaine Montgomery, Eleonora Presani, Emily Hahn, Emily Wood, Eric-Tuan Le, Erik Brinkman, Esteban Arcaute, Evan Dunbar, Evan Smothers, Fei Sun, Felix Kreuk, Feng Tian, Filippos Kokkinos, Firat Ozgenel, Francesco Caggioni, Frank Kanayet, Frank Seide, Gabriela Medina Florez, Gabriella Schwarz, Gada Badeer, Georgia Swee, Gil Halpern, Grant Herman, Grigory Sizov, Guangyi, Zhang, Guna Lakshminarayanan, Hakan Inan, Hamid Shojanazeri, Han Zou, Hannah Wang, Hanwen Zha, Haroun Habeeb, Harrison Rudolph, Helen Suk, Henry Aspegren, Hunter Goldman, Hongyuan Zhan, Ibrahim Damlaj, Igor Molybog, Igor Tufanov, Ilias Leontiadis, Irina-Elena Veliche, Itai Gat, Jake Weissman, James Geboski, James Kohli, Janice Lam, Japhet Asher, Jean-Baptiste Gaya, Jeff Marcus, Jeff Tang, Jennifer Chan, Jenny Zhen, Jeremy Reizenstein, Jeremy Teboul, Jessica Zhong, Jian Jin, Jingyi Yang, Joe Cummings, Jon Carvill, Jon Shepard, Jonathan McPhie, Jonathan Torres, Josh Ginsburg, Junjie Wang, Kai Wu, Kam Hou U, Karan Saxena, Kartikay Khandelwal, Katayoun Zand, Kathy Matosich, Kaushik 13 Veeraraghavan, Kelly Michelena, Keqian Li, Kiran Jagadeesh, Kun Huang, Kunal Chawla, Kyle Huang, Lailin Chen, Lakshya Garg, Lavender A, Leandro Silva, Lee Bell, Lei Zhang, Liangpeng Guo, Licheng Yu, Liron Moshkovich, Luca Wehrstedt, Madian Khabsa, Manav Avalani, Manish Bhatt, Martynas Mankus, Matan Hasson, Matthew Lennie, Matthias Reso, Maxim Groshev, Maxim Naumov, Maya Lathi, Meghan Keneally, Miao Liu, Michael L. Seltzer, Michal Valko, Michelle Restrepo, Mihir Patel, Mik Vyatskov, Mikayel Samvelyan, Mike Clark, Mike Macey, Mike Wang, Miquel Jubert Hermoso, Mo Metanat, Mohammad Rastegari, Munish Bansal, Nandhini Santhanam, Natascha Parks, Natasha White, Navyata Bawa, Nayan Singhal, Nick Egebo, Nicolas Usunier, Nikhil Mehta, Nikolay Pavlovich Laptev, Ning Dong, Norman Cheng, Oleg Chernoguz, Olivia Hart, Omkar Salpekar, Ozlem Kalinli, Parkin Kent, Parth Parekh, Paul Saab, Pavan Balaji, Pedro Rittner, Philip Bontrager, Pierre Roux, Piotr Dollar, Polina Zvyagina, Prashant Ratanchandani, Pritish Yuvraj, Qian Liang, Rachad Alao, Rachel Rodriguez, Rafi Ayub, Raghotham Murthy, Raghu Nayani, Rahul Mitra, Rangaprabhu Parthasarathy, Raymond Li, Rebekkah Hogan, Robin Battey, Rocky Wang, Russ Howes, Ruty Rinott, Sachin Mehta, Sachin Siby, Sai Jayesh Bondu, Samyak Datta, Sara Chugh, Sara Hunt, Sargun Dhillon, Sasha Sidorov, Satadru Pan, Saurabh Mahajan, Saurabh Verma, Seiji Yamamoto, Sharadh Ramaswamy, Shaun Lindsay, Shaun Lindsay, Sheng Feng, Shenghao Lin, Shengxin Cindy Zha, Shishir Patil, Shiva Shankar, Shuqiang Zhang, Shuqiang Zhang, Sinong Wang, Sneha Agarwal, Soji Sajuyigbe, Soumith Chintala, Stephanie Max, Stephen Chen, Steve Kehoe, Steve Satterfield, Sudarshan Govindaprasad, Sumit Gupta, Summer Deng, Sungmin Cho, Sunny Virk, Suraj Subramanian, Sy Choudhury, Sydney Goldman, Tal Remez, Tamar Glaser, Tamara Best, Thilo Koehler, Thomas Robinson, Tianhe Li, Tianjun Zhang, Tim Matthews, Timothy Chou, Tzook Shaked, Varun Vontimitta, Victoria Ajayi, Victoria Montanez, Vijai Mohan, Vinay Satish Kumar, Vishal Mangla, Vlad Ionescu, Vlad Poenaru, Vlad Tiberiu Mihailescu, Vladimir Ivanov, Wei Li, Wenchen Wang, Wenwen Jiang, Wes Bouaziz, Will Constable, Xiaocheng Tang, Xiaojian Wu, Xiaolan Wang, Xilun Wu, Xinbo Gao, Yaniv Kleinman, Yanjun Chen, Ye Hu, Ye Jia, Ye Qi, Yenda Li, Yilin Zhang, Ying Zhang, Yossi Adi, Youngjin Nam, Yu, Wang, Yu Zhao, Yuchen Hao, Yundi Qian, Yunlu Li, Yuzi He, Zach Rait, Zachary DeVito, Zef Rosnbrick, Zhaoduo Wen, Zhenyu Yang, Zhiwei Zhao, and Zhiyu Ma. The llama 3 herd of models, 2024. URL https://arxiv.org/abs/2407.21783. Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, Jie Liu, Lei Qi, Zhiyuan Liu, and Maosong Sun. Olympiadbench: challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems, 2024. URL https://arxiv.org/abs/2402.14008. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the MATH dataset. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2), 2021. URL https://openreview.net/forum?id=7Bywt2mQsCe. Jian Hu. Reinforce++: simple and efficient approach for aligning large language models, 2025. URL https://arxiv.org/abs/2501.03262. Allan Jabri, Kyle Hsu, Ben Eysenbach, Abhishek Gupta, Sergey Levine, and Chelsea Finn. Unsupervised curricula for visual meta-reinforcement learning, 2019. URL https://arxiv.org/abs/ 1912.04226. Niels Justesen, Ruben Rodriguez Torrado, Philip Bontrager, Ahmed Khalifa, Julian Togelius, and Sebastian Risi. Illuminating generalization in deep reinforcement learning through procedural level generation, 2018. URL https://arxiv.org/abs/1806.10729. Amirhossein Kazemnejad, Milad Aghajohari, Eva Portelance, Alessandro Sordoni, Siva Reddy, Aaron Courville, and Nicolas Le Roux. Vineppo: Unlocking rl potential for llm reasoning through refined credit assignment, 2024. URL https://arxiv.org/abs/2410.01679. Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, and Vedant Misra. Solving quantitative reasoning problems with language models, 2022. URL https://arxiv.org/abs/2206.14858. Xuefeng Li, Haoyang Zou, and Pengfei Liu. Limr: Less is more for rl scaling, 2025. URL https://arxiv.org/abs/2502.11886. 14 Ziniu Li, Tian Xu, Yushun Zhang, Zhihang Lin, Yang Yu, Ruoyu Sun, and Zhi-Quan Luo. Remax: simple, effective, and efficient reinforcement learning method for aligning large language models, 2024. URL https://arxiv.org/abs/2310.10505. Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step, 2023. URL https://arxiv.org/abs/2305.20050. Michael Luo, Sijun Tan, Justin Wong, Xiaoxiang Shi, William Y. Tang, Manan Roongta, Colin Cai, Jeffrey Luo, Tianjun Zhang, Li Erran Li, Raluca Ada Popa, and Ion Stoica. Deepscaler: Surpassing o1-preview with 1.5b model by scaling rl, 2025. Notion Blog. Tambet Matiisen, Avital Oliver, Taco Cohen, and John Schulman. Teacher-student curriculum learning, 2017. URL https://arxiv.org/abs/1707.00183. Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Candès, and Tatsunori Hashimoto. s1: Simple test-time scaling, 2025. URL https://arxiv.org/abs/2501.19393. OpenAI, :, Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, Alex Iftimie, Alex Karpenko, Alex Tachard Passos, Alexander Neitz, Alexander Prokofiev, Alexander Wei, Allison Tam, Ally Bennett, Ananya Kumar, Andre Saraiva, Andrea Vallone, Andrew Duberstein, Andrew Kondrich, Andrey Mishchenko, Andy Applebaum, Angela Jiang, Ashvin Nair, Barret Zoph, Behrooz Ghorbani, Ben Rossen, Benjamin Sokolowsky, Boaz Barak, Bob McGrew, Borys Minaiev, Botao Hao, Bowen Baker, Brandon Houghton, Brandon McKinzie, Brydon Eastman, Camillo Lugaresi, Cary Bassin, Cary Hudson, Chak Ming Li, Charles de Bourcy, Chelsea Voss, Chen Shen, Chong Zhang, Chris Koch, Chris Orsinger, Christopher Hesse, Claudia Fischer, Clive Chan, Dan Roberts, Daniel Kappler, Daniel Levy, Daniel Selsam, David Dohan, David Farhi, David Mely, David Robinson, Dimitris Tsipras, Doug Li, Dragos Oprica, Eben Freeman, Eddie Zhang, Edmund Wong, Elizabeth Proehl, Enoch Cheung, Eric Mitchell, Eric Wallace, Erik Ritter, Evan Mays, Fan Wang, Felipe Petroski Such, Filippo Raso, Florencia Leoni, Foivos Tsimpourlas, Francis Song, Fred von Lohmann, Freddie Sulit, Geoff Salmon, Giambattista Parascandolo, Gildas Chabot, Grace Zhao, Greg Brockman, Guillaume Leclerc, Hadi Salman, Haiming Bao, Hao Sheng, Hart Andrin, Hessam Bagherinezhad, Hongyu Ren, Hunter Lightman, Hyung Won Chung, Ian Kivlichan, Ian OConnell, Ian Osband, Ignasi Clavera Gilaberte, Ilge Akkaya, Ilya Kostrikov, Ilya Sutskever, Irina Kofman, Jakub Pachocki, James Lennon, Jason Wei, Jean Harb, Jerry Twore, Jiacheng Feng, Jiahui Yu, Jiayi Weng, Jie Tang, Jieqi Yu, Joaquin Quiñonero Candela, Joe Palermo, Joel Parish, Johannes Heidecke, John Hallman, John Rizzo, Jonathan Gordon, Jonathan Uesato, Jonathan Ward, Joost Huizinga, Julie Wang, Kai Chen, Kai Xiao, Karan Singhal, Karina Nguyen, Karl Cobbe, Katy Shi, Kayla Wood, Kendra Rimbach, Keren Gu-Lemberg, Kevin Liu, Kevin Lu, Kevin Stone, Kevin Yu, Lama Ahmad, Lauren Yang, Leo Liu, Leon Maksin, Leyton Ho, Liam Fedus, Lilian Weng, Linden Li, Lindsay McCallum, Lindsey Held, Lorenz Kuhn, Lukas Kondraciuk, Lukasz Kaiser, Luke Metz, Madelaine Boyd, Maja Trebacz, Manas Joglekar, Mark Chen, Marko Tintor, Mason Meyer, Matt Jones, Matt Kaufer, Max Schwarzer, Meghan Shah, Mehmet Yatbaz, Melody Y. Guan, Mengyuan Xu, Mengyuan Yan, Mia Glaese, Mianna Chen, Michael Lampe, Michael Malek, Michele Wang, Michelle Fradin, Mike McClay, Mikhail Pavlov, Miles Wang, Mingxuan Wang, Mira Murati, Mo Bavarian, Mostafa Rohaninejad, Nat McAleese, Neil Chowdhury, Neil Chowdhury, Nick Ryder, Nikolas Tezak, Noam Brown, Ofir Nachum, Oleg Boiko, Oleg Murk, Olivia Watkins, Patrick Chao, Paul Ashbourne, Pavel Izmailov, Peter Zhokhov, Rachel Dias, Rahul Arora, Randall Lin, Rapha Gontijo Lopes, Raz Gaon, Reah Miyara, Reimar Leike, Renny Hwang, Rhythm Garg, Robin Brown, Roshan James, Rui Shu, Ryan Cheu, Ryan Greene, Saachi Jain, Sam Altman, Sam Toizer, Sam Toyer, Samuel Miserendino, Sandhini Agarwal, Santiago Hernandez, Sasha Baker, Scott McKinney, Scottie Yan, Shengjia Zhao, Shengli Hu, Shibani Santurkar, Shraman Ray Chaudhuri, Shuyuan Zhang, Siyuan Fu, Spencer Papay, Steph Lin, Suchir Balaji, Suvansh Sanjeev, Szymon Sidor, Tal Broda, Aidan Clark, Tao Wang, Taylor Gordon, Ted Sanders, Tejal Patwardhan, Thibault Sottiaux, Thomas Degry, Thomas Dimson, Tianhao Zheng, Timur Garipov, Tom Stasi, Trapit Bansal, Trevor Creech, Troy Peterson, Tyna Eloundou, Valerie Qi, Vineet Kosaraju, Vinnie Monaco, Vitchyr Pong, Vlad Fomenko, Weiyi Zheng, Wenda Zhou, Wes McCabe, Wojciech Zaremba, Yann Dubois, Yinghai Lu, Yining Chen, Young Cha, Yu Bai, Yuchen He, Yuchen Zhang, Yunyun Wang, Zheng Shao, and Zhuohan Li. Openai o1 system card, 2024. URL https://arxiv.org/abs/2412.16720. Rémy Portelas, Cédric Colas, Katja Hofmann, and Pierre-Yves Oudeyer. Teacher algorithms for curriculum learning of deep rl in continuously parameterized environments, 2019. URL https://arxiv.org/abs/1910.07224. Qwen, :, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tianyi Tang, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 technical report, 2025. URL https://arxiv.org/abs/2412.15115. Andrei A. Rusu, Neil C. Rabinowitz, Guillaume Desjardins, Hubert Soyer, James Kirkpatrick, Koray Kavukcuoglu, Razvan Pascanu, and Raia Hadsell. Progressive neural networks, 2022. URL https://arxiv.org/abs/1606.04671. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms, 2017. URL https://arxiv.org/abs/1707.06347. Wei Shen, Guanlin Liu, Zheng Wu, Ruofei Zhu, Qingping Yang, Chao Xin, Yu Yue, and Lin Yan. Exploring data scaling trends and effects in reinforcement learning from human feedback, 2025. URL https://arxiv.org/abs/2503.22230. Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: flexible and efficient rlhf framework. arXiv preprint arXiv: 2409.19256, 2024. Sainbayar Sukhbaatar, Zeming Lin, Ilya Kostrikov, Gabriel Synnaeve, Arthur Szlam, and Rob Intrinsic motivation and automatic curricula via asymmetric self-play, 2018. URL Fergus. https://arxiv.org/abs/1703.05407. RUCAIBox STILL Team. Still-3-1.5b-preview: Enhancing slow thinking abilities of small models through reinforcement learning. 2025. URL https://github.com/RUCAIBox/Slow_ Thinking_with_LLMs. Rui Wang, Joel Lehman, Jeff Clune, and Kenneth O. Stanley. Paired open-ended trailblazer (poet): Endlessly generating increasingly complex and diverse learning environments and their solutions, 2019. URL https://arxiv.org/abs/1901.01753. Yixin Ye, Zhen Huang, Yang Xiao, Ethan Chern, Shijie Xia, and Pengfei Liu. Limo: Less is more for reasoning, 2025. URL https://arxiv.org/abs/2502.03387. Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Tiantian Fan, Gaohong Liu, Lingjun Liu, Xin Liu, Haibin Lin, Zhiqi Lin, Bole Ma, Guangming Sheng, Yuxuan Tong, Chi Zhang, Mofan Zhang, Wang Zhang, Hang Zhu, Jinhua Zhu, Jiaze Chen, Jiangjie Chen, Chengyi Wang, Hongli Yu, Weinan Dai, Yuxuan Song, Xiangpeng Wei, Hao Zhou, Jingjing Liu, Wei-Ying Ma, Ya-Qin Zhang, Lin Yan, Mu Qiao, Yonghui Wu, and Mingxuan Wang. Dapo: An open-source llm reinforcement learning system at scale, 2025. URL https://arxiv.org/abs/2503.14476. Wojciech Zaremba and Ilya Sutskever. Learning to execute, 2015. URL https://arxiv.org/abs/ 1410.4615. Weihao Zeng, Yuzhen Huang, Qian Liu, Wei Liu, Keqing He, Zejun Ma, and Junxian He. Simplerlzoo: Investigating and taming zero reinforcement learning for open base models in the wild, 2025. URL https://arxiv.org/abs/2503.18892."
        },
        {
            "title": "A Implementation Details",
            "content": "A.1 Training Configuration We trained both the actor and critic models using the PPO algorithm. Training was conducted on single node equipped with 8 A100 GPUs. Each model was trained for approximately 100 steps using the veRL library (Sheng et al., 2024). The key training configuration for Qwen 2.5 7B is shown below. For Qwen 2.5 MATH 1.5B, we set max_response_length=3000 and ulysses_sequence_parallel_size=1 to accommodate its shorter context window of 4096 tokens, while keeping all other parameters unchanged. export VLLM_ATTENTION_BACKEND=XFORMERS python3 -m verl.trainer.main_ppo data.train_files=\"$train_files\" data.val_files=\"$test_files\" data.train_batch_size=1024 data.max_prompt_length=1024 data.max_response_length=8000 actor_rollout_ref.model.path=Qwen2.5-7B actor_rollout_ref.actor.optim.lr=1e-6 actor_rollout_ref.model.use_remove_padding=True actor_rollout_ref.actor.ulysses_sequence_parallel_size=2 actor_rollout_ref.model.enable_gradient_checkpointing=True actor_rollout_ref.actor.ppo_mini_batch_size=1024 actor_rollout_ref.actor.use_dynamic_bsz=True actor_rollout_ref.actor.ppo_max_token_len_per_gpu=8000 actor_rollout_ref.actor.fsdp_config.param_offload=False actor_rollout_ref.actor.fsdp_config.optimizer_offload=False actor_rollout_ref.rollout.tensor_model_parallel_size=2 actor_rollout_ref.rollout.name=vllm actor_rollout_ref.rollout.gpu_memory_utilization=0.5 actor_rollout_ref.rollout.log_prob_max_token_len_per_gpu=8000 actor_rollout_ref.ref.fsdp_config.param_offload=True actor_rollout_ref.ref.log_prob_max_token_len_per_gpu=8000 critic.optim.lr=1e-5 critic.ulysses_sequence_parallel_size=2 critic.model.use_remove_padding=True critic.model.path=Qwen2.5-7B critic.model.enable_gradient_checkpointing=True critic.ppo_max_token_len_per_gpu=30000 critic.model.fsdp_config.param_offload=False critic.model.fsdp_config.optimizer_offload=False algorithm.kl_ctrl.kl_coef=0.001 trainer.critic_warmup=0 trainer.logger=[console,wandb] trainer.project_name=verl_examples trainer.experiment_name=Qwen2.5-7B trainer.n_gpus_per_node=8 +trainer.val_before_train=True trainer.nnodes=1 trainer.save_freq=10 trainer.test_freq=5 trainer.total_epochs=15 $@ A.2 ADARFT Parameters To enable effective ADARFT, we tuned several hyperparameters: the training batch size was set to = 1024, the target reward to β = 0.5, the sensitivity parameter to α = 2, the step size to η = 50, and the initial target difficulty to = 0. 17 The curriculum update mechanism is governed by the difference between the current average reward and the target reward β. The difficulty adjustment is computed using scaled hyperbolic tangent function: = η tanh (α (r β)) This formulation yields approximately linear updates when β, promoting stable learning. When the reward diverges significantly from the target, the tanh function saturates, effectively limiting the magnitude of difficulty updates and preventing destabilizing jumps. Because the reward is bounded in [0, 1] and the difficulty metric in [0, 100], we set the step size η = 50 to ensure alignment in scale. The modulation parameter α = 2 provides smooth and controlled progression of difficulty throughout training."
        }
    ],
    "affiliations": [
        "University of Maryland, College Park",
        "University of Southern California"
    ]
}