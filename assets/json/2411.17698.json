{
    "paper_title": "Video-Guided Foley Sound Generation with Multimodal Controls",
    "authors": [
        "Ziyang Chen",
        "Prem Seetharaman",
        "Bryan Russell",
        "Oriol Nieto",
        "David Bourgin",
        "Andrew Owens",
        "Justin Salamon"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Generating sound effects for videos often requires creating artistic sound effects that diverge significantly from real-life sources and flexible control in the sound design. To address this problem, we introduce MultiFoley, a model designed for video-guided sound generation that supports multimodal conditioning through text, audio, and video. Given a silent video and a text prompt, MultiFoley allows users to create clean sounds (e.g., skateboard wheels spinning without wind noise) or more whimsical sounds (e.g., making a lion's roar sound like a cat's meow). MultiFoley also allows users to choose reference audio from sound effects (SFX) libraries or partial videos for conditioning. A key novelty of our model lies in its joint training on both internet video datasets with low-quality audio and professional SFX recordings, enabling high-quality, full-bandwidth (48kHz) audio generation. Through automated evaluations and human studies, we demonstrate that MultiFoley successfully generates synchronized high-quality sounds across varied conditional inputs and outperforms existing methods. Please see our project page for video results: https://ificl.github.io/MultiFoley/"
        },
        {
            "title": "Start",
            "content": "4 2 0 2 6 2 ] . [ 1 8 9 6 7 1 . 1 1 4 2 : r Video-Guided Foley Sound Generation with Multimodal Controls Ziyang Chen1,2 Prem Seetharaman2 Bryan Russell2 Oriol Nieto2 David Bourgin2 Andrew Owens Justin Salamon2 1University of Michigan 2Adobe Research https://ificl.github.io/MultiFoley/ (a) Foley with text control (b) Foley with audio control (c) Foley audio extension Figure 1. MultiFoley for video-guided sound generation with multimodal controls. We generate Foley sounds for silent videos with various control signals to shape their audio. (a) Text prompts, both positive and negative, guide synchronized Foley generation. (b) Reference audio from sound libraries defines the customized audio style. (c) partial audio track is extended to produce complete Foley sound. We encourage the reader to watch and listen to the results in our website."
        },
        {
            "title": "Abstract",
            "content": "Generating sound effects for videos often requires creating artistic sound effects that diverge significantly from reallife sources and flexible control in the sound design. To address this problem, we introduce MultiFoley, model designed for video-guided sound generation that supports multimodal conditioning through text, audio, and video. Given silent video and text prompt, MultiFoley allows users to create clean sounds (e.g., skateboard wheels spinning without wind noise) or more whimsical sounds (e.g., making lions roar sound like cats meow). MultiFoley also allows users to choose reference audio from sound effects (SFX) libraries or partial videos for conditioning. key novelty of our model lies in its joint training on both internet video datasets with low-quality audio and professional SFX recordings, enabling high-quality, full- * Work done during an internship at Adobe. bandwidth (48kHz) audio generation. Through automated evaluations and human studies, we demonstrate that MultiFoley successfully generates synchronized high-quality sounds across varied conditional inputs and outperforms existing methods. Please see our project page for video results: https://ificl.github.io/MultiFoley/ 1. Introduction Sound designers, often create sound effects from different sources that may not resemble the original video, such as everyday objects or pre-recorded sound libraries, to create soundtrack for video that synchronizes with on-screen actions. For example, they use crinkling paper for warm fire, coconut shell for horse hooves, or the cracking of celery for breaking bones. This process is known as Foley [1]. Instead of replicating the true sounds, sound designers aim to achieve an artistic effect that enhances the viewers experience. Recent works [61, 64, 85, 88, 90] mainly formulated the Foley problem as video-to-audio generation task, generating sound effects that align temporally and semantically with the video. However, those approaches restrict audio content and lack the control designers need, as they often create sounds that diverge from real life in many different ways. Current systems only offer limited controls (e.g., conditional video examples [22] or language [92]) and face issues with audio quality and synchronization. To give users more creative control over sound design, we propose MULTIFOLEY, video-guided Foley sound generation framework that integrates multimodal controls, using text, audio, and video conditional signals. Our model provides sound designers fine-grained control over how the audio sounds while easing the burden of synchronizing audio to videos. By jointly training across audio, video, and text modalities, MULTIFOLEY enables flexible control and various Foley applications. Users can customize audio content through text prompts whether or not they match the visuals while maintaining synchronization. For example, users can generate clean sound effects by removing unwanted elements like wind noise using text prompts or they can replace cat meow with lion roar, as shown in Fig. 1. Beyond text, our system can also accept different types of audio and audiovisual conditioning. Users can generate desired sound effects from reference audio in the sound effects (SFX) library or extend the soundtrack from portion of video. One of the key challenges is that internet video soundtracks are often poorly aligned with the visual content (e.g., irrelevant audio) and suffer from low quality (e.g., noisy audio and limited bandwidth). To address this, we jointly train on high-quality SFX libraries (audio-text pairs) alongside internet videos, using language supervision in both cases. This approach enables our model to generate full-bandwidth audio (48kHz) that meets professional standards and enhances precise text-based customization. MULTIFOLEY consists of diffusion transformer, highquality audio autoencoder (based on [50]), frozen video encoder for audio-video synchronization, and novel multiconditional training strategy enabling flexible downstream tasks like audio extension and text-driven sound design. Our key contributions are as follows: We present unified framework for video-guided Foley generation that leverages multiple conditioning modalitiestext, audio, and videowithin single model. We introduce training approach that combines internet video datasets with low-quality audio and professional libraries to enable high-quality, full-bandwidth audio generation at 48kHz. We show that our model supports diverse range of applications, such as text-controlled Foley generation, audiocontrolled Foley generation, and Foley extension, expanding possibilities for creative audio production. Through extensive quantitative evaluations and human studies, we demonstrate that MultiFoley achieves better audio quality and improved cross-modal alignment, outperforming existing methods in key benchmarks. 2. Related Work Video-to-audio generation. Video-to-audio generation has recently gained significant attention. Several works have employed auto-regressive transformer models to generate audio from visual features, e.g., SpecVQGAN [41], Im2Wav [80], FoleyGen [64], and V-AURA [87]. Other approaches have introduced video-to-audio diffusion or flow matching models to address this task, e.g., Diff-Foley [61], Action2Sound [11], VTA-LDM [94], and Frieren [90]. Some researchers have adapted the MaskGIT [10] framework for video-to-audio synthesis [59, 73, 85]. Other approaches use two-stage process: first, extracting time-varying signals like sound onsets or energy curves from videos, then adapting pretrained text-to-audio diffusion models with specialized adapters for video-to-audio generation with optional text [20, 40, 43, 51, 92, 98]. V2A-Mapper [88] translates CLIP visual embeddings into CLAP space to condition audio generation via AudioLDM [54]. Recently, concurrent unpublished work Movie Gen Audio [75] explores generating audio conditioned on video and text. This work demonstrates that text provides complementary information for audio generation. In contrast, our approach stands out by providing richer multimodal controls and Foley applications, such as generating on-screen sounds with semantically different audio via text and audio-conditional Foley generation, as shown in Fig. 1. Diffusion models. Diffusion models [21, 36, 8284] are generative models that learn to reconstruct data by reversing process in which data is gradually corrupted with noise. By iterative denoising, these models generate new samples starting from random noise. Latent diffusion models (LDMs) [77] perform the diffusion process in latent space by translating data using pretrained encoder and decoder pair. These models have proven useful for various generative applications, such as image generation and editing [9, 21, 28, 34, 65, 68, 77, 78], video generation [7, 32, 37, 81], audio generation [23, 24, 52, 55, 62, 95], and 3D generation [8, 38, 58]. They have also been applied to tasks such as semantic segmentation [2, 93], camera pose estimation [89, 97], depth estimation [44, 79], and compositional generation [6, 19, 29, 30, 57]. In our work, we leverage diffusion models for video-guided Foley sound generation, incorporating controls from multiple modalities. Audio-visual learning. Many works have focused on learning multimodal representations from audio-visual data. Some study semantic correspondence between sight and sound, e.g., learning common audio-visual associations [3, 5, 3.1. Generative Formulation Given an input silent video vq, our goal is to generate synchronized soundtrack ˆa. This generation is conditioned on text prompt tc and reference audio-visual pair (ac, vc), which can be part of vq or from different videos. We frame this problem as conditional diffusion task. We learn diffusion model ϵθ, parameterized by θ, to model the conditional vq, tc, ac, vc) from the audio a. distribution pθ(a We perform the diffusion process in latent space [77]. a(a) a and decoder We use pretrained audio latent encoder RTa into the compressed lathat converts the waveform RTzCz , where Ta and Tz represent tent code = the lengths of the waveform and audio latents respectively, and Cz denotes the dimension of the latent features. During the forward diffusion process, we gradually add Gaussian noise to clean audio latents z0 to obtain noisy latent zt at different timesteps . During the reverse { process, the denoiser ϵθ( ) computes the noise estimates ˆϵ = ϵθ(zt, vq, tc, ac, vc, t) and optimize following [36, 77]: 1, ..., } LDM = EϵN (0,I),tU (T ) (cid:104) ϵ 2 ˆϵ 2 (cid:105) , (1) or text encoder v, audio where conditions are encoded by the visual encoder encoder t, respectively. During inference, we iteratively denoise the Gaussian noise zT with the noise estimates ˆϵ to obtain the final clean latent ˆz0, which is then decoded into the waveform ˆa = a(ˆz0). 3.2. Multimodal Conditioning and Training We build our model on the Diffusion Transformer (DiT) [74] and pretrained DAC-VAE [50] for the audio encoder and decoder, as shown in Fig. 3. v(vq) Multimodal conditioning. We use pretrained visual enRTv3HW to encode the silent video vq coder RTvCv , where Tv is the number into features of video frames, H, are video height and width, and Cv is the visual feature dimension size. These video features are interpolated to match the length of the audio latents z, then concatenated along the channel dimension with noisy audio latents zt to maintain the audio-visual temporal alignment, similar to Wang et al. [90]. The combined features are passed to the feed-forward transformer ϵθ( For the text condition tc, we apply frozen text encoder RTtCt, where Tt is the t(tc) to extract embeddings tokenized text sequence length, and Ct is tokens embedding dimension. These text embeddings are then incorporated into ϵθ( To improve our models ability to condition on audio and video, during training we provide the model with sample v(vc), of ground truth audio-visual segments, which we select randomly from the clean video. Our training loss is then only applied to the latents that were not provided ) via cross-attention. a(ac) and ). Figure 2. Radar chart comparison for video-to-audio generation task. Each metric is normalized for better visualization. 31, 53, 67], audio-visual sound localization [4, 39, 72], and audio-visual segmentation [56, 99]. Some investigate temporal correspondence between audio and video [13, 25, 42, 69, 70, 86], studying how sounds synchronize with visual events over time. Some explore spatial correspondence between audio and visual data, particularly how sound conveys information about the spatial environment [1418, 26, 63, 96]. Inspired by them, we propose joint learning approach across three modalitiestext, audio, and videothat enables flexible control over various aspects of generated audio. Generative models for sound design. Foley sound design has traditionally relied on Foley artists to create synchronized sound effects using everyday objects or sound libraries [1]. Recent text-to-audio models [23, 24, 62] give sound designers flexible control, allowing them to generate audio attributes directly from descriptive text. However, sound designers still need to synchronize the generated audio with the video manually. T-Foley [1] proposes to address this issue by guiding foley generation using temporal event features like the RMS of the waveform. Video-to-audio models [87, 90] generate synchronized audio but lack user control options. Du et al. [22] introduces conditional Foley task for generating soundtracks from additional video examples. Building on the work above, our model offers artists improved tools for generating synchronized audio with versatile controls using text, audio, and video. 3. Multimodal Conditional Foley Generation We aim to generate synchronized Foley sounds for silent videos, guided by multimodal inputs like text, audio, or video examples that define how the video should sound. First, we introduce simple framework called MULTIFOLEY that performs video-guided Foley generation with multimodal conditions, shown in Fig. 3. Then, we show various applications our model enables for conditional Foley generation. 3 Figure 3. Method overview. We train our model jointly on standard audio-video dataset VGGSound for VT2A generation and high-quality audio-text dataset Sound ideas for T2A generation. We encode the input audio into latents, adding noise to portion of them. The silent video is encoded into visual features, concatenated with the audio latents along the channel dimension. The text input, including quality tag, is encoded through text encoder and applied via cross-attention. as conditioning signal. At inference, we can obtain our audio-visual conditioning, (ac, vc), from different video. Learning from combined datasets. To enhance audio generation quality and text control, we train our model jointly on an audio-visual dataset, VGGSound [12], and professional audio-text dataset sampled from Sound Ideas1. The audio in these datasets is quite different, since VGGSound is in-the-wild and relatively noisy, whereas Sound Ideas contains professionally recorded sound effects and thus is generally higher quality and full bandwidth. To give the model control over audio quality, we assign quality tags to both datasets: VGGSound examples are paired with low quality tag in prompts, e.g., category, low quality. In contrast, Sound Ideas examples are labeled with high quality tag in their captions. During training, we randomly drop out the caption or quality tag to allow the model to disentangle audio quality from semantics. Implementation. Our denoiser ϵθ consists of 12-layer DiT. We train DAC-VAE to map 48kHz audio waveforms into 40Hz latent features with channel dimension Cz of 64, the encoder is frozen during the training. We use the T5-base text encoder [76] to obtain embeddings with Ct = 768, and we apply CAVP [61] to extract visual features at 8 FPS for 8-sec videos with Tv = 64 and Cv = 512. The model is jointly trained for video-text-to-audio generation on VGGSound, containing 168K samples, and for text-to-audio generation on Sound-ideas, with 400K samples. We balance the datasets to ensure robust training and train the model for 600K iterations with batch size of 128. During training, audio latents are masked with probability of 0.25, where the masking spans 0 to 2 seconds. We also randomly drop the video, caption, and quality tag conditions during the 1https://www.sound-ideas.com/ training. We then finetune this pretrained model for 50k iterations on curated subset of VGGSound, selected for high audio-visual correspondence using an ImageBind [31] score threshold of 0.3. During inference, we use the DDIM [83] method and apply classifier-free guidance scale ranging from 3.0 to 5.0 to denoise over 100 steps. See Appendix A.1 for more details. 3.3. Foley Applications Video-guided Foley with text control. Training audio diffusion with both video and text inputs allows our model to generate Foley sounds controlled by text. This dual conditioning setup enables flexible audio generation, where users can influence sound characteristics based on text prompts. Our approach associates language with video cues, disentangling the semantic and temporal elements of videos. This setup allows for creative Foley applications, such as modifying bird-chirping video to sound like human voice or transforming typewriters sound into piano notes all while remaining synchronized with the video. Also, negative prompting provides way to exclude unwanted audio elements by specifying them in the negative text prompts, offering flexible control over the audio output. To achieve this application, we use classifier-free guidance (CFG) [35] with negative prompting. Given positive text prompt tpos for the desired sound effect and negative prompt tneg representing the original expected sound or an unconditional text embedding t, we compute diffusion step via: ˆϵ = (γ + 1) ϵθ(zt, vq, tpos, t) γ ϵθ(zt, v, tneg, t), (2) where γ controls the guidance strength, and is the unconditional embedding for the input videos. Video-guided Foley with audio-visual control. Our model enables generating synchronized soundtracks guided 4 Table 1. Evaluation of video-to-audio generation. Each method is evaluated on the VGGSound test set across six metrics assessing cross-modal alignment and audio quality. ImageBind and CLAP scores are reported in %. The unit of AV-Sync is seconds. DAC-VAE reconstructs the VGGSound audio and serves as an oracle baseline. The best results are in bold. Method Im2Wav [80] V2A-mapper [88] Diff-Foley [61] FoleyGen [64] VAB [85] Frieren [90] FoleyCrafter [98] MultiFoley (ours) DAC-VAE Sampling rate (Hz) 16K 16K 16K 16K 16K 16K 16K 48K 48K Cross-modal alignment Audio quality ImageBind CLAP AV-Sync FAD@VGG FAD@AUD KLD 22.3 25.2 19.5 24.4 27.9 25.4 30.2 28.0 35. 25.2 24.5 20.5 23.4 23.5 24.7 25.3 34.4 28.2 1.25 1.24 1.01 1.24 1.20 0.87 1.24 0.80 0. 7.36 1.13 6.53 3.01 3.26 1.54 2.74 2.92 1.21 5.88 8.59 4.80 4.62 4.85 5.13 6.89 4.62 5. 2.11 2.40 2.90 2.48 2.18 2.50 2.07 1.43 0.28 by both audio and video inputs. Our model can apply the sound characteristics (e.g., rhythm and timbre) of reference audio from an SFX library to silent video, synchronize the audio with visual events, and enable control over the generated output based on the reference audio-visual conditions. We frame this task as the video-guided audio extension problem, where we prepend the conditional audio latent zc = v(vc) to the noisy audio latents zT along the sequence dimension and apply masked denoising to generate the missing sound: a(ac) and optional video feature γ ˆϵ = (γ +1) ϵθ ([zc; zt], [vc; vq], t) ϵθ ([a; zt], v, t) , (3) where denotes the noisy latent that matches the size of the conditional latent zc. vc and vq are encoded by Video-guided Foley with quality control. We enforce quality control by incorporating quality tags in the text, enabling the generation of clean full-band (48kHz) audio. During inference, we guide the model to produce samples that align with the high-quality audio distribution while steering away from low-quality audio using CFG with negative prompt tneg; the prompt can be low quality or the unconditional text embedding t: v. ˆϵ = (γ + 1) γ ϵθ(zt, vq, tc + high quality, t) ϵθ(zt, v, tneg, t). (4) 4. Experiments In this section, we quantitatively and qualitatively evaluate our method for the tasks of video-to-audio generation and video-guided Foley generation with multimodal controls over text, reference audio, and video. 4.1. Video-to-Audio Generation silent videos, through an automatic quantitative evaluation. Experimental setup. We evaluate this task using videos from the VGGSound test set [12]. To ensure accurate audiovisual correspondence, we apply ImageBind [31] to filter out test samples with score below 0.3, yielding final set of 8,702 videos, following [87, 94]. For each video, we generate 8-second audio samples. We compare our model against several video-to-audio baselines, including the autoregressive models Im2Wav [80] and FoleyGen [64], latent diffusion models Diff-Foley [61], V2A-mapper [88] and FoleyCrafter [98], Frieren [90] based on rectified flow matching, and VAB [85] that apply MaskGIT [10] framework. We also report the performance of the reconstructed audio with our DAC-VAE on the VGGSound test set as an oracle baseline. We trim the generated audio from the baselines to 8 seconds for fair evaluation. Our model approaches the video-to-audio generation task as video-text-to-audio (VT2A) generation, using the VGGSound category name as the text input. During the inference, we use the low quality tag for our models generation to stay within the distribution of the VGGSound dataset and guidance scale of 3.0. Evaluation metrics. Following prior work [64, 87, 90], we evaluated model performance in terms of audio quality and cross-modal alignment. We evaluated audio quality using Frechet Audio Distance (FAD) [45] with VGGish [33], which measures the distribution distance between generated and reference audio. For reference sets, we used the VGGSound test set and Adobe SFX Audition2, denoting these metrics as FAD@VGG and FAD@AUD, respectively. We also use KullbackLeibler Divergence (KLD) [49] to measure the probability distributions of class predictions by the PaSST [48] model between the ground-truth and generated samples. To assess cross-modal alignment, we use First, we evaluate the ability of our model in the video-toaudio generation task, i.e., reproducing the soundtracks for 2https : / / www . adobe . com / products / audition / offers/adobeauditiondlcsfx.html 5 ImageBind [31] to measure the semantic correspondence between the generated audio and the input video. We also compute the CLAP score [91] to evaluate the similarity between category labels and generated audio. To measure the cross-modal (temporal) alignment between generated audio and input videos, we apply Synchformer [42] to estimate the weighted temporal offset in seconds, following [87]. The model classifies the offset from -2.0s and 2.0s (with 0.2s resolution). The final AV-Sync metric is the average of the absolute offsets across all examples. Quantitative results. We show our quantitative results in Tab. 1 and Fig. 2 and demonstrate that our method outperforms all the other methods in multiple metrics, including AV-Sync, CLAP score, FAD@AUD, and KLD. This highlights the strong overall performance of our model against baselines. Notably, our synchronization score is comparable to the DAC-VAE reconstructed results, indicating that we successfully generated synced audio for silent input videos. Additionally, our method achieves the second-best performance on the ImageBind score, reflecting strong cross-modal semantic alignment. We outperform the oracle baseline (DAC-VAE) on the CLAP score indicating that our generated are more semantically aligned to corresponding sound effects. The performance of DAC-VAE on FAD@VGG indicates that the VAE influences the generated data distribution, which in turn affects the FAD evaluation. 4.2. Video-guided Foley with Text Control We conduct experiments to evaluate our models capability for video-guided Foley generation with text controls, focusing on synchronization and diversity of semantics. Experimental setup. To quantitatively evaluate the semantic control of the model with text, we sampled videos of 10 categories from the VGGSound-sync [13] dataset as test set with the ImageBind [31] filtering strategy described above. We individually provided the other 9 category names for each video as target text prompts and tasked the models with generating audio based on the given text and video. We generated 4 audio tracks for each pair. We compared our model with FoleyCrafter [98], which also supports text-based control. We also modify FoleyCrafter to disable its semantic adapter to cut the semantic signal from input videos, using this model solely as video-onset ControlNet on the text-to-audio generation model. We evaluated four variants of our approach: 1) w/ NegP: using target prompts as positive prompts and the ground truth category as negative prompts for classifier-free guidance; 2) w/o NegP: no negative prompts are used; 3) True category: regular video-text-to-audio generation with true category as positive prompts (without negative prompts) to generate expected Foley sound for videos, representing the best synchronization performance that our models could achieve; 4) T2A: text-toaudio generation with the given target individual prompts, Table 2. Evaluation on the Foley generation with text controls. NegP denotes negative prompting. We also include two oracle baselines: one using the true category as text prompts and the other omitting video during inference. The best results are in bold. Method Variation CLAP AV-Sync Score Acc FoleyCrafter [98] (w/o semantic adapter) w/ NegP w/o NegP FoleyCrafter [98] Ours Ours oracle w/o NegP w/ NegP w/o NegP w/ NegP True category T2A 38.4 35.7 31.0 33.4 31.4 30.9 4.2 40.3 99.4 99.9 79.2 94. 85.5 93.2 1.8 100 1.34 1.36 1.29 1.31 0.81 0.93 0.77 1. providing the upper bound for CLAP metrics. Evaluation metrics. To evaluate semantic alignment between target prompts and generated audio, we use two CLAPbased [91] metrics. First, we calculate the CLAP score, defined as the average cosine similarity between the CLAP embeddings of each text prompt and generated audio pair. Additionally, we use the CLAP model as classifier to compare target category scores against original categories (from videos), reporting binary classification accuracy. We also report temporal performance using the aforementioned AVSync metric. Although generated audio-visual examples fall outside the synchronization models training distribution, we found that it returns reliable scores. Results. We present the results in Tab. 2. Our method achieves the highest synchronization performance, comparable to the oracle model (Ourstrue category) that generates soundtracks with original semantics. The CLAP metrics are also reasonable, indicating that our approach effectively generates synchronized audio for input videos while allowing semantic control through text. Although FoleyCrafter [98] performs well on CLAP-based metrics, its AV-Sync score is similar to text-to-audio generation, suggesting it struggles to generate synchronized audio in this task. Additionally, the results of the CLAP accuracy highlight that the negative prompting strategy helps steer the generation away from reproducing the original semantics of the audio, enhancing semantic control through classifier-free guidance while the sync score is slightly dropped. We also show some qualitative examples for text control in Fig. 4, demonstrating that our model enables the disentangling of semantic information from the input video and maintains the temporal information. Human studies. We conducted human evaluation using two-alternative forced choice (2AFC) studies for this task. We selected ten high-quality videos from the VGGSound test set, ensuring variety of categories and clear temporal 6 Figure 4. Qualitative examples for Foley generation with text control. We present generated results for two videos, each with three different text prompts, demonstrating our models ability to produce synchronized soundtracks with varied semantics through text control. Please refer to our website for video results. information for the evaluation. For each video, we used one prompt based on the original videos semantics and another that diverged from it. We compared our method with FoleyCrafter [98]. In the user study, participants watch and listen to two video clips, each paired with an audio sampleone generated by our model and the other from the baseline. They were asked to select which audio (1) best matches the sound of audio prompt, (2) is best synchronized with the video, (3) which audio sounds cleaner and more high definition, and (4) overall sounds best when considering the intended audio for audio prompt. Additional details of the study can be found in Appendix A.3. We demonstrate the user study results in Tab. 3. As can be seen, our method outperforms the baseline in all the aspects. Human evaluators consistently rate our results as being higher in semantic alignment, audio-visual synchronization, and audio quality. It demonstrates our models capability in video-guided Foley generation with text control, as well as the quality of the generated audio. Table 3. Human Evaluation on the Foley generation with text control. We show the win rates of our method against FoleyCrafter [98]. 95% confidence intervals are reported in gray. Pvalues are below 1020. (N = 400 with 20 participants) Comparison Win rate (%) Semantic Sync. Quality Overall Ours vs FoleyCrafter 85.8 (3.4) 94.5 (2.1) 86.5 (3.4) 90.2 (2.9) Table 4. Evaluation on the Foley extension with different control signals. Vq means input slient videos. Tc, Ac and Vc denote text, audio and video conditional signals respectively. The best results are in bold. Eval set VGGSound Conditions T c CLAP AV-Sync 55.4 59.6 59.8 64.3 29.3 73.8 74.4 0.79 0.78 0.77 0. 0.88 0.94 0.87 4.3. Video-guided Foley with Audio-Visual Control Greatest-Hits Our model emerges with the ability to generate video-guided Foley using reference audio-visual examples, effectively transferring sounds from conditional clips to generate synchronized soundtracks for silent videos. We evaluate this on the Foley extension task, where the first few seconds of sounding video serve as the condition, and the model generates the remaining audio for the silent part of the video. Experiment setup. We evaluate this task on two datasets: VGGSound-Sync [13] and Greatest-Hits [71]. We sample 1,000 8-second video examples from VGGSound-Sync as indomain data, and 800 8-second examples from Greatest-Hits as out-of-domain data. For each video, the first 3 seconds Table 5. Quantitative evaluation on quality control and ablation study. We evaluate our model with different inference settings, i.e., using various quality tags and excluding text input. We also ablate the impact of the subset fine-tuning strategy. NegP denotes negative prompting. The best results are in bold. Variation Inference Quality tag ImageBind CLAP AV-Sync FAD@VGG FAD@AUD KLD Ours Full w/o ft. Full Full Full VT2A VT2A VT2A V2A (w/o text) Low Low High - 27.3 28.0 25.8 22.4 33.8 34.4 34.9 19.4 0.81 0.80 0.83 0.77 3.00 2.92 4.37 4. 4.39 4.62 4.09 3.44 1.47 1.43 1.60 2.59 of both audio and video (ac, vc) serve as conditional inputs, while the model generates the remaining 5 seconds of audio ˆa for the corresponding silent video segment vq. We evaluate the results using two metrics: (1) CLAP score to measure the cosine similarity between the ground-truth and generated audio, and (2) AV-Sync score to evaluate synchronization accuracy. We generate four audio clips for each video and compare our models performance across different combinations of multimodal conditional inputs (tc, ac, vc). Text inputs are omitted to evaluate the Greatest-Hits dataset, focusing solely on the audio and video conditions. Results. We present the results in Tab. 4. Across both datasets, CLAP similarity improves substantially with the addition of audio conditions. When video conditions are added, we observe further improvement in the AV-Sync score, particularly for the Greatest-Hits dataset. This suggests that the model effectively leverages in-context learning to interpret information from conditional audio-visual inputs. Additionally, our model demonstrates strong capability in Foley analogy tasks as shown in Fig. 1. For instance, given reference dog bark, the model can produce synchronized Foley audio for different dog barking video, or generate drum sounds for basketball dribble video with reference drum audio sample. Please see our website for examples. 4.4. Video-guided Foley with Quality Control During our experiments, we observed that audio from VGGSound videos downloaded via YouTube is often compressed to 32kHz or lower. When resampled to 48kHz, the high-frequency content above 32kHz is missing, as shown in Fig. 5. Consequently, when training on the VGGSound dataset, models inherently generate audio with quality aligned to 32kHz, reflecting the datasets distribution. Our model incorporates high-quality text-audio data at 48kHz during training to address this limitation, paired with quality tags. This enables the generation of full-band, 48kHz audio for video-guided Foley generation. During inference, we use the high quality tag to guide the model toward generating audio that follows the distribution of the highquality text-audio dataset, ensuring 48kHz output. We provide examples in Fig. 5, demonstrating the models ability to control and improve audio quality. Additionally, quantitative results in Tab. 5 support this, where we obFigure 5. Qualitative results of quality control. We show that VGGSound audio has limited bandwidth and demonstrate our model generates full-band 48kHz audio with quality control. serve that with the high quality tag, our model achieves better performance on the FAD@AUD metric, while performance on FAD@VGG decreases. This suggests the model effectively generates audio that aligns more closely with high-quality sound effect distributions. 4.5. Ablation Subset fine-tuning. As discussed in Sec. 3.2, the VGGSound dataset includes noisy, low-quality samples where audio and video often lack alignment. To enhance model performance, we further fine-tune our pretrained model on subset of VGGSound containing better audio-visual correspondence. We evaluate our pretrained model performance and compare it with the fine-tuned model. We present results in Tab. 5, demonstrating that fine-tuning significantly improves cross-modal alignment metrics. No-text inference. We also perform an ablation study where we remove text conditioning and evaluate our model directly on video-to-audio generation. We report the results in Tab. 5. We observe that while performance on the semantics-related metrics drops significantly, synchronization metrics remain high, indicating that our model relies on text to drive semantic alignment but can retain temporal consistency even in the absence of any conditioning. 8 5. Conclusion In this paper, we present MULTIFOLEY, Foley system designed for video-guided Foley generation using multimodal inputs, including text, audio, and video. We evaluate our model on standard video-to-audio generation tasks, providing quantitative evidence of its effectiveness. We explore the models control capabilities with different conditional inputs through both quantitative and qualitative experiments, illustrating range of Foley applications achievable with our approach. Our work is step toward the broader goal of user-in-the-loop sound design. By providing easy-to-use, multimodal controls, we aim to help users create customized, synchronized high-quality audio. Limitations and broader impacts. Our model is currently trained on small-scale, in-the-wild audio-visual dataset, VGGSound, which constrains its capabilities. We believe larger, high-quality Foley dataset would significantly enhance our models performance and broaden its applicability. Our model currently struggles with handling multiple sound events alongside text, often leading to confusion about the timing of each event. Our method can also create realistic but potentially misleading media. Responsible use is essential to prevent misuse in situations where authenticity matters. Acknowledgements. We thank Sonal Kumar, Hugo Flores Garcıa, Xiulong Liu, Yongqi Wang, and Yiming Zhang for their valuable help and discussion, and Adolfo Hernandez Santisteban for informing the project with his sound design experience. This work was supported in part by an NSF CAREER Award #2339071."
        },
        {
            "title": "References",
            "content": "[1] Vanessa Theme Ament. The Foley grail: The art of performing sound for film, games, and animation. Routledge, 2014. 1, 3 [2] Tomer Amit, Tal Shaharbany, Eliya Nachmani, and Lior Wolf. Segdiff: Image segmentation with diffusion probabilistic models. arXiv preprint arXiv:2112.00390, 2021. 2 [3] Relja Arandjelovic and Andrew Zisserman. Look, listen and learn. In Proceedings of the IEEE international conference on computer vision, pages 609617, 2017. 2 [4] Relja Arandjelovic and Andrew Zisserman. Objects that sound. In Proceedings of the European conference on computer vision (ECCV), pages 435451, 2018. 3 [5] Yuki Asano, Mandela Patrick, Christian Rupprecht, and Andrea Vedaldi. Labelling unlabelled videos from scratch with multi-modal self-supervision. Advances in Neural Information Processing Systems, 33:46604671, 2020. 2 [6] Omer Bar-Tal, Lior Yariv, Yaron Lipman, and Tali Dekel. Multidiffusion: Fusing diffusion paths for controlled image generation. arXiv preprint arXiv:2302.08113, 2023. 2 [7] Omer Bar-Tal, Hila Chefer, Omer Tov, Charles Herrmann, Roni Paiss, Shiran Zada, Ariel Ephrat, Junhwa Hur, Yuanzhen Li, Tomer Michaeli, et al. Lumiere: space-time diffusion model for video generation. arXiv preprint arXiv:2401.12945, 2024. 2 [8] Raphael Bensadoun, Tom Monnier, Yanir Kleiman, Filippos Kokkinos, Yawar Siddiqui, Mahendra Kariya, Omri Harosh, Roman Shapovalov, Benjamin Graham, Emilien Garreau, et al. Meta 3d gen. arXiv preprint arXiv:2407.02599, 2024. 2 [9] Tim Brooks, Aleksander Holynski, and Alexei Efros. Instructpix2pix: Learning to follow image editing instructions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1839218402, 2023. 2 [10] Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William Freeman. Maskgit: Masked generative image transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1131511325, 2022. 2, 5 [11] Changan Chen, Puyuan Peng, Ami Baid, Sherry Xue, WeiNing Hsu, David Harwath, and Kristen Grauman. Action2sound: Ambient-aware generation of action sounds from egocentric videos. In ECCV, 2024. 2 [12] Honglie Chen, Weidi Xie, Andrea Vedaldi, and Andrew Zisserman. Vggsound: large-scale audio-visual dataset. In ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 721725. IEEE, 2020. 4, 5 [13] Honglie Chen, Weidi Xie, Triantafyllos Afouras, Arsha Nagrani, Andrea Vedaldi, and Andrew Zisserman. Audio-visual synchronisation in the wild. arXiv preprint arXiv:2112.04432, 2021. 3, 6, 7 [14] Mingfei Chen, Kun Su, and Eli Shlizerman. Be everywherehear everything (bee): Audio scene reconstruction by sparse audio-visual samples. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 78537862, 2023. [15] Ziyang Chen, Xixi Hu, and Andrew Owens. Structure from silence: Learning scene structure from ambient sound. In 5th Annual Conference on Robot Learning, 2021. [16] Ziyang Chen, David Fouhey, and Andrew Owens. Sound localization by self-supervised time delay estimation. European Conference on Computer Vision (ECCV), 2022. [17] Ziyang Chen, Shengyi Qian, and Andrew Owens. Sound localization from motion: Jointly learning sound direction and camera rotation. In International Conference on Computer Vision (ICCV), 2023. [18] Ziyang Chen, Israel D. Gebru, Christian Richardt, Anurag Kumar, William Laney, Andrew Owens, and Alexander Richard. Real acoustic fields: An audio-visual room acoustics dataset and benchmark. In The IEEE / CVF Computer Vision and Pattern Recognition Conference (CVPR), 2024. 3 [19] Ziyang Chen, Daniel Geng, and Andrew Owens. Images that sound: Composing images and sounds on single canvas. Neural Information Processing Systems (NeurIPS), 2024. 2 [20] Marco Comunit`a, Riccardo Gramaccioni, Emilian Postolache, Emanuele Rodol`a, Danilo Comminiello, and Joshua Reiss. Syncfusion: Multimodal onset-synchronized video-toaudio foley synthesis. In ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 936940. IEEE, 2024. 2 [21] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in neural information processing systems, 34:87808794, 2021. 2 [22] Yuexi Du, Ziyang Chen, Justin Salamon, Bryan Russell, and Andrew Owens. Conditional generation of audio from video via foley analogies. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 24262436, 2023. 2, 3 [23] Zach Evans, CJ Carr, Josiah Taylor, Scott Hawley, and Jordi Pons. Fast timing-conditioned latent audio diffusion. arXiv preprint arXiv:2402.04825, 2024. 2, 3 [24] Zach Evans, Julian Parker, CJ Carr, Zack Zukowski, Josiah Taylor, and Jordi Pons. Stable audio open. arXiv preprint arXiv:2407.14358, 2024. 2, 3 [25] Chao Feng, Ziyang Chen, and Andrew Owens. Selfsupervised video forensics by audio-visual anomaly detection. Computer Vision and Pattern Recognition (CVPR), 2023. [26] Ruohan Gao, Changan Chen, Ziad Al-Halah, Carl Schissler, and Kristen Grauman. Visualechoes: Spatial visual representation learning through echolocation. In European Conference on Computer Vision (ECCV), 2020. 3 [27] Hugo Flores Garcia, Prem Seetharaman, Rithesh Kumar, and Bryan Pardo. Vampnet: Music generation via masked acoustic token modeling. arXiv preprint arXiv:2307.04686, 2023. 13 [28] Daniel Geng and Andrew Owens. Motion guidance: Diffusion-based image editing with differentiable motion estimators. arXiv preprint arXiv:2401.18085, 2024. 2 [29] Daniel Geng, Inbum Park, and Andrew Owens. Visual anagrams: Generating multi-view optical illusions with diffusion models. In CVPR, 2024. 2 [30] Daniel Geng, Inbum Park, and Andrew Owens. Factorized diffusion: Perceptual illusions by noise decomposition. arXiv:2404.11615, 2024. 2 [31] Rohit Girdhar, Alaaeldin El-Nouby, Zhuang Liu, Mannat Singh, Kalyan Vasudev Alwala, Armand Joulin, and Ishan Misra. Imagebind: One embedding space to bind them all. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1518015190, 2023. 3, 4, 5, 6 [32] Rohit Girdhar, Mannat Singh, Andrew Brown, Quentin Duval, Samaneh Azadi, Sai Saketh Rambhatla, Akbar Shah, Xi Yin, Devi Parikh, and Ishan Misra. Emu video: Factorizing textto-video generation by explicit image conditioning. arXiv preprint arXiv:2311.10709, 2023. [33] Shawn Hershey, Sourish Chaudhuri, Daniel PW Ellis, Jort Gemmeke, Aren Jansen, Channing Moore, Manoj Plakal, Devin Platt, Rif Saurous, Bryan Seybold, et al. Cnn architectures for large-scale audio classification. In 2017 ieee international conference on acoustics, speech and signal processing (icassp), pages 131135. IEEE, 2017. 5 [34] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Prompt-to-prompt image editing with cross attention control. arXiv preprint arXiv:2208.01626, 2022. 2 [35] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. 4 [36] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. 2, 3 [37] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik Kingma, Ben Poole, Imagen video: Mohammad Norouzi, David Fleet, et al. High definition video generation with diffusion models. arXiv preprint arXiv:2210.02303, 2022. 2 [38] Lukas Hollein, Ang Cao, Andrew Owens, Justin Johnson, and Matthias Nießner. Text2room: Extracting textured 3d meshes from 2d text-to-image models. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 79097920, 2023. [39] Xixi Hu, Ziyang Chen, and Andrew Owens. Mix and localize: Localizing sound sources in mixtures. Computer Vision and Pattern Recognition (CVPR), 2022. 3 [40] Zhiqi Huang, Dan Luo, Jun Wang, Huan Liao, Zhiheng Li, and Zhiyong Wu. Rhythmic foley: framework for seamless audio-visual alignment in video-to-audio synthesis. arXiv preprint arXiv:2409.08628, 2024. 2 [41] Vladimir Iashin and Esa Rahtu. Taming visually guided sound generation. In British Machine Vision Conference (BMVC), 2021. 2 [42] Vladimir Iashin, Weidi Xie, Esa Rahtu, and Andrew Zisserman. Synchformer: Efficient synchronization from sparse cues. arXiv preprint arXiv:2401.16423, 2024. 3, 6 [43] Yujin Jeong, Yunji Kim, Sanghyuk Chun, and Jiyoung Lee. Read, watch and scream! sound generation from text and video. arXiv preprint arXiv:2407.05551, 2024. 2 [44] Bingxin Ke, Anton Obukhov, Shengyu Huang, Nando Metzger, Rodrigo Caye Daudt, and Konrad Schindler. Repurposing diffusion-based image generators for monocular depth estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 94929502, 2024. [45] Kevin Kilgour, Mauricio Zuluaga, Dominik Roblek, and Matthew Sharifi. Frechet audio distance: metric for evaluating music enhancement algorithms. arXiv preprint arXiv:1812.08466, 2018. 5 [46] Diederik Kingma and Jimmy Ba. Adam: method for In International Conference on stochastic optimization. Learning Representation, 2015. 13 [47] Diederik Kingma. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013. 13 [48] Khaled Koutini, Jan Schluter, Hamid Eghbal-Zadeh, and Gerhard Widmer. Efficient training of audio transformers with patchout. arXiv preprint arXiv:2110.05069, 2021. 5 [49] Solomon Kullback and Richard Leibler. On information and sufficiency. The annals of mathematical statistics, 22(1): 7986, 1951. [50] Rithesh Kumar, Prem Seetharaman, Alejandro Luebs, Ishaan Kumar, and Kundan Kumar. High-fidelity audio compression with improved rvqgan. Advances in Neural Information Processing Systems, 36, 2024. 2, 3, 13 [51] Junwon Lee, Jaekwon Im, Dabin Kim, and Juhan Nam. Video-foley: Two-stage video-to-sound generation via temarXiv preprint poral event condition for foley sound. arXiv:2408.11915, 2024. 2 10 [52] Tingle Li, Renhao Wang, Po-Yao Huang, Andrew Owens, and Gopala Anumanchipalli. Self-supervised audio-visual soundscape stylization. In European Conference on Computer Vision, pages 2040. Springer, 2025. 2 [53] Yan-Bo Lin and Gedas Bertasius. Siamese vision transformers are scalable audio-visual learners. arXiv preprint arXiv:2403.19638, 2024. 3 [54] Haohe Liu, Zehua Chen, Yi Yuan, Xinhao Mei, Xubo Liu, Danilo Mandic, Wenwu Wang, and Mark Plumbley. Audioldm: Text-to-audio generation with latent diffusion models. arXiv preprint arXiv:2301.12503, 2023. [55] Haohe Liu, Qiao Tian, Yi Yuan, Xubo Liu, Xinhao Mei, Qiuqiang Kong, Yuping Wang, Wenwu Wang, Yuxuan Wang, and Mark Plumbley. Audioldm 2: Learning holistic audio generation with self-supervised pretraining. arXiv preprint arXiv:2308.05734, 2023. 2 [56] Jinxiang Liu, Yikun Liu, Fei Zhang, Chen Ju, Ya Zhang, and Yanfeng Wang. Audio-visual segmentation via unlabeled frame exploitation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024. 3 [57] Nan Liu, Shuang Li, Yilun Du, Antonio Torralba, and Joshua Tenenbaum. Compositional visual generation with composable diffusion models. In European Conference on Computer Vision, pages 423439. Springer, 2022. 2 [58] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tokmakov, Sergey Zakharov, and Carl Vondrick. Zero-1-to-3: Zero-shot one image to 3d object. In Proceedings of the IEEE/CVF international conference on computer vision, pages 9298 9309, 2023. 2 [59] Xiulong Liu, Kun Su, and Eli Shlizerman. Tell what you hear from what you see video to audio generation through text. Advances in Neural Information Processing Systems, 2024. 2 [60] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. [61] Simian Luo, Chuanhao Yan, Chenxu Hu, and Hang Zhao. Diff-foley: Synchronized video-to-audio synthesis with latent diffusion models. Advances in Neural Information Processing Systems, 36, 2024. 2, 4, 5 [62] Navonil Majumder, Chia-Yu Hung, Deepanway Ghosal, WeiNing Hsu, Rada Mihalcea, and Soujanya Poria. Tango 2: Aligning diffusion-based text-to-audio generations arXiv preprint through direct preference optimization. arXiv:2404.09956, 2024. 2, 3 [63] Sagnik Majumder, Ziad Al-Halah, and Kristen Grauman. Learning spatial features from audio-visual correspondence in egocentric videos. arXiv preprint arXiv:2307.04760, 2023. 3 [64] Xinhao Mei, Varun Nagaraja, Gael Le Lan, Zhaoheng Ni, Ernie Chang, Yangyang Shi, and Vikas Chandra. FoleyarXiv preprint gen: Visually-guided audio generation. arXiv:2309.10537, 2023. 2, 5 [65] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit: Guided image synthesis and editing with stochastic differential equations. arXiv preprint arXiv:2108.01073, 2021. 2 [66] Daniel Morales-Brotons, Thijs Vogels, and Hadrien Hendrikx. Exponential moving average of weights in deep learning: Dynamics and benefits. Transactions on Machine Learning Research, 2024. 13 [67] Pedro Morgado, Nuno Vasconcelos, and Ishan Misra. Audiovisual instance discrimination with cross-modal agreement. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1247512486, 2021. 3 [68] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models, 2021. 2 [69] Muhammad Adi Nugroho, Sangmin Woo, Sumin Lee, and Changick Kim. Audio-visual glance network for efficient video recognition. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1015010159, 2023. 3 [70] Andrew Owens and Alexei Efros. Audio-visual scene analysis with self-supervised multisensory features. In Proceedings of the European conference on computer vision (ECCV), pages 631648, 2018. 3 [71] Andrew Owens, Phillip Isola, Josh McDermott, Antonio Torralba, Edward Adelson, and William Freeman. Visually indicated sounds. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 24052413, 2016. 7 [72] Sooyoung Park, Arda Senocak, and Joon Son Chung. Can clip help sound source localization? In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 57115720, 2024. 3 [73] Santiago Pascual, Chunghsin Yeh, Ioannis Tsiamas, and Joan Serr`a. Masked generative video-to-audio transformers with enhanced synchronicity. arXiv preprint arXiv:2407.10387, 2024. [74] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 41954205, 2023. 3 [75] Adam Polyak, Amit Zohar, Andrew Brown, Andros Tjandra, Animesh Sinha, Ann Lee, Apoorv Vyas, Bowen Shi, ChihYao Ma, Ching-Yao Chuang, et al. Movie gen: cast of media foundation models. arXiv preprint arXiv:2410.13720, 2024. 2 [76] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter Liu. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of machine learning research, 21(140):167, 2020. 4 [77] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. 2, 3 [78] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S. Sara Mahdavi, Rapha Gontijo Lopes, Tim Salimans, Jonathan Ho, David Fleet, and Mohammad Norouzi. Photorealistic text-to-image diffusion models with deep language understanding, 2022. 2 Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2686626875, 2024. 2 [93] Jiarui Xu, Sifei Liu, Arash Vahdat, Wonmin Byeon, Xiaolong Wang, and Shalini De Mello. Open-vocabulary panoptic segmentation with text-to-image diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 29552966, 2023. 2 [94] Manjie Xu, Chenxing Li, Yong Ren, Rilin Chen, Yu Gu, Wei Liang, and Dong Yu. Video-to-audio generation with hidden alignment. arXiv preprint arXiv:2407.07464, 2024. 2, 5 [95] Jinlong Xue, Yayue Deng, Yingming Gao, and Ya Li. Auffusion: Leveraging the power of diffusion and large language models for text-to-audio generation. arXiv preprint arXiv:2401.01044, 2024. 2 [96] Karren Yang, Bryan Russell, and Justin Salamon. Telling left from right: Learning spatial correspondence of sight and sound. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 99329941, 2020. 3 [97] Jason Zhang, Amy Lin, Moneish Kumar, Tzu-Hsuan Yang, Deva Ramanan, and Shubham Tulsiani. Cameras as rays: Pose estimation via ray diffusion. In International Conference on Learning Representations (ICLR), 2024. 2 [98] Yiming Zhang, Yicheng Gu, Yanhong Zeng, Zhening Xing, Yuancheng Wang, Zhizheng Wu, and Kai Chen. Foleycrafter: Bring silent videos to life with lifelike and synchronized sounds. arXiv preprint arXiv:2407.01494, 2024. 2, 5, 6, 7, 13 [99] Jinxing Zhou, Xuyang Shen, Jianyuan Wang, Jiayi Zhang, Weixuan Sun, Jing Zhang, Stan Birchfield, Dan Guo, Lingpeng Kong, Meng Wang, et al. Audio-visual segmentation with semantics. arXiv preprint arXiv:2301.13190, 2023. 3 [79] Saurabh Saxena, Abhishek Kar, Mohammad Norouzi, and David Fleet. Monocular depth estimation using diffusion models. arXiv preprint arXiv:2302.14816, 2023. [80] Roy Sheffer and Yossi Adi. hear your true colors: Image guided audio generation. In ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 15. IEEE, 2023. 2, 5 [81] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, et al. Make-a-video: Text-to-video generation without text-video data. arXiv preprint arXiv:2209.14792, 2022. 2 [82] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In Proceedings of the 32nd International Conference on Machine Learning, pages 2256 2265, Lille, France, 2015. PMLR. 2 [83] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502, 2020. 4 [84] Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020. [85] Kun Su, Xiulong Liu, and Eli Shlizerman. From vision to audio and beyond: unified model for audio-visual representation and generation. In Forty-first International Conference on Machine Learning, 2024. 2, 5 [86] Jiatian Sun, Longxiulin Deng, Triantafyllos Afouras, Andrew Owens, and Abe Davis. Eventfulness for interactive video alignment. ACM Transactions on Graphics (TOG), 42(4): 110, 2023. 3 [87] Ilpo Viertola, Vladimir Iashin, and Esa Rahtu. Temporally aligned audio for video with autoregression. arXiv preprint arXiv:2409.13689, 2024. 2, 3, 5, 6 [88] Heng Wang, Jianbo Ma, Santiago Pascual, Richard Cartwright, and Weidong Cai. V2a-mapper: lightweight solution for vision-to-audio generation by connecting foundation models. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 1549215501, 2024. 2, 5 [89] Jianyuan Wang, Christian Rupprecht, and David Novotny. Posediffusion: Solving pose estimation via diffusion-aided bundle adjustment. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 97739783, 2023. 2 [90] Yongqi Wang, Wenxiang Guo, Rongjie Huang, Jiawei Huang, Zehan Wang, Fuming You, Ruiqi Li, and Zhou Zhao. Frieren: Efficient video-to-audio generation with rectified flow matching. arXiv preprint arXiv:2406.00320, 2024. 2, 3, [91] Yusong Wu, Ke Chen, Tianyu Zhang, Yuchen Hui, Taylor Berg-Kirkpatrick, and Shlomo Dubnov. Large-scale contrastive language-audio pretraining with feature fusion and In ICASSP 2023-2023 keyword-to-caption augmentation. IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 15. IEEE, 2023. 6 [92] Zhifeng Xie, Shengye Yu, Qile He, and Mengtian Li. Sonicvisionlm: Playing sound with vision language models. In 12 A.1. Implementation Details DAC-VAE. We implemented and trained modified version of the Descript Audio Codec (DAC) [50] using variational autoencoder (VAE) [47]. In this approach, we replaced the residual vector quantizer (RVQ) with VAE objective to encode continuous latents, enabling diffusion models to operate on continuous representations instead of discrete tokens. Our DAC-VAE was trained on audio waveforms at various sampling rates, allowing us to encode 48kHz waveform into latents at 40Hz sampling rate, with feature dimension of 64. We train our DAC-VAE model on variety of proprietary and licensed data spanning speech, music, and everyday sounds. DiT architecture. Our DiT model has 12 layers, each with hidden dimension of 1024, 8 attention heads, and an FFN (Feed-Forward Network) dimension of 3072, totaling 332M parameters. For the audio latents, we use an MLP (MultiLayer Perceptron) to project them into 512-dimensional features. separate MLP maps encoded visual features to 512 dimensions, followed by nearest-neighbor interpolation to upsample them fivefold (from 8Hz to 40Hz). Finally, we concatenate the audio and video features along the channel dimension to form 1024-dimensional inputs, which are then fed into the transformer. Similar to VampNet [27], we use two learnable embeddings to differentiate between conditional input audio latents and noisy latents to be denoised, based on the conditional mask. We then sum the corresponding mask embeddings to the audio latents. During the inference, we create conditional mask to achieve audio-conditioned generation. Training details. We use the AdamW optimizer [46, 60] with learning rate of 104 and apply cosine decay schedule. Training begins with linear warm-up phase for the first 4K iterations, followed by 599.6K iterations. We train our model with Exponential Moving Average (EMA) [66] with EMA decay of 0.99. Throughout the training, we randomly sample from combined datasets where 60% of training examples are from VGGSound and 40% from Sound Ideas. Within VGGSound samples, 60% are dedicated to video-text-tag-to-audio generation, the rest 40% are evenly distributed across different dropout variants (i.e., video+tag, video+text, video-only, text+tag, text-only, tag-only, unconditional). For Sound Ideas samples, 60% are allocated to text-tag-to-audio generation, with the remaining cases divided as follows: 10% for text-only, 15% for tag-only, and 15% for unconditional audio generation. A.2. Additional Experiments Guidance scale ablation. We also examine the effect of the classifier-free guidance (CFG) scale, as shown in Tab. 7. The model shows similar performance with guidance weights between 3.0 and 7.0. On the FAD metrics, higher guidance 13 scale improves FAD@AUD but worsens FAD@VGG, suggesting that the model generates examples that align more with high-quality distributions. We use guidance scales of 3.0 and 5.0 for experiments in the main paper. A.3. Human Studies Videos and prompts. We handpicked 10 high-quality videos from the VGGSound test set, choosing examples that span variety of categories and contain clear, easily perceivable temporal actions. We crafted two text prompts for each video: one matching the original category and another for different target category, shown in Tab. 6. We then generated four 8-second samples for each video and randomly selected one for the final evaluation in the survey. For our models generation, we use the high quality tag for inference. Table 6. Audio prompts for the user studies. We note that the prompts are paired for the same video."
        },
        {
            "title": "ReFoley prompt",
            "content": "playing cello bird chirping dog barking typewriter gunshot chopping wood lion roaring squeezing toys playing trumpet playing golf playing erhu rooster crowing playing drum playing piano snare drum playing kick drum playing cat meowing cracking bones playing saxophone explosion prompt } ? User study survey. In the survey, participants watched and listened to 20 pairs of videos comparing our method with FoleyCrafter [98]. We performed forced-choice experiment where we randomized the left-right presentation order of the video pairs. For each video pair, participants were asked to respond to four questions: 1. Which videos audio best matches the sound of audio { 2. In which video is the timing of the audio best synchronized with what you can see in the video? 4. Assuming the video is meant to sound like 3. Which video has audio that sounds cleaner and more high definition? (Please ignore the type of sound and whether its timed to the video, focus only on the audio quality.) audio { , which video has the best audio overall? prompt } The first question evaluates the semantic alignment between the generated audio and the target audio prompt, ensuring that the sound matches the expected content. The second question evaluates the temporal alignment between the audio and video, focusing on how well the sound synchronizes Table 7. Ablation study for classifier-free guidance scale on video-to-audio generation. The best results are in bold."
        },
        {
            "title": "Variation",
            "content": "ImageBind CLAP AV-Sync FAD@VGG FAD@AUD KLD"
        },
        {
            "title": "Ours",
            "content": "γ = 1.0 γ = 3.0 γ = 4.0 γ = 5.0 γ = 7.0 26.4 28.0 28.1 28.0 27.5 32.4 34.4 34.7 34.8 34.6 0.90 0.80 0.77 0.77 0.75 3.16 2.92 3.05 3.27 3.84 4.27 4.62 4.59 4.48 4. 1.49 1.43 1.43 1.43 1.44 Figure 6. Screenshot of Foley user study. We show the screenshot from our user study survey. We show the instructions and the first two video pair examples and associated questions. with visual cues. The third question ignores content and timing to focus specifically on audio quality, examining aspects such as fidelity and production standards. Finally, the last question offers holistic evaluation, determining which model produces the most effective overall audio. We show screenshot of our user study survey including the instruction block, the first two video pairs, and associated questions in Fig. 6."
        }
    ],
    "affiliations": [
        "Adobe Research",
        "University of Michigan"
    ]
}