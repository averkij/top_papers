{
    "paper_title": "Deriving Character Logic from Storyline as Codified Decision Trees",
    "authors": [
        "Letian Peng",
        "Kun Zhou",
        "Longfei Yun",
        "Yupeng Hou",
        "Jingbo Shang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Role-playing (RP) agents rely on behavioral profiles to act consistently across diverse narrative contexts, yet existing profiles are largely unstructured, non-executable, and weakly validated, leading to brittle agent behavior. We propose Codified Decision Trees (CDT), a data-driven framework that induces an executable and interpretable decision structure from large-scale narrative data. CDT represents behavioral profiles as a tree of conditional rules, where internal nodes correspond to validated scene conditions and leaves encode grounded behavioral statements, enabling deterministic retrieval of context-appropriate rules at execution time. The tree is learned by iteratively inducing candidate scene-action rules, validating them against data, and refining them through hierarchical specialization, yielding profiles that support transparent inspection and principled updates. Across multiple benchmarks, CDT substantially outperforms human-written profiles and prior profile induction methods on $85$ characters across $16$ artifacts, indicating that codified and validated behavioral representations lead to more reliable agent grounding."
        },
        {
            "title": "Start",
            "content": "Letian Peng, Kun Zhou, Longfei Yun, Yupeng Hou, Jingbo Shang University of California, San Diego {lepeng, kuzhou, loyun, yphou, jshang}@ucsd.edu 6 2 0 2 5 1 ] . [ 1 0 8 0 0 1 . 1 0 6 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Role-playing (RP) agents rely on behavioral profiles to act consistently across diverse narrative contexts, yet existing profiles are largely unstructured, non-executable, and weakly validated, leading to brittle agent behavior. We propose Codified Decision Trees (CDT), datadriven framework that induces an executable and interpretable decision structure from largescale narrative data. CDT represents behavioral profiles as tree of conditional rules, where internal nodes correspond to validated scene conditions and leaves encode grounded behavioral statements, enabling deterministic retrieval of context-appropriate rules at execution time. The tree is learned by iteratively inducing candidate sceneaction rules, validating them against data, and refining them through hierarchical specialization, yielding profiles that support transparent inspection and principled updates. Across multiple benchmarks, CDT substantially outperforms human-written profiles and prior profile induction methods on 85 characters across 16 artifacts, indicating that codified and validated behavioral representations lead to more reliable agent grounding."
        },
        {
            "title": "Introduction",
            "content": "LLM-based Role-playing (RP) (Chen et al., 2024; Chen et al.), (i.e., building established characters into LLMs) focuses on How LLMs can interact in preferred way (Yang, 2024), and exhibits broaden applications spanning from emotional support (Ye et al., 2025), creative writing (Gurung and Lapata, 2025), and gaming engines (Yu et al., 2025). Fundamentally, an RP system takes scene as the input and outputs an action, aiming to imitate the target characters behavior pattern. Character profile drives it by grounding the LLM with character-specific information. Codified profile (Peng and Shang, 2025) converts symbolic 1Codes and datasets used in experiments are available at https://github.com/KomeijiForce/Codified_Decision_Tree 1 Figure 1: Grounding RP by Codified Decision Tree. rules (e.g., werewolf transforms into wolf under the full moon) to executable functions (state = wolf if is_full_moon(scene) else human). Such structured design provides explicit, executable constraints, leading to better consistency and interpretability than static textual profiles. Since manually specified profiles are often unavailable or prohibitively expensive to construct, an alternative is to build character profiles from storylines (profiling) automatically. Existing profiling methods (Wang et al., 2025b) predominantly focus on constructing textual profiles by recurrently or aggregatively merging sub-profiles summarized from blocks of storylines, which often conflates behaviors exhibited under different situational contexts. To address this limitation, we introduce framework that directly induces codified profiles from storylines, enabling systematic, situation-specific grounding for scene-level action inference. Specifically, inspired by the structure of decision trees (DTs) and human profiling practices, we propose novel Codified Decision Tree (CDT) method. The core idea is to let the LLM hypothesize candidate scene action triggers from similar (scene, action) pairs in the storyline, and then validate these triggers against the complete set of observed pairs. (cid:1) Concretely, we construct CDT by inducing recursively defined tree of behavioral rules from ob- (cid:1) served (scene, action) pairs. We first cluster semantically similar pairs using text embeddings to surface candidate scene action regularities. Within each cluster, an LLM proposes codified triggers of the form if then that map interpretable situational predicates to actions or action modifiers B. These hypothesized triggers are then systematically validated against the entire dataset and used to grow the tree recursively: highly predictive triggers are promoted to internal nodes that partition the data, unsubstantiated triggers are discarded, and partially predictive triggers are refined into child subtrees over filtered subsets of pairs. This construction process yields hierarchy in which each node stores compact set of codified grounding statements, and edges correspond to discriminative questions about whether scene satisfies the associated conditions. At inference time, novel scene is routed through the CDT by answering these questions, going through edges whose question return True. Statements on nodes along the visited path form its situation-specific grounding information, which is then provided to the RP policy for action generation. The resulting structure supports transparent inspection and editing of behavioral rules, principled incorporation of additional data via local subtree updates, and deterministic retrieval of contextappropriate codified profiles for diverse scenes. An example for CDT traversal is presented in Figure 1, where our protagonist Toyama Kasumi (from BanG Dream! Project) is facing scene inviting her to hold live on the moon. Kasumis CDT works by following only the edges with questions satisfied by the scene, accumulating statements (easily ignited and vocalist position) from visited nodes and deliberately omitting ones from unvisited alternatives (study-related branches), which demonstrates the situation-aware nature of CDT. To test the performance of CDT, we first adapt the existing RP benchmark (Peng and Shang, 2025) to finer-grained one through all action extraction and action sequence modeling. Then, we enrich it by adding new artifacts, curating new styles, and collecting large-scale event story conversations. In total, we have 45 characters, 20, 778 scene-action pairs in the fine-grained Fandom benchmark, 40 characters, 7, 866 pairs in the Bandori benchmark, and an extra 77, 182 pairs of event story conversations from BanG Dream!. Based on these benchmarks, we compare CDT against alternative ways of leveraging the training data, including model fine-tuning, retrieval-based in-context learning, and prior textual profiling methods, and find that CDT yields substantially stronger action-prediction performance. Notably, CDT even surpasses humanwritten profiles, highlighting the effectiveness of data-driven profiling when coupled with codification for situation-specific grounding. We further conduct an in-depth analysis of CDT by examining both its profiling and traversal configurations, including hyperparameter choices, efficiency, clustering mechanisms, and top-k statement selection strategies. In addition, we build conversion pipeline that translates CDT into reader-friendly wiki-style textual profile, and the resulting profiles still outperform human-written ones, underscoring the intrinsic quality of CDTinduced knowledge. We also study the data-driven scaling-up of CDT, showing that more data results in stronger CDT profiling. Finally, we present relation modeling as case study of goal-driven CDT, where the tree is constructed to emphasize specific behavioral aspect of character, demonstrating that such targeted profiling can further improve RP precision. Our contributions are three-fold: We propose Codified Decision Trees (CDT), novel data-driven algorithm that induces executable, situation-specific character profiles from storylines for grounded RP inference; We introduce enriched benchmarks and conduct comprehensive evaluations, concluding that CDT consistently outperforms other profiling approaches, and even human-written profiles; We provide an in-depth analysis of CDT, including configuration setup, training and inference strategies, scaling up, and converting CDT to in-depth textual profiles."
        },
        {
            "title": "2 Background",
            "content": "Role-playing. Role-playing (RP) tasks aim to sustain coherent and persona-consistent behaviors across evolving narrative or simulated environments (Riedl and Bulitko, 2012; Shao et al., 2023a; Chen et al.). Early RP systems relied on handcrafted character sheets and rules that describe goals, traits, and responses, but these were often limited in scope and lacked systematic validation. Recent advances in large language models (LLMs) have enabled richer, adaptive personas (Yan et al., 2023; Moore Wang et al., 2024; Peng and Shang, 2024), where characters can recall, reason, and act through long-horizon interactions. However, maintaining behavioral stability and transparency 2 remains challenging, because profiles represented as plain text are hard to verify or execute, and also prone to cause inconsistency. Structured representations such as reasoning graphs, hierarchical memories, and codified constraints (Cheng et al., 2025; Peng and Shang, 2025; Tang et al., 2025) have been proposed to stabilize persona behavior, but these often require extensive manual definition. Our work builds on this direction by inducing interpretable, executable behavioral structures directly from narrative data, enabling grounded, deterministic roleplay without handcrafted profiles. Grounding System. Grounding connects agents internal reasoning to external narrative or environment states, ensuring that actions and dialogue remain contextually valid (Liang et al., 2022; Wu et al., 2023). In RP and simulation contexts, grounding allows to condition agent responses on evolving world facts, social relations, and temporal cues rather than free-form text history. Prior methods achieve this via world models (Zhang et al., 2024; Liu et al., 2024), graph-based memory (Li et al., 2024), or structured symbolic stores that track entities and events (Sun et al., 2024). These methods improve consistency but often rely on manually crafted schemas or domain-specific rules. In contrast, we automatically construct codified decision trees from large-scale narrative corpora. The induced structure explicitly encodes grounding conditions and decision logic, allowing systematic traversal and verification during execution. Rule Mining. Rule mining seeks to discover conditional regularities, typically expressed as ifthen statements that govern observable behaviors in data (Wang et al., 2024b). Traditional approaches identify statistical or symbolic dependencies between events, while recent LLM-based methods infer and validate such rules through natural language understanding and generation (Gan et al., 2024; Yoneda et al., 2024; Wang et al., 2025a). These rules have been used for planning (Yao et al., 2023; Gao et al., 2023), reasoning (Li et al., 2022), and social simulation (Sun et al., 2024), providing interpretable scaffolds for decision control. However, most existing work treats rule extraction as flat, unstructured process and offer limited mechanisms for recursive refinement or contextual specialization. Our CDT differs by inducing hierarchical, executable rule structures: candidate triggers are mined, validated, and recursively expanded, forming transparent tree that encodes both coverage and precision for role-specific behaviors."
        },
        {
            "title": "3.1 Preliminary",
            "content": "Role-playing system It can be formatted as function = RP(sgx) that takes scenes as inputs and outputs actions conditioning on character xs information gx. As most modern RP systems are driven by LLMs, we replace RP() by LLMRP() in this paper. The process to incorporate gx into the action generation is called grounding. grounding system can be static (e.g., appending xs profile to the context) or dynamic (e.g., retrieving similar scene-action pair). Profile Codification Among dynamic systems, codification (Peng and Shang, 2025) is recently proposed RP grounding paradigm, which converts the static profiles into executable functions : gx. For example, statement in xs profile: is always brave against all kinds of challenges. will be codified as if check(s, challenge exists?) return brave where check(s, q) is discriminator calling that returns True/False/None (None represents Unknown) based on the scene and question q. The discrimination can be executed by the RP LLM itself or fine-tuned model. Such codification process improves the RP LLMs consistency with the profile logic by enforcing reasoning paths rather than leaving it for LLMs to explain. limitation of the existing codification framework is the reliance on human-written profiles for conversion. This paper explores different data-driven codification, which takes set of scene-action pairs = {(si, ai)}i=1:D as input to directly build the codified function without given profiles."
        },
        {
            "title": "3.2 Data Structure",
            "content": "Similar to traditional decision trees, CDTs take textual scene as the input and enable moving between nodes based on discrimination, which is implemented as the mentioned check(s, q) function. The difference is that CDTs do not output single label like traditional decision trees. As grounding system, CDTs will instead be traversed, and all grounding information inside visited nodes will be incorporated for grounding. As shown in Figure 1, node in CDT includes two key elements: 1) set of statements h; 2) set of child nodes with questions for traversal checking. 3 Figure 2: The workflow of codified decision tree (CDT). CDT Traversal (Shown in Figure 1, with another running case available in Figure 7) starts from the root node with an input scene s, and we first append all statements inside the root node to the grounding set g, then for each child node, we check whether check(s, q) is True for the assigned question. If the check is passed, we visit the corresponding child node and recursively execute the flow: 1) appending statements; 2) checking to visit child nodes operations until all reachable nodes are visited. Follow the proposed structure, each node in CDT can be understood as the triggered behavior statements under conditions filtered by multiple questions q."
        },
        {
            "title": "3.3 Recursive Hypothesis-Validation",
            "content": "Based on CDTs definition above, we design the methodology to grow CDT (Figure 2). For initialization, we have root node with no statements or children, but the whole training set of scene-action pairs Dtrain. Then we apply rule mining system (elaborated in 3.4) to hypothesize the causality between scenes and actions (hypothesizing case available in Figure 6). Each hypothesis is in the format (q, h) where is the filtering question and is the behavioral statement. For each (s, a) inside Dtrain, we first check whether is global (q-free) behavior statement according to the h-a natural language inference (NLI) relation. By NLI discrimination on all data, we get the ratio of entailed re, neutral rn, and contradicted rc. If the accuracy reaches an accept threshold θacc, will be directly added to the statement set as globally applicable behavior. re re+rc For (q, h) failed to be established as global statement, we then filter into subset of (s, a) pairs that passes the check(s, q). Based on the accuracy, an accept threshold θacc, and reject threshold θrej, we select the next operation step. If accuracy > θacc, we add leaf node with only in H, which can be accessed from the parent by check(s, q) and wont be further grown, representing critical hit. If accuracy < θrej, (q, h) is abolished with no further operation, representing failed hypothesis. If accuracy falls between θrej and θacc, we view it as logic that requires further exploration for more complex structures. Thus, we will build new empty node connected to its parent by check(s, q), the new node will take the filtered dataset for recursive growth. To avoid recursion lasting too long, we set filtering threshold θf so that the recursion is only triggered when D < θf , which gradually reduces during recursion. The recursion can also be stopped by too small or too deep in the CDT. By such recursive hypothesis-validation mechanism, we can model the logic in different complexities to ensure broad and deep coverage."
        },
        {
            "title": "3.4 Rule Mining",
            "content": "In the recursive hypothesis-validation mechanism ( 3.3), the rule mining system is crucial component to make high-potential hypotheses for (q, h). As we target more precise and efficient mining of (s, a) pairs that represent the causality, we apply clustering algorithm (K-Means) based on the textual embeddings of and a. For a, we apply semantic textual embedding (Gao et al., 2021). For s, as we care more about the potential triggered character behavior, we follow the idea of instructionfollowing embedding (Peng et al., 2024) to use the 4 Haruhi is unworried, pointing out the North High uniform is not distinctive enough to identify at glance Scene Haruhi reveals her concept for the film is for the timid Asahina to go through great struggles and suffering to make the happy ending more satisfying Koizumi asks if there will be any actors in the film other than Asahina, Nagato, and himself Question Whatll be Haruhis next action in response to the current scene? Action Haruhi is inspired to recruit people to play Nagatos minions Table 1: Examples of training/test cases. (10 preceding actions as the scene in real benchmarks) last token prediction hidden state from generative LM as the embedding following the prompt below: {scene} Thus, {character} decides to where the hidden states predicted from to will contain the distribution of all kinds of verbs triggered by to guide the clustering. In contrast, normal semantic embedding will be distracted by surface similarity, especially by clustering scenes from the same episode together, ignoring their difference in impact on the target character. Based on clustering the concatenated embedding between scenes and actions, we prompt LLM to summarize potential rules from each cluster. Diversification We let parent nodes pass their question paths (the sequence of questions to reach the node) and established statements (the validated statements when reaching the node) to children. Such information is incorporated into the hypothesis prompt to instruct the LLM to propose something else to avoid redundant checking."
        },
        {
            "title": "4 Benchmark",
            "content": "While early RP works rely on synthesized datasets (Shao et al., 2023b; Wang et al., 2024c), the RP community has also begun to use humans annotations (e.g., profiles and synopses) (Ran et al., 2025; Wang et al., 2025b) on well-known artifacts for benchmarking. Fandom Benchmark (Peng and Shang, 2025) is such an RP benchmark, which utilizes Fandom2s rich human annotations for both profiles and synopses. Fandom Benchmark extracts only key actions of characters in scenes to evaluate whether RP systems can take given scene to make an action entailed by the ground-truth original actions in NLI. We extend such benchmarking strategy to all actions in the gathered artifacts. ing only key actions in the original benchmark, we utilize an LLM to break narrations into sequences of actions annotated with acting characters (environment when no active character). The preceding 10 actions of each action are taken as the input scene. For each character, we perform chronological split of sceneaction pairs, training on the first half of the storyline and evaluating on the second half, which both prevents data contamination and mirrors real-world prediction of future behavior from past evidence. In addition to the original 6 artifacts (Haruhi, K-On!, Fullmetal Alchemist, JOJO, Game of Thrones, Avatar: The Last Airbender), we incorporate Death Note and Spy Family to broaden the benchmarking scope. We experiment on the main characters that appear throughout the storyline for long-horizon testing. Bandori Conversational Benchmark We curate the Bandori conversational benchmark for assessing dialogue-level role-playing behavior. We collect conversations from the first band story of all eight bands (from PoppinParty to MyGO!!!!!)3, where each utterance is treated as an action. Artifact and character background information is provided in Appendix H. Criterion and Statistics For both benchmarks, predicted next actions are compared against reference actions using reference-prediction NLI relation (entailed for 100, neutral for 50, contradicted for 0), following prior practice (Peng and Shang, 2025). Overall, the fine-grained Fandom benchmark contains 45 characters and 20,778 scene-action pairs, while the Bandori benchmark comprises 40 characters and 7,866 pairs. We further collect BanG Dream! event-story conversations, yielding extra 77,182 pairs for scaling-up analysis. Statistics are provided in Appendix A. Our main content focuses on the NLI score for next action prediction, while multi-dimensional scoring, out-of-domain scenario, and human evaluation can be found in Appendix D, which validates NLI to be cross-metric and human consistent."
        },
        {
            "title": "5.1 Evaluation and Baselines",
            "content": "Fine-grained Fandom Benchmark We use the same source of story synopses crawled from Fandom as the original benchmark. Instead of extractWe include comprehensive baseline methods to cover all types of existing methods that utilize known plots to ground RP in new scenes. 2https://www.fandom.com/ 3e.g., bandori.fandom.com/wiki//PoppinParty/Band_Story/ 5 Data-driven Human Bandori Data-driven Fandom Haruhi K-On! Vanilla Fine-tuning RICL ETA CDT (Ours) CDT-Lite (Ours) Human Profile Codified Human Profile 55.08 51.49 56.83 60.54 61.16 62.17 55.87 57.94 49.92 51.01 55.74 53.83 57.93 57.24 55.86 55.93 SF 56.10 49.14 56.86 58.00 60.35 59.79 59.14 59.38 DN FMA JOJO AGOT ATLA Avg. 62.49 50.79 62.80 63.29 66.34 67.00 64.75 65.56 55.66 44.07 56.33 57.12 58.57 59.04 58.54 57. 54.66 34.92 49.77 51.00 57.40 57.26 55.11 56.56 57.05 41.20 56.46 55.28 63.79 64.27 59.35 62.07 53.56 42.84 52.25 56.23 61.05 61.32 57.98 59. 55.57 45.68 56.01 56.91 60.82 61.01 58.33 59.30 Vanilla Fine-tuning RICL ETA CDT (Ours) CDT-Lite (Ours) Human Human Profile Codified Human Profile PoPiPa AG PasuPare Roselia HHW Monica RAS MyGO Avg. 66.39 69.52 73.56 75.29 84.25 88.38 73.73 73. 66.76 62.76 67.56 72.49 79.92 80.49 72.43 74.00 68.29 64.63 73.06 78.00 78.93 82.47 77.11 78.65 66.83 61.83 67.24 70.91 71.93 72.81 70.08 71. 65.13 62.39 73.63 78.92 80.03 79.66 73.14 72.47 64.06 62.35 65.04 66.82 77.33 78.67 68.08 69.14 67.20 62.64 69.44 72.68 80.08 79.51 71.74 71. 59.37 56.72 61.10 62.89 69.17 70.33 63.91 65.02 65.50 62.86 68.86 72.25 77.71 79.04 71.28 71.87 Table 2: RP performance comparison (NLI score) on fine-grained Fandom and Bandori benchmarks. Retrieval-based Vanilla directly prompts the RP model with the input scene, without any additional information. Fine-tuning adapts the RP model by supervised training on the character-specific sceneaction pairs, enabling implicit memorization of behavioral patterns from past storylines. In-Context Learning (RICL) (Wang et al., 2024a) retrieves set of sceneaction examples from the training data that are most similar to the input scene as in-context examples to guide action generation. Extract-Then-Aggregate (ETA) (Wang et al., 2025b) first extracts textual sub-profiles from blocks of storylines and then aggregates them into single character profile, which is appended to the prompt as grounding information. Human Profile grounds RP using human-written character descriptions, containing natural language canonical traits and behaviors. Our experiment applies the ones written in Fandom wiki. Codified Human Profile (Peng and Shang, 2025) converts human-specified symbolic rules into executable grounding functions that condition actions on scene-specific predicates. 5."
        },
        {
            "title": "Implementation Details",
            "content": "Hyperparameters Detailed hyperparameter setups are placed in Appendix B. Hyperparameter impact analysis can be found in Appendix C. embed actions Clustering We using qwen3-embedding-8b (Zhang et al., 2025). Scenes are embedded using instruction-following representations produced by the generative model qwen3-8b (Yang et al., 2025). by Hypothesis-Validation We use gpt-4.1 to hypothesize candidate sceneaction triggers in natural-language ifthen form. Trigger validation is conducted with gpt-4.1-mini via NLIstyle judgments over all sceneaction pairs. We also introduce lightweight variant CDT-Lite, by replacing gpt-4.1-mini with 0.1B encoder model (deberta-v3-base (He et al., 2021)) distilled from 1% of gpt-4.1-mini discrimination. llama-3.1-8b-instruct is used as RP Model the RP model to generate character actions and responses in main experiments. It is also used to answer discriminative questions during CDT traversal. When using CDT-Lite, question discrimination is instead performed by deberta-v3-base."
        },
        {
            "title": "5.3 Main Results",
            "content": "As shown in Table 2, across all 8 Fandom artifacts, CDT(-Lite) achieve the best scores, consistently surpassing Vanilla prompting, fine-tuning, RICL, and ETA. Fine-tuning often underperforms even the Vanilla baseline, which can be attributed to the strict chronological split of training and test sets, as fitting tokens in the first half storyline leads to repeating similar actions rather than learning global behavior pattern. RICL and ETA provide noticeable gains over Vanilla in several settings, confirming the value of grounding on past storylines, but they remain clearly behind CDT(-Lite), which benefit from structured, situation-specific codification rather than purely textual aggregation. CDT also outperforms the Human Profile and Codified Human Profile baselines, which use human-written profiles as commonly accepted ground truth. While these human profiles are 6 Fandom Haruhi K-On! SF DN FMA JOJO AGOT ATLA Avg. CDT-Lite (dmax = 4) 62.17 57. 59.79 67.00 59.04 57.26 64.27 61. 61.01 w/ Abolished Statements t A i dmax = 1 dmax = 2 dmax = 3 w/o Clustering w/o Inst. Embed. w/o Diversification TopK (Depth Rank) TopK (Accuracy Rank) TopK (Usability Rank) Verbalized CDT Wikified CDT 61.18 60.99 61.03 61.53 59.76 58.87 60.75 59.51 61.40 62.21 62.17 59.21 55.09 55.75 56.61 56.50 55.14 57.19 57.04 56.24 56.46 56.11 57.04 54.93 58.85 60.88 60.83 59.68 58.29 58.15 59. 58.25 58.90 59.27 57.93 56.77 66.21 66.29 68.59 67.91 66.39 65.51 66.25 64.82 66.87 67.78 65.32 67.76 58.01 58.03 58.82 57.87 59.52 59.23 57.84 57.26 58.95 58.72 58.54 57.11 56.24 54.56 55.61 57.15 55.32 56.65 57. 54.26 56.14 57.97 55.01 54.28 61.29 61.03 62.13 63.30 60.25 61.21 62.89 61.00 64.49 63.95 61.60 59.15 59.33 60.43 60.80 61.58 60.88 61.06 59.83 60.59 61.04 61.85 60.98 57.95 59.52 59.65 60.55 60.69 59.44 59.73 60. 58.99 60.53 60.98 59.82 58.40 Table 3: Ablation study and variant experiments. (Bandori part placed in Table 7) strong and generally superior to unguided or purely data-hungry approaches, CDT(-Lite) obtain higher scores for every Fandom artifact, demonstrating that data-driven profiling under the codification framework can exceed manually constructed descriptions in both coverage and situational fidelity. On the Bandori benchmark, we observe an even larger margin: CDT(-Lite) achieve the top performance for all 8 bands, with CDT-Lite slightly ahead of CDT in most cases. Traditional grounding methods (fine-tuning, RICL, ETA) improve over Vanilla but still lag behind our codified approach, and even the best human-written or codified human profiles are outperformed by CDT(-Lite). These results indicate that 1) codified, situation-aware profiling is particularly beneficial for conversational RP, and 2) the lightweight CDT-Lite architecture retains most of the benefits of CDT, often improving further, while enabling cheaper validation, making data-driven codified profiling practical at scale."
        },
        {
            "title": "6 Analyses",
            "content": "Because of the page limitation, we place hyperparameter analysis in Appendix C, efficiency analysis in Appendix E, and running cases in Appendix F."
        },
        {
            "title": "6.1 Ablation Study",
            "content": "With abolished statements This ablation verifies the importance of the statement validation stage by simply appending the abolished statements back to the CDT. Comparing with CDT-Lite in Table 3 shows that blindly accepting LLM hypotheses is consistently worse, highlighting the importance of explicit validation in improving reliability. Node Depth We next ablate the maximum traversal depth dmax, which controls how far scene is allowed to descend in the tree. The case dmax = 1 degenerates CDT into flat, codified profile with Figure 3: Performance scales up with training data. only root-level rules. Ablation result shows that RP performance generally improves when deepening the CDT, but it also observes gradual saturation. Clustering Mechanism To study the role of clustering, we consider two variants: (1) w/o Clustering disables clustering altogether and hypothesize triggers on the entire set of sceneaction pairs at each node, forcing the LLM to explain more heterogeneous mixture of behaviors; (2) w/o Inst. Embed. retains clustering but replace instructionfollowing action embeddings with simpler representations, reducing the semantic alignment between scenes and actions. Both lead to consistent drops compared to CDT-Lite  (Table 3)  , indicating that (i) grouping semantically similar instances before rule induction and (ii) using instructionfollowing embeddings to represent actions are both crucial for discovering clean, reusable triggers. Diversification As shown in Table 3, turning off diversification also results in drop of CDT performance, which validates its contribution in probing more potential candidate behaviors."
        },
        {
            "title": "6.2 Variants",
            "content": "Top-K Policy During inference, CDT may activate multiple codified statements along traver7 sal path, requiring selection policy. We evaluate three Top-K ranking strategies: Depth Rank, which favors deeper (more specific) nodes; Accuracy Rank, which orders statements by validated ; and Usability Rank, which priaccuracy oritizes empirically applicable rules . As shown in Table 3, Usability Rank generally yields the strongest results, suggesting that broadly applicable rules provide better grounding for RP. re re+rn+rc re re+rc CDT-to-Wiki We also explore using CDTs as textual profiles. Verbalized CDT linearizes the tree into explicit if-then rules and appends them to the prompt, preserving conditional structure, while Wikified CDT further rewrites these rules into wiki-style narrative (pipeline described in Appendix F), removing most control-flow cues. Both variants eliminate runtime traversal and can be used by standard RP models. As shown in Table 3, they remain strong baselines and outperform humanwritten profiles, despite sacrificing fine-grained situational control and performance."
        },
        {
            "title": "6.3 Scaling-up by Data",
            "content": "We study how CDT scales with additional supervision by varying the number of training sceneaction pairs. Figure 3 reports CDT performance under different training sizes. For the Fandom benchmark, we select characters with at least 128 actions and randomly sample {16, 32, 64, 128} pairs per character. For the Bandori benchmark, using the richer event-story data, we further scale training up to 1024 pairs for five bands. Across both benchmarks, CDT exhibits clear scaling trend: performance steadily improves as more training data are used. On Fandom, CDT trained with only 64 pairs already surpasses humanwritten profile baselines, showing that limited behavioral evidence can outperform manually authored descriptions. On Bandori, gains continue beyond 128 and up to 1024 pairs, indicating that RP performance benefits from scaling up the number of interaction data."
        },
        {
            "title": "6.4 Relation Modeling",
            "content": "Beyond general profiling, CDT also supports goaldriven specialization toward specific behavioral aspects by instructing the trigger-hypothesis prompt. We demonstrate this capability with relation modeling, which aims to capture characteristic interaction patterns between pairs of characters. For each target character, we filter the training data 8 Character Relation with Suzumiya Haruhi Kyon Akiyama Mio Yor Forger Alphonse Elric Toyama Kasumi Mitake Ran Hikawa Hina Hikawa Sayo Chihaya Anon Tainaka Ritsu Loid Forger Edward Elric Ichigaya Arisa Aoba Moca Hikawa Sayo Hikawa Hina Takamatsu Tomori Target Subset +GD CDT Full Test CDT +GD 56.45 44.93 64.13 75.00 85.95 81.25 85.05 76.52 69.33 67.74 50.00 70.65 81.82 88.02 85.00 88.92 86.87 72. 67.91 52.63 60.05 63.43 85.78 85.12 86.84 76.15 82.18 70.14 54.30 60.76 64.82 86.44 86.68 87.69 80.39 83.06 Table 4: Performance of Goal-driven (GD) CDT. to retain only actions that occur immediately after designated related character speaks or acts, and train relation-specific goal-driven CDT that is instructed to focus on their special interaction patterns. At inference time, scenes are routed through this relation-specific CDT whenever the latest context action is taken by the related character; otherwise, the general CDT is used. As shown in Table 4, this +GD variant consistently outperforms the base CDT on the target subset (cases when interacting with the related character) across all nine character pairs and thus results in gains on the full test set, demonstrating that CDT can model fine-grained relational behaviors without sacrificing generality."
        },
        {
            "title": "6.5 Case Study",
            "content": "Figure 8 contrasts human-written character profile with Wikified CDT for Haruhi Suzumiya, highlighting differences in granularity and behavioral coverage. While the human profile offers compact, intuitive summary of key traits, the Wikified CDT expands these traits into systematically grounded patterns that capture wider range of recurring behaviors and situational triggers. This comparison illustrates how CDT provides more comprehensive behavioral grounding than manually authored profiles."
        },
        {
            "title": "7 Conclusion and Future Work",
            "content": "We present Codified Decision Trees (CDT), data-driven framework that induces executable, situation-specific character profiles from storylines. We show that CDT consistently outperforms previous methods, even using human-written profiles. As future work, we plan to extend CDT to multi-character joint profiling and interaction modeling, continual and online updates from interactive RP logs, and multi-modal settings where scenes include visual or game-state signals, moving toward more broadly controllable, behaviorgrounded agents beyond the RP domain."
        },
        {
            "title": "Limitations",
            "content": "While CDT demonstrates strong performance and interpretability, it currently relies only on storylinederived scene-action pairs and does not incorporate pre-designed persona information (e.g., canonical traits or author intent). We adopt this setting to isolate the effect of data-driven profiling and to avoid introducing external human priors that may be inconsistent or unavailable across domains; incorporating such information as soft priors for trigger induction is natural extension. CDT is also constructed offline and remains static, which simplifies validation and benchmarking but does not capture evolving narratives; enabling incremental or continual CDT updates as new storylines unfold is an important direction for future work. Finally, although we focus on role-playing, the CDT framework is general and could be extended to other situation-aware behavior modeling tasks, such as task-oriented agents or embodied decision-making."
        },
        {
            "title": "References",
            "content": "Jiangjie Chen, Xintao Wang, Rui Xu, Siyu Yuan, Yikai Zhang, Wei Shi, Jian Xie, Shuang Li, Ruihan Yang, Tinghui Zhu, Aili Chen, Nianqi Li, Lida Chen, Caiyu Hu, Siye Wu, Scott Ren, Ziquan Fu, and Yanghua Xiao. 2024. From persona to personalization: survey on role-playing language agents. Preprint, arXiv:2404.18231. Jiangjie Chen, Xintao Wang, Rui Xu, Siyu Yuan, Yikai Zhang, Wei Shi, Jian Xie, Shuang Li, Ruihan Yang, Tinghui Zhu, et al. From persona to personalization: survey on role-playing language agents. Transactions on Machine Learning Research. Xilong Cheng, Yunxiao Qin, Yuting Tan, Zhengnan Li, Ye Wang, Hongjiang Xiao, and Yuan Zhang. 2025. Psymem: Fine-grained psychological alignment and explicit memory control for advanced role-playing llms. arXiv preprint arXiv:2505.12814. Xiangyu Robin Gan, Yuxin Ray Song, Nick Walker, and Maya Cakmak. 2024. Can large language models help developers with robotic finite state machine modification? arXiv preprint arXiv:2412.05625. Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Graham Neubig. 2023. Pal: Program-aided language In International Conference on Machine models. Learning, pages 1076410799. PMLR. Tianyu Gao, Xingcheng Yao, and Danqi Chen. 2021. Simcse: Simple contrastive learning of sentence embeddings. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 7-11 November, 2021, pages 6894 6910. Association for Computational Linguistics. Alexander Gurung and Mirella Lapata. 2025. Learning to reason for long-form story generation. CoRR, abs/2503.22828. Pengcheng He, Jianfeng Gao, and Weizhu Chen. 2021. Debertav3: Improving deberta using electra-style pretraining with gradient-disentangled embedding sharing. arXiv preprint arXiv:2111.09543. Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2022. Lora: Low-rank adaptation of large language models. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net. Shilong Li, Yancheng He, Hangyu Guo, Xingyuan Bu, Ge Bai, Jie Liu, Jiaheng Liu, Xingwei Qu, Yangguang Li, Wanli Ouyang, et al. 2024. Graphreader: Building graph-based agent to enhance long-context abilities of large language models. In Findings of the Association for Computational Linguistics: EMNLP 2024, pages 1275812786. Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rémi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, et al. 2022. Competition-level code generation with alphacode. Science, 378(6624):10921097. Jacky Liang, Wenlong Huang, Fei Xia, Peng Xu, Karol Hausman, Brian Ichter, Pete Florence, and Andy Zeng. 2022. Code as policies: Language model arXiv preprint programs for embodied control. arXiv:2209.07753. Zeyang Liu, Xinrui Yang, Shiguang Sun, Long Qian, Lipeng Wan, Xingyu Chen, and Xuguang Lan. 2024. Grounded answers for multi-agent decision-making problem through generative world model. Advances in Neural Information Processing Systems, 37:46622 46652. Ilya Loshchilov and Frank Hutter. 2019. Decoupled In 7th International weight decay regularization. Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net. Zekun Moore Wang, Zhongyuan Peng, Haoran Que, Jiaheng Liu, Wangchunshu Zhou, Yuhan Wu, Hongcheng Guo, Ruitong Gan, Zehao Ni, Jian Yang, et al. 2024. Rolellm: benchmarking, eliciting, and enhancing role-playing abilities of large language models. Findings of the Association for Computational Linguistics: ACL 2024, pages 1474314777. Letian Peng and Jingbo Shang. 2024. Quantifying and optimizing global faithfulness in persona-driven roleIn Advances in Neural Information Proplaying. cessing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024. Letian Peng and Jingbo Shang. 2025. Codifying charIn Proceedings of the acter logic in role-playing. Thirty-Ninth Annual Conference on Neural Information Processing Systems (NeurIPS 2025) Poster Session. Poster presentation, NeurIPS 2025. Letian Peng, Yuwei Zhang, Zilong Wang, Jayanth Srinivasa, Gaowen Liu, Zihan Wang, and Jingbo Shang. 2024. Answer is all you need: Instruction-following text embedding via answering the question. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 1116, 2024, pages 459477. Association for Computational Linguistics. Yiting Ran, Xintao Wang, Tian Qiu, Jiaqing Liang, Yanghua Xiao, and Deqing Yang. 2025. BOOKWORLD: from novels to interactive agent societies for story creation. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2025, Vienna, Austria, July 27 - August 1, 2025, pages 15898 15912. Association for Computational Linguistics. Mark Riedl and Vadim Bulitko. 2012. Interactive narrative: novel application of artificial intelligence for computer games. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 26, pages 21602165. Yunfan Shao, Linyang Li, Junqi Dai, and Xipeng Qiu. 2023a. Character-llm: trainable agent for roleplaying. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 1315313187. Yunfan Shao, Linyang Li, Junqi Dai, and Xipeng Qiu. 2023b. Character-llm: trainable agent for roleplaying. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023, pages 1315313187. Association for Computational Linguistics. Libo Sun, Siyuan Wang, Xuanjing Huang, and Zhongyu Wei. 2024. Identity-driven hierarchical role-playing agents. arXiv preprint arXiv:2407.19412. Yihong Tang, Kehai Chen, Muyun Yang, Zhengyu Niu, Jing Li, Tiejun Zhao, and Min Zhang. 2025. Thinking in character: Advancing role-playing agents with role-aware reasoning. arXiv preprint arXiv:2506.01748. Liang Wang, Nan Yang, and Furu Wei. 2024a. Learning to retrieve in-context examples for large language models. In Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (Volume 1: Long Papers), pages 17521767, St. Julians, Malta. Association for Computational Linguistics. Xiaochen Wang, Junqing He, Liang Chen, Gholamreza Haffari, Yiru Wang, Zhe Yang, Xiangdi Meng, Kunhao Pan, and Zhifang Sui. 2025a. Sg-fsm: self-guiding zero-shot prompting paradigm for multihop question answering based on finite state machine. In Findings of the Association for Computational Linguistics: NAACL 2025, pages 60256037. Xiaochen Wang, Junqing He, Yiru Wang, Xiangdi Meng, Kunhao Pan, Zhifang Sui, et al. 2024b. Fsm: finite state machine based zero-shot prompting paradigm for multi-hop question answering. arXiv preprint arXiv:2407.02964. Xintao Wang, Heng Wang, Yifei Zhang, Xinfeng Yuan, Rui Xu, Jen-tse Huang, Siyu Yuan, Haoran Guo, Jiangjie Chen, Wei Wang, Yanghua Xiao, and Shuchang Zhou. 2025b. Coser: Coordinating llmbased persona simulation of established roles. CoRR, abs/2502.09082. Xintao Wang, Yunze Xiao, Jen-tse Huang, Siyu Yuan, Rui Xu, Haoran Guo, Quan Tu, Yaying Fei, Ziang Leng, Wei Wang, Jiangjie Chen, Cheng Li, and Yanghua Xiao. 2024c. Incharacter: Evaluating personality fidelity in role-playing agents through psychological interviews. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024, pages 1840 1873. Association for Computational Linguistics. Jimmy Wu, Rika Antonova, Adam Kan, Marion Lepert, Andy Zeng, Shuran Song, Jeannette Bohg, Szymon Rusinkiewicz, and Thomas Funkhouser. 2023. Tidybot: Personalized robot assistance with large language models. Autonomous Robots, 47(8):1087 1102. Ming Yan, Ruihao Li, Hao Zhang, Hao Wang, Zhilan Yang, and Ji Yan. 2023. Larp: Language-agent role play for open-world games. arXiv preprint arXiv:2312.17653. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao Ge, Haoran Wei, Huan Lin, Jialong Tang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jian Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keqin Bao, Kexin Yang, Le Yu, Lianghao Deng, Mei Li, Mingfeng Xue, Mingze Li, Pei Zhang, Peng Wang, Qin Zhu, Rui Men, Ruize Gao, Shixuan Liu, Shuang Luo, Tianhao Li, Tianyi Tang, Wenbiao Yin, Xingzhang Ren, Xinyu Wang, Xinyu Zhang, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yinger Zhang, Yu Wan, Yuqiong Liu, Zekun Wang, Zeyu Cui, Zhenru Zhang, Zhipeng Zhou, and Zihan Qiu. 2025. Qwen3 technical report. CoRR, abs/2505.09388. Diyi Yang. 2024. Human-ai interaction in the age of large language models. In Proceedings of the AAAI 2024 Spring Symposium Series, Stanford, CA, USA, March 25-27, 2024, pages 6667. AAAI Press. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. 2023. 10 React: Synergizing reasoning and acting in language models. In International Conference on Learning Representations (ICLR). Jing Ye, Lu Xiang, Yaping Zhang, and Chengqing Zong. 2025. Sweetiechat: strategy-enhanced roleplaying framework for diverse scenarios handling emotional support agent. In Proceedings of the 31st International Conference on Computational Linguistics, COLING 2025, Abu Dhabi, UAE, January 19-24, 2025, pages 46464669. Association for Computational Linguistics. Takuma Yoneda, Jiading Fang, Peng Li, Huanyu Zhang, Tianchong Jiang, Shengjie Lin, Ben Picker, David Yunis, Hongyuan Mei, and Matthew Walter. 2024. Statler: State-maintaining language models for embodied reasoning. In 2024 IEEE International Conference on Robotics and Automation (ICRA), pages 1508315091. IEEE. Pengfei Yu, Dongming Shen, Silin Meng, Jaewon Lee, Weisu Yin, Andrea Yaoyun Cui, Zhenlin Xu, Yi Zhu, Xingjian Shi, Mu Li, and Alex Smola. 2025. RPGBENCH: evaluating large language models as roleplaying game engines. CoRR, abs/2502.00595. Lin Zhang, Zihan Liu, Yuchen Zhou, Tong Wu, and Jintao Sun. 2024. Grounding large language models in real-world environments using imperfect world models. IJACSA) International Journal of Advanced Computer Science and Applications. Yanzhao Zhang, Mingxin Li, Dingkun Long, Xin Zhang, Huan Lin, Baosong Yang, Pengjun Xie, An Yang, Dayiheng Liu, Junyang Lin, Fei Huang, and Jingren Zhou. 2025. Qwen3 embedding: Advancing text embedding and reranking through foundation models. CoRR, abs/2506.05176."
        },
        {
            "title": "C More Variant Studies",
            "content": "Tables 5 and 6 summarize the statistics of the benchmarks and the resulting CDTs used in our experiments. The fine-grained Fandom benchmark spans eight artifacts with diverse narrative lengths and cast sizes, ranging from relatively compact series (e.g., Haruhi) to long-running storylines with large ensembles (e.g., AGOT and ATLA). The Bandori conversational benchmark covers all eight bands with balanced main-character counts, while the event-story extension substantially increases data scale, yielding over 77K actions in total. Across datasets, average action lengths remain stable, indicating consistent annotation granularity. Table 6 reports structural statistics of the induced CDTs. Despite large variation in data volume, the resulting trees remain shallow and compact, with moderate numbers of nodes and statements per character, reflecting the effectiveness of validation and pruning. Average statement lengths are consistent across artifacts and benchmarks, suggesting that CDT induces comparable, interpretable rule granularity in different narrative domains. These statistics further support that CDT scales to long storylines without uncontrolled growth in profile complexity."
        },
        {
            "title": "B Setup Details",
            "content": "In our experiments, we set θaccept = 0.75, θreject = 0.50, θf = 0.75, max depth = 4, Dmin for node growing = 16, 3 hypotheses per cluster, minimal 16 cases per cluster, and maximum of 8 clusters per node for all main experiments. For distillation in CDT-Lite, we train the deberta-v3-base discriminator with AdamW (Loshchilov and Hutter, 2019) initialized by 105 learning rate, 32 batch size for 1 epoch. For baselines, Fine-tuning adopts LoRA (Hu et al., 2022) fine-tuning with batch size 16, AdamW initialized by 2 104, which enables smooth loss drop. We report the result after the first epoch since the performance continues dropping while fitting on the first half storyline. RICL retrieves the scene with the 8 most similar scene based on qwen3-embedding-8b. ETA segment scene-action pairs by 16 samples in each block, with prompts available in Appendix 13. Human profile and codified human profiles setups follow the previous codification work (Peng and Shang, 2025) using the same distilled discriminator for CDT-Lite. The full ablation and variant study results are placed in Table 7, which reaches similar conclusion about component contribution, top statement selection strategy, and verbalization effect. RP Model Variants Table 8 reports results when varying the RP backbone while keeping CDTLite unchanged. We evaluate smaller RP models (1B and 3B) and compare CDT-Lite against Vanilla prompting and human-written Human Profiles. Across both the Fandom and Bandori benchmarks, CDT-Lite consistently delivers large gains over Vanilla and Human Profile for all model sizes, demonstrating that codified, situation-aware grounding is effective even with limited-capacity RP models. While absolute performance improves with stronger RP backbones, the relative advantage of CDT-Lite remains stable, indicating that its benefits are largely orthogonal to model scale. Notably, CDT-Lite with smaller RP models can approach or even surpass the performance of larger models grounded with human-written profiles, highlighting its practical value in resource-constrained settings. Codifier Variants Table 11 compares different codifiers used for trigger hypothesis and validation when constructing CDT-Lite. Using gpt-4.1 as the codifier generally yields the strongest performance, while gpt-4.1-mini or qwen3-coder achieves slightly lower but still competitive results on both benchmarks. The relatively small gap between these settings suggests that CDT-Lite is robust to codifier strength and can be instantiated with lighter or open-source models at reduced cost. These results further support the scalability of CDT, showing that high-quality codified profiles can be induced without relying exclusively on the strongest available LLMs. θaccept e θ 0.70 0.75 0.80 0.85 0.40 0.50 0. 69.28 69.13 66.05 69.74 70.16 67.82 69.35 70.53 69.33 68.82 69.54 68.89 #Hypothesis 2 3 4 68.53 69.46 70. 70.33 #Max Cluster 4 8 12 68.92 70.16 70.22 70.43 Table 10: CDT performance with different hyperparameter setups. Hyperparameter Setup Table 10 reports the sensitivity of CDT performance to different hyper-"
        },
        {
            "title": "Fandom",
            "content": "Haruhi K-On! #Main Character #Episode #Action #ActionMain Character #Avg. Action Length 5 28 991 781 12.15 5 57 2555 1882 10.51 SF 3 116 7688 3341 11. DN 5 108 5006 2738 13."
        },
        {
            "title": "FMA",
            "content": "5 108 3349 1351 12."
        },
        {
            "title": "JOJO",
            "content": "7 152 2958 1578 11."
        },
        {
            "title": "ATLA",
            "content": "11 73 12073 4859 12.42 4 61 8619 4248 11."
        },
        {
            "title": "PoPiPa",
            "content": "AG"
        },
        {
            "title": "RAS MyGO",
            "content": "#Main Character #Episode #Action #ActionMain Character #Avg. Action Length 5 20 1226 1080 13.18 5 20 1053 914 16.61 5 20 968 791 21.01 5 20 1079 873 21.95 5 20 1122 827 21. 5 20 1040 966 19.66 5 25 1183 795 15.64 5 41 2050 1620 16.48 Bandori (Events)"
        },
        {
            "title": "PoPiPa",
            "content": "AG"
        },
        {
            "title": "RAS MyGO",
            "content": "#Main Character #Episode #Action #ActionMain Character #Avg. Action Length 5 5 5 5 5 5 5 1498 77182 12553 19.63 11365 19. 12058 21.61 11821 21.07 10287 20.82 4758 20.73 2863 19.53 745 15. Table 5: Statistics of benchmarks (Fine-grained Fandom Benchmark and Bandori Conversational Benchmark) used in the experiments."
        },
        {
            "title": "Haruhi",
            "content": "K-On! SF DN #Node #Statement #Avg. Statement Length 3.80 16.80 18.03 32.80 90.80 19. 162.33 259.67 20.46 45.80 135.20 19."
        },
        {
            "title": "FMA",
            "content": "73.40 88.20 19."
        },
        {
            "title": "JOJO",
            "content": "32.14 50.14 18."
        },
        {
            "title": "ATLA",
            "content": "79.73 159.55 20.56 309.50 497.25 20.20 Bandori PoPiPa AG PasuPare Roselia HHW Monica RAS MyGO #Node #Statement #Avg. Statement Length 10.40 61.00 18.35 28.40 107.40 19.23 41.40 99.40 19.74 21.80 68.20 20. 5.20 27.40 18.23 52.00 112.00 18.37 59.00 114.60 19.05 9.80 82.40 18.66 Table 6: Statistics of CDTs used in the experiments. parameter configurations, evaluated on sampled 10% subset of the test set. We vary the trigger acceptance threshold θaccept, rejection threshold θreject, the number of hypotheses proposed per cluster, and the maximum number of clusters expanded at each node. Overall, performance is relatively stable across wide range of settings, indicating that CDT is not overly sensitive to precise hyperparameter tuning. We apply θaccept = 0.75, θreject = 0.50, 3 hypotheses per cluster, and maximum of 8 clusters per node for all main experiments, which achieves strong balance between performance and computational cost. 13 Bandori CDT-Lite w/ Abolished Statements t A i V dmax = 1 dmax = 2 dmax = 3 w/o Clustering w/o Inst. Embed. w/o Diversification TopK (Depth Rank) TopK (Accuracy Rank) TopK (Usability Rank) Verbalized CDT Wikified CDT PoPiPa AG PasuPare Roselia HHW Monica RAS MyGO Avg. 88.38 80.49 86.90 87.15 87.40 88.24 86.99 86.97 88.74 87.58 87.73 88.17 88.57 86. 76.35 77.23 79.79 79.84 79.38 77.66 80.24 76.78 81.26 81.28 79.92 75.92 82.47 78.69 82.19 82.85 81.76 80.67 78.78 80.97 79.57 81.62 83.76 81.32 82.56 72. 79.66 78.67 79.51 70.33 79.04 73.52 74.07 72.71 72.90 73.22 75.91 73. 71.77 72.65 75.19 75.18 73.24 78.52 77.88 79.49 79.63 80.52 78.16 79.13 78.93 79.82 78.92 78.39 81.21 77.44 77.96 78.44 78.97 78.21 79.16 78.26 75.77 76.19 78.84 78.01 77.14 79.54 76.97 79.40 79.06 77.43 78.93 78. 82.24 80.55 77.72 78.10 78.20 67.90 69.77 70.04 70.75 69.70 67.00 69.97 68.86 69.85 70.90 70.12 64.35 77.36 77.90 78.77 78.89 78.27 77.82 78.60 77.69 78.71 79.35 78.70 77.40 Table 7: Bandori part of ablation study and variant experiments. Fandom Haruhi K-On! Vanilla Human Profile CDT-Lite (Ours) Vanilla Human Profile CDT-Lite (Ours) 43.60 42.70 54.30 50.47 50.58 54. 44.20 49.31 53.88 49.73 52.87 53.75 Human Profile (8B) 55.87 55.86 SF 47.16 53.98 58.61 51.67 55.31 56.00 59.14 DN FMA JOJO AGOT ATLA 51.47 54.26 60.07 56.97 60.45 62.57 43.71 47.97 49.82 55.74 53.12 53. 41.45 49.42 52.49 49.57 50.96 49.74 64.75 58.54 55.11 42.56 49.14 50. 49.46 52.38 53.14 59.35 43.18 45.13 51.92 52.77 53.12 54.45 57.98 Bandori PoPiPa AG PasuPare Roselia HHW Monica RAS MyGO Vanilla Human Profile CDT-Lite (Ours) Vanilla Human Profile CDT-Lite (Ours) 58.38 65.34 78.58 63.91 71.96 79.13 56.44 62.90 80.05 62.27 68.13 77.72 Human Profile (8B) 73. 72.43 60.14 66.75 77.47 66.49 74.00 77.29 77.11 56.57 61.03 68.14 60.90 67.64 67. 58.50 65.76 78.90 64.63 70.64 79.26 56.06 55.56 73.42 63.06 65.16 74.18 70.08 73. 68.08 59.97 64.73 73.68 62.95 69.27 74.30 71.74 53.60 54.75 61.16 59.62 59.39 66. 63.91 Table 8: RP performance on variant RP models (1B and 3B), compared with key baselines. Fandom Haruhi K-On! gpt-4.1-mini gpt-4.1 qwen3-coder 60.45 62.17 61. 58.66 57.24 57.05 SF 59.07 59.79 58.83 DN FMA JOJO AGOT ATLA 67.27 67.00 66.39 59.19 59.04 56.60 55.35 57.26 56.88 62.06 64.27 61. 60.95 61.32 60.14 Bandori PoPiPa AG PasuPare Roselia HHW Monica RAS MyGO gpt-4.1-mini gpt-4.1 qwen3-coder 85.62 88.38 82.95 80.24 80.49 81.13 79.26 82.47 79.83 73.66 72.81 74.06 79.63 79.66 80. 77.80 78.67 77.37 77.49 79.51 78.57 73.10 70.33 71.09 Table 9: RP performance on variant codifiers, compared with key baselines."
        },
        {
            "title": "D Evaluation Generality Validation",
            "content": "contradicted while matching score has 89.00% and 90.50% consistency on matched and mismatched. This confirms that both NLI accuracy and the aggregated matching score serve as dependable proxies for RP quality. Open-domain Test We further include two types of open-domain evaluations to assess whether the induced CDTs generalize beyond the storylines on which they are trained. On the Fandom benchmark, we propose 200 starting scenes that initiate novel interactive settings (e.g., battles, casual conversations) between characters from the same artifact, and roll out 10 turns of interaction for each scene. CDT is compared against the Human Profile baseline, with responses evaluated by both an LLM judge (gpt-4.1) and human annotators. As shown in Figure 5, CDT is preferred over Human Profile in 38.0% versus 26.0% of cases by the LLM judge, and 39.0% versus 27.5% by human evaluators, indicating that CDT generalizes better to unseen interactive scenarios. Figure 4: The comparison between grounding methods with matching scores. (CDT represents CDT-Lite) Cross-metric and Manual Validation Figure 4 further shows that the NLI-based matching score is consistent with prior LLM-based multidimensional judgment protocols, validating its use as reliable automatic evaluator. Across both benchmarks, CDT exhibits clear and systematic gains on all evaluated dimensions, intent, emotion, content, stance, and causality, with particularly strong improvements on intent and causality, reflecting CDTs ability to condition behavior on situation-specific triggers rather than generic traits. Improvements in content and stance indicate that CDT grounding helps the RP model select contextually appropriate details and attitudes, while emotion gains suggest better alignment with character affect under varying scenes. We also verify that the trends observed in NLI scores closely match those of manual inspections: based on 200 samples for each class, the NLI discrimination achieves 90.50%, 92.00%, and 88.50% consistency with human judgment on entailed, neutral, and Figure 5: Comparison on open-ended RP by Human and LLM judgment. (CDT represents CDT-Lite) On the Bandori benchmark, we sample 100 sceneaction pairs per band member from the wild event stories (drawn from pool of 77.2K actions) and evaluate CDTs trained only on the first half of the main band stories. Table 11 shows that CDT consistently outperforms Vanilla prompting and Human Profiles across all eight bands, demonstrating robust out-of-domain generalization from curated storylines to heterogeneous, real-world conversational data. Together, these results confirm that codified, situation-aware profiles learned by CDT transfer effectively to novel scenes and interaction patterns beyond the original training distributions. 15 Bandori PoPiPa AG PasuPare Roselia"
        },
        {
            "title": "Metric",
            "content": "CDT-Lite Boosted CDT-Lite #Gen. Call #Disc. Call #Node a #Gen. Call #Disc. Call #Node n 58.47 40.62K 80.13 37.35 12.06K 28.50 23.28 9.04K 5.18 18.57 3.08K 5.82 Table 13: Training and Grounding efficiency of (Boosted) CDT-Lite per character. Vanilla Human Profile CDT 71.80 74.60 85.40 70.20 69.90 81.40 Bandori HHW Monica Vanilla Human Profile CDT 72.70 79.00 80. 69.60 70.00 81.10 68.60 73.70 85.10 RAS 72.20 74.90 81.10 70.60 73.50 74.50 MyGO 63.10 69.80 72.00 Table 11: Out-of-domain (OOD) RP performance evaluation with CDT trained on main band stories tested on wild event stories."
        },
        {
            "title": "E Cost and Efficiency",
            "content": "Boosted CDT To further improve efficiency, we introduce Boosted CDT, which retains only the most important hypothesized triggers (eight in our implementation) before validation. This reduces redundancy and focuses validation on the core behavioral logic, at the cost of omitting some finegrained details. As shown in Table 12, Boosted CDT achieves performance close to CDT-Lite on both Fandom and Bandori, while reducing the number of nodes in the tree by an order of magnitude. Training and Grounding Efficiency Table 13 summarizes the computational efficiency of CDTLite and Boosted CDT. key observation is that CDT construction is validation-heavy: the number of discrimination calls far exceeds the number of generation calls, since each hypothesized trigger must be tested against all sceneaction pairs. This motivates the use of distilled discriminators (e.g., deberta-v3-base) to replace expensive LLM validators. With distilled discrimination, CDT becomes practical to train at scale, as validation, which is the dominant cost, can be performed cheaply without significant performance degradation. The number of nodes in each CDT (Table 12, bottom rows) also approximates the maximal traversal steps needed during inference. Because traversal is performed via distilled discriminators rather than full LLM calls, its runtime cost is negligible compared to generating the final RP response. Consequently, CDT grounding remains lightweight at inference time, and Boosted CDT offers additional speedups in trade-off for lower accuracy by further shrinking the tree. 16 Fandom NLI #Node Bandori NLI #Node Haruhi K-On! Human Profile CDT-Lite Boosted CDT-Lite CDT-Lite Boosted CDT-Lite 55.87 62.17 61.70 3.80 1.60 55.86 57.24 57.19 32.80 6.60 SF 59.14 59.79 60.04 162.33 8.33 DN FMA JOJO AGOT ATLA 64.75 67.00 66.29 45.80 2.40 58.54 59.04 58.53 73.40 5.60 55.11 57.26 58. 32.14 4.71 59.35 64.27 61.42 79.73 4.91 57.98 61.32 60.00 309.50 10.00 PoPiPa AG PasuPare Roselia HHW Monica RAS MyGO Human Profile CDT-Lite Boosted CDT-Lite CDT-Lite Boosted CDT-Lite 73.73 88.38 86.96 10.40 5.80 72.43 80.49 76.19 28.40 6.60 77.11 82.47 79.81 41.40 9. 70.08 72.81 74.88 21.80 4.60 73.14 79.66 78.34 5.20 3.20 68.08 78.67 75.44 52.00 4. 71.74 79.51 78.29 59.00 10.40 63.91 70.33 69.74 9.80 1.80 Table 12: Further boosting the efficiency of CDT and resulted performance."
        },
        {
            "title": "F Profiling and Inference Cases",
            "content": "We visualize running cases of CDT behavior hypothesis construction in Figure 6, and the traversal (grounding) stage in Figure 7, using Haruhi Suzumiya as the instance. Figure 6 shows how CDT generalizes from clustered sceneaction evidence into reusable IFTHEN hypotheses. Each cluster groups semantically similar scenes with an observed action, and the LLM summarizes them into trigger conditions (IF) and behavioral statements (THEN). In this example, the hypotheses characterize Haruhi as someone who eagerly pursues unusual or unexplained phenomena, initiates group activities or investigations, and frequently assigns roles or tasks to other SOS Brigade members. The extracted rules are stored as nodes in the CDT for later retrieval. At inference time, Figure 7 illustrates traversal over the hypothesis tree given new scene. The system checks candidate IF conditions, selects those supported by the scene, and collects the corresponding THEN statements along the traversal path. These statements are merged into compact guidance set that constrains the response generation. In the running case, the merged guidance supports predicting Haruhi will actively steer the filming (e.g., instructing Kyon to modify scene to match her vision), grounded by an entailed reference action from the evidence (e.g., Haruhi using prop guns to drive pigeons toward Asahina). Wikification pipeline The CDT is first linearized into verbalized CDT. Then, the prompts in Figure 13 are used to propose chapters and fill in chapter contents to build the wikified CDT. 18 Figure 6: running case for behavior hypothesis in the CDT building. Figure 7: running case for tree traversal in the CDT grounding. Figure 8: Granularity and coverage comparison between human profile and CDT. (Full wikified CDT in Figure 9, 10, and 11) 19 Figure 9: Full wikified CDT for Haruhi Suzumiya (1/3) 20 Figure 10: Full wikified CDT for Haruhi Suzumiya (2/3) 21 Figure 11: Full wikified CDT for Haruhi Suzumiya (3/3)"
        },
        {
            "title": "G Prompts and Templates",
            "content": "We place the prompts used in the main experiment and analyses in Figures 12, 13, and 14 for result reproduction. 23 Figure 12: Prompts/Templates used in the main experiments for CDT. 24 Figure 13: Prompts/Templates used for baselines and variants. Figure 14: Prompts/Templates used for experiments in appendices. 26 Character & Artifact Background"
        },
        {
            "title": "Information",
            "content": "We place concise descriptions of artifacts and characters used in our experiments from Table 14 to 17."
        },
        {
            "title": "I LLM Usage Statement",
            "content": "We used large language models (LLMs) only for improving grammar, clarity, and copy-editing: overall writing style to enhance readability. The LLMs did not contribute to or modify the scientific content, core ideas, methodology, or experimental results. The authors take full responsibility for the manuscripts final content and accuracy. 27 Artifact Concise Abstract The Melancholy of Haruhi Suzumiya fast-paced school comedymystery in which the whims of the eccentric Haruhi unknowingly distort reality. Everyday club activities are interwoven with supernatural anomalies, while Kyons pragmatic narration provides stability amid escalating chaos. K-On! Fullmetal Alchemist JoJos Bizarre Adventure (Part 3) Spy Family Death Note Game of Thrones Avatar: The Last Airbender gentle, music-focused slice-of-life narrative portraying the everyday experiences of the Light Music Club. Rather than plot-driven conflict, the story highlights friendship, routine, and subtle personal growth through shared musical practice and leisure. serious fantasy adventure following two brothers who turn to alchemy to reclaim what they lost in forbidden ritual. The story blends action with ethical dilemmas, examining themes of sacrifice, human value, and the consequences of power within turbulent political world. worldwide supernatural journey in which Jotaro and his companions battle enemies using Stand abilities. The narrative is defined by inventive combat, strategic confrontations, and flamboyant character interactions that balance intensity with humor. genre-blending family comedy revolving around secret agent who forms fake household to complete covert mission. Espionage, action, and humor coexist with heartfelt moments, as hidden identities collide with genuine emotional bonds. psychological cat-and-mouse thriller in which gifted student gains the power to kill by writing names in supernatural notebook. The narrative explores justice, morality, and ego through intense intellectual duels between equally brilliant adversaries. large-scale political fantasy centered on competing noble houses locked in cycles of alliance, betrayal, and warfare. Personal ambition and moral uncertainty unfold against the backdrop of an approaching existential threat beyond human conflicts. An epic coming-of-age tale following Aang as he learns to master elemental powers to restore balance to the world. The series combines humor and action with emotional development, emphasizing responsibility, forgiveness, and personal identity. Table 14: Concise story descriptions of artifacts used in our experiments (Fine-grained Fandom Benchmark part). Band PoppinParty Afterglow Pastel*Palettes Roselia Concise Description bright, guitar-driven pop-rock band formed by high school friends, defined by upbeat melodies and an earnest, forward-chasing spirit. Their story emphasizes friendship, first dreams, and the steady growth that comes from practicing together and performing as team. straight-ahead rock band built on long-standing childhood bonds, carrying raw, livehouse energy and no-frills attitude toward music. Their narrative centers on loyalty, everyday honesty, and the tension between staying the same and growing up without breaking the groups core. An idol-style unit whose polished, cheerful image is sustained by behind-the-scenes effort, discipline, and constant on-the-job learning. The bands arc focuses on professionalism, teamwork under pressure, and the contrast between staged perfection and genuine personal struggle. gothic, high-intensity band pursuing perfect sound through rigorous practice, strong ambition, and uncompromising standards. Their storyline highlights artistic pride, conflict born from high expectations, and the hard-won trust required to perform at the highest level. Hello, Happy World! flamboyant, joy-first band that treats performance as mission to make the world smile, blending showmanship with playful chaos. Comedic set pieces coexist with sincere warmth, as the members eccentricities ultimately reinforce shared commitment to spreading happiness. Morfonica RAISE SUILEN MyGO!!!!! melodic rock group distinguished by the prominence of violin, combining refined textures with youthful sensitivity and introspective emotion. Their narrative explores confidence, talent gaps, and self-acceptance, as the band learns to transform insecurity into cohesive musical identity. hard-hitting rock/electronic hybrid centered on precision, speed, and stage dominance, built through deliberate recruitment and relentless rehearsal. The story foregrounds professionalism, creative control, and the frictionand eventual cohesionof strong personalities striving for the same peak. volatile, emotion-forward rock band whose sound is shaped by conflict, vulnerability, and the members difficulty with honesty and connection. Their arc focuses on miscommunication, fragile belonging, and the intense catharsis of turning personal pain into music and mutual commitment. Table 15: Concise band descriptions used in our experiments (Bandori Conversational Benchmark part). 28 r ! - F J T A A Haruhi Kyon Nagato Koizumi Asahina Yui Ritsu Mio Mugi Azusa An impulsive, hyperactive high school girl whose restless curiosity and odd worldview trigger the storys cascade of bizarre events. sardonic, level-headed student who narrates events and acts as Haruhi Suzumiyas reluctant yet stabilizing partner. silent, unreadable SOS Brigade member marked by extraordinary intellect and mysterious, otherworldly roots. An always-smiling transfer student and esper who aids the Brigade while carefully guarding critical secrets. timid, kind upperclassman conscripted into the SOS Brigade as their cute, enigmatic mascot, frequently dragged into their antics. The bubbly, scatterbrained lead guitarist of the light music club, whose boundless energyand sweet toothkeeps the band moving. The boisterous, prank-loving drummer whose playful antics and casual leadership keep the group upbeat and united. shy but highly capable bassist, gentle at heart and blessed with sharp musical sensitivity. Tsumugi Kotobuki, kind, affluent keyboardist who loves pampering her friends and making club life feel luxurious. hardworking, gifted junior guitarist who soon becomes essential to the clubs tight sound and practice habits. Edward gifted, stubborn young alchemist who journeys to recover his and his brothers bodies after catastrophic transmutation. Alphonse gentle, big-hearted boy whose soul dwells in hulking suit of armor, traveling with his brother to regain what they lost. talented automail mechanic and the Elrics childhood friend, renowned for her technical skill and steadfast compassion. charismatic, driven State Alchemist and master of flame, intent on reshaping the military from the inside. charismatic, relentless prince from Xing who pursues immortality while carrying heavy duty to his nation. Winry Roy Ling Jotaro stoic, seemingly unshakable high schooler and Stardust Crusaders lead, famed for Star Platinum and iron resolve. Polnareff bold, flamboyant French swordsman who allies with the Crusaders, fighting through the swift Stand Silver Chariot. Joseph DIO Kakyoin Avdol Iggy fast-thinking, over-the-top Joestar whose schemes and bravadoYour next line is. . . repeatedly flip battles in his favor. magnetic, utterly ruthless vampire whose towering ambition and cry of Za Warudo! cement him as legendary foe. composed, analytic ally in Stardust Crusaders, battling with Hierophant Green, Stand that attacks with emerald blasts. wise, steadfast Egyptian Stand user whose Magicians Red commands fierce flames and unshakable backing. grumpy Boston Terrier Stand user with fondness for coffee gum, whose reluctant heroics turn out to be vital. Light Near Misa Mello Loid Yor Anya brilliant, idealistic student who acquires the Death Note and resolves to reshape the world through absolute, lethal justice. An eccentric, reclusive genius detective whose unconventional methods and sharp intuition pit him directly against Kira. calm, analytical prodigy who succeeds L, relying on detached logic and meticulous planning to pursue the truth. devoted idol and second Kira, driven by love and gratitude, whose impulsive loyalty complicates the deadly mind games. volatile, fiercely competitive successor to who embraces risk and criminal alliances to outmaneuver his rivals. An elite undercover agent who assembles fake family for high-stakes mission, balancing espionage with improvised parenthood. soft-spoken civil servant secretly working as lethal assassin, struggling to reconcile her double life with domestic normalcy. cheerful, telepathic child who knows everyones secrets, holding the family together through innocence and quiet insight. Tyrion The razor-witted youngest Lannister, Tyrion navigates Westerosi politics with wit, nerve, and dark humor despite lifetime of scorn for his size. Daenerys An exiled Targaryen princess who starts as hesitant pawn and evolves into determined, power-claiming ruler. Cersei Jaime Robb Eddard Arya Catelyn Sansa Jon Bran Aang Katara Sokka Zuko An ambitious, scheming queen whose beauty conceals ruthless devotion to her family and grip on power. The notorious Kingslayercharming, deadly, and deeply conflictedwhose sworn duties and loyalties are tangled and fraught. The dutiful heir of Winterfell, pushed too soon into command and responsibility by his familys misfortune. The resolute Lord of Winterfell, man of stern honor who serves as Warden of the North. fiercely independent Stark girl who casts off courtly roles in favor of freedom, training, and the blade. The determined Lady of Winterfell, driven by fierce maternal loyalty and firmly practical mind. The elder Stark daughter, cherished for grace and manners, whose romantic dreams collide with brutal reality. Eddards brooding illegitimate son, raised at Winterfell and driven by questions of identity, duty, and quiet resolve. curious young Stark whose devastating fall thrusts him onto an unforeseen and fateful journey. The final Airbender and hesitant Avatar, playful at heart yet burdened with restoring balance to world in war. determined, compassionate waterbender from the Southern Tribe who grounds the group and refuses to tolerate injustice. wisecracking, inventive warrior whose boomerang skills and ingenuity repeatedly end up saving the day. An exiled Fire Nation prince, driven by burning quest for honor that gradually turns into search for new self. Table 16: Simple background information of characters in our experiments (Fandom Benchmark part). 29 P A a a l R a o R M Kasumi Tae Rimi Saaya Arisa Ran Moca Himari Tomoe Tsugumi Aya Hina Chisato Maya Eve Yukina Sayo Lisa Ako Rinko Kokoro Kaoru Hagumi Kanon Misaki Mashiro Touko Nanami Tsukushi Rui CHU2 LAYER LOCK An upbeat, starry-eyed vocalistguitarist whose impulsive enthusiasm pulls people together and kicks off the bands journey. free-spirited lead guitarist with strong technique and quirky instincts, often drifting at her own pace yet boosting the bands sound. shy, gentle bassist who grows braver through performance, bringing careful support and warm sincerity to the group. dependable drummer with caring, family-first mindset, acting as the bands steady backbone in both practice and life. sharp-tongued but reliable keyboardist whose practicality and quick thinking keep the band organized, grounded, and moving forward. blunt, prideful vocalistguitarist who values authenticity, carrying the bands straightforward rock spirit and stubborn resolve. laid-back lead guitarist with mischievous streak, masking keen observation and musical confidence behind casual teasing. bright, encouraging bassist and nominal leader, energizing the group with optimism while trying to hold everyone together. reliable, big-sister drummer who supports others through calm strength, stepping up whenever the band needs stability. kind keyboardist with gentle, practical touch, often mediating tensions and keeping the groups everyday rhythm intact. relentlessly earnest vocalist who chases the idol dream through effort and persistence, learning confidence by doing the work. cheerful, genius guitarist who loves fun above all, acting on bright ideas with little hesitation and lots of momentum. cool, realistic bassist with strong professionalism, frequently reining in chaos while protecting the groups long-term direction. drummer with deep audio-gear passion and technical know-how, becoming animated when music setups and stage craft are involved. sincere keytarist devoted to bushido, whose wholehearted intensity and kindness can be both inspiring and unexpectedly disruptive. fiercely driven vocalist who pursues perfect sound, pushing herself and others with uncompromising standards and focus. serious, disciplined guitarist who relies on hard work over flair, expressing care through responsibility and relentless practice. warm, attentive bassist who acts as the bands emotional glue, balancing high ambition with everyday empathy and reassurance. high-energy drummer with dramatic, chuuni-tinged flair, bringing loud confidence while still craving recognition and growth. shy, soft-spoken keyboardist with exceptional skill, gradually building courage through supportive bonds and shared performances. wealthy, fearless optimist who treats making people smile as mission, turning wild ideas into surprisingly sincere action. theatrical guitarist who plays the prince role with flourish, using charm and melodrama to lift the mood around her. sunny, energetic bassist with an athletic, straightforward vibe, often charging ahead with honest excitement and big smiles. timid but kind drummer who constantly pushes past fear, finding bravery through small steps and friends who believe in her. pragmatic, overworked coordinator (and DJ) who keeps the group functional, often acting as the lone realist amid cheerful chaos. sensitive vocalist and lyricist who struggles with insecurity, slowly learning to voice her feelings through song and companionship. flashy, extroverted lead guitarist who loves attention and momentum, bringing brightness while occasionally stirring trouble by impulse. multi-talented bassist fixated on being normal, masking inner conflict with humor and adaptability across many situations. hardworking drummer and leader who tries to be dependable, persisting through clumsiness with determination and care for the team. cool, perfection-driven violinist and composer who prioritizes results, gradually confronting the role of emotion and trust in music. demanding genius DJ/producer who builds the band with strict control and ambition, driving everyone toward professional-level stage. sharp, charismatic bassistvocalist whose powerful presence and steady musicianship anchor the bands sound under intense expectations. young, earnest guitarist who grows through pressure and mentorship, balancing admiration with the need to prove her own worth. MASKING fearless, high-impact drummer who thrives on adrenaline and volume, powering performances with wild confidence and physical intensity. PAREO devoted keyboardist with shy core and idol-like polish, channeling loyalty and effort into supporting the bands vision. Tomori Anon Raana Soyo Taki withdrawn, highly sensitive vocalist and lyricist who clings to words for connection, turning pain and longing into songs. social, image-savvy rhythm guitarist who wants to belong and be seen, learning sincerity as her confident front gets tested. freewheeling lead guitarist with mysterious, playful calm, following curiosity and sound first while ignoring most social rules. gentle, composed bassist who tries to keep harmony, often caught between caring intentions and the pressure of unresolved history. blunt, intense drummer and composer whose strict standards hide protectiveness, expressing concern through sharp honesty and persistence. Table 17: Simple background information of characters in our experiments (Bandori Benchmark part)."
        }
    ],
    "affiliations": [
        "University of California, San Diego"
    ]
}