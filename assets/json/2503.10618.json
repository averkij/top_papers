{
    "paper_title": "DiT-Air: Revisiting the Efficiency of Diffusion Model Architecture Design in Text to Image Generation",
    "authors": [
        "Chen Chen",
        "Rui Qian",
        "Wenze Hu",
        "Tsu-Jui Fu",
        "Lezhi Li",
        "Bowen Zhang",
        "Alex Schwing",
        "Wei Liu",
        "Yinfei Yang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "In this work, we empirically study Diffusion Transformers (DiTs) for text-to-image generation, focusing on architectural choices, text-conditioning strategies, and training protocols. We evaluate a range of DiT-based architectures--including PixArt-style and MMDiT variants--and compare them with a standard DiT variant which directly processes concatenated text and noise inputs. Surprisingly, our findings reveal that the performance of standard DiT is comparable with those specialized models, while demonstrating superior parameter-efficiency, especially when scaled up. Leveraging the layer-wise parameter sharing strategy, we achieve a further reduction of 66% in model size compared to an MMDiT architecture, with minimal performance impact. Building on an in-depth analysis of critical components such as text encoders and Variational Auto-Encoders (VAEs), we introduce DiT-Air and DiT-Air-Lite. With supervised and reward fine-tuning, DiT-Air achieves state-of-the-art performance on GenEval and T2I CompBench, while DiT-Air-Lite remains highly competitive, surpassing most existing models despite its compact size."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 1 ] . [ 1 8 1 6 0 1 . 3 0 5 2 : r DiT-Air: Revisiting the Efficiency of Diffusion Model Architecture Design in Text to Image Generation Chen Chen, Rui Qian, Wenze Hu, Tsu-Jui Fu, Lezhi Li, Bowen Zhang, Alex Schwing, Wei Liu, Yinfei Yang Apple Inc. {chen chen999}@apple.com"
        },
        {
            "title": "Abstract",
            "content": "In this work, we empirically study Diffusion Transformers (DiTs) for text-to-image generation, focusing on architectural choices, text-conditioning strategies, and training protocols. We evaluate range of DiT-based architectures including PixArt-style and MMDiT variantsand compare them with standard DiT variant which directly processes concatenated text and noise inputs. Surprisingly, our findings reveal that the performance of standard DiT is comparable with those specialized models, while demonstrating superior parameter-efficiency, especially when scaled up. Leveraging the layer-wise parameter sharing strategy, we achieve further reduction of 66% in model size compared to an MMDiT architecture, with minimal performance impact. Building on an in-depth analysis of critical components such as text encoders and Variational Auto-Encoders (VAEs), we introduce DiT-Air and DiT-Air-Lite. With supervised and reward fine-tuning, DiT-Air achieves state-of-theart performance on GenEval and T2I CompBench, while DiT-Air-Lite remains highly competitive, surpassing most existing models despite its compact size. 1. Introduction The field of text-to-image synthesis has witnessed remarkable progress, primarily attributable to the wide adoption of diffusion-based models [14, 20]. Diffusion Transformers (DiTs) [33] have emerged as prominent architectural paradigm, combining the iterative denoising process inherent to diffusion models with the representational efficacy of transformer networks. While existing paradigm variants like PixArt-style models [35] and MMDiT from Stable Diffusion 3 (SD3) [14] have demonstrated strong performance, key aspects of DiTssuch as the choice of architectural components, text-conditioning mechanisms, and training strategieshave not been exhaustively explored t better Figure 1. Comparison of text-to-image generation methods on two metrics, GenEval and T2I CompBench (higher is better for both). Despite significantly smaller model size, our proposed DiTAir achieves state-of-the-art results. Note that, for our model, we report the full model size including text encoder and VAE. detailed parameter breakdown is provided in Appendix G. yet [6, 17]. In this work, we conduct comprehensive investigation into DiT design choices for text-to-image synthesis. Beginning with comparative analysis of the vanilla DiT [33], PixArt-α [4], and MMDiT [14], we develop streamlined architecture. This architecture utilizes concatenated text and noise inputs (following MMDiT) and shared AdaLN parameters (following PixArt-α), eliminating modality-specific projections. This simplification yields substantial parameter savings (66% compared to MMDiT and 25% compared to PixArt-α), while preserving or enhancing performance. Notably, the resulting architecture, named DiT-Air, closely resembles the original DiT, allowing us to leverage existing transformer optimizations. Inspired by parameter-sharing strategies in NLP models such 1 Figure 2. Sample images from our proposed DiT-Air, each with the text prompt below it. See Appendix for more examples. as ALBERT [27], we adopt both full block-sharing and attention-only sharing schemes to further push the parameter efficiency of DiT models. Our ablation studies demonstrate that attention-only sharing provides compelling trade-off, achieving significant parameter reduction with minimal loss in text alignment and generative fidelity. In addition to architectural innovations, DiT-Air benefits from thorough analysis of text-conditioning strategies and variational autoencoders (VAEs). Specifically, we evaluate three primary types of text encoders: CLIP, large language models (LLMs), and the T5 model. Our study includes comprehensive ablations of causal versus bidirectional CLIP, layer selection strategies for both CLIP and LLMs, and final performance comparison of all three encoders. We also introduce refined variational autoencoder (VAE) [24] that better preserves fine-grained visual details, further boosting image quality, especially complex visual features. Finally, with progressive training approach, DiT-Air achieves new state-of-the-art GenEval [15] and T2I CompBench [22] scores of 82.9 and 59.5, respectively. As shown in Figure 1, our model delivers superior performance with outstanding parameter efficiency compared to leading models such as SD3, FLUX, and JanusPro. Example generation results are provided in Figure 2. Our key contributions are as follows: (i) We systematically study the design choices of range of DiT-based architectures including PixArt-style and MMDiT variants. (ii) We introduce DiT-Air and DiT-Air-Lite, novel DiT model family that simply extends standard DiT by directly processing concatenated text and noise inputs. (iii) We demonstrate parameter efficiency, achieving 66% model size reduction with minimal performance impact compared to the state-of-the-art MMDiT. (iv) We establish new stateof-the-art performance on GenEval [15] and T2I CompBench [22]. 2. Related Works 2.1. Text-to-Image Diffusion Models Diffusion models [11, 20, 41] have achieved remarkable success in text-to-image generation [37, 39]. These models generate images by iteratively denoising random noise, guided by semantic text embeddings extracted from pretrained text encoders, such as CLIP [36]. Latent diffusion methods [34, 38] further enhanced training and inference efficiency by operating in latent space defined by pretrained variational autoencoder (VAE) [24], reducing computational costs without sacrificing quality. Recently, flow matching objectives [31, 32, 43] have been introduced to connect source and target image distributions through simplified paths, offering further gains in image quality [14]. Our approach builds upon this paradigm by leveraging pretrained text encoders for conditioning and optimizing conditional flow matching objective in latent space. 2.2. Diffusion Transformers and Text Conditioning Diffusion Transformers (DiTs) [33] were initially proposed to extend the advantages of transformer architectures to class-conditional image generation within diffusion models. Built upon vanilla vision transformers [13], widely utilized in image understanding tasks, DiT explored various approaches for incorporating noise level (time) and class label conditions. The study compared zero-initialized adaptive layer normalization (AdaLN), cross-attention, and in-context conditioning, demonstrating that AdaLN was the most effective strategy through comprehensive experiments. Subsequent works extended DiTs to text-to-image generation, establishing them as popular choice for high-quality open-source models [4, 14, 26]. Among them, PixArt models use cross attention between fixed text embeddings and image features after each of the self attention layers to inject text conditions into transformer models. MMDiT expands the transformer to dual-stream design, with separate query, key, value, and output (QKVO) projections and MLPs for text embeddings and image features. Text and image features interact via scaled dot product attention applied 2 on concatenated feature sequences. Compared to vanilla DiTs with similar number of layers and feature dimensions, both PixArt and MMDiT expand the model size and use the extra parameters to convert semantically rich text embeddings into the visual space. To justify these extra complications for text conditioning, the SD3 paper [14] reports that MMDiT outperforms several DiT variants. However, this study was conducted on the relatively small CC12M [2] dataset and at smaller model scales, without fully accounting for the additional parameters introduced by MMDiTs dual-stream architecture. Moreover, scaling law study by Liang et al. [29] compared cross-attention with in-context conditioning but excluded AdaLN and focused only on models with 1M to 1B parameters, limiting its applicability to larger-scale settings. Through large-scale experiments, we demonstrate that our DiT-Air, which adheres closely to the simpler vanilla DiT architecture, can achieve comparable or superior performance to these more complex models, particularly at scale. Parameter Sharing in Transformers. Parameter sharing has emerged as an effective approach to enhance efficiency in transformer-based architectures. For instance, ALBERT [27] demonstrates that sharing parameters across layers in BERT-like model can substantially reduce model size while retaining competitive performance on NLP tasks. The parameter sharing approach in our proposed diffusion model architecture (Section 3.3.1) is inspired by this work, and offers new avenues for balancing generative quality and model compactness in DiT architectures. 3. Architecture Design 3.1. Latent Diffusion Framework typical text-to-image generation pipeline based on latent diffusion encodes the target image into latent representation z0 Rhwc via variational autoencoder (VAE), i.e., z0 = EVAE(x). Meanwhile, the text prompt is processed by one or more pretrained text encoders (e.g., CLIP, T5), and the resulting embeddings are projected to produce token embeddings Rltextd. During training (see Figure 3), forward diffusion process adds noise to z0, producing noisy latent zt for randomly sampled (0, 1). The model fθ then learns to reverse this process by predicting target quantity such as noise ϵ in denoising models, velocity in vprediction models, or vector field in flow-matching modelsconditioned on the current latent zt, the text embedding c, and the timestep t: ˆyt = fθ(zt, c, t). The training objective minimizes the discrepancy between ˆyt and the true target, effectively denoising zt. At inFigure 3. Overview of Latent Diffusion Training. During training, is encoded into latent z0 via VAE, and the text prompt is mapped to embeddings c. forward diffusion adds noise to z0, and the model learns to reverse this process by predicting the noise (or similar target) at each timestep. ference, fθ iteratively transforms random sample zT back to clean latent ˆz0, which is then decoded by the VAE to produce the final image. 3.2. Text-conditioned Diffusion Transformers In this section, we examine the backbone diffusion model fθ, specifically focusing on two widely-used Diffusion Transformer (DiT) variants: PixArt-α and MMDiT. PixArt-α. PixArt-α (Figure 4a) follows two-step attention process: (1) self-attention on patchified visual tokens, and (2) cross-attention to fixed text embeddings c. These text embeddings remain the same across all layers, providing global conditioning signal for generation. MMDiT. MMDiT (Figure 4b) adopts dual-stream approach in each transformer block: text and visual tokens have separate query, key, value, and output (QKVO) projections. After computing self-attention over the concatenated sequence, tokens from each modality are processed by separate MLPs, preserving modality-specific transformations. This design supports rich multimodal interactions but substantially increases the parameter count. Table 1 compares the main parameter components for PixArt-α and MMDiT, highlighting that MMDiT consumes significantly more parameters due to: Per-layer AdaLN: MMDiT uses independent AdaLN parameters in each layer, while PixArt-α shares them. Dual-Stream Design: MMDiT duplicates QKVO and MLPs for text and image tokens, effectively doubling related parameter sets. 3 (a) PixArt-α (b) MMDiT (c) DiT-Air Figure 4. Comparison of Diffusion Transformer Architectures. Element-wise operations are denoted by , and sequence-wise operations by . The details of inputs c, z, can be found in Figure 3. PixArt-α relies on sequential selfand cross-attention, whereas MMDiT uses dual-stream approach with separate parameters for text and image tokens. Our proposed DiT-Air resembles vanilla DiT that processes concatenated text and noises. Table 1. Parameter counts for PixArt-α, MMDiT, DiT-Air, and DiT-Air-lite (full vs. QKVO). represents the effective embedding size in transformer blocks (i.e., the models hidden dimension), and denotes the number of layers. Our DiT-Air is compact DiT with shared dual-stream AdaLN, hence saves parameters significantly."
        },
        {
            "title": "Component",
            "content": "PixArt-α MMDiT DiT-Air AdaLN Self-MHA Cross-MHA MLP Total 6d2 4N d2 4N d2 8N d2 12N d2 8N d2 16N d2 12d2 4N d2 8N DiT-Air-Lite (full) (attention) 12d2 4d2 8d2 12d2 4d2 8N d2 (6 + 16N )d2 36N d2 (12 + 12N )d2 24d2 (16 + 8N )d 3.3. DiT-Air: Compact Text-to-Image DiT In this section, we introduce DiT-Air, compact DiT Inarchitecture tailored for text-conditioned generation. stead of having the dual-stream approach of MMDiT, DiTAiremploys unified set of QKVO projections and MLPs for both text and image tokens, offering streamlined yet effective design (see Figure 4c). For adaptive layer normalization (AdaLN), DiT-Air combines the dual-stream AdaLN from MMDiT with the parameter-sharing strategy of PixArt-α. By sharing AdaLN parameters across all layers, DiT-Air effectively limits parameter growth as model depth increases, achieving balanced trade-off between efficiency and multimodal capac4 ity. Notably, this approach maintains constant parameter overhead for AdaLN, irrespective of model depth. As shown in Table 1, DiT-Airs design reduces the parameter count by approximately 24N d2 compared to MMDiT, while preserving the same computational complexity (FLOPs). Overall, DiT-Air consumes about 66% fewer parameters than MMDiT and 25% fewer than PixArtα for large . However, two open questions remain: Does sharing AdaLN across all layers adversely affect image quality? How does merging text and image streams influence text alignment and fidelity? We investigate these aspects in Section 4. Further, we also explore whether parameter savings can be pushed even more aggressively, as described next. 3.3.1. DiT-Air-Lite While DiT-Air strikes balance between multimodal capacity and parameter efficiency, some applications (e.g., largescale inference or deployment on resource-constrained hardware) demand even greater reductions in parameter count. Inspired by the shared AdaLN and ALBERT [27], which shares parameters across layers to reduce model size, we propose DiT-Lite to further reduce parameters by sharing Transformer block parameters across layers, either entirely or partially (see Table 1). Performance comparison between MMDiT variants Table 2. with and without sharing AdaLN. Both models are based on the MMDiT/B sized configuration."
        },
        {
            "title": "Model",
            "content": "Params Val. FID CLIP Pick GenE. Aesth. T2I. Per-layer AdaLN 902M 0.422 Shared AdaLN 631M 0.422 14.7 15.0 32.9 32.9 20.28 20.28 69.8 69. 5.62 5.59 51.1 51.0 In this variant, the entire Full Block-Sharing Variant. Transformer block (QKVO projections and MLP) is shared across all layers. This reduces the overall parameter count to 24d2, making the model extremely compact. However, it also limits representational diversity, since every layer applies the exact same transformation, which can degrade performance on complex prompts. Attention-Sharing Variant. more moderate approach involves sharing only the QKVO projections while maintaining distinct MLP for each layer. As shown in Table 1, this partial-sharing strategy reduces the parameter count by approximately 33% compared to the non-shared version, while generally preserving higher fidelity and better text alignment than the fully shared configuration. Having dedicated MLPs for each layer allows the model to capture depth-specific nuances in the text-to-image mapping. 4. Experiments In this section, we evaluate the impact of architectural variations and design choices using standardized training and evaluation protocol. Specifically, by fixing other components (e.g., VAE, text encoder), we isolate the effects of attention mechanisms and parameter utilization, allowing for clear analysis of each architectural change. 4.1. Experimental Setup We outline the dataset, training setup, model scaling strategy, and evaluation metrics used in our experiments. Data: We conduct all ablations on in-house data containing 1.5 billion text-image pairs. Following DALLE 3 [1], we enrich the dataset using synthetic captions generated by pretrained captioning model. To balance real and synthetic data, we adopt 1:9 ratio between original and synthetic captions. Unlike prior work that often relies on smaller subsets, all ablation studies are performed on the full dataset, ensuring more reliable assessment of real-world performance. Training and Inference: To ensure fair and consistent comparisons across experiments, we standardize key components: all models use shared in-house variational autoencoder (VAE) and an in-house CLIP-H model as the default text encoder unless stated otherwise. Implementation details for these components are provided in the Appendix A. All models are trained using flow-matching objective, optimized with AdaFactor at fixed learning rate of 1e-4, and global batch size of 4096 for 1 million steps. For inference, we employ Heun SDE solver [23] with 50 sampling steps and classifier-free guidance scale of 7.5. For model scaling experiments, we evaluate models at five specifications: S, B, L, XL and XXL, which correspond to 12, 18, 24, 30, and 38 transformer layers, respectively. The hidden dimension scales proportionally with depth as = 64 nlayer, following the scaling strategy of SD3 [14]. Unless otherwise specified, all ablation studies use the B-size model. All experiments are done using the axlearn framework.1 Evaluation: We evaluate model performance using combination of validation loss and diverse set of established benchmarks. Following SD3 [14] and MovieGen [35], we report validation loss on both our in-house dataset. While validation loss provides general measure of model fit, it may not accurately capture text alignment performance, particularly in complex generative tasks. As discussed further in Section 5.1, this limitation underscores the importance of incorporating additional metrics that directly evaluate alignment and compositionality. Therefore, we include Frechet Inception Distance [19] on COCO30k [30], CLIPScore [18, 36], and PickScore [25] on MJHQ30k [28], along with GenEval [15], T2I CompBench [21] and LAIONAesthetics Predictor V2 (Aesthetics) [40]. For multicategory benchmarks such as GenEval and T2I CompBench, we report the overall average in the main paper with detailed per-category results deferred to the Appendix. 4.2. Adaptive Layer Normalization Sharing As DiT-Air can be viewed as simplification of MMDiT by unifying text and image streams, we start with investigating the impact of sharing AdaLN parameters across layers on the quality of generated images by comparing two MMDiT variants: one with shared AdaLN parameters and one without. The results are presented in Table 2. As shown, the variant with shared AdaLN parameters is more parameter-efficient, reducing the model size from 902M to 631M. The performance differences between the two variants are minimal, with only slight variations in validation loss and other evaluation metrics, all of which fall within the error bounds. Thus, we conclude that sharing AdaLN parameters improves parameter efficiency without significantly affecting image quality. 4.3. Scaling and Efficiency In this section, we evaluate the performance and efficiency of three architectures: PixArt-α, MMDiT, and DiT-Air, for 1https://github.com/apple/axlearn Table 3. Comparison of DiT-Air (baseline) with DiT-Air-lite (full) and DiT-Air-lite (attention). The Full configuration provides the largest parameter reduction but suffers noticeable performance drop, while Attention strikes more favorable trade-off between compactness and text alignment."
        },
        {
            "title": "Model",
            "content": "Params Val. FID CLIP Pick GenE. Aesth. T2I. DiT-Air/B DiT-Air/B-lite (full) DiT-Air/B-lite (attention) 321M 0.428 16.0 49M 0.461 14. 32.8 31.4 20.2 19.5 70.4 58. 5.58 5.36 51.4 47.6 230M 0.431 17. 32.5 20.1 66.9 5.50 49.6 ably more parameters. This highlights DiT-Airs strength in maintaining competitive performance without the overhead of excessive scaling. No single metric fully captures all aspects of text-toimage alignment and quality, underscoring the importance of evaluating models across diverse set of benchmarks. Overall, the results establish DiT-Air as compelling choice for large-scale text-to-image generation, offering wellbalanced mix of efficiency and performance. 4.4. DiT-Air-Lite Ablation Having established in Section 4.3 that DiT-Air (DiT with shared AdaLN) compares favorably to PixArt-α and the more agMMDiT, we now evaluate DiT-Air-Lite, gressive parameter-sharing extension introduced in Section 3.3.1. We consider two configurations: Full BlockSharing and Attention-Sharing. Full Block-Sharing reuses entire block across all layers, whereas Attention-Sharing keeps distinct MLPs per layer but shares QKVO. Table 3 compares both DiT-Air-lite variants against the baseline. These results confirm that sharing the entire block minimizes parameters aggressively, but at the cost of lower text alignment and aesthetics. In contrast, attention-only sharing still yields substantial parameter savings while incurring only modest performance drops. Overall, DiT-Air-lite (attention) emerges as favorable option when computational and memory constraints are critical, yet text alignment and image quality must remain high. 5. Text Encoders and VAEs In this section we further investigate the impact of other critical components in the text-conditioned image generation task: text encoders and VAEs. 5.1. Text Encoder Ablation We investigate how different text encoders impact the text alignment and overall performance of the DiT-Air architecture. Our study evaluates three primary encoder types: CLIP, T5, and Large Language Models (LLMs). Both the CLIP and LLM models are internal implementations, while Figure 5. Validation Loss vs. Model Size for PixArt-α, MMDiT, and DiT-Air. The plot illustrates the scaling behavior of three architectures across model sizes ranging from to XXL, where the model size refers only to the diffusion transformer component (excluding the text encoder and VAE). The x-axis is in logarithmic scale, and the fitted lines depict the scaling trend using the formula = Sb. Among the three, DiT-Air achieves the best parameter efficiency. various model scales from to XXL (150M to 8B parameters). Our analysis focuses on validation loss and benchmark performance to highlight the relationship between model size, parameter efficiency, and architecture. Validation Loss and Scaling Behavior. Figure 5 illustrates the validation loss versus model size for three architectures. DiT-Air demonstrates the best parameter efficiency, largely due to its shared AdaLN parameters across layers and its single-stream design for QKVO and MLPs. Among the three models, DiT-Air exhibits the steepest scaling curve, reflecting the most efficient reduction in validation loss as model size increases. Notably, at the scale, DiT-Airs validation loss is considerably higher than that of MMDiT; however, as the models scale up to XXL, the loss gap vanishes. PixArt-α follows scaling trend similar to DiT-Airwith slope substantially steeper than that of MMDiTbut for fixed parameter budget, DiT-Air consistently achieves lower validation loss, underscoring its balanced approach to parameter efficiency and performance. Benchmark Performance. Figure 6 compares benchmark performance across all three architectures and model scales. Overall, DiT-Air demonstrates consistently strong performance with significantly higher degree of parameter efficiency. Notably, DiT-Air achieves some of the lowest FID scores while performing on par or slightly better than MMDiT and PixArt-α in PickScore and GenEval. Although MMDiT exhibits higher Aesthetics scores at larger scales and PixArt-α achieves strong CLIPScores, these gains often remain within the error bounds or necessitate consider6 Figure 6. Benchmark Performance Across Model Scales. The plots compare PixArt-α, MMDiT, and DiT-Air across six evaluation metrics. DiT-Air demonstrates strong parameter efficiency, achieving competitive performance with fewer parameters. The x-axis is in logarithmic scale, and error bounds are indicated where applicable. the T5 encoder utilizes the open-sourced T5-XXL model. The analysis includes ablations of causal vs. bidirectional CLIP, layer selection strategies for both CLIP and LLMs, and final comparison of all three encoders. Detailed ablation results are provided in Appendix C. Summary of Key Findings. Our experiments demonstrate that the bidirectional CLIP model consistently outperforms its causal counterpart, showing improved text alignment and image quality across benchmarks. This improvement is attributed to the better synergy between the bidirectional attention in the text encoder and the diffuison transformer. Layer selection experiments, detailed in Appendix C.1.1, indicate that while very shallow layers in the CLIP model underperform, deeper layers yield comparable results.2 For LLM-based encoders, we find that text-only LLMs outperform their multimodal counterparts, particularly in the GenEval metric. Layer selection studies in LLMs reveal that using middle layer combined with layer at approximately 3/4 of the models depth offers the best performance. This behavior likely stems from the LLMs pretraining objective, which is typically next-token prediction. Such an objective emphasizes fine-grained, token-level information at deeper layers, potentially at the expense of broader semantic understanding, which is more beneficial for text-toimage alignment. 2In this paper, shallow layers always refer to those near the input of the text encoder, while deep layers are closer to the output, with depth consistently measured from input to output. Table 4. Comparison of CLIP, LLM, and T5 text embeddings. CLIP achieves superior results across most metrics, while LLM excels in GenEval. In contrast, T5 consistently underperforms."
        },
        {
            "title": "Model",
            "content": "Val. FID CLIP Pick GenE. Aesth. T2I. CLIP (Bidirectional) 0.428 0.427 LLM (Text-only) 0.424 T5-XXL 16.0 16.0 17.5 32.8 32.0 31.8 20.3 20.1 20.0 0.704 0.726 0. 5.58 5.57 5.46 51.4 48.6 48.0 Comparison of CLIP, LLM, and T5. The final comparison of text encoders  (Table 4)  shows that bidirectional CLIP achieves the best performance across most benchmarks, with text-based LLMs also performing strongly, especially in GenEval. The T5-XXL model, while achieving the lowest validation loss, generally lags behind in benchmark performance. This discrepancy highlights an important observation: validation loss alone is not always indicative of text alignment performance, particularly when comparing different text encoder architectures. Models with lower validation loss, such as T5, may not necessarily offer the best real-world performance on established benchmarks. Takeaway for Final Model Design. Based on these findings, our final model adopts hybrid strategy, combining bidirectional CLIP with text-based LLM to leverage both efficient text alignment and deeper semantic understanding. This approach ensures balanced trade-off between parameter efficiency and robust results across diverse metrics. 7 Table 5. Comparison with state-of-the-art (SoTA) models. Total (B) includes parameters from the text encoder, VAE, and diffusion model, while Trainable (B) denotes only those parameters updated during training. The value marked with is estimated from Figure 8 in [14] as the exact number was unavailable."
        },
        {
            "title": "Model",
            "content": "SDXL Base [34] PixArt-α [4] SD3 Medium [14] SD3 [14] Flux-Dev [26] Flux-Schnell [26] JanusPro-7B [7] DiT-Air/L-Lite DiT-Air/XXL Size (B)"
        },
        {
            "title": "Trainable",
            "content": "FID CLIP Pick GenE. Aesth. T2I. 3.5 5.4 7.7 13.6 16.9 16.9 6.9 1.2 6.0 2.6 0.6 2.0 8.0 12.0 12.0 6.9 0.7 2.8 268.0 120.7 26.0 68.7 25.1 17.2 23.1 32. 22.1 27.3 32.0 30.2 33.1 15.5 33.9 34.7 17.0 17.0 20.7 19.7 21.6 16.6 21.5 22.1 55.0 55.7 62.0 74.5 66.7 70.7 80.3 78.4 82. 4.32 5.76 5.99 6.12 6.12 5.95 6.06 6.29 40.6 44.7 52.4 51.4 49.6 49.9 35.2 55.4 59.5 5.2. Progressive VAE Training The selection of Variational Autoencoder (VAE) and its training strategy is critical for achieving high image fidelity in text-to-image generation. Although increasing the channel capacity of the VAE generally improves image reconstruction quality, it can inflate the KL divergence, hindering subsequent diffusion training. To balance this trade-off, we introduce progressive training pipeline. Our approach starts by training lowchannel VAE (e.g., with 4 channels) from scratch. In second stage, an intermediate convolutional layer is replaced with higher channel capacity (e.g., upgraded to 8 channels), followed by continued training. Using this two-stage process, we trained an 8-channel VAE that is employed in all our models. This method not only enhances downstream text-to-image generation but also maintains competitive reconstruction performance compared to the 4-channel or 8channel VAEs trained from scratch. Our ablation studies reveal that training an 8-channel VAE from scratch achieves an rFID of 2.59, whereas our progressive approachstarting with 4-channel VAE and later expanding to 8 channelsattains comparable rFID of 2.61 while reducing the KL divergence from 9105 to 7104. This reduction in divergence is critical, as it leads to improved downstream text-to-image performance (GenEval of 70.4 and T2I CompBench of 51.4) compared to 69.4 and 50.7 when using the 8-channel model trained from scratch. More details and comparisons are provided in Appendix D. 6. Final Models through our progressive training pipeline. Both models undergo multi-stage training process: initial training at 2562 resolution, further training at 5122, followed by supervised fine-tuning (SFT) on curated subset following Dai et al. [9], and finally refined with reward fine-tuning using an approach similar to DRaFT [8] using the HPSv2 [44] reward model.3 represents DiT-Air/XXL. DiT-Air/XXL highcapacity model, combining bidirectional CLIP text encoder for efficient text alignment and text-based LLM for rich semantic understanding, as detailed in Section 5.1. We opt for the XXL configuration to further push the boundaries of image quality. our DiT-Air/L-Lite (attention). For more parameterefficient solution, we introduce DiT-Air/L-Lite (attention), which relies solely on the bidirectional CLIP text encoder and an L-sized architecture. This design has total of 1.15B parametersincluding the text encoder, VAE, and diffusion transformerwhile maintaining competitive performance. Image generation results for both models are provided in Appendix H. Comparison with State-of-the-Art. As shown in Table 5, our models compare favorably with current state-ofthe-art text-to-image systems. In particular, DiT-Air/XXL achieves an exceptional GenEval overall score of 82.9 and T2I CompBench average score of 59.5, outperforming many larger competitors while maintaining relatively In this section, we introduce our two final models, DiT-Air/XXL and DiT-Air/L-Lite (attention), developed 3Detailed training procedures are provided in Appendix A.2.2 (Configurations), (SFT), and (Reward fine-tuning). 8 Table 6. Comparison of different training strategies. Resolutions (2562 vs. 5122) refer to the image size used during pretraining."
        },
        {
            "title": "Training Stage",
            "content": "FID CLIP Pick GenE. Aesth. T2I. Pretrain 2562 Pretrain 5122 Supervised fine-tuning Reward fine-tuning 12.0 13.0 22.5 32.2 33.4 33.5 34.2 34.7 20.5 20.7 21.5 22.1 71.1 74.2 79.0 82. 5.57 5.62 5.89 6.21 50.7 51.7 55.3 59.5 compact size of 5.95B parameters. Meanwhile, DiT-Air/LLite offers compelling balance between efficiency and quality, with strong performance across metrics such as CLIPScore, PickScore, and T2I CompBench scores. Although FID is included for consistency, we note that its reliability diminishes post fine-tuning due to distribution shifts. Multi-stage Training Table 6 details the metrics of the multi-stage training pipeline for DiT-Air/XXL. Increasing the pretraining resolution from 2562 to 5122 slightly increases FID, indicating that higher-resolution data alone does not fundamentally alter the models capacity for coherent image generation under similar training conditions. At the same time, other metrics improve modestly, suggesting that larger input resolution helps capture finer semantic and aesthetic details before any fine-tuning. Subsequent supervised and reward-based fine-tuning produce more noticeable shift in FIDlikely driven by distribution changes introduced by specialized or narrower datayet these stages yield marked improvement in textimage alignment and overall quality. 7. Conclusion In this work, we systematically study the performance of Diffusion Transformers (DiTs) for text-to-image generation, focusing on architectural choices, text-conditioning strategies, and training protocols. We find that the standard DiT architecture, when enhanced with shared AdaLN parameters and configured to directly process concatenated text and noise inputs, achieves superior parameter efficiency compared to alternative approaches, particularly at scale. We introduce DiT-Air and DiT-Air-Lite, two models that enhance the parameter efficiency of the standard DiT backbone while carefully balancing model size and performance for text-to-image generation. Through comprehensive ablation studies of text encoders and variational autoencoders (VAEs), we identified design choices that significantly improve text-conditioned image generation quality. By applying multi-stage training processincluding supervised and reward fine-tuningour final models set new state-of-the-art performance across key text-to-image generation benchmarks, outperforming existing models."
        },
        {
            "title": "Our findings offer valuable insights into the development",
            "content": "of more efficient and expressive text-to-image models, underscoring the potential for further optimizing diffusion architectures and training practices."
        },
        {
            "title": "References",
            "content": "[1] James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Lee, Yufei Guo, et al. Improving image generation with better captions. Computer Science. https://cdn. openai. com/papers/dall-e-3. pdf, 2(3):8, 2023. 5 [2] Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. Conceptual 12m: Pushing web-scale image-text pretraining to recognize long-tail visual concepts. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 35583568, 2021. 3 [3] Junsong Chen, Chongjian Ge, Enze Xie, Yue Wu, Lewei Yao, Xiaozhe Ren, Zhongdao Wang, Ping Luo, Huchuan Lu, and Zhenguo Li. Pixart-σ: Weak-to-strong training of diffusion transformer for 4k text-to-image generation, 2024. 1 [4] Junsong Chen, YU Jincheng, GE Chongjian, Lewei Yao, Enze Xie, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, and Zhenguo Li. Pixart-α: Fast training of diffusion transformer for photorealistic text-to-image synthesis. In The Twelfth International Conference on Learning Representations, 2024. 1, 2, 8 [5] Junsong Chen, Yue Wu, Simian Luo, Enze Xie, Sayak Paul, Ping Luo, Hang Zhao, and Zhenguo Li. Pixart-δ: Fast and controllable image generation with latent consistency models, 2024. 1 [6] Shoufa Chen, Mengmeng Xu, Jiawei Ren, Yuren Cong, Sen He, Yanping Xie, Animesh Sinha, Ping Luo, Tao Xiang, and Juan-Manuel Perez-Rua. Gentron: Diffusion transformers for image and video generation, 2024. 1 [7] Xiaokang Chen, Zhiyu Wu, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, and Chong Ruan. Januspro: Unified multimodal understanding and generation with data and model scaling. arXiv preprint arXiv:2501.17811, 2025. [8] Kevin Clark, Paul Vicol, Kevin Swersky, and David Fleet. Directly fine-tuning diffusion models on differentiable rewards, 2024. 8, 4 [9] Xiaoliang Dai, Ji Hou, Chih-Yao Ma, Sam Tsai, Jialiang Wang, Rui Wang, Peizhao Zhang, Simon Vandenhende, Xiaofang Wang, Abhimanyu Dubey, et al. Emu: Enhancing image generation models using photogenic needles in haystack. arXiv preprint arXiv:2309.15807, 2023. 8, 4 [10] Mostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr Padlewski, Jonathan Heek, Justin Gilmer, Andreas Peter Steiner, Mathilde Caron, Robert Geirhos, Ibrahim Alabdulmohsin, et al. Scaling vision transformers to 22 billion parameters. In International Conference on Machine Learning, pages 74807512. PMLR, 2023. 1 [11] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in neural information processing systems, 34:87808794, 2021. 2 [12] Ming Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng, Chang Zhou, Da Yin, Junyang Lin, Xu Zou, Zhou Shao, 9 Hongxia Yang, et al. Cogview: Mastering text-to-image generation via transformers. Advances in neural information processing systems, 34:1982219835, 2021. [13] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: TransarXiv preprint formers for image recognition at scale. arXiv:2010.11929, 2020. 2 [14] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first international conference on machine learning, 2024. 1, 2, 3, 5, 8 [15] Dhruba Ghosh, Hanna Hajishirzi, and Ludwig Schmidt. Geneval: An object-focused framework for evaluating textto-image alignment, 2023. 2, 5 [16] Xinyu Gong, Wuyang Chen, Tianlong Chen, and Zhangyang Wang. Sandwich Batch Normalization: Drop-In Replacement for Feature Distribution Heterogeneity. In Winter Conference on Applications of Computer Vision (WACV), 2022. 1 [17] Ali Hatamizadeh, Jiaming Song, Guilin Liu, Jan Kautz, and Arash Vahdat. Diffit: Diffusion vision transformers for image generation, 2024. 1 [18] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. Clipscore: reference-free evaluation metric for image captioning, 2022. 5, [19] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium, 2018. 5, 2 [20] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. 1, 2 [21] Kaiyi Huang, Kaiyue Sun, Enze Xie, Zhenguo Li, and Xihui Liu. T2i-compbench: comprehensive benchmark for open-world compositional text-to-image generation, 2023. 5, 2 [22] Kaiyi Huang, Chengqi Duan, Kaiyue Sun, Enze Xie, Zhenguo Li, and Xihui Liu. T2i-compbench++: An enhanced and comprehensive benchmark for compositional text-to-image generation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2025. 2 [23] Patrick Kidger. On Neural Differential Equations. PhD thesis, University of Oxford, 2021. 5, [24] Diederik Kingma and Max Welling. Auto-encoding variational bayes, 2022. 2 [25] Yuval Kirstain, Adam Polyak, Uriel Singer, Shahbuland Matiana, Joe Penna, and Omer Levy. Pick-a-pic: An open dataset of user preferences for text-to-image generation, 2023. 5, 2 [26] Black Forest Labs. Flux. https://github.com/ black-forest-labs/flux, 2024. 2, 8 [27] Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. Albert: lite bert for self-supervised learning of language representations. In International Conference on Learning Representations, 2020. 2, 3, 4 [28] Daiqing Li, Aleks Kamko, Ehsan Akhgari, Ali Sabet, Linmiao Xu, and Suhail Doshi. Playground v2.5: Three insights towards enhancing aesthetic quality in text-to-image generation, 2024. 5 [29] Zhengyang Liang, Hao He, Ceyuan Yang, and Bo Dai. arXiv preprint Scaling laws for diffusion transformers. arXiv:2410.08184, 2024. 3 [30] Tsung-Yi Lin, Michael Maire, Serge Belongie, Lubomir Bourdev, Ross Girshick, James Hays, Pietro Perona, Deva Ramanan, C. Lawrence Zitnick, and Piotr Dollar. Microsoft coco: Common objects in context, 2015. 5 [31] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. arXiv preprint arXiv:2209.03003, 2022. 2 [32] Nanye Ma, Mark Goldstein, Michael Albergo, Nicholas Boffi, Eric Vanden-Eijnden, and Saining Xie. Sit: Exploring flow and diffusion-based generative models with scalable interpolant transformers. In European Conference on Computer Vision, pages 2340. Springer, 2024. 2 [33] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 41954205, 2023. 1, 2 [34] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Improving latent diffusion modRobin Rombach. Sdxl: arXiv preprint els for high-resolution image synthesis. arXiv:2307.01952, 2023. 2, 8 [35] Adam Polyak, Amit Zohar, Andrew Brown, Andros Tjandra, Animesh Sinha, Ann Lee, Apoorv Vyas, Bowen Shi, ChihYao Ma, Ching-Yao Chuang, David Yan, Dhruv Choudhary, Dingkang Wang, Geet Sethi, Guan Pang, Haoyu Ma, Ishan Misra, Ji Hou, Jialiang Wang, Kiran Jagadeesh, Kunpeng Li, Luxin Zhang, Mannat Singh, Mary Williamson, Matt Le, Matthew Yu, Mitesh Kumar Singh, Peizhao Zhang, Peter Vajda, Quentin Duval, Rohit Girdhar, Roshan Sumbaly, Sai Saketh Rambhatla, Sam Tsai, Samaneh Azadi, Samyak Datta, Sanyuan Chen, Sean Bell, Sharadh Ramaswamy, Shelly Sheynin, Siddharth Bhattacharya, Simran Motwani, Tao Xu, Tianhe Li, Tingbo Hou, Wei-Ning Hsu, Xi Yin, Xiaoliang Dai, Yaniv Taigman, Yaqiao Luo, Yen-Cheng Liu, Yi-Chiao Wu, Yue Zhao, Yuval Kirstain, Zecheng He, Zijian He, Albert Pumarola, Ali Thabet, Artsiom Sanakoyeu, Arun Mallya, Baishan Guo, Boris Araya, Breena Kerr, Carleigh Wood, Ce Liu, Cen Peng, Dimitry Vengertsev, Edgar Schonfeld, Elliot Blanchard, Felix Juefei-Xu, Fraylie Nord, Jeff Liang, John Hoffman, Jonas Kohler, Kaolin Fire, Karthik Sivakumar, Lawrence Chen, Licheng Yu, Luya Gao, Markos Georgopoulos, Rashel Moritz, Sara K. Sampson, Shikai Li, Simone Parmeggiani, Steve Fine, Tara Fowler, Vladan Petrovic, and Yuming Du. Movie gen: cast of media foundation models, 2025. 5, 1 [36] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision, 2021. 2, 5 [37] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents, 2022. 2 [38] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image In Proceedings of synthesis with latent diffusion models. the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. 2 [39] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S. Sara Mahdavi, Rapha Gontijo Lopes, Tim Salimans, Jonathan Ho, David Fleet, and Mohammad Norouzi. Photorealistic text-to-image diffusion models with deep language understanding, 2022. 2 [40] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, Patrick Schramowski, Srivatsa Kundurthy, Katherine Crowson, Ludwig Schmidt, Robert Kaczmarczyk, and Jenia Jitsev. Laion-5b: An open large-scale dataset for training next generation image-text models, 2022. 5, 2, 3 [41] Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020. 2 [42] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. 1 [43] Alexander Tong, Kilian Fatras, Nikolay Malkin, Guillaume Huguet, Yanlei Zhang, Jarrid Rector-Brooks, Guy Wolf, and Yoshua Bengio. Improving and generalizing flow-based generative models with minibatch optimal transport. arXiv preprint arXiv:2302.00482, 2023. [44] Xiaoshi Wu, Yiming Hao, Keqiang Sun, Yixiong Chen, Feng Zhu, Rui Zhao, and Hongsheng Li. Human preference score v2: solid benchmark for evaluating human preferences of text-to-image synthesis, 2023. 8, 4 [45] Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding, Jie Tang, and Yuxiao Dong. Imagereward: Learning and evaluating human preferences for textto-image generation. Advances in Neural Information Processing Systems, 36:1590315935, 2023. 2 11 DiT-Air: Revisiting the Efficiency of Diffusion Model Architecture Design in Text to Image Generation"
        },
        {
            "title": "Supplementary Material",
            "content": "A. Implementation Details A.2. Training and Inference Details Our model builds upon diffusion transformer framework with design choices that enhance training stability and performance. A.1. Model A.1.1. Diffusion Transformer Variants We define five variants of the diffusion transformer: S, B, L, XL, and XXL. These correspond to models with 12, 18, 24, 30, and 38 transformer layers, respectively. The hidden dimension scales proportionally with the number of layers as = 64 nlayer, and the number of attention heads is set to equal the transformer depth. A.1.2. Stability Enhancements To improve training stability, we integrate several techniques. First, we apply QK-normalization [10] following the SD3 methodology to stabilize query-key interactions within the attention mechanism. We also employ sandwich normalization [12, 16] in both the attention blocks and MLP modules, method proven effective for large-scale model training. Additionally, rather than using static positional embeddings for visual tokens, we incorporate 2D rotary positional embedding [42] within the attention mechanism to dynamically capture spatial relationships. A.1.3. Conditional Inputs via AdaLN In our implementation, we adopt the MMDiT approach by incorporating pooled text embedding through Adaptive Layer Normalization (AdaLN) alongside the time embedding. We believe that this pooled embedding provides highlevel semantic information with only minimal increase in computational cost. A.1.4. Textual and Language Components For the textual component, we utilize an internal CLIP/H text encoder consisting of 24 transformer layers, hidden dimension of 1024, and 16 attention heads, totaling approximately 335 million parameters. The pooled embedding is generated via last token pooling for causal CLIP or average pooling for bidirectional CLIP. Additionally, our internal language model (LLM) consists of 56 transformer layers, hidden dimension of 6,656, and 16 attention heads, with around 2.8 billion parameters, and employs last token pooling to produce the pooled embedding for AdaLN. A.2.1. Training Objective The model is trained using flow-matching objective: min θ Ez0, ϵN (0,I), c, tP(t) (cid:104) fθ(zt, c, t) z0 + ϵ2 2 (cid:105) In this formulation, the timestep distribution P(t) follows logit-Normal distribution as in SD3 [14], which emphasizes intermediate steps during the flow-matching process. A.2.2. Training Setup Training is conducted on TPU v5p hardware. We use the AdaFactor optimizer with constant learning rate of 1 104, and momentum parameters b1 = 0.9 and b2 = 0.999. For ablation studies, the model is trained for 1 million steps with batch size of 4,096 at resolution of 2562. In contrast, the final models are trained in multiple stages: 1. An initial stage of 500k steps at 2562 resolution with batch size of 4,096. 2. subsequent stage of 100k steps at 5122 resolution with batch size of 2,048. 3. supervised fine-tuning (SFT) stage for 2.5k steps with batch size of 64, where the timestep distribution P(t) is shifted so that the log-SNR aligns with that of the lowresolution training. 4. reward fine-tuning stage for an additional 4.8k steps with batch size of 64. Further details regarding the SFT and reward finetuning stages are provided in Appendix and Appendix F, respectively. A.2.3. Inference During inference, we employ second-order Heun SDE solver [23] with 50 sampling steps, combined with classifier-free guidance scale of 7.5 to steer the sampling process. B. Evaluation We evaluate model performance using combination of validation loss and several established benchmarks. B.1. Validation Loss Recent works such as SD3 [14] and MovieGen [35] have proposed using validation loss as performance estimate. We follow this trend. More specifically, in flow-matching paradigm, the validation loss measures how well the model 1 learns the velocity field induced by the transport equation. With fixed VAE across all experiments, the source and target distributions remain constant (i.e., the standard normal distribution and the VAE-encoded latents, respectively). Consequently, the validation loss directly reflects the models ability to predict the rectified flow trajectory, which often aligns with human perceptual preferences. B.2. Benchmark Metrics We report performance on the following metrics: Frechet Inception Distance (FID): Quantifies the similarity between the generated and real image distributions by comparing their Inception-v3 embeddings [19]. CLIPScore: Assesses the semantic alignment between images and text using CLIP embeddings. Higher scores denote better correspondence [18, 36]. PickScore: Similar to CLIPScore, but based on CLIP model trained on an open dataset of text-to-image prompts and real user preferences, thereby achieving compelling performance in predicting human preferences [25]. GenEval: Provides an overall evaluation of image generation performance. The original implementation, using 4 samples per prompt, tends to exhibit larger uncertainty bounds; to mitigate this, we increase the number of samples to 64 [15]. T2I CompBench: comprehensive benchmark for assessing text-to-image synthesis quality [21]. LAION-Aesthetics Predictor V2: Predicts the aesthetic quality of images, with higher scores indicating superior visual appeal [40]. In our evaluation, images are generated using ImageReward prompts [45] and subsequently assessed with this aesthetic model. B.3. Abbreviation Key For clarity, we list the abbreviations used in tables throughout the paper: Val. Validation Loss. FID Frechet Inception Distance. CLIP CLIPScore. Pick PickScore. GenE. GenEval. Aesth. LAION-Aesthetics Predictor V2. T2I. T2I CompBench. C. Detailed Text Encoder Ablation Studies This section provides detailed analysis of the text encoder ablation experiments, including the impact of causal vs. bidirectional attention in CLIP, the effect of layer selection in both CLIP and LLMs, and comparison between text LLM and multi-modal LLM. The results presented here supplement the summary findings discussed in Section 5.1 of the main paper. Table 7. Zero-Shot performance of causal vs. bidirectional CLIP models on ImageNet and COCO5k."
        },
        {
            "title": "ImageNet",
            "content": "COCO5k Acc@1 Acc@5 I2T@1 I2T@5 T2I@1 T2I@"
        },
        {
            "title": "Causal\nBidirectional",
            "content": "80.6 80.6 96.5 96.5 74.4 74.6 91.5 91.5 53.6 53.8 77.7 78. Table 8. Performance comparison of causal vs. bidirectional CLIP models as text embedding models for text-to-image generation. CLIP Model Val. FID CLIP Pick GenE. Aesth. T2I."
        },
        {
            "title": "0.429\nBidirectional 0.428",
            "content": "16.4 16.0 32.8 32.8 20.2 20.3 0.683 0.704 5.61 5.58 50.6 51. C.1. CLIP C.1.1. Causal vs. Bidirectional Attention in CLIP To assess the impact of attention mechanisms in the text encoder, we compared causal and bidirectional variants of the CLIP/H model. In the causal configuration, the CLIP model employs causal attention with last-token pooling, whereas the bidirectional variant uses global average pooling during contrastive loss training. Both models exhibit comparable performance in standard zero-shot classification and retrieval tasks, as summarized in Table 7. However, as shown in Table 8, the bidirectional CLIP consistently outperforms its causal counterpart in terms of text alignment and image quality benchmarks. We hypothesize that the observed improvements in diffusion models arise specifically from enhanced attention alignment, rather than from intrinsic differences in the pretrained text encoder performance. C.1.2. Layer Selection in CLIP We investigated how the selection of different layers in the bidirectional CLIP text encoder (24 layers) affects performance. Embeddings from layers 6, 12, 18, 23, and 24 were tested, along with concatenation of multiple layers (6, 12, 18, 24) followed by linear projection. The 23rd layer was included as part of this study due to its common use as the penultimate layer in open-source text-to-image models. As shown in Table 9, our results indicate that all deeper layers (12, 18, 23, 24) exhibit comparable performance, while the shallow layer (6) underperforms. Concatenating embeddings from multiple layers did not yield significant improvements, suggesting that single mid-to-deep layer is sufficient for robust text alignment. C.2. LLM C.2.1. Text LLM vs. Multimodal LLM We further investigate the impact of the text encoder by comparing text-only LLM with multimodal LLM 2 Table 9. CLIP Layer Selection Performance. Table 12. Comparison of VAE on reconstruction and generation."
        },
        {
            "title": "Model",
            "content": "Val. FID CLIP Pick GenE. Aesth. T2I."
        },
        {
            "title": "Model KL",
            "content": "rFID FID CLIP Pick GenE. Aesth. T2I. Layer 6 Layer 12 Layer 18 Layer 23 Layer 24 Layer 6 + 12 + 18 + 24 0.428 0.428 0.428 0.428 0.428 15.9 16.0 16.1 15.6 16.0 0.426 15. 32.3 32.7 32.8 32.8 32.8 32.8 20.1 20.2 20.2 20.3 20.3 20.3 68.4 68.3 69.1 70.6 70.4 70. 5.46 5.56 5.56 5.59 5.58 5.60 51.2 50.5 51.2 50.9 51.4 51.2 Table 10. Performance Comparison of text LLM vs. Multimodal LLM. Model Val. FID CLIP Pick GenE. Aesth. T2I. LLM 0.427 MLLM 0.427 16.0 16.4 32.0 31.9 20.1 20.0 72.6 70.0 5.57 5. 48.6 49.2 Table 11. LLM Layer Selection Performance."
        },
        {
            "title": "Model",
            "content": "Val. FID CLIP Pick GenE. Aesth. T2I. Layer 14 Layer 28 Layer 42 0.427 0.427 0.427 Layer 56 (last) 0.427 Layer 28 + 42 0.427 15.8 16.5 17.2 16.0 15.9 32.2 32.3 32.3 32.0 32.3 20.1 20.2 20.1 20.1 20. 68.1 71.7 71.6 72.6 73.0 5.56 5.64 5.61 5.57 5.60 0.494 0.502 0.501 0.486 0.507 (MLLM). Our experiments, summarized in Table 10, reveal that text-only LLMs tend to outperform their multimodal counterparts, particularly on the GenEval metric. C.2.2. Layer Selection in LLMs We also evaluated different layers in the 56-layer textonly LLM to determine the optimal choice for text embeddings. Specifically, we compared embeddings from layer 14 (early), layer 28 (middle), layer 42 (deeper), and concatenation of layers 28 and 42. Our results, summarized in Table 11, indicate that both the middle (28) and deeper (42) layers offer strong balance between preserving low-level token details and capturing high-level semantic representations. In contrast, the final layer, while performing well on GenEval, provides less balanced representations for text-toimage generation tasks, possibly due to over-specialization in the pretraining objective. D. Progressive VAE Training Studies To validate our progressive training approach described in Section 5.2, we conducted experiments on three VAE variants, all employing an 8 compression factor and trained on the OpenImages 9M dataset. The variants are defined as follows: Variant A: VAE with 4 channels trained from scratch."
        },
        {
            "title": "A\nB\nC",
            "content": "7 104 9 105 7 104 4.62 2.59 2.61 17.2 16.3 16.0 32.7 32.8 32.8 20.2 20.2 20.3 69.8 69.4 70. 5.52 5.56 5.58 50.4 50.7 51.4 Variant B: VAE with 8 channels trained from scratch. Variant C: VAE with 8 channels trained using our proposed Progressive training approach. This variant is initially trained with 4 channels (as in Variant A) and subsequently refined by replacing an intermediate convolutional layer with one that uses 8 channels. Our evaluation employs the reconstruction FID (rFID) metric on the COCO validation set to assess image reconstruction quality, along with an evaluation of the downstream diffusion model using DiT-Air/B. The experimental results, summarized in Table 12, indicate that although increasing the channel size significantly enhances reconstruction quality, it also leads to higher KL divergence in the latent features. This elevated KL divergence can impede the latent diffusion models learning, resulting in only marginal gains in final visual generation quality. In contrast, our progressive training pipeline mitigates this issue by first training smaller VAE and then gradually increasing its channel capacity. This approach achieves notable improvements in text-to-image generation while maintaining competitive reconstruction performance. E. Supervised Fine-Tuning and Data Curation In the supervised fine-tuning (SFT) stage, our goal is to refine the pretrained model using very high-quality but relatively small dataset of image-text pairs. To this end, we curated dataset of 1,033 pairs, ensuring that the images and their corresponding captions meet stringent quality standards. The curation process involved several key steps: 1. Automated Filtering: Initially, we applied both the LAION image aesthetics model [40] and our internal photo aesthetics model to the pretraining data. This step allowed us to filter out images that did not meet our high aesthetic standards, ensuring that only the best images were considered. 2. Manual Selection: From the automatically filtered subset, we manually reviewed the images to further refine the selection. The focus here was on achieving diversity across object categories and image styles, with special attention given to important verticals such as people and animals. 3. Caption Curation: For the selected images, we crafted precise captions in the style of our automatic captioning model. This manual curation ensured that each caption 3 Through qualitative observation, we find that the DiTAir/XXL consistently excels in generating complex scenes, capturing intricate structural details, and delivering superior visual quality. It demonstrates strong ability to produce highly detailed and realistic images, even when handling lengthy or complex prompts. Its integrated LLM encoder contributes significantly to its robust text rendering capabilities, maintaining clarity and accuracy even in challenging scenarios. In contrast, the DiT-Air/L-Lite offers well-balanced approach, prioritizing efficiency while still delivering strong performance across variety of tasks. It is particularly well-suited for scenarios with limited computational resources, providing high-quality images and effective handling of most prompts. While the DiT-Air/L-Lite maintains an excellent balance of efficiency and quality, our qualitative observations indicate that it may occasionally struggle with more ambiguous prompts and exhibit limitations in rendering complex text or achieving the same level of visual fidelity as the DiT-Air/XXL. These observations highlight the intended trade-off of the DiT-Air/L-Lite: streamlined design that prioritizes efficiency, making it compelling choice when balancing performance with resource constraints. Overall, these observations highlight the trade-offs between the two models. The DiT-Air/XXL is ideal for tasks that demand high-quality, detailed images and strong text-image alignment, whereas the DiT-Air/L-Lite serves as compelling, resource-efficient alternative for more lightweight use cases. was not only accurate but also semantically well-aligned with the corresponding image. Fine-tuning with the resulting dataset, as demonstrated in [9], can lead the model to converge to state where the generated images surpass the average quality of the pretraining data. Overall, our SFT strategy emphasizes quality over quantity. By leveraging meticulously curated dataset, we ensure that the fine-tuning process yields improved image generation performance, achieving both higher aesthetic quality and better semantic alignment. F. Reward Fine-tuning We adopt an approach similar to DRaFT [8] to fine-tune our models using the HPSv2 [44] reward model. In our setup, the models receive prompt and an initial latent noise zT as inputs, which are then denoised over timesteps to generate the final image I0. The HPSv2 model computes human preference score for the generated image, denoted as r(p, I0), with scores normalized between 0 and 1. Consequently, the loss backpropagated through the sampling chain is defined as 1 r(p, I0). To specifically target areas where the model underperforms, we selected 2,000 of the lowest-scoring prompts as the training data for reward fine-tuning. During this stage, we fine-tune the full set of model parameters, using total sampling timestep = 50 and stop gradient timestep Ts = 25 to ensure that gradients are propagated only from timestep down to Ts, with no gradient updates before Ts. Despite these precautions, we observed reward model hacking phenomenon [8], where HPSv2 occasionally assigns very high scores to poor-quality images. To mitigate this issue, we implemented an early stopping strategy to prevent overfitting and reward hacking. Ultimately, this approach effectively reduced structural artifacts, enhanced text alignment, and improved the overall visual appeal of the generated images. G. State-of-the-Art Model Size Breakdown In Table 13, we present detailed breakdown of the architectural components and parameter counts for various stateof-the-art text-to-image generation models. For JanusPro7B, the SigLIP encoder is omitted for simplicity. The comparison demonstrates that DiT-Air is relatively compactboth in total parameter count and in trainable parameterswhen compared with existing models. H. Generation Examples selection of model-generated images illustrating different capabilities are shown in Figure 7 (generations by DiTAir/XXL) and Figure 8 (generations by DiT-Air/L-Lite). Table 13. Breakdown of architectural components for various text-to-image generation models. Trainable components are marked with ."
        },
        {
            "title": "Model",
            "content": "Text Encoder 1 Text Encoder 2 Diffusion Model"
        },
        {
            "title": "Autoencoder",
            "content": "Total (B) SDXL Base PixArt-α SD3 Medium SD3 Flux-Dev Flux-Schnell JanusPro-7B CLIP/L (123M) Flan-T5-XXL (4.7B) CLIP/L + bigG (817M) CLIP/L + bigG (817M) CLIP/L (123M) CLIP/L (123M) OpenCLIP/g (694M) T5-v1.1-XXL (4.7B) T5-v1.1-XXL (4.7B) T5-XXL (4.7B) T5-XXL (4.7B) DiT-Air/L-Lite DiT-Air/XXL CLIP/H (335M) CLIP/H (335M) LLM (2.8B) U-Net (2.6B) DiT (0.6B) DiT (2.0B) DiT (8.0B) DiT (12B) DiT (12B) LLM (6.9B) DiT (0.7B) DiT (2.8B) 8-ch (84M) 4-ch (80M) 16-ch (85M) 16-ch (85M) 16-ch (85M) 16-ch (85M) 16-ch VQ (85M) 8-ch (84M) 8-ch (84M) 3.50 5.38 7.65 13.60 16.91 16.91 6. 1.15 5.95 5 Figure 7. Sample images from our DiT-Air/XXL illustrating different capabilities 6 Figure 8. Sample images from our DiT-Air/L-Lite illustrating different capabilities 7 Table 14. Supplementary Table: Detailed GenEval and T2I CompBench Breakdown Method GenEval T2I Compbench (%) Overall Single Object Two Objects Counting Colors Position Color Attribution Average Color Shape Texture Spatial Non-Spatial Complex Per-layer Adaln Shared Adaln DiT-Air/B DiT-Air/B-lite (full) DiT-Air/B-lite (attention) Pixart-α/S Pixart-α/B Pixart-α/L Pixart-α/XL Pixart-α/XXL MMDiT/S MMDiT/B MMDiT/L MMDiT/XL MMDiT/XXL DiT-Air/S DiT-Air/B DiT-Air/L DiT-Air/XL DiT-Air/XXL CLIP (Bidirectional) LLM (Text-only) T5-XXL SDXL PixArt-α SD3-medium SD3 Flux-dev Flux-schnell Janus-pro DiT-Air/L-Lite DiT-Air/XXL Pretrain 2562 Pretrain 5122 Supervised fine-tuning Reward fine-tuning Bidirectional Causal Layer 6 Layer 12 Layer 18 Layer 23 Layer 24 Layer 6 + 12 + 8 + 24 LLM MLLM Layer 14 Layer 28 Layer 42 Layer 56 (last) Layer 28 + C 69.8 69.8 70.4 58.4 66.9 61.9 68.1 70.4 70.1 69.9 67.0 69.8 70.9 69.0 69.7 63.4 70.4 69.6 71.1 69. 70.4 72.6 66.9 55.7 47.8 62.0 74.5 66.7 70.7 80.3 77.6 82.9 71.1 74.2 79.0 82.9 70.4 68.3 68.4 68.3 69.1 70.6 70.4 70.4 70.2 69. 68.1 71.7 71.6 70.2 73.0 69.8 69.4 70.4 99.3 99.3 99.7 96.8 99.1 98.7 99.5 99.2 99.2 99.2 99.5 99.3 99.3 98.9 99. 98.0 99.7 99.4 99.3 99.0 99.3 99.2 65.3 98.0 98.0 98.0 99.0 99.0 99.0 99.0 99.9 99.9 99.3 99.4 99.9 100.0 99.7 99.5 99.0 99.5 99.2 99.3 99.7 99. 99.4 99.5 98.9 99.0 98.4 99.4 99.2 99.2 99.6 99.7 88.7 90.0 88.6 69.3 84.3 81.2 89.4 89.1 88.6 88. 89.4 88.7 90.9 87.3 87.4 78.2 88.6 88.4 90.0 89.0 88.7 88.7 82.7 74.0 50.0 74.0 94.0 81.0 92.0 89.0 96.5 98.8 88.7 91.7 96.3 98.8 88.6 83. 85.6 87.0 89.1 88.9 88.6 91.0 87.9 88.5 82.0 87.6 86.1 88.0 87.7 88.5 89.9 88.6 68.6 68.2 68.1 49.8 65. 58.3 67.3 69.5 68.2 71.2 73.6 68.6 71.2 66.3 67.1 57.2 68.1 67.1 71.4 70.1 68.6 65.7 63.9 39.0 44.0 63.0 72.0 79.0 73.0 59.0 80.8 83.8 69.0 73.6 78.7 83. 68.1 67.3 64.9 66.0 66.7 69.7 68.1 67.9 67.4 67.1 59.2 62.6 63.3 67.4 66.3 69.9 67.4 68.1 81.0 79. 80.0 75.7 79.2 79.4 79.6 80.4 83.4 81.9 83.1 81.0 82.3 81.1 83.0 79.0 80.0 79.5 82.1 81.3 81.0 83.6 78.3 85.0 80.0 67.0 89.0 74.0 78.0 90.0 86.8 86. 82.0 84.0 86.1 86.6 80.0 79.0 79.2 80.0 79.5 80.9 80.0 79.6 81.2 80.9 80.7 82.6 82.8 81.2 85.0 79.1 79.8 80. 59.3 58.7 60.8 44.4 54.9 52.0 57.7 58.2 57.9 56.5 58.8 59.3 58.3 53.9 56.9 53.1 60.8 58.9 56.8 54.3 59.3 63.6 49. 23.0 7.00 36.0 60.0 47.0 54.0 66.0 71.3 77.7 55.7 61.4 68.5 77.7 60.8 58.5 58.2 56.7 58.2 60.8 60.8 58.5 60.1 56.6 60.6 64.0 61.8 60.1 66. 55.9 56.9 60.8 Table 2 22.0 23.5 Table 3 25.2 14.1 18.5 Figure 15.3 21.2 24.2 26.1 26.8 29.1 22.0 23.8 26.4 24.3 15.0 25.2 24.7 26.8 24.9 Table 4 22.0 34.7 19.5 Table 15.0 8.00 34.0 33.0 20.0 28.0 79.0 30.5 50.5 Table 6 32.0 35.4 44.6 50.5 Table 8 25.2 21.9 Table 23.6 20.9 22.1 24.1 25.2 26.1 Table 10 24.9 25.2 Table 11 27.3 34.3 37.1 24.9 33.0 Table 25.7 23.0 25.2 8 51.1 51.0 51.4 47.6 49.6 48.3 50.5 50.9 50.9 51.0 50.0 51.1 51.1 50.9 50. 48.9 51.4 51.2 51.1 50.8 51.1 48.6 48.0 40.6 44.6 52.4 51.4 49.6 49.9 35.2 55.4 59.5 50.7 51.7 55.3 59.5 51.4 50.60 51.2 50.5 51.2 50.9 51.4 51. 51.4 51.3 49.4 50.2 50.1 51.4 50.7 50.4 50.7 51.4 81.8 81.8 83.0 79.1 79.9 80.2 82.7 82.7 81.9 82. 80.6 81.8 82.3 82.6 82.3 80.1 83.0 82.6 82.6 82.7 81.8 78.8 76.3 58.8 66.9 81.3 74.1 73.9 52.0 86.1 88.9 81.9 81.9 85.5 88.9 83.0 81. 81.2 81.7 82.4 82.1 83.0 81.8 83.0 82.0 80.4 81.1 79.5 83.0 81.1 80.3 81.1 83.0 62.2 61.9 61.7 57.4 60. 57.5 60.5 61.0 61.3 63.0 60.8 62.2 62.2 63.0 62.7 59.1 61.7 62.5 62.0 62.6 62.2 56.8 57.5 46.9 49.3 58.9 57.2 55.8 33.1 66.6 69.4 61.2 61.2 63.4 69. 61.7 61.5 61.6 61.0 61.5 61.1 61.7 61.3 61.0 62.2 59.3 60.0 60.0 61.0 61.1 61.8 61.0 61.7 74.9 73. 73.4 69.4 71.2 68.8 72.5 73.0 72.7 72.8 72.7 74.9 74.4 73.1 73.2 71.6 73.4 73.7 73.4 73.2 74.9 69.8 69.1 53.0 64.8 73.3 69.2 68.5 40.6 79.5 82. 72.4 73.9 76.9 82.0 73.4 72.3 74.0 73.2 74.3 73.7 73.4 74.1 74.2 73.6 68.7 72.0 72.0 74.2 73.0 72.9 73.1 73. 19.6 19.9 21.6 13.5 18.0 15.8 18.9 19.5 20.4 19.0 18.2 19.6 19.1 18.1 15.7 15.2 21.6 19.6 19.9 17.9 19.6 18.4 17. 21.3 20.6 32.0 28.6 31.2 15.4 27.7 40.6 19.5 22.6 32.7 40.6 21.6 19.3 22.4 18.6 20.5 19.5 21.6 21.0 21.5 20.7 19.8 19.9 20.7 21.5 20. 19.3 21.2 21.6 31.3 31.3 31.3 30.8 31.2 31.0 31.4 31.4 31.3 31.2 31.0 31.3 31.3 31.2 31.3 31.0 31.3 31.1 31.2 31. 31.3 30.9 31.1 31.2 32.0 31.4 31.3 31.5 31.3 31.2 31.9 31.3 31.1 31.6 31.9 31.3 31.4 31.1 31.2 31.0 31.3 31.3 31.2 31.2 31. 31.3 31.1 31.0 31.2 31.1 31.3 31.1 31.3 37.1 37.7 37.7 35.6 37.1 36.2 37.1 37.7 37.5 37.8 36.5 37.1 37.2 37.4 37. 36.6 37.7 37.5 37.6 36.8 37.1 37.1 36.5 32.4 34.3 37.7 37.0 38.6 39.2 41.3 44.1 37.9 39.6 41.9 44.2 37.7 37.2 37.2 37.5 37.4 37.7 37.7 37. 37.8 37.8 37.1 37.1 37.6 37.8 37.7 37.0 36.5 37."
        }
    ],
    "affiliations": [
        "Apple Inc."
    ]
}