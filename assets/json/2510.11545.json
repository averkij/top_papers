{
    "paper_title": "Information-Preserving Reformulation of Reasoning Traces for Antidistillation",
    "authors": [
        "Jiayu Ding",
        "Lei Cui",
        "Li Dong",
        "Nanning Zheng",
        "Furu Wei"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advances in Large Language Models (LLMs) show that extending the length of reasoning chains significantly improves performance on complex tasks. While revealing these reasoning traces helps users better follow, verify, and learn from the model's problem-solving process, it also makes them highly vulnerable to unauthorized distillation. To mitigate this risk, proprietary model providers often adopt aggressive protection strategies, such as replacing detailed reasoning with brief summaries, which deprive users of valuable intermediate information. To address this trade-off, we propose PART, an information-preserving antidistillation reformulation of reasoning traces. Motivated by the difference between how humans understand reasoning traces and how LLMs exploit them for supervised fine-tuning, we design a simple but effective two-step reformulation: removing self-talk behaviors and reordering sub-conclusions. A small auxiliary model is trained to perform this reformulation, incurring minimal computational overhead. Extensive experiments demonstrate that PART consistently disrupts distillation across student models of different sizes and types on various reasoning benchmarks. For instance, when training on reformulated traces, even the performance of a large 32B student model decreases from 54.17 to 46.88 on AIME 2024, corresponding to a 13.5% degradation."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 1 ] . [ 1 5 4 5 1 1 . 0 1 5 2 : r Information-Preserving Reformulation of Reasoning Traces for Antidistillation Jiayu Ding Lei Cui Li Dong Nanning Zheng Furu Wei IAIR, Xian Jiaotong University Microsoft Research"
        },
        {
            "title": "Abstract",
            "content": "Recent advances in Large Language Models (LLMs) show that extending the length of reasoning chains significantly improves performance on complex tasks. While revealing these reasoning traces helps users better follow, verify, and learn from the models problem-solving process, it also makes them highly vulnerable to unauthorized distillation. To mitigate this risk, proprietary model providers often adopt aggressive protection strategies, such as replacing detailed reasoning with brief summaries, which deprive users of valuable intermediate information. To address this trade-off, we propose PART, an information-Preserving Antidistillation Reformulation of reasoning Traces. Motivated by the difference between how humans understand reasoning traces and how LLMs exploit them for supervised fine-tuning, we design simple but effective two-step reformulation: removing self-talk behaviors and reordering sub-conclusions. small auxiliary model is trained to perform this reformulation, incurring minimal computational overhead. Extensive experiments demonstrate that PART consistently disrupts distillation across student models of different sizes and types on various reasoning benchmarks. For instance, when training on reformulated traces, even the performance of large 32B student model decreases from 54.17 to 46.88 on AIME 2024, corresponding to 13.5% degradation. Figure 1: Overview of PART. Directly exposing original reasoning traces leaves them vulnerable to unauthorized distillation, whereas providing only summaries deprives users of the information contained in the reasoning process. PART introduces an information-preserving antidistillation approach through reformulation at both the token level and the structural level. Corresponding author. Preprint. Under review."
        },
        {
            "title": "Introduction",
            "content": "Large language models (LLMs) have recently achieved remarkable progress in domains such as mathematics and programming, largely driven by the use of long reasoning traces under test-time scaling [Ope24b, DAGY+25]. Beyond enhancing performance, these reasoning traces also allow users to gain insights into the models problem-solving process, thereby improving interpretability and trustworthiness of LLMs response. However, exposing original reasoning traces makes them highly vulnerable to unauthorized distillation. It has been shown that supervised fine-tuning (SFT) on as few as ten of thousands of reasoning traces suffices for student models to attain comparable reasoning capabilities, leading to intellectual property leakage [HZL+24]. To mitigate this risk, existing proprietary model providers often adopt restrictive strategies to protect their reasoning traces. Common practices include either eliminating access to the reasoning trace or only revealing condensed summary. While such strategies could prevent distillation, they hinder users from obtaining valuable information in reasoning traces. Recent works have explored antidistillation by controlling the sampling process or fine-tuning the teacher model [STF+25, LTQ+25]. However, these approaches either compromise the performance of the teacher model or incur training costs for large teacher models. To address this issue, we introduce PART, an information-Preserving Antidistillation Reformulation of reasoning Traces. The key insight of PART is that the way models acquire reasoning ability through SFT differs from how humans comprehend reasoning processes. Reasoning traces that are interpretable for humans may not be suitable for distillation [CSW+25]. Leveraging this discrepancy, we could defend distillation while preserving information. Concretely, we reformulate reasoning traces in two steps, modifying them at both the token level and the structural level, and we further train small auxiliary model to perform this reformulation with minimal computational overhead. At the token level, different tokens contribute unequally to parameter updates during SFT: tokens with lower predicted probabilities induce large gradients. Our analysis of student models learning dynamics on teacher-generated sequences reveals that these low-probability tokens contain many self-talk behaviors such as Hmm, Wait, and Lets. While such expressions do not carry reasoning-related information, they drive substantial loss reduction during training, acting as the useful tokens discussed in prior studies [LGG+24]. Removing these expressions therefore disrupts distillation without sacrificing informational content. At the structural level, prior work has shown that the structural perturbations to reasoning traces can significantly impact distillation [LCG+25]. To construct an information-preserving structural perturbation, we exploit the difference between reasoning understanding and generation. Humans do not require strict process-then-conclusion order to comprehend reasoning. Instead, it is common to use conclusion-before-process structures, such as presenting lemma before its proof in mathematics. In contrast, limited by single-step computation, its hard for LLMs to directly generate the correct conclusion without intermediate reasoning steps. Based on this difference, we reorder reasoning traces by placing sub-conclusions ahead of their corresponding reasoning steps. This reordering perturbs the structural patterns on which distillation relies, thereby weakening its effectiveness while maintaining human interpretability. To verify that PART effectively preserves information after reformulation, we evaluated the reformulated reasoning traces produced against the original reasoning traces from three perspectives: lexical similarity, semantic similarity, and human judgment. For lexical similarity, we segment the original reasoning traces into fragments and compute the match ratio in the reformulated reasoning traces using fuzzy matching. Experimental results show that across all similarity thresholds, PART consistently outperforms the summary-based method. For semantic similarity, we employed Qwen3-Embedding-4B [ZLL+25] to map reasoning traces into embeddings and compute the match ratio by using the embeddings of the original traces as queries. Results demonstrate that 90.1% of queries matched the reformulated reasoning traces generated by PART, while only 7.3% matched those produced by the summary-based method. Furthermore, in user study on perceived informativeness, participants generally judged the information in PART reformulations to be comparable to the originals, while clearly preferring PART over the summary-based method for providing richer information. 2 To evaluate the antidistillation capability of PART, we conducted experiments on student models of different sizes and types, comparing their performance when distilled with the original reasoning traces versus the reformulated reasoning traces. Results show that models trained on data reformulated by PART suffer significant degradation across mathematics, coding, and scientific question answering benchmarks. PART demonstrates stable effectiveness across varying model sizes and dataset scale. Notably, even 32B student model exhibited performance drop from 54.17 to 46.8 on AIME 2024, corresponding to 13.5% degradation. The contributions of this paper are summarized as follows: We propose PART, simple but effective reasoning trace reformulation method that disrupts distillation while preserving information. We validate that our approach successfully retains the information contained in the original reasoning traces from multiple perspectives, including lexical similarity, semantic similarity, and human judgment. We conducted extensive distillation experiments and demonstrate that our method effectively degrades the performance of distilled models across student models up to 32B parameters, varying amounts of training data, and diverse downstream tasks. We will release the code and data to facilitate future research on antidistillation and reasoning trace reformulation."
        },
        {
            "title": "2 Method",
            "content": "2.1 Problem Formulation Knowledge distillation aims to leverage strong teacher model to guide lightweight student model S, transferring the teachers capabilities to the student [GYMT21, XLT+24]. common approach to distilling large language models is supervised fine-tuning (SFT) on the data generated by the teacher. The student model is optimized to maximize the log-likelihood of the teachers output conditioned on query q: LSFT(θS) = 1 (cid:88) t= log pθS (yt y<t, q) (1) For reasoning models, each output sequence = (r, a) consists of reasoning trace and final answer a. To interfere with distillation, we consider transformation : (cid:55) that rewrites the reasoning trace. We keep the final answer unchanged, because it conveys the task outcome from which users extract the final result. Existing proprietary models often adopt restrictive disclosure strategies to prevent unauthorized distillation by others. For example, they omit the reasoning trace entirely or present only high-level summary. However, these ways cause substantial information loss for users. Our goal is therefore to design transformation that meets the following two objectives: dataset DT = {(qi, (ri), ai)}N Interfere with distillation. Make the distilled model SDT , which is trained on the modified i=1, yield degraded downstream performance Perf(SDT ). Information preservation. Ensure that the modified reasoning trace (ri) remains humanreadable and preserves as much useful information as possible in each ri, so that it stays interpretable and useful for human readers. Formally, this trade-off can be posed as the constrained optimization problem: arg min Perf(SDT ) s.t. Sim(ri, (ri)) > τ, (2) where Sim(ri, (ri)) is similarity measure between the original and rewritten reasoning trace, and τ is similarity threshold ensuring that remains sufficiently faithful to from human readers perspective. 3 (a) (b) Figure 2: Predicted probabilities of the student model on teacher-generated reasoning traces. (a) Visualization of token-level predicted probabilities, where deeper red indicates lower probabilities. Teacher-generated traces exhibit frequent self-talk behaviors, which conveys little reasoning content yet receives low probabilities. (b) Tracking the probabilities of self-talk-behavior tokens across training stages reveals that they remain persistently lower than the average probabilities, suggesting that these semantically uninformative expressions exert disproportionate influence on gradient updates. 2.2 Reasoning Trace Reformulation To construct an information-preserving antidistillation reformulation method, the key lies in identifying the differences between how LLMs learn reasoning through SFT and how humans comprehend reasoning traces. Prior studies have shown that reasoning traces that are easily understood by humans are not necessarily suitable for distillation; in fact, manually annotated chain-of-thought data sometimes perform poorly in distillation [CSW+25]. Leveraging this discrepancy, we design reformulation method from two complementary perspectives: the token level and the structural level. 1. Removing self-talk behaviors At the token level, we first analyze how different tokens contribute to the SFT from the gradient perspective. For single time step t, yt denotes the ground-truth token at position t, and define the logits vector is denoted as z(t) RV over the vocabulary of size . The token-level loss is L(t)(θ) = log p(t) yt = log( ez(t) j=1 ez(t) (cid:80)V ). The gradient of this loss with respect to the logits is z(t) L(t) = p(t) eyt, where eyt is the one-hot indicator vector for the target token. The squared ℓ2-norm of the gradient vector is (cid:13)z(t) L(t)(cid:13) (cid:13) 2 2 = (cid:13) (cid:88) (cid:16) i=1 p(t) eyt,i (cid:17) = (cid:88) i=1 (p(t) )2 + 1 2p(t) yt (3) (4) (5) This expression reveals direct dependence on the predicted probability of the correct token p(t) yt . When p(t) is small, the gradient norm grows and signals strong update. This indicates that tokens that the model already predicts with high confidence contribute negligible gradients and quickly fade from influencing optimization. In contrast, yt 1, the gradient approaches zero. When p(t) yt 4 low-probability (i.e., poorly predicted) tokens dominate the effective training signal, guiding the model to adjust its parameters toward correcting these mistakes. Based on the analysis, we examine the predicted probabilities of the student model on teachergenerated reasoning traces. Figure 2(a) visualizes the predicted probabilities on segment of teacher-generated trace, where deeper red indicates lower probabilities. We observe that lowprobability tokens contain many self-talk behaviors, phenomenon where the model often speaks in the first person and employs colloquial expressions such as hmm and wait. These expressions contain little information relevant to reasoning. However, the student model assigns low probabilities to such tokens, which results in large gradient. We further track the predicted probabilities of representative tokens like Hmm and Wait across different training stages. As shown in Figure 2(b), these tokens exhibit persistently lower probabilities than the average token, which implies that such semantically uninformative expressions exert disproportionate influence during parameter updates. Previous studies have also explored the different influences of tokens during training. [LGG+24] identified distinct loss patterns across tokens in pretraining: some tokens consistently maintain high or low loss, while only subset exhibits significant loss reduction and are regarded as useful tokens. Tokens associated with self-talk behaviors demonstrate similar pattern, indicating their impact on training. To leverage this, we rewrite the reasoning traces to remove self-talk behaviors. This modification incurs negligible information loss, while deliberately perturbing the gradients associated with lowprobability tokens, thereby affecting the distillation process. 2. Reordering the sub-conclusions At the sequence level, LLMs learn to imitate the overall logical structure of reasoning traces in order to perform reasoning. [LCG+25] demonstrates that structural perturbations to reasoning traces have substantial impact on the performance of distilled models. However, their methods focused on operations such as randomly shuffling or deleting steps, or inserting irrelevant steps, which severely compromise human readability. To design form of structural perturbation that preserves readability for humans, we exploit the difference between generating reasoning process and understanding reasoning process. Reasoning generation proceeds in strictly sequential manner: intermediate steps must be generated before reaching the conclusion. In contrast, comprehension does not require this order; humans often prefer presenting the conclusion first, followed by the supporting process. For example, in mathematical reasoning, lemmas are often stated prior to their proofs, and in academic writing, abstracts precede detailed methods and results. This conclusion-before-process structure can even enhance human understanding of reasoning. For LLMs, reordering sub-conclusions breaks the chain-of-thought structure of original reasoning traces. Since the computational capacity per step is bounded, LLMs without chain-of-thought can only solve problems of limited complexity [LLZM24]. The models struggle to directly generate correct conclusions without generating reasoning processes first. This limitation makes it difficult for an LLM to distill reasoning traces with conclusion-before-process order. Leveraging this asymmetry, we rewrite reasoning traces by reordering them into conclusionbefore-process structure. Specifically, we prompt GPT-4o [Ope24a] to reformulate reasoning traces in chain-of-thought style, where sub-conclusions are first summarized and then placed before their associated reasoning process. 2.3 Training Compact Reformulation Model In practical applications, it is desirable to minimize the cost of reformulation so as to mitigate its impact on the inference service. To this end, we train compact model for reformulation. Each original reasoning trace is divided into multiple segments, which are reformulated individually and then concatenated back into complete trace. We fine-tune Qwen2.5-1.5B-Instruct [QY+25] using paired data consisting of original reasoning traces and their rewritten counterparts generated by GPT-4o. Compared with advanced reasoning models such as DeepSeek-R1, the reformulation model introduces less than 1% additional parameters and incurs only about 4% extra computational overhead. 5 (a) (b) Figure 3: (a) Match ratios under different lexical similarity score thresholds. PART achieves significant higher match ratios than the summary-based method at both step and sentence levels. (b) Human judgment about informativeness. Compared to original reasoning traces, PART is judged similarly informative; compared to the summary-based reformulation, PART is clearly preferred in terms of the informativeness. Section is an example of reformulation generated by our compact reformulation model. Despite its relatively small size, the model effectively accomplishes the reformulation task, successfully performing removal and reordering while preserving the information. In Section 4.3, we present quantitative evaluation of the reformulation model, demonstrating its ability to achieve both antidistillation with information preservation."
        },
        {
            "title": "3 Quality of Reformulated Reasoning Traces",
            "content": "When reformulating reasoning traces to defend distillation, it is necessary not only to ensure the impact on the performance of distilled models but also to maintain the quality of the reformulated traces. In extreme cases, completely nonsensical reasoning traces would indeed prevent successful distillation, but they would also be unreadable to users and thus fail to convey any useful information. To this end, we evaluate the quality of reformulated reasoning traces using three complementary approaches: lexical similarity, semantic similarity, and human judgment. We compare the quality of traces reformulated by PART with that of the original traces and segment-level summaries. These methods verify that our reformulations can disrupt distillation while still preserving the information of the original reasoning traces, thereby ensuring usability for users. 3.1 Lexical Similarity straightforward way to compare the lexical similarity between the original and reformulated reasoning traces is to perform fuzzy matching at the segment level. Specifically, we split the original reasoning trace into sentences or steps and check whether they can be successfully matched within the reformulated reasoning trace. We employ the partial ratio alignment function from the RapidFuzz library to calculate the similarity score, which first performs substring matching and then computes the normalized Indel similarity based on edit distance. As shown in Figure 3(a), we compute the match ratio under different similarity score thresholds. Whether the matching is conducted at the step or sentence level, PART exhibits remarkably high match ratio, substantially surpassing summary-based approaches across different thresholds. Examples of matched text pairs under different thresholds are shown in Section E. At threshold of 0.7, PART achieved match ratio of 91%, whereas the summary-based method achieved only 18%. This indicates that PART is able to interfere with distillation through only minimal textual modifications. 6 3.2 Semantic Similarity To compare the semantic similarity of reasoning traces, we employed Qwen3-Embedding-4B [ZLL+25] to map reasoning traces into text embedding, under the assumption that semantically closer reasoning traces should yield higher embedding similarity. We treated the original reasoning traces as queries, and the reformulated reasoning traces produced by PART and the summary-based approach as candidate documents. Experimental results show that 97.4% of queries successfully matched their corresponding reformulated reasoning traces. Specifically, 90.1% of queries matched the reasoning traces reformulated by PART, whereas only 7.3% matched those reformulated by the summary-based method. Moreover, the average cosine similarity between the original reasoning traces and those reformulated by PART reached 0.950, compared to 0.889 for the summary-based method. These results demonstrate that PART achieves superior semantic similarity to the original reasoning traces. 3.3 Human Judgment To assess user perceptions of reformulated reasoning traces, we conducted questionnaire study on their perceived informativeness. We sampled 50 original reasoning traces, each paired with two reformulated versions: one generated by PART and one by summary-based method. Each participant evaluated four pairs of traces, indicating their preference (A is better, is better, or Tie). Two pairs compared original traces with those reformulated by PART, and the other two compared summary-based reformulations with those from PART. We collected 31 completed questionnaires, from which we obtained 124 comparisons. As shown in Figure 3(b), when comparing reasoning traces produced by PART with the original traces, most participants judged the information content to be comparable. In contrast, when comparing PART with the summary-based method, participants showed clear preference for PART in terms of the richness of information provided."
        },
        {
            "title": "4 Experiments",
            "content": "4.1 Setup To assess the impact of our reasoning trace reformulation method on the effectiveness of distillation, we distill student models using both the original reasoning traces and the rewritten reasoning traces, and compare their performance. Training Setup. We experiment with student models of different sizes and families. Specifically, we follow DeepSeek-R1 [DAGY+25] in selecting the base models: Qwen2.5-Math-1.5B, Qwen2.5Math-7B [YZH+24], Qwen2.5-14B, Qwen2.5-32B [QY+25]. In addition, we also examine distillation with an instruct model as the student model, for which we use Qwen2.5-7B-Instruct. Since the Qwen2.5-Math models only support maximum context length of 4K tokens, we extend their context window by setting the rope theta parameter to 1,000,000 following [LYC+25]. For distillation data, we used the Bespoke-Stratos-17k [Lab25] and OpenThoughts-114k datasets [GMK+25]. We adopt the Llama-Factory [ZZZ+24] framework to perform SFT. Evaluation Setup. For evaluation, we evaluate the distilled models on MATH-500 [HBK+21, LKB+23], AIME 2024 [MAA24], LiveCodeBench v2 [JHG+24], and GPQA-Diamond [RHS+23], covering tasks in mathematical reasoning, code generation, and scientific question answering. To obtain more reliable estimates of pass@1 accuracy, we sample multiple responses per query, thereby reducing variance in the results. Details of training and evaluation setup are provided in Section C. 4.2 Results Table 1 reports the performance of distilled models across different benchmarks. It shows that, regardless of student model size or benchmark, PART consistently leads to significant degradation in the performance of distilled models, thereby providing an effective defense against distillation. For example, even the performance of large 32B student model decreases from 54.17 to 46.88 on AIME 2024, corresponding to 13.5% degradation. 7 Table 1: Performance of distilled models on various benchmarks. MATH500 refers to MATH-500, AIME24 to AIME 2024, LCBv2 to LiveCodeBench v2, and GPQA-D to GPQA-Diamond. More negative values of indicate stronger antidistillation effects."
        },
        {
            "title": "Data",
            "content": "MATH500 AIME24 LCBv2 GPQA-D Average Training Data: Bespoke-Stratos-17k Qwen2.5-Math-1.5B Qwen2.5-Math-7B Qwen2.5-14B Qwen2.5-32B Qwen2.5-7B-Instruct Qwen2.5-Math-7B original PART original PART original PART original PART original PART 72.55 59.05 -13.50 88.95 80.00 -8.95 90.60 82.25 -8.35 92.65 89.65 -3.00 83.05 70.85 -12.20 15.00 8.75 -6.25 32.71 22.08 -10.63 43.75 25.83 -17.92 54.17 46.88 -7.29 21.04 12.29 -8.75 12.23 9.88 -2.35 34.88 28.96 -5.92 55.58 43.98 -11.60 70.99 62.38 -8.61 36.64 27.84 -8. Training Data: OpenThoughts-114k original PART 90.40 78.50 -11.90 46.67 28.54 -18.13 41.98 30.87 -11.11 29.80 25.88 -3.92 43.18 38.00 -5.18 53.28 46.97 -6.31 61.24 55.68 -5.56 43.31 32.32 -10. 45.20 36.49 -8.71 32.40 25.89 -6.51 49.93 42.26 -7.67 60.80 49.76 -11.05 69.76 63.65 -6.12 46.01 35.83 -10.19 56.06 43.60 -12.46 For the student model in reasoning distillation, some studies adopt base model [DAGY+25, LYC+25], while others use an instruct model [Lab25]. We conducted experiments on both choices of student models. When using Qwen2.5-Math-7B as the student model, the average score decreases by 7.67, and when using Qwen2.5-7B-Instruct as the student model, the score decreases by 10.19. This demonstrates that PART is effective against different types of student models. 4.3 Effectiveness of the Reformulation Model To evaluate the generalization capability of the reformulation model, we trained 1.5B reformulation model on reformulated data generated by GPT-4o using the Bespoke-Stratos-17k dataset and applied it to reformulate OpenThoughts-114k. As shown in Table 1, the reasoning traces produced by this small reformulation model also effectively defend against distillation: student models distilled on the reformulated data exhibit significant performance degradation. We further evaluated the quality of traces generated by the reformulation model. For lexical similarity, the match ratio reached 88% under threshold of 0.7. For semantic similarity, the average cosine similarity was 0.94. These similarity metrics are close to those obtained with GPT-4o reformulations and substantially higher than those of the summary-based method. This demonstrates that our reformulation model is also effective in preserving information during reformulation. 4.4 Detectability An additional property of PART is detectability. Similar to LLM watermarking, models distilled on PART-reformulated data can be distinguished from those trained on original reasoning traces. Due to the removal of self-talk behaviors, the distributional patterns of the data undergo significant changes. We computed the term frequency of keywords related to self-talk behaviors: in the original traces, the average frequency of such keywords reached 2.9%, whereas in the reformulated traces it dropped to only 0.4%. Leveraging this substantial discrepancy, even simple classifier based on term-frequency threshold is sufficient to achieve separation, yielding an F1 score of 0.93 and true-positive rate of 88.3% at 1% false-positive rate (TPR@FPR). 8 (a) (b) Figure 4: Performance comparison (a) across different student model sizes and (b) across different data scales of distilled models trained on original versus reformulated traces. Across both factors, PART leads to consistent performance degradation of the distilled models, demonstrating its effectiveness as an antidistillation approach. 4.5 Robustness to Data Scale To evaluate the effectiveness of PART under varying amounts of training data, we sampled subsets of different sizes from the OpenThoughts-114K dataset and its corresponding reformulated traces. As shown in Figure 4(b), PART consistently led to significant degradation in distilled model performance across different data scales. Notably, models trained on large number of reformulated traces still underperformed compared to those trained on only smaller number of original traces. This finding indicates that it is costly to collect more data to offset the impact of PART."
        },
        {
            "title": "5 Conclusion",
            "content": "We presented PART, an information-preserving reformulation of reasoning traces for antidistillation. Leveraging the difference between how humans comprehend reasoning process and how LLMs acquire reasoning ability via supervised fine-tuning, PART applies two simple but effective steps: removing self-talk tokens and reorder sub-conclusions before their supporting process. Across lexical, semantic, and human-judgment evaluations, PART retains the information of original traces, substantially outperforming summary-based method. Distillation experiments show consistent degradation for student models trained on reformulated traces across various benchmarks, robust to model size and data scale. Overall, PART offers practical method to balance interpretability with protection of model intellectual property."
        },
        {
            "title": "References",
            "content": "[CSW+25] Xinghao Chen, Zhijing Sun, Guo Wenjin, Miaoran Zhang, Yanjun Chen, Yirong Sun, Hui Su, Yijie Pan, Dietrich Klakow, Wenjie Li, and Xiaoyu Shen. Unveiling the key factors for distilling chain-of-thought reasoning. In Wanxiang Che, Joyce Nabende, Ekaterina Shutova, and Mohammad Taher Pilehvar, editors, Findings of the Association for Computational Linguistics: ACL 2025, pages 1509415119, Vienna, Austria, July 2025. Association for Computational Linguistics. [DAGY+25] DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, 9 Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J. L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, R. J. Chen, R. L. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, S. S. Li, Shuang Zhou, Shaoqing Wu, Shengfeng Ye, Tao Yun, Tian Pei, Tianyu Sun, T. Wang, Wangding Zeng, Wanjia Zhao, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, W. L. Xiao, Wei An, Xiaodong Liu, Xiaohan Wang, Xiaokang Chen, Xiaotao Nie, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, X. Q. Li, Xiangyue Jin, Xiaojin Shen, Xiaosha Chen, Xiaowen Sun, Xiaoxiang Wang, Xinnan Song, Xinyi Zhou, Xianzu Wang, Xinxia Shan, Y. K. Li, Y. Q. Wang, Y. X. Wei, Yang Zhang, Yanhong Xu, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Wang, Yi Yu, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yuan Ou, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He, Yunfan Xiong, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Y. X. Zhu, Yanhong Xu, Yanping Huang, Yaohui Li, Yi Zheng, Yuchen Zhu, Yunxian Ma, Ying Tang, Yukun Zha, Yuting Yan, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhenda Xie, Zhengyan Zhang, Zhewen Hao, Zhicheng Ma, Zhigang Yan, Zhiyu Wu, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Zizheng Pan, Zhen Huang, Zhipeng Xu, Zhongyu Zhang, and Zhen Zhang. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. [DSG+24] Sumanth Dathathri, Abigail See, Sumedh Ghaisas, Po-Sen Huang, Rob McAdam, Johannes Welbl, Vandana Bachani, Alex Kaskasoli, Robert Stanforth, Tatiana Matejovicova, et al. Scalable watermarking for identifying large language model outputs. Nature, 634(8035):818823, 2024. [GMK+25] Etash Guha, Ryan Marten, Sedrick Keh, Negin Raoof, Georgios Smyrnis, Hritik Bansal, Marianna Nezhurina, Jean Mercat, Trung Vu, Zayne Sprague, Ashima Suvarna, Benjamin Feuer, Liangyu Chen, Zaid Khan, Eric Frankel, Sachin Grover, Caroline Choi, Niklas Muennighoff, Shiye Su, Wanjia Zhao, John Yang, Shreyas Pimpalgaonkar, Kartik Sharma, Charlie Cheng-Jie Ji, Yichuan Deng, Sarah Pratt, Vivek Ramanujan, Jon Saad-Falcon, Jeffrey Li, Achal Dave, Alon Albalak, Kushal Arora, Blake Wulfe, Chinmay Hegde, Greg Durrett, Sewoong Oh, Mohit Bansal, Saadia Gabriel, Aditya Grover, Kai-Wei Chang, Vaishaal Shankar, Aaron Gokaslan, Mike A. Merrill, Tatsunori Hashimoto, Yejin Choi, Jenia Jitsev, Reinhard Heckel, Maheswaran Sathiamoorthy, Alexandros G. Dimakis, and Ludwig Schmidt. Openthoughts: Data recipes for reasoning models, 2025. [GYMT21] Jianping Gou, Baosheng Yu, Stephen Maybank, and Dacheng Tao. Knowledge distillation: survey. International journal of computer vision, 129(6):17891819, 2021. [HBK+21] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2), 2021. [HZL+24] Zhen Huang, Haoyang Zou, Xuefeng Li, Yixiu Liu, Yuxiang Zheng, Ethan Chern, Shijie Xia, Yiwei Qin, Weizhe Yuan, and Pengfei Liu. O1 replication journey part 2: Surpassing o1-preview through simple distillation, big progress or bitter lesson?, 2024. 10 [JHG+24] Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando Solar-Lezama, Koushik Sen, and Ion Stoica. Livecodebench: Holistic and contamination free evaluation of large language models for code. In The Thirteenth International Conference on Learning Representations, 2024. [JSA25] Abhinav Java, Simra Shahid, and Chirag Agarwal. Towards operationalizing right to data protection. In Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 81918205, 2025. [KGW+23] John Kirchenbauer, Jonas Geiping, Yuxin Wen, Jonathan Katz, Ian Miers, and Tom Goldstein. watermark for large language models. In International Conference on Machine Learning, pages 1706117084. PMLR, 2023. [Lab25] Bespoke Labs. Bespoke-stratos: The unreasonable effectiveness of reasoning distillation. https://www.bespokelabs.ai/blog/bespoke-stratos-the-unreasonableeffectiveness-of-reasoning-distillation, 2025. Accessed: 2025-01-22. [LCG+25] Dacheng Li, Shiyi Cao, Tyler Griggs, Shu Liu, Xiangxi Mo, Eric Tang, Sumanth Hegde, Kourosh Hakhamaneshi, Shishir G. Patil, Matei Zaharia, Joseph E. Gonzalez, and Ion Stoica. Llms can easily learn to reason from demonstrations structure, not content, is what matters!, 2025. [LCX+25] Jiahao Li, Yiqiang Chen, Yunbing Xing, Yang Gu, and Xiangyuan Lan. survey on unlearnable data, 2025. [LGG+24] Zhenghao Lin, Zhibin Gou, Yeyun Gong, Xiao Liu, Yelong Shen, Ruochen Xu, Chen Lin, Yujiu Yang, Jian Jiao, Nan Duan, and Weizhu Chen. Not all tokens are what you need for pretraining. In A. Globerson, L. Mackey, D. Belgrave, A. Fan, U. Paquet, J. Tomczak, and C. Zhang, editors, Advances in Neural Information Processing Systems, volume 37, pages 2902929063. Curran Associates, Inc., 2024. [LKB+23] Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step. In The Twelfth International Conference on Learning Representations, 2023. [LL23] Xinzhe Li and Ming Liu. Make text unlearnable: Exploiting effective patterns to protect personal data. In Proceedings of the 3rd Workshop on Trustworthy Natural Language Processing (TrustNLP 2023), pages 249259, 2023. [LLZM24] Zhiyuan Li, Hong Liu, Denny Zhou, and Tengyu Ma. Chain of thought empowers transformers to solve inherently serial problems. In The Twelfth International Conference on Learning Representations, 2024. [LTQ+25] Pingzhi Li, Zhen Tan, Huaizhi Qu, Huan Liu, and Tianlong Chen. Doge: Defensive output generation for llm protection against knowledge distillation. arXiv preprint arXiv:2505.19504, 2025. [LYC+25] Zihan Liu, Zhuolin Yang, Yang Chen, Chankyu Lee, Mohammad Shoeybi, Bryan Catanzaro, and Wei Ping. Acereason-nemotron 1.1: Advancing math and code reasoning through sft and rl synergy, 2025. [MAA24] MAA. American invitational mathematics examination (aime), 2024. [Ope24a] OpenAI. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. [Ope24b] OpenAI. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. [PLH+25] Leyi Pan, Aiwei Liu, Shiyu Huang, Yijian Lu, Xuming Hu, Lijie Wen, Irwin King, and Philip S. Yu. Can llm watermarks robustly prevent unauthorized knowledge distillation?, 2025. 11 [QY+25] Qwen, :, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tianyi Tang, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 technical report, 2025. [RHS+23] David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel Bowman. Gpqa: graduate-level google-proof q&a benchmark. In First Conference on Language Modeling, 2023. [STF+25] Yash Savani, Asher Trockman, Zhili Feng, Avi Schwarzschild, Alexander Robey, arXiv preprint Antidistillation sampling. Marc Finzi, and Zico Kolter. arXiv:2504.13146, 2025. [Tea25] Qwen Team. QwQ-32B: Embracing the power of reinforcement learning, March 2025. [XLT+24] Xiaohan Xu, Ming Li, Chongyang Tao, Tao Shen, Reynold Cheng, Jinyang Li, Can Xu, Dacheng Tao, and Tianyi Zhou. survey on knowledge distillation of large language models, 2024. [YZH+24] An Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong Tu, Jingren Zhou, Junyang Lin, Keming Lu, Mingfeng Xue, Runji Lin, Tianyu Liu, Xingzhang Ren, and Zhenru Zhang. Qwen2.5-math technical report: Toward mathematical expert model via self-improvement, 2024. [ZLL+25] Yanzhao Zhang, Mingxin Li, Dingkun Long, Xin Zhang, Huan Lin, Baosong Yang, Pengjun Xie, An Yang, Dayiheng Liu, Junyang Lin, Fei Huang, and Jingren Zhou. Qwen3 embedding: Advancing text embedding and reranking through foundation models. arXiv preprint arXiv:2506.05176, 2025. [ZWP+25] Han Zhao, Haotian Wang, Yiping Peng, Sitong Zhao, Xiaoyu Tian, Shuaiting Chen, Yunjie Ji, and Xiangang Li. 1.4 million open-source distilled reasoning dataset to empower large language model training, 2025. [ZZZ+24] Yaowei Zheng, Richong Zhang, Junhao Zhang, Yanhan Ye, Zheyan Luo, Zhangchi Feng, and Yongqiang Ma. Llamafactory: Unified efficient fine-tuning of 100+ lanIn Proceedings of the 62nd Annual Meeting of the Association for guage models. Computational Linguistics (Volume 3: System Demonstrations), Bangkok, Thailand, 2024. Association for Computational Linguistics."
        },
        {
            "title": "A Related Work",
            "content": "Reasoning Distillation With the success of test-time scaling [Ope24b, DAGY+25, Tea25], an increasing number of studies have focused on distilling reasoning ability into smaller models. O1 Journey demonstrates that base model fine-tuned on only tens of thousands of reasoning traces can outperform O1-preview [HZL+24]. DeepSeek-R1 adopts reinforcement learning for training, followed by distillation to obtain efficient smaller models [DAGY+25]. In addition, several datasets have collected large-scale reasoning traces from advanced reasoning modelsranging from tens of thousands to millionswhich have been used to train strong distilled models [Lab25, GMK+25, ZWP+25]. LLM Watermarking LLM watermarking focuses on tracking the text generated by LLMs and identifying whether given piece of text was produced by particular LLM. Common approaches achieve this by manipulating the sampling distribution during generation, ensuring detectability without compromising readability or fluency [KGW+23, DSG+24]. [PLH+25] further explores whether the watermark can still be detected when student model is distilled using outputs from protected LLM. While LLM watermarking is also related to model intellectual property detection, its primary emphasis is on post-hoc detection rather than proactively interfering with distillation. Unlearnable Data Unlearnable data focuses on perturbing training data to degrade model performance [LCX+25]. [LL23] introduces hints into the input text, such as inserting class-wise symbols. RegText treats low-frequency, task-representative tokens as spurious words and randomly inserts these spurious words into the text. These approaches emphasize modifications to the input data, inducing models to rely on shortcuts [JSA25]. However, they are unsuitable for antidistillation, since we cannot alter the prompts used by the attacker. Moreover, such methods are typically limited to classification tasks. In contrast, our approach modifies the models generation and does not rely on task-specific designs. Antidistillation Recent work has also begun to explore antidistillation for reasoning models. Antidistillation Sampling poisons reasoning traces by modifying models next-token probability distribution during sampling [STF+25]. This method requires two auxiliary models: proxy student model, and variant of the proxy model obtained by performing single step of gradient ascent on the downstream loss. At each reasoning step, the difference between the logits of these two models is computed to form perturbation vector. Another method DOGe defends against distillation by fine-tuning the teacher model itself, jointly minimizing the SFT loss while maximizing the KL divergence between the teacher model and the proxy student model [LTQ+25]. Both approaches interfere with the teacher modeleither by altering its sampling distribution or modifying its parameters. Moreover, their effectiveness has only been demonstrated on small student models (< 4B parameters). By contrast, PART introduces reasoning traces reformulation that do not affect the teacher models ability to generate correct answers, and has been validated as effective across student models up to 32B in scale."
        },
        {
            "title": "B Prompts",
            "content": "Removing Self-talk Behaviors Rewrite the given text, which is part of complete reasoning process. Convert only the parts expressed in self-talk style into declarative format. Avoid using first-person expressions such as I, me, we, or lets. Do not alter any parts that are not self-talk; keep them exactly as in the original text. Do not add any extra information. Do not include any introductory phrases. Text: 13 Reordering the Sub-conclusions You will process the given text in two steps. The given text is part of complete reasoning process. Step 1: Extract and list the most important sub-conclusions in the given reasoning process. Keep the number of sub-conclusions small and focused. Wrap the sub-conclusions in the tags <SUB> and </SUB> for easy extraction. Step 2: Move the sentences corresponding to these sub-conclusions to appear *before* their respective reasoning processes. Keep the sub-conclusions unnumbered and naturally integrated into the context. Do not modify any other parts of the original text. Wrap the entire transformed text in the tags <REWRITTEN> and </REWRITTEN> for easy extraction. Text:"
        },
        {
            "title": "C Hyperparameter",
            "content": "Table 2: Hyperparameters used in training. We use larger learning rate for smaller model. Hyperparameter Default 1.5B Model Cutoff length Batch size Learning rate Epochs LR scheduler Warmup ratio 16,384 128 3.0e1.0e-4 3 Cosine 0.1 Table 3: Hyperparameters used in evaluation. For AIME24, we increase the number of responses per query due to the small number of problems. Hyperparameter Default AIME24 Temperature Top-p Maximum number of tokens to generate Number of responses per query 0.6 0.95 32,768 4"
        },
        {
            "title": "Reformulated Traces",
            "content": "I need to solve this Okay, lets see. problem where three people (Chef, head server, sous-chef) have transceivers that can communicate directly if within meters. If not, but theres third person acting as bridge, then they can still communicate. The goal is to check if all three can communicate with each other, possibly through the third. Hmm. So for each test case, given and three points, determine if the three form connected graph where each pair is either directly connected (distance R) or connected via the third. Wait, but how exactly? Lets think. The communication can go through one intermediary. So all three must be in chain where each consecutive pair is within R, or perhaps any two are connected via path through the third."
        },
        {
            "title": "E Lexical Similarity",
            "content": "Similarity: 0.63 The goal is to check if all three can communicate with each other, possibly through the third. The problem involves three people (Chef, head server, souschef) who have transceivers that can communicate directly if within meters. If not, but theres third person acting as bridge, then they can still communicate. For each test case, given and three points, it is necessary to determine if the three form connected graph where each pair is either directly connected (distance R) or connected via the third. The condition is that all three can communicate with each other, possibly through one another. The communication can go through one intermediary. Therefore, all three must be in chain where each consecutive pair is within R, or any two are connected via path through the third. Original Step: But wait, let me verify this again to be sure. Because sometimes with clock angle problems, there can be another instance where the angle is 110 degrees, but depending on the direction (whether the minute hand is ahead or behind the hour hand). Let me think. Matched Part of Reformulated Trace: Verification is necessary to ensure accuracy, as there can be another instance where the angle is 110 degrees, depending on the direction of the hands. The formula 30H - 5.5M provides the absolute angle between the two hands. Similarity: 0.75 Original Step: Okay, let me try to figure out this problem. So, we have set of consecutive positive integers starting from 1, and one number is erased. The average of the remaining numbers is 35 and 7/17. We need to find out which number was erased. The options are from 6 to 9, or cannot be determined. Hmm. Matched Part of Reformulated Trace: The problem involves set of consecutive positive integers starting from 1, with one number erased. The average of the remaining numbers is 35 and 7/17. The objective is to identify the erased number. The options range from 6 to 9, or indicate that the number cannot be determined. 15 Similarity: 0.81 Original Step: Wait, just to be thorough, lets make sure none of the other options could work. Lets check option E: = 10ˆx. If we take = 0, then = 1, which would correspond to the rotated point (0,1), which is correct. But lets take another point. If = -1, = 10ˆ(-1) = 0.1, but according to G, when = -1, should be 10. But according to option E, = 10ˆ(-1) = 0.1, which is wrong. So is out. Option D, as we saw, gives = 10ˆ1 = 10 when = -1, which is correct. Matched Part of Reformulated Trace: Check option E: = 10ˆx. Taking = 0, then = 1, which would correspond to the rotated point (0,1), which is correct. However, taking another point: if = -1, = 10ˆ(-1) = 0.1, but according to G, when = -1, should be 10. According to option E, = 10ˆ(-1) = 0.1, which is wrong. Thus, is out. Option gives = 10ˆ1 = 10 when = -1, which is correct. Similarity: 0. Original Step: So Thuy is 21, Kareem 22. So Kareem is indeed higher than Thuy, and Jose is the lowest of the three. Wait, but Thuy is 21, Jose is 20, Kareem 22. So the order from highest to lowest is Kareem, Thuy, Jose. So the largest is Kareem. Therefore, answer C. Matched Part of Reformulated Trace: So Thuy is 21, Kareem 22. So Kareem is indeed higher than Thuy, and Jose is the lowest of the three. Thuy is 21, Jose is 20, Kareem 22. So the order from highest to lowest is Kareem, Thuy, Jose. So the largest is Kareem. Therefore, answer C."
        }
    ],
    "affiliations": [
        "IAIR, Xian Jiaotong University",
        "Microsoft Research"
    ]
}