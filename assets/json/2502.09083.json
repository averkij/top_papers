{
    "paper_title": "Show Me the Work: Fact-Checkers' Requirements for Explainable Automated Fact-Checking",
    "authors": [
        "Greta Warren",
        "Irina Shklovski",
        "Isabelle Augenstein"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The pervasiveness of large language models and generative AI in online media has amplified the need for effective automated fact-checking to assist fact-checkers in tackling the increasing volume and sophistication of misinformation. The complex nature of fact-checking demands that automated fact-checking systems provide explanations that enable fact-checkers to scrutinise their outputs. However, it is unclear how these explanations should align with the decision-making and reasoning processes of fact-checkers to be effectively integrated into their workflows. Through semi-structured interviews with fact-checking professionals, we bridge this gap by: (i) providing an account of how fact-checkers assess evidence, make decisions, and explain their processes; (ii) examining how fact-checkers use automated tools in practice; and (iii) identifying fact-checker explanation requirements for automated fact-checking tools. The findings show unmet explanation needs and identify important criteria for replicable fact-checking explanations that trace the model's reasoning path, reference specific evidence, and highlight uncertainty and information gaps."
        },
        {
            "title": "Start",
            "content": "Show Me the Work: Fact-Checkers Requirements for Explainable Automated Fact-Checking Irina Shklovski ias@di.ku.dk University of Copenhagen Copenhagen, Denmark Linköping University Linköping, Sweden Isabelle Augenstein augenstein@di.ku.dk Department of Computer Science, University of Copenhagen Copenhagen, Denmark Greta Warren grwa@di.ku.dk Department of Computer Science, University of Copenhagen Copenhagen, Denmark 5 2 0 2 3 1 ] . [ 1 3 8 0 9 0 . 2 0 5 2 : r Abstract The pervasiveness of large language models and generative AI in online media has amplified the need for effective automated fact-checking to assist fact-checkers in tackling the increasing volume and sophistication of misinformation. The complex nature of fact-checking demands that automated fact-checking systems provide explanations that enable fact-checkers to scrutinise their outputs. However, it is unclear how these explanations should align with the decision-making and reasoning processes of fact-checkers to be effectively integrated into their workflows. Through semistructured interviews with fact-checking professionals, we bridge this gap by: (i) providing an account of how fact-checkers assess evidence, make decisions, and explain their processes; (ii) examining how fact-checkers use automated tools in practice; and (iii) identifying fact-checker explanation requirements for automated fact-checking tools. The findings show unmet explanation needs and identify important criteria for replicable fact-checking explanations that trace the models reasoning path, reference specific evidence, and highlight uncertainty and information gaps. CCS Concepts Human-centered computing Empirical studies in HCI; Empirical studies in collaborative and social computing; Computing methodologies Natural language processing. Keywords Explainable AI, fact-checking, explanation, natural language processing, misinformation ACM Reference Format: Greta Warren, Irina Shklovski, and Isabelle Augenstein. 2025. Show Me the Work: Fact-Checkers Requirements for Explainable Automated FactChecking. In CHI Conference on Human Factors in Computing Systems (CHI 25), April 26-May 1, 2025, Yokohama, Japan. ACM, New York, NY, USA, 21 pages. https://doi.org/10.1145/3706598."
        },
        {
            "title": "1 Introduction\nThe acceleration of misinformation and disinformation in recent\nyears means that the role of fact-checkers in verifying public infor-\nmation continues to grow in scale and urgency [2]. Fact-checking",
            "content": "CHI 25, April 26-May 1, 2025, Yokohama, Japan 2025. This is the authors version of the work. It is posted here for your personal use. Not for redistribution. The definitive Version of Record was published in CHI Conference on Human Factors in Computing Systems (CHI 25), April 26-May 1, 2025, Yokohama, Japan. ACM ISBN 979-8-4007-1394-1/25/04 https://doi.org/10.1145/3706598.3713277 organisations have struggled to keep up with the increasing volume and sophistication of disinformation spread online [11, 34]. These concerns have been aggravated by the rise of generative AI. For example, publicly available Large Language Models (LLMs) such as ChatGPT1 enable the generation of text to facilitate the production of false news articles and information on social media at vast scale and speed [15], while recent developments in synthetic media generation enable production of convincing images, audio and video to mislead the public [115]. The threat to societal stability posed by misinformation has bolstered calls for the development of AI-based tools to increase fact-checkers capacity to debunk false claims, by fully or partially automating the fact-checking process [32]. This need becomes critical as recent report indicates that the number of fact-checking projects worldwide has plateaued in recent years and decreased in 2024 [106]. Despite the increasing amount of misinformation in the world requiring debunking, fact-checking projects are buckling under insufficient funding and political pressures [63, 87]. The development of Natural Language Processing (NLP) technologies for fact-checking is ramping up (see [32, 50, 67] for surveys), but their successful adoption relies on how effectively they can meet the needs of the fact-checkers they are intended for [70, 98]. Prior research has demonstrated that fact-checking is complex endeavour requiring expert knowledge, research skills, and capacity to judge sources appropriately [47, 48, 81]. Fact-checkers themselves also report being broadly sceptical of AI tools and automation, believing that such technology is incapable of handling the intricacies and complexities of fact-checking (e.g., [62, 81, 85]). Previous work notes that fact-checker scepticism towards automated fact-checking tools could be alleviated if the tools could produce explanations and rationale for their outputs [81]. Yet, it is unclear what specific information these tools must provide to be truly useful and how fact-checker demands for explainability and transparency might align with technical capabilities. The fact-checking process tends to have discrete stages with different goals, comprising different tasks [47, 62, 81]. We speculate that at each of these stages, different types of automation and explanations may be required. However, little research has focused on how technical tools might fit into fact-checker processes and how explanations might assist and support fact-checkers in decision-making. To address this gap we conducted an interview study with 10 fact-checking professionals from five continents, addressing the following research questions: 1https://openai.com/chatgpt/ CHI 25, April 26-May 1, 2025, Yokohama, Japan Warren et al. (1) RQ1: What factors do fact-checkers consider important in explaining their decisions and processes? What judgements do fact-checkers make when searching for claims, retrieving and assessing evidence, and making decisions based on evidence? What factors do fact-checkers consider when explaining their fact-checking decisions? (2) RQ2: For which parts of the fact-checking processes are explanations of automated fact-checking systems useful? For which parts of the fact-checking process are explanations necessary? What sort of information should explanations of automated fact-checking systems contain? (3) RQ3: How can automated explanations of fact-checking decisions address the explanatory needs of fact-checkers? What are the properties of good explanation of an automated fact-checking system? What contextual factors influence the utility of automated fact-checking explanations? Our paper makes the following contributions: (i) we provide an account of how fact-checkers select claims for verification, assess evidence, decide verdicts, and explain their processes; (ii) we characterise how fact-checkers use automated tools in their workflow; (iii) we identify the explanation needs of fact-checkers at each step of the fact-checking process; and (iv) we highlight gaps between the current state of automated fact-checking and the practical needs of fact-checkers and make recommendations for addressing these."
        },
        {
            "title": "2 Related work\n2.1 The structure and practice of fact-checking\nFact-checking has its origins in journalism and has emerged as a\ndistinct practice in the last 20 years, spurred on by the proliferation\nof news through online and social media, and increased political\npolarisation [10, 49, 104]. Current literature distinguishes between\nexternal fact-checking, practised by independent fact-checking or-\nganisations (e.g., Snopes,2 Stowarzyszenie Demagog,3 BOOM4),\nwhich involves analysing and verifying public claims such as those\nmade in political statements, news reports, and on social media\nand internal fact-checking, practised by traditional news organisa-\ntions, cross-checking and correcting other journalists’ work before\npublication to filter inaccuracies and to protect the publisher from\npotential liabilities [49, 62]. In this work, we focus on the needs\nof independent (external) fact-checkers, although we anticipate\nthat our findings will be applicable to internal fact-checkers, whose\nwork tasks typically comprise a subset of the fact-checking tasks.\nThe practice of fact-checking encompasses close collaboration be-\ntween fact-checkers and news editors who supervise the work,\ncopy editors responsible for the quality of fact-checks, investigators\nand researchers with expertise in data analysis and visualisation\ntools, and social media managers who disseminate and maximise\nimpact and engagement of fact-checks [62]. Fact-checkers are typi-\ncally tasked with a wide remit of duties, such as monitoring social\nmedia, fielding reader requests, extracting and prioritising claims,",
            "content": "2https://www.snopes.com/ 3https://demagog.org.pl/ 4https://www.boomlive.in/ researching, consulting data and domain experts, assigning veracity labels, and writing up fact-checks [47, 62, 81]. Ethnographic and interview studies have documented how fact-checkers worldwide follow broadly similar processes [47, 62, 66, 81]. Previous work has grouped these processes into four steps that comprise an archetypal fact-checking pipeline: (i) choosing claims to check; (ii) searching for evidence; (iii) assigning verdict; and (iv) writing and communicating the fact-check [32, 50, 62, 81].5 Previous qualitative studies have provided rich accounts of factcheckers workflows and the challenges they face [81], detailing the human and organisational infrastructures and stakeholder groups that underlie them [62], their core values [36] and tensions between their epistemological ideals and practices [22]. However, the precise decision-making and reasoning processes employed by factcheckers in their work remain unclear. Studies frequently describe the antipathy of fact-checkers towards integrating automated factchecking techniques in their work, referring to their lack of utility in practice and the absence of explanations provided by opaque systems [78, 81]. Yet existing literature does not present tangible solutions for model developers regarding how such flaws can be addressed to support how fact-checkers reason and make decisions about the claims they check. We aim to develop more fine-grained understanding of how fact-checkers evaluate information, assign verdicts, and explain these processes (RQ1), as prerequisite to designing computational systems and tools that support and enhance decision-making."
        },
        {
            "title": "2.2 The state of automated fact-checking\nAutomated fact-checking systems tend to follow a fact-checking\npipeline similar to the one described in the section above [32, 37, 50].\nThe primary tasks are: (i) claim detection and claim filtering [56, 57];\n(ii) evidence retrieval [29, 73]; (iii) veracity prediction [7, 16, 110];\nand (iv) explanation generation [12, 68]. Claim detection involves\nidentifying checkable claims from sources such as social media,\nnews articles or live political debates [57]. Successful techniques\ninvolve human-in-the-loop and semi-supervised active learning\napproaches where the user of a system can provide feedback or\nlabels for selected instances [43, 111]. Additional methods for claim\nfiltering and prioritisation identify urgent claims (based on virality\nor harmfulness) [1, 84, 114]. Stance detection techniques [17, 44, 94]\nare used to classify whether a given piece of evidence supports or\nrefutes a claim. Although veracity prediction is performed based\non the retrieved evidence, the evidence retrieval step is typically\nconducted in a coarse-grained way using standard search engines,\nwhich are optimised for relevance rather than for veracity predic-\ntion, leading to subpar retrieval performance [32, 52, 54]. Previous\nwork in information retrieval (IR) has examined the challenge of re-\ntrieving credible and relevant information from online sources [29].\nAutomated veracity prediction techniques attempt to determine\nthe veracity of a claim given provided evidence, tending to rely\non secondary evidence documents such as news articles [44, 93],\nWikipedia [110] or retrieved online sources [16]. These models vary\nin the number of veracity labels they consider. Where some retain",
            "content": "5Some studies (e.g., [62, 81]) also include the additional step of publishing and disseminating the fact-check in this pipeline, however, as this task is usually performed by social media managers rather than fact-checkers, we focus on the four listed here. Show Me the Work: Fact-Checkers Requirements for Explainable Automated Fact-Checking CHI 25, April 26-May 1, 2025, Yokohama, Japan the original labels assigned by fact-checkers [16], others collapse categories such as true and mostly true, or mixed evidence and unproven [51, 53, 68]. growing body of work has examined methods of generating explanations for the decisions of automated fact-checking systems. Explainable fact-checking methods can be grouped into five categories. Attribution-based explanations highlight aspects of the evidence (e.g., individual tokens or words) that contributed to predicted verdict [94]. Rule-based explanations provide set of rules that describe parts of the decision-making process [45]. Counterfactual or adversarial examples that identify minimal changes in an input that can alter models prediction provide insight to models weaknesses or biases [14]. Case-based explanations provide rationale for models decision by showing how similar instances were assigned the same label by human [31]. Summarisation explanation methods provide natural language summaries of the evidence to demonstrate that it justifies the verdict [12, 68]. All these explanation methods have practical limitations. For example, the validity and utility of feature attributions as explanations is the subject of ongoing debate (e.g., [23, 59]), while rule, summarisation, and casebased explanations assume the existence of an existing knowledge base, fact-checking article already written by fact-checker, and human-annotated similar examples, respectively. Hence, current AI tools for veracity prediction and explanation have limited utility in real-world applications, such as when applied to unseen claims, for which there are no curated evidence documents or fact-checking articles. Not only are comprehensive empirical evaluations of automated fact-checking tools sparse, automated fact-checking research has been criticised for its disconnect from the practical realities of factchecking. Existing evidence demonstrates significant gaps between the capabilities of available automated fact-checking tools and factchecker needs. For example, professional journalists, evaluating one proposed automated tool for veracity prediction, reported that only 59% of the claims were accurately classified, and just 58% of the evidence sentences retrieved were relevant to the claim [82]. content analysis of automated fact-checking research articles found that they were often vague and inconsistent in linking proposed methods to their claimed purposes: only 31% of papers with the stated aim of automating the fact-checking process included evidence retrieval methods, while 81% relied solely on classification (or veracity prediction) [98]. Our work seeks to document the specific tasks where fact-checkers use automated fact-checking tools and where explanations of automated fact-checking are needed (RQ2) and investigate how automated fact-checking tools can better align with fact-checker requirements."
        },
        {
            "title": "2.3 Explainable AI for fact-checking\nVarious desiderata for explanations of AI systems have been pro-\nposed in recent years [70, 86, 105]. Of these, researchers have stip-\nulated eight criteria specific to fact-checking explanations [67].\nThese criteria propose that fact-checking explanations should be\nactionable (i.e., provide steps towards desirable outcome), causal\n(i.e., use a causal model), coherent (i.e., follow natural laws, be rule-\nbased or otherwise deterministic), context-full (i.e., presented in\nthe context of the claim), interactive (i.e., allow users to provide",
            "content": "feedback to the system), parsimonious (i.e., communicate necessary information with minimal redundancy), chronological (i.e., reflect when statement was made and the information available at that time), and impartial (i.e., avoid partisan language or opinions). While these criteria are useful for NLP researchers developing basic explanation techniques, an important caveat is that these criteria are what model developers intuit as satisfactory fact-checking explanation, rather than being grounded in the needs of the people that might use these explanations, such as fact-checkers using automated tools. This limitation is further example of the disconnect between automated fact-checking research and real-world applications and illustrates neglect of stakeholder needs [98], crucial for explanations to be truly useful [70, 74]. Automated fact-checking encompasses different groups of stakeholders (e.g., fact-checkers, content-moderators, model developers, and laypeople), each with distinct explanation needs, tailoring to contextual factors, goals, and levels of expertise [42, 62, 70, 74]. However, existing empirical evaluations of explainable fact-checking are almost exclusively directed at and executed with laypeople from limited selection of Western countries, rather than expert fact-checkers with diverse and varied contexts and perspectives. One such study indicated that neither feature-attribution nor example-based explanations of automated veracity prediction had an effect on laypeoples perceptions of the veracity of news story or their intent to share it, but increased their tendency to over-rely on the AI system when it provided incorrect predictions [75]. separate study also found no effect of example-based explanations on peoples accuracy in predicting the veracity of claim [77]. Illustrating that current explanation methods have little utility for fact-checkers, recent study found that free-text explanations for an automated disinformation detection system improved the performance of laypeople in identifying false information, but not those of journalists [100]. Together, these studies suggest that existing explainability techniques are, at best, ineffective and, at worst, misleading and vulnerable to misuse, potentially bolstering misinformation instead of dispelling it. Both empirical [100] and literature-based [98] analyses indicate that automated explainable fact-checking research is shaped by researcher assumptions rather than by direct engagement with fact-checkers, which hinders their utility [32, 67]. We seek to address these shortcomings by identifying the explanation needs of fact-checkers and how they can be addressed by explainable fact-checking systems (RQ3)."
        },
        {
            "title": "3 Method\nThe central goal of this research is to explore fact-checking as\na primary use case for explainable NLP systems, through a col-\nlaboration between NLP and Human-Computer Interaction (HCI)\nresearchers. We jointly developed new questions and adapted those\nused in prior work on fact-checkers [81] for our interview guide\nfor semi-structured interviews with professional fact-checkers and\njournalists (see Appendix B). To elicit discussion of fine-grained\ndecision-making, we asked fact-checkers to describe their processes\nand reasoning by referencing a claim that they had worked on re-\ncently (RQ1). For each step in the process, we asked participants\nabout their use of automated fact-checking tools and their effective-\nness (RQ2). To uncover specific explanation needs for automated",
            "content": "CHI 25, April 26-May 1, 2025, Yokohama, Japan Warren et al. Participant ID Country Occupation P1 P2 P3 P4 P5 P6 P7 P8 P9 P10 Investigative Journalist Ukraine Fact-Checker Argentina Fact-Checker Poland Investigative Journalist & Trainer USA Poland Fact-Checker Ireland & USA Fact-Checker & Project Manager Poland Zimbabwe Nigeria India Director & Journalist Fact-Checker Investigative Journalist Fact-Checker Organisational Context Freelance Independent Independent Freelance Independent Independent Independent Independent Independent Independent Fact-Checking Experience 8 years 4 years 4 years 12 years 5 years 4 years 4 years 2 years 4 years 6 years Gender Female Female Male Female Female Male Male Female Male Male Table 1: Demographics of the interview participants fact-checking tools, we asked how fact-checkers explained their own decisions and processes and about their understanding of how their automated tools worked (RQ3). Follow-up questions were guided by the interviewees expertise; for instance, participants with more experience using AI-based tools were asked in more detail about how these worked and how they selected certain tools over others. We also adapted the interviews to pursue developing themes. For example, we noticed that fact-checkers were sensitive to confidence scores output by AI tools, so we asked them how they understood such expressions of uncertainty and the strategies they used to resolve them. The study was approved by our institutions Research Ethics Committee (reference number 504-0516/24-5000)."
        },
        {
            "title": "3.1 Participant recruitment\nWe recruited 10 fact-checking professionals (see Table 1 for an\noverview of demographics) by advertising the study on mailing\nlists for fact-checkers and investigative journalists (the Interna-\ntional Fact-Checking Network (IFCN)6 and Hacks/Hackers7), social\nmedia websites (Twitter/X and LinkedIn), and at an international\nconference for professional fact-checkers (GlobalFact11) in June\n2024. Our recruitment material consisted of a brief description of\nthe study, the research team, a link to a webpage containing more\ndetailed information and an online registration form.",
            "content": "In line with prior studies [81], recruiting fact-checkers proved challenging. The interviews took place in June and July 2024, during which several major election campaigns (e.g., the European Parliament elections, Indian general election, United States presidential nominations) took place, which may have exacerbated recruitment challenges. We received large volume of spurious responses (>200), which were identified by rapid form-completion times, invalid links to professional portfolios, and incoherent or inappropriate responses to open-ended questions. After filtering spam responses, we received 31 genuine expressions of interest. We emailed these respondents with information about the study, the information sheet, and an informed consent form. Participants returned the informed consent form, scheduled an interview slot with the first author and filled out short pre-interview questionnaire (Appendix A). 6https://www.poynter.org/ifcn/ 7https://www.hackshackers.com/ Our sample comprised 5 female and 5 male fact-checkers from Europe, Africa, Asia, North America and South America. Eight participants worked for fact-checking organisations, while two (P1 and P4) were freelance investigative journalists who had previously worked for independent fact-checking organisations for at least 8 years. Three participants (P6, P7, and P9) worked for factchecking organisations where they held managerial and editorial roles in addition to fact-checking work. While our participant sample was geographically diverse, previous work has documented that fact-checkers worldwide share common objectives and practices [62, 81]. In keeping with prior studies, our interviewees followed well-defined and consistent work processes, demonstrating many commonalities in the use of automated tools. The diversity of our sample enriched our findings by broadening the range of perspectives and contexts foregrounded in the current work, by providing insight into constraints that fact-checkers adapted to when using automated fact-checking tools, as result of regional (e.g., poor AI tool performance on non-Western languages and accents) and resource-based (e.g., reliance on free or open-source tools) inequities. Our research questions were focused and relatively narrow, and we reached theoretical saturation after eight interviews, at which point no new themes developed from our analyses. We conducted two additional scheduled interviews, which confirmed this."
        },
        {
            "title": "3.3 Data analysis\nWe analysed the interview transcripts using an iterative bottom-\nup, inductive approach inspired by grounded theory [30], using\nNVivo 14 software. The first author coded the transcripts using\nline-by-line open coding, (e.g., \"triangulate verdicts\", \"using multi-\nple tools for diverse results\"). Axial coding (e.g., \"multi-tool usage\")",
            "content": "Show Me the Work: Fact-Checkers Requirements for Explainable Automated Fact-Checking CHI 25, April 26-May 1, 2025, Yokohama, Japan was used to further develop the themes from the data. These codes were reviewed after analysing the first two transcripts and refined accordingly. For example, some participants reported using multiple AI tools to resolve conflicting model uncertainty scores, which was initially assigned to the \"multi-tool usage\" theme. However, after pursuing this theme in later interviews, it became clear that fact-checkers demand for explanations of AI uncertainty was sufficiently distinctive and developed to merit stand-alone theme. The resulting codes were discussed by all authors and categorised into higher-level concepts using selective coding, such as \"AI tools for veracity prediction\". We collaboratively iterated over these concepts to identify relevant themes in the data and map them to our research questions. While there were few outright disagreements during this process, the distinct perspectives of the authors (a team of NLP and HCI researchers) engendered debate and discussion about the technical challenges of addressing the requirements of fact-checkers, as well as fundamental differences in how automated fact-checking is approached in NLP research (e.g., often in artificial set-ups with the aim of improving model performance on benchmark datasets [98]) compared to fact-checking in the wild. These discussions shaped our aim to identify lacunas in NLP and HCI research with potential for concrete improvements in automated tools for fact-checkers. Table 2 displays the main themes related to our research questions developed through our analysis. More detailed examples of the subthemes and codes are available in Appendix C. Research Questions RQ1: What factors do fact-checkers consider important in explaining their decisions and processes? RQ2: For which parts of the fact-checking process are explanations of automated fact-checking systems useful? Themes Evidence quality (10) Deciding verdicts (10) Verdict-dependent (6) Communicating complexity (9) Claim detection (6) Evidence retrieval (8) Veracity prediction (8) Communicating fact-checks (4) RQ3: How can explanations Explain processes (9) of fact-checking decisions address the explanatory needs of fact-checkers? Replicability (9) Explain uncertainty (8) Verifiability (9) Faithfulness (3) Table 2: Key themes relevant to each research question, with the number of participants who mentioned each theme in parentheses."
        },
        {
            "title": "3.4 Methodological limitations\nSimilar to prior research [81] our sample size is small, given the\nrelatively small global population of fact-checkers.8 Furthermore,\nwe only conducted interviews in English, and though our sample",
            "content": "8As of May 2024, there were 439 independent professional fact-checking projects in 111 countries [106] was geographically diverse and contained many non-native English speakers, this constraint may have limited participation. Our findings are also subject to the inherent subjectivity associated with self-reported data [39]."
        },
        {
            "title": "4.1 Claim detection\n4.1.1 Decision-making in claim detection. Participants described\nselecting claims from trending social media posts, such as TikTok,\nFacebook and Twitter/X, as well as receiving tips from the public\nvia tiplines, which could be particularly useful for claims circulating\non private messaging platforms such as WhatsApp.",
            "content": "P9, who had an editorial role, described the process of his organisations weekly editorial meeting: \"Journalists submit claims for likely verification, which review before they jump on it to do their research [...] we discuss what are the trending claims, the ones that are fact-checkable. [Then] we assign them to our fact-checkers.\" 4.1.2 Automated tools for claim detection. Six participants (P2, P2, P6, P7, P9, P10) reported using AI tools for monitoring social media for detecting check-worthy and potentially harmful claims, such as Logically Accelerate9, Rolli10, CrowdTangle11, as well as Full Fact AIs tool12 for live debate claim-monitoring. Despite the proliferation of such tools, cost played key role for fact-checkers coming from lower-resourced locations (P5, P7, P9, P10): \"Unfortunately, we cant afford to pay for the premium features to use some of those [live debate claim detection] tools again. Thats why we restrict ourselves to the open source that are freely available online\" P9, Investigative Journalist, Nigeria 9https://www.logically.ai/accelerate 10https://rolliapp.com/ 11CrowdTangle was discontinued by Meta in August 2024, and replaced by Meta Content Library (https://transparency.meta.com/researchtools/other-datasets/ crowdtangle) 12https://fullfact.org/ai/about/ CHI 25, April 26-May 1, 2025, Yokohama, Japan Warren et al. Figure 1: Description of fact-checkers AI tool use and explanation needs, contrasted with what AI methods and explanation methods have been researched Yet paid tools, even where fact-checkers had resources to use them, created their own challenges. P5, P9, and P10 explained that using open-source tools allowed readers to replicate their fact-checks once the verdict and its explanation are released: \"We are not using any AI recognition tools that are paid [...] because we want to give chance to our readers to check our facts [...] we use those tools that they can use as well.\" P5, Fact-Checker, Poland 4.1.3 Explanation needs for automated claim detection. Participants were generally positive about tools used for claim detection, noting that they helped to expedite identification of potential checkable claims, reduce work required to monitor multiple media channels, and surface claims they may have otherwise missed. Most reported not giving much thought to how these tools work, but those that did, tended to develop their own folk theories [33] about them. For example, P9 described his theories about how the Full Fact tool for live debate claim detection worked from observing patterns in the claims it detected: \"Any claim that has the most popular, most especially, that has this kind of clause, highest, lowest, the best, the most this, the least this. Ive seen it concurrently picking out words or phrases that has those adjectives. So thats how [...] determine, Okay, this is how this algorithm is working [...] especially any sentences that has figures involved in them, it also flags potential claim for us to verify.\" P9, Investigative Journalist, Nigeria Most participants were not interested in understanding how these tools worked, but appreciated what they perceived as support for time-consuming task. Claim detection is relatively low-stakes task where there are many tools, and thus, opportunities for triangulation. Inaccuracies in particular tool were not deemed significant drawback [32]. Where participants developed their own folk theories, such theories helped them decide when and how to use the tools and how to evaluate the delivered suggestions. Of course, risks associated with automated tools for claim detection remain. For example, existing biases in claim selection (e.g., [102]) may be magnified, or new biases may be introduced in the claim selection process [38]. Here, explanations for how and why particular claims are selected may be helpful, but capacity to coordinate the use of multiple tools and to address language deficiencies in tool performance may be more important."
        },
        {
            "title": "4.2 Evidence retrieval\n4.2.1 Decision-making in evidence retrieval. For our participants,\nevidence retrieval was the core of the fact-checking task. The crucial\naspect of this stage was retrieval of primary sources for evidence,\nsuch as court documents, official statements, a person who appears\nto be the source of the claim, or photos and videos of an alleged\nevent:",
            "content": "\"We dont cite, for example, media citing source, we try to find always the most primary, most original source of data [... If] were talking about migration statistics, we try to look at the migration officials, maybe ask them for an official statement.\" P3, FactChecker, Poland Show Me the Work: Fact-Checkers Requirements for Explainable Automated Fact-Checking CHI 25, April 26-May 1, 2025, Yokohama, Japan P1 explained that she avoided citing secondary sources such as related news articles, because \"media will provide interpretation\". Fact-checkers also emphasised the importance of relying on data from official or reputable sources, such as: \"scientific literature, different institutions that publish data, UN [United Nations], WHO [World Health Organisation], EU [European Union], World in Data...the more raw the data, the better\" (P7). During the investigation process, fact-checkers evaluate and interrogate the retrieved data to narrow down to documents that meet their high standards of credibility and robustness: \"You find different sources, and you have to assess whether theyre okay or not [...] looking at the data that is contained, looking whether the report, or the dataset to the question youre trying to answer. Looking at the methodology, whether [...] there were some articles on controversies about this report saying that... its methodologically incorrect [...] assessing whether the database is, in fact, from the correct time-frame you are working on [...] think you could somehow quantify it and have concrete measure of the quality ... Its about relevance, whether the source, in fact, is relevant to the thing youre trying to assess. And whether the source is in any way modified [...] the less modified, the more raw, think its better.\" P7, Director & Journalist, Poland It is important to note here that in contrast to fact-checker emphasis on primary sources, automated fact-checking techniques tend to rely on secondary sources (e.g., Wikipedia [45], news articles [44, 93], or existing fact-checking articles [12, 68]) in part because primary sources require more complex multi-modal processing. As fact-checkers collect evidence, they evaluate each source and compare sources to each other in terms of coverage and quality, building coherent knowledge base. In contrast, automated tools use stance detection to evaluate evidence documents, considering each document on its own terms. This contrast represents challenges for the adoption of automated fact-checking. Fact-checkers may be unlikely to use system that relies on secondary sources, while automated fact-checking systems may struggle to retrieve important primary evidence to reach verdict. 4.2.2 Automated tools for evidence retrieval. Participants used various automated fact-checking tools for assistance in identifying sources, extracting evidence from large text, video, or audio files, and, in some cases, retrieving evidence. For example, P4 and P7 discussed using AI tools for geolocation estimation, P1 described colleague using tool for facial recognition. P3 and P9 mentioned Bot-o-meter, tool that identified bot-based Twitter/X accounts. Participants (P1, P3, P4, P5, P7, P8) also discussed negative experiences using LLM-based chat-bots such as ChatGPT and Microsoft Copilot for evidence retrieval: \"Weve tested it, you know we asked ChatGPT lot of questions about public figures in the Ukrainian context and it provided us with wrong answers ... shows you that you have to double-check; you cannot just trust that\" P1, Investigative Journalist, Ukraine Instead, ChatGPT was more useful for summarising or sorting through evidence. For example, P7 described creating \"custom GPTs\" to assist with extracting evidence from large files: \"In [Chat]GPT, you can upload the documents, file, and you can then converse with it. You can just ask the questions regarding the document\" P7, Director & Journalist, Poland Overall, fact-checkers tended to be wary of relying on automated tools because they needed to be able to check that the evidence is relevant and from reliable sources. Perhaps the biggest barrier to tool usage was language-specific limitations of current AI systems. Fact-checkers who worked with languages other than English (P2, P6, P8, P9, P10) noted that many automated fact-checking tools were less reliable (if at all) for non-English text and video, as well as for non-Western accents in English. As P9 explained: \"Most of the AI tools that have been developed so far do not understand African accents. So they were not able to identify [...] the way talk, have my Nigerian accent. am not British and not American; Im not European either!\" P9, Investigative Journalist, Nigeria P2 explained that while tools trained in English can be ineffective, where tools in local languages are available, they are appreciated: \"There was tool here in Argentina that was called TranscribeMe... and it was tool curated by Argentinians. So that worked awesomely.\" At the same time, two fact-checkers (P8, P10) also mentioned that the quality of misinformation generated by AI (e.g., deepfake video or audio) was also noticeably lower for languages other than English, making it easier to identify and debunk as false. For the same reason, AI-generated disinformation in these languages can be less prevalent: \"Most of the tools [...] people are using to generate deepfakes [...] they havent been trained on Indian language models [...] Im not 100% confident ... [but] that could be why we did not see much deepfakes, and... maybe its also got to do with the cost as well [...] lets say one party wants to defame the other party by creating, you know, generative [AI] then they have to incur lot of cost.\" P10, Fact-Checker, India LLMs are known to perform less well on low-resource languages [21, 69], though recent research has demonstrated that their performance can be improved by increasing model size [5]. However, larger LLMs also tend to require paid subscriptions (e.g., ChatGPT) or, when open source, remain expensive and compute-intensive to run. Poorer performance on non-English claims is an acknowledged issue by automated fact-checking research, however progress remains slow [117]. 4.2.3 Explanation needs for automated evidence retrieval. Although evidence retrieval is by no means low-stakes task, it is laborious one, so fact-checkers seemed willing to use number of tools without needing too many explicit explanations. The explanations they required tended to focus on just enough information to verify claims and check reasoning. Most fact-checkers described having robust editorial process, in which their work is cross-checked by CHI 25, April 26-May 1, 2025, Yokohama, Japan Warren et al. peers and scrutinised by the editorial team before publication. In the same way, they needed to verify automated system output by checking that the evidence used by the model is appropriate and that the evidence infers the predicted output: \"If it gave me an output of [...] claim that it claimed was fact-checked [...] If it gives sources, would still click through to each of those sources and make sure it says what the AI says it says. But if its correct, then great, then can use the content it gave me.\" P4, Investigative Journalist & Trainer, USA In particular, fact-checkers were interested in why particular document or dataset is seen as relevant, given the claim, or which specific parts of the text or image were deemed important. key challenge for fact-checking is that often, the information required to make conclusive judgement is not readily available or accessible. In these situations, fact-checkers contact expert sources, such as officials or academics: \"[If] this is not something can verify personally, this needs [a] new piece of information. So what you do is you reach out to people who can comment on that. So [...] reached out to environmental experts and said, is this realistic number?\" P1, Investigative Journalist, Ukraine This is much harder challenge for automated systems, which instead must be able to first identify when the evidence required to make veracity prediction is not available, and then explain what information is missing, so that the fact-checker can seek it out from expert sources. Previous work has examined the capacity of automated fact-checking systems to identify where there is insufficient evidence to make veracity prediction [13]. Incorporating this alongside explanations could be potentially useful for fact-checkers."
        },
        {
            "title": "4.3 Verdict decision\n4.3.1 Decision-making in veracity prediction. Everything that fact-\ncheckers do culminates in a verdict. As with any complex decision,\ndespite all the evidence it is often difficult for fact-checkers to\narticulate how, in the end, the decision is actually made [27, 96].\nSeveral fact-checkers struggled to articulate their precise reasoning\nprocess, referring to gut-feeling or \"instinct\" (P7). Participants de-\nscribed sometimes having an intuitive sense that a claim was false,\nbased on years of experience:",
            "content": "\"How do explain this? Because [...] when youve been doing this work for almost 6 years now, your mind just gets ingrained and it processes the misinformation quite quickly.\" P10, Fact-checker, India For most, the responsibility of deciding on final verdict is not an individual one. Fact-checking involves highly collaborative workflows and robust editorial processes [47, 62]. P4 recounted \"having to do that fact-check process every time we did new draft of the story [...] redid the fact-check like 7 times overall\". P3 described his organisations procedures of peer review and collective decisionmaking, and their strength in mitigating potential bias in assigning verdicts: \"This [biased decision-making] we try to diminish [...] all people in the team have to agree for that article to be published, because we have different points of view. So when we agree, then there is bigger chance that there is no risk of bias, because theres always risk of bias.\" P3, Fact-Checker, Poland Nevertheless, when pressed, fact-checkers pointed to evidence as the foundation for their decision rationale. They described laying out clear and logical evidence-based argument when communicating the final verdict to the public. Part of the challenge in deciding whether something is true or false, is that most misinformation claims are neither. good lie, after all, always has grain of truth in it. Fact-checkers (P3 and P7) acknowledged that on one hand, assigning labels such as \"true\", \"false\", \"half-true\" had significant advantages; they are clear and are easily and instantly understood by people. On the other hand, such labels can gloss over nuance and \"make reality much simpler than it is\" (P7). Instead of categorical verdicts, number of fact-checking organisations (e.g., Pagella Politica;13 Full Fact14) publish verdicts one or more sentences in length. These verdicts have the advantage of being able to specify which parts of the claim are true or false, allowing for more nuanced conclusions. P7 suggested that more descriptive verdicts may have more powerful long-term effects on readers: \"What does it mean that something is false? [...] think would prefer [readers] to remember the verdict in form of text [for example,] so this claim is against most recent reports. And it can include words such as false, truth, manipulation, unverifiable, but [its] not the sole purpose\" P7, Director & Journalist, Poland However, P7 also noted that: \"text-like claim and verdicts, they will encourage different type of emotional manipulation, meaning, for example, usage of some kind of verbs, which might have not neutral approaches. Which, again, someone might argue that might lead to some kind of polarisation.\" Descriptions for verdict labels are essentially explanations sometimes, we want them to be simple, while at other times we want more complex answers. The appropriate complexity depends on the particular claim, the context, the audience, and many other factors [61, 76]. Claims that require fact-checking can be politically sensitive or controversial, and verdict formats, whether one-word or more elaborate judgements, have simultaneous advantages and drawbacks. Our participants were conscious that even with longer explanations, sometimes all people will remember is the term \"true\" or \"false\", although at other times the fuller explanation is what matters, and this is not always predictable. 4.3.2 Automated tools for veracity prediction. When asked about the use of AI tools for veracity prediction, our participants were as sceptical as those in prior studies [62, 81]. They were concerned about bias, lack of nuance, and that the task is simply too complex for an automated system: \"AI can do some technical work. It can maybe verify images or videos or visuals, but it wont be able to 13https://pagellapolitica.it/ 14https://fullfact.org/ Show Me the Work: Fact-Checkers Requirements for Explainable Automated Fact-Checking CHI 25, April 26-May 1, 2025, Yokohama, Japan explain complex issues like [...] misrepresented [manipulated] quotes, [...] or some wrong accents in particular statement, [...] you would still need journalist because you would need that cultural, social, background knowledge and ability to explain things and in way thats relevant for you and also for your audience.\" P1, Investigative Journalist, Ukraine Aware of the potential opportunities and difficulties posed by LLMs [113], participants were also concerned about AI hallucination and the reliability of output. One fact-checker highlighted the epistemological debate about the nature of truth as key reason they were sceptical about automated fact-checking: \"AI does not really operate on the true, false categories [...] AI is more concerned with just prediction and results... We as humans have been discussing the definition of truth since the ancient Greeks, and dont really think that such difficult concept can be applied in code, like truth is something more complex to be reliably coded in machine.\" P3, Fact-Checker, Poland Many claims dealt with by fact-checkers are not straightforwardly true or false, and therefore require detailed and complex evidence retrieval and evaluation. This process is extensively documented and an important part of the story that accompanies the verdict. Automated fact-checking systems, in contrast, are primarily concerned with veracity prediction as the ultimate goal, where precision is prioritised over how the verdict was reached. One example of this is the phenomenon of shortcut learning in NLP models, in which models make predictions based on dataset biases as opposed to useful features [40]. 4.3.3 Using AI tools for detecting AI manipulation. The introduction of generative AI has resulted in proliferation of misinformation produced using these tools [41]. Our participants felt they needed to use tools to detect such manipulation or at least to \"confirm\" their own intuitions about AI-generated or manipulated media. P5 and P10 described using tools such as Hive Moderation,15 DeepFake-O Meter,16 and Truemedia.org17 by uploading media or text to the platforms, which return verdict on its authenticity, often accompanied by percent confidence level, such as \"Input is likely to contain AI-generated content 99.9%\". Some participants described paying particular attention to the level of confidence accompanying an output: \"I think the confidence [...] does help, [...] it makes me bit more confident as well. was [...] 50% sure...that this is likely to be AI-generated. But now the tool is also giving me confidence.\" P10, Fact-Checker, India Here too, participants were cognisant that AI tools could be unreliable, especially when the confidence level output did not match their feeling that \"something is off\" (P5) about particular piece of media. Multiple participants (P2, P3, P5, P6, P10) described triangulating verdicts by using several AI tools to predict the veracity of the same input, check the same image or video, seek better quality 15https://hivemoderation.com/ 16https://zinc.cse.buffalo.edu/ubmdfl/deep-o-meter/landing_page 17https://www.truemedia.org/ versions of the same video, and other approaches. Fact-checkers were ultimately aware of being in an \"arms-race\" between misinformation production and detection, particularly as generative models improve and cues to detecting manipulated media become harder to identify with the human eye: \"The purveyors of deepfake technology... they are also getting better... initially [...] their images used to have those extra fingers, those waxy textures, which has improved lot with [...] the new versions of Midjourney and Dall-e coming in... theyve upped their game, so the detection tool also needs to get better and better.\" P10, Fact-Checker, India Most importantly, several participants (P2, P3, P5, P6) pointed to the lack of explainability as contributing factor to feeling these tools are unreliable. P3 in particular highlighted the lack of warrant provided by automated fact-checking tools: \"We do not have reliable epistemology of AI, because its opaque... its not explainable, its not transparent, then we have no reason to trust it unconditionally. We have no warrants to the beliefs that we acquire from our different agents, they do not have warrant... maybe they have justification, but they do not have warrant.\" P3, Fact-Checker, Poland P5 described her concerns about the risk of unintentionally misleading her readers as result of not understanding how the automated tools worked, and pointed out the irony of using uninterpretable AI tools for fact-checking: \"And people... they used to trust things on the Internet, and we show them that they shouldnt. And its good to have [...] trusted tools, not the black box tools to explain them, that something cannot be trusted [...] Its bizarre that we use those tools, that just because they look fine... need to trust them, but would love to give like something better for my readers... They have to trust me, and the tools that use as well.\" P5, Fact-checker, Poland While fact-checkers are increasingly forced to rely on AI tools, especially in tackling misinformation produced through generative AI, they struggle with understanding how these tools work, lacking explanations they find useful and finding the tools unreliable when they do use them. While the accuracy of current tools will likely improve, understanding the rationale for predictions and when they should and should not be relied upon remains paramount. 4.3.4 Explanation needs for automated veracity prediction. While our participants clearly sought explanations for how automated systems might assign verdicts to misinformation, they often pointed to three aspects that an explanation should include - the why, the how, and the who. The Why: Explaining model verdicts. As well as explaining their process, automated fact-checking models must also explain how the evidence they gathered justifies the predicted verdict. common theme was that fact-checking explanations must provide the sources used in verifying the claim (P2, P3, P4, P5, P6, P7, P9, P10). More specifically, fact-checkers discussed the function of explanations as CHI 25, April 26-May 1, 2025, Yokohama, Japan Warren et al. cues for where they needed to look and check for themselves; and providing citations and evidence that they can verify. They were particularly interested in local explanations referring to specific elements of the evidence that lead to the models verdict, such as sections of audio that indicate manipulation, or frames in video where generative-AI artefacts were detected. Such explanations were useful not only for identifying rationales for the tools output, but also for expediting fact-checking: \"If... somethings going on in the audio, like you can represent audio in this waveform, right? And you can tell if it was edited [...] And if youre talking about the video this tool can show me the certain frames where you see that theres something going on with the lips of the person whos appearing on the video, or the movements are not right, but its really specific. [...] That would give me also like picture [...] thats something that can show my readers like [...] you should see this little one-second clip really closely because it shows the flaw of the AI tool... that will make my job easier cause dont always have the time and expertise to look so closely at the media.\" P5, Fact-Checker, Poland Although there are automated methods that can highlight regions where content is contributing to prediction, they are limited to known behaviours and interactions, and overall remain in their infancy for video [28]. While measures of certainty that models output alongside verdict were important, several participants (P2, P3, P5, P7) did not understand how such scores were calculated, and how they should be interpreted, \"What does 65 versus 74 confidence mean?\" (P7). One fact-checker described how quantification of model uncertainty alone raised more questions than it answered, and imagined how verdict uncertainty could be conveyed in human-like way, such as highlighting issues with the reliability of sources used in deciding the verdict: \"If [an automated fact-checking tool] said to me, Im only 70% sure because... [...] for example, here is report that mentions data, you need, but this is report done by scientist who has often had his papers retracted. If it gave out this explanation of the uncertainty, it would be amazing... But just grade of certainty, like this is 70% good data, no, because would be more sceptical, not less.\" P3, Fact-Checker, Poland The How: Explaining model processes. As we have noted, factcheckers consider the path to the fact-checking verdict as important as the verdict itself. It is perhaps unsurprising that our participants were keen to understand the process of how an automated factchecking tool arrived at the verdict. In the absence of explanations, some developed their own intuitions or folk theories about how automated tools worked, such as certain words or phrases that were frequently picked up. However, some fact-checkers (P2, P4, P5, P6) expressed concerns that explanations of AI processes would be too complex or time-consuming to understand. For example, although P2 thought it would be \"useful to know, both processes, to know everything\", she could also \"imagine not understanding lot of the process, if its too technical.\" P4 recalled one automated tool that she used, which had included link to an article which described the technical approach used: \"To their credit, they explained it [automated claimdetection tool], but its like 20-page academic paper about deep learning... thats great, that they have that kind of transparency [...] Im relatively technically competent among journalists... have no idea what that means. So think its great for transparency, but realistically, no ones gonna understand that, at least not journalists.\" P4, Investigative Reporter & Trainer, USA Instead, our participants sought explanations of automated systems expressed in terms of human fact-checking processes. For example, wanting to know all the sources and pieces of evidence that were \"checked\" by the model, how they were checked, and rationale for how evidence was selected: \"If it did give me some specifics like, used this tool to search all tweets they ever posted in the last 12 months. And then narrowed it down to these keywords [...] thats something the journalist can understand, and could be really helpful for them to know ... specifically, how did it do that? Cause... the journalist knows what tweets are.\" P4, Investigative Reporter & Trainer, USA P4 was curious about how the tools and the models were designed but did not see this as relevant to fact-checking itself. Instead, fact-checkers wanted explanations that would align with the factchecking process, showing the work \"like we do as human beings\" (P8). While such an explanation format may seem straightforward, it would be challenging to produce from technical point of view, as the language models that underlie automated fact-checking systems do not employ human-like logical reasoning. Yet fact-checks require transparency, and thus, showing the process is paramount for factchecking tools to be useful. The Who: Explaining model training. Seven participants (P1, P2, P4, P5, P7, P9, P10) expressed that knowing about the origins of an automated fact-checking system, and in particular, knowing the data that the system was trained on, was essential in knowing when to rely on the model. Fact-checkers are acutely aware of their own biases and take steps to mitigate these. The same is true of the tools they might use they need to understand the potential for biases the tools may have so they can mitigate these too. Knowing the sources of training data, the amount of training data, and the languages contained in the training data was seen as critical to identifying information gaps and calibrating when to rely on the model. The latter point was strongly evident for participants who worked in non-English speaking countries, and also for participants from Africa who worked with English, but found that automated tools were ineffective in picking up African accents and regionspecific nuances. P9 expressed his hope that \"organisations working towards building AI tools [...] could consider using more African datasets in developing their tools.\" Increased model transparency was also important from an ethical perspective: Show Me the Work: Fact-Checkers Requirements for Explainable Automated Fact-Checking CHI 25, April 26-May 1, 2025, Yokohama, Japan \"I would like to understand who created this particular application or software? Where did they get all the data? How they were testing it and so on [...] So founders, finances, data sources, the algorithms for generating different responses.\" P1, Investigative Journalist, Ukraine The demand for disclosure of training data is an ongoing debate in the AI community, with proposals such as data sheets for datasets [46] and model cards [83], creating structured ways such information could be disclosed. Yet there remains challenge of not only demanding transparency from the providers of many of the available tools, but also with bridging the gap between the technical idea of what is important to disclose with respect to models and data and what fact-checkers expect given their own professional context."
        },
        {
            "title": "4.4 Communication: explaining fact-checks\n4.4.1 Decision-making in explaining and communicating fact-checks.\nAfter reaching a verdict, fact-checkers typically write an article\nconsisting of the claim, the verdict, and the explanation of how\nthey arrived at the verdict. Participants related how explanations\nare key to communicating their work to the public:",
            "content": "\"Without [explanations] it would be censorship. In my opinion, if you have tool which would be just assessing true-false, true-false without explaining... this would not be okay [...] if big company would employ tool that would be just classifier that would [be] just striking, and think some of them already do, dont think its okay. think there needs to be even [a] simple explanation why the decision was made\" P7, Director & Journalist, Poland As such, the same considerations fact-checkers have for evaluating their own explanations of the final verdict are relevant to how we might need to think about the explanations provided by automated fact-checking systems. There are three main considerations that our participants brought up in how they construct explanations of the fact-check for the public: replicability, the type of verdict, and the complexity of the claim fact-checked. Explanations must be replicable. Replicability was identified as the most prominent theme. All participants discussed the importance of including links to sources, public data, and tools used or referenced in the fact-checking process, with the view to including sufficient information that readers can reconstruct and replicate the fact-check. P1 described this practice of providing and signposting evidence as \"self-explanatory\", while P6 identified this commitment to \"show the work\" as the central tenet of fact-checking: \"I think fact-checking is different branch [of journalism] for that reason... Our work has to be shown, like the kind of math class thing back in the day, you know. Show your work. How did you arrive at this? We have to be able to do this.\" P6, Senior Fact-Checker & Project Manager, Ireland/USA To date, explainable automated fact-checking techniques have focused on explaining only the predicted verdict, that is, how the evidence proves whether claim is true or false [67]. Yet focusing on replicability goes beyond providing set of reasons for why judgement was made. The tools and the relevant evidence should enable replication of the process: \"When we publish something, we want our readers to [...] feel that they can trace every step that we took. Like, go on this website, click, this thing, and now we use this database, search for page 43. Now you enter this website, compare this [...] we spell out every step that we took to have completely replicable analysis. So when we use external tools in our analysis, we also want people to be able to enter this website and see for themselves why we used it, and what judgement it spelled out, and why it convinced us, so why should it convince them.\" P3, Fact-Checker, Poland This means that automated explanations must be able to communicate the sources of evidence (e.g., databases, webpages) that were checked, how they were checked (e.g., search terms used), why sources were selected as evidence (e.g., source reliability or relevance), to the extent that it is possible to reproduce the fact-check. Explanations are verdict-dependent. The category of the verdict assigned to claim influenced the form of explanations participants provided to readers. The common binary of either completely true or completely false was the most straightforward to explain. The typical first step in their investigation involved attempting to verify claim in good faith, and, if unable to do so, first explain why the claim is false, where the false information came from (e.g., whether it materialised from misquotation, cherry-picked data, or pure fabrication), before turning to an explanation of why the correct information is true. Claims that were half-true, or contain some element of truth, tended to be more difficult to fact-check and explain than claims that were completely false, as they tended to require additional context and more nuanced analysis, as well as longer and more complex explanations. \"If something is false, were gonna spend the article explaining how we know this is false... [for] misleading... the underlying logic would basically be, this doesnt prove what you think... theres missing parts here... you might want to consider\" P6, Senior FactChecker & Project Manager, Ireland/USA Another challenging verdict was the inconclusive \"unverifiable\" or \"unproven\" claim. Some fact-checkers (P5, P8, P10) recounted being forced to abandon claims due to lack of available information. Others felt it was still worthwhile to publish such verdicts because \"the lack of information is also information. It also explains to our reader that maybe some commonly-repeated myth is not actually based on good data\" (P3). Similarly to the emphasis placed on replicability, fact-checkers found it valuable to transparently lay out all of the existing evidence, allowing the readers to draw their own conclusions. \"in such cases we put all the evidence, all the facts out there. So its left for the readers to now decide, Okay, these are all the facts. Then you can make up your mind or opinion about it.\" P9, Investigative Journalist, Nigeria CHI 25, April 26-May 1, 2025, Yokohama, Japan Warren et al. Current technical approaches to explainability tend to present the same type of explanation, no matter the context, and this clearly needs consideration. Explaining complex stories. Complex verdicts require complex stories, and complex stories are challenging to tell. All participants commented that it is not possible to predict readers background knowledge and tailor the article accordingly. As result, they have to assume minimal audience background knowledge and provide as much detail as possible. For example, claims related to economic and legal issues were highlighted as particularly challenging to explain in an accessible manner, due to their complexity. Scientific and health-related claims were also difficult to explain in convincing way, partly related to their complexity, and partly \"because theres just lot of people who are hardcore conspiracy theorists. So [...] regardless of data that you provide to them, then its just really difficult to prove anything\" (P1). P1 also noted that claims related to history were easier to communicate \"because its more of narrative format\". Here, emphasis on replicability and clarity of structure were key, such as providing logically coherent summary at the beginning to highlight the core argument: \"This summary needs to be little bit like logical reasoning in philosophy [...] Argument 1, Argument 2, Argument 3, they need to completely justify the last point, that is the conclusion, and in the conclusion, in the last bullet point should be the grade: Is it true? Is it false? Is it manipulation? Is it unverifiable? So the arguments in the bullet point list should lead to the conclusion, and then, if it doesnt, then we stop and work on the article more.\" P3, Fact-Checker, Poland Especially where claims and stories were complex, participants acknowledged that despite emphasis on replicability, the explanation they constructed did not reflect all of the research steps they might have performed. Research is often not straightforward enterprise. The explanation, however, is straight path through what can often be labyrinthine process: \"Taking care that the story and article is written in very logical way... research can be chaotic [...] making sure that the story will be step by step.\" P7, Director & Journalist, Poland. Thus the more complex the claim and the verdict, the more our participants emphasised structure and clarity of explanation. Intimately aware of the challenge and sensitivity of their task, they both sought to provide transparency and to ensure that as many as possible in their audience could easily grasp the point. This consideration of an unknown audience is key to the rhetorical purpose of explanations, but rarely directly considered when these are created automatically. 4.4.2 Automated tools for communicating fact-checks. The availability of LLM-based tools that can produce well-appointed text has led to much debate on the future of journalism in general [90]. Fact-checkers were also acutely aware of these systems and their capabilities. Participants for whom English was not their first language (P2, P3, P5, P7, P8) noted that they at times used LLM-based chatbots such as ChatGPT and Microsoft Copilot18 to improve the quality of their writing, summarise fact-checks (e.g., bullet points or short paragraph at the top of the article), edit articles, and disseminate fact-checks. For example, P7 used the paid subscription to ChatGPT and mentioned using the tool for \"cross-checking, whether there is something to be improved [...] sources would have to be verified, but still... the argumentation could be relatively good\". P8 mentioned using ChatGPT to write scripts for video versions of the fact-check article to be disseminated on social media. Participants (P4, P5, P7) also mentioned ethical concerns about using AI in fact-checking, specifically, the use of news articles for LLM training without permission from journalists: \"In the journalism world, people are relatively sceptical in terms of ChatGPT, like generative AI, because, its stolen work, the journalists work, to learn, without compensating them, and thats the reality.\" P7, Director & Journalist, Poland Such ethical considerations could also manifest as stigma for those using the tools: \"Theres sense of pride in not using these kinds of tools [...] remember we had some discussion about ChatGPT, and [...] people were like, No, dont use it because somebody will tell us, youre using... some other peoples work. P7, Director & Journalist, Poland The fact that LLM-driven systems are only able to produce text because people have produced lot of this text first is intractable. Where these tools could be useful, such considerations remained important, especially in an environment that constantly deals with sensitive and political issues while often struggling with precarity (see also [113]). 4.4.3 Explanation needs for fact-checkers. Our participants made it clear that there is difference between the kinds of explanations they might furnish for their readers and the explanations that they themselves required of the systems they used. Fact-checkers require high threshold of certainty in their verdicts to maintain the confidence of their readers and the general public in their work [81]. While our participants attended to measures of model uncertainty provided by automated tools, for several (P3, P5, P7) numerical measures of confidence in the form of percentages or scores were unhelpful and disconnected from how fact-checkers reason about the reliability of evidence. Measures of confidence are not the same as measures of quality of the evidence used: \"Confidence, depending how, if you would have an idea how to present it, would be good, maybe showing the thought process of tool [...] some kind of idea for showing the quality of the research that the tool has done.\" P7, Director & Journalist, Poland In similar way to how fact-checkers have their work cross-checked and scrutinised by peers and the editorial team prior to publication, our participants expected automated tools to provide information so that they could verify model decisions by checking that the evidence used by the model is appropriate and infers the predicted 18https://copilot.microsoft.com/ Show Me the Work: Fact-Checkers Requirements for Explainable Automated Fact-Checking CHI 25, April 26-May 1, 2025, Yokohama, Japan output. To facilitate this, local explanations of model outputs that refer to specific elements of evidence, such as excerpts of documents or sections of images that contributed to the prediction would be helpful: and reflect on the broader implications of AI in fact-checking, as the system of human fact-checking continues to buckle under the increasing onslaught of misinformation. \"I would love to have, for example, timestamps, where [...] have the video, and this AI recognition tool can give me, for example, timestamps and screenshots of this video in those times... something like, Hey, at this moment you can see..., and you can also show your readers that there is some evidence of AI manipulation.\" P5, Fact-checker, Poland Focusing on specific, checkable parts of evidence may also alleviate concerns about automated tools being too complex to explain to journalists with limited technical backgrounds or expertise, by explaining model decisions in fact-checkers own terms. Finally, faithfulness is an important concern when generating automated explanations. Yet there is significant difference between how automated systems arrive at veracity prediction and how factcheckers make verdict decisions. Our participants had mixed views on how complete and faithful models explanation should be to the underlying decision-making process. Contrasting opinions highlighted practical tensions between explaining automated factchecking in manner that reflects fact-checker processes versus faithfulness to the models own decision-making process. Some participants (P5, P6, P8) expressed desire for explanations that would be easy to understand and explain to readers, even if they were not representative of the tools inner workings: \"I understand... maybe its not working like that, inside this black box. But that would be really useful [...] like to show people what are the clues, that give you this high chance that this video is manipulated.\" P5, Fact-Checker, Poland Others worried that if the gap between the explanation and how the model actually works was too large, then the explanation may not be reliable: \"I would worry like well, is it hallucinating the methodology [...] would have to be pretty confident in the tool to use it that way.\" P4, Investigative Reporter & Trainer, USA This mix of opinions reflects ongoing debates in the field of explainable AI about what constitutes good explanation [58, 74]. Our data demonstrate the need to accommodate different audience needs for explanations and the fact that these can change with context, where both model faithfulness and understandability must be considered."
        },
        {
            "title": "5.1 Fact-checker explanation needs versus",
            "content": "automated fact-checking capabilities The process of fact-checking and communicating verdicts is complex and increasingly requires wide range of AI tools, partly due to the sheer volume of misinformation and partly because much of this misinformation is produced by the same tools used for its detection. Ironically, the capacity to produce misinformation continues to outstrip the capacity to detect it. Moreover, veracity detection tools are insufficient alone; they require explanation and automatic explanation generation is more technically limited still. Our participants explained the complex judgments that impacted their use of automated tools and the shifting explanation needs during the different stages of the fact-checking process. In line with prior work [62, 81], fact-checkers in our study noted the time-consuming nature of identifying and triaging claims. Currently, claim detection is perhaps the task that is best equipped with existing tools (see [32] for review). This is relatively lowstakes task, requiring fewer and less detailed explanations of the underlying processes. However, rationales for how and why particular claims were selected can enable fact-checkers to identify flaws or biases in tools. Existing explainability techniques such as importance rankings for claims and feature importance scores [95] are easily implemented for this task. Stakes increase for evidence retrieval, where fact-checkers described the central role of evaluating the relevance and credibility of evidence and the deficiencies of automated tools for this task. Useful explanations could allow fact-checkers to swiftly verify whether given piece of evidence is reliable and, that it has been correctly interpreted by the model. While existing techniques such as attention highlights for text [94] and saliency maps for images and videos [92] are sometimes used for these purposes [75], there are unresolved issues with regard to their faithfulness to the underlying model and legitimacy as explanations [3, 23, 59]. There are opportunities here for design to present output from existing tools in more useful ways, as HCI research in this area is limited. Reflecting the intricate judgments and decision-making processes described by fact-checkers, veracity prediction is the most complex task in automated fact-checking, and explanations for these decisions require more detail and complexity. Fact-checkers seek specific, local explanations that highlight key parts of the evidence documents and link them to the verdict. Current approaches, such as feature importance scores [79, 95] or summaries of evidence, may be able to at least partially address these needs [12, 68], however, issues have been raised about their stability [8] and reliability [103]. Current approaches to communicating uncertainty (i.e., simple percentages) also seem insufficient as the fact-checkers found it difficult to relate them to the quality of evidence and source reliability and to compare across tools. Although prior work has explored human-interpretable confidence scores using case-based and counterfactual explanations for classification tasks [71, 112], reliable, human-like explanations of uncertainty represent challenge. For CHI 25, April 26-May 1, 2025, Yokohama, Japan Warren et al. example, while LLM expressions of uncertainty can reduce peoples over-reliance on them [65], LLMs tend to overestimate their own confidence [55, 107], so these scores are often unreliable. Explanations must also reflect the context, nuance, and complexity in verdicts and claims themselves. We see this in the distinction between static, temporal, and dynamic facts in LLMs, which may impact the desired level of detail in an explanation [80]. The most significant challenge we identified was the fact-checker desire to align model processes with their own approaches, though the logics are fundamentally different. Part of the reason for this requirement is that fact-checking is not just about the verdict itself, it is also about offering replicable process for reaching it. This requires explaining multiple steps in the fact-checking pipeline (evidence retrieval and veracity prediction). While there is existing work on multi-hop [88] and chain-of-thought [89] reasoning, these continue to fall short of the complex logic required to explain realworld claims [9, 32, 67]. Our findings extend evidence of how fact-checkers utilise technology in their work (see [62, 81, 113]) given the recent rise of generative AI and suggest that previously observed resource, financial, and linguistic inequities may be exacerbated by the prevalence of LLMs. While prior research has found that fact-checker use of tools is \"fragmented\" [81], our findings suggest that this may be partly due to the lack of explanatory information these tools provide, leading fact-checkers to hedge their bets by using multiple tools for the same purpose. Many of our participants expressed similar scepticism about automated fact-checking as observed by previous studies [62, 81], however, we also noted some optimism, particularly in terms of AIs potential to reduce workloads. Finally, our findings provide some limited support for previously-proposed criteria for explainable fact-checking [58, 67]. Our participants expected explanations of fact-checks to mirror human processes. That is, explanations should be coherent (follow natural laws) and causal (follow logical reasoning), as well as context-full, that is, presented alongside the corresponding claim and evidence [67]. Fact-checker concerns about bias in automated fact-checking also supported the proposal that impartiality is an important consideration for fact-checking explanations [67]. The property of parsimony in explanation was reflected in the need fact-checkers expressed for specific explanations that are easily verifiable. However, the emphasis that fact-checkers placed on verifying and replicating the models processes suggests that explanations with these necessary details should not be sacrificed for brevity. As such, key humancentred requirements for explanation of automated fact-checking systems must be replicable, verifiable, and able to explain uncertainty and information gaps."
        },
        {
            "title": "5.2 Design implications: Connecting automated\nfact-checking tools to fact-checker processes\nOur findings highlight fundamental differences in the goals and\nperspectives of fact-checkers relative to automated fact-checking\napproaches. Based on these disconnects, we make the following\nrecommendations for the designers of automated fact-checking\ntechnologies for fact-checkers.",
            "content": "assessment. Fact-checkers stressed the significance of primary evidence like quotes, official records, datasets, photographs and video and audio recordings. In contrast, current automated fact-checking approaches and machine learning (ML) methods more broadly to retrieve, process and reason about unprocessed, unannotated multimodal data [6] tend to rely on secondary sources such as news articles [44, 93], existing fact-checking articles [7, 12, 68], Wikipedia [60, 101], or heterogenous online sources [16, 97]. Fact-checkers cautioned against using such secondary sources as they are susceptible to bias or flawed methodological reasoning, and may have limited direct relevance to the claim. Secondary sources may be easier to use, as they incorporate information retrieval methods which rank sources by popularity, but they entail inherently limited and biased data from the fact-checker point of view. Evaluating secondary sources requires cultural understanding to be able to read between the lines, while primary evidence is, of course, messy, unstructured, and potentially missing or incomplete. At minimum, automated fact-checking must evaluate evidence quality, weigh relevance and reliability as fact-checkers do, and explain these assessments alongside the verdict. Previous (pre-LLM) work has examined classifying media bias, factuality, or credibility of news sources [1820, 116], while more recent work has focused on providing more in depth assessments of source reliability [99]. To our knowledge, such efforts have yet to be integrated into explainable automated fact-checking tools. These techniques also fail to sufficiently address the complexity and sophistication that fact-checkers are seeking. 5.2.2 Beyond binary verdicts. Although publishing categorical verdicts (e.g., \"true\", \"false\", \"misleading\", \"unverifiable\", etc.) remains convention alongside fact-checking articles, such labels are reductive, particularly given that misleading claims are rarely straightforwardly false. Some fact-checking organisations publish verdicts of one or multiple sentences, which can address nuance in complex claims. While this \"less confrontational\" approach could increase the effectiveness and informativeness of fact-checking,19 it contrasts with the typical approach for automated fact-checking (and ML classification tasks generally) of labelling inputs according to clearly-defined categories. In fact, for the sake of simplicity, data from fact-checking organisations that do not publish categorical verdict labels are often excluded from automated fact-checking datasets [16, 51], and the multi-label categorisation systems used by fact-checking organisations remain challenge for automated fact-checking techniques, often \"solved\" by collapsing multiple labels (e.g., mapping the verdicts \"mixture\", \"unproven\" and \"undetermined\" onto the label \"not enough information\" [51, 53, 68]; see [50] for review). Show the work. These differences in approaches are impor5.2.3 tant because for fact-checkers, communicating the pathway to the verdict is just as important as the verdict itself. Fact-checkers emphasised that documenting the steps and inferences made at each point in the fact-checking process was central to explaining how they reached their final conclusions. In other words, explaining the 5.2.1 The importance of primary sources. The most obvious disconnect between manual and automated fact-checking is in evidence 19see Full Facts (UK-based fact-checking organisation) justification for this approach: https://fullfact.org/about/frequently-asked-questions/#ratings Show Me the Work: Fact-Checkers Requirements for Explainable Automated Fact-Checking CHI 25, April 26-May 1, 2025, Yokohama, Japan pathway to the verdict was seen as paramount for maintaining credibility. Explainable automated fact-checking continues to prioritise the verdict through veracity prediction, focusing on highlighting important tokens [94], or providing rule-based [4] or text-based summaries [12, 68] of how the evidence justifies the verdict. Automated fact-checking systems are not required to be oracles, but should attempt to explain their processes to be verifiable and useful to fact-checkers."
        },
        {
            "title": "5.3 Future directions for human-centred",
            "content": "automated fact-checking The significant disconnects between the current state of automated fact-checking and the practical realities faced by fact-checkers clearly call for more interdisciplinary and critical efforts to reconsider the goals of such research and the broader societal implications of AIs increasing role in fact-checking and journalism. Future work in explainable automated fact-checking. Our find5.3.1 ings suggest several promising avenues for future work in explainable automated fact-checking for each step in the fact-checking pipeline. For time-intensive tasks such as claim detection and evidence retrieval, in which volume and velocity are main concerns for fact-checkers, there are unexplored opportunities for HCI research to examine how explanations for AI systems (e.g., for claim filtering, information extraction, source evaluation) can better support factchecker ability to determine the credibility and reliability of their recommendations and how such explanations could be designed and delivered more effectively. Successful adoption of automated fact-checking systems for veracity prediction hinges on how well they integrate into factchecker work processes. key challenge is that current automated fact-checking systems simply do not follow human logic. While it is to some degree possible to force them to do so, for example using neuro-symbolic approaches, this has only been successful in highly controlled settings with artificially created datasets [9]. Understanding how to bridge machine logic and fact-checker practices requires stronger human-centred approaches, to develop automated fact-checking systems that better align with fact-checker reasoning. Systems must also provide explanations with practical utility for fact-checkers. For example, although people are sensitive to LLMs indicating some degree of uncertainty in their outputs (e.g., \"Im not sure, but...\") [55, 65, 107], generating explanations that address the sources of uncertainty, and how it may be resolved remains unexplored. While technical approaches can compute uncertainty, HCI research is essential to explore how to present it in ways that are useful to range of audiences. HCI research is well-versed in the idea that knowledge is highly situated and context-dependent, especially for fact-checkers and their work [78]. While there is some existing work on designing explainable AI systems sensitive to peoples contexts, background knowledge and expertise (e.g., [42, 64]), this research is at nascent stage and often remains far removed from the realities of fact-checking. 5.3.2 Ethical considerations and broader implications of automated fact-checking. Effective automated fact-checking approaches have the potential to address the accelerating scale and speed of misinformation spread. However, dependence on algorithmic systems by fact-checking organisations and journalists may increase scepticism and erode public trust in media institutions, due to flawed or biased systems or perceptions that these institutions are abdicating their responsibilities. For example, reliance on such models raises the risk of neglecting cultural and regional nuances, given the well-documented Western-centric biases in LLMs [24, 25, 91]. Over-reliance on LLMs for fact-checking may also depreciate the influence and value of journalists work; recent work suggests retrieval-augmented LLMs tend to favour LLM-generated contexts compared to retrieved (likelier human-written) content [109]. Similar to other algorithmic systems deployed in complex sociotechnical contexts, such as filtering and content moderation, these tools continue to represent the threat of bias and reproduction of existing structural inequalities, increasing the potential of silencing minority voices [35, 72]. While technologies may be developed with good intentions, lack of grounding in local, contextually situated practices can have variety of unintended consequences. This is especially true in fact-checking work where claim verification is often not case of binary true or false, but nuanced judgement of partial veracity. While professional fact-checkers worldwide appear to have surprisingly similar approaches and processes for claim verification, local knowledge and communal practices are key to doing so successfully [108]. Yet, current development of automated fact-checking systems continues to rely more on researcher imaginaries of how fact-checking is performed rather than the actual conditions of fact-checker practice. Given the centrality of factchecking as bulwark against disinformation campaigns and the clear need for effective support tools, human-centred approaches to the development of these tools [78, 113] are key to facilitating ongoing efforts towards transparency and explainability."
        },
        {
            "title": "6 Conclusion\nOur work illustrates that there are critical rifts between the ca-\npabilities of current automated fact-checking tools and the goals\nand needs of fact-checkers. By outlining the specific information\nfact-checkers require of these tools at each step of the process, we\ndemonstrate how these tools can be made more useful. Automated\nfact-checking systems hold great promise to assist fact-checkers in\nmanaging the intensifying volume of misinformation proliferated\nin online media. However, without meaningful engagement with\nfact-checker practices and more interdisciplinary research efforts\nthese tools will not fulfil their potential to support and empower\nfact-checkers in their work. For these tools to be truly effective, it\nis essential that the design of these tools is shaped by a thorough\nunderstanding of fact-checker decision processes and explanation\nneeds. In light of how fact-checking as a practice continues to be\ntargeted for ideological and political reasons [26, 63], we also high-\nlight that while fully automated fact-checking may appear as a\ngood solution to misinformation, such systems may be easier to\nsubvert as political winds shift. Rather than seeking to develop\nfully automated fact-checking systems, we call for systems that can\nsupport and empower fact-checkers in their critical role in society.",
            "content": "CHI 25, April 26-May 1, 2025, Yokohama, Japan Warren et al. Acknowledgments The authors wish to thank the fact-checkers and journalists who generously volunteered to participate in interviews and without whom this research would not have been possible. This research was co-funded by the European Union (ERC, ExplainYourself, 101077481), and supported by the Pioneer Centre for AI, DNRF grant number P1. Views and opinions expressed are those of the authors only and do not necessarily reflect those of the European Union or the European Research Council. Neither the European Union nor the granting authority can be held responsible for them."
        },
        {
            "title": "References",
            "content": "[1] Bill Adair and Mark Stencel. 2020. Lesson in automated journalism: Bring back the humans. https://www.niemanlab.org/2020/07/a-lesson-in-automatedjournalism-bring-back-the-humans/ [2] Zoë Adams, Magda Osman, Christos Bechlivanidis, and Björn Meder. 2023. (Why) is misinformation problem? Perspectives on Psychological Science 18, 6 (2023), 14361463. https://doi.org/10.1177/17456916221141344 [3] Julius Adebayo, Justin Gilmer, Michael Muelly, Ian Goodfellow, Moritz Hardt, and Been Kim. 2018. Sanity Checks for Saliency Maps. In Advances in Neural Information Processing Systems, S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett (Eds.), Vol. 31. Curran Associates, Inc. https://proceedings.neurips.cc/paper_files/paper/2018/file/ 294a8ed24b1ad22ec2e7efea049b8737-Paper.pdf [4] Naser Ahmadi, Joohyung Lee, Paolo Papotti, and Mohammed Saeed. 2019. Explainable Fact Checking with Probabilistic Answer Set Programming. In Conference on Truth and Trust Online. https://doi.org/10.48550/arXiv.1906.09198 [5] Sanchit Ahuja, Divyanshu Aggarwal, Varun Gumma, Ishaan Watts, Ashutosh Sathe, Millicent Ochieng, Rishav Hada, Prachi Jain, Mohamed Ahmed, Kalika Bali, and Sunayana Sitaram. 2024. MEGAVERSE: Benchmarking Large Language Models Across Languages, Modalities, Models and Tasks. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), Kevin Duh, Helena Gomez, and Steven Bethard (Eds.). Association for Computational Linguistics, Mexico City, Mexico, 25982637. https://doi.org/10.18653/v1/2024.naacl-long.143 [6] Mubashara Akhtar, Michael Schlichtkrull, Zhijiang Guo, Oana Cocarascu, Elena Simperl, and Andreas Vlachos. 2023. Multimodal Automated Fact-Checking: Survey. In Findings of the Association for Computational Linguistics: EMNLP 2023, Houda Bouamor, Juan Pino, and Kalika Bali (Eds.). Association for Computational Linguistics, Singapore, 54305448. https://doi.org/10.18653/v1/2023. findings-emnlp.361 [7] Tariq Alhindi, Savvas Petridis, and Smaranda Muresan. 2018. Where is Your Evidence: Improving Fact-checking by Justification Modeling. In Proceedings of the First Workshop on Fact Extraction and VERification (FEVER), James Thorne, Andreas Vlachos, Oana Cocarascu, Christos Christodoulopoulos, and Arpit Mittal (Eds.). Association for Computational Linguistics, Brussels, Belgium, 8590. https://doi.org/10.18653/v1/W18- [8] David Alvarez-Melis and Tommi S. Jaakkola. 2018. On the Robustness of arXiv:1806.08049 Interpretability Methods. CoRR abs/1806.08049 (2018). http://arxiv.org/abs/1806.08049 [9] Rami Aly, Marek Strong, and Andreas Vlachos. 2023. QA-NatVer: Question Answering for Natural Logic-based Fact Verification. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, Houda Bouamor, Juan Pino, and Kalika Bali (Eds.). Association for Computational Linguistics, Singapore, 83768391. https://doi.org/10.18653/v1/2023.emnlp-main.521 [10] Michelle Amazeen. 2020. Journalistic interventions: The structural factors affecting the global emergence of fact-checking. Journalism 21, 1 (2020), 95111. https://doi.org/10.1177/1464884917730217 [11] Phoebe Arnold. 2020. The challenges of online fact checking: how technology can (and cant) help. Technical Report. Full Fact. https://fullfact.org/blog/2020/dec/ the-challenges-of-online-fact-checking-how-technology-can-and-cant-help/ [12] Pepa Atanasova, Jakob Grue Simonsen, Christina Lioma, and Isabelle Augenstein. 2020. Generating Fact Checking Explanations. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel Tetreault (Eds.). Association for Computational Linguistics, Online, 73527364. https://doi.org/10.18653/v1/2020.acl-main.656 [13] Pepa Atanasova, Jakob Grue Simonsen, Christina Lioma, and Isabelle Augenstein. 2022. Fact checking with insufficient evidence. Transactions of the Association for Computational Linguistics 10 (2022), 746763. https://doi.org/10.1162/tacl_ a_00486 [14] Pepa Atanasova, Dustin Wright, and Isabelle Augenstein. 2020. Generating Label Cohesive and Well-Formed Adversarial Claims. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu (Eds.). Association for Computational Linguistics, Online, 31683177. https://doi.org/10.18653/v1/ 2020.emnlp-main.256 [15] Isabelle Augenstein, Timothy Baldwin, Meeyoung Cha, Tanmoy Chakraborty, Giovanni Luca Ciampaglia, David Corney, Renee DiResta, Emilio Ferrara, Scott Hale, Alon Halevy, Eduard Hovy, Heng Ji, Filippo Menczer, Ruben Miguez, Preslav Nakov, Dietram Scheufele, Shivam Sharma, and Giovanni Zagni. 2024. Factuality challenges in the era of large language models and opportunities for fact-checking. Nature Machine Intelligence (2024), 112. https://doi.org/10.1038/ s42256-024-00881-z [16] Isabelle Augenstein, Christina Lioma, Dongsheng Wang, Lucas Chaves Lima, Casper Hansen, Christian Hansen, and Jakob Grue Simonsen. 2019. MultiFC: Real-World Multi-Domain Dataset for Evidence-Based Fact Checking of Claims. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan (Eds.). Association for Computational Linguistics, Hong Kong, China, 46854697. https://doi.org/10.18653/v1/D19-1475 [17] Isabelle Augenstein, Tim Rocktäschel, Andreas Vlachos, and Kalina Bontcheva. 2016. Stance Detection with Bidirectional Conditional Encoding. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, Jian Su, Kevin Duh, and Xavier Carreras (Eds.). Association for Computational Linguistics, Austin, Texas, 876885. https://doi.org/10.18653/v1/D16-1084 [18] Ramy Baly, Giovanni Da San Martino, James Glass, and Preslav Nakov. 2020. We Can Detect Your Bias: Predicting the Political Ideology of News Articles. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu (Eds.). Association for Computational Linguistics, Online, 49824991. https: //doi.org/10.18653/v1/2020.emnlp-main.404 [19] Ramy Baly, Georgi Karadzhov, Dimitar Alexandrov, James Glass, and Preslav Nakov. 2018. Predicting Factuality of Reporting and Bias of News Media Sources. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Ellen Riloff, David Chiang, Julia Hockenmaier, and Junichi Tsujii (Eds.). Association for Computational Linguistics, Brussels, Belgium, 35283539. https://doi.org/10.18653/v1/D18-1389 [20] Ramy Baly, Georgi Karadzhov, Jisun An, Haewoon Kwak, Yoan Dinkov, Ahmed Ali, James Glass, and Preslav Nakov. 2020. What Was Written vs. Who Read It: News Media Profiling Using Text Analysis and Social Media Context. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel Tetreault (Eds.). Association for Computational Linguistics, Online, 33643374. https://doi.org/ 10.18653/v1/2020.acl-main.308 [21] Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei Ji, Tiezheng Yu, Willy Chung, Quyet V. Do, Yan Xu, and Pascale Fung. 2023. Multitask, Multilingual, Multimodal Evaluation of ChatGPT on Reasoning, Hallucination, and Interactivity. In Proceedings of the 13th International Joint Conference on Natural Language Processing and the 3rd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics (Volume 1: Long Papers), Jong C. Park, Yuki Arase, Baotian Hu, Wei Lu, Derry Wijaya, Ayu Purwarianti, and Adila Alfa Krisnadhi (Eds.). Association for Computational Linguistics, Nusa Dua, Bali, 675718. https://doi.org/10. 18653/v1/2023.ijcnlp-main.45 [22] Mette Bengtsson and Sabina Schousboe. 2024. And Thats Fact: Rhetorical Perspective on the Role of Fact-Checkers. Journalism Practice (2024), 119. https://doi.org/10.1080/17512786.2024. [23] Adrien Bibal, Rémi Cardon, David Alfter, Rodrigo Wilkens, Xiaoou Wang, Thomas François, and Patrick Watrin. 2022. Is Attention Explanation? An Introduction to the Debate. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Smaranda Muresan, Preslav Nakov, and Aline Villavicencio (Eds.). Association for Computational Linguistics, Dublin, Ireland, 38893900. https://doi.org/10.18653/v1/2022.acllong.269 [24] Yong Cao, Li Zhou, Seolhwa Lee, Laura Cabello, Min Chen, and Daniel Hershcovich. 2023. Assessing Cross-Cultural Alignment between ChatGPT and Human Societies: An Empirical Study. In Proceedings of the First Workshop on Cross-Cultural Considerations in NLP (C3NLP), Sunipa Dev, Vinodkumar Prabhakaran, David Adelani, Dirk Hovy, and Luciana Benotti (Eds.). Association for Computational Linguistics, Dubrovnik, Croatia, 5367. https: //doi.org/10.18653/v1/2023.c3nlp-1.7 [25] Yang Trista Cao, Anna Sotnikova, Hal Daumé III, Rachel Rudinger, and Linda Zou. 2022. Theory-Grounded Measurement of U.S. Social Stereotypes in English Language Models. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Marine Carpuat, Marie-Catherine de Marneffe, and Ivan Vladimir Meza Ruiz (Eds.). Association for Computational Linguistics, Seattle, United Show Me the Work: Fact-Checkers Requirements for Explainable Automated Fact-Checking CHI 25, April 26-May 1, 2025, Yokohama, Japan States, 12761295. https://doi.org/10.18653/v1/2022.naacl-main. [26] Rebecca Catalanello and Katie Sanders. 2025. Meta is ending its thirdparty fact-checking partnership with US partners. Heres how that program works. https://www.poynter.org/fact-checking/2025/meta-ends-fact-checkingcommunity-notes-facebook/ [27] Valerie Chen, Q. Vera Liao, Jennifer Wortman Vaughan, and Gagan Bansal. 2023. Understanding the Role of Human Intuition on Reliance in Human-AI DecisionMaking with Explanations. Proc. ACM Hum.-Comput. Interact. 7, CSCW2, Article 370 (oct 2023), 32 pages. https://doi.org/10.1145/3610219 [28] Kemal Cizmeciler, Erkut Erdem, and Aykut Erdem. 2022. Leveraging semantic saliency maps for query-specific video summarization. Multimedia Tools and Applications 81, 12 (2022), 1745717482. https://doi.org/10.1007/s11042-02212442-w [29] Charles L.A. Clarke, Maria Maistro, and Mark D. Smucker. 2020. Overview of the TREC 2020 Health Misinformation Track.. In Text Retrieval Conference 2020. https://trec.nist.gov/pubs/trec30/papers/Overview-HM.pdf [30] Juliet Corbin and Anselm Strauss. 2015. Basics of Qualitative Research. Vol. 14. sage. [31] Anubrata Das, Chitrank Gupta, Venelin Kovatchev, Matthew Lease, and Junyi Jessy Li. 2022. ProtoTEx: Explaining Model Decisions with Prototype Tensors. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Smaranda Muresan, Preslav Nakov, and Aline Villavicencio (Eds.). Association for Computational Linguistics, Dublin, Ireland, 29862997. https://doi.org/10.18653/v1/2022.acl-long.213 [32] Anubrata Das, Houjiang Liu, Venelin Kovatchev, and Matthew Lease. 2023. The state of human-centered NLP technology for fact-checking. Information Processing & Management 60, 2 (2023), 103219. https://doi.org/10.1016/j.ipm. 2022.103219 [33] Yael de Haan, Eric van den Berg, Nele Goutier, Sanne Kruikemeier, and Sophie Lecheler. 2022. Invisible friend or foe? How journalists use and perceive algorithmic-driven tools in their research process. Digital Journalism 10, 10 (2022), 17751793. https://doi.org/10.1080/21670811.2022.2027798 [34] Nicholas Diakopoulos. 2020. Computational news discovery: Towards design considerations for editorial orientation algorithms in journalism. Digital journalism 8, 7 (2020), 945967. https://doi.org/10.1080/21670811.2020.1736946 [35] Thiago Dias Oliva, Dennys Marcelo Antonialli, and Alessandra Gomes. 2021. Fighting hate speech, silencing drag queens? artificial intelligence in content moderation and risks to LGBTQ voices online. Sexuality & Culture 25 (2021), 700732. https://doi.org/10.1007/s12119-020-09790-w [36] Laurence Dierickx, Carl-Gustav Lindén, and Andreas Lothe Opdahl. 2023. Automated fact-checking to support professional practices: systematic literature review and meta-analysis. International Journal of Communication 17 (2023), 21. https://ijoc.org/index.php/ijoc/article/view/21071/4287 [37] Alphaeus Dmonte, Roland Oruche, Marcos Zampieri, Prasad Calyam, and Isabelle Augenstein. 2024. Claim Verification in the Age of Large Language Models: Survey. https://doi.org/10.48550/arXiv.2408.14317 arXiv:2408.14317 [cs.CL] [38] Jonathan Dodge, Q. Vera Liao, Yunfeng Zhang, Rachel K. E. Bellamy, and Casey Dugan. 2019. Explaining Models: An Empirical Study of How Explanations Impact Fairness Judgment. In Proceedings of the 24th International Conference on Intelligent User Interfaces (Marina del Ray, California) (IUI 19). Association for Computing Machinery, New York, NY, USA, 275285. https://doi.org/10.1145/ 3301275. [39] Stewart Donaldson and Elisa Grant-Vallone. 2002. Understanding self-report bias in organizational behavior research. Journal of business and Psychology 17 (2002), 245260. https://doi.org/10.1023/A:1019637632584 [40] Mengnan Du, Fengxiang He, Na Zou, Dacheng Tao, and Xia Hu. 2023. Shortcut Learning of Large Language Models in Natural Language Understanding. Commun. ACM 67, 1 (dec 2023), 110120. https://doi.org/10.1145/3596490 [41] Nicholas Dufour, Arkanath Pathak, Pouya Samangouei, Nikki Hariri, Shashi Deshetti, Andrew Dudfield, Christopher Guess, Pablo Hernández Escayola, Bobby Tran, Mevan Babakar, and Christoph Bregler. 2024. AMMeBa: Large-Scale Survey and Dataset of Media-Based Misinformation In-The-Wild. arXiv:2405.11697 [cs.CY] https://arxiv.org/abs/2405.11697 [42] Upol Ehsan, Samir Passi, Q. Vera Liao, Larry Chan, I-Hsiang Lee, Michael Muller, and Mark Riedl. 2024. The Who in XAI: How AI Background Shapes Perceptions of AI Explanations. In Proceedings of the CHI Conference on Human Factors in Computing Systems (Honolulu, HI, USA) (CHI 24). Association for Computing Machinery, New York, NY, USA, Article 316, 32 pages. https://doi.org/10.1145/3613904.3642474 [43] Parsa Farinneya, Mohammad Mahdi Abdollah Pour, Sardar Hamidian, and Mona Diab. 2021. Active Learning for Rumor Identification on Social Media. In Findings of the Association for Computational Linguistics: EMNLP 2021, Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih (Eds.). Association for Computational Linguistics, Punta Cana, Dominican Republic, 45564565. https://doi.org/10.18653/v1/2021.findings-emnlp.387 [44] William Ferreira and Andreas Vlachos. 2016. Emergent: novel data-set for stance classification, In The 15th Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. https://eprints.whiterose.ac.uk/97416/ [45] Mohamed H. Gad-Elrab, Daria Stepanova, Jacopo Urbani, and Gerhard Weikum. 2019. ExFaKT: Framework for Explaining Facts over Knowledge Graphs and Text. In Proceedings of the Twelfth ACM International Conference on Web Search and Data Mining (Melbourne VIC, Australia) (WSDM 19). Association for Computing Machinery, New York, NY, USA, 8795. https://doi.org/10.1145/ 3289600.3290996 [46] Timnit Gebru, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan, Hanna Wallach, Hal Daumé Iii, and Kate Crawford. 2021. Datasheets for datasets. Commun. ACM 64, 12 (2021), 8692. https://doi.org/10.1145/ 3458723 [47] Lucas Graves. 2017. Anatomy of fact check: Objective practice and the contested epistemology of fact checking. Communication, culture & critique 10, 3 (2017), 518537. https://doi.org/10.1111/cccr.12163 [48] Lucas Graves. 2018. Understanding the promise and limits of auReuters Institute for the Study of Journalism https://www.digitalnewsreport.org/publications/2018/factsheettomated fact-checking. (2018). understanding-promise-limits-automated-fact-checking/ [49] Lucas Graves and Michelle Amazeen. 2019. Fact-checking as idea and practice in journalism. Oxford Research Encyclopedia of Communication (2019). https: //doi.org/10.1093/acrefore/9780190228613.013.808 [50] Zhijiang Guo, Michael Schlichtkrull, and Andreas Vlachos. 2022. survey on automated fact-checking. Transactions of the Association for Computational Linguistics 10 (2022), 178206. https://doi.org/10.1162/tacl_a_00454 [51] Ashim Gupta and Vivek Srikumar. 2021. X-Fact: New Benchmark Dataset for Multilingual Fact Checking. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers), Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli (Eds.). Association for Computational Linguistics, Online, 675682. https://doi.org/10.18653/v1/2021.acl-short.86 [52] Lovisa Hagström, Sara Vera Marjanović, Haeun Yu, Arnav Arora, Christina Lioma, Maria Maistro, Pepa Atanasova, and Isabelle Augenstein. 2024. Reality Check on Context Utilisation for Retrieval-Augmented Generation. arXiv:2412.17031 [cs.CL] https://arxiv.org/abs/2412.17031 [53] Andreas Hanselowski, Christian Stab, Claudia Schulz, Zile Li, and Iryna Gurevych. 2019. Richly Annotated Corpus for Different Tasks in Automated Fact-Checking. In Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL), Mohit Bansal and Aline Villavicencio (Eds.). Association for Computational Linguistics, Hong Kong, China, 493503. https://doi.org/10.18653/v1/K19-1046 [54] Momchil Hardalov, Arnav Arora, Preslav Nakov, and Isabelle Augenstein. 2022. Survey on Stance Detection for Misand Disinformation Identification. In Findings of the Association for Computational Linguistics: NAACL 2022, Marine Carpuat, Marie-Catherine de Marneffe, and Ivan Vladimir Meza Ruiz (Eds.). Association for Computational Linguistics, Seattle, United States, 12591277. https://doi.org/10.18653/v1/2022.findings-naacl.94 [55] Sree Harsha Tanneru, Chirag Agarwal, and Himabindu Lakkaraju. 2024. Quantifying Uncertainty in Natural Language Explanations of Large Language Models. In Proceedings of The 27th International Conference on Artificial Intelligence and Statistics (Proceedings of Machine Learning Research, Vol. 238), Sanjoy Dasgupta, Stephan Mandt, and Yingzhen Li (Eds.). PMLR, 10721080. https://proceedings.mlr.press/v238/harsha-tanneru24a.html [56] Naeemul Hassan, Fatma Arslan, Chengkai Li, and Mark Tremayne. 2017. Toward Automated Fact-Checking: Detecting Check-worthy Factual Claims by ClaimBuster. In Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (Halifax, NS, Canada) (KDD 17). Association for Computing Machinery, New York, NY, USA, 18031812. https://doi.org/10.1145/3097983.3098131 [57] Naeemul Hassan, Gensheng Zhang, Fatma Arslan, Josue Caraballo, Damian Jimenez, Siddhant Gawsane, Shohedul Hasan, Minumol Joseph, Aaditya Kulkarni, Anil Kumar Nayak, Vikas Sable, Chengkai Li, and Mark Tremayne. 2017. ClaimBuster: the first-ever end-to-end fact-checking system. Proc. VLDB Endow. 10, 12 (aug 2017), 19451948. https://doi.org/10.14778/3137765.3137815 [58] Alon Jacovi and Yoav Goldberg. 2020. Towards Faithfully Interpretable NLP Systems: How Should We Define and Evaluate Faithfulness?. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel Tetreault (Eds.). Association for Computational Linguistics, Online, 41984205. https://doi.org/10.18653/v1/ 2020.acl-main.386 [59] Sarthak Jain and Byron C. Wallace. 2019. Attention is not Explanation. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), Jill Burstein, Christy Doran, and Thamar Solorio (Eds.). Association for Computational Linguistics, Minneapolis, Minnesota, 35433556. https://doi.org/10.18653/v1/N19-1357 CHI 25, April 26-May 1, 2025, Yokohama, Japan Warren et al. [60] Yichen Jiang, Shikha Bordia, Zheng Zhong, Charles Dognin, Maneesh Singh, and Mohit Bansal. 2020. HoVer: Dataset for Many-Hop Fact Extraction And Claim Verification. In Findings of the Association for Computational Linguistics: EMNLP 2020, Trevor Cohn, Yulan He, and Yang Liu (Eds.). Association for Computational Linguistics, Online, 34413460. https://doi.org/10.18653/v1/ 2020.findings-emnlp. [61] Samuel G.B. Johnson, J. J. Valenti, and Frank C. Keil. 2019. Simplicity and complexity preferences in causal explanation: An opponent heuristic account. Cognitive Psychology 113, May (2019), 101222. https://doi.org/10.1016/j.cogpsych. 2019.05.004 [62] Prerna Juneja and Tanushree Mitra. 2022. Human and Technological Infrastructures of Fact-checking. Proc. ACM Hum.-Comput. Interact. 6, CSCW2, Article 418 (nov 2022), 36 pages. https://doi.org/10.1145/3555143 [63] Joel Kaplan. 2025. More speech and fewer mistakes. https://about.fb.com/ news/2025/01/meta-more-speech-fewer-mistakes/ [64] Sangyeon Kim, Insil Huh, Yujin Park, and Sangwon Lee. 2022. Designing pragmatic explanation for the XAI system based on the users context and background knowledge. In Human-centered artificial intelligence. Elsevier, 117 125. https://doi.org/10.1016/B978-0-323-85648-5.00012-8 [65] Sunnie S. Y. Kim, Q. Vera Liao, Mihaela Vorvoreanu, Stephanie Ballard, and Jennifer Wortman Vaughan. 2024. \"Im Not Sure, But...\": Examining the Impact of Large Language Models Uncertainty Expression on User Reliance and Trust. In Proceedings of the 2024 ACM Conference on Fairness, Accountability, and Transparency (Rio de Janeiro, Brazil) (FAccT 24). Association for Computing Machinery, New York, NY, USA, 822835. https://doi.org/10.1145/3630106. [66] Michael Koliska and Jessica Roberts. 2024. Epistemology of Fact Checking: An Examination of Practices and Beliefs of Fact Checkers Around the World. Digital Journalism (2024), 121. https://doi.org/10.1080/21670811.2024.2361264 [67] Neema Kotonya and Francesca Toni. 2020. Explainable Automated FactChecking: Survey. In Proceedings of the 28th International Conference on Computational Linguistics, Donia Scott, Nuria Bel, and Chengqing Zong (Eds.). International Committee on Computational Linguistics, Barcelona, Spain (Online), 54305443. https://doi.org/10.18653/v1/2020.coling-main.474 [68] Neema Kotonya and Francesca Toni. 2020. Explainable Automated FactChecking for Public Health Claims. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu (Eds.). Association for Computational Linguistics, Online, 77407754. https://doi.org/10.18653/v1/2020.emnlp-main.623 [69] Viet Lai, Nghia Ngo, Amir Pouran Ben Veyseh, Hieu Man, Franck Dernoncourt, Trung Bui, and Thien Nguyen. 2023. ChatGPT Beyond English: Towards Comprehensive Evaluation of Large Language Models in Multilingual Learning. In Findings of the Association for Computational Linguistics: EMNLP 2023, Houda Bouamor, Juan Pino, and Kalika Bali (Eds.). Association for Computational Linguistics, Singapore, 1317113189. https://doi.org/10.18653/v1/2023.findingsemnlp.878 [70] Markus Langer, Daniel Oster, Timo Speith, Holger Hermanns, Lena Kastner, Eva Schmidt, Andreas Sesing, and Kevin Baum. 2021. What do we want from Explainable Artificial Intelligence (XAI)?A stakeholder perspective on XAI and conceptual model guiding interdisciplinary XAI research. Artificial Intelligence 296 (2021), 103473. https://doi.org/10.1016/j.artint.2021.103473 [71] Thao Le, Tim Miller, Ronal Singh, and Liz Sonenberg. 2023. Explaining model confidence using counterfactuals. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 37. 1185611864. https://doi.org/10.1609/aaai.v37i10. 26399 [72] Cinoo Lee, Kristina Gligorić, Pratyusha Ria Kalluri, Maggie Harrington, Esin Durmus, Kiara Sanchez, Nay San, Danny Tse, Xuan Zhao, MarYam Hamedani, et al. 2024. People who share encounters with racism are silenced online by humans and machines, but guideline-reframing intervention holds promise. Proceedings of the National Academy of Sciences 121, 38 (2024), e2322764121. https://doi.org/10.1073/pnas.2322764121 [73] Tongliang Li, Lei Fang, Jian-Guang Lou, Zhoujun Li, and Dongmei Zhang. 2021. AnaSearch: Extract, Retrieve and Visualize Structured Results from Unstructured Text for Analytical Queries. In Proceedings of the 14th ACM International Conference on Web Search and Data Mining (Virtual Event, Israel) (WSDM 21). Association for Computing Machinery, New York, NY, USA, 906909. https://doi.org/10.1145/3437963. [74] Q. Vera Liao and Kush R. Varshney. 2022. Human-Centered Explainable AI (XAI): From Algorithms to User Experiences. arXiv:2110.10790 [cs.AI] https: //arxiv.org/abs/2110.10790 [75] Gionnieve Lim and Simon T. Perrault. 2024. XAI in Automated Fact-Checking? The Benefits Are Modest and Theres No One-Explanation-Fits-All. In Proceedings of the 35th Australian Computer-Human Interaction Conference (Wellington, New Zealand) (OzCHI 23). Association for Computing Machinery, New York, NY, USA, 624638. https://doi.org/10.1145/3638380.3638388 [76] Jonathan B. Lim and Daniel M. Oppenheimer. 2020. Explanatory preferences for complexity matching. PLoS ONE 15, 4 (4 2020). https://doi.org/10.1371/journal. pone.0230929 [77] Rhema Linder, Sina Mohseni, Fan Yang, Shiva Pentyala, Eric Ragan, and Xia Ben Hu. 2021. How level of explanation detail affects human performance in interpretable intelligent systems: study on explainable fact checking. Applied AI Letters 2, 4 (2021), e49. https://doi.org/10.1002/ail2.49 [78] Houjiang Liu, Anubrata Das, Alexander Boltz, Didi Zhou, Daisy Pinaroc, Matthew Lease, and Min Kyung Lee. 2023. Human-centered NLP Fact-checking: Co-Designing with Fact-checkers using Matchmaking for AI. https://doi.org/ 10.48550/arXiv.2308.07213 arXiv:2308.07213 [cs.HC] [79] Scott M. Lundberg and Su-In Lee. 2017. Unified Approach to Interpreting Model Predictions. In Advances in Neural Information Processing Systems, I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (Eds.), Vol. 30. Curran Associates, Inc. https://proceedings.neurips. cc/paper_files/paper/2017/file/8a20a8621978632d76c43dfd28b67767-Paper.pdf [80] Sara Vera Marjanovic, Haeun Yu, Pepa Atanasova, Maria Maistro, Christina Lioma, and Isabelle Augenstein. 2024. DYNAMICQA: Tracing Internal Knowledge Conflicts in Language Models. In Findings of the Association for Computational Linguistics: EMNLP 2024, Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen (Eds.). Association for Computational Linguistics, Miami, Florida, USA, 14346 14360. https://doi.org/10.18653/v1/2024.findings-emnlp. [81] Nicholas Micallef, Vivienne Armacost, Nasir Memon, and Sameer Patil. 2022. True or False: Studying the Work Practices of Professional Fact-Checkers. Proc. ACM Hum.-Comput. Interact. 6, CSCW1, Article 127 (apr 2022), 44 pages. https: //doi.org/10.1145/3512974 [82] Sebastião Miranda, David Nogueira, Afonso Mendes, Andreas Vlachos, Andrew Secker, Rebecca Garrett, Jeff Mitchel, and Zita Marinho. 2019. Automated Fact Checking in the News Room. In The World Wide Web Conference (San Francisco, CA, USA) (WWW 19). Association for Computing Machinery, New York, NY, USA, 35793583. https://doi.org/10.1145/3308558.3314135 [83] Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchinson, Elena Spitzer, Inioluwa Deborah Raji, and Timnit Gebru. 2019. Model Cards for Model Reporting. In Proceedings of the Conference on Fairness, Accountability, and Transparency (Atlanta, GA, USA) (FAT* 19). Association for Computing Machinery, New York, NY, USA, 220229. https://doi.org/10.1145/3287560.3287596 [84] Preslav Nakov, Alberto Barrón-Cedeño, Giovanni Da San Martino, Firoj Alam, Julia Maria Struß, Thomas Mandl, Rubén Míguez, Tommaso Caselli, Mucahid Kutlu, Wajdi Zaghouani, et al. 2022. The clef-2022 checkthat! lab on fighting the covid-19 infodemic and fake news detection. In European Conference on Information Retrieval. Springer, 416428. https://doi.org/10.1007/978-3-03099739-7_52 [85] Preslav Nakov, David Corney, Maram Hasanain, Firoj Alam, Tamer Elsayed, Alberto Barrón-Cedeño, Paolo Papotti, Shaden Shaar, and Giovanni Da San Martino. 2021. Automated Fact-Checking for Assisting Human Fact-Checkers. arXiv:2103.07769 [cs.AI] https://arxiv.org/abs/2103.07769 [86] Meike Nauta, Jan Trienes, Shreyasi Pathak, Elisa Nguyen, Michelle Peters, Yasmin Schmitt, Jörg Schlötterer, Maurice van Keulen, and Christin Seifert. 2023. From Anecdotal Evidence to Quantitative Evaluation Methods: Systematic Review on Evaluating Explainable AI. ACM Comput. Surv. 55, 13s, Article 295 (jul 2023), 42 pages. https://doi.org/10.1145/ [87] International Fact-Checking Network. 2024. State of the Fact-Checkers Report. Technical Report. International Fact-Checking Network. https://www.poynter. org/wp-content/uploads/2024/04/State-of-Fact-Checkers-2023.pdf [88] Wojciech Ostrowski, Arnav Arora, Pepa Atanasova, and Isabelle Augenstein. 2021. Multi-Hop Fact Checking of Political Claims. In Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI-21, Zhi-Hua Zhou (Ed.). International Joint Conferences on Artificial Intelligence Organization, 38923898. https://doi.org/10.24963/ijcai.2021/536 Main Track. [89] Liangming Pan, Xiaobao Wu, Xinyuan Lu, Anh Tuan Luu, William Yang Wang, Min-Yen Kan, and Preslav Nakov. 2023. Fact-Checking Complex Claims with Program-Guided Reasoning. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (Eds.). Association for Computational Linguistics, Toronto, Canada, 69817004. https://doi.org/10.18653/v1/2023.acllong.386 [90] John V. Pavlik. 2023. Collaborating with ChatGPT: Considering the implications of generative artificial intelligence for journalism and media education. Journalism & mass communication educator 78, 1 (2023), 8493. https: //doi.org/10.1177/10776958221149577 [91] Siddhesh Pawar, Junyeong Park, Jiho Jin, Arnav Arora, Junho Myung, Srishti Yadav, Faiz Ghifari Haznitrama, Inhwa Song, Alice Oh, and Isabelle Augenstein. 2024. Survey of Cultural Awareness in Language Models: Text and Beyond. arXiv:2411.00860 [cs.CL] https://arxiv.org/abs/2411.00860 [92] Vitali Petsiuk, Rajiv Jain, Varun Manjunatha, Vlad I. Morariu, Ashutosh Mehra, Vicente Ordonez, and Kate Saenko. 2021. Black-Box Explanation of Object Detectors via Saliency Maps. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 1144311452. https://doi.org/ 10.1109/CVPR46437.2021. Show Me the Work: Fact-Checkers Requirements for Explainable Automated Fact-Checking CHI 25, April 26-May 1, 2025, Yokohama, Japan [93] Dean Pomerleau and Delip Rao. 2017. The fake news challenge: Exploring how artificial intelligence technologies could be leveraged to combat fake news. Fake news challenge (2017). http://www.fakenewschallenge.org/# [94] Kashyap Popat, Subhabrata Mukherjee, Andrew Yates, and Gerhard Weikum. 2018. DeClarE: Debunking Fake News and False Claims using Evidence-Aware Deep Learning. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Ellen Riloff, David Chiang, Julia Hockenmaier, and Junichi Tsujii (Eds.). Association for Computational Linguistics, Brussels, Belgium, 2232. https://doi.org/10.18653/v1/D18-1003 [95] Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. 2016. \"Why Should Trust You?\": Explaining the Predictions of Any Classifier. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (San Francisco, California, USA) (KDD 16). Association for Computing Machinery, New York, NY, USA, 11351144. https://doi.org/10.1145/2939672. 2939778 [96] Eugene Sadler-Smith and Erella Shefy. 2004. The intuitive executive: Understanding and applying gut feelin decision-making. Academy of Management Perspectives 18, 4 (2004), 7691. https://doi.org/10.5465/ame.2004.15268692 [97] Michael Schlichtkrull, Zhijiang Guo, and Andreas Vlachos. 2024. AVERITEC: dataset for real-world claim verification with evidence from the web. In Proceedings of the 37th International Conference on Neural Information Processing Systems (New Orleans, LA, USA) (NIPS 23). Curran Associates Inc., Red Hook, NY, USA, Article 2842, 40 pages. https://doi.org/10.5555/3666122.3668964 [98] Michael Schlichtkrull, Nedjma Ousidhoum, and Andreas Vlachos. 2023. The Intended Uses of Automated Fact-Checking Artefacts: Why, How and Who. In Findings of the Association for Computational Linguistics: EMNLP 2023, Houda Bouamor, Juan Pino, and Kalika Bali (Eds.). Association for Computational Linguistics, Singapore, 86188642. https://doi.org/10.18653/v1/2023.findingsemnlp. [99] Michael Sejr Schlichtkrull. 2024. Generating Media Background Checks for Automated Source Critical Reasoning. In Findings of the Association for Computational Linguistics: EMNLP 2024, Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen (Eds.). Association for Computational Linguistics, Miami, Florida, USA, 49274947. https://doi.org/10.18653/v1/2024.findings-emnlp.283 [100] Vera Schmitt, Luis-Felipe Villa-Arenas, Nils Feldhus, Joachim Meyer, Robert P. Spang, and Sebastian Möller. 2024. The Role of Explainability in Collaborative Human-AI Disinformation Detection. In Proceedings of the 2024 ACM Conference on Fairness, Accountability, and Transparency (Rio de Janeiro, Brazil) (FAccT 24). Association for Computing Machinery, New York, NY, USA, 21572174. https://doi.org/10.1145/3630106.3659031 [101] Tal Schuster, Adam Fisch, and Regina Barzilay. 2021. Get Your Vitamin C! Robust Fact Verification with Contrastive Evidence. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Kristina Toutanova, Anna Rumshisky, Luke Zettlemoyer, Dilek Hakkani-Tur, Iz Beltagy, Steven Bethard, Ryan Cotterell, Tanmoy Chakraborty, and Yichao Zhou (Eds.). Association for Computational Linguistics, Online, 624643. https://doi.org/10.18653/v1/2021.naacl-main.52 [102] Sahajpreet Singh, Sarah Masud, and Tanmoy Chakraborty. 2024. Independent fact-checking organizations exhibit departure from political neutrality. arXiv:2407.19498 [cs.SI] https://arxiv.org/abs/2407.19498 [103] Dylan Slack, Sophie Hilgard, Emily Jia, Sameer Singh, and Himabindu Lakkaraju. 2020. Fooling LIME and SHAP: Adversarial Attacks on Post hoc Explanation Methods. In Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society (New York, NY, USA) (AIES 20). Association for Computing Machinery, New York, NY, USA, 180186. https://doi.org/10.1145/3375627. [104] Sarah Harrison Smith. 2004. The Fact Checkers Bible: Guide to Getting It Right. Anchor. [105] Kacper Sokol and Peter Flach. 2020. Explainability fact sheets: framework for systematic assessment of explainable approaches. In Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency (Barcelona, Spain) (FAT* 20). Association for Computing Machinery, New York, NY, USA, 5667. https://doi.org/10.1145/3351095.3372870 [106] Mark Stencel, Erica Ryan, and Joel Luther. 2024. With half the planet going to the polls in 2024, fact-checking sputters. Technical Report. Duke Reporters Lab. https://reporterslab.org/with-half-the-planet-going-to-the-polls-in-2024fact-checking-sputters/ [107] Mark Steyvers, Heliodoro Tejeda, Aakriti Kumar, Catarina Belem, Sheer Karny, Xinyue Hu, Lukas Mayer, and Padhraic Smyth. 2025. What large language models know and what people think they know. Nature Machine Intelligence (2025), 111. https://doi.org/10.1038/s42256-024-00976-7 [108] Sharifa Sultana and Susan R. Fussell. 2021. Dissemination, Situated Factchecking, and Social Effects of Misinformation among Rural Bangladeshi Villagers During the COVID-19 Pandemic. Proc. ACM Hum.-Comput. Interact. 5, CSCW2, Article 436 (Oct. 2021), 34 pages. https://doi.org/10.1145/ [109] Hexiang Tan, Fei Sun, Wanli Yang, Yuanzhuo Wang, Qi Cao, and Xueqi Cheng. 2024. Blinded by Generated Contexts: How Language Models Merge Generated and Retrieved Contexts When Knowledge Conflicts?. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Lun-Wei Ku, Andre Martins, and Vivek Srikumar (Eds.). Association for Computational Linguistics, Bangkok, Thailand, 62076227. https://doi.org/ 10.18653/v1/2024.acl-long.337 [110] James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. 2018. FEVER: Large-scale Dataset for Fact Extraction and VERification. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), Marilyn Walker, Heng Ji, and Amanda Stent (Eds.). Association for Computational Linguistics, New Orleans, Louisiana, 809819. https://doi.org/10.18653/v1/N18-1074 [111] Sebastian Tschiatschek, Adish Singla, Manuel Gomez Rodriguez, Arpit Merchant, and Andreas Krause. 2018. Fake News Detection in Social Networks via Crowd Signals. In Companion Proceedings of the The Web Conference 2018 (Lyon, France) (WWW 18). International World Wide Web Conferences Steering Committee, Republic and Canton of Geneva, CHE, 517524. https://doi.org/10.1145/3184558. 3188722 [112] Jasper van der Waa, Tjeerd Schoonderwoerd, Jurriaan van Diggelen, and Mark Neerincx. 2020. Interpretable confidence measures for decision support systems. International Journal of Human-Computer Studies 144 (2020), 102493. https: //doi.org/10.1016/j.ijhcs.2020.102493 [113] Robert Wolfe and Tanushree Mitra. 2024. The Impact and Opportunities of Generative AI in Fact-Checking. In Proceedings of the 2024 ACM Conference on Fairness, Accountability, and Transparency (Rio de Janeiro, Brazil) (FAccT 24). Association for Computing Machinery, New York, NY, USA, 15311543. https://doi.org/10.1145/3630106. [114] Dustin Wright and Isabelle Augenstein. 2020. Claim Check-Worthiness Detection as Positive Unlabelled Learning. In Findings of the Association for Computational Linguistics: EMNLP 2020, Trevor Cohn, Yulan He, and Yang Liu (Eds.). Association for Computational Linguistics, Online, 476488. https: //doi.org/10.18653/v1/2020.findings-emnlp.43 [115] Danni Xu, Shaojing Fan, and Mohan Kankanhalli. 2023. Combating Misinformation in the Era of Generative AI Models. In Proceedings of the 31st ACM International Conference on Multimedia (Ottawa ON, Canada) (MM 23). Association for Computing Machinery, New York, NY, USA, 92919298. https: //doi.org/10.1145/3581783.3612704 [116] Chunyuan Yuan, Qianwen Ma, Wei Zhou, Jizhong Han, and Songlin Hu. 2020. Early Detection of Fake News by Utilizing the Credibility of News, Publishers, and Users based on Weakly Supervised Learning. In Proceedings of the 28th International Conference on Computational Linguistics, Donia Scott, Nuria Bel, and Chengqing Zong (Eds.). International Committee on Computational Linguistics, Barcelona, Spain (Online), 54445454. https://doi.org/10.18653/v1/2020.colingmain.475 [117] Caiqi Zhang, Zhijiang Guo, and Andreas Vlachos. 2024. Do We Need LanguageSpecific Fact-Checking Models? The Case of Chinese. arXiv:2401.15498 [cs.CL] https://arxiv.org/abs/2401.15498 Pre-Interview Questionnaire (1) What organisation (if any) do you work for? (2) What is your specific role (if applicable)? (3) How long have you worked as fact-checker? (years/months) (4) How many years have you worked for your current organisation (if applicable)? (years/months) (5) How many years have you worked in your current role? (6) Do you (or have you ever) used artificial intelligence (AI) tools as part of your fact-checking work? What tools have you used? (7) What country do you currently work in? (8) What is your gender? Interview questions 1. Introductory questions (1) Could you describe your job and the work that you do? (2) Can you think of an example of claim that you checked recently? Can you describe the process and steps involved? Backup prompts: What specific goals do you have at each step of the process? CHI 25, April 26-May 1, 2025, Yokohama, Japan Warren et al."
        },
        {
            "title": "C Example themes and codes",
            "content": "(3) How challenging is it to fact-check claims? What is involved? Backup prompt: are there any factors that make some claims particularly challenging to fact-check? 2. Evidence & verification (4) Using the same example (or different one), can you describe how you went about searching for and selecting evidence to fact-check that claim? (5) Now, using the same (or different) example, can you talk about how you decided which veracity label to assign the claim? (6) What (if any) technological tools do you use to help you with verifying claim? What are their advantages and disadvantages? 3. Explanations & transparency (7) Considering the claim you described (or another example), how would you explain how it was given that label? (8) Were there any challenging aspects involved in explaining the decision? (9) Are there any claim-specific factors that might influence the information you present to people? (10) Does the audience influence the kind of information you present to people? For example, if the target audience is knowledgeable, or knows very little about the topic. Are there differences in how you would explain fact-checking decision to an editor compared to reader? (11) What does transparency mean in terms of fact-checking? How do you (or fact-checkers in your organisation in general) achieve transparency? (12) What (if any) technological tools do you use to help you with transparency? Are there any tools that you use to produce explanations of the decisions? What are their advantages and disadvantages? 4. Explanations & automated fact-checking (13) Have you ever heard of any AI-support tools that automate parts of the fact-checking process? Have you ever used any? What do they do, how are they used? What are their advantages and disadvantages? (14) Have you thought about how AI-support tools could be useful to fact-checkers? What steps in the fact-checking process do you think could be supported? How do you think they could be supported? (15) Follow-up question for each step of the process they identify: Imagine you were using an AI assistant or support tool to assist you with [fact-checking step]. What might it do? What sort of information would you want or expect it to provide? What sort of information would help you to ensure that it is working correctly? (16) How useful would it be for an explanation of an AI system to describe how it works in general? (17) What do you think the criteria for good or useful explainable fact-checking system should be? 5. Closing questions (18) Is there anything that we havent discussed that you think is important or relevant? (19) Is there anything else youd like to share? Show Me the Work: Fact-Checkers Requirements for Explainable Automated Fact-Checking CHI 25, April 26-May 1, 2025, Yokohama, Japan Themes RQs Evidence quality (10) RQ1: Decisions Deciding verdicts (10) & processes Verdict-dependent (6) Communicating complexity (9) Sub-themes Searching for evidence, Contextual clues Intuition, Collaboration Subjectivity, Difficult claims Verdict labels, Adding nuance Example codes primary sources, cross-checking past experience, multiple perspectives conflicting evidence, explain falsity binary verdicts, giving more context RQ2: Using tools Claim matching, Efficiency Collecting evidence, Extracting information Claim detection (6) Evidence retrieval (8) Veracity prediction (8) Detect AI-generated content, Multi-tool usage Communicating fact-checks (4) Editing, Background context social media, live debate monitoring summarising documents, data cleaning detecting LLM text, triangulate verdict LLMs for writing, contextualising story Explain processes (9) RQ3: Explanatory Explain uncertainty (8) Noting uncertainty, Unexplained uncertainty needs Human-in-the-loop, Technical knowledge Replicability (9) Verifiability (9) Faithfulness (3) Table 3: Example themes, subthemes and codes developed from analysis of interview transcripts Leave it up to reader, Explaining processes Checking reliability, Signposting evidence AI hallucination, Human-like explanation verify model, understanding algorithm distrust uncertainty, uncertainty source readers check themselves, using links point to where to look, citing sources mirror human processes, inaccurate output"
        }
    ],
    "affiliations": [
        "Linköping University, Linköping, Sweden",
        "University of Copenhagen, Copenhagen, Denmark"
    ]
}