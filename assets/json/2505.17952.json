{
    "paper_title": "Beyond Distillation: Pushing the Limits of Medical LLM Reasoning with Minimalist Rule-Based RL",
    "authors": [
        "Che Liu",
        "Haozhe Wang",
        "Jiazhen Pan",
        "Zhongwei Wan",
        "Yong Dai",
        "Fangzhen Lin",
        "Wenjia Bai",
        "Daniel Rueckert",
        "Rossella Arcucci"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Improving performance on complex tasks and enabling interpretable decision making in large language models (LLMs), especially for clinical applications, requires effective reasoning. Yet this remains challenging without supervised fine-tuning (SFT) on costly chain-of-thought (CoT) data distilled from closed-source models (e.g., GPT-4o). In this work, we present AlphaMed, the first medical LLM to show that reasoning capability can emerge purely through reinforcement learning (RL), using minimalist rule-based rewards on public multiple-choice QA datasets, without relying on SFT or distilled CoT data. AlphaMed achieves state-of-the-art results on six medical QA benchmarks, outperforming models trained with conventional SFT+RL pipelines. On challenging benchmarks (e.g., MedXpert), AlphaMed even surpasses larger or closed-source models such as DeepSeek-V3-671B and Claude-3.5-Sonnet. To understand the factors behind this success, we conduct a comprehensive data-centric analysis guided by three questions: (i) Can minimalist rule-based RL incentivize reasoning without distilled CoT supervision? (ii) How do dataset quantity and diversity impact reasoning? (iii) How does question difficulty shape the emergence and generalization of reasoning? Our findings show that dataset informativeness is a key driver of reasoning performance, and that minimalist RL on informative, multiple-choice QA data is effective at inducing reasoning without CoT supervision. We also observe divergent trends across benchmarks, underscoring limitations in current evaluation and the need for more challenging, reasoning-oriented medical QA benchmarks."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 2 ] . [ 1 2 5 9 7 1 . 5 0 5 2 : r Beyond Distillation: Pushing the Limits of Medical LLM Reasoning with Minimalist Rule-Based RL Che Liu1, Haozhe Wang2, Jiazhen Pan3, Zhongwei Wan4 , Yong Dai5 , Fangzhen Lin2 , Wenjia Bai1 , Daniel Rueckert1,3 , Rossella Arcucci1 1Imperial College London, 2HKUST, 3Technical University of Munich , 4Ohio State University, 5Fudan University (cid:66) che.liu21@imperial.ac.uk Project page: https://cheliu-computation.github.io/AlphaMed/"
        },
        {
            "title": "Abstract",
            "content": "Improving performance on complex tasks and enabling interpretable decision making in large language models (LLMs), especially for clinical applications, requires effective reasoning. Yet this remains challenging without supervised finetuning (SFT) on costly chain-of-thought (CoT) data distilled from closed-source In this work, we present AlphaMed, the first medical models (e.g., GPT-4o). LLM to show that reasoning capability can emerge purely through reinforcement learning (RL), using minimalist rule-based rewards on public multiple-choice QA datasets, without relying on SFT or distilled CoT data. AlphaMed achieves stateof-the-art results on six medical QA benchmarks, outperforming models trained with conventional SFT+RL pipelines. On challenging benchmarks (e.g., MedXpert), AlphaMed even surpasses larger or closed-source models such as DeepSeekV3-671B and Claude-3.5-Sonnet. To understand the factors behind this success, we conduct comprehensive data-centric analysis guided by three questions: (i) Can minimalist rule-based RL incentivize reasoning without distilled CoT supervision? (ii) How do dataset quantity and diversity impact reasoning? (iii) How does question difficulty shape the emergence and generalization of reasoning? Our findings show that dataset informativeness is key driver of reasoning performance, and that minimalist RL on informative, multiple-choice QA data is effective at inducing reasoning without CoT supervision. We also observe divergent trends across benchmarks, underscoring limitations in current evaluation and the need for more challenging, reasoning-oriented medical QA benchmarks. The code and pretrained model weights will be publicly released upon acceptance."
        },
        {
            "title": "Introduction",
            "content": "Recently, the reasoning capabilities of large language models (LLMs) have advanced significantly, achieving impressive results in tasks requiring complex reasoning, such as mathematical problem solving, code generation, and general-purpose benchmarks [14]. These developments highlight the potential of LLMs to generalize and perform multi-step reasoning across domains. In the medical domain, reasoning is particularly crucial. Clinical natural language processing (NLP) tasks often require interpreting nuanced patient information, integrating knowledge from diverse sources, and making informed decisions [57]. More importantly, reasoning provides valuable lens into the models decision-making process, allowing researchers and clinicians to examine how conclusions Equal Contribution Preprint. Under review. are derived. This improves the interpretability and transparency of AI outputs, which are essential for clinical trust [8, 9]. Currently, most medical LLMs acquire reasoning capabilities through supervised fine-tuning (SFT) on chain-of-thought (CoT) datasets, often followed by reinforcement learning (RL) for further refinement. However, this pipeline heavily relies on an initial SFT stage using costly CoT data, which are either manually crafted or distilled from closed-source commercial models such as GPT4o [10, 11]. This dependence not only incurs substantial annotation and distillation costs but also introduces scalability and accessibility challenges, as it ties model development to expensive and external resources. These limitations motivate critical question: Can we achieve medical reasoning through minimalist rule-based RL without relying on distilled CoT data? To address this question, we propose AlphaMed, the first work designed to incentivize reasoning capability solely through minimalist rule-based RL, going beyond conventional approaches that rely on SFT with CoT data. Instead of depending on distilled CoT data supervision, AlphaMed is trained directly via simple rule-based rewards derived from multiple-choice QA datasets. Our key contributions are as follows: We show that minimalist rule-based RL can incentivize reasoning ability in medical LLMs without relying on distilled CoT data, achieving superior performance. We further analyze how dataset quantity, diversity, and especially informativeness impact reasoning performance. We empirically find that higher informativeness enhances reasoning performance, while less-informative data limits gains. We show that reasoning can be incentivized even with lower-difficulty data and further enhanced by harder examples. While high-difficulty samples benefit challenging benchmarks like MedXpert, mix of difficulty levels is essential for robust generalization. Nonmonotonic trends across benchmarks suggest that current evaluations may be insufficient to assess medical LLM reasoning. Building on these insights, we introduce AlphaMed, medical LLM trained solely via minimalist rule-based RL without any SFT on distilled CoT data, and demonstrate that it achieves state-of-the-art performance across six mainstream medical QA benchmarks, outperforming models that use complex training strategies with CoT data and even surpassing larger or closed-source models such as DeepSeek-V3-671B and GPT-4o."
        },
        {
            "title": "2 Related Work",
            "content": "Supervised Fine-Tuning for Reasoning in LLMs. Large language models can acquire complex reasoning skills through SFT on CoT data. For example, [12] showed that training models to generate step-by-step reasoning paths significantly improves performance on math and logic problems. [13] scaled this approach by incorporating broad range of CoT examples into instruction tuning across diverse tasks. [14] proposed STaR, where model bootstraps its own reasoning traces to reduce reliance on human-annotated CoT. However, recent work [15] suggests that SFT often encourages memorization of training rationales rather than true reasoning generalization, limiting robustness in out-of-distribution or unfamiliar tasks. Moreover, obtaining high-quality CoT data is costly, requiring either expert annotations or distillation from proprietary models, posing significant challenges to scalability and adaptability [16]. Reinforcement Learning with Preference Data after SFT. InstructGPT [17] introduced reinforcement learning with human preferences (RLHF) to align model behavior with user intent. Subsequent research has shown that RL can enhance generalization [16, 18] and better capture nuanced human preferences beyond rote memorization [15]. Among RL algorithms, Proximal Policy Optimization (PPO) is widely used, but it is highly resource-intensiverequiring learned reward models that are often sensitive to noise, difficult to interpret, and occasionally misaligned with intended objectives [19]. To address these limitations, Direct Preference Optimization (DPO) [19] eliminates the need for an explicit reward model by directly optimizing over preference pairs. However, DPO still relies on high-quality preference annotations, which are particularly challenging to construct in 2 the medical domain due to clinical ambiguity and lack of universal agreement on what constitutes better response [20]. Recently, DeepSeek-R1-Zero [21] demonstrated that reasoning behavior can be effectively elicited without CoT supervision or preference annotations, instead by leveraging final answers (e.g., multiple-choice accuracy) as rule-based supervision signals [16, 18, 22, 23]. Open-Source Medical LLMs. Open-source medical LLMs have emerged as promising tools for domain-specific clinical reasoning, yet most remain heavily dependent on supervised data or handcrafted feedback. HuatuoGPT [24] was instruction-tuned on ChatGPT-distilled medical dialogues. BioMistral [25] adapted the Mistral architecture to biomedical question answering through continued pretraining [26] and domain-specific instruction tuning. OpenBioLLM [20] and UltraMedical [27] utilized DPO-based preference optimization, but their preference pairs were directly distilled from closed-source models, making supervision ambiguous and potentially inconsistent with expert clinical reasoning. Since human verification of each distilled example is prohibitively costly and impractical, there is no guarantee that the reasoning process reflected in the supervision is valid. HuatuoGPT-o1 [28] further incorporated PPO using self-trained 3B reward model and relied on CoT data distilled from OpenAI o1. However, this approach is resource-intensive and tightly coupled to the quality and coverage of proprietary data, limiting its scalability and generalizability. m1 [29] also adopts SFT on distilled chain-of-thought data, where step-by-step reasoning traces are generated by external large reasoning model, thus still relying on distilled CoT data."
        },
        {
            "title": "3 Preliminaries",
            "content": "Group Relative Policy Optimization (GRPO) Given question-answer pair (q, a), the behaviour policy πold generates set of candidate completions {oi}G i=1 for each question q. Each response receives scalar reward ri, which may be derived from human preference comparisons or automated scoring heuristics; in this work, we use rule-based reward. The relative quality of each response is assessed within the group through normalization. The training objective is: JGRPO(θ) = (q,a)D,{oi}G i=1πold(q) (cid:34) 1 G (cid:88) i=1 1 oi oi (cid:88) (cid:16) t= (cid:16) min ri,t(θ) ˆAi,t, clip(ri,t(θ), 1 ϵ, 1 + ϵ) ˆAi,t (cid:35) (cid:17) (cid:17) (1) where the group-normalized advantage ˆAi,t and the token-level importance weight ri,t(θ) are defined as: ˆAi,t = ri mean({rj}G std({rj}G j=1) j=1) , ri,t(θ) = πθ(oi,t q, oi, < t) πold(oi,t q, oi, < t) . Here, ϵ is hyperparameter controlling the tolerance for policy deviation. The clip function prevents large updates by ensuring that the ratio between the current and reference policy stays within predefined range. Specifically, it clips the importance weight ri,t(θ) to the interval [1 ϵ, 1 + ϵ], thereby stabilizing training and mitigating the risk of policy collapse. This objective encourages the model to improve token probabilities for completions with above-average rewards, while stabilizing updates via clipped importance weight similar to PPO [30]. Rule-based Reward Modelling To enable minimalist RL without relying on external verifiers or human-provided rewards, we adopt simple rule-based approach consistent with [21]. This method directly evaluates the correctness of the models output using binary feedback, eliminating the need for separate reward model: ri = (cid:26)1, if is_answer_correct(ˆyi, y) 0, otherwise (2) Here, is the ground-truth answer and ˆyi denotes the model-generated prediction from the i-th output oi. This straightforward reward mechanism provides clear supervision signal grounded in task accuracy. By leveraging structured outputs (e.g., multiple-choice answers), we enable effective RL without manually written rationales or preference annotations."
        },
        {
            "title": "4 AlphaMed",
            "content": "4.1 Training Configuration We aim to elicit medical reasoning behavior purely through rule-based RL, without relying on SFT with CoT data or RL with rewards from external verifiers. To ensure fair comparison with HuatuoGPT-o1 [28], we adopt Llama3.1-8B-Instruct and Llama3.1-70B-Instruct as backbone models. All experiments are conducted under full parameter tuning with batch size of 512, meaning each batch contains 64 QA pairs and each question generates 8 candidate answers, trained for 300 steps. We use verl2 [31], framework designed for rule-based RL. simple binary reward function, defined in Eq. 2, assigns 1 if the models response ends with correctly formatted boxed answer matching the ground truth (e.g., boxed{C}), and 0 otherwise. The model is optimized using the GRPO objective described in Eq. 1. We train the 8B model on 8 Nvidia A800-80G GPUs and the 70B model on 64 A800-80G GPUs. 4.2 Evaluation Configuration Datasets. We evaluate our models on six medical QA benchmarks, using accuracy as the evaluation metric across all datasets. These include MedQA-USMLE [32] (MedQA), MedMCQA [33] (MedMCQA), PubMedQA [34] (PubMedQA), MMLU-Pro medical subsets [35] (MMLU-ProM), GPQA medical subsets [36] (GPQA-M), and the most recent and challenging large-scale dataset, MedXpertQA [37] (MedXpert). Details are provided in Sec. A.2. Based on their levels of challenge [38], we categorize MedQA, MedMCQA, and PubMedQA [32 34] as normal, while MMLU-ProM and GPQA-M [39, 36] are classified as hard, as they primarily target advanced expert-level knowledge. Finally, MedXpert [37] is designated as hard+, as the original work explicitly highlights its focus on complex clinical reasoning and expert-level decision making, positioning it as one of the most challenging benchmarks to date. Baseline Methods. We compare against broad range of general and medical-specific LLM baselines. General-purpose base instruct models include Qwen2.5-7B/32B/72B and Llama3.1-8B/70B. Medical-specific models cover MedLlama3, OpenBioLLM [40], MMed and MMed-S [41], Med42 [42], and UltraMedical [27], which leverage distilled preference data and RL following SFT. HuatuoGPT-o1 [28] is trained on CoT data distilled from GPT-4o using modelbased RL with large (3B) reward model. m1 [29] is similarly trained with extensive CoT distilled from DeepSeekR1 [21] via SFT."
        },
        {
            "title": "5 Experiments",
            "content": "5.1 Data Curation Initial Data Collection. Following [29], we collect the training splits of three large-scale public multiple-choice medical QA datasets: MedQA [43], MedMCQA [44], and PubMedQA [34]34. MedQA [43] contains expert-level clinical questions from the USMLE. MedMCQA [44] includes factoid and reasoning questions from Indian medical entrance exams (AIIMS, NEET). PubMedQA [34] focuses on biomedical research question answering. Notably, its training split is automatically generated by machine learning model that heuristically converts biomedical research article abstract into yes/no questions and assigns answers based on negation cues. The dataset statistics are summarized in Sec. A.1. Quantifying Data Difficulty. To quantify question difficulty, we perform inference using For each question, we generate five reasoning completions Llama3.1-8B-Instruct [45]. with the following prompt: Please reason step by step, and put the final answer in boxed{}\". We then calculate the proportion of correct predictions among the five outputs, which serves as proxy for the questions difficulty. Based on this proportion, we categorize questions into six difficulty levels (L1L6). Specifically, L1 includes questions where all five comple2https://github.com/volcengine/verl 3We use the official training splits of all three datasets. 4For PubMedQA [34], only questions with definitive answer labels (i.e., A/B/C) are retained. 4 Figure 1: Performance comparison on six medical QA benchmarks. Our models are initialized with Llama3.1-8B-Instruct [45] and trained using minimalist rule-based RL on one of three balanced subsets: MedQA-Sub, MedMCQA-Sub, or PubMedQA-Sub (shown as blue, green, and orange bars, respectively). Despite using only 1,200 examples per subset, all variants of our model achieve substantial improvements over the base Llama3.1-8B-Instruct and match or surpass the strong baseline HuatuoGPT-o1-8B across all benchmarks. tions are correct, L2 where four are correct, and so on, with L6 representing questions where all five completions are incorrect. The difficulty level distribution of each train set as shown in Tab. Figure 2: Dataset analysis and training dynamics. Left: Ratio of effective queries over training steps; each curve corresponds to models trained on specific subset. Middle: Training reward per step for models trained on each subset. Right: Distribution of question lengths (number of tokens) in MedQA, MedMCQA, and PubMedQA [43, 44, 34]. 5.2 RQ1: Can Minimalist RL Incentivize Medical Reasoning Without Distilled-CoT SFT? To investigate whether minimalist rule-based RL can incentivize medical reasoning in LLMs without relying on SFT with distilled CoT data, we conduct pilot study by sampling 200 examples from each difficulty level to construct three balanced subsets (1,200 samples each) from three public medical QA datasets: MedQA-Sub, MedMCQA-Sub, and PubMedQA-Sub. We use Llama3.1-8B-Instruct as the backbone model and train it separately on each subset using minimalist RL. As shown in Fig. 1, all models trained on these subsets achieve substantial gains over the original backbone across all six benchmarks (e.g., +15.5% on MedQA, +8.8% on MedXpert). Remarkably, all variants trained on different subsets perform comparably to or even surpass HuatuoGPT-o1-8B [46], strong baseline trained via SFT on CoT data distilled from GPT4o [47] and further fine-tuned with RL using 3B reward model. Notably, on MedXpert [37], the most challenging benchmark, all three variants outperform HuatuoGPT-o1-8B [46]. These results demonstrate that reasoning capability can be effectively incentivized through minimalist RL on small-scale, low-cost multiple-choice QA data, without relying on SFT with distilled CoT data, and can even outperform models trained with more complex strategies. Surprisingly, multistep reasoning (e.g., Step 1..., Step 2...; see Fig. 11, 12, 13) spontaneously emerges in the models output, which derives the final answer through sequential analysis, despite being supervised only on the final choice, without intermediate reasoning traces like distilled CoT data [29, 46]. This emergent behavior shows that minimalist rule-based RL not only boosts performance but also encourages structured reasoning, offering valuable interpretability into the models decision-making. Performance Variation and Training Dynamics Across Subsets. We observe clear performance differences among training subsets, consistently ranking as MedQA-Sub > MedMCQA-Sub > 5 Figure 3: Effect of data quantity. Average accuracy across six medical QA benchmarks as the number of samples per level increases from 200 to 400, resulting in the total subset size growing from 1,200 to 2,400 examples. Scaling MedQA-Sub and MedMCQA-Sub leads to consistent performance gains, highlighting the value of informative data. In contrast, PubMedQA-Sub shows no improvement, reflecting the limitations of low-informative data sources. Figure 4: Effect of data diversity. Average accuracy across six medical QA benchmarks when models are trained individually on single or combined subsets. Adding MedMCQA-Sub to MedQA-Sub boosts performance, while further adding PubMedQA-Sub reduces it, suggesting that less informative data can negate the benefits of increased diversity. #unique queries PubMedQA-Sub. To understand this variation, we explore the training dynamics of models trained on each subset. As depicted in Fig. 2 (left), following [16], the ratio of effective queries is computed as 1 #solved all+#solved none , where solved all and solved none denote batches in which all responses are either correct or incorrect. Models trained on PubMedQA-Sub exhibit rapid decline in the effective query ratio, indicating premature saturation and reduction in effective samples from the batch. The training reward in Fig. 2 (middle) further supports this: the PubMedQA-Sub variant starts with higher initial reward and increases rapidly, suggesting that the data is easy to learn at the start, but quickly saturates after about 20 steps. In contrast, the MedQA-Sub and MedMCQA-Sub models improve steadily throughout training. Dataset Informativeness as Key Driver. To further investigate these dynamics, we analyze the question length distributions in the source datasets of each subset, as shown in Fig. 2 (right). Notably, MedQA [43] exhibits significantly longer question length distribution compared to MedMCQA [44] and PubMedQA [34], this ordering closely matches the observed performance of model variants trained on the respective subsets. These differences are linked to dataset construction mechanisms: PubMedQA [34] is automatically curated from biomedical literature, often resulting in noisier and less informative questions; MedMCQA [44] is based on human-authored medical school entrance exams, providing more reliable and informative samples; MedQA [43] is sourced from the USMLE, challenging licensing exam, and thus contains the most informative and well-structured questions. Altogether, our findings suggest that question length serves as practical proxy for dataset informativeness in medical QA. High-informativeness, exam-certified data provide more stable and effective learning signals for minimalist RL, whereas noisy, automatically curated data may offer lower informativeness and thus hinder the acquisition of reasoning ability. Finding 1.1: Minimalist rule-based RL enables medical reasoning in LLM beyond reliance on SFT with distilled CoT data. Finding 1.2: Dataset informativeness is critical for training success. LLM trained on low informative or noisy data exhibit degraded performance. Question length serves as practical proxy for informativeness in medical QA. 5.3 RQ2: Impact of Dataset Quantity and Diversity Effect of Dataset Quantity. To investigate the effect of training data size, we increase the number of samples per difficulty level from 200 to 400 for each of the three subsets, resulting in the total 6 Figure 5: Performance on six benchmarks when training on subsets with increasing difficulty levels (L1 to L6). Each blue dot represents separately trained model on subset that includes all data up to the indicated difficulty level; new data are incorporated only through separate training runs, not incrementally during training. While performance on MedXpert [37] increases consistently, trends on other benchmarks vary. Final models trained on the full set (L1L6) generally achieve comparable or superior performance to HuatuoGPT-o1-8B [46]. Figure 6: Performance on six benchmarks when training with distinct difficulty groups: easy (L1+L2), medium (L3+L4), and hard (L5+L6). While harder training data improves MedXpert [37] accuracy, performance on other benchmarks declines, suggesting that relying solely on difficult samples may impair general reasoning ability. number of samples in each subset increasing from 1,200 to 2,400. As shown in Fig. 3, we report the average accuracy across six benchmarks. Scaling MedQA-Sub improves accuracy from 58.96% to 59.88%, and MedMCQA-Sub improves from 57.41% to 58.76%, demonstrating that increasing high-informative data benefits model performance. In contrast, scaling PubMedQA-Sub yields no improvement (55.71% 55.58%), suggesting that adding more low-informative or noisy samples may degrade performance rather than enhance it. Effect of Dataset Diversity. We further examine the effect of dataset diversity by progressively combining subsets. As shown in Fig. 4, adding MedMCQA-Sub to MedQA-Sub further improves performance, highlighting the benefit of combining diverse and informative datasets. However, incorporating PubMedQA-Sub reverses the upward trend and leads to decline in performance, indicating that noisy and less informative data not only fail to contribute but may also harm reasoning ability. 7 Figure 7: Comparison of AlphaMed(8B) with prior models on MMLU-ProM [35] and MedXpert [37]. Despite its smaller scale and use of minimalist RL, AlphaMed(8B) outperforms the larger model QwQ-32B [48] and other baselines. Figure 8: AlphaMed(70B) achieves superior performance over Claude-3.5-Sonnet [49], GPT-4o [47], and DeepSeek-V3 (671B) [50] on MMLU-ProM [35] and MedXpert [37], showcasing its strong reasoning ability. Finding 2: Performance improves with increased data quantity and diversity only when the additional samples are informative; low-quality data harms the learning of reasoning ability. 5.4 RQ3: Impact of Dataset Quality We analyze how increasing training difficulty affects performance across six benchmarks, as shown in Fig. 5. MedQA, MedMCQA, and PubMedQA [43, 44, 34] exhibit inverse U-shaped trends, performance peaks with moderate difficulty (L1L4) and declines with harder samples (L5L6), suggesting diminishing returns from high-difficulty data. In contrast, MMLU-ProM [39] and GPQAM [36] show oscillating patterns, while MedXpert [37] improves steadily with increasing difficulty, highlighting the value of harder samples for complex tasks. To validate this, we train models on three difficulty groups (easy: L1+L2, medium: L3+L4, hard: L5+L6; Fig. 6). On MedXpert [37], models trained on hard samples perform best, confirming their role in promoting advanced reasoning. For other benchmarks, training on easy and medium levels yields better generalization, while hard-only training underperforms. Emerging Reasoning Capability from Simple Data, Indicating Benchmark Limits. Interestingly, models trained only on L1+L2 (a total of 2,400 samples) already match or surpass HuatuoGPT-o1-8B [46] on several benchmarks. As shown in Fig. 5, even on MedXpert, only training with L1 data exceeds HuatuoGPT-o1-8B [46], with further gains from adding more levels, indicating that reasoning can emerge from simple data. These findings underscore the importance of balanced training difficulty to support broad generalization. They also reveal potential pitfall: if high benchmark scores can be achieved without exposure to difficult samples, such scores may not reflect genuine reasoning ability, raising concerns about the adequacy of current benchmark designs. Finding 3.1: Mixed difficulty training is crucial for generalizable reasoning. Finding 3.2: Current benchmarks may insufficient to capture true reasoning progress. 5.5 Main Results Building on the above findings which highlight the importance of dataset quantity, diversity, informativeness, and mixed difficulty for incentivizing reasoning, we construct our final training set accordingly. Specifically, we include all samples from MedQA [43] due to its high informativeness, and sample 1,600 QA pairs from each difficulty level of MedMCQA [44] to match the overall scale of MedQA [43]. PubMedQA [34] is excluded due to its limited informativeness and the performance degradation observed when it is included, as discussed in RQ1 and RQ2. The final training set comprises 19,178 QA pairs. This dataset is used to train our final models: AlphaMed(8B), based on Llama3.1-8B-Instruct, and AlphaMed(70B), based on Llama3.1-70B-Instruct, both optimized using minimalist rule-based RL. Since MedQA [43] and MedMCQA [44] are used for training, we treat PubMedQA [34], MMLU-ProM [39], GPQA-M [51], and MedXpert [37] as out-of-domain (OOD) benchmarks. We present the full results in Tab. 1. Across both model scales, AlphaMed consistently outperforms all compared methods on both in-domain and OOD benchmarks, using only minimalist rule-based RL and multiple-choice QA supervision. Remarkably, this advantage holds even against models trained with more complex strategies [46, 27], including SFT on distilled CoT data [46, 27, 29] Model MedQA MedMCQA PubMedQA MMLU-ProM GPQA-M MedXpert Challenge Level Normal Normal Normal Hard Hard Hard+ In-Domain Out-of-Domain Llama-3.1-8B-Instruct Qwen2.5-7B-Instruct Qwen2.5-7B-Instruct+ MedLlama3-8B-v1 MedLlama3-8B-v2 MMed-8B MMedS-8B MMed-8B-EnIns Med42-8B OpenBioLLM-8B UltraMedical-8B-3 UltraMedical-8B-3.1 HuatuoGPT-o1-8B m1-7B AlphaMed(8B) Llama-3.1-70B-Instruct QwQ-32B Qwen2.5-32B-Instruct Qwen2.5-32B-Instruct+ Qwen2.5-72B-Instruct Qwen2.5-72B-Instruct+ Med42-70B OpenBioLLM-70B UltraMedical-70B-3 HuatuoGPT-o1-70B m1-32B AlphaMed(70B) 58.72 61.51 64.49 55.07 59.39 54.28 57.19 60.33 59.78 55.30 71.09 75.73 72.60 75.81 76.19 78.42 78.62 75.26 74.86 74.55 76.43 51.14 75.10 83.90 83.30 83.50 87.52 < 10B LLMs 75.21 71.30 72.60 52.70 75.50 63.40 77.50 63.80 76.00 70.10 71.00 79.20 79.20 75.80 80.40 > 10B LLMs 78.52 77.85 68.00 68.90 70.80 71.30 78.10 79.30 80.00 80.60 77.60 80.90 56.21 56.56 56.11 34.74 59.34 52.71 47.29 58.09 56.35 54.63 59.22 63.78 60.40 62.54 64.47 72.53 69.71 64.83 64.33 66.60 66.15 62.28 74.23 72.94 73.60 67.34 75.09 58.74 61.17 62.15 27.43 55.11 48.27 33.55 51.60 55.64 49.32 61.50 64.30 63.71 65.86 66.67 74.50 65.23 74.72 74.72 66.06 69.77 54.53 71.92 73.94 76.09 77.94 79.56 42.73 42.56 52.56 30.77 36.41 34.87 22.05 45.90 48.21 41.03 50.00 48.72 55.38 53.08 58. 55.73 56.92 63.85 64.87 62.05 63.85 50.77 50.77 58.72 66.67 66.67 77.46 13.02 12.15 13.18 11.04 13.46 13.73 17.39 18.56 14.63 14.29 15.25 17.39 16.84 19.81 22.14 21.32 21.05 13.87 14.56 14.91 19.65 16.29 21.33 21.67 26.36 25.53 32.56 Table 1: Combined performance of models on six medical QA benchmarks with varying levels of challenge. In-domain and out-of-domain tasks, as well as challenge levels (Normal, Hard, Hard+), are indicated below the task names. m1 denotes models that use test-time scaling during inference. +: using CoT prompting during inference; : trained with distilled CoT data from stronger models (e.g., GPT-4o); : trained with external datasets beyond MedQA and MedMCQA; : trained via RL with verifier reward models or distilled preference data from powerful models (e.g., GPT-4o). AlphaMed (Ours) is trained solely with minimalist rule-based RL on multi-choice QA, without any SFT on distilled CoT data, preference data, or rewards from verifiers. and methods enhanced with test-time scaling [29]. Notably, AlphaMed(8B) surpasses the larger reasoning model QwQ-32B [48] on challenging OOD benchmarks, as shown in Fig. 7. At the 70B scale, AlphaMed(70B) outperforms even closed-source models such as GPT-4o [47] and Claude-3.5-Sonnet [49], as well as the open-source DeepSeek-V3 (671B parameters) [50], as shown in Fig. 8. These results show that minimalist rule-based RL, trained with well-constructed multiple-choice QA dataset, enables effective and scalable medical reasoning in LLMs without relying on distilled CoT supervision."
        },
        {
            "title": "6 Conclusion",
            "content": "We present AlphaMed, the first work to demonstrate that reasoning capabilities can emerge solely through minimalist rule-based RL, without relying on SFT with distilled CoT data. By leveraging only multiple-choice QA datasets, AlphaMed achieves state-of-the-art performance across six diverse and challenging medical QA benchmarks, surpassing models trained with conventional SFT+RL pipelines, and even outperforming closed-source models (e.g., GPT-4o [47]. Through comprehensive data-centric analyses, we show that reasoning ability can be effectively incentivized by selecting data based on informativeness. We further find that increasing the number of informative training samples improves performance, and that varying difficulty levels contribute differently across benchmarks, underscoring the importance of mixing difficulty to promote generalizable reasoning. well-curated dataset with high informativeness and diverse difficulty levels is key to 9 advancing reasoning, without requiring handcrafted rationales or distilled data from closed models. Our findings also reveal critical caveat: while challenging benchmarks benefit from harder training samples, others exhibit mixed or plateauing trends, suggesting that existing benchmarks may be insufficient to evaluate progress of reasoning ability. This highlights the need for more challenging, reasoning-oriented benchmarks. Altogether, AlphaMed not only establishes strong medical LLM, but also offers insights into how models reach final predictions through emergent reasoning, encouraging further exploration of interpretable systems in medical NLP."
        },
        {
            "title": "References",
            "content": "[1] Y. Qin, X. Li, H. Zou, Y. Liu, S. Xia, Z. Huang, Y. Ye, W. Yuan, H. Liu, Y. Li et al., O1 replication journey: strategic progress reportpart 1, arXiv preprint arXiv:2410.18982, 2024. [2] Z. Zeng, Q. Cheng, Z. Yin, B. Wang, S. Li, Y. Zhou, Q. Guo, X. Huang, and X. Qiu, Scaling of search and learning: roadmap to reproduce o1 from reinforcement learning perspective, arXiv preprint arXiv:2412.14135, 2024. [3] J. Wang, M. Fang, Z. Wan, M. Wen, J. Zhu, A. Liu, Z. Gong, Y. Song, L. Chen, L. M. Ni et al., Openr: An open source framework for advanced reasoning with large language models, arXiv preprint arXiv:2410.09671, 2024. [4] M. Y. Guan, M. Joglekar, E. Wallace, S. Jain, B. Barak, A. Heylar, R. Dias, A. Vallone, H. Ren, J. Wei, H. W. Chung, S. Toyer, J. Heidecke, A. Beutel, and A. Glaese, Deliberative alignment: Reasoning enables safer language models, OpenAI Blog, 2024. [Online]. Available: https://openai.com/index/deliberative-alignment/ [5] K. Saab, T. Tu, W.-H. Weng, R. Tanno, D. Stutz, E. Wulczyn, F. Zhang, T. Strother, C. Park, E. Vedadi et al., Capabilities of gemini models in medicine, arXiv preprint arXiv:2404.18416, 2024. [6] J. Chen, C. Gui, A. Gao, K. Ji, X. Wang, X. Wan, and B. Wang, Cod, towards an interpretable medical agent using chain of diagnosis, arXiv preprint arXiv:2407.13301, 2024. [7] V. L. Patel, J. F. Arocha, and J. Zhang, Thinking and reasoning in medicine, The Cambridge handbook of thinking and reasoning, vol. 14, pp. 727750, 2005. [8] S. Xu, Y. Zhou, Z. Liu, Z. Wu, T. Zhong, H. Zhao, Y. Li, H. Jiang, Y. Pan, J. Chen et al., Towards nextgeneration medical agent: How o1 is reshaping decision-making in medical scenarios, arXiv preprint arXiv:2411.14461, 2024. [9] M.-H. Temsah, A. Jamal, K. Alhasan, A. A. Temsah, and K. H. Malki, Openai o1-preview vs. chatgpt in healthcare: new frontier in medical ai reasoning, Cureus, vol. 16, no. 10, p. e70640, 2024. [10] Y. Xie, J. Wu, H. Tu, S. Yang, B. Zhao, Y. Zong, Q. Jin, C. Xie, and Y. Zhou, preliminary study of o1 in medicine: Are we closer to an ai doctor? arXiv preprint arXiv:2409.15277, 2024. [11] J. Chen, X. Wang, K. Ji, A. Gao, F. Jiang, S. Chen, H. Zhang, D. Song, W. Xie, C. Kong et al., Huatuogptii, one-stage training for medical adaption of llms, arXiv preprint arXiv:2311.09774, 2023. [12] J. Wei, X. Wang, D. Schuurmans, M. Bosma, B. Ichter, F. Xia, E. H. Chi, Q. V. Le, and D. Zhou, Chainof-thought prompting elicits reasoning in large language models, Advances in Neural Information Processing Systems, vol. 35, pp. 24 82424 837, 2022. [13] H. W. Chung, L. Hou, S. Longpre, B. Zoph, Y. Tay, W. Fedus, Y. Li, X. Wang, J. Wei et al., Scaling instruction-finetuned language models, arXiv preprint arXiv:2210.11416, 2022. [14] E. Zelikman, Y. Wu, J. Mu, and N. D. Goodman, Star: Bootstrapping reasoning with reasoning, Advances in Neural Information Processing Systems, vol. 35, pp. 15 47615 488, 2022. [15] T. Chu, Y. Zhai, J. Yang, S. Tong, S. Xie, D. Schuurmans, Q. V. Le, S. Levine, and Y. Ma, Sft memorizes, rl generalizes: comparative study of foundation model post-training, arXiv preprint arXiv:2501.17161, 2025. [16] H. Wang, C. Qu, Z. Huang, W. Chu, F. Lin, and W. Chen, Vl-rethinker: Incentivizing self-reflection of vision-language models with reinforcement learning, arXiv preprint arXiv:2504.08837, 2025. [17] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray et al., Training language models to follow instructions with human feedback, Advances in Neural Information Processing Systems, vol. 35, pp. 27 73027 744, 2022. [18] H. Wang, L. Li, C. Qu, F. Zhu, W. Xu, W. Chu, and F. Lin, Learning autonomous code integration for math language models, arXiv preprint arXiv:2502.00691, 2025. [19] R. Rafailov, A. Sharma, E. Mitchell, C. D. Manning, S. Ermon, and C. Finn, Direct preference optimization: Your language model is secretly reward model, Advances in Neural Information Processing Systems, vol. 36, 2023. [20] A. Ura, Openbiollm-70b: Advancing open-source biomedical llms with direct preference optimization, Hugging Face Blog, 2024, available at https://huggingface.co/blog/aaditya/openbiollm. 11 [21] D. Guo, D. Yang, H. Zhang, J. Song, R. Zhang, R. Xu, Q. Zhu, S. Ma, P. Wang, X. Bi et al., Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, arXiv preprint arXiv:2501.12948, 2025. [22] H. Zeng, D. Jiang, H. Wang, P. Nie, X. Chen, and W. Chen, Acecoder: Acing coder rl via automated test-case synthesis, arXiv preprint arXiv:2502.01718, 2025. [23] J. Pan, C. Liu, J. Wu, F. Liu, J. Zhu, H. B. Li, C. Chen, C. Ouyang, and D. Rueckert, Medvlm-r1: Incentivizing medical reasoning capability of vision-language models (vlms) via reinforcement learning, arXiv preprint arXiv:2502.19634, 2025. [24] H. Zhang, J. Chen, F. Jiang, F. Yu, Z. Chen, J. Li, G. Chen, X. Wu, Z. Zhang, Q. Xiao et al., Huatuogpt, towards taming language model to be doctor, arXiv preprint arXiv:2305.15075, 2023. [25] Y. Labrak, A. Bazoge, E. Morin, P.-A. Gourraud, M. Rouvier, and R. Dufour, Biomistral: collection of open-source pretrained large language models for medical domains, arXiv preprint arXiv:2402.10373, 2024. [26] Z. Ke, Y. Shao, H. Lin, T. Konishi, G. Kim, and B. Liu, Continual pre-training of language models, arXiv preprint arXiv:2302.03241, 2023. [27] K. Zhang, S. Zeng, E. Hua, N. Ding, Z.-R. Chen, Z. Ma, H. Li, G. Cui, B. Qi, X. Zhu et al., Ultramedical: Building specialized generalists in biomedicine, Advances in Neural Information Processing Systems, vol. 37, pp. 26 04526 081, 2024. [28] J. Chen, Z. Cai, K. Ji, X. Wang, W. Liu, R. Wang, J. Hou, and B. Wang, Huatuogpt-o1: Towards medical complex reasoning with llms, arXiv preprint arXiv:2412.18925, 2024. [29] X. Huang, J. Wu, H. Liu, X. Tang, and Y. Zhou, m1: Unleash the potential of test-time scaling for medical reasoning with large language models, arXiv preprint arXiv:2504.00869, 2025. [30] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, Proximal policy optimization algorithms, arXiv preprint arXiv:1707.06347, 2017. [31] G. Sheng, C. Zhang, Z. Ye, X. Wu, W. Zhang, R. Zhang, Y. Peng, H. Lin, and C. Wu, Hybridflow: flexible and efficient rlhf framework, arXiv preprint arXiv:2409.19256, 2024. [32] D. Jin, E. Pan, N. Oufattole, W.-H. Weng, H. Fang, and P. Szolovits, What disease does this patient have? large-scale open domain question answering dataset from medical exams, Applied Sciences, vol. 11, no. 14, p. 6421, 2021. [33] A. Pal, L. K. Umapathi, and M. Sankarasubbu, Medmcqa: large-scale multi-subject multi-choice dataset for medical domain question answering, in Conference on health, inference, and learning. PMLR, 2022, pp. 248260. [34] Q. Jin, B. Dhingra, Z. Liu, W. W. Cohen, and X. Lu, Pubmedqa: dataset for biomedical research question answering, arXiv preprint arXiv:1909.06146, 2019. [35] Y. Wang, X. Ma, G. Zhang, Y. Ni, A. Chandra, S. Guo, W. Ren, A. Arulraj, X. He, Z. Jiang et al., Mmlupro: more robust and challenging multi-task language understanding benchmark, in The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2024. [36] D. Rein, B. L. Hou, A. C. Stickland, J. Petty, R. Y. Pang, J. Dirani, J. Michael, and S. R. Bowman, Gpqa: graduate-level google-proof q&a benchmark, in First Conference on Language Modeling, 2024. [37] Y. Zuo, S. Qu, Y. Li, Z. Chen, X. Zhu, E. Hua, K. Zhang, N. Ding, and B. Zhou, Medxpertqa: Benchmarking expert-level medical reasoning and understanding, arXiv preprint arXiv:2501.18362, 2025. [38] X. Tang, D. Shao, J. Sohn, J. Chen, J. Zhang, J. Xiang, F. Wu, Y. Zhao, C. Wu, W. Shi et al., Medagentsbench: Benchmarking thinking models and agent frameworks for complex medical reasoning, arXiv preprint arXiv:2503.07459, 2025. [39] Y. Wang, X. Ma, G. Zhang, Y. Ni, A. Chandra, S. Guo, W. Ren, A. Arulraj, X. He, Z. Jiang et al., Mmlupro: more robust and challenging multi-task language understanding benchmark, arXiv preprint arXiv:2406.01574, 2024. [40] M. S. A. Pal and M. Sankarasubbu, Openbiollms: Advancing open-source large language models for healthcare and life sciences, 2024. 12 [41] P. Qiu, C. Wu, X. Zhang, W. Lin, H. Wang, Y. Zhang, Y. Wang, and W. Xie, Towards building multilingual language model for medicine, Nature Communications, vol. 15, no. 1, p. 8384, 2024. [42] C. Christophe, P. K. Kanithi, P. Munjal, T. Raha, N. Hayat, R. Rajan, A. Al-Mahrooqi, A. Gupta, M. U. Salman, G. Gosal et al., Med42evaluating fine-tuning strategies for medical llms: Full-parameter vs. parameter-efficient approaches, arXiv preprint arXiv:2404.14779, 2024. [43] D. Jin, E. Pan, N. Oufattole, W.-H. Weng, H. Fang, and P. Szolovits, What disease does this patient have? large-scale open domain question answering dataset from medical exams, Applied Sciences, vol. 11, no. 14, p. 6421, 2021. [44] A. Pal, L. K. Umapathi, and M. Sankarasubbu, Medmcqa: large-scale multi-subject multi-choice dataset for medical domain question answering, in Conference on Health, Inference, and Learning. PMLR, 2022, pp. 248260. [45] A. Dubey, A. Jauhri, A. Pandey, A. Kadian, A. Al-Dahle, A. Letman, A. Mathur, A. Schelten, A. Yang, A. Fan et al., The llama 3 herd of models, arXiv preprint arXiv:2407.21783, 2024. [46] J. Chen, Z. Cai, K. Ji, X. Wang, W. Liu, R. Wang, J. Hou, and B. Wang, Huatuogpt-o1, towards medical complex reasoning with llms, arXiv preprint arXiv:2412.18925, 2024. [47] A. Hurst, A. Lerer, A. P. Goucher, A. Perelman, A. Ramesh, A. Clark, A. Ostrow, A. Welihinda, A. Hayes, A. Radford et al., Gpt-4o system card, arXiv preprint arXiv:2410.21276, 2024. [48] Q. Team, Qwq: Reflect deeply on the boundaries of the unknown, November 2024. [Online]. Available: https://qwenlm.github.io/blog/qwq-32b-preview/ [49] Anthropic, The claude 3 model family: Opus, sonnet, haiku, https://www-cdn.anthropic.com/ de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf, 2024. [50] A. Liu, B. Feng, B. Xue, B. Wang, B. Wu, C. Lu, C. Zhao, C. Deng, C. Zhang, C. Ruan et al., Deepseekv3 technical report, arXiv preprint arXiv:2412.19437, 2024. [51] D. Rein, B. L. Hou, A. C. Stickland, J. Petty, R. Y. Pang, J. Dirani, J. Michael, and S. R. Bowman, Gpqa: graduate-level google-proof q&a benchmark, arXiv preprint arXiv:2311.12022, 2023. [52] Q. Team, Qwen2.5: party of https://qwenlm.github.io/blog/qwen2.5/ foundation models, September 2024. [Online]. Available:"
        },
        {
            "title": "Limitations and Future Work",
            "content": "Although AlphaMed achieves impressive results on multiple-choice QA tasks, its capabilities remain constrained by the closed-form nature of these benchmarks. Our evaluations are primarily conducted on existing mainstream medical QA datasets, all of which are close-ended and may not fully capture the spectrum of real-world clinical reasoning. Due to limitations in the current research landscape, it is challenging to systematically assess our models performance on open-ended QA tasks, which not only lack well-established benchmarks but are also inherently subjective, often requiring human evaluation for meaningful assessment. In future work, we aim to design and release open-ended benchmarks that involve human-in-the-loop evaluation, enabling more comprehensive and nuanced assessments of reasoning and decision-making in medical LLMs."
        },
        {
            "title": "Broader Impact",
            "content": "This work demonstrates that the reasoning capability of medical LLMs can be effectively incentivized using only multiple-choice QA data with minimalist rule-based RL, removing the need for SFT on costly distilled CoT data. By eliminating reliance on manual annotation and closed-source supervision, our approach substantially reduces the human effort and resources required for developing high-performing clinical models. However, the emerging reasoning processes in LLMs are inherently difficult to evaluate, as there is often no single ground truth reasoning pathespecially in medicine, where multiple valid clinical justifications may exist for single decision. Nonetheless, exposing these intermediate reasoning steps provides an important opportunity to observe and audit model behavior, ultimately encouraging the development of more transparent and trustworthy medical LLMs."
        },
        {
            "title": "A Appendix",
            "content": "A.1 Difficulty Level Distribution To explore how the difficulty level of training data affects model performance, we annotate each sample by its response consistency across five inference passes of Llama3.1-8B-Instruct [45]. Specifically, L1 denotes samples where the model answers all attempts correctly (easy), while L6 includes those where all predictions are incorrect (hard). Intermediate levels (L2L5) indicate varying degrees of partial correctness. Tab. A.1 summarizes the distribution across MedQA5, MedMCQA6, and PubMedQA7. Table 2: Difficulty Level Distribution. L1 indicates samples where Llama3.1-8B-Instruct [45] predicts correctly in all 5 inference attempts (easiest), while L6 corresponds to samples where all predictions are incorrect (hardest). Intermediate levels (L2L5) reflect partial correctness across attempts. Dataset Total L2 L3 L4 L5 L6 MedQA 10,178 MedMCQA 182,822 PubMedQA 211, 1,970 63,292 97,790 1,471 25,736 41,604 934 14,498 18,596 697 9,922 10,759 713 10,088 9,217 4,393 59,286 33, A.2 Details of Evaluation Datasets To thoroughly assess performance across varying levels of challenge, we evaluate on six medical QA benchmarks, grouped by challenge level: Normal challenge level 5https://huggingface.co/datasets/GBaker/MedQA-USMLE-4-options-hf 6https://huggingface.co/datasets/openlifescienceai/medmcqa 7https://huggingface.co/datasets/qiaojin/PubMedQA 14 MedQA [43]: benchmark derived from US medical licensing exam questions, assessing clinical knowledge across wide range of topics. Evaluation is based on the standard test split. MedMCQA [44]: medical QA dataset based on entrance exams, designed to test foundational medical knowledge through multiple-choice questions. The official test split is used. PubMedQA [34]: biomedical question answering dataset where models choose from three fixed options, yes, no, or maybe, based on associated research abstracts, emphasizing factual understanding in biomedical literature. The official test split is used. Hard challenge level MMLU-ProM [39]: MMLU-ProM is the medical category subset of broad multitask benchmark, focusing on professional-level medicine and related domains. Evaluation is conducted using the standard split established in [46]. GPQA-M [36]: It represents the biomedical subset of graduate-level QA benchmark, featuring expert-curated questions intentionally designed to resist superficial retrieval and demand deep analytical reasoning. The evaluation follows the split from [46]. Hard+ challenge level MedXpert [37]: challenging benchmark designed to assess expert-level medical knowledge, clinical understanding, and complex reasoning. It covers diverse specialties and body systems, incorporates board-style exam questions, and is curated through expert review to ensure high difficulty, accuracy, and relevance to real-world medical decision-making. A.3 Effect of LLM Backbones To assess the generality of our proposed training pipeline and data design, we further apply the same minimalist rule-based RL approach, originally used for Llama3.1-8B-Instruct, to Qwen2.5-7B-Instruct [52]. After training, the resulting AlphaMed(7B) model achieves consistent improvements across all six benchmarks, as shown in Fig. 9. Notably, the gains are particularly substantial on the more challenging datasets, MMLU-ProM [35], GPQA-M [36], and MedXpert [37], demonstrating the robustness of our training strategy in enhancing medical reasoning. These results demonstrate that minimalist rule-based RL can incentivize reasoning capabilities and boost performance, exhibiting robustness across different backbone models. Figure 9: Performance comparison across six medical QA benchmarks. AlphaMed(7B) is initialized from Qwen2.5-7B-Instruct [52] and trained using our constructed training set and minimalist rule-based RL pipeline. It achieves consistent improvements over the base model on all benchmarks. A.4 Success on Small LLM To further evaluate the effectiveness of our minimalist RL pipeline, we apply it to small language model, Qwen2.5-3B-Instruct [52]. As shown in Fig. 10, our approach consistently improves performance across all six medical benchmarks, including substantial gains on MedQA (+11.55%), GPQA-M (+19.19%), and MedXpert (+4.10%). These results demonstrate that our RL framework can effectively incentivize reasoning capabilities even in smaller-scale models, and is not limited to large foundation models. 15 Figure 10: Performance comparison across six medical QA benchmarks. AlphaMed(3B) is initialized from Qwen2.5-3B-Instruct [52] and trained with our constructed dataset using minimalist rule-based RL pipeline. It achieves consistent gains over the base model. A.5 Qualitative Results We present three examples predicted by our model trained with minimalist RL, demonstrating interpretable step by step clinical reasoning across diverse case types. In Fig. 11, the model correctly identifies inappropriate and potentially harmful options (e.g., use of NOACs in patients with mechanical heart valves) and adheres to guidelines by recommending bridging strategies based on patient risk factors and procedural context. In Fig. 12, it performs multi step numerical reasoning to derive absolute risk reduction (ARR) and relative risk (RR), showcasing its ability to integrate clinical knowledge with quantitative interpretation. In Fig. 13, the model applies structured reasoning to diagnose croup in pediatric patient, identifying clinical features, linking them to pathophysiology, and reviewing radiographic findings, despite being supervised only on the final answer choice. This highlights the models capacity for guideline aligned reasoning and emergent interpretability, even without supervision on intermediate reasoning traces. 16 Figure 11: Question and answer pair for Case 1. Cyan text highlights the final predicted choices. Green highlight are used to emphasize reasoning steps and key clinically key information. 17 Figure 12: Question and answer pair for Case 2. Cyan text highlights the final predicted choices. Green highlight are used to emphasize reasoning steps and key clinically key information. 18 Figure 13: Question and answer pair for Case 3. Cyan text highlights the final predicted choices. Green highlight are used to emphasize reasoning steps and key clinically key information."
        }
    ],
    "affiliations": [
        "Fudan University",
        "HKUST",
        "Imperial College London",
        "Ohio State University",
        "Technical University of Munich"
    ]
}