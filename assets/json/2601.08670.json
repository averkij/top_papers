{
    "paper_title": "Parallel Context-of-Experts Decoding for Retrieval Augmented Generation",
    "authors": [
        "Giulio Corallo",
        "Paolo Papotti"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Retrieval Augmented Generation faces a trade-off: concatenating documents in a long prompt enables multi-document reasoning but creates prefill bottlenecks, while encoding document KV caches separately offers speed but breaks cross-document interaction. We propose Parallel Context-of-Experts Decoding (Pced), a training-free framework that shifts evidence aggregation from the attention mechanism to the decoding. Pced treats retrieved documents as isolated \"experts\", synchronizing their predictions via a novel retrieval-aware contrastive decoding rule that weighs expert logits against the model prior. This approach recovers cross-document reasoning capabilities without constructing a shared attention across documents."
        },
        {
            "title": "Start",
            "content": "Parallel Context-of-Experts Decoding for Retrieval Augmented Generation Giulio Corallo SAP Labs, France EURECOM, France giulio.corallo@sap.com Paolo Papotti EURECOM, France papotti@eurecom.fr 6 2 0 2 3 1 ] A . [ 1 0 7 6 8 0 . 1 0 6 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Retrieval Augmented Generation faces tradeoff: concatenating documents in long prompt enables multi-document reasoning but creates prefill bottlenecks, while encoding document KV caches separately offers speed but breaks cross-document interaction. We propose Parallel Context-of-Experts Decoding (PCED), training-free framework that shifts evidence aggregation from the attention mechanism to the decoding. PCED treats retrieved documents as isolated \"experts\", synchronizing their predictions via novel retrieval-aware contrastive decoding rule that weighs expert logits against the model prior. This approach recovers crossdocument reasoning capabilities without constructing shared attention across documents."
        },
        {
            "title": "Introduction",
            "content": "Retrieval Augmented Generation (RAG) augments language models with external corpora to improve factuality and reduce hallucinations (Lewis et al., 2020; Gao et al., 2023; Fan et al., 2024). However, standard pipelines concatenate many retrieved documents into single long context prompt, making inference dominated by prefill latency (Kwon et al., 2023; Zhong et al., 2024; Cheng et al., 2025). Additionally, long contexts increase reasoning failures, as models often struggle to integrate evidence spread across multiple documents (Liu et al., 2024). Parallel KV cache encoding mitigates prefill cost by encoding retrieved documents independently and reusing their cached states at inference time (Yang et al., 2025b,c). However, removing cross-document attention during encoding can substantially degrade performance on multi-hop and reasoning-intensive queries (Yao et al., 2025). We propose Parallel Context-of-Experts Decoding (PCED), training-free framework that shifts document aggregation from attention to decoding. As depicted in Figure 1, at each generation step, PCED treats each document as separate Figure 1: Parallel Context-of-Experts Decoding (PCED) runs one expert per retrieved document (and nocontext, amateur prior) in parallel and chooses each next token based on retrieval support, enabling evidence to be stitched across documents without joint attention. expert, which proposes next-token distribution from its own KV cache, and then weights the bestsupported token so evidence can be efficiently aggregated across documents without building joint attention context. We make three contributions: (1) parallel, modular KV cache framework with decode-time evidence aggregation; (2) token-level expert switching to recover cross-document reasoning via dynamic expert selection at every token step without shared attention; and (3) retrievalintegrated priors that inject scalar scores into the contrastive decoding to gate noise from irrelevant experts. On benchmarks like LOFT and LongBench, PCED outperforms prior parallel methods by up to 70 points and often matches or outperforms long context baselines, while delivering over 180 speedup in time-to-first-token."
        },
        {
            "title": "2 Related Work",
            "content": "We position our work at the intersection of (1) KV caching for parallel prefill, (2) cross-document interaction recovery under independent KV caches, and (3) context-aware decoding. Parallel encoding eliminates prefill cost by precomputing offline per-document KV caches that can be retrieved at inference time. Prior work includes training-free masking for blockwise/paral1 lel attention (Ratner et al., 2023), fine-tuning to mitigate quality degradation under blocked attention (Ma et al., 2025), and interfaces that decouple document encoding from generation (Yen et al., 2024). Systems work integrates KV cache retrieval into RAG pipelines (Lu et al., 2025). These approaches assume documents as independently encodable, while we study how to aggregate evidence across multiple cached documents at inference. Cache merging techniques encode documents independently and then aim to restore the crossdocument attention, as simply concatenating perdocument KV caches does not recover it (Yao et al., 2025). Recent methods achieve this via selective recomputation at merging (Yao et al., 2025), learned bridging tokens for inter-document interactions (Yang et al., 2025b), or trainingfree alignment to approximate sequential attention (APE) (Yang et al., 2025c). Our work preserves per-document modularity while enabling effective cross-document reasoning. Context-aware decoding (CAD) (Shi et al., 2024) improves faithfulness by shifting probability mass toward tokens supported by context; it is related to contrastive decoding (Li et al., 2023) and classifierfree guidance in diffusion models (Ho and Salimans, 2021). However, most CAD formulations assume single supportive context that defines the conditional distribution. DvD (Jin et al., 2024) extends CAD to multiple documents but collapses them into single input sequence, which conflicts with per-document KV cache reuse, where documents must be encoded separately."
        },
        {
            "title": "3 Methodology",
            "content": "We introduce Parallel Context-of-Experts Decoding (PCED), training-free framework for scalable and faithful multi-document generation. RAG pipelines typically employ two-stage process: retrieving candidate documents using embeddings to maximize recall, followed by cross-encoder reranker to reorder candidates and maximize precision. Crucially, the scalar relevance scores produced during these stages are used only for document selection and then discarded. We argue that this discards valuable evidence about how strongly each document should be trusted during decoding. PCED converts these scores into document-level prior that controls how much each expert influences the next-token distribution, via novel retrievalaware contrastive decoding criterion. Offline KV cache preparation. Following prior cache-augmented generation work (Chan et al., 2025; Lu et al., 2025; Yang et al., 2025c; Jin et al., 2025), we assume datastore DB over corpus that stores, for each document di, an embedding ei for retrieval and its precomputed KV cache Ki: DB = {(di, ei, Ki)}D i=1. (1) 1 , . . . , rret 1 , . . . , rrer Retrieval and relevance scoring. Given query q, we retrieve the top-N documents and obtain retrieval scores rret = (rret ). We then rerank these documents with cross-encoder, producing reranker scores rrer = (rrer ). We map both score sets to the range [0, 1). Since rret primarily reflects recall and rrer precision, we fuse them into single per-document relevance score via the rrer harmonic mean rk = 2 rret +rrer rret Parallel Context-of-Experts. As depicted in Figure 1, PCED operates on +1 parallel streams (experts) in single batched forward pass: one amateur expert with an empty cache K0 = (model prior) and contextual experts, one per retrieved document, with caches K1:N and associated relevance scores r1:N . Given batch = {Kk}N k=0, processing the query updates all experts caches in parallel. At each step, this yields per-expert logits sk RV over the vocabulary V. , {1, . . . , }. Retrieval-aware contrastive decoding. For each contextual expert {1, . . . , }, we calibrate logits against the amateur s0 and incorporate retrieval-based prior: ˆsk = (1 + β0) sk β0 s0 (cid:125) (cid:124) (cid:123)(cid:122) Contrastive decoding (2) + γ log rk (cid:124) (cid:123)(cid:122) (cid:125) Retrieval prior Here, β0 controls contrast strength between amateur and expert, and γ controls retrieval gating. We compute β0 dynamically as in AdaCAD (Wang et al., 2025) for the first generated token and keep it fixed thereafter. We empirically set γ = 2.5 for all experiments (ablations in Appendix C.1 for β, C.2 for γ). Finally, the next token yt is the one with the highest score among all experts candidates. (cid:18) (cid:19) yt = arg max vV max k{1,...,N } ˆsk(v) (3) The chosen token is appended to the shared generation history for all experts at each step. 2 Table 1: Main results on RAG and ICL benchmarks. We compare our Parallel Expert Decoding (PCED) framework, equipped with Sparse, Dense, or ColBERT experts, against KV merging (APE), agentic (MapReduce), and standard concatenation baselines. Corpus in Ctx (All) is the baseline with all retrieved candidates in context. (a) MISTRAL-NEMO-13B-INSTRUCT (b) LLAMA-3.1-8B-INSTRUCT KV Merge Agentic PCED Corpus in Ctx KV Merge Agentic PCED Corpus in Ctx Task Dataset APE MapRed. Sparse Dense ColBERT Single All Task Dataset APE MapRed. Sparse Dense ColBERT Single All RAG HOTPOTQA MUSIQUE NQ QAMPARI QUEST Web Tracking7 Date ICL 27.0 11.0 38.0 7.0 1.0 58.9 6.7 40. 56.0 26.0 62.0 85.0 42.0 42.2 13.3 55.6 65.0 36.0 80.0 75.0 55.0 61.1 7.8 57.8 66.0 34.0 81.0 71.0 54.0 62.2 7.8 57. 66.0 35.0 81.0 71.0 54.0 62.2 7.8 57.8 54.0 17.0 60.0 75.0 38.0 35.6 10.0 57.8 64.0 28.0 76.0 74.0 19.0 61.1 6.7 54. RAG HOTPOTQA MUSIQUE NQ QAMPARI QUEST Web Tracking7 Date ICL 16.0 4.0 9.0 7.0 0.0 61.1 3.3 0. 41.0 8.0 50.0 68.0 41.0 56.7 13.3 44.4 64.0 14.0 83.0 77.0 45.0 62.2 11.1 53.3 64.0 21.0 85.0 76.0 40.0 64.4 11.1 47. 64.0 21.0 85.0 76.0 40.0 63.3 11.1 48.9 49.0 7.0 58.0 72.0 39.0 57.8 11.1 51.1 66.0 16.0 79.0 86.0 44.0 57.8 7.8 53."
        },
        {
            "title": "4 Experimental Setup",
            "content": "We test PCED on RAG, In Context Learning (ICL), and long-context QA with distractors. For all methods, we fix the LLM, prompts, and retrieved candidates; varying only how context is incorporated. Datasets and Metrics. We use the LOFT benchmark (Lee et al., 2024) for RAG and ICL. We retrieve fixed pool of the top-90 documents per query, shared across all baselines. Performance is measured via Subspan Exact Match for RAG tasks and Exact Match for ICL tasks. We also evaluate on the query-focused LongBench subsets (Bai et al., 2024) using official metrics. To test robustness to irrelevant context, we concatenate the gold document with K=2 uniformly sampled distractors from other test samples, keeping the corpus-incontext baseline under 128k tokens. LLMs. We report main results with MISTRALNEMO-13B-INSTRUCT (Mistral AI) and LLAMA3.1-8B-INSTRUCT (Grattafiori et al., 2024), and LongBench results with QWEN3-8B (Yang et al., 2025a) extended to 128k tokens with YARN (Peng et al., 2024). Decoding is greedy for all methods. PCED variants. We evaluate three scoring variants: Sparse, Dense, and ColBERT. The set of retrieved documents is identical for all methods. These variants differ only in the relevance signal rk extracted from bge-m3 to weight experts in Eq. 2. Baselines. We compare against three baseline families. Standard concatenation (Corpus in Ctx) conditions on either the Single top-1 document retrieved or All retrieved documents in single prompt (e.g., top-90 for LOFT). KV cache merging (APE), prefills each document independently and merges the resulting KV caches. Agentic aggregation (MAPREDUCE) performs per-document summarization (map) followed by final QA aggregation step (reduce) (Zhou et al., 2025)."
        },
        {
            "title": "5 Results and Discussion",
            "content": "We analyze our results around three main themes: (1) multi-document RAG and ICL with many candidate documents/exemplars, (2) single-document with query-focused understanding and generation tasks (including QA, summarization, code completion, and few-shot inference), and (3) efficiency. Cross-Document Reasoning Emerges at Decode Time. In Table 1, PCED consistently outperforms KV cache merging (APE) in QA benchmarks that require aggregating evidence across multiple documents (e.g., HOTPOTQA, MUSIQUE, QAMPARI, QUEST), and in ICL settings where exemplars must be used jointly. For instance, on LLAMA-3.18B QAMPARI, PCED improves from 7 (APE) to 77 (PCED-SPARSE), and yields up to +23 points over MAPREDUCE (e.g., HOTPOTQA). Moreover, PCED variants often match or exceed full-context concatenation: PCED-DENSE outperforms Corpus in Ctx (All) in 11/16 settings despite encoding each document independently. These results suggest that much of the benefit of cross-document interaction can be recovered at decode time. Figure 2: HotpotQA expert trace. Green dots illustrate the model hopping between multiple gold documents. 3 Table 2: Results on LongBench using QWEN3-8B. PCED against the full-context baseline Corpus in Ctx (All). Single-Doc QA Multi-Doc QA Summ. Few-Shot"
        },
        {
            "title": "NARQA QASPER MULTIF HOTPOT",
            "content": "2WIKI MUSIQUE QMSUM TRIVIAQA REPOB-P Corpus in Ctx (All) PCED (Sparse) PCED (Dense) PCED (ColBERT) 21.1 25.1 25.4 25.4 25.2 24.2 25.7 25.7 52.8 53.0 52.6 52.6 56.3 62.1 62.6 62. 44.2 49.4 49.4 49.4 25.3 33.4 33.3 33.3 22.0 22.7 22.9 22.9 84.0 88.8 88.2 88.2 51.1 59.7 60.1 60.1 Efficiency at Scale. Unlike context concatenation, which incurs high prefill costs, PCED leverages offline, reusable KV caches to reduce Time-To-FirstToken (TTFT). As shown in Figure 3, PCED consistently achieves substantially lower TTFT across all top-K, with gains that scale to over 180 faster TTFT (0.14s vs. 25.50s). On long-context workloads (65k context tokens, 512 generated tokens), it yields 1.7 reduction in end-to-end latency. All results use high-throughput setup with continuous batching and PagedAttention (Kwon et al., 2023) for both methods, validating the methods efficiency under realistic conditions. Table 3: Component Analysis. Disentangling benefits of Contrastive Decoding vs. Retrieval Prior."
        },
        {
            "title": "Only Contrastive Only Retrieval PCED",
            "content": "(γ = 0) (β = 0) LLAMA-8B MISTRAL-13B"
        },
        {
            "title": "HOTPOTQA\nNQ",
            "content": "46 52 57 71 53 70 65 80 64 85 66 Ablations. We verify that both terms in Eq. 2 are important: removing the retrieval prior (γ=0) or the contrastive calibration (β=0) leads to large accuracy drops  (Table 3)  . We further find that Max aggregation best supports token-level expert switching in multi-hop QA, whereas soft mixtures can help in single-doc settings. Full sweeps over β, γ, aggregation rules, and top-k are in Appendix C."
        },
        {
            "title": "6 Conclusion",
            "content": "We presented PCED, training-free decoding framework that enables efficient, multi-document reasoning under parallel, cache-native conditioning. PCED replaces long-context attention with retrieval-aware expert logit fusion at decode time, preserving KV cache modularity while recovering cross-document reasoning. Empirically, it matches or surpasses full-context baselines and is more robust to distractors. This offers an exciting alternative to long context models, allowing the number of documents to scale flexibly with batch size rather than being limited by the training context window. Figure 3: Latency Benchmarks. Comparison of TTFT scalability across Top-k values (left) and total end-toend latency with 65k context (right). Figure 2 illustrates this mechanism: to resolve multi-hop query, the model first locks onto an expert containing the bridging entity, then pivots to second expert for the final answer. By appending the chosen token to all experts shared generation histories, PCED effectively stitches evidence across isolated documents without shared attention. We note that MAPREDUCE remains superior on some settings (e.g., QAMPARI with Mistral), suggesting cases where global synthesis across many documents is beneficial. However, MAPREDUCE relies on multiple LLM calls (per-document summarization and an aggregation pass), while PCED aggregates evidence within single decoding procedure. Less Noise, More Accuracy. PCED also improves performance on tasks where the answer is primarily supported by single document, but must be recovered from large candidate pool. In these settings, full-context concatenation can degrade because relevant evidence is diluted by many near-miss documents and distractors, making attention noisier. By contrast, PCED isolates evidence by treating each document as an independent expert and explicitly emphasizing per-document relevance via retrieval-aware contrastive decoding (Eq. 2), which downweights irrelevant experts. Table 1 shows that this yields strong gains on NQ under LOFT: with Llama, PCED-DENSE improves from 58 (Corpus in Ctx Single) and 79 (All) to 85, similarly with MISTRAL from 60 (Single) and 76 (All) to 81. We observe the same trend in LongBench  (Table 2)  : when the gold evidence is surrounded by irrelevant context, PCED benefits from expert isolation."
        },
        {
            "title": "Limitations",
            "content": "Despite its strong empirical performance and efficiency benefits, PCED has several limitations. Dependence on access to model logits. PCED relies on per-expert token-level logits to perform retrieval-aware contrastive decoding, explicitly calibrating contextual experts against the amateur (prior) expert at each decoding step. This requirement assumes full access to the models output logits. As result, PCED cannot be directly applied to closed-source or API-only language models that expose only sampled tokens or log-probabilities for limited subset of candidates. While this constraint is shared by many contrastive and guidance-based decoding methods, it currently restricts the applicability of PCED to open or self-hosted models. Sensitivity to retrieval quality. Like most RAG approaches, PCED depends on the quality of the retrieved documents and their associated relevance scores. If relevant evidence is not retrieved or is assigned low relevance, the corresponding expert may be underweighted or never selected during decoding. Although retrieval-aware contrastive decoding mitigates noise from weak or irrelevant documents, it cannot recover evidence that is entirely absent from the candidate set. That said, our formulation highlights an interesting direction for future work: rather than relying on external retrieval and reranking models, one could explicitly train language models to accept parallel contextual inputs and to learn, at each next token, which input to attend to. Such an approach could reduce reliance on external retrieval pipelines and enable end-toend learning of expert selection and aggregation, enabling parallelization at inference. Storage-Computation Trade-offs. PCED accelerates inference by effectively offloading online computation to offline storage. By persisting precomputed KV caches, the framework eliminates runtime encoding latency; however, this imposes storage footprint that scales linearly with both corpus size and hidden state dimensionality. For instance, storing FP16 KV caches for the LOFT HOTPOTQA corpus (1,222 passages of 74 tokens on average) using LLAMA-3.1-8B necessitates approximately 11.04 GB of storage. Consequently, PCED is optimally deployed in read-heavy, writerare settings involving static corporasuch as enterprise knowledge baseswhere the amortized storage cost is justified by the substantial reduction in query-time latency."
        },
        {
            "title": "References",
            "content": "Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li. 2024. LongBench: bilingual, multitask benchmark for long context understanding. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 31193137, Bangkok, Thailand. Association for Computational Linguistics. Brian J. Chan, Chao-Ting Chen, Jui-Hung Cheng, and Hen-Hsen Huang. 2025. Dont do rag: When cacheaugmented generation is all you need for knowledge tasks. In Companion Proceedings of the ACM on Web Conference 2025, WWW 25, page 893897, New York, NY, USA. Association for Computing Machinery. Yihua Cheng, Yuhan Liu, Jiayi Yao, Yuwei An, Xiaokun Chen, Shaoting Feng, Yuyang Huang, Samuel Shen, Kuntai Du, and Junchen Jiang. 2025. Lmcache: An efficient kv cache layer for enterprise-scale llm inference. arXiv preprint arXiv:2510.09665. Wenqi Fan, Yujuan Ding, Liangbo Ning, Shijie Wang, Hengyun Li, Dawei Yin, Tat-Seng Chua, and Qing Li. 2024. survey on rag meeting llms: Towards retrieval-augmented large language models. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, KDD 24, page 64916501, New York, NY, USA. Association for Computing Machinery. Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yixin Dai, Jiawei Sun, Haofen Wang, and Haofen Wang. 2023. Retrieval-augmented generation for large language models: survey. arXiv preprint arXiv:2312.10997, 2(1). Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad AlDahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, and 1 others. 2024. The llama 3 herd of models. arXiv preprint arXiv:2407.21783. Jonathan Ho and Tim Salimans. 2021. Classifier-free diffusion guidance. In NeurIPS 2021 Workshop on Deep Generative Models and Downstream Applications. Chao Jin, Zili Zhang, Xuanlin Jiang, Fangyue Liu, Shufan Liu, Xuanzhe Liu, and Xin Jin. 2025. Ragcache: Efficient knowledge caching for retrieval-augmented generation. ACM Trans. Comput. Syst., 44(1). Jing Jin, Houfeng Wang, Hao Zhang, Xiaoguang Li, and Zhijiang Guo. 2024. DVD: Dynamic contrastive decoding for knowledge amplification in 5 In Proceedmulti-document question answering. ings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 46244637, Miami, Florida, USA. Association for Computational Linguistics. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. 2023. Efficient memory management for large language model serving with pagedattention. In Proceedings of the 29th Symposium on Operating Systems Principles, SOSP 23, page 611626, New York, NY, USA. Association for Computing Machinery. Jinhyuk Lee, Anthony Chen, Zhuyun Dai, Dheeru Dua, Devendra Singh Sachan, Michael Boratko, Yi Luan, Sébastien M. R. Arnold, Vincent Perot, Siddharth Dalmia, Hexiang Hu, Xudong Lin, Panupong Pasupat, Aida Amini, Jeremy R. Cole, Sebastian Riedel, Iftekhar Naim, Ming-Wei Chang, and Kelvin Guu. 2024. Can long-context language models subsume retrieval, rag, sql, and more? ArXiv, abs/2406.13121. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela. 2020. Retrieval-augmented generation for knowledgeIn Advances in Neural Inforintensive nlp tasks. mation Processing Systems, volume 33, pages 9459 9474. Curran Associates, Inc. Xiang Lisa Li, Ari Holtzman, Daniel Fried, Percy Liang, Jason Eisner, Tatsunori Hashimoto, Luke Zettlemoyer, and Mike Lewis. 2023. Contrastive decoding: Open-ended text generation as optimization. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1228612312, Toronto, Canada. Association for Computational Linguistics. Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. 2024. Lost in the middle: How language models use long contexts. Transactions of the Association for Computational Linguistics, 12:157173. Songshuo Lu, Hua Wang, Yutian Rong, Zhi Chen, and Yaohua Tang. 2025. TurboRAG: Accelerating retrieval-augmented generation with precomputed KV caches for chunked text. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pages 65996612, Suzhou, China. Association for Computational Linguistics. Dongyang Ma, Yan Wang, and Tian Lan. 2025. Blockattention for efficient prefilling. In The Thirteenth International Conference on Learning Representations. Mistral AI. Mistral nemo. Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. 2024. YaRN: Efficient context window extension of large language models. In The Twelfth International Conference on Learning Representations. Nir Ratner, Yoav Levine, Yonatan Belinkov, Ori Ram, Inbal Magar, Omri Abend, Ehud Karpas, Amnon Shashua, Kevin Leyton-Brown, and Yoav Shoham. 2023. Parallel context windows for large language In Proceedings of the 61st Annual Meetmodels. ing of the Association for Computational Linguistics (Volume 1: Long Papers), pages 63836402, Toronto, Canada. Association for Computational Linguistics. Weijia Shi, Xiaochuang Han, Mike Lewis, Yulia Tsvetkov, Luke Zettlemoyer, and Wen-tau Yih. 2024. Trusting your evidence: Hallucinate less with contextaware decoding. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 2: Short Papers), pages 783791, Mexico City, Mexico. Association for Computational Linguistics. Han Wang, Archiki Prasad, Elias Stengel-Eskin, and Mohit Bansal. 2025. AdaCAD: Adaptively decoding to balance conflicts between contextual and parametric knowledge. In Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 1163611652, Albuquerque, New Mexico. Association for Computational Linguistics. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, and 1 others. 2025a. Qwen3 technical report. arXiv preprint arXiv:2505.09388. Jingbo Yang, Bairu Hou, Wei Wei, Yujia Bao, and Shiyu Chang. 2025b. KVLink: Accelerating large language models via efficient KV cache reuse. In The Thirtyninth Annual Conference on Neural Information Processing Systems. Xinyu Yang, Tianqi Chen, and Beidi Chen. 2025c. APE: Faster and longer context-augmented generation via adaptive parallel encoding. In The Thirteenth International Conference on Learning Representations. Jiayi Yao, Hanchen Li, Yuhan Liu, Siddhant Ray, Yihua Cheng, Qizheng Zhang, Kuntai Du, Shan Lu, and Junchen Jiang. 2025. Cacheblend: Fast large language model serving for rag with cached knowledge fusion. In Proceedings of the Twentieth European Conference on Computer Systems, EuroSys 25, page 94109, New York, NY, USA. Association for Computing Machinery. Howard Yen, Tianyu Gao, and Danqi Chen. 2024. Longcontext language modeling with parallel context encoding. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 25882610, Bangkok, Thailand. Association for Computational Linguistics. Yinmin Zhong, Shengyu Liu, Junda Chen, Jianbo Hu, Yibo Zhu, Xuanzhe Liu, Xin Jin, and Hao Zhang. 2024. Distserve: disaggregating prefill and decoding for goodput-optimized large language model serving. In Proceedings of the 18th USENIX Conference on Operating Systems Design and Implementation, OSDI24, USA. USENIX Association. Zihan Zhou, Chong Li, Xinyi Chen, Shuo Wang, Yu Chao, Zhili Li, Haoyu Wang, Qi Shi, Zhixing Tan, Xu Han, Xiaodong Shi, Zhiyuan Liu, and Maosong Sun. 2025. LLMMapReduce: Simplified longsequence processing using large language models. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2766427678, Vienna, Austria. Association for Computational Linguistics."
        },
        {
            "title": "A Evaluation Setup",
            "content": "This appendix details the prompt templates and instantiation protocols for each dataset. To ensure fair comparison across all methods (Concatenation, KV-merge, MapReduce, and PCED), we fix the underlying dataset fields, system prompt, context template, question template, and answer prefix, and vary only the mechanism of context incorporation. All experiments were executed with fixed random seed (42) to ensure deterministic results. Unless otherwise stated, all reported numbers correspond to single deterministic run per method. Prompt Definitions. Each dataset instance is composed of four standardized fields: system_prompt containing high-level instructions; context_template which wraps the retrieved text; and question_template applied to the user query. A.1 LOFT Benchmark We utilize the LOFT benchmark (Lee et al., 2024). Dataset statistics (e.g., number of examples, context lengths, and task distributions) are reported in Table 1 of the original LOFT paper (Lee et al., 2024). A.1.1 LOFT-RAG Templates For RAG tasks, all methods utilize the prompt configuration defined in Figure 4. The {context} slot is populated according to the specific method. A.1.2 LOFT-ICL Templates For In-Context Learning (ICL) tasks, we enforce strict output format to facilitate automated parsing. The templates are defined in Figure 5. System Prompt You will be given list of documents. You need to read carefully and understand all of them. Then you will be given query, and your goal is to answer the query based on the documents you have read. Context Template {context} Question Template Based on the documents above, can you answer the following query? Write concise answer. query: {question} Figure 4: Prompt template configuration for LOFT-RAG tasks. System Prompt Please answer the following questions and ensure you follow consistent format. In particular, ensure your final answer always looks like Output: [your_answer_here] After Output write ONLY the best option following the example(s). Do NOT write anything else. Context Template Example(s): {context} Question Template Now begin! {question} Figure 5: Prompt template configuration for LOFT-ICL tasks. A.2 Method-Specific Instantiations PCED. PCED treats retrieved documents (RAG) or exemplars (ICL) as independent contextual experts. Concretely, for each query we create contextual expert inputs by applying the dataset system_prompt and context_template to documents, yielding separate (system, context) prompt instances. At decoding time, each expert produces logits conditioned on its own KV cache. We additionally include an amateur expert that represents the model prior: it is instantiated using system_prompt only. All experts share the identical question_template. MapReduce. This method involves two-stage process. First, the map stage summarizes individual documents using the fixed instruction: \"Summarize the given documents concisely, focusing on the key points and main ideas.\" The resulting summaries are concatenated into single prompt. In the subsequent reduce stage, the standard dataset templates (Figure 4 or 5) are used, with the concatenated summaries substituting the raw documents in the {context} slot. 7 A.3 LongBench arctan transform: For LongBench (Bai et al., 2024), we strictly adhere to the official task instructions and question templates outlined in the original papers Appendix B. Dataset statistics (e.g., number of examples, context lengths, and task distributions) are reported in Table 1 of the original LongBench paper (Bai et al., 2024). A.4 Synthetic Dataset To benchmark TTFT and end-to-end latency (Figure 3) under controlled context length, we construct small synthetic dataset with fixed formatting and token budgets. Each instance contains =64 documents; exactly one gold document includes secret code string, while the remaining documents are distractors. We enforce an exact document length of 2048 tokens via padding/truncation. The query asks the model to output the secret code verbatim. We include warmup sample to eliminate one-time initialization effects and stabilize latency measurements. rret = clip (cid:18) 2 π arctan(max(sk, 0)), 0, 1 ϵ . (cid:19) (5) This preserves monotonicity while smoothly compressing large sparse scores. Reranker scores (BGE reranker). We use BAAI/bge-reranker-v2-m3 via FlagReranker. With normalize=True, the reranker applies sigmoid to map raw logits to [0, 1]: rrer = σ(zk) = 1 1 + exp(zk) . (6) As above, we clip to [0, 1 ϵ) before using the values in log rk. Score fusion. After normalization, we combine retrieval and reranker signals into single relevance score using the harmonic mean: rrer 2 rret + rrer rret + ϵ In all experiments we set ϵ = 108. rk = (7) ."
        },
        {
            "title": "Reranker Scores",
            "content": "Motivation. PCED uses retrieval and reranker scores as document-level prior (Eq. 2), where the prior enters as log rk. We therefore map all relevance signals to common range rk [0, 1) (and clip away from 0 to avoid log 0). Retrieval scores (BGE-M3). Let sk denote the raw retrieval score produced by bge-m3 for expert under given scoring mode. Different modes have different score ranges, so we normalize as follows: Dense / ColBERT. For the dense and colbert modes, similarity scores are bounded in [1, 1]. We apply an affine rescaling: rret = clip (cid:18) sk + 1 2 (cid:19) , 0, 1 ϵ , (4) which maps [1, 1] (cid:55) [0, 1], followed by clipping to [0, 1 ϵ). Sparse. For the sparse mode, scores are nonnegative and unbounded. Following standard practice for normalizing unbounded similarity/distance values into [0, 1] (e.g., arctan-based normalization used in hybrid reranking), we apply saturating In this section, we provide detailed analysis of the hyperparameters governing PCED. Unless otherwise stated, all ablations are performed using the PCED-Dense variant on the HOTPOTQA and NATURAL QUESTIONS (NQ) datasets, using both LLAMA-3.1-8B-INSTRUCT and MISTRALNEMO-13B-INSTRUCT. C.1 Impact of Contrastive Strength (β) The contrastive strength parameter β determines how aggressively the expert distribution (sk) is sharpened against the amateur prior (s0). We compare our default dynamic β strategy (derived from AdaCAD) against fixed values β {0.25, 0.5, 0.75, 1.0}. Additionally, we evaluate the setting β = 0, which effectively removes the contrastive component and relies solely on the retrieval prior and raw expert logits. Results are presented in Table 4. We observe three key trends: 1. Necessity of Contrastive Decoding (β > 0): Setting β = 0 generally degrades performance compared to the best contrastive settings, confirming that subtracting the amateur logit helps isolate the specific knowledge provided by the retrieved document. 8 2. Instability of Fixed β: While specific fixed values can achieve high scores on individual tasks (e.g., β = 0.25 on Llama-NQ or β = 0.75 on Llama-HotpotQA), they are inconsistent. value that works well for one dataset may fail on another (e.g., β = 0.75 drops significantly on Mistral-HotpotQA compared to lower values). 3. Robustness of Dynamic β: The dynamic strategy consistently delivers competitive performance across all models and datasets without requiring per-task tuning. We therefore select Dynamic as the default to ensure stability across diverse retrieval scenarios. Table 4: Ablation of Contrastive Strength (β). We compare fixed β values against our Dynamic strategy. The column β = 0 represents standard decoding without the contrastive penalty (only retrieval prior). Bold denotes the best result. No CD Fixed β Ours Model Dataset β = 0 0.25 0. 0.75 1.0 Dynamic LLAMA-8B MISTRAL-13B HOTPOTQA NQ HOTPOTQA NQ 53 70 65 80 65 88 62 83 61 65 62 67 84 58 78 59 62 54 80 64 85 66 C.2 Sensitivity to Retrieval Prior (γ) The parameter γ controls the influence of the retrieval/reranker scores on expert selection via the term γ log rk. We perform sweep over γ {0.5, 1.0, 1.5, 2.0, 3.0, 4.0} and compare these with our chosen default γ = 2.5. Results are shown in Table 5. We observe the following: Under-weighting (γ < 1.5): Lower values often degrade performance (e.g., Llama-NQ drops significantly to 75 at γ = 0.5). This confirms that expert selection cannot rely on internal perplexity alone; strong external relevance signals are necessary to suppress distractors. Over-weighting (γ 4.0): High values yield inconsistent results. While Llama-NQ peaks at γ = 4.0 (87), Llama-HotpotQA degrades compared to lower values (64 vs 66). Excessive gating forces the model to rigidly follow the retrievers ranking, potentially overriding valid reasoning from lower-ranked experts on complex queries. γ = 2.5: The range γ [2.0, 3.0] represents stable \"sweet spot\" across both models and datasets. We select γ = 2.5 as the default because it offers the best trade-off: it maximizes performance on difficult tasks like NQ (matching the high scores of γ = 2.0 3.0) while avoiding the instability seen at the extremes. Table 5: Sensitivity sweep for Retrieval Prior weight (γ). We use Dynamic β for all runs. The main paper uses γ = 2.5. Gamma (γ)"
        },
        {
            "title": "Dataset",
            "content": "0.5 1.0 1.5 2.0 3.0 4.0 2.5 LLAMA-8B HOTPOTQA 65 66 66 65 63 64 75 84 86 85 85 87 NQ MISTRAL-13B HOTPOTQA 64 65 66 65 67 66 78 79 80 81 81 81 NQ 64 85 66 81 C.3 Contrastive Signal vs. Retrieval Score"
        },
        {
            "title": "Only",
            "content": "Finally, we isolate the contribution of the two core components of Equation 2: the contrastive signal (β) and the retrieval prior (γ). Table 6 compares the full PCED method against two ablations: 1. Only Retrieval Scores (β = 0): Expert logits are boosted by retrieval scores but not calibrated against the amateur. 2. Only Contrastive (γ = 0): Expert logits are calibrated via contrastive decoding, but all experts are treated as equally likely (flat prior), ignoring retrieval ranking. The results reveal two distinct findings: Retrieval Prior is Foundational (γ > 0): The \"Only Contrastive\" setting fails catastrophically across all benchmarks (e.g., Llama NQ drops to 52). This confirms that without the external guidance of the retriever to gate irrelevant experts, the model is overwhelmed by noise from distractors. Contrastive Signal is an Amplifier (β > 0): The impact of the contrastive term is modeldependent. For LLAMA-3.1, it is critical: removing it (\"Only Retrieval\") causes massive drop (e.g., NQ falls from 85 to 70), suggesting that Llama requires the amateur subtraction to suppress its own priors and hallucinations. Conversely, MISTRAL is more robust, 9 Figure 6: Performance Stability across Top-k. PCED maintains consistent accuracy from = 8 to 128, confirming that the retrieval prior effectively suppresses noise from additional distractors. C.5 Robustness to Candidate Pool Size (k) We evaluate the stability of PCED (Dense, Llama3.1-8B) as we scale the number of retrieved experts from = 8 to = 128. Results are visualized in Figure 6. We observe two trends: Noise Tolerance: Performance remains nearly constant across all datasets despite 16 increase in experts. For instance, NQ scores stay flat at 85, while HOTPOTQA fluctuates only marginally (6365). This confirms that the retrieval prior (γ log rk) effectively gates low-relevance experts, preventing distractor accumulation. Recall without Penalty: While low is often sufficient, the lack of degradation at = 128 allows users to maximize recall for difficult queries without sacrificing generation quality. achieving strong performance with retrieval scores alone, though the full PCED framework still secures the highest absolute scores in all cases. Table 6: Component Analysis. We disentangle the benefits of the Contrastive Decoding signal versus the Retrieval Prior. Only Contrastive Only Retrieval (No Prior, γ = 0) Full (No CD, β = 0) PCED LLAMA-8B MISTRAL-13B HOTPOTQA NQ HOTPOTQA NQ 46 52 57 71 53 70 65 80 64 85 66 C.4 Ablation of Expert Aggregation Rule PCED aggregates experts via token-wise Max operation. We compare this against two probabilityspace alternatives: Mixture-of-Experts (MoE, weighted sum) and Product-of-Experts (PoE, weighted product), where weights are derived from retrieval scores. Table 7 shows that Max aggregation is critical for multi-hop reasoning (HOTPOTQA), outperforming MoE by 8 points (64 vs. 56). We hypothesize that Max enables sharper token-level expert switching, allowing different documents to dominate different generation steps without their distributions needing to agree. Conversely, on singledocument tasks like NQ, MoE performs slightly better (87 vs. 85), suggesting that soft averaging can be beneficial when evidence is concentrated in one expert and retrieval priors are accurate. Table 7: Aggregation Rule Ablation. Comparison of PCED (Max) vs. probabilistic aggregation (MoE, PoE)."
        },
        {
            "title": "HOTPOTQA NQ",
            "content": "Max (Ours) Mixture (MoE) Product (PoE) 64 56 46 85 87"
        }
    ],
    "affiliations": [
        "EURECOM, France",
        "SAP Labs, France"
    ]
}