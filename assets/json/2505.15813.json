{
    "paper_title": "Meta-Learning an In-Context Transformer Model of Human Higher Visual Cortex",
    "authors": [
        "Muquan Yu",
        "Mu Nan",
        "Hossein Adeli",
        "Jacob S. Prince",
        "John A. Pyles",
        "Leila Wehbe",
        "Margaret M. Henderson",
        "Michael J. Tarr",
        "Andrew F. Luo"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Understanding functional representations within higher visual cortex is a fundamental question in computational neuroscience. While artificial neural networks pretrained on large-scale datasets exhibit striking representational alignment with human neural responses, learning image-computable models of visual cortex relies on individual-level, large-scale fMRI datasets. The necessity for expensive, time-intensive, and often impractical data acquisition limits the generalizability of encoders to new subjects and stimuli. BraInCoRL uses in-context learning to predict voxelwise neural responses from few-shot examples without any additional finetuning for novel subjects and stimuli. We leverage a transformer architecture that can flexibly condition on a variable number of in-context image stimuli, learning an inductive bias over multiple subjects. During training, we explicitly optimize the model for in-context learning. By jointly conditioning on image features and voxel activations, our model learns to directly generate better performing voxelwise models of higher visual cortex. We demonstrate that BraInCoRL consistently outperforms existing voxelwise encoder designs in a low-data regime when evaluated on entirely novel images, while also exhibiting strong test-time scaling behavior. The model also generalizes to an entirely new visual fMRI dataset, which uses different subjects and fMRI data acquisition parameters. Further, BraInCoRL facilitates better interpretability of neural signals in higher visual cortex by attending to semantically relevant stimuli. Finally, we show that our framework enables interpretable mappings from natural language queries to voxel selectivity."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 2 ] . [ 1 3 1 8 5 1 . 5 0 5 2 : r Meta-Learning an In-Context Transformer Model of Human Higher Visual Cortex Muquan Yu1,2 Mu Nan1 Hossein Adeli3 Jacob S. Prince4 John A. Pyles5 Leila Wehbe Margaret M. Henderson6 Michael J. Tarr6 Andrew F. Luo1 1 University of Hong Kong 4 Harvard University 2 Chinese University of Hong Kong 5 University of Washington 3 Columbia University 6 Carnegie Mellon University mqyu@link.cuhk.edu.hk Corresponding author: aluo@hku.hk"
        },
        {
            "title": "Abstract",
            "content": "Understanding functional representations within higher visual cortex is fundamental question in computational neuroscience. While artificial neural networks pretrained on large-scale datasets exhibit striking representational alignment with human neural responses, learning image-computable models of visual cortex relies on individual-level, large-scale fMRI datasets. The necessity for expensive, time-intensive, and often impractical data acquisition limits the generalizability of encoders to new subjects and stimuli. BraInCoRL uses incontext learning to predict voxelwise neural responses from few-shot examples without any additional finetuning for novel subjects and stimuli. We leverage transformer architecture that can flexibly condition on variable number of incontext image stimuli, learning an inductive bias over multiple subjects. During training, we explicitly optimize the model for in-context learning. By jointly conditioning on image features and voxel activations, our model learns to directly generate better performing voxelwise models of higher visual cortex. We demonstrate that BraInCoRL consistently outperforms existing voxelwise encoder designs in low-data regime when evaluated on entirely novel images, while also exhibiting strong test-time scaling behavior. The model also generalizes to an entirely new visual fMRI dataset, which uses different subjects and fMRI data acquisition parameters. Further, BraInCoRL facilitates better interpretability of neural signals in higher visual cortex by attending to semantically relevant stimuli. Finally, we show that our framework enables interpretable mappings from natural language queries to voxel selectivity."
        },
        {
            "title": "Introduction",
            "content": "Human visual cortex transforms raw sensory input into behaviorally-relevant representations of the world. While early visual areas are characterized by retinotopic organization and selective tuning to simple features such as edges and orientation gradients [1, 2, 3, 4, 5], higher-order visual areas demonstrate selectivity to more abstract semantics and categories. While this functional organization is largely consistent across individuals at coarse scale, the spatial distribution and finegrained semantic selectivity within visual cortex varies due to structural differences, developmental experience, and life-long learning [6, 7, 8, 9, 10, 11]. Such functional inter-subject differences pose fundamental challenge in constructing generalizable models of higher visual cortex that can adapt to subject-specific neural organization without exhaustive data collection for every individual. Preprint. Under review. Figure 1: BraInCoRL: Meta-Learning an In-Context Visual Cortex Encoder. (a) The voxelwise brain encoding problem. For each voxel, there is response function that maps from visual stimuli to voxel activation. In practice, we can only observe the noisy measurements from fMRI. The goal is to infer an image-computable function for each voxel to predict its activation. (b) BraInCoRL treats each voxel as meta-learning task, and samples (image, response) pairs from multiple subjects. During testing, the model is conditioned on small number of novel images and measurements from new subject and directly outputs the function parameters. (c) From left to right, the explained variance from the full model trained on 9,000 images from one subject, BraInCoRL with only 100 in-context images from the new subject, and baseline ridge regression also with 100 images (for this baseline, voxelwise regularization is determined using 5-way cross-validation). Our method achieves much higher data efficiency than baseline. (d) Explained variance as function of in-context support set size. As the in-context support set size increases from 0 to 1,000, BraInCoRL steadily improves and approaches the fully trained reference model fit to converge on each subjects full 9,000-image training set, demonstrating high prediction accuracy and data efficiency. Recent advances in deep learning offer promising avenue for addressing this challenge. Vision models pretrained on large-scale image datasets not only achieve strong object recognition performance, but also recapitulate hierarchical processing patterns observed in biological vision [12, 13, 14]. While these models may encapsulate some universal principles of visual processing [15], they do not inherently account for individual differences in cortical organization. To close the gap between artificial and biological systems, researchers have developed image-computable fMRI encoders models that predict brain activity from visual stimuli [16]. These encoders typically regress image features onto voxelwise brain responses using subject-specific data, acting as computational probes of visual processing. Unfortunately, current approaches require many hours of costly fMRI scans per subject to fit these mappings prohibitive bottleneck for scalability to new populations, stimuli, and tasks, especially in clinical settings where collecting large amounts of data is difficult. We bridge this gap with BraInCoRL (Brain In-Context Representation Learning), transformerbased framework that meta-learns to predict subject-specific neural responses from provided examples. Inspired by language models that adapt to new tasks in-context, our approach treats voxel encoding as function inference problem: given handful of stimulus-response pairs from new individual and novel stimuli, BraInCoRL constructs voxelwise encoding model without any further training. By jointly optimizing for in-context learning across diverse subjects and stimuli, our model discovers shared functional principles of higher visual cortex that generalize to new subjects and stimuli represented by only small amount of data. We illustrate the problem we are tackling in Figure 1. We demonstrate that BraInCoRL: (1) Outperforms existing voxelwise encoder models in the low-data regime on novel visual stimuli while exhibiting strong generalization with increasing context. (2) Can generalize to new experiments with different scanning parameters. (3) Through analysis of attention values, learns to rely on images that are reflective of the category selected for in each region. (4) When paired with features from contrastive image-language models, facilitates zero-shot natural languagebased characterization of cortical selectivity, enabling interpretable, finer-grained query-driven functional mapping. 2 Figure 2: Architecture of the In-Context Voxelwise Encoder (BraInCoRL). (1) pretrained feature extractor converts visual stimuli into vector embeddings. (2) higher visual cortex transformer integrates these embeddings with voxel activations to learn context-specific features and generates hyperweights for subsequent voxelwise encoder backbone. (3) The voxelwise encoder, conditioned on the hyperweights, predicts voxel responses for novel stimuli."
        },
        {
            "title": "2 Related work",
            "content": "Computational Encoding and Decoding Models for Visual Cortex. Computational modeling of neural data often involves two complementary approaches: encoding models that map from stimuli to neural activations, and decoding models that map from neural data to stimuli [16, 17, 18, 19, 20, 21, 22, 23, 24]. The development of both approaches has been facilitated by advances in machine learning models. For encoding models, the dominant approach is to combine pretrained deep feature extractors with linear voxelwise weights [25, 26, 27, 28, 29, 30]. More recent approaches have proposed to leverage transformers [31, 32] to learn the relationship between brain regions of single subject. Most similar to our framework is the pioneering work by Adeli et al. [31] and Beliy & Wasserman et al. [33] which uses an auto-decoder based transformer network for multi-subject voxelwise encoding; However these approaches still require fine-tuning for novel subjects. More generally, encoders have been used to investigate the coding properties in higher-order visual areas [34, 35, 36, 37, 38, 39, 40, 41]. Encoders have been further combined with imagegeneration models [42, 43, 44, 45, 46, 47, 48, 49] or language-generation models [50, 51] to explore semantic selectivity. Recent progress on large generative models has enabled stimulus reconstruction from fMRI, EEG, and MEG signals for images [52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63], videos [64, 65, 66, 67, 68, 69, 70], and language/audio [71, 72, 73, 74, 75, 76, 77]. Representational Organization of Visual Cortex. Human visual cortex exhibits hierarchical organization from primary visual to higher-order visual areas. The higher visual cortex is characterized by tiling of semantically specialization. Approaches using functional localizers have identified category selective regions in higher visual that are responsive to faces [78, 79, 80, 81], places [82, 83], bodies [84], objects [85, 86], food [87, 88, 89], and words [90, 91]. While the spatial location of these broad category-selective regions are generally consistent across subjects [92], significant inter-individual variability exists in their anatomical location, spatial extent, and response profiles [6, 7, 8, 9, 93, 94, 95, 96, 97]. Accurately characterizing visual processing in higher-order visual areas necessitates subject-specific encoding models that capture individual diversity [98]. Meta-Learning and In-Context Learning. Our work builds upon meta-learning and in-context learning (ICL). Meta-learning trains models to \"learn to learn\" from distribution of tasks, enabling quick adaptation to new tasks with few examples, via methods like meta-optimization [99, 100, 101] or metric-based approaches [102]. More recently, ICL has emerged as powerful capability in Large Language Models [103, 104], where models adapt to new tasks at inference time solely based on examples provided in their prompt without any parameter updates [105, 106]. This has led to hypotheses that ICL is an emergent form of implicit meta-learning, where transformers effectively learn underlying learning algorithms during pre-training [107, 108]. Our goal is to learn the structure of functions that map between visual stimuli and voxelwise brain response. Our framework combines meta-training (across voxels and subjects) and in-context learning (across stimuli) to enable training free adaptation to novel subjects."
        },
        {
            "title": "3 Methods",
            "content": "Our proposed framework leverages meta-learning and uses few-shot, in-context examples for voxelwise prediction of unseen stimuli (Figure 2). Critically, for unseen subjects, this approach does not require any additional finetuning. We achieve this by treating the mapping function from visual stimuli and the response of individual voxels as set of meta-training tasks. This voxelwise approach is in line with higher-order visual areas being described by multitude of functionally diverse voxels, which we randomly sample during training."
        },
        {
            "title": "3.1 Motivation and Problem Definition",
            "content": "There is substantial inter-subject anatomical and functional variability in the higher visual cortex among humans. Consequently, while one can learn per-subject image-computable encoders that map image features to brain responses with high predictive accuracy, these models require large amounts of within-subject data and do not exploit information across subjects. To account for this variability across individuals, we design our framework to treat individual voxels as the fundamental unit of modeling. Importantly, our method does not assume any overlap in stimuli across subjects, yet still enables us to take advantage of multi-subject training data. We formalize this problem by assuming an image is represented as RGB tensor Ij RHW 3. Given an image and human subject k, there is 1D array of voxel activations (beta values) from higher visual cortex: (β1, β2, ..., βNk )j = Bj,k R1Nk , where the number of voxels will differ between subjects. Given new subject not seen during training, we have small set of seen images (I1, I2, ..., In) and measured brain responses (B1, B2, ..., Bn) for this new subject. Our goal is to estimate the brain response to an arbitrary new image Inovel."
        },
        {
            "title": "3.2 Meta-Learning an In-Context Transformer",
            "content": "Image-computable encoders that map from images to brain responses for single subject are typically considered as function fk(I) B, and jointly model the entire visual cortex. While powerful, this approach cannot be easily extended to the multi-subject scenario, where test-time individuals may have functional and anatomical differences that are not known during training. In contrast, BraInCoRL considers each voxel to have unique and unobserved visual response function fk,v(I) βv. Voxels can be from many different subjects. During training, we consider each voxels response function to be meta-training task, where each task is effectively specified by input images and voxel response pairs. In order to facilitate training-free adaptation on new subjects, we utilize in-context learning across stimuli enabled by transformer backbone. set of images and neural For single voxel we define support responses {(x1, β1), (x2, β2), ..., (xp, βp)}, where xi Rm is the image embedding vector extracted by frozen image feature extractor ϕ(), i.e., xi = ϕ(Ii), and βi is the voxels response observed for image Ii. Each pair is concatenated to form context tokens ci = [xi; vi], and the full context is defined as {c1, . . . , cp}. Unlike traditional in-context inference in large language models, where there is query concatenated to the end of the context, we avoid expensive per-voxel inference by directly generating the parameters for the voxelwise visual response function. During training, we optimize the BraInCoRL transformer with parameters θ such that it outputs voxel response function with parameters ω: ω = Tθ(c1, c2, . . . , cp) ˆβ = fω(I) (1) (2) Since and are differentiable, we optimize θ to maximize the likelihood of observing β given I: (3) E(Iq,βq)fω(I) βTrue2 2 θ = arg min θ In practice, we use mean-squared-error and gradient based mini-batch optimization."
        },
        {
            "title": "3.3 Test-time Context Scaling",
            "content": "At test time, when we encounter new subject, we assume we have access to small set of novel images and the corresponding brain responses we want to predict voxelwise encoder. While 4 Figure 3: Evaluation on NSD. (a) Prediction explained variance of BraInCoRL improves on novel subjects with larger in-context support set size, outperforming within-subject ridge regression and approaching the fully trained reference model fit on each subjects full 9,000-image training set, using far less data. (b) Ablation (100 support images) comparing BraInCoRL variants: the original model trained while holding out the novel subjects 9,000 test-time support images (HO), BraInCoRL model trained without this holdout (no HO), and pretraining-only BraInCoRL model, alongside the within-subject ridge baseline. Results show that finetuning with real fMRI data improves performance, and holding out the test subjects image data does not hinder generalization. (c) Voxelwise explained variance from BraInCoRL (100 images) is strongly correlated with fully trained reference models across different visual encoder backbones. our voxelwise parameterization successfully resolves the challenge of unknown test-time anatomy and geometry, the challenge of unknown test-time context size remains. Unlike transformers in language models, where the output is dependent on the order of the samples, we want our model to be explicitly invariant to input order. In order to facilitate variable length context, we utilize logit scaling [109, 110, 111]. Assuming query/key (q, k) with dk features and length context: αorig = dk ; αscaled = log (c) dk (4) We find this method effectively enables context scaling when trained with variable length context. While the hypernetwork could, in principle, parameterize any class of neural encoders (e.g., MLPs, convolution, attention layers), prior studies utilizing brain data have largely used linear parameterizations that map from deep network features to voxel responses [13, 14], and find that such choice offers high performance and interpretability. Given features R1q from pretrained neural network = ϕ(I), we adopt the same setup and predict the final voxel response: ˆβ = (ϕ(I); ω) = matmul(x, ω) (5)"
        },
        {
            "title": "4 Experiments",
            "content": "We utilize BraInCoRL to generate encoder weights in low-data regime. We start by describing our experiment setup. First, we evaluate the effectiveness of BraInCoRL on novel subjects where there is zero overlap between the training dataset and the evaluated subjects in-context stimulus. We also 5 Table 1: Voxelwise performance across five category-selective regions. Explained variance is shown for our in-context model (BraInCoRL) that uses just 100 in-context images, the fully trained reference model fit to converge on each subjects full 9,000-image training set (Fully Trained), and within-subject ridge regression baselines (100, 300 within-subject test images), plus the FsAverage map averages over other subjects. Our model outperforms both subject-wise and anatomical baselines, and demonstrates strong data-efficiency."
        },
        {
            "title": "Fully Trained",
            "content": "Ridge-100 Ridge-300 FsAverage map BraInCoRL-100 S1 0.19 0.10 0.13 0.13 0. S2 0.16 0.07 0.10 0.06 0.13 S1 0. 0.08 0.13 0.11 0.16 S2 0.27 0.14 0.20 0.19 0. S1 0.28 0.16 0.22 0.09 0.25 S2 0. 0.12 0.16 0.08 0.21 S1 0.11 0.02 0.06 0.06 0. S2 0.11 0.03 0.06 0.03 0.08 S1 0. 0.05 0.10 0.14 0.12 S2 0.17 0.07 0.11 0.18 0. S1 0.18 0.07 0.11 0.08 0.13 S2 0. 0.08 0.12 0.06 0.15 evaluate our framework where data from novel subjects are collected from completely different scanner and protocol. Second, we explore the attention pattern across stimuli for different ROIs, and perform ablations to explore the need for test-time in-context stimulus diversity. Third, we show that our method enables natural language characterizations of the visual cortex using very little data."
        },
        {
            "title": "4.1 Setup",
            "content": "Dataset. We primarily perform experiments with the Natural Scenes Dataset (NSD) [112], but then validate with the BOLD5000 dataset [113]. Both are large-scale neural datasets: NSD is the largest 7T fMRI image viewing dataset available, where eight subjects each viewed 10, 000 images; BOLD5000 is 3T dataset, where four subjects each viewed 5000 images. In NSD each image was viewed up to three times, while in BOLD5000 only subset of images were viewed up to four times. For NSD, of the eight subjects, four subjects (S1, S2, S5, S7) completed scanning and are the focus of our analysis in the main paper. The results of other subjects are presented in the supplemental materials. For each subject, 9, 000 images are unique to each other, while 1, 000 are viewed by all eight subjects. The NSD stimuli were sourced from the MS COCO dataset (as were subset of the BOLD5000 stimuli). Unless otherwise noted, we perform our analysis in subject native volume space (func1pt8mm) for NSD, where the voxelwise betas are z-scored within each session then averaged across repeats of the same stimulus. In order to rigorously evaluate BraInCoRL for given subject, we use the 3 9,000 unique images viewed by the other three subjects as the meta-training data. During the ROI-wise evaluation for NSD, we follow prior work [48] and apply t-statistic cutoff of > 2 by leveraging independent functional localizer data provided with the dataset to threshold the originally broad definitions. During quantitative evaluation, we follow prior work [114] and apply voxel quality cutoff of ncsnr > 0.2. For BOLD5000, we use model trained on four NSD subjects (S1, S2, S5, S7). Following the suggestion of BOLD5000 authors, we only model stimuli with 4 repeats and apply cutoff of ncsnr > 0.3. Voxel responses are averaged across repeats. Training and evaluation. Our training process takes inspiration from LLM based training setups, and we adopt three stage process pretraining, context extension, and supervised fine tuning. In the pretraining stage, we use an analysis-by-synthesis approach without relying on any (real) subject data. We artificially construct large number of voxels with synthesized weights. We derive synthetic voxel responses with normally distributed noise using these synthesized weights and train our model using fixed context size of 500. In the second stage, we randomly sample the context size from Uniform(30, 500) which allows the model to acquire length robustness. Finally, in the finetuning stage, the model is trained on real fMRI data using the subject-specific beta values, enabling adaptation to biologically grounded neural responses. All our evaluation experiments are performed on novel subjects that are unseen by the model during training, with exception of (Figure 3b) no heldout (no HO) ablation study."
        },
        {
            "title": "4.2 Effectiveness of In-Context Higher Visual Cortex Encoders",
            "content": "On NSD. In this experiment, we evaluated BraInCoRL with each subjects 9,000 unique images as incontext support set and the 1,000 shared images as test set, reporting explained variance averaged over 6 Figure 4: UMAP visualization of predicted response weights. We apply UMAP to BraInCoRL -predicted voxelwise weights (100 support images) and show: (a) flatmap for S1 with ROI outlines, (b) the same projection on an inflated surface, and (c) flatmaps for S2, S5, and S7. Color-coded clusters align with body/face regions (EBA, FFA/aTL-faces), place regions (RSC, OPA, PPA), and food regions (in red). Figure 5: Evaluation on BOLD5000. We evaluate BraInCoRL on the BOLD5000 dataset, which was collected using different scanner than NSD. For varying in-context support set sizes, we report voxelwise Pearson correlation between predicted and true responses for both BraInCoRL and within-subject ridge regression. BraInCoRL achieves higher accuracy and greater data efficiency. category-selective ROIs in Table 1. We compare BraInCoRL using just 100 test images against withinsubject ridge regression baseline trained on 100 and 300 support-set images of the test-subject, with the regularization strength selected via 5-fold cross-validation over [103, 102, . . . , 108]. Remarkably, BraInCoRL with only 100 images nearly matches the performance of the fully supervised reference model that is trained by gradient descent on each subjects entire within-subject support-set of 9,000 images until convergence. We also evaluate an anatomical FsAverage baseline which aligns each training subjects anatomy to common template and projects the average response onto novel subjects for prediction. While this baseline benefits from strong anatomical prior, it is outperformed by BraInCoRL, which directly adapts to each subjects unique neural responses with higher efficiency. To evaluate test-time behavior, we assess how performance scales with increasing in-context support size. BraInCoRL consistently outperforms within-subject ridge regression and more efficiently approaches the fully trained reference model (Figure 1c for subject 1 and Figure 3a for subject 2, 5, 7). Moreover, we conduct ablations by evaluating BraInCoRL model trained without holding out the test subjects support images and BraInCoRL model with only pretraining. Results confirm 7 Figure 6: Top contributing support images for each category-selective region in S1. For each of the five category-selective regions, we select the in-context support images with the highest attention weights in BraInCoRLs final attention layer for voxels in that region. We visualize the top 5 contributing images for the place, word, face and body regions, and the top 10 for the food region. Figure 7: Impact of support-set specificity on category-selective ROI encoding performance on NSD. We construct in-context support sets of 100 images based on descending semantic relevance for each ROI (tiers: 1100, 101200, 201300, 301400) and compare them with randomly sampled sets of equal size. Mean explained variance in the target category-selective ROIs increases as semantic specificity decreases, with all curated sets performing worse than random sampling. This suggests that overly specific support sets hinder generalization in voxelwise encoding. This pattern echoes prior findings on diverse stimuli contributing to better encoders [13]. that finetuning with real neural data boosts performance and that BraInCoRL can generalize well to previously unseen images without overfitting (Figure 3b). Additionally, we observe high voxelwise explained variance correlation between BraInCoRL and the fully trained reference model across multiple backbones (Figure 3c). Finally, we apply UMAP to the BraInCoRL predicted responsefunction weights, revealing clear semantic clustering across higher visual areas (Figure 4) that correspond with known visual regions. On BOLD5000. We validate generalization on the BOLD5000 dataset in Figure 5. BOLD5000 has many differences with NSD and represents the challenge of cross-site generalization that is the main objective of our method. BOLD5000 was collected on 3T scanner with different stimulus presentation time, slow-event related trial structure (10s inter-trial interval), different images and image datasets, different voxel size (2mm isotropic), and different subjects. BraInCoRL achieves higher voxelwise Pearson correlations than within-subject ridge regression. Moreover, results remain consistent across different subjects, demonstrating the robustness and reliability of our method."
        },
        {
            "title": "4.3 Semantic Discovery through Text–Image Embedding Alignment",
            "content": "To better understand how BraInCoRL leverages in-context examples, we analyze its internal attention mechanisms to identify images that strongly influence voxel predictions in category-selective regions. In Figure 6, we examine attention weights from BraInCoRLs final attention layer to determine the top-contributing images for each cortical region. The visualized images with the highest attention scores closely align with known semantic preferences of the respective cortical regions. 8 Figure 8: Predicting cortical responses from natural language prompts. For each semantic category, we convert natural language prompt into CLIP text embedding, project it into the image feature space, and use BraInCoRL to predict the corresponding voxel activation map. The predicted activations align closely with true t-statistic of category-selective regions (derived from fMRI functional localizer experiments), illustrating the potential for efficient, language-driven functional mapping of visual cortex. Table 2: Voxelwise prompt classification accuracy. Each cell shows the percentage of voxels in given category selective region (columns) whose peak predicted activation was elicited by specific semantic prompt (rows, see Appendix). Using only 100 support images, BraInCoRL effectively localizes category-selective regions with high data efficiency."
        },
        {
            "title": "Words",
            "content": "S1 0.63 0.30 0.02 0.04 0.01 S2 0.54 0.25 0.09 0.10 0.03 S1 0.30 0.60 0.02 0.08 0. S2 0.16 0.56 0.05 0.18 0.04 S1 0.05 0.05 0.81 0.08 0.01 S2 0.03 0.01 0.88 0.06 0. S1 0.15 0.07 0.10 0.66 0.02 S2 0.19 0.04 0.07 0.64 0.05 S1 0.43 0.15 0.05 0.31 0. S2 0.17 0.16 0.10 0.45 0."
        },
        {
            "title": "Bodies\nFaces\nPlaces\nFood\nWords",
            "content": "However, Figure 7 reveals counterintuitive finding regarding the semantic specificity of in-context support sets. We systematically vary the specificity of the 100-image sets provided to the model, ranging from highly relevant to random selections. Selections are determined via the first text-prompt in each category (see Appendix). We observe that randomly selected images lead to better predictive performance compared to sets composed solely of highly relevant images. This suggests that overly specific context sets may limit the generalization capabilities of the encoder system, and diverse, less semantically constrained images provide richer context for learning robust voxel representations."
        },
        {
            "title": "4.4 Characterizing higher visual cortex with text embeddings to images",
            "content": "In this experiment, we investigate the capability of BraInCoRL to enable interpretable, query-driven functional mapping using natural language prompts. In Figure 8, we demonstrate that natural language prompts can be effectively mapped to predicted voxel activations. For each category selective region, we convert the corresponding natural language prompt into CLIP embedding and project it into the image feature space to directly predict voxel activations. The resulting activation maps closely match expected t-statistics, reflecting BraInCoRLs ability to support intuitive, language-driven cortical queries. In the second analysis  (Table 2)  , we quantitatively assess the accuracy of prompt-driven activation predictions. We measure the fraction of voxels within each category-selective region whose peak predicted activation aligns with the category indicated by the natural language query. Results confirm that BraInCoRL paired with language embeddings achieves high level of alignment between predicted voxel selectivity and query semantics across multiple subjects. The predictions for word-selective voxels were notably less accurate. We hypothesize this discrepancy arises from the developmental and experiential variability inherent to the formation of word-selective regions, as 9 these areas form predominantly through individualized learning experiences, such as reading and linguistic exposure, leading to greater inter-subject variability."
        },
        {
            "title": "5 Discussion",
            "content": "Limitations and Future Work. Here we have shown that meta-learning an in-context model of higher visual cortex can yield high performance and strong data efficiency gains, outperforming anatomical (FsAverage) and subject-wise baselines on novel subjects. Our work currently focuses on static natural images, and extensions to dynamic stimuli would likely require rethinking of the encoder backbone and network structure. Further, while we show strong generalization across scanners and utilize the largest fMRI dataset that is NSD, there may still be limitations in dataset diversity [115]. Collection of larger and more diverse fMRI datasets will help mitigate this issue. Conclusion. We introduce foundation model that serves as an fMRI encoder, mapping from natural images to voxelwise activations. We demonstrate that our method can adapt without any finetuning to new stimuli, subjects, scanners, and scanning protocols. Our model achieves this by meta-learning across voxels from different subjects, and performing in-context learning across stimuli. Our approach has significant data-efficiency and performance gains over baseline methods, and has the potential to help understand cortical structure in data-constrained environments."
        },
        {
            "title": "References",
            "content": "[1] Stephen Kuffler. Discharge patterns and functional organization of mammalian retina. Journal of neurophysiology, 16(1):3768, 1953. [2] David Hubel and Torsten Wiesel. Receptive fields, binocular interaction and functional architecture in the cats visual cortex. The Journal of physiology, 160(1):106, 1962. [3] Joseph Atick and Norman Redlich. What does the retina know about natural scenes? Neural computation, 4(2):196210, 1992. [4] Jan Verweij, Eric Hornstein, and Julie Schnapf. Surround antagonism in macaque cone photoreceptors. Journal of Neuroscience, 23(32):1024910257, 2003. [5] Kalanit Grill-Spector and Rafael Malach. The human visual cortex. Annu. Rev. Neurosci., 27 (1):649677, 2004. [6] Michael Tarr and Isabel Gauthier. FFA: flexible fusiform area for subordinate-level visual processing automatized by expertise. Nature neuroscience, 3(8):764769, 2000. [7] Isabel Gauthier, Pawel Skudlarski, John Gore, and Adam Anderson. Expertise for cars and birds recruits brain areas involved in face recognition. Nature neuroscience, 3(2):191197, 2000. [8] Roel Willems, Marius Peelen, and Peter Hagoort. Cerebral lateralization of face-selective and body-selective visual areas depends on handedness. Cerebral cortex, 20(7):17191725, 2010. [9] Qing Cai, Lise Van der Haegen, and Marc Brysbaert. Complementary hemispheric specialization for language production and visuospatial attention. Proceedings of the National Academy of Sciences, 110(4):E322E330, 2013. [10] Philippe Pinel, Christophe Lalanne, Thomas Bourgeron, Fabien Fauchereau, Cyril Poupon, Eric Artiges, Denis Le Bihan, Ghislaine Dehaene-Lambertz, and Stanislas Dehaene. Genetic and environmental influences on the visual word form and fusiform face areas. Cerebral Cortex, 25(9):24782493, 2015. [11] Zeynep Saygin, David Osher, Elizabeth Norton, Deanna Youssoufian, Sara Beach, Jenelle Feather, Nadine Gaab, John DE Gabrieli, and Nancy Kanwisher. Connectivity precedes function in the development of the visual word form area. Nature neuroscience, 19 (9):12501255, 2016. [12] Daniel LK Yamins, Ha Hong, Charles Cadieu, Ethan Solomon, Darren Seibert, and James DiCarlo. Performance-optimized hierarchical models predict neural responses in higher visual cortex. Proceedings of the national academy of sciences, 111(23):86198624, 2014. [13] Aria Yuan Wang, Kendrick Kay, Thomas Naselaris, Michael Tarr, and Leila Wehbe. Better models of human high-level visual cortex emerge from natural language supervision with large and diverse dataset. Nat Mach Intell, 5:14151426, 2023. [14] Colin Conwell, Jacob Prince, Kendrick Kay, George Alvarez, and Talia Konkle. What can 1.8 billion regressions tell us about the pressures shaping high-level visual representation in brains and machines? BioRxiv, pages 202203, 2022. [15] Zirui Chen and Michael F. Bonner. Universal dimensions of visual representation, 2024. [16] Thomas Naselaris, Kendrick Kay, Shinji Nishimoto, and Jack Gallant. Encoding and decoding in fmri. Neuroimage, 56(2):400410, 2011. [17] Yukiyasu Kamitani and Frank Tong. Decoding the visual and subjective contents of the human brain. Nature neuroscience, 8(5):679685, 2005. [18] Kenneth Norman, Sean Polyn, Greg Detre, and James Haxby. Beyond mind-reading: multi-voxel pattern analysis of fmri data. Trends in cognitive sciences, 10(9):424430, 2006. 11 [19] Kuan Han, Haiguang Wen, Junxing Shi, Kun-Han Lu, Yizhen Zhang, Di Fu, and Zhongming Liu. Variational autoencoder: An unsupervised model for encoding and decoding fmri activity in visual cortex. NeuroImage, 198:125136, 2019. [20] Katja Seeliger, Umut Güçlü, Luca Ambrogioni, Yagmur Güçlütürk, and Marcel AJ van Gerven. Generative adversarial networks for reconstructing natural images from brain activity. NeuroImage, 181:775785, 2018. [21] Guohua Shen, Tomoyasu Horikawa, Kei Majima, and Yukiyasu Kamitani. Deep image reconstruction from human brain activity. PLoS computational biology, 15(1):e1006633, 2019. [22] Ziqi Ren, Jie Li, Xuetong Xue, Xin Li, Fan Yang, Zhicheng Jiao, and Xinbo Gao. Reconstructing seen image from brain activity by visually-guided cognitive representation and adversarial learning. NeuroImage, 228:117602, 2021. [23] Yuqin Dai, Zhouheng Yao, Chunfeng Song, Qihao Zheng, Weijian Mai, Kunyu Peng, Shuai Lu, Wanli Ouyang, Jian Yang, and Jiamin Wu. Mindaligner: Explicit brain functional alignment for cross-subject visual decoding from limited fmri data, 2025. URL https: //arxiv.org/abs/2502.05034. [24] Alessandro Gifford, Benjamin Lahner, Pablo Oyarzo, Aude Oliva, Gemma Roig, and Radoslaw Cichy. What opportunities do large-scale visual neural datasets offer to the vision sciences community? Journal of Vision, 24(10):152152, 2024. [25] Serge Dumoulin and Brian Wandell. Population receptive field estimates in human visual cortex. Neuroimage, 39(2):647660, 2008. [26] Umut Güçlü and Marcel AJ Van Gerven. Deep neural networks reveal gradient in the complexity of neural representations across the ventral stream. Journal of Neuroscience, 35 (27):1000510014, 2015. [27] David Klindt, Alexander Ecker, Thomas Euler, and Matthias Bethge. Neural system identification for large populations separating what and where. Advances in neural information processing systems, 30, 2017. [28] Michael Eickenberg, Alexandre Gramfort, Gaël Varoquaux, and Bertrand Thirion. Seeing it all: Convolutional network layers map the function of the human visual system. NeuroImage, 152:184194, 2017. [29] Haiguang Wen, Junxing Shi, Yizhen Zhang, Kun-Han Lu, Jiayue Cao, and Zhongming Liu. Neural encoding and decoding with deep learning for dynamic natural vision. Cerebral cortex, 28(12):41364160, 2018. [30] Guy Gaziv, Roman Beliy, Niv Granot, Assaf Hoogi, Francesca Strappini, Tal Golan, and Michal Irani. Self-supervised natural image reconstruction and large-scale semantic classification from brain activity. NeuroImage, 254:119121, 2022. [31] Hossein Adeli, Sun Minni, and Nikolaus Kriegeskorte. Predicting brain activity using transformers. bioRxiv, pages 202308, 2023. [32] Guangyin Bao, Qi Zhang, Zixuan Gong, Zhuojia Wu, and Duoqian Miao. Mindsimulator: Exploring brain concept localization via synthetic fmri. arXiv preprint arXiv:2503.02351, 2025. [33] Roman Beliy, Navve Wasserman, Amit Zalcher, and Michal Irani. The wisdom of crowd of brains: universal brain encoder. arXiv preprint arXiv:2406.12179, 2024. [34] Meenakshi Khosla and Leila Wehbe. High-level visual areas act like domain-general filters with strong selectivity and functional specialization. bioRxiv, pages 202203, 2022. [35] Meenakshi Khosla, Keith Jamison, Amy Kuceyeski, and Mert Sabuncu. Characterizing the ventral visual stream with response-optimized neural encoding models. Advances in Neural Information Processing Systems, 35:93899402, 2022. 12 [36] Cory Efird, Alex Murphy, Joel Zylberberg, and Alona Fyshe. Whats the opposite of face? finding shared decodable concepts and their negations in the brain. arXiv e-prints, pages arXiv2405, 2024. [37] Huzheng Yang, James Gee, and Jianbo Shi. Brain decodes deep nets. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2303023040, 2024. [38] Huzheng Yang, James Gee, and Jianbo Shi. Alignedcut: Visual concepts discovery on brainguided universal feature space. arXiv preprint arXiv:2406.18344, 2024. [39] Andrew Luo, Jacob Yeung, Rushikesh Zawar, Shaurya Dewan, Margaret Henderson, Leila Wehbe, and Michael Tarr. Brain mapping with dense features: Grounding cortical semantic selectivity in natural images with vision transformers. arXiv preprint arXiv:2410.05266, 2024. [40] Gabriel H. Sarch, Michael J. Tarr, Katerina Fragkiadaki, and Leila Wehbe. Brain dissection: fmri-trained networks reveal spatial selectivity in the processing of natural images. bioRxiv, 2023. doi: 10.1101/2023.05.29.542635. URL https://www.biorxiv.org/content/ early/2023/11/20/2023.05.29.542635. [41] Alexander Lappe, Anna Bognár, Ghazaleh Ghamkahri Nejad, Albert Mukovskiy, Lucas Martini, Martin Giese, and Rufin Vogels. Parallel backpropagation for shared-feature visualization. Advances in Neural Information Processing Systems, 37:2299323012, 2024. [42] Edgar Walker, Fabian Sinz, Erick Cobos, Taliah Muhammad, Emmanouil Froudarakis, Paul Fahey, Alexander Ecker, Jacob Reimer, Xaq Pitkow, and Andreas Tolias. Inception loops discover what excites neurons most using deep predictive models. Nature neuroscience, 22(12):20602065, 2019. [43] Pouya Bashivan, Kohitij Kar, and James DiCarlo. Neural population control via deep image synthesis. Science, 364(6439):eaav9436, 2019. [44] Carlos Ponce, Will Xiao, Peter Schade, Till Hartmann, Gabriel Kreiman, and Margaret Livingstone. Evolving images for visual neurons using deep generative network reveals coding principles and neuronal preferences. Cell, 177(4):9991009, 2019. [45] Apurva Ratan Murty, Pouya Bashivan, Alex Abate, James DiCarlo, and Nancy Kanwisher. Computational models of category-selective brain regions enable high-throughput tests of selectivity. Nature communications, 12(1):5540, 2021. [46] Zijin Gu, Keith Wakefield Jamison, Meenakshi Khosla, Emily Allen, Yihan Wu, Ghislain St-Yves, Thomas Naselaris, Kendrick Kay, Mert Sabuncu, and Amy Kuceyeski. NeuroGen: activation optimized image synthesis for discovery neuroscience. NeuroImage, 247:118812, 2022. [47] Paweł Pierzchlewicz, Konstantin Willeke, Arne Nix, Pavithra Elumalai, Kelli Restivo, Tori Shinn, Cate Nealley, Gabrielle Rodriguez, Saumil Patel, Katrin Franke, et al. Energy guided diffusion for generating neurally exciting images, 2023. [48] Andrew Luo, Margaret Henderson, Leila Wehbe, and Michael Tarr. Brain diffusion for visual exploration: Cortical discovery using large scale generative models. arXiv preprint arXiv:2306.03089, 2023. [49] Diego García Cerdas, Christina Sartzetaki, Magnus Petersen, Gemma Roig, Pascal Mettes, and Iris Groen. Brainactiv: Identifying visuo-semantic properties driving cortical selectivity using diffusion-based image manipulation. bioRxiv, pages 202410, 2024. [50] Andrew Luo, Margaret Marie Henderson, Michael J. Tarr, and Leila Wehbe. Brainscuba: Fine-grained natural language captions of visual cortex selectivity. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum? id=mQYHXUUTkU. [51] Takuya Matsuyama, Shinji Nishimoto, and Yu Takagi. Lavca: Llm-assisted visual cortex captioning. arXiv preprint arXiv:2502.13606, 2025. 13 [52] Yu Takagi and Shinji Nishimoto. High-resolution image reconstruction with latent diffusion models from human brain activity. bioRxiv, pages 202211, 2022. [53] Zijiao Chen, Jiaxin Qing, Tiange Xiang, Wan Lin Yue, and Juan Helen Zhou. Seeing beyond the brain: Conditional diffusion model with sparse masked modeling for vision decoding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2271022720, 2023. [54] Yizhuo Lu, Changde Du, Dianpeng Wang, and Huiguang He. Minddiffuser: Controlled image reconstruction from human brain activity with semantic and structural diffusion. arXiv preprint arXiv:2303.14139, 2023. [55] Furkan Ozcelik and Rufin VanRullen. Brain-diffuser: Natural scene reconstruction from fmri signals using generative latent diffusion. arXiv preprint arXiv:2303.05334, 2023. [56] Adrien Doerig, Tim Kietzmann, Emily Allen, Yihan Wu, Thomas Naselaris, Kendrick Kay, and Ian Charest. Semantic scene descriptions as an objective of human vision. arXiv preprint arXiv:2209.11737, 2022. [57] Matteo Ferrante, Furkan Ozcelik, Tommaso Boccato, Rufin VanRullen, and Nicola Toschi. Brain captioning: Decoding human brain activity into images and text. arXiv preprint arXiv:2305.11560, 2023. [58] Yulong Liu, Yongqiang Ma, Wei Zhou, Guibo Zhu, and Nanning Zheng. Brainclip: Bridging brain and visual-linguistic representation via clip for generic natural visual stimulus decoding from fmri. arXiv preprint arXiv:2302.12971, 2023. [59] Weijian Mai and Zhijun Zhang. Unibrain: Unify image reconstruction and captioning all in one diffusion model from human brain activity. arXiv preprint arXiv:2308.07428, 2023. [60] Paul Scotti, Mihir Tripathy, Cesar Kadir Torrico Villanueva, Reese Kneeland, Tong Chen, Ashutosh Narang, Charan Santhirasegaran, Jonathan Xu, Thomas Naselaris, Kenneth Norman, et al. Mindeye2: Shared-subject models enable fmri-to-image with 1 hour of data. arXiv preprint arXiv:2403.11207, 2024. [61] Yohann Benchetrit, Hubert Banville, and Jean-Rémi King. Brain decoding: toward real-time reconstruction of visual perception. arXiv preprint arXiv:2310.19812, 2023. [62] Dongyang Li, Chen Wei, Shiying Li, Jiachen Zou, Haoyang Qin, and Quanying Liu. Visual decoding and reconstruction via eeg embeddings with guided diffusion. arXiv preprint arXiv:2403.07721, 2024. [63] Zhanqiang Guo, Jiamin Wu, Yonghao Song, Jiahui Bu, Weijian Mai, Qihao Zheng, Wanli Ouyang, and Chunfeng Song. Neuro-3d: Towards 3d visual decoding from eeg signals, 2024. URL https://arxiv.org/abs/2411.12248. [64] Yu Zhu, Bo Lei, Chunfeng Song, Wanli Ouyang, Shan Yu, and Tiejun Huang. Multi-modal latent variables for cross-individual primary visual cortex modeling and analysis, 2024. URL https://arxiv.org/abs/2412.14536. [65] Steffen Schneider, Jin Hwa Lee, and Mackenzie Weygandt Mathis. Learnable latent embeddings for joint behavioural and neural analysis. Nature, 617(7960):360368, 2023. [66] Zijiao Chen, Jiaxin Qing, and Juan Helen Zhou. Cinematic mindscapes: High-quality video reconstruction from brain activity. Advances in Neural Information Processing Systems, 36: 2484124858, 2023. [67] Zixuan Gong, Guangyin Bao, Qi Zhang, Zhongwei Wan, Duoqian Miao, Shoujin Wang, Lei Zhu, Changwei Wang, Rongtao Xu, Liang Hu, Ke Liu, and Yu Zhang. Neuroclips: Towards high-fidelity and smooth fMRI-to-video reconstruction. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. URL https://openreview. net/forum?id=8qu52Fl1Dt. [68] Jacob Yeung, Andrew Luo, Gabriel Sarch, Margaret Henderson, Deva Ramanan, and Michael Tarr. Neural representations of dynamic visual stimuli. arXiv preprint arXiv:2406.02659, 2024. [69] Xuan-Hao Liu, Yan-Kai Liu, Yansen Wang, Kan Ren, Hanwen Shi, Zilong Wang, Dongsheng Li, Bao-Liang Lu, and Wei-Long Zheng. Eeg2video: Towards decoding dynamic visual perception from eeg signals. Advances in Neural Information Processing Systems, 37:72245 72273, 2024. [70] Camilo Fosco, Benjamin Lahner, Bowen Pan, Alex Andonian, Emilie Josephs, Alex Lascelles, and Aude Oliva. Brain netflix: Scaling data to reconstruct videos from brain signals. In European Conference on Computer Vision, pages 457474. Springer, 2024. [71] Brian Pasley, Stephen David, Nima Mesgarani, Adeen Flinker, Shihab Shamma, Nathan Crone, Robert Knight, and Edward Chang. Reconstructing speech from human auditory cortex. PLoS biology, 10(1):e1001251, 2012. [72] Gaël Varoquaux, Pradeep Reddy Raamana, Denis Engemann, Andrés Hoyos-Idrobo, Yannick Schwartz, and Bertrand Thirion. Assessing and tuning brain decoders: cross-validation, caveats, and guidelines. NeuroImage, 145:166179, 2017. [73] Ludovic Bellier, Anaïs Llorens, Déborah Marciano, Aysegul Gunduz, Gerwin Schalk, Peter Brunner, and Robert Knight. Music can be reconstructed from human auditory cortex activity using nonlinear decoding models. PLoS biology, 21(8):e3002176, 2023. [74] Subba Reddy Oota, Emin Çelik, Fatma Deniz, and Mariya Toneva. Speech language models lack important brain-relevant semantics. arXiv preprint arXiv:2311.04664, 2023. [75] Hyejeong Jo, Yiqian Yang, Juhyeok Han, Yiqun Duan, Hui Xiong, and Won Hee Lee. Are eeg-to-text models working? arXiv preprint arXiv:2405.06459, 2024. [76] Francis Willett, Erin Kunz, Chaofei Fan, Donald Avansino, Guy Wilson, Eun Young Choi, Foram Kamdar, Matthew Glasser, Leigh Hochberg, Shaul Druckmann, et al. high-performance speech neuroprosthesis. Nature, 620(7976):10311036, 2023. [77] Sean Metzger, Kaylo Littlejohn, Alexander Silva, David Moses, Margaret Seaton, Ran Wang, Maximilian Dougherty, Jessie Liu, Peter Wu, Michael Berger, et al. high-performance neuroprosthesis for speech decoding and avatar control. Nature, 620(7976): 10371046, 2023. [78] Justine Sergent, Shinsuke Ohta, and Brennan Macdonald. Functional neuroanatomy of face and object processing: positron emission tomography study. Brain, 115(1):1536, 1992. [79] Truett Allison, Gregory McCarthy, Anna Nobre, Aina Puce, and Aysenil Belger. Human extrastriate visual cortex and the perception of faces, words, numbers, and colors. Cerebral cortex, 4(5):544554, 1994. [80] Gregory McCarthy, Aina Puce, John Gore, and Truett Allison. Face-specific processing in the human fusiform gyrus. Journal of cognitive neuroscience, 9(5):605610, 1997. [81] Nancy Kanwisher, Josh McDermott, and Marvin Chun. The fusiform face area: module in human extrastriate cortex specialized for face perception. Journal of neuroscience, 17(11): 43024311, 1997. [82] Geoffrey Aguirre, John Detre, David Alsop, and Mark DEsposito. The parahippocampus subserves topographical learning in man. Cerebral cortex, 6(6):823829, 1996. [83] Russell Epstein and Nancy Kanwisher. cortical representation of the local visual environment. Nature, 392(6676):598601, 1998. [84] Paul Downing, Yuhong Jiang, Miles Shuman, and Nancy Kanwisher. cortical area selective for visual processing of the human body. Science, 293(5539):24702473, 2001. [85] Kalanit Grill-Spector. The neural basis of object perception. Current opinion in neurobiology, 13(2):159166, 2003. 15 [86] Rafael Malach, JB Reppas, RR Benson, KK Kwong, Jiang, WA Kennedy, PJ Ledden, TJ Brady, BR Rosen, and RB Tootell. Object-related activity revealed by functional magnetic resonance imaging in human occipital cortex. Proceedings of the National Academy of Sciences, 92(18):81358139, 1995. [87] Meenakshi Khosla, N. Apurva Ratan Murty, and Nancy Kanwisher. highly selective response to food in human visual cortex revealed by hypothesis-free voxel decomposition. Current Biology, 32:113, 2022. [88] Ian ML Pennock, Chris Racey, Emily Allen, Yihan Wu, Thomas Naselaris, Kendrick Kay, Anna Franklin, and Jenny Bosten. Color-biased regions in the ventral visual pathway are food selective. Current Biology, 33(1):134146, 2023. [89] Nidhi Jain, Aria Wang, Margaret M. Henderson, Ruogu Lin, Jacob S. Prince, Michael J. Tarr, and Leila Wehbe. Selectivity for food in human ventral visual cortex. Communications Biology 2023 6:1, 6:114, 2 2023. ISSN 2399-3642. doi: 10.1038/s42003-023-04546-2. [90] Laurent Cohen, Stanislas Dehaene, Lionel Naccache, Stéphane Lehéricy, Ghislaine DehaeneLambertz, Marie-Anne Hénaff, and François Michel. The visual word form area: spatial and temporal characterization of an initial stage of reading in normal subjects and posterior split-brain patients. Brain, 123(2):291307, 2000. [91] Stanislas Dehaene, Lionel Naccache, Laurent Cohen, Denis Le Bihan, Jean-François Mangin, Jean-Baptiste Poline, and Denis Rivière. Cerebral mechanisms of word masking and unconscious repetition priming. Nature neuroscience, 4(7):752758, 2001. [92] Hans Op de Beeck, Jennifer Deutsch, Wim Vanduffel, Nancy Kanwisher, and James DiCarlo. stable topography of selectivity for unfamiliar shape classes in monkey inferior temporal cortex. Cerebral Cortex, 18(7):16761694, 2008. [93] Job van den Hurk, Marc Van Baelen, and Hans Op de Beeck. Development of visual category selectivity in ventral visual cortex does not require visual experience. Proceedings of the National Academy of Sciences, 114(22):E4501E4510, 2017. [94] Golijeh Golarai, Alina Liberman, and Kalanit Grill-Spector. Experience shapes the development of neural substrates of face processing in human ventral temporal cortex. Cerebral Cortex, 27(2):bhv314, 2015. [95] Tina Liu, Adrian Nestor, Mark Vida, John Pyles, Christina Patterson, Ying Yang, Fan Nils Yang, Erez Freud, and Marlene Behrmann. Successful reorganization of categoryselective visual cortex following occipito-temporal lobectomy in childhood. Cell reports, 24 (5):11131122, 2018. [96] Hans Op de Beeck, Ineke Pillet, and Brendan Ritchie. Factors determining where categoryselective areas emerge in visual cortex. Trends in cognitive sciences, 23(9):784797, 2019. [97] Nooshin Abbasi, John Duncan, and Reza Rajimehr. Genetic influence is linked to cortical morphology in category-selective areas of visual cortex. Nature Communications, 11(1):709, 2020. [98] Alfonso Nieto-Castañón and Evelina Fedorenko. Subject-specific functional localizers increase sensitivity and functional resolution of multi-subject analyses. Neuroimage, 63(3):16461669, 2012. [99] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In International conference on machine learning, pages 1126 1135. PMLR, 2017. [100] Alex Nichol, Joshua Achiam, and John Schulman. On first-order meta-learning algorithms. arXiv preprint arXiv:1803.02999, 2018. [101] Aravind Rajeswaran, Chelsea Finn, Sham Kakade, and Sergey Levine. Meta-learning with implicit gradients. Advances in neural information processing systems, 32, 2019. 16 [102] Jake Snell, Kevin Swersky, and Richard Zemel. Prototypical networks for few-shot learning. Advances in neural information processing systems, 30, 2017. [103] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877 1901, 2020. [104] Johannes Von Oswald, Eyvind Niklasson, Ettore Randazzo, João Sacramento, Alexander Mordvintsev, Andrey Zhmoginov, and Max Vladymyrov. Transformers learn in-context by gradient descent. In International Conference on Machine Learning, pages 3515135174. PMLR, 2023. [105] Sewon Min, Mike Lewis, Luke Zettlemoyer, and Hannaneh Hajishirzi. Metaicl: Learning to learn in context. arXiv preprint arXiv:2110.15943, 2021. [106] Julian Coda-Forno, Marcel Binz, Zeynep Akata, Matt Botvinick, Jane Wang, and Eric Schulz. Meta-in-context learning in large language models. Advances in Neural Information Processing Systems, 36:6518965201, 2023. [107] Shivam Garg, Dimitris Tsipras, Percy Liang, and Gregory Valiant. What can transformers learn in-context? case study of simple function classes. Advances in Neural Information Processing Systems, 35:3058330598, 2022. [108] Damai Dai, Yutao Sun, Li Dong, Yaru Hao, Shuming Ma, Zhifang Sui, and Furu Wei. Why can gpt learn in-context? language models implicitly perform gradient descent as meta-optimizers. arXiv preprint arXiv:2212.10559, 2022. [109] Jianlin Su. Analyzing the scale operation of attention from the perspective of entropy invariance, Dec 2021. URL https://kexue.fm/archives/8823. [110] David Chiang and Peter Cholak. Overcoming theoretical limitation of self-attention. arXiv preprint arXiv:2202.12172, 2022. [111] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023. [112] Emily Allen, Ghislain St-Yves, Yihan Wu, Jesse Breedlove, Jacob Prince, Logan Dowdle, Matthias Nau, Brad Caron, Franco Pestilli, Ian Charest, et al. massive 7t fmri dataset to bridge cognitive neuroscience and artificial intelligence. Nature neuroscience, 25(1): 116126, 2022. [113] Nadine Chang, John Pyles, Austin Marcus, Abhinav Gupta, Michael Tarr, and Elissa Aminoff. Bold5000, public fMRI dataset while viewing 5000 visual images. Scientific Data, 6(1):118, 2019. [114] Colin Conwell, Jacob Prince, Kendrick Kay, George Alvarez, and Talia Konkle. large-scale examination of inductive biases shaping high-level visual representation in brains and machines. Nature communications, 15(1):9383, 2024. [115] Ken Shirakawa, Yoshihiro Nagano, Misato Tanaka, Shuntaro Aoki, Kei Majima, Yusuke Muraki, and Yukiyasu Kamitani. Spurious reconstruction from brain activity. arXiv preprint arXiv:2405.10078, 2024."
        },
        {
            "title": "Sections",
            "content": "1. Implementation details (Section A.1) 2. Text prompts for category-selective regions (Section A.2) 3. Cortex prediction explained variance for different image encoding backbones and subjects (Section A.3) 4. Voxelwise performance across five category-selective regions for different image encoding backbones and subjects (Section A.4) 5. Voxelwise explained variance across varying support set sizes for more subjects, backbones and for pretrain-only models (Section A.5) 6. Impact of holding out the test subjects unique images during meta-training evaluated on more backbones (Section A.6) 7. Correlation of each backbones predictions with fully trained activation predictions (Section A.7) 8. Voxelwise explained variance evaluation in BOLD5000 for more subjects and different backbones (Section A.8) 9. Dimensional reduction of predicted response function weights on more subjects (Section A.9) 10. Predicting cortical responses from natural language prompts on more subjects (Section A.10) 11. Voxelwise prompt classification accuracy for more subjects (Section A.11) 18 A."
        },
        {
            "title": "Implementation details",
            "content": "Network architecture. Our BraInCoRL model architecture comprises three main components: 1. Input projection. We concatenate each image embedding with its scalar neural response and pass it through single-layer MLP to align the two modalities into unified internal feature space. 2. Transformer encoder. stack of 20 self-attention layers integrates information across all support examples (plus learnable tokens), capturing contextual relationships and the relative importance of each in-context sample. We adopt best practices and utilize SwiGLU activation paired with pre-normalization in the attention block. We utilize 10 heads in each multi-head self attention. 3. Weight prediction. The aggregated representation is fed through another single-layer MLP that outputs weight vector, which is then used to linearly combine unknown-image embeddings to produce the final neural response predictions. The CLIP-based variant (encoding dimension = 512) contains approximately 97.2 parameters; DINO (E = 768) and SIGLIP (E = 1152) variants comprise roughly 112 and 130 parameters, respectively. Model training. Training is implemented in PyTorch on eight NVIDIA RTX 6000 Ada GPUs (48 GB each). We optimize using AdamW (decoupled weight decay 1 104) with an initial learning rate of 1 103, which decays to 1 105 via ReduceLROnPlateau scheduler (factor 0.1, patience 5 epochs, cooldown 2 epochs, threshold 1e-4). Mini-batches randomly sample an in-context support set of 100 images in the first pretraining stage, and randomly sample between 30 and 500 in-context support images in the second context extension stage and the third finetuning stages. Each training stage runs for up to 100 epochs with early stopping based on validation loss (patience: 5 epochs). The training batch size is 80. We allocate 20% of the test set data for validation. Computational cost. With an in-context support set of 100 images, our model predicts responses for 20,000 voxels in the higher visual cortex in under 20 seconds on single RTX 6000 Ada GPU. 19 A.2 Text prompts for category-selective regions We define set of natural language prompts for each semantic category. For every imageprompt pair, we use the CLIP text encoder to generate text encodings. The natural language prompts for each category are listed below: Faces [A photo of persons face, portrait photo of face, face facing the camera, photo of face, photo of human face, photo of faces, People looking at the camera, portrait of person, portrait photo] Bodies [A photo of torso, photo of limbs, photo of bodies, photo of person, photo of people, photo of body, persons arms, persons legs, photo of hands] Places [A photo of bedroom, photo of an office, photo of hallway, photo of doorway, photo of interior design, photo of building, photo of house, photo of nature, photo of landscape] Food [A photo of food, photo of cuisine, photo of fruit, photo of foodstuffs, photo of meal, photo of bread, photo of rice, photo of snack, photo of pastries] Words [A photo of words, photo of glyphs, photo of glyph, photo of text, photo of numbers, photo of letter, photo of letters, photo of writing, photo of text on an object] 20 A.3 Cortex prediction explained variance for different image encoding backbones and subjects In this section, we compare three methods across multiple subjects (S1-S8) and embedding backbones (CLIP, DINO, SigLIP): the fully trained reference model fit to converge on each subjects full 9,000image training set; our BraInCoRL approach, which adapts to new subject with only 100 in-context images; and within-subject ridge regression baseline trained on the same 100 images with the BraInCoRL approach. In every case, BraInCoRL outperforms ridge regression and achieves accuracy similar to that of the fully trained model. Figure S.1: Higher visual cortex explained variance of CLIP backbone. From left to right, we show the explained variance from the full model trained on 9000 images for subject, BraInCoRL with just 100 in-context images from the new subject, and within-subject ridge regression with 100 images using CLIP backbone for subject 1,2,3,4. Figure S.2: Higher visual cortex explained variance of CLIP backbone. From left to right, we show the explained variance from the full model trained on 9000 images for subject, BraInCoRL with just 100 in-context images from the new subject, and within-subject ridge regression with 100 images using CLIP backbone for subject 5, 6, 7, 8. 22 Figure S.3: Higher visual cortex explained variance of DINO backbone. From left to right, we show the explained variance from the full model trained on 9000 images for subject, BraInCoRL with just 100 in-context images from the new subject, and within-subject ridge regression with 100 images using DINO backbone for subject 1, 2, 3, 4. 23 Figure S.4: Higher visual cortex explained variance of DINO backbone. From left to right, we show the explained variance from the full model trained on 9000 images for subject, BraInCoRL with just 100 in-context images from the new subject, and within-subject ridge regression with 100 images using DINO backbone for subject 5, 6, 7, 8. Figure S.5: Higher visual cortex explained variance of SigLIP backbone. From left to right, we show the explained variance from the full model trained on 9000 images for subject, BraInCoRL with just 100 in-context images from the new subject, and within-subject ridge regression with 100 images using SigLIP backbone for subject 1, 2, 3, 4. 25 Figure S.6: Higher visual cortex explained variance of SigLIP backbone. From left to right, we show the explained variance from the full model trained on 9000 images for subject, BraInCoRL with just 100 in-context images from the new subject, and within-subject ridge regression with 100 images using SigLIP backbone for subject 5, 6, 7, 8. 26 A.4 Voxelwise performance across five category-selective regions for different image encoding backbones and subjects In this section, we report voxel-wise explained variance in five category-selective regions (faces, places, bodies, words, and food) with CLIP, DINO and SigLIP backbone for subjects S1-S8. We compare our in-context model (BraInCoRL) against the fully trained reference model fit to converge on each subjects full 9,000-image training set, within-subject ridge regression baselines trained on 100 and 300 images, and the FsAverage map. BraInCoRL outperforms the ridge baselines and closely approaches the performance of the fully trained model. Table S.1: Voxel-wise explained variance with the CLIP backbone for Subjects 1 and 2. We report performance for our in-context model (BraInCoRL), the fully trained reference (Fully Trained), within-subject ridge regression baselines (100, 300), and the FsAverage map across five categoryselective regions (faces, places, bodies, words, food). Faces Places Bodies Words Food Mean Fully Trained Ridge-100 Ridge-300 FsAverage map BraInCoRL-100 0.19 0.10 0.13 0.13 0.16 S2 0.16 0.07 0.10 0. 0.13 S1 0.20 0.08 0.13 0.11 0.16 0.27 0.14 0.20 0.19 0.23 S1 0.28 0.16 0.22 0. 0.25 S2 0.24 0.12 0.16 0.08 0.21 0.11 0.02 0.06 0.06 0.07 S2 0.11 0.03 0.06 0. 0.08 S1 0.16 0.05 0.10 0.14 0.12 0.17 0.07 0.11 0.18 0.13 S1 0.18 0.07 0.11 0. 0.13 S2 0.19 0.08 0.12 0.06 0.15 Table S.2: Voxel-wise explained variance with the CLIP backbone for Subjects 3 and 4. We report performance for our in-context model (BraInCoRL), the fully trained reference (Fully Trained), within-subject ridge regression baselines (100, 300), and the FsAverage map across five categoryselective regions (faces, places, bodies, words, food). Faces Places Bodies Words Food Mean Fully Trained Ridge-100 Ridge-300 FsAverage map BraInCoRL-100 S3 0.16 0.07 0.10 0. 0.12 S4 0.14 0.05 0.09 0.03 0.10 0.16 0.08 0.11 0.14 0.13 S4 0.16 0.05 0.10 0. 0.13 S3 0.17 0.08 0.11 0.11 0.14 0.17 0.08 0.12 0.06 0.13 S3 0.09 0.02 0.04 0. 0.05 S4 0.07 0.01 0.04 0.03 0.04 0.10 0.03 0.05 0.10 0.07 S4 0.12 0.04 0.07 0. 0.08 S3 0.13 0.05 0.08 0.10 0.10 0.14 0.05 0.09 0.04 0.10 Table S.3: Voxel-wise explained variance with the CLIP backbone for Subjects 5 and 6. We report performance for our in-context model (BraInCoRL), the fully trained reference (\"Fully Trained\"), within-subject ridge regression baselines (100, 300), and the FsAverage map across five categoryselective regions (faces, places, bodies, words, food). Faces Places Bodies Words Food Mean Fully Trained Ridge-100 Ridge-300 FsAverage map BraInCoRL-100 S5 0.24 0.11 0.16 0.07 0.20 0.17 0.07 0.11 0.05 0.14 S5 0.32 0.16 0.24 0. 0.29 S6 0.13 0.03 0.07 0.08 0.10 0.26 0.13 0.19 0.06 0.23 S6 0.18 0.09 0.13 0. 0.15 S5 0.17 0.06 0.10 0.05 0.12 0.09 0.01 0.04 0.06 0.05 S5 0.24 0.11 0.16 0. 0.19 S6 0.09 0.02 0.04 0.04 0.05 0.23 0.10 0.15 0.07 0.19 S6 0.11 0.03 0.06 0. 0.08 27 Table S.4: Voxel-wise explained variance with the CLIP backbone for Subjects 7 and 8. We report performance for our in-context model (BraInCoRL), the fully trained oracle (\"Fully Trained\"), withinsubject ridge regression baselines (100, 300), and the FsAverage map across five category-selective regions (faces, places, bodies, words, food). Faces Places Bodies Words Food Mean Fully Trained Ridge-100 Ridge-300 FsAverage map BraInCoRLS7 0.14 0.06 0.09 0.12 0.11 S8 0. 0.03 0.05 0.04 0.07 S7 0.18 0.08 0.12 0.17 0. S8 0.10 0.04 0.06 0.04 0.08 S7 0. 0.11 0.15 0.10 0.18 S8 0.09 0.04 0.05 0.03 0. S7 0.12 0.03 0.06 0.09 0.08 S8 0. 0.00 0.01 0.03 0.02 S7 0.12 0.03 0.06 0.19 0. S8 0.06 0.02 0.03 0.02 0.04 S7 0. 0.06 0.09 0.09 0.12 S8 0.09 0.03 0.05 0.03 0. Table S.5: Voxel-wise explained variance with the DINO backbone for Subjects 1 and 2. We report performance for our in-context model (BraInCoRL), the fully trained oracle (Fully Trained), within-subject ridge regression baselines (100, 300), and the FsAverage map across five categoryselective regions (faces, places, bodies, words, food). Faces Places Bodies Words Food Mean Fully Trained Ridge-100 Ridge-300 FsAverage map BraInCoRL-100 S1 0. 0.03 0.07 0.13 0.14 S2 0.13 0.02 0.05 0.06 0. S1 0.16 0.03 0.07 0.11 0.15 S2 0. 0.05 0.10 0.19 0.21 S1 0.24 0.07 0.14 0.09 0. S2 0.20 0.05 0.10 0.08 0.18 S1 0. 0.00 0.03 0.06 0.07 S2 0.08 0.01 0.03 0.03 0. S1 0.13 0.02 0.05 0.14 0.11 S2 0. 0.03 0.05 0.18 0.12 S1 0.14 0.02 0.06 0.08 0. S2 0.15 0.03 0.06 0.06 0.14 Table S.6: Voxel-wise explained variance with the DINO backbone for Subjects 3 and 4. We report performance for our in-context model (BraInCoRL), the fully trained oracle (Fully Trained), within-subject ridge regression baselines (100, 300), and the FsAverage map across five categoryselective regions (faces, places, bodies, words, food). Faces Places Bodies Words Food Mean Fully Trained Ridge-100 Ridge-300 FsAverage map BraInCoRL-100 S3 0.13 0.02 0.05 0.10 0. S4 0.11 0.00 0.04 0.03 0.10 S3 0. 0.03 0.06 0.14 0.13 S4 0.12 0.01 0.04 0.05 0. S3 0.13 0.03 0.06 0.11 0.13 S4 0. 0.02 0.07 0.06 0.14 S3 0.06 -0.01 0.02 0.07 0. S4 0.05 -0.02 0.01 0.03 0.05 S3 0. 0.00 0.03 0.10 0.06 S4 0.09 -0.02 0.03 0.07 0. S3 0.10 0.01 0.04 0.09 0.09 S4 0. 0.01 0.04 0.04 0.10 Table S.7: Voxel-wise explained variance with the DINO backbone for Subjects 5 and 6. We report performance for our in-context model (BraInCoRL), the fully trained reference (\"Fully Trained\"), within-subject ridge regression baselines (100, 300), and the FsAverage map across five categoryselective regions (faces, places, bodies, words, food). Faces Places Bodies Words Food Mean Fully Trained Ridge-100 Ridge-300 FsAverage map BraInCoRLS5 0.19 0.04 0.08 0.07 0.18 S6 0. 0.02 0.06 0.05 0.12 S5 0.27 0.05 0.13 0.11 0. S6 0.11 0.01 0.04 0.08 0.10 S5 0. 0.07 0.12 0.06 0.21 S6 0.15 0.03 0.07 0.04 0. S5 0.13 0.01 0.04 0.05 0.10 S6 0. -0.01 0.01 0.06 0.04 S5 0.20 0.03 0.08 0.08 0. S6 0.06 -0.01 0.02 0.04 0.04 S5 0. 0.04 0.09 0.07 0.17 S6 0.08 0.01 0.03 0.05 0. 28 Table S.8: Voxel-wise explained variance with the DINO backbone for Subjects 7 and 8. We report performance for our in-context model (BraInCoRL), the fully trained oracle (\"Fully Trained\"), within-subject ridge regression baselines (100, 300), and the FsAverage map across five categoryselective regions (faces, places, bodies, words, food). Faces Places Bodies Words Food Mean Fully Trained Ridge-100 Ridge-300 FsAverage map BraInCoRL-100 0.10 0.02 0.05 0.12 0.10 S8 0.06 -0.00 0.02 0. 0.06 S7 0.15 0.02 0.06 0.17 0.14 0.08 0.00 0.03 0.04 0.07 S7 0.18 0.06 0.10 0. 0.17 S8 0.07 0.00 0.03 0.03 0.07 0.08 0.01 0.03 0.09 0.07 S8 0.02 -0.01 0.00 0. 0.02 S7 0.09 0.00 0.02 0.19 0.07 0.03 -0.00 0.01 0.02 0.04 S7 0.12 0.02 0.05 0. 0.10 S8 0.07 -0.00 0.02 0.03 0.06 Table S.9: Voxel-wise explained variance with the SigLIP backbone for Subjects 1 and 2. We report performance for our in-context model (BraInCoRL), the fully trained oracle (Fully Trained), within-subject ridge regression baselines (100, 300), and the FsAverage map across five categoryselective regions (faces, places, bodies, words, food). Faces Places Bodies Words Food Mean Fully Trained Ridge-100 Ridge-300 FsAverage map BraInCoRL-100 S1 0.19 0.10 0.14 0. 0.17 S2 0.17 0.07 0.11 0.06 0.13 0.21 0.09 0.14 0.11 0.18 S2 0.27 0.14 0.20 0. 0.24 S1 0.30 0.18 0.23 0.09 0.27 0.25 0.12 0.17 0.08 0.21 S1 0.12 0.03 0.07 0. 0.09 S2 0.11 0.04 0.06 0.03 0.08 0.17 0.06 0.11 0.14 0.13 S2 0.18 0.07 0.12 0. 0.14 S1 0.19 0.08 0.12 0.08 0.15 0.20 0.08 0.13 0.06 0.16 Table S.10: Voxel-wise explained variance with the SigLIP backbone for Subjects 3 and 4. We report performance for our in-context model (BraInCoRL), the fully trained oracle (Fully Trained), within-subject ridge regression baselines (100, 300), and the FsAverage map across five category-selective regions (faces, places, bodies, words, food). Faces Places Bodies Words Food Mean Fully Trained Ridge-100 Ridge-300 FsAverage map BraInCoRL-100 S3 0.17 0.07 0.11 0.10 0.12 0.14 0.04 0.08 0.03 0.11 S3 0.17 0.08 0.11 0. 0.13 S4 0.17 0.06 0.10 0.05 0.14 0.18 0.09 0.12 0.11 0.14 S4 0.18 0.07 0.12 0. 0.15 S3 0.10 0.02 0.05 0.07 0.05 0.08 0.01 0.03 0.03 0.05 S3 0.11 0.03 0.06 0. 0.06 S4 0.13 0.04 0.07 0.07 0.09 0.14 0.05 0.08 0.10 0.10 S4 0.15 0.05 0.09 0. 0.12 Table S.11: Voxel-wise explained variance with the SigLIP backbone for Subjects 5 and 6. We report performance for our in-context model (BraInCoRL), the fully trained reference (\"Fully Trained\"), within-subject ridge regression baselines (100, 300), and the FsAverage map across five category-selective regions (faces, places, bodies, words, food). Faces Places Bodies Words Food Mean Fully Trained Ridge-100 Ridge-300 FsAverage map BraInCoRL-100 0.24 0.11 0.17 0.07 0.20 S6 0.18 0.07 0.12 0. 0.14 S5 0.33 0.16 0.24 0.11 0.28 0.15 0.04 0.08 0.08 0.11 S5 0.28 0.14 0.20 0. 0.23 S6 0.19 0.09 0.14 0.04 0.16 0.18 0.06 0.10 0.05 0.12 S6 0.10 0.01 0.04 0. 0.05 S5 0.26 0.11 0.17 0.08 0.19 0.10 0.02 0.05 0.04 0.05 S5 0.24 0.11 0.16 0. 0.19 S6 0.12 0.04 0.07 0.05 0.09 Table S.12: Voxel-wise explained variance with the SigLIP backbone for Subjects 7 and 8. We report performance for our in-context model (BraInCoRL), the fully trained oracle (\"Fully Trained\"), within-subject ridge regression baselines (100, 300), and the FsAverage map across five category-selective regions (faces, places, bodies, words, food). Faces Places Bodies Words Food Mean Fully Trained Ridge-100 Ridge-300 FsAverage map BraInCoRL-100 S7 0. 0.06 0.10 0.12 0.11 S8 0.09 0.02 0.05 0.04 0. S7 0.15 0.08 0.12 0.17 0.15 S8 0. 0.04 0.07 0.04 0.08 S7 0.22 0.11 0.15 0.10 0. S8 0.09 0.03 0.05 0.03 0.08 S7 0. 0.03 0.06 0.09 0.08 S8 0.04 0.00 0.02 0.03 0. S7 0.13 0.03 0.06 0.19 0.09 S8 0. 0.02 0.03 0.02 0.04 S7 0.16 0.06 0.09 0.09 0. S8 0.09 0.03 0.05 0.03 0.07 30 A.5 Voxelwise explained variance across varying support set sizes for more subjects, backbones and for pretrain-only models In this section, we investigate how the size of the in-context support set affects voxelwise predictive performance. We evaluate three image-encoding backbones (CLIP, DINO, SigLIP) on eight subjects (S1S8) by comparing the performance of BraInCoRL with the pretrain-only BraInCoRL (i.e. same architecture but only pretrained), the within-subject ridge-regression baseline, and the fully trained reference model fit to converge on each subjects full 9,000-image training set. Figure S.7: Voxelwise explained variance as function of in-context support set size. Voxelwise explained variance is visualized for subjects 1, 2, 3, 4 for each backbone (CLIP, DINO, SigLIP). we plot results for the BraInCoRL, along with the pretrain-only BraInCoRL (i.e. same architecture but only pretrained), the within-subject ridge-regression baseline, and the fully trained reference model using all 9,000 images. 31 Figure S.8: Voxelwise explained variance as function of in-context support set size. Voxelwise explained variance is visualized for subjects 5, 6, 7, 8 for each backbone (CLIP, DINO, SigLIP). we plot results for the BraInCoRL, along with the pretrain-only BraInCoRL (i.e. same architecture but only pretrained), the within-subject ridge-regression baseline, and the fully trained reference model using all 9,000 images. A.6 Impact of holding out the test subjects unique images during meta-training evaluated on more backbones In this section, we further conduct ablations by evaluating BraInCoRL model trained without holding out the test subjects support images (BraInCoRL no HO) and BraInCoRL model with only pretraining, on DINO and SigLIP backbones. The result indicates that fine-tuning on real neural data enhances performance, and that BraInCoRL is able to generalize effectively to entirely unseen images without having encountered them during training. Figure S.9: Distributions of voxelwise explained variance for subjects 1, 2, 5 and 7 using DINO encoding. Results confirm that finetuning with real neural data boosts performance and that BraInCoRL can generalize well to previously unseen images without requiring them during training. Figure S.10: Distributions of voxelwise explained variance for subjects 1, 2, 5 and 7 using SigLIP encoding. Results confirm that finetuning with real neural data boosts performance and that BraInCoRL can generalize well to previously unseen images without requiring them during training. A.7 Correlation of each backbones predictions with fully trained activation predictions Using various image-encoding backbones, we plot how each subjects BraInCoRL predicted explained variance correlates with the fully trained models explained variance (fully trained model refers to the fully trained reference model fit to converge on each subjects full 9,000-image training set). Across every backbone and all subjects, this correlation remains uniformly high. Figure S.11: Voxelwise explained-variance correlation across backbones for subject 1, 2, 3, 4. Each panel shows scatter of the BraInCoRLs explained-variance predictions (100 in-context examples) versus the fully trained reference models explained variance for each voxel. Rows correspond to subjects (1, 2, 3, 4) and columns to image-encoding backbones (CLIP, DINO, SigLIP). The dashed line marks = x. 34 Figure S.12: Voxelwise explained-variance correlation across backbones for subject 5, 6, 7, 8. Each panel shows scatter of the BraInCoRLs explained-variance predictions (100 in-context examples) versus the fully trained reference models explained variance for each voxel. Rows correspond to subjects (5, 6, 7, 8) and columns to image-encoding backbones (CLIP, DINO, SigLIP). The dashed line marks = x. A.8 Voxelwise explained-variance evaluation in BOLD5000 for more subjects and different backbones In this section, we analyze how varying the number of in-context examples influences voxel-level prediction performance on the BOLD5000 dataset. For subject S2 and S3, we plot the mean Pearsons between model-predicted and actual BOLD responses as function of support-set size, comparing our BraInCoRL model against ridge regression baseline. Results are averaged over five crossvalidation folds. Across all three image-encoding backbones (CLIP, DINO, and SIGLIP), BraInCoRL consistently outperforms ridge regression. Figure S.13: Support-set size versus voxelwise Pearson in BOLD5000. Each panel shows the mean voxelwise Pearson correlation between predicted and actual BOLD5000 responses for BraInCoRL and ridge regression, plotted against the number of in-context samples. 36 A.9 Dimensional reduction of predicted response function weights on more subjects In this section, we utilize UMAP to visualize the BraInCoRL-predicted voxelwise weights under the CLIP image encoding backbone for subject S1-S8. The cortical maps show color-coded mappings that align well with functionally-defined regions. Figure S.14: Dimensional reduction of predicted response weights for subject S1-S4 under CLIP backbone. The cortical maps show color-coded mappings that align well with functionally-defined regions: body and face regions (EBA and FFA/aTL-faces), place regions (RSC/OPA/PPA), and food regions (in red). 37 Figure S.15: Dimensional reduction of predicted response weights for subject S5-S8 under CLIP backbone. The cortical maps show color-coded mappings that align well with functionally-defined regions: body and face regions (EBA and FFA/aTL-faces), place regions (RSC/OPA/PPA), and food regions (in red). 38 A.10 Predicting cortical responses from natural language prompts on more subjects In this section, we further predict cortical responses from natural language prompts on subject 2-8. For each semantic category, we convert natural language prompt into CLIP text embedding, project it into the image feature space, and use BraInCoRL to predict the corresponding voxel activation map. The predicted activations align closely with known t-statistic of category-selective region, illustrating the potential for zero-shot, language-driven functional mapping of visual cortex. Figure S.16: Predicting responses of natural language prompts for subject 2. We convert each text prompt corresponding to semantic category into CLIP text embedding, project it into image-feature space, and predict its cortical activation on subject 2. The resulting activation maps closely match the t-statistics of known category-selective regions, demonstrating the feasibility of language-driven, zero-shot functional mapping of the visual cortex. Figure S.17: Predicting responses of natural language prompts for subject 3. We convert each text prompt corresponding to semantic category into CLIP text embedding, project it into image-feature space, and predict its cortical activation on subject 3. The resulting activation maps closely match the t-statistics of known category-selective regions, demonstrating the feasibility of language-driven, zero-shot functional mapping of the visual cortex. 39 Figure S.18: Predicting responses of natural language prompts for subject 4. We convert each text prompt corresponding to semantic category into CLIP text embedding, project it into image-feature space, and predict its cortical activation on subject 4. The resulting activation maps closely match the t-statistics of known category-selective regions, demonstrating the feasibility of language-driven, zero-shot functional mapping of the visual cortex. Figure S.19: Predicting responses of natural language prompts for subject 5. We convert each text prompt corresponding to semantic category into CLIP text embedding, project it into image-feature space, and predict its cortical activation on subject 5. The resulting activation maps closely match the t-statistics of known category-selective regions, demonstrating the feasibility of language-driven, zero-shot functional mapping of the visual cortex. 40 Figure S.20: Predicting responses of natural language prompts for subject 6. We convert each text prompt corresponding to semantic category into CLIP text embedding, project it into image-feature space, and predict its cortical activation on subject 6. The resulting activation maps closely match the t-statistics of known category-selective regions, demonstrating the feasibility of language-driven, zero-shot functional mapping of the visual cortex. Figure S.21: Predicting responses of natural language prompts for subject 7. We convert each text prompt corresponding to semantic category into CLIP text embedding, project it into image-feature space, and predict its cortical activation on subject 7. The resulting activation maps closely match the t-statistics of known category-selective regions, demonstrating the feasibility of language-driven, zero-shot functional mapping of the visual cortex. 41 Figure S.22: Predicting responses of natural language prompts for subject 8. We convert each text prompt corresponding to semantic category into CLIP text embedding, project it into image-feature space, and predict its cortical activation on subject 8. The resulting activation maps closely match the t-statistics of known category-selective regions, demonstrating the feasibility of language-driven, zero-shot functional mapping of the visual cortex. A.11 Voxelwise prompt classification accuracy for more subjects In this section, we further quantify the semantic specificity of BraInCoRLs voxelwise predictions on subject 3-8. we compute, for each subject and each category-selective ROI, the fraction of voxels whose peak predicted activation corresponded to the semantic category named by the text prompt. Table S.13: Voxelwise prompt classification accuracy for subjects 3 and 4. Each cell shows the percentage of voxels in given category selective region (columns) whose peak predicted activation was elicited by specific semantic prompt (rows, see Appendix) for subject 3 and 4. Using only 100 support images, BraInCoRL effectively localizes category-selective regions with high data efficiency."
        },
        {
            "title": "Words",
            "content": "S3 0.57 0.29 0.04 0.07 0.02 S4 0.42 0.36 0.08 0.09 0.04 S3 0.23 0.60 0.02 0.14 0. S4 0.12 0.66 0.06 0.12 0.04 S3 0.04 0.02 0.84 0.09 0.01 S4 0.03 0.03 0.82 0.09 0. S3 0.16 0.11 0.15 0.53 0.05 S4 0.20 0.05 0.20 0.51 0.05 S3 0.12 0.20 0.08 0.51 0. S4 0.19 0.12 0.16 0.43 0."
        },
        {
            "title": "Bodies\nFaces\nPlaces\nFood\nWords",
            "content": "Table S.14: Voxelwise prompt classification accuracy for subjects 5 and 6. Each cell shows the percentage of voxels in given category selective region (columns) whose peak predicted activation was elicited by specific semantic prompt (rows, see Appendix) for subject 5 and 6. Using only 100 support images, BraInCoRL effectively localizes category-selective regions with high data efficiency."
        },
        {
            "title": "Words",
            "content": "S5 0.54 0.29 0.06 0.09 0.02 S6 0.64 0.25 0.01 0.07 0.03 S5 0.17 0.65 0.05 0.13 0. S6 0.21 0.63 0.01 0.11 0.04 S5 0.01 0.00 0.88 0.10 0.00 S6 0.08 0.04 0.65 0.20 0. S5 0.19 0.03 0.13 0.64 0.01 S6 0.15 0.05 0.09 0.66 0.06 S5 0.27 0.20 0.10 0.39 0. S6 0.25 0.15 0.04 0.41 0."
        },
        {
            "title": "Bodies\nFaces\nPlaces\nFood\nWords",
            "content": "Table S.15: Voxelwise prompt classification accuracy for subjects 7 and 8. Each cell shows the percentage of voxels in given category selective region (columns) whose peak predicted activation was elicited by specific semantic prompt (rows, see Appendix) for subject 7 and 8. Using only 100 support images, BraInCoRL effectively localizes category-selective regions with high data efficiency."
        },
        {
            "title": "Words",
            "content": "S7 0.69 0.19 0.04 0.06 0.02 S8 0.57 0.25 0.05 0.10 0.03 S7 0.26 0.59 0.02 0.11 0. S8 0.15 0.59 0.03 0.20 0.03 S7 0.01 0.01 0.89 0.07 0.01 S8 0.07 0.04 0.58 0.26 0. S7 0.08 0.04 0.12 0.68 0.07 S8 0.20 0.10 0.06 0.57 0.08 S7 0.19 0.22 0.13 0.32 0. S8 0.17 0.18 0.05 0.47 0."
        },
        {
            "title": "Bodies\nFaces\nPlaces\nFood\nWords",
            "content": ""
        }
    ],
    "affiliations": [
        "Carnegie Mellon University",
        "Chinese University of Hong Kong",
        "Columbia University",
        "Harvard University",
        "University of Hong Kong",
        "University of Washington"
    ]
}