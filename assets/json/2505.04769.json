{
    "paper_title": "Vision-Language-Action Models: Concepts, Progress, Applications and Challenges",
    "authors": [
        "Ranjan Sapkota",
        "Yang Cao",
        "Konstantinos I. Roumeliotis",
        "Manoj Karkee"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Vision-Language-Action (VLA) models mark a transformative advancement in artificial intelligence, aiming to unify perception, natural language understanding, and embodied action within a single computational framework. This foundational review presents a comprehensive synthesis of recent advancements in Vision-Language-Action models, systematically organized across five thematic pillars that structure the landscape of this rapidly evolving field. We begin by establishing the conceptual foundations of VLA systems, tracing their evolution from cross-modal learning architectures to generalist agents that tightly integrate vision-language models (VLMs), action planners, and hierarchical controllers. Our methodology adopts a rigorous literature review framework, covering over 80 VLA models published in the past three years. Key progress areas include architectural innovations, parameter-efficient training strategies, and real-time inference accelerations. We explore diverse application domains such as humanoid robotics, autonomous vehicles, medical and industrial robotics, precision agriculture, and augmented reality navigation. The review further addresses major challenges across real-time control, multimodal action representation, system scalability, generalization to unseen tasks, and ethical deployment risks. Drawing from the state-of-the-art, we propose targeted solutions including agentic AI adaptation, cross-embodiment generalization, and unified neuro-symbolic planning. In our forward-looking discussion, we outline a future roadmap where VLA models, VLMs, and agentic AI converge to power socially aligned, adaptive, and general-purpose embodied agents. This work serves as a foundational reference for advancing intelligent, real-world robotics and artificial general intelligence. >Vision-language-action, Agentic AI, AI Agents, Vision-language Models"
        },
        {
            "title": "Start",
            "content": "Vision-Language-Action Models: Concepts, Progress, Applications and Challenges Ranjan Sapkotaa, Yang Caob, Konstantinos I. Roumeliotisc, Manoj Karkeea aCornell University, Biological & Environmental Engineering, Ithaca, New York, USA bThe Hong Kong University of Science and Technology, Department of Computer Science and Engineering, Hong Kong cUniversity of the Peloponnese, Department of Informatics and Telecommunications, Greece"
        },
        {
            "title": "Abstract",
            "content": "Vision-Language-Action (VLA) models mark transformative advancement in artificial intelligence, aiming to unify perception, natural language understanding, and embodied action within single computational framework. This foundational review presents comprehensive synthesis of recent advancements in Vision-Language-Action models, systematically organized across five thematic pillars that structure the landscape of this rapidly evolving field. We begin by establishing the conceptual foundations of VLA systems, tracing their evolution from cross-modal learning architectures to generalist agents that tightly integrate vision-language models (VLMs), action planners, and hierarchical controllers. Our methodology adopts rigorous literature review framework, covering over 80 VLA models published in the past three years. Key progress areas include architectural innovations, parameterefficient training strategies, and real-time inference accelerations. We explore diverse application domains such as humanoid robotics, autonomous vehicles, medical and industrial robotics, precision agriculture, and augmented reality navigation. The review further addresses major challenges across real-time control, multimodal action representation, system scalability, generalization to unseen tasks, and ethical deployment risks. Drawing from the state-of-the-art, we propose targeted solutions including agentic AI adaptation, cross-embodiment generalization, and unified neuro-symbolic planning. In our forward-looking discussion, we outline future roadmap where VLA models, VLMs, and agentic AI converge to power socially aligned, adaptive, and general-purpose embodied agents. This work serves as foundational reference for advancing intelligent, real-world robotics and artificial general intelligence. Keywords: Vision-Language-Action, VLA, Artificial Intelligence, Robotics, Vision-Language Models, AI Agents, Agentic AI"
        },
        {
            "title": "Contents",
            "content": "1 Introduction 2 Concepts of Vision-Language-Action Models . 2.1 Evolution and Timeline . . . . 2.2 Multimodal Integration: From Isolated Pipelines to Unified Agents . . 2.3 Tokenization and Representation: How VLAs Encode the World . . . 2.4 Learning Paradigms: Data Sources and Training Strategies . . . . 2.5 Adaptive Control and Real-Time Execution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2 3 4 4 5 8 10 . . . . . . . . . . . . . . . . . . . . . . . 3 Progress in Vision-Language-Action Models 10 3.1 Architectural Innovations in VLA Models . 10 3.2 Training and Efficiency Advancements in VisionLanguageAction Models 15 3.3 Parameter-Efficient Methods and Acceleration Techniques in VLA Models 15 . 16 . 3.4 Applications of Vision-Language-Action Models . 17 . . . 18 . . . 19 . . 20 . . . 21 Precision and Automated Agriculture . . . 21 Interactive AR Navigation with Vision-Language-Action Models . 3.4.1 Humanoid Robotics 3.4.2 Autonomous Vehicle Systems . 3.4.3 . 3.4.4 Healthcare and Medical Robotics 3.4.5 3.4. Industrial Robotics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 2 0 2 ] . [ 1 9 6 7 4 0 . 5 0 5 2 : r 4 Challenges and Limitations of Vision-Language-Action Models . 4.1 Real-Time Inference Constraints . . 4.2 Multimodal Action Representation and Safety Assurance . . 4.3 Dataset Bias, Grounding, and Generalization to Unseen Tasks 4.4 System Integration Complexity and Computational Demands . . 4.5 Robustness and Ethical Challenges in VLA Deployment . . . . . . . . . . . . . . . . 5 Discussion 5.1 Potential Solutions 5.2 Future Roadmap . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 Conclusion Ranjan Sapkota Email address: rs2672@cornell.edu (Manoj Karkee) Preprint submitted to Proceedings of the IEEE . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22 23 23 24 24 25 25 25 28 Figure 1: Evolution from Isolated Modalities to Unified Vision-LanguageAction Models. This figure illustrates the transition from separate vision, language, and action systems-each limited to its own domain-to integrated VLA models. VLA models enable robots to jointly perceive, understand language, and act, overcoming the fragmentation of earlier approaches and marking major step toward adaptive, generalizable, and intelligent embodied agents. May 9, 2025 1. Introduction Before Vision-Language-Action (VLA) models were developed, progress in robotics and artificial intelligence happened mostly in separate areas: vision systems that could see and recognize images [44, 69], language systems that could understand and generate text [164, 137], and action systems that could control movement [49]. These systems worked well on their own, but they struggled to work together or handle new and unpredictable situations[46, 21]. As result, erstand complex environments or respond flexibly to real-world challenges. As illustrated in Figure 1, traditional computer vision models primarily based on convolutional neural networks (CNNs) , were tailored for narrowly specified tasks such as object detection or classification, requiring extensive labeled datasets and cumbersome retraining for even slight shifts in environment or objectives [156, 62]. These vision models could see (e.g., identifying apples in an orchard, as shown in Figure 1) but lacked any understanding of language or the ability to convert visual insights into purposeful actions. Language models, particularly large language models (LLMs), revolutionized text-based understanding and generation [23]; however, they remained restricted to processing language without the capability to perceive or reason about the physical world [76] (Ripe apples in orchard in Figure 1 exemplifies this limitation). Meanwhile, action-based systems in robotics, relying heavily on hand-crafted policies or reinforcement learning [122], enabled specific behaviors like object manipulation but demanded painstaking engineering and failed to generalize beyond narrowly scripted scenarios [119]. Despite progress with VLMs, which achieved impressive multimodal understanding by combining vision and language [149, 25, 148], there remained conspicuous integration gap: the inability to generate or execute coherent actions based on multimodal input [121, 107]. As further visualized in Figure 1 , most AI systems specialized at most in two modalitiesvisionlanguage, vision-action, or language-actionbut struggled to fully integrate all three into unified, end-to-end framework. Consequently, robots could recognize objects visually (apple), understand corresponding textual instruction (pick the apple), or perform predefined motor action (grasping), yet orchestrating these abilities into fluid, adaptable behavior was beyond reach. The result was fragmented pipeline architecture that could not flexibly adapt to new tasks or environments, leading to brittle generalization and labor-intensive engineering efforts. This highlighted critical bottleneck in embodied AI: without systems that could jointly perceive, understand, and act, intelligent autonomous behavior remained an elusive goal. The pressing need to bridge these gaps catalyzed the emergence of VLA models. VLA models, conceptualized around 2021-2022, and pioneered by efforts such as Google DeepMinds Robotic Transformer 2 (RT-2) [224], introduced transformative architecture that unified perception, reasoning, and control within single framework. As solution to the limitations outlined in Figure 1, VLAs integrate vision inputs, language comprehension, and motor control capabilities, enabling embodied agents to perceive their surroundings, understand complex instructions, and execute appropriate actions dynamically. Early VLA approaches achieved this integration by extending vision-language models to include action tokensnumerical or symbolic representations of robot motor commands, thereby allowing the model to learn from paired vision, language, and trajectory data [121]. This methodological innovation dramatically improved robots ability to generalize to unseen objects, interpret novel language commands, and perform multi-step reasoning in unstructured environments [83]. VLA models represent transformative step in the pursuit of unified multimodal intelligence, overcoming the long-standing limitations of treating vision, language, and action as separate domains [121]. By leveraging internet-scale datasets that integrate visual, linguistic, and behavioral information, VLAs empower robots to not only recognize and describe their environments but also to reason contextually and execute appropriate actions in complex, dynamic settings [196]. The progression illustrated in Figure 1 from isolated vision, language, and action systems to an integrated VLA paradigm-captures fundamental shift toward the development of truly adaptive and generalizable embodied agents. Given the profound implications of this innovation, it is crucial to undertake thorough and systematic review that draws from comprehensive body of literature and critical analysis. First, such review is necessary to clarify the foundational concepts and architectural principles that distinguish VLAs from their predecessors. Second, it provides structured account of the rapid progress and key milestones in the field, enabling researchers and practitioners to appreciate the trajectory of technological advancements. Third, an in-depth review is essential for mapping the diverse range of real-world applications-from household robotics to industrial automation and assistive technologies-where VLAs are already demonstrating transformative potential. Furthermore, by critically examining the current challenges, such as data efficiency, safety, generalization, and ethical considerations, the review identifies barriers that must be addressed for widespread deployment. And Fifth, synthesizing these insights helps to inform the broader AI and robotics communities about emerging research directions and practical considerations, fostering collaboration and innovation. In this review, we systematically analyze the foundational principles, developmental progress, and technical challenges associated with VLA models. Our objective is to consolidate the current understanding of VLAs while identifying limitations and proposing future directions for their evolution. The review begins with detailed examination of key conceptual foundations (Figure 2), including what constitutes VLA model, its historical evolution, multimodal integration mechanisms, and language-based tokenization and encoding strategies. These conceptual components set the stage for understanding how VLAs are structured and function across modalities. Building upon this, we present unified view of recent progress and training efficiency strategies (Figure 3). This includes architectural innovations that have enabled more capable and generalizable VLA models, as well as data-efficient learning frameworks, parameter-efficient modeling techniques, and"
        },
        {
            "title": "Learning\nParadigms",
            "content": "Parameter Efficiency Data-Efficient Learning Acceleration Methods Tokenization & Encoding"
        },
        {
            "title": "Concepts",
            "content": "Applications of VLAs Progress & Training Efficiency"
        },
        {
            "title": "Evolution\nTimeline",
            "content": "Foundation: What are VLAs Architectural Innovations Figure 2: Mindmap for VLA Concepts. This diagram outlines the foundational components of Vision-Language-Action models, including their definitions, historical development, integration of multimodal signals, tokenization techniques, and adaptive execution. It sets the conceptual stage for understanding the structure and purpose of VLAs. model acceleration strategies designed to reduce computational overhead without compromising performance. These advancements are critical for scaling VLA systems to real-world applications. Following this, we delve into comprehensive discussion of the current limitations faced by VLA systems (Figure 4). These include inference bottlenecks, safety concerns, high computational demands, limited generalization, and ethical implications. We not only highlight these pressing challenges but also provide an analytical discussion of potential solutions to address them. Together, these three figures offer visual framework that supports the textual analysis of this review. By outlining the conceptual landscape, recent innovations, and open challenges, this work aims to guide future research and encourage the development of more robust, efficient, and ethically grounded VLA systems. 2. Concepts of Vision-Language-Action Models VLA models represent new class of intelligent systems that jointly process visual inputs, interpret natural language, and generate executable actions in dynamic environments. Technically, VLAs combine vision encoders (e.g., CNNs, ViTs), language models (e.g., LLMs, transformers), and policy modules or planners to achieve task-conditioned control. These Figure 3: Mindmap VLA Progress and Training Efficiency. This figure combines architectural advancements with training optimization methods such as data-efficient learning, parameter reduction, and model acceleration. It provides visual summary of the technical progress driving the scalability and real-world deployment of VLAs. Environment Robustness Ethics & Society Challenges Inference Constraints Multimodal Action Generalization Gaps Compute Demands System Complexity Safety & Bias Figure 4: Mindmap VLA Challenges. This diagram highlights key barriers to robust VLA deployment, including inference limitations, bias, system complexity, generalization gaps, and ethical concerns. It also motivates the need for innovative solutions and future research directions to overcome these challenges. models typically employ multimodal fusion techniquessuch as cross-attention, concatenated embeddings, or token unificationto align sensory observations with textual instructions. 3 Unlike traditional visuomotor pipelines, VLAs support semantic grounding, enabling context-aware reasoning, affordance detection, and temporal planning. typical VLA model observes the environment through camera or sensor data, interprets goals expressed in language (e.g., pick up the red apple) (Figure 5), and outputs low-level or high-level action sequences. Recent advancements integrate imitation learning, reinforcement learning, or retrieval-augmented modules to improve sample efficiency and generalization. This review examines how VLA models have evolved from foundational fusion architectures to general-purpose agents capable of real-world deployment across robotics, navigation, and human-AI collaboration. VLA models are multimodal artificial intelligence systems that unify visual perception, language comprehension, and physical action generation into single framework. These models enable robots or AI agents to interpret sensory inputs (e.g., images, text), understand contextual meaning, and autonomously execute tasks in real-world environments-all through end-to-end learning rather than isolated subsystems. As shown conceptually in Figure 5, VLA models bridge the historical disconnect between visual recognition, language comprehension, and motor execution that limited the capabilities of earlier robotic and AI systems. 2.1. Evolution and Timeline The rapid development of VLA models from 2022-2025 demonstrates three distinct evolutionary phases: 1. Foundational Integration (20222023). Early VLAs established basic visuomotor coordination through multimodal fusion architectures. [157] first combined CLIP embeddings with motion primitives, while [141] demonstrated generalist capabilities across 604 tasks. [18] achieved 97% success rates in manipulation through scaled imitation learning, and [86] introduced temporal reasoning via transformer-based planners. By 2023, [224] enabled visual chain-of-thought reasoning, and [34] advanced stochastic action prediction through diffusion processes. These foundations addressed low-level control but lacked compositional reasoning [216], prompting innovations in affordance grounding [78]. 2. Specialization and Embodied Reasoning (2024). Second-generation VLAs incorporated domain-specific inductive biases. [202] enhanced few-shot adaptation through retrieval-augmented training, while [210] opti- [39] mized navigation via 3D scene-graph integration. introduced reversible architectures for memory efficiency, and [183] addressed partial observability with physics-informed attention. Simultaneously, [5] improved compositional object-centric disentanglement, and [220] extended applications to autonomous driving via multi-modal sensor fusion. These advances required new benchmarking methodologies [196]. understanding through 3. Generalization and Safety-Critical Deployment (2025). Current systems prioritize robustness and human alignment. [205] integrated formal verification for risk-aware 4 decisions, while [42] demonstrated whole-body control through hierarchical VLAs. [19] optimized compute efficiency for embedded deployment, and [102] combined neural-symbolic reasoning for causal inference. Emerging paradigms like [100]s affordance chaining and [13]s simto-real transfer learning address cross-embodiment challenges, while [108] bridges VLAs with human-in-the-loop interfaces through natural language grounding. Figure 6 presents comprehensive timeline highlighting the evolution of 47 VLA models developed between 2022 and 2025. The earliest VLA systems, including CLIPort [157], laid the foundaGato [141], RT-1 [18], and VIMA [86], tion by combining pretrained vision-language representations with task-conditioned policies for manipulation and control. These were followed by ACT [216], RT-2 [224], and VoxPoser [78], which integrated visual chain-of-thought reasoning and affordance grounding. Models like Diffusion Policy [34] and Octo [167] introduced stochastic modeling and scalable data pipelines. In 2024, systems such as Deer-VLA [202], ReVLA [39], and Uni-NaVid [210] added domain specialization and memory-efficient designs, while Occllama [183] and ShowUI [108] tackled partial observability and user interaction. The trajectory continued with robotics-focused VLAs like Quar-VLA [43] and RoboMamba [111]. Recent innovations emphasize generalization and deployment: SafeVLA [205], Humanoid-VLA [42], and MoManipVLA [190] incorporate verification, full-body control, and memory systems. Models such as Gr00t N1 [13] and SpatialVLA [136] further bridge sim-to-real transfer and spatial grounding. This timeline illustrates how VLAs have advanced from modular learning to general-purpose, safe, and embodied intelligence. 2.2. Multimodal Integration: From Isolated Pipelines to Unified Agents central advancement in the emergence of VLA models lies in their ability to perform multimodal integration, the joint processing of vision, language, and action within unified architecture. Traditional robotic systems treated perception, natural language understanding, and control as discrete modules, often linked through manually defined interfaces or data transformations [109, 20, 168]. For instance, classic pipelinebased frameworks required perception model to output symbolic labels, which were then mapped by planner to specific actionsfrequently with domain-specific hand engineering [138, 90]. These approaches lacked adaptability, failed in ambiguous or novel environments, and could not generalize instructions beyond pre-encoded templates. In contrast, modern VLAs fuse modalities end-to-end using large-scale pretrained encoders and transformer-based architectures [188]. This shift enables the model to interpret visual observations and linguistic instructions within the same computational space, allowing flexible, context-aware reasoning [99]. For example, in the task Pick up the red ripe apple, (Figure 5) the vision encodertypically Vision Transformer (ViT) or ConvNeXtsegments and classifies objects in the scene (e.g., Figure 5: Foundational Concept of VLA Models (in an Apple-Picking Scenario) This illustration depicts robotic arm autonomously picking ripe apple in an orchard, guided by VLA model. On the right, flowchart outlines the four key stages of VLA models: Multimodal Integration, Tokenization and Representation, Learning Paradigms, and Adaptive Control and Real-Time Execution. apples, leaves, background), identifying color and ripeness attributes [187]. Meanwhile, the language model, often variant of T5, GPT, or BERT, encodes the instruction into highdimensional embedding. These representations are then fused via cross-attention or joint tokenization schemes, producing unified latent space that informs the action policy [68]. This multimodal synergy was first effectively demonstrated in CLIPort [157], which used CLIP embeddings for semantic grounding and convolutional decoder for pixel-level manipulation. CLIPort bypassed the need for explicit language parsing and directly conditioned visuomotor policies on natural language. Similarly, VIMA [86] advanced this approach by employing transformer encoder to jointly process object-centric visual tokens and instruction tokens, enabling few-shot generalization across spatial reasoning tasks. Recent developments push this fusion further by incorporating temporal and spatial grounding. VoxPoser [78] employs voxel-level reasoning to resolve ambiguities in 3D object selection, while RT-2 [224] fuses visual-language tokens into unified transformer that supports zero-shot generalization to unseen instructions. Another noteworthy contribution is Octo [167], which introduces memory-augmented transformer that enables long-horizon decision-making across diverse scenes, demonstrating the scalability of joint perceptionlanguage-action learning. Crucially, VLAs offer robust solutions to challenges in realworld grounding. For example, Occllama [183] handles occluded object references through attention-based mechanisms, while ShowUI [108] demonstrates natural language interfaces that allow non-expert users to command agents through voice or typed input. These capabilities are only possible because the integration is not limited to surface-level fusion; rather, it captures semantic, spatial, and temporal alignment across modalities. 2.3. Tokenization and Representation: How VLAs Encode the"
        },
        {
            "title": "World",
            "content": "A core innovation that sets VLA models apart from conventional vision-language architectures lies in their token-based representation framework, which enables holistic reasoning linguistic, and physical action over perceptual [125, 215], spaces [106]. Inspired by autoregressive generative models like transformers, modern VLAs encode the world using discrete tokens that unify all modalitiesvision, language, state, and action into shared embedding space [110]. This allows the model to not only understand what needs to be done (semantic reasoning), but also how to do it (control policy execution) in fully learnable and compositional way [192, 117, 170]. Prefix Tokens: Encoding Context and Instruction:Prefix tokens serve as the contextual backbone of VLA models [195, 83]. These tokens encode the environmental scene (via images or video) and the accompanying natural language instruction into compact embeddings that prime the models internal representations [16]. For instance, as depicted in Figure 7in task such as stack the green blocks on the red tray, the image of cluttered tabletop is processed through vision encoder like ViT or ConvNeXt, while the instruction is embedded by large language model (e.g., T5 or LLaMA). These are then 5 CLIPort [157] Gato [141] RT-1 [18] VIMA [86] ACT [216] RT-2 [224] VoxPoser [78] Diffusion Policy [34] Octo [167] OpenVLA [94] 2022 2023 2024 Deer-VLA [202] Uni-NaVid [210] ReVLA [39] Occllama [183] Pi-0 [14] RDT-1B [112] CogAct [102] EdgeVLA [19] 2024 ShowUI [108] NaviLa [32] Quar-VLA [43] Bi-VLA [59] RoboMamba [111] Otter [75] PointVLA [96] CombatVLA [29] HybridVLA [110] 2025 CoVLA [5] OpenDriveVLA [220] ORION [56] ObjectVLA [223] ConRFT [31] Hi Robot [155] TLA [70] RaceVLA [153] 2025 DexVLA [185] Humanoid-VLA [42] SafeVLA [205] MoManipVLA [190] VLA-Cache [195] TinyVLA [186] Gr00t N1 [13] NORA [79] SpatialVLA [136] MoLe-VLA [213] Figure 6: Comprehensive timeline of Vision-Language-Action models (20222025), showing evolution from foundation to 45 specialized VLA systems. Organized chronologically with thematic grouping. transformed into sequence of prefix tokens that establish the models initial understanding of the goal and environmental layout. This shared representation enables crossmodal grounding, allowing the system to resolve spatial references (e.g., on the left, next to the blue cup) and object semantics (green blocks) across both modalities. State Tokens: Embedding the Robots Configuration: In addition to perceiving external stimuli, VLAs must be aware of their internal physical state [186, 111]. This is achieved through the use of state tokens, which encode real-time information about the agents configurationjoint positions, force-torque readings, gripper status, end-effector pose, and even the locations of nearby objects [97]. These tokens are crucial for ensuring situational awareness and safety, especially during manipulation or locomotion [163, 81]. Figure 8 illustrates how VLA models utilize state tokens to enable dynamic, context-aware decision-making in both manipulation and navigation settings. In Figure 8a, robot arm is shown partially extended near fragile object. In such scenarios, state tokens play critical role by encoding real-time proprioceptive information, such as joint angles, gripper pose, and end-effector proximity. These tokens are continuously fused with visual and languagebased prefix tokens, allowing the transformer to reason about physical constraints. The model can thus infer that 6 sor data. These are essential for terrain-aware locomotion and obstacle avoidance. The transformer model integrates this state representation with environmental and instructional context to generate navigation actions that dynamically adapt to changing surroundings. Whether grasping objects in cluttered environments or autonomously navigating uneven terrain, state tokens provide structured mechanism for situational awareness, enabling the autoregressive decoder to produce precise, context-informed action sequences that reflect both internal robot configuration and external sensory data. Action Tokens: Autoregressive Control Generation: The final layer of the VLA token pipeline involves action tokens [93, 94], which are autoregressively generated by the model to represent the next step in motor control [186]. Each token corresponds to low-level control signal, such as joint angle updates, torque values, wheel velocities, or high-level movement primitives [64]. During inference, the model decodes these tokens one step at time, conditioned on prefix and state tokens, effectively turning VLA models into language-driven policy generators [54, 161]. This formulation allows seamless integration with real-world actuation systems, supports variablelength action sequences [10, 77], and enables model finetuning via reinforcement or imitation learning frameworks [214]. Notably, models like RT-2 [224] and PaLM-E [47] exemplify this design, where perception, instruction, and embodiment are merged into unified token stream. For instance, in the apple-picking task as depicted in Figure 9, the model may receive prefix tokens that include the image of the orchard and the text instruction. The state tokens describe the robots current arm posture and whether the gripper is open or closed. Action tokens are then predicted step by step to guide the robotic arm toward the apple, adjust the gripper orientation, and execute grasp with appropriate force. The beauty of this approach is that it allows transformers, which are traditionally used for text generation, to now generate sequences of physical actions in manner similar to generating sentenceonly here, the sentence is the motion. To operationalize the VLA paradigm in robotics, we present in Figure 9 structured pipeline that demonstrates how multimodal informationspecifically vision, language, and proprioceptive stateis encoded, fused, and converted into executable action sequences. This end-to-end loop allows robot to interpret complex tasks like pick the ripe apple near the green leaf and execute precise, context-sensitive manipulations. The system begins with multimodal input acquisition, where three distinct data streams are collected: visual observations (e.g., RGB-D frames), natural language commands, and real-time robot state information (e.g., joint angles or velocity). These are independently tokenized into discrete embeddings using pretrained modules [41, 212]. As depicted in the diagram, the image is processed through Vision Transformer (ViT) backbone to generate vision tokens, the instruction is parsed by Figure 7: The diagram illustrates the end-to-end tokenization and representation process in VLA models. Visual input (e.g., cluttered tabletop) is encoded by vision encoder (e.g., ViT), while natural language instructions (e.g., stack the green blocks) are processed by language encoder (e.g., T5). The system fuses prefix, state, and action tokens through transformer and autoregressively predicts motor actions. Figure 8: Illustrating how VLA models utilize prefix, state, and action tokens in real-world scenarios. In robotic manipulation, state tokens detect arm extension near fragile objects, enabling path adjustment. In navigation, they represent LiDAR and odometry data. The apple-picking task shows how prefix tokens guide goal understanding, while action tokens generate motion sequences for targeted grasping and execution. collision is imminent and adjust the motor commands accordinglye.g., rerouting the arm trajectory or moduIn mobile robotic platforms, as delating force output. picted in Figure 8b, state tokens encapsulate spatial features such as odometry, LiDAR scans, and inertial senF, capturing the semantics, intent, and situational awareness needed for grounded action. Finally, policy decoder such as FAST [133] maps the fused features into 50 discrete action tokens, which can then be decoded into motor commands τ1:N. The decoding process is implemented using transformerbased architecture, as shown in the code snippet titled Action Prediction Code. Transformer object is initialized with 12 layers, model dimension of 512, and 8 attention heads. The fused tokens are passed to the decoder, which autoregressively predicts the next most likely action token conditioned on previous tokens and context. The final motor command sequence is obtained by detokenizing the output. This implementation mirrors how text generation works in large language models, but here the sentence is motion trajectorya novel repurposing of natural language generation techniques for physical action synthesis. Together, Figure 9, Algorithm 1, and the pseudocode illustrate how VLAs unify perception, instruction, and embodiment within coherent and interpretable token space. This modularity allows the framework to generalize across tasks and robot morphologies, facilitating rapid deployment in real-world applications like apple picking, household tasks, and mobile navigation. Importantly, the clarity and separability of the tokenization steps make the architecture extensible, enabling further research on token learning, hierarchical planning, or symbolic grounding in VLA systems. Algorithm 1 VLA Tokenization Pipeline 1: Input: RGB-D frame I, text command , joint angles θ 2: ViT(I) 3: BERT(T ) 4: MLP(θ) 5: CrossAttention(V, L, ) 6: FAST(F) 7: Output: Motor commands τ1:N 400 vision tokens 12 language tokens 64-dim state encoding 512-dim fused token 50 action tokens"
        },
        {
            "title": "Action Prediction Code",
            "content": "# Python-like pseudocode def predict_actions(fused_tokens): transformer = Transformer( num_layers=12, d_model=512, nhead=8 ) action_tokens = transformer.decode( fused_tokens, memory=fused_tokens ) return detokenize(action_tokens) 2.4. Learning Paradigms: Data Sources and Training Strategies Training VLA models requires hybrid learning paradigm that integrates both semantic knowledge from the web and taskgrounded information from robotics datasets [30]. As shown in 8 Figure 9: Illustrating the process of how VLAs Encode the World. VLAs encode the world by converting vision, language, and sensor inputs into tokens, fusing them through cross-attention, predicting action sequences via transformers, and executing tasks with real-time feedback-enabling robots to interpret scenes, follow instructions, and adapt actions dynamically. language model such as BERT or T5 to produce language tokens, and state inputs are transformed via lightweight MLP encoder into compact state tokens. These tokens are then fused using cross-modal attention mechanism, where the model jointly reasons over object semantics, spatial layout, and physical constraints [61]. This fused representation forms the contextual basis for decision-making [74, 116]. In Figure 9, this is denoted as the multimodal fusion step. The fused embedding is passed into an autoregressive decodertypically transformerthat generates series of action tokens. These tokens may correspond to joint displacements, gripper force modulation, or high-level motor primitives (e.g., move to grasp pose, rotate wrist). The action tokens are subsequently translated into control commands and passed to the execution loop, which closes the perception-action cycle by feeding back the robots updated state, thus informing the next inference step. This closed-loop mechanism enables the model to dynamically adapt to perturbations, object shifts, or occlusions in real time [206, 120, 194]. To offer concrete implementation details, Algorithm 1 formalizes the VLA tokenization process. Given an RGB-D frame I, natural language instruction , and joint angle vector θ, the algorithm produces set of action tokens that can be executed in sequence. The image is processed via ViT to produce V, set of 400 visual tokens. In parallel, the instruction is encoded by BERT model to yield L, sequence of 12 semantic language tokens. Simultaneously, robot state θ is passed through multilayer perceptron to generate 64-dimensional state embedding . These tokens are then fused via cross-attention module to produce shared 512-dimensional representation Figure 10: Learning Paradigms: Data Sources and Training Strategies for VLAs. prior sections, the multimodal architecture of VLAs must be exposed to diverse forms of data that support language understanding, visual recognition, and motor control. This is typically achieved through two primary data sources. First, as depicted in figure 10, large-scale internet-derived corpora form the backbone of the models semantic prior. These datasets include image-caption pairs (e.g., COCO, LAION400M), instruction-following datasets (e.g., HowTo100M, WebVid), and visual question-answering corpora (e.g., VQA, GQA). Such datasets enable pretraining of the visual and language encoders, helping the model acquire general representations of objects, actions, and concepts [2]. This phase often uses contrastive or masked modeling objectives, such as CLIP-style contrastive learning or language modeling losses, to align vision and language modalities within shared embedding space [146, 199]. Importantly, this stage gives VLAs foundational understanding of the world that facilitates compositional generalization, object grounding, and zero-shot transfer [28, 15]. However, semantic understanding alone is insufficient for the second physical task execution [36, 178, 107]. Thus, phase focuses on grounding the model in embodied experience [178]. Robot trajectory datasetscollected either from realworld robots or high-fidelity simulatorsare used to teach the model how language and perception translate into action [54]. These include datasets like RoboNet [37], BridgeData [50], and RT-X [175], which provide video-action pairs, joint trajectories, and environment interactions under natural language instructions [123]. Demonstration data may come from kinesthetic teaching, teleoperation, or scripted policies [89, 12]. This phase typically employs supervised learning (e.g., behavior cloning) [55], reinforcement learning (RL), or imitation learning to train the autoregressive policy decoder to predict action tokens based on fused visual-language-state embeddings [65]. Recent works increasingly adopt multistage or multitask training strategies. For example, models are often pretrained on vision-language datasets using masked language modeling, then fine-tuned on robot demonstration data using token-level autoregressive loss [94, 221, 195]. Others use curriculum learning, where simpler tasks (e.g., object pushing) precede more complex ones (e.g., multistep manipulation) [217]. Some approaches further leverage domain adaptation such as in OpenVLA [94] or sim-to-real transfer to bridge the gap between synthetic and real-world distributions [96]. By unifying semantic priors with task execution data, these learning paradigms allow VLA models to generalize across tasks, domains, and embodimentsforming the backbone of scalable, instructionfollowing agents capable of robust real-world operation. Through co-fine-tuning, these datasets are brought into alignment [179, 52]. The model learns to map from visual and linguistic inputs to appropriate action sequences [136]. This training paradigm not only helps the model understand object affordances (e.g., apples can be grasped) and action outcomes (e.g., lifting requires force and trajectory), but also promotes generalization to novel scenarios [100]. model trained on kitchen manipulation tasks may be able to infer how to pick an apple in an outdoor orchard if it has learned general principles of object localization, grasping, and following language directives. Recent architectures, such as Google DeepMinds RT-2 (Robotic Transformer 2) [224], have demonstrated this principle in action. RT-2 treats action generation as form of text generation, where each action token corresponds to discrete command in robots control space. Because the model is 9 trained on both web-scale multimodal data and thousands of robot demonstrations, it can flexibly interpret novel instructions and perform zero-shot generalization to new objects and taskssomething that was largely impossible with traditional control systems or even early multimodal models. bowl. Additionally, UC Berkeleys Octo model (2023) introduced an open-source approach with 93M parameters and diffusion decoders, trained on 800,000 robot demonstrations from the OpenX-Embodiment Dataset, further broadening the research landscape [167]. 2.5. Adaptive Control and Real-Time Execution 3.1. Architectural Innovations in VLA Models Another strength of VLAs lies in their ability to perform adaptive control, using real-time feedback from sensors to adjust behavior on the fly [153]. This is particularly important in dynamic, unstructured environments like orchards, homes, or hospitals, where unexpected changes (e.g., wind moving an apple, lighting changes, human presence) can alter the task parameters. During execution, state tokens are updated in real time, reflecting sensor inputs and joint feedback [195]. The model can then revise its planned actions accordingly. For instance, in the apple-picking scenario, if the target apple shifts slightly or another apple enters the field of view, the model dynamically reinterprets the scene and adjusts the grasp trajectory. This capability mimics human-like adaptability and is core advantage of VLA systems over pipeline-based robotics. 3. Progress in Vision-Language-Action Models The inception of VLA models was catalyzed by the remarkable success of transformer-based LLMs, notably ChatGPT, released in November 2022, which demonstrated unprecedented semantic reasoning capabilities (ChatGPT) [139]. This breakthrough inspired researchers to extend language models to multimodal domains, integrating perception and action for robotics. By 2023, GPT-4 introduced multimodal capabilities, processing both text and images, which spurred efforts to incorporate physical actions (GPT-4) [1]. Concurrently, VLMs like CLIP (2022) [157] and Flamingo (2022) [3] had established robust visualtext alignment through contrastive learning, enabling zero-shot object recognition and laying the groundwork for VLA models (CLIP). These models leveraged large-scale web datasets to align images with textual descriptions, critical precursor to integrating actions. pivotal development was the creation of large-scale robotic datasets, such as RT-1s 130,000 demonstrations, which provided action-grounding data essential for co-training vision, language, and action components [18]. These datasets captured diverse tasks and environments, enabling models to learn generalizable behaviors. Architectural breakthroughs followed with Googles RT-2 in 2023 [17], landmark VLA model that unified vision, language, and action tokens, treating robotic control as an autoregressive sequence prediction task (RT-2 Blog). RT2 discretized actions using Discrete Cosine Transform (DCT) compression and Byte-Pair Encoding (BPE), achieving 63% improvement in performance on novel objects. Multimodal fusion techniques, such as cross-attention transformers, integrated Vision Transformer (ViT)-processed images (e.g., 400 patch tokens) with language embeddings, enabling robots to execute complex commands like Pick the red cup left of the From 2023 to 2024, VLA models underwent significant architectural advancements and refined training methodologies. Dual-system architectures emerged as key innovation, exemplified by NVIDIAs Groot N1 (2025) [13], which combined System 1 (fast diffusion policies with 10ms latency for lowlevel control) and System 2 (LLM-based planners for highlevel task decomposition). This separation enabled efficient coordination between strategic planning and real-time execution, enhancing adaptability in dynamic environments. Other models, like Stanfords OpenVLA (2024) [94], introduced 7B-parameter open-source VLA trained on 970k real-world robot demonstrations, using dual vision encoders (DINOv2 [128] and SigLIP [204]) and Llama 2 language model [172], outperforming larger models like RT-2-X (55B) [94]. Training paradigms evolved to leverage co-fine-tuning on web-scale vision-language data (e.g., LAION-5B) [152] and robotic trajectory data (e.g., RT-X) [175], aligning semantic knowledge with physical constraints [152]. Synthetic data generation tools like UniSim addressed data scarcity by creating photorealistic scenarios, such as occluded objects, crucial for robust training (UniSim [200]). Parameter efficiency was enhanced through Low-Rank Adaptation (LoRA) adapters [72], which allowed domain adaptation without full retraining, reducing GPU hours by 70%. The introduction of diffusion-based policies, as seen in Physical Intelligences pi 0 model (2024) [14], offered improved action diversity but required significant computational resources. These advancements democratized VLA technology, fostering collaboration and accelerating innovation. Recent VLA models have converged toward three major architectural paradigms that balance efficiency, modularity, and robustness: early fusion models, dual-system architectures, and self-correcting frameworks. Each of these innovations addresses specific challenges in grounding, generalization, and action reliability in real-world robotic systems. 1. Early Fusion Models: One class of approaches focuses on fusing vision and language representations at the input stage before passing them to the policy module. Huang et al.s EFVLA model [74], presented at ICLR 2025, exemplifies this trend by retaining the representational alignment established by CLIP [157]. EF-VLA accepts image-text pairs, encodes them with CLIPs frozen encoders, and fuses the resulting embeddings early in the transformer backboneprior to action prediction. This design ensures that the semantic consistency learned during CLIP pretraining is preserved, reducing overfitting and enhancing generalization. Notably, EF-VLA demonstrated 20% performance improvement on compositional manipulation tasks and reached 85% success on previously unseen goal descriptions. By avoiding fine-tuning of the vision-language modules, this approach also preserves computational efficiency and 10 prevents catastrophic forgetting during domain-specific training. 2. Dual-System Architectures: Inspired by dual-process theories of human cognition, models like NVIDIAs Groot N1 (2025) [13] implement two complementary subsystems: fastreactive module (System 1) and slow-reasoning planner (System 2). System 1 comprises diffusion-based control policy that operates at 10 ms latency, ideal for fine-grained, low-level control such as end-effector stabilization or adaptive grasping. In contrast, System 2 uses LLM for task planning, skill composition, and high-level sequencing. The planner parses longhorizon goals (e.g., clean the table) into atomic subtasks, while the low-level controller ensures real-time execution. This decomposition enables multi-timescale reasoning and improved safety, especially in environments where rapid reaction and deliberation must co-exist. In benchmark tests on multi-stage household manipulation, Groot N1 outperformed monolithic models by 17% in success rate and reduced collision failures by 3. Self-Correcting Frameworks: third architectural evolution is the development of self-correcting VLA models, designed to detect and recover from failure conditions without external supervision. SC-VLA (2024) introduces hybrid execution loop featuring fast inference path and slow correcIn this framework, the default behavior is to pretion path. dict poses or actions directly from the fused embedding using lightweight transformer. When failures are detectede.g., unsuccessful grasps or obstacle collisionsthe model invokes secondary process that performs chain-of-thought reasoning [211, 203]. This path queries an internal LLM (or external expert system) to diagnose failure modes and generate correction strategies [48]. For example, if the robot repeatedly misidentifies an occluded object, the LLM may suggest an active viewIn closed-loop experipoint change or gripper reorientation. ments, SC-VLA reduced task failure rates by 35% and significantly improved recoverability in cluttered and adversarial environments. VLA models exhibit rich diversity of architectural designs and functional emphases, which can be systematically organized along the dimensions of end-to-end versus modular pipelines, hierarchical versus flat policy structures, and the balance between low-level control and high-level planning  (Table 1)  . End-to-end VLAs, such as CLIPort [157], RT-1 [18], and OpenVLA [94], process raw sensory inputs directly into motor commands via single unified network. By contrast, component-focused models like VLATest [182] and Chain-ofAffordance [100] decouple perception, language grounding, and action modules, enabling targeted improvements in individual submodules. Hierarchical architectures have emerged to tackle complex, long-horizon tasks by separating strategic decision making from reactive control. For instance, CogACT [102] and NaVILA [32] employ two-tier hierarchy where an LLM-based planner issues subgoals to low-level controller, thereby combining the strengths of System 2 reasoning and System 1 execution. Similarly, ORION [56] integrates QT-Former for longterm context aggregation with generative trajectory planner in cohesive framework. Low-level policy emphasis is typified by diffusion-based controllers (e.g. Pi-0 [14], DexGraspVLA [219]), which excel at producing smooth, diverse motion distributions but ofIn contrast, high-level ten incur higher computational cost. planners (e.g. FAST Pi-0 Fast [133], CoVLA [5]) focus on rapid subgoal generation or coarse trajectory prediction, delegating fine-grained control to specialized modules or traditional motion planners. End-to-end dual-system models like HybridVLA [110] and Helix [166] blur these distinctions by jointly training both components while preserving modular interpretability. Table 1 further highlights how recent VLAs balance these trade-offs. Models such as OpenDriveVLA [220] and CombatVLA [29] prioritize hierarchical planning in dynamic, safetycritical domains, whereas lightweight, edge-targeted systems like Edge VLA [19] and TinyVLA [186] emphasize real-time low-level policies at the expense of high-level reasoning. This classification framework not only clarifies the design space of VLAs but also guides future development by pinpointing underexplored combinationssuch as fully end-to-end, hierarchical models optimized for embedded deploymentthat promise to advance both the capabilities and the applicability of VLA systems across robotics, autonomous driving, and beyond. The classification in Table 1 is significant because it provides clear framework for comparing diverse VLA architectures, highlighting how design choicessuch as end-to-end integration versus hierarchical decompositionimpact task performance, scalability, and adaptability. By categorizing models along dimensions like low-level policy execution and highlevel planning, researchers can pinpoint strengths and limitations of existing approaches and identify opportunities for innovation. This taxonomy aids in selecting appropriate architectures for specific applications (e.g., real-time control vs. strategic reasoning) and guides future development toward hybrid systems that balance responsiveness with cognitive planning, ultimately accelerating progress in embodied AI. Additionally, to synthesize recent advancements in VLA models, Table 2 presents comparative summary of notable systems developed from 2022 through 2025. Building upon architectural innovations such as early fusion, dual-system processing, and selfcorrecting feedback loops, these models incorporate diverse design philosophies and training strategies. Each entry highlights the models key componentsvision and language encoders, action decodersand the datasets used to ground their capabilities. Models like CLIPort [157] and RT-2 [224] laid early foundations by aligning semantic embeddings with action policies, while more recent frameworks like Pi-Zero, CogACT [102], and Groot N1 [13] introduce scalable architectures with diffusion-based or high-frequency controllers. Several models leverage multimodal pretraining with internet-scale visionlanguage corpora and robot trajectory datasets, enhancing generalization and zero-shot capabilities [223, 219, 218, 198]. This tabulated comparison serves as reference point for researchers seeking to understand the functional diversity, domain applicability, and emerging trends in VLA design across real and simulated environments. 11 Table 2: Summary of VLA models, detailing each models name, architecture features, training dataset, and highlighting their key strengths or unique capabilities in robotics and AI tasks. Model (Reference) Architecture Components Training Dataset Key Strength / Uniqueness CLIPort [157] Vision Encoder: CLIPResNet50 + TransporterResNet Selfcollected [SC] HighLevel Planner RT-1 [18] RT2 [224] Gato [141] Language Encoder: CLIPGPT Action Decoder: LingUNet Vision Encoder: EfficientNet Language Encoder: Universal Sentence Encoder Action Decoder: Transformer Vision Encoder: ViT22B/ViT-4B Language Encoder: PaLIX/PaLM-E Action Decoder: Symboltuning Vision Encoder: ViT Language Encoder: SentencePiece Action Decoder: Transformer VIMA [86] Vision Encoder: ViT + Mask R-CNN Language Encoder: Action Decoder: Transformer ACT [216] Vision Encoder: ResNet-18 Language Encoder: Action Decoder: CVAETransformer Octo [167] Vision Encoder: CNN Language Encoder: T5base Action Decoder: Diffusion Transformer Combines semantic CLIP features with spatial Transporter network for precise SE(2) manipulation. Pioneering Transformer architecture with discretized actions for multi-task kitchen manipulation. First large VLA cofinetuned on internet VQA data and robot data for emergent capabilities. RT-1Kitchen [SC] VQA + RT-1Kitchen Selfcollected [SC] Generalist agent handling Atari, captioning, and robotics through unified tokenization. VIMAData [SC] Multi-modal prompt handling with 6 types of vision-language grounding tasks. ALOHA [SC] Temporal ensembling for smooth bimanual manipulation with 0.1mm precision. Open XEmbodiment First policy trained on 4M+ robot trajectories from 22 robot types. VoxPoser [78] Vision Encoder: ViLD + Zero-shot MDETR Language Encoder: GPT-4 Action Decoder: MPC Vision Encoder: ResNet-18 Language Encoder: Action Decoder: UNet/Transformer Selfcollected [SC] Diffusion Policy [34] LLM+VLM composition for constraintaware motion planning without training. Pioneering diffusionbased visuomotor policy handling multimodal action distributions."
        },
        {
            "title": "Continued on next page",
            "content": "Table 1: Taxonomy of VLA models showing structured classification based on architectural paradigms and scientific priorities. We differentiate models by their support for end-to-end execution, hierarchical planningcontrol decomposition, or component-focused modularity, and further by their emphasis on low-level motor policies versus high-level task planners."
        },
        {
            "title": "Year",
            "content": "EndtoEnd 2024 2024 2024 2024 2024 2024 2022 2022 2022 2022 2023 2023 2023 2023 2024 2024 2024 2024 2024 2024 CLIPort [157] RT-1 [18] Gato [141] VIMA [86] Diffusion Policy [34] ACT [216] VoxPoser [78] Seer [63] Octo [167] OpenVLA [94] CogACT [102] VLATest [182] NaVILA [32] RoboNurseVLA [103] Mobility VLA [35] RevLA [39] Uni-NaVid [210] RDT-1B [112] RoboMamba [111] Chain-ofAffordance [100] Edge VLA [19] ShowUI-2B [108] Pi-0 [14] FAST Fast) [133] 2025 OpenVLA-OFT [93] CoVLA [5] 2025 OpenDriveVLA [220] 2025 2025 ORION [56] 2025 UAV-VLA [150] 2025 CombatVLA [29] 2025 HybridVLA [110] 2025 NORA [79] 2025 SpatialVLA [136] 2025 MoLe-VLA [213] 2025 JARVIS-VLA [101] 2025 UP-VLA [209] 2025 Shake-VLA [92] 2025 DexGraspVLA [219] 2025 DexVLA [185] 2025 Humanoid-VLA [42] 2025 ObjectVLA [223] 2024 2024 2024 2025 (PiHie rarc hical Comp onent Focused LowLevel Policy 12 Architecture Components"
        },
        {
            "title": "Continued from previous page\nTraining\nDataset",
            "content": "Key Strength / Uniqueness Model (Reference) Architecture Components"
        },
        {
            "title": "Continued from previous page\nTraining\nDataset",
            "content": "Key Strength / Uniqueness Model (Reference) OpenVLA [94] Vision Encoder: DINOv2 + SigLIP OXE + DROID Open-source alternative to RT-2 with efficient LoRA fine-tuning. CogACT [102] Vision Encoder: DINOv2 ViT-L/14, SigLIP ViTSo400M/14 Language Encoder: Llama-2 (via Prismatic7B VLM) Action Decoder: Diffusion Transformer (DiT-Base, 300M parameters) Chain-ofAffordance (CoA) [100] Vision Encoder: Visual encoder with affordance feature extraction Language Encoder: Transformer-based language module for sequential reasoning prompts Action Decoder: Autoregressive and diffusion policy with affordance-conditioned outputs Vision Encoder: SigLIP + DINOv2 Language Encoder: Qwen2 (0.5B parameters) Action Decoder: Joint control prediction (nonautoregressive) Edge VLA (EVLA) [19] ShowUI2B [108] Vision Encoder: UI-Guided Visual Token Selection (transformer-based) Groot N1 [13] Language Encoder: Interleaved vision-languageaction streaming Action Decoder: Transformer for GUI action sequence prediction Vision Encoder: NVIDIA Eagle-2 VLM backbone (vision-language) Language Encoder: Integrated with VLM for high-level planning and reasoning Action Decoder: Diffusion Transformer (DiT) for precise, high-frequency action generation Open XEmbodiment (OXE) subset, realworld Realman & Franka tasks LIBERO benchmark, real and simulated manipulation tasks Bridge dataset, OXE, 1.2M textimage pairs 256K highquality GUI instructionfollowing dataset Multimodal data: human demonstrations, robot trajectories, synthetic simulation, and internet video Componentized VLA with specialized diffusion action transformer; outperforms OpenVLA by 59.1% in real-world success, excels at adaptation and generalization to new robots and unseen objects. Incorporates reasoning via sequential affordances (object, grasp, spatial, movement); achieves superior LIBERO performance over OpenVLA, excelling in spatial reasoning and obstacle avoidance for precise task completion. Lightweight VLA model optimized for edge devices (e.g., Jetson Nano) with 3050 Hz inference; achieves performance comparable to OpenVLA while enabling efficient, real-time deployment on low-power hardware. Lightweight 2Bparameter VLA specialized for digital task automation; excels at GUI/web navigation and screenshot grounding with efficient token selection and unified vision-languageaction reasoning. Hybrid dual-system architecture for generalist humanoid robots, combining high-level planning with diffusion-based execution; enables dexterous, multistep control and strong generalization across tasks and embodiments."
        },
        {
            "title": "Continued on next page",
            "content": "(PiZero) [14] Pi-0 Fast [133] OpenVLAOFT [93] Language Encoder: Prismatic-7B Action Decoder: Symboltuning Vision Encoder: PaliGemma VLM backbone Language Encoder: PaliGemma (multimodal) Action Decoder: 300Mparameter diffusion model Vision Encoder: PaliGemma VLM backbone Language Encoder: PaliGemma (multimodal) Action Decoder: Autoregressive Transformer with FAST (Frequency-space Action Sequence Tokenization) Vision Encoder: SigLIP + DINOv2 (multi-view) Language Encoder: Llama-2 7B Action Decoder: Parallel decoding with action chunking and L1 regression RDT1B [112] Vision Encoder: Multiview RGB image encoder Helix 1 Language Encoder: Transformer-based language module Action Decoder: Diffusion Transformer with unified action space Vision Encoder: Opensource VLM (System 2) for multimodal scene and language understanding at 79 Hz Language Encoder: Integrated with VLM for broad generalization and semantic comprehension Action Decoder: Transformer-based visuomotor policy (System 1) for continuous, full upper-body control at 200 Hz Pi-CrossEmbodiment Robot dataset PiCrossEmbodiment Robot dataset LIBERO benchmark, bimanual ALOHA 1M+ multirobot episodes (46 datasets), finetuned on 6K+ bimanual ALOHA episodes End-toend on Figure robot data (pixels and language to actions) Lightweight, efficient VLA model ( 3B params) excelling at general robot control and bimanual manipulation, with strong open-world generalization across diverse robots and tasks [1][5][7]. Variant of Pi-0 optimized for highfrequency, real-time control using compressed action tokens; achieves up to 15x faster inference for discrete robot actions and strong generalization. Optimized finetuning variant of OpenVLA achieving 97.1% success on LIBERO, with 26 faster inference via parallel decoding and action chunking; excels at highfrequency bimanual control. 1.2B-parameter diffusion foundation model for bimanual manipulation; excels at languageconditioned, dexterous control and zero-shot generalization, with strong but task-specific performance in multi-object settings. First VLA model for real-time, high-DoF humanoid control; enables zero-shot generalization, finegrained dexterity, and collaborative multi-robot manipulation in open-world tasks."
        },
        {
            "title": "Continued on next page",
            "content": "13 Architecture Components"
        },
        {
            "title": "Continued from previous page\nTraining\nDataset",
            "content": "Key Strength / Uniqueness Architecture Components"
        },
        {
            "title": "Continued from previous page\nTraining\nDataset",
            "content": "Key Strength / Uniqueness Model (Reference) Model (Reference) Seer [63] Vision Encoder: Visual backbone optimized for grounding and perception LIBERO benchmark Language Encoder: Transformer-based language module Action Decoder: Autoregressive action prediction head DiffusionVLA [184] Vision Encoder: Transformer-based visual encoder for contextual perception Language Encoder: Autoregressive reasoning module with next-token prediction Action Decoder: Diffusion policy head for robust action sequence generation Vision Encoder: CLIP + CNN Language Encoder: LLaMA-2 (task command + nav goal) Action Decoder: Twolevel controller: topological graph planner + RL-based locomotion Vision Encoder: SAM2 + RGB-D Language Encoder: LLaMA 2 + voice-to-text encoder Action Decoder: Joint pose regression with gripper classifier Vision Encoder: Longcontext ViT + goal image encoder Language Encoder: T5based instruction encoder Action Decoder: Graph planner with visual goal localization Vision Encoder: FastViT with low-latency encoding Language Encoder: Compact language encoder (128-d) Action Decoder: Diffusion policy decoder (50M params) NaVILA [32] RoboNurseVLA [103] Mobility VLA [35] TinyVLA [186] VLA model focused on visual perception and action prediction; achieves competitive results with strong visual grounding for manipulation, but is outperformed by OpenVLA-OFT [1][6][8][10]. Leverages diffusionbased action modeling for precise control; demonstrates robustness and interpretability, but is less generalizable than CoA in spatial configurations. LIBERO benchmark, factory sorting, zero-shot binpicking tasks Realworld legged robot nav demos Modular hierarchy enables robust terrain generalization and 88% real-world nav success using natural language Surgical handover videos and voice prompts Enables accurate, real-time surgical tool handover; strong robustness to tool novelty and dynamic OR scenes QUARVLA [43] Vision Encoder: CLIP + proprioceptive embedding Language Encoder: BERT + custom grounding adapter Action Decoder: Transformer for full-body command decoding ChatVLA [221] Vision Encoder: Vision encoder integrated with Phase-Aligned transformer Language Encoder: Prismatic LLM with Mixtureof-Experts Action Decoder: Unified vision-language-action planner Vision Encoder: CLIP + 3D Point Cloud (via skip blocks) Language Encoder: LLaMA-2 Action Decoder: Transformer with spatial token fusion Vision Encoder: SigLIP with token memory buffer Language Encoder: Prismatic-7B PointVLA [96] VLACache [195] Action Decoder: Transformer with dynamic token reuse HybridVLA [110] Vision Encoder: CLIP + DINOv Language Encoder: LLaMA-2 Action Decoder: Hybrid diffusion + autoregressive ensemble Vision Encoder: Multistage ViT with STAR router Language Encoder: CogKD-enhanced transformer Action Decoder: Sparse transformer with dynamic routing Vision Encoder: ViT for aerial imagery Language Encoder: GPT for instruction parsing Action Decoder: Transformer-based path planner MINT dataset: visionlanguage instruction tours MiniALOHA + SC tasks Robust navigation from multimodal input; generalizes across large unseen spaces via topological mapping MoLeVLA [213] Outperforms OpenVLA in speed and precision; does not require pretraining; inference 5x faster with minimal compute UAVVLA [150]"
        },
        {
            "title": "Continued on next page",
            "content": "14 QUART dataset (locomotion + manipulation) Quadruped-specific control with strong sim-to-real transfer and fine-grained instruction alignment Unified chataction dataset (web, robot) Excels at joint VQA and planning; mitigates forgetting; efficient across manipulation and conversational tasks Few-shot spatial tasks (real + sim) Excels at longhorizon and spatial reasoning tasks; avoids retraining by preserving pretrained 2D knowledge ALOHA + realworld sim fusion 4050% faster inference with near-zero loss; dynamically reuses static features for real-time robotics RT-X + synthetic task fusion Achieves robust control in complex multi-arm settings via dynamic ensemble; strong sim2real generalization RLBench + realworld manipulation tasks Brain-inspired efficiency with 5.6x speedup; selective layer activation with high task success (+8%) Satellite + UAV imagery instructions Zero-shot aerial task planning; intuitive language grounding; scalable to large unmapped environments"
        },
        {
            "title": "Continued on next page",
            "content": "Architecture Components"
        },
        {
            "title": "Continued from previous page\nTraining\nDataset",
            "content": "Key Strength / Uniqueness Model (Reference) DexGraspVLA [219] Vision Encoder: Objectcentric spatial ViT Language Encoder: Transformer with grasp sequence reasoning Action Decoder: Diffusion controller for grasp pose generation GraspVLA [38] Vision Encoder: MultiInterleaveVLA [51] view DINO-v2 and SigLIP fusion; InternLM2 language model Language Encoder: VLM predicts bounding boxes and grasp poses Action Decoder: Flowmatching based action expert via Progressive Action Generation (PAG) Vision Encoder: InternVL2.5 and OWLv2 for open-vocabulary and image-text token integration Language Encoder: Qwen2.5 for instruction parsing and visual-language verification Action Decoder: Continuous action predictor adapted from OpenVLA and π0 with diffusion-policy controller Dexterous grasping benchmark (sim + real) 90%+ zero-shot success on diverse objects; excels at lighting, background variation, and unseen conditions SynGrasp1B (1B synthetic frames), GRIT (Internet grounding dataset) First syntheticdata-pretrained grasping VLA; enables sim-toreal generalization, robust grasp policy via PAG; supports zero-shot and fewshot generalization to long-tail object classes and humancentric preferences Open Interleaved XEmbodiment (210k episodes from 11 realworld datasets) First end-to-end VLA model for interleaved image-text instructions; improves out-of-domain generalization 23 and enables zero-shot execution from handdrawn sketches and novel multimodal prompts 3.2. Training and"
        },
        {
            "title": "Advancements",
            "content": "in VisionLanguageAction Models VLA models have seen rapid progress in training and optimization techniques to reconcile multimodal inputs, reduce compute requirements, and enable real-time control. Key areas of advancement include: Data-Efficient Learning. Co-fine-tuning on massive visionlanguage corpora (e.g. LAION-5B) and robotic trajectory collections (e.g. Open X-Embodiment) aligns semantic understanding with motor skills. OpenVLA (7 params) achieves 16.5 % higher success rate than 55 Bparameter RT-2 variant, demonstrating that co-finetuning yields strong generalization with fewer parameters [152, 175, 94]. Synthetic Data Generation via UniSim produces photorealistic scenesincluding occlusions and dynamic lightingto augment rare edge-case scenarios, improving model robustness in cluttered environments by over 20 % [200, 167]. Self-Supervised Pretraining adopts contrastive objectives (`a la CLIP) to learn joint visualtext embeddings before action fine-tuning, reducing reliance on task-specific labels. Qwen2-VL leverages 15 self-supervised alignment to accelerate downstream grasp-and-place convergence by 12 % [137, 76]. Parameter-Efficient Adaptation. Low-Rank Adaptation (LoRA) inserts lightweight adapter matrices into frozen transformer layers, cutting trainable weights by up to 70 % while retaining performance [72]. The Pi-0 Fast variant uses merely 10 adapter parameters atop static backbone to deliver continuous 200 Hz control with negligible accuracy loss [133]. Inference Acceleration. Compressed Action Tokens (FAST) and Parallel Decoding in dual-system frameworks (e.g. Groot N1) yield 2.5 faster policy steps, achieving sub-5 ms latencies at modest cost to trajectory smoothness [13, 161]. Hardware-Aware Optimizationsincluding tensorcore quantization and pipelined attention kernelsshrink runtime memory footprints below 8 GB and enable real-time inference on embedded GPUs [93]. Together, these methods have transformed VLAs into practical agents capable of handling language-conditioned, visionguided tasks in dynamic, real-world settings. 3.3. Parameter-Efficient Methods and Acceleration Techniques in VLA Models Building on advances in data-efficient training, recent work has focused on reducing the parameter footprint and improving inference speed of VLA modelscritical for deployment on resource-constrained robotic platforms. 1. Low-Rank Adaptation (LoRA). LoRA injects small trainable rank-decomposition matrices into frozen transformer layers, enabling fine-tuning of billion-parameter In VLAs with only few million additional weights. OpenVLA, LoRA adapters (20 parameters) tuned 7 B-parameter backbone on commodity GPUs in under 24 h, cutting GPU compute by 70 % compared to full backpropagation [72, 94]. Crucially, LoRA-adapted models retain their high-level language grounding and visual reasoning capabilities while adapting to new robotic manipulation tasks (e.g. novel object shapes), making large VLAs accessible to labs without supercomputing resources. 2. Quantization. Reducing weight precision to 8-bit integers (INT8) shrinks model size by half and doubles on-chip throughput. OpenVLA experiments show that INT8 quantization on Jetson Orin maintains 97 % of full-precision task success across pick-and-place benchmarks, with only 5 % drop in fine-grained dexterity tasks [152, 94]. Complementary methods such as post-training quantization with per-channel calibration further minimize accuracy loss in high-dynamic-range sensor inputs [128]. These optimizations allow continuous control loops at 30 Hz on 50 edge modules. 3. Model Pruning. Structured pruning removes entire attention heads or feed-forward sublayers identified as redundant. While less explored in VLA than in pure vision or language models, early studies on Diffusion Policy demonstrate that pruning up to 20 % of ConvNet-based vision encoders yields negligible performance degradation in grasp stability [34]. Similar schemes applied to transformer-based VLAs (e.g. RDT-1B) can reduce memory footprint by 25 % with under 2 % drop in task success, paving the way for sub-4 GB deployments [112, 102]. 4. Compressed Action Tokenization (FAST). FAST reformulates continuous action outputs as frequency-domain tokens, compressing long control sequences into concise descriptors. The Pi-0 Fast variant achieved 15 faster inference with 300 M-parameter diffusion head by tokenizing 1000 ms action windows into 16 discrete tokens, enabling 200 Hz policy rates on desktop GPUs [133]. This approach trades minimal trajectory granularity for large speedups, suited for high-frequency control in dynamic tasks like bimanual assembly. 5. Parallel Decoding and Action Chunking. Autoregressive VLAs traditionally decode actions token by token, incurring sequential latency. Parallel decoding architectures (e.g. in Groot N1) decode groups of spatialtemporal tokens concurrently, achieving 2.5 reduction in end-toend latency on 7-DoF arms at 100 Hz, with less than 3 mm positional error increase [13, 161]. Action chunking further abstracts multi-step routines into single tokens (e.g. pick-and-place-cup), cutting inference steps by up to 40 % in long-horizon tasks like kitchen workflows [86]. 6. Reinforcement LearningSupervised Hybrid Training. The iRe-VLA framework alternates between reinforcement learning (RL) in simulation and supervised finetuning on human demonstrations to stabilize policy updates. By leveraging Direct Preference Optimization (DPO) to shape reward models and Conservative QLearning to avoid extrapolation error, iRe-VLA reduces sample complexity by 60 % versus pure RL, while maintaining the semantic fidelity imparted by languageconditioned priors [123, 65]. This hybrid approach yields robust policies for tasks with sparse feedback, such as dynamic obstacle avoidance. 7. Hardware-Aware Optimizations. Compiler-level graph rewrites and kernel fusion (e.g. via NVIDIA TensorRTLLM) exploit target hardware featurestensor cores, fused attention, and pipelined memory transfersto accelerate both transformer inference and diffusion sampling. In OpenVLA-OFT, such optimizations reduced inference latency by 30 % on RTX A2000 GPUs and lowered energy per inference by 25 % compared to standard PyTorch execution [93]. This makes real-time VLAs feasible on mobile robots and drones with strict power budgets. Discussion. Parameter-efficient adaptation and inference acceleration techniques collectively democratize VLA deployment: LoRA and quantization empower smaller labs to fine-tune 16 and operate billion-parameter VLAs on consumer-grade hardware, unlocking cutting-edge semantic understanding for robots [72, 94]. Pruning and FAST tokenization compress model and action representations, enabling sub-4 GB, sub-5 ms control loops without sacrificing precision in dexterous tasks [112, 133]. Parallel decoding and action chunking overcome sequential bottlenecks of autoregressive policies, supporting 100200 Hz decision rates needed for agile manipulation and legged locomotion [13, 161]. Hybrid RL-SL training stabilizes exploration in complex environments, while hardware-aware compilation ensures real-time performance on edge accelerators [123, 93]. Together, these advances make it practical to embed VLA models across industrial manipulators, assistive drones, and consumer robots, bridging the gap from research prototypes to real-world autonomy. 3.4. Applications of Vision-Language-Action Models VLA models are rapidly emerging as foundational building blocks for embodied intelligence, integrating perception, natural language understanding, and motor control within unified architecture. By encoding visual and linguistic modalities into shared semantic spaces and generating contextually grounded actions, VLA models enable seamless interaction between agents and their environments [102, 220]. This multimodal capacity has positioned VLAs as transformative agents across wide spectrum of real-world applications. In humanoid robotics, systems like Helix and RoboNurse-VLA combine vision, language, and dexterous manipulation to assist with domestic tasks and surgical operations, demonstrating real-time reasoning and safety-aware control [103, 186]. In autonomous vehicles, models such as OpenDriveVLA and ORION process dynamic visual streams and natural language instructions to make transparent, adaptive driving decisions in complex urban environments [56, 220]. Industrial deployments leverage VLA architectures for high-precision assembly, inspection, and collaborative manufacturing [102]. In agriculture, VLA-powered robotic systems enable vision-guided fruit harvesting, plant monitoring, and anomaly detection, reducing labor dependency and increasing sustainability. Furthermore, recent advances in interactive augmented reality systems utilize VLA models for real-time, language-conditioned spatial navigation, guiding users in indoor and outdoor settings based on voice or visual cues [150, 59]. Across these domains, VLAs offer unified framework for robust, adaptable, and semantically aligned task execution, marking pivotal shift toward embodied generalist agents. Table 3 in the appendix shows the recent VLA models by summarizing their methodologies, application domains, and key innovations. The following subsections chronologically explore the application areas in depth as shown in Figure 11. Precision & Automated Agriculture"
        },
        {
            "title": "Interactive AR\nNavigation",
            "content": "Healthcare & Medical Robotics"
        },
        {
            "title": "Autonomous\nVehicle\nSystems",
            "content": "Figure 11: Mind-map of application domains for VisionLanguageAction models. 3.4.1. Humanoid Robotics Humanoid robots, designed to mimic the form and functionality of the human body, represent one of the most demanding yet impactful domains for the deployment of VLA models. These platforms must seamlessly perceive complex environments, understand spoken or written natural language, and perform intricate physical tasks with human-level dexterity [144, 22]. The core strength of VLA models lies in their ability to unify perception, cognition, and control into single, endto-end trainable frameworkallowing humanoid robots to interpret visual inputs (e.g., RGB-D imagery of cluttered scenes), comprehend linguistic instructions (e.g., place the spoon in the drawer), and generate precise motor trajectories [118, 222]. Recent advances have significantly accelerated the deployment of VLAs in humanoid robotics. For example, Helix2, humanoid robot developed by Figure AI, leverages fully integrated VLA model to perform full-body manipulation at high frequency, controlling arms, hands, torso, and even fine-grained finger motion in real time. The architecture follows dualsystem design: multimodal transformer processes inputs such as language commands and vision streams, while real-time motor policy outputs dense action vectors at 200 Hz. This allows Helix to generalize across previously unseen objects and tasks, adapting fluidly to changing environments without the need for task-specific retraining. 2https://www.figure.ai/news/helix The key advantage of VLAs in humanoid systems is their ability to scale across diverse tasks using shared representations [8]. Unlike traditional robotic systems that rely on taskspecific programming or modular pipelines, VLA-powered humanoids operate under unified token-based framework. Vision inputs are encoded via pretrained vision-language models like DINOv2 or SigLIP, while instructions are processed using large language models such as Llama-2 or GPT-style encoders. These representations are fused into prefix tokens that capture the full context of the scene and task. Action tokens are then generated autoregressively, similar to language decoding, but represent motor commands for the robots joints and effectors. This capability enables humanoid robots to operate effectively in human-centric spaces, such as households, hospitals, and retail environments. In domestic settings, VLA-powered robots can clean surfaces, prepare simple meals, or organize objects simply by interpreting voice commands [118, 222]. In healthcare, systems like RoboNurse-VLA [103] have demonstrated the ability to perform precise instrument handovers to surgeons using real-time voice and visual cues. In retail, humanoid platforms equipped with VLAs can assist with customer queries, restock shelves, and navigate store layouts without explicit pre-programming [8]. What distinguishes modern humanoid VLAs is their ability to run on embedded, low-power hardware, making real-world deployment viable. For instance, systems such as TinyVLA [186] and MoManipVLA [190] demonstrate efficient inference pipelines that run on Jetson-class GPUs, enabling mobile deployment without compromising performance. These models exploit techniques like diffusion-based policies, LoRA-based fine-tuning, and dynamic token caching to minimize compute cost while retaining high precision and generalization. In logistics and manufacturing, humanoid VLAs are already making commercial impact. Robots like Figure 01 are deployed in warehouses to perform repetitive, physically intensive taskssuch as picking, sorting, and shelvingalongside human workers. Their ability to handle novel object categories and dynamically changing scenes is powered by continual learning and robust multimodal grounding [195, 102]. As VLA models continue to advance in their capacity for diverse action generation, spatial reasoning, and real-time adaptation, humanoid robots are emerging as highly capable assistants across homes, industrial settings, and public spaces. Their strength lies in their ability to unify perception, language comprehension, and motor control through shared token-based architectureenabling seamless, context-aware behavior in unstructured human environments. For example, as depicted in the figure 12, consider Helix, state-of-the-art humanoid robot equipped with nextgeneration VLA model. When instructed verbally, Please take the water bottle from the fridge, Helix activates its integrated perception system, where foundation vision-language model (e.g., SigLIP or DINOv2) segments the visual scene to identify the refrigerator, its handle, and the bottle. The language input is processed by large language model such as LLaMA-2, which tokenizes the instruction and fuses it with the visual context. This fused representation is passed to hierarchical controller: Figure 12: This figure illustrates Helix, next-generation humanoid robot executing household task using VLA framework. Upon receiving verbal command, Helix integrates vision-language model (e.g., SigLIP) and language model (e.g., LLaMA-2) to jointly perceive and interpret the environment. hierarchical VLA controller plans and executes sub-tasksopening the fridge, grasping bottlewhile an agentic AI module adapts actions in real time. This demonstrates VLA-based generalist robotics with dynamic task adaptation and safe, semantically grounded manipulation. the high-level policy plans the task sequence (locate handle, pull door, identify bottle, grasp), while mid-level planner defines motor primitives, such as grasp type and joint trajectories. The low-level VLA controlleroften based on diffusion policy networksexecutes these actions with sub-second latency. Upon encountering variations (e.g., tilted bottle or slippery grip), Helixs agentic AI module performs micro-policy refinement in real time, adjusting its grip based on feedback. This example illustrates the transformative potential of humanoid VLAs. From kitchens to clinics, these systems not only interpret complex instructions and execute physical tasks with dexterity but also adapt to environmental unpredictability. By embedding agentic reasoning and safety alignment mechanisms, modern humanoid robots powered by VLAs are transitioning from narrow-task performers to generalist, trustworthy collaborators. As energy-efficient models like TinyVLA and MoManipVLA mature, deployment on mobile, low-power platforms becomes increasingly practicalushering in new era of embodied, socially aligned AI. 3.4.2. Autonomous Vehicle Systems Autonomous vehicles (AVs), including self-driving cars, trucks, and aerial drones, represent frontier application domain for VLA models, where safety-critical decision-making demands tightly coupled perception, semantic understanding, and real-time action generation. Unlike traditional modular AV pipelines that decouple perception, planning, and control, VLA frameworks offer an integrated architecture that processes multimodal inputsincluding visual streams, natural language instructions, and internal state informationwithin unified autoregressive model capable of outputting precise control signals. VLA models empower AVs to comprehend complex environments beyond pixel-level object recognition. For instance, self-driving car navigating an urban setting must detect traffic signs, understand pedestrian behavior, and interpret navigation commands such as take the second right after the gas station. These tasks involve fusing visual and linguistic signals to understand spatial relationships, predict intent, and generate context-aware driving actions. VLAs encode this information through token-based representations, where visual encoders (e.g., ViT, CLIP), language models (e.g., LLaMA-2), and trajectory decoders operate in coherent semantic space, enabling the vehicle to reason about high-level goals and translate them into low-level motion. notable contribution in this direction is CoVLA [5], which provides comprehensive dataset pairing over 80 hours of realworld driving videos with synchronized sensor streams (e.g., LiDAR, odometry), detailed natural language annotations, and high-resolution driving trajectories. This dataset enables training VLA models to align perceptual and linguistic features with physical actions. CoVLA employs CLIP for visual grounding, LLaMA-2 for instruction embedding, and trajectory decoders for motion prediction. This configuration allows AVs to interpret verbal cues (e.g., yield to ambulance) and environmental conditions (e.g., merging traffic) to make transparent and safe driving decisions. OpenDriveVLA [220] advances the state of VLA modeling by integrating hierarchical alignment of 2D/3D multi-view vision tokens with natural language inputs. Its architecture leverages both egocentric spatial perception and external scene understanding to construct dynamic agent-environment-ego interaction model. Through autoregressive decoding, OpenDriveVLA generates both action plans (e.g., steering angle, 18 acceleration) and trajectory visualizations interpretable to humans. Its end-to-end framework achieves state-of-the-art performance on planning benchmarks and question-answering tasks related to driving scenarios, demonstrating its robustness in urban navigation and behavioral prediction. Another seminal model, ORION [56], pushes the boundaries of closed-loop autonomous driving by incorporating QTFormer to retain long-horizon visual context, large language model for reasoning over traffic narratives, and generative trajectory planner. ORION excels at aligning the discrete reasoning space of vision-language models with the continuous control space of AV motion. This unified optimization results in accurate visual question answering (VQA) and trajectory planning, crucial for scenarios involving ambiguous human instructions or occluded obstacles (e.g., take the exit after the red truck). For example, as depicted in Figure 13 consider an autonomous delivery vehicle, AutoNav, operating in dense urban environment using next-generation VLA architecture. As AutoNav receives cloud-based instructionDrop off the package near the red awning beside the bakery, then return to base avoiding construction zonesits onboard VLM (e.g., CLIP or SigLIP) parses the visual stream from multiple cameras, identifying dynamic landmarks such as bakery signs, red awnings, and traffic cones. Simultaneously, the LLM module grounded in LLaMA-2 decodes the instruction and fuses it with real-time sensory context including LiDAR, GPS, and inertial odometry. hierarchical control stack processes these multimodal signals via an autoregressive VLA decoder that integrates egocentric views and world-centric maps to plan adaptive paths. As the vehicle approaches the delivery location, unexpected pedestrian activity prompts an agentic submodule to trigger trajectory re-planning using reinforcement learning-inspired policy refinement routine. At the same time, AutoNav audibly warns pedestrians and recalibrates its speed to maintain safety margins. This interplay of semantic understanding, perceptual grounding, and adaptive control exemplifies the power of VLA-based systems in achieving interpretable, human-aligned behavior in safety-critical scenarios. It also demonstrates how such integration can surpass traditional perception-planning-control pipelines in autonomy, transparency, and decision-making agility. In aerial robotics, VLAs enhance the capabilities of delivery drones and UAVs. Models such as UAV-VLA [150] combine satellite imagery, natural language mission descriptions, and onboard sensing to execute high-level commands (e.g., deliver to the rooftop pad with the blue tarp). These systems use modular VLA architectures, where vision-language planner parses global context and flight controller executes precise waypoints, supporting applications in logistics, disaster response, and military reconnaissance. As autonomous systems increasingly operate in unstructured environments, VLAs provide scalable, interpretable, and dataefficient alternative to traditional pipelines. By learning from large-scale multimodal datasets and modeling decision-making as token prediction, VLAs align human-level semantics with robotic motion, paving the way for safer, smarter autonomous Figure 13: This illustration depicts an autonomous delivery vehicle powered by VLA system, integrating VLMs for visual grounding, LLMs for instruction parsing, and VLA decoder for path planning. Agentic AI enables adaptive trajectory refinement in dynamic environments, exemplifying how multimodal integration drives safe, interpretable, and autonomous decision-making in realworld navigation tasks. driving and navigation technologies. 3.4.3. Industrial Robotics Industrial robotics is undergoing paradigm shift with the integration of VLA models, enabling new generation of intelligent robots capable of high-level reasoning, flexible task execution, and natural communication with human operators [27, 7]. Traditional industrial robots typically operate in highly structured environments using rigid programming, often requiring extensive reconfiguration and manual intervention when adapting to new assembly lines or product variants [6, 142]. Such systems lack the semantic grounding and adaptability required for modern dynamic manufacturing settings. VLA models, by contrast, offer more human-interpretable and generalizable framework. Through the joint embedding of visual inputs (e.g., component layout or conveyor belt state), natural language instructions (e.g., tighten the screw on the red module), and robot state, VLAs can infer context and execute appropriate control commands in real-time [105, 58, 121]. Vision transformers (e.g., ViT, DINOv2), large language models (e.g., LLaMA-2), and autoregressive or diffusion-based action decoders form the backbone of these systems, allowing the robot to parse multimodal instructions and perform actions grounded in its environment. One of the most significant contributions in this domain is CogACT [102], componentized VLA framework explicitly designed for industrial robotic manipulation. Unlike early VLAs that relied on frozen language-vision embeddings followed by direct action quantization, CogACT introduces diffusion-based action transformer that models action sequences more robustly and adaptively. The system uses visual-language encoder (e.g., Prismatic-7B) to extract highlevel scene and instruction embeddings, which are then passed to diffusion transformer (DiT-Base) to generate fine-grained 19 motor actions. This modular separation enables superior generalization to unseen tools, parts, and layouts while preserving interpretability and robustness under real-world constraints. Furthermore, CogACT demonstrates rapid adaptation across different robot embodimentssuch as 6-DoF arms or bimanual systemsthrough efficient fine-tuning, making it suitable for deployment across heterogeneous factory environments [102]. Empirical evaluations show that CogACT outperforms prior models like OpenVLA by over 59% in real-world task success rates, especially in complex, high-precision tasks such as multistep assembly, screw fastening, and part sorting. As manufacturing shifts toward Industry 4.0 paradigms, VLAs promise to reduce programming overhead, support voice-commanded robot programming, and facilitate real-time human-robot collaboration on mixed-initiative tasks. While execution precision, safety guarantees, and latency optimizations remain areas of active research, the use of VLA models in industrial robotics marks substantial step toward autonomous, intelligent, and adaptable robotic factories. 3.4.4. Healthcare and Medical Robotics Healthcare and medical robotics represent high-stakes and adaptability are safety, domain where precision, paramountqualities that VLA models are increasingly well-suited to provid [103, 151]. Traditional medical robotic systems rely heavily on teleoperation or pre-programmed behaviors [130, 158], limiting their autonomy and responsiveness in dynamic surgical or care environments. In contrast, VLA models offer flexible framework that integrates real-time visual perception, language comprehension, and fine-grained motor control, enabling medical robots to understand high-level instructions and autonomously perform intricate procedures or assistance tasks [102, 43, 174]. In surgical robotics, VLAs can dramatically enhance capabilities in minimally invasive operations [40, 177]. These systems can fuse laparoscopic video feeds [98], anatomical maps [114, 40], and voice commands into unified tokenized representation using vision encoders (e.g., ViT, SAM-2) and language models (e.g., LLaMA, T5) [181]. For instance, as depicted in Figure 14a, in task like apply suture to the left coronary artery, the vision module identifies the anatomical target, while the language module contextualizes the instruction. The action decoder then translates the fused semantic embedding into stepwise motion commands with sub-millimeter precision. This enables the robot to adaptively reposition tools, apply dynamic force feedback, and avoid critical structures, reducing the need for surgeon micromanagement and minimizing risk of human error. Beyond the operating room, VLA models are powering new generation of patient-assistive robots in eldercare, rehabilitation, and hospital logistics. These systems can autonomously perceive patient behavior, understand spoken or gestural input, and execute responsive tasks such as retrieving medication, guiding mobility aids, or notifying caregivers during emergencies. For example, as depicted in Figure 14b, VLA-enabled robot can visually detect patient attempting to rise from bed, Figure 14: a) This figure illustrates VLA surgical system executing the task apply suture to the left coronary artery. The vision module identifies anatomical targets, the language model interprets the instruction, and the action decoder generates precise motor commands, enabling adaptive tool control, real-time feedback, and safe autonomous operation; b) VLA-powered assistive robot perceives patient behavior, processes verbal requests (e.g., bring my walker), and autonomously executes context-aware motion plans, enabling real-time assistance in eldercare, rehabilitation, and hospital logistics without relying on predefined scripts or manual oversight. interpret verbal request such as bring my walker, and generate context-appropriate motion plan to assistwithout predefined scripts or constant supervision. Recent VLA frameworks such as RoboNurse-VLA [103] highlight the real-world feasibility of this approach. RoboNurse employs SAM-2 for semantic scene segmentation and LLaMA2 for command comprehension, integrated into real-time voice-to-action pipeline that enables robots to assist with surgical instrument handovers in operating rooms [103]. The system demonstrates robustness to diverse tools, varied lighting conditions, and noisy environmentscommon challenges in clinical settings. Additionally, VLA architectures offer advantages in explainability and auditability, both critical in regulated medical domains [173, 113]. Scene grounding and trajectory prediction can be visualized and reviewed post-hoc [208], which could facilitate clinical trust and enabling FDA-style validation pipelines. LoRA-based fine-tuning allows adaptation to specific 20 hospital environments or procedural workflows with minimal data and compute [9, 176, 114]. Importantly, the multimodal foundation of VLA models enables cross-domain transferability: the same model trained on surgical tool manipulation can be adapted to patient mobility tasks with modest retraining [45]. This modularity significantly reduces development time and cost compared to task-specific automation systems [73]. As medical robotics transitions from teleoperated assistance to semi-autonomous and collaborative systems, VLA models stand at the core of this transformation. By combining high-level semantic understanding with lowlevel control, VLAs provide unified solution for scalable, human-aligned, and adaptive robotic healthcare [193, 221, 209]. As healthcare systems face increasing demand and workforce shortages, VLA-driven robotics will play crucial role in enhancing medical precision, operational efficiency, and patient-centered care. 3.4.5. Precision and Automated Agriculture As illustrated in Figure 15, VLA models are emerging as transformative tools in precision and automated agriculture, offering intelligent, adaptive solutions for labor-intensive tasks across diverse farming landscapes [57, 150]. Unlike traditional agricultural automation systems that depend on rigid, sensordriven pipelines and require manual reprogramming for each task or environmental variation [169, 84], VLAs integrate multimodal perception, natural language understanding, and realtime action generation within unified framework [131, 66]. This enables autonomous ground robots and drones to interpret complex field scenes, follow spoken or text-based farming instructions, and generate context-aware actions such as selective fruit picking or adaptive irrigation. The ability of VLAs to dynamically adjust to occlusions, terrain irregularities, or varying crop typescombined with training on synthetic, photorealistic datasetsallows them to generalize across geographies and seasons. By leveraging action tokenization [189], transformer-based policy generation [11, 67], and techniques like LoRA fine-tuning [72], these systems are redefining the scalability and intelligence of agricultural robotics for sustainable and precision-driven farming. In modern orchards and crop fields, VLAs can process visual inputs from RGB-D cameras, multispectral sensors, or drones to monitor plant growth, detect diseases, and identify nutrient deficiencies. Vision transformers (e.g., ConvNeXt, DINOv2) encode spatial and semantic information from visual scenes, while large language models (e.g., T5, LLaMA) parse natural language commandssuch as inspect the east plot for powdery mildew or harvest ripe apples near the irrigation trench. Through token fusion, these modalities are aligned in shared representation space, allowing robots to execute fine-grained, context-aware actions with precision. For instance, in fruit-picking tasks, as illustrated in Figure 15, VLA-equipped ground robot can identify ripe produce using image-based ripeness cues, interpret user-specified criteria such as pick only Grade fruits, and execute motion sequences via action tokens that control its end-effector. This approach ensures minimal crop damage, optimizes pick rates, and allows real-time adaptation to unexpected variables like occlusions or terrain shifts. In irrigation management, drones guided by VLA models can interpret field maps and verbal instructions to selectively water stressed zones, reducing water usage by up to 30%. Moreover, VLA models support dynamic reconfiguration and lifelong learning. With access to synthetic training datasets generated from photorealistic simulations of crop environments (e.g., 3D orchard renderings), models can be trained to recognize pests, weeds, and crop maturity stages without extensive manual annotation. Techniques like LoRA adapters and diffusion-based policy tuning further enhance generalization to novel crops, seasons, and geographical regions. The integration of VLAs into agricultural workflows offers significant benefits: reduced dependence on skilled labor, higher yield through targeted intervention, and enhanced environmental sustainability through optimized input usage. As global food systems grapple with climate variability and resource constraints, VLA-enabled agriculture will play pivotal role in advancing scalable, intelligent, and sustainable farming practices tailored to real-world complexity. 3.4.6. Interactive AR Navigation with Vision-Language-Action"
        },
        {
            "title": "Models",
            "content": "Interactive Augmented Reality (AR) navigation represents frontier where the VLA models can significantly enhance human-environment interaction by providing intelligent, context-aware guidance in real-time [26, 80, 197]. In this paradigm, VLAs process continuous streams of visual data from AR-enabled devicessuch as smart glasses or smartphonesalongside natural language queries to generate dynamic navigational cues overlaid directly onto the users view of the physical world. Unlike traditional GPS-based systems that rely on rigid maps and limited user input [24, 159], VLAbased AR agents interpret complex visual scenes (e.g., intersections, indoor hallways, signage) and respond to free-form instructions such as take me to the nearest pharmacy with wheelchair ramp or show the quietest route to the conference room. Technically, these models integrate vision encoder (e.g., ViT, DINOv2) that extracts scene representations from firstperson RGB frames, language encoder (e.g., T5 or LLaMA) that processes user prompts or voice commands, and an action decoder that predicts tokenized navigation cues such as directional overlays, waypoints, or voice instructions. transformer-based architecture fuses these modalities to reason about both the spatial layout and semantic intent, allowing the AR agent to adaptively highlight paths, landmarks, and hazards directly within the users field of view [163, 129]. For example, as shown in Figure 16, in crowded airport, the VLA agent could visually identify escalators, gates, or baggage claims while understanding query like how do reach Gate 22 without stairs?, adjusting the route in response to real-time occupancy and obstacles. VLAs also support interaction loops that enable users to refine instructions (e.g., avoid busy areas or take the scenic route) and receive context-aware feedback, improving accessibility for the visually impaired or cognitively challenged. In 21 Figure 15: This diagram illustrates the application of VLA models in precision and automated agriculture. ground robot uses vision encoders to detect ripe fruits and interprets instructions such as pick only Grade fruits through language encoders. Action tokens then guide robotic manipulators for efficient, damage-free picking. Drones leverage VLA models to analyze aerial imagery and verbal commands for targeted irrigation. Synthetic training environments and LoRA-based adaptation enable models to generalize across crop types, environmental conditions, and geographies. This VLA-driven pipeline promotes sustainable agriculture by improving productivity, reducing manual labor, and enhancing decision-making through multimodal perception and control. logistics and indoor navigation, these systems can be integrated with IoT sensors and digital twins to guide warehouse workers, maintenance teams, or delivery robots through complex environments. Furthermore, personalized navigation can be achieved through continual fine-tuning, where VLA models learn user preferences and local spatial layouts over time. As AR hardware becomes more affordable and integrated into daily life, VLA-powered navigation systems will enable seamless spatial understanding, multimodal interaction, and autonomous guidance in public, industrial, and assistive contextsredefining how humans perceive, explore, and interact with physical spaces. 4. Challenges and Limitations of Vision-Language-Action"
        },
        {
            "title": "Models",
            "content": "VLA models face spectrum of interrelated challenges that impede their translation from research prototypes to robust, real-world systems. First, achieving real-time, resource-aware inference remains difficult: models like DeeR-VLA leverage dynamic early-exit architectures to cut computation 56 on manipulation benchmarks while preserving accuracy, yet their gains diminish in complex scenarios [202]. Similarly, UniNaVid compresses egocentric video tokens for 5 Hz navigation but still struggles under highly ambiguous instructions and Figure 16: Showing how VLA models enable interactive AR navigation by fusing real-time visual perception, language understanding, and action planning. In dynamic environments such as airports, VLAs interpret user queries like avoid stairs to Gate 22, analyze visual scenes (e.g., detecting escalators), and adjust navigational paths accordingly, supporting personalized, accessible, and context-aware mobility guidance. longer horizons [210]. Coupled with limited object generalization, even advanced hybrid vision-language grounding schemes (e.g., ObjectVLA) generalize to only 64 % of novel objects, un22 derscoring persistent gaps in open-world robustness [223]. Second, adapting VLA models with minimal supervision and ensuring stable policy updates under scarce, noisy data is nontrivial. ConRFT combines behavior cloning and Qlearning with human-in-the-loop fine-tuning to rapidly converge to 96.3% success over eight contact-rich tasks, yet it relies heavily on expert interventions and reward shaping [31]. Hierarchical frameworks such as Hi Robot decouple high-level reasoning from low-level execution to improve instruction fidelity, but coordinating these modules and grounding ambiguous feedback remains challenging [155]. Likewise, TLAs fusion of tactile streams with language commands achieves over 85 % success on unseen peg-in-hole tasks, but dataset breadth and real-time multi-step decoding still limit broader generalization [70]. Furthermore, ensuring safety, generalization, and end-to-end reliability in dynamic environments demands new modeling and evaluation standards. Occupancy-Language-Action models like OccLLaMA unify 3D scene understanding with action planning, yet they must scale to richer scene dynamics and semantic consistency across modalities [183]. RaceVLA pushes high-speed drone navigation via quantized, iterative control loops, but its visualphysical generalization trails larger VLAs and dedicated reasoners [153]. Model-merging strategies in ReVLA recover lost out-of-domain visual robustnessimproving OOD grasp success by up to 77 %but introduce extra computation and complexity [39]. Finally, SafeVLA formulates constraints via constrained Markov decision processes to cut unsafe behavior by over 80 %, yet defining comprehensive, non-restrictive safety rules for diverse real-world tasks remains an open problem [205]. Addressing these intersecting limitations is critical for VLA models to achieve reliable, autonomous operation against the full complexity of realworld robotics. Building upon the critical limitations outlined above, it is imperative to map each challenge to targeted mitigation strategies and forecast their system-level impact. Table 4 distills this mapping into three columnsidentifying core limitations, proposing concrete technical remedies drawn from recent advances, and articulating the anticipated benefits for real-world VLA deployment. For instance, tackling real-time inference constraints leverages parallel decoding and quantized transformer pipelines with hardware acceleration (e.g., TensorRT) to sustain control loop rates in drones and manipulators [100, 94, 60, 110]. Addressing multimodal action representation via hybrid diffusionautoregressive policies enriches models capacity to produce varied, context-sensitive motor commands for complex tasks [133, 121]. To guarantee safety in open worlds, dynamic risk assessment modules and adaptive planning layers can be integrated, ensuring robust emergency stop behaviors in unpredictable settings [143, 180, 87]. Similarly, dataset bias and grounding are countered through curated debiased corpora and advanced contrastive fine-tuning, bolstering fairness and semantic fidelity when generalizing to novel objects these solution pathand scenes [145, 16, 136]. Together, waysand others spanning simulation-to-real transfer, tactile integration, and energy-efficient architecturesframe comprehensive roadmap for transitioning VLA research into reliable, scalable autonomy. The remainder of this section is organized into five focused subsections, each examining distinct cluster of VLA challenges identified in the literature. First, we analyze real-time inference constraints and the emerging methods that address them. Next, we delve into multimodal action representation alongside safety assurance in open-world settings. We then discuss dataset bias, grounding strategies, and generalization to unseen tasks, followed by an exploration of system integration complexity and computational demands. Finally, we consider robustness and the ethical implications of deploying VLAs in real-world applications. 4.1. Real-Time Inference Constraints Real-time inference remains significant limitation in deploying VLA models, particularly in latency-critical applications like robotic manipulation, autonomous driving, and drone control. VLAs typically depend on autoregressive decoding strategies, which sequentially generate action tokens based on previous predictions. While effective for many tasks, this method severely restricts inference speed, typically achieving only 35 Hz. This rate falls dramatically short of the 100 Hz or greater frequency required by robotic systems for precise and fluid real-time control. For instance, when robotic arm manipulates delicate objects, frequent positional updates are essential to maintain accuracy and prevent damage. Models such as OpenVLA [94] and Pi-0 [14] face inherent challenges with this sequential token generation approach, thereby limiting their effectiveness in dynamic environments. Emerging solutions such as parallel decoding, exemplified by NVIDIAs Groot N1 model [13], aim to accelerate inference by predicting multiple tokens simultaneously. Groot N1 achieves approximately 2.52 speedup over traditional decoding methods; however, this parallelism often introduces tradeoffs in trajectory smoothness, resulting in jerky or suboptimal robot movements. Such movements are undesirable in sensitive applications like surgical robotics, where precision and fluidity are paramount. Thus, achieving rapid inference without compromising output quality remains an open challenge. Additionally, hardware limitations exacerbate real-time inference constraints. For example, processing high-dimensional visual embeddings, typically involving over 400 vision tokens at 512 dimensions each, requires approximately 1.2 GB/s memory bandwidth. This demand significantly exceeds the capacity of current embedded systems or edge-AI hardware such as NVIDIA Jetson platforms, thereby restricting practical deployment. Even with efficient quantization techniques, which reduce the precision of floating-point operations to alleviate memory constraints, models frequently experience accuracy degradation, especially in tasks demanding sub-millimeter precision, such as bimanual robotic manipulation or medical robotics. 4.2. Multimodal Action Representation and Safety Assurance Multimodal Action Representation: One significant limitation of current VLA models is accurately representing mul23 timodal actions, particularly in scenarios requiring continuous and nuanced control [51, 38]. Traditional discrete tokenization methods, such as those dividing actions into 256 distinct bins, inherently lack precision, creating substantial errors in fine-grained tasks like delicate robotic grasping or intricate surgical procedures [133]. For instance, during precise robotic manipulation in assembly tasks, discrete representations can result in misaligned or imprecise actions, undermining performance and reliability. On the other hand, continuous multilayer perceptron (MLP) based approaches face the risk of mode collapse [126, 179], where models converge prematurely to single action trajectories, despite multiple viable paths available. This diminishes the flexibility necessary for adaptive decisionmaking in highly dynamic environments. Emerging diffusionbased policies, exemplified by models like Pi-Zero and RDT-1B [112], offer richer multimodal action representation capable of capturing diverse action possibilities. However, their substantial computational overheadapproximately three times that of conventional transformer-based decodersrenders them impractical for real-time deployment. Consequently, VLA models currently struggle with complex dynamic tasks, such as robotic navigation in densely crowded spaces or sophisticated bimanual manipulations [59, 191], where multiple strategic actions may be equally valid and contextually dependent. Safety Assurance in Open Worlds: Another critical challenge facing VLAs is ensuring robust safety in dynamic, unpredictable environments characteristic of real-world scenarios [33, 205]. Many current implementations depend heavily on predefined hardcoded force and torque thresholds, significantly constraining their adaptability in encountering unforeseen or novel conditions, such as unexpected obstacles or sudden environmental changes [121]. Models used for collision prediction typically attain only about 82% accuracy in cluttered and dynamic spaces, posing serious risks in applications such as warehouse logistics or domestic robotics, where safety margins are minimal [217, 94]. Moreover, the essential safety mechanisms like emergency stops incorporate substantial latencyoften between 200 and 500 millisecondsdue to comprehensive safety verifications [132, 94]. This delay, although seemingly minor, can prove hazardous in high-speed operations or critical interventions, such as automated driving or emergency robotic responses. 4.3. Dataset Bias, Grounding, and Generalization to Unseen"
        },
        {
            "title": "Tasks",
            "content": "A significant obstacle limiting the effectiveness of VLA models is the pervasive presence of dataset bias and grounding deficiencies. Current training datasets, predominantly sourced from web-crawled repositories, frequently exhibit inherent biases [165, 91]. Studies indicate that approximately 17% of associations within standard datasets are skewed toward stereotypical interpretations, such as disproportionately associating terms like doctor with male figures [171, 95]. These biases propagate through training, resulting in VLAs that produce semantically misaligned or contextually inappropriate responses when deployed in diverse environments. For instance, models such as OpenVLA have been documented to overlook approximately 23% of object references in novel settings, significantly impairing their practical utility in real-world applications where accurate interpretation of instructions is critical [94]. This grounding issue also extends to challenges in compositional generalization, where VLAs often falter when encountering rare or unconventional combinations, such as interpreting phrase like yellow horse because of underrepresentation in training corpora. These shortcomings highlight an urgent need for carefully curated, balanced, and comprehensive datasets, coupled with advanced grounding algorithms designed to mitigate biases and enhance semantic alignment across varied contexts. Complementing the challenges posed by dataset bias is the broader issue of generalization to unseen tasks, critical barrier for the practical deployment of VLAs. While existing models demonstrate proficiency in familiar environments or tasks similar to their training scenarios, their performance significantly degradesoften by as much as 40%when encountering entirely novel tasks or unfamiliar variations. For example, VLA trained specifically on domestic tasks may struggle or outright fail when introduced into industrial or agricultural settings, largely due to discrepancies in object types, environmental dynamics, and operational constraints. This limitation arises primarily from overfitting to narrowly scoped training distributions and insufficient exposure to diverse task representations. Consequently, current VLAs exhibit limited proficiency in zero-shot or few-shot learning scenarios, impeding their adaptability and scalability. 4.4. System Integration Complexity and Computational Demands Integrating VLA models within dual-system architectures, which combine high-level cognitive planning (System 2) and real-time physical control (System 1), presents significant complexity in robotic applications. primary challenge arises from temporal mismatches between these two systems. Typically, System 2 leverages large language models (LLMs) such as GPT or Llama-2 for complex task decomposition and strategic planning. These models, due to their substantial computational requirements, often exhibit processing times of approximately 800 ms or more per inference cycle. Conversely, System 1 components, tasked with executing rapid, low-level motor actions through control loops, operate on millisecond timescalesoften around 10 ms intervals. This stark discrepancy in operational cadence leads to synchronization difficulties, causing delays and potentially suboptimal execution trajectories. For example, NVIDIAs Groot N1 model demonstrates an effective integration of these two systems but still suffers from occasional jerkiness in motion due to asynchronous interaction, highlighting this intrinsic challenge. Furthermore, the feature space misalignment between highdimensional vision encoders, such as Vision Transformers (ViT), and lower-dimensional action decoders exacerbates integration complexity. When attempting to reconcile these disparate embeddings, the coherence between perceptual understanding and actionable commands can deteriorate significantly. OpenVLA [94] and RoboMamba [111], which utilize 24 transformer-based visual processing and subsequent action decoding, illustrate these integration challengesresulting in diminished performance when ported from simulation environments to physical hardware deployments. Such discrepancies manifest as high as 32% reduction in performance, primarily due to mismatches between simulated dynamics and real-world sensor noise or calibration issues. Energy and compute demands constitute another significant barrier for VLA deployment, particularly in edge computing contexts typical of autonomous drones, mobile robots, and wearable robotic systems. The substantial parameter counts typical of advanced VLAsfor instance, models possessing upwards of 7 billion parametersnecessitate computational resources often exceeding 28 GB of VRAM in their native form. These requirements vastly outpace the capabilities of most current edge-oriented processors and GPUs, restricting the practical applicability of sophisticated VLAs outside specialized, high-resource environments. 4.5. Robustness and Ethical Challenges in VLA Deployment The practical deployment of VLA models faces substantial challenges regarding robustness to environmental variability and ethical considerations. Environmental robustness refers to the VLAs capacity to maintain stable and accurate performance across dynamically changing conditions. Real-world environments frequently introduce unpredictable variations such as fluctuating lighting, weather conditions, or partial occlusions. For instance, vision modules within VLAs, such as those employed by OpenDriveVLA [220], exhibit accuracy reductions of approximately 2030% under low-contrast or shadow-heavy scenarios due to inadequate processing capabilities of current visual encoders. Similarly, linguistic comprehension in VLAs like CoVLA [5] is adversely affected in acoustically noisy or ambiguous contexts, where instructions can become difficult to interpret accurately, leading to task execution errors. Additionally, robotic manipulation tasks using VLA-equipped systems such as RoboMamba [111] frequently struggle with cluttered environments, misjudging positions or orientations of partially occluded objects, thereby compromising task success. 5. Discussion As illustrated in Figure 17, VLA models face multifaceted set of challenges that span algorithmic, computational, and ethical dimensions. First, achieving real-time inference on resource-constrained hardware remains difficult due to the sequential nature of autoregressive decoders and the high dimensionality of multimodal inputs. Second, fusing vision, language, and action into coherent policies introduces safety vulnerabilities when encountering unanticipated environmental changes. Third, dataset bias and grounding errors compromise generalization, often causing models to fail on out-of-distribution tasks. Fourth, integrating diverse componentsperception, reasoning, controlyields complex architectures that are hard to optimize and maintain. Fifth, the energy and compute demands of large VLA systems hinder deployment on embedded or mobile platforms. Finally, robustness to environmental variability and ethical considerations, such as privacy and bias mitigation, add layers of societal and regulatory concern. Collectively, these limitations constrain the practical adoption of VLA models in real-world robotics, autonomous systems, and interactive applications. The potential solutions to these challenges are discussed in the below points. 5.1. Potential Solutions Real-Time Inference Constraints. Future research must develop VLA architectures that harmonize latency, throughput, and task-specific accuracy. One promising direction is the integration of specialized hardware acceleratorssuch as FPGA-based vision processors and tensor cores optimized for sparse matrix operationsto execute convolutional and transformer layers at sub-millisecond scales [94, 100]. Model compression techniques like LowRank Adaptation (LoRA) [72] and knowledge distillation can shrink parameter counts by up to 90%, reducing both memory footprint and inference time while retaining over 95% of original performance on benchmark tasks. Progressive quantization strategies that combine mixed-precision arithmetic (e.g., FP16/INT8) with blockwise calibration can further cut computation by 24 with minimal accuracy loss [93]. Adaptive inference architectures that dynamically adjust network depth or width based on input complexityakin to early-exit branches in DeeR-VLA [202]can reduce average compute by selectively bypassing transformer layers when visual scenes or linguistic commands are simple. Finally, efficient tokenization schemes leveraging subword patch embeddings and dynamic vocabulary allocation can compress visual and linguistic input into compact representations, minimizing token counts without sacrificing semantic richness [133]. Together, these innovations can enable sub50 ms end-to-end inference on commodity edge GPUs, paving the way for latency-sensitive applications in autonomous drone flight, real-time teleoperation, and collaborative manufacturing. Multimodal Action Representation and Safety Assurance. Addressing multimodal action representation and robust safety requires end-to-end frameworks that unify perception, reasoning, and control under stringent safety constraints. Hybrid policy architectures combining diffusion-based sampling for low-level motion primitives [34] with autoregressive high-level planners [186] enable compact stochastic representations of diverse action trajectories, improving adaptability in dynamic environments. Safety can be enforced via real-time risk assessment modules that ingest multi-sensor fusion streamsvisual, depth, and proprioceptive datato predict collision probability and joint stress thresholds, triggering emergency stop circuits when predefined safety envelopes are breached [143, 180]. Reinforcement learning algorithms augmented with constrained optimization (e.g., Lagrangian methods in SafeVLA [205]) can learn 25 Figure 17: Figure maps six core VLA challengesreal-time inference, multimodal fusion safety, dataset bias, integration complexity, compute demands, and robustness/ethicsagainst six targeted solutions: adaptive pruning, hybrid policy architectures, meta/transfer learning, LoRA/quantization, domain randomization, and ethical oversight. This systematic alignment clarifies pathways to robust, efficient, and safe VLA deployment across broader real-world robotic domains. policies that maximize task success while strictly respecting safety constraints. Online model adaptation techniquessuch as rule-based RL (GRPO) and Direct Preference Optimization (DPO)further refine action selection under new environmental conditions, ensuring consistent safety performance across scenarios [87]. Crucially, embedding formal verification layers that symbolically analyze planner outputs before execution can guarantee compliance with safety invariants, even for neuralnetworkbased controllers. Integrating these methodologies will produce VLA systems that not only execute complex, multimodal actions but do so with provable safety in unstructured, real-world settings. Dataset Bias, Grounding, and Generalization to Unseen Tasks. Robust generalization demands both broadened data diversity and advanced learning paradigms. Curating large-scale, debiased multimodal datasetscombining web-scale imagetext corpora like LAION-5B [152] with robot-centric trajectory archives such as Open X-Embodiment [175]lays the groundwork for equitable semantic grounding. Hard-negative sampling and contrastive fine-tuning of visionlanguage backbones (e.g., CLIP variants) can mitigate spurious correlations and enhance semantic fidelity [16, 212]. Meta-learning frameworks enable rapid adaptation to novel tasks by learning shared priors across task families, as demonstrated in vision-language robotic navigation models [136]. Continual learning algorithmswith replay buffers and regularization strategiespreserve old knowledge while integrating new concepts, addressing catastrophic forgetting in VLA models [39]. Transfer learning from 3D perception domains (e.g., point cloud reasoning in 3D-VLA [217]) can imbue models with spatial inductive biases, improving out-of-distribution robustness. Finally, simulation-to-real (sim2real) fine-tuning with domain randomization and real-world calibrationsuch as dynamic lighting, texture, and physics variationsensures that policies learned in synthetic environments transfer effectively to physical robots [4, 53]. These combined strategies will empower VLAs to generalize confidently to unseen objects, scenes, and tasks in real-world deployments. System Integration Complexity and Computational Demands. To manage the intricate orchestration of multimodal pipelines under tight compute budgets, researchers must embrace model modularization and hardwaresoftware codesign. Low-Rank Adaptation (LoRA) adapters can be injected into pretrained transformer layers, enabling task-specific finetuning without modifying core weights [72]. Knowledge distillation from 26 large teacher VLAs into lightweight student networksusing studentteacher mutual information objectivesyields compact models with 510 fewer parameters while retaining 9095% task performance [93]. Mixed-precision quantization augmented by quantizationaware training can compress weights to 48 bits, cutting memory bandwidth and energy consumption by over 60% [94]. Hardware accelerators tailored for VLA workloadssupporting sparse tensor operations, dynamic token routing, and fused visionlanguage kernelscan deliver sustained 100+ TOPS throughput within 2030 power envelope, meeting the demands of embedded robotic platforms [133, 186]. Toolchains like TensorRTLLM [100] and TVM can optimize end-to-end VLA graphs for specific edge devices, fusing layers and precomputing static subgraphs. Emerging architectures such as TinyVLA demonstrate that sub1 parameter VLAs can achieve nearstate-of-the-art performance on manipulation benchmarks with realtime inference, charting path for widespread deployment in resource-constrained settings. Robustness and Ethical Challenges in VLA Deployment. Ensuring VLA robustness and ethical integrity requires both technical and governance measures. Domain randomization and synthetic augmentation pipelineslike UniSims closedloop sensor simulatorgenerate photorealistic variations in lighting, occlusion, and sensor noise, enhancing model resilience to environmental shifts [200]. Adaptive recalibration modules, which adjust perception thresholds and control gains based on real-time feedback, further mitigate drift and sensor degradation over prolonged operation. On the ethical front, bias auditing tools must scan training datasets for skewed demographic or semantic distributions, followed by corrective fine-tuning using adversarial debiasing and counterfactual augmentation [145, 212]. Privacy-preserving inferencingvia ondevice processing, homomorphic encryption for sensitive data streams, and differential privacy during trainingsafeguards user data in applications like healthcare and smart homes [124, 140]. Socioeconomic impacts can be managed through transparent impact assessments and stakeholder engagement, ensuring that VLA adoption complements human labor through upskilling programs rather than displacing workers en masse. Finally, establishing regulatory frameworks and industry standards for VLA safety and accountability will underpin responsible innovation, balancing technical capabilities with societal values. 5.2. Future Roadmap The future of VLA models lies at the intersection of increasingly powerful multimodal foundations, agentic reasoning, and embodied continual learning. Over the next decade, we anticipate several converging trends that will propel VLAs from capable but narrow task specialists toward the core of truly generalist robotic intelligence. 27 1. Multimodal Foundation Models as the Cortex. Todays VLAs typically couple vision-language backbone with task-specific policy heads. Tomorrow, we expect single, massive multimodal foundation modeltrained on web-scale image, video, text, and affordance datato serve as shared perceptual and conceptual cortex. This foundation will encode not only static scenes but also dynamics, physics, and common-sense world knowledge, enabling downstream action learners to tap into unified representational substrate rather than reinventing basic perceptual skills for every robot or domain. 2. Agentic, Self-Supervised Lifelong Learning. Rather than static pretraining, future VLAs will engage in continual, self-supervised interaction with their environments. Agentic frameworkswhere the model generates its own exploration objectives, hypothesizes outcomes, and selfcorrects via simulated or real rolloutswill drive rapid skill acquisition. By formulating internal sub-goals (learn to open drawers, map furniture affordances) and integrating reinforcement-style feedback, VLA-driven humanoid could autonomously expand its capabilities over years of deployment, much like human apprentice. 3. Hierarchical, Neuro-Symbolic Planning. To scale from low-level motor primitives to high-level reasoning, VLAs will adopt hierarchical control architectures. top-level language-grounded planner (perhaps an LLM variant finetuned for affordance reasoning) will decompose complex instructions (prepare cup of tea) into sequences of sub-tasks (fetch kettle, fill water, heat water, steep tea bag). Mid-level modules will translate these into parameterized motion plans, and low-level diffusion or transformer-based controllers will generate smooth, compliant trajectories in real time. This neuro-symbolic blend ensures both the interpretability of structured plans and the flexibility of learned policies. 4. Real-Time Adaptation via World Models. Robustness in unstructured settings demands that VLAs maintain an internal, predictive world modelan up-to-date simulation of objects, contacts, and agent dynamics. As the robot acts, it will continuously reconcile its predictions with sensor feedback, using model-based corrective actions when discrepancies arise (e.g., slipping grasp). Advances in differentiable physics and video-to-state encoders will make these world models both accurate and efficient enough for on-board, real-time use. Cross-Embodiment and Transfer Learning: The era of training separate VLAs for each robot morphology will give way to embodiment-agnostic policies. By encoding actions in an abstract, kinematicagnostic space (e.g., apply grasp force at these affordance points), future VLAs will transfer skills seamlessly between wheeled platforms, quadrupeds, and humanoids. Combined with meta-learning, new robot can bootstrap prior skills with only few minutes of calibration data. Safety, Ethics, and Human-Centered Alignment As VLAs gain autonomy, built-in safety and value alignment become non-negotiable. Future systems will integrate real-time risk estimatorsassessing potential harm to humans or property before executing high-risk maneuversand seek natural language consent for ambiguous situations. Regulatory constraints and socially aware policies will be baked into the VLA stack, ensuring that robots defer to human preferences and legal norms. Figure 18: This conceptual illustration presents Eva, future humanoid assistant powered by Vision-Language Models (VLMs), VLA frameworks, and agentic AI systems. VLMs enable semantic scene understanding and object affordance prediction, while VLAs translate language-grounded instructions into hierarchical motor plans. Agentic AI modules ensure adaptive learning, selfrefinement, and interactive decision-making in open-ended environments. Together, these components represent foundational blueprint for Artificial General Intelligence (AGI) in robotics, where perception, language understanding, planning, and safe autonomous behavior converge in real-world, socially aware tasks. As illustrated in Figure 18, the future of VLA-based robotics lies in the integration of three foundational components: Vision-Language Models (VLMs), VLA architectures, and agentic AI systems. Consider Eva, generalist humanoid assistant operating in household. At the perception layer, Evas foundation VLM interprets multimodal inputs by segmenting visual scenes into discrete object-level representations, predicting affordances (e.g., graspable, fragile), and simulating dynamic behaviors through an internal world model. This VLM layer enables high-level visual understanding grounded in language semantics and physical properties. Upon receiving user command such as Eva, clean the coffee spill and water the plants, the VLA module activates. This core architecture combines tokenized language inputs and sensory feedback to perform hierarchical task planning. high-level planner decomposes the instruction into actionable subtasks (e.g., locate cloth, wipe spill, retrieve watering can), which are then converted into motion trajectories via mid-level policy module. These plans are passed to low-level diffusion-policy controller, responsible for generating smooth, physics-aware joint movements tailored to the robots embodiment. Complementing these is Evas agentic AI module, which supports continual learning and adaptation. When confronted with unexpected challengeslike sticky stainEva invokes an internal selfimprovement loop, running real-time simulated variations to refine its wiping strategy without human supervision. Safety and alignment are ensured through human-aware policies: proxim28 ity sensors, real-time monitoring, and verbal confirmations before high-risk actions. Overnight, Eva performs autonomous review of performance logs, refining sub-policies via simulated rollouts. Together, this VLM-VLA-agentic triad marks significant leap toward embodied AGI. It enables robots like Eva to perceive, plan, act, adapt, and safely coexist with humans, ultimately transforming how intelligent systems interact with real-world environments in robust, interpretable, and humanaligned ways. 6. Conclusion In this comprehensive review, we systematically evaluated the recent developments, methodologies, and applications of VLA models published over the last three years. Our analysis began with the foundational concepts of VLAs, defining their role as multimodal systems that unify visual perception, natural language understanding, and action generation in physical or simulated environments. We traced their evolution and timeline, detailing key milestones that marked the transition from isolated perception-action modules to fully unified, instructionfollowing robotic agents. We highlighted how multimodal integration has maturedfrom loosely coupled pipelines to transformer-based architectures that enable seamless coordination between modalities. Next, we examined tokenization and representation techniques, focusing on how VLAs encode visual and linguistic information, including action primitives and spatial semantics. We explored learning paradigms, detailing the datasets and training strategiesfrom supervised learning and imitation learning to reinforcement learning and multimodal pretrainingthat have shaped VLA performance. In our section on adaptive control and real-time execution, we addressed how modern VLAs are optimized for dynamic environments, discussing policies that support latency-sensitive tasks. We then categorized major architectural innovations, surveying over 50 recent VLA models. This included advancements in model design, memory systems, and interaction fidelity. We further studied training and efficiency strategies, including parameterefficient methods like LoRA, quantization, and model pruning, alongside acceleration techniques such as parallel decoding and hardware-aware inference. Our analysis continued with realworld applications of VLA models, showcasing their deployment across six domains: humanoid robotics, autonomous vehicles, industrial automation, healthcare, agriculture, and augmented reality (AR) navigation. Each application was reviewed with examples of model performance, domain-specific challenges, and generalizability. In addressing challenges and limitations, we focused on five core areas: real-time inference, multimodal action representation and safety, bias and generalization, system integration and compute constraints, and ethical deployment. We proposed potential solutions drawn from current literature, including model compression, cross-modal grounding, domain adaptation, and agentic learning frameworks. Finally, our discussion and future roadmap articulated how the convergence of VLMs, VLA architectures, and agentic AI systems is steering robotics toward artificial general intelligence (AGI). This review provides unified understanding of VLA advancements, identifies unresolved challenges, and outlines structured path forward for developing intelligent, embodied, and human-aligned agents."
        },
        {
            "title": "Acknowledgement",
            "content": "This work was supported by the National Science Foundation and the United States Department of Agriculture, National Institute of Food and Agriculture through the Artificial Intelligence (AI) Institute for Agriculture Program under Award AWD003473, and AWD004595, Accession Number 1029004, Robotic Blossom Thinning with Soft Manipulators."
        },
        {
            "title": "Declarations",
            "content": "The authors declare no conflicts of interest."
        },
        {
            "title": "Statement on AI Writing Assistance",
            "content": "ChatGPT and Perplexity were utilized to enhance grammatical accuracy and refine sentence structure; all AI-generated revisions were thoroughly reviewed and edited for relevance. Additionally, ChatGPT-4o was employed to generate realistic visualizations."
        },
        {
            "title": "References",
            "content": "[1] Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F.L., Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S., et al., 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774 . [2] Agarwal, L., Verma, B., 2024. From methods to datasets: survey on image-caption generators. Multimedia Tools and Applications 83, 2807728123. [3] Alayrac, J.B., Donahue, J., Luc, P., Miech, A., Barr, I., Hasson, Y., Lenc, K., Mensch, A., Millican, K., Reynolds, M., et al., 2022. Flamingo: visual language model for few-shot learning. Advances in neural information processing systems 35, 2371623736. [4] Anderson, P., Shrivastava, A., Truong, J., Majumdar, A., Parikh, D., Batra, D., Lee, S., 2021. Sim-to-real transfer for vision-and-language navigation, in: Conference on Robot Learning, PMLR. pp. 671681. [5] Arai, H., Miwa, K., Sasaki, K., Watanabe, K., Yamaguchi, Y., Aoki, S., Yamamoto, I., 2025. Covla: Comprehensive vision-language-action dataset for autonomous driving, in: 2025 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), IEEE. pp. 19331943. [6] Asif, S., Bueno, M., Ferreira, P., Anandan, P., Zhang, Z., Yao, Y., Ragunathan, G., Tinkler, L., Sotoodeh-Bahraini, M., Lohse, N., et al., 2025. Rapid and automated configuration of robot manufacturing cells. Robotics and Computer-Integrated Manufacturing 92, 102862. [7] Assres, G., Bhandari, G., Shalaginov, A., Gronli, T.M., Ghinea, G., 2025. State-of-the-art and challenges of engineering ml-enabled software systems in the deep learning era. ACM Computing Surveys . [8] Asuzu, K., Singh, H., Idrissi, M., 2025. Humanrobot interaction Intelligent through joint robot planning with large language models. Service Robotics , 117. [9] Ayaz, M., Khan, M., Saqib, M., Khelifi, A., Sajjad, M., Elsaddik, A., 2024. Medvlm: Medical vision-language model for consumer devices. IEEE Consumer Electronics Magazine . [10] Bartoccioni, F., Ramzi, E., Besnier, V., Venkataramanan, S., Vu, T.H., Xu, Y., Chambon, L., Gidaris, S., Odabas, S., Hurych, D., et al., 2025. Vavim and vavam: Autonomous driving through video generative modeling. arXiv preprint arXiv:2502.15672 . 29 [11] Bathula, N.V., Paleti, I., Pagidi, S., Akkumahanthi, S.S., Guduru, N.T., 2024. Policy learning-based image captioning with vision transformer, in: 2024 IEEE International Students Conference on Electrical, Electronics and Computer Science (SCEECS), IEEE. pp. 16. [12] Belkhale, S., Ding, T., Xiao, T., Sermanet, P., Vuong, Q., Tompson, J., Chebotar, Y., Dwibedi, D., Sadigh, D., 2024. Rt-h: Action hierarchies using language. arXiv preprint arXiv:2403.01823 . [13] Bjorck, J., Castaneda, F., Cherniadev, N., Da, X., Ding, R., Fan, L., Fang, Y., Fox, D., Hu, F., Huang, S., et al., 2025. Gr00t n1: An open foundation model for generalist humanoid robots. arXiv preprint arXiv:2503.14734 . [14] Black, K., Brown, N., Driess, D., Esmail, A., Equi, M., Finn, C., Fusai, N., Groom, L., Hausman, K., Ichter, B., et al., 2024. Pi-0: visionlanguage-action flow model for general robot control. arXiv preprint arXiv:2410.24164 . [15] Bolya, D., Huang, P.Y., Sun, P., Cho, J.H., Madotto, A., Wei, C., Ma, T., Zhi, J., Rajasegaran, J., Rasheed, H., et al., 2025. Perception encoder: The best visual embeddings are not at the output of the network. arXiv preprint arXiv:2504.13181 . [16] Bordes, F., Pang, R.Y., Ajay, A., Li, A.C., Bardes, A., Petryk, S., Manas, O., Lin, Z., Mahmoud, A., Jayaraman, B., et al., 2024. An introduction to vision-language modeling. arXiv preprint arXiv:2405.17247 . [17] Brohan, A., Brown, N., Carbajal, J., Chebotar, Y., Chen, X., Choromanski, K., Ding, T., Driess, D., Dubey, A., Finn, C., et al., 2023. Rt-2: Vision-language-action models transfer web knowledge to robotic control. arXiv preprint arXiv:2307.15818 . [18] Brohan, A., Brown, N., Carbajal, J., Chebotar, Y., Dabis, J., Finn, C., Gopalakrishnan, K., Hausman, K., Herzog, A., Hsu, J., et al., 2022. Rt1: Robotics transformer for real-world control at scale. arXiv preprint arXiv:2212.06817 . [19] Budzianowski, P., Maa, W., Freed, M., Mo, J., Xie, A., Tipnis, V., Bolte, B., 2024. Edgevla: Efficient vision-language-action models. environments 20, 3. [20] Cangelosi, A., Metta, G., Sagerer, G., Nolfi, S., Nehaniv, C., Fischer, K., Tani, J., Belpaeme, T., Sandini, G., Nori, F., et al., 2010. Integration of action and language knowledge: roadmap for developmental robotics. IEEE Transactions on Autonomous Mental Development 2, 167195. [21] Cao, J., Gan, Z., Cheng, Y., Yu, L., Chen, Y.C., Liu, J., 2020. Behind the scene: Revealing the secrets of pre-trained vision-and-language models, in: Computer VisionECCV 2020: 16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part VI 16, Springer. pp. 565580. [22] Cao, L., 2024. Ai robots and humanoid ai: Review, perspectives and directions. arXiv preprint arXiv:2405.15775 . [23] Chang, Y., Wang, X., Wang, J., Wu, Y., Yang, L., Zhu, K., Chen, H., Yi, X., Wang, C., Wang, Y., et al., 2024. survey on evaluation of large language models. ACM transactions on intelligent systems and technology 15, 145. [24] Chatzopoulos, D., Bermejo, C., Huang, Z., Hui, P., 2017. Mobile augmented reality survey: From where we are to where we go. Ieee Access 5, 69176950. [25] Chen, B., Xu, Z., Kirmani, S., Ichter, B., Sadigh, D., Guibas, L., Xia, F., 2024a. Spatialvlm: Endowing vision-language models with spatial reasoning capabilities, in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1445514465. [26] Chen, H., Hou, L., Wu, S., Zhang, G., Zou, Y., Moon, S., Bhuiyan, M., 2024b. Augmented reality, deep learning and vision-language query system for construction worker safety. Automation in Construction 157, 105158. [27] Chen, H., Li, S., Fan, J., Duan, A., Yang, C., Navarro-Alarcon, D., Zheng, P., 2025a. Human-in-the-loop robot learning for smart manufacturing: human-centric perspective. IEEE Transactions on Automation Science and Engineering . [28] Chen, H., Liu, B., Wang, S., Wang, X., Han, W., Zhu, Y., Wang, X., Bi, Y., 2025b. Language modulates vision: Evidence from neural networks and human brain-lesion models. arXiv preprint arXiv:2501.13628 . [29] Chen, P., Bu, P., Wang, Y., Wang, X., Wang, Z., Guo, J., Zhao, Y., Zhu, Q., Song, J., Yang, S., et al., 2025c. Combatvla: An efficient visionlanguage-action model for combat tasks in 3d action role-playing games. arXiv preprint arXiv:2503.09527 . [30] Chen, X., Xu, W., Kan, S., Zhang, L., Jin, Y., Cen, Y., Li, Y., 2025d. Vision-semantics-label: new two-step paradigm for action recognition with large language model. IEEE Transactions on Circuits and Systems for Video Technology . [31] Chen, Y., Tian, S., Liu, S., Zhou, Y., Li, H., Zhao, D., 2025e. Conrft: reinforced fine-tuning method for vla models via consistency policy. arXiv preprint arXiv:2502.05450 . [32] Cheng, A.C., Ji, Y., Yang, Z., Gongye, Z., Zou, X., Kautz, J., Bıyık, E., Yin, H., Liu, S., Wang, X., 2024a. Navila: Legged robot visionlanguage-action model for navigation. arXiv preprint arXiv:2412.04453 . [33] Cheng, H., Xiao, E., Yu, C., Yao, Z., Cao, J., Zhang, Q., Wang, J., Sun, M., Xu, K., Gu, J., et al., 2024b. Manipulation facing threats: Evaluating physical vulnerabilities in end-to-end vision language action models. arXiv preprint arXiv:2409.13174 . [34] Chi, C., Xu, Z., Feng, S., Cousineau, E., Du, Y., Burchfiel, B., Tedrake, R., Song, S., 2023. Diffusion policy: Visuomotor policy learning via action diffusion. The International Journal of Robotics Research , 02783649241273668. [35] Chiang, H.T.L., Xu, Z., Fu, Z., Jacob, M.G., Zhang, T., Lee, T.W.E., Yu, W., Schenck, C., Rendleman, D., Shah, D., et al., 2024. Mobility vla: Multimodal instruction navigation with long-context vlms and topological graphs. arXiv preprint arXiv:2407.07775 . [36] Dang, R., Yuan, Y., Zhang, W., Xin, Y., Zhang, B., Li, L., Wang, L., Zeng, Q., Li, X., Bing, L., 2025. Ecbench: Can multi-modal foundation models understand the egocentric world? holistic embodied cognition benchmark. arXiv preprint arXiv:2501.05031 . [37] Dasari, S., Ebert, F., Tian, S., Nair, S., Bucher, B., Schmeckpeper, K., Singh, S., Levine, S., Finn, C., 2019. Robonet: Large-scale multi-robot learning. arXiv preprint arXiv:1910.11215 . [38] Deng, S., Yan, M., Wei, S., Ma, H., Yang, Y., Chen, J., Zhang, Z., Yang, T., Zhang, X., Cui, H., Zhang, Z., Wang, H., 2025. Graspvla: grasping foundation model pre-trained on billion-scale synthetic action data. URL: https://arxiv.org/abs/2505.03233, arXiv:2505.03233. [39] Dey, S., Zaech, J.N., Nikolov, N., Van Gool, L., Paudel, D.P., 2024. Revla: Reverting visual domain limitation of robotic foundation models. arXiv preprint arXiv:2409.15250 . [40] Ding, D., Yao, T., Luo, R., Sun, X., 2025a. Visual question answering in robotic surgery: comprehensive review. IEEE Access . [41] Ding, J., Zhang, Y., Shang, Y., Zhang, Y., Zong, Z., Feng, J., Yuan, Y., Su, H., Li, N., Sukiennik, N., et al., 2024a. Understanding world or predicting future? comprehensive survey of world models. arXiv preprint arXiv:2411.14499 . [42] Ding, P., Ma, J., Tong, X., Zou, B., Luo, X., Fan, Y., Wang, T., Lu, H., Mo, P., Liu, J., et al., 2025b. Humanoid-vla: Towards universal humanoid control with visual integration. arXiv preprint arXiv:2502.14795 . [43] Ding, P., Zhao, H., Zhang, W., Song, W., Zhang, M., Huang, S., Yang, N., Wang, D., 2024b. Quar-vla: Vision-language-action model for quadruped robots, in: European Conference on Computer Vision, Springer. pp. 352367. [44] Donahue, J., Anne Hendricks, L., Guadarrama, S., Rohrbach, M., Venugopalan, S., Saenko, K., Darrell, T., 2015. Long-term recurrent convolutional networks for visual recognition and description, in: Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 26252634. [45] Dong, H., Liu, M., Zhou, K., Chatzi, E., Kannala, J., Stachniss, C., Fink, O., 2025. Advances in multimodal adaptation and generalization: From traditional approaches to foundation models. arXiv preprint arXiv:2501.18592 . [46] Doveh, S., Arbelle, A., Harary, S., Schwartz, E., Herzig, R., Giryes, R., Feris, R., Panda, R., Ullman, S., Karlinsky, L., 2023. Teaching structured vision & language concepts to vision & language models, in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 26572668. [47] Driess, D., Xia, F., Sajjadi, M.S., Lynch, C., Chowdhery, A., Wahid, A., Tompson, J., Vuong, Q., Yu, T., Huang, W., et al., 2023. Palm-e: An embodied multimodal language model. Openreview . [48] Duan, J., Pumacay, W., Kumar, N., Wang, Y.R., Tian, S., Yuan, W., Krishna, R., Fox, D., Mandlekar, A., Guo, Y., 2024. Aha: visionlanguage-model for detecting and reasoning over failures in robotic manipulation. arXiv preprint arXiv:2410.00371 . [49] Duarte, N.F., Rakovic, M., Tasevski, J., Coco, M.I., Billard, A., SantosVictor, J., 2018. Action anticipation: Reading the intentions of humans and robots. IEEE Robotics and Automation Letters 3, 41324139. [50] Ebert, F., Yang, Y., Schmeckpeper, K., Bucher, B., Georgakis, G., Daniilidis, K., Finn, C., Levine, S., 2021. Bridge data: Boosting generalization of robotic skills with cross-domain datasets. arXiv preprint arXiv:2109.13396 . [51] Fan, C., Jia, X., Sun, Y., Wang, Y., Wei, J., Gong, Z., Zhao, X., Tomizuka, M., Yang, X., Yan, J., Ding, M., 2025. Interleave-vla: Enhancing robot manipulation with interleaved image-text instructions. URL: https://arxiv.org/abs/2505.02152, arXiv:2505.02152. [52] Fan, L., Chen, K., Xu, Z., Yuan, M., Huang, P., Huang, W., 2024. Language reasoning in vision-language-action model for robotic grasping, in: 2024 China Automation Congress (CAC), IEEE. pp. 66566661. [53] Fang, Y., Yang, Y., Zhu, X., Zheng, K., Bertasius, G., Szafir, D., Ding, M., 2025. Rebot: Scaling robot learning with real-to-sim-to-real robotic video synthesis. arXiv preprint arXiv:2503.14526 . [54] Firoozi, R., Tucker, J., Tian, S., Majumdar, A., Sun, J., Liu, W., Zhu, Y., Song, S., Kapoor, A., Hausman, K., et al., 2023. Foundation models in robotics: Applications, challenges, and the future. The International Journal of Robotics Research , 02783649241281508. [55] Foster, D.J., Block, A., Misra, D., 2024. need? understanding horizon in imitation learning. arXiv:2407.15007 . Is behavior cloning all you arXiv preprint [56] Fu, H., Zhang, D., Zhao, Z., Cui, J., Liang, D., Zhang, C., Zhang, D., Xie, H., Wang, B., Bai, X., 2025. Orion: holistic end-to-end autonomous driving framework by vision-language instructed action generation. arXiv preprint arXiv:2503.19755 . [57] Gao, B., Liu, Y., Li, Y., Li, H., Li, M., He, W., 2025a. vision-language model for predicting potential distribution land of soybean double cropping. Frontiers in Environmental Science 12, 1515752. [58] Gao, J., Belkhale, S., Dasari, S., Balakrishna, A., Shah, D., Sadigh, D., 2025b. taxonomy for evaluating generalist robot policies. arXiv preprint arXiv:2503.01238 . [59] Gbagbe, K.F., Cabrera, M.A., Alabbas, A., Alyunes, O., Lykov, A., Tsetserukou, D., 2024. Bi-vla: Vision-language-action model-based system for bimanual robotic dexterous manipulations, in: 2024 IEEE International Conference on Systems, Man, and Cybernetics (SMC), IEEE. pp. 28642869. [60] Geens, R., 2024. Bringing generative ai to edge devices through interoperable compute cores, in: Flanders AI Research Day, Location: Ghent. [61] Ghosh, A., Acharya, A., Saha, S., Jain, V., Chadha, A., 2024. Exploring the frontier of vision-language models: survey of current methodologies and future directions. arXiv preprint arXiv:2404.07214 . [62] Gu, J., Wang, Z., Kuen, J., Ma, L., Shahroudy, A., Shuai, B., Liu, T., Wang, X., Wang, G., Cai, J., et al., 2018. Recent advances in convolutional neural networks. Pattern recognition 77, 354377. [63] Gu, X., Wen, C., Ye, W., Song, J., Gao, Y., 2023. Seer: Language instructed video prediction with latent diffusion models. arXiv preprint arXiv:2303.14897 . [64] Gu, Z., Li, J., Shen, W., Yu, W., Xie, Z., McCrory, S., Cheng, X., Shamsah, A., Griffin, R., Liu, C.K., et al., 2025. Humanoid locomotion and manipulation: Current progress and challenges in control, planning, and learning. arXiv preprint arXiv:2501.02116 . [65] Guo, Y., Zhang, J., Chen, X., Ji, X., Wang, Y.J., Hu, Y., Chen, J., 2025. Improving vision-language-action model with online reinforcement learning. arXiv preprint arXiv:2501.16664 . [66] Guruprasad, P., Sikka, H., Song, J., Wang, Y., Liang, P.P., 2024. Benchmarking vision, language, & action models on robotic learning tasks. arXiv preprint arXiv:2411.05821 . [67] Haldar, S., Peng, Z., Pinto, L., 2024. Baku: An efficient transformer for multi-task policy learning. arXiv preprint arXiv:2406.07539 . [68] Han, S., Wang, M., Zhang, J., Li, D., Duan, J., 2024. review of large language models: Fundamental architectures, key technological evolutions, interdisciplinary technologies integration, optimization and compression techniques, applications, and challenges. Electronics 13, 5040. [69] Hanson, A., Riseman, E., 2014. The visions image-understanding system, in: Advances in Computer Vision. Psychology Press, pp. 1114. [70] Hao, P., Zhang, C., Li, D., Cao, X., Hao, X., Cui, S., Wang, S., 2025. Tla: Tactile-language-action model for contact-rich manipulation. arXiv 30 preprint arXiv:2503.08548 . [71] Hong, Y., 2025. Building 3D Foundation Models for the Embodied Minds. Ph.D. thesis. University of California, Los Angeles. [72] Hu, E.J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., Chen, W., et al., 2022. Lora: Low-rank adaptation of large language models. ICLR 1, 3. [73] Hu, Y., Tang, J., Gong, X., Zhou, Z., Zhang, S., Elvitigala, D.S., Mueller, F., Hu, W., Quigley, A.J., 2025. Vision-based multimodal interfaces: survey and taxonomy for enhanced context-aware system design. arXiv preprint arXiv:2501.13443 . [74] Huang, H., Liu, F., Fu, L., Wu, T., Mukadam, M., Malik, J., Goldberg, K., Abbeel, P., 2024. Early fusion helps vision language action models generalize better, in: 1st Workshop on X-Embodiment Robot Learning. [75] Huang, H., Liu, F., Fu, L., Wu, T., Mukadam, M., Malik, J., Goldberg, K., Abbeel, P., 2025a. Otter: vision-language-action model with textaware visual feature extraction. arXiv preprint arXiv:2503.03734 . [76] Huang, S., Dong, L., Wang, W., Hao, Y., Singhal, S., Ma, S., Lv, T., Cui, L., Mohammed, O.K., Patra, B., et al., 2023a. Language is not all you need: Aligning perception with language models. Advances in Neural Information Processing Systems 36, 7209672109. [77] Huang, W., Gu, Q., Ye, N., 2025b. Decision spikeformer: Spike-driven transformer for decision making. arXiv preprint arXiv:2504.03800 . [78] Huang, W., Wang, C., Zhang, R., Li, Y., Wu, J., Fei-Fei, L., 2023b. Voxposer: Composable 3d value maps for robotic manipulation with language models. arXiv preprint arXiv:2307.05973 . [79] Hung, C.Y., Sun, Q., Hong, P., Zadeh, A., Li, C., Tan, U., Majumder, N., Poria, S., et al., 2025. Nora: small open-sourced generalist vision language action model for embodied tasks. arXiv preprint arXiv:2504.19854 . [80] Ikeda, B., Gramopadhye, M., Nekervis, L., Szafir, D., 2025. Marcer: Multimodal augmented reality for composing and executing robot tasks, in: 2025 20th ACM/IEEE International Conference on Human-Robot Interaction (HRI), IEEE. pp. 529539. [81] Imran, A., Gopalakrishnan, K., 2025. Foundation models in robotics, in: AI for Robotics: Toward Embodied and General Intelligence in the Physical World. Springer, pp. 139210. [82] Intelligence, P., Black, K., Brown, N., Darpinian, J., Dhabalia, K., Driess, D., Esmail, A., Equi, M., Finn, C., Fusai, N., et al., 2025. pi0.5: vision-language-action model with open-world generalization. arXiv preprint arXiv:2504.16054 . [83] Jeong, H., Lee, H., Kim, C., Shin, S., 2024. survey of robot intelligence with large language models. Applied Sciences 14, 8868. [84] Jha, K., Doshi, A., Patel, P., Shah, M., 2019. comprehensive review on automation in agriculture using artificial intelligence. Artificial Intelligence in Agriculture 2, 112. [85] Jiang, J., Xiao, W., Lin, Z., Zhang, H., Ren, T., Gao, Y., Lin, Z., Cai, Z., Yang, L., Liu, Z., 2024. Solami: Social vision-language-action modeling for immersive interaction with 3d autonomous characters. arXiv preprint arXiv:2412.00174 . [86] Jiang, Y., Gupta, A., Zhang, Z., Wang, G., Dou, Y., Chen, Y., Fei-Fei, L., Anandkumar, A., Zhu, Y., Fan, L., 2022. Vima: General robot manipulation with multimodal prompts. arXiv preprint arXiv:2210.03094 2, 6. [87] Jiang, Y., Zhang, R., Wong, J., Wang, C., Ze, Y., Yin, H., Gokmen, C., Song, S., Wu, J., Fei-Fei, L., 2025. Behavior robot suite: Streamlining real-world whole-body manipulation for everyday household activities. arXiv preprint arXiv:2503.05652 . [88] Jones, J., Mees, O., Sferrazza, C., Stachowicz, K., Abbeel, P., Levine, S., 2025. Beyond sight: Finetuning generalist robot policies with heterogeneous sensors via language grounding. arXiv preprint arXiv:2501.04693 . [89] Karamcheti, S., Zhai, A.J., Losey, D.P., Sadigh, D., 2021. Learning visually guided latent actions for assistive teleoperation, in: Learning for dynamics and control, PMLR. pp. 12301241. [90] Katiyar, N., 2023. Model-Driven Framework for Domain-Specific Adaptation of Time Series Forecasting Pipeline. McGill University (Canada). [91] Kelly, C., Hu, L., Yang, B., Tian, Y., Yang, D., Yang, C., Huang, Z., Li, Z., Hu, J., Zou, Y., 2024. Visiongpt: Vision-language understanding agent using generalized multimodal framework. arXiv preprint arXiv:2403.09027 . [92] Khan, M.H., Asfaw, S., Iarchuk, D., Cabrera, M.A., Moreno, L., Tokmurziyev, I., Tsetserukou, D., 2025. Shake-vla: Vision-language-action model-based system for bimanual robotic manipulations and liquid mixing. arXiv preprint arXiv:2501.06919 . [93] Kim, M.J., Finn, C., Liang, P., 2025. Fine-tuning vision-languagearXiv preprint action models: Optimizing speed and success. arXiv:2502.19645 . [94] Kim, M.J., Pertsch, K., Karamcheti, S., Xiao, T., Balakrishna, A., Nair, S., Rafailov, R., Foster, E., Lam, G., Sanketi, P., et al., 2024. Openvla: An open-source vision-language-action model. arXiv preprint arXiv:2406.09246 . [95] Lee, N., Bang, Y., Lovenia, H., Cahyawijaya, S., Dai, W., Fung, P., 2023. Survey of social bias in vision-language models. arXiv preprint arXiv:2309.14381 . [96] Li, C., Wen, J., Peng, Y., Peng, Y., Feng, F., Zhu, Y., 2025a. Pointvla: Injecting the 3d world into vision-language-action models. arXiv preprint arXiv:2503.07511 . [97] Li, D., Jin, Y., Sun, Y., Yu, H., Shi, J., Hao, X., Hao, P., Liu, H., Sun, F., Zhang, J., et al., 2024a. What foundation models can bring for robot learning in manipulation: survey. arXiv preprint arXiv:2404.18201 . [98] Li, J., Skinner, G., Yang, G., Quaranto, B.R., Schwaitzberg, S.D., towards multimodal surarXiv preprint Kim, P.C., Xiong, J., 2024b. Llava-surg: gical assistant via structured surgical video learning. arXiv:2408.07981 . [99] Li, J., Wei, P., Han, W., Fan, L., 2023. Intentqa: Context-aware video intent reasoning, in: Proceedings of the IEEE/CVF international conference on computer vision, pp. 1196311974. [100] Li, J., Zhu, Y., Tang, Z., Wen, J., Zhu, M., Liu, X., Li, C., Cheng, R., Peng, Y., Feng, F., 2024c. Improving vision-language-action models via chain-of-affordance. arXiv preprint arXiv:2412.20451 . [101] Li, M., Wang, Z., He, K., Ma, X., Liang, Y., 2025b. Jarvis-vla: Posttraining large-scale vision language models to play visual games with keyboards and mouse. arXiv preprint arXiv:2503.16365 . [102] Li, Q., Liang, Y., Wang, Z., Luo, L., Chen, X., Liao, M., Wei, F., Deng, Y., Xu, S., Zhang, Y., et al., 2024d. Cogact: foundational visionlanguage-action model for synergizing cognition and action in robotic manipulation. arXiv preprint arXiv:2411.19650 . [103] Li, S., Wang, J., Dai, R., Ma, W., Ng, W.Y., Hu, Y., Li, Z., 2024e. Robonurse-vla: Robotic scrub nurse system based on vision-languageaction model. arXiv preprint arXiv:2409.19590 . [104] Li, Y., Deng, Y., Zhang, J., Jang, J., Memmel, M., Yu, R., Garrett, C.R., Ramos, F., Fox, D., Li, A., et al., 2025c. Hamster: Hierarchical action models for open-world robot manipulation. arXiv preprint arXiv:2502.05485 . [105] Li, Y., Gong, Z., Li, H., Huang, X., Kang, H., Bai, G., Ma, X., 2025d. Robotic visual instruction. arXiv preprint arXiv:2505.00693 . [106] Li, Y., Lai, Z., Bao, W., Tan, Z., Dao, A., Sui, K., Shen, J., Liu, D., Liu, H., Kong, Y., 2025e. Visual large language models for generalized and specialized applications. arXiv preprint arXiv:2501.02765 . [107] Li, Z., Wu, X., Du, H., Nghiem, H., Shi, G., 2025f. Benchmark evaluations, applications, and challenges of large vision language models: survey. arXiv preprint arXiv:2501.02189 1. [108] Lin, K.Q., Li, L., Gao, D., Yang, Z., Wu, S., Bai, Z., Lei, W., Wang, L., Shou, M.Z., 2024. Showui: One vision-language-action model for gui visual agent. arXiv preprint arXiv:2411.17465 . [109] Lin, Y., Zhou, H., Chen, M., Min, H., 2019. Automatic sorting system for industrial robot with 3d visual perception and natural language interaction. Measurement and Control 52, 100115. [110] Liu, J., Chen, H., An, P., Liu, Z., Zhang, R., Gu, C., Li, X., Guo, Z., Chen, S., Liu, M., et al., 2025a. Hybridvla: Collaborative diffusion and autoregression in unified vision-language-action model. arXiv preprint arXiv:2503.10631 . [111] Liu, J., Liu, M., Wang, Z., An, P., Li, X., Zhou, K., Yang, S., Zhang, R., Guo, Y., Zhang, S., 2024a. Robomamba: Efficient vision-languageaction model for robotic reasoning and manipulation. Advances in Neural Information Processing Systems 37, 4008540110. [112] Liu, S., Wu, L., Li, B., Tan, H., Chen, H., Wang, Z., Xu, K., Su, H., Zhu, J., 2024b. Rdt-1b: diffusion foundation model for bimanual manipulation. arXiv preprint arXiv:2410.07864 . [113] Liu, Y., Cao, X., Chen, T., Jiang, Y., You, J., Wu, M., Wang, X., Feng, M., Jin, Y., Chen, J., 2025b. From screens to scenes: survey of em31 bodied ai in healthcare. arXiv preprint arXiv:2501.07468 . preprint arXiv:2503.23037 . [114] Liu, Y., Cao, X., Chen, T., Jiang, Y., You, J., Wu, M., Wang, X., Feng, M., Jin, Y., Chen, J., 2025c. survey of embodied ai in healthcare: Techniques, applications, and opportunities. arXiv preprint arXiv:2501.07468 . [115] Liu, Z., Liang, H., Huang, X., Xiong, W., Yu, Q., Sun, L., Chen, C., He, C., Cui, B., Zhang, W., 2024c. Synthvlm: High-efficiency and high-quality synthetic data for vision language models. arXiv preprint arXiv:2407.20756 . [116] Lu, H., Li, H., Shahani, P.S., Herbers, S., Scheutz, M., 2025. Probing vision-language-action model for symbolic states and integration into cognitive architecture. arXiv preprint arXiv:2502.04558 . [117] Lu, J., Clark, C., Lee, S., Zhang, Z., Khosla, S., Marten, R., Hoiem, D., Kembhavi, A., 2024. Unified-io 2: Scaling autoregressive multimodal models with vision language audio and action, in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2643926455. [118] Lu, Y., Liao, Z., 2023. Towards happy housework: Scenario-based experience design for household cleaning robotic system. EAI Endorsed Transactions on Scalable Information Systems 10. [119] Luo, J., Xu, C., Wu, J., Levine, S., 2024. Precise and dexterous robotic arXiv manipulation via human-in-the-loop reinforcement learning. preprint arXiv:2410.21845 . [120] Lyu, J., Li, Z., Shi, X., Xu, C., Wang, Y., Wang, H., 2025. Dywa: Dynamics-adaptive world action model for generalizable non-prehensile manipulation. arXiv preprint arXiv:2503.16806 . [121] Ma, Y., Song, Z., Zhuang, Y., Hao, J., King, I., 2024. survey arXiv preprint on vision-language-action models for embodied ai. arXiv:2405.14093 . [122] Mohammed, M.Q., Chung, K.L., Chyi, C.S., 2020. Review of deep reinforcement learning-based object grasping: Techniques, open challenges, and recommendations. Ieee Access 8, 178450178481. [123] Moroncelli, A., Soni, V., Shahid, A.A., Maccarini, M., Forgione, M., Piga, D., Spahiu, B., Roveda, L., 2024. Integrating reinforcement learning with foundation models for autonomous robotics: Methods and perspectives. arXiv preprint arXiv:2410.16411 . [124] Mumuni, A., Mumuni, F., 2025. Large language models for artificial general intelligence (agi): survey of foundational principles and approaches. arXiv preprint arXiv:2501.03151 . [125] Ni, F., Hao, J., Wu, S., Kou, L., Yuan, Y., Dong, Z., Liu, J., Li, M., Zhuang, Y., Zheng, Y., 2024. Peria: Perceive, reason, imagine, act via holistic language and vision planning for manipulation. Advances in Neural Information Processing Systems 37, 1754117571. [126] Nie, Y., Li, L., Gan, Z., Wang, S., Zhu, C., Zeng, M., Liu, Z., Bansal, M., Wang, L., 2021. Mlp architectures for vision-and-language modeling: An empirical study. arXiv preprint arXiv:2112.04453 . [127] Noorani, E., Serlin, Z., Price, B., Velasquez, A., 2025. From abstraction to reality: Darpas vision for robust sim-to-real autonomy. arXiv preprint arXiv:2503.11007 . [128] Oquab, M., Darcet, T., Moutakanni, T., Vo, H., Szafraniec, M., Khalidov, V., Fernandez, P., Haziza, D., Massa, F., El-Nouby, A., et al., 2023. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193 . [129] Pang, J., Zheng, P., Fan, J., Liu, T., 2025. Towards cognition-augmented human-centric assembly: visual computation perspective. Robotics and Computer-Integrated Manufacturing 91, 102852. [130] Pantalone, D., Faini, G.S., Cialdai, F., Sereni, E., Bacci, S., Bani, D., Bernini, M., Pratesi, C., Stef`ano, P., Orzalesi, L., et al., 2021. Robotassisted surgery in space: pros and cons. review from the surgeons point of view. npj Microgravity 7, 56. [131] Park, S.M., Kim, Y.G., 2023. Visual language integration: survey and open challenges. Computer Science Review 48, 100548. [132] Patel, D., Eghbalzadeh, H., Kamra, N., Iuzzolino, M.L., Jain, U., Desai, R., 2023. Pretrained language models as visual planners for human assistance, in: Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 1530215314. [133] Pertsch, K., Stachowicz, K., Ichter, B., Driess, D., Nair, S., Vuong, Q., Mees, O., Finn, C., Levine, S., 2025. Fast: Efficient action tokenization for vision-language-action models. arXiv preprint arXiv:2501.09747 . [134] Plaat, A., van Duijn, M., van Stein, N., Preuss, M., van der Putten, P., Batenburg, K.J., 2025. Agentic large language models, survey. arXiv [135] Polubarov, A., Lyubaykin, N., Derevyagin, A., Zisman, I., Tarasov, D., Nikulin, A., Kurenkov, V., 2025. Vintix: Action model via in-context reinforcement learning. arXiv preprint arXiv:2501.19400 . [136] Qu, D., Song, H., Chen, Q., Yao, Y., Ye, X., Ding, Y., Wang, Z., Gu, J., Zhao, B., Wang, D., et al., 2025. Spatialvla: Exploring spatial representations for visual-language-action model. arXiv preprint arXiv:2501.15830 . [137] Radford, A., Narasimhan, K., Salimans, T., Sutskever, I., et al., 2018. Improving language understanding by generative pre-training . [138] Rawal, P.K., 2025. An Intelligent Versatile Pipeline for 6D Localization of Industrial Components in Production Environment. Ph.D. thesis. Fraunhofer Verlag. [139] Ray, P.P., 2023. Chatgpt: comprehensive review on background, applications, key challenges, bias, ethics, limitations and future scope. Internet of Things and Cyber-Physical Systems 3, 121154. [140] Raza, S., Qureshi, R., Zahid, A., Fioresi, J., Sadak, F., Saeed, M., Sapkota, R., Jain, A., Zafar, A., Hassan, M.U., et al., 2025. Who is responsible? the data, models, users or regulations? responsible generative ai for sustainable future. arXiv preprint arXiv:2502.08650 . [141] Reed, S., Zolna, K., Parisotto, E., Colmenarejo, S.G., Novikov, A., Barth-Maron, G., Gimenez, M., Sulsky, Y., Kay, J., Springenberg, J.T., et al., 2022. generalist agent. arXiv preprint arXiv:2205.06175 . [142] Rodriguez-Guerra, D., Sorrosal, G., Cabanes, I., Calleja, C., 2021. Human-robot interaction review: Challenges and solutions for modern industrial environments. Ieee Access 9, 108557108578. [143] Rodriguez-Juan, J., Ortiz-Perez, D., Garcia-Rodriguez, J., Tomas, D., Nalepa, G.J., 2025. Integrating advanced vision-language models for context recognition in risks assessment. Neurocomputing 618, 129131. [144] Roychoudhury, A., Khorshidi, S., Agrawal, S., Bennewitz, M., 2023. Perception for humanoid robots. Current Robotics Reports 4, 127140. [145] Sahili, Z.A., Patras, I., Purver, M., 2025. Scaling for fairness? analyzing model size, data composition, and multilinguality in vision-language bias. arXiv preprint arXiv:2501.13223 . [146] Sameni, S., Kafle, K., Tan, H., Jenni, S., 2024. Building vision-language models on solid foundations with masked distillation, in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1421614226. [147] Samson, M., Muraccioli, B., Kanehiro, F., 2025. Scalable, trainingfree visual language robotics: modular multi-model framework for consumer-grade gpus, in: 2025 IEEE/SICE International Symposium on System Integration (SII), IEEE. pp. 193198. [148] Sapkota, R., Karkee, M., 2025. Object detection with multimodal large vision-language models: An in-depth review. Available at SSRN 5233953 . [149] Sapkota, R., Roumeliotis, K.I., Cheppally, R.H., Calero, M.F., Karkee, M., 2025. review of 3d object detection with vision-language models. arXiv preprint arXiv:2504.18738 . [150] Sautenkov, O., Yaqoot, Y., Lykov, A., Mustafa, M.A., Tadevosyan, G., Akhmetkazy, A., Cabrera, M.A., Martynov, M., Karaf, S., Tsetserukou, D., 2025. Uav-vla: Vision-language-action system for large scale aerial mission generation. arXiv preprint arXiv:2501.05014 . [151] Schmidgall, S., Cho, J., Zakka, C., Hiesinger, W., 2024. Gp-vls: arXiv preprint general-purpose vision language model for surgery. arXiv:2407.19305 . [152] Schuhmann, C., Beaumont, R., Vencu, R., Gordon, C., Wightman, R., Cherti, M., Coombes, T., Katta, A., Mullis, C., Wortsman, M., et al., 2022. Laion-5b: An open large-scale dataset for training next generation image-text models. Advances in neural information processing systems 35, 2527825294. [153] Serpiva, V., Lykov, A., Myshlyaev, A., Khan, M.H., Abdulkarim, A.A., Sautenkov, O., Tsetserukou, D., 2025. Racevla: Vla-based racing drone navigation with human-like behaviour. arXiv preprint arXiv:2503.02572 . [154] Sharshar, A., Khan, L.U., Ullah, W., Guizani, M., 2025. Vision-language models for edge networks: comprehensive survey. arXiv preprint arXiv:2502.07855 . [155] Shi, L.X., Ichter, B., Equi, M., Ke, L., Pertsch, K., Vuong, Q., Tanner, J., Walling, A., Wang, H., Fusai, N., et al., 2025. Hi robot: Open-ended instruction following with hierarchical vision-language-action models. arXiv preprint arXiv:2502.19417 . 32 [156] Shin, H.C., Roth, H.R., Gao, M., Lu, L., Xu, Z., Nogues, I., Yao, J., Mollura, D., Summers, R.M., 2016. Deep convolutional neural networks for computer-aided detection: Cnn architectures, dataset characteristics and transfer learning. IEEE transactions on medical imaging 35, 1285 1298. [157] Shridhar, M., Manuelli, L., Fox, D., 2022. Cliport: What and where pathways for robotic manipulation, in: Conference on robot learning, PMLR. pp. 894906. [158] Si, W., Wang, N., Yang, C., 2021. review on manipulation skill acquisition through teleoperation-based learning from demonstration. Cognitive Computation and Systems 3, 116. [159] Singh, S., Singh, J., Shah, B., Sehra, S.S., Ali, F., 2022. Augmented reality and gps-based resource efficient navigation system for outdoor environments: Integrating device camera, sensors, and storage. Sustainability 14, 12720. [160] Song, M., Deng, X., Zhou, Z., Wei, J., Guan, W., Nie, L., 2025a. survey on diffusion policy for robotic manipulation: Taxonomy, analysis, and future directions. Authorea Preprints . [161] Song, W., Chen, J., Ding, P., Zhao, H., Zhao, W., Zhong, Z., Ge, Z., Ma, J., Li, H., 2025b. Accelerating vision-language-action model integrated with action chunking via parallel decoding. arXiv preprint arXiv:2503.02310 . [162] Sun, H., Wang, H., Ma, C., Zhang, S., Ye, J., Chen, X., Lan, X., 2025a. Prism: Projection-based reward integration for scene-aware real-to-sim-to-real transfer with few demonstrations. arXiv preprint arXiv:2504.20520 . [163] Sun, J., Mao, P., Kong, L., Wang, J., 2025b. review of embodied grasping. Sensors (Basel, Switzerland) 25, 852. [164] Sutskever, I., Martens, J., Hinton, G.E., 2011. Generating text with recurrent neural networks, in: Proceedings of the 28th international conference on machine learning (ICML-11), pp. 10171024. [165] Szot, A., Mazoure, B., Agrawal, H., Hjelm, R.D., Kira, Z., Toshev, A., 2024. Grounding multimodal large language models in actions. Advances in Neural Information Processing Systems 37, 2019820224. [166] Team, G.R., Abeyruwan, S., Ainslie, J., Alayrac, J.B., Arenas, M.G., Armstrong, T., Balakrishna, A., Baruch, R., Bauza, M., Blokzijl, M., et al., 2025. Gemini robotics: Bringing ai into the physical world. arXiv preprint arXiv:2503.20020 . [167] Team, O.M., Ghosh, D., Walke, H., Pertsch, K., Black, K., Mees, O., Dasari, S., Hejna, J., Kreiman, T., Xu, C., et al., 2024. Octo: An opensource generalist robot policy. arXiv preprint arXiv:2405.12213 . [168] Tellex, S., Gopalan, N., Kress-Gazit, H., Matuszek, C., 2020. Robots that use language. Annual Review of Control, Robotics, and Autonomous Systems 3, 2555. [169] Tian, H., Wang, T., Liu, Y., Qiao, X., Li, Y., 2020. Computer vision technology in agricultural automationa review. Information processing in agriculture 7, 119. [170] Tian, K., Jiang, Y., Yuan, Z., Peng, B., Wang, L., 2024. Visual autoregressive modeling: Scalable image generation via next-scale prediction. Advances in neural information processing systems 37, 8483984865. [171] Torres, N., Ulloa, C., Araya, I., Ayala, M., Jara, S., 2024. comprehensive analysis of gender, racial, and prompt-induced biases in large language models. International Journal of Data Science and Analytics , 138. [172] Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et al., 2023. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288 . [173] Trivedi, C., Bhattacharya, P., Prasad, V.K., Patel, V., Singh, A., Tanwar, S., Sharma, R., Aluvala, S., Pau, G., Sharma, G., 2024. Explainable ai for industry 5.0: vision, architecture, and potential directions. IEEE Open Journal of Industry Applications . [174] Verbaan, L., 2024. Perception and control with large language models in robotic manipulation. TU Delft Library . [175] Vuong, Q., Levine, S., Walke, H.R., Pertsch, K., Singh, A., Doshi, R., Xu, C., Luo, J., Tan, L., Shah, D., et al., 2023. Open xembodiment: Robotic learning datasets and rt-x models, in: Towards Generalist Robots: Learning Paradigms for Scalable Skill Acquisition@ CoRL2023. [176] Wang, G., Bai, L., Nah, W.J., Wang, J., Zhang, Z., Chen, Z., Wu, J., Islam, M., Liu, H., Ren, H., 2024a. Surgical-lvlm: Learning to adapt large vision-language model for grounded visual question answering in robotic surgery. arXiv preprint arXiv:2405.10948 . [177] Wang, H., Xing, Z., Wu, W., Yang, Y., Tang, Q., Zhang, M., Xu, Y., Zhu, L., 2024b. Non-invasive to invasive: Enhancing ffa synthesis from cfp with benchmark dataset and novel network, in: Proceedings of the 1st International Workshop on Multimedia Computing for Health and Medicine, pp. 715. [178] Wang, J., Guo, D., Liu, H., 2025a. Where to learn: Embodied perception learning planned by vision-language models. IEEE Transactions on Cognitive and Developmental Systems . [179] Wang, S., 2025. Roboflamingo-plus: Fusion of depth and rgb perception with vision-language models for enhanced robotic manipulation. arXiv preprint arXiv:2503.19510 . [180] Wang, T., Han, C., Liang, J.C., Yang, W., Liu, D., Zhang, L.X., Wang, Q., Luo, J., Tang, R., 2024c. Exploring the adversarial vulnerabilities of vision-language-action models in robotics. arXiv preprint arXiv:2411.13587 . [181] Wang, Y., Wu, S., Zhang, Y., Yan, S., Liu, Z., Luo, J., Fei, H., 2025b. Multimodal chain-of-thought reasoning: comprehensive survey. arXiv preprint arXiv:2503.12605 . [182] Wang, Z., Zhou, Z., Song, J., Huang, Y., Shu, Z., Ma, L., 2024d. Towards testing and evaluating vision-language-action models for robotic manipulation: An empirical study. arXiv preprint arXiv:2409.12894 . [183] Wei, J., Yuan, S., Li, P., Hu, Q., Gan, Z., Ding, W., 2024. Occllama: An occupancy-language-action generative world model for autonomous driving. arXiv preprint arXiv:2409.03272 . [184] Wen, J., Zhu, M., Zhu, Y., Tang, Z., Li, J., Zhou, Z., Li, C., Liu, X., Peng, Y., Shen, C., et al., 2024. Diffusion-vla: Scaling robot foundation models via unified diffusion and autoregression. arXiv preprint arXiv:2412.03293 . [185] Wen, J., Zhu, Y., Li, J., Tang, Z., Shen, C., Feng, F., 2025a. Dexvla: Vision-language model with plug-in diffusion expert for general robot control. arXiv preprint arXiv:2502.05855 . [186] Wen, J., Zhu, Y., Li, J., Zhu, M., Tang, Z., Wu, K., Xu, Z., Liu, N., Cheng, R., Shen, C., et al., 2025b. Tinyvla: Towards fast, data-efficient vision-language-action models for robotic manipulation. IEEE Robotics and Automation Letters . [187] Woo, S., Debnath, S., Hu, R., Chen, X., Liu, Z., Kweon, I.S., Xie, S., 2023. Convnext v2: Co-designing and scaling convnets with masked autoencoders, in: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1613316142. [188] Wu, J., Zhong, M., Xing, S., Lai, Z., Liu, Z., Chen, Z., Wang, W., Zhu, X., Lu, L., Lu, T., et al., 2024a. Visionllm v2: An end-to-end generalist multimodal large language model for hundreds of vision-language tasks. Advances in Neural Information Processing Systems 37, 6992569975. [189] Wu, W., Feng, X., Gao, Z., Kan, Y., 2024b. Smart: scalable multiagent real-time motion generation via next-token prediction. Advances in Neural Information Processing Systems 37, 114048114071. [190] Wu, Z., Zhou, Y., Xu, X., Wang, Z., Yan, H., 2025. Momanipvla: Transferring vision-language-action models for general mobile manipulation. arXiv preprint arXiv:2503.13446 . [191] Xiang, T.Y., Jin, A.Q., Zhou, X.H., Gui, M.J., Xie, X.L., Liu, S.Q., Wang, S.Y., Duang, S.B., Wang, S.C., Lei, Z., et al., 2025. Vla modelexpert collaboration for bi-directional manipulation learning. arXiv preprint arXiv:2503.04163 . [192] Xiong, J., Liu, G., Huang, L., Wu, C., Wu, T., Mu, Y., Yao, Y., Shen, H., Wan, Z., Huang, J., et al., 2024. Autoregressive models in vision: survey. arXiv preprint arXiv:2411.05902 . [193] Xu, D., Chen, Y., Wang, J., Huang, Y., Wang, H., Jin, Z., Wang, H., Yue, W., He, J., Li, H., et al., 2024a. Mlevlm: Improve multi-level progressive capabilities based on multimodal large language model for medical visual question answering, in: Findings of the Association for Computational Linguistics ACL 2024, pp. 49774997. [194] Xu, J., Sun, Q., Han, Q.L., Tang, Y., 2025a. When embodied ai meets industry 5.0: human-centered smart manufacturing. IEEE/CAA Journal of Automatica Sinica 12, 485501. [195] Xu, S., Wang, Y., Xia, C., Zhu, D., Huang, T., Xu, C., 2025b. Vlacache: Towards efficient vision-language-action model via adaptive token caching in robotic manipulation. arXiv preprint arXiv:2502.02175 . [196] Xu, Z., Wu, K., Wen, J., Li, J., Liu, N., Che, Z., Tang, J., 2024b. 33 survey on robotics with foundation models: toward embodied ai. arXiv preprint arXiv:2402.02385 . [197] Xue, H., Ren, J., Chen, W., Zhang, G., Fang, Y., Gu, G., Xu, H., Lu, C., 2025. Reactive diffusion policy: Slow-fast visual-tactile policy learning for contact-rich manipulation. arXiv preprint arXiv:2503.02881 . [198] Yang, R., Chen, G., Wen, C., Gao, Y., 2025. Fp3: 3d foundation policy for robotic manipulation. arXiv preprint arXiv:2503.08950 . [199] Yang, Y., Huang, W., Wei, Y., Peng, H., Jiang, X., Jiang, H., Wei, F., Wang, Y., Hu, H., Qiu, L., et al., 2023a. Attentive mask clip, in: Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 27712781. [200] Yang, Z., Chen, Y., Wang, J., Manivasagam, S., Ma, W.C., Yang, A.J., Urtasun, R., 2023b. Unisim: neural closed-loop sensor simulator, in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 13891399. [201] Ye, S., Jang, J., Jeon, B., Joo, S., Yang, J., Peng, B., Mandlekar, A., Tan, R., Chao, Y.W., Lin, B.Y., et al., 2024. Latent action pretraining from videos. arXiv preprint arXiv:2410.11758 . [202] Yue, Y., Wang, Y., Kang, B., Han, Y., Wang, S., Song, S., Feng, J., Huang, G., 2024. Deer-vla: Dynamic inference of multimodal large language models for efficient robot execution. Advances in Neural Information Processing Systems 37, 5661956643. [203] Zawalski, M., Chen, W., Pertsch, K., Mees, O., Finn, C., Levine, S., 2024. Robotic control via embodied chain-of-thought reasoning. arXiv preprint arXiv:2407.08693 . [204] Zhai, X., Mustafa, B., Kolesnikov, A., Beyer, L., 2023. Sigmoid loss for language image pre-training, in: Proceedings of the IEEE/CVF international conference on computer vision, pp. 1197511986. [205] Zhang, B., Zhang, Y., Ji, J., Lei, Y., Dai, J., Chen, Y., Yang, Y., 2025a. Safevla: Towards safety alignment of vision-language-action model via safe reinforcement learning. arXiv preprint arXiv:2503.03480 . [206] Zhang, H., Ding, P., Lyu, S., Peng, Y., Wang, D., 2025b. Gevrm: Goalexpressive video generation model for robust visual manipulation. arXiv preprint arXiv:2502.09268 . [207] Zhang, H., Yu, H., Zhao, L., Choi, A., Bai, Q., Yang, B., Xu, W., 2025c. Slim: Sim-to-real legged instructive manipulation via long-horizon visuomotor learning. arXiv preprint arXiv:2501.09905 . [208] Zhang, H., Zantout, N., Kachana, P., Wu, Z., Zhang, J., Wang, W., 2024a. Vla-3d: dataset for 3d semantic scene understanding and navigation. arXiv preprint arXiv:2411.03540 . [209] Zhang, J., Guo, Y., Hu, Y., Chen, X., Zhu, X., Chen, J., 2025d. Upvla: unified understanding and prediction model for embodied agent. arXiv preprint arXiv:2501.18867 . [210] Zhang, J., Wang, K., Wang, S., Li, M., Liu, H., Wei, S., Wang, Z., Zhang, Z., Wang, H., 2024b. Uni-navid: video-based vision-languageaction model for unifying embodied navigation tasks. arXiv preprint arXiv:2412.06224 . [211] Zhang, K., Yin, Z.H., Ye, W., Gao, Y., 2024c. Learning manipulation skills through robot chain-of-thought with sparse failure guidance. arXiv preprint arXiv:2405.13573 . [212] Zhang, K., Yun, P., Cen, J., Cai, J., Zhu, D., Yuan, H., Zhao, C., Feng, T., Wang, M.Y., Chen, Q., et al., 2025e. Generative artificial intelligence in robotic manipulation: survey. arXiv preprint arXiv:2503.03464 . [213] Zhang, R., Dong, M., Zhang, Y., Heng, L., Chi, X., Dai, G., Du, L., Wang, D., Du, Y., Zhang, S., 2025f. Mole-vla: Dynamic layer-skipping vision language action model via mixture-of-layers for efficient robot manipulation. arXiv preprint arXiv:2503.20384 . [214] Zhao, H., Song, W., Wang, D., Tong, X., Ding, P., Cheng, X., Ge, Z., 2025a. More: Unlocking scalability in reinforcement learning for quadruped vision-language-action models. arXiv preprint arXiv:2503.08007 . [215] Zhao, Q., Lu, Y., Kim, M.J., Fu, Z., Zhang, Z., Wu, Y., Li, Z., Ma, Q., Han, S., Finn, C., et al., 2025b. Cot-vla: Visual chain-of-thought reasoning for vision-language-action models. arXiv preprint arXiv:2503.22020 . [216] Zhao, T.Z., Kumar, V., Levine, S., Finn, C., 2023. Learning finegrained bimanual manipulation with low-cost hardware. arXiv preprint arXiv:2304.13705 . [217] Zhen, H., Qiu, X., Chen, P., Yang, J., Yan, X., Du, Y., Hong, Y., Gan, C., 2024. 3d-vla: 3d vision-language-action generative world model. arXiv preprint arXiv:2403.09631 . [218] Zheng, J., Li, J., Liu, D., Zheng, Y., Wang, Z., Ou, Z., Liu, Y., Liu, J., Zhang, Y.Q., Zhan, X., 2025. Universal actions for enhanced embodied foundation models. arXiv preprint arXiv:2501.10105 . [219] Zhong, Y., Huang, X., Li, R., Zhang, C., Liang, Y., Yang, Y., Chen, Y., 2025. Dexgraspvla: vision-language-action framework towards general dexterous grasping. arXiv preprint arXiv:2502.20900 . [220] Zhou, X., Han, X., Yang, F., Ma, Y., Knoll, A.C., 2025a. Opendrivevla: Towards end-to-end autonomous driving with large vision language action model. arXiv preprint arXiv:2503.23463 . [221] Zhou, Z., Zhu, Y., Zhu, M., Wen, J., Liu, N., Xu, Z., Meng, W., Cheng, R., Peng, Y., Shen, C., et al., 2025b. Chatvla: Unified multimodal understanding and robot control with vision-language-action model. arXiv preprint arXiv:2502.14420 . [222] Zhu, D.H., Chang, Y.P., 2020. Robot with humanoid hands cooks food better? effect of robotic chef anthropomorphism on food quality predicInternational Journal of Contemporary Hospitality Management tion. 32, 13671383. [223] Zhu, M., Zhu, Y., Li, J., Zhou, Z., Wen, J., Liu, X., Shen, C., Peng, Y., Feng, F., 2025. Objectvla: End-to-end open-world object manipulation without demonstration. arXiv preprint arXiv:2502.19250 . [224] Zitkovich, B., Yu, T., Xu, S., Xu, P., Xiao, T., Xia, F., Wu, J., Wohlhart, P., Welker, S., Wahid, A., et al., 2023. Rt-2: Vision-language-action models transfer web knowledge to robotic control, in: Conference on Robot Learning, PMLR. pp. 21652183."
        },
        {
            "title": "Appendix Table",
            "content": "The following appendix tables present comprehensive overview of recent developments and challenges in VLA models. Table 3 systematically compares state-of-the-art VLA methodologies, their application domains, and key innovations across robotics, autonomous systems, and embodied AI platforms. This comparative summary highlights core architectural advances, deployment contexts, and technical contributionsproviding valuable insight into the evolving landscape of generalist and task-specific VLA models. Meanwhile, Table 4 presents structured synthesis of the major technical and practical challenges facing VLA model deployment, alongside potential solutions and their expected impact. This includes limitations such as real-time inference constraints, multimodal integration issues, and ethical concerns, with proposed resolutions ranging from architectural innovations to scalable training techniques and cross-modal alignment strategies. Together, these tables serve as detailed reference for researchers, developers, and practitioners aiming to understand both the current capabilities and outstanding barriers in VLA-based intelligent systems. Table 3: Comparison of VLA methodologies, application areas, and innovations. This comprehensive table compares cutting-edge VLA models by summarizing their methodologies, application domains, and key innovations. VLA Methodology VLA Application Area Strength and Key Innovation & Reference Year CogACT [102] & 2024 VLATest [182] & 2024 NaVILA [32] & 2024 RoboNurse-VLA [103] & 2024 Mobility [35] & 2024 VLA CoVLA [5] & 2025 OpenDriveVLA [220] & ORION [56] & 2025 QUAR-VLA [43] & 2025 TinyVLA [186] & 2025 Componentized VLA with specialized action module using diffusion transformers Automated framework for large-scale VLA model testing in manipulation Two-level VLA: high-level vision-language generates mid-level nav commands, RL locomotion executes SAM 2 vision, Llama 2 language, real-time voiceto-action pipeline Hierarchical VLA: long-context VLM for multimodal goal localization, topological graph for lowlevel navigation CLIP for vision, Llama-2 for language, trajectory prediction for action Hierarchical alignment of 2D/3D visual tokens and language embeddings; autoregressive agent-envego modeling QT-Former for history context, LLM for reasoning, generative planner for trajectory prediction QUART model fuses vision and language for action generation Compact VLA with fast multimodal backbone, diffusion policy decoder UAV-VLA [150] & Modular VLA: GPT for goal extraction, VLM for object search, GPT for action generation Bi-VLA [59] & 2025 Multimodal transformer links vision, language, and bimanual action modules languageIndustrial robotics, guided manipulation Robotic manipulation: benchmarking VLA robustness and reliability Legged robot navigation via natural language in cluttered, realworld scenes Surgical assistance: precise instrument grasp and handover in OR Multimodal instruction navigation with demonstration tours (MINT) in real-world environments Autonomous driving, dataset for VLA model training End-to-end autonomous driving Holistic E2E autonomous driving Quadruped robots: navigation, manipulation, whole-body tasks Robotic manipulation: fast, data-efficient visuomotor control Large-scale UAV mission planning from natural language and satellite imagery Bimanual dexterous manipulation for household tasks ChatVLA [221] & 2025 RoboMamba [111] & 2025 Phased Alignment Training, Mixture-of-Experts for vision-language-action integration Mamba-based VLA: vision encoder co-trained with SSM for reasoning and SE(3) action Unified multimodal understanding and robot control Robotic reasoning and manipulation, efficient pose prediction OTTER [75] & 2025 Text-aware visual feature extraction with frozen pre-trained VLMs Robotic manipulation: zero-shot generalization to novel tasks PointVLA [96] & 2025 Injects 3D point cloud features into frozen pretrained VLA via modular skip-blocks VLA-Cache [195] & 2025 CombatVLA [29] & 2025 Adaptive token caching with selective reuse of static visual tokens Trains on video-action AoT sequences, integrates truncated AoT for fast inference HybridVLA [110] & 2025 Unified LLM with collaborative diffusion and autoregressive action policies NORA [79] & 2025 3B-parameter VLA using Qwen-2.5-VL-3B backbone and FAST+ tokenizer SpatialVLA [136] & 2025 Ego3D Position Encoding and Adaptive Action Grids for spatially-aware VLA MoLe-VLA [213] & 2025 JARVIS-VLA [101] & 2025 Mixture-of-Layers with dynamic layer-skipping via STAR router and CogKD Post-trains large VLMs with visual-language guidance and action head for keyboard/mouse control UP-VLA [209] & 2025 Unified VLA with joint multi-modal understanding and future prediction objectives Shake-VLA [92] & 2025 Modular VLA system with vision, speech-to-text, RAG, anomaly detection, and bimanual arms MoRE [214] & 2025 Sparse Mixture-of-Experts (MoE) with LoRA modules, RL-based Q-function training DexGraspVLA [219] & 2025 Hierarchical VLA: pre-trained vision-language planner + diffusion-based low-level controller DexVLA [185] & 2025 Plug-in billion-param diffusion action expert, embodiment curriculum learning spatial longRobotic manipulation: reasoning, few-shot, horizon tasks Robotic manipulation: efficient, real-time VLA inference 3D ARPGs: real-time combat understanding and tactical action Robotic manipulation: singlearm, dual-arm, diverse real/sim tasks Generalist embodied robotics: efficient real-world and simulated task execution Generalist robot manipulation: zerocross-robot, multi-task, shot control Efficient RLBench and real-world tasks Visual games (Minecraft): 1k+ tasks, open-world, instruction following Embodied agents: manipulation tasks, precise spatial reasoning robot manipulation: Bimanual robotic cocktail mixing: ingredient detection, recipe adaptation, liquid measurement Quadruped robots: multi-task navigation, locomotion, and manipulation General grasping: dexterous robust across diverse objects, lighting, and backgrounds singleGeneral robot control: arm, bimanual, dexterous hand, long-horizon tasks 35 Robust action modeling, rapid adaptation, strong generalization, much higher task success rates Diverse scene generation, multi-model/task evaluation, reveals robustness gaps, guides VLA improvement Modular mid/low-level split, strong generalization, 88% real-world success, robust to diverse terrains Accurate, real-time handover, robust to unseen tools, excels at complex, dynamic surgical tasks High success on complex language+image tasks, robust to novel queries, leverages demonstration videos, scalable to large spaces Large-scale, richly annotated dataset; enables interpretable scene understanding and robust path planning Unified semantic space, dynamic interaction modeling, state-of-the-art planning and QA performance Aligns reasoning and action spaces, unified optimization for VQA and planning, superior closed-loop results Tight vision-language-action integration, fine-grained instruction alignment, strong sim-to-real generalization No pre-training needed, faster inference, strong generalization, outperforms OpenVLA on efficiency and accuracy Efficient flight path/action plan generation, no prior training, intuitive human-UAV interaction, benchmarked performance Accurate code/action generation, high adaptability, strong vision-language-action integration, robust realworld performance Minimizes forgetting/interference, excels at VQA and manipulation, efficient, outperforms SOTA VLA models Linear-complexity inference, minimal fine-tuning, fast and accurate reasoning and manipulation, SOTA efficiency Preserves semantic alignment, no VLM fine-tuning, task-relevant feature selection, strong zero-shot performance No retraining, preserves 2D knowledge, strong 3D spatial reasoning, excels at few-shot and dynamic tasks 4050% faster, minimal accuracy loss, dynamic layerwise token reuse, practical for real-world robots 50x faster inference, outperforms all baselines, surpasses human success rate, strong tactical reasoning Adaptive action ensemble, robust control, strong generalization, outperforms SOTA on complex manipulations Low computational overhead, strong visual reasoning, fast action decoding, outperforms larger VLA models 3D spatial integration, adaptive action discretization, strong generalization and transfer, open-sourced code Selective layer activation, 5.6x faster, preserves cognition, +8% mean success, brain-inspired efficiency Self-supervised post-training, 40%+ over baselines, strong world knowledge, state-of-the-art generalization, open-sourced Captures both high-level semantics and low-level spatial dynamics, 33% better on Calvin ABC-D, excels at realworld tasks needing fine spatial control 100% task success, robust in noisy/cluttered settings, accurate ingredient handling, flexible recipe adaptation, real-world deployment Scalable RL fine-tuning on mixed-quality data, strong multi-task and OOD generalization, outperforms baselines in sim and real-world Iterative domain-invariant representation, strong zeroshot generalization, 90%+ success on unseen scenarios, consistent performance across variations Cross-embodiment action modeling, efficient curriculum training, rapid adaptation, SOTA on complex tasks without task-specific tuning Challenge / Limitation Real-Time Inference Constraints"
        },
        {
            "title": "Dataset Bias\nGrounding",
            "content": "and Limited 3D Perception and Reasoning Cross-Embodiment Generalization"
        },
        {
            "title": "Annotation\nplexity and Cost",
            "content": "ComSim-to-Real Transfer Gap Integration of Physical Knowledge Multi-Modal Integration (e.g., tactile, audio) LongHandling Horizon, Multi-Stage Tasks System Integration Complexity"
        },
        {
            "title": "Generalization\nUnseen Tasks",
            "content": "to Robustness to Environmental Variability"
        },
        {
            "title": "Ethical and Societal\nImplications",
            "content": "Table 4: Challenges, Potential Solutions, and Expected Impact of VLA Models"
        },
        {
            "title": "Expected Impact",
            "content": "Adopt parallel decoding, quantized transformers, and hardware acceleration [100, 94](e.g., TensorRT); minimize autoregressive overhead [60, 110] Hybrid tokenization using diffusion and autoregressive policies [133]; train on diverse demonstrations and multi-modal outputs [121] Integrate dynamic risk assessment modules [143, 180]; low-latency emergency stop circuits and adaptive planning layers [87] Curate diverse, debiased datasets [145]; apply improved grounding techniques such as CLIP finetuning with hard negatives [212, 16] Integrate 3D sensors (e.g., depth, LiDAR), develop 3D-aware architectures, and leverage point cloud fusion with vision-language inputs Train with diverse robot types and morphologies, use embodiment-agnostic representations, and apply cross-domain adaptation techniques [201] Employ weak supervision, active learning, and synthetic data generation to reduce reliance on extensive manual annotation [115] Use domain adaptation, sim-to-real fine-tuning, and real-world calibration strategies[162, 104] Incorporate physics-based priors, simulation environments, and dynamics modeling into training pipelines [42] Fuse additional sensory modalities (tactile, audio) with vision and language [88]; develop multimodal transformers Design hierarchical policies, memory-augmented networks, and trajectory planning modules [106] pruning, LoRA adapters, and deployment Develop unified Transformer backbones [218]; incorporate temporal alignment layers and sim-toreal transfer learning strategies [127, 207] Apply model quantization-aware training, on low-power accelerators Use compositional generalization, few-shot metalearning, and task-agnostic pretraining pipelines [135, 113] Use domain randomization, sensor fusion, and perception-action real-time pipelines [121] Enforce privacy via on-device processing and anonymization [116, 154, 195, 29]; audit model fairness; build regulatory frameworks for trust recalibration of Supports real-time robotic control and broader deployment in time-sensitive domains[194, 150] (e.g., drones, manipulators) Improves handling of complex, dynamic manipulation tasks with multiple viable solutions [59] Ensures reliability and safety in unpredictable environments (homes, factories, healthcare settings) Enhances model fairness, semantic fidelity [85], and generalizability to novel realworld inputs [175, 217, 136] Enables more accurate spatial reasoning, manipulation, and navigation in complex real-world environments [100] Facilitates transfer of policies and knowledge across different robot platforms and configurations [209, 94] Lowers development costs and accelerates scaling to new tasks and domains [180, 215] Improves reliability and consistency of VLA models when deployed outside simulation environments [4, 53] Enhances the models ability to predict and plan actions that respect real-world physical constraints [82] Expands task repertoire and robustness to ambiguous or visually occluded scenarios [61, 106, 71] Improves performance on complex, sequential requiring planning and memory [102, 215, 175, 136] Enables seamless planning-control coordination and robust transfer to physical robots [147, 160] Facilitates scalable, efficient deployment of VLAs in embedded and mobile platforms [195, 213, 96, 190] Reduces task-specific overfitting, enabling robust zeroand few-shot adaptation [75, 186, 215] Enhances performance in changing or cluttered environments with minimal degradation [214, 186] Promotes equitable and trustworthy VLA adoption across social, medical, and labor domains [124, 140, 166, 134] tasks"
        }
    ],
    "affiliations": [
        "Cornell University, Biological & Environmental Engineering, Ithaca, New York, USA",
        "The Hong Kong University of Science and Technology, Department of Computer Science and Engineering, Hong Kong",
        "University of the Peloponnese, Department of Informatics and Telecommunications, Greece"
    ]
}