{
    "paper_title": "Wikontic: Constructing Wikidata-Aligned, Ontology-Aware Knowledge Graphs with Large Language Models",
    "authors": [
        "Alla Chepurova",
        "Aydar Bulatov",
        "Yuri Kuratov",
        "Mikhail Burtsev"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Knowledge graphs (KGs) provide structured, verifiable grounding for large language models (LLMs), but current LLM-based systems commonly use KGs as auxiliary structures for text retrieval, leaving their intrinsic quality underexplored. In this work, we propose Wikontic, a multi-stage pipeline that constructs KGs from open-domain text by extracting candidate triplets with qualifiers, enforcing Wikidata-based type and relation constraints, and normalizing entities to reduce duplication. The resulting KGs are compact, ontology-consistent, and well-connected; on MuSiQue, the correct answer entity appears in 96% of generated triplets. On HotpotQA, our triplets-only setup achieves 76.0 F1, and on MuSiQue 59.8 F1, matching or surpassing several retrieval-augmented generation baselines that still require textual context. In addition, Wikontic attains state-of-the-art information-retention performance on the MINE-1 benchmark (86%), outperforming prior KG construction methods. Wikontic is also efficient at build time: KG construction uses less than 1,000 output tokens, about 3$\\times$ fewer than AriGraph and $<$1/20 of GraphRAG. The proposed pipeline enhances the quality of the generated KG and offers a scalable solution for leveraging structured knowledge in LLMs."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 2 ] . [ 1 0 9 5 0 0 . 2 1 5 2 : r Wikontic: Constructing Wikidata-Aligned, Ontology-Aware Knowledge Graphs with Large Language Models Alla Chepurova1,2 Aydar Bulatov1,2 Yuri Kuratov1,2 Mikhail Burtsev3 1Cognitive AI Systems Lab, Moscow, Russia 2Moscow Independent Research Institute of Artificial Intelligence, Moscow, Russia 3London Institute for Mathematical Sciences, London, UK {chepurova,bulatov,kuratov}@cogailab.com, mb@lims.ac.uk"
        },
        {
            "title": "Abstract",
            "content": "Knowledge graphs (KGs) provide structured, verifiable grounding for large language models (LLMs), but current LLM-based systems commonly use KGs as auxiliary structures for text retrieval, leaving their intrinsic quality underexplored. In this work, we propose Wikontic, multi-stage pipeline that constructs KGs from open-domain text by extracting candidate triplets with qualifiers, enforcing Wikidatabased type and relation constraints, and normalizing entities to reduce duplication. The resulting KGs are compact, ontology-consistent, and well-connected; on MuSiQue, the correct answer entity appears in 96% of generated triplets. On HotpotQA, our triplets-only setup achieves 76.0 F1, and on MuSiQue 59.8 F1, matching or surpassing several retrieval-augmented generation baselines that still require textual context. In addition, Wikontic attains stateof-the-art information-retention performance on the MINE-1 benchmark (86%), outperforming prior KG construction methods. Wikontic is also efficient at build time: KG construction uses less than 1,000 output tokens, about 3 fewer than AriGraph and <1/20 of GraphRAG. The proposed pipeline enhances the quality of the generated KG and offers scalable solution for leveraging structured knowledge in LLMs. Wikontic is available at https://github.com/screemix/Wikontic."
        },
        {
            "title": "Introduction",
            "content": "A substantial amount of the worlds knowledge exists solely in unstructured textual form, such as news, scientific articles, blogs and posts on social networks. While large language models (LLMs) are capable of extracting insights from these data, their internal representations are latent and often unverifiable, making them susceptible to hallucinations. In contrast, knowledge graphs (KGs) record information as explicit subject-relationobject triplets, supporting verifiable queries, incremental updates, and multi-step reasoning (e.g., in answering compositional questions), making KGs reliable complement to both LLMs and retrievalaugmented generation (RAG) systems. Therefore, creating high-quality KGs directly from raw text provides reliable, transparent knowledge that complements LLMs and RAG systems. Extracting structured knowledge from text is long-standing challenge in Information Retrieval. common formulation is closed information extraction (cIE), which assumes predefined ontology with fixed sets of entity types and relation predicates drawn from an existing KG, and seeks to recover all triplets that conform to that schema. Classical cIE pipelines decompose the task into stages such as named entity recognition and relation classification (Zeng et al., 2014; Zhang et al., 2020). However, separating these stages leads to error accumulation and prevents sharing of information across tasks. More recent end-to-end approaches cast extraction as sequence-to-sequence problem, training models to directly generate triplets from text (Distiawan et al., 2019; Cabot and Navigli, 2021; Josifoski et al., 2021) or complete missing links in KG (Yao et al., 2019). While this reduces error propagation, such models remain difficult to adapt to new domains, requiring costly retraining on high-quality annotated corpora that remain scarce. LLMs offer promising alternative: their broad knowledge and strong prompting abilities enable open-domain extraction without expensive task-specific training (Wang et al., 2020; Josifoski et al., 2023; Chepurova et al., 2024). In contrast to cIE, open information extraction (oIE) does not impose predefined entity and relation names, ontology constraints, and constructs KGs from scratch. This flexibility makes oIE an attractive tool for augmenting LLMs and RAG systems, with recent work demonstrating reduced inference costs and more reliable retrieval (Chen et al., 2024; Gutierrez et al., 2024; Guo et al., 2024; Li et al., 2024; Han et al., 2024; Gutiérrez et al., 2025). For instance, AriGraph (Anokhin et al., 2024) learns KGs to create semantic longterm memory, while Distill-SynthKG (Choubey et al., 2024) integrates mixed texttriplet structures to improve question answering. However, most current oIE pipelines rely on KGs mainly as auxiliary scaffolds for structuring text retrieval, rather than treating the KG itself as high-quality knowledge resource. This perspective overlooks the potential of compact and non-redundant KGs to represent information directly, and leaves their quality and reasoning capabilities underexploited. As result, the practical use of oIE KGs remains limited. Extracted triplets often contain heterogeneous surface formsfor instance, NYC located in USA versus New York City in-country United Statesfragmenting the KG into redundant or inconsistent representations. With scale, synonymy, coreference, and predicate variation accumulate, eroding the very strengths that motivate KG construction in the first place: precision, interpretability, and logical consistency. To address this, we combine the flexibility of oIE with the structural rigor of cIE by leveraging external ontologies. Wikidata (Vrandeˇcic, 2012), one of the largest community-maintained knowledge bases, offers rich entity classes, relation schemas, and domainrange constraints across more than 100M entities. Its breadth allows coverage from common sense to specialized domains, while its formal constraints provide principled supervision for validating LLM outputs. Yet, integrating such ontology guidance into fully automated pipeline (i) extracting candidate poses key challenges: triplets without predefined labels, (ii) typing and disambiguating entities under ontology classes despite lexical ambiguity, and (iii) refining nodes and edges iteratively while preserving alignment. In this paper, we address these challenges with Wikontic, multi-stage framework that constructs Wikidata-aligned, ontology-aware KGs directly from text using LLMs. Unlike prior work that apply Wikidata ontology only for evaluation or entity linking (Polat et al., 2025), we integrate large-scale ontology from Wikidata directly into the information extraction pipeline. Wikontic includes six components: (i) curated ontology database derived from Wikidata, (ii) candidate triplet extraction with qualifiers, (iii) ontology-aware triplet refinement enforcing schema constraints, (iv) subject/object name refinement for entity deduplication, (v) KG storage, and (vi) retrieval for multi-hop question answering. Starting from unstructured text, Wikontic extracts triplets with LLMs, types and validates them against Wikidata, and deduplicates entities to yield compact, consistent KGs. The resulting KGs are both interpretable and effective. When used as the sole knowledge source for multi-hop QA, Wikontic achieves competitive performance with RAG and KG-based methods that rely on raw text as context (Lee et al., 2024; Li et al., 2024; Anokhin et al., 2024; Panda et al., 2024; Gutiérrez et al., 2025). Moreover, our graphs exhibit superior coverage of salient information and strong internal connectivity between relevant nodes. In summary, our main contributions are: 1. We introduce Wikontic, which (1) extracts candidate triplets, (2) enforces schema and ontology constraints on types and domainrange, (3) and performs alias-aware entity normalization and deduplication to reduce redundancy, yielding ontologyconsistent KGs. 2. We show that Wikontics KGs are ontologyconsistent and low redundancy with strong coverage and connectivity; on MuSiQue, the correct answer entity is present in 96% of generated triplets. 3. Using the KG as the sole knowledge source (no access to the original text) on multi-hop question answering, Wikontic attains 76.0 F1 on HotpotQA and 59.8 F1 on MuSiQue, matching or surpassing several RAG/KG baselines that still rely on text. 4. Wikontic achieves state-of-the-art results on the MINE-1 benchmark, reaching 86% informationretention scores. 5. Wikontics KG construction uses less than 1,000 output tokens which is 3 fewer than AriGraph and <1/20 of GraphRAG."
        },
        {
            "title": "2 Methods: Wikontic",
            "content": "Wikontic is multi-stage pipeline for constructing high-quality, ontology-aware KGs directly from unstructured text (Figure 1). Unlike prior approaches that directly map text to graph form and often yield noisy, redundant, or inconsistent outputs, our pipeline explicitly integrates LLMs with Wikidataderived ontological constraints, entity normalization, and iterative refinement. To enable triplet validation and alignment, the pipeline stores ontology rules and current KG (Section 2.1). The triplet extraction pipeline (Section 2.2) consists of three main stages: (i) triplet candidate extraction with contextual metadata, (ii) ontology-aware triplet refinement, and (iii) entity Figure 1: Overview of Wikontic: an ontology-guided pipeline that constructs Wikidata-aligned KG from text. (1) An LLM extracts candidate (subject, relation, object) triplets (gray). (2) The extracted triplets are then refined using Wikidatas ontology: entity types are assigned (colored nodes), and relations that violate ontology constraints are corrected or removed. (3) Finally, entities names are normalized and duplicate surface forms are merged. The resulting graph is de-duplicated, ontology-consistent, and ready for downstream tasks. normalization and deduplication. These stages aim to enforce structural validity and reduce redundancy, to produce cleaner and more semantically coherent KG that can replace raw text in RAG for multi-hop QA tasks (Section 2.3)."
        },
        {
            "title": "2.1 Ontology and KG Databases",
            "content": "We built custom ontology schema database derived from Wikidata. The schema database includes properties (i.e., relations) and their compatible entity types. Properties required solely for linking external data (e.g., multimedia or external identifiers) were excluded, leaving 2,464 factual properties with suitable datatypes (e.g., WikibaseItem, Quantity, Point in time). For each property, we retrieved subject and object type constraints from Wikidata (e.g., Q21503250 for subject, Q21510865 for object). These constraints define type compatibility rules, specifying which entity classes relation can logically connect and thereby guiding ontologyconsistent triplet construction. To support constraint generalization, we recursively expanded entity types using instance of (P31) and subclass of (P279) relations, building full taxonomies from each type up to the root. Such hierarchy is essential because relation constraints are defined at different levels of abstraction. For example, relation may allow connections between instances of the broader class audiovisual work, even if the entity is typed more specifically as film. By propagating the allowed properties of each parent type downwards to its children entity type, we ensure that entities can still be matched to valid relations whenever the constraint applies to its parent class. We collected labels and aliases for all entity types and relations. Dense retrieval indexes for relation and entity type names, as well as their respective aliases, support semantic search. These indexes allow us to semantically align relation and entity types names from extracted triplets with Wikidata definitions, even when surface forms differ. The KG database stores triplets, canonical entity names, and aliases. dense retrieval index over aliases supports efficient linking and deduplication. As extraction proceeds, new entities are inserted with canonical labels and aliases, keeping the KG compact yet incrementally extensible. Dense retrieval indexes used in both databases were built with Contriever embeddings (Izacard et al., 2021) and Atlas MongoDB vector search1. MongoDBs hybrid support for structured queries and dense retrieval enables both efficient graph and semantic search."
        },
        {
            "title": "2.2 Ontology-aware Triplet Extraction",
            "content": "Stage 1: Candidate Triplet Extraction. We extract factual triplet candidates from unstructured text with an LLM, capturing subject-relation-object triplets, along with contextual qualifiers that enhance the semantic meaning of the triplet. LLM is prompted with instruction and in-context examples to extract triplets that include entity types for both the subject and object, as well as additional metadata that mirrors the structure of Wikidata qualifiers. These qualifiers are essential because they capture contextual information such as time, location, or conditions. While such details usually cannot be expressed as standalone facts, they are critical for preserving factual precision and avoiding loss of accurate knowledge during knowledge extraction. For instance, given the text \"In 2010, Christopher Nolan directed the science fiction movie Inception\", the extracted triplet would be: (Nolan, directed, Inception) with entities types (human, film) and the qualifier: {point in time: 2010}. Further details and examples are provided in Appendix A.4. However, LLM outputs may be semantically redundant or structurally inconsistent. The names of entity and relation may not align with existing entities and relations already present in the KG. For example, an LLM might extract \"Nolan\" as the subject from one input text and extract \"Christopher Nolan\" from the other one; or relations like \"directed\" vs. \"director\" might appear in different grammatical forms or inverse directions in different input texts (see Figure 1, bottom). Without additional correction, these inconsistencies lead to entity duplication and increased KG size, which degrades both storage efficiency and downstream reasoning. Thus, to improve consistency and reduce redundancy, next steps of the pipeline validate extracted triplets using Wikidatas ontology and normalize entity names. Stage 2: Ontology-aware Refinement. At this stage, each candidate triplet is refined using the schema and constraints of the Wikidata ontology: 1https://www.mongodb.com/products/platform/ atlas-vector-search Entity typing: For both subject and object, we retrieve the top-10 candidate types from the dense retrieval index. The LLM then selects the most plausible type. We then add supertypes from the taxonomy to ensure coverage when constraints are defined at higher abstraction levels. Relation validation: Using Wikidata constraints, we identify all relations that can legally connect the subject and object types, including inverse combinations of subject and object types (e.g., directed vs. director). Candidate relations are ranked by cosine similarity to the originally extracted relation. Triplet backbone reconstruction: The text, triplet, and valid relations are passed to the LLM, which selects the most plausible ontology-valid configuration, yielding refined triplet backbone. This stage enforces structural validity, semantic alignment, and consistency with Wikidatas ontology. worked example is shown in Figure 5 (Appendix A.6). Stage 3: Entity Normalization and Alias-aware Deduplication. While the focus of the previous step is validating triplet structure and semantics, this step aligns entity names to unified vocabulary of existing KG entries to reduce duplication and ensure consistency of the constructed KG. For each refined triplet, we link its subject and object names to existing entities in the KG that share the same entity type or compatible parent type from the taxonomy. Using precomputed embeddings of entity aliases from the KG, we retrieve top-10 candidates and rank them by cosine similarity to the surface forms of the extracted mentions. The top-10 candidates, together with their types, are passed to the LLM to determine whether the extracted entity is synonymous with one of the existing entries. On match, we replace the mention with the canonical KG label and store the surface form as an alias; otherwise, we preserve new entity and add its surface form to the alias collection. This step aims to ensure that the resulting KG is compact by avoiding redundant entities with different surface form and evolving by supporting incremental updates with discovery on new entities. detailed example for the second step is provided on Figure 6 in Appendix A.6. We implement final ontology verification step to ensure that extracted triplets comply with the structural and semantic constraints of the target KG. triplet is verified if (i) its subject and object types, together with the relation, are defined in the ontology, and (ii) the relations domain and range constraints are satisfied. Triplets that fail these checks are flagged as ontology-misaligned but retained, as they remain linked to the main KG through the entity name refinement step. Preserving them enables computation of an ontology alignment score and provides interpretable cues for identifying or revising schema-inconsistent facts."
        },
        {
            "title": "2.3 Retrieval for QA",
            "content": "We address multi-hop question answering with the constructed KG via an iterative retrieval that decomposes the question into subquestions, grounding each step in the retrieved KG context. Given question, the LLM decomposes it into the first 1-hop subquestion. For each subquestion (1) the LLM identifies explicitly mentioned or potentially relevant entities; (2) links extracted entities to KG nodes and selects those most relevant for the current step; (3) given the retrieved subgraph formed by the neighborhood of the selected entities, the LLM generates an answer to the subquestion; (4) conditioned on the previous answer, the LLM formulates the next subquestion. This iterative process continues for up to five subquestions, after which the LLM produces the final answer. Implementation details and prompt templates are provided in Appendix A.5."
        },
        {
            "title": "2.4 Evaluation",
            "content": "Existing benchmarks for triplet extraction suffer from substantial limitations: annotated closed IE datasets are small-scale, noisy, and often incomplete (Josifoski et al., 2023, 2021; Huguet Cabot and Navigli, 2021), while open IE corpora are difficult to align with real-world KGs and provide unreliable ground truth for evaluation (Stanovsky et al., 2018). Constructing high-quality datasets is costly, as annotators must not only identify all explicit and implicit entity and relation mentions but also align them to the complex schemas of large KGs such as Wikidata, which contain thousands of entity types and relations. Recently, the MINE benchmark (Mo et al., 2025) was introduced to address some of these issues by evaluating KGs through information retention rather than exact triplet-level supervision. We evaluated Wikontic on the MINE-1 task and observed much higher information retention performance. However, while MINE provides useful and scalable proxy for KG quality, it does not include complete ground-truth triplets. Therefore, it cannot measure classical precision or recall and instead captures only the degree to which constructed KG preserves factual information. To further address this, we adopt an alternative evaluation strategy. We measure KG quality through (1) structural compactness (i.e., nonredundancy and deduplication) and (2) performance in downstream multi-hop QA. In this setup, the LLM must answer factual questions in textfree setting using only the constructed KG, without access to the original source texts, unlike retrievalaugmented methods such as HippoRAG (Gutierrez et al., 2024), AriGraph (Anokhin et al., 2024), and Holmes (Panda et al., 2024). This design makes QA functional proxy for two key properties of an extracted KG: (a) factual correctness, since noisy or invalid triplets directly impede correct answers, and (b) coverage and completeness, since incomplete graphs restrict multi-hop reasoning. Despite existing advances in aligning KGs with LLMs and adapting for QA (Han and Shareghi, 2022; Dai et al., 2025; Sui et al., 2024; Pan et al., 2024), we deliberately refrain from training additional models to estimate the KG quality itself. To compare predicted answers with ground truth, we apply normalization procedure that lowercases all strings and removes punctuation. To account for lexical variation, we further expand entity matching using the alias mappings stored in the KG. If the models predicted answer matches any canonical entity or one of its aliases, the corresponding alias set is treated as the set of valid candidate answers. We perform evaluations on KGs extracted using different LLMs: gpt-4.1, gpt-4.1-mini, gpt-4o-mini2, and Llama-3.3-70b-Instruct3, and assess the quality of the resulting KG on two multi-hop QA datasets: MuSiQue (Trivedi et al., 2022) and HotpotQA (Yang et al., 2018). We used the same questions and candidate passages, including both supporting and distractor passages, used in HippoRAG (Gutierrez et al., 2024) and AriGraph (Anokhin et al., 2024) to compare the results with existing methods."
        },
        {
            "title": "3.1 Evaluations on MINE-1",
            "content": "We evaluated Wikontic on the MINE-1 benchmark, which measures how much factual information from the source text is retained in the con2https://platform.openai.com/docs/models 3https://huggingface.co/meta-llama/ Llama-3.3-70B-Instruct Figure 2: Distribution of MINE-1 scores across 100 articles for GraphRAG, KGGen, and Wikontic. Dotted vertical lines are averaged scores. Wikontic scored 84% on average, substantially outperforming GraphRAG 47.80% and KGGen 66%. structed KGs using an LLM-as-a-judge protocol from the original study (Mo et al., 2025). Figure 2 displays the retention scores distribution in articles of MINE-1 for KGGen, GraphRAG, and Wikontic. Table 1 demonstrates the results for both KGGen and Wikontic with different LLM backbones. Wikontic consistently outperforms KGGen, reaching 84% with gpt-4o and 86% with gpt-4.1-mini, compared to KGGens best score of 73% (Claude Sonnet 3.5). These results demonstrate that Wikontic effectively preserves factual information during the construction of the KG. Method HippoRAG AriGraph Wikontic (1-3) w/o ontology (2) w/o ontology (2) and normalization (3) R 234.9 130.1 228.0 115.6 248.8 104.8 232.4 106.7 273.0 140.9 w/o filtered triplets 239.9 99.5 Avg. degree 4.0 3.9 4.3 4.4 Unique per 1.8 2.0 2.5 2.6 diversity per 2 1.1 1.01 1.03 1.06 4.2 4. 2.3 2.6 1.09 1.0 Table 2: KGs structural statistics for MuSiQue QA corpus: the number of unique entities (E) and relations (R), average entity degree (Avg. degree), the number of unique entities per relation (Unique per r) and the average relation diversity per two entities (r diversity per 2 e). Method KGGen, Claude Sonnet 3.5 KGGen, GPT-4o KGGen, Gemini 2.0 Flash GraphRAG, gpt4o Wikontic, gpt4o Wikontic, gpt4.1-mini MINE-1 Score (%) 73 66 44 48 84 86 Table 1: MINE-1 information-retention scores for KGGen and Wikontic. Wikontic achieves the highest retention performance across all evaluated LLMs."
        },
        {
            "title": "3.2 Graph Quality Analysis",
            "content": "We aim to comprehensively evaluate the structure, information content, and usability of knowledge graphs created by Wikontic in challenging information extraction setting. Given the limited availability of approaches that assess the KG quality directly, we use proxy evaluation methodology based on the MuSiQue QA dataset to examine how effectively the knowledge is represented in the resulting KG. KG can be formally represented as = (T , E, R), where is the set of entities e, is the set of relations and is the set of triplets: E. For efficient knowledge storage and retrieval, the KG should satisfy the size, density and diversity desiderata, which can be directly evaluated using graph statistics  (Table 2)  . Graph size is directly connected to the number of stored facts, and can be represented by the average number of entities and relations per MuSiQue sample, denoted by and R. Wikontic without ontology (Stage 2) and normalization (Stage 3) yields diverse KGs with the highest number of unique entities and relations, followed by HippoRAG. However, the sheer volume of relations does not necessarily make retrieval more informative. Each relation should also be well-represented across various unique entities to ensure that relation names are standardized and meaningful. This propto the question. In multi-hop question answering, entities in question will unlikely be in the same context as the answer entity, meaning they may not be direct neighbors in the resulting KG. However, in KG with sufficient coverage, there has to be reasonably short path connecting these two entities. To measure the overall coverage of various KG construction methods, we estimate whether the answer to the question is present in the KG as an entity and does the path from question to answer exist in the graph. Due to differences in pipelines, we cast all entity and relation names to lowercase and remove punctuation to standardize their format. To account for possible differences in entity naming, we consider two entities matching if one is substring of another. Table 3 presents coverage and size metrics for KGs built by our pipeline, AriGraph and HippoRAG on samples from the MuSiQue dataset. Since baseline KGs are available only for gpt4omini model and 80 common test samples, we use the same configuration to ensure fair comparison. We estimated the standard deviation using bootstrapping. \"Contains Answer\" represents the percentage of cases when the answer entity is present in the full KG, or 5and 10hop neighborhood of entities present in MuSiQue questions. We ablate Wikontic by removing one or multiple pipeline steps at time. \"Ontology Entailment\" is the percentage of triplets, where subject, object and relation match the ontology. Method HippoRAG AriGraph Wikontic w/o ontology (2) w/o ontology (2) and normalization (3) w/o filtered triplets w/o qualifiers Contains Answer (%) Total 10-hop 5-hop 96.32.1 67.55.3 68.85.2 79.94.5 40.05.5 41.35.5 96.22.1 66.35.3 68.85.2 97.51.7 66.35.3 70.05.2 Ontology Entailment (%) - - 96.5 15.2 96.22.1 65.05.4 68.85.2 93.82.7 63.85.4 66.35.3 85.04.0 51.35.7 53.85. 12.4 100.0 96.5 Table 3: Knowledge coverage of graphs built using different extraction methods on the MuSiQue dataset with mini-sized models. (Left) Percentage of cases where the correct answer to question appears in the full constructed graph or within the 5-, and 10-hop neighborhoods of the question nodes. Wikontic pipelines provide the best answer coverage while maintaining high ontology agreement. (Right) Average percentage of triplets in each sample that are entailed by the Wikidata ontology. Both Wikontic and HippoRAG achieve over 96% answer coverage, surpassing AriGraphs 79.9%. Notably, only 3.5% of triples in Wikontic are Figure 3: Wikontic produces the most dense KGs for MuSiQue questions. For each question, subgraphs are constructed around its entities, and their sizes are reported relative to the full KG. The figure shows the relative sizes of 1 to 10-hop neighborhoods and the entire connected component containing the question, defined as all nodes reachable from any question node. erty is reflected by the average number of unique entities per relation, which is significantly higher in refinement-augmented Wikontic versions. High KG connectivity, represented by the average entity degree, ensures efficient retrieval, especially with limited search depth. Entity normalization (Stage 3) is key to building KGs with the highest density among the compared methods. The relation diversity, or the number of unique relations per two entities, captures variety of information stored in the KG. Here, Wikontic versions with relaxed ontology constraint remain the most diverse. In dense graph, important nodes should have many neighbors. We select the entities from MuSiQue questions and assess their neighborhood in graphs built by various methods (Figure 3). The largest possible neighborhood is the main connected component containing the given entities. In KGs built by the full Wikontic pipeline, each neighborhood contains the greatest number of entities, again underscoring strong KG connectivity and the importance of ontology."
        },
        {
            "title": "3.3 Answer Coverage",
            "content": "In the task of question answering, the primary purpose of the extracted KG is to extract as much relevant information from the context as possible. Ideally, the resulting graph should contain all entities mentioned in the question as well as the answer Method Wikontic, gpt4.1 Wikontic, gpt4.1-mini Wikontic, gpt4o-mini MuSiQue HotpotQA EM F1 EM F1 46.80.8 42.60.7 42.10.1 59.80.3 55.90.3 53.30.1 64.50.4 59.70.6 53.71. 76.00.4 71.70.8 65.81.0 Full context, gpt4 Supporting facts, gpt4 ReadAgent (Lee et al., 2024), gpt4 GraphReader (Li et al., 2024), gpt4 GraphRAG (Edge et al., 2024), gpt4o-mini AriGraph (Anokhin et al., 2024), gpt4o-mini AriGraph (Anokhin et al., 2024), gpt4 HOLMES (Panda et al., 2024), gpt4 33.5 45.0 35.0 38.0 40.0 36.5 45.0 48.0 42.7 56.0 45.1 47.4 53.5 47.9 57.0 58.0 53.0 57.0 48.9 55.0 58.7 60.0 68.0 66.0 68.4 73.8 62.0 70.0 63.3 68.0 74.7 78. Wikontic, Llama 3.3 HippoRAG v2 (Gutiérrez et al., 2025), Llama 3.3 37.70.6 37.2 49.70.4 48.6 55.10.5 62.7 67.40.5 75.5 Table 4: Exact Match (EM) and F1 scores on the MuSiQue and HotpotQA. Wikontic operates solely on KG triplets without accessing the source text, yet achieves performance comparable to or even exceeding both raw-text baselines (Full Context, Supporting Facts) and retrieval-augmented KG approaches that still rely on source text access. ontology-misaligned, confirming that the generated knowledge is largely schema-consistent, only small portion of produced triplets requires correction to fully satisfy ontology constraints. Without ontology constraints, Wikontic reaches the highest coverage (97.5%), avoiding mismatches between Wikidata and MuSiQue entities and making it particularly effective for open-domain QA. When ontology consistency is required, the filtered Wikontic variant attains 100% ontology entailment with competitive 93.8% coverage. These findings suggest that Wikontic variants provide the best solution both with and without the ontology. Ontology and filtering help achieve the most standardized graph and strong answer coverage by thorough triplet refinement and deduplication strategies, Wikontic achieves the best coverage of information essential for question answering, while maintaining the highest connectivity levels, crucial for enabling graph search."
        },
        {
            "title": "3.4 Computational Efficiency",
            "content": "We evaluated the computational efficiency of different KG-construction methods by counting the number of input (prompt) and output (completion) tokens required to build KG from single paragraph in the MuSiQue dataset. Table 5 presents the estimated token-based costs for Wikontic, AriGraph, and GraphRAG, based on publicly available data and original implementations."
        },
        {
            "title": "Prompt\nCompletion",
            "content": "12,687 881 11,000 2,500 115,000 20,000 Table 5: Mean token efficiency for KG construction per text paragraph of Wikontic compared with AriGraph and GraphRAG on the MuSiQue dataset. around 3-5 times more expensive45 and computationally intensive than input tokens (Zhou et al., 2024). Under this metric, Wikontic is more efficient, producing KGs with roughly three times fewer output tokens than AriGraph (881 vs 2,500) and about twenty times fewer than GraphRAG (881 vs 20,000). This shows that Wikontic achieves comparable KG construction quality while using significantly fewer output tokens."
        },
        {
            "title": "3.5 Performance on QA Tasks",
            "content": "Table 4 reports Exact Match (EM) and F1 scores on the MuSiQue and HotpotQA datasets. Unlike retrieval-augmented approaches such as HippoRAG and AriGraph, which use KGs primarily to retrieve and process relevant text passages, our method performs reasoning directly over structured triplets without accessing the original documents. Despite this constraint, using our triplet-only contexts for gpt-4.1, it achieves strong results, reaching 64.5 EM and 76.0 F1 on HotpotQA and 46.8 EM and 59.8 F1 on MuSiQue. These scores surpass several retrieval-based methods, including ReadAgent and GraphReader, and are comparable key indicator of computational cost is the number of completion tokens, which are typically 4https://claude.com/platform/api/ 5https://openai.com/api/pricing/ to more resource-intensive systems such as AriGraph and HOLMES that rely on more rich textual context. This demonstrates that complete and wellstructured symbolic representations of KGs can serve as sufficient and reliable information source for multi-hop reasoning."
        },
        {
            "title": "3.6 Ablation Study",
            "content": "EM Method variant F1 Wikontic (gpt4.1-mini) 42.60.7 55.90.3 23.90.2 39.40.4 36.50.8 50.01.4 36.31.2 48.81.0 w/o qualifiers w/o aliases w/o ontology (2) w/o ontology (2) and normalization (3) 27.01.2 36.92.2 Single-step QA 31.30.6 43.40. Table 6: Ablations of the Wikontic pipeline on MuSiQue. Single-step QA omits iterative subquestion reasoning. Removing ontology or entity normalization yields the largest drop, highlighting their importance for accurate reasoning over the constructed KG. To evaluate the contribution of individual Wikontic components, we conducted ablations  (Table 6)  . Removing qualifiers leads to substantial performance drop (15.9 EM, 15.7 F1) on MuSiQue, indicating that qualifier information is essential for capturing fine-grained relational context. Excluding aliases (introduced at Stage 3) moderately decreases performance, confirming that alias expansion improves entity matching in question answering. Eliminating ontology integration (Stage 2) reduces both EM and F1, demonstrating the importance of type and schema constraints for consistent KG construction. When both ontology and entity normalization are removed (Stages 2 and 3), performance degrades most severely. The singlestep QA variant also performs significantly worse, confirming that multi-hop question decomposition is essential for effective reasoning over the constructed KG. Overall, these findings show that each component contributes meaningfully to Wikontics performance, with ontology-guided refinement and iterative retrieval being the most critical for downstream reasoning over the constructed KG. cating entities and relations. While using KGs as the sole knowledge source for multi-hop question answering, Wikontic achieves competitive performance with retrieval-augmented and KG-based baselines that still rely on source texts, demonstrating that ontology-guided KG construction is viable alternative to passage-level retrieval. On MuSiQue, Wikontic includes 3845 more unique entities than HippoRAG and AriGraph and contains the correct answer entity in 97.5% of cases. Within 10-hop subgraph, it maintains 70% answer coverage and denser local connectivity. For QA, it attains 64.5 EM / 76.0 F1 on HotpotQA and 46.8 EM / 59.8 F1 on MuSiQue, outperforming textreliant systems (ReadAgent, GraphReader) and approaching larger text-dependent methods (AriGraph, HOLMES). Moreover, Wikontic achieves state-of-the-art results on the MINE-1 benchmark, achieving 8486% information-retention scores and surpassing GraphRAG and KGGen. These findings indicate that the proposed pipeline preserves substantial amount of factual information from source texts. Our approach is also token-efficient: during KG construction, Wikontic uses under 1,000 output tokens, about 3 fewer than AriGraph and 1/20 of GraphRAG, while preserving accuracy. Moreover, only 3.5% of extracted triplets are flagged as ontology-misaligned, indicating that nearly all generated knowledge is schema-consistent, minimizing the need for manual correction and significantly reducing annotation overhead. Beyond efficiency, the pipeline is adaptable: it can operate without an ontology or integrate domain-specific ontologies, enabling applications across specialized domains. Moreover, as LLMs increasingly serve as data generators, Wikontic provides principled foundation for producing verified, structured KG data suitable for fine-tuning smaller task-specific models. Our findings show that ontology-aware KG construction enables scalable, interpretable, and verifiable transformation of unstructured text into structured knowledge, bridging together symbolic reasoning and generative language modeling."
        },
        {
            "title": "Limitations",
            "content": "We introduced Wikontic, fully automated pipeline that uses LLMs to construct KGs from unstructured text. The pipeline produces compact, internally consistent graphs by aligning extracted triplets with the Wikidata ontology and dedupliOur experiments are restricted to proprietary OpenAI models (GPT-4.1, GPT-4.1-mini, GPT-4omini) and the open-source Llama 3.3-70B. Token efficiency is measured as model-generated tokens during answer production. This metric reflects provider billing but not end-to-end latency or throughput. Every stage of the pipeline currently uses LLMs with instructions and in-context examples prompting. Because the pipeline now yields its own annotated data, several stages could be replaced in future work by smaller, task-specific models fine-tuned on this data, thereby improving efficiency and lowering computational cost."
        },
        {
            "title": "References",
            "content": "Zirui Guo, Lianghao Xia, Yanhua Yu, Tu Ao, and Chao Huang. 2024. Lightrag: Simple and fast retrievalaugmented generation. Bernal Gutierrez, Yiheng Shu, Yu Gu, Michihiro Yasunaga, and Yu Su. 2024. Hipporag: Neurobiologically inspired long-term memory for large language models. volume 37, pages 5953259569. Bernal Jiménez Gutiérrez, Yiheng Shu, Weijian Qi, Sizhe Zhou, and Yu Su. 2025. From rag to memory: Non-parametric continual learning for large language models. arXiv preprint arXiv:2502.14802. Petr Anokhin, Nikita Semenov, Artyom Sorokin, Dmitry Evseev, Mikhail Burtsev, and Evgeny Burnaev. 2024. Arigraph: Learning knowledge graph world models with episodic memory for llm agents. arXiv preprint arXiv:2407.04363. Haoyu Han, Yu Wang, Harry Shomer, Kai Guo, Jiayuan Ding, Yongjia Lei, Mahantesh Halappanavar, Ryan Rossi, Subhabrata Mukherjee, Xianfeng Tang, and 1 others. 2024. Retrieval-augmented generation with graphs (graphrag). arXiv preprint arXiv:2501.00309. Pere-Lluís Huguet Cabot and Roberto Navigli. 2021. Rebel: Relation extraction by end-to-end language generation. In Findings of the Association for Computational Linguistics: EMNLP 2021, pages 2370 2381. Hanzhu Chen, Xu Shen, Qitan Lv, Jie Wang, Xiaoqi Ni, and Jieping Ye. 2024. Sac-kg: Exploiting large language models as skilled automatic constructors for domain knowledge graph. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 4345 4360. Alla Chepurova, Yurii Kuratov, Aydar Bulatov, and Mikhail Burtsev. 2024. Prompt me one more time: two-step knowledge extraction pipeline In Proceedings with ontology-based verification. of TextGraphs-17: Graph-based Methods for Natural Language Processing, pages 6177. Prafulla Kumar Choubey, Xin Su, Man Luo, Xiangyu Peng, Caiming Xiong, Tiep Le, Shachar Rosenman, Vasudev Lal, Phil Mui, Ricky Ho, and 1 others. 2024. Distill-synthkg: Distilling knowledge graph synthesis workflow for improved coverage and efficiency. arXiv preprint arXiv:2410.16597. Jiuzhou Han and Ehsan Shareghi. 2022. Self-supervised graph masking pre-training for graph-to-text generation. arXiv preprint arXiv:2210.10599. Pere-Lluís Huguet Cabot and Roberto Navigli. 2021. Rebel: Relation extraction by end-to-end language generation. In Findings of the Association for Computational Linguistics: EMNLP 2021, Online and in the Barceló Bávaro Convention Centre, Punta Cana, Dominican Republic. Association for Computational Linguistics. Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bojanowski, Armand Joulin, and Edouard Grave. 2021. Unsupervised dense information retrieval with contrastive learning. arXiv preprint arXiv:2112.09118. Martin Josifoski, Nicola De Cao, Maxime Peyrard, Fabio Petroni, and Robert West. 2021. Genie: Generative information extraction. arXiv preprint arXiv:2112.08340. Martin Josifoski, Marija Sakota, Maxime Peyrard, and Robert West. 2023. Exploiting asymmetry for synthetic training data generation: Synthie and the case of information extraction. arXiv preprint arXiv:2303.04132. Xinbang Dai, Yuncheng Hua, Tongtong Wu, Yang Sheng, Qiu Ji, and Guilin Qi. 2025. Large language models can better understand knowledge graphs than we thought. Knowledge-Based Systems, 312:113060. Kuang-Huei Lee, Xinyun Chen, Hiroki Furuta, John Canny, and Ian Fischer. 2024. human-inspired reading agent with gist memory of very long contexts. arXiv preprint arXiv:2402.09727. Bayu Distiawan, Gerhard Weikum, Jianzhong Qi, and Rui Zhang. 2019. Neural relation extraction for knowledge base enrichment. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 229240. Darren Edge, Ha Trinh, Newman Cheng, Joshua Bradley, Alex Chao, Apurva Mody, Steven Truitt, Dasha Metropolitansky, Robert Osazuwa Ness, and Jonathan Larson. 2024. From local to global: graph rag approach to query-focused summarization. arXiv preprint arXiv:2404.16130. Shilong Li, Yancheng He, Hangyu Guo, Xingyuan Bu, Ge Bai, Jie Liu, Jiaheng Liu, Xingwei Qu, Yangguang Li, Wanli Ouyang, and 1 others. 2024. Graphreader: Building graph-based agent to enhance long-context abilities of large language models. arXiv preprint arXiv:2406.14550. Belinda Mo, Kyssen Yu, Joshua Kazdan, Joan Cabezas, Proud Mpala, Lisa Yu, Chris Cundy, Charilaos Kanatsoulis, and Sanmi Koyejo. 2025. Kggen: Extracting knowledge graphs from plain text with language models. arXiv preprint arXiv:2502.09956. Ranran Haoran Zhang, Qianying Liu, Aysa Xuemo Fan, Heng Ji, Daojian Zeng, Fei Cheng, Daisuke Kawahara, and Sadao Kurohashi. 2020. Minimize exposure bias of seq2seq models in joint entity and relation extraction. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 236246. Zixuan Zhou, Xuefei Ning, Ke Hong, Tianyu Fu, Jiaming Xu, Shiyao Li, Yuming Lou, Luning Wang, Zhihang Yuan, Xiuhong Li, and 1 others. 2024. survey on efficient inference for large language models. arXiv preprint arXiv:2404.14294. Shirui Pan, Linhao Luo, Yufei Wang, Chen Chen, Jiapu Wang, and Xindong Wu. 2024. Unifying large language models and knowledge graphs: roadmap. IEEE Transactions on Knowledge and Data Engineering, 36(7):35803599. Pranoy Panda, Ankush Agarwal, Chaitanya Devaguptapu, Manohar Kaul, and Prathosh Ap. 2024. Holmes: Hyper-relational knowledge graphs for multi-hop question answering using llms. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1326313282. Fina Polat, Ilaria Tiddi, and Paul Groth. 2025. Testing prompt engineering methods for knowledge extraction from text. Semantic Web, 16(2):SW243719. Gabriel Stanovsky, Julian Michael, Luke Zettlemoyer, and Ido Dagan. 2018. Supervised open information extraction. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 885 895. Yuan Sui, Yufei He, Zifeng Ding, and Bryan Hooi. 2024. Can knowledge graphs make large language models more trustworthy? an empirical study over arXiv preprint open-ended question answering. arXiv:2410.08085. Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. 2022. Musique: Multihop questions via single-hop question composition. Transactions of the Association for Computational Linguistics, 10:539554. Denny Vrandeˇcic. 2012. Wikidata: new platform for collaborative data collection. In Proceedings of the 21st International Conference on World Wide Web, page 10631064, New York, NY, USA. Association for Computing Machinery. Chenguang Wang, Xiao Liu, and Dawn Song. 2020. Language models are open knowledge graphs. arXiv preprint arXiv:2010.11967. Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher Manning. 2018. Hotpotqa: dataset for diverse, explainable multi-hop question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 23692380. Liang Yao, Chengsheng Mao, and Yuan Luo. 2019. Kgbert: Bert for knowledge graph completion. arXiv preprint arXiv:1909.03193. Daojian Zeng, Kang Liu, Siwei Lai, Guangyou Zhou, and Jun Zhao. 2014. Relation classification via convolutional deep neural network. In Proceedings of COLING 2014, the 25th international conference on computational linguistics: technical papers, pages 23352344. Prompt 1. Candidate Triplet Extraction You are an algorithm designed to extract structured knowledge from texts to build Wikidata-like knowledge graph. knowledge graph consists of triplets in the format (subject, relation, object), where: Subject: named entity or concept describing group of people, events, or abstract objects that serves as the source of the relation. Relation: Wikidata-style predicate connecting the subject and the object. Object: named entity or concept describing group of people, events, or abstract objects related to the subject. Additionally, some triplets may have qualifiers that provide more context (e.g., date, place, or other attributes). Qualifiers should have relations and objects like triplets do, but instead of subject, their relation connects an object and the triplet they qualify. Qualifiers must always be attached to triplet and never exist as standalone triplets. You will receive text labeled Text:. Your task is to extract meaningful triplets that represent factual relationships. Output Format. Return only triplets in JSON format as list of dictionaries: \"subject\": Subject entity. \"relation\": Relation connecting subject and object. \"object\": Object entity. \"qualifiers\": List of dictionaries, each with: \"relation\": Relation connecting triplet and object. \"object\": Object entity connected to the main triplet. \"subject_type\": Class that describes the subject. \"object_type\": Class that describes the object."
        },
        {
            "title": "A Appendix",
            "content": "A.1 Reproducibility We release: (1) prompts and source code for each pipeline stage; (2) scripts to build Wikidata-derived ontology used for ontology-aware stages; (3) the KG-only multi-hop QA component. We also plan to release all KGs that were build with Wikontic pipeline. The source code is included as Supplementary Material with this submission. We reported metrics averaged across multiple runs (Tables 4, 6) with standard deviation. For Table 3, standard deviations are estimated via bootstrap resampling. The exact prompts used at each stage are provided in the Appendix A.4, A.5. A.2 Dataset Statistics The test splits of HotpotQA and MuSiQue consist of 1000 samples each. We provide the QA evaluation results for Wikontic and HippoRAG for the whole evaluation set and for AriGraph we report the openly available results for 200 test samples. To compare the statistics of KGs we use 80 MuSiQue samples that are commonly available for all compared methods. A.3 Computational resources All experiments on knowledge graph (KG) construction and question answering (QA) were conducted using the OpenAI and OpenRouter APIs. Across all datasets, models, and ablation studies, KG construction and QA (each QA experiment repeated three times to compute mean and standard deviation) required total cost of approximately $500. A.4 Prompts for triplet extraction Here we provide excerpts of prompts that were used in our KG construction pipeline. Prompt 1 was used for candidate triplet extraction. Subsequently, Prompt 2 was used to refine entity types for both subject and object entities. Prompt 3 was used for choosing relation among ones that can legally connect chosen entity types by Wikidata constraints. Finally, Prompt 6 was used to refine surface forms of subject and object. All prompts, instructions, in-context examples are available with the code. Prompt 2. Triplet Backbone Refinement - choosing Relevant Entity Types Prompt 3. Triplet Backbone Refinement - choosing Relevant Relation You are given factual triplet extracted from text. The triplet follows the format (subject, relation, object), where: You are given factual triplet extracted from text. The triplet follows the format (subject, relation, object), where: Subject: named entity or concept that represents person, group, event, or abstract entity serving as the source of the relation. Relation: Wikidata-style predicate that defines the connection between the subject and the object. Object: named entity or concept that represents person, group, event, or abstract entity related to the subject. Subject type: class that describes the object. Object type: class that describes the subject. The extracted entity types of both subject and object were mapped to set of similar Wikidata-style entity types based on semantic similarity. Your Task: You will be provided with the following: Text: The original sentence or passage from which the triplet was extracted. Extracted Triplet: The factual triplet derived from the text. Candidate subject types: similar entity types for subject type of extracted triplet retrieved from Wikidata. Candidate object types: similar entity types for object type of extracted triplet retrieved from Wikidata. Select the most appropriate candidate entity types for both subject and object from the provided candidates that best match the meaning of previously extracted triplet and original text. Provide ONLY an answer in JSON format with the following keys: \"subject_type\": Selected subject type candidate. \"object_type\": Selected object type candidate. Subject: named entity or concept that represents person, group, event, or abstract entity serving as the source of the relation. Relation: Wikidata-style predicate that defines the connection between the subject and the object. Object: named entity or concept that represents person, group, event, or abstract entity related to the subject. Subject type: class that describes the object. Object type: class that describes the subject. The extracted relation has been mapped to set of similar Wikidata-style relations based on semantic similarity and the entity types they can connect. Your Task: You will be provided with the following: Text: The original sentence or passage from which the triplet was extracted. Extracted Triplet: The factual triplet derived from the text. Candidate relations: list of relation (or in other words property) names similar to the extracted relation from triplet retrieved from Wikidata. Candidate relations: list of relation (or in other words property) names similar to the extracted relation from triplet retrieved from Wikidata. Select the most appropriate relation candidate from the provided candidate triplets that best match the meaning of previously extracted triplet and original text. Provide only an answer in JSON format with the following keys: \"relation\": Relation for the selected triplet. Prompt 4. Entity Names Refinement In the previous step, there was extracted triplet akin to one in Wikidata knowledge graph from the text. Triplet contains two entities (subject and object) and one relation that connects these subject and object. Using semantic similarity, we linked subject name with top similar exact names from the knowledge graph built from previously seen texts. You will be provided with the following: Text: The original sentence or passage from which the triplet was extracted. Extracted Triplet: structured representation in the format \"subject\": \"...\", \"relation\": \"...\", \"object\": \"...\" . Original Subject: subject name that needs refinement. Candidate Subjects: list of possible entity names from previously seen texts. Your Task: Select the most contextually appropriate subject name from the Candidate Subjects list that best matches subject from extracted triplet and context of the given Text. If an exact or semantically appropriate match is found, return the corresponding name exactly as it appears in the list. If no suitable match exists, return the string \"None\". Do not modify name from the candidate list in case of match, add explanations, or provide any additional text. Propmt 5. Entity extraction for question answering Extract wikidata-like entities from the question below. It is guaranteed that there is at least one mentioned entity. Extract any entity, whether name entity or an abstract entity, that might help retrieve the information to answer the question. Provide output in json format, no additional symbols. Output should be represented as LIST of extracted entities names. Prompt 6. Entity linking for question answering Task: Identify relevant entities from pre-constructed knowledge graph that might help to answer provided question. Input Structure: The question will be labeled as \"Question:\". list of entities from the knowledge graph will be labeled as \"Entities:\". A.5 Prompts for question answering Selection Criteria: Here we provide excerpts of prompts that were used for KG grounded question answering. Prompt 5 was used to extract entities relevant for the question. Then, with Prompt 6 the LLM was instructed to choose relevant entities among entities similar to the extracted ones in the KG. Prompt 7 was used to decompose the question on single-hop subquestion conditioned on extracted entities or previously answered subquestion. Prompt 8 was used to check if question is answered by sequence of subquestions and corresponding answers. All prompts, instructions, in-context examples are available with the code. Relevance means an entity is directly or indirectly useful for answering the question. Look for names, events, dates, and other related concepts or entities that match or connect to key concepts in the question. Do not ignore possible indirect relevance (e.g., if the question asks about competition, teams or winners of that competition may be useful). Response Format: Always return at least one relevant entity. It is guaranteed that there is at least one. The output must be JSON list of dictionaries, where each dictionary contains key \"entity\": the name of the chosen relevant entity Do not return an empty list. Select the best possible options. Prompt 7. Question decomposition Prompt 8. Check if question is answered You are an assistant for stepwise question decomposition. You will be given three inputs: An original multi-hop question. You are reasoning assistant for multi-hop question answering. Your task: Decide whether list of subquestions and their answers fully resolves the original multi-hop question. 1-hop sub-question that has already been answered. Input format: The answer to that 1-hop sub-question. Your task: Reformulate the original multi-hop question by integrating obtained answer from sub-question, so the new question has (n-1) hops. Rules: Only perform one reasoning hop at time. Do not generate additional reasoning steps beyond this hop. Original multi-hop question: <text> Question->answer sequence: [a list of subquestions and their answers, ending with the most recent one] Output rules: If the sequence of subquestions and answers completely and directly resolves the original multi-hop question, output only the final answer to the original multi-hop question (not just the last subanswer, i.e. answer the original question). Do not include explanations or text, just reformulated question. If the sequence is not sufficient and more reasoning or hops are needed, output exactly: NOT FINAL include any prefixes like \"Final answer:\", Do not \"Answer:\", suffixes, formatting, original questions or explanations. Output must be single line: either string with the final answer to the original multi-hop question or the exact string NOT FINAL. <example> Original multi-hop question: Who was the spouse of the person who wrote The Iron Heel? Question->answer sequence: Who wrote The Iron Heel? Jack London Who was the spouse of Jack London? Charmian London Expected output: Charmian London </example> <example> Original multi-hop question: Which countrys capital is closest to the birthplace of Nikola Tesla? Question->answer sequence: Where was Nikola Tesla born? Smiljan, Croatia Expected output: NOT FINAL </example> A.6 Triplet extraction pipeline examples Figure 4: Overview of the multi-stage pipeline for KG extraction from unstructured text. The process consists of (1) LLM-based triplet extraction, (2) ontology-based validation of triplet structure, and (3) entity linking and normalization. Figure 5: Ontology-based triplet refinement process. For each extracted triplet, we retrieve and extend candidate entity types using Wikidatas type hierarchy, identify valid relations allowed to use between extracted entities based on ontology constraints, and re-rank relation candidates using semantic similarity. The final triplet configuration is selected by an LLM. Figure 6: Entity refinement step for KG construction. For each refined triplet, candidate subject and object entities are retrieved from the existing KG based on their type and semantic similarity. An LLM determines whether the extracted entity matches an existing one or should be preserved as new entry. This process reduces redundancy and supports incremental KG updates."
        }
    ],
    "affiliations": [
        "Cognitive AI Systems Lab, Moscow, Russia",
        "London Institute for Mathematical Sciences, London, UK",
        "Moscow Independent Research Institute of Artificial Intelligence, Moscow, Russia"
    ]
}