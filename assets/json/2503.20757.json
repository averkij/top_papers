{
    "paper_title": "MCTS-RAG: Enhancing Retrieval-Augmented Generation with Monte Carlo Tree Search",
    "authors": [
        "Yunhai Hu",
        "Yilun Zhao",
        "Chen Zhao",
        "Arman Cohan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce MCTS-RAG, a novel approach that enhances the reasoning capabilities of small language models on knowledge-intensive tasks by leveraging retrieval-augmented generation (RAG) to provide relevant context and Monte Carlo Tree Search (MCTS) to refine reasoning paths. MCTS-RAG dynamically integrates retrieval and reasoning through an iterative decision-making process. Unlike standard RAG methods, which typically retrieve information independently from reasoning and thus integrate knowledge suboptimally, or conventional MCTS reasoning, which depends solely on internal model knowledge without external facts, MCTS-RAG combines structured reasoning with adaptive retrieval. This integrated approach enhances decision-making, reduces hallucinations, and ensures improved factual accuracy and response consistency. The experimental results on multiple reasoning and knowledge-intensive datasets datasets (i.e., ComplexWebQA, GPQA, and FoolMeTwice) show that our method enables small-scale LMs to achieve performance comparable to frontier LLMs like GPT-4o by effectively scaling inference-time compute, setting a new standard for reasoning in small-scale models."
        },
        {
            "title": "Start",
            "content": "MCTS-RAG: Enhance Retrieval-Augmented Generation with Monte Carlo Tree Search Yunhai Hu Yilun Zhao Chen Zhao Arman Cohan Yale University New York University https://github.com/yale-nlp/MCTS-RAG 5 2 0 2 6 ] . [ 1 7 5 7 0 2 . 3 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "We introduce MCTS-RAG, novel approach that enhances the reasoning capabilities of small language models on knowledge-intensive tasks by leveraging retrieval-augmented generation (RAG) to provide relevant context and Monte Carlo Tree Search (MCTS) to refine reasoning paths. MCTS-RAG dynamically integrates retrieval and reasoning through an iterative decision-making process. Unlike standard RAG methods, which typically retrieve information independently from reasoning and thus integrate knowledge suboptimally, or conventional MCTS reasoning, which depends solely on internal model knowledge without external facts, MCTS-RAG combines structured reasoning with adaptive retrieval. This integrated approach enhances decision-making, reduces hallucinations, and ensures improved factual accuracy and response consistency. The experimental results on multiple reasoning and knowledge-intensive datasets datasets (i.e., ComplexWebQA, GPQA, and FoolMeTwice) show that our method enables small-scale LMs to achieve performance comparable to frontier LLMs like GPT-4o by effectively scaling inference-time compute, setting new standard for reasoning in small-scale models."
        },
        {
            "title": "Introduction",
            "content": "Recent advancements in MCTS-based reasoning have demonstrated remarkable improvements in structured decision-making and logical inference (Kocsis and Szepesvári, 2006; Browne et al., 2012; Xie et al., 2024a). The rStar framework (Qi et al., 2024), for instance, has shown that systematic search and exploration can significantly enhance reasoning performance, enabling small-scale LMs (i.e., models with up to 7B parameters) to compete with much larger models. However, key limitation of these approaches is their heavy Equal contributions. Correspondence: Yilun Zhao (yilun.zhao@yale.edu) reliance on internal knowledge, which hinders their effectiveness in knowledge-intensive tasks. On the other hand, RAG has been widely used to solve knowledge-intensive tasks (Lewis et al., 2020; Karpukhin et al., 2020; Izacard and Grave, 2021), but its effectiveness with small-scale LMs remains limited. small-scale LMs struggle with query formulation and retrieved content comprehension, often generating vague queries and misinterpreting key details (Fan et al., 2025). Moreover, existing RAG systems do not dynamically adjust their retrieval strategies based on changing informational or reasoning requirements, which results in unnecessary or repetitive retrieval steps (Li et al., 2024; Gao et al., 2024). For example, when answering multi-hop question like Which novel inspired the movie that won Best Picture in 1994?, standard retrieval system might retrieve documents about Forrest Gump (i.e., Best Picture winner in 1994), but fail to recognize the need for additional reasoning or retrieval steps to establish the connection between Forrest Gump and the novel written by Winston Groom. This limitation arises because small-scale language models often lack the ability to refine queries iteratively and integrate retrieved information into coherent reasoning process. To address the aforementioned limitations, we propose MCTS-RAG, novel framework that integrates MCTSs reasoning and search capabilities with adaptive retrieval mechanisms. At high level, MCTS-RAG operates by iteratively refining both retrieval and reasoning through search-based process. Given query, it explores multiple reasoning paths, dynamically incorporating retrieval actions at key decision points. Retrieved knowledge is then used to evaluate intermediate states, and beneficial retrieval pathways are reinforced through backpropagation. This structured search mechanism ensures that the model efficiently acquires and utilizes relevant information for more accurate reasoning. In contrast, by integrating retrieval with search-based reasoning, MCTS-RAG is able to systematically explore relevant knowledge and reason over it to obtain the correct answer. MCTS-RAG has the following key features: Improved reasoning accuracy: New retrieval actions enable SLMs to acquire external knowledge and enhance the quality of question answering (3.2). Optimized query formulation: The refinement process ensures that each query focuses on specific information needs, improving the effectiveness of retrieval query generation (3.3). Enhanced retrieval quality: Reflecting on and summarizing retrieved information helps reduce semantic discrepancies and ensures alignment with the core problem (3.3). MCTS-RAG demonstrates superior performance on various knowledgeintensive benchmarks, including ComplexWebQA (CMQA) (Talmor and Berant, 2018), GPQA (Rein et al., 2024), and FoolMeTwice (FMT) (Eisenschlos et al., 2021a). Specifically, it achieves over 20% improvement with Llama 3.1-8B and 6% with Qwen2.5-7B on CWQA, roughly 15% and 10% gains on GPQA, and over 10% (Llama) and 4% (Qwen) on FMT, while outperforming other baselines like Standard RAG, ReAct (Yao et al., 2023b), Self-Ask (Press et al., 2023), Search-O1 (Li et al., 2025), and rStar (Qi et al., 2024) by effectively retrieving and integrating evidence through refined multi-step reasoning that minimizes hallucinations."
        },
        {
            "title": "2 Related Work",
            "content": "Inference-time Scaling. Inference-time scaling enhances reasoning without modifying model parameters by optimizing computational allocation during generation. core approach involves reasoning diversification and selection: generating multiple candidates (Wang et al., 2023) and choosing optimal outputs via voting (Liang et al., 2024) or verifier-guided ranking (Cobbe et al., 2021). Structured search algorithms, such as beam search (Xie et al., 2024b) and tree-of-thought frameworks (Yao et al., 2023a), explicitly model reasoning paths. Recently, Monte Carlo Tree Search (MCTS) has been applied to balance exploration and exploitation in reasoning tasks, iteratively refining solutions through selection, expansion, simulation, and backpropagation (Hao et al., 2023). Further, integrating MCTS with LLMs using value functions (Zhang et al., 2024) or predefined reasoning heuristics (Qi et al., 2024) has improved efficiency in mathematical reasoning and code generation. Retrieval-Augmented Generation. The RAG system enhances LLMs in knowledge-intensive tasks by incorporating external information. Query optimization techniques, including expansion and transformation, improve retrieval quality (Ma et al.; Jagerman et al., 2023). Iterative retrieval methods, such as IRCoT (Trivedi et al., 2023) and ITERRETGEN (Shao et al.), refine retrieval and generation. LLM-driven retrieval strategies, such as WebGPT (Nakano et al., 2021) and Toolformer (Schick et al., 2023), have demonstrated notable improvements in efficiency by leveraging large language models to interact with external tools or search engines, thus streamlining the process of gathering relevant data. Meanwhile, self-reflection mechanisms in systems like Self-RAG (Asai et al.; Islam et al., 2024) and Auto-RAG (Yu et al., 2024) further enhance retrieval relevance by employing iterative introspection to refine intermediate outputs. To address this, reasoning-intensive retrieval methods have emerged. For example, BRIGHT (Su et al., 2024) introduces complex, reasoningdriven queries that challenge traditional retrieval approaches, while Rank1 (Weller et al., 2025) leverages advanced inference-time reranking to identify nuanced relationships missed by standard methods. Despite these advancements, however, these methods often overlook alternative solutions due to their linear reasoning approach and the limited capabilities of small-scale LMs."
        },
        {
            "title": "3.1 Preliminaries",
            "content": "rStar (Qi et al., 2024) is recently proposed selfconsistency framework designed to enhance the reasoning capabilities of language models without requiring additional fine-tuning or reliance on stronger teacher models. rStar achieves this by breaking down the reasoning process into two distinct yet interconnected phases: generation and discrimination. In the Generation Phase, the model proactively explores multiple reasoning trajectories through human-like reasoning actions, including step-by-step inference and question decomposition. Subsequently, the Discrimination Phase evaluates these candidate reasoning paths, selecting and refining them to identify the most logically consistent and accurate responses. However, the original rStar framework is limited by its inability to dynamically acquire external knowledge, restricting its performance in Figure 1: An illustration of MCTS-RAG workflow for answering the question sampled from ComplexWebQA. knowledge-intensive queries. To address the inherent limitations of rStar, we propose an integrated reasoning framework that combines the iterative reasoning capabilities of rStar with RAG. At high level, our approach builds on the iterative generative-discriminative structure of rStar and introduces additional operations specifically designed to facilitate dynamic external knowledge retrieval. This enables the language model to seamlessly integrate relevant external information into its reasoning process, significantly improving factual accuracy and decision robustness. The following subsections detail the proposed MCTS-RAG framework."
        },
        {
            "title": "3.2 Action Space Definition",
            "content": "We design set of discrete actions at each MCTS decision point: A1A3 from rStar (Qi et al., 2024), along with two new RAG-related actions A4 and A5 and summary action A6, enabling dynamic knowledge acquisition and enhanced reasoning synergy for improved decision-making. A1: Direct Answer: Provide an immediate response based on existing reasoning or previously known context, suitable for straightforward queries or when additional analysis is unnecessary. A2: Quick Reasoning: Execute rapid, incremental reasoning steps based on the current context, ideal for exploratory paths or preliminary judgments to efficiently guide the search. A3: Decompose Question: Break complex queries into smaller, manageable subquestions, allowing for clearer problemsolving pathways and improved reasoning efficiency, particularly beneficial for multipart or intricate problems. A4: Retrieval Reasoning: Actively retrieve relevant knowledge from internal or external sources before proceeding with the next reasoning step, critical for queries requiring supplementary information or when existing context is incomplete. A5: Retrieval Decompose: Integrate both decomposition and retrieval, first breaking down complex questions and then acquiring relevant knowledge to solve individual subproblems. This action is highly effective for queries involving detailed context-dependent sub-questions. A6: Summarized Answer: Generate concise, structured summary that synthesizes results from previous reasoning and retrieved information, providing coherent and comprehensive responses especially useful for queries that demand summarization or integration of multifaceted information. Each action is designed to address specific aspects of the reasoning-retrieval interplay, ensuring that the model can adapt its strategy dynamically as it navigates through the problem space. To further enhance exploration, we employ Upper Confidence Bound for Trees (UCT) (Kocsis and Szepesvári, 2006) in our MCTS frameworka crucial method that balances exploitation and exploration. The UCT formula is: UCT(s, a) = Q(s, a) + (cid:115) ln (s) (s, a) , where Q(s, a) = Q(s,a) (s,a) is the average reward for action in state s, with Q(s, a) as the cumulative reward and (s, a) as the visit count. (s) is the total number of visits to state s. is the exploration constant, controlling the balance between exploitation and exploration. Within MCTS-RAG, search depth limits how many levels are expanded from the root node to control the search range, while the number of rollouts indicates how many times the simulation is run from selected node until termination or preset limit to estimate its value. By running simulations within controlled depth and updating node statistics via UCT, MCTS effectively balances exploration and exploitation with finite computational resources, continuously refining its search strategy."
        },
        {
            "title": "3.3 Retrieval Process",
            "content": "Our approach dynamically retrieves information within an evolving MCTS reasoning environment, enabling timely and relevant integration of external knowledge. The model autonomously determines when retrieval is required, generates targeted queries, and critically integrates external knowledge to improve reasoning accuracy. By interweaving retrieval with reasoning, we streamline information flow and produce concise yet informative outputs. If previously retrieved data adequately answers the current reasoning stepdetermined by checking whether the information satisfies predefined accuracy thresholds or resolves open reasoning pathsthe model foregoes additional retrieval, thus avoiding redundancy. Figure 2: An illustration of MCTS-RAG retrieval process (i.e., R1-R4) within one step of the retrieval decomposition action highlighted in Figure 2. R1: Query Generation: If knowledge gap is detected, the model generates search queries. R2: Query Execution: External retrieval tools are used to obtain the most relevant information. R3: Knowledge Reflection: Retrieved data is evaluated for relevance and consistency to determine its inclusion in the reasoning process. R4: Summary Answer: Refined information is integrated, enabling the model to answer subquestions or advance reasoning. This interleaved retrieval process ensures that the models reasoning is continuously updated and validated against external data, thereby reducing errors and enhancing the robustness of final output."
        },
        {
            "title": "3.4 Determing Final Answer",
            "content": "At the conclusion of the MCTS exploration (illustrated in the bottom part of Figure 2), the best answer is selected through voting mechanism and consistency analysis over candidate solutions. Specifically, each reasoning trajectory obtained from the MCTS yields candidate answer cj, resulting in candidate answer set = {c1, c2, . . . , cM }. These candidate answers are grouped into set of unique answers = {a1, a2, . . . , aN } based on semantic consistency. The final score for each unique answer ak is computed as the sum of the rewards of all candidates grouped under ak, where the reward of each candidate cj is the product of rewards for all nodes along its corresponding reasoning trajectory. Score(ak) = (cid:80) cj C(ak) Reward(cj) (cid:80) cj Reward(cj) The best answer is then determined as = arg max ak Score(ak), (1) (2) ensuring that the most frequent and consistent reasoning trajectory is chosen."
        },
        {
            "title": "4 Experiments",
            "content": "4.1 Experimental Setup We evaluate Qwen2.5-7B and Llama 3.1-8B on three complex reasoning tasks: ComplexWebQA (CWQA) (Talmor and Berant, 2018), which requires multi-step reasoning over webbased queries; Graduate-Level Google-Proof QA (GPQA) (Rein et al., 2023), which tests knowledgeintensive science question answering; and FoolMeTwice (FMT) (Eisenschlos et al., 2021b), challenging fact-checking benchmark that assesses the models ability to verify factual claims. Baselines. We evaluate the following baseline methods for comparison: Chain-of-Thought (CoT) (Wei et al., 2022) prompting encourages the model to generate explicit step-by-step reasoning to improve complex problem-solving. Standard RAG (Ma et al., 2023) performs single-pass retrieval to augment the models responses but lacks iterative refinement. ReAct (Yao et al., 2023b) alternates between reasoning and retrieval, allowing the model to dynamically refine its understanding based on external evidence. Self-Ask with Search (Self-Ask) (Press et al., 2023) with Search decomposes complex queries into subquestions, retrieves relevant external information, and synthesizes the answers to enhance multi-step reasoning. SearchO1 (Li et al., 2025) executes single retrieval step before generating an answer, limiting its ability to iteratively verify information. Finally, the original rStar (Qi et al., 2024) algorithm employs structured iterative reasoning process but does not fully leverage dynamic retrieval or decomposition. Implementation Details. We compare MCTSRAG with other baselines using the same language models, Qwen2.5-7B and LLaMA 3.1-8B, to ensure fair comparison of reasoning capabilities. To maintain consistency across methods, we use the same retrieval corpus and retriever, thus avoiding discrepancies caused by varying information access. Specifically, we rely on the Bing Search Engine and LangChain for retrieval, as Bing provides extensive and up-to-date web information while LangChain supports robust retrievalaugmented generation workflows. For different datasets, CWQA is obtained from its web snippets, GPQA uses corpus sourced from Wikipedia and Bing, and FMT draws on its own related documents. This setup ensures that variations in performance stem from differences in reasoning mechanisms. To facilitate structured reasoning, we configure our setup with rollout of 4, allowing multiple steps of reasoning expansion. Each query can be decomposed into at most two subquestions, ensuring controlled breakdown of complex queries. We set the maximum reasoning depth to 5, enabling deep but efficient multi-hop reasoning. 4.2 Main Findings Table 1 compares reasoning methods on CWQA, GPQA, and FMT for Llama 3.1-8B and Qwen2.57B. Our approach consistently outperforms baselines, demonstrating strong multi-step reasoning and retrieval capabilities. On CWQA, it achieves over 20% gain with Llama 3.1-8B and around 6% with Qwen2.5-7B. Similarly, it surpasses competitors on GPQA by roughly 15% and 10%, respectively, benefiting from refined verification strategies. On FMT, it leads by over 10% with Llama 3.1-8B and 4% with Qwen2.5-7B, proving its resilience against misleading distractors. These results highlight our methods superior generalization and efficiency, especially in fact-checking and science-related tasks. Compared to baselines like Standard RAG, ReAct, Self-Ask, and Search-O1, Our structured multi-step reasoning can retrieve and process evidence more accurately, and on average we improve the performance by about 14% over the baseline under three datasets. Unlike rStar, it enables broader retrieval, extracting critical insights while minimizing hallucinations, achieving an average improvement of 17%. This framework sets new benchmark for complex reasoning, delivering high accuracy and efficiency in diverse problem-solving scenarios."
        },
        {
            "title": "4.3 Fine-grained Analysis",
            "content": "We evaluate the effectiveness of retrieval actions and rollout times in Table 2. Specifically, we conduct an ablation by disabling different retrieval modules (A4, A5, or both) to gauge their impact Methods Qwen2.5-7B Llama 3.1-8B Settings CWQA GPQA FMT CWQA GPQA FMT CWQA GPQA FMT Analysis of Retrieval Modules CoT 34.65 GPT-4o 54.45 Qwen2.5-72B 44.55 rStar 55.45 Standard RAG 44.21 GPT-4o 59.40 Qwen2.5-72B 48.51 45.54 44.55 49.50 ReAct Self-Ask Search-O1 35.00 57.25 27.72 52.98 55.44 54.45 40.59 58.41 44.55 32.32 55.94 37.62 40.59 58.41 35.64 54.90 61.38 59.40 43.13 59.40 48.51 41.58 62.37 47.52 42.57 60.91 44.55 54.45 64.35 44. 28.71 56.50 52.98 55.44 40.59 58.41 28.71 56.42 31.68 51.48 54.90 61.38 43.13 59.40 34.31 55.44 57.84 58.41 58.82 62.87 MCTS-RAG 61.38 64.64 68.28 67.32 74.25 74.25 Disable A4&A5 Disable A4 Disable Enable All 55.45 55.70 56.20 56.70 32.32 36.27 44.11 53.20 Analysis of Rollout Numbers 4 rollout (main setting) 8 rollout 12 rollout 16 rollout 61.38 64.35 68.65 71.20 64.64 63.72 75.15 84. 50.40 55.94 62.37 66.50 68.28 68.12 69.35 74.14 Table 1: Answer accuracy of MCTS-RAG and other methods (both with and without retrieval modules). Table 2: Answer accuracy of Qwen2.5-7B-Instruct under different retrieval and rollout settings. on overall performance. In addition, we vary the number of rollouts from 4 to 16 to investigate how deeper search affects accuracy and efficiency. Impact of Different Actions. Retrieval actions, especially A4 and A5, are key for multi-step reasoning. Enabling all retrievals boosts GPQA (+20.88%) and FMT (+16.10%). Disabling A5 improves GPQA (+7.84%) and FMT (+6.43%) over disabling A4, suggesting A4s stronger role. CWQA sees minimal impact (+1.25%). These findings highlight retrieval trade-offs and the importance of recursive evidence aggregation. Impact of Different Rollout Strategies. More rollouts enhance performance, particularly for GPQA. Increasing from 4 to 8 slightly aids CWQA (+3%), while 8 to 12 boosts GPQA (+11%). Scaling to 16 further improves GPQA (+9%) and FMT (+5%), reinforcing the value of iterative reasoning."
        },
        {
            "title": "4.4 Human Analysis and Case Study",
            "content": "To better understand the strengths and limitations of MCTS-RAG, we conduct comprehensive analysis of its successful cases in comparison to baseline methods, along with thorough error analysis. Successful Case Analysis. Our case study reveals the following two key improvements introduced by MCTS-RAG: (1) Enhanced External Knowledge Utilization: Compared to other reasoning methods, MCTS-RAG achieves higher accuracy, primarily due to its richer reasoning space and more effective utilization of external knowledge. Figure 6 clearly illustrates how Monte Carlo Tree Search tightly integrates reasoning and retrieval processes, significantly enhancing the quality and richness of information used during reasoning, thereby substantially improving inference accuracy. (2) Reduced Hallucination Risks: Moreover, MCTS-RAG mitigates hallucination risks through detailed and explicit reasoning steps. On one hand, the explicit reasoning pathways enable the model to more accurately interpret retrieved external knowledge, reducing errors arising from ambiguity or misunderstanding (as illustrated in Figure 7 in Appendix). On the other hand, these thorough reasoning procedures generate clearer and more contextually relevant queries, thus improving the precision of external information retrieval (as illustrated in Figure 8 in Appendix). Consequently, MCTS-RAG demonstrates substantial advantages over traditional reasoning methods in terms of improved accuracy and robustness. Error Case Analysis Our human analysis identifies the following three primary error types in MCTS-RAG: (1) Amplification Error: As illustrated in Figure 3, early retrieval errors in MCTSRAG can be magnified, causing incorrect information to dominate subsequent reasoning and ultimately leading to incorrect final answer. (2) Factual Confusion: We reveal that semantic mismatches between retrieved text and the reasoning process can lead to conflations or hallucinations. Figure 4 presents details on how semantically divergent retrieval results can lead to incorrect final answers. (3) Information Overload: Excessive additional information in MCTS-RAG can cause certain reasoning paths to deviate from the original question, leading to incorrect conclusions. Figure 5 presents detailed example of some reasoning paths that prioritize irrelevant aspects."
        },
        {
            "title": "5 Conclusion",
            "content": "In this work, we propose MCTS-RAG, an approach that integrates Monte Carlo Tree Search with Retrieval-Augmented Generation to improve multi-step reasoning accuracy and reliability. By effectively allocating search and retrieval processes, MCTS-RAG excels at handling cross-domain tasks that require in-depth external knowledge. It not only enables flexible formulation of high-quality retrieval queries but also refines the reasoning path through iterative tree exploration, thus reducing hallucinations caused by shallow retrieval or simplistic reasoning. Experimental results show that MCTSRAG has achieved good results in scenarios such as complex reasoning tasks, knowledge-enhanced scientific question-answering tasks, and challenging fact-checking tasks."
        },
        {
            "title": "Acknowledgments",
            "content": "We are grateful to Google TRC program for providing computing resources and Together AI for granting LLM API credits."
        },
        {
            "title": "Limitations and Future Work",
            "content": "MCTS-RAG integrates MCTS-based reasoning and RAG to enhance reasoning capabilities, but several errors persist. Amplification errors occur when early retrieval mistakes propagate through search iterations. Factual confusion arises from semantic mismatches leading to incorrect reasoning. Information overload happens when excessive retrieval results cause reasoning to deviate from the target. Additionally, search latency remains challenge, as deep MCTS search trees significantly increase reasoning time, particularly with multiple retrieval steps. Action selection complexity arises because the optimal choice among A1-A6 depends on query difficulty, necessitating more adaptive decision mechanism. Inefficient expansion occurs when MCTS explores unnecessary branches due to lack of effective pruning based on retrieval confidence or early error detection. Addressing these issues is essential for improving efficiency and reasoning accuracy. We encourage future work to focus on optimizing search efficiency by developing adaptive action selection strategies, confidence-based retrieval filtering, and error-aware pruning mechanisms to improve MCTS exploration. Additionally, integrating reinforcement learning for dynamic search policy refinement may further enhance reasoning accuracy. Addressing these challenges will contribute to the development of more robust and scalable reasoning models, bridging the gap between retrieval-based methods and human-like problem-solving."
        },
        {
            "title": "References",
            "content": "Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. Self-rag: Learning to retrieve, generate, and critique through self-reflection. In The Twelfth International Conference on Learning Representations. Cameron Browne, Edward Powley, Daniel Whitehouse, Simon Lucas, Peter Cowling, Philipp Rohlfshagen, Stephen Tavener, Diego Perez, Spyridon Samothrakis, and Simon Colton. 2012. survey of monte carlo tree search methods. IEEE Transactions on Computational Intelligence and AI in games, 4(1):143. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. 2021. Training verifiers to solve math word problems. Julian Eisenschlos, Bhuwan Dhingra, Jannis Bulian, Benjamin Börschinger, and Jordan Boyd-Graber. 2021a. Fool me twice: Entailment from Wikipedia gamification. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 352365, Online. Association for Computational Linguistics. Julian Eisenschlos, Bhuwan Dhingra, Jannis Bulian, Benjamin Börschinger, and Jordan Boyd-Graber. 2021b. Fool me twice: Entailment from wikipedia gamification. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Tianyu Fan, Jingyuan Wang, Xubin Ren, and Chao Huang. 2025. Minirag: Towards extremely simple retrieval-augmented generation. Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Meng Wang, and Haofen Wang. 2024. Retrieval-augmented generation for large language models: survey. Shibo Hao, Yi Gu, Haodi Ma, Joshua Hong, Zhen Wang, Daisy Wang, and Zhiting Hu. 2023. Reasoning with language model is planning with world model. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 81548173, Singapore. Association for Computational Linguistics. Shayekh Islam, Md Asib Rahman, KSM Tozammel Hossain, Enamul Hoque, Shafiq Joty, and Md Rizwan Parvez. 2024. Open-rag: Enhanced retrieval augmented reasoning with open-source large language models. In Findings of the Association for Computational Linguistics: EMNLP 2024, pages 14231 14244. Gautier Izacard and Edouard Grave. 2021. Leveraging passage retrieval with generative models for open domain question answering. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 874880, Online. Association for Computational Linguistics. Rolf Jagerman, Honglei Zhuang, Zhen Qin, Xuanhui Wang, and Michael Bendersky. 2023. Query expansion by prompting large language models. arXiv preprint arXiv:2305.03653. Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. 2020. Dense passage retrieval for opendomain question answering. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 67696781, Online. Association for Computational Linguistics. Levente Kocsis and Csaba Szepesvári. 2006. Bandit based monte-carlo planning. In European conference on machine learning, pages 282293. Springer. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al. 2020. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in Neural Information Processing Systems, 33:94599474. Jiatao Li, Xinyu Hu, and Xiaojun Wan. 2024. Smartrag: Selection using determinantal matrices for augmented retrieval. Xiaoxi Li, Guanting Dong, Jiajie Jin, Yuyao Zhang, Yujia Zhou, Yutao Zhu, Peitian Zhang, and Zhicheng Dou. 2025. Search-o1: Agentic search-enhanced large reasoning models. CoRR, abs/2501.05366. Tian Liang, Zhiwei He, Wenxiang Jiao, Xing Wang, Yan Wang, Rui Wang, Yujiu Yang, Shuming Shi, and Zhaopeng Tu. 2024. Encouraging divergent thinking in large language models through multi-agent debate. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 1788917904, Miami, Florida, USA. Association for Computational Linguistics. Xinbei Ma, Yeyun Gong, Pengcheng He, Nan Duan, et al. Query rewriting in retrieval-augmented large language models. In The 2023 Conference on Empirical Methods in Natural Language Processing. Xinbei Ma, Yeyun Gong, Pengcheng He, Hai Zhao, and Nan Duan. 2023. Query rewriting in retrievalaugmented large language models. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 53035315, Singapore. Association for Computational Linguistics. Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al. 2021. Webgpt: Browser-assisted questionanswering with human feedback. arXiv preprint arXiv:2112.09332. Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah Smith, and Mike Lewis. 2023. Measuring and narrowing the compositionality gap in language models. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 56875711, Singapore. Association for Computational Linguistics. Zhenting Qi, Mingyuan Ma, Jiahang Xu, Li Lyna Zhang, Fan Yang, and Mao Yang. 2024. Mutual reasoning makes smaller llms stronger problem-solvers. David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel Bowman. 2023. Gpqa: graduate-level google-proof q&a benchmark. arXiv preprint arXiv:2311.12022. David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel R. Bowman. 2024. GPQA: graduate-level google-proof q&a benchmark. In First Conference on Language Modeling. Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Eric Hambro, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. 2023. Toolformer: Language models can teach themselves to use tools. Advances in Neural Information Processing Systems, 36:6853968551. Zhihong Shao, Yeyun Gong, Minlie Huang, Nan Duan, Weizhu Chen, et al. Enhancing retrievalaugmented large language models with iterative retrieval-generation synergy. In The 2023 Conference on Empirical Methods in Natural Language Processing. Hongjin Su, Howard Yen, Mengzhou Xia, Weijia Shi, Niklas Muennighoff, Han-yu Wang, Haisu Liu, Quan Shi, Zachary Siegel, Michael Tang, Ruoxi Sun, Jinsung Yoon, Sercan Arik, Danqi Chen, and Tao Yu. 2024. Bright: realistic and challenging benchmark for reasoning-intensive retrieval. Alon Talmor and Jonathan Berant. 2018. The web as knowledge-base for answering complex questions. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 641651. Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. 2023. Interleaving retrieval with chain-of-thought reasoning for knowledgeIn Proceedings of intensive multi-step questions. the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1001410037, Toronto, Canada. Association for Computational Linguistics. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed H. Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2023. Self-consistency improves chain of thought reasoning in language models. In The Eleventh International Conference on Learning Representations. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837. Orion Weller, Kathryn Ricci, Eugene Yang, Andrew Yates, Dawn Lawrie, and Benjamin Van Durme. 2025. Rank1: Test-time compute for reranking in information retrieval. Yuxi Xie, Anirudh Goyal, Wenyue Zheng, Min-Yen Kan, Timothy Lillicrap, Kenji Kawaguchi, and Michael Shieh. 2024a. Monte carlo tree search boosts reasoning via iterative preference learning. arXiv preprint arXiv:2405.00451. Yuxi Xie, Kenji Kawaguchi, Yiran Zhao, James Xu Zhao, Min-Yen Kan, Junxian He, and Michael Xie. 2024b. Self-evaluation guided beam search for reasoning. Advances in Neural Information Processing Systems, 36. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas Griffiths, Yuan Cao, and Karthik Narasimhan. 2023a. Tree of thoughts: deliberate In problem solving with large language models. Proceedings of the 37th International Conference on Neural Information Processing Systems, pages 1180911822. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. 2023b. React: Synergizing reasoning and acting in language models. In International Conference on Learning Representations (ICLR). Tian Yu, Shaolei Zhang, and Yang Feng. 2024. Auto-rag: Autonomous retrieval-augmented generation for large language models. arXiv preprint arXiv:2411.19443. Dan Zhang, Sining Zhoubian, Ziniu Hu, Yisong Yue, Yuxiao Dong, and Jie Tang. 2024. Rest-mcts*: Llm self-training via process reward guided tree search. arXiv preprint arXiv:2406.03816."
        },
        {
            "title": "A Prompts for Each Action",
            "content": "A1 (Direct Response) Template: chat between curious user and an AI assistant. gives step-by-step solutions to the users questions. In the end of the assistants response, final answer must be given in the format of \"The answer is: answer. <ANSWER>.\", where <ANSWER> should be concise"
        },
        {
            "title": "The assistant",
            "content": "Usage Example: {examples} Instruction: {instruction} Note: Please answer in complete sentence. A2 (One-Step Reasoning) Template: chat between curious user and an AI assistant. gives step-by-step solutions to the users questions with each step numbered. the format \"The answer is: concise answer. At the final step, conclusive answer must be given in <ANSWER>.\", where <ANSWER> should be The assistant Instruction: {instruction} Note: Lets think step by step. A3 (Decompose Answer) Template: Given question, decompose it into sub-questions. sub-question, provide an answer in one complete sentence ending with \"The answer is \". the sub-question with \"Now we can answer the question: question>\". When the original question is answerable, start <original For each A4 (Transform Retrieve Query) Template: Given question, generate search query that would help gather information to answer it. retrieves useful evidence or additional details relevant to the question. search results are both relevant and helpful. one complete sentence, starting with \"The query is: query>\"."
        },
        {
            "title": "Please answer in",
            "content": "<your retrieve Question: {question} A5 (Reflect Retrieved Knowledge) Template: chat between curious user and an AI assistant. evaluates whether the retrieved information is relevant to the search query and sufficient to answer the question. concise evaluation in one complete sentence, starting with \"Evaluation:\"."
        },
        {
            "title": "Please provide",
            "content": "Instruction: Please assess if the retrieved information is related to the query and can be used to answer the question. A6 (Summarize Answers) Template: Analyze the provided Knowledge and extract key information relevant to the Original Question. organized format."
        },
        {
            "title": "Present your analysis in a concise and",
            "content": "Input: - Original Question: {original_question} - Knowledge: {retrieved_context} Output Format: Point 1: Key Points: information; Point 3: Relevant information; Point 2: Relevant Relevant information... Requirement: The output must be single line summarizing all key points in one sentence without redundant description."
        },
        {
            "title": "B Error Analysis",
            "content": "Figure 3: An illustration of MCTS Amplification Error. Early MCTS retrieval errors amplify mistakes, leading to final answer favoring incorrect paths. Figure 4: An illustration of Factual Confusion. Wrong understanding of the relationship between project launch and moon landing, leading to wrong answers. Figure 5: An illustration of Information Overload. Too much coastline information, resulting in the model answering the coastline length instead of the capital city. Figure 6: Illustration of how MCTS-RAG achieves rich reasoning space and tightly integrates reasoning with retrieval. Figure 7: Illustration of the effectiveness of MCTS-RAG. How further reasoning reduces retrieval-introduced hallucinations and improves accuracy. Figure 8: An illustration of the effectiveness of MCTSRAG. Based on clear chain of reasoning, it can generate higher quality retrieval queries and final answers, reduce hallucinations and improve accuracy. Figure 9: An illustration of standard RAG. Because the reasoning process is not clear enough, the final answer to the question is an illusion and the answer is wrong."
        }
    ],
    "affiliations": [
        "New York University",
        "Yale University"
    ]
}