{
    "paper_title": "Training-Free Tokenizer Transplantation via Orthogonal Matching Pursuit",
    "authors": [
        "Charles Goddard",
        "Fernando Fernandes Neto"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We present a training-free method to transplant tokenizers in pretrained large language models (LLMs) by reconstructing unseen token embeddings via Orthogonal Matching Pursuit (OMP). Specifically, we approximate each out-of-vocabulary token as a sparse linear combination of shared tokens, in two phases: first, compute each new token's representation in the donor embedding space with a small dictionary of shared anchor tokens, then transfer these same sparse coefficients back into the base model's embedding space. On two challenging cross-tokenizer tasks--Llama$\\to$Mistral NeMo (12B) and Qwen$\\to$Llama (1B)--we show that OMP achieves best zero-shot preservation of the base model's performance across multiple benchmarks, while other zero-shot approaches degrade significantly. Compared to baselines (zero-init, mean-init, and existing approaches like WECHSEL, FOCUS, ZETT), OMP consistently achieves the best overall performance, effectively bridging large tokenizer discrepancies without gradient updates. Our analysis further identifies mismatched numerical tokenization schemes as a critical challenge for preserving mathematical reasoning capabilities. This technique enables direct reuse of pretrained model weights with new tokenizers, facilitating cross-tokenizer knowledge distillation, speculative decoding, ensembling, merging, and domain-specific vocabulary adaptations. We integrate our method into the open-source mergekit-tokensurgeon tool for post hoc vocabulary realignment."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 ] . [ 1 7 0 6 6 0 . 6 0 5 2 : r Training-Free Tokenizer Transplantation via Orthogonal Matching Pursuit"
        },
        {
            "title": "Charles Goddard and Fernando Fernandes Neto",
            "content": "{charles,fernando}@arcee.ai Abstract We present training-free method to transplant tokenizers in pretrained large language models (LLMs) by reconstructing unseen token embeddings via Orthogonal Matching Pursuit (OMP). Specifically, we approximate each out-of-vocabulary token as sparse linear combination of shared tokens, in two phases: first, compute each new tokens representation in the donor embedding space with small dictionary of shared anchor tokens, then transfer these same sparse coefficients back into the base models embedding space. On two challenging cross-tokenizer tasksLlamaMistral NeMo (12B) and QwenLlama (1B)we show that OMP achieves best zero-shot preservation of the base models performance across multiple benchmarks, while other zero-shot approaches degrade significantly. Compared to baselines (zero-init, mean-init, and existing approaches like WECHSEL, FOCUS, ZETT), OMP consistently achieves the best overall performance, effectively bridging large tokenizer discrepancies without gradient updates. Our analysis further identifies mismatched numerical tokenization schemes as critical challenge for preserving mathematical reasoning capabilities. This technique enables direct reuse of pretrained model weights with new tokenizers, facilitating cross-tokenizer knowledge distillation, speculative decoding, ensembling, merging, and domain-specific vocabulary adaptations. We integrate our method into the open-source mergekit-tokensurgeon tool for post hoc vocabulary realignment."
        },
        {
            "title": "Introduction",
            "content": "Large language models (LLMs) are typically constrained by the tokenizer chosen during pretraining, which defines fixed vocabulary for processing text. However, these vocabularies are not universal; they may sub-optimally tokenize other languages, dialects, or domains [31, 9]. While replacing models tokenizer post-training is highly desirable in many scenarios, doing so without performance degradation remains formidable technical challenge. The core difficulty lies in embedding initialization for new tokens. Naive initialization can significantly degrade performance [26, 12], while continuing pretraining to learn new embeddings can be prohibitively expensive. [4, 2] Meanwhile, existing zero-shot heuristics yield uneven results and can cause large performance drops on tasks like question answering or reasoning [9, 16], especially when fundamental representational structures, like numerical tokenization schemes, differ between models. This vocabulary mismatch creates practical barriers in several important scenarios. In speculative decoding [22, 1], small model proposes partial outputs for larger model to verify, requiring 1 aligned tokenizers for direct logit comparison. Knowledge distillation [17] similarly requires matched vocabularies when transferring token-level supervision from teacher to student [3]. Model ensembling or merging [13] presupposes that tokens map to the same embedding dimensions, with different tokenizers blocking direct output combination. An effective method for post hoc tokenizer transferwithout retrainingwould unlock these capabilities, greatly expanding the uses for existing models. In this paper, we propose fully training-free approach to tokenizer transplantation, where we reconstruct embeddings for the new vocabulary in the base models embedding space using Orthogonal Matching Pursuit (OMP). Our key insight is to represent each new tokens donor embedding as sparse combination of shared token embeddings. Transplantation then becomes straightforward: we replicate those same sparse coefficients in the base models embedding space. This yields new vocabulary embedding matrix aligned with the new tokenizer but consistent with the base model embedding geometry. We evaluate OMP-based transplantation in two cross-tokenizer experiments: LlamaMistral NeMo (12B) and QwenLlama (1B). In both scenarios, OMP preserves model performance across classification, reasoning, and perplexity benchmarks far better than simple heuristics (zero, mean) or other zero-shot token-initialization approaches. We integrate this strategy into the mergekit-tokensurgeon open-source toolkit, enabling immediate usage in cross-tokenizer knowledge distillation, speculative decoding pipelines, or domain-vocabulary expansions, with no retraining required. Contributions. 1. We propose training-free tokenizer transplant method using Orthogonal Matching Pursuit in embedding space. 2. We demonstrate superior zero-shot performance over existing approaches, evaluated on LlamaMistral NeMo and QwenLlama tasks. 3. We identify and analyze the significant negative impact of mismatched numerical tokenization schemes on mathematical reasoning performance, highlighting key limitation of zero-shot transplantation. 4. We highlight how OMP-based transplantation resolves key bottlenecks in cross-tokenizer use cases (e.g., teacher-student distillation, speculative decoding, or domain expansions). 5. We provide an efficient incremental-QR OMP implementation and release it in mergekit-tokensurgeon for reproducibility."
        },
        {
            "title": "2 Related Work",
            "content": "Embedding Initialization for New Vocabularies. Assigning embeddings to new or unseen tokens is well-explored problem in model adaptation. Naive (random/zero) initialization typically degrades performance substantially [16]. More sophisticated approaches often compute embeddings by averaging or combining subword pieces from the old vocabulary [26, 12]. For example, Minixhofer et al. [26] propose WECHSEL, which uses bilingual word embeddings to map subword pieces from one language to another, then continues pretraining to refine the new embeddings. Similarly, Dobler and de Melo [9] (FOCUS) and Ostendorff and Rehm [28] (CLPTransfer) base their initialization on overlapping tokens, weighting them to approximate the new tokens distribution. These methods yield better initial embeddings but often still rely on some continued training to recover the original 2 model performance. In contrast, our method requires no further training for high quality result. Additionally, OMP produces signed coefficients and therefore explores larger linear subspace instead of restricting to the convex hull of anchors. Zero-Shot Tokenizer Transfer. ZeTT [27] trains hypernetwork for arbitrary tokenizers, applicable zero-shot after meta-training. Other tools [18, 13] copy or average existing embeddings. Our work aligns with these methods goals of minimal or no additional training, but uses Orthogonal Matching Pursuit (OMP) to anchor new token embeddings in sparse set of existing embeddings. This approach yields more accurate approximations, especially with large vocabulary differences, and does not require meta-trained hypernetwork. Sparse Approximation Techniques. Orthogonal Matching Pursuit (OMP) [30] is standard technique for sparse reconstruction, used widely in signal processing and dictionary learning. While it has seen limited use in NLP[24, 32], it has not to our knowledge been adopted for cross-tokenizer embedding transfer. Our approach is the first to apply OMP as solution to cross-tokenizer embedding alignment."
        },
        {
            "title": "3.1 Problem Setup and Notation\nLet Mbase be a pretrained base model with vocabulary Vbase and embedding matrix E(base) ∈\nR|Vbase|×dbase . We want to replace its vocabulary with Vdonor from another model Mdonor, which has\nan embedding matrix E(donor) ∈ R|Vdonor|×ddonor. Note that ddonor may differ from dbase. We denote\nV∩ = Vbase ∩ Vdonor as the set of shared (overlapping) tokens.",
            "content": "Case 1: Shared Tokens. If V, we simply copy e(base) to the new embedding matrix E(new). Case 2: Unseen Tokens. When / Vbase, we have donor embedding e(donor) sponding e(base) vocabularies. In other words: but no correby referencing anchor tokens that exist in both . We must approximate e(base) e(donor) (cid:88) jA αj e(donor) , V, k, and we then place e(new) = (cid:88) αj e(base) . jA Here, is chosen sparsity level (e.g., 8, 32, 64), and is the small set of anchor tokens chosen by an OMP solver in the donors embedding space. By applying those coefficients in the base models space, we create the new embedding e(new) . As only the unit-agnostic coefficient vector α is transferred, there is no dependence on matching embedding dimensionality. 3 RdV, target = e(donor) Rd, sparsity k. (cid:3) jV Algorithm 1 OMP for donor-space approximation Require: Dictionary Φ = (cid:2)e(donor) Ensure: set of indices Λ and sparse coeffs RV. 1: Initialize Λ , v. 2: for = 1 . . . do (cid:12) (cid:12) λ arg max (cid:12) (cid:12) (cid:12)r, ϕj 3: (cid:12) jV Λ Λ {λ} Solve xΛ = arg minxΛ ΦΛxΛ2 2 ΦΛxΛ 4: 5: 6: 7: end for 8: return (Λ, x) where is zero outside Λ"
        },
        {
            "title": "3.2 Orthogonal Matching Pursuit",
            "content": "Orthogonal Matching Pursuit [30] is greedy algorithm that builds sparse representation of target vector with respect to columns in dictionary Φ. In our setting: We want = e(donor) Rddonor, Φ = (cid:2)e(donor) (cid:3) RddonorV. jV min xRV Φx2 subject to x0 k. OMP iteratively selects the dictionary column most correlated with the current residual, updates the partial least-squares solution, and refines the residual. After steps, we have small set of anchor indices Λ plus nonzero coefficients xΛ. Algorithm 1 shows the procedure. Efficient Implementation with Incremental QR Decomposition. Solving least-squares problem each iteration can be expensive. We adopt an incremental QR factorization of ΦΛ to update solutions more efficiently. Rather than solving the least-squares problem from scratch at each iteration, we maintain QR decomposition of the chosen columns of Φ and update it incrementally as we add new columns. (the Given Φt1 = Qt1Rt1, when we add column ϕλt component orthogonal to existing columns); (2) normalize to get qt = ˆq/ˆq2; and (3) extend Qt1 with qt and update Rt1 accordingly. For numerical stability we re-orthogonalize at fixed intervals, but note that the atom selection of OMP is itself well-suited to avoiding catastrophic cancellationresults with and without re-orthogonalization differ only up to floating point precision. This approach reduces the per-iteration complexity from O(td2) to O(td), making the algorithm , we: (1) compute ˆq = ϕλt Qt1QT t1ϕλt practical for large numbers of anchor points. Forming the Base Embedding. After obtaining the sparse coefficients xΛ, we place e(new) = (cid:88) xj e(base) . jΛ We repeat this for all unseen tokens / Vbase, while shared tokens simply copy their existing base embeddings. Finally, we replace the original embedding matrix E(base) with E(new), ensuring the model now matches the donor tokenizers vocabulary and ID mapping."
        },
        {
            "title": "3.3 Geometric Justification via Approximate Orthogonal Alignment",
            "content": "Our methods rationale relies on two principles from embedding alignment and sparse recovery: Independently trained embedding spaces (e.g., for Approximate Orthogonal Equivalence. words or LLM tokens [20, 21]) have been shown to often align via an approximately orthogonal transformation Rdbaseddonor (where Idbase ) on their shared subspace [7]. Thus, for V, e(donor) , meaning applied to linear combinations of donor embeddings approximates the same combinations in base space. , Iddonor e(base) Sparse Reconstruction Stability. Orthogonal Matching Pursuit (OMP) [30, 33] reliably reconstructs target vector from few dictionary atoms if the dictionary exhibits mild incoherence or Restricted Isometry Properties. If e(donor) , OMP finds {αj} with provably small error e(donor) jA αj e(donor) jA αj e(donor) (cid:80) (cid:80) 2. j Putting It Together. Combining these, if e(donor) (cid:80) jA αj e(donor) , applying yields: e(donor) (cid:88) jA αj e(donor) (cid:88) jA αj e(base) = e(new) . Transplanting OMP-derived coefficients into the base models embedding space thus implicitly performs least-squares projection and an approximate orthogonal alignment. This provides conceptual justification for our methods effectiveness, and though we do not prove (or seek to prove) this alignment universally exists, empirical results suggest it holds in practice."
        },
        {
            "title": "4 Experiments and Results",
            "content": "We evaluate our OMP-based approach on two cross-tokenizer settings: LlamaMistral NeMo (12B): Llama 3s [14] 128k vocabulary transplanted into 12B Mistral NeMo, which has 131k tokens. Overlap: 71k tokens ( 54%). QwenLlama (1B): Qwen 2.5s [34] 152k vocabulary transplanted into 1B-parameter Llama 3 model with 128k vocabulary. Overlap: 110k tokens ( 86%). In each case, we compare: 1. OMP (our method) at various in {8, 16, 32, 64, 256}. 2. ZeroEmbed : all unseen tokens get zero vectors. 3. MeanEmbed : all unseen tokens get the mean embedding of the base vocabulary. 4. Published approaches (ZETT, FOCUS, WECHSEL, CLPTransfer) applied strictly zero-shot (i.e., no additional training) using the official released implementations. We evaluate perplexity on WikiText, plus classification or QA accuracy on MMLU [15], ARC [5], GSM8K [6], LAMBADA [29], XNLI [8], and Paws-X [35], among others, using Eleuther AIs LM Evaluation Harness [11]. For published baselines (such as ZETT, FOCUS, WECHSEL, and CLPTransfer), 5 we applied only their proposed embedding initialization techniques, omitting any subsequent finetuning or continued pre-training steps suggested in their original methodologies, to ensure direct comparison of zero-shot initialization quality. We additionally compare post-training results given identical token budgets in Section 4.5. Table 1 summarizes the evaluation settings and typical standard errors (stderr) associated with the primary metric for key benchmarks, as computed by the evaluation harness via bootstrapping [11]. These errors quantify uncertainty stemming from the finite size of the evaluation dataset sample. Since the standard errors were consistently small relative to the performance differences observed between methods and did not affect the relative rankings or main conclusions, we omit them from the main results tables (Tables 2, 3, 4, 5) for clarity. Table 1: Evaluation settings and representative standard errors (stderr) for key benchmarks."
        },
        {
            "title": "Metric",
            "content": "Few-shot"
        },
        {
            "title": "StdErr",
            "content": "MMLU ARC Challenge GSM8K XNLI AGIEval Lambada (EN) WikiText acc (agg.) acc_norm exact_match (flex) acc (agg.) acc (agg.) acc bits_per_byte 0 0 5 0 0 0 0 0.004 0.014 0.013 0.003 0.005 0.006 N/A* * LM Eval Harness does not report bootstrapped stderr for perplexity metrics. (flex) refers to flexible answer extraction; (agg.) is aggregated score."
        },
        {
            "title": "4.1 Llama→Mistral NeMo (12B)",
            "content": "In LlamaMistral NeMo, the baseline NeMo model has strong MMLU accuracy (64.5%). Table 2 shows that OMP with = 64 preserves MMLU at 62.2% (-3.6%), while naive baselines drop to 58% (-9.3%). While all zero-shot transplants significantly degrade mathematical tasks, OMP remains the most consistent overall."
        },
        {
            "title": "4.2 Qwen→Llama (1B) Results",
            "content": "The QwenLlama transplantation presents an interesting scenario due to its very high token overlap ( 86%), which includes almost all English tokens relevant to benchmarks like MMLU and ARC. Table 3 shows key metrics. Here the resilience of ZeroEmbed and MeanEmbed suggests key factor is avoiding interference with these shared embeddings by new token initializations. OMP navigates this high-overlap scenario adeptly. It preserves near-baseline performance for MMLU (36.40% vs. 36.73% baseline) and ARC (36.26% vs. 36.26% baseline), and performs strongly on Belebele, with only slight increase in perplexity on WikiText (0.7159 vs. 0.6605 bits/byte). This performance outperforms all simpler heuristics. For GSM8K, again all zero-shot methods degrade severely; OMP is still competitive. 6 Table 2: LlamaMistral NeMo results. Relative performance changes shown in parentheses."
        },
        {
            "title": "Model",
            "content": "MMLU ARC-C XNLI GSM8K AGIEval Lambada WikiText (EN) (Bits/Byte) Baseline (Mistral NeMo) 0.6452 0. 0.4367 0.5588 0.3752 0.7819 0.5296 OMP-K OMP-K32 OMP-K"
        },
        {
            "title": "ZeroEmbed",
            "content": "MeanEmbed 0.6129 (-5.00%) 0.6158 (-4.56%) 0.6222 (-3.57%) 0.5239 (-9.84%) 0.5273 (-9.25%) 0.5179 0.3782 (-10.87%) (-13.39%) (-76.93%) 0.3780 0. 0.1289 (-13.44%) (-76.80%) 0.3780 0.1463 (-13.43%) (-73.81%) 0. 0.2585 0.3286 0.0076 (-64.37%) (-55.51%) (-24.74%) (-98.64%) 0.5852 (-9.29%) 0.5936 (-8.00%) 0. 0.0334 0.3689 (-14.83%) (-15.53%) (-94.03%) 0.3693 (-14.54%) (-15.43%) (-90.64%) 0.0523 0.4966 0.3137 (-16.38%) 0.3146 (-16.15%) 0.3158 (-15.83%) 0.2447 (-34.78%) 0.2905 (-22.57%) 0.2952 (-21.31%) 0.6897 (-11.79%) 0.6905 (-11.69%) 0.6895 (-11.81%) 0.0000 (-100.00%) 0.6598 (-15.61%) 0.6670 (-14.69%) 0.7798 (+47.24%) 0.8048 (+51.95%) 0.7417 (+40.05%) 5.8441 (+1003.47%) 0.9881 (+86.58%) 0.8993 (+69.81%)"
        },
        {
            "title": "4.3 Effect of k.",
            "content": "We examined how the sparsity parameter affects performance across benchmarks. Our experiments tested values ranging from = 8 to = 256. In the QwenLlama experiment, we found minimal gains beyond = 16, with performance plateauing for most tasks around = 3264. For instance, MMLU scores were 36.41%, 36.40%, and 36.41% for = 8, = 32, and = 256, respectively. In the LlamaMistral NeMo case, we observed more substantial improvements with higher values, particularly in mathematical tasks like GSM8K (12.89% at = 8 vs. 14.63% at = 64). However, increasing beyond = 64 yielded diminishing returns. We recommend = 64 as offering the best balance of performance and efficiency."
        },
        {
            "title": "4.4 Numerical Performance Degradation.",
            "content": "A striking observation across both the LlamaMistral NeMo  (Table 2)  and QwenLlama  (Table 3)  experiments is the dramatic degradation in mathematical reasoning performance, particularly on GSM8K (drops of -73.8% and -78.7% for OMP-K64, respectively, compared to baselines). We hypothesize this stems from fundamental differences in numerical tokenization schemes. Mistral NeMo and Qwen employ single-digit tokenization (e.g., \"1234\" \"1\", \"2\", \"3\", \"4\"), resulting in only 10 base numeric tokens. In contrast, Llama 3 uses triplet-based chunking (e.g., \"1234\" \"123\", \"4\"), leading to vastly larger numerical vocabulary (1110 dedicated number tokens). During transplantation between models with mismatched schemes (like QwenLlama or LlamaMistral NeMo), the vast majority of one models numeric tokens lack direct equivalents and must be approximated via OMP using potentially non-numeric anchors. This likely introduces systematic distortions in the models learned numerical representations and operations, which may rely on specific geometric structures tied to the pretraining tokenization scheme. For example, if models represent numbers on generalized helix [19] structure sensitive to the sequence of numeric tokens, 7 Table 3: QwenLlama (1B) tokenizer-transplant performance."
        },
        {
            "title": "Model",
            "content": "MMLU ARC-C GSM8K Belebele WikiText (Bits/Byte) Baseline (Llama) 0.3673 0.3626 0. 0.2813 0.6605 OMP-K8 OMP-K32 OMP-K"
        },
        {
            "title": "FOCUS",
            "content": "WECHSEL ZETT ZeroEmbed MeanEmbed 0.3641 (-0.85%) 0.3640 (-0.89%) 0.3640 (-0.89%) 0.3626 (+0.00%) 0.3626 (+0.00%) 0.3626 (+0.00%) 0.0144 (-78.65%) 0.0144 (-78.65%) 0.0144 (-78.65%) 0.0174 (-74.16%) 0.0099 (-85.39%) 0.0000 0.0000 0.2295 0.2858 (-37.52%) (-21.18%) (-100.00%) 0.3549 (-2.12%) 0.2389 0.2695 (-26.62%) 0. (-36.98%) (-34.12%) 0.2634 0.3055 (-28.27%) (-15.76%) (-100.00%) 0.3626 (+0.00%) 0.3626 (+0.00%) 0.0144 (-78.65%) 0.0144 (-78.65%) 0.3640 (-0.89%) 0.3641 (-0.85%) 0.2687 (-4.46%) 0.2693 (-4.25%) 0.2704 (-3.86%) 0.2299 (-18.26%) 0.2478 (-11.91%) 0.2470 (-12.17%) 0.2480 (-11.82%) 0.2648 (-5.87%) 0.2577 (-8.38%) 0.7160 (+8.40%) 0.7160 (+8.39%) 0.7159 (+8.38%) 6791.6044 (+1e6%) 0.8870 (+34.29%) 3.1852 (+382.24%) 1.1426 (+72.99%) 0.7169 (+8.53%) 0.7180 (+8.70%) then approximating embeddings across mismatched schemes (like reconstructing Llamas triplet 123 using Qwens 1, 2, 3 anchors via OMP) could fail to preserve this geometric arrangement and thus impair arithmetic capabilities. Tellingly, transplanted models in these mismatched scenarios frequently produce single-digit answers to multi-digit problems, suggesting that mathematical reasoning capabilities are tightly coupled to the specific tokenization patterns encountered during pretraining. To validate this hypothesis, we conducted an additional experiment transplanting the Mistral NeMo tokenizer into the Qwen 2.5 7B model. Crucially, both models utilize the same single-digit numerical tokenization scheme, although their overall vocabularies differ. As shown in Table 4 the GSM8K performance drop for OMP-K64 in this Mistral NeMoQwen transplantation was only -5.6% (from 82.6% to 78.0%). This contrasts sharply with the >70% degradation observed when numeric schemes were mismatched. This result strongly supports our hypothesis: when numeric tokenization schemes are aligned, OMP successfully preserves mathematical performance to much greater degree. While OMP effectively bridges semantic representations for general text tokens, the structural differences in representing numbers pose unique challenge that significantly impacts arithmetic and quantitative reasoning unless the underlying numeric tokenization strategy is preserved or highly similar. Specialized handling of numeric tokens during transplantation may be required to fully retain mathematical abilities across arbitrary tokenizer pairs."
        },
        {
            "title": "Model",
            "content": "Baseline (Qwen) OMP-K"
        },
        {
            "title": "ZeroEmbed",
            "content": "Table 4: Benchmark results for Mistral NeMoQwen MMLU ARC-C XNLI GSM8K AGIEval Lambada WikiText (EN) (Bits/Byte) 0.7196 0.5137 0. 0.8264 0.5639 0.7173 0.5847 0.7064 (-1.83%) 0.6839 (-4.95%) 0.6874 (-4.46%) 0.4872 (-5.15%) 0.4693 (-8.64%) 0.4804 (-6.48%) 0.3769 (-13.22%) 0.3560 (-18.04%) 0.3542 (-18.47%) 0.7801 (-5.60%) 0.7900 (-4.40%) 0.7779 (-5.87%) 0.4423 (-21.56%) 0.4288 (-23.96%) 0.4278 (-24.13%) 0.2456 (-56.44%) 0.6396 (-10.82%) 0.6309 (-12.04%) 0.6299 (-12.18%) 0.6781 (+15.97%) 0.7524 (+28.69%) 0.7722 (+32.07%) 0.0000 (-100.00%) 5.8987 (+908.82%)"
        },
        {
            "title": "CLPTransfer",
            "content": "0.2297 0.2474 0.3326 0.0038 (-68.08%) (-51.83%) (-23.44%) (-99.54%)"
        },
        {
            "title": "4.5 Continued Pre-Training",
            "content": "Although our approach does not require additional training, we note that partial fine-tuning or domain adaptation can boost performance further. Our experiments confirm that even small amount of continued training helps recover performance on sensitive tasks (like GSM8K), but zero-shot OMP alone already outperforms other zero-shot heuristics. The models were fine-tuned for single epoch on two-billion token subset of the DCLM [23] dataset. All baseline models were tuned with the AdamW optimizer and learning rate of 5e-6. OMP required much lower learning rate of 4e-7. This might suggest that the OMP-initialized embeddings are already well-positioned and sensitive to larger updates. The results of this experiment are shown in Table 5. We note the unexpected slight degradation of OMP on XNLI post-CPT. While the exact cause is unclear, given XNLIs cross-lingual nature and the English-only CPT dataset, this might suggest that CPT subtly disrupted the initial zero-shot alignment for this specific task, potentially due to the sensitivity of the OMP-initialized embeddings (as evidenced by the required lower learning rate)."
        },
        {
            "title": "4.6 Computational Efficiency.",
            "content": "Beyond performance metrics, the practical utility of tokenizer transplantation depends on its computational cost. We measured the approximate execution time for the QwenLlama (1B) task, involving the approximation of 41,000 tokens. OMP proves highly efficient: on single H100 GPU, it required only 38 seconds (with = 8) and 74 seconds (with = 32). Naive methods like ZeroEmbed and MeanEmbed had negligible computational cost (effectively instantaneous on CPU). In contrast, other non-trivial zero-shot approaches were significantly more demanding: CLPTransfer took approximately 9 hours on CPU, and FOCUS required similar duration (exact time not recorded but observed to be comparable). ZeTT involves substantial one-time meta-training cost (39 hours on an 8x H100 GPU node in our setup) followed by fast per-model application step (2 minutes on single H100 GPU for this task). This comparison highlights OMPs advantage as truly post hoc, lightweight solution for rapid deployment without extensive pre-computation or runtimes. 9 Table 5: Performance comparison of tokenizer transplantation methods for the QwenLlama pair with and without continued pretraining on the DCLM dataset. Percentages in gray show relative performance change compared to the baseline model. Lower values are better for WikiText (Bits/Byte); higher values are better for all other metrics."
        },
        {
            "title": "Baseline",
            "content": "Zero-Shot"
        },
        {
            "title": "FOCUS",
            "content": "OMP-K64 0.3673 0.4086 0.2690 0.2634 (-28.3%) 0.2695 (-26.6%) 0.3640 (-0.9%) 0.3437 (-15.9%) 0.3578 (-12.4%) 0.3430 (-16.1%) 0.2487 (-7.6%) 0.2577 (-4.2%) 0.2605 (-3.1%) 0.2638 (-1.9%) 0.2579 (-4.1%) 0.2604 (-3.2%) With Continued Pre-Training ZETT-DCLM-2B FOCUS-DCLM-2B 0.3151 (-14.2%) 0.3444 (-6.2%) OMP-K64-DCLM-2B 0.3725 (+1.4%) 0.3784 (-7.4%) 0.3750 (-8.2%) 0.3388 (-17.1%) (EN) 0.6204 0.3460 (-44.2%) 0.5936 (-4.3%) 0.6200 (-0.1%) 0.6115 (-1.4%) 0.6198 (-0.1%) 0.6185 (-0.3%) (Bits/Byte) 0.2813 0.6605 0.2480 (-11.8%) 0.2478 (-11.9%) 0.2704 (-3.9%) 0.2725 (-3.1%) 0.2559 (-9.0%) 0.2724 (-3.2%) 1.1426 (+73.0%) 0.8870 (+34.3%) 0.7159 (+8.4%) 0.6819 (+3.2%) 0.6733 (+1.9%) 0.6730 (+1.9%)"
        },
        {
            "title": "5 Discussion",
            "content": "Why Does OMP Work Well? Word-embedding spaces exhibit approximate local linearity and analogical structure [25] and large-language-model embeddings form tight semantic clusters in high dimensions [10]. OMP leverages both facts: by greedily selecting signed, k-sparse code of shared anchor embeddings, it captures the dominant semantic directions of an unseen token without any gradient updates. Applications. 1. Cross-tokenizer knowledge distillation. Teacher and student often differ in vocabulary; by transplanting the teachers tokenizer onto the student, we can directly apply cross-entropy or logit-based distillation with matched token IDs. 2. Speculative decoding. [22] smaller draft model must share vocabulary with the larger verifier model. OMP-based transplantation allows arbitrary pairs of models to be used regardless of original vocabulary. 3. Domain vocabulary expansions. Instead of full replacement, OMP can be used to initialize embeddings for new domain-specific tokens (e.g., medical, chemical, code, or multilingual) by reconstructing them from existing anchors. This adds minimal overhead and preserves existing performance. Limitations & Future Work. Our approach depends on having at least some overlap in V; if none exists, the method cannot be appliedthough as any modern byte-level or Unicode-complete tokenizer assigns some ID to every code-point sequence, in practice > 0. Structurally different numeric tokenization schemes (such as digit clustering or right-to-left vs. left-to-right segmentation) may degrade math tasks significantly. Building on our finding of this critical limitation, key direction for future work is the development of hybrid transplantation strategies. Such strategies would leverage OMP for general vocabulary while employing specialized handling for numeric tokens specifically designed to bridge these structural representational gaps. Other avenues include bridging strategies for near-disjoint vocabularies or applying other sparse coding techniques."
        },
        {
            "title": "Broader Impacts and Ethical Considerations",
            "content": "The primary motivation behind our work is to enhance the flexibility and reusability of pretrained language models for beneficial applications such as improved knowledge distillation, efficient speculative decoding, and domain-specific adaptations. By enabling training-free tokenizer transplantation, we aim to lower technical barriers and promote innovation in these areas. However, as with any technology that increases the ease of model modification and interoperability, there are potential negative societal impacts to consider. While our method does not inherently create new malicious capabilities within model, it could inadvertently lower the technical or computational barriers for adapting existing models for unintended or harmful purposes. For instance: Facilitating Adaptation of Problematic Models: If base model possesses capabilities that could be misused (e.g., generating sophisticated disinformation, exhibiting strong biases, or producing unsafe content), our technique could make it more efficient for malicious actors to adapt such model to new vocabularies or integrate it into harmful pipelines. The training-free nature reduces the cost and expertise typically associated with such re-tokenization efforts. Propagation of Biases and Harms: The transplantation process directly transfers learned representations. If the base model contains unmitigated biases or safety flaws, these could be seamlessly propagated when its vocabulary is altered, potentially affecting new languages or domains targeted by the transplanted tokenizer if not carefully evaluated. It must be emphasized that the ethical responsibilities associated with the deployment of LLMs remain with the developers and users. Our tool is component that operates on existing models, and the onus is on the user to ensure that the base models are used responsibly and that any model resulting from tokenizer transplantation is thoroughly evaluated for safety, fairness, and its intended application before deployment. We advocate for continued research into robust evaluation techniques and safeguards for all language models, regardless of their tokenization scheme."
        },
        {
            "title": "6 Conclusion",
            "content": "We introduce training-free approach for tokenizer transplantation using Orthogonal Matching Pursuit on shared-token embeddings. Our experiments show that OMP preserves perplexity and classification accuracy far better than naive heuristics, enabling convenient reuse of pretrained LLM weights under new tokenizers. This approach unlocks crucial applications (cross-tokenizer distillation, speculation, domain expansions) by eliminating vocabulary mismatches without any additional training. Our implementation is openly available in mergekit-tokensurgeon, inviting 11 broader adoption and future enhancements. By highlighting the power of sparse approximation in embedding space, we hope to inspire further modular, post hoc enhancements for pretrained LLMs."
        },
        {
            "title": "References",
            "content": "[1] Zachary Ankner, Rishab Parthasarathy, Aniruddha Nrusimha, Christopher Rinard, Jonathan Ragan-Kelley, and William Brandon. Hydra: Sequentially-dependent draft heads for medusa decoding. arXiv preprint arXiv:2402.05109, 2024. [2] Mikel Artetxe, Sebastian Ruder, and Dani Yogatama. On the cross-lingual transferability of monolingual representations. In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel Tetreault, editors, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 46234637, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.421. URL https://aclanthology.org/2020.acl-main.421/. [3] Nicolas Boizard, Kevin El Haddad, Céline Hudelot, and Pierre Colombo. Towards cross-tokenizer distillation: the universal logit distillation loss for llms. arXiv preprint arXiv:2402.12030, 2024. [4] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901, 2020. [5] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018. [6] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. [7] Alexis Conneau, Guillaume Lample, MarcAurelio Ranzato, Ludovic Denoyer, and Hervé Jégou. Word translation without parallel data, 2018. URL https://arxiv.org/abs/1710.04087. [8] Alexis Conneau, Guillaume Lample, Ruty Rinott, Adina Williams, Samuel Bowman, Holger Schwenk, and Veselin Stoyanov. Xnli: Evaluating cross-lingual sentence representations. arXiv preprint arXiv:1809.05053, 2018. [9] Konstantin Dobler and Gerard de Melo. FOCUS: Effective embedding initialization for monoIn Houda Bouamor, Juan Pino, and Kalika lingual specialization of multilingual models. Bali, editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 1344013454, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.829. URL https://aclanthology.org/2023. emnlp-main.829/. [10] Kawin Ethayarajh. How contextual are contextualized word representations? Comparing the geometry of BERT, ELMo, and GPT-2 embeddings. In Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan, editors, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 5565, Hong Kong, China, November 2019. Association 12 for Computational Linguistics. doi: 10.18653/v1/D19-1006. URL https://aclanthology. org/D19-1006/. [11] Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noach, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. framework for few-shot language model evaluation, 12 2023. URL https://zenodo.org/ records/10256836. [12] Leonidas Gee, Andrea Zugarini, Leonardo Rigutini, and Paolo Torroni. Fast vocabulary transfer for language model compression. In Yunyao Li and Angeliki Lazaridou, editors, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing: Industry Track, pages 409416, Abu Dhabi, UAE, December 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.emnlp-industry.41. URL https://aclanthology.org/ 2022.emnlp-industry.41/. [13] Charles Goddard, Shamane Siriwardhana, Malikeh Ehghaghi, Luke Meyers, Vladimir Karpukhin, Brian Benedict, Mark McQuade, and Jacob Solawetz. Arcees MergeKit: toolkit for merging large language models. In Franck Dernoncourt, Daniel Preoţiuc-Pietro, and Anastasia Shimorina, editors, Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing: Industry Track, pages 477485, Miami, Florida, US, November 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.emnlp-industry.36. URL https://aclanthology.org/2024.emnlp-industry.36/. [14] Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. [15] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020. [16] John Hewitt. Initializing new word embeddings for pretrained language models, 2021. URL https:/nlp.stanford.edu/johnhew//vocab-expansion.html. [17] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in neural network. arXiv preprint arXiv:1503.02531, 2015. [18] jukofyork. Vocab transplantation tool (github repository), 2025. URL https://github.com/ jukofyork/transplant-vocab. [19] Subhash Kantamneni and Max Tegmark. Language models use trigonometry to do addition. arXiv preprint arXiv:2502.00873, 2025. [20] Saurabh Kulshreshtha, Jose Luis Redondo Garcia, and Ching-Yun Chang. Cross-lingual alignment methods for multilingual BERT: comparative study. In Trevor Cohn, Yulan He, and Yang Liu, editors, Findings of the Association for Computational Linguistics: EMNLP 2020, pages 933942, Online, November 2020. Association for Computational Linguistics. doi: 10. 18653/v1/2020.findings-emnlp.83. URL https://aclanthology.org/2020.findings-emnlp. 83/. 13 [21] Andrew Lee, Melanie Weber, Fernanda Viégas, and Martin Wattenberg. Shared global and local geometry of language model embeddings, 2025. URL https://arxiv.org/abs/2503.21073. [22] Yaniv Leviathan, Matan Kalman, and Yossi Matias. Fast inference from transformers via speculative decoding. In International Conference on Machine Learning, pages 1927419286. PMLR, 2023. [23] Jeffrey Li, Alex Fang, Georgios Smyrnis, Maor Ivgi, Matt Jordan, Samir Gadre, Hritik Bansal, Etash Guha, Sedrick Keh, Kushal Arora, Saurabh Garg, Rui Xin, Niklas Muennighoff, Reinhard Heckel, Jean Mercat, Mayee Chen, Suchin Gururangan, Mitchell Wortsman, Alon Albalak, Yonatan Bitton, Marianna Nezhurina, Amro Abbas, Cheng-Yu Hsieh, Dhruba Ghosh, Josh Gardner, Maciej Kilian, Hanlin Zhang, Rulin Shao, Sarah Pratt, Sunny Sanyal, Gabriel Ilharco, Giannis Daras, Kalyani Marathe, Aaron Gokaslan, Jieyu Zhang, Khyathi Chandu, Thao Nguyen, Igor Vasiljevic, Sham Kakade, Shuran Song, Sujay Sanghavi, Fartash Faghri, Sewoong Oh, Luke Zettlemoyer, Kyle Lo, Alaaeldin El-Nouby, Hadi Pouransari, Alexander Toshev, Stephanie Wang, Dirk Groeneveld, Luca Soldaini, Pang Wei Koh, Jenia Jitsev, Thomas Kollar, Alexandros G. Dimakis, Yair Carmon, Achal Dave, Ludwig Schmidt, and Vaishaal Shankar. Datacomp-lm: In search of the next generation of training sets for language models. arXiv preprint arXiv:2406.11794, 2024. [24] Remya R. K. Menon, Gargi, and Samili. Clustering of words using dictionary-learnt word representations. In 2016 International Conference on Advances in Computing, Communications and Informatics (ICACCI), pages 15391545, 2016. doi: 10.1109/ICACCI.2016.7732267. [25] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781, 2013. [26] Benjamin Minixhofer, Fabian Paischer, and Navid Rekabsaz. WECHSEL: Effective initialization of subword embeddings for cross-lingual transfer of monolingual language models. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 39924006, Seattle, United States, July 2022. Association for Computational Linguistics. URL https://aclanthology.org/2022. naacl-main.293. [27] Benjamin Minixhofer, Edoardo Maria Ponti, and Ivan Vulić. Zero-shot tokenizer transfer, 2024. URL https://arxiv.org/abs/2405.07883. [28] Malte Ostendorff and Georg Rehm. Efficient language model training through cross-lingual and progressive transfer learning, 2023. [29] Denis Paperno, Germán Kruszewski, Angeliki Lazaridou, Quan Ngoc Pham, Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fernández. The lambada dataset: Word prediction requiring broad discourse context. arXiv preprint arXiv:1606.06031, 2016. [30] Yagyensh Chandra Pati, Ramin Rezaiifar, and Perinkulam Sambamurthy Krishnaprasad. Orthogonal matching pursuit: Recursive function approximation with applications to wavelet decomposition. In Proceedings of 27th Asilomar conference on signals, systems and computers, pages 4044. IEEE, 1993. 14 [31] Phillip Rust, Jonas Pfeiffer, Ivan Vulić, Sebastian Ruder, and Iryna Gurevych. How good is your tokenizer? on the monolingual performance of multilingual language models. pages 31183135, 01 2021. doi: 10.18653/v1/2021.acl-long.243. [32] Konstantinos Skianis, Nikolaos Tziortziotis, and Michalis Vazirgiannis. Orthogonal matching pursuit for text classification. arXiv preprint arXiv:1807.04715, 2018. [33] Joel Tropp and Anna Gilbert. Signal recovery from random measurements via orthogonal matching pursuit. IEEE Transactions on information theory, 53(12):46554666, 2007. [34] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115, 2024. [35] Yinfei Yang, Yuan Zhang, Chris Tar, and Jason Baldridge. Paws-x: cross-lingual adversarial dataset for paraphrase identification. arXiv preprint arXiv:1908.11828, 2019."
        },
        {
            "title": "A Appendix",
            "content": "A.1 Token Decomposition Examples These examples show how tokens are represented as linear combinations of other tokens in the embedding space. Token 读者 불구하고 它是 várias 消者 营利 专家 必 要 が あ り ます 氟 geführt Sparse Linear Decomposition readers 0.147 + reader 0.129 + 作者 0.126 + 움 0.139 + 学生 0.111 + retail 0.112 + visually 0.107 + 讀 0.149 그러나 0.142 + 인데 0.122 + 통해 0.101 + らず 0.094 + Despite 0.090 + のに 0.106 + является 0.090 + -D -0.073 它 0.245 + 是 0.126 + 这是 0.154 + {}: 0.111 + Exist 0.106 + metro 0.108 + cht -0.095 + 地说 0.091 varias 0.208 + diversas 0.151 + muit 0.125 + varios 0.161 + uma 0.089 + various 0.090 + ários 0.097 + Brief -0.077 consumers 0.122 + 市 0.099 + 고객 0.113 + 企 0.101 + Consumers 0.112 + 篇 -0.072 + visitors 0.080 + portions -0.078 -profit 0.141 + 莉 0.124 + .ylim 0.127 + lucrative 0.130 + .Windows 0.097 + purpos 0.147 + 营 0.095 + -selling 0.106 experts 0.166 + expert 0.163 + 教师 0.119 + 主任 0.118 + 政策 0.095 + Scientists 0.125 + 咨询 0.105 + 资产 0.098 できます 0.197 + 있습니다 0.164 + необходимо 0.114 + harus 0.092 + べき 0.126 + However 0.086 + dispositivo 0.115 + muss 0.092 fluoride 0.123 + 弗 0.150 + lithium 0.141 + 氧 0.112 + 0.139 + ann -0.103 + fluor 0.134 + dynam -0.098 gemacht 0.161 + führt 0.202 + 한다 0.097 + geben 0.088 + であり 0.076 + chnitt 0.089 + iliated 0.077 + Sch -0.061 Figure 1: Sparse linear decompositions of selected tokens from Qwen 2.5s vocabulary. Each token is decomposed into weighted sum of = 8 basis tokens, with coefficients colored according to magnitude (green for positive, red for negative). A.2 Assets and Software Used Table 6 details the key models, datasets (beyond standard benchmarks cited in text), and software libraries used in this research, along with their sources and licenses, to aid reproducibility. Standard evaluation benchmarks like MMLU, ARC, GSM8K, etc., are cited in the main text and were accessed via the LM Evaluation Harness. 16 Table 6: Overview of Key Assets and Software Used. Asset Name Creator / Origin Version / Source / Access"
        },
        {
            "title": "Models",
            "content": "Llama 3.2 1B Meta AI [14] Qwen 2.5 7B Alibaba [34] Mistral NeMo 12B Mistral AI"
        },
        {
            "title": "Identifier",
            "content": "2024 re2024 reSept. lease Sept. lease July 2024 release https:// https:// huggingface.co/ meta-llama/Llama-3. 2-1B https:// huggingface.co/ Qwen/Qwen2.5-7B Llama 3 Community License Apache 2.0 Apache 2.0 huggingface. co/mistralai/ Mistral-Nemo-Base-2407 pip install transformers==4.50.0 pip install lm-eval-harness==0.4.8 https://github.com/ arcee-ai/mergekit pip install axolotl==0.8.1 Apache 2. MIT License BuSL-1.0 Apache 2.0 Software transformers Hugging Face 4.50.0 LM Eval Harness mergekit EleutherAI [11] 0.4.8 Arcee AI [13] Hash: 4da40d2 axolotl Axolotl AI 0.8."
        }
    ],
    "affiliations": [
        "arcee.ai"
    ]
}