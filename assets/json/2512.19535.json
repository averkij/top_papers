{
    "paper_title": "CASA: Cross-Attention via Self-Attention for Efficient Vision-Language Fusion",
    "authors": [
        "Moritz Böhle",
        "Amélie Royer",
        "Juliette Marrie",
        "Edouard Grave",
        "Patrick Pérez"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Vision-language models (VLMs) are commonly trained by inserting image tokens from a pretrained vision encoder into the textual stream of a language model. This allows text and image information to fully attend to one another within the model, but becomes extremely costly for high-resolution images, long conversations, or streaming videos, both in memory and compute. VLMs leveraging cross-attention are an efficient alternative to token insertion but exhibit a clear performance gap, in particular on tasks involving fine-grained visual details. We find that a key to improving such models is to also enable local text-to-text interaction in the dedicated cross-attention layers. Building on this, we propose CASA, Cross-Attention via Self-Attention, a simple and efficient paradigm which substantially reduces the gap with full token insertion on common image understanding benchmarks, while enjoying the same scalability as cross-attention models when applied to long-context multimodal tasks such as streaming video captioning. For samples and code, please see our project page at https://kyutai.org/casa ."
        },
        {
            "title": "Start",
            "content": "CASA: Cross-Attention via Self-Attention for Efficient Vision-Language Fusion Moritz Bohle Amelie Royer amelie@kyutai.org moritz@kyutai.org Juliette Marrie Edouard Grave egrave@kyutai.org juliette@kyutai.org Patrick Perez patrick@kyutai.org 5 2 0 2 2 ] . [ 1 5 3 5 9 1 . 2 1 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Vision-language models (VLMs) are commonly trained by inserting image tokens from pretrained vision encoder into the textual stream of language model. This allows text and image information to fully attend to one another within the model, but becomes extremely costly for high-resolution images, long conversations, or streaming videos, both in memory and compute. VLMs leveraging cross-attention are an efficient alternative to token insertion but exhibit clear performance gap, in particular on tasks involving fine-grained visual details. We find that key to improving such models is to also enable local text-to-text interaction in the dedicated cross-attention layers. Building on this, we propose CASA, Cross-Attention via Self-Attention, simple and efficient paradigm which substantially reduces the gap with full token insertion on common image understanding benchmarks, while enjoying the same scalability as crossattention models when applied to long-context multimodal tasks such as streaming video captioning. For samples and code, please see our project page at kyutai.org/casa. 1. Introduction Cross-attention has long been popular mechanism for fusing information in transformers, starting from encoderdecoder architecture in language models [41] to early vision-language models (VLMs) [2, 22]. Most recent stateof-the-art VLMs, however, rarely fuse modalities through cross-attention, but instead insert visual embeddings from pretrained image encoder into the language models input stream, directly interleaving them with the text embeddings [5, 10, 42, 51]. While insertion-based fusion has proven remarkably effective, it incurs substantial computational and memory cost, particularly for high-resolution images or video streams, for which thousands of image tokens may need to be added to the input of the language model. As result, wide range of techniques has been explored to reduce the computational burden of token insertion such as reducing the number of image tokens [22, 24, 43], inserting Equal contribution. Figure 1. Token insertion vs. cross-attention (CA) vs. CASA. In controlled experiment, we compare various cross-attention (vanilla, + gating, + registers) models with the insertion of visual tokens directly in the language models text tokens input. We find that while CA methods achieve comparable performance on general visual-question-answering tasks º, they significantly lag behind on fine-grained benchmarks (cid:219) such as chart or document understanding. Interestingly, we find this trend to be closely mirrored in the literature when comparing current state-of-the-art cross-attentionand insertion-based models, see Appendix C.1. Based on detailed analysis, we propose Cross-Attention via SelfAttention (CASA), which largely closes the gap to token insertion. image embeddings only in early layers [8] or compressing the KV cache of the model at inference [6, 36, 53]. Departing from token insertion, recent works have revisited cross-attention as naturally efficient alternative that lends itself particularly well for streaming applications on long multimodal sequences [7, 25, 37, 50], exploring different design choices to narrow the performance gap with insertion-based fusion. These include architectural changes regarding the placement of cross-attention layers [50], the addition of learnable visual tokens that interact with the language stream [7], or the update of the visual embeddings across the layers of the language models via dedicated modules [25]. While recent cross-attention VLMs have successfully closed the performance gap for general VQA benchmarks [50], they still lag behind on several tasks such as document and chart understanding, as shown in Figure 1; the root of this discrepancy remains poorly understood. To shed light on this issue, we investigate the limitations of current cross-attention designs and propose CASA, Cross-Attention via Self-Attention, new fusion mechanism that closes the gap to insertion-based VLMs while maintaining the efficiency benefits of cross-attention. Simi1 lar to cross-attention, CASA injects visual information only through dedicated attention layers, without inserting any image tokens in the text stream of the language model. Unlike cross-attention, however, CASA allows text tokens to self-attend to other text tokens in smaller local attention windows. This endows the model with implicit gating abilities, in contrast to commonly used explicit gated crossattention designs [25, 50]. Through thorough ablations, we find this to be crucial component allowing CASA to close the performance gap to insertion-based models. We also show that CASA can be trained efficiently by leveraging modern efficient implementations of blockwise attention. We demonstrate CASAs effectiveness through extensive experiments, when starting from pretrained textonly language model as well as when adapting an existing VLM. When trained from scratch from text-only backbone, CASA outperforms larger cross-attention-based models and performs on par with recent open-source insertionbased models of comparable size. When used to adapt pretrained token insertion model such as Qwen2.5-VL [5], CASA achieves results close to those of the original architecture while substantially reducing memory usage during training and improving both speed and efficiency in multiturn conversations. Finally, to assess CASAs scalability in the context of real-time video understanding, we apply CASA to live video captioning, an underexplored and computationally demanding task. In particular, CASAs local attention mechanism enables continuous visual updates with low latency and almost constant memory cost, while the self-attention layers on text-only tokens preserve the longrange textual consistency required for this task. In summary, our contributions are: (1) We introduce Cross-Attention via Self-Attention (CASA), leveraging block-wise self-attention to efficiently inject visual information into LLMs. Through extensive experiments, we show that CASA constitutes competitive alternative to token insertion, with substantially reduced memory and computational cost. (2) We show that CASA can be used both to train new multimodal models from text-only language models as well as to adapt existing VLMs. In both settings, CASA achieves performance close to the base insertionbased architecture and outperforms prior cross-attentionbased approaches. (3) We show that the self-attention component of CASA is crucial for its performance: First, crossattention designs lacking this component perform significantly worse than their CASA counterparts; Second, ablating the self-attention component in trained CASA mod- (4) Finally, els leads to significant performance drops. we demonstrate CASAs ability to handle real-time streaming video captioning at low latency, setting that is prohibitively costly for insertion-based models. To foster reproducibility, we release our inference code and trained models. See our project page at kyutai.org/casa. 2. Related work Insertion-based fusion. Token insertion has become the dominant paradigm for training VLMs: Visual embeddings from pretrained encoder are inserted directly into the language models input sequence, interleaving them with text embeddings. The visual and textual information thus interact through self-attention layers without any architectural changes. Recent token insertion models [5, 10, 28, 42, 52] have achieved strong multimodal performance with this strategy. However, the number of visual tokens rapidly grows with image resolution or video length, thus increasing the memory and computational costs at both training and inference time, and requiring dedicated techniques to handle longer contexts, e.g., by modifying RoPE [5, 16]. Token compression. To limit the number of visual tokens inserted into the LLM [5, 10, 27, 51], prior work employs compression techniques such as pixel unshuffling [38], hierarchical token merging [24], attention pooling [43], querybased compression, e.g., Q-Formers [22, 23, 52], global pooling [27], or low frame-rate sampling [43]. While these partially alleviate the high cost of token insertion, visual token compression often comes at the expense of fine spatial and temporal details at high compression rates [43]. While our proposed CASA mechanism inherently avoids the cost of token insertion, we note that such compression techniques might still complement it to further reduce the cost of the cross-attention itself. Cross-attention-based fusion for VLMs was popularized by Flamingo [2], one of the first large-scale VLMs, in which frozen LLM is conditioned on visual inputs through gated cross-attention. It was adopted in subsequent works [4, 21], and recently revisited in [7, 25, 37, 50], which leverage the natural scalability of cross-attention for long-context or streaming applications. However, the practical benefits of cross-attention-based VLMs come at the cost of markedly weaker performance on tasks requiring fine-grained visual understanding, such as chart or document reading, even when compared to much smaller token-insertion models. Prior work has attempted to address this issue by updating the visual representations throughout the depth of the model [25], adding register tokens [7], or introducing bespoke architectural changes [50]. Nonetheless, the performance gap largely remains. In this work, we show that the gap can largely be explained (and thus addressed) by the lack of text-to-text interaction in cross-attention. Our proposed CASA mechanism retains simple design, close to that of standard cross-attention, but additionally integrates self-attention component. We find that this leads to substantial performance boost, closing the gap to token insertion on tasks where cross-attention VLMs tend to struggle. Streaming models. Recent work has tackled the task of visual understanding in streaming context, typically for 2 videos [6, 47, 53]. Such tasks imply strict memory and computational budget for the model to execute in real-time and for as long as possible. To address the limitations of insertion-based models, recent approaches attempt to limit the KV cache size through compression [53], pruning of the oldest video frames [6, 36], or by adding condensed visual memories to the text stream [47, 56]. Closest to our work, StreamChat [25] adopts cross-attention-based design that updates its visual keys and values at each decoding step to align the current text stream with the latest video frame. The resulting model naturally lends itself to streaming applications, similar to our work. To reduce the performance gap of cross-attention, StreamChat applies additional dedicated FFN layers to the visual tokens throughout the network. In contrast, CASA only requires lightweight and simple change to the cross-attention layers, and achieves similar performance with much smaller LLM backbone. 3. Approach 3.1. Preliminary The de-facto standard to inject multimodal information in modern LLMs is via explicit token insertion. Given text tokens x1...T and tokenized image y1...N inserted at time in the conversation, all text tokens xi following the image interact with image tokens through self-attention as: xi (1) xi + MHA(xi x1...K y1...N xK+1...i) , with MHA(u v) the standard multi-head attention [41] with query and key and value v. Compute and memory costs thus rapidly grow with the image token count, becoming major bottleneck for videos or high resolution images [43]. In contrast, cross-attention-based models inject visual information directly into the text embeddings through an additional attention layer, i.e. text tokens xi>K following the image at time are passed through an additional crossattention (CA) operation with xi>K as queries and y1...yN as keys and values:"
        },
        {
            "title": "CA xi",
            "content": "xi + 1i>K MHA(xi y1...N ) . (2) In the literature, these additional cross-attention layers are , [50]) of ev- , [25]), or in-parallel ( placed either after ( ery self-attention layer in the backbone LLM. At inference, cross-attention-based injection is particularly efficient for long contexts with multiple images. Providing new image in conversation only requires updating the input to the cross-attention layer without affecting the underlying LLMs KV cache, while the overhead of the additional layers is negligible. Despite these practical advantages, cross-attention-based models lag behind their token insertion counterparts on common VLM benchmarks. We aim to understand and close the gap between cross-attention and token insertion for visual knowledge injection. 3 (a) In the proposed CASA layers, text tokens attend causally to both the text and image tokens. The resulting update can then be reinjected in the text stream as an additive residual (CASA). Other variants for injecting the CASA output back in the text tokens are showcased in Figure (b). (b) CASA layers can be placed either before (CASA) or in parallel (CASA) of the self-attention layers, both designs being common in the cross-attention literature. We also propose CASA, variant unique to CASA where CASA layers replace self-attention layers. This is only possible because text tokens attend to both text and image tokens in CASA. Figure 2. CASA (Cross-Attention via Self-Attention) injects visual information through cross-attention layers in which the current text tokens attend to the concatenation of themselves and image tokens, in causal manner. This provides natural gating mechanism that outperforms standard cross-attention-based VLM architectures. During training, CASA leverages recent blockwise attention mechanism to remain efficient. At inference, CASA benefits from the same practical advantages as cross-attention and can handle long interleaved image-text sequences without affecting the KV cache and memory usage of the underlying LLM. 3.2. CASA: Cross-Attention via Self-Attention We hypothesize that key weakness of cross-attention as knowledge injection mechanism is its possible destructive effect on the input text embeddings. Previous works have introduced gating strategies to mitigate this effect, which have not proved sufficient to fully close the gap [25, 50]. To remedy this issue, we propose to preserve interaction between text tokens during visual knowledge injection and introduce CASA: Cross-Attention via Self-Attention. As illustrated in Figure 2, CASA layers allow text tokens to attend not only to image tokens but also to one another. More precisely, every text token attends to the concatenation of the currently relevant image tokens and all text tokens positioned in between the image and itself, as follows:"
        },
        {
            "title": "CASA xi",
            "content": "xi +1i>K MHA(xi y1...N xK+1...i) . (3) We find that this simple change to Equation 2 greatly improves the performance of cross-attention fusion. Consequently, in CASA layers text and image tokens only interact in local attention windows, whose boundaries are defined by the occurrence of new image in the conversation. Similar to cross-attention, (i) image tokens do not pass through the self-attention layers and FFNs of the LLM, reducing compute over insertion, (ii) and CASA scales linearly in the number of image tokens per image, as image tokens are only used as keys and values, never as queries. This significantly reduces the computational and memory costs of working with long multimodal sequences with many inserted images when compared to token insertion, where all image and text tokens (causally) attend to each other. CASA performs implicit gating. Compared to crossattention-based models, CASA mainly differs in the inputs to the cross-attention operation, as text tokens are also used as keys and values. In practice, this design endows the model with natural gating abilities, as the attention softmax inherently balances the relative contributions of image and text tokens (see ablations in Section 4.4). Consequently, CASA does not require any additional explicit gating mechanism as is often done in cross-attention fusion literature. CASA is modular. CASA easily integrates into existing models, as it applies separate attention operations that act independently of the self-attention layers. In particular, they do not affect the LLMs context length or positional embeddings, which is typically addressed by dedicated manipulations of the RoPE embeddings [1, 5, 10, 42] or context extension techniques [9, 35, 44]. Furthermore, as we show in Section 4.3, we can adapt an existing token-insertion VLM architecture into an efficient CASA one by only training the additional CASA layers. This can be done efficiently as CASA layers only account for 352M-680M extra training parameters, i.e., 16-17% increase on the 2B-3B architectures we consider in practice. CASA as drop-in replacement for self-attention. By design, CASA uses causal attention between text tokens, as in standard self-attention layers. Leveraging this property, we also propose CASA, variant of CASA in which CASA layers directly replace subset of the the LLMs self-attention layers rather than using CASA as an additive update, as is commonly done in cross-attention-based fusion. In practice, we observe that CASA performs best when CASA layers replace only small subset of the selfattention layers, thus providing further compute and memory gains at the cost of small performance drop. 3.3. Efficiency Analysis"
        },
        {
            "title": "FUSION",
            "content": "# EXTRA"
        },
        {
            "title": "TIME",
            "content": "Insertion Cross-attention CASA CASA 0 352M 352M 0 (T + )2 2 + 2 + (N + TW ) max (cid:0)T 2, 2(cid:1) 52.4 40.3 40.1 32.7 10h29mn 8h21mn 10h14mn 7h2mn Table 1. Summary of memory and compute cost of different vision-language fusion mechanisms. We report the number of added parameters, the memory cost associated to attention blocks, and the max memory usage and training time observed in single training run in our experiments. and denote the number of text and image tokens, and TW the average number of text tokens in CASA local attention window. For practical costs, we report the maximal memory reserved in GB, and total training times. Note that for token insertion (marked in grey), we need to reduce the training sequence length due to memory constraints and set and to roughly 75% of that of the cross-attention-based models; otherwise, token-insertion runs exceed memory limits. Inference efficiency. Compared to token insertion, CASA has the same practical advantages as cross-attention: (i) image tokens are not forwarded through FFNs thus reducing compute, (ii) image tokens are not inserted in the KV cache, thus allowing for longer text context length, and (iii) the memory cost of CASA does not depend on the number of images inserted at different points in time. Training efficiency. Our models are trained on mixture of public vision-language datasets, primarily composed of question-answer pairs over single images. Due to the high disparity in sequence lengths between samples, standard batching would result in large number of padding tokens. To avoid these wasteful tokens during training, we employ multimodal sequence packing, as commonly done in LLM training [13, 48] and modern VLM pipelines [5, 51]. As result, every training sample is long sequence of interleaved image and text. To match the desired crossattention inference behavior, where images are replaced onthe-fly as they are inserted, we employ the efficient blockwise attention implementation of Flash-Attention2 [11] in the CASA layers during training. As illustrated in Figure 3, we define the attention blocks with the image insertion points acting as natural window delimiters1 and, to avoid quadratic scaling with the number of image tokens, implement an asymmetric attention operation with the text tokens as queries and both text and images as keys and values. In the next section, we evaluate CASA on common VLM benchmarks when starting from either languageonly LLM or pretrained token-insertion VLM and assess its efficiency for real-time live video captioning. We discuss the inference and training efficiency of CASA below, and summarize the respective costs of different vision-language fusion techniques in Table 1. 1Importantly, when using asymmetric attention, placing the image anywhere other than at the beginning of window breaks causality between the text tokens during training in the current implementation of FlashAttention-2, where the attention mask is bottom-right aligned. 4 Figure 3. CASA vs. SA. (Left) Instead of inserting image tokens directly into the stream of text tokens at the desired position (dashed lines, bottom), CASA (top) uses block-wise attention to augment groups of tokens with visual information. Compared to token insertion, the cost of self-attention (SA) thus reduces from O(T +N ) to O(max(Twindow +Nwindow, )), for text and image tokens. To illustrate this, we show which tokens query at given text position attends to, showing masked tokens at lower opacity. (Right) Attention matrices for CASA and SA layers: while in SA layers text only attends to text, CASA layers apply block-wise attention between text and images. 4. Experiments 4.1. Experimental Setting Below we provide brief summary of our experimental setup. Please refer to Appendix B.1 for full details. Training data. We train our models on FineVision [45] along with subset of LLaVA-OneVision-1.5 [3]. Both are curated collections of publicly available image-text datasets covering wide range of tasks such as captioning, document and chart reading, general VQA, etc. For video training, we further train our models with the aforementioned image-text dataset alongside LLaVA-Video-178K [57]. Backbones. To showcase CASAs applicability for both extending language-only models with visual understanding as well as adapting existing VLMs, we train our models in two settings: (i) Starting from Helium1-2B [19] as textonly LLM backbone, we jointly finetune the backbone and CASA layers; (ii) We adapt frozen Qwen2.5-VL-3B [5] VLM by only learning added CASA layers. In both cases, the CASA layers are initialized from the self-attention layers of the respective backbone. Lastly, we use the vision encoder of Qwen2.5-VL [5] to embed images and videos, and finetune its last 4 layers when training on image data; we freeze it when further finetuning on video inputs. Benchmarks. We evaluate our models on common VLM benchmarks on variety of tasks: document understanding (DocVQA [30]), chart understanding (ChartQA [29], InfoVQA [31]), visual reading (TextVQA [39], text OCRBench [26]), and general QA (RealWorldQA [46], AI2D [18], GQA [17], MME [14]). Similarly, we evaluate our video models for multi-choice video QA (MVBench [23], VideoMME [15], NExT-QA [39], PerceptionTest [34]) and long video understanding (MLVU [59]). Training compute. As mentioned in Section 3.3, we use multimodal sequence packing during training, interleaving text-image sequences capped to 2048 text and 20,480 image tokens per GPU. We process images at native resolution up to Rmax =8962 pixels (1024 tokens per image) and downscale larger images to the maximum resolution while maintaining their aspect ratio; for token-insertion experiments, we use Rmax =6722 and cap sequences at 1024 text tokens and 10,240 image tokens due to the additional computational and memory costs. For videos, we use Rmax =4482 and extract 2 frames per second, with maximum clip duration of 3 minutes, resulting in up to 46,080 image tokens. We freeze the image encoder and do not employ sequence packing as samples are already long enough to fill the GPU memory. We train for 100k steps for Helium1, 60k for adapting Qwen2.5-VL on images, and further 15k for videos. 4.2. From LLM to VLM with CASA Layers We first train the text-only Helium1-2B with additional CASA layers for our three CASA variants: (i) CASA, the lightest approach, which replaces every 4th self-attention layer with CASA layer, (ii) CASA, where the outputs of self-attention and CASA layers are summed at every layer, and (iii) CASA, where every self-attention layer is preceded by CASA layer. We report results on VLM benchmarks in Table 2, where we compare CASA to mPLUG-Owl3 [50], state-of-the-art cross-attention VLM, as well as token insertion models, including version of Helium1 trained with token insertion in our own setting, and SmolVLM [28], an open-source VLM trained on public data similar to ours. For reference, we also report performance of recent proprietary VLMs (InternVL2.5 [10], Qwen2-VL [42], and Video-LLaMA3 [51]). As shown in Table 2, all CASA variants significantly outperform the cross-attention-based mPLUG-Owl3, even surpassing its 8B variant, in line with the motivational results we present in Figure 1. Consequently, CASA substantially narrows the performance gap to token insertion compared to standard cross-attention approaches, in particular on datasets requiring understanding text or high-resolution details (e.g., ChartQA, DocVQA, InfoVQA). performance gap to token insertion remains, with 7 percent for CASA relative to an average drop of the insertion-based Helium1. The largest drops are observed on InfoVQA and ChartVQA, two datasets requiring # train High-res Document/Chart Understanding Scene Text Understanding Knowledge / General QA tokens CHARTQA DOCVQA INFOVQA OCRBENCH TEXTVQA REALWORLDQA AI2D GQA MME Token Insertion Proprietary InternVL2.5 [10] Qwen2-VL [42] VideoLLaMA3 [51] Token Insertion Public data SmolVLM [28] InsertionHe-2B 0.5T 1.4T - - 0.1T Cross-attention-based Public data mPLUG-Owl3 8B [50] StreamChat 7B [25] mPLUG-Owl3 2B CASAHe-2B 0.1T - 0.1T 0.3T 0.3T 0.3T 79.2 73.5 79.8 68.7 81.6 59.2 48.5 73.0 73.8 73. 88.7 90.1 91.9 80.0 89.1 55.9 48.2 81.3 82.8 83.7 60.9 65.5 69. 42.2 61.8 36.8 28.1 44.8 48.2 48.6 804 767 779 729 527 450 694 728 723 74.3 79.7 80.1 73.0 75.5 69.0 72.4 62. 70.5 70.3 71.0 60.1 62.9 67.3 51.0 59.9 63.9 61.7 56.9 55.6 57.0 58.3 74.9 69.9 78. 59.5 59.8 62.7 2005 1872 1901 59.7 67.7 49.2 55.5 1568 1732 73.4 76.6 62. 61.4 63.5 63.3 65.0 62.4 61.0 54.1 55.0 54.6 1940 1520 1551 1574 1620 1572 : Reproduced with the publicly available models at huggingface. : Results and model not publicly available. Table 2. Comparing CASA, cross-attention, and insertion-based models. Unless specified, all models are built on 2B LLMs. We use lmms-eval [54] for evaluation, and re-evaluate existing models when benchmark results are not provided in the original work. All CASA variants significantly outperform current SotA cross-attention-based VLMs, narrowing the gap to insertion-based VLMs. Document/Chart Understanding"
        },
        {
            "title": "Scene Text Understanding",
            "content": "Knowledge / General QA Res. CHARTQA DOCVQA INFOVQA OCRBENCH TEXTVQA REALWORLDQA AI2D GQA MME Qwen2.5-VL 3B (reported) Qwen2.5-VL 3B (reproduced) CASA Qwen2.5-VL Native 8962 8962 84.0 83.5 82. 93.6 91.5 88.9 77.1 71.6 59.6 797 804 79.3 79.2 77.4 - 62.2 62.5 81.6 80.1 75. 61.0 - 2249 59.4 1918 Table 3. Adapting frozen pretrained Qwen2.5-VL to CASA by training the new CASA layers recovers most of the performance of the base model and outperforms our CASAHe-2B models in fewer training steps by leveraging strong pretrained VLM backbone. fine-grained visual understanding of charts and diagrams. Nonetheless, CASA offers clear practical advantages for streaming applications, which we showcase on the task of live video captioning in Section 4.5. For further ablation experiments and discussion on the differences between the CASA designs, see Section 4.4. 4.3. Adapting VLM from Insertion to CASA Even when starting from smaller LLMs, training VLM from scratch is costly and it is very common for practitioners to instead leverage VLMs pretrained on private highquality datasets. Similarly, we adapt pretrained VLM, Qwen2.5-VL-3B [5], by replacing its full token-insertion mechanism with CASA. We train the added CASA layers alongside the four last blocks of the visual encoder, while keeping the rest of the VLM frozen. In this setting, we find that CASA significantly outperforms CASA, which suggests that keeping CASA layers in parallel of SA layers is less disruptive when adapting frozen VLM. Image evaluation. We report results in Table 3, directly comparing to the base model Qwen2.5-VL. As in Section 4.2, changing to CASA-based design incurs small per5 percent drop). In particular, the largest formance gap ( 6 drop occurs on Infographic-VQA and AI2D, two datasets specialized in diagram and infographic understanding, consistent with the trends we observed in the previous section. Nonetheless, CASAQwen2.5-VL easily outperforms the fully finetuned CASAHe-2B models from Section 4.2, with smaller training cost, showing the clear benefit of directly adapting strong pretrained VLMs. Video evaluation. As Qwen2.5-VL-3B displays remarkable video understanding abilities, we further fine-tune CASAQwen2.5-VL on videos [57], see Table 4. In line with our 5.6 points prior results, the CASA-adapted model lands below the base model, performing on par or better than the larger cross-attention-based mPLUG-Own3-8B. Max Res. Native 3842 4482 VIDEOMME w/o sub w/ sub NEXT QA PERCEP. Test MV Bench MLVU 61.5 53.5 58.6 67.6 - 62.4 - 78.6 77.9 67.0 - 58.1 66.9 54.5 58. 68.2 - 65.1 Qwen2.5VL 3B mPlug-Owl3 8B CASA Q2.5VL Table 4. Video benchmarks. Adapting Qwen2.5-VL with CASA layers retains much of its performance and outperforms the significantly larger cross-attention-based mPLUG-Owl 8B."
        },
        {
            "title": "MODEL",
            "content": "CASA He-2B + FFN UPDATES"
        },
        {
            "title": "BENCHMARK",
            "content": "HRES OCR VQA 55.7 58.1 64.1 66.6 44.1 43. AVG. 54.7 56.0 Table 5. Updating image embeddings through the underlying LLMs FFNs provides modest increase in performance, but comes with non-negligible memory and compute costs. Running this ablation thus required smaller experimental setup where the image encoder is frozen, to compensate for the increased memory."
        },
        {
            "title": "MODEL",
            "content": ""
        },
        {
            "title": "BENCHMARK",
            "content": "HRES OCR VQA AVG. (a) Cross-attention vs. CASA CASA CASA CASA CA GATED CA 66.4 66.6 63.7 30.0 32.0 72.3 71.2 68.9 31.1 32.2 57.6 56.3 56.1 59.8 60.4 (b) Impact of compression on token insertion TOKEN INSERTION (1024) Q-FORMER (128) Q-FORMER (32) 72.3 62.7 56. 71.9 70.8 67.0 58.6 57.8 56.9 65.4 64.7 63.2 40.3 41.5 67.6 62.7 59.9 Table 6. Additional ablations. In (a), we compare crossattention-based fusion with CASA in identical training settings using the same configuration as Table 2 with fewer training steps. Our results highlight the improved performance of CASA on tasks involving text reading or high-resolution images. In (b) we evaluate the impact of token compression, common solution for making token insertion more efficient. Reducing the number of image tokens quickly deteriorates performance on tasks requiring high resolution images. Furthermore, as we showcase in Section 4.5, it is not sufficient to solve the memory bottleneck inherent to image token insertion as the KV cache grows during streaming inference. 4.4. Ablation Experiments For ease of reading, in the ablation experiment results we group the 9 benchmarks into 3 groups: HRES, highres. chart and document reading (DocVQA, InfoVQA, ChartQA), OCR, reading text in natural images (OCRBench, TextVQA), and VQA, general-knowledge visual question answering (RealWorldQA, AI2D, GQA, MME). Updating the image embeddings. Recent cross-attention VLMs [25] update the image embeddings by passing them through dedicated FFN layers to improve performance. We evaluate the benefits of this approach through small-scale ablation with frozen image encoder, as propagating the image tokens through FFNs substantially increases memory usage. As shown in Table 5, updating the image tokens yields slightly improved performance (about two points on reading tasks), in line with prior work [25]. However, the additional computational and memory costs at training and inference outweigh these relatively modest gains. Comparison of CASA variants. In Table 6 (top), we re-"
        },
        {
            "title": "BENCHMARK",
            "content": "HRES OCR VQA AVG. CASA LAYERS CASA He-2B (reference) EVERY 8 BLOCKS EVERY 4 BLOCKS EVERY 2 BLOCKS FIRST 8 BLOCKS FIRST AND LAST 4 BLOCKS 60.3 58.2 58.5 56.0 52.7 39.3 67.8 67.8 66.5 64.9 62.0 54. 55.0 53.1 50.6 39.7 47.4 45.5 61.0 59.7 58.5 53.5 54.0 46.4 Table 7. Impact of the placement of CASA layers assessed in smaller ablation setup. As CASA layers perform local attention, as opposed to the standard global attention of self-attention layers, they may be detrimental to performance. We find that placing them uniformly and sparingly across the architecture works best. Documents / Chart Scene Text Knowledge / General QA CHART DOC INFO OCR TEXT RW AI2D GQA MME CASAQwen2.5-VL - RND - RND TXT - SELF 82.4 80.7 76.2 63.2 88.9 59.6 87.4 84.3 72.5 59.9 55.6 43. 790 780 758 652 77.4 62.5 75.1 59. 1918 79.0 76.7 67.8 61.8 52.3 40.9 74.5 62.5 37.7 59.1 53.6 36.0 1882 1687 Table 8. Importance of self-attention in CASA. Comparison of the performance drop when, at inference, attention from the current token is masked either toward random past image or text token (- RND), toward random past text token (- RND TXT), or toward itself (- SELF); lowest results in bold red. - SELF performs worst by far, highlighting the reliance on self-attention of CASA layers. port results for training CASA from the text-only Helium12B model [19] using all three variants: CASA, CASA, and CASA. CASA and CASA achieve comparable performance: CASA is slightly below in our reduced ablation setting with shorter training, but slightly above in the full training regime reported in Table 2. CASA performs marginally worse than CASA and CASA, but provides substantial gains in memory efficiency and inference speed. In contrast, when adapting Qwen2.5-VL, CASA clearly outperforms CASA, suggesting that CASA is less invasive and better suited for adapting frozen pretrained VLM. Position of CASA layers. As mentioned in Section 3.2, our lightweight CASA alternative is only introduced in subset of the models transformer blocks. This is in particular due to the fact that CASA layers only compute local attention patterns, inside contiguous windows of tokens relevant to the same image, as described in Section 3.3. Thus introducing too many CASA layers in the architecture may be detrimental to performance as they are not equivalent to self-attention layers which perform global attention on the text tokens. In Table 7, we study the impact of where CASA layers are introduced: We find that CASA performs best when CASA layers are introduced uniformly across the architecture, with as few as 5 CASA layers. CASA as an alternative to cross-attention. To further fairly assess CASA as stronger alternative to cross7 attention, we also compare our CASA variants to crossattention-based model trained in our setup, both with simple cross-attention and gated cross-attention variant similar to that of mPLUG-Owl3 [50]. Table 6 (top) highlights the stronger performance of CASA when comparing it to cross-attention based fusion in identical training settings. Impact of text-to-text interaction in CASA layers. To assess the importance of the text-to-text interaction in CASA layers, we evaluate the performance drop when forcefully limiting it at inference, bringing CASA layers closer to standard cross-attention: Table 8 reports the models performance when manually adding specific attention mask in CASA layers at inference; Specifically, we mask the attention from the current token towards (i) itself, (ii) randomly selected past text token or (iii) any randomly selected past token. While (ii) and (iii) have little impact, preventing token to attend to itself in the CASA attention operation severely degrades performance across all benchmarks. As further discussed in Appendix A, we hypothesize that the inclusion of text tokens self-attention in CASA layers creates natural gating mechanism through the softmax of the attention operator and thus regulates how visual information can influence the textual stream, as opposed to standard cross-attention-based approaches. Link to token compression. To reduce token insertion costs, it is common to compress the number of image tokens before inserting them in the text stream [22, 24, 27, 43, 52]. In Table 6 (bottom), we report results of training Helium12B with either full token insertion or Q-Former-based compression [22], in which small transformer block is applied to the image tokens produced by the vision encoder, alongside learnable queries; only the queries are then inserted in the LLMs textual stream. For general VQA tasks, we find that even aggressive token compression has limited impact on the performance, but for tasks requiring more detailed representations (HRES, OCR) we observe significant performance drops. Hence, compressed insertion can be practical alternative to full token insertion if the task does not involve fine-grained visual detail. Nonetheless, as we will see in Section 4.5, even with compression we still reach the models memory and context length limits when dealing with long streaming video understanding. That said, we note that token compression is orthogonal to CASA, and both could be combined for particularly constrained memory or compute budgets. 4.5. Application to Live Video Captioning An important motivation for efficient image-text fusion are streaming video understanding applications. This comes with two particular challenges: (i) Memory growth needs to be controlled in order to handle longer videos, and (ii) for streaming applications, the model latency needs to stay below the frame rate to avoid accumulating delay over time."
        },
        {
            "title": "Model",
            "content": "Training steps Winrate (%) CASA Qwen2.5-VL (Ours, 3B) CASA Qwen2.5-VL 3k 6k 8k 12k 15k 17k 20k Baselines as reported in LiveCC, 7B LiveCC-7B-Base LiveCC-7B-Instruct Qwen2-VL-7B-LiveCC 25.4 27.6 33.7 34.3 36.3 39.4 39.0 43.2 41.5 33.7 Table 9. Streaming video captioning. LLM as judge evaluation on LiveSports3k with GPT-4o as judge. We rely on the evaluation pipeline and ground-truth captions provided in LiveCC [6]. By design, CASA is well equipped to address both of these issues. To illustrate this, we tackle the task of live video captioning. We leverage Live-WhisperX-526K, an instruction training dataset recently introduced in LiveCC [6]. It is composed of video frames extracted at framerate of 2fps and interleaved with the text transcript of the videos original audio. We use this dataset to further finetune our CASAQwen2.5-VL model for the task of live video captioning. We provide qualitative samples of live captioning results in Figure 4 and in our project page at kyutai.org/casa. Quantitative evaluation. We evaluate our LiveCC-tuned model on the LiveSports3K benchmark proposed in [6]. The dataset consists of videos of sports events ( 20 seconds long). The captions are evaluated using an LLM as judge, following the evaluation protocol provided in LiveCCs repository with reference captions generated with GPT-4o and further relying on GPT-4o as the judge. In Table 9, we report results for our CASAQwen2.5-VL trained on LiveCC, evaluated across training steps, and compare to the results reported in the original LiveCC paper. Notably, despite its smaller size (3B vs. 7B), our CASA model obtains scores similar to those of LiveCC models [6]. Real-world performance. As can be seen in Figure 5, the memory cost of token insertion methods increases more rapidly than for the CASA model. While token compression reduces the cost of token insertion for short conversations, it cannot alone prevent the increased memory usage leading to OOM when the number of tokens is too high. In Figure 6, we also report the wall-time of the same models (recorded on single H100 GPU) as function of the number of frames inserted. As expected, generation with insertionbased models becomes progressively slower as image tokens accumulate in the KV cache. In contrast, CASA maintains high inference speed over much longer horizons. 8 This video shows [00:010.0s] the Apollo 13 [00:020.0s] spacecraft during its [00:030.0s] 40 years of space camp missions at [00:050.0s] NASAs Johnson Space Center [00:060.0s] in Houston Texas [00:070.0s] and it was filmed by NASA [00:080.0s] Space Camp Director [00:090.0s] Steve Smith who has been with us for over [00:100.0s] 25 years now [00:110.0s] at this point he had to go back home [00:120.0s] to his job as [00:130.0s] director but were going [00:150.0s] with him here today [00:160.0s] so you can see how theyve changed [00:170.0s] over time from [00:180.0s] Apollo era [00:190.0s] missions all right [00:200.0s] well start Figure 4. Live captioning. We display captions generated by our CASA Qwen2.5-VL-LiveCC model. Each text span is annotated with the corresponding frames timestamp (top) and the models delay as [timestamp delay]. As shown in Figure 5, CASAs outputs are generated in real-time and with little memory building up over time; for further qualitative examples, including insertion-based VLMs, see App. D. that CASA substantially outperforms conventional crossattention, both when training vision-language model from scratch and when adapting an existing one. CASA closes much of the gap to full token insertion while retaining the computational benefits of cross-attention: Visual tokens are never updated through feedforward networks, not do they In particular, this enables take space in the KV cache. CASA to be employed for real-time streaming applications. Acknowledgements. This project is funded by Iliad Group, CMA CGM Group and Schmidt Sciences. The authors thank Alexandre Defossez for his support and feedback throughout the project."
        },
        {
            "title": "References",
            "content": "[1] Pravesh Agrawal, Szymon Antoniak, Emma Bou Hanna, Baptiste Bout, Devendra Chaplot, Jessica Chudnovsky, Diogo Costa, Baudouin De Monicault, Saurabh Garg, Theophile Gervet, et al. Pixtral 12b. arXiv:2410.07073, 2024. 4 [2] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: Visual Language Model for Few-Shot Learning. NeurIPS, 2022. 1, 2 [3] Xiang An, Yin Xie, Kaicheng Yang, Wenkang Zhang, Xiuwei Zhao, Zheng Cheng, Yirui Wang, Songcen Xu, Changrui Chen, Chunsheng Wu, et al. Llava-onevision-1.5: Fully open framework for democratized multimodal training. arXiv:2509.23661, 2025. 5, 12 [4] Anas Awadalla, Irena Gao, Josh Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe, Yonatan Bitton, Samir Gadre, Shiori Sagawa, Jenia Jitsev, Simon Kornblith, Pang Wei Koh, Gabriel Ilharco, Mitchell Wortsman, and Ludwig Schmidt. OpenFlamingo: An Open-Source Framework for Training Large Autoregressive Vision-Language Models. arXiv:2308.01390, 2023. 2 [5] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Figure 5. Real-time memory usage (reported in MB, log scale) of CASA compared to token insertion techniques with different levels of token compression (via Q-Former), measured on single H100 GPU. CASA memory usage builds up more slowly than the token-insertion-based techniques, even with token compression. Figure 6. Walltime in streaming inference. We record the walltime as function of the number of frames inserted in streaming captioning scenario, for CASA and token-compression techniques (Q-Former with different numbers of query tokens). While token compression mitigates the computational cost of token insertion for short videos, CASA maintains low latency inference for longer times. Note that for better readability, we only plot subset of markers, although the plotted measurements occur at every frame. 5. Conclusions We introduce CASA, new fusion mechanism that departs from standard cross-attention by allowing text-to-text attention alongside text-to-image attention. This design provides an implicit gating effect, which we identify as key factor of improvement. Through extensive experiments, we show 9 Wang, Jun Tang, et al. Qwen2.5-VL Technical Report. arXiv:2502.13923, 2025. 1, 2, 4, 5, 6 [6] Joya Chen, Ziyun Zeng, Yiqi Lin, Wei Li, Zejun Ma, and Mike Zheng Shou. LiveCC: Learning Video LLM with In CVPR, 2025. Streaming Speech Transcription at Scale. 1, 3, 8, 14, [7] Kaibing Chen, Dong Shen, Hanwen Zhong, Huasong Zhong, Kui Xia, Di Xu, Wei Yuan, Yifei Hu, Bin Wen, Tianke Zhang, et al. EVLM: An Efficient Vision-Language Model for Visual Understanding. arXiv:2407.14177, 2024. 1, 2 [8] Liang Chen, Haozhe Zhao, Tianyu Liu, Shuai Bai, Junyang Lin, Chang Zhou, and Baobao Chang. An Image is Worth 1/2 Tokens After Layer 2: Plug-and-Play Inference Acceleration for Large Vision-Language Models. In ECCV, 2024. 1 [9] Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. Extending context window of large language models via positional interpolation. arXiv preprint arXiv:2306.15595, 2023. 4 [10] Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, et al. Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling. arXiv:2412.05271, 2024. 1, 2, 4, 5, 6 [11] Tri Dao. FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning. In ICLR, 2024. 4 [12] Matt Deitke, Christopher Clark, Sangho Lee, Rohun Tripathi, Yue Yang, Jae Sung Park, Mohammadreza Salehi, Niklas Muennighoff, Kyle Lo, Luca Soldaini, et al. Molmo and PixMo: Open Weights and Open Data for State-of-theArt Vision-Language Models. In CVPR, 2025. 13 [13] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. 2407.21783, 2024. 4 [14] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Zhenyu Qiu, Wei Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, and Rongrong Ji. Mme: comprehensive evaluation benchmark for multimodal large language models. ArXiv, abs/2306.13394, 2023. [15] Chaoyou Fu, Yuhan Dai, Yongdong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, et al. Video-MME: The FirstEver Comprehensive Evaluation Benchmark of Multi-modal LLMs in Video Analysis. In CVPR, 2025. 5 [16] Junqi Ge, Ziyi Chen, Jintao Lin, Jinguo Zhu, Xihui Liu, Jifeng Dai, and Xizhou Zhu. V2pe: Improving multimodal long-context capability of vision-language models with variable visual position encoding. ArXiv, abs/2412.09616, 2024. 2 [17] Drew Hudson and Christopher Manning. GQA: New Dataset for Real-World Visual Reasoning and Compositional Question Answering. In CVPR, 2019. 5, 13 [18] Aniruddha Kembhavi, Mike Salvato, Eric Kolve, Minjoon Seo, Hannaneh Hajishirzi, and Ali Farhadi. Diagram Is Worth Dozen Images. In ECCV, 2016. 5 [19] Kyutai. Helium1: modular and multilingual LLM, 2025. 5, [20] Hugo Laurencon, Andres Marafioti, Victor Sanh, and Building and better understanding insights and future directions. Leo Tronchon. vision-language models: arXiv:2408.12637, 2024. 13 [21] Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Fanyi Pu, Joshua Adrian Cahyono, Jingkang Yang, Chunyuan Li, and Ziwei Liu. Otter: Multi-Modal Model with In-Context Instruction Tuning. IEEE TPAMI, 2025. 2 [22] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models. In International Conference on Machine Learning (ICML), 2023. 1, 2, 8, 14 [23] Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, et al. MVBench: Comprehensive Multi-modal Video Understanding Benchmark. In CVPR, 2024. 2, 5 [24] Xinhao Li, Yi Wang, Jiashuo Yu, Xiangyu Zeng, Yuhan Zhu, Haian Huang, Jianfei Gao, Kunchang Li, Yinan He, Chenting Wang, Yu Qiao, Yali Wang, and Limin Wang. VideoChat-Flash: Hierarchical Compression for Long-Context Video Modeling. arXiv:2501.00574, 2024. 1, 2, [25] Jihao Liu, Zhiding Yu, Shiyi Lan, Shihao Wang, Rongyao Fang, Jan Kautz, Hongsheng Li, and Jose Alvare. StreamChat: Chatting with Streaming Video. arXiv:2412.08646, 2024. 1, 2, 3, 6, 7 [26] Yuliang Liu, Zhang Li, Mingxin Huang, Biao Yang, Wenwen Yu, Chunyuan Li, Xu-Cheng Yin, Cheng-Lin Liu, Lianwen Jin, and Xiang Bai. OCRBench: On the Hidden Mystery of OCR in Large Multimodal Models. Science China Information Sciences, 67(12), 2024. 5 [27] Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Shahbaz Khan. Video-ChatGPT: Towards Detailed Video Understanding via Large Vision and Language VideoLLaMA 3: Frontier Multimodal Foundation Models for Image and Video Understanding . In Proceedings of the Association for Computational Linguistics (ACL), 2024. 2, 8 [28] Andres Marafioti, Orr Zohar, Miquel Farre, Merve Noyan, Elie Bakouch, Pedro Cuenca, Cyril Zakka, Loubna Ben Allal, Anton Lozhkov, Nouamane Tazi, et al. SmolVLM: Redefining small and efficient multimodal models. arXiv:2504.05299, 2025. 2, 5, 6 [29] Ahmed Masry, Xuan Long Do, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. ChartQA: Benchmark for Question Answering about Charts with Visual and Logical Reasoning. In Findings of the association for computational linguistics: ACL, 2022. 5 [30] Minesh Mathew, Dimosthenis Karatzas, and CV Jawahar. In DocVQA: Dataset for VQA on Document Images. WACV, 2021. 5 [31] Minesh Mathew, Viraj Bagal, Rub`en Tito, Dimosthenis InfographKaratzas, Ernest Valveny, and CV Jawahar. icVQA. In WACV, 2022. 5 [32] Ahmed Nassar, Andres Marafioti, Matteo Omenetti, Maksym Lysak, Nikolaos Livathinos, Christoph Auer, Lucas Morin, Rafael Teixeira de Lima, Yusik Kim, Said 10 SmolDocling: An ultra-compact visionGurbuz, et al. language model for end-to-end multi-modal document conversion. arXiv:2503.11576, 2025. 13 [33] Linke Ouyang, Yuan Qu, Hongbin Zhou, Jiawei Zhu, Rui Zhang, Qunshu Lin, Bin Wang, Zhiyuan Zhao, Man Jiang, Xiaomeng Zhao, et al. OmniDocBench: Benchmarking Diverse PDF Document Parsing with Comprehensive Annotation. In CVPR, 2025. 13 [34] Viorica Patraucean, Lucas Smaira, Ankush Gupta, Adria Recasens, Larisa Markeeva, Dylan Banarse, Skanda Koppula, Mateusz Malinowski, Yi Yang, Carl Doersch, et al. Perception Test: Diagnostic Benchmark for Multimodal Video Models. In NeurIPS, 2023. [35] Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. Yarn: Efficient context window extension of large language models. arXiv preprint arXiv:2309.00071, 2023. 4 [36] Rui Qian, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Shuangrui Ding, Dahua Lin, and Jiaqi Wang. Streaming Long In Video Understanding with Large Language Models. NeurIPS, 2024. 1, 3 [37] Amelie Royer, Moritz Bohle, Gabriel de Marmiesse, Laurent Mazare, Neil Zeghidour, Alexandre Defossez, and Patrick Perez. Vision-Speech Models: Teaching Speech Models to Converse about Images. arXiv:2503.15633, 2025. 1, 2 [38] Wenzhe Shi, Jose Caballero, Ferenc Huszar, Johannes Totz, Andrew Aitken, Rob Bishop, Daniel Rueckert, and Zehan Wang. Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network. In CVPR, 2016. 2 [39] Amanpreet Singh, Vivek Natarjan, Meet Shah, Yu Jiang, Xinlei Chen, Devi Parikh, and Marcus Rohrbach. Towards VQA Models That Can Read. In CVPR, 2019. 5 [40] Krishna Srinivasan, Karthik Raman, Jiecao Chen, Michael Bendersky, and Marc Najork. WIT: Wikipedia-based Image Text Dataset for Multimodal Multilingual Machine LearnIn Proceedings of the 44th international ACM SIGIR ing. conference on research and development in information retrieval, 2021. 13 [41] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is All you Need. In NeurIPS, 2017. 1, 3 [42] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-VL: Enhancing VisionLanguage Models Perception of the World at Any Resolution. arXiv:2409.12191, 2024. 1, 2, 4, 5, [43] Yi Wang, Xinhao Li, Ziang Yan, Yinan He, Jiashuo Yu, Xiangyu Zeng, Chenting Wang, Changlian Ma, Haian InternVideo2.5: EmpowerHuang, Jianfei Gao, et al. ing Video MLLMs with Long and Rich Context Modeling. arXiv:2501.12386, 2025. 1, 2, 3, 8 [44] Hongchen Wei and Zhenzhong Chen. Visual context window extension: new perspective for long video understanding. In ACM MM, 2025. 4 [45] Luis Wiedmann, Orr Zohar, Amir Mahla, Xiaohan Wang, Rui Li, Thibaud Frere, Leandro von Werra, Aritra Roy Gosthipaty, and Andres Marafioti. FineVision: Open Data Is All You Need, 2025. 5, 12 [46] xAI. Grok-1.5 vision preview, 2024. Dataset obtained at https://huggingface.co/datasets/lmmslab/RealWorldQA. 5 [47] Ruyi Xu, Guangxuan Xiao, Yukang Chen, Liuning He, StreamingVLM: Infinite Video Streams. Kelly Peng, Yao Lu, and Song Han. Real-Time Understanding for arXiv:2510.09608, 2025. 3 [48] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv:2505.09388, 2025. 4 [49] Jiabo Ye, Anwen Hu, Haiyang Xu, Qinghao Ye, Ming Yan, Guohai Xu, Chenliang Li, Junfeng Tian, Qi Qian, Ji Zhang, et al. UReader: Universal OCR-free Visuallysituated Language Understanding with Multimodal Large Language Model. In Findings of the Association for Computational Linguistics: EMNLP 2023, 2023. 13 [50] Jiabo Ye, Haiyang Xu, Haowei Liu, Anwen Hu, Ming Yan, Qi Qian, Ji Zhang, Fei Huang, and Jingren Zhou. mPLUGOwl3: Towards Long Image-Sequence Understanding in In ICLR, 2025. 1, Multi-Modal Large Language Models. 2, 3, 5, 6, 8 [51] Boqiang Zhang, Kehan Li, Zesen Cheng, Zhiqiang Hu, Yuqian Yuan, Guanzheng Chen, Sicong Leng, Yuming Jiang, Hang Zhang, Xin Li, et al. VideoLLaMA 3: Frontier Multimodal Foundation Models for Image and Video Understanding. arXiv:2501.13106, 2025. 1, 2, 4, 5, 6 [52] Hang Zhang, Xin Li, and Lidong Bing. Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding. arXiv:2306.02858, 2023. 2, [53] Haoji Zhang, Yiqin Wang, Yansong Tang, Yong Liu, Jiashi Feng, and Xiaojie Jin. Flash-VStream: Efficient Real-Time Understanding for Long Video Streams. In ICCV, 2025. 1, 3 [54] Kaichen Zhang, Bo Li, Peiyuan Zhang, Fanyi Pu, Joshua Adrian Cahyono, Kairui Hu, Shuai Liu, Yuanhan Zhang, Jingkang Yang, Chunyuan Li, et al. Lmms-eval: Reality check on the evaluation of large multimodal models. In Findings of the Association for Computational Linguistics: NAACL 2025, pages 881916, 2025. 6 [55] Liang Zhang, Anwen Hu, Haiyang Xu, Ming Yan, Yichen Xu, Qin Jin, Ji Zhang, and Fei Huang. TinyChart: Efficient chart understanding with program-of-thoughts learning and visual token merging. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 18821898, 2024. 13 [56] Pan Zhang, Xiaoyi Dong, Yuhang Cao, Yuhang Zang, Rui Qian, Xilin Wei, Lin Chen, Yifei Li, Junbo Niu, Shuangrui Ding, et al. InternLM-XComposer2.5-OmniLive: Comprehensive Multimodal System for Long-term Streaming Video and Audio Interactions. arXiv:2412.09596, 2024. 3 [57] Yuanhan Zhang, Jinming Wu, Wei Li, Bo Li, Zejun Ma, Ziwei Liu, and Chunyuan Li. Video Instruction Tuning With Synthetic Data, 2024. 5, 6, 12, 13, 16 11 A. Implicit Gating in CASA [58] Bo Zhao, Boya Wu, Muyang He, and Tiejun Huang. SVIT: Scaling up Visual Instruction Tuning. arXiv:2307.04087, 2023. 13 [59] Junjie Zhou, Yan Shu, Bo Zhao, Boya Wu, Zhengyang Liang, Shitao Xiao, Minghao Qin, Xi Yang, Yongping Xiong, Bo Zhang, et al. MLVU: Benchmarking Multi-task Long Video Understanding. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 13691 13701, 2025. 5 Figure 7. Average attention scores in CASA layers (log scale). Complementing our ablation study in Table 8, we visualize the average CASA attention pattern found in CASA layer for trained CASA Qwen2.5-VL model. The attention scores are averaged across heads, layers, and for multiple input images of resolution 896896. In detail, we show the attention of query tokens to 1) the vision start and end tokens of the Qwen2.5-VL model, 2) image tokens, grouped and averaged by 16, 3) the average across other text tokens, and 4) the query itself. The attention of query to itself is by far the strongest, consistent with the detrimental effect we found when ablating this component in Table 8. The average attention of the query to other text tokens is also higher than to any image tokens, further highlighting the implicit balancing of image and text tokens contributions naturally occurring in the attention operation of CASA layers. To complement the ablation results of Table 8, we visualize the average CASA attention distribution of trained CASAQwen2.5-VL model, aggregated over heads, layers and multiple high-resolution images, in Figure 7. The plot shows that query tokens exhibit pronounced attention-toself, several orders of magnitude larger than their attention to any individual image token, as well as high attention to the other text tokens. This pattern reinforces our finding that the self-attention component acts as an implicit gating mechanism in CASA, balancing the contribution of the image and text tokens in the attention operation. B. Experimental Details B.1. Training Details Image data. We train on FineVision [45], subset of LLaVA-OneVision-1.5 [3], and LLaVA-Video-178K [57] datasets. LLaVA-Video-178K is video instruction-tuning dataset comprising 1.3M questionanswer pairs over 178K videos ranging from 0 to 3 minutes, annotated with open-ended and multiple-choice questions generated using GPT-4o and human input. FineVision and LLaVA-OneVision-1.5 are curated collections of publicly available imagetext datasets with 24M images covering captioning, chart reading, grounding and counting, mathematics, document understanding and OCR, 12 Figure 8. Insertion vs. Cross-Attention vs. CASA. Similar to our ablation experiments (right, same as Fig. 1), we find (left) that the current state-of-the-art cross-attention-based models (mPlug-Owl3) lag behind insertion-based models (here, Qwen2.5 3B), particularly on tasks that require high degree of visual detail (cid:219). This holds true even when significantly scaling up the mPlug-Owl3 model (2B8B). and general VQA. In FineVision, we replace Doclingmatix [32] with Docmatix [20] and over-sample it by factor of 6. FineVision and LLaVA-OneVision-1.5 overlap substantially, so we retain only subset of LLaVAOneVision-1.5 datasets not already included in FineVision: OmniDocBench [33], allenai-pixmo-docs [12], amc-aime, aops-forum, arxiv-figs, diagram, GQA [17], infographic-azuregpt4v, invoices, latex-ocr, llava-cot-100k, llava-wild, llrv-gpt4v, olympiads, oroikon-chart-captioning, rootsautomation, sherlock, SVIT [58], TinyChart [55], UReader-chart [49], UReader-ocr [49], UReader-tr [49], viquae, vision-oritented, visual-chat, and WIT [40]. We process images at their native resolution using the Qwen 2.5-VL. Note that Qwen 2.5-VLs visual encoder applies pixel unshuffling, reducing the number of visual tokens by factor of 4: For instance, 8962 image yields 1024 image tokens, while 4482 frame yields 256. During training, we use image resolution up to 8962 pixels (6722 when training with full token insertion due to memory constraints), and up to 4482 pixels for video frames, to compensante for the higher number of images. Video data. In LLaVA-Video-178K, videos are up to 3 minutes long; we sample frames at 2 fps, except for clips shorter than 10 s, for which we uniformly sample 20 frames. Training schedule. Training is performed on 64 NVIDIA H100 GPUs with 2 gradient accumulation steps. Our VLM training starting from Helium1-2B takes 3 days, and our adaptation of Qwen2.5-VL-3B takes 2 days for the imageonly training stage, and 1 day for image-video training. Each GPU processes either sequence of image samples (with multimodal sequence packing, as described in Section 3.3) or single video sample. More specifically, for each batch, we sample either packed image sequence consisting of multiple question-answer pairs from the image training data (see above) or single video sample, at ratio 3:1 (image:video). The packed image sequence is limited by maximum number of text and image tokens, specified in Table 10, which also reports the trained parameters for each training stage. Optimization. We use standard cross-entropy-based nexttoken-prediction loss applied only to the answer tokens of given question-answer pair. We use Adam with constant learning rate schedule apart from linear warmup and 4 for training new padecay. The learning rate is set to 10 rameters, and 10 5 for adapting existing parameters. B.2. Architecture Details RoPE in CASA. We apply the language backbones RoPE implementation within CASA layers, i.e. uni-modal RoPE for Helium1-2B and multi-modal RoPE for Qwen2.5-VL. Multimodal sequence packing. When training full token insertion models with sequence packing, we employ blockwise attention to guarantee that the self-attention operations are properly masked. Consequently, each question-answer sample in the packed sequence only attends to itself without carrying over any textual context from preceding samples in the sequence. This makes the procedure equivalent to batched training, while being more efficient as it avoids padding samples to the maximum sequence length within the batch. Attention windows in CASA. As detailed in Section 3.3 and Figure 3, the attention operation in CASA layers acts in local attention windows, which are naturally delimited by image occurrences: Each window consists of single image (or multiple consecutive images) followed by the associated text. Consequently, (i) during text-image training with packed multimodal sequences, CASA windows consist of question-answer pairs and their associated image(s), (ii) when training on LLaVA-Video-178K [57], the entire"
        },
        {
            "title": "Trained params",
            "content": "#Tokens Image encoder (4 last blocks) LLM #Steps"
        },
        {
            "title": "Text",
            "content": "Helium1-2B"
        },
        {
            "title": "Image stage",
            "content": "Qwen2.5-VL Image training Image-video training LiveCC training 100k 20,4801 20481 65k 15k 20k 20,480 46,080 30,720 2048 3072 1For insertion-based models, we use 10,240 image tokens, 1024 text tokens, and image size 672 instead of 896 due to memory constraints. Table 10. Training configurations. Apart from newly introduced parameters (i.e., CASA layers, Q-Former [22]) which are always trained at base learning rate of 104, previously existing parameters (i.e., the image encoder and pretrained language model) are either frozen or trained with learning rate of 105. For all experiments (except smaller-scale ablations), we train on 64 NVIDIA H100 GPUs with 2 gradient accumulation steps, i.e. the maximum number of tokens per gradient update is 128 #{Image + text tokens}. video along with the corresponding question-answer pair are contained in single window, (iii) when training on LiveCC [6], each window consists of single frame (extracted at 2fps) and the corresponding closed captions for this timestamp, which is typically only few tokens long. Nevertheless, the global coherence of the entire video script is preserved through the text-only tokens interactions in the self-attention layers. and"
        },
        {
            "title": "VISION START",
            "content": "Chat template. For Qwen2.5-VL, we use the models provided chat template, which includes preand post-image tokens ( ), userassistant turn delimiters, and system prompt; we simply omit the insertion of image-token placeholders when training CASA. For Helium1-2B, we use minimal template in which user and assistant turns are wrapped with their respective start and end tokens, without any preor post-image tokens."
        },
        {
            "title": "VISION END",
            "content": "Image processing. As we rely on the vision encoder of Qwen2.5-VL for all of our models, we apply the corresponding preprocessing for images (patch size of 14, flattening, etc.). Note that for videos, however, we do not use Qwen2.5-VLs video-processing strategy of temporally downscaling the video frames by factor of 2, as we found it detrimental in our experiments. Instead, we process videos frame per frame in the same way as images. C. Additional Results C.1. Cross-Attention vs Insertion VLMs Figure 8 compares insertion-based, cross-attention-based, and CASA architectures across diverse set of visionlanguage benchmarks, both trained in our controlled ablation setting (left, same as Figure 1) as well as for models found in the literature (right, including our CASA-based variant of Qwen2.5VL). Both figures are consistent: we observe that insertionbased models, exemplified by Qwen2.5VL 3B, maintain clear advantage over current state-of-the-art cross-attentionbased approaches (mPLUG-Owl3), especially on tasks that demand fine-grained visual understanding. Notably, this performance gap persists even when the cross-attention model is scaled substantially from 2B to 8B parameters. In contrast, we find that CASA is able to significantly close the gap to insertion-based models, whilst maintaining the benefits of cross-attention designs during inference. C.2. Efficiency Analysis In Table 11, we evaluate the memory and computation costs of CASA and token insertion in controlled setting. Specifically, we aim to measure the cost associated to the different design choices that differentiate CASA from full token insertion: (i) not passing image tokens through FFNs, (ii) using only text tokens as queries in CASA layers and (iii) at inference, only attending to the latest image inside the CASA layers as opposed to inserting it in the text streams. We rely on FlashAttention as an efficient implementation of all attention operations, and we employ the Pytorch profiler to measure (i) the average time spent on GPU per call, (ii) the total max memory allocated and (iii) the total user time. Importantly, we observe that the core bottleneck of all designs is the cost of passing through the FFNs, which typically consumes 1030 more memory than the attention computation. Consequently, the dominant computational benefit of CASA comes from not sending image tokens through FFNs, as this yields 4 memory reduction in our setting (1024 image tokens, 50 text tokens, 10 windows). This is consistent with our training setup, where full token insertion required halving the sequence lengths and reducing the maximum image resolution from 8962 to 6722 to fit within the memory constraints of an NVIDIA H100 GPU. As discussed in Table 5, we note that updating the image embeddings through the FFNs can in fact bring performance improvements to CASA, but, as Table 11 shows, this comes at significant additional computational costs. Additionally, CASAs forward pass through the FFNs is 16 faster during training, as it does not process image tokens. However, as shown in Table 1, the insertion based model did not train more slowly than CASA, owing to the shorter sequence lengths and lower image resolution used 14 # Tokens: 50 / 1024 (image / text)"
        },
        {
            "title": "CASA",
            "content": "# Windows:"
        },
        {
            "title": "Applied to image token",
            "content": "Memory (GB) Total time (ms) GPU Time per call (ms) Memory (GB) Total time (ms) GPU Time per call (ms) SA 0.12 0.39 0. 0.08 154 0."
        },
        {
            "title": "CASA",
            "content": "1.33 3.95 3.91 0.36 61 61 1.45 4.34 4.28 0.44 215 61.3 0.04 0.23 0.21 0.01 83 0.22 SA 0.01 0.22 0.18 0.004 83 0."
        },
        {
            "title": "FFN",
            "content": "0.33 0.24 0.20 0.36 62 62 Total CASA Total CASA 0.38 ( 0.69 ( 0.59 ( 74%) 84%) 86%) 0.38 ( 14%) 228 (+ 6%) 62.4 (+ 2%) 0.37 ( 0.47 ( 0.41 ( 74%) 89%) 90%) 0.38 ( 14%) 145 ( 33%) 62.2 (+ 1%) Table 11. Efficiency analysis of CASA and token insertion for single layer. We use packed sequence of 10 samples, each with 1024 image tokens and 50 text tokens, and average measurements across 100 calls. We report results at training, processing the whole sequence all at once, as well as for generating the entire sequence auto-regressively at inference. In particular, we find that (1) as CASA does not update the image embeddings through the FFNs, it yields substantial memory gains compared to token insertion and (2) given that image tokens are only used as keys and values in the attention computations, not as queries in CASA layers, further memory gains are obtained compared to the self-attention layer when performing token insertion. High-res Document/Chart Understanding Scene Text Understanding Knowledge / General QA CHARTQA DOCVQA INFOVQA OCRBENCH TEXTVQA REALWORLDQA AI2D GQA MME InsertionHe-2B CASA He-2B 448 672 896 448 672 896 1344 78.4 81.4 81.6 81.6 70.1 73.6 73.4 73.6 73.6 86.7 89.1 88.0 66.7 79.3 83.7 81.5 43.5 56.2 61.8 62. 36.0 44.1 48.6 48.1 684 719 728 740 670 714 723 738 68.6 74.3 75.5 75.6 64.0 69.0 71.0 71.3 57.1 59.5 59.9 57. 54.5 57.6 58.3 56.7 68.0 67.7 67.7 67.6 63.5 63.3 63.3 63.0 55.5 55.5 55.5 55.5 54.4 54.6 54.6 54.6 1744 1738 1732 1535 1557 1572 1568 Table 12. Impact of image resolution at evaluation. We evaluate our CASA He-2B and InsertionHe-2B models, for different input image resolutions. For comparison, in Table 2, these models are evaluated at their training resolution (respectively 8962 and 6722 max resolution) when training the insertion model, which also reduces computational cost of the image encoders forward pass. Compared to CASA, CASA is around 30% faster at both training and inference, as each attention block contains either CASA layer or self-attention layer, whereas CASA runs through both and sums their outputs, doubling the overall number of attention layers. C.3. Impact of image resolution In Table 2, we report results on VLM benchmarks for CASAHe-2B and InsertionHe-2B evaluated at maximum resolutions of 8962. In Table 12, we additionally show results when varying the maximum resolution at inference (4482, 6722, 8962, and 13442). Both models benefit from higher resolutions. In particular, although InsertionHe-2B was trained with maximum resolution of 6722, evaluating it at 8962 yields performance boost, especially on reading and high resolution document and chart understanding tasks (e.g., +5 points on InfoVQA). C.4. Training Speed In Figure 9, we the performance of our CASAQwen2.5-VL model, adapted from frozen Qwen2.5-VL report Figure 9. Performance across training for CASA different benchmark groups HRE, OCR, and VQA. Q2.5-VL for the backbone, evaluated at different epochs. For readability, we gather benchmark results into three groups: high-resolution document and chart reading (HRES), reading in natural images (OCR), and general VQA (VQA). Interestingly, we find that adapting the pretrained VLM backbone to use CASA layers instead of token insertion is very fast: Most performance gains occur within the first 20,000 training steps, while the rest of the training allows us to more closely match the pretrained backbone. 15 D. Live Video Captioning We also experiment with the task of streaming video captioning. To that end, we finetune the CASAQwen2.5-VL model on the recent LiveCC dataset [6]. Specifically, following [6], we train on mix of Live-Whisper-526K [6] and LLaVA-Video-178K [57] for up to 20,000 steps at batch size of 64 and 2 gradient accumulation steps. Qualitative evaluation. First, we showcase set of qualitative videos with caption subtitles generated with our CASAQwen2.5-VL-LiveCC. The resulting videos can be found on our project page, and for static visualization of video excerpts, see Figure 4 and Figure 10. Qualitative comparison to token insertion. To further assess the inference costs of CASA vs. token insertion, we also finetune Q-64He-2b model on LiveCC, where Q-64 denotes token insertion baseline trained with Q-Former that compresses image tokens into 64 learned queries. We select Q-64 as practical trade-off: As illustrated in Figure 6, higher token counts exhaust GPU memory and accumulate delay too quickly to generate long videos. On our project page, we provide qualitative video samples along with the memory cost incurred by token insertion, which grows rapidly over time. For instance on 40 seconds videos, the Q-64 GPU memory usage increases by 1GB, while CASAs stays almost constant. In addition, as the KV cache also quickly grows as the image tokens are inserted, the Q-64 model rapidly hits the base context length of the Helium1 backbone, leading to more generation artifacts. Thus, even with low number of tokens per image, token-insertion requires base model with high context length, or context extension techniques, to handle videos. In contrast, the memory cost of CASA grows very slowly when applied to live video captioning, and the context length is independent of the number of frames. 16 [00:000.0s] This video shows [00:010.0s] 16 [00:020.0s] -year [00:030.0s] -old female [00:040.0s] [00:090.0s] [00:140.0s] hunting [00:150.0s] [00:120.0s] [00:160.0s] [00:100.0s] [00:110.0s] [00:130.0s] [00:170.0s] [00:050.0s] American [00:060.0s] [00:190.0s] [00:180.0s] [00:200.0s] and [00:070.0s] [00:080.0s] alligator [00:000.0s] This [00:090.0s] [00:100.0s] video shows [00:010.0s] [00:120.0s] [00:110.0s] lion [00:020.0s] [00:030.0s] [00:040.0s] [00:050.0s] [00:060.0s] [00:070.0s] [00:080.0s] hunting [00:130.0s] [00:140.0s] [00:150.0s] [00:16-0.3s] [00:170.0s] [00:180.0s] [00:190.0s] calf [00:200.0s] [00:000.0s] [00:080.0s] This [00:090.0s] video [00:100.0s] shows [00:110.0s] you [00:120.0s] [00:010.0s] the [00:020.0s] [00:130.0s] that [00:140.0s] [00:030.0s] [00:040.0s] [00:050.0s] [00:060.0s] [00:070.0s] car [00:150.0s] [00:160.0s] [00:170.0s] [00:180.0s] [00:190.0s] [00:200.0s] Figure 10. Live captioning examples Q-64. Similar to Figure 4 we show excerpts of video generated by Q-Former (with 64 tokens per image) insertion based model. For full video samples, see the project page. 17 [00:000.0s] This video shows [00:010.0s] pangolin [00:020.0s] [00:09-0.4s] tubulidae family. the [00:080.0s] [00:180.0s] [00:19-0.0s] mammal. [00:200.0s] [00:100.0s] [00:03-0.0s] moving around. [00:040.0s] [00:050.0s] Its [00:060.0s] [00:11-0.0s] Its [00:120.0s] [00:13-0.1s] reptile. [00:140.0s] [00:150.0s] Its [00:160.0s] [00:070.0s] member of [00:170.0s] [00:000.0s] This video shows [00:01-0.1s] lioness [00:020.0s] [00:080.0s] oness is looking for [00:18-0.0s] [00:09-0.2s] Kalahari Desert. [00:19-0.0s] food for her [00:200.0s] [00:100.0s] [00:030.0s] and her cubs [00:040.0s] [00:05-0.1s] searching for [00:060.0s] [00:11-0.0s] Its [00:120.0s] [00:13-0.1s] hot summer day. [00:140.0s] [00:150.0s] The [00:160.0s] [00:070.0s] food in the [00:17-0.5s] li- [00:000.0s] This video shows [00:010.0s] 10 [00:020.0s] [00:110.0s] two -story [00:120.0s] [00:100.0s] [00:03-0.6s] ,000 square foot [00:040.0s] [00:05-0.1s] apartment. [00:060.0s] [00:130.0s] apartment [00:140.0s] [00:150.0s] building, [00:160.0s] [00:17-0.0s] and [00:180.0s] [00:070.0s] Its [00:080.0s] [00:090.0s] [00:19-1.2s] its 10 ,000 square [00:20-0.5s] Figure 11. Live captioning examples CASA. Similar to Figure 4 we show excerpts of video generated by CASA model. For full video samples, see the project page."
        }
    ],
    "affiliations": [
        "Kyutai"
    ]
}