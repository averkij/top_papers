{
    "paper_title": "V-GameGym: Visual Game Generation for Code Large Language Models",
    "authors": [
        "Wei Zhang",
        "Jack Yang",
        "Renshuai Tao",
        "Lingzheng Chai",
        "Shawn Guo",
        "Jiajun Wu",
        "Xiaoming Chen",
        "Ganqu Cui",
        "Ning Ding",
        "Xander Xu",
        "Hu Wei",
        "Bowen Zhou"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Code large language models have demonstrated remarkable capabilities in programming tasks, yet current benchmarks primarily focus on single modality rather than visual game development. Most existing code-related benchmarks evaluate syntax correctness and execution accuracy, overlooking critical game-specific metrics such as playability, visual aesthetics, and user engagement that are essential for real-world deployment. To address the gap between current LLM capabilities in algorithmic problem-solving and competitive programming versus the comprehensive requirements of practical game development, we present V-GameGym, a comprehensive benchmark comprising 2,219 high-quality samples across 100 thematic clusters derived from real-world repositories, adopting a novel clustering-based curation methodology to ensure both diversity and structural completeness. Further, we introduce a multimodal evaluation framework with an automated LLM-driven pipeline for visual code synthesis using complete UI sandbox environments. Our extensive analysis reveals that V-GameGym effectively bridges the gap between code generation accuracy and practical game development workflows, providing quantifiable quality metrics for visual programming and interactive element generation."
        },
        {
            "title": "Start",
            "content": "V-GameGym: Visual Game Generation for Code Large Language Models Wei Zhang1, Jack Yang, Renshuai Tao3, Lingzheng Chai, Shawn Guo, Jiajun Wu, Xiaoming Chen4, Ganqu Cui1*, Ning Ding1, Xander Xu2, Hu Wei2*, Bowen Zhou1* 1Shanghai AI Lab; 2Alibaba Group; 3Beijing Jiaotong University; 4AIStrong; 5 2 0 2 4 2 ] . [ 1 6 3 1 0 2 . 9 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Code large language models have demonstrated remarkable capabilities in programming tasks, yet current benchmarks primarily focus on single modality rather than visual game development. Most existing code-related benchmarks evaluate syntax correctness and execution accuracy, overlooking critical game-specific metrics such as playability, visual aesthetics, and user engagement that are essential for real-world deployment. To address the gap between current LLM capabilities in algorithmic problemsolving and competitive programming versus the comprehensive requirements of practical game development, we present V-GameGym, comprehensive benchmark comprising 2,219 high-quality samples across 100 thematic clusters derived from real-world repositories, adopting novel clustering-based curation methodology to ensure both diversity and structural completeness. Further, we introduce multimodal evaluation framework with an automated LLM-driven pipeline for visual code synthesis using complete UI sandbox environments. Our extensive analysis reveals that V-GameGym effectively bridges the gap between code generation accuracy and practical game development workflows, providing quantifiable quality metrics for visual programming and interactive element generation."
        },
        {
            "title": "Introduction",
            "content": "Recent advances in code large language models (code LLMs) have demonstrated remarkable capabilities in programming tasks, building upon foundational models such as Qwen-Coder (Hui et al., 2024), StarCoder (Li et al., 2023; Lozhkov et al., 2024b), and DeepSeek-Coder (Guo et al., 2024b), establishing strong baselines for code generation (Chen et al., 2021; Zhuo et al., 2024; Liu et al., 2024b), completion (Yang et al., 2024b), and understanding tasks (Lu et al., 2021). These LLMs * Corresponding Author. Figure 1: visual programming about the flappy bird style arcade game. adopt specialized training strategies combining pretraining on large code corpora from repositories like GitHub, followed by post-training to align outputs with programming best practices. The recent LLMs like Claude 4 and GLM4.5 (Anthropic, 2025; Zeng et al., 2025) exhibit enhanced reasoning capabilities for complex programming scenarios. Further, Kimi-K2 (Team et al., 2025) focuses on long-context code comprehension and generation. The focus of these advanced LLMs is not on solving algorithmic problems, but rather on visual programming to provide more intuitive demonstrations of model performance. The opensource community (Chen et al., 2023) has begun developing specialized evaluations for game generation tasks. Visual game synthesis (Tong et al., 2025) further advances this domain by incorporating multi-modal understanding to generate games with coherent visual and interactive elements. However, these approaches primarily focus on code generation accuracy and syntax correctness, overlooking critical game-specific evaluation metrics such as playability, visual aesthetics, user engagement, and performance optimization. The absence of comprehensive evaluation frameworks and targeted improvement methodologies limits the practical deployment of code LLMs in professional game development workflows. Figure 2: Overview of the V-GameGym framework from data collection to evaluation. In this work, we first introduce comprehensive benchmark, V-GameGym, comprising 2,219 high-quality samples across 100 thematic clusters derived from real-world Pygame repositories. The process begins by filtering Python source files from large open-source repositories (OpenCoder and The Stack v2) to identify Pygamerelated projects, then applies clustering-based curation strategy that partitions the code corpus using high-dimensional feature vectors and selects the highest-quality program from each cluster based on structural completeness metrics. The curated seed dataset is then processed through an automated LLM-driven pipeline that analyzes code intent, transforms interactive programs into self-contained demonstrations, verifies execution in sandboxed environments with automated error correction, and generates natural language requirement specifications. Finally, the dataset undergoes human validation by 8 graduate students who manually check approximately 2,219 Pygame programs using complete UI sandbox environment to ensure code integrity and quality. The contributions are summarized as follows: (1) We propose V-GameGym comprised of 2,219 manually verified samples sourced from 2,190 distinct repositories, comprehensive code generation benchmark for evaluating multimodal game development capabilities, encompassing 100 clusters with diverse functional characteristics. (2) We introduce novel clustering-based curation methodology that combines high-dimensional feature extraction with quality-based selection, ensuring both diversity and structural completeness in the dataset. (3) We systematically construct multimodal evaluation framework with an automated LLM-driven pipeline for code transformation and requirement synthesis, validated through comprehensive human annotation involving 8 graduate students. Notably, extensive analysis reveals that V-GameGym effectively captures the complexity spectrum of realworld game development tasks with quantifiable quality metrics."
        },
        {
            "title": "2.1 V-GameGym Task Definition\nLet I and C denote the spaces of natural language\ninstructions and program source codes, respectively.\nThe model under evaluation, Mθ, is a generative\nmodel parameterized by θ that approximates the\nconditional probability distribution P (C|I) where\nI ∈ I and C ∈ C. The comprehensive process of\ngeneration and evaluation for a given instruction I\nis defined by the following sequence.",
            "content": "Code Generation code instance is sampled from the models output distribution: Pθ(I). Execution & Artifact Synthesis The generated code is executed by deterministic environment function : A, which synthesizes set of multimedia artifacts (V, S) A. Here, = represents the artifact space, composed of the video space and the image space S: (V, S) = E(C). Multimodal Scoring The quality of the generation is quantified by comprehensive scoring function by aggregating scores from multiple assessment modalities: Score(I, C, V, S) = (cid:88) 1k wk ϕk (1) where ϕ1(I, C), ϕ2(I, S), and ϕ3(I, V) represent modality-specific assessment functions for code, Figure 3: Correlation between model size and games solved. Figure 4: Overall Requirement Length Distribution Figure 6: Log-Scale Requirement Length Histogram Figure 5: Linear-Scale Requirement Length Histogram static visuals, and dynamic gameplay, respectively, and wk are their corresponding weights satisfying (cid:80) wk = 1. Score Distribution Metrics To provide granular insights into model performance patterns, we categorize each games final score into four quality bands. Excellent (80-100): Games demonstratFigure 7: Requirement Length Comparison by Cluster ing superior implementation quality with minimal issues. Good (60-80): Games with solid functionality and minor deficiencies. Fair (40-60): Games with basic functionality but notable limitations. Poor (0-40): Games with significant implementation failures or non-functional code."
        },
        {
            "title": "2.2 V-GameGym Construction",
            "content": "Data Collection Our raw data is sourced from two extensive, publicly available code corpora: OpenCoder (Huang et al., 2025) and The Stack v2 (Lozhkov et al., 2024a). To construct domainspecific dataset, we engineered high-throughput filtering pipeline. This pipeline leverages parallel processing to stream and analyze all Python source files, systematically isolating code that explicitly contains the pygame keyword. This procedure allowed us to efficiently distill targeted corpus of Pygame-related projects from the broader, more generalized repositories, forming the foundation for all subsequent curation steps. Clustering-based Curation To ensure the resulting dataset exhibits both high quality and functional diversity, we implemented rigorous curation strategy that can be described as formalized selection principle. The process first partitions the entire corpus based on high-dimensional feature representation and then selects the most structurally complete program from each partition. Let = {c1, c2, . . . , cn} be the initial corpus of code samples. Let v(c) Rd be the highdimensional feature vector extraction function described previously, which maps code sample to its quantitative fingerprint (encompassing size, structure, API usage, and semantics). Let Squality(c) be the scalar heuristic score that evaluates the structural completeness of program. The corpus is first partitioned into clusters, = {C1, C2, . . . , Ck}, using the MiniBatchKMeans algorithm on the feature vectors v(c). The final curated seed dataset, Dseed, is then constructed by selecting the single element from each cluster that maximizes the quality score Squality: Dseed = (cid:91) i=1 (cid:26) (cid:27) arg max cCi Squality(c) (2) the where MiniBatchKMeans(D, v, k). clusters Ci are the result of This selection principle, articulated in Equation 2, formally captures our two-stage methodology. The clustering operation partitions the dataset based on functional and structural similarity (as defined by v), thereby ensuring diversity. The subsequent arg max operation within each disjoint set Ci guarantees that the selected program is the most complete and runnable exemplar of that particular"
        },
        {
            "title": "General Statistics",
            "content": "Total Samples (Unique Games) Unique Source Repositories Unique Clusters"
        },
        {
            "title": "Value",
            "content": "2,219 2,190 100 Requirement Metrics (Average per game) Requirement Length (characters) Word Count Number of Sentences 1,210 178 9.6 Reference Code Metrics (Average per game) Lines of Code Code Length (characters) Number of Functions Number of Classes Execution & Quality Metrics"
        },
        {
            "title": "Execution Success Rate\nVideo Coverage\nAverage Images per Game\nAverage Recording Duration",
            "content": "257 8,533 2.8 2.4 100.0% 100.0% 9.9 10.0 Table 1: Overall Statistics of the V-GameGym Dataset. functional group, based on the quality heuristic: Squality(c) = (cid:88) Cstruct wf I(f c) + Slen(L(c)) This decoupling of the clustering metric from the selection metric is intentional. It allows us to group programs by rich definition of functional behavior (v) while applying simpler, targeted heuristic (Squality) to ensure each chosen sample meets minimum standard of structural integrity. Test Set Construction The seed dataset is then processed through an automated Language Model (Claude-Sonnet-4)-driven pipeline to construct the final test set, composed of (requirement, code) instruction pairs. This pipeline operationalizes closed-loop analyze-inject-validate-generate workflow. The process commences with an Intent Analysis stage, where the LLM parses the seed code to infer its core game mechanics and objectives. This is followed by Autonomous Interactive Behavior Injection stage, which refactors the original, often interactive, code into self-contained, autonomous demonstration that executes for fixed duration. The transformed artifact then undergoes Execution Verification within sandboxed environment. Any execution failures initiate Self-Correction loop, wherein the error logs are fed back to the LLM for automated debugging and regeneration. Upon successful validation, Requirement Generation module prompts the LLM to synthesize high-level, natural language requirement specification for the program, emulating the perspective of product manager. This rigorous process ensures that every entry in the final test Model Size Final Code Image Video Excellent Good Fair Poor gpt-5 o3 gpt-5-mini gemini-2.5-pro o4-mini gpt-4.1-2025-04-14 grok-4 gemini-2.5-flash chatgpt-4o-latest claude-sonnet-4-20250514-thinking claude-sonnet-4-20250514 o3-mini-2025-01-31 gpt-4o-mini-2024-07-"
        },
        {
            "title": "Proprietary LLMs",
            "content": "(cid:181) (cid:181) (cid:181) (cid:181) (cid:181) (cid:181) (cid:181) (cid:181) (cid:181) (cid:181) (cid:181) (cid:181) (cid:181) 45.0 44.8 43.5 43.5 43.0 42.5 42.0 42.0 41.2 40.5 40.2 38.2 33.9 96.6 92.3 96.7 89.1 87.8 91.8 83.9 92.8 82.5 90.3 87.7 89.3 70.4 17.6 20.2 15.7 19.1 19.8 17.6 19.8 16.5 19.9 14.4 15.7 11.9 15.5 400B+ Open-Weight LLMs Qwen3-Coder-480B-A35B-Instruct DeepSeek-V3-0324 DeepSeek-V3.1 DeepSeek-R1-0528 kimi-k2-0905-preview 32B/480B 37B/671B 37B/671B 37B/671B 32B/1000B 41.3 41.1 40.9 38.7 23.5 85.3 83.6 83.1 88.1 66.3 18.3 19.3 19.3 13.4 2.0 100B-400B Open-Weight LLMs Qwen3-235B-A22B-Thinking-2507 Qwen3-235B-A22B Qwen3-235B-A22B-Instruct-2507 GLM-4.5 GLM-4.5-Air gpt-oss-120b 22B/235B 235B 22B/235B 32B/355B 12B/106B 5.1B/117B 42.3 41.2 41.1 40.0 39.4 43.4 84.5 81.3 85.3 84.7 85.4 90.1 20.0 19.8 18.2 17.0 16.3 19.7 Qwen3-32B Seed-OSS-36B-Instruct Qwen3-30B-A3B-Thinking-2507 QwQ-32B Qwen3-30B-A3B Qwen3-Coder-30B-A3B-Instruct Qwen3-30B-A3B-Instruct-2507 DeepSeek-R1-Distill-Llama-70B Qwen2.5-72B-Instruct Qwen2.5-Coder-32B-Instruct DeepSeek-R1-Distill-Qwen-32B Qwen2.5-32B-Instruct gpt-oss-20b Qwen3-14B Qwen2.5-Coder-14B-Instruct DeepSeek-R1-Distill-Qwen-14B Qwen3-8B Qwen3-4B Qwen3-4B-Thinking-2507 Seed-Coder-8B-Instruct Llama-3.1-8B-Instruct Qwen2.5-Coder-7B-Instruct Qwen3-1.7B Llama-3.2-3B-Instruct deepseek-coder-7b-instruct-v1.5 Hunyuan-7B-Instruct Qwen2.5-Coder-3B-Instruct OpenCoder-8B-Instruct Llama-3.2-1B-Instruct DeepSeek-R1-Distill-Llama-8B Qwen3-0.6B Qwen2.5-Coder-0.5B-Instruct DeepSeek-R1-Distill-Qwen-7B DeepSeek-R1-Distill-Qwen-1.5B 30B-100B Open-Weight LLMs 32B 36B 3B/30B 32B 3B/30B 3B/30B 30B 70B 72B 32B 32B 32B 40.4 40.3 40.0 39.6 39.6 39.0 38.6 35.3 34.6 34.4 33.4 31.8 81.6 88.3 80.7 79.7 78.4 83.8 81.4 74.1 73.2 74.5 71.9 66.4 18.9 16.4 18.9 18.5 19.7 16.6 16.5 15.8 14.7 13.8 14.4 14. 10B-30B Open-Weight LLMs 3.6B/21B 14B 14B 14B 42.2 38.8 30.2 27.4 88.8 79.1 68.5 65.3 18.6 18.4 10.9 8.7 Below 10B Open-Weight LLMs 8B 4B 4B 8B 8B 7B 1.7B 3B 7B 7B 3B 8B 1B 8B 0.6B 0.5B 7B 1.5B 36.9 34.4 34.3 33.9 29.4 27.6 25.0 24.5 24.0 21.0 20.4 20.1 15.5 15.1 13.7 12.8 12.1 8.5 76.2 72.7 70.0 73.2 62.9 63.9 57.3 55.5 53.8 57.8 53.8 49.0 40.3 42.5 35.1 34.6 35.8 25.4 17.2 15.1 16.1 14.0 13.0 9.1 9.1 9.5 9.0 2.8 4.4 6.5 4.3 1.7 4.8 3.6 0.3 0.0 20.7 21.9 18.0 22.2 21.4 18.1 22.4 16.7 21.3 16.9 17.4 13.3 15.8 20.5 20.5 20.2 14.6 2. 22.4 22.6 19.7 18.3 16.5 20.3 20.6 16.2 20.4 20.6 20.7 16.7 17.8 16.0 15.9 14.9 13.9 15.1 19.2 18.8 11.2 8.3 17.3 15.5 16.8 14.4 12.3 9.7 8.7 8.5 9.2 2.2 2.9 4.7 1.9 1.0 1.1 0.0 0.1 0.0 83 65 61 45 36 47 21 28 25 36 36 26 4 20 22 25 32 22 14 16 31 23 52 8 25 13 10 9 22 11 4 3 9 0 2 31 9 0 1 5 1 2 4 1 0 0 0 0 0 0 0 0 0 0 0 0 0 288 341 236 337 313 263 327 252 305 204 207 204 134 287 311 296 174 322 302 308 216 230 324 274 234 279 268 274 226 223 188 174 167 145 127 299 245 87 77 187 144 168 137 84 69 43 46 38 12 10 7 4 6 3 0 0 0 676 686 655 672 682 641 650 634 613 624 616 417 459 644 638 611 544 695 668 589 661 533 634 642 585 589 610 597 543 571 448 449 425 411 389 626 567 327 198 546 464 465 422 319 250 216 210 229 59 85 152 57 26 56 39 3 1 1172 1127 1267 1165 1188 1268 1221 1304 1276 1355 1360 1572 1622 1268 1248 1287 1469 1180 1235 1306 1311 1433 1209 1295 1375 1338 1331 1339 1428 1414 1579 1593 1618 1663 1701 1263 1398 1804 1943 1480 1610 1584 1656 1815 1899 1959 1963 1952 2148 2124 2060 2158 2187 2160 2179 2216 2218 Table 2: Comprehensive Performance Evaluation, showing Final Score, Code Score, Image Score, Video Score, and score distribution. Bold indicate highest performance; underlined indicate second-highest performance. set is correct, executable, and paired with corresponding high-level description."
        },
        {
            "title": "2.3 Data Statistics Overview",
            "content": "Human Check and Annotation To ensure the quality of V-GameGym, 8 graduate students used UI sandbox and LLM assistance to verify nearly 2,219 Pygame code cases and their visual outputs. Data Statistics Table 1 presents comprehensive statistical overview of the V-GameGym dataset. The benchmark is substantial in scale, comprising 2,219 unique games sourced from 2,190 distinct repositories and organized into 100 thematic clusters. The complexity of the tasks is reflected in the metrics for both the natural language requirements and the reference code; on average, each games requirements consist of 178 words, while the corresponding reference code implementation spans 257 lines. Critically, the datasets high quality is underscored by 100% execution success rate and complete video coverage for all samples, ensuring its reliability for evaluation purposes. Analysis of V-GameGym Requirements Analysis of the requirement texts reveals highly rightskewed length distribution, as visualized in the violin plot and histogram (Figures 4, 5). This distribution is characterized by preponderance of concise specifications, evidenced by mean length (570) substantially exceeding the median (297), with 80% of texts falling under 1000 characters. On logarithmic scale, the distribution approximates log-normal form (Figure 6). Crucially, this length variation correlates with task type. box plot comparison across the top 10 requirement clusters (Figure 7) demonstrates significant heterogeneity, suggesting that the clustering effectively segments tasks by their underlying complexity or documentation style. Scaling Law of Visual Game Generation Figure 3, the results reveal statistically observable positive correlation between the count of model parameters and the performance of the task. The models with smaller parameters (0.5B-3B) exhibit consistently lower performance metrics, typically achieving fewer than 400 solved games, while intermediate-scale models (7B-32B parameters) demonstrate moderate performance ranges of 200600 solved games. Large-scale models (70B+ parameters) achieve superior performance outcomes, with solved game counts reaching 600-1000+. However, the data suggests that the relationship exhibits logarithmic characteristics rather than linear scaling, indicating diminishing marginal returns as parameter count increases. Significantly, the LLMs (e.g. gpt-oss-20b, Seed-OSS-36B-Instruct, and Qwen3-Coder-30B-A3B) with similar parameters suggest that architectural innovations, training methodologies, and algorithmic optimizations may constitute equally critical factors in achieving state-of-the-art performance. We can obtain the formula for model size and performance as: = log(N ) + B, where is the number of the resolved problems and is the number of model parameters (A = 127.2, = 135.6)."
        },
        {
            "title": "3 Experiment Setup",
            "content": "Experiment Code LLMs We evaluate all LLMs on Ubuntu 22.04, equipped with an Intel Xeon (R) Gold 6348 CPU @2.60GHz, eight NVIDIA H800 GPUs (80 GB), and 528 GB of memory. The software setup includes NVIDIA-SMI version 535.104.05 and CUDA 12.3. We set temperature to 0.0 for LLMs when inferring by sglang v0.5.1 (Sglang Team). Evaluated Models For comprehensive and thorough evaluation, we assess 70 widely used models, including both proprietary and opensource ones. For proprietary models, we evaluate series from leading labs such as OpenAIs GPT (e.g., gpt-5) (OpenAI, 2023, 2025a) and reasoning models (o3, o4-mini) (OpenAI, 2025c), Anthropics claude-sonnet-4 (Anthropic, 2025), Googles gemini-2.5 series (Google, 2025b,a), and xAIs grok-4 (xAI, 2025). For open-source models, our testing spans diverse range from major tech companies. This includes the extensive Qwen family (Hui et al., 2024; Qwen, 2025; Yang et al., 2025), ByteDances Seed series (Seed, 2025; Seed et al., 2025), Moonshot AIs Kimi-K2 (Team et al., 2025), various DeepSeek models (DeepSeek-AI et al., 2025b,a), Metas Llama series (Llama-3.1, Llama-3.2) (Grattafiori et al., 2024) and Zhipu AIs GLM models (Zeng et al., 2025). The evaluation also incorporates community and research-driven models like OpenAIs gpt-oss (OpenAI, 2025b) and OpenCoder (Huang et al., 2025). Judge Models We employ an LLM-as-Judge using Qwen3-Coder-480B-A35B-Instruct to evaluate code scores and Qwen2.5-VL-72B for image/video scores."
        },
        {
            "title": "4 Analysis",
            "content": "Main Result Note that the sum of distribution counts may not equal 2,219 for some models due to execution failures that prevent complete end-to-end evaluation pipeline completion. In Table 2, several important trends can be observed. Clear Performance Hierarchy Proprietary models generally lead, with GPT-5 topping the list at 45.0 points. Among open-source models, large-parameter models (400B+) perform best, such as Qwen3-Coder480B and the DeepSeek-V3 series, both exceeding 40 points. Imbalanced Capability Dimensions All models perform strongly in code generation (most over 70 points) but are generally weaker in image and video evaluation (most under 25 points), indicating that current models still have significant room for improvement in visual representation and dynamic effect generation. Pronounced Scale Effect Open-source models exhibit clear positive correlation between scale and performance, improving from an average of around 20 points for models under 10B to over 40 points for 400B+ models. Long-Tail Distribution of Quality Most generated games fall into the Poor and Fair categories, with limited samples reaching the Excellent standard, reflecting that high-quality game code generation remains challenging. Multi-Dimensional Capability Analysis of TopPerforming Models Figure 8 highlights distinct model capabilities. clear code-visual trade-off exists: GPT-5 excels at code (96.6) but is weak in vision (17.6/20.7), while o3 is more balanced and leads in image score (20.2). Notably, opensource models like gpt-oss-120b now rival proprietary systems in game development, narrowing the capability gap. Evaluation Score Distribution Across Task Dimensions Figure 9 shows clear hierarchy in AI capabilities for game development. Models excel at code evaluation, achieving high and varied scores, but struggle significantly with visual tasks like screenshot and video evaluations. The consistently low scores in these visual areas highlight major weakness in the models visual understanding and generation abilities. Game Difficulty Distribution Figure 10 presents typical right-skewed distribution, with most games concentrated in the low solution rate range (where only few models can solve them), indicating that the games in the test set are generally difficult. The peak on the left shows considerable number of games that no model could solve, while the number of simple games that could be solved by most models is small. This distribution is beneficial for distinguishing the capability differences among various models. Performance Analysis Across Game Difficulty Tiers In Figure 11, the difficulty tier analysis reveals that while all models experience performance degradation on harder games, the relative ranking between top-tier models remains remarkably stable across difficulty levels. This consistency validates the benchmarks discriminative power and suggests Figure 8: Radar chart comparing the top 10 models across four key performance dimensions. Figure 9: Distribution of evaluation scores across three key dimensions: Code, Screenshot, and Video. Figure 10: Game difficulty distribution by number of solving models. that superior models maintain their advantages regardless of task complexity. Evaluation Dimension Correlations In Figure 12, the correlation analysis shows moderate to strong positive correlations between all evaluation dimensions, indicating that models with superior code generation capabilities tend to also excel in visual assessment tasks. This suggests that game development requires integrated multimodal understanding rather than isolated technical skills. tasks, with foundational models like Qwen2.5/3Coder (Hui et al., 2024), Seed-Coder (Rozière et al., 2023), GLM-4.5 (Zeng et al., 2025), and Kimi-K2 (Team et al., 2025) excelling in general code generation and understanding. The success of multi-agent collaboration (Guo et al., 2024c; Wang et al., 2023a) inspires the use of languagespecific agent to formulate multilingual instruction dataset. Subsequently, instruction tuning (Ouyang et al., 2022; Zhang et al., 2023; Wang et al., 2023b) enhances the ability of the LLMs to generalize and follow instructions (Wang et al., 2023b; Chaudhary, 2023; Luo et al., 2023; Wei et al., 2023; Yu et al., 2023). series of code benchmarks is proposed to evaluate different aspects of the code LLMs, including realistic (Liu et al., 2024b; Zhuo et al., 2024; Zhang et al., 2025b) and multilingual scenarios (Cassano et al., 2023; Chai et al., 2024; Liu et al., 2024a; Zhang et al., 2025a). Game for Large Language Models The intersection of games and large language models (LLMs) has emerged as rich area of research encompassing multiple paradigms and applications. Early works established the potential of using game environments as training grounds for LLMs, and then they extended to more complex games (e.g. minecraft (Gong et al., 2024), social deduction games (Light et al., 2023; Wu et al., 2024), textbased adventure games (Guertler et al., 2025)). Subsequent research (Yao et al., 2025) has explored LLMs as players in various game contexts, from traditional board games requiring strategic reasoning to complex multiplayer online environments that demand natural language communication and coordination. The recent work KORGym (Shi et al., 2025) offers over fifty games in either textual or visual formats. But these benchmarks focus on text reasoning, ignoring the evaluation for the code large language model. In this work, we introduce V-GameGym to evaluate the coding capability of LLMs to create the visual games."
        },
        {
            "title": "6 Conclusion",
            "content": "Figure 11: Performance comparison of top 15 models across Easy, Medium, and Hard difficulty tiers, showing consistent ranking patterns and scaling challenges. Figure 12: Correlation matrix between Code, Screenshot, and Video evaluation dimensions, demonstrating the interdependence of multimodal capabilities in game development. Overall Correlation vs. Top-Tier Specialization While this positive correlation holds true across the entire model population, more nuanced picture emerges when examining the elite models, as highlighted in Figure 8. For instance, models like GPT-5 demonstrate specialization, achieving near-perfect code scores at the expense of comparatively lower visual scores, suggesting potential capability trade-off at the frontier of performance. This indicates that while foundational capabilities are interconnected, advanced models may adopt different strategies to allocate their reasoning budget, prioritizing either logical code structure or visual aesthetics."
        },
        {
            "title": "5 Related Work",
            "content": "Code Large Language Models Code-specific large language models (Code LLMs) (Li et al., 2023; Rozière et al., 2023; Guo et al., 2024a; Yang et al., 2024a,b) demonstrate remarkable performance in software engineering and agentic We introduce V-GameGym, multimodal benchmark for evaluating code LLMs in visual game generation. Built by curating 2,219 high-quality Pygame samples, our framework assesses both code generation and visual capabilities. Our evaluation of 70 models reveals significant performance gap between proprietary and open-source systems, with top models succeeding only 45%. The benchmark highlights critical limitations in visual understanding and dynamic gameplay generation, providing foundation for advancing AI-assisted game development."
        },
        {
            "title": "References",
            "content": "Anthropic. 2025. Claude 3.7 sonnet and claude code. Federico Cassano, John Gouwar, Daniel Nguyen, Sydney Nguyen, Luna Phipps-Costin, Donald Pinckney, Ming-Ho Yee, Yangtian Zi, Carolyn Jane Anderson, Molly Feldman, and 1 others. 2023. Multipl-e: scalable and polyglot approach to benchmarking neural code generation. IEEE Transactions on Software Engineering. Linzheng Chai, Shukai Liu, Jian Yang, Yuwei Yin, Ke Jin, Jiaheng Liu, Tao Sun, Ge Zhang, Changyu Ren, Hongcheng Guo, and 1 others. 2024. Mceval: Massively multilingual code evaluation. arXiv preprint arXiv:2406.07436. Sahil Chaudhary. 2023. Code alpaca: An instructionfollowing llama model for code generation. https: //github.com/sahil280114/codealpaca. Dake Chen, Hanbin Wang, Yunhao Huo, Yuzhao Li, and Haoyang Zhang. 2023. Gamegpt: Multi-agent collaborative framework for game development. arXiv preprint arXiv:2310.08067. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Pondé de Oliveira Pinto, Jared Kaplan, Harrison Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, and 39 others. 2021. Evaluating large language models trained on code. abs/2107.03374. DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, and 181 others. 2025a. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. Preprint, arXiv:2501.12948. DeepSeek-AI, Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Daya Guo, Dejian Yang, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, and 181 others. 2025b. Deepseek-v3 technical report. Preprint, arXiv:2412.19437. Ran Gong, Qiuyuan Huang, Xiaojian Ma, Yusuke Noda, Zane Durante, Zilong Zheng, Demetri Terzopoulos, Li Fei-Fei, Jianfeng Gao, and Hoi Vo. 2024. MindaIn Findings gent: Emergent gaming interaction. of the Association for Computational Linguistics: NAACL 2024, Mexico City, Mexico, June 16-21, 2024, pages 31543183. Association for Computational Linguistics. Google. 2025a. gemini 2.5-flash. Google. 2025b. gemini 2.5-pro. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad AlDahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, and 542 others. 2024. The llama 3 herd of models. Preprint, arXiv:2407.21783. Leon Guertler, Bobby Cheng, Simon Yu, Bo Liu, Leshem Choshen, and Cheston Tan. 2025. Textarena. arXiv preprint arXiv:2504.11442. Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen, Xiao Bi, Y. Wu, Y. K. Li, Fuli Luo, Yingfei Xiong, and Wenfeng Liang. 2024a. Deepseek-coder: When the large language model meets programming the rise of code intelligence. Preprint, arXiv:2401.14196. Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen, Xiao Bi, Wu, YK Li, and 1 others. 2024b. Deepseekcoder: When the large language model meets programmingthe rise of code intelligence. arXiv preprint arXiv:2401.14196. Taicheng Guo, Xiuying Chen, Yaqi Wang, Ruidi Chang, Shichao Pei, Nitesh V. Chawla, Olaf Wiest, and Xiangliang Zhang. 2024c. Large language model based multi-agents: survey of progress and challenges. CoRR, abs/2402.01680. Siming Huang, Tianhao Cheng, J. K. Liu, Jiaran Hao, Liuyihan Song, Yang Xu, J. Yang, Jiaheng Liu, Chenchen Zhang, Linzheng Chai, Ruifeng Yuan, Zhaoxiang Zhang, Jie Fu, Qian Liu, Ge Zhang, Zili Wang, Yuan Qi, Yinghui Xu, and Wei Chu. 2025. Opencoder: The open cookbook for top-tier code large language models. Preprint, arXiv:2411.04905. Binyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang, Bowen Yu, Kai Dang, and 1 others. 2024. Qwen2. 5-coder technical report. arXiv preprint arXiv:2409.12186. Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, Qian Liu, Evgenii Zheltonozhskii, Terry Yue Zhuo, Thomas Wang, Olivier Dehaene, Mishig Davaadorj, Joel Lamy-Poirier, João Monteiro, Oleh Shliazhko, and 48 others. 2023. Starcoder: may the source be with you! CoRR, abs/2305.06161. Jonathan Light, Min Cai, Sheng Shen, and Ziniu Hu. 2023. Avalonbench: Evaluating llms playing the game of avalon. arXiv preprint arXiv:2310.05036. Shukai Liu, Linzheng Chai, Jian Yang, Jiajun Shi, He Zhu, Liran Wang, Ke Jin, Wei Zhang, Hualei Zhu, Shuyue Guo, and 1 others. 2024a. Mdeval: Massively multilingual code debugging. arXiv preprint arXiv:2411.02310. Siyao Liu, He Zhu, Jerry Liu, Shulin Xin, Aoyan Li, Rui Long, Li Chen, Jack Yang, Jinxiang Xia, ZY Peng, and 1 others. 2024b. Fullstack bench: Evaluating llms as full stack coder. arXiv preprint arXiv:2412.00535. Anton Lozhkov, Raymond Li, Loubna Ben Allal, Federico Cassano, Joel Lamy-Poirier, Nouamane Tazi, Ao Tang, Dmytro Pykhtar, Jiawei Liu, Yuxiang Wei, Tianyang Liu, Max Tian, Denis Kocetkov, Arthur Zucker, Younes Belkada, Zijian Wang, Qian Liu, Dmitry Abulkhanov, Indraneil Paul, and 47 others. 2024a. Starcoder 2 and the stack v2: The next generation. Preprint, arXiv:2402.19173. Anton Lozhkov, Raymond Li, Loubna Ben Allal, Federico Cassano, Joel Lamy-Poirier, Nouamane Tazi, Ao Tang, Dmytro Pykhtar, Jiawei Liu, Yuxiang Wei, and 1 others. 2024b. Starcoder 2 and the arXiv preprint stack v2: The next generation. arXiv:2402.19173. Shuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey Svyatkovskiy, Ambrosio Blanco, Colin B. Clement, Dawn Drain, Daxin Jiang, Duyu Tang, Ge Li, Lidong Zhou, Linjun Shou, Long Zhou, Michele Tufano, Ming Gong, Ming Zhou, Nan Duan, Neel Sundaresan, and 3 others. 2021. Codexglue: machine learning benchmark dataset for code understanding and generation. In Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, December 2021, virtual. Ziyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang Hu, Chongyang Tao, Jing Ma, Qingwei Lin, and Daxin Jiang. 2023. Wizardcoder: Empowering code large language models with evolinstruct. CoRR, abs/2306.08568. OpenAI. 2023. GPT-4 technical report. CoRR, abs/2303.08774. OpenAI. 2025a. Introducing gpt-4.5. OpenAI. 2025b. Introducing gpt-oss. OpenAI. 2025c. Introducing o3-and-o4-mini. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F. Christiano, Jan Leike, and Ryan Lowe. 2022. Training language models to follow instructions with human feedback. In Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022. Qwen. 2025. Qwq-32b: Embracing the power of reinforcement learning. Baptiste Rozière, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jérémy Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton, Manish Bhatt, Cristian Canton-Ferrer, Aaron Grattafiori, Wenhan Xiong, Alexandre Défossez, Jade Copet, and 6 others. 2023. Code llama: Open foundation models for code. CoRR, abs/2308.12950. ByteDance Seed. 2025. Seed-oss open-source models release. ByteDance Seed, Yuyu Zhang, Jing Su, Yifan Sun, Chenguang Xi, Xia Xiao, Shen Zheng, Anxiang Zhang, Kaibo Liu, Daoguang Zan, and 1 others. 2025. Seed-coder: Let the code model curate data for itself. arXiv preprint arXiv:2506.03524. Sglang Team. Sglang project. Jiajun Shi, Jian Yang, Jiaheng Liu, Xingyuan Bu, Jiangjie Chen, Junting Zhou, Kaijing Ma, Zhoufutu Wen, Bingli Wang, Yancheng He, and 1 others. 2025. Korgym: dynamic game platform for llm reasoning evaluation. arXiv preprint arXiv:2505.14552. Kimi Team, Yifan Bai, Yiping Bao, Guanduo Chen, Jiahao Chen, Ningxin Chen, Ruijue Chen, Yanru Chen, Yuankun Chen, Yutian Chen, and 1 others. 2025. Kimi k2: Open agentic intelligence. arXiv preprint arXiv:2507.20534. Jingqi Tong, Jixin Tang, Hangcheng Li, Yurong Mou, Ming Zhang, Jun Zhao, Yanbo Wen, Fan Song, Jiahao Zhan, Yuyang Lu, Chaoran Tao, and 1 others. 2025. Game-rl: Synthesizing verifiable game tasks at scale to boost vlms general reasoning. arXiv preprint arXiv:2505.13886. Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, Wayne Xin Zhao, Zhewei Wei, and Ji-Rong Wen. 2023a. survey on large language model based autonomous agents. CoRR, abs/2308.11432. Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, and Hannaneh Hajishirzi. 2023b. Self-instruct: Aligning language models with self-generated instructions. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023, pages 1348413508. Association for Computational Linguistics. Yuxiang Wei, Zhe Wang, Jiawei Liu, Yifeng Ding, and Lingming Zhang. 2023. Magicoder: Source code is all you need. CoRR, abs/2312.02120. Shuang Wu, Liwen Zhu, Tao Yang, Shiwei Xu, Qiang Fu, Yang Wei, and Haobo Fu. 2024. Enhance reasoning for large language models in the game werewolf. arXiv preprint arXiv:2402.02330. xAI. 2025. grok4. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao Ge, Haoran Wei, Huan Lin, Jialong Tang, and 41 others. 2025. Qwen3 technical report. Preprint, arXiv:2505.09388. Jian Yang, Jiaxi Yang, Ke Jin, Yibo Miao, Lei Zhang, Liqun Yang, Zeyu Cui, Yichang Zhang, Binyuan Hui, and Junyang Lin. 2024a. Evaluating and aligning codellms on human preference. arXiv preprint arXiv:2412.05210. Jian Yang, Jiajun Zhang, Jiaxi Yang, Ke Jin, Lei Zhang, Qiyao Peng, Ken Deng, Yibo Miao, Tianyu Liu, Zeyu Cui, and 1 others. 2024b. Execrepobench: Multilevel executable code completion evaluation. arXiv preprint arXiv:2412.11990. Jianzhu Yao, Kevin Wang, Ryan Hsieh, Haisu Zhou, Tianqing Zou, Zerui Cheng, Zhangyang Wang, and Pramod Viswanath. 2025. Spin-bench: How well do llms plan strategically and reason socially? arXiv preprint arXiv:2503.12349. Zhaojian Yu, Xin Zhang, Ning Shang, Yangyu Huang, Can Xu, Yishujie Zhao, Wenxiang Hu, and Qiufeng Yin. 2023. Wavecoder: Widespread and versatile enhanced instruction tuning with refined data generation. CoRR, abs/2312.14187. Aohan Zeng, Xin Lv, Qinkai Zheng, Zhenyu Hou, Bin Chen, Chengxing Xie, Cunxiang Wang, Da Yin, Hao Zeng, Jiajie Zhang, and 1 others. 2025. Glm-4.5: Agentic, reasoning, and coding (arc) foundation models. arXiv preprint arXiv:2508.06471. Renrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li, Peng Gao, and Yu Qiao. 2023. Llama-adapter: Efficient fine-tuning of language models with zero-init attention. CoRR, abs/2303.16199. Wei Zhang, Jian Yang, Jiaxi Yang, Ya Wang, Zhoujun Li, Zeyu Cui, Binyuan Hui, and Junyang Lin. 2025a. Turning the tide: Repository-based code reflection. Preprint, arXiv:2507.09866. Wei Zhang, Yi Zhang, Li Zhu, Qianghuai Jia, Feijun Jiang, Hongcheng Guo, Zhoujun Li, and Mengping Zhou. 2025b. Adc: Enhancing function calling via adversarial datasets and code line-level feedback. In ICASSP 2025 - 2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 15. Terry Yue Zhuo, Minh Chien Vu, Jenny Chim, Han Hu, Wenhao Yu, Ratnadira Widyasari, Imam Nur Bani Yusuf, Haolan Zhan, Junda He, Indraneil Paul, and 1 others. 2024. Bigcodebench: Benchmarking code generation with diverse function calls and complex instructions. arXiv preprint arXiv:2406.15877."
        },
        {
            "title": "A Comprehensive Leaderboard Ranking Models",
            "content": "Figure 13 reveals clear performance hierarchy with o3 achieving the highest success rate by solving 1,092 games, followed closely by Gemini-2.5-Pro (1,054 games) and GPT-5 (1,047 games). Notably, proprietary models dominate the top positions, with 5 out of the top 6 performers being closed-source systems. Among open-source models, the Qwen3 series demonstrates exceptional performance, with multiple variants appearing in the top rankings. The Qwen3-235B-A22B-Thinking-2507 model achieves the highest open-source performance at 1,039 games solved, ranking 4th overall. The strong showing of thinking-enhanced variants (e.g., Qwen3-235B-A22B-Thinking-2507) suggests that reasoning-augmented architectures provide substantial benefits for complex code generation tasks. The performance gap between the leading models and lower-ranked ones is substantial, with success rates ranging from approximately 49% (1,092/2,219) at the top to 35% (786/2,219) for the 30th-ranked model. This distribution indicates that while current state-of-the-art models can successfully generate functional game code for roughly half of the benchmark tasks, there remains significant room for improvement in achieving consistent, high-quality game development capabilities across diverse requirements. Figure 13: Comprehensive Leaderboard Ranking Models by the Number of Games Successfully Solved Out."
        },
        {
            "title": "B Complete Leaderboard",
            "content": "Now, we show the complete leaderboard in Table 3."
        },
        {
            "title": "C Comprehensive Performance Comparison Across Different Evaluation Dimensions",
            "content": "Figure 14 presents comprehensive performance comparison across four key evaluation dimensions. The final performance ranking (a) shows proprietary models dominating the leaderboard, with GPT-5 achieving the highest score of 45.0, followed closely by O3 at 44.8. Code generation performance (b) reveals the strongest capability across all models, with scores ranging from 70-97 points, indicating mature syntactic and logical programming abilities. However, significant performance gap emerges in visual assessment tasks: image evaluation (c) shows dramatically lower scores (0-20 points), while video evaluation (d) exhibits similar patterns with scores reaching only up to 22.6. This stark contrast between code generation and visual evaluation performance highlights fundamental challenge in current language Model Size Final Code Image Video Excellent Good Fair Poor gpt-5 o3 gpt-5-mini gemini-2.5-pro o4-mini gpt-4.1-2025-04-14 grok-4 gemini-2.5-flash chatgpt-4o-latest claude-sonnet-4-20250514-thinking claude-sonnet-4-20250514 o3-mini-2025-01-31 gpt-4o-2024-11-20 gpt-4o-mini-2024-07-"
        },
        {
            "title": "Proprietary LLMs",
            "content": "(cid:181) (cid:181) (cid:181) (cid:181) (cid:181) (cid:181) (cid:181) (cid:181) (cid:181) (cid:181) (cid:181) (cid:181) (cid:181) (cid:181) 45.0 44.8 43.5 43.5 43.0 42.5 42.0 42.0 41.2 40.5 40.2 38.2 37.6 33.9 96.6 92.3 96.7 89.1 87.8 91.8 83.9 92.8 82.5 90.3 87.7 89.3 76.6 70.4 17.6 20.2 15.7 19.1 19.8 17.6 19.8 16.5 19.9 14.4 15.7 11.9 17.5 15.5 400B+ Open-Weight LLMs Qwen3-Coder-480B-A35B-Instruct DeepSeek-V3-0324 DeepSeek-V3.1 DeepSeek-R1 DeepSeek-R1-0528 DeepSeek-V3 kimi-k2-0905-preview 32B/480B 37B/671B 37B/671B 37B/671B 37B/671B 37B/671B 32B/1000B 41.3 41.1 40.9 40.1 38.7 36.7 23.5 85.3 83.6 83.1 81.0 88.1 73.4 66.3 18.3 19.3 19.3 19.2 13.4 17.7 2.0 100B-400B Open-Weight LLMs Qwen3-235B-A22B-Thinking-2507 Qwen3-235B-A22B Qwen3-235B-A22B-Instruct-2507 GLM-4.5 GLM-4.5-Air gpt-oss-120b 22B/235B 235B 22B/235B 32B/355B 12B/106B 5.1B/117B 42.3 41.2 41.1 40.0 39.4 43.4 84.5 81.3 85.3 84.7 85.4 90.1 20.0 19.8 18.2 17.0 16.3 19.7 Qwen3-32B Seed-OSS-36B-Instruct Qwen3-30B-A3B-Thinking-2507 QwQ-32B Qwen3-30B-A3B Qwen3-Coder-30B-A3B-Instruct Qwen3-30B-A3B-Instruct-2507 DeepSeek-R1-Distill-Llama-70B Zhihu-ai-Zhi-Create-Qwen3-32B Qwen2.5-72B-Instruct Qwen2.5-Coder-32B-Instruct DeepSeek-R1-Distill-Qwen-32B Qwen2.5-32B-Instruct gpt-oss-20b Qwen3-14B Qwen2.5-14B-Instruct Qwen2.5-Coder-14B-Instruct DeepSeek-R1-Distill-Qwen-14B Qwen3-8B Qwen3-4B Qwen3-4B-Thinking-2507 Seed-Coder-8B-Instruct Llama-3.1-8B-Instruct Qwen2.5-Coder-7B-Instruct Qwen2.5-7B-Instruct Qwen3-1.7B Llama-3.2-3B-Instruct deepseek-coder-7b-instruct-v1.5 Hunyuan-7B-Instruct Qwen2.5-Coder-3B-Instruct OpenCoder-8B-Instruct Qwen2.5-3B-Instruct Qwen2.5-Coder-1.5B-Instruct OpenCoder-1.5B-Instruct Llama-3.2-1B-Instruct Qwen2.5-1.5B-Instruct DeepSeek-R1-Distill-Llama-8B DeepSeek-R1-0528-Qwen3-8B Qwen3-0.6B Qwen2.5-Coder-0.5B-Instruct DeepSeek-R1-Distill-Qwen-7B Qwen2.5-0.5B-Instruct DeepSeek-R1-Distill-Qwen-1.5B 30B-100B Open-Weight LLMs 32B 36B 3B/30B 32B 3B/30B 3B/30B 30B 70B 32B 72B 32B 32B 32B 40.4 40.3 40.0 39.6 39.6 39.0 38.6 35.3 35.1 34.6 34.4 33.4 31.8 81.6 88.3 80.7 79.7 78.4 83.8 81.4 74.1 75.8 73.2 74.5 71.9 66.4 18.9 16.4 18.9 18.5 19.7 16.6 16.5 15.8 15.2 14.7 13.8 14.4 14. 10B-30B Open-Weight LLMs 3.6B/21B 14B 14B 14B 14B 42.2 38.8 30.3 30.2 27.4 88.8 79.1 66.4 68.5 65.3 18.6 18.4 11.4 10.9 8.7 Below 10B Open-Weight LLMs 8B 4B 4B 8B 8B 7B 7B 1.7B 3B 7B 7B 3B 8B 3B 1.5B 1.5B 1B 1.5B 8B 8B 0.6B 0.5B 7B 0.5B 1.5B 36.9 34.4 34.3 33.9 29.4 27.6 26.1 25.0 24.5 24.0 21.0 20.4 20.1 18.7 17.3 16.9 15.5 15.2 15.1 14.0 13.7 12.8 12.1 10.9 8.5 76.2 72.7 70.0 73.2 62.9 63.9 59.8 57.3 55.5 53.8 57.8 53.8 49.0 46.9 46.6 43.1 40.3 40.1 42.5 41.3 35.1 34.6 35.8 30.8 25.4 17.2 15.1 16.1 14.0 13.0 9.1 9.2 9.1 9.5 9.0 2.8 4.4 6.5 5.0 3.9 5.6 4.3 3.7 1.7 0.4 4.8 3.6 0.3 1.7 0.0 20.7 21.9 18.0 22.2 21.4 18.1 22.4 16.7 21.3 16.9 17.4 13.3 18.6 15.8 20.5 20.5 20.2 20.1 14.6 18.9 2. 22.4 22.6 19.7 18.3 16.5 20.3 20.6 16.2 20.4 20.6 20.7 16.7 17.8 16.0 14.4 15.9 14.9 13.9 15.1 19.2 18.8 13.0 11.2 8.3 17.3 15.5 16.8 14.4 12.3 9.7 9.2 8.7 8.5 9.2 2.2 2.9 4.7 4.1 1.2 1.9 1.9 1.8 1.0 0.3 1.1 0.0 0.1 0.0 0.0 83 65 61 45 36 47 21 28 25 36 36 26 12 4 20 22 25 15 32 3 22 14 16 31 23 52 8 25 13 10 9 22 11 4 3 3 9 0 2 31 9 0 0 1 5 1 2 4 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 288 341 236 337 313 263 327 252 305 204 207 204 224 134 287 311 296 278 174 204 322 302 308 216 230 324 274 234 279 268 274 226 223 188 184 174 167 145 127 299 245 92 87 77 187 144 168 137 84 69 52 43 46 38 12 10 7 9 2 9 4 2 6 3 3 0 0 0 0 676 686 655 672 682 641 650 634 613 624 616 417 531 459 644 638 611 607 544 564 695 668 589 661 533 634 642 585 589 610 597 543 571 448 426 449 425 411 389 626 567 348 327 198 546 464 465 422 319 250 230 216 210 229 59 85 152 94 58 102 57 63 26 8 56 39 3 16 1 1172 1127 1267 1165 1188 1268 1221 1304 1276 1355 1360 1572 1452 1622 1268 1248 1287 1319 1469 1447 1180 1235 1306 1311 1433 1209 1295 1375 1338 1331 1339 1428 1414 1579 1606 1593 1618 1663 1701 1263 1398 1779 1804 1943 1480 1610 1584 1656 1815 1899 1937 1959 1963 1952 2148 2124 2060 2116 2159 2108 2158 2154 2187 2207 2160 2179 2216 2200 2218 Table 3: Comprehensive Performance Evaluation, showing Final Score, Code Score, Image Score, Video Score, and score distribution. Bold indicate highest performance; underlined indicate second-highest performance. models - while they excel at producing syntactically correct and logically sound code, generating visually appealing and functionally rich interactive games remains substantially more difficult. The consistent ranking patterns across visual modalities suggest that models capable of generating better static visual content also tend to produce superior dynamic gameplay experiences. (a) Final Performance (b) Code Generation Performance (c) Image Evaluation Performance (d) Video Evaluation Performance Figure 14: Comprehensive performance comparison across different evaluation dimensions. Performance comparison across four evaluation dimensions: (a) Overall scores with GPT-5 and o3 leading, (b) High code generation scores (70-97), (c-d) Low visual evaluation scores (0-20), revealing models excel at code generation but struggle with visual assessment. V-GameGym Reference Code Analysis To comprehensively analyze the character length of reference code within the dataset, we employed three complementary visualization methods. Figure 15(a), Violin Plot, reveals the overall probability density distribution of the data, exhibiting clear unimodal shape with its peak concentrated around 8,500 characters. This figure intuitively displays the core statistical characteristics of the data: median of 8,488 characters, with 50% of the data falling within an interquartile range (IQR) spanning 3,180 characters. Figure 15(b) provides more refined depiction of this distribution through histogram with kernel density estimate (KDE) curve under linear coordinates. The calculated mean of 8,533 characters is notably close to the median, suggesting an approximately symmetrical distribution. Concurrently, the cumulative distribution function (CDF) curve on the right offers quantitative perspective on the data; for instance, approximately 80% of code samples have length below 10,000 characters. Finally, to effectively examine the datas full dynamic range, particularly its long-tail portion, Figure 15(c) employs logarithmic axis. This view compresses the larger value ranges, allowing extreme values at both ends of the distribution to be clearly presented, thus completely illustrating the entire distribution from the shortest to the longest code segments. In summary, these three figures collectively provide detailed and multifaceted representation of the datasets central tendency, dispersion, and distributional shape. (a) Violin plot showing distribution peaked at 8,500 characters. Figure 15: Comprehensive analysis of reference code character length distribution using three complementary visualization methods: density estimation, linear-scale histogram, and logarithmic-scale representation. (b) Histogram showing symmetric distribution. (c) Log-scale view showing full range distribution. V-GameGym Word Cloud Analysis Figure 16 presents word cloud analysis comparing the linguistic content of the natural language requirements against the Python code solutions in our dataset. The Requirements Word Cloud (left) highlights strong focus on core game mechanics, with dominant terms such as player, screen, game, and create. This confirms the prompts are well aligned with the intended domain. The Code Tokens Word Cloud (right) reveals the most frequent Pygame API calls and programming constructs, including render, font, random, and time, outlining the key technical skills required. The clear semantic alignment between the two clouds demonstrates direct and coherent mapping from the problem descriptions to their programmatic solutions, validating the datasets suitability for evaluating an LLMs code generation capabilities in this domain. V-GameGym Reference Code Patterns Quantitative Analysis To deeply understand the inherent structure and common practices of code within the V-GameGym dataset, we conducted quantitative analysis of code patterns, with results shown in Figure 18. The results reveal library usage frequency, core game loop mechanisms, code structure paradigms, and overall complexity distribution, respectively. On one hand, they generally adhere to standard Pygame development paradigms; on the other hand, they exhibit significant diversity in code structure and complexity, making this dataset an ideal resource for training and evaluating code generation models. Figure 16: V-GameGym Comparative Word Cloud Analysis of Requirements and Code Corpora. Figure 17: Distribution of code complexity scores. (a) Occurrence frequency of core game loop patterns. (b) Distribution of code structure elements. (c) Import frequency of common Python libraries. Figure 18: Comprehensive analysis of reference code character length distribution. Library Import Frequency This plot displays the most frequently imported Python libraries in the V-GameGym dataset. The results indicate that pygame is widely used as the core framework, while random and math libraries also appear frequently, reflecting the common presence of random elements and mathematical computation requirements in game development. This confirms that the samples in the dataset follow standard Pygame development practices. Game Loop Patterns This plot quantifies the occurrences of key code patterns that constitute typical game loop. The high frequency of calls to pygame.event highlights the central role of eventdriven programming in game interaction. Similarly, the frequent use of pygame.display.update and clock.tick, corresponding to screen rendering and frame rate control respectively, is fundamental for building real-time, smooth gaming experiences. Code Structure Distribution This pie chart depicts the relative proportions of classes, functions, and comments within the code. Analysis shows that functions are the primary units of code organization, while the use of classes also accounts for significant proportion, indicating certain application of object-oriented programming (OOP) principles in the samples. The proportion of comments provides an indirect measure of code readability and maintainability. Code Complexity Score Distribution To assess the structural complexity of the code, we defined complexity score (calculated as number of functions + 2 * number of classes). The histogram in this figure shows that the complexity scores exhibit right-skewed distribution, indicating that most game codes in the dataset have relatively simple structures, but it also includes portion of complex projects with highly intricate structures (e.g., large number of classes and functions). V-GameGym Quality Score Prediction Model Results To evaluate the performance of our Random Forest regression model for quality score prediction, multifaceted analysis was conducted, as illustrated in Figure 19. (a) Comparison of actual vs. predicted quality scores. (b) Distribution of prediction residuals. Figure 19: Quality Score Prediction Distribution. Feature Importance This panel presents the top ten most influential features in determining the models predictions, ranked by their Gini importance. The analysis reveals that metrics related to code volume and complexity, such as code_char_len (total characters in the code) and code_word_count, are the strongest predictors. This insight underscores the significant relationship between the sheer size of the codebase and its perceived quality score within this dataset. Residuals Distribution This histogram displays the distribution of the prediction residuals, calculated as the difference between the actual and predicted scores (Actual - Predicted). The distribution is approximately centered around zero and exhibits quasi-normal shape, suggesting that the model has no systematic bias (i.e., it does not consistently overor under-predict). This desirable characteristic indicates that the models errors are random, which is key assumption for well-fitted regression model. V-GameGym Distribution of Game Samples Across the Top 30 Source Repositories Figure 20 provides quantitative analysis of the contribution frequency from the top 30 source repositories within the curated dataset. The horizontal bar chart illustrates the number of game samples sourced from each unique repository, which are ranked in descending order of their contribution count. prominent characteristic revealed by the visualization is the highly granular and flat distribution of samples. The data indicates that the contributions are thinly spread across wide array of sources, with the most frequent repositories supplying maximum of only three game samples. substantial cohort of repositories provided two samples each, followed by another group contributing single instances. This flat, long-tail distribution pattern underscores the extensive diversity of the datasets origins. By sourcing small number of games from large pool of independent repositories, we effectively minimize the risk of stylistic and structural bias that could arise from over-representing few dominant sources. The resulting heterogeneity ensures broad and more representative collection of programming patterns, architectural designs, and implementation logic. This characteristic is fundamental to the datasets objective of serving as robust foundation for training generalizable models in tasks such as automated code generation and program analysis. Figure 20: Distribution of game samples across the top 30 source repositories."
        },
        {
            "title": "I Model Similarity Analysis",
            "content": "In Figure 21, the similarity clustering reveals distinct model families with comparable problem-solving patterns. Models from the same architecture family (e.g., Qwen3 variants, DeepSeek series) tend to cluster together, indicating that foundational architecture and training methodologies significantly influence which games models can successfully solve. Interestingly, some cross-family clusters emerge between models of similar scale, suggesting that parameter count plays crucial role in determining capability overlap beyond architectural differences."
        },
        {
            "title": "J Score Threshold Sensitivity Analysis",
            "content": "In Figure 22, the threshold sensitivity analysis demonstrates remarkable ranking stability across different score cutoffs. As the threshold increases from 20 to 80 points, all models show expected performance degradation, but their relative positions remain largely unchanged. This robustness validates our evaluation methodology and suggests that the observed performance differences reflect genuine capability gaps rather than evaluation artifacts. The parallel decline curves indicate that our scoring system maintains discriminative power across the full quality spectrum."
        },
        {
            "title": "K Score Distribution Characteristics",
            "content": "Figure 23 reveals diverse score distribution patterns among top-performing models. Some models exhibit narrow, concentrated distributions around their median scores, indicating consistent performance across different game types. Others show broader, multi-modal distributions, suggesting specialized strengths in particular game categories. The distribution shapes provide insights into model reliability - models Figure 21: Hierarchical clustering of models based on solved game overlap using Jaccard similarity index. Figure 22: Pass rate variations across different score thresholds, demonstrating ranking stability. with tighter distributions may be more predictable for production use, while those with wider distributions might excel in specific domains but struggle with others. Figure 23: Violin plots showing score distribution patterns for top 20 models. Representative Head-to-Head Comparisons In Figure 24, the head-to-head comparisons reveal nuanced competitive dynamics between leading models. Points above the diagonal line indicate games where the y-axis model outperforms the x-axis model, and vice versa. The scatter patterns show that even among top-tier models, performance advantages are game-specific rather than universal. Some model pairs exhibit complementary strengths, suggesting potential ensemble benefits. The analysis also reveals that certain games consistently favor particular model architectures, indicating systematic biases in problem-solving approaches."
        },
        {
            "title": "M Comprehensive Performance Heatmap",
            "content": "In Figure 25, the performance heatmap provides granular view of model capabilities across the most challenging benchmark subset. The clear progression from lighter (better performance) to darker (poorer performance) colors as difficulty increases confirms the validity of our difficulty ordering. Notably, even the highest-performing models struggle with the rightmost games, indicating these represent genuine frontier challenges. The heatmap also reveals interesting patterns where certain models show unexpected strength on specific difficult games, suggesting specialized capabilities that average performance metrics might obscure. The clustering of similar performance patterns across model families reinforces the architectural influence on problem-solving approaches. (a) Head-to-head comparison of Gemini-2.5-pro and GPTOSS-120B. (b) Head-to-head comparison of Gemini-2.5-pro and Grok-4. (c) Head-to-head comparison of Gemini-2.5-pro and o4mini. (d) Head-to-head comparison of Gemini-2.5-pro and Qwen3235B-A22B-Thinking-2507. (e) Head-to-head comparison of and GPT-OSS-120B DeepSeek-V3-0324. (f) Head-to-head comparison of GPT-OSS-120B and GPTOSS-20B. (g) Head-to-head comparison of GPT-OSS-120B and Grok4. (h) Head-to-head comparison of GPT-OSS-120B and Qwen3235B-A22B. (i) Head-to-head comparison of Grok-4 and DeepSeek-V30324. (j) Head-to-head comparison of Grok-4 and GPT-OSS-20B. (k) Head-to-head comparison of o4-mini and GPT-OSS120B. (l) Head-to-head comparison of o4-mini and Grok-4. (m) Head-to-head comparison of Qwen3-235B-A22BThinking-2507 and DeepSeekV3-0324. (n) Head-to-head comparison of Qwen3-235B-A22BThinking-2507 and GPT-OSS120B. (o) Head-to-head comparison Qwen3-235B-A22Bof Thinking-2507 and Grok-4. (p) Head-to-head comparison Qwen3-235B-A22Bof Thinking-2507 and o4-mini. Figure 24: Direct performance comparisons between selected model pairs showing competitive advantages across individual games. Figure 25: Performance matrix of top 25 models on the 60 most challenging games, ordered by increasing difficulty and overall model performance."
        },
        {
            "title": "N Seed Code Dataset Quality Analysis",
            "content": "To provide comprehensive insights into the characteristics and quality of our seed dataset, we conducted extensive statistical analysis across multiple dimensions. The analysis encompasses cluster distribution, quality metrics, file characteristics, module usage patterns, and structural complexity. Figure 26: Cluster size distribution and sample selection strategy, showing uniform selection of 25 samples from each of the 100 clusters across 168,287 total objects. Cluster Coverage and Sample Selection Figure 26 illustrates the distribution of objects across our 100 clusters and the uniform selection strategy employed. The analysis reveals significant variation in cluster sizes, ranging from hundreds to thousands of objects per cluster, with total of 168,287 objects processed. Our systematic selection of 25 samples per cluster ensures balanced representation across all functional categories, achieving 1.49% overall selection rate. This uniform sampling strategy effectively mitigates bias toward popular game types while maintaining diversity across the entire functional spectrum. Quality Score Distribution Analysis Figure 27 demonstrates the high quality of our curated dataset, with 79.7% of samples achieving quality scores above 70. The distribution exhibits strong right skew with mean score of 86.2 and median of 100.0, indicating that our clustering-based selection successfully identified structurally complete and well-implemented code samples. The concentration of samples in the high-quality range validates our selection methodology and ensures that the benchmark provides reliable reference implementations for evaluation purposes. File Size Characteristics The file size analysis in Figure 28 reveals log-normal distribution with mean of 10.2 KB and median of 5.5 KB. This size distribution indicates that most games in our dataset are compact, self-contained implementations suitable for educational and prototyping purposes, while still including complex examples exceeding 50 KB. The predominance of smaller files (under 20 KB) aligns with typical Pygame project patterns and ensures computational efficiency during evaluation while maintaining functional completeness. Pygame Module Usage Patterns Figure 29 quantifies the frequency of core Pygame API usage across our dataset. The analysis shows that fundamental modules like pygame.display (91.5%) and pygame.event (68.3%) are nearly universal, confirming adherence to standard Pygame development patterns. The moderate usage of advanced features like pygame.sprite (21.3%) and pygame.mixer (19.2%) indicates balanced representation of both basic and sophisticated game development techniques within our corpus. Game Type Distribution The game type analysis in Figure 30 demonstrates substantial diversity in our dataset, with arcade games comprising the largest category (47.3%) followed by shooter games (17.7%) Figure 27: Distribution of code quality scores across 2,500 selected samples. And 2500 is the seed set selected after clustering, and 2219 is the final test set after LLM pipeline and manual verification. Figure 28: File size distribution showing log-normal characteristics with mean 10.2 KB and median 5.5 KB, indicating predominantly compact but complete implementations. and other miscellaneous types (23.7%). This distribution reflects the natural prevalence of different game genres in the Pygame community while ensuring adequate representation of specialized categories like physics simulations, RPGs, and educational games. The balanced representation across game types enhances the benchmarks ability to evaluate diverse programming patterns and game mechanics. Figure 29: Frequency analysis of Pygame module usage, with core modules like display (91.5%) and event handling (68.3%) showing high adoption rates. Figure 30: Distribution of game types in the dataset, representation across eight distinct genres. Code Structure Analysis Figure 31 provides quantitative insights into the structural characteristics of our dataset. The average code file contains 12.0 functions, 2.2 classes, and 283.3 lines of code, indicating well-structured implementations that follow object-oriented programming principles. The prevalence of for loops (10.6 per file) and event handlers reflects the iterative and interactive nature of game programming, while the consistent presence of game loops and display updates confirms adherence to standard Pygame architectural patterns. Cluster Quality Correlation The scatter plot in Figure 32 examines the relationship between cluster size and average quality scores. The weak positive correlation (0.138) suggests that larger clusters do not necessarily contain higher-quality code, validating our quality-based selection approach within each cluster. This analysis confirms that our methodology successfully identifies the best exemplars from each functional group regardless of the clusters overall size, ensuring consistent quality across diverse game categories. Figure 31: Structural metrics of code samples, showing average counts of functions (12.0), classes (2.2), and other programming constructs per file. Figure 32: Correlation analysis between cluster size and average quality scores, showing weak correlation (0.138) that validates quality-based selection within clusters. Complexity by Game Type Figure 33 reveals significant variation in code complexity across different game genres. Physics simulation and RPG games exhibit the highest complexity scores, reflecting their sophisticated mechanics and state management requirements. In contrast, educational and puzzle games show lower complexity, aligning with their focus on simplicity and clarity. This complexity distribution Figure 33: Box plot analysis of code complexity scores across game types, with physics simulations and RPGs showing highest complexity variance. Figure 34: Radar chart analysis of quality components across different score tiers, highlighting strengths in structure and organization for high-quality samples. ensures that our benchmark captures the full spectrum of programming challenges inherent in different game development domains. Quality Components Radar Analysis The radar chart in Figure 34 provides multi-dimensional view of quality factors across different performance tiers. High-quality samples (85+) consistently excel across all dimensions, particularly in basic structure, display setup, and code organization. The analysis reveals that documentation and error handling are key differentiators between quality tiers, while basic functionality components like pygame initialization and game loops are well implemented across all levels. This comprehensive quality assessment ensures that our dataset maintains high standards while capturing diverse implementation approaches."
        },
        {
            "title": "Complete Pipeline Performance",
            "content": "End-to-End Workflow (Generation Recording Evaluation) Pipeline Components: 1. Code Generation: OpenAI API with parallel processing 2. Game Recording: Optimized pygame execution with media capture 3. Multi-modal Evaluation: Code + Screenshot + Video analysis Performance Optimizations: - Async I/O for file operations - Batch processing for efficiency - Configurable worker pools - Resume capability for interrupted runs - Streaming API responses Quality Assurance: - Automatic retry mechanisms - JSON validation with error handling - Progress tracking and recovery - Comprehensive logging and statistics"
        },
        {
            "title": "Reliability",
            "content": ">80% end-to-end success rate from requirement to final evaluation score. Handles 1000+ games with configurable parallelization and resource management. Robust error handling with automatic recovery and detailed failure analysis."
        },
        {
            "title": "Game Code Generation Case",
            "content": "User Request (Game Requirement + System Prompt + Code Template ) Generate complete pygame code based on the following game requirement: \"Create simple Snake game where the player controls snake to eat food and grow longer. The game should have collision detection and score display.\" System Prompt: \"You are pygame game development expert, good at quickly developing small games based on requirements.\" Requirements: 1. Generate complete and runnable pygame code 2. The game should automatically run for 10 seconds and then exit 3. Include all necessary import statements, especially import time 4. Add time-based automatic exit mechanism 5. Add visual timer showing elapsed time 6. Set reasonable FPS Code Template Structure: ```python import pygame import time start_time = time.time() # In main loop: current_time = time.time() if current_time - start_time >= 10: running = False ```"
        },
        {
            "title": "Avg Generation Time",
            "content": "Complete pygame Snake game with automatic exit, timer display, and proper game loop implementation. 85.3% of generated codes compile and run successfully. 2.3 seconds per game with parallel processing."
        },
        {
            "title": "Game Recording Pipeline",
            "content": "Recording Process (Code Execution + Screenshot Capture + Video Recording ) Execute generated pygame code with optimized performance: - Record duration: 10-30 seconds - Video FPS: 3-30 (configurable) - Screenshot format: JPG (faster) or PNG - Async I/O for better performance Screenshot Capture at specific timestamps: ```python _screenshot_times = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] # Capture at each second for consistent evaluation ``` Optimized surface conversion: ```python def _pygame_surface_to_cv2_optimized(surface): raw = pygame.image.tobytes(surface, 'RGB') img = np.frombuffer(raw, dtype=np.uint8) return cv2.cvtColor(img, cv2.COLOR_RGB2BGR) ``` Video Generation with optimized settings: - Use mp4v codec for fast encoding - Batch write frames for efficiency - Configurable frame interval based on target FPS - Background thread for async I/O operations"
        },
        {
            "title": "Success Rate",
            "content": "10 screenshots + 1 gameplay video per game with consistent quality. Average 1.2s per game with 20 parallel workers. 92.7% games successfully recorded with complete media files. Multi-Modal Game Evaluation System"
        },
        {
            "title": "Game Evaluation Framework",
            "content": "Evaluation Components (Code Analysis + Screenshot Review + Video Assessment ) Code Quality Evaluation (0-100 points): - Functionality (0-25): Implementation completeness - Code Quality (0-25): Structure and readability - Game Logic (0-25): Logic correctness - Technical Implementation (0-25): pygame usage efficiency Visual Quality Assessment from Screenshots (0-100 points): - Visual Completeness (0-25): UI elements presence - UI Design (0-25): Layout and visual effects - Function Display (0-25): Key features visibility - Overall Quality (0-25): Visual completion level Up to 20 screenshots analyzed per game for comprehensive coverage. Dynamic Behavior Analysis from Video (0-100 points): - Animation Effect (0-25): Smoothness and naturalness - Interaction Logic (0-25): User input responsiveness - Game Flow (0-25): Gameplay continuity - Dynamic Quality (0-25): Overall playability"
        },
        {
            "title": "Evaluation Speed",
            "content": "Average of three evaluation components with automatic retry mechanism for robust scoring. Up to 10 attempts with JSON parsing validation for reliable results. 4 parallel processes with streaming responses for efficient processing."
        }
    ],
    "affiliations": [
        "AIStrong",
        "Alibaba Group",
        "Beijing Jiaotong University",
        "Shanghai AI Lab"
    ]
}