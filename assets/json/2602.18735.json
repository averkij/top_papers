{
    "paper_title": "LaS-Comp: Zero-shot 3D Completion with Latent-Spatial Consistency",
    "authors": [
        "Weilong Yan",
        "Haipeng Li",
        "Hao Xu",
        "Nianjin Ye",
        "Yihao Ai",
        "Shuaicheng Liu",
        "Jingyu Hu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "This paper introduces LaS-Comp, a zero-shot and category-agnostic approach that leverages the rich geometric priors of 3D foundation models to enable 3D shape completion across diverse types of partial observations. Our contributions are threefold: First, \\ourname{} harnesses these powerful generative priors for completion through a complementary two-stage design: (i) an explicit replacement stage that preserves the partial observation geometry to ensure faithful completion; and (ii) an implicit refinement stage ensures seamless boundaries between the observed and synthesized regions. Second, our framework is training-free and compatible with different 3D foundation models. Third, we introduce Omni-Comp, a comprehensive benchmark combining real-world and synthetic data with diverse and challenging partial patterns, enabling a more thorough and realistic evaluation. Both quantitative and qualitative experiments demonstrate that our approach outperforms previous state-of-the-art approaches. Our code and data will be available at \\href{https://github.com/DavidYan2001/LaS-Comp}{LaS-Comp}."
        },
        {
            "title": "Start",
            "content": "LaS-Comp: Zero-shot 3D Completion with LatentSpatial Consistency Weilong Yan1 Haipeng Li2 Hao Xu3 Nianjin Ye4 Yihao Ai Shuaicheng Liu2 Jingyu Hu3 1National University of Singapore 2University of Electronic Science and Technology of China 3The Chinese University of Hong Kong 4Changhong Intelligent Robot 6 2 0 2 1 2 ] . [ 1 5 3 7 8 1 . 2 0 6 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "This paper introduces LaS-Comp, zero-shot and category-agnostic approach that leverages the rich geometric priors of 3D foundation models to enable 3D shape completion across diverse types of partial observations. Our contributions are threefold: First, LaS-Comp harnesses these powerful generative priors for completion through complementary two-stage design: (i) an explicit replacement stage that preserves the partial observation geometry to ensure faithful completion; and (ii) an implicit refinement stage ensures seamless boundaries between the observed and synthesized regions. Second, our framework is training-free and compatible with different 3D foundation models. Third, we introduce Omni-Comp, comprehensive benchmark combining real-world and synthetic data with diverse and challenging partial patterns, enabling more thorough and realistic evaluation. Both quantitative and qualitative experiments demonstrate that our approach outperforms previous state-of-the-art approaches. Our code and data will be available at LaS-Comp. 1. Introduction Shape completion is fundamental problem in 3D vision and graphics, aiming to reconstruct complete 3D shapes from partial observations, with broad applications in robotics [29, 30], autonomous driving [63, 64, 72], and AR/VR [66]. An effective shape completion approach should meet the following requirements: (i) robustly handle diverse partial patterns, from single-view scans to missing semantic parts; (ii) well generalize across broad object categories; (iii) avoid dependence on paired partialcomplete datasets; and (iv) flexibly support both text guidance and automatic completion without user input for applications. Traditional methods [43, 44, 47, 48, 50] use hand-crafted priors to infer the missing geometry from partial observations. However, such priors lack flexibility and fail to genCorresponding author Figure 1. Our new framework supports category-agnostic shape completion across diverse partial patterns, including (a) random crops, (b) single-view scans, and (c) missing semantic parts. It further supports both unconditional and text-guided completion, offering flexible control for real-world applications, see (d). eralize to diverse real-world shapes. Subsequently, [11, 69] propose to learn mapping from partial to complete shapes by training neural network. Yet, these methods rely on paired partial-complete datasets, limiting their generalizability to unseen categories. To overcome such constraints, recent works [25, 31, 34] leverage generative priors from large pre-trained models to enable category-agnostic shape completion. However, these methods assume that partial input can be rendered into at least one complete image, where the object appears geometrically complete in the rendered view. When this assumption fails, e.g. , the incomplete regions are visible from any viewpoint (see Fig. 1(c)), the incomplete renderings lead to degraded results. Recent 3D foundation models [16, 36, 58, 59, 71, 75], 1 trained on large-scale datasets [12, 13], show strong crosscategory and in-the-wild shape generation capabilities, enabling wide range of downstream applications such as shape editing [21, 23, 24, 27, 28] and analysis [14, 74]. In contrast, existing shape completion methods struggle to generalize across the diverse input partial patterns and shape categories. This gap motivates us to exploit the powerful geometric priors embedded in these foundation models for more robust and generalizable shape completion. Yet, unlike earlier 3D generative models [22, 27, 40, 41, 76] that operate directly in the spatial domain, e.g. , points or voxels, these foundation models adopt latent-generativebased pipeline: variational autoencoder first maps shape to compact latent space, and diffusion or flow-matching model is trained over this space. While this design effectively compresses 3D data and supports scalable training of large foundation models, it simultaneously introduces unique challenge for shape completion. straightforward approach for shape completion within the latent framework is to encode the partial input into the latent space and directly use the latent codes of these observed regions as conditions to guide the generative model to predict the missing geometry. However, we observe that even when complete shape and its partial input share identical geometry in the overlapping regions, their latent encodings in those regions differ significantly. Therefore, directly completing shapes in latent space becomes unreliable due to this domain gap. To address this challenge, we propose zero-shot LatentSpatial Consistency Completion (LaS-Comp) framework, which bridges the gap between the latent and spatial domains of pre-trained 3D foundation models for faithful and category-agnostic shape completion. LaSComp operates via complementary two-stage design: First, we introduce an Explicit Replacement Stage (ERS), ensuring fidelity to the input partial shape by directly injecting its geometric information into the latent representation. Second, we adopt an Implicit Alignment Stage (IAS) to improve smoothness between observed and synthesized regions by refining the latent features with geometryalignment loss. By combining explicit geometric replacement with implicit latent refinement, LaS-Comp unleashes the power of 3D foundation models to deliver high-fidelity shape completions. Our framework is compatible with different latentgenerative-based 3D foundation models [58, 59]. By exploiting the rich geometric priors encoded by these models, our framework is capable of robustly handling diverse partial patterns, ranging from single-view scans to irregularly missing regions; see Fig. 1(a-c). Furthermore, our approach leverages the built-in classifier-free guidance (CFG) mechanism of these foundation models to support both text-guided and unconditional shape completion, offering flexible user control; see Fig. 1(d). Finally, we found the evaluation gap in existing benchmarks [5, 25], which are mostly limited to single-view partial scans and do not reflect real-world diversity. To bridge this gap, we introduce Omni-Comp: new benchmark composed of real-world scans and synthetic data with diverse, challenging partial patterns, enabling more comprehensive in-the-wild evaluation. In summary, our contributions are listed as follows: We introduce LaS-Comp, novel zero-shot, categoryagnostic 3D shape completion framework. LaS-Comp bridges the gap between latent and spatial domains via two-stage design: the Explicit Replacement Stage (ERS) preserves fidelity to the partial input, while the Implicit Alignment Stage (IAS) ensures boundary coherence. Our framework is training-free, compatible with different 3D foundation models, and highly efficient. It completes each shape in 20 seconds, making it over 3 faster than existing zero-shot methods. We introduce Omni-Comp, new benchmark composed of both real-world scans and synthetic shapes with diverse partial patterns to enable more comprehensive in-the-wild evaluation. Extensive experiments on this benchmark and others confirm our methods state-of-the-art performance. 2. Related Work Supervised Shape Completion. Supervised shape completion methods learn to reconstruct complete 3D shapes from partial inputs by training on paired datasets of partial and complete shapes. Pioneering works [11, 69] established this paradigm by introducing paired datasets and end-to-end networks to directly learn the mapping from partial to complete shapes. Subsequent studies [26, 53, 56, 62, 73] employed coarse-to-fine refinement to better reconstruct missing regions and recover fine geometric details. Transformerbased approaches [35, 51, 54, 60, 65, 67, 77] leverage attention mechanisms to effectively aggregate multi-scale geometric features, leading to higher-fidelity shape completions. Meanwhile, diffusion-based methods [6, 8, 42, 76] formulate shape completion as conditional generation task, where the partial input serves as condition for the diffusion model [18] to produce completed shape. Although effective on in-domain data, these methods fail to generalize to unseen categories and partial patterns. Moreover, collecting large-scale paired datasets is costly, motivating the exploration of unsupervised or self-supervised alternatives. Unsupervised Shape Completion. To alleviate reliance on paired data, recent approaches explore unsupervised shape completion. [4] first leverages unpaired data and adversarial losses. Follow-up works [1, 55, 57, 61, 70] further advance research through various unsupervised strategies, e.g., cycle consistency and latent space alignment. Building upon these efforts, recent self-supervised approaches [7, 9, 19, 32, 38] relax the need for complete shapes by 2 Figure 2. Overview of the LaS-Comp framework. Starting from Gaussian noise, the process iteratively refines latent feature xt under the guidance of the partial input Sp. At each iteration t, this refinement is performed in two stages: the Explicit Replacement Stage (ERS) and the Implicit Alignment Stage (IAS). The ERS explicitly injects the known geometry of Sp into xt to produce an updated latent . The IAS then refines using gradient-based optimization, yielding spatially aligned latent xtdt for the next step. After the final iteration, the completed shape Sc is obtained by decoding the refined latent. learning geometric priors directly from partial observations. However, these methods are trained on ShapeNet [3], which contains limited categories, leading to degraded performance on real-world data or unseen categories. Shape Completion with Generative Priors. To overcome the limitations of paired-data supervision, recent works [25, 31, 34] leverage priors from large-scale pretrained generative models for category-agnostic 3D shape completion. SDS-Complete [31] distills 2D priors from Stable Diffusion [46] to optimize the signed distance field, while ComPC [25] extends this idea by initializing partial inputs as 3D Gaussians and utilizing Zero-1-to-3 [39] to render multi-view images and iteratively refine them for shape completion. GenPC [34] renders partial input into an image, reconstructs coarse shape via image-to-3D models [20, 49], and refines it using Zero-1-to-3 [39]. However, these methods rely on the assumption that partial inputs can be rendered into at least one reasonably complete image. When the partial input fails to yield complete image from any viewpoint, the incomplete renderings often cause suboptimal completion results. To address the limitations of existing approaches, we propose the first framework that fully leverages latent-generative-based 3D foundation models for high-fidelity, zero-shot, and category-agnostic shape completion. Without relying on any rendering assumptions, our method robustly handles highly diverse partial input patterns, from single-view scans to irregular occlusions. Moreover, it is fully training-free, enabling seamless deployment across wide range of real-world scenarios. 3. Method 3.1. Overview Given partial 3D shape Sp Rk3, we desire to generate completed shape Sc Rk3 that is geometrically faithful to Sp. To achieve this, we introduce the LaS-Comp framework, which effectively leverages the powerful geometric priors of pre-trained 3D foundation models [58, 59] for zero-shot shape completion. Through the classifier-free guidance sampling mechanism [17] of 3D foundation models, our method supports both text-guided and unconditional shape completion. As illustrated in Fig. 2, starting from Gaussian noise, our framework iteratively refines it through multiple denoising steps under the guidance of the partial input Sp, progressively recovering the underlying complete geometry. At each iteration [0, 1], LaS-Comp takes the current latent feature xt Rn3c as input, together with the partial shape Sp, and performs two complementary operations: Explicit Replacement Stage (ERS) explicitly injects the partial input Sp into xt, producing an updated latent that enforces strict fidelity to the partial input. Implicit Alignment Stage (IAS) refines through onestep optimization guided by geometric-alignment loss, which encourages the smoothness between synthesized and observed regions, producing coherent latent xtdt for the next step (dt is the length of denoising step). After the final iteration, the refined latent feature x0 is decoded through the decoder to produce the completed shape: Sc = D(x0). The details of ERS and IAS are demonstrated in Sec. 3.2 and Sec. 3.3, respectively. 3.2. Explicit Replacement Stage Taking the latent feature xt from the previous generative step and the partial shape Sp as input, the ERS performs primary completion by injecting the geometry of Sp into the completion process, producing an updated latent feature that explicitly encodes the observed regions. As depicted in Fig. 3, motivated by the latent decomposition in FlowDPS [33], we decompose the generation at each timestep into two branches to enhance both fidelity and diversity: (i) clean branch for enforcing the input fidelity; and (ii) noisy branch for improving generation diversity. 3 Rather than employing ˆx1t for latent update, we introduce novel Partial-aware Noise Schedule (PNS) to modulate the stochasticity during denoising. The motivation is that ˆx1t treats all spatial regions equally, whereas shape completion is asymmetric: observed regions should remain stable to preserve the input geometry, while missing regions should permit higher stochasticity to explore plausible completions. The final noisy latent (cid:16) 1t is then composed as: (cid:17) 1t ˆx1t + ϵ1 +(1M )ϵ2, (6) = 1t where ϵ1, ϵ2 (0, I) and is downsampled to size of n3 to match the dimensions of the latent features. This design applies the following mechanisms: For the observed regions (M = 1): Since these regions correspond to the reliable partial input, their latent features should remain largely stable during denoising, with only minimal stochastic disturbance. To achieve this, we blend the model-predicted noisy latent ˆx1t with Gaust. This sian noise ϵ1 using the coefficients schedule imposes time-dependent perturbation magnitude: early iterations inject greater randomness to allow adjustments that maintain overall coherence, while later iterations gradually reduce stochasticity and preserve the geometry of the observed regions. 1 and For the missing regions (M = 0): No reliable observations are available to constrain the denoising process. To encourage broad exploration of the possible completions, we replace the model-predicted noisy latent with pure Gaussian noise ϵ2. This promotes diversity in the generated geometry for the missing areas. With the updated noise-free latent tent 1t, we reconstruct the updated latent state current timestep via the forward flow interpolation: 0t and the noisy lat for the = (1t) x 0t + 1t, (7) the resulting latent encodes the partial input geometry via the ERS, where the observed regions are directly imposed in the spatial domain. Although this explicit spatial replacement preserves fidelity to the input, it may introduce inconsistencies or discontinuities near the boundaries between observed and synthesized regions. To mitigate these artifacts and improve local coherence, we feed into the Implicit Alignment Stage (IAS) for refinement step. 3.3. Implicit Alignment Stage To improve coherence between observed and synthesized regions, we introduce the Implicit Alignment Stage (IAS), which refines the ERS output before the next iteration. The refinement begins by estimating noise-free latent from . Refer to the Eq. (1), we reuse the notation ˆx0t to denote the noise-free latent predicted from : ˆx0t = G(x , t), (8) Figure 3. Overview of the Explicit Replacement Stage (ERS). At each timestep t, ERS decomposes the latent generation into two parallel branches. The clean branch (top) enforces spatial consistency, yielding 0t. Concurrently, the noisy branch (bottom) enhances fidelity, producing 1t. These two branch outputs are then interpolated to compute the final aligned latent . Clean Branch. Given the current latent feature xt, the clean branch uses the generator to estimate the noise-free latent feature ˆx0t based on the linear flow path: ˆx0t = xt G(xt, t), (1) where G(xt, t) is the predicted velocity from xt towards x0. This latent feature ˆx0t is then decoded into the spatial domain to yield complete shape prediction: S0t = D(ˆx0t). (2) Although S0t RN 3 provides complete shape produced by G, it does not yet faithfully preserve the observed regions of the partial input Sp. To enforce this fidelity, we first voxelize the input partial shape Sp into an occupancy grid of size 3 aligned with the spatial domain of S0t. We then construct binary spatial mask {0, 1}N 3 , where = 1 indicates occupied voxels observed in Sp. Using this mask, we perform spatial replacement to inject the known geometry into the generative prediction: 0t = Sp + S0t (1M ), (3) where is element-wise multiplication. This operation explicitly replaces the geometry with Sp for fidelity. The missing regions are then completed by the generative prediction S0t. Next, we feed 0t into the encoder to produce latent feature 0t that incorporates the geometries from the partial input as follows: 0t = E(S 0t). (4) Noisy Branch. Concurrently, the noisy branch estimates the noisy latent ˆx1t using the same generator prediction G(xt, t) as in Eq. (1): ˆx1t = xt + (1t) G(xt, t). (5) 4 then this predicted noise-free latent ˆx0t is then decoded into its spatial domain to obtain S0t: S0t = D(ˆx0t). (9) To reduce the boundary artifacts introduced by the ERS, we define geometry-alignment loss Lalign to refine the latent feature. This loss provides localized correction within the reliable masked regions, helping smooth the discontinuities. Specifically, we use the mask to select the masked regions and compute BCE loss between the predicted occupancy and the corresponding voxels of the partial input: Lalign = BCE(S0t , Sp ). (10) Notice that this loss is not used to update the model parameters. Instead, we compute the gradient of the loss with respect to the latent feature, and optimize it via single-step gradient update to refine it: xaligned 0t = ˆx0t η ˆx0tLalign, (11) where η is the learning rate. By refining the feature ˆx0t to better match the reliable masked regions, we obtain an updated latent feature xaligned that is more coherent. This updated feature is then used to compute the latent state xtdt for the next iteration as follows: 0t xtdt = xaligned 0t + (tdt) G(x , t). (12) The latent feature xtdt then serves as the input for the next iteration of the completion process. 4. Experiments 4.1. Datasets and Implementation details Evaluation Datasets. We conduct comprehensive evaluation across multiple benchmarks. (i) Redwood [5]: We follow [25, 31, 34], utilizing 10 reconstructed meshes from real-world RGB-D scans, with single scans as the partial input. (ii) Synthetic dataset [25]: Following [25, 37], this contains 12 objects from different categories, with virtuallyrendered single scans as the partial input. (iii) KITTI [15]: We follow [4, 70], using cars from real-world LiDAR scans, with extremely sparse points as the partial input. (iv) ScanNet [10]: We follow [4, 9, 70], cropping 48 chairs and 49 tables from real-world single scans (no ground truths). Proposed Omni-Comp Benchmark. These aforementioned benchmarks, while valuable, highlight clear limitations for comprehensive evaluation, including limited scales (e.g. , about 10 meshes in Redwood/Synthetic), restricted category diversity (e.g. , 2 categories in KITTI/ScanNet), and confinement to single partial pattern (e.g. , depth/LiDAR scans). To address these limitations, we introduce Omni-Comp, new benchmark designed for 5 more comprehensive and robust evaluation of 3D shape completion. Our benchmark features challenging set of 30 objects, each from distinct category, curated from diverse sources: 10 real-world scans from Redwood [5] (chosen for complex geometry), 10 real-world everyday objects from YCB [2] (motivated by downstream applications, e.g. , robotic grasping), and 10 synthetic shapes from [52] (chosen for rich semantic structure). Critically, inspired by [70], our benchmark generates three distinct partiality patterns for each object: (i) Single Scan: using the projection of the captured depth map for real-world data, and simulating standard depth camera capture for synthetic data; (ii) Random Crop: Representing arbitrary occlusions by randomly cropping portion; and (iii) Semantic Part: Keeping semantic component and removing other parts. By creating two samples for each pattern per object, the benchmark comprises 180 partial samples with ground truths. More details are in the supplementary material. Evaluation Metrics. We follow previous works [25, 31, 34, 70] to assess completion quality from three perspectives: (i) Chamfer Distance (CD) and Earth Movers Distance (EMD), reported both 102, which measures completion accuracy by the distance between the prediction and the ground truth; (ii) Unidirectional Chamfer Distance (UCD) and Unidirectional Hausdorff Distance (UHD), reported 104 and 102, which measure the fidelity of the prediction compared with the partial input; and (iii) Minimum Matching Distance (MMD) and Total Mutual Difference (TMD), reported both 102, which assess completion diversity across multiple outputs. More details are in the supplementary material. Baselines. We compare our method against comprehensive set of recent approaches, categorized into three main groups, including supervised methods [54, 68, 77], unsupervised methods [9, 70], and generative prior-based (zeroshot) methods [25, 31, 34]. To ensure fair comparison, we adopt the same experiment settings following previous zero-shot 3D completion methods [25, 31, 34, 70]. 4.2. Comparison with State-of-the-art Methods We present comprehensive quantitative and qualitative analyses across multiple benchmarks for assessment in this section. For fair visualization, all point clouds are converted to meshes using [45]. Evaluation on Completion Correctness. We first evaluate the completion correctness on single-scan partial shapes from the real-world Redwood dataset [5] and synthetic data [25, 37], as shown in Tab. 1 and Tab. 2. Our method achieves state-of-the-art performance in most categories and demonstrates significant improvements compared to Specifically, we outperform recent zero-shot methods. ComPC [25] by 27.2% in CD and 29.0% in EMD, and surpass GenPC [34] by 18.4% in CD and 36.1% in EMD, reTable 1. Quantitative comparisons on Redwood [5]. We highlight the best and second-best results. CD / EMD Table Exe-Chair Out-Chair Old-Chair Vase Off-Can Vespa Tricycle Trash Couch Average SVDFormer [77] AdaPoinTr [68] Shape-Inv [70] P2C [9] SDS-Comp [31] PCDreamer [54] GenPC [34] ComPC [25] Ours (Direct3D-S2) Ours (TRELLIS) 5.48/6.68 5.02/6.25 1.58/2.84 1.57/2.64 1.35/2.30 0.82/2.36 1.28/2.07 1.73/3.29 0.75/1.29 0.82/1.45 3.20/5.85 2.58/4.80 3.59/5.75 3.87/6.42 1.96/2.65 1.43/3.56 1.43/2.29 1.29/1.85 1.54/2.16 1.25/1.65 0.79/1.45 0.82/1.37 1.36/2.12 1.28/2.10 2.51/3.92 1.19/2.07 1.16/1.68 1.35/1.94 0.96/1.37 0.96/1. 3.79/5.95 3.62/5.64 4.55/7.39 3.72/5.82 2.77/3.77 2.61/5.34 1.36/2.20 1.14/1.63 1.11/1.69 1.07/1.37 5.70/6.98 5.14/6.50 3.91/7.40 4.54/7.04 3.00/5.25 2.41/3.64 2.86/4.85 2.89/4.13 5.13/6.71 4.47/6.35 3.10/4.77 3.36/4.82 3.79/4.28 2.62/3.89 2.72/4.36 3.55/3.92 3.10/4.91 1.96/3.54 4.36/7.24 6.75/10.7 3.36/5.73 2.20/4.30 1.36/2.47 1.21/1.86 2.73/4.96 1.83/3.66 5.05/7.40 7.78/11.2 3.18/3.49 2.53/3.82 1.38/2.97 2.65/2. 2.61/3.89 2.09/3.61 2.90/2.92 2.57/2.53 1.35/2.08 1.17/1.78 1.96/2.41 2.07/1.70 3.67/5.05 1.21/3.08 2.48/3.83 3.28/4.21 2.69/3.21 1.55/2.47 2.31/3.17 2.15/2.34 1.21/1.85 1.32/1. 1.86/2.92 1.01/2.09 1.95/2.76 2.55/3.16 2.95/4.56 1.47/2.61 1.58/2.78 1.59/2.58 1.95/2.26 0.88/1.14 3.54/5.15 2.77/4.33 3.19/5.15 3.87/5.81 2.74/3.93 1.88/3.41 1.74/2.88 1.95/2.59 1.64/2.19 1.42/1.84 Figure 4. Qualitative comparison on Redwood dataset [5]. We compare with various supervised and unsupervised methods [25, 31, 54, 68, 77], and visualize the output as meshes utilizing the commonly-used mesh reconstruction method [45]. spectively. We attribute this significant gap to fundamental difference in methodology. Prior methods [25, 34] either rely on generating multiple 2D views for supervision or lift 2D rendered image to 3D with post-processing. In contrast, our approach directly conducts completion by leveraging both the strong generative priors and the rich 3D information within the partial input, enabling more robust and accurate inference of the missing geometry. We show qualitative comparisons in Fig. 4. For specific examples, such as the leaf structures of the plant (see Row (a)) and the rim and wheels of the trash bin (see Rows (b-c)), our method achieves high fidelity in the observed regions while generating missing parts with precise surface geometry and coherent topology. In contrast, existing methods either distort the visible geometry or generate over-smoothed, implausible structures. This demonstrates that enforcing latent-spatial consistency with strong 3D generative priors yields superior geometric realism and structural integrity. Figure 5. Visual examples on ScanNet [10] and KITTI [15] realworld datasets, which only contain real scans of table, chair, and car, with very sparse points. LiDAR scans from KITTI [15] and noisy depth scans from ScanNet [10] As shown in Tab. 5, our method outperforms the latest zero-shot method ComPC [25] by large margin in almost all categories, confirming its robustness and superior fidelity on challenging real-world data. Some qualitative examples can be found in Fig. 5. Evaluation on Completion Fidelity. To test generalization and robustness to more challenging inputs, we evaluate on datasets with complex partiality, including sparse Evaluation on Omni-Comp. Our proposed Omni-Comp dataset specifically tests generalization across diverse partial patterns in both real-world and synthetic scenarios. As Table 2. Quantitative comparisons on the synthetic data [25, 37]. We highlight the best and second-best results. CD / EMD Horse Max-Planck Armadillo Cow Homer Teapot Bunny Nefertiti Bimba Ogre Lucy Dragon Average SVDFormer [77] AdaPoinTr [68] Shape-Inv [70] PCDreamer [54] ComPC [25] 4.33/5.17 8.84/7.95 4.88/5.45 8.60/8.52 6.55/10.4 4.94/5.72 2.52/4.34 5.20/3.85 1.29/1.77 1.22/1. 4.97/6.13 3.50/4.39 2.30/3.20 5.00/5.91 9.61/9.19 5.46/5.94 7.54/7.18 4.90/5.55 2.08/2.93 2.88/4.35 5.12/5.66 5.14/5.97 3.48/4.51 2.28/3.30 3.92/4.53 9.33/8.86 5.54/6.16 8.16/7.62 4.53/5.41 1.85/2.79 3.07/4.54 5.07/5.64 4.79/7.22 4.74/7.83 2.36/3.75 3.29/4.53 5.79/6.58 4.39/5.70 5.29/6.85 5.76/9.51 2.99/4.07 4.48/7.91 4.62/6.68 2.78/4.53 1.79/3.06 1.54/2.97 1.95/3.57 5.48/5.66 2.72/3.43 3.83/5.39 2.57/3.97 1.50/2.61 2.41/4.97 2.86/4.11 2.18/3.23 1.78/1.90 1.32/1.65 1.09/1.34 1.46/1.76 1.84/2.13 1.65/1.90 1.56/2.11 1.97/2.70 1.99/3.10 1.61/2.09 Ours (Direct3D-S2) 0.96/1.42 1.11/1.63 0.97/1.31 0.89/1.20 Ours (TRELLIS) 1.15/1.67 1.21/1.63 0.87/1.26 0.88/1.24 1.55/1.97 0.96/1.40 1.11/1.58 1.03/1.51 0.96/1.43 1.51/2.12 1.11/1.57 1.06/1.48 1.23/1.54 0.62/0.87 0.61/0.82 1.60/1.73 0.81/1.12 1.50/1.55 0.86/1.16 1.30/1.87 1.73/2.36 1.11/1.41 Table 3. Completion fidelity on ScanNet [10] and KITTI [15]. UCD / UHD ScanNet-Chair ScanNet-Table KITTI-Car SVDFormer [77] AdaPoinTr [68] Shape-Inv [70] P2C [9] PCDreamer [54] ComPC [25] Ours 1.4 / 2.4 1.4 / 2.4 4.0 / 9.3 3.4 / 5.0 1.5 / 3.5 2.0 / 5.3 0.8 / 2.0 1.4 / 2.4 1.3 / 2.4 6.6 / 11.0 2.4 / 4.9 1.3 / 3.7 3.0 / 7. 1.8 / 5.0 1.6 / 4.9 5.3 / 12.5 3.9 / 5.8 2.7 / 6.6 1.1 / 5.7 0.9 / 2.0 1.4 / 4.5 Table 4. Quantitative comparisons on our proposed Omni-Comp. CD / EMD Single Scan Random Crop Semantic Part SVDFormer [77] AdaPoinTr [68] Shape-Inv [70] SDS-Comp [31] PCDreamer [54] ComPC [25] 5.32 / 6.84 5.27 / 6.87 4.98 / 7.31 5.42 / 6.27 3.32 / 4.74 4.24 / 4.61 5.35 / 6.73 5.33 / 6.68 5.39 / 6.79 5.12 / 6.35 3.78 / 4.55 5.48 / 5.95 5.66 / 7.19 5.59 / 7.23 5.37 / 6.91 5.60 / 6.75 3.88 / 4.98 6.37 / 6.18 Ours 2.21 / 3.15 2.60 / 3.31 3.30 / 3.68 shown in Tab. 4, we report the average metrics on all shapes according to different partial patterns. Previous unsupervised approaches [25, 31] are heavily optimized for singlescan patterns, suffer consistent and significant performance drop when generalizing to other partiality types. In contrast, our method leverages pattern-agnostic 3D prior, enabling it to maintain robust and superior performance across all categories and patterns. On average, we achieve improvements of 49.6% in CD and 39.4% in EMD over ComPC [25], demonstrating the effectiveness of our idea. We provide some qualitative comparisons in Fig. 6. Specifically, baselines reliance on rendered 2D views that contain the complete object contour [25, 54] fails on complex inputs where contours are ill-posed (see Rows (c-d)). Furthermore, [25, 54] fail to complete the random crop and semantic part due to out-of-distribution categories and partial patterns (see Rows (a-b) and (e-f)). In stark contrast, our method proves robust across all these challenging cases, leveraging its pattern-agnostic 3D prior to reconstruct structurally plausible and geometrically detailed completions, demonstrating its strong generalizability. Evaluation on Completion Diversity. Finally, we assess completion diversity against the latest generative-based Figure 6. Qualitative comparisons on cases with different partial patterns from our Omni-Comp benchmark. Our approach produces more reasonable results than the latest methods [25, 54]. baselines [25, 54] on Redwood [5] and synthetic [25, 37] datasets, reporting average metrics over 5 random runs per object for each method. As shown in Tab. 5, our approach achieves consistently better MMD and TMD. Notably, while PCDreamer [54] utilizes generative models, its reliance on varying multi-view depth maps produces limited geometric variation, resulting in low TMD scores. Our strong performance on both metrics verifies that our method can generate diverse shape completions, as shown in Fig. 7. 4.3. Ablation Studies We conduct ablation studies to validate the contribution of each component in our framework, and the quantitative and qualitative results are presented in Tab. 6 and Fig. 8. 7 Table 5. Completion diversity evaluation on Redwood [5] and synthetic data [25, 37]. MMD / TMD PCDreamer [54] ComPC [25] Ours (Direct3D-S2) Ours (TRELLIS) Redwood 1.88 / 0.13 2.01 / 0.72 1.81 / 1.35 1.62 / 0. Synthetic 2.86 / 0.11 1.65 / 0.62 1.23 / 0.89 1.20 / 0.94 Table 6. Ablation studies. The baseline uses only latent replacement for completion. CD / EMD Redwood Synthetic (a) Baseline (latent replacement) (b) Full w/o ERS (c) Full w/o PNS (d) Full w/o IAS (e) IAS w/ 10 optimization steps 2.15 / 2.93 3.42 / 4.94 1.94 / 2.56 1.88 / 2.14 1.42 / 1.87 2.33 / 2.78 3.53 / 4.85 2.27 / 2.67 1.17 / 1.56 1.20 / 1.48 (f) Full pipeline (Ours) 1.42 / 1. 1.11 / 1.41 Naive Baseline. We first establish naive baseline that only utilizes the latents from the partial shape for replacement. Due to the latent discrepancy of the corresponding regions of partial and ground truth samples, it produces poor completion results both quantitatively and qualitatively, as shown in Tab. 6 (comparing Rows (a) and (f)) and Fig. 8. Analysis of Key Components. We analyze the contribution of our three key components, Explicit Replacement Stage (ERS), Partial-aware Noise Schedule (PNS), and Implicit Alignment Stage (IAS), by ablating them individually. w/o ERS: Comparing Rows (b) and (f) in Tab. 6, removing the ERS causes the most significant performance drop. Without this explicit geometric conditioning, the model fails to preserve the input structure and instead hallucinates novel shape based on the generative prior (e.g. , completely different horse pose in Fig. 8(c)). This confirms that ERS is critical for ensuring input fidelity. w/o PNS: Comparing Rows (c) and (f) in Tab. 6, disabling the PNS leads to noticeable stripe-like artifacts on the surface. We attribute this to the mismatch in the denoising process between the clean (known) partial input and the noisy (unknown) regions being generated. PNS is thus essential for seamlessly blending these two regions. w/o IAS: Comparing Rows (d) and (f) in Tab. 6, removing the IAS, which harmonizes the latent context via optimization, results in small holes and boundary inconsistencies, as highlighted by red boxes in Fig. 8. This demonstrates that ERS alone is insufficient; IAS is necessary to ensure the final latent representation is coherent. Optimization steps in IAS: We also do ablation about the optimization steps in IAS. As can be seen in Tab. 6 Figure 7. Visual examples of the completion diversity from our method on the real-world data. Figure 8. Visual comparison of the ablation studies. The red boxes highlight the artifacts and holes. (e), using 10 steps for optimization at each timestep does not show obvious performance gain compared with (f). The full pipeline successfully combines these components to achieve geometrically accurate and fidelitypreserving completions, with the best performance in Tab. 6. The horse, as shown in Fig. 8 (f), exhibits excellent geometric quality without noticeable artifacts. 5. Conclusion We presented LaS-Comp, zero-shot and category-agnostic framework for 3D shape completion that leverages the rich geometric priors of 3D foundation models. The method integrates two complementary components, an Explicit Replacement Stage and an Implicit Alignment Stage, to jointly ensure high-fidelity reconstruction and global geometric coherence. We also introduce Omni-Comp, benchmark combining real-world scans and synthetic shapes with diverse partial patterns for comprehensive evaluation. Experiments on Omni-Comp and standard benchmarks show that our method generalizes well across diverse categories and partial patterns, surpassing existing approaches. Limitations. Although our method outperforms existing approaches on real-world scans, extremely noisy inputs remain challenging and may lead to imperfect completions. Such cases arise when heavy noise obscures the underlying geometry, leaving limited cues for recovery. Further discussion is provided in the supplementary material."
        },
        {
            "title": "References",
            "content": "[1] Yingjie Cai, Kwan-Yee Lin, Chao Zhang, Qiang Wang, Xiaogang Wang, and Hongsheng Li. Learning Structured Latent Space for Unsupervised Point Cloud Completion. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 55435553, 2022. 2 [2] Berk alli, Aaron Walsman, Arjun Singh, Siddhartha S. Srinivasa, Pieter Abbeel, and Aaron M. Dollar. Benchmarking in Manipulation Research: The YCB Object and Model Set and Benchmarking Protocols. In International Conference on Advanced Robotics (ICAR), 2015. 5 [3] Angel X. Chang, Thomas Funkhouser, Leonidas J. Guibas, Pat Hanrahan, Qixing Huang, Zimo Li, Silvio Savarese, Manolis Savva, Shuran Song, Hao Su, et al. ShapeNet: An Information-Rich 3D Model Repository. arXiv preprint arXiv:1512.03012, 2015. 3 [4] Xuelin Chen, Baoquan Chen, and Niloy Mitra. Unpaired Point Cloud Completion on Real Scans using Adversarial In International Conference on Learning RepreTraining. sentations (ICLR), 2020. 2, 5 [5] Sungjoon Choi, Qian-Yi Zhou, Stephen Miller, and Vladlen Koltun. Large Dataset of Object Scans. arXiv:1602.02481, 2016. 2, 5, 6, 7, 8 [6] Jisheng Chu, Wenrui Li, Xingtao Wang, Kanglin Ning, Yidan Lu, and Xiaopeng Fan. Digging into Intrinsic Contextual Information for High-fidelity 3D Point Cloud Completion. In AAAI Conference on Artificial Intelligence (AAAI), pages 25732581, 2025. [7] Lei Chu, Hao Pan, and Wenping Wang. Unsupervised Shape Completion via Deep Prior in the Neural Tangent Kernel Perspective. ACM Transactions on Graphics, 40(3):117, 2021. 2 [8] Ruihang Chu, Enze Xie, Shentong Mo, Zhenguo Li, Matthias Nießner, Chi-Wing Fu, and Jiaya Jia. DiffComplete: Diffusion-based Generative 3D Shape Completion. In Conference on Neural Information Processing Systems (NeurIPS), pages 7595175966, 2023. 2 [9] Ruikai Cui, Shi Qiu, Saeed Anwar, Jiawei Liu, Chaoyue Xing, Jing Zhang, and Nick Barnes. P2C: Self-supervised Point Cloud Completion from Single Partial Clouds. In IEEE International Conference on Computer Vision (ICCV), pages 1435114360, 2023. 2, 5, 6, 7 [10] Angela Dai, Angel X. Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and Matthias Nießner. ScanNet: Richly-Annotated 3D Reconstructions of Indoor Scenes. In 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 24322443, 2017. 5, 6, 7 [11] Angela Dai, Charles Ruizhongtai Qi, and Matthias Nießner. Shape Completion using 3D-Encoder-Predictor CNNs and Shape Synthesis. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 58685877, 2017. 1, 2 [12] Matt Deitke, Ruoshi Liu, Matthew Wallingford, Huong Ngo, Oscar Michel, Aditya Kusupati, Alan Fan, Christian Laforte, Vikram Voleti, Samir Yitzhak Gadre, et al. ObjaversearXiv preprint XL: Universe of 10M+ 3D Objects. arXiv:2307.05663, 2023. 2 [13] Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs, Oscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana Ehsani, Aniruddha Kembhavi, and Ali Farhadi. Objaverse: Universe of Annotated 3D Objects. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 1314213153, 2023. [14] Keyu Du, Jingyu Hu, Haipeng Li, Hao Xu, Haibin Huang, Chi-Wing Fu, and Shuaicheng Liu. Hierarchical neural semantic representation for 3d semantic correspondence. In Proceedings of SIGGRAPH Asia, pages 111, 2025. 2 [15] Andreas Geiger. Are We Ready for Autonomous Driving? In Proceedings of The KITTI Vision Benchmark Suite. the 2012 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), page 33543361, 2012. 5, 6, 7 [16] Xianglong He, Zi-Xin Zou, Chia-Hao Chen, Yuan-Chen Guo, Ding Liang, Chun Yuan, Wanli Ouyang, Yan-Pei Cao, and Yangguang Li. SparseFlex: High-Resolution and Arbitrary-Topology 3D Shape Modeling. In IEEE International Conference on Computer Vision (ICCV), 2025. 1 [17] Jonathan Ho and Tim Salimans. Classifier-free Diffusion Guidance. arxiv preprint arxiv:2207.12598, 2022. 3 [18] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising DifIn Conference on Neural Infusion Probabilistic Models. formation Processing Systems (NeurIPS), pages 68406851, 2020. 2 [19] Sangmin Hong, Mohsen Yavartanoo, Reyhaneh Neshatavar, and Kyoung Mu Lee. ACL-SPC: Adaptive Closed-Loop system for Self-supervised Point Cloud Completion. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 94359444, 2023. 2 [20] Yicong Hong, Kai Zhang, Jiuxiang Gu, Sai Bi, Yang Zhou, Difan Liu, Feng Liu, Kalyan Sunkavalli, Trung Bui, and Hao Tan. LRM: Large Reconstruction Model for Single Image to In International Conference on Learning Representa3D. tions (ICLR), 2024. 3 [21] Jingyu Hu*, Ka-Hei Hui*, Zhengzhe Liu, Hao Zhang, and Chi-Wing Fu. Clipxplore: Coupled clip and shape spaces for 3d shape exploration. In Proceedings of SIGGRAPH Asia, pages 112, 2023. 2 [22] Jingyu Hu, Ka-Hei Hui, Zhengzhe Liu, Ruihui Li, and ChiWing Fu. Neural wavelet-domain diffusion for 3d shape generation, inversion, and manipulation. ACM Transactions on Graphics (TOG), 42(6), 2024. 2 [23] Jingyu Hu, Ka-Hei Hui, Zhengzhe Liu, Hao Zhang, and ChiWing Fu. Cns-edit: 3d shape editing via coupled neural In Proceedings of SIGGRAPH, pages shape optimization. 112, 2024. [24] Jingyu Hu, Bin Hu, Ka-Hei Hui, Haipeng Li, Zhengzhe Liu, Daniel Cohen-Or, and Chi-Wing Fu. Pegasus: 3d personalization of geometry and appearance. arXiv preprint arXiv:2602.08198, 2026. 2 [25] Tianxin Huang, Zhiwen Yan, Yuyang Zhao, and Gim Lee. ComPC: Completing 3D Point Cloud with 2D Diffusion Priors. In International Conference on Learning Representations (ICLR), pages 5176551784, 2025. 1, 2, 3, 5, 6, 7, 8 9 [26] Zitian Huang, Yikuan Yu, Jiawen Xu, Feng Ni, and Xinyi Le. PF-Net: Point Fractal Network for 3D Point Cloud Completion. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 76627670, 2020. 2 [27] Ka-Hei Hui, Ruihui Li, Jingyu Hu, and Chi-Wing Fu. Neural wavelet-domain diffusion for 3d shape generation. In Proceedings of SIGGRAPH Asia, pages 19, 2022. 2 [28] Ka-Hei Hui, Ruihui Li, Jingyu Hu, and Chi-Wing Fu. Neural template: Topology-aware reconstruction and disentangled generation of 3d meshes. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 18572 18582, 2022. [29] Shun Iwase, Katherine Liu, Vitor Guizilini, Adrien Gaidon, Kris Kitani, Rares, Ambrus, , and Sergey Zakharov. Zero-Shot Multi-Object Scene Completion. In ECCV, 2024. 1 [30] Shun Iwase, Muhammad Zubair Irshad, Katherine Liu, Vitor Guizilini, Robert Lee, Takuya Ikeda, Amma Ayako, Koichi Nishiwaki, Kris Kitani, Rares Ambrus, and Sergey Zakharov. ZeroGrasp: Zero-Shot Shape Reconstruction Enabled Robotic Grasping. In CVPR, 2025. 1 [31] Yoni Kasten, Ohad Rahamim, and Gal Chechik. Point Cloud Completion with Pretrained Text-to-image Diffusion ModIn Conference on Neural Information Processing Sysels. tems (NeurIPS), pages 1217112191, 2023. 1, 3, 5, 6, 7 [32] Jihun Kim, Hyeokjun Kwon, Yunseo Yang, and Kuk-Jin Yoon. Learning Point Cloud Completion without ComIn IEEE Inplete Point Clouds: Pose-aware Approach. ternational Conference on Computer Vision (ICCV), pages 1415714167, 2023. 2 [33] Jeongsol Kim, Bryan Sangwoo Kim, and Jong Chul Ye. FlowDPS : Flow-Driven Posterior Sampling for Inverse In Proceedings of the IEEE/CVF International Problems. Conference on Computer Vision (ICCV), pages 12328 12337, 2025. 3 [34] An Li, Zhe Zhu, and Mingqiang Wei. GenPC: Zero-shot Point Cloud Completion via 3D Generative Priors. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 13081318, 2025. 1, 3, 5, 6 [35] Shanshan Li, Pan Gao, Xiaoyang Tan, and Mingqiang Wei. ProxyFormer: Proxy Alignment Assisted Point Cloud ComIn IEEE pletion with Missing Part Sensitive Transformer. Conference on Computer Vision and Pattern Recognition (CVPR), pages 94669475, 2023. [36] Yangguang Li, Zi-Xin Zou, Zexiang Liu, Dehu Wang, Yuan Liang, Zhipeng Yu, Xingchao Liu, Yuan-Chen Guo, Ding Liang, Wanli Ouyang, et al. TripoSG: High-Fidelity 3D Shape Synthesis using Large-Scale Rectified Flow Models. arXiv preprint arXiv:2502.06608, 2025. 1 [37] Yaron Lipman, David Levin, and Daniel Cohen-Or. Green Coordinates. ACM Transactions on Graphics, 27(3):110, 2008. 5, 7, 8 [38] Mengya Liu, Ajad Chhatkuli, Janis Postels, Luc Van Gool, and Federico Tombari. Self-supervised Shape Completion via Involution and Implicit Correspondences. In European Conference on Computer Vision (ECCV), pages 212229, 2024. 2 [39] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tokmakov, Sergey Zakharov, and Carl Vondrick. Zero-1-to-3: Zero-shot One Image to 3D Object. In IEEE International Conference on Computer Vision (ICCV), pages 92989309, 2023. 3 [40] Zhengzhe Liu, Jingyu Hu, Ka-Hei Hui, Xiaojuan Qi, Daniel Cohen-Or, and Chi-Wing Fu. Exim: hybrid explicitimplicit representation for text-guided 3d shape generation. ACM Transactions on Graphics (SIGGRAPH Asia), 42(6): 112, 2023. 2 [41] Shitong Luo and Wei Hu. Diffusion Probabilistic Models for 3D Point Cloud Generation. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 2837 2845, 2021. [42] Zhaoyang Lyu, Zhifeng Kong, Xudong Xu, Liang Pan, and Dahua Lin. Conditional Point Diffusion-Refinement Paradigm for 3D Point Cloud Completion. In International Conference on Learning Representations (ICLR), 2022. 2 [43] Mark Pauly, Niloy Mitra, Joachim Giesen, Markus Gross, and Leonidas Guibas. Example-Based 3D Scan Completion. In Eurographics Symposium on Geometry Processing (SGP), 2005. 1 [44] Mark Pauly, Niloy Mitra, Johannes Wallner, Helmut Pottmann, and Leonidas Guibas. Discovering Structural Regularity in 3D Geometry. In ACM Transactions on Graphics (SIGGRAPH), pages 111, 2008. 1 [45] Songyou Peng, Chiyu Max Jiang, Yiyi Liao, Michael Niemeyer, Marc Pollefeys, and Andreas Geiger. Shape As Points: Differentiable Poisson Solver. In Conference on Neural Information Processing Systems (NeurIPS), 2021. 5, 6 [46] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-Resolution Image In IEEE ConferSynthesis with Latent Diffusion Models. ence on Computer Vision and Pattern Recognition (CVPR), pages 1068410695, 2022. 3 [47] Chao-Hui Shen, Hongbo Fu, Kang Chen, and Shi-Min Hu. Structure Recovery by Part Assembly. ACM Transactions on Graphics (SIGGRAPH), 31(6):111, 2012. [48] Minhyuk Sung, Vladimir Kim, Roland Angst, and Leonidas Guibas. Data-driven Structural Priors for Shape Completion. ACM Transactions on Graphics (SIGGRAPH), 34(6):111, 2015. 1 [49] Jiaxiang Tang, Zhaoxi Chen, Xiaokang Chen, Tengfei Wang, Gang Zeng, and Ziwei Liu. LGM: Large Multi-View GausIn sian Model for High-Resolution 3D Content Creation. European Conference on Computer Vision (ECCV), pages 1 18, 2024. 3 [50] Sebastian Thrun and Ben Wegbreit. Shape from Symmetry. In IEEE International Conference on Computer Vision (ICCV), pages 18241831, 2005. 1 [51] Jun Wang, Ying Cui, Dongyan Guo, Junxia Li, Qingshan Liu, and Chunhua Shen. PointAttN: You only Need AttenIn AAAI Conference on tion for Point Cloud Completion. Artificial Intelligence (AAAI), pages 54725480, 2024. 2 [52] Penghao Wang, Yiyang He, Xin Lv, Yukai Zhou, Lan Xu, Jingyi Yu, and Jiayuan Gu. PartNeXt: Next-Generation 10 Dataset for Fine-Grained and Hierarchical 3D Part Understanding. In Conference on Neural Information Processing Systems (NeurIPS), 2025. Correction. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 98689877, 2023. 1 [53] Xiaogang Wang, Marcelo Ang Jr, and Gim Hee Lee. Cascaded Refinement Network for Point Cloud Completion. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 790799, 2020. 2 [54] Guangshun Wei, Yuan Feng, Long Ma, Chen Wang, Yuanfeng Zhou, and Changjian Li. PCDreamer: Point Cloud In ProCompletion Through Multi-view Diffusion Priors. ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 2724327253, 2025. 2, 5, 6, 7, 8 [55] Xin Wen, Zhizhong Han, Yan-Pei Cao, Pengfei Wan, Wen Zheng, and Yu-Shen Liu. Cycle4Completion: Unpaired Point Cloud Completion using Cycle Transformation with In IEEE Conference on ComMissing Region Coding. puter Vision and Pattern Recognition (CVPR), pages 13080 13089, 2021. 2 [56] Xin Wen, Peng Xiang, Zhizhong Han, Yan-Pei Cao, Pengfei Wan, Wen Zheng, and Yu-Shen Liu. PMP-Net: Point Cloud Completion by learning Multi-step Point Moving Paths. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 74437452, 2021. 2 [57] Rundi Wu, Xuelin Chen, Yixin Zhuang, and Baoquan Chen. Multimodal Shape Completion via Conditional Generative In European Conference on ComAdversarial Networks. puter Vision (ECCV), pages 281296, 2020. [58] Shuang Wu, Youtian Lin, Feihu Zhang, Yifei Zeng, Yikang Yang, Yajie Bao, Jiachen Qian, Siyu Zhu, Philip Torr, Xun Cao, and Yao Yao. Direct3D-S2: Gigascale 3D Generation Made Easy with Spatial Sparse Attention. In Conference on Neural Information Processing Systems (NeurIPS), 2025. 1, 2, 3 [59] Jianfeng Xiang, Zelong Lv, Sicheng Xu, Yu Deng, Ruicheng Wang, Bowen Zhang, Dong Chen, Xin Tong, and Jiaolong Yang. Structured 3D Latents for Scalable and Versatile 3D In IEEE Conference on Computer Vision and Generation. Pattern Recognition (CVPR), pages 2146921480, 2025. 1, 2, 3 [60] Peng Xiang, Xin Wen, Yu-Shen Liu, Yan-Pei Cao, Pengfei Wan, Wen Zheng, and Zhizhong Han. SnowflakeNet: Point Cloud Completion by Snowflake Point Deconvolution with Skip-Transformer. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 54995509, 2021. 2 [61] Chulin Xie, Chuxin Wang, Bo Zhang, Hao Yang, Dong Chen, and Fang Wen. Style-based Point Generator with AdIn IEEE versarial Rendering for Point Cloud Completion. Conference on Computer Vision and Pattern Recognition (CVPR), pages 46194628, 2021. 2 [62] Haozhe Xie, Hongxun Yao, Shangchen Zhou, Jiageng Mao, Shengping Zhang, and Wenxiu Sun. GRNet: Gridding Residual Network for Dense Point Cloud Completion. In European Conference on Computer Vision (ECCV), pages 365381, 2020. 2 [63] Weilong Yan, Robby T. Tan, Bing Zeng, and Shuaicheng Liu. Deep Homography Mixture for Single Image Rolling Shutter [64] Weilong Yan, Ming Li, Haipeng Li, Shuwei Shao, and Robby T. Tan. Synthetic-to-Real Self-supervised Robust Depth Estimation via Learning with Motion and Structure Priors. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 21880 21890, 2025. [65] Xingguang Yan, Liqiang Lin, Niloy J. Mitra, Dani Lischinski, Daniel Cohen-Or, and Hui Huang. ShapeFormer: Shapelet Transformer for Multivariate Time Series Classification. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 62396249, 2022. 2 [66] Hong-Xing Yu, Haoyi Duan, Charles Herrmann, William T. Freeman, and Jiajun Wu. WonderWorld: Interactive 3D Scene Generation from Single Image. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 59165926, 2025. 1 [67] Xumin Yu, Yongming Rao, Ziyi Wang, Zuyan Liu, Jiwen Lu, and Jie Zhou. PoinTr: Diverse Point Cloud Completion In IEEE Conference with Geometry-Aware Transformers. on Computer Vision and Pattern Recognition (CVPR), pages 1249812507, 2021. 2 [68] Xumin Yu, Yongming Rao, Ziyi Wang, Jiwen Lu, and Jie Zhou. AdaPoinTr: Diverse Point Cloud Completion with IEEE TransacAdaptive Geometry-Aware Transformers. tions on Pattern Analysis and Machine Intelligence, 45(12): 1411414130, 2023. 5, 6, 7 [69] Wentao Yuan, Tejas Khot, David Held, Christoph Mertz, and In InMartial Hebert. PCN: Point Completion Network. ternational Conference on 3D Vision (3DV), pages 728737, 2018. 1, 2 [70] Junzhe Zhang, Xinyi Chen, Zhongang Cai, Liang Pan, Haiyu Zhao, Shuai Yi, Chai Kiat Yeo, Bo Dai, and Chen Change Loy. Unsupervised 3D Shape Completion through GAN Inversion. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2021. 2, 5, 6, [71] Longwen Zhang, Ziyu Wang, Qixuan Zhang, Qiwei Qiu, Anqi Pang, Haoran Jiang, Wei Yang, Lan Xu, and Jingyi Yu. CLAY: Controllable Large-scale Generative Model for Creating High-quality 3D Assets. ACM Transactions on Graphics (SIGGRAPH), 43(4):120, 2024. 1 [72] Mengtan Zhang, Yi Feng, Qijun Chen, and Rui Fan. DCPIDepth: Explicitly Infusing Dense Correspondence Prior to Unsupervised Monocular Depth Dstimation. IEEE Transactions on Image Processing, 34:42584272, 2025. 1 [73] Wenxiao Zhang, Qingan Yan, and Chunxia Xiao. Detail Preserved Point Cloud Completion via Separated Feature AgIn European Conference on Computer Vision gregation. (ECCV), pages 512528, 2020. 2 [74] Xindan Zhang, Weilong Yan, Yufei Shi, Xuerui Qiu, Tao He, Ying Li, Ming Li, and Hehe Fan. 4DPC2hat: Towards Dynamic Point Cloud Understanding with Failure-Aware Bootstrapping. arXiv preprint arXiv:2602.03890, 2026. 2 [75] Zibo Zhao, Zeqiang Lai, Qingxiang Lin, Yunfei Zhao, Haolin Liu, Shuhui Yang, Yifei Feng, Mingxin Yang, Sheng 11 Zhang, Xianghui Yang, et al. Hunyuan3D 2.0: Scaling Diffusion Models for High Resolution Textured 3D Assets Generation. arXiv preprint arXiv:2501.12202, 2025. [76] Linqi Zhou, Yilun Du, and Jiajun Wu. 3D Shape Generation and Completion through Point-Voxel Diffusion. In IEEE International Conference on Computer Vision (ICCV), pages 58265835, 2021. 2 [77] Zhe Zhu, Honghua Chen, Xing He, Weiming Wang, Jing Qin, and Mingqiang Wei. SVDFormer: Complementing Point Cloud via Self-view Augmentation and Self-structure Dual-Generator. In IEEE International Conference on Computer Vision (ICCV), pages 1450814518, 2023. 2, 5, 6,"
        }
    ],
    "affiliations": [
        "Changhong Intelligent Robot",
        "National University of Singapore",
        "The Chinese University of Hong Kong",
        "University of Electronic Science and Technology of China"
    ]
}