{
    "paper_title": "UniMoE-Audio: Unified Speech and Music Generation with Dynamic-Capacity MoE",
    "authors": [
        "Zhenyu Liu",
        "Yunxin Li",
        "Xuanyu Zhang",
        "Qixun Teng",
        "Shenyuan Jiang",
        "Xinyu Chen",
        "Haoyuan Shi",
        "Jinchao Li",
        "Qi Wang",
        "Haolan Chen",
        "Fanbo Meng",
        "Mingjun Zhao",
        "Yu Xu",
        "Yancheng He",
        "Baotian Hu",
        "Min Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advances in unified multimodal models indicate a clear trend towards comprehensive content generation. However, the auditory domain remains a significant challenge, with music and speech often developed in isolation, hindering progress towards universal audio synthesis. This separation stems from inherent task conflicts and severe data imbalances, which impede the development of a truly unified audio generation model. To address this challenge, we propose UniMoE-Audio, a unified speech and music generation model within a novel Dynamic-Capacity Mixture-of-Experts (MoE) framework. Architecturally, UniMoE-Audio introduces a Top-P routing strategy for dynamic expert number allocation, and a hybrid expert design comprising routed experts for domain-specific knowledge, shared experts for domain-agnostic features, and null experts for adaptive computation skipping. To tackle data imbalance, we introduce a three-stage training curriculum: 1) Independent Specialist Training leverages original datasets to instill domain-specific knowledge into each \"proto-expert\" without interference; 2) MoE Integration and Warmup incorporates these specialists into the UniMoE-Audio architecture, warming up the gate module and shared expert using a subset of balanced dataset; and 3) Synergistic Joint Training trains the entire model end-to-end on the fully balanced dataset, fostering enhanced cross-domain synergy. Extensive experiments show that UniMoE-Audio not only achieves state-of-the-art performance on major speech and music generation benchmarks, but also demonstrates superior synergistic learning, mitigating the performance degradation typically seen in naive joint training. Our findings highlight the substantial potential of specialized MoE architecture and curated training strategies in advancing the field of universal audio generation. Homepage: https://mukioxun.github.io/Uni-MoE-site/home.html"
        },
        {
            "title": "Start",
            "content": "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 1 UniMoE-Audio: Unified Speech and Music Generation with Dynamic-Capacity MoE Zhenyu Liu, Yunxin Li, Xuanyu Zhang, Qixun Teng, Shenyuan Jiang, Xinyu Chen, Haoyuan Shi, Jinchao Li, Qi Wang, Haolan Chen, Fanbo Meng, Mingjun Zhao, Yu Xu, Yancheng He, Baotian Hu, Min Zhang AbstractRecent advances in unified multimodal models indicate clear trend towards comprehensive content generation. However, the auditory domain remains significant challenge, with music and speech often developed in isolation, hindering progress towards universal audio synthesis. This separation stems from inherent task conflicts and severe data imbalances, which impede the development of truly unified audio generation model. To address this challenge, we propose UniMoE-Audio, unified speech and music generation model within novel Dynamic-Capacity Mixture-of-Experts (MoE) framework. Architecturally, UniMoE-Audio introduces Top-P routing strategy for dynamic expert number allocation, and hybrid expert design comprising routed experts for domain-specific knowledge, shared experts for domain-agnostic features, and null experts for adaptive computation skipping. To tackle data imbalance, we introduce three-stage training curriculum: 1) Independent Specialist Training leverages original datasets to instill domain-specific knowledge into each proto-expert without interference; 2) MoE Integration and Warmup incorporates these specialists into the UniMoE-Audio architecture, warming up the gate module and shared expert using subset of balanced dataset; and 3) Synergistic Joint Training trains the entire model end-to-end on the fully balanced dataset, fostering enhanced cross-domain synergy. Extensive experiments show that UniMoE-Audio not only achieves state-of-the-art performance on major speech and music generation benchmarks, but also demonstrates superior synergistic learning, mitigating the performance degradation typically seen in naive joint training. Our findings highlight the substantial potential of specialized MoE architecture and curated training strategies in advancing the field of universal audio generation. Homepage: https://mukioxun.github.io/Uni-MoE-site/home.html. Index TermsMixture of Experts, Multimodal Large Language Model, Speech Synthetic, Music Generation."
        },
        {
            "title": "A Hallmark of human intelligence is the seamless ability",
            "content": "to perceive, reason, and create across multiple modalities, effortlessly blending language, vision, and audio. Emulating this holistic capability represents grand challenge and core objective in the pursuit of more general artificial intelligence. The recent ascendancy of Large Language Models (LLMs) has served as powerful catalyst, paving the way for unified models that can understand and generate content across these diverse data streams. Significant progress has been made in systems that jointly process text, images, video, and even speech within single architecture [1], [2], [3], [4], [5], [6]. Nevertheless, critical imbalance persists in the treatment of the auditory domain. While speech has been primary focus of integration [5], [6], musica domain of comparable complexity and cultural richnessremains largely siloed and excluded from these unified frameworks. This fundamental omission not only curtails the ambition of universal audio synthesis but also stands as significant impediment to developing AI with truly comprehensive multimodal intelligence. The primary obstacle to unifying speech and music generation stems from two fundamental challenges. The first is Zhenyu Liu, Yunxin Li, Xuanyu Zhang, Qixun Teng, Shenyuan Jiang, Xinyu Chen, Haoyuan Shi, Jinchao Li, Qi Wang, Baotian Hu and Min Zhang are with the Department of Computer Science and Technology, Harbin Institute of Technology, Shenzhen, China. (e-mail: liuzhenyuhit@gmail.com, hubaotian@hit.edu.cn, and zhangmin2021@hit.edu.cn) Zhenyu Liu, Baotian Hu and Min Zhang are also with the Shenzhen Loop Area Institute, Shenzhen, China. Baotian Hu is the corresponding author. (e-mail: hubaotian@hit.edu.cn) task conflict, arising from the divergent objectives of speech and music generation. The former is primarily concerned with semantic intelligibility and speaker identity, whereas the latter focuses on capturing complex structures like harmony and rhythm. This divergence creates conflicting optimization pressures within shared model, where progress on one task can impede the other. Recently, the MoE paradigm has emerged as promising architecture for mitigating conflicts of multimodal understanding [7], [8], [4]. Despite these advances, its application and further optimization for unified audio generation remain largely unexplored. Beyond task conflict, another major hurdle is data imbalance. Highquality, large-scale speech corpora are far more abundant than their musical counterparts. The detrimental effects of this disparity are evident in prior work [9]. Consequently, naive joint training approach often allows the data-rich speech task to dominate the learning process, resulting in substantial degradation in musical quality. Our preliminary experiments empirically confirm this degradation (Figure 1), showing that jointly trained model performs significantly worse than specialized models, with the performance drop being particularly severe for the data-scarce music task. Therefore, the central scientific question we address is: how to overcome both task conflict and data imbalance, enabling shared model to master speech and music generation synergistically? Our approach addresses these challenges at both the architectural and training curriculum levels. Architecturally, we propose UniMoE-Audio, which leverages novel DynamicCapacity MoE for mitigating task conflict. Instead of directly applying the conventional MoE, we provide two key archi5 2 0 2 5 1 ] . [ 1 4 4 3 3 1 . 0 1 5 2 : r JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 2 Fig. 1: Performance of UniMoE-Audio. Left: Comparison against specialized baselines reveals the failure of naive joint training, which causes clear performance degradation on speech generation and more significant decline on music generation. In contrast, our UniMoE-Audio yields synergistic gains across both tasks. Right: Radar charts show UniMoEAudio achieving the best comprehensive performance against leading models on wide array of speech (a) and music (b) metrics. tectural optimizations to improve both routing flexibility and functional decoupling. First, we introduce dynamiccapacity routing strategy that replaces the conventional fixedcapacity routing. Based on the Top-P sampling, this strategy dynamically adjusts the number of experts allocated to each token based on their complexity, thus enabling more flexible expert combinations. Second, we present hybrid expert design to establish clear functional specialization, comprising: 1) conditional routed experts for domain-specific knowledge; 2) constantly active shared experts to handle domain-agnostic features; and 3) null experts to skip computation adaptively. While our architecture provides the structural means to mitigate task conflict, we introduce tightly coupled threestage training curriculum to address data imbalance. The curriculum unfolds as follows: (1) Independent Specialist Training leverages the original, uncurated datasets to instill domain-specific knowledge into each proto-expert without interference. (2) MoE Integration and Warmup then integrates these specialists into the UniMoE-Audio architecture. This stage begins by creating curated, balanced dataset via rigorous data filtering pipeline. To ensure training stability, the newly added components (i.e. the gate module and the shared expert) are then exclusively warmed up on small subset of this curated data. (3) Synergistic Joint Training finally trains the entire model on the full balanced dataset, fostering effective knowledge transfer across domains. Our main contributions can be summarized as follows: We propose UniMoE-Audio, unified speech and music generation model based on novel DynamicCapacity MoE framework. By integrating Top-P routing strategy for adaptive resource allocation and hybrid expert design for functional decoupling, our architecture effectively mitigates the inherent task conflict between speech and music generation. To leverage this architecture and tackle data imbalance, we introduce data-aware, three-stage training curriculum. This curriculum systematically overcomes the data imbalance challenge by orchestrating independent specialist training, router warmup, and synergistic joint training, enabling robust and effective learning from highly imbalanced data sources without resorting to conventional data sampling. We provide extensive experiments to show the UniMoE-Audios effectiveness, achieving state-of-theart or competitive performance on major speech and music generation benchmarks. Furthermore, our indepth analysis reveals the dynamic activation patterns of the MoE model, offering valuable insights into how the unified MoE model navigates diverse audio generation tasks."
        },
        {
            "title": "2 RELATED WORK\n2.1 Domain-Specific Audio Generation Models",
            "content": "Large Spoken Models. The paradigm of generative AI, powered by Large Language Models (LLMs), has recently catalyzed revolution in text-to-speech (TTS), giving rise to the field of the Large Spoken Models. This approach fundamentally reframes speech synthesis as conditional language modeling problem. Typically, Speech LLM consists of large, decoder-only Transformer and neural audio codec. Given textual prompt and optional voice conditions, the Transformer autoregressively generates sequence of discrete audio tokens, which are then converted back into continuous waveform by the codec. This framework has enabled unprecedented capabilities in zero-shot voice cloning and expressive, controllable speech generation. The seminal work in this area, VALL-E [10], pioneered this approach by discretizing speech into acoustic tokens via the EnCodec [11] and modeling them conditioned on text. This breakthrough laid the groundwork for proliferation of subsequent models, including VALL-E [12], SpearTTS [13], and Makea-Voice [14], which further refined tokenization schemes and text-to-acoustic alignment. Building on this foundation, the field has seen rapid advancements towards greater robustness and versatility. For instance, CosyVoice [15] leverages multi-task, multi-stage training curriculum to achieve stateof-the-art performance across wide array of speech synthesis tasks. Concurrently, StepAudio [6] demonstrates the JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 power of training on massive-scale synthetic data to produce exceptionally high-fidelity speech with rich emotional and stylistic diversity. Large Music Models. Mirroring the evolution in speech synthesis, the field of music generation has also increasingly adopted the Large Language Model paradigm, reframing music composition as sequence generation task guided by textual or visual prompts. While diffusion-based models like MusicLM [16] and Stable Audio Open [17] have achieved remarkable results, autoregressive models have demonstrated compelling alternative. MusicGen [18] was pivotal work that validated the feasibility of modeling music with single Transformer decoder, generating highfidelity music from discrete tokens. Pushing the boundaries further, subsequent works have explored more complex architectures and functionalities. Built upon the architecture of Llama2 [19], YuE [20] introduced track-decoupled prediction strategy to handle long-form music generation. MuMuLlama [21] introduces multimodal music generation by jointly training on text-to-music and vision-to-music tasks. These advancements collectively indicate the power and viability of autoregressive framework for controllable music synthesis. While the aforementioned studies demonstrate substantial advancements in speech and music generation, they primarily focus on advancing the state-of-the-art within their respective domains. Our work, in contrast, shifts the focus from domain-specific excellence to the challenge of crossdomain unification. This line of inquiry is prompted by the observation that both fields, despite their distinct objectives, have independently converged on similar technical paradigm: autoregressive modeling of discrete audio tokens. This parallel evolution suggests the potential for single unified architecture that handle both speech and music generation, yet the feasibility and inherent complexities of such unification remain largely unexplored. Therefore, our work represents foundational investigation into this underexplored area, aiming to broaden the scope of what generative audio models can achieve. modalities. Specifically, we leverage MoE architecture to mitigate task conflict and data-aware, three-stage training curriculum to address data imbalance, aiming to provide more principled and effective pathway toward truly unified and high-fidelity audio generation."
        },
        {
            "title": "3 UNIMOE-AUDIO",
            "content": "Our proposed model, UniMoE-Audio, is unified generative framework designed to synthesize both speech and music from multimodal inputs, including text, audio, and video. As illustrated in Figure 2, the core innovation of the architecture lies in the Dynamic-Capacity MoE implementation, which deviates from conventional MoE in two aspects: (1) novel Top-P routing strategy for dynamic experts number allocation, and (2) hybrid expert design comprising routed, shared, and null experts."
        },
        {
            "title": "3.1 Input Representation and Tokenization",
            "content": "Audio Tokenization. Following established practices in audio generation, we employ neural audio codec to transform continuous waveforms into sequence of discrete acoustic tokens. Specifically, we utilize the DAC codec [23], which represents each audio frame using multi-channel codebook. Unlike some works [24], [9] that employ the Depth Transformer to predict tokens for each channel sequentially, we adopt more parameter-efficient approach. Our model predicts all channels with multi-head output layer. This design avoids the introduction of additional sequential modules, thereby reducing the overall parameter count and computational latency. Visual Embedding. To process visual inputs (e.g., from video), we follow the Qwen-VL [25], using Visual Transformer (ViT) to encode the input image into patches. These visual features are then mapped into the language models embedding space via projector module, yielding sequence of soft visual tokens that can be seamlessly integrated with text and audio representations."
        },
        {
            "title": "2.2 Unified Audio Generation Models",
            "content": "The ambition of universal audio model has prompted several initial investigations into unifying diverse audio generation tasks within single framework. notable early attempt, UniAudio [9], proposed general-purpose text-to-audio model capable of generating various types of audio. However, as naive joint training approach, it reportedly suffered from the problem of data imbalance, leading to limited performance on data-scarce tasks such as music generation. More recently, AudioX [22] demonstrated impressive capabilities in generating sound effects and music from multimodal inputs like text, images, and video, utilizing Diffusion Transformer architecture. While powerful, its scope notably omits speech generation, prevalent and critical audio modality, thus not addressing the full challenge of speech-music unification. In contrast to these approaches, our work directly confronts the core challenges that have hindered previous unification efforts. Rather than relying on simple joint training, we propose framework that explicitly accounts for the inherent differences between audio"
        },
        {
            "title": "3.2 Dynamic-Capacity MoE",
            "content": "A primary limitation of conventional MoE models is their static Top-K routing strategy, which allocates fixed number of experts to each token. This approach is computationally sub-optimal, as it may over-allocate computational resources to simple tokens while under-powering complex ones that require more extensive processing. To address this, we introduce Top-P routing mechanism that dynamically allocates the number of activated experts for each token based on the routing probability of the router module. Given an input tensor RN for an FFN layer, where is the sequence length and is the hidden dimension, linear module first computes the gating probabilities for all experts: = Softmax(XWg), (1) where Wg RdE is the trainable gating matrix and RN represents the probability distribution over experts for each token. We interpret this distribution as the routers confidence. The objective is to select the smallest set of experts whose JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 4 Fig. 2: An overview of the UniMoE-Audio framework. Left: UniMoE-Audio is unified model capable of performing speech and music generation by leveraging multimodal conditional inputs, including Voice Cloning, Text-to-Speech (TTS), Text-to-Music (T2M), and Video-to-Music (V2M). Center: The core architecture of our model is Transformer with DynamicCapacity MoE layers. Right: We propose novel Top-P routing strategy, which dynamically selects the number of experts allocated to each token based on their complexity. cumulative probability exceeds predefined threshold p, thereby balancing computational cost and predictive accuracy. This can be formulated as finding an index set for each token such that: = arg min s.t. (cid:88) iI Pi p. (2) To efficiently solve this, we employ the classic Top-P sampling algorithm, sorting expert probabilities in descending order and selecting the smallest set whose cumulative sum exceeds the threshold p. The experts included in this sum are selected for computation. This approach naturally links the number of selected experts to the complexity of token, which is reflected in the routers probability distribution: lowentropy distributions correspond to simpler tokens, while high-entropy ones indicate more complex tokens requiring more experts. The final output of the MoE layer is weighted sum of the outputs from the selected experts, where the weights are the normalized gating probabilities: = (cid:88) iI Pi jI Pj (cid:80) Ei(X), (3) where is the set of selected expert indices for given token, and Ei(X) is the output of the i-th expert. While routed experts excel at learning domain-specific knowledge through conditional activation, they are inefficient for acquiring common knowledge, as inactive experts are excluded from the learning process. To address this, we functionally decouple the expert pool. Specifically, we incorporate set of shared experts that operate in parallel with the routed ones, which is constantly activated for all tokens, aimed at capturing common knowledge and offloading the computational burden in routed experts, allowing the routed experts to dedicate their full capacity to mastering domain-specific patterns. Furthermore, while our proposed routing strategy enables adaptive expert allocation, the range of activated expert number is inherently constrained. For set of Nr routed experts and the probability threshold p, the number of activated experts is confined to the range [1, pNr]. This prevents true computation skipping for simple tokens or activating all the router experts for the most demanding ones, limiting the models adaptive potential. To overcome this, we introduce the null expert: parameter-free module whose output is constant zero tensor. By incorporating Nn null experts into the routing pool, the possible number of activated routed experts now spans the expanded range of [0, p(Nr + Nn)]. This not only enhances the combinatorial flexibility of expert selection but also enables true adaptive computation skipping."
        },
        {
            "title": "4 TRAINING",
            "content": "The successful unification of speech and music generation hinges not only on the model architecture but also on training strategy that can effectively navigate the challenges of data imbalance and task conflict. To this end, we devise comprehensive approach encompassing both rigorous data governance and principled, three-stage training curriculum. JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST"
        },
        {
            "title": "4.1 Training Data",
            "content": "TABLE 1: Overview of Datasets Used in Different Tasks Task Datasets Number Duration (hours) Speech Synthesis Mandarin TTS English TTS Text-to-Music Free-music-archive [26] MusicNet [27] MU2Gen [21] Video-to-Music V2M [28] 180K 100K 106K 320 22K 20K 20K 10K 8.2K 37 1.2K 600 To support the unified generation of speech and music, we constructed comprehensive, multi-task dataset collection, detailed in Table 1. Our data strategy involves two key components: large-scale, imbalanced raw dataset for initial specialist training, and smaller, high-quality balanced dataset for subsequent MoE joint training. Our data curation process began with the collection of extensive raw data across four distinct categories: Chinese TTS (ZhTTS), English TTS (EnTTS), Text-to-Music (T2M), and Video-to-Music (V2M). As shown in Table 1, this resulted in highly imbalanced corpus, with speech data (approx. 30K hours) vastly outnumbering music data (approx. 10K hours). All data underwent rigorous pipeline of automatic annotation, multi-metric filtering, and deduplication to ensure quality. This large-scale raw dataset is exclusively used in the pre-training stage of our training curriculum to train the individual proto-experts, allowing each specialist to leverage the maximum available data for its domain without being affected by the data imbalance of other tasks. To mitigate task dominance in the later joint training stages, we constructed high-quality, balanced dataset. This dataset was created by carefully sampling 15K high-quality samples from each of the four task domains (ZhTTS, EnTTS, T2M, V2M) from our curated raw data pools. This results in final balanced set of 60K samples, ensuring that the model receives equal exposure to each task during the critical MoE warmup and synergistic joint training stages. This balanced approach is crucial for preventing the model from developing bias towards the data-rich speech tasks and for fostering true cross-domain synergy."
        },
        {
            "title": "4.2 Three-stage Training Curriculum",
            "content": "A naive joint training approach on the imbalanced dataset would inevitably lead to the data-rich speech task dominating the learning process. Conversely, simple up-sampling or down-sampling from the outset either sacrifices data diversity or discards valuable resources. To systematically circumvent this dilemma, we propose data-aware, threestage training curriculum, designed to decouple task-specific learning from synergistic optimization. Independent Specialist Training. The primary objective of this stage is to mitigate task conflict at its source and maximize data utilization. We leverage the full, imbalanced raw datasets to train separate, dense models for each task, as listed in Table 2. This complete isolation allows each modelwhich will serve as proto-expertto master its domain-specific knowledge without interference from other tasks. This process effectively injects specialized knowledge into the parameters of each future expert, pre-assigning their intended function before they are integrated. MoE Integration and Warmup. In this stage, we transition from individual specialists to the unified UniMoEAudio model. Specifically, the Feed-Forward Network (FFN) block from each of the four proto-experts is split into two halves, creating total of eight domain-specialized routed experts. Shared components, such as the attention and layer normalization modules, are initialized by averaging their corresponding parameters from all four proto-experts, while the vision transformer inherits its parameters directly from the Expert-V2M model. Once assembled, the weights of these pre-trained routed experts are initially frozen. The key challenge here is to stably integrate the newly introduced, randomly initialized components: the routing module and the two shared experts. Naive joint training would expose the well-trained experts to arbitrary routing decisions, risking catastrophic forgetting. To prevent this, we perform crucial calibration step: using only the balanced dataset, we exclusively train the gate modules and shared experts. This allows the routers to learn meaningful dispatch patterns based on the experts pretrained specializations and stabilizes the shared components before full-model training. Synergistic Joint Training. With stable and calibrated routing mechanism in place, the final stage aims to foster synergistic learning across all tasks. We unfreeze the entire model and conduct end-to-end fine-tuning on the larger, balanced fine-tuning dataset. To maintain routing efficiency and prevent the collapse of expert specialization during joint training, we employ an auxiliary load-balancing loss. The weight of this loss is linearly annealed over the course of training. Initially, high weight encourages the model to prioritize balanced expert utilization, promoting exploration. As training progresses, the weight decreases, shifting the optimization focus toward maximizing the primary sequence generation objective and exploiting the learned, efficient routing patterns for superior performance."
        },
        {
            "title": "5 EXPERIMENTS\n5.1 UniMoE-Audio Setting",
            "content": "This section outlines the configurations of all model variants evaluated in our experiments, with key specifications summarized in Table 2. Our experiments involve three main categories of models, all developed based on the Qwen2.5VL architecture: Specialist Models: Four 3.1B dense models, each trained on single task (Chinese TTS, English TTS, T2M, V2M). These serve as the foundational protoexperts and represent the performance of dedicated, single-task systems. Unify-Baseline: 7.1B dense model trained via direct joint training on the combined dataset. It serves as strong baseline to ablate the benefits of our MoE architecture and specialized training curriculum. UniMoE-Audio: Our proposed 7.1B unified model, featuring Dynamic-Capacity MoE architecture. Its activated parameter count is variable, governed by Top-P routing strategy (p = 0.7), averaging approximately 4.8B activated parameters during inference. JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 TABLE 2: Model configurations and parameters of all model variants used in our experiments. The Unify-Baseline and UniMoE-Audio models are designed to have comparable total parameter count for fair comparison. 6 Name Expert-ZhTTS Expert-EnTTS Expert-T2M Expert-V2M Unify-Baseline UniMoE-Audio Unify Audio Generation Dynamic-Capacity MoE Avg: 4.8B (Min: 2.8B, Max: 5.9B) Task Chinese TTS English TTS Text to Music Video to Music Unify Audio Generation Activated Param 3.1B 3.1B 3.1B 3.1B 7.1B Architecture Dense Dense Dense Dense Dense Total Param 3.1B 3.1B 3.1B 3.1B 7.1B 7.1B"
        },
        {
            "title": "5.2 Implementation Details",
            "content": "We employ the AdamW [29] optimizer in conjunction with cosine learning rate scheduler across all training stages. Subsequently, in the independent specialist training stage, we utilize 48 Ascend 910B GPUs, with global batch size of 48 and base learning rate of 1e-4. In the MoE integration and warmup stage, we utilize 196 Ascend 910B GPUs for MoE training, with global batch size of 784 and base learning rate of 3e-5. Finally, in the synergistic joint training stage, we utilize 196 Ascend 910B GPUs, with global batch size of 3136 and base learning rate of 1e-5. We adopt expert parallelism with four-way partitioning, meaning only two routed experts are loaded on each GPU. Semantic Alignment between text and audio is measured using CLAP score [36]. To provide more robust assessment, we also report the CLaMP3 score [37], which leverages more advanced multilingual framework. Audio Quality and Diversity are evaluated using suite of metrics: Frechet Audio Distance (FAD) [38] with OpenL3 embeddings, Kullback-Leibler (KL) divergence based on PaSST [39] predictions, and Inception Score (IS). Aesthetic Quality is evaluated using three specialized metrics from [40]: Production Complexity (PC), Production Quality (PQ), and Content Enjoyment (CE)."
        },
        {
            "title": "5.3 Evaluation Setting",
            "content": "Our evaluation setting comprehensively assesses both speech and music generation capabilities across range of standard benchmarks and metrics. Speech Synthesis. For speech synthesis, we evaluate models on both English and Mandarin benchmarks, focusing on three primary aspects: content intelligibility, speaker similarity, and perceptual quality. Our evaluation benchmark includes the Seed-TTS test set [30], the LibriSpeech test-clean set [31], and AISHELL-3 [32]. For content intelligibility and perceptual quality, we utilize consistent voice prompt to isolate the models generative quality from prompt variations. Content Intelligibility is measured by Word Error Rate (WER) for English and Character Error Rate (CER) for Mandarin, computed using the Whisperlarge-v3 [33] and Paraformer-zh[34] ASR engines, respectively. Perceptual Quality is assessed using UTMOS [35], neural MOS predictor that serves as an objective proxy for subjective human ratings. Speaker Similarity is quantified by the cosine similarity of speaker embeddings extracted from fine-tuned WavLM model, following the methodology of SeedTTS [30]. Music Generation. For music generation, we evaluate both text-to-music (T2M) and video-to-music (V2M) tasks, assessing semantic alignment, audio quality, and aesthetic quality. The T2M task is evaluated on MusicCaps [16] and V2M-bench[28], and the V2M task is evaluated on V2Mbench. Notably, to align with the setting of MusicCaps, all video and audio samples from V2M-Bench are segmented into 10-second clips."
        },
        {
            "title": "5.4 Overall Performance",
            "content": "We conducted comprehensive evaluation of UniMoEAudio against state-of-the-art specialized models and strong baselines across variety of speech and music generation tasks. As detailed in Table 3 and Table 4, our results demonstrate that UniMoE-Audio model can achieve competitive or even superior performance in both domains, effectively overcoming the typical trade-offs associated with joint multitask training. Takeaway 1: UniMoE-Audio achieves strong performance in speech synthesis with remarkable data efficiency. As shown in Table 3, UniMoE-Audio demonstrates exceptional capabilities in speech synthesis. For example, on the SeedTTSEN benchmark, it achieves new state-of-the-art in perceptual quality with UTMOS of 4.36, while also delivering highly competitive intelligibility (WER 1.9). This strong performance is also observed in other datasets. Notably, this performance is achieved using only 280K hours of speech data, rivaling or even surpassing dedicated models like Higgs audio V2 and Step-audio 2 mini, which were trained on 10M hours and 8M hours speech data. This highlights the remarkable data efficiency and strong learning capability endowed by our unified architecture and training curriculum. However, we also observe that the performance of speaker similarity remains inferior to the state-of-the-art model, which may be attributable to insufficient data scale. Takeaway 2: UniMoE-Audio excels in generating aesthetically superior music with strong semantic relevance. In the domain of music generation  (Table 4)  , UniMoEAudio consistently prioritizes and achieves superior aesthetic quality. Across both T2M and V2M tasks, our model obtains the highest scores in all aesthetic metrics (PC, PQ, CE), indicating its strength in producing richer, more enjoyable musical content. While its reference-similarity-based audio quality scores (i.e. FAD and KL) are inferior, we posit that JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 TABLE 3: Performance on English and Mandarin speech synthesis benchmarks. The best performance for each metric is highlighted in bold. WER and CER measure content intelligibility (lower is better), UTMOS measure perceptual quality (higher is better), and SIM measure speaker similarity with reference voice. UniMoE-Audio achieves state-of-the-art (SOTA) results on multiple key metrics and demonstrates highly competitive performance on others. 7 Method UniAudio [22] Mini-CPM-O-2.6 [41] Qwen2.5-Omni [3] Step-audio [6] Step-audio 2 mini [42] Higgs audio V2 [43] MiMo [44] Unify-Baseline UniMoE-Audio SeedTTS-EN WER UTMOS SeedTTS-ZH librispeech SIM CER UTMOS SIM WER UTMOS AISHELL-3 CER UTMOS 7.2 3.4 2.1 2.2 1.6 1.0 4.6 2.5 1.9 3.46 3.49 4.16 3.84 4.22 4.00 3.06 3.67 4.36 0.40 0.36 - 0.52 0.47 0.67 - 0.47 0.56 - 13.0 1.6 1.0 1.6 0.8 1.0 2.0 0.8 - 2.94 3.28 3.23 3.40 3.27 2.35 3.29 3.73 - 0.47 - 0.62 0.63 0.73 - 0.57 0.65 20.2 11.1 7.6 5.0 3.5 3.6 7.3 10.8 4.4 3.26 3.76 4.19 4.37 4.35 4.26 2.83 3.97 4.23 - 13.1 2.5 2.7 3.2 5.9 6. 4.2 1.6 - 3.30 3.38 3.69 4.00 3.89 2.32 3.45 3.86 TABLE 4: Performance on text-to-music and video-to-music generation benchmarks. The best performance for each metric is highlighted in bold. PC, PQ, and CE measure the aesthetic quality (higher is better). CLAP and CLaMP3 measure semantic alignment between the description and generated music (higher is better). KL and FAD assess audio quality against reference tracks (lower is better), while IS assess audio diversity (higher is better). UniMoE-Audio demonstrates superior performance in aesthetic quality, while remaining highly competitive in semantic alignment and audio quality. Dataset Method Task PC PQ CE CLAP KL CLaMP3 IS FAD MusicCap V2M-bench V2M-bench YuE [20] Stable Audio Open 1.0 [17] AudioX [22] MusicGen [18] MUMU-LLAMA [21] Unify-Baseline UniMoE-Audio YuE [20] Stable Audio Open 1.0 [17] AudioX [22] MusicGen [18] MUMU-LLAMA [21] Unify-Baseline UniMoE-Audio AudioX [22] Unify-Baseline UniMoE-Audio T2M 3.45 T2M 3.70 T2M 5.00 T2M 4.78 T2M 5. T2M 5.66 T2M 6.00 T2M 3.78 T2M 3.41 T2M 4.60 T2M 4.64 T2M 5.19 T2M 5.71 T2M 5.75 V2M 4.44 V2M 4.61 V2M 5.88 7.25 7.29 6.67 7.37 7. 6.48 7.77 7.25 7.46 7.30 7.37 7.73 5.68 7.58 7.44 5.50 7.62 5.84 6.02 6.14 6.57 6. 5.30 7.34 6.01 5.69 6.06 6.24 6.75 4.33 6.85 6.06 4.29 6.96 0.18 0.30 0.25 0.26 0. 0.14 0.29 0.15 0.34 0.30 0.28 0.17 0.23 0.31 - - - 2.12 1.44 1.20 1.21 1. 1.57 1.39 1.27 1.91 2.12 1.27 0.92 1.89 1.06 1.84 2.01 1.69 0.09 0.11 0.12 0.10 0. 0.07 0.12 0.13 0.16 0.11 0.15 0.13 0.15 0.19 - - - 2.09 2.74 3.02 1.68 1. 1.57 1.93 1.79 3.13 3.64 1.70 1.42 1.83 2.17 3.14 1.74 3.31 9.02 3.72 1.64 7.02 8. 9.64 6.43 4.29 2.94 4.26 3.39 2.54 3.27 3.11 2.94 3.24 2.89 this reflects our models strength in creative generation rather than mere imitation of reference tracks, which explores broader and more diverse acoustic space. Furthermore, the model attains strong semantic alignment with textual prompts, as evidenced by high CLAP and CLaMP3 scores. This combination of superior aesthetic quality and precise semantic alignment demonstrates UniMoE-Audios capability as powerful and versatile music generation system. Takeaway 3: The MoE architecture is critical for mitigating task conflict and enabling multi-domain excellence. direct comparison between UniMoE-Audio (MoE) and Unify-Baseline (Dense) provides strong empirical evidence supporting our architectural choice. Across both speech and music domains, the dynamic-capacity MoE consistently and significantly outperforms the dense baseline, despite the similar model size. This stark performance gap demonstrates that naive joint training leads to catastrophic interference, whereas our dynamic-capacity MoE architecture, by dynamically activating specialized experts, effectively resolves this conflict and unlocks high performance in both domains. Takeaway 4: Our training approach effectively mitigates the inherent data imbalance in multi-task learning. The Unify-Baseline model serves as stark illustration of the catastrophic forgetting induced by data imbalance in naive joint training. While its performance on the data-dominant Mandarin TTS task (comprising about 40% of the data) remains reasonable, its ability to generate coherent music is severely compromised, as evidenced by its poor music generation performance. In stark contrast, UniMoE-Audio demonstrates robust performance even on the most resourcelimited task, Video-to-Music (V2M), which constitutes merely 5% of the training data. This success is directly attributable to our methodology. By first training proto-experts on individual tasks, we pre-assign their specialized roles. The subsequent MoE integration then allows the model to dynamically route inputs to the relevant experts, effectively preventing the knowledge of data-scarce tasks from being overwritten during joint training. This demonstrates that our approach effectively mitigates the typical pitfalls of naive joint training, preserving high-quality generation capabilities JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 across all supported domains, irrespective of their data representation."
        },
        {
            "title": "6 DISCUSSION",
            "content": "8 Fig. 4: Visualization of the dynamic computational budget allocated by our Top-P routing mechanism. The figure illustrates the proportion of tokens activating varying number of experts at each layer, revealing rise-and-fall pattern where more computational resources are adaptively assigned to the middle layers. For clarity, counts of activated experts are grouped into bins (e.g., Expert 2-3 represents tokens activating either 2 or 3 experts). smoother warmup phase across both tasks. This suggests that end-to-end training of the full MoE is inherently less stable. By pre-stabilizing the routing logic during the warmup, our curriculum mitigates the risk of suboptimal performance and ensures more robust convergence path during the final joint training stage. Loss Disparity Reflects Intrinsic Task Complexity. The music generation task consistently shows higher loss than the speech task (converging near 60 vs. 40). This empirically validates our hypothesis that music, with its complex structures, is an intrinsically more difficult task to model. This difficulty gap highlights the necessity of our MoE architecture and staged curriculum, which prevent the easier speech task from dominating the learning process, problem often seen in naive joint training."
        },
        {
            "title": "6.2 Analysis of Dynamic Expert Allocation",
            "content": "To investigate the operational dynamics of our Top-P routing strategy, we analyze the distribution of the number of activated experts per token across different layers, as shown in Figure 4. The visualization reveals clear pattern of hierarchical computational demand. In the initial layers (e.g., layers 0-3), most tokens are routed to smaller number of experts (typically 1-3). This likely corresponds to low-level feature extraction. As information propagates to the middle layers (e.g., layers 4-13), it allocates larger computational budget, with the majority of tokens activating 4-5 experts. This allocation peaks around layer 12, indicating that the model concentrates its most intensive computations here for complex feature abstraction and cross-modal fusion. Subsequently, in the final, deeper layers (14-17), the trend reverses, and the allocated budget decreases again, likely focusing on integrating features for final output generation. Crucially, this non-uniform, layer-wise allocation pattern highlights core advantage of Top-P routing over conventional Top-K. common Top-K strategy would enforce fixed computational budget at every layer, irrespective of the layers function. In contrast, our model learns to dynamically tailor its capacity, assigning more resources Fig. 3: Training loss for the speech generation task (top) and music generation task (bottom). The plots show the transition from the Warmup Training Stage (blue) to the Synergistic Joint Training Stage (orange). The solid line represents the moving average of the loss. In this section, we delve into deeper analysis of UniMoEAudios training dynamics and internal mechanics. We first examine the training loss curves to gain insights into our three-stage curriculum (6.1). Subsequently, we analyze the expert utilization patterns to understand how the model allocates its capacity across different layers (6.2). Finally, we analyze the distribution of expert loading across speech and music generation tasks (6.3)."
        },
        {
            "title": "6.1 Training Loss Analysis",
            "content": "Figure 3 illustrates the training loss dynamic for speech (top) and music (bottom) generation, separated into the warmup and synergistic joint training stages. These curves provide several key insights into our training curriculum: Warmup Stage is Essential for Stable Router Calibration. The loss reduction magnitude during the warmup phase is comparable to that of the subsequent joint training phase, underscores its critical role. This confirms that calibrating the routing mechanism is non-trivial optimization problem. Our staged approach effectively decouples this from expert optimization, allowing the router to learn stable expert dispatch patterns before full-model training, thus preventing initial instability from corrupting the pre-trained experts. Staged Training Enhances Overall Stability. The joint training phase exhibits higher loss volatility compared to the JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 9 Fig. 5: Analysis of expert routing dynamics in UniMoE-Audio across transformer layers. The top-left All Experts plot illustrates the routing frequency for each of the eight routed experts (E1-E8, colored) and the null expert (E9, gray). The subsequent nine plots provide granular breakdown for each expert, showing the proportion of tokens routed from the Music (lighter shade) versus the TTS (darker shade) task. to the middle layers. Furthermore, even within single layer, the distribution is not monolithic; the model adaptively assigns larger budget to hard tokens while conserving resources on easy ones. This inherent flexibility validates the efficacy of Top-P routing in creating more efficient and intelligent architecture that allocates its computational power precisely where it is needed most."
        },
        {
            "title": "6.3 Expert Routing Visualization",
            "content": "To delve into the expert loading distribution of UniMoEAudio, we visualize the expert routing statistics in Figure 5. The figure provides comprehensive overview, showing the overall expert utilization (top-left) and detailed breakdown of task preference (Music vs. Speech) for each of the eight routed experts (E1-E8) and the null expert (E9). The analysis reveals several key findings. Effective Load Balancing Prevents Expert Collapse. The All Experts subplot shows remarkably balanced workload across all layers. No single routed expert is either over-utilized or ignored, and the null expert (E9) is also consistently engaged. This demonstrates that our training approach successfully prevents expert collapsea common failure mode in MoE training where the router shows strong preference for certain experts. This balanced utilization confirms that all experts are actively contributing to the models computation. Experts Exhibit Clear and Consistent Task Specialization. The individual plots for Experts 1 through 8 provide striking evidence of learned task specialization. clear division of labor is visible: Experts 1-4 consistently show strong preference for Speech tokens, while Experts 5-8 are overwhelmingly activated by Music tokens. For instance, across most layers, Expert 1 (red) is almost exclusively chosen for TTS, whereas Expert 5 (green) is predominantly chosen for Music. This strong, persistent specialization directly validates our training strategy. Initializing the model with pre-trained protoexperts successfully instills domain-specific knowledge, and the subsequent training preserves these roles, allowing the model to route tasks to the most qualified specialist. Hierarchical Processing Emerges from Shallow to Deep Layers. While specialization is strong overall, we find that experts of initial layers (e.g., 1-5) show more mixed activation between TTS and Music compared to the deeper layers. For example, in Expert 2 and Expert 6, the proportion of the non-preferred task is visibly higher in the first few layers. This suggests an emergent hierarchical processing scheme: shallower layers likely handle more universal, lowlevel features common to both speech and music (e.g., basic frequencies), while deeper layers focus on processing more abstract, domain-specific information, such as phonetics for TTS or harmony for Music. The Role of the Null Expert in Adaptive Computation The behavior of the null expert (E9) provides profound insight into the models learned efficiency. While the All Experts plot shows it handles substantial workload, the dedicated Expert 9 plot reveals dynamic, layer-dependent preference. In shallower layers, it prunes simple tokens from both tasks equally. However, in the deeper layers (25-32), it is overwhelmingly activated by speech tokens. This strongly suggests that once high-level features are formed, the model identifies the TTS task as computationally simpler and learns to skip redundant computations for it. This not only validates the null expert as mechanism for learned efficiency but also provides empirical evidence that our model dynamically understands the varying complexity of each task across its depth."
        },
        {
            "title": "7 CONCLUSION\nIn this paper, we addressed the long-standing challenge of\nunifying speech and music generation, a task hindered by\ntask conflict and data imbalance. We introduced UniMoE-\nAudio that leverages a dynamic-capacity Mixture-of-Experts\narchitecture to mitigate task conflict, in conjunction with a\ndata-aware, three-stage training curriculum to overcome\ndata imbalance. Experiments across diverse benchmarks\nshow that UniMoE-Audio not only matches or surpasses\nstrong domain-specific baselines, but also enables synergistic\nlearning across audio domains—effectively avoiding the\nperformance degradation observed in naive joint training.",
            "content": "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 Our work provides robust blueprint for building unified generative audio models, with future directions include the incorporation of broader range of audio types and the optimization of MoE architecture."
        },
        {
            "title": "REFERENCES",
            "content": "[1] J. Zhan, J. Dai, J. Ye, Y. Zhou, D. Zhang, Z. Liu, X. Zhang, J. Fu, T. Gui, T. Sun, R. Yuan, G. Zhang, L. Li, H. Yan, Y. Jiang, and X. Qiu, Anygpt: Unified multimodal LLM with discrete sequence modeling, in Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024, L. Ku, A. Martins, and V. Srikumar, Eds. Association for Computational Linguistics, 2024, pp. 96379662. [Online]. Available: https://doi.org/10.18653/v1/2024.acl-long.521 [3] [4] [2] C. Wu, J. Li, J. Zhou, J. Lin, K. Gao, K. Yan, S.-m. Yin, S. Bai, X. Xu, Y. Chen et al., Qwen-image technical report, arXiv preprint arXiv:2508.02324, 2025. J. Xu, Z. Guo, J. He, H. Hu, T. He, S. Bai, K. Chen, J. Wang, Y. Fan, K. Dang, B. Zhang, X. Wang, Y. Chu, and J. Lin, Qwen2.5-omni technical report, CoRR, vol. abs/2503.20215, 2025. [Online]. Available: https://doi.org/10.48550/arXiv.2503.20215 I. AI, B. Gong, C. Zou, C. Zheng, C. Zhou, C. Yan, C. Jin, C. Shen, D. Zheng, F. Wang, F. Xu, G. Yao, J. Zhou, J. Chen, J. Sun, J. Liu, J. Zhu, J. Peng, K. Ji, K. Song, K. Ren, L. Wang, L. Ru, L. Xie, L. Tan, L. Xue, L. Wang, M. Bai, N. Gao, P. Chen, Q. Guo, Q. Zhang, Q. Xu, R. Liu, R. Xiong, S. Gao, T. Liu, T. Li, W. Chai, X. Xiao, X. Wang, X. Chen, X. Lu, X. Li, X. Dong, X. Yu, Y. Yuan, Y. Gao, Y. Sun, Y. Chen, Y. Wu, Y. Lyu, Z. Ma, Z. Feng, Z. Fang, Z. Qiu, Z. Huang, and Z. He, Ming-omni: unified multimodal model for perception and generation, CoRR, vol. abs/2506.09344, 2025. [Online]. Available: https://doi.org/10.48550/arXiv.2506.09344 [5] KimiTeam, D. Ding, Z. Ju, Y. Leng, S. Liu, T. Liu, Z. Shang, K. Shen, W. Song, X. Tan, H. Tang, Z. Wang, C. Wei, Y. Xin, X. Xu, J. Yu, Y. Zhang, X. Zhou, Y. Charles, J. Chen, Y. Chen, Y. Du, W. He, Z. Hu, G. Lai, Q. Li, Y. Liu, W. Sun, J. Wang, Y. Wang, Y. Wu, Y. Wu, D. Yang, H. Yang, Y. Yang, Z. Yang, A. Yin, R. Yuan, Y. Zhang, and Z. Zhou, Kimi-audio technical report, CoRR, vol. abs/2504.18425, 2025. [Online]. Available: https://doi.org/10.48550/arXiv.2504.18425 [6] A. Huang, B. Wu, B. Wang, C. Yan, C. Hu, C. Feng, F. Tian, F. Shen, J. Li, M. Chen, P. Liu, R. Miao, W. You, X. Chen, X. Yang, Y. Huang, Y. Zhang, Z. Gong, Z. Zhang, H. Zhou, J. Sun, B. Li, C. Feng, C. Wan, H. Hu, J. Wu, J. Zhen, R. Ming, S. Yuan, X. Zhang, Y. Zhou, B. Li, B. Ma, H. Wang, K. An, W. Ji, W. Li, X. Wen, X. Kong, Y. Ma, Y. Liang, Y. Mou, B. Ahmidi, B. Wang, B. Li, C. Miao, C. Xu, C. Wang, D. Shi, D. Sun, D. Hu, D. Sai, E. Liu, G. Huang, G. Yan, H. Wang, H. Jia, H. Zhang, J. Gong, J. Guo, J. Liu, J. Liu, J. Feng, J. Wu, J. Wu, J. Yang, J. Wang, J. Zhang, J. Lin, K. Li, L. Xia, L. Zhou, L. Zhao, L. Gu, M. Chen, M. Wu, M. Li, M. Li, M. Li, M. Liang, N. Wang, N. Hao, Q. Wu, Q. Tan, R. Sun, S. Shuai, S. Pang, S. Yang, S. Gao, S. Yuan, S. Liu, S. Deng, S. Jiang, S. Liu, T. Cao, T. Wang, W. Deng, W. Xie, W. Ming, and W. He, Step-audio: Unified understanding and generation in intelligent speech interaction, CoRR, vol. abs/2502.11946, 2025. [Online]. Available: https://doi.org/10.48550/arXiv.2502. [7] B. Lin, Z. Tang, Y. Ye, J. Cui, B. Zhu, P. Jin, J. Zhang, M. Ning, and L. Yuan, Moe-llava: Mixture of experts for large vision-language models, CoRR, vol. abs/2401.15947, 2024. [Online]. Available: https://doi.org/10.48550/arXiv.2401.15947 [8] Y. Li, S. Jiang, B. Hu, L. Wang, W. Zhong, W. Luo, L. Ma, and M. Zhang, Uni-moe: Scaling unified multimodal llms with mixture of experts, IEEE Trans. Pattern Anal. Mach. Intell., vol. 47, no. 5, pp. 34243439, 2025. [Online]. Available: https://doi.org/10.1109/TPAMI.2025.3532688 [9] D. Yang, J. Tian, X. Tan, R. Huang, S. Liu, H. Guo, X. Chang, J. Shi, S. Zhao, J. Bian, Z. Zhao, X. Wu, and H. M. Meng, Uniaudio: Towards universal audio generation with large language models, in Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net, 2024. [Online]. Available: https://openreview.net/forum?id=SRmZw7nEGW [10] S. Chen, S. Liu, L. Zhou, Y. Liu, X. Tan, J. Li, S. Zhao, Y. Qian, and F. Wei, VALL-E 2: Neural codec language models are human parity zero-shot text to speech synthesizers, CoRR, vol. abs/2406.05370, 2024. [Online]. Available: https: //doi.org/10.48550/arXiv.2406.05370 [11] A. Defossez, J. Copet, G. Synnaeve, and Y. Adi, High fidelity neural audio compression, CoRR, vol. abs/2210.13438, 2022. [Online]. Available: https://doi.org/10.48550/arXiv.2210.13438 [12] Z. Zhang, L. Zhou, C. Wang, S. Chen, Y. Wu, S. Liu, Z. Chen, Y. Liu, H. Wang, J. Li, L. He, S. Zhao, and F. Wei, Speak foreign languages with your own voice: Cross-lingual neural codec language modeling, CoRR, vol. abs/2303.03926, 2023. [Online]. Available: https://doi.org/10.48550/arXiv.2303.03926 [13] E. Kharitonov, D. Vincent, Z. Borsos, R. Marinier, S. Girgin, O. Pietquin, M. Sharifi, M. Tagliasacchi, and N. Zeghidour, Speak, read and prompt: High-fidelity text-to-speech with minimal supervision, Trans. Assoc. Comput. Linguistics, vol. 11, pp. 17031718, 2023. [Online]. Available: https://doi.org/10.1162/tacl 00618 [14] R. Huang, C. Zhang, Y. Wang, D. Yang, L. Liu, Z. Ye, Z. Jiang, C. Weng, Z. Zhao, and D. Yu, Make-a-voice: Unified voice synthesis with discrete representation, CoRR, vol. abs/2305.19269, 2023. [Online]. Available: https://doi.org/10.48550/arXiv.2305. 19269 [15] Z. Du, Q. Chen, S. Zhang, K. Hu, H. Lu, Y. Yang, H. Hu, S. Zheng, Y. Gu, Z. Ma, Z. Gao, and Z. Yan, Cosyvoice: scalable multilingual zero-shot text-to-speech synthesizer based on supervised semantic tokens, CoRR, vol. abs/2407.05407, 2024. [Online]. Available: https://doi.org/10.48550/arXiv.2407.05407 [16] A. Agostinelli, T. I. Denk, Z. Borsos, J. H. Engel, M. Verzetti, A. Caillon, Q. Huang, A. Jansen, A. Roberts, M. Tagliasacchi, M. Sharifi, N. Zeghidour, and C. H. Frank, Musiclm: Generating music from text, CoRR, vol. abs/2301.11325, 2023. [Online]. Available: https://doi.org/10.48550/arXiv.2301.11325 [17] Z. Evans, J. D. Parker, C. Carr, Z. Zukowski, J. Taylor, and J. Pons, Stable audio open, in 2025 IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP 2025, Hyderabad, India, April 6-11, 2025. IEEE, 2025, pp. 15. [Online]. Available: https://doi.org/10.1109/ICASSP49660.2025.10888461 - 16, in Neural [18] J. Copet, F. Kreuk, I. Gat, T. Remez, D. Kant, G. Synnaeve, Y. Adi, and A. Defossez, Simple and controllable music generation, in Advances Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 2023, A. Oh, T. Naumann, A. Globerson, 10 K. Saenko, M. Hardt, and S. Levine, Eds., 2023. [Online]. Available: http://papers.nips.cc/paper files/paper/2023/hash/ 94b472a1842cd7c56dcb125fb2765fbd-Abstract-Conference.html [19] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale, D. Bikel, L. Blecher, C. Canton-Ferrer, M. Chen, G. Cucurull, D. Esiobu, J. Fernandes, J. Fu, W. Fu, B. Fuller, C. Gao, V. Goswami, N. Goyal, A. Hartshorn, S. Hosseini, R. Hou, H. Inan, M. Kardas, V. Kerkez, M. Khabsa, I. Kloumann, A. Korenev, P. S. Koura, M. Lachaux, T. Lavril, J. Lee, D. Liskovich, Y. Lu, Y. Mao, X. Martinet, T. Mihaylov, P. Mishra, I. Molybog, Y. Nie, A. Poulton, J. Reizenstein, R. Rungta, K. Saladi, A. Schelten, R. Silva, E. M. Smith, R. Subramanian, X. E. Tan, B. Tang, R. Taylor, A. Williams, J. X. Kuan, P. Xu, Z. Yan, I. Zarov, Y. Zhang, A. Fan, M. Kambadur, S. Narang, A. Rodriguez, R. Stojnic, S. Edunov, and T. Scialom, Llama 2: Open foundation and fine-tuned chat models, CoRR, vol. abs/2307.09288, 2023. [Online]. Available: https://doi.org/10.48550/arXiv.2307.09288 [20] R. Yuan, H. Lin, S. Guo, G. Zhang, J. Pan, Y. Zang, H. Liu, Y. Liang, W. Ma, X. Du, X. Du, Z. Ye, T. Zheng, Y. Ma, M. Liu, Z. Tian, Z. Zhou, L. Xue, X. Qu, Y. Li, S. Wu, T. Shen, Z. Ma, J. Zhan, C. Wang, Y. Wang, X. Chi, X. Zhang, Z. Yang, X. Wang, S. Liu, L. Mei, P. Li, J. Wang, J. Yu, G. Pang, X. Li, Z. Wang, X. Zhou, L. Yu, E. Benetos, Y. Chen, C. Lin, X. Chen, G. Xia, Z. Zhang, C. Zhang, W. Chen, X. Zhou, X. Qiu, R. B. Dannenberg, Z. Liu, J. Yang, W. Huang, W. Xue, X. Tan, and Y. Guo, Yue: Scaling open foundation models for long-form music generation, CoRR, vol. abs/2503.08638, 2025. [Online]. Available: https://doi.org/10.48550/arXiv.2503.08638 [21] S. Liu, A. S. Hussain, Q. Wu, C. Sun, and Y. Shan, Mumu-llama: Multi-modal music understanding and generation via large language models, CoRR, vol. abs/2412.06660, 2024. [Online]. Available: https://doi.org/10.48550/arXiv.2412.06660 [22] Z. Tian, Y. Jin, Z. Liu, R. Yuan, X. Tan, Q. Chen, W. Xue, and Y. Guo, Audiox: Diffusion transformer for anything-to-audio generation, CoRR, vol. abs/2503.10522, 2025. [Online]. Available: https://doi.org/10.48550/arXiv.2503.10522 [23] R. Kumar, P. Seetharaman, A. Luebs, I. Kumar, and K. Kumar, JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 11 - 16, in Neural High-fidelity audio compression with improved RVQGAN, Information Processing Systems 36: in Advances Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 2023, A. Oh, T. Naumann, A. Globerson, 10 K. Saenko, M. Hardt, and S. Levine, Eds., 2023. [Online]. Available: http://papers.nips.cc/paper files/paper/2023/hash/ 58d0e78cf042af5876e12661087bea12-Abstract-Conference.html [24] A. Defossez, L. Mazare, M. Orsini, A. Royer, P. Perez, H. Jegou, E. Grave, and N. Zeghidour, Moshi: speech-text foundation model for real-time dialogue, CoRR, vol. abs/2410.00037, 2024. [Online]. Available: https://doi.org/10.48550/arXiv.2410.00037 [25] P. Wang, S. Bai, S. Tan, S. Wang, Z. Fan, J. Bai, K. Chen, X. Liu, J. Wang, W. Ge, Y. Fan, K. Dang, M. Du, X. Ren, R. Men, D. Liu, C. Zhou, J. Zhou, and J. Lin, Qwen2-vl: Enhancing vision-language models perception of the world at any resolution, CoRR, vol. abs/2409.12191, 2024. [Online]. Available: https://doi.org/10.48550/arXiv.2409. [26] M. Defferrard, K. Benzi, P. Vandergheynst, and X. Bresson, FMA: dataset for music analysis, in Proceedings of the 18th International Society for Music Information Retrieval Conference, ISMIR 2017, Suzhou, China, October 23-27, 2017, S. J. Cunningham, Z. Duan, X. Hu, and D. Turnbull, Eds., 2017, pp. 316323. [Online]. Available: https://ismir2017.smcnus.org/wp-content/uploads/ 2017/10/75 Paper.pdf [27] J. Thickstun, Z. Harchaoui, and S. M. Kakade, Learning features of music from scratch, in 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net, 2017. [Online]. Available: https://openreview.net/forum?id=rkFBJv9gg [28] Z. Tian, Z. Liu, R. Yuan, J. Pan, Q. Liu, X. Tan, Q. Chen, W. Xue, and Y. Guo, Vidmuse: simple video-to-music generation framework with long-short-term modeling, in IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2025, Nashville, TN, USA, June 11-15, 2025. Computer Vision Foundation / IEEE, 2025, pp. 18 78218 793. [Online]. Available: https://openaccess.thecvf.com/content/CVPR2025/html/Tian VidMuse Simple Video-to-Music Generation Framework with Long-Short-Term Modeling CVPR 2025 paper.html [29] I. Loshchilov and F. Hutter, Decoupled weight decay regularization, in 7th International Conference on Learning ICLR 2019, New Orleans, LA, USA, May 6Representations, 9, 2019. OpenReview.net, 2019. [Online]. Available: https: //openreview.net/forum?id=Bkg6RiCqY7 [30] P. Anastassiou, J. Chen, J. Chen, Y. Chen, Z. Chen, Z. Chen, J. Cong, L. Deng, C. Ding, L. Gao, M. Gong, P. Huang, Q. Huang, Z. Huang, Y. Huo, D. Jia, C. Li, F. Li, H. Li, J. Li, X. Li, X. Li, L. Liu, S. Liu, S. Liu, X. Liu, Y. Liu, Z. Liu, L. Lu, J. Pan, X. Wang, Y. Wang, Y. Wang, Z. Wei, J. Wu, C. Yao, Y. Yang, Y. Yi, J. Zhang, Q. Zhang, S. Zhang, W. Zhang, Y. Zhang, Z. Zhao, D. Zhong, and X. Zhuang, Seed-tts: family of high-quality versatile speech generation models, CoRR, vol. abs/2406.02430, 2024. [Online]. Available: https://doi.org/10.48550/arXiv.2406.02430 [31] V. Panayotov, G. Chen, D. Povey, and S. Khudanpur, Librispeech: An ASR corpus based on public domain audio books, in 2015 IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP 2015, South Brisbane, Queensland, Australia, April 19-24, 2015. IEEE, 2015, pp. 52065210. [Online]. Available: https://doi.org/10.1109/ICASSP.2015.7178964 [32] Y. Shi, H. Bu, X. Xu, S. Zhang, and M. Li, AISHELL-3: multi-speaker mandarin TTS corpus, in 22nd Annual Conference of the International Speech Communication Association, Interspeech 2021, Brno, Czechia, August 30 - September 3, 2021, H. Hermansky, H. Cernock y, L. Burget, L. Lamel, O. Scharenborg, and P. Motlıcek, Eds. ISCA, 2021, pp. 27562760. [Online]. Available: https://doi.org/10.21437/Interspeech.2021- [33] A. Radford, J. W. Kim, T. Xu, G. Brockman, C. McLeavey, and I. Sutskever, Robust speech recognition via large-scale weak supervision, in International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, ser. Proceedings of Machine Learning Research, A. Krause, E. Brunskill, K. Cho, B. Engelhardt, S. Sabato, and J. Scarlett, Eds., vol. 202. PMLR, 2023, pp. 28 49228 518. [Online]. Available: https://proceedings.mlr.press/v202/radford23a.html [34] Z. Gao, S. Zhang, I. McLoughlin, and Z. Yan, Paraformer: Fast and accurate parallel transformer for non-autoregressive end-to-end speech recognition, in 23rd Annual Conference of the International Speech Communication Association, Interspeech 2022, Incheon, Korea, September 18-22, 2022, H. Ko and J. H. L. Hansen, Eds. ISCA, 2022, pp. 20632067. [Online]. Available: https://doi.org/10.21437/Interspeech.2022-9996 [35] T. Saeki, D. Xin, W. Nakata, T. Koriyama, S. Takamichi, and H. Saruwatari, UTMOS: utokyo-sarulab system for voicemos challenge 2022, in 23rd Annual Conference of the International Speech Communication Association, Interspeech 2022, Incheon, Korea, September 18-22, 2022, H. Ko and J. H. L. ISCA, 2022, pp. 45214525. [Online]. Available: Hansen, Eds. https://doi.org/10.21437/Interspeech.2022-439 [36] Y. Wu, K. Chen, T. Zhang, Y. Hui, T. Berg-Kirkpatrick, and S. Dubnov, Large-scale contrastive language-audio pretraining with feature fusion and keyword-to-caption augmentation, on Acoustics, Speech and in IEEE International Conference June Island, Greece, Signal Processing ICASSP 2023, Rhodes 4-10, 2023. IEEE, 2023, pp. 15. [Online]. Available: https: //doi.org/10.1109/ICASSP49357.2023.10095969 [37] S. Wu, Z. Guo, R. Yuan, J. Jiang, S. Doh, G. Xia, J. Nam, X. Li, F. Yu, and M. Sun, Clamp 3: Universal music information retrieval across unaligned modalities and unseen languages, in Findings of the Association for Computational Linguistics, ACL 2025, Vienna, Austria, July 27 - August 1, 2025, W. Che, J. Nabende, E. Shutova, and M. T. Pilehvar, Eds. Association for Computational Linguistics, 2025, pp. 26052625. [Online]. Available: https://aclanthology.org/2025.findings-acl.133/ [38] K. Kilgour, M. Zuluaga, D. Roblek, and M. Sharifi, Frechet audio distance: reference-free metric for evaluating music enhancement algorithms, in 20th Annual Conference of the International Speech Communication Association, Interspeech 2019, Graz, Austria, September 15-19, 2019, G. Kubin and Z. Kacic, ISCA, 2019, pp. 23502354. [Online]. Available: https: Eds. //doi.org/10.21437/Interspeech.2019- [39] K. Koutini, J. Schl uter, H. Eghbal-zadeh, and G. Widmer, Efficient training of audio transformers with patchout, in 23rd Annual Conference of the International Speech Communication Association, Interspeech 2022, Incheon, Korea, September 18-22, 2022, H. Ko ISCA, 2022, pp. 27532757. [Online]. and J. H. L. Hansen, Eds. Available: https://doi.org/10.21437/Interspeech.2022-227 [40] A. Tjandra, Y. Wu, B. Guo, J. Hoffman, B. Ellis, A. Vyas, B. Shi, S. Chen, M. Le, N. Zacharov, C. Wood, A. Lee, and W. Hsu, Meta audiobox aesthetics: Unified automatic quality assessment for speech, music, and sound, CoRR, vol. abs/2502.05139, 2025. [Online]. Available: https://doi.org/10.48550/arXiv.2502.05139 [41] Y. Yao, T. Yu, A. Zhang, C. Wang, J. Cui, H. Zhu, T. Cai, H. Li, W. Zhao, Z. He et al., Minicpm-v: gpt-4v level mllm on your phone, arXiv preprint arXiv:2408.01800, 2024. [42] B. Wu, C. Yan, C. Hu, C. Yi, C. Feng, F. Tian, F. Shen, G. Yu, H. Zhang, J. Li, M. Chen, P. Liu, W. You, X. T. Zhang, X. Li, X. Yang, Y. Deng, Y. Huang, Y. Li, Y. Zhang, Z. You, B. Li, C. Wan, H. Hu, J. Zhen, S. Chen, S. Yuan, X. Zhang, Y. Jiang, Y. Zhou, Y. Yang, B. Li, B. Ma, C. Song, D. Pang, G. Hu, H. Sun, K. An, N. Wang, S. Gao, W. Ji, W. Li, W. Sun, X. Wen, Y. Ren, Y. Ma, Y. Lu, B. Wang, B. Li, C. Miao, C. Liu, C. Xu, D. Shi, D. Hu, D. Wu, E. Liu, G. Huang, G. Yan, H. Zhang, N. Hao, H. Jia, H. Zhou, J. Sun, J. Wu, J. Wu, J. Yang, J. Yang, J. Lin, K. Li, L. Yang, L. Shi, L. Zhou, L. Gu, M. Li, M. Li, M. Li, N. Wu, Q. Han, Q. Tan, S. Pang, S. Fan, S. Liu, T. Cao, W. Lu, W. He, W. Xie, X. Zhao, X. Li, Y. Yu, Y. Yang, Y. Liu, Y. Lu, Y. Wang, Y. Ding, Y. Liang, Y. Lu, Y. Luo, Y. Yin, Y. Zhan, and Y. Zhang, Step-audio 2 technical report, CoRR, vol. abs/2507.16632, 2025. [Online]. Available: https://doi.org/10.48550/arXiv.2507.16632 [43] Boson AI, Higgs Audio V2: Redefining Expressiveness in Audio Generation, https://github.com/boson-ai/higgs-audio, 2025, gitHub repository. Release blog available at https://www.boson. ai/blog/higgs-audio-v2. [44] L.-C.-T. Xiaomi, Mimo-audio: Audio language models are fewshot learners, 2025. [Online]. Available: GitHub-XiaomiMiMo/ MiMo-Audio"
        }
    ],
    "affiliations": [
        "Department of Computer Science and Technology, Harbin Institute of Technology, Shenzhen, China",
        "Shenzhen Loop Area Institute, Shenzhen, China"
    ]
}