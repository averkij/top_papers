{
    "paper_title": "V-Retrver: Evidence-Driven Agentic Reasoning for Universal Multimodal Retrieval",
    "authors": [
        "Dongyang Chen",
        "Chaoyang Wang",
        "Dezhao SU",
        "Xi Xiao",
        "Zeyu Zhang",
        "Jing Xiong",
        "Qing Li",
        "Yuzhang Shang",
        "Shichao Ka"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Multimodal Large Language Models (MLLMs) have recently been applied to universal multimodal retrieval, where Chain-of-Thought (CoT) reasoning improves candidate reranking. However, existing approaches remain largely language-driven, relying on static visual encodings and lacking the ability to actively verify fine-grained visual evidence, which often leads to speculative reasoning in visually ambiguous cases. We propose V-Retrver, an evidence-driven retrieval framework that reformulates multimodal retrieval as an agentic reasoning process grounded in visual inspection. V-Retrver enables an MLLM to selectively acquire visual evidence during reasoning via external visual tools, performing a multimodal interleaved reasoning process that alternates between hypothesis generation and targeted visual verification.To train such an evidence-gathering retrieval agent, we adopt a curriculum-based learning strategy combining supervised reasoning activation, rejection-based refinement, and reinforcement learning with an evidence-aligned objective. Experiments across multiple multimodal retrieval benchmarks demonstrate consistent improvements in retrieval accuracy (with 23.0% improvements on average), perception-driven reasoning reliability, and generalization."
        },
        {
            "title": "Start",
            "content": "V-Retrver: Evidence-Driven Agentic Reasoning for Universal Multimodal Retrieval Dongyang Chen * 1 Chaoyang Wang * 2 Dezhao SU 3 Xi Xiao 1 Zeyu Zhang 4 Jing Xiong 5 Qing Li 6 Yuzhang Shang 2 Shichao Kan 7 Home: https://github.com/chendy25/V-Retrver HF:https://huggingface.co/V-Retrver 6 2 0 2 5 ] . [ 1 4 3 0 6 0 . 2 0 6 2 : r Abstract Multimodal Large Language Models (MLLMs) have recently been applied to universal multimodal retrieval, where Chain-of-Thought (CoT) reasoning improves candidate reranking. However, existing approaches remain largely languagedriven, relying on static visual encodings and lacking the ability to actively verify fine-grained visual evidence, which often leads to speculative reasoning in visually ambiguous cases. We propose V-Retrver, an evidence-driven retrieval framework that reformulates multimodal retrieval as an agentic reasoning process grounded in visual inspection. V-Retrver enables an MLLM to selectively acquire visual evidence during reasoning via external visual tools, performing multimodal interleaved reasoning process that alternates between hypothesis generation and targeted visual verification. To train such an evidence-gathering retrieval agent, we adopt curriculum-based learning strategy combining supervised reasoning activation, rejection-based refinement, and reinforcement learning with an evidence-aligned objective. Experiments across multiple multimodal retrieval benchmarks demonstrate consistent improvements in retrieval accuracy (with 23.0% improvements on average), perception-driven reasoning reliability, and generalization. 1. Introduction The rapid development of Multimodal Large Language Models (MLLMs) has substantially advanced universal multi- *Equal contribution 1Tsinghua University 2University of Central Florida 3Fudan University 4The Australian National University 5The University of Hong Kong 6Pengcheng Laboratory 7Central South University. Correspondence to: Xi Xiao <xiaox@sz.tsinghua.edu.cn>. Preprint. February 6, 2026. 1 modal retrieval (Chen et al., 2024c; Lin et al., 2024a; Wang et al., 2024b; Zhu et al., 2025d; Sun et al.), enabling single model to support diverse retrieval scenarios such as text-to-image, image-to-text, and interleaved multimodal queries. Recent works further demonstrate that incorporating Chain-of-Thought (CoT) reasoning can improve retrieval performance by enhancing interpretability and candidate discrimination (Zhu et al., 2025d; Xu et al., 2025c; Narayan et al., 2025). However, despite these advances, existing CoT-based retrieval systems remain fundamentally language-driven, even when retrieval decisions critically depend on visual evidence. This limitation becomes particularly pronounced in visually ambiguous retrieval scenarios, where candidate images share similar semantic content but differ in fine-grained visual attributes such as object appearance, style, or local context. Most current MLLM-based retrieval methods (Liu et al., 2025; Chen et al., 2024c; Lin et al., 2024a) compress visual inputs into fixed embeddings or textual descriptions, forcing the reasoning process to rely on language alone to infer visual differences. Consequently, the model often produces speculative or hallucinated reasoning when the required evidence lies in the visual modality. Even recent reasoning-enhanced retrieval frameworks, such as Retrv-R1 (Zhu et al., 2025d) and MM-R5 (Xu et al., 2025b), improve textual reasoning depth but still rely on single-pass visual encoding, lacking the ability to actively verify visual hypotheses during reasoning. To overcome this gap, we propose V-Retrver, an evidencedriven retrieval framework that reformulates multimodal retrieval as an agentic reasoning process grounded in visual inspection. Instead of treating visual representations as static inputs, V-Retrver enables an MLLM to selectively acquire visual evidence during reasoning by invoking external visual tools. Through multimodal interleaved Chain-ofThought process, the model alternates between hypothesis generation and targeted visual verification, allowing it to dynamically resolve visual ambiguities and progressively refine ranking decisions, as illustrated in Fig. 1. V-Retrver: Evidence-Driven Agentic Reasoning for Universal Multimodal Retrieval Figure 1. Comparison between text-based CoT (left) and multimodal interleaved CoT (right) for multimodal retrieval. Text-based CoT relies on language-driven inference over static visual representations, often failing to resolve fine-grained differences. In contrast, V-Retrver performs multimodal interleaved CoT reasoning by invoking visual tools to inspect candidate images, enabling grounded reasoning and more reliable ranking decisions. Training such an evidence-gathering retrieval agent requires not only strong reasoning ability but also effective alignment between retrieval performance and visual tool usage. We therefore adopt curriculum-based training strategy consisting of three stages. First, cold-start supervised stage initializes the model with basic reasoning capabilities and operation formatting using synthesized high-quality CoT data. Second, rejection sampling fine-tuning consolidates high-quality reasoning trajectories and improves structural compliance. Finally, we introduce Evidence-Aligned Policy Optimization (EAPO), instantiated via Group Relative Policy Optimization (GRPO) (Guo et al., 2025), which reinforces correct ranking decisions while encouraging informative visual verification and discouraging redundant tool usage. Extensive experiments on the universal multimodal retrieval benchmark M-BEIR, as well as multiple out-of-domain datasets, demonstrate that V-Retrver consistently outperforms strong baselines across diverse retrieval settings. The results show that V-Retrver achieves higher retrieval accuracy, more reliable perception-grounded reasoning, and stronger generalization ability, validating the effectiveness of interleaved visual reasoning for multimodal retrieval. In summary, our contributions are three-fold: We propose V-Retrver, an evidence-driven agentic retrieval framework that enables MLLMs to actively acquire visual evidence during multimodal reasoning. We introduce curriculum-based training strategy with an evidence-aligned reinforcement learning objective that jointly improves reasoning quality, ranking accuracy, and efficient visual tool usage. Extensive experiments across multiple benchmarks demonstrate that V-Retrver consistently outperforms existing methods and generalizes well to diverse multimodal retrieval scenarios. 2. Related Work Multi-modal Large Language Models. In recent years, the rapid advancement of multimodal large language models (MLLMs) has driven the deep integration of visual perception and language reasoning, leading to the emergence of series of high-performing open-source models, notably the LLaVA (Liu et al., 2024; Guo et al., 2024; Zhang et al., 2025c; Lin et al., 2023a; Li et al., 2023a), Qwen-VL (Bai et al., 2023; Wang et al., 2024a; Yang et al., 2024), and InternVL (Chen et al., 2024b; Gao et al., 2024; Lu et al., 2025) series. In parallel, large-scale models such as Flamingo (Alayrac et al., 2022), mPLUG-Owl (Ye et al., 2 V-Retrver: Evidence-Driven Agentic Reasoning for Universal Multimodal Retrieval 2023; 2024b;a), and GPT-4V (Yang et al., 2023) pursue more holistic vision-language modeling paradigm, incorporating advanced mechanisms including mixture-of-experts architectures (Shu et al., 2024; Li et al., 2025b; Shen et al., 2024) and image generation components (Xie et al., 2024; Xu et al., 2025a). However, these models generally lack reasoning capabilities such as Chain-of-Thought and test-time scalability (Muennighoff et al., 2025; Zhang et al., 2025b; Chen et al., 2024a), and to large extent still decouple visual perception from text reasoning processes. Multimodal Retrieval. Recent advances in deep learning (Zhu et al., 2021; 2024; 2025a;c;b; Ji et al., 2024) have substantially propelled progress across broad spectrum of retrieval tasks, including textimage cross-modal retrieval (Pham et al., 2024; Fu et al., 2024; Zhang et al., 2020; Chun et al., 2021; Kim et al., 2023b;a), composed image retrieval (Baldrati et al., 2022; Saito et al., 2023; Gu et al., 2024; Suo et al., 2024; Baldrati et al., 2023), multimodal document retrieval (Chen et al., 2023; Hu et al., 2023; Liu et al., 2023), and instruction-based image retrieval (Wu et al., 2021; Zhang et al., 2024a; Asai et al., 2023). Among these approaches, visionlanguage models (VLMs), particularly CLIP (Radford et al., 2021), have demonstrated strong effectiveness and scalability in multimodal retrieval scenarios (Baldrati et al., 2022; Wei et al., 2024b; Sain et al., 2023; Pei et al., 2023; Jin et al., 2024). For instance, Kim et al. (Kim et al., 2023a) improve CLIP via prompt tuning, enabling enhanced generalization across diverse retrieval settings. More recently, multimodal large language models (MLLMs) have been introduced to further advance retrieval performance (Liu et al., 2025; Jiang et al., 2024; Lin et al., 2024a; Zhou et al., 2024). Some approaches (Zhou et al., 2024; Lan et al., 2025; Lin et al., 2024a; Zhang et al., 2024b; Jian et al., 2025; Gu et al., 2025) utilize embeddings extracted from MLLMs to perform similarity-based retrieval. Others approaches, such as LamRA (Liu et al., 2025; Li et al., 2025a), employ MLLMs as reranking agents to refine candidate lists and select the most relevant results. Retrv-R1(Zhu et al., 2025d) equips the model with text reasoning capabilities for multimodal retrieval tasks through reinforcement learning. In contrast to prior work, we introduce V-Retrver, an evidencedriven retrieval framework, which can adaptively adjust its visual exploration strategy during reasoning by invoking visual tools, enabling more flexible and effective reasoning process and thereby achieving significant improvements in retrieval performance. 3. Method 3.1. Problem Formulation We study the problem of universal multimodal retrieval. Given query of arbitrary modality (text, image, or interleaved multimodal input) and candidate pool Ω = {cn}N n=1, the objective is to identify the most relevant candidate ˆc Ω. Conventional multimodal retrieval approaches typically formulate this problem as static similarity matching or language-only reranking over fixed visual representations. Such formulations implicitly assume that all necessary visual evidence has been fully encoded into embeddings or textual descriptions prior to reasoning. However, this assumption breaks down in fine-grained or visually ambiguous retrieval scenarios, where subtle local details determine relevance and cannot be reliably inferred from compressed representations alone. To address this limitation, we reformulate multimodal retrieval as an evidence-grounded reasoning problem. Under this formulation, retrieval is no longer single-pass inference process, but an iterative decision-making procedure in which the model is required to actively acquire and verify visual evidence during ranking. Specifically, the retrieval process consists of three tightly coupled steps: (i) generating hypotheses about candidate relevance based on available information, (ii) selectively inspecting visual evidence to resolve uncertainty, and (iii) refining the ranking decision based on verified observations. This perspective naturally gives rise to an agentic reranking paradigm, where retrieval model is endowed with the ability to reason, inspect, and revise its decisions, rather than passively scoring candidates using fixed representations. 3.2. Overview of V-Retrver Building on the above formulation, we propose V-Retrver, an evidence-driven reasoning framework for universal multimodal retrieval, As illustrated in Fig. 2. V-Retrver follows coarse-to-fine retrieval pipeline that decouples efficient candidate proposal from computationally intensive evidencebased reasoning. In the first stage, an embedding model ϕ encodes the query and each candidate cn into shared representation space, retrieving the top-K candidates based on similarity. We adopt the same method as LamRA (Liu et al., 2025) for constructing the embedding model ϕ. This stage serves as an efficient candidate proposal mechanism and substantially reduces the search space: = {ck}K k=1, N. In the second stage, V-Retrver employs reasoning agent θ to perform fine-grained reranking over the reduced candidate set C. Crucially, θ is not conventional reranker that operates over static features. Instead, it is designed as an agentic evidence-gathering model that can iteratively reason, invoke visual inspection tools, and revise its ranking decisions based on newly acquired visual observations. The final prediction is produced as: ˆc = θ(q, C). V-Retrver: Evidence-Driven Agentic Reasoning for Universal Multimodal Retrieval Figure 2. Overview of the V-Retrver framework. The left panel illustrates the inference pipeline, featuring coarse-to-fine process with embedding-based retrieval and agentic reranking. The right panel details the three training stages we proposed, including Cold Start, Rejection sampling Fine-Tuning, and EAPO. The remainder of this section details the core mechanisms that enable evidence-driven reasoning in V-Retrver, including multimodal interleaved reasoning, visual tools, and curriculum-based training strategy. ically acquired visual evidence, MIER mitigates speculative inference and hallucination, enabling more reliable ranking decisions in visually ambiguous cases. 3.3. Multimodal Interleaved Evidence Reasoning We introduce Multimodal Interleaved Evidence Reasoning (MIER), reasoning paradigm that tightly interleaves textual hypothesis generation with targeted visual evidence acquisition. Unlike language-only Chain-of-Thought reasoning, MIER allows intermediate reasoning steps to be explicitly grounded in visual observations obtained on demand. Formally, given an initial textual query T0 and candidate image set I0, the reasoning agent iteratively produces outputs: Ok = fMLLM (cid:0){Ti, Ci, Vi}k i=0 (cid:1), where Ti denotes textual reasoning step, Ci denotes tool invocation request, and Vi represents the visual evidence returned by the tool. parser then determines whether to extract the next reasoning step and tool request (Tk+1, Ck+1), or to terminate the process and output final ranking. If tool is invoked, the corresponding visual tool is executed and returns new visual evidence Vk+1, which is appended to the reasoning context. This process yields multimodal reasoning trajectory: τ = {T1, C1, V1, T2, C2, V2, . . . , Tn, An}, where An denotes the final ranked list of candidates. By explicitly grounding intermediate reasoning steps in dynam4 3.4. Visual Tools To support MIER, we equip the reasoning agent with set of Visual Tools, which serve as external perceptual interfaces for selective visual inspection. These tools allow the model to control what to observe and where to focus during reasoning. Specifically, we implement two tools: (1) SELECT-IMAGE, which enables the agent to select subset of candidate images for closer inspection when multiple candidates exhibit high semantic similarity. (2) ZOOM-IN, which performs localized zoom-in operations on specified regions of an image, allowing fine-grained analysis of discriminative visual attributes such as objects, textures, or spatial configurations. These tools facilitate selective perception during retrieval. Rather than encoding all visual information upfront, the agent dynamically expands its visual receptive field only when necessary, closely mirroring human retrieval behavior in which ambiguous candidates are resolved by looking again at critical details. 3.5. Training V-Retrver via Curriculum-Based Agentic Learning Training V-Retrver requires transforming general-purpose MLLM into an agent capable of stable, evidence-driven V-Retrver: Evidence-Driven Agentic Reasoning for Universal Multimodal Retrieval reasoning and strategic tool usage. To this end, we design three-stage curriculum that progressively builds reasoning structure, reliability, and decision-making optimality. let Ωlist denote the set of trajectories whose final answers strictly follow the required integer ranking list format. We define: Stage I: Reasoning Activation via Supervised FineTuning. We begin with cold-start supervised fine-tuning stage to activate basic reasoning and tool-use behaviors. Since existing retrieval datasets lack annotated reasoning trajectories, we synthesize multimodal Chain-of-Thought data using Qwen2.5-VL-72B-Instruct. These trajectories include structured reasoning steps and valid tool invocation patterns. After applying rule-based filtering to remove logically inconsistent or malformed samples, the base model is fine-tuned using standard SFT loss. This stage establishes foundational reasoning syntax and tool awareness, but does not yet guarantee robustness or optimal tool-use strategies. Stage II: Rejection Fine-Tuning for Reasoning Reliability. Although Stage activates tool-use behavior, the resulting policy exhibits high variance and produces large fraction of low-quality trajectories. To improve reasoning reliability, we perform Rejection Sampling Fine-Tuning (RSFT). For each training instance, we sample multiple reasoning trajectories and retain only those that strictly satisfy formatting constraints and yield correct retrieval rankings. Fine-tuning on this filtered dataset significantly improves logical consistency and format compliance, providing stable initialization for reinforcement learning. Stage III: Evidence-Aligned Policy Optimization. While the previous stages activate structured reasoning and improve trajectory reliability, they do not explicitly optimize how visual evidence should be acquired during retrieval. In practice, the model may either underutilize visual inspection or invoke tools redundantly without contributing to better ranking decisions. To address this limitation, we introduce Evidence-Aligned Policy Optimization (EAPO), reinforcement learning objective that explicitly aligns retrieval performance with effective and economical visual verification behavior. rformat(oi) = 1 2 I{oiΩtag} + 1 2 I{oiΩlist}, (2) where I{} is the indicator function. This term primarily serves as stabilizing signal, preventing malformed trajectories from dominating policy updates. Soft Ranking Reward. To mitigate the sparsity of binary correctness signals in retrieval tasks, we introduce soft ranking reward rrank that provides dense feedback based on the relative position of the correct candidate. Let denote the 1-indexed rank of the ground-truth candidate in the predicted list of trajectory oi. If the correct candidate does not appear within the top-Kr positions or the output is invalid, the reward is set to zero. Otherwise, it is defined as: (cid:18) (cid:19) rrank(oi) = exp , (3) (k 1)2 2σ2 where σ controls the sensitivity to ranking errors. This formulation encourages the agent to continuously improve ranking quality rather than optimizing sparse top-1 signal. Tool-Use Reward. The tool-use reward rtool directly governs the agents evidence acquisition behavior, encouraging visual inspection only when it contributes to correct decisions while discouraging redundant or excessive tool usage. Let Ntool denote the number of valid visual tool invocations in trajectory oi, and let be the rank position of the correct candidate. We define: rtool(oi) = η I{k=1} I{Ntool>0} ρ max(0, Ntool τ ), (4) where η incentivizes successful evidence-based verification, ρ penalizes excessive tool invocations, and τ specifies tolerance threshold. This design explicitly encodes the principle that effective tool usage, rather than frequent usage, should be rewarded. EAPO formulates multimodal retrieval as trajectory-level decision-making problem, where each reasoning trajectory oi is evaluated based on both ranking quality and evidence utilization. Specifically, we define composite reward: Policy Optimization. We instantiate EAPO using Group Relative Policy Optimization (GRPO) (Guo et al., 2025). Given group of trajectories sampled for the same query, we compute normalized advantages: Ri = αrformat(oi) + βrrank(oi) + rtool(oi), (1) where the three components respectively encourage structural correctness, accurate ranking, and informative visual inspection. Below, we detail each reward term. Format Compliance Reward. The format compliance reward rformat ensures that the model adheres to the required reasoning and output protocols, which is essential for stable policy optimization with structured multimodal outputs. Let Ωtag denote the set of trajectories whose outputs are correctly enclosed by predefined <think> and <answer> tags, and Ai = Ri mean(R) std(R) . The final optimization objective is: (cid:34) JEAPO(θ) = 1 (cid:88) i=1 πθ(oiq) πθold (oiq) (5) (cid:35) Ai λKL(πθπref) . (6) Through EAPO, the model learns not only what to rank, but also how and when to acquire visual evidence in order to support reliable and efficient retrieval decisions. 5 V-Retrver: Evidence-Driven Agentic Reasoning for Universal Multimodal Retrieval 4. Experiments 4.1. Experimental Setup Table 1. Summary of the evaluation benchmarks. The benchmarks are categorized into Supervised and Zero-shot settings. # Queries represents the number of test queries, and # Candidates denotes the number of test candidates per query. Benchmark Supervised M-BEIR (Wei et al., 2024a) # Queries # Candidates 190K 5.6M Zero-shot CIRCO (Baldrati et al., 2023) GeneCIS (Vaze et al., 2023) Visual Storytelling (Huang et al., 2016) Visual Dialog (Das et al., 2017) Multi-round FashionIQ (Yuan & Lam, 2021) 800 8K 5K 2K 2.4K 120K 10 15 8K 2K 6.2K Datasets and Metrics. We utilize the M-BEIR (Wei et al., 2024a) dataset for training. The M-BEIR dataset encompasses eight distinct retrieval tasks across 10 different retrieval datasets, comprising total of 1.1M training samples. As shown in Table 1, to evaluate the versatility of V-Retrver, across various retrieval tasks, we conduct assessments on the M-BEIR test set. Furthermore, we investigate V-Retrvers generalization ability on other previously unseen datasets, including CIRCO (Baldrati et al., 2023),GeneCIS (Vaze et al., 2023), Visual Storytelling (Huang et al., 2016), Visual Dialog (Das et al., 2017), among others. We adhere to the standard evaluation metrics established for each dataset.We primarily utilize Recall@K as the evaluation metric for the retrieval tasks. Additionally, for specific datasets like CIRCO, we report MAP@5 to provide more nuanced evaluation of ranking quality. Experiment Settings & Baselines. We establish three distinct experiment settings: (i) To validate the versatility of our method across range of retrieval tasks, we train V-Retrver on all 8 tasks in the M-BEIR benchmark and evaluate its performance on the test sets. For the baselines, we compare our model against: (1) foundational VLMs (e.g., Qwen2.5VL, CLIP, BLIP); (2) fine-tuned universal retrievers such as UniIR-BLIPFF and UniIR-CLIPSF; and (3) recent reasoningenhanced models and universal retriever, including VisionR1 (Huang et al., 2025), VLM-R1 (Shen et al., 2025), MMEmbed (Lin et al., 2024a), LamRA (Liu et al., 2025) and UMARVEL (Li et al., 2025a) to demonstrate the advantages of our visual CoT framework. (ii) To evaluate the generalization ability on previously unseen retrieval datasets, we perform zero-shot experiments on 5 datasets not encountered during training. In this case, the baseline includes selection of universal retrievers, such as E5-V, MagicLens, and MM-Embed. (iii) To investigate the generalization capacity on unseen retrieval tasks, we intentionally exclude data from three retrieval tasks: image-to-image retrieval, text-image-to-text retrieval, and text-image-to-text-image retrieval. Training is then conducted on the remaining five tasks with the evaluation of these excluded tasks. Sliding Window Reranking. Following the coarse-to-fine paradigm, V-Retrver employs sliding window strategy to rerank the initial retrieval results. Specifically, we first retrieve the top candidates using the MLLM-based embedding model ϕ as described in Sec. 3.1. Inspired by the iterative reranking approach in (Zhang et al., 2025a), we set the window size to = 20 with stride of 10 to efficiently identify the most relevant items. This results in four MLLM reasoning calls per query to progressively refine the results into finalized rank. This sliding window approach allows our model to perform fine-grained multimodal reasoning over large candidate pool while maintaining manageable computational overhead. Implementation Details. Our model is initialized based on Qwen2.5-VL-7B-Instruct (Bai et al., 2025). For the SFT and Rejection Fine-Tuning stages, we utilize the LLaMAFactory (Zheng et al., 2024) framework and conduct training on 8 A800 GPUs with batch size of 64 and learning rate of 1 105 for two epochs. The RL training is based on the verl-tool (Jiang et al., 2025) framework, which extends the functionalities of verl (Sheng et al., 2024) and vLLM (Kwon et al., 2023) to provide specialized support for multimodal tool-augmented multi-turn training and evaluation. For the RL stage, the model is trained for 1 epoch with learning rate of 1 106, using 8 rollouts per query. Throughout all training stages, the vision encoder remains frozen, while the language model is fine-tuned. The number of candidates input to the MLLM θ is set to 20. During the M-BEIR evaluation, experiments are conducted in the local pool, with V-Retrver reranking the top-50 results. For experiments on unseen datasets, reranking is applied to the top-10 results. The soft ranking sensitivity σ is set to 1.0, and the ranking reward threshold Kr is set to 5. The reward weighting factors α and β are fixed at 0.2 and 0.8, respectively. Regarding the tool-use mechanism, the hyperparameters in Eq. (4) are configured as η = 0.2, ρ = 0.1, and τ = 1. Additionally, we use KL penalty coefficient λ = 0 in the EAPO objective. 4.2. Main Results Performance on M-BEIR. As presented in Table 2, VRetrver-7B establishes new state-of-the-art across the MBEIR benchmark with an average Recall of 69.7%. This represents significant improvement of +4.9% over the strongest baseline U-MARVEL-7B(64.8%). The advantages of our method are particularly evident in scenarios requiring fine-grained visual detail, such as (qi, qt) ci on FIQ and CIRR. In contrast, V-Retrver achieves 51.2% on FIQ and 73.5% on CIRR. These scores substantially outperform U-MARVEL-7B, which achieves 38.2% and 63.2% respectively. These results confirm that the multimodal in6 V-Retrver: Evidence-Driven Agentic Reasoning for Universal Multimodal Retrieval Table 2. Comparison with other methods on M-BEIR test set. R@K refers to the Recall@K metric. qt, qi, ct and ci denote the text query, image query, text candidates and image candidates, respectively. Abbreviations used include VN for VisualNews, F200K for Fashion200K, InfoS for InfoSeek, and FIQ for FashionIQ. The best results are highlighted in bold. qt ci qt ct qt (ci, ct) qi ct qi ci (qi, qt) ct (qi, qt) ci (qi, qt) (ci, ct) Models VN COCO F200K WebQA EDIS WebQA VN COCO F200K NIGHTS OVEN InfoS FIQ CIRR OVEN InfoS Avg. R@5 R@5 R@10 R@5 R@5 R@5 R@5 R@5 R@10 R@ R@5 R@5 R@10 R@5 R@5 R@5 43.3 CLIP-L (Radford et al., 2021) 30.1 SigLIP (Zhai et al., 2023) 16.4 BLIP (Li et al., 2022) BLIP2 (Li et al., 2023b) 16.7 UniIR-BLIPFF (Wei et al., 2024b) 23.4 UniIR-CLIPSF (Wei et al., 2024b) 42.6 Qwen2.5-VL-3B (Bai et al., 2025) 36.0 Qwen2.5-VL-7B (Bai et al., 2025) 40.2 Vision-R1-7B (Huang et al., 2025) 41.9 VLM-R1-7B (Shen et al., 2025) 40.5 MM-Embed-7B (Lin et al., 2024a) 41.0 LamRA-7B (Liu et al., 2025) 48.0 U-MARVEL-7B (Li et al., 2025a) 49.4 61.1 75.7 74.4 63.8 79.7 81.1 67.8 71.9 75.0 77.2 71.3 85.2 85.6 V-Retrver-7B 51. 87.5 6.6 36.5 15.9 14.0 26.1 18.0 16.1 20.3 22.0 22.5 17.1 32.9 34.2 40.3 36.2 39.8 44.9 38.6 80.0 84.7 69.5 71.9 70.6 72.3 95.9 96.7 98.5 96.9 43.3 27.0 26.8 26.9 50.9 59.4 45.2 49.4 51.3 50.0 68.8 75.8 81. 82.9 45.1 43.5 20.3 24.5 79.8 78.7 61.7 64.5 69.1 67.9 85.0 87.7 89.4 90.2 41.3 30.8 17.2 15.0 22.8 43.1 23.3 29.3 35.4 36.2 41.3 48.6 50.5 79.0 88.2 83.2 80.0 89.9 92.3 82.3 84.6 85.1 86.3 90.1 92.3 88.4 52. 94.8 7.7 34.2 19.9 14.2 28.9 18.3 12.0 19.4 22.4 20.9 18.4 36.1 37.7 37.8 26.1 28.9 27.4 25.4 33.0 32.0 20.9 25.5 25.9 26.4 32.4 33.5 34.7 39.8 24.2 29.7 16.1 12.2 41.0 45.5 36.7 42.4 48.8 48.8 42.1 59.2 63. 20.5 25.1 10.2 5.5 22.4 27.9 22.3 32.1 44.0 37.5 42.3 64.1 62.9 7.0 14.4 2.3 4.4 29.2 24.4 24.3 25.0 29.2 29.9 25.7 37.8 38.2 13.2 22.7 10.6 11.8 52.2 44.6 53.5 55.1 57.7 57.4 50.0 63.3 63.2 38.8 41.7 27.4 27.3 55.8 67.6 56.4 60.8 66.2 64.0 64.1 79.2 80.8 69.8 73. 51.2 73.5 87.8 26.4 27.4 16.6 15.8 33.0 48.9 49.8 54.9 59.0 62.3 57.7 78.3 78.9 85.0 32.5 37.2 26.8 24.8 46.8 50.6 42.4 46.7 50.2 50.0 52.7 63.7 64. 69.7 Table 3. Experimental results on unseen datasets. qdialog and (qi qt) refer to the dialog queries and multi-interleaved imagetext queries, respectively. Table 4. Experimental results on held-out tasks. indicates that training is performed on the remaining tasks, w/o any exposure to the three held-out tasks. (qi, qt) ci qdialog ci (qi qt) ci qi ci (qi, qt) ct (qi, qt) (ci, ct) Models CIRCO GeneCIS MAP@5 R@1 CLIP-L (Radford et al., 2021) UniIR-CLIP (Wei et al., 2024b) E5-V (Jiang et al., 2024) MagicLens-L (Zhang et al., 2024a) MM-Embed-7B (Lin et al., 2024a) LamRA-7B (Liu et al., 2025) V-Retrver-7B 4.0 12.5 24.8 29.6 35.5 42.8 48.2 13.3 16.8 18.5 16.3 22.9 24.8 30.7 VisD R@ 23.7 26.8 54.6 28.0 64.7 70.9 75.1 VIST MT-FIQ Models NIGHTS OVEN InfoS OVEN InfoS Avg. R@1 R@ 0.6 0.6 10.0 3.3 25.7 28.6 31.2 17.7 39.4 19.2 22.6 59.0 63.9 68.3 R@5 R@5 R@5 R@ R@5 Supervised UniIR-BLIPFF (Wei et al., 2024b) UniIR-CLIPSF (Wei et al., 2024b) Zero-shot Qwen2.5-VL-7B (Bai et al., 2025) Vision-R1-7B (Huang et al., 2025) LamRA-7B (Liu et al., 2025) V-Retrver-7B 33.0 32.0 20.3 22.9 29.2 36.2 41.0 45. 22.4 27.9 55.8 67.6 38.5 39.8 46.9 57.8 40.4 42.9 54.2 65.9 53.6 57.4 65.1 75.3 33.0 48. 44.9 46.5 59.1 70.3 37.0 44.4 39.5 41.9 50.9 61.1 terleaved chain-of-thought reasoning method can effectively improve the models information retrieval capabilities. Generalization to Unseen Datasets. The zero-shot evaluation results in Table 3 underscore the robustness of our reasoning framework on datasets not encountered during training. V-Retrver consistently outperforms specialized models and generalist MLLMs. Notably, on CIRCO which features distinct domain shifts, V-Retrver achieves MAP@5 of 48.2. This significantly surpasses the specialized MMEmbed-7B (35.5) and LamRA-7B (42.8). Similarly, on GeneCIS, our model attains an R@1 of 30.7 compared to 24.8 for LamRA-7B. We attribute this generalization to reinforcement learning. Robustness on Held-out Tasks. To verify task-level adaptability, we evaluate V-Retrver on retrieval tasks where specific modality combinations were strictly excluded during training. As shown in Table 4, even without prior exposure to these formats, the model achieves an average Recall of 61.1%, significantly outperforming LamRA-7B (50.9%) by margin of 10.2%. These results empirically demonstrate that the MIER framework effectively decouples the reasoning process from specific input types, empowering the model to leverage interleaved evidence for accurate retrieval even in challenging zero-shot scenarios. 4.3. Ablation Study & Analysis Impact of Training Stages. Table 6 presents the ablation results for each training stage. The row w/o SFT & RSFT & RL refers to directly prompting the untrained backbone for tool use, which results in performance collapse to 45.8%, even lower than the Qwen2.5-VL-7B baseline (47.2%), indicating that zero-shot tool invocation without alignment is ineffective. The w/o RSFT & RL setting includes only the SFT stage, which activates basic tool-use ability and raises the average recall to 59.4%. Removing only RSFT (w/o RSFT) means the model is trained with SFT and RL, skipping the rejection sampling phase, and achieves 66.3%. The w/o RL configuration applies SFT and RSFT but omits reinforcement learning, resulting in 60.9%. Finally, the full pipeline reaches the highest performance at 67.2%. These results highlight the importance of structured curriculum learning, as each stage addresses specific shortcomings of the previous one. 7 V-Retrver: Evidence-Driven Agentic Reasoning for Universal Multimodal Retrieval (a) Rank Reward (b) Response Length (c) Tool Calls Figure 3. RL Training curves. Table 5. Ablation study on visual tool-use mechanism. We compare the proposed multimodal interleaved CoT (with Visual Tool) against text-only reasoning baseline (w/o Visual Tool) under the same RL training framework. Table 6. Ablation study on training stages and components. We investigate the impact of Cold Start (SFT), Rejection Sampling Fine-Tuning (RSFT), and Reinforcement Learning (RL) using Qwen2.5-VL-7B as the backbone. qt ci qi ct (qi, qt) ci (qi, qt) ct qt ci qi ct (qi, qt) ci (qi, qt) ct Variants Qwen2.5-VL-7B (Bai et al., 2025) RL w/o tool V-Retrver-7B COCO F200K R@5 R@10 71.9 84. 87.5 19.4 33.2 37.8 CIRR R@5 55.1 66. 73.5 R@5 42.4 63.2 69.8 47.2 61.8 67. OVEN Avg. Training Stage Qwen2.5-VL-7B (Bai et al., 2025) w/o SFT & RSFT & RL w/o RSFT & RL w/o RSFT w/o RL V-Retrver-7B COCO F200K R@5 R@10 71.9 71.5 83.2 87.2 83.9 87.5 19.4 18.1 31.6 37.3 32.8 37. CIRR R@5 55.1 53.4 63.7 72.4 65.3 73.5 OVEN Avg. R@5 42.4 40.2 59.0 68.3 61.5 69.8 47.2 45.8 59.4 66.3 60.9 67.2 Effectiveness of Visual Tool. To isolate the impact of tool-use, we train variant of Qwen2.5-VL-7B-Instruct using end-to-end RL with text-based CoT reasoning on the same training dataset (RL w/o tool). As shown in Table 5. The text-only variant achieves an average recall of 61.8%, whereas V-Retrver reaches 67.2%. The findings confirm that incorporating vision tools yields supplementary, highfidelity insights that text reasoning alone cannot capture from static representations. Specifically, the ability to actively zoom in or select images allows the model to resolve fine-grained ambiguities that are often lost in compressed visual embeddings, proving indispensable for truly precise multimodal retrieval. 4.4. Training Curves Fig.3 illustrates the evolution of ranking accuracy, reasoning density, and tool-use efficiency throughout the RL training process. As the training progresses, the models retrieval accuracy exhibits generally upward trend, indicating that EAPO effectively enhances the models perception-driven reasoning. Regarding tool-use behavior, we observe that the number of effective tool calls is slightly lower than the total number of invocations in the initial stages. This suggests that while the model acquired basic tool-use capabilities during the SFT and RSFT stages, it still occasionally committed formatting inconsistencies or logical missteps. As training continues, these two curves converge, demonstrating that RL further reinforces tool-use robustness and eliminates erroneous calls. This convergence signifies that the policy 8 optimization process successfully penalizes hallucinated tool actions, steering the agent toward more rigorous execution of tool protocols. Additionally, the average response length and tool frequency decrease before stabilizing; this indicates the model learns to autonomously judge the necessity of visual evidence, effectively suppressing redundant reasoning and focusing its attention on resolving critical visual ambiguities through more grounded and purposeful multimodal trajectories. 5. Conclusion In this paper, we presented V-Retrver, an evidence-driven MLLM framework tailored for universal multimodal retrieval. V-Retrver adopts multimodal interleaved Chain-ofThought (CoT) reasoning, enabling the model to dynamically inspect and verify candidate images through visual tool invocation, thereby achieving more fine-grained ranking of candidate result lists. We adopt three-stage training pipeline to multimodal interleaved CoT reasoning abilities. Extensive experimental results demonstrate that V-Retrver achieves significant improvements in both model effectiveness and task generalization. We regard V-Retrver to be an important step toward effectively introducing agentic MLLMs to enhance downstream multimodal tasks, laying solid foundation for building general agentic MLLMs with advanced reasoning capabilities. V-Retrver: Evidence-Driven Agentic Reasoning for Universal Multimodal Retrieval"
        },
        {
            "title": "Impact Statement",
            "content": "This paper presents work whose goal is to advance the field of machine learning. There are many potential societal consequences of our work, none of which we feel must be specifically highlighted here."
        },
        {
            "title": "References",
            "content": "Alayrac, J.-B., Donahue, J., Luc, P., Miech, A., Barr, I., Hasson, Y., Lenc, K., Mensch, A., Millican, K., Reynolds, M., et al. Flamingo: visual language model for fewshot learning. Advances in neural information processing systems, 35:2371623736, 2022. Asai, A., Schick, T., Lewis, P., Chen, X., Izacard, G., Riedel, S., Hajishirzi, H., and Yih, W.-t. Task-aware retrieval with instructions. In Findings of the Association for Computational Linguistics: ACL 2023, pp. 36503675, 2023. Bai, J., Bai, S., Chu, Y., Cui, Z., Dang, K., Deng, X., Fan, Y., Ge, W., Han, Y., Huang, F., et al. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023. Bai, S., Chen, K., Liu, X., Wang, J., Ge, W., Song, S., Dang, K., Wang, P., Wang, S., Tang, J., et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. Baldrati, A., Bertini, M., Uricchio, T., and Del Bimbo, A. Effective conditioned and composed image retrieval In Proceedings of the combining clip-based features. IEEE/CVF conference on computer vision and pattern recognition, pp. 2146621474, 2022. Baldrati, A., Agnolucci, L., Bertini, M., and Del Bimbo, A. Zero-shot composed image retrieval with textual inversion. In Proceedings of the International Conference on Computer Vision, 2023. Chen, Y., Hu, H., Luan, Y., Sun, H., Changpinyo, S., Ritter, A., and Chang, M.-W. Can pre-trained vision and language models answer visual information-seeking questions? In Proceedings of the Conference on Empirical Methods in Natural Language Processinng, 2023. Chen, Z., Wang, W., Cao, Y., Liu, Y., Gao, Z., Cui, E., Zhu, J., Ye, S., Tian, H., Liu, Z., et al. Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling. arXiv preprint arXiv:2412.05271, 2024a. Chen, Z., Wu, J., Wang, W., Su, W., Chen, G., Xing, S., Zhong, M., Zhang, Q., Zhu, X., Lu, L., et al. Internvl: Scaling up vision foundation models and aligning for In Proceedings of the generic visual-linguistic tasks. IEEE/CVF conference on computer vision and pattern recognition, pp. 2418524198, 2024b. Chen, Z., Xu, C., Qi, Y., and Guo, J. Mllm is strong reranker: Advancing multimodal retrieval-augmented generation via knowledge-enhanced reranking and noisearXiv preprint arXiv:2407.21439, injected training. 2024c. Chun, S., Oh, S. J., De Rezende, R. S., Kalantidis, Y., and Larlus, D. Probabilistic embeddings for cross-modal retrieval. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 84158424, 2021. Das, A., Kottur, S., Gupta, K., Singh, A., Yadav, D., Moura, J. M., Parikh, D., and Batra, D. Visual dialog. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2017. Fu, Z., Zhang, L., Xia, H., and Mao, Z. Linguistic-aware patch slimming framework for fine-grained cross-modal alignment. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 26307 26316, 2024. Gao, Z., Chen, Z., Cui, E., Ren, Y., Wang, W., Zhu, J., Tian, H., Ye, S., He, J., Zhu, X., et al. Mini-internvl: flexibletransfer pocket multi-modal model with 5% parameters and 90% performance. Visual Intelligence, 2(1):117, 2024. Gu, G., Chun, S., Kim, W., Kang, Y., and Yun, S. Languageonly training of zero-shot composed image retrieval. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1322513234, 2024. Gu, T., Yang, K., Feng, Z., Wang, X., Zhang, Y., Long, D., Chen, Y., Cai, W., and Deng, J. Breaking the modality barrier: Universal embedding learning with multimodal In Proceedings of the 33rd ACM International llms. Conference on Multimedia, pp. 28602869, 2025. Guo, D., Yang, D., Zhang, H., Song, J., Zhang, R., Xu, R., Zhu, Q., Ma, S., Wang, P., Bi, X., et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Guo, Z., Xu, R., Yao, Y., Cui, J., Ni, Z., Ge, C., Chua, T.-S., Liu, Z., and Huang, G. Llava-uhd: an lmm perceiving any aspect ratio and high-resolution images. In European Conference on Computer Vision, pp. 390406. Springer, 2024. Hu, H., Luan, Y., Chen, Y., Khandelwal, U., Joshi, M., Lee, K., Toutanova, K., and Chang, M.-W. Open-domain visual entity recognition: Towards recognizing millions of wikipedia entities. In Proceedings of the International Conference on Computer Vision, 2023. 9 V-Retrver: Evidence-Driven Agentic Reasoning for Universal Multimodal Retrieval Huang, T.-H., Ferraro, F., Mostafazadeh, N., Misra, I., Agrawal, A., Devlin, J., Girshick, R., He, X., Kohli, P., Batra, D., et al. Visual storytelling. In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics, 2016. Huang, W., Jia, B., Zhai, Z., Cao, S., Ye, Z., Zhao, F., Hu, Y., and Lin, S. Vision-r1: Incentivizing reasoning capability in multimodal large language models. arXiv preprint arXiv:2503.06749, 2025. Ji, D., Zhao, F., Zhu, L., Jin, W., Lu, H., and Ye, J. Discrete latent perspective learning for segmentation and detection. arXiv preprint arXiv:2406.10475, 2024. Jian, W., Zhang, Y., Liang, D., Xie, C., He, Y., Leng, D., and Yin, Y. Rzenembed: Towards comprehensive multimodal retrieval. arXiv preprint arXiv:2510.27350, 2025. large language-and-vision assistant for biomedicine in one day. Advances in Neural Information Processing Systems, 36:2854128564, 2023a. Li, J., Li, D., Xiong, C., and Hoi, S. Blip: Bootstrapping language-image pre-training for unified vision-language In Proceedings of the understanding and generation. International Conference on Machine Learning, 2022. Li, J., Li, D., Savarese, S., and Hoi, S. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In Proceedings of the International Conference on Machine Learning, 2023b. Li, X., Li, C., Chen, S.-Z., and Chen, X. U-marvel: Unveiling key factors for universal multimodal retrieval via embedding learning with mllms. arXiv preprint arXiv:2507.14902, 2025a. Jiang, D., Lu, Y., Li, Z., Lyu, Z., Nie, P., Wang, H., Su, A., Chen, H., Zou, K., Du, C., et al. Verltool: Towards holistic agentic reinforcement learning with tool use. arXiv preprint arXiv:2509.01055, 2025. Li, Y., Jiang, S., Hu, B., Wang, L., Zhong, W., Luo, W., Ma, L., and Zhang, M. Uni-moe: Scaling unified multimodal IEEE Transactions on llms with mixture of experts. Pattern Analysis and Machine Intelligence, 2025b. Jiang, T., Song, M., Zhang, Z., Huang, H., Deng, W., Sun, F., Zhang, Q., Wang, D., and Zhuang, F. E5-v: Universal embeddings with multimodal large language models. arXiv preprint arXiv:2407.12580, 2024. Lin, B., Ye, Y., Zhu, B., Cui, J., Ning, M., Jin, P., and Yuan, L. Video-llava: Learning united visual representation by alignment before projection. arXiv preprint arXiv:2311.10122, 2023a. Jin, H., Zhang, Y., Shi, L., Zhang, S., Kou, F., Yang, J., Zhu, C., and Luo, J. An end-to-end graph attention network hashing for cross-modal retrieval. Advances in Neural Information Processing Systems, 37:21062126, 2024. Lin, S.-C., Lee, C., Shoeybi, M., Lin, J., Catanzaro, B., and Ping, W. Mm-embed: Universal multimodal retrieval with multimodal llms. arXiv preprint arXiv:2411.02571, 2024a. Kim, D., Kim, N., and Kwak, S. Improving cross-modal retrieval with set of diverse embeddings. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 2342223431, 2023a. Lin, W., Chen, J., Mei, J., Coca, A., and Byrne, B. Finegrained late-interaction multi-modal retrieval for retrieval augmented visual question answering. In Advances in Neural Information Processing Systems, 2023b. Kim, J. M., Koepke, A., Schmid, C., and Akata, Z. Exposing and mitigating spurious correlations for cross-modal retrieval. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 25852595, 2023b. Kwon, W., Li, Z., Zhuang, S., Sheng, Y., Zheng, L., Yu, C. H., Gonzalez, J. E., Zhang, H., and Stoica, I. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS Symposium on Operating Systems Principles, 2023. Lan, Z., Niu, L., Meng, F., Zhou, J., and Su, J. Llave: Large language and vision embedding models with hardness-weighted contrastive learning. arXiv preprint arXiv:2503.04812, 2025. Li, C., Wong, C., Zhang, S., Usuyama, N., Liu, H., Yang, J., Naumann, T., Poon, H., and Gao, J. Llava-med: Training Lin, W., Mei, J., Chen, J., and Byrne, B. PreFLMR: Scaling up fine-grained late-interaction multi-modal retrievers. In Association for Computational Linguistics, 2024b. Liu, S., Cheng, H., Liu, H., Zhang, H., Li, F., Ren, T., Zou, X., Yang, J., Su, H., Zhu, J., et al. Llava-plus: Learning to use tools for creating multimodal agents. In European Conference on Computer Vision, pp. 126142. Springer, 2024. Liu, Y., Chen, P., Cai, J., Jiang, X., Hu, Y., Yao, J., Wang, Y., and Xie, W. Lamra: Large multimodal model as your advanced retrieval assistant. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2025. Liu, Z., Xiong, C., Lv, Y., Liu, Z., and Yu, G. Universal vision-language dense retrieval: Learning unified representation space for multi-modal retrieval. In The Eleventh 10 V-Retrver: Evidence-Driven Agentic Reasoning for Universal Multimodal Retrieval International Conference on Learning Representations, 2023. of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1930519314, 2023. Lu, D., Sun, Y., Zhang, Z., Huang, L., Zeng, J., Shu, M., and Cao, H. Internvl-x: Advancing and accelerating internvl series with efficient visual token compression. arXiv preprint arXiv:2503.21307, 2025. Shen, H., Liu, P., Li, J., Fang, C., Ma, Y., Liao, J., Shen, Q., Zhang, Z., Zhao, K., Zhang, Q., et al. Vlm-r1: stable and generalizable r1-style large vision-language model. arXiv preprint arXiv:2504.07615, 2025. Marino, K., Rastegari, M., Farhadi, A., and Mottaghi, R. Ok-vqa: visual question answering benchmark requirIn Proceedings of the IEEE ing external knowledge. Conference on Computer Vision and Pattern Recognition, 2019. Mensink, T., Uijlings, J., Castrejon, L., Goel, A., Cadar, F., Zhou, H., Sha, F., Araujo, A., and Ferrari, V. Encyclopedic vqa: Visual questions about detailed properties of fine-grained categories. In Proceedings of the International Conference on Computer Vision, 2023. Muennighoff, N., Yang, Z., Shi, W., Li, X. L., Fei-Fei, L., Hajishirzi, H., Zettlemoyer, L., Liang, P., Cand`es, E., and Hashimoto, T. s1: Simple test-time scaling. arXiv preprint arXiv:2501.19393, 2025. Narayan, K., Xu, Y., Cao, T., Nerella, K., Patel, V. M., Shiee, N., Grasch, P., Jia, C., Yang, Y., and Gan, Z. Deepmmsearch-r1: Empowering multimodal llms in multimodal web search. arXiv preprint arXiv:2510.12801, 2025. Pei, R., Liu, J., Li, W., Shao, B., Xu, S., Dai, P., Lu, J., and Yan, Y. Clipping: Distilling clip-based models with student base for video-language retrieval. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1898318992, 2023. Pham, K., Huynh, C., Lim, S.-N., and Shrivastava, A. Composing object relations and attributes for image-text matching. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 14354 14363, 2024. Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al. Learning transferable visual models from natural language supervision. In Proceedings of the International Conference on Machine Learning, 2021. Sain, A., Bhunia, A. K., Chowdhury, P. N., Koley, S., Xiang, T., and Song, Y.-Z. Clip for all things zero-shot sketchbased image retrieval, fine-grained or not. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 27652775, 2023. Saito, K., Sohn, K., Zhang, X., Li, C.-L., Lee, C.-Y., Saenko, K., and Pfister, T. Pic2word: Mapping pictures to words for zero-shot composed image retrieval. In Proceedings Shen, L., Chen, G., Shao, R., Guan, W., and Nie, L. Mome: Mixture of multimodal experts for generalist multimodal large language models. arXiv preprint arXiv:2407.12709, 2024. Sheng, G., Zhang, C., Ye, Z., Wu, X., Zhang, W., Zhang, R., Peng, Y., Lin, H., and Wu, C. Hybridflow: flexible and efficient rlhf framework. arXiv preprint arXiv: 2409.19256, 2024. Shu, F., Liao, Y., Zhuo, L., Xu, C., Zhang, L., Zhang, G., Shi, H., Chen, L., Zhong, T., He, W., et al. Llava-mod: Making llava tiny via moe knowledge distillation. arXiv preprint arXiv:2408.15881, 2024. Sun, N., Tang, J., Sun, L., Chen, R., Lu, Y., Chu, X., and Ling, H. Reflection from retrieval: Mllm-guided iterative reasoning for zero-shot composed image retrieval. Suo, Y., Ma, F., Zhu, L., and Yang, Y. Knowledge-enhanced dual-stream zero-shot composed image retrieval. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2695126962, 2024. Vaze, S., Carion, N., and Misra, I. Genecis: benchmark for general conditional image similarity. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2023. Wang, P., Bai, S., Tan, S., Wang, S., Fan, Z., Bai, J., Chen, K., Liu, X., Wang, J., Ge, W., et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024a. Wang, Y., Wang, L., Zhou, Q., Wang, Z., Li, H., Hua, G., and Tang, W. Multimodal llm enhanced cross-lingual cross-modal retrieval. In Proceedings of the 32nd ACM International Conference on Multimedia, pp. 82968305, 2024b. Wei, C., Chen, Y., Chen, H., Hu, H., Zhang, G., Fu, J., Ritter, A., and Chen, W. Uniir: Training and benchmarking universal multimodal information retrievers. In Proceedings of the European Conference on Computer Vision, 2024a. Wei, C., Chen, Y., Chen, H., Hu, H., Zhang, G., Fu, J., Ritter, A., and Chen, W. Uniir: Training and benchmarking universal multimodal information retrievers. In European Conference on Computer Vision, pp. 387404. Springer, 2024b. 11 V-Retrver: Evidence-Driven Agentic Reasoning for Universal Multimodal Retrieval Wu, H., Gao, Y., Guo, X., Al-Halah, Z., Rennie, S., Grauman, K., and Feris, R. Fashion iq: new dataset towards retrieving images by natural language feedback. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2021. Xie, J., Mao, W., Bai, Z., Zhang, D. J., Wang, W., Lin, K. Q., Gu, Y., Chen, Z., Yang, Z., and Shou, M. Z. Show-o: One single transformer to unify multimodal understanding and generation. arXiv preprint arXiv:2408.12528, 2024. Xu, C., Wang, X., Liao, Z., Li, Y., Hou, T., and Deng, Z. Show-o turbo: Towards accelerated unified multimodal understanding and generation. arXiv preprint arXiv:2502.05415, 2025a. Xu, M., Dong, J., Hou, J., Wang, Z., Li, S., Gao, Z., Zhong, R., and Cai, H. MM-R5: Multimodal reasoning-enhanced reranker via reinforcement learning for document retrieval. arXiv preprint arXiv:2506.12364, 2025b. Xu, M., Dong, J., Hou, J., Wang, Z., Li, S., Gao, Z., Zhong, R., and Cai, H. Mm-r5: Multimodal reasoning-enhanced reranker via reinforcement learning for document retrieval. arXiv preprint arXiv:2506.12364, 2025c. Yang, A., Yang, B., Zhang, B., Hui, B., Zheng, B., Yu, B., Li, C., Liu, D., Huang, F., Wei, H., et al. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115, 2024. Yang, Z., Li, L., Lin, K., Wang, J., Lin, C.-C., Liu, Z., and Wang, L. The dawn of lmms: Preliminary explorations with gpt-4v (ision). arXiv preprint arXiv:2309.17421, 9 (1):1, 2023. Ye, J., Xu, H., Liu, H., Hu, A., Yan, M., Qian, Q., Zhang, J., Huang, F., and Zhou, J. mplug-owl3: Towards long image-sequence understanding in multi-modal large language models. arXiv preprint arXiv:2408.04840, 2024a. Ye, Q., Xu, H., Xu, G., Ye, J., Yan, M., Zhou, Y., Wang, J., Hu, A., Shi, P., Shi, Y., et al. mplug-owl: Modularization empowers large language models with multimodality. arXiv preprint arXiv:2304.14178, 2023. Ye, Q., Xu, H., Ye, J., Yan, M., Hu, A., Liu, H., Qian, Q., Zhang, J., and Huang, F. mplug-owl2: Revolutionizing multi-modal large language model with modality collaboration. In Proceedings of the ieee/cvf conference on computer vision and pattern recognition, pp. 13040 13051, 2024b. Yuan, Y. and Lam, W. Conversational fashion image retrieval via multiturn natural language feedback. In Proceedings of the International ACM SIGIR Conference on Research and Development in Information Retrieval, 2021. Zhai, X., Mustafa, B., Kolesnikov, A., and Beyer, L. Sigmoid loss for language image pre-training. In Proceedings of the International Conference on Computer Vision, 2023. Zhang, K., Luan, Y., Hu, H., Lee, K., Qiao, S., Chen, W., Su, Y., and Chang, M.-W. Magiclens: self-supervised image retrieval with open-ended instructions. In Proceedings of the 41st International Conference on Machine Learning, pp. 5940359420, 2024a. Zhang, L., Wang, B., Qiu, X., Reddy, S., and Agrawal, A. REARANK: Reasoning re-ranking agent via reinforcement learning. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pp. 24582471, 2025a. Zhang, Q., Lei, Z., Zhang, Z., and Li, S. Z. Context-aware attention network for image-text retrieval. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 35363545, 2020. Zhang, Q., Lyu, F., Sun, Z., Wang, L., Zhang, W., Guo, Z., Wang, Y., King, I., Liu, X., and Ma, C. What, how, where, and how well? survey on test-time scaling in large language models. arXiv preprint arXiv:2503.24235, 2025b. Zhang, S., Fang, Q., Yang, Z., and Feng, Y. Llava-mini: Efficient image and video large multimodal models with one vision token. arXiv preprint arXiv:2501.03895, 2025c. Zhang, X., Zhang, Y., Xie, W., Li, M., Dai, Z., Long, D., Xie, P., Zhang, M., Li, W., and Zhang, M. Gme: Improving universal multimodal retrieval by multimodal llms. arXiv preprint arXiv:2412.16855, 2024b. Zheng, Y., Zhang, R., Zhang, J., Ye, Y., Luo, Z., Feng, Z., and Ma, Y. Llamafactory: Unified efficient finetuning of 100+ language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations), Bangkok, Thailand, 2024. Association for Computational Linguistics. URL http://arxiv.org/abs/2403. 13372. Zhou, J., Liu, Z., Liu, Z., Xiao, S., Wang, Y., Zhao, B., Zhang, C. J., Lian, D., and Xiong, Y. Megapairs: Massive data synthesis for universal multimodal retrieval. arXiv preprint arXiv:2412.14475, 2024. Zhu, L., Ji, D., Zhu, S., Gan, W., Wu, W., and Yan, J. Learning statistical texture for semantic segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1253712546, 2021. 12 V-Retrver: Evidence-Driven Agentic Reasoning for Universal Multimodal Retrieval Zhu, L., Chen, T., Ji, D., Ye, J., and Liu, J. Llafs: When large language models meet few-shot segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 30653075, 2024. Zhu, L., Chen, T., Ji, D., Xu, P., Ye, J., and Liu, J. Llafs++: Few-shot image segmentation with large language models. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2025a. Zhu, L., Chen, T., Ji, D., Ye, J., and Liu, J. Not every patch is needed: Towards more efficient and effective backbone for video-based person re-identification. IEEE Transactions on Image Processing, 2025b. Zhu, L., Chen, T., Yin, J., See, S., Soh, D. W., and Liu, J. Replay master: Automatic sample selection and effective memory utilization for continual semantic segmentation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2025c. Zhu, L., Ji, D., Chen, T., Wu, H., and Wang, S. Retrvr1: reasoning-driven mllm framework for universal and efficient multimodal retrieval. arXiv preprint arXiv:2510.02745, 2025d. 13 V-Retrver: Evidence-Driven Agentic Reasoning for Universal Multimodal Retrieval A. Prompt Template A.1. System Prompt Fig. 4 illustrate the system prompt for both training and inference. Figure 4. System Prompt template for training and inference. 14 V-Retrver: Evidence-Driven Agentic Reasoning for Universal Multimodal Retrieval A.2. User Prompt Fig. 5 illustrate the user prompt for both training and inference. Figure 5. User Prompt template for training and inference. 15 V-Retrver: Evidence-Driven Agentic Reasoning for Universal Multimodal Retrieval A.3. Annotation Prompt Fig. 6 illustrate the annotation prompt. Specifically for the CoT annotation process, the annotation prompt  (Fig. 6)  is inserted into the user prompt to guide the generation. Figure 6. Annotation Prompt template. 16 V-Retrver: Evidence-Driven Agentic Reasoning for Universal Multimodal Retrieval B. Details about M-BEIR Dataset We present the details for the M-BEIR benchmark in Table 7. It is important to note that the M-BEIR benchmark applies additional processing to the datasets it incorporates, which may result in differences from the standard evaluation of individual datasets. For instance, the candidate pool of the CIRR dataset in M-BEIR includes training data, which essentially increases the evaluations difficulty compared to the original CIRR dataset. For more comprehensive understanding of these differences, we refer the readers to the original UniIR (Wei et al., 2024a) paper. Table 7. Summary of the M-BEIR benchmarks. Dataset Domain # Train # Dev # Test # Pool Task qt ci qt ct qt (ci, ct) qi ct qi ci (qi, qt) ct (qi, qt) ci (qi, qt) (ci, ct) VisualNews MSCOCO Fashion200K WebQA EDIS WebQA VisualNews MSCOCO Fashion200K NIGHTS OVEN InfoSeek FashionIQ CIRR OVEN InfoSeek News Misc. Fashion Wiki News Wiki News Misc. Fashion Misc. Wiki Wiki Fashion Misc. Wiki Wiki 99K 100K 15K 16K 26K 17K 100K 113K 15K 16K 150K 141K 16K 26K 157K 143K 1.1M 20K 24.8K 1.7K 20K 24.8K 1.7K 1.7K 3.2K 1.7K 20K 5K 4.8K 2K 50K 11K 2K 2K 2.4K 3.2K 2.5K 20K 5K 4.8K 2K 50K 11K 6K 4K 14.7K 17.6K 14.7K 17.6K 182K 190K 542K 5K 201K 544K 1M 403K 537K 25K 61K 40K 676K 611K 74K 21K 335K 481K 5.6M 8 tasks 10 datasets 4 domains C. Details about Unseen Dataset Here, we present the details of the Unseen Dataset in Table 8. Many of them are actually adapted from MSCOCO or FashionIQ, however, note that, their captions or query formats are significantly different. Therefore, we still treat these datasets as unseen datasets. For instance, The query format of CIRCO combines reference image with relative caption. These differences create substantial disparity compared to the original COCO dataset. Table 8. Summary of the Unseen Dataset. Image Source Task Query Format Candidate Format MSCOCO unlabeled set (qi, qt) ci <image><relative caption> Dataset CIRCO GeneCIS Visual Dialog MSCOCO MSCOCO Visual Storytelling Flickr MT-FIQ FashionIQ <image> <image> <image> <image> <image> (qi, qt) ci <image><relative caption> qdialog ci <Q1><A1> <Qj><Aj> (qi qt) ci <text1><image1> <textj> (qi qt) ci <image1><relative caption1> <imagej><relative captionj> V-Retrver: Evidence-Driven Agentic Reasoning for Universal Multimodal Retrieval D. Exploration of RAG Applications To further validate the practical utility of our framework, we extend our evaluation to Retrieval-Augmented Generation (RAG) scenarios. Following the experimental setup of LamRA (Liu et al., 2025),we evaluate our method on three Knowledge-based Visual Question Answering (KVQA) benchmarks. Specifically, we train the retrieval and VQA tasks simultaneously during the training process, allowing the model to to align the agentic visual reasoning process with downstream generation needs. As detailed in Table 9, V-Retrver achieves superior performance in both retrieval precision and VQA accuracy, demonstrating that our Multimodal Interleaved Evidence Reasoning significantly enhances MLLM capabilities in RAG settings. Table 9. Comparison of RAG capabilities on KVQA tasks. Method OKVQA (Marino et al., 2019) Infoseek (Chen et al., 2023) E-VQA (Mensink et al., 2023) PreFLMR (Lin et al., 2024b) LamRA-7B (Liu et al., 2025) V-Retrver-7B RA-VQAv2 w/ PreFLMR (Lin et al., 2023b) LamRA-7B (Liu et al., 2025) V-Retrver-7B Retrieval (PR@5) 70.9 89.0 90. 61.9 64.3 65.7 VQA (ACC) 62.1 73.4 78.3 32.1 28.8 31.9 73.7 75.0 78.1 54.5 56.2 58. E. Algorithms and Detailed Analysis In this section, we present the formal algorithms for the inference and training processes of V-Retrver, followed by complexity analysis. E.1. Inference Process The inference process of V-Retrver, formulated as coarse-to-fine pipeline with sliding window agentic reasoning, is detailed in Algorithm 1. E.2. Training Pipeline The three-stage curriculum learning strategy, designed to progressively align the model with evidence-driven retrieval objectives, is presented in Algorithm 2. 18 V-Retrver: Evidence-Driven Agentic Reasoning for Universal Multimodal Retrieval Algorithm 1 V-Retrver Inference Pipeline n=1, Embedding Model Φ, Reasoning Agent πθ, Top-K size K, Window size Input: Query q, Candidate Pool Ω = {cn}N , Stride Output: Ranked Candidate List ˆL {// Stage 1: Coarse Retrieval (Embedding-based)} Compute similarity scores sn = cos(Φ(q), Φ(cn)) for all cn Ω Select top-K candidates: Ctop Top-K(Ω, {sn}) {// Stage 2: Agentic Reranking (Reasoning-based)} Initialize global ranking list Lglobal Split Ctop into windows {w1, w2, . . . , wm} with size and stride for each window wj {w1, . . . , wm} do Initialize context H0 (q, wj, Instruction) 0 while True do Generate output: ot πθ(Ht) if ot contains <tool call> then Parse action at and arguments from ot Execute visual tool: vobs ftool(at, wj) Update context: Ht+1 Ht ot vobs else if ot contains <answer> then Parse local rank list ˆrj from ot Update Lglobal with local rank ˆrj break end if + 1 end while end for ˆL AggregateRanks(Lglobal) 19 V-Retrver: Evidence-Driven Agentic Reasoning for Universal Multimodal Retrieval Algorithm 2 Curriculum-Based Agentic Training Input: Pretrained MLLM θinit, Retrieval Dataset D, Synth Model Msyn Output: Optimized Policy πθ {// Stage 1: Reasoning Activation (SFT)} Synthesize CoT data: Dsf {(q, c, τcot)} using Msyn on Filter Dsf for format compliance Update θsf Minimize LSF (θinit, Dsf t) {// Stage 2: Reliability Refinement (Rejection Sampling)} Initialize Drsf for each (q, c) do Sample trajectories {τ1, . . . , τk} πθsf (q, c) if IsFormatValid(τi) IsRankCorrect(τi) then Add valid τi to Drsf end if end for Update θrsf Minimize LSF (θsf t, Drsf t) {// Stage 3: Evidence-Aligned Policy Optimization (EAPO)} Initialize θ θrsf t, Reference policy πref θrsf while not converged do Sample batch of queries Bq for each query Bq do Sample group of trajectories = {o1, . . . , oG} πθ(q) Compute rewards R(oi) = αrf mt(oi) + βrrank(oi) + rtool(oi) end for Compute advantages Ai via Group Normalization over Compute GRPO loss JEAP O(θ) Update θ Optimize JEAP O(θ) end while return πθ V-Retrver: Evidence-Driven Agentic Reasoning for Universal Multimodal Retrieval F. Qualitative Examples To provide an intuitive illustration of our approach and to further demonstrate the effectiveness of the proposed V-Retrver, we present some qualitative results (Fig. 7, Fig.8, Fig. 9, Fig. 10 and Fig. 11). These examples illustrate how V-Retrver performs accurate retrieval through fine-grained and structured reasoning, thereby highlighting the strong effectiveness of the proposed method. G. Limitations and Future Works Despite its strong performance, V-Retrver still has several limitations. First, the current visual toolset is restricted to image selection and zoom-in operations, and may be insufficient for more complex visual reasoning that requires object-level manipulation or multi-step spatial analysis. Second, our training relies on synthesized reasoning trajectories and curated rewards, which may introduce biases and limit robustness under more diverse or noisy real-world settings. Future work will explore lightweight and adaptive inference strategies to reduce computational overhead, expand the visual tool repertoire to support richer perceptual operations. We also plan to extend the framework to broader downstream tasks such as multimodal recommendation and retrieval-augmented generation, further advancing general-purpose agentic MLLMs. V-Retrver: Evidence-Driven Agentic Reasoning for Universal Multimodal Retrieval Figure 7. qualitative example of the retrieval result generated from V-Retrver. 22 V-Retrver: Evidence-Driven Agentic Reasoning for Universal Multimodal Retrieval Figure 8. qualitative example of the retrieval result generated from V-Retrver. V-Retrver: Evidence-Driven Agentic Reasoning for Universal Multimodal Retrieval Figure 9. qualitative example of the retrieval result generated from V-Retrver. 24 V-Retrver: Evidence-Driven Agentic Reasoning for Universal Multimodal Retrieval Figure 10. qualitative example of the retrieval result generated from V-Retrver. V-Retrver: Evidence-Driven Agentic Reasoning for Universal Multimodal Retrieval Figure 11. qualitative example of the retrieval result generated from V-Retrver."
        }
    ],
    "affiliations": [
        "Central South University",
        "Fudan University",
        "Pengcheng Laboratory",
        "The Australian National University",
        "The University of Hong Kong",
        "Tsinghua University",
        "University of Central Florida"
    ]
}