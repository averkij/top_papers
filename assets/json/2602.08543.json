{
    "paper_title": "GISA: A Benchmark for General Information-Seeking Assistant",
    "authors": [
        "Yutao Zhu",
        "Xingshuo Zhang",
        "Maosen Zhang",
        "Jiajie Jin",
        "Liancheng Zhang",
        "Xiaoshuai Song",
        "Kangzhi Zhao",
        "Wencong Zeng",
        "Ruiming Tang",
        "Han Li",
        "Ji-Rong Wen",
        "Zhicheng Dou"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The advancement of large language models (LLMs) has significantly accelerated the development of search agents capable of autonomously gathering information through multi-turn web interactions. Various benchmarks have been proposed to evaluate such agents. However, existing benchmarks often construct queries backward from answers, producing unnatural tasks misaligned with real-world needs. Moreover, these benchmarks tend to focus on either locating specific information or aggregating information from multiple sources, while relying on static answer sets prone to data contamination. To bridge these gaps, we introduce GISA, a benchmark for General Information-Seeking Assistants comprising 373 human-crafted queries that reflect authentic information-seeking scenarios. GISA features four structured answer formats (item, set, list, and table), enabling deterministic evaluation. It integrates both deep reasoning and broad information aggregation within unified tasks, and includes a live subset with periodically updated answers to resist memorization. Notably, GISA provides complete human search trajectories for every query, offering gold-standard references for process-level supervision and imitation learning. Experiments on mainstream LLMs and commercial search products reveal that even the best-performing model achieves only 19.30\\% exact match score, with performance notably degrading on tasks requiring complex planning and comprehensive information gathering. These findings highlight substantial room for future improvement."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 9 ] . [ 1 3 4 5 8 0 . 2 0 6 2 : r GISA: Benchmark for General Information Seeking Assistant Yutao Zhu1,,# , Xingshuo Zhang1,, Maosen Zhang1,, Jiajie Jin1 , Liancheng Zhang1, Xiaoshuai Song1, Kangzhi Zhao2 , Wencong Zeng2 , Ruiming Tang2, Han Li2 , Ji-Rong Wen1, Zhicheng Dou1,# Equal Contribution Affiliation: 1Renmin University of China; 2Kuaishou Technology The advancement of large language models (LLMs) has significantly accelerated the development of search agents capable of autonomously gathering information through multi-turn web interactions. Various benchmarks have been proposed to evaluate such agents. However, existing benchmarks often construct queries backward from answers, producing unnatural tasks misaligned with real-world needs. Moreover, these benchmarks tend to focus on either locating specific information or aggregating information from multiple sources, while relying on static answer sets prone to data contamination. To bridge these gaps, we introduce GISA, benchmark for General Information-Seeking Assistants comprising 373 human-crafted queries that reflect authentic information-seeking scenarios. GISA features four structured answer formats (item, set, list, and table), enabling deterministic evaluation. It integrates both deep reasoning and broad information aggregation within unified tasks, and includes live subset with periodically updated answers to resist memorization. Notably, GISA provides complete human search trajectories for every query, offering gold-standard references for process-level supervision and imitation learning. Experiments on mainstream LLMs and commercial search products reveal that even the best-performing model achieves only 19.30% exact match score, with performance notably degrading on tasks requiring complex planning and comprehensive information gathering. These findings highlight substantial room for future improvement. # Contact: yutaozhu94@gmail.com, dou@ruc.edu.cn Code: https://github.com/RUC-NLPIR/GISA Leaderboard: https://ruc-nlpir.github.io/GISA"
        },
        {
            "title": "1 Introduction",
            "content": "With the development of large language models (LLMs), LLM-driven agents have been widely applied in many tasks (Park et al., 2023; Yu et al., 2024; Qian et al., 2024; Li et al., 2025d; Gao et al., 2024). Among these applications, information retrieval stands out as fundamental need in daily life that has been notably transformed by agent-based systems. Traditional search engines require users to manually formulate queries, navigate through multiple web pages, and synthesize information themselves. This process is often time-consuming and requires significant effort. In contrast, search agents leverage the strong reasoning capabilities of modern LLMs to understand complex user information needs. These agents can autonomously conduct multiple rounds of searching and web browsing to collect relevant information and fulfill user requirements. By automating the process of querying and information gathering, search agents significantly reduce user burden and improve information access efficiency. As critical tool for information acquisition, search agents have attracted increasing attention from both academia and industry (Li et al., 2025c,b,d), becoming essential components in many research-oriented products and knowledge discovery platforms (such as OpenAI Deep Research and Gemini Deep Research).1 high-quality benchmark is essential to systematically evaluate the capabilities of search agents and identify directions for further improvement. During the past few years, the research community has proposed various benchmarks to evaluate agents abilities to retrieve and synthesize information from the web (Wei et al., 2025; Xi et al., 2025b; Chen et al., 2025; Mialon et al., 2024; Wong et al., 2025). However, upon closer examination, we identify several critical limitations in existing benchmarks that hinder their effectiveness in comprehensively evaluating general-purpose 1OpenAI Deep Research: https://openai.com/index/introducing-deep-research/, Gemini Deep Research: https://gemini.google/overview/ deep-research/. 1 Table 1 Comparison between GISA with other agentic search benchmarks. Benchmark Deep Wide Question-driven Human Trajectory Live Answer Type Evaluation BrowseComp-EN/ZH InfoDeepSeek ScholarSearch GAIA Xbench-DeepSearch WideSearch DeepWideSearch GISA (Ours) Part of Part of Item Item Item Item, List Item Table Table LLM LLM LLM Metric-based LLM LLM LLM Item, Set, List, Table Metric-based information-seeking agents. First, to increase task complexity or assess agents planning capabilities over challenging problems, many existing benchmarks adopt reverse-engineering approach that constructs queries backward from pre-selected answers (e.g., BrowseComp (Wei et al., 2025)). While this approach can produce difficult tasks, the resulting queries often deviate significantly from authentic human information needs. Some constructed problems may even be unsolvable through natural forward search process. Consequently, optimizing agent performance on such benchmarks may not translate into improved real-world user experience. Second, existing benchmarks mainly focus on evaluating agents ability to perform deep search within the web, i.e., navigating through multiple links and pages to locate specific piece of information (Wei et al., 2025; Wu et al., 2025; Xi et al., 2025b). However, real-world information needs often require not only depth but also breadth, where agents must gather and aggregate information from diverse sources. Although recent efforts such as WideSearch (Wong et al., 2025) have begun to explore the evaluation of broad information collection capabilities, achieving balanced and unified evaluation of both deep and wide search remains an open challenge. Third, search serves as primary means for humans to acquire information, particularly timely and up-to-date knowledge. Ideally, benchmarks should evolve alongside the information landscape to remain relevant. Nevertheless, most existing benchmarks, for the sake of evaluation convenience and reproducibility, rely on questions with long-term stable answers (Xi et al., 2025b). As LLMs are continuously trained on increasingly recent data, such static benchmarks may become out-of-date, as models may have already memorized the answers during pre-training, thereby failing to genuinely test their search capabilities. To address these limitations, we propose GISA, benchmark for General Information-Seeking Assistants containing carefully crafted queries along with their answers and human-annotated search trajectories. GISA distinguishes itself through the following key features: Diverse answer formats with deterministic evaluation: GISA formulates answers into four structured types: items, sets, lists, and tables. This design enables deterministic and reproducible evaluation using strict matching metrics, avoiding the subjectivity and instability of LLM-based judgment while preserving task complexity and diversity. Unification of deep and wide search capabilities: Real-world information seeking often requires both exploring deep reasoning paths and aggregating information across diverse sources. GISA integrates both dimensions, evaluating an agents ability to perform vertical investigation and horizontal summarization within complex, long-horizon tasks. Dynamic and anti-static evaluation: To prevent data contamination from pre-training memorization, GISA categorizes queries into stable and live subsets. The live subset requires accessing real-time information and is periodically updated, ensuring the benchmark remains challenging and resistant to memorization over time. Process-level supervision via human trajectories: Beyond question-answer pairs, GISA provides complete human search trajectories for every query. These trajectories serve as gold standards for process reward modeling and imitation learning, while also verifying that all tasks are solvable through realistic search behaviors. We construct GISA through rigorous multi-stage annotation process involving question design, answer labeling, and human trajectory collection, with each stage undergoing multiple rounds of quality verification. On average, each query requires over one hour of human effort from initial design to final validation. The final benchmark comprises 373 high-quality queries spanning diverse domains and complexity levels. To facilitate future research, we release comprehensive documentation, evaluation scripts, and public leaderboard open for submission. 2 We evaluate diverse set of state-of-the-art LLMs and commercial search products on GISA. Results show that even the best-performing model achieves only 19.30% overall exact match, with commercial deep research systems struggling to outperform LLM-based agents. These findings underscore the challenging nature of GISA and highlight significant opportunities for advancing general-purpose information-seeking agents."
        },
        {
            "title": "2 Related Work",
            "content": "Search Agent. LLM-driven search agents have evolved beyond early retrieval-augmented generation into complex systems capable of autonomous planning and dynamic decision-making (Zhu et al., 2023; Xi et al., 2025a). To meet diverse information needs, current research employs structures ranging from breadth-oriented parallel search to depth-oriented serial reasoning (Li et al., 2025c; Ko et al., 2025; Li et al., 2025a), while further integrating complex architectures like knowledge graph, Monte Carlo Tree Search (MCTS), and multi-agent collaboration (Lu et al., 2025; Ren et al., 2025; Huang et al., 2025). In terms of optimization, combining Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) significantly enhances agent planning and tool usage in dynamic environments, and drives the emergence of applications centered on web search and research (Lin et al., 2025; Li et al., 2025d; Jin et al., 2025; Li et al., 2025b). Despite these advances, current evaluation systems remain limited by unrealistic task construction, lack of unified assessment for deep reasoning and breadth aggregation, over-reliance on unstable LLM-based scoring, and data contamination in static benchmarks. Consequently, accurately measuring comprehensive agent performance in real-world, dynamic web environments remains challenge. Benchmark for Agentic Search. The rapid advancement of search agents has given rise to variety of benchmarks, as shown in Table 1. Benchmarks including BrowseComp (Wei et al., 2025), InfoDeepSeek (Xi et al., 2025b), and xbenchDeepSearch (Chen et al., 2025) focus on deep search, evaluating multi-hop reasoning and cross-page information integration (Mialon et al., 2024; Xu et al., 2025; Pham et al., 2025; Chen et al., 2025; Zhou et al., 2025c,a,b). However, tasks in these datasets are often reverse-engineered from answers; while challenging, they may deviate from real user queries. Conversely, benchmarks like WideSearch (Wong et al., 2025) emphasize reliability in large-scale atomic information collection and structured organization. While DeepWideSearch (Lan et al., 2025) attempts to evaluate both depth and breadth by combining BrowseComp and WideSearch, it still lacks task-driven construction and the support of real user behavioral trajectories. Moreover, most existing benchmarks rely on static snapshots, making it difficult to discern whether an agent acts based on web information or its own internal knowledge. In comparison, GISA provides realistic, dynamic, and process-interpretable benchmark. To ensure tasks are meaningful and aligned with actual information needs, GISA centers on human-designed tasks covering both deep and wide information seeking. To prevent data contamination, GISA maintains queries involving live and evolving information. Finally, GISA provides complete human search trajectories, enabling fine-grained process analysis and serving as references for training agents to emulate human search strategies."
        },
        {
            "title": "3 Benchmark",
            "content": "In this section, we detail the design and construction process of GISA, followed by the evaluation protocol."
        },
        {
            "title": "3.2 Construction\nThe construction of GISA is a human-centered process designed to ensure that each query is challenging, realistic,\nand strictly aligned with the goal of evaluating real-world information-seeking capabilities. The entire workflow,\nranging from initial question design to final inclusion, is governed by rigorous criteria and multi-stage verification. As",
            "content": "3 Figure 1 Illustration of benchmark construction process and some example queries with various types of answers. illustrated in Figure 1, the construction pipeline comprises four distinct stages: (1) brainstorming, (2) query refinement, (3) human annotation, and (4) quality checking. 3.2.1 Brainstorming To ensure the benchmark covers diverse range of topics that truly reflect human interests, we adopt the taxonomy from BrowseComp (Wei et al., 2025). This taxonomy categorizes information needs into ten distinct domains: TV Shows & Movies, Science & Technology, Art, History, Sports, Music, Video Games, Geography, Politics, and Other. The distribution of topics in GISA is illustrated in Figure 2. Based on these categories, we employ an open-ended browsing strategy to stimulate query generation. Annotators are instructed to visit domain-specific websites, such as news aggregators for Politics or encyclopedic archives for History, and freely explore content. During this process, they are encouraged to document any questions that arise spontaneously from the information they consume. This approach mimics the natural cognitive process of encountering information and seeking further knowledge (Kidd and Hayden, 2015). For instance, as illustrated in Figure 1, an annotator browsing political news might encounter report regarding President Yoon Suk Yeol and the declaration of martial law. This stimulus could trigger specific inquiries, such as What is the definition of martial law? or Are there historical precedents for martial law in South Korea? By systematizing this browsing-inspired workflow across all ten domains, we successfully obtain large collection of seed questions. 3.2.2 Query Refinement After obtaining diverse pool of seed questions, the next step is to transform these raw questions into formal and structured queries. This stage is critical for realizing two core design goals of GISA: enabling deterministic evaluation through structured answer formats, and unifying deep and wide search capabilities within realistic tasks. In this stage, annotators formulate the final query while determining the optimal format of the target answer. We 4 Figure 2 Distribution of topics in GISA. Figure 3 Statistics of human annotation time on GISA. categorize the target answer structures into four types: item, set, list, and table (example queries are shown in the bottom side of Figure 1). This structured design enables deterministic and reproducible evaluation using strict matching metrics, avoiding the subjectivity of LLM-based judgment. If query is compatible with multiple formats, annotators are allowed to select the most appropriate format. Besides, to ensure the uniqueness of search trajectories in the dataset, we adopt strict one-to-one rule: each seed question is used to construct exactly one formal query. Crucially, this formulation process involves preliminary feasibility check that ensures queries require genuine search effort spanning both depth and breadth. Annotators are required to perform quick search to verify that the target answer is not readily available in pre-aggregated form on existing web pages. For example, rather than simply asking for the total count of South Korean presidents, fact often directly displayed in search snippets, we instead request structured table listing all presidents along with whether each declared martial law during their tenure (as illustrated in Figure 1). This task exemplifies the integration of deep and wide search: the agent must first gather basic information about all presidents (broad aggregation), then investigate each presidents historical record to determine martial law declarations (deep reasoning for individual verification). Such queries cannot be answered by retrieving single pre-existing source, but require systematic collection and synthesis across multiple pages. This filtering step eliminates trivial queries and ensures that GISA evaluates agents ability to perform both vertical investigation and horizontal summarization within unified tasks. Furthermore, we address the temporal nature of information needs. Annotators must classify each query as either stable (immutable for at least three years) or live (subject to change). For the latter, we commit to monthly maintenance schedule, updating the ground-truth regularly to ensure the benchmark remains accurate over time. Beyond general formulation, we impose strict constraints for structured output formats to ensure the evaluation is deterministic: (1) For list-type queries, the sorting criterion must be explicitly defined in the prompt (e.g., sorted alphabetically by name) to avoid ambiguity in the output order. (2) For table-type queries, the schema must be fully specified. Annotators must define the exact column names to be retrieved and designate primary column for sorting. To handle duplicate values in the primary column, multi-level sorting criteria (i.e., secondary and tertiary keys) must be provided as tie-breaking rules, applied sequentially until unique row order is guaranteed. 3.2.3 Human Annotation Prior to the official annotation, pilot phase is conducted using five trial questions. This phase serves to train annotators on tool usage and align their outputs with our strict formatting specifications (e.g., ensuring column headers match the table schema, and verifying sorting logic). Feedback is provided iteratively during this phase. To maintain data integrity, these pilot samples and their ground-truths are excluded from the final dataset. To capture the fine-grained decision-making processes of human searchers, we develope custom browser extension. This tool operates in the background to log interaction data, including: (1) search queries issued to Google; (2) content of search engine results pages; (3) click-through behaviors; and (4) precise timestamps for duration analysis. The annotation workflow follows standardized protocol: (1) Initialization: The annotator activates the extension to begin session for specific query. 5 Table 2 Statistics of GISA. The input length is measured by the average number of tokens, while the output length is measured by the average number of items. Answer Type # Stable # Live Input Len Output Len Item Set List Table 12 33 30 148 10 17 18 105 42.96 43.40 54.23 98. 1.00 16.90 13.67 45.98 (2) Search and restrictions: Annotators are restricted to using Google Search as their retrieval tool.2 While they may navigate freely through resulting web links, the use of other search engines or LLMs is prohibited. (3) Exception handling: In cases where the target information is inaccessible, or where sources exhibit irreconcilable conflicts, annotators are instructed to document the specific issue and skip the query. These flagged queries are collected and returned to the query refinement stage for revision. (4) Answer construction: Annotators use spreadsheet software to organize intermediate findings and format the final answer. (5) Submission: Upon completion, the final answer is saved as CSV file, and the extension exports the search trajectory as JSON log. Finally, we employ post-processing script to parse these logs into the standardized trajectory format (as shown in Appendix A). We present the statistics of human annotation time on GISA in Figure 3. 3.2.4 Quality Checking In the final phase, the triplet of query, search trajectory, and answer is distributed to dedicated team of verifiers. This stage involves three rigorous validation steps: (1) Verifiers first inspect the consistency of the recorded search logs. They screen for two common issues: (a) missing initial queries, where the log fails to capture the start of the session because the annotator performs the first search before activating the tool; and (b) noise injection, where irrelevant queries or browsing behaviors are recorded because the annotator fails to terminate the session immediately after the task. Trajectories exhibiting these issues are flagged for re-annotation. (2) Upon clearing the trajectory check, verifiers evaluate the correctness of the final answer. They are granted the autonomy to conduct independent web searches, referencing the recorded trajectory at their discretion, to validate the facts. Beyond factual accuracy, verifiers strictly enforce compliance with the pre-defined format (e.g., specific table schema) and sorting constraints. If errors are identified (e.g., citation failures, missing entries, or hallucinations), the answer undergoes revision. critical constraint is applied during this correction: the revised answer must be fully deducible from the information present in the original search trajectory. If the original trajectory is insufficient to support the corrected answer, the instance is deemed invalid and discarded, triggering re-annotation of the query. Finally, to ensure that the benchmark evaluates search capabilities rather than the parametric memory of LLMs, we conduct memorization check using the DeepSeek-V3.2 model (DeepSeek-AI, 2025). We feed the queries into the model with its reasoning and web search capabilities disabled. If the model can answer query perfectly using only its internal knowledge, the query is considered solved by current pre-training data and is consequently excluded from the dataset. This ensures that GISA focuses exclusively on tasks requiring external information retrieval. After filtering, we obtain final set of 373 queries, with detailed statistics reported in Table 2. Throughout the annotation process, we recruit 15 graduate students specializing in information retrieval as expert annotators. Their domain knowledge ensures proficiency in formulating effective search strategies and synthesizing information from multiple sources. The entire pipeline, from initial question design to final answer validation, requires 2Given that Google Search may occasionally trigger automatic AI-generated summaries, annotators are explicitly instructed to ignore these modules. They are strictly prohibited from viewing or utilizing any information provided by these built-in LLMs. This constraint ensures that the recorded trajectories reflect complete and organic human information-seeking process rather than the passive consumption of AI-synthesized answers. 6 over one hour per query on average, reflecting the complexity and rigor of our annotation process. Figure 3 provides detailed statistics of the time spent on the human annotation stage (i.e., trajectory collection and answer labeling)."
        },
        {
            "title": "3.3 Evaluation",
            "content": "3.3.1 Data Preprocessing To facilitate automated evaluation, we incorporate specific output format constraints within the agents instructions, requiring the final results to be encapsulated in TSV format within Markdown code blocks. We employ regular expressions to target and extract content from these blocks in the agents final response. To ensure parsing robustness, the extraction pipeline automatically filters out redundant empty lines and handles cases where the code block may be missing by treating the entire response as raw content for fallback processing. Both the extracted predictions and the ground-truth answers undergo uniform normalization procedure to eliminate the impact of formatting discrepancies. This process begins with standardizing column headers through lowercase conversion and whitespace removal. For cell values, we implement multi-tiered cleaning logic: numeric entries are stripped of currency symbols, commas, and percentage signs, with the latter being converted to decimal equivalents. These values are then represented as strings, where integers are formatted directly and floating-point numbers are rounded to maximum of six decimal places with trailing zeros removed. Additionally, string-based entries are normalized to lowercase and stripped of interior spaces, newlines, and Markdown artifacts such as asterisks. Finally, all null or missing values are represented as empty strings, ensuring consistent and objective comparison at the cell level. 3.3.2 Metrics Since our benchmark contains various types of answers (i.e., item, set, list, and table), we employ specific metrics tailored to each type to comprehensively evaluate the performance. Universal Metric: Exact Match First, we apply exact match (EM) as the strictest evaluation metric across all answer types. This metric assigns score of 1 only if the generated result is completely identical to the ground truth; otherwise, the score is 0. Metrics for Specific Answer Types Depending on the answer type, we use additional metrics to capture different aspects of the models performance: (1) Set-type: For answers where the output is collection of items but the order does not matter, we use the F1 score to evaluate the overlap between the predicted set and the ground-truth set, allowing us to assess both precision and recall. (2) List-type: For ordered lists, both the content accuracy and the sequence of items are crucial. We initially consider using ranking-biased overlap (RBO) (Webber et al., 2010) to evaluate ranking quality. However, RBO cannot properly handle cases where the generated results contain duplicate items, which occasionally occurs in our task. Therefore, we adopt two-metric approach that separately evaluates content accuracy and order accuracy. For content accuracy, we still use the standard F1 score, which measures the precision and recall of items in the predicted list compared to the ground truth, regardless of their order. For order accuracy, we employ the Sequence Matcher algorithm, which computes an order-aware similarity score.3 Given ground-truth list and predicted list, the order score is calculated as: Order Score = 2M /T , where represents the number of matching elements (considering both content and position), and is the total number of elements in both lists. The matching process identifies the longest contiguous matching subsequences and recursively applies to remaining portions. The order score ranges from 0 to 1, where 1 indicates perfect match in both content and order. (3) Table-type: Following the protocols of previous wide search benchmarks (Wong et al., 2025), we evaluate tabular data at two granularities. We use row-level F1 to measure the accuracy of entire rows and item-level F1 to assess the correctness of individual cell values. 3difflib.SequenceMatcher, https://docs.python.org/3/library/difflib.html"
        },
        {
            "title": "4.1 Baseline Methods & Settings\nWe evaluate two paradigms of search agents on GISA: ReAct-based LLM Agents and Commercial Agent Systems.\nTo ensure fair comparison, all agents receive prompts containing identical task descriptions, formatting constraints,\ncurrent date information, and target queries. ReAct-based agents are additionally provided with tool-use specifications\nfollowing the standard ReAct (Yao et al., 2023) workflow.",
            "content": "(1) ReAct-based LLM Agents. We equip these agents with two core tools: Search and Browse, powered by the Google Serper API and Jina API, respectively.4 Following previous studies (Li et al., 2025b,d), content retrieved via Browse is summarized before being returned to the model to minimize noise. For this summarization task, we use the same underlying LLM as the agent, operating in non-thinking mode (when applicable). To leverage the inherent capabilities of each model, we utilize their native function-calling interfaces to construct the agents. For models supporting thinking mode, we employ an interleaved reasoning approach, allowing the model to perform multiple tool calls within continuous thinking path. We set maximum of 30 tool invocations per task, 8,192 output tokens per step, and disable parallel function calling for fair comparison. retry strategy is implemented to ensure execution stability. Our evaluation covers diverse suite of state-of-the-art models: Qwen3-235B-A22B (Yang et al., 2025), Claude 4.5 Sonnet, Gemini 3 Pro, GPT-5.2, DeepSeek-V3.2 (DeepSeek-AI, 2025), GLM-4.7 (Zeng et al., 2025), Seed-1.8, Qwen3-Max (Yang et al., 2025), and Kimi K2.5. Unless otherwise specified, the native thinking mode is enabled by default with the maximum thinking budget allocated. We utilize official APIs for DeepSeek-V3.2, Qwen-Max, and Kimi K2.5, while all other models are accessed via the OpenRouter platform.5 (2) Commercial Agent Systems. In addition to custom implementations, we evaluate closed-source commercial systems to establish baseline for industrial-grade performance. These include DeepSearch systems (GPT-4o Search Preview, Perplexity Sonar Pro Search, and Google Search AI Mode) and DeepResearch systems (OpenAI o4-mini DeepResearch). Google Search AI Mode results are manually collected and converted to CSV format; all other systems are accessed via OpenRouter APIs. More implementation details can be found at Appendix B."
        },
        {
            "title": "4.2 Main Results\nTable 3 presents the performance of various LLM-based agents and commercial search systems on GISA. We highlight\nseveral key findings from our evaluation.",
            "content": "(1) Significant room for improvement remains. Even the best-performing model, Claude 4.5 Sonnet (thinking), achieves only 19.30% overall EM score, indicating that current search agents are far from solving complex information-seeking tasks reliably. Through manual inspection of search trajectories, we identify several recurring failure modes: (a) limited problem decomposition capabilities, where agents fail to devise effective search plans for complex queries; (b) insufficient self-correction abilities, where agents struggle to adjust strategies based on intermediate search results; and (c) inadequate web traversal skills, where agents fail to exploit hyperlinks within pages, leading to incomplete information gathering. (2) Task complexity scales with information breadth. Performance degrades substantially as the required information scope increases. Agents perform reasonably well on item-type questions but struggle significantly on table-type tasks, which demand collecting and organizing information across multiple dimensions. Notably, in GISA, the amount of information to be gathered does not necessarily correlate with question difficulty, i.e., an item-type question may still require extensive search before arriving at the correct answer. The pronounced performance drop on structured answer formats (lists and tables) suggests that current models face challenges not only in information collection but also in answer organization and formatting. (3) Efficient tool usage matters. Interestingly, the best-performing Claude 4.5 Sonnet model uses tools quite efficiently, with moderate numbers of search and browse calls compared to other models. In contrast, models like DeepSeek-V3.2 and GLM-4.7 invoke substantially more tools yet achieve lower scores. This suggests that excessive tool usage does 4Serper: https://serper.dev/, Jina: https://jina.ai/reader 5OpenRouter: https://openrouter.ai/ 8 Table 3 Experimental results of all methods on GISA. The overall score is calculated by the weighted average of all EM scores. Tool calling number with is provided by the API service provider. Model / System Item Set List Table Overall Avg. # Tool Calls EM EM F1 EM F1 Order EM Row-F1 Item-F1 EM Search Browse 4.35 9.49 LLM-based ReAct Agents Qwen3-235B-A22B (thinking) 40.91 18.00 52.37 14.58 36.48 35.96 Claude 4.5 Sonnet (non-thinking) 59.09 26.00 60.87 22.92 58.76 57.78 Claude 4.5 Sonnet (thinking) Gemini 3 Pro (low) Gemini 3 Pro (high) GPT-5.2 (thinking) DeepSeek-V3.2 (non-thinking) DeepSeek-V3.2 (thinking) GLM-4.7 (thinking) Seed-1.8 (thinking) Qwen3-Max (thinking) Kimi K2.5 (thinking) 28.32 47.85 63.64 28.00 64.86 22.92 59.24 56.42 13.04 49.92 7.11 45.93 45.45 28.00 63.82 27.08 57.55 56.37 8.70 47.01 50.00 22.00 62.66 27.08 60.87 60.12 9.49 43.04 63.64 26.00 62.70 16.67 54.11 53.17 6.72 44.14 22.73 20.00 52.00 22.92 56.02 55.45 6.32 43.44 63.64 28.00 60.79 20.83 62.25 60.41 8.30 43.97 50.00 22.00 59.44 20.83 51.99 50.97 38.49 45.45 32.00 56.77 16.67 56.11 53.54 6.32 48.48 59.09 30.00 63.45 25.00 66.51 64.08 10.67 45.19 7.91 68.18 28.00 61.71 18.75 50.52 48. GPT-4o Search Preview OpenAI o4 Mini Deep Research Perplexity Sonar Pro Search Google Search AI Mode 4.00 Commercial Agent Systems 38.70 13.64 36.65 36.00 18.18 14.00 63.03 18.75 53.72 52.59 34.74 33.16 22.73 20.00 47.04 40.64 39.36 31.82 20.00 46.34 6.25 8.33 8. 4.74 3.56 3.95 5.53 29.59 36.78 34.76 31.15 43.93 63.71 65.17 64.93 66.02 60.20 62.24 62.42 61.28 57.13 66.86 61.23 45.61 56.47 49.05 50.79 9.65 16. 19.30 14.74 15.28 15.82 11.53 14.47 14.21 13.40 17.96 15.55 2.16 10.11 7.57 9.13 11.87 8.14 12.62 12.14 15.75 7.94 11.50 13.02 4.03 5.67 4.63 6.69 5.79 13.78 10.18 12.35 12.26 4.14 8.94 7.89 5.63 7.78 7.51 9.38 1.00 69.88 - - - - - - not necessarily improve performance; rather, noise from irrelevant retrieved content and increased context length may negatively impact reasoning quality. (4) Reasoning mode provides consistent gains. Comparing thinking and non-thinking variants of the same model family reveals clear benefits from extended inference-time computation. Claude 4.5 Sonnet improves from 16.36% to 19.30% overall EM, and DeepSeek-V3.2 improves from 11.53% to 14.47% when thinking mode is enabled. However, these gains come at the cost of significantly higher token consumption, presenting trade-off between performance and efficiency. (5) Commercial search systems underperform. Surprisingly, commercial deep research and search products do not outperform LLM-based ReAct agents. GPT-4o Search Preview, which performs only single retrieval per query, is clearly insufficient for the complex tasks in GISA. OpenAI o4 Mini Deep Research and Perplexity Sonar Pro Search, despite their sophisticated pipelines, suffer from poor instruction-following, resulting in numerous formatting errors in their final answers. Google Search AI Mode demonstrates relatively better performance among commercial systems; while it lags behind LLM-based agents in accuracy, it offers substantial advantages in response speed, positioning itself closer to traditional search engine experiences."
        },
        {
            "title": "4.3 Further Analysis",
            "content": "We conduct further analyses to better understand model behavior and identify potential directions for improvement. Comparison with Human Search Behavior. To better understand the gap between model and human search strategies, we compare the behavioral patterns of Claude 4.5 Sonnet (thinking) with human annotations, analyzing the average number of search queries and browsed pages, query refinement patterns, and the correlation between human-model similarity and task performance (detailed metrics for all models are provided in Appendix C). We observe that humans and models exhibit different strategies: humans issue fewer queries (3.53 on average) but browse substantially more pages (19.03), whereas models search more frequently (7.57 queries) but browse far fewer pages (4.63). This suggests that humans favor thorough exploration within search results, while models rely on repeated querying rather than deep examination of retrieved content. Additionally, humans show higher adjacent query overlap (0.31 vs. 0.22), indicating more targeted query refinement, whereas models tend to construct less related queries between consecutive searches. We also find positive correlation between human-model behavioral similarity and task performance: the high-similarity group achieves an average F1 of 0.76, compared to 0.56 for the low-similarity group, and successfully 9 Figure 4 Performance of Kimi K2.5 (thinking) and Claude 4.5 (thinking) on different subsets of GISA. Figure 5 Inference-time scaling performance of Qwen3-Max over 40 random samples. solved cases exhibit higher URL overlap rates (0.31 vs. 0.15). These findings suggest that aligning model behavior more closely with human search strategies, particularly in terms of deeper content exploration and more coherent query refinement, may be promising direction for improving agent performance. Performance Difference across Question Types. We analyze model performance across stable and live subsets to explore the potential impact of data contamination. Intriguingly, Kimi K2.5, the most recently released (Jan. 28, 2026) model in our evaluation, achieves notably lower performance on the live subset compared to the stable subset (11.33% vs. 18.39% overall EM), while other models (Claude 4.5 Sonnet) show no significant difference. We hypothesize that this difference arises because Kimi K2.5s training data, being the most recent, is more likely to contain answers to stable questions. In contrast, older models may not have memorized either subset, making their performance uniformly dependent on search capabilities. This finding validates the design of our live subset: as models are trained on increasingly current data, static benchmarks risk measuring memorization rather than search ability. The live subset, with its periodically updated answers, provides more robust evaluation that remains resistant to such contamination over time. Inference-time Scaling. Recent research has shown that allocating more computational resources at inference time can effectively improve model performance (DeepSeek-AI, 2025; Wong et al., 2025; Wei et al., 2025). To investigate whether search agents benefit from such scaling, we conduct experiments on 40 randomly sampled queries using Qwen3-Max, generating independent runs per query (k {1, 2, 4, 8, 16}). We report Best@k (whether any attempt succeeds) and Majority@k (confidence-weighted voting). As shown in Figure 5, both metrics improve consistently as increases. Best@k rises from 8.90% to 22.22% at k=16, 2.5 improvement, suggesting that models have potential capabilities not reliably activated in single attempts. Majority@k also improves but consistently lags behind Best@k, indicating that selecting the correct answer from multiple candidates remains challenging. These results demonstrate that inference-time scaling offers promising direction for improving search agents, while also highlighting the need for better answer verification mechanisms. Error Analysis. To gain deeper insight into the failure modes of current search agents, we manually analyze 50 error cases from the best-performing model (Claude 4.5 Sonnet with thinking). As shown in Figure 6, we categorize the identified errors into three levels based on where they occur in the search pipeline (noting that single sample may exhibit multiple error types). (1) At the comprehension level, query misunderstanding accounts for only 3.2% of errors, indicating that current LLMs possess strong semantic understanding capabilities. (2) The majority of failures occur at the search level, comprising 49.2% of all errors. These include inability to formulate effective queries (14.3%), failure to exploit hyperlinks within pages for deeper exploration (17.5%), 10 Figure 6 Illustration of different error types and their ratios. Table 4 Average cost per question for each model on GISA. The input and output token counts reflect only the main reasoning chain, while tokens consumed by the browsing tool are reported separately. Prices are reported in US dollars per million tokens, with RMB converted at rate of 7:1. Model # Input # Output # Tool Input # Tool Output Input Price Output Price Average Cost 162888.63 46399.86 Qwen3-235B-A22B (thinking) 7185.19 Claude 4.5 Sonnet (non-thinking) 321680.56 267248.29 Claude 4.5 Sonnet (thinking) 7619.70 242960.98 27886.18 Gemini 3 Pro (low) 292792.10 35536.54 Gemini 3 Pro (high) 471289.43 20408.00 GPT-5.2 (thinking) 499634.20 11204.50 DeepSeek-V3.2 (non-thinking) 594672.30 16671.11 DeepSeek-V3.2 (thinking) 567243.06 45994.95 GLM-4.7 (thinking) 358530.71 15270.81 Seed-1.8 (thinking) 444898.78 15354.54 Qwen3-Max (thinking) 416011.44 14367.39 Kimi K2.5 (thinking) 134491.03 156620.54 133077.07 134920.32 138258.97 234744.65 222052.20 256626.42 278159.63 216757.47 199051.86 183547.81 32838.69 5173.38 4308.98 6702.21 7145.17 14354.61 7938.16 10004.80 41877.75 10905.33 10042.04 10318.11 0.29 3.00 3.00 2.00 2.00 1.75 0.29 0.29 0.29 0.11 0.36 0. 2.86 15.00 15.00 12.00 12.00 14.00 0.43 0.43 1.14 1.14 1.43 3.00 0.31 1.62 1.37 1.17 1.37 1.72 0.22 0.26 0.34 0.10 0.27 0.42 and inability to initiate verification queries when encountering conflicting information across sources (17.5%). These errors suggests that current agents lack effective strategies for thorough web exploration and information verification. (3) At the output level, we observe failures in accurate information extraction from retrieved pages (15.9%) and instruction-following errors (31.7%), such as generating incorrect table headers. Notably, instruction-following errors constitute the largest single category, which is consistent with our main finding that commercial systems often struggle with output formatting. Cost Analysis. Table 4 presents the average token consumption and cost per query for each model evaluated on GISA. We report the number of input and output tokens in the main reasoning chain, tokens consumed by the browsing tool, official API prices, and the resulting average cost per query. Based on the results, we have several observations. First, Claude 4.5 Sonnet, despite achieving the best performance, requires relatively high costs ($1.37$1.62 per query) due to its premium pricing ($3.00/$15.00 per million input/output tokens). However, Claude demonstrates remarkable efficiency in token usage, consuming significantly fewer tokens than most other models. Interestingly, the thinking mode variant is both more effective and cheaper than its non-thinking counterpart, as it uses tools more efficiently and generates fewer total tokens. Second, Chinese models (DeepSeek, GLM, Seed, Qwen, and Kimi) offer substantially lower costs, ranging from $0.10 to $0.42 per query. Among these, Seed-1.8 achieves the lowest cost at $0.10 per query, while DeepSeek-V3.2 provides an attractive balance between cost ($0.26) and performance. Third, we observe that token consumption does not necessarily correlate with performance. Models like DeepSeek-V3.2 and GLM-4.7 consume the most tokens (over 500K input tokens per query) yet do not achieve top performance, suggesting that effective tool usage and reasoning quality matter more than sheer volume of computation. Finally, GPT-5.2 incurs the highest average cost ($1.72) due to both high token consumption and relatively expensive pricing, without delivering proportionally better results. These findings highlight the importance of considering cost-effectiveness when deploying search agents in practice, as the most expensive models are not always the best performing."
        },
        {
            "title": "5 Limitations",
            "content": "While GISA provides comprehensive benchmark for evaluating information-seeking agents, we acknowledge several limitations. First, due to the complexity of the annotation process, which requires substantial human involvement at every stage, the scale of our benchmark remains relatively limited (373 queries). Although sufficient for evaluation purposes, this size may not support large-scale model training such as supervised fine-tuning. Second, GISA currently focuses exclusively on text-based information seeking and does not incorporate multimodal content such as images or videos. As search agents evolve to integrate visual capabilities from GUI agents, future benchmarks should consider evaluating agents ability to process and extract information from multimodal web content, which would better simulate real-world human information gathering. Third, due to cost constraints, we set maximum of 30 tool invocations per query in our experiments. While this limit is sufficient for most tasks, we observed that small number of cases failed to collect adequate information due to this constraint. Future work could explore more flexible 11 resource allocation strategies."
        },
        {
            "title": "6 Ethical Considerations",
            "content": "We strictly adhere to ethical standards throughout the construction of GISA. For annotation tool development, our browser extension is engineered to record only task-relevant interactions within specific search sessions, with no personally identifiable information, cookies, or extra browsing history collected. All annotators participated voluntarily with informed consent. To mitigate potential biases, we intentionally diversified query domains, though we acknowledge that the benchmark may still underrepresent certain languages or specialized domains. Regarding potential misuse, GISA focuses exclusively on publicly available information and does not include queries related to sensitive personal data or harmful content."
        },
        {
            "title": "7 Conclusion",
            "content": "This paper introduces GISA, benchmark for evaluating general information-seeking agents, comprising 373 humancrafted queries with four structured answer formats (item, set, list, and table). GISA addresses key limitations of existing benchmarks by unifying the evaluation of deep reasoning and broad information aggregation, enabling deterministic evaluation through structured answers, incorporating live subset with periodically updated answers to resist data contamination, and providing complete human search trajectories for process-level supervision. Our evaluation of mainstream LLMs and commercial search products reveals that current systems still fall substantially short of human performance, with even the best model achieving only 18.23% overall exact match. These results highlight significant room for improvement in problem decomposition, adaptive search planning, and efficient tool utilization, and we hope GISA will serve as valuable resource for developing more capable information-seeking agents."
        },
        {
            "title": "8 Acknowledgments",
            "content": "This work was supported by the National Natural Science Foundation of China (Grant No. 62402497) and the China Postdoctoral Science Foundation under Grant Number 2025T180440. The authors would like to thank Jinghan Yang, Zhao Wang, Binyu Xie, Shangze Li, Yifei Chen, Zhixin Lin, and Yuyao Zhang for data annotations."
        },
        {
            "title": "References",
            "content": "Kaiyuan Chen, Yixin Ren, Yang Liu, Xiaobo Hu, Haotong Tian, Tianbao Xie, Fangfu Liu, Haoye Zhang, Hongzhang Liu, Yuan Gong, Chen Sun, Han Hou, Hui Yang, James Pan, Jianan Lou, Jiayi Mao, Jizheng Liu, Jinpeng Li, Kangyi Liu, Kenkun Liu, Rui Wang, Run Li, Tong Niu, Wenlong Zhang, Wenqi Yan, Xuanzheng Wang, Yuchen Zhang, Yi-Hsin Hung, Yuan Jiang, Zexuan Liu, Zihan Yin, Zijian Ma, and Zhiwen Mo. xbench: Tracking agents productivity scaling with profession-aligned real-world evaluations. CoRR, abs/2506.13651, 2025. doi: 10.48550/ARXIV.2506.13651. https://doi.org/10.48550/arXiv.2506.13651. DeepSeek-AI. Deepseek-v3.2: Pushing the frontier of open large language models. CoRR, abs/2512.02556, 2025. doi: 10.48550/ ARXIV.2512.02556. https://doi.org/10.48550/arXiv.2512.02556. Shanghua Gao, Ada Fang, Yepeng Huang, Valentina Giunchiglia, Ayush Noori, Jonathan Richard Schwarz, Yasha Ektefaie, Jovana Kondic, and Marinka Zitnik. Empowering biomedical discovery with AI agents. CoRR, abs/2404.02831, 2024. doi: 10.48550/ ARXIV.2404.02831. https://doi.org/10.48550/arXiv.2404.02831. Lisheng Huang, Yichen Liu, Jinhao Jiang, Rongxiang Zhang, Jiahao Yan, Junyi Li, and Wayne Xin Zhao. Manusearch: Democratizing deep search in large language models with transparent and open multi-agent framework. CoRR, abs/2505.18105, 2025. doi: 10.48550/ARXIV.2505.18105. https://doi.org/10.48550/arXiv.2505.18105. Bowen Jin, Hansi Zeng, Zhenrui Yue, Dong Wang, Hamed Zamani, and Jiawei Han. Search-r1: Training llms to reason and leverage search engines with reinforcement learning. CoRR, abs/2503.09516, 2025. doi: 10.48550/ARXIV.2503.09516. https: //doi.org/10.48550/arXiv.2503.09516. Celeste Kidd and Benjamin Y. Hayden. The psychology and neuroscience of curiosity. Neuron, 88(3):449460, November 2015. ISSN 0896-6273. doi: 10.1016/j.neuron.2015.09.010. http://dx.doi.org/10.1016/j.neuron.2015.09.010. 12 Dayoon Ko, Jihyuk Kim, Haeju Park, Sohyeon Kim, Dahyun Lee, Yongrae Jo, Gunhee Kim, Moontae Lee, and Kyungjae Lee. Hybrid deep searcher: Integrating parallel and sequential search reasoning. CoRR, abs/2508.19113, 2025. doi: 10.48550/ARXIV.2508.19113. https://doi.org/10.48550/arXiv.2508.19113. Tian Lan, Bin Zhu, Qianghuai Jia, Junyang Ren, Haijun Li, Longyue Wang, Zhao Xu, Weihua Luo, and Kaifu Zhang. Deepwidesearch: Benchmarking depth and width in agentic information seeking. CoRR, abs/2510.20168, 2025. doi: 10.48550/ARXIV.2510.20168. https://doi.org/10.48550/arXiv.2510.20168. Baixuan Li, Dingchu Zhang, Jialong Wu, Wenbiao Yin, Zhengwei Tao, Yida Zhao, Liwen Zhang, Haiyang Shen, Runnan Fang, Pengjun Xie, Jingren Zhou, and Yong Jiang. Parallelmuse: Agentic parallel thinking for deep information seeking. CoRR, abs/2510.24698, 2025a. doi: 10.48550/ARXIV.2510.24698. https://doi.org/10.48550/arXiv.2510.24698. Kuan Li, Zhongwang Zhang, Huifeng Yin, Liwen Zhang, Litu Ou, Jialong Wu, Wenbiao Yin, Baixuan Li, Zhengwei Tao, Xinyu Wang, Weizhou Shen, Junkai Zhang, Dingchu Zhang, Xixi Wu, Yong Jiang, Ming Yan, Pengjun Xie, Fei Huang, and Jingren Zhou. Websailor: Navigating super-human reasoning for web agent. CoRR, abs/2507.02592, 2025b. doi: 10.48550/ARXIV.2507.02592. https://doi.org/10.48550/arXiv.2507.02592. Xiaoxi Li, Guanting Dong, Jiajie Jin, Yuyao Zhang, Yujia Zhou, Yutao Zhu, Peitian Zhang, and Zhicheng Dou. Search-o1: Agentic search-enhanced large reasoning models. CoRR, abs/2501.05366, 2025c. doi: 10.48550/ARXIV.2501.05366. https: //doi.org/10.48550/arXiv.2501.05366. Xiaoxi Li, Jiajie Jin, Guanting Dong, Hongjin Qian, Yutao Zhu, Yongkang Wu, Ji-Rong Wen, and Zhicheng Dou. Webthinker: Empowering large reasoning models with deep research capability. CoRR, abs/2504.21776, 2025d. doi: 10.48550/ARXIV.2504.21776. https://doi.org/10.48550/arXiv.2504.21776. Minhua Lin, Zongyu Wu, Zhichao Xu, Hui Liu, Xianfeng Tang, Qi He, Charu C. Aggarwal, Hui Liu, Xiang Zhang, and Suhang Wang. comprehensive survey on reinforcement learning-based agentic search: Foundations, roles, optimizations, evaluations, and applications. CoRR, abs/2510.16724, 2025. doi: 10.48550/ARXIV.2510.16724. https://doi.org/10.48550/arXiv.2510.16724. Rui Lu, Zhenyu Hou, Zihan Wang, Hanchen Zhang, Xiao Liu, Yujiang Li, Shi Feng, Jie Tang, and Yuxiao Dong. Deepdive: Advancing deep search agents with knowledge graphs and multi-turn RL. CoRR, abs/2509.10446, 2025. doi: 10.48550/ARXIV.2509.10446. https://doi.org/10.48550/arXiv.2509.10446. Grgoire Mialon, Clmentine Fourrier, Thomas Wolf, Yann LeCun, and Thomas Scialom. GAIA: benchmark for general AI assistants. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. https://openreview.net/forum?id=fibxvahvs3. Joon Sung Park, Joseph C. OBrien, Carrie Jun Cai, Meredith Ringel Morris, Percy Liang, and Michael S. Bernstein. Generative agents: Interactive simulacra of human behavior. In Sean Follmer, Jeff Han, Jrgen Steimle, and Nathalie Henry Riche, editors, Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology, UIST 2023, San Francisco, CA, USA, 29 October 20231 November 2023, pages 2:12:22. ACM, 2023. doi: 10.1145/3586183.3606763. https://doi.org/10.1145/3586183.3606763. Thinh Pham, Nguyen Nguyen, Pratibha Zunjare, Weiyuan Chen, Yu-Min Tseng, and Tu Vu. Sealqa: Raising the bar for reasoning in search-augmented language models. CoRR, abs/2506.01062, 2025. doi: 10.48550/ARXIV.2506.01062. https://doi.org/10.48550/ arXiv.2506.01062. Chen Qian, Wei Liu, Hongzhang Liu, Nuo Chen, Yufan Dang, Jiahao Li, Cheng Yang, Weize Chen, Yusheng Su, Xin Cong, Juyuan Xu, Dahai Li, Zhiyuan Liu, and Maosong Sun. Chatdev: Communicative agents for software development. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024, pages 1517415186. Association for Computational Linguistics, 2024. doi: 10.18653/V1/2024.ACL-LONG.810. https://doi.org/10.18653/v1/2024.acl-long.810. Ruiyang Ren, Yuhao Wang, Junyi Li, Jinhao Jiang, Wayne Xin Zhao, Wenjie Wang, and Tat-Seng Chua. Holistically guided monte carlo tree search for intricate information seeking. CoRR, abs/2502.04751, 2025. doi: 10.48550/ARXIV.2502.04751. https://doi.org/10.48550/arXiv.2502.04751. William Webber, Alistair Moffat, and Justin Zobel. similarity measure for indefinite rankings. ACM Trans. Inf. Syst., 28(4): 20:120:38, 2010. doi: 10.1145/1852102.1852106. https://doi.org/10.1145/1852102.1852106. Jason Wei, Zhiqing Sun, Spencer Papay, Scott McKinney, Jeffrey Han, Isa Fulford, Hyung Won Chung, Alex Tachard Passos, William Fedus, and Amelia Glaese. Browsecomp: simple yet challenging benchmark for browsing agents. CoRR, abs/2504.12516, 2025. doi: 10.48550/ARXIV.2504.12516. https://doi.org/10.48550/arXiv.2504.12516. Ryan Wong, Jiawei Wang, Junjie Zhao, Li Chen, Yan Gao, Long Zhang, Xuan Zhou, Zuo Wang, Kai Xiang, Ge Zhang, Wenhao Huang, Yang Wang, and Ke Wang. Widesearch: Benchmarking agentic broad info-seeking. CoRR, abs/2508.07999, 2025. doi: 10.48550/ARXIV.2508.07999. https://doi.org/10.48550/arXiv.2508.07999. 13 Jialong Wu, Wenbiao Yin, Yong Jiang, Zhenglin Wang, Zekun Xi, Runnan Fang, Linhai Zhang, Yulan He, Deyu Zhou, Pengjun Xie, and Fei Huang. Webwalker: Benchmarking llms in web traversal. In Wanxiang Che, Joyce Nabende, Ekaterina Shutova, and Mohammad Taher Pilehvar, editors, Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2025, Vienna, Austria, July 27 - August 1, 2025, pages 1029010305. Association for Computational Linguistics, 2025. https://aclanthology.org/2025.acl-long.508/. Yunjia Xi, Jianghao Lin, Yongzhao Xiao, Zheli Zhou, Rong Shan, Te Gao, Jiachen Zhu, Weiwen Liu, Yong Yu, and Weinan Zhang. survey of llm-based deep search agents: Paradigm, optimization, evaluation, and challenges. CoRR, abs/2508.05668, 2025a. doi: 10.48550/ARXIV.2508.05668. https://doi.org/10.48550/arXiv.2508.05668. Yunjia Xi, Jianghao Lin, Menghui Zhu, Yongzhao Xiao, Zhuoying Ou, Jiaqi Liu, Tong Wan, Bo Chen, Weiwen Liu, Yasheng Wang, Ruiming Tang, Weinan Zhang, and Yong Yu. Infodeepseek: Benchmarking agentic information seeking for retrieval-augmented generation. CoRR, abs/2505.15872, 2025b. doi: 10.48550/ARXIV.2505.15872. https://doi.org/10.48550/arXiv.2505.15872. Yilong Xu, Xiang Long, Zhi Zheng, and Jinhua Gao. Ravine: Reality-aligned evaluation for agentic search. CoRR, abs/2507.16725, 2025. doi: 10.48550/ARXIV.2507.16725. https://doi.org/10.48550/arXiv.2507.16725. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao Ge, Haoran Wei, Huan Lin, Jialong Tang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jian Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keqin Bao, Kexin Yang, Le Yu, Lianghao Deng, Mei Li, Mingfeng Xue, Mingze Li, Pei Zhang, Peng Wang, Qin Zhu, Rui Men, Ruize Gao, Shixuan Liu, Shuang Luo, Tianhao Li, Tianyi Tang, Wenbiao Yin, Xingzhang Ren, Xinyu Wang, Xinyu Zhang, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yinger Zhang, Yu Wan, Yuqiong Liu, Zekun Wang, Zeyu Cui, Zhenru Zhang, Zhipeng Zhou, and Zihan Qiu. Qwen3 technical report. CoRR, abs/2505.09388, 2025. doi: 10.48550/ARXIV.2505.09388. https://doi.org/10.48550/arXiv.2505.09388. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik R. Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023. https://openreview.net/forum?id=WE_vluYUL-X. Yangyang Yu, Zhiyuan Yao, Haohang Li, Zhiyang Deng, Yuechen Jiang, Yupeng Cao, Zhi Chen, Jordan W. Suchow, Zhenyu Cui, Rong Liu, Zhaozhuo Xu, Denghui Zhang, Koduvayur Subbalakshmi, Guojun Xiong, Yueru He, Jimin Huang, Dong Li, and Qianqian Xie. Fincon: synthesized LLM multi-agent system with conceptual verbal reinforcement for enhanced financial decision making. In Amir Globersons, Lester Mackey, Danielle Belgrave, Angela Fan, Ulrich Paquet, Jakub M. Tomczak, and Cheng Zhang, editors, Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024, 2024. http://papers.nips.cc/paper_files/paper/2024/ hash/f7ae4fe91d96f50abc2211f09b6a7e49-Abstract-Conference.html. Aohan Zeng, Xin Lv, Qinkai Zheng, Zhenyu Hou, Bin Chen, Chengxing Xie, Cunxiang Wang, Da Yin, Hao Zeng, Jiajie Zhang, Kedong Wang, Lucen Zhong, Mingdao Liu, Rui Lu, Shulin Cao, Xiaohan Zhang, Xuancheng Huang, Yao Wei, Yean Cheng, Yifan An, Yilin Niu, Yuanhao Wen, Yushi Bai, Zhengxiao Du, Zihan Wang, Zilin Zhu, Bohan Zhang, Bosi Wen, Bowen Wu, Bowen Xu, Can Huang, Casey Zhao, Changpeng Cai, Chao Yu, Chen Li, Chendi Ge, Chenghua Huang, Chenhui Zhang, Chenxi Xu, Chenzheng Zhu, Chuang Li, Congfeng Yin, Daoyan Lin, Dayong Yang, Dazhi Jiang, Ding Ai, Erle Zhu, Fei Wang, Gengzheng Pan, Guo Wang, Hailong Sun, Haitao Li, Haiyang Li, Haiyi Hu, Hanyu Zhang, Hao Peng, Hao Tai, Haoke Zhang, Haoran Wang, Haoyu Yang, He Liu, He Zhao, Hongwei Liu, Hongxi Yan, Huan Liu, Huilong Chen, Ji Li, Jiajing Zhao, Jiamin Ren, Jian Jiao, Jiani Zhao, Jianyang Yan, Jiaqi Wang, Jiayi Gui, Jiayue Zhao, Jie Liu, Jijie Li, Jing Li, Jing Lu, Jingsen Wang, Jingwei Yuan, Jingxuan Li, Jingzhao Du, Jinhua Du, Jinxin Liu, Junkai Zhi, Junli Gao, Ke Wang, Lekang Yang, Liang Xu, Lin Fan, Lindong Wu, Lintao Ding, Lu Wang, Man Zhang, Minghao Li, Minghuan Xu, Mingming Zhao, and Mingshu Zhai. GLM-4.5: agentic, reasoning, and coding (ARC) foundation models. CoRR, abs/2508.06471, 2025. doi: 10.48550/ARXIV.2508.06471. https://doi.org/10.48550/arXiv.2508.06471. Heng Zhou, Ao Yu, Yuchen Fan, Jianing Shi, Li Kang, Hejia Geng, Yongting Zhang, Yutao Fan, Yuhao Wu, Tiancheng He, Yiran Qin, Lei Bai, and Zhenfei Yin. Livesearchbench: An automatically constructed benchmark for retrieval and reasoning over dynamic knowledge. CoRR, abs/2511.01409, 2025a. doi: 10.48550/ARXIV.2511.01409. https://doi.org/10.48550/arXiv.2511.01409. Junting Zhou, Wang Li, Yiyan Liao, Nengyuan Zhang, Tingjia Miao, Zhihui Qi, Yuhan Wu, and Tong Yang. Scholarsearch: Benchmarking scholar searching ability of llms. CoRR, abs/2506.13784, 2025b. doi: 10.48550/ARXIV.2506.13784. https://doi.org/ 10.48550/arXiv.2506.13784. Peilin Zhou, Bruce Leon, Xiang Ying, Can Zhang, Yifan Shao, Qichen Ye, Dading Chong, Zhiling Jin, Chenxuan Xie, Meng Cao, Yuxin Gu, Sixin Hong, Jing Ren, Jian Chen, Chao Liu, and Yining Hua. Browsecomp-zh: Benchmarking web browsing ability of large language models in chinese. CoRR, abs/2504.19314, 2025c. doi: 10.48550/ARXIV.2504.19314. https://doi.org/10.48550/arXiv. 2504.19314. Yutao Zhu, Huaying Yuan, Shuting Wang, Jiongnan Liu, Wenhan Liu, Chenlong Deng, Zhicheng Dou, and Ji-Rong Wen. Large language models for information retrieval: survey. CoRR, abs/2308.07107, 2023. doi: 10.48550/ARXIV.2308.07107. https: //doi.org/10.48550/arXiv.2308.07107. 14 Figure 7 The JSON template of annotated trajectories. Table 5 Comparison of search behaviors between different models and human annotators. Model Search Similarity Search Diversity Browsing Similarity Qwen3-235B-A22B (thinking) Claude 4.5 Sonnet (non-thinking) Claude 4.5 Sonnet (thinking) Gemini 3 Pro (low) Gemini 3 Pro (high) GPT-5.2 (thinking) DeepSeek-V3.2 (non-thinking) DeepSeek-V3.2 (thinking) GLM-4.7 (thinking) Seed-1.8 (thinking) Qwen3-Max (thinking) Kimi K2.5 (thinking) 30.59 32.21 30.02 35.98 36.30 24.82 35.54 38.97 34.36 31.42 36.68 32.69 43.05 24.03 21.73 38.36 36.54 23.00 26.74 28.87 29.00 39.43 31.81 25.92 13.08 17.83 17.63 14.14 14.12 8.81 13.42 13.82 14.92 17.42 15.69 16."
        },
        {
            "title": "A Data Format",
            "content": "GISA is organized into three components: questions, answers, and human trajectories. (1) Questions: Each question is stored as JSON object containing the unique identifier, encrypted question text, answer type, question type (stable or live), and topic category. The encryption is applied to prevent data contamination from model pre-training. (2) Answers: Each answer is stored as separate CSV file corresponding to its question identifier. The CSV format accommodates all four answer types: item answers are placed in the first cell, set and list answers contain single column, and table answers contain multiple columns with predefined schema. (3) Human Trajectories: Each human trajectory is stored as JSON file containing three fields: the search queries issued by the annotator, the search engine results for each query, and the click actions recording navigations between pages. An example is illustrated in Figure 7. 15 System Prompt for ReAct-based Agent You are deep research assistant. Your core function is to conduct thorough, multi-source investigations into any topic. You must handle both broad, open-domain inquiries and queries within specialized academic fields. For every request, synthesize information from credible, diverse sources. You have 30 chances to call tools, use them wisely. # Final Answer Format When you have gathered sufficient information, you must output the final answer within <answer></answer> tags. Inside these tags, you must strictly follow the TSV (Tab-Separated Values) format enclosed in code block ```tsv. Determine the nature of the answer (Item, List, or Table) and format it as follows: 1. Single Item (Fact/Value): Use single column with the header Value. 2. List: Use single column with the header Item. 3. Table (Structured Data): Use standard TSV with appropriate headers for each column. CRITICAL: The content inside ```tsv must be valid TSV format. Always include header row. Do not add markdown notes or explanations inside the code block. Put any summary text outside the code block but still inside the <answer> tags. Current date: {current_date} User Question: {question} Figure 8 The system prompt used in ReAct-based Agent."
        },
        {
            "title": "B Implementation Details",
            "content": "Following recent studies (Li et al., 2025b,d; Xi et al., 2025a), we adopt the ReAct (Yao et al., 2023) architecture for our agent implementation. The system prompt and tool definitions are provided in Figure 8 and Figure 10, respectively. Each model is equipped with two tools: Search and Browse, with maximum limit of 30 tool invocations per session. Search Tool: This tool is implemented using the Google Search engine (through Serper API). It accepts query as input and retrieves the top-10 search results. Browse Tool: This tool is built upon the Jina API to extract content from given URL. To reduce noise, the retrieved content is summarized by an LLM. For experimental consistency, the summarization model is identical to the agents backbone model, using the non-thinking mode to reduce cost. The prompt for commercial systems follows similar structure and is shown in Figure 9. Human-Model Search Behavior Analysis We provide detailed metrics quantifying the similarity between model and human search behaviors. We compute three metrics for each model, which are defined as: Search Similarity measures the overlap between model-generated queries and human queries. For each human query, we compute the Jaccard similarity with all model queries and take the maximum value. The Jaccard similarity is defined as the ratio of the intersection to the union of term sets: Sim(Qh, Qm) = ThTm/ThTm, where Th and Tm are the term sets of human and model queries after lowercasing and tokenization. The questionlevel similarity is the average of all human query scores, and the final score is averaged across all questions. Search Diversity measures the diversity of consecutive queries, computed as the average Jaccard similarity between adjacent queries. Lower values indicate more diverse query reformulations. Browsing Similarity measures the overlap between URLs visited by the model and those visited by human annotators, computed as the Jaccard similarity of the two URL sets. From the results shown in Table 5, we can observe: First, search similarity scores range from 24.82% (GPT-5.2) to 38.97% (DeepSeek-V3.2 thinking), indicating moderate alignment between model and human query formulations. 16 Prompt for Commercial Systems You are helpful assistant. Given an users question, your task is to thinking step by step and output the final answer in the format of TSV. # Final Answer Format You must output the final answer within <answer></answer> tags. Inside these tags, you must strictly follow the TSV (Tab-Separated Values) format enclosed in code block ```tsv. Determine the nature of the answer (Item, List, or Table) and format it as follows: 1. Single Item (Fact/Value): Use single column with the header Value. 2. List: Use single column with the header Item. 3. Table (Structured Data): Use standard TSV with appropriate headers for each column. CRITICAL: The content inside ```tsv must be valid TSV. Always include header row. Do not add markdown notes or explanations inside the code block. Put any summary text outside the code block but still inside the <answer> tags. Current date: {current_date} User Question: {question} Figure 9 The system prompt used in commercial systems. Tools Definition 1. Search Description: Perform Google web search and return the top search results. Parameters: query (string, required): The search query string to be issued to Google. 2. Browse Description: Visit one or more web pages and return summarized version of their content based on specific goal. Parameters: url (array of strings, required): One or more URLs of the web pages to visit. goal (string, required): The specific information objective to focus on when summarizing the web pages. Figure 10 The functional schema of the tools. Second, Claude 4.5 Sonnet exhibits the lowest search diversity (21.73%), suggesting more focused and incremental query refinement similar to human behavior, while Qwen3-235B-A22B shows the highest diversity (43.05%), indicating more exploratory search patterns. Third, browsing similarity remains relatively low across all models (8.81%17.83%), confirming that models and humans often follow different navigation paths even when seeking the same information. Notably, Claude 4.5 Sonnet achieves the highest browsing similarity (17.83%), which aligns with its superior task performance."
        }
    ],
    "affiliations": [
        "Kuaishou Technology",
        "Renmin University of China"
    ]
}