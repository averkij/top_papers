{
    "paper_title": "SWE-SQL: Illuminating LLM Pathways to Solve User SQL Issues in Real-World Applications",
    "authors": [
        "Jinyang Li",
        "Xiaolong Li",
        "Ge Qu",
        "Per Jacobsson",
        "Bowen Qin",
        "Binyuan Hui",
        "Shuzheng Si",
        "Nan Huo",
        "Xiaohan Xu",
        "Yue Zhang",
        "Ziwei Tang",
        "Yuanshuai Li",
        "Florensia Widjaja",
        "Xintong Zhu",
        "Feige Zhou",
        "Yongfeng Huang",
        "Yannis Papakonstantinou",
        "Fatma Ozcan",
        "Chenhao Ma",
        "Reynold Cheng"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Resolution of complex SQL issues persists as a significant bottleneck in real-world database applications. Current Large Language Models (LLMs), while adept at text-to-SQL translation, have not been rigorously evaluated on the more challenging task of debugging SQL issues. To address this gap, we introduce BIRD-CRITIC, a new SQL issue debugging benchmark comprising 530 PostgreSQL tasks (BIRD-CRITIC-PG) and 570 multi-dialect tasks (BIRD-CRITIC-Multi), distilled from authentic user issues and replayed within new environments to facilitate rigorous evaluation. Baseline evaluations underscore the task's complexity, with the leading reasoning model O3-Mini achieving only 38.87% success rate on BIRD-CRITIC-PG and 33.33% on BIRD-CRITIC-Multi. Meanwhile, advancing open-source models for database tasks is crucial for empowering local development while safeguarding data privacy. Therefore, we present Six-Gym (Sql-fIX-Gym), a training environment for elevating open-source model capabilities for SQL issue debugging. This environment leverages SQL-Rewind strategy, which automatically generates executable issue-solution datasets by reverse-engineering issues from verified SQLs. However, popular trajectory-based fine-tuning methods do not explore substantial supervisory signals. We further propose f-Plan Boosting, which extracts high-level debugging plans from SQL solutions, enabling teacher LLMs to produce 73.7% more successful trajectories for training. We integrate these components into an open-source agent, Bird-Fixer. Based on Qwen-2.5-Coder-14B, Bird-Fixer achieves 38.11% success rate on BIRD-CRITIC-PG and 29.65% on BIRD-CRITIC-Multi, surpassing leading proprietary models such as Claude-3.7-Sonnet and GPT-4.1, marking a significant step toward democratizing sophisticated SQL-debugging capabilities. The leaderboard and source code are available: https://bird-critic.github.io/"
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 2 ] . [ 1 1 5 9 8 1 . 6 0 5 2 : r SWE-SQL: Illuminating LLM Pathways to Solve User SQL Issues in Real-World Applications Jinyang Li1,6, Xiaolong Li1,6, Ge Qu1,6, Per Jacobsson2, Bowen Qin6, Binyuan Hui6, Shuzheng Si5,6, Nan Huo1,6, Xiaohan Xu1,6, Yue Zhang3, Ziwei Tang1,6, Yuanshuai Li3, Florensia Widjaja3, Xintong Zhu3, Feige Zhou3, Yongfeng Huang4,6, Yannis Papakonstantinou2, Fatma Ozcan2, Chenhao Ma3,6, Reynold Cheng1,6 1 HKU STAR Lab 2 Google Cloud 3 CUHKSZ 4 CUHK 5 THU 6 The BIRD Team bird.bench25@gmail.com https://bird-critic.github.io/"
        },
        {
            "title": "Abstract",
            "content": "Resolution of complex SQL issues persists as significant bottleneck in realworld database applications. Current Large Language Models (LLMs), while adept at text-to-SQL translation, have not been rigorously evaluated on the more challenging task of debugging on SQL issues. In order to address this gap, we introduce BIRD-CRITIC, new SQL issue debugging benchmark comprising 530 carefully curated PostgreSQL tasks (BIRD-CRITIC-PG) and 570 multi-dialect tasks (BIRD-CRITIC-MULTI), which are distilled from authentic user issues and replayed within new environments to facilitate rigorous and contamination-free evaluation. Baseline evaluations on BIRD-CRITIC underscore the tasks complexity, with the leading reasoning model O3-MINI achieving only 38.87% success rate on BIRD-CRITIC-PG and 33.33% on BIRD-CRITIC-MULTI. Meanwhile, advancing open-source models for database tasks is crucial, empowering local development while safeguarding data privacy. Therefore, we present SIX-GYM (Sql-fIX-Gym), training environment for elevating the capabilities of open-source models specifically for SQL issue debugging. This environment leverages SQLRewind strategy, which automatically generates executable issue-solution datasets by reverse-engineering issues from verified SQLs. However, popular trajectorybased fine-tuning methods do not explore substantial supervisory signals. We further propose -Plan Boosting, which extracts high-level debugging plans automatically from SQL solutions, enabling the teacher LLMs to harvest and produce 73.7% more successful trajectories for training. We integrate these components into an open-source agent, BIRD-FIXER. Based on Qwen-2.5-Coder-14B, BIRDFIXER raises its success rate to 38.11% on BIRD-CRITIC-PG and 29.65% on BIRD-CRITIC-MULTI, surpassing many leading proprietary models such as Claude-3.7-Sonnet and GPT-4.1, marking significant step toward democratizing sophisticated SQL-debugging capabilities for both research and industry."
        },
        {
            "title": "Introduction",
            "content": "Relational Databases (RDBs) serve as the bedrock for data storage and information retrieval across countless modern applications, ranging from financial systems to web services and scientific research platforms [9, 35, 20, 36]. Structured Query Language (SQL), as the standard language for interacting with these systems, is thus critical interface for data manipulation, querying, and administration [3, 2]. Despite its widespread adoption and apparent simplicity for basic operations, mastering Equal contribution. Preprint. Under review. Figure 1: Illustration of the SQL issue debugging process in BIRD-CRITIC. It should start with user issue query (left) and issue SQL query (center-left), LLMs will produce corrected SQL solution (right) based on reasoning and interaction with the environment. SQL and troubleshooting complex queries or unexpected behaviors remains significant challenge for users of all experience levels. The complexity of query semantics, diverse behaviors across different SQL operations (Create, Read, Update, Delete), evolving database features, and the need to understand underlying data schemas contribute to steep learning curve and frequent user issues. Resolving these SQL issues often demands considerable manual efforts, domain expertise, and time, representing significant bottleneck in data-driven workflows and software development cycles [1, 26, 13, 41, 14]. Support forums, Q&A sites, and internal helpdesks, such as StackOverflow, are replete with user requests seeking assistance in debugging faulty queries, optimizing performance, or understanding why query generates unexpected results. Therefore, automating this process holds huge value in improving productivity and reducing reliance on specialized human experts. Recent advancements in Large Language Models (LLMs) have demonstrated remarkable capabilities in natural language understanding and code generation [6, 55, 8, 39, 52], notably achieving impressive results in converting natural language descriptions into SQL queries (text-to-SQL) [25, 47, 30, 23]. However, diagnosing and fixing existing incorrect or suboptimal SQL code presents more complex challenges. As shown in Figure 1, debugging such issues requires not only understanding the users intent, often in verbose and long-context description, but also analyzing the query logic underneath, identifying subtle errors, and intensively interacting with the database schema. Despite the practical importance of this task, the capabilities of current LLMs in SQL issue resolution have not been systematically investigated. In this work, we are targeting to bridge this critical gap by two primary contributions. First, we present BIRD-CRITIC, carefully curated benchmark built from authentic StackOverflow bugfix threads. It comes in two subsets: (1) BIRD-CRITIC-PG with 530 PostgreSQL-only tasks, and (2) BIRD-CRITIC-MULTI, whose 570 tasks are distributed across 4 major open-source dialects containing PostgreSQL, MySQL, SQL Server, and Oracle. Each task undergoes rigorous reconstruction where the underlying knowledge structures and debugging heuristics are extracted, and the scenario is reproduced within controlled sandbox environment by new RDBs and conditions. This process ensures that tasks remain relevant while minimizing potential exposure to pre-training data. Furthermore, execution accuracy (EX) in standard text-to-SQL is inadequate for the diverse types of issues in BIRD-CRITIC, frequently leading to false negatives. Specifically, tasks involving database state changes, i.e., via Data Manipulation Language (DML) or Data Definition Language operations (DDL), frequently permit multiple functionally equivalent solutions that may differ syntactically or include non-impacting elements [54, 4]; reliance on strict EX matching would incorrectly penalize such valid SQL solutions. Therefore, each task is augmented with custom evaluation scripts containing specific test cases designed to evaluate functional correctness, enabling precise calculation of task success rates. Our baseline evaluations on BIRD-CRITIC underscore the complexity of SQL issue debugging, in which even advanced reasoning models, O3-mini, only achieves 38.87% success rate on BIRD-CRITIC-PG and 33.33% on BIRD-CRITIC-MULTI. Second, inspired by prior work on code generation environments [29], we propose SIX-GYM (SQL-FIX-GYM), training environment designed to enhance the SQL debugging capabilities of open-source models. core innovation within SIX-GYM is the SQL-Rewind strategy, an automated 2 Figure 2: Example task structure within the BIRD-CRITIC benchmark, demonstrating the transformation from user-reported issue and error SQL to revised SQL solution. methodology for generating large-scale, executable issue-solution datasets. This strategy operates by taking verified, correct SQL queries and systematically introducing plausible errors, effectively reverse-engineering realistic debugging scenarios. common practice [29, 17] of such environments involves using an advanced teacher LLM to generate successful task execution trajectories for finetuning student smaller models. However, we find that this approach underutilizes the guidance available from ground-truth or reference solutions, potentially limiting the quantity and diversity of effective training trajectories. To address this, we introduce the Functional Plan (f -plan) Boosting strategy. This method first infers the underlying debugging logic by comparing the problematic SQL and the correct solution, representing this logic as step-by-step pseudo-functional code plan. Afterwards, guided by this -plan, teacher LLM employs our designed agent scaffold, SQL-ACT, to execute the debugging task within the environment. This plan-guided approach generates significant 73.7% increase in more successful trajectories, providing richer data for fine-tuning open-source models, particularly smaller ones, to effectively interact with the database environment and debug complex SQL issues. The agent fine-tuned using this -plan boosted data is termed BIRD-FIXER. Our experiments demonstrate that BIRD-FIXER significantly enhances the performance of opensource models from various families. Notably, BIRD-FIXER fine-tuned on Qwen-2.5-Coder-14B achieves 38.11% Success Rate (SR) on BIRD-CRITIC-PG and 29.65% on BIRD-CRITICMULTI, surpassing the performance of the highly capable models such as Claude-3.7-Sonnet and GPT-4.1. This result marks significant advancement towards democratizing sophisticated SQL debugging capabilities for both research and practical industry applications."
        },
        {
            "title": "2 Problem Definition",
            "content": "In this paper, we introduce more complex but realistic task of SQL issue resolution. This task starts with user-provided issue SQL query σissue, natural language problem description detailing the issue and intent, and the database schema S. The goal is to generate revised SQL query (σpred) that corrects the fault while preserving the users intent. This mapping is: σpred = fθ(P, S, σissue). (1) The desired output σpred must satisfy the users underlying intentions as inferred from the triplet (P, S, σissue). In BIRD-CRITIC, we annotated referenced ground-truth solution SQLs as σ and develop tailored evaluation scripts (detailed in Section 3) for each task, enabling precise evaluation of the functional correctness of predicted solution SQLs."
        },
        {
            "title": "3 BIRD-CRITIC Benchmark",
            "content": "Annotator Group. BIRD-CRITIC is developed via multi-stage annotation converting raw user issues into executable, verifiable tasks. This involves two annotation groups: 1) 10 qualified database/SQL annotators, who pass strict entry test as detailed in Appendix B.1 and systematic training shown in Appendix B.2 to promise the quality of annotation; 2) 3 senior database experts/scientists for final data collection decision. This process is visually outlined in Figure 2. 3 Table 1: Data Characteristics STATISTIC Total Issues # of query-like issues # of management issues # of personalization issues PG 530 291 88 151 MULTI 570 304 104 162 user query length (mean/max) issue SQL length (mean/max) solution SQL length (mean/max) # distinct test cases # of preprocess SQLs # of clean_up SQLs 162.98/1046 133.29/1262 112.64/853 365 643 287 165.75/1058 125.86/1254 117.46/859 317 571 inter-agreement 94.53 92.98 Figure 3: Distribution of issue categories in all BIRDCRITIC, derived from an analysis of SQL usage in the real-world database applications. detailed distribution is in Appendix E. Environment Setup. We leverage relational databases from the BIRD-SQL development set [25] chosen for its domain diversity across real data-science tasks (California Schools, Financial, Superheroes) and its permissive license. We migrate their original SQLite schemas to PostgreSQL, MySQL, SQL Server, and Oracle, four widely used production-grade dialects. During migration, we go beyond direct dialect translation by refining table and column names. We adjust data types and introduce guarded alterations to schema components to reduce potential information leakage (see Appendix A.2). To pair these databases with realistic debugging scenarios, we collect SQL issue queries from Stack Overflow, following strict protocol shown in Appendix A.3. Issue Reproduction. Following the initial collection of candidate issues, we start reproducing them in our environment in following produces as illustrated in Figure 2: (1) Distilling Intent and Error: Precisely identifying the users underlying goal and the specific reason of the issue exhibited by σissue. The core reason of the issue is documented. (2) Schema Mapping: Assigning the issue to one of the adapted BIRD-SQL database schemas (S) that provides suitable context for the problem. (3) Reproducibility Verification: We adapt and execute σissue against the chosen database, verifying through execution logs that the error appears as expected. This entire process transforms potentially ambiguous web forum post into standardized, reproducible problem instance (P, S, σissue) ready for solution annotation. Solution SQL & Evaluation Script Annotation. Annotators carefully review the reproduced issue (P, S, σissue) and craft new σ. This annotation requires ensuring that σ can accurately fulfill the users objective as inferred from and the context of σissue. Also, to ensure robust evaluation, each task is annotated with evaluation scripts consisting of specific test cases written by Python and SQLs. Details can be found in Appendix C. We report the Success Rate (SR %), considering task solved only when σpred successfully passes all test cases in its evaluation script. Validation. After annotation, BIRD-CRITIC undergoes cross-validation, with annotators exchanging data for review. This verification involves three steps: (1) enhancing test case functions with additional test cases for robust SQL code validation; (2) red teaming the SQL by introducing errors to make sure evaluation scripts can flag these errors. (3) Annotators first attempt to resolve disagreements through discussion. Persistent issues are escalated to the expert team for final determination, which may involve modification or rejection of the disputed annotation. Benchmark Statistics. Table 1 summarizes the key properties of the BIRD-CRITIC benchmark, and Figure 3 visualizes the distribution of its underlying knowledge categories. The distribution of benchmark, is detailed in Appendix E. side-by-side comparison with standard text-to-SQL benchmarks (Table 5 in Appendix E.1) exposes three distinctive challenges introduced by BIRDCRITIC: non-query-like problems, multi-dialect complexities, and the most verbose but authentic user queries. As far as we know, BIRD-CRITIC is the first debugging benchmark for SQL applications. These aspects establish BIRD-CRITIC as crucial benchmark for rigorously evaluating LLM proficiency in solving authentic SQL issues."
        },
        {
            "title": "4 SIX-GYM: An Automated SQL Debugging Environment for LLMs",
            "content": "This section introduces SIX-GYM, dedicated training environment for enhancing the SQL debugging capabilities of LLMs. This environment is built upon SQL-Rewind, which is responsible for the automated generation of comprehensive suite of SQL issue instances. Overview. GYM-like datasets have proven effective for training LLMs as agents for complex tasks [29]. However, manually collecting and annotating these datasets is labor-intensive and difficult to scale, especially for debugging tasks. Thus, we introduce SQL-Rewind, which addresses this by inverting the debugging paradigm: starting with correct SQL queries (σ) and systematically introducing realistic issues to generate issue SQLs (σissue) and user issue query P. This approach enables efficient creation of large-scale training data without human annotation. The pseudo-algorithm is shown in Appendix F.1. Solution SQL Collection. We begin with raw StackOverflow issue data and enforce two principles against data overlap: (i) any issue used to construct BIRD-CRITIC tasks is excluded from SIX-GYM, and (ii) SQL-Rewind operates only on the 12 databases in the training databases of BIRD-SQL, while BIRD-CRITIC evaluation is confined to databases drawn solely from the BIRD-SQL dev set. We mine new candidate SQLs via rule-based regular expressions, then leverage Gemini-2.0-Flash to align table and column references to 12 databases in SIX-GYM, while preserving the original SQLs logical structure. To validate these adapted SQL queries as ground truth solutions, each was executed against its target database; only those queries that completed without error and yielded non-null result were accepted into our final corpus of solution SQLs (σ). Synthetic Issue Generation and Automated Verification. We employ Gemini-2.0-Flash to automate the entire process of issue reproduction and verification. Initially, the model summarizes issue reasons (rissue) and modifies solution SQL (σ) to create issue SQL (σissue) guided by rissue. Concurrently, it generates evaluation scripts comprising test cases designed to be passed by solution SQLs but failed by issue SQLs. The model then automatically validates whether the logic of triplet σissue, rissue, σ is coherent and whether the evaluation script accurately identifies errors while allowing solution SQLs to pass. This validation process undergoes 3 iterative refinements; if the components are deemed compatible, the data is added to our collection. User Issue Query Generation. Finally, we employ Gemini-2.0-Flash again to simulate realistic user issue description P. The generated includes the user intent, issue description, and requirements. Each must be logically consistent with S, σissue, T, σ. It undergoes up to 3 rounds of optimization by the model to reduce hallucinations. The resulting tuples are collected as final data. Using this SQL-Rewind strategy, we successfully generate approximately 3,301 high-quality synthetic data instances, forming training environment we term SIX-GYM."
        },
        {
            "title": "5 BIRD-FIXER: Elevating Open-Source LLMs to an SQL Issue Fixer",
            "content": "5.1 Agent Scaffold: SQL-ACT ReAct [44] interleaves internal reasoning (thoughts ti), external actions (ai), and observations (oi), and has proved highly effective for state-of-the-art code agents [29, 39, 40]. Building upon this paradigm, we introduce SQL-ACT, specialized agent scaffold tailored for SQL tasks, particularly targeting challenges presented in benchmarks like BIRD-CRITIC. Unlike tool-based agents whose action space is restricted to finite, hand-crafted set of operations, SQL-ACT treats arbitrary SQL commands as actions, dramatically enlarging the space of possible manipulations and enabling richer, more flexible debugging strategies. At each step the agent emits tuple (cid:0)ti, σi, oi (cid:1), where σi is the SQL statement executed at step i. The complete execution trajectory is therefore τ = ((t1, σ1, o1), (t2, σ2, o2), . . . , (tn, σn, on)). As shown in Section 6.2, SQL-ACT is not only simpler to implement than TOOL-ACT but also delivers consistently higher accuracy in SQL issues solutions. 5 Figure 4: LLM agent performance for BIRD-CRITIC-PG. TOOACT employs constrained toolkit as actions, while SQLACT executes SQLs as actions. 5.2 Trajectory Collection and Agent Fine-Tuning -Plan Boosting. The standard gym-style practice involves strong teacher LLM on the environment and logs only those trajectories that reach the reference solution. In our experiments, running Gemini-2.0-Flash with SQL-ACT on SIX-GYM produces just 1,254 successful trajectories, which just utilizes 38.0% of the data. To augment successful trajectories, we introduce -Plan Boosting, two-phase self-distillation loop: (1) Backward inference. Given the problem (P, S, σissue) and its corrected query σ, the teacher annotates step-by-step symbolic functional plan = (f1, . . . , fk), where each fi represents an abstract debugging operation that maps σissue toward σ. Since such plan contains few tokens yet exhibits more structured format, it is especially amenable to execution by LLMs [7, 19]. (2) Forward validation. Using only the context (P, S, σissue) and the candidate plan , the teacher LLM regenerates solution by SQL-ACT. The plan is accepted iff the regenerated solution SQL passes every test cases in , producing reliable pair (cid:10)(P, S, σissue), (cid:11). After rollout we discard and retain only the executable trace τ = (cid:0)(t1, σ1, o1), . . . , (tn, σn, on)(cid:1). single pass of -Plan Boosting produces total 2,178 successful trajectories, an increase of 73.7% over the vanilla collection pipeline, which we then use to fine-tune the open-source models via Low-Rank Adaptation (LoRA) [16]. Generative Thought Mode (GTM). The generalization of the agent can degrade when it predicts thoughts and actions jointly, because the model tends to overfit to the SQL patterns seen during fine-tuning. To counter this problem, we introduce Generative Thought Mode (GTM), which explicitly decouples the two predictions, akin to how Skip-gram in Word2Vec separates target and context words [27]. Let MO be the fine-tuned model, MB the original base model, and Hi1 = ((t1, σ1, o1), . . . , (ti1, σi1, oi1)) the interaction history. During the inference step i, the finetuned model first proposes thoughtaction pair (ti, σi) = MO(Hi1), from which only the thought ti is extracted. The SQL action is then generated by the base model, σi = MB(Hi1, ti), leveraging its wide-coverage knowledge of diverse SQL dialects. GTM preserves the specialized debugging logic learned by MO, fully taking advantage of generative features of auto-regressive models [51], while mitigating overfitting of SQL patterns during training."
        },
        {
            "title": "6 Experiments",
            "content": "6.1 SetUp primary categories, including Claude-3.7-Sonnet, Models. We evaluate the performance of several popular and strong LLMs across Gemini-2.0-Flash, general-purpose models: two GPT-4.1, Qwen-2.5-Coder-32B, Meta-Llama-3.1-8B, Meta-Llama-3.3-70B, Phi-4 and DeepSeek-V3. The second category consists of models specifically renowned for their advanced reasoning capabilities: O3-mini, O1-preview, Gemini-2.0-Flash-Thinking, and Claude-3.7-Sonnet-Thinking. The implementation details are in Appendix H.2. Table 2: Success Rate (SR %) of different models on BIRD-CRITIC-PG and BIRD-CRITICMULTI, grouped by each issue and dialect categories. Bold numbers indicate the highest score in each column, and underlined numbers indicate the second highest. \"Quer.\" = query-like issues, \"Mana.\" = data-management issues, \"Pers.\" = personalized-function issues. \"PG.\" = PostgreSQL, \"My.\" = MySQL, \"Server\" = SQL-Server. Model BIRD-CRITIC-PG BIRD-CRITIC-MULTI Quer. Mana. Pers. Overall PG. My. Server Oracle Overall Meta-Llama-3.1-8B Phi-4 Deepseek-V3 Gemini-2.0-Flash Meta-Llama-3.3-70B Qwen2.5-Coder-32B Claude-3.7-Sonnet GPT-4.1 18.21 30.24 25.09 27.84 27.84 31.62 27.15 31.27 Gemini-2.0-Flash-Thinking Claude-3.7-Sonnet-Thinking O1-Preview-2024-09-12 O3-Mini-2025-01-31 27.15 29.55 29.90 32.30 General-Purpose Models 22.73 37.50 35.23 44.32 32.95 38.64 43.18 55.68 53.41 45.45 53.41 57.95 11.26 25.83 28.48 29.14 27.81 24.50 35.10 38.41 16.98 30.19 27.74 30.94 28.68 30.75 32.08 37.36 13.04 25.72 27.17 27.54 26.81 28.26 32.61 36.23 13.27 27.55 26.53 22.45 22.45 24.49 30.61 28. Reasoning Models 33.11 35.76 37.09 40.40 33.21 33.96 35.85 38.87 28.99 35.51 40.94 41.30 35.71 31.63 33.67 26.53 21.43 23.47 21.43 31.63 28.57 30.61 21.43 29. 37.76 27.55 33.67 32.65 3.06 8.16 14.29 7.14 14.29 9.18 18.37 9.18 19.39 15.31 11.22 18.37 12.81 22.63 23.86 23.86 24.21 24.74 27.89 29.12 30.00 30.00 33.33 33.33 Advanced Agentic Methods. Agentic workflows have shown considerable promise for addressing complex tasks. Accordingly, we also benchmark LLM agent performance on BIRD-CRITIC. Broadly, agentic systems can be classified into two main categories based on their action types. The first, which we term TOOL-ACT, involves agents employing pre-defined tools tailored to specific tasks. We implement Tool-Act guided by SOTA agents Spider-Agent [22] and InterCode [43] in SQL tasks. The second category, CODE-ACT [39], allows for more flexible, free-form actions where LLMs generate code to perform operations. In the context of this research, we implement specific variant called SQL-ACT, where the LLMs generate SQL queries as their actions as introduced in Section 5.1. 6.2 Main Results Baseline Results. An evaluation of mainstream Large Language Models (LLMs) on BIRD-CRITIC is detailed in Table 2. We can observe that: (1) Superior Performance of Reasoning-Oriented Models. clear performance advantage is evident for reasoning-oriented LLMs. These models surpass general-purpose counterparts by an average Success Rate (SR) of 6.13 % on PostgreSQL issues and 8.03 % on multi-dialect issues. This disparity underscores the computationally intensive, reasoning-driven nature of SQL-issue debugging, task that demonstrably benefits from models capable of intermediate inferential steps. (2) Persistent Challenge Posed by SQL Issue Debugging. Despite ongoing advancements in LLM capabilities, BIRD-CRITIC continue to present considerable challenge. The top-performing model, O3-Mini-2025-01-31, achieves an overall SR of only 38.87% on PostgreSQL issues and 33.33% on multi-dialect issues, leaving large head-room for future research. (3) Heterogeneous Difficulty Across Issue Categories. An analysis of performance across distinct SQL issue categories reveals clear differences in difficulty. Issues related to data management, such as DML operations: insertions, deletions, updates, and DDL operations like schema modifications) are found to be relatively more manageable. On average, reasoning models achieved 52.6% SR and general-purpose models 38.8% SR in data management. Issues associated with Personalized functions also demonstrate moderate success rates. In contrast, Query-like issues present the greatest challenge for all LLMs. 7 Table 3: Detailed comparison of BIRD-FIXER with other strong baselines on BIRD-CRITIC-PG and BIRD-CRITIC-MULTI. shows relative improvement of BIRD-FIXER compared to base model. Model BIRD-CRITIC-PG (SR %, ) BIRD-CRITIC-MULTI (SR %, ) Base SQL-ACT BIRD-FIXER (%) Base SQL-ACT BIRD-FIXER (%) Llama-3.1-8B 16.98 Qwen-2.5-Coder-7B 23.40 Qwen-2.5-Coder-14B 31.32 30.19 Phi-4 16.42 26.60 31.13 29.43 24.34 31.32 38.11 38.11 +43.34 12.81 +33.84 17.89 +21.68 24.04 +26.23 22.63 13.64 17.19 23.33 19.80 18.25 21.58 29.65 27. +42.46 +20.58 +23.36 +20.58 These issues require an understanding of logical flaws within complex SELECT statements, particularly those involving joins, subqueries, aggregations, and conditional filtering. Unlike more standardized data management operations, SELECT queries exhibit remarkable diversity in their logic, structure, and intent, mostly reflected by the wide variety of underlying business requirements they serve, making their error patterns significantly harder to predict and correct. As evidenced in Figure 5, Query-like issues contain the most diverse functions, leading to the lowest performance of both general-purpose and reasoning models, which presents strong negative correlation. Figure 5: Success Rate vs Query Diversity (by Issue Category). It shows strong negative correlation (r = -0.89) between n-gram of tokens and model performance after normalization. (4) Dialect-Specific Performance Variations. Model effectiveness exhibits notable dependency on the specific SQL dialect, as observed within the BIRD-CRITIC-MULTI. Specifically, Gemini-2.0-Flash-Thinking demonstrates the lower performance on PostgreSQL with 28.99% SR. In contrast, it becomes the most proficient for SQL Server (37.76% SR), with clear margin over other evaluated models in that dialect. Such variations are plausibly attributable to differential distributions of SQL dialects within the respective training corpora of these models, suggesting that the composition of training data significantly influences dialect-specific debugging capabilities. (5) Agentic Workflow Performance. Figure 4 compares the performance of different LLM-based agents on BIRD-CRITIC-PG. The results show that agentic workflows markedly boost LLM accuracy on issue debugging tasks, which benefits from iterative interaction with its environment. Additionally, the SQL-ACT agent mostly outperforms the TOOL-ACT agent, suggesting that the richer, more flexible action space offered by SQL-ACT better equips LLMs to address the diverse and uncertain challenges encountered during debugging. 6.3 Performance Analysis of BIRD-FIXER Overall Performance of BIRD-FIXER. Table 3 reports the performance gains achieved by BIRDFIXER across three model families: Llama, Qwen, and Phi, which range from roughly 7B to 14B parameters. For each model, BIRD-FIXER delivers substantial improvements, demonstrating that the benefits of SQL-ACT + -plan and SIX-GYM are architecture-agnostic and scalable. The table also exposes limitation of small language models (SLMs) in agentic workflow only by inference: on several models, agent performance actually declines, suggesting that long, complicated interaction histories can overwhelm SLMs. By contrast, our methods equip these compact models with richer interaction capabilities, enabling them to navigate complex environments far more effectively. This benefit is especially valuable for privacy-sensitive SQL workloads: running 714B parameter agent locally avoids any exposure of proprietary data to cloud services. Notably, BIRD-FIXER based on 14B base models, e.g., Qwen-2.5-Coder-14B, BIRD-FIXER presents competitive performance to O3-mini and outperforms the Claude-3.7-Sonnet agent on BIRD-CRITIC-PG, suggesting promising path toward this goal of privacy while keeping effectiveness. 8 Generalization to Multi-Dialect SQL Issue Debugging. Although BIRD-FIXER is fine-tuned only on PostgreSQL trajectories within SIX-GYM, it generalizes robustly to other SQL dialects, as evidenced by the multi-dialect results in Table 3. That is because GTM elicits each model to produce reusable debugging strategy trained in SIX-GYM while keeping pretrained knowledge of dialect variation. In conclusion, BIRD-FIXER exhibits strong cross-dialect reasoning without any extra data collection or further training, underscoring its practicality for heterogeneous database stacks. 6.4 Ablation Study of BIRD-FIXER Figure 6 shows the ablation study of BIRD-FIXER, highlighting: GTM (Generative Thought Mode): Removing GTM causes the fine-tuned model MO to predict both thought and SQL action directly. The performance drop to 33.33% indicates that GTM effectively leverages the base model MB for SQL generation guided by MOs thought, mitigating overfitting to SQL patterns and better utilizing MBs broad SQL knowledge. -Plan Boosting: Using only trajectories from the vanilla collection pipeline reduces performance to 32.45% in BIRDCRITIC-PG, highlighting -Plan Boostings importance in generating diverse, high-quality training trajectories crucial for complex reasoning tasks. Figure 6: Ablation study of components in BIRD-FIXER. 6.5 Error Analysis To understand how far current LLM-based agents still are from fully resolving user-reported SQL issues, we sample 100 failed tasks from BIRD-CRITIC-PG by 4 agents: O3-mini, GPT-4.1, Claude-3.7-Sonnet, and BIRD-FIXER. It can be concluded: Projection Mismatch errors (26.9%), where models misinterpret output requirements by, for instance, adding unexpected columns or misapplying aggregations; Chain of Errors (27.3%), characterized by cascading failures due to partial problem resolution that overlooks dependent issues such as sequence updates accompanying primary key modifications; Incorrect Logic (44.5%), the most prevalent, highlighting fundamental misunderstandings of data structures or transformation methodologies, particularly in complex operations like JSON array manipulation, leading to syntactically plausible but semantically flawed SQL; and Syntax Errors (29.3%), indicating technical implementation flaws such as type mismatches (e.g., DATE versus TIMESTAMP) or improperly formatted intervals, especially in specialized SQL contexts like recursive queries. The detailed examples for each category are in Figure 8."
        },
        {
            "title": "7 Related Work",
            "content": "Large Language Models for Text-to-SQL. The automated conversion of natural language queries into Structured Query Language (SQL), known as Text-to-SQL, has garnered significant attention due to its practical utility in the era of big data [49, 42, 33]. The advent of LLMs has notably advanced the capabilities in this domain. For instance, DIN-SQL [31], DAIL-SQL [10], TA-SQL [34], and Chase-SQL [32] have demonstrated SOTA performance on standard benchmarks like Spider [46] and BIRD [25], primarily by leveraging in-context learning with powerful foundation models like GPT-4. Also Supervised fine-tuning can fuel smaller LLMs towards stronger text-to-SQL parsers as evidenced by XiYanSQL[11], Arctic[50], OmniSQL [24], CodeS [23]. Beyond direct generation, agentic workflows such as MAC-SQL [38], InterCode [43], which empowers LLMs to interact with database environments and gather contextual information, are pushing the boundaries of LLM cognition in handling complex and previously unseen databases. Concurrently, the field is evolving towards addressing more sophisticated, industry-relevant Text-to-SQL challenges. Initiatives like Beaver [5] and the Spider 2.0 [22] signify shift from end-user focused queries to tasks requiring deeper BI knowledge and handling of larger schemas. This progression naturally leads to critical, but underexplored, question: Can LLMs effectively diagnose and resolve issues within existing, user-provided SQL queries? LLMs for Program Repair. Program repair provides complementary lens through which to evaluate and enhance the reasoning abilities of LLMs. At the function level, DEBUGBENCH [37] 9 offers multi-language suite that stresses fundamental programming logic. Repository-scale efforts such as SWE-BENCH [18] move closer to realistic software engineering, while follow-up studies, including SWE-LANCE [28] and MULTI-SWE [48] highlight the limitations of even sophisticated LLM-driven agents on complex, multi-language projects (e.g. Python, Java). Despite this rapid progress in general-purpose code fixing, SQL-specific debugging remains largely unexplored, even though databases are the backbone of most data-centric applications. To the best of our knowledge, our work is the first to formally cast SQL issue repair as benchmark task, and to propose methods that adapt and augment open-source LLMs for automated SQL debugging."
        },
        {
            "title": "8 Conclusion",
            "content": "We introduced BIRD-CRITIC, the first benchmark for SQL issue debugging tasks. Experiments show that SOTA LLMs solve fewer than 40% SR, underscoring the challenge. We also create SIX-GYM, an automated training environment which can produce thousands of high-quality agent trajectories without human annotation. Built on top of these trajectories, we proposed SQL-Act, lightweight agent scaffold, and applied trajectory-level augmentation (f -plan) to fine-tune open-source LLMs, leading to the Bird-Fixer. Despite using only 714 parameter backbones, BIRD-FIXER outperforms larger proprietary models and generalizes across four SQL dialects without additional training. Our research charts path toward robust, real-world SQL issue debugging assistants."
        },
        {
            "title": "References",
            "content": "[1] Alireza Ahadi, Julia Prior, Vahid Behbood, and Raymond Lister. Students syntactic mistakes in writing seven different types of sql queries and its application to predicting students success. In Proceedings of the 47th ACM Technical Symposium on Computing Science Education, pages 401406. ACM, 2016. [2] Michael Armbrust, Reynold S. Xin, Cheng Lian, Yin Huai, Davies Liu, Joseph K. Bradley, Xiangrui Meng, Tomer Kaftan, Michael J. Franklin, Ali Ghodsi, and Matei Zaharia. Spark SQL: relational data processing in spark. In Timos K. Sellis, Susan B. Davidson, and Zachary G. Ives, editors, Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data, Melbourne, Victoria, Australia, May 31 - June 4, 2015, pages 13831394. ACM, 2015. [3] Donald D. Chamberlin and Raymond F. Boyce. SEQUEL: structured english query language. In Proceedings of the 1974 ACM SIGMOD Workshop on Data Description, Access and Control, pages 249264. ACM, 1974. [4] Bikash Chandra, Ananyo Banerjee, Udbhas Hazra, Mathew Joseph, and S. Sudarshan. Automated grading of sql queries. In 35th IEEE International Conference on Data Engineering (ICDE), pages 16301633. IEEE, 2019. [5] Peter Baile Chen, Fabian Wenz, Yi Zhang, Devin Yang, Justin Choi, Nesime Tatbul, Michael Cafarella, Çagatay Demiralp, and Michael Stonebraker. Beaver: an enterprise benchmark for text-to-sql. arXiv preprint arXiv:2409.02038, 2024. [6] Wenhu Chen, Xueguang Ma, Xinyi Wang, and William W. Cohen. Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. Trans. Mach. Learn. Res., 2023, 2023. [7] Wenhu Chen, Xueguang Ma, Xinyi Wang, and William W. Cohen. Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. Transactions on Machine Learning Research, 2023. ISSN 2835-8856. [8] Xinyun Chen, Maxwell Lin, Nathanael Schärli, and Denny Zhou. Teaching large language models to self-debug. In The Twelfth International Conference on Learning Representations, 2024. [9] C. J. Date. An Introduction to Database Systems. Addison-Wesley / Pearson, 8th edition, 2003. [10] Dawei Gao, Haibin Wang, Yaliang Li, Xiuyu Sun, Yichen Qian, Bolin Ding, and Jingren Zhou. Text-to-sql empowered by large language models: benchmark evaluation. arXiv preprint arXiv:2308.15363, 2023. [11] Yingqi Gao, Yifu Liu, Xiaoxia Li, Xiaorong Shi, Yin Zhu, Yiming Wang, Shiqi Li, Wei Li, Yuntao Hong, Zhiling Luo, Jinyang Gao, Liyu Mou, and Yu Li. preview of xiyan-sql: multi-generator ensemble framework for text-to-sql. arXiv preprint arXiv:2411.08599, 2024. URL https://arxiv.org/abs/2411.08599. [12] Zhipeng Gao, Xin Xia, David Lo, John C. Grundy, Xindong Zhang, and Zhenchang Xing. know what you are searching for: Code snippet recommendation from stack overflow posts. ACM Trans. Softw. Eng. Methodol., 32(3):80:180:42, 2023. [13] Sneha Gathani, Peter Lim, and Leilani Battle. Debugging database queries: survey of tools, techniques, and users. In CHI 20: CHI Conference on Human Factors in Computing Systems, Honolulu, HI, USA, April 25-30, 2020, pages 116. ACM, 2020. [14] Sabaat Haroon, Chris Brown, and Muhammad Ali Gulzar. Desql: Interactive debugging of SQL in data-intensive scalable computing. Proc. ACM Softw. Eng., 1(FSE):767788, 2024. [15] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022, 2022. 11 [16] Edward Hu, yelong shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations, 2022. [17] Naman Jain, Jaskirat Singh, Manish Shetty, Liang Zheng, Koushik Sen, and Ion Stoica. R2egym: Procedural environments and hybrid verifiers for scaling open-weights swe agents. arXiv preprint arXiv:2504.07164, 2025. [18] Carlos Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik Narasimhan. SWE-bench: Can language models resolve real-world github issues? In The Twelfth International Conference on Learning Representations, 2024. [19] Anubha Kabra, Sanketh Rangreji, Yash Mathur, Aman Madaan, Emmy Liu, and Graham Neubig. Program-aided reasoners (better) know what they know. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), Mexico City, Mexico, June 2024. Association for Computational Linguistics. [20] Sean Kandel, Andreas Paepcke, Joseph M. Hellerstein, and Jeffrey Heer. Enterprise data analysis and visualization: An interview study. IEEE Trans. Vis. Comput. Graph., 18(12): 29172926, 2012. [21] Yuhang Lai, Chengxi Li, Yiming Wang, Tianyi Zhang, Ruiqi Zhong, Luke Zettlemoyer, WenTau Yih, Daniel Fried, Sida I. Wang, and Tao Yu. DS-1000: natural and reliable benchmark for data science code generation. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, volume 202 of Proceedings of Machine Learning Research, pages 1831918345. PMLR, 2023. [22] Fangyu Lei, Jixuan Chen, Yuxiao Ye, Ruisheng Cao, Dongchan Shin, Hongjin SU, ZHAOQING SUO, Hongcheng Gao, Wenjing Hu, Pengcheng Yin, Victor Zhong, Caiming Xiong, Ruoxi Sun, Qian Liu, Sida Wang, and Tao Yu. Spider 2.0: Evaluating language models on real-world enterprise text-to-SQL workflows. In The Thirteenth International Conference on Learning Representations, 2025. [23] Haoyang Li, Jing Zhang, Hanbing Liu, Ju Fan, Xiaokang Zhang, Jun Zhu, Renjie Wei, Hongyan Pan, Cuiping Li, and Hong Chen. Codes: Towards building open-source language models for text-to-sql. Proceedings of the ACM on Management of Data, 2(3):128, 2024. [24] Haoyang Li, Shang Wu, Xiaokang Zhang, Xinmei Huang, Jing Zhang, Fuxin Jiang, Shuai Wang, Tieying Zhang, Jianjun Chen, Rui Shi, et al. Omnisql: Synthesizing high-quality text-to-sql data at scale. arXiv preprint arXiv:2503.02240, 2025. [25] Jinyang Li, Binyuan Hui, Ge Qu, Jiaxi Yang, Binhua Li, Bowen Li, Bailin Wang, Bowen Qin, Ruiying Geng, Nan Huo, et al. Can llm already serve as database interface? big bench for large-scale database grounded text-to-sqls. Advances in Neural Information Processing Systems, 36, 2024. [26] Floris Miedema, Janet Spacco, and Raymond Lister. Patterns of SQL mistakes among novice programmers: An exploratory study. In Proc. 26th ACM Conf. on Innovation and Technology in Computer Science Education (ITiCSE), pages 5561. ACM, 2021. doi: 10.1145/3456565. 3456622. [27] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeff Dean. Distributed representations of words and phrases and their compositionality. Advances in neural information processing systems, 26, 2013. [28] Samuel Miserendino, Michele Wang, Tejal Patwardhan, and Johannes Heidecke. Swe-lancer: Can frontier llms earn $1 million from real-world freelance software engineering? arXiv preprint arXiv:2502.12115, 2025. [29] Jiayi Pan, Xingyao Wang, Graham Neubig, Navdeep Jaitly, Heng Ji, Alane Suhr, and Yizhe Zhang. Training software engineering agents and verifiers with swe-gym, 2024. URL https: //arxiv.org/abs/2412.21139. 12 [30] Mohammadreza Pourreza and Davood Rafiei. Din-sql: Decomposed in-context learning of text-to-sql with self-correction. Advances in Neural Information Processing Systems, 36: 3633936348, 2023. [31] Mohammadreza Pourreza and Davood Rafiei. DIN-SQL: Decomposed in-context learning of text-to-SQL with self-correction. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/forum?id=p53QDxSIc5. [32] Mohammadreza Pourreza, Hailong Li, Ruoxi Sun, Yeounoh Chung, Shayan Talaei, Gaurav Tarlok Kakkar, Yu Gan, Amin Saberi, Fatma Ozcan, and Sercan Arik. CHASESQL: Multi-path reasoning and preference optimized candidate selection in text-to-SQL. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?id=CvGqMD5OtX. [33] Bowen Qin, Binyuan Hui, Lihan Wang, Min Yang, Jinyang Li, Binhua Li, Ruiying Geng, Rongyu Cao, Jian Sun, Luo Si, Fei Huang, and Yongbin Li. survey on text-to-sql parsing: Concepts, methods, and future directions. In arXiv:2208.13629, 2022. [34] Ge Qu, Jinyang Li, Bowen Li, Bowen Qin, Nan Huo, Chenhao Ma, and Reynold Cheng. Before generation, align it! novel and effective strategy for mitigating hallucinations in text-to-SQL generation. In Findings of the Association for Computational Linguistics ACL 2024. Association for Computational Linguistics, August 2024. [35] Margo I. Seltzer, Keith Bostic, Michael Stonebraker, and Joseph M. Hellerstein, editors. Readings in Database Systems, 4th Edition. MIT Press, Cambridge, MA, 2005. [36] Ashish Thusoo, Joydeep Sen Sarma, Namit Jain, Zheng Shao, Prasad Chakka, Ning Zhang, Suresh Anthony, Hao Liu, and Raghotham Murthy. Hive - petabyte scale data warehouse using hadoop. In Proceedings of the 26th International Conference on Data Engineering, ICDE 2010, March 1-6, 2010, Long Beach, California, USA, pages 9961005. IEEE Computer Society, 2010. [37] Runchu Tian, Yining Ye, Yujia Qin, Xin Cong, Yankai Lin, Yinxu Pan, Yesai Wu, Hui Haotian, Liu Weichuan, Zhiyuan Liu, and Maosong Sun. DebugBench: Evaluating debugging capability of large language models. In Findings of the Association for Computational Linguistics: ACL 2024, pages 41734198, aug 2024. [38] Bing Wang, Changyu Ren, Jian Yang, Xinnian Liang, Jiaqi Bai, Linzheng Chai, Zhao Yan, Qian-Wen Zhang, Di Yin, Xing Sun, et al. Mac-sql: multi-agent collaborative framework for text-to-sql. In Proceedings of the 31st International Conference on Computational Linguistics, pages 540557, 2025. [39] Xingyao Wang, Yangyi Chen, Lifan Yuan, Yizhe Zhang, Yunzhu Li, Hao Peng, and Heng Ji. Executable code actions elicit better llm agents. In ICML, 2024. [40] Xingyao Wang, Boxuan Li, Yufan Song, Frank F. Xu, Xiangru Tang, Mingchen Zhuge, Jiayi Pan, Yueqi Song, Bowen Li, Jaskirat Singh, Hoang H. Tran, Fuqiang Li, Ren Ma, Mingzhang Zheng, Bill Qian, Yanjun Shao, Niklas Muennighoff, Yizhe Zhang, Binyuan Hui, Junyang Lin, Robert Brennan, Hao Peng, Heng Ji, and Graham Neubig. Openhands: An open platform for AI software developers as generalist agents. In The Thirteenth International Conference on Learning Representations, 2025. [41] Zuozhi Wang, Avinash Kumar, Shengquan Ni, and Chen Li. Demonstration of interactive runtime debugging of distributed dataflows in texera. Proc. VLDB Endow., 13(12):29532956, 2020. [42] Navid Yaghmazadeh, Yuepeng Wang, Isil Dillig, and Thomas Dillig. Sqlizer: query synthesis from natural language. Proceedings of the ACM on Programming Languages, 1(OOPSLA): 126, 2017. [43] John Yang, Akshara Prabhakar, Karthik Narasimhan, and Shunyu Yao. Intercode: Standardizing and benchmarking interactive coding with execution feedback. In Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2023. 13 [44] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik R. Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023. [45] Ziyu Yao, Daniel S. Weld, Wei-Peng Chen, and Huan Sun. Staqc: systematically mined In Proceedings of the 2018 World Wide Web question-code dataset from stack overflow. Conference on World Wide Web, WWW 2018, Lyon, France, April 23-27, 2018, pages 1693 1703. ACM, 2018. [46] Tao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga, Dongxu Wang, Zifan Li, James Ma, Irene Li, Qingning Yao, Shanelle Roman, Zilin Zhang, and Dragomir Radev. Spider: large-scale human-labeled dataset for complex and cross-domain semantic parsing and text-to-SQL task. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, page 39113921, 2018. [47] Tao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga, Dongxu Wang, Zifan Li, James Ma, Irene Li, Qingning Yao, Shanelle Roman, Zilin Zhang, and Dragomir Radev. Spider: large-scale human-labeled dataset for complex and cross-domain semantic parsing and text-to-SQL task. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 39113921, 2018. [48] Daoguang Zan, Zhirong Huang, Wei Liu, Hanwu Chen, Linhao Zhang, Shulin Xin, Lu Chen, Qi Liu, Xiaojian Zhong, Aoyan Li, et al. Multi-swe-bench: multilingual benchmark for issue resolving. arXiv preprint arXiv:2504.02605, 2025. [49] John M. Zelle and Raymond J. Mooney. Learning to parse database queries using inductive logic programming. In Proceedings of the Fourteenth National Conference on Artificial Intelligence and Ninth Conference on Innovative Applications of Artificial Intelligence, pages 10501055, 1996. [50] Bohan Zhai, Canwen Xu, Yuxiong He, and Zhewei Yao. Excot: Optimizing reasoning for text-to-sql with execution feedback. arXiv preprint arXiv:2503.19988, 2025. [51] Lunjun Zhang, Arian Hosseini, Hritik Bansal, Mehran Kazemi, Aviral Kumar, and Rishabh Agarwal. Generative verifiers: Reward modeling as next-token prediction. In The Thirteenth International Conference on Learning Representations, 2025. [52] Tianyu Zheng, Ge Zhang, Tianhao Shen, Xueling Liu, Bill Yuchen Lin, Jie Fu, Wenhu Chen, and Xiang Yue. OpenCodeInterpreter: Integrating code generation with execution and refinement. In Findings of the Association for Computational Linguistics: ACL 2024, pages 1283412859, Bangkok, Thailand, August 2024. Association for Computational Linguistics. [53] Yaowei Zheng, Richong Zhang, Junhao Zhang, Yanhan Ye, and Zheyan Luo. LlamaFactory: Unified efficient fine-tuning of 100+ language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations). Association for Computational Linguistics, 2024. [54] Qi Zhou, Joy Arulraj, Shamkant B. Navathe, William Harris, and Dong Xu. Automated verification of query equivalence using satisfiability modulo theories. Proc. VLDB Endowment, 12(11):12761288, 2019. [55] Terry Yue Zhuo, Vu Minh Chien, Jenny Chim, Han Hu, Wenhao Yu, Ratnadira Widyasari, Imam Nur Bani Yusuf, Haolan Zhan, Junda He, Indraneil Paul, Simon Brunner, Chen GONG, James Hoang, Armel Randy Zebaze, Xiaoheng Hong, Wen-Ding Li, Jean Kaddour, Ming Xu, Zhihan Zhang, Prateek Yadav, Naman Jain, Alex Gu, Zhoujun Cheng, Jiawei Liu, Qian Liu, Zijian Wang, David Lo, Binyuan Hui, Niklas Muennighoff, Daniel Fried, Xiaoning Du, Harm de Vries, and Leandro Von Werra. Bigcodebench: Benchmarking code generation with diverse function calls and complex instructions. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?id=YrycTjllL0."
        },
        {
            "title": "A Environment Setup Details",
            "content": "A.1 SQL Dialects Implementation For the implementation of SQL dialects, we set up sandbox environment using Docker2 containers. This environment consists of four database containers and one evaluation container, all managed via docker-compose.yml configuration. The databases used in this setup include: Table 4: SQL Dialects used in BIRD-CRITIC. Dialect Version URL PostgreSQL MySQL Microsoft SQL Server Oracle 14.12 8.4 Community Edition 2022 19.3.0 Developer Edition https://www.postgresql.org/ https://www.mysql.com/ https://www.microsoft.com/sql-server https://www.oracle.com/database/ Each of these databases is deployed in its own container, ensuring isolation and compatibility with the respective SQL dialects. The containers are connected through Docker Compose, allowing seamless interaction between the databases and the evaluation environment. A.2 Databases Migration & Modifications Our initial setup begins with the BIRD-SQL development database, which is based on SQLite. The migration process is carried out using Navicat3, powerful database management tool. This tool is used to migrate the original SQLite databases to the four SQL dialects mentioned above. After the migration, the schema structures of the databases are manually verified to ensure that they reflect the correct translations between different dialects. SQL queries, such as SELECT * FROM <table>, are executed to check data consistency and ensure that the migration retains the integrity of the original data. This step ensures that the translated databases can be used reliably for testing and evaluating SQL queries in the BIRD-CRITIC framework. A.3 Issue Collection Protocol User Issue Query Collection. StackOverflow, prominent online Q&A platform for software development under research-friendly license (CC BY-SA 4.0), is frequently utilized as primary data source for code-related evaluation research, [21, 45, 12]. To ensure the issue quality, we pre-define rigorous protocol based on 4 criteria: 1) presence of executable SQL code with identifiable errors or inefficiencies, 2) representation of significant database concepts from academic literature or real-world debugging practice, 3) appropriate complexity (queries exceeding 100 tokens or incorporating nontrivial function usage) and 4) sufficient contextual information to prevent ambiguity. We incorporate candidate issues that fulfilled at least 3 criteria, thereby assembling representative collection of SQL challenges that authentically reflect the obstacles encountered in professional database application environments. Annotators meticulously review the reproduced issue (P, S, σissue) and craft new σ. This annotation requires ensuring that σ: (1) Correctly Implements Intent: Accurately fulfills the users objective as inferred from and the context of σissue. (2) Resolves the Error: Explicitly fixes the identified flaw(s) in σissue. (3) Is Functionally Correct: Executes successfully on the target database instance (conforming to S) within the specified dialect and produces the expected, correct results. (4) Adheres to Best Practices: Solution SQLs should present reasonably efficient and well-formed query. As shown in Figure 1, this results in curated \"Solution Query\" (σ) paired with the user query and issue SQLs. Finally, to ensure robust evaluation, we annotate each task with evaluation scripts consisting of specific test cases written by Python and SQLs. Details can be found in Appendix C. 2https://www.docker.com 3https://www.navicat.com Figure 7: Examples of training materials by screenshots for BIRD-CRITIC annotators. Left: Docker setup instructions for creating the standardized annotation environment. Middle: Data annotation tutorials with detailed procedures for reproducing SQL issues. Right: Entry examination outline used to evaluate annotator proficiency across various SQL debugging challenges."
        },
        {
            "title": "B Annotator Qualification",
            "content": "B.1 Annotator Entrance Test To ensure high-quality annotations for the BIRD-CRITIC benchmark, we implemented rigorous training process for all annotators. Each potential annotator underwent comprehensive training program before contributing to the benchmark creation. B.2 Training Tutorial Annotators participated in an intensive tutorial program covering essential aspects of SQL issue debugging, including: Database environment setup Database schema analysis and comprehension SQL error identification patterns and common debugging approaches Systematic issue reproduction techniques Solution validation and evaluation script development Best practices for creating test cases across different SQL dialects (PostgreSQL, MySQL, Oracle, and SQL Server) The training materials included detailed documentation, practical examples, and hands-on exercises that mirrored the complexity and diversity of real-world SQL issues. Annotators were introduced to the specific annotation workflow required for BIRD-CRITIC benchmark creation. B.3 Qualification Test Following the week-long training phase, each candidate annotator was required to complete qualification test consisting of ten representative SQL issue debugging tasks. For each task, candidates had to: 1. Correctly identify the underlying issue in the problematic SQL 2. Reproduce the issue in the controlled environment 3. Develop solution SQL that resolved the identified problems 4. Create comprehensive test cases to validate solution correctness 5. Document their reasoning and approach 16 Only candidates who successfully completed all ten tasks with satisfactory quality were approved as annotators for the BIRD-CRITIC benchmark. This stringent qualification process ensured that all annotators met the high standards required for creating robust and trustworthy benchmark. The qualification test success rate was approximately 90%, indicating the effectiveness of our tutorial materials and instruction program in preparing candidates for SQL issue debugging tasks. All annotators who contributed to the final BIRD-CRITIC benchmark successfully passed this qualification process."
        },
        {
            "title": "C Evaluation Script Details",
            "content": "To rigorously evaluate the correctness and suitability of generated SQL solutions (σpred), particularly in the context of issue resolution, evaluation methodologies must extend beyond superficial syntactic checks or simple result set comparisons. We annotate each task with specific test case functions, which encompass four categories of SQL issue types in BIRD-CRITIC: Query-like Issues: Predominantly for conventional SELECT queries. Given that BIRDCRITIC already provides issue SQLs that deliver original user intents, the solution SQLs must preserve these intentions while addressing identified problems. This protocol assesses correctness by executing σpred and the ground-truth σ on the database instance and verifying the semantic equivalence of their result sets, typically accommodating variations in tuple ordering unless explicitly constrained by the task specifications. Management Issues: Essential for tasks involving Data Manipulation Language (DML: UPDATE, INSERT, DELETE), Data Definition Language (DDL: CREATE, ALTER), Data Control Language (DCL: GRANT, REVOKE), or complex multi-step procedures. For these cases, domain experts manually design test cases to ensure that the results executed by σpred fulfill the specified user requirements. Personalization Issues: For tasks imposing specific syntactic or semantic constraints on the solution (e.g., mandatory use of certain SQL features, avoidance of others, derived from the problem description P), this category extends the test case functions of the previous two categories while enforcing additional compliance criteria."
        },
        {
            "title": "D Evaluation Metrics",
            "content": "In BIRD-CRITIC, we adopt the Task Resolution Success Rate (SR %) as metric. This metric measures the percentage of tasks for which model generates SQL solution σpred that successfully passes the all curated test cases in the evaluation script. Formally, let be the total number of tasks in the evaluation set, and let Ti represent the dedicated evaluation script designed for task i. generated solution σpred,i for task is considered successful if and only if Ti(σpred,i) returns passing outcome (returns True). The overall Success Rate is then calculated as: SR = 1 (cid:88) i=1 I(Ti(σpred,i) = True) where I() denotes the indicator function, evaluating to 1 if the condition is true and 0 otherwise. This metric directly leverages the outcomes of our comprehensive, category-aware test case framework. Since each test function Ti is tailored to the specific nature of the users issue, evaluating semantic equivalence of results (Soft EX), correctness of database state transitions, adherence to explicit constraints via parsing as appropriate, the SR provides holistic measure of models capability. It assesses the models ability to generate solutions that are not merely executable, but are functionally correct and contextually appropriate for resolving the specific problem presented in the task instance (P, S, σissue). We argue that this success rate provides more rigorous and practically relevant assessment of SQL issue resolution capabilities compared to metrics focused solely on execution or partial component matching. 17 Table 5: Data statistics of features in BIRD-CRITIC compared to related benchmarks. []: Results taken from public available Spider 2.0 Lite Gold SQL. EM refers to the Exact Match, EX refers to Execution Accuracy, and PCM-F1 refers Partial Component Match F1. Dataset # Eval # Toks. / # Toks. / SQL Evaluation Metric Non Query-like Multi-Dialect Spider 1.0 SEDE BIRD-SQL Spider 2.0 BEAVER BIRD-CRITIC PG BIRD-CRITIC MULTI 1,034 857 1,543 547 203 530 570 14.28 14.34 18.36 61.93 59.27 307.35 296. 30.18 101.3 50.01 412.37 538.13 111.47 112.64 EM/EX PCM-F1 EX EX EX Test Cases Test Cases"
        },
        {
            "title": "E More Statistics",
            "content": "E.1 Comparison of BIRD-CRITIC with other conversational Text-to-SQL benchmarks This section compares BIRD-CRITIC with other benchmarks, highlighting its advantages in handling significantly longer user queries and supporting non-query-like SQL statements (e.g., DML, DDL), which present additional challenges. Additionally, the custom-designed test cases ensure faithful evaluation of SQL solutions, while the multi-dialect support enables more comprehensive evaluation across diverse environments E.2 Detailed Statistics of BIRD-CRITIC-MULTI This section focuses on the detailed statistics of the BIRD-CRITIC-MULTI dataset, emphasizing its support for multiple SQL dialects and showcasing the distribution of query types, SQL issues, and test cases across diverse dialects. Table 6: Statistics grouped by Category and Dialect Count Query Issue SQL Solution SQL Test Cases Category Query Management Personalization Dialect PostgreSQL MySQL Oracle SQLServer 304 104 162 276 98 98 98 Mean Max Mean Max Mean Max Mean Max 179.12 141.44 146. 1058 519 528 168.20 68.80 100.21 1262 267 1073 126.72 102.76 113.01 152.61 152.86 171.52 192.17 1058 435 421 78.17 65.12 265.36 214.31 1073 230 1262 798 103.45 93.40 155.92 145.89 853 578 778 578 778 853 542 80.29 189.53 160. 151.30 93.34 93.95 95.57 134 733 517 733 281"
        },
        {
            "title": "F Algorithm",
            "content": "F.1 SQL Rewind Algorithm We formalize the end-to-end SQL-Rewind pipeline in Algorithm 1, outlining each stage from raw post extraction to the construction of high-quality training tuples. 18 Algorithm 1 Automatic construction of SIX-GYM training instances with SQL-Rewind. Require: Draw (Stack Overflow posts), (training databases); target_size; max_iter Ensure: target_size procedure SQL_REWIND for each post in Draw do if OVERLAP_WITH_BIRD_CRITIC(post) then collected training tuples continue end if EXTRACT_SQL(post) for each sql in do for each db in do regex extraction sol_sql ADAPT_SCHEMA(sql, db) if EXEC_OK(sol_sql, db) then for 1 to max_iter do issue synthesis and verification (σissue, rissue, ) GEN_ISSUE(sol_sql, db) if VALIDATE(σissue, rissue, T, sol_sql, db) then break end if end for if validation failed then continue end if for 1 to max_iter do GEN_USER_QUERY(σissue, rissue, T, db) if CONSISTENT(P, σissue, T, sol_sql) then break end if end for if consistency failed then continue end if {db.S, P, σissue, T, sol_sql} if target_size then break all loops user query generation end if end if end for end for end for return end procedure F.2 BIRD-FIXER Algorithm"
        },
        {
            "title": "G Error Analysis Details",
            "content": "Figure 8 shows examples for each error type, along with an analysis of why the LLM-generated SQL failed the issue SQL query resolution."
        },
        {
            "title": "H Experiment Details",
            "content": "H.1 Alias of LLMs The following aliases are used for the models in this work: Claude-3.7-Sonnet: claude-3-7-sonnet-20250219 Claude-3.7-Sonnet-Thinking: refers to claude-3-7-sonnet-20250219 with extended thinking Algorithm 2 BIRD-FIXER: Functional planning, backward inference, and forward validation for SQL issue fixing. Require: P, S, σissue; σ, ; = (f1, . . . , fk) Ensure: Trajectory τ = ((t1, σ1, o1), . . . , (tn, σn, on)) Function: BIRD-FIXER procedure FUNCTIONALPLAN Annotate symbolic functional plan = (f1, . . . , fk) from teacher LLM for each fi in do fi represents an abstract debugging operation mapping σissue to σ end for end procedure procedure BACKWARDINFERENCE Given the problem (P, S, σissue) and the corrected query σ Generate step-by-step functional plan = (f1, . . . , fk) is annotated by the teacher LLM to map σissue to σ end procedure procedure FORWARDVALIDATION Using (P, S, σissue) and candidate plan Regenerate solution using SQL-ACT with teacher LLM if Regenerated SQL passes all test cases in then Accept Retain executable trace τ = ((t1, σ1, o1), . . . , (tn, σn, on)) else Discard plan end if end procedure Figure 8: Detailed Error Analysis 20 O3-Mini: O3-Mini-2025-01-31 O1-Preview: O1-Preview-2024-09-12 GPT-4.1: gpt-4.1-2025-04-14 Gemini-2.0-Flash: gemini-2.0-flash Gemini-2.0-Flash-Thinking: gemini-2.0-flash-thinking-exp-01-21 deepseek-v3: deepseek-chat deepseek-r1: deepseek-reasoner All open-source models are downloaded from Hugging Face4: Llama: Meta-Llama-3.1-8B-Instruct, Meta-Llama-3.3-70B-Instruct Qwen-Coder: Qwen2.5-Coder-7B-Instruct, Qwen2.5-Coder-14B-Instruct, Qwen2.5-Coder-32B-Instruct Phi: Phi-4 H.2 Model Implementation Details For inference with proprietary models, we use official API providers, including OpenAI (https: //openai.com/), Anthropic (https://www.anthropic.com/), Google (https:// gemini.google.com/), and Deepseek (https://www.deepseek.com/). The total API cost for proprietary models is around $200 USD. For open-source models, we fine-tune all our models using the LlaMa-Factory library [53] (version 0.9.2) https://github.com/hiyouga/LLaMA-Factory with LoRA [15]. All our experiments are conducted on 8H100 GPU with 80GB memory. We set the low-rank dimensions as 8, the learning rate as 5e5, and the batch size as 4. The specific training hours for each backbone model are shown in Table 7. We use VLLM5 (version 0.6.4.post1) to perform inference. We set the temperature as 0.1, the top as 0.95, and the maximum input token length as 8000. We report the experimental results as the average of five repeated trials. The total GPU hours spent on inference are approximately 20 hours. Table 7: GPU hours spent to train each backbone model."
        },
        {
            "title": "GPU Hours",
            "content": "Meta-Llama-3.1-8B Qwen2.5-Coder-7B Qwen2.5-Coder-14B Phi-4 24.88 22.00 35.93 31.42 H.3 Agent Implementation Details All agent designs follow the ReAct framework [44], which uses interleaving Thought, Action, Observation steps. Specifically: SQL-ACT: The action is the freedom to execute any executable SQL query. Tool-ACT: Actions are predefined and include: Schema Inspection: Reveals table/column information. Sample Data: Previews example rows from table. Solution Query: The final, correct SQL query that resolves the users issue. 4https://huggingface.co/ 5https://docs.vllm.ai/en/latest"
        },
        {
            "title": "I Limitation And Future Work",
            "content": "Our work primarily focuses on SQL content and knowledge by simplifying the impact of external workflows through containerized Docker environments. Workflow operations such as file reading and editing represent important considerations for future development in BIRD-CRITIC 1.5. Actually, We conducted preliminary experiments on models performing workflow-integrated content-based tasks, where LLMs not only check and revise SQL issues but also save results to files. This integration resulted in substantial performance drop, with success rates dropping from approximately 30% to 10%. However, we prioritize SQL knowledge improvement in this work since significant opportunities for advancement remain in this domain. Similar to most complex task evaluations [55], BIRD-CRITIC employs single-turn evaluation while striving to make task descriptions as clear as possible. However, real-world applications typically require crucial interaction between users and agents since most users cannot articulate their intents or queries with complete clarity and may need multi-turn interactions for clarification or additional information processing. Our recent work, BIRD-Interact6, evaluates text-to-SQL performance of LLM agents through dynamic interaction by multi-turn conversational and agentic interactions. Future work will extend BIRD-CRITIC to incorporate dynamic user-SQL debugging processes, better simulating the complexity of real-world agent-human interactionsá."
        },
        {
            "title": "J Broader Impact",
            "content": "Our work presents an approach to training open-source models specifically designed for debugging SQL issues. Additionally, we introduce workflow for constructing robust benchmarks from diverse open platforms, such as StackOverflow, through reproducible loop to mitigate potential data leakage. Furthermore, our research primarily targets technical SQL knowledge within the programming domain. Thus, it does not directly engage with or pose risks concerning broader societal issues."
        },
        {
            "title": "K Prompt",
            "content": "Baseline Prompt for resolving SQL issues with an LLM You are SQL assistant. Your task is to understand user issue and correct their problematic SQL given the database schema. Please wrap your corrected SQL with ```sqln[Your Fixed SQL]n``` tags in your response. Database Schema: {SCHEMA} User issue: {USER_ISSUE} Problematic SQL: {ISSUE_SQL} Corrected SQL: 6https://bird-interact.github.io/"
        },
        {
            "title": "Prompt used to generate Thought",
            "content": "Interact with the \"{db_id}\" database using PostgreSQL to solve the user issue. You will be given the following information: 1. Database schema: complete CREATE TABLE ... DDL. 2. User Issue: natural language description of the desired outcome or the current bug. 3. Problematic SQL: the query (or queries) that presently fail to meet the requirement. Use interleaving Thought, Action, Observation steps. Thought can reason about the possible errors or other information you think you need for debugging about the current situation. For instance, it could be: Diagnosis of the bug you see in the current query. Hypotheses you want to confirm (e.g., Maybe the join is missing date filter). Reasoning that led you to the next SQL step (checking row counts, inspecting NULLs, etc.). brief plan for what you will try next. Action can only be the executable PostgreSQL SQL. The Observation would be the execution results feedback from the environment. Wrap your thought in the <thought>[Your Thought]</thought> tag and your action in <action>[Executable SQL]</action>. The input for you is as follows: Database Schema {SCHEMA} User Issue {USER_ISSUE} Problematic SQL {ISSUE_SQL} Important Rules: MOST IMPORTANT: Wrap <thought>[Your Thought]</thought> tag and your action in the <action>[Executable SQL]</action> tag. thought your the in The action inside the <action></action> tags must be pure PostgreSQL statements that can be executed directly, without any comments or needs for additional post-processing. Now generate the thought and action of the next round given the trajectory history and the input. You still have {turn} turns left. React {history} <thought>"
        },
        {
            "title": "Prompt used to generate Action",
            "content": "Interact with the \"{db_id}\" database using PostgreSQL to solve the user issue. You will be given the following information: 1. Database schema: complete CREATE TABLE ... DDL. 2. User Issue: natural language description of the desired outcome or the current bug. 3. Problematic SQL: the query (or queries) that presently fails to meet the requirement. Use interleaving Thought, Action, Observation steps. Thought can reason about the possible errors or other information you need for debugging about the current situation. For instance, it could be: Diagnosis of the bug you see in the current query. Hypotheses you want to confirm (e.g., Maybe the join is missing date filter). Reasoning that led you to the next SQL step (checking row counts, inspecting NULLs, etc.). brief plan for what you will try next. Action can only be the executable PostgreSQL SQL according to the corresponding thought. The Observation would be the execution results feedback from the environment. Your task is to generate the action for the current round thought given the react history. Wrap your action in <action>[Executable SQL]</action>. If you think the debugging process is done, just output <action>[DONE]</action> as the action. The input for you is as follows: Database Schema {SCHEMA} User Issue {USER_ISSUE} Problematic SQL {ISSUE_SQL} Important Rules: MOST IMPORTANT: Wrap your action in <action>[Executable SQL]</action>. The action inside the <action></action> tags must be pure PostgreSQL statements that can be executed directly, without any comments or needs for additional post-processing. If you believe the debugging process is finished, output <action>[DONE]</action> as the action for this turn. Now generate the action of this round given the trajectory history and current thought. Generating multiple rounds at once is NOT ALLOWED! You still have {turn} turns left. React {history} <action>"
        },
        {
            "title": "Prompt used to generate Corrected SQL",
            "content": "You are text-to-SQL expert. You will be given the following information: 1. Database schema: complete CREATE TABLE ... DDL. 2. User Issue: natural language description of the desired outcome or the current bug. 3. Problematic SQL: the query (or queries) that presently fails to meet the requirement. 4. React Thought Chain: history of your prior debugging iterations, formatted as sequence of thought action observation tuples. Each tuple is separated from the next by blank line ( n). Thought - Your reasoning: hypotheses about errors, assumptions, or additional data requirements. Action - pure PostgreSQL statement executed to test or correct the issue. Observation - The execution result returned by the database engine. Your task is to understand all these contents and generate the final PostgreSQL that could accurately solve the user issue. The input is as follows: Database Schema {SCHEMA} User Issue {USER_ISSUE} Problematic SQL sql_list = {ISSUE_SQL} React Thought Chain {HISTORY} Now generate the final PostgreSQL that could accurately solve the user issue and could be directly executed. Wrap your answer in the sql n[Your Answer] tag. Do not give me extra explanations or comments. 25 BIRD-Fixer Example Figure 9: An illustrative example of the BIRD-Fixer debugging workflow."
        }
    ],
    "affiliations": [
        "CUHK",
        "CUHKSZ",
        "Google Cloud",
        "HKU STAR Lab",
        "THU",
        "The BIRD Team"
    ]
}