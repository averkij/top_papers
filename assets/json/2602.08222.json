{
    "paper_title": "Weak-Driven Learning: How Weak Agents make Strong Agents Stronger",
    "authors": [
        "Zehao Chen",
        "Gongxun Li",
        "Tianxiang Ai",
        "Yifei Li",
        "Zixuan Huang",
        "Wang Zhou",
        "Fuzhen Zhuang",
        "Xianglong Liu",
        "Jianxin Li",
        "Deqing Wang",
        "Yikun Ban"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "As post-training optimization becomes central to improving large language models, we observe a persistent saturation bottleneck: once models grow highly confident, further training yields diminishing returns. While existing methods continue to reinforce target predictions, we find that informative supervision signals remain latent in models' own historical weak states. Motivated by this observation, we propose WMSS (Weak Agents Can Make Strong Agents Stronger), a post-training paradigm that leverages weak checkpoints to guide continued optimization. By identifying recoverable learning gaps via entropy dynamics and reinforcing them through compensatory learning, WMSS enables strong agents to improve beyond conventional post-training saturation. Experiments on mathematical reasoning and code generation datasets show that agents trained with our approach achieve effective performance improvements, while incurring zero additional inference cost."
        },
        {
            "title": "Start",
            "content": "Weak-Driven Learning: How Weak Agents make Strong Agents Stronger Zehao Chen * 1 2 Gongxun Li * 1 2 Tianxiang Ai * 2 Yifei Li 1 2 Zixuan Huang 1 Wang Zhou 2 Tao Huang 2 Fuzhen Zhuang 1 Xianglong Liu 1 Jianxin Li 1 Deqing Wang 1 Yikun Ban 1 6 2 0 2 9 ] . [ 1 2 2 2 8 0 . 2 0 6 2 : r Abstract As post-training optimization becomes central to improving large language models, we observe persistent saturation bottleneck: once models grow highly confident, further training yields diminishing returns. While existing methods continue to reinforce target predictions, we find that informative supervision signals remain latent in models own historical weak states. Motivated by this observation, we propose WMSS (Weak Agents Can Make Strong Agents Stronger), post-training paradigm that leverages weak checkpoints to guide continued optimization. By identifying recoverable learning gaps via entropy dynamics and reinforcing them through compensatory learning, WMSS enables strong agents to improve beyond conventional post-training saturation. Experiments on mathematical reasoning and code generation datasets show that agents trained with our approach achieve effective performance improvements, while incurring zero additional inference cost. Code is available at https://github. com/chenzehao82/Weak-Driven-Learning.git. 1. Introduction The dominant post-training paradigms, including Supervised Fine-Tuning (SFT) (Ouyang et al., 2022; Touvron et al., 2023; Zou et al., 2025; Chen et al., 2025), Knowledge Distillation (KD) (Hinton et al., 2015; Gou et al., 2021; Agarwal et al., 2023), and Curriculum Learning (Bengio et al., 2009; Xu et al., 2023), share common principle: learning from stronger supervision signals. These approaches rely on mimicking objective, training models to approximate high-quality labels or the outputs of stronger teacher, under the assumption that closer alignment leads to improved performance. *Equal contribution Corresponding Author 1Beihang University 2China Telecom eSurfing Cloud. If you have any questions, feel free to contact Yikun Ban <yikunb@buaa.edu.cn>, or Zehao Chen <gyy chenzehao@chinatelecom.cn or zehaochenacid@buaa.edu.cn>. Figure 1. Paradigm Comparison: Distillation-Based Learning vs. Weak-Driven Learning. While highly effective during early training, such paradigms are increasingly observed to suffer from performance saturation as optimization proceeds (Chen et al., 2024; Dong et al., 2024a). Specifically, the logit margindefined as the gap between the target logit and the average non-target logitsgrows rapidly in early epochs but stabilizes thereafter. Once this margin saturates, the decision boundary becomes effectively fixed, and gradients induced by standard supervised objectives diminish, limiting further improvement. Existing remedies, such as continued SFT or self-revision and reflection-based fine-tuning, have shown limited effectiveness in addressing this saturation bottleneck (Gudibande et al., 2023; Zhou et al., 2024; Huang et al., 2024; Stechly et al., 2024). Despite architectural or procedural differences, these methods still rely on reinforcing correct targets and therefore struggle to provide informative learning signals once supervised gradients vanish. In this work, we approach this challenge from fundamentally different perspective by introducing new post-training paradigm, termed Weak-Driven Learning. Our approach is inspired by common human learning phenomenon: in complex collaborative problem-solving, strong individual working alongside weaker teammate is often forced to further refine their reasoning in order to complete the task successfully. Crucially, this improvement does not arise from repeatedly applying the correct solution, but from observing, analyzing, and correcting the weaker teammates mistakes. Such mistakes expose plausible yet incorrect reasoning paths that must be explicitly ruled out, thereby sharpening the distinction between correct and incorrect decisions. Guided by this insight, we challenge the conventional as1 Weak-Driven Learning: How Weak Agents make Strong Agents Stronger sumption that learning with weaker models necessarily degrades performance. Instead, we show that weak agents can provide informative error signals that continue to drive improvement even when standard supervision saturates. We formalize this principle as Weak-Driven Learning, capturing the idea that Weak Agents can Make Strong Agents Stronger. Unlike knowledge distillation, which depends on access to stronger teacher that is often expensive or unavailable, weak-driven learning leverages weak reference models that are easy to obtain, such as historical checkpoints of the model itself. As illustrated in Figure 1, weak-driven learning fundamentally inverts the conventional learning flow. Instead of transferring knowledge from stronger teacher to weaker student, weak agent injects structured uncertainty and exposes failure modes that compel an already strong agent to further refine its decision boundary. By explicitly identifying and distancing the strong model from these failure modes, learning can continue beyond the saturation point of standard supervision. Rather than discarding early-stage checkpoints as inferior intermediates, our approach actively reuses them as source of corrective supervision, enabling sustained model evolution without introducing additional inference cost. To distinguish this paradigm clearly from existing posttraining strategies, we now provide formal definition of Weak-Driven Learning. Definition: Weak-Driven Learning Weak-Driven Learning refers to class of post-training paradigms in which the improvement of strong model is driven by systematic discrepancies between its predictions and those of weaker reference model (e.g., historical checkpoint), rather than by imitation of stronger teacher. Formally, given strong agent Mstrong, weak agent Mweak, and task dataset D, weak-driven learning constructs training signals by jointly leveraging the outputs of Mstrong and Mweak on D. These joint signals are then used to further optimize the strong agent. As result, the strong agent Mstrong is updated into stronger agent M+ strong with improved task performance. Our contributions are summarized as follows: Learning paradigm. We introduce Weak-Driven Learning, new post-training paradigm that highlights the overlooked role of weak agentssuch as historical model checkpointsas driving signals that can further improve strong agents. Training framework. We propose WMSS, practical post-training framework that operationalizes weak-driven 2 learning through joint optimization of weak and strong models via logit mixing. This mechanism compels the strong model to refine its decision boundary and sustain meaningful gradients in saturated regimes, without additional inference overhead. Theoretical analysis. We provide gradient-level analysis of the joint training mechanism, theoretically demonstrating how incorporating weak-model logits reshapes the optimization landscape, prevents gradient vanishing on non-target tokens, and maintains effective learning pressure beyond standard supervision. Empirical performance. We empirically demonstrate that WMSS consistently improves performance on challenging benchmarks, including mathematical reasoning and code generation, compared to standard SFT baselines. These gains arise purely from improved optimization dynamics during training and incur no additional inference cost. 2. Related Work Post-training Paradigms: From SFT to Knowledge Distillation. Supervised Fine-Tuning (SFT) has established itself as the cornerstone of aligning Large Language Models (LLMs) (Ouyang et al., 2022; Touvron et al., 2023; Yang et al., 2026; Huang et al., 2026; He et al., 2025). To further enhance efficiency, Knowledge Distillation (KD) is frequently integrated into post-training. Traditional approaches, including standard KD (Hinton et al., 2015) and Self-Distillation methods (Zhang et al., 2019; Xu et al., 2023), typically operate on premise of mimicry: the model is optimized to approximate superior distribution, derived either from larger teacher or its own smoothed predictions. Recent LLM-specific variants like GKD (Agarwal et al., 2023) and MiniLLM (Gu et al., 2023) refine this by addressing distribution mismatch, yet they retain the core Strong-to-Weak or Self-Smoothing logic. While effective for knowledge transfer, these mimicry-based objectives share critical limitation with SFT where the diminishing supervisory gradients near convergence lead to optimization saturation and logit rigidification. In contrast, WMSS abandons the mimicry objective. We utilize historical weak checkpoints not as targets to approximate, but as sources of structured uncertainty to reactivate gradients, thereby breaking the optimization saturation barrier where SFT and standard KD stagnate. Weak-to-Strong Generalization & Supervision. The potential for weak supervisors to elicit superior capabilities has attracted significant interest. Burns et al. (Burns et al., 2023) formalized the phenomenon of weak-to-strong generalization, demonstrating that strong models can generalize beyond the imperfect labels provided by weaker supervisors. This paradigm has been further extended to iterative Weak-Driven Learning: How Weak Agents make Strong Agents Stronger Figure 2. Weak-Driven Learning. Overview of WMSS. The framework has three phases: (1) initialization, (2) activate SFT data via curriculum learning, and (3) jointly train weak and strong models to obtain stronger model; The right panel visualizes the joint-training principle through logit mixing and gradient amplification. self-improvement (Gulcehre et al., 2023). Unlike these methods which focus on elicitation in the absence of ground truth, WMSS addresses optimization saturation in fully supervised settings. We utilize historical weak logits not as targets to follow, but as corrective signals to sustain gradient flow. 3. Preliminaries We consider an autoregressive language model Mθ that maps context sequence to next-token distribution over vocabulary V. At decoding step t, the model outputs logits zt RV, which induce probability distribution Pθ( x) = Softmax(zt). (1) Supervised fine-tuning. In standard supervised fine-tuning (SFT), we minimize the negative log-likelihood over dataset D: LSFT(θ) = E(x,y)D [log Pθ(y x)] (2) of the next-token distribution: H(Pθ( x)) = (cid:88) vV Pθ(v x) log Pθ(v x). (3) Let ℓ(x, y; θ) = log Pθ(y x) be the per-token crossentropy loss. Its gradient w.r.t. the logit of any token is ℓ zt[k] = Pθ(k x) I[k = y]. In particular, for any negative token = y, we have (cid:12) (cid:12) (cid:12) (cid:12) ℓ zt[k] (cid:12) (cid:12) (cid:12) (cid:12) = Pθ(k x) (4) (5) i.e., the gradient magnitude on negative class is proportional to its assigned probability. Weak-driven learning explicitly leverages this property: by reintroducing probability mass to plausible but suppressed alternatives through logit mixing, it amplifies informative gradients on unresolved decision regions, thereby sustaining effective optimization beyond the supervised saturation regime. where denotes the ground-truth next token under context (we omit the time index when clear). Predictive entropy. To quantify uncertainty and diagnose optimization stagnation, we monitor the predictive entropy 4. How Can Weak Make Strong Stronger In this section, we describe how weak agent can be leveraged to further strengthen strong agent. 3 Weak-Driven Learning: How Weak Agents make Strong Agents Stronger Weak and strong agent initilization. In WMSS, the weak agent Mweak (a.k.a. Mref ) is initialized from the base checkpoint and co-trained with the strong agent during joint training. We begin with Phase 1 (Initialization): starting from base model M0, we perform standard SFT to obtain M1, and set Regression repair (γ). If Hi > 0, the strong agent is less certain than the weak reference. Since the weak model handled the sample better, this is strong evidence the sample is learnable (not irreducible noise) and indicates regression/catastrophic forgetting; we up-weight such samples to recover the lost boundary. Mweak M0, Mstrong M1. (6)"
        },
        {
            "title": "The weak agent provides a corrective signals via its logits",
            "content": "zweak(x) RV (7) which preserve softer decision boundary and highlight plausible distractors(non-target tokens), stabilizing continued optimization. 4.1. Curriculum-Enhanced Data Activation At this stage, training data for the strong agent Mstrong is constructed via curriculum-enhanced data activation process guided by the weak reference Mweak. Rather than uniformly sampling from fixed corpus, we use the weak strong model pair to diagnose uncertainty dynamics on individual samples and selectively activate data that is most informative for continued learning. Entropy dynamics. For each sample xi, define predictive entropy under model as H(M; xi) = H(PM( xi)) . (8) We measure how uncertainty changes from the weak reference to the current strong agent by Hi = H(Mstrong; xi) H(Mweak; xi). (9) Here, H(Mweak; xi) captures historical difficulty, while Hi indicates whether the strong agent has become more certain (learned) or more uncertain (regressed) than the weak reference. Curriculum construction. We sample training examples using mixture of three entropy-based signals: pi α H(Mweak; xi) + β (cid:2)Hi + + γ (cid:2)Hi (cid:3) (cid:3) + (10) where [u]+ := max(u, 0) and normalization is over i. The three terms have an intuitive role: Base difficulty (α). Up-weights samples that the weak reference found uncertain, ensuring inherently hard concepts remain emphasized. Consolidation (β). If Hi < 0, the strong agent is more certain than the weak reference; large decreases can indicate fast but brittle learning, so we revisit these samples to stabilize retention. 4 The resulting curriculum-enhanced data activation yields the training set for the subsequent stage, in which explicit weak guidance is incorporated during training. 4.2. Weak-Driven Learning: Joint Training of Weak and Strong The core operation of WMSS is Joint Training: given weak agent Mweak and strong agent Mstrong, we convert the weak agents reference signals into additional optimization pressure, driving the strong agent toward an even stronger checkpoint M+ strong through gradient amplification. This realizes the principle Weak + Strong stronger Strong. Joint training via logit mixing. For training pair (x, y), let zstrong(x), zweak(x) RV denote the next-token logits produced by Mstrong and Mweak, respectively. We construct joint probability map by linearly mixing the logits: zmix(x) = λ zstrong(x) + (1 λ) zweak(x), (11) where λ [0, 1]. We then optimize the strong agent using the mixed-logit distribution Pmix( := Softmax(zmix(x)): x) Lmix = E(x,y)D [log Pmix(y x)] (12) Gradients propagate through zmix via standard backpropagation. Intuitively, the weak agent assigns non-negligible probability mass to plausible but incorrect tokens that the strong agent may have already suppressed. By mixing logits, these hard negatives are reintroduced under Pmix, preventing their associated gradients from vanishing. This mechanism reactivates informative learning signals in saturated regions of the decision space, enabling continued refinement of the strong agents decision boundary. 4.3. Training Pipeline Algorithm 1 summarizes WMSS. Phase 1 (Initialization) trains M0 with SFT to obtain Mstrong and initializes Mweak M0, Mstrong M1. Phase 2 (Curriculum) constructs an active training distribution by computing entropy dynamics between Mweak and the current Mstrong, yielding curriculum-weighted dataset Dactive via Eq. (10). Phase 3 (Weak-driven Training) then performs joint training on Dactive by mixing logits as in Eq. (11) and optimizing Weak-Driven Learning: How Weak Agents make Strong Agents Stronger // Step A: Curriculum-Enhanced Data Activation Calculate via Eq. 10 using and H(Mt1) Algorithm 1 Weak Agents make Strong Agents Stronger Require: Dataset D, Base Model M0 Require: Iterations K, Hyperparams λ 1: Phase 1: Initialization 2: M1 Train(M0, D, LSFT) 3: Phase 2: Iterative Training Loop 4: for = 1 to do 5: 6: H(Mt) H(Mt1) 7: 8: Dactive WeightedSample(D, p) // Step B: Weak-Driven Learning 9: Initialize Mθ Mt 10: for batch (x, y) Dactive do 11: 12: 13: 14: 15: end for 16: 17: Mt+1 Mθ 18: end for 19: return MK+1 zweak Forward(Mt1, x) zstrong Forward(Mθ, x) zmix λzstrong + (1 λ)zweak Update Mθ via CE(zmix, y) the mixed-logit cross-entropy loss in Eq. (12). This allows Mstrong to suppress weak-revealed distractors and continue strengthening beyond saturation, producing M+ strong. 5. Why Can Weak Make Strong Stronger In this section, we formalize the key advantage of weakdriven learning: joint logit mixing with weak agent amplifies informative gradients in saturated regions, enabling continued optimization of strong agent beyond standard supervised fine-tuning. We show that, by reintroducing probability mass to plausible but suppressed alternatives, joint training prevents gradient vanishing on non-target tokens and sustains effective learning pressure even after the strong models decision boundary has stabilized. 5.1. Why Joint Training Amplifies Gradients in Saturated Regions Consider weak agent Mweak (initialized from historical checkpoint and co-trained during joint training) and strong agent Mstrong (current trainable agent) that, for context x, produce logits zweak(x), zstrong(x) RV over the vocabulary V, and the mixed logits zmix(x) = (1 λ)zweak(x) + λzstrong(x), λ [0, 1]. (13) Operationally, zmix(x) is the fused logit map used for joint training, injecting the weak agents uncertainty while preserving the strong agents target direction. Let denote the target index, ey the one-hot vector, and define Pmix( x) = Softmax(zmix(x)) and Pstrong( x) = 5 Softmax(zstrong(x)). More generally, for any logit map z(x), let Pz( x) = Softmax(z(x)). The cross-entropy gradient on fused logits is gmix = zmix(x)L = Pmix( x) ey, (14) For any negative token = y, the gradient component is gmix[k] = Pmix(k x), so increasing negative probability mass directly increases the gradient magnitude on that token. Standard SFT on the strong agent gives gsft = Pstrong( x) ey and can saturate when Pstrong(k x) is already small. Margins and hard negatives. Define the target margin for any negative token = as mk(z(x)) = z(x)[y] z(x)[k]. (15) The margin measures separation: smaller margins mean higher confusion. We define the hard-negative set target-vs-distractor = {k = : mk(zweak(x)) < mk(zstrong(x))}. (16) These are tokens where the weak agent is less separated than the strong agent, i.e., the weak agent highlights unresolved boundaries. The mixed margin is convex combination: mk(zmix(x)) = (1 λ)mk(zweak(x)) + λmk(zstrong(x)). (17) Thus mixing shrinks margins toward the weak agent on H, raising the relative probability of hard negatives. Theorem 5.1 (Total negative-mass increase under uniform margin shrinkage). If mk(zweak(x)) mk(zstrong(x)) for all = y, then Pmix(y x) Pstrong(y x), (cid:88) k=y Pmix(k x) (cid:88) k=y Pstrong(k x). (18) The theorem shows that, when the weak agent is uniformly more uncertain than the strong agent, joint logit mixing necessarily shifts probability mass from the target to negative tokens. As consequence, gradient magnitude on negative classes is amplified, preventing vanishing gradients in saturated regions. This mechanism explains why weakdriven learning enables continued refinement of the strong agents decision boundary even after standard supervised fine-tuning has converged. complete proof is provided in Appendix B. Corollary 5.2 (Per-token amplification on hard negatives). For any H, the mixed negative gradient satisfies Pmix(k x) Pstrong(k x) whenever Pmix(y x) Pstrong(y x) exp((1 λ)mk) , (19) where mk = mk(zstrong(x)) mk(zweak(x)) > 0. Weak-Driven Learning: How Weak Agents make Strong Agents Stronger This corollary provides token-level characterization of gradient amplification: for hard negatives, joint logit mixing increases their assigned probabilityand thus their gradient magnitude, under mild and explicit conditions on the relative target probabilities. Proposition 5.3 (Logit updates on negative and target tokens). Under joint training, for agent {weak, strong} with scale sweak = 1 λ, sstrong = λ, zi,k ηsi Pmix(k x) zi,y ηsi (1 Pmix(y x)). (k = y), (20) Thus any increase in Pmix(k x) directly amplifies suppression of hard negatives, while decrease in Pmix(y x) strengthens the upward push on the target logit. The proposition shows that any increase in the mixed probability Pmix(k x) directly amplifies the suppressive update on non-target tokens, while corresponding decrease in Pmix(y x) strengthens the upward update on the target logit. These dynamics are consistent with the empirical reduction of the non-target logit mean observed in our logit statistics. Summary (Why Weak-Driven Learning Works): These theoretical results formalize the key advantage of weak-driven learning over standard SFT. By redistributing probability mass toward hard negatives in saturated regions, joint logit mixing with weak agent increases gradient magnitudes for the strong agent that would otherwise vanish under SFT. While the dominant effect arises from amplified suppression of non-target tokens, the concurrent reduction in target probability further strengthens the positive update on the correct class. Together, these effects alleviate optimization saturation and enable the strong agent to continue refining its decision boundary beyond the convergence point of conventional supervised fine-tuning. Mechanistic interpretation (three stages) of weak-driven learning: Stage I: saturated-region amplification. Early in joint training, the weak agent is more confused on many nontarget tokens, so mixing increases total negative mass (Appendix B, Eq. (39)); gmix is biased toward hard negatives, and the strong agent dominates the effective update (Appendix C, Eq. (70)). Stage II: gradient shielding. As the strong agent becomes confident, the Softmax Hessian collapses (Appendix C, Eq. (76)) and the cross-Hessian between agents vanishes (Eq. (75)). The weak agent then receives little curvature information, so its influence on training diminishes even though it still provides the fused logits. Stage III: null-space drift. Because Softmax is invariant to global logit shifts (Eq. (78)), the loss is flat along the mean 6 direction. When gradients are small, stochastic updates can drift in this null space, producing the observed mean-logit drift without changing centered sharpness (Appendix C). 6. Experiments We conduct extensive experiments to evaluate the effectiveness and efficiency of our proposed method. First, we evaluate WMSSs performance on two representative tasks, math reasoning and code generation. Arithmetic & Code Generation Datasets. To comprehensively evaluate WMSS, we utilize broad suite of (i) Arithmetic reasoning: AIME2025, reasoning tasks: MATH500, AMC23, AQuA (Ling et al., 2017),GSM8K (Cobbe et al., 2021), MAWPS (Koncel-Kedziorski et al., 2016), and SVAMP (Patel et al., 2021). (ii) Code Generation: HumanEval (Chen et al., 2021), MBPP (Austin et al., 2021). Implementation Details. For WMSS, we employ models from both the Qwen family (Qwen3-4B-Base, Qwen3-8BBase) (Yang et al., 2025) as the backbone. We utilize the AM-1.4M dataset (Zhao et al., 2025) for supervised finetuning. All experiments are conducted on eight NVIDIA H800 GPUs. Our training pipeline is implemented based on the TRL library, utilizing the Qwen3 architecture from the transformers library. We configure the global learning rate to 1 105 and set the maximum sequence length to 8192 to accommodate complex reasoning chains. Regarding the hyperparameters of WMSS, we use α = 0.1, β = 0.8, and γ = 0.1. Furthermore, for logit mixing, we set the mixing coefficient to λ = 0.5 to balance the weak and strong logits in the joint-training loss. Baselines. To evaluate the effectiveness of WMSS, we compare it against several competitive fine-tuning strategies: Standard SFT: The conventional supervised finetuning paradigm that optimizes the standard next-token prediction objective. It serves as deterministic reference, devoid of any curriculum strategies or logit-space noise injection mechanisms. UNDIAL(Dong et al., 2024b): method that subtracts one-hot vector scaled by random Gaussian noise from the target tokens logits to suppress specific outputs.. NEFTune (Jain et al., 2023): robust noise-injection baseline that adds random noise to the embedding vectors during training. Following the original implementation, we set the noise scaling hyperparameter α = 5. To ensure fair comparison, all other experimental configurationsincluding the learning rate, maximum sequence Weak-Driven Learning: How Weak Agents make Strong Agents Stronger Table 1. Main Results on Qwen3-4B-Base and Qwen3-8B-Base. We benchmark WMSS against standard SFT, UNDIAL and NEFTune across 7 math datasets and 2 code datasets. All models are fine-tuned for total of 2 epochs, and all reported results are averaged over 3 runs. Bold indicates the best performance, and underline denotes the second best. Methods SFT UNDIAL NEFTune WMSS SFT UNDIAL NEFTune WMSS SFT UNDIAL NEFTune WMSS AIME2025 MATH500 AMC23 AQUA GSM8K MAWPS SVAMP Avg. HumanEval MBPP Avg. Math Code 12.2 12.2 16.7 20.0 15.6 10.0 15.6 20. 2.2 4.4 2.2 5.5 Qwen3-4B-Base as base model. 61.8 55.1 58.9 67.6 83.9 82.9 86.7 88.5 91.3 90.9 93.8 96.2 Qwen3-8B-Base as base model. 63.0 66.9 71.3 77.3 87.5 89.9 90.3 92.5 95.1 96.4 97.3 97.8 Qwen2.5-3B as base model. 51.3 49.3 47.1 53.8 76.6 77.4 78.1 80. 91.6 91.3 90.3 92.6 85.5 84.1 87.7 90.3 88.6 91.2 91.5 94.0 84.4 83.2 84.7 86.0 64.1 61.2 65.0 69.1 66.7 67.7 68.5 72. 53.5 53.5 53.7 56.2 47.7 40.0 43.3 50.0 45.0 48.3 40.0 52.5 21.6 18.3 22.5 22.5 66.1 63.1 68.2 71.3 72.1 71.5 73.7 75. 46.9 50.7 51.3 52.0 68.5 71.3 74.2 74.4 78.3 79.7 80.5 83.9 51.4 50.8 51.8 53.7 57.7 54.6 55.1 59.2 64.0 61.0 64.3 71. 42.7 42.5 41.5 43.3 63.1 63.0 64.7 66.8 71.2 70.4 72.4 77.6 47.1 46.7 46.7 48.5 length, and optimizer settingsare kept strictly consistent with those used in WMSS. 6.1. Main Results Table 1 shows that WMSS consistently breaks the optimization plateau of standard SFT across both model scales and task domains. On Qwen3-4B-Base, standard SFT reaches 64.1% accuracy on math reasoning, whereas WMSS improves performance to 69.1%, yielding +5.0% absolute gain. similar improvement is observed on code generation, where accuracy increases from 63.1% to 66.8%. These gains further scale with model capacity. On Qwen3-8BBase, WMSS raises math accuracy from 66.7% to 72.9% (+6.2%), and improves code generation performance from 71.2% to 77.6%. Overall, the consistent improvements across both math and code tasks suggest that WMSS enhances general reasoning capabilities rather than being confined to single domain. Adaptive Gains across Difficulty Regimes. deeper breakdown reveals that WMSS acts as an adaptive regularizer across both model scales. The improvements are most profound on Hard and competition-level tasks: on AIME2025, WMSS boosts the SFT(Qwen3-4B-Base) baseline from 12.2% to 20.0% and lifts SFT(Qwen3-8B-Base) from 15.6% to 20.0%. This capability is further reinforced on AMC23, where our method steadily improves the 4B model (47.7% 50.0%) and delivers substantial leap on the 8B model (45.0% 52.5%), recovering from the dip observed in standard SFT. On Medium tasks like AQUA, WMSS effectively rectifies optimiza7 tion issues, with Qwen3-8B-Base exhibiting massive gain (63.0% 77.3%) and Qwen3-4B-Base showing robust growth (61.8% 67.6%). Even on Easy benchmarks (e.g., MAWPS), both models push towards saturation (reaching 96.2% and 97.8% respectively) without suffering from catastrophic forgetting. WMSS vs. UNDIAL. We compare WMSS against the UNDIAL baseline to evaluate the optimal strategy for logit adjustment. While UNDIAL focuses on directly suppressing the target token via stochastic penalties, Table 1 reveals that this approach degrades performance (Avg -1.4%). This suggests that naively penalizing the ground truth disrupts the primary training signal. In sharp contrast, WMSS achieves robust gains (surpassing the baseline by 7.9% on Qwen34B-Base Math). This validates that distractors creates more effective gradient signal than dampening the target itself, as it refines the decision boundary by widening the margin between the correct prediction and competing errors without sacrificing confidence in the ground truth. WMSS vs. NEFTune. We benchmark WMSS against NEFTune to contrast targeted correction with blind regularization. NEFTune mitigates overfitting by injecting random noise into embeddings, which encourages the model to rely on pre-trained knowledge rather than memorization. However, it remains agnostic to the models specific error patterns. In contrast, WMSS leverages historical confusion to construct structurally informed corrective signals. This methodological divergence leads to consistent performance gains on held-out test sets. In particular, WMSS signifWeak-Driven Learning: How Weak Agents make Strong Agents Stronger Table 2. Logit Dynamics Analysis (Qwen3-4B-Base). Comparison of logit statistics at Epoch 3. Values in parentheses indicate the relative change of WMSS vs. SFT. Metric Target Strength (ztarget) Distractors Mean (zbg) SFT WMSS 35.88 2.09 36.10 (+0.6%) 0.90 (-56.9%) Target-to-Background Gap (gap) 33.79 35.20 (+4.2%) 3.45 (+17.7%) Logit Variance (σ) 2.93 Figure 4. Limits of SFT Logit Growth on Qwen3-4B-Base. After an initial growth phase, both correct and incorrect token logits saturate, preventing standard SFT from further enlarging their margin. in sharper decision boundaries and the robust performance gains observed in our experiments. 6.4. Ablation Study Table 3 deconstructs the incremental contributions within the WMSS framework. The CEDA module first establishes robust foundation by filtering high-value samples, improving the average accuracy by +2.2% (54.1% 56.3%), though its impact on the challenging AIME task remains limited (12.2% 13.3%). The addition of JTWS marks critical turning point, utilizing corrective signals to bridge the reasoning gap and lifting the average accuracy to 58.2%. Finally, the full model achieves maximal robustness, peaking at 59.9% on average and nearly doubling the baseline performance on AIME (20.0%). This progression confirms that while data curation provides necessary baseline, the synergistic integration of weak-to-strong signals is essential for overcoming the optimization bottlenecks in complex mathematical reasoning. 7. Conclusion In this work, we introduced WMSS, training paradigm that fundamentally inverts the traditional distillation logic. Rather than relying on stronger teachers, we demonstrate that weak historical checkpoints harbor critical corrective signals required to break the optimization stagnation of standard SFT. By structurally leveraging these signals to amplify gradients from hard negatives, our method achieves remarkable scalability, notably doubling performance on Figure 3. Convergence Analysis across Tasks. The training trajectory of WMSS (Qwen3-4B-Base) over 4 epochs. icantly outperforms NEFTune on math reasoning benchmarks (69.1% vs. 65.0% on Qwen3-4B-Base; see Table 1), indicating that generic regularization alone is insufficient for harder reasoning tasks. These results suggest that overcoming the bottlenecks of mathematical reasoning requires structurally aligned guidance rather than purely stochastic perturbations. 6.2. Convergence Analysis across Tasks Figure 3 visualizes training trajectories across seven datasets, revealing two-stage pattern: rapid acquisition followed by asymptotic stabilization. While substantial gains occur within the first three epochs, marginal utility diminishes thereafter. Crucially, we observe over-optimization in the final stages; notably, AMC2023 (brown line) exhibits sharp regression after Epoch 3, while GSM8K shows volatility. This identifies Epoch 4 as the critical inflection point to maximize generalization before the onset of catastrophic forgetting. 6.3. Statistical Analysis: Why WMSS Works As visualized in Figure 4, standard SFT optimization inevitably encounters saturation bottleneck where the model loses the gradient drive to distinguish valid tokens from distractors: the target logits hit ceiling (plateauing at 35.88), while the non-target logits cease to decline (zbg sticking at 2.09). WMSS breaks this deadlock through distinct suppression-dominant mechanism (i.e., the non-target logit mean drops more than the target logit rises) detailed in Table 2. Rather than relying on forcing the already high target logits higher (showing only marginal +0.6% increase), our method aggressively suppresses background interference(the non-target logit mean (zbg)), precipitating dramatic 56.9% reduction in the Non-Target Logit Mean (zbg : 2.09 0.90). This suppression effectively clears the signal path, expanding the Target-to-Background Gap (gap) by +1.41 points; crucially, due to the exponential nature of the Softmax function (P ez), this linear expansion is significantly amplified in probability space, resulting 8 Weak-Driven Learning: How Weak Agents make Strong Agents Stronger Table 3. Ablation on Qwen3-4B-Base. The Baseline denotes standard SFT. We incrementally add CEDA (Curriculum-Enhanced Data Activation.) and JTWS (Joint Training of Weak and Strong). WMSS represents the full model (Baseline + CEDA + JTWS). Method AIME MATH500 GSM8K Avg. Baseline + CEDA + JTWS WMSS 12.2 13.3 16.7 20.0 66.1 69.4 70.2 71.3 83.9 86.2 87.6 88. 54.1 56.3 58.2 59.9 AIME2025. Beyond these benchmarks, our findings offer proof-of-concept for autonomous self-evolution: effective alignment does not strictly demand external supervision or larger models. Instead, the waste of traininghistorical confusion and distractorscan be repurposed as the fuel for breaking performance ceilings. This opens new, dataefficient frontier for post-training large language models, suggesting that the key to stronger agents may lie in understanding their weaker selves."
        },
        {
            "title": "Impact Statement",
            "content": "This paper presents paradigm shift in post-training optimization by introducing Weak-Driven Learning, which challenges the conventional assumption that supervision must originate from superior source. By demonstrating that strong models can transcend optimization plateaus using signals from weaker historical checkpoints, our work suggests pathway toward more autonomous and scalable model evolution, particularly in the context of Weak-toStrong Generalization. This approach provides an alternative to standard supervised fine-tuning and reduces reliance on expensive high-quality annotations or computationally heavy teacher models. This paper presents work whose goal is to advance the field of Machine Learning. While there are many potential societal consequences of our work, none are identified that must be specifically highlighted beyond those commonly associated with large language models."
        },
        {
            "title": "References",
            "content": "Agarwal, R., Vieillard, N., Stanczyk, P., Ramos, S., Geist, M., and Bachem, O. Gkd: Generalized knowledge distillation for auto-regressive sequence models. CoRR, 2023. Austin, J., Odena, A., Nye, M., Bosma, M., Michalewski, H., Dohan, D., Jiang, E., Cai, C., Terry, M., Le, Q., et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021. Bengio, Y., Louradour, J., Collobert, R., and Weston, J. Curriculum learning. In Proceedings of the 26th annual international conference on machine learning, pp. 4148, 2009. 9 Burns, C., Izmailov, P., Kirchner, J. H., Baker, B., Gao, L., Aschenbrenner, L., Chen, Y., Ecoffet, A., Joglekar, M., Leike, J., et al. Weak-to-strong generalization: Eliciting strong capabilities with weak supervision. arXiv preprint arXiv:2312.09390, 2023. Chen, L., Li, S., Yan, J., Wang, H., Gunaratna, K., Yadav, V., Kumar, Z., Liu, Y., Parikh, D., and Xu, S. Alpagasus: Training better alpaca with fewer data. In The Twelfth International Conference on Learning Representations, 2024. Chen, M., Tworek, J., Jun, H., Yuan, Q., de Oliveira Pinto, H. P., Kaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman, G., Ray, A., Puri, R., Krueger, G., Petrov, M., Khlaaf, H., Sastry, G., Mishkin, P., Chan, B., Gray, S., Ryder, N., Pavlov, M., Power, A., Kaiser, L., Bavarian, M., Winter, C., Tillet, P., Such, F. P., Cummings, D., Plappert, M., Chantzis, F., Barnes, E., Herbert-Voss, A., Guss, W. H., Nichol, A., Paino, A., Tezak, N., Tang, J., Babuschkin, I., Balaji, S., Jain, S., Saunders, W., Hesse, C., Carr, A. N., Leike, J., Achiam, J., Misra, V., Morikawa, E., Radford, A., Knight, M., Brundage, M., Murati, M., Mayer, K., Welinder, P., McGrew, B., Amodei, D., McCandlish, S., Sutskever, I., and Zaremba, W. Evaluating large language models trained on code, 2021. URL https://arxiv.org/abs/2107.03374. Chen, Z., Ai, T., Li, Y., Li, G., Wei, Y., Zhou, W., Li, G., Yu, B., Chen, Z., Sun, H., et al. Llmboost: Make large language models stronger with boosting. arXiv preprint arXiv:2512.22309, 2025. Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano, R., et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. Dong, G., Yuan, H., Lu, K., Li, C., Xue, M., Liu, D., Wang, W., Yuan, Z., Zhou, C., and Zhou, J. How abilities in large language models are affected by supervised fine-tuning data composition. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 177198, 2024a. Dong, Y. R., Lin, H., Belkin, M., Huerta, R., and Vulic, I. Undial: Self-distillation with adjusted logits for robust unlearning in large language models, 2024b. URL https: //arxiv.org/abs/2402.10052. Gou, J., Yu, B., Maybank, S. J., and Tao, D. Knowledge distillation: survey. International Journal of Computer Vision, 129(6):17891819, March 2021. ISSN 1573-1405. doi: 10.1007/s11263-021-01453-z. URL http://dx.doi. org/10.1007/s11263-021-01453-z. Weak-Driven Learning: How Weak Agents make Strong Agents Stronger Gu, Y., Dong, L., Wei, F., and Huang, M. Minillm: Knowledge distillation of large language models. arXiv preprint arXiv:2306.08543, 2023. Gudibande, A., Wallace, E., Snell, C., Geng, X., Liu, H., Abbeel, P., Levine, S., and Song, D. The false promise of imitating proprietary llms, 2023. URL https://arxiv.org/ abs/2305.15717. Gulcehre, C., Paine, T. L., Srinivasan, S., Konyushkova, K., Weerts, L., Sharma, A., Siddhant, A., Ahern, A., Wang, M., Gu, C., Macherey, W., Doucet, A., Firat, O., and de Freitas, N. Reinforced self-training (rest) for language modeling, 2023. URL https://arxiv.org/abs/ 2308.08998. He, X., Ban, Y., Zou, J., Wei, T., Cook, C., and He, J. Llmforest: Ensemble learning of llms with graph-augmented prompts for data imputation. In Findings of the Association for Computational Linguistics: ACL 2025, pp. 69216936, 2025. Hinton, G., Vinyals, O., and Dean, J. the knowledge in neural network. arXiv:1503.02531, 2015. Distilling arXiv preprint Huang, J., Chen, X., Mishra, S., Zheng, H. S., Yu, A. W., Song, X., and Zhou, D. Large language models cannot self-correct reasoning yet, 2024. URL https://arxiv.org/ abs/2310.01798. Huang, Z., Xia, X., Ren, Y., Zheng, J., Xiao, X., Xie, H., Huaqiu, L., Liang, S., Dai, Z., Zhuang, F., Li, J., Ban, Y., and Wang, D. Real-time aligned reward model beyond semantics, 2026. URL https://arxiv.org/abs/2601.22664. Jain, N., yeh Chiang, P., Wen, Y., Kirchenbauer, J., Chu, H.-M., Somepalli, G., Bartoldson, B. R., Kailkhura, B., Schwarzschild, A., Saha, A., Goldblum, M., Geiping, J., and Goldstein, T. Neftune: Noisy embeddings improve instruction finetuning, 2023. URL https://arxiv.org/abs/ 2310.05914. Koncel-Kedziorski, R., Roy, S., Amini, A., Kushman, N., and Hajishirzi, H. Mawps: math word problem repository. In Proceedings of the 2016 conference of the north american chapter of the association for computational linguistics: human language technologies, pp. 11521157, 2016. Ling, W., Yogatama, D., Dyer, C., and Blunsom, P. Program induction by rationale generation: Learning to solve and explain algebraic word problems. arXiv preprint arXiv:1705.04146, 2017. Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., et al. Training language models to follow instructions 10 with human feedback. Advances in neural information processing systems, 35:2773027744, 2022. Patel, A., Bhattamishra, S., and Goyal, N. Are nlp models really able to solve simple math word problems? arXiv preprint arXiv:2103.07191, 2021. Stechly, K., Valmeekam, K., and Kambhampati, S. On the self-verification limitations of large language models on reasoning and planning tasks, 2024. URL https://arxiv. org/abs/2402.08115. Touvron, H. et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. Xu, C., Sun, Q., Zheng, K., Geng, X., Zhao, P., Feng, J., Tao, C., and Jiang, D. Wizardlm: Empowering large language models to follow complex instructions. arXiv preprint arXiv:2304.12244, 2023. Yang, A., Li, A., Yang, B., Zhang, B., Hui, B., Zheng, B., Yu, B., Gao, C., Huang, C., Lv, C., Zheng, C., Liu, D., Zhou, F., Huang, F., Hu, F., Ge, H., Wei, H., Lin, H., Tang, J., Yang, J., Tu, J., Zhang, J., Yang, J., Yang, J., Zhou, J., Zhou, J., Lin, J., Dang, K., Bao, K., Yang, K., Yu, L., Deng, L., Li, M., Xue, M., Li, M., Zhang, P., Wang, P., Zhu, Q., Men, R., Gao, R., Liu, S., Luo, S., Li, T., Tang, T., Yin, W., Ren, X., Wang, X., Zhang, X., Ren, X., Fan, Y., Su, Y., Zhang, Y., Zhang, Y., Wan, Y., Liu, Y., Wang, Z., Cui, Z., Zhang, Z., Zhou, Z., and Qiu, Z. Qwen3 technical report, 2025. URL https://arxiv.org/ abs/2505.09388. Yang, F., Chen, Z., Wang, X., Lu, X., Chai, J., Yin, G., Lin, W., Ma, S., Zhuang, F., Wang, D., Yang, Y., Li, J., and Ban, Y. Your group-relative advantage is biased, 2026. URL https://arxiv.org/abs/2601.08521. Zhang, L., Song, J., Gao, A., Chen, J., Bao, C., and Ma, K. Be your own teacher: Improve the performance of convolutional neural networks via self distillation, 2019. URL https://arxiv.org/abs/1905.08094. Zhao, H., Wang, H., Peng, Y., Zhao, S., Tian, X., Chen, S., Ji, Y., and Li, X. 1.4 million open-source distilled reasoning dataset to empower large language model training, 2025. URL https://arxiv.org/abs/2503.19633. Zhou, C., Liu, P., Xu, P., Iyer, S., Sun, J., Mao, Y., Ma, X., Efrat, A., Yu, P., Yu, L., et al. Lima: Less is more for alignment. Advances in Neural Information Processing Systems, 36, 2024. Zou, J., Ban, Y., Li, Z., Qi, Y., Qiu, R., Yang, L., and He, J. Transformer copilot: Learning from the mistake log in llm fine-tuning. arXiv preprint arXiv:2505.16270, 2025. Weak-Driven Learning: How Weak Agents make Strong Agents Stronger Table 4. Hyperparameter Sensitivity Analysis on Qwen3-4B-Base. We evaluate the impact of the mixing coefficients α (base difficulty), β (consolidation), and γ (regression repair) on mathematical reasoning. Bold denotes the best performance. Setup Coefficients Accuracy (%) α β γ AIME 2025 MATH 500 Config Ours (Selected) Config Config Config Config 0.1 0.7 0.2 0.1 0.8 0.1 0.2 0.6 0.2 0.0 0.9 0.1 0.0 1.0 0.0 0.05 0.85 0.1 10.0 16.7 10.7 10.3 9.7 14. 68.2 68.2 67.8 70.2 68.3 69.5 A. Additional Experiments A.1. Sensitivity of Curriculum, Consolidation, and Repair Coefficients. We further investigate the impact of the curriculum coefficient α, the consolidation coefficient β, and the regression-repair coefficient γ. Table 4 presents the results on Qwen3-4B-Base. We observe distinct trade-off between standard mathematical proficiency (MATH 500) and complex reasoning capability (AIME 2025). Configuration (α = 0.1, β = 0.9, γ = 0), which disables the regression-repair signal, achieves the highest accuracy on MATH 500 (70.2%). However, its performance on the more challenging AIME benchmark drops significantly to 10.3%. This suggests that while strong consolidation of the target distribution (β = 0.9) helps with standard problems, it leads to optimization stagnation on harder tasks. In contrast, our selected configuration (α = 0.1, β = 0.8, γ = 0.1) introduces controlled regression-repair emphasis. Although this slightly reduces MATH performance (70.2% 67.5%), it yields decisive +6.4% gain on AIME (10.3% 16.7%). This confirms that the additional gradient signal induced by γ is essential for breaking through reasoning bottlenecks, justifying our choice of parameters. A.2. Sensitivity Analysis on Mixing Coefficient λ The mixing coefficient λ explicitly controls the relative weight between the weak and strong models in joint training, with fused logits zmix = (1 λ)z1 + λz2. Smaller λ assigns more influence to the weaker model, while larger λ emphasizes the stronger models direct fit to the targets. We sweep λ over [0.1, 0.9] with finer grid in [0.4, 0.6]. Table 5 shows broad inverted U-shape: the best average performance occurs at λ = 0.42 (Avg. 75.5%), and strong plateau persists in λ [0.42, 0.48]. All benchmarks are evaluated with three-run averages; minor fluctuations on small sets (e.g., AIME25, 30 problems) are still expected, but the optimal region remains stable. At the extremes, the behavior aligns with the mechanism. As λ 1, zmix z2 and the joint training reduces to relying on the strong model, so the compensatory interaction weakens and accuracy drops (e.g., Avg. 67.6% at λ = 0.9). As λ 0, the weak model dominates the fused logits, reducing effective target learning and risking underfitting; this is especially pronounced for weaker base models (e.g., at λ = 0.3, weaker model attains only 13% accuracy on MATH500). Numerical consistency with the gradient-share crossover. Following the theoretical analysis in Appendix C, we can estimate the effective sensitivity ratio using the centered-norm proxy: α (cid:18) z22 z12 (cid:19)2 . (21) From the logits evaluation report, z22 1240.10 and z12 1034.50, giving α 1.44. Plugging into the crossover formula in Eq. (71), λcross 1 1 + 0.455, α (22) which lies close to the empirically strongest region (λ [0.42, 0.48]). We stress that this is heuristic consistency check: α is phase-dependent and the linearization is local, so the theory predicts broad optimum region rather than sharp inversion point. Weak-Driven Learning: How Weak Agents make Strong Agents Stronger Table 5. Sensitivity Analysis of Mixing Coefficient λ. We report the accuracy (%) on Qwen3-4B-Base across varying λ values. The curve exhibits broad inverted U-shape, with the strongest performance typically in the λ [0.42, 0.48] range (peak at λ = 0.42 in Avg.), reflecting balanced trade-off between learning new features and preserving historical constraints. Dataset Mixing Coefficient λ 0.1 0. 0.3 0.4 0.42 0.44 0.46 0.48 0.5 0.52 0.54 0.56 0.58 0. 0.7 0.8 0.9 7.8 10.0 12.2 16.7 20.0 20.0 16.7 20.0 20.0 21.1 16.7 16.7 16.7 10.0 20.0 20.0 12.2 AIME25 MATH500 71.4 67.1 70.8 73.1 74.9 74.5 73.2 73.3 73.3 70.7 74.0 73.0 73.3 68.9 71.7 70.9 66.1 77.2 72.8 74.7 75.2 73.9 73.1 72.4 71.8 73.9 71.0 71.3 71.9 70.2 64.8 66.9 61.4 60.8 AQUA 88.2 89.1 89.4 91.9 91.8 91.7 91.0 91.4 91.0 91.2 91.4 90.6 90.9 89.2 89.3 88.1 85.7 GSM8K 94.9 96.6 95.5 97.8 98.2 97.8 97.8 98.0 95.7 96.5 97.2 96.8 97.5 94.7 96.2 95.5 93.6 MAWPS 90.4 91.8 93.3 94.9 94.1 93.6 93.5 92.9 92.6 92.4 93.5 92.8 92.9 90.8 91.9 89.6 87.4 SVAMP Avg. 71.7 71.2 72.7 74.9 75.5 75.1 74.1 74.6 74.4 73.8 74.0 73.6 73.6 69.7 72.7 70.9 67.6 Table 6. Extended Logit Statistics Before vs. After Joint Training (Qwen3-4B-Base). Weak = model 0, strong = model 1. Pre denotes before joint training, and Stronger (Post) denotes the jointly trained strong branch after training. denotes PostPre (percent change in parentheses). Centered norm values are taken directly from the logit analysis report."
        },
        {
            "title": "Post",
            "content": ""
        },
        {
            "title": "Pre",
            "content": "Stronger (Post) 3.65 3.06 9.42 2.65 Mean logit zmean Std σ Centered norm z2 1191.33 1034.50 Max logit zmax 38.50 Min logit zmin -22.50 L2 norm z2 Entropy Max prob pmax 2.87 3.18 1240.10 48.75 43.25 -24.00 -26.63 1978.92 3858.60 1879.68 (+95.0%) 1865. 5.77 (+158.1%) -0.41 (-13.4%) -156.83 (-13.2%) -4.75 (-11.0%) 1.50 (-6.2%) 1.91 (+367.3%) -0.30 (-36.1%) 0.44 0.85 2.43 0.53 0.52 0.83 0.97 3.16 1229.79 60.50 -35.75 1688.59 0.48 0. -1.90 (-66.2%) -0.02 (-0.6%) -10.31 (-0.8%) 11.75 (+24.1%) -9.12 (+34.2%) -177.37 (-9.5%) 0.04 (+9.1%) -0.02 (-2.4%) A.3. Extended Logit Statistics To provide more complete view of the logit dynamics underlying joint training, we report additional statistics from the logit analysis report in Table 6. Beyond the non-target logit mean reduction shown in Table 2, the extended metrics highlight pronounced mean drift in the weak model and stable sharpness in the strong (post) model. Implementation details. All logit statistics are computed from 200 randomly sampled examples drawn from the AM-1.4M training dataset used in our experiments. We run forward passes for the preand post-joint-training checkpoints on these examples and aggregate statistics across the sampled set. The centered-norm definition used in the table follows Appendix (Eq. (56)). 12 Weak-Driven Learning: How Weak Agents make Strong Agents Stronger B. Supplementary Theory: Gradient Amplification under Logit Mixing Notation. Fix context and target token index y, and analyze single joint-training step; we omit the conditioning on for readability. The weak and strong models Mweak and Mstrong produce logits zweak(x), zstrong(x) RV, and the mixed logits are zmix(x) = (1 λ)zweak(x) + λzstrong(x) with λ [0, 1]. For any logit map z(x), define Pz( x) = Softmax(z(x)) and use the shorthand Pz(k); let ey denote the one-hot target vector. We use subscript {weak, strong} to index the two models when writing model-agnostic expressions. For any negative token = y, the margin is mk(z(x)) = z(x)[y] z(x)[k]. B.1. Setup and Baseline We analyze joint training under logit mixing. The mixed logits are We distinguish the mixed-logit loss and the SFT loss on the strong model: zmix = (1 λ)zweak + λzstrong, λ [0, 1]. Lmix = log Pmix(y), LSFT = log Pstrong(y), where Pmix = Pzmix and Pstrong = Pzstrong . The joint-training gradient with respect to fused logits is In standard SFT on the strong model alone, the corresponding gradient is gmix = zmixLmix = Pmix() ey. gsft = zstrong LSFT = Pstrong() ey. For each model, gradients are scaled by the mixing coefficients: zweak Lmix = (1 λ)gmix, zstrong Lmix = λgmix. B.2. Log-Odds and Margin Contraction Definition B.1 (Target margin). For any negative token = y, define the margin mk(z) = z[y] z[k]. Lemma B.2 (Softmax log-odds). For any = y, the log-odds under logits satisfy log Pz(k) Pz(y) = mk(z). Proof. By definition, Pz(k)/Pz(y) = exp(z[k] z[y]) = exp(mk(z)). Lemma B.3 (Margin mixing). For any = y, the mixed margin equals the convex combination mk(zmix) = (1 λ)mk(zweak) + λmk(zstrong). Proof. Using (23) and (28), we expand mk(zmix) = zmix[y] zmix[k] = (cid:2)(1 λ)zweak[y] + λzstrong[y](cid:3) (cid:2)(1 λ)zweak[k] + λzstrong[k](cid:3) = (1 λ)(zweak[y] zweak[k]) + λ(zstrong[y] zstrong[k]) = (1 λ)mk(zweak) + λmk(zstrong). 13 (23) (24) (25) (26) (27) (28) (29) (30) (31) Weak-Driven Learning: How Weak Agents make Strong Agents Stronger B.3. Negative-Gradient Amplification Definition B.4 (Hard-negative set). Define the hard-negative set = {k = : mk(zweak) < mk(zstrong)}, i.e., tokens for which the weak model has smaller margin (more confusion). Lemma B.5 (Relative probability increase on hard negatives). For any H, Pmix(k) Pmix(y) > Pstrong(k) Pstrong(y) . (32) (33) Proof. By (30), mk(zmix) < mk(zstrong) for H. Applying (29) yields larger log-odds ratio for the mixed logits. Corollary B.6 (Sufficient condition for per-token amplification). For any H, the negative-token gradient on fused logits satisfies Pmix(k) Pstrong(k) whenever Pmix(y) Pstrong(y) exp((1 λ)mk) , (34) where mk = mk(zstrong) mk(zweak) > 0. Proof. We begin by expressing probabilities via log-odds (Lemma (29)): Pmix(k) Pstrong(k) = = Pmix(y) exp(mk(zmix)) Pstrong(y) exp(mk(zstrong)) Pmix(y) Pstrong(y) exp(mk(zstrong) mk(zmix)) . Using the margin mixing identity (Lemma (30)), mk(zstrong) mk(zmix) = mk(zstrong) (cid:2)(1 λ)mk(zweak) + λmk(zstrong)(cid:3) Substituting (36) into (35) yields = (1 λ)(cid:0)mk(zstrong) mk(zweak)(cid:1) = (1 λ)mk. Pmix(k) Pstrong(k) = Pmix(y) Pstrong(y) exp((1 λ)mk) . We now solve for the condition under which Pmix(k) Pstrong(k): (35) (36) (37) Pmix(k) Pstrong(k) 1 Pmix(y) Pstrong(y) Pmix(y) Pstrong(y) exp((1 λ)mk) 1 exp((1 λ)mk) . (38) This is exactly the stated sufficient condition (34). Theorem B.7 (Total negative-mass increase under uniform margin shrinkage). If mk(zweak) mk(zstrong) for all = y, then Pmix(y) Pstrong(y) and Pmix(k) Pstrong(k). (cid:88) (cid:88) k=y k=y Proof. By Lemma (30) and the assumption mk(zweak) mk(zstrong), mk(zmix) = (1 λ)mk(zweak) + λmk(zstrong) mk(zstrong) = y. Applying Lemma (29) yields, for each = y, exp(cid:0) mk(zmix)(cid:1) exp(cid:0) mk(zstrong)(cid:1). 14 (39) (40) (41) Weak-Driven Learning: How Weak Agents make Strong Agents Stronger Summing (41) over = gives"
        },
        {
            "title": "Since",
            "content": "exp(cid:0) mk(zmix)(cid:1) (cid:88) k=y (cid:88) k=y exp(cid:0) mk(zstrong)(cid:1). Pz(y) = 1 + (cid:88) k=y 1 exp(cid:0) mk(z)(cid:1) , the denominator for Pmix(y) is no smaller than that for Pstrong(y), and thus Pmix(y) Pstrong(y). Finally, because probabilities sum to one, (cid:88) k=y Pmix(k) = 1 Pmix(y) 1 Pstrong(y) = (cid:88) k=y Pstrong(k), (42) (43) (44) (45) which proves (39). Proposition B.8 (Logit updates emphasize negative suppression). For any negative token = y, the logit update for model {weak, strong} under joint training is zi[k] ηsi Pmix(k), sweak = 1 λ, sstrong = λ, while for the target token, zi[y] ηsi (1 Pmix(y)). (46) (47) Thus any increase in Pmix(k) directly amplifies the suppression of hard negatives, while decrease in Pmix(y) strengthens the upward push on the target logit. Proof. From (27), the model-i logit gradient satisfies where gmix = Pmix() ey (Eq. (25)). For negative token = y, this gives ziLmix = si gmix, and for the target token, Lmix zi[k] = si Pmix(k), Lmix zi[y] = si (Pmix(y) 1). Under first-order update zi ηziLmix, we obtain zi[k] ηsi Pmix(k), zi[y] ηsi (1 Pmix(y)), which matches (46) and (47). (48) (49) (50) (51) Remark B.9 (Consistency with logits statistics). The logits evaluation report shows that after joint training the strong model exhibits lower mean logits and more extreme tails (e.g., larger magnitude minima and maxima). The mechanism above provides local explanation: increased negative mass amplifies downward updates on many incorrect tokens, while the target receives stronger upward push when Pmix(y) decreases. Mean shifts can further accumulate along shift-invariant directions (Appendix C). Informally, this gradient amplification provides extra training signal compared to SFT, which helps reduce saturation and move optimization past local minima. 15 Weak-Driven Learning: How Weak Agents make Strong Agents Stronger C. Supplementary Theoretical Analysis of Joint Training Dynamics C.1. Problem Setting and Notation Notation. Fix context and target index y, and analyze single joint-training step; we omit the conditioning on for readability. The weak and strong models Mweak and Mstrong have parameters θweak and θstrong, and produce logits zweak(x), zstrong(x) RV. The mixed logits are zmix = (1 λ)zweak + λzstrong, λ [0, 1]. (52) For any logit map z(x), define Pz( x) = Softmax(z(x)) and use the shorthand Pz(k); let Pmix = Pzmix and ey denote the one-hot target. We use subscript {weak, strong} to index the two models in generic expressions. Let 1 be the all-ones vector in RV. We use the mixed-logit loss and the residual Lmix = log Pmix(y), = zmixLmix = Pmix() ey. (53) (54) For any negative token = y, the margin is mk(z) = z[y] z[k]. We use η to denote the learning rate. For each model {weak, strong}, let Ji = zi/θi and define the Jacobian Gram matrix Ki = JiJ . (55) Definition C.1 (Centered logits and centered norm). Let = 1 norm are 1z be the logit mean. The centered logits and centered (cid:118) (cid:117) (cid:117) (cid:116) (z[k] z)2. (cid:88) (56) = z1, z2 = Since z2 = (cid:112)V Std(z), the centered norm is shift-invariant measure of sharpness. k=1 C.2. First-Order Gradients and Centered Linearized Dynamics By the chain rule, zweak Lmix = (1 λ)g, zstrong Lmix = λg. (57) (58) Thus both models receive gradients in the same direction but with different magnitudes. Under first-order (local) linearization, we apply Taylor expansion of logits around θi: zi(θi + θi) zi(θi) + Jiθi, {weak, strong}. With θi = ηθiLmix, this yields zi ηJiJ = ηKig, and incorporating the mixing weights in (57)(58) yields zi ηsiKig, sweak = 1 λ, sstrong = λ. (59) (60) (61) To remove mean-shift effects, let Π = 1 the centered kernel 11 be the centering projector. Since 1g = 0, we have Πg = and define Then the centered logit dynamics are Ki = ΠKiΠ. zi ηsi Kig. (62) (63) Lemma C.2 (PSD of centered kernel). For each model {weak, strong}, Ki is positive semidefinite. 16 Weak-Driven Learning: How Weak Agents make Strong Agents Stronger Proof. For any RV, Kix = xΠJiJ Πx = Πx2 2 0. The fused centered update is doubly weighted combination of model updates: zmix = (1 λ)zweak + λzstrong = η (cid:104) (1 λ)2 Kweak + λ2 Kstrong (cid:105) g. The corresponding first-order loss decrease follows from Taylor expansion of Lmix at zmix: Lmix zmixL mixzmix = gzmix. Substituting (64) gives Lmix gzmix = η (cid:104) (1 λ)2g Kweakg + λ2g Kstrongg (cid:105) . (64) (65) (66) C.3. Stage I: Hard-Negative Amplification and Strong-Model Dominance Early in joint training, the weak model is more confused, so for many hard negatives we have mk(zweak) < mk(zstrong). By the logit-mixing analysis in Appendix (e.g., Eq. (39)), mixing shrinks these margins and increases total negative probability mass. Consequently, the residual grows in magnitude and is biased toward hard negatives: the weak model acts as gradient amplifier, not competitor in final accuracy. The effective per-step loss decrease attributable to model {weak, strong} is Ei ηs2 Kig, sweak = 1 λ, sstrong = λ. (67) Dominance corresponds to Estrong > Eweak. Define the directional sensitivity κi = Kig/g2 2. Assumption C.3 (Sensitivity advantage of the strong model). Because the strong model is sharper (larger centered norm / lower entropy), its centered kernel responds more strongly along the residual direction: Kstrongg α Kweakg, α > 1. Substituting (68) into (66) yields the total effective rate S(λ) = (1 λ)2 + αλ2, and the strong model dominates the joint update when Solving (70) gives the gradient-share crossover λ2α > (1 λ)2. λcross = 1 1 + . α (68) (69) (70) (71) Remark C.4 (Crossover vs. accuracy). Equation (71) characterizes local crossover of gradient contribution under one-step linearization. It does not predict an accuracy inversion. In practice, both and Kweak, Kstrong evolve with λ and training time, while optimizer dynamics smooth the trajectory, yielding broad optimum region rather than sharp transition. Remark C.5 (Softmax amplification). Because Pmix(k) exp(zmix[k]), modest logit differences can disproportionately tilt Pmix and g. This amplifies hard-negative gradients and reinforces the early dominance in (70). C.4. Stage II: Gradient Shielding via Hessian Contraction We next analyze why the weak model loses effective training signal once the strong model becomes confident. 17 Weak-Driven Learning: How Weak Agents make Strong Agents Stronger C.4.1. SOFTMAX JACOBIAN AND LOSS HESSIAN Recall Pmix = Softmax(zmix). The Jacobian of Softmax is Pmix zmix = diag(Pmix) PmixP mix. Since ey is constant, the Hessian of the cross-entropy loss with respect to zmix is HL = 2 zmix Lmix = (Pmix ey) zmix = diag(Pmix) PmixP mix. Lemma C.6 (PSD of the loss Hessian). HL is positive semidefinite. Proof. For any RV, (72) (73) vHLv = (cid:88) Pmix[j] v2 (cid:88) 2 Pmix[j] vj = VarPmix(v) 0. (74) C.4.2. INTERACTION HESSIAN BETWEEN MODELS The mixed logits depend on both models, so the cross-Hessian captures how updates in the strong model affect the gradient seen by the weak model: Hws = zstrong [zweak Lmix] = (1 λ) (Pmix ey) zmix zmix zstrong = λ(1 λ)HL. (75) C.4.3. SHIELDING IN THE CONFIDENT REGIME Assume the target index is and the strong model drives the mixed prediction to Pmix(y) 1. Then Pmix(k) 0 for all = y, which implies Consequently, lim Pmixey HL = 0. lim Pmixey Hws = 0, (76) (77) and the weak model receives vanishing curvature information. This is the mathematical form of gradient shielding: once the strong model dominates and the prediction saturates, both the first-order residual and its local sensitivity collapse, making it difficult for the weak model to receive informative updates. C.5. Stage III: Null-Space Drift and Mean Shift We now explain the observed mean drift of the weak model logits. C.5.1. SHIFT INVARIANCE OF THE MIXED SOFTMAX Softmax is invariant to global shifts. For any scalar c, Softmax(z + c1) = Softmax(z). Applying this to the fused logits, let weak = zweak + c1. Then mix = (1 λ)z weak + λzstrong = zmix + (1 λ)c1, (78) (79) and by (78) the predictive distribution is unchanged. Hence, the loss is flat along the mean-shift direction of each model. 18 Weak-Driven Learning: How Weak Agents make Strong Agents Stronger C.5.2. ZERO-EIGENVALUE DIRECTION OF THE HESSIAN Let 1 denote the all-ones vector. From (73), HL1 = diag(Pmix)1 Pmix(P mix1) = Pmix Pmix = 0. (80) Thus 1 is zero-eigenvalue direction of the Hessian, confirming that the loss has no curvature along global shifts. C.5.3. WHY DRIFT ACCUMULATES IN THE NULL SPACE When the strong model has already fit the data, the expected gradient seen by the weak model is near zero. With stochastic training, the update can be modeled as which is random walk. The logit space decomposes as θ+ = θ ηϵ, ϵ (0, Σ), = z1 + z, z1 = 0, (81) (82) where the null space is span{1} and the active space is its orthogonal complement. In the active space, even small gradients can weakly restore the distribution shape. In the null space, there is no restoring force; thus the variance of grows with time, leading to the observed mean drift without corresponding increase in centered norm. D. System Prompts We employ specific system prompt to enforce the Chain-of-Thought (CoT) reasoning format. The exact instruction provided to the model is detailed in Table 7. System Prompt for Reasoning Enforcement You are helpful assistant. To answer the users question, you first think about the reasoning process and then provide the user with the answer. The reasoning process and answer are enclosed within <think> and <answer> tags, respectively, i.e., <think> reasoning process here </think> <answer> answer here </answer>. Table 7. The system prompt used to guide the models output structure, ensuring distinct separation between internal reasoning and the final response. E. Data Construction High-quality data is pivotal for effective alignment. We construct our training mixture from two primary domains: mathematics and code. Mathematical Reasoning: We source our math corpus from the AM-1.4M dataset (Zhao et al., 2025). To ensure the rigor of the reasoning chains, we utilized the math verify library to rigorously parse and cross-check final solutions against ground truths. Only samples that successfully passed this verification were retained, resulting in 111,709 high-quality samples. Code Generation: For the coding domain, we curated data from AM-1.4M dataset. We implemented an executionbased filtering pipeline: code blocks were explicitly extracted from the responses and validated against corresponding test cases to ensure functional correctness. After deduplication and strict quality control, we obtained 104,077 valid code samples. In total, the curated dataset comprises approximately 215k samples. We conducted training and evaluation for both tasks independently. Unless otherwise specified, all results reported in the main text are obtained by training with sequence length of 8192 tokens and evaluating with 4096 tokens. In addition, we consider two aligned context-length settings4k/4k and 8k/8kunder which our method also demonstrates consistent effectiveness."
        }
    ],
    "affiliations": [
        "Beihang University",
        "China Teleco"
    ]
}