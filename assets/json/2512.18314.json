{
    "paper_title": "MatSpray: Fusing 2D Material World Knowledge on 3D Geometry",
    "authors": [
        "Philipp Langsteiner",
        "Jan-Niklas Dihlmann",
        "Hendrik P. A. Lensch"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Manual modeling of material parameters and 3D geometry is a time consuming yet essential task in the gaming and film industries. While recent advances in 3D reconstruction have enabled accurate approximations of scene geometry and appearance, these methods often fall short in relighting scenarios due to the lack of precise, spatially varying material parameters. At the same time, diffusion models operating on 2D images have shown strong performance in predicting physically based rendering (PBR) properties such as albedo, roughness, and metallicity. However, transferring these 2D material maps onto reconstructed 3D geometry remains a significant challenge. We propose a framework for fusing 2D material data into 3D geometry using a combination of novel learning-based and projection-based approaches. We begin by reconstructing scene geometry via Gaussian Splatting. From the input images, a diffusion model generates 2D maps for albedo, roughness, and metallic parameters. Any existing diffusion model that can convert images or videos to PBR materials can be applied. The predictions are further integrated into the 3D representation either by optimizing an image-based loss or by directly projecting the material parameters onto the Gaussians using Gaussian ray tracing. To enhance fine-scale accuracy and multi-view consistency, we further introduce a light-weight neural refinement step (Neural Merger), which takes ray-traced material features as input and produces detailed adjustments. Our results demonstrate that the proposed methods outperform existing techniques in both quantitative metrics and perceived visual realism. This enables more accurate, relightable, and photorealistic renderings from reconstructed scenes, significantly improving the realism and efficiency of asset creation workflows in content production pipelines."
        },
        {
            "title": "Start",
            "content": "MatSpray: Fusing 2D Material World Knowledge on 3D Geometry Philipp Langsteiner philipp.langsteiner@uni-tuebingen.de Jan-Niklas Dihlmann jan-niklas.dihlmann@uni-tuebingen.de Hendrik Lensch hendrik.lensch@uni-tuebingen.de 5 2 0 2 0 ] . [ 1 4 1 3 8 1 . 2 1 5 2 : r Figure 1. MatSpray Overview we utilize 2D material world knowlegde from 2D diffusion models to reconstruct 3D relightable objects. Given multi-view images of target object, we first generate per-view PBR material predictions (base color, roughness, metallic) using any 2D diffusion-based material model. These 2D estimates are then integrated into 3D Gaussian Splatting reconstruction via Gaussian ray tracing. Finally, neural refinement stage applies softmax-based restriction to enforce multi-view consistency and enhance the physical accuracy of the materials. The resulting 3D assets feature high-quality, fully relightable PBR materials under novel illumination. Project page: https://matspray.jdihlmann.com/"
        },
        {
            "title": "Abstract",
            "content": "Manual modeling of material parameters and 3D geometry is time consuming yet essential task in the gaming and film industries. While recent advances in 3D reconstruction have enabled accurate approximations of scene geometry and appearance, these methods often fall short in relighting scenarios due to the lack of precise, spatially varying material parameters. At the same time, diffusion models operating on 2D images have shown strong performance in predicting physically based rendering (PBR) properties such as albedo, roughness, and metallicity. However, transferring these 2D material maps onto reconstructed 3D geometry remains significant challenge. We propose framework for fusing 2D material data into 3D geometry using combination of novel learning-based and projection-based approaches. We begin by reconstructing scene geometry via Gaussian Splatting. From the input images, diffusion model generates 2D maps for albedo, roughness, and metallic parameters. Any existing diffusion model that can convert images or videos to PBR materials can be applied. The predictions are further integrated into the 3D representation either by optimizing an image-based loss or by directly projecting the material parameters onto the Gaussians using Gaussian ray tracing. To enhance fine-scale accuracy and multi-view consistency, we further introduce lightweight neural refinement step (Neural Merger), which takes ray-traced material features as input and produces detailed adjustments. Our results demonstrate that the proposed methods outperform existing techniques in both quantitative metrics and perceived visual realism. This enables more accurate, relightable, and photorealistic renderings from 1 reconstructed scenes, significantly improving the realism and efficiency of asset creation workflows in content production pipelines. Project page: https://matspray. jdihlmann.com/ 1. Introduction Editing and relighting real scenes captured with casual cameras is central to many vision and graphics applications. While modern neural 3D reconstruction methods can produce impressive geometry and appearance from images, they often entangle illumination with appearance, yielding textures or coefficients that are not physically meaningful for relighting. Classical inverse rendering requires strong assumptions about lighting and exposure and remains fragile when materials vary spatially. In parallel, recent 2D material predictors learn rich priors from large-scale data and can produce plausible material maps from images, yet they operate in 2D and are not directly consistent across views or attached to 3D representation. We introduce method to transfer 2D material predictions onto 3D Gaussian representation to obtain relightable assets with spatially varying base color, roughness, and metallic parameters. The approach projects 2D material maps to 3D via efficient ray-traced assignment, refines materials with small MLP to reduce multi-view inconsistencies, and supervises rendered material maps directly with the 2D predictions to preserve plausible priors while discouraging baked-in lighting. This combination yields cleaner albedo, more accurate roughness, and informed metallic estimates, enabling higher-quality relighting compared to pipelines that learn only appearance. Our contributions are: World Material Fusion. plug-and-play pipeline that, to our knowledge, is the first to fuse swappable diffusionbased 2D PBR priors (world material knowledge) with 3D Gaussian material optimization via Gaussian ray tracing and PBR consistent supervision to obtain relightable assets. Neural Merger. softmax neural merger that aggregates per-Gaussian, multi-view material estimates, suppresses baked-in lighting, and enforces cross-view consistency while stabilizing joint environment map optimization. Faster Reconstruction. simple projection and optimization scheme that reconstructs high-quality relightable 3D materials with 3.5 less per-scene optimization time than IRGS [9]. 2. Related Work Materials Spatially varying BRDFs (svBRDFs) have long been studied, with early high-resolution texel-based representations enabling point-wise material parameterizaIn this work, we adopt CookTorrance tion [25, 44]. variant, which is based on the widely used Disney principled BRDF and real-time formulations in major engines [5, 19, 23]. Building on this foundation, recent research has explored richer material parameterizations and extended the expressiveness of svBRDF models [12]. Diffusion Diffusion models enable high-fidelity image synthesis and conditioning, with efficient latent-space formulations and extensions to video generation [2, 13, 14, 42]. For material estimation from 2D images, large-scale diffusion priors have been used to infer PBR maps [8, 20, 27, 55, 59]. Of particular relevance is DiffusionRenderer by Huang et al. [16], whose results on high-quality material maps motivated this work. Diffusion approaches have been further explored for related tasks such as HDR prediction, texture estimation, and relighting [1, 30, 54]. Gaussian Ray Tracing We exploit Gaussian Ray Tracing as mechanism for transferring 2D material data to 3D. Although the field remains in its early stages, emerging works have explored stochastic and explicit Gaussian raytracing methods [34, 39, 47] and related neural optimization schemes [10]. Our formulation builds upon insights from Mai et al. [34] and Moenne-Loccoz et al. [39], extending them toward material-aware 3D reconstruction. Novel View Synthesis and Scene Reconstruction Neural representations for novel view synthesis and scene reconstruction have rapidly advanced 3D modeling, with NeRF and Gaussian Splatting providing strong foundations for radiance-based scene representations [21, 37]. Material modeling atop radiance fields has been explored through inverse rendering and relighting [3, 4, 43, 57], where coupling with signed distance fields (SDFs) improves physical plausibility [36, 50]. Gaussian-based inverse rendering approaches such as R3DGS [9] reconstruct materials via perscene optimization, while IRGS [11] extends this with 2D Gaussians and deferred shading for improved appearance modeling. Complementary efforts leverage diffusion models for 3D reconstruction and sparse-view recovery [31, 38], and hybrid cues or mesh integration further enhance geometric fidelity [28, 29, 48, 52]. Beyond these, extensions of 2D and 3D Gaussians have enabled spatially varying materials, advanced relighting, and richer reflectance modeling [6, 7, 15, 18, 22, 24, 26, 32, 33, 46, 51, 53, 58]. In contrast to R3DGS and IRGS, which optimize material and geometry parameters per scene, our approach employs Gaussian Ray Tracing to lift 2D material estimates by diffusion models into 3D representations, exploiting the world-knowledge priors learned by the diffusion models and the broader understanding of material behavior they 2 Figure 2. Pipeline. From multi-view images, diffusion predictor yields per-view material maps. We reconstruct the objects geometry using 3D Gaussian Splatting. Then we project 2D materials to 3D via ray tracing, and refine per Gaussian materials with our Neural Merger that has softmax output layer, choosing between the projected values. We then supervise the produced material maps using the predicted 2D material maps. Additionally, using deferred shading we supervise by PBR-based photometric rendering loss with the multi-view ground truth images of the object. provide. This enables faster, more consistent reconstruction and material reasoning across scenes. 3. Method Our method recovers consistent 3D PBR materials from multiple views by combining 2D diffusion predictions with 3D Gaussian representation. We first obtain material maps (base color, roughness, metallic) per view from any diffusion material predictor, making our approach compatible with wide range of existing and future diffusion models. Scene geometry is reconstructed via relightable Gaussian Splatting pipeline (R3DGS) [9, 21], which provides both geometry and normals. The 2D material estimates are then transferred to 3D and jointly refined for multi-view consistency by newly introduced Neural Merger step. The materials maps and normals are further refined based on rendering loss, evaluated with deferred rendering. The illumination is modeled by an optimizable environment map during refinement. Specifically, we (1) lift 2D materials to 3D via Gaussian ray tracing, (2) refine per-Gaussian material parameters with the Neural Merger across views, and (3) supervise rendered material maps to preserve plausible 2D priors while suppressing baked-in lighting. construction of 3D PBR materials. Each material channel, base color (three channels), roughness (single channel), and metallic (single channel), is inferred explicitly. In practice, we evaluate multiple prebuilt diffusion-based predictors and select the one that provides the best fidelity and consistency balance for our data. In our experiments, this is DiffusionRenderer [27]. We also tested Marigold [20] and RGB-toX [55]. We choose DiffusionRenderer because it achieves about thirty percent higher PSNR than the other methods. DiffusionRenderer predicts material maps from short frame batches and uses limited temporal context to improve consistency and material understanding within each batch. While this improves local consistency, the predicted materials in batch can still differ across views. The model cannot recover the complete environment illumination from small input batch, which may lead to small shifts in color, roughness, or metallic appearance within batch. In addition, results from separate batches may not align, as these images may introduce new information that was not visible in previous batches. These variations make direct projection of the predicted maps unreliable and often result in blurry and washed out material maps. Our method resolves this by refining and merging the estimates into one consistent 3D representation. 3.2. 2D-to-3D Material Lifting 3.1. Diffusion Material Prediction We leverage pretrained diffusion priors to predict physically meaningful per-view material maps, enabling accurate reFor each view vi, we collect the material attributes (base color, roughness, metallic) corresponding to every Gaussian from the pixels within its projected footprint pp using 3 Gaussian ray tracing, following the formulation of Mai et al. [35]. Their approach determines each Gaussian or ellipsoids contribution to ray based on density. Because the opacity α used in Gaussian Splatting [21] does not directly correspond to physical density, we adopt the formulation by Moenne-Loccoz et al. [39], which allows the direct use of Gaussian Splatting opacity α in ray tracing. For Gaussian with mean µ and covariance Σ, the point of maximum response xmax along ray with origin and direction is τmax = (µ o)Σ1d dΣ1d , xmax = + τmaxd. (1) The corresponding opacity αmax, given base opacity α and falloff parameter λ > 0, is captured by the diffusion priors while enforcing coherence across views. For each Gaussian g, the Neural Merger takes as input the projected material values mg,v for all views {1, . . . , }, along with the Gaussian position pg, encoded using positional encoding. The input is processed by lightweight MLP fθ to produce unnormalized weights hg,v for each view: [hg,1, hg,2, . . . , hg,V ] = fθ (cid:16) pg, mg,1, . . . , mg,V (cid:17) . (7) softmax function then converts these outputs into normalized weights: wg,v = exp(hg,v) v=1 exp(hg,v) (cid:80)V , (cid:88) v=1 wg,v = 1. (8) αmax = α exp (cid:18) 1 2 λ(xmax µ)Σ1(xmax µ) (cid:19) . The merged material mg for the Gaussian is computed as the weighted sum of the per-view predictions: (2) Material values per pixel mp are then assigned to the Gaussians intersected by the ray and aggregated across all pixels in each Gaussians footprint fpg,vi per view. To reduce color distortion from outliers and overlapping footprints, we compute median of all assigned material parameters mg,vi per Gaussian: mg,vi = medianpfpg,vi (mp). (3) After computing the median for each view, the resulting values are assigned to their corresponding Gaussians. Gaussians not intersected in any view are removed. Grid-based supersampling per pixel is used to ensure stable Gaussian hits. For each Gaussian g, we now obtain arrays of material estimates across all views: basecolorg = {bg,1, bg,2, . . . , bg,n}, metallicg = {mg,1, mg,2, . . . , mg,n}, roughnessg = {rg,1, rg,2, . . . , rg,n}. (4) (5) (6) These arrays contain the inconsistent material values per view produced by DiffusionRenderer. These inconsistencies motivate the subsequent Neural Merger step, which refines the estimates into coherent 3D representation. 3.3. Neural Merger To reduce inconsistencies across views, we introduce the Neural Merger, which predicts weights per view for the material parameters collected during the projection step for each Gaussian. It fuses the predictions into single, consistent estimate. The key idea is to interpolate between the predicted values rather than allowing the network to freely generate new colors or material values. This ensures that the merged results remain consistent with the world knowledge mg = (cid:88) v=1 wg,v mg,v. (9) The Neural Merger is optimized during the refinement explained in the next section. Using the softmax weighting is crucial. Without it, the merger can converge faster than the environment map optimization, producing unrealistic material values that match the ground truth only superficially. By interpolating among the predicted values, the Neural Merger produces physically plausible material estimates while allowing the environment map to converge reliably. In our framework, we use separate Neural Merger for each material channel, enabling improved disentanglement of the materials. 3.4. Refinement, Supervision and Loss Functions The Neural Merger produces material values per Gaussian, which are then rasterized into material maps. These maps are iteratively refined using two complementary supervised losses. First, we supervise the rendered material maps against the diffusion models 2D material predictions using an L1 loss. This loss is applied exclusively to the material parameters, thereby optimizing only the Neural Merger. Given the rendered material maps Mrender and the diffusion-predicted material maps M2D, the material supervision loss LImage is defined as: LImage = Mrender M2D 1. (10) Second, the rasterized materials are used for deferred shading to generate physically based rendering (PBR) image. This rendered image is then compared to the groundtruth input using the loss introduced in Gaussian Splatting [21]. The rendering supervision loss is defined as: L3DGS = λ L1(IPBR, IGT)+(1λ) LSSIM(IPBR, IGT), (11) Figure 3. Relighting Comparison between our method, an extended version of R3DGS [9] and IRGS [11]. The objects are all relit under the same environment maps. In IRGS, reconstructed scene geometry might partially occlude the environment map. where IPBR denotes the rendered image, IGT is the groundtruth image, and λ [0, 1] (typically set to 0.8). This loss supervises both the Neural Merger and the environment map estimation, ensuring that the final rendering is consistent with the input views. 4. Experiments We evaluate our method on both synthetic and real-world datasets from the Navi dataset [17], comparing against state-of-the-art approaches for 3D material estimation and relighting, specifically an extended version of R3DGS [9] (modified to support metallic materials) and IRGS [11]. Our evaluation includes qualitative comparisons of material maps and relighting quality, quantitative metrics across material channels, computational performance, and an ablation study. 4.1. Implementation Details All methods use the same experimental setup for fair comparison. Synthetic scenes are initialized with unit cube point cloud containing 100,000 points sampled uniformly at random. Real-world scenes use structure-from-motion reconstruction from COLMAP [45] for both initialization and camera inference. We evaluate on 17 synthetic objects with ground-truth material maps for quantitative analysis. The synthetic dataset uses 100 training images and 200 evaluation images per object. Real-world objects use all available images (average of 27 per object). To handle highly specular objects, we train Gaussian Splatting on DiffusionRenderer normals as RGB, which helps guide the geometry more effectively, since Gaussian Splatting alone often struggles with specular surfaces and can produce holes in the reconstruction. The Neural Merger consists of separate MLPs for each material channel (basecolor, roughness, metallic), each with 3 hidden layers of 128 neurons and ReLU activations. The final layer outputs view-specific weights passed through softmax to form probability distribution. We spend 30.000 iterations for the 3D Gaussian geometry optimization and 10.000 for the material refinement. All experiments run on an NVIDIA RTX 4090 GPU [40]. We using estimation evaluate material PSNR, SSIM [49], and LPIPS [56] between predicted and For relighting, we renground-truth material maps. der novel views under different environment maps and compare against ground-truth renderings using the same metrics. 4.2. Qualitative Results Relighting Quality Figure 3 compares relighting quality across methods. Our method produces results that more closely resemble ground truth, particularly for specular objects. The extended R3DGS struggles with specular materials, often producing brighter images than ground truth 5 Figure 4. Material Maps produced by our method compared to extended R3DGS [9], which can also predict metallic material maps, IRGS [11], and the DiffusionRenderer material output produced on the test images that are not used for training. We show four images each, where the top left is the base color, top right is the roughness, bottom left is metallic, and bottom right are the normals. Figure 5. Real-World Comparison of our method, extended R3DGS [9] and IRGS [11]. The ground truth images show the object masked (top) and unmasked (bottom) to give better understanding of the object and the surrounding lighting. due to unconstrained material maps during joint environment map optimization. In contrast, our method constrains material maps, enabling more accurate environment map optimization and improved material estimation. IRGS exhibits artifacts such as floaters and overly flat surfaces. While it reconstructs flat surfaces well using 2D 6 Table 1. Quantitative Comparison of material estimation and relighting results on 17 synthetic objects. We compare against IRGS [11] and an extended version of R3DGS [9] that supports metallic materials. * For non-metallic objects, our model correctly optimizes the parameter to zero, which can result in infinite PSNR when all metallic maps are predicted as zero."
        },
        {
            "title": "Ours",
            "content": "Ext. R3DGS"
        },
        {
            "title": "IRGS",
            "content": "PSNR SSIM LPIPS PSNR SSIM LPIPS PSNR SSIM LPIPS"
        },
        {
            "title": "Relighting\nBaseColor\nRoughness\nMetallic",
            "content": "27.282 21.341 15.331 /27.202 0.897 0.873 0.820 0.893 0.080 0.125 0.181 0.106 25.483 18.360 14.473 10.073 0.875 0.832 0.763 0.693 0.094 0.158 0.216 0. 24.409 19.204 16.182 N/A 0.850 0.750 0.744 N/A 0.166 0.139 0.192 N/A Gaussians, it loses fine detail compared to our method. Moreover, IRGS cannot predict metallic materials, preventing accurate reconstruction of highly specular metallic surfaces. Our method achieves clear advantages in reconstructing metallic and highly specular materials while preserving geometric detail across both diffuse and flat surfaces. On the real-world relighting results in Figure 5 one can observe the most consistent results with our method. With R3DGS, the estimated color appears too bright, the objects are too shiny and the geometry is less smooth, while IRGS produces smooth geometry but too dark colors. Material Maps Figure 4 compares material maps across methods. Our method effectively removes baked-in lighting effects from basecolor maps, producing nearly diffuse basecolors with minimal shadowing, while other methods exhibit visible shadows and residual lighting. Compared to DiffusionRenderers original predictions, which exhibit significant view-dependent inconsistencies, our Neural Merger enhances multi-view consistency. in roughness and metallic This is particularly evident maps, where DiffusionRenderers per-view predictions vary across views. Our approach removes these inconsistencies while preserving high-quality spatially varying material properties. Our metallic maps match ground truth well, with predictions substantially closer than competing methods. For roughness, our method guides material maps toward consistent values for surfaces sharing the same properties, improving upon the 2D predictions. While DiffusionRenderer struggles with roughness accuracy (reflected in darker roughness maps), our Neural Merger refines initial predictions by enforcing view consistency, resulting in more physIn contrast, R3DGS tends to ically plausible parameters. overestimate roughness due to specular highlights appearing in only subset of the images, biasing optimization toward diffuse surfaces. IRGS produces overly uniform roughness where fine details are difficult to discern. Although IRGS normals are slightly closer to ground truth in some regions, our method significantly improves normals compared to extended R3DGS despite starting from the same 3D Gaussian geometry. Overall, our method produces qualitatively superior material maps across basecolor, normals, metallic, and roughness channels. 4.3. Quantitative Results Table 1 shows quantitative results on 17 synthetic objects. Our method consistently outperforms all baselines in relighting quality and basecolor estimation, consistent with the qualitative comparisons. For roughness, IRGS achieves slightly higher PSNR (16.182 vs. 15.331), but our method achieves better SSIM (0.820 vs. 0.744) and LPIPS (0.181 vs. 0.192), indicating better preservation of structural information and perceptual quality. All methods face challenges in roughness estimation, which remains difficult problem. For metallic maps, our method shows substantial improvement. When correctly predicting fully non-metallic objects, PSNR becomes infinite, which occurs exclusively for our method. Those objects are left out in the actual PSNR calculation. While DiffusionRenderer often predicts partial metallicity in certain views our method enforces view-consistent material estimation, producing stable nonmetallic predictions. Table 2. Runtime Comparison between our method and IRGS on the Navi dataset. All timings are reported in seconds and measured on an NVIDIA RTX 4090 GPU. Our method is approximately 3.5 faster than IRGS. Stage Ours (s) IRGS (s) Diffusion Predictions Gaussian Splatting Normal Generation (R3DGS) Material Optimization Total 112 131 270 975 1488 - 2490 - 2857 7 4.4. Computation Time Table 2 shows runtime breakdown on the Navi dataset [17]. DiffusionRenderer requires 112 seconds (6 seconds per image) to process the full image set. Gaussian Splatting takes 131 seconds on average (ranging from 64 to 274 seconds depending on object complexity). Normal generation using R3DGS takes 270 seconds on average (247347 seconds). Material optimization requires 975 seconds on average, extending up to 3,631 seconds (approximately one hour) for complex objects. In total, our method takes 1,488 seconds (25 minutes) on average, approximately 3.5 faster than IRGS (5,347 seconds, 89 minutes). 4.5. Ablation Study Table 3 demonstrates that the Neural Merger is the key component responsible for our methods superior performance. The full model achieves the highest scores across all metrics (PSNR: 29.164, SSIM: 0.9105, LPIPS: 0.0626), significantly outperforming all ablated variants. This improvement stems from the Neural Mergers ability to enforce multi-view consistency while preserving high-quality material properties predicted by the diffusion model. Figure 6. Qualitative Comparison of our ablations. The differences are most apparent in the base color, which has the largest impact on the qualitative appearance during relighting and consists of three channels rather than one. Table 3. Quantitative Ablation study on subset of the evaluated objects. The Supervised variant uses only the predicted 2D diffusion-based material maps for supervision, without any 3D optimization. The Proj. Average variant directly projects all 2D predictions onto the Gaussians and averages them, without applying any further training or optimization. The Full model includes the Neural Merger. Method PSNR SSIM LPIPS Full Supervised Proj. Average 29.164 24.809 25.555 0.9105 0.889 0.866 0.0626 0.0792 0.122 8 The Supervised variant performs worse than the full model, relying solely on diffusion predictions without geometric or photometric optimization. Interestingly, it is even worse than the Proj. Average baseline, which simply projects diffusion predictions into Gaussians without training. We attribute this to view-dependent effects captured in optimized Gaussians that are absent without optimization. The qualitative results in Figure 6 show that the Neural Merger produces cleaner and more consistent material maps. Visualized using base color (where baked-in lighting effects are most evident), the Neural Merger yields substantial improvements in both sharpness and color fidelity compared to the Proj. Average baseline, which serves as our initialization. These results validate that the Neural Merger enhances visual quality and enforces consistency with underlying physical properties, resulting in more accurate and realistic relighting outcomes. 5. Conclusion MatSpray enables casual acquisition and high-quality reconstruction of photorealistic relightable 3D assets with spatially varying materials. It effectively lifts 2D material predictions to 3D to fuse them with the 3D Gaussian geometry. It employs pretrained 2D diffusion-based material estimators without requiring additional expensive training on large-scale 3D PBR data sets. Introducing the Neural Merger, our method significantly improves multi-view consistencies, which even video-based material prediction models still struggle with. The resulting relightable 3D models feature improved quality both in the estimated material maps as well as in the final relit appearance, even more so for highly specular objects. As demonstrated, MatSpray outperforms current state-of-the-art methods and excels in reconstructing accurate metallic maps for both synthetic and real-world inputs. The approach provides powerful tool for easy 3D content generation. Limitations While our approach drastically improves multi-view consistency, the overall material quality remains dependent on the performance of the chosen diffusion model. However, our PBR-to-image loss partially corrects small deviations in the diffusion predictions 4 DiffusionRenderer vs. MatSpray. Our method struggles when inconsistent geometry and normals are produced by the underlying R3DGS method [9], though the photometric loss may partially mitigate these issues. Additionally, very small or flat Gaussians might sometimes be missed during ray tracing. Future work could address missing Gaussians through projection transformer combination assignment similar to [41]. Future Work The high quality of the resulting 3D geometry-material association could be exploited for accurate 3D object part segmentation. This segmentation, paired with matching language features, might enable objectspecific constraints in reconstruction or natural interface for manipulating both geometry and reflections. 6. Acknowledgements Funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under Germanys Excellence Strategy EXC number 2064/1 Project number 390727645. This work was supported by the German Research Foundation (DFG): SFB 1233, Robust Vision: Inference Principles and Neural Mechanisms, TP 02, project number: 276693517. This work was supported by the Tubingen AI Center. The authors thank the International Max Planck Research School for Intelligent Systems (IMPRS-IS) for supporting Jan-Niklas Dihlmann."
        },
        {
            "title": "References",
            "content": "[1] Hrishav Bakul Barua, Kalin Stefanov, Ganesh Krishnasamy, KokSheik Wong, and Abhinav Dhall. Physhdr: When lighting meets materials and scene geometry in hdr reconstruction, 2025. 2 [2] Andreas Blattmann, Robin Rombach, Kaan Oktay, and Bjorn Ommer. Align your latents: High-resolution video arXiv preprint synthesis with latent diffusion models. arXiv:2304.08818, 2023. 2 [3] Mark Boss, Raphael Braun, Varun Jampani, Jonathan Barron, Ce Liu, and Hendrik Lensch. Nerd: Neural reflectance In Proceedings of decomposition from image collections. the IEEE/CVF International Conference on Computer Vision, pages 1268412694, 2021. 2 [4] Mark Boss, Varun Jampani, Raphael Braun, Ce Liu, Jonathan Barron, and Hendrik Lensch. Neural-pil: Neural pre-integrated lighting for reflectance decomposition. Advances in Neural Information Processing Systems, 34: 1069110704, 2021. 2 [5] Brent Burley. Physically based shading at disney. In ACM SIGGRAPH 2012 Courses: Physically Based Shading in Theory and Practice, Los Angeles, CA, 2012. Course Notes. 2 [6] Hongze Chen, Zehong Lin, and Jun Zhang. Gi-gs: Global illumination decomposition on gaussian splatting for inverse rendering. arXiv preprint arXiv:2410.02619, 2024. [7] Jan-Niklas Dihlmann, Arjun Majumdar, Andreas Engelhardt, Raphael Braun, and Hendrik Lensch. Subsurface scattering for gaussian splatting. Advances in Neural Information Processing Systems, 37:121765121789, 2024. 2 [8] Andreas Engelhardt, Mark Boss, Vikram Voleti, Chun-Han Yao, Hendrik P.A. Lensch, and Varun Jampani. SViM3D: Stable video material diffusion for single image 3d generation. In ICCV, 2025. 2 [9] Jian Gao, Chun Gu, Youtian Lin, Zhihao Li, Hao Zhu, Xun Cao, Li Zhang, and Yao Yao. Relightable 3d gaussians: Realistic point cloud relighting with brdf decomposition and In European Conference on Computer Vision, ray tracing. pages 7389. Springer, 2024. 2, 3, 5, 6, 7, 8, 12, 13 [10] Shrisudhan Govindarajan, Daniel Rebain, Kwang Moo Yi, and Andrea Tagliasacchi. Radiant foam: Real-time differentiable ray tracing. arXiv preprint arXiv:2502.01157, 2025. 2 [11] Chun Gu, Xiaofei Wei, Zixuan Zeng, Yuxuan Yao, and Li Zhang. Irgs: Inter-reflective gaussian splatting with 2d gaussian ray tracing. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1094310952, 2025. 2, 5, 6, 7, 12, 13 [12] Yu Guo, Zhiqiang Lao, Xiyun Song, Yubin Zhou, Zongfang Lin, and Heather Yu. epbr: Extended pbr materials in image synthesis. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 327336, 2025. [13] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In Advances in Neural Information Processing Systems, 2020. 2 [14] Jonathan Ho, William Chan, Chitwan Saharia, David J. Fleet, Mohammad Norouzi, and Tim Salimans. Video diffusion models. In Advances in Neural Information Processing Systems, 2022. 2 [15] Chenxiao Hu, Meng Gai, Guoping Wang, and Sheng Li. illumination for dynamic 3d gaussian Real-time global scenes. arXiv preprint arXiv:2503.17897, 2025. 2 [16] Yifeng Huang, Zhang Chen, Yi Xu, Minh Hoai, and Zhong Li. Dualmat: Pbr material estimation via coherent dual-path diffusion. arXiv preprint arXiv:2508.05060, 2025. 2 [17] Varun Jampani, Kevis-Kokitsi Maninis, Andreas Engelhardt, Arjun Karpur, Karen Truong, Kyle Sargent, Stefan Popov, Andre Araujo, Ricardo Martin-Brualla, Kaushal Patel, Daniel Vlasic, Vittorio Ferrari, Ameesh Makadia, Ce Liu, Yuanzhen Li, and Howard Zhou. Navi: Categoryagnostic image collections with high-quality 3d shape and pose annotations. In NeurIPS, 2023. 5, 8 [18] Joanna Kaleta, Kacper Kania, Tomasz Trzcinski, and Marek Kowalski. Lumigauss: Relightable gaussian splatting in the wild. In 2025 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), pages 110. IEEE, 2025. 2 [19] Brian Karis. Real shading in unreal engine 4. Technical report, Epic Games, 2013. 2 [20] Bingxin Ke, Kevin Qu, Tianfu Wang, Nando Metzger, Shengyu Huang, Bo Li, Anton Obukhov, and Konrad Schindler. Marigold: Affordable adaptation of diffusionbased image generators for image analysis, 2025. 2, 3 [21] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkuhler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM Trans. Graph., 42(4):1391, 2023. 2, 3, 4 [22] Georgios Kouros, Minye Wu, and Tinne Tuytelaars. Rgs-dr: Reflective gaussian surfels with deferred rendering for shiny objects. arXiv preprint arXiv:2504.18468, 2025. 2 [23] Sebastien Lagarde and Charles de Rousiers. Moving frostbite to physically based rendering 3.0. In ACM SIGGRAPH 2014 Courses: Physically Based Shading in Theory and Practice, Vancouver, Canada, 2014. Course Slides. 2 9 [24] Shuichang Lai, Letian Huang, Jie Guo, Kai Cheng, Bowen Pan, Xiaoxiao Long, Jiangjing Lyu, Chengfei Lv, and Yanwen Guo. Glossygs: Inverse rendering of glossy objects with IEEE Transactions on Visualization 3d gaussian splatting. and Computer Graphics, 2025. [25] Hendrik P. A. Lensch, Jan Kautz, Michael Goesele, Wolfgang Heidrich, and Hans-Peter Seidel. Image-based reconstruction of spatially varying materials. In Proceedings of the 12th Eurographics Conference on Rendering, page 103114, Goslar, DEU, 2001. Eurographics Association. 2 [26] Jingzhi Li, Zongwei Wu, Eduard Zamfir, and Radu Timofte. Recap: Better gaussian relighting with cross-environment In Proceedings of the Computer Vision and Patcaptures. tern Recognition Conference, pages 2130721316, 2025. 2 [27] Ruofan Liang, Zan Gojcic, Huan Ling, Jacob Munkberg, Jon Hasselgren, Zhi-Hao Lin, Jun Gao, Alexander Keller, Nandita Vijaykumar, Sanja Fidler, and Zian Wang. Diffusionrenderer: Neural inverse and forward rendering with video diffusion models. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2025. 2, 3, 12 [28] Zhihao Liang, Qi Zhang, Ying Feng, Ying Shan, and Kui Jia. Gs-ir: 3d gaussian splatting for inverse rendering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2164421653, 2024. 2 [29] Lianjun Liao, Chunhui Zhang, Tong Wu, Henglei Lv, Bailin Deng, and Lin Gao. Rosgs: Relightable outdoor scenes with gaussian splatting, 2025. 2 [30] Yehonathan Litman, Fernando De la Torre, and Shubham Tulsiani. Lightswitch: Multi-view relighting with materialguided diffusion. arXiv preprint arXiv:2508.06494, 2025. 2 [31] Bo Liu, Runlong Li, Li Zhou, and Yan Zhou. Dt-nerf: diffusion and transformer-based optimization approach for neural radiance fields in 3d reconstruction, 2025. 2 [32] Shiyong Liu, Xiao Tang, Zhihao Li, Yingfan He, Chongjie Ye, Jianzhuang Liu, Binxiao Huang, Shunbo Zhou, and Xiaofei Wu. Occlugaussian: Occlusion-aware gaussian splatarXiv ting for large scene reconstruction and rendering. preprint arXiv:2503.16177, 2025. 2 [33] Zhenyuan Liu, Yu Guo, Xinyuan Li, Bernd Bickel, and Bigs: Bidirectional gaussian primitives arXiv preprint relightable 3d gaussian splatting. Ran Zhang. for arXiv:2408.13370, 2024. 2 [34] Alexander Mai, Peter Hedman, George Kopanas, Dor Verbin, David Futschik, Qiangeng Xu, Falko Kuester, Jonathan Barron, and Yinda Zhang. Ever: Exact volumetric ellipsoid rendering for real-time view synthesis. arXiv preprint arXiv:2410.01804, 2024. 2 [35] Alexander Mai, Peter Hedman, George Kopanas, Dor Verbin, David Futschik, Qiangeng Xu, Falko Kuester, Jonathan Barron, and Yinda Zhang. Ever: Exact volumetric ellipsoid rendering for real-time view synthesis. arXiv preprint arXiv:2410.01804, 2024. 4 [36] Shi Mao, Chenming Wu, Zhelun Shen, Yifan Wang, Dayan Wu, and Liangjun Zhang. Neus-pir: Learning relightable neural surface using pre-integrated rendering. Computational Visual Media, 2025. 2 [37] Ben Mildenhall, Pratul Srinivasan, Matthew Tancik, Jonathan Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. Communications of the ACM, 65(1):99106, 2021. 2 [38] Niluthpol Chowdhury Mithun, Tuan Pham, Qiao Wang, Ben Southall, Kshitij Minhas, Bogdan Matei, Stephan Mandt, Supun Samarasekera, and Rakesh Kumar. Diffusionguided gaussian splatting for large-scale unconstrained 3d arXiv preprint reconstruction and novel view synthesis. arXiv:2504.01960, 2025. 2 [39] Nicolas Moenne-Loccoz, Ashkan Mirzaei, Or Perel, Riccardo de Lutio, Janick Martinez Esturo, Gavriel State, Sanja Fidler, Nicholas Sharp, and Zan Gojcic. 3d gaussian ray tracing: Fast tracing of particle scenes. ACM Transactions on Graphics and SIGGRAPH Asia, 2024. 2, 4 [40] NVIDIA Corporation. Nvidia geforce rtx 4090 specifications, 2022. 24GB GDDR6X VRAM, 16,384 CUDA cores, 450W TDP. 5 [41] Kerui Ren, Jiayang Bai, Linning Xu, Lihan Jiang, Jiangmiao Pang, Mulin Yu, and Bo Dai. Mv-colight: Efficient object compositing with consistent lighting and shadow generation. arXiv preprint arXiv:2505.21483, 2025. 8 [42] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image In Proceedings of synthesis with latent diffusion models. the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022. [43] Viktor Rudnev, Mohamed Elgharib, William Smith, Lingjie Liu, Vladislav Golyanik, and Christian Theobalt. Nerf for outdoor scene relighting. In European Conference on Computer Vision, pages 615631. Springer, 2022. 2 [44] Yoichi Sato, Mark D. Wheeler, and Katsushi Ikeuchi. Object shape and reflectance modeling from observation. In Proceedings of the 24th Annual Conference on Computer Graphics and Interactive Techniques, page 379387, USA, 1997. ACM Press/Addison-Wesley Publishing Co. 2 [45] Johannes Schonberger and Jan-Michael Frahm. StructureIn Proceedings of the IEEE confrom-motion revisited. ference on computer vision and pattern recognition, pages 41044113, 2016. 5 [46] Hanxiao Sun, Yupeng Gao, Jin Xie, Jian Yang, and Beibei Wang. Svg-ir: Spatially-varying gaussian splatting for inverse rendering. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1614316152, 2025. 2 [47] Xin Sun, Iliyan Georgiev, Yun Fei, and Miloˇs Haˇsan. Stochastic ray tracing of 3d transparent gaussians. arXiv preprint arXiv:2504.06598, 2025. 2 [48] Zipeng Wang and Dan Xu. Hyrf: Hybrid radiance fields for efficient and high-quality novel view synthesis. The ThirtyNinth Annual Conference on Neural Information Processing Systems (NeurIPS), 2025. [49] Zhou Wang, Alan Bovik, Hamid Sheikh, and Eero Simoncelli. Image quality assessment: from error visibility to structural similarity. IEEE transactions on image processing, 13(4):600612, 2004. 5 [50] Sean Wu, Shamik Basu, Tim Broedermann, Luc Van Gool, and Christos Sakaridis. Pbr-nerf: Inverse rendering with physics-based neural fields. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 10974 10984, 2025. 2 [51] Zirui Wu, Jianteng Chen, Laijian Li, Shaoteng Wu, Zhikai Zhu, Kang Xu, Martin Oswald, and Jie Song. 3d gaussian inverse rendering with approximated global illumination. arXiv preprint arXiv:2504.01358, 2025. 2 [52] Haofei Xu, Songyou Peng, Fangjinhua Wang, Hermann Blum, Daniel Barath, Andreas Geiger, and Marc Pollefeys. Depthsplat: Connecting gaussian splatting and depth. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1645316463, 2025. 2 [53] Xingyuan Yang and Min Wei. Gogs: High-fidelity geometry and relighting for glossy objects via gaussian surfels, 2025. 2 [54] Zhi Ying, Boxiang Rong, Jingyu Wang, and Maoyuan Xu. Chord: Chain of rendering decomposition for pbr material estimation from generated texture images, 2025. [55] Zheng Zeng, Valentin Deschaintre, Iliyan Georgiev, Yannick Hold-Geoffroy, Yiwei Hu, Fujun Luan, Ling-Qi Yan, and Miloˇs Haˇsan. Rgbx: Image decomposition and synthesis using materialand lighting-aware diffusion models. In ACM SIGGRAPH 2024 Conference Papers, New York, NY, USA, 2024. Association for Computing Machinery. 2, 3 [56] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. Proceedings of the IEEE conference on computer vision and pattern recognition, pages 586595, 2018. 5 [57] Xiuming Zhang, Pratul Srinivasan, Boyang Deng, Paul Debevec, William Freeman, and Jonathan Barron. Nerfactor: Neural factorization of shape and reflectance under an unknown illumination. ACM Transactions on Graphics (ToG), 40(6):118, 2021. 2 [58] Xiaoming Zhao, Pratul Srinivasan, Dor Verbin, Keunhong Park, Ricardo Martin Brualla, and Philipp Henzler. Illuminerf: 3d relighting without inverse rendering. Advances in Neural Information Processing Systems, 37:4259342617, 2024. 2 [59] Rongjia Zheng, Qing Zhang, Chengjiang Long, and Wei-Shi Zheng. Dnf-intrinsic: Deterministic noise-free diffusion for indoor inverse rendering. arXiv preprint arXiv:2507.03924, 2025. 2 11 MatSpray: Fusing 2D Material World Knowledge on 3D Geometry"
        },
        {
            "title": "Overview",
            "content": "This supplementary material provides extended results, analyses, and implementation details that complement the findings in the main paper. For ease of navigation, the main components are summarized here and referenced through the corresponding section labels. Additional Videos and Real-World Objects (A): detailed collection of video comparisons and reconstructions of real objects that highlight the performance and stability of our method relative to earlier approaches. Neural Merger Ablation (B): An extended analysis of the importance of the final Softmax layer in the Neural Merger, supported by qualitative and quantitative evidence. Tone Mapping Analysis (C): discussion of the tone mapping behaviour of DiffusionRenderer, how this affects predicted base color, roughness and metallic maps, and why this creates mismatch when compared to linear ground truth. Implementation Details (D): description of our training setup, super sampling strategy, Neural Merger inputs and other practical considerations that are important for stable optimization. A. Additional Videos and Real-World Objects Figure 7 shows the thumbnail that links to all additional videos included with this supplementary material. These videos provide an extensive visual comparison of our method with Extended R3DGS [9], IRGS [11], and the forward renderer of DiffusionRenderer [27]. While the main paper presents representative examples, the extended videos Figure 7. Thumbnail showing six videos that can be viewed here. Three videos show relighting comparison and three show material prediction comparisons. 12 give more complete picture of the consistency and stability of our approach, especially compared to the produced material maps of DiffusionRenderer. Across the set of videos, our method consistently produces reconstructions that remain stable across all viewpoints, without the flickering or structural collapse that can be observed in the other methods. This is particularly visible in objects with complex geometry or pronounced specular highlights such as the Kettle. Extended R3DGS often fails to maintain surface smoothness and yields unstable representations. On the other hand IRGS tends to oversmooth surfaces and tends to bake in specular reflections of metallic objects into its base color. In contrast, our approach maintains coherent structure even under strong lighting variations. To illustrate this, we provide three relighting videos: White Golden Airplane, Stone Birdhouse, and Kettle. Additionally, three videos visualize predicted material properties: Yellow Airplane, Birdhouse with Yellow Flower, and Chair. These examples show that DiffusionRenderer, despite being trained on its own dataset, still produces inconsistent material maps that vary strongly with camera angle and lighting. Our method mitigates these issues and aligns predictions across views more reliably. Real-World Objects Figure 8 shows additional real objects reconstructed by our method and by Extended R3DGS and IRGS. Here, each method is evaluated under two relighting settings and compared in base color and normals. The differences are most obvious in the base color: our base color is locally sharp and coherent across the surface, while both baselines exhibit noise, distortions, or view-dependent artifacts. The relighting results further demonstrate that our predicted materials generalize well across lighting conditions, while the other methods still have lighting effects baked into their materials (R3DGS) or tend to be washed out (IRGS). B. Neural Merger Ablation The Neural Merger plays key role in ensuring that the material parameters assigned to each Gaussian remain stable and consistent across all viewpoints. One central element of the Neural Merger is the final Softmax layer, which normalizes its output into weights acting as weighted average of the inputs. Although this layer may apFigure 8. Additional real objects reconstructed with our method, Extended R3DGS [9] and IRGS [11]. The figure includes relighting under two environments, base color and normal maps. Figure 9. The impact of the Softmax layer in the Neural Merger. Without it, lighting and shadow patterns leak into the material maps, leading to inconsistent relighting. 13 Figure 10. Tone mapping applied by DiffusionRenderer significantly alters the appearance of material maps. The alpha mask removes background content and focuses on the region of interest. pear to be small architectural detail, it has sizable impact on the quality of the final reconstruction. Without the Softmax normalization, the Neural Merger becomes unconstrained and starts to absorb illumination cues directly from the training images. In other words, instead of learning clean, view-independent materials, the MLP blends in signals that correspond to lighting variations and shadows. Because these patterns differ between viewpoints, the network produces material values that vary from view to view, which leads to inconsistency during rendering. Although this might also be additionally influenced by slight variations in the 2D Diffusion predictions geometry. This behaviour becomes especially problematic under relighting, because the embedded shadows and highlights interfere with the simulated lighting and produce unrealistic results. Figure 9 shows comparison between the full method, the version without the Softmax, and the linear ground truth. The differences become clear when observing fine geometric structures and shadow placement. Without Softmax, shadows from the input images appear in the base color maps and the renderings become blurry in high detail areas. These issues are especially visible in the lower birdhouse example, where the version without Softmax fails to maintain consistent materials on the swim ring and the surrounding areas. We further quantify these findings in Table 4, which reports results across all scenes in the dataset. The full model outperforms the version without the Softmax across all metrics, with especially large gains in perceptual similarity (LPIPS). This confirms that the Softmax-based normalization is not merely numerical improvement but key component that ensures robustness and prevents the network from encoding view-dependent appearance into the materials. C. DiffusionRenderer Tone Mapping Analysis One recurring observation in our experiments was that the base color predicted by our method tended to appear darker than the linear ground truth material map. This appeared to be miss-prediction of the 2D material maps by DiffusionRenderer for few objects. However, this discoloration appeared in almost all objects that we tested, hinting towards systemic problem in DiffusionRenderer. Figure 10 illustrates this systemic discoloration of the predicted base color. This indicates that during training DiffusionRenderer was supervised using tone-mapped ground truth images. Our analysis suggests that DiffusionRenderer employs filmic or AgX tone mapping curve. These tone-mapping algorithms compress high dynamic range values into the Table 4. Ablation Study on all objects. Removing the Softmax layer causes the network to encode lighting, which degrades all metrics. Method PSNR SSIM LPIPS Full Without Softmax 27.282 24.600 0.897 0.874 0.080 0.114 14 prevents the model from overfitting shadows while still enforcing high fidelity in the material maps. During training, we also apply random view sampling to avoid biasing the model toward any particular viewpoint. Super Sampling key technical detail is the use of super sampling during the projection of material values into the Gaussian representation. We employ 1616 grid of rays per pixel to ensure that even small or distant Gaussians receive material parameters. With fewer samples, Gaussians are occasionally missed leading to patchy geometry and low resolution material parameter transferal. Figure 11 shows an example where lower sampling rate produces obvious reconstruction defects. Merger Inputs Finally, Figure 12 illustrates the input to the Neural Merger. The features consist of NeRF-style positional encoding of the Gaussian location along with the projected base color, roughness and metallic values. The combination of positional encoding and projected materials allows the network to balance local detail with global consistency, which is essential for producing clean results under relighting. range expected by standard displays, which improves visual quality but complicates the recovery of physically meaningful material parameters. In particular, these tone mappings are not analytically invertible, and even approximate inverse curves introduce errors, especially near shadows or highlights. Base color is affected in predictable way, because tone mapping acts like softened gamma curve. Applying an inverse gamma of roughly one point eight partially recovers the linear values but cannot undo the full nonlinearity. Roughness is affected more severely, because its values occupy small part of the zero to one interval, which collapses under tone mapping. Metallic values, on the other hand, remain closer to either zero or one and thus suffer less from compression. These effects explain why our predicted material maps sometimes differ from the linear ground truth as they closely match DiffusionRenderers tone-mapped output. D. Implementation Details Our experiments were performed on an NVIDIA RTX 4090 GPU with PyTorch, C++ and Optix. To keep the input consistent with the internal resolution of DiffusionRenderer, we render all training views at resolution of 512512 pixel. This choice ensures that the reconstruction quality aligns with the scale at which DiffusionRenderer was originally trained. In scenes with strong specular highlights, we disable geometry learning entirely and keep the Gaussian positions fixed, because additional geometric optimization tends to destabilize the representation under these conditions. The Neural Merger is optimized using learning rate of zero point zero zero one. Material supervision uses an L1loss with weight of 1.0, as we found that this balance Figure 11. Super sampling avoids missed Gaussians and ensures reliable projection of material supervision. Lower sampling rates cause holes and unstable geometry. Figure 12. The input to the Neural Merger includes positional codes and projected material parameters."
        }
    ],
    "affiliations": [
        "University of Tübingen"
    ]
}