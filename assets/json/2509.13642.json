{
    "paper_title": "LLM-I: LLMs are Naturally Interleaved Multimodal Creators",
    "authors": [
        "Zirun Guo",
        "Feng Zhang",
        "Kai Jia",
        "Tao Jin"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We propose LLM-Interleaved (LLM-I), a flexible and dynamic framework that reframes interleaved image-text generation as a tool-use problem. LLM-I is designed to overcome the \"one-tool\" bottleneck of current unified models, which are limited to synthetic imagery and struggle with tasks requiring factual grounding or programmatic precision. Our framework empowers a central LLM or MLLM agent to intelligently orchestrate a diverse toolkit of specialized visual tools, including online image search, diffusion-based generation, code execution, and image editing. The agent is trained to select and apply these tools proficiently via a Reinforcement Learning (RL) framework that features a hybrid reward system combining rule-based logic with judgments from LLM and MLLM evaluators. Trained on a diverse new dataset using four different model backbones, LLM-I demonstrates state-of-the-art performance, outperforming existing methods by a large margin across four benchmarks. We also introduce a novel test-time scaling strategy that provides further performance gains. Project Page: https://github.com/ByteDance-BandAI/LLM-I."
        },
        {
            "title": "Start",
            "content": "LLM-I: LLMs are Naturally Interleaved Multimodal Creators Zirun Guo1,2, Feng Zhang2, Kai Jia2, Tao Jin1 1Zhejiang University, 2ByteDance, BandAI 5 2 0 2 7 1 ] . [ 1 2 4 6 3 1 . 9 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "We propose LLM-Interleaved (LLM-I), flexible and dynamic framework that reframes interleaved image-text generation as tool-use problem. LLM-I is designed to overcome the one-tool bottleneck of current unified models, which are limited to synthetic imagery and struggle with tasks requiring factual grounding or programmatic precision. Our framework empowers central LLM or MLLM agent to intelligently orchestrate diverse toolkit of specialized visual tools, including online image search, diffusion-based generation, code execution, and image editing. The agent is trained to select and apply these tools proficiently via Reinforcement Learning (RL) framework that features hybrid reward system combining rule-based logic with judgments from LLM and MLLM evaluators. Trained on diverse new dataset using four different model backbones, LLM-I demonstrates state-of-the-art performance, outperforming existing methods by large margin across four benchmarks. We also introduce novel test-time scaling strategy that provides further performance gains. Date: September 18, 2025 Email: Zirun Guo at zrguo.cs@gmail.com Project Page: https://github.com/ByteDance-BandAI/LLM-I"
        },
        {
            "title": "1 Introduction",
            "content": "AI is shifting from single-modality systems to multimodal ones that can process mixed data like text, images, and sound. key frontier is interleaved image-text generation [Ge et al., 2024, Tian et al., 2024, Xie et al., 2025, Zhou et al., 2025b, Xia et al., 2025, Chen et al., 2025]: producing coherent, alternating sequence of text and images from single prompt. However, the task is technically demanding, requiring high-fidelity text and images with strict cross-modal consistency. This involves maintaining narrative coherence, consistent visual style and entities, and strong semantic alignment between each image and its accompanying text. To address these challenges, the research community has largely converged on two dominant architectural paradigms for interleaved image-text generation. The first, two-stage or compositional approach, leverages the distinct strengths of separate, state-of-the-art models [Zhou et al., 2025b] or add decoders [Ge et al., 2024] after the text generation. In this paradigm, powerful LLM, such as GPT-4o [Hurst et al., 2024], acts as high-level reasoning engine. It interprets the users request to produce sequence of textual narratives, which are then passed to separate, high-fidelity text-to-image diffusion model, such as DALL-E [Betker et al., 2023] or Seedream [Gao et al., 2025], for visual synthesis. However, it often suffers from semantic gap, where the LLMs textual representation of desired image may not perfectly align with the diffusion models interpretation, leading to inconsistencies. Furthermore, these systems lack flexibility, as they are typically restricted to generating fixed number of images per response. Seeking to close this gap and achieve greater architectural elegance, significant research effort has been 1 directed towards developing unified, end-to-end models [Xie et al., 2025, Zhou et al., 2025a, Deng et al., 2025] that handle both multimodal understanding and generation within single, integrated framework. Despite their notable advancements, current unified models for interleaved generation suffer from critical and largely unaddressed limitation: the one-tool bottleneck. While these unified models excel at generating novel, high-fidelity synthetic imagery from textual prompts, they are inherently ill-suited for tasks that require factual grounding such as real-world images or programmatic precision such as data analysis and visualizations. This architectural commitment creates rigid system that forces single tool to solve all visual generation problems, regardless of its suitability. This one-tool bottleneck reflects deeper paradigm choice in AI development: the pursuit of an omniscient solver that embeds all knowledge within its parameters, rather than proficient tool-user that knows how to leverage external resources. The latter approach is inherently more flexible, scalable, and robust. tool-augmented system can be easily updated with new capabilities by simply adding new tool to its repertoire, whereas monolithic model requires complete and computationally prohibitive retraining to acquire new skills. In this paper, we introduce LLM-Interleaved (LLM-I), flexible and dynamic framework that employs an LLM or MLLM as an agentic planner. This central agent leverages its sophisticated reasoning and multimodal understanding capabilities to intelligently orchestrate diverse suite of external, specialized visual tools for image generations. Our framework equips the central agent with toolkit of four distinct and complementary visual tools which are online image search, diffusion-based generation, code generation and execution, and image edit tool. To ensure the agent uses these tools proficiently, we develope Reinforcement Learning (RL) framework that incorporates hybrid reward design, combining rule-based rewards and LLM and MLLM judges. We build diverse dataset for training and evaluate LLM-I using four different backbone models, finding that it outperforms state-of-the-art methods by large margin across four benchmarks. Additionally, we propose novel test-time scaling strategy that improves performance even further. We summarize our key contributions as follows: 1. Novel Framework for Interleaved Generation: We propose new and flexible paradigm, LLM-I, for interleaved image-text generation. Our framework recasts the LLM/MLLM not as an end-to-end generator but as an intelligent agent that orchestrates toolkit of external, specialized visual models. This approach decouples high-level reasoning from low-level synthesis, enabling unprecedented flexibility and contextappropriateness in the generated multimodal content. 2. New Dataset and Benchmark: We introduce diverse dataset and difficult benchmark for interleaved image-text generation. Our work moves beyond the scope of previous datasets by requiring multiple forms of images, including retrieved real-world photos, synthetic visuals, and programmatic visualizations. 3. Strong Performance: LLM-I outperforms previous SOTA methods by large margin across four benchmarks. Thourgh test-time scaling, the performance is further improved."
        },
        {
            "title": "2 Related Work",
            "content": "Interleaved Image-Text Generation. While current MLLMs, such as the QwenVL [Bai et al., 2025] and InternVL [Zhu et al., 2025] series, excel at processing interleaved image-text inputs, they lack the capability for interleaved generation. Two primary approaches have emerged to address this limitation. The first involves leveraging an external image decoder or diffusion model, as seen in models like NExT-GPT [Wu et al., 2024] and SEED-X [Ge et al., 2024]. These methods typically optimize set of learnable visual tokens that serve as input for diffusion-based image decoder or directly input all the texts into the diffusion model. The second category consists of unified multimodal models that either integrate an autoregressive model with diffusion model [Zhou et al., 2025a, Xie et al., 2025] or are entirely autoregressive [Team, 2024, Chern et al., 2024] to achieve unified training and alignment. However, significant drawback of both paradigms is their inherent unsuitability for tasks requiring factual grounding, such as generating photorealistic images of specific entities, or programmatic precision, such as data analysis and visualization. Diverging from these methods, our approach reframes the LLM or MLLM as an agentic planner that orchestrates four external tools. This tool-augmented framework allows for the creation of wide range of visual content, from photorealistic and 2 creative imagery to accurate data visualizations, thereby overcoming the key weaknesses of prior generative systems. Reinforcement Learning. RL has become crucial component in developing the latest generation of large models [Guo et al., 2025a], often yielding superior generalization capabilities compared to purely supervised methods. While Proximal Policy Optimization (PPO) [Schulman et al., 2017] is the most common algorithm for fine-tuning LLMs, its reliance on value model has spurred the popularity of value-free alternatives like GRPO [Shao et al., 2024] and DAPO [Yu et al., 2025]. Although many recent works have successfully applied these algorithms to enhance the reasoning abilities of LLMs and MLLMs [Zheng et al., 2025, Guo et al., 2025b, Hong et al., 2025], our research explores different direction. Instead of focusing on reasoning, we investigate how RL can be used to improve multimodal alignment, the ability to intelligently use tools, and the overall quality of generated reports. Tool Usage of LLMs. The ability of LLMs to utilize external tools [Feng et al., 2025, Wu et al., 2025] has significantly expanded their capabilities, transforming them from simple text generators into sophisticated agents capable of reasoning, decision-making, and task automation across various domains. For instance, proprietary models like the OpenAI o3 [OpenAI, 2025c] and DeepResearch [OpenAI, 2025a] model can leverage various tools for web search, code execution, and image processing. Similarly, Gemini 2.5 Pro [Comanici et al., 2025] and its DeepResearch [Google, 2024] can call external tools for functions like code execution, web search, or file processing. In the open-source community, projects such as Search-o1 [Li et al., 2025] and Openthinkimg [Su et al., 2025] have also demonstrated the impressive performance improvements of tool-augmented LLMs and MLLMs. Building on these advancements, RL training can further enhance this capability, enabling an LLM to intelligently select the appropriate tool to use, making it possible to address wider and more complex range of problems."
        },
        {
            "title": "3.1 Tool Usage",
            "content": "3.1.1 Motivation As we discussed above, current methods [Chern et al., 2024, Wu et al., 2024, Zhou et al., 2025a, Xie et al., 2025] are locked into single mode of creation, limiting the scope, factuality, and utility of the narratives they can produce. It is instructive to consider how humans approach similar task, such as authoring blog post or technical report. When writer needs to insert an image, they rarely create it from scratch. Instead, they act as an intelligent agent, selecting the best external tool for the job. If they need picture of the Eiffel Tower, they use search engine to find real photograph. To display quarterly sales data, they would use software like PowerPoint or coding library to generate precise chart. They might also use an image editing tool like Photoshop to make adjustments, such as cropping photo, adjusting its colors, or adding annotations to highlight key information. This human workflow is not monolithic; it is dynamic, flexible, and tool-centric. The writers primary skill is not drawing but reasoning and orchestrating diverse set of specialized tools to achieve their goal. Therefore, we argue that paradigm that mimics this human-like, tool-using strategy holds significant advantages over current monolithic models. An AI system that can intelligently invoke external tools is inherently more flexible, scalable, and robust. It can ground its generations in factual reality by searching the web, provide precise data visualizations through code execution, and still retain the ability for other tasks. This approach directly overcomes the one-tool bottleneck, moving beyond the limited omniscient solver paradigm towards more powerful and practical proficient tool-user. 3.1.2 Toolkit Motivated by this insight, we introduce flexible and dynamic framework where an LLM or MLLM serves as an agentic planner. We empower this central agent to intelligently orchestrate suite of distinct visual tools to construct rich, interleaved content. Specifically, our framework equips the agent with capabilities for online image search, diffusion-based generation, code execution for data visualization, and image editing. Figure 1 Overview of the LLM-I framework. 1. Online Image Search: Invoked for requests demanding factual grounding, such as specific real-world entities, landmarks, or current events. This tool ensures visual authenticity and provides access to up-to-date information beyond the models training data cutoff. In our paper, we use Google Search API [Google, 2025b]. 2. Diffusion-based Generation: Selected for tasks requiring the creative synthesis of novel or abstract concepts, or complex compositions that do not exist in reality. We support Seedream 3.0 [Gao et al., 2025] in our paper. 3. Code Execution: Utilized primarily for generating data visualizations like charts, graphs, and plots from structured data. We use Python as the programming language and build controlled sandbox environment. 4. Image Editing: Engaged to perform modifications on existing visual content, whether inputted, retrieved or generated. We support Seededit 3.0 [Wang et al., 2025] in our project. 3.1.3 How to call tool? To empower the LLM to dynamically orchestrate our suite of visual tools, we design robust and flexible tool invocation framework. Instead of complex, multi-turn interactions or fine-tuning on specific API call formats, our approach is guided by system prompt that instructs the model to embed specific placeholder tag wherever visual element is required in the narrative. This method allows the LLM to autonomously decide when and how to use tool within single generative pass. The core of our framework is the structured tag, <imgen>{...}</imgen>, which encapsulates all the necessary information for generating or retrieving an image. When the LLM determines that an image is needed, it generates this tag in the following JSON-like format: <imgen>{\"source\":\"<source type>\", \"description\":\"<general title>\", \"params\":{...}}</imgen> For search, the params contains single key query which holds practical and concise search string for web image search engine. For diffusion, it contains the key prompt, which provides descriptive text prompt for the generative model. For code, it contains the key code which holds the raw Python code snippet required to generate plot or visualization. For edit, it contains two keys, img index, the 0-based index of previously image in the sequence to be modified, and prompt, textual instruction describing the desired edit. When the tag is detected in the generated sequence, parser processes this output, identifies each tag, and dispatches call to the corresponding external tool using the provided parameters. The tag is then replaced in the text with the image returned by the tool, resulting in the final, seamless multimodal document."
        },
        {
            "title": "3.2 RL Recipe",
            "content": ""
        },
        {
            "title": "3.2.1 Dataset Construction",
            "content": "To effectively train our model to master the agentic tool-use framework, we first construct high-quality RL dataset. The central design philosophy is tool-oriented, aimed at teaching the model to invoke diverse set of tools under various constraints. The dataset is bifurcated into two primary categories: text-only inputs and text-and-image inputs. The generation process is automated using Gemini 2.5 Pro [Comanici et al., 2025]. We guide prompt creation through categorical scaffolding system that defines the target tool(s), pre-designed specific theme for the tool, an image count which implicitly specifies how many images should be given in the response, and difficulty level (low, medium, high). crucial principle is that all generated prompts are implicit; they describe desired outcome and image number that necessitates specific tool without ever naming it, thereby encouraging the model to reason about tool selection and image number. To counteract the agents potential aversion to more error-prone tools during RL (a form of reward hacking), we deliberately increase the representation of prompts requiring code and search, which have higher failure rates than the more predictable diffusion tool. For the text-and-image input subset, the generation process is adapted to produce both an instructional prompt and textual description of required input images. This description is then used to synthesize the image via Nano Banana [Google, 2025a]. The composition of this subset is slightly weighted towards the edit tool, as its function is inherently tied to modifying existing visual content. To ensure the quality and fidelity of the entire dataset, we implement rigorous multi-stage validation pipeline using GPT-4o [Hurst et al., 2024] as an independent adjudicator. This pipeline verifies three key aspects for each sample: the consistency of the intended image count, the appropriateness of the designated tool for the given instruction, and, for the text-and-image subset, the cross-modal alignment between the synthesized input image and its textual description. Any sample that fails validation check is discarded, resulting in high-quality, unambiguous dataset optimized for robust RL-based agent training. Finally, we get around 4k samples. critical feature of this dataset is the annotation of each prompt with an image num constraint. This metadata guides the RL training process by specifying the rules of image generation for each task (Section 3.2.2). The constraint falls into one of four categories: images are disallowed (-1), their use is unconstrained (0), precise quantity is required (n > 0), or at least one image is mandatory (Inf). 3.2.2 Reward With the instruction dataset in place, we employ an RL strategy to fine-tune the models ability to appropriately call and parameterize the visual tools. Our approach is distinguished by multi-faceted reward function that combines deterministic rules Rrule with sophisticated judgments from both LLM Rllm and MLLM Rmllm. This composite reward signal not only provides holistic assessment of the generated output but also decreases reward hacking. The first component is deterministic, rule-based reward Rrule that enforces adherence to generation constraints and ensures the correctness of the <imgen> tag format. In Section 3.2.1, we set required image number Nreq for each single item. For categorical constraints, the reward is binary. When images are disallowed (Nreq = 1) or when at least one is required (Nreq = inf), the model receives score of 1 for compliance and 0 for violation. When there is no constraint (Nreq = 0), the score is always 1, as any output is considered valid. For quantitative constraints where precise number of images is required (Nreq = n), the reward is designed to penalize both underand over-generation: Rrule = ( Ngen Nreq max(0, 1 α (Ngen Nreq)) if 0 Ngen Nreq if Ngen > Nreq (1) where Ngen is the number of generated images in the model response and α is the penalty factor of extra images which is set to 0.3 by default. 5 Figure 2 Overview of test-time scaling framework for LLM-I. The second component Rllm leverages an external LLM as judge to assess the quality of the language and the logic of the tool invocation. This judge evaluates two criteria on 1-to-5 scale: (i) the fluency, coherence, and relevance of the textual narrative, and (ii) the quality of the tool-use tags, including the naturalness of their placement and the semantic appropriateness of the chosen source and params. The third reward component Rmllm employs an MLLM to evaluate the final interleaved output. After the images are generated and integrated, this judge scores three key aspects of multimodal quality on 1-to-5 scale: (i) the technical and aesthetic quality of the image itself, (ii) the semantic alignment between the image and its surrounding text, and (iii) the relevance of the image to the overall task objective. The scores from the LLM and MLLM judges are normalized to [0, 1] range. The final reward signal R, is then composed from all three components. Notably, the rule-based reward Rrule, acts as multiplicative gate on the MLLM reward Rmllm. This formulation ensures that visual quality is considered only if the model has first satisfied the explicit image count constraint. The composite reward is thus defined as: = wruleRrule + wllmRllm + wmllmRmllmRrule (2) where wrule, wllm, and wmllm are the trade-offs between the three losses."
        },
        {
            "title": "3.3 Test-time Scaling",
            "content": "To further enhance the performance of our agentic framework, we introduce test-time scaling [Snell et al., 2025, Muennighoff et al., 2025] paradigm that leverages additional computational resources during inference. The goal is to improve both the reliability of tool usage and the overall quality of the final multimodal response. The workflow is illustrated in Figure 2. Given user query, the model first generates multiple complete candidate responses through stochastic sampling. Each candidate may contain tool calls (e.g., search, diffusion, code, or editing), interleaved with natural language. The initial candidates are passed through Tool Call Check filter. This stage validates the structural integrity and executability of the tool invocations. Responses with malformed or failed tool calls are discarded. From the pool of successful candidates, selector model (an LLM/MLLM) evaluates their overall quality and relevance to the prompt, selecting the top-k most promising responses for further enhancement. Then, each of the selected candidates undergoes targeted enhancement process based on the type of tool used. When response requests an image, we concurrently query both the online image search module and the diffusion model. The resulting candidates are evaluated by an MLLM, which selects the most semantically aligned option. If code execution fails, the erroneous code and the associated error message are provided to model. The model revises the code, which is then re-executed in sandboxed environment until valid visualization is obtained or exceeding fixed number of attempts. After the enhancement, the refined interleaved multimodal responses are passed to an MLLM for polishing. This step improves the coherence and alignment between modalities, ensuring that visual outputs are seamlessly integrated with textual explanations. Finally, selector model ranks the polished candidates and chooses the single best response as the final output."
        },
        {
            "title": "4 Benchmark",
            "content": "To rigorously assess models capability in generating sophisticated, interleaved text-image reports, we develop new benchmark. This is motivated by two primary limitations we observe in existing public benchmarks [Liu et al., 2024, Zhou et al., 2025b, Chen et al., 2025]. First, current benchmarks often feature overly simplistic and generic prompts, such as Generate travel guide to Beijing with text and images. The tasks in such benchmarks do not necessitate deep reasoning, and the requested images are often decorative rather than integral to the content (shown in Figure 10). These images typically have low informational density, are stylistically uniform (e.g., lifestyle photos), and can be adequately produced by standard diffusion models without complex planning. Consequently, they fail to test models ability to generate meaningful, context-aware visuals that are essential for high-quality report. Second, the evaluation protocols of existing benchmarks rely heavily on subjective metrics. They commonly employ models like GPT-4o to score outputs based on broad criteria such as text-image alignment, text quality, and image quality. This approach is problematic, as LLMs tend to assign forgivingly high scores even to suboptimal outputs. In our preliminary tests, we observe instances where model fails to generate an image and instead provides only textual description, yet still receives favorable score from the GPT-4o evaluator. This highlights the unreliability of using vague, subjective rubrics for evaluation. To overcome these challenges, our benchmark introduces new paradigm for both task design and evaluation. We reframe the task of interleaved generation as mini-project. Each prompt in our benchmark provides background context or specific data. The tasks are designed to demand images with high informational value and stylistic diversity, moving beyond simple photographic illustrations. The required images include visuals like data analysis, scientific illustrations, and creative content. In this framework, images are not merely supplementary; they are an indispensable component of the report, carrying critical information that is synergistic with the text. The goal is to ensure that each image serves distinct purpose, reflecting genuine user need for visual information. We present four samples in Figure 4, 5, 12, and 13. To address the issue of subjective evaluation, we transition from broad rubrics to sample-specific, objective evaluation protocol. Instead of asking an LLM for holistic quality score, we design unique set of concrete and verifiable criteria for each mini-project\" sample. For instance, for report on sales trends, the evaluation criteria include specific, verifiable checks such as Does the report accurately generate line chart for sales from 2014 to 2025 with correct points and labels according to the provided data? For each sample in our benchmark, we define 10 distinct evaluation metrics. We utilize GPT-4o to assess the generated report against these specific rules, assigning score on three-point scale: 0 (requirement not met), 1 (partially met), or 2 (fully met). This method transforms the evaluation from subjective assessment into more objective and reliable measurement of models capabilities. Our final benchmark is concise yet comprehensive, comprising 30 meticulously designed and manually vetted samples. These samples cover diverse range of topics and user requirements, with 18 being text-only inputs and 12 being multi-modal inputs. We deliberately emphasize quality over quantity. The compact size of 30 samples is strategic choice to facilitate rigorous and manageable human evaluation. Our approach ensures that each sample can be carefully analyzed, enabling deeper and more accurate understanding of model performance."
        },
        {
            "title": "5.1 Setup",
            "content": "Data and Benchmarks: We train our model using the data constructed in Section 3.2.1, which is split into training set and an in-domain test set containing over 200 samples. For comprehensive evaluation, we utilize this in-domain test set along with three out-of-domain (OOD) benchmarks. On the in-domain set, we employ the same metrics used during training: rule-based metric, LLM-based judgments, and MLLM-based judgments. For OOD evaluation, we use the public OpenING benchmark [Zhou et al., 2025b], which has over 2,000 samples, and adopt the seven metrics from the original paper. Besides, we use the public benchmark ISG [Chen et al., 2025], which has over 1,000 samples, and adopt the four metrics from the original paper. 7 Table 1 Results on the OpenING benchmark. is evaluated with text-only input samples. IT Coherency refers to image-text conherency and MS consistency means multi-step consistency. Qwen series are all evaluated with tools. Model Completeness Quality Richness Correctness Human Alignment IT Coherency MS Consistency Overall GPT4o+DALLE3 Gemini+FLUX NExT-GPT Show-o SEED-X Anole Qwen2.5-VL-7B Qwen2.5-VL-32B MLLM-I-7B MLLM-I-32B Qwen3-4B-Instruct Qwen3-30B-Instruct LLM-I-4B LLM-I-30B 8.66 7.58 3.89 4.37 5.65 6. 2.97 6.78 6.00 8.35 6.26 8.05 8.63 9.19 8.01 7.26 4.25 4.79 6.07 6.02 3.90 6.82 6.75 8.07 6.88 7.63 8.03 8. 7.42 6.48 3.35 3.83 4.92 5.28 2.50 5.89 5.53 7.48 5.55 7.09 7.54 8.08 7.98 7.03 3.61 3.76 5.77 5. 3.07 6.34 5.85 7.79 6.09 7.56 8.03 8.61 8.77 7.98 5.35 5.78 7.03 6.91 4.37 7.25 7.24 8.44 6.95 8.12 8.69 8. 8.15 6.98 3.32 4.04 5.72 4.90 2.03 5.69 5.85 7.35 5.11 6.90 7.87 8.40 8.38 7.33 3.85 4.33 5.72 5. 3.82 7.15 6.50 8.38 6.86 8.13 8.45 8.91 8.20 7.23 3.95 4.41 5.84 5.75 3.24 6.56 6.25 7.98 6.24 7.64 8.18 8. Table 2 Results on the LLMI-Bench. is evaluated with text-only input samples. Tool Acc refers the success rate of tool invocation. Model GPT-5 wTool GPT-4o wTool Anole Qwen2.5-VL-7B wTool Qwen2.5-VL-32B wTool Qwen2.5-VL-72B wTool MLLM-I-7B MLLM-I-32B Qwen3-4B-Instruct wTool Qwen3-30B-Instruct wTool LLM-I-4B LLM-I-30B Rubric Human Tool Acc Overall 53.8 70.4 27.4 28.5 58.9 73.1 67.1 92.5 73.6 81.4 88.9 94.8 48.3 62.8 18.2 19.3 51.1 59.8 61.9 82.1 62.3 69.2 72.9 83. 28.1 67.9 - 44.3 93.4 60.1 97.4 99.2 68.7 83.1 100.0 100.0 43.4 67.0 22.8 30.7 67.8 64.3 75.5 91.3 68.2 77.9 82.3 92. Additionally, we introduce our novel and much more difficult LLMI-Bench, whose manageable size enables multifaceted evaluation through rubric-based scoring rate from GPT-4o, rule-based metric to measure tool-call success, and rigorous human evaluation. For this human assessment, we design five-point Likert scale with detailed criteria for each point and calculate the final metric as the average overall scoring rate. Training: We conduct experiments using four different backbones, covering both LLMs and MLLMs. They include Qwen3-4B-Instruct, Qwen3-30B-Instruct (MoE model), Qwen2.5-VL-7B, and Qwen2.5-VL-32B. For MoE model, we use GSPO [Zheng et al., 2025] as the RL algorithm while for others we use GRPO [Shao et al., 2024]. We use batch size of 32 with cosine learning rate scheduler where the initial learning is set to 1e-6, minimum learning rate ratio is set to 0.01, and the warm-up step is 5. Following Yu et al. [2025], we use the token-level loss. For GSPO, the clipping ratios are set to 3e-4 (low) and 4e-4 (high). For judgement, we use Qwen3-235B-Instruct-2507 [Yang et al., 2025] as the LLM judge and Qwen2.5-VL-72B-Instruct [Bai et al., 2025] as the MLLM judge. The reward trade-off coefficients are set to wrule = 0.2, wllm = 0.5, and wmllm = 0.3."
        },
        {
            "title": "5.2 Main Results",
            "content": "Tables 1, 2, 3 and 4 present the detailed results of our model across four distinct benchmarks. Our evaluation compares LLM-I against diverse set of baselines categorized into three main types: (i) two-stage or compositional methods such as GPT-4o+DALLE-3, Gemini+FLUX/Stable Diffusion 3 (SD3) [Esser et al., 2024], NExT-GPT [Wu et al., 2024], SEED-X [Ge et al., 2024], and ISG [Chen et al., 2025]; (ii) unified models including Show-o (Autoregressive+Diffusion) [Xie et al., 2025] and Anole (Pure Autoregressive) [Chern et al., 8 Table 3 Results on the ISG benchmark. is evaluated with text-only input samples. Model Structural Holistic Block Image Show-o Anole Gemini+SD3 ISG Qwen2.5-VL-7B Qwen2.5-VL-32B MLLM-I-7B MLLM-I-32B Qwen3-4B Qwen3-30B LLM-I-4B LM-I-30B 0.295 0.000 0.385 0.871 0.085 0.221 0.607 0.776 0.068 0.267 0.881 0. 2.329 2.810 5.827 6.262 4.932 6.354 6.381 8.112 5.621 7.848 8.413 8.492 1.962 - 3.081 5.515 1.152 2.105 3.584 5.722 1.621 3.811 7.701 8. 0.078 - 0.113 0.574 0.016 0.088 0.274 0.419 0.086 0.267 0.511 0.618 Table 4 Results on the test set of the dataset. is evaluated with text-only input samples. IT means image-text and IQ means image-question. Qwen series are all evaluated with tools. Model Image num Text Quality Tag Quality Image Quality IT Alignment IQ Alignment Overall Qwen2.5-VL-7B Qwen2.5-VL-32B MLLM-I-7B MLLM-I-32B Qwen3-4B-Instruct Qwen3-30B-Instruct LLM-I-4B LLM-I-30B 13.2 54.3 88.7 95. 46.5 55.3 88.6 93.0 3.1 3.8 4.0 4.7 4.5 4.8 4.8 4.9 1.5 2.0 3.4 4.3 2.9 4.0 4.6 4.8 3.7 3.8 3.9 4. 4.0 3.9 4.2 4.3 2.8 3.6 3.9 4.2 3.9 3.9 4.2 4.6 2.9 3.5 3.7 4.3 3.9 3.9 4.3 4.6 25.9 52.7 70.6 85. 57.7 68.7 85.2 89.9 2024]; and (iii) tool-augmented methods, featuring GPT-5 [OpenAI, 2025b] and GPT-4o with suite of tools that includes search, diffusion, code, and editing capabilities. Across all four benchmarks, we observe that LLM-I exhibits SOTA performance, consistently and significantly outperforming baseline models. In the general qualitative assessment shown in Table 1 and 3, the entire LLM-I family shows highly competitive performance, outperforming other leading models and unified approaches, which underscores our models robustness in generating complete, high-quality, and well-aligned content. This superiority is even more pronounced on specialized benchmarks. On the LLMI-Bench evaluation in Table 2, LLM-I models drastically outperform dedicated tool-using agents, including GPT-4o w/Tool and the anticipated GPT-5 w/Tool. This success is largely attributable to our models exceptional tool invocation capabilities, with LLM-I-4B and LLM-I-30B achieving perfect 100.0 Tool Accuracy. Additionally, we present the metrics during the RL training in Figure 3. We can observe that with the RL training, the instruction following ability, writing ability and the ability to find images of the model all increases, indicating the effectiveness of RL training. Figure 6 Tool F1 score curve during RL training. Furthermore, we present two examples in Figure 4 and 5. From the examples, we can observe that LLM-I can intelligently invoke different kinds of tools for image presentation. This shows great advantages over previous methods when requiring real and precise images. In the dataset construction stage in Section 3.2.1, we define target tool list for each data item which is verfied by Gemini2.5 Pro and GPT-4o. To further validate the intelligence of tool invocation, we visualize the tool F1 score in Figure 6 which evaluates the precision and recall of different tools during the training process. From the figure, the F1 score steadily improves during the (a) Rule-based Reward (b) MLLM Judge Reward (c) LLM Judge Reward Figure 3 Reward curve during RL training of Qwen2.5-VL-32B. Table 6 Results of reward ablation experiments on the OpenING benchmark. Model Completeness Quality Richness Correctness Human Align. IT Conhe. MS Consis. Overall Qwen3-4B-Instruct LLM-I-4B w/o rule-based reward w/o LLM judge w/o MLLM judge 6. 8.63 4.22 8.29 8.17 6.88 8.03 6.55 7.77 7.66 5. 7.54 3.93 7.23 7.20 6.09 8.03 4.55 7.69 7.60 6. 8.69 5.68 8.38 8.23 5.11 7.87 2.34 7.44 7.39 6. 8.45 6.05 8.18 8.04 6.24 8.18 4.76 7.85 7.76 RL process, indicating that the model becomes increasingly adept at selecting appropriate tools according to the given context. Notably, no explicit reward is assigned to tool usage; the improvement arises naturally during RL training. This finding suggests that RL not only encourages tool invocation but also enhances the models ability to make smarter tool choices for achieving better imagetext alignment."
        },
        {
            "title": "5.3 Test-Time Scaling",
            "content": "Table 5 presents the performance of our proposed test-time scaling strategy. As detailed in Section 3.3, this strategy comprises four stages: initial top-k selection, tool enhancement, polishing, and final selection. In our experiments, we set the initial selection parameter as 4 and sample 2 images for both the search and diffusion tools. The Qwen2.5-VL-72B model serves as both the selector and the polisher. The results in Table 5 demonstrate the efficacy of our approach. The initial top-k selection and tool enhancement stages substantially boost performance. The subsequent polishing stage also provides improvement. By integrating all four stages, our model surpasses the performance of its 30B counterpart, validating the effectiveness of our test-time scaling strategy. We also analyze the computational overhead introduced by this strategy. key advantage is that tool invocations can be processed in parallel. Consequently, the primary overhead consists of only four additional forward passes from the selector/polisher model. The selection process is particularly efficient, as the model only needs to output the optimal index rather than generating full response. In contrast, the polishing stage is the most time-consuming, as it requires rewriting the entire response."
        },
        {
            "title": "5.4 Ablation Study",
            "content": "Table 5 Results of test-time scaling on LLMI-Bench. Model Rubric Time LLMI-4B - stage1 - stage2 - stage3 full TTS LLM-I-30B 88.9 91.2 91.4 89.4 95.1 94.8 0 <1s 1s 16s/it 20s/it Effectiveness of the Reward Design. We conduct an ablation study to evaluate the individual contributions of our three reward components: rule-based reward, an LLM judge, and an MLLM judge. The results are 10 Figure 4 Example generated by LLM-I on LLMI-Bench. Some text is omitted due to space constraints. presented in Table 6. The full LLM-I-4B model, trained on all three rewards, establishes strong baseline with an overall score of 8.18, demonstrating the effectiveness of the combined reward strategy. Particularly, the removal of the rule-based reward proves to be the most detrimental, causing the overall score to plummet to 4.76. As shown in Figure 7, without the rule-based reward, the model will not generate the image to obtain high score. Comparatively, the performance drop is less severe when removing either the LLM or MLLM judge because their evaluation capabilities likely overlap. Both of these judges assess more nuanced, qualitative aspects of the output like text and image quality. Ultimately, the study confirms that while the rule-based reward provides an essential foundation, the synergistic combination of all three rewards is necessary to achieve the models peak performance. Tools. To assess the contribution of individual tools, we perform tool ablation study and report the results in Table 7. The results reveal the importance of comprehensive toolkit, especially for the trained LLMI-4B model. Restricting LLMI-4B to only diffusion or only search leads to significant performance degradation. This indicates that its high performance is contingent on its ability to flexibly leverage multiple tools. Figure 7 The rule-based reward curve during RL training. Interestingly, the Qwen3-4B models performance improves to 75.2 when restricted to only search, surpassing its full-toolkit baseline. This counterintuitive result suggests that while the model benefits from the search tool, it may struggle with tool selection when presented with multiple options when it is not trained to use the tools. Forcing it to use only its most effective tool eliminates potential errors in tool orchestration, thereby improving its overall score. From the table, we can observe that the combination of various tools greatly enhances the performance of the model after training. This demonstrates the effectiveness of our proficient tool user argument for interleaved generation. Table 7 Ablation experiments of the tools on LLMIBench. Model Rubric Qwen3-4B-Instruct wTool - only diffusion - only search LLM-I-4B - only diffusion - only search 73.6 66.5 75.2 88.9 76.5 77.5 11 Figure 5 Example generated by MLLM-I on LLMI-Bench. Some text is omitted due to space constraints."
        },
        {
            "title": "6 Conclusion",
            "content": "In this paper, we introduce LLM-Interleaved (LLM-I), framework that overcomes the one-tool bottleneck in interleaved image-text generation by employing an LLM as an agentic planner. This agent dynamically orchestrates suite of specialized tools, including web search, diffusion models, code execution, and image editing, to create rich multimodal narratives. LLM-I significantly outperforms state-of-the-art methods, demonstrating that powerful LLMs possess natural, emergent capability for complex multimodal creation when properly augmented. By championing flexible \"proficient tool-user\" paradigm, this work paves the way for future research into expanding the agents toolkit and enhancing its reasoning for more generalist and capable creative AI."
        },
        {
            "title": "References",
            "content": "Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2.5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Lee, Yufei Guo, et al. Improving image generation with better captions. Computer Science. https://cdn. openai. com/papers/dall-e-3. pdf, 2(3):8, 2023. Dongping Chen, Ruoxi Chen, Shu Pu, Zhaoyi Liu, Yanru Wu, Caixi Chen, Benlin Liu, Yue Huang, Yao Wan, Pan Zhou, and Ranjay Krishna. Interleaved scene graphs for interleaved text-and-image generation assessment. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum? id=rDLgnYLM5b. Ethan Chern, Jiadi Su, Yan Ma, and Pengfei Liu. Anole: An open, autoregressive, native large multimodal models for interleaved image-text generation. arXiv preprint arXiv:2407.06135, 2024. Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. Chaorui Deng, Deyao Zhu, Kunchang Li, Chenhui Gou, Feng Li, Zeyu Wang, Shu Zhong, Weihao Yu, Xiaonan Nie, Ziang Song, et al. Emerging properties in unified multimodal pretraining. arXiv preprint arXiv:2505.14683, 2025. Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first international conference on machine learning, 2024. Jiazhan Feng, Shijue Huang, Xingwei Qu, Ge Zhang, Yujia Qin, Baoquan Zhong, Chengquan Jiang, Jinxin Chi, and Wanjun Zhong. Retool: Reinforcement learning for strategic tool use in llms. arXiv preprint arXiv:2504.11536, 2025. Yu Gao, Lixue Gong, Qiushan Guo, Xiaoxia Hou, Zhichao Lai, Fanshi Li, Liang Li, Xiaochen Lian, Chao Liao, Liyang Liu, et al. Seedream 3.0 technical report. arXiv preprint arXiv:2504.11346, 2025. Yuying Ge, Sijie Zhao, Jinguo Zhu, Yixiao Ge, Kun Yi, Lin Song, Chen Li, Xiaohan Ding, and Ying Shan. Seed-x: Multimodal models with unified multi-granularity comprehension and generation. arXiv preprint arXiv:2404.14396, 2024. Google. Try deep research and our new experimental model in gemini, your ai assistant, 2024. URL https: //blog.google/products/gemini/google-gemini-deep-research/. Google. Introducing gemini 2.5 flash image, 2025a. URL https://developers.googleblog.com/ introducing-gemini-2-5-flash-image/. Google. Serpapi, 2025b. URL https://serpapi.com/. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025a. Zirun Guo, Minjie Hong, and Tao Jin. Observe-r1: Unlocking reasoning abilities of mllms with dynamic progressive reinforcement learning. arXiv preprint arXiv:2505.12432, 2025b. Minjie Hong, Zirun Guo, Yan Xia, Zehan Wang, Ziang Zhang, Tao Jin, and Zhou Zhao. Apo: Enhancing reasoning ability of mllms via asymmetric policy optimization. arXiv preprint arXiv:2506.21655, 2025. Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. Xiaoxi Li, Guanting Dong, Jiajie Jin, Yuyao Zhang, Yujia Zhou, Yutao Zhu, Peitian Zhang, and Zhicheng Dou. Search-o1: Agentic search-enhanced large reasoning models. arXiv preprint arXiv:2501.05366, 2025. Minqian Liu, Zhiyang Xu, Zihao Lin, Trevor Ashby, Joy Rimchala, Jiaxin Zhang, and Lifu Huang. Holistic evaluation for interleaved text-and-image generation. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 2200222016, 2024. 13 Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Candès, and Tatsunori Hashimoto. s1: Simple test-time scaling, 2025. URL https: //arxiv.org/abs/2501.19393. OpenAI. Deep research system card, 2025a. URL https://cdn.openai.com/deep-research-system-card.pdf. OpenAI. Introducing gpt-5, 2025b. URL https://openai.com/index/introducing-gpt-5/. OpenAI. Introducing openai o3 and o4-mini, 2025c. URL https://openai.com/index/ introducing-o3-and-o4-mini/. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms, 2017. URL https://arxiv.org/abs/1707.06347. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models, 2024. URL https://arxiv.org/abs/2402.03300. Charlie Victor Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling LLM test-time compute optimally can be more effective than scaling parameters for reasoning. In The Thirteenth International Conference on Learning Representations, 2025. Zhaochen Su, Linjie Li, Mingyang Song, Yunzhuo Hao, Zhengyuan Yang, Jun Zhang, Guanjie Chen, Jiawei Gu, Juntao Li, Xiaoye Qu, et al. Openthinkimg: Learning to think with images via visual tool reinforcement learning. arXiv preprint arXiv:2505.08617, 2025. Chameleon Team. Chameleon: Mixed-modal early-fusion foundation models. arXiv preprint arXiv:2405.09818, 2024. Changyao Tian, Xizhou Zhu, Yuwen Xiong, Weiyun Wang, Zhe Chen, Wenhai Wang, Yuntao Chen, Lewei Lu, Tong Lu, Jie Zhou, Hongsheng Li, Yu Qiao, and Jifeng Dai. Mm-interleaved: Interleaved image-text generative modeling via multi-modal feature synchronizer, 2024. URL https://arxiv.org/abs/2401.10208. Peng Wang, Yichun Shi, Xiaochen Lian, Zhonghua Zhai, Xin Xia, Xuefeng Xiao, Weilin Huang, and Jianchao Yang. Seededit 3.0: Fast and high-quality generative image editing. arXiv preprint arXiv:2506.05083, 2025. Jinming Wu, Zihao Deng, Wei Li, Yiding Liu, Bo You, Bo Li, Zejun Ma, and Ziwei Liu. Mmsearch-r1: Incentivizing lmms to search. arXiv preprint arXiv:2506.20670, 2025. Shengqiong Wu, Hao Fei, Leigang Qu, Wei Ji, and Tat-Seng Chua. Next-gpt: Any-to-any multimodal llm. In Forty-first International Conference on Machine Learning, 2024. Peng Xia, Siwei Han, Shi Qiu, Yiyang Zhou, Zhaoyang Wang, Wenhao Zheng, Zhaorun Chen, Chenhang Cui, Mingyu Ding, Linjie Li, Lijuan Wang, and Huaxiu Yao. MMIE: Massive multimodal interleaved comprehension benchmark for large vision-language models. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?id=HnhNRrLPwm. Jinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang, Weihao Wang, Kevin Qinghong Lin, Yuchao Gu, Zhijie Chen, Zhenheng Yang, and Mike Zheng Shou. Show-o: One single transformer to unify multimodal understanding and generation. In The Thirteenth International Conference on Learning Representations, 2025. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Weinan Dai, Tiantian Fan, Gaohong Liu, Lingjun Liu, Xin Liu, Haibin Lin, Zhiqi Lin, Bole Ma, Guangming Sheng, Yuxuan Tong, Chi Zhang, Mofan Zhang, Wang Zhang, Hang Zhu, Jinhua Zhu, Jiaze Chen, Jiangjie Chen, Chengyi Wang, Hongli Yu, Yuxuan Song, Xiangpeng Wei, Hao Zhou, Jingjing Liu, Wei-Ying Ma, Ya-Qin Zhang, Lin Yan, Mu Qiao, Yonghui Wu, and Mingxuan Wang. Dapo: An open-source llm reinforcement learning system at scale, 2025. URL https://arxiv.org/abs/2503.14476. Chujie Zheng, Shixuan Liu, Mingze Li, Xiong-Hui Chen, Bowen Yu, Chang Gao, Kai Dang, Yuqiong Liu, Rui Men, An Yang, et al. Group sequence policy optimization. arXiv preprint arXiv:2507.18071, 2025. Chunting Zhou, LILI YU, Arun Babu, Kushal Tirumala, Michihiro Yasunaga, Leonid Shamis, Jacob Kahn, Xuezhe Ma, Luke Zettlemoyer, and Omer Levy. Transfusion: Predict the next token and diffuse images with one multi-modal model. In The Thirteenth International Conference on Learning Representations, 2025a. 14 Pengfei Zhou, Xiaopeng Peng, Jiajun Song, Chuanhao Li, Zhaopan Xu, Yue Yang, Ziyao Guo, Hao Zhang, Yuqi Lin, Yefei He, et al. Opening: comprehensive benchmark for judging open-ended interleaved image-text generation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 5666, 2025b. Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Hao Tian, Yuchen Duan, Weijie Su, Jie Shao, et al. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models. arXiv preprint arXiv:2504.10479, 2025. 15 Figure 8 Examples in our training dataset."
        },
        {
            "title": "A Dataset Details",
            "content": "In Section 3.2.1, we detail the design of our dataset, where each item is associated with required number of images and corresponding tool list. However, rather than stating these metadata explicitly in the prompt, we guide the model to use specific tools and generate the correct number of images implicitly. Figure 8 presents four examples from our training set, with the text that guides the image generation highlighted in yellow. As shown, the prompts do not explicitly state how to generate the image, but the necessary tools are strongly suggested. For instance, in the first example, the phrase add yellow star to mark... implies the need for an image editing tool. Similarly, in the second example, the request for graph comparing the electric range suggests using code interpreter. Furthermore, the required number of images is also not explicitly stated. The model must therefore fully comprehend the prompts intent to determine the correct number of images to generate. We present the distribution of the datasets in Figure 9. 16 Figure 9 Distribution of our constructed training dataset. Figure 10 An example generated by LLM-I-4B in the ISG benchmark."
        },
        {
            "title": "B More Examples",
            "content": "To further demonstrate the superiority of LLM-I, we provide additional examples drawn from diverse benchmarks and model backbones. The generated results are shown in Figure 10, 11, 12, and 13. Whether on relatively simple tasks such as the ISG benchmark or OpenING benchmark, or on more challenging tasks such as LLMI-Bench, our method - scaling from 4B to 32B models - consistently produces rich, complete responses accompanied by high-quality and highly relevant images. These examples across multiple benchmarks clearly validate both the generalization ability and the superiority of our approach. 17 Figure 11 An example generated by LLM-I-30B in the OpenING benchmark. 18 Figure 12 An example generated by MLLM-I-7B in LLMI-Bench. 19 Figure 13 An example generated by MLLM-I-32B in LLMI-Bench."
        }
    ],
    "affiliations": [
        "BandAI",
        "ByteDance",
        "Zhejiang University"
    ]
}